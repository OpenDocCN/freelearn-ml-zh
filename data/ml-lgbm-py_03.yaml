- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: An Overview of LightGBM in Python
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 中 LightGBM 的概述
- en: In the previous chapter, we looked at ensemble learning methods for decision
    trees. Both **bootstrap aggregation** (**bagging**) and gradient boosting were
    discussed in detail, with practical examples of how to apply the techniques in
    scikit-learn. We also showed how **gradient-boosted decision trees** (**GBDTs**)
    are slow to train and may underperform on some problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了决策树的集成学习方法。详细讨论了 **bootstrap aggregation**（**bagging**）和梯度提升，并提供了如何在
    scikit-learn 中应用这些技术的实际示例。我们还展示了 **梯度提升决策树**（**GBDTs**）训练速度慢，在某些问题上可能表现不佳。
- en: This chapter introduces LightGBM, a gradient-boosting framework that uses tree-based
    learners. We look at the innovations and optimizations LightGBM makes to the ensemble
    learning methods. Further details and examples are given for using LightGBM practically
    via Python. Finally, the chapter includes a modeling example using LightGBM, incorporating
    more advanced techniques for model validation and parameter optimization.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 LightGBM，这是一个使用基于树的学习者的梯度提升框架。我们探讨了 LightGBM 对集成学习方法的创新和优化。还提供了使用 Python
    实际应用 LightGBM 的详细信息和示例。最后，本章包含了一个使用 LightGBM 的建模示例，结合了更高级的模型验证和参数优化技术。
- en: By the end of the chapter, you will have a thorough understanding of the theoretical
    and practical properties of LightGBM, allowing us to dive deeper into using LightGBM
    for data science and production systems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将对 LightGBM 的理论和实际特性有深入的了解，这将使我们能够更深入地探讨在数据科学和生产系统中使用 LightGBM。
- en: 'The main topics of this chapter are set out here:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要主题如下：
- en: Introducing LightGBM
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 LightGBM
- en: Getting started with LightGBM in Python
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中开始使用 LightGBM
- en: Building LightGBM models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建 LightGBM 模型
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The chapter includes examples and code excerpts illustrating how to use LightGBM
    in Python. Complete examples and instructions for setting up a suitable environment
    for this chapter are available a[t https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3)-3.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含示例和代码片段，说明如何在 Python 中使用 LightGBM。有关设置本章所需环境的完整示例和说明可在 [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3)
    找到。
- en: Introducing LightGBM
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 LightGBM
- en: LightGBM is an open source, gradient-boosting framework for tree-based ensembles
    ([https://github.com/microsoft/LightGBM](https://github.com/microsoft/LightGBM)).
    LightGBM focuses on efficiency in speed, memory usage, and improved accuracy,
    especially for problems with high dimensionality and large data sizes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 是一个开源的基于树的集成梯度提升框架 ([https://github.com/microsoft/LightGBM](https://github.com/microsoft/LightGBM))。LightGBM
    专注于速度、内存使用和改进的准确性，特别是在高维度和大数据量问题上。
- en: 'LightGBM was first introduced in the paper *LightGBM: A Highly Efficient Gradient
    Boosting Decision* *Tree* [1].'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 'LightGBM 首次在论文 *LightGBM: A Highly Efficient Gradient Boosting Decision Tree*
    [1] 中介绍。'
- en: The efficiency and accuracy of LightGBM are achieved via several technical and
    theoretical optimizations to the standard ensemble learning methods, particularly
    GBDTs. Additionally, LightGBM supports distributed training of ensembles with
    optimizations in network communication and support for GPU-based training of tree
    ensembles.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 的效率和精度是通过针对标准集成学习方法的几个技术和理论优化实现的，特别是 GBDTs。此外，LightGBM 支持通过优化网络通信和基于
    GPU 的树集成训练进行分布式训练。
- en: 'LightGBM supports many **machine learning** (**ML**) applications: regression,
    binary and multiclass classification, cross-entropy loss functions, and ranking
    via LambdaRank.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 支持许多 **机器学习**（**ML**）应用：回归、二分类和多分类、交叉熵损失函数以及通过 LambdaRank 进行排序。
- en: The LightGBM algorithm is also very customizable via its hyperparameters. It
    supports many metrics and features, including **Dropouts meet Multiple Additive
    Regression Trees** (**DART**), bagging (random forests), continuous training,
    multiple metrics, and early stopping.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 算法也可以通过其超参数进行高度定制。它支持许多指标和功能，包括 **Dropouts meet Multiple Additive Regression
    Trees**（**DART**）、bagging（随机森林）、连续训练、多个指标和早期停止。
- en: This section reviews the theoretical and practical optimizations LightGBM utilizes,
    including a detailed overview of the hyperparameters to control LightGBM features.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了LightGBM使用的理论和实践优化，包括控制LightGBM特征的超参数的详细概述。
- en: LightGBM optimizations
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LightGBM优化
- en: At its core, LightGBM implements the same ensemble algorithms we discussed in
    the previous chapter. However, LightGBM applies theoretical and technical optimizations
    to improve performance and accuracy while significantly reducing memory usage.
    Next, we discuss the most significant optimizations implemented in LightGBM.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，LightGBM实现了我们在上一章中讨论的相同集成算法。然而，LightGBM通过理论和技术的优化来提高性能和准确性，同时显著减少内存使用。接下来，我们将讨论LightGBM中实施的最显著的优化。
- en: Computational complexity in GBDTs
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GBDT中的计算复杂度
- en: 'First, we must understand where the inefficiency in building GBDTs stems from
    to understand how LightGBM improves the efficiency of GBDTs. The most computationally
    complex part of the GBDT algorithm is training the regression tree for each iteration.
    More specifically, finding the optimal split is very expensive. Pre-sort-based
    algorithms are among the most popular methods for finding the best splits [2],
    [3]. A naïve approach requires the data to be sorted by feature for every decision
    node with algorithmic complexity O(#data × #feature). Pre-sort-based algorithms
    sort the data once before training, which reduces the complexity when building
    a decision node to O(#data) [2]. Even with pre-sorting, the complexity is too
    high for large datasets when finding splits for decision nodes.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，我们必须理解构建GBDT中的低效性来源，才能理解LightGBM如何提高GBDT的效率。GBDT算法中最计算复杂的部分是每次迭代的回归树训练。更具体地说，找到最优分割是非常昂贵的。基于预排序的算法是寻找最佳分割的最流行方法之一[2]，[3]。一种简单的方法要求对每个决策节点按特征对数据进行排序，算法复杂度为O(#data
    × #feature)。基于预排序的算法在训练前对数据进行一次排序，这降低了构建决策节点的复杂度到O(#data) [2]。即使有预排序，当寻找决策节点的分割时，复杂度对于大型数据集来说仍然太高。'
- en: Histogram-based sampling
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于直方图的采样
- en: 'An alternative approach to pre-sorting involves building histograms for continuous
    features [4]. The continuous values are added into discrete bins when building
    these **feature histograms**. Instead of using the data directly when calculating
    the splits for decision nodes, we can now use the histogram bins. Constructing
    the histograms has a complexity of O(#data). However, the complexity for building
    a decision node now reduces to O(#bins), and since the number of bins is much
    smaller than the amount of data, this significantly speeds up the process of building
    regression trees, as illustrated in the following diagram:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 预排序的另一种方法涉及为连续特征构建直方图[4]。在构建这些**特征直方图**时，连续值被添加到离散的箱中。在计算决策节点的分割时，我们不再直接使用数据，而是现在可以使用直方图箱。构建直方图的复杂度为O(#data)。然而，构建决策节点的复杂度现在降低到O(#bins)，由于箱的数量远小于数据量，这显著加快了构建回归树的过程，如下面的图所示：
- en: '![Figure 3.1 – Creating feature histograms from continuous features allows
    calculating splits for decision nodes using bin boundary values instead of having
    to sample each data point, significantly reducing the algorithm’s complexity since
    #bins << #data](img/B16690_03_1.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1 – 从连续特征创建特征直方图允许使用箱边界值来计算决策节点的分割，而不是必须对每个数据点进行采样，这显著降低了算法的复杂性，因为#bins
    << #data](img/B16690_03_1.jpg)'
- en: 'Figure 3.1 – Creating feature histograms from continuous features allows calculating
    splits for decision nodes using bin boundary values instead of having to sample
    each data point, significantly reducing the algorithm’s complexity since #bins
    << #data'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '图3.1 – 从连续特征创建特征直方图允许使用箱边界值来计算决策节点的分割，而不是必须对每个数据点进行采样，这显著降低了算法的复杂性，因为#bins
    << #data'
- en: A secondary optimization that stems from using histograms is “histogram subtraction”
    for building the histograms for the leaves. Instead of calculating the histogram
    for each leaf, we can subtract the leaf’s neighbor’s histogram from the parent’s
    histogram. Choosing the leaf with the smaller amount of data leads to a smaller
    O(#data) complexity for the first leaf and O(#bin) complexity for the second leaf
    due to histogram subtraction.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由使用直方图产生的二级优化是“直方图减法”，用于构建叶子的直方图。我们不需要为每个叶子计算直方图，而是可以从父直方图中减去叶子的邻居直方图。选择数据量较小的叶子会导致第一个叶子的O(#data)复杂度较小，由于直方图减法，第二个叶子的O(#bin)复杂度较小。
- en: 'A third optimization that LightGBM applies using histograms is to reduce the
    memory cost. Feature pre-sorting requires a supporting data structure (a dictionary)
    for each feature. No such data structures are required when building histograms,
    reducing memory costs. Further, since #bins is small, a smaller data type, such
    as `uint8_t`, can store the training data, reducing memory usage.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM使用直方图应用的一种第三种优化是减少内存成本。特征预排序需要为每个特征提供一个支持数据结构（一个字典）。在构建直方图时不需要这样的数据结构，从而降低了内存成本。此外，由于#bins很小，可以使用较小的数据类型，如`uint8_t`来存储训练数据，从而减少内存使用。
- en: 'Detailed information regarding the algorithms for building feature histograms
    is available in the paper *CLOUDS: A decision tree classifier for large* *datasets*
    [4].'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 关于构建特征直方图算法的详细信息可在论文《CLOUDS：用于大型*数据集*的决策树分类器》[4]中找到。
- en: Exclusive Feature Bundling
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独家功能捆绑
- en: '**Exclusive Feature Bundling** (**EFB**) is another data-based optimization
    that LightGBM applies when working with sparse data (sparse data is pervasive
    in high-dimensional datasets). When the feature data is sparse, it’s common to
    find that many features are *mutually exclusive*, signifying they never present
    non-zero values simultaneously. Combining these features into a single one is
    generally safe, given this exclusivity. EFB is illustrated in the following diagram:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**独家功能捆绑**（**EFB**）是LightGBM在处理稀疏数据（稀疏数据在高维数据集中普遍存在）时应用的一种基于数据的优化。当特征数据稀疏时，通常会发现许多特征是**相互排斥**的，这意味着它们永远不会同时呈现非零值。考虑到这种排他性，将这些特征组合成一个单一的特征通常是安全的。EFB在以下图中展示：'
- en: '![Figure 3.2 – Building a feature bundle from two mutually exclusive features](img/B16690_03_2.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图3.2 – 从两个相互排斥的特征构建特征捆绑](img/B16690_03_2.jpg)'
- en: Figure 3.2 – Building a feature bundle from two mutually exclusive features
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – 从两个相互排斥的特征构建特征捆绑
- en: 'Bundling mutually exclusive features allows building the same feature histograms
    as from the individual features [1]. The optimization reduces the complexity of
    building feature histograms from O(#data × #feature) to O(#data × #bundle). For
    datasets where there are many mutually exclusive features, this dramatically improves
    performance since # bundle ≪ #feature. Detailed algorithms for, and proof of the
    correctness of, EFB are available in [1].'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '将相互排斥的特征捆绑在一起，可以构建与单个特征相同的特征直方图[1]。这种优化将构建特征直方图的复杂度从O(#数据 × #特征)降低到O(#数据 ×
    #捆绑)。对于存在许多相互排斥特征的数据库，这显著提高了性能，因为#捆绑远小于#特征。EFB的详细算法及其正确性的证明可在[1]中找到。'
- en: Gradient-based One-Side Sampling
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于梯度的单侧采样
- en: A final data-based optimization available in the LightGBM framework is **Gradient-based
    One-Side Sampling** (**GOSS**) [1]. GOSS is a method of discarding training data
    samples that no longer contribute significantly to the training process, effectively
    reducing the training data size and speeding up the process.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM框架中可用的最后一种基于数据的优化是**基于梯度的单侧采样**（**GOSS**）[1]。GOSS是一种丢弃不再对训练过程有显著贡献的训练数据样本的方法，从而有效地减少了训练数据的大小并加快了过程。
- en: We can use the gradient calculation of each sample to determine its importance.
    If the gradient change is small, it implies that the training error was also small,
    and we can infer that the tree is well fitted to the specific data instance [1].
    One option would be to discard all instances with small gradients. However, this
    changes the distribution of the training data, reducing the tree’s ability to
    generalize. GOSS is a method for choosing which instances to keep in the training
    data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用每个样本的梯度计算来确定其重要性。如果梯度变化很小，这表明训练误差也很小，我们可以推断出树对特定数据实例拟合得很好[1]。一个选择是丢弃所有梯度小的实例。然而，这改变了训练数据的分布，减少了树泛化的能力。GOSS是一种选择保留在训练数据中的实例的方法。
- en: 'To maintain the data distribution, GOSS is applied as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持数据分布，GOSS按照以下方式应用：
- en: The data samples are sorted by the absolute value of their gradients.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据样本按其梯度的绝对值排序。
- en: The top a × 100% instances are then selected (instances with large gradients).
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后选择前a × 100%的实例（梯度大的实例）。
- en: A random sample of b × 100% instances is then taken from the rest of the data.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后从剩余的数据中随机抽取b × 100%的实例样本。
- en: 'A factor is added to the loss function (for these instances) to amplify their
    influence: 1 − a _ b , thereby compensating for the underrepresentation of data
    with small gradients.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在损失函数（对于这些实例）中添加一个因子以放大其影响：1 − a _ b，从而补偿小梯度数据的代表性不足。
- en: Therefore, GOSS samples a large portion of instances with large gradients and
    a random portion of instances with small gradients and amplifies the influence
    of the small gradients when calculating information gain.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，GOSS从具有大梯度的实例中采样大量实例，并从具有小梯度的实例中随机采样一部分实例，在计算信息增益时放大小梯度的影响。
- en: The downsampling enabled by GOSS can significantly reduce the amount of data
    processed during training (and the training time for the GBDTs), especially in
    the case of large datasets.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GOSS启用的下采样可以显著减少训练过程中处理的数据量（以及GBDT的训练时间），尤其是在大型数据集的情况下。
- en: Best-first tree growth
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最佳优先树增长
- en: The most common method for building decision trees is to grow the tree by level
    (that is, one level at a time). LightGBM uses an alternative approach and grows
    the tree leaf-wise or best-first. The leaf-wise approach selects an existing leaf
    with the most significant change in the loss of the tree and builds the tree from
    there. The downside of this approach is that if the dataset is small, the tree
    is likely to overfit the data. A maximum depth has to be set to counteract this.
    However, if the number of leaves to construct is fixed, leaf-wise tree building
    is shown to outperform level-wise algorithms [5].
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 构建决策树最常见的方法是按层次增长（即，一次增长一个层次）。LightGBM采用了一种替代方法，通过叶节点或最佳优先的方式增长树。叶节点方法选择具有最大损失变化的现有叶节点，并从那里构建树。这种方法的一个缺点是，如果数据集很小，树很可能会过拟合数据。必须设置最大深度来抵消这一点。然而，如果构建的叶节点数量是固定的，叶节点树构建已被证明优于层次算法[5]。
- en: L1 and L2 regularization
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: L1 和 L2 正则化
- en: LightGBM supports both L1 and L2 regularization of the objective function when
    training the regression trees in the ensemble. From [*Chapter 1*](B16690_01.xhtml#_idTextAnchor014)*,
    Introducing Machine Learning*, we recall that regularization is a way to control
    overfitting. In the case of decision trees, simpler, shallow trees overfit less.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 在集成中训练回归树时支持目标函数的 L1 和 L2 正则化。从 [*第1章*](B16690_01.xhtml#_idTextAnchor014)
    *介绍机器学习* 中，我们回忆起正则化是控制过拟合的一种方法。在决策树的情况下，更简单、更浅的树过拟合较少。
- en: 'To support L1 and L2 regularization, we extend the objective function with
    a regularization term, as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持L1和L2正则化，我们通过添加正则化项扩展了目标函数，如下所示：
- en: obj = L(y, F(x)) + Ω(w)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: obj = L(y, F(x)) + Ω(w)
- en: Here, L(y, F(x)) is the loss function discussed in [*Chapter 2*](B16690_02.xhtml#_idTextAnchor036)*,
    Ensemble Learning – Bagging and Boosting*, and Ω(w) is the regularization function
    defined over w, the leaf scores (the leaf score is the output calculated from
    the leaf as per *step 2.3* in the GBDT algorithm defined in [*Chapter 2*](B16690_02.xhtml#_idTextAnchor036)*,
    Ensemble Learning – Bagging* *and Boosting*).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，L(y, F(x)) 是在[*第2章*](B16690_02.xhtml#_idTextAnchor036)中讨论的损失函数，*集成学习 – Bagging
    和 Boosting*，而 Ω(w) 是定义在 w 上的正则化函数，即叶得分（叶得分是根据 GBDT 算法中定义的 *步骤 2.3* 计算的叶输出，该算法在[*第2章*](B16690_02.xhtml#_idTextAnchor036)中讨论，*集成学习
    – Bagging 和 Boosting*）。
- en: The regularization term effectively adds a penalty to the objective function,
    where we aim to penalize more complex trees prone to overfitting.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化项有效地向目标函数添加了惩罚，我们的目标是惩罚更复杂的树，这些树容易过拟合。
- en: 'There are multiple definitions for Ω. A typical implementation for the terms
    in decision trees is this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Ω 有多个定义。决策树中这些项的典型实现如下：
- en: Ω(w) = α∑ i n |w i| + λ∑ i n w i 2
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Ω(w) = α∑ i n |w i| + λ∑ i n w i 2
- en: Here, α∑ i n |w i| is the L1 regularization term, controlled by the parameter
    α, 0 ≤ α ≤ 1, and λ∑ i n w i 2 is the L2 regularization term, controlled by the
    parameter λ.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，α∑ i n |w i| 是由参数 α 控制的 L1 正则化项，0 ≤ α ≤ 1，而 λ∑ i n w i 2 是由参数 λ 控制的 L2 正则化项。
- en: L1 regularization has the effect of driving leaf scores to zero by penalizing
    leaves with large absolute outputs. *Smaller leaf outputs have a smaller effect
    on the tree’s prediction, effectively simplifying* *the tree*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: L1 正则化通过惩罚具有大绝对输出的叶节点，将叶得分驱动到零。*较小的叶输出对树的预测影响较小，从而有效地简化了* *树*。
- en: L2 regularization is similar but has an outsized effect on outliers’ leaves
    due to taking the square of the output.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: L2 正则化类似，但由于输出取平方，对异常值叶节点有更大的影响。
- en: Finally, when larger trees are built (trees with more leaves, and therefore
    a large w vector), both sum terms for Ω(w) increase, increasing the objective
    function output. Therefore, *larger trees are penalized*, and overfitting is reduced.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当构建较大的树（具有更多叶节点，因此具有较大的 w 向量）时，Ω(w) 的两个求和项都会增加，从而增加目标函数的输出。因此，*较大的树会受到惩罚*，从而减少过拟合。
- en: Summary of LightGBM optimizations
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LightGBM优化总结
- en: 'In summary, LightGBM improves upon the standard ensemble algorithms by doing
    the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，LightGBM通过以下方式改进了标准集成算法：
- en: Implementing histogram-based sampling of features to reduce the computational
    cost of finding optimal splits
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现基于直方图的采样特征以减少寻找最优分割的计算成本
- en: Calculating exclusive feature bundles to reduce the number of features in sparse
    datasets
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过计算独家特征包来减少稀疏数据集中的特征数量
- en: Applying GOSS to downsample the training data without losing accuracy
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用GOSS以在不损失准确性的情况下对训练数据进行下采样
- en: Building trees leaf-wise to improve accuracy
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以叶节点的方式构建树以提高准确性
- en: Overfitting can be controlled through L1 and L2 regularization and other control
    parameters
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过L1和L2正则化以及其他控制参数可以控制过拟合
- en: In conjunction, the optimizations improve the computational performance of LightGBM
    by **orders of magnitude** (**OOM**) over the standard GBDT algorithm. Additionally,
    LightGBM is implemented in C++ with a Python interface, which results in much
    faster code than Python-based GBDTs, such as in scikit-learn.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 结合优化，这些优化将LightGBM的计算性能提高了与标准GBDT算法相比的**数量级**（**OOM**）。此外，LightGBM是用C++实现的，具有Python接口，这使得代码比基于Python的GBDT（如scikit-learn）快得多。
- en: Finally, LightGBM also has support for improved data-parallel and feature-parallel
    distributed training. Distributed training and GPU support are discussed in a
    later [*Chapter 11*](B16690_11.xhtml#_idTextAnchor177)*, Distributed and GPU-Based
    Learning* *with LightGBM*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，LightGBM还支持改进的数据并行和特征并行分布式训练。分布式训练和GPU支持将在后面的[*第11章*](B16690_11.xhtml#_idTextAnchor177)*，使用LightGBM的分布式和基于GPU的学习*中讨论。
- en: Hyperparameters
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数
- en: LightGBM exposes many parameters that can be used to customize the training
    process, goals, and performance. Next, we discuss the most notable parameters
    and how they may be used to control specific phenomena.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM公开了许多参数，可用于自定义训练过程、目标和性能。接下来，我们将讨论最显著的参数以及它们如何用于控制特定现象。
- en: Note
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The core LightGBM framework is developed in C++ but includes APIs to work with
    LightGBM in C, Python, and R. The parameters discussed in this section are the
    framework parameters and are exposed differently by each API. The following section
    discusses the parameters available when using Python.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 核心LightGBM框架是用C++开发的，但包括用于在C、Python和R中与LightGBM一起工作的API。本节讨论的参数是框架参数，并且每个API以不同的方式暴露。以下章节将讨论使用Python时可用参数。
- en: 'The following are **core framework parameters** used to control the optimization
    process and goal:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用以控制优化过程和目标的**核心框架参数**：
- en: '`objective`: LightGBM supports the following optimization objectives, among
    others—`regression` (including regression applications with other loss functions
    such as Huber and Fair), `binary` (classification), `multiclass` (classification),
    `cross-entropy`, and `lambdarank` for ranking problems.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`目标`：LightGBM支持以下优化目标，包括但不限于——`回归`（包括具有Huber和Fair等损失函数的回归应用），`二元`（分类），`多类`（分类），`交叉熵`，以及用于排序问题的`lambdarank`。'
- en: '`boosting`: The boosting parameter controls the boosting type. By default,
    this is set to `gbdt`, the standard GBDT algorithm. The other options are `dart`
    and `rf` for random forests. The random forest mode does not perform boosting
    but instead builds a random forest.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boosting`：提升参数控制提升类型。默认情况下，此参数设置为`gbdt`，即标准GBDT算法。其他选项是`dart`和`rf`，用于随机森林。随机森林模式不执行提升，而是构建随机森林。'
- en: '`num_iterations` (or `n_estimators`): Controls the number of boosting iterations
    and, therefore, the number of trees built.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_iterations`（或`n_estimators`）：控制提升迭代次数，因此也控制构建的树的数量。'
- en: '`num_leaves`: Controls the maximum number of leaves in a single tree.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_leaves`：控制单个树中的最大叶节点数。'
- en: '`learning_rate`: Controls the learning, or shrinkage rate, which is the contribution
    of each tree to the overall prediction.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：控制学习或收缩率，即每个树对整体预测的贡献。'
- en: LightGBM also provides many parameters to control the learning process. We’ll
    discuss these parameters relative to how they may be used to tune specific aspects
    of training.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM还提供了许多参数来控制学习过程。我们将讨论这些参数相对于它们如何用于调整训练的特定方面。
- en: 'The following control parameters can be used to improve **accuracy**:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下控制参数可用于提高**准确性**：
- en: '`boosting`: Use `dart`, which has been shown to outperform standard GBDTs.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boosting`：使用`dart`，这已被证明优于标准GBDT。'
- en: '`learning_rate`: The learning rate must be tuned alongside `num_iterations`
    for better accuracy. A small learning rate with a large value for `num_iterations`
    leads to better accuracy at the expense of optimization speed.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：学习率必须与`num_iterations`一起调整以获得更好的准确率。较小的学习率与较大的`num_iterations`值相结合，可以在牺牲优化速度的情况下提高准确率。'
- en: '`num_leaves`: A larger number of leaves improves accuracy but may lead to overfitting.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_leaves`：较大的叶子数量可以提高准确率，但可能导致过拟合。'
- en: '`max_bin`: The maximum number of bins in which features are bucketed when constructing
    histograms. A larger `max_bin` size slows the training and uses more memory but
    may improve accuracy.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_bin`：在构建直方图时，将特征分桶的最大数量。较大的`max_bin`大小会减慢训练速度并使用更多内存，但可能提高准确率。'
- en: 'The following **learning control parameters** can be used to deal with **overfitting**:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下**学习控制参数**可用于处理**过拟合**：
- en: '`bagging_fraction` and `bagging_freq`: Setting both parameters enables feature
    bagging. Bagging may be used in addition to boosting and doesn’t force the use
    of a random forest. Enabling bagging reduces overfitting.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bagging_fraction`和`bagging_freq`：设置这两个参数可以启用特征袋装。除了提升之外，还可以使用袋装，并且不强制使用随机森林。启用袋装可以减少过拟合。'
- en: '`early_stopping_round`: Enables early stopping and controls the number of iterations
    used to determine whether training should be stopped. Training is stopped if no
    improvement is made to any metric in the iterations set by `early_stopping_round`.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`early_stopping_round`：启用早期停止并控制用于确定是否停止训练的迭代次数。如果在`early_stopping_round`设置的迭代中任何指标没有改进，则停止训练。'
- en: '`min_data_in_leaf`: The minimum samples allowed in a leaf. Larger values reduce
    overfitting.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_data_in_leaf`：叶子中允许的最小样本数。较大的值可以减少过拟合。'
- en: '`min_gain_to_split`: The minimum amount of information gain required to perform
    a split. Higher values reduce overfitting.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_gain_to_split`：执行分割所需的最小信息增益量。较高的值可以减少过拟合。'
- en: '`reg_alpha`: Controls L1 regularization. Higher values reduce overfitting.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reg_alpha`：控制L1正则化。较高的值可以减少过拟合。'
- en: '`reg_lambda`: Controls L2 regularization. Higher values reduce overfitting.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reg_lambda`：控制L2正则化。较高的值可以减少过拟合。'
- en: '`max_depth`: Controls the maximum depth of individual trees. Shallower trees
    reduce overfitting.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：控制单个树的最大深度。较浅的树可以减少过拟合。'
- en: '`max_drop`: Controls the maximum number of dropped trees when using the DART
    algorithm (is only used when `boosting` is set to `dart`). A larger value reduces
    overfitting.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_drop`：控制使用DART算法（仅在`boosting`设置为`dart`时使用）时丢弃的最大树的数量。较大的值可以减少过拟合。'
- en: '`extra_trees`: Enables the **Extremely Randomized Trees** (**ExtraTrees**)
    algorithm. LightGBM then chooses a split threshold at random for each feature.
    Enabling Extra-Trees can reduce overfitting. The parameter can be used in conjunction
    with any boosting mode.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extra_trees`：启用**极端随机树**（**ExtraTrees**）算法。LightGBM将为每个特征随机选择一个分割阈值。启用Extra-Trees可以减少过拟合。该参数可以与任何提升模式一起使用。'
- en: 'The parameters discussed here include only some of the parameters available
    in LightGBM and focus on improving accuracy and overfitting. A complete list of
    parameters [is available at the following link: https://lightgbm.rea](https://lightgbm.readthedocs.io/en/latest/Parameters.xhtml)dthedocs.io/en/latest/Parameters.xhtml.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里讨论的参数仅包括LightGBM中可用参数的一部分，并专注于提高准确性和防止过拟合。完整的参数列表[可在以下链接中找到：https://lightgbm.readthedocs.io/en/latest/Parameters.xhtml](https://lightgbm.readthedocs.io/en/latest/Parameters.xhtml)。
- en: Limitations of LightGBM
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LightGBM的局限性
- en: 'LightGBM is designed to be more efficient and effective than traditional methods.
    It is particularly well known for its ability to handle large datasets. However,
    as with any algorithm or framework, it also has its limitations and potential
    disadvantages, including the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM被设计得比传统方法更高效和有效。它特别以其处理大数据集的能力而闻名。然而，与任何算法或框架一样，它也有其局限性和潜在缺点，包括以下内容：
- en: '**Sensitive to overfitting**: LightGBM can be sensitive to overfitting, especially
    with small or noisy datasets. Care should be taken to monitor and control for
    overfitting when using LightGBM.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对过拟合敏感**：LightGBM可能对过拟合敏感，尤其是在小或噪声数据集上。在使用LightGBM时，应小心监控和控制过拟合。'
- en: '**Optimal performance requires tuning**: As discussed previously, LightGBM
    has many hyperparameters that need to be properly tuned to get the best performance
    from the algorithm.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最佳性能需要调整**：如前所述，LightGBM有许多超参数需要适当调整，以从算法中获得最佳性能。'
- en: '**Lack of representation learning**: Unlike **deep learning** (**DL**) approaches,
    which excel at learning from raw data, LightGBM requires feature engineering to
    be applied to the data before learning. Feature engineering is a time-consuming
    process that requires domain knowledge.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏表示学习**：与擅长从原始数据中学习的 **深度学习**（**DL**）方法不同，LightGBM 在学习之前需要应用特征工程到数据上。特征工程是一个耗时且需要领域知识的过程。'
- en: '**Handling sequential data**: LightGBM is not inherently designed for working
    with sequential data such as time series. For LightGBM to be used with time-series
    data, feature engineering needs to be applied to create lagged features and capture
    temporal dependencies.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理序列数据**：LightGBM 本身并不是为处理序列数据（如时间序列）而设计的。为了使用 LightGBM 处理时间序列数据，需要应用特征工程来创建滞后特征并捕捉时间依赖性。'
- en: '**Complex interactions and non-linearities**: LightGBM is a decision-tree-driven
    approach that might be incapable of capturing complex feature interactions and
    non-linearities. Proper feature engineering needs to be applied to ensure the
    algorithm models these.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂交互和非线性**：LightGBM 是一种以决策树为驱动的方法，可能无法捕捉复杂的特征交互和非线性。需要应用适当的特征工程来确保算法能够建模这些。'
- en: Although these are potential limitations of using the algorithm, they may not
    apply to all use cases. LightGBM is often a very effective tool in the right circumstances.
    As with any model, understanding the trade-offs is vital to making the right choice
    for your application.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些是使用该算法的潜在局限性，但它们可能并不适用于所有用例。在适当的情境下，LightGBM 经常是一个非常有效的工具。与任何模型一样，理解权衡对于为您的应用程序做出正确的选择至关重要。
- en: In the next session, we look at getting started using the various LightGBM APIs
    with Python.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何使用 Python 的各种 LightGBM API 开始使用。
- en: Getting started with LightGBM in Python
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Python 中开始使用 LightGBM
- en: 'LightGBM is implemented in C++ but has official C, R, and Python APIs. This
    section discusses the Python APIs that are available for working with LightGBM.
    LightGBM provides three Python APIs: the standard **LightGBM** API, the **scikit-learn**
    API (which is fully compatible with other scikit-learn functionality), and a **Dask**
    API for working with Dask. Dask is a parallel computing library discussed in [*Chapter
    11*](B16690_11.xhtml#_idTextAnchor177)*, Distribu*[*ted and GPU-Based Lea*](https://www.dask.org/)*rning
    with* *LightGBM* ([https://www.dask.org/](https://www.dask.org/)).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 使用 C++ 实现，但提供了官方的 C、R 和 Python API。本节讨论可用于与 LightGBM 一起工作的 Python API。LightGBM
    提供了三个 Python API：标准的 **LightGBM** API、与其它 scikit-learn 功能完全兼容的 **scikit-learn**
    API，以及用于与 Dask 一起工作的 **Dask** API。Dask 是在第 [*第 11 章*](B16690_11.xhtml#_idTextAnchor177)*
    中讨论的并行计算库，*分布式和基于 GPU 的学习* ([https://www.dask.org/](https://www.dask.org/))。
- en: Throughout the rest of the book, we mainly use the scikit-learn API for LightGBM,
    but let’s first look at the standard Python API.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的其余部分，我们主要使用 LightGBM 的 scikit-learn API，但让我们首先看看标准的 Python API。
- en: LightGBM Python API
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LightGBM Python API
- en: The best way to dive into the Python API is with a hands-on example. The following
    are excerpts from a code listing that illustrates the use of the LightGBM Python
    API. The complete code example is available at [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 深入了解 Python API 的最佳方式是通过动手示例。以下是从代码列表中摘录的片段，说明了 LightGBM Python API 的使用。完整的代码示例可在
    [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3)
    找到。
- en: 'LightGBM needs to be imported. The import is often abbreviated as `lgb`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 需要导入 LightGBM。导入通常简写为 `lgb`：
- en: '[PRE0]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: LightGBM provides a `Dataset` wrapper class to work with data. `Dataset` supports
    a variety of formats. Commonly, it is used to wrap a `numpy` array or a `pandas`
    DataFrame. `Dataset` also accepts a `Path` to a CSV, TSV, LIBSVM text file, or
    LightGBM `Dataset` binary file. When a path is supplied, LightGBM loads the data
    from the disk.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 提供了一个 `Dataset` 包装类来处理数据。`Dataset` 支持多种格式。通常，它用于包装 `numpy` 数组或 `pandas`
    DataFrame。`Dataset` 还接受 CSV、TSV、LIBSVM 文本文件或 LightGBM `Dataset` 二进制文件的 `Path`。当提供路径时，LightGBM
    会从磁盘加载数据。
- en: 'Here, we load our Forest Cover dataset from `sklearn` and wrap the `numpy`
    arrays in a LightGBM `Dataset`:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从 `sklearn` 加载我们的 Forest Cover 数据集，并将 `numpy` 数组包装在 LightGBM 的 `Dataset`
    中：
- en: '[PRE1]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We subtract 1 from the `y_train` and `y_test` arrays because the classes supplied
    by `sklearn` are labeled in the range [1, 7], whereas LightGBM expects zero-indexed
    class labels in the range [0, 7].
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`y_train`和`y_test`数组中减去1，因为`sklearn`提供的类别标签在范围[1, 7]内，而LightGBM期望零索引的类别标签在范围[0,
    7]内。
- en: 'We cannot set up the parameters for training. We’ll be using the following
    parameters:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法设置训练的参数。我们将使用以下参数：
- en: '[PRE2]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We are using the standard GBDT as a boosting type and setting the objective
    to multiclass classification for seven classes. During training, we are going
    to capture the `auc_mu` metric. AU C μ is a multiclass adaptation of the **area
    under the receiver operating characteristic curve** (**AUC**), as defined by Kleiman
    and Page [6].
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用标准的GBDT作为提升类型，并将目标设置为七类的多分类。在训练过程中，我们将捕获`auc_mu`指标。AU C μ是多类版本的**受试者工作特征曲线下面积**（**AUC**），如Kleiman和Page
    [6]所定义。
- en: We set `num_leaves` and `learning_rate` to reasonable values for the problem.
    Finally, we specify `force_row_wise` as `True`, a recommended setting for large
    datasets.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`num_leaves`和`learning_rate`设置为适合该问题的合理值。最后，我们指定`force_row_wise`为`True`，这是大型数据集的一个推荐设置。
- en: 'LightGBM’s training function also supports **callbacks**. A callback is a hook
    into the training process that is executed each boosting iteration. To illustrate
    their purpose, we’ll be using the following callbacks:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM的训练函数也支持**回调**。回调是训练过程中的一个钩子，在每个提升迭代中执行。为了说明它们的目的，我们将使用以下回调：
- en: '[PRE3]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We use the `log_evaluation` callback with a period of 15, which logs (prints)
    our metrics to standard output every 15 boosting iterations. We also set a `record_evaluation`
    callback that captures our evaluation metrics in the `metrics` dictionary. We
    also specify an `early_stopping` callback, with stopping rounds set to 15\. The
    `early_stopping` callback stops training if no validation metrics improve after
    the specified number of stopping rounds.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`log_evaluation`回调，周期为15，它每15次提升迭代将我们的指标记录（打印）到标准输出。我们还设置了一个`record_evaluation`回调，它将我们的评估指标捕获在`metrics`字典中。我们还指定了一个`early_stopping`回调，停止轮次设置为15。如果经过指定的停止轮次后没有验证指标改进，`early_stopping`回调将停止训练。
- en: 'Finally, we also use the `reset_parameter` callback to implement **learning
    rate decay**. The decay function is defined as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还使用`reset_parameter`回调来实现**学习率衰减**。衰减函数定义如下：
- en: '[PRE4]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `reset_parameter` callback takes a function as input. The function receives
    the current iteration and returns the parameter value. Learning rate decay is
    a technique where we decrease the value of the learning rate over time. Learning
    rate decay improved the overall accuracy achieved. Ideally, we want the initial
    trees to have a more significant impact on correcting the prediction errors. In
    contrast, later on, we want to reduce the impact of additional trees and have
    them make minor adjustments to the errors. We implement a slight exponential decay
    that reduces the learning rate from 0.09 to 0.078 throughout training.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`reset_parameter`回调接受一个函数作为输入。该函数接收当前迭代次数并返回参数值。学习率衰减是一种技术，随着时间的推移降低学习率的值。学习率衰减提高了整体准确度。理想情况下，我们希望初始树对纠正预测错误有更大的影响。相比之下，后期我们希望减少额外树的影响，并让它们对错误进行微小调整。我们在整个训练过程中实施了一种轻微的指数衰减，将学习率从0.09降低到0.078。'
- en: 'Now, we are ready for training. We use `lgb.train` to train the model:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好进行训练。我们使用`lgb.train`来训练模型：
- en: '[PRE5]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We use 150 boosting rounds (or boosted trees). In conjunction with a lower learning
    rate, having many boosting rounds should improve accuracy.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用150次提升轮次（或提升树）。结合较低的学习率，拥有许多提升轮次应该可以提高准确度。
- en: 'After training, we can use `lgb.predict` to get predictions for our test set
    and calculate the F1 score:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以使用`lgb.predict`来获取测试集的预测并计算F1分数：
- en: '[PRE6]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The LightGBM predict function outputs an array of activations, one for each
    class. Therefore, we use `np.argmax` to choose the class with the highest activation
    as the predicted class. LightGBM also has support for some plotting functions.
    For instance, we can use `plot_metric` to plot our AU C μ results as captured
    in the `metrics`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM的预测函数输出一个激活数组，每个类别一个。因此，我们使用`np.argmax`来选择具有最高激活的类别作为预测类别。LightGBM也支持一些绘图函数。例如，我们可以使用`plot_metric`来绘制我们在`metrics`中捕获的AU
    C μ结果：
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The results of this are shown in *Figure 3**.3*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果在*图3**.3*中显示。
- en: '![Figure 3.3 – A plot of the ​AU ​C​ <?AID d835?><?AID df41?>​​​ metric per
    training iteration created using lgb.plot_metric](img/B16690_03_3.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3 – 使用lgb.plot_metric创建的每个训练迭代的AU C <?AID d835?><?AID df41?> metric的绘图](img/B16690_03_3.jpg)'
- en: Figure 3.3 – A plot of the AU C 𝝁 metric per training iteration created using
    lgb.plot_metric
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – 使用lgb.plot_metric创建的每个训练迭代的AU C 𝝁指标的绘图
- en: 'Running the preceding code should produce a LightGBM GBDT tree with an F1 score
    of around 0.917, in line with the score the Random Forest and Extra-Trees algorithms
    achieved in [*Chapter 2*](B16690_02.xhtml#_idTextAnchor036)*, Ensemble Learning
    – Bagging and Boosting*. However, LightGBM is significantly faster in reaching
    these accuracies. LightGBM completed the training in just 37 seconds on our hardware:
    this is 4.5 times faster than running Extra-Trees on the same problem and hardware
    and 60-70 times faster than scikit-learn’s `GradientBoostingClassifier` in our
    testing.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码应该会产生一个LightGBM GBDT树，其F1分数大约为0.917，与随机森林和Extra-Trees算法在[*第2章*](B16690_02.xhtml#_idTextAnchor036)*，集成学习
    – Bagging和Boosting*中实现的分数一致。然而，LightGBM在达到这些准确度方面要快得多。在我们的硬件上，LightGBM仅用了37秒就完成了训练：这比在相同问题和硬件上运行Extra-Trees快4.5倍，比我们在测试中使用的scikit-learn的`GradientBoostingClassifier`快60-70倍。
- en: LightGBM scikit-learn API
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LightGBM scikit-learn API
- en: 'We now take a look at the scikit-learn Python API for LightGBM. The scikit-learn
    API provides four classes: `LGBMModel`, `LGBMClassifier`, `LGBMRegressor`, and
    `LGBMRanker`. Each of these provides the same functionality as the LightGBM Python
    API, but with the same convenient scikit-learn interfaces we have worked with
    before. Additionally, the scikit-learn classes are compatible and interoperable
    with the rest of the scikit-learn ecosystem.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看scikit-learn Python API的LightGBM。scikit-learn API提供了四个类：`LGBMModel`、`LGBMClassifier`、`LGBMRegressor`和`LGBMRanker`。这些类都提供了与LightGBM
    Python API相同的功能，但具有我们之前使用过的相同方便的scikit-learn接口。此外，scikit-learn类与scikit-learn生态系统的其余部分兼容和可互操作。
- en: Let’s replicate the previous example using the scikit-learn API.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用scikit-learn API重复之前的示例。
- en: 'The dataset is loaded precisely as before. The scikit-learn API doesn’t require
    wrapping the data in a `Dataset` object. We also don’t have to zero-index our
    target classes, as scikit-learn supports any label for the classes:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的加载方式与之前完全相同。scikit-learn API不需要将数据包装在`Dataset`对象中。我们也不需要为目标类进行零索引，因为scikit-learn支持任何标签的类：
- en: '[PRE8]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The scikit-learn API also supports LightGBM callbacks; as such, we use the
    same callbacks as before:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn API还支持LightGBM回调；因此，我们使用与之前相同的回调：
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then create the `LGBMClassifier` exactly as we would any other scikit-learn
    model. When creating the classifier, we also set the parameters:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建`LGBMClassifier`的方式与创建任何其他scikit-learn模型的方式完全相同。在创建分类器时，我们还设置了参数：
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Note that we do not have to specify the number of classes; scikit-learn infers
    this automatically. We then call `fit` on the model, passing the training and
    test data along with our callbacks:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不需要指定类的数量；scikit-learn会自动推断。然后我们在模型上调用`fit`，传递训练数据和测试数据以及我们的回调：
- en: '[PRE11]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we evaluate our model with the F1 score. We don’t have to use `np.argmax`
    on the predictions as this is done automatically with the scikit-learn API:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用F1分数评估我们的模型。我们不需要在预测上使用`np.argmax`，因为这是由scikit-learn API自动完成的：
- en: '[PRE12]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Overall, we can see that using LightGBM via the scikit-learn API is more straightforward
    than the standard Python API. The scikit-learn API was also approximately 40%
    faster than the LightGBM API on our hardware. This section examined the ins and
    outs of using the various Python APIs available for LightGBM. The following section
    looks at training LightGBM models using the scikit-learn API.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来看，我们可以看到通过scikit-learn API使用LightGBM比使用标准的Python API更简单。在我们的硬件上，scikit-learn
    API也比LightGBM API快约40%。本节探讨了使用LightGBM的各种Python API的优缺点。下一节将探讨使用scikit-learn API训练LightGBM模型。
- en: Building LightGBM models
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建LightGBM模型
- en: This section provides an end-to-end example of solving a real-world problem
    using LightGBM. We provide a more detailed look at data preparation for a problem
    and explain how to find suitable parameters for our algorithms. We use multiple
    variants of LightGBM to explore relative performance and compare them against
    random forests.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了一个使用LightGBM解决实际问题的端到端示例。我们更详细地查看问题的数据准备，并解释如何为我们的算法找到合适的参数。我们使用多个LightGBM变体来探索相对性能，并将它们与随机森林进行比较。
- en: Cross-validation
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证
- en: Before we delve into solving a problem, we need to discuss a better way of validating
    algorithm performance. Splitting the data into two or three subsets is standard
    practice when training a model. The training data is used to train the model,
    the validation data is a hold-out set used to validate the data during training,
    and the test data is used to validate the performance after training.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨解决问题之前，我们需要讨论一种更好的验证算法性能的方法。在训练模型时，将数据分成两个或三个子集是标准做法。训练数据用于训练模型，验证数据是用于在训练期间验证数据的保留集，测试数据用于在训练后验证性能。
- en: In previous examples, we have done this split only once, building a single training
    and test to train and validate the model. The issue with this approach is that
    our model could get “lucky.” If, by chance, our test set closely matches the training
    data but is not representative of real-world data, we would report a good test
    error, even though we can’t be confident of our model’s performance.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的例子中，我们只进行了一次这种分割，构建了一个单独的训练和测试集来训练和验证模型。这种方法的问题是我们模型可能会“幸运”。如果我们测试集偶然与训练数据非常接近，但并不代表现实世界数据，我们可能会报告一个很好的测试误差，尽管我们无法对我们的模型性能有信心。
- en: An alternative is to do the dataset splitting multiple times and train the model
    multiple times, once for each split. This approach is called **cross-validation**.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是多次进行数据集分割并多次训练模型，每次分割训练一次。这种方法称为**交叉验证**。
- en: 'The most common application of cross-validation is *k-fold cross-validation*.
    With k-fold cross-validation, we choose a value, *k*, and partition the (shuffled)
    dataset into *k* subsamples (or folds). We then repeat the training process *k*
    times, using a different subset as the validation data and all other subsets as
    training data. The model’s performance is calculated as the mean (or median) score
    across all folds. The following diagram illustrates this process:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证最常见的应用是*k*折交叉验证。在k折交叉验证中，我们选择一个值*k*，并将（随机打乱的）数据集分成*k*个子样本（或折）。然后我们重复训练过程*k*次，使用不同的子集作为验证数据，所有其他子集作为训练数据。模型性能的计算是所有折的平均（或中位数）分数。以下图表说明了这个过程：
- en: '![Figure 3.4 – k-fold cross-validation with k = 3; the original dataset is
    shuffled and split into 3 equal parts (or folds); training and validation are
    repeated for each combination of subsampled data, and the average performance
    is reported](img/B16690_03_4.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图3.4 – k折交叉验证，k = 3；原始数据集被随机打乱并分成3个相等的部分（或折）；对每个子样本数据的组合重复训练和验证，并报告平均性能](img/B16690_03_4.jpg)'
- en: Figure 3.4 – k-fold cross-validation with k = 3; the original dataset is shuffled
    and split into 3 equal parts (or folds); training and validation are repeated
    for each combination of subsampled data, and the average performance is reported
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – k折交叉验证，k = 3；原始数据集被随机打乱并分成3个相等的部分（或折）；对每个子样本数据的组合重复训练和验证，并报告平均性能
- en: Using a high value for *k* reduces the chance that the model coincidentally
    shows good performance and indicates how the model might perform in the real world.
    However, the entire training process is repeated for each fold, which could be
    computationally expensive and time-consuming. Therefore, we need to balance the
    resources available with the need to validate the model. A typical value for *k*
    is 5 (the default for scikit-learn), also called 5-fold cross-validation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用高值的*k*可以降低模型偶然表现出良好性能的可能性，并表明模型在现实世界中的可能表现。然而，整个训练过程需要为每个折重复，这可能会非常耗费计算资源和时间。因此，我们需要平衡可用的资源与验证模型的需求。*k*的典型值是5（scikit-learn的默认值），也称为5折交叉验证。
- en: Stratified k-fold validation
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分层k折验证
- en: A problem that might arise with k-fold cross-validation is that, due to chance,
    a fold may contain samples from only a single class. **Stratified sampling** solves
    this issue by preserving the percentage of samples for each class when creating
    folds. In this way, each fold has the same distribution of classes as the original
    dataset. When applied to cross-validation, this technique is called stratified
    k-fold cross-validation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在k折交叉验证中可能出现的问题之一是，由于偶然性，一个折可能只包含来自单个类的样本。**分层抽样**通过在创建折时保留每个类的样本百分比来解决此问题。这样，每个折都具有与原始数据集相同的类别分布。当应用于交叉验证时，这种技术称为分层k折交叉验证。
- en: Parameter optimization
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数优化
- en: '**Parameter optimization**, also called parameter tuning, is the process of
    finding good hyperparameters for the model and training process specific to the
    problem being solved. In the previous examples of training models, we have been
    setting the model and training algorithm’s parameters based on intuition and minimal
    experimentation. There is no guarantee that the parameter choices were optimal
    for the optimization problem.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数优化**，也称为参数调整，是寻找针对特定问题的模型和训练过程的好超参数的过程。在之前的训练模型示例中，我们一直是基于直觉和最小实验来设置模型和训练算法的参数。没有保证参数选择对优化问题是最佳的。'
- en: But how might we go about finding the best parameter choices? A naïve strategy
    is to try an extensive range of values for a parameter, find the best value, and
    then repeat the process for the following parameter. However, it is frequently
    the case that parameters are **co-dependent**. When we change one parameter, the
    optimal value for another might differ. An excellent example of co-dependence
    in GBDTs is the number of boosting rounds and the learning rate. Having a small
    learning rate necessitates more boosting rounds. Therefore, optimizing the learning
    rate and then, independently, the number of boosting rounds is unlikely to produce
    optimal results. *Both parameters must be optimized* *in unison*.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何找到最佳参数选择呢？一种天真策略是尝试一个参数的广泛范围值，找到最佳值，然后对下一个参数重复此过程。然而，参数通常是**相互依赖**的。当我们改变一个参数时，另一个参数的最佳值可能会不同。GBDTs
    中相互依赖的一个优秀例子是提升轮数和学习率。拥有较小的学习率需要更多的提升轮数。因此，独立优化学习率和提升轮数不太可能产生最佳结果。*这两个参数必须同时优化*。
- en: Grid search
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网格搜索
- en: An approach that accounts for parameter co-dependence is grid search. With grid
    search, a parameter grid is set up. The grid consists of a range of values to
    try for each parameter we are optimizing. An exhaustive search is then performed,
    training and validating the model on each possible combination of parameters.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑参数相互依赖的方法是网格搜索。在网格搜索中，设置一个参数网格。网格由我们正在优化的每个参数要尝试的值范围组成。然后执行穷举搜索，在每个参数组合上训练和验证模型。
- en: 'Here is an example of a parameter grid for three parameters:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个三个参数的参数网格示例：
- en: '[PRE13]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Each parameter is specified with a range of possible values. The previous grid
    would require 150 trails to search.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 每个参数都指定了一个可能的值范围。之前的网格需要150次尝试来搜索。
- en: Since grid search is exhaustive, it has the advantage that it is guaranteed
    to find the best combination of parameters within the ranges specified. However,
    the downside to grid search is the cost. Trying each possible combination of parameters
    is very expensive and quickly becomes intractable for many parameters and large
    parameter ranges.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 由于网格搜索是穷举的，它有一个优点，即保证在指定的范围内找到最佳参数组合。然而，网格搜索的缺点是成本。尝试每个可能的参数组合非常昂贵，并且对于许多参数和大的参数范围很快就会变得难以处理。
- en: Scikit-learn provides a utility class to implement grid search and perform cross-validation
    at the same time. `GridSearchCV` takes a model, a parameter grid, and the number
    of cross-validation folds as parameters. `GridSearchCV` then proceeds to search
    the grid for the best parameters, using cross-validation to validate the performance
    for each combination of parameters. We’ll illustrate the use of `GridSearchCV`
    in the next section.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 提供了一个实用类来实现网格搜索并同时执行交叉验证。`GridSearchCV` 接收一个模型、一个参数网格和交叉验证折数作为参数。`GridSearchCV`
    然后开始搜索网格以找到最佳参数，使用交叉验证来验证每个参数组合的性能。我们将在下一节中展示 `GridSearchCV` 的使用方法。
- en: Parameter optimization is a crucial part of the modeling process. Finding suitable
    parameters for a model could be the difference between a successful or failed
    process. However, as discussed previously, parameter optimization is also often
    enormously expensive regarding time and computational complexity, necessitating
    a trade-off between cost and performance.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 参数优化是建模过程中的关键部分。为模型找到合适的参数可能是成功或失败过程的关键。然而，正如之前所讨论的，参数优化在时间和计算复杂度上通常也非常昂贵，这需要在成本和性能之间进行权衡。
- en: Predicting student academic success
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测学生学术成功
- en: We now move on to our example. We build a model to predict students’ dropout
    rate based on a range of social and economic factors using LightGBM [7] ([https://archive-beta.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success](https://archive-beta.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success)).
    The data is available in CSV format. We start by exploring the data.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在继续到我们的例子。我们构建了一个模型，基于一系列社会和经济因素使用LightGBM [7] ([https://archive-beta.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success](https://archive-beta.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success))
    来预测学生的辍学率。数据以CSV格式提供。我们首先从探索数据开始。
- en: Exploratory data analysis
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: 'One of the most fundamental properties of any dataset is the shape: the rows
    and columns our data consists of. It’s also an excellent way to validate that
    the data read succeeded. Here, our data consists of 4,424 rows and 35 columns.
    Taking a random sample of the data gives us a sense of the columns and their values:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 任何数据集的最基本属性之一是形状：我们的数据由行和列组成。这也是验证数据读取是否成功的一个很好的方法。在这里，我们的数据由4,424行和35列组成。随机抽取数据样本让我们对列及其值有了概念：
- en: '[PRE14]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we can run `df.info()` to see all the columns, their non-null counts,
    and their data types:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以运行`df.info()`来查看所有列、它们的非空计数和它们的数据类型：
- en: '[PRE15]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Running the preceding code shows us that most columns are integer types, except
    for the `Target` column, with a few floats in between. The `Target` column is
    listed as type `object`; if we look at the values in the sample, we can see the
    `Target` column consists of `Graduate`, `Dropout`, and `Enrolled` strings. LightGBM
    can’t work with strings as targets, so we’ll map these to integer values before
    training our models.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码显示，大多数列是整数类型，除了`Target`列，中间有一些浮点数。`Target`列被列为`object`类型；如果我们查看样本中的值，我们可以看到`Target`列由`Graduate`、`Dropout`和`Enrolled`字符串组成。LightGBM不能处理字符串作为目标，所以我们将在训练模型之前将这些映射到整数值。
- en: We can also run `df.describe()` to get a statistical description (mean, standard
    deviation, min, max, and percentiles) of the values in each column. Calculating
    descriptive statistics helps check the bounds of the data (not a big problem with
    working with decision tree models) and check for outliers. For this dataset, there
    aren’t any data bounds or outlier concerns.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以运行`df.describe()`来获取每个列的统计描述（平均值、标准差、最小值、最大值和分位数）。计算描述性统计有助于检查数据的界限（与决策树模型一起工作时不是大问题）并检查异常值。对于这个数据集，没有数据界限或异常值问题。
- en: 'Next, we need to check for duplicated and missing values. We need to drop the
    rows containing missing values or impute appropriate substitutes if there are
    any missing values. We can check for missing values using the following code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要检查重复和缺失值。我们需要删除包含缺失值的行，或者如果有任何缺失值，需要用适当的替代值进行插补。我们可以使用以下代码检查缺失值：
- en: '[PRE16]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Running the preceding code shows us there are no missing values for this dataset.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码显示，这个数据集没有缺失值。
- en: 'To locate duplicates, we can run the following code:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定位重复项，我们可以运行以下代码：
- en: '[PRE17]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: There are also no duplicates in the dataset. If there were any duplicated data,
    we would drop the extra rows.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中也没有重复项。如果有任何重复数据，我们会删除额外的行。
- en: 'We also need to check the distribution of the target class to ensure it is
    balanced. Here, we show a histogram that indicates the target class distribution.
    We create the histogram using Seaborn’s `countplot()` method, like so:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要检查目标类的分布，以确保它是平衡的。在这里，我们展示了一个直方图，表明目标类的分布。我们使用Seaborn的`countplot()`方法创建直方图，如下所示：
- en: '[PRE18]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Figure 3.5 – Distribution of target class in the academic success dataset](img/B16690_03_5.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图3.5 – 学术成功数据集中目标类的分布](img/B16690_03_5.jpg)'
- en: Figure 3.5 – Distribution of target class in the academic success dataset
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 – 学术成功数据集中目标类的分布
- en: Although not perfectly balanced, the target distribution is not overly skewed
    to any one class, and we don’t have to perform any compensating action.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管目标分布并不完全平衡，但它也没有过度偏向任何一类，我们不需要执行任何补偿操作。
- en: 'So far, we have found that our dataset is suitable for modeling (we still need
    to remap `Target`) and clean (it does not contain missing or duplicated values
    and is well balanced). We can now take a deeper look at some features, starting
    with feature correlation. The following code plots a correlation heatmap. Pairwise
    Pearson correlations are calculated using `df.corr()`. The screenshot that follows
    the snippet shows a correlation heatmap built using pairwise Pearson correlations:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们发现我们的数据集适合建模（我们仍然需要重新映射 `Target`）和清洗（它不包含缺失或重复的值，并且平衡良好）。现在我们可以更深入地查看一些特征，从特征相关性开始。以下代码绘制了一个相关性热图。成对皮尔逊相关性是通过
    `df.corr()` 计算的。随后的截图显示了使用成对皮尔逊相关性构建的相关性热图：
- en: '[PRE19]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Figure 3.6 – Pairwise Pearson feature correlation of the academic success
    dataset](img/B16690_03_6.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – 学术成功数据集的成对皮尔逊特征相关性](img/B16690_03_6.jpg)'
- en: Figure 3.6 – Pairwise Pearson feature correlation of the academic success dataset
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 学术成功数据集的成对皮尔逊特征相关性
- en: 'We can see three patterns of correlations: first-semester credits, enrollments,
    evaluations, and approvals are all correlated. First-semester and second-semester
    values for these are also correlated. These correlations imply that students tend
    to see through the year once enrolled instead of dropping out mid-semester. Although
    correlated, the correlations aren’t strong enough to consider dropping any features.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到三种相关性的模式：第一学期学分、入学、评估和批准都是相关的。这些特征的第一学期和第二学期的值也是相关的。这些相关性表明，一旦学生入学，他们倾向于看到整个学年而不是在学期中辍学。尽管存在相关性，但这些相关性并不足够强，以至于考虑删除任何特征。
- en: The third correlation pattern is between `Nacionality` and `International`,
    which are strongly correlated.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种相关性模式是 `Nacionality` 和 `International` 之间的，它们高度相关。
- en: Note
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The word *Nacionality* refers to *nationality*. We have retained the spelling
    from the original dataset here too for the purpose of consistency.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 单词 *Nacionality* 指的是 *nationality*。我们在这里也保留了原始数据集中的拼写，以保持一致性。
- en: 'A closer look at `Nacionality` shows that almost all rows have a single value:
    the country where the dataset was collected. The strong correlation implies the
    same for `International`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察 `Nacionality` 特征会发现，几乎所有行只有一个值：数据集收集的国家。强烈的相关性意味着 `International` 特征也是这样：
- en: '[PRE20]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following screenshot shows a stacked bar plot of the nationalities:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了国籍的堆叠条形图：
- en: '![Figure 3.7 – Distribution of the ‘Nacionality’ feature, showing almost all
    rows have a single value in the academic success dataset](img/B16690_03_7.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7 – 学术成功数据集中 ‘Nacionality’ 特征的分布，显示几乎所有行只有一个值](img/B16690_03_7.jpg)'
- en: Figure 3.7 – Distribution of the ‘Nacionality’ feature, showing almost all rows
    have a single value in the academic success dataset
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 学术成功数据集中 ‘Nacionality’ 特征的分布，显示几乎所有行只有一个值
- en: The distribution of `'Nacionality'` and `'International'` means that they are
    not very informative (nearly all rows have the same value), so we can drop them
    from the dataset.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`''Nacionality''` 和 `''International''` 的分布意味着它们不是很有信息量（几乎所有行都有相同的值），因此我们可以从数据集中删除它们。'
- en: 'Finally, we notice the `''Gender''` feature. When working with gender, it’s
    always good to check for bias. We can visualize the distribution of the `''Gender''`
    feature relative to the target classes using a histogram. The results are shown
    in the screenshot that follows this code snippet:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们注意到 `'Gender'` 特征。当处理性别信息时，总是好的做法检查是否存在偏见。我们可以使用直方图来可视化 `'Gender'` 特征相对于目标类别的分布。结果展示在随后的代码片段之后的截图里：
- en: '[PRE21]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Figure 3.8 – Distribution of the ‘Gender’ feature in the academic success
    dataset](img/B16690_03_8.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.8 – 学术成功数据集中 ‘Gender’ 特征的分布](img/B16690_03_8.jpg)'
- en: Figure 3.8 – Distribution of the ‘Gender’ feature in the academic success dataset
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – 学术成功数据集中 ‘Gender’ 特征的分布
- en: There is a slight bias toward female students, but not enough to warrant concern.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对女性学生的轻微偏见，但不足以引起关注。
- en: Modeling
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 建模
- en: 'We can now prepare our dataset for modeling. We must map our `Target` values
    to integers and drop the `Nacionality` and `International` features. We also need
    to remove the spaces in the feature names. LightGBM cannot work with spaces in
    the names; we can replace them with underscores:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以为建模准备我们的数据集。我们必须将我们的 `Target` 值映射到整数，并删除 `Nacionality` 和 `International`
    特征。我们还需要删除特征名称中的空格。LightGBM 不能处理名称中的空格；我们可以用下划线替换它们：
- en: '[PRE22]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We train and compare four models: a LightGBM GBDT, a LightGBM DART tree, a
    LightGBM DART tree with GOSS, and a scikit-learn random forest.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练并比较了四个模型：一个LightGBM GBDT，一个LightGBM DART树，一个带有GOSS的LightGBM DART树，以及一个scikit-learn随机森林。
- en: We’ll perform parameter optimization with 5-fold cross-validation using `GridSearchCV`
    to ensure good performance for the models.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用5折交叉验证和`GridSearchCV`进行参数优化，以确保模型有良好的性能。
- en: 'The following code sets up the parameter optimization for the GBDT. A similar
    pattern is followed for the other models, which can be seen in the source code:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码设置了GBDT的参数优化。其他模型遵循类似的模式，可以在源代码中看到：
- en: '[PRE23]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Running the preceding code takes some time, but once completed, it prints the
    best parameters found along with the score of the best model.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码需要一些时间，但一旦完成，它将打印出找到的最佳参数以及最佳模型的分数。
- en: 'After all the models are trained, we can evaluate each using F1-scoring, taking
    the mean of 5-fold cross-validation, using the best parameters found. The following
    code illustrates how to do this for the GBDT model:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有模型训练完成后，我们可以使用F1评分对每个模型进行评估，取5折交叉验证的平均值，使用找到的最佳参数。以下代码演示了如何对GBDT模型进行此操作：
- en: '[PRE24]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Jupyter notebooks for [the parameter optimization for each model are available
    in the GitHub repository: https://github.com/Pack](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3)tPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[每个模型的参数优化Jupyter笔记本可在GitHub仓库中找到：https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3]'
- en: 'The following table summarizes the best parameter values found and the cross-validated
    F1 scores for each model:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了找到的最佳参数值和每个模型的交叉验证F1分数：
- en: '| **Model** | **Learning** **Rate** | **Max** **Depth** | **Min Child** **Samples**
    | **N** **Estimators** | **Num** **Leaves** | **Min** **Samples Leaf** | **Min**
    **Samples Split** | **F1 score** |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **学习率** | **最大深度** | **最小子样本数** | **估计器数** | **叶子数** | **最小叶子样本数**
    | **最小分割样本数** | **F1分数** |'
- en: '| GBDT | 0.1 | - | 10 | 100 | 32 | N/A | N/A | 0.716 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| GBDT | 0.1 | - | 10 | 100 | 32 | N/A | N/A | 0.716 |'
- en: '| DART | 0.1 | 128 | 30 | 150 | 128 | N/A | N/A | 0.703 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| DART | 0.1 | 128 | 30 | 150 | 128 | N/A | N/A | 0.703 |'
- en: '| DART (GOSS) | 0.1 | 128 | 30 | 150 | 128 | N/A | N/A | 0.703 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| DART (GOSS) | 0.1 | 128 | 30 | 150 | 128 | N/A | N/A | 0.703 |'
- en: '| Random Forest | N/A | N/A | N/A | 150 | N/A | 10 | 20 | 0.665 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | N/A | N/A | N/A | 150 | N/A | 10 | 20 | 0.665 |'
- en: Table 3.1 – Summary of best parameters found for each model with the corresponding
    F1 scores
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1 – 每个模型找到的最佳参数及其对应的F1分数
- en: As we can see from the table, the LightGBM models performed much better than
    the scikit-learn random forest. Both DART models achieved nearly the same F1 score,
    with GOSS having a slightly lower F1 score (the table values are rounded to 3
    digits).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 从表中我们可以看出，LightGBM模型的表现远优于scikit-learn随机森林。两个DART模型实现了几乎相同的F1分数，GOSS的F1分数略低（表中的值已四舍五入到三位数字）。
- en: This concludes our end-to-end example of exploring a dataset and building an
    optimized model for the dataset (using parameter grid search). We look at more
    complicated datasets in the coming chapters and delve deeper into analyzing model
    performance.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们探索数据集并为其构建优化模型（使用参数网格搜索）的端到端示例。在接下来的章节中，我们将查看更复杂的数据集，并深入分析模型性能。
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter introduced LightGBM as a library to train boosted machines efficiently.
    We looked at where the complexity of building GBDTs comes from and the features
    in LightGBM that address them, such as histogram-based sampling, feature bundling,
    and GOSS. We also reviewed LightGBM’s most important hyperparameters.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了LightGBM作为训练提升机的库。我们探讨了构建GBDT的复杂性的来源，以及LightGBM中解决这些问题的特性，例如基于直方图的采样、特征捆绑和GOSS。我们还回顾了LightGBM最重要的超参数。
- en: We also gave a detailed overview of using LightGBM in Python, covering both
    the LightGBM Python API and the scikit-learn API. We then built our first tuned
    models using LightGBM to predict student academic performance, utilizing cross-validation
    and grid-search-based parameter optimization.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还详细概述了在Python中使用LightGBM的方法，包括LightGBM Python API和scikit-learn API。然后我们使用LightGBM构建了第一个调整后的模型来预测学生的学术表现，利用交叉验证和基于网格搜索的参数优化。
- en: In the next chapter, we compare LightGBM against another popular gradient-boosting
    library, XGBoost, and DL techniques for tabular data.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们比较 LightGBM 与另一个流行的梯度提升库 XGBoost 以及表格数据的 DL 技术。
- en: References
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '| *[**1]* | *G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye and
    T.-Y. Liu, “LightGBM: A Highly Efficient Gradient Boosting Decision Tree,” in
    Advances in Neural Information Processing* *Systems, 2017.* |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| *[**1]* | *G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye 和 T.-Y.
    Liu, “LightGBM: A Highly Efficient Gradient Boosting Decision Tree,” in Advances
    in Neural Information Processing* *Systems, 2017.* |'
- en: '| *[**2]* | *M. Mehta, R. Agrawal and J. Rissanen, “SLIQ: A fast scalable classifier
    for data mining,” in Advances in Database Technology—EDBT’96: 5th International
    Conference on Extending Database Technology Avignon, France, March 25-29, 1996
    Proceedings* *5, 1996.* |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| *[**2]* | *M. Mehta, R. Agrawal 和 J. Rissanen, “SLIQ: A fast scalable classifier
    for data mining,” in Advances in Database Technology—EDBT’96: 5th International
    Conference on Extending Database Technology Avignon, France, March 25-29, 1996
    Proceedings* *5, 1996.* |'
- en: '| *[**3]* | *J. Shafer, R. Agrawal, M. Mehta and others, “SPRINT: A scalable
    parallel classifier for data mining,” in* *Vldb, 1996.* |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| *[**3]* | *J. Shafer, R. Agrawal, M. Mehta 和其他人， “SPRINT: A scalable parallel
    classifier for data mining,” in* *Vldb, 1996.* |'
- en: '| *[**4]* | *S. Ranka and V. Singh, “CLOUDS: A decision tree classifier for
    large datasets,” in Proceedings of the 4th Knowledge Discovery and Data Mining*
    *Conference, 1998.* |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| *[**4]* | *S. Ranka 和 V. Singh, “CLOUDS: A decision tree classifier for large
    datasets,” in Proceedings of the 4th Knowledge Discovery and Data Mining* *Conference,
    1998.* |'
- en: '| *[**5]* | *H. Shi, “Best-first decision tree* *learning,” 2007.* |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| *[**5]* | *H. Shi, “Best-first decision tree* *learning,” 2007.* |'
- en: '| *[**6]* | *R. Kleiman and D. Page, “Aucμ: A performance metric for multi-class
    machine learning models,” in International Conference on Machine* *Learning, 2019.*
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| *[**6]* | *R. Kleiman 和 D. Page, “Aucμ: A performance metric for multi-class
    machine learning models,” in International Conference on Machine* *Learning, 2019.*
    |'
- en: '| *[**7]* | *V. Realinho, J. Machado, L. Baptista and M. V. Martins, Predicting
    student dropout and academic success,* *Zenodo, 2021.* |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| *[**7]* | *V. Realinho, J. Machado, L. Baptista 和 M. V. Martins, Predicting
    student dropout and academic success,* *Zenodo, 2021.* |'
