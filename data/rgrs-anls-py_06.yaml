- en: Chapter 6. Achieving Generalization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 实现泛化
- en: We have to confess that, until this point, we've delayed the crucial moment
    of truth when our linear model has to be put to the test and verified as effectively
    predicting its target. Up to now, we have just considered whether we were doing
    a good modeling job by naively looking at a series of good-fit measures, all just
    telling us if the linear model could be apt at predicting based solely on the
    information in our training data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须承认，到目前为止，我们推迟了线性模型必须接受测试和验证以有效预测其目标的关键真实时刻。到目前为止，我们只是通过天真地查看一系列拟合度指标来判断我们是否做得好，所有这些指标都在告诉我们线性模型能否仅基于我们的训练数据中的信息进行适当的预测。
- en: Unless you love sink-or-swim situations, in much the same procedure you'd employ
    with new software before going into production, you need to apply the correct
    tests to your model and to be able to anticipate its live performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你喜欢“沉或浮”的情况，在进入生产前，你将使用与新的软件相同的程序，你需要对你的模型应用正确的测试，并能够预测其现场表现。
- en: Moreover, no matter your level of skill and experience with such types of models,
    you can easily be misled into thinking you're building a good model just on the
    basis of the same data you used to define it. We will therefore introduce you
    to the fundamental distinction between in-sample and out-of-sample statistics
    and demonstrate how they risk diverging when you use too many predictors, too
    few predictors, or simply just the wrong ones.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，无论你对这类模型的技能和经验水平如何，你很容易被误导，认为只要基于你用来定义它的相同数据，你就是在构建一个好的模型。因此，我们将向您介绍样本内和样本外统计之间的基本区别，并展示当使用过多的预测变量、过少的预测变量或仅仅是错误的变量时，它们如何存在风险而偏离。
- en: Here we are then, ready at last to check whether we have done a good job or
    have to rethink everything from scratch. In this pivotal chapter of the book,
    before proceeding to more complex techniques, we will introduce you to key data
    science recipes to thoroughly test your model, fine-tune it optimally, make it
    economical, and pit it against real, fresh data without any concerns.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们现在终于准备好检查我们是否做得很好，或者是否需要从头开始重新思考一切了。在本书的这个关键章节中，在继续介绍更复杂的技术之前，我们将向您介绍关键的数据科学配方，以彻底测试您的模型，优化其性能，使其经济高效，并在没有任何顾虑的情况下，将其与真实、新鲜的数据进行对比。
- en: 'In this chapter, you''ll get to know how to:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解如何：
- en: Test your models, using the most appropriate cost measure, on a validation/test
    set or using cross-validation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最合适的成本度量，在验证/测试集或使用交叉验证上测试你的模型
- en: Select the best features on the basis of statistical tests and experiments
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据统计测试和实验选择最佳特征
- en: Make your model more economical by tweaking the cost function
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调整成本函数使你的模型更加经济
- en: Use stability selection, an almost automated method for variable selection
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用稳定性选择，一种几乎自动化的变量选择方法
- en: Checking on out-of-sample data
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查样本外数据
- en: Until this point in the book, we have striven to make the regression model fit
    data, even by modifying the data itself (inputting missing data, removing outliers,
    transforming for non-linearity, or creating new features). By keeping an eye on
    measures such as R-squared, we have tried our best to reduce prediction errors,
    though we have no idea to what extent this was successful.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在努力使回归模型拟合数据，甚至通过修改数据本身（输入缺失数据、去除异常值、进行非线性转换或创建新特征）。通过关注如R平方等指标，我们尽力减少预测误差，尽管我们不知道这有多成功。
- en: The problem we face now is that we shouldn't expect a well fit model to automatically
    perform well on any new data during production.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在面临的问题是，我们不应该期望一个拟合良好的模型在生产过程中自动在所有新的数据上表现良好。
- en: While defining and explaining the problem, we recall what we said about underfitting.
    Since we are working with a linear model, we are actually expecting to apply our
    work to data that has a linear relationship with the response variable. Having
    a linear relationship means that, with respect to the level of the response variable,
    our predictors always tend to constantly increase (or decrease) at the same rate.
    Graphically, on a scatterplot, this is refigured by a straight and very elongated
    cloud of points that could be crossed by a straight regression line with little
    or minimal prediction error.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义和解释问题时，我们回忆起我们关于欠拟合所说的内容。由于我们正在使用线性模型，我们实际上期望将我们的工作应用于与响应变量具有线性关系的数据。具有线性关系意味着，在响应变量的水平上，我们的预测变量总是以相同的速率不断增加（或减少）。在散点图上，这可以通过一条直线和非常细长的点云来表示，这些点云可以被一条直线回归线穿过，预测误差很小或最小。
- en: When the relationship is instead non-linear, the rate of change and direction
    are mutable (alternatively increasing or decreasing). In such a situation, in
    order to have the linear model work better, we will have to try to make the relationship
    straight by opportune transformations. Otherwise we will have to try guessing
    the response by a not-always-successful approximation of a non-linear shape to
    a linear one.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当关系是非线性的，变化率和方向是可变的（或者说是增加或减少）时，为了使线性模型更好地工作，我们将不得不尝试通过适当的变换使关系变得直线。否则，我们将不得不尝试通过非线性形状到线性形状的不总是成功的近似来猜测响应。
- en: If for instance the relationship is quadratic (so the functional shape is that
    of a parabola), using a line will pose the problem of a systematic underestimation
    or overestimation of the predicted values at certain ranges in the predictor's
    values. This systematic error is called bias and it is typical of simple models
    such as linear regression. A prediction model with a high bias will systematically
    tend to generate erroneous predictions in certain situations. Since inaccuracy
    of predictions is an undesirable characteristic for a tool that should be able
    to provide effective forecasts, we have to strive to achieve a better fit to the
    response by adding new variables and transforming the present ones by polynomial
    expansion or other transformations. Such efforts constitute the so-called **feature
    creation phase**.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果关系是二次的（因此函数形状是抛物线），使用直线将导致在预测变量值的一定范围内对预测值的系统性低估或高估的问题。这种系统性错误称为偏差，它是简单模型（如线性回归）的典型特征。具有高偏差的预测模型将系统地倾向于在特定情况下产生错误的预测。由于预测的不准确性是一个不希望的特性，对于应该能够提供有效预测的工具来说，我们必须努力通过添加新变量和通过多项式扩展或其他变换来转换现有变量，以实现更好的响应拟合。这些努力构成了所谓的**特征创建阶段**。
- en: By doing so, we may find ourselves in a different but no less problematic situation.
    In fact, when we render our model more and more complex, it will not just better
    fit the response by catching more and more parts of the unknown function that
    ties it to the predictors, but also, by adding more and more terms, we are enabling
    our model to receive that part of the information that is exclusively specific
    to the data at hand (we call this noise), making it more and more unable to work
    properly with different data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们可能会发现自己处于一个不同但同样有问题的情况。事实上，当我们使我们的模型越来越复杂时，它不仅会更好地拟合响应，通过捕捉更多未知函数与预测变量相关的部分，而且通过添加更多和更多的项，我们使模型能够接收那些仅与当前数据相关的信息（我们称之为噪声），这使得模型越来越不能正确地处理不同的数据。
- en: You could think about it as a *power of memorization* so that, the more complex
    the learning algorithm, the more space there will be to fit not-so-useful information
    from the data we are using for learning. This memorization brings very inconvenient
    consequences. Though our model appears to have a good fit on our data, as soon
    as it is applied to a different set, it reveals its inability to predict correctly.
    In such a situation, contrary to before when the errors were systematic (systematic
    under- or over-estimation), errors will appear to be erratic, depending on the
    dataset. This is called variance of the estimates and it could prove more of a
    problem to you because it can leave you unaware of its existence until you test
    it against real data. It tends to strike in more complex algorithms and, in its
    simplest form, linear regression tends to present a higher bias on the estimates
    than variance. Anyway, adding too many terms and interactions or resorting to
    polynomial expansion does expose linear models to overfitting.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将其视为一种*记忆能力*，因此，学习算法越复杂，就有更多的空间来拟合我们从学习数据中使用的不是很有用的信息。这种记忆会带来非常不便的后果。尽管我们的模型似乎在我们的数据上拟合得很好，但一旦它应用于不同的数据集，它就会显示出其无法正确预测的能力。在这种情况下，与之前错误是系统性的（系统性低估或高估）相反，错误将显得是随机的，这取决于数据集。这被称为估计的方差，它可能对你来说是一个更大的问题，因为它可以在你测试它之前让你对其存在一无所知。它倾向于在更复杂的算法中发生，在其最简单的形式中，线性回归倾向于在估计上比方差呈现更高的偏差。无论如何，添加过多的项和交互作用或求助于多项式展开确实会使线性模型面临过拟合的风险。
- en: Testing by sample split
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过样本分割进行测试
- en: Since we expect the ability to generalize to new data from a model and since
    we are seldom interested in just fitting or simply memorizing the present data,
    we need to take some cautionary steps as we build our model. To fight against
    this problem, the practice of learning from data has defined over the years a
    series of procedures, based on the scientific method of validating and testing,
    that we are going to illustrate and practice ourselves.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们期望模型能够泛化到新的数据，而且我们很少仅仅对拟合或简单地记忆现有数据感兴趣，因此在构建模型时我们需要采取一些预防措施。为了对抗这个问题，多年来从数据中学习的实践已经定义了一系列基于科学验证和测试方法的程序，我们将展示并亲自实践这些方法。
- en: First, if we want our model to generalize well on new data, we have to test
    it in such a situation. This means that, if getting new data is not an easy task
    or a feasible one, we should reserve some data for our tests from the beginning.
    We can achieve that by randomly splitting our data into two parts, a training
    set and a test set, using 70–80 percent of the data for the training part and
    the residual 20–30 percent for testing purposes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果我们希望我们的模型在新数据上具有良好的泛化能力，我们必须在这种情况下对其进行测试。这意味着，如果获取新数据不是一项容易的任务或可行的任务，我们应该从一开始就为测试保留一些数据。我们可以通过随机将数据分为两部分，即训练集和测试集，使用70-80%的数据进行训练，剩余的20-30%用于测试目的。
- en: 'Scikit-learn''s `cross_validation` module offers a series of methods that can
    help us in dealing with all these operations. Let''s try it by operating on our
    usual Boston Housing dataset:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn的`cross_validation`模块提供了一系列方法，可以帮助我们处理所有这些操作。让我们通过操作我们常用的波士顿住房数据集来尝试一下：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After having loaded it, let''s first split it into train and test parts:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载完成后，我们首先将其分为训练和测试两部分：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`train_test_split` will separate the data according to the specified quota
    for testing indicated in the `test_size` parameter. The split will be a random
    one, and you can deterministically control the results (for replication purposes)
    using a specific numeric seed in the `random_state` parameter (our choice for
    the seed is `101`).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_test_split`将根据`test_size`参数中指定的测试比例来分割数据。分割将是随机的，你可以通过在`random_state`参数中使用特定的数字种子来确定性控制结果（用于复现目的）（我们选择的种子是`101`）。'
- en: 'Sometimes, reserving an out-of-sample (comprising what is not in-sample—that
    is, used as a sample for learning from training activity data) is not enough,
    because we may have to tune some parameters or make specific choices and we want
    to test the alternatives without having to use the test data. The solution is
    to reserve another part of our data for validation purposes, which implies checking
    what parameters could be optimal for our model. We can achieve that using `train_test_split`
    in two steps:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，仅仅保留一个样本外数据（包含不在样本内——即用作从训练活动数据中学习的样本）是不够的，因为我们可能需要调整一些参数或做出特定的选择，并且我们希望测试替代方案而不必使用测试数据。解决方案是保留我们数据的一部分用于验证目的，这意味着检查哪些参数可能对我们模型是最优的。我们可以通过两步使用`train_test_split`来实现这一点：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Cross-validation
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证
- en: 'Though helpful in measuring the true error of an hypothesis, dividing your
    data into train and test (and sometimes also into validation) sets presents some
    risks that you have to take into account:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在衡量假设的真实误差方面很有帮助，但将数据分为训练集和测试集（有时也分为验证集）会带来一些风险，你必须考虑到：
- en: Since it involves sub-sampling (you casually draw out a part of your initial
    sample), you may incur the risk of drawing sets that are too favorable or unfavorable
    for training and testing
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于它涉及子采样（你随意抽取初始样本的一部分），你可能会承担抽取对训练和测试过于有利或不利的集的风险。
- en: By leaving aside a portion of your sample, you reduce the number of examples
    to learn from, whereas linear models need as many as possible in order to reduce
    the variance of the estimates, disambiguate collinear variables, and properly
    model non-linearity
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过留出一部分样本，你减少了可以从中学习的示例数量，而线性模型需要尽可能多的示例来减少估计的方差，消除共线性变量，并正确地模拟非线性。
- en: Though we always suggest drawing a small test sample (say 10% of the data) as
    a final check of the validity of your work, the best way to avoid the aforementioned
    problems, and easily manage different comparisons of models and parameters, is
    to apply cross-validation, which requires you to split your data for both training
    and testing but it does so repetitively until every observation has played the
    role of training and testing.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们总是建议抽取一个小测试样本（比如数据的10%）作为你工作有效性的最终检查，但避免上述问题的最佳方法，以及轻松管理不同模型和参数的比较，是应用交叉验证，这要求你为训练和测试分割数据，但它会反复进行，直到每个观测值都扮演了训练和测试的角色。
- en: In other words, you decide how many mutually exclusive parts to split your data
    into, then you repeatedly keep on training your model using all the folds but
    a different one every time; this plays the role of a test set.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，你决定将数据分割成多少互斥的部分，然后你反复使用除了不同的一次之外的所有折来训练你的模型；这起到了测试集的作用。
- en: Note
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The number of parts you split your data into is usually set to 3, 5, 10, or
    20 and you decide on a large number of splits (each one called a **fold**) when
    you have little training data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你将数据分割成多少部分通常设置为3、5、10或20，当你有少量训练数据时，你决定一个较大的分割数（每个分割称为**折**）。
- en: When you have completed the validation, using every single split available as
    the test set, you first take the average of the results, which tells you with
    a good degree of accuracy the overall performance of your model when faced with
    new data (new but not too dissimilar from the one you have at hand). Then you
    also notice the standard deviation of the cross-validated performances. This is
    important because, if there's a high deviation (over half of the average performance
    value), it can indicate that the model has a high variance of the estimates and
    that it needs more data to work well.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当你完成验证后，使用所有可用的分割作为测试集，你首先计算结果的平均值，这以相当高的准确性告诉你，当面对新数据时（新数据但与手头的数据不太相似）你的模型的整体性能。然后你也注意到交叉验证性能的标准差。这很重要，因为如果存在高偏差（超过平均性能值的一半），这可能表明模型估计的方差很高，并且需要更多的数据才能良好工作。
- en: In the following example, you can look at how `KFold` and `StratifiedKFold`
    (from the Scikit-learn's `cross_validation` module) work.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，你可以看看`KFold`和`StratifiedKFold`（来自Scikit-learn的`cross_validation`模块）是如何工作的。
- en: 'They are both iterators: you draw the indices for training and testing for
    each round of cross validation, with the sole difference that `KFold` just applies
    a random draw. Instead, `StratifiedKFold` takes account of the distribution of
    a target variable that you want distributed in your training and test samples
    as if it were on the original set.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 它们都是迭代器：你为每一轮交叉验证抽取训练和测试的索引，唯一的区别在于`KFold`只是进行随机抽取。相反，`StratifiedKFold`会考虑到你希望在训练和测试样本中分布的目标变量的分布，就像它在原始集合上一样。
- en: 'As parameters to both classes, you should provide:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作为两个类的参数，你应该提供：
- en: The count of observations to `KFold` and the target vector to `StratifiedKFold`
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KFold`的观测计数和`StratifiedKFold`的目标向量'
- en: The number of folds (10 is usually the standard choice, but you can decrease
    the number of folds if you have many observations, or you can increase it if your
    dataset is small)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 折叠数（通常选择10，但如果你有大量观测值，可以减少折叠数，或者如果你的数据集很小，可以增加折叠数）
- en: 'You should also decide:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该决定：
- en: Whether to shuffle the data or take it as it is (shuffling is always recommended)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否要打乱数据或按原样取（打乱总是推荐）
- en: Whether to apply a random seed and make the results replicable
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否应用随机种子并使结果可重复
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `n_jobs` parameter will set the number of threads involved in the computation
    of the results by leveraging parallel computations. When it is set to `−1` it
    will automatically use all the available threads, speeding up the calculations
    to the maximum on your computer. Anyway, depending on the system you are working
    on, sometimes setting the parameter to something different than `1` will cause
    problems, slowing down the results. In our examples, as a precautionary measure,
    it is always set to `1`, but you can change its value if you need to cut short
    the computational time.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_jobs`参数将通过利用并行计算来设置参与结果计算的线程数。当它设置为`-1`时，将自动使用所有可用的线程，在您的计算机上尽可能快地加速计算。无论如何，根据您正在工作的系统，有时将参数设置为不同于`1`的值可能会导致问题，减慢结果。在我们的示例中，作为预防措施，它始终设置为`1`，但如果你需要缩短计算时间，你可以更改其值。'
- en: 'At first, we try to get the cross-validation score of an over-parameterized
    model (a second-degree polynomial expansion of the original features of the Boston
    dataset). Please notice that the results are negative (though they are squared
    errors) because of the internals of the automatic function for computing the cross-validation
    of a model, `cross_val_score`, from Scikit-learn. This function requires the model,
    the features, and the target variable as input. It also accepts a cross validation
    iterator of your choice for the parameter `cv`, a string for `scoring` indicating
    the name of the scoring function to be used (more on this can be found at: [http://scikit-learn.org/stable/modules/model_evaluation.html](http://scikit-learn.org/stable/modules/model_evaluation.html));
    and finally the number of threads working in parallel on your PC by specifying
    `n_jobs` (`1` indicates that only one thread is working whereas `−1` indicates
    all the available threads in the system are used):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们尝试获取一个过参数化模型（波士顿数据集原始特征的二次多项式展开）的交叉验证分数。请注意，由于Scikit-learn中计算模型交叉验证的自动函数`cross_val_score`的内部机制，结果为负（尽管它们是平方误差）。这个函数需要模型、特征和目标变量作为输入。它还接受一个用于参数`cv`的交叉验证迭代器，一个用于`scoring`的字符串，表示要使用的评分函数的名称（更多内容请参阅：[http://scikit-learn.org/stable/modules/model_evaluation.html](http://scikit-learn.org/stable/modules/model_evaluation.html)）；最后，通过指定`n_jobs`来指定在您的PC上并行工作的线程数（`1`表示只有一个线程在工作，而`-1`表示使用系统中的所有可用线程）：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Tip
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The mean squared error is negative because of the internals of the function,
    which can only maximize, whereas our cost metric has to be minimized; this is
    why it has become negative
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差为负是因为函数的内部机制只能最大化，而我们的成本指标需要最小化；这就是为什么它变成了负数
- en: 'After removing the sign, we can take both the average and the standard deviation.
    Here, we can also notice that the standard deviation is high, and maybe we should
    then try to control the distribution of the target variable, since in the real
    estate business there are outlying observations due to very rich residential areas:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在去掉符号后，我们可以取平均值和标准差。在这里，我们还可以注意到标准差很高，因此我们可能需要尝试控制目标变量的分布，因为在房地产业务中，由于非常富有的住宅区，存在异常观测值：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To apply such a control, we stratify the target variable; that is, we divide
    it into bins and we expect the bin distribution to be kept during the cross-validation
    process:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用这种控制，我们将目标变量分层；也就是说，我们将其划分为区间，并期望在交叉验证过程中保持区间分布：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the end, controlling for the response distribution really lowers the standard
    deviation of the estimated error (and our expected average). A successful stratification
    attempt in cross-validation suggests that we should train on a correctly distributed
    training sample, otherwise we may achieve an outcome model not always working
    properly due to bad sampling.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，控制响应分布实际上降低了估计误差的标准差（以及我们的预期平均值）。在交叉验证中成功分层尝试表明，我们应该在正确分布的训练样本上训练，否则我们可能会得到一个由于采样不良而始终不能正常工作的结果模型。
- en: Tip
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: As a final remark on the topic of cross-validation, we suggest using it mostly
    for evaluating parameters, and always relying on a small drawn out test set for
    performance validation. In fact, it is a bit tricky, but if you cross-validate
    too many times (for example changing the seed) looking for the best performance,
    you will end up with the best result, which is another form of overfitting called
    snooping (this also happens if you do the same with the test set). Instead, when
    you use cross-validation to choose between parameters, you just decide on the
    best among the options, not on the absolute cross-validation value.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 关于交叉验证的最后一个评论，我们建议主要用它来评估参数，并且始终依赖于一个小型的抽取测试集进行性能验证。实际上，这有点棘手，但如果交叉验证次数太多（例如改变种子）以寻找最佳性能，你最终会得到最佳结果，这是另一种称为窥探的过拟合形式（如果你对测试集做同样的事情也会发生）。相反，当你使用交叉验证来选择参数时，你只需决定在选项中哪个是最好的，而不是绝对交叉验证值。
- en: Bootstrapping
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bootstrapping
- en: Sometimes, if the training data is really small, even dividing into folds can
    penalize how the model is trained. The statistical technique of bootstrapping
    allows repeating the training and testing validation sequence (allowing precise
    estimations of both the mean and standard deviation of expected results) for a
    large number of times by trying to replicate the underlying distribution of the
    data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，如果训练数据真的非常小，即使是划分成折叠也可能惩罚模型的训练方式。重抽样技术允许通过尝试复制数据的潜在分布来重复训练和测试验证序列（允许对预期结果的均值和标准差进行精确估计）多次。
- en: 'Bootstrapping is based on sampling with repetition, which implies allowing
    an observation to be drawn multiple times. Usually bootstraps draw the number
    of observations equivalent to the original size of the dataset. Also, there''s
    always a part of the observations that it is left untouched, equivalent to a third
    of the available observations, which can be used for validating:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Bootstrapping基于重复抽样的方法，这意味着允许一个观测值被多次抽取。通常，重抽样抽取的观测数与原始数据集的大小相当。此外，总有一部分观测值保持未动，相当于可用观测值的三分之一，可以用来验证：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output will be shown as the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将显示如下：
- en: '![Bootstrapping](img/00108.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![Bootstrapping](img/00108.jpeg)'
- en: 'As illustrated by the preceding example (unfortunately, this method is not
    part of Scikit-learn, having being recently deprecated), in a set of 10 observations,
    on average four observations are left available for testing purposes. However,
    in a bootstrapping process, it is not just the left out cases that provide insight.
    A model is in fact fitted to the training dataset, and we can also inspect how
    the coefficients are determined in the bootstrap replications, thus allowing us
    to figure out how stable each coefficient is:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示（遗憾的是，这种方法不是Scikit-learn的一部分，最近已被弃用），在10个观测值中，平均有四个观测值可用于测试目的。然而，在重抽样过程中，不仅仅是排除的案例提供了洞察。模型实际上拟合到训练数据集，我们还可以检查在重抽样复制中系数是如何确定的，从而让我们了解每个系数的稳定性：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For instance, the tenth coefficient index (PTRATIO) is quite stable in both
    sign and value:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，第十个系数索引（PTRATIO）在符号和值上都非常稳定：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Whereas the sixth coefficient (AGE) has great variability, often even changing
    sign:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 而第六个系数（AGE）具有很大的变异性，经常甚至改变符号：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In conclusion, bootstrap is a form of replication that can be run as many times
    as you decide, and this allows you to create multiple models and evaluate their
    results in a similar way to a cross-validation procedure.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，bootstrap是一种可以运行多次的复制形式，这允许你创建多个模型，并以类似交叉验证过程的方式评估它们的结果。
- en: Greedy selection of features
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征贪婪选择
- en: By following our experiments throughout the book, you may have noticed that
    adding new variables is always a great success in a linear regression model. That's
    especially true for training errors and it happens not just when we insert the
    right variables but also when we place the wrong ones. Puzzlingly, when we add
    redundant or non-useful variables, there is always a more or less positive impact
    on the fit of the model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通过跟随本书中的实验，你可能已经注意到，在线性回归模型中添加新变量总是大获成功。这尤其适用于训练误差，而且这种情况不仅发生在我们插入正确的变量时，也发生在我们放置错误的变量时。令人费解的是，当我们添加冗余或无用的变量时，模型拟合度总是或多或少地有所提高。
- en: The reason is easily explained; since regression models are high-bias models,
    they find it beneficial to augment their complexity by increasing the number of
    coefficients they use. Thus, some of the new coefficients can be used to fit the
    noise and other details present in data. It is precisely the memorization/overfitting
    effect we discussed before. When you have as many coefficients as observations,
    your model can become saturated (that's the technical term used in statistics)
    and you could have a perfect prediction because basically you have a coefficient
    to learn every single response in the training set.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 原因很容易解释；由于回归模型是高偏差模型，它们发现通过增加它们使用的系数数量来增加其复杂性是有益的。因此，一些新的系数可以用来拟合数据中存在的噪声和其他细节。这正是我们之前讨论的记忆/过度拟合效应。当你有与观察值一样多的系数时，你的模型可能会饱和（这是统计学中使用的术语），你可能会得到完美的预测，因为基本上你有一个系数来学习训练集中每个响应。
- en: 'Let''s make this concept more concrete with a quick example using a training
    set (in-sample observations) and a test set (out-sample observations). Let''s
    start by finding out how many cases and features we have and what the baseline
    performance is (for both in-sample and out-sample):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个训练集（样本内观察）和一个测试集（样本外观察）的快速示例来使这个概念更具体。让我们首先找出我们有多少个案例和特征，以及基线性能是什么（对于样本内和样本外）：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Tip
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The best approach would be to use a cross validation or bootstrap for such an
    experiment, not just a plain train/test split, but we want to make it fast, and
    that's the reason why we decided on such a solution. We assure you that using
    more sophisticated estimation techniques doesn't change the results of the experiment.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳方法将是使用交叉验证或自助法进行此类实验，而不仅仅是简单的训练/测试分割，但我们希望使其快速，这就是我们选择这种解决方案的原因。我们向您保证，使用更复杂的估计技术不会改变实验的结果。
- en: 'Therefore, we have similar in-sample and out-sample errors. We can start working
    on improving our model using polynomial expansions:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在样本内和样本外的误差是相似的。我们可以开始通过多项式展开来改进我们的模型：
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'First, we apply the second-order polynomial expansion:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应用二次多项式展开：
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: It seems that the good in-sample results have little correspondence with the
    out-sample test. Though the out-sample performance has improved, the lack of comparability
    in results is a clear sign of overfitting; there are some more useful coefficients
    in the model but most of them are just there to catch noise in data.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，好的样本内结果与样本外测试结果几乎没有对应关系。尽管样本外性能有所提高，但结果缺乏可比性是过度拟合的明显迹象；模型中有些系数更有用，但大多数只是用来捕捉数据中的噪声。
- en: 'We now go to extremes and we test the third-degree polynomial expansion (using
    only interactions though):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们走向极端，测试三次多项式展开（尽管只使用交互项）：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now, clearly something very bad has happened to our model. Having more coefficients
    than observations (`p>n`), we achieved a perfect fit on our training set. However,
    on the out-sample validation, it seems that our model achieved the same performance
    as a random number generator. In the next few paragraphs, we will show you how
    to take advantage of an increased number of features without incurring any of
    the problems demonstrated by the previous code snippets.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，显然我们的模型出了大问题。由于系数多于观察值（`p>n`），我们在训练集上实现了完美拟合。然而，在样本外验证中，我们的模型似乎达到了与随机数生成器相同的性能。在接下来的几段中，我们将向您展示如何利用增加的特征数量，而不会产生之前代码片段中展示的任何问题。
- en: The Madelon dataset
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Madelon数据集
- en: For the task of selecting the best subset of variables among many noisy and
    collinear ones, we decided to accompany our usual Boston house dataset with a
    tricky one, the Madelon dataset ([https://archive.ics.uci.edu/ml/datasets/Madelon](https://archive.ics.uci.edu/ml/datasets/Madelon)).
    It is an artificial dataset (that is generated using an algorithm) presented at
    the NIPS 2003 (the seventh Annual Conference on Neural Information Processing
    Systems) during a contest on feature selection.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在众多嘈杂且共线的变量中选择最佳子集的任务，我们决定将我们常用的波士顿房价数据集与一个棘手的Madelon数据集（[https://archive.ics.uci.edu/ml/datasets/Madelon](https://archive.ics.uci.edu/ml/datasets/Madelon)）一起使用。这是一个人工数据集（使用算法生成），在2003年NIPS（第七届神经信息处理系统年会）期间的一个特征选择竞赛中展出。
- en: The dataset is particularly challenging because it has been generated by placing
    32 distinct clusters of points (16 from the positive group, 16 from the negative
    one) on the vertices of a five-dimension hypercube. The resulting 500 features
    and 2,000 cases have been extracted from various transformations of the five metric
    dimensions. To make things harder, some random numbers have been added to the
    features to act as noise and a few responses have been flipped (the flipped ones
    amount to 1%). All these intricate transformations make dealing with the modeling
    quite difficult, especially for linear models, since the relationship of most
    of the features with the response is definitely non-linear. This is really helpful
    for our exemplification because it clearly demonstrates how a direct inclusion
    of all the features is detrimental to the accuracy of out-of-sample predictions.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集尤其具有挑战性，因为它是在将32个不同的点簇（16个来自正组，16个来自负组）放置在五维超立方体的顶点上生成的。从五个度量维度的各种变换中提取了500个特征和2000个案例。为了使事情更难，一些随机数被添加到特征中作为噪声，并且一些响应被翻转（翻转的占1%）。所有这些复杂的变换使得处理建模相当困难，尤其是对于线性模型，因为大多数特征与响应的关系肯定是非线性的。这对我们的示例非常有帮助，因为它清楚地表明直接包含所有特征会损害样本外预测的准确性。
- en: 'To download and make available on your computer such an interesting and challenging
    dataset, please carry out the following instructions and allow some time for your
    computer to download the data from the external website where it is stored:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载并将这样一个有趣且具有挑战性的数据集上传到您的计算机上，请执行以下说明，并允许您的计算机从存储数据的外部网站下载数据所需的时间：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After finishing loading both the training and validation sets, we can start
    exploring some of the information available:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载完训练集和验证集后，我们可以开始探索一些可用的信息：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Naturally, we won''t touch the validation set (we won''t even glance at it
    or it would be snooping), but we can try to figure out the situation with the
    training set:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，我们不会触及验证集（我们甚至不会瞥一眼，否则就是窥探），但我们可以尝试通过训练集来了解情况：
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is quite lengthy and it is put in matrix form (therefore it is not
    reported here), but it really tells us everything about the mean, min, max, variance,
    skewness, and kurtosis for each feature in the dataset. A fast glance through
    it doesn''t reveal anything special; however, it explicits that all the variables
    have an approximately normal distribution and that they have a limited range of
    values. We can proceed with our exploration using a graphical representation of
    correlations among the variables:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 输出相当长，并以矩阵形式呈现（因此在此处未报告），但它确实告诉我们关于数据集中每个特征的均值、最小值、最大值、方差、偏度和峰度的所有信息。快速浏览它并没有揭示任何特别之处；然而，它明确指出所有变量都近似呈正态分布，并且它们的值范围有限。我们可以使用变量之间的相关性图继续我们的探索：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Check the following screenshot:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下截图：
- en: '![The Madelon dataset](img/00109.jpeg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![The Madelon dataset](img/00109.jpeg)'
- en: After a glance at a portion of the features and their respective correlation,
    we can notice that just a couple of them have a significant correlation, whereas
    the others are mildly related. This gives the impression of noisy relationships
    between them, thus rendering an effective selection quite complicated.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 简单浏览一下部分特征及其相关系数后，我们可以注意到其中只有几个具有显著的相关性，而其他则只有轻微的相关性。这给人一种它们之间关系嘈杂的印象，因此有效的选择变得相当复杂。
- en: As a last step, we check how a simple logistic regression model would score
    in terms of the error measured using the area under the curve metric.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，我们检查一个简单的逻辑回归模型在曲线下面积指标测量的误差方面会得分如何。
- en: Tip
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Area under the curve (AUC) is a measure derived from comparing the rate of correct
    positive results against the rate of incorrect ones at different classification
    thresholds. It is a bit tricky to calculate, so we suggest always relying on the
    `roc_auc_score` function from the `sklearn.metrics` module.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线下面积（AUC）是通过比较在不同分类阈值下正确正例率与错误率之间的比率得出的一个度量。计算起来有点棘手，所以我们建议始终依赖 `sklearn.metrics`
    模块中的 `roc_auc_score` 函数。
- en: Logistic regression classifies an observation as positive if the threshold is
    over 0.5 since such a split is always proved to be optimal, but we can freely
    change that threshold. To increase the precision at a top selection of results
    we just raise the threshold from 0.5 to 1.0 (raising the threshold increases the
    accuracy in the selected range). Instead, if we intend to increase the total number
    of guessed positive cases we just choose a threshold inferior to 0.5 down to almost
    0.0 (lowering the threshold increases the coverage of positive cases in the selected
    range).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归将观察结果分类为正例，如果阈值超过 0.5，因为这种分割总是被证明是最优的，但我们可以自由地改变这个阈值。为了提高顶级结果选择的精度，我们只需将阈值从
    0.5 提高到 1.0（提高阈值会增加所选范围内的准确性）。相反，如果我们打算增加猜测的正例总数，我们只需选择一个低于 0.5 的阈值，直到几乎为 0.0（降低阈值会增加所选范围内正例的覆盖率）。
- en: The AUC error measure helps us determine whether our predictions are ordered
    properly, no matter their effective precision in terms of value. Thus, AUC is
    the ideal error measure to evaluate an algorithm for selection. If you order results
    properly based on probability, no matter if the guessed probability is correct
    or not, you can simply pick the correct selection to be used by your project by
    changing the 0.5 threshold—that is, by taking a certain number of the top results.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: AUC 错误度量帮助我们确定我们的预测是否按顺序排列得当，无论它们在价值方面的有效精度如何。因此，AUC 是评估选择算法的理想错误度量。如果你根据概率正确地排列结果，无论猜测的概率是否正确，你都可以通过改变
    0.5 阈值——也就是说，通过选择一定数量的顶级结果——简单地选择用于你项目的正确选择。
- en: 'In our case, the baseline AUC measure is `0.602`, a quite disappointing value
    since a random selection should bring us a `0.5` value (`1.0` is the maximum possible):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，基线 AUC 度量为 `0.602`，这是一个相当令人失望的值，因为随机选择应该给我们带来 `0.5` 的值（`1.0` 是可能的最大值）：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Univariate selection of features
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征的单变量选择
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Feature selection can help in both increasing the model out-sample performance
    and its human readability by retaining only the most predictive set of variables
    in the model, in some cases just the best ones and in others the set that works
    the best in unison.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择可以通过仅保留模型中最具预测性的变量集来帮助提高模型的样本外性能和其可读性，在某些情况下只是最好的那些，在其他情况下是协同工作效果最好的那些。
- en: There are quite a few feature selection methods. The simplest approach is the
    univariate method, which evaluates how good a variable is by estimating its predictive
    value when taken alone in respect of the response.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择方法有很多。最简单的方法是单变量方法，它通过估计变量在单独考虑时相对于响应的预测值来评估变量的好坏。
- en: 'This usually involves using statistical tests, and Scikit-learn offers three
    possible tests:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常涉及使用统计测试，Scikit-learn 提供了三种可能的测试：
- en: The `f_regression` class, which works out an F-test (a statistical test for
    comparing different regression solutions) and a p-value (interpretable as the
    probability value in which we observed a difference by chance) and reveals the
    best features for a regression
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`f_regression` 类，它执行 F 测试（一种比较不同回归解决方案的统计测试）和 p 值（可以解释为观察到的差异是偶然发生的概率值），并揭示回归的最佳特征'
- en: The `f_class`, which is an Anova F-test (a statistical test for comparing differences
    among classes), another statistical and related method that will prove useful
    for classification problems
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`f_class`，这是一个 Anova F 测试（一种比较类别之间差异的统计测试），另一种对分类问题有用的统计和相关方法'
- en: The `Chi2` class, which is a chi-squared test (a statistical test on count data),
    a good choice when your problem is classification and your answer variable is
    a count or a binary (in every case, a positive number such as units sold or money
    earned)
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Chi2` 类，这是一个卡方测试（一种针对计数数据的统计测试），当你的问题是分类且你的答案变量是计数或二进制（在所有情况下，都是正数，如销售单位或赚取的金钱）时，是一个很好的选择'
- en: All such tests output a score and a statistical test expressed by a p-value.
    High scores, confirmed by small p-values (under 0.05, indicating a low probability
    that the score has been obtained by luck), will provide you with confirmation
    that a certain variable is useful for predicting your target.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些测试都会输出一个分数和一个用p值表示的统计测试。高分，由小的p值（小于0.05，表示得分是通过运气获得的概率很低）证实，将为你提供确认，表明某个变量对你的目标预测是有用的。
- en: 'In our example, we will use `f_class` (since we are working on a classification
    problem now) and we will have the `SelectPercentile` function help us by selecting
    a certain percentage of high-scoring features:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们将使用`f_class`（因为我们现在正在处理一个分类问题）并且我们将使用`SelectPercentile`函数帮助我们选择一定百分比的得分较高的特征：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After selecting the upper half, hoping to have cut off the most irrelevant
    features and to have kept the important ones, we plot our results on an histogram
    to reveal the distribution of the scores:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择了上半部分后，希望已经切掉了最不相关的特征并保留了重要的特征，我们在直方图上绘制我们的结果以揭示得分的分布：
- en: '[PRE22]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Look at the following screenshot:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 看看下面的截图：
- en: '![Univariate selection of features](img/00110.jpeg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![特征的单变量选择](img/00110.jpeg)'
- en: 'Noticeably, most scores are near zero, with a few high-ranking ones. Now we
    are going to pick the features we assume to be important by directly selecting
    a threshold empirically chosen for its convenience:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，大多数分数接近零，只有少数得分较高。现在我们将通过直接选择一个为了方便而经验性地选择的阈值来选择我们假设重要的特征：
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we have reduced our dataset to just the core features. At this point,
    it does make sense to test a polynomial expansion and try to automatically catch
    any relevant non-linear relationship in our model:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经将我们的数据集缩减到仅包含核心特征。在这个时候，测试多项式展开并尝试自动捕捉模型中的任何相关非线性关系是有意义的：
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The resulting validation score (out-sample) is about 0.81, a very promising
    value given our initial overfitted score of 0.82 on the training set. Of course,
    we can decide to stop here or try to go on filtering even the polynomial expansion;
    feature selection is really a never-ending job, though after a certain point you
    have to realize that only slightly incremental results are possible from further
    tuning.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的验证分数（外部样本）约为0.81，考虑到我们在训练集上的初始过拟合分数为0.82，这是一个非常有希望的价值。当然，我们可以决定在这里停止，或者尝试进一步过滤多项式展开；特征选择实际上是一项永无止境的工作，尽管在某个点上你必须意识到，进一步的调整只能带来微小的增量结果。
- en: Recursive feature selection
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 递归特征选择
- en: The only problem with univariate selection is that it will decide the best features
    by considering each feature separately from the others, not verifying how they
    work together in unison. Consequently, redundant variables are not infrequently
    picked (due to collinearity).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 单变量选择的唯一问题是它将通过单独考虑每个特征，而不是验证它们如何协同工作来决定最佳特征。因此，冗余变量并不罕见地被选中（由于多重共线性）。
- en: A multivariate approach, such as recursive elimination, can avoid this problem;
    however, it is more computationally expensive.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 多变量方法，如递归消除，可以避免这个问题；然而，它计算成本更高。
- en: Recursive elimination works by starting with the full model and by trying to
    exclude each variable in turn, evaluating the removal effect by cross-validation
    estimation. If certain variables have a negligible effect on the model's performance,
    then the elimination algorithm just prunes them. The process stops when any further
    removal is proven to hurt the ability of the model to predict correctly.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 递归消除通过从完整模型开始，并尝试依次排除每个变量，通过交叉验证估计来评估移除效果。如果某些变量对模型性能的影响可以忽略不计，那么消除算法就会剪枝它们。当任何进一步的移除被证明会损害模型正确预测的能力时，这个过程就会停止。
- en: 'Here is a demonstration of how `RFECV`, Scikit-learn''s implementation of recursive
    elimination, works. We will use the Boston dataset enhanced by second-degree polynomial
    expansion, thus working on a regression problem this time:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是`RFECV`，Scikit-learn的递归消除实现的一个演示。我们将使用通过二次多项式扩展增强的波士顿数据集，因此这次我们处理的是一个回归问题：
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Given an estimator (our model), a cross validation iterator, and an error measure,
    `RFECV` will find out after a while that half of the features can be dropped from
    the model without fear of worsening its performance:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个估计器（我们的模型）、一个交叉验证迭代器和误差度量，`RFECV`会在一段时间后找出，可以放心地从模型中删除一半的特征，而不用担心会降低其性能：
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'A test-based check will reveal that now the out-sample performance is 11.5\.
    For further confirmation, we can also run a cross-validation and obtain a similar
    result:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 基于测试的检查将揭示现在的外部样本性能为 11.5。为了进一步确认，我们还可以运行交叉验证并获得类似的结果：
- en: '[PRE27]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Regularization optimized by grid-search
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过网格搜索优化的正则化
- en: Regularization is another way to modify the role of variables in a regression
    model to prevent overfitting and to achieve simpler functional forms. The interesting
    aspect of this alternative approach is that it actually doesn't require manipulating
    your original dataset, making it suitable for systems that learn and predict online
    from large amounts of features and observations, without human intervention. Regularization
    works by enriching the learning process using a penalization for too complex models
    to shrink (or reduce to zero) coefficients relative to variables that are irrelevant
    for your prediction term or are redundant, as they are highly correlated with
    others present in the model (the collinearity problem seen before).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是修改回归模型中变量角色的另一种方法，以防止过拟合并实现更简单的函数形式。这种替代方法的有趣之处在于，它实际上不需要操作你的原始数据集，这使得它适合于从大量特征和观察中在线学习和预测的系统，而不需要人为干预。正则化通过使用对过于复杂的模型进行惩罚来丰富学习过程，以缩小（或减少到零）与预测项无关的变量或冗余变量的系数（如之前看到的共线性问题）。
- en: Ridge (L2 regularization)
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ridge (L2 正则化)
- en: 'The idea behind ridge regression is simple and straightforward: if the problem
    is the presence of many variables, which affect the regression model because of
    their coefficient, all we have to do is reduce their coefficient so their contribution
    is minimized and they do not influence the result so much.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Ridge 回归背后的思想简单直接：如果问题是存在许多变量，由于它们的系数影响回归模型，我们只需减少它们的系数，使它们的贡献最小化，这样它们就不会对结果产生太大影响。
- en: Such a result is easily achieved by working out a different cost function. Working
    on the error in respect of the answer, the cost function can be balanced by imposing
    a penalization value depending on how large the coefficients are.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算不同的损失函数，这样的结果很容易实现。在考虑答案的错误时，可以通过施加一个依赖于系数大小的惩罚值来平衡损失函数。
- en: 'In the following formula, a reprisal of the formula in the [Chapter 2](part0018_split_000.html#H5A42-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 2. Approaching Simple Linear Regression"), *Approaching Simple Linear
    Regression* paragraph *Gradient descent at work*, the weight update is modified
    by the presence of a negative term, which is the square of the weight reduced
    by a factor expressed by lambda. Consequently, the larger the coefficient, the
    more it will be reduced during the update phase of the gradient descent optimization:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下公式中，是对 [第 2 章](part0018_split_000.html#H5A42-a2faae6898414df7b4ff4c9a487a20c6
    "第 2 章。接近简单线性回归") 中公式的一个重述，*接近简单线性回归* 段落 *梯度下降在起作用*，权重更新通过存在一个负项而修改，这个负项是权重减去一个由
    lambda 表示的因子的平方。因此，系数越大，在梯度下降优化的更新阶段减少的幅度就越大：
- en: '![Ridge (L2 regularization)](img/00111.jpeg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![Ridge (L2 正则化)](img/00111.jpeg)'
- en: In the preceding formula, each single coefficient `j`, whose value is represented
    by `w[j]`, is updated by the gradient descent learning rate α `/ n`, where `n`
    is the number of observations. The learning rate is multiplied by the summed deviance
    of the prediction (the gradient). The novelty is the presence in the gradient
    of a penalization, calculated as the squared coefficient multiplied by a `λ` lambda
    coefficient.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，每个单独的系数 `j`，其值由 `w[j]` 表示，通过梯度下降学习率 α `/ n` 进行更新，其中 `n` 是观察数的数量。学习率乘以预测的偏差总和（梯度）。新意在于梯度中存在一个惩罚项，该惩罚项是系数的平方乘以一个
    `λ` lambda 系数。
- en: In this way, the error will be propagated to the coefficients only if there
    is an advantage (a large deviance in predictions), otherwise the coefficients
    will be reduced in value. The advantage is controlled by the `λ` lambda value,
    which has to be found empirically according to the specific model that we are
    building.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，只有当存在优势（预测中的大偏差）时，错误才会传播到系数上，否则系数的值将减少。优势由 `λ` lambda 值控制，该值必须根据我们正在构建的特定模型进行经验性寻找。
- en: 'An example will clarify how this new approach works. First, we have to use
    the `Ridge` class from Scikit-learn, if our problem is a regression, or we use
    the penalty parameter in the `LogisticRegression` specification (`LogisticRegression(C=1.0,
    penalty=''l2'', tol=0.01)`):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子将阐明这种新方法是如何工作的。首先，我们必须使用Scikit-learn中的`Ridge`类，如果我们的问题是回归，或者我们在`LogisticRegression`规范中使用惩罚参数（`LogisticRegression(C=1.0,
    penalty='l2', tol=0.01)`）：
- en: '[PRE28]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The impact of regularization on the model is controlled by the `alpha` parameter
    in the `Ridge`, and by the `C` parameter in `LogisticRegression`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化对模型的影响由`Ridge`中的`alpha`参数和`LogisticRegression`中的`C`参数控制。
- en: 'The smaller the value of `alpha`, the less the coefficient values are controlled
    by the regularization, the higher its value with increased regularization, the
    more the coefficients are shrunk. Its functioning can be easily memorized as a
    shrinkage parameter: the higher the value, the higher the shrinkage of the complexity
    of the model. However, the C parameter in `LogisticRegression` is exactly the
    inverse, with smaller values corresponding to high regularization (*alpha = 1
    / 2C*).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`alpha`的值越小，系数值受正则化的控制就越少，其值随着正则化的增加而增加，系数就越会被压缩。其功能可以简单地记住作为一个压缩参数：值越高，模型的复杂性压缩就越高。然而，`LogisticRegression`中的C参数正好相反，较小的值对应于高正则化（*alpha
    = 1 / 2C*）。'
- en: 'After having completely fitted the model, we can have a look at how the values
    of coefficients are defined now:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全拟合模型之后，我们可以看看系数值的定义现在是如何的：
- en: '[PRE29]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now, the average coefficient value is almost near zero and the values are placed
    in a much shorter range than before. In the regularized form, no single coefficient
    has the weight to influence or, worse, disrupt a prediction.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，平均系数值几乎接近零，并且值被放置在一个比之前更短的范围内。在正则化形式中，没有任何单个系数具有影响或，更糟糕的是，破坏预测的权重。
- en: Grid search for optimal parameters
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化参数的网格搜索
- en: Until now, we haven't had much to decide about the model itself, no matter whether
    we decided on a logistic or a linear regression. All that mattered was to properly
    transform our variables (and actually, we have learned that this is not an easy
    task either); however, the introduction of the L2 parameter brings forth much
    more complexity since we also have to heuristically set a value to maximize the
    performance of the model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有太多关于模型本身的决策要做，无论我们决定使用逻辑回归还是线性回归。重要的是正确地转换我们的变量（实际上，我们已经了解到这也不是一项容易的任务）；然而，L2参数的引入带来了更多的复杂性，因为我们还必须启发式地设置一个值以最大化模型的性能。
- en: Keeping on working with cross-validation, which ensures we evaluate the performance
    of our model in a realistic way, a good solution to this problem is to check systematically
    the result of our model given a range of possible values of our parameter.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用交叉验证，这确保我们以现实的方式评估模型性能，解决这个问题的好方法是系统地检查给定参数可能值范围的模型结果。
- en: 'The `GridSearchCV` class in the Scikit-learn package can be set using our preferred
    `cv` iterator and scoring after setting a dictionary explicating what parameters
    have to be changed in the model (the key) and a range of values to be evaluated
    (a list of values related to the key), finally assigning it to the `param_grid`
    parameter of the class:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn包中的`GridSearchCV`类可以通过我们首选的`cv`迭代器和评分设置，在设置一个字典来解释模型中必须更改的参数（键）和要评估的值范围（与键相关的值列表）之后，将其分配给类的`param_grid`参数：
- en: '[PRE30]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The result of the search, which can take some time when there are many possible
    model variations to test, can be explored using the attribute `grid_scores_`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索的结果，当有大量可能的模型变体要测试时可能需要一些时间，可以通过属性`grid_scores_`来探索：
- en: '[PRE31]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The maximum scoring value (actually using RMSE we should minimize the result,
    so the grid search works with the negative value of RMSE) is achieved when alpha
    is `0.001`. In addition, the standard deviation of the cross-validation score
    is minimal in respect of our possible solutions, confirming to us that it is the
    best solution available at hand.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当`alpha`为`0.001`时，达到了最大的评分值（实际上，我们应该使用RMSE的最小化结果，因此网格搜索使用RMSE的负值）。此外，交叉验证评分的标准差相对于我们的可能解决方案是最小的，这证实了它是我们目前可用的最佳解决方案。
- en: Tip
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you would like to further optimize the results, just explore, using a second
    grid search, the range of values around the winning solution—that is, in our specific
    case from `0.0001` to `0.01`, you may find a slightly better value in terms of
    expected results or stability of the solution (expressed by the standard deviation).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要进一步优化结果，只需使用第二次网格搜索探索获胜解决方案周围的价值范围——即在我们的特定情况下从 `0.0001` 到 `0.01`，你可能会找到一个稍微更好的值，从预期的结果或解决方案的稳定性（用标准差表示）的角度来看。
- en: Naturally, `GridSearchCV` can be used effectively when more parameters to be
    optimized are involved. Please be aware that the more parameters, the more trials
    have to be made, and the resulting number is a combination—that is, a multiplication—of
    all the possible values to be tested. Consequently, if you are testing four values
    of a hypermeter and four of another one, in the end you will need *4 × 4* trials
    and, depending on the cross-validation folds, let's say in our case 10, you'll
    have your CPU compute *4 × 4 × 10 = 160* models. Searches that are more complex
    may even involve testing thousands of models, and although `GridSearchCV` can
    parallelize all its computations, in certain cases it can still be a problem.
    We are going to address a possible solution in the next paragraph.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，当涉及更多需要优化的参数时，`GridSearchCV` 可以有效地使用。请注意，参数越多，需要进行的试验就越多，结果是所有可能测试值的组合——即乘积。因此，如果你正在测试一个超参数的四个值和另一个超参数的四个值，最终你需要进行
    *4 × 4* 次试验，并且根据交叉验证的折数，比如说在我们的例子中是10，你将让你的CPU计算 *4 × 4 × 10 = 160* 个模型。更复杂的搜索甚至可能涉及测试成千上万的模型，尽管
    `GridSearchCV` 可以并行化所有计算，但在某些情况下仍然可能是一个问题。我们将在下一段中讨论一个可能的解决方案。
- en: Tip
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'We have illustrated how to grid-search using the more general `GridSearchCV`.
    There is anyway a specialized function for automatically creating out-of-the-box
    a cross-validated optimized ridge regression using Scikit-learn: `RidgeCV`. There
    are automated classes also for the other regularization variants we are going
    to illustrate, `LassoCV` and `ElasticNetCV`. Actually, these classes, apart from
    being more synthetic than the approach we described, are much faster in finding
    the best parameter because they follow an optimization path (so they actually
    do not exhaustively search along the grid).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了如何使用更通用的 `GridSearchCV` 进行网格搜索。无论如何，Scikit-learn 提供了一个专门的功能，用于自动创建交叉验证优化的岭回归：`RidgeCV`。还有用于我们即将展示的其他正则化变体的自动化类，例如
    `LassoCV` 和 `ElasticNetCV`。实际上，这些类除了比我们描述的方法更简洁外，在寻找最佳参数方面也更快，因为它们遵循一个优化路径（因此它们实际上并没有在网格上全面搜索）。
- en: Random grid search
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机网格搜索
- en: Searching for good combinations of hyper-parameters in a grid is a really time-consuming
    task, especially if there are many parameters; the number of combinations can
    really explode and thus your CPU can take a long time to compute the results.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在网格中寻找良好的超参数组合是一项非常耗时的工作，尤其是如果有许多参数；组合的数量可能会急剧增加，因此你的CPU可能需要很长时间来计算结果。
- en: Moreover, it is often the case that not all hyper-parameters are important;
    in such a case, when grid-searching, you are really wasting time checking on a
    large number of solutions that aren't really distinguishable from one another,
    while instead omitting to check important values on critical parameters.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通常并非所有超参数都是重要的；在这种情况下，当进行网格搜索时，你实际上是在浪费时间检查大量彼此之间没有明显区别的解决方案，而忽略了检查关键参数的重要值。
- en: The solution is a random grid search, which is not only much speedier than the
    grid search, but it is also much more efficient, as pointed out in a paper by
    the scholars James Bergstra and Yoshua Bengio ([http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是一个随机网格搜索，它不仅比网格搜索快得多，而且效率也更高，正如学者詹姆斯·伯格斯特拉和约书亚·本吉奥在论文中指出的（[http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)）。
- en: Random search works by sampling possible parameters from ranges or distribution
    that you point out (the `NumPy` package has quite a lot of distributions that
    can be used, but for this test we found that `logspace` function is ideal for
    systematically exploring the L1/L2 range). Given a certain number of trials, there
    is a high chance that you can get the right hyper-parameters.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索通过从您指定的范围或分布中采样可能的参数来工作（`NumPy`包有很多可以使用的分布，但在这个测试中我们发现`logspace`函数对于系统地探索L1/L2范围是理想的）。给定一定数量的试验，您有很大机会可以得到正确的超参数。
- en: 'Here, we try using just `10` values sampled from `100` possible ones (so reducing
    our running time to `1/10` in respect of a grid search):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们尝试使用从`100`个可能的值中采样的`10`个值（因此将我们的运行时间减少到网格搜索的`1/10`）：
- en: '[PRE32]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Tip
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: As a heuristic, the number of trials of a random search depends on the number
    of possible combinations that may be tried under a grid search. As a matter of
    statistical probability, it has been empirically observed that the most efficient
    number of random trials should be between 30 and 60\. More than 60 random trials
    is unlikely to bring many more performance improvements from tuning hyper parameters
    than previously assessed.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种启发式方法，随机搜索的试验次数取决于在网格搜索下可能尝试的组合数量。从统计概率的角度来看，已经观察到最有效的随机试验次数应该在30到60之间。超过60次随机试验不太可能比之前评估的从调整超参数中获得更多的性能提升。
- en: Lasso (L1 regularization)
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lasso (L1 regularization)
- en: Ridge regression is not really a selection method. Penalizing the useless coefficients
    through keeping them all in the model won't provide much clarity about what variables
    work the best in your linear regression and won't improve its comprehensibility.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归实际上并不是一种选择方法。通过在模型中保留所有系数来惩罚无用的系数，并不能提供很多关于哪些变量在您的线性回归中表现最好的清晰信息，也不会提高其可理解性。
- en: 'The lasso regularization, a recent addition by Rob Tibshirani, using the absolute
    value instead of the quadratic one in the regularization penalization, does help
    to shrink many coefficient values to zero, thus making your vector of resulting
    coefficients sparse:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso正则化，由Rob Tibshirani最近提出，在正则化惩罚中使用绝对值而不是二次值，这确实有助于将许多系数值缩小到零，从而使您的结果系数向量变得稀疏：
- en: '![Lasso (L1 regularization)](img/00112.jpeg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![Lasso (L1 regularization)](img/00112.jpeg)'
- en: Again, we have a formula similar to the previous one for L2 regularization but
    now the penalization term is made up of `λ` lambda multiplied by the absolute
    value of the coefficient.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们有一个类似于之前L2正则化的公式，但现在惩罚项由`λ` lambda乘以系数的绝对值组成。
- en: 'The procedure is the same as in the ridge regression; you just have to use
    a different class called `Lasso`. If instead your problem is a classification
    one, in your logistic regression you just have to specify that the parameter `penalty`
    is `''l1''`:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程与岭回归相同；您只需使用一个名为`Lasso`的不同类即可。如果您的问题是分类问题，在您的逻辑回归中只需指定参数`penalty`为`'l1'`：
- en: '[PRE33]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let''s check what happens to the previously seen regularization of the linear
    regression on the Boston dataset when using `Lasso`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查当使用`Lasso`时，之前在波士顿数据集上看到的线性回归正则化发生了什么变化：
- en: '[PRE34]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: From the viewpoint of performance, we obtained a slightly worse but comparable
    mean squared error value.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 从性能的角度来看，我们得到了一个略差但可比较的均方误差值。
- en: Tip
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: You will have noticed that using the `Lasso` regularization takes more time
    (there are usually more iterations) than applying the ridge one. A good strategy
    for speeding up things is to apply the lasso only on a subset of the data (which
    should take less time), find out the best alpha, and then apply it directly to
    your complete sample to verify whether the performance results are consistent.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，使用`Lasso`正则化比应用岭回归需要更多的时间（通常有更多的迭代）。一个加快事情的好策略是只对数据的一个子集应用Lasso（这应该会花费更少的时间），找出最佳的alpha值，然后直接应用到您的完整样本上以验证性能结果是否一致。
- en: 'However, what is most interesting is evaluating what coefficients have been
    reduced to zero:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最有趣的是评估哪些系数被减少到零：
- en: '[PRE35]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now, our second-degree polynomial expansion has been reduced to just `20` working
    variables, as if the model has been reduced by a recursive selection, with the
    advantage that you don't have to change the dataset structure; you just apply
    your data to the model and only the right variables will work out the prediction
    for you.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的二次多项式展开已经减少到仅有`20`个工作变量，就像模型通过递归选择进行了简化，其优势在于你不需要改变数据集结构；你只需将你的数据应用到模型中，只有正确的变量才会为你计算出预测结果。
- en: Tip
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you are wondering what kind of regularization to use first, `ridge` or `lasso`,
    a good rule of thumb is to first run a linear regression without any regularization
    and check the distribution of the standardized coefficients. If there are many
    with similar values, then `ridge` is the best choice; if instead you notice that
    there are a few important coefficients and many lesser ones, using `lasso` is
    advisable to remove the unimportant ones. In any case, when you have more variables
    than observations, you should always use `lasso`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道首先使用哪种正则化，`ridge`还是`lasso`，一个很好的经验法则是首先运行一个没有任何正则化的线性回归，并检查标准化系数的分布。如果有许多值相似，那么`ridge`是最好的选择；如果你注意到只有少数重要系数和许多较小的系数，那么使用`lasso`来移除不重要的系数是可取的。在任何情况下，当你有比观察更多的变量时，你应该始终使用`lasso`。
- en: Elastic net
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弹性网络
- en: 'Lasso can rapidly and without much hassle reduce the number of working variables
    in a prediction model, rendering it simpler and much more generalizable. Its strategy
    is simple: it aims to retain only the variables that contribute to the solution.
    Consequently, if, by chance, among your features you have a couple of strongly
    collinear variables, an L1 regularization will keep just one of them, on the basis
    of the characteristics of the data itself (noise and correlation with other variables
    contribute to the choice).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso可以快速且几乎无障碍地减少预测模型中的工作变量数量，使其更加简单和更具可推广性。其策略很简单：它旨在仅保留对解有贡献的变量。因此，如果你在特征中偶然有几个强共线性变量，L1正则化将仅保留其中一个，基于数据本身的特性（噪声和其他变量的相关性有助于选择）。
- en: Such a characteristic anyway may prove undesirable because of the instability
    of the L1 solution (the noise and strength of correlations may change with the
    data) since having all the correlated variables in the model guarantees a more
    reliable model (especially if they all depend on a factor that is not included
    into the model). Thus, the alternative elastic net approach has been devised by
    combining the effects of L1 and L2 regularization.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的特征无论如何可能因为L1解（噪声和相关性可能随着数据变化）的不稳定性而变得不理想，因为模型中包含所有相关变量可以保证模型更加可靠（特别是如果它们都依赖于模型中未包含的因素）。因此，通过结合L1和L2正则化的效果，已经设计出了弹性网络替代方法。
- en: In elastic net (Scikit-learn's `ElasticNet` class), you always have an `alpha`
    parameter that controls the impact of regularization on the determination of the
    model's coefficients, plus a `l1_ratio` parameter that helps weight the combination
    between the L1 and L2 parts of the regularization part of the cost function. When
    the parameter is `0.0`, there is no role for L1 so it is equivalent to a ridge.
    When it is set to `1.0`, you have a lasso regression. Intermediate values act
    by mixing the effects of both types of regularizations; thus, while some variables
    will still be reduced to zero value coefficients, collinear variables will be
    reduced to the same coefficient, allowing them all to be still present in the
    model formulation.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在弹性网络（Scikit-learn的`ElasticNet`类）中，你始终有一个`alpha`参数，它控制正则化对模型系数确定的影响，还有一个`l1_ratio`参数，它有助于权衡成本函数正则化部分的L1和L2部分之间的组合。当参数为`0.0`时，L1没有作用，因此相当于岭回归。当设置为`1.0`时，你有一个lasso回归。中间值通过混合两种正则化的效果起作用；因此，尽管一些变量将被减少到零值系数，但共线性变量将被减少到相同的系数，允许它们仍然存在于模型公式中。
- en: 'In the following example, we try solving our model with elastic net regularization:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的例子中，我们尝试使用弹性网络正则化来解决我们的模型：
- en: '[PRE36]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'By introspecting the solution, we realize that this is achieved by excluding
    a larger number of variables than a pure L1 solution; however, the resulting performance
    is similar to a L2 solution:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 通过内省解决方案，我们意识到这是通过排除比纯L1解更多的变量来实现的；然而，最终的性能与L2解相似：
- en: '[PRE37]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Stability selection
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稳定性选择
- en: As presented, L1-penalty offers the advantage of rendering your coefficients'
    estimates sparse, and effectively it acts as a variable selector since it tends
    to leave only essential variables in the model. On the other hand, the selection
    itself tends to be unstable when data changes and it requires a certain effort
    to correctly tune the C parameter to make the selection most effective. As we
    have seen while discussing elastic net, the peculiarity resides in the behavior
    of Lasso when there are two highly correlated variables; depending on the structure
    of the data (noise and correlation with other variables), L1 regularization will
    choose just one of the two.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，L1惩罚的优势在于使你的系数估计稀疏，并且实际上它充当了一个变量选择器，因为它倾向于只留下模型中的必要变量。另一方面，当数据变化时，选择本身往往是不稳定的，需要一定的努力来正确调整C参数，以使选择最有效。正如我们在讨论弹性网络时所看到的，特殊性在于Lasso在有两个高度相关变量时的行为；根据数据结构（噪声与其他变量的相关性），L1正则化将只选择这两个变量中的一个。
- en: In the field of studies related to bioinformatics (DNA, molecular studies),
    it is common to work with a large number of variables based on a few observations.
    Typically, such problems are denominated p >> n (features are much more numerous
    than cases) and they present the necessity to select what features to use for
    modeling. Because the variables are numerous and also are quite correlated among
    themselves, resorting to variable selection, whether by greedy selection or L1-penalty,
    can lead to more than one outcome taken from quite a large range of possible solutions.
    Two scholars, Nicolai Meinshausen and Peter Buhlmann, respectively from the University
    of Oxford and ETH Zurich, have come up with the idea of trying to leverage this
    instability and turn it into a surer selection.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在与生物信息学（DNA、分子研究）相关的研究领域，通常基于少量观察工作于大量变量。通常，这类问题被称为p >> n（特征远多于案例）并且它们需要选择用于建模的特征。由于变量众多，并且它们之间也相当相关，因此求助于变量选择，无论是贪婪选择还是L1惩罚，可能会导致从相当大的可能解决方案范围内得到多个结果。来自牛津大学和苏黎世联邦理工学院（ETH
    Zurich）的两位学者，Nicolai Meinshausen和Peter Buhlmann，分别提出了尝试利用这种不稳定性并将其转化为更可靠选择的观点。
- en: 'Their idea is straightforward: since L1-penalty is influenced by the cases
    and variables present in the dataset to choose a certain variable over others
    in the case of multicollinearity, we can subsample the cases and the variables
    to involve and fit with them a L1-penalized model repetitively. Then, for each
    run, we can record the features that got a zero coefficient and the one that didn''t.
    By pooling these multiple results, we can calculate a frequency statistic of how
    many times each feature got a non-zero value. In such a fashion, even if the results
    are unstable and uncertain, the most informative features will score a non-zero
    coefficient more often than less informative ones. In the end, a threshold can
    help to exactly retain the important variables and discard the unimportant ones
    and the collinear, but not so relevant, ones.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的想法很简单：由于L1惩罚受到数据集中存在的案例和变量的影响，在多重共线性情况下选择某个变量而不是其他变量，我们可以对案例和变量进行子采样，并反复用它们拟合一个L1惩罚模型。然后，对于每次运行，我们可以记录得到零系数的特征以及没有得到零系数的特征。通过汇总这些多个结果，我们可以计算每个特征得到非零值的频率统计。以这种方式，即使结果不稳定且不确定，最有信息量的特征会比信息量较少的特征更频繁地得到非零系数。最终，一个阈值可以帮助精确地保留重要的变量，并丢弃不重要的变量以及共线性但不太相关的变量。
- en: Tip
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The scoring can also be interpreted as a ranking of each variable's role in
    the model.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 分数也可以解释为对每个变量在模型中角色的排序。
- en: 'Scikit-learn offers two implementations of stability selection: `RandomizedLogisticRegression`
    for classification tasks and `RandomizedLasso` as a regressor. They are both in
    the `linear_model` module.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn提供了两种稳定性选择的实现：`RandomizedLogisticRegression`用于分类任务，`RandomizedLasso`作为回归器。它们都在`linear_model`模块中。
- en: 'They also both share the same key hyper-parameters:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 它们还共享相同的几个关键超参数：
- en: '`C` : is the regularization parameter, by default set to `1.0`. If you can
    manage to find a good C on all the data by cross-validation, put that figure in
    the parameter. Otherwise, start confidently using the default value; it is a good
    compromise.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`C`：是正则化参数，默认设置为`1.0`。如果你能够通过交叉验证在所有数据上找到一个好的C值，就将这个数字放入参数中。否则，可以自信地使用默认值；这是一个很好的折衷方案。'
- en: '`scaling` : is the percentage of feature to be kept at every iteration, the
    default value of `0.5` is a good figure; lower the number if there are many redundant
    variables in your data.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scaling`：是每次迭代要保留的特征的百分比；默认值`0.5`是一个很好的数值；如果数据中有许多冗余变量，则降低该数值。'
- en: '`sample_fraction` : is the percentage of observations to be kept; the default
    value of `0.75` should be decreased if you suspect outliers in your data (so they
    will less likely be drawn).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_fraction`：是要保留的观察值的百分比；如果怀疑数据中有异常值（因此它们不太可能被抽取），则应降低默认值`0.75`。'
- en: '`n_resampling` : the number of iterations; the more the better, but 200-300
    resamples should bear good results.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_resampling`：迭代次数；越多越好，但200-300次重采样应该能得到良好的结果。'
- en: Experimenting with the Madelon
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Madelon上进行实验
- en: From our past experimentations, stability selection does help to quickly fix
    any problem inherent to variable selection, even when dealing with sparse variables
    such as textual data rendered into indicator variables.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们过去的实验来看，稳定性选择确实有助于快速解决变量选择中固有的任何问题，即使处理的是稀疏变量，如转换为指示变量的文本数据。
- en: 'To demonstrate its effectiveness, we are going to apply it to the Madelon dataset,
    trying to get a better AUC score after stability selection:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明其有效性，我们将将其应用于Madelon数据集，尝试在稳定性选择后获得更好的AUC分数：
- en: '[PRE38]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Since it is a classification problem, we are going to use the `RandomizedLogisticRegression`
    class, setting `300` resamples and subsampling 15% of variables and 50% of observations.
    As a threshold, we are going to retain all those features that appear significant
    in the model at least 3% of the time. Such settings are quite strict, but they
    are due to the presence of high redundancy in the dataset and extreme instability
    of L1 solutions.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个分类问题，我们将使用`RandomizedLogisticRegression`类，设置`300`次重采样，并子采样15%的变量和50%的观察值。作为阈值，我们将保留在模型中至少出现3%时间的所有显著特征。这样的设置相当严格，但这是由于数据集存在高度冗余和L1解的极端不稳定性所致。
- en: Fitting the solution using a `make_pipeline` command allows us to create a sequence
    of actions to be first fitted and used on training data and then reapplied, using
    the same configuration, to the validation data. The idea is to first select the
    important and relevant features based on stability selection and then to create
    interactions (just multiplicative terms) using polynomial expansion to catch the
    non-linear components in the data with new derived features. If we were to create
    polynomial expansion without first selecting which variables we should use, then
    our dataset would exponentially grow in the number of variables and it could prove
    impossible even to store it in-memory.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`make_pipeline`命令拟合解决方案允许我们创建一系列操作，首先在训练数据上拟合和使用，然后使用相同的配置重新应用于验证数据。其想法是首先根据稳定性选择选择重要的相关特征，然后使用多项式展开创建交互（仅乘法项）来捕捉数据中的非线性成分，并使用新导出的特征。如果我们不先选择应该使用哪些变量就创建多项式展开，那么我们的数据集在变量数量上会呈指数增长，甚至可能无法在内存中存储。
- en: '`RandomizedLogisticRegression` acts more as a pre-processing filter than a
    predictive model: after fitting, though allowing us to have a glance at the produced
    scores, it won''t allow any prediction on the basis of the host of created models,
    but it will allow us to transform any dataset similar to ours (the same number
    of columns), keeping only the columns whose score is above the threshold that
    we initially defined when we instantiated the class.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomizedLogisticRegression`更像是一个预处理过滤器而不是预测模型：拟合后，虽然可以让我们查看生成的分数，但它不会基于创建的模型进行任何预测，但它将允许我们将任何类似于我们的数据集（相同数量的列）进行转换，只保留分数高于我们最初在实例化类时定义的阈值的列。'
- en: 'In our case, after having the resamples run, and it may take some time, we
    can try to figure out how many variables have been retained by the model:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，在完成重采样后，这可能需要一些时间，我们可以尝试找出模型保留了多少变量：
- en: '[PRE39]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Here, 19 variables constitute a small set, which can be expanded into four-way
    interactions of the type `var1 × var2 × var3 × var4`, allowing us to better map
    the unknown transformations at the origin of the Madelon dataset.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，19个变量构成一个小集合，可以扩展为四变量交互类型`var1 × var2 × var3 × var4`，这使我们能够更好地映射Madelon数据集起源处的未知转换。
- en: '[PRE40]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: A final test on the obtained probability estimates reveals to us that we reached
    an AUC value of `0.885`, a fairly good improvement from the initial `0.602` baseline.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 对获得的概率估计进行最终测试揭示给我们，我们已经达到了`0.885`的AUC值，这是一个相当好的从初始`0.602`基线上的改进。
- en: Summary
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: During this chapter, we have covered quite a lot of ground, finally exploring
    the most experimental and scientific part of the task of modeling linear regression
    or classification models.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们覆盖了相当多的内容，最终探索了建模线性回归或分类模型任务中最实验性和科学的部分。
- en: Starting with the topic of generalization, we explained what can go wrong in
    a model and why it is always important to check the true performances of your
    work by train/test splits and by bootstraps and cross-validation (though we recommend
    using the latter more for validation work than general evaluation itself).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 从泛化的主题开始，我们解释了模型可能出错的地方以及为什么总是很重要通过训练/测试分割、通过自助法和交叉验证来检查您工作的真实性能（尽管我们建议更多地使用后者进行验证工作而不是一般评估本身）。
- en: Model complexity as a source of variance in the estimate gave us the occasion
    to introduce variable selection, first by greedy selection of features, univariate
    or multivariate, then using regularization techniques, such as Ridge, Lasso and
    Elastic Net.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 模型复杂度作为估计中方差的一个来源，给了我们引入变量选择的机会，首先是通过贪婪选择特征，无论是单变量还是多变量，然后使用正则化技术，如岭回归、Lasso和弹性网络。
- en: Finally, we demonstrated a powerful application of Lasso, called stability selection,
    which, in the light of our experience, we recommend you try for many feature selection
    problems.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们展示了Lasso的一个强大应用，称为稳定性选择，根据我们的经验，我们推荐您在许多特征选择问题中尝试这种方法。
- en: In the next chapter, we will deal with the problem of incrementally growing
    datasets, proposing solutions that may work well even if your problem is that
    of datasets too large to easily and timely fit into the memory of your working
    computer.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将处理增量增长数据集的问题，提出即使您的数据集太大，难以及时装入工作计算机内存的问题，也可能有效的解决方案。
