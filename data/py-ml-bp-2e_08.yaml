- en: Classifying Images with Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积神经网络进行图像分类
- en: In this chapter, we're going to explore the vast and awesome world of computer
    vision.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将探索计算机视觉的广阔而精彩的世界。
- en: If you've ever wanted to construct a predictive machine learning model using
    image data, this chapter will serve as an easily-digestible and practical resource.
    We'll go step by step through building an image-classification model, cross-validating
    it, and then building it in a better way. At the end of this chapter, we'll have
    a *darn good* model and discuss some paths for future enhancement.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经想过使用图像数据构建一个预测性的机器学习模型，本章将作为一个易于消化且实用的资源。我们将一步一步地构建一个图像分类模型，对其进行交叉验证，然后以更好的方式构建它。在本章的结尾，我们将有一个*相当不错*的模型，并讨论一些未来增强的路径。
- en: Of course, some background in the fundamentals of predictive modeling will help
    this to go smoothly. As you'll soon see, the process of converting images into
    usable features for our model might might feel new, but once our features are
    extracted, the model-building and cross-validation processes are exactly the same.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，了解一些预测建模的基础将有助于让这一过程顺利进行。正如你很快就会看到的那样，将图像转换为可用于我们模型的特征的过程可能会让人觉得是新鲜的，但一旦提取了特征，模型构建和交叉验证的过程就完全一样。
- en: In this chapter, we're going to build a convolutional neural network to classify
    images of articles of clothing from the Zalando Research dataset—a dataset of
    70,000 images, each depicting 1 of 10 possible articles of clothing such as T-shirt/top,
    a pair of pants, a sweater, a dress, a coat, a sandal, a shirt, a sneaker, a bag,
    or an ankle boot. But first, we'll explore some of the fundamentals together,
    starting with image-feature extraction and walking through how convolutional neural
    networks work.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建一个卷积神经网络，用于对 Zalando Research 数据集中的服装图像进行分类——该数据集包含 70,000 张图像，每张图像展示了
    10 种可能的服装类别之一，例如 T 恤/上衣、裤子、毛衣、连衣裙、外套、凉鞋、衬衫、运动鞋、包或短靴。但首先，我们将一起探索一些基础知识，从图像特征提取开始，并逐步了解卷积神经网络的工作原理。
- en: So, let's get started. Seriously!.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始吧。真的！
- en: 'Here''s what we''ll cover in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Image-feature extraction
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像特征提取
- en: 'Convolutional neural networks:'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络：
- en: Network topology
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络拓扑
- en: Convolutional layers and filters
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层与滤波器
- en: Max pooling layers
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大池化层
- en: Flattening
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展平
- en: Fully-connected layers and output
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层与输出
- en: Building a convolutional neural network to classify images in the Zalando Research
    dataset, using Keras
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras 构建卷积神经网络来对 Zalando Research 数据集中的图像进行分类
- en: Image-feature extraction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像特征提取
- en: When dealing with unstructured data, be it text or images, we must first convert
    the data into a numerical representation that's usable by our machine learning
    model. The process of converting data that is non-numeric into a numerical representation
    is called **feature extraction**. For image data, our features are the pixel values
    of the image.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理非结构化数据时，无论是文本还是图像，我们必须首先将数据转换为机器学习模型可以使用的数字表示。将非数字数据转换为数字表示的过程称为**特征提取**。对于图像数据来说，我们的特征就是图像的像素值。
- en: First, let's imagine a 1,150 x 1,150 pixel grayscale image. A 1,150 x 1,150
    pixel image will return a 1,150 x 1,150 matrix of pixel intensities. For grayscale
    images, the pixel values can range from 0 to 255, with 0 being a completely black
    pixel, and 255 being a completely white pixel, and shades of gray in between.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，假设我们有一张 1,150 x 1,150 像素的灰度图像。这样的一张图像将返回一个 1,150 x 1,150 的像素强度矩阵。对于灰度图像，像素值的范围是从
    0 到 255，其中 0 代表完全黑色的像素，255 代表完全白色的像素，而 0 到 255 之间的值则代表不同的灰色阴影。
- en: To demonstrate what this looks like in code, let's extract the features from
    our grayscale cat burrito. The image is available on GitHub at [https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08](https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08)
    as `grayscale_cat_burrito.jpg`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这在代码中的表现，我们来提取我们灰度猫卷饼图像的特征。该图像可以在 GitHub 上找到，链接是 [https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08](https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08)，文件名是`grayscale_cat_burrito.jpg`。
- en: I've made the image assets used throughout this chapter available to you at [https://github.com/mroman09/packt-image-assets](https://github.com/mroman09/packt-image-assets).
    You can find our cat burritos there!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经将本章中使用的图像资源提供给你，链接为[https://github.com/mroman09/packt-image-assets](https://github.com/mroman09/packt-image-assets)。你可以在那里找到我们的猫肉卷！
- en: 'Let''s now take a look at a sample of this in the following code:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来看一下以下代码中的一个示例：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you're unable to read a `.jpg` by running the preceding code, just install
    `PIL` by running `pip install pillow`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你无法通过运行前面的代码读取`.jpg`文件，只需运行`pip install pillow`安装`PIL`。
- en: In the preceding code, we imported `pandas` and two submodules: `image` and `pyplot`,
    from `matplotlib`. We used the `imread` method from `matplotlib.image` to read-in
    the image.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们导入了`pandas`和两个子模块：`image`和`pyplot`，来自`matplotlib`。我们使用了`matplotlib.image`中的`imread`方法来读取图像。
- en: 'Running the preceding code gives us the following output:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码会得到以下输出：
- en: '![](img/cd46ced0-2f96-410d-9801-0f7aa0f1fb90.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd46ced0-2f96-410d-9801-0f7aa0f1fb90.png)'
- en: The output is a two-dimensional `numpy` ndarray that contains the features of
    our model. As with most applied machine learning applications, there are several
    preprocessing steps you'll want to perform on these extracted features, some of
    which we'll explore together on the Zalando fashion dataset later in this chapter,
    but these are the raw extracted features of the image!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个二维的`numpy` ndarray，包含了我们模型的特征。像大多数应用机器学习的场景一样，您可能需要对这些提取的特征执行若干预处理步骤，其中一些我们将在本章稍后与Zalando时尚数据集一起探讨，但这些就是图像的原始提取特征！
- en: 'The shape of the extracted features for our grayscale image is `image_height`
    rows x `image_width` columns. We can check the shape easily by running the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的灰度图像特征的形状为`image_height`行 × `image_width`列。我们可以通过运行以下代码轻松检查图像的形状：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding code returns this output:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码返回了以下输出：
- en: '![](img/69f79aee-22b1-481c-b230-2035e84f9d23.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69f79aee-22b1-481c-b230-2035e84f9d23.png)'
- en: 'We can check the maximum and minimum pixel values in our ndarray easily, too:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以轻松检查`ndarray`中的最大和最小像素值：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This returns the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下结果：
- en: '![](img/0eb77b3e-f4b9-483c-9426-4fa4498031ae.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0eb77b3e-f4b9-483c-9426-4fa4498031ae.png)'
- en: 'Finally, we can display our grayscale image from our ndarray by running this
    code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过运行以下代码从`ndarray`中显示灰度图像：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The preceding code returns our image, which is available at [https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08](https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08) as
    `output_grayscale_cat_burrito.png`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码返回了我们的图像，该图像可在[https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08](https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08)上找到，文件名为`output_grayscale_cat_burrito.png`。
- en: The feature-extraction process for color images is identical; however, with
    color images, the shape of our ndarray output will be three-dimensional—a **tensor**—representing
    the **red, green, and blue** (**RGB**) pixel values of our image. Here, we'll
    carry out the same process as before, this time on a color version of the cat
    burrito. The image is available on GitHub at [https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08](https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08) as `color_cat_burrito.jpg`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 彩色图像的特征提取过程是相同的；不过，对于彩色图像，我们的`ndarray`输出的形状将是三维的——一个**张量**——表示图像的**红、绿、蓝**（**RGB**）像素值。在这里，我们将执行与之前相同的过程，这次是在猫肉卷的彩色版本上进行。该图像可在GitHub上通过[https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08](https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08)
    访问，文件名为`color_cat_burrito.jpg`。
- en: 'Let''s extract the features from our color version of the cat burrito by using
    the following code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过以下代码提取猫肉卷的彩色版本特征：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Running this code returns the following output:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码将返回以下输出：
- en: '![](img/642c0bf0-c57c-491e-92e5-b0923f98f55f.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/642c0bf0-c57c-491e-92e5-b0923f98f55f.png)'
- en: Again, here we see that this image contains three channels. Our `color_cat_burrito`
    variable is a tensor that contains three matrices that tell us what the RGB values
    are for each pixel in our image.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在这里我们看到该图像包含三个通道。我们的`color_cat_burrito`变量是一个张量，包含三个矩阵，告诉我们图像中每个像素的RGB值。
- en: 'We can display the color image from our ndarray by running the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下代码来显示`ndarray`中的彩色图像：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This returns our color image. The image is available on GitHub at [https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08](https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08) as `output_color_cat_burrito.png`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回了我们的彩色图像。图像可以在GitHub上找到，链接为[https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08](https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08)，文件名为`output_color_cat_burrito.png`。
- en: This is the first step of our image-feature extraction. We've taken a single
    image at a time and converted those images into numeric values using just a few
    lines of code. In doing so, we saw that extracting features from grayscale images
    produces a two-dimensional ndarray and extracting features from color images produces
    a tensor of pixel-intensity values.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们图像特征提取的第一步。我们一次处理一张图像，并通过几行代码将这些图像转换为数值。通过这一过程，我们看到，从灰度图像中提取特征会产生一个二维的ndarray，而从彩色图像中提取特征会产生一个像素强度值的张量。
- en: 'However, there''s a slight problem. Remember, this is just a single image,
    a single training sample, a single *row* of our data. In the instance of our grayscale
    image, if we were to flatten this matrix into a single row, we would have `image_height` x `image_width`
    columns, or in our case, 1,322,500 columns. We can confirm that in code by running
    the following snippet:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里有一个小问题。记住，这只是一张单独的图像，一条单独的训练样本，一行*数据*。以我们的灰度图像为例，如果我们将这个矩阵展平为一行，我们将拥有`image_height`
    x `image_width`列，或者在我们的例子中，是1,322,500列。我们可以通过运行以下代码片段来确认这一点：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is an issue! As with other machine learning modeling tasks, high dimensionality
    leads to model-performance issues. At this magnitude of dimensionality, any model
    we build will likely overfit, and training times will be slow.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个问题！与其他机器学习建模任务一样，高维度会导致模型性能问题。在如此高维度的情况下，我们构建的任何模型都可能会出现过拟合，且训练时间会很慢。
- en: This dimensionality problem is endemic to computer-vision tasks of this sort.
    Even a dataset of a lower resolution, 400 x 400 pixel grayscale cat burritos,
    would leave us with 160,000 features per image.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个维度问题是这种计算机视觉任务的普遍问题。即便是一个分辨率较低的数据集，比如400 x 400像素的灰度猫卷饼图像，也会导致每张图像有160,000个特征。
- en: 'There is, however, a known solution to this problem: convolutional neural networks.
    In the next section, we''ll continue our feature-extraction process using convolutional
    neural networks to build lower-dimensional representations of these raw image
    pixels. We''ll go over the mechanics of how they work and continue to build an
    idea of why they''re so performant in image-classification tasks.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，解决这个问题的已知方法是：卷积神经网络。在接下来的部分，我们将继续使用卷积神经网络进行特征提取，以构建这些原始图像像素的低维表示。我们将讨论它们的工作原理，并继续了解它们在图像分类任务中为何如此高效。
- en: Convolutional neural networks
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Convolutional neural networks are a class of neural network that resolve the
    high-dimensionality problem we alluded to in the previous section, and, as a result,
    excel at image-classification tasks. It turns out that image pixels in a given
    image region are highly correlated—they tell us similar information about that
    specific image region. Accordingly, using convolutional neural networks, we can
    scan regions of an image and summarize that region in lower-dimensional space.
    As we'll see, these lower-dimensional representations, called **feature maps**,
    tell us many interesting things about the presence of all sorts of shapes—from
    the simplest lines, shadows, loops, and swirls, to very abstract, complex forms
    specific to our data, in our case, cat ears, cat faces, or tortillas—and do this
    in fewer dimensions than the original image.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络是一类解决我们在上一部分提到的高维度问题的神经网络，因此在图像分类任务中表现出色。事实证明，给定图像区域中的图像像素是高度相关的——它们为我们提供了关于该特定图像区域的相似信息。因此，使用卷积神经网络，我们可以扫描图像的区域，并在较低维度的空间中总结该区域。正如我们将看到的，这些低维表示，称为**特征图**，告诉我们关于各种形状存在的许多有趣的事情——从最简单的线条、阴影、环路和漩涡，到非常抽象、复杂的形式，特定于我们的数据，在我们的例子中是猫耳、猫脸或玉米饼——并且在比原始图像更少的维度中完成这一切。
- en: After using convolutional neural networks to extract these lower-dimensional
    features from our images, we'll pass the output of the convolutional neural network
    into a network suitable for the classification or regression task we want to perform.
    In our case, when modeling the Zalando research dataset, the output of our convolutional
    neural network will be passed into a fully-connected neural network for multi-class
    classification.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用卷积神经网络从图像中提取这些低维特征之后，我们将把卷积神经网络的输出传入一个适合进行分类或回归任务的网络。在我们的例子中，当建模 Zalando
    研究数据集时，卷积神经网络的输出将传入一个全连接神经网络，用于多分类。
- en: But how does this work? There are several key components we'll discuss with
    respect to convolutional neural networks on grayscale images, and these are all
    important for building our understanding.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这是怎么运作的呢？我们将讨论几个关键组件，这些组件对于卷积神经网络在灰度图像上的应用非常重要，它们对我们构建理解至关重要。
- en: Network topology
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络拓扑
- en: 'You may have encountered a diagram similar to the aforementioned that depicts
    a convolutional neural network to a feedforward neural network architecture. We''ll
    be building something such as this very soon! But what''s being depicted here?
    Check it out:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能见过类似于上述的图示，它将卷积神经网络与前馈神经网络架构进行了对比。我们很快也会构建一个类似的东西！但这里描绘的是什么呢？看看这个：
- en: '![](img/82007989-4f27-47f2-bd42-69f1e2c7a49c.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82007989-4f27-47f2-bd42-69f1e2c7a49c.png)'
- en: In the preceding diagram, on the very left, we have our input. These are the
    extracted features of our image, the matrix (as was the case with the grayscale
    cat burrito) of values ranging from 0 to 255 that describe the pixel intensities
    present in the image.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，最左边是我们的输入。这些是我们图像的提取特征，是一个值范围从0到255的矩阵（就像灰度猫卷饼的情况一样），描述了图像中像素的强度。
- en: Next, we pass the data through alternating convolutional and max-pooling layers.
    These layers define the convolutional neural network component of the architecture
    depicted. We'll describe what each of these layer types do in the following two
    sections.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据通过交替的卷积层和最大池化层。这些层定义了所描绘架构中的卷积神经网络组件。在接下来的两部分中，我们将描述这些层类型的作用。
- en: After this, we pass the data to a fully-connected layer before arriving at the
    output layer. These two layers describe a fully-connected neural network. You're
    free to use any multi-class classification algorithm you like here, instead of
    a fully-connected neural network—a **logistic regression** or **random forest
    classifier**, perhaps—but for our dataset, we'll be using a fully-connected neural
    network.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将数据传递给一个全连接层，然后到达输出层。这两层描述了一个全连接神经网络。你可以在这里使用任何你喜欢的多分类算法，而不是全连接神经网络——例如**逻辑回归**或**随机森林分类器**——但对于我们的数据集，我们将使用全连接神经网络。
- en: 'The output layer depicted is the same as for any other multi-class classifier.
    Sticking with our cat burrito example, let''s suppose we were building a model
    to predict what kind of cat burrito an image was from five distinct classes: chicken
    cat burrito, steak cat burrito, cat burrito al pastor, vegetarian cat burrito,
    or fish cat burrito (I''ll let you use your imagination to visualize what our
    training data might look like). The output layer would be the predicted probability
    that the image belonged to one of the five classes, with `max(probability)` indicating
    what our model believes to be the most likely class.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 所描绘的输出层与其他任何多分类分类器相同。以我们的猫卷饼示例为例，假设我们正在构建一个模型，用来预测图像属于五种不同类别中的哪一种：鸡肉猫卷饼、牛排猫卷饼、牧羊猫卷饼、素食猫卷饼，或者鱼类猫卷饼（我让你发挥想象，想象我们的训练数据可能是什么样子）。输出层将是图像属于五个类别之一的预测概率，`max(probability)`
    表示我们模型认为最有可能的类别。
- en: 'At a high level, we''ve walked through the architecture, or **topology** of
    the preceding network. We''ve discussed our input versus the convolutional neural
    network component versus the fully-connected neural network component of the preceding
    topology. Let''s dig just a bit deeper now and add some concepts that allow us
    to describe the topology in more detail:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次上看，我们已经介绍了前面网络的架构或**拓扑**。我们讨论了输入与卷积神经网络组件以及前面拓扑中的全连接神经网络组件之间的关系。现在让我们更深入一点，加入一些概念，帮助我们更详细地描述拓扑：
- en: How many convolutional layers does the network have? Two.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络有多少个卷积层？两个。
- en: And in each convolutional layer, how many feature maps are there? There are
    seven in convolutional layer 1 and 12 in convolutional layer 2.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那么在每个卷积层中，有多少个特征图？卷积层1有7个，卷积层2有12个。
- en: How many pooling layers does the network have? Two.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络有多少个池化层？两个。
- en: How many fully-connected layers are there? One.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全连接层有多少层？一层。
- en: How many **neurons** are in the fully-connected layer? 10.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全连接层中有多少个**神经元**？10个。
- en: What is the output? Five.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出是多少？五。
- en: The modeler's decision to use two convolutional layers versus any other number
    or just a single fully-connected layer versus any other number should be thought
    of as the **hyperparameters** of the model. That is, it's something that we, as
    the modelers, should experiment with and cross-validate but not a parameter our
    model is explicitly learning and optimizing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 模型者决定使用两个卷积层而不是其他数量的层，或使用单个完全连接层而不是其他层数，应该被视为模型的**超参数**。也就是说，这些是我们作为模型开发者应该进行实验和交叉验证的内容，而不是模型显式学习和优化的参数。
- en: There are other useful things you can infer about the problem you're solving
    just by looking at the network's topology. As we discussed, the fact that our
    network's output layer contains five nodes lets us know that this neural network
    was designed to solve a multi-class classification task for which there are five
    classes. If it were a regression or a binary classification problem, our network's
    architecture would (in most cases) have a single output node. We also know that
    the modeler used seven filters in the first convolutional layer and 12 kernels
    in the second convolutional layer because of the number of feature maps resulting
    from each layer (we'll discuss what these kernels are in some more detail in the
    next section).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过查看网络的拓扑结构，你可以推断出一些关于你正在解决的问题的有用信息。正如我们讨论的那样，我们的网络输出层包含五个节点，这表明该神经网络是为了解决一个有五个类别的多类别分类任务。如果它是回归问题或二元分类问题，我们的网络架构通常（在大多数情况下）会只有一个输出节点。我们还知道，模型者在第一个卷积层使用了七个滤波器，在第二个卷积层使用了12个内核，因为每一层产生的特征图数量（我们将在下一节详细讨论这些内核是什么）。
- en: Great! We learned some useful jargon that will help us describe our networks
    and build our conceptual understanding of how they work. Now let's explore the
    convolutional layers of our architecture.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们学到了一些有用的术语，它们将帮助我们描述我们的网络，并构建对网络如何工作的概念理解。现在让我们探索一下我们架构中的卷积层。
- en: Convolutional layers and filters
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层和滤波器
- en: 'Convolutional layers and filters are at the heart of convolutional neural networks.
    In these layers, we slide a filter (also referred to in this text as a **window**
    or **kernel**) over our ndarray feature and take the inner product at each step.
    Convolving our ndarray and kernel in this way results in a lower-dimensional image
    representation. Let''s explore how this works on this grayscale image (available
    in image-assets repository):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层和滤波器是卷积神经网络的核心。在这些层中，我们将一个滤波器（在本文中也称为**窗口**或**内核**）滑动过我们的ndarray特征，并在每一步进行内积运算。以这种方式对ndarray和内核进行卷积，最终得到一个低维的图像表示。让我们看看在这张灰度图像上是如何工作的（可在image-assets库中找到）：
- en: '![](img/cfd2a688-480d-4f87-82e0-cd686e4bdf3c.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfd2a688-480d-4f87-82e0-cd686e4bdf3c.png)'
- en: The preceding image is a 5 x 5 pixel grayscale image shows a black diagonal
    line against a white background.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 上图是一个5 x 5像素的灰度图像，显示了一个黑色的对角线，背景是白色的。
- en: 'Extracting the features from the following diagram, we get the following matrix
    of pixel intensities:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下图示中提取特征后，我们得到如下的像素强度矩阵：
- en: '![](img/f0fe030e-ccaa-4d16-9edb-32c7012b9689.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0fe030e-ccaa-4d16-9edb-32c7012b9689.png)'
- en: 'Next, let''s assume we (or Keras) instantiate the following kernel:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，假设我们（或Keras）实例化了以下内核：
- en: '![](img/d0f28169-8372-4341-954f-3867f5b576af.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0f28169-8372-4341-954f-3867f5b576af.png)'
- en: We'll now visualize the convolution process. The movement of the window starts
    from the top, left of our image matrix. We'll slide the window right by a predetermined
    stride size. In this case, our stride size will be 1, but in general the stride
    size should be considered another hyperparameter of your model. Once the window
    reaches the rightmost edge of the image, we'll slide our window down by 1 (our
    stride size), move the window back to the leftmost edge of the image, and start
    the process of taking the inner product again.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将可视化卷积过程。窗口的移动从图像矩阵的左上角开始。我们将窗口向右滑动一个预定的步长。在这种情况下，步长为 1，但通常步长大小应该视为模型的另一个超参数。一旦窗口到达图像的最右边缘，我们将窗口向下滑动
    1（即步长大小），然后将窗口移回到图像的最左边，重新开始内积计算的过程。
- en: 'Now let''s do this step by step:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们一步一步来做：
- en: 'Slide the kernel over the top-left part of the matrix and calculate the inner
    product:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将内核滑过矩阵的左上部分并计算内积：
- en: '![](img/16616ed8-ac7b-4479-84a4-013bb46bb2c5.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16616ed8-ac7b-4479-84a4-013bb46bb2c5.png)'
- en: 'I''ll explicitly map out the inner product for this first step so that you
    can easily follow along:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我将显式地展示第一步的内积计算，以便你能轻松跟上：
- en: '`(0x0)+(255x0)+(255x0)+(255x0)+(0x1)+(255x0)+(255x0)+(255x0)+(0x0) = 0`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`(0x0)+(255x0)+(255x0)+(255x0)+(0x1)+(255x0)+(255x0)+(255x0)+(0x0) = 0`'
- en: We write the result to our feature map and continue!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将结果写入特征图并继续！
- en: 'Take the inner product and write the result to our feature map:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算内积并将结果写入我们的特征图：
- en: '![](img/6a00c530-8e36-4547-9038-4cfadbd29e61.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a00c530-8e36-4547-9038-4cfadbd29e61.png)'
- en: 'Step 3:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第 3 步：
- en: '![](img/65e5b537-dd45-4afd-a1e5-6f1679b47ddb.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65e5b537-dd45-4afd-a1e5-6f1679b47ddb.png)'
- en: 'We''ve reached the rightmost edge of the image. Slide the window down by 1,
    our stride size, and start the process again at the leftmost edge of the image:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经到达图像的最右边缘。将窗口向下滑动 1 个单位，即我们的步长大小，然后从图像的最左边开始重新开始这个过程：
- en: '![](img/a19790b1-455f-4a34-96f0-aeb86d966c6a.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a19790b1-455f-4a34-96f0-aeb86d966c6a.png)'
- en: 'Step 5:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第 5 步：
- en: '![](img/1d16c2c3-909d-4a32-b221-8111f72cb5c9.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d16c2c3-909d-4a32-b221-8111f72cb5c9.png)'
- en: 'Step 6:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第 6 步：
- en: '![](img/bb8ce393-a48b-40c5-86cf-e7d5cac709a1.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb8ce393-a48b-40c5-86cf-e7d5cac709a1.png)'
- en: 'Step 7:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第 7 步：
- en: '![](img/6c75d988-2688-4c58-a16e-ba6a3b23da98.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c75d988-2688-4c58-a16e-ba6a3b23da98.png)'
- en: 'Step 8:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第 8 步：
- en: '![](img/b403905a-5501-4067-92ab-e41a221b564d.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b403905a-5501-4067-92ab-e41a221b564d.png)'
- en: 'Step 9:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第 9 步：
- en: '![](img/71dd0a5d-47a9-4e7d-804e-f1e833273591.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/71dd0a5d-47a9-4e7d-804e-f1e833273591.png)'
- en: 'Voila! We''ve now represented our original 5 x 5 image in a 3 x 3 matrix (our
    feature map). In this toy example, we''ve been able to reduce the dimensionality
    from 25 features down to just 9\. Let''s take a look at the image that results
    from this operation:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 看！我们已经将原始的 5 x 5 图像表示为 3 x 3 的矩阵（我们的特征图）。在这个简单的示例中，我们已经将维度从 25 个特征减少到只有 9 个特征。让我们看看这个操作后的结果图像：
- en: '![](img/9dac7706-ca00-4455-8cb1-6fa21d61e9f2.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9dac7706-ca00-4455-8cb1-6fa21d61e9f2.png)'
- en: If you're thinking that this looks exactly like our original black diagonal
    line but smaller, you're right. The values the kernel takes determine what's being
    identified, and in this specific example, we used what's called an **identity
    kernel**. Kernels taking other values will return other properties of the image—detecting
    the presence of lines, edges, outlines, areas of high contrast, and more.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得这看起来和我们原始的黑色对角线图像一样，只是变小了，你是对的。内核的取值决定了识别到的内容，在这个具体示例中，我们使用了所谓的**单位矩阵内核**。如果使用其他值的内核，它将返回图像的其他特征——例如检测线条、边缘、轮廓、高对比度区域等。
- en: We'll apply multiple kernels to the image, simultaneously, at each convolutional
    layer. The number of kernels used is up to the modeler—another hyperparameter.
    Ideally, you want to use as few as possible while still achieving acceptable cross-validation
    results. The simpler the better! However, depending on the complexity of the task,
    we may see performance gains by using more. The same thinking can can be applied
    when tuning the other hyperparameters of the model, such as the number of layers
    in the network or the number of neurons per layer. We're trading simplicity for
    complexity, and generalizability and speed for detail and precision.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在每个卷积层同时应用多个内核对图像进行处理。使用的内核数量由模型设计者决定——这是另一个超参数。理想情况下，你希望在实现可接受的交叉验证结果的同时，尽可能使用最少的内核。越简单越好！然而，根据任务的复杂性，使用更多内核可能会带来性能提升。相同的思路也适用于调节模型的其他超参数，比如网络中的层数或每层的神经元数。我们在追求简洁与复杂性之间做出权衡，同时在通用性、速度、细节和精度之间进行选择。
- en: While the number of kernels is our choice, the values that each kernel takes
    is a parameter of our model, which is learned from our training data and optimized
    during training in a manner that reduces the cost function.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 核心数量是我们的选择，而每个核心的取值是我们模型的一个参数，这个参数是通过训练数据学习得到的，并在训练过程中通过优化减少成本函数来调整。
- en: 'We''ve seen the step-by-step process of how to convolve a filter with our image
    features to create a single feature map. But what happens when we apply multiple
    kernels simultaneously? And how do these feature maps pass through each layer
    of the network? Lets have a look at the following screenshot:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何一步步将过滤器与图像特征进行卷积，以创建单一的特征图。那么，当我们同时应用多个核时会发生什么呢？这些特征图如何通过网络的每一层传递？让我们看看以下截图：
- en: '![](img/5550bd45-05ec-4fa9-9611-e5e383a9564c.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5550bd45-05ec-4fa9-9611-e5e383a9564c.png)'
- en: 'Image source: Lee et al., Convolutional Deep Belief Networks for Scalable Unsupervised
    Learning of Hierarchical Representations, via stack exchange. Source text here:
    https://ai.stanford.edu/~ang/papers/icml09-ConvolutionalDeepBeliefNetworks.pdf'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Lee 等人，《卷积深度信念网络用于可扩展的无监督学习层次表示》，来自 Stack Exchange。源文本请参见：https://ai.stanford.edu/~ang/papers/icml09-ConvolutionalDeepBeliefNetworks.pdf
- en: The preceding screenshot visualizes the feature maps generated at each convolutional
    layer of a network trained on images of faces. In the early layers of the network
    (at the very bottom), we detect the presence of simple visual structures—simple
    lines and edges. We did this with our identity kernel! The output of this first
    layer gets passed on to the next layer (the middle row), which combines these
    simple shapes into abstract forms. We see here that the combination of edges build
    the components of a face—eyes, noses, ears, mouths, and eyebrows. The output of
    this middle layer, in turn, gets passed to a final layer, which combines the combination
    of edges into complete objects—in this case, different people's faces.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的截图展示了一个训练过面孔图像的网络在每个卷积层生成的特征图。在网络的早期层（最底部），我们检测到简单的视觉结构——简单的线条和边缘。我们是通过使用我们的身份核来做到这一点的！这一层的输出会传递到下一层（中间一行），该层将这些简单的形状组合成抽象的形式。我们在这里看到，边缘的组合构建了面部的组成部分——眼睛、鼻子、耳朵、嘴巴和眉毛。中间层的输出又会传递到最终层，该层将边缘的组合合成完整的物体——在这种情况下，是不同人的面孔。
- en: 'One particularly powerful property of this entire process is that all of these
    features and representations are learned from the data. At no point do we explicitly
    tell our model: *Model, for this task, I''d like to use an identity kernel and
    a bottom sobel kernel in the first convolutional layer because I think these two
    kernels will extract the most signal-rich feature maps*. Once we''ve set the hyperparameter
    for the number of kernels we want to use, the model learns through optimization
    what lines, edges, shadows, and complex combinations thereof are best suited to
    determine what a face is or isn''t. The model performs this optimization with
    no domain-specific, hardcoded rules about what faces, cat burritos, or clothes
    are.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这个整个过程的一个特别强大的特性是，所有这些特征和表示都是从数据中学习出来的。在任何时候，我们都不会明确地告诉我们的模型：*模型，对于这个任务，我想在第一个卷积层使用一个身份核和一个底部
    Sobel 核，因为我认为这两个核将提取出最丰富的特征图*。一旦我们设置了要使用的核数量的超参数，模型通过优化学习到哪些线条、边缘、阴影及其复杂组合最适合判断什么是面孔，什么不是。模型进行这种优化时，并没有使用任何关于面孔、猫卷饼或衣服的领域特定的硬编码规则。
- en: There are many other fascinating properties of convolutional neural networks,
    which we won't cover in this chapter. However, we did explore the fundamentals,
    and hopefully you have a sense of the importance of using convolutional neural
    networks to extract highly expressive, signal-rich, low-dimensional features.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络还有许多其他迷人的特性，这些我们在本章中不再讨论。然而，我们确实探讨了其基础知识，并且希望你能感受到使用卷积神经网络来提取高表达性、信号丰富、低维度特征的重要性。
- en: Next, we'll discuss *Max pooling layers*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论*最大池化层*。
- en: Max pooling layers
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大池化层
- en: 'We''ve discussed the importance of reducing our dimensional space and how we
    use convolutional layers to achieve this. We use max pooling layers for the same
    reason—to further reduce dimensionality. Quite intuitively, as the name suggests,
    with max pooling, we slide a window over our feature map and take the max value
    for the window. Let''s return to the feature map from our diagonal-line example
    to illustrate, this as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了减少维度空间的重要性，以及如何使用卷积层来实现这一点。我们使用最大池化层有同样的原因——进一步减少维度。很直观地说，正如名字所示，最大池化是我们将一个窗口滑动到特征图上，并取该窗口的最大值。让我们回到我们对角线示例中的特征图来说明这一点，如下所示：
- en: '![](img/8c08fee6-0ca0-4ec3-85ef-5a8894ca291d.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c08fee6-0ca0-4ec3-85ef-5a8894ca291d.png)'
- en: 'Let''s see what happens when we max pool the preceding feature map using a
    2 x 2 window. Again, all we''re doing here is returning `max(values in window)`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当我们使用2 x 2窗口进行最大池化时，前面的特征图会发生什么。再说一遍，我们这里只是返回`max(窗口中的值)`：
- en: 'Return `max(0,255,255,0)`, which gets us 255:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回`max(0,255,255,0)`，结果是255：
- en: '![](img/6c75d908-819d-450b-ad34-eb16754aca77.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c75d908-819d-450b-ad34-eb16754aca77.png)'
- en: 'Step 2:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二步：
- en: '![](img/cfa2a066-bab6-4b25-bc96-c98f85bda1e5.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfa2a066-bab6-4b25-bc96-c98f85bda1e5.png)'
- en: 'Step 3:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三步：
- en: '![](img/38d64d16-026a-4c48-9afd-413a09c42d45.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/38d64d16-026a-4c48-9afd-413a09c42d45.png)'
- en: 'Step 4:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第四步：
- en: '![](img/32f83e69-0ce7-41c5-971b-06e015cd1e3a.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32f83e69-0ce7-41c5-971b-06e015cd1e3a.png)'
- en: By max pooling our feature map with a 2 x 2 window, we've knocked a column and
    a row off, getting us from a 3 x 3 representation to a 2 x 2—Not bad!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用2 x 2窗口进行最大池化，我们去掉了一列和一行，将表示从3 x 3变成了2 x 2——不错吧！
- en: There are other forms of pooling as well—average pooling and min pooling, for
    example; however, you'll see max pooling used most often.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他形式的池化，比如平均池化和最小池化；然而，你会发现最大池化是最常用的。
- en: Next, we'll discuss flattening, a step we'll perform to turn our max-pooled
    feature map into the right shape for modeling.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论展平，这是一个步骤，我们将执行此操作，将我们的最大池化特征图转换为适合建模的形状。
- en: Flattening
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 展平
- en: So far, we've focused on building as condensed and expressive a representation
    of our features as possible and used convolutional neural networks and max pooling
    layers to do this. The last step of our transformation is to flatten our convolved
    and max-pooled ndarray, in our example a 2 x 2 matrix, into a single row of training
    data.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们专注于尽可能构建一个紧凑且富有表现力的特征表示，并通过卷积神经网络和最大池化层来实现这一目标。我们转换的最后一步是将我们的卷积和最大池化后的ndarray（在我们的示例中是一个2
    x 2的矩阵）展平为一行训练数据。
- en: 'Our max-pooled diagonal black line example would look something like the following,
    in code:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最大池化对角线黑线示例在代码中看起来像下面这样：
- en: '[PRE7]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Running this code returns the following output:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码将返回以下输出：
- en: '![](img/43d89914-dd37-48cd-af87-1bc40ec8cce9.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43d89914-dd37-48cd-af87-1bc40ec8cce9.png)'
- en: 'We can check the shape here by running the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下代码来检查形状：
- en: '[PRE8]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This returns this output:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这会返回以下输出：
- en: '![](img/9325c607-8ce4-4e88-bdcf-e6355d65483f.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9325c607-8ce4-4e88-bdcf-e6355d65483f.png)'
- en: 'To turn this matrix into a single training sample, we just run `flatten()`.
    Let''s do this and look at the shape of our flattened matrix:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这个矩阵转换为单个训练样本，我们只需要运行`flatten()`。让我们来做这个，并查看我们展平后的矩阵的形状：
- en: '[PRE9]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This generates the following output:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成以下输出：
- en: '![](img/e63af8b4-430b-47df-a61b-c6fc09bce587.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e63af8b4-430b-47df-a61b-c6fc09bce587.png)'
- en: What started as a 5 x 5 matrix of pixel intensities is now a single row with
    four features. We can now pass this into a fully-connected neural network.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最初是一个5 x 5的像素强度矩阵，现在变成了一个包含四个特征的单行数据。我们现在可以将其传入一个全连接的神经网络。
- en: Fully-connected layers and output
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全连接层和输出
- en: The fully-connected layers are where we map our input—the rows resulting from
    us convolving, max-pooling, and flattening our original extracted features—to
    our target class or classes. Here, each input is connected to every **neuron**
    or **node** in the following layer. The strength of these connections, or **weights**,
    and a **bias** term present in each node of the network are parameters of the
    model, optimized throughout the training process to minimize an objective function.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层是我们将输入——通过卷积、最大池化和展平操作得到的行——映射到目标类别或类别的地方。在这里，每个输入都与下一层中的每个**神经元**或**节点**相连接。这些连接的强度，或称为**权重**，以及每个节点中存在的**偏置**项，是模型的参数，这些参数在整个训练过程中不断优化，以最小化目标函数。
- en: 'The final layer of our model will be our output layer, which gives us our model
    predictions. The number of neurons in our output layer and the **activation function**
    we apply to it are determined by the kind of problem we''re trying to solve: regression,
    binary classification, or multi-class classification. We''ll see exactly how to
    set up the fully-connected and output layers for a multi-class classification
    task when we start working with the Zalando Research fashion dataset in the next
    section.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的最后一层将是输出层，它给出我们的模型预测结果。输出层中神经元的数量以及我们应用的**激活函数**由我们要解决的问题类型决定：回归、二分类或多分类。当我们在下一节开始使用Zalando
    Research的时尚数据集时，我们将看到如何为多分类任务设置全连接层和输出层。
- en: The fully-connected layers and output—that is, the feedforward neural network
    component of our architecture—belong to a distinct neural network type from the
    convolutional neural networks we discussed in this section. We briefly described
    how feedforward networks work in this section only to provide color on how the
    classifier component of our architecture works. You can always substitute this
    portion of the architecture for a classifier you are more familiar with, such
    as a **logit**!
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层和输出层——即我们架构中的前馈神经网络组件——属于一种与我们在本节中讨论的卷积神经网络不同的神经网络类型。我们在本节中简要描述了前馈网络的工作原理，目的是为了帮助理解我们架构中的分类器组件如何工作。你可以随时将这一部分架构替换为你更熟悉的分类器，例如**logit**！
- en: With this fundamental knowledge, you're now ready to build your network!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些基础知识，你现在可以开始构建你的网络了！
- en: Building a convolutional neural network to classify images in the Zalando Research
    dataset, using Keras
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras构建卷积神经网络，分类Zalando Research数据集中的图像
- en: In this section, we'll be building our convolutional neural network to classify
    images of clothing, using Zalando Research's fashion dataset. The repository for
    this dataset is available at [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建卷积神经网络来分类Zalando Research的服装图片，使用该公司的时尚数据集。该数据集的仓库可以在[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)找到。
- en: 'This dataset contains 70,000 grayscale images—each depicting an article of
    clothing—from 10 possible clothing articles. Specifically, the target classes
    are as follows: T-shirt/top, pants, sweater, dress, coat, sandal, shirt, sneaker,
    bag, and ankle boot.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含70,000张灰度图像——每张图像展示了一种服装——这些服装来自10种可能的服装类型。具体而言，目标类别如下：T恤/上衣、裤子、毛衣、连衣裙、外套、凉鞋、衬衫、运动鞋、包和踝靴。
- en: Zalando, a Germany-based e-commerce company, released this dataset to provide
    researchers with an alternative to the classic MNIST dataset of handwritten digits.
    Additionally, this dataset, which they call **Fashion MNIST**, is a bit more challenging
    to predict excellently—the MNIST handwritten-digits dataset can be predicted with
    99.7% accuracy without the need for extensive preprocessing or particularly deep
    neural networks.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Zalando是一家总部位于德国的电子商务公司，发布了这个数据集，以为研究人员提供经典手写数字MNIST数据集的替代方案。此外，这个数据集，他们称之为**Fashion
    MNIST**，在预测准确性上稍有挑战——MNIST手写数字数据集可以在没有大量预处理或特别深度神经网络的情况下以99.7%的准确率进行预测。
- en: 'So, let''s get started! Follow these steps:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们开始吧！请按照以下步骤操作：
- en: 'Clone the repository to our desktop. From the terminal, run the following:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将仓库克隆到我们的桌面。在终端中运行以下命令：
- en: '[PRE10]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If you haven't done so already, please install Keras by running `pip install
    keras` from the command line. We'll also need to install TensorFlow. To do this,
    run `pip install tensorflow` from the command line.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有安装Keras，请通过命令行运行`pip install keras`进行安装。我们还需要安装TensorFlow。为此，请在命令行中运行`pip
    install tensorflow`。
- en: 'Import the libraries we''ll be using:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入我们将要使用的库：
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Many of these libraries should look familiar by now. However, for some of you,
    this may be your first time using Keras. Keras is a popular Python deep learning
    library. It's a wrapper that can run on top of machine learning frameworks such
    as TensorFlow, CNTK, or Theano.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这些库中的许多应该已经很熟悉了。然而，对于你们中的一些人来说，这可能是第一次使用Keras。Keras是一个流行的Python深度学习库。它是一个可以运行在TensorFlow、CNTK或Theano等机器学习框架之上的封装库。
- en: For our project, Keras will be running TensorFlow under the hood. Using TensorFlow
    directly would allow us more explicit control of the behavior of our networks;
    however, because TensorFlow uses dataflow graphs to represent its operations,
    this can take some getting used to. Luckily for us, Keras abstracts a lot of this
    away and its API is a breeze to learn for those comfortable with `sklearn`.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的项目，Keras将在后台运行TensorFlow。直接使用TensorFlow可以让我们更明确地控制网络的行为；然而，由于TensorFlow使用数据流图来表示其操作，因此这可能需要一些时间来适应。幸运的是，Keras抽象了很多内容，它的API对于熟悉`sklearn`的人来说非常容易学习。
- en: The only other library that may be new to some of you here will be the **Python
    Imaging Library** (**PIL**). PIL provides certain image-manipulation functionalities.
    We'll use it to visualize our Keras network's topology.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能对你们中的一些人来说是新的库是**Python Imaging Library**（**PIL**）。PIL提供了一些图像处理功能。我们将使用它来可视化我们Keras网络的拓扑结构。
- en: 'Load in the data. Zalando has provided us with a helper script that does the
    loading in for us. We just have to make sure that `fashion-mnist/utils/` is in
    our path:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据。Zalando为我们提供了一个辅助脚本，帮助我们进行数据加载。我们只需要确保`fashion-mnist/utils/`在我们的路径中：
- en: '[PRE12]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Load in the data using the helper script:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用辅助脚本加载数据：
- en: '[PRE13]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Take a look at the shapes of `X_train`, `X_test`, `y_train`, and `y_test`:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看一下`X_train`、`X_test`、`y_train`和`y_test`的形状：
- en: '[PRE14]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Running that code gives us the following output:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码会给出以下输出：
- en: '![](img/a8500b89-ac19-4d4c-b8b6-8506b8303741.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8500b89-ac19-4d4c-b8b6-8506b8303741.png)'
- en: 'Here, we can see our training set contains 60,000 images and our test contains
    10,000 images. Each image is currently a vector of values 784 that are elements
    long. Let''s now check the data types:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们的训练集包含60,000张图像，测试集包含10,000张图像。每张图像当前是一个长度为784的值的向量。现在，让我们检查一下数据类型：
- en: '[PRE15]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This returns the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下内容：
- en: '![](img/ec3a5302-7c97-4748-8bae-72a155ad21db.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec3a5302-7c97-4748-8bae-72a155ad21db.png)'
- en: 'Next, let''s see what the data looks like. Remember, in its current form, each
    image is a vector of values. We know the images are grayscale, so to visualize
    each image, we''ll have to reshape these vectors into a 28 x 28 matrix. Let''s
    do this and peek at the first image:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看数据的样子。记住，在当前形式下，每张图像是一个值的向量。我们知道这些图像是灰度图，所以为了可视化每张图像，我们必须将这些向量重新构造成一个28
    x 28的矩阵。我们来做一下这个，并瞥一眼第一张图像：
- en: '[PRE16]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This generates the following output:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![](img/02c86512-feb3-4b76-bf86-1b412698621e.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02c86512-feb3-4b76-bf86-1b412698621e.png)'
- en: 'Awesome! We can check to see the class this image belongs to by running the
    following:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们可以通过运行以下代码来查看这张图像所属的类别：
- en: '[PRE17]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This generates the following output:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![](img/03c7a2b5-8520-4a03-9418-10238801bc15.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03c7a2b5-8520-4a03-9418-10238801bc15.png)'
- en: 'The classes are encoded from 0-9\. In the README, Zalando provides us with
    the mapping:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 类别被编码为0-9。在README文件中，Zalando提供了我们需要的映射：
- en: '![](img/e7cf2657-275b-4cfa-9f61-c0dc75214a1b.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7cf2657-275b-4cfa-9f61-c0dc75214a1b.png)'
- en: 'Given this, we now know our first image is of an ankle boot. Sweet! Let''s
    create an explicit mapping of these encoded values to their class names. This
    will come in handy momentarily:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，我们现在知道我们的第一张图像是一个踝靴。太棒了！让我们创建一个明确的映射，将这些编码值与它们的类别名称对应起来。稍后这会很有用：
- en: '[PRE18]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Great. We've seen a single image, but we still need to get a feel for what's
    in our data. What do the images look like? Getting a grasp of this will tell us certain
    things. As an example, I'm interested to see how visually distinct the classes
    are. Classes that look similar to other classes will be harder for a classifier
    to differentiate than classes that are more unique.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 很好。我们已经看到了单张图片，但我们仍然需要了解数据中的内容。图片长什么样子？理解这一点可以告诉我们一些信息。举个例子，我很想看看这些类别在视觉上有多么明显区分。看起来与其他类别相似的类别，比起那些更独特的类别，会更难以被分类器区分。
- en: 'Here, we define a helper function to help us through our visualization journey:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义一个辅助函数，帮助我们完成可视化过程：
- en: '[PRE19]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: What does this function do? It creates a grid of images selected at random from
    the data so that we can view multiple images simultaneously.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数做什么？它从数据中随机选择一组图像，创建一个图像网格，这样我们就能同时查看多张图像。
- en: It takes as arguments the desired number of image rows (`plot_rows`), image
    columns (`plot_columns`), our `X_train` (`feature_array`), and `y_train` (`target_array`)
    and generates a matrix of images that's `plot_rows` x `plot_columns` large. As
    optional arguments, you can specify a `cmap`, or colormap (the default is `‘gray'`
    because these are grayscale images), and a `random_seed`, if replicating the visualization
    is important.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 它的参数包括所需的图像行数（`plot_rows`）、图像列数（`plot_columns`）、我们的`X_train`（`feature_array`）和`y_train`（`target_array`），并生成一个`plot_rows`
    x `plot_columns`大小的图像矩阵。作为可选参数，您可以指定一个`cmap`，即色图（默认值为`‘gray'`，因为这些是灰度图像），以及一个`random_seed`，如果复制可视化很重要的话。
- en: 'Let''s see how to run this, as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何运行，如下所示：
- en: '[PRE20]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This returns the following:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下结果：
- en: '![](img/21924c3b-e0af-4a11-a139-27f612ad96c5.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21924c3b-e0af-4a11-a139-27f612ad96c5.png)'
- en: Visualization output
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化输出
- en: 'Remove the `random_seed` argument and rerun this function several times. Specifically,
    run the following code:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 移除`random_seed`参数，并多次重新运行这个函数。具体来说，运行以下代码：
- en: '[PRE21]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You may have noticed that at this resolution some classes look quite similar
    and others quite distinct. For example, samples of the t-shirt/top target class
    can look very similar to samples from the shirt and coat target classes, whereas
    the sandal target class seems to be quite different than the rest. This is food
    for thought when thinking about where our model may be weak versus where it's
    likely to be strong.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在这个分辨率下，一些类看起来非常相似，而其他一些类则非常不同。例如，t-shirt/top目标类的样本可能看起来与shirt和coat目标类的样本非常相似，而sandal目标类似乎与其他类明显不同。在考虑模型可能的弱点与强项时，这是值得思考的内容。
- en: 'Now let''s take a peek at the distribution of target classes in our dataset.
    Will we have to do any upsampling or downsampling? Let''s check:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看数据集中目标类的分布情况。我们需要做上采样或下采样吗？让我们检查一下：
- en: '[PRE22]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Running the preceding code generates the following plot:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码会生成以下图表：
- en: '![](img/6bc6b269-edaa-4f0c-a68a-980ae4c01d42.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6bc6b269-edaa-4f0c-a68a-980ae4c01d42.png)'
- en: Awesome! No class-balancing to do here.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！这里不需要做类平衡调整。
- en: Next, let's start preprocessing our data to get it ready for modeling.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们开始预处理数据，为建模做好准备。
- en: 'As we discussed in our *Image-feature extraction* section, these grayscale
    images contain pixel values ranging from 0 to 255\. We confirm this by running
    the following code:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*图像特征提取*部分讨论的，这些灰度图像包含从0到255的像素值。我们通过运行以下代码来确认这一点：
- en: '[PRE23]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This returns the following values:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下值：
- en: '![](img/0782a868-cbe5-4b34-a5ed-33ae7d0d6511.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0782a868-cbe5-4b34-a5ed-33ae7d0d6511.png)'
- en: 'For the purposes of modeling, we''re going to want to normalize these values
    on a 0–1 scale. This is a common preprocessing step when preparing image data
    for modeling. Keeping our values in this range will allow our neural network to
    converge more quickly. We can normalize the data by running the following:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建模的目的，我们需要将这些值归一化到0-1的范围内。这是准备图像数据进行建模时的常见预处理步骤。保持这些值在这个范围内可以让我们的神经网络更快收敛。我们可以通过运行以下代码来归一化数据：
- en: '[PRE24]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Our data is now scaled from 0.0 to 1.0\. We can confirm this by running the
    following code:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据现在已从0.0缩放到1.0。我们可以通过运行以下代码来确认这一点：
- en: '[PRE25]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This returns the following output:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下输出：
- en: '![](img/5fca7448-9f2a-4680-be20-ec3b55f749de.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5fca7448-9f2a-4680-be20-ec3b55f749de.png)'
- en: 'The next preprocessing step we''ll need to perform before running our first
    Keras network will be to reshape our data. Remember, the shapes of our `X_train` and
    `X_test` are currently (60,000, 784) and (10,000,784), respectively. Our images
    are still vectors. For us to convolve these lovely kernels all over the image,
    we''ll need need to reshape them into their 28 x 28 matrix form. Additionally,
    Keras requires that we explicitly declare the number of channels for our data.
    Accordingly, when we reshape these grayscale images for modeling, we''ll declare
    `1`:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行我们的第一个Keras网络之前，我们需要执行的下一个预处理步骤是调整数据的形状。记住，`X_train`和`X_test`当前的形状分别是(60,000,
    784)和(10,000, 784)。我们的图像仍然是向量。为了能够将这些美丽的卷积核应用到整个图像上，我们需要将它们调整为28 x 28的矩阵形式。此外，Keras要求我们明确声明数据的通道数。因此，当我们将这些灰度图像调整为建模格式时，我们将声明`1`：
- en: '[PRE26]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Lastly, we''ll one-hot encode our `y` vectors to conform with the target shape
    requirements of Keras:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将对`y`向量进行独热编码，以符合Keras的目标形状要求：
- en: '[PRE27]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We''re now ready for modeling. Our first network will have eight hidden layers.
    The first six hidden layers will consist of alternating convolutional and max
    pooling layers. We''ll then flatten the output of this network and feed that into
    a two-layer feedforward neural network before generating our predictions. Here''s
    what this looks like, in code:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备开始建模。我们的第一个网络将有八个隐藏层。前六个隐藏层将由交替的卷积层和最大池化层组成。然后我们将把这个网络的输出展平，并将其输入到一个两层的前馈神经网络中，最后生成预测。代码如下所示：
- en: '[PRE28]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s describe what''s happening on each line in some depth:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入描述每一行的内容：
- en: '**Line 1**: Here, we just instantiate our model object. We''ll further define
    the architecture—that is, the number of layers—sequentially with a series of `.add()`
    method calls that follow. This is the beauty of the Keras API.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第1行**：这里，我们只是实例化了我们的模型对象。接下来，我们将通过一系列的`.add()`方法调用依次定义架构——即层的数量。这就是Keras
    API的魅力所在。'
- en: '**Line 2**: Here, we add our first convolutional layer. We specify `35` kernels,
    each 3 x 3 in size. After this, we specify the image input shape, 28 x 28 x 1\.
    We only have to specify the input shape in the first `.add()` call of our network.
    Lastly, we specify our activation function as `relu`. Activation functions transform
    the output of a layer before it''s passed into the next layer. We''ll apply activation
    functions to our `Conv2D` and `Dense` layers. These transformations have many
    important properties. Using `relu` here speeds up the convergence of our network, [http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf
    and `relu`, relative to alternative activation functions, isn''t expensive to
    compute—we''re just transforming negative values to 0, and otherwise keeping all
    positive values. Mathematically, the `relu` function is given by `max(0, value)`.
    For the purpose of this chapter, we''ll stick to the `relu` activation for every
    layer but the output layer.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第2行**：这里，我们添加了第一个卷积层。我们指定了`35`个卷积核，每个大小为3 x 3。之后，我们指定了图像输入的形状，28 x 28 x 1。我们只需要在网络的第一次`.add()`调用中指定输入形状。最后，我们将激活函数指定为`relu`。激活函数在将输出传递到下一层之前，对输出进行变换。我们将在`Conv2D`和`Dense`层上应用激活函数。这些变换具有许多重要的性质。在这里使用`relu`可以加速网络的收敛，[http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)，并且与其他激活函数相比，`relu`计算起来并不昂贵——我们只是将负值变为0，其他正值保持不变。从数学上讲，`relu`函数为`max(0,
    value)`。在本章中，我们将为每一层使用`relu`激活函数，除了输出层。'
- en: '**Line 3**: Here, we add our first max pooling layer. We specify that the window
    size of this layer will be  2 x 2.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第3行**：这里，我们添加了第一个最大池化层。我们指定该层的窗口大小为2 x 2。'
- en: '**Line 4**: This is our second convolutional layer. We set it up just as we
    set up the first convolutional layer.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第4行**：这是我们的第二个卷积层。我们设置它的方式与第一个卷积层完全相同。'
- en: '**Line 5**: This is the second max pooling layer. We set this layer up just
    as we set up the first max pooling layer.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第5行**：这是第二个最大池化层。我们设置该层的方式与第一个最大池化层完全相同。'
- en: '**Line 6**: This is our third and final convolutional layer. This time, we
    add additional filters (`45` versus the `35` in previous layers). This is just
    a hyperparameter, and I encourage you to try multiple variations of this.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第6行**：这是我们第三个也是最后一个卷积层。这次，我们增加了额外的过滤器（`45`个，而之前的层是`35`个）。这只是一个超参数，我鼓励你尝试不同的变体。'
- en: '**Line 7**: This is the third and final max pooling layer. It''s configured
    the same as all max pooling layers that came before it.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第7行**：这是第三个也是最后一个最大池化层。它的配置与之前的所有最大池化层完全相同。'
- en: '**Line 8**: Here''s where we flatten the output of our convolutional neural
    network.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第8行**：这是我们展平卷积神经网络输出的地方。'
- en: '**Line 9**: Here''s the first layer of our fully-connected network. We specify
    `64` neurons in this layer and a `relu` activation function.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第9行**：这是我们全连接网络的第一层。在这一层，我们指定了`64`个神经元，并使用`relu`激活函数。'
- en: '**Line 10**: Here''s the second layer of our fully-connected network. We specify
    `32` neurons for this layer and a `relu` activation function.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第10行**：这是我们全连接网络的第二层。在这一层，我们指定了`32`个神经元，并使用`relu`激活函数。'
- en: '**Line 11**: This is our output layer. We specify `10` neurons, equal to the
    number of target classes in our data. Since this is a multi-class classification
    problem, we specify a `softmax` activation function. The output will represent
    the predicted probability of the image belonging to classes 0–9\. These probabilities
    will sum to `1`. The highest predicted probability of the `10` will represent
    the class our model believes to be the most likely class.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第11行**：这是我们的输出层。我们指定了`10`个神经元，等于我们数据中目标类别的数量。由于这是一个多分类问题，我们指定了`softmax`激活函数。输出将表示图像属于类别0到9的预测概率。这些概率的和为`1`。这10个概率中，最高的一个将代表我们的模型认为最有可能的类别。'
- en: '**Line 12**: Here''s where we compile our Keras model. In the compile step,
    we specify our optimizer, `Adam`, a **gradient-descent** algorithm that automatically
    adapts its learning rate. We specify our **loss function**—in this case, `categorical
    cross entropy` because we''re performing a multi-class classification problem.
    Lastly, for the metrics argument, we specify `accuracy`. By specifying this, Keras
    will inform us of our train and validation accuracy for each epoch that our model
    runs.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第12行**：这是我们编译Keras模型的地方。在编译步骤中，我们指定了优化器`Adam`，这是一种**梯度下降**算法，能够自动调整学习率。我们还指定了**损失函数**——在这种情况下，使用`categorical
    cross entropy`，因为我们正在执行多分类问题。最后，在metrics参数中，我们指定了`accuracy`。通过指定这一点，Keras将在每个epoch结束时告诉我们训练和验证准确率。'
- en: 'We can get a summary of our model by running the following:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下命令来获取模型的总结：
- en: '[PRE29]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This outputs the following:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出如下内容：
- en: '![](img/485b7f6f-0585-468a-b359-a5b0cd770863.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/485b7f6f-0585-468a-b359-a5b0cd770863.png)'
- en: Notice how the output shapes change as the data passes through the model. Specifically,
    look at the shape of our output after the flattening occurs—just 45 features.
    The raw data in `X_train` and `X_test` consisted of 784 features per row, so this
    is fantastic!
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当数据通过模型时，输出形状如何变化。特别是，观察扁平化操作后的输出形状——只有45个特征。`X_train`和`X_test`中的原始数据每行有784个特征，所以这非常棒！
- en: You'll need to install `pydot` to render the visualization. To install it, run
    `pip install pydot` from the terminal. You may need to restart your kernel for
    the install to take effect.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要安装`pydot`来渲染可视化。要安装它，请在终端运行`pip install pydot`。你可能需要重新启动内核以使安装生效。
- en: 'Using the `plot_model` function in Keras, we can visualize the topology of
    our network differently. To do this, run the following code:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Keras中的`plot_model`函数，我们可以以不同的方式可视化网络的拓扑结构。要做到这一点，请运行以下代码：
- en: '[PRE30]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Running the preceding code saves the topology to `Conv_model1.png` and generates
    the following:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码将保存拓扑到`Conv_model1.png`并生成如下内容：
- en: '![](img/b88953fb-ec6c-4b7a-bf4a-62caa3e1e736.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b88953fb-ec6c-4b7a-bf4a-62caa3e1e736.png)'
- en: This model will take several minutes to fit. If you have concerns about your
    system's hardware specs, you can easily reduce the training time by reducing the
    number of epochs to `10`.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型需要几分钟才能拟合。如果你担心系统的硬件规格，可以通过将训练周期数减少到`10`来轻松缩短训练时间。
- en: 'Running the following code block will fit the model:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下代码块将拟合模型：
- en: '[PRE31]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In the fit step, we specify our `X_train` and `y_train`. We then specify the
    number of epochs we'd like to train the model. Then we plug in the validation
    data—`X_test` and `y_test`—to observe our model's out-of-sample performance. I
    like to save the `model.fit` step as a variable, `my_fit_model`, so we can later
    easily visualize the training and validation losses over epochs.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合步骤中，我们指定了`X_train`和`y_train`。然后我们指定了希望训练模型的周期数。接着，我们输入验证数据——`X_test`和`y_test`——以观察模型在外样本上的表现。我喜欢将`model.fit`步骤保存为变量`my_fit_model`，这样我们就可以在后面轻松地可视化每个epoch的训练和验证损失。
- en: 'As the code runs, you''ll see the model''s train and validation loss, and accuracy
    after each epoch. Let''s plot our model''s train loss and validation loss using
    the following code:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 随着代码运行，你将看到每个epoch后模型的训练损失、验证损失和准确率。我们可以使用以下代码绘制模型的训练损失和验证损失：
- en: '[PRE32]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Running the preceding code generates the following plot. Your plot won''t be
    identical—there are several stochastic processes taking place here—but it should
    look roughly the same:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码将生成如下图。你的图可能不会完全相同——这里有几个随机过程在进行——但它应该大致相同：
- en: '![](img/d0a689ed-6b00-4d32-baf9-733f49b1804e.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0a689ed-6b00-4d32-baf9-733f49b1804e.png)'
- en: 'A quick glance at this plot shows us that our model is overfitting. We see
    our train loss continue to fall in every epoch, but the validation loss doesn''t
    move in lockstep. Let''s glance at our accuracy scores to grasp how well this
    model did at the classification task. We can do this by running the following
    code:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 一瞥这个图表，我们可以看出我们的模型出现了过拟合。我们看到每个训练周期的训练损失持续下降，但验证损失没有同步变化。让我们看看准确率，以了解这个模型在分类任务中的表现如何。我们可以通过运行以下代码来查看：
- en: '[PRE33]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This generates the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下结果：
- en: '![](img/cfd56e4a-0ee6-4060-ad8b-04cc3521cc0b.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfd56e4a-0ee6-4060-ad8b-04cc3521cc0b.png)'
- en: 'This plot, too, tells us we''ve overfit. But it appears as though our validation
    accuracy is in the high 80s, which is great! To get the max accuracy our model
    achieved and the epoch in which it occurred, we can run the following code:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表也告诉我们我们已经过拟合。但看起来我们的验证准确率已接近 80% 高位，真不错！为了获得模型达成的最大准确率以及出现该准确率的训练周期，我们可以运行以下代码：
- en: '[PRE34]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Your specific results will differ from mine, but here''s my output:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 你的结果可能会与我的不同，但这是我的输出：
- en: '![](img/7e6eb9cd-38e2-4a6b-a1f6-fd4b19c2adac.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e6eb9cd-38e2-4a6b-a1f6-fd4b19c2adac.png)'
- en: Using our convolutional neural network, we achieved a max classification accuracy
    of 89.48% in the 21st epoch. This is amazing! But we've still got to address that
    overfitting problem. Next, we'll rebuild our model using **dropout regularization**.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的卷积神经网络，我们在第21个周期达到了最高的分类准确率89.48%。这真是太棒了！但我们仍然需要解决过拟合问题。接下来，我们将通过使用**dropout正则化**来重建模型。
- en: Dropout regularization is a form of regularization we can apply to the fully-connected
    layers of our neural network. Using dropout regularization, we randomly drop neurons
    and their connections from the network during training. By doing this, the network
    doesn't become too reliant on the weights or biases associated with any specific
    node, allowing it to generalize better out of sample.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 正则化是我们可以应用于神经网络全连接层的一种正则化方法。使用 dropout 正则化时，我们在训练过程中随机丢弃神经元及其连接。通过这样做，网络不会过于依赖于与特定节点相关的权重或偏置，从而能更好地进行样本外泛化。
- en: 'Here, we add dropout regularization, specifying that we''d like to drop `35%` of
    the neurons at each `Dense` layer:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们添加了 dropout 正则化，指定在每个 `Dense` 层丢弃 `35%` 的神经元：
- en: '[PRE35]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Running the preceding code will compile our new model. Let''s have another
    look at the summary by rerunning the following:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码将编译我们的新模型。让我们通过重新运行以下代码来再次查看总结：
- en: '[PRE36]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Running the preceding code returns the following output:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码会返回以下输出：
- en: '![](img/819a4e32-36b5-4fe9-91b8-2ceb69e1f8aa.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/819a4e32-36b5-4fe9-91b8-2ceb69e1f8aa.png)'
- en: 'Let''s refit our model by rerunning the following:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过重新运行以下代码来重新拟合我们的模型：
- en: '[PRE37]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Once your model has refit, rerun the plot code to visualize loss. Here''s mine:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型重新拟合完成，重新运行绘图代码以可视化损失。这是我的结果：
- en: '![](img/ad4333d4-48bf-43f0-962c-725832e0a4d3.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad4333d4-48bf-43f0-962c-725832e0a4d3.png)'
- en: This looks better! The difference between our training and validation losses
    has shrunk, which was the intended purpose, though there does appear to be some
    room for improvement.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来更好了！我们训练和验证损失之间的差距缩小了，这是我们预期的效果，尽管仍然有一些改进的空间。
- en: 'Next, re-plot your accuracy curves. Here are mine for this run:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，重新绘制准确率曲线。这是我这次训练的结果：
- en: '![](img/97fd4ea0-c7dc-412b-bd43-8da26dc2cf95.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97fd4ea0-c7dc-412b-bd43-8da26dc2cf95.png)'
- en: 'This also looks better from an overfitting perspective. Fantastic! What was
    the best classification accuracy we achieved after applying regularization? Let''s
    run the following code:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 从过拟合的角度看，这也显得更好。太棒了！在应用正则化后，我们达成的最佳分类准确率是多少？让我们运行以下代码：
- en: '[PRE38]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'My output from this run of the model was as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我这次运行模型的输出如下：
- en: '![](img/f8b9853f-45f0-440c-89fd-fa95f6b5b572.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8b9853f-45f0-440c-89fd-fa95f6b5b572.png)'
- en: Interesting! The best validation accuracy we achieved was lower than that in
    our unregularized model, but not by much. And it's still quite good! Our model
    is telling us that we predict the correct type of clothing article 88.85% of the
    time.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 有意思！我们达到的最佳验证准确率比未正则化模型的还低，但差距不大。而且它仍然相当不错！我们的模型告诉我们，我们预测正确衣物类别的概率为88.85%。
- en: One way to think about how well we've done here is to compare our model's accuracy
    with the **baseline accuracy** for our dataset. The baseline accuracy is simply
    the score we would get by naïvely selecting the most-commonly occurring class
    in the dataset. For this specific dataset, because the classes are perfectly balanced
    and there are 10 classes, the baseline accuracy is 10%. Our model handily beats
    this baseline accuracy. It's clearly learned something about the data!
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 评估我们在这里取得的成果的一种方式是将我们的模型准确率与数据集的**基准准确率**进行比较。基准准确率就是通过天真地选择数据集中最常出现的类别所得到的分数。对于这个特定的数据集，因为类别是完美平衡的且有10个类别，所以基准准确率是10%。我们的模型轻松超越了这一基准准确率，显然它已经从数据中学到了一些东西！
- en: There are so many different places you can go from here! Try building deeper
    models or grid-searching over the many hyperparameters we used in our models.
    Assess your classifier's performance as you would with any other model—try building
    a confusion matrix to understand what classes we predicted well and what classes
    we weren't as strong in!
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，你可以去探索很多不同的方向！尝试构建更深的模型，或者对我们在模型中使用的众多超参数进行网格搜索。像评估任何其他模型一样评估你的分类器性能——试着构建一个混淆矩阵，了解我们预测准确的类别以及哪些类别的表现较弱！
- en: Summary
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We certainly covered a lot of ground here! We talked about how to extract features
    from images, how convolutional neural networks work, and then we built a convolutional
    neural network to a fully-connected network architecture. Along the way, we picked
    up lots of new jargon and concepts, too!
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里确实覆盖了很多内容！我们讨论了如何从图像中提取特征，卷积神经网络是如何工作的，接着我们又将卷积神经网络构建成一个全连接网络架构。在这个过程中，我们还学习了许多新的术语和概念！
- en: Hopefully, after reading this chapter, you feel that these image-classification
    techniques—knowledge of which you may have once considered the province of sorcerers—is
    actually just a series of mathematical optimizations carried out for intuitive
    reasons! And hopefully this content can help move you forward in tackling an image-processing
    project that interests you!
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 希望在阅读本章后，你会觉得这些图像分类技术——你曾经可能认为只有巫师才能掌握的知识——实际上只是一系列为了直观原因进行的数学优化！希望这些内容能够帮助你在处理感兴趣的图像处理项目时取得进展！
