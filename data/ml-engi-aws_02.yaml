- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Deep Learning AMIs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习 AMIs
- en: In the *Essential prerequisites* section of [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017),
    *Introduction to ML Engineering on AWS*, it probably took us about an hour or
    so to set up our Cloud9 environment. We had to spend a bit of time installing
    several packages, along with a few dependencies, before we were able to work on
    the actual **machine learning** (**ML**) requirements. On top of this, we had
    to make sure that we were using the right versions for certain packages to avoid
    running into a variety of issues. If you think this is error-prone and tedious,
    imagine being given the assignment of preparing 20 ML environments for a team
    of data scientists! Let me repeat that… *TWENTY*! It would have taken us around
    15 to 20 hours of doing the same thing over and over again. After a week of using
    the ML environments you prepared, the data scientists then requested that you
    also install the deep learning frameworks **TensorFlow**, **PyTorch**, and **MXNet**
    inside these environments since they’ll be testing different deep learning models
    using these ML frameworks. At this point, you may already be asking yourself,
    “*Is there a better way to do this?*“. The good news is that there are a variety
    of ways to handle these types of requirements in a more efficient manner. One
    of the possible solutions is to utilize **Amazon Machine Images** (**AMIs**),
    specifically the AWS **Deep Learning AMIs** (**DLAMIs**) to significantly speed
    up the process of preparing ML environments. When launching new instances, these
    AMIs would serve as pre-configured templates containing the relevant software
    and environment configuration.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第1章*](B18638_01.xhtml#_idTextAnchor017) 的 *基本先决条件* 部分，即 *AWS 机器学习工程简介*，我们可能花费了大约一个小时来设置我们的
    Cloud9 环境。在能够着手处理实际的 **机器学习** （**ML**） 需求之前，我们必须花费一些时间安装几个包，以及一些依赖项。除此之外，我们还得确保使用某些包的正确版本，以避免遇到各种问题。如果你认为这很易出错且繁琐，想象一下被分配给一个数据科学团队准备
    20 个 ML 环境的任务！让我再重复一遍…… *二十个*！这将花费我们大约 15 到 20 个小时重复做同样的事情。在使用你准备的 ML 环境一周后，数据科学家们随后要求你也在这些环境中安装深度学习框架
    **TensorFlow**、**PyTorch** 和 **MXNet**，因为他们将使用这些 ML 框架测试不同的深度学习模型。在这个时候，你可能已经在问自己，“*有没有更好的方法来做这件事？*”。好消息是，有各种方法可以更有效地处理这些类型的需求。其中一种可能的解决方案是利用
    **Amazon Machine Images** （**AMIs**），特别是 AWS **深度学习 AMIs** （**DLAMIs**），以显著加快准备
    ML 环境的过程。在启动新实例时，这些 AMIs 将作为预配置的模板，包含相关的软件和环境配置。
- en: Before the **DLAMIs** existed, ML engineers had to spend hours installing and
    configuring deep learning frameworks inside EC2 instances before they could run
    ML workloads in the AWS cloud. The process of manually preparing these ML environments
    from scratch is tedious and error-prone as well. Once the DLAMIs were made available,
    data scientists and ML engineers were able to run their ML experiments straight
    away using their preferred deep learning framework.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **DLAMIs** 存在之前，机器学习工程师必须在能够运行 AWS 云中的 ML 工作负载之前，花费数小时在 EC2 实例中安装和配置深度学习框架。从头开始手动准备这些
    ML 环境的过程既繁琐又易出错。一旦 DLAMIs 可用，数据科学家和机器学习工程师就能直接使用他们偏好的深度学习框架进行 ML 实验。
- en: In this chapter, we will see how convenient it is to set up a GPU instance using
    a framework-specific Deep Learning AMI. We will then train a deep learning model
    using **TensorFlow** and **Keras** inside this environment. Once the training
    step is complete, we will evaluate the model using a test dataset. After that,
    we will perform the cleanup steps and terminate the EC2 instance. Toward the end
    of this chapter, we will also have a short discussion on how AWS pricing works
    for EC2 instances. This will help equip you with the knowledge required to manage
    the overall cost of running ML workloads inside these instances.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到使用特定框架的深度学习 AMI 设置 GPU 实例是多么方便。然后，我们将在这个环境中使用 **TensorFlow** 和 **Keras**
    训练深度学习模型。一旦训练步骤完成，我们将使用测试数据集评估模型。之后，我们将执行清理步骤并终止 EC2 实例。在本章的结尾，我们还将简要讨论 AWS 对
    EC2 实例的定价方式。这将帮助你掌握管理这些实例中运行 ML 工作负载所需的总成本的知识。
- en: 'That said, we will cover the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，在本章中，我们将涵盖以下主题：
- en: Getting started with Deep Learning AMIs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用 Deep Learning AMIs
- en: Launching an EC2 instance using a Deep Learning AMI
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习 AMI 启动 EC2 实例
- en: Downloading the sample dataset
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载示例数据集
- en: Training an ML model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练机器学习模型
- en: Loading and evaluating the model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载和评估模型
- en: Cleaning up
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理工作
- en: Understanding how AWS pricing works for EC2 instances
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解AWS对EC2实例的定价方式
- en: The hands-on solutions in this chapter will help you migrate any of your existing
    **TensorFlow**, **PyTorch**, and **MXNet** scripts and models to the AWS cloud.
    In addition to the cost discussions mentioned earlier, we will also talk about
    a few security guidelines and best practices to help us ensure that the environments
    we set up have a good starting security configuration. With these in mind, let’s
    get started!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的动手实践解决方案将帮助你将任何现有的**TensorFlow**、**PyTorch**和**MXNet**脚本和模型迁移到AWS云。除了前面提到的成本讨论之外，我们还将讨论一些安全指南和最佳实践，以帮助我们确保我们设置的环境具有良好的初始安全配置。考虑到这些，让我们开始吧！
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Before we start, we must have a web browser (preferably Chrome or Firefox) and
    an AWS account to use for the hands-on solutions in this chapter. Make sure that
    you have access to the AWS account you used in [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017),
    *Introduction to ML Engineering on AWS*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们必须有一个网络浏览器（最好是Chrome或Firefox）和一个AWS账户，用于本章的动手实践解决方案。确保你有访问你在[*第1章*](B18638_01.xhtml#_idTextAnchor017)，*AWS上的机器学习工程简介*中使用的AWS账户。
- en: 'The Jupyter notebooks, source code, and other files used for each chapter are
    available in this book’s GitHub repository: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每章使用的Jupyter笔记本、源代码和其他文件都可在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS)。
- en: Getting started with Deep Learning AMIs
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用Deep Learning AMIs
- en: 'Before we talk about DLAMIs, we must have a good idea of what AMIs are. We
    can think of an AMI as the “DNA” of an organism. Using this analogy, the organism
    would correspond and map to one or more EC2 instances:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论DLAMIs之前，我们必须对AMIs有一个很好的了解。我们可以将AMI比作生物体的“DNA”。使用这个类比，生物体会对应并映射到一个或多个EC2实例：
- en: '![Figure 2.1 – Launching EC2 instances using Deep Learning AMIs ](img/B18638_02_001.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – 使用Deep Learning AMIs启动EC2实例](img/B18638_02_001.jpg)'
- en: Figure 2.1 – Launching EC2 instances using Deep Learning AMIs
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 使用Deep Learning AMIs启动EC2实例
- en: If we were to launch two EC2 instances using the same AMI (similar to what is
    shown in *Figure 2.1*), both instances would have the same set of installed packages,
    frameworks, tools, and operating systems upon instance launch. Of course, not
    everything needs to be the same as these instances may have different instance
    types, different security groups, and other configurable properties.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用相同的AMI（类似于*图2.1*中所示）启动两个EC2实例，那么在实例启动时，这两个实例将具有相同的已安装软件包、框架、工具和操作系统。当然，并非所有内容都需要相同，因为这些实例可能有不同的实例类型、不同的安全组和其他可配置属性。
- en: AMIs allow engineers to easily launch EC2 instances in consistent environments
    without having to spend hours installing different packages and tools. In addition
    to the installation steps, these EC2 instances need to be configured and optimized
    before they can be used for specific workloads. Pre-built AMIs such as DLAMIs
    have popular deep learning frameworks such as **TensorFlow**, **PyTorch**, and
    **MXNet** pre-installed already. This means that data scientists, developers,
    and ML engineers may proceed with performing ML experiments and deployments without
    having to worry about the installation and setup process.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: AMIs允许工程师轻松地在一致的环境中启动EC2实例，而无需花费数小时安装不同的软件包和工具。除了安装步骤之外，这些EC2实例在使用特定工作负载之前还需要进行配置和优化。预构建的AMI，如DLAMIs，已经预装了流行的深度学习框架，如**TensorFlow**、**PyTorch**和**MXNet**。这意味着数据科学家、开发人员和机器学习工程师可以继续进行机器学习实验和部署，而无需担心安装和设置过程。
- en: If we had to prepare 20 ML environments with these deep learning frameworks
    installed, I’m pretty sure that it would not take us 20 or more hours to do so.
    If we were to use DLAMIs, probably 2 to 3 hours would be more than enough to get
    the job done. *You don’t believe me?* *In the next section, we will do just that!*
    Of course, we will only be preparing a single ML environment instead of 20\. While
    working on the hands-on solutions in this chapter, you will notice a significant
    speed boost when setting up and configuring the prerequisites needed to run the
    ML experiments.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要准备安装了这些深度学习框架的 20 个 ML 环境，我非常确信这不会花费我们 20 个或更多小时。如果我们使用 DLAMI，可能 2 到 3
    个小时就足够完成任务了。*你不相信我吗？* *在下一节中，我们将会这样做!* 当然，我们只会准备一个 ML 环境，而不是 20 个。当在本章中处理实际解决方案时，你将注意到在设置和配置运行
    ML 实验所需的先决条件时，速度会有显著提升。
- en: Note
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is important to note that we have the option to build on top of existing
    AMIs and prepare our own custom AMIs. Then, we can use these custom AMIs when
    launching new EC2 instances.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，我们有选择在现有的 AMI 上构建并准备我们自己的定制 AMI。然后，我们可以在启动新的 EC2 实例时使用这些定制 AMI。
- en: Launching an EC2 instance using a Deep Learning AMI
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习 AMI 启动 EC2 实例
- en: Launching an EC2 instance from a DLAMI is straightforward. Once we have an idea
    of which DLAMI to use, the rest of the steps would just be focused on configuring
    and launching the EC2 instance. The cool thing here is that we are not limited
    to launching a single instance from an existing image. During the configuration
    stage, before an instance is launched from an AMI, it is important to note that
    we can specify the desired value for the number of instances to be launched (for
    example, `20`). This would mean that instead of launching a single instance, we
    would launch 20 instances all at the same time instead.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从 DLAMI 启动 EC2 实例非常简单。一旦我们确定了要使用的 DLAMI，接下来的步骤就主要集中在配置和启动 EC2 实例上。这里有趣的是，我们不仅限于从一个现有的镜像中启动单个实例。在配置阶段，在从
    AMI 启动实例之前，需要注意的是，我们可以指定要启动的实例数量（例如，`20`）。这意味着我们不会启动单个实例，而是同时启动 20 个实例。
- en: '![Figure 2.2 – Steps to launch an EC2 instance using a DLAMI ](img/B18638_02_002.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – 使用 DLAMI 启动 EC2 实例的步骤](img/B18638_02_002.jpg)'
- en: Figure 2.2 – Steps to launch an EC2 instance using a DLAMI
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 使用 DLAMI 启动 EC2 实例的步骤
- en: We will divide this section into four parts. As shown in the preceding diagram,
    we’ll start by locating the framework-specific Deep Learning AMI in the `p3.2xlarge`,
    as the instance type. We’ll then configure the security settings, including the
    network security settings, to be used by the instance. Finally, we will launch
    the instance and connect to it from the browser using **EC2 Instance Connect**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这个部分分为四个部分。如图所示，我们首先将定位到 `p3.2xlarge` 作为实例类型，寻找框架特定的深度学习 AMI。然后，我们将配置实例将使用的安全设置，包括网络安全设置。最后，我们将启动实例，并通过
    **EC2 实例连接** 从浏览器连接到它。
- en: Locating the framework-specific DLAMI
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定位框架特定的 DLAMI
- en: When looking for an AMI, the first place we should check is the **AWS AMI Catalog**.
    In the AMI Catalog, we should find a variety of DLAMIs. These DLAMIs can be categorized
    into either multi-framework DLAMIs or framework-specific DLAMIs. *What’s the difference?*
    Multi-framework DLAMIs include multiple frameworks in a single AMI such as **TensorFlow**,
    **PyTorch**, or **MXNet**. This allows for easy experimentation and exploration
    of several frameworks for developers, ML engineers, and data scientists. On the
    other hand, framework-specific DLAMIs are more optimized for production environments
    and support only a single framework. In this chapter, we will be working with
    the framework-specific (TensorFlow) Deep Learning AMI.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在寻找 AMI 时，我们应该首先检查的地方是 **AWS AMI 目录**。在 AMI 目录中，我们应该找到各种 DLAMI。这些 DLAMI 可以分为多框架
    DLAMI 或框架特定 DLAMI。*有什么区别？* 多框架 DLAMI 在单个 AMI 中包含多个框架，例如 **TensorFlow**、**PyTorch**
    或 **MXNet**。这允许开发人员、机器学习工程师和数据科学家轻松地进行多个框架的实验和探索。另一方面，框架特定 DLAMI 更适合生产环境，并且只支持单个框架。在本章中，我们将使用框架特定的（TensorFlow）深度学习
    AMI。
- en: 'In the next set of steps, we will navigate to the AMI Catalog and use the framework-specific
    (TensorFlow) Deep Learning AMI to launch an instance:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将导航到 AMI 目录，并使用框架特定的（TensorFlow）深度学习 AMI 来启动实例：
- en: 'Navigate to the AWS Management Console and then type `ec2` in the search bar.
    Select **EC2** from the list of results:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到 AWS 管理控制台，然后在搜索栏中输入`ec2`。从结果列表中选择**EC2**：
- en: '![Figure 2.3 – Navigating to the EC2 console ](img/B18638_02_003.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 导航到 EC2 控制台](img/B18638_02_003.jpg)'
- en: Figure 2.3 – Navigating to the EC2 console
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 导航到 EC2 控制台
- en: We should see a list of matching results such as **EC2**, **EC2 Image Builder**,
    and **AWS Compute Optimizer**, similar to what is shown in *Figure 2.2*. From
    this list, we’ll choose the first one, which should redirect us to the EC2 console.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到一系列匹配结果，如 **EC2**、**EC2 图像构建器**和**AWS 计算优化器**，类似于 *图 2.2* 中所示。从这个列表中，我们将选择第一个，这将带我们转到
    EC2 控制台。
- en: In the sidebar, locate and click **AMI Catalog** under **Images** to navigate
    to the **EC2** > **AMI Catalog** page.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在侧边栏中，找到并点击**镜像**下的**AMI 目录**以导航到**EC2** > **AMI 目录**页面。
- en: 'Next, type `deep learning ami` in the search bar within the **AMI Catalog**
    page. Make sure that you press **Enter** to search for relevant AMIs related to
    the search query:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在**AMI 目录**页面内的搜索栏中输入`deep learning ami`。确保按下**Enter**键以搜索与搜索查询相关的相关 AMI：
- en: '![Figure 2.4 – Searching for the framework-specific Deep Learning AMI ](img/B18638_02_004.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.4 – 搜索框架特定的深度学习 AMI](img/B18638_02_004.jpg)'
- en: Figure 2.4 – Searching for the framework-specific Deep Learning AMI
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – 搜索框架特定的深度学习 AMI
- en: As shown in the preceding screenshot, we should have a couple of matching results
    under **Quickstart AMIs**. There should be matching results under **AWS Marketplace
    AMIs** and **Community AMIs** as well. Quickstart AMIs include the commonly used
    AMIs for key workloads such as the **Amazon Linux 2** AMI, the **Ubuntu Server
    20.04 LTS** AMI, the **Deep Learning AMI** (Amazon Linux 2) AMI, and more. AWS
    Marketplace AMIs include several AMIs created by AWS, along with AMIs created
    by trusted third-party sources. These should include AMIs such as the **OpenVPN
    Access Server** AMI, the **Kali Linux** AMI, and the **Splunk Enterprise** AMI.
    All publicly available AMIs can be found under **Community AMIs**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，我们应该在**快速启动 AMI**下有几项匹配结果。在**AWS Marketplace AMI**和**社区 AMI**下也应有所匹配结果。快速启动
    AMI 包括用于关键工作负载的常用 AMI，例如 **Amazon Linux 2** AMI、**Ubuntu Server 20.04 LTS** AMI、**深度学习
    AMI**（Amazon Linux 2）AMI 等。AWS Marketplace AMI 包括 AWS 创建的几个 AMI，以及由受信任的第三方来源创建的
    AMI。这些应包括 **OpenVPN 访问服务器** AMI、**Kali Linux** AMI 和 **Splunk 企业版** AMI。所有公开可用的
    AMI 都可以在**社区 AMI**下找到。
- en: 'Scroll down the list of **Quickstart AMIs** and locate the framework-specific
    Deep Learning AMI, as shown in the following screenshot:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动**快速启动 AMI**列表，找到框架特定的深度学习 AMI，如图下所示：
- en: '![Figure 2.5 – Locating the TensorFlow DLAMI ](img/B18638_02_005.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.5 – 定位 TensorFlow DLAMI](img/B18638_02_005.jpg)'
- en: Figure 2.5 – Locating the TensorFlow DLAMI
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 – 定位 TensorFlow DLAMI
- en: Here, we are choosing the framework-specific (TensorFlow) Deep Learning AMI
    for **Amazon Linux 2** since we’ll be training an ML model using TensorFlow later
    in this chapter. Verify the selection by reading the name and description of the
    AMI. Then, click the **Select** button.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们选择框架特定的（TensorFlow）深度学习 AMI 用于 **Amazon Linux 2**，因为我们将在本章后面使用 TensorFlow
    训练 ML 模型。通过阅读 AMI 的名称和描述来验证选择。然后，点击**选择**按钮。
- en: 'After you have clicked the **Select** button in the previous step, scroll up
    to the top of the page and click the **Launch Instance with AMI** button, as shown
    in the following screenshot:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上一步点击了**选择**按钮后，向上滚动到页面顶部并点击**使用 AMI 启动实例**按钮，如图下所示：
- en: '![Figure 2.6 – Launch Instance with AMI ](img/B18638_02_006.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6 – 使用 AMI 启动实例](img/B18638_02_006.jpg)'
- en: Figure 2.6 – Launch Instance with AMI
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 – 使用 AMI 启动实例
- en: As we can see, the **Launch Instance with AMI** button is just beside the **Create
    Template with AMI** button.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，**使用 AMI 启动实例**按钮就在**使用 AMI 创建模板**按钮旁边。
- en: Important Note
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: There are no additional costs associated with the usage of **AWS Deep Learning
    AMIs**. This means that we only need to consider the costs associated with the
    infrastructure resources created. However, the usage of other AMIs may not be
    free. For example, AMIs created by other companies (from the list available under
    **AWS Marketplace AMIs**) may have an additional charge per hour of use. That
    said, it is important to check for any additional charges on top of the infrastructure
    resources launched using these AMIs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**AWS深度学习AMI**没有额外的费用。这意味着我们只需要考虑与创建的基础设施资源相关的成本。然而，使用其他AMI可能并不免费。例如，由其他公司创建的AMI（在**AWS
    Marketplace AMI**下提供的列表中）可能每小时收取额外费用。话虽如此，检查使用这些AMI启动的基础设施资源之上的任何额外费用是很重要的。
- en: 'Clicking the **Launch Instance with AMI** button should redirect you to the
    **Launch an instance** page, as shown in the following screenshot:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**使用AMI启动实例**按钮应将您重定向到如图所示的**启动实例**页面：
- en: '![Figure 2.7 – The Launch an instance page ](img/B18638_02_007.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图2.7 – 启动实例页面](img/B18638_02_007.jpg)'
- en: Figure 2.7 – The Launch an instance page
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 – 启动实例页面
- en: Since AWS regularly updates the experience of launching and managing resources
    in the console, you might see a few differences while you are performing the next
    set of steps. However, the desired final configuration will be the same, regardless
    of what the console looks like while you are working on this section.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于AWS定期更新控制台中启动和管理资源的功能，您在执行下一组步骤时可能会看到一些差异。然而，无论您在处理本节时控制台看起来如何，期望的最终配置都将保持不变。
- en: Under `MLE-CH02-DLAMI` in the **Name** field.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**名称**字段下的`MLE-CH02-DLAMI`。
- en: After setting the **Name** field’s value, the next step involves choosing the
    desired instance type for our EC2 instance. Before we proceed with selecting the
    desired instance type, we must have a quick discussion about what instances are
    available and which types of instances are suitable for large-scale ML workloads.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置**名称**字段值之后，下一步涉及选择我们EC2实例所需的目标实例类型。在我们进行选择目标实例类型之前，我们必须简要讨论一下可用的实例类型以及哪些类型的实例适合大规模机器学习工作负载。
- en: Choosing the instance type
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择实例类型
- en: When performing deep learning experiments, data scientists and ML engineers
    generally prefer GPU instances over CPU instances. **Graphics Processing Units**
    (**GPUs**) help significantly speed up deep learning experiments since GPUs can
    be used to process multiple parallel computations at the same time. Since GPU
    instances are usually more expensive than CPU instances, data scientists and ML
    engineers use a combination of both types when dealing with ML requirements. For
    example, ML practitioners may limit the usage of GPU instances just for training
    deep learning models only. This means that CPU instances would be used instead
    for inference endpoints where the trained models are deployed. This would be sufficient
    in most cases, and this would be considered a very practical move once costs are
    taken into consideration.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行深度学习实验时，数据科学家和机器学习工程师通常更倾向于选择GPU实例而不是CPU实例。**图形处理单元**（**GPU**）可以显著加速深度学习实验，因为GPU可以同时处理多个并行计算。由于GPU实例通常比CPU实例更昂贵，数据科学家和机器学习工程师在处理机器学习需求时通常会使用这两种类型的组合。例如，机器学习从业者可能仅将GPU实例的使用限制在训练深度学习模型上。这意味着CPU实例将用于部署训练好的模型的推理端点。这在大多数情况下是足够的，并且一旦考虑到成本，这将被视为一个非常实用的举措。
- en: '![Figure 2.8 – CPU instances versus GPU instances ](img/B18638_02_008.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图2.8 – CPU实例与GPU实例](img/B18638_02_008.jpg)'
- en: Figure 2.8 – CPU instances versus GPU instances
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 – CPU实例与GPU实例
- en: That said, we need to identify which instances fall under the group of GPU instances
    and which instances fall under the CPU instances umbrella. The preceding diagram
    shows some examples of GPU instances, including `p3.2xlarge`, `dl1.24xlarge`,
    `g3.4xlarge`, `p2.8xlarge`, and `g4ad.8xlarge`. There are other examples of GPU
    instance types not in this list, but you should be able to identify these just
    by checking the instance family. For example, we are sure that `p3.8xlarge` is
    a GPU instance type since it belongs to the same family as the `p3.2xlarge` instance
    type.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们需要确定哪些实例属于GPU实例组，哪些实例属于CPU实例的范畴。前面的图表显示了一些GPU实例的例子，包括`p3.2xlarge`、`dl1.24xlarge`、`g3.4xlarge`、`p2.8xlarge`和`g4ad.8xlarge`。还有其他不在列表中的GPU实例类型，但你应该只需通过检查实例家族就能识别这些实例。例如，我们确信`p3.8xlarge`是一个GPU实例类型，因为它属于与`p3.2xlarge`实例类型相同的家族。
- en: 'Now that we have a better idea of what CPU and GPU instances are, let’s proceed
    with locating and choosing `p3.2xlarge` from the list of options for our instance
    type:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经对CPU和GPU实例有了更好的了解，让我们继续在实例类型的选项列表中定位并选择`p3.2xlarge`：
- en: 'Continuing where we left off in the *Locating the framework-specific DLAMI*
    section, let’s locate and click the **Compare instance types** link under the
    **Instance type** pane. This should redirect you to the **Compare instance types**
    page, as shown in the following screenshot:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“定位框架特定的DLAMI”部分我们停止的地方继续，让我们在“实例类型”面板下找到并点击**比较实例类型**链接。这应该会重定向到**比较实例类型**页面，如下面的截图所示：
- en: '![Figure 2.9 – Compare instance types ](img/B18638_02_009.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图2.9 – 比较实例类型](img/B18638_02_009.jpg)'
- en: Figure 2.9 – Compare instance types
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 – 比较实例类型
- en: Here, we can see the different instance types, along with their corresponding
    specs and cost per hour.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到不同的实例类型，以及它们对应的规格和每小时成本。
- en: 'Click the search field (with the **Filter instance types** placeholder text).
    This should open a drop-down list of options, as shown in the following screenshot:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击搜索字段（带有**过滤实例类型**占位文本）。这应该打开一个选项的下拉列表，如下面的截图所示：
- en: '![Figure 2.10 – Using the Filter instance types search field ](img/B18638_02_010.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图2.10 – 使用“过滤实例类型”搜索字段](img/B18638_02_010.jpg)'
- en: Figure 2.10 – Using the Filter instance types search field
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 – 使用“过滤实例类型”搜索字段
- en: Locate and select **GPUs** from the list of options. This should open the **Add
    filter for GPUs** window.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在选项列表中找到并选择**GPU**。这应该打开**添加GPU过滤器**窗口。
- en: In the `0` in the text field beside it. Click the **Confirm** button afterward.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在其旁边的文本字段中的`0`上。点击随后的**确认**按钮。
- en: Note
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The filter we applied should limit the set of results to GPU instances. We should
    find several accelerated computing instance families such as *P3*, *P2*, *G5*,
    *G4dn*, and *G3*, to name a few.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用过的过滤器应该将结果集限制为GPU实例。我们应该找到几个加速计算实例家族，例如*P3*、*P2*、*G5*、*G4dn*和*G3*，仅举几个例子。
- en: 'Next, let’s click the **Preferences** button, as highlighted in the following
    screenshot:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们点击下面的截图中所突出的**首选项**按钮：
- en: '![Figure 2.11 – Opening the Preferences window ](img/B18638_02_011.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图2.11 – 打开首选项窗口](img/B18638_02_011.jpg)'
- en: Figure 2.11 – Opening the Preferences window
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11 – 打开首选项窗口
- en: 'This should open the **Preferences** window. Under **Attribute columns**, ensure
    that the **GPUs** radio button is toggled on, as shown in the following screenshot:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该打开**首选项**窗口。在**属性列**下，确保**GPU**单选按钮被选中，如下面的截图所示：
- en: '![Figure 2.12 – Displaying the GPUs attribute column ](img/B18638_02_012.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图2.12 – 显示GPU属性列](img/B18638_02_012.jpg)'
- en: Figure 2.12 – Displaying the GPUs attribute column
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 – 显示GPU属性列
- en: 'Click the **Confirm** button afterward. This should update the table list display
    and show us the number of GPUs of each of the instance types in the list, as shown
    here:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 点击随后的**确认**按钮。这应该更新表格列表显示，并显示列表中每种实例类型的GPU数量，如下所示：
- en: '![Figure 2.13 – GPUs of each instance type ](img/B18638_02_013.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图2.13 – 每个实例类型的GPU](img/B18638_02_013.jpg)'
- en: Figure 2.13 – GPUs of each instance type
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13 – 每个实例类型的GPU
- en: Here, we should see a pattern that the number of GPUs generally increases as
    the instance type becomes “larger” within the same instance family.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们应该看到一种模式，即随着实例类型在同一个实例家族中变得“更大”，GPU的数量通常会增加。
- en: Locate and select the row corresponding to the **p3.2xlarge** instance type.
    Take note of the number of GPUs available, along with the cost per hour (on-demand
    Linux pricing) for the **p3.2xlarge** instance type.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到并选择对应于**p3.2xlarge**实例类型的行。注意可用的GPU数量，以及**p3.2xlarge**实例类型的每小时成本（按需Linux定价）。
- en: Click the **Select instance type** button (located at the lower right portion
    of the screen) afterward.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后点击屏幕右下角的**选择实例类型**按钮。
- en: This should close the **Compare instance types** window and return you to the
    **Launch an instance** page.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会关闭**比较实例类型**窗口，并返回到**启动实例**页面。
- en: Ensuring a default secure configuration
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确保默认安全配置
- en: 'When launching an EC2 instance, we need to manage the security configuration,
    which will affect how the instance will be accessed. This involves configuring
    the following:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当启动EC2实例时，我们需要管理安全配置，这将影响实例的访问方式。这包括配置以下内容：
- en: '**Key pair**: Files containing credentials used to securely access the instance
    (for example, using SSH)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密钥对**：包含用于安全访问实例的凭据的文件（例如，使用SSH）'
- en: '**Virtual Private Cloud** (**VPC**): A logically isolated virtual network that
    dictates how resources are accessed and how resources communicate with each other'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟专用云**（**VPC**）：一个逻辑上隔离的虚拟网络，它规定了如何访问资源以及资源之间如何相互通信'
- en: '**Security group**: A virtual firewall that controls traffic going in and out
    of the EC2 instance using rules that filter the traffic based on the configured
    protocol and ports'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全组**：一个虚拟防火墙，使用基于配置的协议和端口的规则来过滤进出EC2实例的流量'
- en: 'That said, let’s proceed with completing the remaining configuration parameters
    before we launch the EC2 instance:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，在我们启动EC2实例之前，让我们先完成剩余的配置参数：
- en: Continuing where we left off in the *Choosing the instance type* section, let’s
    proceed with creating a new key pair. Under **Key pair (login)**, locate and click
    **Create new key pair**.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们之前在*选择实例类型*部分结束的地方继续，让我们继续创建一个新的密钥对。在**密钥对（登录）**下，找到并点击**创建新的密钥对**。
- en: In the `dlami-key`) for `RSA`
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`dlami-key`) for `RSA`
- en: '`.pem`'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.pem`'
- en: Click the `.pem` file to your local machine. Note that we won’t need this `.pem`
    file for the hands-on solutions in this chapter since we’ll be accessing the instance
    later using **EC2 Instance Connect** (through the browser).
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`.pem`文件到您的本地机器。请注意，我们在这个章节的动手实践中不需要这个`.pem`文件，因为我们稍后将通过**EC2实例连接**（通过浏览器）使用它来访问实例。
- en: Important Note
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Never share the downloaded key file since this is used to access the instance
    via SSH. For production environments, consider hiding non-public instances inside
    a properly configured VPC as well. There’s a lot to discuss when it comes to securing
    our ML environments. We will talk about security in detail in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187),
    *Security, Governance, and Compliance Strategies*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 永远不要共享下载的密钥文件，因为这是通过SSH访问实例所用的。对于生产环境，考虑将非公开实例隐藏在正确配置的VPC内部。关于保护我们的机器学习环境有很多要讨论的。我们将在[*第9章*](B18638_09.xhtml#_idTextAnchor187)中详细讨论安全，*安全、治理和合规策略*。
- en: Under `vpc-xxxxxxxx (default)`
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`vpc-xxxxxxxx（默认）`下
- en: '`Enable`'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`启用`'
- en: '`Create security group`'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`创建安全组`'
- en: 'Under **Inbound security group rules** of **Network settings**, specify a set
    of security group rules, similar to what is configured in the following screenshot:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**网络设置**的**入站安全组规则**下，指定一组安全组规则，类似于以下截图中所配置的：
- en: '![Figure 2.14 – Inbound security groups rules ](img/B18638_02_014.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图2.14 – 入站安全组规则](img/B18638_02_014.jpg)'
- en: Figure 2.14 – Inbound security groups rules
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 – 入站安全组规则
- en: 'As you can see, we will be configuring the new security group with the following
    rules:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们将使用以下规则配置新的安全组：
- en: '`SSH`; `TCP`; `22`; `Anywhere` | `0.0.0.0/0`; `SSH` – allows any “computer”
    such as your local machine to connect to the EC2 instance via the **Secure Shell**
    (SSH) protocol over port 22'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SSH`；`TCP`；`22`；`任何地方` | `0.0.0.0/0`；`SSH` – 允许任何“计算机”例如您的本地机器通过**安全壳**（SSH）协议在端口22上连接到EC2实例'
- en: '`Custom TCP`; `TCP`; `6006`; `Anywhere` | `0.0.0.0/0`; `Tensorboard` – allows
    any “computer” such as your local machine to access port 6006 of the EC2 instance
    (which may be running an application such as **TensorBoard**)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`自定义TCP`；`TCP`；`6006`；`任何地方` | `0.0.0.0/0`；`Tensorboard` – 允许任何“计算机”例如您的本地机器访问EC2实例的6006端口（可能运行着**TensorBoard**等应用程序）'
- en: '`Custom TCP`; `TCP`; `8888`; `Anywhere` | `0.0.0.0/0`; `Jupyter` – allows any
    “computer” such as your local machine to access port 8888 of the EC2 instance
    (which may be running an application such as the **Jupyter Notebook** app)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`自定义 TCP`; `TCP`; `8888`; `任何地方` | `0.0.0.0/0`; `Jupyter` – 允许任何“计算机”例如您的本地机器访问
    EC2 实例的 8888 端口（可能运行着如 **Jupyter Notebook** 应用程序之类的应用程序）'
- en: You may proceed with the next step once you have configured the new security
    group with **Security group name – required** and **Description – required** and
    the relevant set of **Inbound security group rules**.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您配置了新的安全组，包括 **安全组名称 – 必需** 和 **描述 – 必需** 以及相关的 **入站安全组规则**，您就可以进行下一步操作。
- en: Note
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Note that this configuration needs to be reviewed and secured further once we
    need to prepare our setup for production use. For one, `0.0.0.0/0`) since this
    configuration allows any computer or server to access our instance through the
    open ports. That said, we could have limited access to only the IP address of
    our local machine. In the meantime, the configuration we have should do the trick
    since we will delete the instance immediately once we have completed this chapter.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一旦我们需要为生产使用准备设置，这个配置就需要被审查并进一步加固。首先，`0.0.0.0/0`)，因为这个配置允许任何计算机或服务器通过开放的端口访问我们的实例。话虽如此，我们可能只限制本地机器的
    IP 地址访问。在此期间，我们现有的配置应该足够用，因为我们将在完成本章后立即删除该实例。
- en: 'Locate and click the **Add new volume** button under **Configure storage**:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **配置存储** 下定位并点击 **添加新卷** 按钮：
- en: '![Figure 2.15 – Configuring the storage settings ](img/B18638_02_015.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.15 – 配置存储设置](img/B18638_02_015.jpg)'
- en: Figure 2.15 – Configuring the storage settings
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15 – 配置存储设置
- en: Specify `35` in the text field between **1x** and **GiB**. similar to what we
    have in the preceding screenshot.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **1x** 和 **GiB** 之间的文本字段中指定 `35`。类似于前一个屏幕截图中的设置。
- en: There are a few more options we can configure and tweak (under **Advanced Details**)
    but we’ll leave the default values as-is.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以配置和调整（在 **高级详情** 下）的几个更多选项，但我们将保留默认值不变。
- en: Launching the instance and connecting to it using EC2 Instance Connect
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 EC2 实例连接启动实例并连接到它
- en: There are different ways to connect to an EC2 instance. Earlier, we configured
    the instance so that it can be accessed via SSH using a key file (for example,
    from the Terminal of your local machine). Another possible option is to use **EC2
    Instance Connect** to access the instance through the browser. We can also access
    the instance via SSH using **Session Manager**. In this section, we’ll use EC2
    Instance Connect to access our instance.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方式可以连接到 EC2 实例。之前，我们配置了实例，使其可以通过密钥文件（例如，从您的本地机器的终端）使用 SSH 访问。另一个可能的选项是使用
    **EC2 实例连接** 通过浏览器访问实例。我们还可以使用 **会话管理器** 通过 SSH 访问实例。在本节中，我们将使用 EC2 实例连接来访问我们的实例。
- en: 'Continuing where we left off in the *Ensuring a default secure configuration*
    section, let’s proceed with launching the EC2 instance and access it from the
    browser:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *确保默认安全配置* 部分我们停止的地方继续，现在让我们启动 EC2 实例并通过浏览器访问它：
- en: Once you have configured the storage settings, locate and click the **Launch
    instance** button under the **Summary** pane (located at the right portion of
    the screen). Make sure that you terminate this instance within the hour it has
    been launched as the per-hour rate of these types of instances is a bit higher
    relative to other instance types. You may check the *Cleaning up* section of this
    chapter for more details.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦您已配置存储设置，请定位并点击屏幕右侧的 **摘要** 下的 **启动实例** 按钮。确保在启动后的一小时内终止此实例，因为这些类型的实例的每小时费用相对于其他实例类型要高一些。您可以在本章的
    *清理* 部分查看更多详细信息。
- en: Note
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure that the value specified in the `1`. Technically, we can launch 20
    instances all in one go by setting this value to `20`. However, we don’t want
    to do this as this would be very expensive and wasteful. For now, let’s stick
    to `1` as this should be more than enough to handle the deep learning experiments
    in this chapter.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在 `1` 中指定的值。技术上，我们可以通过将此值设置为 `20` 一次性启动 20 个实例。然而，我们不想这样做，因为这会非常昂贵且浪费。现在，让我们保持
    `1`，这应该足以处理本章中的深度学习实验。
- en: 'You should see a success notification, along with the instance ID of the resource
    being launched, similar to what is shown in the following screenshot:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该会看到一个成功通知，以及正在启动的资源实例 ID，类似于以下屏幕截图所示：
- en: '![Figure 2.16 – Launch success notification ](img/B18638_02_016.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图2.16 – 启动成功通知](img/B18638_02_016.jpg)'
- en: Figure 2.16 – Launch success notification
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16 – 启动成功通知
- en: Click the link containing the instance ID (`i-xxxxxxxxxxxxxxxxx`), as highlighted
    in the preceding screenshot, to navigate to the `MLE-CH02-DLAMI`) to appear in
    the list of instances.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 点击包含实例ID（`i-xxxxxxxxxxxxxxxxx`）的链接，如图中高亮显示，以导航到`MLE-CH02-DLAMI`，使其出现在实例列表中。
- en: Note
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Wait for a minute or two before proceeding with the next step. In case you experience
    an `InsufficientInstanceCapacity` error while launching the instance, feel free
    to use a different `p3` instance. To troubleshoot this further, you may also refer
    to [https://aws.amazon.com/premiumsupport/knowledge-center/ec2-insufficient-capacity-errors/](https://aws.amazon.com/premiumsupport/knowledge-center/ec2-insufficient-capacity-errors/)
    for more information.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行下一步之前，请等待一分钟或两分钟。如果在启动实例时遇到`InsufficientInstanceCapacity`错误，请随意使用不同的`p3`实例。要进一步排除故障，您还可以参考[https://aws.amazon.com/premiumsupport/knowledge-center/ec2-insufficient-capacity-errors/](https://aws.amazon.com/premiumsupport/knowledge-center/ec2-insufficient-capacity-errors/)以获取更多信息。
- en: 'Select the instance by toggling the checkbox highlighted in the following screenshot.
    Click the **Connect** button afterward:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过切换以下屏幕截图中的复选框选择实例。之后点击**连接**按钮：
- en: '![Figure 2.17 – Connecting to the instance directly ](img/B18638_02_017.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图2.17 – 直接连接到实例](img/B18638_02_017.jpg)'
- en: Figure 2.17 – Connecting to the instance directly
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17 – 直接连接到实例
- en: Here, we can see that there’s an option to connect directly to the instance
    using the browser.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到有一个选项可以直接使用浏览器连接到实例。
- en: 'In the `AA.BB.CC.DD`) to a text editor on your local machine. Note that you
    will get a different public IP address value. We will use this IP address value
    later in this chapter when accessing `root`) and then click **Connect**:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`AA.BB.CC.DD`到您本地机器上的文本编辑器。请注意，您将获得不同的公网IP地址值。我们将在本章后面使用此IP地址值来访问`root`并然后点击**连接**：
- en: '![Figure 2.18 – EC2 Instance Connect ](img/B18638_02_018.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图2.18 – EC2实例连接](img/B18638_02_018.jpg)'
- en: Figure 2.18 – EC2 Instance Connect
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.18 – EC2实例连接
- en: 'This should open a new tab that will allow us to run terminal commands directly
    from the browser. If you are getting a **There was a problem connecting to your
    instance** error message, wait for about 2 to 3 minutes before refreshing the
    page or clicking the **Retry** button:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会打开一个新标签页，允许我们从浏览器直接运行终端命令。如果您收到**无法连接到您的实例**的错误消息，请在刷新页面或点击**重试**按钮之前等待大约2到3分钟：
- en: '![Figure 2.19 – EC2 Instance Connect terminal ](img/B18638_02_019.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图2.19 – EC2实例连接终端](img/B18638_02_019.jpg)'
- en: Figure 2.19 – EC2 Instance Connect terminal
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.19 – EC2实例连接终端
- en: As we can see, `TensorFlow 2.9.1` and other utility libraries are installed
    in `/usr/local/bin/python3.9`. Note that you may get different TensorFlow and
    Python versions, depending on the version of the DLAMI you use to launch the instance.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，`TensorFlow 2.9.1`和其他实用库已安装在`/usr/local/bin/python3.9`中。请注意，您可能会根据您用于启动实例的DLAMI版本获得不同的TensorFlow和Python版本。
- en: Wasn’t that easy? At this point, we should now be able to perform deep learning
    experiments using TensorFlow without having to install additional tools and libraries
    inside the EC2 instance.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 难道不是很容易吗？到这一点，我们现在应该能够使用TensorFlow进行深度学习实验，而无需在EC2实例内部安装额外的工具和库。
- en: Note
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note that this process of launching instances from AMIs can be further sped
    up using **Launch Templates**, which already specify instance configuration information
    such as the AMI ID, instance type, key pair, and security groups. We won’t cover
    the usage of Launch Templates in this book, so feel free to check the following
    link for more details: [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-templates.xhtml](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-templates.xhtml).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，使用AMI启动实例的过程可以通过**启动模板**进一步加速，这些模板已经指定了实例配置信息，例如AMI ID、实例类型、密钥对和安全组。本书不会涵盖启动模板的使用，因此请随意查看以下链接以获取更多详细信息：[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-templates.xhtml](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-templates.xhtml)。
- en: Downloading the sample dataset
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载示例数据集
- en: 'In the succeeding sections of this chapter, we will work with a very simple
    synthetic dataset that contains only two columns – *x* and *y*. Here, *x* may
    represent an object’s relative position on the *X*-axis, while *y* may represent
    the same object’s position on the *Y*-axis. The following screenshot shows an
    example of what the data looks like:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后续部分，我们将使用一个非常简单的合成数据集，该数据集只包含两列 – *x* 和 *y*。在这里，*x* 可能代表对象在 *X* 轴上的相对位置，而
    *y* 可能代表同一对象在 *Y* 轴上的位置。以下截图显示了数据的示例：
- en: '![Figure 2.20 – Sample dataset ](img/B18638_02_020.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.20 – 示例数据集](img/B18638_02_020.jpg)'
- en: Figure 2.20 – Sample dataset
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.20 – 示例数据集
- en: ML is about finding patterns. With this dataset, we will build a model that
    tries to predict the value of *y* given the value of *x* later in this chapter.
    Once we’re able to build models with a simple example like this, it will be much
    easier to deal with more realistic datasets that contain more than two columns,
    similar to what we worked with in [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017),
    *Introduction to ML Engineering on AWS*.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是关于寻找模式。在本章的后面，我们将构建一个模型，尝试根据 *x* 的值预测 *y* 的值。一旦我们能够构建像这样的简单示例模型，处理包含超过两列的更真实的数据集将会容易得多，就像我们在
    [*第 1 章*](B18638_01.xhtml#_idTextAnchor017) *AWS 机器学习工程简介* 中处理的那样。
- en: Note
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In this book, we won’t limit ourselves to just tabular data and simple datasets.
    In [*Chapter 6*](B18638_06.xhtml#_idTextAnchor132), *SageMaker Training and Debugging
    Solutions*, for example, we’ll work with labeled image data and build two image
    classification models using several capabilities and features of **Amazon SageMaker**.
    In [*Chapter 7*](B18638_07.xhtml#_idTextAnchor151), *SageMaker Deployment Solutions*,
    we’ll work with text data and deploy a **natural language processing** (**NLP**)
    model using a variety of deployment options.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们不会仅限于表格数据和简单的数据集。例如，在 [*第 6 章*](B18638_06.xhtml#_idTextAnchor132) *SageMaker
    训练和调试解决方案* 中，我们将处理标记的图像数据，并使用 **Amazon SageMaker** 的多个功能和特性构建两个图像分类模型。在 [*第 7
    章*](B18638_07.xhtml#_idTextAnchor151) *SageMaker 部署解决方案* 中，我们将处理文本数据，并使用各种部署选项部署一个
    **自然语言处理** (**NLP**) 模型。
- en: 'That said, let’s continue where we left off in the *Launching the instance
    and connecting to it using EC2 Instance Connect* section and proceed with downloading
    the dataset we will use to train the deep learning model in this chapter:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们继续在 *启动实例并使用 EC2 实例连接连接到它* 部分中留下的地方，并继续下载我们将用于本章训练深度学习模型的数据库集：
- en: 'In the `data` directory:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `data` 目录下：
- en: '[PRE0]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Download the training, validation, and test datasets using the `wget` command:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `wget` 命令下载训练、验证和测试数据集：
- en: '[PRE1]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Optionally, we can install the `tree` utility using the `yum` package management
    tool:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可选地，我们可以使用 `yum` 软件包管理工具安装 `tree` 工具：
- en: '[PRE4]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If this is your first time encountering the `tree` command, it is used to list
    the directories and files in a tree-like structure.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是你第一次遇到 `tree` 命令，它用于以树状结构列出目录和文件。
- en: Note
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'It is also possible to create a custom AMI from an EC2 instance. If we were
    to create a custom AMI from the EC2 instance we are using right now, we would
    be able to launch new EC2 instances from the new custom AMI with the following
    installed already: (1) installed frameworks, libraries, and tools from the DLAMI,
    and (2) the `tree` utility we installed before the custom AMI was created.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以从 EC2 实例创建一个自定义 AMI。如果我们现在从正在使用的 EC2 实例创建一个自定义 AMI，我们就可以使用以下已安装的内容启动新的 EC2
    实例： (1) 从 DLAMI 安装的框架、库和工具，以及 (2) 在创建自定义 AMI 之前安装的 `tree` 工具。
- en: 'Use the `tree` command to see the current set directories and files in the
    current directory:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `tree` 命令查看当前目录下的所有文件夹和文件：
- en: '[PRE5]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This should yield a tree-like structure, similar to what is shown in the following
    screenshot:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会产生一个类似以下截图所示的树状结构：
- en: '![Figure 2.21 – Results after using the tree command ](img/B18638_02_021.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.21 – 使用 tree 命令后的结果](img/B18638_02_021.jpg)'
- en: Figure 2.21 – Results after using the tree command
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.21 – 使用 tree 命令后的结果
- en: Here, we can see that we have successfully downloaded the CSV files using the
    `wget` command earlier.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们已成功使用之前安装的 `wget` 命令下载了 CSV 文件。
- en: 'Now, let’s verify and check the contents of one of the CSV files we have downloaded.
    Use the `head` command to see the first few rows of the `training_data.csv` file:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们验证并检查我们下载的 CSV 文件之一的内容。使用 `head` 命令查看 `training_data.csv` 文件的前几行：
- en: '[PRE6]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This should give us rows of *(x,y) pairs*, similar to what is shown in the
    following screenshot:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会给我们提供*(x,y)*对，类似于以下截图所示：
- en: '![Figure 2.22 – The first few rows of the training_data.csv file ](img/B18638_02_022.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图2.22 – training_data.csv文件的前几行](img/B18638_02_022.jpg)'
- en: Figure 2.22 – The first few rows of the training_data.csv file
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.22 – training_data.csv文件的前几行
- en: You may check the contents of `validation_data.csv` and `test_data.csv` using
    the `head` command as well.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用`head`命令检查`validation_data.csv`和`test_data.csv`的内容。
- en: Note
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is important to note that the first column in this example is the *y* column.
    Some ML practitioners follow a convention where the first column is used as the
    target column (the column containing the values we want to predict using the other
    columns of the dataset). When using certain algorithms such as the **XGBoost**
    and **Linear Learner** built-in algorithms of **SageMaker**, the first column
    is assumed to be the target column. If you are using your own custom scripts to
    load the data, you can follow any convention you would like since you have the
    freedom of how the data is loaded and interpreted from a file.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，在这个例子中，第一列是*y*列。一些机器学习实践者遵循一种惯例，即第一列用作目标列（包含我们希望使用数据集的其他列预测的值的列）。当使用某些算法，如**SageMaker**内置的**XGBoost**和**线性学习器**算法时，假定第一列是目标列。如果你使用自己的自定义脚本来加载数据，你可以遵循任何你喜欢的惯例，因为你对数据的加载和从文件中解释数据的自由度很大。
- en: You have probably noticed by now that, so far in this book, we have been using
    clean and preprocessed datasets. In real ML projects, you’ll be dealing with raw
    data with a variety of issues such as missing values and duplicate rows. In [*Chapter
    5*](B18638_05.xhtml#_idTextAnchor105), *Pragmatic Data Processing and Analysis*,
    we’ll be working with a “dirtier” version of the *bookings* dataset and use a
    variety of AWS services and capabilities such as **AWS Glue DataBrew** and **Amazon
    SageMaker Data Wrangler** to analyze, clean, and process the data. In this chapter,
    however, we will work with a “clean” dataset since we need to focus on training
    a deep learning model using **TensorFlow** and **Keras**. That said, let’s proceed
    with generating a model that accepts *x* as the input and returns a predicted
    *y* value as the output.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你可能已经注意到，在这本书中，我们一直在使用干净且预处理过的数据集。在真实的机器学习项目中，你将处理各种问题，如缺失值和重复行等原始数据。在[*第5章*](B18638_05.xhtml#_idTextAnchor105)，“实用数据处理和分析”中，我们将处理*bookings*数据集的“更脏”版本，并使用各种AWS服务和功能，如**AWS
    Glue DataBrew**和**Amazon SageMaker Data Wrangler**来分析、清理和处理数据。然而，在这一章中，我们将使用“干净”的数据集，因为我们需要专注于使用**TensorFlow**和**Keras**训练深度学习模型。话虽如此，让我们继续生成一个接受*x*作为输入并返回预测*y*值作为输出的模型。
- en: Training an ML model
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练机器学习模型
- en: 'In [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017), *Introduction to ML Engineering
    on AWS*, we trained a binary classifier model that aims to predict if a hotel
    booking will be canceled or not using the available information. In this chapter,
    we will use the (intentionally simplified) dataset from *Downloading the Sample
    Dataset* and train a regression model that will predict the value of *y* (continuous
    variable) given the value of *x*. Instead of relying on ready-made AutoML tools
    and services, we will be working with a custom script instead:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第1章*](B18638_01.xhtml#_idTextAnchor017)，“AWS上机器学习工程的介绍”中，我们训练了一个二元分类器模型，旨在使用可用信息预测酒店预订是否会取消。在本章中，我们将使用（有意简化的）从*下载示例数据集*中获得的（intentionally
    simplified）数据集，并训练一个回归模型，该模型将根据*x*的值预测*y*（连续变量）的值。我们将不依赖现成的AutoML工具和服务，而是使用自定义脚本：
- en: '![Figure 2.23 – Model life cycle ](img/B18638_02_023.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图2.23 – 模型生命周期](img/B18638_02_023.jpg)'
- en: Figure 2.23 – Model life cycle
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.23 – 模型生命周期
- en: When writing a custom training script, we usually follow a sequence similar
    to what is shown in the preceding diagram. We start by defining and compiling
    a model. After that, we load the data and use it to train and evaluate the model.
    Finally, we serialize and save the model into a file.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当编写自定义训练脚本时，我们通常遵循与前面图表中所示类似的顺序。我们首先定义和编译一个模型。之后，我们加载数据并使用它来训练和评估模型。最后，我们将模型序列化并保存到文件中。
- en: Note
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: What happens after the model has been saved? The model file can be used and
    loaded in an inference endpoint — a web server that uses a trained ML model to
    perform predictions (for example, predicted *y* values) given a set of input values
    (for example, input *x* values). In the *Loading and evaluating the model* section
    of this chapter, we’ll load the generated model file inside a Jupyter Notebook
    using the `load_model()` function from `tf.keras.models`. We’ll then use the `predict()`
    method to perform sample predictions using a provided test dataset.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 模型保存后会发生什么？模型文件可以在推理端点中使用和加载——这是一个使用训练好的机器学习模型进行预测（例如，预测*y*值）的Web服务器，给定一组输入值（例如，输入*x*值）。在本章的*加载和评估模型*部分，我们将使用`tf.keras.models`模块中的`load_model()`函数在Jupyter
    Notebook中加载生成的模型文件。然后，我们将使用`predict()`方法使用提供的测试数据集进行样本预测。
- en: 'In this chapter, we will work with a script file that uses **TensorFlow** and
    **Keras** to build a **neural network** model – an interconnected group of nodes
    that can learn complex patterns between inputs and outputs. As we will be working
    with neural networks and deep learning concepts in this book, we must have a basic
    understanding of the following concepts:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用一个脚本文件，该文件使用**TensorFlow**和**Keras**构建**神经网络**模型——一个可以学习输入和输出之间复杂模式的节点互联组。由于我们将在这本书中处理神经网络和深度学习概念，我们必须对以下概念有一个基本了解：
- en: '**Neurons**: These are the building blocks of neural networks that accept and
    process input values to produce output values. *How are the output values computed?*
    Each of the input values passing through the neuron is multiplied by the associated
    **weight** values and then a numerical value (also known as the **bias**) is added
    afterward. A non-linear function called the **activation function** is then applied
    to the resulting value, which would yield the output. This non-linear function
    helps neural networks learn complex patterns between the input values and the
    output values. We can see a representation of a neuron in the following diagram:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经元（Neurons**）：这些是神经网络的基本构建块，它们接受并处理输入值以产生输出值。*输出值是如何计算的？*每个通过神经元的输入值都会乘以相关的**权重**值，然后加上一个数值（也称为**偏置**）。随后，将一个称为**激活函数**的非线性函数应用于所得值，从而产生输出。这个非线性函数有助于神经网络学习输入值和输出值之间的复杂模式。我们可以在以下图中看到一个神经元的表示：'
- en: '![Figure 2.24 – A representation of a neuron ](img/B18638_02_024.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图2.24 – 神经元的表示](img/B18638_02_024.jpg)'
- en: Figure 2.24 – A representation of a neuron
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.24 – 神经元的表示
- en: Here, we can see that we can compute the value of *y* with a formula involving
    the *x* input values, the corresponding weight values, the bias, and the activation
    function. That said, we can think of a neuron as a “mathematical function” and
    a neural network as a “bunch of mathematical functions” trying to map input values
    with output values through the continuous update of weight and bias values.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以通过一个包含输入值、相应的权重值、偏置和激活函数的公式来计算*y*的值。换句话说，我们可以将神经元视为一个“数学函数”，将神经网络视为“一串数学函数”，这些函数试图通过不断更新权重和偏置值来映射输入值和输出值。
- en: '**Layers**: Layers are composed of a group of neurons located at a specific
    location or depth in a neural network:'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层（Layers**）：层由位于神经网络特定位置或深度的神经元组成：'
- en: '![Figure 2.25 – An input layer, output layer, and multiple hidden layers ](img/B18638_02_025.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图2.25 – 输入层、输出层和多个隐藏层](img/B18638_02_025.jpg)'
- en: Figure 2.25 – An input layer, output layer, and multiple hidden layers
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.25 – 输入层、输出层和多个隐藏层
- en: Here, we can see the different layers of a neural network. The **input layer**
    is the layer receiving the input values, while the **output layer** is the layer
    generating the output values. Between the input layer and the output layer are
    processing layers called **hidden layers**, which process and transform the data
    from the input layer to the output layer. (Neural networks with more than one
    or two hidden layers are generally called **deep neural networks**.)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到神经网络的各个层次。**输入层**是接收输入值的层，而**输出层**是生成输出值的层。在输入层和输出层之间是称为**隐藏层**的处理层，它们处理和转换从输入层到输出层的数据。（具有一个或两个以上隐藏层的神经网络通常称为**深度神经网络**。）
- en: '**Forward propagation**: This refers to the forward flow of information from
    the input layer to the hidden layers and then to the output layers to generate
    the output values.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正向传播**：这指的是从输入层到隐藏层，然后到输出层的正向信息流，以生成输出值。'
- en: '**Cost function**: This function is used to compute how far off the predicted
    computed value is from the actual value. Given that the goal of training a neural
    network is to generate a predicted value as close as possible to the actual value,
    we should be aiming to look for a minimum value of the cost function (which represents
    the error of the model) using optimization algorithms such as **Gradient Descent**.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数**：此函数用于计算预测计算值与实际值之间的偏差。鉴于训练神经网络的目的是生成尽可能接近实际值的预测值，我们应该通过使用如**梯度下降**之类的优化算法来寻找损失函数的最小值（这代表了模型的误差）。'
- en: '**Backpropagation**: This is the process of adjusting the weights in a neural
    network based on the difference between the predicted values and the actual values
    (which involves calculating the **gradients** or making small updates to the weights
    in each layer):'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向传播**：这是根据预测值和实际值之间的差异调整神经网络中权重的过程（这涉及到计算**梯度**或对每一层的权重进行小幅度更新）：'
- en: '![Figure 2.26 – Backpropagation ](img/B18638_02_026.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.26 – 反向传播](img/B18638_02_026.jpg)'
- en: Figure 2.26 – Backpropagation
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.26 – 反向传播
- en: Here, we can see that backpropagation involves propagating the computed error
    backward from the output layer to the input layer (and updating the weights accordingly).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到反向传播涉及从输出层向输入层传播计算出的误差（并相应地更新权重）。
- en: '**Learning rate**: This influences the amount used to adjust the weights in
    the network concerning the loss gradient while training the neural network.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**：这影响了在训练神经网络时根据损失梯度调整网络中权重的数量。'
- en: '**Epoch**: This is a training iteration that involves one forward and one backward
    propagation using the entire training dataset. After each training iteration,
    the weights of the neural network are updated, and the neural network is expected
    to perform better in mapping the set of input values into the set of output values.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时代**：这是一个涉及使用整个训练数据集进行一次正向传播和一次反向传播的训练迭代。在每次训练迭代之后，神经网络的权重都会更新，并且期望神经网络在将输入值映射到输出值时表现更好。'
- en: Note
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'We won’t dive deep into the details of deep learning and neural networks in
    this book. If you are interested in learning more about these topics, there are
    several books available online: [https://www.amazon.com/Neural-Network/s?k=Neural+Network](https://www.amazon.com/Neural-Network/s?k=Neural+Network).'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入探讨深度学习和神经网络的细节。如果您想了解更多关于这些主题的信息，网上有几种书籍可供选择：[https://www.amazon.com/Neural-Network/s?k=Neural+Network](https://www.amazon.com/Neural-Network/s?k=Neural+Network)。
- en: 'Now that we have a better idea of what neural networks are, we can proceed
    with training a neural network model. In the next set of steps, we will use a
    custom script to train a deep learning model with the data downloaded in the previous
    section:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对神经网络有了更好的了解，我们可以继续训练神经网络模型。在接下来的步骤中，我们将使用自定义脚本来训练一个深度学习模型，该模型使用上一节下载的数据：
- en: 'First, let’s create a directory named `logs` using the `mkdir` command:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们使用 `mkdir` 命令创建一个名为 `logs` 的目录：
- en: '[PRE7]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, use the `wget` command to download the `train.py` file:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用 `wget` 命令下载 `train.py` 文件：
- en: '[PRE8]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Use the `tree` command to quickly check what the file and directory structure
    looks like:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `tree` 命令快速检查文件和目录结构：
- en: '[PRE9]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This should yield a tree-like structure, similar to what is shown in the following
    screenshot:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会生成一个类似以下截图所示的树状结构：
- en: '![Figure 2.27 – Results after using the tree command ](img/B18638_02_027.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.27 – 使用 tree 命令后的结果](img/B18638_02_027.jpg)'
- en: Figure 2.27 – Results after using the tree command
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.27 – 使用 tree 命令后的结果
- en: Note that the data and log directories are at the same level as the `train.py`
    file.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，数据和日志目录与 `train.py` 文件处于同一级别。
- en: 'Before running the `train.py` file, execute the following command:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行 `train.py` 文件之前，执行以下命令：
- en: '[PRE10]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This will help us avoid the **successful NUMA node read from SysFS had negative
    value (-1)** warning message when listing the GPU devices later in this chapter.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这将帮助我们避免在列出本章后面的 GPU 设备时出现的**成功从 SysFS 读取 NUMA 节点具有负值 (-1)** 警告信息。
- en: 'Before running the downloaded `train.py` script, let’s check its contents by
    opening [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter02/train.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter02/train.py)
    in a separate browser tab:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行下载的 `train.py` 脚本之前，让我们通过在单独的浏览器标签页中打开 [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter02/train.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter02/train.py)
    来检查其内容：
- en: '![Figure 2.28 – The train.py file ](img/B18638_02_028.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.28 – train.py 文件](img/B18638_02_028.jpg)'
- en: Figure 2.28 – The train.py file
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.28 – train.py 文件
- en: 'In the preceding screenshot, we can see that our `train.py` script does the
    following:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，我们可以看到我们的 `train.py` 脚本执行以下操作：
- en: (`prepare_model()` function
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （`prepare_model()` 函数
- en: (`load_data()` function
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （`load_data()` 函数
- en: (**3**) prepares the **TensorBoard** callback object
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （**3**）准备 **TensorBoard** 回调对象
- en: (`fit()` method and passes the `callback` parameter value
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （`fit()` 方法并传递 `callback` 参数值
- en: (`save()` method
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （`save()` 方法
- en: Note
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is important to note that the `prepare_model()` function in our `train.py`
    script performs both the *define model* and *compile model* steps. The neural
    network defined in this function is a sample sequential model with five layers.
    For more information, feel free to check out the implementation of the `prepare_model()`
    function at [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter02/train.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter02/train.py#L24).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，我们 `train.py` 脚本中的 `prepare_model()` 函数执行了 *定义模型* 和 *编译模型* 两个步骤。在此函数中定义的神经网络是一个具有五个层的示例顺序模型。有关更多信息，请随时查看
    `prepare_model()` 函数的实现，链接为 [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter02/train.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter02/train.py#L24)。
- en: 'Let’s start the training step by running the following in the EC2 Instance
    Connect terminal:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在 EC2 Instance Connect 终端中运行以下命令来开始训练步骤：
- en: '[PRE11]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This should yield a set of logs, similar to what is shown in the following
    screenshot:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会生成一组日志，类似于以下屏幕截图所示：
- en: '![Figure 2.29 – train.py script logs ](img/B18638_02_029.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.29 – train.py 脚本日志](img/B18638_02_029.jpg)'
- en: Figure 2.29 – train.py script logs
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.29 – train.py 脚本日志
- en: Note that the training step may take around 5 minutes to complete. Once the
    `train.py` script has finished executing, you may check the new files generated
    inside the `logs` and `model` directories using the `tree` command.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，训练步骤可能需要大约 5 分钟才能完成。一旦 `train.py` 脚本执行完毕，您可以使用 `tree` 命令检查 `logs` 和 `model`
    目录下生成的新文件。
- en: Note
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: What’s happening here? Here, the `fit()` method of the model we defined in `train.py`
    is training the model with the number of epochs (iterations) set to `500`. For
    each iteration, we are updating the weights of the neural network to minimize
    the “error” between the actual values and the predicted values (for example, using
    cross-validation data).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了什么？在这里，我们定义在 `train.py` 中的模型的 `fit()` 方法正在使用设置为 `500` 的周期数（迭代次数）来训练模型。对于每次迭代，我们正在更新神经网络的权重，以最小化实际值和预测值之间的“误差”（例如，使用交叉验证数据）。
- en: 'Next, run the following command to run the `tensorBoard` application, which
    can help visualize and debug ML experiments:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，运行以下命令以运行 `tensorBoard` 应用程序，该程序可以帮助可视化和调试机器学习实验：
- en: '[PRE12]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Open a new browser tab and open `http://<IP ADDRESS>:6006`. Replace `<IP ADDRESS>`
    with the public IP address we copied to our text editor in the *Launching an EC2
    instance using a Deep Learning AMI* section:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的浏览器标签页并打开 `http://<IP ADDRESS>:6006`。将 `<IP ADDRESS>` 替换为我们复制到文本编辑器中的公共
    IP 地址，在 *使用深度学习 AMI 启动 EC2 实例* 部分中：
- en: '![Figure 2.30 – TensorBoard ](img/B18638_02_030.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.30 – TensorBoard](img/B18638_02_030.jpg)'
- en: Figure 2.30 – TensorBoard
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.30 – TensorBoard
- en: This should load a web application, similar to what is shown in the preceding
    screenshot. We won’t dive deep into what we can do with TensorBoard, so feel free
    to check out [https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)
    for more information.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会加载一个类似于前一个屏幕截图所示的网页应用程序。我们不会深入探讨我们可以用 TensorBoard 做什么，所以请随时查看 [https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)
    获取更多信息。
- en: Note
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: How do we interpret these charts? As shown in *Figure 2.30*, the training and
    validation loss generally decrease over time. In the first chart (top), the *X*-axis
    corresponds to the epoch number, while the *Y*-axis shows the training and validation
    loss. It should be noted that in this chart, the train and validation “learning
    curves” are overlapping and both continue to decrease up to a certain point as
    the number of epochs or iterations increases. It should be noted that these types
    of charts help diagnose ML model performance, which would be useful in avoiding
    issues such as **overfitting** (where the trained model performs well on the training
    data but performs poorly on unseen data) and **underfitting** (where the trained
    model performs poorly on the training dataset and unseen data). We won’t discuss
    this in detail, so feel free to check other ML and deep learning resources available.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解释这些图表？如图2.30所示，训练和验证损失通常随时间而降低。在第一个图表（顶部），*X*轴对应于epoch编号，而*Y*轴显示训练和验证损失。需要注意的是，在这个图表中，训练和验证的“学习曲线”是重叠的，并且随着epoch或迭代次数的增加，两者都会继续下降到某个点。需要注意的是，这类图表有助于诊断机器学习模型性能，这在避免诸如**过拟合**（训练模型在训练数据上表现良好，但在未见过的数据上表现不佳）和**欠拟合**（训练模型在训练数据集和未见过的数据上表现不佳）等问题时非常有用。我们不会详细讨论这个问题，所以请随意查看其他机器学习和深度学习资源。
- en: Navigate back to the **EC2 Instance Content** terminal and stop the running
    **TensorBoard** application process with *Ctrl* + *C*.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回到**EC2实例内容**终端，并使用*Ctrl* + *C*停止正在运行的**TensorBoard**应用程序进程。
- en: At this point, we should have the artifacts of a trained model inside the `model`
    directory. In the next section, we will load and evaluate this model inside a
    Jupyter Notebook environment.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们应该在`model`目录内拥有训练好的模型的工件。在下一节中，我们将在Jupyter Notebook环境中加载并评估这个模型。
- en: Loading and evaluating the model
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载和评估模型
- en: In the previous section, we trained our deep learning model using the terminal.
    When performing ML experiments, it is generally more convenient to use a web-based
    interactive environment such as the **Jupyter Notebook**. We can technically run
    all the succeeding code blocks in the terminal, but we will use the Jupyter Notebook
    instead for convenience.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用终端训练了我们的深度学习模型。在进行机器学习实验时，通常更方便使用基于Web的交互式环境，如**Jupyter Notebook**。技术上我们可以在终端中运行所有后续的代码块，但为了方便，我们将使用Jupyter
    Notebook。
- en: 'In the next set of steps, we will launch the Jupyter Notebook from the command
    line. Then, we will run a couple of blocks of code to load and evaluate the ML
    model we trained in the previous section. Let’s get started:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将从命令行启动Jupyter Notebook。然后，我们将运行几个代码块来加载和评估我们在上一节中训练的机器学习模型。让我们开始吧：
- en: 'Continuing where we left off in the *Training an ML model* section, let’s run
    the following command in the **EC2 Instance Connect** terminal:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*训练ML模型*部分我们停止的地方继续，让我们在**EC2实例连接**终端中运行以下命令：
- en: '[PRE13]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This should start the Jupyter Notebook and make it accessible through port
    `8888`:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该启动Jupyter Notebook，并通过端口`8888`使其可访问：
- en: '![Figure 2.31 – Jupyter Notebook token ](img/B18638_02_031.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图2.31 – Jupyter Notebook令牌](img/B18638_02_031.jpg)'
- en: Figure 2.31 – Jupyter Notebook token
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.31 – Jupyter Notebook令牌
- en: Make sure that you copy the generated random token from the logs generated after
    running the `jupyter notebook` command. Refer to the preceding screenshot on where
    to get the generated token.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 确保复制从运行`jupyter notebook`命令后生成的随机令牌。参考前面的截图，了解如何获取生成的令牌。
- en: 'Open a new browser tab and open the `http://<IP ADDRESS>:8888`. Replace `<IP
    ADDRESS>` with the public IP address we copied to our text editor in the *Launching
    an EC2 instance using a Deep Learning AMI* section:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的浏览器标签页，并打开`http://<IP ADDRESS>:8888`。将`<IP ADDRESS>`替换为我们在*使用深度学习AMI启动EC2实例*部分复制到文本编辑器的公共IP地址：
- en: '![Figure 2.32 – Accessing the Jupyter Notebook ](img/B18638_02_032.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图2.32 – 访问Jupyter Notebook](img/B18638_02_032.jpg)'
- en: Figure 2.32 – Accessing the Jupyter Notebook
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.32 – 访问Jupyter Notebook
- en: Here, we can see that we are required to input a password or token before we
    can use the **Jupyter Notebook**. Simply input the token obtained from the logs
    generated in the previous step.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到在使用**Jupyter Notebook**之前，我们需要输入密码或令牌。只需输入从上一步骤生成的日志中获得的令牌。
- en: Important Note
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Note that this setup is not ready for use in production environments. For more
    information on how to secure the Jupyter Notebook server, check out https://jupyter-notebook.readthedocs.io/en/stable/security.xhtml.
    We will also discuss a few strategies to improve the security of this setup in
    [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187), *Security, Governance, and Compliance
    Strategies*.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个设置尚未准备好在生产环境中使用。有关如何安全地配置 Jupyter Notebook 服务器的信息，请参阅 https://jupyter-notebook.readthedocs.io/en/stable/security.xhtml。我们还将讨论一些提高此设置安全性的策略，详见
    [*第9章*](B18638_09.xhtml#_idTextAnchor187)，*安全、治理和合规策略*。
- en: 'Create a new notebook by clicking **New** and selecting **Python 3 (ipykernel)**
    from the list of dropdown options, similar to what is shown in the following screenshot:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击 **新建** 并从下拉选项中选择 **Python 3 (ipykernel)** 来创建一个新的笔记本，类似于以下截图所示：
- en: '![Figure 2.33 – Creating a new Jupyter Notebook ](img/B18638_02_033.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.33 – 创建新的 Jupyter Notebook](img/B18638_02_033.jpg)'
- en: Figure 2.33 – Creating a new Jupyter Notebook
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.33 – 创建新的 Jupyter Notebook
- en: This should open a blank notebook where we can run our Python code.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该打开一个空白笔记本，我们可以在这里运行我们的 Python 代码。
- en: 'Import `tensorflow` and then use `list_physical_devices()` to list the visible
    GPUs in our instance:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `tensorflow` 然后使用 `list_physical_devices()` 列出实例中的可见 GPU：
- en: '[PRE14]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This should return a list with a single `PhysicalDevice` object, similar to
    `[PhysicalDevice(name='/physical_device:GPU:0',device_type='GPU')]`.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该返回一个包含单个 `PhysicalDevice` 对象的列表，类似于 `[PhysicalDevice(name='/physical_device:GPU:0',device_type='GPU')]`。
- en: Note
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: 'Since we are using a `p3.2xlarge` instance, the preceding block of code returned
    a single visible GPU device. If we launched a `p3.16xlarge` instance, we should
    get 8 visible GPU devices instead. Note that we can significantly reduce the training
    time by utilizing multiple GPU devices at the same time through parallelism techniques
    such as **data parallelism** (where the same model is used in each GPU but trained
    with different chunks of the dataset) and **model parallelism** (where the model
    is divided into several parts equal to the number of GPUs). Of course, the ML
    experiment scripts need to be modified to utilize multiple GPUs. For more information
    on how to use a GPU in TensorFlow, feel free to check the following link for more
    details: [https://www.tensorflow.org/guide/gpu](https://www.tensorflow.org/guide/gpu).'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是 `p3.2xlarge` 实例，前面的代码块返回了一个可见的 GPU 设备。如果我们启动一个 `p3.16xlarge` 实例，我们应该得到
    8 个可见 GPU 设备。请注意，我们可以通过并行技术（如 **数据并行**（在每个 GPU 中使用相同的模型，但使用数据集的不同部分进行训练）和 **模型并行**（将模型分成与
    GPU 数量相等的几个部分））同时利用多个 GPU 设备来显著减少训练时间。当然，ML 实验脚本需要修改以利用多个 GPU。有关如何在 TensorFlow
    中使用 GPU 的更多信息，请查阅以下链接以获取更多详细信息：[https://www.tensorflow.org/guide/gpu](https://www.tensorflow.org/guide/gpu)。
- en: 'Load the model using `tf.keras.models.load_model()`. Inspect the model using
    `model.summary()`:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `tf.keras.models.load_model()` 加载模型。使用 `model.summary()` 检查模型：
- en: '[PRE16]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This should yield a model summary, as shown in the following screenshot:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该生成一个模型摘要，如下面的截图所示：
- en: '![Figure 2.34 – Model summary ](img/B18638_02_034.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.34 – 模型摘要](img/B18638_02_034.jpg)'
- en: Figure 2.34 – Model summary
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.34 – 模型摘要
- en: This model summary should reflect the properties of the model we prepared and
    trained in the *Training an ML model* section.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型摘要应反映我们在 *训练 ML 模型* 部分准备和训练的模型的属性。
- en: Important Note
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 重要备注
- en: 'Make sure to load only ML models from trusted sources using the `load_model()`
    function (along with other similar functions). Attackers can easily prepare a
    model (with a malicious payload) that, when loaded, will give the attacker access
    to the server running the ML scripts (for example, through a **reverse shell**).
    For more information on this topic, you may check the author’s talk on how to
    hack and secure ML environments and systems: [https://speakerdeck.com/arvslat/pycon-apac-2022-hacking-and-securing-machine-learning-environments-and-systems?slide=21](https://speakerdeck.com/arvslat/pycon-apac-2022-hacking-and-securing-machine-learning-environments-and-systems?slide=21).'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 确保仅使用 `load_model()` 函数（以及其他类似函数）从可信来源加载 ML 模型。攻击者可以轻松地准备一个带有恶意负载的模型，当加载时，将允许攻击者访问运行
    ML 脚本的服务器（例如，通过 **反向 shell**）。有关此主题的更多信息，您可以查看作者关于如何黑客攻击和确保 ML 环境和系统安全的演讲：[https://speakerdeck.com/arvslat/pycon-apac-2022-hacking-and-securing-machine-learning-environments-and-systems?slide=21](https://speakerdeck.com/arvslat/pycon-apac-2022-hacking-and-securing-machine-learning-environments-and-systems?slide=21)。
- en: 'Define the `load_data()` function, which will return the values of a CSV file
    with the specified file location:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`load_data()`函数，该函数将返回指定文件位置的CSV文件的值：
- en: '[PRE18]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, let’s test if the loaded model can perform predictions given as a set
    of input values. Load the test data using `load_data()` and perform a few sample
    predictions using `model.predict()`:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们测试加载的模型是否能够根据一组输入值进行预测。使用`load_data()`加载测试数据，并使用`model.predict()`进行一些样本预测：
- en: '[PRE25]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This should yield an array of floating-point values, similar to what is shown
    in the following screenshot:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会得到一个浮点数值数组，类似于以下屏幕截图所示：
- en: '![Figure 2.35 – Prediction results ](img/B18638_02_035.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![图2.35 – 预测结果](img/B18638_02_035.jpg)'
- en: Figure 2.35 – Prediction results
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.35 – 预测结果
- en: Here, we have the array of predicted *y* target values that correspond to each
    of the five input *x* values. Note that these predicted *y* values are different
    from the actual *y* values loaded from the `test_data.csv` file.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有与每个五个输入*x*值对应的预测*y*目标值数组。请注意，这些预测*y*值与从`test_data.csv`文件加载的实际*y*值不同。
- en: 'Evaluate the loaded model using `model.evaluate()`:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`model.evaluate()`评估加载的模型：
- en: '[PRE28]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This should give us a value similar to or close to `2.705784797668457`. If
    you are wondering what this number means, this is the numerical value corresponding
    to *how far* the predicted values are from the actual values:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该给出一个类似于或接近于`2.705784797668457`的值。如果你想知道这个数字的含义，这是预测值与实际值之间距离的数值表示：
- en: '![Figure 2.36 – How model evaluation works ](img/B18638_02_036.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![图2.36 – 模型评估的工作原理](img/B18638_02_036.jpg)'
- en: Figure 2.36 – How model evaluation works
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.36 – 模型评估的工作原理
- en: Here, we can see an example of how model evaluation works for regression problems.
    First, evaluation metrics such as **Root Mean Square Error** (**RMSE**), **Mean
    Square Error** (**MSE**), and **Mean Absolute Error** (**MAE**) compute the differences
    between the actual and predicted values of *y* before computing for a single evaluation
    metric value. This means that a model with a lower RMSE value generally makes
    fewer mistakes compared to a model with a higher RMSE value.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到一个回归问题中模型评估工作的例子。首先，评估指标如**均方根误差**（**RMSE**）、**均方误差**（**MSE**）和**平均绝对误差**（**MAE**）在计算单个评估指标值之前，计算实际值和预测值之间的差异。这意味着具有较低RMSE值的模型通常比具有较高RMSE值的模型犯的错误更少。
- en: At this point, you may decide to build a custom backend API utilizing the preceding
    blocks of code, along with Python web frameworks such as **Flask**, **Pyramid**,
    or **Django**. However, you may want to check other built-in solutions first,
    such as **TensorFlow Serving** (an ML model serving system for TensorFlow models),
    which is designed for production environments.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能会决定构建一个自定义的后端API，使用前面代码块以及Python Web框架，如**Flask**、**Pyramid**或**Django**。然而，你可能首先想检查其他内置解决方案，例如**TensorFlow
    Serving**（一个用于TensorFlow模型的机器学习模型服务系统），它专为生产环境设计。
- en: If you think about it, we have completed an entire ML experiment in the last
    couple of sections *without having to install any additional libraries, packages,
    or frameworks* (other than the optional `tree` utility). With that, you have learned
    how useful and powerful **Deep Learning AMIs** are! Again, if we had to set up
    20 or more ML environments like this, it would take us maybe less than 2 hours
    to get everything set up and ready.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细想想，我们在上一两个部分中已经完成了一个完整的机器学习实验，而无需安装任何额外的库、包或框架（除了可选的`tree`工具）。有了这个，你已经了解了**深度学习AMI**是多么有用和强大！再次强调，如果我们必须设置20个或更多的类似ML环境，可能只需不到2小时就能完成所有设置和准备。
- en: Cleaning up
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清理
- en: 'Now that we have completed an end-to-end ML experiment, it’s about time we
    perform the cleanup steps to help us manage costs:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了一个端到端的机器学习实验，是时候执行清理步骤以帮助我们管理成本了：
- en: Close the browser tab that contains the **EC2 Instance Connect** terminal session.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭包含**EC2实例连接**终端会话的浏览器标签页。
- en: 'Navigate to the **EC2 instance** page of the instance we launched using the
    Deep Learning AMI. Click **Instance state** to open the list of dropdown options
    and then click **Terminate instance**:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到使用深度学习AMI启动的**EC2实例**页面。点击**实例状态**以打开下拉选项列表，然后点击**终止实例**：
- en: '![Figure 2.37 – Terminating the instance ](img/B18638_02_037.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![图2.37 – 终止实例](img/B18638_02_037.jpg)'
- en: Figure 2.37 – Terminating the instance
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.37 – 终止实例
- en: As we can see, there are other options available, such as **Stop instance**
    and **Reboot instance**. If you do not want to delete the instance yet, you may
    want to stop the instance instead and start it at a later date and time. Note
    that a stopped instance will incur costs since the attached EBS volume is not
    deleted when an EC2 instance is stopped. That said, it is preferable to terminate
    the instance and delete any attached EBS volume if there are no critical files
    stored in the EBS volume.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，还有其他选项可用，例如**停止实例**和**重启实例**。如果你现在不想删除实例，你可能想先停止实例，然后在稍后的日期和时间再启动它。请注意，停止的实例会产生费用，因为当EC2实例停止时，附加的EBS卷不会被删除。话虽如此，如果没有存储在EBS卷中的关键文件，最好是终止实例并删除任何附加的EBS卷。
- en: In the **Terminate instance?** window, click **Terminate**. This should delete
    the EC2 instance, along with the volume attached to it.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**终止实例？**窗口中，点击**终止**。这应该会删除EC2实例及其附加的卷。
- en: Unused resources should be turned off, terminated, or deleted when no longer
    needed to manage and reduce costs. As our ML and ML engineering requirements need
    more resources, we will have to make use of several cost optimization strategies
    to manage costs. We will discuss some of these strategies in the next section.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 当不再需要时，应关闭、终止或删除未使用的资源以管理和降低成本。随着我们的机器学习和机器学习工程需求需要更多资源，我们将不得不利用几种成本优化策略来管理成本。我们将在下一节讨论其中一些策略。
- en: Understanding how AWS pricing works for EC2 instances
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解AWS如何为EC2实例定价
- en: Before we end this chapter, we must have a good idea of how AWS pricing works
    when dealing with EC2 instances. We also need to understand how the architecture
    and setup affect the overall cost of running ML workloads in the cloud.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本章之前，我们必须对AWS在处理EC2实例时的定价有一个很好的了解。我们还需要了解架构和设置如何影响在云中运行机器学习工作负载的整体成本。
- en: Let’s say that we initially have a single `p2.xlarge` instance running 24/7
    for an entire month in the Oregon region. Inside this instance, the data science
    team regularly runs a script that trains a deep learning model using the preferred
    ML framework. This training script generally runs for about 3 hours twice every
    week. Given the unpredictable schedule of the availability of new data, it’s hard
    to know when the training script will be run to produce a new model. The resulting
    ML model then gets deployed immediately to a web API server, which serves as the
    inference endpoint within the same instance. *Given this information, how much
    would the setup cost?*
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们最初在俄勒冈地区运行一个`p2.xlarge`实例，全天候运行整整一个月。在这个实例内部，数据科学团队定期运行一个脚本，使用首选的机器学习框架训练深度学习模型。这个训练脚本通常每周运行两次，每次大约3小时。由于新数据的可用性时间表不可预测，很难知道何时运行训练脚本以生成新的模型。生成的机器学习模型随后立即部署到Web
    API服务器，该服务器作为同一实例内的推理端点。*根据这些信息，这个设置的成本会是多少？*
- en: '![Figure 2.38 – Approximate cost of running a p2.xlarge instance per month
    ](img/B18638_02_038.jpg)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![图2.38 – 每月运行一个p2.xlarge实例的大致成本](img/B18638_02_038.jpg)'
- en: Figure 2.38 – Approximate cost of running a p2.xlarge instance per month
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.38 – 每月运行一个p2.xlarge实例的大致成本
- en: 'Here, we can see that the total cost for this setup would be around at least
    *$648 per month*. *How were we able to get this number?* We start by looking for
    the on-demand cost per hour of running a `p2.xlarge` instance in the Oregon region
    (using the following link as a reference: [https://aws.amazon.com/ec2/pricing/on-demand/](https://aws.amazon.com/ec2/pricing/on-demand/)).
    At the time of writing, the on-demand cost per hour of a `p2.xlarge` instance
    would be *$0.90 per hour* in the Oregon (`us-west-2`) region. Since we will be
    running this instance 24/7 for an entire month, we’ll have to compute the *estimated
    total number of hours per month*. Assuming that we have about 30 days per month,
    we should approximately have a total of *720 hours in a single month* – that is,
    `24 hours per day x 30 days = 720 hours`.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到这个设置的总体成本大约至少为*每月* *$648*。*我们是如何得到这个数字的？* 我们首先查找在俄勒冈地区运行`p2.xlarge`实例每小时的按需成本（使用以下链接作为参考：[https://aws.amazon.com/ec2/pricing/on-demand/](https://aws.amazon.com/ec2/pricing/on-demand/))。在撰写本文时，俄勒冈（`us-west-2`）地区`p2.xlarge`实例每小时的按需成本为*每小时*
    *$0.90*。由于我们将全天候运行此实例整整一个月，我们需要计算*每月的估计总小时数*。假设我们每月大约有30天，我们应大约有*单月720小时* – 即`每天24小时
    x 30天 = 720小时`。
- en: Note that we can also use *730.001 hours* as a more accurate value for the total
    number of hours per month. However, we’ll stick with 720 hours for now to simplify
    things a bit. The next step is to multiply the *cost per hour of running the EC2
    instance* (`$0.90 per hour`) and the *total number of hours per month* (`720 hours
    per month`). This would give us the total cost of running the EC2 instance in
    a single month (`$0.90 x 720 = $648`).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们也可以使用*730.001小时*作为每月总小时数的更准确值。然而，我们现在将坚持720小时以简化事情。下一步是将*EC2实例的每小时运行成本*（*每小时*
    *$0.90*）和*每月总小时数*（*每月720小时*）相乘。这将给我们提供单个月内运行EC2实例的总成本（*$0.90 x 720 = $648*）。
- en: Note
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'To simplify the computations in this section, we will only consider the cost
    per hour of using the EC2 instances. In real life, we’ll need to take into account
    the costs associated with using other resources such as the EBS volumes, VPC resources
    (NAT gateway), and more. For a more accurate set of estimates, make sure to use
    the **AWS Pricing Calculator**: https://calculator.aws/.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化本节中的计算，我们只考虑使用EC2实例的每小时成本。在现实生活中，我们还需要考虑使用其他资源（如EBS卷、VPC资源（NAT网关）等）相关的成本。为了得到更准确的估计，请确保使用**AWS定价计算器**：https://calculator.aws/。
- en: After a while, the data science team decided to train another model inside the
    same instance where we are already running a training script and a web server
    (inference endpoint). Worried that they might encounter performance issues and
    bottlenecks while running the two training scripts at the same time, the team
    requested for the `p2.xlarge` instance to be upgraded to a `p2.8xlarge` instance.
    *Given this information, how much would the new setup cost?*
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 过了一段时间，数据科学团队决定在我们已经运行训练脚本和Web服务器（推理端点）的同一实例中训练另一个模型。担心他们在同时运行两个训练脚本时可能会遇到性能问题和瓶颈，团队要求将`p2.xlarge`实例升级到`p2.8xlarge`实例。*根据这个信息，新的设置成本会是多少？*
- en: '![Figure 2.39 – Approximate cost of running a p2.8xlarge instance per month
    ](img/B18638_02_039.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![图2.39 – 每月运行p2.8xlarge实例的大致成本](img/B18638_02_039.jpg)'
- en: Figure 2.39 – Approximate cost of running a p2.8xlarge instance per month
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.39 – 每月运行p2.8xlarge实例的大致成本
- en: Here, we can see that the total cost for this setup would be around at least
    *$5,184 per month*. *How were we able to get this number?* We must follow a similar
    set of steps as with the previous example and look for the on-demand cost per
    hour of running a `p2.8xlarge` instance. Here, we can see that the cost of running
    a `p2.8xlarge` instance (*$7.20 per hour*) is eight times the cost of running
    a `p2.xlarge` instance (*$0.90 per hour*). That said, we’re expecting the overall
    cost to be eight times as well compared to the original setup that we had earlier.
    After multiplying the *cost per hour of running the p2.8xlarge instance* (`$7.20
    per hour`) and the *total number of hours per month* (`720 hours per month`),
    we should get the total cost of running the `p2.8xlarge` instance in a single
    month (`$7.20 x 720 = $5,184`).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到这个设置的总体成本大约至少为*每月* *$5,184*。*我们是如何得到这个数字的？*我们必须遵循与上一个示例类似的步骤，并查找运行`p2.8xlarge`实例的每小时按需成本。在这里，我们可以看到运行`p2.8xlarge`实例的成本（*每小时*
    *$7.20*）是运行`p2.xlarge`实例成本（*每小时* *$0.90*）的八倍。话虽如此，我们预计总体成本也将是之前原始设置的八倍。在将`p2.8xlarge`实例的*每小时运行成本*（*每小时*
    *$7.20*）和*每月总小时数*（*每月720小时*）相乘后，我们应该得到单个月内运行`p2.8xlarge`实例的总成本（*$7.20 x 720 =
    $5,184*）。
- en: Using multiple smaller instances to reduce the overall cost of running ML workloads
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用多个较小的实例来降低运行机器学习工作负载的总体成本
- en: At this point, you might be wondering if there is a better way to set things
    up to significantly lower the cost while running the same set of ML workloads.
    The good news is that there’s a variety of ways to improve what we have so far
    and reduce the cost from *$5,184 per month* to a much smaller value such as *$86.40
    per month*! Note that this is also significantly smaller compared to the cost
    of running the original setup (*$648 per month*). *How do we do this?*
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能想知道是否有更好的方法来设置，以在运行相同的机器学习工作负载的同时显著降低成本。好消息是，有各种方法可以改进我们目前的情况，并将成本从*每月*
    *$5,184*降低到更小的值，例如*每月* *$86.40*！请注意，这与原始设置的运行成本（*每月* *$648*）相比也小得多。*我们是如何做到这一点的？*
- en: The first thing we need to do is utilize multiple “smaller” instances instead
    of a single `p2.8xlarge` instance. One possible setup is to use a `p2.xlarge`
    instance (*$0.90 per hour*) for each of the training scripts. Since we are working
    with two training scripts, we’ll have a total of two `p2.xlarge` instances. In
    addition to this, we’ll be using an `m6i.large` instance (*$0.096 per hour*) to
    host the inference endpoint where the model is deployed. Since the training scripts
    are only expected to run when there’s new data available (approximately twice
    per week), we can have `p2.xlarge` instances running only when we need to run
    the training scripts. This means that if we have around *720 hours per month*,
    a `p2.xlarge` instance associated with one of the training scripts should only
    run for about *24 hours per month* in total (with the instance turned off the
    majority of the time).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是利用多个“较小”的实例，而不是单个`p2.8xlarge`实例。一种可能的配置是使用每个训练脚本一个`p2.xlarge`实例（每小时*0.90美元*）。由于我们正在处理两个训练脚本，我们将有两个`p2.xlarge`实例。除此之外，我们还将使用一个`m6i.large`实例（每小时*0.096美元*）来托管模型部署的推理端点。由于训练脚本只有在有新数据可用时才会运行（大约每周两次），我们可以让`p2.xlarge`实例只在需要运行训练脚本时运行。这意味着如果我们有大约*每月720小时*，与其中一个训练脚本关联的`p2.xlarge`实例总共应该只运行大约*每月24小时*（实例大部分时间处于关闭状态）。
- en: Note
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '*How did we get this number*? Since the training script is expected to run
    for about 3 hours twice every week, then the formula would be `[3 hours per run]
    x [2 times per week] x [4 weeks]`, which would yield a value of 24 hours. This
    means that each of the `p2.xlarge` instances would cost around *$21.60 per month*
    if these would only run for a total of about 24 hours in a single month.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们是如何得到这个数字的*？由于训练脚本预计每周运行两次，每次大约3小时，那么公式将是 `[每次运行3小时] x [每周2次] x [4周]`，这将得出24小时的结果。这意味着如果这些`p2.xlarge`实例在一个月内总共只运行大约24小时，那么每个`p2.xlarge`实例的月费用大约为*每月21.60美元*。'
- en: 'Even if these `p2.xlarge` instances are turned off most of the time, our ML
    inference endpoint would still be running 24/7 in its dedicated `m6i.large` instance.
    The cost of running the `m6i.large` instance for an entire month would be around
    *$69.12 per month* (using the `[$0.096 per hour] x [720 hours per month]` formula):'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这些`p2.xlarge`实例大部分时间都是关闭的，我们的机器学习推理端点仍然会在其专用的`m6i.large`实例上24/7运行。运行整个月的`m6i.large`实例的费用大约为*每月69.12美元*（使用公式`[$0.096每小时]
    x [每月720小时]`）：
- en: '![Figure 2.40 – Using multiple smaller instances to reduce the overall cost
    ](img/B18638_02_040.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![图2.40 – 使用多个较小的实例以降低整体成本](img/B18638_02_040.jpg)'
- en: Figure 2.40 – Using multiple smaller instances to reduce the overall cost
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.40 – 使用多个较小的实例以降低整体成本
- en: 'That said, we should be able to reduce the overall cost to around *$112.32
    per month*, similar to what is shown in the preceding diagram. *How were we able
    to get this number*? We simply added the expected costs of running each instance
    in a month: `$21.60 + $21.60 + $69.12 = $112.32`.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们应该能够将整体成本降低到大约*每月112.32美元*，这与前面显示的图表相似。*我们是如何得到这个数字的*？我们只是简单地将一个月内运行每个实例的预期成本相加：$21.60
    + $21.60 + $69.12 = $112.32。
- en: Using spot instances to reduce the cost of running training jobs
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用即时实例降低训练作业的运行成本
- en: It is important to note that we can further decrease this cost by utilizing
    `p2.xlarge` instances used to run the training scripts. With spot instances, we
    can reduce the cost of using a specific EC2 instance type by around 60% to 90%
    by utilizing the spare compute capacity available in AWS. This means that instead
    of paying *$0.90 per hour* when running `p2.xlarge` instances, we may only pay
    *$0.36 per hour*, assuming that we’ll have around 60% savings using spot instances.
    *What’s the catch when using spot instances?* When using spot instances, there’s
    a chance for the applications running inside these instances to be interrupted!
    This means that we should only run tasks (such as ML training jobs) that can be
    resumed after an unexpected interruption.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，我们可以通过利用用于运行训练脚本的`p2.xlarge`实例来进一步降低成本。使用即时实例，我们可以通过利用AWS中可用的备用计算能力，将特定EC2实例类型的成本降低约60%至90%。这意味着在运行`p2.xlarge`实例时，我们可能只需支付*每小时0.36美元*，假设我们使用即时实例可以节省大约60%。*使用即时实例时有什么风险吗*？当使用即时实例时，应用程序运行在这些实例内部可能会被中断！这意味着我们只应该运行那些在意外中断后可以恢复的任务（例如机器学习训练作业）。
- en: Note
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: How did we get this number? 60% savings is equivalent to multiplying the on-demand
    cost per hour (*$0.90 per hour*) by 0.40\. This would give us `[$0.90 per hour]
    x [0.40] = [$0.36 per hour]`.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是如何得到这个数字的？60% 的节省相当于将按需每小时成本（每小时 *$0.90）乘以 0.40。这将给我们 `[$0.90 每小时] x [0.40]
    = [$0.36 每小时]`。
- en: 'Since interruptions are possible when using spot instances, it is not recommended
    that you use them for the 24/7 `m6i.large` instance where the web server (inference
    endpoint) is running:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用按需实例时可能出现中断，因此不建议您将其用于 24/7 运行的 `m6i.large` 实例，其中运行着 Web 服务器（推理端点）：
- en: '![Figure 2.41 – Using spot instances to reduce the cost of running training
    jobs ](img/B18638_02_041.jpg)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.41 – 使用按需实例降低训练作业的运行成本](img/B18638_02_041.jpg)'
- en: Figure 2.41 – Using spot instances to reduce the cost of running training jobs
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.41 – 使用按需实例降低训练作业的运行成本
- en: Once we’ve utilized spot instances for the `p2.xlarge` instances, we’ll be able
    to reduce the overall cost to around *$86.40 per month*, similar to what we have
    in the preceding diagram. Again, this final value excludes the other costs to
    simplify the computations a bit. However, as you can see, this value is significantly
    smaller than the cost of running a single `p2.8xlarge` instance (*$5,184 per month*).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们为 `p2.xlarge` 实例使用了按需实例，我们就能将整体成本降低到大约 *$86.40 每月*，这与前面图表中的情况相似。再次强调，这个最终值不包括其他成本，以简化计算。然而，正如你所看到的，这个值比运行单个
    `p2.8xlarge` 实例的成本（每月 *$5,184*）要小得多。
- en: Wasn’t that amazing?! We just changed the architecture a bit and we were able
    to reduce the cost from *$5,184 per month* to *$86.40 per month*! Note that there
    are other ways to optimize the overall costs of running ML workloads in the cloud
    (for example, utilizing **Compute Savings Plans**). What you learned in this section
    should be enough for now as we’ll continue with these types of discussions over
    the next few chapters of this book.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是令人惊叹吗？！我们只是稍微改变了架构，就能将成本从 *$5,184 每月* 降低到 *$86.40 每月*！请注意，还有其他方法可以优化在云中运行机器学习工作负载的整体成本（例如，利用
    **计算节省计划**）。本节所学的内容现在应该足够了，因为我们将在本书的下一章继续讨论这类话题。
- en: Summary
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we were able to launch an EC2 instance using a **Deep Learning
    AMI**. This allowed us to immediately have an environment where we can perform
    our ML experiments without worrying about the installation and setup steps. We
    then proceeded with using **TensorFlow** to train and evaluate our deep learning
    model to solve a regression problem. We wrapped up this chapter by having a short
    discussion on how AWS pricing works for EC2 instances.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们能够使用 **深度学习 AMI** 启动一个 EC2 实例。这使我们能够立即拥有一个可以进行机器学习实验的环境，而无需担心安装和设置步骤。然后我们使用
    **TensorFlow** 训练和评估我们的深度学习模型来解决回归问题。我们通过简要讨论 AWS 对 EC2 实例的定价方式来结束本章。
- en: In the next chapter, we will focus on how **AWS Deep Learning Containers** help
    significantly speed up the ML experimentation and deployment process.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将重点介绍 **AWS 深度学习容器** 如何显著加快机器学习实验和部署过程。
- en: Further reading
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'We are only scratching the surface of what we can do with Deep Learning AMIs.
    In addition to the convenience of having preinstalled frameworks, DLAMIs make
    it easy for ML engineers to utilize other optimization solutions such as **AWS
    Inferentia**, **AWS Neuron**, **distributed training**, and **Elastic Fabric Adapter**.
    For more information, feel free to check out the following resources:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是触及了使用深度学习 AMI 可以做到的事情的表面。除了拥有预安装框架的便利性外，DLAMIs 还使机器学习工程师能够利用其他优化解决方案，例如
    **AWS Inferentia**、**AWS Neuron**、**分布式训练** 和 **弹性布线适配器**。有关更多信息，请随时查看以下资源：
- en: '*What is the AWS Deep Learning AMI?* ([https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.xhtml](https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.xhtml))'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*什么是 AWS 深度学习 AMI？* ([https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.xhtml](https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.xhtml))'
- en: '*How AWS Pricing Works* ([https://docs.aws.amazon.com/whitepapers/latest/how-aws-pricing-works/how-aws-pricing-works.pdf](https://docs.aws.amazon.com/whitepapers/latest/how-aws-pricing-works/how-aws-pricing-works.pdf))'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AWS 定价如何工作* ([https://docs.aws.amazon.com/whitepapers/latest/how-aws-pricing-works/how-aws-pricing-works.pdf](https://docs.aws.amazon.com/whitepapers/latest/how-aws-pricing-works/how-aws-pricing-works.pdf))'
- en: '*Elastic Fabric Adapter* ([https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-efa.xhtml](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-efa.xhtml))'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*弹性布线适配器* ([https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-efa.xhtml](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-efa.xhtml))'
