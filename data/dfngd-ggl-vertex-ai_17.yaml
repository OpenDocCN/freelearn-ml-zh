- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Natural Language Models – Detecting Fake News Articles!
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言模型 – 检测假新闻文章！
- en: A significant amount of content on the internet is in textual format. Almost
    every organization stores lots of internal data and resources as text documents.
    **Natural language processing** (**NLP**) is a subfield of machine learning that’s
    concerned with organizing, understanding, and making decisions based on textual
    input data. Over the past decade, NLP has become the utmost important aspect of
    transforming business processes and making informed decisions. For example, a
    sentiment analysis model can help a business understand the high-level sentiments
    of their customers toward their products and services. A topic modeling algorithm
    combined with sentiment analysis can figure out the key pain points of the customers
    and thus it can inform the business decisions to make customer satisfaction a
    priority.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网上大量的内容是以文本格式存在的。几乎每个组织都存储了大量的内部数据和资源作为文本文档。**自然语言处理**（**NLP**）是机器学习的一个子领域，它关注于组织、理解和基于文本输入数据做出决策。在过去十年中，NLP已经成为转变业务流程和做出明智决策的最重要方面。例如，情感分析模型可以帮助企业了解其客户对其产品和服务的整体情感。将主题建模算法与情感分析相结合可以找出客户的关键痛点，从而可以告知企业将客户满意度作为优先事项。
- en: 'In this chapter, we will develop an ML system that can recognize fake news
    articles. Such systems can help in keeping the information and news on the internet
    more accurate and safer. We will cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开发一个机器学习系统，该系统能够识别假新闻文章。这样的系统可以帮助确保互联网上的信息和新闻更加准确和安全。我们将涵盖以下主要主题：
- en: Detecting fake news using NLP
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用NLP检测假新闻
- en: Launching model training on Vertex AI
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Vertex AI上启动模型训练
- en: BERT-based fake news classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于BERT的假新闻分类
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The code samples used in this chapter can be found in this book’s GitHub repository:
    [https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter17](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter17).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的代码示例可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter17](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter17)。
- en: Detecting fake news using NLP
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NLP检测假新闻
- en: Nowadays, due to the increase in the use of the internet, it has become really
    easy to spread fake news. A large number of users are consuming and posting content
    on the internet via their social media accounts daily. It has become difficult
    to distinguish the real news from the fake news. Fake news, however, can do significant
    damage to a person, society, organization, or political party. Looking at the
    scale, it is impossible to skim through every article manually or using a human
    reviewer. Thus, there is a need to develop smart algorithms that can automatically
    detect fake news articles and stop the spread of dangerous news as soon as it
    is generated.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于互联网使用的增加，散布假新闻变得非常容易。每天都有大量用户通过他们的社交媒体账户在互联网上消费和发布内容。区分真实新闻和假新闻变得困难。然而，假新闻可以对个人、社会、组织或政党造成重大损害。从规模上看，手动或使用人工审阅员浏览每一篇文章是不可能的。因此，有必要开发智能算法，能够自动检测假新闻文章，并在假新闻生成时立即阻止其传播。
- en: ML-based classification algorithms can be used to detect fake news. First, we
    need a good training dataset to train the classification model on so that it can
    learn the common patterns of fake news and thus automatically distinguish it from
    real news. In this section, we will train an ML model to classify articles as
    “fake” versus “real.”
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基于机器学习的分类算法可以用来检测假新闻。首先，我们需要一个良好的训练数据集来训练分类模型，以便它能够学习假新闻的共同模式，从而自动将其与真实新闻区分开来。在本节中，我们将训练一个机器学习模型来将文章分类为“假”或“真”。
- en: Fake news classification with random forest
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用随机森林进行假新闻分类
- en: In this section, we will use a tree-based classification algorithm known as
    random forest to detect fake news articles. In the last section of this chapter,
    we will also train a complex deep learning-based classifier and compare the accuracy
    of both models. Let’s dive into the experiment. All the code related to these
    experiments can be found in this book’s GitHub repository, as mentioned in the
    Technical requirements section.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一种名为随机森林的基于树的分类算法来检测假新闻文章。在本章的最后部分，我们还将训练一个基于复杂深度学习的分类器，并比较两个模型的准确率。让我们开始实验。与这些实验相关的所有代码都可以在本书的GitHub仓库中找到，如技术要求部分所述。
- en: About the dataset
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于数据集
- en: We have downloaded the dataset from Kaggle, which has an open-to-use license.
    The dataset contains about 72k news articles with titles, texts, and labels. Almost
    50% of the articles are “fake,” while the remainder are “real.” We will utilize
    this dataset to train an NLP-based classification model that can detect fake news.
    We will keep some parts of this dataset as unseen data so that we can test the
    model results after training. The link for downloading the data can be found in
    the Jupyter Notebook in this book’s GitHub repository.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已从Kaggle下载了数据集，该数据集具有开放使用的许可。数据集包含约72k篇新闻文章，包括标题、文本和标签。大约50%的文章是“假新闻”，其余的是“真实新闻”。我们将利用这个数据集来训练一个基于NLP的分类模型，该模型可以检测假新闻。我们将保留数据集的一部分作为未见数据，以便在训练后测试模型结果。下载数据的链接可以在本书GitHub仓库中的Jupyter笔记本中找到。
- en: Note
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We have already downloaded and decompressed the data in the same directory as
    the Jupyter Notebook.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将数据下载并解压缩到与Jupyter笔记本相同的目录中。
- en: Now, let’s jump into the implementation part. We will start by importing useful
    Python libraries.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入实现部分。我们将从导入有用的Python库开始。
- en: Importing useful libraries
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入有用的库
- en: 'The first step is to load some useful Python libraries in a notebook cell:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是在笔记本单元格中加载一些有用的Python库：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we will load and verify the input dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将加载并验证输入数据集。
- en: Reading and verifying the data
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取和验证数据
- en: 'Here, we will read the data from a CSV file into a pandas DataFrame called
    `news_df`. We will print the shape of the DataFrame and a few top entries:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将从CSV文件中读取数据到名为`news_df`的pandas DataFrame中。我们将打印DataFrame的形状和一些顶级条目：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output of this cell is shown in *Figure 17**.1*. As we can see, there are
    72,134 news articles in this table, each with a title, body, and label:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 该单元格的输出显示在*图17.1*。正如我们所见，这个表中包含72,134篇新闻文章，每篇都有标题、正文和标签：
- en: '![Figure 17.1 – Fake news article detection dataset overview](img/B17792_17_1.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图17.1 – 假新闻文章检测数据集概述](img/B17792_17_1.jpg)'
- en: Figure 17.1 – Fake news article detection dataset overview
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1 – 假新闻文章检测数据集概述
- en: Now, let’s see if there are any missing values present in this data table.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这个数据表中是否有任何缺失值。
- en: NULL value check
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NULL值检查
- en: 'We need to check if there are any NULL values present in the dataset. There
    are different ways of handling NULL values. If the percentage of NULL values is
    very low, we can choose to drop those rows from the table; otherwise, we can fill
    those entries with some value. In our case, we will fill the NULL fields using
    an empty string value:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要检查数据集中是否存在任何NULL值。处理NULL值有不同的方法。如果NULL值的百分比非常低，我们可以选择从表中删除这些行；否则，我们可以用某个值填充这些条目。在我们的情况下，我们将使用空字符串值填充NULL字段：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here’s the output of this cell:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该单元格的输出：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As we can see, there’s a very small number of entries with NULL values. Let’s
    fill them with empty strings:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，有非常少数的条目具有NULL值。让我们用空字符串填充它们：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can now move on to data cleaning and pre-processing.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以继续进行数据清洗和预处理。
- en: Combining title and text into a single column
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将标题和文本合并到单列
- en: 'Let’s create a new column called `content` with combined `title` and `text`
    elements so that it contains all the textual information available related to
    the news article. Once we’ve done this, we will be able to use this column for
    model training and classification purposes:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个名为`content`的新列，其中包含合并的`title`和`text`元素，以便它包含有关新闻文章的所有可用文本信息。一旦我们完成这项工作，我们就可以使用这个列进行模型训练和分类：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that our textual content is in a single column, we can start cleaning and
    preparing it for the model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们文本内容在单列中，我们可以开始清洗并准备它以供模型使用。
- en: Cleaning and pre-processing data
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清洗和预处理数据
- en: ML algorithms are very sensitive to noisy data. So, it is of utmost importance
    to clean and process the data before passing it into the model for training; this
    will allow the model to learn useful information from it. As we are using a classical
    ML algorithm here, we will need to do some aggressive cleaning and pre-processing.
    In the case of deep learning, data processing is not required (as shown in the
    last section of this chapter). When we solve NLP problems using classical ML algorithms,
    we often use feature extraction methods such as TF and TF-IDF. As we know, these
    feature extraction methods are sensitive to the count of words, so it becomes
    important to remove less meaningful words (such as stopwords) and characters from
    the text.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法对噪声数据非常敏感。因此，在将数据传递到模型进行训练之前进行数据清洗和处理至关重要；这将允许模型从中学习有用的信息。由于我们在这里使用的是经典的机器学习算法，我们需要进行一些积极的清洗和预处理。在深度学习的情况下，数据预处理不是必需的（如本章最后部分所示）。当我们使用经典的机器学习算法解决NLP问题时，我们通常使用诸如TF和TF-IDF之类的特征提取方法。正如我们所知，这些特征提取方法对单词计数敏感，因此从文本中删除不太有意义的单词（如停用词）和字符变得很重要。
- en: 'In this experiment, we will follow these steps to clean and pre-process the
    data:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们将遵循以下步骤来清洗和预处理数据：
- en: Remove special characters and numbers from the text.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从文本中删除特殊字符和数字。
- en: Convert the text into lowercase (so that “HELLO” and “hello” are the same for
    the classification algorithm).
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本转换为小写（这样“HELLO”和“hello”对分类算法来说是相同的）。
- en: Split the content by space to get a list of words.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过空格分割内容以获取单词列表。
- en: Remove stopwords. These are common English words and are often meaningless in
    a sentence. Examples include they, the, and, he, and him.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除停用词。这些是常见的英语单词，在句子中通常没有意义。例如包括they、the、and、he和him。
- en: Apply stemming. This involves reducing the words to their root forms (for example,
    “happiness” should be reduced to “happy” so that different variations of the same
    word are equal for the model).
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用词干提取。这涉及到将单词还原为其基本形式（例如，“happiness”应还原为“happy”，以便相同单词的不同变体对模型来说是相同的）。
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'When we install the `nltk` library, it doesn’t automatically download all the
    required resources related to it. In our case, we will have to explicitly download
    the English language stopwords. We can do that by running the following command
    in a terminal:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们安装`nltk`库时，它不会自动下载与之相关的所有必需资源。在我们的情况下，我们将必须明确在终端中运行以下命令来下载英语语言中的停用词：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is our data cleaning and pre-processing function. As this function runs
    on the entire dataset, it takes some time to complete:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们的数据清洗和预处理函数。由于这个函数在整个数据集上运行，所以需要一些时间来完成：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, let’s separate our content and labels into arrays for modeling purposes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将内容和标签分离成数组，以便进行建模。
- en: Separating the data and labels
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分离数据和标签
- en: 'Here, we are separating the data and labels and putting them into two separate
    lists:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在分离数据和标签，并将它们放入两个单独的列表中：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, let’s convert the text into numeric values.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将文本转换为数值。
- en: Converting text into numeric data
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将文本转换为数值数据
- en: 'As ML algorithms only understand numbers, we will need to convert the textual
    data into numeric format. In our experiment, we will be creating TF-IDF features:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习算法只理解数字，我们需要将文本数据转换为数值格式。在我们的实验中，我们将创建TF-IDF特征：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can now split the data into training and test partitions so that we can test
    the results of our model after training.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将数据分成训练集和测试集，以便在训练后测试我们模型的成果。
- en: Note
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In a real NLP project, We must split the dataset into training and test sets
    before applying numerical transformations. The transformation function (such as
    `TfidfVectorizer`) should only be fit to the training data and then applied to
    test data. This is because, in a real-world setting, we might get some unknown
    words in the dataset and our model is not supposed to see those words during training.
    Another issue with this setting is that it causes data leakage as the statistics
    that are calculated over the entire dataset also belong to the test partition.
    In this example, we have done this transformation before splitting the dataset
    just for simplicity.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际的NLP项目中，在应用数值转换之前，我们必须将数据集分成训练集和测试集。转换函数（如`TfidfVectorizer`）应该仅对训练数据进行拟合，然后应用于测试数据。这是因为，在现实世界的设置中，我们可能在数据集中遇到一些未知单词，我们的模型在训练期间不应该看到这些单词。这种设置的问题还包括它会导致数据泄露，因为整个数据集上计算出的统计数据也属于测试分区。在这个例子中，我们为了简单起见，在分割数据集之前就进行了这种转换。
- en: Splitting the data
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分割
- en: 'Next, we must split the data into training and test partitions. We will use
    about 80% of the data for training and the remaining 20% for testing:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须将数据分割为训练和测试分区。我们将使用大约80%的数据进行训练，剩余的20%用于测试：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Our training and test data partitions are now ready to be fed to the model.
    Next, we’ll define the model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练和测试数据分区现在已准备好输入模型。接下来，我们将定义模型。
- en: Defining the random forest classifier
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义随机森林分类器
- en: 'For our simple experiment, we are using default hyperparameter values for the
    random forest model. However, in a real-world use case, we can experiment with
    different sets of hyperparameter values to get the best results. Alternatively,
    we can utilize hyperparameter tuning to find the best hyperparameters for our
    model:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的简单实验，我们正在使用随机森林模型的默认超参数值。然而，在实际应用中，我们可以尝试不同的超参数值组合以获得最佳结果。或者，我们可以利用超参数调优来找到我们模型的最佳超参数：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let’s go ahead and train the model on the training partition.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续在训练分区上训练模型。
- en: Training the model
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Let’s fit our model to the training dataset:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将模型拟合到训练数据集：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here’s the output:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是输出结果：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Our model training is now complete, which means we can start predicting the
    test data to check the model’s results.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已完成模型训练，这意味着我们可以开始预测测试数据以检查模型的结果。
- en: Predicting the test data
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测测试数据
- en: 'Here, we’re using our trained random forest classifier to make predictions
    on the test partition. The `predict` function gives the class-level output, while
    the `predict_proba` function gives the probabilistic outputs:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用训练好的随机森林分类器对测试分区进行预测。`predict`函数给出类别级别的输出，而`predict_proba`函数给出概率输出：
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we have made predictions on the entire set. Let’s check how well our model
    did.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经对整个数据集进行了预测。让我们看看我们的模型做得如何。
- en: Checking the results/metrics on the test dataset
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在测试数据集上检查结果/指标
- en: 'The next important step is to check and verify the performance of our model
    on the test dataset. Here, we will use sklearn’s classification report method
    to get the precision, recall, and F1 score for each class. Check out the following
    code snippet:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个重要步骤是检查并验证我们的模型在测试数据集上的性能。在这里，我们将使用sklearn的分类报告方法来获取每个类别的精确度、召回率和F1分数。查看以下代码片段：
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here is the output of the classification report:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是分类报告的输出：
- en: '[PRE17]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As we can see, our model has about 93% precision and recall for both classes.
    The overall accuracy is also about 93%. So, we can say that our model is good
    enough to identify about 93% of the fake news articles.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们的模型对于两个类别都有大约93%的精确度和召回率。总体准确率也大约是93%。因此，我们可以说我们的模型足够好，能够识别大约93%的假新闻文章。
- en: 'Next, let’s plot the ROC curve. The ROC curve is a graph between the **false
    positive rate** (**FPR**) and **true positive rate** (**TPR**) of a classification
    model:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们绘制ROC曲线。ROC曲线是分类模型的**假阳性率**（**FPR**）和**真阳性率**（**TPR**）之间的图形：
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Check out *Figure 17**.2* to see the ROC curve for our experiment. In a typical
    ROC curve, the X-axis represents the FPR and the Y-axis represents the TPR of
    the model. The **area under the ROC curve**, also known as **ROC-AUC**, indicates
    the quality of a classification model. Having a higher area value signifies a
    better model:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 查看*图17.2*以了解我们实验的ROC曲线。在典型的ROC曲线中，X轴代表FPR，Y轴代表TPR。**ROC曲线下的面积**，也称为**ROC-AUC**，表示分类模型的质量。面积值越高，表示模型越好：
- en: '![Figure 17.2 – The ROC curve for the fake news classification model](img/B17792_17_2.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图17.2 – 假新闻分类模型的ROC曲线](img/B17792_17_2.jpg)'
- en: Figure 17.2 – The ROC curve for the fake news classification model
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.2 – 假新闻分类模型的ROC曲线
- en: Let’s also check out the confusion matrix to see where our model is making mistakes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也查看混淆矩阵，看看我们的模型在哪里出错。
- en: Confusion matrix
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: 'Finally, let’s also print the confusion matrix for our classification. A confusion
    matrix shows the number of correct and incorrect classifications of each class.
    It also shows which other classes were predicted as mistakes if the classification
    is wrong (false positives and false negatives):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们也打印出我们分类的混淆矩阵。混淆矩阵显示了每个类别的正确和错误分类的数量。它还显示了如果分类错误（假阳性和假阴性）的话，哪些其他类别被预测为错误：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here’s the output:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是输出结果：
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Our experiment is now complete. If the results are satisfactory, we can go ahead
    and deploy this model as an API. If the results are still not acceptable, we can
    do more experiments with different settings.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验现在已经完成。如果结果令人满意，我们可以继续部署此模型作为API。如果结果仍然不可接受，我们可以进行更多不同设置的实验。
- en: If we want to do a lot of experiments in parallel, we can launch many parallel
    experiments via Vertex AI training jobs without needing to monitor them constantly
    and check back later when training is complete. In the next section, we will see
    how Vertex AI training jobs can be configured.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要并行进行大量实验，我们可以通过Vertex AI训练作业启动许多并行实验，而无需不断监控它们，并在训练完成后稍后再进行检查。在下一节中，我们将看到如何配置Vertex
    AI训练作业。
- en: Launching model training on Vertex AI
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Vertex AI上启动模型训练。
- en: 'In this section, we will launch our training experiment as a Vertex AI training
    job. There are multiple advantages of launching training jobs on Vertex AI instead
    of doing it in a Juypter Notebook:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将以Vertex AI训练作业的形式启动我们的训练实验。与在Jupyter Notebook中执行相比，在Vertex AI上启动训练作业有多个优点：
- en: The flexibility to launch any number of parallel experiments
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以灵活地启动任意数量的并行实验。
- en: We can choose the best hardware for model training, which is very important
    when accelerators are needed to train deep learning models.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以选择最佳的硬件进行模型训练，这在需要训练深度学习模型时非常重要。
- en: We don’t need active monitoring regarding training progress
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不需要对训练进度进行主动监控。
- en: There’s no fear of the Jupyter Notebook crashing
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有Jupyter Notebook崩溃的担忧。
- en: Vertex AI training jobs can be configured to log metadata and experiments in
    the Google Cloud Console UI
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vertex AI训练作业可以配置为在Google Cloud Console UI中记录元数据和实验。
- en: In this section, we will create and launch a Vertex AI training job for our
    experiment. There are two main things we need to do to launch a Vertex AI training
    job. First, we need to put the dataset in a location that will be accessible to
    the Vertex AI job (such as GCS or BigQuery). Second, we need to put the model
    training code together into a single `task.py` file so that it can be packaged
    into a training container with all the necessary dependencies.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本节中，我们将为我们的实验创建并启动一个Vertex AI训练作业。启动Vertex AI训练作业主要有两件事要做。首先，我们需要将数据集放置在一个Vertex
    AI作业可访问的位置（例如GCS或BigQuery）。其次，我们需要将模型训练代码整理到一个单独的`task.py`文件中，以便将其打包到包含所有必要依赖项的训练容器中。
- en: 'Here are the steps we need to follow to create and launch our Vertex AI training
    job:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们需要遵循的步骤来创建和启动我们的Vertex AI训练作业：
- en: Upload the dataset to GCS or BigQuery (we will use GCS).
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集上传到GCS或BigQuery（我们将使用GCS）。
- en: 'Create a `task.py` file that does the following:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个执行以下操作的`task.py`文件：
- en: Reads data from GCS
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从GCS读取数据。
- en: Does the necessary data preparation
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行必要的数据准备。
- en: Trains the RF model
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练RF模型。
- en: Saves the trained model into GCS
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将训练好的模型保存到GCS。
- en: Does prediction on the test set
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在测试集上进行预测。
- en: (Optionally) Saves predictions to GCS
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: （可选）将预测保存到GCS。
- en: Prints some results/metrics
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打印一些结果/指标。
- en: Use a prebuilt training image.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预构建的训练镜像。
- en: Launch Vertex AI training.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Vertex AI训练。
- en: Monitor the progress on the Google Cloud Console UI.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Cloud Console UI上监控进度。
- en: Considering these steps, we have already created a `task.py` file for our experiment;
    it can be found in this book’s GitHub repository. Next, we will learn how to launch
    the job using this `task.py` file.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些步骤，我们已经在我们的实验中创建了一个`task.py`文件；它可以在本书的GitHub仓库中找到。接下来，我们将学习如何使用这个`task.py`文件来启动作业。
- en: Setting configurations
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置配置。
- en: 'Here, we will define the configurations related to the project and data locations
    that will be necessary while launching the training job on Vertex AI. The following
    snippet shows some configurations related to our experiment:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将定义在Vertex AI上启动训练作业时必要的与项目和数据位置相关的配置。以下代码片段显示了与我们实验相关的某些配置：
- en: '[PRE21]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Let’s initialize the Vertex AI SDK with appropriate variables.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用适当的变量初始化Vertex AI SDK。
- en: Initializing the Vertex AI SDK
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化Vertex AI SDK。
- en: 'Here, we’re initializing the Vertex AI SDK to set the project, location, and
    staging bucket for our jobs:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在初始化Vertex AI SDK以设置作业的项目、位置和暂存存储桶：
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now that our configurations have been set, we can start defining the Vertex
    AI training job.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了配置，我们可以开始定义Vertex AI训练作业。
- en: Defining the Vertex AI training job
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义Vertex AI训练作业。
- en: 'The following code block defines a Vertex AI training job for our experiment.
    Here, we pass `display_name`, which will help us locate our job within the console
    UI. Note that we are passing our `task.py` file as the script path variable. `container_uri`
    is the prebuilt container that will be used to launch the job. Finally, we can
    specify any additional Python packages that are required to run our training code.
    In our case, we need to install the `nltk` package for some NLP-related functionalities:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块定义了我们的实验的Vertex AI训练作业。在这里，我们传递`display_name`，这将帮助我们定位控制台UI中的作业。请注意，我们正在将我们的`task.py`文件作为脚本路径变量传递。`container_uri`是用于启动作业的预构建容器。最后，我们可以指定运行我们的训练代码所需的任何额外的Python包。在我们的情况下，我们需要安装`nltk`包以实现一些NLP相关的功能：
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Our Vertex AI-based custom training job is now ready. Let’s run it.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于Vertex AI的自定义训练作业现在已准备就绪。让我们运行它。
- en: Running the Vertex AI job
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行Vertex AI作业
- en: 'We are all set to launch our training job. We are using an `n1-standard-16`
    type of machine for our experiment that can be modified as per our needs. Check
    out the following snippet, which launches our training job on Vertex AI:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好启动我们的训练作业。我们使用`n1-standard-16`类型的机器进行实验，可以根据我们的需求进行修改。查看以下代码片段，它将在Vertex
    AI上启动我们的训练作业：
- en: '[PRE24]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'After launching the job, we should see a URL in the output pointing to the
    job within the Cloud Console UI. The output should look something like this:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 启动作业后，我们应该在输出中看到一个URL，指向云控制台UI中的作业。输出应该看起来像这样：
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: With that, we have successfully launched our experiment as a training job on
    Vertex. We can now monitor the progress of our job using the Cloud Console UI.
    Next, we’ll solve this problem using a deep learning approach so that we hopefully
    get better results.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们已经成功地将我们的实验作为一个训练作业在Vertex上启动。现在我们可以使用云控制台UI监控作业的进度。接下来，我们将使用深度学习方法来解决这个问题，希望得到更好的结果。
- en: BERT-based fake news classification
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于BERT的虚假新闻分类
- en: In our first experiment, we trained a classical random forest classifier on
    TF-IDF features to detect fake versus real news articles and got an accuracy score
    of about 93%. In this section, we will train a deep learning model for the same
    task and see if we get any accuracy gains over the classical tree-based approach.
    Deep learning has changed the way we used to solve NLP problems. Classical approaches
    required hand-crafted features, most of which were related to the frequency of
    words appearing in a document. Looking at the complexity of languages, just knowing
    the count of words in a paragraph is not enough. The order in which words occur
    also has a significant impact on the overall meaning of the paragraph or sentence.
    Deep learning approaches such as **Long-Short-Term-Memory** (**LSTM**) also consider
    the sequential dependency of words in sentences or paragraphs to get a more meaningful
    feature representation. LSTM has achieved great success in many NLP tasks but
    there have been some limitations. As these models are trained sequentially, it
    becomes really difficult to scale these models. Secondly, when we work with very
    long sequences, LSTMs suffer from context loss and thus they are not ideal for
    understanding the context of longer sequences. Due to some limitations, including
    the ones discussed here, new ways of learning context from sequential inputs were
    invented.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一个实验中，我们使用TF-IDF特征在经典随机森林分类器上进行训练，以检测虚假新闻与真实新闻文章，并获得了大约93%的准确率。在本节中，我们将训练一个深度学习模型来完成相同的任务，并看看我们是否能比经典的基于树的算法获得更高的准确率。深度学习已经改变了我们解决自然语言处理（NLP）问题的方式。经典方法需要手工制作的特征，其中大部分与文档中出现的单词频率有关。考虑到语言的复杂性，仅仅知道段落中单词的数量是不够的。单词出现的顺序也对段落或句子的整体意义有显著影响。深度学习方法，如**长短期记忆网络（LSTM**）也考虑了句子或段落中单词的顺序依赖性，以获得更有意义的特征表示。LSTM在许多NLP任务中取得了巨大成功，但也存在一些局限性。由于这些模型是按顺序训练的，因此很难扩展这些模型。其次，当我们处理非常长的序列时，LSTMs会遭受上下文丢失，因此它们不适合理解较长序列的上下文。由于一些局限性，包括这里讨论的局限性，人们发明了从顺序输入中学习上下文的新方法。
- en: The advent of transformer-based models was groundbreaking for the field of NLP,
    as well as Vision AI. Transformer-based models heavily rely on attention mechanisms
    to capture the context and inter-sequence patterns, and they are also able to
    handle very long sequences of inputs. **Bidirectional Encoder Representations
    from Transformers** (**BERT**) is a family of NLP models that are based on some
    parts of the transformer architecture. BERT-based models have achieved great success
    in tons of NLP tasks, some of which seemed close to impossible in past decades.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 基于转换器的模型的出现对自然语言处理（NLP）和视觉人工智能领域具有里程碑意义。基于转换器的模型严重依赖于注意力机制来捕捉上下文和序列间的模式，并且它们也能够处理非常长的输入序列。**来自转换器的双向编码器表示**（**BERT**）是一系列基于转换器架构一部分的
    NLP 模型。基于 BERT 的模型在众多 NLP 任务中取得了巨大成功，其中一些在过去几十年中似乎几乎不可能实现。
- en: Another advantage of working with deep learning models is that we don’t have
    to train them from scratch every time. We always utilize pre-trained models and
    then fine-tune them on our domain-specific data to get great results faster and
    without requiring a lot of domain-specific training data. This approach is termed
    **transfer learning** and is where large deep learning models are pre-trained
    with huge amounts of data, after which they can be utilized for many downstream
    domain-specific tasks as they can be fine-tuned with a small amount of domain-specific
    training data.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与深度学习模型合作的一个优点是，我们不必每次都从头开始训练它们。我们总是利用预训练模型，然后在我们的特定领域数据上微调它们，以更快地获得出色的结果，而且不需要大量的特定领域训练数据。这种方法被称为**迁移学习**，这是大型深度学习模型在大量数据上预训练后，可以用于许多下游特定领域任务，因为它们可以用少量特定领域训练数据进行微调。
- en: BERT for fake news classification
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT 用于假新闻分类
- en: In this experiment, we will utilize a pre-trained BERT model and fine-tune it
    slightly on our news article training dataset. Let’s get started.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们将利用一个预训练的 BERT 模型，并在我们的新闻文章训练数据集上稍作微调。让我们开始吧。
- en: Importing useful libraries
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入有用的库
- en: 'In this experiment, we will utilize PyTorch as a framework for fine-tuning
    the BERT model. We also utilize the `transformers` library from Hugging Face to
    load the pre-trained weights of the BERT-based model with some other tooling that
    is useful for setting up fine-tuning:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们将利用 PyTorch 作为微调 BERT 模型的框架。我们还利用 Hugging Face 的 `transformers` 库来加载基于
    BERT 的模型的预训练权重，以及一些用于设置微调的有用工具：
- en: '[PRE26]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now, let’s start preparing the dataset.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始准备数据集。
- en: The dataset
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: 'We will work with the same dataset that we used in the first experiment. So,
    we will follow the same steps here as well – we will load the data, treat NULL
    values, and create a content column with all the necessary text:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与第一个实验中相同的同一个数据集。因此，我们在这里也将遵循相同的步骤——我们将加载数据，处理 NULL 值，并创建一个包含所有必要文本的内容列：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we will convert the text in the content column to lowercase:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将内容列中的文本转换为小写：
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, we will separate text from labels and store them as lists:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将文本和标签分开，并将它们存储为列表：
- en: '[PRE29]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here’s the output:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: 72134 72134
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 72134 72134
- en: Now, let’s prepare our dataset as per the requirements of BERT model inputs.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们根据 BERT 模型输入的要求准备我们的数据集。
- en: Data preparation
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: As we are working with the BERT model now, we don’t need to perform lots of
    data cleanups, such as removing numbers, removing stopwords, stemming, and so
    on. Each BERT model has a tokenizer that is utilized to convert textual data into
    numeric IDs. So, we will need to find the appropriate BERT tokenizer (which can
    be loaded by the `transformers` library from Hugging Face), do tokenization, and
    also create attention masks for training purposes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在正在使用 BERT 模型，我们不需要执行大量的数据清理，例如删除数字、删除停用词、词干提取等。每个 BERT 模型都有一个标记器，用于将文本数据转换为数字
    ID。因此，我们需要找到适当的 BERT 标记器（可以通过 Hugging Face 的 `transformers` 库加载），进行标记化，并为训练目的创建注意力掩码。
- en: 'Let’s create the tokenizer object:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建标记器对象：
- en: '[PRE30]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here, we’re defining a function that will create tokenized text and attention
    masks for training:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个函数，该函数将创建用于训练的标记化文本和注意力掩码：
- en: '[PRE31]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we must generate encodings and attention masks for each input text:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须为每个输入文本生成编码和注意力掩码：
- en: '[PRE32]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, let’s convert the lists into PyTorch tensors:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将列表转换为 PyTorch 张量：
- en: '[PRE33]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here, we’re calling the necessary method, to prepare our data:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们调用必要的函数，以准备我们的数据：
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now, let’s split our data into training and test partitions.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将我们的数据分成训练和测试部分。
- en: Splitting the data
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分割
- en: 'Here, we will create a tensor dataset out of our input IDs, attention masks,
    and labels and split it into training and test sets. Similar to the first experiment,
    we will be using about 80% of the data for training (or fine-tuning) purposes
    and the remaining 20% to test the results and metrics. The following snippet shows
    how to create and split the tensor dataset:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将从输入ID、注意力掩码和标签创建一个张量数据集，并将其分为训练集和测试集。类似于第一个实验，我们将使用大约80%的数据进行训练（或微调）目的，剩余的20%用于测试结果和指标。以下代码片段显示了如何创建和分割张量数据集：
- en: '[PRE35]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here’s the output:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '[PRE36]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now, let’s define data loader objects with the required batch size.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义具有所需批量大小的数据加载器对象。
- en: Creating data loader objects for batching
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建用于批处理的数据加载器对象
- en: 'The next step is to create data loader objects for both the training and test
    partitions. We will have a batch size of 32 for the training data and a batch
    size of 1 for the test data:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是为训练和测试分区创建数据加载器对象。我们将为训练数据设置32个批大小，为测试数据设置1个批大小：
- en: '[PRE37]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Our data is now ready for the model. This means we can load the model and start
    training.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据现在已准备好用于模型。这意味着我们可以加载模型并开始训练。
- en: Loading the pre-trained BERT model
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载预训练的BERT模型
- en: 'Here, we will load the pre-trained weights of the BERT-based model so that
    we can fine-tune it further on our custom dataset. The pre-trained weights of
    many BERT variants are available on Hugging Face and can be loaded through the
    `transformers` library, as shown in the following snippet:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将加载基于BERT的模型的预训练权重，以便我们可以在自定义数据集上进一步微调。许多BERT变体的预训练权重都可在Hugging Face上找到，并且可以通过`transformers`库加载，如下面的代码片段所示：
- en: '[PRE38]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Using the `device` variable, we can choose to load our model on an accelerator,
    such as a GPU. This snippet downloads the pre-trained weights of the `bert-base-uncased`
    model with a classification layer of two labels. Executing this snippet also prints
    the BERT architecture summary, which will look something similar to the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`device`变量，我们可以选择在加速器上加载我们的模型，例如GPU。此代码片段下载了具有两个标签分类层的`bert-base-uncased`模型的预训练权重。执行此代码片段还会打印BERT架构摘要，其外观类似于以下内容：
- en: '[PRE39]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now that our model has been loaded, let’s define the optimization settings.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的模型已经加载，让我们定义优化设置。
- en: Optimizer
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器
- en: 'Here, we’re defining the `AdamW` optimizer and setting a custom learning rate:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了`AdamW`优化器并设置了自定义学习率：
- en: '[PRE40]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Let’s also define a scheduler for our model training.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也为模型训练定义一个调度器。
- en: Scheduler
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调度器
- en: 'Here, we’re setting up training steps and a training scheduler. We’re planning
    to fine-tune our model for just `3` epochs on our training partition, after which
    we will check the results on our test set:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们设置训练步骤和训练调度器。我们计划在我们的训练分区上微调模型仅`3`个周期，之后我们将检查测试集上的结果：
- en: '[PRE41]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Now, we are all set to start training the model.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好开始训练模型。
- en: Training BERT
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练BERT
- en: 'Here, we will fine-tune the BERT model on our training data for `3` epochs,
    as defined in the previous sub-section:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将根据前一个子节中定义的`3`个周期，在训练数据上微调BERT模型：
- en: '[PRE42]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This training snippet prints the model loss after each epoch of training is
    completed. The loss output from our experiment is as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 此训练代码片段在每个训练周期完成后打印模型损失。我们的实验的损失输出如下：
- en: '[PRE43]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now that our model training (or fine-tuning) is complete, we can save the model
    weights:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的模型训练（或微调）已完成，我们可以保存模型权重：
- en: '[PRE44]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: With that, we have successfully trained and saved the model. Now, let’s move
    on to model evaluation.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们已经成功训练并保存了模型。现在，让我们继续进行模型评估。
- en: Loading model weights for evaluation
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载模型权重进行评估
- en: 'Here, we’re loading our trained model weights for evaluation purposes:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们加载我们的训练模型权重以进行评估：
- en: '[PRE45]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Let’s check the accuracy of the trained model on the test dataset.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查训练模型在测试数据集上的准确率。
- en: Calculating the accuracy of the test dataset
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算测试数据集的准确率
- en: Now that our model has been trained and loaded for evaluation, we can make predictions
    on the test dataset and check its accuracy.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的模型已经训练并加载用于评估，我们可以在测试数据集上进行预测并检查其准确率。
- en: 'We will store the predictions in lists and also count the number of correct
    predictions in the following variables:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下变量中存储预测结果，并计算正确预测的数量：
- en: '[PRE46]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Here, we’re running model predictions on the test data:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们在测试数据上运行模型预测：
- en: '[PRE47]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can calculate the accuracy by dividing the correct predictions by the total
    predictions:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将正确预测的数量除以总预测数量来计算准确率：
- en: '[PRE48]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output of this cell shows that our model has about 99% accuracy on the
    test dataset. This is a huge improvement over the classic random forest model:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这个单元格的输出显示，我们的模型在测试数据集上的准确率约为99%。这比经典的随机森林模型有了巨大的改进：
- en: '[PRE49]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Finally, let’s print the confusion matrix:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们打印出混淆矩阵：
- en: '[PRE50]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Here is the output:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是输出结果：
- en: '[PRE51]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Now, we can generate a classification report for our model.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以为我们的模型生成一个分类报告。
- en: Classification report
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类报告
- en: 'Finally, we will print the classification report of our experiment to understand
    the precision, recall, and F1 score of each class:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将打印出我们实验的分类报告，以了解每个类别的精确率、召回率和F1分数：
- en: '[PRE52]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Here’s the output:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是输出结果：
- en: '[PRE53]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The preceding output shows that our BERT-based classification model is extremely
    accurate with an accuracy of about 99%. Similarly, for each class, we have precision
    and recall scores of about 99%. This experiment showed that using a pre-trained
    deep learning model can enhance the accuracy of classification by a great margin.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出表明，我们的基于BERT的分类模型非常准确，准确率约为99%。同样，对于每个类别，我们都有大约99%的精确率和召回率。这个实验表明，使用预训练的深度学习模型可以大幅度提高分类的准确率。
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter was about a real-world NLP use case for detecting fake news. In
    the current era of the internet, spreading fake news has become quite easy and
    it can be dangerous for the reputation of a person, society, organization, or
    political party. As we have seen in our experiments, ML classification can be
    used as a powerful tool for detecting fake news articles. Deep learning-based
    approaches can further improve the results of text classification use cases without
    requiring much fine-tuning data.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了关于检测假新闻的真实世界NLP应用案例。在当前互联网时代，传播假新闻变得相当容易，这对个人、社会、组织或政党的声誉可能构成危险。正如我们在实验中所看到的，机器学习分类可以被用作检测假新闻文章的强大工具。基于深度学习的方案可以进一步提高文本分类用例的结果，而无需进行大量的微调数据。
- en: After reading this chapter, you should be confident about training and applying
    classification models on text classification use cases, similar to fake news detection.
    You should also have a good understanding of the cleaning and pre-processing steps
    that are needed to apply classical models, such as random forest, on text data.
    At this point, you should be able to launch large-scale ML experiments as Vertex
    AI training jobs. Finally, you should have a good understanding of how deep learning-based
    BERT models can be applied and fine-tuned for text classification use cases.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本章之后，你应该对在文本分类用例（如假新闻检测）中训练和应用分类模型充满信心。你也应该对应用经典模型（如随机森林）所需的清理和预处理步骤有很好的理解。此时，你应该能够启动大规模机器学习实验作为Vertex
    AI训练作业。最后，你应该对如何应用和微调基于深度学习的BERT模型以用于文本分类用例有很好的理解。
