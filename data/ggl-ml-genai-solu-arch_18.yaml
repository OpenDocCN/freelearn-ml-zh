- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Introduction to Generative AI
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式人工智能简介
- en: '**Generative artificial intelligence** (**GenAI**) is certainly the term on
    everybody’s lips at the moment. If you haven’t already had an opportunity to “get
    your hands dirty” with GenAI, then you will soon learn why it is currently taking
    the world by storm as we dive into the kinds of amazing things we can do with
    this relatively new set of technologies. In this chapter, we will explore some
    of the concepts underpinning what GenAI is and its distinctions from other **artificial
    intelligence** (**AI**)/**machine learning** (**ML**) approaches. We’ll also cover
    some of the major historical developments that have led to its meteoric rise,
    and examples of how it is being used today.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成式人工智能**（**GenAI**）无疑是现在大家都在谈论的术语。如果你还没有机会“亲身体验”GenAI，那么你很快就会了解到为什么它目前正席卷全球，当我们深入探讨我们可以用这套相对较新的技术做些什么时。在本章中，我们将探讨支撑GenAI是什么以及它与其他**人工智能**（**AI**）/**机器学习**（**ML**）方法的区别的一些概念。我们还将涵盖一些导致其迅速崛起的主要历史发展，以及它今天是如何被使用的例子。'
- en: 'We’ll begin the chapter by introducing some fundamental concepts before moving
    on to more complex topics and the evolution of various GenAI approaches, such
    as **autoencoders** (**AEs**), **generative adversarial networks** (**GANs**),
    diffusion, and **large language models** (**LLMs**). Considering that this is
    an introductory chapter, we will mainly lay the groundwork for the deeper dives
    that will follow in later chapters. Specifically, this chapter covers the following
    topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍一些基本概念，然后进入更复杂的话题以及各种生成式人工智能（GenAI）方法的演变，例如**自编码器**（**AEs**）、**生成对抗网络**（**GANs**）、扩散和**大型语言模型**（**LLMs**）。鉴于这是一个入门章节，我们将主要为后续章节中更深入的探讨打下基础。具体来说，本章涵盖了以下主题：
- en: Fundamentals of GenAI
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GenAI的基础知识
- en: GenAI techniques and evolution
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GenAI技术和演变
- en: LLMs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs
- en: As makes logical sense, let’s begin with the fundamentals!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 按照逻辑顺序，让我们从基础知识开始！
- en: Fundamentals of GenAI
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GenAI的基础知识
- en: This section introduces the basic concepts we need to understand when discussing
    GenAI, starting with an overview of what GenAI is!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了在讨论GenAI时我们需要理解的基本概念，从对GenAI的概述开始！
- en: What is GenAI?
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是GenAI？
- en: I’ll begin explaining this topic by focusing on what distinguishes GenAI from
    the other AI/ML approaches we’ve covered. Think about all of the various algorithms
    and approaches I’ve described so far throughout this book, and, more specifically,
    think about the primary goal that was being pursued in each approach. Whether
    we were using linear regression in `scikit-learn` to predict a numeric value for
    some target variable based on its features, logistic regression in XGBoost to
    implement a binary classifier model or using time-series data to predict the future
    in TensorFlow, there is one common theme, which is that we were trying to **predict**
    or **estimate** something.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我将通过关注区分GenAI与其他我们已讨论过的AI/ML方法的特点来开始解释这个主题。想想这本书中我描述的所有各种算法和方法，更具体地说，想想每种方法追求的主要目标。无论是我们使用`scikit-learn`中的线性回归来根据特征预测某个目标变量的数值，还是在XGBoost中使用逻辑回归来实现二元分类模型，或者在TensorFlow中使用时间序列数据来预测未来，都有一个共同的主题，那就是我们试图**预测**或**估计**某事。
- en: The key concept to understand here is that the output from our various sophisticated
    models was generally a distinct data point that was either right or wrong, or
    at least as close to right as possible. Our models typically produced a single,
    simple answer based on relationships the model had learned (or estimated) in the
    data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里要理解的关键概念是，我们各种复杂模型的输出通常是一个明确的数据点，要么是正确的，要么是错误的，或者至少尽可能接近正确。我们的模型通常基于模型在数据中学习（或估计）的关系产生一个单一、简单的答案。
- en: Even in the case of **unsupervised learning** (**UL**) approaches such as K-means
    clustering, the model simply finds mathematical relationships in the data and
    categorizes the data points into groups based on those relationships.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是在像K-means聚类这样的**无监督学习**（**UL**）方法中，模型也只是在数据中找到数学关系，并根据这些关系将数据点分类到不同的组中。
- en: The huge leap forward that has been achieved with the introduction of GenAI
    is that the models can now go beyond simple “yes”/“no” answers or numeric estimates
    based on sheer mathematical number crunching and pattern recognition. With GenAI,
    the models can now create (or **generate**) new data. That’s the big difference,
    and it turns out that the implications of this are pretty huge!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通用人工智能的引入所带来的巨大飞跃是，模型现在可以超越简单的“是”/“否”答案或基于纯粹数学计算和模式识别的数值估计。有了通用人工智能，模型现在可以创建（或**生成**）新的数据。这就是最大的区别，而且这一点的意义相当巨大！
- en: 'Taking a step back for a moment: when I was young, I thought that AI research
    could design machines that think like humans, and that fascinated me, so I started
    learning about how ML algorithms work. I was quite disappointed to learn that,
    although some models can do an amazing job at “understanding” consumer behavior
    and accurately recommending products that a given person might like to purchase,
    or even diagnose potential illnesses based on input data related to a patient,
    it was all just based on feeding large amounts of data into a mathematical algorithm
    that “learns” to detect a pattern in the data. There was no actual “intelligence”
    there, although the science is still fascinating.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 退一步来说：当我年轻的时候，我以为人工智能研究可以设计出像人类一样思考的机器，这让我着迷，所以我开始学习机器学习算法是如何工作的。我相当失望地发现，尽管一些模型可以在“理解”消费者行为和准确推荐某人可能喜欢购买的产品方面做得非常出色，或者甚至根据与患者相关的输入数据诊断潜在的疾病，但这一切都是基于将大量数据输入到“学习”在数据中检测模式的数学算法中。那里并没有真正的“智能”，尽管科学仍然令人着迷。
- en: The fact that GenAI models can go beyond specific mathematical answers and create
    new content is a significant leap forward in the pursuit of AI. Have a chat with
    any of the latest and greatest GenAI models out there, and I don’t doubt for a
    moment that you will be very impressed by the kinds of mind-boggling things they
    can do, such as composing music, painting an imaginative scene, writing a catchy
    poem, or creating a web application. You’ll learn how to harness GenAI for implementing
    many different kinds of use cases later in this book, and I’m sure you’ll agree
    that it’s a dramatic technological advancement.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通用人工智能（GenAI）模型能够超越特定的数学答案并创造新内容，这是在人工智能追求过程中的一大飞跃。与任何最新的顶级通用人工智能模型进行一次对话，我毫不怀疑你将会非常震撼于它们能够做到令人惊叹的事情，比如创作音乐、描绘一个富有想象力的场景、写一首吸引人的诗，或者创建一个网络应用程序。你将在本书的后面部分学习如何利用通用人工智能来实现许多不同的用例，我相信你也会同意这是一项重大的技术进步。
- en: It’s important to understand, however, that the amazing feats performed by GenAI
    models are still powered by many of the AI/ML concepts we have already covered
    in this book. At its core, GenAI works by understanding and replicating the underlying
    patterns and structures in the data it has been trained on, and it still uses
    algorithms and **neural networks** (**NNs**) to perform these kinds of activities,
    although the network architectures used, and the novel ways in which they are
    used, are considerably more advanced. In the case of GenAI, the goal is not just
    to interpret the data but to form an understanding that can be used to create
    something new. Let’s dive into these topics in more detail to better understand
    what sets GenAI apart from other AI approaches.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，重要的是要理解，通用人工智能模型所完成的惊人壮举仍然依赖于我们在这本书中已经涵盖的许多人工智能/机器学习概念。在核心上，通用人工智能通过理解和复制其训练数据中的底层模式和结构来工作，它仍然使用算法和**神经网络**（**NNs**）来执行这些活动，尽管所使用的网络架构以及它们的使用方式都相当先进。在通用人工智能的情况下，目标不仅仅是解释数据，而是形成可以用来创造新事物的理解。让我们更深入地探讨这些主题，以更好地理解什么是使通用人工智能与其他人工智能方法不同的。
- en: What is non-GenAI?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是非通用人工智能？
- en: Now that GenAI has taken the world by storm, how can we refer to all of the
    other AI/ML approaches that existed before it (that is, much of the stuff we already
    covered in this book)? One of the most popular emerging terms for this is “traditional
    AI/ML.” You may also hear it being referred to as “predictive AI/ML” because the
    goal is generally to predict something, or “discriminative AI/ML” in the case
    of classification use cases.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在通用人工智能已经席卷全球，我们如何称呼它之前的所有人工智能/机器学习方法（即，本书中我们已经涵盖的大部分内容）？最流行的术语之一是“传统人工智能/机器学习”。你也可能听到它被称为“预测人工智能/机器学习”，因为目标通常是预测某事，或者在分类用例中被称为“判别人工智能/机器学习”。
- en: Diving deeper into GenAI versus non-GenAI
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨通用人工智能与非通用人工智能的区别
- en: To understand the basic differences between GenAI and non-GenAI, we need to
    revisit some of the mathematical concepts that underpin AI/ML. Don’t worry – we’ll
    just cover the concepts at a level required to define the distinctions between
    GenAI and non-GenAI.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解生成式AI（GenAI）与非生成式AI之间的基本区别，我们需要回顾一些支撑AI/ML的数学概念。别担心——我们只需覆盖到定义生成式AI与非生成式AI之间的区别所需的概念水平。
- en: The role of probability
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概率的角色
- en: Remember that mathematical probability is a fundamental concept in many ML use
    cases. For example, when we ask a model to classify a data point into a specific
    category, it rarely will do so with 100% certainty. Instead, it will calculate
    the probability of the data point belonging to each category. Then we, or the
    model itself, depending on the implementation, can select the category with the
    highest probability.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，数学概率是许多机器学习（ML）用例中的基本概念。例如，当我们要求一个模型将数据点分类到特定类别时，它很少会以100%的确定性这样做。相反，它会计算数据点属于每个类别的概率。然后我们，或者模型本身，根据实现方式，可以选择概率最高的类别。
- en: One of the key factors that distinguishes GenAI from traditional AI is how probability
    is used in the learning process. Let’s explore this in more detail, starting with
    traditional AI.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 区分生成式AI（GenAI）与传统AI的一个关键因素是概率在学习过程中的使用方式。让我们更详细地探讨这一点，从传统AI开始。
- en: Traditional AI and conditional probability
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 传统AI和条件概率
- en: 'In the case of traditional AI, the models try to estimate the probability that
    the target variable `Y` contains a specific value based on the values of the predictor
    variables (or features) in the dataset. This is referred to as the **conditional
    probability** of the target variable’s values based on the values of the input
    features. Conditional probability is the probability of an event occurring given
    that another event has already happened, and it is represented by the following
    formula:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统AI的情况下，模型试图根据数据集中预测变量（或特征）的值来估计目标变量`Y`包含特定值的概率。这被称为基于输入特征值的目标变量值的**条件概率**。条件概率是在另一个事件已经发生的情况下发生事件的概率，它由以下公式表示：
- en: P(B|A) = P(A∩B) / P(A)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: P(B|A) = P(A∩B) / P(A)
- en: 'Here, the following applies:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，以下规则适用：
- en: P(B|A) is the conditional probability of B given A.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(B|A) 是在A的条件下B的条件概率。
- en: P(A ∩ B) is the probability of both A and B occurring. This is also referred
    to as the “joint probability,” which we will explore in more detail later.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(A ∩ B) 是事件A和事件B同时发生的概率。这也被称为“联合概率”，我们将在稍后更详细地探讨。
- en: P(A) is the probability of A occurring.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(A) 是事件A发生的概率。
- en: To understand conditional probability, imagine a game scenario in which there
    are three straws hidden from our view. Of the three straws, two are long straws,
    and one is a short straw. We will take turns picking straws, and whoever picks
    the short straw loses. Each time it’s our turn to pick a straw, there’s a specific
    probability that we will pick the short straw.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解条件概率，可以想象一个游戏场景，其中有三根稻草隐藏在我们的视线之外。在这三根稻草中，两根是长稻草，一根是短稻草。我们将轮流抽稻草，抽到短稻草的人输。每次轮到我们抽稻草时，都有特定的概率我们会抽到短稻草。
- en: Initially, no straws were picked, so in the first turn, the probability of picking
    the short straw is 1/3 (or approximately 33.3%). Now, let’s imagine that we pick
    a straw, and it turns out to be a long straw. This means that two straws remain,
    and since we picked a long straw, it means that one long straw remains, as well
    as the short straw. In the next turn, the probability of picking the short straw
    is, therefore, 1/2 (or 50%).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时没有抽稻草，所以在第一轮，抽到短稻草的概率是1/3（或大约33.3%）。现在，让我们想象我们抽了一根稻草，结果是一根长稻草。这意味着还剩下两根稻草，由于我们抽到了长稻草，这意味着还剩下一根长稻草以及一根短稻草。在下一轮，抽到短稻草的概率因此是1/2（或50%）。
- en: The important thing to note here is that since we already picked a long straw,
    it influences the probability distribution of the overall scenario. This is a
    very simple example of the concept. In ML, there can be many factors (that is,
    features) involved, leading to very large numbers of potential combinations of
    those factors that could influence the outcome. This is why ML models usually
    need to crunch through large amounts of data to learn patterns in those various
    combinations of features.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的重要一点是，因为我们已经选择了一根长吸管，它会影响整个场景的概率分布。这是一个非常简单的概念示例。在ML中，可能涉及许多因素（即特征），导致可能影响结果的因素组合数量非常大。这就是为什么ML模型通常需要处理大量数据来学习这些特征组合中的模式。
- en: 'Mapping this back to ML use cases, consider the dataset represented in *Figure
    15**.1*, which shows a simplified version of the Titanic dataset from OpenML ([https://www.openml.org/search?type=data&id=40945](https://www.openml.org/search?type=data&id=40945)).
    The target variable is represented by the `survived` column, which is highlighted
    by the green box. This represents *B* in our preceding equation. All of the other
    columns combined constitute the input features, and they represent *A* in our
    previous equation, as highlighted by the red box. Essentially, this depicts that
    when an ML model learns to predict the value of the target variable based on the
    values of the input variables, it is asking the question: “What is the probability
    of *B* (that is, the target variable), given the values of *A* (that is, the input
    variables)?” In other words: “What is the probability of *B*, given *A*?”:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 将这一点映射回ML用例，考虑*图15.1*中所示的数据集，它展示了OpenML中泰坦尼克号数据集的简化版本（[https://www.openml.org/search?type=data&id=40945](https://www.openml.org/search?type=data&id=40945)）。目标变量由`survived`列表示，并用绿色框突出显示。这代表了我们前面方程中的*B*。所有其他列的组合构成了输入特征，它们代表我们前面方程中的*A*，如红色框所示。本质上，这描绘了当ML模型学习根据输入变量的值来预测目标变量的值时，它是在问：“在*A*（即输入变量）的值给定的情况下，*B*（即目标变量）的概率是多少？”换句话说：“在*A*的情况下，*B*的概率是多少？”：
- en: '![Figure 15.1: Separation of target variable from input feature](img/B18143_15_1.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图15.1：目标变量与输入特征的分离](img/B18143_15_1.jpg)'
- en: 'Figure 15.1: Separation of target variable from input feature'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1：目标变量与输入特征的分离
- en: Next, let’s look at how probability can be used in a slightly different way
    in GenAI use cases.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看概率如何在GenAI用例中以稍微不同的方式被使用。
- en: GenAI and joint probability
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GenAI与联合概率
- en: 'In the case of GenAI, the models are designed to learn the **joint probability
    distribution** of the dataset. As we briefly saw in the preceding equation for
    conditional probability, joint probability refers to the probability of two or
    more events happening at the same time. We can use this to understand how different
    variables or events are connected and how they influence each other. It is represented
    as the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在GenAI的情况下，模型被设计来学习数据集的**联合概率分布**。正如我们在前面关于条件概率的方程中简要看到的，联合概率指的是两个或更多事件同时发生的概率。我们可以利用这一点来理解不同的变量或事件是如何相互关联以及它们是如何相互影响的。它表示如下：
- en: P(A∩B) = P(A) × P(B)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: P(A∩B) = P(A) × P(B)
- en: 'Here, the following applies:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，以下适用：
- en: P(A) is the probability of A occurring
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(A)是事件A发生的概率
- en: P(B) is the probability of B occurring
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(B)是事件B发生的概率
- en: 'Let’s again use an example to describe this concept in more detail. A common
    analogy is to imagine rolling two fair six-sided dice and calculating the probability
    of both dice showing a certain number; for example:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次用一个例子来更详细地描述这个概念。一个常见的类比是想象掷两个公平的六面骰子，并计算两个骰子同时显示某个数字的概率；例如：
- en: '**Event A**: The first die shows a 3'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件A**：第一个骰子显示3'
- en: '**Event B**: The second die shows a 5'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件B**：第二个骰子显示5'
- en: 'Here, the joint probability P(A ∩ B) is the probability of both the first die
    showing a 3 and the second die showing a 5 when they come to rest. Since each
    die roll is independent and each side has a 1/6 chance of appearing, the joint
    probability is the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，联合概率P(A ∩ B)是当两个骰子静止时，第一个骰子显示3和第二个骰子显示5的概率。由于每次掷骰子都是独立的，每个面有1/6的概率出现，因此联合概率如下：
- en: P(A∩B) = P(A) × P(B) = 1/6 x 1/6 = 1/36
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: P(A∩B) = P(A) × P(B) = 1/6 x 1/6 = 1/36
- en: This means there’s a 1/36 chance that both dice will show the exact numbers
    you predicted. The key thing to understand here is the events are independent,
    but there’s a shared probability distribution that governs the overall outcome.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着两个骰子同时显示你预测的精确数字的概率是1/36。这里要理解的关键点是事件是独立的，但有一个共享的概率分布来控制整体结果。
- en: 'Mapping this back to ML use cases again, the joint probability distribution
    in a dataset includes the target variable as well as the input variables, as represented
    by the red box in *Figure 15**.2*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 再次将此映射回机器学习用例，数据集中的联合概率分布包括目标变量以及输入变量，如图15.2中的红色方框所示。2*：
- en: '![Figure 15.2: Joining target variable with input features](img/B18143_15_2.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图15.2：将目标变量与输入特征连接](img/B18143_15_2.jpg)'
- en: 'Figure 15.2: Joining target variable with input features'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2：将目标变量与输入特征连接
- en: A major difference here is that, while discriminative models use conditional
    probability to predict the value of the target variable given the values of the
    input variables, generative models try to learn the overall joint probability
    distribution of the dataset, including the target variable.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这里一个主要的不同点是，虽然判别模型使用条件概率来预测给定输入变量值的目标变量的值，但生成模型试图学习数据集的整体联合概率分布，包括目标变量。
- en: By learning the overall joint probability distribution of the dataset, the model
    can develop an understanding of how the dataset is constructed and how all of
    the features relate to each other, including the target variable. In this manner,
    by accurately approximating how the training dataset is constructed, it can estimate
    how to create similar data points that do not already exist in the dataset but
    are of similar structure and composition – that is, it can generate new, similar
    content.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过学习数据集的整体联合概率分布，模型可以理解数据集是如何构建的，以及所有特征是如何相互关联的，包括目标变量。通过这种方式，通过准确近似训练数据集的构建方式，它可以估计如何创建在数据集中尚未存在但具有相似结构和组成的相似数据点——也就是说，它可以生成新的、相似的内容。
- en: 'In addition to the joint probability distribution, today’s generative models
    learn hidden relationships and structure in the dataset by using the attention
    mechanism outlined in the famous *Attention Is All You Need* (Vaswani et al.,
    2017) paper that we’ve referenced numerous times in this book. Before the attention
    mechanism was developed, models mostly treated all inputs equally. And, considering
    that all input features (and related hidden features) carry information, treating
    all inputs equally results in a low signal-to-noise ratio that limits the effectiveness
    of the learning and prediction processes. For example, when a model tries to predict
    the next word in a sentence, not all prior words in the sentence contribute equally
    to predicting the next word. Instead, the most probable next word may be more
    dependent on (or more highly correlated with) the existence of specific other
    words in the sentence. For example, consider the sentence: “I added a pinch of
    salt and a dash of [BLANK].” When a modern generative model attempts to predict
    the next word in that sentence, it will likely predict the word “pepper.” While
    the word “pepper” must make sense in the overall context of the sentence (that
    is, it will have varying degrees of contextual relationships to the other words
    in the sentence), the word “salt” likely has a higher impact on the prediction
    than other words, such as “pinch,” and the attention mechanism helps the models
    to learn these kinds of important relationships.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 除了联合概率分布之外，今天的生成模型通过使用我们在本书中多次引用的著名论文《Attention Is All You Need》（Vaswani等人，2017年）中概述的注意力机制来学习数据集中的隐藏关系和结构。在注意力机制开发之前，模型主要平等地对待所有输入。考虑到所有输入特征（和相关隐藏特征）都携带信息，平等对待所有输入会导致信号与噪声比低，从而限制了学习和预测过程的有效性。例如，当模型试图预测句子中的下一个单词时，句子中不是所有先前的单词都对预测下一个单词有同等贡献。相反，最可能的下一个单词可能更多地依赖于（或与）句子中特定其他单词的存在。例如，考虑句子：“我加了一撮盐和一点[空白]。”当现代生成模型试图预测该句子的下一个单词时，它可能会预测单词“胡椒”。虽然单词“胡椒”必须在句子的整体语境中有意义（即，它将与句子中的其他单词有不同程度的关系），但单词“盐”对预测的影响可能比其他单词，如“一撮”，更大，而注意力机制有助于模型学习这些重要关系。
- en: While this section focuses mainly on the differences between GenAI and “traditional
    AI,” I also want to mention that the demarcation is not always so clear, and sometimes
    the lines may be blurred.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本节主要关注GenAI和“传统AI”之间的区别，但我还想提到，这种界限并不总是如此清晰，有时界限可能会变得模糊。
- en: Blurring the lines
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 界限模糊
- en: It’s important to understand that the demarcations between non-GenAI and GenAI
    often become blurred because many applications combine both approaches. For example,
    a model or application that analyzes text and generates a summary may use both
    discriminative and generative processes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解，非GenAI和GenAI之间的界限往往变得模糊，因为许多应用结合了这两种方法。例如，一个分析文本并生成摘要的模型或应用可能会同时使用判别性和生成性过程。
- en: Consider chatbots as an example. Chatbots commonly generate responses by constructing
    sentences “on the fly.” A popular approach in constructing sentences is to predict
    the most appropriate next word based on previous words in the sentence. This is
    an instance of using conditional probability. However, if the application or model
    only did this, then its abilities would be quite limited. In order to generate
    complex and coherent responses that adhere to **natural language** (**NL**) constructs,
    the model needs to have learned an accurate representation of the structure (that
    is, the joint probability distribution) of the language in which it generates
    responses.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以聊天机器人为例。聊天机器人通常通过“即时”构建句子来生成响应。构建句子的一个流行方法是基于句子中的先前单词预测最合适的下一个单词。这是一个使用条件概率的例子。然而，如果应用或模型只做这件事，那么它的能力将非常有限。为了生成复杂且连贯的响应，并遵循**自然语言**（NL）结构，模型需要学习到它生成响应的语言结构的准确表示（即，联合概率分布）。
- en: An additional interesting scenario is the use of GenAI to produce new data that
    can then be used as features for traditional ML models. An example of this use
    case, as suggested by my colleague, Jeremy Wortz, would be to ask a generative
    model to rate songs with a score (let’s say between 1 and 5) based on a customer
    persona, and then make playlists with those generated features from the ratings
    and outputs from a process called “chain-of-thought reasoning,” which essentially
    asks the LLM to elaborate on its thought process. I will describe the chain-of-thought
    concept in more detail later.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的场景是使用GenAI生成新的数据，这些数据随后可以用作传统ML模型的特征。我的同事杰里米·沃茨（Jeremy Wortz）建议的一个例子是要求一个生成模型根据客户画像对歌曲进行评分（比如说1到5分），然后根据评分和“思维链推理”过程（即让LLM详细阐述其思维过程）生成的特征制作播放列表。我将在稍后更详细地描述思维链的概念。
- en: Now that we’ve discussed the differences between GenAI and “traditional AI,”
    we will explore the development of techniques that have led to the evolution of
    GenAI.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了GenAI和“传统AI”之间的区别，我们将探讨导致GenAI演化的技术发展。
- en: GenAI techniques and evolution
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GenAI技术和演化
- en: As we’ve already discussed, while there are some distinctions between GenAI
    and “traditional AI,” they share much of the same history. For example, in [*Chapter
    1*](B18143_01.xhtml#_idTextAnchor015), we discussed a brief history of AI/ML,
    and in *Chapters 9* and *14*, we learned about the evolution of various types
    of NNs, such as **recurrent NNs** (**RNNs**), **long short-term memory** (**LSTM**)
    networks, and Transformers. All of those concepts and milestones also apply to
    the history and evolution of GenAI. Considering that GenAI is a relatively newer
    subset of AI, we can view its evolutionary timeline as an extension of the evolution
    of AI in general. Therefore, the topics in this section build upon what we’ve
    already covered in that regard.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，虽然GenAI和“传统AI”之间有一些区别，但它们有着许多相同的历史。例如，在[*第一章*](B18143_01.xhtml#_idTextAnchor015)中，我们讨论了AI/ML的简要历史，在*第九章*和*第十四章*中，我们学习了各种类型的NN（如**循环神经网络**（RNNs））、**长短期记忆**（LSTM）网络和Transformers的演变。所有这些概念和里程碑也适用于GenAI的历史和演变。鉴于GenAI是相对较新的AI子集，我们可以将其演化时间线视为AI总体演化的延伸。因此，本节中的主题建立在之前我们已经覆盖的内容之上。
- en: The evolution of GenAI could comprise an entire book by itself, and for the
    purpose of this current book, it would be an unnecessary level of information
    to cover all of the major milestones and developments that contributed to the
    evolution of GenAI in great detail. Instead, I will summarize some of the most
    prominent milestones and contributory developments at a high level, such as Markov
    chains and **hidden Markov models** (**HMMs**), **restricted Boltzmann machines**
    (**RBMs**), **deep belief networks** (**DBNs**), AEs, and GANs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: GenAI的演变本身可以构成一本书，为了本书的目的，全面覆盖所有对GenAI演变做出重大贡献的主要里程碑和发展将是不必要的详细信息级别。相反，我将从高层次总结一些最突出的里程碑和贡献性发展，例如马尔可夫链和**隐马尔可夫模型**（HMMs）、**受限玻尔兹曼机**（RBMs）、**深度信念网络**（DBNs）、AEs和GANs。
- en: 'It’s important also to note that some mechanisms that were originally developed
    primarily for discriminative use cases can be reappropriated for generative use
    cases. For example, while simple Naïve Bayes classifiers are commonly used to
    predict a given class based on the features in the dataset (that is, an application
    of conditional probability for discriminative use cases), the algorithm is also
    capable of learning an approximation of the joint probability distribution of
    the dataset during training due to how it applies Bayes’ Theorem with the (naïve)
    assumption that each feature is independent. (To learn more about how Naïve Bayes
    classifiers work, I recommend reviewing the paper at the following URL: [https://doi.org/10.48550/arXiv.1404.0933](https://doi.org/10.48550/arXiv.1404.0933)).
    A similar reappropriation can be done with more complex applications of Bayes’
    Theorem, such as Bayesian networks.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，一些最初主要为了判别性用例开发的机制可以被重新用于生成性用例。例如，虽然简单的朴素贝叶斯分类器通常用于根据数据集中的特征预测给定类别（即，判别性用例的条件下概率应用），但由于它应用贝叶斯定理时（天真地）假设每个特征是独立的，该算法在训练过程中也能够学习数据集联合概率分布的近似。（要了解更多关于朴素贝叶斯分类器的工作原理，我建议查阅以下URL的论文：[https://doi.org/10.48550/arXiv.1404.0933](https://doi.org/10.48550/arXiv.1404.0933)）。类似的重用也可以应用于更复杂的贝叶斯定理应用，如贝叶斯网络。
- en: Before diving into specific GenAI approaches, I want to introduce two important
    concepts that will form the foundation for many of the topics that we will discuss
    throughout the remainder of this book, referred to as **embeddings** and **latent
    space**.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨具体的GenAI方法之前，我想介绍两个重要的概念，这两个概念将构成本书余下部分讨论的许多主题的基础，被称为**嵌入**和**潜在空间**。
- en: Embeddings and latent space
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入和潜在空间
- en: In [*Chapter 7*](B18143_07.xhtml#_idTextAnchor215), we discussed the topic of
    dimensionality reduction, and we used mechanisms such as **principal component
    analysis** (**PCA**) to project features from our datasets into lower-dimensional
    feature spaces. These lower-dimensional feature spaces can be referred to as “latent
    spaces,” and the representations of our data in the latent space can be referred
    to as “embeddings.” Allow me to explain these important concepts in more detail,
    starting with embeddings.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第七章*](B18143_07.xhtml#_idTextAnchor215)中，我们讨论了降维的话题，并使用了如**主成分分析**（PCA）这样的机制将我们的数据集特征投影到低维特征空间。这些低维特征空间可以被称为“潜在空间”，而我们在潜在空间中的数据表示可以被称为“嵌入”。让我更详细地解释这些重要概念，从嵌入开始。
- en: Embeddings
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入
- en: Embeddings are numerical representations of data in a lower-dimensional space
    (that is, the latent space). They can be seen as numerical “fingerprints” that
    capture the meaning or characteristics of the original data. For example, word
    embeddings capture the meaning of words, and if words such as “king” and “queen”
    have similar embeddings, a language model can infer a relationship between the
    two. Models can also embed other types of data, such as images, audio, and graphs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是数据在低维空间（即潜在空间）中的数值表示。它们可以被视为捕获原始数据意义或特征的数值“指纹”。例如，词嵌入捕获单词的意义，如果“king”和“queen”有相似的嵌入，语言模型可以推断两者之间的关系。模型还可以嵌入其他类型的数据，如图像、音频和图。
- en: Next, let’s dive into the concept of latent space in more detail.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们更详细地探讨潜在空间的概念。
- en: Latent space
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 潜在空间
- en: We use the term “latent space” to define the abstract feature space where the
    intrinsic properties or features of a dataset are represented. This space captures
    underlying structure and patterns within the data that might not be apparent in
    its original form (hence the term “latent”).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用“潜在空间”这个术语来定义一个抽象的特征空间，其中数据集的内在属性或特征被表示。这个空间捕捉了数据中可能在其原始形式中不明显的基本结构和模式（因此得名“潜在”）。
- en: It’s important to understand that the latent space and its dimensions map in
    some way to the original features. This means that relationships among the original
    features are captured as relationships among the projected features in the latent
    space (that is, semantic context is represented in some way). These representations
    and mappings are generally learned by models during the training process and are
    therefore often not easily interpretable by humans, but there are also methods
    by which we can explicitly create embeddings for the contents of our datasets,
    which I will describe in more detail in [*Chapter 16*](B18143_16.xhtml#_idTextAnchor383).
    For now, I’ll briefly explain how embeddings and latent spaces are used.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解潜在空间及其维度以某种方式映射到原始特征。这意味着原始特征之间的关系被捕获为潜在空间中投影特征之间的关系（即，语义上下文以某种方式表示）。这些表示和映射通常在训练过程中由模型学习，因此通常不容易被人类解释，但也有一些方法可以通过我们为数据集的内容显式创建嵌入，我将在[*第16章*](B18143_16.xhtml#_idTextAnchor383)中更详细地描述。现在，我将简要解释嵌入和潜在空间是如何使用的。
- en: Using embeddings and latent space
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用嵌入和潜在空间
- en: When models have created embeddings for our data, these representations in the
    latent space can be used in interesting ways. One of the most useful ways to use
    this data is to measure the distances between embeddings in the latent space,
    using familiar distance metrics such as Euclidean distance or cosine similarity.
    Considering that the embeddings in the latent space capture the essence of the
    concepts they represent, we can identify concepts that may be similar or related
    to the original space. An example would be products in a retail website’s product
    catalog. By embedding the product details and identifying which ones are near
    each other in the latent space, our models can get an understanding of which products
    may be related to each other. A recommender system could then use this information
    to display insights such as “customers who purchased this item also purchased
    these other items.”
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型为我们创建嵌入时，这些潜在空间中的表示可以以有趣的方式使用。使用这些数据最有用的方式之一是测量潜在空间中嵌入之间的距离，使用熟悉的距离度量，如欧几里得距离或余弦相似度。考虑到潜在空间中的嵌入捕捉了它们所代表概念的精髓，我们可以识别出可能与原始空间相似或相关的概念。一个例子是零售网站产品目录中的产品。通过嵌入产品细节并识别在潜在空间中彼此靠近的产品，我们的模型可以了解哪些产品可能彼此相关。然后，推荐系统可以使用这些信息来显示诸如“购买此商品的客户还购买了这些其他商品”之类的见解。
- en: You may ask, “Why not just perform the same kinds of actions in the original
    space?” The process of encoding embeddings converts the objects to vectors, providing
    a much more efficient way to represent the concepts. Also, rather than dealing
    with words and images, ML algorithms and models love to work with vectors, so
    these vectorized representations are more well suited to ML use cases.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，“为什么不在原始空间中执行相同类型的操作？”嵌入编码的过程将对象转换为向量，提供了一种更有效的方式来表示概念。此外，与处理单词和图像相比，机器学习算法和模型更喜欢与向量一起工作，因此这些向量化的表示更适合机器学习用例。
- en: Another example that highlights the efficiency of embeddings is when we use
    ML models for image-processing use cases. Consider high-definition images that
    contain millions of pixels, in which each pixel is considered a feature. If we
    have a dataset consisting of hundreds of millions of images, and each image has
    millions of features, this could lead to extremely compute-intensive training
    and inference processing. Instead, mapping the features to a lower-dimensional
    feature space will significantly optimize processing efficiency. Also, individual
    pixels often don’t convey much meaning; rather, it’s often the relationships between
    pixels and patterns that define what an image represents.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个突出嵌入效率的例子是当我们使用机器学习模型进行图像处理用例时。考虑包含数百万像素的高清图像，其中每个像素都被视为一个特征。如果我们有一个包含数亿张图像的数据集，并且每张图像都有数百万个特征，这可能导致极其计算密集的训练和推理处理。相反，将特征映射到低维特征空间将显著优化处理效率。此外，单个像素通常不传达很多意义；相反，通常是像素之间的关系和模式定义了图像所代表的内容。
- en: 'We’ll revisit the concepts of embeddings and the latent space many more times
    throughout the rest of this book because they are such foundational concepts in
    GenAI. Now that I’ve introduced these important concepts, we’ll begin our journey
    through various important milestones and approaches that led to the development
    of GenAI, as outlined at a high level in *Figure 15**.3*:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的其余部分多次回顾嵌入和潜在空间的概念，因为它们是GenAI中的基础概念。现在，我已经介绍了这些重要概念，我们将开始探索各种重要里程碑和途径，这些途径导致了GenAI的发展，如*图15.3*所示的高层次概述：
- en: '![Figure 15.3: Milestones in GenAI evolution](img/B18143_15_3.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图15.3：通用人工智能（GenAI）演变的里程碑](img/B18143_15_3.jpg)'
- en: 'Figure 15.3: Milestones in GenAI evolution'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3：通用人工智能（GenAI）演变的里程碑
- en: Of course, many additional GenAI models and approaches have been developed and
    released in the past few decades beyond those shown in *Figure 15**.3*, but here
    I’m focusing on high-level milestones and developments that have most notably
    influenced the evolution of GenAI. Let’s begin diving into each one, starting
    with Markov chains and HMMs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在过去几十年中，除了*图15.3*中所示的那些之外，还开发并发布了许多额外的GenAI模型和方法。但在这里，我专注于对GenAI演变影响最显著的、高层次里程碑和发展。让我们开始深入探讨每一个，从马尔可夫链和HMMs开始。
- en: Markov chains and HMMs
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 马尔可夫链和HMMs
- en: The concept of Markov chains was introduced by Andrey Markov in 1906, and they
    are based on what’s referred to as the Markov property, which posits that the
    next or future state in a sequence or scenario depends only on the current state.
    They can be used to predict (and therefore generate) the next item in a sequence.
    HMMs extend this concept to include hidden states that cannot be directly observed
    in a given scenario. A very simple example would be the potential correlation
    between current weather conditions and umbrella sales. If we can directly observe
    the weather, and we see that it is currently raining, then that’s an observable
    state we can use in a Markov chain to predict an increase in umbrella sales. On
    the other hand, if we cannot observe the weather for some reason (perhaps we’re
    working in a store that’s below ground level in a mall) but we notice an increase
    in umbrella sales, then we could surmise that it’s currently raining outside.
    In this case, the state of the weather is hidden, but we can speculate the probability
    of rain based on the secondary observable state of increased umbrella sales. This
    would be a very simple representation of a hidden state in an HMM.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链的概念是由安德烈·马尔可夫在1906年提出的，它们基于所谓的马尔可夫性质，即在一个序列或场景中的下一个或未来状态只取决于当前状态。它们可以用来预测（因此生成）序列中的下一个项目。HMMs将这一概念扩展到包括在给定场景中无法直接观察到的隐藏状态。一个非常简单的例子就是当前天气条件和雨伞销售之间的潜在相关性。如果我们可以直接观察到天气，并且我们看到现在正在下雨，那么这就是我们可以用于马尔可夫链来预测雨伞销售增加的可观察状态。另一方面，如果我们由于某种原因无法观察到天气（例如，我们可能在商场地下层的商店工作），但我们注意到雨伞销售增加，那么我们可以推测外面现在正在下雨。在这种情况下，天气的状态是隐藏的，但我们可以根据雨伞销售增加的次级可观察状态来推测下雨的概率。这将是HMM中隐藏状态的非常简单的表示。
- en: The next concept I’d like to introduce briefly is RBMs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我接下来要简要介绍的概念是RBMs。
- en: RBMs
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RBMs
- en: '**Boltzmann machines** (**BMs**) are a type of **energy-based model** (**EBM**)
    that borrows some concepts from physics. “Boltzmann” refers to Ludwig Boltzmann,
    a physicist associated with statistical mechanics, which uses statistics and probability
    theory to model the behavior of microscopic particles. The Boltzmann probability
    distribution (also called the Gibbs distribution) provides the probability of
    a system being in a particular state based on its energy and temperature. A key
    concept here is that states with lower energy are more likely to occur than states
    with higher energy.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**玻尔兹曼机**（**BMs**）是一种基于**能量模型**（**EBM**）的类型，它借鉴了一些物理学概念。“玻尔兹曼”指的是路德维希·玻尔兹曼，一位与统计力学相关的物理学家，统计力学使用统计学和概率论来模拟微观粒子的行为。玻尔兹曼概率分布（也称为吉布斯分布）根据系统的能量和温度提供系统处于特定状态的概率。这里的一个关键概念是，能量较低的状态比能量较高的状态更有可能发生。'
- en: 'Mapping this to data science: Unlike the conditional probability used in traditional
    AI as described earlier in this chapter, EBMs use an “energy function” to assign
    a value to every possible configuration of variables, where lower “energy” values
    represent a more likely or desirable configuration.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 将其映射到数据科学：与传统AI中描述的先验概率不同，EBMs使用“能量函数”为每个变量的可能配置分配一个值，其中较低的“能量”值代表更可能或更理想的配置。
- en: 'Due to the work by Geoffrey Hinton and Ruslan Salakhutdinov, BMs were refined
    to form RBMs, which are a type of **artificial NN** (**ANN**) that consists of
    two layers:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于杰弗里·辛顿和鲁斯兰·萨拉胡丁诺夫的工作，玻尔兹曼机被精炼成RBMs，这是一种由两层组成的**人工神经网络**（**ANN**）：
- en: The visible layer, which represents the input data (for example, the pixels
    in an image or the words in a sentence).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可见层，它代表输入数据（例如，图像中的像素或句子中的单词）。
- en: The hidden layer, which learns to capture higher-level features and dependencies
    in the data. This relates to the concept of “latent space” that we introduced
    earlier.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层，它学会捕捉数据中的高级特征和依赖关系。这与我们之前介绍的“潜在空间”概念相关。
- en: With RBMs, the learning process involves adjusting the weights between the visible
    and hidden layers to minimize what’s referred to as the **reconstruction error**,
    which measures how well the RBM can reproduce the original input data. By doing
    this, the RBM learns to model the probability distribution of the input data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RBMs，学习过程涉及调整可见层和隐藏层之间的权重，以最小化所谓的**重建误差**，这衡量了RBM能够多好地重现原始输入数据。通过这样做，RBM学会模拟输入数据的概率分布。
- en: In the original BM, all of the layers were connected, and the models were quite
    complex and compute-intensive to train. However, in RBMs, while the visible layers
    and hidden layers are fully connected to each other, there are no connections
    within a layer (hence the term “restricted”), which makes them less computationally
    intensive to train.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始玻尔兹曼机中，所有层都是相互连接的，模型相当复杂，训练起来计算量很大。然而，在RBMs中，尽管可见层和隐藏层之间是完全连接的，但层内没有连接（因此得名“受限”），这使得它们在训练时计算量较小。
- en: 'RBMs are commonly used for UL use cases, especially for feature extraction
    and dimensionality reduction, and they can also be seen as a kind of building
    block for **deep learning** (**DL**), which can be used to build many other kinds
    of models, including classification and regression. The idea of using RBMs as
    building blocks brings me to the next topic: DBNs.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: RBMs通常用于UL用例，尤其是在特征提取和降维方面，它们也可以被视为**深度学习**（**DL**）的一种构建块，可以用来构建许多其他类型的模型，包括分类和回归。使用RBMs作为构建块的想法让我想到了下一个话题：DBNs。
- en: DBNs
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DBNs
- en: DBNs can be viewed as a composition of simple, unsupervised networks such as
    RBMs, where each sub-network’s hidden layer serves as the visible layer for the
    next sub-network. By stacking multiple RBMs, each layer can learn increasingly
    abstract and complex representations of the data.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: DBNs可以被视为由RBMs等简单无监督网络组成的组合，其中每个子网络的隐藏层作为下一个子网络的可见层。通过堆叠多个RBMs，每一层都可以学习数据越来越抽象和复杂的表示。
- en: 'Training DBNs consists of two main phases: pre-training and **fine-tuning**
    (**FT**). During pre-training, the DBN is trained one layer at a time in an unsupervised
    manner. Each layer is trained as an RBM, which learns to represent the features
    passed from the layer below. This layer-wise pre-training helps initialize the
    weights of the network in a way that makes the subsequent FT phase more effective.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 训练DBN包括两个主要阶段：预训练和**微调**（**FT**）。在预训练期间，DBN以无监督的方式逐层训练。每一层都被训练为一个限制性玻尔兹曼机（RBM），它学会表示从下层传递过来的特征。这种逐层预训练有助于以使后续的微调阶段更有效的方式初始化网络的权重。
- en: After pre-training, a DBN can sample from the representations it has learned
    to generate new data similar to the original dataset. We can also use **supervised
    learning** (**SL**) approaches to fine-tune the network for more specific applications.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练之后，一个深度信念网络（DBN）可以从它所学习到的表示中采样生成与原始数据集相似的新数据。我们还可以使用**监督学习**（**SL**）方法来微调网络以适应更具体的应用。
- en: Each of the techniques and algorithms I’ve outlined so far in this chapter are
    what I would consider to be fundamental milestones and conceptual building blocks
    that contribute to the development of the more advanced GenAI applications we
    see today. The next two approaches I will outline are significant steps forward
    in the evolution of GenAI. The first I will introduce is the concept of AEs.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本章中迄今为止概述的每个技术和算法都是我认为是基本里程碑和概念构建块，这些里程碑和构建块有助于推动我们今天看到的更高级的生成式人工智能（GenAI）应用的发展。接下来我将概述的两个方法在GenAI的演变中是重要的步骤。我将介绍的第一个概念是自动编码器（AEs）。
- en: AEs
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动编码器
- en: Most – if not all – of the datasets we use for ML use cases represent specific
    concepts with intrinsic properties (or features); for example, people, cars, medical
    images, or even more specific concepts, such as people who boarded the Titanic
    or items purchased on a company’s website.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于机器学习用例的大多数——如果不是所有——数据集都代表具有内在属性（或特征）的特定概念；例如，人、汽车、医学图像，甚至是更具体的概念，如登上泰坦尼克号的人或在公司网站上购买的商品。
- en: In the case of discriminative ML use cases, the models generally try to predict
    some kind of output based on the values of the features associated with each instance
    of the concept being represented. For example, which customers will likely purchase
    a particular product based on their prior purchasing history?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在判别式机器学习（ML）用例的情况下，模型通常试图根据与所表示概念每个实例相关的特征值来预测某种输出。例如，根据他们之前的购买历史，哪些客户可能会购买某个特定的产品？
- en: In the case of GenAI, on the other hand, the models are often expected to generate
    new data points that validly represent the concept originally represented in the
    training dataset. For example, having been trained on images of many different
    concepts, such as cats and motorcycles, the model may be asked to draw a cartoon
    image of a cat on a motorcycle. To do this, the model needs to “understand” what
    those concepts are.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，在生成式人工智能（GenAI）的情况下，模型通常被期望生成代表训练数据集中原始概念的新数据点。例如，如果一个模型在许多不同概念（如猫和摩托车）的图像上进行了训练，那么可能会要求它绘制一张猫骑摩托车的卡通图像。为了做到这一点，模型需要“理解”这些概念是什么。
- en: Earlier in this chapter, we introduced the topics of embeddings and the latent
    space, in which latent representations of the data points in our dataset are defined.
    The process of creating an embedding in the latent space can be referred to as
    **encoding**, and AEs are a type of ANN used for UL of efficient representations
    (encodings) for our datasets. At a high level, an AE takes input data, converts
    it into a smaller, dense representation that captures the essence of the data,
    and then reconstructs the input data as closely as possible from this representation.
    Let’s dive into how they work.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期，我们介绍了嵌入和潜在空间的主题，其中定义了数据集中数据点的潜在表示。在潜在空间中创建嵌入的过程可以被称为**编码**，自动编码器（AEs）是一种用于对数据集的高效表示（编码）进行无监督学习（UL）的ANN。从高层次来看，自动编码器接收输入数据，将其转换为更小、更密集的表示，该表示捕获数据的本质，然后尽可能从该表示中重建输入数据。让我们深入了解它们是如何工作的。
- en: How AEs work
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自动编码器的工作原理
- en: 'AEs are generally made up of three main components: the **encoder**, the latent
    space (also referred to as the **bottleneck**), and the **decoder**, as depicted
    in *Figure 15**.4*:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器通常由三个主要组件组成：**编码器**、潜在空间（也称为**瓶颈**）和**解码器**，如图*图15**.4*所示：
- en: '![Figure 15.4: AE architecture](img/B18143_15_4.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图15.4：AE架构](img/B18143_15_4.jpg)'
- en: 'Figure 15.4: AE architecture'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4：AE架构
- en: During the forward pass in an AE, each input data point is passed through the
    encoder part of the network. This section of the network compresses the input
    into a smaller, dense representation in the latent space (hidden layer). The encoded
    data is then passed through the decoder part of the network, and the decoder tries
    to reconstruct the original input data from the compressed representation. This
    is the interesting difference between AEs and traditional, discriminative models
    – that is, while traditional models generally try to predict an output (*Y*) based
    on an input (*X*), AEs try to predict (or generate) the original input (*X*).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动编码器（AE）的前向传递过程中，每个输入数据点都会通过网络中的编码器部分。这一部分网络将输入压缩成潜在空间（隐藏层）中更小、更密集的表示。然后，编码后的数据会通过网络中的解码器部分，解码器试图从压缩的表示中重建原始输入数据。这是自动编码器（AE）与传统、判别性模型之间有趣的不同之处——也就是说，传统模型通常试图根据输入（*X*）预测输出（*Y*），而自动编码器（AE）则试图预测（或生成）原始输入（*X*）。
- en: Just as we did in traditional NNs, we can calculate the difference between the
    output value and the expected value (*X*), which is referred to as the **reconstruction
    error**, and then we can use backpropagation to update the network and continue
    the training cycle as usual. The quality of the reconstruction depends on how
    well the AE has learned to represent the data in the latent space.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在传统的神经网络中所做的那样，我们可以计算输出值与预期值（*X*）之间的差异，这被称为**重建误差**，然后我们可以使用反向传播来更新网络并继续常规的训练周期。重建的质量取决于自动编码器（AE）在潜在空间中学习表示数据的程度。
- en: During this training process, the encoder learns to retain only the most relevant
    features of the data, effectively learning a new way to represent the input data
    in a smaller-dimensional space. The latent space then contains the compressed
    knowledge that the network has learned about the data – that is, the essence of
    the data. To again paraphrase a description by my colleague, Jeremy Wortz, the
    concept of bottleneck features relates to the quality of the embeddings; since
    we are typically “squeezing” our most critical information between the encoder
    and decoder, feature importance is implicitly optimized.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个训练过程中，编码器学习只保留数据中最相关的特征，有效地学习了一种在较小维度的空间中表示输入数据的新方法。潜在空间随后包含了网络学习到的关于数据的压缩知识——即数据的本质。再次引用我的同事Jeremy
    Wortz的描述，瓶颈特征的概念与嵌入的质量相关；由于我们通常在编码器和解码器之间“挤压”我们最关键的信息，因此特征的重要性被隐式优化。
- en: Note, however, that while AEs can learn to reconstruct the original dataset
    and thereby learn how to perform dimensionality reduction to accurately represent
    the data in a lower-dimensional space, they are not designed to generate new data.
    To enable that functionality, we need to bring probability distributions into
    the image, and this is what has given rise to the development of **variational
    AEs** (**VAEs**), which I’ll describe next.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的是，尽管自动编码器（AE）可以学习重建原始数据集，从而学习如何执行降维以在低维空间中准确表示数据，但它们并不是为了生成新数据而设计的。为了实现这一功能，我们需要将概率分布引入图像中，这就是变分自动编码器（VAEs）发展的原因，我将在下面描述。
- en: VAEs
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VAEs
- en: VAEs extend the AE concept by introducing probabilistic approaches to enable
    the generation of new data points that are similar in structure to the training
    data. Building on what we’ve discussed about how AEs work, the easiest way to
    describe VAEs is to highlight their subtle differences from regular AEs.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: VAE通过引入概率方法扩展了自动编码器（AE）的概念，以生成与训练数据结构相似的新数据点。基于我们关于自动编码器（AE）如何工作的讨论，描述VAE的最简单方法就是强调它们与常规自动编码器之间的细微差别。
- en: As with traditional AEs, VAEs consist of an encoder and a decoder. However,
    rather than mapping input data to a fixed, deterministic vector as in regular
    AEs, the encoder in a VAE maps the input data to a probability distribution over
    the latent space. That’s a lot to take in, so let’s clarify that further.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的自动编码器（AE）一样，变分自动编码器（VAE）由编码器和解码器组成。然而，与常规的自动编码器不同，VAE中的编码器将输入数据映射到潜在空间上的概率分布。这需要很多内容，所以让我们进一步澄清。
- en: As we learned in the previous section, regular AEs encode the input data into
    a vector that contains latent features representing the original inputs. However,
    in the case of VAEs, instead of learning a single point in the latent space for
    each data sample, the encoder learns probability distributions. To be more precise,
    instead of outputting specific feature values in the latent space, the encoder
    outputs parameters (such as mean and standard deviation) that describe the probability
    distribution for features in the latent space. This distribution represents where
    the encoder believes the input data should be encoded. This approach introduces
    randomness or variability into the encoding process, which is important for potentially
    generating new data points that are similar to the input data, and not just reconstructions
    of the original data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在前一节所学，常规的AE将输入数据编码成一个包含表示原始输入的潜在特征的向量。然而，在VAEs的情况下，编码器不是为每个数据样本学习潜在空间中的单个点，而是学习概率分布。更准确地说，编码器不是在潜在空间中输出特定的特征值，而是输出描述潜在空间中特征概率分布的参数（如均值和标准差）。这个分布代表了编码器认为输入数据应该被编码的位置。这种方法在编码过程中引入了随机性或可变性，这对于可能生成与输入数据相似的新数据点至关重要，而不仅仅是原始数据的重建。
- en: 'The following are the high-level steps that are performed when an input is
    passed through the VAE:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在输入通过VAE时执行的高级步骤：
- en: The encoder provides the parameters of a probability distribution within the
    latent space.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器提供潜在空间内概率分布的参数。
- en: A point is sampled from this learned distribution.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这个学习到的分布中采样一个点。
- en: This sampled point then gets passed through the decoder, which tries to reconstruct
    the input data from this probabilistic encoding.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将这个采样点通过解码器，解码器试图从这个概率编码中重建输入数据。
- en: The goal, again, is to minimize the difference between the original input and
    its reconstruction, similar to regular AEs.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 目标，再次强调，是使原始输入与其重建之间的差异最小化，类似于常规的AE。
- en: However, in addition to the reconstruction error, VAEs have an extra term in
    their loss function referred to as the **Kullback-Leibler** (**KL**) divergence,
    which measures how much the learned distributions for each input deviate from
    a standard normal distribution. Without getting into too much detail regarding
    the mathematics involved, the important thing to understand is that the KL divergence
    regularization enforces smoothness and continuity in the latent space, which makes
    the models more robust and can help reduce overfitting.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，除了重建误差之外，VAEs 在其损失函数中还有一个额外的项，被称为**Kullback-Leibler**（**KL**）散度，它衡量了每个输入学习到的分布与标准正态分布偏离的程度。不深入数学细节，重要的是要理解KL散度正则化强制在潜在空间中保持平滑和连续性，这使得模型更加鲁棒，并有助于减少过拟合。
- en: We covered some complex details in this section, but the key takeaway is that
    VAEs introduce probabilistic mechanisms that enable them to go beyond simply reconstructing
    the input data and begin to generate new, similar data points.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了一些复杂细节，但关键要点是VAEs引入了概率机制，使它们能够超越仅仅重建输入数据，并开始生成新的、类似的数据点。
- en: We are now firmly getting into the realm of generative models, and the next
    approach I will outline is that of GANs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经深入到生成模型的领域，接下来我将概述GANs的方法。
- en: GANs
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GANs
- en: We briefly introduced GANs in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245),
    and in this section, we will explore them in more detail. You may be familiar
    with the concept of “deepfakes,” in which AI is used to create photorealistic
    images or movies or audiorealistic tracks. By “photorealistic,” we mean synthetic
    data that looks like real photographs or video, and by “audiorealistic,” we mean
    synthetic data that sounds like real audio recordings. GANs are a popular mechanism
    for generating synthetic data that mimics real-world data. As with any technology,
    GANs could be used for malicious purposes, such as generating deepfakes of real
    people without their consent, but they have many useful applications that we will
    cover shortly. However, let’s first dive into what GANs are and how they work.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第9章*](B18143_09.xhtml#_idTextAnchor245)中简要介绍了GAN，在本节中，我们将更详细地探讨它们。你可能熟悉“深度伪造”的概念，其中AI被用来创建逼真的图像或电影或音频轨道。通过“逼真”，我们指的是看起来像真实照片或视频的合成数据，而通过“音频逼真”，我们指的是听起来像真实音频录音的合成数据。GAN是生成模仿真实世界数据的合成数据的一种流行机制。与任何技术一样，GAN可能被用于恶意目的，例如未经同意生成真实人物的深度伪造，但它们有许多有用的应用，我们将在稍后介绍。然而，让我们首先深入了解GAN是什么以及它们是如何工作的。
- en: High-level concepts – the generator and the discriminator
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 高级概念——生成器和判别器
- en: The name “generative adversarial networks” (Goodfellow et al., 2014) may sound
    somewhat abstract, but it perfectly describes the concepts used in this approach
    to GenAI. In short, GAN implementations consist of two NNs that work in an adversarial
    manner (that is, they work against each other), in which one of the networks generates
    synthetic data, and the other network tries to ascertain whether the data is real
    or not. The network that generates the data is called, not surprisingly, the **generator**,
    and the network that tries to ascertain the realness of the data is called the
    **discriminator**. The main premise is that the generator tries to fool the discriminator
    into believing that the data is real.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: “生成对抗网络”（Goodfellow et al., 2014）这个名字可能听起来有些抽象，但它完美地描述了这种用于生成式人工智能（GenAI）的方法中所使用的概念。简而言之，GAN的实现包括两个神经网络，它们以对抗的方式工作（即，它们相互对抗），其中一个网络生成合成数据，而另一个网络试图确定数据是否真实。生成数据的网络被称为，不出所料，**生成器**，而试图确定数据真实性的网络被称为**判别器**。主要前提是生成器试图欺骗判别器，让它相信数据是真实的。
- en: One could imagine that having an effective generator is the most important requirement
    for a GAN, but it is also essential to have an effective discriminator because
    the discriminator can be seen as the **quality control** (**QC**) mechanism for
    generated data. If you have an ineffective discriminator, it could easily be fooled
    by data that does not accurately mimic real data. The more effective your discriminator
    is at identifying fake data, the harder your generator will need to work to create
    realistic data. Therefore, both sides of the adversarial partnership need to be
    trained effectively in order for the GAN to create high-quality data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 可以想象，拥有一个有效的生成器是GAN最重要的要求之一，但同样重要的是要有一个有效的判别器，因为判别器可以被视为生成数据的**质量控制**（**QC**）机制。如果你有一个无效的判别器，它很容易被不准确地模仿真实数据的数据所欺骗。你的判别器在识别伪造数据方面越有效，你的生成器就需要越努力地工作来创建逼真的数据。因此，为了使GAN能够创建高质量的数据，对抗伙伴的双方都需要得到有效的训练。
- en: The analogy that is often used to describe this process is to imagine a person
    who wants to create forgeries of expensive art pieces (that is, a forger) and
    another person who is dedicated to identifying whether a piece of art is real
    or a forgery (that is, an art expert). In the beginning, the forgeries might be
    amateur attempts that are easy to identify as fakes. However, as the forger refines
    their work and learns to create more convincing forgeries, the art expert must
    become more skilled at identifying minute nuances that distinguish real art from
    fake art pieces. This cycle then repeats until, ideally, the generator creates
    data that is indistinguishable from the real data.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 常用来描述这一过程的类比是想象一个想要伪造昂贵艺术品的人（即，一个伪造者）和另一个致力于识别艺术品是否为真或伪造的人（即，一个艺术专家）。一开始，伪造品可能是容易识别为赝品的业余尝试。然而，随着伪造者改进他们的作品并学会创造更令人信服的伪造品，艺术专家必须变得更加擅长识别区分真艺术品和伪造艺术品之间的细微差别。然后，这个周期就会重复，直到理想情况下，生成器创建的数据与真实数据无法区分。
- en: Diving deeper – GAN training process
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 深入了解——GAN训练过程
- en: In a GAN, both networks are usually **convolutional NNs** (**CNNs**). The discriminator
    is usually a binary classifier that classifies the data as either real or fake.
    This means that its training process involves many of the same steps that are
    already familiar to us from earlier chapters in this book, in which we trained
    classifier models.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在GAN中，两个网络通常是**卷积神经网络**（**CNNs**）。判别器通常是一个二元分类器，它将数据分类为真实或伪造。这意味着其训练过程涉及许多我们在这本书的早期章节中已经熟悉的步骤，其中我们训练了分类器模型。
- en: As an example, let’s imagine that we want to generate images of cats. The discriminator
    could learn to identify images of cats by being trained on a labeled dataset.
    In this dataset, some of the inputs are real images of cats, and they are labeled
    accordingly.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们假设我们想要生成猫的图像。判别器可以通过在标记的数据集上进行训练来学习识别猫的图像。在这个数据集中，一些输入是真实的猫的图像，并且相应地进行了标记。
- en: As we briefly mentioned in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245),
    the generator and the discriminator engage in a kind of adversarial game. In this
    game, the objective of the discriminator is to *minimize* the error rate for classifying
    images of cats (that is, it wants to accurately identify images of cats as much
    as possible). On the other hand, the generator is trying to *maximize* the discriminator’s
    error rate. Due to the opposing objectives of each of the game’s participants,
    where one participant wants to minimize a certain metric and the other participant
    wants to maximize that same metric, this is referred to as a **minimax** game.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第9章*](B18143_09.xhtml#_idTextAnchor245)中简要提到的，生成器和判别器进行一种对抗游戏。在这场游戏中，判别器的目标是*最小化*分类猫图像的错误率（也就是说，它尽可能准确地识别猫的图像）。另一方面，生成器试图*最大化*判别器的错误率。由于游戏参与者的目标是对立的，其中一方希望最小化某个指标，而另一方希望最大化相同的指标，因此这被称为**最小-最大**游戏。
- en: 'In addition to training the discriminator on the labeled training dataset,
    the discriminator also receives outputs from the generator and is asked to classify
    those outputs, as depicted in *Figure 15**.5*:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在标记的训练数据集上训练判别器外，判别器还会接收到生成器的输出，并被要求对这些输出进行分类，如图*15**.5*所示：
- en: '![Figure 15.5: GAN (source: https://developers.google.com/machine-learning/gan/gan_structure)](img/B18143_15_5.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图15.5：GAN（来源：https://developers.google.com/machine-learning/gan/gan_structure）](img/B18143_15_5.jpg)'
- en: 'Figure 15.5: GAN (source: https://developers.google.com/machine-learning/gan/gan_structure)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5：GAN（来源：https://developers.google.com/machine-learning/gan/gan_structure）
- en: If the discriminator thinks a data point is real when, in fact, it was created
    by the generator, that counts as an error. This, therefore, increases the error
    rate for the discriminator, which is a win for the generator. Conversely, if the
    discriminator accurately classifies the data point as fake, then it reduces the
    error rate for the discriminator (which is, of course, a win for the discriminator).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果判别器认为一个数据点是真实的，而实际上它是被生成器创建的，这算作一个错误。因此，这会增加判别器的错误率，这对生成器来说是一个胜利。相反，如果判别器能够准确地将数据点分类为伪造的，那么它就会降低判别器的错误率（当然，这对判别器来说也是一个胜利）。
- en: While the discriminator uses a typical mechanism such as gradient descent to
    learn from its mistakes and minimize the loss, the generator can also learn from
    the discriminator’s mistakes, and adjust its weights in a reverse process that
    aims to increase the discriminator’s error rate. This is the novel approach used
    by GANs, in which the generator doesn’t receive direct labels of “good” or “bad”
    during training, but instead receives feedback on the discriminator’s performance.
    If the discriminator accurately identifies a fake, then the gradients are sent
    back to the generator so that it can update its weights to create an output that
    better fools the discriminator in the next round.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当判别器使用典型的机制，如梯度下降，从其错误中学习并最小化损失时，生成器也可以从判别器的错误中学习，并通过一个旨在增加判别器错误率的反向过程调整其权重。这是GANs使用的创新方法，在训练过程中，生成器不会直接接收到“好”或“坏”的直接标签，而是接收到关于判别器性能的反馈。如果判别器能够准确识别出伪造的，那么梯度就会发送回生成器，以便它能够更新其权重，在下一轮中创建一个更能欺骗判别器的输出。
- en: During this process, the generator learns the underlying probability distribution
    of the real data and becomes more adept at generating new data that aligns with
    that probability distribution (that is, new data with similar properties as the
    real data). It’s also important to understand that each network takes turns in
    the training process. When the generator is being trained, the discriminator’s
    weights are frozen, and vice versa.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，生成器学习真实数据的潜在概率分布，并变得更加擅长生成与该概率分布一致的新数据（即具有与真实数据相似属性的新数据）。同时，了解每个网络在训练过程中轮流进行也很重要。当生成器正在训练时，判别器的权重被冻结，反之亦然。
- en: By pitting the models against each other in this fashion, they must continually
    get better in order to outperform each other. As the discriminator gets better
    at identifying fakes, the generator learns to generate more realistic data to
    fool the discriminator, and so on.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以这种方式让模型相互对抗，它们必须不断变得更好才能超越对方。随着判别器在识别伪造数据方面变得更好，生成器学会生成更逼真的数据来欺骗判别器，如此循环。
- en: GANs gained a lot of popularity since they were first created by Ian Goodfellow
    and colleagues in 2014, and multiple different types of GAN implementations have
    emerged, such as **conditional GANs** (**cGANs**), CycleGANs, StyleGANs, **deep
    convolutional GANs** (**DCGANs**), and progressive GANs. A comprehensive discussion
    of all of these variants would constitute more detail than is necessary in this
    book. I encourage you to research these variants further if you have a specific
    interest in them.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 2014 年由 Ian Goodfellow 和同事们首次创建以来，GANs 获得了极大的流行，出现了多种不同类型的 GAN 实现，例如 **条件
    GANs**（**cGANs**）、CycleGANs、StyleGANs、**深度卷积 GANs**（**DCGANs**）和渐进式 GANs。对所有这些变体的全面讨论将超出本书所需的详细程度。如果您对这些变体有特别的兴趣，我鼓励您进一步研究。
- en: Important note – look out for “mode collapse”
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示 – 注意“模式崩溃”
- en: In addition to the typical challenges faced when training many types of NN architectures,
    such as exploding and vanishing gradients, GANs also often suffer from a specific
    challenge that is related to their adversarial training process. This problem
    is called “mode collapse,” and it refers to a situation in which the generator
    may start producing a limited variety of outputs, especially if those outputs
    successfully trick the discriminator. Since the generator’s primary goal is to
    fool the discriminator, it may learn to generate patterns that are effective in
    doing so but do not accurately represent the target data distribution. This is
    an ongoing area of research, in which new mechanisms are being introduced to combat
    this phenomenon.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在训练许多类型的神经网络架构时面临的典型挑战，如梯度爆炸和梯度消失，GANs 还经常遭受与它们的对抗训练过程相关的特定挑战。这个问题被称为“模式崩溃”，指的是生成器可能开始产生有限种类的输出，尤其是如果这些输出成功地欺骗了判别器。由于生成器的主要目标是欺骗判别器，它可能学会生成有效的模式来做到这一点，但这些模式并不能准确代表目标数据分布。这是一个持续的研究领域，其中正在引入新的机制来对抗这种现象。
- en: Next, let’s discuss some common applications of GANs.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论一些 GANs 的常见应用。
- en: Applications of GANs
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GANs 的应用
- en: In addition to generating photorealistic images and audiorealistic data, GANs
    can be used for other interesting use cases, such as style transfer and image-to-image
    translation. For example, some online services enable you to upload a photo and
    convert it into a cartoon or make it look like an oil painting in the style of
    a famous artist such as Van Gogh. While these are fun applications, GANs are also
    utilized in various use cases such as text generation, new drug discovery, and
    other types of synthetic data generation.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 除了生成逼真的图像和逼真的音频数据外，GANs 还可用于其他有趣的用例，例如风格迁移和图像到图像的翻译。例如，一些在线服务允许你上传一张照片，并将其转换为卡通或模仿梵高等著名艺术家的风格，使其看起来像油画。虽然这些应用很有趣，但
    GANs 还被用于各种用例，如文本生成、新药发现以及其他类型的合成数据生成。
- en: We’ll discuss the importance of high-quality synthetic data later, but next,
    I’d like to introduce another popular GenAI approach, referred to as **diffusion**.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后面讨论高质量合成数据的重要性，但接下来，我想介绍另一种流行的 GenAI 方法，称为 **扩散**。
- en: Diffusion
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩散
- en: Diffusion is utilized for many of the same kinds of use cases as GANs, such
    as image and audio generation, image-to-image translation, style transfer, and
    even new drug discovery. However, diffusion uses a different approach and often
    provides more favorable results for some use cases. We’ll dive into some of the
    differences between diffusion models and GANs in this section, but first, as always,
    let’s learn about what diffusion is and how it works.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散被用于许多与GANs相同类型的用例，如图像和音频生成、图像到图像的翻译、风格迁移，甚至新的药物发现。然而，扩散使用不同的方法，并且对于某些用例通常提供更理想的结果。在本节中，我们将深入了解扩散模型和GANs之间的差异，但首先，像往常一样，让我们了解什么是扩散以及它是如何工作的。
- en: High-level concepts – noising and denoising
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 高层次概念——噪声和去噪
- en: 'At a high level, diffusion consists of two main steps:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，扩散主要由两个主要步骤组成：
- en: Adding noise to images
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向图像添加噪声
- en: Reversing the process – that is, removing noise to work back toward the original
    image
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反转这个过程——即去除噪声以回到原始图像
- en: 'I’ll describe these concepts in much more detail, starting with clarifying
    what it means to add noise to an image. *Figure 15**.6* shows an image consisting
    of noise:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我将更详细地描述这些概念，首先从阐明向图像添加噪声的含义开始。*图15.6*显示了包含噪声的图像：
- en: '![Figure 15.6: Image of noise (source: https://commons.wikimedia.org/wiki/File:256x256_Dissolve_Noise_Texture.png)](img/B18143_15_6.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图15.6：噪声图像（来源：https://commons.wikimedia.org/wiki/File:256x256_Dissolve_Noise_Texture.png)](img/B18143_15_6.jpg)'
- en: 'Figure 15.6: Image of noise (source: https://commons.wikimedia.org/wiki/File:256x256_Dissolve_Noise_Texture.png)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.6：噪声图像（来源：https://commons.wikimedia.org/wiki/File:256x256_Dissolve_Noise_Texture.png）
- en: Some of you may be familiar with the kind of image shown in *Figure 15**.6*
    if you’ve ever seen noise displayed on a TV screen when it is not tuned to a channel
    that provides a image signal. This image consists purely of noise, in which we
    cannot make out any discernable shapes. However, if we were to take a high-quality
    image and add some noise to it, then it would become somewhat blurry, but we could
    still make out what’s in the image, as long as not too much noise is added. The
    more noise we add, the more difficult it becomes to identify the contents of the
    image.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你们中的一些人曾经看到过*图15.6*所示的图像，那么你们可能对在电视屏幕上未调谐到提供图像信号的频道时显示的噪声很熟悉。这个图像纯粹由噪声组成，我们无法辨认出任何可辨别的形状。然而，如果我们向一张高质量的图像添加一些噪声，那么它就会变得有些模糊，但我们仍然可以辨认出图像中的内容，只要添加的噪声不是太多。我们添加的噪声越多，识别图像内容就越困难。
- en: This concept is used in the **noising** process when training diffusion models.
    We take images and add small amounts of noise to them to make it progressively
    more difficult to identify what’s in the images. This is also referred to as the
    **forward** **diffusion** process.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念在训练扩散模型时的**噪声**过程中被使用。我们向图像添加少量噪声，使其逐渐难以识别图像中的内容。这也被称为**正向****扩散**过程。
- en: The **reverse diffusion** or **denoising** process then tries to take a noisy
    image and work back toward the original image. This might sound a bit pointless
    – that is, why bother adding noise to images and then trying to train a model
    to learn how to remove noise to generate the original images? Well, just as in
    the case of GANs, we are training our model to understand the probability distribution
    of the original dataset. Diving into a bit more detail, our model actually learns
    to predict noise in the input. The next step of the process, then, is to simply
    remove that noise from the input in order to estimate or generate the desired
    denoised output.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，**反向扩散**或**去噪**过程试图从一个噪声图像中恢复，并逐步回到原始图像。这听起来可能有点没有意义——也就是说，为什么要费心向图像添加噪声，然后再训练一个模型来学习如何去除噪声以生成原始图像？好吧，正如GANs的情况一样，我们正在训练我们的模型来理解原始数据集的概率分布。更深入一点，我们的模型实际上学会了预测输入中的噪声。因此，过程的下一步就是简单地从输入中去除噪声，以便估计或生成所需的去噪输出。
- en: The reason this is called diffusion may be somewhat self-explanatory, but more
    specifically, the name relates to a concept in the field of non-equilibrium thermodynamics.
    That’s a bit of a mouthful, and, don’t worry – we’re not going to dive into the
    physical concept except to briefly introduce why it is named as such.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这个被称为扩散的原因可能有些不言自明，但更具体地说，这个名字与非平衡热力学领域的一个概念有关。这听起来有点复杂，但不用担心——我们不会深入探讨物理概念，只是简要介绍为什么它被这样命名。
- en: The analogy often used to explain this concept is to think of a drop of ink
    being added to a bucket of water. When the ink is added to the water, at first,
    it occupies a small space in a specific location in the bucket. However, over
    time, the ink dissipates throughout the bucket of water. Soon, it has spread throughout
    the bucket, and the bucket then contains a mixture of water and ink. The color
    of the contents of the bucket may be different from the color that existed before
    you added the ink, but it’s no longer possible to pinpoint the specific location
    of the ink in the bucket. This is the forward diffusion process, in which the
    ink diffuses throughout the water in the bucket.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 常常用来解释这个概念的类比是想象将一滴墨水添加到水桶中。当墨水被添加到水中时，最初，它在桶中一个特定的位置占据了一小片空间。然而，随着时间的推移，墨水会在整个水桶中扩散。很快，它就遍布了整个水桶，水桶中包含着水和墨水的混合物。水桶中内容的颜色可能与你添加墨水之前存在的颜色不同，但已无法确定墨水在水桶中的具体位置。这就是正向扩散过程，其中墨水在整个水桶中的水中扩散。
- en: In the physical world, this process of diffusion is usually impossible to reverse
    – that is, no matter what you try to do, you will not be able to separate the
    ink from the water and condense the drop of ink back into the original position
    it occupied when it was first added to the bucket.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在物理世界中，这种扩散过程通常是不可能逆转的——也就是说，无论你尝试做什么，你都无法将墨水从水中分离出来，并将墨水滴重新凝结回最初添加到桶中的位置。
- en: In the case of diffusion in ML, however, we try to reverse the process and get
    back to the original input state.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在机器学习中的扩散过程中，我们试图逆转这个过程，回到原始输入状态。
- en: Diving deeper
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 深入探讨
- en: 'It’s important to understand that noise is added to the data in a controlled
    manner. We don’t simply add completely random noise, but instead, we add what
    is referred to as **Gaussian noise**, meaning the noise is characterized by the
    “normal” (or “Gaussian”) probability distribution. As a refresher, a Gaussian
    distribution is represented by the familiar “bell curve” as depicted in *Figure
    15**.7*:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解噪声是以受控方式添加到数据中的。我们不仅仅添加完全随机的噪声，而是添加所谓的**高斯噪声**，这意味着噪声具有“正常”（或“高斯”）概率分布的特征。作为复习，高斯分布由熟悉的“钟形曲线”表示，如*图15**.7*所示：
- en: '![Figure 15.7: Gaussian distribution](img/B18143_15_7.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图15.7：高斯分布](img/B18143_15_7.jpg)'
- en: 'Figure 15.7: Gaussian distribution'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.7：高斯分布
- en: As we can see in *Figure 15**.7*, the Gaussian distribution is symmetrical,
    with its mean at the center of the bell curve. The width of the curve represents
    the variance (or its square root, the standard deviation) of the distribution
    – that is, how far from the mean we are likely to see data points that fit the
    distribution. In a Gaussian distribution, data points that are most likely to
    occur appear closer to the mean, while rarer data points are further from the
    mean. In *Figure 15**.7*, the mean is 0, and the standard deviation is 1, which
    is a special type of Gaussian distribution referred to as the **standard normal
    distribution**. By using normally distributed noise, we can easily change the
    intensity of noise by tweaking the variance of the distribution.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在*图15**.7*中看到的那样，高斯分布是对称的，其均值位于钟形曲线的中心。曲线的宽度代表分布的方差（或其平方根，即标准差）——也就是说，我们可能会看到与分布相符合的数据点距离均值有多远。在高斯分布中，最有可能发生的数据点出现在均值附近，而较少见的数据点则离均值更远。在*图15**.7*中，均值为0，标准差为1，这是一种被称为**标准正态分布**的特殊类型的高斯分布。通过使用正态分布的噪声，我们可以通过调整分布的方差来轻松改变噪声的强度。
- en: In addition to controlling noise by ensuring it fits the normal distribution,
    we also introduce noise in a controlled manner by incrementally adding noise to
    our source images during training, rather than adding too much noise all at once.
    The addition of noise in this manner is performed according to a **schedule**,
    in which more noise is added at different time intervals in the process. Remember
    that, as we discussed in earlier sections of this chapter, randomization is important
    in generative processes because we don’t just want to produce exact images from
    the original dataset but instead want to produce similar images.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过确保其符合正态分布来控制噪声外，我们还在训练过程中以可控的方式引入噪声，通过逐步向我们的源图像添加噪声，而不是一次性添加过多噪声。以这种方式添加噪声是根据一个**时间表**进行的，在这个过程中，在不同的时间间隔内添加更多的噪声。记住，正如我们在本章前面的部分所讨论的，在生成过程中随机化很重要，因为我们不仅想要从原始数据集中生成精确的图像，而是想要生成相似的图像。
- en: To introduce some controllable randomization into the process, the mean and
    the variance of noise added in each step are different, and we also add different
    amounts of noise at various parts of the schedule. Each time we add noise, we
    form a step in a Markov chain. As you may remember from earlier in this chapter,
    each step in a Markov chain is dependent only on the step that directly precedes
    it, and this is important to understand in the context of the reverse diffusion
    process. As we add noise in each step (during the forward diffusion process),
    we are training our model to identify (or predict) noise that has been added.
    Then, during the reverse diffusion process, we start with a noisy image, and we
    want to try to generate the image in the previous step in the Markov chain and
    progressively work our way through the chain in that manner until we get back
    to an approximation of the original image.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在过程中引入一些可控的随机化，每一步中添加噪声的均值和方差是不同的，我们还在时间表的各个部分添加不同数量的噪声。每次添加噪声，我们就在马尔可夫链中形成一个步骤。如您在本章前面的部分所记得，马尔可夫链中的每一步只依赖于直接
    preceding 它的步骤，这在反向扩散过程的上下文中非常重要。当我们每一步（在正向扩散过程中）添加噪声时，我们正在训练我们的模型来识别（或预测）已经添加的噪声。然后，在反向扩散过程中，我们从一个噪声图像开始，我们想要尝试生成马尔可夫链中前一步的图像，并以此方式逐步通过链，直到我们回到原始图像的近似。
- en: To describe this process in more detail, imagine that we start with a image
    of a cat, and we progressively add noise in each step until we finally end up
    with a image similar to the image depicted in *Figure 15**.6*, which consists
    almost purely of noise. It would be very difficult to try to jump from such an
    extremely noisy image back to a image of a cat, so we instead train our model
    on how to progressively work backward through each step in the Markov chain, predicting
    the small amount of noise that was added in each step. As our model gets better
    and better, it can more clearly distinguish between noise and the underlying probability
    distribution of the original dataset. This understanding of probability distributions
    allows us to sample new images by starting with pure noise and iteratively denoising
    them according to what the model has learned.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更详细地描述这个过程，想象一下我们从一个猫的图像开始，我们逐步在每一步添加噪声，直到最终我们得到一个与*图15**.6*相似的图像，该图像几乎完全是噪声。试图从如此极端的噪声图像跳回到猫的图像是非常困难的，所以我们相反地训练我们的模型，使其能够逐步反向通过马尔可夫链中的每一步，预测每一步中添加的小量噪声。随着我们的模型越来越好，它能够更清楚地区分噪声和原始数据集的潜在概率分布。这种对概率分布的理解使我们能够通过从纯噪声开始并迭代去噪，根据模型所学的知识来采样新的图像。
- en: At the beginning of this section, I promised that I would outline some important
    differences between diffusion and GANs. Let’s take a look at those differences
    next.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的开始，我承诺要概述一些扩散与 GANs 之间的重要差异。让我们接下来看看这些差异。
- en: Differences between diffusion and GANs
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 扩散与 GANs 的差异
- en: I mentioned the concept of mode collapse in the context of training GANs and
    how that can introduce instability into the training process. Diffusion uses a
    more stable training process, but diffusion models require many steps to generate
    data, which can be computationally intensive. Some recent advances aim to reduce
    the number of required steps and computational costs, but this is still a notable
    consideration for diffusion models.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我在GANs训练的上下文中提到了模式坍塌的概念，以及这如何可能引入训练过程中的不稳定性。扩散使用了一个更稳定的训练过程，但扩散模型需要许多步骤来生成数据，这可能会计算密集。一些最近的进展旨在减少所需的步骤和计算成本，但这仍然是扩散模型的一个显著考虑因素。
- en: Similarly, at inference time, generating samples from diffusion models can be
    computationally expensive, whereas GANs can generate samples more quickly because
    they require only a single forward pass through the generator network.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在推理时间，从扩散模型中生成样本可能计算成本很高，而GANs可以更快地生成样本，因为它们只需要通过生成网络的单次正向传递。
- en: Which approach is best, then? Well, it’s a matter of selecting the right tool
    for the job based on the business requirements for a specific use case.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，哪种方法最好呢？好吧，这是一个根据特定用例的业务需求选择正确工具的问题。
- en: 'Next, it’s time to move on to discussing perhaps the crowning glory of the
    GenAI world in recent times: LLMs.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，是时候讨论可能是最近GenAI世界中的巅峰之作：LLMs了。
- en: LLMs
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs
- en: This is another case in which the name of the technology very accurately describes
    what the technology is; LLMs are large models that are particularly useful for
    language-based use cases, such as the summarization of large amounts of textual
    data, or chatbots that can have a conversation with a human.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这又是一个技术名称非常准确地描述了该技术是什么的例子；LLMs是大型模型，特别适用于基于语言的使用案例，例如大量文本数据的摘要，或者能够与人类进行对话的聊天机器人。
- en: Behind the scenes, they use statistical methods to process, predict, and generate
    language based on the context provided to them. They are trained on diverse datasets
    that include large amounts of textual information, from books and articles to
    websites and human interactions, enabling them to learn language patterns, grammar,
    and semantics.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，它们使用统计方法根据提供的上下文处理、预测和生成语言。它们在包含大量文本信息的多样化数据集上进行训练，从书籍和文章到网站和人类交互，使它们能够学习语言模式、语法和语义。
- en: How large are they? Well, some of the latest models at the time of writing this
    in February 2024 consist of billions or even trillions of parameters. That’s pretty
    huge! Somebody might read this book 20 years from now and laugh at the fact that
    we considered these sizes to be huge, but these are the biggest models on the
    planet at the moment, and they constitute an enormous advancement from any models
    that have existed before now.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 它们有多大呢？好吧，截至2024年2月撰写本文时的一些最新模型由数十亿甚至数万亿个参数组成。这相当巨大！二十年后，有人可能会读到这本书并嘲笑我们把这些大小视为巨大，但这些都是目前地球上最大的模型，并且与之前存在的任何模型相比，它们构成了巨大的进步。
- en: Before diving into details on how LLMs are created, let’s take a look at some
    historical milestones that have led to their evolution.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨LLMs是如何创建的细节之前，让我们先看看一些历史里程碑，这些里程碑导致了它们的演变。
- en: Evolution of LLMs
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs的演变
- en: The history of language models began with simple rule-based systems that used
    hand-coded rules to interpret and generate language. It turns out that trying
    to hand-code all of the complex rules of human language is quite impractical,
    but the research in this field had to start somewhere.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的历史始于简单的基于规则的系统，这些系统使用手工编写的规则来解释和生成语言。事实证明，试图手工编写人类语言的所有复杂规则是非常不切实际的，但这一领域的研究必须从某个地方开始。
- en: Next, statistical methods such as N-gram models used probabilities to predict
    the likelihood of a sequence of words. They were trained on large bodies of text
    and could capture more language nuances than rule-based systems, but they still
    struggled with long-term dependencies in text.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，统计方法如N-gram模型使用概率来预测一系列单词的可能性。它们在大规模文本上进行了训练，能够比基于规则的系统捕捉到更多的语言细微差别，但它们在处理文本中的长期依赖关系方面仍然存在困难。
- en: The next step forward was to use models such as HMMs and simple NNs for language
    tasks, and while these models offered better performance by learning more advanced
    patterns in data, they were still limited in complexity.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 向前迈出的下一步是使用如HMMs和简单的NNs等模型进行语言任务，虽然这些模型通过学习数据中的更高级模式提供了更好的性能，但它们在复杂性方面仍然有限。
- en: Major breakthroughs began when scientists started applying **deep NNs** (**DNNs**)
    to language use cases. In [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245) and
    [*Chapter 14*](B18143_14.xhtml#_idTextAnchor348), we discussed the evolution of
    RNNs, LSTM networks, and Transformers, and how they each enabled progressively
    more complex language processing. The pivotal event that occurred in the industry
    was when Google invented the Transformer architecture (Vaswani et al., 2017),
    which has become the primary technology used in all of the biggest and most advanced
    LLMs in the industry today.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当科学家开始将**深度神经网络**（DNNs）应用于语言用例时，重大突破开始了。在[*第九章*](B18143_09.xhtml#_idTextAnchor245)和[*第十四章*](B18143_14.xhtml#_idTextAnchor348)中，我们讨论了RNNs、LSTM网络和Transformers的演变，以及它们如何各自使语言处理变得更加复杂。行业中的一个关键事件是谷歌发明了Transformer架构（Vaswani等人，2017年），这已成为今天行业中最庞大和最先进的LLMs的主要技术。
- en: When we discussed the overall evolution of AI/ML in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015),
    I mentioned that it was not only the development of more complex ML algorithms
    that led to the kinds of breakthroughs we’ve seen in recent decades but also advancements
    in computing power and the proliferation of available data for training models.
    These factors all work together to form a kind of ecosystem in which we continue
    to advance all of these technologies in concert.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在[*第一章*](B18143_01.xhtml#_idTextAnchor015)中讨论人工智能/机器学习的整体演变时，我提到，不仅更复杂的机器学习算法的发展导致了我们在最近几十年看到的突破，而且计算能力的进步和可用于训练模型的数据的激增也起到了作用。这些因素共同形成了一种生态系统，我们在其中共同推进所有这些技术。
- en: Next, we’ll dive into how LLMs are created.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入了解LLMs是如何创建的。
- en: Building LLMs
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建大型语言模型（LLMs）
- en: 'Let’s get one thing out of the way, right from the beginning; it’s unlikely
    that you or I could create our own LLMs from scratch, for two main reasons:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一开始就澄清一件事；由于两个主要原因，你或我不太可能从头开始创建自己的LLMs：
- en: Training LLMs requires obscenely large amounts of data. Some of the LLMs you
    or I would interact with are trained on the entire internet, for example. This
    is one of the things that makes them so powerful and knowledgeable; they are trained
    on all publicly available data that humans have ever created, in addition to some
    private and proprietary datasets owned or sourced by the companies that trained
    them.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练LLMs需要极其大量的数据。例如，你或我会与之交互的一些LLMs是在整个互联网上训练的。这就是它们如此强大和知识渊博的原因之一；它们不仅训练了人类所创建的所有公开可用的数据，还训练了一些由训练它们的公司拥有或获取的私有和专有数据集。
- en: It is generally extremely expensive to train LLMs due to the sheer amount of
    computing power needed. I’m referring to using thousands of high-performance accelerators,
    such as the latest and greatest GPUs and TPUs, for months on end to train these
    models, and those things don’t come cheap!
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于需要大量的计算能力，训练LLMs通常极其昂贵。我指的是使用数千个高性能加速器，如最新的GPU和TPUs，连续数月来训练这些模型，而这些东西并不便宜！
- en: As a result, most people and companies will use LLMs that have already been
    **pre-trained**, which I will describe next.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，大多数人公司和公司将使用已经**预训练**的LLMs，我将在下面进行描述。
- en: The LLM training process
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM训练过程
- en: 'While the companies that build commercially available LLMs almost certainly
    use a lot of secret magic behind the scenes that they are unlikely to share externally,
    the following high-level steps are generally involved in the LLM training process:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然构建商业可用LLMs的公司几乎肯定在幕后使用了大量的秘密魔法，他们不太可能对外分享，但以下这些高级步骤通常涉及在LLM训练过程中：
- en: Unsupervised or semi-supervised pre-training
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督或半监督预训练
- en: Supervised tuning
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督微调
- en: Let’s take a look at each of these in more detail.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看这些内容。
- en: Unsupervised or semi-supervised pre-training
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 无监督或半监督预训练
- en: Remember from our activities earlier in this book that supervised training requires
    labeled datasets, which can be cumbersome and expensive to source or create. Bearing
    in mind that LLMs are often trained on enormous bodies of data such as the entire
    internet, it would be impossible to label all of that data. Instead, a very clever
    trick is used to train LLMs in such a way that they can learn from these large
    bodies of text without the need for explicit labeling, and that trick is called
    **masked language** **modeling** (**MLM**).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 记得本书前面的活动中提到的，监督训练需要标记的数据集，这可能很麻烦，而且获取或创建成本高昂。考虑到LLM通常是在大量数据上训练的，比如整个互联网，标记所有这些数据是不可能的。相反，使用一个非常巧妙的技巧来训练LLM，这样它们可以从这些大量的文本中学习，而不需要显式标记，这个技巧被称为**掩码语言模型**（**MLM**）。
- en: 'MLM is actually a very simple yet effective concept; it’s essentially a game
    of “fill in the blanks.” You may remember playing this game as a child or encountering
    it in exam scenarios. Quite simply, we take a sentence and blank out (or mask)
    one or more of the words, and the task is then to guess what the missing words
    should be. For example, consider the following sentence:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: MLM实际上是一个非常简单但有效的概念；它本质上是一个“填空”游戏。你可能记得小时候玩过这个游戏，或者在考试场景中遇到过。简单来说，我们取一个句子，并掩盖（或掩盖）一个或多个单词，然后任务是猜测缺失的单词应该是什么。例如，考虑以下句子：
- en: “You really hit the nail on the [BLANK] with that last comment.”
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: “你真的在最后那个评论中一针见血地指出了[空白]。”
- en: What do you think would be the most appropriate word to use for filling in the
    *[BLANK]* portion of that sentence? If you’re familiar with the term, “hit the
    nail on the head,” then you may have guessed that “head” would be the best word
    to use, and you’d be correct in guessing that.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为哪个词最适合用来填补那个句子的*[空白]*部分？如果你熟悉“一针见血”这个术语，那么你可能已经猜到“头”将是最佳选择，而且你的猜测是正确的。
- en: This approach can be extremely effective in training LLMs because we can feed
    millions of sentences to the LLM, randomly masking out various words and asking
    the LLM to predict what the correct words should be. The beauty of this approach
    is that the masked-out words become the labels, so we don’t need to explicitly
    label the dataset. This is why we refer to this process as UL or **semi-supervised
    learning** (**SSL**). For example, after the LLM predicts the word to use, we
    can reveal the word that the sentence actually contained in the masked-out position,
    and the model can then learn from that. If the model predicted a different word,
    it would count as an error, and the loss function would reflect this accordingly
    during training.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在训练LLM时可以非常有效，因为我们可以向LLM提供数百万个句子，随机掩盖各种单词，并要求LLM预测正确的单词应该是什么。这种方法的优点是，掩盖的单词成为标签，所以我们不需要显式标记数据集。这就是为什么我们称这个过程为UL或**半监督学习**（**SSL**）。例如，在LLM预测了要使用的单词之后，我们可以揭示句子中实际包含在掩盖位置上的单词，然后模型可以从中学习。如果模型预测了不同的单词，它将被视为错误，损失函数将在训练期间相应地反映这一点。
- en: 'The important thing to note is that the model gets to see billions of different
    sentences during training, and by doing so, it sees words being used in various
    contexts. Over time, it builds up an understanding of what each word means and
    which other words it commonly appears alongside. Conceptually, it can build up
    a graph of how various words relate to each other, as depicted in *Figure 15**.8*:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的重要一点是，模型在训练过程中可以看到数十亿个不同的句子，通过这样做，它可以看到单词在各种语境中的使用。随着时间的推移，它建立起对每个单词含义的理解以及它通常与哪些其他单词一起出现。从概念上讲，它可以建立起一个图，展示各种单词之间的关系，如图*图15.8*所示：
- en: '![Figure 15.8: Graph of word associations](img/B18143_15_8.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图15.8：单词关联图](img/B18143_15_8.jpg)'
- en: 'Figure 15.8: Graph of word associations'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.8：单词关联图
- en: 'In *Figure 15**.8*, we can see how various words in the English language relate
    to each other. Notice how some words may be ambiguous, such as the word “Turkey,”
    which could relate to the country, to the animal, or to food. Consider the following
    sentences:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图15.8*中，我们可以看到英语中各种单词之间的关系。注意有些单词可能是模糊的，比如单词“Turkey”，它可以指代国家、动物或食物。考虑以下句子：
- en: “I really like eating turkey.”
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我真的很喜欢吃火鸡。”
- en: “Last year, we went on vacation in Turkey.”
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “去年，我们去土耳其度假了。”
- en: “I really liked the food in Turkey.”
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我真的很喜欢土耳其的食物。”
- en: Assume that the LLM has seen all of those sentences during training. By seeing
    these sentences (and billions more) and learning how words are used in various
    contexts, the LLM forms a probabilistic understanding of the meaning (or **semantic
    context**) of the words. For example, how does it know that the word “Turkey”
    might refer to a country based on a sentence about people going on vacation there?
    Well, from seeing the word “vacation” in millions or billions of sentences, it
    comes to understand that people go on vacation in places, so in this context,
    “Turkey” must be a place. It also would have seen other sentences containing the
    words “Turkey” and “immigration,” and it would have developed an understanding
    that immigration refers not only to places but, more specifically, to countries.
    It would also have learned what the word “country” means and that its plural form
    is “countries,” based on seeing those words in many other contexts. Similarly,
    it learns that the word “eat” refers to food, so in the sentence “I really like
    eating turkey,” it understands that “turkey” must be a type of food.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 假设LLM在训练过程中看到了所有这些句子。通过看到这些句子（以及数十亿个其他句子）并学习词汇在不同语境中的使用方式，LLM形成了对词汇意义（或**语义上下文**）的概率理解。例如，它是如何知道“Turkey”这个词可能指一个国家，基于关于人们去那里度假的句子？嗯，通过在数百万或数十亿个句子中看到“vacation”这个词，它理解到人们去地方度假，所以在这个语境中，“Turkey”必须是一个地方。它也可能会看到包含“Turkey”和“immigration”这两个词的其他句子，并且会形成对移民不仅指地方，而且更具体地指国家的理解。它还会通过在许多其他语境中看到这些词来学习“country”这个词的含义以及它的复数形式是“countries”。同样，它学习到“eat”这个词指的是食物，所以在这个句子“我真的很喜欢吃火鸡”中，它理解到“turkey”必须是一种食物。
- en: 'I’ve presented just a handful of simple sentences here, and we can already
    see a proliferation of different potential combinations of words and how they
    relate to each other. Imagine the complexity of the combinations that exist when
    we bring every sentence on the internet into scope. This is what LLMs learn: extremely
    complex webs of associations and the underlying meaning of the concepts those
    words represent. This is an enormous step forward in terms of intelligence that
    LLMs bring to the AI industry, and this is actually how our own minds learn to
    understand words, also, which is fascinating. As children, we don’t just learn
    individual words, but we learn words as representations of concepts, usually by
    forming graphs of associations. For example, imagine you were a young child who
    was trying to climb up a tree, and your parent said, “Don’t climb that tree, or
    you might fall and injure yourself.” You may not have heard all of those words
    before, but from hearing that sentence, you might learn about the concept of a
    tree, or of climbing, or falling, or injuring, and you also may intuit that falling
    and injuring are undesirable things to happen.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里只展示了一小部分简单的句子，但我们已经可以看到不同潜在词汇组合的激增以及它们之间的关系。想象一下，当我们把互联网上的每一句话都纳入考虑范围时，组合的复杂性会有多大。这就是LLM（大型语言模型）学习的内容：极其复杂的关联网络以及这些词汇所代表的概念的深层含义。这在智能方面是LLM为AI行业带来的巨大进步，而且这也是我们自己的大脑学习理解词汇的方式，这非常有趣。作为孩子，我们不仅学习单个词汇，而且学习词汇作为概念的代表，通常是通过形成关联图。例如，想象你是一个试图爬树的小孩子，你的父母说：“别爬那棵树，否则你可能会摔倒受伤。”你可能之前没有听过这些词汇，但通过听到这个句子，你可能会了解树、爬树、摔倒或受伤的概念，你也可能会直觉到摔倒和受伤是不希望发生的事情。
- en: In addition to masking words within sentences, another method called **next
    sentence prediction** (**NSP**) may also be used to train our LLMs to not only
    predict words but, as the name suggests, also to predict entire sentences based
    on the provided context. In this case, the LLM is presented with pairs of sentences,
    and sometimes these sentences have a natural sequential relationship (for example,
    two consecutive lines from an article), or in other cases, they are unrelated
    sentences. The LLM’s task is to determine if the second sentence logically follows
    from the first, and by doing this, it learns to discriminate between logical and
    incoherent sequences of sentences. This pushes the LLM to reason about context
    across multiple sentences, which helps it generate coherent text in longer responses
    to requests.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在句子中遮蔽单词之外，还有一种称为**下一句预测**（**NSP**）的方法也可以用来训练我们的LLMs，使其不仅能够预测单词，正如其名称所暗示的，还能根据提供的上下文预测整个句子。在这种情况下，LLM会看到句子对，有时这些句子有自然的顺序关系（例如，文章中的连续两行），或者在其他情况下，它们是不相关的句子。LLM的任务是判断第二个句子是否逻辑上跟随第一个句子，通过这样做，它学会了区分逻辑上连贯和不连贯的句子序列。这促使LLM在多个句子之间推理上下文，这有助于它在更长的响应请求中生成连贯的文本。
- en: It’s important to note that there’s an additional layer of abstraction in how
    the LLMs learn. Earlier in this chapter, we discussed embeddings and latent space.
    Generally, the kinds of associations I’ve just outlined occur in the latent space.
    The latent space is the LLM’s representation of the world and all of the various
    concepts and associations it learns. Concepts with similar meaning or semantic
    context may be located nearer to each other in the latent space.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，LLMs的学习中还有一个额外的抽象层。在本章的早期，我们讨论了嵌入和潜在空间。一般来说，我刚才概述的关联发生在潜在空间中。潜在空间是LLM对世界的表示以及它所学习的所有各种概念和关联。具有相似意义或语义上下文的概念可能在潜在空间中彼此更近。
- en: After the LLM has learned everything it can learn from the available data via
    the process I’ve just described, the pre-training phase is complete. At this point,
    the LLM has built its latent space representation of the world, and the next step
    is to teach it how to be useful in responding to requests from humans or other
    machines. Many different processes can be used for this purpose, but I’ll start
    with the common practice of supervised tuning.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM通过我刚才描述的过程从可用数据中学习到它能学到的一切之后，预训练阶段就完成了。在这个时候，LLM已经建立了它对世界的潜在空间表示，下一步是教它如何对人类或其他机器的请求做出有用的响应。可以用于此目的的不同过程有很多，但我会从常见的监督调优实践开始。
- en: Supervised tuning of LLMs
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs的监督调优
- en: As we know by now, supervised training means that we have a dataset that contains
    labels that can be used to teach a model. For example, if we want our model to
    perform sentiment analysis, we can label phrases in the dataset with the sentiments
    expressed by those phrases, such as positive, negative, or neutral. Similarly,
    for summarization, we could show the model examples of good summarizations, and
    the model could learn from those examples.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，监督训练意味着我们有一个包含标签的数据集，这些标签可以用来教导一个模型。例如，如果我们想让我们的模型执行情感分析，我们可以在数据集中对短语进行标记，以反映这些短语所表达的情感，例如积极、消极或中性。同样，对于摘要，我们可以向模型展示好的摘要示例，模型可以从这些示例中学习。
- en: While enormous datasets are usually needed for the pre-training phase, supervised
    tuning of LLMs requires smaller datasets that are carefully curated and labeled
    for the particular task at hand. Considering that LLMs usually contain a lot of
    knowledge from the pre-training phase, surprisingly good tuning results can come
    from just a few (or perhaps a few hundred) examples in the tuning dataset. Again,
    this is similar to how humans incrementally learn new skills. For example, imagine
    there are two people, one of whom has never learned to drive, and another who
    has been driving cars for 20 years. Now, we want each of those people to learn
    how to drive a large truck by next week. Who is most likely to succeed? The person
    who has been driving cars for 20 years already has a lot of knowledge of the rules
    of the road and how to operate vehicles, so learning how to drive a large truck
    will just require a relatively small amount of incremental learning in comparison
    to the person who has never driven any vehicles before.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在预训练阶段通常需要巨大的数据集，但LLM的监督式调整需要更小的数据集，这些数据集需要精心策划和标记，以适应特定任务。考虑到LLM通常包含大量来自预训练阶段的知识，仅从调整数据集中的少量（或可能只有几百个）示例中就可以获得令人惊讶的调整结果。再次强调，这与人类逐步学习新技能的方式相似。例如，想象有两个人，其中一个人从未学习过开车，另一个人已经开车20年了。现在，我们希望这两个人都能在下周学会开大型卡车。谁更有可能成功？那个已经开车20年的人已经对交通规则和如何操作车辆有了很多知识，因此与从未开过车的人相比，学习如何开大型卡车只需要相对较少的增量学习。
- en: It’s also important to understand that there are various approaches or levels
    of tuning that we can apply. For example, **few-shot learning** (**FSL**) refers
    to a practice in which we provide just a few examples to the LLM. We could even
    provide these examples along with our prompt (that is, our request to the LLM)
    rather than needing to provide a dedicated training dataset. This can be effective
    for some tasks, but, of course, it is quite limited since we are only providing
    a few examples. When examples are provided along with a prompt, this is an example
    of prompt engineering, which is an emerging science in itself that focuses on
    how to craft requests to LLMs in ways that produce the best results. In these
    cases, the underlying LLM generally does not get retrained on the examples we
    provide. Instead, the LLM simply uses the examples provided to tailor its responses
    in alignment with those examples.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是要理解，我们可以应用各种不同的调整方法或级别。例如，**少样本学习**（**FSL**）指的是一种实践，即我们只向LLM提供少量示例。我们甚至可以在我们的提示（即我们对LLM的请求）中提供这些示例，而不是需要提供专门的训练数据集。这对于某些任务可能是有效的，但当然，由于我们只提供了少量示例，所以这种方法相当有限。当示例与提示一起提供时，这是一个提示工程的例子，提示工程本身是一门新兴的科学，它专注于如何以产生最佳结果的方式向LLM提出请求。在这些情况下，底层LLM通常不会根据我们提供的示例进行重新训练。相反，LLM只是使用提供的示例来调整其响应，使其与这些示例保持一致。
- en: On the other side of the spectrum is **full FT** (**FFT**). This level of tuning
    could require thousands of examples, and the model’s parameters get updated based
    on those examples (that is, the model learns and incorporates the new data into
    its core structure).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在光谱的另一端是**全FT**（**FFT**）。这种级别的调整可能需要数千个示例，并且模型的参数会根据这些示例进行更新（即模型学习并将新数据纳入其核心结构）。
- en: Then, there are levels in between, such as **adapter tuning**, in which **adapter
    layers** are inserted into the original model, and only those layers get tuned
    and updated while the original model weights are frozen. Another type of tuning
    may focus only on updating the output layer of the model while leaving the rest
    of the model untouched.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，还有介于两者之间的级别，例如**适配器调整**，在这种调整中，**适配器层**被插入到原始模型中，并且只有这些层被调整和更新，而原始模型的权重保持冻结。另一种调整可能只关注更新模型的输出层，而其余模型保持不变。
- en: On a slightly separate but related note, I want to briefly introduce the topic
    of **reinforcement learning from human feedback** (**RLHF**). Rather than updating
    the LLM with more data, this method mainly focuses on teaching the LLM to respond
    in ways that are more favorable to humans, such as aligning with human values.
    In this approach, the LLM may provide multiple responses to a prompt, and a human
    can select (and therefore label) which response is preferred, based on nuances
    of human communication and culture, such as tone, sentiment, ethical considerations,
    and many other factors. We introduced the concept of **reinforcement learning**
    (**RL**) in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015). In the case of RLHF
    for tuning LLMs, the human feedback is used as the reward signal, teaching the
    LLM to generate outputs that better align with human preferences, even if there
    might be multiple technically “correct” ways to respond.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个稍微独立但相关的方面，我想简要介绍**从人类反馈中进行强化学习**（**RLHF**）这一主题。这种方法不是通过添加更多数据来更新LLM，而是主要关注教会LLM以更符合人类的方式做出反应，例如与人类价值观保持一致。在这种方法中，LLM可能会对一个提示提供多个响应，人类可以根据人类沟通和文化的细微差别（如语气、情感、道德考虑等许多因素）选择（并因此标记）哪个响应更受欢迎。我们在[*第一章*](B18143_01.xhtml#_idTextAnchor015)中介绍了**强化学习**（**RL**）的概念。在RLHF调整LLM的情况下，人类反馈被用作奖励信号，教会LLM生成更符合人类偏好的输出，即使可能存在多种技术上“正确”的响应方式。
- en: We will explore tuning in more detail in the next chapter, but before we move
    on to that chapter, let’s reflect on what we’ve learned so far.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章更详细地探讨调整，但在我们进入下一章之前，让我们反思一下到目前为止我们已经学到了什么。
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We started this chapter by introducing the fundamental topics that underpin
    GenAI, including concepts such as embeddings and latent space. We then described
    what GenAI is and how it contrasts against “traditional AI,” whereby traditional
    AI typically tries to predict a specific answer, such as a revenue forecast based
    on historical data or identifying whether a image contains a cat, but GenAI goes
    beyond those kinds of tasks and creates new content.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以介绍支撑GenAI的基本主题开始本章，包括诸如嵌入和潜在空间等概念。然后，我们描述了GenAI是什么，以及它与“传统AI”的区别，传统AI通常试图根据历史数据预测特定答案，例如收入预测或识别图像中是否含有猫，但GenAI超越了这些任务，并创造了新的内容。
- en: We dived into the role of probability in GenAI versus traditional AI, and we
    discussed how traditional AI often uses conditional probability to predict the
    values of a target variable based on the values of features in the dataset. On
    the other hand, GenAI approaches typically try to learn the joint probability
    distribution of both the features and the target variable.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了概率在GenAI和传统AI中的作用，并讨论了传统AI通常如何使用条件概率根据数据集中特征的值来预测目标变量的值。另一方面，GenAI的方法通常试图学习特征和目标变量的联合概率分布。
- en: Next, we explored the evolution of GenAI and various model development milestones
    that led to the kinds of models we use today. We began this exploration by covering
    early developments such as Markov chains, HMMs, RBMs, and DBNs. We then covered
    more recent developments such as AEs, GANs, and diffusion.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们探讨了GenAI的演变以及导致我们今天使用的各种模型的各种模型开发里程碑。我们通过介绍早期发展，如马尔可夫链、HMMs、RBMs和DBNs开始了这一探索。然后，我们介绍了更近期的进展，如AEs、GANs和扩散。
- en: Finally, we dived into the world of LLMs. We highlighted major milestones in
    their evolution by refreshing what we had learned in previous chapters about RNNs,
    LSTM networks, and Transformers. We then discussed how LLMs are trained, including
    the kinds of mechanisms used in the pre-training phase, such as MLM and NSP, and
    how FFT can be used to further train the models for specific use cases.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们深入到了LLMs的世界。我们通过回顾之前章节中关于RNNs、LSTM网络和Transformers的学习内容，突出了它们发展过程中的主要里程碑。然后，我们讨论了LLMs的训练过程，包括预训练阶段使用的各种机制，如MLM和NSP，以及如何使用FFT来进一步训练针对特定用例的模型。
- en: Although this is an introduction chapter for the GenAI section of our book,
    we’ve covered a lot of important concepts in a considerable amount of depth, and
    this will give us a strong foundation for the remaining chapters.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是我们书中GenAI部分的介绍章节，但我们已经相当深入地覆盖了许多重要概念，这将为我们剩余的章节提供一个坚实的基础。
- en: With that in mind, let’s continue our journey into the world of GenAI.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，让我们继续我们的GenAI之旅。
