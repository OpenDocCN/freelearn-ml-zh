- en: Pre-Model Workflow and Pre-Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型前工作流与预处理
- en: 'In this chapter we will see the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到以下内容：
- en: Creating sample data for toy analysis
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为玩具分析创建样本数据
- en: Scaling data to the standard normal distribution
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据缩放到标准正态分布
- en: Creating binary features through thresholding
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过阈值创建二元特征
- en: Working with categorical variables
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理分类变量
- en: Imputing missing values through various strategies
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过各种策略填补缺失值
- en: A linear model in the presence of outliers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在离群值时的线性模型
- en: Putting it all together with pipelines
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用管道将所有内容整合起来
- en: Using Gaussian processes for regression
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用高斯过程进行回归
- en: Using SGD for regression
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SGD 进行回归
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: What is data, and what are we doing with it?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是数据，我们对数据的处理目的是什么？
- en: A simple answer is that we attempt to place our data as points on paper, graph
    them, think, and look for simple explanations that approximate the data well.
    The simple geometric line of *F=ma* (force being proportional to acceleration)
    explained a lot of noisy data for hundreds of years. I tend to think of data science
    as data compression at times.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的答案是，我们试图将数据点放在纸上，将其绘制成图，思考并寻找能够很好地近似数据的简单解释。简单的几何线 *F=ma*（力与加速度成正比）解释了数百年的大量噪声数据。我有时倾向于将数据科学看作是数据压缩。
- en: Sometimes, when a machine is given only win-lose outcomes (of winning games
    of checkers, for example) and trained, I think of artificial intelligence. It
    is never taught explicit directions on how to play to win in such a case.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，当机器只接受输赢结果（例如玩跳棋的游戏结果）并进行训练时，我认为这就是人工智能。在这种情况下，它从未被明确教导如何玩游戏以获得胜利。
- en: 'This chapter deals with the pre-processing of data in scikit-learn. Some questions
    you can ask about your dataset are as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了在 scikit-learn 中的数据预处理。你可以向数据集提问的问题如下：
- en: Are there missing values in your dataset?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集是否存在缺失值？
- en: Are there outliers (points far away from the others) in your set?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集中是否存在离群值（远离其他点的值）？
- en: What are the variables in the data like? Are they continuous quantities or categories?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中的变量是什么类型的？它们是连续变量还是分类变量？
- en: What do the continuous variable distributions look like? Can any of the variables
    in your dataset be described by normal distributions (bell-shaped curves)?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续变量的分布是什么样的？数据集中的某些变量是否可以用正态分布（钟形曲线）来描述？
- en: Can any continuous variables be turned into categorical variables for simplicity?
    (This tends to be true if the distribution takes on very few particular values
    and not a continuous-like range of values.)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否可以将任何连续变量转化为分类变量以简化处理？（如果数据的分布只有少数几个特定值，而不是类似连续范围的值，这种情况通常成立。）
- en: What are the units of the variables involved? Will you mix the variables somehow
    in the machine learning algorithm you chose to use?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 涉及的变量单位是什么？你是否会在选择的机器学习算法中混合这些变量？
- en: These questions can have simple or complex answers. Thankfully, you ask them
    many times, even on the same dataset, and after these recipes you will have some
    practice at crafting answers to pre-processing machine learning questions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题可能有简单或复杂的答案。幸运的是，你会多次提问，甚至在同一个数据集上，也会不断练习如何回答机器学习中的预处理问题。
- en: 'Additionally, we will see pipelines: a great organizational tool to make sure
    we perform the same operations on both the training and testing sets without errors
    and with relatively little work. We will also see regression examples: **stochastic
    gradient descent** (**SGD**) and Gaussian processes.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将了解管道：一种很好的组织工具，确保我们在训练集和测试集上执行相同的操作，避免出错并且工作量相对较少。我们还将看到回归示例：**随机梯度下降**（**SGD**）和高斯过程。
- en: Creating sample data for toy analysis
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为玩具分析创建样本数据
- en: If possible, use some of your own data for this book, but in the event you cannot,
    we'll learn how we can use scikit-learn to create toy data. scikit-learn's pseudo,
    theoretically constructed data is very interesting in its own right.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能，使用你自己的一些数据来学习本书中的内容。如果你无法这样做，我们将学习如何使用 scikit-learn 创建玩具数据。scikit-learn
    的伪造、理论构建的数据本身非常有趣。
- en: Getting ready
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Very similar to getting built-in datasets, fetching new datasets, and creating
    sample datasets, the functions that are used follow the naming convention `make_*`.
    Just to be clear, this data is purely artificial:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与获取内置数据集、获取新数据集和创建样本数据集类似，所使用的函数遵循 `make_*` 命名约定。为了明确，这些数据完全是人工合成的：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To save typing, import the `datasets` module as `d`, and `numpy` as `np`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了省略输入，导入`datasets`模块为`d`，`numpy`为`np`：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How to do it...
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: This section will walk you through the creation of several datasets. In addition
    to the sample datasets, these will be used throughout the book to create data
    with the necessary characteristics for the algorithms on display.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将带你逐步创建几个数据集。除了示例数据集外，这些数据集将贯穿整本书，用于创建具有算法所需特征的数据。
- en: Creating a regression dataset
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建回归数据集
- en: 'First, the stalwart—regression:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先是可靠的——回归：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: By default, this will generate a tuple with a 100 x 100 matrix—100 samples by
    100 features. However, by default, only 10 features are responsible for the target
    data generation. The second member of the tuple is the target variable. It is
    also possible to get more involved in generating data for regression.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，这将生成一个包含100 x 100矩阵的元组——100个样本和100个特征。然而，默认情况下，只有10个特征负责目标数据的生成。元组的第二个成员是目标变量。实际上，也可以更深入地参与回归数据的生成。
- en: 'For example, to generate a 1,000 x 10 matrix with five features responsible
    for the target creation, an underlying bias factor of 1.0, and 2 targets, the
    following command will be run:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，为了生成一个1000 x 10的矩阵，其中五个特征负责目标的创建，偏置因子为1.0，并且有两个目标，可以运行以下命令：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Creating an unbalanced classification dataset
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个不平衡的分类数据集
- en: Classification datasets are also very simple to create. It's simple to create
    a base classification set, but the basic case is rarely experienced in practice—most
    users don't convert, most transactions aren't fraudulent, and so on.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 分类数据集也非常容易创建。创建一个基本的分类数据集很简单，但基本情况在实践中很少见——大多数用户不会转换，大多数交易不是欺诈性的，等等。
- en: 'Therefore, it''s useful to explore classification on unbalanced datasets:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，探索不平衡数据集上的分类是非常有用的：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Creating a dataset for clustering
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建聚类数据集
- en: Clusters will also be covered. There are actually several functions to create
    datasets that can be modeled by different cluster algorithms.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类也会涉及到。实际上，有多个函数可以创建适用于不同聚类算法的数据集。
- en: 'For example, blobs are very easy to create and can be modeled by k-means:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，blobs非常容易创建，并且可以通过k-means来建模：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will look like the following:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将看起来像这样：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/0bf9b285-7654-42fc-a2cb-4f14e9d40e44.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0bf9b285-7654-42fc-a2cb-4f14e9d40e44.png)'
- en: How it works...
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Let's walk you through how scikit-learn produces the regression dataset by taking
    a look at the source code (with some modifications for clarity). Any undefined
    variables are assumed to have the default value of `make_regression`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看源代码（做了一些修改以便清晰）来逐步了解scikit-learn如何生成回归数据集。任何未定义的变量假定其默认值为`make_regression`。
- en: 'It''s actually surprisingly simple to follow. First, a random array is generated
    with the size specified when the function is called:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，跟着做是非常简单的。首先，生成一个随机数组，大小由调用函数时指定：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Given the basic dataset, the target dataset is then generated:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 给定基本数据集后，接着生成目标数据集：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The dot product of `X` and `ground_truth` are taken to get the final target
    values. Bias, if any, is added at this time:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 计算`X`和`ground_truth`的点积来得到最终的目标值。此时，如果有偏置，也会被加上：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The dot product is simply a matrix multiplication. So, our final dataset will
    have `n_samples`, which is the number of rows from the dataset, and `n_target,`
    which is the number of target variables.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 点积其实就是矩阵乘法。因此，我们的最终数据集将包含`n_samples`，即数据集的行数，和`n_target`，即目标变量的数量。
- en: Due to NumPy's broadcasting, bias can be a scalar value, and this value will
    be added to every sample. Finally, it's a simple matter of adding any noise and
    shuffling the dataset. Voila, we have a dataset that's perfect for testing regression.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于NumPy的广播机制，偏置可以是一个标量值，并且这个值将添加到每个样本中。最后，只需简单地加入噪声并打乱数据集。瞧，我们得到了一个非常适合回归测试的数据集。
- en: Scaling data to the standard normal distribution
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据缩放至标准正态分布
- en: A pre-processing step that is recommended is to scale columns to the standard
    normal. The standard normal is probably the most important distribution in statistics.
    If you've ever been introduced to statistics, you must have almost certainly seen
    z-scores. In truth, that's all this recipe is about—transforming our features
    from their endowed distribution into z-scores.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐的预处理步骤是将列缩放至标准正态分布。标准正态分布可能是统计学中最重要的分布。如果你曾接触过统计学，你几乎肯定见过z分数。事实上，这就是这个方法的核心——将特征从其原始分布转换为z分数。
- en: Getting ready
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: The act of scaling data is extremely useful. There are a lot of machine learning
    algorithms, which perform differently (and incorrectly) in the event the features
    exist at different scales. For example, SVMs perform poorly if the data isn't
    scaled because they use a distance function in their optimization, which is biased
    if one feature varies from 0 to 10,000 and the other varies from 0 to 1.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放数据的操作非常有用。许多机器学习算法在特征存在不同尺度时表现不同（甚至可能出错）。例如，如果数据没有进行缩放，支持向量机（SVM）的表现会很差，因为它们在优化中使用距离函数，而如果一个特征的范围是0到10,000，另一个特征的范围是0到1，距离函数会出现偏差。
- en: 'The `preprocessing` module contains several useful functions for scaling features:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`preprocessing`模块包含了几个有用的特征缩放函数：'
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Load the Boston dataset:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 加载波士顿数据集：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How to do it...
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Continuing with the Boston dataset, run the following commands:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续使用波士顿数据集，运行以下命令：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'There''s actually a lot to learn from this initially. Firstly, the first feature
    has the smallest mean but varies even more than the third feature. The second
    feature has the largest mean and standard deviation—it takes the widest spread
    of values:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一开始就可以学到很多东西。首先，第一个特征的均值最小，但变化范围比第三个特征更大。第二个特征的均值和标准差最大——它需要分布最广的数值：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: How it works...
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The centering and scaling function is extremely simple. It merely subtracts
    the mean and divides by the standard deviation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 中心化和缩放函数非常简单。它只是将均值相减并除以标准差。
- en: 'Pictorially and with pandas, the third feature looks as follows before the
    transformation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通过图示和pandas，第三个特征在变换之前如下所示：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/0d0ec1e7-2c6e-447a-a34c-c6a548aab0db.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d0ec1e7-2c6e-447a-a34c-c6a548aab0db.png)'
- en: 'This is what it looks like afterward:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 变换后的样子如下：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/1a8a94aa-e223-4952-94f9-2ab4258b4705.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a8a94aa-e223-4952-94f9-2ab4258b4705.png)'
- en: The *x* axis label has changed.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*轴标签已更改。'
- en: 'In addition to a function, there is also a centering and scaling class that
    is easy to invoke, and this is particularly useful when used in conjunction with
    pipelines, which are mentioned later. It''s also useful for the centering and
    scaling class to persist across individual scaling:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 除了函数，还有一个易于调用的中心化和缩放类，特别适用于与管道一起使用，管道在后面会提到。这个类在跨个别缩放时也特别有用：
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Scaling features to a mean of zero and a standard deviation of one isn't the
    only useful type of scaling.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 将特征缩放到均值为零、标准差为一并不是唯一有用的缩放类型。
- en: 'Pre-processing also contains a `MinMaxScaler` class, which will scale the data
    within a certain range:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理还包含一个`MinMaxScaler`类，它可以将数据缩放到某个特定范围内：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It''s very simple to change the minimum and maximum values of the `MinMaxScaler`
    class from its defaults of `0` and `1`, respectively:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易将`MinMaxScaler`类的最小值和最大值从默认的`0`和`1`更改为其他值：
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Furthermore, another option is normalization. This will scale each sample to
    have a length of one. This is different from the other types of scaling done previously,
    where the features were scaled. Normalization is illustrated in the following
    command:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，另一种选择是归一化。归一化会将每个样本缩放到长度为1。这与之前进行的缩放不同，之前是缩放特征。归一化的操作可以通过以下命令实现：
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If it's not apparent why this is useful, consider the Euclidean distance (a
    measure of similarity) between three of the samples, where one sample has the
    values *(1, 1, 0)*, another has *(3, 3, 0)*, and the final has *(1, -1, 0)*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不清楚为什么这样做有用，可以考虑三组样本之间的欧几里得距离（相似度度量），其中一组样本的值为*(1, 1, 0)*，另一组的值为*(3, 3, 0)*，最后一组的值为*(1,
    -1, 0)*。
- en: The distance between the first and third vector is less than the distance between
    the first and second although the first and third are orthogonal, whereas the
    first and second only differ by a scalar factor of three. Since distances are
    often used as measures of similarity, not normalizing the data first can be misleading.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个和第三个向量之间的距离小于第一个和第二个向量之间的距离，尽管第一和第三是正交的，而第一和第二仅通过一个标量因子差异为三。由于距离通常用作相似度的度量，不对数据进行归一化可能会导致误导。
- en: 'From an alternative perspective, try the following syntax:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从另一个角度来看，尝试以下语法：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: All the rows are normalized and consist of vectors of length one. In three dimensions,
    all normalized vectors lie on the surface of a sphere centered at the origin.
    The information left is the direction of the vectors because, by definition, by
    normalizing you are dividing the vector by its length. Do always remember, though,
    that when performing this operation you have set an origin at *(0, 0, 0)* and
    you have turned any row of data in the array into a vector relative to this origin.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 所有行都被归一化，并且由长度为1的向量组成。在三维空间中，所有归一化的向量都位于以原点为中心的球面上。剩下的信息是向量的方向，因为按定义，归一化是通过将向量除以其长度来完成的。然而，请始终记住，在执行此操作时，你已将原点设置为*(0,
    0, 0)*，并且你已将数组中的任何数据行转换为相对于此原点的向量。
- en: Creating binary features through thresholding
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过阈值化创建二元特征
- en: In the last recipe, we looked at transforming our data into the standard normal
    distribution. Now, we'll talk about another transformation, one that is quite
    different. Instead of working with the distribution to standardize it, we'll purposely
    throw away data; if we have good reason, this can be a very smart move. Often,
    in what is ostensibly continuous data, there are discontinuities that can be determined
    via binary features.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了如何将数据转换为标准正态分布。现在，我们将讨论另一种完全不同的转换方法。我们不再通过处理分布来标准化它，而是故意丢弃数据；如果有充分的理由，这可能是一个非常聪明的举动。通常，在看似连续的数据中，会存在一些可以通过二元特征确定的间断点。
- en: Additionally, note that in the previous chapter, we turned a classification
    problem into a regression problem. With thresholding, we can turn a regression
    problem into a classification problem. This happens in some data science contexts.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，在上一章中，我们将分类问题转化为回归问题。通过阈值化，我们可以将回归问题转化为分类问题。在一些数据科学的场景中，这种情况是存在的。
- en: Getting ready
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Creating binary features and outcomes is a very useful method, but it should
    be used with caution. Let''s use the Boston dataset to learn how to turn values
    into binary outcomes. First, load the Boston dataset:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 创建二元特征和结果是一种非常有用的方法，但应谨慎使用。让我们使用波士顿数据集来学习如何将值转换为二元结果。首先，加载波士顿数据集：
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: How to do it...
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Similar to scaling, there are two ways to binarize features in scikit-learn:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 与缩放类似，scikit-learn中有两种方式可以将特征二值化：
- en: '`preprocessing.binarize`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preprocessing.binarize`'
- en: '`preprocessing.Binarizer`'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preprocessing.Binarizer`'
- en: The Boston dataset's `target` variable is the median value of houses in thousands.
    This dataset is good for testing regression and other continuous predictors, but
    consider a situation where we want to simply predict whether a house's value is
    more than the overall mean.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 波士顿数据集的`target`变量是以千为单位的房屋中位数值。这个数据集适合用来测试回归和其他连续预测模型，但可以考虑一种情况，我们只需要预测房屋的价值是否超过整体均值。
- en: 'To do this, we will want to create a threshold value of the mean. If the value
    is greater than the mean, produce a `1`; if it is less, produce a `0`:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为此，我们需要创建一个均值的阈值。如果值大于均值，返回`1`；如果小于均值，返回`0`：
- en: '[PRE22]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This was easy, but let''s check to make sure it worked correctly:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这很简单，但让我们检查一下以确保它正常工作：
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Given the simplicity of the operation in NumPy, it''s a fair question to ask
    why you would want to use the built-in functionality of scikit-learn. Pipelines,
    covered in the *Putting it all together with pipelines* recipe, will help to explain
    this; in anticipation of this, let''s use the `Binarizer` class:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鉴于NumPy中操作的简单性，提出为什么要使用scikit-learn的内建功能是一个合理的问题。在*将一切通过管道组合起来*这一节中介绍的管道将有助于解释这一点；为了预见到这一点，让我们使用`Binarizer`类：
- en: '[PRE24]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: There's more...
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多……
- en: Let's also learn about sparse matrices and the `fit` method.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也来了解稀疏矩阵和`fit`方法。
- en: Sparse matrices
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏矩阵
- en: 'Sparse matrices are special in that zeros aren''t stored; this is done in an
    effort to save space in memory. This creates an issue for the binarizer, so to
    combat it, a special condition for the binarizer for sparse matrices is that the
    threshold cannot be less than zero:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏矩阵的特殊之处在于零值并不被存储；这是为了节省内存空间。这样会为二值化器带来问题，因此，为了应对这一问题，针对稀疏矩阵，二值化器的特殊条件是阈值不能小于零：
- en: '[PRE25]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The fit method
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`fit`方法'
- en: The `fit` method exists for the binarizer transformation, but it will not fit
    anything; it will simply return the object. The object, however, will store the
    threshold and be ready for the `transform` method.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit`方法是针对二值化转换存在的，但它不会对任何东西进行拟合；它只会返回该对象。该对象会存储阈值，并准备好进行`transform`方法。'
- en: Working with categorical variables
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理分类变量
- en: Categorical variables are a problem. On one hand they provide valuable information;
    on the other hand, it's probably text—either the actual text or integers corresponding
    to the text—such as an index in a lookup table.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 类别变量是一个问题。一方面，它们提供了有价值的信息；另一方面，它们很可能是文本——无论是实际的文本还是与文本对应的整数——例如查找表中的索引。
- en: So, we clearly need to represent our text as integers for the model's sake,
    but we can't just use the id field or naively represent them. This is because
    we need to avoid a similar problem to the *Creating binary features through thresholding*
    recipe. If we treat data that is continuous, it must be interpreted as continuous.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，显然我们需要将文本表示为整数，以便于模型处理，但不能仅仅使用ID字段或天真地表示它们。这是因为我们需要避免类似于*通过阈值创建二进制特征*食谱中出现的问题。如果我们处理的是连续数据，它必须被解释为连续数据。
- en: Getting ready
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: The Boston dataset won't be useful for this section. While it's useful for feature
    binarization, it won't suffice for creating features from categorical variables.
    For this, the iris dataset will suffice.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 波士顿数据集对于本节不适用。虽然它对于特征二值化很有用，但不足以从类别变量中创建特征。为此，鸢尾花数据集就足够了。
- en: For this to work, the problem needs to be turned on its head. Imagine a problem
    where the goal is to predict the sepal width; in this case, the species of the
    flower will probably be useful as a feature.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其生效，问题需要彻底转变。设想一个问题，目标是预测花萼宽度；在这种情况下，花卉的物种可能作为一个特征是有用的。
- en: How to do it...
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'Let''s get the data sorted first:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先整理数据：
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Place `X` and `y`, all of the numerical data, side-by-side. Create an encoder
    with scikit-learn to handle the category of the `y` column:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`X`和`y`，所有数值数据，放在一起。使用scikit-learn创建一个编码器来处理`y`列的类别：
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: How it works...
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'The encoder creates additional features for each categorical variable, and
    the value returned is a sparse matrix. The result is a sparse matrix by definition;
    each row of the new features has `0` everywhere, except for the column whose value
    is associated with the feature''s category. Therefore, it makes sense to store
    this data in a sparse matrix. The `cat_encoder` is now a standard scikit-learn
    model, which means that it can be used again:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器为每个类别变量创建附加特征，返回的值是一个稀疏矩阵。根据定义，结果是一个稀疏矩阵；新特征的每一行除了与特征类别关联的列外，其他位置都是`0`。因此，将这些数据存储为稀疏矩阵是合理的。现在，`cat_encoder`是一个标准的scikit-learn模型，这意味着它可以再次使用：
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In the previous chapter, we turned a classification problem into a regression
    problem. Here, there are three columns:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们将一个分类问题转化为回归问题。在这里，有三列数据：
- en: The first column is `1` if the flower is a Setosa and `0` otherwise
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一列是`1`，如果花是Setosa，则为`1`，否则为`0`。
- en: The second column is `1` if the flower is a Versicolor and `0` otherwise
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二列是`1`，如果花是Versicolor，则为`1`，否则为`0`。
- en: The third column is `1` if the flower is a Virginica and `0` otherwise
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三列是`1`，如果花是Virginica，则为`1`，否则为`0`。
- en: Thus, we could use any of these three columns to create a regression similar
    to the one in the previous chapter; we will perform a regression to determine
    the degree of setosaness of a flower as a real number. The matching statement
    in classification is whether a flower is a Setosa one or not. This is the problem
    statement if we perform binary classification of the first column.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以使用这三列中的任何一列来创建与上一章类似的回归；我们将执行回归以确定花卉的Setosa程度作为一个实数。如果我们对第一列进行二分类，这就是分类中的问题陈述，判断花卉是否是Setosa。
- en: scikit-learn has the capacity for this type of multi-output regression. Compare
    it with multiclass classification. Let's try a simple one.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn具有执行此类型的多输出回归的能力。与多类分类相比，让我们尝试一个简单的例子。
- en: 'Import the ridge regression regularized linear model. It tends to be very well
    behaved because it is regularized. Instantiate a ridge regressor class:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 导入岭回归正则化线性模型。由于它是正则化的，通常表现得非常稳定。实例化一个岭回归器类：
- en: '[PRE29]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now import a multi-output regressor that takes the ridge regressor instance
    as an argument:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在导入一个多输出回归器，将岭回归器实例作为参数：
- en: '[PRE30]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'From earlier in this recipe, transform the target variable `y` to a three-part
    target variable, `y_multi`, with `OneHotEncoder()`. If `X` and `y` were part of
    a pipeline, the pipeline would transform the training and testing sets separately,
    and this is preferable:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 从本食谱前面的部分，将目标变量`y`转换为三部分目标变量`y_multi`，并使用`OneHotEncoder()`。如果`X`和`y`是管道的一部分，管道将分别转换训练集和测试集，这是更可取的做法：
- en: '[PRE31]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Create training and testing sets:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 创建训练集和测试集：
- en: '[PRE32]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Fit the multi-output estimator:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合多输出估计器：
- en: '[PRE33]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Predict the multi-output target on the testing set:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集上预测多输出目标：
- en: '[PRE34]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Use the `binarize` function from the previous recipe to turn each real number
    into the integers `0` or `1`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面配方中的 `binarize` 函数将每个实数转换为整数 `0` 或 `1`：
- en: '[PRE35]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can measure the overall multi-output performance with the `roc_auc_score`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `roc_auc_score` 来衡量整体的多输出性能：
- en: '[PRE36]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Or, we can do it flower type by flower type, column by column:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以逐种花朵类型、逐列进行：
- en: '[PRE37]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: There's more...
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'In the preceding multi-output regression, you could be concerned with the dummy
    variable trap: the collinearity of the outputs. Without dropping any output columns,
    you assume that there is a fourth option: that a flower can be of none of the
    three types. To prevent the trap, drop the last column and assume that the flower
    has to be of one of the three types as we do not have any training examples where
    it is not one of the three flower types.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的多输出回归中，你可能会担心虚拟变量陷阱：输出之间的共线性。在不删除任何输出列的情况下，你假设存在第四种选择：即花朵可以不是三种类型中的任何一种。为了避免陷阱，删除最后一列，并假设花朵必须是三种类型之一，因为我们没有任何训练样本显示花朵不是三种类型中的一种。
- en: There are other ways to create categorical variables in scikit-learn and Python.
    The `DictVectorizer` class is a good option if you like to limit the dependencies
    of your projects to only scikit-learn and you have a fairly simple encoding scheme.
    However, if you require more sophisticated categorical encoding, patsy is a very
    good option.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 和 Python 中还有其他方法可以创建分类变量。如果你希望将项目的依赖项仅限于 scikit-learn，并且有一个相对简单的编码方案，`DictVectorizer`
    类是一个不错的选择。然而，如果你需要更复杂的分类编码，patsy 是一个非常好的选择。
- en: DictVectorizer class
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DictVectorizer 类
- en: 'Another option is to use `DictVectorizer` class. This can be used to directly
    convert strings to features:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是使用 `DictVectorizer` 类。这可以直接将字符串转换为特征：
- en: '[PRE38]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Imputing missing values through various strategies
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过各种策略填充缺失值
- en: Data imputation is critical in practice, and thankfully there are many ways
    to deal with it. In this recipe, we'll look at a few of the strategies. However,
    be aware that there might be other approaches that fit your situation better.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 数据填充在实践中至关重要，幸运的是有很多方法可以处理它。在本配方中，我们将查看几种策略。然而，请注意，可能还有其他方法更适合你的情况。
- en: This means scikit-learn comes with the ability to perform fairly common imputations;
    it will simply apply some transformations to the existing data and fill the NAs.
    However, if the dataset is missing data, and there's a known reason for this missing
    data—for example, response times for a server that times out after 100 ms—it might
    be better to take a statistical approach through other packages, such as the Bayesian
    treatment via PyMC, hazards models via Lifelines, or something home-grown.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 scikit-learn 具备执行常见填充操作的能力；它会简单地对现有数据应用一些变换并填充缺失值。然而，如果数据集缺失数据，并且我们知道这种缺失数据的原因——例如服务器响应时间在
    100 毫秒后超时——那么通过其他包采用统计方法可能会更好，比如通过 PyMC 进行的贝叶斯处理，或通过 Lifelines 进行的危险模型，或是自定义的处理方法。
- en: Getting ready
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备好
- en: 'The first thing to do when learning how to input missing values is to create
    missing values. NumPy''s masking will make this extremely simple:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 学习如何输入缺失值时，首先要做的就是创建缺失值。NumPy 的掩码处理将使这变得极其简单：
- en: '[PRE39]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: To unravel this a bit, in case NumPy isn't too familiar, it's possible to index
    arrays with other arrays in NumPy. So, to create the random missing data, a random
    Boolean array is created, which is of the same shape as the iris dataset. Then,
    it's possible to make an assignment via the masked array. It's important to note
    that because a random array is used, it is likely that your `masking_array` will
    be different from what's used here.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了稍微澄清一下，如果你对 NumPy 不太熟悉，在 NumPy 中可以使用其他数组来索引数组。所以，为了创建随机缺失数据，创建了一个与鸢尾花数据集形状相同的随机布尔数组。然后，可以通过掩码数组进行赋值。需要注意的是，由于使用了随机数组，因此你的
    `masking_array` 可能与此处使用的不同。
- en: 'To make sure this works, use the following command (since we''re using a random
    mask, it might not match directly):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保这有效，请使用以下命令（由于我们使用了随机掩码，它可能不会直接匹配）：
- en: '[PRE40]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: How to do it...
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'A theme prevalent throughout this book (due to the theme throughout scikit-learn)
    is reusable classes that fit and transform datasets that can subsequently be used
    to transform unseen datasets. This is illustrated as follows:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本书中的一个常见主题（由于 scikit-learn 中的主题）是可重用的类，这些类能够拟合和转换数据集，随后可以用来转换未见过的数据集。如下所示：
- en: '[PRE41]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Notice the difference in the position `[0, 0]`:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意 `[0, 0]` 位置的差异：
- en: '[PRE42]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: How it works...
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The imputation works by employing different strategies. The default is mean,
    but in total there are the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 填充操作通过采用不同的策略进行。默认值是均值，但总共有以下几种策略：
- en: '`mean` (default)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean`（默认值）'
- en: '`median`'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`median`（中位数）'
- en: '`most_frequent` (mode)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`most_frequent`（众数）'
- en: 'scikit-learn will use the selected strategy to calculate the value for each
    non-missing value in the dataset. It will then simply fill the missing values.
    For example, to redo the iris example with the median strategy, simply reinitialize
    impute with the new strategy:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 将使用所选策略计算数据集中每个非缺失值的值，然后简单地填充缺失值。例如，要使用中位数策略重新执行鸢尾花示例，只需用新策略重新初始化填充器：
- en: '[PRE43]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: If the data is missing values, it might be inherently dirty in other places.
    For instance, in the example in the preceding, *How to do it...* section, `np.nan`
    (the default missing value) was used as the missing value, but missing values
    can be represented in many ways. Consider a situation where missing values are
    `-1`. In addition to the strategy to compute the missing value, it's also possible
    to specify the missing value for the imputer. The default is `nan`, which will
    handle `np.nan` values.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据中缺少值，那么其他地方可能也存在数据质量问题。例如，在上面提到的 *如何操作...* 部分中，`np.nan`（默认的缺失值）被用作缺失值，但缺失值可以用多种方式表示。考虑一种情况，缺失值是
    `-1`。除了计算缺失值的策略外，还可以为填充器指定缺失值。默认值是 `nan`，它会处理 `np.nan` 值。
- en: 'To see an example of this, modify `iris_X` to have `-1` as the missing value.
    It sounds crazy, but since the iris dataset contains measurements that are always
    possible, many people will fill the missing values with `-1` to signify they''re
    not there:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看此示例，请将 `iris_X` 修改为使用 `-1` 作为缺失值。这听起来很疯狂，但由于鸢尾花数据集包含的是永远可能测量的数据，许多人会用 `-1`
    来填充缺失值，以表示这些数据缺失：
- en: '[PRE44]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Filling these in is as simple as the following:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 填充这些缺失值的方法非常简单，如下所示：
- en: '[PRE45]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: There's more...
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Pandas also provides a functionality to fill in missing data. It actually might
    be a bit more flexible, but it is less reusable:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 也提供了一种填充缺失数据的功能。它可能更加灵活，但也较少可重用：
- en: '[PRE46]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'To mention its flexibility, `fillna` can be passed any sort of statistic, that
    is, the strategy is more arbitrarily defined:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明其灵活性，`fillna` 可以传入任何类型的统计量，也就是说，策略可以更加随意地定义：
- en: '[PRE47]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: A linear model in the presence of outliers
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存在离群点的线性模型
- en: In this recipe, instead of traditional linear regression we will try using the
    Theil-Sen estimator to deal with some outliers.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将尝试使用 Theil-Sen 估计器来处理一些离群点，而不是传统的线性回归。
- en: Getting ready
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'First, create the data corresponding to a line with a slope of `2`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一条斜率为 `2` 的数据线：
- en: '[PRE48]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](img/d1ebad85-c139-418a-94ec-06c1d3ce5791.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1ebad85-c139-418a-94ec-06c1d3ce5791.png)'
- en: 'Add noise to that data and label it as `y_noisy`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 给数据添加噪声，并将其标记为 `y_noisy`：
- en: '[PRE49]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![](img/f9790eae-a74e-4d0f-93b0-45057e7afff8.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9790eae-a74e-4d0f-93b0-45057e7afff8.png)'
- en: How to do it...
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Import both `LinearRegression` and `TheilSenRegressor`. Score the estimators
    using the original line as the testing set, `y_truth`:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `LinearRegression` 和 `TheilSenRegressor`。使用原始线作为测试集 `y_truth` 对估计器进行评分：
- en: '[PRE50]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Plot the lines. Note that **o****rdinary least squares** (**OLS**) is way off
    the true line, `y_truth`. Theil-Sen overlaps the real line:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制这些线条。请注意，**普通最小二乘法**（**OLS**）与真实线 `y_truth` 相差甚远，而 Theil-Sen 则与真实线重叠：
- en: '[PRE51]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '![](img/87bf00b0-a0bf-4e0e-8e82-8fcdd3a54a7d.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87bf00b0-a0bf-4e0e-8e82-8fcdd3a54a7d.png)'
- en: 'Plot the dataset and the estimated lines:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制数据集和估计的线条：
- en: '[PRE52]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![](img/24418ea6-bb8c-4d1d-90bf-c357f9d6bd18.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24418ea6-bb8c-4d1d-90bf-c357f9d6bd18.png)'
- en: How it works...
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `TheilSenRegressor` is a robust estimator that performs well in the presence
    of outliers. It uses the measurement of medians, which is more robust to outliers.
    In OLS regression, errors are squared, and thus a squared error can decrease good
    results.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`TheilSenRegressor` 是一种鲁棒估计器，在存在离群点的情况下表现良好。它使用中位数的测量，更加稳健于离群点。在 OLS 回归中，误差会被平方，因此平方误差可能会导致好的结果变差。'
- en: 'You can try several robust estimators in scikit-learn Version 0.19.0:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 scikit-learn 版本 0.19.0 中尝试几种鲁棒估计器：
- en: '[PRE53]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: As you can see, the robust linear estimators Theil-Sen, **random sample consensus**
    (**RANSAC**), and the Huber regressor out-perform the other linear regressors
    in the presence of outliers.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在存在异常值的情况下，稳健的线性估计器 Theil-Sen、**随机样本一致性**（**RANSAC**）和 Huber 回归器的表现优于其他线性回归器。
- en: Putting it all together with pipelines
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将一切整合到管道中
- en: Now that we've used pipelines and data transformation techniques, we'll walk
    through a more complicated example that combines several of the previous recipes
    into a pipeline.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经使用了管道和数据转换技术，我们将通过一个更复杂的例子，结合之前的几个实例，演示如何将它们组合成一个管道。
- en: Getting ready
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this section, we'll show off some more of pipeline's power. When we used
    it earlier to impute missing values, it was only a quick taste; here, we'll chain
    together multiple pre-processing steps to show how pipelines can remove extra
    work.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示管道的更多强大功能。当我们之前用它来填补缺失值时，只是简单体验了一下；这里，我们将把多个预处理步骤链起来，展示管道如何去除额外的工作。
- en: 'Let''s briefly load the iris dataset and seed it with some missing values:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简单加载鸢尾花数据集，并给它添加一些缺失值：
- en: '[PRE54]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: How to do it...
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现…
- en: The goal of this chapter is to first impute the missing values of `iris_data`,
    and then perform PCA on the corrected dataset. You can imagine (and we'll do it
    later) that this workflow might need to be split between a training dataset and
    a holdout set; pipelines will make this easier, but first we need to take a baby
    step.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是首先填补`iris_data`的缺失值，然后对修正后的数据集执行PCA。你可以想象（我们稍后会做）这个工作流程可能需要在训练数据集和保留集之间拆分；管道将使这更容易，但首先我们需要迈出小小的一步。
- en: 'Let''s load the required libraries:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载所需的库：
- en: '[PRE55]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Next, create the `imputer` and `pca` classes:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建`imputer`和`pca`类：
- en: '[PRE56]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now that we have the classes we need, we can load them into `Pipeline`:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经有了需要的类，我们可以将它们加载到`Pipeline`中：
- en: '[PRE57]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: This takes a lot more management if we use separate steps. Instead of each step
    requiring a fit transform, this step is performed only once, not to mention that
    we only have to keep track of one object!
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用单独的步骤，这需要更多的管理。与每个步骤都需要进行拟合转换不同，这个步骤只需执行一次，更不用说我们只需要跟踪一个对象！
- en: How it works...
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Hopefully it was obvious, but each step in a pipeline is passed to a pipeline
    object via a list of tuples, with the first element getting the name and the second
    getting the actual object. Under the hood, these steps are looped through when
    a method such as `fit_transform` is called on the pipeline object.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 希望大家已经明白，每个管道中的步骤都是通过元组列表传递给管道对象的，第一个元素是名称，第二个元素是实际的对象。在幕后，当调用像`fit_transform`这样的函数时，这些步骤会在管道对象上循环执行。
- en: 'This said, there are quick and dirty ways to create a pipeline, much in the
    same way there was a quick way to perform scaling, though we can use `StandardScaler`
    if we want more power. The `pipeline` function will automatically create the names
    for the pipeline objects:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，确实有一些快速且简便的方式来创建管道，就像我们之前有一种快速的方式来执行缩放操作一样，尽管我们可以使用`StandardScaler`来获得更强大的功能。`pipeline`函数将自动为管道对象创建名称：
- en: '[PRE58]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This is the same object that was created in the more verbose method:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在更详细的方法中创建的相同对象：
- en: '[PRE59]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: There's more...
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'We just walked through pipelines at a very high level, but it''s unlikely that
    we will want to apply the base transformation. Therefore, the attributes of each
    object in a pipeline can be accessed using a `set_params` method, where the parameter
    follows the `<step_name>__<step_parameter>` convention. For example, let''s change
    the `pca` object to use two components:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚以很高的层次走过了管道，但不太可能希望直接应用基本的转换。因此，可以通过`set_params`方法访问管道中每个对象的属性，其中参数遵循`<step_name>__<step_parameter>`的约定。例如，假设我们想把`pca`对象改为使用两个主成分：
- en: '[PRE60]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Notice, `n_components=2` in the preceding output. Just as a test, we can output
    the same transformation we have already done twice, and the output will be an
    N x 2 matrix:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的输出中有`n_components=2`。作为测试，我们可以输出之前已经做过两次的相同变换，输出将是一个 N x 2 的矩阵：
- en: '[PRE61]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Using Gaussian processes for regression
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用高斯过程进行回归
- en: In this recipe, we'll use a Gaussian process for regression. In the linear models
    section, we will see how representing prior information on the coefficients was
    possible using Bayesian ridge regression.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用高斯过程进行回归。在线性模型部分，我们将看到如何通过贝叶斯岭回归表示系数的先验信息。
- en: With a Gaussian process, it's about the variance and not the mean. However,
    with a Gaussian process, we assume the mean is 0, so it's the covariance function
    we'll need to specify.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在高斯过程中，关注的是方差而非均值。然而，我们假设均值为0，所以我们需要指定的是协方差函数。
- en: The basic setup is similar to how a prior can be put on the coefficients in
    a typical regression problem. With a Gaussian process, a prior can be put on the
    functional form of the data, and it's the covariance between the data points that
    is used to model the data, and therefore, must fit the data.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 基本设置类似于在典型回归问题中如何对系数设置先验。在高斯过程（Gaussian Process）中，可以对数据的函数形式设置先验，数据点之间的协方差用于建模数据，因此必须与数据相匹配。
- en: 'A big advantage of Gaussian processes is that they can predict probabilistically:
    you can obtain confidence bounds on your predictions. Additionally, the prediction
    can interpolate the observations for the available kernels: predictions from regression
    are smooth and thus a prediction between two points you know about is between
    those two points.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯过程的一个大优点是它们可以进行概率预测：你可以获得预测的置信区间。此外，预测可以插值可用内核的观测值：回归的预测是平滑的，因此两个已知点之间的预测位于这两个点之间。
- en: A disadvantage of Gaussian processes is lack of efficiency in high-dimensional
    spaces.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯过程的一个缺点是在高维空间中的效率较低。
- en: Getting ready
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'So, let''s use some regression data and walk through how Gaussian processes
    work in scikit-learn:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们使用一些回归数据，逐步了解高斯过程如何在scikit-learn中工作：
- en: '[PRE62]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: How to do it…
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到……
- en: 'We have the data, we''ll create a scikit-learn `GaussianProcessRegressor` object.
    Let''s look at the `gpr` object:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有数据，将创建一个scikit-learn的`GaussianProcessRegressor`对象。让我们看看`gpr`对象：
- en: '[PRE63]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'There are a few parameters that are important and must be set:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个重要的参数必须设置：
- en: '`alpha`: This is a noise parameter. You can assign a noise value for all observations
    or assign `n` values in the form of a NumPy array where `n` is the length of the
    target observations in the training set you pass to `gpr` for training.'
  id: totrans-257
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`：这是一个噪声参数。你可以为所有观测值指定一个噪声值，或者以NumPy数组的形式分配`n`个值，其中`n`是传递给`gpr`进行训练的训练集目标观测值的长度。'
- en: '`kernel`: This is a kernel that approximates a function. The default in a previous
    version of scikit-learn was **radial basis functions** (**RBF**), and we will
    construct a flexible kernel from constant kernels and RBF kernels.'
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel`：这是一个逼近函数的内核。在scikit-learn的早期版本中，默认的内核是**径向基函数**（**RBF**），我们将通过常量内核和RBF内核构建一个灵活的内核。'
- en: '`normalize_y`:  You can set it to true if the mean of the target set is not
    zero. If you leave it set to false, it still works fairly well.'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`normalize_y`：如果目标集的均值不为零，可以将其设置为True。如果设置为False，效果也相当不错。'
- en: '`n_restarts_optimizer`: Set this to 10-20 for practical use. This is the number
    of iterations to optimize the kernel.'
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_restarts_optimizer`：设置为10-20以供实际使用。该值表示优化内核时的迭代次数。'
- en: 'Import the required kernel functions and set a flexible kernel:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的内核函数并设置一个灵活的内核：
- en: '[PRE64]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Finally, instantiate and fit the algorithm. Note that `alpha` is set to `5`
    for all values. I came up with that number as being around one-fourth of the target
    values:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，实例化并拟合算法。请注意，`alpha`对所有值都设置为`5`。我之所以选择这个数字，是因为它大约是目标值的四分之一：
- en: '[PRE65]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Store the predictions on unseen data as `test_preds`:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将对未见数据的预测存储为`test_preds`：
- en: '[PRE66]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Plot the results:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制结果：
- en: '[PRE67]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '![](img/1a8840b6-2a13-4f42-9d39-ff0de8bb8cf4.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a8840b6-2a13-4f42-9d39-ff0de8bb8cf4.png)'
- en: Cross-validation with the noise parameter
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用噪声参数进行交叉验证
- en: You might wonder if that is the best noise parameter, `alpha=5`? To figure this
    out, try some cross-validation.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，这是否是最佳的噪声参数`alpha=5`？为了弄清楚这一点，尝试一些交叉验证。
- en: 'First, produce a cross-validation score with `alpha=5`. Note the scorer within
    the `cross_val_score` object is `neg_mean_absolute_error`, as the default R-squared
    score is hard to read for this dataset:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用`alpha=5`生成交叉验证得分。注意，`cross_val_score`对象中的评分器是`neg_mean_absolute_error`，因为该数据集的默认R方得分难以读取：
- en: '[PRE68]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Look at the scores in `scores_5`:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看`scores_5`中的得分：
- en: '[PRE69]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Observe that the scores in the last fold do not look the same as the other three.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，最后一次折叠中的得分与其他三次折叠不一样。
- en: 'Now produce a report with `alpha=7`:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在使用`alpha=7`生成报告：
- en: '[PRE70]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'This score looks a little better. Now, try `alpha=7` and `normalize_y` set
    to `True`:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个得分看起来更好一些。现在，尝试将`alpha=7`，并将`normalize_y`设置为`True`：
- en: '[PRE71]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'This looks even better, as the mean is higher and the standard deviation is
    lower. Let''s select the last model for final training:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这看起来更好，因为均值较高，标准差较低。让我们选择最后一个模型进行最终训练：
- en: '[PRE72]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Predict it:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行预测：
- en: '[PRE73]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Visualize the results:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化结果：
- en: '![](img/a5930ca5-55ac-412c-8b48-5282cc355af9.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5930ca5-55ac-412c-8b48-5282cc355af9.png)'
- en: 'The residuals look a bit more centered. You can also pass a NumPy array for
    `alpha`:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 残差看起来更加集中。你也可以为 `alpha` 传递一个NumPy数组：
- en: '[PRE74]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'This leads to the following graphs:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生以下图表：
- en: '![](img/877acdd9-751d-40ab-a2db-8ab3246271bb.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/877acdd9-751d-40ab-a2db-8ab3246271bb.png)'
- en: The array alphas are not compatible with `cross_val_score`, so I cannot select
    this model as the best model by looking at the final graphs and deciding which
    is the best. So, our final model selection is `gpr7n` with `alpha=7` and `normalize_y=True`.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 数组 alphas 与 `cross_val_score` 不兼容，因此我无法通过查看最终图形并判断哪一个是最佳模型来选择该模型。所以，我们最终选择的模型是
    `gpr7n`，并且设置了 `alpha=7` 和 `normalize_y=True`。
- en: There's more...
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: Underneath it all, the kernel computes covariances between points in `X`. It
    assumes that similar points in the inputs should lead to similar outputs. Gaussian
    processes are great for confidence predictions and smooth-like outputs. (Later,
    we will see random forests, that do not lead to smooth outputs even though they
    are very predictive.)
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一切的背后，核函数计算了`X`中各点之间的协方差。它假设输入中相似的点应该导致相似的输出。高斯过程在置信度预测和光滑输出方面表现得非常好。（稍后我们将看到随机森林，尽管它们在预测方面非常准确，但并不会产生光滑的输出。）
- en: 'We may need to understand the uncertainty in our estimates. If we pass the
    `eval_MSE` argument as true, we''ll get `MSE` and the predicted values, so we
    can make the predictions. A tuple of predictions and `MSE` is returned, from a
    mechanics standpoint:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能需要了解我们估计值的不确定性。如果我们将 `eval_MSE` 参数设置为真，我们将得到 `MSE` 和预测值，从而可以进行预测。从机械学角度来看，返回的是预测值和
    `MSE` 的元组：
- en: '[PRE75]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Plot all of the predictions with error bars as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，绘制所有带误差条的预测：
- en: '[PRE76]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '![](img/0116515f-41b8-4bfd-9c78-911c85eac557.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0116515f-41b8-4bfd-9c78-911c85eac557.png)'
- en: 'Set `n=20` in the preceding code to look at fewer points:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中设置 `n=20` 以查看较少的点：
- en: '![](img/003714ee-b7a8-4bad-a8d8-81da8b3abe4e.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/003714ee-b7a8-4bad-a8d8-81da8b3abe4e.png)'
- en: The uncertainty is very high for some points. As you can see, there is a lot
    of of variance in the estimates for many of the given points. However, the overall
    error is not that bad.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些点，不确定性非常高。如你所见，许多给定点的估计值有很大的差异。然而，整体误差并不算太差。
- en: Using SGD for regression
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SGD进行回归
- en: In this recipe, we'll get our first taste of stochastic gradient descent. We'll
    use it for regression here.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将首次体验随机梯度下降。我们将在回归中使用它。
- en: Getting ready
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: SGD is often an unsung hero in machine learning. Underneath many algorithms,
    there is SGD doing the work. It's popular due to its simplicity and speed—these
    are both very good things to have when dealing with a lot of data. The other nice
    thing about SGD is that while it's at the core of many machine learning algorithms
    computationally, it does so because it easily describes the process. At the end
    of the day, we apply some transformation on the data, and then we fit our data
    to the model with a loss function.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: SGD（随机梯度下降）通常是机器学习中的一个默默无闻的英雄。在许多算法背后，正是SGD在默默地执行工作。由于其简单性和速度，SGD非常受欢迎——这两者在处理大量数据时是非常有用的。SGD的另一个优点是，尽管它在许多机器学习算法的计算核心中扮演重要角色，但它之所以如此有效，是因为它能够简明地描述这一过程。归根结底，我们对数据应用一些变换，然后用损失函数将数据拟合到模型中。
- en: How to do it…
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'If SGD is good on large datasets, we should probably test it on a fairly large
    dataset:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果SGD在大数据集上表现良好，我们应该尝试在一个相对较大的数据集上测试它：
- en: '[PRE77]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: It's probably worth gaining some intuition about the composition and size of
    the object. Thankfully, we're dealing with NumPy arrays, so we can just access
    `nbytes`. The built-in Python way to access the object's size doesn't work for
    NumPy arrays.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 了解对象的组成和大小可能是值得的。幸运的是，我们处理的是NumPy数组，因此可以直接访问 `nbytes`。Python内建的访问对象大小的方法对于NumPy数组不起作用。
- en: 'This output can be system dependent, so you may not get the same results:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该输出可能与系统有关，因此你可能无法得到相同的结果：
- en: '[PRE78]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'To get some human perspective, we can convert `nbytes` to megabytes. There
    are roughly 1 million bytes in a megabyte:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了获得一些人类的视角，我们可以将 `nbytes` 转换为兆字节。大约每兆字节包含100万字节：
- en: '[PRE79]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'So, the number of bytes per data point is as follows:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，每个数据点的字节数如下：
- en: '[PRE80]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Well, isn't that tidy, and fairly tangential, for what we're trying to accomplish?
    However, it's worth knowing how to get the size of the objects you're dealing
    with.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，对于我们想要实现的目标，这样不显得有点杂乱无章吗？然而，了解如何获取你正在处理的对象的大小是很重要的。
- en: 'So, now that we have the data, we can simply fit a `SGDRegressor`:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所以，现在我们有了数据，我们可以简单地拟合一个`SGDRegressor`：
- en: '[PRE81]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: So, we have another beefy object. The main thing to know now is that our loss
    function is `squared_loss`, which is the same thing that occurs during linear
    regression. It is also worth noting that shuffle will generate a random shuffle
    of the data. This is useful if you want to break a potentially spurious correlation.
    With `X`, scikit-learn will automatically include a column of ones.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们有了另一个庞大的对象。现在要了解的主要内容是我们的损失函数是`squared_loss`，这与线性回归中的损失函数相同。还需要注意的是，`shuffle`会对数据进行随机打乱。如果你想打破一个可能存在的虚假相关性，这个功能会非常有用。使用`X`时，scikit-learn会自动包含一列1。
- en: 'We can then predict, as we previously have, using scikit-learn''s consistent
    API. You can see we actually got a really good fit. There is barely any variation,
    and the histogram has a nice normal look.:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以像以前一样，使用scikit-learn一致的API进行预测。你可以看到我们实际上得到了一个非常好的拟合。几乎没有任何变化，且直方图看起来像是正态分布。
- en: '[PRE82]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '![](img/e0b40007-829c-455a-8bcc-bcb3866d8f80.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0b40007-829c-455a-8bcc-bcb3866d8f80.png)'
- en: How it works…
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Clearly, the fake dataset we used wasn't too bad, but you can imagine datasets
    with larger magnitudes. For example, if you worked on Wall Street on any given
    day, there might be 2 billion transactions on any given exchange in a market.
    Now, imagine that you have a week's or a year's data. Running in-core algorithms
    does not work with huge volumes of data.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，我们使用的虚拟数据集还不错，但你可以想象一些具有更大规模的数据集。例如，如果你在华尔街工作，在任何一天，某个市场上的交易量可能达到20亿笔。现在，想象一下你有一周或一年的数据。处理大量数据时，内存中的算法并不适用。
- en: The reason this is normally difficult is that to do SGD, we're required to calculate
    the gradient at every step. The gradient has the standard definition from any
    third calculus course.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 之所以通常比较困难，是因为要执行SGD，我们需要在每一步计算梯度。梯度有任何高等微积分课程中的标准定义。
- en: 'The gist of the algorithm is that at each step, we calculate a new set of coefficients
    and update this with a learning rate and the outcome of the objective function.
    In pseudocode, this might look like the following:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的要点是，在每一步中，我们计算一组新的系数，并用学习率和目标函数的结果更新它。在伪代码中，这可能看起来像这样：
- en: '[PRE83]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'The relevant variables are as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 相关变量如下：
- en: '`w`: This is the coefficient matrix.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`w`：这是系数矩阵。'
- en: '`learning_rate`: This shows how big a step to take at each iteration. This
    might be important to tune if you aren''t getting good convergence.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：这表示每次迭代时步长的大小。如果收敛效果不好，可能需要调整这个值。'
- en: '`gradient`: This is the matrix of second derivatives.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient`：这是二阶导数的矩阵。'
- en: '`cost`: This is the squared error for regression. We''ll see later that this
    cost function can be adapted to work with classification tasks. This flexibility
    is one thing that makes SGD so useful.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cost`：这是回归的平方误差。稍后我们会看到，这个代价函数可以适应分类任务。这种灵活性是SGD如此有用的一个原因。'
- en: This will not be so bad, except for the fact that the gradient function is expensive.
    As the vector of coefficients gets larger, calculating the gradient becomes very
    expensive. For each update step, we need to calculate a new weight for every point
    in the data, and then update. SGD works slightly differently; instead of the previous
    definition for batch gradient descent, we'll update the parameter with each new
    data point. This data point is picked at random, hence the name stochastic gradient
    descent.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这不会太难，除了梯度函数很昂贵这一点。随着系数向量的增大，计算梯度变得非常昂贵。对于每一次更新步骤，我们需要为数据中的每一个点计算一个新的权重，然后更新。SGD的工作方式略有不同；与批量梯度下降的定义不同，我们将每次用新的数据点来更新参数。这个数据点是随机选取的，因此得名随机梯度下降。
- en: A final note on SGD is that it is a meta-heuristic that gives a lot of power
    to several machine learning algorithms. It is worth checking out some papers on
    meta-heuristics applied to various machine learning algorithms. Cutting-edge solutions
    might be innocently hidden in such papers.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 关于SGD的最后一点是，它是一种元启发式方法，赋予了多种机器学习算法强大的能力。值得查阅一些关于元启发式方法在各种机器学习算法中的应用的论文。前沿解决方案可能就隐藏在这些论文中。
