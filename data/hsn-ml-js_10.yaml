- en: Natural Language Processing in Practice
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理实践
- en: Natural language processing is the science (and art) of parsing, analyzing,
    and reconstructing natural language, such as written or spoken English, French,
    or German. It's not an easy task; **natural language processing** (**NLP**) is
    an entire field of research with a vibrant academic research community and significant
    financial backing from major tech firms. Every time companies such as Google,
    Apple, Amazon, and Microsoft invest in their Google Assistant, Siri, Alexa, and
    Cortana products, the field of NLP gets a little more funding. In short, NLP is
    why you can talk to your phone and your phone can talk back to you.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理是解析、分析和重建自然语言（如书面或口语英语、法语或德语）的科学（和艺术）。这不是一项容易的任务；**自然语言处理**（**NLP**）是一个完整的研究领域，拥有充满活力的学术研究社区和来自主要科技公司的重大资金支持。每当谷歌、苹果、亚马逊和微软投资其谷歌助手、Siri、Alexa和Cortana产品时，NLP领域就会获得更多资金。简而言之，NLP是您能够与手机交谈，手机也能对您说话的原因。
- en: Siri is a lot more than NLP. We, as consumers, like to criticize our **artificial
    intelligence** (**AI**) assistants when they get something laughably wrong. But
    they are truly marvels of engineering, and the fact that they get *anything *right
    is a wonder!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Siri不仅仅是NLP。作为消费者，我们喜欢批评我们的**人工智能**（**AI**）助手当它们犯下可笑的错误。但它们确实是工程奇迹，它们能够做到任何正确的事情都是一个奇迹！
- en: 'If I look at my phone and say, *Ok Google, give me directions to 7-Eleven*,
    my phone will automatically wake up and respond to me, *Ok, going to 7-Eleven
    on Main Ave, make the next right*. Let''s think about what that takes to accomplish:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我看向我的手机并说，“Ok Google，给我去7-Eleven的路线”，我的手机将自动唤醒并对我回应，“好的，去Main Ave的7-Eleven，下一个右转”。让我们思考一下要完成这个任务需要什么：
- en: My sleeping phone is monitoring for my pre-trained OK Google catchphrase.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的睡眠中的手机正在监控我预先训练的“OK Google”短语。
- en: The audio buffer gets an audio hash match on the OK Google soundwaves that I
    trained, and wakes up the phone.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音频缓冲区在训练的OK Google声音波上得到音频哈希匹配，并唤醒手机。
- en: The phone starts capturing audio, which is just a time-series vector of numbers
    (representing sound wave intensity).
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手机开始捕捉音频，这只是一个表示声音波强度的数字时间序列向量。
- en: The speech audio is decoded to phonemes, or textual representation of phonetic
    sounds. Several candidates for each utterance are generated.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音音频被解码为音素，或语音声音的文本表示。为每个话语生成几个候选者。
- en: The candidate phonemes are combined together to try to make words. The algorithm
    uses a max-likelihood or other estimator to figure out which of the combinations
    is most likely to be a real sentence that would be used in the current context.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将候选音素组合在一起，试图形成单词。算法使用最大似然或其他估计器来确定哪种组合最有可能是在当前上下文中实际使用的句子。
- en: The resultant sentence must be parsed for meaning, so many types of preprocessing
    are performed, and each word is tagged with its possible **parts of speech** (**POS**).
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果句子必须解析其意义，因此执行了许多类型的预处理，并且每个单词都被标记为其可能的**词性**（**POS**）。
- en: A learning system (typically an ANN) will try to determine intent given the
    phrase's subject, object, and verb.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个学习系统（通常是人工神经网络）将尝试根据短语的主题、宾语和动词确定意图。
- en: The actual intent must be carried out by a subroutine.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际意图必须由子例程执行。
- en: A response to the user must be formulated. In the case where the response can't
    be scripted, it must be generated algorithmically.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须制定对用户的响应。在响应无法脚本化的情况下，它必须通过算法生成。
- en: A text-to-speech algorithm decodes the response into phonemes and must then
    synthesize natural-sounding speech, which then plays over your phone's speakers.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本到语音算法将响应解码为音素，然后必须合成听起来自然的语音，该语音随后通过手机的扬声器播放。
- en: Congratulations, you're on your way to getting your Slurpee! Your experience
    is powered by several ANNs, many different uses of various NLP tools, giant corpuses
    of data, and millions of engineer-hours of effort to build and maintain. This
    experience also explains the close relationship between NLP and ML—they are not
    the same thing, but they're partnered together at the forefront of technology.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你，你正在走向获得你的Slurpee！您的体验由多个人工神经网络、各种NLP工具的多种用途、庞大的数据集以及数百万工程师小时的努力来构建和维护。这种体验还解释了NLP和ML之间的密切关系——它们不是同一件事，但它们在技术前沿并肩作战。
- en: 'Obviously, there''s more to NLP than the topics we can cover in 25 pages. This
    chapter doesn''t aim to be comprehensive; its aim is to familiarize you with the
    most common tactics you''ll employ when solving ML problems that involve natural
    language. We''ll take a whirlwind tour of seven NLP-related concepts:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，NLP的内容远不止25页所能涵盖的主题。本章的目标不是全面介绍；它的目标是使你熟悉在解决涉及自然语言的ML问题时最常用的策略。我们将快速浏览七个与NLP相关的概念：
- en: Measuring string distance
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量字符串距离
- en: The TF-IDF metric
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF度量
- en: Tokenizing text
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分词
- en: Stemming words
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词干提取
- en: Phonetics
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音学
- en: Parts of speech tagging
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性标注
- en: Word embedding with Word2vec
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Word2vec进行词嵌入
- en: 'Don''t worry if those topics look daunting. We''ll introduce each one in turn
    and show lots of examples. There''s a lot of jargon involved in NLP, and many
    edge cases, so the topic seems unapproachable at first glance. But the topic is
    *natural language* after all: we speak it every day! Once we learn the jargon,
    the topic becomes quite intuitive, because we all have a very strong intuitive
    understanding of language.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些主题看起来令人畏惧，请不要担心。我们将逐一介绍每个主题，并展示许多示例。在NLP中涉及许多术语，以及许多边缘情况，所以乍一看这个主题似乎难以接近。但毕竟，这个主题是**自然语言**：我们每天都在说它！一旦我们学会了术语，这个主题就变得相当直观，因为我们大家对语言都有非常强烈的直观理解。
- en: 'We''ll start our discussion with a simple question: how do you measure the
    distance between *quit *and *quote*? We already know that we can measure the distance
    between two points in space, so now let''s take a look at measuring the distance
    between two words.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个简单的问题开始我们的讨论：你如何测量*quit*和*quote*之间的距离？我们已经知道我们可以测量空间中两点之间的距离，那么现在让我们来看看如何测量两个单词之间的距离。
- en: String distance
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字符串距离
- en: It is always convenient to be able to measure some form of distance between
    two points. In previous chapters, we used the distance between points to aid in
    clustering and classification. We can do the same for words and passages in NLP.
    The problem, of course, is that words are made up of letters, and distances are
    made up of numbers—so how do we make a number out of two words?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 总是能够测量两点之间某种形式的距离是非常方便的。在之前的章节中，我们使用了点之间的距离来辅助聚类和分类。我们也可以在NLP中对单词和段落做同样的事情。当然，问题是单词由字母组成，而距离由数字组成——那么我们如何从两个单词中得出一个数字呢？
- en: Enter Levenshtein distance*—*a simple metric that measures the number of single-character
    edits it would take to transform one string into the other. The Levenshtein distance
    allows insertions, deletions, and substitutions. A modification of the Levenshtein
    distance, called the **Damerau-Levenshtein distance**, also allows transpositions,
    or the swapping of two neighboring letters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 输入Levenshtein距离*—*这是一个简单的度量，它衡量将一个字符串转换为另一个字符串所需的单字符编辑次数。Levenshtein距离允许插入、删除和替换。Levenshtein距离的一种修改版本，称为**Damerau-Levenshtein距离**，也允许交换两个相邻字母。
- en: 'To illustrate this concept with an example, let''s try transforming the word
    **crate** into the word **plate**:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用示例说明这个概念，让我们尝试将单词**crate**转换为单词**plate**：
- en: Replace the **r** with **l **to get **clate**
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将**r**替换为**l**以得到**clate**
- en: Replace the **c **with a **p **to get **plate**
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将**c**替换为**p**以得到**plate**
- en: The Levenshtein distance between crate and plate is therefore 2.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: crate和plate之间的Levenshtein距离因此是2。
- en: 'The distance between **plate** and **laser** is 3:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**板**和**激光器**之间的距离是3：'
- en: Delete the **p** to get **late**
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除**p**以得到**late**
- en: Insert an **r** to get **later**
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插入一个**r**以得到**later**
- en: Replace the **t** with an **s** to get **laser**
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将**t**替换为**s**以得到**laser**
- en: 'Let''s confirm these examples in code. Create a new directory called `Ch10-NLP`
    and add the following `package.json` file:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在代码中确认这些示例。创建一个名为`Ch10-NLP`的新目录，并添加以下`package.json`文件：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then issue `yarn install` from the command line to install the dependencies.
    This `package.json` file is a little different from the one in previous chapters,
    because the `wordnet-db` dependency is not compatible with the Browserify bundler.
    We will therefore have to omit some advanced JavaScript features in this chapter.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然后从命令行运行`yarn install`来安装依赖项。这个`package.json`文件与之前章节中的文件略有不同，因为`wordnet-db`依赖项与Browserify打包器不兼容。因此，我们将不得不在本章中省略一些高级JavaScript功能。
- en: 'Create a directory called `src` and add to it an `index.js` file to which you''ll
    add the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`src`的目录，并向其中添加一个`index.js`文件，你将在其中添加以下内容：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You'll use these imports for the rest of the chapter, so keep them in the `index.js`
    file. However, the rest of the code we use in this chapter will be fungible; you
    may delete old irrelevant code as you work through the examples in this chapter
    if you wish.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在本章的其余部分使用这些导入，所以请将它们保存在`index.js`文件中。然而，本章中我们使用的其余代码将是可互换的；如果你愿意，在处理本章中的示例时可以删除旧的不相关代码。
- en: 'Let''s take a look at Levenshtein distance using the `natural.js` library:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`natural.js`库来看看 Levenshtein 距离：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Run `yarn start` from the command line and you''ll see the following output:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令行中运行`yarn start`，你会看到以下输出：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Try experimenting with a few pairs of words and see if you can calculate the
    distances in your head to get an intuitive feel for it.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试对几对单词进行实验，看看你是否能在大脑中计算出距离，以获得对它的直观感受。
- en: The Levenshtein distance has many uses, since it is a metric and not any specific
    tool. Other systems, such as spellcheckers, suggester's, and fuzzy matcher's,
    use Levenshtein, or edit distance metrics in their own algorithms.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Levenshtein 距离有许多用途，因为它是一个度量标准，而不是任何特定的工具。其他系统，如拼写检查器、建议器和模糊匹配器，在自己的算法中使用 Levenshtein
    或编辑距离度量。
- en: 'Let''s take a look at a more advanced metric: the TF-IDF score, which represents
    how interesting or important a particular word is among a set of documents.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个更高级的度量标准：TF-IDF 分数，它表示一个特定单词在文档集中有多有趣或重要。
- en: Term frequency - inverse document frequency
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词频-逆文档频率
- en: One of the most popular metrics used in search relevance, text mining, and information
    retrieval is the **term frequency-inverse document frequency** (**TF-IDF**) score.
    In essence, TF-IDF measures how significant a word is to a particular document. The
    TF-IDF metric therefore only makes sense in the context of a word in a document
    that's part of a larger corpus of documents.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索相关性、文本挖掘和信息检索中最受欢迎的度量标准之一是**词频-逆文档频率**（**TF-IDF**）分数。本质上，TF-IDF 衡量一个词对特定文档的重要性。因此，TF-IDF
    度量标准因此只在单词属于更大文档集的文档的上下文中才有意义。
- en: Imagine you have a corpus of documents, such as blog posts on varying topics,
    that you want to make searchable. The end user of your application runs a search
    query for *fashion style*. How do you then find matching documents and rank them
    by relevance?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你有一批文档，比如不同主题的博客文章，你希望使其可搜索。你的应用程序的最终用户运行了一个搜索查询，搜索的是*fashion style*。那么，你如何找到匹配的文档并根据相关性对它们进行排序？
- en: The TF-IDF score is made of two separate but related components. The first is *term
    frequency*, or the relative frequency of a specific term in a given document.
    If a 100-word blog post contains the word *fashion *four times, then the term
    frequency of the word *fashion* is 4% for that one document.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 分数由两个单独但相关的组成部分组成。第一个是*词频*，即在给定文档中一个特定词的相对频率。如果一个100字的博客文章中包含单词*fashion*四次，那么该文档中单词*fashion*的词频是4%。
- en: Note that term frequency only requires a single term and a single document as
    parameters; the full corpus of documents is not required for the term frequency
    component of TF-IDF.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，词频只需要一个词和一个文档作为参数；TF-IDF 的词频组件不需要整个文档集。
- en: Term frequency by itself is not sufficient to determine relevance, however.
    Words such as *this* and *the* appear very frequently in most text and will have
    high term frequencies, but those words are not typically relevant to any search.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 单独的词频不足以确定相关性。像*this*和*the*这样的词在大多数文本中都非常常见，并且会有很高的词频，但这些词通常与任何搜索都不相关。
- en: 'We therefore introduce a second metric to our calculation: inverse document
    frequency. This metric is essentially the inverse of the percentage of documents
    that a given word appears in. If you have 1,000 blog posts, and the word *fashion*
    appears in 50 of them, the (non-inverse) document frequency of that word is 5%.
    The inverse document frequency is an extension of this concept, given by taking
    the log of the inverse document frequency.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在计算中引入了第二个度量标准：逆文档频率。这个度量标准本质上是一个给定单词出现在文档中的百分比的倒数。如果你有1,000篇博客文章，而单词*fashion*出现在其中的50篇，那么该单词的非逆文档频率是5%。逆文档频率是这个概念的扩展，通过取逆文档频率的对数给出。
- en: If n[fashion ]is the number of documents containing the word *fashion* and *N*
    is the total number of documents, then the inverse document frequency is given
    by *log(N / n[fashion])*. In our example, the inverse document frequency of the
    word *fashion* is roughly 1.3.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果n[fashion]是包含单词*fashion*的文档数量，而*N*是文档总数，那么逆文档频率由*log(N / n[fashion])*给出。在我们的例子中，单词*fashion*的逆文档频率大约是1.3。
- en: If we now consider the word *the*, which may appear in 90% of documents, we
    find that the inverse document frequency of *the* is 0.0451, much smaller than
    the 1.3 we got for *fashion*. The inverse document frequency therefore measures
    how rare or unique a given word is across a set of documents; higher values mean
    the word is more rare. The parameters required to calculate inverse document frequency
    are the term itself and the corpus of documents (unlike term frequency, which
    only requires one document).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在考虑单词*the*，它可能出现在90%的文档中，我们发现*the*的逆文档频率是0.0451，远小于我们为*fashion*得到的1.3。因此，逆文档频率衡量的是给定单词在文档集中的稀有程度或独特性；值越高，意味着单词越稀有。计算逆文档频率所需的参数是术语本身和文档语料库（与仅需要一个文档的词频不同）。
- en: The TF-IDF score is calculated by multiplying the term frequency and inverse
    document frequency together. The result is a single metric that encapsulates how
    significant or interesting a single term is to a specific document, considered
    across all documents that you've seen. Words such as *the* and *that* may have
    high term frequencies in any one document, but because they are prevalent across
    all documents, their overall TF-IDF score will be very low. Words, such as *fashion*,
    that exist only in a subset of documents will have a higher TF-IDF score. When
    comparing two separate documents that both contain the word *fashion*, the document
    that uses it more often will have a higher TF-IDF score, as the inverse document
    frequency portion will be the same for both documents.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF分数是通过将词频和逆文档频率相乘来计算的。结果是单个指标，它封装了单个术语对特定文档的重要性或兴趣，考虑了您所看到的所有文档。像*the*和*that*这样的词可能在任何单个文档中具有高词频，但由于它们在所有文档中都普遍存在，它们的总体TF-IDF分数将非常低。像*fashion*这样的词，只存在于文档的子集中，将具有更高的TF-IDF分数。当比较两个都包含单词*fashion*的单独文档时，使用它更频繁的文档将具有更高的TF-IDF分数，因为两个文档的逆文档频率部分将是相同的。
- en: When scoring search results for relevance, the most common approach is to calculate
    the TF-IDF scores for each term in the search query and for each document in the
    corpus. The individual TF-IDF scores for each query term can be added together,
    and the resultant sum can be called the **relevance score** of that particular
    document. Once all matching documents are scored in this manner, you can sort
    by relevance and display them in that order. Most full-text search systems, such
    as Lucene and Elasticsearch, use this sort of approach to relevance scoring.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在对搜索结果进行相关性评分时，最常见的方法是计算搜索查询中每个术语以及语料库中每个文档的TF-IDF分数。每个查询术语的个别TF-IDF分数可以相加，得到的总和可以称为该特定文档的**相关性分数**。一旦所有匹配的文档都以这种方式评分，就可以按相关性排序并按此顺序显示它们。大多数全文搜索引擎，如Lucene和Elasticsearch，都使用这种相关性评分方法。
- en: 'Let''s see this in practice, using the `natural.js` TF-IDF tool. Add the following
    to `index.js`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用`natural.js` TF-IDF工具来实际看看。将以下内容添加到`index.js`中：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This code defines a `fulltextSearch` function that accepts a search query and
    an array of documents to be searched. Each document is added to the TF-IDF database
    object, where it is automatically tokenized by `natural.js`. Run the program with `yarn
    start` and you''ll see the following output:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码定义了一个`fulltextSearch`函数，该函数接受一个搜索查询和要搜索的文档数组。每个文档都添加到TF-IDF数据库对象中，其中它被`natural.js`自动分词。使用`yarn
    start`运行程序，您将看到以下输出：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The first two documents, which have nothing to do with fashion or style, return
    scores of zero. The term frequency component for both *fashion* and *style* in
    those documents is zero, so the overall score becomes zero. The third document
    also has a score of zero. This document does make a reference to fashion, however,
    the tokenizer was not able to reconcile the word *fashionable* with *fashion*,
    as no stemming has been performed. We'll discuss both tokenization and stemming
    in depth in the later sections of this chapter, but for now it's sufficient to
    know that *stemming *is an operation that reduces a word to its root form.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个文档与时尚或风格无关，返回的分数为零。这些文档中*时尚*和*风格*的词频组件为零，因此整体分数变为零。第三个文档的分数也是零。然而，该文档确实提到了时尚，但是分词器无法将单词*时尚的*与*时尚*相匹配，因为没有进行词干提取。我们将在本章后面的部分深入讨论分词和词干提取，但就目前而言，了解*词干提取*是一种将单词还原为其词根形式的操作就足够了。
- en: Documents three and four have non-zero scores. Document three has a higher score
    because it includes both the words *fashion* and *style*, whereas document four
    only includes the word *style*. This simple metric has done a surprisingly good
    job of capturing relevance, which is why it's so widely used.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个和第四个文档的分数不为零。第三个文档的分数更高，因为它包含了*时尚*和*风格*这两个词，而第四个文档只包含了*风格*这个词。这个简单的指标在捕捉相关性方面做得出奇的好，这也是为什么它被广泛使用的原因。
- en: 'Let''s update our code to add a stemming operation. After applying stemming
    to the text, we would expect document two to also have a non-zero relevance score,
    since *fashionable* should be transformed to *fashion* by the stemmer. Add the
    following code to `index.js`:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更新我们的代码以添加一个词干提取操作。在应用词干提取到文本之后，我们预计第二个文档也将有一个非零的相关性分数，因为*时尚的*应该被词干提取器转换为*时尚*。将以下代码添加到`index.js`中：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We have added a `stemAndTokenize` helper method and applied it both to the
    documents added to the database and to the search query. Run the code with `yarn
    start` and you''ll see the updated output:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经添加了一个`stemAndTokenize`辅助方法，并将其应用于添加到数据库中的文档以及搜索查询。使用`yarn start`运行代码，你会看到更新的输出：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As expected, document two now has a non-zero score because the stemmer was able
    to transform the word *fashionable* into *fashion*. Documents two and four have
    the same score, but only because this is a very simple example; with a much larger
    corpus we would not expect the inverse document frequencies of the terms *fashion*
    and *style* to be equivalent.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，第二个文档现在有一个非零分数，因为词干提取器能够将单词*时尚的*转换为*时尚*。第二个和第四个文档的分数相同，但这仅仅是因为这是一个非常简单的例子；在一个更大的语料库中，我们不会期望*时尚*和*风格*这两个词的逆文档频率是相等的。
- en: Search relevance and ranking is not the only application for TF-IDF. This metric
    is widely used across many use cases and problem domains. One interesting use
    for TF-IDF is article summarization. In article summarization, the goal is to
    reduce a written passage to only a few sentences that effectively summarize the
    passage.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF不仅用于搜索相关性和排名。这个指标在许多用例和问题领域中得到了广泛的应用。TF-IDF的一个有趣用途是文章摘要。在文章摘要中，目标是减少一段文字，只保留几个能够有效总结该段落的句子。
- en: One approach to the article summarization problem is to consider each sentence
    or paragraph in an article to be a separate document. After indexing each sentence
    for TF-IDF, you then evaluate each individual word's TF-IDF score and use that
    to score each sentence as a whole. Pick the top three or five sentences and display
    them in their original order, and you will have a decent summary.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 解决文章摘要问题的方法之一是将文章中的每个句子或段落视为一个单独的文档。在为TF-IDF索引每个句子之后，然后评估每个单词的TF-IDF分数，并使用这些分数对整个句子进行评分。选择前三或五个句子，并按原始顺序显示它们，你将得到一个不错的摘要。
- en: 'Let''s see this in action, using both `natural.js` and `compromise.js`. Add
    the following code to `index.js`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个实际应用，使用`natural.js`和`compromise.js`。将以下代码添加到`index.js`中：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding `summarize` method implements the following procedure:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的`summarize`方法实现了以下步骤：
- en: Use `compromise.js` to extract sentences from the article
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`compromise.js`从文章中提取句子
- en: Add each individual sentence to the TF-IDF database
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个单独的句子添加到TF-IDF数据库中
- en: For each word in the article, calculate its TF-IDF score for each sentence
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于文章中的每个单词，计算其在每个句子中的TF-IDF分数
- en: Add each word's TF-IDF score to a list of total scores for each sentence (the `scoresMap`
    object)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个单词的TF-IDF分数添加到每个句子的总分数列表（`scoresMap`对象）中
- en: Convert `scoresMap` into an array to make sorting easier
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`scoresMap`转换为数组，以便排序更简单
- en: Sort `scoresArray` by descending relevance score
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按降序相关性分数对`scoresArray`进行排序
- en: Remove all but the top-scoring sentences
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除除了得分最高的句子之外的所有句子
- en: Re-sort `scoresArray` by the chronological order of sentences
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按句子的时间顺序重新排序`scoresArray`
- en: Build the summary by joining the top sentences together
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过连接得分最高的句子来构建摘要
- en: 'Let''s add a simple article to the code and try out both a three-sentence and
    a five-sentence summary. In this example, I''ll use the first few paragraphs of
    this section, but you can replace the text with anything you like. Add the following
    to `index.js`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在代码中添加一个简单的文章，并尝试使用三句和五句的摘要。在这个例子中，我会使用本节的前几段，但你可以用任何你喜欢的内容替换文本。将以下内容添加到`index.js`中：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When you run the code with `yarn start`, you''ll see the following output:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用`yarn start`运行代码时，你会看到以下输出：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The quality of these summaries illustrates both the power and flexibility of
    the `tf-idf metric`, while also highlighting the fact that you don't always need
    advanced ML or AI algorithms to accomplish interesting tasks. There are many other
    uses of TF-IDF, so you should consider using this metric any time you need the
    relevance of a word or term as it pertains to a document in a corpus.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些摘要的质量展示了`tf-idf度量`的强大功能和灵活性，同时也突出了这样一个事实：你并不总是需要高级的ML或AI算法来完成有趣的任务。TF-IDF有许多其他用途，所以你应该考虑在需要将单词或术语与语料库中的文档的相关性相关联时使用此度量。
- en: In this section, we made use of tokenizers and stemmers without formally introducing
    them. These are core concepts in NLP, so let's now introduce them formally.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用了分词器和词干提取器，但没有正式介绍它们。这些是NLP中的核心概念，所以现在让我们正式介绍它们。
- en: Tokenizing
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词
- en: Tokenizing is the act of transforming an input string, such as a sentence, paragraph,
    or even an object such as an email, into individual *tokens*. A very simple tokenizer
    might take a sentence or paragraph and split it by spaces, thus generating tokens
    that are individual words. However, tokens do not necessarily need to be words,
    nor does every word in an input string need to be returned by the tokenizer, nor
    does every token generated by the tokenizer need to be present in the original
    text, nor does a token need to represent only one word. We therefore use the term
    *token* rather than *word* to describe the output of a tokenizer, as tokens are
    not always words.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是将输入字符串（如句子、段落，甚至是一个对象，如电子邮件）转换为单个*tokens*的行为。一个非常简单的分词器可能会将句子或段落按空格分割，从而生成单个单词的tokens。然而，tokens不一定是单词，输入字符串中的每个单词也不一定需要被分词器返回，分词器生成的每个tokens也不一定需要在原始文本中存在，而且一个tokens也不一定只代表一个单词。因此，我们使用*token*这个词而不是*word*来描述分词器的输出，因为tokens并不总是单词。
- en: The manner in which you tokenize text before processing it with an ML algorithm
    has a major effect on the performance of the algorithm. Many NLP and ML applications
    use a *bag-of-words* approach, in which only the words or tokens matter but their
    order does not, as in the Naive Bayes classifier we explored in [Chapter 5](8ba34275-43c8-4d0c-a8d3-8e0dd89dd2f9.xhtml), *Classification
    Algorithms*. However, a tokenizer that generates *bigrams*, or pairs of words
    found next to each other, will actually preserve some of the positional and semantic
    meaning of the original text even when used with a bag-of-words algorithm.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用机器学习算法处理文本之前进行分词的方式对算法的性能有重大影响。许多NLP和ML应用使用*词袋模型*方法，其中只关注单词或tokens，而不关注它们的顺序，就像我们在第5章中探讨的朴素贝叶斯分类器一样，*分类算法*。然而，生成*二元组*（即相邻单词的成对）的分词器实际上在用于词袋模型算法时，会保留原始文本的一些位置和语义意义。
- en: There are many ways to tokenize text. As mentioned, the simplest method is to
    split a sentence by spaces to generate a *token stream *of individual words. There
    are numerous problems with the simple approach, however. For one, the algorithm
    will treat capitalized words as being distinct from their lowercase versions;
    Buffalo and buffalo are considered two separate words or tokens. Sometimes this
    is desirable, other times it's not. Oversimplified tokenization will also treat
    contractions such as *won't* as being distinct and separate from the words *will
    not*, which will get split into two separate tokens, *will and not*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 文本标记化有许多方法。如前所述，最简单的方法是将句子通过空格拆分以生成一个*标记流*，其中包含单个单词。然而，简单方法存在许多问题。首先，算法将大写单词视为与其小写版本不同；Buffalo和buffalo被视为两个不同的单词或标记。有时这是可取的，有时则不然。过于简化的标记化还将像*won't*这样的缩写视为独立且与单词*will
    not*不同，后者将被拆分为两个单独的标记，*will*和*not*。
- en: In most cases, that is, in 80% of applications, the simplest tokenization that
    one should consider is a tokenizer that converts all text to lowercase, removes
    punctuation and newlines, removes formatting and markup such as HTML, and even
    removes *stopwords* or common words such as *this* or *the*. In other cases, more
    advanced tokenization is necessary, and in some cases, simpler tokenization is
    desired.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，即在80%的应用中，一个人应该考虑的最简单的标记化是，将所有文本转换为小写，删除标点符号和新行，删除格式化和标记，如HTML，甚至删除*停用词*或常见单词，如*this*或*the*。在其他情况下，需要更高级的标记化，在某些情况下，需要更简单的标记化。
- en: In this section, I've been describing the act of tokenization as a compound
    process, including case transformations, removing non-alphanumeric characters,
    and stopword filtering. However, tokenizer libraries will each have their own
    opinion as to what the roles and responsibilities of the tokenizer are. You may
    need to combine a library's tokenization tool with other tools in order to achieve
    the desired effect.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我一直在描述标记化行为作为一个复合过程，包括大小写转换、删除非字母数字字符和停用词过滤。然而，标记化库将各自有自己的观点，关于标记化器的角色和责任。您可能需要将库的标记化工具与其他工具结合使用，以实现所需的效果。
- en: 'First, let''s build our own simple tokenizer. This tokenizer will convert a
    string to lowercase, remove non-alphanumeric characters, and also remove words
    that are fewer than three characters in length. Add the following to your `index.js`
    file, either replacing the Levenshtein distance code or adding beneath it:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们构建自己的简单标记化器。这个标记化器将字符串转换为小写，删除非字母数字字符，并删除长度少于三个字符的单词。将以下内容添加到您的`index.js`文件中，要么替换Levenshtein距离代码，要么添加到其下方：
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This `simpleTokenizer` will convert the string to lowercase, remove apostrophes
    in the middle of a word (so that *won't* becomes *wont*), and filter out all other
    non-word characters by replacing them with spaces. It then splits the string by
    the space character, returning an array, and finally removes any items that have
    fewer than three characters.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`simpleTokenizer`会将字符串转换为小写，删除单词中间的撇号（因此*won't*变为*wont*），并通过将所有其他非单词字符替换为空格来过滤掉所有其他非单词字符。然后，它通过空格字符拆分字符串，返回一个数组，并最终删除任何少于三个字符的项目。
- en: 'Run `yarn start` and you''ll see the following:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`yarn start`，您将看到以下内容：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This token stream can then be given to an algorithm, either in an ordered or
    unordered fashion. A classifier, such as Naive Bayes, will ignore the order and
    analyze each word as if it were independent.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个标记流可以被提供给一个算法，无论是按顺序还是无序。例如，朴素贝叶斯分类器将忽略顺序，并将每个单词视为独立进行分析。
- en: 'Let''s compare our simple tokenizer to two tokenizers provided by `natural.js` and `compromise.js`.
    Add the following to your `index.js` file:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较我们的简单标记化器与`natural.js`和`compromise.js`提供的两个标记化器。将以下内容添加到您的`index.js`文件中：
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Running the code with `yarn start` will yield the following output:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`yarn start`运行代码将产生以下输出：
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see, short words have been preserved, and contractions, such as *I've*,
    have been split up into separate tokens. Additionally, capitalization has been
    preserved.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，短单词已被保留，并且像*I've*这样的缩写已被拆分为单独的标记。此外，大小写也被保留。
- en: 'Let''s try another one of `natural.js` tokenizers:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试另一个`natural.js`标记化器：
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This will result in:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This tokenizer continues to split on punctuation, however, the punctuation itself
    is preserved. In applications where punctuation is important, this may be desired.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个标记化器继续在标点符号上拆分，但标点符号本身被保留。在标点符号重要的应用中，这可能是有需求的。
- en: 'Other tokenizer libraries, such as the one in `compromise.js`, take a more
    intelligent approach and even perform POS tagging in order to parse and understand
    the sentence while tokenizing. Let''s try a number of `compromise.js` tokenizing
    techniques:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 其他分词库，例如`compromise.js`中的分词库，采取了一种更智能的方法，甚至在分词的同时进行词性标注，以便在分词过程中解析和理解句子。让我们尝试几种`compromise.js`的分词技术：
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Run the new code with `yarn start` and you''ll see the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`yarn start`运行新代码，您将看到以下内容：
- en: '[PRE18]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `words()` tokenizer does not split contractions apart like the `natural.js`
    tokenizer did. Additionally, `compromise.js` gives you the capability to extract
    specific entity types from the text. We can separately extract adjectives, nouns,
    verbs, questions, contractions (even with the capability to expand contractions);
    we can also use `compromise.js` to extract dates, hashtags, lists, clauses, and
    numerical values.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`words()`分词器不会像`natural.js`分词器那样将缩写词分开。此外，`compromise.js`还为您提供从文本中提取特定实体类型的能力。我们可以分别提取形容词、名词、动词、疑问词、缩写词（甚至具有扩展缩写词的能力）；我们还可以使用`compromise.js`提取日期、标签、列表、从句和数值。'
- en: It is also not a requirement that your tokens must map directly to words and
    phrases in the input text. For instance, when developing a spam filter for an
    email system, you might find that including some data from the email header in
    the token stream gives you a huge accuracy improvement. Whether the email passes
    SPF and DKIM checks may be a very strong signal to your spam filter. You might
    also find that differentiating body text from the subject line is also advantageous;
    it may also be the case that words that appear as hyperlinks are stronger signals
    than plaintext.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 您的标记不必直接映射到输入文本中的单词和短语。例如，当为电子邮件系统开发垃圾邮件过滤器时，您可能会发现将一些来自电子邮件头部的数据包含在标记流中可以大幅提高准确性。电子邮件是否通过SPF和DKIM检查可能对您的垃圾邮件过滤器来说是一个非常强烈的信号。您还可能发现区分正文文本和主题行也是有益的；可能的情况是，作为超链接出现的单词比纯文本中的单词是更强的信号。
- en: 'Often, the simplest way to tokenize this type of semi-structured data is to
    prefix the tokens with a character or set of characters that normally would not
    be allowed by the tokenizer. For example, tokens in the subject line of an email
    may be prefixed by `_SUBJ:` and tokens that appear in hyperlinks may be prefixed
    by `_LINK:`. To illustrate this, here''s an example of what a token stream might
    look like for an email:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，对这种半结构化数据进行分词的最简单方法是在标记前加上一个或一组通常不允许分词器使用的字符。例如，电子邮件主题行中的标记可能以`_SUBJ:`为前缀，而出现在超链接中的标记可能以`_LINK:`为前缀。为了说明这一点，这里是一个电子邮件标记流的示例：
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Even if a Naive Bayes classifier has never seen references to pharmaceuticals
    before, it may see that most spam emails have failed their DKIM check and still
    flag this message as spam. Or perhaps you work closely with the accounting department
    and they often get emails about payments, but almost never receive a legitimate
    email with the word `pay` in a hyperlink to an external site; the differentiation
    of the *pay* token appearing in plaintext versus the `_LINK:pay` token appearing
    in a hyperlink may make all the difference between an email being classified as
    spam or not.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 即使朴素贝叶斯分类器以前从未见过关于药品的引用，它也可能发现大多数垃圾邮件邮件都未能通过DKIM检查，但仍将此消息标记为垃圾邮件。或者，也许您与会计部门紧密合作，他们经常收到有关付款的电子邮件，但几乎从未收到包含指向外部网站的超链接中的单词`pay`的合法电子邮件；在纯文本中出现的`*pay*`标记与在超链接中出现的`_LINK:pay`标记之间的区分可能对电子邮件是否被分类为垃圾邮件有决定性的影响。
- en: In fact, one of the earliest spam filtering breakthroughs, developed by Paul
    Graham, of Y Combinator fame, used this approach of annotated email tokens to
    mark a significant improvement in the accuracy of early spam filters.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，最早期的垃圾邮件过滤突破之一，由Y Combinator的保罗·格雷厄姆开发，就是使用这种带有注释的电子邮件标记的方法，显著提高了早期垃圾邮件过滤器的准确性。
- en: Another approach to tokenization is *n-gram* tokenization, which splits an input
    string into N-sized groups of neighboring tokens. In fact, all tokenization is
    n-gram tokenization, however, in the preceding examples, N is set to 1\. More
    typically, n-gram tokenization refers to schemes where N > 1\. Most commonly,
    you'll encounter *bigram* and *trigram* tokenization.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种分词方法是*n-gram*分词，它将输入字符串分割成N个相邻标记的N大小组。实际上，所有分词都是n-gram分词，然而，在前面的例子中，N被设置为1。更典型的是，n-gram分词通常指的是N
    > 1的方案。最常见的是*二元组*和*三元组*分词。
- en: The purpose of bigram and trigram tokenization is to preserve some context around
    individual words. An example related to sentiment analysis is an easy visualization.
    The phrase *I did not love the movie* will be tokenized (with a *unigram* tokenizer,
    or n-gram tokenizer where N = 1) to *I*, *did*, *not*, *love*, *the*, *movie*.
    When using a bag-of-words algorithm, such as Naive Bayes, the algorithm will see
    the word *love* and guess that the sentence has a positive sentiment, as bag-of-words
    algorithms do not consider the relationships between words.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 二元和三元标记化的目的是保留围绕单个单词的一些上下文。与情感分析相关的一个例子是易于可视化。短语*I did not love the movie*将被标记化（使用单语标记化器，或n-gram标记化器，其中N
    = 1）为*I*，*did*，*not*，*love*，*the*，*movie*。当使用如朴素贝叶斯这样的词袋算法时，算法将看到单词*love*并猜测句子具有积极情感，因为词袋算法不考虑单词之间的关系。
- en: A bigram tokenizer, on the other hand, can trick a naive algorithm into considering
    the relationships between words, because every *pair* of words becomes a token.
    The preceding phrase, processed with a bigram tokenizer, will become *I did*,
    *did not*, *not love*, *love the*, *the movie*. Even though each token is composed
    of two individual words, the algorithm operates on tokens and therefore will treat
    *not love* differently from *I love*. A sentiment analyzer will therefore have
    more context around each word and will be able to distinguish negations (*not
    love*) from positive phrases.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，二元标记化器可以欺骗一个简单的算法去考虑单词之间的关系，因为每一对单词都变成了一个标记。使用二元标记化器处理的前一个短语将变成*I did*，*did
    not*，*not love*，*love the*，*the movie*。尽管每个标记由两个单独的单词组成，但算法是在标记上操作的，因此会将*not love*与*I
    love*区别对待。因此，情感分析器将围绕每个单词有更多的上下文，并能区分否定（*not love*）和积极短语。
- en: 'Let''s try out the `natural.js` bigram tokenizer on our earlier example sentence.
    Add the following code to `index.js`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在先前的示例句子上尝试`natural.js`二元标记化器。将以下代码添加到`index.js`中：
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Running the code with `yarn start` will yield:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`yarn start`运行代码将产生：
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The biggest issue with n-gram tokenization is that it dramatically increases
    the entropy of the data domain. When training an algorithm on n-grams, you're
    not just on the hook for making sure the algorithm learns all the significant
    words, but also all the significant *pairs *of words. There are many more pairs
    of words than there are unique words, so n-gram tokenization will only work when
    you have a very large and comprehensive training set.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram标记化最大的问题是它会显著增加数据域的熵。当在n-gram上训练算法时，你不仅要确保算法学习到所有重要的单词，还要学习到所有重要的**单词对**。单词对的数量比唯一的单词数量要多得多，因此n-gram标记化只有在你有一个非常庞大且全面的训练集时才能工作。
- en: One clever way around the n-gram entropy issue, particularly for dealing with
    negations in sentiment analysis, is to transform the token immediately following
    the negation in the same way we handled email headers and subject lines. For example,
    the phrase *not love* can be tokenized as *not*, *_NOT:love*, or *not*, *!love*,
    or even just *!love* (discarding *not* as an individual token).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一种巧妙地绕过n-gram熵问题的方法，尤其是在处理情感分析中的否定时，是将否定词后面的标记以与处理电子邮件标题和主题行相同的方式进行转换。例如，短语*not
    love*可以被标记为*not*, *_NOT:love*，或者*not*, *!love*，甚至只是*!love*（将*not*作为一个单独的标记丢弃）。
- en: Under this scheme, the phrase *I did not love the movie* will get tokenized
    as *I*, *did*, *not*, *_NOT:love*, *the*, *movie*. The advantage of this approach
    is that the contextual negation still gets preserved, but in general we are still
    using low-entropy unigrams that can be trained with a smaller dataset.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案下，短语*I did not love the movie*将被标记化为*I*，*did*，*not*，*_NOT:love*，*the*，*movie*。这种方法的优势在于上下文否定仍然得到了保留，但总的来说，我们仍然使用低熵的单语标记，这些标记可以用较小的数据集进行训练。
- en: There are many ways to tokenize text, and each has its advantages and disadvantages.
    As always, the approach you choose will depend on the task at hand, the training
    data available, and the problem domain itself.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 标记文本有许多方法，每种方法都有其优缺点。正如往常一样，你选择的方法将取决于手头的任务、可用的训练数据以及问题域本身。
- en: Keep the topic of tokenization in mind throughout the next few sections, as
    those topics can also be applied to the tokenization process. For example, you
    can stem words after tokenizing to further reduce entropy, or you can filter your
    tokens by their TF-IDF score, therefore only using the most interesting words
    in a document.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，请始终牢记分词的主题，因为这些主题也可以应用于分词过程。例如，您可以在分词后对单词进行词干提取以进一步减少熵，或者您可以根据它们的TF-IDF分数过滤您的标记，因此只使用文档中最有趣的单词。
- en: In order to continue our discussion about entropy, let's take a moment to discuss *stemming*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续我们关于熵的讨论，让我们花一点时间来讨论*词干提取*。
- en: Stemming
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词干提取
- en: Stemming is a type of transformation that can be applied to a single word, though
    typically the stemming operation occurs right after tokenizing. Stemming after
    tokenizing is so common that `natural.js` offers a `tokenizeAndStem` convenience
    method that can be attached to the `String` class prototype.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取是一种可以应用于单个单词的转换类型，尽管通常词干操作发生在分词之后。在分词后进行词干提取非常常见，以至于`natural.js`提供了一个`tokenizeAndStem`便利方法，可以附加到`String`类原型上。
- en: Specifically, stemming reduces a word to its root form, for instance by transforming
    *running* to *run*. Stemming your text after tokenizing can significantly reduce
    the entropy of your dataset, because it essentially de-duplicates words with similar
    meanings but different tenses or inflections. Your algorithm will not need to
    learn the words *run*, *runs*, *running*, and *runnings* separately, as they will
    all get transformed into *run*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，词干提取将单词还原为其词根形式，例如将*running*转换为*run*。在分词后对文本进行词干提取可以显著减少数据集的熵，因为它本质上去除了具有相似意义但时态或词形不同的单词。您的算法不需要分别学习单词*run*、*runs*、*running*和*runnings*，因为它们都将被转换为*run*。
- en: The most popular stemming algorithm, the *Porter* *stemmer*, is a heuristic
    algorithm that defines a number of staged rules for the transformation. But, in
    essence, it boils down to cutting the standard verb and noun inflections off the
    end of the word and dealing with specific edge cases and common irregular forms
    as they arise.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的词干提取算法，即*Porter*词干提取器，是一种定义了多个阶段规则的启发式算法。但本质上，它归结为从单词末尾切掉标准的动词和名词词形变化，并处理出现的特定边缘情况和常见不规则形式。
- en: In one sense, stemming is a sort of compression algorithm that discards information
    about inflections and specific word forms, but retains the conceptual information
    left behind by the word root. Stemming should therefore not be used in cases where
    the inflection or form of the language itself is important.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，词干提取是一种压缩算法，它丢弃了关于词形变化和特定单词形式的信息，但保留了由词根留下的概念信息。因此，在词形变化或语言形式本身很重要的场合不应使用词干提取。
- en: 'For the same reason, stemming excels in situations where the conceptual information
    is more important than the form. Topic extraction is a good example: it doesn''t
    matter if someone is writing about their own experiences as a runner versus their
    experience watching track races—they''re still writing about running.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于同样的原因，词干提取在概念信息比形式更重要的情况下表现优异。主题提取就是一个很好的例子：无论是某人写关于自己作为跑者的经历还是观看田径比赛的经历，他们都是在写关于跑步。
- en: Because stemming reduces data entropy, it is very effectively employed when
    the dataset is small or modest in size. Stemming cannot be applied carelessly,
    however. A very large dataset may incur an accuracy penalty if you use stemming
    unnecessarily. You destroy information when you stem text, and models with very
    large training sets may have been able to use that extra information to generate
    better predictions.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 由于词干提取减少了数据熵，因此在数据集较小或适度大小时非常有效地使用。然而，词干提取不能随意使用。如果在不必要的情况下使用词干提取，非常大的数据集可能会因准确性降低而受到惩罚。您在提取文本时会破坏信息，具有非常大的训练集的模型可能已经能够使用这些额外信息来生成更好的预测。
- en: 'In practice, you should never have to guess whether your model will perform
    better with or without stemming: you should try both ways and see which performs
    better. I can''t tell you *when *to use stemming, I can only tell you why it works
    and why it sometimes doesn''t.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，您永远不需要猜测您的模型是否在带词干或不带词干的情况下表现更好：您应该尝试两种方法，看看哪种表现更好。我无法告诉您何时使用词干提取，我只能告诉您为什么它有效，以及为什么有时它不起作用。
- en: 'Let''s try out the `natural.js` Porter stemmer, and we''ll combine it with
    our tokenization from earlier. Add the following to `index.js`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一下`natural.js`的Porter词干提取器，并将其与之前的分词结合起来。将以下内容添加到`index.js`中：
- en: '[PRE22]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Run the code with `yarn start` and you''ll see the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`yarn start`运行代码，你会看到以下内容：
- en: '[PRE23]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This simple example illustrates how words with different forms get reduced into
    their conceptual meanings. It also illustrates that there is no guarantee that
    the stemmer will create *real *words (you won't find `lucki` in the dictionary),
    only that it will reduce entropy for a set of similarly constructed words.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的例子说明了不同形式的单词是如何被简化为其概念意义的。它还说明了，并不能保证词干提取器会创建出**真实**的单词（你不会在词典中找到`lucki`），而只是它会为一系列结构相似的单词减少熵。
- en: There are other stemmer algorithms that try to approach the problem more linguistically.
    That type of stemming is called **lemmatization**, and the analog to stems is
    called the **lemma**, or the dictionary form of a word. In essence, a lemmatizer
    is a stemmer that first determines the part of speech of the word (typically requiring
    a dictionary, such as *WordNet*), and then applies in-depth rules for that specific
    part of speech, potentially involving more lookup tables. As an example, the word
    *better* is unchanged by stemming, but it is transformed into the word *good*
    by lemmatization. Lemmatization is not necessary in most everyday tasks, but may
    be useful when your problem requires more precise linguistic rules or drastically
    reduced entropy.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 有其他词干提取算法试图从更语言学角度来解决这个问题。这种类型的词干提取被称为**词元化**，而词元的对应物称为**词元**，或单词的词典形式。本质上，词元化器是一个词干提取器，它首先确定单词的词性（通常需要一个词典，如*WordNet*），然后应用针对该特定词性的深入规则，可能涉及更多的查找表。例如，单词*better*在词干提取中保持不变，但通过词元化它被转换成单词*good*。在大多数日常任务中，词元化并不是必要的，但在你的问题需要更精确的语言学规则或显著减少熵时可能是有用的。
- en: 'We can''t discuss NLP or linguistics without also discussing the most common
    mode of communication: speech. How does a speech-to-text or text-to-speech system
    actually know how to say the hundreds of thousands of defined words in the English
    language, plus an arbitrary amount of names? The answer is *phonetics*.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在讨论自然语言处理或语言学时，不能不讨论最常见的交流方式：语音。语音转文字或文字转语音系统实际上是如何知道如何说出英语中定义的数十万个单词，以及任意数量的名字的呢？答案是**声音学**。
- en: Phonetics
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 声音学
- en: Speech detection, such as those used in speech-to-text systems, is a surprisingly
    difficult problem. There are so many variations in styles of speaking, pronunciation,
    dialect, and accent, as well as variations in rhythm, tone, speed, and elocution,
    plus the fact that audio is a simple one-dimensional time-domain signal, that
    it's no surprise that even today's state-of-the-art smartphone tech is *good,
    not great*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 语音检测，如语音转文字系统中使用的，是一个出人意料困难的问题。说话的风格、发音、方言和口音，以及节奏、音调、速度和发音的变化如此之多，再加上音频是一个简单的一维时间域信号的事实，因此，即使是当今最先进的智能手机技术也只是**良好，而非卓越**。
- en: While modern speech-to-text goes much deeper than what I'll present here, I
    would like to show you the concept of *phonetic algorithms*. These algorithms
    transform a word into something resembling a phonetic hash, such that it is easy
    to identify words that sound similar to one another.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然现代语音转文字技术比我要展示的深入得多，但我希望向你展示**声音学算法**的概念。这些算法将一个单词转换成类似声音散列的东西，使得识别听起来相似的字词变得容易。
- en: The *metaphone* algorithm is one such phonetic algorithm. Its aim is to reduce
    a word down to a simplified phonetic form, with the ultimate goal of being able
    to index similar pronunciations. Metaphone uses an alphabet of 16 characters: 0BFHJKLMNPRSTWXY.
    The 0 character represents the *th* sound, *X* represents a *sh* or *ch* sound,
    and the other letters are pronounced as usual. Nearly all vowel information is
    lost in the transformation, though some are preserved if they are the first sound
    in a word.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**元音算法**就是这样一种声音学算法。它的目的是将一个单词简化为一个简化的声音形式，最终目标是能够索引相似的发音。元音算法使用16个字符的字母表：0BFHJKLMNPRSTWXY。0字符代表**th**音，*X*代表**sh**或**ch**音，其他字母按常规发音。几乎所有的元音信息都在转换中丢失，尽管如果它们是一个单词的第一个声音，一些元音会被保留。'
- en: A simple example illustrates where phonetic algorithms can be useful. Imagine
    that you're in charge of a search engine and people keep searching for *knowledge
    is power, France is bacon*. You, having familiarity with art history, would understand
    that it was actually Francis Bacon who said *knowledge is power*, and that your
    users have simply misheard the quote. You'd like to add a *Did you mean: **Francis
    Bacon ***link to your search results, but don't know how to approach the problem.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的例子说明了音位算法可能在哪里有用。想象一下，你负责一个搜索引擎，人们不断搜索“知识就是力量，法国是培根”。你熟悉艺术史，会明白实际上是弗朗西斯·培根说过“知识就是力量”，而你的用户只是听错了引言。你希望在你的搜索结果中添加一个“你是指：**弗朗西斯·培根**”的链接，但你不知道如何解决这个问题。
- en: 'Let''s take a look at how the Metaphone algorithm would phoneticize the terms
    `France is Bacon` and `Francis Bacon`. Add the following to `index.js`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Metaphone算法如何将`France is Bacon`和`Francis Bacon`这两个术语音位化。在`index.js`中添加以下内容：
- en: '[PRE24]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'When you run the code with `yarn start`, you''ll see the following:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用`yarn start`运行代码时，你会看到以下内容：
- en: '[PRE25]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Francis has transformed into `FRNSS`, France has transformed into `FRNS`, and
    Bacon has transformed into `BKN`. Intuitively, these strings represent the most
    distinguishable sounds used to pronounced the word.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 弗朗西斯已经变成了`FRNSS`，法国变成了`FRNS`，而培根变成了`BKN`。直观上，这些字符串代表了用来发音单词的最易区分的音素。
- en: After phoneticizing, we can use the Levenshtein distance to measure the similarity
    between two words. If you ignore the space, *FRNSS BKN* and *FRNS IS BKN* only
    have a Levenshtein distance of one between them (the addition of the *I*); these
    two phrases therefore sound very similar. You can use this information, combined
    with the rest of the search term and a reverse lookup, to determine that `France
    is Bacon` is a plausible mispronunciation of `Francis Bacon`, and that `Francis
    Bacon` is actually the correct topic to present in your search results. Phonetic
    misspellings and misunderstandings, such as `France is Bacon`, are so common that
    we even use them in some spellchecker tools.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在音位化之后，我们可以使用Levenshtein距离来衡量两个单词之间的相似度。如果你忽略空格，*FRNSS BKN*和*FRNS IS BKN*之间只有一个Levenshtein距离（添加了*I*）；因此这两个短语听起来非常相似。你可以使用这些信息，结合搜索词的其余部分和反向查找，来确定`France
    is Bacon`是`Francis Bacon`可能的误读，并且`Francis Bacon`实际上是你在搜索结果中应该展示的正确主题。像`France
    is Bacon`这样的音位拼写错误和误解非常普遍，以至于我们在一些拼写检查工具中也使用它们。
- en: A similar approach is used in speech-to-text systems. The recording system does
    its best to capture the specific vowel and consonant sounds you make and uses
    a phonetic index (a reverse lookup of phonetics mapped to various dictionary words)
    to come up with a set of candidate words. Typically, a neural network will then
    determine which is the most likely combination of words considering both the confidence
    of the phonetic form and the semantic meaningfulness or meaninglessness of the
    resultant statements. The set of words that makes the most sense is what is presented
    to you.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在语音到文本系统中，使用了一种类似的方法。录音系统尽力捕捉你发出的特定元音和辅音音素，并使用音位索引（音位映射到各种词典单词的反向查找）来提出一组候选单词。通常，一个神经网络将确定哪种单词组合最有可能，考虑到音位形式的置信度和结果语句的语义意义或无意义。最有意义的单词集就是展示给你的。
- en: 'The `natural.js` library also provides a convenience method to compare two
    words, returning *true *if they sound alike. Try the following code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`natural.js`库还提供了一个方便的方法来比较两个单词，如果它们听起来相似则返回`true`。尝试以下代码：'
- en: '[PRE26]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: When run, this will return `true` and then `false`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时，这将返回`true`然后`false`。
- en: You should consider using phonetic algorithms any time your problem involves
    pronunciation or working with similar-sounding words and phrases. This is usually
    restricted to more specialized fields, but speech-to-text and text-to-speech systems
    are becoming very popular, and you may find yourself needing to update your search
    algorithm for phonetic sound-alikes if users start interacting with your service
    by speech in the future.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的问题涉及发音或处理类似发音的单词和短语时，你应该考虑使用音位算法。这通常限于更专业的领域，但语音到文本和文本到语音系统变得越来越受欢迎，你可能会发现自己需要更新你的搜索算法以适应语音相似音素，如果用户未来通过语音与你服务互动的话。
- en: Speaking of speech systems, let's now take a look at POS tagging and how it
    can be used to extract semantic information from phrases—such as commands you
    might issue to your smartphone assistant.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 说到语音系统，现在让我们看看 POS 标注以及它是如何用于从短语中提取语义信息的——例如，您可能对智能手机助手下达的命令。
- en: Part of speech tagging
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词性标注
- en: A **part of speech** (**POS**) tagger analyzes a piece of text, such as a sentence,
    and determines each individual word's POS in the context of the sentence. The
    only way to accomplish this is with a dictionary lookup, so it is not an algorithm
    that can be developed from first principles alone.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**词性**（**POS**）标注器分析一段文本，如一个句子，并确定句子中每个单词的词性。唯一实现这一点的方法是字典查找，因此它不是一个仅从第一原理开发的算法。'
- en: A great use case for POS tagging is intent extraction from commands. For instance,
    when you say *Siri, please order me a pizza from John's pizzeria*, the AI system
    will tag the command with parts of speech in order to extract the subject, verb,
    object, and any other relevant details from the command.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: POS 标注的一个很好的用例是从命令中提取意图。例如，当你对 Siri 说“请从约翰的比萨店为我订一份披萨”时，人工智能系统将使用词性对命令进行标注，以便从命令中提取主语、谓语、宾语以及任何其他相关细节。
- en: Additionally, POS tagging is often used as a supporting tool for other NLP operations.
    Topic extraction, for instance, makes heavy use of POS tagging in order to separate
    people, places, and topics from verbs and adjectives.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，POS 标注通常用作其他 NLP 操作的辅助工具。例如，主题提取就大量使用了 POS 标注，以便将人、地点和主题从动词和形容词中分离出来。
- en: Keep in mind that POS tagging is never perfect, due to the ambiguity of the
    English language in particular. Many words can be used both as a noun and a verb,
    so many POS taggers will return a list of candidate parts of speech for a given
    word. Libraries that perform POS tagging have a wide range of sophistication,
    ranging from simple heuristics, to dictionary lookups, to advanced models that
    attempt to determine the POS based on context.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，由于英语语言的歧义性，POS 标注永远不会完美。许多词既可以作名词也可以作动词，因此许多 POS 标注器将为给定单词返回一系列候选词性。执行 POS
    标注的库具有广泛的复杂性，从简单的启发式方法到字典查找，再到基于上下文尝试确定词性的高级模型。
- en: The `compromise.js` library has a flexible POS tagger and matching/extraction
    system. The `compromise.js` library is unique in that it aims to be *good enough*
    but not comprehensive; it is trained on only the most common words in the English
    language, which is enough to give 80-90% accuracy for most cases while still being
    a fast and small library.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`compromise.js` 库具有灵活的 POS 标注器和匹配/提取系统。`compromise.js` 库的独特之处在于它旨在“足够好”但不是全面的；它仅训练了英语中最常见的单词，这对于大多数情况来说足够提供
    80-90% 的准确性，同时仍然是一个快速且小巧的库。'
- en: 'Let''s see the `compromise.js` POS tagging and matching in action. Add the
    following code to `index.js`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 `compromise.js` 的 POS 标注和匹配的实际效果。将以下代码添加到 `index.js` 中：
- en: '[PRE27]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Using `compromise.js` allows us to extract just the verbs, or just the nouns
    (and other parts of speech) from the command. Running the code with `yarn start`
    will yield:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `compromise.js` 允许我们从命令中提取仅动词，或仅名词（以及其他词性）。使用 `yarn start` 运行代码将产生：
- en: '[PRE28]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The POS tagger has identified `order` as the sole verb in the sentence; this
    information can then be used to load up the correct subroutine for making orders
    that's built into Siri's AI system. The extracted nouns can then be sent to the
    subroutine in order to determine what type of order to make and from where.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: POS 标记器已将 `order` 识别为句子中的唯一动词；然后可以使用此信息来加载 Siri 人工智能系统中内置的用于下订单的正确子程序。然后可以将提取出的名词发送到子程序，以确定要下何种类型的订单以及从哪里下。
- en: Impressively, the POS tagger has also identified `John's pizzeria` as a single
    noun, rather than considering the words `John's` and `pizzeria` to be separate
    nouns. The tagger has understood that `John's` is a possessive, and therefore
    applies to the word following it.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 令人印象深刻的是，POS 标注器还将 `John's pizzeria` 识别为一个单独的名词，而不是将 `John's` 和 `pizzeria` 视为单独的名词。标注器已经理解
    `John's` 是一个所有格，因此适用于其后的单词。
- en: 'We can also use `compromise.js` to write parsing and extraction rules for common
    commands. Let''s try one out:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 `compromise.js` 编写用于常见命令的解析和提取规则。让我们试一个例子：
- en: '[PRE29]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Running the code with `yarn start` will yield:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `yarn start` 运行代码将产生：
- en: '[PRE30]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The same matching selector is able to capture both of these commands, ignoring
    the addressee of the command (Siri or Google) through match groups (denoted with `[]`).
    Because both commands follow the verb-noun-noun pattern, both will match the selector.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的匹配选择器能够捕捉这两个命令，通过匹配组（用`[]`表示）忽略命令的接收者（Siri或Google）。因为这两个命令都遵循动词-名词-名词的模式，所以两者都会匹配选择器。
- en: 'Of course, this selector by itself is not enough to build a full AI system
    such as Siri or Google Assistant. This tool would be used near the beginning of
    the AI system process in order to determine the user''s overall intent, based
    on predefined but flexible command formats. You could program a system to respond
    to phrases such as *Open my #Noun*, where the noun can be `calendar` or `email`
    or `Spotify`, or *Write an email to #Noun*, and so on. This tool can be used as
    a first step toward building your own speech or natural language command system,
    as well as for various topic-extraction applications.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，仅凭这个选择器本身是不够构建一个完整的AI系统，如Siri或Google Assistant的。这个工具将在AI系统过程的早期使用，以便根据预定义但灵活的命令格式确定用户的整体意图。你可以编程一个系统来响应诸如“打开我的#名词”这样的短语，其中名词可以是“日历”、“电子邮件”或“Spotify”，或者“给#名词写一封电子邮件”，等等。这个工具可以用作构建自己的语音或自然语言命令系统的第一步，以及用于各种主题提取应用。
- en: Throughout this chapter, we've discussed the foundational tools used in NLP.
    Many advanced NLP tasks use an ANN as part of the learning process, but for many
    novice practitioners, it's unclear exactly how words and natural language should
    be sent to the input layer of an ANN. In the next section, we will discuss *word
    embedding*, particularly the Word2vec algorithm, which can be used to feed words
    into an ANN and other systems.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了NLP中使用的基石工具。许多高级NLP任务将ANN作为学习过程的一部分，但对于许多新手实践者来说，如何将单词和自然语言发送到ANN的输入层并不明确。在下一节中，我们将讨论“词嵌入”，特别是Word2vec算法，它可以用来将单词输入到ANN和其他系统中。
- en: Word embedding and neural networks
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入和神经网络
- en: Throughout this chapter, we've discussed various NLP techniques, particularly
    with regards to preprocessing text. In many use cases, we will need to interact
    with an ANN to perform the final analysis. The type of analysis is not relevant
    to this section, but imagine you're developing a sentiment analysis ANN. You appropriately
    tokenize and stem your training text, then, as you attempt to train your ANN on
    your preprocessed text, you realize you have no idea how to get words into a neural
    network.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了各种NLP技术，特别是关于文本预处理。在许多用例中，我们需要与ANN交互以执行最终分析。分析的类型与这一节无关，但想象你正在开发一个情感分析ANN。你适当地标记和词干化你的训练文本，然后，当你尝试在预处理后的文本上训练你的ANN时，你意识到你不知道如何将单词输入到神经网络中。
- en: The simplest approach is to map each input neuron in the network to an individual
    unique word. When processing a document, you can set the input neuron's value
    to the term frequency (or absolute count) of that word in the document. You'll
    have a network where one input neuron responds to the word *fashion*, another
    neuron responds to *technology*, another neuron responds to *food*, and so on.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是将网络中的每个输入神经元映射到一个独特的单词。在处理文档时，你可以将输入神经元的值设置为该单词在文档中的词频（或绝对计数）。你将拥有一个网络，其中一个输入神经元对单词“时尚”做出反应，另一个神经元对“技术”做出反应，另一个神经元对“食物”做出反应，等等。
- en: This approach will work, but it has several drawbacks. The topology of an ANN
    must be defined in advance, so you must know how many unique words are in your
    training set before you start training the network; this will become the size
    of the input layer. This also means that your network is not capable of learning
    new words after it has been trained. To add a new word to the network, you must
    essentially build and train a new network from scratch.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以工作，但它有几个缺点。ANN的拓扑结构必须预先定义，因此在开始训练网络之前，你必须知道你的训练集中有多少独特的单词；这将成为输入层的大小。这也意味着一旦网络被训练，它就无法学习新单词。要向网络添加新单词，你实际上必须从头开始构建和训练一个新的网络。
- en: Additionally, throughout a corpus of documents, you may encounter tens of thousands
    of unique words. This has a huge negative impact on the efficiency of the ANN,
    as you will need an input layer with, say, 10,000 neurons. This will dramatically
    increase the training time required by the network as well as the memory and processing
    requirements of the system.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在整个文档语料库中，你可能会遇到成千上万的独特单词。这会对ANN的效率产生巨大的负面影响，因为你将需要一个有10,000个神经元的输入层。这将大大增加网络所需的训练时间，以及系统的内存和处理需求。
- en: The approach of one-word-per-neuron also intuitively feels inefficient. While
    your corpus contains 10,000 unique words, most of them will be rare and only appear
    in a few documents. For most documents, only a few hundred input neurons will
    be activated, with the others set to zero. This amounts to what is called a **sparse
    matrix** or **sparse vector**, or a vector where most of the values are zero.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元对应一个单词的方法在直观上感觉效率不高。虽然你的语料库包含10,000个独特的单词，但其中大多数将是罕见的，并且只出现在少数文档中。对于大多数文档，只有几百个输入神经元会被激活，其他则设置为零。这相当于所谓的**稀疏矩阵**或**稀疏向量**，或者是一个大部分值都是零的向量。
- en: A more evolved approach is therefore required when natural language interacts
    with ANNs. A family of techniques called *word embedding* can analyze a corpus
    of text and transform each word into a fixed-length vector of numerical values.
    The vector is a fixed-length representation of a word in much the same way that
    a hash (such as md5 or sha1) is a fixed-length representation of arbitrary data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当自然语言与人工神经网络（ANNs）交互时，需要一种更高级的方法。一种被称为**词嵌入**的技术族可以分析文本语料库，并将每个单词转换为一个固定长度的数值向量。这个向量与哈希（如md5或sha1）作为任意数据的固定长度表示方式类似，也是单词的固定长度表示。
- en: Word embedding confers several advantages, particularly when used with ANNs.
    Because the word vectors are fixed in length, the topology of the network can
    be decided beforehand and can also handle the appearance of new words after the
    initial training.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入提供了几个优势，尤其是在与人工神经网络结合使用时。由于单词向量长度固定，网络的拓扑结构可以在事先决定，并且也可以处理初始训练后新词的出现。
- en: The word vectors are also *dense vectors*, meaning that you don't need 10,000
    input neurons in your network. A good value for the size of a word vector (and
    the size of the input layer) is somewhere between 100-300 items. This factor alone
    significantly reduces the dimensionality of your ANN and will allow for much faster
    training and convergence of the model.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 单词向量也是**密集向量**，这意味着你不需要在你的网络中有10,000个输入神经元。单词向量（以及输入层）的大小一个好的值是在100-300项之间。这个因素本身就可以显著降低你的ANN的维度，并允许模型训练和收敛更快。
- en: 'There are many word embedding algorithms to choose from, but the current state-of-the-art
    choice is the Word2vec algorithm, developed at Google. This particular algorithm
    also has another desirable trait: similar words will be clustered close to one
    another in terms of their vector representations.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多词嵌入算法可供选择，但当前最先进的选项是谷歌开发的Word2vec算法。这个特定的算法还有一个令人向往的特性：在向量表示方面，相似的单词会聚集在一起。
- en: Earlier in this chapter, we saw that we can use string distance to measure the
    typographical distance between two words. We can also use the string distance
    between two phonetic representations of words to measure how similar they sound.
    When using Word2vec, you can measure the distance between two word vectors to
    get the *conceptual *distance between two words.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期，我们看到了我们可以使用字符串距离来衡量两个单词之间的印刷距离。我们还可以使用两个单词的音位表示之间的字符串距离来衡量它们听起来有多相似。当使用Word2vec时，你可以测量两个单词向量之间的距离，以获取两个单词之间的**概念**距离。
- en: The Word2vec algorithm is itself a shallow neural network that trains itself
    on your corpus of text. The algorithm uses n-grams to develop a sense of the context
    between words. If the words *fashion* and *blogger* often appear next to each
    other in your corpus, Word2vec will assign similar vectors to those words. If
    *fashion* and *mathematics* rarely appear together, their resultant vectors will
    be separated by some distance. The distance between two word vectors therefore
    represents their conceptual and contextual distance, or how alike two words are
    in terms of their semantic content and context.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec算法本身是一个浅层神经网络，它在你文本语料库上自我训练。该算法使用n-gram来发展单词之间的上下文感觉。如果你的语料库中“时尚”和“博主”经常一起出现，Word2vec将为这些单词分配相似的向量。如果“时尚”和“数学”很少一起出现，它们的结果向量将被一定距离分开。因此，两个词向量之间的距离代表了它们的概念和上下文距离，或者两个单词在语义内容和上下文方面有多相似。
- en: This trait of the Word2vec algorithm also confers its own efficiency and accuracy
    advantage to the ANN that ultimately processes the data, as the word vectors will
    activate similar input neurons for similar words. The Word2vec algorithm has not
    only reduced the dimensionality of the problem, but it has also added contextual
    information to the word embedding's. This additional contextual information is
    exactly the type of signal that ANNs are highly proficient at picking up on.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec算法的这一特性也赋予了最终处理数据的ANN自己的效率和准确性优势，因为词向量将为相似单词激活相似的输入神经元。Word2vec算法不仅降低了问题的维度，还为词嵌入添加了上下文信息。这种额外的上下文信息正是ANN非常擅长捕捉的信号类型。
- en: 'The following is an example of a common workflow involving both natural language
    and ANNs:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个涉及自然语言和人工神经网络的常见工作流程示例：
- en: Tokenize and stem all text
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对所有文本进行分词和词干提取
- en: Remove stopwords from text
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本中移除停用词
- en: Determine the appropriate ANN input layer size; use this value both for the
    input layer and the Word2vec dimensionality
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定适当的ANN输入层大小；使用此值既用于输入层也用于Word2vec的维度
- en: Use Word2vec to generate word embedding's for your text
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Word2vec为你的文本生成词嵌入
- en: Use the word embedding's to train the ANN on your task
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用词嵌入来训练ANN以完成你的任务
- en: When evaluating a new document, tokenize, stem, and vectorize the document before
    passing it to the ANN
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评估新文档时，在将其传递给ANN之前对文档进行分词、词干提取和向量化
- en: Using a word-embedding algorithm such as Word2vec will not only improve the
    speed and memory performance of your model, but it will likely also increase the
    accuracy of your model due to the contextual information that the Word2vec algorithm
    preserves. It should also be noted that Word2vec is, like n-gram tokenization,
    one possible way to trick a naive bag-of-words algorithm into taking word context
    into account, as the Word2vec algorithm itself uses n-grams to develop the embedding's.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Word2vec等词嵌入算法不仅可以提高你模型的速度和内存性能，而且由于Word2vec算法保留的上下文信息，它可能还会提高你模型的准确性。还应注意的是，Word2vec就像n-gram分词一样，是欺骗朴素词袋算法考虑词上下文的一种可能方式，因为Word2vec算法本身使用n-gram来开发嵌入。
- en: While word embedding is used primarily in NLP, the same approach can be used
    in other fields, such as genetics and biochemistry. In those fields, it is sometimes
    advantageous to be able to vectorize sequences of proteins or amino acids such
    that similar structures will have similar vector embedding's.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然词嵌入主要在自然语言处理中使用，但同样的方法也可以用于其他领域，例如遗传学和生物化学。在这些领域中，有时能够将蛋白质或氨基酸序列向量化是有利的，这样相似的结构的向量嵌入也将相似。
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Natural language processing is a rich field of study with many advanced techniques
    and wide applications in ML, computational linguistics, and artificial intelligence.
    In this chapter, however, we focused on the specific tools and tactics that are
    most prevalent in everyday ML tasks.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理是一个研究领域，拥有许多高级技术，并在机器学习、计算语言学和人工智能中有广泛的应用。然而，在本章中，我们专注于在日常工作任务中最普遍使用的特定工具和策略。
- en: The techniques presented in this chapter are building blocks that can be mixed
    and matched in order to achieve many different outcomes. Using the information
    in this chapter alone, you can build a simple full-text search engine, an intent
    extractor for spoken or written commands, an article summarizer, and many other
    impressive tools. However, the most impressive applications of NLP arise when
    these techniques are combined with advanced learning models, such as ANNs and
    RNNs.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中介绍的技术是构建模块，可以混合搭配以实现许多不同的结果。仅使用本章中的信息，你可以构建一个简单的全文搜索引擎，一个用于语音或书面命令的意图提取器，一个文章摘要器，以及许多其他令人印象深刻的工具。然而，当这些技术与高级学习模型（如ANNs和RNNs）结合时，NLP的最令人印象深刻的应用才真正出现。
- en: In particular, you learned about word metrics, such as string distance and TF-IDF
    relevance scoring; preprocessing and dimensionality reduction techniques, such
    as tokenization and stemming; phonetic algorithms, such as the Metaphone algorithm;
    part of speech extraction and phrase parsing; and converting words to vectors
    using word embedding algorithms.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是您学习了关于单词度量，如字符串距离和TF-IDF相关性评分；预处理和降维技术，如分词和词干提取；语音算法，如Metaphone算法；词性提取和短语解析；以及使用词嵌入算法将单词转换为向量。
- en: You have also been introduced, through numerous examples, to two excellent JavaScript
    libraries, `natural.js` and `compromise.js`, which can be used to easily accomplish
    most of the NLP tasks relevant to ML. You were even able to write an article summarizer
    in 20 lines of code!
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 您还通过众多示例介绍了两个优秀的JavaScript库，`natural.js`和`compromise.js`，这些库可以轻松完成与机器学习相关的多数NLP任务。您甚至能用20行代码编写一个文章摘要器！
- en: In the next chapter, we'll discuss how everything you've learned so far can
    be put together in a real-time, user-facing JavaScript application.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论如何将您迄今为止所学的一切整合到一个实时、面向用户的JavaScript应用程序中。
