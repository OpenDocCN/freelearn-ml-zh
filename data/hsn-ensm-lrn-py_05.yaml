- en: Voting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投票
- en: 'The most intuitive of all ensemble learning methods is **majority voting**.
    It is intuitive, as the aim is to output the most popular (or most voted for)
    of the base learner''s predictions. This chapter covers the basic theory as well
    as practical implementations concerning majority voting. By the end of this chapter,
    you will be able to do the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 所有集成学习方法中最直观的就是**多数投票**。它之所以直观，是因为其目标是输出基学习器预测中最流行（或得票最多）的结果。本章将介绍关于多数投票的基本理论及实际实现。通过本章学习后，你将能够做到以下几点：
- en: Understand majority voting
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解多数投票
- en: Understand the difference between hard and soft majority voting and their respective
    strengths and weaknesses
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解硬投票和软投票的区别，以及它们各自的优缺点
- en: Implement both versions in Python
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中实现两种版本
- en: Utilize the voting technique to improve the performance of classifiers on the
    breast cancer dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用投票技术来提高分类器在乳腺癌数据集上的表现
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will require basic knowledge of machine learning techniques and algorithms.
    Furthermore, a knowledge of python conventions and syntax is required. Finally,
    familiarity with the NumPy library will greatly help the reader to understand
    some custom algorithm implementations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要具备基本的机器学习技术和算法知识。此外，还要求了解 Python 语法规范，最后，熟悉 NumPy 库将大大帮助读者理解一些自定义算法的实现。
- en: 'The code files of this chapter can be found on GitHub:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在 GitHub 上找到：
- en: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter03](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter03)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter03](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter03)'
- en: Check out the following video to see the Code in Action: [http://bit.ly/2M52VY7](http://bit.ly/2M52VY7).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频以查看代码实例：[http://bit.ly/2M52VY7](http://bit.ly/2M52VY7)。
- en: Hard and soft voting
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬投票和软投票
- en: 'Majority voting is the simplest ensemble learning technique that allows the
    combination of multiple base learner''s predictions. Similar to how elections
    work, the algorithm assumes that each base learner is a voter and each class is
    a contender. The algorithm takes votes into consideration in order to elect a
    contender as the winner. There are two main approaches to combining multiple predictions
    with voting: one is hard voting and the other is soft voting. We present both
    approaches here.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 多数投票是最简单的集成学习技术，它允许将多个基学习器的预测结果结合起来。类似于选举的工作原理，算法假设每个基学习器是一个选民，每个类别是一个竞争者。算法根据投票情况来选举获胜的竞争者。结合多个预测的投票方法主要有两种：一种是硬投票，另一种是软投票。我们在这里介绍这两种方法。
- en: Hard voting
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬投票
- en: 'Hard voting combines a number of predictions by assuming that the most voted
    class is the winner. In a simple case of two classes and three base learners,
    if a target class has at least two votes, it becomes the ensemble''s output, as
    shown in the following diagram. Implementing a hard voting classifier is as simple
    as counting the votes for each target class:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 硬投票通过假设得票最多的类别为胜者来结合多个预测。在一个简单的两类三基学习者的情况下，如果某个目标类别至少有两个投票，它就成为集成模型的输出，如下图所示。实现一个硬投票分类器就像是统计每个目标类别的投票数一样简单：
- en: '![](img/cf089773-85dd-4f2a-84de-a4c10f12254f.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf089773-85dd-4f2a-84de-a4c10f12254f.png)'
- en: Voting with two classes and three base learners
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用两类和三个基学习者进行投票
- en: For example, let's say that there are three different base learners, who are predicting
    whether a sample belongs to one of three classes with a certain probability (*Table
    1*).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设有三个不同的基学习者，他们在预测一个样本是否属于三个类别中的某一个，并给出相应的概率（*表 1*）。
- en: 'In the following table, each learner predicts the probability that the instance
    belongs to a certain class:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，每个学习者预测实例属于某个类别的概率：
- en: '|  | **Class A** | **Class B** | **Class C** |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|  | **类别 A** | **类别 B** | **类别 C** |'
- en: '| **Learner 1** | 0.5 | 0.3 | 0.2 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **学习者 1** | 0.5 | 0.3 | 0.2 |'
- en: '| **Learner 2** | 0 | 0.48 | 0.52 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **学习者 2** | 0 | 0.48 | 0.52 |'
- en: '| **Learner 3** | 0.4 | 0.3 | 0.3 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| **学习者 3** | 0.4 | 0.3 | 0.3 |'
- en: Assigned class probabilities
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 分配的类别概率
- en: In this example, class A has two votes, while class C has only one. According
    to hard voting, class A will be the prediction of the ensemble. It's a fairly
    robust method of combining many base learners, although it doesn't take into account
    that some classes may be chosen by a base learner only because they are marginally
    better than the others.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，类别 A 有两个投票，而类别 C 只有一个投票。根据硬投票，类别 A 将成为集成的预测结果。这是一种非常稳健的基学习者合并方法，尽管它没有考虑到某些类别可能仅因略微优于其他类别而被某个基学习者选择。
- en: Soft voting
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软投票
- en: 'Soft voting takes into account the probability of the predicted classes. In
    order to combine the predictions, soft voting calculates the average probability
    of each class and assumes that the winner is the class with the highest average
    probability.In the simple case of three base learners and two classes, we must
    take into consideration the predicted probability for each class and average them
    across the three learners:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 软投票考虑了预测类别的概率。为了合并预测，软投票计算每个类别的平均概率，并假设胜者是具有最高平均概率的类别。在三个基学习者和两个类别的简单情况下，我们必须考虑每个类别的预测概率，并在三个学习者中求其平均：
- en: '![](img/9a69ad7d-0f1d-4116-abd2-572423922f69.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a69ad7d-0f1d-4116-abd2-572423922f69.png)'
- en: Soft voting with two classes and three base learners
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 软投票：两个类别和三个基学习者
- en: Using our previous example, and by taking the average of each column for *Table
    1*, we can expand it, adding a row for the average probability.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们之前的例子，并通过对*表 1*中每一列的平均值求平均，我们可以扩展该表，添加一行用于显示平均概率。
- en: 'The following table shows the predicted probabilities for each class by each
    learner, as well as the average probability:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了每个学习者对每个类别的预测概率，以及平均概率：
- en: '|  | **Class A** | **Class B** | **Class C** |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | **类别 A** | **类别 B** | **类别 C** |'
- en: '| **Learner 1** | 0.5 | 0.3 | 0.2 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| **学习者 1** | 0.5 | 0.3 | 0.2 |'
- en: '| **Learner 2** | 0 | 0.48 | 0.52 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| **学习者 2** | 0 | 0.48 | 0.52 |'
- en: '| **Learner 3** | 0.4 | 0.3 | 0.3 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **学习者 3** | 0.4 | 0.3 | 0.3 |'
- en: '| **Average** | 0.3 | 0.36 | 0.34 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| **平均** | 0.3 | 0.36 | 0.34 |'
- en: Predicted probabilities for each class by each learner, as well as the average
    probability
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 每个学习者对每个类别的预测概率，以及平均概率
- en: As we can see, class A has an average probability of 0.3, class B has an average
    probability of 0.36, and class C has an average probability of 0.34, making class
    B the winner. Note that class B is not selected by any base learner as the predicted
    class, but by combining the predicted probabilities, class B arises as the best
    compromise between the predictions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，类别 A 的平均概率为 0.3，类别 B 的平均概率为 0.36，类别 C 的平均概率为 0.34，因此类别 B 获胜。注意，类别 B 并不是由任何基学习者选作预测类别，但通过合并预测概率，类别
    B 成为预测中最好的折中选择。
- en: In order for soft voting to be more effective than hard voting, the base classifiers
    must produce good estimates regarding the probability that a sample belongs to
    a specific class. If the probabilities are meaningless (for example, if they are
    always 100% for one class and 0% for all others), soft voting could be even worse
    than hard voting.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让软投票比硬投票更有效，基分类器必须提供关于样本属于特定类别的概率的良好估计。如果这些概率没有意义（例如，如果它们总是对于某一类别为 100%，而对于所有其他类别为
    0%），那么软投票可能会比硬投票更糟糕。
- en: 'A note on voting: it is impossible to have a perfect voting system, as has
    been proved by Dr. Kenneth Arrow with his impossibility theorem. Nonetheless,
    certain types of voting systems can better reflect the preferences of a population.
    Soft voting better reflects the individual learner''s preferences, as it takes
    into account the rating (probabilities) instead of the ranking (predicted class).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 关于投票的说明：正如 Kenneth Arrow 博士通过其不可能性定理所证明的那样，完美的投票系统是不可实现的。然而，某些类型的投票系统能够更好地反映一个群体的偏好。软投票更能反映个体学习者的偏好，因为它考虑的是评分（概率），而不是排名（预测类别）。
- en: For more on the impossibility theorem, see A difficulty in the concept of social
    welfare. *Arrow, K.J., 1950*. *Journal of political economy*, 58(4), pp.328-346.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有关不可能性定理的更多内容，请参见《社会福利概念中的困难》*Arrow, K.J., 1950*。*政治经济学杂志*，58(4)，第328-346页。
- en: ​Python implementation
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ​Python 实现
- en: 'The simplest way to implement hard voting in Python is to use scikit-learn to
    create base learners, train them on some data, and combine their predictions on
    test data. In order to do so, we will go through the following steps:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中实现硬投票的最简单方法是使用*scikit-learn*来创建基学习者，训练它们以适应某些数据，并将它们的预测结果结合起来应用于测试数据。为此，我们将按照以下步骤进行：
- en: Load the data and split it into train and test sets
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据并将其拆分为训练集和测试集
- en: Create some base learners
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一些基础学习器
- en: Train them on the train data
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据上训练它们
- en: Produce predictions for the test data
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为测试数据生成预测
- en: Combine predictions using hard voting
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用硬投票合并预测结果
- en: Compare the individual learner's predictions as well as the combined predictions
    with the ground truth (actual correct classes)
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将各个学习器的预测结果与合并后的预测结果与实际的正确类别（ground truth）进行比较
- en: Although scikit-learn has implementations for voting, by creating a custom implementation,
    it will be easier to understand how the algorithm works. Furthermore, it will
    enable us to better understand how to process and analyze a base learner's outputs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管scikit-learn提供了投票的实现，通过创建自定义实现，我们可以更容易理解算法的工作原理。此外，这还将帮助我们更好地理解如何处理和分析基础学习器的输出。
- en: Custom hard voting implementation
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义硬投票实现
- en: 'In order to implement a custom hard voting solution, we will use three base
    learners: a **Perceptron** (a neural network with a single neuron), a **Support
    Vector Machine** (**SVM**), and a **Nearest Neighbor**. These are contained in
    the `sklearn.linear_model`, `sklearn.svm`, and `sklearn.neighbors` packages. Furthermore,
    we will use the `argmax` function from NumPy. This function returns the index
    of an array''s (or array-like data structure) element with the highest value.
    Finally, `accuracy_score` will calculate the accuracy of each classifier on our
    test data:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现自定义硬投票解决方案，我们将使用三个基础学习器：**感知机**（一个单神经元的神经网络）、**支持向量机**（**SVM**）和**最近邻**。它们分别包含在`sklearn.linear_model`、`sklearn.svm`和`sklearn.neighbors`包中。此外，我们将使用NumPy的`argmax`函数。此函数返回数组（或类数组数据结构）中最大值元素的索引。最后，`accuracy_score`将计算每个分类器在我们测试数据上的准确性：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then instantiate our base learners. We hand-picked their hyperparameters
    to ensure that they are diverse in order to produce a well-performing ensemble.
    As `breast_cancer` is a classification dataset, we use `SVC`, the classification
    version of SVM, along with `KNeighborsClassifier` and `Perceptron`. Furthermore,
    we set the random state of `Perceptron` to 0 in order to ensure the reproducibility
    of our example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们实例化我们的基础学习器。我们精心挑选了它们的超参数，以确保它们在多样性上有所体现，从而能够产生一个表现良好的集成模型。由于`breast_cancer`是一个分类数据集，我们使用`SVC`，即SVM的分类版本，以及`KNeighborsClassifier`和`Perceptron`。此外，我们将`Perceptron`的随机状态设置为0，以确保示例的可复现性：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We split the data into train and test sets, using 100 instances for our test
    set and train our base learners on the train set:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据拆分为训练集和测试集，使用100个实例作为测试集，并在训练集上训练我们的基础学习器：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'By storing each base learner''s prediction in `predictions_1`, `predictions_2`,
    and `predictions_3`, we can further analyze and combine them into our ensemble.
    Note that we trained each classifier individually; additionally, as well as that
    each classifier produces predictions for the test data autonomously. As mentioned
    in [Chapter 2](d7921006-351e-4c21-ab54-f1dc834557dc.xhtml), *Getting Started with
    Ensemble Learning*, this is the main characteristic of non-generative ensemble
    methods:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将每个基础学习器的预测存储在`predictions_1`、`predictions_2`和`predictions_3`中，我们可以进一步分析并将它们合并成我们的集成模型。请注意，我们分别训练了每个分类器；此外，每个分类器都会独立地对测试数据进行预测。正如在[第二章](d7921006-351e-4c21-ab54-f1dc834557dc.xhtml)《集成学习入门》中提到的那样，*这是非生成性集成方法的主要特点*：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Following the predictions, we combine the predictions of each base learner
    for each test instance. The `hard_predictions` list will contain the ensemble''s
    predictions (output). By iterating over every test sample with `for i in range(test_samples)`,
    we count the total number of votes that each class has received from the three
    base learners. As the dataset contains only two classes, we need a list of two
    elements: `counts = [0 for _ in range(2)]`. In `# --- SECTION 3 ---`, we stored
    each base learner''s predictions in an array. Each one of those array''s elements
    contains the index of the instance''s predicted class (in our case, 0 and 1).
    Thus, we increase the corresponding element''s value in `counts[predictions_1[i]]` by
    one to count the base learner''s vote. Then, `argmax(counts)` returns the element
    (class) with the highest number of votes:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 根据预测结果，我们将每个基学习器对每个测试实例的预测结果进行合并。`hard_predictions` 列表将包含集成模型的预测结果（输出）。通过 `for
    i in range(test_samples)` 遍历每个测试样本，我们统计每个类别从三个基学习器收到的投票总数。由于数据集仅包含两个类别，我们需要一个包含两个元素的列表：`counts
    = [0 for _ in range(2)]`。在 `# --- SECTION 3 ---` 中，我们将每个基学习器的预测结果存储在一个数组中。该数组的每个元素包含实例预测类别的索引（在我们这里是
    0 和 1）。因此，我们通过将 `counts[predictions_1[i]]` 中相应元素的值加 1 来统计基学习器的投票数。接着，`argmax(counts)`
    会返回获得最多投票的元素（类别）：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we calculate the accuracy of the individual base learners as well
    as the ensemble with `accuracy_score`, and print them on screen:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过 `accuracy_score` 计算每个基学习器以及集成模型的准确度，并将结果打印在屏幕上：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The final output is as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出如下：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Analyzing our results using Python
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Python 分析我们的结果
- en: The final accuracy achieved is 1% better than the best of the three classifiers
    (the **k-Nearest Neighbors** (**k-NN**) classifier). We can visualize the learner's
    errors in order to examine why the ensemble performs in this specific way.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的准确度比三种分类器中最好的分类器（**k-最近邻** (**k-NN**) 分类器）高出 1%。我们可以通过可视化学习器的错误来分析集成模型为何以这种特定方式表现。
- en: 'First, we `import matplotlib` and use a specific `seaborn-paper` plotting style
    with `mpl.style.use(''seaborn-paper'')`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们 `import matplotlib` 并使用特定的 `seaborn-paper` 绘图风格，方法是 `mpl.style.use('seaborn-paper')`：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we calculate the errors by subtracting our prediction from the actual
    target. Thus, we get a -1 each time the learner predicts a positive (1) when the
    true class is negative (0), and a 1 when it predicts a negative (0) while the
    true class is positive (1). If the prediction is correct, we get a zero (0):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过从预测结果中减去实际目标值来计算错误。因此，每次学习器预测为正类（1），而真实类别为负类（0）时，我们得到 -1；每次学习器预测为负类（0），而真实类别为正类（1）时，我们得到
    1。如果预测正确，我们得到 0：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For each base learner, we plot the instances where they have predicted the wrong
    class. Our aim is to scatter plot the `x` and `y` lists. These lists will contain
    the instance number (the `x` list) and the type of error (the `y` list). With `plt.scatter`,
    we can specify the coordinates of our points using the aforementioned lists, as
    well as specify how these points are depicted. This is important in order to ensure
    that we can simultaneously visualize all the errors of the classifiers as well
    as the relationship between them.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个基学习器，我们绘制其预测错误的实例。我们的目标是绘制 `x` 和 `y` 列表的散点图。这些列表将包含实例编号（`x` 列表）和错误类型（`y`
    列表）。通过 `plt.scatter`，我们可以使用上述列表来指定点的坐标，并且可以指定这些点的表现方式。这一点非常重要，因为我们可以同时可视化所有分类器的错误及其相互关系。
- en: 'The default shape for each point is a circle. By specifying the `marker` parameter,
    we can alter this shape. Furthermore, with the `s` parameter, we can specify the
    marker''s size. Thus, the first learner (k-NN) will have a round shape of size
    120, the second learner (Perceptron) will have an `x` shape of size 60, and the
    third learner (SVM) will have a round shape of size 20\. The `if not errors_*[i]
    == 0` guard ensures that we will not store correctly classified instances:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 每个点的默认形状是圆形。通过指定 `marker` 参数，我们可以改变这个形状。此外，通过 `s` 参数，我们可以指定标记的大小。因此，第一个学习器（k-NN）将具有大小为
    120 的圆形，第二个学习器（感知器）将具有大小为 60 的 `x` 形状，而第三个学习器（SVM）将具有大小为 20 的圆形。`if not errors_*[i]
    == 0` 的保护条件确保我们不会存储正确分类的实例：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, we specify the figure''s title and labels, and plot the legend:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们指定图表的标题和标签，并绘制图例：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As the following shows, there are five samples where at least two learners
    predict the wrong class. These are the 5 cases out of the 100 that the ensemble
    predicts wrong, as the most voted class is wrong, thus producing a 95% accuracy.
    In all other cases, two out of three learners predict the correct class, thus
    the ensemble predicts the correct class as it is the most voted:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，有五个样本至少有两个学习器预测了错误的类别。这五个案例是100个样本中集成预测错误的5个，因为最投票的类别是错的，从而导致95%的准确性。在所有其他情况下，三个学习器中有两个预测了正确的类别，因此集成模型预测了正确的类别，因为它是最投票的：
- en: '![](img/0ba4d397-fc7a-43f0-8009-ef44003bcf9a.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ba4d397-fc7a-43f0-8009-ef44003bcf9a.png)'
- en: Learner errors on the test set
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 学习器在测试集上的错误
- en: Using scikit-learn
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn
- en: The scikit-learn library includes many ensemble learning algorithms, including
    voting. In order to implement hard voting, we will follow the same procedure as
    we did previously, except this time, we will not implement the individual fitting,
    predicting, and voting ourselves. Instead, we will use the provided implementation,
    which enables quick and easy training and testing.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn库包含许多集成学习算法，包括投票。为了实现硬投票，我们将遵循与之前相同的程序，不过这次我们不再自己实现个别的拟合、预测和投票过程。而是使用提供的实现，这使得训练和测试变得快速而简单。
- en: Hard voting implementation
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬投票实现
- en: 'Similarly to our custom implementation, we import the required libraries, split
    our train and test data, and instantiate our base learners. Furthermore, we import
    scikit-learn''s `VotingClassifier` voting implementation from the `sklearn.ensemble` package,
    as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们自定义实现类似，我们导入所需的库，划分训练和测试数据，并实例化我们的基础学习器。此外，我们从`sklearn.ensemble`包中导入scikit-learn的`VotingClassifier`投票实现，如下所示：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Following the above code, we instantiate the `VotingClassifier` class, passing
    as a parameter a list of tuples with the names and objects of our base classifiers.
    Note that passing the parameters outside of a list will result in an error:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码之后，我们实例化`VotingClassifier`类，传入一个包含基础分类器名称和对象的元组列表作为参数。请注意，如果将参数传递在列表外部，将会导致错误：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, having instantiated the classifier, we can use it in the same way as any
    other classifier, without having to tend to each base learner individually. The
    following two sections execute the fitting and prediction for all base learners
    as well as the calculation of the most voted class for each test instance:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，实例化了分类器后，我们可以像使用任何其他分类器一样使用它，而无需单独处理每个基础学习器。接下来的两部分执行了所有基础学习器的拟合和预测，以及为每个测试实例计算最投票的类别：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we can print the accuracy of the ensemble:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以打印集成模型的准确性：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This is the same as our custom implementation:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们自定义实现相同：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that `VotingClassifier` will not fit the objects that you pass as parameters,
    but will, instead, clone them and fit the cloned objects. Thus, if you try to
    print the accuracy of each individual base learner on the test set, you will get `NotFittedError`,
    as the objects that you have access to are, in fact, not fitted. This is the only
    drawback of using scikit-learn's implementation over a custom one.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`VotingClassifier`不会拟合作为参数传入的对象，而是会克隆它们并拟合克隆的对象。因此，如果你尝试打印每个基础学习器在测试集上的准确性，你将得到`NotFittedError`，因为你访问的对象实际上并没有被拟合。这是使用scikit-learn的实现而非自定义实现的唯一缺点。
- en: Soft voting implementation
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软投票实现
- en: Scikit-learn's implementation allows for soft voting as well. The only requirement
    is that the base learners implement the `predict_proba` function. In our example,
    `Perceptron` does not implement the function at all, while `SVC` only produces
    probabilities when it is passed the `probability=True` argument. Having these
    limitations in mind, we swap our `Perceptron` with a Naive Bayes classifier implemented
    in the `sklearn.naive_bayes` package.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn的实现也支持软投票。唯一的要求是基础学习器必须实现`predict_proba`函数。在我们的示例中，`Perceptron`完全没有实现该函数，而`SVC`仅在传递`probability=True`参数时才会生成概率。考虑到这些限制，我们将`Perceptron`替换为`sklearn.naive_bayes`包中实现的朴素贝叶斯分类器。
- en: 'To actually use soft voting, the `VotingClassifier` object must be initialized
    with the `voting=''soft''` argument. Except for the changes mentioned here, the
    majority of the code remains the same. Load the libraries and datasets, and produce
    a train/test split as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要实际使用软投票，`VotingClassifier`对象必须使用`voting='soft'`参数进行初始化。除了这里提到的更改外，大部分代码保持不变。加载库和数据集，并按如下方式进行训练/测试集划分：
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Instantiate the base learners and voting classifier. We use a Gaussian Naive
    Bayes implemented as `GaussianNB`. Note that we use `probability=True` in order
    for the `GaussianNB` object to be able to produce probabilities:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化基学习器和投票分类器。我们使用一个高斯朴素贝叶斯分类器，命名为`GaussianNB`。注意，我们使用`probability=True`，以便`GaussianNB`对象能够生成概率：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We fit both `VotingClassifier` and the individual learners. We want to analyze
    our results, and, as mentioned earlier, the classifier will not fit the objects
    that we pass as arguments, but will instead clone them. Thus, we have to manually
    fit our learners as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拟合`VotingClassifier`和单独的学习器。我们希望分析我们的结果，正如前面提到的，分类器不会拟合我们传入的对象，而是会克隆它们。因此，我们需要手动拟合我们的学习器，如下所示：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We predict the test set''s targets using both the voting ensemble and the individual
    learners:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用投票集成和单独的学习器预测测试集的目标：
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we print the accuracy of each base learner and the soft voting ensemble''s
    accuracy:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们打印每个基学习器的准确率以及软投票集成的准确率：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The final output is as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出如下：
- en: '[PRE21]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Analyzing our results
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析我们的结果
- en: As is evident, the accuracy achieved by soft voting is 2% worse than the best
    learner and on par with the second-best learner. We would like to analyze our
    results similarly to how we analyzed the performance of our hard voting custom
    implementation. But as soft voting takes into account the predicted class probabilities,
    we cannot use the same approach. Instead, we will plot the predicted probability
    for each instance to be classified as positive by each base learner as well as
    the average probability of the ensemble.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，软投票的准确率比最佳学习器低2%，并与第二最佳学习器持平。我们希望像分析硬投票自定义实现的性能一样分析我们的结果。但由于软投票考虑了预测的类别概率，我们不能使用相同的方法。相反，我们将绘制每个基学习器预测的每个实例作为正类的概率，以及集成模型的平均概率。
- en: 'Again, we `import matplotlib` and set the plotting style:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们`import matplotlib`并设置绘图样式：
- en: '[PRE22]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We calculate the ensemble''s errors with `errors = y_test-hard_predictions`
    and get the predicted probabilities of each base learner with the `predict_proba(x_test)` function.
    All base learners implement this function, as it is a requirement for utilizing
    them in a soft voting ensemble:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过`errors = y_test-hard_predictions`计算集成模型的误差，并使用`predict_proba(x_test)`函数获取每个基学习器的预测概率。所有基学习器都实现了这个函数，因为这是在软投票集成中使用它们的要求：
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Following this, for each wrongly classified instance, we store the predicted
    probability that the instance belongs to in class 0\. We also implement this for
    each base learner, as well as their average. Each `probabilities_*` array,  is
    a two-dimensional array. Each row contains the predicted probability that the
    corresponding instance belongs to class 0 or class 1\. Thus, storing one of the
    two is sufficient. In the case of a dataset with *N* classes, we would have to
    store at least *N*-1 probabilities in order to get a clear picture:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，对于每个错误分类的实例，我们存储该实例属于类0的预测概率。我们也对每个基学习器以及它们的平均值实现此功能。每个`probabilities_*`数组是一个二维数组，每行包含对应实例属于类0或类1的预测概率。因此，存储其中一个就足够了。如果数据集有*N*个类别，我们至少需要存储*N*-1个概率，才能获得清晰的视图：
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, we plot the probabilities as bars of different widths with `plt.bar`.
    This ensures that any overlapping bars will still be visible. The third `plt.bar`
    argument dictates the bar''s width. We scatter plot the average probability as
    a black ''X'' and ensure that it will be plotted over any bar with `zorder=10`.
    Finally, we plot a threshold line at 0.5 probability with `plt.plot(y, c=''k'',
    linestyle=''--'')`, ensuring that it will be a black dotted line with `c=''k'',
    linestyle=''--''`. If the average probability is above the line, the sample is
    classified as positive, as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`plt.bar`将概率绘制为不同宽度的条形图。这确保了任何重叠的条形图仍然可以被看到。第三个`plt.bar`参数决定了条形图的宽度。我们使用散点图标记平均概率为黑色“X”，并确保它绘制在任何条形图之上，使用`zorder=10`。最后，我们绘制一条在0.5概率处的阈值线，使用`plt.plot(y,
    c='k', linestyle='--')`，确保它为黑色虚线，`c='k', linestyle='--'`。如果平均概率高于该线，样本将被分类为正类，如下所示：
- en: '[PRE25]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding code outputs the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出如下：
- en: '![](img/557d63bc-fadb-42d6-bdea-ace7257b02da.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/557d63bc-fadb-42d6-bdea-ace7257b02da.png)'
- en: Predicted and average probabilities for the test set
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集的预测和平均概率
- en: 'As we can see, only two samples have an extreme average probability (sample
    22 with p = 0.98 and 67 with p = 0.001). The other four are quite close to 50%.
    For three out of these four samples, SVM seems to assign a very high probability
    to the wrong class, thus greatly affecting the average probability. If SVM did
    not overestimate the probability of these samples as much, the ensemble could
    well out perform each individual learner. For the two extreme cases, nothing can
    be done, as all three learners agree on the miss classification. We can try to
    swap our SVM for another k-NN with a significantly higher number of neighbors.
    In this case, `(learner_3 = neighbors.KNeighborsClassifier(n_neighbors=50) )`,
    we can see that the ensemble''s accuracy is greatly increased. The ensemble''s
    accuracies and errors are as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，只有两个样本具有极端的平均概率（样本22的p = 0.98和样本67的p = 0.001）。其余四个样本的概率接近50%。在这四个样本中，有三个样本SVM似乎给出了一个错误类别的极高概率，从而大大影响了平均概率。如果SVM没有对这些样本的概率进行如此高估，集成模型可能会比每个单独的学习器表现得更好。对于这两个极端情况，无法采取任何措施，因为所有三个学习器都一致地将其分类错误。我们可以尝试用另一个邻居数显著更多的k-NN替换SVM。在这种情况下，`(learner_3
    = neighbors.KNeighborsClassifier(n_neighbors=50))`，我们可以看到集成模型的准确率大幅提高。集成模型的准确率和错误如下：
- en: '[PRE26]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Take a look at the following screenshot:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下截图：
- en: '![](img/f7a6f54d-3272-4c1a-bfc4-430a293f2a76.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f7a6f54d-3272-4c1a-bfc4-430a293f2a76.png)'
- en: Predicted and average probabilities for the test set with two k-NNs
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用两个k-NN的测试集的预测值和平均概率
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we presented the most basic ensemble learning method: voting.
    Although it is quite simple, it can prove to be effective and an easy way to combine
    many machine learning models. We presented hard and soft voting, a custom implementation
    for hard voting, and scikit-learn implementations for both hard and soft voting.
    Finally, we presented a way to analyze the ensemble''s performance by plotting
    each base learner''s errors using `matplotlib`. The chapter''s key points are
    summarized below.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了最基本的集成学习方法：投票法。虽然它相当简单，但它可以证明是有效的，并且是结合多个机器学习模型的一种简便方法。我们介绍了硬投票和软投票、硬投票的自定义实现，以及scikit-learn中硬投票和软投票的实现。最后，我们展示了通过使用`matplotlib`绘制每个基学习器的错误来分析集成模型性能的方法。以下是本章的关键点总结。
- en: '**Hard voting** assumes that the most voted class is the winner. **Soft voting**
    assumes that the class with the highest average probability is the winner. **Soft
    voting** requires that the base classifiers predict the **probability** of each
    class for every instance with a relatively high accuracy. Scikit-learn implements
    voting ensembles using the `VotingClassifier` class. An array of tuples in the
    form of `[(learner_name, learner_object),…]` is passed to `VotingClassifier`.
    The `VotingClassifier` does not train the objects passed as arguments. Instead,
    a copy is generated and trained. The default mode of `VotingClassifier `implements
    hard voting. To use soft voting, pass the `voting=''soft''` argument to the constructor. Soft
    voting requires that the base learners return probabilities for each prediction. If
    a base learner greatly takes over or underestimates the probabilities, the ensemble''s
    predictive ability will suffer.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**硬投票**假设得票最多的类别是赢家。**软投票**假设具有最高平均概率的类别是赢家。**软投票**要求基学习器以较高的准确度预测每个实例的每个类别的**概率**。Scikit-learn通过`VotingClassifier`类实现投票集成。一个元组数组，格式为`[(learner_name,
    learner_object), …]`，被传递给`VotingClassifier`。`VotingClassifier`并不直接训练作为参数传递的对象，而是生成并训练一个副本。`VotingClassifier`的默认模式实现硬投票。要使用软投票，可以将`voting=''soft''`参数传递给构造函数。软投票要求基学习器返回每个预测的概率。如果基学习器大幅高估或低估了概率，集成模型的预测能力将受到影响。'
- en: In the next chapter, we will discuss about another non-generative method, Stacking,
    and how it can be utilized in both regression and classification problems.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论另一种非生成方法——堆叠法（Stacking），以及它如何应用于回归和分类问题。
