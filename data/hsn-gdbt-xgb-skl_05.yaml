- en: '*Chapter 4*: From Gradient Boosting to XGBoost'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*：从梯度提升到XGBoost'
- en: XGBoost is a unique form of gradient boosting with several distinct advantages,
    which will be explained in [*Chapter 5*](B15551_05_Final_NM_ePUB.xhtml#_idTextAnchor117),
    *XGBoost Unveiled*. In order to understand the advantages of XGBoost over traditional
    gradient boosting, you must first learn how traditional gradient boosting works.
    The general structure and hyperparameters of traditional gradient boosting are
    incorporated by XGBoost. In this chapter, you will discover the power behind gradient
    boosting, which is at the core of XGBoost.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost是一种独特的梯度提升形式，具有多个显著的优势，这些优势将在[*第5章*](B15551_05_Final_NM_ePUB.xhtml#_idTextAnchor117)，《XGBoost揭示》中进行解释。为了理解XGBoost相较于传统梯度提升的优势，您必须首先了解传统梯度提升是如何工作的。XGBoost融入了传统梯度提升的结构和超参数。在本章中，您将发现梯度提升的强大能力，而这正是XGBoost的核心所在。
- en: In this chapter, you will build gradient boosting models from scratch before
    comparing gradient boosting models and errors with previous results. In particular,
    you will focus on the **learning rate** hyperparameter to build powerful gradient
    boosting models that include XGBoost. Finally, you will preview a case study on
    exoplanets highlighting the need for faster algorithms, a critical need in the
    world of big data that is satisfied by XGBoost.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将从零开始构建梯度提升模型，并与之前的结果对比梯度提升模型和错误。特别地，您将专注于**学习率**超参数，构建强大的梯度提升模型，其中包括XGBoost。最后，您将预览一个关于外行星的案例研究，强调对更快算法的需求，这种需求在大数据领域中至关重要，而XGBoost正好满足了这一需求。
- en: 'In this chapter, we will be covering the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主要主题：
- en: From bagging to boosting
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从袋装法到提升法
- en: How gradient boosting works
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升的工作原理
- en: Modifying gradient boosting hyperparameters
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改梯度提升的超参数
- en: Approaching big data – gradient boosting versus XGBoost
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面对大数据的挑战——梯度提升与XGBoost的对比
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code for this chapter is available at [https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter04](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter04).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在[https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter04](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter04)找到。
- en: From bagging to boosting
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从袋装法到提升法
- en: In [*Chapter 3*](B15551_03_Final_NM_ePUB.xhtml#_idTextAnchor070), *Bagging with
    Random Forests*, you learned why ensemble machine learning algorithms such as
    random forests make better predictions by combining many machine learning models
    into one. Random forests are classified as bagging algorithms because they take
    the aggregates of bootstrapped samples (decision trees).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第3章*](B15551_03_Final_NM_ePUB.xhtml#_idTextAnchor070)，《随机森林的袋装法》中，您学习了为什么像随机森林这样的集成机器学习算法通过将多个机器学习模型结合成一个，从而做出更好的预测。随机森林被归类为袋装算法，因为它们使用自助法样本的聚合（决策树）。
- en: Boosting, by contrast, learns from the mistakes of individual trees. The general
    idea is to adjust new trees based on the errors of previous trees.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，提升方法通过学习每棵树的错误来进行优化。其一般思路是基于前一棵树的错误来调整新树。
- en: In boosting, correcting errors for each new tree is a distinct approach from
    bagging. In a bagging model, new trees pay no attention to previous trees. Also,
    new trees are built from scratch using bootstrapping, and the final model aggregates
    all individual trees. In boosting, however, each new tree is built from the previous
    tree. The trees do not operate in isolation; instead, they are built on top of
    one another.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在提升方法（boosting）中，每棵新树的错误修正是与袋装法（bagging）不同的。在袋装模型中，新树不会关注之前的树。此外，新树是通过自助法（bootstrapping）从零开始构建的，最终的模型将所有单独的树进行聚合。然而，在提升方法中，每棵新树都是基于前一棵树构建的。这些树并不是孤立运作的，而是相互叠加构建的。
- en: Introducing AdaBoost
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍AdaBoost
- en: '**AdaBoost** is one of the earliest and most popular boosting models. In AdaBoost,
    each new tree adjusts its weights based on the errors of the previous trees. More
    attention is paid to predictions that went wrong by adjusting weights that affect
    those samples at a higher percentage. By learning from its mistakes, AdaBoost
    can transform weak learners into strong learners. A weak learner is a machine
    learning algorithm that barely performs better than chance. By contrast, a stronger
    learner has learned a considerable amount from data and performs quite well.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**AdaBoost**是最早且最受欢迎的提升模型之一。在AdaBoost中，每一棵新树都会根据前一棵树的错误来调整权重。通过调整权重，更多地关注预测错误的样本，给予这些样本更高的权重。通过从错误中学习，AdaBoost能够将弱学习者转变为强学习者。弱学习者是指那些表现几乎与随机猜测一样的机器学习算法。相比之下，强学习者则从数据中学习到了大量信息，表现优异。'
- en: The general idea behind boosting algorithms is to transform weak learners into
    strong learners. A weak learner is hardly better than random guessing. But there
    is a purpose behind the weak start. Building on this general idea, boosting works
    by focusing on iterative error correction, *not* by establishing a strong baseline
    model. If the base model is too strong, the learning process is necessarily limited,
    thereby undermining the general strategy behind boosting models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 提升算法背后的基本思想是将弱学习者转变为强学习者。弱学习者几乎不比随机猜测更好。但这种弱开始是有目的的。基于这一基本思想，提升通过集中精力进行迭代的错误修正来工作，*而不是*建立一个强大的基准模型。如果基准模型太强，学习过程就会受到限制，从而削弱提升模型背后的整体策略。
- en: Weak learners are transformed into strong learners through hundreds of iterations.
    In this sense, a small edge goes a long way. In fact, boosting has been one of
    the best general machine learning strategies in terms of producing optimal results
    for the past couple of decades.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过数百次迭代，弱学习者被转化为强学习者。从这个意义上来说，微小的优势可以带来很大不同。事实上，在过去几十年里，提升方法一直是产生最佳结果的机器学习策略之一。
- en: A detailed study of AdaBoost is beyond the scope of this book. Like many scikit-learn
    models, it's straightforward to implement AdaBoost in practice. The `AdaBoostRegressor`
    and `AdaBoostClassifier` algorithms may be downloaded from the `sklearn.ensemble`
    library and fit to any training set. The most important AdaBoost hyperparameter
    is `n_estimators`, the number of trees (iterations) required to create a strong
    learner.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不深入研究AdaBoost的细节。像许多scikit-learn模型一样，在实践中实现AdaBoost非常简单。`AdaBoostRegressor`和`AdaBoostClassifier`算法可以从`sklearn.ensemble`库中下载，并适用于任何训练集。最重要的AdaBoost超参数是`n_estimators`，即创建强学习者所需的树的数量（迭代次数）。
- en: Note
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For further information on AdaBoost, check out the official documentation at
    [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)
    for classifiers and [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html)
    for regressors.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有关AdaBoost的更多信息，请查阅官方文档：[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)（分类器）和[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html)（回归器）。
- en: We will now move on to gradient boosting, a strong alternative to AdaBoost with
    a slight edge in performance.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将介绍梯度提升，它是AdaBoost的强有力替代方案，在性能上略有优势。
- en: Distinguishing gradient boosting
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 区分梯度提升
- en: 'Gradient boosting uses a different approach than AdaBoost. While gradient boosting
    also adjusts based on incorrect predictions, it takes this idea one step further:
    gradient boosting fits each new tree entirely based on the errors of the previous
    tree''s predictions. That is, for each new tree, gradient boosting looks at the
    mistakes and then builds a new tree completely around these mistakes. The new
    tree doesn''t care about the predictions that are already correct.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升采用与AdaBoost不同的方法。虽然梯度提升也基于错误的预测进行调整，但它更进一步：梯度提升根据前一棵树的预测错误完全拟合每一棵新树。也就是说，对于每一棵新树，梯度提升首先查看错误，然后围绕这些错误完全构建一棵新树。新树不会关心那些已经正确的预测。
- en: 'Building a machine learning algorithm that solely focuses on the errors requires
    a comprehensive method that sums errors to make accurate final predictions. This
    method leverages residuals, the difference between the model''s predictions and
    actual values. Here is the general idea:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个仅关注错误的机器学习算法需要一种全面的方法，累加错误以做出准确的最终预测。这种方法利用了残差，即模型预测值与实际值之间的差异。其基本思想如下：
- en: '*Gradient boosting computes the residuals of each tree''s predictions and sums
    all the residuals to score the model.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*梯度提升计算每棵树预测的残差，并将所有残差加总来评估模型。*'
- en: It's essential to understand **computing** and **summing residuals** as this
    idea is at the core of XGBoost, an advanced version of gradient boosting. When
    you build your own version of gradient boosting, the process of computing and
    summing residuals will become clear. In the next section, you will build your
    own version of a gradient boosting model. First, let's learn in detail how gradient
    boosting works.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 理解**计算**和**累加残差**至关重要，因为这个思想是XGBoost（梯度提升的高级版本）核心原理之一。当你构建自己的梯度提升版本时，计算和累加残差的过程会变得更加清晰。在下一节中，你将构建自己的梯度提升模型。首先，让我们详细了解梯度提升的工作原理。
- en: How gradient boosting works
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升的工作原理
- en: In this section, we will look under the hood of gradient boosting and build
    a gradient boosting model from scratch by training new trees on the errors of
    the previous trees. The key mathematical idea here is the residual. Next, we will
    obtain the same results using scikit-learn's gradient boosting algorithm.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入了解梯度提升的内部原理，通过对前一棵树的错误训练新树，从零开始构建一个梯度提升模型。这里的核心数学思想是残差。接下来，我们将使用scikit-learn的梯度提升算法获得相同的结果。
- en: Residuals
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 残差
- en: The residuals are the difference between the errors and the predictions of a
    given model. In statistics, residuals are commonly analyzed to determine how good
    a given linear regression model fits the data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 残差是给定模型的预测与实际值之间的差异。在统计学中，残差常常被分析，以判断线性回归模型与数据的拟合程度。
- en: 'Consider the following examples:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下示例：
- en: Bike rentals
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自行车租赁
- en: 'a) *Prediction*: 759'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'a) *预测*: 759'
- en: 'b) *Result*: 799'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'b) *结果*: 799'
- en: 'c) *Residual*: 799 - 759 = 40'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'c) *残差*: 799 - 759 = 40'
- en: Income
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收入
- en: 'a) *Prediction*: 100,000'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'a) *预测*: 100,000'
- en: 'b) *Result*: 88,000'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'b) *结果*: 88,000'
- en: 'c) *Residual*: 88,000 –100,000 = -12,000'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'c) *残差*: 88,000 – 100,000 = -12,000'
- en: As you can see, residuals tell you how far the model's predictions are from
    reality, and they may be positive or negative.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，残差告诉你模型的预测与实际之间的偏差，残差可能是正的，也可能是负的。
- en: 'Here is a visual example displaying the residuals of a **linear regression**
    line:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个展示**线性回归**线的残差的可视化示例：
- en: '![Figure 4.1 – Residuals of a linear regression line](img/B15551_04_01.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – 线性回归线的残差](img/B15551_04_01.jpg)'
- en: Figure 4.1 – Residuals of a linear regression line
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 线性回归线的残差
- en: The goal of linear regression is to minimize the square of the residuals. As
    the graph reveals, a visual of the residuals indicates how well the line fits
    the data. In statistics, linear regression analysis is often performed by graphing
    the residuals to gain deeper insight into the data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的目标是最小化残差的平方。正如图表所示，残差的可视化展示了线性拟合数据的效果。在统计学中，线性回归分析通常通过绘制残差图来深入了解数据。
- en: In order to build a gradient boosting algorithm from scratch, we will compute
    the residuals of each tree and fit a new model to the residuals. Let's do this
    now.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从零开始构建一个梯度提升算法，我们将计算每棵树的残差，并对残差拟合一个新模型。现在我们开始吧。
- en: Learning how to build gradient boosting models from scratch
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何从零开始构建梯度提升模型
- en: Building a gradient boosting model from scratch will provide you with a deeper
    understanding of how gradient boosting works in code. Before building a model,
    we need to access data and prepare it for machine learning.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始构建梯度提升模型将帮助你更深入理解梯度提升在代码中的工作原理。在构建模型之前，我们需要访问数据并为机器学习做准备。
- en: Processing the bike rentals dataset
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理自行车租赁数据集
- en: 'We continue with the bike rentals dataset to compare new models with the previous
    models:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续使用自行车租赁数据集，比较新模型与旧模型的表现：
- en: 'We will start by importing `pandas` and `numpy`. We will also add a line to
    silence any warnings:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从导入`pandas`和`numpy`开始，并添加一行代码来关闭任何警告：
- en: '[PRE0]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, load the `bike_rentals_cleaned` dataset and view the first five rows:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，加载`bike_rentals_cleaned`数据集并查看前五行：
- en: '[PRE1]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Your output should look like this:'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你的输出应如下所示：
- en: '![Figure 4.2 – First five rows of Bike Rental Dataset](img/B15551_04_02.jpg)'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图4.2 – 自行车租赁数据集的前五行](img/B15551_04_02.jpg)'
- en: Figure 4.2 – First five rows of Bike Rental Dataset
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.2 – 自行车租赁数据集的前五行
- en: 'Now, split the data into `X` and `y`. Then, split `X` and `y` into training
    and test sets:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将数据拆分为`X`和`y`。然后，将`X`和`y`拆分为训练集和测试集：
- en: '[PRE2]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It's time to build a gradient boosting model from scratch!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候从头开始构建梯度提升模型了！
- en: Building a gradient boosting model from scratch
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从头开始构建梯度提升模型
- en: 'Here are the steps for building a gradient boosting machine learning model
    from scratch:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从头开始构建梯度提升机器学习模型的步骤：
- en: 'Fit the data to the decision tree: You may use a decision tree stump, which
    has a `max_depth` value of `1`, or a decision tree with a `max_depth` value of
    `2` or `3`. The initial decision tree, called a `max_depth=2` and fit it on the
    training set as `tree_1`, since it''s the first tree in our ensemble:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拟合到决策树：你可以使用决策树桩，其`max_depth`值为`1`，或者使用深度为`2`或`3`的决策树。初始决策树被称为`max_depth=2`，并将其拟合到训练集上，作为`tree_1`，因为它是我们集成中的第一棵树：
- en: '[PRE3]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Make predictions with the training set: Instead of making predictions with
    the test set, predictions in gradient boosting are initially made with the training
    set. Why? To compute the residuals, we need to compare the predictions while still
    in the training phase. The test phase of the model build comes at the end, after
    all the trees have been constructed. The predictions of the training set for the
    first round are obtained by adding the `predict` method to `tree_1` with `X_train`
    as the input:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练集进行预测：与使用测试集进行预测不同，梯度提升法中的预测最初是使用训练集进行的。为什么？因为要计算残差，我们需要在训练阶段比较预测结果。模型构建的测试阶段是在所有树构建完毕后才会进行。第一轮的训练集预测结果是通过将`predict`方法应用到`tree_1`，并使用`X_train`作为输入来得到的：
- en: '[PRE4]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Compute the residuals: The residuals are the differences between the predictions
    and the target column. The predictions of `X_train`, defined here as `y_train_pred`,
    are subtracted from `y_train`, the target column, to compute the residuals:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算残差：残差是预测值与目标列之间的差异。将`X_train`的预测值，定义为`y_train_pred`，从`y_train`目标列中减去，来计算残差：
- en: '[PRE5]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The residuals are defined as `y2_train` because they are the new target column
    for the next tree.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 残差定义为`y2_train`，因为它是下一棵树的目标列。
- en: 'Fit the new tree on the residuals: Fitting a new tree on the residuals is different
    than fitting a model on the training set. The primary difference is in the predictions.
    In the bike rentals dataset, when fitting a new tree on the residuals, we should
    progressively get smaller numbers.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新树拟合到残差上：将新树拟合到残差上与将模型拟合到训练集上有所不同。主要的区别在于预测。对于自行车租赁数据集，在将新树拟合到残差上时，我们应逐渐得到更小的数字。
- en: 'Initialize a new tree and fit it on `X_train` and the residuals, `y2_train`:'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初始化一棵新树，并将其拟合到`X_train`和残差`y2_train`上：
- en: '[PRE6]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Repeat steps 2-4: As the process continues, the residuals should gradually
    approach `0` from the positive and negative direction. The iterations continue
    for the number of estimators, `n_estimators`.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2-4：随着过程的进行，残差应逐渐从正向和负向逼近`0`。迭代会持续进行，直到达到估计器的数量`n_estimators`。
- en: 'Let''s repeat the process for a third tree as follows:'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们重复第三棵树的过程，如下所示：
- en: '[PRE7]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This process may continue for dozens, hundreds, or thousands of trees. Under
    normal circumstances, you would certainly keep going. It will take more than a
    few trees to transform a weak learner into a strong learner. Since our goal is
    to understand how gradient boosting works behind the scenes, however, we will
    move on now that the general idea has been covered.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个过程可能会持续几十棵、几百棵甚至几千棵树。在正常情况下，你当然会继续进行。要将一个弱学习器转变为强学习器，肯定需要更多的树。然而，由于我们的目标是理解梯度提升背后的工作原理，因此在一般概念已经覆盖的情况下，我们将继续前进。
- en: 'Sum the results: Summing the results requires making predictions for each tree
    with the test set as follows:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 汇总结果：汇总结果需要为每棵树使用测试集进行预测，如下所示：
- en: '[PRE8]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Since the predictions are positive and negative differences, summing the predictions
    should result in predictions that are closer to the target column as follows:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于预测值是正负差异，将预测值进行汇总应能得到更接近目标列的预测结果，如下所示：
- en: '[PRE9]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Lastly, let''s compute the **mean squared error** (**MSE**) to obtain the results
    as follows:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们计算**均方误差**（**MSE**）来获得结果，如下所示：
- en: '[PRE10]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here is the expected output:'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是预期的输出：
- en: '[PRE11]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Not bad for a weak learner that isn't yet strong! Now let's try to obtain the
    same result using scikit-learn.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个尚未强大的弱学习器来说，这样的表现还不错！现在，让我们尝试使用scikit-learn获得相同的结果。
- en: Building a gradient boosting model in scikit-learn
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在scikit-learn中构建梯度提升模型
- en: 'Let''s see whether we can obtain the same result as in the previous section
    using scikit-learn''s `GradientBoostingRegressor`. This may be done through a
    few hyperparameter adjustments. The advantage of using `GradientBoostingRegressor`
    is that it''s much faster to build and easier to implement:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看能否通过调整一些超参数，使用scikit-learn的`GradientBoostingRegressor`得到与前一节相同的结果。使用`GradientBoostingRegressor`的好处是，它构建得更快，且实现起来更简单：
- en: 'First, import the regressor from the `sklearn.ensemble` library:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从`sklearn.ensemble`库中导入回归器：
- en: '[PRE12]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'When initializing `GradientBoostingRegressor`, there are several important
    hyperparameters. To obtain the same results, it''s essential to match `max_depth=2`
    and `random_state=2`. Furthermore, since there are only three trees, we must have
    `n_estimators=3`. Finally, we must set the `learning_rate=1.0` hyperparameter.
    We will have much to say about `learning_rate` shortly:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在初始化`GradientBoostingRegressor`时，有几个重要的超参数。为了获得相同的结果，必须将`max_depth=2`和`random_state=2`匹配。此外，由于只有三棵树，我们必须设置`n_estimators=3`。最后，我们还必须设置`learning_rate=1.0`超参数。稍后我们会详细讨论`learning_rate`：
- en: '[PRE13]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that the model has been initialized, it can be fit on the training data
    and scored against the test data:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在模型已经初始化，可以在训练数据上进行拟合，并在测试数据上进行评分：
- en: '[PRE14]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The result is as follows:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE15]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The result is the same to 11 decimal places!
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果在小数点后11位都是相同的！
- en: Recall that the point of gradient boosting is to build a model with enough trees
    to transform a weak learner into a strong learner. This is easily done by changing
    `n_estimators`, the number of iterations, to a much larger number.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回顾一下，梯度提升的关键是构建一个足够多的树的模型，将弱学习器转变为强学习器。通过将`n_estimators`（迭代次数）设置为一个更大的数字，这可以很容易地实现。
- en: 'Let''s build and score a gradient boosting regressor with 30 estimators:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们构建并评分一个拥有30个估算器的梯度提升回归器：
- en: '[PRE16]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The result is as follows:'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE17]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The score is an improvement. Now let''s look at 300 estimators:'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分有所提升。现在，让我们看看300个估算器的情况：
- en: '[PRE18]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The result is this:'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果是这样的：
- en: '[PRE19]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This is a surprise! The score has gotten worse! Have we been misled? Is gradient
    boosting not all that it's cracked up to be?
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是个惊讶！得分变差了！我们是不是被误导了？梯度提升真如人们所说的那样有效吗？
- en: Whenever you get a surprise result, it's worth double-checking the code. Now,
    we changed `learning_rate` without saying much about it. So, what happens if we
    remove `learning_rate=1.0` and use the scikit-learn defaults?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 每当得到一个意外的结果时，都值得仔细检查代码。现在，我们更改了`learning_rate`，却没有详细说明。那么，如果我们移除`learning_rate=1.0`，并使用scikit-learn的默认值会怎样呢？
- en: 'Let''s find out:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看：
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The result is this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是这样的：
- en: '[PRE21]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Incredible! By using the scikit-learn default for the `learning_rate` hyperparameter,
    the score has changed from `936` to `654`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 难以置信！通过使用scikit-learn对`learning_rate`超参数的默认值，得分从`936`变为`654`。
- en: In the next section, we will learn more about the different gradient boosting
    hyperparameters with a focus on the `learning_rate` hyperparameter.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入学习不同的梯度提升超参数，重点关注`learning_rate`超参数。
- en: Modifying gradient boosting hyperparameters
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修改梯度提升超参数
- en: In this section, we will focus on the `learning_rate`, the most important gradient
    boosting hyperparameter, with the possible exception of `n_estimators`, the number
    of iterations or trees in the model. We will also survey some tree hyperparameters,
    and `subsample`, which results in `RandomizedSearchCV` and compare results with
    XGBoost.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点关注`learning_rate`，这是最重要的梯度提升超参数，可能唯一需要注意的是`n_estimators`，即模型中的迭代次数或树的数量。我们还将调查一些树的超参数，以及`subsample`，这会导致`RandomizedSearchCV`，并将结果与XGBoost进行比较。
- en: learning_rate
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: learning_rate
- en: In the last section, changing the `learning_rate` value of `GradientBoostingRegressor`
    from `1.0` to scikit-learn's default, which is `0.1`, resulted in enormous gains.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，将`GradientBoostingRegressor`的`learning_rate`值从`1.0`更改为scikit-learn的默认值`0.1`，得到了巨大的提升。
- en: '`learning_rate`, also known as the *shrinkage*, shrinks the contribution of
    individual trees so that no tree has too much influence when building the model.
    If an entire ensemble is built from the errors of one base learner, without careful
    adjustment of hyperparameters, early trees in the model can have too much influence
    on subsequent development. `learning_rate` limits the influence of individual
    trees. Generally speaking, as `n_estimators`, the number of trees, goes up, `learning_rate`
    should go down.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`learning_rate`，也称为*收缩率*，会缩小单棵树的贡献，以确保在构建模型时没有一棵树的影响过大。如果整个集成模型是通过一个基础学习器的误差构建的，而没有仔细调整超参数，模型中的早期树可能会对后续的发展产生过大的影响。`learning_rate`限制了单棵树的影响。通常来说，随着`n_estimators`（树的数量）的增加，`learning_rate`应该减小。'
- en: Determining an optimal `learning_rate` value requires varying `n_estimators`.
    First, let's hold `n_estimators` constant and see what `learning_rate` does on
    its own. `learning_rate` ranges from `0` to `1`. A `learning_rate` value of `1`
    means that no adjustments are made. The default value of `0.1` means that the
    tree's influence is weighted at 10%.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 确定最优的`learning_rate`值需要调整`n_estimators`。首先，让我们保持`n_estimators`不变，看看`learning_rate`单独的表现。`learning_rate`的取值范围是从`0`到`1`。`learning_rate`值为`1`意味着不做任何调整。默认值`0.1`表示树的影响权重为10%。
- en: 'Here is a reasonable range to start with:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个合理的起始范围：
- en: '`learning_rate_values = [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5, 1.0]`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`learning_rate_values = [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5, 1.0]`'
- en: 'Next, we will loop through the values by building and scoring a new `GradientBoostingRegressor`
    to see how the scores compare:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过构建和评分新的`GradientBoostingRegressor`来遍历这些值，看看得分如何比较：
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The learning rate values and scores are as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率的值和得分如下：
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As you can see from the output, the default `learning_rate` value of `0.1` gives
    the best score for 300 trees.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中可以看出，默认的`learning_rate`值为`0.1`时，300棵树的得分最好。
- en: 'Now let''s vary `n_estimators`. Using the preceding code, we can generate `learning_rate`
    plots with `n_estimators` of 30, 300, and 3,000 trees, as shown in the following
    figure:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们调整`n_estimators`。使用前面的代码，我们可以生成`learning_rate`图，其中`n_estimators`为30、300和3,000棵树，如下图所示：
- en: '![Figure 4.3 – learning_rate plot for 30 trees](img/B15551_04_03.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3 – 30棵树的learning_rate图](img/B15551_04_03.jpg)'
- en: Figure 4.3 – learning_rate plot for 30 trees
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – 30棵树的learning_rate图
- en: As you can see, with 30 trees, the `learning_rate` value peaks at around `0.3`.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，使用30棵树时，`learning_rate`值在大约`0.3`时达到峰值。
- en: 'Now, let''s take a look at the `learning_rate` plot for 3,000 trees:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下3,000棵树的`learning_rate`图：
- en: '![Fig 4.4 -- learning_rate plot for 3,000 trees](img/B15551_04_04.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4 -- 3,000棵树的learning_rate图](img/B15551_04_04.jpg)'
- en: Fig 4.4 -- learning_rate plot for 3,000 trees
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 -- 3,000棵树的learning_rate图
- en: With 3,000 trees, the `learning_rate` value peaks at the second value, which
    is given as `0.05`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用3,000棵树时，`learning_rate`值在第二个值，即`0.05`时达到峰值。
- en: These graphs highlight the importance of tuning `learning_rate` and `n_estimators`
    together.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图表突显了调优`learning_rate`和`n_estimators`的重要性。
- en: Base learner
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础学习器
- en: The initial decision tree in the gradient boosting regressor is called the **base
    learner** because it's at the base of the ensemble. It's the first learner in
    the process. The term *learner* here is indicative of a *weak learner* transforming
    into a *strong learner*.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升回归器中的初始决策树被称为**基础学习器**，因为它是集成模型的基础。它是过程中的第一个学习器。这里的*学习器*一词表示一个*弱学习器*正在转变为*强学习器*。
- en: Although base learners need not be fine-tuned for accuracy, as covered in [*Chapter
    2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047), *Decision Trees in Depth*,
    it's certainly possible to tune base learners for gains in accuracy.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基础学习器不需要为准确性进行微调，正如在[*第2章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)《决策树深入解析》中所述，当然也可以通过调优基础学习器来提高准确性。
- en: 'For instance, we can select a `max_depth` value of `1`, `2`, `3`, or `4` and
    compare results as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以选择`max_depth`值为`1`、`2`、`3`或`4`，并比较结果如下：
- en: '[PRE24]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The result is as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: A `max_depth` value of `3` gives the best results.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_depth`值为`3`时，得到最佳结果。'
- en: Other base learner hyperparameters, as covered in [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047),
    *Decision Trees in Depth*, may be tuned in a similar manner.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 其他基础学习器的超参数，如在[*第2章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)《决策树深入解析》中所述，也可以采用类似的方式进行调整。
- en: subsample
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 子样本（subsample）
- en: '`subsample` is a subset of samples. Since samples are the rows, a subset of
    rows means that all rows may not be included when building each tree. By changing
    `subsample` from `1.0` to a smaller decimal, trees only select that percentage
    of samples during the build phase. For example, `subsample=0.8` would select 80%
    of samples for each tree.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`subsample`是样本的一个子集。由于样本是行，子集意味着在构建每棵树时可能并不是所有的行都会被包含。当将`subsample`从`1.0`改为更小的小数时，树在构建阶段只会选择该百分比的样本。例如，`subsample=0.8`会为每棵树选择80%的样本。'
- en: 'Continuing with `max_depth=3`, we try a range of subsample percentages to improve
    results:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用`max_depth=3`，我们尝试不同的`subsample`百分比，以改善结果：
- en: '[PRE26]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The result is as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE27]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: A `subsample` value of `0.7` with 300 trees and `max_depth` of `3` produces
    the best score yet.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`subsample`值为`0.7`，300棵树和`max_depth`为`3`时，获得了目前为止最佳的得分。
- en: When `subsample` is not equal to `1.0`, the model is classified as **stochastic
    gradient descent**, where *stochastic* indicates that some randomness is inherent
    in the model.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当`subsample`不等于`1.0`时，模型被归类为**随机梯度下降**，其中*随机*意味着模型中存在某些随机性。
- en: RandomizedSearchCV
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机搜索交叉验证（RandomizedSearchCV）
- en: 'We have a good working model, but we have not yet performed a grid search,
    as covered in [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047), *Decision
    Trees in Depth*. Our preliminary analysis indicates that a grid search centered
    around `max_depth=3`, `subsample=0.7`, `n_estimators=300`, and `learning_rate
    = 0.1` is a good place to start. We have already shown that as `n_estimators`
    goes up, `learning_rate` should go down:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个良好的工作模型，但尚未进行网格搜索，如[*第2章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)《决策树深入分析》中所述。我们的初步分析表明，以`max_depth=3`、`subsample=0.7`、`n_estimators=300`和`learning_rate
    = 0.1`为中心进行网格搜索是一个不错的起点。我们已经展示了，当`n_estimators`增加时，`learning_rate`应当减少：
- en: 'Here is a possible starting point:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是一个可能的起点：
- en: '[PRE28]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Since `n_estimators` is going up from the starting value of 300, `learning_rate`
    is going down from the starting value of `0.1`. Let's keep `max_depth=3` to limit
    the variance.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于`n_estimators`从初始值300增加，`learning_rate`从初始值`0.1`减少。我们保持`max_depth=3`以限制方差。
- en: With 27 possible combinations of hyperparameters, we use `RandomizedSearchCV`
    to try 10 of these combinations in the hopes of finding a good model.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在27种可能的超参数组合中，我们使用`RandomizedSearchCV`尝试其中的10种组合，希望找到一个好的模型。
- en: Note
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: While 27 combinations are feasible with `GridSearchCV`, at some point you will
    end up with too many possibilities and `RandomizedSearchCV` will become essential.
    We use `RandomizedSearchCV` here for practice and to speed up computations.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管`GridSearchCV`可以实现27种组合，但最终你会遇到可能性过多的情况，`RandomizedSearchCV`在此时变得至关重要。我们在这里使用`RandomizedSearchCV`进行实践并加速计算。
- en: 'Let''s import `RandomizedSearchCV` and initialize a gradient boosting model:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们导入`RandomizedSearchCV`并初始化一个梯度提升模型：
- en: '[PRE29]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, initialize `RandomizedSearchCV` with `gbr` and `params` as inputs in
    addition to the number of iterations, the scoring, and the number of folds. Recall
    that `n_jobs=-1` may speed up computations and `random_state=2` ensures the consistency
    of results:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，初始化`RandomizedSearchCV`，以`gbr`和`params`作为输入，除了迭代次数、评分标准和折叠数。请记住，`n_jobs=-1`可能加速计算，而`random_state=2`确保结果的一致性：
- en: '[PRE30]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now fit the model on the training set and obtain the best parameters and scores:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在在训练集上拟合模型，并获取最佳参数和分数：
- en: '[PRE31]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The result is as follows:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE32]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: From here, it's worth experimenting by changing parameters individually or in
    pairs. Even though the best model currently has `n_estimators=300`, it's certainly
    possible that raising this hyperparameter will obtain better results with careful
    adjustment of the `learning_rate` value. `subsample` may be experimented with
    as well.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从这里开始，值得逐个或成对地调整参数进行实验。尽管当前最好的模型有`n_estimators=300`，但通过谨慎调整`learning_rate`，提高该超参数可能会得到更好的结果。`subsample`也可以进行实验。
- en: 'After a few rounds of experimentation, we obtained the following model:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经过几轮实验后，我们得到了以下模型：
- en: '[PRE33]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The result is the following:'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE34]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: With a larger value for `n_estimators` at `1600`, a smaller `learning_rate`
    value at `0.02`, a comparable `subsample` value of `0.75`, and the same `max_depth`
    value of `3`, we obtained the best `597`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在`n_estimators`为`1600`、`learning_rate`为`0.02`、`subsample`为`0.75`、`max_depth`为`3`时，我们得到了最佳得分`597`。
- en: It may be possible to do better. We encourage you to try!
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 可能还有更好的方法。我们鼓励你尝试！
- en: Now, let's see how XGBoost differs from gradient boosting using the same hyperparameters
    covered thus far.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看XGBoost与梯度提升在使用迄今为止所涉及的相同超参数时有何不同。
- en: XGBoost
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost
- en: XGBoost is an advanced version of gradient boosting with the same general structure,
    meaning that it transforms weak learners into strong learners by summing the residuals
    of trees.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost是梯度提升的高级版本，具有相同的一般结构，这意味着它通过将树的残差求和，将弱学习器转化为强学习器。
- en: The only difference in hyperparameters from the last section is that XGBoost
    refers to `learning_rate` as `eta`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节中的超参数唯一的不同之处是，XGBoost将`learning_rate`称为`eta`。
- en: Let's build an XGBoost regressor with the same hyperparameters to compare the
    results.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用相同的超参数构建一个XGBoost回归模型来比较结果。
- en: 'Import `XGBRegressor` from `xgboost`, and then initialize and score the model
    as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 从`xgboost`导入`XGBRegressor`，然后初始化并评分模型，代码如下：
- en: '[PRE35]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The result is this:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE36]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The score is better. The reason as to why the score is better will be revealed
    in the next chapter, [*Chapter 5*](B15551_05_Final_NM_ePUB.xhtml#_idTextAnchor117),
    *XGBoost Unveiled*.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 分数更好。为什么分数更好将在下一章中揭示，[*第5章*](B15551_05_Final_NM_ePUB.xhtml#_idTextAnchor117)，*XGBoost揭秘*。
- en: Accuracy and speed are the two most important concepts when building machine
    learning models, and we have shown multiple times that XGBoost is very accurate.
    XGBoost is preferred over gradient boosting in general because it consistently
    delivers better results, and because it's faster, as demonstrated by the following
    case study.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性和速度是构建机器学习模型时最重要的两个概念，我们已经多次证明XGBoost非常准确。XGBoost通常比梯度提升更受欢迎，因为它始终提供更好的结果，并且因为它更快，以下案例研究对此做出了证明。
- en: Approaching big data – gradient boosting versus XGBoost
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面对大数据——梯度提升与XGBoost的对比
- en: In the real world, datasets can be enormous, with trillions of data points.
    Limiting work to one computer can be disadvantageous due to the limited resources
    of one machine. When working with big data, the cloud is often used to take advantage
    of parallel computers.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，数据集可能庞大，包含万亿个数据点。仅依靠一台计算机可能会因为资源有限而不利于工作。处理大数据时，通常使用云计算来利用并行计算机。
- en: Datasets are big when they push the limits of computation. So far in this book,
    by limiting datasets to tens of thousands of rows with a hundred or fewer columns,
    there should have been no significant time delays, unless you ran into errors
    (happens to everyone).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集之所以被认为是“大”，是因为它们突破了计算的极限。在本书至此为止，数据集的行数限制在数万行，列数不超过一百列，应该没有显著的时间延迟，除非你遇到错误（每个人都会发生）。
- en: In this section, we examine **exoplanets** over time. The dataset has 5,087
    rows and 3,189 columns that record light flux at different times of a star's life
    cycle. Multiplying columns and rows together results in 1.5 million data points.
    Using a baseline of 100 trees, we need 150 million data points to build a model.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将随时间考察**系外行星**。数据集包含5,087行和3,189列，记录了恒星生命周期不同阶段的光通量。将列和行相乘得到150万数据点。以100棵树为基准，我们需要1.5亿个数据点来构建模型。
- en: In this section, my 2013 MacBook Air had wait times of about 5 minutes. New
    computers should be faster. I have chosen the exoplanet dataset so that wait times
    play a significant role without tying up your computer for a very long time.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我的2013款MacBook Air等待时间大约为5分钟。新电脑应该会更快。我选择了系外行星数据集，以便等待时间对计算有显著影响，同时不会让你的计算机长时间占用。
- en: Introducing the exoplanet dataset
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍系外行星数据集
- en: 'The exoplanet dataset is taken from Kaggle and dates from around 2017: [https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data](https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data).
    The dataset contains information about the light of stars. Each row is an individual
    star and the columns reveal different light patterns over time. In addition to
    light patterns, an exoplanet column is labeled `2` if the star hosts an exoplanet;
    otherwise, it is labeled `1`.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 系外行星数据集来自Kaggle，数据大约来自2017年：[https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data](https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data)。该数据集包含关于恒星光的资料。每一行是一个单独的恒星，列展示了随时间变化的不同光模式。除了光模式外，如果恒星有系外行星，系外行星列标记为`2`；否则标记为`1`。
- en: The dataset records the light flux from thousands of stars. **Light flux**,
    often referred to as **luminous flux**, is the perceived brightness of a star.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集记录了成千上万颗恒星的光通量。**光通量**，通常被称为**光亮通量**，是恒星的感知亮度。
- en: Note
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The perceived brightness is different than actual brightness. For instance,
    an incredibly bright star very far away may have a small luminous flux (looks
    dim), while a moderately bright star that is very close, like the sun, may have
    a large luminous flux (looks bright).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 感知亮度与实际亮度不同。例如，一颗非常明亮但距离遥远的恒星，其光通量可能很小（看起来很暗），而像太阳这样距离很近的中等亮度恒星，其光通量可能很大（看起来很亮）。
- en: When the light flux of an individual star changes periodically, it is possible
    that the star is being orbited by an exoplanet. The assumption is that when an
    exoplanet orbits in front of a star, it blocks a small fraction of the light,
    reducing the perceived brightness by a very slight amount.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 当单颗恒星的光通量周期性变化时，可能是由于该恒星被外行星所绕。假设是外行星在恒星前方运行时，它会阻挡一小部分光线，导致观测到的亮度略微减弱。
- en: Tip
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Finding exoplanets is rare. The predictive column, on whether a star hosts an
    exoplanet or not, has very few positive cases, resulting in an imbalanced dataset.
    Imbalanced datasets require extra precautions. We will cover imbalanced datasets
    in [*Chapter 7*](B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161), *Discovering
    Exoplanets with XGBoost*, where we go into further detail with this dataset.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找外行星是非常罕见的。预测列，关于恒星是否拥有外行星，正例非常少，导致数据集不平衡。不平衡的数据集需要额外的注意。在[*第7章*](B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161)《使用XGBoost发现外行星》中，我们将进一步探讨该数据集的不平衡问题。
- en: Next, let's access the exoplanet dataset and prepare it for machine learning.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们访问外行星数据集并为机器学习做准备。
- en: Preprocessing the exoplanet dataset
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对外行星数据集进行预处理
- en: The exoplanet dataset has been uploaded to our GitHub page at [https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter04](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter04).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 外行星数据集已经上传到我们的GitHub页面：[https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter04](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter04)。
- en: 'Here are the steps to load and preprocess the exoplanet dataset for machine
    learning:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是加载和预处理外行星数据集以供机器学习使用的步骤：
- en: 'Download `exoplanets.csv` in the same folder as your Jupyter Notebook. Then,
    open the file and take a look:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载`exoplanets.csv`文件，并将其与Jupyter Notebook放在同一文件夹下。然后，打开该文件查看内容：
- en: '[PRE37]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The DataFrame will look as shown in the following figure:'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DataFrame会如下所示：
- en: '![Fig 4.5 – Exoplanet DataFrame](img/B15551_04_05.jpg)'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.5 – 外行星数据框](img/B15551_04_05.jpg)'
- en: Fig 4.5 – Exoplanet DataFrame
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.5 – 外行星数据框
- en: Not all columns are shown due to space limitations. The flux columns are floats,
    while the `Label` column is `2` for an exoplanet star and `1` for a non-exoplanet
    star.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于空间限制，并非所有列都会显示。光通量列为浮动数值类型，而`Label`列对于外行星恒星是`2`，对于非外行星恒星是`1`。
- en: 'Let''s'' confirm that all columns are numerical with `df.info()`:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用`df.info()`来确认所有列都是数值型的：
- en: '[PRE38]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The result is as follows:'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE39]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As you can see from the output, `3197` columns are floats and `1` column is
    an `int`, so all columns are numerical.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从输出结果可以看出，`3197`列是浮动数值类型，`1`列是`int`类型，所以所有列都是数值型的。
- en: 'Now, let''s confirm the number of null values with the following code:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们用以下代码确认空值的数量：
- en: '[PRE40]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output is as follows:'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE41]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The output reveals that there are no null values.
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果显示没有空值。
- en: 'Since all columns are numerical with no null values, we may split the data
    into training and test sets. Note that the 0th column is the target column, `y`,
    and all other columns are the predictor columns, `X`:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于所有列都是数值型且没有空值，我们可以将数据拆分为训练集和测试集。请注意，第0列是目标列`y`，其余列是预测列`X`：
- en: '[PRE42]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: It's time to build a gradient boosting classifier to predict whether stars host
    exoplanets.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候构建梯度提升分类器来预测恒星是否拥有外行星了。
- en: Building gradient boosting classifiers
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建梯度提升分类器
- en: Gradient boosting classifiers work in the same manner as gradient boosting regressors.
    The difference is primarily in the scoring.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升分类器的工作原理与梯度提升回归器相同，主要的区别在于评分方式。
- en: 'Let''s start by importing `GradientBoostingClassifer` and `XGBClassifier` in
    addition to `accuracy_score` so that we may compare both models:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始导入`GradientBoostingClassifer`和`XGBClassifier`，并导入`accuracy_score`，以便我们可以比较这两个模型：
- en: '[PRE43]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Next, we need a way to compare models using a timer.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一种方法来使用定时器比较模型。
- en: Timing models
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型计时
- en: Python comes with a `time` library that can be used to mark time. The general
    idea is to mark the time before and after a computation. The difference between
    these times tells us how long the computation took.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Python自带了一个`time`库，可以用来标记时间。一般的做法是在计算前后标记时间，时间差告诉我们计算花费的时间。
- en: 'The `time` library is imported as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`time`库的导入方法如下：'
- en: '[PRE44]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Within the `time` library, the `.time()` method marks time in seconds.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在`time`库中，`.time()`方法以秒为单位标记时间。
- en: 'As an example, see how long it takes to run `df.info()` by assigning start
    and end times before and after the computation using `time.time()`:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，查看通过使用`time.time()`在计算前后标记开始和结束时间，`df.info()`运行需要多长时间：
- en: '[PRE45]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output is as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE46]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The runtime is as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时间如下：
- en: '[PRE47]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Your results will differ from ours, but hopefully it's in the same ballpark.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 你的结果可能会与我们的不同，但希望在同一数量级范围内。
- en: Let's now compare `GradientBoostingClassifier` and `XGBoostClassifier` with
    the exoplanet dataset for its speed using the preceding code to mark time.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用前面的代码标记时间，比较`GradientBoostingClassifier`和`XGBoostClassifier`在外行星数据集上的速度。
- en: Tip
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Jupyter Notebooks come with magic functions, denoted by the `%` sign before
    a command. `%timeit` is one such magic function. Instead of computing how long
    it takes to run the code once, `%timeit` computes how long it takes to run code
    over multiple runs. See [ipython.readthedocs.io/en/stable/interactive/magics.html](http://ipython.readthedocs.io/en/stable/interactive/magics.html)
    for more information on magic functions.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter Notebooks配有魔法函数，用`%`符号标记在命令前。`%timeit`就是这样一个魔法函数。与计算运行一次代码所需的时间不同，`%timeit`会计算多次运行代码所需的时间。有关魔法函数的更多信息，请参见[ipython.readthedocs.io/en/stable/interactive/magics.html](http://ipython.readthedocs.io/en/stable/interactive/magics.html)。
- en: Comparing speed
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较速度
- en: 'It''s time to race `GradientBoostingClassifier` and `XGBoostClassifier` with
    the exoplanet dataset. We have set `max_depth=2` and `n_estimators=100` to limit
    the size of the model. Let''s start with `GradientBoostingClassifier`:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候用外行星数据集对`GradientBoostingClassifier`和`XGBoostClassifier`进行速度对比了。我们设置了`max_depth=2`和`n_estimators=100`来限制模型的大小。我们从`GradientBoostingClassifier`开始：
- en: 'First, we will mark the start time. After building and scoring the model, we
    will mark the end time. The following code may take around 5 minutes to run depending
    on the speed of your computer:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将标记开始时间。在构建并评分模型后，我们将标记结束时间。以下代码可能需要大约5分钟来运行，具体时间取决于你的计算机速度：
- en: '[PRE48]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The result is this:'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE49]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '`GradientBoostingRegressor` took over 5 minutes to run on my 2013 MacBook Air.
    Not bad for 150 million data points on an older computer.'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`GradientBoostingRegressor`在我的2013款MacBook Air上运行了超过5分钟。对于一台旧电脑来说，在150百万数据点的情况下，表现还不错。'
- en: Note
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: While a score of 98.7% percent is usually outstanding for accuracy, this is
    not the case with imbalanced datasets, as you will see in [*Chapter 7*](B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161),
    *Discovering Exoplanets with XGBoost*.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管98.7%的得分通常在准确性上非常出色，但对于不平衡数据集而言情况并非如此，正如你将在[*第七章*](B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161)中看到的那样，*使用XGBoost发现外行星*。
- en: 'Next, we will build an `XGBClassifier` model with the same hyperparameters
    and mark the time in the same manner:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将构建一个具有相同超参数的`XGBClassifier`模型，并以相同的方式标记时间：
- en: '[PRE50]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The result is as follows:'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE51]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: On my 2013 MacBook Air, XGBoost took under 2 minutes, making it more than twice
    as fast. It's also more accurate by half a percentage point.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的2013款MacBook Air上，XGBoost运行时间不到2分钟，速度是原来的两倍以上。它的准确性也提高了半个百分点。
- en: When it comes to big data, an algorithm twice as fast can save weeks or months
    of computational time and resources. This advantage is huge in the world of big
    data.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据的领域中，一个速度是原来两倍的算法可以节省数周甚至数月的计算时间和资源。这一优势在大数据领域中是巨大的。
- en: In the world of boosting, XGBoost is the model of choice due to its unparalleled
    speed and impressive accuracy.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在提升法的世界里，XGBoost因其无与伦比的速度和出色的准确性而成为首选模型。
- en: As for the exoplanet dataset, it will be revisited in [*Chapter 7*](B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161),
    *Discovering Exoplanets with XGBoost*, in an important case study that reveals
    the challenges of working with imbalanced datasets along with a variety of potential
    solutions to those challenges.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 至于外行星数据集，它将在[*第七章*](B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161)中重新讨论，在一个重要的案例研究中揭示了处理不平衡数据集时的挑战，以及针对这些挑战的多种潜在解决方案。
- en: Note
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'I recently purchased a 2020 MacBook Pro and updated all software. The difference
    in time using the same code is staggering:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我最近购买了一台2020款的MacBook Pro并更新了所有软件。使用相同代码时，时间差异惊人：
- en: 'Gradient Boosting Run Time: 197.38 seconds'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升运行时间：197.38秒
- en: 'XGBoost Run Time: 8.66 seconds'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost运行时间：8.66秒
- en: More than a 10-fold difference!
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 超过10倍的差异！
- en: Summary
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned the difference between bagging and boosting. You
    learned how gradient boosting works by building a gradient boosting regressor
    from scratch. You implemented a variety of gradient boosting hyperparameters,
    including `learning_rate`, `n_estimators`, `max_depth`, and `subsample`, which
    results in stochastic gradient boosting. Finally, you used big data to predict
    whether stars have exoplanets by comparing the times of `GradientBoostingClassifier`
    and `XGBoostClassifier`, with `XGBoostClassifier` emerging as twice to over ten
    times as fast and more accurate.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了bagging与boosting的区别。你通过从零开始构建一个梯度提升回归器，学习了梯度提升是如何工作的。你实现了各种梯度提升超参数，包括`learning_rate`、`n_estimators`、`max_depth`和`subsample`，这导致了随机梯度提升。最后，你利用大数据，通过比较`GradientBoostingClassifier`和`XGBoostClassifier`的运行时间，预测星星是否拥有系外行星，结果表明`XGBoostClassifier`的速度是前者的两倍，甚至超过十倍，同时更为准确。
- en: The advantage of learning these skills is that you now understand when to apply
    XGBoost rather than similar machine learning algorithms such as gradient boosting.
    You can now build stronger XGBoost and gradient boosting models by properly taking
    advantage of core hyperparameters, including `n_estimators` and `learning_rate`.
    Furthermore, you have developed the capacity to time all computations instead
    of relying on intuition.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 学习这些技能的优势在于，你现在能明白何时使用XGBoost，而不是像梯度提升这样的类似机器学习算法。你现在可以通过正确利用核心超参数，包括`n_estimators`和`learning_rate`，来构建更强大的XGBoost和梯度提升模型。此外，你也已经具备了为所有计算设定时间的能力，而不再依赖直觉。
- en: Congratulations! You have completed all of the preliminary XGBoost chapters.
    Until now, the purpose has been to introduce you to machine learning and data
    analytics within the larger XGBoost narrative. The aim has been to show how the
    need for XGBoost emerged from ensemble methods, boosting, gradient boosting, and
    big data.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经完成了所有初步的XGBoost章节。到目前为止，目的在于让你了解机器学习和数据分析在更广泛的XGBoost叙事中的背景。目的是展示XGBoost如何从集成方法、提升、梯度提升以及大数据的需求中应运而生。
- en: The next chapter starts a new leg on our journey with an advanced introduction
    to XGBoost, where you will learn the mathematical details behind the XGBoost algorithm
    in addition to hardware modifications that XGBoost makes to improve speed. You'll
    also be building XGBoost models using the original Python API in a historically
    relevant case study on the discovery of the Higgs boson. The chapters that follow
    highlight exciting details, advantages, nuances, and tricks and tips to build
    swift, efficient, powerful, and industry-ready XGBoost models that you can use
    for years to come.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将为我们的旅程开启新的一篇，带来XGBoost的高级介绍。在这里，你将学习XGBoost算法背后的数学细节，并了解XGBoost如何通过硬件修改来提高速度。此外，你还将通过一个关于希格斯玻色子发现的历史性案例，使用原始的Python
    API构建XGBoost模型。接下来的章节将重点介绍如何构建快速、高效、强大且适合行业应用的XGBoost模型，其中涵盖了令人兴奋的细节、优势、技巧和窍门，这些模型将在未来多年内为你所用。
