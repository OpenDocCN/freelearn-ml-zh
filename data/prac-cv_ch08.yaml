- en: 3D Computer Vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 三维计算机视觉
- en: In the last few chapters, we have discussed the extraction of an object and
    semantic information from images. We saw how good feature extraction leads to
    object detection, segmentation, and tracking. This information explicitly requires
    the geometry of the scene; in several applications, knowing the exact geometry
    of a scene plays a vital role.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后几章中，我们讨论了从图像中提取对象和语义信息。我们看到了如何通过良好的特征提取实现对象检测、分割和跟踪。这些信息明确地需要场景的几何形状；在许多应用中，了解场景的确切几何形状起着至关重要的作用。
- en: In this chapter, we will see a discussion leading to the three-dimensional aspects
    of an image. Here, we will begin by using a simple camera model to understand
    how pixel values and real-world points are linked correspondingly. Later, we will
    study methods for computing depth from images and also methods of computing the
    motion of a camera from a sequence of images.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨导致图像三维方面的讨论。在这里，我们将首先使用一个简单的相机模型来理解像素值和现实世界点是如何相应地联系起来的。稍后，我们将研究从图像序列中计算深度和计算摄像头运动的方法。
- en: 'We will cover the following topics in the chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: RGDB dataset
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RGDB数据集
- en: Applications to extract features from images
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从图像中提取特征的应用
- en: Image formation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像形成
- en: Aligning of images
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像对齐
- en: Visual odometry
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉里程计
- en: Visual SLAM
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉SLAM
- en: Dataset and libraries
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集和库
- en: In this chapter, we will be using OpenCV for most of the applications. In the
    last section, for **Visual Simultaneous Localization and Mapping** (**vSLAM**)
    techniques, we will see the use of an open source repository; directions for its
    use are mentioned in the section. The dataset is the `RGBD` dataset, consisting
    of a sequence of images captured using RGB and a depth camera. To download this
    dataset, visit the following link and download the fr1/xyz tar file: [https://vision.in.tum.de/data/datasets/rgbd-dataset/download](https://vision.in.tum.de/data/datasets/rgbd-dataset/download).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用OpenCV进行大多数应用。在最后一节中，对于**视觉同时定位与建图（vSLAM**）技术，我们将看到开源存储库的使用；其使用说明在章节中提及。数据集是`RGBD`数据集，由使用RGB和深度相机捕获的一系列图像组成。要下载此数据集，请访问以下链接并下载`fr1/xyz`tar文件：[https://vision.in.tum.de/data/datasets/rgbd-dataset/download](https://vision.in.tum.de/data/datasets/rgbd-dataset/download)。
- en: 'Alternatively, use the following (Linux only) command in a Terminal:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，在终端中使用以下（仅限Linux）命令：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Applications
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用
- en: 'While deep learning can extract good features for high-level applications,
    there are areas that require pixel level matching to compute geometric information
    from an image. Some of the applications that use this information are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然深度学习可以提取适用于高级应用的良好特征，但还有一些领域需要像素级匹配来从图像中计算几何信息。使用这些信息的某些应用包括：
- en: '**Drones**: In commercial robots like drones, the image sequence is used to
    compute the motion of the camera mounted on them. This helps them to make robust
    motion estimations and, in addition to other **Inertial Measurement Units** (**IMU**)
    such as gyroscopes, accelerometers, and so on, the overall motion is estimated
    more accurately.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无人机**：在无人机等商用机器人中，图像序列被用来计算安装在它们上的摄像头的运动。这有助于它们进行稳健的运动估计，并且除了陀螺仪、加速度计等其他**惯性测量单元（IMU**）之外，整体运动估计更加准确。'
- en: '**Image editing applications**: Smartphones and professional applications for
    image editing include tools like panorama creation, image stitching, and so on.
    These apps compute orientation from common pixels across image samples and align
    the images in one target orientation. The resulting image looks as if it has been
    stitched by joining the end of one image to another.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像编辑应用**：智能手机和专业图像编辑应用包括创建全景、图像拼接等工具。这些应用通过计算图像样本中的公共像素来计算方向，并将图像对齐到单一的目标方向。结果图像看起来就像是通过将一个图像的末端连接到另一个图像来拼接的。'
- en: '**Satellites or space vehicles**: In the remote operation of satellites or
    robots, it is hard and erroneous to obtain orientation after a significant motion.
    If the robot moves along a path on the moon, it might get lost due to an error
    in its local GPS systems or inertial measurement units. In order to build more
    robust systems, an image-based orientation of the camera is also computed and
    fused other sensor data to obtain more robust motion estimates.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卫星或航天器**：在卫星或机器人的远程操作中，在显著运动后获得方向是困难和错误的。如果机器人沿着月球上的路径移动，它可能会因为其局部GPS系统或惯性测量单元的错误而迷路。为了构建更健壮的系统，还计算并融合了摄像机的基于图像的方向和其他传感器数据，以获得更稳健的运动估计。'
- en: '**Augmented Reality**: With the boom in smartphones and apps and the availability
    of better hardware, several computer vision algorithms that use geometry information
    can now run in real time. **Augmented Reality** (**AR**) apps and games use geometrical
    properties from a sequence of images. These further combine this information with
    other sensor data to create a seamless AR experience and we can see a virtual
    object as if it is a real object placed on the table. Tracking planar objects
    and computing the relative positions of objects and the camera is crucial in these
    applications.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强现实**：随着智能手机和应用程序的兴起以及更好硬件的可用性，现在可以使用几何信息的几个计算机视觉算法可以实时运行。**增强现实**（**AR**）应用程序和游戏使用一系列图像的几何属性。这些进一步将此信息与其他传感器数据相结合，以创建无缝的AR体验，我们可以看到虚拟物体就像它是一个放在桌子上的真实物体。在这些应用中，跟踪平面物体和计算物体与摄像机的相对位置至关重要。'
- en: Image formation
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像形成
- en: 'The basic camera model is a pinhole camera, though the real-world cameras that
    we use are far more complex models. A pinhole camera is made up of a very small
    slit on a plane that allows the formation of an image as depicted in the following
    figure:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基本相机模型是针孔相机，尽管我们使用的现实世界相机要复杂得多。针孔相机由一个平面上非常小的缝隙组成，允许形成如图所示的图像：
- en: '![](img/326a14a8-9a7f-47ef-8bf8-1b6897d80f00.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/326a14a8-9a7f-47ef-8bf8-1b6897d80f00.png)'
- en: This camera converts a point in the physical world, often termed the *real world*,
    to a pixel on an image plane. The conversion follows the transformation of the
    three-dimensional coordinate to two-dimensional coordinates. Here in the image
    plane, the coordinates are denoted as where ![](img/ef95d229-0447-4c95-922a-d5c95b7d3b06.png),
    *P[i]* is any point on an image. In the physical world, the same point is denoted
    by ![](img/e2f62f14-34eb-4b77-a17b-627cdf391a4a.png), where *P[w]* is any point
    in the physical world with a global reference frame.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这台相机将物理世界中的点，通常称为*现实世界*，转换为图像平面上的一点。转换遵循三维坐标到二维坐标的变换。在这里，图像平面上的坐标表示为 ![](img/ef95d229-0447-4c95-922a-d5c95b7d3b06.png)，*P[i]*
    是图像上的任意一点。在物理世界中，相同的点表示为 ![](img/e2f62f14-34eb-4b77-a17b-627cdf391a4a.png)，其中
    *P[w]* 是具有全局参考框架的物理世界中的任意一点。
- en: '*P[i](x'', y'')* and *P[w](x, y, z)* can be related as, for an ideal pin hole
    camera:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*P[i](x'', y'')* 和 *P[w](x, y, z)* 可以通过理想的针孔相机相关联，如下所示：'
- en: '![](img/3b1b711b-8dfc-4770-994b-b61e4e047b74.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3b1b711b-8dfc-4770-994b-b61e4e047b74.png)'
- en: Here, *f* is focal length of the camera.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*f* 是相机的焦距。
- en: For further discussion on geometry of image formation, it is necessary to introduce
    the homogeneous coordinate system. The physical world coordinate system is referred
    to as **Euclidean coordinate system**. In the image plane, a point *P'* with *(x,
    y)* coordinates is represented in homogeneous coordinate system as *(x, y, 1)*.
    Similarly a point *P[w]* with *(x, y, z)* in world coordinates can be represented
    in homogeneous coordinate system as (x, y, z, 1) .
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像形成几何学的进一步讨论，有必要引入齐次坐标系。物理世界坐标系被称为**欧几里得坐标系**。在图像平面上，具有 *(x, y)* 坐标的点 *P'*
    在齐次坐标系中表示为 *(x, y, 1)*。同样，具有 *(x, y, z)* 的世界坐标的点 *P[w]* 可以在齐次坐标系中表示为 (x, y, z,
    1) 。
- en: To convert back from homogeneous to Euclidean, we need to divide by the last
    coordinate value. To convert a point on an image plane in homogeneous system as
    (x,y, w) to Euclidean system as (x/w, y/w) . Similarly, for a 3D point in a homogeneous
    system given by (x, y, z, w), the Euclidean coordinates are given by (x/w, y/w,
    z/ w). In this book, the use of homogeneous coordinate systems will be explicitly
    mentioned; otherwise we will see equations in the Euclidean coordinate system.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要从齐次坐标转换回欧几里得坐标，我们需要除以最后一个坐标值。将图像平面上齐次坐标系中的点(x, y, w)转换为欧几里得坐标系中的点(x/w, y/w)。同样，对于齐次坐标系中给出的3D点(x,
    y, z, w)，欧几里得坐标由(x/w, y/w, z/w)给出。在这本书中，我们将明确提及齐次坐标系统的使用；否则，我们将看到欧几里得坐标系中的方程。
- en: 'Image formation comes from transforming physical world coordinates to image
    plane coordinates but losing information about an extra dimension. This means
    that when we construct an image we are losing depth information for each pixel.
    As a result, converting back from image pixel coordinates to real-world coordinates
    is not possible. As shown in the following figure, for a point **P[I]** in the
    figure there can be an infinite number of points lying along the line. Points **P1**,
    **P2**, and **P3** have the same image pixel location, and therefore estimations
    of depth (distance from camera) are lost during image formation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图像形成是从物理世界坐标到图像平面坐标的转换，但丢失了关于额外维度的信息。这意味着当我们构建图像时，我们正在丢失每个像素的深度信息。因此，从图像像素坐标转换回真实世界坐标是不可能的。如图所示，对于图中的点**P[I]**，可以沿直线存在无限多个点。点**P1**、**P2**和**P3**具有相同的图像像素位置，因此在图像形成过程中丢失了深度（距离相机）的估计：
- en: '![](img/464d2f09-3ffa-4842-87c5-5a3781d1b612.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/464d2f09-3ffa-4842-87c5-5a3781d1b612.png)'
- en: 'Let us observe a point world from two images. If we know the optical center
    of a camera that constructs an image and the point location of two images, we
    can get much more information. The following figure explains **Epipolar Geometry**
    using two images:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从两个图像中观察一个世界点。如果我们知道构建图像的相机的光学中心和两个图像中的点位置，我们可以获得更多信息。以下图使用两个图像解释**极线几何**：
- en: '![](img/15a9a57e-f709-4eba-9dc7-3155994cf8c3.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/15a9a57e-f709-4eba-9dc7-3155994cf8c3.png)'
- en: 'In the previous figure, the camera centers **O[1]** and **O[2]** are connected
    to point **P[w]** in the world, and the plane forming the line **P[w]**, **O[1]**,
    **O[2]** is the epipolar plane. The points where the camera''s center line O[1]O[2] 
    intersects with the image are epipoles for the image. These may or may not lie
    on the image. In cases where both the image planes are parallel, the epipoles
    will lie at infinity. Here, we can define an epipolar constraint, as if we know
    the transformation between camera center **O[1]** and **O[2]** as translation
    T and rotation R, we can compute the location of point **P1** in **Image 1** to
    the corresponding location in **Image 2**. Mathematically, this is written as
    follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，相机中心**O[1]**和**O[2]**连接到世界中的点**P[w]**，形成线**P[w]**、**O[1]**、**O[2]**的平面是极线平面。相机中心线O[1]O[2]与图像相交的点是该图像的极点。这些点可能位于图像上，也可能不在图像上。在两个图像平面都平行的情况下，极点将位于无穷远处。在这里，我们可以定义一个极线约束，如果我们知道相机中心**O[1]**和**O[2]**之间的变换为平移T和旋转R，我们可以计算**P1**在**图像1**中的位置对应到**图像2**中的位置。数学上，这可以表示如下：
- en: '![](img/8137ecdd-e645-4387-bee2-1613721c4bee.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8137ecdd-e645-4387-bee2-1613721c4bee.png)'
- en: 'Inversely, if know the location of corresponding points in two images, we would
    like to compute the rotation matrix R and translation matrix T between the two
    camera centers. Here, if the two cameras are different, the camera centers can
    be at the different distance from the image plane and, therefore, we would require
    camera intrinsic parameters too. Mathematically, this is written as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，如果我们知道两个图像中对应点的位置，我们希望计算两个相机中心之间的旋转矩阵R和平移矩阵T。在这里，如果两个相机不同，相机中心可以位于图像平面的不同距离处，因此我们还需要相机内参参数。数学上，这可以表示如下：
- en: '![](img/85ab1610-68d7-4026-8425-da6370aa5def.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/85ab1610-68d7-4026-8425-da6370aa5def.png)'
- en: Here, *F* is called the **fundamental matrix** and *K* is our **camera intrinsic
    matrix** for each camera. Computing *F*, we can know the correct transformation
    between the two camera poses and can convert any point on one image plane to another.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*F*被称为**基本矩阵**，*K*是每个相机的**相机内参矩阵**。计算*F*，我们可以知道两个相机姿态之间的正确变换，并将一个图像平面上的任何点转换为另一个图像平面。
- en: In the next section, we will see transformations between images and their applications.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到图像之间的变换及其应用。
- en: Aligning images
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像配准
- en: Image alignment is a problem for computing a transformation matrix so that on
    applying that transformation to an input image, it can be converted to the target
    image plane. As a result, the resulting images look like they are stitched together
    and form a continuous larger image.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图像配准是计算变换矩阵以将输入图像应用于该变换，从而将其转换为目标图像平面的问题。因此，生成的图像看起来像是拼接在一起，形成一个连续的大图像。
- en: 'Panorama is one such example of aligning images, where we collect images of
    a scene with changing camera angles and the resulting image is a combination of
    images aligned. A resulting image is as shown, as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 全景是图像配准的一个示例，其中我们收集具有变化相机角度的场景图像，生成的图像是配准图像的组合。结果图像如下所示：
- en: '![](img/f75b5df7-a580-47da-bfd8-cae9d1f85a5e.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f75b5df7-a580-47da-bfd8-cae9d1f85a5e.png)'
- en: In the preceding figure, an example of panorama creation is shown. Using a camera,
    we collect multiple images for the same scene by adding overlapping regions. As
    the camera is moved, often, the pose changes significantly, so therefore for different
    poses of the camera a transformation matrix is computed.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，展示了全景创建的一个示例。使用相机，我们通过添加重叠区域来收集同一场景的多个图像。由于相机移动时，姿态通常会显著变化，因此对于相机的不同姿态，需要计算一个变换矩阵。
- en: 'Let''s get started with a basic method to compute this transformation matrix.
    The following code works inside Jupyter notebook too. In the following block of
    code, we define a function to compute **oriented BRIEF** (**ORB**) keypoints.
    There is a descriptor for each keypoint also:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从计算这个变换矩阵的基本方法开始。以下代码在Jupyter笔记本中也可以工作。在以下代码块中，我们定义了一个函数来计算**定向BRIEF**（**ORB**）特征点。每个特征点也有一个描述符：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once we have feature keypoints, we match them using a brute force matcher,
    as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了特征点，我们就使用暴力匹配器进行匹配，如下所示：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This is our main function for computing the fundamental matrix:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的主要函数，用于计算基本矩阵：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the next section, we will extend relative transformation between images to
    compute camera pose and also estimate the trajectory of the camera.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将扩展图像之间的相对变换来计算相机姿态并估计相机的轨迹。
- en: Visual odometry
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉测距仪
- en: Odometry is the process of incrementally estimating the position of a robot
    or device. In the case of a wheeled robot, it uses wheel motion or inertial measurement
    using tools such as gyroscopes or accelerometers to estimate the robot's position
    by summing over wheel rotations. Using **visual odometry** (**VO**), we can estimate
    the odometry of cameras using only image sequences by continuously estimating
    camera motion.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 测距仪是逐步估计机器人或设备位置的过程。对于轮式机器人，它使用轮子运动或使用陀螺仪或加速度计等工具进行惯性测量，通过求和轮子旋转来估计机器人的位置。使用**视觉测距仪**（**VO**），我们可以通过连续估计相机运动，仅使用图像序列来估计相机的测距仪。
- en: 'A major use of VO is in autonomous robots like drones, where gyroscopes and
    accelerometers are not robust enough for motion estimation. However, there are
    several assumptions and challenges in using VO:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: VO的主要用途是在无人机等自主机器人中，因为陀螺仪和加速度计在运动估计方面不够稳健。然而，在使用VO时存在几个假设和挑战：
- en: Firstly, objects in the scene for the camera should be static. While the camera
    captures a sequence of the image, the only moving object should be the camera
    itself.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，对于相机的场景中的物体应该是静态的。当相机捕捉一系列图像时，唯一移动的物体应该是相机本身。
- en: Moreover, during the estimation of VO, if there are significant illumination
    changes, like light source appearance, drastic changes to pixel values might occur
    in subsequent images. As a result, VO suffers from large errors or complete dysfunction.
    The same case applies to dark environments; due to the lack of illumination, VO
    is not able to estimate robust motion.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，在估计VO期间，如果存在显著的照明变化，如光源出现，后续图像中的像素值可能会发生剧烈变化。因此，VO会遭受大的误差或完全失效。同样，在黑暗环境中，由于缺乏照明，VO无法估计稳健的运动。
- en: 'The process of VO is described as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: VO的过程描述如下：
- en: Initialize the starting position as the origin, for the frame of reference.
    All the subsequent motion estimation is done with respect to this frame.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将起始位置初始化为原点，作为参考框架。所有后续的运动估计都是相对于这个框架进行的。
- en: As an image arrives, compute features and match corresponding features with
    previous frames to get a transformation matrix.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当图像到达时，计算特征并将相应的特征与前一帧匹配以获得变换矩阵。
- en: Use the historical transformation matrix between all subsequent frames to compute
    the trajectory of the camera.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所有后续帧之间的历史变换矩阵来计算相机的轨迹。
- en: 'This process is shown in the following figure:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程如下图中所示：
- en: '![](img/b2d2ae77-0f07-4849-923e-e3bdf08d3532.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b2d2ae77-0f07-4849-923e-e3bdf08d3532.png)'
- en: Here, **I[i]** is the i^(th) image received from the camera and **T[ij]** is
    the transformation matrix computed using feature matching between *i* and *j*
    images. The trajectory of camera motion is shown with stars, where *P[i]* is the
    estimated pose of the camera at the *i*^(th) image. This can be a two-dimensional
    pose with (*x*, *y*) angle as well as a three-dimensional pose. Each *P[j]* is
    computed as applying the transformation *T[ij]* on *P[i]*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**I[i]** 是从相机接收到的第i幅图像，**T[ij]** 是使用图像i和j之间的特征匹配计算出的变换矩阵。相机运动轨迹用星号表示，其中**P[i]**
    是第i幅图像中相机的估计姿态。这可能是一个二维姿态，具有(*x*, *y*)角度，也可能是一个三维姿态。每个**P[j]** 都是通过将变换**T[ij]**
    应用到**P[i]** 上来计算的。
- en: 'Other than the assumption mentioned earlier, there are a few limitations to
    VO estimation:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面提到的假设之外，VO估计还有一些局限性：
- en: As more images are observed from the sequence, the errors in trajectory estimation
    are accumulated. This results in an overall drift in the computed track of camera
    motion.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着从序列中观察到的图像越来越多，轨迹估计中的误差会累积。这导致计算出的相机运动轨迹出现整体漂移。
- en: In cases of sudden motion in camera,  the image feature match between the corresponding
    two images will be significantly erroneous. As a result, the estimated transformation
    between the frames will also have huge errors and, therefore, the overall trajectory
    of camera motion gets highly distorted.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在相机突然运动的情况下，对应两幅图像之间的图像特征匹配将出现显著错误。因此，帧间估计的变换也将有巨大的误差，因此，相机运动的整体轨迹会高度扭曲。
- en: Visual SLAM
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉SLAM
- en: SLAM refers to Simultaneous Localization and Mapping and is one of the most
    common problems in robot navigation. Since a mobile robot does not have hardcoded
    information about the environment around itself, it uses sensors onboard to construct
    a representation of the region. The robot tries to estimate its position with
    respect to objects around it like trees, building, and so on. This is, therefore,
    a chicken-egg problem, where the robot first tries to localize itself using objects
    around it and then uses its obtained location to map objects around it; hence
    the term *Simultaneous  Localization and Mapping*. There are several methods for
    solving the SLAM problem. In this section, we will discuss special types of SLAM
    using a single RGB camera.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: SLAM代表同时定位与建图，是机器人导航中最常见的问题之一。由于移动机器人没有关于其周围环境的硬编码信息，它使用机载传感器来构建该区域的表示。机器人试图估计其相对于周围物体（如树木、建筑物等）的位置。因此，这是一个“先有鸡还是先有蛋”的问题，其中机器人首先尝试使用周围的物体来定位自己，然后使用其获得的位置来绘制周围的物体；因此，术语为“同时定位与建图”。解决SLAM问题有几种方法。在本节中，我们将讨论使用单个RGB相机进行特殊类型的SLAM。
- en: 'Visual SLAM methods extend visual odometry by computing a more robust camera
    trajectory as well as constructing a robust representation of the environment.
    An overview of Visual SLAM in action is shown in the following figure:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉SLAM方法通过计算更稳健的相机轨迹以及构建环境的稳健表示来扩展视觉里程计。以下图中展示了视觉SLAM的实际应用概述：
- en: '![](img/f8abfbd3-a703-44ca-a782-c9728fe0ea75.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f8abfbd3-a703-44ca-a782-c9728fe0ea75.png)'
- en: This is an overview of a generic SLAM method composed of an undirected graph.
    Each node of the graph is composed of a keyframe which represents unique information
    about the world and also contains the camera pose (*x*,*y*, angle) for the location.
    In between, keyframes are frames that overlap significantly with the keyframes
    scene, however, they help in computing robust estimates of pose for the next frame.
    Here, a camera starts the process by initializing a keyframe at the origin. As
    the camera moves along a trajectory, the SLAM system updates the graph by adding
    keyframes or frames based on criteria. If the camera returns back to a previously
    seen area, it links up with the old frame, creating a cyclic structure in the
    graph. This is often called **loop closure** and helps correct the overall graph
    structure. The edges connecting nodes to another in the graph are usually weighted
    with a  transformation matrix between the pose of the two nodes. Overall, the
    graph structure is corrected by improving the position of keyframes. This is done
    by minimizing overall error. Once a graph is constructed, it can be saved and
    used for localizing a camera by matching to the nearest keyframe.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个由无向图组成的通用SLAM方法的概述。图中的每个节点由一个关键帧组成，它代表关于世界的独特信息，并且包含该位置的相机姿态（*x*，*y*，角度）。在它们之间，关键帧是与场景中的关键帧显著重叠的帧，然而，它们有助于计算下一帧姿态的鲁棒估计。在这里，相机通过在原点初始化一个关键帧来开始这个过程。随着相机沿着轨迹移动，SLAM系统通过根据标准添加关键帧或帧来更新图。如果相机返回到一个之前看到过的区域，它将与旧帧连接起来，在图中创建一个循环结构。这通常被称为**闭环检测**，有助于纠正整体图结构。连接图中节点到另一个节点的边通常用两个节点姿态之间的转换矩阵加权。总体而言，通过改进关键帧的位置来纠正图结构。这是通过最小化总体误差来完成的。一旦构建了一个图，就可以将其保存并用于通过匹配到最近的关键帧来定位相机。
- en: 'In this section, we will see a popular robust method, ORB SLAM, using monocular
    cameras. This method constructs a graph structure similar to that which was shown
    previously to keep track of camera pose and works on RGB image frames from a simple
    camera. The steps involved can be summarized as:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到一个使用单目相机的流行鲁棒方法，即ORB SLAM。这种方法构建了一个类似于之前展示的图结构，以跟踪相机姿态，并处理来自简单相机的RGB图像帧。涉及到的步骤可以总结如下：
- en: '**Input**: In the case of the monocular camera, the input is a single captured
    frame.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入**：在单目摄像头的情况下，输入是一个捕获的单帧。'
- en: '**Initialization**: Once the process starts, a map is initialized with the
    origin, and the first node of a keyframe graph is constructed.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化**：一旦进程开始，就会用一个原点初始化一个地图，并构建一个关键帧图的第一个节点。'
- en: 'There are three threads that run in parallel for the system:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 系统中并行运行三个线程：
- en: '**Tracking**: For each incoming frame, ORB features are extracted for matching.
    These features are matched with previously seen frames and are then used to compute
    the relative pose of the current frame. This also decides if the current frame
    is to be kept as a keyframe or used as a normal frame.'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪**：对于每个进入的帧，提取ORB特征以进行匹配。这些特征与之前看到的帧进行匹配，然后用于计算当前帧的相对姿态。这也决定了当前帧是作为关键帧保留还是用作普通帧。'
- en: '**Local mapping**: If a new keyframe is determined from tracking, the overall
    map is updated with the insertion of a new node in the graph. While a new connection
    between neighbourhood keyframes is formed, redundant connections are removed.'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部映射**：如果从跟踪中确定了一个新的关键帧，则通过在图中插入一个新节点来更新整体地图。在形成新的邻域关键帧之间的连接的同时，会移除冗余连接。'
- en: '**Loop closure**: If there is a previously seen keyframe that matches the current
    keyframe, a loop is formed. This gives extra information about drift caused by
    the trajectory of the camera pose and as a result, all node poses in the graph
    map is corrected by an optimization algorithm.'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**闭环检测**：如果有一个之前看到的关键帧与当前关键帧匹配，就会形成一个闭环。这提供了关于由相机姿态轨迹引起的漂移的额外信息，因此，通过优化算法，图中的所有节点姿态都得到了纠正。'
- en: In the following section, we will use an implementation of ORB SLAM2 from [https://github.com/raulmur/ORB_SLAM2](https://github.com/raulmur/ORB_SLAM2).
    This is not a Python-based implementation. The instruction provided there can
    be used to build the package and can be used to see visual SLAM. However, for
    demonstration purposes, we will use a Docker container version of it.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用来自[https://github.com/raulmur/ORB_SLAM2](https://github.com/raulmur/ORB_SLAM2)的ORB
    SLAM2的实现。这不是一个基于Python的实现。那里提供的说明可用于构建包，并可用于查看视觉SLAM。然而，出于演示目的，我们将使用它的Docker容器版本。
- en: A Docker is a container platform that provides the distributed shipping of an
    environment as if they are packed inside a ship container,  as well as code to
    run applications. We need to install the Docker platform and pull an image of
    the environment, as well as the code. The environment inside the image is independent
    of the platform we use, as long as the Docker platform is installed. If you want
    to learn more about Docker and containers, the following website provides more
    details, as well as installation instructions: [https://www.docker.com/what-docker](https://www.docker.com/what-docker).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 是一个容器平台，它提供了一种将环境分布式地打包在船用集装箱中，以及运行应用程序的代码的方式。我们需要安装 Docker 平台并拉取环境的镜像，以及代码。镜像内的环境与我们使用的平台无关，只要安装了
    Docker 平台即可。如果您想了解更多关于 Docker 和容器的内容，以下网站提供了更多详细信息以及安装说明：[https://www.docker.com/what-docker](https://www.docker.com/what-docker)。
- en: 'Once Docker is installed, we can begin with the following steps for ORB SLAM
    2\. Let''s start by pulling a Docker image (this is similar to cloning a repository)
    for ORB SLAM:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 安装完成后，我们可以开始以下步骤来运行 ORB SLAM 2。让我们首先拉取 ORB SLAM 的 Docker 镜像（这类似于克隆仓库）：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This will download the environment for the package and pre-build the ORB SLAM2
    repository so that we don't have to build it again. All the dependencies for this
    repository are already satisfied inside the Docker image.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这将下载该软件包的环境并预先构建 ORB SLAM2 仓库，这样我们就不需要再次构建它。这个仓库的所有依赖项已经在 Docker 镜像内部满足。
- en: Once the Docker image is downloaded, let's get started with downloading the
    dataset. In this section, we will use the `TUM RGBD` dataset, which was collected
    specifically to evaluate SLAM and VO methods. Earlier in this chapter, under dataset
    and libraries, we saw how to download this dataset. We will use the extracted
    dataset in the following section.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦下载了 Docker 镜像，让我们开始下载数据集。在本节中，我们将使用 `TUM RGBD` 数据集，该数据集专门收集用于评估 SLAM 和 VO
    方法。在本章早期，在数据集和库部分，我们看到了如何下载这个数据集。我们将在下一节中使用提取的数据集。
- en: Since this implementation of ORB SLAM uses a GUI interface to output the results,
    we will first add the GUI interface to the Docker image. The following code assumes
    a Linux environment.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个 ORB SLAM 的实现使用 GUI 界面输出结果，我们首先将 GUI 界面添加到 Docker 镜像中。以下代码假设在 Linux 环境下。
- en: 'For the GUI output from ORB SLAM, add this as the first step, otherwise, visual
    SLAM will run but there will be an error:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ORB SLAM 的 GUI 输出，请将此作为第一步添加，否则视觉 SLAM 将运行但会出现错误：
- en: '[PRE5]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, let''s launch the downloaded image using the Docker platform, though with
    several parameters:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 Docker 平台启动下载的镜像，尽管带有几个参数：
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, the `-e` and `-v` parameters are used to set the display environment
    for GUI. The dataset downloaded before is shared inside Docker using `- v $PATH_TO_DOWNLOADED_DATASET:$PATH_INSIDE_DOCKER`.
    Finally, the name of the image is `orb-slam: latest`*,* which we downloaded earlier
    using Docker pull, and we asked it to run bash inside Docker using `/bin/bash`.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，`-e` 和 `-v` 参数用于设置 GUI 的显示环境。之前下载的数据集通过 `- v $PATH_TO_DOWNLOADED_DATASET:$PATH_INSIDE_DOCKER`
    在 Docker 内部共享。最后，镜像的名称是 `orb-slam: latest`*，这是我们之前使用 Docker pull 下载的，并要求它在 Docker
    内运行 bash `/bin/bash`。'
- en: 'On running the previous command, we can see a change in the Terminal, as if
    we logging in to a new computer Terminal. Let''s go and run ORB-SLAM as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的命令后，我们可以在终端中看到变化，就像登录到一个新的计算机终端一样。让我们按照以下方式运行 ORB-SLAM：
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, the first parameter is to run Monocular Visual SLAM, as there are other
    methods too. The other parameters are to run the type of dataset that we had downloaded
    earlier. If there is any change in the dataset, these parameters are to be changed
    accordingly.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，第一个参数是运行单目视觉 SLAM，因为还有其他方法。其他参数是运行我们之前下载的数据集类型。如果数据集有任何变化，这些参数需要相应地更改。
- en: 'On this command, after some time there will be two windows, shown as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个命令执行一段时间后，将出现两个窗口，如下所示：
- en: '![](img/30ce9f93-8e72-400e-a603-0bcf7713d6d2.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/30ce9f93-8e72-400e-a603-0bcf7713d6d2.png)'
- en: Here, the window on the right is the input dataset, with the keypoints detected
    in each frame. While the window on the left details the Visual SLAM happening.
    As we can see there are blue boxes that show the keyframe graph creation, with
    the current state of the position of the camera and its links with the historical
    position. As the camera in the dataset is moved, the graph is created and adjusted
    as more observations are found. The result is an accurate trajectory of the camera
    as well as adjusted keypoints.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，右侧的窗口是输入数据集，其中显示了每帧中检测到的关键点。而左侧的窗口详细展示了正在发生的视觉SLAM。正如我们所见，有蓝色框显示关键帧图的创建，以及当前相机位置及其与历史位置的联系。随着数据集中相机的移动，随着更多观测值的发现，图被创建和调整。结果是相机精确的轨迹以及调整后的关键点。
- en: Summary
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, the aim was to view computer vision from a geometrical point
    of view. Starting with understanding how an image is formed using a pinhole camera,
    there was a discussion on how to incorporate three-dimensional worlds using multi-image
    formation. We saw an explanation of Visual Odometry with an introduction to Visual
    SLAM. The various steps involved in SLAM were explained and a demo of using ORB-SLAM
    was also shown, so that we could see a SLAM operation as it happened. This is
    basic motivation to extend the SLAM solution for various other datasets, and so
    create interesting applications.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，目标是从一个几何的角度来审视计算机视觉。从理解如何使用针孔相机形成图像开始，讨论了如何通过多图像形成结合三维世界。我们解释了视觉里程计，并介绍了视觉SLAM。解释了SLAM中涉及的各个步骤，并展示了ORB-SLAM的使用演示，以便我们可以看到SLAM操作的实际发生。这是扩展SLAM解决方案以适应各种其他数据集的基本动机，从而创建有趣的应用。
- en: References
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Sturm Jürgen, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers.
    *A Benchmark for the Evaluation of RGB-D SLAM Systems*. In Intelligent Robots
    and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 573-580\. IEEE,
    2012.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sturm Jürgen, Nikolas Engelhard, Felix Endres, Wolfram Burgard, 和 Daniel Cremers.
    *RGB-D SLAM系统评估的基准*. 在《智能机器人系统》（IROS）2012年IEEE/RSJ国际会议上，第573-580页。IEEE，2012年。
- en: 'Mur-Artal Raul, Jose Maria Martinez Montiel, and Juan D. Tardos. *ORB-SLAM:
    A Versatile and Accurate Monocular SLAM System*. IEEE Transactions on Robotics 31,
    no. 5 (2015): 1147-1163.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mur-Artal Raul, Jose Maria Martinez Montiel, 和 Juan D. Tardos. *ORB-SLAM：一种通用且精确的单目SLAM系统*.
    IEEE机器人学汇刊，第31卷第5期（2015年）：1147-1163。
- en: 'Rublee Ethan, Vincent Rabaud, Kurt Konolige, and Gary Bradski. *ORB: an efficient
    alternative to SIFT or SURF*. In Computer Vision (ICCV), 2011 IEEE international
    conference on, pp. 2564-2571\. IEEE, 2011.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rublee Ethan, Vincent Rabaud, Kurt Konolige, 和 Gary Bradski. *ORB：一种比SIFT或SURF更有效的替代方案*.
    在《计算机视觉》（ICCV）2011年IEEE国际会议上，第2564-2571页。IEEE，2011年。
