- en: '*Chapter 8*: Jumpstarting ML with SageMaker JumpStart and Autopilot'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 8 章*：使用 SageMaker JumpStart 和 Autopilot 快速启动机器学习'
- en: '**SageMaker JumpStart** offers complete solutions for select use cases as a
    starter kit for the world of **machine learning** (**ML**) with Amazon SageMaker
    without any code development. SageMaker JumpStart also catalogs popular pretrained
    **computer vision** (**CV**) and **natural language processing** (**NLP**) models
    for you to easily deploy or fine-tune for your dataset. **SageMaker Autopilot**
    is an AutoML solution that explores your data, engineers features on your behalf,
    and trains an optimal model from various algorithms and hyperparameters. You don''t
    have to write any code: Autopilot does it for you and returns notebooks to show
    you how it does it.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**SageMaker JumpStart** 作为 Amazon SageMaker 机器学习（**ML**）世界入门套件，为选定的用例提供完整解决方案，无需任何代码开发。SageMaker
    JumpStart 还为您整理了流行的预训练 **计算机视觉**（**CV**）和 **自然语言处理**（**NLP**）模型，以便您轻松部署或微调到您的数据集。**SageMaker
    Autopilot** 是一种 AutoML 解决方案，它探索您的数据，代表您构建特征，并从各种算法和超参数中训练最优模型。您无需编写任何代码：Autopilot
    会为您完成，并返回笔记本以展示它是如何做到的。'
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Launching a SageMaker JumpStart solution
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动 SageMaker JumpStart 解决方案
- en: Deploying and fine-tuning a model from the SageMaker JumpStart model zoo
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 SageMaker JumpStart 模型库部署和微调模型
- en: Creating a high-quality model with SageMaker Autopilot
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SageMaker Autopilot 创建高质量模型
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, you need to have permission to use JumpStart templates. You
    can confirm it from your domain and user profile. The code used in this chapter
    can be found at [https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter08](https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter08).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您需要拥有使用 JumpStart 模板的权利。您可以从您的域和用户配置文件中确认。本章使用的代码可以在 [https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter08](https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter08)
    找到。
- en: Launching a SageMaker JumpStart solution
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动 SageMaker JumpStart 解决方案
- en: SageMaker JumpStart is particularly useful if you would like to learn a set
    of best practices for how AWS services should be used together to create an ML
    solution. You can do the same, too. Let's open up the JumpStart browser. There
    are multiple ways to open it, as shown in *Figure 8.1*. You can open it from the
    SageMaker Studio Launcher on the right or from the JumpStart asset browser in
    the left sidebar.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想学习一套 AWS 服务如何一起使用以创建 ML 解决方案的最佳实践，SageMaker JumpStart 特别有用。您也可以这样做。让我们打开
    JumpStart 浏览器。打开它的方式有多种，如图 *图 8.1* 所示。您可以从右侧的 SageMaker Studio 启动器或左侧的 JumpStart
    资产浏览器打开它。
- en: '![Figure 8.1 – Opening the JumpStart browser from the Launcher or the left
    sidebar'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.1 – 从启动器或左侧侧边栏打开 JumpStart 浏览器'
- en: '](img/B17447_08_001.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17447_08_001.jpg](img/B17447_08_001.jpg)'
- en: Figure 8.1 – Opening the JumpStart browser from the Launcher or the left sidebar
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 从启动器或左侧侧边栏打开 JumpStart 浏览器
- en: A new tab named **SageMaker JumpStart** will pop up in the main working area.
    Go to the **Solutions** section and click **View all**, as shown in *Figure 8.2*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在主工作区域将弹出一个名为 **SageMaker JumpStart** 的新标签页。转到 **解决方案** 部分，点击 **查看所有**，如图 *图
    8.2* 所示。
- en: '![Figure 8.2 – Viewing all solutions in JumpStart'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.2 – 在 JumpStart 中查看所有解决方案'
- en: '](img/B17447_08_02.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17447_08_02.jpg](img/B17447_08_02.jpg)'
- en: Figure 8.2 – Viewing all solutions in JumpStart
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 在 JumpStart 中查看所有解决方案
- en: Let's next move on to the solutions catalog for industries.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们转向行业解决方案目录。
- en: Solution catalog for industries
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 行业解决方案目录
- en: There are more than a dozen solutions available in JumpStart as shown in *Figure
    8.3*. These solutions are based on use cases spanning multiple industries, including
    manufacturing, retail, and finance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 8.3* 所示，JumpStart 中有十多种解决方案可供选择。这些解决方案基于涵盖多个行业的用例，包括制造、零售和金融。
- en: '![Figure 8.3 – JumpStart solution catalog – Click each card to see more information'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.3 – JumpStart 解决方案目录 – 点击每张卡片查看更多信息'
- en: '](img/B17447_08_003.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17447_08_003.jpg](img/B17447_08_003.jpg)'
- en: Figure 8.3 – JumpStart solution catalog – Click each card to see more information
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – JumpStart 解决方案目录 – 点击每张卡片查看更多信息
- en: They are created by AWS developers and architects who know the given industry
    and use case. You can read more about each use case by clicking on the card. You
    will be greeted with a welcome page describing the use case, methodology, dataset,
    solution architecture, and any other external resources. On each solution page,
    you should also see a **Launch** button, which will deploy the solution and all
    cloud resources into your AWS account from a CloudFormation template.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 它是由了解该行业和用例的AWS开发人员和架构师创建的。您可以通过点击卡片了解更多关于每个用例的信息。您将看到一个欢迎页面，描述用例、方法、数据集、解决方案架构以及任何其他外部资源。在每个解决方案页面上，您还应该看到一个**启动**按钮，该按钮将从CloudFormation模板部署解决方案和所有云资源到您的AWS账户。
- en: Let's use the **Product Defect Detection** solution from the catalog as our
    example, and we will walk through the deployment and the notebooks together.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以目录中的**产品缺陷检测**解决方案为例，我们将一起了解部署和笔记本。
- en: Deploying the Product Defect Detection solution
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署产品缺陷检测解决方案
- en: 'Visual inspection is widely adopted as a quality control measure in manufacturing
    processes. Quality control used to be a manual process where staff members would
    visually inspect the product either on the line or via imagery captured with cameras.
    However, manual inspection does not scale for the large quantities of products
    created in factories today. ML is a powerful tool that can identify product defects
    at an error rate that may, if trained properly, be even better than a human inspector.
    The **Product Defect Detection** SageMaker JumpStart solution is a great starting
    point to jump-start your CV project to detect defects in images using a state-of-the-art
    deep learning model. You will see how SageMaker manages training with a PyTorch
    script, and how model hosting is used. You will also learn how to make inferences
    against a hosted endpoint. The dataset is a balanced dataset across six types
    of surface defects and contains ground truths for both classification and drawing
    bounding boxes. Please follow these steps and read through the content of the
    notebooks:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉检查在制造过程中被广泛采用作为一种质量控制措施。质量控制过去是一个手动过程，员工会在生产线或通过相机捕获的图像上对产品进行视觉检查。然而，对于今天工厂中生产的大量产品，手动检查无法扩展。机器学习是一个强大的工具，可以以可能比经过适当训练的人类检查员更低的错误率识别产品缺陷。**产品缺陷检测**
    SageMaker JumpStart解决方案是一个很好的起点，可以帮助您启动CV项目，使用最先进的深度学习模型检测图像中的缺陷。您将了解SageMaker如何使用PyTorch脚本进行训练管理，以及如何使用模型托管。您还将学习如何对托管端点进行推理。数据集是一个平衡的数据集，包含六种表面缺陷的类型，并包含分类和绘制边界框的地面真实值。请按照以下步骤阅读笔记本的内容：
- en: From the **Solution** catalog, please select **Product Defect Detection in Images**.
    As shown in *Figure 8.4*, you can read about the solution on the main page. You
    can learn about the sample data, the algorithm, and the cloud solution architecture.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**解决方案**目录中，请选择**产品缺陷检测图像**。如图8.4所示，您可以在主页上了解解决方案。您可以了解样本数据、算法和云解决方案架构。
- en: '![Figure 8.4 – Main page of the Product Defect Detection in Images solution'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.4 – 产品缺陷检测图像解决方案的主页'
- en: '](img/B17447_08_04.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17447_08_04.jpg)'
- en: Figure 8.4 – Main page of the Product Defect Detection in Images solution
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 产品缺陷检测图像解决方案的主页
- en: Hit the **Launch** button, as shown in *Figure 8.4*, to start the deployment.
    You should see the deployment in progress on the screen. What is happening is
    that we just initiated a resource deployment using **AWS CloudFormation** in the
    background. AWS CloudFormation is a service that helps create, provision, and
    manage AWS resources in an orderly fashion through a template in JSON or YAML
    declarative code. This deployment takes a couple of minutes.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击如图8.4所示的**启动**按钮开始部署。您应该在屏幕上看到部署进度。正在发生的事情是我们刚刚在后台使用**AWS CloudFormation**启动了一个资源部署。AWS
    CloudFormation是一种服务，它通过JSON或YAML声明性代码中的模板，以有序的方式帮助创建、配置和管理AWS资源。此部署需要几分钟。
- en: Once the solution becomes `0_demo.ipynb`, from the solution. This notebook is
    the first of four notebooks that are deployed as part of the CloudFormation setup
    into your home directory at `S3Downloads/jumpstart-prod-dfd_xxxxxxx/notebooks/`.
    The notebook requires the **SageMaker JumpStart PyTorch 1.0** kernel as we are
    going to build a PyTorch-based solution. The kernel startup might take a minute
    or two if this is the first time using the kernel.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦解决方案变为 `0_demo.ipynb`，从解决方案中运行。这个笔记本是四个笔记本中的第一个，作为 CloudFormation 设置的一部分部署到你的家目录
    `S3Downloads/jumpstart-prod-dfd_xxxxxxx/notebooks/` 中。该笔记本需要 **SageMaker JumpStart
    PyTorch 1.0** 内核，因为我们将要构建一个基于 PyTorch 的解决方案。如果这是第一次使用内核，内核启动可能需要一两分钟。
- en: Run all the cells in the `0_demo.ipynb` notebook. This notebook downloads the
    `NEU-DET` detection dataset to the filesystem and creates a SageMaker hosted endpoint
    using the SageMaker SDK's `sagemaker.pytorch.PyTorchModel` class for a pretrained
    PyTorch model. At the end of the notebook, you should see a figure showing the
    patches detected by the pretrained model compared to the ground truth, as in *Figure
    8.5*.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `0_demo.ipynb` 笔记本中的所有单元格。该笔记本将 `NEU-DET` 检测数据集下载到文件系统，并使用 SageMaker SDK
    的 `sagemaker.pytorch.PyTorchModel` 类创建一个用于预训练 PyTorch 模型的 SageMaker 托管端点。在笔记本末尾，你应该看到一个显示预训练模型检测到的补丁与真实值相比的图像，如图
    *图 8.5* 所示。
- en: '![Figure 8.5 – Final output of the 0_demo.ipynb notebook, showing a steel surface
    example, the ground truth, and the model prediction by a pretrained model'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.5 – 0_demo.ipynb 笔记本最终输出，展示了一个钢表面示例、真实值和预训练模型进行的模型预测'
- en: '](img/B17447_08_05.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17447_08_05.jpg)'
- en: Figure 8.5 – Final output of the 0_demo.ipynb notebook, showing a steel surface
    example, the ground truth, and the model prediction by a pretrained model
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 0_demo.ipynb 笔记本最终输出，展示了一个钢表面示例、真实值和预训练模型进行的模型预测
- en: This notebook demonstrates a key flexibility SageMaker offers, that is, you
    can bring a model trained from outside of SageMaker and host it in SageMaker.
    To create a SageMaker model from a PyTorch model, you need the model file `.pt`/`.pth`
    archived in a `model.tar.gz` archive and an entry point, `detector.py` script
    in this case, that instructs how the inference should be made. We can take a look
    at the `detector.py` script to learn more.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本演示了 SageMaker 提供的关键灵活性，即你可以将 SageMaker 外部训练的模型带到 SageMaker 中托管。要从 PyTorch
    模型创建 SageMaker 模型，你需要将模型文件 `.pt`/`.pth` 存档在 `model.tar.gz` 归档中，并有一个入口点，在这个例子中是
    `detector.py` 脚本，该脚本指示如何进行推理。我们可以查看 `detector.py` 脚本来了解更多信息。
- en: '(Optional) Add a new cell and fill in the following commands:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （可选）添加一个新的单元格并填写以下命令：
- en: '[PRE0]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will get the entire code base locally. Please open the `detector.py` file
    and locate the part that SageMaker uses to make inferences:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这将获取整个代码库到本地。请打开 `detector.py` 文件，找到 SageMaker 用于进行推理的部分：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: SageMaker requires at least a `model_fn(model_dir)` function when importing
    a PyTorch model to instruct how the model is defined. In this example, `Detection()`class
    is a `GeneralizedRCNN` model defined in `S3Downloads/jumpstart-prod-dfd_xxxxxx/notebooks/sagemaker_defect_detection/models/ddn.py`
    with weights loaded from the provided model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当导入 PyTorch 模型时，SageMaker 至少需要一个 `model_fn(model_dir)` 函数来指示模型是如何定义的。在这个例子中，`Detection()`
    类是一个在 `S3Downloads/jumpstart-prod-dfd_xxxxxx/notebooks/sagemaker_defect_detection/models/ddn.py`
    中定义的 `GeneralizedRCNN` 模型，其权重是从提供的模型中加载的。
- en: Note
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Other inference related functions you can implement include the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以实现的其它相关推理函数包括以下内容：
- en: 'Deserializing the invoke request body into an object we can perform prediction
    on:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 将调用请求体反序列化为我们可以对其执行预测的对象：
- en: '`input_object = input_fn(request_body, request_content_type)`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_object = input_fn(request_body, request_content_type)`'
- en: 'Performing prediction on the deserialized object with the loaded model:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用加载的模型对反序列化的对象进行预测：
- en: '`prediction = predict_fn(input_object, model)`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`prediction = predict_fn(input_object, model)`'
- en: 'Serializing the prediction result into the desired response content type:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 将预测结果序列化为所需的响应内容类型：
- en: '`output = output_fn(prediction, response_content_type)`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`output = output_fn(prediction, response_content_type)`'
- en: SageMaker has default implementations for these three functions if you don't
    override them. If you have a custom approach for making inferences, you can override
    these functions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有覆盖这些函数，SageMaker 为这三个函数提供了默认实现。如果你有自定义的推理方法，你可以覆盖这些函数。
- en: Proceed to the end of the notebook and click on `1_retrain_from_checkpoint.ipynb`.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续到笔记本的末尾并点击 `1_retrain_from_checkpoint.ipynb`。
- en: 'Run all the cells in `1_retrain_from_checkpoint.ipynb`. This notebook fine-tunes
    the pretrained model from a checkpoint with the downloaded dataset for a few more
    epochs. The solution includes training code in `detector.py` from `osp.join(sources,
    "source_dir.tar.gz")`. The solution uses the SageMaker SDK''s PyTorch estimator
    to create a training job that launches an on-demand compute resource of one `ml.g4dn.2xlarge`
    instance and trains it from a provided pretrained checkpoint. The training takes
    about 10 minutes. The following lines of code show how you can feed the training
    data and a pretrained checkpoint to SageMaker PyTorch estimator to perform a model
    fine-tuning job:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `1_retrain_from_checkpoint.ipynb` 笔记本中的所有单元格。这个笔记本使用下载的数据集对预训练模型进行了额外的几个周期的微调。解决方案包括
    `detector.py` 文件中的训练代码，该文件位于 `osp.join(sources, "source_dir.tar.gz")`。解决方案使用 SageMaker
    SDK 的 PyTorch 估算器创建一个训练作业，该作业启动一个按需计算资源，即一个 `ml.g4dn.2xlarge` 实例，并从提供的预训练检查点开始训练。训练大约需要
    10 分钟。以下代码行显示了如何将训练数据和预训练检查点输入到 SageMaker PyTorch 估算器以执行模型微调作业：
- en: '[PRE2]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'The naming of the dictionary keys to `.fit()` call is done by design. These
    keys are registered as environment variables with a SM_CHANNEL_ prefix inside
    the training container and can be accessed in the training script. The keys need
    to match what is written in the `detector.py` file in order to make this `.fit()`
    training call work. For example, see line 310 and 349 in `detector.py`:'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`.fit()` 调用的字典键的命名是经过设计的。这些键在训练容器内部注册为带有 SM_CHANNEL_ 前缀的环境变量，可以在训练脚本中访问。键需要与
    `detector.py` 文件中写入的内容匹配，以便使这个 `.fit()` 训练调用生效。例如，请参阅 `detector.py` 中的第 310 行和第
    349 行：'
- en: '`aa("--data-path", metavar="DIR", type=str, default=os.environ["SM_CHANNEL_TRAINING"])`'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`aa("--data-path", metavar="DIR", type=str, default=os.environ["SM_CHANNEL_TRAINING"])`'
- en: '`aa("--resume-sagemaker-from-checkpoint", type=str, default=os.getenv("SM_CHANNEL_PRETRAINED_CHECKPOINT",
    None))`'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`aa("--resume-sagemaker-from-checkpoint", type=str, default=os.getenv("SM_CHANNEL_PRETRAINED_CHECKPOINT",
    None))`'
- en: After the training, the model is deployed as a SageMaker hosted endpoint, as
    in the `0_demo.ipynb` notebook. In the end, a comparison between the ground truth,
    the inference from the pretrained model from `0_demo.ipynb`, and the inference
    from the fine-tuned model is visualized. We can see that the inference from the
    fine-tuned model has one fewer false positive, yet still isn't able to pick up
    a patch on the right side of the sample image. This should be considered a false
    negative.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，模型作为 SageMaker 主机端点部署，如 `0_demo.ipynb` 笔记本中所示。最后，将真实值、从 `0_demo.ipynb`
    预训练模型得到的推理结果以及微调模型的推理结果进行了可视化比较。我们可以看到，微调模型的推理结果少了一个误报，但仍然无法识别样本图像右侧的补丁。这应该被视为一个漏报。
- en: Proceed to click on `2_detection_from_scratch.ipynb`.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 `2_detection_from_scratch.ipynb`。
- en: Run all the cells in the `2_detection_from_scratch.ipynb` notebook. Instead
    of training from a checkpoint, we train a model from scratch with 10 epochs using
    the same dataset and compare the inference to that from the pretrained model.
    The model is significantly undertrained, as expected with the small epoch size
    used. You are encouraged to increase the epoch size (the `EPOCHS` variable) to
    300 to achieve better performance. However, this will take significantly more
    than 10 minutes.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `2_detection_from_scratch.ipynb` 笔记本中的所有单元格。我们不是从检查点开始训练，而是使用相同的训练数据集从头开始训练一个模型，使用
    10 个周期，并将推理结果与预训练模型的推理结果进行比较。由于使用了较小的周期数，模型训练不足，这是预期的。我们鼓励您将周期数（`EPOCHS` 变量）增加到
    300 以获得更好的性能。然而，这将需要超过 10 分钟的时间。
- en: Note
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: We control whether we train from a checkpoint or from scratch by whether we
    include a `pretrained_checkpoint` key in a dictionary to `.fit()` or not.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过是否在字典中包含 `pretrained_checkpoint` 键来控制是否从检查点或从头开始训练。
- en: Proceed to click on `3_classification_from_scratch.ipynb`.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 `3_classification_from_scratch.ipynb`。
- en: In this notebook, we train a classification model using `classifier.py` for
    50 epochs, instead of an object detection model from scratch, using the NEU-CLS
    classification dataset. A classification model is different from the previous
    object detection models. Image classification recognizes the types of defect in
    an entire image, whereas an object detection model can also localize where the
    defect is. Image classification is useful if you do not need to know the location
    of the defect, and can be used as a triage model for product defects.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个笔记本中，我们使用`classifier.py`训练了一个分类模型，共进行了50个epoch，而不是从头开始训练一个目标检测模型，使用了NEU-CLS分类数据集。分类模型与之前的目标检测模型不同。图像分类可以识别整个图像中的缺陷类型，而目标检测模型还可以定位缺陷的位置。如果你不需要知道缺陷的位置，图像分类非常有用，并且可以作为产品缺陷的分级模型使用。
- en: 'Training a classification model is faster, as you can see from the job. The
    classification accuracy on the validation set reaches `0.99`, as shown in the
    cell output from the training job, which is very accurate:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从作业中可以看出，训练分类模型更快。正如训练作业的单元格输出所示，验证集上的分类准确率达到了`0.99`，这是一个非常准确的值：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is the end of the solution. Please make sure to execute the last cell in
    each notebook to delete the models and endpoints, especially the last cell in
    the `0_demo.ipynb` notebook, where the deletion is commented out. Please uncomment
    this and execute it to delete the pretrained model and endpoint.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这就是解决方案的结束。请确保在每个笔记本中执行最后一个单元格以删除模型和端点，特别是`0_demo.ipynb`笔记本中的最后一个单元格，其中删除操作被注释掉了。请取消注释并执行它以删除预训练模型和端点。
- en: With this SageMaker JumpStart solution, you built and trained four deep learning
    models based on a PyTorch implementation of Faster RCNN to detect and classify
    six types of defects in steel imagery with minimal coding effort. You also hosted
    them as SageMaker endpoints for real-time prediction. You can expect a similar
    experience with other solutions in SageMaker JumpStart to learn different aspects
    of SageMaker features used in the context of solving common use cases.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个SageMaker JumpStart解决方案，你基于Faster RCNN的PyTorch实现构建并训练了四个深度学习模型，以最小的编码工作量检测和分类钢图像中的六种缺陷类型。你还将它们作为SageMaker端点托管以进行实时预测。你可以期待在SageMaker
    JumpStart的其他解决方案中体验到类似的效果，以学习在解决常见用例时使用的SageMaker功能的各个方面。
- en: Now, let's switch gears to the SageMaker JumpStart model zoo.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转换到SageMaker JumpStart模型库。
- en: SageMaker JumpStart model zoo
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SageMaker JumpStart模型库
- en: There are more than 200 popular prebuilt and pretrained models in SageMaker
    JumpStart for you to use out of the box or continue to train for your use case.
    What are they good for? Training an accurate deep learning model is time consuming
    and complex, even with the most powerful GPU machine. It also requires large amounts
    of training and labeled data. Now, with these models that have been developed
    by the community, pretrained on large datasets, you do not have to reinvent the
    wheel.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在SageMaker JumpStart中，有超过200个流行的预构建和预训练模型供你直接使用或继续训练以适应你的用例。它们有什么好处？训练一个准确的深度学习模型既耗时又复杂，即使是最强大的GPU机器也是如此。它还需要大量的训练和标记数据。现在，有了这些由社区开发、在大数据集上预训练的模型，你不必重新发明轮子。
- en: Model collection
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型集合
- en: 'There are two groups of models: **text models** and **vision models** in SageMaker
    JumpStart model zoo. These models are the most popular ones among the ML community.
    You can quickly browse the models in SageMaker JumpStart and select the one that
    meets your needs. On each model page, you will see an introduction to the model,
    its usage, and how to prepare a dataset for fine-tuning purposes. You can deploy
    models into AWS as a hosted endpoint for your use case or fine-tune the model
    further with your own dataset.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在SageMaker JumpStart模型库中有两组模型：**文本模型**和**视觉模型**。这些模型在机器学习社区中非常受欢迎。你可以快速浏览SageMaker
    JumpStart中的模型，并选择满足你需求的模型。在每一个模型页面上，你将看到该模型的介绍、用法以及如何为微调准备数据集。你可以将模型部署到AWS作为你的用例的托管端点，或者使用自己的数据集进一步微调模型。
- en: 'Text models are sourced from the following three hubs: TensorFlow Hub, PyTorch
    Hub, and Hugging Face. Each model is specifically trained for a particular type
    of NLP task using a dataset such as text classification, question answering, or
    text generation. Notably, there are many flavors of **Bidirectional Encoder Representations
    from Transformers** (**BERT**), **Cross-lingual Language Model** (**XLM**), **ELECTRA**,
    and **Generative Pretrained Transformer** (**GPT)** up for grabs.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 文本模型来源于以下三个中心：TensorFlow Hub、PyTorch Hub和Hugging Face。每个模型都是针对特定类型的NLP任务使用如文本分类、问答或文本生成等数据集进行专门训练的。值得注意的是，有许多版本的**双向编码器表示从Transformer**（**BERT**）、**跨语言语言模型**（**XLM**）、**ELECTRA**和**生成预训练Transformer**（**GPT**）可供选择。
- en: Vision models are sourced from TensorFlow Hub, PyTorch Hub, and Gluon CV. There
    are models that perform image classification, image feature vector extraction,
    and object detection. **Inception**, **SSD**, **ResNet**, and **Faster R-CNN**
    models are some of the most notable and widely used models in the field of CV.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉模型来源于TensorFlow Hub、PyTorch Hub和Gluon CV。这里有执行图像分类、图像特征向量提取和目标检测的模型。**Inception**、**SSD**、**ResNet**和**Faster
    R-CNN**模型是该领域最著名和最广泛使用的模型。
- en: Deploying a model
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署模型
- en: Let's find a question-answering model and see how we can deploy it to our AWS
    account. In the search bar, type in `question` **answering** hit **Return**, and
    you should see a list of models that perform such tasks returned to you, as shown
    in *Figure 8.6*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们找到一个问答模型，看看我们如何将其部署到我们的AWS账户。在搜索栏中输入`问答`并按**回车键**，你应该会看到返回一个执行此类任务的模型列表，如图*图8.6*所示。
- en: '![Figure 8.6 – Searching for question-answering models'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.6 – 搜索问答模型'
- en: '](img/B17447_08_06.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17447_08_06.jpg)'
- en: Figure 8.6 – Searching for question-answering models
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 搜索问答模型
- en: Let's find and double-click `OpenWebTextCorpus` and is distilled from the RoBERTa
    model checkpoint. It has 6 layers, 768 hidden, 12 heads, and 82 million parameters.
    82 million! It is not easy to train such a large model, for sure. Luckily with
    SageMaker JumpStart, we have a model that we can deploy out of the box. As shown
    in *Figure 8.7*, please expand the **Deployment Configuration** section, choose
    **Ml.M5.Xlarge** as the machine type, leave the endpoint name as default, and
    hit **Deploy**. Ml.M5.Xlarge is a general-purpose instance type that has 4 vCPU
    and 16 GB of memory, which is sufficient for this example. The deployment will
    take a couple of minutes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们找到并双击`OpenWebTextCorpus`，它是由RoBERTa模型检查点提取的。它有6层，768个隐藏单元，12个头，和8200万个参数。8200万个！训练这样一个大型模型当然不容易。幸运的是，有了SageMaker
    JumpStart，我们可以直接部署一个模型。如图*图8.7*所示，请展开**部署配置**部分，选择**Ml.M5.Xlarge**作为机器类型，将端点名称保留为默认值，然后点击**部署**。Ml.M5.Xlarge是一种通用实例类型，具有4个vCPU和16GB的内存，这对于本例来说已经足够了。部署将需要几分钟。
- en: '![Figure 8.7 – Deploying a JumpStart DistilRoBERTa Base model'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.7 – 部署JumpStart DistilRoBERTa Base模型'
- en: '](img/B17447_08_07.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17447_08_07.jpg)'
- en: Figure 8.7 – Deploying a JumpStart DistilRoBERTa Base model
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 部署JumpStart DistilRoBERTa Base模型
- en: Once the model is deployed, a notebook will be provided to you to show how you
    can make an API call to the hosted endpoint (*Figure 8.8*). You can find a list
    of models in the JumpStart left sidebar.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型部署完成，将提供一个笔记本，展示如何向托管端点发出API调用（如图*图8.8*）。你可以在JumpStart的左侧侧边栏中找到模型列表。
- en: '![Figure 8.8 – Opening a sample inference notebook after the model is deployed'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.8 – 部署模型后打开一个示例推理笔记本'
- en: '](img/B17447_08_08.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17447_08_08.jpg)'
- en: Figure 8.8 – Opening a sample inference notebook after the model is deployed
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 部署模型后打开一个示例推理笔记本
- en: 'In the sample notebook, two questions from the **SQuAD v2** dataset, one of
    the most widely used question-answering datasets for evaluation, are provided
    to show how inferencing can be done. Let''s also ask our model other questions
    based on the following passage (Can you guess where you''ve read it before? Yes,
    it''s the opening statement of this chapter!):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例笔记本中，提供了来自**SQuAD v2**数据集的两个问题，这是最广泛使用的问答数据集之一，用于评估。让我们也根据以下段落（你能猜到你以前在哪里读过吗？是的，这是本章的开篇陈述！）向我们的模型提出其他问题：
- en: 'Context:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文：
- en: 'SageMaker JumpStart offers complete solutions for select use cases as a starter
    kit to the world of machine learning (ML) with Amazon SageMaker without any code
    development. SageMaker JumpStart also catalogs popular pretrained computer vision
    (CV) and natural language processing (NLP) models for you to easily deploy or
    fine-tune to your dataset. SageMaker Autopilot is an AutoML solution that explores
    your data, engineers features on your behalf, and trains an optimal model from
    various algorithms and hyperparameters. You don''t have to write any code: Autopilot
    does it for you and returns notebooks to show how it does it.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker JumpStart 作为入门套件，为世界各地的机器学习（ML）爱好者提供 Amazon SageMaker 的完整解决方案，无需任何代码开发。SageMaker
    JumpStart 还为您整理了流行的预训练计算机视觉（CV）和自然语言处理（NLP）模型，以便您轻松部署或微调到您的数据集。SageMaker Autopilot
    是一个 AutoML 解决方案，它探索您的数据，代表您构建特征，并从各种算法和超参数中训练最优模型。您无需编写任何代码：Autopilot 会为您完成，并返回笔记本以展示它是如何做到的。
- en: 'Questions:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：
- en: '*What does SageMaker JumpStart do?*'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SageMaker JumpStart 做什么？*'
- en: '*What is NLP?*'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*什么是自然语言处理（NLP）？*'
- en: 'In the notebook, we should add the following to the second cell:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中，我们应该在第二个单元格中添加以下内容：
- en: '[PRE4]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the third cell, append the two new question context pairs to the list in
    the `for` loop and execute all cells in the notebook:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三个单元格中，将两个新的问题上下文对添加到 `for` 循环中的列表中，并执行笔记本中的所有单元格：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: And voila! We get responses from our model that answer our questions about SageMaker
    JumpStart's capabilities and the full form of NLP as natural language processing.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！我们从模型那里得到了回答，这些问题是关于 SageMaker JumpStart 的功能和自然语言处理（NLP）的全称。
- en: Fine-tuning a model
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调模型
- en: It is typical to perform model fine-tuning when you take a pretrained model
    off the shelf to expose the model to your dataset so that it can perform better
    on your dataset compared to the performance without such exposure. Furthermore,
    model fine-tuning takes less training time and requires a smaller amount of labeled
    data compared to training a model from scratch. To fine-tune a pretrained model
    from SageMaker JumpStart, first we need to make sure that the model you would
    like to use supports fine-tuning. You can find this attribute in the overview
    cards. Secondly, you need to point a dataset to the model. Taking the DistilRoBERTa
    Base model as an example, SageMaker JumpStart provides the default dataset of
    **SQuAD-v2**, which allows you to quickly start a training job. You can also create
    a dataset of your own by following the instructions on the JumpStart model page.
    We are going to do just that.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当您从架子上取下一个预训练模型，将其暴露于您的数据集，以便它在您的数据集上表现优于未暴露时的性能时，进行模型微调是典型的。此外，与从头开始训练模型相比，模型微调所需的时间更少，并且需要的标记数据量更小。要微调
    SageMaker JumpStart 中的预训练模型，首先我们需要确保您想使用的模型支持微调。您可以在概述卡中找到此属性。其次，您需要将数据集指向模型。以
    DistilRoBERTa Base 模型为例，SageMaker JumpStart 提供了默认的 **SQuAD-v2** 数据集，这允许您快速开始训练作业。您也可以按照
    JumpStart 模型页面上的说明创建自己的数据集。我们就是要这样做。
- en: 'Let''s fine-tune the base DistilRoBERTa Base model with some questions and
    answers about Buddhism, which is one of the topics in the `SquAD-v2` dataset.
    Please follow these steps:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用关于佛教的一些问题和答案来微调基础 DistilRoBERTa Base 模型，佛教是 `SQuAD-v2` 数据集中的一个主题。请按照以下步骤操作：
- en: 'Open the `chapter08/1-prep_data_for_finetune.ipynb` notebook in the repository
    and execute all cells to download the dataset, extract the paragraphs that are
    related to Buddhism, and organize them as the fine-tune trainer expects. This
    is detailed on the description page in the `data.csv` file:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在存储库中打开 `chapter08/1-prep_data_for_finetune.ipynb` 笔记本，执行所有单元格以下载数据集，提取与佛教相关的段落，并按照微调训练器期望的方式组织它们。这在
    `data.csv` 文件中的描述页上有详细说明：
- en: The first column of the `data.csv` should have a question.
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data.csv` 的第一列应该有一个问题。'
- en: The second column should have the corresponding context.
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二列应该有相应的上下文。
- en: The third column should have the integer character starting position for the
    answer in the context.
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三列应该有答案在上下文中的整数字符起始位置。
- en: The fourth column should have the integer character ending position for the
    answer in the context.
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四列应该有答案在上下文中的整数字符结束位置。
- en: '**Output**: A trained model that can be deployed for inference.'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输出**：一个可以部署进行推理的训练模型。'
- en: 'At the end of the notebook, the `data.csv` file will be uploaded to your SageMaker
    default bucket: `s3://sagemaker-<region>-<accountID>/chapter08/buddhism/data.csv`.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 笔记本末尾，`data.csv` 文件将被上传到您的 SageMaker 默认存储桶：`s3://sagemaker-<region>-<accountID>/chapter08/buddhism/data.csv`。
- en: Once this is done, let's switch back to the model page and configure the fine-tuning
    job. As in *Figure 8.9*, select `-buddhism` onto the model name, leave the machine
    type and hyperparameters as their defaults, and hit **Train**. The default **Ml.P3.2xlarge**
    instance type, with one NVIDIA Tesla V100 GPU, is a great choice for fast model
    fine-tuning. The default hyperparameter setting performs fine-tuning with a **batch
    size of 4**, **learning rate of 2e-5**, and **3 epochs**. This is sufficient for
    us to demonstrate how the fine-tuning works. Feel free to change the values here
    to reflect your actual use case.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成此操作后，让我们切换回模型页面并配置微调作业。如图 8.9 所示，将 `-buddhism` 添加到模型名称中，保留机器类型和超参数为默认值，然后点击
    **训练**。默认的 **Ml.P3.2xlarge** 实例类型，配备一个 NVIDIA Tesla V100 GPU，是快速模型微调的一个很好的选择。默认的超参数设置使用
    **批量大小为 4**、**学习率为 2e-5** 和 **3 个训练轮数** 进行微调。这对于我们演示微调的工作原理是足够的。您可以随意更改这里的值以反映您的实际使用情况。
- en: '![Figure 8.9 – Configuring a fine-tuning job for a custom dataset'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.9 – 为自定义数据集配置微调作业'
- en: '](img/B17447_08_09.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17447_08_09.jpg)'
- en: Figure 8.9 – Configuring a fine-tuning job for a custom dataset
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 – 为自定义数据集配置微调作业
- en: The training job should take about 6 minutes with the **Ml.P3.2xlarge** instance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 **Ml.P3.2xlarge** 实例，训练作业应该大约需要 6 分钟。
- en: Once the job completes, you can deploy the model to an endpoint with an **Ml.M5.Xlarge**
    instance, as shown in *Figure 8.10*. Ml.M5.Xlarge is a general-purpose CPU instance,
    which is a good starting point for model hosting.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作业完成后，您可以将模型部署到具有 **Ml.M5.Xlarge** 实例的端点，如图 8.10 所示。Ml.M5.Xlarge 是一种通用 CPU 实例，是模型托管的一个良好起点。
- en: '![Figure 8.10 – Deploying the fine-tuned model'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.10 – 部署微调后的模型'
- en: '](img/B17447_08_010.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17447_08_010.jpg)'
- en: Figure 8.10 – Deploying the fine-tuned model
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – 部署微调后的模型
- en: Of course, we now need to test how well the fine-tuned model performs on questions
    related to Buddha and Buddhism. Once the deployment finishes, you will be prompted
    with an option to open a prebuilt notebook to use the endpoint, similar to what
    is shown in *Figure 8.8*.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们现在需要测试微调后的模型在涉及佛陀和佛教的问题上的表现。一旦部署完成，您将收到一个选项，可以打开一个预构建的笔记本来使用端点，类似于图 8.8
    所示。
- en: 'We can replace the question-context pair in the second cell with the following
    snippet from [https://www.history.com/topics/religion/buddhism](https://www.history.com/topics/religion/buddhism):'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以将第二个单元格中的问题-上下文对替换为以下来自 [https://www.history.com/topics/religion/buddhism](https://www.history.com/topics/religion/buddhism)
    的片段：
- en: '[PRE6]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Then, execute the cells in the notebook and you will see how well our new model
    performs.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，执行笔记本中的单元格，您将看到我们的新模型表现如何。
- en: It's not quite what we would like the model to be. This is due to the very small
    epochs used and perhaps the unoptimized batch size and learning rate. As we are
    providing new data points for the model, the weights in the network are once again
    being updated and need to perform training for a sufficient number of epochs to
    converge on a lower loss and thus create a more accurate model. These hyperparameters
    often need to be tuned in order to obtain a good model even with fine-tuning.
    You are encouraged to further experiment with different hyperparameters to see
    if the model provides better responses to the questions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是我们希望模型达到的效果。这是由于使用了非常小的训练轮数，以及可能未优化的批量大小和学习率。由于我们为模型提供了新的数据点，网络中的权重再次更新，需要经过足够多的训练轮数以收敛到更低的损失，从而创建一个更准确的模型。这些超参数通常需要调整，以便即使在微调的情况下也能获得一个好的模型。我们鼓励您进一步实验不同的超参数，看看模型是否能够更好地回答问题。
- en: We have just created three ML models, which are supposed to be complex and difficult
    to train, without much coding at all. Now we are going to learn how to use SageMaker
    Autopilot to automatically create a high-quality model without any code.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚创建了三个机器学习模型，这些模型应该是复杂且难以训练的，但几乎不需要编写任何代码。现在我们将学习如何使用 SageMaker Autopilot
    自动创建一个高质量的模型，而无需任何代码。
- en: Creating a high-quality model with SageMaker Autopilot
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SageMaker Autopilot 创建高质量模型
- en: Have you ever wanted to build an ML model without the hassle of data preprocessing,
    feature engineering, exploring algorithms, and optimizing the hyperparameters?
    Have you ever thought about how, for some use cases, you just wanted something
    quick to see if ML is even a possible approach for a certain business use case?
    Amazon SageMaker Autopilot makes it easy for you to build an ML model for tabular
    datasets without any code.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾想过在没有数据预处理、特征工程、探索算法和优化超参数的麻烦的情况下构建机器学习模型？你是否曾想过，对于某些用例，你只是想快速看看机器学习是否是某个特定商业用例的可行方法？Amazon
    SageMaker Autopilot使你能够轻松地为表格数据集构建机器学习模型，而无需任何代码。
- en: Wine quality prediction
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 葡萄酒质量预测
- en: To demonstrate SageMaker Autopilot, let's use a wine quality prediction use
    case. The wine industry has been searching for a technology that can help winemakers
    and the market to assess the quality of wine faster and with a better standard.
    Wine quality assessment and certification is a key part of the wine market in
    terms of production and sales and prevents the illegal adulteration of wines.
    Wine assessment is performed by expert oenologists based on physicochemical and
    sensory tests that produce features such as density, alcohol level, and pH level.
    However, when a human is involved, the standard can vary between oenologists or
    between testing trials. Having an ML approach to support oenologists in providing
    analytical information therefore becomes an important task in the wine industry.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示SageMaker Autopilot，让我们使用葡萄酒质量预测用例。葡萄酒行业一直在寻找一种可以帮助酿酒师和市场更快、更标准地评估葡萄酒质量的技术。葡萄酒质量评估和认证是葡萄酒市场生产和销售的关键部分，并防止葡萄酒的非法掺假。葡萄酒评估由专家品酒师根据物理化学和感官测试进行，产生如密度、酒精水平和pH值等特征。然而，当涉及人类时，标准可能在品酒师之间或测试试验之间有所不同。因此，拥有一种机器学习方法来支持品酒师提供分析信息，在葡萄酒行业中成为一项重要任务。
- en: We are going to train an ML model to predict wine quality based on the physicochemical
    sensory values for 4,898 white wines produced between 2004 and 2007 in Portugal.
    The dataset is available from UCI at https://archive.ics.uci.edu/ml/datasets/Wine+Quality.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练一个机器学习模型，根据2004年至2007年间在葡萄牙生产的4,898种白葡萄酒的物理化学感官值来预测葡萄酒质量。数据集可在UCI的https://archive.ics.uci.edu/ml/datasets/Wine+Quality处获得。
- en: Setting up an Autopilot job
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置Autopilot作业
- en: 'Let''s begin:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧：
- en: Please open the [chapter08/2-prep_data_for_sm_autopilot.ipynb](http://chapter08/2-prep_data_for_sm_autopilot.ipynb)
    notebook from the repository, and execute all of the cells to download the data
    from the source, hold out a test set, and upload the training data to an S3 bucket.
    Please note the paths to the training data.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请打开存储库中的[chapter08/2-prep_data_for_sm_autopilot.ipynb](http://chapter08/2-prep_data_for_sm_autopilot.ipynb)笔记本，并执行所有单元格以从源下载数据，保留测试集，并将训练数据上传到S3存储桶。请注意训练数据的路径。
- en: Next, open the Launcher and select **New Autopilot Experiment**, as in *Figure
    8.11*.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，打开启动器并选择**新建Autopilot实验**，如*图8.11*所示。
- en: '![Figure 8.11 – Creating a new Autopilot experiment'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.11 – 创建新的Autopilot实验'
- en: '](img/B17447_08_011.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17447_08_011.jpg)'
- en: Figure 8.11 – Creating a new Autopilot experiment
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 – 创建新的Autopilot实验
- en: A new window will pop up for us to configure an Autopilot job.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 将弹出一个新窗口，供我们配置Autopilot作业。
- en: As shown in *Figure 8.12*, provide an `white-wine-predict-quality`.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如*图8.12*所示，提供`white-wine-predict-quality`。
- en: '![Figure 8.12 – Configuring an Autopilot job'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.12 – 配置Autopilot作业'
- en: '](img/B17447_08_012.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17447_08_012.jpg)'
- en: Figure 8.12 – Configuring an Autopilot job
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 – 配置Autopilot作业
- en: As shown in *Figure 8.12*, provide the training data in the `sagemaker-<region>-<accountID>`
    from the `sagemaker-studio-book/chapter08/winequality/winequality-white-train.csv`
    file from the **Dataset file name** drop-down menu. Set **Target** to **quality**
    to predict the quality of wine with the rest of attributes in the CSV file.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如*图8.12*所示，从**数据集文件名**下拉菜单中选择`sagemaker-<region>-<accountID>`中的训练数据。将**目标**设置为**quality**，使用CSV文件中的其余属性来预测葡萄酒的质量。
- en: In the lower half of the configuration page, as shown in *Figure 8.13*, provide
    a path to save the output data to, check the `sagemaker-<region>-<accountID>`
    from the `sagemaker-studio-book/chapter08/winequality/` path into the **Dataset
    directory name** field as the output location. This path is where we have the
    training CSV file.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在配置页面的下半部分，如图8.13所示，提供保存输出数据的路径，将`sagemaker-<region>-<accountID>`从`/sagemaker-studio-book/chapter08/winequality/`路径检查到**数据集目录名称**字段中，作为输出位置。这个路径是我们训练CSV文件所在的位置。
- en: '![Figure 8.13 – Configuring an Autopilot job'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 8.13 – 配置Autopilot作业'
- en: '](img/B17447_08_013.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17447_08_013.jpg](img/B17447_08_013.jpg)'
- en: Figure 8.13 – Configuring an Autopilot job
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 8.13 – 配置Autopilot作业
- en: As shown in *Figure 8.13*, choose **Multiclass classification** from the **Select
    the machine learning problem type** drop-down menu. Then choose **F1macro** from
    the **Objective metric** drop-down menu so that we can expect a more balanced
    model should the data be biased toward a certain quality rank.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如图8.13所示，从**选择机器学习问题类型**下拉菜单中选择**多类分类**。然后从**目标指标**下拉菜单中选择**F1macro**，以便在数据偏向某个质量等级时，我们可以期望得到一个更平衡的模型。
- en: As shown in *Figure 8.13*, choose **Yes** for **Do you want to run a complete
    experiment?**. Then toggle the **Auto deploy** option to **off** as we would like
    to walk through the evaluation process in SageMaker Studio before deploying our
    best model.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如图8.13所示，对于**您想运行完整实验吗？**选择**是**。然后切换**自动部署**选项为**关闭**，因为我们希望在SageMaker Studio中走完评估过程后再部署我们的最佳模型。
- en: As shown in *Figure 8.13*, expand the `100` in the **Max candidates** field.
    By default, Autopilot runs 250 training jobs with different preprocessing steps,
    training algorithms, and hyperparameters. By using a limited number of candidates,
    we should expect the full experiment to complete faster than with the default
    setting.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如图8.13所示，展开**最大候选数**字段中的`100`。默认情况下，Autopilot运行250个具有不同预处理步骤、训练算法和超参数的训练作业。通过使用有限数量的候选者，我们应该期望完整实验比默认设置更快完成。
- en: Hit **Create Experiment** to start the Autopilot job.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建实验**以启动Autopilot作业。
- en: You will see a new window that shows the progress of the Autopilot job. Please
    let it crunch the numbers a bit and come back in a couple of minutes. You will
    see more progress and output in the progress tab, as shown in *Figure 8.14*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到一个新窗口，显示Autopilot作业的进度。请让它处理一下数字，过几分钟再回来。您将在进度标签中看到更多的进度和输出，如图8.14所示。
- en: '![Figure 8.14 – Viewing the progress of an Autopilot experiment'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 8.14 – 查看Autopilot实验的进度'
- en: '](img/B17447_08_014.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17447_08_014.jpg](img/B17447_08_014.jpg)'
- en: Figure 8.14 – Viewing the progress of an Autopilot experiment
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 8.14 – 查看Autopilot实验的进度
- en: A lot is going on here. Let's dive in.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多事情在进行中。让我们深入了解一下。
- en: Understanding an Autopilot job
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解Autopilot作业
- en: Amazon SageMaker Autopilot executes an end-to-end ML model-building exercise
    automatically. It performs **exploratory data analysis** (**EDA**), does data
    preprocessing, and creates feature engineering and a model-training recipe. It
    then executes the recipe in order to find the best model given the conditions.
    You can see the progress in the middle portion of *Figure 8.14*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker Autopilot 自动执行端到端的机器学习模型构建练习。它执行**探索性数据分析**（**EDA**），进行数据预处理，并创建特征工程和模型训练配方。然后按照配方执行，以找到给定条件下的最佳模型。您可以在图8.14的中间部分查看进度。
- en: What makes Autopilot unique is the full visibility that it provides. Autopilot
    unboxes the typical AutoML black box by giving you the EDA results and the code
    that Autopilot runs to perform the feature engineering and ML modeling in the
    form of Jupyter notebooks. You can access the two notebooks by clicking the **Open
    data exploration notebook** button for the EDA results and the **Open candidate
    generation notebook** button for the recipe.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使Autopilot独特的是它提供的全面可见性。Autopilot通过提供EDA结果和Autopilot执行特征工程和机器学习建模的代码（以Jupyter笔记本的形式）来解包典型的AutoML黑盒。您可以通过点击**打开数据探索笔记本**按钮来访问EDA结果，以及点击**打开候选生成笔记本**按钮来访问配方。
- en: The data exploration notebook is helpful for understanding the data, the distribution,
    and how Autopilot builds the recipe based on the characteristics of the data.
    For example, Autopilot looks for missing values in the dataset, the distribution
    of numerical features, and the cardinality of the categorical features. This information
    gives data scientists a baseline understanding of the data, along with actionable
    insights on whether the input data contains reasonable entries or not. Should
    you see many features with high percentages of missing values (the **Percent of
    Missing Values** section), you could take the suggested actions to investigate
    the issue from the data creation perspective and apply some level of pre-processing
    to either remove the feature or apply domain-specific imputation. You may ask,
    "*Doesn't Autopilot apply data pre-processing and feature engineering to the data?*"
    Yes, it does. However, Autopilot does not have domain-specific knowledge of your
    data. You should expect a more generic, data science-oriented approach to the
    issues surfaced by Autopilot, which may not be as effective.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索笔记本有助于理解数据、分布以及Autopilot如何根据数据的特征构建食谱。例如，Autopilot会在数据集中寻找缺失值、数值特征的分布以及分类特征的基数。这些信息为数据科学家提供了对数据的基准理解，以及关于输入数据是否包含合理条目的可操作见解。如果你看到许多特征具有高比例的缺失值（**缺失值百分比**部分），你可以采取建议的行动从数据创建的角度调查问题，并对特征进行一些预处理，要么删除特征，要么应用特定领域的插补。你可能想知道，“*Autopilot不是应用数据预处理和特征工程到数据上吗？*”是的，它确实如此。然而，Autopilot并不具备你数据的特定领域知识。你应该期待Autopilot对提出的问题采取更通用的、以数据科学为导向的方法，这可能不会那么有效。
- en: 'The candidate generation notebook prescribes a recipe for how the model should
    be built and trained based on the EDA of the data. The amount of code might look
    daunting, but if you read through it carefully, you can see, for example, what
    data preprocessing steps and modeling approaches Autopilot is attempting, as shown
    in the **Candidate Pipelines** section. The following is one example of this:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 候选生成笔记本规定了基于数据的EDA（探索性数据分析）如何构建和训练模型的方法。代码的数量可能看起来令人畏惧，但如果你仔细阅读，你可以看到，例如，Autopilot正在尝试的数据预处理步骤和建模方法，如**候选管道**部分所示。以下是一个例子：
- en: '[PRE7]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Autopilot bases the pipelines on three algorithms: **XGBoost**, **linear learner**,
    and **multi-layer perceptron** (**MLP)**. XGBoost is a popular gradient-boosted
    tree algorithm that combines an ensemble of weak predictors to form the final
    predictor in an efficient and flexible manner. XGBoost is one of SageMaker''s
    built-in algorithms. Linear learner, also a SageMaker built-in algorithm, trains
    multiple linear models with different hyperparameters, and finds the best model
    with a distributed stochastic gradient descent optimization. MLP is a neural network-based
    supervised learning algorithm that can have multiple hidden layers of neurons
    to create a non-linear model.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Autopilot基于三种算法构建管道：**XGBoost**、**线性学习器**和**多层感知器**（**MLP**）。XGBoost是一种流行的梯度提升树算法，以高效和灵活的方式结合了一组弱预测器来形成最终的预测器。XGBoost是SageMaker内置算法之一。线性学习器也是一种SageMaker内置算法，它使用不同的超参数训练多个线性模型，并通过分布式随机梯度下降优化找到最佳模型。MLP是一种基于神经网络的监督学习算法，它可以有多个隐藏层，以创建非线性模型。
- en: You can also see the list of hyperparameters and ranges Autopilot is exploring
    (the **MultiAlgorithm Hyperparameter Tuning** section). Not only does Autopilot
    provide you visibility, but it also gives you full control of the experimentation.
    You can click on the **Import notebook** button the top right to get a copy of
    the notebook that you can actually customize and execute to obtain your next best
    model.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以看到Autopilot正在探索的超参数和范围列表（**多算法超参数调整**部分）。Autopilot不仅提供了可见性，还给了你对实验的完全控制权。你可以点击右上角的**导入笔记本**按钮来获取一个你可以自定义并执行以获得下一个最佳模型的笔记本副本。
- en: Evaluating Autopilot models
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估Autopilot模型
- en: 'If you see that the job status in the tab, as shown in *Figure 8.14*, has changed
    to **Completed**, then it is time to evaluate the models Autopilot has generated.
    Autopilot has trained 100 models using various mixtures of feature engineering,
    algorithms, and hyperparameters as you can see in the list of trials. This leaderboard
    also shows the performance metric, the F1 score on a random validation split,
    used to evaluate the models. You can click on **Objective: F1** to sort the models
    by score.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在标签页中看到的工作状态，如图8.14所示，已更改为**完成**，那么就是时候评估自动驾驶仪生成的模型了。如您在试验列表中所见，自动驾驶仪已使用各种特征工程、算法和超参数的组合训练了100个模型。排行榜还显示了用于评估模型的性能指标，即随机验证分割上的F1分数。您可以通过点击**目标：F1**来按分数排序模型。
- en: Let's take a closer look at the best model, the one that has the highest F1
    score and a star next to the trial name. Right-click on the trial and select **Open
    in model details** to view more information.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看最佳模型，即具有最高F1分数且试验名称旁边有星号的模型。右键单击试验并选择**在模型详情中打开**以查看更多信息。
- en: '![Figure 8.15 – Viewing Autopilot model details in SageMaker Studio'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.15 – 在SageMaker Studio中查看自动驾驶仪模型详情'
- en: '](img/B17447_08_015.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17447_08_015.jpg)'
- en: Figure 8.15 – Viewing Autopilot model details in SageMaker Studio
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 – 在SageMaker Studio中查看自动驾驶仪模型详情
- en: Autopilot reports a lot of detail on this page, as shown in *Figure 8.15*. First
    of all, we can see that this model is built based on the **XGBoost** algorithm.
    We also see a chart of feature importance that Autopilot generates for our convenience.
    This chart tells us how the model considers the importance, or contribution, of
    the input features. Autopilot computes the **SHapley Additive exPlanations** (**SHAP**)
    values using **SageMaker Clarify** for this XGBoost model and dataset. SHAP values
    explain how features contribute to the model forming the decision based on game
    theory.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶仪在本页上报告了很多细节，如图8.15所示。首先，我们可以看到这个模型是基于**XGBoost**算法构建的。我们还看到了自动驾驶仪为我们生成的特征重要性图表。这个图表告诉我们模型如何考虑输入特征的重要性或贡献。自动驾驶仪使用**SageMaker
    Clarify**为这个XGBoost模型和数据集计算**SHapley Additive exPlanations**（**SHAP**）值。SHAP值解释了特征如何根据博弈论对模型形成决策做出贡献。
- en: Note
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can hover over the bars to see the actual values. SageMaker provides more
    detail so that you can learn more about how these SHAP values are calculated in
    the white papers in the **Want to learn more?** section.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将鼠标悬停在条形上以查看实际值。SageMaker提供了更多细节，以便您可以在**想了解更多？**部分的白皮书中了解这些SHAP值是如何计算的。
- en: Back to the chart, you can also download an automatically generated PDF report
    that contains this chart for review and distribution (**Export PDF report**).
    If you want to work with the raw data in JSON format in order to integrate the
    SHAP values in other applications, you can download the data (**Download raw data**).
    By clicking the two buttons, you will be redirected to the S3 console as shown
    in *Figure 8.16*. You can download the file from the S3 bucket on the console
    by clicking the **Download** button.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 返回图表，您还可以下载一个自动生成的包含此图表的PDF报告，以便审阅和分发（**导出PDF报告**）。如果您想以JSON格式处理原始数据以便在其他应用程序中集成SHAP值，您可以下载数据（**下载原始数据**）。通过点击这两个按钮，您将被重定向到如图8.16所示的S3控制台。您可以通过点击**下载**按钮从控制台上的S3存储桶下载文件。
- en: '![Figure 8.16 – Downloading the feature importance PDF report in the S3 console'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.16 – 在S3控制台中下载特征重要性PDF报告'
- en: '](img/B17447_08_016.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17447_08_016.jpg)'
- en: Figure 8.16 – Downloading the feature importance PDF report in the S3 console
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16 – 在S3控制台中下载特征重要性PDF报告
- en: 'Besides the feature importance, the model performance on the training and validation
    sets is also very important in understanding how the model would perform in real
    life. You can see the metrics captured during the training run in the `ObjectiveMetric`
    used to rank the models on the leaderboard, we see the following metrics:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 除了特征重要性之外，模型在训练集和验证集上的表现对于理解模型在实际生活中的表现同样非常重要。您可以在用于在排行榜上对模型进行排名的`ObjectiveMetric`中看到训练运行期间捕获的指标，我们看到了以下指标：
- en: '`train:f1`'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train:f1`'
- en: '`train:merror`'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train:merror`'
- en: '`validation:f1`'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`validation:f1`'
- en: '`validation:merror`'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`validation:merror`'
- en: They are the multi-class F1macro and the multi-class error for the train and
    validation split of the data. As you can tell by the identical values, `ObjectiveMetric`
    is essentially `validation:f1`. With `train:f1` well above `validation:f1`, we
    may come to the conclusion that the model is overfitted to the training dataset.
    But why is this?
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 它们是多类F1宏度量以及数据训练和验证分割的多类错误。正如你可以通过相同的值所知，`ObjectiveMetric`基本上等同于`validation:f1`。由于`train:f1`远高于`validation:f1`，我们可能会得出结论，该模型过度拟合了训练数据集。但为什么会这样呢？
- en: We can further verify the model performance in more detail with the test data
    that we held out at the beginning. Please open the `chapter08/3-evaluate_autopilot_models.ipynb`
    notebook from the repository and execute all cells. In this notebook, you will
    retrieve the top models based on the `ObjectiveMetric` from the Autopilot job,
    perform inference in the cloud using the `TOP_N_CANDIDATES` to a different number.
    You should see the F1 score computed with the macro, an unweighted mean, weighted
    approaches, a classification report (from a sklearn function), and a confusion
    matrix on the test data as the output of the last cell.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用我们在一开始就保留的测试数据进一步详细验证模型性能。请从存储库中打开`chapter08/3-evaluate_autopilot_models.ipynb`笔记本，并执行所有单元格。在这个笔记本中，你将根据`ObjectiveMetric`从Autopilot作业中检索顶级模型，使用`TOP_N_CANDIDATES`在云中进行推理，针对不同的数量。你应该会看到计算出的F1分数，包括宏度量、未加权平均值、加权方法、分类报告（来自sklearn函数）和测试数据上的混淆矩阵，这是最后一个单元格的输出。
- en: 'With the top model, a couple of things jump out at me here. The data is imbalanced
    in nature. There is a higher concentration of scores `5`, `6`, and `7`. Few wines
    got a score of `3`, `4`, or `8`. The confusion matrix also shows that wines that
    got a score of `3` were all incorrectly classified. Under this situation, the
    `f1` macro measure will be drastically lowered by incorrect classification of
    a minority class out of proportion. If we look at the weighted version of the
    `f1` score, we get a significantly higher score as the scoring weights the dominant
    classes more heavily:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 使用顶级模型时，这里有几个方面让我印象深刻。数据在本质上是不平衡的。分数为`5`、`6`和`7`的葡萄酒数量较多。很少有葡萄酒得到`3`、`4`或`8`的分数。混淆矩阵还显示，得到`3`分的葡萄酒都被错误分类了。在这种情况下，由于对少数类的错误分类不成比例，`f1`宏度量将大幅降低。如果我们查看`f1`得分的加权版本，我们会得到一个显著更高的分数，因为评分更重视主导类：
- en: '[PRE8]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It is also important to measure the model''s performance using the metrics
    that matter the most to the use case. As the author of the cited study stated
    about the importance of precision measure:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对用例最重要的度量来衡量模型性能也很重要。正如被引用研究的作者关于精确度度量重要性的陈述：
- en: '"This statistic is important in practice, since in a real deployment setting
    the actual values are unknown and all predictions within a given column would
    be treated the same."'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '"这个统计量在实践中很重要，因为在实际的部署设置中，实际值是未知的，并且给定列内的所有预测都会被同等对待。"'
- en: 'We should compare the precision measure used in the original research study
    (in *Table 3* in the study, linked in the *Further reading* section) where the
    individual precisions are the following:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该比较原始研究研究中使用的精确度度量（在研究中的*表3*，在*进一步阅读*部分中链接），其中个体精确度如下：
- en: '4: 63.3%'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4: 63.3%'
- en: '5: 72.6%'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '5: 72.6%'
- en: '6: 60.3%'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '6: 60.3%'
- en: '7: 67.8%'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '7: 67.8%'
- en: '8: 85.5%'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '8: 85.5%'
- en: when tolerance `T = 0.5` for white wines. Our first Autopilot model overperforms
    in precision in some categories and underperforms in others.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当对白葡萄酒的容忍度`T = 0.5`时。我们的第一个Autopilot模型在某些类别中精确度表现优异，而在其他类别中表现不佳。
- en: Another strategy to find a model that serves the business problem better is
    to evaluate more models in addition to the best model suggested by Autopilot.
    We can see the evaluation for two others (or more, depending on your setting for
    `TOP_N_CANDIDATES`). We find that even though the second and third models have
    lower `validation:f1` (macro) scores than the first model, they actually have
    higher F1 scores on the held-out test set. The individual precision scores for
    the third model are all better than the model in the original research, except
    for class 5, by 2.6%. What a charm! The third model in the leaderboard actually
    has better performance on the test data as measured by the precision metric, which
    makes the most sense to the use case.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种找到更适合业务问题的模型的方法是评估更多模型，而不仅仅是 Autopilot 建议的最佳模型。我们可以看到另外两个（或更多，取决于您对 `TOP_N_CANDIDATES`
    的设置）的评估。我们发现，尽管第二和第三个模型的 `validation:f1`（宏）分数低于第一个模型，但它们在保留的测试集上的 F1 分数实际上更高。第三个模型的个别精确度分数都优于原始研究中的模型，除了第
    5 类，高出 2.6%。多么迷人！排行榜中的第三个模型在测试数据上的表现（按精确度指标衡量）实际上更好，这对于用例来说最有意义。
- en: After evaluation, we can deploy the optimal model into an endpoint for real-time
    inference. Autopilot makes it easy to deploy a model. In the leaderboard, select
    the line item that you would like to deploy, and click on the **Deploy model**
    button. A new page will pop up for you to configure the endpoint. Most options
    are straightforward and self-explanatory for an experienced SageMaker Studio user.
    Two things to note are that you can enable the data capture, which is useful if
    you want to set up SageMaker Model Monitor later. If you want the model to return
    more than just the **predicted_label**, such as the hard label of the winning
    class in a multiclass use case, you can choose to return the **probability** of
    the winning label, the **labels** of all classes, and the **probabilities** of
    all classes. The order of the selection will also determine the order of the output.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 评估后，我们可以将最佳模型部署到端点进行实时推理。Autopilot 使得部署模型变得简单。在排行榜中，选择您想要部署的项目，然后点击**部署模型**按钮。将弹出一个新页面，供您配置端点。对于经验丰富的
    SageMaker Studio 用户来说，大多数选项都是直观且不言自明的。有两点需要注意：您可以选择启用数据捕获，这在您想设置 SageMaker 模型监控器时很有用。如果您希望模型返回的不仅仅是**预测标签**，例如在多类用例中获胜类的硬标签，您可以选择返回获胜标签的**概率**、所有类的**标签**和所有类的**概率**。选择顺序也将决定输出顺序。
- en: Summary
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced two features integrated into SageMaker Studio—JumpStart
    and Autopilot—with three ML use cases to demonstrate low-to-no code ML options
    for ML developers. We learned how to browse JumpStart solutions in the catalog
    and how to deploy an end-to-end CV solution from JumpStart to detect defects in
    products. We also deployed and fine-tuned a question-answering model using the
    DistilRoBERTa Base model from the JumpStart model zoo without any ML coding. With
    Autopilot, we built a white wine quality prediction model simply by pointing Autopilot
    to a dataset stored in S3 and starting an Autopilot job – no code necessary. It
    turned out that Autopilot even outperforms the model created by the original researchers,
    which may have taken months of research.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了集成到 SageMaker Studio 的两个功能——JumpStart 和 Autopilot，以及三个机器学习用例，以展示为机器学习开发者提供的低代码到无代码的机器学习选项。我们学习了如何在目录中浏览
    JumpStart 解决方案，以及如何从 JumpStart 部署端到端的计算机视觉解决方案以检测产品缺陷。我们还使用 JumpStart 模型动物园中的
    DistilRoBERTa Base 模型部署并微调了一个问答模型，而无需任何机器学习编码。使用 Autopilot，我们只需将 Autopilot 指向存储在
    S3 中的数据集并启动一个 Autopilot 作业，就可以简单地构建一个白葡萄酒质量预测模型——无需编写代码。结果证明，Autopilot 的表现甚至超过了原始研究人员可能花费数月时间创建的模型。
- en: 'With the next chapter, we begin the next part of the book: *Production and
    Operation of Machine Learning with SageMaker Studio*. We will learn how we can
    move from prototyping to production ML training at scale with distributed training
    in SageMaker, how to monitor model training easily with SageMaker Debugger, how
    to save training cost with managed spot training..'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们开始本书的下一部分：*使用 SageMaker Studio 的机器学习生产与操作*。我们将学习如何通过 SageMaker 的分布式训练从原型设计过渡到大规模的机器学习训练，如何使用
    SageMaker Debugger 轻松监控模型训练，以及如何通过托管 Spot 训练节省训练成本。
- en: Further reading
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information take a look at the following resources:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 想要了解更多信息，请查看以下资源：
- en: P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. *Modeling wine preferences
    by data mining from physicochemical properties*. In Decision Support Systems,
    Elsevier, 47(4):547-553, 2009. [https://bit.ly/3enCZUz]( https://bit.ly/3enCZUz)
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P. Cortez, A. Cerdeira, F. Almeida, T. Matos 和 J. Reis. *通过数据挖掘物理化学性质建模葡萄酒偏好*。载于《决策支持系统》，Elsevier，第
    47 卷第 4 期，第 547-553 页，2009 年。[https://bit.ly/3enCZUz](https://bit.ly/3enCZUz)
