- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Online Learning for Time-Series
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列的在线学习
- en: In this chapter, we are going to dive into online learning and streaming data
    for time-series. Online learning means that we continually update our model as
    new data is coming in. The advantage of online learning algorithms is that they
    can handle the high speed and possibly large size of streaming data and are able
    to adapt to new distributions of the data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨时间序列的在线学习和流数据。在线学习意味着随着新数据的到来，我们不断更新模型。在线学习算法的优势在于，它们能够处理高速度和可能的大规模流数据，并能够适应数据的新分布。
- en: We will discuss drift, which is important because the performance of a machine
    learning model can be strongly affected by changes to the dataset to the point
    that a model will become obsolete (stale).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论漂移，漂移非常重要，因为机器学习模型的性能可能会因为数据集的变化而受到强烈影响，直到模型变得过时（陈旧）。
- en: We are going to discuss what online learning is, how data can change (drift),
    and how adaptive learning algorithms combine drift detection methods to adjust
    to this change in order to avoid the degradation of performance or costly retraining.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论什么是在线学习，数据如何变化（漂移），以及自适应学习算法如何结合漂移检测方法来适应这种变化，从而避免性能下降或代价高昂的再训练。
- en: 'We''re going to cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Online learning for time-series
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列的在线学习
- en: Online algorithms
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线算法
- en: Drift
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 漂移
- en: Drift detection methods
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 漂移检测方法
- en: Adaptive learning methods
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自适应学习方法
- en: Python practice
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python实践
- en: We'll start with a discussion of online learning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从在线学习的讨论开始。
- en: Online learning for time-series
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列的在线学习
- en: There are two main scenarios of learning – online learning and offline learning.
    **Online learning** means that you are fitting your model incrementally as the
    data flows in (streaming data). On the other hand, **offline learning**, the more
    commonly known approach, implies that you have a static dataset that you know
    from the start, and the parameters of your machine learning algorithm are adjusted
    to the whole dataset at once (often loading the whole dataset into memory or in
    batches).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 学习有两种主要场景——在线学习和离线学习。**在线学习**意味着您在数据流入时逐步拟合模型（流数据）。另一方面，**离线学习**，即更常见的方式，意味着您有一个从一开始就已知的静态数据集，机器学习算法的参数一次性调整整个数据集（通常将整个数据集加载到内存中或分批处理）。
- en: 'There are three major use cases for online learning:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习有三种主要的应用场景：
- en: Big data
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据
- en: Time constraints (for example, real time)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间限制（例如，实时）
- en: Dynamic environments
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态环境
- en: Typically, in online learning settings, you have more data, and it is appropriate
    for big data. Online learning can be applied to large datasets, where it would
    be computationally infeasible to train over the entire dataset.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在在线学习环境中，您有更多的数据，且非常适合大数据。在线学习可以应用于大数据集，因为在这些数据集上训练整个数据集在计算上不可行。
- en: Another use case for online learning is where the inference and fitting are
    performed under time constraints (for example, a real-time application), and many
    online algorithms are very resource-efficient in comparison to offline algorithms.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习的另一个应用场景是在时间限制下进行推断和拟合（例如，实时应用），与离线算法相比，许多在线算法在资源消耗上非常高效。
- en: A common application of online learning is on time-series data, and a particular
    challenge is that the underlying generating process of the time-series observations
    can change over time. This is called concept drift. While in the offline setting
    the parameters are fixed, in online learning, the parameters are continuously
    adapted based on new data. Therefore, online learning algorithms can deal with
    changes in the data, and some can deal with concept drift.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习的一个常见应用是在时间序列数据上，特别的挑战是时间序列观察的基础生成过程可能随时间变化。这被称为概念漂移。在离线设置中，参数是固定的，而在在线学习中，参数会根据新数据持续调整。因此，在线学习算法可以处理数据的变化，其中一些算法能够处理概念漂移。
- en: 'The table below summarizes some more differences between online and offline
    learning:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了在线学习和离线学习的一些区别：
- en: '|  | Offline | Online |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  | 离线 | 在线 |'
- en: '| Necessity to monitor | Yes, models can become stale (the model will lose
    performance) | Adapt to changing data |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 需要监控 | 是的，模型可能变得陈旧（模型将失去性能） | 适应变化的数据 |'
- en: '| Retraining costs | Expensive (from scratch) | Cheap (incremental) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 再训练成本 | 昂贵（从头开始） | 便宜（增量式） |'
- en: '| Memory requirements | Possibly high memory demands | Low |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 内存需求 | 可能需要较高的内存 | 低 |'
- en: '| Applications | Image classifiers, speech recognition, etc, where data is
    assumed to be static | Finance, e-commerce, economics, and health-care, where
    data is dynamically changing |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 应用领域 | 图像分类、语音识别等，假设数据是静态的 | 金融、电商、经济学和医疗保健等，数据是动态变化的 |'
- en: '| Tools | tslearn, sktime, prophet | Scikit-Multiflow, River |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 工具 | tslearn, sktime, prophet | Scikit-Multiflow, River |'
- en: 'Figure 8.1: Online vs offline learning methods in time-series'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：在线学习与离线学习方法的对比（时间序列）
- en: There are lots of other tools that are not specific to online learning but support
    online learning, such as the most popular deep learning libraries – PyTorch and
    TensorFlow, where models inherently support online learning and data loaders support
    streaming scenarios – through iterators, where data can be loaded in as needed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多不专门针对在线学习的工具，但支持在线学习，例如最受欢迎的深度学习库——PyTorch和TensorFlow，这些库的模型本身支持在线学习，数据加载器支持流式场景——通过迭代器，可以按需加载数据。
- en: 'A streaming formulation of a supervised machine learning problem can be posed
    as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 监督式机器学习问题的流式处理公式可以如下表示：
- en: A data point ![](img/B17577_08_001.png) is received at time *t*
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据点 ![](img/B17577_08_001.png) 在时间*t*到达
- en: The online algorithm predicts the label
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在线算法预测标签
- en: The true label is revealed before the next data point comes in
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个数据点到达之前，真实标签会被揭示
- en: In a batch setting, a set of *n* points ![](img/B17577_08_002.png) arrive all
    at once at time *t*, and all *n* points will be predicted by the online model
    before the true labels are revealed and the next batch of points arrives.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在批处理设置中，一组*n*个数据点 ![](img/B17577_08_002.png) 会在时间*t*同时到达，所有*n*个数据点将在真实标签被揭示之前，由在线模型进行预测，然后才会到达下一批数据点。
- en: 'We can demonstrate the difference in Python code snippets to show the characteristic
    patterns of machine learning in online and offline settings. You should be familiar
    with offline learning, which looks like this for features `X`, target vector `y`,
    and model parameters `params`:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过Python代码片段演示差异，展示在线与离线设置下机器学习的典型模式。你应该对离线学习熟悉，它的表现形式如下：特征`X`，目标向量`y`，和模型参数`params`：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This should be familiar from previous chapters such as *Chapter 7*, *Machine
    Learning Models for Time-Series*. For simplicity, we are omitting data loading,
    preprocessing, cross-validation, and parameter tuning, among other issues.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该是从之前的章节中熟悉的内容，比如*第7章*，*时间序列的机器学习模型*。为了简化，我们省略了数据加载、预处理、交叉验证和参数调优等问题。
- en: 'Online learning follows this pattern:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习遵循以下模式：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we are feeding point by point to the model. Again, this is simplified
    – I've omitted setting the parameters, loading the dataset, and more.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们是逐个数据点喂给模型。再次强调，这只是简化版——我省略了设置参数、加载数据集等内容。
- en: 'These snippets should make the main difference clear: learning on the whole
    dataset at once (offline) against learning on single points one by one (online).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代码片段应该清楚地说明主要的区别：一次性在整个数据集上学习（离线）与逐个数据点学习（在线）。
- en: 'I should mention evaluation methods for online methods:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该提到在线方法的评估方法：
- en: Holdout
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留集
- en: Prequential
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预先评估
- en: In **Holdout**, we can apply the current model to the independent test set.
    This is popular in batch as well as online (stream) learning and gives an unbiased
    performance estimation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在**保留集**方法中，我们可以将当前模型应用于独立的测试集。这种方法在批处理和在线（流式）学习中都很流行，并能提供无偏的性能估计。
- en: In **Prequential Evaluation**, we test as we are going through the sequence.
    Each new data point is first tested and then trained on.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在**预先评估**中，我们在通过数据序列的过程中进行测试。每个新的数据点会先进行测试，然后再进行训练。
- en: An interesting aspect of online learning is model selection, that is, how to
    select the best model among a set of candidate models. We looked at model selection
    for time-series models in *Chapter 4*, *Machine Learning Models for Time-Series*.
    There are different options for model selection in the online setting.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习的一个有趣方面是模型选择，也就是如何在一组候选模型中选择最佳模型。我们在*第4章*，*时间序列的机器学习模型*中讨论了时间序列模型的模型选择。在在线环境中，模型选择有不同的选项。
- en: In a **Multi-Armed Bandit** (also **K-Armed Bandit**) problem, limited resources
    must be allocated between competing choices in a way that maximizes expected gain.
    Each choice ("arm") comes with its reward, which can be learned over time. Over
    time, we can adapt our preference for each of these arms and choose optimally
    in terms of expected reward. Similarly, by learning expected rewards for competing
    classification or regression models, methods for multi-armed bandits can be applied
    for model selection. In the practice section, we'll discuss multi-armed bandits
    for model selection.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在**多臂强盗**（也称为**K臂强盗**）问题中，有限的资源必须在多个竞争选项之间分配，以最大化预期收益。每个选择（“臂”）都有其回报，可以随着时间的推移进行学习。随着时间的推移，我们可以调整对这些臂的偏好，并根据预期回报进行最优选择。同样，通过学习不同分类或回归模型的预期回报，基于多臂强盗的方法可以应用于模型选择。在实践部分，我们将讨论用于模型选择的多臂强盗算法。
- en: In the following sections, we'll look at incremental methods and drift in more
    detail.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更详细地探讨增量方法和漂移。
- en: Online algorithms
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在线算法
- en: Where data becomes available gradually over time or its size exceeds system
    memory limits, then incremental machine learning algorithms, whether supervised
    learning or unsupervised, can update parameters on parts of the data rather than
    starting the learning from scratch. **Incremental learning** is where parameters
    are continuously adapted to adjust a model to new input data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据逐渐变得可用，或其大小超出系统内存限制时，增量式机器学习算法（无论是监督学习还是无监督学习）可以在数据的部分上更新参数，而不是从头开始学习。**增量学习**是指通过不断调整模型来适应新的输入数据。
- en: Some machine learning methods inherently support incremental learning. Neural
    networks (as in deep learning), nearest neighbor, and evolutionary methods (for
    example, genetic algorithms) are incremental and can therefore be applied in online
    learning settings, where they are continuously updated.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习方法天生支持增量学习。神经网络（如深度学习）、最近邻算法和进化方法（例如遗传算法）都是增量式的，因此可以应用于在线学习环境，其中它们会不断更新。
- en: Incremental algorithms may have random access to previous samples or prototypes
    (selected samples). These algorithms, such as based on the nearest neighbor algorithm,
    are called incremental algorithms with partial memory. Their variants can be suitable
    for cyclic drift scenarios.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 增量算法可能会随机访问以前的样本或原型（选定的样本）。这些算法，如基于最近邻算法的增量算法，称为具有部分记忆的增量算法。它们的变体适用于周期性漂移场景。
- en: Many well-known machine learning algorithms have incremental variants such as
    the adaptive random forest, the adaptive XGBoost classifier, or the incremental
    support vector machine.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 许多著名的机器学习算法都有增量式的变体，如自适应随机森林、自适应XGBoost分类器或增量支持向量机。
- en: Both reinforcement learning and active learning can be seen as types of online
    learning because they work in an online or active manner. We are going to discuss
    reinforcement learning in *Chapter 11*, *Reinforcement Learning for Time-Series*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习和主动学习可以看作是在线学习的类型，因为它们以在线或主动的方式进行工作。我们将在*第11章*中讨论强化学习，*时间序列的强化学习*。
- en: In online learning, updates are calculated continuously. At the heart of this
    is running statistics, so it could be illustrative to show how mean and variance
    can be calculated incrementally (in an online setting).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在在线学习中，更新是持续进行的。其核心是运行统计，因此展示如何在在线环境中增量计算均值和方差会很有帮助。
- en: 'Let''s look at the formulas for online arithmetic mean and online variance.
    As for the **online mean**, updating the mean ![](img/B17577_08_003.png) at time
    point *t* can be done as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下在线算术平均值和在线方差的公式。至于**在线均值**，在时间点*t*更新均值 ![](img/B17577_08_003.png) 可以按以下方式进行：
- en: '![](img/B17577_08_004.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_08_004.png)'
- en: where ![](img/B17577_08_005.png) is the number of previous updates – sometimes
    this is written as ![](img/B17577_08_006.png).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B17577_08_005.png) 是之前更新的次数——有时也写作 ![](img/B17577_08_006.png)。
- en: 'The **online variance** ![](img/B17577_08_007.png) can be calculated based
    on the online mean and the running sum of squares ![](img/B17577_08_008.png):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**在线方差** ![](img/B17577_08_007.png) 可以基于在线均值和运行中的平方和 ![](img/B17577_08_008.png)
    进行计算：'
- en: '![](img/B17577_08_009.png)![](img/B17577_08_010.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_08_009.png)![](img/B17577_08_010.png)'
- en: A downside to offline algorithms is that they are sometimes more difficult to
    implement and that there's a learning curve to getting up to speed with libraries,
    algorithms, and methods.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 离线算法的一个缺点是，它们有时更难实现，并且在掌握库、算法和方法时会有一定的学习曲线。
- en: scikit-learn, the standard library for machine learning in Python, only has
    a limited number of incremental algorithms. It is focused on batch-learning models.
    In contrast, there are specialized libraries for online learning with adaptive
    and incremental algorithms that cover many use cases, such as imbalanced datasets.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn是Python中机器学习的标准库，但它只有有限的增量算法，主要集中在批量学习模型上。相比之下，有许多专门用于在线学习的库，这些库具有自适应和增量算法，能够覆盖许多使用场景，如不平衡数据集。
- en: 'Research engineers, students, and machine learning researchers from the University
    of Waikato (New Zealand), Télécom ParisTech, and the École Polytechnique in Paris
    have been working on the **River library**. River is the result of merging two
    libraries: Creme (intended as a pun on incremental) and Scikit-Multiflow. River
    comes with many meta and ensemble methods. As a cherry on top, many of these meta
    or ensemble methods can use scikit-learn models as base models.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 来自新西兰怀卡托大学、巴黎电信学院（Télécom ParisTech）和巴黎综合理工学院（École Polytechnique）的研究工程师、学生和机器学习研究者们一直在开发**River库**。River是由两个库合并而成：Creme（作为增量的双关语）和Scikit-Multiflow。River包含了许多元方法和集成方法。作为点睛之笔，这些元方法或集成方法中的许多可以使用scikit-learn模型作为基础模型。
- en: At the time of writing, the River library has 1,700 stars and implements many
    unsupervised and supervised algorithms. At the time of writing, the documentation
    of River is still a work in progress, but lots of functionality is available,
    as we'll see in the practical section at the end of this chapter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，River库拥有1700个星标，并实现了许多无监督和有监督算法。虽然截至目前，River的文档仍在完善中，但许多功能已经可用，我们将在本章结尾的实践部分中看到。
- en: 'This chart shows the popularity of River and Scikit-Multiflow over time (by
    the number of stars on GitHub):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了River和Scikit-Multiflow随着时间的推移的受欢迎程度（以GitHub上的星标数为依据）：
- en: '![online_learning-star_history.png](img/B17577_08_01.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![online_learning-star_history.png](img/B17577_08_01.png)'
- en: 'Figure 8.2: Star histories of the River and Scikit-Multiflow libraries'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2：River和Scikit-Multiflow库的星标历史
- en: We can see that, while Scikit-Multiflow has risen steadily, this rise has been
    mostly flat. River overtook Scikit-Multiflow in 2019 and has continued to receive
    many star ratings from GitHub users. These star ratings are similar to a "like"
    on a social media platform.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，尽管Scikit-Multiflow稳步上升，但这一增长大多保持平稳。River在2019年超过了Scikit-Multiflow，并继续获得GitHub用户的大量星标。这些星标类似于社交媒体平台上的“点赞”。
- en: 'This table shows a few online algorithms, some of which are suitable for drift
    scenarios:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 本表展示了一些在线算法，其中部分适用于漂移场景：
- en: '| Algorithm | Description |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 描述 |'
- en: '| **Very Fast Decision Tree** (**VFDT**) | Decision tree made up of splits
    based on a few examples. Also called Hoeffding Tree. Struggles with drift. |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| **非常快速决策树**（**VFDT**） | 基于少量示例进行分裂的决策树，也叫做Hoeffding树。对漂移较为敏感。 |'
- en: '| **Extremely Fast Decision Tree** (**EFDT**) | Incrementally builds a tree
    by creating a split when confident and replaces the split if a better split is
    available. Assumes a stationary distribution. |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| **极快速决策树**（**EFDT**） | 当有足够信心时增量构建树，并在有更好分裂时替换当前分裂。假设数据分布是平稳的。 |'
- en: '| Learn++.NSE | Ensemble of classifiers for incremental learning from non-stationary
    environments. |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Learn++.NSE | 用于非平稳环境的增量学习分类器集成。 |'
- en: 'Figure 8.3: Online machine learning algorithms – some of them suitable for
    drift'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3：在线机器学习算法——其中一些适用于漂移场景
- en: The online algorithm par excellence is the **Hoeffding Tree** (Geoff Hulten,
    Laurie Spencer, and Pedro Domingos, 2001), also called **Very Fast Decision Tree**
    (**VFDT**). It is one of the most widely used online decision tree induction algorithms.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最具代表性的在线算法是**Hoeffding树**（Geoff Hulten、Laurie Spencer和Pedro Domingos，2001），也叫做**非常快速决策树**（**VFDT**）。它是最广泛使用的在线决策树归纳算法之一。
- en: While some online learning algorithms are reasonably efficient, the attained
    performance can be highly sensitive to the ordering of data points, and potentially,
    they might never escape from a local minimum they ended up in, driven by early
    examples. Appealingly, VFDTs provide high classification accuracy with theoretical
    guarantees that they will converge toward the performance of decision trees over
    time. In fact, the probability that a VFDT and a conventionally trained tree will
    differ in their tree splits decreases exponentially with the number of examples.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些在线学习算法相对高效，但其性能可能对数据点的顺序极为敏感，且它们可能永远无法摆脱由早期样本驱动的局部最小值。令人吸引的是，VFDT（非常快的决策树）提供了高分类精度，并且具有理论保证，随着时间的推移，它们将趋向于决策树的表现。事实上，VFDT和传统训练树在树分裂上的差异的概率会随着样本数量的增加而指数下降。
- en: 'The **Hoeffding bound**, proposed by Wassily Hoeffding in 1963, states that
    with probability ![](img/B17577_08_011.png), the calculated mean ![](img/B17577_08_012.png)
    of a random variable *Z*, calculated over *n* samples, deviates less than ![](img/B17577_08_013.png)
    from the true mean ![](img/B17577_08_014.png):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**Hoeffding界限**，由Wassily Hoeffding于1963年提出，表示以概率![](img/B17577_08_011.png)，随机变量*Z*的计算均值![](img/B17577_08_012.png)在*n*个样本中计算后，与真实均值![](img/B17577_08_014.png)的偏差小于![](img/B17577_08_013.png)：'
- en: '![](img/B17577_08_015.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_08_015.png)'
- en: In this equation, *R* is the range of the random variable *Z*. This bound is
    independent of the probability distribution that is generating the observations.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，*R*是随机变量*Z*的范围。这个界限与生成观测值的概率分布无关。
- en: As data comes in, new branches are continuously added and obsolete branches
    are cut out from the Hoeffding Tree. Problematically, however, under concept drift,
    some nodes may no longer satisfy the Hoefdding boundary.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据的输入，Hoeffding树会不断添加新的分支，并且淘汰过时的分支。然而，问题在于，在概念漂移的情况下，某些节点可能不再满足Hoeffding边界。
- en: In the next section, we'll look at drift, why you should care, and what to do
    about drift.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论漂移，为什么你需要关注它，以及如何处理漂移。
- en: Drift
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 漂移
- en: 'A major determinant of data quality is drift. **Drift** (also: **dataset shift**)
    means that the patterns in data change over time. Drift is important because the
    performance of a machine learning model can be adversely affected by changes to
    the dataset.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量的一个主要决定因素是漂移。**漂移**（也称为：**数据集漂移**）意味着数据中的模式随着时间的推移发生变化。漂移很重要，因为机器学习模型的表现可能会因为数据集的变化而受到不利影响。
- en: 'Drift transitions can occur abruptly, incrementally, gradually, or be recurring.
    This is illustrated here:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 漂移过渡可以突如其来，也可以逐渐发生、增量式发生，或者是周期性发生。下面是一个示例：
- en: '![../Conceptdrift4%20-%20page%201.png](img/B17577_08_02.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![../Conceptdrift4%20-%20page%201.png](img/B17577_08_02.png)'
- en: 'Figure 8.4: Four types of concept drift transitions'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4：四种类型的概念漂移过渡
- en: When the transition is abrupt, it happens from one time step to another without
    apparent preparation or warning. In contrast, it can also be incremental in the
    sense that there's first a little shift, then a bigger shift, then a bigger shift
    again.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当过渡突然而至时，它会从一个时间步骤跳跃到另一个时间步骤，没有明显的准备或警告。与此相对，它也可能是逐步的，首先是小幅变化，然后是更大的变化，再然后是更大的变化。
- en: When a transition happens gradually, it can look like a back and forth between
    different forces until a new baseline is established. Yet another type of transition
    is recurring or cyclical when there's a regular or recurring shift between different
    baselines.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当过渡逐渐发生时，它可能表现为不同力量之间的来回波动，直到建立一个新的基线。另一种过渡类型是周期性的，当存在不同基线之间的规律性或重复性的变化时。
- en: 'There are different kinds of drift:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 漂移有不同种类：
- en: Covariate drift
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协变量漂移
- en: Prior-probability drift
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 先验概率漂移
- en: Concept drift
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概念漂移
- en: '**Covariate drift** describes a change in the independent variables (features).
    An example could be a regulatory intervention, where new laws would shake up the
    market landscape, and consumer behavior would follow different behaviors from
    before. An example would be if we want to predict chronic disease within 10 years
    given smoking behaviors, and smoking suddenly becomes much less prevalent, because
    of new laws. This means that our prediction could be less reliable.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**协变量漂移**描述了自变量（特征）的变化。一个例子可能是法规干预，其中新的法律会震动市场格局，消费者行为也会与之前不同。例如，如果我们想预测吸烟行为下的慢性疾病风险，并且吸烟变得不再普遍，因为有了新的法律，这意味着我们的预测可能会变得不那么可靠。'
- en: '**Probability drift** is a change in the target variable. An example could
    be that in fraud detection, the fraud incidence ratio changes; in retail, the
    average value of merchandise increases. One reason for drift could be seasonality
    – for example, selling more coats in winter.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**概率漂移**是目标变量的变化。例如，在欺诈检测中，欺诈发生的比例发生变化；在零售中，商品的平均价值增加。漂移的一个原因可能是季节性因素——例如，在冬季卖出更多的外套。'
- en: In **concept drift**, a change occurs in the relationship between the independent
    and the target variables. The concept the term refers to is the relationship between
    independent and dependent variables. For example, if we wanted to predict the
    number of cigarettes smoked, we could assume that our model would become useless
    after the introduction of new laws. Please note that often, the term concept drift
    is applied in a broader sense as anything non-stationary.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在**概念漂移**中，自变量与目标变量之间的关系发生了变化。这个术语所指的概念是自变量与因变量之间的关系。例如，如果我们想预测吸烟的数量，我们可以假设，在新法律出台后，我们的模型将变得无效。请注意，通常“概念漂移”一词在更广泛的意义上应用，指的是任何非平稳的变化。
- en: '**Covariate drift**: a change in the features *P(x)*.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**协变量漂移**：特征*P(x)*的变化。'
- en: '**Label drift (**or **prior probability drift)**: a change in the target variable
    *P(y)*.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**标签漂移（**或**先验概率漂移）**：目标变量*P(y)*的变化。'
- en: '**Concept drift**: (in supervised machine learning) changes in the conditional
    distribution of the target – in other words, the relationship between independent
    and dependent variables changes *P(y|X)*.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**概念漂移**：（在有监督的机器学习中）目标条件分布的变化——换句话说，自变量与因变量之间的关系发生了变化*P(y|X)*。'
- en: Commonly, when building machine learning models, we assume that points within
    different parts of the dataset belong to the same distribution.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在构建机器学习模型时，我们假设数据集不同部分中的数据点属于相同的分布。
- en: While occasional anomalies, such as abnormal events, would usually be treated
    as noise and ignored, when there is a change in the distribution, models often
    have to be rebuilt from scratch based on new samples in order to capture the latest
    characteristics. This is the reason why we are testing time-series models with
    walk-forward validation as discussed in *Chapter 7*, *Machine Learning Models
    for Time-Series*. However, this training from scratch can be time-consuming and
    heavy on computing resources.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管偶尔出现的异常现象，如异常事件，通常会被视为噪声并被忽略，但当分布发生变化时，模型通常需要基于新的样本从头开始重建，以捕捉最新的特征。这就是我们在*第7章*《*时间序列的机器学习模型*》中讨论的使用前向验证测试时间序列模型的原因。然而，从头开始训练可能非常耗时并且需要大量的计算资源。
- en: Drift causes problems for machine learning models since models can become stale
    – they become unreliable over time since the relationships they capture are no
    longer valid. This results in the performance degradation of these models. Therefore,
    approaches to forecasting, classification, regression, or anomaly detection should
    be able to detect and react to concept drift in a timely manner, so that the model
    can be updated as soon as possible. Machine learning models are often retrained
    periodically to avoid performance degradation happening. Alternatively, retraining
    can be triggered when needed based on either the performance monitoring of models or
    based on change detection methods.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 漂移给机器学习模型带来了问题，因为模型可能会变得陈旧——它们随着时间推移变得不可靠，因为它们捕捉到的关系不再有效。这导致了这些模型的性能下降。因此，预测、分类、回归或异常检测的方法应能够及时检测并应对概念漂移，以便尽快更新模型。机器学习模型通常会定期重新训练，以避免性能下降。或者，也可以根据模型的性能监控或基于变化检测方法在需要时触发重新训练。
- en: As for applications on time-series, in many domains, such as finance, e-commerce,
    economics, and healthcare, the statistical properties of the time-series can change,
    rendering forecasting models useless. Puzzlingly, although the concept of the
    drift problem is well investigated in the literature, little effort has been invested
    in tackling it with time-series methods.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于时间序列的应用，在许多领域，如金融、电商、经济学和医疗健康，时间序列的统计特性可能会发生变化，从而使得预测模型变得无用。令人困惑的是，尽管漂移问题的概念在文献中已有充分研究，但在使用时间序列方法应对这一问题方面，投入的努力却很少。
- en: 'Gustavo Oliveira and others proposed in 2017 ("*Time-Series Forecasting in
    the Presence of Concept Drift: A PSO-based Approach*") training several time-series
    forecasting models. At each point in time, the parameters for each of these models
    were changed weighted by the latest performance (particle swarm optimization).
    When the best models (best particles) diverged beyond a certain confidence interval,
    retraining of models was triggered.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Gustavo Oliveira等人于2017年提出（"《在概念漂移下的时间序列预测：一种基于PSO的方法》"）训练多个时间序列预测模型。在每个时间点，这些模型的参数会根据最新的性能加权变化（粒子群优化）。当最佳模型（最佳粒子）超出某个置信区间时，便触发了模型的重新训练。
- en: 'The charts below illustrate a combination of error-triggered retraining and
    online learning, one approach to time-series forecasting:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了错误触发的重新训练和在线学习相结合的一种时间序列预测方法：
- en: '![https://github.com/GustavoHFMO/IDPSO-ELM-S/raw/master/images/idpso_elm_s_execution.png](img/B17577_08_03.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![https://github.com/GustavoHFMO/IDPSO-ELM-S/raw/master/images/idpso_elm_s_execution.png](img/B17577_08_03.png)'
- en: 'Figure 8.5: Online learning and retraining for time-series forecasting (IDPSO-ELM-S)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5：时间序列预测的在线学习与重新训练（IDPSO-ELM-S）
- en: You can see the error rates increasing periodically as concept drift is occurring,
    and, where, based on the concept of drift detection, retraining is triggered.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到随着概念漂移的发生，误差率周期性地增加，并且根据漂移检测的概念，触发了重新训练。
- en: Many online models have been specifically adapted to be robust to or handle
    concept drift. In this section, we'll discuss some of the most popular or best-performing
    ones. We'll also discuss methods for drift detection.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 许多在线模型已经专门适应了对概念漂移的鲁棒性或处理能力。在本节中，我们将讨论一些最流行或表现最佳的模型。我们还将讨论漂移检测方法。
- en: Drift detection methods
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 漂移检测方法
- en: There are lots of different methods to explicitly detect drift and distributional
    changes in data streams. Page-Hinkley (Page, 1954) and Geometric Moving Average
    (Roberts, 2000) are two of the pioneers.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的方法可以显式地检测数据流中的漂移和分布变化。Page-Hinkley（Page，1954）和几何移动平均（Roberts，2000）是其中的先驱者。
- en: Drift detectors monitor the model performance usually through a performance
    metric, however, they can also be based on input features, although this is more
    of an exception. The basic idea is that when there is a change in the class distribution
    of the samples, the model does not correspond anymore to the current distribution,
    and the performance degrades (the error rate increases). Therefore, quality control
    of the model performance can serve as drift detection.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 漂移检测器通常通过性能指标来监控模型性能，但它们也可以基于输入特征，尽管这更多的是一种例外。基本思想是，当样本的类别分布发生变化时，模型不再与当前分布相对应，性能下降（误差率增加）。因此，模型性能的质量控制可以作为漂移检测的依据。
- en: 'Drift detection methods can be categorized into at least three groups (after
    João Gama and others, 2014):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 漂移检测方法可以至少分为三类（根据João Gama等人，2014年）：
- en: Statistical process control
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计过程控制
- en: Sequential analysis
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序分析
- en: Windows-based comparison
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于窗口的比较
- en: Statistical process control methods take into account summary statistics such
    as the mean and standard deviation of model predictions. For example, the **Drift
    Detection Method** (**DDM**; João Gama and others, 2004) alerts if the error rate
    surpasses the previously recorded minimum error rate by three standard deviations.
    According to statistical learning theory, in a continuously trained model, errors
    should diminish with the number of samples, so this threshold should only be exceeded
    in the case of drift.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 统计过程控制方法考虑了模型预测的汇总统计量，如均值和标准差。例如，**漂移检测方法**（**DDM**；João Gama等人，2004）会在误差率超过之前记录的最小误差率三倍标准差时发出警报。根据统计学习理论，在持续训练的模型中，随着样本数量的增加，错误应该减少，因此只有在发生漂移的情况下，这个阈值才会被超过。
- en: Sequential methods are based on thresholds of model predictions. For example,
    in the **Linear Four Rates** (Wang, 2015) method, the rates in the contingency
    table are updated incrementally. Significance is calculated according to a threshold
    that is estimated once at the start by Monte Carlo sampling. This method can handle
    class imbalance better than DDM.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序方法基于模型预测的阈值。例如，在**线性四率**（Wang，2015）方法中，列联表中的比率会递增更新。显著性是根据一个在开始时通过蒙特卡洛抽样估算的阈值来计算的。该方法比DDM更能处理类别不平衡。
- en: '**Contingency table**: a table that compares the frequency distribution of
    variables. Specifically in machine learning classification, the table displays
    the predicted number of labels over the test set against the actual labels. In
    the case of binary classification, the cells show true positives, false positives,
    false negatives, and true negatives.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**列联表**：比较变量频率分布的表格。具体来说，在机器学习分类中，表格显示了预测标签在测试集上的数量与实际标签的对比。在二分类的情况下，单元格显示真正例、假正例、假反例和真反例。'
- en: Windows-based approaches monitor the distribution of errors. For example, **ADWIN**
    (**ADaptive WINdowing**) was published by Albert Bifet and Ricard Gavaldà in 2007\.
    Prediction errors within a time window *W* are partitioned into smaller windows,
    and the differences in mean error rates within these windows are compared to the
    Hoeffding bound. The original version proposes a variation of this strategy that
    has a time complexity of *O(log W)*, where *W* is the length of the window.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 基于窗口的方法监控错误的分布。例如，**ADWIN** (**自适应滑动窗口**) 是由Albert Bifet和Ricard Gavaldà于2007年提出的。时间窗口*W*内的预测错误被划分成更小的窗口，并将这些窗口内的平均误差率差异与Hoeffding界限进行比较。原始版本提出了一种变体，其时间复杂度为*O(log
    W)*，其中*W*是窗口的长度。
- en: 'Some methods for drift detection are listed here:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出了一些漂移检测方法：
- en: '| Algorithm | Description | Type |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 描述 | 类型 |'
- en: '| **Adaptive Windowing** (**ADWIN**) | Adaptive sliding window algorithm based
    on thresholds. | Window-based |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| **自适应窗口法** (**ADWIN**) | 基于阈值的自适应滑动窗口算法。 | 基于窗口 |'
- en: '| **Drift Detection Method** (**DDM**) | Based on the premise that the model''s
    error rate should decrease over time. | Statistical |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| **漂移检测方法** (**DDM**) | 基于模型错误率应随时间减少的前提。 | 统计方法 |'
- en: '| **Early Drift Detection Method** (**EDDM**) | Statistics over the average
    distance between two errors. Similar to DDM, but better for gradual drift. | Statistical
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| **早期漂移检测法** (**EDDM**) | 基于两个错误之间的平均距离统计。与DDM类似，但更适用于渐进式漂移。 | 统计方法 |'
- en: '| **Hoffding''s Drift Detection** (**HDDM**) | Non-parametric method based
    on Hoeffding''s bounds – either moving average-test or moving weighted average-test.
    | Window-based |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| **霍夫丁漂移检测** (**HDDM**) | 基于Hoeffding界限的非参数方法——移动平均检验或加权移动平均检验。 | 基于窗口 |'
- en: '| **Kolmogorov-Smirnov Windowing** (**KSWIN**) | Kolmogorov-Smirnov test in
    windows of a time-series. | Window-based |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| **Kolmogorov-Smirnov窗口法** (**KSWIN**) | 在时间序列的窗口中进行Kolmogorov-Smirnov检验。
    | 基于窗口 |'
- en: '| Page-Hinkley | Statistical test for mean changes in Gaussian signals. | Sequential
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Page-Hinkley | 用于高斯信号均值变化的统计检验。 | 序列化方法 |'
- en: 'Figure 8.6: Drift detection algorithms'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6：漂移检测算法
- en: Kolmogorov-Smirnov is a nonparametric test of the equality of continuous, one-dimensional
    probability distributions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Kolmogorov-Smirnov是一个非参数检验，用于检验连续一维概率分布的相等性。
- en: These methods can be used in the context of both regression and classification
    (and, by extension, forecasting). They can be used to trigger the retraining of
    models. For example, Hassan Mehmood and others (2021) retrained time-series forecasting
    models (among other models, they used Facebook's Prophet) if drift was detected.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法可以用于回归和分类（以及预测）场景中。它们可以用来触发模型的重新训练。例如，Hassan Mehmood等人（2021）在检测到漂移时，会重新训练时间序列预测模型（其中包括Facebook的Prophet模型）。
- en: Drift detectors all have their assumptions regarding input data. It is important
    to know these assumptions, and I've tried to outline these in the table, so you
    use the right detector with your dataset.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 漂移检测器都有其关于输入数据的假设。了解这些假设非常重要，我尝试在表格中概述了这些假设，以便你能选择适合你的数据集的检测器。
- en: The drift detection methods listed above all incur a labeling cost. Since they
    all monitor the prediction results of a base classifier or an ensemble, they require
    that the class labels is available right after prediction. This constraint is
    unrealistic in some practical problems. There are other methods, not listed here,
    that can be based on anomaly detection (or novelty detection), feature distribution
    monitoring, or model-dependent monitoring. We saw a few of these methods in *Chapter
    6*, *Unsupervised Methods for Time-Series*.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 上述列出的漂移检测方法都有标签成本。由于它们都监控基本分类器或集成分类器的预测结果，因此要求在预测后立即获得类标签。在某些实际问题中，这一限制是不切实际的。这里没有列出其他一些方法，这些方法可以基于异常检测（或新颖性检测）、特征分布监控或模型依赖监控。我们在*第6章*，*时间序列的无监督方法*中看到了一些这些方法。
- en: In the next section, we'll look at some methods that were designed to be resistant
    to drift.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍一些旨在抵抗漂移的方法。
- en: Adaptive learning methods
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自适应学习方法
- en: '**Adaptive learning** refers to incremental methods with drift adjustment.
    This concept refers to updating predictive models online to react to concept drifts.
    The goal is that by taking drift into account, models can ensure consistency with
    the current data distribution.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**自适应学习**是指具有漂移调整的增量方法。这个概念指的是通过在线更新预测模型，以应对概念漂移。目标是通过考虑漂移，使得模型能够确保与当前数据分布的一致性。'
- en: Ensemble methods can be coupled with drift detectors to trigger the retraining
    of base models. They can monitor the performance of base models (often with ADWIN)
    – underperforming models get replaced with retrained models if the new models
    are more accurate.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法可以与漂移检测器结合使用，以触发基础模型的重新训练。它们可以监控基础模型的表现（通常使用 ADWIN）——表现不佳的模型会被重新训练过的更精确模型替换。
- en: As a case in point, the **Adaptive XGBoost** algorithm (**AXGB**; Jacob Montiel
    and others, 2020) is an adaptation of XGBoost for evolving data streams, where
    new subtrees are created from mini-batches of data as new data becomes available.
    The maximum ensemble size is fixed, and once this size is reached, the ensemble
    is updated on new data.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，**自适应 XGBoost** 算法 (**AXGB**；Jacob Montiel 等人，2020年) 是 XGBoost 在处理不断变化的数据流时的适应性改编，其中新的子树是从数据的小批量中创建的，随着新数据的到来。这种算法的最大集成大小是固定的，一旦达到该大小，集成就会在新数据上进行更新。
- en: 'In the Scikit-Multiflow and River libraries, there are several methods that
    couple machine learning methods with drift-detection methods, which regulate adaptation.
    Many of these were published by the maintainers of the two libraries. Here''s
    a list with a few of these methods:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scikit-Multiflow 和 River 库中，有几种方法将机器学习方法与漂移检测方法结合，这些方法调节适应性。这些方法中的许多都是由这两个库的维护者发布的。以下是其中一些方法的列表：
- en: '| Algorithm | Description |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 描述 |'
- en: '| **K-Nearest Neighbors** (**KNN**) classifier with ADWIN change detector |
    KNN with ADWIN change detector to decide which samples to keep or forget. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| **K-近邻** (**KNN**) 分类器与 ADWIN 漂移检测器 | 使用 ADWIN 漂移检测器的 KNN，决定保留或遗忘哪些样本。 |'
- en: '| Adaptive Random Forest | Includes drift detectors per tree. It starts training
    in the background after a warning has been detected, and replaces the old tree
    if drift occurs. |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 自适应随机森林 | 每棵树都包含漂移检测器。它在检测到警告后开始在后台训练，如果发生漂移，则替换旧的树。 |'
- en: '| Additive Expert ensemble classifier | Implements pruning strategies – the
    oldest or weakest base model will be removed. |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 加法专家集成分类器 | 实现了修剪策略 —— 最旧或最弱的基础模型将被移除。 |'
- en: '| **Hoeffding Adaptive Tree** (**HAT**) | Pairs ADWIN to detect drift and a
    Hoeffding Tree model to learn. |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| **Hoeffding 自适应树** (**HAT**) | 将 ADWIN 与 Hoeffding 树模型结合，检测漂移并进行学习。 |'
- en: '| Very Fast Decision Rules | Similar to VFDT, but rule ensembles instead of
    a tree. In Scikit-Multiflow drift detection with ADWIN, DDM, and EDDM is supported.
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 非常快速的决策规则 | 类似于 VFDT，但使用规则集成而不是树。在 Scikit-Multiflow 中支持通过 ADWIN、DDM 和 EDDM
    进行漂移检测。 |'
- en: '| Oza Bagging ADWIN | Instead of sampling with replacement, each sample is
    given a weight. In River, this can be combined with the ADWIN change detector.
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Oza Bagging ADWIN | 不同于有放回采样，每个样本都赋予一个权重。在 River 中，可以将其与 ADWIN 漂移检测器结合使用。
    |'
- en: '| Online CSB2 | Online boosting algorithm that compromises between AdaBoost
    and AdaC2, and optionally uses a change detector. |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 在线 CSB2 | 一种在线提升算法，折衷了 AdaBoost 和 AdaC2，并可选择使用漂移检测器。 |'
- en: '| Online Boosting | AdaBoost with ADWIN drift detection. |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 在线提升 | 带有 ADWIN 漂移检测的 AdaBoost。 |'
- en: 'Figure 8.7: Adaptive learning algorithms'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7：自适应学习算法
- en: These methods are robust to drift by regulating the adaptation or learning with
    the concept of drift detection.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法通过调节适应性或学习，以漂移检测的概念来应对漂移。
- en: Let's try out a few of these methods!
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试这些方法中的一些！
- en: Python practice
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 实践
- en: 'The installation in this chapter is very simple, since, in this chapter, we''ll
    only use River. We can quickly install it from the terminal (or similarly from
    Anaconda Navigator):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的安装非常简单，因为在本章中，我们只会使用 River。我们可以从终端快速安装它（或通过 Anaconda Navigator 安装）：
- en: '[PRE2]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We'll execute the commands from the Python (or IPython) terminal, but equally,
    we could execute them from a Jupyter notebook (or a different environment).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 Python（或 IPython）终端执行这些命令，但同样，我们也可以在 Jupyter Notebook（或其他环境）中执行它们。
- en: Drift detection
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 漂移检测
- en: Let's start off by trying out drift detection with an artificial time-series.
    This follows the example in the tests of the River library.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从尝试使用人工时间序列进行漂移检测开始。这个例子来自River库的测试。
- en: 'We''ll first create an artificial time-series that we can test:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先创建一个可以测试的人工时间序列：
- en: '[PRE3]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This time-series is composed of two series that have different characteristics.
    Let's see how quickly the drift detection algorithms pick up on this.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个时间序列由两组具有不同特征的序列组成。让我们看看漂移检测算法多快能够检测到这一点。
- en: 'Running the drift detector over this means iterating over this dataset and
    feeding the values into the drift detector. We''ll create a function for this:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集上运行漂移检测器意味着我们需要遍历数据集，并将值输入到漂移检测器中。我们将为此创建一个函数：
- en: '[PRE4]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we can try the ADWIN drift detection method on this time-series. Let''s
    create another method to plot the drift points overlaid over the time-series:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在这个时间序列上尝试ADWIN漂移检测方法。让我们创建另一个方法，绘制漂移点与时间序列的叠加图：
- en: '[PRE5]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This is the plot for the ADWIN drift points:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这是ADWIN漂移点的图示：
- en: '![ADWIN_drift_detection.png](img/B17577_08_04.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![ADWIN_drift_detection.png](img/B17577_08_04.png)'
- en: 'Figure 8.9: ADWIN drift points on our artificial dataset'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9：我们人工数据集上的ADWIN漂移点
- en: I'd encourage you to play around with this and to also try out the other drift
    detection methods.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励你尝试一下，并尝试其他漂移检测方法。
- en: Next, we'll do a regression task.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进行回归任务。
- en: Regression
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归
- en: We are going to estimate the occurrence of medium-class solar flares.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将估计中等强度太阳耀斑的发生。
- en: For this, we'll use the solar flares dataset from the UCI machine learning repository.
    The River library ships with a zipped column-separated dataset of the dataset,
    and we'll load this, specify the column types, and choose the outputs we are interested
    in.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将使用来自UCI机器学习库的太阳耀斑数据集。River库附带了该数据集的压缩列分隔数据集，我们将加载它，指定列类型，并选择我们感兴趣的输出。
- en: 'Let''s plot the ADWIN results now:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们绘制ADWIN结果：
- en: '[PRE6]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Please note how we are choosing the number of targets and the converters, which
    contain the types for all feature columns.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们如何选择目标数量和转换器，这些转换器包含所有特征列的类型。
- en: 'Let''s have a look at what this looks like:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这会是什么样子：
- en: '[PRE7]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We see the first point of the dataset (the first row of the dataset):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到数据集的第一个点（数据集的第一行）：
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_XK8zOB/Screenshot
    2021-07-05 at 23.37.50.png](img/B17577_08_05.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_XK8zOB/Screenshot
    2021-07-05 at 23.37.50.png](img/B17577_08_05.png)'
- en: 'Figure 8.10: First point of the solar flare dataset for medium-sized flares'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10：中等强度太阳耀斑数据集的第一个点
- en: We see the ten feature columns as a dictionary and the output as a float.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到十个特征列作为字典，输出为浮动值。
- en: 'Let''s build our model pipeline in River:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在River中构建我们的模型管道：
- en: '[PRE8]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'A pipeline like this is very pleasant to read: numeric features get min-max
    scaled, while string features get one-hot encoded. The preprocessed features get
    fed into a Hoeffding Tree model for regression.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的管道非常易于阅读：数值特征进行最小-最大缩放，而字符串特征则进行独热编码。预处理后的特征输入到Hoeffding树模型中进行回归。
- en: 'We can now learn our model prequentially, by predicting values and then training
    them as discussed before:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以按预序列化的方式训练我们的模型，预测值并进行训练，就像之前讨论过的那样：
- en: '[PRE9]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We are using the **Mean Absolute Error** (**MAE**) as our metric.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用**平均绝对误差**（**MAE**）作为我们的评估指标。
- en: We get an MAE of 0.096979.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的MAE为0.096979。
- en: 'This prequential evaluation `evaluate.progressive_val_score()` is equivalent
    to the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预序列化评估`evaluate.progressive_val_score()`等同于以下内容：
- en: '[PRE10]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: I've added two extra lines to collect the error over time as the algorithm learns.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我已添加了两行代码来收集算法学习过程中的误差。
- en: 'Let''s plot this:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制一下这个图：
- en: '[PRE11]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This plot shows how this error evolves as a function of the number of points
    the algorithm encounters:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图显示了这个误差如何随着算法遇到的点数而变化：
- en: '![solar_flares_regression_mae.png](img/B17577_08_06.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![solar_flares_regression_mae.png](img/B17577_08_06.png)'
- en: 'Figure 8.11: MAE by the number of points'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11：根据点数计算的MAE
- en: We can see that, after 20-30 points, after the metric stabilizes, the Hoeffding
    Tree starts learning and the error keeps decreasing until about 800 points, at
    which point the error increases again. This could be a row ordering effect.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在20到30个点之后，度量稳定后，Hoeffding树开始学习，误差不断下降，直到大约800个点，之后误差再次增加。这可能是行排序效应。
- en: 'A dataset that has concept drift is the use case for an adaptive model. Let''s
    compare adaptive and non-adaptive models on a dataset with concept drift:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 概念漂移的数据集是适应性模型的典型应用场景。让我们在一个有概念漂移的数据集上比较适应性和非适应性模型：
- en: '[PRE12]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We will compare the Hoeffding Tree Regressor, the Adaptive Hoeffding Tree Regressor,
    and the Adaptive Random Forest Regressor. We take the default settings for each
    model.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将比较Hoeffding树回归器、适应性Hoeffding树回归器和适应性随机森林回归器。我们将采用每个模型的默认设置。
- en: 'We can use a synthetic dataset for this test. We can train each of the aforementioned
    models on the data stream and look at the **Mean Squared Error** (**MSE**) metric:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一个合成数据集来进行此测试。我们可以在数据流上训练前述的每个模型，并查看**均方误差**（**MSE**）指标：
- en: '[PRE13]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `evaluate.progressive_val_score` method iterates over each point of the
    dataset and updates the metric. We get the following result:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate.progressive_val_score`方法遍历数据集的每个点并更新度量。我们得到如下结果：'
- en: '[PRE14]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Your results might vary a bit because of the nature of these algorithms. We
    could set a random number generator seed to avoid this, however, I found it worth
    emphasizing this point.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些算法的性质，你的结果可能会有所不同。我们可以设置随机数生成器的种子来避免这种情况，然而，我认为强调这一点是值得的。
- en: We see the model error (MSE) in scientific notation, which helps in understanding
    the numbers, since they are quite large. You see the errors expressed in two parts,
    first a factor and then the order of magnitude as exponents to ten. The orders
    of magnitudes are the same for the three models, however, the Adaptive Random
    Forest Regressor obtained about a fifth of the error of what the other two got.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到模型误差（MSE）以科学计数法表示，这有助于理解这些数字，因为它们相当大。你会看到误差分为两部分，首先是因子，然后是以10为指数的数量级。三个模型的数量级相同，然而，适应性随机森林回归器的误差大约只有其他两个模型的五分之一。
- en: 'We can also visualize the error over time as the models learn and adapt:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过可视化误差随时间的变化，观察模型的学习和适应过程：
- en: '![performance_adaptive_models.png](img/B17577_08_07.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![performance_adaptive_models.png](img/B17577_08_07.png)'
- en: 'Figure 8.12: Model performance for a concept drift data stream (MSE)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12：概念漂移数据流的模型表现（MSE）
- en: There's no non-adaptive version of the random forest algorithm in River, so
    we can't compare this. We can't draw a clear conclusion about whether adaptive
    algorithms actually work better.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: River中没有非适应性版本的随机森林算法，所以我们无法进行比较。我们无法得出适应性算法是否真正更有效的明确结论。
- en: There are lots of other models, meta models, and preprocessors to try out if
    you want to have a play around.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想尝试不同的模型、元模型和预处理器，还有很多其他选项可以尝试。
- en: Model selection
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型选择
- en: We've mentioned model selection with multi-armed bandits earlier in this chapter,
    and here we'll go through a practical example. This is based on documentation
    in River.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章早些时候提到过使用多臂赌博机进行模型选择，这里我们将通过一个实际示例来演示。这基于River文档中的内容。
- en: Let's use `UCBRegressor` to select the best learning rate for a linear regression
    model. The same pattern can be used more generally to select between any set of
    (online) regression models.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`UCBRegressor`来选择线性回归模型的最佳学习率。相同的模式可以更广泛地应用于选择任何一组（在线）回归模型。
- en: 'First, we define the models:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义模型：
- en: '[PRE15]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We build and evaluate our models on the TrumpApproval dataset:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在TrumpApproval数据集上构建并评估我们的模型：
- en: '[PRE16]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We''ll apply the UCB bandit, which calculates reward for regression models:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用UCB赌博机算法，该算法计算回归模型的奖励：
- en: '[PRE17]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The bandit provides methods to train its models in an online fashion:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 该赌博机提供了以在线方式训练模型的方法：
- en: '[PRE18]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can inspect the number of times (as a percentage) each arm has been pulled.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查每个“臂”被拉动的次数（百分比）。
- en: '[PRE19]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The percentages for the four models are as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 四个模型的百分比如下：
- en: '[PRE20]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can also look at the average reward of each model:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看每个模型的平均奖励：
- en: '[PRE21]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The reward is as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励如下：
- en: '[PRE22]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can also plot the reward over time as it gets updated based on model performance:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制奖励随时间的变化，并根据模型表现进行更新：
- en: '![bandit_reward.png](img/B17577_08_08.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![bandit_reward.png](img/B17577_08_08.png)'
- en: 'Figure 8.13: Reward over time'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13：奖励随时间的变化
- en: You can see that the rewards slowly become known as we step through the data
    and the models get updated. The model rewards clearly separate at around 100 time
    steps, and at around 1,000 time steps, seem to have converged.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，随着我们逐步处理数据并更新模型，奖励逐渐变得明显。模型奖励在大约100个时间步长时明显分开，而在大约1000个时间步长时，似乎已经收敛。
- en: 'We can also plot the percentage of the time each of the different models have
    been chosen at each step (this is based on the reward):'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制不同模型在每个步骤中被选择的百分比（这是基于奖励的）：
- en: '![bandit_percentage.png](img/B17577_08_09.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![bandit_percentage.png](img/B17577_08_09.png)'
- en: 'Figure 8.14: Ratios of models chosen over time'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14：随着时间推移选择的模型比例
- en: This distribution roughly follows the reward distribution over time. This should
    be expected since the model choice depends on reward (and a random number that
    regulates exploration).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分布大致跟随随时间变化的奖励分布。这是可以预期的，因为模型选择依赖于奖励（以及一个调节探索的随机数）。
- en: We can also select the best model (the one with the highest average reward).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以选择最佳模型（即具有最高平均奖励的模型）。
- en: '[PRE23]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The learning rate chosen by the bandit is:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 赌博机选择的学习率是：
- en: '[PRE24]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The learning rate is 0.01.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率为 0.01。
- en: Summary
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we''ve discussed online learning. We''ve talked about some
    of the advantages of online learning methods:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了在线学习。我们谈到了在线学习方法的一些优点：
- en: They are efficient and can handle high-speed throughput
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们高效，能够处理高速吞吐量。
- en: They can work on very large datasets
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以处理非常大的数据集。
- en: And they can adjust to changes in data distributions
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们能够适应数据分布的变化。
- en: Concept drift is a change in the relationship between data and the target to
    learn. We've talked about the importance of drift, which is that the performance
    of a machine learning model can be strongly affected by changes to the dataset
    to the point that a model will become obsolete (stale).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 概念漂移是数据与目标之间关系的变化。我们已经讨论了漂移的重要性，机器学习模型的性能可能会受到数据集变化的强烈影响，甚至导致模型过时（陈旧）。
- en: Drift detectors don't monitor the data itself, but they are used to monitor
    model performance. Drift detectors can make stream learning methods robust against
    concept drift, and in River, many adaptive models use a drift detector for partial
    resets or for changing learning parameters. Adaptive models are algorithms that
    combine drift detection methods to avoid the degradation of performance or costly
    retraining. We've given an overview of a few adaptive learning algorithms.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 漂移检测器不直接监控数据本身，而是用于监控模型性能。漂移检测器可以使流学习方法对概念漂移具有鲁棒性，在 River 中，许多自适应模型使用漂移检测器进行部分重置或调整学习参数。自适应模型是将漂移检测方法结合使用的算法，旨在避免性能退化或避免高成本的重新训练。我们概述了几种自适应学习算法。
- en: In the Python practice, we've played around with a few of the algorithms in
    the River library, including drift detection, regression, and model selection
    with a multi-armed bandit approach.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 实践中，我们尝试了 River 库中的一些算法，包括漂移检测、回归和使用多臂赌博机方法的模型选择。
