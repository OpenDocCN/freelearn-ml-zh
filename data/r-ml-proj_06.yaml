- en: Image Recognition Using Deep Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度神经网络进行图像识别
- en: In 1966, Professor Seymour Papert at MIT conceptualized an ambitious summer
    project titled *The Summer Vision Project*. The task for the graduate student
    was to *plug a camera into a computer and enable it to understand what it sees*! I
    am sure it would have been super-difficult for the graduate student to have finished
    this project, as even today the task remains half complete.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在1966年，麻省理工学院的Seymour Papert教授构想了一个名为*夏季视觉项目*的雄心勃勃的夏季项目。对于研究生来说，任务是*将摄像头连接到计算机上，并使其能够理解它所看到的内容*！我相信研究生完成这个项目会非常困难，因为即使今天，这个任务仍然只完成了一半。
- en: A human being, when they look outside, is able to recognize the objects that
    they see. Without thinking, they are able to classify a cat as a cat, a dog as
    a dog, a plant as a plant, an animal as an animal—this is happening because the
    human brain draws knowledge from its extensive prelearned database. After all,
    as human beings, we have millions of years' worth of evolutionary context that
    enables us draw inferences from the thing that we see. Computer vision deals with
    replicating the human vision processes so as to pass them on to machines and automate
    them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当人类向外看时，能够识别他们所看到的物体。他们无需思考，就能将一只猫归类为猫，一只狗归类为狗，一株植物归类为植物，一个动物归类为动物——这是因为人类大脑从其广泛的先验学习数据库中提取知识。毕竟，作为人类，我们有数百万年的进化背景，使我们能够从我们所看到的事物中得出推论。计算机视觉处理的是复制人类视觉过程，以便将其传递给机器并自动化它们。
- en: 'This chapter is all about learning the theory and implementation of computer
    vision through **machine learning** (**ML**). We will build a feedforward deep
    learning network and **LeNet** to enable handwritten digit recognition. We will
    also build a project that uses a pretrained Inception-BatchNorm network to identify
    objects in an image. We will cover the following topics as we progress in this
    chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章全部关于通过机器学习（**ML**）学习计算机视觉的理论和实现。我们将构建一个前馈深度学习网络和**LeNet**以实现手写数字识别。我们还将构建一个使用预训练的Inception-BatchNorm网络来识别图像中对象的工程项目。随着我们在本章的进展，我们将涵盖以下主题：
- en: Understanding computer vision
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解计算机视觉
- en: Achieving computer vision with deep learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过深度学习实现计算机视觉
- en: Introduction to the MNIST dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MNIST数据集简介
- en: Implementing a deep learning network for handwritten digit recognition
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现用于手写数字识别的深度学习网络
- en: Implementing computer vision with pretrained models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练模型实现计算机视觉
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For the projects covered in this chapter, we'll make use of a very popular open
    dataset called MNIST. We'll use **Apache MXNet**, a modern open source deep learning
    software framework to train and deploy the required deep neural networks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章涵盖的项目，我们将使用一个非常流行的开源数据集MNIST。我们将使用**Apache MXNet**，这是一个现代开源深度学习软件框架，用于训练和部署所需的深度神经网络。
- en: Understanding computer vision
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解计算机视觉
- en: 'In today''s world, we have advanced cameras that are very successful at mimicking
    how a human eye captures light and color; but image-capturing in the right way
    is just stage one in the whole image-comprehension aspect. Post image-capturing,
    we will need to enable technology that interprets what has been captured and build
    context around it. This is what the human brain does when the eyes see something.
    Here comes the huge challenge: we all know that computers see images as huge piles
    of integer values that represent intensities across a spectrum of colors, and
    of course, computer have no context associated with the image itself. This is
    where ML comes into play. ML allows us to train a context for a dataset such that
    it enables computers to understand what objects certain sequences of numbers actually
    represent.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今世界，我们拥有先进的摄像头，它们在模仿人眼捕捉光和色彩方面非常成功；但在整个图像理解方面，正确地捕捉图像仅仅是第一步。在图像捕捉之后，我们需要启用能够解释所捕捉内容并围绕其构建上下文的技术。这就是人类大脑在眼睛看到某物时所做的事情。接下来是巨大的挑战：我们都知道，计算机将图像视为代表一系列颜色范围内强度的整数值的巨大堆叠，当然，计算机与图像本身没有关联的上下文。这就是机器学习（ML）发挥作用的地方。机器学习使我们能够为数据集训练一个上下文，这样它就能使计算机理解某些数字序列实际上代表什么对象。
- en: 'Computer vision is one of the emerging areas where ML is applied. It can be
    used for several purposes in various domains, including healthcare, agriculture,
    insurance, and the automotive industry. The following are some of its most popular
    applications:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉是机器学习应用中新兴的领域之一。它可以在多个领域用于多种目的，包括医疗保健、农业、保险和汽车行业。以下是一些其最流行的应用：
- en: Detecting diseases from medical images, such as CT scan/MRI scan images
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从医学图像中检测疾病，如CT扫描/MRI扫描图像
- en: Identifying crop diseases and soil quality to support a better crop yield
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别作物疾病和土壤质量以支持更好的作物产量
- en: Identifying oil reserves from satellite images
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从卫星图像中识别石油储备
- en: Self-driving cars
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动驾驶汽车
- en: Monitoring and managing skin condition for psoriasis patients
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控和管理银屑病患者的皮肤状况
- en: Classifying and distinguishing weeds from crops
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分杂草和作物
- en: Facial recognition
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部识别
- en: Extracting information from personal documents, such as passports and ID cards
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从个人文件中提取信息，例如护照和身份证
- en: Detecting terrain for drones and airplanes
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为无人机和飞机检测地形
- en: Biometrics
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生物识别
- en: Public surveillance
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公共监控
- en: Organizing personal photos
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织个人照片
- en: Answering visual questions
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回答视觉问题
- en: This is just the tip of the iceberg. It's not an overstatement to say that there
    is no domain where we cannot find an application for computer vision. Therefore,
    computer vision is a key area for ML practitioners to focus on.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是冰山一角。说没有哪个领域我们不能找到计算机视觉的应用，并不夸张。因此，计算机视觉是机器学习从业者需要关注的重点领域。
- en: Achieving computer vision with deep learning
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用深度学习实现计算机视觉
- en: 'To start with, let''s understand the term **deep learning**. It simply means
    **multilayered neural networks**. The multiple layers enable deep learning to
    be an enhanced and powerful form of a neural network. **Artificial neural networks**
    (**ANNs**) have been in existence since the 1950s. They have always been designed
    with two layers; however, deep learning models are built with multiple hidden
    layers. The following diagram shows a hypothetical deep learning model:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解术语**深度学习**。它简单地说就是**多层神经网络**。多层使得深度学习成为神经网络的一种增强和强大的形式。**人工神经网络**（**ANNs**）自20世纪50年代以来一直存在。它们一直被设计为两层；然而，深度学习模型是构建在多个隐藏层之上的。以下图表展示了一个假设的深度学习模型：
- en: '![](img/9d09fdae-a6ec-4587-9d21-fe9efc0062dc.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9d09fdae-a6ec-4587-9d21-fe9efc0062dc.png)'
- en: Deep learning model—High level architecture
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型—高级架构
- en: Neural networks are heavy on computation, therefore the **central processing
    unit** (**CPU**) that can be enabled with a maximum of 22 cores is generally thought
    of as an infrastructure blocker until recently. This infrastructure limitation
    also limited the usage of neural networks to solve real-world problems. However,
    recently, the availability of a **graphical processing unit** (**GPU**) with thousands
    of cores enabled has exponentially powerful computation possibilities when compared
    to CPUs. This gave a huge push to the usage of deep learning models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在计算上很重，因此能够启用最多22个核心的**中央处理器**（**CPU**）通常被认为是一个基础设施瓶颈，直到最近。这种基础设施限制也限制了神经网络解决现实世界问题的使用。然而，最近，具有数千个核心的**图形处理单元**（**GPU**）的可用性，与CPU相比，提供了指数级强大的计算可能性。这极大地推动了深度学习模型的使用。
- en: Data comes in many forms, such as tables, sounds, HTML files, TXT files, and
    images. Linear models do not generally learn from non-linear data. Non-linear
    algorithms, such as decision trees and gradient-boosting machines, also do not
    learn well from this kind of data. One the other hand, deep learning models that
    create non-linear interactions among the features give better solutions with non-linear
    data, so they have become the preferred models in the ML community.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以多种形式存在，如表格、声音、HTML文件、TXT文件和图像。线性模型通常无法从非线性数据中学习。非线性算法，如决策树和梯度提升机，也无法很好地从这类数据中学习。另一方面，创建特征之间非线性交互的深度学习模型，在处理非线性数据时能提供更好的解决方案，因此它们已成为机器学习社区中首选的模型。
- en: A deep learning model consists of a chain of interconnected neurons that creates
    the neural architecture. Any deep learning model will have an input layer, two
    or more hidden layers (middle layers), and an output layer. The input layer consists
    of neurons equal to the number of input variables in the data. Users can decide
    on the number of neurons and the number of hidden layers that a deep learning
    network should have. Generally, it is something that is optimized by the user
    building the network through a cross-validation strategy. The choice of the number
    of neurons and the number of hidden layers represents the challenge of the researcher.
    The number of neurons in the output layer is decided based on the outcome of the
    problem. For example, one output neuron in case it is regression, for a classification
    problem the output neurons is equal to the number of classes involved in the problem
    on-hand.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型由一系列相互连接的神经元组成，这些神经元创建了神经网络架构。任何深度学习模型都将有一个输入层、两个或更多隐藏层（中间层）和一个输出层。输入层由与数据中输入变量数量相等的神经元组成。用户可以决定深度学习网络应该有多少个神经元和隐藏层。通常，这是通过用户通过交叉验证策略构建网络来优化的。神经元数量和隐藏层数量的选择代表了研究者的挑战。输出层中的神经元数量基于问题的结果来决定。例如，如果是回归问题，则只有一个输出神经元；对于分类问题，输出神经元等于涉及问题的类别数量。
- en: Convolutional Neural Networks
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: There are multiple types of deep learning algorithms, the one we generally use
    in computer vision is called a **Convolutional Neural Network **(**CNN**). CNNs
    break down images into small groups of pixels and then run calculations on them
    by applying filters. The result is then compared against pixel matrices they already
    know about. This helps CNNs to come up with a probability for the image belonging
    to one of the known classes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法有多种类型，我们在计算机视觉中一般使用的一种称为**卷积神经网络**（**CNN**）。CNNs将图像分解成像素的小组，然后通过应用过滤器对这些像素进行计算。然后将结果与它们已经了解的像素矩阵进行比较。这有助于CNNs为图像属于已知类别之一提供概率。
- en: In the first few layers, the CNN identifies shapes, such as curves and rough
    edges, but after several convolutions, they are able to recognize objects such
    as animals, cars, and humans.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几层中，CNN识别形状，如曲线和粗糙边缘，但经过几次卷积后，它们能够识别动物、汽车和人类等物体。
- en: When the CNN is first built for the available data, the filter values of the
    network are randomly initialized and so the predictions it produce are mostly
    false. But then it keeps comparing its own predictions on labeled datasets to
    the actual ones, updating the filter values and improving performance of the CNN
    with each iteration.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当CNN首次为可用数据构建时，网络的滤波器值是随机初始化的，因此它产生的预测大多是错误的。但是，然后它不断将其在标记数据集上的预测与实际值进行比较，更新滤波器值，并在每次迭代中提高CNN的性能。
- en: Layers of CNNs
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNNs的层
- en: 'A CNN consists of an input and an output layer; it also has various hidden
    layers. The following are the various hidden layers in a CNN:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个CNN由一个输入层和一个输出层组成；它还有各种隐藏层。以下是一个CNN中的各种隐藏层：
- en: '**Convolution**: Assume that we have an image represented as pixels, a convolution
    is something where we have a little matrix nearly always 3 x 3 in deep learning
    and multiply every element of the matrix by every element of 3 x 3 section of
    the image and then add them all together to get the result of that convolution
    at one point. The following diagram illustrates the process of convolution on
    a pixel:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积**：假设我们有一个以像素表示的图像，在深度学习中，卷积通常是一个3 x 3的小矩阵，我们将矩阵的每个元素与图像的3 x 3部分的每个元素相乘，然后将它们全部加起来，得到该点卷积的结果。以下图表说明了卷积在像素上的过程：'
- en: '![](img/5357ec97-6f89-4348-92a2-26e7a6cea9b4.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5357ec97-6f89-4348-92a2-26e7a6cea9b4.png)'
- en: Convolution application on an image
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像上的卷积应用
- en: '**Rectified Linear Unit** (**ReLU**): A non-linear activation that throws away
    the negatives in an input matrix. For example, let''s assume we have a 3 x 3 matrix
    with negative numbers, zeros, and positive numbers as values in the cells of the
    matrix. Given this matrix as input to ReLU, it transforms all negative numbers
    in the matrix to zeros and returns the 3 x 3 matrix. ReLU is an activation function
    that can be defined as part of the CNN architecture. The following diagram demonstrates
    the function of ReLU in CNNs:'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReLU（修正线性单元）**：一个非线性激活，它丢弃输入矩阵中的负数。例如，假设我们有一个3 x 3的矩阵，其值在矩阵的单元格中为负数、零和正数。将此矩阵作为ReLU的输入，它将矩阵中的所有负数转换为零并返回3
    x 3矩阵。ReLU是一个可以作为CNN架构一部分定义的激活函数。以下图表展示了ReLU在CNN中的功能：'
- en: '![](img/ad096701-f380-414d-8028-f55c77a9f905.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ad096701-f380-414d-8028-f55c77a9f905.png)'
- en: Rectified Linear Unit (ReLU) in CNNs
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: CNN中的ReLU
- en: '**Max pooling**: Max pooling is something that can be set as a layer in the CNN
    architecture. It allows to identify if the specific characteristic is present
    in the previous level. It replaces the highest value in an input matrix with the
    maximum and gives the output. Let''s consider an example, with a 2 x 2 max pooling
    layer, given a 4 x 4 matrix as input, the max pooling layer replaces each 2 x
    2 in the input matrix with the highest value among the four cells. The output
    matrix thus obtained is non-overlapping and it''s an image representation with
    a reduced resolution. The following diagram illustrates the functionality of max
    pooling in a CNN:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大池化**：最大池化可以在CNN架构中设置为一个层。它允许识别特定特征是否存在于前一层。它将输入矩阵中的最高值替换为最大值并给出输出。让我们考虑一个例子，给定一个2
    x 2的最大池化层，输入一个4 x 4的矩阵，最大池化层将输入矩阵中的每个2 x 2替换为四个单元格中的最高值。因此获得的输出矩阵是非重叠的，它是一个具有降低分辨率的图像表示。以下图表说明了CNN中最大池化的功能：'
- en: '![](img/ce2cc9f2-2ea0-4c38-9c48-f99f26af8ebc.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ce2cc9f2-2ea0-4c38-9c48-f99f26af8ebc.png)'
- en: Functionality of max pooling layer in CNNs
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: CNN中最大池化层的功能
- en: There are various reasons to apply max pooling, such as to reduce the amount
    of parameters and computation load, to eliminate overfitting, and, most importantly,
    to force the neural network to see the larger picture, as in previous layers it
    was focused on seeing bits and pieces of the image.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 应用最大池化的原因有很多，例如减少参数数量和计算负载，消除过拟合，最重要的是，迫使神经网络看到更大的图景，因为在之前的层中，它专注于看到图像的片段。
- en: '**Fully-connected layer**: Also known as a **dense layer**, this involves a
    linear operation on the layer''s input vector. The layer ensures every input is
    connected to every output by a weight.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全连接层**：也称为**密集层**，它涉及对层的输入向量进行线性操作。该层确保每个输入都通过权重与每个输出相连接。'
- en: '**Softmax**: An activation function that is generally applied at the last layer
    of the deep neural network. In a multiclass classification problem, we require
    the fully-connected output of a deep learning network to be interpreted as a probability.
    The total probability of a particular observation in data (for all classes) should
    add up to 1, and the probability of the observation belonging to each class should
    range between 0 and 1\. Therefore, we transform each output of the fully-connected
    layer as a portion of a total sum. However, instead of simply doing the standard
    proportion, we apply this non-linear exponential function for a very specific
    reason: we would like to make our highest output as close to 1 as possible and
    our lower output as close to 0\. Softmax does this by pushing the true linear
    proportions closer to either 1 or 0.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Softmax**：通常应用于深度神经网络最后一层的激活函数。在多类分类问题中，我们需要深度学习网络的全连接输出被解释为概率。数据中特定观察的总概率（对于所有类别）应加起来为1，并且观察属于每个类别的概率应在0到1之间。因此，我们将全连接层的每个输出转换为总和中的一部分。然而，我们不是简单地做标准比例，而是出于一个非常具体的原因应用这个非线性指数函数：我们希望将最高输出尽可能接近1，将较低输出尽可能接近0。Softmax通过将真实线性比例推向1或0来实现这一点。'
- en: 'The following diagram illustrates the softmax activation function:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了softmax激活函数：
- en: '![](img/532ddd0f-24a2-41f4-bdac-293eecfdaf2e.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/532ddd0f-24a2-41f4-bdac-293eecfdaf2e.png)'
- en: Softmax activation function
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax激活函数
- en: '**Sigmoid**: This is similar to softmax, except that it is applied to a binary
    classification, such as cats versus dogs. With this activation function, the class
    to which the observation belongs is assigned a higher probability compared to
    the other class. Unlike softmax, the probabilities do not have to add up to 1.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sigmoid**：这与softmax类似，但应用于二分类，例如猫与狗。使用这种激活函数，观察所属的类别被分配比其他类别更高的概率。与softmax不同，概率不需要加起来等于1。'
- en: Introduction to the MXNet framework
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MXNet框架简介
- en: MXNet is a super-powerful open source deep learning framework that is built
    to ease the development of deep learning algorithms. It is used to define, train,
    and deploy deep neural networks. MXNet is lean, flexible, and ultra-scalable,
    that is, it allows fast model training and supports a flexible programming model
    with multiple languages. The problem with existing deep learning frameworks, such
    as Torch7, Theano, and Caffe, is that users need to learn another system or a
    different programming flavor.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet是一个功能强大的开源深度学习框架，旨在简化深度学习算法的开发。它用于定义、训练和部署深度神经网络。MXNet轻量级、灵活且超可扩展，即它允许快速模型训练并支持多种语言的灵活编程模型。现有深度学习框架（如Torch7、Theano和Caffe）的问题在于，用户需要学习另一个系统或不同的编程风格。
- en: However, MXNet resolves this issue by supporting multiple languages, such as
    C++, Python, R, Julia, and Perl. This eliminates the need for users to learn a
    new language; therefore, they can use the framework and simplify network definitions.
    MXNet models are able to fit in small amounts of memory and they can be trained
    on CPUs, GPUs, and on multiple machines with ease. The `mxnet` package is readily
    available for the R language and the details of the install can be looked up in
    **Apache Incubator** at [https://mxnet.incubator.apache.org](https://mxnet.incubator.apache.org).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，MXNet通过支持多种语言（如C++、Python、R、Julia和Perl）解决了这个问题，从而消除了用户学习新语言的需求；因此，他们可以使用该框架并简化网络定义。MXNet模型能够适应小量的内存，并且可以轻松地在CPU、GPU以及多台机器上训练。`mxnet`包对R语言来说已经可用，安装详情可以在[Apache孵化器](https://mxnet.incubator.apache.org)中查阅。
- en: Understanding the MNIST dataset
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解MNIST数据集
- en: '**Modified National Institute of Standards and Technology** (**MNIST**) is
    a dataset that contains images of handwritten digits. This dataset is pretty popular
    in the ML community for implementing and testing computer vision algorithms. The
    MNIST dataset is an open dataset made available by Professor Yann LeCun at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/),
    where separate files that represent the training dataset and test dataset are
    available. The labels corresponding to the test and training datasets are also
    available as separate files. The training dataset has 60,000 samples and the test
    dataset has 10,000 samples.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**修改后的国家标准与技术研究院**（**MNIST**）是一个包含手写数字图像的数据集。这个数据集在机器学习社区中非常流行，用于实现和测试计算机视觉算法。MNIST数据集是一个由Yann
    LeCun教授提供的开源数据集[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)，其中提供了表示训练集和测试集的单独文件。测试集和训练集的标签也作为单独的文件提供。训练集有60,000个样本，测试集有10,000个样本。'
- en: 'The following diagram shows some sample images from the MNIST dataset. Each
    of the images also comes with a label indicating the digit shown in the following
    screenshot:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了MNIST数据集的一些样本图像。每个图像还附带一个标签，指示以下截图中显示的数字：
- en: '![](img/9f8f082b-0bbb-4e47-ab65-6cd430a0b1d5.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9f8f082b-0bbb-4e47-ab65-6cd430a0b1d5.png)'
- en: Sample images from MNIST dataset
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集的样本图像
- en: 'The labels for the images shown in the preceding diagram are **5**, **0**,
    **4**, and **1**. Each image in the dataset is a grayscale image and is represented
    in 28 x 28 pixels. A sample image represented with pixels is shown in the following
    screenshot:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 前一图中显示的图像标签是**5**、**0**、**4**和**1**。数据集中的每个图像都是灰度图像，并以28 x 28像素表示。以下截图显示了以像素表示的样本图像：
- en: '![](img/853fd20e-35be-49ed-949c-57b05ea199fd.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/853fd20e-35be-49ed-949c-57b05ea199fd.png)'
- en: Sample image from MNIST dataset represented with 28 * 28 pixels
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集的样本图像，以28 * 28像素表示
- en: 'It is possible to flatten the 28 x 28 pixel matrix and represent it as a vector
    of 784 pixel values. Essentially, the training dataset is a 60,000 x 784 matrix
    that could be used with ML algorithms. The test dataset is a 10,000 x 784 matrix.
    The training and test datasets may be downloaded from the source with the following
    code:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将 28 x 28 像素矩阵展平，表示为 784 个像素值的向量。本质上，训练数据集是一个 60,000 x 784 的矩阵，可以与 ML 算法一起使用。测试数据集是一个
    10,000 x 784 的矩阵。可以使用以下代码从源下载训练和测试数据集：
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once the data is downloaded and unzipped, we will see the files in our working
    directory. However, these files are in binary format and they cannot be directly
    loaded through the regular `read.csv` command. The following custom function code
    helps to read the training and test data from the binary files:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据下载并解压，我们将在工作目录中看到文件。然而，这些文件是二进制格式，不能通过常规的 `read.csv` 命令直接加载。以下自定义函数代码有助于从二进制文件中读取训练和测试数据：
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The functions may be called with the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下代码调用这些函数：
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In RStudio, when we execute the code, we see `train` , `test`,  `train.y`,
    and `test.y` displayed under the Environment tab. This confirms that the datasets
    are successfully loaded and the respective dataframes are created, as shown in
    the following screenshot:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RStudio 中，当我们执行代码时，我们可以在环境标签下看到 `train`、`test`、`train.y` 和 `test.y`。这证实了数据集已成功加载，并且相应的数据框已创建，如下面的截图所示：
- en: '![](img/b85709ce-90db-428b-a38a-aa7c6f57cb78.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b85709ce-90db-428b-a38a-aa7c6f57cb78.png)'
- en: 'Once the image data is loaded into the dataframe, it is in the form of a series
    of numbers that represent the pixel values. The following is a helper function
    that visualizes the pixel data as an image in RStudio:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦图像数据被加载到数据框中，它就变成了代表像素值的数字序列。以下是一个辅助函数，它将像素数据可视化为 RStudio 中的图像：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `show_digit()` function may be called like any other R function with the
    dataframe record number as a parameter. For example, the function in the following
    code block helps to visualize the `3` record in the training dataset as an image
    in RStudio:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`show_digit()` 函数可以像任何其他 R 函数一样调用，以数据框记录号作为参数。例如，以下代码块中的函数有助于将训练数据集中的 `3` 记录可视化为
    RStudio 中的图像：'
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will give the following output:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/88617ad9-3c55-44fb-9a2e-8957dae8647c.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/88617ad9-3c55-44fb-9a2e-8957dae8647c.png)'
- en: 'Dr. David Robinson, in his blog on *Exploring handwritten digit classification:
    a tidy analysis of the MNIST dataset* ([http://varianceexplained.org/r/digit-eda/](http://varianceexplained.org/r/digit-eda/)),
    performed a beautiful exploratory data analysis of the MNIST dataset, which will
    help you better understand the dataset.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 大卫·罗宾逊博士在他的博客 *探索手写数字分类：MNIST 数据集的整洁分析* ([http://varianceexplained.org/r/digit-eda/](http://varianceexplained.org/r/digit-eda/))
    中，对 MNIST 数据集进行了美丽的探索性数据分析，这将帮助您更好地理解数据集。
- en: Implementing a deep learning network for handwritten digit recognition
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现手写数字识别的深度学习网络
- en: The `mxnet` library offers several functions that enable us to define the layers
    and activations that comprise the deep learning network. The definition of layers,
    the usage of activation functions, and the number of neurons to be used in each
    of the hidden layers is generally termed the **network architecture**. Deciding
    on the network architecture is more of an art than a science. Often, several iterations
    of experiments may be needed to decide on the right architecture for the problem.
    We call it an art as there are no exact rules for finding the ideal architecture.
    The number of layers, neurons in these layers, and the type of layers are pretty
    much decided through trial and error.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`mxnet` 库提供了几个函数，使我们能够定义构成深度学习网络的层和激活。层的定义、激活函数的使用以及每个隐藏层中要使用的神经元数量通常被称为**网络架构**。决定网络架构更多的是一种艺术而非科学。通常，可能需要多次实验迭代来决定适合该问题的正确架构。我们称之为艺术，因为没有找到理想架构的精确规则。层的数量、这些层中的神经元类型以及层的类型很大程度上是通过试错来决定的。'
- en: 'In this section, we''ll build a simple deep learning network with three hidden
    layers. Here is the general architecture of our network:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个具有三个隐藏层的简单深度学习网络。以下是我们的网络的一般架构：
- en: The input layer is defined as the initial layer in the network. The `mx.symbol.Variable` MXNet
    function defines the input layer.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入层被定义为网络中的初始层。`mx.symbol.Variable` MXNet 函数定义了输入层。
- en: A fully-connected layer is defined, also called a dense layer, with 128 neurons
    as the first hidden layer in the network. This can be done using the `mx.symbol.FullyConnected` MXNet function.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在网络中定义了一个全连接层，也称为密集层，作为第一个隐藏层，具有128个神经元。这可以通过`mx.symbol.FullyConnected` MXNet函数实现。
- en: A ReLU activation function is defined as part of the network. The `mx.symbol.Activation`
    function helps us to define the ReLU activation function as part of the network.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ReLU激活函数作为网络的一部分被定义。`mx.symbol.Activation`函数帮助我们定义网络中的ReLU激活函数。
- en: Define the second hidden layer; it is another dense layer with 64 neurons. This
    can be accomplished through the `mx.symbol.FullyConnected` function, similar to
    the first hidden layer.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义第二个隐藏层；它是一个具有64个神经元的另一个密集层。这可以通过`mx.symbol.FullyConnected`函数实现，类似于第一个隐藏层。
- en: Apply a ReLU activation function on the second hidden layer's output. This can
    be done through the `mx.symbol.Activation` function.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第二个隐藏层的输出上应用ReLU激活函数。这可以通过`mx.symbol.Activation`函数实现。
- en: The final hidden layer in our network is another fully-connected layer, but
    with just ten outputs (equal to the number of classes). This can be done through
    the `mx.symbol.FullyConnected` function as well.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们网络中的最后一个隐藏层也是一个全连接层，但只有十个输出（等于类别的数量）。这也可以通过`mx.symbol.FullyConnected`函数实现。
- en: The output layer needs to be defined and this should be probabilities of prediction
    for each class; therefore, we apply softmax at the output layer. The `mx.symbol.SoftmaxOutput`
    function enables us to configure the softmax in the output.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出层需要被定义，并且这应该是每个类别的预测概率；因此，我们在输出层应用softmax。`mx.symbol.SoftmaxOutput`函数使我们能够配置输出中的softmax。
- en: We are not saying that this is the best network architecture possible for the
    problem, but this is the network we are going to build to demonstrate the implementation
    of a deep learning network with MXNet.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不是说这是针对该问题的最佳网络架构，但这是我们打算构建以展示使用MXNet实现深度学习网络架构的架构。
- en: 'Now that we have a blueprint in place, let''s delve into coding the network
    using the following code block:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了蓝图，让我们深入编写以下代码块来构建网络：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will give the following output:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, define the three layers and start training the network to obtain class
    probabilities and ensure the results are reproducible using the following code
    block:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，定义三个层并开始训练网络以获得类别概率，并确保使用以下代码块可重现结果：
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will give the following output:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To make predictions on the test dataset and get the label for each observation
    in the test dataset, use the following code block:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要在测试数据集上进行预测并获取测试数据集中每个观察的标签，请使用以下代码块：
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This will give the following output:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s check the performance of the model using the following code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下代码检查模型的性能：
- en: '[PRE11]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will give the following output:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To visualize the network architecture, use the following code:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化网络架构，请使用以下代码：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This will give the following output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/1b6945d7-f014-474a-b699-64cef7c3374d.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1b6945d7-f014-474a-b699-64cef7c3374d.png)'
- en: With the simple architecture running for a few minutes on a CPU-based laptop
    and with minimal effort, we were able to achieve an accuracy of `97.7%` on the
    test dataset. The deep learning network was able to learn to interpret the digits
    by seeing the images it was given as input. The accuracy of the system can be
    further improved by altering the architecture or by increasing the number of iterations.
    It may be noted that, in the earlier experiment, we ran it for 10 iterations.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于CPU的笔记本电脑上运行简单的架构几分钟，并且付出最小的努力，我们能够在测试数据集上实现`97.7%`的准确率。深度学习网络能够通过观察它提供的输入图像来学习解释数字。通过改变架构或增加迭代次数，可以进一步提高系统的准确率。值得注意的是，在早期实验中，我们运行了10次迭代。
- en: 'The number of iterations can simply be amended when model-building through
    the `num.round` parameter. There is no hard-and-fast rule in terms of the optimal
    number of rounds, so this is something to be determined by trial and error. Let''s
    build the model with 50 iterations and observe its impact on performance. The
    code will remain the same as the earlier project, except with the following amendment
    to the model-building code:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型构建过程中，可以通过`num.round`参数简单地修改迭代次数。关于最佳迭代次数，也没有硬性规定，所以这也是需要通过试错来确定的事情。让我们构建一个包含50次迭代的模型，并观察其对性能的影响。代码将与早期项目相同，只是在模型构建代码中进行了以下修改：
- en: '[PRE14]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Observe that the `num.round` parameter is now set to `50`, instead of the earlier
    value of `10`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到`num.round`参数现在设置为`50`，而不是之前的`10`。
- en: 'This will give the following output:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can observe from the output that 100% accuracy was obtained with the training
    dataset. However, with the test dataset, we observe the accuracy as 98%. Essentially,
    our model is expected to perform the same with both the training and test dataset
    for it to be called a good model. Unfortunately, in this case, we have encountered
    a situation known as **overfitting,** which means that the model we created did
    not generalize well. In other words, the model has trained itself with too many
    parameters or it got trained for too long and has become super-specialized with
    data in the training dataset alone; as an effect, it is not doing a good job with
    new data. Model generalization is something we should specifically aim for. There
    is a technique, known as **dropout**, that can help us to overcome the overfitting
    issue.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从输出中观察到，训练数据集达到了100%的准确率。然而，在测试数据集上，我们观察到的准确率为98%。本质上，我们的模型在训练和测试数据集上都应该表现出相同的性能，才能被称为一个好的模型。不幸的是，在这种情况下，我们遇到了一种称为**过拟合**的情况，这意味着我们创建的模型泛化能力不好。换句话说，模型用太多的参数进行了训练，或者训练时间过长，仅针对训练数据集的数据变得过于专业化；结果，它在新数据上表现不佳。模型泛化是我们应该特别追求的目标。有一种称为**dropout**的技术可以帮助我们克服过拟合问题。
- en: Implementing dropout to avoid overfitting
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现dropout以避免过拟合
- en: Dropout is defined in the network architecture after the activation layers,
    and it randomly sets activations to zero. In other words, dropout randomly deletes
    parts of the neural network, which allows us to prevent overfitting. We can't
    overfit exactly to our training data when we're consistently throwing away information
    learned along the way. This allows our neural network to learn to generalize better.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是在激活层之后的网络架构中定义的，它会随机将激活值设置为0。换句话说，dropout随机删除了神经网络的一部分，这使我们能够防止过拟合。当我们持续丢弃在过程中学习到的信息时，我们无法完全过拟合我们的训练数据。这允许我们的神经网络更好地学习泛化。
- en: 'In MXNet, dropout can be easily defined as part of network architecture using
    the `mx.symbol.Dropout` function. For example, the following code defines dropouts
    post the first ReLU activation (`act1`) and second ReLU activation (`act2`):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在MXNet中，可以使用`mx.symbol.Dropout`函数轻松地将dropout定义为网络架构的一部分。例如，以下代码定义了在第一个ReLU激活（`act1`）和第二个ReLU激活（`act2`）之后的dropout：
- en: '[PRE16]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `data` parameter specifies the input that the dropout takes and the value
    of `p` specifies the amount of dropout to be done. In case of `dropout1`, we are
    specifying that 50% of weights are to be dropped. Again, there is no hard-and-fast
    rule in terms of how much dropout should be included and at what layers. This
    is something to be determined through trial and error. The code with dropouts
    almost remains identical to the earlier project except that it now includes the
    dropouts after the activations:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`data`参数指定了dropout所接受的输入和`p`的值指定了要执行的dropout量。在`dropout1`的情况下，我们指定要丢弃50%的权重。再次强调，关于应该包含多少dropout以及在哪一层，并没有硬性规定。这是需要通过试错来确定的事情。带有dropout的代码几乎与早期项目相同，只是现在包括了激活层之后的dropout：'
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will give the following output and the visual network architecture:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出和可视化的网络架构：
- en: '[PRE18]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Take a look at the following diagram:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下下面的图：
- en: '![](img/0e7e2381-a190-44e2-a9cd-cb85033c0b96.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e7e2381-a190-44e2-a9cd-cb85033c0b96.png)'
- en: We can see from the output that dropout is now included as part of the network
    architecture. We also observe that this network architecture yields a lower accuracy
    on the test dataset when compared with our initial project. One reason could be
    that the dropout percentages (50% and 30%) we included are too high. We could
    play with these percentages and rebuild the model to determine whether the accuracy
    gets better. The idea, however, is to demonstrate the use of dropout as a regularization
    technique so as to avoid overfitting in deep neural networks.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中我们可以看到，dropout现在被包含在网络架构中。我们还观察到，与我们的初始项目相比，这个网络架构在测试数据集上的准确率更低。一个可能的原因是我们包含的dropout百分比（50%和30%）太高。我们可以调整这些百分比，并重新构建模型以确定准确率是否会提高。然而，这个想法是演示dropout作为正则化技术的作用，以避免深度神经网络过拟合。
- en: 'Apart from dropout, there are other techniques you could employ to avoid an
    overfitting situation:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 除了dropout之外，还有其他技术可以用来避免过拟合的情况：
- en: '**Addition of data**: Adding more training data.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加数据**：添加更多训练数据。'
- en: '**Data augmentation**: Creating additional data synthetically by applying techniques
    such as flipping, distorting, adding random noise, and rotation. The following
    screenshot shows sample images created after applying data augmentation:'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据增强**：通过应用翻转、扭曲、添加随机噪声和旋转等技术，合成地创建额外的数据。以下截图显示了应用数据增强后生成的样本图像：'
- en: '![](img/cdd28deb-0725-4120-aeec-075ba07f4e98.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cdd28deb-0725-4120-aeec-075ba07f4e98.png)'
- en: Sample images from applying data augmentation
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 应用数据增强的样本图像
- en: '**Reducing complexity of the network architecture**: Fewer layers, fewer epochs,
    and so on.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降低网络架构的复杂性**：更少的层、更少的epoch等。'
- en: '**Batch normalization**: A process of ensuring that the weights generated in
    the network do not push very high or very low. This is generally achieved by subtracting
    the mean and dividing by the standard deviation of all weights at a layer from
    each weight in a layer. It shields against overfitting, performs regularization,
    and significantly improves the training speed.  The `mx.sym.batchnorm()` function
    enables us to define batch normalization after the activation.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量归一化**：确保网络中生成的权重不会推得太高或太低的过程。这通常是通过从每一层的每个权重中减去该层所有权重的平均值并除以标准差来实现的。它能够防止过拟合，执行正则化，并显著提高训练速度。`mx.sym.batchnorm()`函数使我们能够在激活后定义批量归一化。'
- en: We will not focus on developing another project with batch normalization as
    using this function in the project is very similar to the other functions we used
    in our earlier projects. So far, we have focused on increasing the epochs to improve
    the performance of the model, another option is to try a different architecture
    and evaluate whether that improves the accuracy on the test dataset. On that note,
    let's explore LeNet, which is specifically designed for optical character recognition
    in documents.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会专注于开发另一个使用批量归一化的项目，因为在项目中使用此函数与我们之前项目中使用的其他函数非常相似。到目前为止，我们一直专注于通过增加epoch来提高模型的性能，另一个选择是尝试不同的架构，并评估它是否提高了测试数据集上的准确率。关于这一点，让我们探索LeNet，它专门设计用于文档的光学字符识别。
- en: Implementing the LeNet architecture with the MXNet library
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MXNet库实现LeNet架构
- en: In their 1998 paper, *Gradient-Based Learning Applied to Document Recognition*,
    LeCun et al. introduced the LeNet architecture.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们1998年的论文《基于梯度的学习应用于文档识别》中，LeCun等人介绍了LeNet架构。
- en: 'The LeNet architecture consists of two sets of convolutional, activation, and
    pooling layers, followed by a fully-connected layer, activation, another fully-connected
    layer, and finally a softmax classifier. The following diagram illustrates the
    LeNet architecture:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet架构由两套卷积、激活和池化层组成，随后是一个全连接层、激活、另一个全连接层，最后是一个softmax分类器。以下图表说明了LeNet架构：
- en: '![](img/3fe3623a-6aaf-4b55-b59a-1de8f19a48ea.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3fe3623a-6aaf-4b55-b59a-1de8f19a48ea.png)'
- en: LeNet architecture
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet架构
- en: 'Now, let''s implement the LeNet architecture with the `mxnet` library in our
    project using the following code block:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下代码块在我们的项目中使用`mxnet`库实现LeNet架构：
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will give the following output and the visual network architecture:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出和可视化的网络架构：
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Take a look at the following diagram:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下图表：
- en: '![](img/75a79bd5-f0a2-4b1a-a66e-25c17044489d.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/75a79bd5-f0a2-4b1a-a66e-25c17044489d.png)'
- en: The code ran for less than 5 minutes on my 4-core CPU box, but still got us
    a 98% accuracy on the test dataset with just three epochs. We can also see that
    we obtained 98% accuracy with both the training and test datasets, confirming
    that there is no overfitting.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的4核CPU盒子上，代码运行时间不到5分钟，但仍然在测试数据集上仅用三个训练轮次就达到了98%的准确率。我们还可以看到，在训练和测试数据集上我们都获得了98%的准确率，这证实了没有过拟合。
- en: 'We see `tanh` is used as the activation function; let''s experiment and see
    whether it has any impact if we change it to ReLU. The code for the project will
    be identical except that we need to find and replace `tanh` with ReLU. We will
    not repeat the code as the only lines that have changed from the earlier project
    are as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到`tanh`被用作激活函数；让我们实验一下，看看如果我们将其更改为ReLU，它是否有任何影响。项目的代码将与之前的项目相同，只是我们需要找到并替换`tanh`为ReLU。我们不会重复代码，因为只有以下几行与早期项目不同：
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You will get the following output on running the code with ReLU as the activation
    function:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用ReLU作为激活函数运行代码时，将得到以下输出：
- en: '[PRE22]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: With ReLU being used as the activation function, we do not see a significant
    improvement in the accuracy. It stayed at 98%, which is the same as obtained with
    the `tanh` activation function.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ReLU作为激活函数时，我们没有看到准确率有显著提高。它保持在98%，这与使用`tanh`激活函数获得的结果相同。
- en: As a next step, we could try to rebuild the model with additional epochs to
    see whether the accuracy improves. Alternatively, we could try tweaking the number
    of filters and filter sizes per convolutional layer to see what happens! Further
    experiments could also include adding more layers of several kinds. We don't know
    what the result is going to be unless we experiment!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，我们可以尝试通过增加额外的训练轮次来重建模型，看看是否可以提高准确率。或者，我们可以尝试调整卷积层中过滤器的数量和大小，看看会发生什么！进一步的实验还可以包括添加更多种类的层。除非我们进行实验，否则我们不知道结果会是什么！
- en: Implementing computer vision with pretrained models
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练模型实现计算机视觉
- en: In [Chapter 1](d8e2df34-05df-451e-88ce-62fdf17184d4.xhtml), *Exploring the Machine
    Learning Landscape*, we touched upon a concept called **transfer learning**. The
    idea is to take the knowledge learned in a model and apply it to another related
    task. Transfer learning is used on almost all computer vision tasks nowadays.
    It's rare to train models from scratch unless there is a huge labeled dataset
    available for training.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](d8e2df34-05df-451e-88ce-62fdf17184d4.xhtml)《探索机器学习领域》中，我们提到了一个名为**迁移学习**的概念。其想法是将模型中学到的知识应用到另一个相关任务中。现在几乎所有的计算机视觉任务都使用了迁移学习。除非有大量的标记数据集可用于训练，否则很少从头开始训练模型。
- en: Generally, in computer vision, CNNs try to detect edges in the earlier layers,
    shapes in the middle layer, and some task-specific features in the later layers.
    Irrespective of the image to be detected by the CNNs, the function of the earlier
    and middle layers remains the same, which makes it possible to exploit the knowledge
    gained by a pretrained model. With transfer learning, we can reuse the early and
    middle layers and only retrain the later layers. It helps us to leverage the labeled
    data of the task it was initially trained on.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，在计算机视觉中，卷积神经网络（CNNs）试图在早期层检测边缘，在中层层检测形状，并在后期层检测一些特定任务的特征。无论CNNs要检测的图像是什么，早期和中层层的功能保持不变，这使得我们可以利用预训练模型获得的知识。通过迁移学习，我们可以重用早期和中层层，只需重新训练后期层。这有助于我们利用最初训练任务上的标记数据。
- en: 'Transfer learning offers two main advantages: it saves us training time and
    ensures that we have a good model even if we have a lot less labelled training
    data.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习提供了两个主要优势：它节省了我们的训练时间，并确保即使我们拥有的标记训练数据很少，我们也能有一个好的模型。
- en: '`Xception`, `VGG16`, `VGG19`, `ResNet50`, `InceptionV3`, `InceptionResNetV2`,
    `MobileNet`, `DenseNet`, `NASNet`, `MobileNetV2`, `QuocNet`, `AlexNet`, `Inception`
    (GoogLeNet), and `BN-Inception-v2` are some widely-used pretrained models. While
    we won''t delve into the details of each of these pretrained models, the idea
    of this section is to implement a project to detect the contents of images (input)
    by making use of a pretrained model through MXNet.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`Xception`、`VGG16`、`VGG19`、`ResNet50`、`InceptionV3`、`InceptionResNetV2`、`MobileNet`、`DenseNet`、`NASNet`、`MobileNetV2`、`QuocNet`、`AlexNet`、`Inception`（GoogLeNet）和`BN-Inception-v2`是一些广泛使用的预训练模型。虽然我们不会深入探讨这些预训练模型的细节，但本节的想法是通过MXNet利用预训练模型来实施一个检测图像（输入）内容的项目。'
- en: 'In the code presented in this section, we make use of the pretrained Inception-BatchNorm
    network to predict the class of an image. The pretrained model needs to be downloaded
    to the working directory prior to running the code. The model can be downloaded
    from [http://data.mxnet.io/mxnet/data/Inception.zip](http://data.mxnet.io/mxnet/data/Inception.zip).
    Let''s explore the following code to label a few test images using the `inception_bn`
    pretrained model:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中展示的代码中，我们使用了预训练的Inception-BatchNorm网络来预测图像的类别。在运行代码之前，需要将预训练模型下载到工作目录。模型可以从[http://data.mxnet.io/mxnet/data/Inception.zip](http://data.mxnet.io/mxnet/data/Inception.zip)下载。让我们探索以下代码，使用`inception_bn`预训练模型标记一些测试图像：
- en: '[PRE23]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This will result in the following output:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/15bfe7cc-5c35-4403-b29f-14806e65f863.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/15bfe7cc-5c35-4403-b29f-14806e65f863.png)'
- en: 'To process the images and predict the image IDs that have the highest probability
    of using the pretrained model, we use the following code:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用预训练模型处理图像并预测具有最高概率的图像ID，我们使用以下代码：
- en: '[PRE24]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This will result in the following output with the IDs of the highest probabilities:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出，其中包含最高概率的ID：
- en: '[PRE25]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s print the labels that correspond to the top-three predicted IDs with
    the highest probabilities using the following code:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下代码打印出与最高概率预测ID对应的标签：
- en: '[PRE26]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This will give the following output:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'From the output, we see that it has correctly labelled the image that is passed
    as input. We can test a few more images with the following code to confirm that
    the classification is done correctly:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出结果中，我们可以看到它已经正确地标记了作为输入传递的图像。我们可以使用以下代码测试更多图像，以确认分类是否正确：
- en: '[PRE28]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This will give the following output:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/c8595173-55f3-466a-a41e-f56c844220ad.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c8595173-55f3-466a-a41e-f56c844220ad.png)'
- en: 'Take a look at the following code:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 看看以下代码：
- en: '[PRE29]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Likewise, we can try for a third image using the following code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以使用以下代码尝试第三张图像：
- en: '[PRE30]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This will give the following output:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/0549ed49-82c2-4ead-9f2d-8c43781f2dc1.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0549ed49-82c2-4ead-9f2d-8c43781f2dc1.png)'
- en: 'Take a look at the following output:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 看看以下输出：
- en: '[PRE31]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Summary
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about computer vision and its association with deep
    learning. We explored a specific type of deep learning algorithm, CNNs, that is
    widely used in computer vision. We studied an open source deep learning framework
    called MXNet. After a detailed discussion of the MNIST dataset, we built models
    using various network architectures and successfully classified the handwritten
    digits in the MNIST dataset. At the end of the chapter, we delved into the concept
    of transfer learning and explored its association with computer vision. The last
    project we built in this chapter classified images using an Inception-BatchNorm
    pretrained model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了计算机视觉及其与深度学习的关联。我们探索了一种特定的深度学习算法，即计算机视觉中广泛使用的卷积神经网络（CNNs）。我们研究了名为MXNet的开源深度学习框架。在详细讨论MNIST数据集之后，我们使用各种网络架构构建了模型，并成功地对MNIST数据集中的手写数字进行了分类。在本章的结尾，我们深入探讨了迁移学习概念及其与计算机视觉的关联。本章最后一个项目使用Inception-BatchNorm预训练模型对图像进行了分类。
- en: In the next chapter, we will explore an unsupervised learning algorithm called
    the autoencoder neural network. I am really excited to implement a project to
    capture credit card fraud using autoencoders. Are you game? Let's go!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索一种称为自动编码器神经网络的非监督学习算法。我非常期待实现一个使用自动编码器捕获信用卡欺诈的项目。你准备好了吗？让我们出发吧！
