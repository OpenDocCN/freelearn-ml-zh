- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Ensemble Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习
- en: Anyone who works with data analysis and machine learning will understand that
    no method is ideal or universal. This is why there are so many methods. Researchers
    and enthusiasts have been searching for years for a compromise between the accuracy,
    simplicity, and interpretability of various models. Moreover, how can we increase
    the accuracy of the model, preferably without changing its essence? One way to
    improve the accuracy of models is to create and train model **ensembles**—that
    is, sets of models used to solve the same problem. The ensemble training methodology
    is the training of a final set of simple classifiers, with a subsequent merging
    of the results of their predictions into a single forecast of the aggregated algorithm.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 任何从事数据分析与机器学习工作的人都会理解没有哪种方法是理想的或通用的。这就是为什么有这么多方法。研究人员和爱好者多年来一直在寻找各种模型在准确性、简单性和可解释性之间的折衷方案。此外，我们如何在不改变模型本质的情况下提高模型的准确性？提高模型准确性的方法之一是创建和训练模型**集成**——即用于解决同一问题的模型集合。集成训练方法是对一组简单分类器的训练，随后将它们预测结果合并成一个聚合算法的单个预测。
- en: This chapter describes what ensemble learning is, what types of ensembles exist,
    and how they can help to obtain better predictive performance. In this chapter,
    we will also implement examples of these approaches with different C++ libraries.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述了集成学习是什么，存在哪些类型的集成，以及它们如何帮助获得更好的预测性能。在本章中，我们还将使用不同的 C++ 库实现这些方法的示例。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: An overview of ensemble learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成学习的概述
- en: Learning about decision trees and random forests
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习决策树和随机森林
- en: Examples of using C++ libraries for creating ensembles
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 C++ 库创建集成示例
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The technologies and installations required in the chapter are as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所需的技术和安装如下：
- en: The `Dlib` library
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dlib` 库'
- en: The `mlpack` library
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlpack` 库'
- en: A modern C++ compiler with C++20 support
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持 C++20 的现代 C++ 编译器
- en: A CMake build system version >= 3.22
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CMake构建系统版本 >= 3.22
- en: 'The code files for this chapter can be found at the following GitHub repo:
    [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter09](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter09)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在以下 GitHub 仓库中找到：[https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter09](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter09)
- en: An overview of ensemble learning
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习的概述
- en: The training of an ensemble of models is understood to be the procedure of training
    a final set of elementary algorithms whose results are then combined to form the
    forecast of an aggregated classifier. The model ensemble’s purpose is to improve
    the accuracy of the prediction of the aggregated classifier, particularly when
    compared with the accuracy of every single elementary classifier. It is intuitively
    clear that combining simple classifiers can give a more accurate result than each
    simple classifier separately. Despite that, simple classifiers can be sufficiently
    accurate on particular datasets, but at the same time, they can make mistakes
    on different datasets.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型的训练被理解为训练一组最终的基本算法的过程，其结果随后被组合以形成聚合分类器的预测。集成模型的目的在于提高聚合分类器预测的准确性，尤其是在与每个单独的基本分类器的准确性相比时。直观上很明显，将简单的分类器组合起来可以比单独的每个简单分类器给出更准确的结果。尽管如此，简单的分类器在特定的数据集上可能足够准确，但与此同时，它们可能在不同的数据集上犯错误。
- en: An example of ensembles is **Condorcet’s jury theorem** (1784). A jury must
    come to a correct or incorrect consensus, and each juror has an independent opinion.
    If the probability of the correct decision of each juror is more than 0.5, then
    the probability of a correct decision from the jury as a whole (tending toward
    1) increases with the size of the jury. If the probability of making the correct
    decision is less than 0.5 for each juror, then the probability of making the right
    decision monotonically decreases (tending toward zero) as the jury size increases.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 集成的一个例子是**孔多塞陪审团定理**（1784）。陪审团必须得出正确或错误的共识，并且每个陪审员都有独立的观点。如果每个陪审员做出正确决定的概率超过
    0.5，那么陪审团作为一个整体做出正确决定的概率（趋向于 1）会随着陪审团规模的增加而增加。如果每个陪审员做出正确决定的概率小于 0.5，那么随着陪审团规模的增加，做出正确决定的概率单调递减（趋向于零）。
- en: 'The theorem is as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 定理如下：
- en: '*N*: The number of jury members'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N*: 陪审团成员的数量'
- en: '![](img/B22503_Formula_001.png): The probability of the jury member making
    the right decision'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![公式001](img/B22503_Formula_001.png): 陪审团成员做出正确决定的概率'
- en: '*μ*: The probability of the entire jury making the correct decision'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*μ*: 整个陪审团做出正确决定的概率'
- en: '*m*: The minimum majority of jury members:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m*: 陪审团成员的最小多数：'
- en: '![](img/B22503_Formula_002.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![公式002](img/B22503_Formula_002.jpg)'
- en: '![](img/B22503_Formula_003.png): The number of combinations of *N* by *i*:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![公式003](img/B22503_Formula_003.png): 由 *N* 个元素组成的 *i* 的组合数：'
- en: '![](img/B22503_Formula_004.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![公式004](img/B22503_Formula_004.jpg)'
- en: If ![](img/B22503_Formula_005.png) then ![](img/B22503_Formula_006.png)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ![公式005](img/B22503_Formula_005.png) 则 ![公式006](img/B22503_Formula_006.png)
- en: If ![](img/B22503_Formula_007.png) then ![](img/B22503_Formula_008.png)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ![公式007](img/B22503_Formula_007.png) 则 ![公式008](img/B22503_Formula_008.png)
- en: 'Therefore, based on general reasoning, three reasons why ensembles of classifiers
    can be successful can be distinguished, as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基于一般推理，可以区分出为什么分类器集成可以成功的三种原因，如下：
- en: '**Statistical**: The classification algorithm can be viewed as a search procedure
    in the space of the **H hypothesis**, concerned with the distribution of data
    in order to find the best hypothesis. By learning from the final dataset, the
    algorithm can find many different hypotheses that describe the training sample
    equally well. By building an ensemble of models, we *average out* the error of
    each hypothesis and reduce the influence of instabilities and randomness in the
    formation of a new hypothesis.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统计性**: 分类算法可以被视为在 **H 假设**空间中的搜索过程，关注数据的分布以找到最佳假设。通过从最终数据集中学习，算法可以找到许多不同的假设，这些假设可以同样好地描述训练样本。通过构建模型集成，我们可以
    *平均*每个假设的错误，并减少新假设形成中的不稳定性和随机性的影响。'
- en: '**Computational**: Most learning algorithms use methods for finding the extremum
    of a specific objective function. For example, neural networks use **gradient
    descent** (**GD**) methods to minimize prediction errors. Decision trees use greedy
    algorithms that minimize data entropy. These optimization algorithms can become
    stuck at a local extremum point, which is a problem because their goal is to find
    a global optimum. The ensembles of models combining the results of the prediction
    of simple classifiers, trained on different subsets of the source data, have a
    higher chance of finding a global optimum since they start a search for the optimum
    from different points in the initial set of hypotheses.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算方法**: 大多数学习算法使用寻找特定目标函数极值的方法。例如，神经网络使用 **梯度下降**（**GD**）方法来最小化预测误差。决策树使用最小化数据熵的贪婪算法。这些优化算法可能会陷入局部极值点，这是一个问题，因为它们的目的是找到全局最优解。结合简单分类器预测结果的模型集合，由于它们从初始假设集的不同点开始寻找最优解，因此有更高的机会找到全局最优解。'
- en: '**Representative**: A combined hypothesis may not be in the set of possible
    hypotheses for simple classifiers. Therefore, by building a combined hypothesis,
    we expand the set of possible hypotheses.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代表性**: 组合假设可能不在简单分类器的可能假设集中。因此，通过构建组合假设，我们扩展了可能的假设集。'
- en: Condorcet’s jury theorem and the reasons provided previously are not entirely
    suitable for real, practical situations because the algorithms are not independent
    (they solve one problem, they learn on one target vector, and can only use one
    model, or a small number of models).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 康多塞陪审团定理和之前提供的理由并不完全适用于现实、实际的情况，因为算法不是独立的（它们解决一个问题，在一个目标向量上学习，只能使用一个模型，或者少数几个模型）。
- en: Therefore, the majority of techniques in applied ensemble development are aimed
    at ensuring that the ensemble is diverse. This allows the errors of individual
    algorithms in individual objects to be compensated for by the correct operations
    of other algorithms. Overall, building the ensemble results in an improvement
    in both the quality and variety of simple algorithms. The goal is to create a
    diverse set of predictions that complement each other and reduce the overall variance
    and bias of the ensemble’s predictions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，应用集成开发的大多数技术都是为了确保集成是多样化的。这使得单个算法在单个对象中的错误可以通过其他算法的正确操作来补偿。总的来说，构建集成提高了简单算法的质量和多样性。目标是创建一个多样化的预测集，这些预测相互补充，并减少集成预测的整体方差和偏差。
- en: The simplest type of ensemble is model averaging, whereby each member of the
    ensemble makes an equal contribution to the final forecast. The fact that each
    model has an equal contribution to the final ensemble’s forecast is a limitation
    of this approach. The problem is in unbalanced contributions. Despite that, there
    is a requirement that all members of the ensemble have prediction skills higher
    than random chance.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的集成类型是模型平均，其中集成中的每个成员对最终预报的贡献是相等的。每个模型对最终集成预报的贡献相等是这种方法的一个局限性。问题在于贡献不平衡。尽管如此，仍然要求集成中的所有成员的预测技能高于随机机会。
- en: However, it is known that some models work much better or much worse than other
    models. Some improvements can be made to solve this problem, using a weighted
    ensemble in which the contribution of each member to the final forecast is weighted
    by the performance of the model. When the weight of the model is a small positive
    value and the sum of all weights equals 1, the weights can indicate the percentage
    of confidence in (or expected performance from) each model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，众所周知，一些模型的表现比其他模型好得多或差得多。可以通过使用加权集成来解决此问题，其中每个成员对最终预报的贡献由该模型的性能加权。当模型的权重是一个小的正值且所有权重的总和等于1时，权重可以表示对每个模型的信心百分比（或预期性能）。
- en: 'At this time, the most common approaches to ensemble construction are as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，构建集成最常见的方法如下：
- en: '**Bagging**: This is an ensemble of models studying in parallel on different
    random samples from the same training set. The final result is determined by the
    voting of the algorithms of the ensemble. For example, in classification, the
    class that is predicted by the most classifiers is chosen.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**袋装**：这是一组模型，它们在来自同一训练集的不同随机样本上并行研究。最终结果由集成算法的投票决定。例如，在分类中，选择被最多分类器预测的类别。'
- en: '**Boosting**: This is an ensemble of models trained sequentially, with each
    successive algorithm being trained on samples in which the previous algorithm
    made a mistake.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升**：这是一组按顺序训练的模型，每个后续算法都在前一个算法出错的数据样本上训练。'
- en: '**Stacking**: This is an approach whereby a training set is divided into *N*
    blocks, and a set of simple models is trained on *N-1* of them. An *N-th* model
    is then trained on the remaining block, but the outputs of the underlying algorithms
    (forming the so-called **meta-attribute**) are used as the target variable.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**堆叠**：这是一种方法，其中训练集被分为*N*个块，并在其中的*N-1*个块上训练一组简单模型。然后，第*N*个模型在剩余的块上训练，但使用底层算法的输出（形成所谓的**元属性**）作为目标变量。'
- en: '**Random forest**: This is a set of decision trees built independently, and
    whose answers are averaged and decided by a majority vote.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机森林**：这是一组独立构建的决策树，其答案通过平均和多数投票来决定。'
- en: The following sections discuss the previously described approaches in detail.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下几节将详细讨论之前描述的方法。
- en: Using a bagging approach for creating ensembles
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用袋装方法创建集成
- en: Bagging (from the bootstrap aggregation) is one of the earliest and most straightforward
    types of ensembles. Bagging is based on the statistical bootstrap method, which
    aims to obtain the most accurate sample estimates and to extend the results to
    the entire population. The bootstrap method is as follows.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装（来自自助聚合）是最早和最直接类型的集成之一。袋装基于统计自助方法，旨在获得最准确的样本估计并将结果扩展到整个总体。自助方法如下。
- en: Suppose there is an *X* dataset of size *M*. Evenly select from the dataset
    *N* objects and return each object back to the dataset after selection. Before
    selecting the next one, we can generate *N* sub-datasets. This procedure means
    that *N* times, we select an arbitrary sample object (we assume that each object
    is *picked up* with the same probability ![](img/B22503_Formula_009.png)), and
    each time, we choose from all the original *M* objects. Also, this procedure is
    called sampling with replacement, and it means that each element in the dataset
    has an equal chance of being selected multiple times.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个大小为*M*的*X*数据集。从数据集中均匀选择*N*个对象，并在选择后将其放回数据集。在选择下一个之前，我们可以生成*N*个子数据集。这个程序意味着*N*次，我们选择一个任意的样本对象（我们假设每个对象被*选中*的概率相同
    ![](img/B22503_Formula_009.png))，每次，我们都从所有原始*M*个对象中选择。此外，这个过程也称为带替换的抽样，这意味着数据集中的每个元素都有被选中多次的机会。
- en: We can imagine this as a bag from which balls are taken. The ball selected at
    a given step is returned to the bag following its selection, and the next choice
    is again made with equal probability from the same number of balls. Note that
    due to the ball being returned each time, there are repetitions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以想象这是一个袋子，从中取出球。在给定步骤选中的球在选中后返回到袋子中，下一次选择仍然是从相同数量的球中按相同概率进行。请注意，由于每次都返回球，因此存在重复。
- en: Each new selection is denoted as *X*1\. Repeating the procedure *k* times, we
    generate *k* sub-datasets. Now, we have a reasonably large number of samples,
    and we can evaluate various statistics of the original distribution.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 每次新的选择表示为 *X*1。重复该过程 *k* 次，我们生成 *k* 个子数据集。现在，我们有一个相当大的样本数量，我们可以评估原始分布的各种统计量。
- en: The main descriptive statistics are the sample mean, median, and standard deviation.
    Summary statistics—for example, the sample mean, median, and correlation—can vary
    from sample to sample. The bootstrap idea is to use sampling results as a fictitious
    population to determine the sample distribution of statistics. The bootstrap method
    analyzes a large number of phantom samples, called **bootstrap samples**. For
    each sample, an estimate of the target statistics is calculated, then the estimates
    are averaged. The bootstrap method can be viewed as a modification of the **Monte**
    **Carlo method**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 主要描述性统计量是样本均值、中位数和标准差。总结统计量（例如，样本均值、中位数和相关性）可能因样本而异。自助法的想法是使用采样结果作为虚构的总体来确定统计量的样本分布。自助法分析大量被称为**自助样本**的虚拟样本。对于每个样本，计算目标统计量的估计值，然后对这些估计值进行平均。自助法可以看作是**蒙特卡洛方法**的修改。
- en: 'Suppose there is the *X* training dataset. With the help of the bootstrap method,
    we can generate ![](img/B22503_Formula_010.png) sub-datasets. Now, on each sub-dataset,
    we can train our ![](img/B22503_Formula_011.png) classifier. The final classifier
    averages these classifier responses (in the case of classification, this corresponds
    to a vote), as follows: ![](img/B22503_Formula_012.png). The following diagram
    shows this scheme:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有 *X* 训练数据集。借助自助法，我们可以生成 ![](img/B22503_Formula_010.png) 子数据集。现在，在每一个子数据集上，我们可以训练我们的
    ![](img/B22503_Formula_011.png) 分类器。最终的分类器平均这些分类器的响应（在分类的情况下，这对应于投票），如下所示：![](img/B22503_Formula_012.png)。以下图表显示了此方案：
- en: '![Figure 9.1 – Bagging approach scheme](img/B19849_09_1.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – Bagging方法方案](img/B19849_09_1.jpg)'
- en: Figure 9.1 – Bagging approach scheme
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – Bagging方法方案
- en: 'Consider the regression problem by using simple algorithms ![](img/B22503_Formula_013.png).
    Suppose that there is a true answer function for all *y(x)* objects, and there
    is also a distribution on ![](img/B22503_Formula_014.png) objects. In this case,
    we can write the error of each regression function as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑使用简单算法 ![](img/B22503_Formula_013.png) 解决回归问题。假设对于所有 *y(x)* 对象都有一个真实的答案函数，并且在
    ![](img/B22503_Formula_014.png) 对象上也有一个分布。在这种情况下，我们可以写出每个回归函数的误差如下：
- en: '![](img/B22503_Formula_015.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22503_Formula_015.jpg)'
- en: 'We can also write the expectation of the **mean squared error** (**MSE**) as
    follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以写出**均方误差**（**MSE**）的期望值如下：
- en: '![](img/B22503_Formula_016.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22503_Formula_016.jpg)'
- en: 'The average error of the constructed regression functions is as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 构建的回归函数的平均误差如下：
- en: '![](img/B22503_Formula_017.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22503_Formula_017.jpg)'
- en: 'Now, suppose the errors are unbiased and uncorrelated, as shown here:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设误差是无偏且不相关的，如图所示：
- en: '![](img/B22503_Formula_018.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22503_Formula_018.jpg)'
- en: 'Now we can write a new regression function that averages the responses of the
    functions we have constructed, as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以编写一个新的回归函数，该函数平均我们构建的函数的响应，如下所示：
- en: '![](img/B22503_Formula_019.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22503_Formula_019.jpg)'
- en: 'Let’s find its **root MSE** (**RMSE**) to see the effect of averaging, as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们找到其**均方根误差**（**RMSE**）以查看平均化的效果，如下所示：
- en: '![](img/B22503_Formula_020.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22503_Formula_020.jpg)'
- en: Thus, averaging the answers has allowed us to reduce the average square of the
    error by *n* times.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，平均答案使我们能够将平均平方误差减少 *n* 倍。
- en: Bagging also allows us to reduce the variance of the trained algorithm and prevent
    overfitting. The effectiveness of bagging is based on the underlying algorithms,
    which are trained on various sub-datasets that are quite different, and their
    errors are mutually compensated during voting. Also, outlying objects may not
    fall into some of the training sub-datasets, which also increases the effectiveness
    of the bagging approach.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging还可以帮助我们减少训练算法的方差，防止过拟合。Bagging的有效性基于底层算法，这些算法在相当不同的子数据集上训练，并且在投票过程中相互补偿错误。此外，异常值可能不会落入某些训练子数据集中，这也增加了Bagging方法的有效性。
- en: Bagging is useful with small datasets when the exclusion of even a small number
    of training objects leads to the construction of substantially different simple
    algorithms. In the case of large datasets, sub-datasets that are significantly
    smaller than the original ones are usually generated.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当排除少量训练对象会导致构建出显著不同的简单算法时，bagging对于小型数据集是有用的。在大型数据集的情况下，通常生成比原始数据集显著小的子数据集。
- en: Notice that the assumption about uncorrelated errors is rarely satisfied. If
    this assumption is incorrect, then the error reduction is not as significant as
    we might have assumed.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，关于误差不相关的假设很少得到满足。如果这个假设不正确，那么误差减少的幅度不如我们可能假设的那么显著。
- en: In practice, bagging provides a good improvement to the accuracy of results
    when compared to simple individual algorithms, particularly if a simple algorithm
    is sufficiently accurate but unstable. Improving the accuracy of the forecast
    occurs by reducing the spread of the error-prone forecasts of individual algorithms.
    The advantage of the bagging algorithm is its ease of implementation, as well
    as the possibility of paralleling the calculations for training each elementary
    algorithm on different computational nodes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，与简单的单个算法相比，bagging在提高结果准确性方面提供了良好的改进，尤其是如果简单算法足够准确但不够稳定时。通过减少单个算法预测错误的波动来提高预测的准确性。bagging算法的优点在于其实施简单，以及可以在不同的计算节点上并行计算训练每个基本算法。
- en: Using a gradient boosting method for creating ensembles
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用梯度提升方法创建集成
- en: The main idea of boosting is that the elementary algorithms are not built independently.
    We build every sequential algorithm so that it corrects the mistakes of the previous
    ones and therefore improves the quality of the whole ensemble. The first successful
    version of boosting was **Adaptive Boosting** (**AdaBoost**). It is now rarely
    used since gradient boosting has supplanted it.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 提升法的主要思想是基本算法不是独立构建的。我们构建每个顺序算法，使其纠正前一个算法的错误，从而提高整个集成质量。提升法第一个成功的版本是**自适应提升法**（**AdaBoost**）。现在它很少被使用，因为梯度提升已经取代了它。
- en: 'Suppose that we have a set of pairs, where each pair consists of attribute
    *x* and target variable *y*, ![](img/B22503_Formula_021.png). On this set, we
    restore the dependence of the form ![](img/B22503_Formula_022.png). We restore
    it by the approximation ![](img/B22503_Formula_023.png). To select the best approximation
    solution, we use a specific loss function of the form ![](img/B22503_Formula_024.png),
    which we should optimize as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一组由属性*x*和目标变量*y*组成的对，![](img/B22503_Formula_021.png)。在这个集合上，我们恢复形式![](img/B22503_Formula_022.png)的依赖关系。我们通过近似![](img/B22503_Formula_023.png)来恢复它。为了选择最佳的近似解，我们使用特定形式的损失函数![](img/B22503_Formula_024.png)，我们应该按照以下方式优化它：
- en: '![](img/B22503_Formula_025.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![公式图片](img/B22503_Formula_025.jpg)'
- en: 'We also can rewrite the expression in terms of mathematical expectations, since
    the amount of data available for learning is limited, as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可用于学习的有限数据量，我们也可以用数学期望来重写表达式，如下所示：
- en: '![](img/B22503_Formula_026.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![公式图片](img/B22503_Formula_026.jpg)'
- en: 'Our approximation is inaccurate. However, the idea behind boosting is that
    such an approximation can be improved by adding to the model with the result of
    another model that corrects its errors, as illustrated here:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的近似是不准确的。然而，提升法的理念在于，通过添加另一个模型的结果来纠正其错误，从而可以改进这种近似，如图所示：
- en: '![](img/B22503_Formula_027.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![公式图片](img/B22503_Formula_027.jpg)'
- en: 'The following equation shows the ideal error correction model:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方程显示了理想的误差校正模型：
- en: '![](img/B22503_Formula_028.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![公式图片](img/B22503_Formula_028.jpg)'
- en: 'We can rewrite this formula in the following form, which is more suitable for
    the corrective model:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个公式重写为以下形式，这更适合校正模型：
- en: '![](img/B22503_Formula_029.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![公式图片](img/B22503_Formula_029.jpg)'
- en: Based on the preceding assumptions listed, the goal of boosting is to approximate
    ![](img/B22503_Formula_030.png) to make its results correspond as closely as possible
    to the *residuals* ![](img/B22503_Formula_031.png). Such an operation is performed
    sequentially—that is, ![](img/B22503_Formula_032.png) improves the results of
    the previous ![](img/B22503_Formula_033.png) function.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面列出的假设，提升的目标是逼近 ![](img/B22503_Formula_030.png)，使其结果尽可能接近 *残差* ![](img/B22503_Formula_031.png)。这种操作是顺序执行的，即
    ![](img/B22503_Formula_032.png) 改善了先前 ![](img/B22503_Formula_033.png) 函数的结果。
- en: A further generalization of this approach allows us to consider the residuals
    as a negative gradient of the loss function, specifically of the form ![](img/B22503_Formula_034.png).
    In other words, gradient boosting is a method of GD with the loss function and
    its gradient replacement.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的进一步推广使我们能够将残差视为损失函数的负梯度，具体形式为 ![](img/B22503_Formula_034.png)。换句话说，梯度提升是一种使用损失函数及其梯度的GD方法。
- en: Now, knowing the expression of the loss function gradient, we can calculate
    its values on our data. Therefore, we can train models so that our predictions
    are better correlated with this gradient (with a minus sign). Hence, we will solve
    the regression problem, trying to correct the predictions for these residuals.
    For classification, regression, and ranking, we always minimize the squared difference
    between the residuals and our predictions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，已知损失函数梯度的表达式，我们可以在我们的数据上计算其值。因此，我们可以训练模型，使我们的预测与这个梯度（带有负号）更好地相关。因此，我们将解决回归问题，试图纠正这些残差的预测。对于分类、回归和排序，我们总是最小化残差与我们的预测之间的平方差。
- en: 'In the gradient boosting method, an approximation of the function of the following
    form is used:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度提升方法中，使用以下形式的函数逼近：
- en: '![](img/B22503_Formula_035.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22503_Formula_035.jpg)'
- en: 'This is the sum of ![](img/B22503_Formula_036.png) functions of the ![](img/B22503_Formula_037.png)
    class; they are collectively called **weak models** (algorithms). Such an approximation
    is carried out sequentially, starting from the initial approximation, which is
    a certain constant, as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 ![](img/B22503_Formula_037.png) 类的 ![](img/B22503_Formula_036.png) 函数的总和；它们统称为
    **弱模型**（算法）。这种逼近是顺序进行的，从初始逼近开始，初始逼近是一个确定的常数，如下所示：
- en: '![](img/B22503_Formula_038.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22503_Formula_038.jpg)'
- en: 'Unfortunately, the choice of the ![](img/B22503_Formula_039.png) optimal function
    at each step for an arbitrary loss function is extremely difficult, so a more
    straightforward approach is used. The idea is to use the GD method by using differentiable
    ![](img/B22503_Formula_040.png) functions and a differentiable loss function,
    as illustrated here:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于任意损失函数，在每一步选择 ![](img/B22503_Formula_039.png) 最佳函数极其困难，因此采用了一种更直接的方法。想法是使用可微分的
    ![](img/B22503_Formula_040.png) 函数和可微分的损失函数，如图所示：
- en: '![](img/B22503_Formula_041.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22503_Formula_041.jpg)'
- en: 'The boosting algorithm is then formed as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然后提升算法形成如下：
- en: Initialize the model with constant values, like this:![](img/B22503_Formula_042.jpg)
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用常数值初始化模型，如下所示：![](img/B22503_Formula_042.jpg)
- en: 'Repeat the specified number of iterations and do the following:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复指定的迭代次数并执行以下操作：
- en: 'Calculate the pseudo-residuals, as follows:'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式计算伪残差：
- en: '![](img/B22503_Formula_043.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22503_Formula_043.jpg)'
- en: Here, *n* is the number of training samples, *m* is the iteration number, and
    *L* is the loss function.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*n* 是训练样本的数量，*m* 是迭代次数，*L* 是损失函数。
- en: Train the elementary algorithm (regression model) ![](img/B22503_Formula_044.png)
    on pseudo-residuals with data of the form ![](img/B22503_Formula_045.png).
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在伪残差数据上训练基础算法（回归模型）![](img/B22503_Formula_044.png)。
- en: 'Calculate the ![](img/B22503_Formula_046.png) coefficient by solving a one-dimensional
    optimization problem as follows:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过以下一维优化问题计算 ![](img/B22503_Formula_046.png) 系数：
- en: '![](img/B22503_Formula_047.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22503_Formula_047.jpg)'
- en: 'Update the model, as follows:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式更新模型：
- en: '![](img/B22503_Formula_048.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22503_Formula_048.jpg)'
- en: 'The inputs to this algorithm are as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法的输入如下：
- en: The ![](img/B22503_Formula_049.png) dataset
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B22503_Formula_049.png) 数据集'
- en: The number of *M* iterations
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*M* 迭代次数'
- en: The *L( y, f )* loss function with an analytically written gradient (such a
    form of gradient allows us to reduce the number of numerical calculations)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有解析梯度（这种形式的梯度可以减少数值计算的数量）的 *L( y, f)* 损失函数
- en: The choice of the family of functions of the *h (x)* elementary algorithms,
    with the procedure of their training and hyperparameters
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*h(x)*基本算法的函数族选择，以及它们的训练过程和超参数'
- en: The constant for the initial approximation, as well as the ![](img/B22503_Formula_050.png)-optimal
    coefficient, can be found by a binary search, or by another line search algorithm
    relative to the initial loss function (rather than the gradient).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 初始近似的常数以及![](img/B22503_Formula_050.png)-最优系数可以通过二分搜索找到，或者通过相对于初始损失函数（而不是梯度）的另一种线性搜索算法找到。
- en: 'Examples of loss functions for regression are as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 回归损失函数的例子如下：
- en: '![](img/B22503_Formula_051.png): An *L*2 loss, also called **Gaussian loss**.
    This formula is the classic conditional mean and the most common and simple option.
    If there are no additional information or model sustainability requirements, it
    should be used.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![公式](img/B22503_Formula_051.png)：一个*L*2损失，也称为**高斯损失**。这个公式是经典的条件均值，最常见和简单的选项。如果没有额外的信息或模型可持续性要求，应该使用它。'
- en: '![](img/B22503_Formula_052.png): An *L*1 loss, also called **Laplacian loss**.
    This formula, at first glance, is not very differentiable and determines the conditional
    median. The median, as we know, is more resistant to outliers. Therefore, in some
    problems, this loss function is preferable since it does not penalize large deviations
    as much as a quadratic function.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![公式](img/B22503_Formula_052.png)：一个*L*1损失，也称为**拉普拉斯损失**。这个公式乍一看并不非常可微，并确定条件中位数。正如我们所知，中位数对异常值更具抵抗力。因此，在某些问题中，这个损失函数更可取，因为它不像二次函数那样对大的偏差进行惩罚。'
- en: '![](img/B22503_Formula_053.png) : An *L*q loss, also called **Quantile loss**.
    If we don’t want a conditional median but do want a conditional 75% quantile,
    we would use this option with ![](img/B22503_Formula_054.png). This function is
    asymmetric and penalizes more observations that turn out to be on the side of
    the quantile we need.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![公式](img/B22503_Formula_053.png)：一个*L*q损失，也称为**分位数损失**。如果我们不想要条件中位数，但想要条件75%分位数，我们将使用这个选项与![公式](img/B22503_Formula_054.png)一起。这个函数是不对称的，对那些最终出现在所需分位数一侧的观察值进行更多惩罚。'
- en: 'Examples of loss functions for classification are as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 分类损失函数的例子如下：
- en: '![](img/B22503_Formula_055.png): Logistic loss, also known as **Bernoulli loss**.
    An interesting property of this loss function is that we penalize even correctly
    predicted class labels. By optimizing this loss function, we can continue to distance
    classes and improve the classifier even if all observations are correctly predicted.
    This function is the most standard and frequently used loss function in a binary
    classification task.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![公式](img/B22503_Formula_055.png)：逻辑损失，也称为**伯努利损失**。这个损失函数的一个有趣特性是我们甚至对正确预测的类别标签进行惩罚。通过优化这个损失函数，我们可以在所有观察都正确预测的情况下，继续使类别分离并提高分类器。这个函数是二元分类任务中最标准、最常用的损失函数。'
- en: '![](img/B22503_Formula_056.png): **AdaBoost loss**. It so happens that the
    classic AdaBoost algorithm that uses this loss function (different loss functions
    can also be used in the AdaBoost algorithm) is equivalent to gradient boosting.
    Conceptually, this loss function is very similar to logistic loss, but it has
    a stronger exponential penalty for classification errors and is used less frequently.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![公式](img/B22503_Formula_056.png)：**AdaBoost损失**。恰好使用这个损失函数的经典AdaBoost算法（AdaBoost算法也可以使用不同的损失函数）等同于梯度提升。从概念上讲，这个损失函数与逻辑损失非常相似，但它对分类错误的指数惩罚更强，使用频率较低。'
- en: The idea of bagging is that it can be used with a gradient boosting approach
    too, which is known as **stochastic gradient boosting**. In this way, a new algorithm
    is trained on a sub-sample of the training set. This approach can help us to improve
    the quality of the ensemble and reduce the time it takes to build elementary algorithms
    (whereby each is trained on a reduced number of training samples).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的思想是它可以与梯度提升方法一起使用，这被称为**随机梯度提升**。这样，新的算法是在训练集的子样本上训练的。这种方法可以帮助我们提高集成算法的质量，并减少构建基本算法所需的时间（每个算法都是在减少的训练样本数量上训练的）。
- en: Although boosting itself is an ensemble, other ensemble schemes can be applied
    to it—for example, by averaging several boosting methods. Even if we average boosts
    with the same parameters, they will differ due to the stochastic nature of the
    implementation. This randomness comes from the choice of random sub-datasets at
    each step or selecting different features when we are building decision trees
    (if they are chosen as elementary algorithms).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管提升本身是一种集成方法，但可以将其应用于其他集成方案，例如通过平均几种提升方法。即使我们使用相同的参数平均提升，由于实现的随机性，它们也会有所不同。这种随机性来源于每一步选择随机子数据集或在我们构建决策树时选择不同的特征（如果它们被选为基本算法）。
- en: 'Currently, the base **gradient boosting machine** (**GBM**) has many extensions
    for different statistical tasks. These are as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，基础的 **梯度提升机** （**GBM**） 已针对不同的统计任务有许多扩展。具体如下：
- en: GLMBoost and GAMBoost as an enhancement of the existing **generalized additive**
    **model** (**GAM**)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GLMBoost 和 GAMBoost 作为现有 **广义加性** **模型** （**GAM**） 的增强
- en: CoxBoost for survival curves
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoxBoost 用于生存曲线
- en: RankBoost and LambdaMART for ranking
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RankBoost 和 LambdaMART 用于排序
- en: 'Additionally, there are many implementations of the same GBM under different
    names and different platforms, such as these:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，同一 GBM 在不同名称和不同平台上也有许多实现，例如以下这些：
- en: '**Stochastic GBM**'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机 GBM**'
- en: '**Gradient boosted decision** **trees** (**GBDT**)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度提升决策** **树** （**GBDT**）'
- en: '**Gradient boosted regression** **trees** (**GBRT**)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度提升回归** **树** （**GBRT**）'
- en: '**Multiple additive regression** **trees** (**MART**)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多重加性回归** **树** （**MART**）'
- en: '**Generalized boosting** **machine** (**GBM**)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广义提升** **机** （**GBM**）'
- en: Furthermore, boosting can be applied and used over a long period of time in
    the ranking tasks undertaken by search engines. The task is written based on a
    loss function, which is penalized for errors in the order of search results; therefore,
    it becomes convenient to insert it into a GBM.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，提升可以在搜索引擎承担的排序任务中长期应用和使用。该任务基于损失函数编写，该函数对搜索结果顺序中的错误进行惩罚；因此，将其插入 GBM 中变得方便。
- en: Using a stacking approach for creating ensembles
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用堆叠方法创建集成
- en: The purpose of stacking is to use different algorithms trained on the same data
    as elementary models. A meta-classifier is then trained on the results of the
    elementary algorithms or source data, also supplemented by the results of the
    elementary algorithms themselves. Sometimes, a meta-classifier uses the estimates
    of distribution parameters that it receives (for example, estimates of the probabilities
    of each class for classification) for its training, rather than the results of
    elementary algorithms.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠的目的是使用在相同数据上训练的不同算法作为基本模型。然后在一个元分类器上训练基本算法或源数据的结果，也补充了基本算法本身的结果。有时，元分类器使用它收到的分布参数估计（例如，分类中每个类的概率估计）进行训练，而不是基本算法的结果。
- en: The most straightforward stacking scheme is blending. For this scheme, we divide
    the training set into two parts. The first part is used to teach a set of elementary
    algorithms. Their results can be considered new features (meta-features). We then
    use them as complementary features with the second part of the dataset and train
    the new meta-algorithm. The problem with such a blending scheme is that neither
    the elementary algorithms nor the meta-algorithm use the entire set of data for
    training. To improve the quality of blending, you can average the results of several
    blends trained at different partitions in the data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的堆叠方案是混合。对于这个方案，我们将训练集分为两部分。第一部分用于教授一组基本算法。它们的结果可以被视为新特征（元特征）。然后我们使用它们作为与数据集第二部分的补充特征，并训练新的元算法。这种混合方案的问题是，既没有基本算法也没有元算法使用整个数据集进行训练。为了提高混合的质量，你可以平均在不同数据分区上训练的几个混合的结果。
- en: A second way to implement stacking is to use the entire training set. In some
    sources, this is known as *generalization*. The entire set is divided into parts
    (folds), then the algorithm sequentially goes through the folds and teaches elementary
    algorithms on all the folds except the one randomly chosen fold. The remaining
    fold is used for the inference of the elementary algorithms. The output values
    of elementary algorithms are interpreted as the new meta-attributes (or new features)
    calculated from the folds. In this approach, it is also desirable to implement
    several different partitions into folds, and then average the corresponding meta-attributes.
    For a meta-algorithm, it makes sense to apply regularization or add some normal
    noise to the meta-attributes. The coefficient with which this addition occurs
    is analogous to the regularization coefficient. We can summarize that the basic
    idea behind the described approach is to use a set of base algorithms; then, using
    another meta-algorithm, we combine their predictions with the aim of reducing
    the generalization error.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 实现堆叠的另一种方式是使用整个训练集。在某些资料中，这被称为 *泛化*。整个集被分成几部分（折），然后算法依次遍历这些折，并在除了随机选择的折之外的所有折上教授基本算法。剩下的折被用来推断基本算法。基本算法的输出值被解释为从折中计算出的新元属性（或新特征）。在这种方法中，也期望实现几个不同的折划分，然后平均相应的元属性。对于一个元算法，对元属性应用正则化或添加一些正常噪声是有意义的。这种添加的系数类似于正则化系数。我们可以总结说，描述的方法背后的基本思想是使用一组基础算法；然后，使用另一个元算法，我们将它们的预测结合起来，目的是减少泛化误差。
- en: Unlike boosting and traditional bagging, you can use algorithms of a different
    nature (for example, a ridge regression in combination with a random forest) in
    stacking. However, it is essential to remember that for different algorithms,
    different feature spaces are needed. For example, if categorical features are
    used as target variables, then the random forest algorithm can be used as-is,
    but for the regression algorithms, you must first run one-hot encoding.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与提升和传统的袋装不同，在堆叠中，你可以使用不同性质的算法（例如，结合随机森林的岭回归）。然而，记住对于不同的算法，需要不同的特征空间是至关重要的。例如，如果将分类特征用作目标变量，那么可以直接使用随机森林算法，但对于回归算法，你必须首先进行
    one-hot 编码。
- en: Since meta-features are the results of already trained algorithms, they strongly
    correlate. This fact is *a priori* one of the disadvantages of this approach;
    the elementary algorithms are often under-optimized during training to combat
    correlation. Sometimes, to combat this drawback, the training of elementary algorithms
    is used not on the target feature, but on the differences between a feature and
    the target.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于元特征是已经训练好的算法的结果，它们具有很强的相关性。这个事实是 *先验的*，这是这种方法的一个缺点；基本算法在训练过程中往往被欠优化，以对抗相关性。有时，为了克服这个缺点，基本算法的训练不是在目标特征上，而是在特征与目标之间的差异上进行的。
- en: Using the random forest method for creating ensembles
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用随机森林方法创建集成
- en: Before we move to the random forest method, we need to familiarize ourselves
    with the decision tree algorithm, which is the basis for the random forest ensemble
    algorithm.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们转向随机森林方法之前，我们需要熟悉决策树算法，它是随机森林集成算法的基础。
- en: Decision tree algorithm overview
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树算法概述
- en: A decision tree is a supervised machine learning algorithm based on how a human
    solves the task of forecasting or classification. Generally, this is a *k*-dimensional
    tree with decision rules in the nodes and a prediction of the objective function
    at the leaf nodes. The decision rule is a function that allows you to determine
    which of the child nodes should be used as a parent for the considered object.
    There can be different types of objects in the decision tree leaf—namely, the
    class label assigned to the object (in the classification tasks), the probability
    of the class (in the classification tasks), and the value of the objective function
    (in the regression task).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种基于人类解决预测或分类任务的监督机器学习算法。通常，这是一个具有决策规则的节点和叶节点上目标函数预测的 *k* 维树。决策规则是一个函数，它允许你确定哪个子节点应该被用作考虑对象的父节点。决策树的叶节点中可以存在不同类型的对象——即分配给对象的类别标签（在分类任务中）、类别的概率（在分类任务中）以及目标函数的值（在回归任务中）。
- en: In practice, binary decision trees are used more often than trees with an arbitrary
    number of child nodes.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，二叉决策树比具有任意数量子节点的树使用得更频繁。
- en: 'The algorithm for constructing a decision tree in its general form is formed
    as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 构建决策树的一般算法如下：
- en: Firstly, check the criterion for stopping the algorithm. If this criterion is
    executed, select the prediction issued for the node. Otherwise, we have to split
    the training set into several non-intersecting smaller sets.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，检查算法停止的标准。如果执行了这个标准，就选择为节点发布的预测。否则，我们必须将训练集分割成几个不相交的小集合。
- en: 'In the general case, a ![](img/B22503_Formula_057.png) decision rule is defined
    at the *t* node, which takes into account a certain range of values. This range
    is divided into *R*t disjoint sets of objects: ![](img/B22503_Formula_058.png),
    where *R*t is the number of descendants of the node, and each ![](img/B22503_Formula_059.png)is
    a set of objects that fall into the ![](img/B22503_Formula_060.png)descendant.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一般情况下，一个 ![](img/B22503_Formula_057.png) 决策规则在 *t* 节点定义，它考虑了一定范围内的值。这个范围被划分为
    *R*t 个不相交的对象集合：![](img/B22503_Formula_058.png)，其中 *R*t 是节点的后代数量，每个 ![](img/B22503_Formula_059.png)
    是落入 ![](img/B22503_Formula_060.png) 后代的对象集合。
- en: Divide the set in the node according to the selected rule, and repeat the algorithm
    recursively for each node.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据所选规则将节点中的集合分割，并对每个节点递归地重复算法。
- en: 'Most often, the ![](img/B22503_Formula_061.png) decision rule is simply the
    feature—that is, ![](img/B22503_Formula_062.png). For partitioning, we can use
    the following rules:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的决策规则是 ![](img/B22503_Formula_061.png)，即特征——也就是说，![](img/B22503_Formula_062.png)。对于分区，我们可以使用以下规则：
- en: '![](img/B22503_Formula_063.png) for chosen boundary values ![](img/B22503_Formula_064.png).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B22503_Formula_063.png) 对于选择的边界值 ![](img/B22503_Formula_064.png)。'
- en: '![](img/B22503_Formula_065.png), where ![](img/B22503_Formula_066.png)is a
    vector’s scalar product. In fact, it is a corner value check.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B22503_Formula_065.png)，其中 ![](img/B22503_Formula_066.png) 是一个向量的标量积。实际上，这是一个角值检查。'
- en: '![](img/B22503_Formula_067.png), where the distance ![](img/B22503_Formula_068.png)
    is defined in some metric space (for example, ![](img/B22503_Formula_069.png)).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B22503_Formula_067.png)，其中距离 ![](img/B22503_Formula_068.png) 定义在某些度量空间中（例如，![](img/B22503_Formula_069.png)）。'
- en: '![](img/B22503_Formula_070.png), where ![](img/B22503_Formula_071.png) is a
    predicate.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B22503_Formula_070.png)，其中 ![](img/B22503_Formula_071.png) 是一个谓词。'
- en: In general, you can use any decision rules, but those that are easiest to interpret
    are better since they are easier to configure. There is no particular point in
    taking something more complicated than predicates since you can create a tree
    with 100% accuracy on the training set with the help of the predicates.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你可以使用任何决策规则，但那些最容易解释的更好，因为它们更容易配置。没有必要采取比谓词更复杂的东西，因为你可以借助谓词创建一个在训练集上具有100%准确性的树。
- en: Usually, a set of decision rules is chosen to build a tree. To find the optimal
    one among them for each particular node, we need to introduce a criterion for
    measuring optimality. The ![](img/B22503_Formula_072.png) measure is introduced
    for this and is used to measure how objects are scattered (regression) or how
    the classes are mixed (classification) in a specific ![](img/B22503_Formula_073.png)
    node. This measure is called the **impurity function**. It is required for finding
    a maximum of ![](img/B22503_Formula_074.png) according to all features and parameters
    from a set of decision rules, in order to select a decision rule. With this choice,
    we can generate the optimal partition for the set of objects in the current node.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，选择一组决策规则来构建树。为了在每个特定节点中找到它们中的最优者，我们需要引入一个衡量最优性的标准。为此引入了 ![](img/B22503_Formula_072.png)
    度量，用于测量特定 ![](img/B22503_Formula_073.png) 节点中对象如何分散（回归）或类别如何混合（分类）。这个度量称为**不纯度函数**。它需要根据决策规则集的所有特征和参数找到
    ![](img/B22503_Formula_074.png) 的最大值，以便选择决策规则。通过这个选择，我们可以为当前节点中的对象集合生成最优分区。
- en: 'Information gain, ![](img/B22503_Formula_075.png), is how much information
    we can get for the selected split, and is calculated as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益，![](img/B22503_Formula_075.png)，是指对于所选的分割我们可以获得多少信息，其计算方法如下：
- en: '![](img/B22503_Formula_076.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22503_Formula_076.jpg)'
- en: 'In the preceding equation, the following applies:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，以下适用：
- en: '*R* is the number of sub-nodes the current node is broken into'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R* 是当前节点被分割成子节点的数量'
- en: '*t* is the current node'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t* 是当前节点'
- en: '![](img/B22503_Formula_077.png) are the descendant nodes that are obtained
    with the selected partition'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![img/B22503_Formula_077.png](img/B22503_Formula_077.png) 是通过所选分区获得的后代节点'
- en: '![](img/B22503_Formula_078.png) is the number of objects in the training sample
    that fall into the child *i*'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![img/B22503_Formula_078.png](img/B22503_Formula_078.png) 是落在当前节点子 *i* 的对象数量'
- en: '![](img/B22503_Formula_079.png) is the number of objects trapped in the current
    node'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![img/B22503_Formula_079.png](img/B22503_Formula_079.png) 是当前节点中捕获的对象数量'
- en: '![](img/B22503_Formula_080.png) are the objects trapped in the *t*ith vertex'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![img/B22503_Formula_080.png](img/B22503_Formula_080.png) 是被捕获在 *t* 顶点的对象'
- en: 'We can use the MSE or the **mean absolute error** (**MAE**) as the ![](img/B22503_Formula_081.png)
    impurity function for regression tasks. For classification tasks, we can use the
    following functions:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 MSE 或 **平均绝对误差**（**MAE**）作为回归任务的 ![img/B22503_Formula_081.png](img/B22503_Formula_081.png)
    杂乱函数。对于分类任务，我们可以使用以下函数：
- en: Gini criterion ![](img/B22503_Formula_082.png) as the probability of misclassification,
    specifically if we predict classes with probabilities of their occurrence in a
    given node
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gini 准则 ![img/B22503_Formula_082.png](img/B22503_Formula_082.png) 作为误分类的概率，具体来说，如果我们预测给定节点中类别的发生概率
- en: Entropy ![](img/B22503_Formula_083.png) as a measure of the uncertainty of a
    random variable
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熵 ![img/B22503_Formula_083.png](img/B22503_Formula_083.png) 作为随机变量的不确定性度量
- en: Classification error ![](img/B22503_Formula_084.png) as the error rate in the
    classification of the most potent class
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将分类错误 ![img/B22503_Formula_084.png](img/B22503_Formula_084.png) 作为最强类别的分类错误率
- en: In the functions described previously, ![](img/B22503_Formula_085.png) is an
    *a priori* probability of encountering an object of class *i* in a node *t*—that
    is, the number of objects in the training sample with labels of class *i* falling
    into *t* divided by the total number of objects in *t* (![](img/B22503_Formula_086.png)).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前描述的函数中，![img/B22503_Formula_085.png](img/B22503_Formula_085.png) 是节点 *t*
    中遇到类别 *i* 对象的 **先验概率**——即训练样本中标签为类别 *i* 并落入 *t* 的对象数量除以 *t* 中的对象总数（![img/B22503_Formula_086.png](img/B22503_Formula_086.png)）。
- en: 'The following rules can be applied as stopping criteria for building a decision
    tree:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 以下规则可以作为构建决策树的停止标准：
- en: Limiting the maximum depth of the tree
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制树的最大深度
- en: Limiting the minimum number of objects in the sheet
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制叶片中的最小对象数量
- en: Limiting the maximum number of leaves in a tree
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制树中叶子的最大数量
- en: Stopping if all objects in the node belong to the same class
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果节点中的所有对象都属于同一类，则停止
- en: Requiring that information gain is improved by at least 8% during splitting
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要求在分裂过程中信息增益至少提高 8%
- en: There is an error-free tree for any training set, which leads to the problem
    of overfitting. Finding the right stopping criterion to solve this problem is
    challenging. One solution is **pruning**—after the whole tree is constructed,
    we can cut some nodes. Such an operation can be performed using a test or validation
    set. Pruning can reduce the complexity of the final classifier and improve predictive
    accuracy by reducing overfitting.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何训练集，都有一个无错误的树，这导致了过拟合的问题。找到正确的停止标准来解决这个问题是具有挑战性的。一种解决方案是**剪枝**——在整棵树构建完成后，我们可以剪掉一些节点。这种操作可以使用测试或验证集来完成。剪枝可以减少最终分类器的复杂性，并通过减少过拟合来提高预测精度。
- en: 'The pruning algorithm is formed as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝算法如下：
- en: We build a tree for the training set.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为训练集构建一个树。
- en: Then, we pass a validation set through the constructed tree and consider any
    internal node *t* and its left and right sub-nodes ![](img/B22503_Formula_087.png),
    ![](img/B22503_Formula_088.png).
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将验证集通过构建的树，并考虑任何内部节点 *t* 及其左右子节点 ![img/B22503_Formula_087.png](img/B22503_Formula_087.png)，![img/B22503_Formula_088.png](img/B22503_Formula_088.png)。
- en: If no one object from the validation sample has reached *t*, then we can say
    that this node (and all its subtrees) is insignificant, and make *t* the leaf
    (set the predicate’s value for this node equal to the set of the majority class
    using the training set).
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果验证样本中没有对象达到 *t*，那么我们可以说这个节点（及其所有子树）是不重要的，并将 *t* 设为叶子（使用训练集将此节点的谓词值设置为多数类的集合）。
- en: 'If objects from the validation set have reached *t*, then we have to consider
    the following three values:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果验证集中的对象达到了 *t*，那么我们必须考虑以下三个值：
- en: The number of classification errors from a subtree of *t*
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 *t* 子树的分类错误数量
- en: The number of classification errors from the ![](img/B22503_Formula_089.png)
    subtree
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 ![img/B22503_Formula_089.png](img/B22503_Formula_089.png) 子树的分类错误数量
- en: The number of classification errors from the ![](img/B22503_Formula_090.png)
    subtree
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 ![img/B22503_Formula_090.png](img/B22503_Formula_090.png) 子树的分类错误数量
- en: 'If the value for the first case is zero, then we make node *t* as a leaf node
    with the corresponding prediction for the class. Otherwise, we choose the minimum
    of these values. Depending on which of them is minimal, we do the following, respectively:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果第一个案例的值为零，则将节点*t*作为具有相应类别预测的叶节点。否则，我们选择这些值中的最小值。根据哪个是最小的，我们分别执行以下操作：
- en: If the first is minimal, do nothing
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果第一个是最小的，则什么都不做
- en: If the second is minimal, replace the tree from node *t* with a subtree from
    node ![](img/B22503_Formula_091.png)
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果第二个是最小的，则用节点![img/B22503_Formula_091.png](img/B22503_Formula_091.png)的子树替换节点*t*。
- en: If the third is minimal, replace the tree from node *t* with a subtree from
    node ![](img/B22503_Formula_092.png)
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果第三个是最小的，则用节点![img/B22503_Formula_092.png](img/B22503_Formula_092.png)的子树替换节点*t*。
- en: Such a procedure regularizes the algorithm to beat overfitting and increase
    the ability to generalize. In the case of a *k*-dimensional tree, different approaches
    can be used to select the forecast in the leaf. We can take the most common class
    among the objects of the training that fall under this leaf for classification.
    Alternatively, we can calculate the average of the objective functions of these
    objects for regression.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的过程使算法规范化，以克服过拟合并提高泛化能力。在*k*-维树的情况下，可以使用不同的方法来选择叶中的预测。对于分类，我们可以选择落在该叶下的训练对象中最常见的类别。或者，我们可以计算这些对象的客观函数的平均值来进行回归。
- en: We apply a decision rule to a new object starting from the tree root to predict
    or classify new data. Thus, it is determined which subtree the object should go
    into. We recursively repeat this process until we reach some leaf node and, finally,
    we return the value of the leaf node we found as the result of classification
    or regression.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从树根开始应用决策规则到一个新对象，以预测或分类新数据。因此，确定对象应该进入哪个子树。我们递归地重复此过程，直到达到某个叶节点，最后返回我们找到的叶节点的值，作为分类或回归的结果。
- en: Random forest method overview
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林方法概述
- en: Decision trees are a suitable family of elementary algorithms for bagging since
    they are quite complicated and can ultimately achieve zero errors on any training
    set. We can use a method that uses random subspaces (such as bagging) to reduce
    the correlation between trees and avoid overfitting. The elementary algorithms
    are trained on different subsets of the feature space, which are also randomly
    selected. An ensemble of decision tree models using the random subspace method
    can be constructed using the following algorithm.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是用于袋装算法的合适的基本算法族，因为它们相当复杂，并且最终可以在任何训练集上实现零误差。我们可以使用一种使用随机子空间（如袋装）的方法来减少树之间的相关性，避免过拟合。基本算法在不同的特征空间子集上训练，这些子集也是随机选择的。可以使用以下算法构建使用随机子空间方法的决策树模型集合。
- en: 'Where the number of objects for training is *N* and the number of features
    is ![](img/B22503_Formula_093.png), proceed as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 其中训练对象的数量为*N*，特征的数量为![img/B22503_Formula_093.png](img/B22503_Formula_093.png)，按照以下步骤进行：
- en: Select ![](img/B22503_Formula_094.png) as the number of individual trees in
    the ensemble.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择![img/B22503_Formula_094.png](img/B22503_Formula_094.png)作为集成中的单个树的数量。
- en: For each individual ![](img/B22503_Formula_095.png) tree, select ![](img/B22503_Formula_096.png)
    as the number of features for ![](img/B22503_Formula_097.png). Typically, only
    one value is used for all trees.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个单独的![img/B22503_Formula_095.png](img/B22503_Formula_095.png)树，选择![img/B22503_Formula_096.png](img/B22503_Formula_096.png)作为![img/B22503_Formula_097.png](img/B22503_Formula_097.png)的特征数量。通常，所有树都使用一个值。
- en: For each tree, create an ![](img/B22503_Formula_098.png) training subset using
    the bootstrap method.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每棵树，使用自举方法创建一个![img/B22503_Formula_098.png](img/B22503_Formula_098.png)训练子集。
- en: 'Now, build decision trees from ![](img/B22503_Formula_098.png) samples as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，按照以下方式从![img/B22503_Formula_098.png](img/B22503_Formula_098.png)个样本构建决策树：
- en: Select ![](img/B22503_Formula_100.png) random features from the source, then
    the optimal division of the training set will limit its search to them.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从源中选择![img/B22503_Formula_100.png](img/B22503_Formula_100.png)个随机特征，然后最优的训练集分割将限制其搜索范围。
- en: According to a given criterion, we choose the best attribute and make a split
    in the tree according to it.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据给定的标准，我们选择最佳的属性，并根据它对树进行分割。
- en: The tree is built until no more than ![](img/B22503_Formula_101.png) objects
    remain in each leaf, until we reach a certain height of the tree, or until the
    training set is exhausted.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 树的构建直到每个叶节点中剩余的对象不超过![img/B22503_Formula_101.png](img/B22503_Formula_101.png)，直到达到树的某个高度，或者直到训练集耗尽。
- en: 'Now, to apply the ensemble model to a new object, it is necessary to combine
    the results of individual models by majority voting or by combining *a posteriori*
    probabilities. An example of a final classifier is as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要将集成模型应用于新对象，必须通过多数投票或通过结合**后验**概率来组合单个模型的结果。以下是一个最终分类器的示例：
- en: '![](img/B22503_Formula_102.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![公式](img/B22503_Formula_102.jpg)'
- en: 'Consider the following fundamental parameters of the algorithm and their properties:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下算法的基本参数及其属性：
- en: '**The number of trees**: The more trees, the better the quality, but the training
    time and the algorithm’s workload also increase proportionally. Often, with an
    increasing number of trees, the quality of the training set rises (it can even
    go up to 100% accuracy), but the quality of the test set is asymptotic (so you
    can estimate the minimum required number of trees).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树的数量**：树越多，质量越好，但训练时间和算法的工作量也会成比例增加。通常，随着树的数量增加，训练集的质量会提高（甚至可以达到100%的准确率），但测试集的质量是渐近的（因此可以估计所需的最小树的数量）。'
- en: '**The number of features for the splitting selection**: With an increasing
    number of features, the forest’s construction time increases too, and the trees
    become more uniform than before. Often, in classification problems, the number
    of attributes is chosen equal to ![](img/B22503_Formula_103.png) and ![](img/B22503_Formula_104.png)
    for regression problems.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分裂选择的特征数量**：随着特征数量的增加，森林的构建时间也会增加，树比以前更均匀。在分类问题中，通常选择与![公式](img/B22503_Formula_103.png)和![公式](img/B22503_Formula_104.png)相等的属性数量，对于回归问题。'
- en: '**Maximum tree depth**: The smaller the depth, the faster the algorithm is
    built and will work. As the depth increases, the quality during training increases
    dramatically. The quality may also increase on the test set. It is recommended
    to use the maximum depth (except when there are too many training objects and
    we obtain very deep trees, the construction of which takes considerable time).
    When using shallow trees, changing the parameters associated with limiting the
    number of objects in the leaf and for splitting does not lead to a significant
    effect (the leaves are already large). Using shallow trees is recommended in tasks
    with a large number of noisy objects (outliers).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大树深度**：深度越小，算法构建和运行的速度越快。随着深度的增加，训练过程中的质量会显著提高。测试集上的质量也可能提高。建议使用最大深度（除非训练对象太多，我们得到非常深的树，构建这些树需要相当多的时间）。当使用浅层树时，改变与限制叶节点中对象数量和分裂相关的参数不会产生显著影响（叶子已经很大）。在具有大量噪声对象（异常值）的任务中推荐使用浅层树。'
- en: '**The impurity function**: This is a criterion for choosing a feature (decision
    rule) for branching. It is usually MSE/MAE for regression problems. For classification
    problems, it is the Gini criterion, the entropy, or the classification error.
    The balance and depth of trees may vary depending on the specific impurity function
    we choose.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**杂质函数**：这是选择分支特征（决策规则）的标准。对于回归问题，通常是均方误差（MSE）/平均绝对误差（MAE）。对于分类问题，是基尼准则、熵或分类误差。树的平衡和深度可能取决于我们选择的特定杂质函数。'
- en: We can consider a random forest as bagging decision trees, and during these
    trees’ training, we use features from a random subset of features for each partition.
    This approach is a universal algorithm since random forests exist for solving
    problems of classification, regression, clustering, anomaly search, and feature
    selection, among other tasks.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将随机森林视为袋装决策树，在这些树的训练过程中，我们为每个分区使用特征随机子集的特征。这种方法是一个通用算法，因为随机森林可以用于解决分类、回归、聚类、异常搜索和特征选择等任务。
- en: In the following section, we will see how to use different C++ libraries for
    developing machine learning model ensembles.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何使用不同的C++库来开发机器学习模型集成。
- en: Examples of using C++ libraries for creating ensembles
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用C++库创建集成示例
- en: The following sections will show how to use ensembles within the `Dlib` and
    `mlpack` libraries. There are out-of-the-box implementations of random forest
    and gradient boosting algorithms in these libraries; we will show how to use their
    `mlpack` library.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节将展示如何在`Dlib`和`mlpack`库中使用集成。这些库中有现成的随机森林和梯度提升算法的实现；我们将展示如何使用它们的`mlpack`库。
- en: Ensembles with Dlib
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Dlib的集成
- en: There is only the random forest algorithm implementation in the `Dlib` library,
    and in this section, we will show the specific API for using it in practice.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dlib` 库中只有随机森林算法的实现，在本节中，我们将展示如何在实践中使用它的具体 API。'
- en: 'To show the random forest algorithm application, we need to have some dataset
    for this task. Let’s create an artificial dataset that models the cosine function.
    First, we define datatypes to represent samples and label items. The following
    code sample shows how it is done:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示随机森林算法的应用，我们需要为此任务创建一些数据集。让我们创建一个模拟余弦函数的人工数据集。首先，我们定义数据类型来表示样本和标签项。以下代码示例显示了如何实现：
- en: '[PRE0]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we define the `GenerateData` function:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义 `GenerateData` 函数：
- en: '[PRE1]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `GenerateData` function takes three parameters: the `start` and `end` values
    of the generation range and the `n` numbers of points to generate. The implementation
    simply calculates cosine values in the loop. The function returns a pair of `std::vector`
    type objects containing the `double` values. The result of this function will
    be used for testing.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`GenerateData` 函数接受三个参数：生成范围的起始值和结束值以及要生成的点数 `n`。实现方式简单，在循环中计算余弦值。该函数返回一个包含
    `double` 值的 `std::vector` 类型对象对。此函数的结果将用于测试。'
- en: 'To show that the random forest algorithm really can approximate values, we
    will add some noise to the original data. The following code snippet shows the
    `GenerateNoiseData` function implementation:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明随机森林算法确实可以近似值，我们将向原始数据添加一些噪声。以下代码片段显示了 `GenerateNoiseData` 函数的实现：
- en: '[PRE2]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `GenerateNoiseData` function also calculates cosine values in the simple
    loop. It takes the same input parameters as the `GenerateData` function. However,
    instead of the sequential value generation, this function samples a random value
    from the specified range on each iteration. For each sample, it calculates the
    cosine value and also adds the noise samples. The noise is generated by random
    distribution. The function also returns two `std::vector` type objects containing
    the `double` values, the first one for training inputs and the second one for
    target values.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '`GenerateNoiseData` 函数也在简单的循环中计算余弦值。它接受与 `GenerateData` 函数相同的输入参数。然而，此函数不是按顺序生成值，而是在每次迭代中从指定的范围内采样一个随机值。对于每个样本，它计算余弦值并添加噪声样本。噪声是通过随机分布生成的。该函数还返回两个包含
    `double` 值的 `std::vector` 类型对象，第一个用于训练输入，第二个用于目标值。'
- en: 'Using these data generation functions, we can create the training and the test
    datasets as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些数据生成函数，我们可以创建训练集和测试集，如下所示：
- en: '[PRE3]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, the usage of the Dlib random forest implementation is very simple. The
    following code snippet shows it:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Dlib 随机森林实现的用法非常简单。以下代码片段展示了它：
- en: '[PRE4]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we used the instance of the `random_forest_regression_trainer` class named
    `trainer` to create the `random_forest` object with the `train` method. The `trainer`
    object was configured with the number of trees to use. The `random_forest_regression_trainer`
    class was parametrized with the `dense_feature_extractor` class—this is the only
    feature extractor class provided now by the `Dlib` library, but you can create
    a custom one. The `train` method simply takes two `std::vector` type objects,
    the first one for the input data values and the second one for the target data
    values.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用名为 `trainer` 的 `random_forest_regression_trainer` 类的实例，通过 `train` 方法创建
    `random_forest` 对象。`trainer` 对象配置了要使用的树的数量。`random_forest_regression_trainer`
    类使用 `dense_feature_extractor` 类进行参数化——这是 `Dlib` 库现在提供的唯一特征提取类，但你可以创建一个自定义的。`train`
    方法简单地接受两个 `std::vector` 类型对象，第一个用于输入数据值，第二个用于目标数据值。
- en: After the training, the `random_forest` object was created, and it was used
    as a functional object to make a prediction for a single value.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，创建了 `random_forest` 对象，并将其用作功能对象来对一个单一值进行预测。
- en: 'The following figure shows the result of applying the random forest algorithm
    from the `Dlib` library. The original data is shown as stars and the predicted
    data is shown as lines:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了从 `Dlib` 库应用随机森林算法的结果。原始数据以星号表示，预测数据以线条表示：
- en: '![Figure 9.2 – Regression with random forest and Dlib](img/B19849_09_2.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 使用 Dlib 的随机森林回归](img/B19849_09_2.jpg)'
- en: Figure 9.2 – Regression with random forest and Dlib
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 使用 Dlib 的随机森林回归
- en: Note that this method is not very applicable to the regression task on this
    dataset. You can see that global trends were learned successfully but there are
    a lot of errors in small details.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这种方法对这个数据集上的回归任务不太适用。你可以看到全局趋势被成功学习，但在细节上有很多错误。
- en: Ensembles with mlpack
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用mlpack的集成
- en: 'There are two ensemble learning algorithms in the `mlpack` library: the random
    forest and AdaBoost algorithms. For this set of samples, we will use the *Breast
    Cancer Wisconsin (Diagnostic)* dataset located at [https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic).
    It is taken from *D*. *Dua, and C. Graff (2019), UCI Machine Learning* *Repository*
    ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlpack`库中有两种集成学习算法：随机森林和AdaBoost算法。对于这组样本，我们将使用位于[https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)的*Wisconsin乳腺癌诊断*数据集。它来自*D*.
    *Dua, 和 C. Graff (2019), UCI机器学习* *Repository* ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml))。'
- en: 'There are 569 instances in this dataset, and each instance has 32 attributes:
    the ID, the diagnosis, and 30 real-value input features. The diagnosis can have
    two values: *M =* malignant, and *B =* benign. Other attributes have 10 real-value
    features computed for each cell nucleus, as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集中有569个实例，每个实例有32个属性：ID、诊断和30个实值输入特征。诊断可以有两个值：*M =* 恶性，和*B =* 良性。其他属性为每个细胞核计算了10个实值特征，如下所示：
- en: Radius (mean distance from the center to the perimeter)
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半径（从中心到周界的平均距离）
- en: Texture (standard deviation of grayscale values)
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纹理（灰度值的标准差）
- en: Perimeter
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周长
- en: Area
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面积
- en: Smoothness (local variation in radius lengths)
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 光滑度（半径长度的局部变化）
- en: Compactness
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 紧凑度
- en: Concavity (severity of concave portions of the contour)
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凹陷（轮廓凹部分的严重程度）
- en: Concave points (number of concave portions of the contour)
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凹点（轮廓凹部分的数目）
- en: Symmetry
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对称性
- en: Fractal dimension (*coastline approximation*—1)
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分形维度（海岸线近似—1）
- en: This dataset can be used for a binary classification task.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集可以用于二元分类任务。
- en: Data preparation for mlpack
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: mlpack的数据准备
- en: There is the `DatasetInfo` class in `mlpack` to describe a dataset. An instance
    of this class can be used with different algorithms. Also, there is the `data::Load`
    function in `mlpack` that can automatically load datasets from the `.csv`, `.tsv`,
    and `.txt` files. However, this function assumes that there is only numerical
    data in such files that can be interpreted as a matrix. In our case, the data
    is in the `.csv` format but the `Diagnosis` column contains the string values
    `M` and `B`. So, we simply convert them into `0` and `1`.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在`mlpack`中有一个`DatasetInfo`类用于描述数据集。这个类的实例可以与不同的算法一起使用。此外，`mlpack`中还有一个`data::Load`函数，可以自动从`.csv`、`.tsv`和`.txt`文件中加载数据集。然而，这个函数假设这些文件中只包含可以解释为矩阵的数值数据。在我们的情况下，数据以`.csv`格式存在，但`Diagnosis`列包含字符串值`M`和`B`。因此，我们简单地将它们转换为`0`和`1`。
- en: 'When we have the correct `.csv` file, the data can be loaded in the following
    way:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有了正确的`.csv`文件后，数据可以按照以下方式加载：
- en: '[PRE5]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We passed into the `Load` function the dataset filename and references to the
    `data` matrix object and the `info` object. Also, notice that we asked the function
    to generate an exception in the fail case by passing the last parameter as `true`.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据集文件名和`data`矩阵对象以及`info`对象的引用传递给`Load`函数。注意，我们通过将最后一个参数传递为`true`来请求函数在失败情况下生成异常。
- en: 'Then we split the data into the input and target parts as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们按照以下方式将数据分为输入和目标部分：
- en: '[PRE6]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we used the `row` method of the matrix object to get the particular row.
    Then we converted its values into the `site_t` type with the `arma::conv_to` function
    because the first row in our dataset consists of labels. Finally, we removed the
    first row from the `data` object to make it usable as input data.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了矩阵对象的`row`方法来获取特定的行。然后我们使用`arma::conv_to`函数将其值转换为`site_t`类型，因为我们的数据集的第一行包含标签。最后，我们从`data`对象中移除了第一行，使其可以作为输入数据使用。
- en: 'Having input data and labels matrices, we can split them into the training
    and testing parts as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有输入数据和标签矩阵后，我们可以按照以下方式将它们分为训练和测试部分：
- en: '[PRE7]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We used the `head_cols` method of the matrix object to take the first `train_num`
    columns from the input data and label them as the train values, and we used the
    `tail_cols` method of the matrix object to take the last columns as the test values.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用矩阵对象的 `head_cols` 方法从输入数据中取出前 `train_num` 列并将其标记为训练值，并使用矩阵对象的 `tail_cols`
    方法取出最后的列作为测试值。
- en: Using random forest with mlpack
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 `mlpack` 的随机森林
- en: 'The random forest algorithm in the `mlpack` library is located in the `RandomForest`
    class. This class has two main methods: `Train` and `Classify`. The `Train` method
    can be used as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlpack` 库中的随机森林算法位于 `RandomForest` 类中。此类有两个主要方法：`Train` 和 `Classify`。`Train`
    方法可以使用如下方式：'
- en: '[PRE8]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The first four parameters are self-descriptive. The last ones are more algorithm-specific.
    The `minimumLeafSize` parameter is the minimum number of points in each tree’s
    leaf nodes. The `minimumGainSplit` parameter is the minimum gain for splitting
    a decision tree node. The `maximumDepth` parameter is the maximum allowed tree
    depth.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 前四个参数具有自描述性。最后几个参数更具有算法特定性。`minimumLeafSize` 参数是每个树叶子节点的最小点数。`minimumGainSplit`
    参数是分割决策树节点的最小增益。`maximumDepth` 参数是允许的最大树深度。
- en: After the use of the `Train` method with the training data, the `rf` object
    can be used for the classification with the `Classify` method. This method takes
    as its first parameter the single value or the row matrix as input, and the second
    parameter is the reference to the single prediction value or the vector of predictions
    that will be filled by this method.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 `Train` 方法处理训练数据后，可以使用 `rf` 对象通过 `Classify` 方法进行分类。此方法将单个值或行矩阵作为输入的第一个参数，第二个参数是单个预测值或由该方法填充的预测值向量。
- en: 'There is the `Accuracy` class in `mlpack` that can be used to estimate an algorithm’s
    accuracy. It can work with different algorithm objects and have a unified interface.
    We can use it as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlpack` 中有一个 `Accuracy` 类，可以用来估计算法的准确度。它可以与不同的算法对象一起工作，并具有统一的接口。我们可以如下使用它：'
- en: '[PRE9]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We used the `Evaluate` method to get the accuracy value for the random forest
    algorithm trained with our data. The printed value is `Random Forest accuracy
    =` `0.971014`.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `Evaluate` 方法获取使用我们的数据训练的随机森林算法的准确度值。打印的值是 `Random Forest accuracy = 0.971014`。
- en: Using AdaBoost with mlpack
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 `mlpack` 的 AdaBoost
- en: 'Another ensemble-based algorithm in the `mlpack` library is AdaBoost. It is
    based on an ensemble of weak learners that is used to produce a strong learner.
    Let’s define an AdaBoost algorithm object based on simple perceptrons as weak
    learners, as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlpack` 库中另一个基于集成算法的是 AdaBoost。它基于弱学习者的集成，用于生成强学习者。以下是基于简单感知器作为弱学习者的 AdaBoost
    算法对象的定义：'
- en: '[PRE10]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We parametrized the `AdaBoost` class with the `Perceptron` class as the template
    parameter. After the `AdaBoost` object is instantiated, we can use the `Train`
    method to train it with our dataset. The following code snippet shows how to use
    the `Train` method:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `Perceptron` 类作为模板参数来参数化 `AdaBoost` 类。在 `AdaBoost` 对象实例化后，我们可以使用 `Train`
    方法用我们的数据集对其进行训练。以下代码片段显示了如何使用 `Train` 方法：
- en: '[PRE11]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The first three input parameters are pretty obvious—the input data, labels,
    and number of classes for classification. Then we passed the `p` object; it’s
    the instance of the `Perceptron` class, our weak learner. After the weak learner
    object, we passed into the `Train` method the number of iterations to learn and
    the accuracy tolerance to stop learning early.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 前三个输入参数非常明显——输入数据、标签和分类的类别数。然后我们传递了 `p` 对象；它是 `Perceptron` 类的实例，我们的弱学习者。在弱学习者对象之后，我们将学习次数和提前停止学习的准确度容忍度传递给
    `Train` 方法。
- en: 'After the strong learner `ab` is trained, we can use the `Classify` method
    to get classification for a new data value. Also, we can use an object of the
    `Accuracy` class to estimate the accuracy of the trained algorithm. We already
    saw how to use `Accuracy` in the previous chapter. Its API is the same for all
    algorithms in `mlpack`. For `AdaBoost`, we can use it as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在强学习者 `ab` 训练完成后，我们可以使用 `Classify` 方法对新数据值进行分类。此外，我们还可以使用 `Accuracy` 类的实例来估计训练算法的准确度。我们已经在上一章中看到了如何使用
    `Accuracy`。它的 API 对于 `mlpack` 中的所有算法都是相同的。对于 `AdaBoost`，我们可以如下使用：
- en: '[PRE12]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For the `AdaBoost` algorithm with the same dataset, we got the following output:
    `AdaBoost accuracy = 0.985507`. The accuracy is slightly better than we got with
    the random forest algorithm.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 对于与相同数据集的 `AdaBoost` 算法，我们得到了以下输出：`AdaBoost 准确率 = 0.985507`。准确率略高于我们使用随机森林算法得到的准确率。
- en: Using a stacking ensemble with mlpack
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 mlpack 的堆叠集成
- en: To show the implementation of more ensemble learning techniques, we can develop
    the stacking approach manually. This is not hard with the `mlpack` library, or
    indeed any other library.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示更多集成学习技术的实现，我们可以手动开发堆叠方法。使用 `mlpack` 库或任何其他库都不是很难。
- en: 'The stacking approach is based on learning a set of weak learners. Usually,
    the k-fold technique is used to implement this. It means that we learn a weak
    model on `k-1` folds and use the last fold for predictions. Let’s see how we can
    create k-fold-splitting using `mlpack`. We will use the same dataset as we did
    for the previous subsections. The main idea is to repeat the dataset to be able
    to get different folds just by using indices. The following code snippet defines
    the `KfoldDataSet` structure with just one method and constructor:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠方法基于学习一组弱学习器。通常，k-fold 技术用于实现这一点。这意味着我们在 `k-1` 折上学习一个弱模型，并使用最后一折进行预测。让我们看看我们如何使用
    `mlpack` 创建 k-fold 拆分。我们将使用与前面小节相同的相同数据集。主要思想是重复数据集，以便仅通过使用索引就能得到不同的折。以下代码片段定义了具有一个方法和构造函数的
    `KfoldDataSet` 结构：
- en: '[PRE13]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The constructor takes the input data, labels, and number of folds for splitting.
    The `get_fold` method takes an index of a fold and returns four values:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数接受输入数据、标签和用于拆分的折数。`get_fold` 方法接受一个折的索引并返回四个值：
- en: The matrix contains `k-1` folds of the input data
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵包含 `k-1` 折的输入数据
- en: The row matrix contains `k-1` folds of labels
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行矩阵包含 `k-1` 折的标签
- en: The matrix contains the last fold of the input data
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵包含输入数据的最后一折
- en: The row matrix contains the last fold of labels
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行矩阵包含最后一折的标签
- en: 'Constructor implementation can be as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数的实现可以如下：
- en: '[PRE14]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, we calculated the `fold_size` values by dividing the total number of samples
    in the input data by the number of folds. The total number of samples can be unaligned
    with the number of folds, so the last fold size can be different. That is why
    we additionally calculated the `last_fold_size` value to be able to make a correct
    splitting later. Having the fold size values, we used the `arma::join_rows` function
    to repeat training samples. This function joins two matrices; for the first parameter,
    we used the original sample matrices, and for the second, we used reduced ones.
    We took just the `k-1` columns for the second parameter using the `cols` method
    of the matrix object.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过将输入数据中的样本总数除以折数来计算 `fold_size` 值。样本总数可能与折数不匹配，因此最后一折的大小可能不同。这就是为什么我们额外计算了
    `last_fold_size` 值，以便能够正确地拆分。有了折的大小值，我们使用了 `arma::join_rows` 函数来重复训练样本。这个函数连接两个矩阵；对于第一个参数，我们使用了原始样本矩阵，对于第二个参数，我们使用了减少的矩阵。我们使用矩阵对象的
    `cols` 方法只取第二个参数的 `k-1` 列。
- en: 'When we have the repeated data samples, the `get_fold` method implementation
    can be as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有重复的数据样本时，`get_fold` 方法的实现可以如下：
- en: '[PRE15]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The most important part of the `get_fold` method is to get the correct number
    of samples that belong to the `k-1` subset and the last fold subset. So, at first,
    we checked whether the required fold to be split is last or not because the last
    fold may contain a different size of samples. Having this information, we just
    multiplied the `k-1` or `k-2` numbers by the fold size, and conditionally added
    the last fold size to have the sample subsets. For the last fold, we also conditionally
    got the fold size depending on the required fold-splitting index.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_fold` 方法最重要的部分是获取属于 `k-1` 子集和最后一折子集的正确样本数量。因此，首先我们检查要拆分的所需折是否为最后一折，因为最后一折可能包含不同大小的样本。有了这个信息，我们只需将
    `k-1` 或 `k-2` 的数字乘以折的大小，并根据条件添加最后一折的大小以获得样本子集。对于最后一折，我们也根据所需的折拆分索引有条件地获取折的大小。'
- en: Having the correct subset sizes, we used the `colptr` method to get the pointer
    to the starting column sample from the repeated data. We used such a pointer and
    the subset size to initialize the `arma::mat` object pointing to the existing
    data without copying by setting the `copy_aux_mem` constructor parameter to `false`.
    Using such an approach, we initialized matrices for the `k-1` fold samples and
    the last fold samples and returned them as a tuple.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得正确的子集大小后，我们使用了`colptr`方法来获取从重复数据中开始列样本的指针。我们使用这样的指针和子集大小来初始化指向现有数据的`arma::mat`对象，而不进行复制，通过将`copy_aux_mem`构造函数参数设置为`false`。使用这种方法，我们初始化了`k-1`个折叠样本和最后一个折叠样本的矩阵，并将它们作为元组返回。
- en: 'Having the `KfoldDataSet` class, we can move further and implement the `StackingClassification`
    function. Its declaration can be as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有`KfoldDataSet`类后，我们可以进一步实现`StackingClassification`函数。其声明可以如下所示：
- en: '[PRE16]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It will take the number of classification classes, the train input and label
    data, and the test input and label data to estimate the accuracy of the algorithm.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 它将使用分类类别数量、训练输入和标签数据以及测试输入和标签数据来估计算法的准确性。
- en: 'The `StackingClassification` function implementation can be split into the
    following steps:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '`StackingClassification`函数的实现可以分解为以下步骤：'
- en: Prepare training dataset.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备训练数据集。
- en: Create meta-datasets.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建元数据集。
- en: Train meta-model.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练元模型。
- en: Train weak models.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练弱模型。
- en: Evaluate the test data.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估测试数据。
- en: 'Let’s take a look at each of them, one by one. The dataset preparation can
    be implemented as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一查看它们。数据集准备可以如下实现：
- en: '[PRE17]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We used the `ShuffleData` function to randomize the training and the testing
    data, and we used the `sample_scaler` object of the `StandardScaler` class to
    scale our train and test data to the zero mean and unit variance. Notice that
    we fitted a scaler object on the train data and then used it with the `Transform`
    method. We will use this scaler object later for the test data too.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了`ShuffleData`函数来随机化训练和测试数据，并使用了`StandardScaler`类的`sample_scaler`对象来将我们的训练和测试数据缩放到零均值和单位方差。请注意，我们在训练数据上拟合了一个缩放器对象，然后使用`Transform`方法使用它。我们稍后会使用这个缩放器对象来处理测试数据。
- en: 'Having the prepared dataset, we can create a meta-dataset using weak (or elementary)
    algorithms that will be used for the stacking. It will be three weak algorithms’
    models:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备好数据集后，我们可以使用用于堆叠的弱（或基本）算法来创建元数据集。它将是三个弱算法的模型：
- en: Softmax regression
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softmax回归
- en: Decision tree
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Linear SVM
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性SVM
- en: 'The meta-dataset generation is based on the training and the evaluation of
    the weak models on the k-fold splits prepared from the original dataset. We already
    created the `KFoldDataset` class for this purpose and its usage will be as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据集生成基于在从原始数据集准备好的k-fold分割上对弱模型进行训练和评估。我们已创建了`KFoldDataset`类来完成此目的，其用法如下：
- en: '[PRE18]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here, we instantiated the `meta_train` dataset object for the 30-fold split.
    The following code snippet shows how the meta dataset can be generated using three
    weak models:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们实例化了`meta_train`数据集对象以进行30个折叠分割。以下代码片段显示了如何使用三个弱模型生成元数据集：
- en: '[PRE19]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The meta-dataset was stored in the `meta_train_inputs` and `meta_train_labels`
    matrix objects. Using the loop, we iterated the 30-fold indices and for each index,
    we called the `get_fold` method of the `meta_train` object. This call gave us
    the *k*th fold split, which contained the four matrices for the training and the
    evaluation. Then we trained the local weak objects, which, in our case, were instances
    of the `LinearSVM`, `SoftmaxRegression`, and `DecisionTree` class objects.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据集存储在`meta_train_inputs`和`meta_train_labels`矩阵对象中。使用循环，我们迭代了30个折叠索引，并对每个索引，我们调用了`meta_train`对象的`get_fold`方法。这个调用给了我们第*k*个折叠分割，其中包含了用于训练和评估的四个矩阵。然后我们训练了本地弱对象，在我们的例子中，这些是`LinearSVM`、`SoftmaxRegression`和`DecisionTree`类对象的实例。
- en: For their training, we used the fold’s training inputs and labels. Having -
    trained weak models(the `LinearSVM`, `SoftmaxRegression`, and `DecisionTree models`),
    we evaluated them using the `Classify` method on the fold’s test input located
    in the `fold_valid_input` object. The classification result was placed in the
    `predictions` object. All three classification results were stacked into the new
    `meta_feature` matrix object using the `join_cols` function. So, we replaced the
    original dataset features with new meta-features. This `meta_feature` object was
    added to the `meta_train_inputs` object using the `join_rows` method. The fold’s
    test labels located in `fold_valid_labels` were added to the meta-dataset using
    the `join_rows` function on the `meta_train_labels` object.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 对于它们的训练，我们使用了折的训练输入和标签。在训练了弱模型（`LinearSVM`、`SoftmaxRegression`和`DecisionTree`模型）之后，我们使用`Classify`方法在`fold_valid_input`对象中找到的折测试输入上对它们进行了评估。分类结果被放置在`predictions`对象中。使用`join_cols`函数将所有三个分类结果堆叠到新的`meta_feature`矩阵对象中。因此，我们用新的元特征替换了原始数据集的特征。这个`meta_feature`对象使用`join_rows`方法添加到`meta_train_inputs`对象中。位于`fold_valid_labels`中的折测试标签使用`join_rows`函数添加到元数据集中。
- en: 'After the meta-dataset was created, we used the `DecisionTree` instance to
    train the meta-model as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建元数据集之后，我们使用`DecisionTree`实例按以下方式训练元模型：
- en: '[PRE20]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To be able to use this meta-model, we have to create a weak model again. It
    will be used to generate meta-input features for this meta-model. It can be done
    as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够使用这个元模型，我们必须再次创建一个弱模型。它将被用来为这个元模型生成元输入特征。可以按照以下方式完成：
- en: '[PRE21]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, we used the whole training dataset for training each of the weak models.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了整个训练数据集来训练每个弱模型。
- en: 'Having trained the ensemble, we can evaluate it on the test dataset. Since
    we used data preprocessing, we should also transform our test data in the same
    way as we transformed our training data. We scale the testing data as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练了集成之后，我们可以在测试数据集上对其进行评估。由于我们使用了数据预处理，我们也应该以与转换训练数据相同的方式转换我们的测试数据。我们按以下方式缩放测试数据：
- en: '[PRE22]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The ensemble evaluation starts by predicting meta-features, using the weak
    models we trained before. We will store predictions from every weak model in the
    following objects:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 集成评估从使用我们之前训练的弱模型预测元特征开始。我们将每个弱模型的预测存储在以下对象中：
- en: '[PRE23]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Here, we stacked predictions from each of the weak models into the `meta_eval_inputs`
    object as we did for the meta-dataset creation.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们像创建元数据集时那样，将每个弱模型的预测堆叠到`meta_eval_inputs`对象中。
- en: 'After we have created the meta-features, we can pass them as input to the `Classify`
    method of the `meta_model` object to generate the real predictions. We can also
    calculate the accuracy, like this:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创建了元特征之后，我们可以将它们作为输入传递给`meta_model`对象的`Classify`方法以生成真实预测。我们还可以计算准确度，如下所示：
- en: '[PRE24]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The output of this code is `Stacking ensemble accuracy = 0.985507`. You can
    see that this ensemble performs better than the random forest implementation,
    even with default settings. In the case of some additional tuning, it could give
    even better results.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码的输出是`Stacking ensemble accuracy = 0.985507`。你可以看到，即使使用默认设置，这个集成也比随机森林实现表现得更好。在进一步调整的情况下，它可能会给出更好的结果。
- en: Summary
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we examined various methods for constructing ensembles of machine
    learning algorithms. The main purposes of creating ensembles are to reduce the
    error of the elementary algorithms, expand the set of possible hypotheses, and
    increase the probability of reaching the global optimum during optimization.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了构建机器学习算法集成的各种方法。创建集成的目的是减少基本算法的错误，扩展可能假设的集合，并在优化过程中增加达到全局最优的概率。
- en: 'We saw that there are three main approaches to building ensembles: training
    elementary algorithms on various datasets and averaging the errors (bagging),
    consistently improving the results of the previous, weaker algorithms (boosting),
    and learning the meta-algorithm from the results of elementary algorithms (stacking).
    Note that the methods of building ensembles that we’ve covered, except stacking,
    require that the elementary algorithms belong to the same class, and this is one
    of the main requirements for ensembles. It is also believed that boosting gives
    more accurate results than bagging but, at the same time, is more prone to overfitting.
    The main disadvantage of stacking is that it begins to significantly improve the
    results of elementary algorithms only with a relatively large number of training
    samples.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，构建集成的方法主要有三种：在各个数据集上训练基本算法并平均误差（袋装），持续改进先前较弱算法的结果（提升），以及从基本算法的结果中学习元算法（堆叠）。请注意，我们已涵盖的构建集成的方法，除了堆叠外，都需要基本算法属于同一类别，这是集成的主要要求之一。人们还认为，提升比袋装给出更准确的结果，但同时也更容易过拟合。堆叠的主要缺点是它只有在相对较大的训练样本数量下才开始显著提高基本算法的结果。
- en: In the next chapter, we will discuss the fundamentals of **artificial neural
    networks** (**ANNs**). We’ll look at the historical aspect of their creation,
    go through the basic mathematical concepts used in ANNs, implement a **multilayer
    perceptron** (**MLP**) network and a simple **convolutional neural network** (**CNN**),
    and discuss what deep learning is and why it is so trendy.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论**人工神经网络**（**ANNs**）的基础知识。我们将探讨其创建的历史背景，介绍ANN中使用的基本数学概念，实现一个**多层感知器**（**MLP**）网络和一个简单的**卷积神经网络**（**CNN**），并讨论深度学习是什么以及为什么它如此流行。
- en: Further reading
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Ensemble methods: Bagging &* *Boosting*: [https://medium.com/@sainikhilesh/difference-between-bagging-and-boosting-f996253acd22](mailto:https://medium.com/@sainikhilesh/difference-between-bagging-and-boosting-f996253acd22)'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*集成方法：袋装与提升*：[https://medium.com/@sainikhilesh/difference-between-bagging-and-boosting-f996253acd22](mailto:https://medium.com/@sainikhilesh/difference-between-bagging-and-boosting-f996253acd22)'
- en: '*How to explain gradient* *boosting*: [https://explained.ai/gradient-boosting/](https://explained.ai/gradient-boosting/)'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何解释梯度提升*：[https://explained.ai/gradient-boosting/](https://explained.ai/gradient-boosting/)'
- en: 'Original article by Jerome Friedman called *Greedy Function Approximation:
    A Gradient Boosting* *Machine*: [https://jerryfriedman.su.domains/ftp/trebst.pdf](https://jerryfriedman.su.domains/ftp/trebst.pdf)'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原文由杰罗姆·弗里德曼撰写，题为*贪婪函数逼近：梯度提升机*：[https://jerryfriedman.su.domains/ftp/trebst.pdf](https://jerryfriedman.su.domains/ftp/trebst.pdf)
- en: 'Ensemble Learning to Improve Machine Learning Results: [https://www.kdnuggets.com/2017/09/ensemble-learning-improve-machine-learning-results.html](https://www.kdnuggets.com/2017/09/ensemble-learning-improve-machine-learning-results.html)'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成学习方法提高机器学习结果：[https://www.kdnuggets.com/2017/09/ensemble-learning-improve-machine-learning-results.html](https://www.kdnuggets.com/2017/09/ensemble-learning-improve-machine-learning-results.html)
- en: 'Introduction to decision trees: [https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/](https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/)'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树简介：[https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/](https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/)
- en: 'How to visualize decision trees: [https://explained.ai/decision-tree-viz/](https://explained.ai/decision-tree-viz/)'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何可视化决策树：[https://explained.ai/decision-tree-viz/](https://explained.ai/decision-tree-viz/)
- en: '*Understanding Random* *Forest*: [https://towardsdatascience.com/understanding-random-forest-58381e0602d2](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解随机森林*：[https://towardsdatascience.com/understanding-random-forest-58381e0602d2](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)'
- en: 'Part 3: Advanced Examples'
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3部分：高级示例
- en: In this part, we’ll describe what neural networks are and how they can be applied
    to solving image classification tasks. We’ll also describe what modern **large
    language models** (**LLMs**) are and how they assist in solving neural processing
    tasks such as sentiment analysis.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分中，我们将描述神经网络是什么以及它们如何应用于解决图像分类任务。我们还将描述现代**大型语言模型**（**LLMs**）是什么以及它们如何帮助解决诸如情感分析之类的神经网络处理任务。
- en: 'This part comprises the following chapters:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包括以下章节：
- en: '[*Chapter 10*](B19849_10.xhtml#_idTextAnchor539), *Neural Networks for Image
    Classification*'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B19849_10.xhtml#_idTextAnchor539), *用于图像分类的神经网络*'
- en: '[*Chapter 11*](B19849_11.xhtml#_idTextAnchor642), *Sentiment Analysis with
    BERT and Transfer Learning*'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B19849_11.xhtml#_idTextAnchor642)，*使用BERT和迁移学习进行情感分析*'
