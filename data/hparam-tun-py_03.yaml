- en: '*Chapter 2:* Introducing Hyperparameter Tuning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第二章：介绍超参数调整*'
- en: Every **machine learning** (**ML**) project should have a clear goal and success
    metrics. The success metrics can be in the form of business and/or technical metrics.
    Evaluating business metrics is hard, and often, they can only be evaluated after
    the ML model is in production. On the other hand, evaluating technical metrics
    is more straightforward and can be done during the development phase. We, as ML
    developers, want to achieve *the best technical metrics that we can get* since
    this is something that we can optimize.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 每个机器学习（**ML**）项目都应该有一个明确的目标和成功指标。成功指标可以是商业和/或技术指标。评估商业指标很困难，通常只能在ML模型投入生产后才能进行评估。另一方面，评估技术指标更为直接，可以在开发阶段进行。作为ML开发者，我们希望实现*我们能得到的最佳技术指标*，因为这是我们能够优化的。
- en: In this chapter, we'll learn one out of several ways to optimize the chosen
    technical metrics, called **hyperparameter tuning**. We will start this chapter
    by understanding what hyperparameter tuning is, along with its goal. Then, we'll
    discuss the difference between a hyperparameter and a parameter. We'll also learn
    the concept of hyperparameter space and possible distributions of hyperparameter
    values that you may find in practice.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习优化所选技术指标（称为**超参数调整**）的几种方法之一。我们将从理解超参数调整是什么以及其目标开始本章。然后，我们将讨论超参数与参数之间的区别。我们还将学习超参数空间以及你可能在实践中遇到的超参数值的可能分布。
- en: By the end of this chapter, you will understand the concept of hyperparameter
    tuning and hyperparameters themselves. Understanding these concepts is crucial
    for you to get a bigger picture of what will be discussed in the next chapters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将理解超参数调整的概念以及超参数本身。理解这些概念对于你获得对下一章将要讨论的内容的更全面了解至关重要。
- en: 'In this chapter, we''ll be covering the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: What is hyperparameter tuning?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是超参数调整？
- en: Demystifying hyperparameters versus parameters
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 揭秘超参数与参数的区别
- en: Understanding hyperparameter space and distributions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解超参数空间和分布
- en: What is hyperparameter tuning?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是超参数调整？
- en: '**Hyperparameter tuning** is a process whereby we search for the best set of
    hyperparameters of an ML model from all of the candidate sets. It is the process
    of optimizing the technical metrics we care about. The goal of hyperparameter
    tuning is simply to get the *maximum evaluation score* on the validation set *without
    causing an overfitting issue*.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**超参数调整**是一个过程，通过这个过程，我们从所有候选集中搜索ML模型的最佳超参数集。它是优化我们关心的技术指标的过程。超参数调整的目标仅仅是确保在验证集上获得*最大的评估分数*，而不引起过拟合问题。'
- en: Hyperparameter tuning is one of the *model-centric* approaches to optimizing
    a model's performance. In practice, it is suggested to *prioritize data-centric*
    approaches over a model-centric approach when it comes to optimizing a model's
    performance. Data-centric means that we are focusing on cleaning, sampling, augmenting,
    or modifying the data, while model-centric means that we are focusing on the model
    and its configuration.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整是优化模型性能的*以模型为中心*方法之一。在实践中，当涉及到优化模型性能时，建议优先考虑以数据为中心的方法而不是以模型为中心的方法。以数据为中心意味着我们专注于清理、采样、增强或修改数据，而以模型为中心意味着我们专注于模型及其配置。
- en: To understand why data-centric is prioritized over model-centric, let's say
    you are a cook in a restaurant. When it comes to cooking, no matter how expensive
    and fancy your kitchen setups are, if the ingredients are not in a good condition,
    it's impossible to serve high-quality food to your customers. In that analogy,
    ingredients refer to the data, and kitchen setups refer to the model and its configuration.
    No matter how fancy and complex our model is, if we do not have good data or features
    in the first place, then we can't achieve the maximum evaluation score. This is
    expressed in the famous saying, **garbage in, garbage out** (**GIGO**).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么以数据为中心比以模型为中心更重要，让我们假设你是一家餐厅的厨师。当涉及到烹饪时，无论你的厨房设备多么昂贵和复杂，如果原料状况不佳，你就无法为顾客提供高质量的食品。在这个类比中，原料指的是数据，而厨房设备指的是模型及其配置。无论我们的模型多么复杂和花哨，如果我们最初没有良好的数据或特征，我们就无法实现最大的评估分数。这可以用著名的说法来表达，**垃圾输入，垃圾输出**（**GIGO**）。
- en: In model-centric approaches, hyperparameter tuning is performed after we have
    found the most suitable model framework or architecture. So, it can be said that
    *hyperparameter tuning is the ultimate step* in optimizing the model's performance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在以模型为中心的方法中，我们在找到最合适的模型框架或架构之后进行超参数调整。因此，可以说*超参数调整是优化模型性能的最终步骤*。
- en: Now that you are aware of hyperparameter tuning and its purpose, let's discuss
    hyperparameters themselves What actually is a hyperparameter? What is the difference
    between hyperparameters and parameters? We will discuss this in the next section.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经了解了超参数调整及其目的，让我们来讨论超参数本身。究竟什么是超参数？超参数和参数之间的区别是什么？我们将在下一节中讨论这个问题。
- en: Demystifying hyperparameters versus parameters
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭秘超参数与参数的区别
- en: The *key difference* between a hyperparameter and a parameter is how its value
    is generated. A **parameter** value is generated by the model during the model-training
    phase. In other words, its value is *learned from the given data* instead of given
    by the developer. On the other hand, a **hyperparameter** value is *given by the
    developer* since it can't be estimated from the data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数和参数之间的*关键区别*在于其值的生成方式。**参数**的值是在模型训练阶段由模型生成的。换句话说，其值是从给定的数据中*学习得到的*，而不是由开发者提供的。另一方面，**超参数**的值是由开发者*提供的*，因为它不能从数据中估计出来。
- en: 'Parameters are like the heart of the model. Poorly estimated parameters will
    result in a poorly performing model. In fact, when we said we are training a model,
    it actually means that we are providing the data to the model so that the model
    can estimate the value of its parameters, which is usually done by performing
    some kind of optimization algorithm. Here are several examples of parameters in
    ML:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 参数就像是模型的“心脏”。参数估计不准确会导致模型表现不佳。实际上，当我们说我们在训练一个模型时，实际上意味着我们在向模型提供数据，以便模型可以估计其参数的值，这通常是通过执行某种优化算法来完成的。以下是机器学习（ML）中参数的几个例子：
- en: Coefficients (![](img/Formula_B18753_02_001.png)) in linear regression
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归中的系数（![](img/Formula_B18753_02_001.png)）
- en: Weights (![](img/Formula_B18753_02_002.png)) in a **multilayer perceptron**
    (**MLP**)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层感知器（**MLP**）中的权重（![](img/Formula_B18753_02_002.png)）
- en: 'Hyperparameters, on the other hand, are a set of values that support the model-training
    process. They are defined by the developer without knowing the exact impact on
    the model''s performance. That''s why we need to perform hyperparameter tuning
    to get the best out of our model. The searching process can be done via exhaustive
    search, heuristic search, Bayesian optimization, or multi-fidelity optimization,
    which will be discussed in the following chapters. Here are several examples of
    hyperparameters:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，超参数是一组支持模型训练过程的值。它们由开发者定义，而不了解它们对模型性能的确切影响。这就是为什么我们需要进行超参数调整，以从我们的模型中获得最佳效果。搜索过程可以通过穷举搜索、启发式搜索、贝叶斯优化或多保真优化来完成，这些将在接下来的章节中讨论。以下是超参数的几个例子：
- en: Dropout rate, number of epochs, and batch size in a **neural network** (**NN**)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络（**NN**）中的dropout率、epoch数和批量大小
- en: Maximum depth and splitting criterion in a decision tree
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树中的最大深度和分割标准
- en: Number of estimators in a random forest
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林中的估计器数量
- en: You also need to be aware that there are models without hyperparameters or parameters,
    but not both of them. For instance, a linear regression model is a model that
    has only parameters but doesn't have any hyperparameters. On the other hand, **K-Nearest
    Neighbors** (**KNN**) is an instance of a model that doesn't contain any parameters
    but has a *k* hyperparameter.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要意识到，有些模型既没有超参数也没有参数，但两者都不具备。例如，线性回归模型是一个只有参数但没有超参数的模型。另一方面，**K-最近邻**（**KNN**）是一个不包含任何参数但有一个*k*超参数的模型实例。
- en: More possible confusion may appear when we start writing our code and developing
    the ML model. In programming, arguments in a particular function or class are
    also often called parameters. What if we utilize a class that implements an ML
    model, such as a decision-tree model? What should we call the maximum depth or
    splitting criterion arguments that need to be passed to the class? Are they parameters
    or hyperparameters? The correct answer is *both*! They are *parameters to the
    class* and they are *hyperparameters to the decision-tree model*. It's just a
    matter of perspective!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始编写代码并开发机器学习模型时，可能会出现更多的混淆。在编程中，特定函数或类中的参数也经常被称为参数。如果我们使用实现机器学习模型的类，比如决策树模型，我们应该如何称呼需要传递给类的最大深度或分裂标准参数？它们是参数还是超参数？正确的答案是*两者都是*！它们是*类的参数*，同时也是*决策树模型的超参数*。这只是一个视角问题！
- en: In this section, we have learned what hyperparameters and parameters are, as
    well as what makes them different. In the next section, we will dive deeper into
    the realm of hyperparameters.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了超参数和参数的概念，以及它们之间的区别。在下一节中，我们将更深入地探讨超参数的领域。
- en: "Understanding hyperparameter space \Land distributions"
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解超参数空间和分布
- en: '**Hyperparameter space** is defined as the universal set of possible hyperparameter
    value combinations—in other words, it is the space containing all possible hyperparameter
    values that will be used as the search space during the hyperparameter-tuning
    phase. That''s why it is also often called the hyperparameter-tuning **search
    space**. This space is *predefined* before the hyperparameter-tuning phase so
    that the search will be performed only on this space.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**超参数空间**被定义为所有可能的超参数值组合的集合——换句话说，它是包含所有可能用作超参数调整阶段搜索空间的超参数值的集合。这就是为什么它也经常被称为超参数调整的**搜索空间**。这个空间在超参数调整阶段之前是**预定义的**，以便搜索只在这个空间内进行。'
- en: For example, we want to perform hyperparameter tuning on a NN. Let's say we
    want to search what is the best value for the dropout rate, the number of epochs,
    and batch-size hyperparameters.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们想在神经网络上执行超参数调整。假设我们想搜索dropout率、epoch数和批大小超参数的最佳值。
- en: The dropout rate is bounded in nature. Its value can only be between `0` and
    `1`, while for the number of epochs and batch-size hyperparameters, in theory,
    we can specify any positive integer value. However, there are other considerations
    that we need to think of. A higher batch size will usually produce a better model
    performance, but it will be bounded by the memory size of our physical computer.
    As for the number of epochs, if we go with too high a value, we will more likely
    be faced with an overfitting issue. That's why we need to *set boundaries* for
    the values of possible hyperparameters, which we call the hyperparameter space.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: dropout率在本质上是有界的。它的值只能在`0`和`1`之间，而对于epoch数和批大小超参数，从理论上讲，我们可以指定任何正整数值。然而，我们还需要考虑其他因素。通常，较大的批大小会产生更好的模型性能，但它将受到我们物理计算机内存大小的限制。至于epoch数，如果我们选择过高的值，我们更有可能遇到过拟合问题。这就是为什么我们需要为可能超参数的值设置边界，我们称之为超参数空间。
- en: Hyperparameters can be in the form of *discrete or continuous* values. A discrete
    hyperparameter can be in the form of integer or string data types, while a continuous
    hyperparameter will always be in the form of real numbers or floating data types.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数可以是*离散或连续*值的形式。离散超参数可以是整数或字符串数据类型，而连续超参数将始终是实数或浮点数据类型。
- en: When defining a hyperparameter space, for *some hyperparameter-tuning methods*,
    it is not enough to only specify the possible values of each hyperparameter we
    care about. We also need to define what is the underlying **distribution** for
    each hyperparameter. Here, a distribution acts as some kind of *policy* that rules
    how likely it is that a specific value will be tested during the hyperparameter-tuning
    phase. If it is a uniform distribution, then all possible values have the same
    probability of being chosen.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义超参数空间时，对于**某些超参数调整方法**，仅指定我们关心的每个超参数的可能值是不够的。我们还需要定义每个超参数的潜在**分布**。在这里，分布充当某种*策略*，决定了在超参数调整阶段测试特定值的可能性。如果是一个均匀分布，那么所有可能的值都有相同的选择概率。
- en: 'There are many types of probability distributions that can be used: uniform,
    log-uniform, normal, log-normal, and many more. There is no best practice when
    it comes to choosing the appropriate distribution; you can just treat it as another
    hyperparameter. It is worth noting that there are distributions specifically for
    continuous hyperparameters, and there are also distributions for discrete ones.
    For discrete hyperparameter distribution, some distributions are specifically
    designed for discrete values—for instance, an integer uniform distribution—but
    there are also distributions that are adjusted from a continuous distribution.
    The latter types of discrete distributions usually have a *discretized* or *quantized*
    prefix on their name—for instance, a quantized uniform distribution.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用的概率分布类型有很多：均匀分布、对数均匀分布、正态分布、对数正态分布等等。在选择合适的分布时没有最佳实践；您只需将其视为另一个超参数。值得注意的是，有一些分布专门用于连续超参数，也有一些用于离散超参数。对于离散超参数分布，有些分布专门设计用于离散值——例如，整数均匀分布——但也有从连续分布调整而来的分布。后一种类型的离散分布通常在其名称上带有*离散化*或*量化*前缀——例如，量化均匀分布。
- en: It is also worth noting that *not all hyperparameters are equally significant*
    when it comes to impacting the model's performance—that's why it is recommended
    that you prioritize. We do not have to perform hyperparameter tuning on all of
    the hyperparameters of a model—just *focus on more important hyperparameters*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得注意的是，并非所有超参数对模型性能的影响都是同等重要的——这就是为什么建议您优先考虑。我们不必对模型的所有超参数进行超参数调优——只需*关注更重要的超参数*。
- en: In this section, we have learned about hyperparameter space and the concept
    of a hyperparameter distribution and looked at examples of hyperparameter distributions
    you may find in practice.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了超参数空间以及超参数分布的概念，并探讨了实践中可能遇到的超参数分布的例子。
- en: Summary
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have learned all we need to know about hyperparameter tuning,
    starting from what it is, what is its goal, and when we should perform hyperparameter
    tuning. We have also discussed the difference between hyperparameters and parameters,
    the concept of hyperparameter space, and the concept of hyperparameter distributions.
    Having a clear picture of the concept of hyperparameter tuning and hyperparameters
    themselves will help you a lot in the following chapters.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了关于超参数调优所需了解的所有内容，从它是什么、其目标是什么以及何时应该进行超参数调优开始。我们还讨论了超参数与参数之间的区别、超参数空间的概念以及超参数分布的概念。对超参数调优的概念和超参数本身有一个清晰的了解将极大地帮助您在接下来的章节中。
- en: As stated previously, we will discuss all of the four categories of hyperparameter-tuning
    methods in this book. In [*Chapter 3*](B18753_03_ePub.xhtml#_idTextAnchor031),
    *Exploring Exhaustive Search*, we will start discussing the first group and the
    most widely used hyperparameter-tuning methods in practice. There will be both
    high-level and detailed explanations to help you understand each of the methods
    more easily.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，本书将讨论所有四种超参数调优方法的类别。在[*第3章*](B18753_03_ePub.xhtml#_idTextAnchor031)《探索穷举搜索》中，我们将开始讨论第一组，也是实践中最广泛使用的超参数调优方法。这里将提供高级和详细的解释，以帮助您更容易地理解每种方法。
