- en: Predicting User Behavior with Tree-Based Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于树的算法预测用户行为
- en: This chapter will introduce decision trees, random forests, and gradient-boosted
    trees. Decision trees methodology is a popular technique used in data science
    that provides a visual representation of how the information in the training set
    can be represented as a hierarchy. Traversing the hierarchy based on an observation
    helps you to predict the probability of that event.  We will explore how to use
    these algorithms can be used to predict when a user may click on online advertisement
    based on existing advertising click records. Additionally, we will show how to
    use AWS **Elastic MapReduce** (**EMR**) with Apache Spark and the SageMaker XGBoost
    service to engineer models in the context of big data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍决策树、随机森林和梯度提升树。决策树方法是一种在数据科学中广泛使用的流行技术，它以可视化的方式展示了训练集中的信息如何表示为一个层次结构。根据观察结果遍历层次结构可以帮助你预测该事件发生的概率。我们将探讨如何使用这些算法来预测用户可能会点击在线广告的时间，基于现有的广告点击记录。此外，我们还将展示如何使用AWS
    **弹性映射减少**（**EMR**）与Apache Spark以及SageMaker XGBoost服务在大数据环境中构建模型。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding decision trees
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解决策树
- en: Understanding random forests algorithms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解随机森林算法
- en: Understanding gradient boosting algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解梯度提升算法
- en: Predicting clicks on log streams
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测日志流中的点击
- en: Understanding decision trees
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解决策树
- en: Decision trees graphically show the decisions to be made, the observed events
    that may occur, and the probabilities of the outcomes given a specific set of
    observable events occurring together. Decision trees are used as a popular machine
    learning algorithm, where, based on a dataset of observable events and the known
    outcomes, we can construct a decision tree that can represent the probability
    of an event occurring.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树图形化地展示了将要做出的决策、可能发生的观察事件以及给定一组特定的可观察事件同时发生时的结果概率。决策树作为一种流行的机器学习算法，基于一组可观察事件的数据集和已知的输出结果，我们可以构建一个决策树来表示事件发生的概率。
- en: 'The following table shows a very simple example of how decision trees can be
    generated:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了决策树如何生成的一个非常简单的例子：
- en: '| **Car Make** | **Year** | **Price** |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| **汽车品牌** | **年份** | **价格** |'
- en: '| BMW | 2015 | >$40K |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 宝马 | 2015 | >$40K |'
- en: '| BMW | 2018 | >$40K |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 宝马 | 2018 | >$40K |'
- en: '| Honda  | 2015 | <$40K |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 本田  | 2015 | <$40K |'
- en: '| Honda  | 2018 | >$40K |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 本田  | 2018 | >$40K |'
- en: '| Nissan | 2015 | <$40K |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 尼桑 | 2015 | <$40K |'
- en: '| Nissan | 2018 | >$40K |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 尼桑 | 2018 | >$40K |'
- en: 'This is a very simple dataset that is represented by the following decision
    tree:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的数据集，它由以下决策树表示：
- en: '![](img/d6743663-fd91-403f-9a57-775f6fe90608.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6743663-fd91-403f-9a57-775f6fe90608.png)'
- en: The aim of the machine learning algorithm is to generate decision trees that
    best represent the observations in the dataset. For a new observation, if we traverse
    the decision tree, the leaf nodes represent the class variable or event that is
    most likely to occur. In the preceding example, we have a dataset that has information
    regarding the make and the year of a used car. The class variable (also called
    the **feature label**) is the price of the car. We can observe in the dataset
    that, irrespective of the year variable value, the price of a BMW car is greater
    than $40,000\. However, if the make of the car is not BMW, the cost of the car
    is determined by the year the car was produced. The example is based on a very
    small amount of data. However, the decision tree represents the information in
    the dataset, and if we have to determine the cost of a new car where the make
    is BMW and year is 2015, then we can predict that the cost is greater than $40,000\.
    For more complex decision trees, the leaf nodes also have a probability associated
    with them that represents the probability of the class value occurring. In this
    chapter, we will study algorithms that can be used to generate such decision trees.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法的目标是生成最能代表数据集中观测值的决策树。对于一个新观测值，如果我们遍历决策树，叶节点代表最有可能发生的类变量或事件。在先前的例子中，我们有一个包含关于二手车品牌和年份的信息的数据集。类变量（也称为**特征标签**）是汽车的价格。我们可以在数据集中观察到，无论年份变量的值如何，宝马汽车的价格都超过40,000美元。然而，如果汽车的品牌不是宝马，汽车的成本将由汽车生产的年份决定。这个例子基于非常少量的数据。然而，决策树代表了数据集中的信息，如果我们必须确定品牌为宝马且年份为2015的新车的成本，那么我们可以预测其成本将超过40,000美元。对于更复杂的决策树，叶节点还与一个概率相关联，该概率代表类值发生的概率。在本章中，我们将研究可以用来生成此类决策树的算法。
- en: Recursive splitting
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归分割
- en: Decision trees can be built by recursively splitting the dataset into subsets.
    During each split, we evaluate splits based on all the input attributes and use
    a cost function to determine which split has the lowest cost. The cost functions
    generally evaluate the loss of information when we split the dataset into two
    branches. This partitioning of the dataset into smaller subsets is also referred
    to as recursive partitioning. The cost of splitting the datasets into subsets
    is generally determined by how records with similar class variables are grouped
    together in each dataset. Hence, the most optimal split would be when observations
    in each subset will have the same class variable values.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以通过递归地将数据集划分为子集来构建。在每次分割过程中，我们根据所有输入属性评估分割，并使用成本函数来确定哪个分割的成本最低。成本函数通常评估将数据集分割成两个分支时的信息损失。这种将数据集分割成更小子集的过程也被称为递归分割。分割数据集的成本通常由每个数据集中具有相似类变量的记录如何分组在一起来决定。因此，最优的分割是在每个子集中的观测值将具有相同的类变量值时。
- en: Such recursive splitting of decision trees is a top-down approach in generating
    decision trees. This is also a greedy algorithm since we made the decision at
    each point on how to divide the dataset, without considering how it may affect
    the later splits.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种决策树的递归分割是一种自上而下的生成方法。这同样是一种贪婪算法，因为我们在每个点上做出了如何划分数据集的决定，而没有考虑它可能对后续分割的影响。
- en: In the preceding example, we made the first split based on the make of the car.
    This is because one of our subsets, where the make is BMW, has a 100% probability
    of the price of the car being greater than $40,000\. Similarly, if we had made
    a split based on the year, we would also get a subset of the year equal to 2018
    that also has a 100% probability of the cost of the car is greater than $40,000\.
    Hence, for the same dataset, we can generate multiple decision trees that represent
    the dataset. There are various cost functions that we will look at that generate
    different decision trees based on the same dataset.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的例子中，我们是根据汽车的品牌进行第一次分割的。这是因为我们的一个子集，其中品牌是宝马，有100%的概率汽车的价格将超过40,000美元。同样，如果我们根据年份进行分割，我们也会得到一个年份等于2018的子集，该子集也有100%的概率汽车的成本将超过40,000美元。因此，对于相同的数据集，我们可以生成多个代表数据集的决策树。我们将查看各种成本函数，它们基于相同的数据集生成不同的决策树。
- en: Types of decision trees
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树的类型
- en: 'There are following two main types of decision trees that most data scientists
    have to work with based on the class variables in the dataset:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据集中的类变量，大多数数据科学家必须处理以下两种主要的决策树类型：
- en: '**Classification trees: **Classification trees are decision trees that are
    used to predict discrete values. This means that the class variable of the dataset
    used to generate classification trees is a discrete value. The preceding example
    regarding car prices at the start of this section is a classification tree as
    it only has two values of the class variable.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类树**：分类树是用于预测离散值的决策树。这意味着用于生成分类树的数据库中的类别变量是离散值。本节开头关于汽车价格的例子就是一个分类树，因为它只有两个类别变量的值。'
- en: '**Regression trees: **Regression trees are decision trees that are used to
    predict real numbers, such as the example in C*hapter 3**, Predicting House Value
    with Regression Algorithms*, where we were predicting the price of the house.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归树**：回归树是用于预测实数的决策树，例如在**第 3 章**中的例子，**使用回归算法预测房价**，我们预测的是房价。'
- en: The term **Classification and Regression Trees** (**CART**) is used to describe
    the algorithm for generating decision trees. CART is a popular algorithm for decision
    trees. Other popular decision tree algorithms include ID3 and C4.5\. These algorithms
    are different from each other in terms of the cost functions they use for splitting
    the dataset and the criteria used to determine when to stop splitting.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 术语**分类与回归树**（**CART**）用于描述生成决策树的算法。CART 是一种流行的决策树算法。其他流行的决策树算法包括 ID3 和 C4.5。这些算法在用于分割数据集的成本函数以及确定何时停止分割的准则方面各不相同。
- en: Cost functions
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本函数
- en: As discussed in the section on *Recursive splitting*, we need cost functions
    to determine whether splitting on a given input variable is better than other
    variables. The effectiveness of these cost functions is crucial for the quality
    of the decision trees being built. In this section, we'll discuss two popular
    cost functions for generating a decision tree.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如在*递归分割*章节中所述，我们需要成本函数来确定在给定输入变量上分割是否比其他变量更好。这些成本函数的有效性对于构建决策树的质量至关重要。在本节中，我们将讨论生成决策树时两种流行的成本函数。
- en: Gini Impurity
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gini 不纯度
- en: 'Gini Impurity is defined as the measurement of the likelihood of incorrect
    classification of a random observation, given the random observation is classified
    based on the distribution of the class variables in the dataset. Consider a dataset
    with  ![](img/1cb79df3-f4d0-4635-a7eb-a36ca0e0d4e7.png) class variables, and  ![](img/2cfcf87b-3030-4cc6-ba1c-c60a62ddbc18.png) is
    the fraction of observations in the dataset labeled as ![](img/f7eea272-393c-47d6-9b01-81233bc5a30d.png).
    *Gini Impurity* can be calculated using the following formula:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Gini 不纯度定义为在随机观测值被基于数据集中类别变量的分布进行分类的情况下，对随机观测值进行错误分类的可能性的度量。考虑一个包含  ![图片](img/1cb79df3-f4d0-4635-a7eb-a36ca0e0d4e7.png)
    类别变量的数据集，其中  ![图片](img/2cfcf87b-3030-4cc6-ba1c-c60a62ddbc18.png) 是数据集中标记为  ![图片](img/f7eea272-393c-47d6-9b01-81233bc5a30d.png)
    的观测值的比例。*Gini 不纯度*可以使用以下公式计算：
- en: '![](img/55adff7c-2b06-42b2-8fb0-ad42229aa761.png)      .. 4.1'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/55adff7c-2b06-42b2-8fb0-ad42229aa761.png)  .. 4.1'
- en: Gini Impurity tells us the amount of noise present in the dataset, based on
    the distributions of various class variables.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Gini 不纯度告诉我们数据集中存在的噪声量，这是基于各种类别变量的分布情况。
- en: 'For example, in the car price dataset presented at the start of *Understanding
    Decision Trees* section, we have two class variables: greater than 40,000 and
    less than 40,000\. If we had to calculate the Gini Impurity of the dataset, it
    could be calculated as shown in the following formula:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在*理解决策树*部分开头展示的汽车价格数据集中，我们有两个类别变量：大于 40,000 和小于 40,000。如果我们必须计算数据集的 Gini
    不纯度，它可以按照以下公式计算：
- en: '![](img/31bfe354-0fab-4186-adb4-18ee95eaf77f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/31bfe354-0fab-4186-adb4-18ee95eaf77f.png)'
- en: Hence, there is a lot of noise in the base dataset since each class variable
    has 50% of the observations.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基础数据集中存在大量的噪声，因为每个类别变量都有 50% 的观测值。
- en: 'However, when we create a branch where the make of the car, the Gini Impurity
    of that subset of the dataset is calculated as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们创建一个以汽车品牌为分支时，该数据集子集的 Gini 不纯度计算如下：
- en: '![](img/a3bab794-0e25-46aa-a736-3190545ed1fa.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a3bab794-0e25-46aa-a736-3190545ed1fa.png)'
- en: '![](img/658abb2a-deee-4d28-ab16-80ec3f2aba2c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/658abb2a-deee-4d28-ab16-80ec3f2aba2c.png)'
- en: '![](img/0ce42a6c-fe7d-4249-9883-a8d70c2ec65f.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0ce42a6c-fe7d-4249-9883-a8d70c2ec65f.png)'
- en: Since the branch of for BMW only contains class values of *>40K*, there is no
    noise in the branch and the value of Gini Impurity is *0*. Note that when the
    subset of the data only has one class value, the Gini Impurity value is always
    *0*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于宝马分支只包含*>40K*的类值，因此该分支没有噪声，基尼不纯度的值为*0*。请注意，当数据子集只有一个类值时，基尼不纯度的值总是*0*。
- en: 'Gini Impurity is used to calculate the Gini Index for each attribute. The Gini
    Index is a weighted sum of all the values of an attribute on which we create branches.
    For an attribute, ![](img/f6e07e55-9112-4986-8e76-7b75afbfd099.png), that has
    ![](img/ff667824-2211-4440-a6bd-fba1eea27034.png) unique values, Gini Gain is
    calculated using formula below. ![](img/5e9ca5cf-4d11-4848-990b-c1b3ac2e3961.png) is the
    fraction of observations in the dataset where the value of the attribute, ![](img/ea9db92c-195e-4477-be7f-68149a481142.png),
    is ![](img/98d43c54-cf8a-464d-9376-503ccb926ee0.png):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼不纯度用于计算每个属性的基尼指数。基尼指数是我们创建分支的属性所有值的加权总和。对于具有![图片](img/f6e07e55-9112-4986-8e76-7b75afbfd099.png)个唯一值的属性，基尼增益是使用以下公式计算的。![图片](img/5e9ca5cf-4d11-4848-990b-c1b3ac2e3961.png)是数据集中属性值![图片](img/ea9db92c-195e-4477-be7f-68149a481142.png)为![图片](img/98d43c54-cf8a-464d-9376-503ccb926ee0.png)的观察值的比例：
- en: '![](img/43d86f3b-a08a-4d4e-b190-a83c7424690d.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/43d86f3b-a08a-4d4e-b190-a83c7424690d.png)'
- en: 'Hence, in our preceding example, the Gini Index for the *Make* attribute that
    has three distinct values is calculated as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们前面的例子中，对于具有三个不同值的*Make*属性，基尼指数的计算如下：
- en: '![](img/54b85ded-6b89-4176-b5ee-b331e651c0e6.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/54b85ded-6b89-4176-b5ee-b331e651c0e6.png)'
- en: Similarly, we calculate the Gini Index for other attributes. In our example,
    the Gini Index for the *Year* attribute is 0.4422\. We encourage you to calculate
    this value on your own. Our aim is to pick the attribute that generates the lowest
    Gini Index score. For a perfect classification, where all the class values in
    each branch are the same, the Gini Index score will be 0.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们计算其他属性的基尼指数。在我们的例子中，*Year*属性的基尼指数为0.4422。我们鼓励您自己计算这个值。我们的目标是选择产生最低基尼指数得分的属性。对于完美的分类，即每个分支中的所有类值都相同，基尼指数得分将为0。
- en: Information gain
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信息增益
- en: Information gain is based on the concept of entropy, which is commonly used
    in physics to represent the unpredictability in a random variable. For example,
    if we have an unbiased coin, the entropy of the coin is represented as *1*, as
    it has the highest unpredictability. However, if a coin is biased, and has a 100%
    chance of heads, the entropy of the coin is 0.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益基于熵的概念，这在物理学中常用来表示随机变量的不可预测性。例如，如果我们有一个无偏的硬币，硬币的熵表示为*1*，因为它具有最高的不可预测性。然而，如果一个硬币是偏的，并且有100%的机会出现正面，那么硬币的熵为0。
- en: 'This concept of entropy can also be used to determine the unpredictability
    of the class variable in a given branch. The entropy, denoted as *H*, of a branch
    is calculated using the formula below. ![](img/faf67a04-8f90-4c08-b99d-a1ae95804c69.png)
    represents the entropy of the attribute. ![](img/a03e73dd-2003-4c47-a55a-8b355a5ded39.png)
    is the number of class variables in the dataset. ![](img/5a31aed3-e450-45d4-9618-e8721da55b32.png)
    is the fraction of observations in the dataset that belong to the class, ![](img/fbd47654-3821-45aa-89dd-d73f69e93690.png):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 熵的概念也可以用来确定给定分支中类变量的不可预测性。分支的熵，用*H*表示，是根据以下公式计算的。![图片](img/faf67a04-8f90-4c08-b99d-a1ae95804c69.png)代表属性的熵。![图片](img/a03e73dd-2003-4c47-a55a-8b355a5ded39.png)是数据集中类变量的数量。![图片](img/5a31aed3-e450-45d4-9618-e8721da55b32.png)是数据集中属于该类的观察值的比例，![图片](img/fbd47654-3821-45aa-89dd-d73f69e93690.png)：
- en: '![](img/5d65e3d7-daba-4b80-b152-dade6bf59135.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5d65e3d7-daba-4b80-b152-dade6bf59135.png)'
- en: '**Step 1**: In our example, for the entire dataset, we can calculate the entropy
    of the dataset as follows.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1**：在我们的例子中，对于整个数据集，我们可以按照以下方式计算数据集的熵。'
- en: '![](img/01cf8bda-1f09-470f-aa69-9846c0257f32.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/01cf8bda-1f09-470f-aa69-9846c0257f32.png)'
- en: '**Step 2**: In our decision tree, we split the tree based on the make of the
    car. Hence, we also calculate the entropy of each branch of the tree, as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2**：在我们的决策树中，我们根据汽车的型号来分割树。因此，我们也计算了树的每个分支的熵，如下所示：'
- en: '![](img/db694e43-90fd-41a6-b5c3-1843aa950295.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/db694e43-90fd-41a6-b5c3-1843aa950295.png)'
- en: '![](img/dbc40d20-a5fb-47a0-bf2b-68bde5a06fbd.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dbc40d20-a5fb-47a0-bf2b-68bde5a06fbd.png)'
- en: '![](img/b9e63287-3d3e-41bc-be71-2384ef33fdc2.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b9e63287-3d3e-41bc-be71-2384ef33fdc2.png)'
- en: '**Step 3**: Based on the entropy of the parent and the branches, we can evaluate
    the branch using a measure called **information gain**. For a parent branch, ![](img/16b7bb2a-eac9-4d30-8422-29ecf3315a9c.png),
    and attribute, ![](img/88e885c5-21d3-47e0-a455-15404f53baf9.png), information
    gain is represented as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3**：基于父节点和分支的熵，我们可以使用称为**信息增益**的度量来评估分支。对于父分支![图片](img/16b7bb2a-eac9-4d30-8422-29ecf3315a9c.png)和属性![图片](img/88e885c5-21d3-47e0-a455-15404f53baf9.png)，信息增益表示如下：'
- en: '![](img/bdea57aa-19c4-4d55-b646-4844a3415ba5.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bdea57aa-19c4-4d55-b646-4844a3415ba5.png)'
- en: '![](img/873963f4-165f-49bf-9406-1ab0817f9ce8.png) is the weighted sum of the
    entropy of the children. In our example, ![](img/b1162a12-fc43-4cda-ad99-31d329d8df34.png) of
    the *Make* attribute is calculated as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/873963f4-165f-49bf-9406-1ab0817f9ce8.png)是子节点熵的加权总和。在我们的例子中，*Make*属性的![图片](img/b1162a12-fc43-4cda-ad99-31d329d8df34.png)计算如下：'
- en: '![](img/ef8027cc-d825-411c-8cef-ccdf02baba9e.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ef8027cc-d825-411c-8cef-ccdf02baba9e.png)'
- en: 'Hence, the information gain for the *Make* attribute is calculated as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*Make*属性的信息增益计算如下：
- en: '![](img/85f98981-1681-43e7-b486-1da57abb3f24.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/85f98981-1681-43e7-b486-1da57abb3f24.png)'
- en: Similarly, we can calculate the information gain score for other attributes.
    The attribute with the highest information gain should be used to split the dataset
    for the highest quality of a decision tree. Information gain is used in the ID3
    and C4.5 algorithms.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以计算其他属性的信息增益分数。应该使用信息增益最高的属性来分割数据集，以获得最高质量的决策树。信息增益在ID3和C4.5算法中使用。
- en: Criteria to stop splitting trees
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停止分割树的准则
- en: 'As decision tree generation algorithms are recursive, we need a criterion that
    indicates when to stop splitting the trees. There are various criteria we can
    set to stop splitting the trees. Let us now look at the list of commonly used
    criteria:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于决策树生成算法是递归的，我们需要一个准则来指示何时停止分割树。我们可以设置各种准则来停止分割树。现在让我们看看常用准则的列表：
- en: '**Number of observations in the node**:We can set criteria to stop the recursion
    in a branch if the number of observations is less than a pre-specified amount.
    A good rule of thumb is to stop the recursion when there is fewer than 5% of the
    total training data in a branch. If we over split the data, such that each node
    only has one data point, it leads to overfitting the decision tree to the training
    data. Any new observation that has not been previously seen will not be accurately
    classified in such trees.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点中的观测数**：我们可以设置标准，如果分支中的观测数少于预先指定的数量，则停止分支的递归。一个很好的经验法则是当分支中有少于5%的总训练数据时停止递归。如果我们过度分割数据，以至于每个节点只有一个数据点，这会导致决策树过度拟合训练数据。任何之前未见过的新观察结果将无法在这些树中得到准确分类。'
- en: '**Purity of the node**:In the *Gini Impurity* section, we learned to calculate
    the likelihood of error in classifying a random observation. We can also use the
    same methodology to calculate the purity of the dataset. If the purity of the
    subset in a branch is greater than a pre-specified threshold, we can stop splitting
    based on that branch.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点的纯度**：在*基尼不纯度*部分，我们学习了如何计算对随机观察进行分类的错误概率。我们也可以使用相同的方法来计算数据集的纯度。如果一个分支中的子集纯度大于预先指定的阈值，我们可以停止基于该分支的分割。'
- en: '**The depth of the tree**:We can also pre-specify the limit on the depth of
    the tree. If the depth of any branch exceeds the limit, we can stop splitting
    the branch further.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树的深度**：我们也可以预先指定树的深度限制。如果任何分支的深度超过限制，我们可以停止进一步分割该分支。'
- en: '**Pruning trees**:Another strategy is to let the trees grow fully. This avoids
    the branch splitting being terminated prematurely, without looking ahead. However,
    after the full tree is built, it is likely that the tree is large and there may
    be overfitting in some branches. Hence, pruning strategies are applied to evaluate
    each branch of the tree; any branch that introduces less than the pre-specified
    amount of impurity in the parent branch is eliminated. There are various techniques
    to prune decision trees. We encourage our readers to explore this topic further
    in the libraries that they implement their decision trees in.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修剪树木**：另一种策略是让树木充分生长。这样做可以避免分支过早终止，而不考虑未来。然而，在完全构建了整棵树之后，树可能很大，某些分支可能存在过拟合。因此，应用修剪策略来评估树的每一分支；任何引入的杂质少于预先指定的父分支杂质量的分支将被消除。修剪决策树有多种技术。我们鼓励读者在实现决策树的库中进一步探索这个主题。'
- en: Understanding random forest algorithms
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解随机森林算法
- en: There are two main disadvantages to using decision trees. First, the decision
    trees use algorithms that make a choice to split on an attribute based on a cost
    function. The decision tree algorithm is a greedy algorithm that optimizes toward
    a local optimum when making every decision regarding splitting the dataset into
    two subsets. However, it does not explore whether making a suboptimal decision
    while splitting over an attribute, would lead to a more optimal decision tree
    in the future. Hence, we do not get a globally optimum tree when running this
    algorithm. Second, decision trees tend to overfit to the training data. For example,
    a small sample of observations available in the dataset may lead to a branch that
    provides a very high probability of a certain class event occurring. This leads
    to the decision trees being really good at generating correct predictions for
    the dataset that was used for training. However, for observations that they have
    never seen before, decision trees may not be accurate due to overfitting to the training
    data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策树有两个主要缺点。首先，决策树使用基于成本函数选择在属性上分割的算法。决策树算法是一种贪婪算法，在做出关于将数据集分割成两个子集的每个决策时，都趋向于局部最优。然而，它并没有探索在属性上做出次优决策是否会导致未来有更优的决策树。因此，当我们运行此算法时，我们不会得到全局最优的树。其次，决策树倾向于过度拟合训练数据。例如，数据集中可用的少量观察结果可能导致一个分支，该分支提供了非常高的某个类事件发生的概率。这导致决策树在生成用于训练的数据集的正确预测方面表现得非常好。然而，对于他们以前从未见过的观察结果，由于过度拟合训练数据，决策树可能不准确。
- en: To tackle these issues, the random forest algorithm can be used to improve the
    accuracy of the existing decision tree algorithms. In this approach, we divide
    the training data into random subsets and create a collection of decision trees,
    each based on a subset. This tackles the issue of overfitting, as we no longer
    rely on one tree to make the decision that has overfit to the entire training
    set. Secondly, this also helps with the issue of splitting on only one attribute
    based on a cost function. Different decision trees in random forests may make
    decisions on splitting based on different attributes, based on the random sample
    they are training on.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，可以使用随机森林算法来提高现有决策树算法的准确性。在这种方法中，我们将训练数据划分为随机子集，并创建一个决策树集合，每个决策树基于一个子集。这解决了过拟合的问题，因为我们不再依赖于一棵树来做出对整个训练集过度拟合的决策。其次，这也帮助解决了仅基于成本函数在一个属性上分割的问题。随机森林中的不同决策树可能会基于它们训练的随机样本，基于不同的属性做出分割决策。
- en: During the prediction phase, the random forest algorithm gets a probability
    of an event from each branch and uses a voting methodology to generate a prediction.
    This helps us suppress predictions from trees that may have overfitted or made
    sub-optimal decisions when generating the trees. Such an approach to divide the
    training set into random subsets and train multiple machine learning models is
    known as **Bagging**. The Bagging approach can also be applied to other machine
    learning algorithms.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测阶段，随机森林算法从每个分支获取事件的概率，并使用投票方法生成预测。这有助于我们抑制可能过度拟合或做出次优决策的树的预测。将训练集划分为随机子集并训练多个机器学习模型的方法被称为**Bagging**。Bagging方法也可以应用于其他机器学习算法。
- en: Understanding gradient boosting algorithms
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解梯度提升算法
- en: Gradient boosting algorithms are also used to address the disadvantages of the
    decision tree algorithm. However, unlike the random forests algorithm, which trains
    multiple trees based on random subsets of training data, gradient-boosting algorithms
    train multiple trees sequentially by reducing the errors in the decision trees.
    Gradient boosting decision trees are based on a popular machine learning technique
    called **Adaptive Boosting**, where we learn why a machine learning model is making
    errors, and then train a new machine learning model that reduces the errors from
    the previous models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升算法也被用来解决决策树算法的缺点。然而，与基于训练数据随机子集训练多个树的随机森林算法不同，梯度提升算法通过减少决策树中的错误来顺序地训练多个树。梯度提升决策树基于一种流行的机器学习技术，称为**自适应提升**，其中我们学习为什么机器学习模型会出错，然后训练一个新的机器学习模型来减少先前模型的错误。
- en: Gradient boosting algorithms discover patterns in the data that are difficult
    to represent in the decision trees, and add a greater weight to the training examples,
    which can lead to correct predictions. Thus, similar to random forests, we generate
    multiple decision trees from subsets of the training data. However, during each
    step, the subset of training data is not selected randomly. Instead, we create
    a subset of training data, where the examples that would lead to fewer errors
    in decision trees are prioritized. We stop this process when we cannot observe
    patterns in errors that may lead to more optimizations.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升算法在数据中发现了难以在决策树中表示的规律，并给训练示例添加了更大的权重，这可能导致正确的预测。因此，与随机森林类似，我们从训练数据的一个子集中生成多个决策树。然而，在每一步中，训练数据的子集并不是随机选择的。相反，我们创建一个训练数据子集，其中优先考虑那些会导致决策树中错误更少的示例。当我们无法观察到可能导致更多优化的错误模式时，我们停止这个过程。
- en: Examples of how random forest algorithms and gradient-boosting algorithms are
    implemented are provided in the next section.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分提供了随机森林算法和梯度提升算法实现的示例。
- en: Predicting clicks on log streams
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测日志流中的点击
- en: In this section, we will show you how to use tree-based methods to predict who
    will click on a mobile advertisement given a set of conditions, such as region,
    where the ad is shown, time of day, location of the banner, and the application
    delivering the advertisement.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将向您展示如何使用基于树的方法来预测在给定一系列条件（如地区、广告展示的位置、一天中的时间、横幅的位置以及提供广告的应用程序）的情况下，谁会点击移动广告。
- en: The dataset we will use throughout the rest of the chapter is obtained from
    *Shioji, Enno, 2017, Adform click prediction dataset,* [https://doi.org/10.7910/DVN/TADBY7](https://doi.org/10.7910/DVN/TADBY7)*,
    Harvard Dataverse, V2*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的其余部分使用的数据集来自 *Shioji, Enno, 2017, Adform点击预测数据集*，[https://doi.org/10.7910/DVN/TADBY7](https://doi.org/10.7910/DVN/TADBY7)，哈佛数据集，V2。
- en: The main task is to build a classifier capable of predicting whether a user
    will click on an advertisement given the conditions. Having such a model is very
    useful for ad-tech platforms that select which ads to show to users and when.
    These platforms can use these models to only show ads to users who are likely
    to click on the ad being delivered.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 主要任务是构建一个分类器，能够根据条件预测用户是否会点击广告。拥有这样的模型对于选择向用户展示哪些广告以及何时展示的广告技术平台非常有用。这些平台可以使用这些模型只为可能点击所提供广告的用户展示广告。
- en: The dataset is large enough (5 GB) to justify the use of technologies that span
    multiple machines to perform the training. We will first look at how to use AWS
    EMR to carry out this task with Apache Spark. We will also show how to do this
    with SageMaker services.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集足够大（5 GB），足以证明使用跨多台机器的技术来执行训练是合理的。我们首先将探讨如何使用AWS EMR结合Apache Spark来完成这项任务。我们还将展示如何使用SageMaker服务来完成这项任务。
- en: Introduction to Elastic MapReduce (EMR)
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性MapReduce (EMR) 简介
- en: EMR is an AWS service that allows us to run and scale Apache Spark, Hadoop,
    HBase, Presto, Hive, and other big data frameworks. We will cover more EMR details
    in [Chapter 15](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml), *Tuning Clusters
    for Machine Learning*. However, for now, let's think of EMR as a service that
    allows us to launch several interconnected machines with running software, such
    as Apache Spark, that coordinates distributed processing. EMR clusters have a
    master and several slaves. The master typically orchestrates the jobs, whereas
    the slaves process and combine the data to provide the master with a result. This
    result can range from a simple number (for example, a count of rows) to a machine
    learning model capable of making predictions. The Apache Spark Driver is the machine
    that coordinates the jobs necessary to complete the operation. The driver typically
    runs on the master node but it can also be configured to run on a slave node.
    The Spark executors (the demons that Spark uses to crunch the data) typically
    run on the EMR slaves.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: EMR是AWS服务，允许我们运行和扩展Apache Spark、Hadoop、HBase、Presto、Hive和其他大数据框架。我们将在第15章中详细介绍EMR的更多细节，*调整集群以进行机器学习*。然而，现在让我们将EMR视为一种服务，允许我们启动几个运行软件（如Apache
    Spark）的相互连接的机器，这些软件协调分布式处理。EMR集群有一个主节点和几个从节点。主节点通常协调作业，而从节点处理和合并数据，为主节点提供结果。这个结果可以从一个简单的数字（例如，行数计数）到一个能够进行预测的机器学习模型不等。Apache
    Spark Driver是协调完成操作所需作业的机器。驱动程序通常在主节点上运行，但它也可以配置在从节点上运行。Spark executors（Spark用于处理数据的恶魔）通常在EMR从节点上运行。
- en: 'EMR can also host notebook servers that connect to the cluster. This way, we
    can run our notebook paragraphs and this will trigger any distributed processing
    through Apache Spark. There are two ways to host notebooks on Apache Spark: EMR
    notebooks and JupyterHub EMR Application. We will use the first method in this
    chapter, and will cover JupyterHub in [Chapter 15](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml),
    *Tuning Clusters for Machine Learning*.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: EMR还可以托管连接到集群的笔记本服务器。这样，我们可以运行笔记本段落，这将触发通过Apache Spark的任何分布式处理。在Apache Spark上托管笔记本有两种方式：EMR笔记本和JupyterHub
    EMR应用程序。我们将在本章中使用第一种方法，并在[第15章](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml)“针对机器学习调整集群”中介绍JupyterHub。
- en: Through EMR notebooks, you can launch the cluster and the notebook at the same
    time through the **EMR notebooks** link on the console ([https://console.aws.amazon.com/elasticmapreduce/home](https://console.aws.amazon.com/elasticmapreduce/home)).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通过EMR笔记本，你可以通过控制台上的**EMR笔记本**链接同时启动集群和笔记本（[https://console.aws.amazon.com/elasticmapreduce/home](https://console.aws.amazon.com/elasticmapreduce/home)）。
- en: 'You can create the cluster and notebook simultaneously by clicking on the Create
    Notebook button, as seen in the following screenshot:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过点击“创建笔记本”按钮同时创建集群和笔记本，如下截图所示：
- en: '![](img/de2ef949-ffb7-4b85-b029-e104dab52790.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/de2ef949-ffb7-4b85-b029-e104dab52790.png)'
- en: 'Once you create the notebook, it will click on the Open button, as shown in
    the following screenshot:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 创建笔记本后，它将点击“打开”按钮，如下截图所示：
- en: '![](img/804659f6-2720-4e02-bb01-6f62a27b27e6.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/804659f6-2720-4e02-bb01-6f62a27b27e6.png)'
- en: 'Clicking on the Open button opens the notebook for us to start coding. The
    notebook is a standard Jupyter Notebook as it can be seen in the following screenshot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“打开”按钮将打开笔记本，以便我们开始编码。正如以下截图所示，笔记本是一个标准的Jupyter Notebook：
- en: '![](img/f9a106eb-25fa-4d0f-9af5-3bad1df69518.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/f9a106eb-25fa-4d0f-9af5-3bad1df69518.png)'
- en: Alternatively, you can create the cluster separately and attach the notebook
    to the cluster. The advantage of doing so is that you have access to additional
    advanced options.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以单独创建集群，然后将笔记本附加到集群上。这样做的好处是你可以访问额外的先进选项。
- en: 'We recommend at least 10 machines (for instance, 10 m5.xlarge nodes) to run
    the code from this chapter in a timely fashion.  Additionally, we suggest you
    increase the Livy session timeout if your jobs take longer than an hour to complete.
    For such jobs, the notebook may get disconnected from the cluster. Livy is the
    software responsible for the communication between the notebook and the cluster.
    The following screenshot shows the create cluster options including a way to extend
    the Livy session timeout:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议至少使用10台机器（例如，10个m5.xlarge节点）来及时运行本章中的代码。另外，如果你的作业完成时间超过一小时，我们建议你增加Livy会话超时时间。对于此类作业，笔记本可能会与集群断开连接。Livy是负责笔记本与集群之间通信的软件。以下截图显示了创建集群选项，包括扩展Livy会话超时时间的方法：
- en: '![](img/92fd675d-eee4-4138-ae7d-4179f17419a4.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/92fd675d-eee4-4138-ae7d-4179f17419a4.png)'
- en: On [Chapter 15](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml), *Tuning Clusters
    for Machine Learning, *we will cover more details regarding cluster configuration.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第15章](691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml)“针对机器学习调整集群”中，我们将详细介绍集群配置的更多细节。
- en: Training with Apache Spark on EMR
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在EMR上使用Apache Spark进行训练
- en: Let's now explore the training with Apache Spark on EMR.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来探索在EMR上使用Apache Spark进行训练。
- en: Getting the data
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: The first step is to upload the data to EMR. You can do this straight from the
    notebook or download the dataset locally and then uploaded it to S3 using the
    command-line tools from AWS (awscli). In order to use the command-line tools from
    AWS, you need to create AWS access keys on the IAM console. Details on how to
    do that can be found here: [https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将数据上传到EMR。你可以直接从笔记本中这样做，或者将数据集本地下载后，使用AWS的命令行工具（awscli）将其上传到S3。为了使用AWS的命令行工具，你需要在IAM控制台上创建AWS访问密钥。有关如何操作的详细信息，请参阅此处：[https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html)。
- en: Once you have your AWS access and secret keys, you can configure them by executing
    `aws configure` on the command line.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了AWS访问密钥和秘密密钥，你可以在命令行上执行`aws configure`来配置它们。
- en: 'First, we will get a portion of the dataset through the following `wget` command:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将通过以下`wget`命令获取数据集的一部分：
- en: '[PRE0]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will unzip and upload the CSV dataset onto a `s3` bucket called `mastering-ml-aws` as
    shown by the following command:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将解压并将 CSV 数据集上传到名为 `mastering-ml-aws` 的 `s3` 存储桶，如下命令所示：
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once the data is in S3, we can come back to our notebook and start coding to
    train the classifier.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据在 S3 中，我们就可以回到我们的笔记本并开始编写代码来训练分类器。
- en: Preparing the data
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: The EMR notebooks, as opposed to the examples we ran locally in previous chapters
    *(*[Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml),* Classifying Twitter
    Feeds with Naive Bayes and* [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml),
    *Predicting House Value with Regression Algorithms)* have implicit variables to
    access the Spark context. In particular, the Spark session is named `spark`. The
    first paragraph run will always initialize the context and trigger the Spark driver.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在前几章中在本地运行的示例相比（*[第 2 章](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml)，*使用朴素贝叶斯分类
    Twitter 推文和* [第 3 章](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml)，*使用回归算法预测房屋价值），EMR
    笔记本具有隐式变量以访问 Spark 上下文。特别是，Spark 会话被命名为 `spark`。第一次运行的第一段将始终初始化上下文并触发 Spark 驱动器。
- en: 'In the following screenshot, we can see the spark application starting and
    a link to the Spark UI:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的屏幕截图中，我们可以看到 Spark 应用程序启动以及 Spark UI 的链接：
- en: '![](img/02ff36ff-e8d8-4321-9ae9-36c3f28ffe4e.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/02ff36ff-e8d8-4321-9ae9-36c3f28ffe4e.png)'
- en: 'The next step is to load our dataset and explore the different the first few
    rows by running the following snippet:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是加载我们的数据集并通过运行以下代码片段来探索前几行：
- en: '[PRE2]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output of the above show command is:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的 `show` 命令的输出如下：
- en: '![](img/5e2044ea-ac04-4c4d-8a48-9b1fb485b0b0.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5e2044ea-ac04-4c4d-8a48-9b1fb485b0b0.png)'
- en: 'The `spark.read.json`  method, the first command from the preceding code block, reads
    the JSON data into a dataset similar to what we''ve done before with CSV using
    `spark.read.csv`. We can observe our dataset has 10 features and an `l` column
    indicating the label which we''re trying to predict, that is, if the user clicked
    (1) or didn''t click (0) in the advertisement. You might realize that some features
    are multivalued (more than one value in a cell) and some are null. To simplify
    the code examples in this chapter we will just pick the first five features by
    constructing a new dataset and name these features `f0` through `f4` while also
    replacing null features with the value `0` and only taking the first value in
    the case of multivalued features:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.read.json` 方法，上一段代码块中的第一个命令，将 JSON 数据读取到一个数据集，类似于我们之前使用 `spark.read.csv`
    处理 CSV 数据所做的那样。我们可以观察到我们的数据集有 10 个特征和一个 `l` 列，表示我们试图预测的标签，即用户是否点击了（1）或未点击（0）广告。你可能意识到一些特征是多值的（单元格中有多个值）并且一些是空的。为了简化本章的代码示例，我们将只选择前五个特征，通过构建一个新的数据集并将这些特征命名为
    `f0` 到 `f4`，同时将空特征替换为值 `0`，并在多值特征的情况下只取第一个值：'
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `selectExpr` command above allows us to use SQL-like operations. In this
    particular case we will use coalesce operation which transforms any null expressions
    into the value `0`. Also note that we're always just taking the first value for
    multivalued features.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的 `selectExpr` 命令允许我们使用类似 SQL 的操作。在这种情况下，我们将使用 coalesce 操作，该操作将任何空表达式转换为值
    `0`。此外，请注意，我们总是只取多值特征的第一个值。
- en: Generally, it's a bad idea to discard features as they might carry important
    predictive value. Likewise, replacing nulls for a fixed value can also be sub-optimal.
    We should consider common imputation techniques for missing values such as replacing
    with a point estimate (medians, modes, and means are commonly used). Alternatively,
    a model can be trained to fill in the missing value from the remaining features.
    In order to keep our focus on using trees in this chapter, we won't go deeper
    on the issue of missing values.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，丢弃特征是一个坏主意，因为它们可能携带重要的预测价值。同样，用固定值替换空值也可能不是最优的。我们应该考虑常见的缺失值填充技术，例如用点估计（中位数、众数和平均值是常用的）。或者，可以训练一个模型来从剩余特征中填充缺失值。为了保持本章专注于使用树，我们不会深入探讨缺失值的问题。
- en: 'Our `df` dataset now looks as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `df` 数据集现在看起来如下：
- en: '![](img/1172103f-359d-456d-84fb-10caa8390080.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1172103f-359d-456d-84fb-10caa8390080.png)'
- en: 'Now we do something quite Spark specific, which is to reshuffle the different
    portions of the CSV into different machines and cache them in memory. The command
    to do such thing is as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们做一件相当特定于 Spark 的事情，即将 CSV 的不同部分重新洗牌到不同的机器上并在内存中缓存它们。执行此操作的命令如下：
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Since we will repeatedly iterate on processing the same dataset, by loading
    it in memory, it will significantly speed up any future operation made for `df` .
    The repartitioning helps to make sure the data is better distributed throughout
    the cluster, hence increasing the parallelization.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将反复迭代处理相同的数据集，通过将其加载到内存中，这将显著加快对`df`进行的任何未来操作。重新分区有助于确保数据在整个集群中分布得更好，从而提高并行化程度。
- en: 'The `describe()` method builds a dataframe with some basic stats (`min`, `max`,
    `mean`,  `count`) of the different fields in our dataset, as seen in the following
    screenshot:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`describe()` 方法构建了一个包含我们数据集中不同字段的基本统计信息（`min`，`max`，`mean`，`count`）的数据框，如下截图所示：'
- en: '![](img/a789e41c-f793-4e40-b93e-490782298ade.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a789e41c-f793-4e40-b93e-490782298ade.png)'
- en: We can observe that most features range from low negative values to very large
    integers, suggesting these are anonymized feature values for which a hash function
    was applied.  The field we're trying to predict is `click`, which is `1` when
    the user clicked on the advertisement and 0 when the user didn't click.  The mean
    value for the click column informs us that there is certain degree of label imbalance
    (as about 18% of the instances are clicks). Additionally, the `count` row tell
    us that there is a total of 12,000,000 rows on our dataset.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，大多数特征的范围从较低的负值到非常大的整数，这表明这些是经过匿名处理的特征值，对它们应用了哈希函数。我们试图预测的字段是`click`，当用户点击广告时为`1`，未点击时为`0`。点击列的均值告诉我们存在一定程度的标签不平衡（大约18%的实例是点击）。此外，`count`行告诉我们我们的数据集中总共有1,200,000,000行。
- en: 'Another useful inspection is to understand the cardinality of the categorical
    values.   The following screenshot from our notebooks shows the different number
    of unique values each feature gets:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的检查是了解分类值的基数。以下是我们笔记本中的截图，显示了每个特征得到的唯一值的数量：
- en: '![](img/5b320793-9f7f-49df-bf65-c31acbf00164.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5b320793-9f7f-49df-bf65-c31acbf00164.png)'
- en: As you can see, the f4 feature is an example of a category that has many distinct
    values. These kinds of features often require special attention, as we will see
    later in this section.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，f4特征是一个具有许多不同值的分类的例子。这类特征通常需要特别注意，正如我们将在本节后面看到的那样。
- en: 'Decision trees and most of Spark ML libraries require our features to be numerical
    only. It happens by chance that our features are already in numerical form, but
    these really represent categories which were hashed into numbers. In [Chapter
    2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying Twitter Feeds with
    Naive Bayes,* we learned that in order to train a classifier, we need to provide
    a vector of numbers. For this reason, we need to transform our categories into
    numbers in our dataset to include them in our vectors. This transformation is
    often called **feature encoding**. There are two popular ways to do this: through
    one-hot encoding or categorical encoding (also called **string indexing**).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树和Spark ML库中的大多数库都需要我们的特征是数值型的。碰巧我们的特征已经是数值形式，但它们实际上代表的是被哈希成数字的分类。在[第2章](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml)中，我们学习了为了训练分类器，我们需要提供一个数字向量。因此，我们需要将我们的分类转换成数字，以便将它们包含在我们的向量中。这种转换通常被称为**特征编码**。有两种流行的方法可以实现这一点：通过独热编码或分类编码（也称为**字符串索引**）。
- en: 'In the following generic examples, we assume that the `site_id` feature could
    only take up to three distinct values: `siteA`, `siteB`, and `siteC`.  These examples
    will also illustrate the case in which we have string features to encode into
    numbers (not integer hashes as in our dataset).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下通用示例中，我们假设`site_id`特征只能取最多三个不同的值：`siteA`，`siteB`和`siteC`。这些示例还将说明我们如何将字符串特征编码成数字（不同于我们数据集中的整数哈希）的情况。
- en: Categorical encoding
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类编码
- en: 'Categorical encoding (or string indexing) is the simplest kind of encoding,
    in which we assign a number to each site value. Let''s look at an example in the
    following table:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 分类编码（或字符串索引）是最简单的一种编码方式，其中我们为每个站点值分配一个数字。让我们看看以下表中的例子：
- en: '| `site_id` | `site_id_indexed` |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| `site_id` | `site_id_indexed` |'
- en: '| `siteA` | `1` |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| `siteA` | `1` |'
- en: '| `siteB` | `2` |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| `siteB` | `2` |'
- en: '| `siteC` | `3` |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| `siteC` | `3` |'
- en: One-hot encoding
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独热编码
- en: 'In this kind of encoding, we create new binary columns for each possible site
    value and set the value as `1` when the value is present, as shown in the following
    table:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种编码中，我们为每个可能的站点值创建新的二进制列，并在值存在时将其设置为`1`，如下表所示：
- en: '| `site_id` | `siteA` | `siteB` | `siteC` |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| `site_id` | `siteA` | `siteB` | `siteC` |'
- en: '| `siteA` | `1` | `0` | `0` |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| `siteA` | `1` | `0` | `0` |'
- en: '| `siteB` | `0` | `1` | `0` |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| `siteB` | `0` | `1` | `0` |'
- en: '| `siteC` | `0` | `0` | `1` |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| `siteC` | `0` | `0` | `1` |'
- en: Categorical encoding is simple; however, it may create an artificial ordering
    of the features, and some ML algorithms are sensitive to that. One-hot encoding
    has the additional benefit of supporting multi-valued features (for example, if
    a row has two sites, we can set a `1` in both columns). However, one-hot encoding
    adds more features to our dataset, which increases the dimensionality. Adding
    more dimensions to our dataset makes the training more complex and may reduce
    its predictive ability. This is known as the **curse of dimensionality**.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 分类编码很简单；然而，它可能会创建特征的虚假排序，并且一些机器学习算法对这一点很敏感。独热编码有额外的优点，可以支持多值特征（例如，如果某行有两个站点，我们可以在两个列中都设置`1`）。然而，独热编码会增加我们的数据集特征数量，从而增加维度。向数据集中添加更多维度会使训练更加复杂，并可能降低其预测能力。这被称为**维度诅咒**。
- en: 'Let''s see how we would use categorical encoding on a sample of our dataset
    to transform the C1 feature (a categorical feature) into numerical values:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何在数据集的一个样本上使用分类编码，将C1特征（一个分类特征）转换为数值：
- en: '[PRE5]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding code first instantiates a `StringIndexer` that will encode column
    `f0` into a new column `f0_index` upon fitting, goes through the dataset and finds
    distinct feature values that assign an index based on the popularity of such values.
    Then we can use the `transform()` method to get indices for each value. The output
    of the preceding final `show()` command is shown in the following screenshot:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码首先实例化一个`StringIndexer`，该索引器将在拟合时将列`f0`编码到新的列`f0_index`中，遍历数据集并找到不同的特征值，根据这些值的流行度分配索引。然后我们可以使用`transform()`方法为每个值获取索引。上述最后的`show()`命令的输出如下所示：
- en: '![](img/1d2c0b3a-c31c-4f11-bc7f-e7f75243b79e.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d2c0b3a-c31c-4f11-bc7f-e7f75243b79e.png)'
- en: In the above screenshot we can see the numerical value that each  raw (hashed)
    categorical value was assigned to.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述截图中，我们可以看到每个原始（散列）分类值被分配的数值。
- en: 'To perform one-hot encoding on the values, we use the `OneHotEncoder` transformer:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要对值执行独热编码，我们使用`OneHotEncoder`转换器：
- en: '[PRE6]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding commands generates the following output:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令生成了以下输出：
- en: '![](img/0303965c-ce6c-422a-bbbb-91c92eb20b7a.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0303965c-ce6c-422a-bbbb-91c92eb20b7a.png)'
- en: 'Note how the different `f0` values get mapped to the corresponding boolean
    vector. We did the encoding for just one feature; however, for training, we need
    to go through the same process for several features. For this reason, we built
    a function that builds all the indexing and encoding stages necessary for our
    pipeline:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意不同的`f0`值是如何映射到相应的布尔向量的。我们只为一个特征进行了编码；然而，为了训练，我们需要对多个特征执行相同的过程。因此，我们构建了一个函数，该函数构建了我们管道所需的所有索引和编码阶段：
- en: '[PRE7]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following code builds a training pipeline, including the `DecisionTree`
    estimator:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码构建了一个包含`DecisionTree`估计器的训练管道：
- en: '[PRE8]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding code,`VectorAssembler` constructs a vector with all features
    that require encoding as well as the numerical features ( `VectorAssembler` can
    take as input columns that can be vectors or scalars so you can use numerical
    features directly if existent in your dataset). Given the high number of one-hot-encoded
    values, the feature vector can be huge and make the trainer very slow or require
    massive amounts of memory. One way to mitigate that is to use a **chi-squared**
    feature selector. In our pipeline, we have selected the best 100 features. By
    best, we mean the features that have more predictive power—note how the chi-squared
    estimator takes both the features and the label to decide on the best features.
    Finally, we include the decision engine estimator stage, which is the one that
    will actually create the classifier.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`VectorAssembler`构建了一个包含所有需要编码的特征以及数值特征的向量（`VectorAssembler`可以接受作为输入的列，这些列可以是向量或标量，因此如果您的数据集中存在数值特征，您可以直接使用它们）。由于独热编码值数量众多，特征向量可能非常大，这会使训练器非常慢或需要大量的内存。减轻这种情况的一种方法是用**卡方**特征选择器。在我们的管道中，我们选择了最佳的100个特征。这里的“最佳”是指具有更多预测能力的特征——注意卡方估计器如何同时考虑特征和标签来决定最佳特征。最后，我们包括决策引擎估计器阶段，这是实际创建分类器的阶段。
- en: If we attempt to string index features with very large cardinality, the driver
    will collect all possible values (in order to keep a value-to-index dictionary
    for transformation). In such an attempt, the driver will most likely run out of
    memory as we're looking at millions of distinct values to keep. For these cases,
    you need other strategies, such as keeping only the features with the most predictive
    ability or considering only the most popular values. Check out our article, which
    includes a solution to this problem at [https://medium.com/dataxutech/how-to-write-a-custom-spark-classifier-categorical-naive-bayes-60f27843b4ad](https://medium.com/dataxutech/how-to-write-a-custom-spark-classifier-categorical-naive-bayes-60f27843b4ad).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试将具有非常大的基数（cardinality）的特征索引串联起来，驱动程序将收集所有可能的值（为了保留一个值到索引的字典以进行转换）。在这种尝试中，驱动程序很可能会耗尽内存，因为我们正在查看数百万个不同的值来保留。对于这些情况，你需要其他策略，例如只保留最具预测能力的特征，或者只考虑最受欢迎的值。查看我们的文章，其中包含了解决这个问题的方案，请参阅[https://medium.com/dataxutech/how-to-write-a-custom-spark-classifier-categorical-naive-bayes-60f27843b4ad](https://medium.com/dataxutech/how-to-write-a-custom-spark-classifier-categorical-naive-bayes-60f27843b4ad)。
- en: Training a model
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个模型
- en: 'Our pipeline is now constructed, so we can proceed to split our dataset for
    testing and training and then we fit the model:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据管道现在已构建完成，因此我们可以继续将数据集分割为测试和训练集，然后拟合模型：
- en: '[PRE9]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once this is executed, the Spark Driver will figure out the best plan for distributing
    the processing necessary to train the model across many machines.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦执行，Spark驱动程序将确定在多台机器上分配训练模型所需处理的最佳方案。
- en: 'By following the Spark UI link shown at the beginning of this section, we can
    see the status of the different jobs running on EMR:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 通过跟随本节开头显示的Spark UI链接，我们可以查看在EMR上运行的不同作业的状态：
- en: '![](img/1ded0e39-d9eb-4dd5-9e98-a0a93532ff6a.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1ded0e39-d9eb-4dd5-9e98-a0a93532ff6a.png)'
- en: Once the model is trained, we can explore the decision tree behind it. We can
    do this by inspecting the last stage of the pipeline (that is, the decision tree
    model).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们可以探索其背后的决策树。我们可以通过检查管道的最后一个阶段（即决策树模型）来实现这一点。
- en: 'The following code snippet shows the result of outputting the decision tree
    in text format:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段显示了以文本格式输出决策树的结果：
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Note how each decision is based on a feature that takes a value of `0` or `1`.
    This is because we have used one-hot encoding on our pipeline. If we had used
    the categorical encoding (string indexing), we would have seen a condition that
    involves several indexed values, such as the following example:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 注意每个决策都是基于一个取值为`0`或`1`的特征。这是因为我们在管道中使用了one-hot编码。如果我们使用了分类编码（字符串索引），我们会看到涉及多个索引值的条件，例如以下示例：
- en: '[PRE11]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Evaluating our model
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估我们的模型
- en: Contrary to our Twitter classification problem in [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml),
    *Classifying Twitter Feeds with Naive Baye*s, the label in this dataset is very
    skewed. This is because there are only a few occasions where users decide to click
    on ads. The accuracy measurement we used in [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml),
    *Classifying Twitter Feeds with Naive Bayes*, would not be suitable, as a model
    that never predicts a click would still have very high accuracy (all non-clicks
    would result in correct predictions). Two possible alternatives for this case
    could be to use metrics derived from the ROC or **precision-recall curves**, which
    can be seen in the following section.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在第2章中讨论的Twitter分类问题[第2章](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml)，“使用朴素贝叶斯分类Twitter帖子”，这个数据集的标签非常倾斜。这是因为用户决定点击广告的情况很少。我们在第2章中使用的准确性度量[第2章](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml)，“使用朴素贝叶斯分类Twitter帖子”，将不适用，因为一个从未预测点击的模型仍然会有非常高的准确性（所有非点击都会导致正确预测）。对于这种情况，两种可能的替代方案是使用ROC或**精确率-召回率曲线**衍生出的度量，这些将在下一节中展示。
- en: Area Under ROC Curve
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ROC曲线下的面积
- en: The **Receiver Operating Characteristic** (**ROC**) is a representation of a
    trade-off between true-positive rates and false-positive rates. True-positive
    rates describe how good a model is at predicting a positive class when the actual
    class is positive. True-positive rates are calculated as the ratio of true positives
    predicted by a model, to the sum of true positives and false negatives. False-positive
    rates describe how often the model predicts the positive class, when the actual
    class is negative. False-positive rates are calculated as the ratio of false positives,
    to the sum of false positives and true negatives. ROC is a plot where the *x*
    axis is represented by the false-positive rate with a range of 0-1, while the
    *y* axis is represented as the true-positive rate. **Area Under Curve** (**AUC**)
    is the measure of the area under the ROC curve. AUC is a measure of predictiveness
    of a classification model.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收者操作特征**（**ROC**）是真正例率和假正例率之间权衡的表示。真正例率描述了当实际类别为正时，模型预测正类别的良好程度。真正例率是模型预测的真正例与真正例和假负例之和的比率。假正例率描述了模型预测正类别的频率，当实际类别为负时。假正例率是假正例与假正例和真正例之和的比率。ROC是一个图表，其中*x*轴表示范围在0-1之间的假正例率，而*y*轴表示真正例率。**曲线下面积**（**AUC**）是ROC曲线下面积的措施。AUC是分类模型预测性的衡量指标。'
- en: 'Three examples of receiver operator curves are seen in the following screenshot:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的屏幕截图显示了三个接收者操作曲线的例子：
- en: '![](img/fc95ba2b-6336-4f9b-96e7-dec2cbbcb0d4.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fc95ba2b-6336-4f9b-96e7-dec2cbbcb0d4.png)'
- en: In the preceding plot, the dotted line represents an example of when `AUC` is
    `1`. Such AUCs occur when all the positive outcomes are classified correctly.
    The solid line represents the `AUC` that is `0.5`. For a binary classifier, the `AUC`
    is `0.5` when the predictions coming from the machine learning model are similar
    to randomly generating an outcome. This indicates that the machine learning model
    is no better than a random-number generator in predicting outcomes. The dashed
    line represents the `AUC` that is `0.66`. This happens when a machine learning
    model predicts some examples correctly, but not all. However, if the `AUC` is
    higher than `0.5` for the binary classifier, the model is better than just randomly
    guessing the outcome. However, if it is below 0.5, this means that the machine
    learning model is worse than a random-outcome generator. Thus, AUC is a good measure
    of comparing machine learning models and evaluating their effectiveness.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，虚线代表当`AUC`为`1`的情况。这种`AUC`出现在所有正结果都被正确分类时。实线代表`AUC`为`0.5`的情况。对于二元分类器来说，当机器学习模型的预测结果与随机生成结果相似时，`AUC`为`0.5`。这表明机器学习模型在预测结果方面并不比随机数生成器更好。虚线代表`AUC`为`0.66`的情况。这发生在机器学习模型正确预测了一些例子，但并非全部。然而，如果二元分类器的`AUC`高于`0.5`，则说明模型比随机猜测结果更好。但如果它低于`0.5`，这意味着机器学习模型比随机结果生成器更差。因此，AUC是衡量机器学习模型和评估其有效性的良好指标。
- en: Area under the precision-recall curve
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确率-召回率曲线下的面积
- en: The precision-recall curve represents a tradeoff between precision and recall in
    a prediction model. **Precision** is defined as the ratio of true positives to
    the total number of positive predictions made by the model. **Recall** is defined
    as the ratio of positive predictions to the total number of actual positive predictions.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 精确率-召回率曲线表示预测模型中精确率和召回率之间的权衡。**精确率**定义为模型做出的所有正预测中真正例的比例。**召回率**定义为正预测与实际正预测总数的比例。
- en: Note that the precision-recall curve does not model true negative values. This
    is useful in cases of the unbalanced dataset. ROC curves may provide a very optimistic
    view of a model if the model is good at classifying true negatives and generates
    a smaller number of false positives.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，精确率-召回率曲线不模拟真正负值。这在数据集不平衡的情况下很有用。如果模型擅长分类真正负值并生成较少的假阳性，ROC曲线可能会提供一个非常乐观的模型视图。
- en: 'The following plot shows an example of a precision-recall curve:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表展示了一个精确率-召回率曲线的例子：
- en: '![](img/d6012ad5-b4ee-4bdd-965c-db7d4adb5d20.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d6012ad5-b4ee-4bdd-965c-db7d4adb5d20.png)'
- en: In the preceding screenshot, the dashed line shows when the area under precision-recall
    curve is `0.5`. This indicates that the precision is always 0.5, which is similar
    to a random-number generator. The solid line represents the precision-recall curve
    that is better than random. The precision recall curve also can be used to evaluate
    a machine learning model, similar to the ROC area. However, the precision-recall
    curve should be used when the dataset is unbalanced, and the ROC should be used
    when the dataset is balanced.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图上，虚线表示当精确度-召回率曲线下的面积是 `0.5` 的时候。这表明精确度始终为0.5，类似于随机数生成器。实线代表比随机更好的精确度-召回率曲线。精确度-召回率曲线也可以用来评估机器学习模型，类似于ROC面积。然而，当数据集不平衡时，应使用精确度-召回率曲线，而当数据集平衡时，应使用ROC。
- en: 'So, going back to our example, we can use Spark''s `BinaryClassificationEvaluator`
    to calculate the scores by providing the actual and predicted labels on our test
    dataset. First we will apply the model on our test dataset to get the predictions
    and scores:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，回到我们的示例，我们可以使用Spark的`BinaryClassificationEvaluator`通过提供测试数据集上的实际和预测标签来计算分数。首先，我们将模型应用于我们的测试数据集以获取预测和分数：
- en: '[PRE12]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'By applying the previous transformation `test_transformed` will have all columns
    included in `test_df` plus an additional one called `rawPrediction` which will
    have a score which can be used for evaluation:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用前面的转换 `test_transformed`，将包含在 `test_df` 中的所有列以及一个额外的列 `rawPrediction`，该列将有一个用于评估的分数：
- en: '[PRE13]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The output of the preceding command is 0.43\. The fact that we got an ROC metric
    lower than 0.5 means that our classifier is even worse than random classifier
    and hence it is not a good model for predicting clicks! In the next section, we
    will show how to use ensemble models to improve our predictive ability.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出是 0.43\. 我们得到一个低于0.5的ROC指标意味着我们的分类器甚至比随机分类器还要差，因此它不是一个好的点击预测模型！在下一节中，我们将展示如何使用集成模型来提高我们的预测能力。
- en: Training tree ensembles on EMR
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在EMR上训练树集成
- en: Decision trees can be useful for understanding the decisions made by our classifier,
    especially when decision trees are small and readable. However, decision trees
    tend to overfit the data (by learning the details of the training dataset and
    not being able to generalize on new data). For this reason, ML practitioners tend
    to use tree ensembles, such as random forests and gradient-boosted trees, which
    are explained in the previous sections in this chapter under *Understanding gradient
    boosting algorithms* and *Understanding random forest algorithms*.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以用来理解我们的分类器所做的决策，尤其是在决策树较小且可读时。然而，决策树往往会对数据进行过拟合（通过学习训练数据集的细节而无法对新数据进行泛化）。因此，机器学习从业者倾向于使用树集成，如随机森林和梯度提升树，这些在本书的
    *理解梯度提升算法* 和 *理解随机森林算法* 部分中已有解释。
- en: 'In our code examples, to use random forests or gradient boosted trees, we just
    need to replace the last stage of our pipeline with the corresponding constructor:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的代码示例中，要使用随机森林或梯度提升树，我们只需将我们的管道的最后一个阶段替换为相应的构造函数：
- en: '[PRE14]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Note how we get a better ROC value with random forests on our sampled dataset:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何在我们的样本数据集上通过随机森林获得更好的ROC值：
- en: '[PRE15]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can see that now we get a ROC greater than 0.5 which means that our model
    has improved an is now better than random guessing. Similarly, you can train a
    gradient boosted tree with the `pyspark.mllib.tree.GradientBoostedTrees` class.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到现在我们得到了一个大于0.5的ROC值，这意味着我们的模型已经改进，现在比随机猜测要好。同样，你可以使用`pyspark.mllib.tree.GradientBoostedTrees`类训练梯度提升树。
- en: Training gradient-boosted trees with the SageMaker services
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker服务训练梯度提升树
- en: In the *Training a model* and *Evaluating our model* sections, we learned how
    to build and evaluate a random forest classifier using Spark on EMR. In this section,
    we will see how to train a gradient boosted tree using the SageMaker services
    through the SageMaker notebooks. The XGBoost SageMaker service allows us to train
    gradient-boosted trees in a distributed fashion. Given that our clickthrough data
    is relatively large, it will be convenient to use such a service.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *训练模型* 和 *评估我们的模型* 部分中，我们学习了如何在EMR上使用Spark构建和评估随机森林分类器。在本节中，我们将通过SageMaker笔记本了解如何使用SageMaker服务训练梯度提升树。XGBoost
    SageMaker服务允许我们以分布式方式训练梯度提升树。鉴于我们的点击数据相对较大，使用此类服务将非常方便。
- en: Preparing the data
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: In order to use the SageMaker services, we will need to place our training and
    testing data in S3\. The documentation at [https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html)
    requires us to drop the data as CSV files where the first column indicates the
    training label (target feature) and the rest of the columns represent the training
    features (other formats are supported but we will use CSV in our example). For
    splitting and preparing the data in this way, EMR is still the best option as
    we want our data preparation to be distributed as well. Given our testing and
    training Spark datasets from the last *Preparing the data* section, we can apply
    the pipeline model, not for getting predictions in this case, but instead, for
    obtaining the selected encoded features for each row.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用 SageMaker 服务，我们需要将我们的训练和测试数据放在 S3 上。[https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html)
    中的文档要求我们将数据作为 CSV 文件降级，其中第一列表示训练标签（目标特征），其余列表示训练特征（支持其他格式，但我们将使用 CSV 作为示例）。为了以这种方式分割和准备数据，EMR
    仍然是最佳选择，因为我们希望我们的数据准备也是分布式的。鉴于上一节“准备数据”中的测试和训练 Spark 数据集，我们可以应用管道模型，在这种情况下不是用于获取预测，而是用于获取每行的选定编码特征。
- en: 'In the following snippet, for both `test_df` and `train_df` we apply the model
    transformation:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下片段中，我们对 `test_df` 和 `train_df` 都应用了模型转换：
- en: '[PRE16]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following screenshot shows the last three columns of the `test_transformed`
    dataframe:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了 `test_transformed` 数据框的最后三列：
- en: '![](img/04932beb-ce2f-4b4b-af48-02cff4bab6f9.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/04932beb-ce2f-4b4b-af48-02cff4bab6f9.png)'
- en: The transformed datasets includes the feature vector column (named `selected_features`
    with a size of 100). We need to transform these two columns into a CSV with 101
    columns (the `click` and the `selected_features` vectors flattened out). A simple
    transformation in Spark allows us to do this. We define a `deconstruct_vector`
    function, which we will use to obtain a Spark dataframe with the label and each
    vector component as a distinct column. We then save that to S3 both for training
    and testing as a CSV without headers, as SageMaker requires.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的数据集包括特征向量列（命名为 `selected_features`，大小为 100）。我们需要将这些两列转换成一个包含 101 列的 CSV
    文件（`click` 和 `selected_features` 向量展开）。Spark 中的一个简单转换使我们能够做到这一点。我们定义了一个 `deconstruct_vector`
    函数，我们将使用它来获取一个 Spark 数据框，其中标签和每个向量组件作为单独的列。然后我们将这些数据保存到 S3，用于训练和测试，作为没有标题的 CSV
    文件，因为 SageMaker 需要。
- en: 'In the following code snippet, we provide the `deconstruct_vector` function
    as well as the series of transformations needed to save the dataframe:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码片段中，我们提供了 `deconstruct_vector` 函数以及保存数据框所需的系列转换：
- en: '[PRE17]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In a similar fashion, we will save an additional CSV file that will not include
    the label (just the features) under the `s3://mastering-ml-aws/chapter4/test-trans-vec-csv-no-label` path.
    We will use this dataset to score the testing dataset through the SageMaker batch
    transform job in the next section, *Training with SageMaker XGBoost*.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，我们将在 `s3://mastering-ml-aws/chapter4/test-trans-vec-csv-no-label` 路径下保存一个额外的
    CSV 文件，该文件不包括标签（只有特征）。我们将在下一节“使用 SageMaker XGBoost 训练”中，通过 SageMaker 批量转换作业使用这个数据集来评分测试数据集。
- en: Training with SageMaker XGBoost
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SageMaker XGBoost 训练
- en: 'Now that our datasets for training and testing are in S3 in the right format,
    we can launch our SageMaker notebook instance and start coding our trainer. Let''s
    perform the following steps:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们训练和测试的数据集已经以正确的格式保存在 S3 上，我们可以启动我们的 SageMaker 笔记本实例并开始编写我们的训练器代码。让我们执行以下步骤：
- en: 'Instantiate the SageMaker session, container, and variables with the location
    of our datasets:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们数据集的位置实例化 SageMaker 会话、容器和变量：
- en: '[PRE18]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Create a classifier by instantiating a SageMaker estimator and providing the
    basic parameters, such as the number and type of machines to use (details can
    be found in the AWS documentation at [https://sagemaker.readthedocs.io/en/stable/estimators.html](https://sagemaker.readthedocs.io/en/stable/estimators.html) ):'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过实例化 SageMaker 估算器并提供基本参数（例如要使用的机器数量和类型等详细信息可在 AWS 文档中找到，链接为 [https://sagemaker.readthedocs.io/en/stable/estimators.html](https://sagemaker.readthedocs.io/en/stable/estimators.html)）来创建一个分类器：
- en: '[PRE19]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Set the hyperparameters of our trainer. The details can be found in the documentation
    (and we will cover it in more detail in [Chapter 14](7de65295-dd1f-4eb3-af00-3868ed7e2df9.xhtml),
    *Optimizing SageMaker and Spark Machine Learning Models*). The main parameter
    to look at here is the objective, which we have set for binary classification
    (using a logistic regression score, which is the standard way XGBoost performs
    classification). XGBoost can also be used for other problems, such as regressions
    or multi-class classification:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置训练器的超参数。详细信息可以在文档中找到（我们将在第14章中更详细地介绍，*优化SageMaker和Spark机器学习模型*）。这里要查看的主要参数是目标，我们将其设置为二元分类（使用逻辑回归分数，这是XGBoost执行分类的标准方式）。XGBoost也可以用于其他问题，如回归或多类分类：
- en: '[PRE20]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Before fitting the model, we need to specify the location and format of the
    input (there are a couple of formats accepted; we have chosen CSV for our example):'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在拟合模型之前，我们需要指定输入的位置和格式（接受几种格式；在我们的示例中我们选择了CSV）：
- en: '[PRE21]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Invoking the `fit` function will train the model with the data provided (that
    is, the data we saved in S3 through our EMR/Spark preparation):'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`fit`函数将使用提供的数据（即，我们通过EMR/Spark准备存储在S3中的数据）训练模型：
- en: '[PRE22]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The logs will show the some details about the training and validation error
    being optimized by XGBoost, as well as the status of the job and training costs.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 日志将显示XGBoost正在优化的训练和验证错误的一些细节，以及作业状态和训练成本。
- en: Applying and evaluating the model
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用和评估模型
- en: The following steps will show you how to use `sagemaker` to create batch predictions
    so you can evaluate the model.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将展示如何使用`sagemaker`创建批量预测，以便您可以评估模型。
- en: 'In order to obtain predictions, we can use a batch transform job:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得预测，我们可以使用批量转换作业：
- en: '[PRE23]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'For every file in the input `s3` directory, the batch transform job will produce
    a file with the scores:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入`s3`目录中的每个文件，批量转换作业将生成一个包含分数的文件：
- en: '[PRE24]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The preceding command generates the following output:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 前面命令生成了以下输出：
- en: '[PRE25]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can then load this single-column CSV file into a `pandas` dataframe:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将这个单列CSV文件加载到`pandas`数据框中：
- en: '[PRE26]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'These scores represent probabilities (derived via logistic regression). If
    we had set the objective to binary: hinge, we would get actual predictions instead.
    Choosing which kind to use depends on the type of application. In our case, it
    seems useful to gather probabilities, as any indication of a particular user being
    more likely to perform clicks would help to improve the marketing targeting.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分数代表概率（通过逻辑回归得出）。如果我们把目标设置为二元：hinge，我们将得到实际的预测。选择使用哪种类型取决于应用类型。在我们的案例中，收集概率似乎很有用，因为任何表明特定用户更有可能进行点击的迹象都将有助于提高营销定位。
- en: 'One of the advantages of SageMaker XGBoost is that it provides a serialization
    in S3 of a compatible XGBoost model with Python’s standard serialization library
    (pickle). As an example, we will take a portion of our test data in S3 and run
    the model to get scores. With this, we can compute the area under the ROC curve
    by performing the following steps:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker XGBoost的一个优点是它提供了与Python标准序列化库（pickle）兼容的XGBoost模型的S3序列化。作为一个例子，我们将从S3中的测试数据的一部分运行模型以获取分数。通过这样做，我们可以通过以下步骤计算ROC曲线下的面积：
- en: 'Locate the model tarball in `s3`:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`s3`中定位模型tar包：
- en: '[PRE27]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output looks as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE28]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Copy the model from S3 to our local directory and uncompress the tarball:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型从S3复制到我们的本地目录并解压tar包：
- en: '[PRE29]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here is the output of the preceding command, showing the name of the file uncompressed
    from the tarball:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面命令的输出，显示了从tar包中解压的文件名：
- en: '[PRE30]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once the model is locally downloaded and untared, we can load the model in
    memory via the `pickle` serialization library:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型被本地下载并解压，我们可以通过`pickle`序列化库将模型加载到内存中：
- en: '[PRE31]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Define the names of our columns (`f0` to `f99` for the features, and `click`
    as the label) and load the validation data from S3:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们列的名称（`f0`到`f99`为特征，`click`为标签）并从S3加载验证数据：
- en: '[PRE32]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'To create predictions with `xgboost`, we need to assemble a matrix from our
    `pandas` dataframe. Select all columns except the first one (which is the label),
    and then construct a DMatrix. Call the predict method from the `xgboost` model
    to get the scores for every row:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用`xgboost`创建预测，我们需要从我们的`pandas`数据框组装一个矩阵。选择除第一个（标签）之外的所有列，然后构建一个DMatrix。从`xgboost`模型调用predict方法以获取每行的分数：
- en: '[PRE33]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In the following screenshot, the reader can see how the dataframe looks:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的屏幕截图中，读者可以看到数据框的外观：
- en: '![](img/fd2a3999-60a2-431b-966a-ff6fe1cf1c58.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fd2a3999-60a2-431b-966a-ff6fe1cf1c58.png)'
- en: 'Given the `click` column and the `score` column, we can construct the ROC AUC:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定`点击`列和`得分`列，我们可以构建ROC AUC：
- en: '[PRE34]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: For our sample, we get a AUC value of 0.67, which is comparable to the value
    we got with Spark's random forests.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的样本，我们得到一个AUC值为0.67，这与我们使用Spark的随机森林得到的结果相当。
- en: In this chapter, we did not focus on building the most optimal model for our
    dataset. Instead, we focused on providing simple and popular transformations and
    tree models you can use to classify large volumes of data.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们没有专注于为我们的数据集构建最优模型。相反，我们专注于提供简单且流行的转换和树模型，您可以使用它们来分类大量数据。
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we covered the basic theoretical concepts for understanding
    tree ensembles and showed ways to train and evaluate these models in EMR, through
    Apache Spark, as well as through the SageMaker XGBoost service. Decision tree
    ensembles are one of the most popular classifiers, for many reasons:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了理解树集成的基本理论概念，并展示了如何通过Apache Spark以及SageMaker XGBoost服务在EMR中训练和评估这些模型。决策树集成是最受欢迎的分类器之一，原因有很多：
- en: They are able to find complex patterns in relatively short training time and
    with few resources. The XGBoost library is known as the most popular classifier
    among Kaggle competition winners (these are competitions held to find the best
    model for an open dataset).
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们能够在相对较短的训练时间和少量资源中找到复杂的模式。XGBoost库被认为是Kaggle竞赛获胜者中最受欢迎的分类器（这些竞赛是为了寻找开放数据集的最佳模型而举办的）。
- en: It's possible to understand why the classifier is predicting a given value.
    Following the decision tree paths or just looking at the feature importance are
    quick ways to understand the rationale behind the decisions made by tree ensembles.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有可能理解为什么分类器正在预测一个特定的值。遵循决策树路径或只是查看特征重要性是快速理解树集成决策背后的理由的快捷方式。
- en: Implementations of distributed training are available through Apache Spark and
    XGBoost.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Apache Spark和XGBoost可以提供分布式训练的实现。
- en: In the next chapter, we will look into how to use machine learning to cluster
    customers based on their behavioral patterns.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何使用机器学习根据客户的行为模式对客户进行聚类。
- en: Exercises
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: What is the main difference between random forests and gradient-boosted trees?
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机森林和梯度提升树之间主要区别是什么？
- en: Explain why the Gini Impurity may be interpreted as the misclassification rate.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释为什么Gini不纯度可以解释为误分类率。
- en: Explain why it is necessary to perform feature encoding for categorical features.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释为什么对分类特征进行特征编码是必要的。
- en: In this chapter, we provided two ways to do feature encoding. Find one other
    way to encode categorical features.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本章中，我们提供了两种进行特征编码的方法。找出另一种对分类特征进行编码的方法。
- en: Explain why the accuracy metric we used in [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying
    Twitter Feeds with Naive Bayes,* is not suitable for predicting clicks on our
    dataset.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释为什么我们在[第2章](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml)中使用的准确度指标，“使用朴素贝叶斯分类Twitter帖子”不适合预测我们数据集中的点击。
- en: Find other objectives we can use for the XGBoost algorithm. When would you use
    each objective?
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出我们可以用于XGBoost算法的其他目标。你会在什么时候使用每个目标？
