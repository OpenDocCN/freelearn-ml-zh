- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Labeling Image Data Using Data Augmentation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据增强对图像数据进行标记
- en: In this chapter, we will learn how to label image data using data augmentation
    for semi-supervised machine learning. We will use the CIFAR-10 dataset and the
    MNIST dataset of handwritten digits to generate labels using data augmentation.
    From there we will build an image classification machine learning model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用数据增强对图像数据进行标记，以进行半监督机器学习。我们将使用CIFAR-10数据集和MNIST手写数字数据集来生成标签，然后我们将构建一个图像分类机器学习模型。
- en: Data augmentation plays a crucial role in data labeling by enhancing the diversity,
    size, and quality of the dataset. Data augmentation techniques generate additional
    samples by applying transformations to existing data. This effectively increases
    the size of the dataset, providing more examples for training and improving the
    model’s ability to generalize.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强在数据标记中起着至关重要的作用，通过增强数据集的多样性、大小和质量。数据增强技术通过对现有数据进行变换来生成额外的样本。这有效地增加了数据集的大小，为训练提供了更多示例，并提高了模型泛化的能力。
- en: 'In this chapter, we will cover the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下内容：
- en: How to prepare training data with image data augmentation and implement support
    vector machines
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用图像数据增强准备训练数据并实现支持向量机
- en: How to implement convolutional neural networks with augmented image data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用增强图像数据实现卷积神经网络
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, we will use the CIFAR-10 dataset, which is a publicly available
    image dataset consisting of 60,000 32x32 color images in 10 classes ([http://www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html)),
    along with the famous MNIST handwritten digits dataset.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将使用CIFAR-10数据集，这是一个包含10个类别的60,000个32x32彩色图像的公开图像数据集([http://www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html))，以及著名的MNIST手写数字数据集。
- en: Training support vector machines with augmented image data
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用增强图像数据训练支持向量机
- en: '**Support Vector Machines** (**SVMs**) are widely used in machine learning
    to solve classification problems. SVMs are known for their high accuracy and ability
    to handle complex datasets. One of the challenges in training SVMs is the availability
    of large and diverse datasets. In this section, we will discuss the importance
    of data augmentation in training SVMs for image classification problems. We will
    also provide Python code examples for each technique.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVMs**）在机器学习中广泛用于解决分类问题。SVMs以其高准确性和处理复杂数据集的能力而闻名。训练SVMs的一个挑战是大型和多样化的数据集的可用性。在本节中，我们将讨论数据增强在训练SVMs进行图像分类问题中的重要性。我们还将为每种技术提供Python代码示例。'
- en: '![](img/B18944_06_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18944_06_01.jpg)'
- en: Figure 6.1 – SVM separates class A and class B with largest margin
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – SVM使用最大间隔将类别A和类别B分开
- en: SVMs are a type of supervised learning algorithm used for classification and
    regression analysis. SVMs can be used for outlier detection. SVMs were originally
    designed for classification tasks, but can also be adapted for anomaly or outlier
    detection as well.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: SVMs是一种用于分类和回归分析的监督学习算法。SVMs可用于异常检测。SVMs最初是为分类任务设计的，但也可以用于异常或异常检测。
- en: The objective of SVMs is to find the hyperplane that maximizes the margin between
    two classes of data. The hyperplane is defined as the decision boundary that separates
    the data points of two classes. The margin is the distance between the hyperplane
    and the nearest data point of each class.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: SVMs的目标是找到最大化两类数据之间间隔的超平面。超平面被定义为分隔两类数据点的决策边界。间隔是超平面与每个类最近的点之间的距离。
- en: SVMs use something called the *kernel trick*. Let’s understand what this is
    next.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: SVMs使用一种称为*核技巧*的东西。让我们接下来了解这是什么。
- en: Kernel trick
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 核技巧
- en: Let’s say you have data points on a sheet of paper, and you want to separate
    them into two groups. Imagine you have a magic wand (i.e., the kernel trick) that
    allows you to lift the points off the paper into the air. In the air, you can
    easily draw a line or a curve to separate the floating points.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在一张纸上有点，你想要将它们分成两组。想象你有一根魔法棒（即核技巧），它允许你将点从纸上抬起进入空中。在空中，你可以轻松地画一条线或曲线来分隔漂浮的点。
- en: Now, when you’re satisfied with the separation in the air, you use the magic
    wand again to bring everything back down to the paper. Miraculously, the separation
    you drew in the air translates to a more complex decision boundary on the paper
    that effectively separates your original data points.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当你对空气中的分离效果满意时，再次使用魔棒将所有内容拉回到纸上。奇迹般地，你在空中绘制的分离线在纸上转化为一个更复杂的决策边界，有效地分离了你的原始数据点。
- en: In the SVM world, this “magic wand” is the kernel trick. It allows SVMs to implicitly
    work in a higher-dimensional space, making it possible to find more intricate
    decision boundaries that weren’t achievable in the original space. The key is
    that you don’t have to explicitly compute the coordinates of the higher-dimensional
    space; the kernel trick does this for you.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在SVM（支持向量机）的世界里，这个“魔棒”就是核技巧。它允许SVM在更高维的空间中隐式地工作，使得找到更复杂的决策边界成为可能，这些决策边界在原始空间中是无法实现的。关键是，你不必显式地计算更高维空间的坐标；核技巧会为你完成这项工作。
- en: In summary, the kernel trick lifts your data into a higher-dimensional space,
    where SVMs can find more sophisticated ways to separate different classes. It’s
    a powerful tool for handling complex data scenarios.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，核技巧将你的数据提升到更高维的空间，在那里SVM可以找到更复杂的方法来分离不同的类别。它是处理复杂数据场景的强大工具。
- en: SVMs leverage the kernel trick to transform the input data into a higher-dimensional
    space, where a linear decision boundary can be found. The kernel function plays
    a crucial role in this process, mapping the input data into a feature space where
    the relationships between variables may be more easily separable.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: SVM利用核技巧将输入数据转换到更高维的空间，在那里可以找到一个线性决策边界。核函数在这个过程中起着至关重要的作用，它将输入数据映射到一个特征空间，其中变量之间的关系可能更容易分离。
- en: The most commonly used kernel functions are the linear kernel, which represents
    a linear decision boundary, the polynomial kernel, which introduces non-linearity
    with higher-order polynomial features, and the **radial basis function** (**RBF**)
    kernel, which allows for a more flexible and non-linear decision boundary. The
    choice of the kernel function and its parameters significantly influences the
    SVM’s ability to model complex relationships in the data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的核函数包括线性核，它表示线性决策边界；多项式核，它通过引入高阶多项式特征引入非线性；以及**径向基函数**（**RBF**）核，它允许更灵活的非线性决策边界。核函数的选择及其参数显著影响SVM建模数据中复杂关系的能力。
- en: As we now have a basic idea about SVMs, let us next understand data augmentation,
    image data augmentation, and the various techniques used for this.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经对SVM有了基本的了解，接下来让我们了解数据增强、图像数据增强以及用于此的各种技术。
- en: Data augmentation
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: Data augmentation is the process of creating new data points from the existing
    data points by applying various transformations such as rotation, translation,
    and scaling. Data augmentation is used to increase the size of the training dataset
    and improve the generalizability and accuracy of the model by helping the model
    to learn more features and patterns in the data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强是通过应用各种变换（如旋转、平移和缩放）从现有数据点创建新数据点的过程。数据增强通过帮助模型在数据中学习更多特征和模式，用于增加训练数据集的大小并提高模型的泛化能力和准确性。
- en: Image data augmentation
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像数据增强
- en: Image data augmentation is a technique of augmenting the image dataset to improve
    the accuracy of the model. The following is a selection of the techniques that
    can be used for image data augmentation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据增强是一种增强图像数据集的技术，以提高模型的准确性。以下是一些可用于图像数据增强的技术选择。
- en: Image rotation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像旋转
- en: 'Image rotation is a technique where an image is rotated by a certain angle.
    This technique is used to increase the size of the training dataset and improve
    the model’s ability to recognize objects from different angles. The Python code
    for image rotation is as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图像旋转是一种技术，其中图像通过一定角度进行旋转。这项技术用于增加训练数据集的大小并提高模型从不同角度识别对象的能力。图像旋转的Python代码如下：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code, we load the image from the image path and rotate it by
    a given number of degrees. This creates a new dataset for the same image from
    different angles and improves model training.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们从图像路径加载图像，并以给定的度数旋转它。这为同一图像从不同角度创建了一个新的数据集，并提高了模型训练的效果。
- en: Image translation
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像平移
- en: 'Image translation is a technique where an image is shifted horizontally or
    vertically by a certain amount of pixels. This technique is used to increase the
    size of the training dataset and improve the model’s ability to recognize objects
    in different positions. The Python code for image translation is as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图像翻译是一种技术，其中图像通过一定的像素量水平或垂直移动。这种技术用于增加训练数据集的大小并提高模型识别不同位置对象的能力。图像翻译的Python代码如下：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding code, we define a Python function that shifts the image by
    a certain amount of pixels.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们定义了一个Python函数，该函数通过一定的像素量移动图像。
- en: Image scaling
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像缩放
- en: 'Image scaling is an augmentation technique where an image is scaled up or down
    by a certain factor. This technique is used to increase the size of the training
    dataset and improve the model’s ability to recognize objects at different scales.
    The Python code for image scaling is as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图像缩放是一种增强技术，其中图像通过一定的因子放大或缩小。这种技术用于增加训练数据集的大小并提高模型识别不同尺度对象的能力。图像缩放的Python代码如下：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code, we change the image size by multiplying the image by
    a scale factor in a Python function. Next, let’s see how to implement an SVM with
    data augmentation using the CIFAR-10 dataset.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们通过在Python函数中将图像乘以一个缩放因子来改变图像大小。接下来，让我们看看如何使用CIFAR-10数据集实现具有数据增强的SVM。
- en: Implementing an SVM with data augmentation in Python
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Python中实现具有数据增强的SVM
- en: In this section, we will provide a step-by-step guide to implement an SVM with
    data augmentation in Python using the CIFAR-10 dataset. We will start by introducing
    the CIFAR-10 dataset and then move on to loading the dataset in Python. We will
    then preprocess the data for SVM training and implement an SVM with the default
    hyperparameters and dataset. Next, we train and evaluate the performance of the
    SVM with an augmented dataset, showing that the performance of the SVM improves
    on the augmented dataset.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将提供使用CIFAR-10数据集在Python中实现具有数据增强的SVM的逐步指南。我们将首先介绍CIFAR-10数据集，然后转到在Python中加载数据集。然后，我们将对数据进行SVM训练的前处理，并实现具有默认超参数和数据集的SVM。接下来，我们将使用增强数据集训练和评估SVM的性能，以表明SVM在增强数据集上的性能有所提高。
- en: Introducing the CIFAR-10 dataset
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍CIFAR-10数据集
- en: 'The CIFAR-10 dataset is a commonly used image classification dataset that consists
    of 60,000 32x32 color images in 10 classes. The classes are: airplane, automobile,
    bird, cat, deer, dog, frog, horse, ship, and truck. The dataset is divided into
    50,000 training images and 10,000 testing images. The dataset is preprocessed
    in a way that the training set and test set have an equal number of images from
    each class.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10数据集是一个常用的图像分类数据集，包含10个类别的60,000个32x32彩色图像。这些类别包括：飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。数据集分为50,000个训练图像和10,000个测试图像。数据集经过预处理，使得训练集和测试集中每个类别的图像数量相等。
- en: Loading the CIFAR-10 dataset in Python
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Python中加载CIFAR-10数据集
- en: 'To load the CIFAR-10 dataset in Python, we will use the `cifar10` module from
    the Keras library. If you don’t have Keras installed, you can install it using
    the following command:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中加载CIFAR-10数据集时，我们将使用Keras库中的`cifar10`模块。如果您还没有安装Keras，可以使用以下命令进行安装：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once you have installed Keras, you can load the CIFAR-10 dataset using the
    following code:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Keras后，您可以使用以下代码加载CIFAR-10数据集：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `cifar10.load_data()` function returns two tuples: `(x_train, y_train)`
    and `(x_test, y_test)`. The `x_train` and `x_test` tuples contain the input images,
    while the `y_train` and `y_test` tuples contain the corresponding class labels
    for the input images.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`cifar10.load_data()`函数返回两个元组：`(x_train, y_train)`和`(x_test, y_test)`。`x_train`和`x_test`元组包含输入图像，而`y_train`和`y_test`元组包含输入图像对应的类标签。'
- en: Preprocessing the data for SVM training
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对SVM训练数据进行预处理
- en: In this section, we will first convert the input images from 3D matrices to
    2D matrices. We will also normalize the pixel values of the input images to be
    between 0 and 1\. Finally, we will reshape the input images and convert the class
    labels to one-hot encoded vectors.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先将输入图像从3D矩阵转换为2D矩阵。我们还将输入图像的像素值归一化到0到1之间。最后，我们将输入图像重塑并转换类标签为one-hot编码向量。
- en: 'The `reshape()` function is used to reshape the input images from 3D matrices
    to 2D matrices. The `-1` argument tells the function to infer the number of columns
    based on the number of rows and the size of each row:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`reshape()`函数将输入图像从3D矩阵重塑为2D矩阵。`-1`参数告诉函数根据行数和每行的大小推断列数：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The pixel values of the input images are normalized to be between 0 and 1 by
    dividing them by 255, which is the maximum pixel value:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像的像素值通过除以255（这是最大像素值）进行归一化，使其介于0和1之间：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `to_categorical()` function is used to convert the class labels to one-hot
    encoded vectors. The `num_classes` variable is set to `10`, which is the number
    of classes in the CIFAR-10 dataset:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`to_categorical()`函数将类别标签转换为独热编码向量。`num_classes`变量设置为`10`，这是CIFAR-10数据集中的类别数量：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Implementing an SVM with the default hyperparameters
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用默认超参数实现SVM
- en: 'Hyperparameters in SVMs are parameters that are not learned from the data but
    are set prior to the training process. They control the behavior of the SVM model
    and can significantly impact its performance. Here are some important hyperparameters
    in SVM:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: SVM中的超参数是在训练过程之前设置的参数，而不是从数据中学习的参数。它们控制SVM模型的行为，并可能对其性能产生重大影响。以下是SVM中的一些重要超参数：
- en: '**Kernel**: The kernel function determines the type of decision boundary used
    by the SVM. Common kernel functions include the linear, polynomial, **radial basis
    function** (**RBF**), and sigmoid function. The choice of kernel depends on the
    data and problem at hand.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核函数**：核函数决定了SVM使用的决策边界的类型。常见的核函数包括线性、多项式、**径向基函数**（**RBF**）和sigmoid函数。核函数的选择取决于数据和问题。'
- en: '**Regularization parameter (C)**: Regularization is a technique used to prevent
    overfitting or underfitting of the model. Regularization methods help to control
    the complexity of the model and improve its generalization on unseen data.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化参数（C）**：正则化是一种用于防止模型过拟合或欠拟合的技术。正则化方法有助于控制模型的复杂性并提高其在未见数据上的泛化能力。'
- en: For a binary classification problem, the decision boundary is a hyperplane that
    separates data into two classes. The margin is the distance between this hyperplane
    and the nearest data point from either class. The “width” of the margin is the
    actual numerical value of the distance or gap between the decision boundary and
    the nearest data point.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于二元分类问题，决策边界是一个将数据分为两个类别的超平面。边缘是此超平面与任一类别最近的数据点之间的距离。边缘的“宽度”是决策边界与最近数据点之间距离的实际数值或间隙。
- en: A wider margin implies a larger separation between classes, providing more room
    for potential misclassifications without affecting the decision boundary. The
    regularization parameter, often denoted as C, controls the trade-off between achieving
    a low training error rate and maintaining a wide margin. A smaller C value allows
    for more misclassifications but results in a larger margin, while a larger C value
    tries to minimize misclassifications at the cost of a narrower margin.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 较大的边缘意味着类别之间的分离更大，为潜在的误分类提供了更多空间，而不会影响决策边界。正则化参数，通常表示为C，控制了实现低训练错误率与保持宽边缘之间的权衡。较小的C值允许更多的误分类，但会导致更大的边缘，而较大的C值试图以牺牲较窄的边缘为代价来最小化误分类。
- en: '**Gamma (for RBF kernels)**: The gamma parameter influences the shape of the
    decision boundary for SVMs with the RBF kernel. It determines the reach of each
    training sample and affects the smoothness of the decision boundary. Higher gamma
    values tend to result in more complex decision boundaries.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伽马（对于RBF核）**：伽马参数影响具有RBF核的SVM决策边界的形状。它决定了每个训练样本的可达范围并影响决策边界的平滑度。较高的伽马值往往会导致更复杂的决策边界。'
- en: '**Degree (for polynomial kernels)**: The degree parameter specifies the degree
    of the polynomial kernel function. It determines the nonlinearity of the decision
    boundary. Higher degree values allow for more complex decision boundaries but
    may increase the risk of overfitting.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**度（对于多项式核）**：度参数指定多项式核函数的度。它决定了决策边界的非线性。较高的度值允许更复杂的决策边界，但可能增加过拟合的风险。'
- en: These hyperparameters need to be carefully tuned to achieve the best performance
    of the SVM model. Grid search, random search, or other optimization techniques
    can be employed to explore different combinations of hyperparameter values and
    select the optimal set.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些超参数需要仔细调整以达到SVM模型的最佳性能。可以使用网格搜索、随机搜索或其他优化技术来探索不同超参数值的组合，并选择最佳集。
- en: To implement an SVM with the default hyperparameters, we will use the `svm.SVC`
    class from the scikit-learn library. We will first create an instance of the `SVC`
    class and then fit the training data to the classifier.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现具有默认超参数的SVM，我们将使用scikit-learn库中的`svm.SVC`类。我们首先创建一个`SVC`类的实例，然后将训练数据拟合到分类器。
- en: 'An instance of the `SVC` class is created using `svm.SVC()`. By not specifying
    any hyperparameters, it uses the default values for the kernel, regularization
    parameter (C), and other relevant parameters:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`svm.SVC()`创建了一个`SVC`类的实例。通过不指定任何超参数，它使用默认的核函数、正则化参数（C）和其他相关参数的值：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `fit()` function is used to fit the training data to the classifier:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()`函数用于将训练数据拟合到分类器：'
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Evaluating SVM on the original dataset
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估原始数据集上的SVM
- en: We evaluate the performance of the original dataset to compare the performance
    with the augmented dataset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估原始数据集的性能，以比较与增强数据集的性能。
- en: 'To evaluate the performance of SVM on the original dataset, we will use the
    `predict()` function to predict the class labels of the test data and then use
    the `accuracy_score()` function from the scikit-learn library to calculate the
    accuracy of the classifier:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估原始数据集上SVM的性能，我们将使用`predict()`函数预测测试数据的类别标签，然后使用scikit-learn库中的`accuracy_score()`函数计算分类器的准确率：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `predict()` function is used to predict the class labels of the test data.
    The `accuracy_score()` function is used to calculate the accuracy of the classifier
    by comparing the predicted class labels to the actual class labels.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict()`函数用于预测测试数据的类别标签。`accuracy_score()`函数通过比较预测的类别标签和实际类别标签来计算分类器的准确率。'
- en: The accuracy of the SVM model on the test dataset is around `47.97%`, which
    is not very good. This indicates that the SVM model is not able to learn all the
    important features and patterns in the original dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: SVM模型在测试数据集上的准确率约为`47.97%`，这并不很好。这表明SVM模型无法学习原始数据集中所有的重要特征和模式。
- en: Implementing an SVM with an augmented dataset
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现使用增强数据集的SVM
- en: 'To implement SVM with data augmentation, we will use the `ImageDataGenerator`
    class from the Keras library to generate new training data. We will first create
    an instance of the `ImageDataGenerator` class and then use the `flow()` function
    to generate new batches of training data:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现具有数据增强的SVM，我们将使用Keras库中的`ImageDataGenerator`类来生成新的训练数据。我们首先创建一个`ImageDataGenerator`类的实例，然后使用`flow()`函数生成新的训练数据批次：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `ImageDataGenerator()` function creates an instance of the `ImageDataGenerator`
    class. The `rotation_range`, `width_shift_range`, `height_shift_range`, `shear_range`,
    `zoom_range`, and `horizontal_flip` arguments are used to specify the types of
    data augmentation to be applied to the training data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`ImageDataGenerator()`函数创建了一个`ImageDataGenerator`类的实例。`rotation_range`、`width_shift_range`、`height_shift_range`、`shear_range`、`zoom_range`和`horizontal_flip`参数用于指定要应用于训练数据的增强数据类型。'
- en: The `flow()` function is used to generate new batches of training data from
    the original training data and the `ImageDataGenerator` object.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`flow()`函数用于从原始训练数据和`ImageDataGenerator`对象生成新的训练数据批次。'
- en: Training the SVM on augmented data
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在增强数据上训练SVM
- en: 'To train SVM on augmented data, we will use the `partial_fit()` function of
    the `SVC` class to train the classifier on each batch of training data generated
    by the `ImageDataGenerator` object:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要在增强数据上训练支持向量机（SVM），我们将使用`SVC`类的`partial_fit()`函数，在每个由`ImageDataGenerator`对象生成的训练数据批次上训练分类器：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `classes` argument is used to specify the unique classes in the training
    data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`classes`参数用于指定训练数据中的唯一类别。'
- en: Evaluating the SVM’s performance on the augmented dataset
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估增强数据集上SVM的性能
- en: 'To evaluate the performance of the SVM on the augmented dataset, we will again
    use the `predict()` function to predict the class labels of the test data and
    then use the `accuracy_score()` function to calculate the accuracy of the classifier
    by comparing the predicted class labels to the actual class labels:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估SVM在增强数据集上的性能，我们将再次使用`predict()`函数来预测测试数据的类别标签，然后使用`accuracy_score()`函数通过比较预测的类别标签和实际类别标签来计算分类器的准确度：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The accuracy of the SVM model on the augmented test dataset is around `54.75%`,
    which is better than the previous accuracy. This indicates that the SVM model
    is able to learn more important features and patterns in the augmented dataset,
    and is able to generalize better to new data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在增强的测试数据集上，SVM模型的准确率约为`54.75%`，这比之前的准确率要好。这表明SVM模型能够在增强数据集中学习到更多重要的特征和模式，并且能够更好地泛化到新数据。
- en: To summarize, in this section, we have discussed the importance of data augmentation
    in training SVMs for image classification. We have used the CIFAR-10 dataset to
    illustrate the impact of data augmentation on the performance of the SVM model.
    We have also provided Python code examples for loading the CIFAR-10 dataset, training
    an SVM model on the original dataset, and training an SVM model on the augmented
    dataset.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在本节中，我们讨论了数据增强在训练用于图像分类的SVM中的重要性。我们使用CIFAR-10数据集来说明数据增强对SVM模型性能的影响。我们还提供了加载CIFAR-10数据集、在原始数据集上训练SVM模型以及在增强数据集上训练SVM模型的Python代码示例。
- en: The results show that data augmentation can improve the performance of SVM models
    on image classification tasks. By applying random rotations, translations, and
    scaling, we can generate new images that the SVM model can use to learn more features
    and patterns. This enables the SVM model to generalize better to new data and
    achieve better accuracy.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，数据增强可以提高SVM模型在图像分类任务上的性能。通过应用随机旋转、平移和缩放，我们可以生成新的图像，SVM模型可以使用这些图像来学习更多特征和模式。这使得SVM模型能够更好地泛化到新数据并实现更高的准确度。
- en: In the next section, we will see how to implement the SVM with data augmentation
    using the MNIST handwritten digits dataset.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何使用MNIST手写数字数据集实现具有数据增强的SVM。
- en: Image classification using the SVM with data augmentation on the MNIST dataset
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在MNIST数据集上使用具有数据增强的SVM进行图像分类
- en: 'Let us see how we can apply data augmentation for image classification using
    an SVM with the MNIST dataset. All the steps are similar to the previous example
    with the CIFAR-10 dataset, except the dataset itself:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用MNIST数据集和SVM进行图像分类应用数据增强。所有步骤都与之前使用CIFAR-10数据集的示例类似，只是数据集本身不同：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As stated, this code is similar to the previous example, except that we are
    now using the MNIST dataset and the images are grayscale and of size 28x28\. We
    have also modified the input shape of the SVM model and the image data generator
    to accommodate the new image size and color channel.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，此代码与之前的示例类似，只是我们现在使用的是MNIST数据集，图像是灰度且大小为28x28。我们还修改了SVM模型的输入形状和图像数据生成器，以适应新的图像大小和颜色通道。
- en: The results show that data augmentation can also improve the performance of
    SVM models on the MNIST dataset. By applying random rotations, translations, and
    scaling, we can generate new images that the SVM model can use to learn more features
    and patterns. This enables the SVM model to generalize better to new data and
    achieve better accuracy.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，数据增强也可以提高SVM模型在MNIST数据集上的性能。通过应用随机旋转、平移和缩放，我们可以生成新的图像，SVM模型可以使用这些图像来学习更多特征和模式。这使得SVM模型能够更好地泛化到新数据并实现更高的准确度。
- en: In addition, we have used grid search to find the optimal hyperparameters for
    the SVM model. This is important because the performance of SVM models is highly
    dependent on the choice of hyperparameters. By tuning the hyperparameters using
    grid search, we can improve the accuracy of the SVM model even further.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还使用网格搜索来找到SVM模型的最佳超参数。这很重要，因为SVM模型的表现高度依赖于超参数的选择。通过使用网格搜索调整超参数，我们可以进一步提高SVM模型的准确度。
- en: Overall, this example code demonstrates the effectiveness of data augmentation
    in improving the performance of SVM models on image classification tasks. It also
    highlights the importance of hyperparameter tuning using grid search to achieve
    the best possible accuracy.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这个示例代码展示了数据增强在提高SVM模型在图像分类任务性能方面的有效性。它还强调了使用网格搜索进行超参数调整以实现最佳准确度的重要性。
- en: To summarize, data augmentation is a powerful technique for improving the performance
    of machine learning models on image classification tasks. By generating new images
    that the model can use to learn more features and patterns, we can improve the
    generalization ability of the model and achieve better accuracy. SVM models are
    particularly well suited for image classification tasks and can benefit greatly
    from data augmentation. With the help of Python libraries such as scikit-learn
    and TensorFlow, we can easily implement SVM models with data augmentation and
    achieve state-of-the-art performance on image classification tasks.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，数据增强是一种强大的技术，可以提高机器学习模型在图像分类任务上的性能。通过生成模型可以用来学习更多特征和模式的新图像，我们可以提高模型的一般化能力，并实现更高的准确率。SVM模型特别适合图像分类任务，并且可以从数据增强中受益良多。借助Python库如scikit-learn和TensorFlow，我们可以轻松实现带有数据增强的SVM模型，并在图像分类任务上实现最先进的性能。
- en: Next, let us see how to implement convolutional neural networks with augmented
    training data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何使用增强训练数据实现卷积神经网络。
- en: Convolutional neural networks using augmented image data
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用增强图像数据的卷积神经网络
- en: '**Convolutional Neural Networks** (**CNNs**) have revolutionized the field
    of computer vision by demonstrating exceptional performance in various image-related
    tasks such as object detection, image classification, and segmentation. However,
    the availability of large, annotated datasets for training CNNs is often a challenge.
    Fortunately, one effective approach to overcome this limitation is through the
    use of **image data** **augmentation** techniques.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）通过在诸如目标检测、图像分类和分割等图像相关任务中展现出卓越的性能，彻底改变了计算机视觉领域。然而，为CNNs训练提供大量、标注的数据集通常是一个挑战。幸运的是，一种有效克服这一局限性的方法是通过使用**图像数据****增强**技术。'
- en: Let’s start from scratch and explain what CNNs are and how they work. Imagine
    you have a picture, say a photo of a cat, and you want to teach a computer how
    to recognize that it’s a cat. CNNs are like a special type of computer program
    that helps computers understand and recognize things in images, just like how
    you recognize objects in photos.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从零开始，解释CNNs是什么以及它们是如何工作的。想象你有一张图片，比如说一张猫的照片，你想要教计算机如何识别它是一张猫。CNNs就像一种特殊的计算机程序，帮助计算机理解和识别图像中的事物，就像你识别照片中的物体一样。
- en: An image is made up of tiny dots called pixels. Each pixel has a color, and
    when you put them all together, you get an image. The more pixels you have, the
    more detailed the image is. When you look at a picture, your brain doesn’t try
    to understand it all at once. Instead, it focuses on small parts, like the shape
    of an ear or the color of an eye. This is how we recognize things. We break the
    big picture into small pieces and understand them one by one.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由称为像素的小点组成。每个像素都有颜色，当你把它们全部放在一起时，你就得到了一张图像。像素越多，图像越详细。当你看一张图片时，你的大脑不会试图一次性理解它。相反，它会专注于小部分，比如耳朵的形状或眼睛的颜色。这就是我们识别事物的方式。我们把大图像分解成小块，并逐一理解它们。
- en: CNNs work a bit like the human brain, breaking images down into small parts.
    These small parts are called “features” or “filters.” Imagine these filters as
    tiny windows that move across the picture. These windows look at a small part
    of the image at a time and learn what’s important in it.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）的工作原理有点像人脑，将图像分解成小块。这些小块被称为“特征”或“过滤器”。想象这些过滤器就像在图片上移动的小窗户。这些窗户一次查看图像的一小部分，并学习其中的重要信息。
- en: How CNNs work
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNNs的工作原理
- en: 'Let us understand how CNNs work for the desired output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解CNNs是如何为期望的输出工作的：
- en: '**Convolution**: This is the first step. It’s like moving the small window
    (filter) over the picture. The filter checks the colors and shapes in the area
    it’s looking at and learns what’s important.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**卷积**：这是第一步。它就像在图片上移动一个小窗口（过滤器）。过滤器检查它所查看区域中的颜色和形状，并学习其中的重要信息。'
- en: '**Pooling**: After the different parts of the image have been looked at, the
    CNN doesn’t need all the details. Pooling is like taking a summary of what it’s
    seen. It simplifies things, but we don’t lose the important parts.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**池化**：在查看图像的不同部分之后，CNN不需要所有细节。池化就像是对它所看到的内容做一个总结。它简化了事物，但我们没有丢失重要的部分。'
- en: '**Fully connected layers**: After looking at the many small parts and summarizing
    them, everything is connected together next. It’s like putting the pieces of a
    puzzle together to see the whole picture. This helps the CNN understand the entire
    image and make a final decision on what is depicted in it.'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**全连接层**：在观察了许多小部分并总结它们之后，接下来将所有部分连接在一起。这就像把拼图碎片拼在一起，以看到整个画面。这有助于卷积神经网络理解整个图像，并对其中的内容做出最终判断。'
- en: After the convolutional layers have processed the image by extracting various
    features and patterns, the fully connected layers play a crucial role in bringing
    all the information together to make a comprehensive decision about the content
    of the image. This process is akin to assembling the pieces of a puzzle, where
    each piece corresponds to a specific feature detected by the convolutional layers.
    By connecting these pieces, the network gains a holistic understanding of the
    image.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在卷积层通过提取各种特征和模式处理图像之后，全连接层在将所有信息汇总以对图像内容做出全面决策方面发挥着关键作用。这个过程类似于组装拼图碎片，其中每个碎片对应于卷积层检测到的特定特征。通过连接这些碎片，网络获得了对图像的整体理解。
- en: However, as powerful as fully connected layers are, there is a risk of overfitting,
    a situation where the model becomes too specialized in the training data and performs
    poorly on new, unseen data. To mitigate this, regularization techniques are often
    employed.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，尽管全连接层非常强大，但它们也存在过拟合的风险，即模型在训练数据上变得过于专业化，在新数据上表现不佳。为了减轻这种风险，通常会采用正则化技术。
- en: '**Regularization in fully connected layers**: Regularization is a set of techniques
    used to prevent overfitting and enhance the generalization capabilities of a model.
    In the context of fully connected layers, regularization methods are applied to
    control the complexity of the model and avoid relying too heavily on specific
    features present in the training data.'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**全连接层中的正则化**：正则化是一组用于防止过拟合并增强模型泛化能力的技巧。在全连接层的上下文中，正则化方法被应用于控制模型的复杂性，并避免过度依赖训练数据中存在的特定特征。'
- en: '**Training a CNN**: To teach a CNN to recognize cats, you’d show it lots of
    cat pictures. It looks at them, learns the important features, and gets better
    over time at recognizing them. It also needs to see pictures of things that are
    not cats, so it can tell the difference.'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练卷积神经网络**：为了教会卷积神经网络识别猫，你需要向它展示大量的猫图片。它观察它们，学习重要的特征，并随着时间的推移在识别它们方面变得更好。它还需要看到不是猫的图片，这样它才能区分它们。'
- en: '**Making predictions**: Once a CNN is trained, you can show it a new picture,
    and it will try to find the important features just like it learned. If it finds
    enough cat-like features, it will say, “Hey, that’s a cat!”'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**做出预测**：一旦卷积神经网络被训练，你可以向它展示一张新图片，它将尝试找到它学习到的重要特征。如果它找到了足够的猫类特征，它会说：“嘿，那是一只猫！”'
- en: So, in simple terms, a CNN is like a computer program that learns to recognize
    things in pictures by looking at small parts of the image, finding important features,
    and making decisions based on those features.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，简单来说，卷积神经网络就像一个计算机程序，通过观察图像的小部分、寻找重要特征并基于这些特征做出决策来学习识别图像中的事物。
- en: As we’ve seen, a CNN’s architecture constitutes convolution, pooling, and fully
    connected layers. The architecture specifies how the model is structured, including
    the number of layers, the size of filters, and the connections between neurons.
    The architecture guides how the learned weights and features are used to process
    images and make predictions.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，卷积神经网络的架构由卷积、池化和全连接层组成。架构指定了模型的构建方式，包括层数、滤波器的大小以及神经元之间的连接。架构指导了如何使用学习到的权重和特征来处理图像和做出预测。
- en: 'So, the final model is, in essence, a combination of the architecture, the
    learned weights, and the learned features. Let’s break down a couple of these
    elements:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最终的模型本质上是由架构、学习到的权重和学习到的特征组合而成的。让我们分解其中的一些元素：
- en: '**Learned weights**: These are the parameters that the CNN has learned during
    the training process. The model adjusts these weights to make accurate predictions.
    These weights are essentially the “knowledge” the model gains during training.
    They represent how important certain features are for making decisions.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习到的权重**：这些是卷积神经网络在训练过程中学习到的参数。模型调整这些权重以做出准确的预测。这些权重基本上是模型在训练过程中获得的“知识”。它们代表了某些特征对于做出决策的重要性。'
- en: '**Learned features**: Features, in the context of a CNN, are visual patterns
    and characteristics of images. They are representations of important information
    within the image. These features are not directly visible to us but are learned
    by the network through the layers of convolution and pooling. Features are abstract
    representations of the image that help the model recognize patterns and objects.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习到的特征**：在 CNN 的上下文中，特征是图像的视觉模式和特征。它们是图像中重要信息的表示。这些特征对我们来说不是直接可见的，而是通过网络层的卷积和池化学习得到的。特征是图像的抽象表示，有助于模型识别模式和对象。'
- en: In practice, these learned weights and features are stored in the model’s parameters.
    When you save a trained CNN model, you are saving these parameters, which can
    be used to make predictions on new, unseen images. The model takes an image as
    input, processes it through its layers, and uses the learned weights and features
    to make predictions, such as classifying objects in the image or detecting specific
    patterns.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这些学习到的权重和特征存储在模型的参数中。当你保存一个训练好的 CNN 模型时，你正在保存这些参数，这些参数可以用于对新、未见过的图像进行预测。模型接收一个图像作为输入，通过其层进行处理，并使用学习到的权重和特征进行预测，例如对图像中的对象进行分类或检测特定模式。
- en: We will now delve into the powerful combination of CNNs and image data augmentation.
    By artificially augmenting the data, CNNs can be exposed to a broader range of
    variations during training to help them generalize better to unseen images.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将深入探讨 CNN 和图像数据增强的强大组合。通过人工增强数据，CNN 可以在训练过程中接触到更广泛的变化范围，从而帮助它们更好地泛化到未见过的图像。
- en: Some of the benefits and considerations of using image data augmentation are
    reducing overfitting, enhancing model robustness, and improving generalization
    performance. Whether you are a beginner or an experienced practitioner, this section
    serves as a comprehensive guide to understanding and implementing image data augmentation
    in the context of CNNs, assisting you in taking your computer vision projects
    to new heights.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用图像数据增强的一些好处和考虑因素包括减少过拟合、增强模型鲁棒性和提高泛化性能。无论你是初学者还是有经验的从业者，本节都作为理解并实现 CNN 上下文中的图像数据增强的全面指南，帮助你将计算机视觉项目提升到新的高度。
- en: Practical example of a CNN using data augmentation
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN 使用数据增强的实用示例
- en: 'Let us see how to implement image data augmentation on a CNN. To do so, you
    can follow these steps:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在 CNN 上实现图像数据增强。为此，你可以遵循以下步骤：
- en: '**Step 1**: Start by importing the necessary libraries, including Keras and
    NumPy:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1**：首先导入必要的库，包括 Keras 和 NumPy：'
- en: '[PRE15]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`ImageDataGenerator` object and specify the desired data augmentation techniques:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`ImageDataGenerator` 对象并指定所需的数据增强技术：'
- en: '[PRE16]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `ImageDataGenerator` object will generate batches of augmented data using
    the specified data augmentation techniques. In this example, we are using rotation,
    width and height shifts, and horizontal flipping.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`ImageDataGenerator` 对象将使用指定的数据增强技术生成增强数据的批次。在本例中，我们使用了旋转、宽度和高度偏移以及水平翻转。'
- en: '**Step 3**: Load the original dataset and split it into training and validation
    sets:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3**：加载原始数据集并将其分为训练集和验证集：'
- en: '[PRE17]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, we are using the `flow_from_directory()` function to load the original
    dataset from the specified directory. We also specify the target size of the images,
    the batch size, and the class mode (categorical in this case). We split the data
    into training and validation sets using the `subset` parameter.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 `flow_from_directory()` 函数从指定的目录加载原始数据集。我们还指定了图像的目标大小、批处理大小和类别模式（在本例中为分类）。我们使用
    `subset` 参数将数据分为训练集和验证集。
- en: 'In the provided code snippet, the `flow_from_directory` function is used to
    generate a data generator to load images from a directory. Let’s break down the
    parameters:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在提供的代码片段中，使用了 `flow_from_directory` 函数来生成数据生成器，用于从目录中加载图像。让我们分解一下参数：
- en: '`''/path/to/dataset''`: This is the path to the directory containing the dataset.
    The function will look for subdirectories inside this directory, where each subdirectory
    represents a different class or category.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''/path/to/dataset''`：这是包含数据集的目录的路径。函数将在该目录内查找子目录，其中每个子目录代表不同的类别或类别。'
- en: '`target_size=(224, 224)`: `target_size` is the size to which all images will
    be resized during loading. In this case, each image will be resized as a square
    with dimensions of 224x224 pixels. Standardizing the image size is important for
    consistency and compatibility with neural network models, especially when using
    pre-trained models that expect a specific input size.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_size=(224, 224)`: `target_size` 是在加载过程中所有图像将被调整的大小。在这种情况下，每张图像将被调整为
    224x224 像素的正方形。标准化图像大小对于一致性和与神经网络模型的兼容性很重要，尤其是在使用期望特定输入大小的预训练模型时。'
- en: '`batch_size=32`: `batch_size` determines the number of images loaded and processed
    in each iteration during training or validation. A larger batch size can lead
    to faster training but may require more memory. Smaller batch sizes are often
    used when memory is limited or for fine-tuning models. It also affects the gradient
    update during training, impacting the stability and convergence of the training
    process.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size=32`: `batch_size` 决定了在训练或验证过程中每次迭代中加载和处理的图像数量。较大的批处理大小可以加快训练速度，但可能需要更多的内存。当内存有限或用于微调模型时，通常使用较小的批处理大小。它还会影响训练过程中的梯度更新，从而影响训练过程的稳定性和收敛性。'
- en: '`class_mode=''categorical''`: `class_mode` specifies how the target classes
    are represented. In this case, it is set to `categorical`, indicating that the
    labels are one-hot encoded (a binary matrix representation of class membership).
    Other possible values include `binary` for binary classification, `sparse` for
    integer-encoded class labels, and `None` for no labels (used for test datasets).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_mode=''categorical''`: `class_mode` 指定了目标类别的表示方式。在这种情况下，它被设置为 `categorical`，表示标签是一维编码（类成员关系的二进制矩阵表示）。其他可能的值包括
    `binary` 用于二分类，`sparse` 用于整数编码的类别标签，以及 `None` 用于没有标签（用于测试数据集）。'
- en: '`subset=''validation''`: Subset is used to specify whether the generator is
    for the training set or the validation set. In this case, it is set to `validation`,
    indicating that the generator is for the validation set. When using subset, make
    sure the dataset directory contains subdirectories like `train` and `validation`
    to facilitate the split.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subset=''validation''`: 子集用于指定生成器是为训练集还是验证集。在这种情况下，它被设置为 `validation`，表示生成器是为验证集。当使用子集时，请确保数据集目录包含
    `train` 和 `validation` 等子目录，以方便分割。'
- en: In summary, these parameters help configure the data generator to load and preprocess
    images from a directory. The choices made for target size, batch size, and class
    mode are often determined by the requirements of the machine learning model being
    used, the available computing resources, and the characteristics of the dataset.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这些参数有助于配置数据生成器以从目录中加载和预处理图像。针对目标大小、批处理大小和类模式的选项通常由所使用的机器学习模型的要求、可用的计算资源以及数据集的特征决定。
- en: '**Step 4**: Create a CNN model:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4**：创建一个 CNN 模型：'
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, we are creating a simple CNN model with four convolutional layers and
    one fully connected layer. We are using ReLU activation for the convolutional
    layers and softmax activation for the output layer. We also compile the model
    with the categorical cross-entropy loss function, the Adam optimizer, and the
    accuracy metric.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在创建一个简单的 CNN 模型，具有四个卷积层和一个全连接层。我们使用 ReLU 激活函数用于卷积层，softmax 激活函数用于输出层。我们还使用分类交叉熵损失函数、Adam
    优化器和准确度指标编译模型。
- en: 'In the preceding code snippet, a CNN model is being created using the Keras
    library. Let’s break down the components:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，正在使用 Keras 库创建一个 CNN 模型。让我们分解一下这些组件：
- en: '`activation=''relu''` is used for the convolutional and dense layers. ReLU
    is an activation function that introduces non-linearity to the model. It outputs
    the input directly if it is positive; otherwise, it outputs zero. ReLU is preferred
    for CNNs because it helps the model learn complex patterns and relationships in
    data. It is computationally efficient and mitigates the vanishing gradient problem.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation=''relu''` 用于卷积层和密集层。ReLU 是一个激活函数，它向模型引入非线性。如果输入为正，则直接输出输入；否则，输出零。ReLU
    由于有助于模型学习数据中的复杂模式和关系而受到 CNN 的青睐。它计算效率高，并减轻了梯度消失问题。'
- en: '**The effect of ReLU**: ReLU introduces non-linearity, enabling the model to
    learn complex features and relationships in the data. It helps address the vanishing
    gradient problem, promoting more efficient training by allowing the model to propagate
    gradients during backpropagation.'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**ReLU 的影响**：ReLU 引入了非线性，使模型能够学习数据中的复杂特征和关系。它有助于解决梯度消失问题，通过允许模型在反向传播过程中传播梯度，从而促进更有效的训练。'
- en: '`activation=''softmax''` is used for the output layer. Softmax is a function
    that converts raw scores (logits) into probabilities. It is often used in the
    output layer of a multi-class classification model. In this binary classification
    case (two classes), the softmax activation function normalizes the output scores
    for each class, assigning a probability to each class. The class with the highest
    probability is considered the model’s prediction. Softmax is useful for producing
    probability distributions over multiple classes, making it suitable for classification
    problems.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation=''softmax''` 用于输出层。Softmax 是一个将原始分数（logits）转换为概率的函数。它常用于多类分类模型的输出层。在这个二元分类案例（两类）中，softmax
    激活函数将每个类别的输出分数进行归一化，为每个类别分配一个概率。概率最高的类别被认为是模型的预测。Softmax 对于产生多个类别的概率分布很有用，使其适合分类问题。'
- en: '**The effect of Softmax**: Softmax converts raw model outputs into probability
    distributions over classes. It ensures that the predicted probabilities sum to
    1, facilitating a meaningful interpretation of the model’s confidence in each
    class. In binary classification, it is often used in conjunction with categorical
    cross-entropy loss.'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**Softmax 的影响**：Softmax 将原始模型输出转换为类别的概率分布。它确保预测概率之和为 1，便于对模型对每个类别的信心进行有意义的解释。在二元分类中，它通常与交叉熵损失一起使用。'
- en: Why should we use them? ReLU is chosen for its simplicity, computational efficiency,
    and effectiveness in training deep neural networks. Softmax is selected for the
    output layer to obtain class probabilities, which are valuable for interpreting
    and evaluating the model’s predictions.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们应该使用它们？ReLU 被选择是因为其简单性、计算效率和在训练深度神经网络中的有效性。Softmax 被选为输出层以获得类别概率，这对于解释和评估模型的预测非常有价值。
- en: In summary, ReLU and softmax activations contribute to the effectiveness of
    the CNN model by introducing non-linearity, promoting efficient training, and
    producing meaningful probability distributions for classification. They are widely
    used in CNNs for image classification tasks.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，ReLU 和 softmax 激活通过引入非线性、促进高效训练和产生有意义的分类概率分布，有助于 CNN 模型的有效性。它们在图像分类任务中广泛应用于
    CNN。
- en: 'In the provided code snippet, the model is compiled with three important components
    – categorical cross-entropy loss, the Adam optimizer, and the accuracy metric.
    Let’s delve into each of them:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在提供的代码片段中，模型通过三个重要组件进行编译——交叉熵损失、Adam 优化器和准确度指标。让我们深入了解每个组件：
- en: '`loss=''categorical_crossentropy''`):'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss=''categorical_crossentropy''`):'
- en: Categorical cross-entropy is a loss function commonly used for multi-class classification
    problems.
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉熵损失函数是常用于多类分类问题的损失函数。
- en: In this context, the model is designed for binary classification (two classes),
    but it uses categorical cross-entropy to handle a case where there are more than
    two classes. The target labels are expected to be one-hot-encoded.
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个背景下，模型是为二元分类（两类）设计的，但它使用交叉熵来处理有超过两类的情况。目标标签应为一热编码。
- en: The loss function measures the dissimilarity between the predicted probabilities
    (obtained from the softmax activation in the output layer) and the true class
    labels.
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数衡量预测概率（从输出层的 softmax 激活中获得）与真实类别标签之间的差异。
- en: The goal during training is to minimize this loss, effectively improving the
    model’s ability to make accurate class predictions.
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，目标是最小化这个损失，从而有效提高模型进行准确类别预测的能力。
- en: '`optimizer=''adam''`):'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer=''adam''`):'
- en: '**Adaptive Moment Estimation** (**Adam**) is an optimization algorithm widely
    used to train neural networks.'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应动量估计**（**Adam**）是一种广泛用于训练神经网络的优化算法。'
- en: It combines ideas from two other optimization algorithms – **Root Mean Square
    Propagation** (RMSprop) and Momentum.
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它结合了两种其他优化算法的思想——**均方根传播**（RMSprop）和动量。
- en: Adam adapts the learning rates of each parameter individually, making it well-suited
    for a variety of optimization problems.
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adam 适应每个参数的个别学习率，使其非常适合各种优化问题。
- en: It is known for its efficiency and effectiveness in training deep neural networks
    and is often a default choice for many applications.
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它以其在训练深度神经网络中的效率和有效性而闻名，并且通常是许多应用的默认选择。
- en: '`metrics=[''accuracy'']`):'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics=[''accuracy'']`):'
- en: Accuracy is a metric used to evaluate the performance of a classification model.
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率是用于评估分类模型性能的指标。
- en: In the context of binary classification, accuracy measures the proportion of
    correctly classified instances (both true positives and true negatives) among
    all instances.
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在二元分类的背景下，准确率衡量的是所有实例中正确分类的实例比例（包括真正的正例和真正的负例）。
- en: The accuracy metric is essential for assessing how well the model performs on
    the training and validation datasets.
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率指标对于评估模型在训练和验证数据集上的表现至关重要。
- en: While accuracy is a commonly used metric, it might not be sufficient for imbalanced
    datasets, where one class is much more prevalent than the other. In such cases,
    additional metrics such as precision, recall, or F1 score may be considered.
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然准确率是一个常用的指标，但它可能不足以用于不平衡的数据集，其中一类比另一类更为普遍。在这种情况下，可能需要考虑额外的指标，如精确率、召回率或F1分数。
- en: In summary, the choice of categorical cross-entropy loss, the Adam optimizer,
    and the accuracy metric during compilation reflects the best practices for training
    a binary classification model. These choices are based on their effectiveness
    in optimizing the model parameters, handling multi-class scenarios, and providing
    a straightforward evaluation of classification accuracy.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在编译过程中选择分类交叉熵损失、Adam优化器和准确率指标反映了训练二元分类模型的最佳实践。这些选择基于它们在优化模型参数、处理多类场景和提供分类准确度直接评估方面的有效性。
- en: '**Step 5**: Train the model using the augmented dataset:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 5**: 使用增强数据集训练模型：'
- en: '[PRE19]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We use the `fit()` function to train the model on the augmented dataset. We
    specify the training and validation generators, the number of steps per epoch,
    the validation steps, and the number of epochs.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`fit()`函数在增强数据集上训练模型。我们指定了训练和验证生成器、每个周期的步骤数、验证步骤数和周期数。
- en: 'In this code snippet, the `fit()` function is used to train the model on an
    augmented dataset. Let’s break down the key components:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，使用`fit()`函数在增强数据集上训练模型。让我们分解一下关键组件：
- en: '`train_generator`):The training generator is an instance of a data generator
    that generates batches of training data with augmentation on the fly in *Step
    3*. A data generator is a way to efficiently load and preprocess data in chunks
    during training rather than loading the entire dataset into memory. `train_generator`
    is responsible for providing the model with batches of augmented training data.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_generator`):训练生成器是一个数据生成器的实例，它在*步骤 3*中实时对训练数据进行增强。数据生成器是一种在训练过程中以块的形式高效加载和预处理数据的方法，而不是将整个数据集加载到内存中。`train_generator`负责为模型提供增强后的训练数据批次。'
- en: '`val_generator`): Similar to the training generator, the validation generator
    is an instance of a data generator that generates batches of validation data.
    The validation generator provides a separate set of data that the model has not
    seen during training. It helps assess the model’s generalization to unseen examples
    and prevents overfitting.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`val_generator`):与训练生成器类似，验证生成器是一个数据生成器的实例，它生成验证数据批次。验证生成器提供了一组模型在训练期间未见过的数据。它有助于评估模型对未见示例的泛化能力，并防止过拟合。'
- en: '`steps_per_epoch=train_generator.samples // 32`): `steps_per_epoch` specifies
    the number of batches of data to process in each epoch of training. It is calculated
    as the total number of samples in the training dataset divided by the batch size
    (`32` in this case). Each step involves a forward pass (prediction) and a backward
    pass (gradient computation and parameter updates) on a batch of data. A smaller
    `steps_per_epoch` value means that the model will see fewer batches in each epoch,
    potentially leading to faster training but with less exposure to the entire dataset.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`steps_per_epoch=train_generator.samples // 32`):`steps_per_epoch`指定每个训练周期中要处理的数据批次数量。它计算为训练数据集中样本总数除以批次大小（在本例中为`32`）。每个步骤涉及对数据批次的前向传递（预测）和反向传递（梯度计算和参数更新）。较小的`steps_per_epoch`值意味着模型在每个周期中看到的批次更少，这可能导致训练速度更快，但整个数据集的曝光度较低。'
- en: '`validation_steps=val_generator.samples // 32`): `validation_steps` is similar
    to `steps_per_epoch` but for the validation dataset. It determines the number
    of batches processed during each validation epoch. Like `steps_per_epoch`, it
    is calculated based on the total number of samples in the validation dataset divided
    by the batch size.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`validation_steps=val_generator.samples // 32`）：`validation_steps`与`steps_per_epoch`类似，但用于验证数据集。它决定了每个验证周期中处理的批次数。与`steps_per_epoch`一样，它是基于验证数据集中样本总数除以批大小来计算的。'
- en: '`epochs=10`): Epoch specifies the number of times the entire dataset is processed
    during training. Training for more epochs allows the model to learn from the data
    over multiple passes, potentially improving performance. However, training for
    too many epochs may lead to overfitting, where the model memorizes the training
    data but fails to generalize to new data.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs=10`）：周期数指定了在训练期间整个数据集被处理的次数。进行更多周期的训练允许模型在多次遍历数据中学习，从而可能提高性能。然而，过多的周期训练可能会导致过拟合，此时模型会记住训练数据，但无法泛化到新数据。'
- en: Adjusting the batch size, steps per epoch, and validation steps can impact the
    training speed and memory requirements. A larger batch size and more steps per
    epoch may lead to slower training but can be more memory-efficient. The number
    of epochs should be chosen carefully to balance model training and prevent overfitting.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 调整批大小、每个周期的步数和验证步数会影响训练速度和内存需求。较大的批大小和更多的每个周期步数可能会导致训练速度变慢，但可能更节省内存。应该仔细选择周期数，以平衡模型训练并防止过拟合。
- en: In summary, the settings provided to `fit()` control how the model is trained,
    the data it sees in each epoch, and the evaluation of the validation set. Properly
    tuning these settings is crucial to achieving good model performance and preventing
    issues such as overfitting.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，提供给`fit()`的设置控制了模型的训练方式、每个周期中模型看到的数据以及验证集的评估。正确调整这些设置对于实现良好的模型性能和防止过拟合等问题至关重要。
- en: By following these steps, you can implement supervised CNNs using image data
    augmentation in Keras. This can help improve the performance of your model and
    make it more robust to variations in the input data.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这些步骤，您可以使用Keras中的图像数据增强实现监督CNN。这可以帮助提高模型性能，并使其对输入数据的变更加鲁棒。
- en: CNN using image data augmentation with the CIFAR-10 dataset
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用CIFAR-10数据集进行图像数据增强的CNN
- en: 'Let us see some example Python code for a supervised CNN using image data augmentation
    with the CIFAR-10 dataset:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些使用CIFAR-10数据集进行图像数据增强的监督CNN的Python代码示例：
- en: '[PRE20]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The preceding code defines the architecture of a CNN using the Keras library.
    Let’s go through each line to understand the purpose and functionality of each
    component.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码定义了使用Keras库的CNN架构。让我们逐行分析，以了解每个组件的目的和功能。
- en: 'The following line creates a sequential model, which allows us to stack layers
    on top of each other sequentially:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 以下行创建了一个顺序模型，这允许我们按顺序堆叠层：
- en: '[PRE21]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following code snippet adds a 2D convolutional layer to the model. It has
    32 filters, a filter size of `(3, 3)`, the ReLU activation function, and the `''same''`
    padding. The `input_shape` parameter is set to the shape of the input data (`x_train`)
    without the batch dimension:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段向模型添加了一个2D卷积层。它有32个滤波器，滤波器大小为`(3, 3)`，ReLU激活函数，以及`'same'`填充。`input_shape`参数设置为输入数据（`x_train`）的形状，但不包括批维度：
- en: 'Let’s break down the following CNN code snippet to understand it more in depth:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入分析以下CNN代码片段：
- en: '[PRE22]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '**2D convolutional layer addition**: In deep learning for image processing,
    convolutional layers are crucial to learning hierarchical features from input
    images. Convolutional layers are used to detect local patterns in the input data.
    Each filter in the convolutional layer learns to recognize different features
    or patterns. The code adds a layer to the neural network model, and specifically,
    it’s a 2D convolutional layer.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**2D卷积层添加**：在图像处理的深度学习中，卷积层对于从输入图像中学习层次特征至关重要。卷积层用于检测输入数据中的局部模式。卷积层中的每个滤波器都学习识别不同的特征或模式。代码向神经网络模型添加了一层，具体来说，是一个2D卷积层。'
- en: 'The convolutional layer has the following configurations:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层有以下配置：
- en: '**Filters**: There are 32 filters. Filters are small grids that slide over
    the input data to detect patterns or features.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滤波器**：有32个滤波器。滤波器是滑动在输入数据上以检测模式或特征的小网格。'
- en: '**Filter size**: Each filter has a size of (3, 3). This means it considers
    a 3x3 grid of pixels at some point during the convolution operation capturing
    local information.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滤波器大小**：每个滤波器的大小为（3, 3）。这意味着在卷积操作过程中，它考虑了某个点的3x3像素网格，以捕获局部信息。'
- en: '**Activation function**: The ReLU activation function is applied element-wise
    to the output of each convolutional operation. ReLU introduces non-linearity,
    allowing the model to learn complex patterns.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**：ReLU激活函数逐元素应用于每个卷积操作的输出。ReLU引入了非线性，使模型能够学习复杂的模式。'
- en: '`Same` padding is used. Padding is a technique to preserve spatial dimensions
    after convolution preventing information loss at the edges of the image. `Same`
    padding pads the input so that the output has the same spatial dimensions as the
    it.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`Same`填充。填充是一种在卷积后保持空间维度的技术，防止在图像边缘丢失信息。`Same`填充填充输入，使得输出具有与输入相同的空间维度。
- en: '`input_shape` parameter is set to the shape of the input data (`x_train`) without
    the batch dimension. The input shape determines the size of the input data that
    the layer will process. In this case, it is set to the shape of the training data
    `x_train` without considering the batch dimension.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_shape`参数设置为输入数据的形状（`x_train`），不包括批量维度。输入形状决定了层将处理的数据输入大小。在这种情况下，它设置为训练数据`x_train`的形状，不考虑批量维度。'
- en: In summary, this code snippet adds a convolutional layer to the neural network
    model, configuring it with specific parameters for filter size, number of filters,
    activation function, and padding. The convolutional layer plays a crucial role
    in learning hierarchical features from input images.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，此代码片段向神经网络模型添加了一个卷积层，并使用特定的参数配置了滤波器大小、滤波器数量、激活函数和填充。卷积层在从输入图像学习层次特征中起着至关重要的作用。
- en: 'The following line adds another 2D convolutional layer with the same specifications
    as the previous one, but without specifying the input shape. The model will infer
    the input shape based on the previous layer:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 以下行添加了一个与之前相同的2D卷积层，但没有指定输入形状。模型将根据前一层推断输入形状：
- en: '[PRE23]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The following line adds a max-pooling layer with a pool size of (2, 2), which
    reduces the spatial dimensions of the input by taking the maximum value within
    each pool:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下行添加了一个池大小为（2, 2）的最大池化层，通过在每个池内取最大值来减少输入的空间维度：
- en: '[PRE24]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following line adds a dropout layer with a rate of 0.25, which randomly
    sets 25% of the input units to 0 during training. Dropout helps prevent overfitting
    by introducing randomness and reducing the reliance on specific features:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下行添加了一个丢弃率为0.25的dropout层，在训练期间随机将25%的输入单元设置为0。Dropout通过引入随机性和减少对特定特征的依赖来帮助防止过拟合：
- en: '[PRE25]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The code continues adding more convolutional layers, max-pooling layers, and
    dropout layers, and finally ends with fully connected (dense) layers:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 代码继续添加更多的卷积层、最大池化层和dropout层，最后以全连接（密集）层结束：
- en: '[PRE26]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following line flattens the previous layer’s output to a 1D tensor, preparing
    it to be connected to a dense layer:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 以下行将前一层输出展平为1D张量，为将其连接到密集层做准备：
- en: '[PRE27]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following line adds a dense layer with 512 units and ReLU activation:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下行添加了一个具有512个单元和ReLU激活的密集层：
- en: '[PRE28]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following line adds a dropout layer with a rate of 0.5:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下行添加了一个丢弃率为0.5的dropout层：
- en: '[PRE29]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following line adds a final dense layer with 10 units and softmax activation,
    which produces a probability distribution over the 10 classes for classification:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下行添加了一个具有10个单元和softmax激活的最终密集层，它为分类生成10个类别的概率分布：
- en: '[PRE30]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following code initializes an instance of the `ImageDataGenerator` class
    from Keras, which is used for data augmentation in image datasets:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码初始化了Keras中的`ImageDataGenerator`类的一个实例，该类用于图像数据集的数据增强：
- en: '[PRE31]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This code defines a CNN with two convolutional layers, two max-pooling layers,
    and three fully connected layers. Data augmentation is performed using the `ImageDataGenerator`
    class, which randomly applies various transformations to the training images to
    generate more training data. The model is trained for 100 epochs using the `fit`
    method with the data generator as the input. Finally, the model is evaluated on
    the test set using the `evaluate` method.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码定义了一个具有两个卷积层、两个最大池化层和三个全连接层的CNN。使用`ImageDataGenerator`类执行数据增强，该类随机应用于训练图像以生成更多训练数据。使用`fit`方法以数据生成器作为输入训练模型100个epoch。最后，使用`evaluate`方法在测试集上评估模型。
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered a variety of image data augmentation techniques.
    We learned how to implement an SVM with data augmentation in Python using the
    scikit-learn and Keras libraries. We first implemented SVM with the default hyperparameters
    and evaluated the performance of the classifier on the original dataset. We then
    implemented an SVM with data augmentation and trained the classifier on each batch
    of training data generated by the `ImageDataGenerator` object. Finally, we evaluated
    the performance of the classifier on the augmented dataset.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了各种图像数据增强技术。我们学习了如何在Python中使用scikit-learn和Keras库实现带有数据增强的支持向量机（SVM）。我们首先使用默认的超参数实现了SVM，并在原始数据集上评估了分类器的性能。然后，我们实现了带有数据增强的SVM，并在由`ImageDataGenerator`对象生成的每个训练数据批次上训练了分类器。最后，我们在增强数据集上评估了分类器的性能。
- en: We also saw how to implement a CNN using augmentation with the CIFAR-10 dataset.
    Using data augmentation, we were able to improve the accuracy of the classifier
    on the augmented dataset. This demonstrates the effectiveness of data augmentation
    in improving the performance of machine learning models, especially in cases where
    the available dataset is limited.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到了如何使用CIFAR-10数据集实现CNN，并使用数据增强。通过数据增强，我们能够提高分类器在增强数据集上的准确性。这证明了数据增强在提高机器学习模型性能方面的有效性，尤其是在可用数据集有限的情况下。
- en: Data augmentation can reduce the need for manual annotation by creating variations
    of existing labeled data. Instead of labeling each transformed image separately,
    augmentation techniques allow for the generation of additional labeled samples
    without the need for additional human annotation efforts.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强可以通过创建现有标记数据的变体来减少对人工标注的需求。而不是单独对每个转换后的图像进行标注，增强技术允许在不需额外人工标注努力的情况下生成额外的标记样本。
- en: In the next chapter, we will explore how to label text data using generative
    models.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何使用生成模型标注文本数据。
- en: 'Part 3: Labeling Text, Audio, and Video Data'
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：标注文本、音频和视频数据
- en: In this part of the book, you will explore how to read text, audio, and video
    data using Python, analyze the data, and extract features. The content delves
    into various methods for programmatically labeling text, video, and audio data
    in Python, leveraging OpenAI’s large language models, as well as semi-supervised
    and unsupervised techniques such as K-means clustering. Additionally, this section
    aids in understanding different open source data annotation tools such as Label
    Studio, CVAT, pyOpenAnnotate, and Azure Machine Learning for image, video, audio,
    and text data, providing a comprehensive comparison between them.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这一部分，您将探索如何使用Python读取文本、音频和视频数据，分析数据并提取特征。内容深入探讨了在Python中通过编程对文本、视频和音频数据进行标记的各种方法，利用OpenAI的大型语言模型，以及半监督和无监督技术，如K-means聚类。此外，本节还帮助理解不同的开源数据标注工具，如Label
    Studio、CVAT、pyOpenAnnotate和Azure Machine Learning，用于图像、视频、音频和文本数据，并提供它们之间的全面比较。
- en: 'This part comprises the following chapters:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包括以下章节：
- en: '[*Chapter 7*](B18944_07.xhtml#_idTextAnchor147), *Labeling Text Data*'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B18944_07.xhtml#_idTextAnchor147)，*标注文本数据*'
- en: '[*Chapter 8*](B18944_08.xhtml#_idTextAnchor176), *Exploring Video Data*'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B18944_08.xhtml#_idTextAnchor176)，*探索视频数据*'
- en: '[*Chapter 9*](B18944_09.xhtml#_idTextAnchor204), *Labeling Video Data*'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B18944_09.xhtml#_idTextAnchor204)，*标注视频数据*'
- en: '[*Chapter 10*](B18944_10.xhtml#_idTextAnchor221), *Exploring Audio Data*'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B18944_10.xhtml#_idTextAnchor221)，*探索音频数据*'
- en: '[*Chapter 11*](B18944_11.xhtml#_idTextAnchor248), *Labeling Audio Data*'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B18944_11.xhtml#_idTextAnchor248)，*标注音频数据*'
- en: '[*Chapter 12*](B18944_12.xhtml#_idTextAnchor267), *Hands-On Exploring Data
    Labeling Tools*'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第12章*](B18944_12.xhtml#_idTextAnchor267)，*动手探索数据标注工具*'
