- en: 'Chapter 3: Code Meets Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章：代码遇见数据
- en: In this chapter, we'll get started with hands-on **MLOps** implementation as
    we learn by solving a business problem using the MLOps workflow discussed in the
    previous chapter. We'll also discuss effective methods of source code management
    for **machine learning** (**ML**), explore data quality characteristics, and analyze
    and shape data for an ML solution.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过解决一个业务问题来开始动手实践**MLOps**的实施，我们将在前一章讨论的MLOps工作流程中学习。我们还将讨论**机器学习**（**ML**）的源代码管理有效方法，探索数据质量特性，并分析和塑造数据以适应ML解决方案。
- en: 'We begin this chapter by categorizing the business problem to curate a best-fit
    MLOps solution for it. Following this, we''ll set up the required resources and
    tools to implement the solution. 10 guiding principles for source code management
    for ML are discussed to apply clean code practices. We will discuss what constitutes
    good-quality data for ML and much more, followed by processing a dataset related
    to the business problem and ingesting and versioning it to the ML workspace. Most
    of the chapter is hands-on and designed to equip you with a good understanding
    of and experience with MLOps. For this, we''re going to cover the following main
    topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从对业务问题进行分类开始，以编制最适合的MLOps解决方案。在此之后，我们将设置实施解决方案所需的资源和工具。讨论了10个关于ML源代码管理的指导原则，以应用干净的代码实践。我们将讨论构成ML高质量数据的内容，以及更多内容，然后处理与业务问题相关的数据集，并将其摄入和版本控制到ML工作区。本章的大部分内容都是实践性的，旨在让你对MLOps有一个良好的理解和经验。为此，我们将在本章中涵盖以下主要主题：
- en: Business problem analysis and categorizing the problem
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 业务问题分析和问题分类
- en: Setting up resources and tools
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置资源和工具
- en: 10 principles of source code management for machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习源代码管理的10个原则
- en: Good data for machine learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适合机器学习的好数据
- en: Data preprocessing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Data registration and versioning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据注册和版本控制
- en: Toward an ML pipeline
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向ML管道迈进
- en: Without further ado, let's jump into demystifying the business problem and implementing
    the solution using an MLOps approach.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 不再拖延，让我们直接进入揭秘业务问题并使用MLOps方法实施解决方案。
- en: Business problem analysis and categorizing the problem
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 业务问题分析和问题分类
- en: In the previous chapter, we looked into the following business problem statement.
    In this section, we will demystify the problem statement by categorizing it using
    the principles to curate an implementation roadmap. We will glance at the dataset
    given to us to address the business problem and decide what type of ML model will
    address the business problem efficiently. Lastly, we'll categorize the MLOps approach
    for implementing robust and scalable ML operations and decide on tools for implementation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们研究了以下业务问题陈述。在本节中，我们将通过使用原则对问题进行分类，以编制实施路线图来揭秘问题陈述。我们将快速查看我们用于解决业务问题的数据集，并决定哪种ML模型能够高效地解决业务问题。最后，我们将对实施稳健和可扩展的ML操作进行分类，并决定实施所需的工具。
- en: 'Here is the problem statement:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是问题陈述：
- en: You work as a data scientist with a small team of data scientists for a cargo
    shipping company based in Finland. 90% of goods are imported into Finland via
    cargo shipping. You are tasked with saving 20% of the costs for cargo operations
    at the port of Turku, Finland. This can be achieved by developing an ML solution
    that predicts weather conditions at the port 4 hours in advance. You need to monitor
    for possible rainy conditions, which can distort operations at the port with human
    resources and transportation, which in turn affects supply chain operations at
    the port. Your ML solution will help port authorities to predict possible rain
    4 hours in advance; this will save 20% of costs and enable smooth supply chain
    operations at the port.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你作为数据科学家，在一个位于芬兰的货运公司的小型数据科学家团队中工作。90%的货物通过货运进入芬兰。你被分配的任务是在芬兰图尔库港的货运操作中节省20%的成本。这可以通过开发一个预测港口4小时后天气状况的ML解决方案来实现。你需要监测可能出现的雨天，这可能会因为人力资源和交通而扭曲港口的运营，进而影响港口的供应链运营。你的ML解决方案将帮助港口当局提前4小时预测可能的降雨；这将节省20%的成本，并使港口的供应链运营更加顺畅。
- en: The first step in solving a problem is to simplify and categorize it using an
    appropriate approach. In the previous chapter, we discussed how to categorize
    a business problem to solve it using ML. Let's apply those principles to chart
    a clear roadmap to implementing it.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 解决问题的第一步是使用适当的方法对其进行简化和分类。在前一章中，我们讨论了如何将商业问题分类以使用机器学习来解决它。现在，让我们将这些原则应用到制定其实施的清晰路线图。
- en: First, we'll see what type of model we will train to yield the maximum business
    value. Secondly, we will identify the right approach for our MLOps implementation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看我们将训练哪种类型的模型以产生最大的商业价值。其次，我们将确定我们 MLOps 实现的正确方法。
- en: 'In order to decide on the type of model to train, we can start by having a
    glance at the dataset available on GitHub: [https://github.com/PacktPublishing/EngineeringMLOps](https://github.com/PacktPublishing/EngineeringMLOps).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了决定要训练的模型类型，我们可以先浏览 GitHub 上可用的数据集：[https://github.com/PacktPublishing/EngineeringMLOps](https://github.com/PacktPublishing/EngineeringMLOps)。
- en: 'Here is a snapshot of `weather_dataset_raw.csv`, in *Figure 3.1*. The file
    size is 10.7 MB, the number of rows is 96,453, and the file is in CSV format:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 3.1* 中，这是一个 `weather_dataset_raw.csv` 数据集的快照。文件大小为 10.7 MB，行数为 96,453，文件格式为
    CSV：
- en: '![Figure 3.1 – Dataset snapshot'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.1 – 数据集快照'
- en: '](img/B16572_03_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16572_03_01.jpg)'
- en: Figure 3.1 – Dataset snapshot
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 数据集快照
- en: 'By assessing the data, we can categorize the business problem as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过评估数据，我们可以将商业问题分类如下：
- en: '`Weather condition` column depicts whether an event has recorded rain, snow,
    or clear conditions. This can be framed or relabeled as `rain` or `no rain` and
    used to perform binary classification. Hence, it is straightforward to solve the
    business problem with a supervised learning approach.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`天气状况` 列描述了一个事件是否记录了雨、雪或晴朗的条件。这可以被框架化或重新标记为 `rain` 或 `no rain` 并用于执行二元分类。因此，使用监督学习方法解决商业问题很简单。'
- en: '**MLOps approach**: By observing the problem statement and data, here are the
    facts:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLOps 方法**：通过观察问题陈述和数据，以下是事实：'
- en: '(a) Data: The training data is 10.7 MB. The data size is reasonably small (it
    cannot be considered big data).'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (a) 数据：训练数据大小为 10.7 MB。数据量相对较小（不能算作大数据）。
- en: '(b) Operations: We need to train, test, deploy, and monitor an ML model to
    forecast the weather at the port of Turku every hour (4 hours in advance) when
    new data is recorded.'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (b) 操作：我们需要训练、测试、部署和监控一个机器学习模型，以预测图尔库港每小时的天气情况（新数据记录时提前 4 小时）。
- en: '(c) Team size: A small/medium team of data scientists, no DevOps engineers.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (c) 团队规模：一个小/中型数据科学家团队，没有 DevOps 工程师。
- en: Based on the preceding facts, we can categorize the operations into **small
    team ops**; there is no need for big data processing and the team is small and
    agile. Now we will look at some suitable tools to implement the operations needed
    to solve the business problem at hand.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述事实，我们可以将操作分类为 **小型团队操作**；不需要大数据处理，团队规模小且敏捷。现在，我们将探讨一些适合的工具来实施解决当前业务问题所需的操作。
- en: 'For us to get a holistic understanding of MLOps implementation, we will implement
    the business problems using two different tools simultaneously:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们对 MLOps 实施有一个全面的理解，我们将同时使用两种不同的工具来实施业务问题：
- en: '**Azure Machine Learning** (Microsoft Azure)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Machine Learning**（微软 Azure）'
- en: '**MLflow (**an open source cloud and platform-agnostic tool**)**'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLflow**（一个开源的云和平台无关的工具）'
- en: 'We use these two tools to see how things work from a pure cloud-based approach
    and from an open source / cloud-agnostic approach. All the code and CI/CD operations
    will be managed and orchestrated using Azure DevOps, as shown in *Figure 3.2*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这两个工具来查看纯基于云的方法和开源/云无关的方法是如何工作的。所有代码和 CI/CD 操作都将使用 Azure DevOps 进行管理和编排，如图
    *图 3.2* 所示：
- en: '![Figure 3.2 – MLOps tools for the solution'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.2 – 解决方案中的 MLOps 工具'
- en: '](img/B16572_03_02.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16572_03_02.jpg)'
- en: Figure 3.2 – MLOps tools for the solution
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 解决方案中的 MLOps 工具
- en: Now, we will set up the tools and resources needed to implement the solution
    for the business problem. As we will use Python as the primary programming language,
    it is a pre-requisite to have **Python 3** installed within your Mac, Linux, or
    Windows OS.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将设置实施商业问题解决方案所需的工具和资源。由于我们将使用 Python 作为主要的编程语言，因此需要在您的 Mac、Linux 或 Windows
    操作系统中安装 **Python 3**。
- en: Setting up the resources and tools
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置资源和工具
- en: If you have these tools already installed and set up on your PC, feel free to
    skip this section; otherwise, follow the detailed instructions to get them up
    and running.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经在你的PC上安装并设置了这些工具，请随意跳过此部分；否则，请按照详细的说明来启动它们。
- en: Installing MLflow
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装MLflow
- en: We get started by installing MLflow, which is an open source platform for managing
    the ML life cycle, including experimentation, reproducibility, deployment, and
    a central model registry.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先安装MLflow，它是一个开源平台，用于管理ML生命周期，包括实验、可重复性、部署和中央模型注册。
- en: 'To install MLflow, go to your terminal and execute the following command:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装MLflow，请转到你的终端并执行以下命令：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After successful installation, test the installation by executing the following
    command to start the `mlflow` tracking UI:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 安装成功后，通过执行以下命令来测试安装，以启动`mlflow`跟踪UI：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Upon running the `mlflow` tracking UI, you will be running a server listening
    at port `5000` on your machine, and it outputs a message like the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`mlflow`跟踪UI时，你将在你的机器上运行一个监听端口`5000`的服务器，并输出如下信息：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can access and view the `mlflow` UI at `http://localhost:5000`. When you
    have successfully installed `mlflow` and run the tracking UI, you are ready to
    install the next tool.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在`http://localhost:5000`访问和查看`mlflow` UI。当你成功安装`mlflow`并运行跟踪UI时，你就可以安装下一个工具了。
- en: Azure Machine Learning
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure机器学习
- en: 'Azure Machine Learning provides a cloud-based ML platform for training, deploying,
    and managing ML models. This service is available on Microsoft Azure, so the pre-requisite
    is to have a free subscription to Microsoft Azure. Please create a free account
    with around $170 of credit, which is sufficient to implement the solution, here:
    [https://azure.microsoft.com/](https://azure.microsoft.com/).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Azure机器学习提供了一个基于云的ML平台，用于训练、部署和管理ML模型。此服务可在Microsoft Azure上使用，因此先决条件是拥有一个Microsoft
    Azure的免费订阅。请创建一个大约有170美元信用额的免费账户，这足以实现解决方案，请在此处创建：[https://azure.microsoft.com/](https://azure.microsoft.com/)。
- en: When you have access/a subscription to Azure, move on to the next section to
    get Azure Machine Learning up and running.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当你拥有Azure的访问/订阅时，继续下一部分以启动Azure机器学习。
- en: Creating a resource group
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建资源组
- en: 'A **resource group** is a collection of related resources for an Azure solution.
    It is a container that ties up all the resources related to a service or solution.
    Creating a resource group enables easy access and management of a solution. Let''s
    get started by creating your own resource group:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源组**是一组与Azure解决方案相关的相关资源的集合。它是一个容器，将所有与某个服务或解决方案相关的资源捆绑在一起。创建资源组可以方便地访问和管理解决方案。让我们从创建自己的资源组开始：'
- en: Open the Azure portal.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Azure门户。
- en: Access the portal menu (go to the portal's home page if you are not there by
    default) and hover over the resource group icon in the navigation section. A **Create**
    button will appear; click on it to create a new resource group:![Figure 3.3 –
    Creating a resource group
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问门户菜单（如果你默认不在主页上，请转到门户的主页），将鼠标悬停在导航部分中的资源组图标上。将出现一个**创建**按钮；点击它以创建一个新的资源组：![图3.3
    – 创建资源组
- en: '](img/B16572_03_03.jpg)'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B16572_03_03.jpg]'
- en: Figure 3.3 – Creating a resource group
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.3 – 创建资源组
- en: Create a resource group with the name of your choice (`Learn_MLOps` is recommended),
    as shown in *Figure 3.3*.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为你选择的资源组（推荐使用`Learn_MLOps`），如图3.3所示。
- en: Select a region close to you to get the optimal performance and pricing. For
    example, in *Figure 3.3* a resource group with the name `Learn MLOps` and region
    **(Europe) North Europe** is ready to be created. After you click the **Review
    + Create** button and Azure validates the request, the final **Create** button
    will appear. The final **Create** button should be pressed to create the new resource
    group.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个靠近你的区域以获得最佳性能和定价。例如，在图3.3中，一个名为`Learn MLOps`且区域为**（欧洲）北欧**的资源组已准备好创建。在点击**审查
    + 创建**按钮并Azure验证请求后，将出现最终的**创建**按钮。应该按下最终的**创建**按钮来创建新的资源组。
- en: When the resource group is reviewed and created, you can set up and manage all
    the services related to the ML solution in this resource group. The newly created
    resource group will be listed in the resource group list.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当资源组经过审查并创建后，你可以在该资源组中设置和管理所有与ML解决方案相关的服务。新创建的资源组将列在资源组列表中。
- en: Creating an Azure Machine Learning workspace
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建Azure机器学习工作区
- en: 'An ML workspace is a central hub for tracking and managing your ML training,
    deploying, and monitoring experiments. To create an Azure Machine Learning workspace,
    go to the Azure portal menu, click on `Machine Learning` and select it. You will
    see the following screen:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ML 工作区是跟踪和管理您的 ML 训练、部署和监控实验的中心枢纽。要创建 Azure Machine Learning 工作区，请转到 Azure 门户菜单，点击
    `Machine Learning` 并选择它。您将看到以下屏幕：
- en: '![Figure 3.4 – Creating an Azure Machine Learning workspace'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.4 – 创建 Azure Machine Learning 工作区'
- en: '](img/B16572_03_04.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16572_03_04.jpg)'
- en: Figure 3.4 – Creating an Azure Machine Learning workspace
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 创建 Azure Machine Learning 工作区
- en: Name the workspace with the name of your choice (for example, we've named it
    **MLOps_WS** in *Figure 3.4*). Select the resource group you created earlier to
    tie this ML service to it (**Learn_MLOps** is selected in *Figure 3.4*). Finally,
    hit the **Review + create** button and you will be taken to a new screen with
    the final **Create** button. Press the final **Create** button to create your
    Azure Machine Learning workspace.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用您选择的名称命名工作区（例如，我们在 *图 3.4* 中将其命名为 **MLOps_WS**）。选择您之前创建的资源组以将此 ML 服务与其关联（*图
    3.4* 中选择的是 **Learn_MLOps**）。最后，点击 **Review + create** 按钮，您将被带到一个新的屏幕，其中包含最终的 **Create**
    按钮。按下最终的 **Create** 按钮以创建您的 Azure Machine Learning 工作区。
- en: After creating the Azure Machine Learning workspace (`Learn_MLOps`), the Azure
    platform will deploy all the resources this service needs. The resources deployed
    with the Azure Machine Learning instance (`Learn_MLOps`), such as Blob Storage,
    Key Vault, and Application Insights, are provisioned and tied to the workspace.
    These resources will be consumed or used via the workspace and the SDK.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Azure Machine Learning 工作区（`Learn_MLOps`）后，Azure 平台将部署此服务所需的所有资源。与 Azure
    Machine Learning 实例（`Learn_MLOps`）一起部署的资源，如 Blob 存储、密钥保管库和应用洞察，将配置并关联到工作区。这些资源将通过工作区和
    SDK 进行消耗或使用。
- en: 'You can find detailed instructions on creating an Azure Machine Learning instance
    here: [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下位置找到创建 Azure Machine Learning 实例的详细说明：[https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace)。
- en: Installing Azure Machine Learning SDK
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 Azure Machine Learning SDK
- en: 'Go to the terminal or command line in your PC and install the Azure Machine Learning
    SDK, which will be extensively used in the code to orchestrate the experiment.
    To install it, run the following command:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的 PC 的终端或命令行中，安装 Azure Machine Learning SDK，该 SDK 将在代码中广泛用于编排实验。要安装它，请运行以下命令：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can find detailed instructions here: [https://docs.microsoft.com/en-us/python/api/overview/azure/ml/install?view=azure-ml-py](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/install?view=azure-ml-py).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下位置找到详细说明：[https://docs.microsoft.com/en-us/python/api/overview/azure/ml/install?view=azure-ml-py](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/install?view=azure-ml-py)。
- en: Azure DevOps
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure DevOps
- en: 'All the source code and CI/CD-related operations will be managed and orchestrated
    using Azure DevOps. The code we manage in the repository in Azure DevOps will
    be used to train, deploy, and monitor ML models enabled by CI/CD pipelines. Let''s
    start by creating an Azure DevOps subscription:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所有源代码和 CI/CD 相关的操作都将通过 Azure DevOps 进行管理和编排。我们在 Azure DevOps 仓库中管理的代码将用于通过 CI/CD
    管道训练、部署和监控 ML 模型。让我们先创建一个 Azure DevOps 订阅：
- en: Create a free account at [dev.azure.com](http://dev.azure.com). A free account
    can be created using a pre-existing Microsoft or GitHub account.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 [dev.azure.com](http://dev.azure.com) 上创建一个免费账户。您可以使用现有的 Microsoft 或 GitHub
    账户创建一个免费账户。
- en: Create a project named `Learn_MLOps` (make it public or private depending on
    your preference).
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `Learn_MLOps` 的项目（根据您的偏好将其设置为公开或私有）。
- en: Go to the **repos** section. In the **Import a repository** section, press the
    **Import** button.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到 **repos** 部分。在 **Import a repository** 部分中，点击 **Import** 按钮。
- en: 'Import a repository from a public GitHub project from this repository: [https://github.com/PacktPublishing/EngineeringMLOps](https://github.com/PacktPublishing/EngineeringMLOps
    ) (as shown in *Figure 3.5*):'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从此仓库导入公共 GitHub 项目的仓库：[https://github.com/PacktPublishing/EngineeringMLOps](https://github.com/PacktPublishing/EngineeringMLOps
    )（如 *图 3.5* 所示）：
- en: '![Figure 3.5 – Import the GitHub repository into the Azure DevOps project'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.5 – 将 GitHub 仓库导入到 Azure DevOps 项目中'
- en: '](img/B16572_03_05.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16572_03_05.jpg)'
- en: Figure 3.5 – Import the GitHub repository into the Azure DevOps project
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 将 GitHub 仓库导入到 Azure DevOps 项目中
- en: After importing the GitHub repository, files from the imported repository will
    be displayed.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 导入GitHub仓库后，将显示导入仓库的文件。
- en: JupyterHub
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JupyterHub
- en: 'Lastly, we''ll need an interactive data analysis and visualization tool to
    process data using our code. For this, we use **JupyterHub**. This is a common
    data science tool used widely by data scientists to process data, visualize data,
    and train ML models. To install it, follow two simple steps:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要一个交互式数据分析和可视化工具来使用我们的代码处理数据。为此，我们使用**JupyterHub**。这是一个常用的数据科学工具，被数据科学家广泛用于处理数据、可视化数据和训练机器学习模型。要安装它，请遵循以下两个简单步骤：
- en: 'Install JupyterHub via the command line on your PC:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的PC上通过命令行安装JupyterHub：
- en: '[PRE4]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You may find detailed instructions here: [https://jupyterhub.readthedocs.io/en/stable/quickstart.html](https://jupyterhub.readthedocs.io/en/stable/quickstart.html).'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以在此处找到详细说明：[https://jupyterhub.readthedocs.io/en/stable/quickstart.html](https://jupyterhub.readthedocs.io/en/stable/quickstart.html)。
- en: Install Anaconda.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装Anaconda。
- en: 'Anaconda is needed as it installs dependencies, setup environments, and services
    to support the JupyterHub. Download Anaconda and install it as per the detailed
    instructions here: [https://docs.anaconda.com/anaconda/install/](https://docs.anaconda.com/anaconda/install/).'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Anaconda是必需的，因为它安装依赖项、设置环境和支持JupyterHub的服务。根据以下详细说明下载并安装Anaconda：[https://docs.anaconda.com/anaconda/install/](https://docs.anaconda.com/anaconda/install/)。
- en: Now that we are set up for the hands-on implementation, let's look at what it
    takes to manage good code and data.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们已经为动手实现做好了准备，让我们看看管理良好代码和数据需要什么。
- en: 10 principles of source code management for ML
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习源代码管理的10个原则
- en: 'Here are 10 principles that can be applied to your code to ensure the quality,
    robustness, and scalability of your code:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些原则可以应用于你的代码，以确保代码的质量、健壮性和可扩展性：
- en: '**Modularity:** It is better to have modular code than to have one big chunk.
    Modularity encourages reusability and facilitates upgrading by replacing the required
    components. To avoid needless complexity and repetition, follow this golden rule:'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模块化**：拥有模块化代码比拥有一个大块代码更好。模块化鼓励重用并便于通过替换所需组件进行升级。为了避免不必要的复杂性和重复，请遵循以下黄金法则：'
- en: Two or more ML components should be paired only when one of them uses the other.
    If none of them uses each other, then pairing should be avoided.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两个或多个机器学习组件只有在其中一个使用另一个时才应配对。如果它们都不使用对方，则应避免配对。
- en: An ML component that is not tightly paired with its environment can be more
    easily modified or replaced than a tightly paired component.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与其环境紧密配对的机器学习组件比紧密配对的组件更容易修改或替换。
- en: '**Single task dedicated functions:** Functions are important building blocks
    of pipelines and the system, and they are small sections of code that are used
    to perform particular tasks. The purpose of functions is to avoid repetition of
    commands and enable reusable code. They can easily become a complex set of commands
    to facilitate tasks. For readable and reusable code, it is more efficient to have
    a single function dedicated to a single task instead of multiple tasks. It is
    better to have multiple functions than one long and complex function.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单一任务专用函数**：函数是管道和系统的基本构建块，它们是用于执行特定任务的代码小段。函数的目的是避免命令重复并使代码可重用。它们可以很容易地变成一组复杂的命令以方便任务。为了使代码可读和可重用，最好有一个专门用于单一任务的函数，而不是多个任务。拥有多个函数比一个长而复杂的函数更好。'
- en: '`Error 300`. Structuring blocks of code and trying to limit the maximum levels
    of indentation for functions and classes can enhance the readability of the code.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`错误300`。对代码块进行结构化并尝试限制函数和类的最大缩进级别可以提高代码的可读性。'
- en: '**Clean code:** If you have to explain the code, it''s not that good. Clean
    code is self-explanatory. It focuses on high readability, optimal modularity,
    reusability, non-repeatability, and optimal performance. Clean code reduces the
    cost of maintaining and upgrading your ML pipelines. It enables a team to perform
    efficiently and can be extended to other developers.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清洁代码**：如果你必须解释代码，那么它就不是很好。清洁代码是自我解释的。它侧重于高可读性、最佳模块化、可重用性、非重复性和最佳性能。清洁代码降低了维护和升级你的机器学习管道的成本。它使团队能够高效地工作，并且可以扩展到其他开发者。'
- en: 'To understand this in depth, read *Clean Code: A Handbook of Agile Software
    Craftsmanship* by **Robert C Martin**.'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '要深入了解这一点，请阅读**罗伯特·马丁**的《Clean Code: A Handbook of Agile Software Craftsmanship》。'
- en: '**Testing:** It is vital to ensure the robustness of a system, and testing
    plays an important role in this. In general, testing extends to unit testing and
    acceptance testing. Unit testing is a method by which components of source code
    are tested for robustness with coerced data and usage methods to determine whether
    the component is fit for the production system. Acceptance tests are done to test
    the overall system to ensure the system realizes user requirements; end-to-end
    business flows are verified in real-time scenarios. Testing is vital to ensure
    the efficient working of code: "if it isn''t tested, it is broken."'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试：** 确保系统的健壮性至关重要，而测试在这一过程中发挥着重要作用。一般来说，测试包括单元测试和验收测试。单元测试是一种通过强制数据和使用方法测试源代码组件的健壮性的方法，以确定该组件是否适合生产系统。验收测试是为了测试整个系统，以确保系统实现了用户需求；在实时场景中验证端到端业务流程。测试对于确保代码的高效运行至关重要：“如果未经测试，它就是有缺陷的。”'
- en: 'To learn more about the implementation of unit testing, read this documentation:
    [https://docs.python.org/3/library/unittest.html](https://docs.python.org/3/library/unittest.html).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于单元测试实现的信息，请阅读此文档：[https://docs.python.org/3/library/unittest.html](https://docs.python.org/3/library/unittest.html)。
- en: '**Version control (code, data and models):** Git is used for version control
    of code in ML systems. The purpose of version control is to ensure that all the
    team members working on the system have access to up-to-date code and that code
    is not lost when there is a hardware failure. One rule of working with Git should
    be to not break the master (branch). This means when you have working code in
    the repository and you add new features or make improvements, you do this in a
    feature branch, which is merged to the master branch when the code is working
    and reviewed. Branches should be given a short descriptive name, such as feature/label-encoder.
    Branch naming and approval guidelines should be properly communicated and agreed
    upon with the team to avoid any complexity and unnecessary conflicts. Code review
    is done with pull requests to the repository of the code. Usually, it is best
    to review code in small sets, less than 400 lines. In practice, it often means
    one module or a submodule at a time.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**版本控制（代码、数据和模型）：** 在机器学习系统中，Git用于代码的版本控制。版本控制的目的确保所有参与系统开发的团队成员都能访问最新的代码，并且在硬件故障时代码不会丢失。与Git一起工作的一个规则应该是不要破坏主分支（分支）。这意味着当你有一个在仓库中的工作代码，并且添加新功能或进行改进时，你应该在一个功能分支中这样做，当代码工作并且经过审查后，将其合并到主分支。分支应该有一个简短且描述性的名称，例如feature/label-encoder。分支命名和审批指南应该与团队进行适当的沟通和同意，以避免任何复杂性和不必要的冲突。代码审查是通过向代码仓库提交拉取请求来完成的。通常，最好以小于400行的代码块进行审查。在实践中，这通常意味着一次一个模块或子模块。'
- en: Versioning of data is essential for ML systems as it helps us to keep track
    of which data was used for a particular version of code to generate a model. Versioning
    data can enable reproducing models and compliance with business needs and law.
    We can always backtrack and see the reason for certain actions taken by the ML
    system. Similarly, versioning of models (artifacts) is important for tracking
    which version of a model has generated certain results or actions for the ML system.
    We can also track or log parameters used for training a certain version of the
    model. This way, we can enable end-to-end traceability for model artifacts, data,
    and code. Version control for code, data, and models can enhance an ML system
    with great transparency and efficiency for the people developing and maintaining
    it.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据版本化对于机器学习系统至关重要，因为它帮助我们追踪特定版本的代码所使用的数据。数据版本化可以使得模型的可复现性以及满足商业需求和法律法规成为可能。我们可以随时回溯并查看机器学习系统采取某些行动的原因。同样，模型（工件）的版本化对于追踪哪个版本的模型生成了机器学习系统的特定结果或行动也很重要。我们还可以追踪或记录用于训练模型特定版本的参数。这样，我们可以实现模型工件、数据和代码的端到端可追溯性。代码、数据和模型的版本控制可以增强机器学习系统的透明度和效率，这对于开发和维护它的人来说是非常有益的。
- en: '`print` statements are good for testing and debugging but not ideal for production.
    The logger contains information, especially system information, warnings, and
    errors, that are quite useful in the monitoring of production systems.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`print`语句对于测试和调试很有用，但不适合生产环境。日志记录器包含的信息，尤其是系统信息、警告和错误，对于监控生产系统非常有用。'
- en: '**Error handling:** Error handling is vital for handling edge cases, especially
    ones that are hard to anticipate. It is recommended to catch and handle exceptions
    even if you think you don''t need to, as prevention is better than cure. Logging
    combined with exception handling can be an effective way of dealing with edge
    cases.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误处理**：错误处理对于处理边缘情况至关重要，特别是那些难以预测的情况。即使你认为不需要，也建议捕获和处理异常，因为预防胜于治疗。将日志记录与异常处理结合起来可以是一种处理边缘情况的有效方法。'
- en: '**Readability:** Code readability enables information transfer, code efficiency,
    and code maintainability. It can be achieved by following principles such as following
    industry-standard coding practices such as PEP-8 ([https://www.python.org/dev/peps/pep-0008/](https://www.python.org/dev/peps/pep-0008/))
    or the JavaScript standard style (depending on the language you are using). Readability
    is also increased by using docstrings. A docstring is a text that is written at
    the beginning of, for example, a function, describing what it does and possibly
    what it takes as input. In some cases, it is enough to have a one-liner explanation,
    such as this:'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可读性**：代码可读性有助于信息传递、代码效率和代码可维护性。可以通过遵循行业标准的编码实践，如PEP-8 ([https://www.python.org/dev/peps/pep-0008/](https://www.python.org/dev/peps/pep-0008/))
    或JavaScript标准风格（取决于你使用的语言）来实现。使用文档字符串也可以提高可读性。文档字符串是在例如函数开头编写的文本，描述它做什么以及可能需要什么输入。在某些情况下，一行解释就足够了，例如：'
- en: '[PRE5]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A longer docstring is needed for a more complex function. Explaining the arguments
    and returns is a good idea:'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于更复杂的函数，需要更长的文档字符串。解释参数和返回值是一个好主意：
- en: '[PRE6]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Commenting and documenting:** Commenting and documentation are vital for
    maintaining sustainable code. It is not always possible to explain the code clearly.
    Comments can be useful in such cases to prevent confusion and explain the code. Comments
    can convey information such as copyright info, intent, clarification of code,
    possible warnings, and elaboration of code. Elaborate documentation of the system
    and modules can enable a team to perform efficiently, and the code and assets
    can be extended to other developers. For documentation, open source tools are
    available for documenting APIs such as Swagger ([https://swagger.io](https://swagger.io))
    and Read the Docs ([https://readthedocs.org](https://readthedocs.org)). Using
    the right tools for documentation can enable efficiency and standardize knowledge
    for developers.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**注释和文档化**：注释和文档对于维护可持续的代码至关重要。并非总是能够清晰地解释代码。在这种情况下，注释可以有用，以防止混淆并解释代码。注释可以传达诸如版权信息、意图、代码澄清、可能的警告和代码详细说明等信息。对系统和模块的详细文档可以使团队高效地工作，并且代码和资产可以扩展给其他开发者。对于文档，有开源工具可用于文档化API，例如Swagger
    ([https://swagger.io](https://swagger.io)) 和 Read the Docs ([https://readthedocs.org](https://readthedocs.org))。使用正确的工具进行文档化可以提高效率并标准化开发者的知识。'
- en: What is good data for ML?
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是有用的机器学习数据？
- en: 'Good ML models are a result of training on good-quality data. Before proceeding
    to ML training, a pre-requisite is to have good-quality data. Therefore, we need
    to process the data to increase its quality. So, determining the quality of data
    is essential. Five characteristics will enable us to discern the quality of data,
    as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的机器学习模型是训练在高质量数据上的结果。在进行机器学习训练之前，一个先决条件是拥有高质量的数据。因此，我们需要处理数据以提高其质量。所以，确定数据的质量是至关重要的。以下五个特征将使我们能够辨别数据的质量：
- en: '**Accuracy**: Accuracy is a crucial characteristic of data quality, as having
    inaccurate data can lead to poor ML model performance and consequences in real
    life. To check the accuracy of the data, confirm whether the information represents
    a real-life situation or not.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性**：准确性是数据质量的关键特征，因为不准确的数据可能导致机器学习模型性能不佳，并在现实生活中产生后果。为了检查数据的准确性，确认信息是否代表现实生活中的情况。'
- en: '**Completeness**: In most cases, incomplete information is unusable and can
    lead to incorrect outcomes if an ML model is trained on it. It is vital to check
    the comprehensiveness of the data.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整性**：在大多数情况下，不完整的信息是不可用的，如果机器学习模型基于它进行训练，可能会导致错误的输出。检查数据的完整性至关重要。'
- en: '**Reliability**: Contradictions or duplications in data can lead to the unreliability
    of the data. Reliability is a vital characteristic; trusting the data is essential,
    primarily when it is used to make real-life decisions using ML. To some degree,
    we can assess the reliability of data by examining bias and distribution. In case
    of any extremities, the data might not be reliable for ML training or might carry
    bias.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可靠性**：数据中的矛盾或重复可能导致数据不可靠。可靠性是一个关键特性；信任数据至关重要，尤其是在使用机器学习做出现实生活中的决策时。在一定程度上，我们可以通过检查偏差和分布来评估数据的可靠性。在任何极端情况下，数据可能不适合用于机器学习训练，或者可能携带偏差。'
- en: '**Relevance**: The relevance of data plays an essential role in contextualizing
    and determining if irrelevant information is being gathered. Having relevant data
    can enable appropriate decisions in real-life contexts using ML.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相关性**：数据的相关性在确定是否收集了不相关信息以及进行上下文化中起着至关重要的作用。拥有相关数据可以使得在现实生活场景中使用机器学习做出适当的决策。'
- en: '**Timeliness**: Obsolete or out-of-date information costs businesses time and
    money; having up-to-date information is vital in some cases and can improve the
    quality of data. Decisions enabled by ML using untimely data can be costly and
    can lead to wrong decisions.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**及时性**：过时或陈旧的信息会耗费企业的时间和金钱；在某些情况下，拥有最新信息至关重要，并且可以提高数据质量。使用过时数据通过机器学习做出的决策可能代价高昂，并可能导致错误决策。'
- en: When these five characteristics are maximized, it ensures the highest data quality.
    With these principles in mind, let's delve into the implementation, where code
    meets data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当这五个特性最大化时，将确保数据质量达到最高。带着这些原则，让我们深入到实现中，即代码与数据的结合。
- en: 'Firstly, let''s assess the data and process it to get it ready for ML training.
    To get started, clone the repository you imported to your Azure DevOps project
    (from GitHub):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们评估数据并对其进行处理，以便为机器学习训练做好准备。要开始，请将您导入的存储库克隆到您的 Azure DevOps 项目中（从 GitHub）：
- en: '[PRE7]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, open your terminal and access the folder of the cloned repository and
    spin up the JupyterLab server for data processing. To do so, type the following
    command in the terminal:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，打开您的终端，访问克隆存储库的文件夹，并启动用于数据处理的 JupyterLab 服务器。为此，请在终端中输入以下命令：
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This will automatically open a window in your browser at `http://localhost:8888`
    where you can code and execute the code on the JupyterLab interface. In the `Code_meets_data_c3`
    folder, there is a Python script (`dataprocessing.py`) and a `.ipynb` notebook
    (`dataprocessing.ipynb`); feel free to run any of these files or create a new
    notebook and follow the upcoming steps.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这将自动在您的浏览器中打开一个窗口，地址为 `http://localhost:8888`，您可以在其中使用 JupyterLab 界面进行编码和执行代码。在
    `Code_meets_data_c3` 文件夹中，有一个 Python 脚本 (`dataprocessing.py`) 和一个 `.ipynb` 笔记本
    (`dataprocessing.ipynb`)；您可以随意运行这些文件中的任何一个，或者创建一个新的笔记本，并按照接下来的步骤操作。
- en: We will perform computing for tasks as described in *Figure 3.6*. Data processing
    will be done locally on your PC, followed by ML training, deploying, and monitoring
    on compute targets in the cloud. This is to acquire experience of implementing
    models in various setups. In the rest of this chapter, we will do data processing
    (locally) to get the data to the best quality in order to do ML training (in the
    cloud, which is described in the next chapter).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行如图 3.6 所述的任务的计算。数据处理将在您的 PC 上进行，随后在云中的计算目标上进行机器学习训练、部署和监控。这是为了获得在多种设置中实现模型的经验。在本章的剩余部分，我们将进行数据处理（本地），以便将数据质量提升到最佳，以便进行机器学习训练（在下一章中描述的云中进行）。
- en: '![Figure 3.6 – Computation locations for data and ML tasks'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.6 – 数据和机器学习任务的计算位置]'
- en: '](img/B16572_03_06.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16572_03_06.jpg]'
- en: Figure 3.6 – Computation locations for data and ML tasks
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 数据和机器学习任务的计算位置
- en: 'To process raw data and get it ready for ML, you will do the compute and data
    processing on your local PC. We start by installing and importing the required
    packages and importing the raw dataset (as shown in the `dataprocessing.ipynb`
    and `.py` scripts). Python instructions in the notebooks must be executed in the
    existing notebook:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理原始数据并将其准备好用于机器学习，您将在您的本地 PC 上进行计算和数据处理。我们首先安装和导入所需的包，并导入原始数据集（如 `dataprocessing.ipynb`
    和 `.py` 脚本中所示）。笔记本中的 Python 指令必须在现有的笔记本中执行：
- en: '[PRE9]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With this, you have imported the dataset into a pandas DataFrame, `df`, for
    further processing.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，您已经将数据集导入了一个 pandas DataFrame，名为 `df`，以便进行进一步的处理。
- en: Data preprocessing
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Raw data cannot be directly passed to the ML model for training purposes. We
    have to refine or preprocess the data before training the ML model. To further
    analyze the imported data, we will perform a series of steps to preprocess the
    data into a suitable shape for the ML training. We start by assessing the quality
    of the data to check for accuracy, completeness, reliability, relevance, and timeliness. After
    this, we calibrate the required data and encode text into numerical data, which
    is ideal for ML training. Lastly, we will analyze the correlations and time series,
    and filter out irrelevant data for training ML models.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据不能直接用于训练机器学习模型。在训练机器学习模型之前，我们必须对数据进行精炼或预处理。为了进一步分析导入的数据，我们将执行一系列步骤，将数据预处理成适合机器学习训练的形状。我们首先评估数据的质量，以检查准确性、完整性、可靠性、相关性和时效性。在此之后，我们将校准所需数据并将文本编码为数值数据，这对于机器学习训练是理想的。最后，我们将分析相关性和时间序列，并过滤掉与训练机器学习模型无关的数据。
- en: Data quality assessment
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据质量评估
- en: 'To assess the quality of the data, we look for accuracy, completeness, reliability,
    relevance, and timeliness. Firstly, let''s check if the data is complete and reliable
    by assessing the formats, cumulative statistics, and anomalies such as missing
    data. We use pandas functions as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估数据的质量，我们寻找准确性、完整性、可靠性、相关性和时效性。首先，让我们检查数据是否完整和可靠，通过评估格式、累积统计和异常，如缺失数据。我们使用以下
    pandas 函数：
- en: '[PRE10]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'By using the `describe` function, we can observe descriptive statistics in
    the output as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 `describe` 函数，我们可以在输出中观察到以下描述性统计：
- en: '![Figure 3.7 – Descriptive statistics of the DataFrame'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.7 – DataFrame 的描述性统计'
- en: '](img/B16572_03_07.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16572_03_07.jpg)'
- en: Figure 3.7 – Descriptive statistics of the DataFrame
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – DataFrame 的描述性统计
- en: 'Some observations can be made to conclude the data is coherent, and relevant
    as it depicts real-life statistics such as a mean temperature of ~11 C and a wind
    speed of ~10 kmph. Minimum temperatures in Finland tend to reach around ~-21 C,
    and there is an average visibility of 10 km. Facts like these depict the relevance
    and data origin conditions. Now, let''s observe the column formats:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 可以进行一些观察，以得出数据是一致的、相关的结论，因为它描述了现实生活中的统计数据，如平均温度约为 ~11 摄氏度和风速约为 ~10 公里/小时。芬兰的最低温度通常达到约
    ~-21 摄氏度，平均能见度为 10 公里。这样的事实描述了数据的关联性和数据来源条件。现在，让我们观察列格式：
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here are the formats of each column:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是每列的格式：
- en: '`S_No`                                           `int64`'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`S_No`                                           `int64`'
- en: '`Timestamp`                                `object`'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Timestamp`                                `object`'
- en: '`Location`                                     `object`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Location`                                     `object`'
- en: '`Temperature_C`                         `float64`'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Temperature_C`                         `float64`'
- en: '`Apparent_Temperature_C`      `float64`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Apparent_Temperature_C`      `float64`'
- en: '`Humidity`                                    `float64`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Humidity`                                    `float64`'
- en: '`Wind_speed_kmph`                  `float64`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Wind_speed_kmph`                  `float64`'
- en: '`Wind_bearing_degrees`           `int64`'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Wind_bearing_degrees`           `int64`'
- en: '`Visibility_km`                              `float64`'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Visibility_km`                              `float64`'
- en: '`Pressure_millibars`                    `float64`'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pressure_millibars`                    `float64`'
- en: '`Weather_conditions`                `object`'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Weather_conditions`                `object`'
- en: '`dtype:`                                         `object`'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype:`                                         `object`'
- en: 'Most of the columns are numerical (`float` and `int`), as expected. The `Timestamp`
    column is in `object` format, which needs to be changed to `DateTime` format:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数列都是数值型（`float` 和 `int`），正如预期的那样。`Timestamp` 列是 `object` 格式，需要将其更改为 `DateTime`
    格式：
- en: '[PRE12]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Using pandas'' `to_datetime` function, we convert `Timestamp` to `DateTime`
    format. Next, let''s see if there are any null values. We use pandas'' `isnull`
    function to check this:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pandas 的 `to_datetime` 函数，我们将 `Timestamp` 转换为 `DateTime` 格式。接下来，让我们看看是否存在任何空值。我们使用
    pandas 的 `isnull` 函数来检查这一点：
- en: '[PRE13]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Upon checking for any null values, if null values are discovered, as a next
    step the calibration of missing data is essential.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查任何空值后，如果发现空值，则作为下一步，校准缺失数据至关重要。
- en: Calibrating missing data
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 校准缺失数据
- en: 'It is not ideal to have missing values in the data as it is a sign of poor
    data quality. Missing data or values can be replaced using various techniques
    without compromising the correctness and reliability of data. After inspecting
    the data we have been working on, some missing values are observed. We use the
    `Forward fill` method to handle missing data:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中存在缺失值并不理想，因为这表明数据质量较差。可以使用各种技术来替换缺失数据或值，而不会影响数据的正确性和可靠性。在检查我们所工作的数据后，我们发现了一些缺失值。我们使用`前向填充`方法来处理缺失数据：
- en: '[PRE14]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`NaN` or null values have only been observed in the `Weather_conditions` column.
    We replace the `NaN` values by using the `fillna()` method from pandas and the
    forward fill (`ffill`) method. As weather is progressive, it is likely to replicate
    the previous event in the data. Hence, we use the forward fill method, which replicates
    the last observed non-null value until another non-null value is encountered.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`NaN`或空值仅在`Weather_conditions`列中观察到。我们使用pandas的`fillna()`方法和前向填充（`ffill`）方法来替换`NaN`值。由于天气是渐进的，数据中可能会复制上一个事件。因此，我们使用前向填充方法，该方法复制最后一个观察到的非空值，直到遇到另一个非空值。'
- en: Label encoding
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标签编码
- en: 'As the machines do not understand human language or text, all the text has
    to be converted into numbers. Before that, let''s process the text. We have a
    `Weather_conditons` column in text with values or labels such as `rain`, `snow`,
    and `clear`. These values are found using pandas'' `value_counts()` function,
    as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器不理解人类语言或文本，所有文本都必须转换为数字。在那之前，让我们处理文本。我们有一个包含文本值或标签的`Weather_conditions`列，例如`rain`、`snow`和`clear`。这些值是通过pandas的`value_counts()`函数找到的，如下所示：
- en: '[PRE15]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`Weather_conditions` can be simplified by categorizing the column label into
    two labels, `rain` or `no_rain`. Forecasting in these two categories will enable
    us to solve the business problem for the cargo company:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`Weather_conditions`可以通过将列标签分类为两个标签`rain`或`no_rain`来简化。在这两个类别中进行预测将使我们能够解决货运公司的业务问题：'
- en: '[PRE16]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will replace both `snow` and `clear` values with `no_rain` as both conditions
    imply no rain conditions at the port. Now that labels are processed, we can convert
    the `Weather_conditions` column into a machine-readable form or numbers using
    `rain` and `no_rain`, label encoding can be efficient as it converts these values
    to 0 and 1\. If there are more than two values, **one-hot encoding** is a good
    choice because assigning incremental numbers to categorical variables can give
    the variables higher priority or numerical bias during training. One-hot encoding
    prevents bias or higher preference for any variable, ensuring neutral privileges
    to each value of categorical variables. In our case, as we have only two categorical
    variables, we perform label encoding using scikit-learn as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这将用`no_rain`替换`snow`和`clear`值，因为这两种条件都意味着港口没有降雨。现在标签已经处理完毕，我们可以使用`rain`和`no_rain`将`Weather_conditions`列转换为机器可读的形式或数字。标签编码可以很有效，因为它将这些值转换为0和1。如果有超过两个值，**独热编码**是一个不错的选择，因为将增量数字分配给分类变量可以在训练期间赋予变量更高的优先级或数值偏差。独热编码防止对任何变量的偏差或更高的偏好，确保对分类变量的每个值都给予中立的特权。在我们的案例中，因为我们只有两个分类变量，所以我们使用scikit-learn执行标签编码，如下所示：
- en: '[PRE17]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here, we import the `LabelEncoder()` function, which will encode the `Weather_conditions`
    column into 0s and 1s using the `fit_transform()` method. We can do this by replacing
    the previous textual column with a label encoded or machine-readable form to column
    `Weather_condition` as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们导入`LabelEncoder()`函数，该函数将使用`fit_transform()`方法将`Weather_conditions`列编码为0和1。我们可以通过以下方式用标签编码或机器可读的形式替换之前的文本列到`Weather_condition`列：
- en: '[PRE18]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, we concatenate our new label-encoded or machine-readable `Weather_condition`
    column to the DataFrame and drop the previous non-machine readable or textual
    `Weather_conditions` column. Data is now in machine-readable form and ready for
    further processing. You can check the transformed data by executing `df.head()`
    in the notebook (optional).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将我们的新标签编码或机器可读的`Weather_condition`列连接到DataFrame中，并删除之前的非机器可读或文本形式的`Weather_conditions`列。数据现在以机器可读的形式存在，并准备好进行进一步处理。您可以在笔记本中执行`df.head()`来检查转换后的数据（可选）。
- en: New feature – Future_weather_condition
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 新功能 – 未来天气状况
- en: 'As we are tasked with forecasting weather conditions 4 hours in the future,
    we create a new feature named `Future_weather_condition` by shifting `Current_weather_condition`
    by four rows, as each row is recorded with a time gap of an hour. `Future_weather_condition`
    is the label of future weather conditions 4 hours ahead. We will use this new
    feature as a dependent variable to forecast using ML:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是预测4小时后的天气状况，因此我们通过将`Current_weather_condition`向右移动四行来创建一个名为`Future_weather_condition`的新特征，因为每一行记录的时间间隔为一个小时。`Future_weather_condition`是4小时后未来天气状况的标签。我们将使用这个新特征作为依赖变量，使用机器学习进行预测：
- en: '[PRE19]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We will use pandas' `dropna()` function on the DataFrame to discard or drop
    null values, because some rows will have null values due to shifting to a new
    column.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在DataFrame上使用pandas的`dropna()`函数来丢弃或删除空值，因为一些行由于移动到新列而会有空值。
- en: Data correlations and filtering
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据相关性和过滤
- en: 'Now that the data is fully machine readable, we can observe the correlations
    using the **Pearson correlation coefficient** to observe how every single column
    is related to the other columns. Data and feature correlation is a vital step
    before feature selection for ML model training, especially when the features are
    continuous, like in our case. The Pearson correlation coefficient is a statistical
    linear correlation between each variable (*X* and *y*) that produces a value between
    *+1* and *-1*. A value of *+1* is a positive linear correlation, *-1* is a negative
    linear correlation, and *0* is no linear correlation. It can be used to understand
    the relationship between continuous variables, though it is worth noting that
    Pearson correlation does not mean causation. We can observe Pearson correlation
    coefficients for our data using pandas as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经完全机器可读，我们可以使用**皮尔逊相关系数**来观察每一列与其他列之间的关系。在机器学习模型训练之前，数据与特征的相关性分析是特征选择的重要步骤，尤其是在特征是连续的，如我们案例中那样。皮尔逊相关系数是每个变量（*X*和*y*）之间的统计线性相关，产生一个介于*+1*和*-1*之间的值。*+1*表示正线性相关，*-1*表示负线性相关，而*0*表示没有线性相关。它可以用来理解连续变量之间的关系，但值得注意的是，皮尔逊相关系数并不意味着因果关系。我们可以使用pandas来观察我们数据中的皮尔逊相关系数，如下所示：
- en: '[PRE20]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here is the heatmap of the `Pearson` correlation results:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`Pearson`相关系数热图：
- en: '![Figure 3.8 – Heatmap of correlation scores'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.8 – 相关系数热图'
- en: '](img/B16572_03_08.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16572_03_08.jpg)'
- en: Figure 3.8 – Heatmap of correlation scores
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 – 相关系数热图
- en: 'From the heatmap in *Figure 3.8*, we can see that the `Temperature` and `Apparent_Temperature_C`
    coefficient is `0.99`. `S_No` (Serial number) is a continuous value, which is
    more or less like an incremental index for a DataFrame and can be discarded or
    filtered out as it does not provide great value. Hence both `Apparent_Temperature`
    and `S_No` are dropped or filtered. Now let''s observe our dependent variable,
    `Future_weather_condition`, and its correlation with other independent variables:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图3.8*中的热图中，我们可以看到`Temperature`和`Apparent_Temperature_C`的系数为`0.99`。`S_No`（序列号）是一个连续值，它大致相当于DataFrame的增量索引，可以丢弃或过滤掉，因为它并不提供很大的价值。因此，`Apparent_Temperature`和`S_No`都被丢弃或过滤掉了。现在让我们观察我们的依赖变量`Future_weather_condition`及其与其他自变量之间的相关性：
- en: '![Figure 3.9 – Pearson correlation for Future_weather_condition'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.9 – `Future_weather_condition`的皮尔逊相关系数'
- en: '](img/B16572_03_09.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16572_03_09.jpg)'
- en: Figure 3.9 – Pearson correlation for Future_weather_condition
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 – `Future_weather_condition`的皮尔逊相关系数
- en: Anything between 0.5 and 1.0 has a positive correlation and anything between
    -0.5 and -1.0 has a negative correlation. Judging from the graph, there is a positive
    correlation with `Current_weather_condition`, and `Temperature_C` is also positively
    correlated with `Future_weather_c`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 任何介于0.5和1.0之间的值都有正相关，任何介于-0.5和-1.0之间的值都有负相关。从图中可以看出，与`Current_weather_condition`有正相关，`Temperature_C`也与`Future_weather_c`正相关。
- en: Time series analysis
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列分析
- en: 'As the temperature is a continuous variable, it is worth observing its progression
    over time. We can visualize a time series plot using matplotlib as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 由于温度是一个连续变量，观察其随时间的变化趋势是很有意义的。我们可以使用matplotlib可视化时间序列图，如下所示：
- en: '[PRE21]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here''s the resulting plot:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成的图表：
- en: '![ Figure 3.10 – Time series progression of Temperature in C'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.10 – 温度随时间的变化趋势图'
- en: '](img/B16572_03_10.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16572_03_10.jpg)'
- en: Figure 3.10 – Time series progression of Temperature in C
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 – 温度随时间的变化趋势图
- en: After assessing the time series progression of temperature in *Figure 3.10*,
    we can see that it depicts a stationary pattern since the mean, variance, and
    covariance are observed to be stationary over time. Stationary behaviors can be
    trends, cycles, random walks, or a combination of the three. It makes sense, as
    temperature changes over seasons and follows seasonal patterns. This brings us
    to the end of data analysis and processing; we are now ready to register the processed
    data in the workspace before proceeding to train the ML model.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估 *3.10* 图中温度的时间序列进展后，我们可以看到它描绘了一个平稳模式，因为均值、方差和协方差在时间上观察到是平稳的。平稳行为可以是趋势、周期、随机游走或这三种的结合。这是有意义的，因为温度随季节变化并遵循季节性模式。这使我们到达数据分析和处理阶段的尾声；我们现在准备好在训练机器学习模型之前将处理后的数据注册到工作区。
- en: Data registration and versioning
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据注册和版本控制
- en: 'It is vital to register and version the data in the workspace before starting
    ML training as it enables us to backtrack our experiments or ML models to the
    source of data used for training the models. The purpose of versioning the data
    is to backtrack at any point, to replicate a model''s training, or to explain
    the workings of the model as per the inference or testing data for explaining
    the ML model. For these reasons, we will register the processed data and version
    it to use it for our ML pipeline. We will register and version the processed data
    to the Azure Machine Learning workspace using the Azure Machine Learning SDK as
    follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始机器学习训练之前，注册和版本控制工作区中的数据至关重要，因为它使我们能够回溯我们的实验或机器学习模型到用于训练模型的数据源。版本化数据的目的是在任何时间点回溯，以复制模型的训练，或根据推理或测试数据解释模型的工作原理。出于这些原因，我们将注册处理后的数据并将其版本化，以便用于我们的机器学习管道。我们将使用
    Azure 机器学习 SDK 将处理后的数据注册和版本化到 Azure 机器学习工作区，如下所示：
- en: '[PRE22]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Fetch your `subscription ID`, `resource_group` and `workspace_name` from the
    Azure Machine Learning portal, as shown in *Figure 3.11*:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Azure 机器学习门户获取您的 `subscription ID`、`resource_group` 和 `workspace_name`，如图
    *3.11* 所示：
- en: '![Figure 3.11 – Workspace credentials (Resource group, Subscription ID, and
    Workspace name)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 3.11 – Workspace credentials (Resource group, Subscription ID, and
    Workspace name)]'
- en: '](img/B16572_03_11.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16572_03_11.jpg]'
- en: Figure 3.11 – Workspace credentials (Resource group, Subscription ID, and Workspace
    name)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 3.11 – Workspace credentials (Resource group, Subscription ID, and
    Workspace name)]'
- en: 'By requesting the workspace credentials, a workspace object is obtained. When
    running the `Workspace()` function, your notebook will be connected to the Azure
    platform. You will be prompted to click on an authentication link and provide
    a random code and the Azure account details. After that, the script will confirm
    the authentication. Using the workspace object, we access the default data store
    and upload the required data files to the data store on Azure Blob Storage connected
    to the workspace:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 通过请求工作区凭据，将获得一个工作区对象。当运行 `Workspace()` 函数时，您的笔记本将连接到 Azure 平台。您将被提示点击一个身份验证链接，并提供一个随机代码和
    Azure 账户详情。之后，脚本将确认身份验证。使用工作区对象，我们访问默认数据存储并将所需的数据文件上传到与工作区连接的 Azure Blob Storage
    上的数据存储：
- en: '[PRE23]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`Tabular.from_delimited_files()` may cause a failure in Linux or MacOS machines
    that do not have .NET Core 2.1 installed. For correct installation of this dependency,
    follow these instructions: [https://docs.microsoft.com/en-us/dotnet/core/install/linux](https://docs.microsoft.com/en-us/dotnet/core/install/linux).
    After successfully executing the preceding commands, you will upload the data
    file to the data store and see the result shown in *Figure 3.12*. You can preview
    the dataset from the datastore as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tabular.from_delimited_files()` 函数可能在未安装 .NET Core 2.1 的 Linux 或 MacOS 机器上导致失败。为了正确安装此依赖项，请按照以下说明操作：[https://docs.microsoft.com/en-us/dotnet/core/install/linux](https://docs.microsoft.com/en-us/dotnet/core/install/linux)。成功执行上述命令后，您将上传数据文件到数据存储，并看到如图
    *3.12* 所示的结果。您可以通过以下方式从数据存储预览数据集：'
- en: '[PRE24]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'When the data is uploaded to the data store, then we will register the dataset
    to the workspace and version it as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据上传到数据存储时，我们将按照以下方式将数据集注册到工作区并对其进行版本控制：
- en: '[PRE25]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `register(...)` function registers the dataset to the workspace, as shown
    in *Figure 3.12*. For detailed documentation, visit https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets#register-datasets:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`register(...)` 函数将数据集注册到工作区，如图 *3.12* 所示。有关详细文档，请访问 https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets#register-datasets:'
- en: '![Figure 3.12 – Processed dataset registered in the ML workspace'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.12 – 已注册在ML工作区中的处理数据集]'
- en: '](img/B16572_03_12.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16572_03_12.jpg]'
- en: Figure 3.12 – Processed dataset registered in the ML workspace
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 – 已注册在ML工作区中的处理数据集
- en: Toward the ML Pipeline
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面向ML管道
- en: So far, we have processed the data by working on irregularities such as missing
    data, selected features by observing correlations, created new features, and finally
    ingested and versioned the processed data to the Machine learning workspace. There
    are two ways to fuel the data ingestion for ML model training in the ML pipeline.
    One way is from the central storage (where all your raw data is stored) and the
    second way is using a feature store. As knowledge is power, Let's get to know
    the use of the feature store before we move to the ML pipeline.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经通过处理不规则性（如缺失数据）、通过观察相关性选择特征、创建新特征，最后将处理后的数据导入并版本化管理到机器学习工作区。在机器学习管道中，有两种方式可以加载数据以用于ML模型训练。一种是从中央存储（存储所有原始数据的地方）加载，第二种方式是使用特征存储。因为知识就是力量，在我们进入机器学习管道之前，让我们先了解特征存储的使用。
- en: Feature Store
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征存储
- en: A feature store compliments the central storage by storing important features
    and make them available for training or inference. A feature store is a store
    where you transform raw data into useful features that ML models can use directly
    to train and infer to make predictions. Raw Data typically comes from various
    data sources, which are structured, unstructured, streaming, batch, and real-time.
    It all needs to get pulled, transformed (using a feature pipeline), and stored
    somewhere, and that somewhere can be the feature store. The feature store then
    takes the data and makes it available for consumption. Data scientists tend to
    duplicate work (especially data processing). It can be avoided if we have a centralized
    feature store. Feature store allows data scientists to efficiently share and reuse
    features with other teams and thereby increase their productivity as they don't
    have to pre-process features from scratch.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储通过存储重要特征并使其可用于训练或推理来补充中央存储。特征存储是一个将原始数据转换为ML模型可以直接用于训练和推理以做出预测的有用特征的存储库。原始数据通常来自各种数据源，这些数据源可以是结构化、非结构化、流式、批量或实时的。所有这些都需要被拉取、转换（使用特征管道）并存储在某处，而那个地方可以是特征存储。然后特征存储将数据提供给消费。数据科学家倾向于重复工作（尤其是数据处理）。如果我们有一个集中的特征存储，就可以避免这种情况。特征存储允许数据科学家与其他团队高效地共享和重用特征，从而提高他们的生产力，因为他们不必从头开始预处理特征。
- en: '![Figure 3.13: Feature store workflow'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.13：特征存储工作流程]'
- en: '](img/B16572_03_13.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16572_03_13.jpg]'
- en: 'Figure 3.13: Feature store workflow'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13：特征存储工作流程
- en: 'As we can see in *Figure 3.13*, a **Feature Store** is using a **Feature Pipeline**
    connected to a **Central Storage** (which stores data from multiple sources) to
    transform and store raw data into useful features for ML training. The features
    stored in the feature store can be retrieved for training, serving, or discovering
    insights or trends. Here are some benefits of using a feature Store:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图3.13*所示，**特征存储**使用一个连接到**中央存储**（存储来自多个来源的数据）的**特征管道**来转换和存储原始数据，使其成为ML训练的有用特征。存储在特征存储中的特征可以用于训练、服务或发现洞察或趋势。以下是使用特征存储的一些好处：
- en: Efficient **Feature Engineering** for **Training Data**
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为**训练数据**进行高效的**特征工程**
- en: Avoid unnecessary data pre-processing before training
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免在训练前进行不必要的预处理
- en: Avoid repetitive feature engineering
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免重复的特征工程
- en: Features available for quick inferencing (testing)
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于快速推理（测试）的特征
- en: System support for serving of features
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统支持特征服务
- en: Exploratory Data Analysis by feature Store
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过特征存储进行探索性数据分析
- en: Opportunity to reuse models features
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新使用模型特征的机会
- en: Quick Queries on features
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对特征进行快速查询
- en: Reproducibility for training data sets
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据集的可重复性
- en: Monitoring feature drift in production (we will learn about feature drift in
    [*Chapter 12*](B16572_12_Final_JM_ePub.xhtml#_idTextAnchor222), *Model Serving
    and Monitoring*)
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控生产中的特征漂移（我们将在[*第12章*](B16572_12_Final_JM_ePub.xhtml#_idTextAnchor222)，*模型服务和监控*)中学习特征漂移）
- en: Features available for data drift monitoring
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于数据漂移监控的功能
- en: It is good to know the advantages of a feature store as it can be useful to
    fuel the ML pipeline (especially the data ingestion step), however not suitable
    for all cases. It depends on your use case. For our use case implementation, we
    will not use feature store but proceed to liaisoning data directly from central
    storage where we have preprocessed and registered the datasets we need for training
    and testing. With ingested and versioned data, you are set to proceed towards
    building your ML Pipeline. The ML pipeline will enable further feature engineering,
    feature scaling, curating training, and testing datasets that will be used to
    train ML models and tune hyperparameters for machine learning training. The ML
    pipeline and functionalities will be performed over cloud computing resources,
    unlike locally on your computer as we did in this chapter. It will be purely cloud-based.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 了解特征存储的优势是好的，因为它可以为机器学习管道（尤其是数据摄取步骤）提供动力，但并不适用于所有情况。这取决于你的用例。对于我们的用例实现，我们不会使用特征存储，而是直接从中央存储中联络数据，在那里我们已经预处理并注册了我们用于训练和测试的数据集。有了摄取和版本化的数据，你就可以开始构建你的机器学习管道了。机器学习管道将使进一步的特性工程、特性缩放、整理训练和测试数据集成为可能，这些数据集将被用于训练机器学习模型和调整机器学习训练的超参数。机器学习管道和功能将在云计算资源上执行，而不是像本章中我们在本地计算机上所做的那样。它将是纯云基础的。
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have learned how to identify a suitable ML solution to a
    business problem and categorize operations to implement suitable MLOps. We set
    up our tools, resources, and development environment. 10 principles of source
    code management were discussed, followed by data quality characteristics. Congrats!
    So far, you have implemented a critical building block of the MLOps workflow –
    data processing and registering processed data to the workspace. Lastly, we had
    a glimpse into the essentials of the ML pipeline.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何识别适合商业问题的机器学习解决方案，并将操作分类以实施合适的MLOps。我们设置了我们的工具、资源和开发环境。讨论了10个源代码管理的原则，随后是数据质量特性。恭喜！到目前为止，你已经实现了MLOps工作流程的关键构建块——数据处理和将处理后的数据注册到工作区。最后，我们简要了解了机器学习管道的基本要素。
- en: 'In the next chapter, you will do the most exciting part of MLOps: building
    the ML pipeline. Let''s press on!'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，你将进行MLOps中最激动人心的部分：构建机器学习管道。让我们继续前进！
