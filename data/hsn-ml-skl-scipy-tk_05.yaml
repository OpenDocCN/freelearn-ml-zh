- en: Preparing Your Data
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'In the previous chapter, we dealt with clean data, where all the values were
    available to us, all the columns had numeric values, and when faced with too many
    features, we had a regularization technique on our side. In real life, it will
    often be the case that the data is not as clean as you would like it to be. Sometimes,
    even clean data can still be preprocessed in ways to make things easier for our
    machine learning algorithm. In this chapter, we will learn about the following
    data preprocessing techniques:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们处理的是干净的数据，其中所有值都可以使用，所有列都有数值，当面对过多特征时，我们有正则化技术作为支持。在现实生活中，数据往往不像你期望的那样干净。有时候，即使是干净的数据，也可能会以某种方式进行预处理，以使我们的机器学习算法更容易处理。在本章中，我们将学习以下数据预处理技术：
- en: Imputing missing values
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充缺失值
- en: Encoding non-numerical columns
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码非数值型列
- en: Changing the data distribution
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变数据分布
- en: Reducing the number of features via selection
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过特征选择减少特征数量
- en: Projecting data into new dimensions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据投影到新维度
- en: Imputing missing values
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 填充缺失值
- en: '"It is a capital mistake to theorize before one has data."'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: “在没有数据之前，理论化是一个重大错误。”
- en: – Sherlock Holmes
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: – 夏洛克·福尔摩斯
- en: 'To simulate a real-life scenario where the data has missing values, we will
    create a dataset with people''s weights as a function of their height. Then, we
    will randomly remove 75% of the values in the `height` column and set them to
    `NaN`:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟现实生活中数据缺失的情况，我们将创建一个数据集，记录人的体重与身高的关系。然后，我们将随机删除`height`列中75%的值，并将它们设置为`NaN`：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We used a random number generator with an underlying **bino****mial**/**Bernoulli****distribution
    here to decide whether each sample will be removed. The distribution's *n* value
    is set to `1`—that is, it is a Bernoulli distribution—and its *p* value is set
    to `0.25`—that is, each sample has a 25% chance of staying. Whenever the returned
    value of the generator is `0`, the sample is set to `NaN`. As you can see, due
    to the nature of the random generator, the final percentage of `NaN` values may
    be slightly more or less than 75%.**
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用了一个带有底层**二项**/**伯努利**分布的随机数生成器来决定每个样本是否会被删除。该分布的*n*值设置为`1`——也就是伯努利分布——而*p*值设置为`0.25`——也就是说，每个样本有25%的机会保留。当生成器返回的值为`0`时，该样本被设置为`NaN`。正如你所看到的，由于随机生成器的性质，最终`NaN`值的百分比可能会略高或略低于75%。
- en: '**Here are the first four rows of the DataFrame that we have just created.
    Only the `height` column, with the missing values, and the weights are shown here:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**这是我们刚刚创建的DataFrame的前四行。这里只显示了有缺失值的`height`列和体重：'
- en: '![](img/f0490c7d-9a69-48d0-a630-f9ff69a63fcd.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0490c7d-9a69-48d0-a630-f9ff69a63fcd.png)'
- en: 'We can also check what percentage of each column has missing values by using
    the following code:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用以下代码来检查每一列缺失值的百分比：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When I ran the previous line, 77% of the values were missing. Note that you
    may get a different ratio of missing values than the ones I've got here, thanks
    to the random number generator used.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当我运行前一行时，77%的值是缺失的。请注意，由于使用了随机数生成器，您可能得到的缺失值比例与我这里得到的有所不同。
- en: None of the regressors we have seen so far will accept this data with all the
    `NaN` values in it. Therefore, we need to convert those missing values into something.
    Deciding on which values to fill in place of the missing values is the job of
    the data imputation process.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止我们看到的所有回归器都无法接受包含所有`NaN`值的数据。因此，我们需要将这些缺失值转换为某些值。决定用什么值来填补缺失值是数据填充过程的任务。
- en: There are different kinds of imputation techniques. We are going to try them
    here and observe their effect on our weight estimations. Keep in mind that we
    happen to know the original `height` data without any missing values, and we know
    that using a ridge regressor on the original data gives us an MSE value of `3.4`.
    Let's keep this piece of information as a reference for now.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同类型的填充技术。我们将在这里尝试它们，并观察它们对我们体重估计的影响。请记住，我们恰好知道原始的`height`数据没有任何缺失值，而且我们知道使用岭回归器对原始数据进行回归会得到`3.4`的MSE值。现在就暂时将这个信息作为参考。
- en: Setting missing values to 0
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将缺失值设置为0
- en: 'One simple approach would be to set all the missing values to `0`. The following
    code will make our data usable once more:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是将所有缺失值设置为`0`。以下代码将使我们的数据再次可用：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Fitting a ridge regressor on the newly imputed column will give us an MSE value
    of `365`:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在新填充的列上拟合岭回归器将得到`365`的MSE值：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Although we were able to use the regressor, its error is huge compared to our
    reference scenario. To understand the effect of zero imputation, let''s plot the
    imputed data and use the regressor''s coefficients to see what kind of line it
    created after training. Let''s also plot the original data for comparison. I am
    sure the code for generating the following graph is straightforward to you by
    now, so I''ll skip it:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们能够使用回归器，但其误差与我们的参考场景相比仍然很大。为了理解零填充的效果，让我们绘制填充后的数据，并使用回归器的系数来查看训练后创建的线条类型。让我们还绘制原始数据进行比较。我相信生成以下图表的代码对您来说现在已经很简单了，所以我会跳过它：
- en: '![](img/695556d0-4d87-40c0-9bfa-cc6047a03ddd.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/695556d0-4d87-40c0-9bfa-cc6047a03ddd.png)'
- en: We already know by now that a linear model is only capable of fitting a continuous
    straight line onto the data (or a hyperplane, in the case of higher dimensions).
    We also know that `0` is not a reasonable height for anyone. Nevertheless, with
    zero imputation, we introduced a bunch of values where the heights are `0` and
    the weights range between `10` and `90` or so. This obviously confused our regressor,
    as we can see in the right-hand side graph.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经知道线性模型只能将连续直线拟合到数据上（或在更高维度情况下的超平面）。我们还知道`0`不是任何人的合理身高。尽管如此，在零填充的情况下，我们引入了一堆身高为`0`、体重在`10`到`90`左右的值。这显然让我们的回归器感到困惑，正如我们在右侧图表中所看到的。
- en: A non-linear regressor, such as a decision tree, will be able to deal with this
    problem much better than its linear counterpart. Actually, for tree-based models,
    I'd suggest you try replacing the missing values in *x* with values that don't
    exist in your data. For example, you may experiment with setting the height to
    `-1`, in this case.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性回归器（例如决策树）将能够比其线性对应更好地处理这个问题。实际上，对于基于树的模型，我建议您尝试将*x*中的缺失值替换为数据中不存在的值。例如，在这种情况下，您可以尝试将身高设置为`-1`。
- en: Setting missing values to the mean
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将缺失值设置为均值
- en: Another name for statistical mean is *expected value*. That's because the mean
    serves as a biased estimation of the data. Having that said, replacing missing
    values with the column's mean values sounds like a plausible idea.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 统计均值的另一个名称是*期望值*。这是因为均值充当数据的有偏估计。话虽如此，用列的均值值替换缺失值听起来是一个合理的想法。
- en: In this chapter, I am fitting a regressor on the entire dataset. I am not concerned
    about splitting the data into training and test sets here since I am mainly bothered
    with how the regressor behaves with imputation. Nevertheless, in real life, you
    will just want to learn the mean value of the training set and use it to impute
    the missing values for both the training and test sets.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我正在整个数据集上拟合一个回归器。我不关心将数据拆分为训练集和测试集，因为我主要关心回归器在填充后的行为。尽管如此，在现实生活中，您只需了解训练集的均值，并使用它来填补训练集和测试集中的缺失值。
- en: 'scikit-learn''s `SimpleImputer` feature makes it possible to find out the mean
    value from the training set and use it to impute both the training and test sets.
    It does so by using our favorite `fit()` and `transform()` methods. But let''s
    stick to a one-step `fit_transform()` function here since we only have one set:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的`SimpleImputer`功能使得可以从训练集中找出均值并将其用于填补训练集和测试集。它通过我们喜爱的`fit()`和`transform()`方法来实现。但在这里我们将坚持一步`fit_transform()`函数，因为我们只有一个数据集：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We have a single column to impute here, which is why I used `[:, 0]` to access
    its values after imputation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们需要填补一个单列，这就是为什么我在填补后使用`[:, 0]`来访问其值。
- en: 'A ridge regressor will give us an MSE value of `302`. To understand where this
    improvement came from, let''s plot the model''s decision and compare it to the
    previous one with zero imputation:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归器将给我们一个MSE值为`302`。为了理解这种改进来自哪里，让我们绘制模型的决策并与零填充前进行比较：
- en: '![](img/98dbc89a-dd22-4eef-b614-ef2a182e3f81.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98dbc89a-dd22-4eef-b614-ef2a182e3f81.png)'
- en: Clearly, the model's decisions make more sense now. You can see how the dotted
    line coincides with the actual non-imputed data points.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，模型的决策现在更有意义了。您可以看到虚线与实际未填充数据点重合。
- en: In addition to using **mean** as a strategy, the algorithm can also find the
    **median** of the training data. The median is usually a better option if your
    data has outliers. In the case of non-numerical features, you should instead use
    the `most_frequent` option as your strategy.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用**均值**作为策略外，该算法还可以找到训练数据的**中位数**。如果您的数据存在异常值，中位数通常是一个更好的选择。在非数值特征的情况下，您应该选择`most_frequent`选项作为策略。
- en: Using informed estimations for missing values
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用有依据的估算填充缺失值
- en: 'Using a single value for all missing values may not be ideal. For example,
    we know here that our data includes male and female samples and each sub-sample
    has a different average height. The `IterativeImputer()` method is an algorithm
    that can use neighboring features to estimate the missing values in a certain
    feature. Here, we use the gender information to infer values to use when imputing
    the missing heights:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有缺失值使用相同的值可能并不理想。例如，我们知道数据中包含男性和女性样本，每个子样本的平均身高不同。`IterativeImputer()` 方法是一种可以利用相邻特征来估算某个特征缺失值的算法。在这里，我们使用性别信息来推断填充缺失身高时应使用的值：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We now have two values to be used for imputation:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了两个用于填充的数据值：
- en: '![](img/2605aa12-8ccd-478a-9c1a-a2de70a0a91d.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2605aa12-8ccd-478a-9c1a-a2de70a0a91d.png)'
- en: The MSE value is `96` this time. This strategy is the clear winner here.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这次 MSE 值为 `96`。这个策略显然是胜者。
- en: We only had one feature with missing values here. In the case of multiple features,
    the `IterativeImputer()` method loops over all the features. It uses all the features
    but one to predict the missing values of the remaining one via regression. Once
    it is done looping over all the features, it may repeat the entire process more
    than once until the values converge. There are parameters to decide which regression
    algorithm to use, what order to use when looping over the features, and what the
    maximum number of iterations allowed is. Clearly, this strategy may be computationally
    expensive with bigger datasets and a higher number of incomplete features. Furthermore,
    the`IterativeImputer()` implementation is still experimental, and its API might
    change in the future.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里只有一个特征有缺失值。在多个特征的情况下，`IterativeImputer()` 方法会遍历所有特征。它使用除一个特征外的所有特征通过回归预测剩余特征的缺失值。完成所有特征遍历后，它可能会多次重复此过程，直到值收敛。该方法有一些参数可以决定使用哪种回归算法、遍历特征时的顺序以及允许的最大迭代次数。显然，对于较大的数据集和更多缺失特征，计算开销可能较大。此外，`IterativeImputer()`
    的实现仍处于实验阶段，其 API 未来可能会发生变化。
- en: A column with too many missing values carries too little information for our
    estimate to use. We can try our best to impute those missing values; but nevertheless,
    dropping the entire column and not using it at all is sometimes the best option,
    especially if the majority of the values are missing.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有过多缺失值的列对我们的估算几乎没有信息价值。我们可以尽力填充这些缺失值；然而，有时放弃整列并完全不使用它，尤其是当大多数值都缺失时，是最好的选择。
- en: Encoding non-numerical columns
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码非数值列
- en: '"Every decoding is another encoding."'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '"每一次解码都是一次编码。"'
- en: – David Lodge
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: – 大卫·洛奇（David Lodge）
- en: 'Non-numeric data is another issue that algorithm implementations cannot deal
    with. In addition to the core scikit-learn implementation, `scikit-learn-contrib`
    has a list of satellite projects. These projects provide additional tools to our
    data arsenal, and here is how they describe themselves:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 非数值数据是算法实现无法处理的另一个问题。除了核心的 scikit-learn 实现之外，`scikit-learn-contrib` 还列出了多个附加项目。这些项目为我们的数据工具库提供了额外的工具，以下是它们如何描述自己的：
- en: '"scikit-learn-contrib is a GitHub organization for gathering high-quality,
    scikit-learn - compatibleprojects. It also provides a template for establishing
    new scikit-learn compatible projects."'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '"scikit-learn-contrib 是一个 GitHub 组织，旨在汇集高质量的 scikit-learn 兼容项目。它还提供了一个模板，用于建立新的
    scikit-learn 兼容项目。"'
- en: 'We are going to use one of these projects here—`category_encoders`. This allows
    us to encode non-numerical data into different forms. First, we will install the
    library using the `pip` installer, as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用一个项目——`category_encoders`。它允许我们将非数值数据编码成不同的形式。首先，我们将使用 `pip` 安装这个库，命令如下：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Before jumping into the different encoding strategies, let''s first create
    a fictional dataset to play with:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解不同的编码策略之前，让我们首先创建一个虚拟数据集来进行实验：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will then split it into two equal halves:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将其分成两个相等的部分：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Keep in mind that the core scikit-learn library implements two of the encoders
    we are going to see here—`preprocessing.OneHotEncoder` and `preprocessing.OrdinalEncoder`.
    Nevertheless, I prefer the `category_encoders` implementation for its richness
    and versatility.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，核心的 scikit-learn 库实现了我们将在此看到的两种编码器——`preprocessing.OneHotEncoder` 和 `preprocessing.OrdinalEncoder`。不过，我更喜欢
    `category_encoders` 的实现，因为它更丰富、更灵活。
- en: Now, on to our first, and most popular, encoding strategy—one-hot encoding.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入第一个也是最流行的编码策略——独热编码（one-hot encoding）。
- en: One-hot encoding
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独热编码
- en: '**One-hot encoding**, also known as dummy encoding, is the most common method
    for dealing with categorical features. If you have a column containing the `red`,
    `green`, and `blue` values, it sounds logical to convert them into three columns—`is_red`,
    `is_green`, and `is_blue`—and fill these columns with ones and zeroes, accordingly.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**一热编码**，也叫虚拟编码，是处理类别特征的最常见方法。如果你有一列包含`red`、`green`和`blue`值的数据，那么将它们转换为三列——`is_red`、`is_green`和`is_blue`——并根据需要填充这些列的值为1和0，看起来是很合乎逻辑的。'
- en: 'Here is the code for decoding our datasets using `OneHotEncoder`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用`OneHotEncoder`解码数据集的代码：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'I set `use_cat_names=True` to use the encoded values when assigning column
    names. The `handle_unknown` parameter tells the encoder how to deal with values
    in the test set that don''t exist in the training set. For example, we have no
    clothing of the `XS` or `S`sizesin our training set. We also don''t have any `Adidas`
    clothing in there. That''s why these records in the test set are converted to`NaN`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我设置了`use_cat_names=True`，以在分配列名时使用编码后的值。`handle_unknown`参数告诉编码器如何处理测试集中在训练集中不存在的值。例如，我们的训练集中没有`XS`或`S`尺码的衣物，也没有`Adidas`品牌的衣物。这就是为什么测试集中的这些记录会被转换为`NaN`：
- en: '![](img/6bd386a2-a300-4117-b707-13c8a1482737.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6bd386a2-a300-4117-b707-13c8a1482737.png)'
- en: You still have to impute those `NaN` values. Otherwise, we can just set those
    values to `0` by setting `handle_unknown` to `value`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你仍然需要填补那些`NaN`值。否则，我们可以通过将`handle_unknown`设置为`value`来将这些值设置为`0`。
- en: One-hot encoding is recommended for linear models and **K-Nearest Neighbor**
    (**KNN**) algorithms. Nevertheless, due to the fact that one column may be expanded
    into too many columns and some of them may be inter-dependent, regularization
    or feature selection are recommended here. We will look further at feature selection
    later in this chapter, and the KNN algorithm will be discussed later in this book.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一热编码推荐用于线性模型和**K-最近邻**（**KNN**）算法。尽管如此，由于某一列可能会扩展成过多列，并且其中一些列可能是相互依赖的，因此建议在这里使用正则化或特征选择。我们将在本章后面进一步探讨特征选择，KNN算法将在本书后面讨论。
- en: Ordinal encoding
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序数编码
- en: 'Depending on your use case, you may need to encode your categorical values
    in a way that reflects their order. If I am going to use this data to predict
    the level of demand for the items, then I know that it isn''t the case that the
    larger the item''s size, the higher the demand for it. So, one-hot encoding may
    still be apt for the sizes here. However, if we are to predict the amount of material
    needed to create each item of clothing, then we need to encode the sizes in a
    way that implies that `XL` needs more material than `L`. In this case, we are
    concerned with the order of those values and so we use `OrdinalEncoder`, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的使用场景，你可能需要以反映顺序的方式对类别值进行编码。如果我要使用这些数据来预测物品的需求量，那么我知道，物品尺码越大，并不意味着需求量越高。因此，对于这些尺码，一热编码仍然适用。然而，如果我们要预测每件衣物所需的材料量，那么我们需要以某种方式对尺码进行编码，意味着`XL`需要比`L`更多的材料。在这种情况下，我们关心这些值的顺序，因此我们使用`OrdinalEncoder`，如下所示：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Note that we have to specify the mapping by hand. We want `XS` to be encoded
    as `1`, `S` as `2`, and so on. As a result, we get the following DataFrame:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们必须手动指定映射。我们希望将`XS`编码为`1`，`S`编码为`2`，依此类推。因此，我们得到了以下的DataFrame：
- en: '![](img/5eacc447-36d8-47b3-a376-2a656401341c.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5eacc447-36d8-47b3-a376-2a656401341c.png)'
- en: This time, the encoded data fits into just one column, and the values missing
    from the training set are encoded as `-1`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，编码后的数据只占用了一列，而训练集中缺失的值被编码为`-1`。
- en: This encoding method is recommended for non-linear models, such as decision
    trees. As for linear models, they may interpret `XL` (encoded as `5`) to be five
    times the size of `XS` (encoded as `1`). That's why one-hot encoding is still
    preferred for linear models. Furthermore, coming up with meaningful mappings and
    setting it by hand can be time-consuming.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这种编码方法推荐用于非线性模型，例如决策树。至于线性模型，它们可能会将`XL`（编码为`5`）解释为`XS`（编码为`1`）的五倍。因此，对于线性模型，一热编码仍然是首选。此外，手动设置有意义的映射可能会非常耗时。
- en: Target encoding
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标编码
- en: 'One obvious way to encode categorical features, in a supervised learning scenario,
    is to base the encoding on the target values. Say we want to estimate the price
    of an item of clothing. We can replace the brand names with the average price
    for all items of the same brand in our training dataset. Nevertheless, there is
    one obvious problem here. Say one brand happens to appear only once or twice in
    our training set. There is no guarantee that these few appearances are good representations
    of the brand''s price. In another world, using the target values just like that
    may result in overfitting, and the resulting model may not generalize well when
    dealing with new data. That''s why the `category_encoders` library has multiple
    variations of target encoding; they all have the same underlying objective, but
    each of them has a different method for dealing with the aforementioned overfitting
    issue. Here are some examples of these implementations:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在有监督学习场景中，编码类别特征的一种显而易见的方法是基于目标值进行编码。假设我们要估计一件衣物的价格。我们可以将品牌名称替换为我们训练数据集中相同品牌所有物品的平均价格。然而，这里有一个明显的问题。假设某个品牌在我们的训练集中只出现一次或两次。不能保证这几次出现能够很好地代表该品牌的价格。换句话说，单纯使用目标值可能会导致过度拟合，最终模型在处理新数据时可能无法很好地泛化。这就是为什么
    `category_encoders` 库提供了多种目标编码变体的原因；它们都有相同的基本目标，但每种方法都有不同的处理上述过度拟合问题的方式。以下是一些这些实现的示例：
- en: Leave-one-out cross-validation
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 留一法交叉验证
- en: The target encoder
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标编码器
- en: The catboost encoder
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CatBoost 编码器
- en: The M-estimator
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: M估计器
- en: Leave-one-out is probably the most well-known implementation of the ones listed.
    In the training data, it replaces the categorical value in raw data with the mean
    of the corresponding target values of all the rows with the same categorical value
    except for this particular raw data. For the test data, it just uses the mean
    of the corresponding targets of each category value learned from the training
    data. Furthermore, the encoder also has a parameter called `sigma`, which allows
    you to add noise to the learned mean to prevent even moreoverfitting.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 留一法可能是列出的方法中最著名的一种。在训练数据中，它将原始数据中的类别值替换为所有具有相同类别值但不包括该特定原始数据行的其他行的目标值均值。对于测试数据，它只使用从训练数据中学习到的每个类别值对应的目标均值。此外，编码器还有一个名为
    `sigma` 的参数，允许您向学习到的均值添加噪声，以防止过度拟合。
- en: Homogenizing the columns' scale
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 同质化列的尺度
- en: 'Different numerical columns may have different scales. One column''s age is
    in the tens, while its salary is typically in the thousands. As we saw earlier,
    putting different columns into a similar scale helps in some cases. Here are some
    of the cases where scaling is recommended:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的数值列可能具有不同的尺度。一列的年龄可能在十位数，而它的薪资通常在千位数。如我们之前所见，将不同的列调整到相似的尺度在某些情况下是有帮助的。以下是一些建议进行尺度调整的情况：
- en: It allows gradient-descent solvers to converge quicker.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以帮助梯度下降法的求解器更快地收敛。
- en: It is needed for algorithms such as KNN and **Principle Component Analysis**
    (**PCA**)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是KNN和**主成分分析**（**PCA**）等算法所必需的。
- en: When training an estimator, it puts the features on a comparable scale, which
    helps when juxtaposing their learned coefficients.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练估计器时，它将特征放置在一个可比的尺度上，这有助于对比它们的学习系数。
- en: In the next sections, we are going to examine the most commonly used scalers.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨最常用的标准化器。
- en: The standard scaler
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准标准化器
- en: 'This converts the features into normal distribution by setting their mean to
    `0` and their standard deviation to `1`. This is done using the following operation,
    where a column''s mean value is subtracted from each value in it, and then the
    result is divided by the column''sstandard deviation value:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过将特征的均值设置为 `0`，标准差设置为 `1`，将特征转换为正态分布。此操作如下，首先从每个值中减去该列的均值，然后将结果除以该列的标准差：
- en: '![](img/03fc9e7e-443a-43d5-a663-7d920b2f9413.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03fc9e7e-443a-43d5-a663-7d920b2f9413.png)'
- en: 'The scaler''s implementation can be used as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化器的实现可以如下使用：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Once fitted, you can also find out the mean and variance for each column in
    the training data via the `mean_` and `var_` attributes. In the presence of outliers,
    the standard scaler does not guarantee balanced feature scales.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦拟合完成，您还可以通过 `mean_` 和 `var_` 属性查找训练数据中每一列的均值和方差。在存在异常值的情况下，标准化器无法保证特征尺度的平衡。
- en: The MinMax scaler
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MinMax 标准化器
- en: 'This squeezes the features into a certain range, typically between `0` and
    `1`. If you need to use a different range, you can set it using the `feature_range`
    parameter. This scaler works as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这会将特征压缩到一个特定的范围，通常是在`0`到`1`之间。如果你需要使用不同的范围，可以通过`feature_range`参数来设置。这个标准化方法的工作方式如下：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Once fitted, you can also find out the minimum and maximum values for each column
    in the training data with the `data_min_`**and `data_max_`attributes. Since all
    samples are limited to a predefined range, outliers may force inliers to be squeezed
    into a small subset of this range.**
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦拟合，你还可以通过`data_min_`**和`data_max_`属性找出训练数据中每一列的最小值和最大值。由于所有样本都被限制在预定范围内，异常值可能会迫使正常值被压缩到该范围的一个小子集内。**
- en: '**## RobustScaler'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**## RobustScaler'
- en: 'This is similar to the standard scaler, but uses the data quantiles instead
    to be more robust to the outliers'' effect on the mean and standard deviation.
    It''s advised that you use this if your data has outliers, and it can be used
    as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这与标准缩放器相似，但使用数据分位数来增强对异常值对均值和标准差影响的鲁棒性。如果数据中存在异常值，建议使用这个方法，使用方式如下：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Other scalers also exist; however, I have covered the most commonly used scalers
    here. Throughout this book, we will be using the aforementioned scalers. All scalers
    have an `inverse_transform()` method, so you can restore a feature's original
    scales if needed. Furthermore, if you cannot load all training data into memory
    at once, or if the data comes in batches, you can then call the scaler's `partial_fit()`
    method with each batch instead of calling the `fit()` method for the entire dataset
    once.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他的标准化方法；不过，我这里只涵盖了最常用的标准化方法。在本书中，我们将使用上述标准化方法。所有标准化方法都有一个`inverse_transform()`方法，所以如果需要，你可以恢复特征的原始尺度。此外，如果你无法一次性将所有训练数据加载到内存中，或者数据是按批次来的，那么你可以在每一批次上调用标准化方法的`partial_fit()`方法，而不是对整个数据集一次性调用`fit()`方法。
- en: Selecting the most useful features
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择最有用的特征
- en: '"More data, such as paying attention to the eye colors of the people around
    when crossing the street, can make you miss the big truck."'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: “更多的数据，比如在过马路时注意周围人们的眼睛颜色，可能会让你错过那辆大卡车。”
- en: – Nassim Nicholas Taleb
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: – 纳西姆·尼古拉斯·塔勒布
- en: We have seen, in previous chapters, that too many features can degrade the performance
    of our models. What is known as the curse of dimensionality may negatively impact
    an algorithm's accuracy, especially if there aren't enough training samples. Furthermore,
    it can also lead to more training time and higher computational requirements.
    Luckily, we have also learned how to regularize our linear models or limit the
    growth of our decision trees to combat the effect of feature abundance. Nevertheless,
    we may sometimes end up using models where regularization is not an option. Additionally,
    we may still need to get rid of some pointless features to reduce the algorithm's
    training time and computational needs. In these situations, feature selection
    is wise to use as a first step.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经看到，特征过多可能会降低模型的表现。所谓的维度诅咒可能会对算法的准确性产生负面影响，特别是当训练样本不足时。此外，这也可能导致更多的训练时间和更高的计算需求。幸运的是，我们也学会了如何对我们的线性模型进行正则化，或是限制决策树的生长，以应对特征过多的影响。然而，有时我们可能会使用一些无法进行正则化的模型。此外，我们可能仍然需要去除一些无意义的特征，以减少算法的训练时间和计算需求。在这些情况下，特征选择作为第一步是明智的选择。
- en: 'Depending on whether we are dealing with labeled or unlabeled data, we can
    choose different methods for feature selection. Furthermore, some methods are
    more computationally expensive than others, and some lead to more accurate results.
    In the following sections, we are going to see how those different methods can
    be used and, to demonstrate that, we will load scikit-learn''s `wine` dataset:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们处理的是标记数据还是未标记数据，我们可以选择不同的特征选择方法。此外，一些方法比其他方法计算开销更大，有些方法能带来更准确的结果。在接下来的部分中，我们将看到如何使用这些不同的方法，并且为了演示这一点，我们将加载scikit-learn的`wine`数据集：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We then split the data as we usually do:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们像平常一样拆分数据：
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `wine` dataset has 13 features and is used for classification tasks. In
    the following sections, we are going to discover which features are less important
    than the others.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`wine`数据集有13个特征，通常用于分类任务。在接下来的部分中，我们将探索哪些特征比其他特征更不重要。'
- en: VarianceThreshold
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VarianceThreshold
- en: 'If you recall, when we used the `PolynomialFeatures` transformer, it added
    a column where all the values were set to `1`. Additionally, categorical encoders,
    such as one-hot encoding, can result in columns where almost all of the values
    are `0`. It''s also common,in real-life scenarios, to have columns where all the
    data in it is identical or almost identical. Variance is the most obvious way
    to measure the amount of variation in a dataset, so `VarianceThreshold`allows
    us to set a minimum threshold for an accepted variance in each feature. In the
    following code, we will set the variance threshold to `0`. It then goes through
    the training set to learn which features deserve to stay:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，当我们使用`PolynomialFeatures`转换器时，它添加了一列，所有的值都被设置为`1`。此外，像独热编码这样的类别编码器，可能会导致几乎所有值都为`0`的列。在现实场景中，通常也会有某些列，列中的数据完全相同或几乎相同。方差是衡量数据集变异量的最直观方法，因此`VarianceThreshold`允许我们为每个特征设置最小的方差阈值。在以下代码中，我们将方差阈值设置为`0`，然后它会遍历训练集，学习哪些特征应该保留：
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Like all of our other modules, this one also provides the usual `fit()`, `transform()`,
    and `fit_transform()` methods. However, I prefer not to use them here since we
    already gave our columns names, and the `transform()` functions don''t honor the
    names we have given. That''s why I prefer to use another method called `get_support()`.
    This method returns a list of Booleans, where any `False` values correspond to
    columns that ought to be removed based on the threshold we set. Here is how I
    remove unnecessary features using the `pandas` library''s `iloc` function:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 和我们其他所有模块一样，这个模块也提供了常见的`fit()`、`transform()`和`fit_transform()`方法。然而，我更倾向于不在这里使用它们，因为我们已经给我们的列命名了，而`transform()`函数不会尊重我们所赋予的名称。因此，我更喜欢使用另一种方法叫做`get_support()`。这个方法返回一个布尔值列表，任何`False`值对应的列应该被移除，基于我们设置的阈值。以下是我如何使用`pandas`库的`iloc`函数移除不必要的特征：
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can also print the feature names and sort them according to their variance,
    as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以打印特征名并根据它们的方差进行排序，如下所示：
- en: '[PRE18]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This gives us the following table:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下表格：
- en: '![](img/7529285b-b8e7-4bdd-a5f7-f57ee42d0707.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7529285b-b8e7-4bdd-a5f7-f57ee42d0707.png)'
- en: 'We can see that none of our features have zero variance; therefore, none of
    them are removed. You may decide to use a higher threshold—for example, setting
    the threshold to `0.05` will get rid of `nonflavanoid_phenols`. However, let me
    list the key advantages and disadvantages of this module to help you decide when
    and how to use it:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们的特征没有零方差；因此，没有特征会被移除。你可以决定使用更高的阈值——例如，将阈值设置为`0.05`，这将移除`nonflavanoid_phenols`特征。然而，让我列出这个模块的主要优缺点，帮助你决定何时以及如何使用它：
- en: Unlike the other feature selection methods we are going to see in a bit, this
    one does not use data labels when selecting features. This is useful when dealing
    with unlabeled data, as in unsupervised learning scenarios.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与我们接下来会看到的其他特征选择方法不同，这个方法在选择特征时不使用数据标签。在处理无标签数据时，特别是在无监督学习场景中，这非常有用。
- en: The fact that it is label-agnostic also means that a low variance feature might
    still correlate well with our labels and removing it is a mistake.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不依赖标签的特性也意味着，一个低方差的特征可能仍然与我们的标签高度相关，去除它可能会是一个错误。
- en: The variance, just like the mean, is scale-dependent. A list of numbers from
    `1` to `10` has a variance of `8.25`, while the list of `10, 20, 30,...100` has
    a variance of `825.0`. We can clearly see this in the variance of `proline`. This
    makes the numbers in our table incomparable and makes it hard to pick a correct
    threshold. One idea may be to scale your data before calculating its variance.
    However, keep in mind that you cannot use `StandardScaler` since it deliberately
    unifies the variance of all features. So, I would find `MinMaxScaler` more meaningful
    here.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方差和均值一样，依赖于数据的尺度。一个从`1`到`10`的数字列表的方差为`8.25`，而`10, 20, 30,...100`的列表的方差为`825.0`。我们可以从`proline`的方差中清楚地看到这一点。这使得表格中的数字不可比，并且很难选择正确的阈值。一个思路是，在计算方差之前对数据进行缩放。然而，记住你不能使用`StandardScaler`，因为它故意统一了所有特征的方差。所以，我认为在这里使用`MinMaxScaler`更有意义。
- en: In summary, I find the variance threshold handy in removing zero-variance features.
    As for the remaining features, I'd let the next feature selection algorithms deal
    with them, especially when dealing with labeled data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我发现方差阈值对于去除零方差特征非常方便。至于剩余的特征，我会让下一个特征选择算法来处理它们，特别是在处理标记数据时。
- en: Filters
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过滤器
- en: Now that our data comes with labels, it makes sense to use the correlation between
    each feature and the labels to decide which features are more useful for our model.
    This category of feature-selection algorithms deals with each individual feature
    and measures its usefulness in relation to the label; this algorithm is called
    *filters*. In other words, the algorithm takes each column in *x* and uses some
    measure to evaluate how useful it is in predicting *y*. Useful columns stay, while
    the rest are removed. The way that usefulness is measured is what differentiates
    one filter selector from the other. For the sake of clarity, I am going to focus
    on two selectors here since each one has its roots in a different scientific field,
    and understanding them both serves as a good foundation for future concepts. The
    two concepts are **ANOVA (F-values)** and **mutual information**.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据有标签了，因此利用每个特征与标签之间的相关性来决定哪些特征对我们的模型更有用是合乎逻辑的。这类特征选择算法处理每个独立的特征，并根据与标签的关系来衡量其有用性；这种算法叫做
    *filters*。换句话说，算法对 *x* 中的每一列使用某种度量来评估它在预测 *y* 时的有效性。有效的列被保留，而其他列则被移除。衡量有效性的方式就是区分不同筛选器的关键。为了更清楚地说明，我将重点讨论两个筛选器，因为每个筛选器都根植于不同的科学领域，理解它们有助于为未来的概念打下良好的基础。这两个概念是
    **ANOVA (F 值)** 和 **互信息**。
- en: f-regression and f-classif
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: f-regression 和 f-classif
- en: 'As its name suggests, `f_regression`is used for feature selection in regression
    tasks. `f_classif` is its classification cousin. `f_regression` has its roots
    in the field of statistics. Its scikit-learn implementation uses the Pearson correlation
    coefficient to calculate the correlation between each column in *x* and *y*. The
    results are then converted into F-values and P-values, but let''s keep that conversion
    aside since the correlation coefficient is the key here. We start by subtracting
    the mean values for each column from all the values in the same column, which
    is similar to what we did in `StandardScaler`, but without dividing the values
    by their standard deviation. Then, we calculate the correlation coefficient using
    the following formula:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称所示，`f_regression` 用于回归任务中的特征选择。`f_classif` 是它的分类兄弟。`f_regression` 根植于统计学领域。它在
    scikit-learn 中的实现使用皮尔逊相关系数来计算 *x* 和 *y* 中每列之间的相关性。结果会转换为 F 值和 P 值，但我们先不谈这个转换，因为相关系数才是关键。我们首先从每列的所有值中减去均值，这与我们在
    `StandardScaler` 中所做的类似，但不需要将值除以标准差。接着，我们使用以下公式计算相关系数：
- en: '![](img/df560025-dd75-46a6-bf25-99321f67aadf.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df560025-dd75-46a6-bf25-99321f67aadf.png)'
- en: Since the mean is subtracted, the values for the *x* and *y* values are positive
    when an instance is above its column's mean value, and negative when it is below.
    So, this equation is maximized so that every time *x* is above average, *y* is
    also above average, and whenever *x* is below average, *y* follows suit. The maximum
    value for this equation is `1`. We canthen say that *x* and *y* are perfectly
    correlated. The equation is `-1` when *x* and *y* stubbornly go opposite ways,
    in other words negatively correlated. A zero result means that *x* and *y* are
    uncorrelated (that is, independent or orthogonal).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于已减去均值，当一个实例高于其列的均值时，*x* 和 *y* 的值为正，而当其低于均值时，则为负。因此，这个方程会被最大化，使得每当 *x* 高于平均值时，*y*
    也高于平均值，而每当 *x* 低于平均值时，*y* 也随之下降。这个方程的最大值是`1`。因此，我们可以说 *x* 和 *y* 完全相关。当 *x* 和 *y*
    顽固地朝相反方向变化时，即呈负相关，方程值为`-1`。零结果意味着 *x* 和 *y* 不相关（即独立或正交）。
- en: Usually, statisticians write this equation differently. The fact that the mean
    is subtracted from *x* and *y* is usually written down as a part of the equation.
    Then, the numerator is clearly the covariance and the denominator is the product
    of the two variances. Nevertheless, I deliberately chose not to follow the statistical
    convention here so that our natural language processing friends feel at home once
    they realize that this is the exact same equation as for cosine similarity. There,
    *x* and *y* are seen as vectors, the numerator is their dot product, and the denominator
    is the product of their magnitudes. Consequently, the two vectors are perfectly
    correlated (go in the same direction) when the angle between them is `0` (cosine
    `0` = `1`). Conversely, they are independent when they are perpendicular to each
    other, hence the term *orthogonal*. One takeaway from this visual interpretation
    is that this metric only considers the linear relationship between *x* and *y*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，统计学家会以不同的方式书写这个方程。通常会将* x *和* y *减去均值的事实写成方程的一部分。然后，分子显然是协方差，分母是两个方差的乘积。然而，我故意选择不遵循统计学惯例，以便我们的自然语言处理的朋友们在意识到这与余弦相似度的方程完全相同时，能够感到熟悉。在这种情况下，*
    x *和* y *被视为向量，分子是它们的点积，分母是它们的模长的乘积。因此，当它们之间的角度为`0`（余弦`0` = `1`）时，两个向量是完全相关的（方向相同）。相反，当它们彼此垂直时，它们是独立的，因此被称为*正交*。这种可视化解释的一个要点是，这个度量只考虑了*
    x *和* y *之间的线性关系。
- en: For the case of classification, a one-way ANOVA test is performed. This compares
    the variance between the different class labels to the variance within each class.
    Just like its regression cousin, it measures the linear dependence between the
    features and the class labels.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，会执行单因素方差分析（ANOVA）测试。它比较不同类别标签之间的方差与每个类别内部的方差。与回归分析类似，它衡量特征与类别标签之间的线性依赖关系。
- en: 'Enough theory for now; let''s use `f_classif` to pick the most useful features
    in our dataset:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在先不谈太多理论；让我们使用 `f_classif` 来选择数据集中最有用的特征：
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Let's keep the resulting *f* and *p* values to one side for now. After explaining
    the mutual information approach for feature selection, we will use these values
    to contrast the two approaches.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时将结果中的*f*和*p*值放到一边。在解释特征选择的互信息方法之后，我们将使用这些值来对比这两种方法。
- en: Mutual information
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 互信息
- en: 'This approach has its roots in a different scientific field called **inf****ormation****theory**.
    This field was introduced by Claude Shannon to solve issues relating to signal
    processing and data compression. When we send a message made up of zeros and ones,
    we may know the exact content of this message, but can we actually quantify the
    amount of information this very message carries? Shannon solved this problem by
    borrowing the concept of **entropy** from thermodynamics. Further down the line
    comes the concept of **mutual****in****formation**. It quantifies the amount of
    information obtained about one variable when observing another variable. The formula
    for mutual information is as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法起源于一个不同的科学领域，叫做**信息理论**。该领域由克劳德·香农（Claude Shannon）提出，旨在解决与信号处理和数据压缩相关的问题。当我们发送一个由零和一组成的消息时，我们可能知道这个消息的确切内容，但我们能否真正量化这个消息所携带的信息量呢？香农通过借用热力学中的**熵**概念来解决这个问题。进一步发展出来的是**互信息**的概念。它量化了通过观察一个变量时，获得关于另一个变量的信息量。互信息的公式如下：
- en: '![](img/2ffde4a4-87ac-413b-bfd6-51ccaad381c0.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ffde4a4-87ac-413b-bfd6-51ccaad381c0.png)'
- en: 'Before dissecting this equation, keep the following in mind:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在解析这个方程之前，请记住以下几点：
- en: '*P(x)* is the probability of *x* taking a certain value, as is *P(y)* for *y*.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(x)* 是 *x* 取某个特定值的概率，*P(y)* 也是 *y* 取某个特定值的概率。'
- en: '*P(x, y)* is known as joint probability, which is the probability of both *x*
    and *y* taking a specific pair of values.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(x, y)* 被称为联合概率，它表示 *x* 和 *y* 同时取特定一对值的概率。'
- en: '*P(x, y)* only equals the product of *P(x)* * *P(y)*if *x* and *y* are independent.
    Otherwise, its value is more or less than their product, depending on whether
    *x* and *y* are positively or negatively correlated.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(x, y)* 只有在 *x* 和 *y* 独立时才等于 *P(x)* * *P(y)*。否则，根据 *x* 和 *y* 的正相关或负相关关系，它的值会大于或小于它们的乘积。'
- en: The double summation and the first part of the equation, *P(x, y)*, are our
    way of calculating a weighted average for all possible values of *x* and *y*.
    The logarithmic part is what we care about, and it is known as point-wise mutual
    information. If *x* and *y* are independent, the fraction is equal to `1` and
    its logarithm is `0`. In other words, we get `0` when the two variables are uncorrelated.
    Otherwise, the sign of the outcome points to whether *x* and *y* are positively
    or negatively correlated.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 双重求和和方程的第一部分，*P(x, y)*，是我们计算所有可能的 *x* 和 *y* 值的加权平均值的方法。我们关心的是对数部分，它被称为点对点互信息。如果
    *x* 和 *y* 是独立的，那么这个分数等于 `1`，它的对数为 `0`。换句话说，当这两个变量不相关时，结果为 `0`。否则，结果的符号指示 *x* 和
    *y* 是正相关还是负相关。
- en: 'Here is how we get the mutual information coefficient for each feature:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们如何计算每个特征的互信息系数：
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Unlike Pearson's correlation coefficient, mutual information captures any kind
    of correlation, whether it is linear or not.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 与皮尔逊相关系数不同，互信息能够捕捉任何类型的相关性，无论其是否是线性的。
- en: Comparing and using the different filters
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较并使用不同的过滤器
- en: 'Let''s now compare our mutual information scores to the F-values. To do so,
    we will put them both into one DataFrame and use the `pandas` styling feature
    to plot bar charts within the DataFrame, as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来将互信息得分与 F 值进行比较。为此，我们将两者放入一个 DataFrame，并使用 `pandas` 的样式功能在 DataFrame 内绘制柱状图，如下所示：
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This gives us the following DataFrame:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了以下的 DataFrame：
- en: '![](img/fa99ed07-b716-49d6-96b8-5772ea9a0914.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa99ed07-b716-49d6-96b8-5772ea9a0914.png)'
- en: 'As you can see, they mostly agree on the order of feature importance, yet they
    still disagree sometimes. I used each of the two methods to select the top four
    features, then compared the accuracy of a **logistic regression** classifier to
    that of a decision tree classifier with each feature selection method. Here are
    the results of the training set:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它们在特征重要性排序上大体一致，但有时仍会有所不同。我使用这两种方法分别选择了四个最重要的特征，然后比较了 **逻辑回归** 分类器与决策树分类器在每种特征选择方法下的准确性。以下是训练集的结果：
- en: '![](img/3a966549-afc2-4938-8c62-46d178946711.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a966549-afc2-4938-8c62-46d178946711.png)'
- en: As you can tell, each of the two selection methods worked better for one of
    the two classifiers here. It seems that `f_classif` served the linear model better
    due to its linear nature, while the non-linear model favored an algorithm that
    captures non-linear correlations. I have not found any literature confirming the
    generality of this speculation, however.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这两种选择方法分别对两种分类器的效果不同。似乎`f_classif`更适用于线性模型，因为它具有线性特性，而非线性模型则更倾向于捕捉非线性相关性的算法。然而，我并未找到任何文献来确认这一猜测的普遍性。
- en: It is hard not to see the underlying theme that links the two measures. The
    numerator calculates some intra-variable information—the covariance, dot product,
    or join probability. The denominator calculates the product of inter-variable
    information—the variance, norms, or probability. This very theme will continue
    to appear in different topics in the future. One day, we might use cosine similarity
    to compare two documents; another day, we might use mutual information to evaluate
    a clustering algorithm.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 不难看出，两个度量之间有一个潜在的共同主题。分子计算的是一些变量内的信息——协方差、点积或联合概率；分母计算的是变量间的信息的乘积——方差、范数或概率。这个主题将在未来的不同话题中继续出现。有一天，我们可能会使用余弦相似度来比较两篇文档；另一天，我们可能会使用互信息来评估聚类算法。
- en: Evaluating multiple features at a time
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 同时评估多个特征
- en: 'The feature selection methods shownin the *Filters* section of this chapter
    are also regarded as univariate feature selection methods since they check each
    feature separately before deciding whether to keep it. This can result in any
    of the two following issues:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 本章 *Filters* 部分所展示的特征选择方法也被认为是单变量特征选择方法，因为它们会在决定是否保留一个特征之前，单独检查每一个特征。这可能会导致以下两种问题之一：
- en: If two features are highly correlated, we only want to keep one of them. However,
    due to the nature of the univariate feature selection, they will both still be
    selected.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两个特征高度相关，我们只希望保留其中一个。然而，由于单变量特征选择的特性，它们仍然会同时被选择。
- en: If two features are not very useful on their own, yet their combination is useful.
    They will still be removed due to the way the univariate feature selection methods
    work.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两个特征本身并不非常有用，但它们的组合却有用，那么它们仍然会被移除，因为单变量特征选择方法的工作方式就是如此。
- en: 'To deal with these issues, we may decide to use one of the following solutions:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这些问题，我们可以决定使用以下解决方案之一：
- en: '**Using estimators for feature selection**: Typically, regressors and classifiers
    assign values to the features used after training, signifying their importance.
    So, we can use an estimator''s coefficients (or feature importance) to add or
    remove features from our initial feature set. scikit-learn''s **Recursive Feature
    Elimination** (**RFE**) algorithm starts with an initial set of features. Then,
    it iteratively removes features with each iteration using the trained model''s
    coefficients. The `SelectFromModel` algorithm is a meta-transformer that can make
    use of a regularized model to remove features with zero or near-zero coefficients.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用估计器进行特征选择**：通常，回归器和分类器会在训练后给特征赋值，表示它们的重要性。因此，我们可以使用估计器的系数（或特征重要性）来添加或移除我们初始特征集中的特征。scikit-learn的**递归特征消除**（**RFE**）算法从初始特征集开始。然后，它通过每次迭代使用训练模型的系数逐步移除特征。`SelectFromModel`算法是一种元转换器，可以利用正则化模型来移除系数为零或接近零的特征。'
- en: '**Using estimators with built-in feature selection**: In other words, this
    means using a regularized estimator such as lasso, where feature selection is
    part of the estimator''s objectives.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用内置特征选择的估计器**：换句话说，这意味着使用正则化估计器，如Lasso，其中特征选择是估计器目标的一部分。'
- en: In summary, methods such as using variance thresholds and filters are quick
    to perform but have their drawbacks when it comes to feature correlation and interaction.
    More computationally expensive methods, such as wrappers, deal with these issues
    but are prone to overfitting.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，像使用方差阈值和滤波器这种方法执行起来比较快，但在特征相关性和交互作用方面有其缺点。计算开销更大的方法，如包装法，能够解决这些问题，但容易发生过拟合。
- en: If you ask me about my recommendations for feature selection, personally, my
    go-to method would be regularization after removing the zero-variance features,
    unless I am dealing with a huge amount of features where training on the entire
    set is unfeasible. In these cases, I'd use a univariate feature selection method
    while being careful about removing features that might end up being useful. I'd
    still use a regularized model afterward to deal with any multicollinearity.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你问我关于特征选择的建议，个人来说，我的首选方法是在去除零方差特征后进行正则化，除非我处理的是大量特征，在这种情况下，训练整个特征集不可行。对于这种情况，我会使用单变量特征选择方法，同时小心去除那些可能有用的特征。之后，我仍然会使用正则化模型来处理任何多重共线性问题。
- en: In the end, the proof of the pudding is in the eating, and empirical results
    via trial and error may trump my recommendations. Furthermore, besides improving
    the final model's accuracy, feature selection can still be used to understand
    the data at hand. The feature importance scores can still be used to inform business
    decisions. For example, if our label states whether a user is going to churn,
    we can come up with a hypothesis that the top-scoring features affect the churn
    rate the most. Then, we can run experiments by changing the relevant parts of
    our product to see whether we can decrease the churn rate.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，验证一切的标准在于实际效果，通过反复试验和错误得到的实证结果可能会超越我的建议。此外，除了提高最终模型的准确性，特征选择仍然可以用来理解手头的数据。特征重要性评分仍然可以用于指导商业决策。例如，如果我们的标签表示用户是否会流失，我们可以提出一个假设，认为得分最高的特征对流失率的影响最大。然后，我们可以通过调整产品的相关部分来进行实验，看看是否能够减少流失率。
- en: Summary
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Pursuing a data-related career requires a tendency to deal with imperfections.
    Dealing with missing values is one step that we cannot progress without. So, we
    started this chapter by learning about different data imputation methods. Additionally,
    suitable data for one task may not be perfect for another. That's why we learned
    about feature encoding and how to change categorical and ordinal data to fit into
    our machine learning needs. Helping algorithms to perform better can require rescaling
    the numerical features. Therefore, we learned about three scaling methods. Finally,
    data abundance can be a curse on our models, so feature selection is one prescribed
    way to deal with the curse of dimensionality, along with regularization.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 从事与数据相关的职业需要有应对不完美情况的倾向。处理缺失值是我们无法忽视的一步。因此，我们从学习不同的数据填补方法开始这一章。此外，适用于某一任务的数据可能不适用于另一个任务。这就是为什么我们学习了特征编码以及如何将类别数据和顺序数据转换为适合机器学习需求的形式。为了帮助算法表现得更好，我们可能需要重新调整数值特征的尺度。因此，我们学习了三种缩放方法。最后，数据过多可能会成为模型的诅咒，因此特征选择是应对维度灾难的一个有效方法，常与正则化一起使用。
- en: One main theme that ran through this entire chapter is the trade-off between
    simple and quick methods versus more informed and computationally expensive methods
    that may result in overfitting. Knowing which methods to use requires an understanding
    of their underlying theories, in addition to a willingness to experiment and use
    iterations. So, I decided to go a bit deeper into the theoretical background where
    needed, not only so that it helps you pick your methods wisely, but also so that
    it allows you to come up with your own methods in the future.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 贯穿这一章的一个主要主题是简单快速的方法与更为深思熟虑且计算开销大的方法之间的权衡，这些方法可能会导致过拟合。知道该使用哪些方法需要了解它们背后的理论，同时也需要有实验和迭代的意愿。因此，我决定在必要时深入探讨理论背景，这不仅有助于你明智地选择方法，还能让你未来能够提出自己的方法。
- en: Now that we have the main data preprocessing tools on our side, we are ready
    to move on to our next algorithm—KNN.****
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经掌握了主要的数据预处理工具，接下来就可以进入下一个算法——KNN。****
