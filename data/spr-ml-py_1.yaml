- en: First Step Towards Supervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向监督学习迈出的第一步
- en: 'In this book, we will learn about the implementation of many of the common
    machine learning algorithms you interact with in your daily life. There will be
    plenty of math, theory, and tangible code examples to satisfy even the biggest
    machine learning junkie and, hopefully, you''ll pick up some useful Python tricks
    and practices along the way. We are going to start off with a very brief introduction
    to supervised learning, sharing a real-life machine learning demo; getting our
    Anaconda environment setup done; learning how to measure the slope of a curve,
    Nd-curve, and multiple functions; and finally, we''ll discuss how we know whether
    or not a model is good. In this chapter, we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将学习实现许多你在日常生活中接触到的常见机器学习算法。这里将有大量的数学、理论和具体的代码示例，足以满足任何机器学习爱好者的需求，并且希望你在此过程中能学到一些有用的Python技巧和实践。我们将从简要介绍监督学习开始，展示一个真实的机器学习示范；完成Anaconda环境的设置；学习如何衡量曲线的斜率、Nd-曲线和多个函数；最后，我们将讨论如何判断一个模型是否优秀。在本章中，我们将涵盖以下主题：
- en: An example of supervised learning in action
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习应用示例
- en: Setting up the environment
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置环境
- en: Supervised learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Hill climbing and loss functions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爬山法与损失函数
- en: Model evaluation and data splitting
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估与数据分割
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you will need to install the following software, if you haven''t
    already done so:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，如果你还没有安装以下软件，你需要进行安装：
- en: Jupyter Notebook
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jupyter Notebook
- en: Anaconda
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anaconda
- en: Python
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python
- en: The code files for this chapter can be found at [https:/​/​github.​com/​PacktPublishing/
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在[https:/​/​github.​com/​PacktPublishing/](https://github.com/PacktPublishing/)找到。
- en: Supervised-Machine-Learning-with-Python](https://github.com/PacktPublishing/Supervised-Machine-Learning-with-Python).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[Supervised-Machine-Learning-with-Python](https://github.com/PacktPublishing/Supervised-Machine-Learning-with-Python)'
- en: An example of supervised learning in action
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习应用示例
- en: 'First, we will take a look at what we can do with supervised machine learning.
    With the following Terminal prompt, we will launch a new Jupyter Notebook:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将看看可以通过监督机器学习做些什么。通过以下终端提示符，我们将启动一个新的Jupyter Notebook：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once we are inside this top-level, `Hands-on-Supervised-Machine-Learning-with-Python-master`
    home directory, we will go directly inside the `examples` directory:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们进入这个顶级目录`Hands-on-Supervised-Machine-Learning-with-Python-master`，我们将直接进入`examples`目录：
- en: '![](img/6b307221-119e-4772-a9d0-44d3186979f3.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b307221-119e-4772-a9d0-44d3186979f3.png)'
- en: 'You can see that our only Notebook in here is `1.1 Supervised Learning Demo.ipynb`:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们这里唯一的Notebook是`1.1 Supervised Learning Demo.ipynb`：
- en: '![](img/c853b1ee-ae6e-4447-b516-ffc646c710d0.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c853b1ee-ae6e-4447-b516-ffc646c710d0.png)'
- en: We have the supervised learning demo Jupyter Notebook. We are going to be using
    a UCI dataset called the `Spam` dataset. This is a list of different emails that
    contain different features that correspond to spam or not spam. We want to build
    a machine learning algorithm that can predict whether or not we have an email
    coming in that is going to be spam. This could be extremely helpful for you if
    you're running your own email server.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个监督学习示范的Jupyter Notebook。我们将使用一个名为`Spam`的UCI数据集。这个数据集包含了不同的电子邮件，并且每封邮件都有不同的特征，这些特征对应于是否是垃圾邮件。我们希望构建一个机器学习算法，能够预测即将收到的电子邮件是否是垃圾邮件。如果你正在运行自己的电子邮件服务器，这将对你非常有帮助。
- en: 'So, the first function in the following code is simply a request''s get function.
    You should already have the dataset, which is already sitting inside the `examples`
    directory. But in case you don''t, you can go ahead and run the following code.
    You can see that we already have `spam.csv`, so we''re not going to download it:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以下面的代码中的第一个函数只是一个请求的get函数。你应该已经拥有数据集，它已经在`examples`目录中了。但如果没有，你可以继续运行以下代码。你可以看到我们已经有了`spam.csv`，所以我们不需要重新下载：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we will use the `pandas` library. This is a data analysis library from
    Python. You can install it when we go through the next stage, which is the environment
    setup. This library is a data frame data structure that is a kind of native Python,
    which we will use as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`pandas`库。它是一个来自Python的数据分析库。在我们进行下一阶段的环境设置时，你可以安装它。这个库提供了数据框结构，是一种原生的Python数据结构，我们将如下使用它：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This allows us to lay out our data in the following format. We can use all
    sorts of different statistical functions that are nice to use when you''re doing
    machine learning:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许我们将数据以如下格式呈现。我们可以使用各种统计函数，这些函数在进行机器学习时非常实用：
- en: '![](img/9205d451-daa7-42ca-ae8f-075ae077fe9e.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9205d451-daa7-42ca-ae8f-075ae077fe9e.png)'
- en: If some of this terminology is not familiar to you, don't panic yet—we will
    learn about these terminologies in detail over the course of the book.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一些术语你不太熟悉，别着急——我们将在本书中详细学习这些术语。
- en: 'For `train_test_split`, we will take the `df` dataset and split it into two
    parts: train set and test set. In addition to that, we have the target, which
    is a `01` variable that indicates true or false for spam or not spam. We will
    split that as well, which includes the corresponding vector of true or false labels.
    By splitting the labels, we get `3680` training samples and `921` test samples,
    file as shown in the following code snippet:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`train_test_split`，我们将使用`df`数据集并将其分为两个部分：训练集和测试集。此外，我们还会有目标变量，它是一个`01`变量，表示是否为垃圾邮件的真假。我们也会分割这个目标变量，包括相应的真假标签向量。通过划分标签，我们得到`3680`个训练样本和`921`个测试样本，文件如以下代码片段所示：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码的输出如下：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Notice that we have a lot more training samples than test samples, which is
    important for fitting our models. We will learn about this later in the book.
    So, don't worry too much about what's going on here, as this is all just for demo
    purposes.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们的训练样本比测试样本多，这对于调整我们的模型非常重要。我们将在本书后面学习到这一点。所以，暂时不用太担心这里发生了什么，这些内容仅仅是为了演示目的。
- en: 'In the following code, we have the `packtml` library. This is the actual package
    that we are building, which is a classification and regression tree classifier.
    `CARTClassifier` is simply a generalization of a decision tree for both regression
    and classification purposes. Everything we fit here is going to be a supervised
    machine learning algorithm that we build from scratch. This is one of the classifiers
    that we are going to build in this book. We also have this utility function for
    plotting a learning curve. This is going to take our train set and break it into
    different folds for cross-validation. We will fit the training set in different
    stages of numbers of training samples, so we can see how the learning curve converges
    between the train and validation folds, which determines how our algorithm is
    learning, essentially:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们使用了`packtml`库。这是我们正在构建的实际包，是一个分类和回归树分类器。`CARTClassifier`实际上是决策树的一个泛化版本，用于回归和分类目的。我们在这里拟合的所有内容都是我们从头开始构建的监督学习算法。这是我们将在本书中构建的分类器之一。我们还提供了这个用于绘制学习曲线的实用函数。这个函数将接收我们的训练集并将其分成不同的折叠进行交叉验证。我们将在不同阶段拟合训练集中的样本数量，以便我们可以看到学习曲线在训练集和验证集折叠之间的收敛情况，这本质上决定了我们的算法如何学习：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We will go ahead and run the preceding code and plot how the algorithm has learned
    across the different sizes of our training set. You can see we're going to fit
    it for 4 different training set sizes at 3 folds of cross-validation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续运行前面的代码，并绘制算法在不同训练集大小下的学习情况。你可以看到，我们将为四种不同的训练集大小进行拟合，并使用三折交叉验证。
- en: 'So, what we''re actually doing is fitting 12 separate models, which will take
    a few seconds to run:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，实际上我们是在拟合12个独立的模型，这需要几秒钟的时间：
- en: '![](img/1923903f-00ea-423c-985e-9142be846bd2.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1923903f-00ea-423c-985e-9142be846bd2.png)'
- en: In the preceding output, we can see our **Training score** and our **Validation
    score**. The **Training score** diminishes as it learns to generalize, and our
    **Validation score** increases as it learns to generalize from the training set
    to the validation set. So, our accuracy is hovering right around 92-93% on our
    validation set.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的输出中，我们可以看到我们的**训练得分**和**验证得分**。**训练得分**在学习如何泛化时会减少，而我们的**验证得分**随着算法从训练集泛化到验证集而提高。所以，在我们的验证集中，准确率大约徘徊在92%到93%之间。
- en: 'We will use the hyperparameters from the very best one here:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用来自最优模型的超参数：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Logistic regression
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'In this section, we will learn about logistic regression, which is another
    classification model that we''re going to build from scratch. We will go ahead
    and fit the following code:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习逻辑回归，这是我们将从头开始构建的另一个分类模型。我们将继续拟合以下代码：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This is much faster than the decision tree. In the following output, you can
    see that we converge a lot more around the 92.5% range. This looks a little more
    consistent than our decision tree, but it doesn''t perform quite well enough on
    the validation set:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这比决策树要快得多。在下面的输出中，你可以看到我们在92.5%区间附近聚集得更多。这看起来比我们的决策树更一致，但在验证集上的表现还是不够好：
- en: '![](img/fa022926-38db-42f3-8dc2-f205f5120b4f.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa022926-38db-42f3-8dc2-f205f5120b4f.png)'
- en: In the following screenshot, there are encoded records of spam emails. We will
    see how this encoding performs on an email that we can read and validate. So,
    if you have visited the UCI link that was included at the top of the Jupyter Notebook,
    it will provide a description of all the features inside the dataset. We have
    a lot of different features here that are counting the ratio of particular words
    to the number of words in the entire email. Some of those words might be free
    and some credited. We also have a couple of other features that are counting character
    frequencies, the number of exclamation points, and the number of concurrent capital
    runs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在下方的截图中，有垃圾邮件的编码记录。我们将看到这种编码在我们可以读取并验证的邮件上的表现。所以，如果你访问了顶部Jupyter Notebook中包含的UCI链接，它将提供数据集中所有特征的描述。这里有很多不同的特征，它们计算特定词汇与整个邮件中的词汇总数的比例。其中一些词汇可能是免费的，一些是受信的。我们还有其他几个特征，计算字符频率、感叹号的数量以及连续大写字母序列的数量。
- en: 'So, if you have a really highly capitalized set of words, we have all these
    features:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你有一组高度资本化的词汇，我们有这些特征：
- en: '![](img/278f0298-97cf-4a1e-abd2-79fc4470b12d.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/278f0298-97cf-4a1e-abd2-79fc4470b12d.png)'
- en: 'In the following screenshot, we will create two emails. The first email is
    very obviously spam. Even if anyone gets this email, no one will respond to it:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在下方的截图中，我们将创建两封电子邮件。第一封邮件显然是垃圾邮件。即使有人收到这封邮件，也不会有人回应：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output of the preceding code snippet is as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段的输出结果如下：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The second email looks less like spam:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 第二封邮件看起来不像垃圾邮件：
- en: '![](img/239df941-2895-4c5e-b9ab-d7e4d5304eca.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/239df941-2895-4c5e-b9ab-d7e4d5304eca.png)'
- en: The model that we have just fit is going to look at both of the emails and encode
    the features, and will classify which is, and which is not, spam.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚拟合的模型将查看这两封邮件并编码这些特征，然后会分类哪些是垃圾邮件，哪些不是垃圾邮件。
- en: 'The following function is going to encode those emails into the features we
    discussed. Initially, we''re going to use a `Counter` function as an object, and
    tokenize our emails. All we''re doing is splitting our email into a list of words,
    and then the words can be split into a list of characters. Later, we''ll count
    the characters and words so that we can generate our features:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数将把这些电子邮件编码成我们讨论过的特征。最初，我们将使用一个`Counter`函数作为对象，并对电子邮件进行分词。我们所做的就是将电子邮件拆分成一个词汇列表，然后这些词汇可以被拆分成一个字符列表。稍后，我们将计算这些字符和单词的数量，以便生成我们的特征：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'So, all those features that we have up at the beginning tell us what words
    we''re interested in counting. We can see that the original dataset is interested
    in counting words such as address, email, business, and credit, and then, for
    our characters, we''re looking for opened and closed parentheses and dollar signs
    (which are quite relevant to our spam emails). So, we''re going to count all of
    those shown as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，开始时我们提到的所有特征告诉我们我们感兴趣的词汇是什么。我们可以看到，原始数据集关注的词汇包括地址、电子邮件、商业和信用，而对于我们的字符，我们寻找的是打开和关闭的括号以及美元符号（这些与垃圾邮件非常相关）。所以，我们将统计所有这些特征，具体如下：
- en: '![](img/defb48de-1917-450d-8b29-3bfd7edc6f46.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/defb48de-1917-450d-8b29-3bfd7edc6f46.png)'
- en: 'Apply the ratio and keep track of the total number of `capital_runs`, computing
    the mean average, maximum, and minimum:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 应用比例并跟踪`capital_runs`的总数，计算平均值、最大值和最小值：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'When we run the preceding code, we get the following output. This is going
    to encode our emails. This is just simply a vector of all the different features.
    It should be about 50 characters long:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行之前的代码时，我们得到以下输出。这将编码我们的电子邮件。这只是一个包含所有不同特征的向量。它大约应该有50个字符长：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出结果如下：
- en: '![](img/49f17f3e-ff3a-4a64-9765-05e471651690.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49f17f3e-ff3a-4a64-9765-05e471651690.png)'
- en: 'When we fit the preceding values into our models, we will see whether our model
    is any good. So, ideally, we will see that the actual fake email is predicted
    to be fake, and the actual real email is predicted to be real. So, if the emails
    are predicted as fake, our spam prediction is indeed spam for both the decision
    tree and the logistic regression. Our true email is not spam, which perhaps is
    even more important, because we don''t want to filter real email into the spam
    folder. So, you can see that we fitted some pretty good models here that apply
    to something that we would visually inspect as true spam or not:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将前面的值输入到我们的模型中时，我们将看到模型的效果如何。理想情况下，我们会看到实际的假邮件被预测为假，实际的真实邮件被预测为真实。因此，如果邮件被预测为假，我们的垃圾邮件预测确实为垃圾邮件，无论是在决策树还是逻辑回归模型中。我们的真实邮件不是垃圾邮件，这一点可能更为重要，因为我们不想将真实邮件误筛入垃圾邮件文件夹。所以，你可以看到我们在这里拟合了一些非常不错的模型，这些模型可以应用于我们直观判断是否为垃圾邮件的情景：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/8aa61090-9474-4640-bf82-5c88c68fb8c6.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8aa61090-9474-4640-bf82-5c88c68fb8c6.png)'
- en: This is a demo of the actual algorithms that we're going to build from scratch
    in this book, and can be applied to real-world problems.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将在本书中从零开始构建的实际算法的演示，可以应用于真实世界的问题。
- en: Setting up the environment
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置环境
- en: We will go ahead and get our environment set up. Now that we have walked through
    the preceding example, let's go ahead and get our Anaconda environment set up.
    Among other things, Anaconda is a dependency management tool that will allow us
    to control specific versioning of each of the packages that we want to use. We
    will go to the Anaconda website through this link, [https://www.anaconda.com/download/](https://www.anaconda.com/download/),
    and click on the Download tab.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续进行环境设置。现在我们已经完成了前面的示例，让我们开始设置我们的Anaconda环境。Anaconda除了其他功能外，还是一个依赖管理工具，它能帮助我们控制我们想要使用的每个包的特定版本。我们将通过这个链接，[https://www.anaconda.com/download/](https://www.anaconda.com/download/)，访问Anaconda官网，并点击“Download”标签页。
- en: The package that we're building is not going to work with Python 2.7\. So, once
    you have Anaconda, we will perform a live coding example of an actual package
    setup, as well as the environment setup that's included in the `.yml` file that
    we built.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在构建的这个包将无法与Python 2.7兼容。所以，一旦你有了Anaconda，我们将进行一个实际的编码示例，展示如何设置包以及`.yml`文件中的环境设置。
- en: 'Once you have Anaconda set up inside the home directory, we are going to use
    the `environment.yml` file. You can see that the name of the environment we''re
    going to create is `packt-sml` for supervised machine learning. We will need NumPy,
    SciPy, scikit-learn, and pandas. These are all scientific computing and data analysis
    libraries. Matplotlib is what we were using to plot those plots inside the Jupyter
    Notebook, so you''re going to need all those plots. The `conda` package makes
    it really easy to build this environment. All we have to do is type `conda env
    create` and then `-f` to point it to the file, go to `Hands-on-Supervised-Machine-Learning-with-Python-master`,
    and we''re going to use the `environment.yml` as shown in the following command:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你在主目录中设置好了Anaconda，我们将使用`environment.yml`文件。你可以看到我们将要创建的环境名称是`packt-sml`，用于有监督的机器学习。我们将需要NumPy、SciPy、scikit-learn和pandas。这些都是科学计算和数据分析的库。Matplotlib是我们在Jupyter
    Notebook中用于绘制图表的工具，因此你需要这些图表。`conda`包使得构建这个环境变得非常简单。我们只需输入`conda env create`，然后使用`-f`参数指定文件，进入`Hands-on-Supervised-Machine-Learning-with-Python-master`目录，我们将使用如下命令中的`environment.yml`：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As this is the first time you're creating this, it will create a large script
    that will download everything you need. Once you have created your environment,
    you need to activate it. So, on a macOS or a Linux machine, we will type `source
    activate packt-sml`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是你第一次创建这个环境，它会创建一个大型脚本，下载你所需的所有内容。一旦创建了环境，你需要激活它。因此，在macOS或Linux机器上，我们将输入`source
    activate packt-sml`。
- en: 'If you''re on Windows, simply type `activate packt-sml`, which will activate
    that environment:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Windows系统，只需输入`activate packt-sml`，这将激活该环境：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/205cbcc3-10ea-48a9-a83b-76fcf196cffc.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/205cbcc3-10ea-48a9-a83b-76fcf196cffc.png)'
- en: 'In order to build the package, we will type the `cat setup.py` command. We
    can inspect this quickly:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建这个包，我们将输入`cat setup.py`命令。我们可以快速检查一下：
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Take a look at this `setup.py`. Basically, this is just using setup tools to
    install the package. In the following screenshot, we see all the different sub
    models:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 看看这个 `setup.py`。基本上，这是使用 setup 工具安装包的方式。在下面的截图中，我们可以看到所有不同的子模块：
- en: '![](img/c57903bf-a407-461b-93d6-f2798ddf73a0.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c57903bf-a407-461b-93d6-f2798ddf73a0.png)'
- en: 'We will build the package by typing the `python setup.py install` command.
    Now, when we go into Python and try to import `packtml`, we get the following
    output:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过键入 `python setup.py install` 命令来构建这个包。现在，当我们进入 Python 并尝试导入 `packtml` 时，我们将看到以下输出：
- en: '![](img/273e6b37-dfe5-4dbd-ae08-a34b4635bd8b.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/273e6b37-dfe5-4dbd-ae08-a34b4635bd8b.png)'
- en: In this section, we have installed the environment and built the package. In
    the next section, we will start covering some of the theory behind supervised
    machine learning.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们已经安装了环境并构建了包。在下一部分中，我们将开始讨论监督学习背后的一些理论。
- en: Supervised learning
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: In this section, we will formally define what machine learning is and, specifically,
    what supervised machine learning is.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将正式定义什么是机器学习，特别是什么是监督学习。
- en: In the early days of AI, everything was a rules engine. The programmer wrote
    the function and the rules, and the computer simply followed them. Modern-day
    AI is more in line with machine learning, which teaches a computer to write its
    own functions. Some may contest that oversimplification of the concept, but, at
    its core, this is largely what machine learning is all about.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AI 的早期，所有的东西都是规则引擎。程序员编写函数和规则，计算机简单地遵循这些规则。现代 AI 更符合机器学习的理念，它教计算机自己编写函数。有人可能会认为这是一种过于简单化的说法，但从本质上讲，这正是机器学习的核心内容。
- en: 'We''re going to look at a quick example of what machine learning is and what
    it is not. Here, we''re using scikit-learn''s datasets, submodule to create two
    objects and variables, also known as covariance or features, which are along the
    column axis. `y` is a vector with the same number of values as there are rows
    in `X`. In this case, `y` is a class label. For the sake of an example, `y` here
    could be a binary label corresponding to a real-world occurrence, such as the
    malignancy of a tumor. `X` is then a matrix of attributes that describe `y`. One
    feature could be the diameter of the tumor, and another could indicate its density.
    The preceding explanation can be seen in the following code:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个简单的例子来展示什么是机器学习，什么不是机器学习。在这里，我们使用 scikit-learn 的 datasets 子模块创建两个对象和变量，这些对象和变量也叫做协方差或特征，沿着列轴排列。`y`
    是一个向量，它的值的数量与 `X` 中的行数相同。在这个例子中，`y` 是一个类别标签。为了举例说明，`y` 这里可能是一个二分类标签，表示一个实际的情况，比如肿瘤的恶性程度。接着，`X`
    是一个描述 `y` 的属性矩阵。一个特征可能是肿瘤的直径，另一个特征可能表示肿瘤的密度。前面的解释可以通过以下代码来体现：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'A rules engine, by our definition, is simply business logic. It can be as simple
    or as complex as you need it to be, but the programmer makes the rules. In this
    function, we''re going to evaluate our `X` matrix by returning `1`, or `true`,
    where the sums over the rows are greater than `0`. Even though there''s some math
    involved here, there is still a rules engine, because we, the programmers, defined
    a rule. So, we could theoretically get into a gray area, where the rule itself
    was discovered via machine learning. But, for the sake of argument, let''s take
    an example that the head surgeon arbitrarily picks `0` as our threshold, and anything
    above that is deemed as cancerous:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的定义，规则引擎只是业务逻辑。它可以简单也可以复杂，完全取决于你的需求，但规则是由程序员定义的。在这个函数中，我们将通过返回 `1` 或 `true`
    来评估我们的 `X` 矩阵，当行的和大于 `0` 时。尽管这里涉及了一些数学运算，但仍然可以看作是一个规则引擎，因为我们（程序员）定义了规则。因此，我们理论上可以进入一个灰色区域，其中规则本身是通过机器学习发现的。但为了便于讨论，假设主刀医生随意选择
    `0` 作为阈值，任何超过该值的都被认为是癌症：
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output of the preceding code snippet is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码片段的输出如下：
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, as mentioned before, our rules engine can be as simple or as complex as
    we want it to be. Here, we''re not only interested in `row_sums`, but we have
    several criteria to meet in order to deem something cancerous. The minimum value
    in the row must be less than `-1.5`, in addition to one or more of the following
    three criteria:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们的规则引擎可以简单也可以复杂。在这里，我们不仅仅关注 `row_sums`，我们还有多个标准来判断某样东西是否为癌症。行中的最小值必须小于
    `-1.5`，并且还需要满足以下三个标准之一或多个：
- en: The row sum exceeds `0`
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行和超过 `0`
- en: The sum of the rows is evenly divisible by `0.5`
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行和能被 `0.5` 整除
- en: The maximum value of the row is greater than `1.5`
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行的最大值大于`1.5`
- en: 'So, even though our math is a little more complex here, we''re still just building
    a rules engine:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管我们的数学这里有些复杂，但我们仍然只是在构建一个规则引擎：
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下：
- en: '[PRE21]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, let''s say that our surgeon understands and realizes they''re not the
    math or programming whiz that they thought they were. So, they hire programmers
    to build them a machine learning model. The model itself is a function that discovers
    parameters that complement a decision function, which is essentially the function
    the machine itself learned. So, parameters are things we''ll discuss in our next
    [Chapter 2](b51fa9b6-9158-4bcf-9998-18a8f91d3d06.xhtml), *Implementing Parametric
    Models,* which are parametric models. So, what''s happening behind the scenes
    when we invoke the `fit` method is that the model learns the characteristics and
    patterns of the data, and how the `X` matrix describes the `y` vector. Then, when
    we call the `predict` function, it applies its learned decision function to the
    input data to make an educated guess:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们的外科医生意识到他们并不是自己认为的那种数学或编程天才。于是，他们雇佣程序员为他们构建机器学习模型。该模型本身是一个函数，发现补充决策函数的参数，而决策函数本质上就是机器自己学习到的函数。所以，参数是我们将在接下来的[第二章](b51fa9b6-9158-4bcf-9998-18a8f91d3d06.xhtml)《实现参数化模型》中讨论的内容，它们是参数化模型。因此，当我们调用`fit`方法时，背后发生的事情是模型学习数据的特征和模式，以及`X`矩阵如何描述`y`向量。然后，当我们调用`predict`函数时，它会将学习到的决策函数应用于输入数据，从而做出合理的猜测：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下：
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'So, now we''re at a point where we need to define specifically what supervised
    learning is. Supervised learning is precisely the example we just described previously.
    Given our matrix of examples, *X*, in a vector of corresponding labels, *y*, that learns
    a function which approximates the value of *y* or ![](img/ec167647-fd63-4575-af06-3c72ac2af74b.png):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们到了需要明确界定有监督学习到底是什么的时刻。有监督学习正是我们刚才描述的那个例子。给定我们的样本矩阵*X*，以及对应标签的向量*y*，它学习一个函数，近似*y*的值或！[](img/ec167647-fd63-4575-af06-3c72ac2af74b.png)：
- en: '![](img/8bae7a15-da5a-4a4b-ab78-6e5a9587ff51.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8bae7a15-da5a-4a4b-ab78-6e5a9587ff51.png)'
- en: There are other forms of machine learning that are not supervised, known as
    **unsupervised machine learning**. These do not have labels and are more geared
    toward pattern recognition tasks. So, what makes something supervised is the presence
    of labeled data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他形式的机器学习不是有监督的，称为**无监督学习**。这些没有标签，更侧重于模式识别任务。所以，决定是否有监督的标志就是数据是否有标签。
- en: 'Going back to our previous example, when we invoke the `fit` method, we learn
    our new decision function and then, when we call `predict`, we''re approximating
    the new `y` values. So, the output is this ![](img/68d94b46-bc5d-4c32-9c89-4805452ca273.png) we
    just looked at:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们之前的例子，当我们调用`fit`方法时，我们学习了新的决策函数，然后当我们调用`predict`时，我们正在近似新的`y`值。所以，输出就是我们刚才看到的这个！[](img/68d94b46-bc5d-4c32-9c89-4805452ca273.png)
- en: '![](img/4430bb58-e5a2-4710-8fb3-a957fc2e1684.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4430bb58-e5a2-4710-8fb3-a957fc2e1684.png)'
- en: Supervised learning learns a function from labelled samples that approximates
    future `y` values. At this point, you should feel comfortable explaining the abstract
    concept—just the high-level idea of what supervised machine learning is.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督学习从标记样本中学习一个函数，该函数近似未来的`y`值。在这一点上，你应该能够清楚地解释这个抽象概念——也就是有监督机器学习的高层次概念。
- en: Hill climbing and loss functions
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 攀升法和损失函数
- en: In the last section, we got comfortable with the idea of supervised machine
    learning. Now, we will learn how exactly a machine learns underneath the hood.
    This section is going to examine a common optimization technique used by many
    machine learning algorithms, called **hill climbing**. It is predicated on the
    fact that each problem has an ideal state and a way to measure how close or how
    far we are from that. It is important to note that not all machine learning algorithms
    use this approach.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们已经熟悉了有监督机器学习的概念。现在，我们将学习机器如何在幕后进行学习。本节将探讨许多机器学习算法使用的常见优化技术——**爬山法**。它基于这样一个事实：每个问题都有一个理想状态，并且有一种方法来衡量我们离这个理想状态有多近或多远。需要注意的是，并不是所有的机器学习算法都使用这种方法。
- en: Loss functions
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: First, we'll cover loss functions, and then, prior to diving into hill climbing
    and descent, we'll take a quick math refresher.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍损失函数，然后，在深入了解爬山法和下降法之前，我们将进行一个简短的数学复习。
- en: There's going to be some math in this lesson, and while we try to shy away from
    the purely theoretical concepts, this is something that we simply have to get
    through in order to understand the guts of most of these algorithms. There will
    be a brief applied section at the end. Don't panic if you can't remember some
    of the calculus; just simply try to grasp what is happening behind the black box.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这节课会涉及一些数学内容，虽然我们尽量避免纯理论的概念，但这正是我们必须要了解的内容，以便理解大多数这些算法的核心。课程结束时会有一个简短的应用部分。如果你无法记住某些微积分内容，不必恐慌；只要尽量理解黑盒背后发生的事情。
- en: So, as mentioned before, a machine learning algorithm has to measure how close
    it is to some objective. We define this as a cost function, or a loss function.
    Sometimes, we hear it referred to as an objective function. Although not all machine
    learning algorithms are designed to directly minimize a loss function, we're going
    to learn the rule here rather than the exception. The point of a loss function
    is to determine the goodness of a model fit. It is typically evaluated over the
    course of a model's learning procedure and converges when the model has maximized
    its learning capacity.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，机器学习算法必须衡量它与某个目标的接近程度。我们将此定义为成本函数或损失函数。有时，我们也听到它被称为目标函数。虽然并不是所有机器学习算法都旨在直接最小化损失函数，但我们将在这里学习规则而不是例外。损失函数的目的是确定模型拟合的好坏。它通常会在模型的学习过程中进行评估，并在模型最大化其学习能力时收敛。
- en: 'A typical loss function computes a scalar value which is given by the true
    labels and the predicted labels. That is, given our actual *y* and our predicted
    *y*, which is ![](img/6adc46e5-4e64-4050-b55c-5c1d8e0e8e55.png). This notation
    might be cryptic, but all it means is that some function, *L*, which we''re going
    to call our loss function, is going to accept the ground truth, which is *y* and
    the predictions, ![](img/6fa4fd62-405e-4353-b7bf-35e9022c2f5a.png), and return
    some scalar value. The typical formula for the loss function is given as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的损失函数计算一个标量值，该值由真实标签和预测标签给出。也就是说，给定我们的实际*y*和预测的*y*，即![](img/6adc46e5-4e64-4050-b55c-5c1d8e0e8e55.png)。这个符号可能有点难懂，但它的意思是，某个函数*L*，我们将其称为损失函数，将接受真实值*y*和预测值![](img/6fa4fd62-405e-4353-b7bf-35e9022c2f5a.png)，并返回一个标量值。损失函数的典型公式如下：
- en: '![](img/18aa419c-7dd3-45df-8bcf-d7724f238d73.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18aa419c-7dd3-45df-8bcf-d7724f238d73.png)'
- en: 'So, I''ve listed several common loss functions here, which may or may not look
    familiar. **Sum of Squared Error** (**SSE**) is a metric that we''re going to
    be using for our regression models:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我列出了几个常见的损失函数，它们可能看起来熟悉，也可能不熟悉。**平方误差和**（**SSE**）是我们将在回归模型中使用的度量：
- en: '![](img/fc07628b-c403-4b88-8337-300ee6bcf651.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc07628b-c403-4b88-8337-300ee6bcf651.png)'
- en: 'Cross entropy is a very commonly used classification metric:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵是一个非常常用的分类度量：
- en: '![](img/29b4930c-d8a3-4fd2-b805-b7a4466b5e58.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29b4930c-d8a3-4fd2-b805-b7a4466b5e58.png)'
- en: In the following diagram, the *L* function on the left is simply indicating
    that it is our loss function over *y* and ![](img/ba99764f-61f6-4266-806b-7acbb95ed809.png) given
    parameter theta. So, for any algorithm, we want to find the set of the theta parameters
    that minimize the loss. That is, if we're predicting the cost of a house, for
    example, we may want to estimate the cost per square foot as accurately as possible
    so as to minimize how wrong we are.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图示中，左侧的*L*函数仅仅表示它是关于*y*的损失函数，并且是给定参数theta的![](img/ba99764f-61f6-4266-806b-7acbb95ed809.png)。因此，对于任何算法，我们都希望找到一组theta参数，使得损失最小化。也就是说，如果我们在预测房价时，我们可能希望尽可能准确地估算每平方英尺的价格，以最小化我们的预测误差。
- en: 'Parameters are often in a much higher dimensional space than can be represented
    visually. So, the big question we''re concerned with is the following: How can
    we minimize the cost? It is typically not feasible for us to attempt every possible
    value to determine the true minimum of a problem. So, we have to find a way to
    descend this nebulous hill of loss. The tough part is that, at any given point,
    we don''t know whether the curve goes up or down without some kind of evaluation.
    And that''s precisely what we want to avoid, because it''s very expensive:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 参数通常存在于一个比视觉上能够表示的更高维的空间中。因此，我们关注的一个大问题是：如何最小化成本？通常情况下，我们不可能尝试所有可能的值来确定问题的真实最小值。所以，我们必须找到一种方法来下降这个模糊的损失山丘。困难的部分在于，在任何给定的点上，我们都不知道曲线是上升还是下降，除非进行某种评估。这正是我们要避免的，因为这样做非常昂贵：
- en: '![](img/8dfdcaa5-bf54-47ff-a8af-dbfc885a2b29.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8dfdcaa5-bf54-47ff-a8af-dbfc885a2b29.png)'
- en: We can describe this problem as waking up in a pitch-black room with an uneven
    floor and trying to find the lowest point in the room. You don't know how big
    the room is. You don't know how deep or how high it gets. Where do you step first?
    One thing we can do is to examine exactly where we stand and determine which direction
    around us slopes downward. To do that, we have to measure the slope of the curve.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个问题描述为在一个漆黑的房间里醒来，房间的地板不平，试图找到房间的最低点。你不知道房间有多大，也不知道它有多深或多高。你首先该在哪里踩？我们可以做的一件事是检查我们站立的位置，确定周围哪个方向是向下倾斜的。为此，我们需要测量曲线的坡度。
- en: Measuring the slope of a curve
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量曲线的坡度
- en: 'The following is a quick refresher on scalar derivatives. To compute the slope
    at any given point, the standard way is to typically measure the slope of the
    line between the point we''re interested in and some secant point, which we''ll
    call delta *x*:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是标量导数的快速复习。为了计算任何给定点的坡度，标准的方式通常是测量我们感兴趣的点与某个割线点之间的坡度，我们称这个点为delta *x*：
- en: '![](img/2ec22951-f7ed-4b45-bc08-48fba2dd67ce.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ec22951-f7ed-4b45-bc08-48fba2dd67ce.png)'
- en: 'As the distance between *x* and its neighbor delta *x* approaches *0*, or as
    our limit approaches *0*, we arrive at the slope of the curve. This is given by
    the following formula:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当*x*与其邻近值delta *x*之间的距离趋近于*0*，或者当我们的极限趋近于*0*时，我们就能得到曲线的坡度。这个坡度由以下公式给出：
- en: '![](img/30cef7ab-b444-4823-aacf-64bdda6d6016.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30cef7ab-b444-4823-aacf-64bdda6d6016.png)'
- en: 'There are several different notations that you may be familiar with. One is
    *f* prime of *x*. The slope of a constant is *0*. So, if *f(x)* is *9*, in other
    words, if *y* is simply *9*, it never changes. There is no slope. So, the slope
    is *0*, as shown:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种你可能熟悉的符号。其中一个是*f*对*x*的导数。常数的坡度是*0*。因此，如果*f(x)*是*9*，换句话说，如果*y*仅仅是*9*，它永远不变。没有坡度。所以，坡度是*0*，如图所示：
- en: '![](img/7dcce59e-5617-411b-921e-e5497a64a9b4.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7dcce59e-5617-411b-921e-e5497a64a9b4.png)'
- en: 'We can also see the power law in effect here in the second example. This will
    come in useful later on. If we multiply the variable by the power, and decrement
    the power by one, we get the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在第二个例子中看到幂律的作用。这在之后会非常有用。如果我们将变量乘以幂，并将幂减去一，我们得到以下结果：
- en: '![](img/22f03da9-d95c-4c95-9857-020d6fe1b9f3.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22f03da9-d95c-4c95-9857-020d6fe1b9f3.png)'
- en: Measuring the slope of an Nd-curve
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量Nd曲线的坡度
- en: In order to measure the slope of a vector or a multi-dimensional surface, we
    will introduce the idea of partial derivatives, which are simply derivatives with
    respect to a variable, with all the other variables held as constants. So, our
    solution is a vector of dimension *k*, where *k* is the number of variables that
    our function takes. In this case, we have *x* and *y*. Each respective position
    in the vector that we solve is a derivative with respect to the corresponding
    function's positional variable.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测量一个向量或多维曲面的坡度，我们将引入偏导数的概念，偏导数就是对某一变量求导数，而将所有其他变量视为常数。因此，我们的解是一个维度为*k*的向量，其中*k*是我们函数所涉及的变量的数量。在这种情况下，我们有*x*和*y*。我们求解的向量中每个相应的位置都是对相应函数的定位变量求导的结果。
- en: 'From a conceptual level, what we''re doing is we''re holding one of the variables
    still and changing the other variables around it to see how the slope changes.
    Our denominator''s notation indicates which variable we''re measuring the slope
    with, with respect to that point. So, in this case, the first position, *d(x)*,
    is showing that we''re taking the partial derivative of function *f* with respect
    to *x*, where we hold *y* constant. And then, likewise, in the second one, we''re
    taking the derivative of function f with respect to *y*, holding *x* constant.
    So, what we get in the end is called a gradient, which is a super keyword. It
    is simply just a vector of partial derivatives:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念层面来看，我们所做的是保持一个变量不变，同时改变周围的其他变量，看看坡度如何变化。我们分母的符号表示我们正在测量哪个变量的坡度，并且是相对于该点进行的。因此，在这种情况下，第一个位置，*d(x)*，表示我们正在对函数*f*进行关于*x*的偏导数，保持*y*不变。同样，第二个位置，我们对函数*f*进行关于*y*的导数，保持*x*不变。因此，最终得到的是一个梯度，它是一个非常重要的关键词。它只是一个偏导数的向量：
- en: '![](img/62ac4d23-5f7d-460a-858a-bdd93f6f3646.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62ac4d23-5f7d-460a-858a-bdd93f6f3646.png)'
- en: '![](img/e86981ff-c747-49ba-a53d-8ee22588f525.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e86981ff-c747-49ba-a53d-8ee22588f525.png)'
- en: Measuring the slope of multiple functions
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量多个函数的坡度
- en: 'We want to get really complicated, though, and measure the slopes of multiple
    functions at the same time. All we''ll end up with is a matrix of gradients along
    the rows. In the following formula, we can see the solution that we just solved
    from the previous example:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们想要变得更复杂，计算多个函数在同一时间的斜率。我们最终得到的只是一个沿着行的梯度矩阵。在下面的公式中，我们可以看到之前示例中我们刚刚解出的结果：
- en: '![](img/38704fda-a85a-4fec-9e1c-4fbbcc29ff32.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/38704fda-a85a-4fec-9e1c-4fbbcc29ff32.png)'
- en: 'In the next formula, we have introduced this new function, called *g*. We see
    the gradient for function *g*, with each position corresponding to the partial
    derivative with respect to the variables *x* and *y*:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个公式中，我们引入了这个新的函数，称为*g*。我们看到了函数*g*的梯度，每个位置对应于相对于变量*x*和*y*的偏导数：
- en: '![](img/0a984ad0-25f6-4b2f-8613-03876048985a.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a984ad0-25f6-4b2f-8613-03876048985a.png)'
- en: 'When we stack these together into a matrix, what we get is a Jacobian. You
    don''t need to solve this, but you should understand that what we''re doing is
    taking the slope of a multi-dimensional surface. You can treat it as a bit of
    a black box as long as you understand that. This is exactly how we''re computing
    the gradient and the Jacobian:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这些组合成一个矩阵时，得到的就是雅可比矩阵。你不需要解这个矩阵，但你应该理解我们所做的是计算一个多维表面的斜率。只要理解这一点，你可以把它当作一个黑盒。我们就是这样计算梯度和雅可比矩阵的：
- en: '![](img/f5f178d0-8f97-4450-8043-e9fb391a1bfe.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5f178d0-8f97-4450-8043-e9fb391a1bfe.png)'
- en: Hill climbing and descent
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 爬山与下坡
- en: We will go back to our example—the lost hill that we looked at. We want to find
    a way to select a set of theta parameters that is going to minimize our loss function, *L*.
    As we've already established, we need to climb or descend the hill, and understand
    where we are with respect to our neighboring points without having to compute
    everything. To do that, we need to be able to measure the slope of the curve with
    respect to the theta parameters. So, going back to our house example, as mentioned
    before, we want to know how much correct the incremental value of cost per square
    foot makes. Once we know that, we can start taking directional steps toward finding
    the best estimate. So, if you make a bad guess, you can turn around and go in
    exactly the other direction. So, we can either climb or descend the hill depending
    on our metric, which allows us to optimize the parameters of a function that we
    want to learn irrespective of how the function itself performs. This is a layer
    of abstraction. This optimization process is called gradient descent, and it supports many
    of the machine learning algorithms that we will discuss in this book.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将回到之前的例子——我们观察过的失落的山丘。我们想找到一组能够最小化我们的损失函数*L*的theta参数。正如我们已经确定的那样，我们需要爬山或者下山，并且了解自己在相对于邻近点的位置，而不必计算所有的内容。为了做到这一点，我们需要能够测量曲线相对于theta参数的斜率。所以，回到之前的房屋例子，我们希望知道每平方英尺成本增量的正确值是多少。一旦知道了这个，我们就可以开始朝着找到最佳估计的方向迈出步伐。如果你做出了一个错误的猜测，你可以转身并朝着完全相反的方向前进。所以，我们可以根据我们的度量标准爬山或下山，这使得我们能够优化我们想要学习的函数的参数，而不管该函数本身的表现如何。这是一层抽象。这个优化过程称为梯度下降，它支持我们在本书中将讨论的许多机器学习算法。
- en: 'The following code shows a simple example of how we can measure the gradient
    of a matrix with respect to theta. This example is actually a simplified snippet
    of the learning component of logistic regression:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了一个如何计算矩阵相对于theta的梯度的简单示例。这个示例实际上是逻辑回归学习部分的简化代码：
- en: '[PRE24]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: At the very top, we randomly initialize `X` and `y`, which is not part of the
    algorithm. So, `x` here is the sigmoid function, also called the **logistic function**.
    The word logistic comes from logistic progression. This is a necessary transformation
    that is applied in logistic regression. Just understand that we have to apply
    that; it's part of the function. So, we initialize our `theta` vector, with respect
    to which we're going to compute our gradient as zeros. Again, all of them are
    zeros. Those are our parameters. Now, for each iteration, we're going to get our
    ![](img/c700ee8d-f92f-44b3-9ac0-54c1cb781e40.png), which is our estimated `y`,
    if you recall. We get that by taking the dot product of our `X` matrix against
    our theta parameters, pushed through that logistic function, `h`, which is our
    ![](img/9fa6e3b4-1d08-487f-b97c-17fbef8dfa08.png).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在最开始，我们随机初始化了`X`和`y`，这并不是算法的一部分。所以，这里的`x`是sigmoid函数，也叫做**逻辑函数**。逻辑这个词来源于逻辑进展。这是逻辑回归中应用的必要变换。只需要理解我们必须应用它；这是函数的一部分。所以，我们初始化我们的`theta`向量，用来计算梯度，初始值是零。再强调一次，所有的值都是零。它们是我们的参数。现在，对于每次迭代，我们会得到我们的
    ![](img/c700ee8d-f92f-44b3-9ac0-54c1cb781e40.png)，也就是我们估计的`y`，如果你还记得的话。我们通过将`X`矩阵与我们的theta参数做点积，并通过那个逻辑函数`h`，也就是我们的
    ![](img/9fa6e3b4-1d08-487f-b97c-17fbef8dfa08.png)，来得到这个值。
- en: Now, we want to compute the gradient of that dot product between the residuals
    and the input matrix, `X`, of our predictors. The way we compute our residuals
    is simply `y` minus ![](img/038cd708-4e73-4af7-a532-e5f7a70612f5.png), which gives
    the residuals. Now, we have our ![](img/e627b377-2a40-4d23-9e9d-409a0db456e6.png).
    How do we get the gradient? The gradient is just the dot product between the input
    matrix, `X`, and those residuals. We will use that gradient to determine which
    direction we need to step in. The way we do that is we add the gradient to our
    theta vector. Lambda regulates how quickly we step up or down that gradient. So,
    it's our learning rate. If you think of it as a step size—going back to that dark
    room example—if it's too large, it's easy to overstep the lowest point. But if
    it's too small, you're going to spend forever inching around the room. So, it's
    a bit of a balancing act, but it allows us to regulate the pace at which we update
    our theta values and descend our gradient. Again, this algorithm is something
    we will cover in the next chapter.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想计算残差和预测器输入矩阵`X`之间的点积的梯度。我们计算残差的方式是将`y`减去 ![](img/038cd708-4e73-4af7-a532-e5f7a70612f5.png)，这就得到了残差。现在，我们得到了我们的
    ![](img/e627b377-2a40-4d23-9e9d-409a0db456e6.png)。我们怎么得到梯度呢？梯度就是输入矩阵`X`和这些残差之间的点积。我们将利用这个梯度来确定我们需要朝哪个方向迈步。我们做的方式是将梯度加到我们的theta向量中。Lambda调节我们在梯度上向上或向下迈步的速度。所以，它是我们的学习率。如果你把它当作步长——回到那个黑暗房间的例子——如果步长太大，很容易越过最低点。但如果步长太小，你会在房间里不停地绕圈子。所以，这是一种平衡，但它使我们能够调节更新theta值和下降梯度的速度。再次强调，这个算法将在下一章中讲解。
- en: 'We get the output of the preceding code as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到前面代码的输出如下：
- en: '[PRE25]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This example demonstrates how our gradient or slope actually changes as we adjust
    our coefficients and vice versa.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了当我们调整系数时，梯度或斜率是如何变化的，反之亦然。
- en: In the next section, we will see how to evaluate our models and learn the cryptic
    `train_test_split`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何评估我们的模型，并了解神秘的`train_test_split`。
- en: Model evaluation and data splitting
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估与数据划分
- en: In this chapter, we will define what it means to evaluate a model, best practices
    for gauging the advocacy of a model, how to split your data, and several considerations
    that you'll have to make when preparing your split.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将定义评估模型的意义、评估模型效果的最佳实践、如何划分数据，以及在准备数据划分时你需要考虑的若干事项。
- en: It is important to understand some core best practices of machine learning.
    One of our primary tasks as ML practitioners is to create a model that is effective
    for making predictions on new data. But how do we know that a model is good? If
    you recall from the previous section, we defined supervised learning as simply
    a task that learns a function from labelled data such that we can approximate
    the target of the new data. Therefore, we can test our model's effectiveness.
    We can determine how it performs on data that is never seen—just like it's taking
    a test.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 理解一些机器学习的核心最佳实践非常重要。作为机器学习从业者，我们的主要任务之一是创建一个能够有效预测新数据的模型。那么我们如何知道一个模型是否优秀呢？如果你回想一下上一部分，我们将监督学习定义为从标注数据中学习一个函数，以便能够近似新数据的目标。因此，我们可以测试模型的有效性。我们可以评估它在从未见过的数据上的表现——就像它在参加考试一样。
- en: Out-of-sample versus in-sample evaluation
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 样本外评估与样本内评估
- en: 'Let''s say we are training a small machine which is a simple classification
    task. Here''s some nomenclature you''ll need: the in-sample data is the data the
    model learns from and the out-of-sample data is the data the model has never seen
    before. One of the pitfalls many new data scientists make is that they measure
    their model''s effectiveness on the same data that the model learned from. What
    this ends up doing is rewarding the model''s ability to memorize, rather than
    its ability to generalize, which is a huge difference.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在训练一台小型机器，这是一个简单的分类任务。这里有一些你需要了解的术语：样本内数据是模型从中学习的数据，而样本外数据是模型以前从未见过的数据。许多新手数据科学家的一个常见错误是，他们在模型学习的同一数据上评估模型的效果。这样做的结果是奖励模型的记忆能力，而不是其概括能力，这两者有着巨大的区别。
- en: 'If you take a look at the two examples here, the first presents a sample that
    the model learned from, and we can be reasonably confident that it''s going to
    predict one, which would be correct. The second example presents a new sample,
    which appears to resemble more of the zero class. Of course, the model doesn''t
    know that. But a good model should be able to recognize and generalize this pattern,
    shown as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看这里的两个例子，第一个展示了模型从中学习的样本，我们可以合理地相信它将预测出一个正确的结果。第二个例子展示了一个新的样本，看起来更像是零类。当然，模型并不知道这一点。但一个好的模型应该能够识别并概括这种模式，如下所示：
- en: '![](img/2fbc7355-5703-4c20-86a5-84d65ef157cb.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fbc7355-5703-4c20-86a5-84d65ef157cb.png)'
- en: 'So, now the question is how we can ensure both in-sample and out-of-sample
    data for the model to prove its worth. Even more precisely, our out-of-sample
    data needs to be labeled. New or unlabeled data won''t suffice because we have
    to know the actual answer in order to determine how correct the model is. So,
    one of the ways we can handle this in machine learning is to split our data into
    two parts: a training set and a testing set. The training set is what our model
    will learn on; the testing set is what our model will be evaluated on. How much
    data you have matters a lot. In fact, in the next sections, when we discuss the
    bias-variance trade-off, you''ll see how some models require much more data to
    learn than others do.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在的问题是如何确保模型的**样本内数据**和**样本外数据**能够证明其有效性。更精确地说，我们的样本外数据需要被标注。新的或未标注的数据是不够的，因为我们必须知道实际答案，以便确定模型的准确性。因此，机器学习中应对这种情况的一种方式是将数据分为两部分：训练集和测试集。训练集是模型将从中学习的数据；测试集是模型将接受评估的数据。你拥有的数据量非常重要。实际上，在接下来的部分中，当我们讨论偏差-方差权衡时，你会看到一些模型需要比其他模型更多的数据来进行学习。
- en: Another thing to keep in mind is that if some of the distributions of your variables
    are highly skewed, or you have rare categorical levels embedded throughout, or
    even class imbalance in your `y` vector, you may end up getting a bad split. As
    an example, let's say you have a binary feature in your `X` matrix that indicates
    the presence of a very rare sensor for some event that occurs every 10,000 occurrences.
    If you randomly split your data and all of the positive sensor events are in your
    test set, then your model will learn from the training data that the sensor is
    never tripped and may deem that as an unimportant variable when, in reality, it
    could be hugely important, and hugely predictive. So, you can control these types
    of issues with stratification.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点需要记住的是，如果某些变量的分布高度偏斜，或者你的数据中嵌入了稀有的类别水平，甚至你的`y`向量中存在类别不平衡，你可能会得到一个不理想的划分。举个例子，假设在你的`X`矩阵中有一个二元特征，表示某个事件的传感器是否存在，而该事件每10,000次发生才会触发一次。如果你随机划分数据，所有正向的传感器事件都在测试集里，那么你的模型会从训练数据中学到传感器从未被触发，可能会认为它是一个不重要的变量，而实际上，它可能是一个非常重要、且具有很强预测能力的变量。所以，你可以通过分层抽样来控制这些类型的问题。
- en: Splitting made easy
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 划分变得简单
- en: 'Here, we have a simple snippet that demonstrates how we can use the scikit-learn
    library to split our data into training and test sets. We''re loading the data
    in from the datasets module and passing both `X` and `y` into the split function.
    We should be familiar with loading the data up. We have the `train_test_split`
    function from the `model_selection` submodule in `sklearn`. This is going to take
    any number of arrays. So, 20% is going to be `test_size`, and the remaining 80%
    of that data will be training. We define `random_state`, so that our split can
    be reproducible if we ever have to prove exactly how we got this split. There''s
    also the `stratify` keyword, which we''re not using here, which can be used to
    `stratify` a split for rare features or an imbalanced `y` vector:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一段简单的代码示例，演示了如何使用scikit-learn库将数据划分为训练集和测试集。我们从datasets模块加载数据，并将`X`和`y`传递给划分函数。我们应该熟悉数据的加载。我们使用了`sklearn`中的`model_selection`子模块的`train_test_split`函数。这个函数可以接受任意数量的数组。所以，20%将作为`test_size`，剩余的80%作为训练集。我们定义了`random_state`，以便如果需要证明我们是如何得到这个划分的，它可以被重现。这里还有一个`stratify`关键词，虽然我们没有使用它，它可以用于针对稀有特征或不平衡的`y`向量进行分层抽样。
- en: '[PRE26]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '[PRE27]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Summary
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced supervised learning, got our environment put
    together, and learned about hill climbing and model evaluation. At this point,
    you should understand the abstract conceptual underpinnings of what makes a machine
    learn. It's all about optimizing a number of loss functions. In the next chapter,
    we'll jump into parametric models and even code some popular algorithms from scratch.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了监督学习，搭建了我们的环境，并学习了爬山法和模型评估。到目前为止，你应该理解了机器学习背后的抽象概念基础。这一切都与优化多个损失函数有关。在下一章，我们将深入探讨参数化模型，甚至从零开始编写一些流行算法的代码。
