- en: Chapter 7. Tree-Based Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 树方法
- en: In this chapter, we are going to present one of the most intuitive ways to create
    a predictive model—using the concept of a tree. Tree-based models, often also
    known as decision tree models, are successfully used to handle both regression
    and classification type problems. We'll explore both scenarios in this chapter,
    and we'll be looking at a range of different algorithms that are effective in
    training these models. We will also learn about a number of useful properties
    that these models possess, such as their ability to handle missing data and the
    fact that they are highly interpretable.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍创建预测模型最直观的方法之一——使用树的概念。基于树的模型，通常也称为决策树模型，成功地用于处理回归和分类类型的问题。我们将在本章中探讨这两种场景，并查看一系列在训练这些模型方面有效的不同算法。我们还将了解这些模型所具有的一些有用特性，例如它们处理缺失数据的能力以及它们的高度可解释性。
- en: The intuition for tree models
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 树模型的直观理解
- en: A **decision tree** is a model with a very straightforward structure that allows
    us to make a prediction on an output variable, based on a series of rules arranged
    in a tree-like structure. The output variable that we can model can be categorical,
    allowing us to use a decision tree to handle classification problems. Equally,
    we can use decision trees to predict a numerical output, and in this way we'll
    also be able to tackle problems where the predictive task is a regression task.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**是一种结构非常直观的模型，它允许我们根据一系列以树状结构排列的规则，对输出变量进行预测。我们可以建模的输出变量可以是分类的，这样我们就可以使用决策树来处理分类问题。同样，我们也可以使用决策树来预测数值输出，这样我们也将能够解决预测任务为回归任务的问题。'
- en: Decision trees consist of a series of split points, often referred to as **nodes**.
    In order to make a prediction using a decision tree, we start at the top of the
    tree at a single node known as the **root node**. The root node is a decision
    or split point, because it imposes a condition in terms of the value of one of
    the input features, and based on this decision we know whether to continue on
    with the left part of the tree or with the right part of the tree. We repeat this
    process of choosing to go left or right at each **inner node** that we encounter
    until we reach one of the **leaf nodes**. These are the nodes at the base of the
    tree, which give us a specific value of the output to use as our prediction.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树由一系列称为**节点**的分割点组成。为了使用决策树进行预测，我们从树顶的单一节点开始，这个节点被称为**根节点**。根节点是一个决策或分割点，因为它根据输入特征中的一个特征值提出条件，基于这个决策我们知道是继续树的左部分还是右部分。我们在遇到的每个**内部节点**上重复选择向左或向右的过程，直到我们达到一个**叶节点**。这些是树的底部节点，它们给出了输出变量的特定值，作为我们的预测使用。
- en: To illustrate this, let's look at a very simple decision tree in terms of two
    features, *x1* and *x2*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，让我们看看一个由两个特征*x1*和*x2*组成的非常简单的决策树。
- en: '![The intuition for tree models](img/00125.jpeg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![树模型的直观理解](img/00125.jpeg)'
- en: Note that the tree is a recursive structure, in that the left and right parts
    of the tree that lie beneath a particular node are trees themselves. They are
    referred to as the **left subtree** and the **right subtree** respectively and
    the nodes that they lead to are the **left child** and **right child**. To understand
    how we go about using a decision tree in practice, we can try a simple example.
    Suppose we want to use our tree to predict the output for an observation where
    the value of *x1* is 96.0 and the value of *x2* is 79.9\. We start at the root
    and make a decision as to which subtree to follow. Our value of *x2* is larger
    than 23, so we follow the right branch and come to a new node with a new condition
    to check. Our value of *x1* is larger than 46, so we once again take the right
    branch and arrive at a leaf node. Thus, we output the value indicated by the leaf
    node, which is -3.7\. This is the value that our model predicts given the pair
    of inputs that we specified.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，树是一个递归结构，因为位于特定节点下的树的左右部分本身也是树。它们分别被称为**左子树**和**右子树**，它们所指向的节点分别是**左孩子**和**右孩子**。为了理解我们在实践中如何使用决策树，我们可以尝试一个简单的例子。假设我们想用我们的树来预测一个观察值的输出，其中*x1*的值为96.0，*x2*的值为79.9。我们从根节点开始，决定跟随哪个子树。我们的*x2*值大于23，所以我们跟随右分支，来到一个新的节点，需要检查新的条件。我们的*x1*值大于46，所以我们再次选择右分支，到达一个叶节点。因此，我们输出叶节点指示的值，即-3.7。这是我们的模型根据我们指定的输入对预测的值。
- en: 'One way of thinking about decision trees is that they are in fact encoding
    a series of if-then rules leading to distinct outputs. For every leaf node, we
    can write a single rule (using the boolean `AND` operator if necessary to join
    together multiple conditions) that must hold true for the tree to output that
    node''s value. We can extract all of these if-then rules by starting at the root
    node and following every path down the tree that leads to a leaf node. For example,
    our small regression tree leads to the following three rules, one for each of
    its leaf nodes:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的一种思考方式是，它们实际上是在编码一系列导致不同输出的if-then规则。对于每个叶节点，我们可以写一条规则（如果需要，可以使用布尔`AND`运算符将多个条件连接起来），这条规则必须为真，树才能输出该节点的值。我们可以通过从根节点开始，沿着每条通向叶节点的路径向下遍历树，来提取所有这些if-then规则。例如，我们的小回归树导致以下三条规则，每一条对应其一个叶节点：
- en: '`If (x2 < 23) Then Output 2.1`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`如果(x2 < 23) 则输出 2.1`'
- en: '`If (x2 > 23) AND (x1 < 46) Then Output 1.2`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`如果(x2 > 23) AND (x1 < 46) 则输出 1.2`'
- en: '`If (x2 > 23) AND (x1 > 46) Then Output -3.7`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`如果(x2 > 23) AND (x1 > 46) 则输出 -3.7`'
- en: Note that we had to join together two conditions for each one of the last two
    rules using the `AND` operator, as the corresponding paths leading down to a leaf
    node included more than one decision node (counting the root node).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们必须使用`AND`运算符将最后两条规则中的两个条件连接起来，因为通向叶节点的路径包含多个决策节点（包括根节点）。
- en: 'Another way to think about decision trees is that they partition the feature
    space into a series of rectangular regions in two dimensions, cubes in three dimensions,
    and hypercubes in higher dimensions. Remember that the number of dimensions in
    the feature space is just the number of features. The feature space for our example
    regression tree has two dimensions and we can visualize how this space is split
    up into rectangular regions as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考决策树的方式是，它们将特征空间划分为二维中的一系列矩形区域，三维中的立方体，以及更高维度的超立方体。记住，特征空间中的维度数就是特征的数量。我们示例回归树的特征空间有两个维度，我们可以如下可视化这个空间是如何划分为矩形区域的：
- en: '![The intuition for tree models](img/00126.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![树模型的直观理解](img/00126.jpeg)'
- en: 'The rule-based interpretation and the space partitioning interpretation are
    equivalent views of the same model. The space partitioning interpretation in particular
    is very useful in helping us appreciate one particular characteristic of decision
    trees: they must completely cover all possible combinations of input features.
    Put differently, there should be no particular input for which there is no path
    to a leaf node in the decision tree. Every time we are given a value for our input
    features, we should always be able to return an answer. Our feature space partitioning
    interpretation of a decision tree essentially tells us that there is no point
    or space of points that doesn''t belong to a particular partition with an assigned
    value. Similarly, with our if-then ruleset view of a decision tree, we are saying
    that there is always one rule that can be used for any input feature combination,
    and therefore we can reorganize our rules into an equivalent `if-then-else` structure
    where the last rule is an `else` statement.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 规则解释和空间划分解释是同一模型的等效视角。特别是空间划分解释在帮助我们理解决策树的一个特定特性方面非常有用：它们必须完全覆盖所有可能的输入特征组合。换句话说，对于决策树中不存在到达叶节点的路径的特定输入，应该没有。每次我们给出输入特征的值时，我们都应该始终能够返回一个答案。我们的决策树特征空间划分解释本质上告诉我们，没有不属于特定分区并分配了值的点或点的空间。同样，从我们的决策树if-then规则集视角来看，我们是在说对于任何输入特征组合，总有一条规则可以使用，因此我们可以将规则重新组织成一个等效的`if-then-else`结构，其中最后一个规则是`else`语句。
- en: Algorithms for training decision trees
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树训练算法
- en: 'Now that we have understood how a decision tree works, we''ll want to address
    the issue of how we can train one using some data. There are several algorithms
    that have been proposed to build decision trees, and in this section we will present
    a few of the most well-known. One thing we should bear in mind is that, whatever
    tree-building algorithm we choose, we will have to answer four fundamental questions:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了决策树的工作原理，我们接下来想要解决的是如何使用一些数据来训练一个决策树的问题。已经提出了几种算法来构建决策树，在本节中，我们将介绍其中一些最著名的算法。我们应该记住的一点是，无论我们选择哪种树构建算法，我们都必须回答四个基本问题：
- en: For every node (including the root node), how should we choose the input feature
    to split on and, given this feature, what is the value of the split point?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个节点（包括根节点），我们应该如何选择用于分割的输入特征，以及给定这个特征，分割点的值是多少？
- en: How do we decide whether a node should become a leaf node or if we should make
    another split point?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何决定一个节点应该成为叶节点，还是我们应该创建另一个分割点？
- en: How deep should our tree be allowed to become?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该允许树有多深？
- en: Once we arrive at a leaf node, what value should we predict?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦我们到达叶节点，我们应该预测什么值？
- en: Note
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A great introduction to decision trees is *Chapter 3* of *Machine Learning*,
    *Tom Mitchell*. This book was probably the first comprehensive introduction to
    machine learning and is well worth reading. Although published in 1997, much of
    the material in the book remains relevant today. Furthermore, according to the
    book's website at [http://www.cs.cmu.edu/~tom/mlbook.html](http://www.cs.cmu.edu/~tom/mlbook.html),
    there is a planned second edition in the works.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的优秀介绍可以在《机器学习》的*第3章*中找到，作者是*Tom Mitchell*。这本书可能是对机器学习最全面的介绍之一，值得一读。尽管这本书是在1997年出版的，但书中的大部分内容至今仍然适用。此外，根据书中网站[http://www.cs.cmu.edu/~tom/mlbook.html](http://www.cs.cmu.edu/~tom/mlbook.html)的信息，正在计划出版第二版。
- en: Classification and regression trees
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类和回归树
- en: The **Classification and regression tree** (**CART**) methodology, which we
    will henceforth refer to simply as CART, is one of the earliest proposed approaches
    to building tree-based models. As the name implies, the methodology encompasses
    both an approach to building regression trees and an approach to building classification
    trees.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类和回归树**（**CART**）方法，我们以后将简单地称之为CART，是早期提出的构建基于树模型的最早方法之一。正如其名所示，该方法包括构建回归树和分类树的方法。'
- en: CART regression trees
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CART回归树
- en: For regression trees, the key intuition with the CART approach is that, at any
    given point in the tree, we choose both the input feature to split on and the
    value of the split point within that feature, by finding which combination of
    these maximizes the reduction in the **sum of squared error** (**SSE**). For every
    leaf node in a regression tree built using CART, the predicted value is simply
    the average value of the output predicted by all the data points that are assigned
    to that particular leaf node. To determine whether a new split point should be
    made or whether the tree should grow a leaf node, we simply count the number of
    data points that are currently assigned to a node; if this value is less than
    a predetermined threshold, we create a new leaf node.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归树，使用 CART 方法的关键直觉是，在树的任何给定点，我们通过找到最大化这些组合中平方误差和（**SSE**）减少的组合，来选择分割的输入特征和该特征内的分割点值。对于使用
    CART 构建的回归树中的每个叶节点，预测值只是分配给该特定叶节点的所有数据点预测输出的平均值。为了确定是否应该创建新的分割点或者树是否应该生长一个叶节点，我们只需计算当前分配给节点的数据点的数量；如果这个值小于一个预定的阈值，我们创建一个新的叶节点。
- en: 'For any given node in the tree, including the root node, we begin by having
    some data points assigned to that node. At the root node, all the data points
    are assigned, but once we make a split, some of the data points are assigned to
    the left child and the remaining points are assigned to the right child. The starting
    value of the SSE is just the sum of squared error computed using the average value
    ![CART regression trees](img/00127.jpeg) of the output variable ![CART regression
    trees](img/00128.jpeg) for the *n* data points assigned to the current node:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于树中的任何给定节点，包括根节点，我们首先将一些数据点分配给该节点。在根节点，所有数据点都被分配，但一旦我们进行分割，一些数据点被分配给左子节点，剩余的点被分配给右子节点。SSE
    的起始值只是使用分配给当前节点的 *n* 个数据点的输出变量的平均值 ![CART 回归树](img/00127.jpeg) 计算的平方误差和 ![CART
    回归树](img/00128.jpeg)：
- en: '![CART regression trees](img/00129.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![CART 回归树](img/00129.jpeg)'
- en: 'If we split these data points into two groups of size *n[1]* and *n[2]* so
    that *n[1] + n[2] = n*, and we compute the new SSE for all the data points as
    the sum of the SSE values for each of the two new groups, we have:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这些数据点分成两个大小为 *n[1]* 和 *n[2]* 的组，使得 *n[1] + n[2] = n*，并且计算所有数据点的新的 SSE 作为两个新组
    SSE 值的总和，我们有：
- en: '![CART regression trees](img/00130.jpeg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![CART 回归树](img/00130.jpeg)'
- en: Here, the first sum iterates over *j*, which is the new indices of the data
    points in the first group corresponding to the left child, and the second sum
    iterates over *k*, which is the new indices of the data points inside the second
    group belonging to the right child. The idea behind CART is that we find a way
    to form these two groups of data points by considering every possible feature
    and every possible split point within that feature so that this new quantity is
    minimized. Thus, we can think of our error function in CART as the SSE.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，第一个求和是对 *j* 进行迭代，*j* 是第一组中对应左子节点的数据点的新的索引，第二个求和是对 *k* 进行迭代，*k* 是第二组内部属于右子节点的数据点的新的索引。CART
    的基本思想是通过考虑每个可能的特征以及该特征内的每个可能的分割点，找到一种方法来形成这两组数据点，从而使这个新的量最小化。因此，我们可以将 CART 中的误差函数视为
    SSE。
- en: One of the natural advantages of CART, and tree-based models in general, is
    that they are capable of handling various input types, from numerical inputs (both
    discrete and continuous) to binary inputs as well as categorical inputs. Numerical
    inputs can be ordered in a natural way by sorting them in ascending order, for
    example. When we do this, we can see that, if we have *k* distinct numbers, there
    are *k-1* distinct ways to split these into two groups so that all the numbers
    in one group are smaller than all the numbers in the second group, and both groups
    have at least one element. This is simply done by picking the numbers themselves
    as split points and not counting the smallest number as a split point (which would
    produce an empty group). So if we have a feature vector *x* with the numbers `{5.6,
    2.8, 9.0}`, we first sort these into `{2.8, 5.6, 9.0}`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: CART及其一般基于树的模型的一个自然优势是它们能够处理各种输入类型，从数值输入（离散和连续）到二进制输入以及分类输入。数值输入可以通过按升序排序以自然方式排序，例如。当我们这样做时，我们可以看到，如果我们有
    *k* 个不同的数字，那么将这些数字分成两组，使得一个组中的所有数字都小于第二个组中的所有数字，并且两个组至少有一个元素，有 *k-1* 种不同的分割方式。这很简单，只需选择数字本身作为分割点，而不将最小的数字作为分割点（这将产生一个空组）。所以如果我们有一个包含数字
    `{5.6, 2.8, 9.0}` 的特征向量 *x*，我们首先将这些数字排序为 `{2.8, 5.6, 9.0}`。
- en: 'Then, we take each number except the smallest `{2.8}`to form a split point
    and a corresponding rule that checks whether the input value is smaller than the
    split point. In this way, we produce the only two possible groupings for our feature
    vector:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们取除了最小的 `{2.8}` 之外的所有数字来形成一个分割点，并形成一个相应的规则来检查输入值是否小于分割点。通过这种方式，我们产生了我们特征向量的唯一两种可能的分组：
- en: '`Group1 = {2.8}, Group2 = {5.6, 9.0} IF x < 5.6 THEN Group1 ELSE Group2`'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Group1 = {2.8}, Group2 = {5.6, 9.0} IF x < 5.6 THEN Group1 ELSE Group2`'
- en: '`Group1 = {2.8, 5.6}, Group2 = {9.0} IF x < 9.0 THEN Group1 ELSE Group2`'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Group1 = {2.8, 5.6}, Group2 = {9.0} IF x < 9.0 THEN Group1 ELSE Group2`'
- en: Note that it is important to have at least one element in each group, otherwise
    we haven't actually split our data. Binary input features can also be handled
    by simply using the split point that corresponds to putting all data points that
    have this feature take the first value in the first group, and the remaining data
    points that have the second value of this feature in the second group.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每个组至少有一个元素是很重要的，否则我们实际上并没有分割我们的数据。二进制输入特征也可以通过简单地使用将所有具有此特征的第一值的数据点放入第一组，而具有此特征的第二个值的数据点放入第二组的分割点来处理。
- en: Handling unordered categorical input features (factors) is substantially harder
    because there is no natural order. As a result, any combination of levels can
    be assigned to the first group and the remainder to the second group. If we are
    dealing with a factor that has *k* distinct levels, then there are *2^(k-1)-1*
    possible ways to form two groups with at least one level assigned to each group.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 处理无序的分类输入特征（因子）要困难得多，因为没有自然顺序。因此，任何级别的组合都可以分配给第一组，其余的分配给第二组。如果我们处理一个有 *k* 个不同级别的因子，那么有
    *2^(k-1)-1* 种可能的分组方式，每个组至少有一个级别。
- en: So, a binary-valued feature has one possible split, as we know, and a three-valued
    feature has three possible splits. With the numerical feature vector containing
    the numbers `{5.6, 2.8, 9.0}`, we've already seen two possible splits. The third
    possible split that could arise if these numbers were labels is the one in which
    one group has data points with this feature taking the value `5.6`, and another
    group with the two values `2.8` and `9.0`. Clearly, this is not a valid split
    when we treat the feature as numerical.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个二值特征有一个可能的分割，正如我们所知，一个三值特征有三个可能的分割。在包含数字 `{5.6, 2.8, 9.0}` 的数值特征向量中，我们已经看到了两个可能的分割。如果这些数字是标签，可能出现的第三个分割是这样一个分割：一个组的数据点具有这个特征的值为
    `5.6`，另一个组具有两个值 `2.8` 和 `9.0`。显然，当我们把特征视为数值时，这不是一个有效的分割。
- en: As a final note, we always have the option of a one-versus-all approach for
    categorical input features, which is essentially the same as considering splits
    in which one group always consists of a single element. This is not always a good
    idea as it may turn out that a particular subset of the levels when taken together
    is more predictive of an output compared to a single level. If this is the case,
    the resulting tree will probably be more complex, having a greater number of node
    splits.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要注意的是，对于分类输入特征，我们始终可以选择一对一的方法，这本质上与考虑一个组始终由单个元素组成的分割相同。这并不总是一个好主意，因为它可能最终会显示出，当某些级别组合在一起时，它们可能比单个级别更能预测输出。如果这种情况发生，生成的树可能会更复杂，节点分割的数量会更多。
- en: 'There are various ways to deal with the large increase in complexity associated
    with finding and evaluating all the different split points for categorical input
    features, but we won''t go into further detail right now. Instead, let''s write
    some R code to see how we might find the split point of a numerical input feature
    using the SSE criterion that CART uses:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以处理与找到和评估所有不同分割点相关的大幅增加的复杂性，但我们现在不会进一步详细介绍。相反，让我们编写一些R代码来查看我们如何使用CART使用的SSE标准来找到数值输入特征的分割点：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In practice, 20 data points might be a suitable number to use as a threshold
    for building a leaf node, but for this example we will simply suppose that we
    wanted to make a new split using this data. We have two input features, `x1` and
    `x2`. The former is a binary input feature that we have coded using the numerical
    labels 0 and 1\. This allows us to reuse the functions we just wrote to compute
    the possible splits. The latter is a numerical input feature. By applying our
    `compute_all_SSE_splits()` function on each feature separately, we can compute
    all the possible split points for each feature and their SSE. The following two
    plots show these SSE values for each feature in turn:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，20个数据点可能是一个合适的数量，可以用作构建叶子节点的阈值，但在这个例子中，我们将简单地假设我们想要使用这些数据来创建一个新的分割。我们有两个输入特征，`x1`和`x2`。前者是一个二进制输入特征，我们使用数值标签0和1进行编码。这允许我们重用我们刚刚编写的函数来计算可能的分割。后者是一个数值输入特征。通过分别对每个特征应用我们的`compute_all_SSE_splits()`函数，我们可以计算每个特征的所有可能的分割点及其SSE。以下两个图表依次显示了每个特征的这些SSE值：
- en: '![CART regression trees](img/00131.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![CART回归树](img/00131.jpeg)'
- en: 'Looking at both plots, we can see that the best possible split produces an
    SSE value of `124.05` and this can be achieved by splitting on the feature `x2`
    at the value `18.7`. Consequently, our regression tree would contain a node with
    the following splitting rule:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这两个图表，我们可以看到最佳分割产生的SSE（总平方误差）值为`124.05`，这可以通过在特征`x2`的值`18.7`处进行分割来实现。因此，我们的回归树将包含以下分割规则：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The CART methodology always applies the same logic to determine whether to make
    a new split at each node as well as how to pick which feature and value to split
    on. This recursive approach of splitting up the data points at each node to build
    the regression tree is why this process is also known as **recursive partitioning**.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: CART方法始终应用相同的逻辑来确定在每个节点是否进行新的分割，以及如何选择分割的特征和值。这种在节点处递归分割数据点以构建回归树的方法也是为什么这个过程也被称为**递归分割**。
- en: Tree pruning
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 树剪枝
- en: If we were to allow the recursive partitioning process to repeat indefinitely,
    we would eventually terminate by having leaf nodes with a single data point each,
    because that is when we cannot split the data any further. This model would fit
    the training data perfectly, but it is highly unlikely that its performance would
    generalize on unseen data. Thus, tree-based models are susceptible to overfitting.
    To combat this, we need to control the depth of our final decision tree.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们允许递归分割过程无限期地重复，我们最终将通过每个叶子节点包含一个数据点来终止，因为那时我们不能再分割数据了。这个模型将完美地拟合训练数据，但它在未见过的数据上的性能很可能不会泛化。因此，基于树的模型容易过拟合。为了解决这个问题，我们需要控制最终决策树的深度。
- en: The process of removing nodes from the tree to limit its size and complexity
    is known as **pruning**. One possible pruning method is to impose a threshold
    for the smallest number of data points that can be used in order to create a new
    split in the tree instead of creating a leaf node. This will create leaf nodes
    earlier on in the procedure and the data points that are assigned to them may
    not all have the same output. In this case, we can simply predict the average
    value for regression (and the most popular class for classification). This is
    an example of **pre-pruning**, as we are pruning the tree while building it and
    before it is fully constructed.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从树中移除节点以限制其大小和复杂性的过程被称为**剪枝**。一种可能的剪枝方法是为创建新分割点而不是创建叶节点而使用可以使用的最小数据点数量设置一个阈值。这将使在程序早期就创建叶节点，分配给它们的那些数据点可能并不都具有相同的输出。在这种情况下，我们可以简单地预测回归的平均值（以及分类中最受欢迎的类别）。这是一个**预剪枝**的例子，因为我们是在树构建过程中以及它完全构建之前进行剪枝的。
- en: Intuitively, we should be able to see that, the larger the depth of the tree
    and the smaller the average number of data points assigned to leaf nodes, the
    greater the degree of overfitting. Of course, if we have fewer nodes in the tree,
    we probably aren't being granular enough in our modeling of the underlying data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，我们应该能够看到，树的深度越大，分配给叶节点的平均数据点数越小，过拟合的程度就越大。当然，如果我们树中的节点较少，我们可能没有足够细致地模拟底层数据。
- en: The question of how large a tree should be allowed to grow is thus effectively
    a question of how to model our data as closely as possible while controlling the
    degree of overfitting. In practice, using pre-pruning is tricky as it is difficult
    to find an appropriate threshold.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，树木应该允许生长到多大这个问题实际上是一个如何尽可能紧密地模拟我们的数据同时控制过拟合程度的问题。在实践中，使用预剪枝是棘手的，因为很难找到一个合适的阈值。
- en: 'Another regularization process that is used by the CART methodology to prune
    trees is known as **cost-complexity tuning**. In effect, trees are often allowed
    to grow fully using the recursive partitioning approach described in the previous
    section. Once this completes, we prune the resulting tree, that is to say, we
    start removing split points and merging leaf nodes to shrink the tree according
    to a certain criterion. This is known as **post-pruning**, as we prune the tree
    after it has been built. When we construct the original tree, the error function
    that we use is the SSE. To prune the tree, we use a penalized version of the SSE
    for minimizing:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: CART方法用来剪枝的另一种正则化过程被称为**成本复杂度调整**。实际上，树通常被允许使用上一节中描述的递归分割方法完全生长。一旦完成，我们就剪枝得到的树，也就是说，我们开始移除分割点并合并叶节点，根据一定标准缩小树的大小。这被称为**后剪枝**，因为我们是在树构建之后进行剪枝的。当我们构建原始树时，我们使用的误差函数是SSE。为了剪枝，我们使用惩罚版本的SSE来最小化：
- en: '![Tree pruning](img/00132.jpeg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![树木修剪](img/00132.jpeg)'
- en: Here, *α* is a complexity parameter controlling the degree of regularization
    and *Tp* is the number of nodes in the tree, which is a way to model the size
    of the tree. Similar to the way in which lasso limits the size of regression coefficients
    in generalized linear models, this regularization procedure limits the size of
    the resulting tree. A very small value of *α* results in a small degree of pruning,
    which in the limit of *α* taking the value of 0 corresponds to no pruning at all.
    On the other hand, using a high value for this parameter results in trees that
    are much shorter, which, at its limit, can result in a zero-sized tree with no
    splits at all that predicts a single average value of the output for all possible
    inputs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*α*是一个控制正则化程度的复杂度参数，*Tp*是树中的节点数，这是模拟树大小的一种方式。类似于lasso在广义线性模型中限制回归系数大小的方式，这种正则化过程限制了结果树的大小。*α*的值非常小会导致剪枝程度很小，当*α*取值为0时，极限情况下表示根本不进行剪枝。另一方面，使用这个参数的高值会导致树变得非常短，在极限情况下，可以导致没有分割点且大小为零的树，预测所有可能输入的输出平均值。
- en: It turns out that every particular value of *α* corresponds to a unique tree
    structure that minimizes this penalized form of the SSE for that particular value.
    Put differently, given a particular value for *α*, there is a unique and predictable
    way to prune a tree in order to minimize the penalized SSE, but the details of
    this procedure are beyond the scope of this book. For now, we can simply assume
    that every value of *α* is associated with a single tree.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，每个 *α* 的特定值都对应于一个独特的树结构，该结构最小化了该特定值的惩罚形式的SSE。换句话说，给定一个特定的 *α* 值，有一个独特且可预测的方式来修剪树以最小化惩罚SSE，但这个过程的细节超出了本书的范围。现在，我们只需假设每个
    *α* 值都与一棵树相关联。
- en: This particular feature is very useful, as we don't have any ambiguity in picking
    a tree once we settle on a value for the complexity parameter *α*. It does not,
    however, give us a way to determine what actual value we should use. Cross-validation,
    which we saw in [Chapter 5](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 5. Neural Networks"), *Support Vector Machines*, is a commonly used approach
    designed to estimate an appropriate value of this parameter. Cross-validation
    applied to this problem would involve partitioning the data into *k* folds. We
    then train and prune *k* trees by using all the data excluding a single fold and
    repeating this for each of the *k* folds. Finally, we measure the SSE on the folds
    held out for testing and average the results. We can repeat our cross-validation
    procedure for different values of *a*. Another approach when more data is available
    is to use a validation dataset for evaluating models that have been trained on
    the same training dataset, but with different values of *α*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定功能非常有用，因为我们一旦确定了复杂度参数 *α* 的值，在选择树时就不会有任何歧义。然而，它并没有给我们提供确定实际应该使用什么值的方法。交叉验证，我们在[第五章](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7
    "第五章。神经网络")中看到的，*支持向量机*，是一种常用的方法，旨在估计这个参数的适当值。将交叉验证应用于这个问题将涉及将数据分成 *k* 个部分。然后我们通过使用除了一个部分之外的所有数据来训练和修剪
    *k* 棵树，并对每一部分重复此过程。最后，我们在为测试保留的部分上测量SSE，并平均结果。我们可以对 *a* 的不同值重复我们的交叉验证过程。当有更多数据可用时，另一种方法是使用验证数据集来评估在相同训练数据集上训练但
    *α* 值不同的模型。
- en: Missing data
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺失数据
- en: One characteristic of decision trees is that they have a natural way of handling
    missing data during training. For example, when we consider which feature to split
    on at a particular node, we can ignore data points that have a missing value for
    a particular feature and compute the potential reduction in our error function
    (deviance, SSE, and so on) using the remaining data points. Note that while this
    approach is handy, it could potentially increase the bias of the model substantially,
    especially if we are ignoring a large portion of our available training data because
    of missing values.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的一个特点是它们在训练过程中自然地处理缺失数据。例如，当我们考虑在特定节点上分割哪个特征时，我们可以忽略具有特定特征缺失值的点，并使用剩余的数据点计算我们的误差函数（偏差、SSE等）的潜在减少。请注意，虽然这种方法很方便，但它可能会大大增加模型的偏差，特别是如果我们因为缺失值而忽略了大量可用训练数据的话。
- en: One might wonder whether we are able to handle missing values during prediction
    for unseen data points. If we are at a particular node in the tree that is splitting
    on a feature, and for which our test data point has a missing value, we are seemingly
    stuck. In practice, this situation can be dealt with via the use of **surrogate
    splits**. The key notion behind these is that, for every node in the tree, apart
    from the feature that was optimally chosen to split on, we keep track of a list
    of other features that produce splits in the data similar to the feature we actually
    chose. In this way, when our testing data point has a missing value for a feature
    that we need in order to make a prediction, we can refer to a node's surrogate
    splits instead, and use a different feature for this node.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 人们可能会想知道我们是否能够在预测未见数据点时处理缺失值。如果我们处于一个在某个特征上分割的特定节点，并且我们的测试数据点在该特征上有一个缺失值，我们似乎就陷入了困境。在实践中，这种情况可以通过使用**代理分割**来处理。这些方法背后的关键概念是，对于树中的每个节点，除了最优分割特征外，我们还跟踪一个其他特征列表，这些特征在数据中产生与实际选择的特征相似的分割。这样，当我们的测试数据点在需要做出预测的特征上有一个缺失值时，我们可以参考节点的代理分割，并使用不同的特征来处理这个节点。
- en: Regression model trees
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归模型树
- en: One of the potential drawbacks of regression trees built with CART is that,
    even though we limit the number of data points that are assigned to a particular
    leaf node, these may still have significant variations in the output variable
    among themselves. When this happens, taking the average value and using this as
    a single prediction for that leaf node may not be the best idea.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CART 构建的回归树的一个潜在缺点是，尽管我们限制了分配给特定叶节点的数据点的数量，但这些数据点之间仍然可能在输出变量上有显著的差异。当这种情况发生时，取平均值并以此作为该叶节点的一个单一预测可能不是最好的主意。
- en: '**Regression model trees** attempt to overcome this limitation by using the
    data points at the leaf nodes to construct a linear model to predict the output.
    The original regression model tree algorithm was developed by *J. Ross Quinlan*
    and is known as **M5**. The M5 algorithm computes a linear model at each node
    in the tree. For a test data point, we first compute the decision path traversed
    from the root node to the leaf node. The prediction that is then made is the output
    of the linear model associated with that leaf node.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**回归模型树**试图通过使用叶节点上的数据点来构建一个线性模型以预测输出，来克服这个限制。原始的回归模型树算法是由 *J. Ross Quinlan*
    开发的，被称为 **M5**。M5 算法在树的每个节点上计算一个线性模型。对于测试数据点，我们首先计算从根节点到叶节点的决策路径。然后做出的预测是与该叶节点相关的线性模型的输出。'
- en: 'M5 also differs from the algorithm used in CART in that it employs a different
    criterion to determine which feature to split on. This criterion is the weighted
    reduction in standard deviation:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: M5 与 CART 中使用的算法的不同之处在于，它采用不同的标准来确定在哪个特征上进行分割。这个标准是加权标准差的减少：
- en: '![Regression model trees](img/00133.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![回归模型树](img/00133.jpeg)'
- en: This general equation assumes that we split the data into *p* partitions (as
    we have seen for trees, *p* is typically 2). For each partition *i*, we compute
    the standard deviation *σ[i]*. Then we compute the weighted average of these standard
    deviations using the relative size of each partition (*n[i]/n*) as the weights.
    This is subtracted from the initial standard deviation of the unpartitioned data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个通用方程假设我们将数据分成 *p* 个分区（正如我们在树结构中看到的，*p* 通常为 2）。对于每个分区 *i*，我们计算标准差 *σ[i]*。然后，我们使用每个分区的相对大小（*n[i]/n*）作为权重，计算这些标准差的加权平均值。这个值从未分区数据的初始标准差中减去。
- en: The idea behind this criterion is that splitting a node should produce groups
    of data points that within each group display less variability with respect to
    the output variable than all the data points when grouped together. We'll have
    a chance to see *M5 trees* as well as CART trees in action later on in this chapter.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这个标准的背后思想是，分割一个节点应该产生数据点组，在每组中，与所有数据点分组在一起相比，输出变量的变异性更小。我们将在本章后面有机会看到 *M5 树*
    以及 CART 树的实际应用。
- en: CART classification trees
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CART 分类树
- en: Building classification trees using the CART methodology continues the notion
    of recursively splitting up groups of data points in order to minimize some error
    function. Our first guess for an appropriate error function is the classification
    accuracy. It turns out that this is not a particularly good measure to use to
    build a classification tree.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CART 方法构建分类树继续了递归分割数据点组以最小化某些误差函数的概念。我们首先猜测一个合适的误差函数是分类准确度。结果证明，这不是构建分类树的一个特别好的度量。
- en: What we would actually like to use is a measure for node purity that would score
    nodes based on whether they contain data points primarily belonging to one of
    the output classes. This is a very intuitive idea because what we are effectively
    aiming for in a classification tree is to eventually be able to group our training
    data points into sets of data points at the leaf nodes, so that each leaf node
    contains data points belonging to only one of the classes. This will mean that
    we can confidently predict this class if we arrive at that leaf node during prediction.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上想使用的是节点纯度的度量，该度量会根据节点是否包含主要属于一个输出类别的数据点来评分。这是一个非常直观的想法，因为我们实际上在分类树中追求的是最终能够将我们的训练数据点分组到叶节点上的数据点集合中，使得每个叶节点只包含属于一个类别的数据点。这意味着如果我们预测时到达那个叶节点，我们可以自信地预测这个类别。
- en: 'One possible measure of node purity, frequently used with CART for classification
    trees, is the **Gini index**. For an output variable with *K* different classes,
    the Gini index *G* is defined as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 节点纯度的一个可能度量，常与CART分类树一起使用，是**基尼指数**。对于一个有 *K* 个不同类别的输出变量，基尼指数 *G* 定义如下：
- en: '![CART classification trees](img/00134.jpeg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![CART分类树](img/00134.jpeg)'
- en: To calculate the Gini index, we compute an estimate of the probability of every
    class and multiply this with the probability of not being that class. We then
    add up all these products. For a binary classification problem, it should be easy
    to see that the Gini index evaluates to *2* ![CART classification trees](img/00135.jpeg)
    (*1- * ![CART classification trees](img/00135.jpeg)), where ![CART classification
    trees](img/00135.jpeg) is the estimated probability of one of the classes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算基尼指数，我们计算每个类别的概率估计，并将其与不是该类别的概率相乘。然后我们将所有这些乘积相加。对于二元分类问题，应该很容易看出基尼指数等于 *2*
    ![CART分类树](img/00135.jpeg) (*1- * ![CART分类树](img/00135.jpeg))，其中 ![CART分类树](img/00135.jpeg)
    是一个类别的估计概率。
- en: 'To compute the Gini index at a particular node in a tree, we can simply use
    the ratio of the number of data points labeled as class *k* over the total number
    of data points as an estimate for the probability of a data point belonging to
    class *k* at the node in question. Here is a simple R function to compute the
    Gini index:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算树中特定节点的基尼指数，我们可以简单地使用标记为类别 *k* 的数据点数与总数据点数的比率，作为该节点中数据点属于类别 *k* 的概率的估计。以下是一个简单的R函数来计算基尼指数：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To compute the Gini index, our `gini_index()` function first tabulates all
    the entries in a vector. It divides each of these frequency counts with the total
    number of counts to transform them into probability estimates. Finally, it computes
    the product (*1-*) for each of these and sums up all the terms. Let''s try a few
    examples:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算基尼指数，我们的 `gini_index()` 函数首先将向量中的所有条目进行汇总。它将每个这些频率计数除以总计数，将它们转换为概率估计。最后，它计算每个这些的乘积
    (*1-*) 并对所有这些项进行求和。让我们尝试几个例子：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Note how the Gini index for a completely pure node (a node with only one class)
    is 0\. For a binary output with equal proportions of the two classes, the Gini
    index is 0.5\. Similar to the standard deviation in regression trees, we use the
    weighted reduction in the Gini index, where we weigh each partition by its relative
    size, to determine appropriate split points:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，完全纯的节点（只有一个类别的节点）的基尼指数为0。对于两个类别比例相等的二元输出，基尼指数为0.5。类似于回归树中的标准差，我们使用加权减少的基尼指数，其中我们按相对大小权衡每个分区，以确定适当的分割点：
- en: '![CART classification trees](img/00136.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![CART分类树](img/00136.jpeg)'
- en: 'Another commonly used criterion is deviance. When we studied logistic regression,
    we saw that this is just the constant *-2* multiplied by the log-likelihood of
    the data. In a classification tree setting, we compute the deviance of a node
    in a classification tree as:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用的标准是偏差。当我们研究逻辑回归时，我们看到了这仅仅是常数 *-2* 乘以数据的对数似然。在分类树设置中，我们计算分类树中一个节点的偏差如下：
- en: '![CART classification trees](img/00137.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![CART分类树](img/00137.jpeg)'
- en: Unlike the Gini index, the total number of observations *n[k]* at a node affects
    the value of deviance. All nodes that have the same proportion of data points
    across different classes will have the same value of the Gini index, but if they
    have different numbers of observations, they will have different values of deviance.
    In both splitting criteria, however, a completely pure node will have a value
    of 0 and a positive value otherwise.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与基尼指数不同，节点处的观察总数 *n[k]* 影响偏差的值。所有具有不同类别间数据点相同比例的节点将具有相同的基尼指数值，但如果它们有不同的观察数，它们将具有不同的偏差值。然而，在两种分割标准中，一个完全纯的节点将具有0值，否则为正值。
- en: Aside from using a different splitting criterion, the logic to build a classification
    tree using the CART methodology is exactly parallel to that of building a regression
    tree. Missing values are handled in the same way and the tree is pre-pruned in
    the same way using a threshold on the number of data points left to build leaf
    nodes. The tree is also post-pruned using the same cost-complexity approach outlined
    for regression trees, but after replacing the SSE as the error function with either
    the Gini index or the deviance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用不同的分裂标准外，使用CART方法构建分类树的逻辑与构建回归树的逻辑完全平行。缺失值以相同的方式处理，并且使用剩余数据点构建叶节点的数量阈值对树进行预剪枝。树也使用与回归树中概述的相同成本复杂度方法进行后剪枝，但用Gini指数或偏差替换了SSE作为误差函数。
- en: C5.0
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C5.0
- en: The **C5.0** algorithm developed by *Ross Quinlan* is an algorithm to build
    a decision tree for classification. This algorithm is the latest in a chain of
    successively improved versions starting from an algorithm known as **ID3**, which
    developed into **C4.5** (and an open source implementation in the Java programming
    language known as **J48**) before culminating in C5.0\. There are many good acronyms
    used for decision trees, but thankfully many of them are related to each other.
    The C5.0 chain of algorithms has several differences from the CART methodology,
    most notably in the choice of the splitting criterion as well as in the pruning
    procedure.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由*罗斯·奎因兰*开发的**C5.0**算法是一种用于构建分类决策树的算法。这个算法是自一个被称为**ID3**的算法开始的一系列连续改进版本中的最新版本，该算法发展成了**C4.5**（以及在Java编程语言中称为**J48**的开源实现），最终演变为C5.0。用于决策树的好缩写有很多，但幸运的是，其中许多都是相互关联的。C5.0算法链与CART方法有几个不同之处，最显著的是在分裂标准的选择以及剪枝过程上。
- en: 'The splitting criterion used with C5.0 is known as **entropy** or the **information
    statistic**, and has its roots in information theory. Entropy is defined as the
    average number of binary digits (bits) needed to communicate information via a
    message as a function of the probabilities of the different symbols used. Entropy
    also has roots in statistical physics, where it is used to represent the degree
    of chaos and uncertainty in a system. When the symbols or components of a system
    have equal probabilities, there is a high degree of uncertainty, but entropy is
    lower when one symbol is far likelier than the others. This observation renders
    the definition of entropy very useful in measuring node purity. The formal definition
    of entropy in bits for the multiclass scenario with *K* classes is:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: C5.0使用的分裂标准被称为**熵**或**信息统计量**，其根源在于信息理论。熵被定义为通过消息传递所需平均二进制数字（比特）的数量，作为不同符号概率的函数。熵在统计物理学中也有其根源，在那里它被用来表示系统中的混沌和不确定程度。当一个系统的符号或组成部分具有相等的概率时，存在很高的不确定性，但当某个符号比其他符号远更可能时，熵较低。这一观察使得熵的定义在衡量节点纯度时非常有用。对于具有*K*个类别的多类场景，熵的正式定义（以比特为单位）是：
- en: '![C5.0](img/00138.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![C5.0](img/00138.jpeg)'
- en: 'In the binary case, the equation simplifies to (where *p* arbitrarily refers
    to the probability of one of the two classes):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在二进制情况下，方程简化为（其中*p*任意指代两个类别中的一个的概率）：
- en: '![C5.0](img/00139.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![C5.0](img/00139.jpeg)'
- en: 'We can compare entropy to the Gini index for binary classification in the following
    plot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下图表中比较熵与二分类的Gini指数：
- en: '![C5.0](img/00140.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![C5.0](img/00140.jpeg)'
- en: 'From the plot, we can see that both functions have the same general shape for
    the binary class problem. Recall that the lower the entropy, the lower the uncertainty
    we have about the distribution of our classes and hence we have higher node purity.
    Consequently, we want to minimize the entropy as we build our tree. In ID3, the
    splitting criterion that is used is the weighted entropy reduction, which is also
    known as **information gain**:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中我们可以看到，对于二类问题，这两个函数具有相同的一般形状。回想一下，熵越低，我们对类别分布的不确定性就越低，因此节点纯度就越高。因此，我们在构建树的过程中希望最小化熵。在ID3中，使用的分裂标准是加权熵减少，也称为**信息增益**：
- en: '![C5.0](img/00141.jpeg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![C5.0](img/00141.jpeg)'
- en: It turns out that this criterion suffers from **selection bias**, in that it
    tends to favor categorical variables because of the large number of possible groupings
    compared to the linear range of splits we find with continuous features. To combat
    this, from C4.5 onwards the criterion was refined into the **information gain
    ratio**. This is a normalized version of information gain, where we normalize
    with respect to a quantity known as the **split information value**.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，这个标准存在**选择偏差**的问题，因为它倾向于偏好分类变量，因为与我们在连续特征中找到的分割范围相比，可能的分组数量要多得多。为了解决这个问题，从C4.5开始，这个标准被细化为**信息增益率**。这是信息增益的标准化版本，其中我们相对于一个称为**分割信息值**的量进行标准化。
- en: 'This in turn represents the potential increase in information that we can get
    just by the size of the partitions themselves. A high split information value
    occurs when we have evenly sized partitions and a low value occurs when most of
    the data points are concentrated in a small number of the partitions. In summary,
    we have this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这反过来又代表了仅通过分区本身的大小就能获得的潜在信息增加。当我们有大小均匀的分区时，就会发生高分割信息值；而当大多数数据点集中在少数几个分区中时，就会发生低值。总的来说，我们有以下内容：
- en: '![C5.0](img/00142.jpeg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![C5.0](img/00142.jpeg)'
- en: The C5.0 chain of algorithms also incorporate alternative methods to prune a
    tree that go beyond the simple elimination of nodes and sub-trees. For example,
    inner nodes may be removed before leaf nodes so that the nodes beneath the removed
    node (the sub-tree) are pushed up (raised) to replace the removed node. C5.0,
    in particular, is a very powerful algorithm that also contains improvements to
    speed, memory usage, native boosting (covered in the next chapter) capabilities,
    as well as the ability to specify a cost matrix so that the algorithm can avoid
    making certain types of misclassifications over others, just as we saw with support
    vector machines in the previous chapter.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: C5.0算法链还包含了超越简单节点和子树消除的剪枝方法。例如，内部节点可以在叶节点之前被移除，这样被移除节点（子树）下的节点（子树）就会被推上去（提升）以替换被移除的节点。特别是C5.0是一个非常强大的算法，它还包含了改进速度、内存使用、原生提升（将在下一章中介绍）的能力，以及指定成本矩阵的能力，这样算法就可以避免在某些类型的误分类上比其他类型更频繁地发生，正如我们在上一章中看到的支持向量机那样。
- en: We'll demonstrate how to build trees with C5.0 in R in a subsequent section.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中演示如何在R中使用C5.0构建树。
- en: Predicting class membership on synthetic 2D data
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在合成2D数据上预测类别成员资格
- en: Our first example showcasing tree-based methods in R will operate on a synthetic
    dataset that we have created. The dataset can be generated using commands in the
    companion R file for this chapter, available from the publisher. The data consists
    of 287 observations of two input features, `x1` and `x2`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一个展示R中基于树的方法的例子将操作于我们创建的合成数据集。该数据集可以使用本章配套R文件的命令生成，该文件由出版商提供。数据包括287个观测值，两个输入特征`x1`和`x2`。
- en: 'The output variable is a categorical variable with three possible classes:
    `a`, `b`, and `c`. If we follow the commands in the code file, we will end up
    with a data frame in R, `mcdf`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 输出变量是一个具有三个可能类别的分类变量：`a`、`b`和`c`。如果我们遵循代码文件中的命令，我们将最终在R中得到一个数据框`mcdf`：
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This problem is actually very simple because, on the one hand, we have a very
    small dataset with only two features, and on the other the classes happen to be
    quite well separated in the feature space, something that is very rare. Nonetheless,
    our objective in this section is to demonstrate the construction of a classification
    tree on *well-behaved* data before we get our hands (or keyboards) dirty on a
    real-world dataset in the next section.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题实际上非常简单，因为一方面，我们有一个非常小的数据集，只有两个特征，另一方面，类别在特征空间中恰好被很好地分开，这是非常罕见的。尽管如此，在本节中，我们的目标是演示在下一节在真实世界数据集上动手（或键盘）之前，在*表现良好*的数据上构建分类树。
- en: To build a classification tree for this dataset, we will use the `tree` package,
    which provides us with the `tree()` function that trains a model using the CART
    methodology. As is the norm, the first parameter to be provided is a formula and
    the second parameter is the data frame. The function also has a `split` parameter
    that identifies the criterion to be used for splitting. By default, this is set
    to `deviance` for the deviance criterion, for which we observed better performance
    on this dataset. We encourage readers to repeat these experiments by setting the
    `split` parameter to `gini` for splitting on the Gini index.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为这个数据集构建一个分类树，我们将使用 `tree` 包，它为我们提供了 `tree()` 函数，该函数使用 CART 方法训练模型。按照惯例，第一个要提供的参数是一个公式，第二个参数是数据框。该函数还有一个
    `split` 参数，用于标识用于分割的标准。默认情况下，这个参数设置为 `deviance`，对于偏差标准，我们在该数据集上观察到更好的性能。我们鼓励读者通过将
    `split` 参数设置为 `gini` 来在基尼指数上分割重复这些实验。
- en: 'Without further ado, let us train our first decision tree:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 不再拖延，让我们训练我们的第一个决策树：
- en: '[PRE7]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We invoke the `summary()` function on our trained model to get some useful
    information about the tree we built. Note that for this example, we won''t be
    splitting our data into a training and test set, as our goal is to discuss the
    quality of the model fit first. From the provided summary, we seem to have only
    misclassified a single example in our entire dataset. Ordinarily, this would raise
    suspicion that we are overfitting; however, we already know that our classes are
    well separated in the feature space. We can use the `plot()` function to plot
    the shape of our tree as well as the `text()` function to display all the relevant
    labels so we can fully visualize the classifier we have built:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在我们的训练模型上调用 `summary()` 函数来获取有关我们构建的树的一些有用信息。请注意，对于这个例子，我们不会将我们的数据分成训练集和测试集，因为我们的目标是首先讨论模型拟合的质量。从提供的摘要来看，我们似乎在整个数据集中只误分类了一个示例。通常情况下，这会引发我们过度拟合的怀疑；然而，我们已经知道我们的类别在特征空间中分布良好。我们可以使用
    `plot()` 函数来绘制树的形状，以及使用 `text()` 函数来显示所有相关的标签，这样我们就可以完全可视化我们构建的分类器：
- en: '[PRE8]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This is the plot that is produced:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是产生的图表：
- en: '![Predicting class membership on synthetic 2D data](img/00143.jpeg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![在合成2D数据上预测类别成员](img/00143.jpeg)'
- en: Note that our plot shows a predicted class for every node, including non-leaf
    nodes. This simply allows us to see which class is predominant at every step of
    the tree. For example, at the root node, we see that the predominant class is
    class `b`, simply because this is the most commonly represented class in our dataset.
    It is instructive to be able to see the partitioning of our 2D space that our
    decision tree represents.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们的图表显示了每个节点的预测类别，包括非叶节点。这仅仅允许我们看到在树的每一步中哪个类别占主导地位。例如，在根节点，我们看到主导类别是类别 `b`，仅仅因为这个类别在我们的数据集中是最常见的。能够看到我们的决策树所表示的2D空间的划分是有教育意义的。
- en: 'For one and two features, the `tree` package allows us to use the `partition.tree()`
    function to visualize our decision tree. We have done this and superimposed our
    original data over it in order to see how the classifier has partitioned the space:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个和两个特征，`tree` 包允许我们使用 `partition.tree()` 函数来可视化我们的决策树。我们已经这样做了，并将我们的原始数据叠加在其上，以便看到分类器是如何划分空间的：
- en: '![Predicting class membership on synthetic 2D data](img/00144.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![在合成2D数据上预测类别成员](img/00144.jpeg)'
- en: Most of us would probably identify six clusters in our data; however, the clusters
    on the top-right of the plot are both assigned to class `b` and so the tree classifier
    has identified this entire region of space as a single leaf node. Finally, we
    can spot the misclassified point belonging to class `b` that has been assigned
    to class `c` (it is the triangle in the middle of the top part of the graph).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大多数人可能会在我们的数据中识别出六个簇；然而，图表右上角的簇都被分配到了类别 `b`，因此树分类器已经将这个整个空间区域识别为单个叶节点。最后，我们可以看到属于类别
    `b` 但被分配到类别 `c` 的误分类点（它在图表顶部中间的三角形中）。
- en: Another interesting observation to make is how efficiently the space has been
    partitioned into rectangles in this particular case (only five rectangles for
    a dataset with six clusters). On the other hand, we can expect this model to have
    some instabilities because several of the boundaries of the rectangles are very
    close to data points in the dataset (and thus close to the edges of a cluster).
    Consequently, we should also expect to obtain a lower accuracy with unseen data
    that is generated from the same process that generated our training data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得注意的有趣观察是，在这个特定情况下，空间是如何有效地被划分为矩形的（对于有六个聚类的数据集，只有五个矩形）。另一方面，我们可以预期这个模型可能存在一些不稳定性，因为几个矩形的边界与数据集中的数据点非常接近（因此接近聚类的边缘）。因此，我们也应该预期，使用与生成我们的训练数据相同过程生成的未见数据，将获得较低的准确度。
- en: In the next section, we will build a tree model for a real-world classification
    problem.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将为现实世界的分类问题构建一个树模型。
- en: Predicting the authenticity of banknotes
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测纸币的真实性
- en: In this section, we will study the problem of predicting whether a particular
    banknote is genuine or whether it has been forged. The *banknote authentication
    dataset* is hosted at [https://archive.ics.uci.edu/ml/datasets/banknote+authentication](https://archive.ics.uci.edu/ml/datasets/banknote+authentication).
    The creators of the dataset have taken specimens of both genuine and forged banknotes
    and photographed them with an industrial camera. The resulting grayscale image
    was processed using a type of time-frequency transformation known as a **wavelet
    transform**. Three features of this transform are constructed, and, along with
    the image entropy, they make up the four features in total for this binary classification
    task.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究预测特定纸币是真是伪造的问题。*纸币认证数据集*托管在[https://archive.ics.uci.edu/ml/datasets/banknote+authentication](https://archive.ics.uci.edu/ml/datasets/banknote+authentication)。数据集的创建者从真币和伪造币中取了样本，并用工业相机拍摄。生成的灰度图像使用一种称为**小波变换**的时间-频率变换进行处理。构建了该变换的三个特征，加上图像熵，它们构成了这个二元分类任务中的四个特征。
- en: '| Column name | Type | Definition |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 类型 | 定义 |'
- en: '| --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `waveletVar` | Numerical | Variance of the wavelet-transformed image |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| `waveletVar` | 数值 | 小波变换图像的方差 |'
- en: '| `waveletSkew` | Numerical | Skewness of the wavelet-transformed image |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| `waveletSkew` | 数值 | 小波变换图像的偏度 |'
- en: '| `waveletCurt` | Numerical | Kurtosis of the wavelet-transformed image |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| `waveletCurt` | 数值 | 小波变换图像的峰度 |'
- en: '| `entropy` | Numerical | Entropy of the image |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| `熵` | 数值 | 图像的熵 |'
- en: '| `class` | Binary | Authenticity (a 0 output means genuine and a 1 output
    means forged) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| `class` | 二元 | 真实性（0输出表示真币，1输出表示伪造币） |'
- en: 'First, we will split our 1,372 observations into training and test sets:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将我们的1,372个观察值分为训练集和测试集：
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we will introduce the `C50` R package that contains an implementation
    of the C5.0 algorithm for classification. The `C5.0()` function that belongs to
    this package also takes in a formula and a data frame as its minimum required
    input. Just as before, we can use the `summary()` function to examine the resulting
    model. Instead of reproducing the entire output of the latter, we''ll focus on
    just the tree that is built:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍包含C5.0算法分类实现的`C50` R包。属于此包的`C5.0()`函数也接受公式和数据框作为其最小必需输入。就像之前一样，我们可以使用`summary()`函数来检查生成的模型。我们不会重现后者的整个输出，而是只关注构建的树：
- en: '[PRE10]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we can see, it is perfectly acceptable to use a feature more than once in
    the tree in order to make a new split. The numbers in brackets to the right of
    the leaf nodes in the tree indicate the number of observations from each class
    that are assigned to that node. As we can see, the vast majority of the leaf nodes
    in the tree are pure nodes, so that only observations from one class are assigned
    to them.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在树中使用一个特征多次以创建新的分割是完全可接受的。树中叶节点右侧括号中的数字表示分配给该节点的每个类别的观察数。如我们所见，树中的绝大多数叶节点是纯节点，因此只分配了来自一个类的观察值。
- en: 'Only two leaf nodes have a single observation each from the minority class
    for that node, and with this we can infer that we only made two mistakes in our
    training data using this model. To see if our model has overfitted the data or
    whether it really can generalize well, we''ll test it on our test set:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 只有两个叶节点各自来自少数类的单个观察值，因此我们可以推断，使用这个模型我们只犯了两个训练数据错误。为了看看我们的模型是否过度拟合了数据，或者它是否真的可以很好地泛化，我们将在测试集上对其进行测试：
- en: '[PRE11]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The test accuracy is near perfect, a rare sight and the last time in this chapter
    that we'll be finished so easily! As a final note, `C50()` also has a `costs`
    parameter, which is useful for dealing with asymmetric error costs.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 测试准确率几乎完美，这是一个罕见的景象，也是本章中我们最后一次如此轻松地完成！最后值得一提的是，`C50()` 还有一个 `costs` 参数，这对于处理不对称错误成本非常有用。
- en: Predicting complex skill learning
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测复杂技能学习
- en: In this section, we'll have a chance to explore data from an innovative and
    recent project known as *SkillCraft*. The interested reader can find out more
    about this project on the web by going to [http://skillcraft.ca/](http://skillcraft.ca/).
    The key premise behind the project is that, by studying the performance of players
    in a **real-time strategy** (**RTS**) game that involves complex resource management
    and strategic decisions, we can study how humans learn complex skills and develop
    speed and competence in dynamic resource allocation scenarios. To achieve this,
    data has been collected from players playing the popular real-time strategy game,
    *Starcraft 2*, developed by *Blizzard*.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将有机会探索一个名为 *SkillCraft* 的新近项目的数据。感兴趣的读者可以通过访问 [http://skillcraft.ca/](http://skillcraft.ca/)
    在网上了解更多关于这个项目的信息。该项目背后的关键前提是，通过研究涉及复杂资源管理和战略决策的实时策略游戏（**RTS**）中玩家的表现，我们可以研究人类如何学习复杂技能，并在动态资源分配场景中提高速度和竞争力。为了实现这一点，已经收集了玩家在由
    *Blizzard* 开发的热门实时策略游戏 *Starcraft 2* 中玩游戏的资料。
- en: In this game, players compete against each other on one of many fixed maps and
    starting locations. Each player must choose a fictional race from three available
    choices and start with six worker units, which are used to collect one of two
    game resources. These resources are needed in order to build military and production
    buildings, military units unique to each race, research technologies, and to build
    more worker units. The game involves a mix of economic advancement, military growth,
    and military strategy in real-time engagements.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个游戏中，玩家在许多固定地图和起始位置之一与其他玩家竞争。每个玩家必须从三个可选的虚构种族中选择一个，并从六个工人单位开始，这些单位用于收集两种游戏资源中的一种。这些资源是建造军事和生产建筑、每个种族独特的军事单位、研究技术和建造更多工人单位所必需的。游戏涉及经济进步、军事增长和实时交战中的军事策略。
- en: Players are pitted against each other via an online matching algorithm that
    groups players into leagues according to their perceived level of skill. The algorithm's
    perception of a player's skill changes over time on the basis of that player's
    performance across the games in which the player participates. There are eight
    leagues in total, which are uneven in population in that the lower leagues tend
    to have more players and the upper leagues have fewer players.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 玩家通过在线匹配算法相互对抗，该算法根据玩家感知的技能水平将玩家分组到联赛中。算法对玩家技能的感知会根据玩家在参与的比赛中表现的变化而随时间变化。总共有八个联赛，人口分布不均，低级别联赛通常有更多玩家，而高级别联赛玩家较少。
- en: Having a basic understanding of the game, we can download the SkillCraft1 Master
    Table dataset from the UCI Machine Learning repository by going to [https://archive.ics.uci.edu/ml/datasets/SkillCraft1+Master+Table+Dataset](https://archive.ics.uci.edu/ml/datasets/SkillCraft1+Master+Table+Dataset).
    The rows of this dataset are individual games that are played and the features
    of the games are metrics of a player's playing speed, competence, and decision-making.
    The authors of the dataset have used both standard performance metrics familiar
    to players of the game, as well as other metrics such as **Perception Action Cycles**
    (**PACs**), which attempt to quantify a player's actions at the fixed location
    on the map at which a player is looking during a particular time window.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对游戏有基本了解后，我们可以通过访问[https://archive.ics.uci.edu/ml/datasets/SkillCraft1+Master+Table+Dataset](https://archive.ics.uci.edu/ml/datasets/SkillCraft1+Master+Table+Dataset)从UCI机器学习仓库下载SkillCraft1大师表数据集。该数据集的行是玩过的单个游戏，游戏特征是玩家游戏速度、能力和决策的指标。数据集的作者使用了玩家熟悉的标准化性能指标，以及其他指标，如**感知动作周期**（**PACs**），这些指标试图量化玩家在特定时间窗口内查看地图上固定位置的动作。
- en: The task at hand is to predict which of the eight leagues a player is currently
    assigned to on the basis of these performance metrics. Our output variable is
    an ordered categorical variable because we have eight distinct leagues ordered
    from 1 to 8, where the latter corresponds to the league with players of the highest
    skill.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当前任务是根据这些性能指标预测玩家目前被分配到八个联赛中的哪一个。我们的输出变量是一个有序分类变量，因为我们有八个不同的联赛，从1到8排序，其中后者对应于拥有最高技能玩家的联赛。
- en: 'One possible way to deal with ordinal outputs is to treat them as a numeric
    variable, modeling this as a regression task, and build a regression tree. The
    following table describes the features and output variables that we have in our
    dataset:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 处理有序输出的一个可能方法是将它们视为数值变量，将其建模为回归任务，并构建回归树。以下表格描述了我们数据集中的特征和输出变量：
- en: '| Feature name | Type | Description |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 特征名称 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `Age` | Numeric | Player''s age |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| `Age` | 数值 | 玩家年龄 |'
- en: '| `HoursPerWeek` | Numeric | Reported hours spent playing per week |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| `HoursPerWeek` | 数值 | 每周报告的游戏时间 |'
- en: '| `TotalHours` | Numeric | Reported total hours ever spent playing |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| `TotalHours` | 数值 | 报告的累计游戏时间 |'
- en: '| `APM` | Numeric | Game actions per minute |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| `APM` | 数值 | 每分钟游戏动作数 |'
- en: '| `SelectByHotkeys` | Numeric | Number of unit or building selections made
    using hotkeys per timestamp |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| `SelectByHotkeys` | 数值 | 每个时间戳使用快捷键进行的单位或建筑选择数量 |'
- en: '| `AssignToHotkeys` | Numeric | Number of units or buildings assigned to hotkeys
    per timestamp |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| `AssignToHotkeys` | 数值 | 每个时间戳分配给快捷键的单位或建筑数量 |'
- en: '| `UniqueHotkeys` | Numeric | Number of unique hotkeys used per timestamp |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| `UniqueHotkeys` | 数值 | 每个时间戳使用的独特快捷键数量 |'
- en: '| `MinimapAttacks` | Numeric | Number of attack actions on minimap per timestamp
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| `MinimapAttacks` | 数值 | 每个时间戳在最小地图上的攻击动作数量 |'
- en: '| `MinimapRightClicks` | Numeric | Number of right-clicks on minimap per timestamp
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| `MinimapRightClicks` | 数值 | 每个时间戳在最小地图上的右键点击次数 |'
- en: '| `NumberOfPACs` | Numeric | Number of PACs per timestamp |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| `NumberOfPACs` | 数值 | 每个时间戳的PAC数量 |'
- en: '| `GapBetweenPACs` | Numeric | Mean duration in milliseconds between PACs |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| `GapBetweenPACs` | 数值 | PACs之间的平均持续时间（毫秒） |'
- en: '| `ActionLatency` | Numeric | Mean latency from the onset of a PAC to their
    first action in milliseconds |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| `ActionLatency` | 数值 | 从PAC开始到第一次动作的平均延迟（毫秒） |'
- en: '| `ActionsInPAC` | Numeric | Mean number of actions within each PAC |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| `ActionsInPAC` | 数值 | 每个PAC内的平均动作数 |'
- en: '| `TotalMapExplored` | Numeric | The number of 24x24 game coordinate grids
    viewed by the player per timestamp |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| `TotalMapExplored` | 数值 | 玩家在每个时间戳查看的24x24游戏坐标网格数量 |'
- en: '| `WorkersMade` | Numeric | Number of worker units trained per timestamp |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| `WorkersMade` | 数值 | 每个时间戳训练的工人单位数量 |'
- en: '| `UniqueUnitsMade` | Numeric | Unique units made per timestamp |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `UniqueUnitsMade` | 数值 | 每个时间戳制作的独特单位 |'
- en: '| `ComplexUnitsMade` | Numeric | Number of complex units trained per timestamp
    |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| `ComplexUnitsMade` | 数值 | 每个时间戳训练的复杂单位数量 |'
- en: '| `ComplexAbilitiesUsed` | Numeric | Abilities requiring specific targeting
    instructions used per timestamp |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| `ComplexAbilitiesUsed` | 数值 | 每个时间戳使用需要特定目标指令的能力 |'
- en: '| `LeagueIndex` | Numeric | Bronze, Silver, Gold, Platinum, Diamond, Master,
    GrandMaster, and Professional leagues coded 1-8 (output) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| `LeagueIndex` | 数值 | 青铜、白银、黄金、白金、钻石、大师、宗师和职业联赛，编码为1-8（输出） |'
- en: Tip
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If the reader has never played a real-time strategy game like *Starcraft 2*
    on a computer before, it is likely that many of the features used by the dataset
    will sound arcane. If one simply takes on board that these features represent
    various aspects of a player's level of performance in the game, it will still
    be possible to follow all the discussion surrounding the training and testing
    of our regression tree without any difficulty.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果读者之前从未在电脑上玩过像*星际争霸2*这样的实时策略游戏，那么数据集中使用的许多特征可能听起来很神秘。如果一个人只是接受这些特征代表玩家在游戏中的表现各个方面，那么仍然可以毫无困难地跟随关于我们回归树训练和测试的所有讨论。
- en: To start with, we load this dataset onto the data frame `skillcraft`. Before
    beginning to work with the data, we will have to do some preprocessing. Firstly,
    we'll drop the first column. This simply has a unique game identifier that we
    don't need and won't use. Secondly, a quick inspection of the imported data frame
    will show that three columns have been interpreted as factors because the input
    dataset contains a question mark to denote a missing value. To deal with this,
    we first need to convert these columns to numeric columns, a process that will
    introduce missing values in our dataset.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将这个数据集加载到数据框`skillcraft`中。在开始处理数据之前，我们必须做一些预处理。首先，我们将删除第一列。这只是一个唯一的游戏标识符，我们不需要也不会使用。其次，快速检查导入的数据框将显示，有三个列被解释为因子，因为输入数据集包含一个问号来表示缺失值。为了处理这个问题，我们首先需要将这些列转换为数值列，这个过程将在我们的数据集中引入缺失值。
- en: Next, although we've seen that trees are quite capable of handling these missing
    values, we are going to remove the few rows that have them. We will do this because
    we want to be able to compare the performance of several different models in this
    chapter and in the next, not all of which support missing values.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，尽管我们已经看到树可以很好地处理这些缺失值，但我们还是打算删除包含这些值的几行。我们会这样做，因为我们想要能够比较本章和下一章中几个不同模型的性能，并不是所有这些模型都支持缺失值。
- en: 'Here is the code for the preprocessing steps just described:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是上述预处理步骤的代码：
- en: '[PRE12]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As usual, the next step will be to split our data into training and test sets:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，下一步将是将我们的数据分为训练集和测试集：
- en: '[PRE13]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This time, we will use the `rpart` package in order to build our decision tree
    (along with the `tree` package, these two are the most commonly used packages
    for building tree-based models in R). This package provides us with an `rpart()`
    function to build our tree. Just as with the `tree()` function, we can build a
    regression tree using the default behavior by simply providing a formula and our
    data frame:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们将使用`rpart`包来构建我们的决策树（与`tree`包一起，这两个包是R中构建基于树的模型最常用的包）。这个包为我们提供了一个`rpart()`函数来构建我们的树。就像`tree()`函数一样，我们可以通过简单地提供一个公式和我们的数据框来使用默认行为构建一个回归树：
- en: '[PRE14]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can plot our regression tree to see what it looks like:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以绘制我们的回归树来查看其外观：
- en: '[PRE15]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This is the plot that is produced:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成的图表：
- en: '![Predicting complex skill learning](img/00145.jpeg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![预测复杂技能学习](img/00145.jpeg)'
- en: 'To get a sense of the accuracy of our regression tree, we will compute predictions
    on the test data and then measure the SSE. This can be done with the help of a
    simple function that we will define, `compute_SSE()`, which calculates the sum
    of squared error when given a vector of target values and a vector of predicted
    values:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解我们的回归树的准确性，我们将对测试数据进行预测，然后测量SSE。这可以通过我们定义的一个简单函数`compute_SSE()`来完成，它计算给定目标值向量和预测值向量时的平方误差之和：
- en: '[PRE16]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Tuning model parameters in CART trees
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整CART树模型参数
- en: So far, all we have done is use default values for all the parameters of the
    recursive partitioning algorithm for building the tree. The `rpart()` function
    has a special `control` parameter to which we can provide an object containing
    the values of any parameters we wish to override. To build this object, we must
    use the special `rpart.control()` function. There are a number of different parameters
    that we could tweak, and it is worth studying the help file for this function
    to learn more about them.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所做的一切只是为构建树的递归划分算法的所有参数使用默认值。`rpart()`函数有一个特殊的`control`参数，我们可以提供一个包含我们希望覆盖的任何参数值的对象。要构建这个对象，我们必须使用特殊的`rpart.control()`函数。我们可以调整许多不同的参数，研究这个函数的帮助文件以了解更多关于它们的信息。
- en: 'Here we will focus on three important parameters that affect the size and complexity
    of our tree. The `minsplit` parameter holds the minimum number of data points
    that are needed in order for the algorithm to attempt a split before it is forced
    to create a leaf node. The default value is 30\. The `cp` parameter is the complexity
    parameter we have seen before and the default value of this is 0.01\. Finally,
    the `maxdepth` parameter limits the maximum number of nodes between a leaf node
    and the root node. The default value of 30 is quite liberal here, allowing for
    fairly large trees to be built. We can try out a different regression tree by
    specifying some values for these that are different from their default. We''ll
    do this, and see if this affects the SSE performance on our test set:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将关注三个影响我们树的大小和复杂性的重要参数。`minsplit` 参数表示算法在被迫创建叶节点之前尝试分割所需的最小数据点数。默认值是 30。`cp`
    参数是我们之前见过的复杂度参数，其默认值为 0.01。最后，`maxdepth` 参数限制了叶节点和根节点之间的最大节点数。这里的默认值 30 相当宽松，允许构建相当大的树。我们可以通过指定与默认值不同的值来尝试不同的回归树。我们将这样做，看看这会影响测试集上的
    SSE 性能：
- en: '[PRE17]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Using these values we are trying to limit the tree to a depth of 10, while making
    it easier to force a split by needing 20 or more data points at a node. We are
    also lowering the effect of regularization by setting the complexity parameter
    to 0.001\. This is a completely random choice that happens to give us a worse
    SSE value on our test set. In practice, what is needed is a systematic way to
    find appropriate values of these parameters for our tree by trying out a number
    of different combinations and using cross-validation as a way to estimate their
    performance on unseen data.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些值，我们试图将树限制在深度为 10，同时通过在节点上需要 20 个或更多数据点来简化强制分割。我们还通过将复杂度参数设置为 0.001 来降低正则化的影响。这是一个完全随机的选择，不幸的是，它在我们的测试集上给出了更差的
    SSE 值。在实践中，需要一种系统的方法，通过尝试多种不同的组合并使用交叉验证作为评估它们在未见数据上性能的一种方式，来找到适合我们树的这些参数的适当值。
- en: 'Essentially, we would like to tune our regression tree training and in [Chapter
    5](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7 "Chapter 5. Neural
    Networks"), *Support Vector Machines*, we met the `tune()` function inside the
    `e1071` package, which can help us do just that. We will use this function with
    `rpart()` and provide it with ranges for the three parameters we just discussed:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们希望调整我们的回归树训练，在[第 5 章](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7
    "第 5 章。神经网络")中，*支持向量机*，我们遇到了 `e1071` 包内的 `tune()` 函数，它可以帮助我们做到这一点。我们将使用这个函数与 `rpart()`
    一起，并为刚才讨论的三个参数提供范围：
- en: '[PRE18]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Running the preceding tasks will likely take several minutes to complete, as
    there are many combinations of parameters. Once the procedure completes, we can
    train a tree with the suggested values:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的任务可能需要几分钟才能完成，因为有许多参数组合。一旦程序完成，我们可以用建议的值训练一个树：
- en: '[PRE19]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Indeed, we have a lower SSE value with these settings on our test set. If we
    type in the name of our new regression tree model, `regree.tuned`, we'll see that
    we have many more nodes in our tree, which is now substantially more complex.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，在我们的测试集上，这些设置带来了更低的 SSE 值。如果我们输入我们新的回归树模型名称，`regree.tuned`，我们会看到我们的树中有更多的节点，这使得树现在变得更加复杂。
- en: Variable importance in tree models
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 树模型中的变量重要性
- en: For large trees such as this, plotting is less useful as it is very hard to
    make the plot readable. One interesting plot that we can obtain is a plot of **variable
    importance**. For every input feature, we keep track of the reduction in the optimization
    criterion (for example, deviance or SSE) that occurs every time it is used anywhere
    in the tree. We can then tally up this quantity for all the splits in the tree
    and thus obtain relative amounts of variable importance.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对于如此大的树，绘图不太有用，因为很难使绘图可读。我们可以获得的一个有趣的绘图是 **变量重要性** 的绘图。对于每个输入特征，我们跟踪每次它在树中的任何地方被使用时发生的优化标准（例如，偏差或
    SSE）的减少。然后我们可以累计树中所有分割的这个数量，从而获得变量重要性的相对量。
- en: Intuitively, features that are highly important will tend to have been used
    early to split the data (and hence appear higher up in the tree, closer to the
    root node) as well as more often. If a feature is never used, then it is not important
    and in this way we can see that we have a built-in feature selection.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，高度重要的特征往往会很早就被用来分割数据（因此出现在树中较高的位置，接近根节点），以及更频繁地使用。如果一个特征从未被使用过，那么它就不重要，这样我们就可以看到我们有一个内置的特征选择。
- en: 'Note that this approach is sensitive to correlation in the features. When trying
    to determine what feature to split on, we may randomly end up picking between
    two highly correlated features resulting in the model using more features than
    necessary and as a result, the importance of these features is lower than if either
    had been chosen on its own. It turns out that variable importance is automatically
    computed by `rpart()` and stored in the `variable.importance` attribute on the
    tree model that is returned. Plotting this using `barplot()` produces the following:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这种方法对特征之间的相关性很敏感。在尝试确定要分割哪个特征时，我们可能会随机选择两个高度相关的特征，导致模型使用比必要的更多特征，因此这些特征的重要性低于单独选择任何一个。实际上，变量重要性是由`rpart()`自动计算的，并存储在返回的树模型上的`variable.importance`属性中。使用`barplot()`绘制它会产生以下结果：
- en: '![Variable importance in tree models](img/00146.jpeg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![树模型中的变量重要性](img/00146.jpeg)'
- en: To an experienced player of the RTS genre, this graph looks quite reasonable
    and intuitive. The biggest separator of skill according to this graph is the average
    number of game actions that a player makes in a minute (`APM`). Experienced and
    effective players are capable of making many actions, whereas less experienced
    players will make fewer.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RTS类型的经验玩家来说，这个图表看起来相当合理和直观。根据这个图表，技能的最大分隔点是玩家每分钟做出的平均游戏动作数（`APM`）。经验丰富且有效的玩家能够做出很多动作，而经验较少的玩家则会做出较少的动作。
- en: At first glance, this may seem to be simply a matter of acquiring so-called
    muscle memory and developing faster reflexes, but in actuality it is knowing which
    actions to carry out, and playing with strategy and planning during the game (a
    characteristic of better players), which also significantly increases this metric.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，这似乎仅仅是获得所谓的肌肉记忆和培养更快反应速度的问题，但实际上是知道要执行哪些动作，在游戏中进行策略和计划（这是优秀玩家的特征），这也显著提高了这个指标。
- en: Another speed-related attribute is the `ActionLatency` feature, which essentially
    measures the time between choosing to focus the map on a particular location on
    the battlefield and executing the first action at that location. Better players
    will spend less time looking at a map location and will be faster at selecting
    units, giving orders, and deciding what to do given an image of a situation in
    the game.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个与速度相关的属性是`ActionLatency`特征，它本质上衡量的是从选择在战场上关注特定位置到在该位置执行第一个动作之间的时间。更好的玩家在查看地图位置上花费的时间会更少，并且在选择单位、下达命令和根据游戏中的情况图像做出决定方面会更快。
- en: Regression model trees in action
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归模型树在实际应用中
- en: We'll wrap up the experiments in this chapter with a very short demonstration
    of how to run a regression model tree in R, followed by some information around
    the notion of improving the M5 model.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的实验中通过一个非常简短的演示来结束，演示如何在R中运行回归模型树，然后是一些关于改进M5模型概念的信息。
- en: 'We can do this very easily using the `RWeka` package, which contains the `M5P()`
    function. This follows the typical convention of requiring a formula and a data
    frame with the training data:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用包含`M5P()`函数的`RWeka`包非常容易地做到这一点。这遵循了典型的惯例，即需要一个公式和一个包含训练数据的data frame：
- en: '[PRE20]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note that we get almost comparable performance to our tuned CART tree using
    the default settings. We'll leave the readers to explore this function further,
    but we will be revisiting this dataset once again in [Chapter 9](part0071_split_000.html#23MNU2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 9. Ensemble Methods"), *Ensemble Methods*.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用默认设置几乎可以达到与调整过的CART树相当的性能。我们将让读者进一步探索这个函数，但我们将再次在[第9章](part0071_split_000.html#23MNU2-c6198d576bbb4f42b630392bd61137d7
    "第9章。集成方法")中回顾这个数据集，*集成方法*。
- en: Note
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A good reference on regression model trees containing several case studies is
    the original paper by *Quinlan*, entitled *Learning with continuous cases*, from
    the proceedings of the *Australian Joint Conference on Artificial Intelligence*
    (1992).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 关于包含多个案例研究的回归模型树的好参考是Quinlan的原始论文，题为*Learning with continuous cases*，发表于1992年的*澳大利亚联合人工智能会议*。
- en: Improvements to the M5 model
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: M5模型的改进
- en: The standard M5 algorithm tree currently has been received as the most state-of-the-art
    model among decision trees for completing complex regression tasks. This is mainly
    because of the accurate results it yields as well as its ability to handle tasks
    with a very large number of dimensions with upwards of hundreds of attributes.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的M5算法树目前已被接受为在完成复杂回归任务中决策树中最先进的一种模型。这主要是因为它产生的准确结果以及它处理具有数百个属性的大量维度任务的能力。
- en: In an attempt to improve on or otherwise optimize the standard M5 algorithm,
    *M5Flex* has recently been introduced as perhaps the most viable option. The M5Flex
    algorithm approach will attempt to *augment* a standard M5 tree model with *domain
    knowledge*. In other words, M5Flex empowers someone who has familiarity with the
    data population to review and choose the *split attributes* and *split values*
    for those important nodes (within the model tree) with the assumption that, since
    they may "know best," the resulting model will be even more accurate, consistent,
    and appropriate for practical applications than it would be by relying exclusively
    on the standard M5\. One drawback or criticism to using M5Flex is that, in most
    cases, a domain expert may not always be available.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改进或优化标准的M5算法，最近推出了*M5Flex*，可能是最可行的选择。M5Flex算法方法将尝试通过*领域知识*来*增强*标准的M5树模型。换句话说，M5Flex赋予那些熟悉数据集的人审查和选择那些重要节点（在模型树中）的*分割属性*和*分割值*的权力，假设他们“知道得最好”，因此产生的模型将比仅依赖标准M5的模型更加准确、一致，并且更适合实际应用。使用M5Flex的一个缺点或批评是，在大多数情况下，领域专家可能并不总是可用。
- en: 'Still another attempt at improving M5 is **M5opt**. M5opt is a semi-non-greedy
    algorithm utilizing the unassuming approach of not trying to solve your globally
    complex optimization problem holistically, or as "one entity", but rather by splitting
    the procedure of generating the tree layers into two distinct steps, each using
    a different type or nature of algorithm, based upon the layer of the tree:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: M5的另一种改进尝试是**M5opt**。M5opt是一种半非贪婪算法，它采用了一种不试图整体解决全局复杂优化问题的方法，或者不将其视为“一个整体”，而是将生成树层的程序分为两个不同的步骤，每个步骤使用不同类型或性质的算法，这取决于树的层：
- en: '**Global optimization**: Generate upper layers of the tree (from the first
    layer) by using a global (multi-extremum) optimization algorithm (or a better-than-greedy
    approach algorithm).'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**全局优化**：使用全局（多极值）优化算法（或优于贪婪方法的算法）生成树的顶层（从第一层开始）。'
- en: '**Greedy searching**: Generate the rest of the tree (the tree''s lower layers)
    by using a faster "greedy algorithm" like the standard M5.'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**贪婪搜索**：使用类似于标准M5的更快“贪婪算法”生成树的其余部分（树的底层）。'
- en: Additionally, the layer up to which global optimization is applied could be
    different in different branches. However, it would be reasonable to fix it at
    some value for all branches; this allows for a flexible trade-off between speed
    and optimization. Although using the M5opt algorithm to optimize the process of
    constructing tree models has been shown to be successful in yielding models more
    accurate than those created using standard M5, the computational costs will be
    increased due to the nature of how "non-greedy" algorithms work.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，应用全局优化的层在不同分支中可能不同。然而，将所有分支固定在某个值上似乎是合理的；这允许在速度和优化之间进行灵活的权衡。尽管使用M5opt算法优化构建树模型的过程已被证明可以产生比使用标准M5创建的模型更准确的模型，但由于“非贪婪”算法的工作性质，计算成本将会增加。
- en: To address this, one can control the cost by reviewing what tree level is "most
    appropriate," or which level would yield the most accuracy with the least possible
    cost required, and then performing the more exhaustive, non-greedy search at that
    level of the tree.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，可以通过审查哪个树层“最合适”，或者哪个层可以以最小的成本产生最大的准确性来控制成本，然后在树的该层进行更彻底的非贪婪搜索。
- en: Further attempts to optimize standard M5 have been to try to combine both the
    M5opt and the M5flex approaches.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步优化标准M5的尝试包括尝试结合M5opt和M5flex方法。
- en: Finally, **artificial neural networks** (**ANN's**) discussed in [Chapter 5](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 5. Neural Networks"), *Neural Networks* have been offered as an alternative
    to standard M5, but only in such scenarios where the tree model is presumed to
    be less complex. In complex models, M5 almost always outperforms ANN.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在[第5章](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7 "第5章。神经网络")中讨论的**人工神经网络**（**ANNs**），作为一种替代标准M5的方法被提出，但仅限于那些假设树模型较为简单的情况下。在复杂模型中，M5几乎总是优于ANN。
- en: Summary
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we learned how to build decision trees for regression and classification
    tasks. We saw that, although the idea is simple, there are several decisions that
    we have to make in order to construct our tree model, such as what splitting criterion
    to use, as well as when and how to prune our final tree.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何构建用于回归和分类任务的决策树。我们了解到，尽管这个想法很简单，但在构建我们的树模型时，我们仍需做出几个决定，例如选择何种分割标准，以及何时以及如何修剪我们的最终树。
- en: In each case, we considered a number of viable options and it turns out that
    there are several algorithms that are used to build decision tree models. Some
    of the best qualities of decision trees are the fact that they are typically easy
    to implement and very easy to interpret, while making no assumptions about the
    underlying model of the data. Decision trees have native options for performing
    feature selection and handling missing data, and are very capable of handling
    a wide range of feature types.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在每种情况下，我们都考虑了多种可行的选项，结果发现，有几种算法被用来构建决策树模型。决策树的一些最佳特性是它们通常易于实现和解释，同时不对数据的潜在模型做出假设。决策树具有原生选项来执行特征选择和处理缺失数据，并且能够处理广泛的特征类型。
- en: Having said that, we saw that, from a computational perspective, finding a split
    for categorical variables is quite expensive due to the exponential growth of
    the number of possible splits. In addition, we saw that categorical features can
    often tend to impose selection bias in splitting criteria such as information
    gain, because of this large number of potential splits.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们从计算的角度看到，由于可能分割数量的指数增长，找到分类变量的分割点相当昂贵。此外，我们还看到，由于潜在分割点数量众多，分类特征往往倾向于在信息增益等分割标准中引入选择偏差。
- en: Another drawback to using decision tree models is the fact that they can be
    unstable in the sense that small changes in the data can potentially alter a splitting
    decision high up in the tree, and consequently we can end up with a very different
    tree after that. Additionally, and this is particularly relevant to regression
    problems, because of the finite number of leaf nodes, our model may not be sufficiently
    granular in its output. Finally, although there are several different approaches
    to pruning, we should note that decision trees can be vulnerable to overfitting.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策树模型的另一个缺点是它们可能不稳定，这意味着数据中的微小变化可能会改变树中较高位置的分割决策，结果我们可能会得到一个完全不同的树。此外，特别是对于回归问题而言，由于叶节点数量有限，我们的模型在输出上可能不够细致。最后，尽管有几种不同的剪枝方法，但我们应注意到决策树可能容易过拟合。
- en: In the next chapter, we are not going to focus on a new type of model. Instead,
    we are going to look at different techniques to combine multiple models together,
    such as bagging and boosting. Collectively, these are known as ensemble methods.
    These methods have been demonstrated to be quite effective in improving the performance
    of simpler models, and overcoming some of the limitations just discussed for tree-based
    models, such as model instability and susceptibility to overfitting.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们不会关注新的模型类型。相反，我们将探讨不同的技术来组合多个模型，例如袋装法和提升法。这些方法统称为集成方法。这些方法已被证明在提高简单模型的性能和克服前面讨论的基于树的模型的局限性（如模型不稳定和易过拟合）方面非常有效。
- en: We'll present a well-known algorithm, AdaBoost, which can be used with a number
    of models that we've seen so far. In addition, we will also introduce random forests,
    as a special type of ensemble model specifically designed for decision trees.
    Ensemble methods in general are typically not easy to interpret, but for random
    forests we can still use the notion of variable importance that we saw in this
    chapter in order to get an overall idea of which features our model relies upon
    the most.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍一个著名的算法，AdaBoost，它可以与迄今为止我们所看到的一些模型一起使用。此外，我们还将介绍随机森林，作为一种专门为决策树设计的特殊集成模型。一般来说，集成方法通常不易解释，但对于随机森林，我们仍然可以使用本章中看到的变量重要性的概念，以便对模型最依赖哪些特征有一个整体的认识。
