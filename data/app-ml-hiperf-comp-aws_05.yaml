- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Data Analysis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分析
- en: One of the fundamental principles behind any large-scale data science procedure
    is the simple fact that any **Machine Learning** (**ML**) model produced is only
    as good as the data on which it is trained. Beginner data scientists often make
    the mistake of assuming that they just need to find the right ML model for their
    use case and then simply train or fit the data to the model. However, nothing
    could be further from the truth. Getting the best possible model requires exploring
    the data, with the goal being to fully understand the data. Once the data scientist
    understands the data and how the ML model can be trained on it, the data scientist
    often spends most of their time further cleaning and modifying the data, also
    referred to as wrangling the data, to prepare it for model training and building.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 任何大规模数据科学程序背后的一个基本原理是这样一个简单的事实：任何**机器学习**（**ML**）模型产生的质量仅与其训练数据相当。初学者数据科学家常常犯的错误是认为他们只需要找到适合他们用例的正确
    ML 模型，然后简单地训练或拟合数据到模型中。然而，事实并非如此。获得最佳模型需要探索数据，目标是完全理解数据。一旦数据科学家理解了数据以及如何在该数据上训练
    ML 模型，数据科学家通常会花费大部分时间进一步清理和修改数据，这通常被称为数据整理，以准备模型训练和构建。
- en: While this data analysis task may seem conceptually straightforward, the task
    becomes far more complicated when we factor in the *type* (images, text, tabular,
    and so on) and the *amount*/*volume* of data we are exploring. Furthermore, where
    the data is stored and getting access to it can also make the exercise even more
    overwhelming for the data scientist. For example, useful ML data may be stored
    within a data warehouse or located within various relational databases, often
    requiring various tools or programmatic API calls to mine the right data. Likewise,
    key information may be located across multiple file servers or within various
    buckets of a cloud-based object store. Locating the data and ensuring the correct
    permissions to access the data can further delay a data scientist from getting
    started.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个数据分析任务在概念上可能看起来很简单，但当我们考虑到我们正在探索的数据的类型（图像、文本、表格等）以及数据的数量/体积时，任务会变得更加复杂。此外，数据的存储位置以及获取数据的权限也可能使数据科学家的工作更加困难。例如，有用的机器学习数据可能存储在数据仓库中，或者位于各种关系型数据库中，通常需要各种工具或程序性
    API 调用来挖掘正确的数据。同样，关键信息可能分布在多个文件服务器或云对象存储的各个存储桶中。定位数据并确保正确访问数据的权限可能会进一步延迟数据科学家开始工作。
- en: So, with these challenges in mind, in this chapter, we will review some practical
    ways to explore, understand, and essentially wrangle different types, as well
    as large quantities of data, to train ML models. Additionally, we will examine
    some of the capabilities and services that AWS provides to make this task less
    daunting.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，考虑到这些挑战，在本章中，我们将回顾一些实际的方法来探索、理解和本质上处理不同类型，以及大量数据，以训练机器学习模型。此外，我们还将检查 AWS
    提供的一些功能和服务，以使这项任务不那么令人畏惧。
- en: 'Therefore, this chapter will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本章将涵盖以下主题：
- en: Exploring data analysis methods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索数据分析方法
- en: Reviewing AWS services for data analysis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查 AWS 服务用于数据分析
- en: Analyzing large amounts of structured and unstructured data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析大量结构化和非结构化数据
- en: Processing data at scale on AWS
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 AWS 上大规模处理数据
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You should have the following prerequisites before getting started with this
    chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本章之前，您应该具备以下先决条件：
- en: Familiarity with AWS services and their basic usage.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉 AWS 服务及其基本用法。
- en: A web browser (for the best experience, it is recommended that you use a Chrome
    or Firefox browser).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个网络浏览器（为了获得最佳体验，建议您使用 Chrome 或 Firefox 浏览器）。
- en: 'An AWS account (if you are unfamiliar with how to get started with an AWS account,
    you can go to this link: [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 AWS 账户（如果您不熟悉如何开始使用 AWS 账户，您可以访问此链接：[https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/))。
- en: 'Familiarity with the AWS Free Tier (the Free Tier will allow you to access
    some of the AWS services for free, depending on resource limits. You can familiarize
    yourself with these limits at this link: [https://aws.amazon.com/free/](https://aws.amazon.com/free/)).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉 AWS 免费层（免费层允许您根据资源限制免费访问一些 AWS 服务。您可以通过此链接了解这些限制：[https://aws.amazon.com/free/](https://aws.amazon.com/free/))。
- en: Example Jupyter notebooks for this chapter are provided in the companion GitHub
    repository ([https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter05](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter05)).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的示例Jupyter笔记本可以在配套的GitHub仓库中找到（[https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter05](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter05))。
- en: Exploring data analysis methods
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据分析方法
- en: As highlighted at the outset of this chapter, the task of gathering and exploring
    these various sources of data can seem somewhat daunting. So, you may be wondering
    at this point *where and how to begin the data analysis process?* To answer this
    question, let’s explore some of the methods we can use to analyze your data and
    prepare it for the ML task.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章开头所强调的，收集和探索这些各种数据来源的任务可能显得有些令人畏惧。因此，你可能此时正在想*在哪里以及如何开始数据分析过程？*为了回答这个问题，让我们探讨一些我们可以用来分析你的数据并为机器学习任务做准备的方法。
- en: Gathering the data
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集数据
- en: One of the first steps to getting started with a data analysis task is to gather
    the relevant data from various silos into a specific location. This single location
    is commonly referred to as a data lake. Once the relevant data has been co-located
    into a single data lake, the activity of moving data in or out of the lake becomes
    significantly easier.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 开始数据分析任务的第一步之一是将来自各个孤岛的相关数据收集到一个特定的位置。这个单一的位置通常被称为数据湖。一旦相关数据被集中到一个单一的数据湖中，在湖中移动数据进出就变得显著更容易。
- en: For example, let’s imagine for a moment that a data scientist is tasked with
    building a product recommendation model. Using the data lake as a central store,
    they can query a customer database to get all the customer-specific data, typically
    from a relational database or a data warehouse, and then combine the customer’s
    clickstream data from the web application’s transaction logs to get a common source
    of all the required information to predict product recommendations. Moreover,
    by sourcing product-specific image data from the product catalog, the data scientist
    can further explore the various characteristics of product images that may enhance
    or contribute to the ML model’s predictive potential.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们暂时想象一下，一个数据科学家被分配了一个构建产品推荐模型的任务。利用数据湖作为中心存储，他们可以查询客户数据库以获取所有客户特定的数据，通常来自关系型数据库或数据仓库，然后结合来自Web应用程序事务日志的客户点击流数据，以获取预测产品推荐所需的所有信息的共同来源。此外，通过从产品目录中获取产品特定的图像数据，数据科学家可以进一步探索可能增强或有助于机器学习模型预测潜力的产品图像的各种特征。
- en: So, once that holistic dataset is gathered together and stored in a common repository
    or data store, we can move on to the next approach to data analysis, which is
    understanding the data structure.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一旦收集了完整的数据集并将其存储在共同的仓库或数据存储中，我们就可以继续进行数据分析的下一个步骤，即理解数据结构。
- en: Understanding the data structure
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数据结构
- en: Once the data has been gathered into a common location, before a data scientist
    can fully investigate how it can be used to suggest an ML hypothesis, we need
    to understand the structure of the data. Since the dataset may be created from
    multiple sources, understanding the structure of the data is important before
    it can be analyzed effectively.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被收集到一个共同的位置，在数据科学家能够完全调查如何使用它来提出机器学习假设之前，我们需要了解数据的结构。由于数据集可能来自多个来源，因此在有效分析之前理解数据的结构是很重要的。
- en: For instance, if we continue with the product recommendation example, the data
    scientist may work with structured customer data in the form of a tabular dataset
    from a relational database or data warehouse. Added to this, when pulling the
    customer interaction data from the web servers, the data scientist may work with
    time series or JSON formatted data, commonly referred to as semi-structured data.
    Lastly, when incorporating product images into the mix, the data scientist will
    deal with image data, which is an example of unstructured data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们继续以产品推荐为例，数据科学家可能将与来自关系型数据库或数据仓库的表格数据形式的结构化客户数据一起工作。此外，当从Web服务器中提取客户交互数据时，数据科学家可能需要处理时间序列或JSON格式数据，通常被称为半结构化数据。最后，当将产品图像纳入其中时，数据科学家将处理图像数据，这是非结构化数据的一个例子。
- en: So, understanding the nature or structure of the data determines how to extract
    the critical information we need and analyze it effectively. Moreover, knowing
    the type of data we’re dealing with will also influence the type of tools, such
    as **Application Programming Interfaces** (**APIs**), and even the infrastructure
    resources required to understand data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，理解数据的本质或结构决定了如何提取我们需要的临界信息并有效地进行分析。此外，了解我们处理的数据类型也将影响所需的工具类型，例如**应用程序编程接口**（**APIs**），甚至理解数据所需的基础设施资源。
- en: Note
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We will be exploring these tools and infrastructure resources in depth further
    on in the chapter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的后续部分深入探讨这些工具和基础设施资源。
- en: Once we understand the data structure, we can apply this understanding to another
    technique of data analysis, that is, describing the data itself.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们理解了数据结构，我们就可以将这种理解应用到数据分析的另一种技术中，即描述数据本身。
- en: Describing the data
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 描述数据
- en: Once we understand the data’s structure, we can describe or summarize the characteristics
    of the data to further explore how these characteristics influence our overall
    hypothesis. This methodology is commonly referred to as applying **descriptive
    statistics** to the data, whereby a data scientist will try to describe and understand
    the various features of the dataset by summarizing the collective properties of
    each feature within the data in terms of centrality, variability, and data counts.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们理解了数据结构，我们就可以描述或总结数据的特征，以进一步探索这些特征如何影响我们的整体假设。这种方法通常被称为将**描述性统计**应用于数据，其中数据科学家将通过总结数据中每个特征的集体属性来尝试描述和理解数据集中的各种特征，这些属性以中心性、变异性和数据计数的形式表示。
- en: Let’s explore what each of these terms means to see how they can be used to
    describe the data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索这些术语的含义，看看它们如何被用来描述数据。
- en: Determining central tendency
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确定中心趋势
- en: By using descriptive statistics to summarize the central tendency of the data,
    we are essentially focusing on the average, middle, or center position within
    the distribution of a specific feature of the dataset. This gives the data scientist
    an idea of what is normal or average about a feature of the dataset, allowing
    them to compare these averages with other features of the data or even the entirety
    of the data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用描述性统计来总结数据的中心趋势，我们实际上是在关注数据集中特定特征的分布中的平均、中间或中心位置。这使数据科学家能够了解数据集中某个特征的正常或平均情况，从而允许他们将这些平均值与其他数据特征或整个数据进行比较。
- en: For example, let’s say that customer A visited our website 10 times a day but
    only purchased 1 item. By comparing the average visits of customer A with the
    total number of customer visits, we can see how customer A ranks in comparison.
    If we then compare the number of items purchased with the total number of items,
    we can further gauge what is considered normal based on the customer’s ranking.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设客户A每天访问我们的网站10次，但只购买了一件商品。通过比较客户A的平均访问次数与总客户访问次数，我们可以看到客户A的排名情况。如果我们再比较购买的商品数量与总商品数量，我们可以进一步衡量基于客户排名的正常情况。
- en: Measuring variability
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测量变异
- en: Using descriptive statistics to measure the variability of the data or how the
    data is spread is extremely important in ML. Understanding how the data is distributed
    will give the data scientist a good idea of whether the data is proportional or
    not. For example, in the product recommendation use case where we have data with
    a greater spread of customers who purchase books versus customers who purchase
    lawnmowers. In this case, when a model is trained on this data, it will be biased
    toward recommending books over lawnmowers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用描述性统计来衡量数据的变异或数据分布情况在机器学习中非常重要。了解数据的分布将使数据科学家能够很好地了解数据是否成比例。例如，在产品推荐用例中，我们有数据表明购买书籍的客户比购买割草机的客户分布更广。在这种情况下，当模型基于这些数据进行训练时，它将倾向于推荐书籍而不是割草机。
- en: Counting the data points
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算数据点
- en: Not to be confused with dataset sizes, dataset counts refer to the number or
    quantity of individual data points within the dataset. Summarizing the number
    of data points for each feature within the dataset can further help the data scientist
    to verify whether they have an adequate number of data points or observations
    for each feature. Having sufficient quantities of data points will further help
    to justify the overall hypothesis.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 不要与数据集大小混淆，数据集计数指的是数据集中单个数据点的数量或数量。总结数据集中每个特征的数据点数量可以进一步帮助数据科学家验证他们是否为每个特征有足够数量的数据点或观测值。拥有足够数量的数据点将进一步帮助证明整体假设。
- en: Additionally, by comparing the individual quantities of data points for each
    feature, the data scientist can determine whether there are any missing data points.
    Since the majority of ML algorithms don’t deal well with missing data, the data
    scientist can circumvent any unnecessary issues during the model training process
    by dealing with these missing values during the data analysis process.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过比较每个特征的个别数据点的数量，数据科学家可以确定是否存在任何缺失数据点。由于大多数机器学习算法处理缺失数据的能力不佳，数据科学家可以在数据分析过程中处理这些缺失值，从而在模型训练过程中避免任何不必要的麻烦。
- en: Note
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: While these previously shown descriptive techniques can help us understand the
    characteristics of the data, a separate branch of statistics, called **inferential
    statistics**, is also required to measure and understand how features interact
    with one another within the entire dataset. This factor is important when dealing
    with large quantities of data where inferential techniques will need to be applied
    if we don’t have a mechanism to analyze large datasets at scale.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然之前展示的描述性技术可以帮助我们理解数据的特征，但还需要一个名为**推断统计**的独立统计学分支来衡量和理解特征在整个数据集中如何相互作用。当处理大量数据时，这一点尤为重要，如果没有机制来大规模分析大数据集，就需要应用推断技术。
- en: We’ve all heard the saying that *a picture paints a thousand words*. So, once
    we have a good understanding of the dataset’s characteristics, another important
    data analysis technique is to visualize these characteristics.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都听说过“一图胜千言”的说法。因此，一旦我们充分理解了数据集的特征，另一个重要的数据分析技术就是可视化这些特征。
- en: Visualizing the data
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据可视化
- en: While summarizing the characteristics of the data provides useful information
    to the data scientist, we are essentially adding more data to the analysis task.
    Plotting or charting this additional information can potentially reveal further
    characteristics of the data that summary and inferential statistics may miss.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然总结数据的特征为数据科学家提供了有用的信息，但我们实际上是在分析任务中添加更多数据。绘制或图表化这些附加信息可能会揭示数据的一些特征，这些特征在总结和推断统计中可能会被忽略。
- en: For example, using visualization to understand the variance and spread of the
    data points, a data scientist may use a bar chart to group the various data points
    into *bins* with equal ranges to visualize the distribution of data points in
    each *bin*. Furthermore, by using a boxplot, a data scientist can visualize whether
    there are any outlying data points that influence the overall distribution of
    the data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用可视化来理解数据点的方差和分布，数据科学家可能会使用条形图将各种数据点分组到具有相等范围的*桶*中，以可视化每个*桶*中数据点的分布。此外，通过使用箱线图，数据科学家可以可视化是否存在任何影响数据整体分布的异常数据点。
- en: Depending on the type of data and structure of the dataset, many different types
    of plots and charts can be used to visualize the data. It is outside the scope
    of this chapter to dive into each and every type of plot available and how it
    can be used. However, it is sufficient to say that data visualization is an essential
    methodology for exploratory data analysis to verify data quality and help the
    data scientist become more familiar with the structure and characteristics of
    the dataset.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据类型和数据集的结构，可以使用许多不同类型的图表和图形来可视化数据。本章的范围不包括深入探讨每种可用的图表类型及其使用方法。然而，可以说数据可视化是探索性数据分析的一个基本方法，用于验证数据质量并帮助数据科学家更熟悉数据集的结构和特征。
- en: Reviewing the data analytics life cycle
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 审查数据分析生命周期
- en: 'While there are many other data analysis methodologies, of which we have only
    touched on four, we can summarize the overall data analysis methodology in the
    following steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多其他的数据分析方法，我们只涉及了四种，但我们可以在以下步骤中总结整体数据分析方法：
- en: Identify the use case and questions that need to be answered from the data,
    plus the features the ML model needs to predict.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定需要从数据中回答的用例和问题，以及机器学习模型需要预测的特征。
- en: Gather or mine the data into a common data store.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集或挖掘数据到一个公共数据存储库中。
- en: Explore and describe the data.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索并描述数据。
- en: Visualize the data.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化数据。
- en: Clean the data and prepare it for model training, plus account for any missing
    data.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清洗数据并准备用于模型训练，同时考虑任何缺失的数据。
- en: Engineer new features to enhance the hypothesis and improve the ML model’s predictive
    capability.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设计新的特征以增强假设并提高机器学习模型的预测能力。
- en: Rinse and repeat to ensure that the data, as well as the ML model, addresses
    the business use case.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复上述步骤以确保数据以及机器学习模型满足业务用例。
- en: Now that we have reviewed some of the important data analysis methodologies
    and the analysis life cycle, let’s review some of the capabilities and services
    that AWS provides to apply these techniques, especially at scale.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了一些重要的数据分析方法和分析生命周期，让我们来回顾一下 AWS 提供的一些能力和服务，特别是用于大规模应用这些技术的情况。
- en: Reviewing the AWS services for data analysis
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查看用于数据分析的 AWS 服务
- en: AWS provides multiple services that are geared to help the data scientist analyze
    either structured, semi-structured, or unstructured data at scale. A common style
    across all these services is to provide users with the flexibility of choice to
    match the right aspects of each service as it applies to the use case. At times,
    it may seem confusing to the user which service to leverage for their use case.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 提供了多项服务，旨在帮助数据科学家在规模上分析结构化、半结构化或非结构化数据。所有这些服务的一个共同风格是向用户提供选择的灵活性，以便根据用例匹配每个服务的适当方面。有时，用户可能会对选择哪个服务来满足他们的用例感到困惑。
- en: Thus, in this section, we will map some of the AWS capabilities to the methodologies
    we’ve reviewed in the previous section.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本节中，我们将把一些 AWS 能力映射到我们在上一节中回顾的方法。
- en: Unifying the data into a common store
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据统一到一个公共存储库中
- en: To address the requirement of storing the relevant global population of data
    from multiple sources in a common store, AWS provides the Amazon **Simple Storage
    Service** (**S3**) object storage service, allowing users to store structured,
    semi-structured, and unstructured data as objects within buckets.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足在公共存储库中存储来自多个来源的相关全球数据集的需求，AWS 提供了 Amazon **简单存储服务**（**S3**）对象存储服务，允许用户在存储桶中将结构化、半结构化和非结构化数据作为对象存储。
- en: Note
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: 'If you are unfamiliar with the S3 service, how it works, and how to use it,
    you can review the S3 product page here: [https://aws.amazon.com/s3/](https://aws.amazon.com/s3/).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不太熟悉 S3 服务，它的运作方式以及如何使用它，你可以在此处查看 S3 产品页面：[https://aws.amazon.com/s3/](https://aws.amazon.com/s3/)。
- en: Consequently, S3 is the best place to create a data lake as it has unrivaled
    security, availability, and scalability. Incidentally, S3 also provides multiple
    additional resources to bring data into the store.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，S3 是创建数据湖的最佳位置，因为它具有无与伦比的安全性、可用性和可伸缩性。顺便提一下，S3 还提供了多种额外的资源，以便将数据引入存储库。
- en: However, setting up and managing data lakes can be time-consuming and intricate,
    and may take up to several weeks to set it up, based on your requirements. It
    often requires loading data from multiple different sources, setting up partitions,
    enabling encryption, and providing auditable access. Subsequently, AWS provides
    **AWS Lake Formation** ([https://aws.amazon.com/lake-formation](https://aws.amazon.com/lake-formation))
    to build secure data lakes in mere days.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，设置和管理数据湖可能既耗时又复杂，根据您的需求，设置可能需要几周时间。通常需要从多个不同的来源加载数据，设置分区，启用加密，并提供可审计的访问。随后，AWS
    提供了 **AWS Lake Formation**（[https://aws.amazon.com/lake-formation](https://aws.amazon.com/lake-formation)）服务，只需几天即可构建安全的数据湖。
- en: Creating a data structure for analysis
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建用于分析的数据结构
- en: As was highlighted in the previous section, understanding the underlying structure
    of our data is critical to extracting the key information needed for analysis.
    So, once our data is stored in S3, we can leverage **Amazon Athena** ([https://aws.amazon.com/athena](https://aws.amazon.com/athena))
    using **Structured Query Language** (**SQL**), or leverage **Amazon EMR** ([https://aws.amazon.com/emr/](https://aws.amazon.com/emr/))
    to analyze large-scale data, using open source tooling, such as **Apache Spark**
    ([https://spark.apache.org/](https://spark.apache.org/)) and the **PySpark** ([https://spark.apache.org/docs/latest/api/python/index.html?highlight=pyspark](https://spark.apache.org/docs/latest/api/python/index.html?highlight=pyspark))
    Python interface. Let’s explore these analytics services further by starting with
    an overview of Amazon Athena.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，了解我们数据的基础结构对于提取分析所需的关键信息至关重要。因此，一旦我们的数据存储在S3中，我们就可以使用**Amazon Athena**
    ([https://aws.amazon.com/athena](https://aws.amazon.com/athena))通过**结构化查询语言**（**SQL**）进行查询，或者使用**Amazon
    EMR** ([https://aws.amazon.com/emr/](https://aws.amazon.com/emr/))来分析大规模数据，使用开源工具，如**Apache
    Spark** ([https://spark.apache.org/](https://spark.apache.org/))和**PySpark** ([https://spark.apache.org/docs/latest/api/python/index.html?highlight=pyspark](https://spark.apache.org/docs/latest/api/python/index.html?highlight=pyspark))
    Python接口。让我们通过从Amazon Athena的概述开始进一步探索这些分析服务。
- en: Reviewing Amazon Athena
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查Amazon Athena
- en: Athena makes it easy to define a schema, a conceptual design of the data structure,
    and query the structured or semi-structured data in S3 using SQL, making it easy
    for the data scientist to obtain key information for analysis on large datasets.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Athena使得定义模式变得容易，这是一种数据结构的概念性设计，并使用SQL在S3中查询结构化或半结构化数据，这使得数据科学家能够轻松地获取分析大型数据集所需的关键信息。
- en: One critical aspect of Athena is the fact that it is **serverless**. This is
    of key importance to data scientists because there are no requirements for building
    and managing infrastructure resources. This means that data scientists immediately
    start their analysis tasks without needing to rely on a platform or infrastructure
    team to develop and build an analytics architecture.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Athena的一个关键方面是它实际上是**无服务器的**。这对数据科学家来说非常重要，因为不需要构建和管理基础设施资源。这意味着数据科学家可以立即开始他们的分析任务，而无需依赖平台或基础设施团队来开发和构建分析架构。
- en: However, the expertise to perform SQL queries may or may not be within a data
    scientist’s wheelhouse since the majority of practitioners are more familiar with
    Python data analysis tools, such as **pandas** ([https://pandas.pydata.org/](https://pandas.pydata.org/)).
    This is where Spark and EMR come in. Let’s review how Amazon EMR can help.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，执行SQL查询的技能可能或可能不在数据科学家的技能范围内，因为大多数从业者更熟悉Python数据分析工具，例如**pandas** ([https://pandas.pydata.org/](https://pandas.pydata.org/))。这正是Spark和EMR发挥作用的地方。让我们回顾一下Amazon
    EMR如何提供帮助。
- en: Reviewing Amazon EMR
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查Amazon EMR
- en: Amazon **EMR** or **Elastic MapReduce** is essentially a managed infrastructure
    provided by AWS, on which you can run Apache Spark. Since it’s a managed service,
    EMR allows the infrastructure team to easily provision, manage, and automatically
    scale large Spark clusters, allowing data scientists to run petabyte-scale analytics
    on their data using tools they are familiar with.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon **EMR**或**弹性MapReduce**本质上是由AWS提供的一种托管基础设施，您可以在其上运行Apache Spark。由于它是一种托管服务，EMR允许基础设施团队轻松配置、管理和自动扩展大型Spark集群，使数据科学家能够使用他们熟悉的工具在数据上运行PB级规模的统计分析。
- en: There are two key points to be aware of when leveraging EMR and Spark for data
    analysis. Firstly, unlike Athena, EMR is not serverless and requires an infrastructure
    team to provision and manage a cluster of EMR nodes. While these tasks have been
    automated when using EMR, taking between 15 to 20 minutes to provision a cluster,
    the fact still remains that these infrastructure resources require a build-out
    before the data scientist can leverage them.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在利用EMR和Spark进行数据分析时，有两个关键点需要注意。首先，与Athena不同，EMR不是无服务器架构，需要基础设施团队来配置和管理一组EMR节点。虽然使用EMR时这些任务已经自动化，配置一个集群大约需要15到20分钟，但事实仍然如此，这些基础设施资源在数据科学家可以利用它们之前需要先进行搭建。
- en: Secondly, EMR with Spark makes use of **Resilient Distributed Datasets** (**RDDs**)
    ([https://spark.apache.org/docs/3.2.1/rdd-programming-guide.html#resilient-distributed-datasets-rdds](https://spark.apache.org/docs/3.2.1/rdd-programming-guide.html#resilient-distributed-datasets-rdds))
    to perform petabyte-scale data analysis by alleviating the memory limitations
    often imposed when using pandas. Essentially, this allows the data scientist to
    perform analysis tasks on the entire population of data as opposed to extracting
    a small enough sample to fit into memory, performing the descriptive analysis
    tasks on the said sample, and then inferring the analysis back onto the global
    population. Having the ability to execute an analysis of all of the data in a
    single step can significantly reduce the time taken for the data scientist to
    describe and understand the data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，EMR与Spark结合使用**弹性分布式数据集**（**RDDs**）([https://spark.apache.org/docs/3.2.1/rdd-programming-guide.html#resilient-distributed-datasets-rdds](https://spark.apache.org/docs/3.2.1/rdd-programming-guide.html#resilient-distributed-datasets-rdds))，通过缓解使用pandas时通常施加的内存限制，来执行PB级数据分析。本质上，这允许数据科学家对整个数据集进行分析任务，而不是提取足够小的样本以适应内存，在所述样本上执行描述性分析任务，然后将分析推断回全局人口。能够在单步中执行对全部数据的分析可以显著减少数据科学家描述和理解数据所需的时间。
- en: Visualizing the data at scale
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在大规模上可视化数据
- en: As if ingesting and analyzing large-scale datasets isn’t complicated enough
    for a data scientist, using programmatic visualization libraries such as Matplotlib
    ([https://matplotlib.org/](https://matplotlib.org/)) and Seaborn (https://seaborn.pydata.org/)
    can further complicate the analysis task.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据科学家来说，处理和分析大规模数据集已经足够复杂，而使用程序化可视化库，如Matplotlib ([https://matplotlib.org/](https://matplotlib.org/))
    和 Seaborn (https://seaborn.pydata.org/) )，则进一步增加了分析任务的复杂性。
- en: So, in order to assist data scientists in visualizing data and gaining additional
    insights plus performing both descriptive and inferential statistics, AWS provides
    the **Amazon QuickSight** ([https://aws.amazon.com/quicksight/](https://aws.amazon.com/quicksight/))
    service. QuickSight allows data scientists to connect to their data on S3, as
    well as other data sources, to create interactive charts and plots.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了帮助数据科学家可视化数据、获得额外的洞察力以及执行描述性和推断性统计，AWS提供了**Amazon QuickSight** ([https://aws.amazon.com/quicksight/](https://aws.amazon.com/quicksight/))服务。QuickSight允许数据科学家连接到其S3上的数据以及其他数据源，以创建交互式图表和图形。
- en: Furthermore, leveraging QuickSight for data visualization tasks does not require
    the data scientist to rely on their infrastructure teams to provision resources
    as QuickSight is also serverless.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，利用QuickSight进行数据可视化任务不需要数据科学家依赖他们的基础设施团队来提供资源，因为QuickSight也是无服务器的。
- en: Choosing the right AWS service
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择正确的AWS服务
- en: As you can imagine, AWS provides many more services and capabilities for large-scale
    data analysis, with S3, Athena, and QuickSight being only a few of the more common
    technologies that specifically focus on data analytics tasks. Choosing the right
    capability is dependent on the use case and may require integrating other infrastructure
    resources. The key takeaway from this brief introduction to these services is
    that, where possible, data scientists should not be burned by having to manage
    resources outside of the already complicated task of data analysis.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所想，AWS为大规模数据分析提供了许多更多服务和功能，其中S3、Athena和QuickSight只是专注于数据分析任务的更常见技术中的少数几种。选择正确的功能取决于用例，可能需要集成其他基础设施资源。从这个对这些服务的简要介绍中，我们可以得出的关键结论是，在可能的情况下，数据科学家不应该因为不得不管理数据分析这一复杂任务之外的资源而遭受损失。
- en: Therefore, from the perspective of the data scientist or ML practitioner, AWS
    provides a dedicated service with capabilities specifically dedicated to common
    ML tasks called **Amazon** **SageMaker** ([https://aws.amazon.com/sagemaker/](https://aws.amazon.com/sagemaker/)).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从数据科学家或机器学习实践者的角度来看，AWS提供了一个专门的服务，该服务具有专门针对常见机器学习任务的**Amazon SageMaker**
    ([https://aws.amazon.com/sagemaker/](https://aws.amazon.com/sagemaker/))功能。
- en: Therefore, in the next section, we will demonstrate how SageMaker can help with
    analyzing large-scale data for ML without the data scientist having to personally
    manage or rely on infrastructure teams to manage resources.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在下一节中，我们将展示SageMaker如何帮助分析大规模数据，以便数据科学家无需亲自管理或依赖基础设施团队来管理资源。
- en: Analyzing large amounts of structured and unstructured data
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析大量结构化和非结构化数据
- en: Up until this point in the chapter, we have reviewed some of the typical methods
    for large-scale data analysis and introduced some of the key AWS services that
    focus on making the analysis task easier for users. In this section, we will practically
    introduce Amazon SageMaker as a comprehensive service that allows both the novice
    as well as the experienced ML practitioner to perform these data analysis tasks.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经回顾了一些典型的大规模数据分析方法，并介绍了一些专注于简化用户分析任务的 AWS 服务。在本节中，我们将实际介绍 Amazon SageMaker
    作为一项综合服务，它允许新手和经验丰富的 ML 实践者执行这些数据分析任务。
- en: While SageMaker is a fully managed infrastructure provided by AWS along with
    tools and workflows that cater to each step of the ML process, it also offers
    a fully **Integrated Development Environment** (**IDE**) specifically for ML development
    called **Amazon SageMaker Studio** ([https://aws.amazon.com/sagemaker/studio/](https://aws.amazon.com/sagemaker/studio/)).
    SageMaker Studio provides a data scientist with the capabilities to develop, manage,
    and view each part of the ML life cycle, including exploratory data analysis.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 SageMaker 是 AWS 提供的完全托管的基础设施，包括满足 ML 流程每个步骤的工具和工作流程，但它还提供了一个专门用于 ML 开发的完全**集成开发环境**（**IDE**），称为**Amazon
    SageMaker Studio**（[https://aws.amazon.com/sagemaker/studio/](https://aws.amazon.com/sagemaker/studio/)）。SageMaker
    Studio 为数据科学家提供开发、管理和查看 ML 生命周期每个部分的能力，包括探索性数据分析。
- en: But, before jumping into a hands-on example where we can perform large-scale
    data analysis using Studio, we need to configure a SageMaker domain. A SageMaker
    Studio domain comprises a set of authorized data scientists, pre-built data science
    tools, and security guard rails. Within the domain, these users can share access
    to AWS analysis services, ML experiment data, visualizations, and Jupyter notebooks.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在我们进行一个实际示例之前，该示例展示了如何使用 Studio 进行大规模数据分析，我们需要配置一个 SageMaker 域。SageMaker
    Studio 域包括一组授权的数据科学家、预构建的数据科学工具和安全防护栏。在域内，这些用户可以共享访问 AWS 分析服务、ML 实验数据、可视化和 Jupyter
    笔记本。
- en: Let’s get started.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Setting up EMR and SageMaker Studio
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 EMR 和 SageMaker Studio
- en: 'We will use an **AWS CloudFormation** ([https://aws.amazon.com/cloudformation/](https://aws.amazon.com/cloudformation/))
    template to perform the following tasks:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个 **AWS CloudFormation**（[https://aws.amazon.com/cloudformation/](https://aws.amazon.com/cloudformation/)）模板来完成以下任务：
- en: Launch a SageMaker Studio domain along with a *studio-user*
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动 SageMaker Studio 域以及一个 *studio-user*
- en: Create a standard EMR cluster with no authentication enabled, including the
    other infrastructure required, such as a **Virtual Private Cloud** (**VPC**),
    subnets, and other resources
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个未启用身份验证的标准 EMR 集群，包括所需的其它基础设施，如**虚拟私有云**（**VPC**）、子网和其他资源
- en: Note
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You will incur a cost for EMR when you launch this CloudFormation template.
    Therefore, make sure to refer to the *Clean up* section at the end of the chapter.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当您启动此 CloudFormation 模板时，您将产生 EMR 的费用。因此，请确保参考章节末尾的**清理**部分。
- en: The CloudFormation template that we will use in the book is originally taken
    from [https://aws-ml-blog.s3.amazonaws.com/artifacts/sma-milestone1/template_no_auth.yaml](https://aws-ml-blog.s3.amazonaws.com/artifacts/sma-milestone1/template_no_auth.yaml)
    and has been modified to run the code provided with the book.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在书中使用的 CloudFormation 模板最初取自 [https://aws-ml-blog.s3.amazonaws.com/artifacts/sma-milestone1/template_no_auth.yaml](https://aws-ml-blog.s3.amazonaws.com/artifacts/sma-milestone1/template_no_auth.yaml)，并已修改以运行书中提供的代码。
- en: 'To get started with launching the CloudFormation template, use your AWS account
    to run the following steps:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始启动 CloudFormation 模板，请使用您的 AWS 账户执行以下步骤：
- en: Log into your AWS account and open the SageMaker management console ([https://console.aws.amazon.com/sagemaker/home](https://console.aws.amazon.com/sagemaker/home)),
    preferably as an admin user. If you don’t have admin user access, make sure you
    have permission to create an EMR cluster, Amazon SageMaker Studio, and S3\. You
    can refer to *Required Permissions* ([https://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-emr-required-permissions.html](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-emr-required-permissions.html))
    for details on the permission needed.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录您的 AWS 账户并打开 SageMaker 管理控制台（[https://console.aws.amazon.com/sagemaker/home](https://console.aws.amazon.com/sagemaker/home)），最好是以管理员用户身份。如果您没有管理员用户访问权限，请确保您有创建
    EMR 集群、Amazon SageMaker Studio 和 S3 的权限。您可以参考**所需权限**（[https://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-emr-required-permissions.html](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-emr-required-permissions.html)）以获取所需权限的详细信息。
- en: Go to the S3 bucket and upload the contents of the S3 folder from the GitHub
    repository. Go to the `templates` folder, click on `template_no_auth.yaml`, and
    copy `Object URL`.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 S3 桶，上传 GitHub 仓库中 S3 文件夹的内容。转到 `templates` 文件夹，点击 `template_no_auth.yaml`，并复制
    `对象 URL`。
- en: Make sure you have the `artifacts` folder parallel to the `templates` folder
    in the S3 bucket as well.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保您在 S3 桶中 `artifacts` 文件夹与 `templates` 文件夹平行。
- en: Search for the `CloudFormation` service and click on it.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索 `CloudFormation` 服务并点击它。
- en: 'Once in the CloudFormation console, click on the **Create stack** orange button,
    as shown in *Figure 5**.1*:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦进入 CloudFormation 控制台，点击如图 *图 5*.1* 所示的 **创建堆栈** 橙色按钮：
- en: '![Figure 5.1 – AWS CloudFormation console](img/B18493_05_001.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – AWS CloudFormation 控制台](img/B18493_05_001.jpg)'
- en: Figure 5.1 – AWS CloudFormation console
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – AWS CloudFormation 控制台
- en: 'In the **Specify template** section, select **Amazon S3 URL** as **Template
    source** and enter **Amazon S3 URL** noted in *step 2*, as shown in *Figure 5**.2,*
    and click on the **Next** button:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **指定模板** 部分，选择 **Amazon S3 URL** 作为 **模板源** 并输入 *步骤 2* 中注明的 **Amazon S3 URL**，如图
    *图 5*.2* 所示，然后点击 **下一步** 按钮：
- en: '![Figure 5.2 – Create stack](img/B18493_05_002.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 创建堆栈](img/B18493_05_002.jpg)'
- en: Figure 5.2 – Create stack
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 创建堆栈
- en: Enter the stack name of your choice and click on the **Next** button.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入您选择的堆栈名称，然后点击 **下一步** 按钮。
- en: On the **Configure stack options** page, keep the default settings and click
    on the **Next** button at the bottom of the page.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **配置堆栈选项** 页面上，保持默认设置，然后点击页面底部的 **下一步** 按钮。
- en: 'On the **Review** page, scroll to the bottom of the screen, click on the **I
    acknowledge that AWS CloudFormation might create IAM resources with custom names**
    checkbox, and click on the **Create stack** button, as shown in *Figure 5**.3*:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **审查** 页面上，滚动到屏幕底部，点击 **我承认 AWS CloudFormation 可能会使用自定义名称创建 IAM 资源** 复选框，然后点击
    **创建堆栈** 按钮，如图 *图 5*.3* 所示：
- en: '![Figure 5.3 – Review stack](img/B18493_05_003.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 审查堆栈](img/B18493_05_003.jpg)'
- en: Figure 5.3 – Review stack
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 审查堆栈
- en: Note
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The CloudFormation template will take 5-10 minutes to launch.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: CloudFormation 模板启动需要 5-10 分钟。
- en: 'Once it is launched, go to **Amazon SageMaker**, click on **SageMaker Studio**,
    and you will see **SageMaker Domain** and **studio-user** configured for you,
    as shown in *Figure 5**.4*:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦启动，前往 **Amazon SageMaker**，点击 **SageMaker Studio**，您将看到为您配置的 **SageMaker 域**
    和 **studio-user**，如图 *图 5*.4* 所示：
- en: '![Figure 5.4 – SageMaker Domain](img/B18493_05_004.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – SageMaker 域](img/B18493_05_004.jpg)'
- en: Figure 5.4 – SageMaker Domain
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – SageMaker 域
- en: Click on the **Launch app** dropdown next to **studio-user** and select **Studio**,
    as shown in the preceding screenshot. After this, you will be presented with a
    new JupyterLab interface ([https://jupyterlab.readthedocs.io/en/latest/user/interface.html](https://jupyterlab.readthedocs.io/en/latest/user/interface.html)).
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **studio-user** 旁边的 **启动应用程序** 下拉菜单，选择 **Studio**，如图所示的先前的屏幕截图。之后，您将看到一个全新的
    JupyterLab 界面 ([https://jupyterlab.readthedocs.io/en/latest/user/interface.html](https://jupyterlab.readthedocs.io/en/latest/user/interface.html))。
- en: Note
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is recommended that you familiarize yourself with the Studio UI by reviewing
    the Amazon SageMaker Studio UI documentation ([https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html)),
    as we will be referencing many of the SageMaker-specific widgets and views throughout
    the chapter.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 建议您通过查看 Amazon SageMaker Studio UI 文档 ([https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html))
    来熟悉 Studio UI，因为我们将在本章中引用许多 SageMaker 特定的小部件和视图。
- en: 'To make it easier to run the various examples within the book using the Studio
    UI, we will clone the source code from the companion GitHub repository. Within
    the Studio UI, click on the **Git** icon in the left sidebar and once the resource
    panel opens, click on the **Clone a repository** button to launch the **Clone
    a repo** dialog box, as shown in *Figure 5**.5*:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使使用 Studio UI 运行书中的各种示例更容易，我们将从配套的 GitHub 仓库克隆源代码。在 Studio UI 中，点击左侧侧边栏中的
    **Git** 图标，一旦资源面板打开，点击 **克隆仓库** 按钮，以启动 **克隆仓库** 对话框，如图 *图 5*.5* 所示：
- en: '![Figure 5.5 – Clone a repo](img/B18493_05_005.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 克隆仓库](img/B18493_05_005.jpg)'
- en: Figure 5.5 – Clone a repo
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 克隆仓库
- en: Enter the URL for the companion repository ([https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS.git](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS.git))
    and click the **CLONE** button.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入配套仓库的 URL ([https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS.git](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS.git))
    并点击 **CLONE** 按钮。
- en: The cloned repository will now appear in the `Applied-Machine-Learning-and-High-Performance-Computing-on-AWS`
    folder to expand it.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆的仓库现在将出现在 `Applied-Machine-Learning-and-High-Performance-Computing-on-AWS`
    文件夹中，以便展开。
- en: Then double-click on the `Chapter05` folder to open it for browsing.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后双击 `Chapter05` 文件夹以打开它进行浏览。
- en: We are now ready to analyze large amounts of structured data using SageMaker
    Studio. However, before we can start the analysis, we need to acquire the data.
    Let’s take a look at how to do that.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 SageMaker Studio 分析大量结构化数据。然而，在我们开始分析之前，我们需要获取数据。让我们看看如何做到这一点。
- en: Analyzing large amounts of structured data
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析大量结构化数据
- en: 'Since the objective of this section is to provide a hands-on example for analyzing
    large-scale structured data, our first task will be to synthesize a large amount
    of data. Using the Studio UI, execute the following steps:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本节的目标是提供一个分析大规模结构化数据的动手示例，我们的第一个任务将是合成大量数据。使用 Studio UI，执行以下步骤：
- en: Using the left `1_data_generator.ipynb` file to launch the Jupyter notebook.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用左侧的`1_data_generator.ipynb`文件启动 Jupyter 笔记本。
- en: 'When prompted with the **Set up notebook environment** dialog box, ensure that
    **Data Science** is selected from the **Image** drop-down box, as well as **Python
    3** for the **Kernel** option. *Figure 5**.6* shows an example of the dialog box:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当出现 **设置笔记本环境** 对话框时，请确保从 **图像** 下拉菜单中选择 **数据科学**，以及 **内核** 选项中的 **Python 3**。*图
    5**.6 展示了对话框的示例：
- en: '![Figure 5.6 – Set up notebook environment](img/B18493_05_006.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – 设置笔记本环境](img/B18493_05_006.jpg)'
- en: Figure 5.6 – Set up notebook environment
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – 设置笔记本环境
- en: Once these options have been set, click the **Select** button to continue.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置好这些选项后，点击 **选择** 按钮继续。
- en: Next, you should see a **Starting notebook kernel…** message. The notebook kernel
    will take a couple of minutes to load.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你应该会看到一个 **启动笔记本内核…** 消息。笔记本内核需要几分钟来加载。
- en: Once the notebook has loaded, run the notebook by clicking on the **Kernel**
    menu and selecting the **Restart kernel and Run All** **Cells…** option.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 笔记本加载完成后，通过点击 **内核** 菜单并选择 **重启内核并运行所有单元格…** 选项来运行笔记本。
- en: After the notebook has executed all the code cells, we can dive into exactly
    what the notebook does, starting with a review of the dataset.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本执行完所有代码单元格后，我们可以深入了解笔记本做了什么，从检查数据集开始。
- en: Reviewing the dataset
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查数据集
- en: The dataset we will be using within this example is the California housing dataset
    ([https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html)).
    This dataset was derived from the 1990 US census, using one row per census block
    group. A block group is the smallest geographical unit for which the US Census
    Bureau publishes sample data. A block group typically has a population of 600
    to 3,000 people.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本例中使用的数据集是加利福尼亚住房数据集 ([https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html))。这个数据集是从
    1990 年美国人口普查中得出的，每个普查区块组一行。区块组是美国人口普查局发布样本数据的最小地理单位。一个区块组通常有 600 到 3,000 的人口。
- en: The dataset is incorporated into the `sklearn` Python library ([https://scikit-learn.org/stable/index.html](https://scikit-learn.org/stable/index.html)).
    The scikit-learn library includes a dataset module that allows us to download
    popular reference datasets, such as the California housing dataset, from **StatLib
    Datasets** **Archive** (http://lib.stat.cmu.edu/datasets/).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集被纳入 `sklearn` Python 库 ([https://scikit-learn.org/stable/index.html](https://scikit-learn.org/stable/index.html))。scikit-learn
    库包括一个数据集模块，允许我们从 **StatLib 数据集存档**（http://lib.stat.cmu.edu/datasets/）下载流行的参考数据集，例如加利福尼亚住房数据集。
- en: Dataset citation
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集引用
- en: Pace, R. Kelley, and Ronald Barry, Sparse Spatial Autoregressions, Statistics
    and Probability Letters, 33 (1997) 291-297.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Pace, R. Kelley, 和 Ronald Barry，稀疏空间自回归，统计学与概率通讯，33 (1997) 291-297。
- en: One key thing to be aware of is that this dataset only has 20,640 samples and
    is only around 400 KB in size. So, I’m sure you’ll agree that it doesn’t exactly
    qualify as a large amount of structured data. So, the primary objective of the
    notebook we’ve just executed is to use this dataset as a basis for synthesizing
    a much larger amount of structured data and then storing this new dataset on S3
    for analysis.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一个需要注意的关键点是，这个数据集只有20,640个样本，大小约为400 KB。所以，我相信您会同意这并不完全符合大量结构化数据的定义。因此，我们刚刚执行的笔记本的主要目标是使用这个数据集作为合成大量结构化数据的基础，然后将这个新数据集存储在S3上进行分析。
- en: Let’s walk through the code to see how this is done.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过代码来查看这是如何完成的。
- en: Installing the Python libraries
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装Python库
- en: 'The first five code cells within the notebook are used to install and upgrade
    the necessary Python libraries to ensure we have the correct versions for the
    SageMaker SDK, scikit-learn, and the **Synthetic Data Vault**. The following code
    snippet shows the consolidation of these five code cells:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本中的前五个代码单元用于安装和升级必要的Python库，以确保我们有SageMaker SDK、scikit-learn和**合成数据仓库**的正确版本。以下代码片段展示了这五个代码单元的合并：
- en: '[PRE0]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: There is no specific reason we upgrade and install the SageMaker and scikit-learn
    libraries except to ensure conformity across the examples within this chapter.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们升级和安装SageMaker和scikit-learn库没有特定的原因，只是为了确保本章示例的一致性。
- en: 'Once the required libraries have been installed, we load them and configure
    our global variables. The following code snippet shows how we import the libraries
    and configure the SageMaker default S3 bucket parameters:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了所需的库，我们就加载它们并配置全局变量。以下代码片段展示了我们如何导入库并配置SageMaker默认的S3存储桶参数：
- en: '[PRE1]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'However, before we can synthesize a larger dataset and upload this to S3, we
    need to download the California housing dataset. As you can see from the following
    code snippet, we create two local folders called `data` and `raw`, then download
    the data using the `fetch_california_housing()` method from `sklearn.datasets`.
    The resultant data variable allows us to describe the data, as well as capture
    the data itself as a two-dimensional data structure called `df_data`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们能够合成更大的数据集并将其上传到S3之前，我们需要下载加利福尼亚住房数据集。如您从以下代码片段中可以看到，我们创建了两个本地文件夹`data`和`raw`，然后使用`sklearn.datasets`中的`fetch_california_housing()`方法下载数据。结果数据变量使我们能够描述数据，以及将数据本身作为二维数据结构`df_data`捕获：
- en: '[PRE2]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `df_data` variable is the essential representation of our structured data,
    with columns showing the data labels and rows showing the observations or records
    for each label. Think of this structure as similar to a spreadsheet or relational
    table.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`df_data`变量是我们结构化数据的必要表示，列显示数据标签，行显示每个标签的观测或记录。将这种结构想象成类似于电子表格或关系表。'
- en: 'Using the `df_data` variable, we further describe this structure as well as
    perform some of the descriptive statistics and visualization described in the
    *Exploring data analysis methods* section of this chapter. For example, the following
    code snippet shows how to describe the type of data we are dealing with. You will
    recall that understanding the data type is crucial for appreciating the overall
    schema or structure of the data:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`df_data`变量，我们进一步描述这种结构，并执行本章“探索数据分析方法”部分中描述的一些描述性统计和可视化。例如，以下代码片段展示了如何描述我们正在处理的数据类型。您会记得，理解数据类型对于欣赏数据的整体模式或结构至关重要：
- en: '[PRE3]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Furthermore, we can define a Python function called `plot_boxplot()` to visualize
    the data included in the `df_data` variable. You will recall that visualizing
    the data provides further insight into the data. For example, as you can see from
    the next code snippet, we can visualize the overall distribution of the average
    number of rooms or `avgNumrooms` in the house:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以定义一个名为`plot_boxplot()`的Python函数来可视化`df_data`变量中包含的数据。您会记得，可视化数据可以提供对数据的进一步了解。例如，您可以从下一个代码片段中看到，我们可以可视化房屋的平均房间数或`avgNumrooms`的整体分布：
- en: '[PRE4]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As we can see from *Figure 5**.7*, the resultant boxplot from the code indicates
    that the average number of rooms for the California housing data is **5**:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们从**图5.7**.7中可以看到，代码生成的箱线图表明加利福尼亚住房数据的平均房间数为**5**：
- en: '![Figure 5.7 – Average number of rooms](img/B18493_05_007.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图5.7 – 房间平均数量](img/B18493_05_007.png)'
- en: Figure 5.7 – Average number of rooms
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 – 房间平均数量
- en: Additionally, you will note from *Figure 5**.7* that there is somewhat of an
    equal distribution to the upper and lower bounds of the data. This indicates that
    we have a good distribution of data for the average number of bedrooms and therefore,
    we don’t need to augment this data point.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还会注意到在**图5.7**中，数据的上限和下限之间有一种某种程度的均匀分布。这表明我们有一个很好的数据分布，对于平均卧室数量来说，因此，我们不需要增强这个数据点。
- en: 'Finally, you will recall from the *Counting the data points* section that we
    can circumvent any unnecessary issues during the model training process by determining
    whether or not there are any missing values in the data. For example, the next
    code snippet shows how we can review a sum of any missing values in the `df_data`
    variable:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可能还记得在**计数数据点**部分，我们可以通过确定数据中是否存在缺失值来避免模型训练过程中的任何不必要的麻烦。例如，下一个代码片段展示了我们如何查看`df_data`变量中任何缺失值的总和：
- en: '[PRE5]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: While we’ve only covered a few analytics methodologies to showcase the analytics
    life cycle, a key takeaway from these examples is that the data is easy to analyze
    since it’s small enough to fit into memory. So, as data scientists, we did not
    have to capture a sample of the global population to analyze the data and then
    infer that analysis back onto the larger dataset. Let’s see whether this holds
    true once we synthesize a larger dataset.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们只介绍了几种分析方法来展示分析生命周期，但从中可以得出的一个关键结论是，由于数据量足够小，可以放入内存中，因此数据很容易分析。所以，作为数据科学家，我们不需要捕获全球人口的样本来分析数据，然后再将分析结果推断回更大的数据集。让我们看看当我们合成一个更大的数据集时，这个结论是否仍然成立。
- en: Synthesizing large data
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 合成大量数据
- en: The last part of the notebook involves using the Synthetic Data Vault ([https://sdv.dev/SDV/index.html](https://sdv.dev/SDV/index.html)),
    or the `sdv` Python library. This ecosystem of Python libraries uses ML models
    that specifically focus on learning from structured tabular and time series datasets
    and on creating synthetic data that carries the same format, statistical properties,
    and structure as the original dataset.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本的最后部分涉及使用合成数据仓库([https://sdv.dev/SDV/index.html](https://sdv.dev/SDV/index.html))，或`sdv`
    Python库。这个Python库生态系统使用专门关注从结构化表格和时间序列数据集学习以及创建与原始数据集具有相同格式、统计属性和结构的合成数据的ML模型。
- en: 'In our example notebook, we use a TVAE ([https://arxiv.org/abs/1907.00503](https://arxiv.org/abs/1907.00503))
    model to generate a larger version of the California housing data. For example,
    the following code snippet shows how we define and train a TVAE model on the `df_data`
    variable:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例笔记本中，我们使用一个TVAE（[https://arxiv.org/abs/1907.00503](https://arxiv.org/abs/1907.00503)）模型来生成加利福尼亚住房数据的一个更大版本。例如，以下代码片段展示了我们如何在`df_data`变量上定义和训练一个TVAE模型：
- en: '[PRE6]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once we’ve trained the model, we can load it to generate 1 million new observations
    or rows in a variable called `synthetic_data`. The following code snippet shows
    an example of this:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了模型，我们就可以将其加载到`synthetic_data`变量中，生成一百万个新的观测值或行。以下代码片段展示了这个例子：
- en: '[PRE7]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, as shown, we use the following code snippet to compress the data and
    leverage the SageMaker SDK’s `upload_data()` method to store the data in S3:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，正如所示，我们使用以下代码片段来压缩数据，并利用SageMaker SDK的`upload_data()`方法将数据存储在S3中：
- en: '[PRE8]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With a dataset of 1 million rows stored in S3, we finally have an example of
    a large amount of structured data. Now we can use this data to demonstrate how
    to leverage the highlighted analysis methods at scale on structured data using
    Amazon EMR.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在S3中存储了100万行数据集后，我们终于有一个大量结构化数据的例子。现在我们可以使用这些数据来展示如何使用Amazon EMR在结构化数据上大规模利用突出显示的分析方法。
- en: Analyzing large-scale data using an EMR cluster with SageMaker Studio
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用EMR集群和SageMaker Studio分析大规模数据
- en: 'To get started with analyzing the large-scale synthesized dataset we’ve just
    created, we can execute the following steps in the Studio UI:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始分析我们刚刚创建的大规模合成数据集，我们可以在Studio UI中执行以下步骤：
- en: Using the left-hand navigation panel, double-click on the `2_data_exploration_spark.ipynb`
    notebook to launch it.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用左侧导航面板，双击`2_data_exploration_spark.ipynb`笔记本以启动它。
- en: As we saw with the previous example, when prompted with the **Set up notebook
    environment** dialog box, select **SparkMagic** as **Image** and **PySpark** as
    **Kernel**.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们通过前面的示例所看到的，当出现**设置笔记本环境**对话框时，选择**SparkMagic**作为**图像**，并将**PySpark**作为**内核**。
- en: Once the notebook is ready, click on the **Kernel** menu option and once again
    select the **Restart kernel and Run All Cells…** option to execute the entire
    notebook.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦笔记本准备就绪，点击**内核**菜单选项，然后再次选择**重启内核并运行所有单元格…**选项以执行整个笔记本。
- en: 'While the notebook is running, we can start reviewing what we are trying to
    accomplish within the various code cells. As you can see from the first code cell,
    we connect to the EMR cluster we provisioned in the *Setting up EMR and SageMaker*
    *Studio* section:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当笔记本正在运行时，我们可以开始审查我们在各个代码单元格中试图完成的事情。正如您从第一个代码单元格中可以看到的，我们连接到在*设置EMR和SageMaker*
    *工作室*部分配置的EMR集群：
- en: '[PRE9]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the next code cell, shown by the following code, we read the synthesized
    dataset from S3\. Here we create a `housing_data` variable by using PySpark’s
    `sqlContext` method to read the raw data from S3:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个代码单元格中，以下代码显示了从S3读取合成的数据集。我们在这里创建一个`housing_data`变量，使用PySpark的`sqlContext`方法从S3读取原始数据：
- en: '[PRE10]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Once we have this variable assigned, we can use PySpark and the EMR cluster
    to execute the various data analysis tasks on the entire population of the data
    without having to ingest a sample dataset on which to perform the analysis.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们分配了这个变量，我们就可以使用PySpark和EMR集群在数据的整个总体上执行各种数据分析任务，而无需摄入用于分析的样本数据集。
- en: While the notebook provides multiple examples of different analysis methodologies
    that are specific to the data, we will focus on the few examples that relate to
    the exploration we’ve already performed on the original California housing dataset
    to illustrate how these same methodologies can be applied at scale.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然笔记本提供了针对数据的多种特定分析方法的示例，但我们将重点关注与我们已经在原始加利福尼亚住房数据集上进行的探索相关的少数示例，以说明这些相同的方法如何在大规模上应用。
- en: Reviewing the data structure and counts
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 复习数据结构和计数
- en: 'As already mentioned, understanding the type of data, its structure, and the
    counts is an important part of the analysis. To perform this analysis on the entirety
    of `housing_data`, we can execute the following code:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 正如已经提到的，了解数据的类型、结构和计数是分析的重要部分。为了在整个`housing_data`上执行此分析，我们可以执行以下代码：
- en: '[PRE11]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Executing this code produces the following output, where we can see that we
    have 1 million observations, as well as the data types for each feature:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码会产生以下输出，其中我们可以看到我们有100万个观测值，以及每个特征的类型：
- en: '[PRE12]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Next, we can determine whether or not there are any missing values and how to
    deal with them.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以确定是否存在任何缺失值以及如何处理它们。
- en: Handling missing values
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: 'You will recall that ensuring that there are no missing values is an important
    methodology for any data analysis. To expose any missing data, we can run the
    following code to create a count of any missing values for each column or feature
    within this large dataset:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 您会记得，确保没有缺失值是任何数据分析的重要方法。为了暴露任何缺失数据，我们可以运行以下代码来创建每个列或特征中缺失值的计数：
- en: '[PRE13]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If we do find any missing values, there are a number of techniques we can use
    to deal with them. For example, we delete rows containing missing values, using
    the `dropna()` method on the `housing_data` variable. Alternatively, depending
    on the number of missing values, we can use imputation techniques to infer a value
    based on the mean or median of the feature.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们确实发现任何缺失值，我们可以使用多种技术来处理它们。例如，我们可以使用`dropna()`方法在`housing_data`变量上删除包含缺失值的行。或者，根据缺失值的数量，我们可以使用插补技术根据特征的均值或中位数推断一个值。
- en: Analyzing the centrality and variability of the data
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分析数据的中心性和变异性
- en: Remember that understanding how the data is distributed will give us a good
    idea of whether the data is proportional or not. This analysis task also provides
    an idea of whether we have outliers within our data that skew the distribution
    or spread. Previously, it was emphasized that visualizing the data distribution
    using bar charts and boxplots can further assist in determining the variability
    of the data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，了解数据的分布情况将给我们一个很好的想法，了解数据是否成比例。这项分析任务还提供了一个想法，即我们数据中是否有异常值扭曲了分布或扩散。之前已经强调，使用条形图和箱线图可视化数据分布可以进一步帮助确定数据的变异性。
- en: 'To accommodate this task, the following code highlights an example of capturing
    the features we wish to analyze and plotting their distribution as a bar chart:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应这项任务，以下代码突出显示了一个捕获我们希望分析的特征并绘制其分布作为条形图的示例：
- en: '[PRE14]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After executing this code on the large data, we can see an example of the resultant
    distribution for the average number of rooms (`avgNumRooms`), the average number
    of bedrooms (`avgNumBedrooms`), and block population (`population`) features in
    *Figure 5**.8*:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在对大量数据进行代码执行后，我们可以在 *图 5.8* 中看到平均房间数量 (`avgNumRooms`)、平均卧室数量 (`avgNumBedrooms`)
    和区块人口 (`population`) 特征的结果分布示例：
- en: '![Figure 5.8 – Feature distribution](img/B18493_05_008.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8 – 特征分布](img/B18493_05_008.jpg)'
- en: Figure 5.8 – Feature distribution
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – 特征分布
- en: 'As you can see from *Figure 5**.8*, both the `avgNumBedrooms` and `population`
    features are not centered around the mean or average for the feature. Additionally,
    the spread for the `avgNumBedrooms` feature is significantly skewed toward the
    lower end of the spectrum. This factor could indicate that there are potential
    outliers or too many data points that are consolidated between `avgNumBedrooms`
    feature:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从 *图 5.8* 中所见，`avgNumBedrooms` 和 `population` 特征都没有围绕均值或平均特征值进行中心化。此外，`avgNumBedrooms`
    特征的分布显著偏向光谱的较低端。这个因素可能表明存在潜在的异常值或过多的数据点集中在 `avgNumBedrooms` 特征之间：
- en: '[PRE15]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The resultant boxplot from running this code cell is shown in *Figure 5**.9*:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码单元格的结果箱线图显示在 *图 5.9* 中：
- en: '![Figure 5.9 – Boxplot for the average number of bedrooms](img/B18493_05_009.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – 房间平均数量的箱线图](img/B18493_05_009.jpg)'
- en: Figure 5.9 – Boxplot for the average number of bedrooms
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 房间平均数量的箱线图
- en: '*Figure 5**.9* clearly shows that there are a number of outliers that cause
    the data to be skewed. Therefore, we need to resolve these discrepancies as part
    of our data analysis and before ML models can be trained on our large dataset.
    The following code snippet shows how to query the data from the boxplot values
    and then simply remove it, to create a variable called `housing_df_with_no_outliers`:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.9* 清楚地显示了导致数据偏斜的多个异常值。因此，我们需要在数据分析和在大型数据集上训练机器学习模型之前解决这些差异。以下代码片段展示了如何从箱线图中查询数据，然后简单地删除它，以创建一个名为
    `housing_df_with_no_outliers` 的变量：'
- en: '[PRE16]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once we have our `housing_df_with_no_outliers`, we can use the following code
    to create a new boxplot of the variability of the `avgNumBedrooms` feature:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了 `housing_df_with_no_outliers`，我们可以使用以下代码创建 `avgNumBedrooms` 特征变异性的新箱线图：
- en: '[PRE17]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '*Figure 5**.10* shows an example of a boxplot produced from executing this
    code:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.10* 展示了执行此代码后生成的箱线图示例：'
- en: '![Figure 5.10 – Boxplot of the average number of bedrooms](img/B18493_05_010.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10 – 房间平均数量的箱线图](img/B18493_05_010.jpg)'
- en: Figure 5.10 – Boxplot of the average number of bedrooms
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – 房间平均数量的箱线图
- en: From *Figure 5**.10*, we can clearly see that the outliers have been removed.
    Subsequently, we can perform a similar procedure on the `avgNumRooms` and `population`
    features.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *图 5.10* 中，我们可以清楚地看到已经移除了异常值。随后，我们可以在 `avgNumRooms` 和 `population` 特征上执行类似的程序。
- en: While these examples only show some of the important methodologies highlighted
    within the data analysis life cycle, an important takeaway from this exercise
    is that due to the integration of SageMaker Studio and EMR, we’re able to accomplish
    the data analysis tasks on large-scale structured data without having to capture
    a sample of the global population and then infer that analysis back onto the larger
    dataset. However, along with analyzing the data at scale, we also need to ensure
    that any preprocessing tasks are also executed at scale.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些例子只展示了数据分析生命周期中突出的一些重要方法，但从这个练习中我们可以得到的一个重要启示是，由于集成了 SageMaker Studio 和
    EMR，我们能够在大规模结构化数据上完成数据分析任务，而无需捕获全球人口的样本，然后再将分析推断回更大的数据集。然而，在分析大规模数据的同时，我们还需要确保任何预处理任务也能在大规模上执行。
- en: Next, we will review how to automate these preprocessing tasks at scale using
    SageMaker.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将回顾如何使用 SageMaker 自动化这些预处理任务的大规模执行。
- en: Preprocessing the data at scale
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大规模预处理数据
- en: The SageMaker service takes care of the heavy lifting and scaling of data transformation
    or preprocessing tasks using one of its core components called `preprocess.py`.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 服务通过其核心组件之一 `preprocess.py` 来处理数据转换或预处理任务的重负载和扩展。
- en: 'The following code snippet shows how the code to remove outliers can be converted
    into a Python script:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何将删除异常值的代码转换为 Python 脚本：
- en: '[PRE18]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Once the Python script is created, we can load the appropriate SageMaker SDK
    libraries and configure the S3 locations for the input data as well as the transformed
    output data, as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了Python脚本，我们可以加载适当的SageMaker SDK库，并配置输入数据以及转换后的输出数据的S3位置，如下所示：
- en: '[PRE19]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we can instantiate an instance of the SageMaker `PySparkProcessor()`
    class ([https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor))
    as a `spark_processor` variable, as can be seen in the following code:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以创建SageMaker `PySparkProcessor()`类的实例，作为`spark_processor`变量，如下代码所示：
- en: '[PRE20]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With the `spark_processor` variable defined, we can then call the `run()` method
    to execute a SageMaker Processing job. The following code demonstrates how to
    call the `run()` method and supply the `preprocess.py` script along with the input
    and output locations for the data as `arguments`:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了`spark_processor`变量后，我们可以调用`run()`方法来执行SageMaker Processing作业。以下代码演示了如何调用`run()`方法，并附带`preprocess.py`脚本以及数据输入和输出位置的`arguments`：
- en: '[PRE21]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the background, SageMaker will create an ephemeral Spark cluster and execute
    the `preprocess.py` script on the input data. Once the data transformations are
    completed, SageMaker will store the resultant dataset on S3 and then decommission
    the Spark cluster, all while redirecting the execution log output back to the
    Jupyter notebook.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在后台，SageMaker将创建一个临时的Spark集群，并在输入数据上执行`preprocess.py`脚本。一旦数据转换完成，SageMaker将把结果数据集存储在S3上，然后解散Spark集群，同时将执行日志输出重定向回Jupyter笔记本。
- en: While this technique makes the complicated task of analyzing large amounts of
    structured data much easier to scale, there is still the question of how to perform
    a similar procedure on unstructured data.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种技术使得分析大量结构化数据的复杂任务更容易扩展，但仍然存在如何在非结构化数据上执行类似过程的问题。
- en: Let’s review how to solve this problem next.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下如何解决这个问题。
- en: Analyzing large amounts of unstructured data
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析大量非结构化数据
- en: In this section, we will use unstructured data (horse and human images) downloaded
    from [https://laurencemoroney.com/datasets.html](https://laurencemoroney.com/datasets.html).
    This dataset can be used to train a binary image classification model to classify
    horses and humans in the image. From the SageMaker Studio, launch the `3_unstructured_data_s3.ipynb`
    notebook with **PyTorch 1.8 Python 3.6 CPU Optimized** selected from the **Image**
    drop-down box as well as **Python 3** for the **Kernel** option. Once the notebook
    is opened, restart the kernel and run all the cells as mentioned in the *Analyzing
    large-scale data using an EMR cluster with SageMaker* *Studio* section.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用从[https://laurencemoroney.com/datasets.html](https://laurencemoroney.com/datasets.html)下载的非结构化数据（马和人类图像）进行操作。这个数据集可以用来训练一个二进制图像分类模型，以在图像中区分马和人类。从SageMaker
    Studio启动`3_unstructured_data_s3.ipynb`笔记本，从**图像**下拉菜单中选择**PyTorch 1.8 Python 3.6
    CPU Optimized**，以及**Python 3**作为**内核**选项。一旦笔记本打开，重启内核并运行如*使用SageMaker Studio分析大规模数据*部分中提到的所有单元格。
- en: After the notebook has executed all the code cells, we can dive into exactly
    what the notebook does.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本执行完所有代码单元格后，我们可以深入了解笔记本具体做了什么。
- en: As you can see in the notebook, we first download the horse-or-human data from
    [https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip](https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip)
    and then unzip the file.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如笔记本中所示，我们首先从[https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip](https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip)下载马或人类数据，然后解压文件。
- en: 'Once we have the data, we will convert the images to high resolution using
    the `super-image` library:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了数据，我们将使用`super-image`库将图像转换为高分辨率：
- en: 'We will first download the pretrained model with `scale = 4`, which means that
    we intend to increase the resolution of the images four times, as shown in the
    following code block:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先下载具有`scale = 4`的预训练模型，这意味着我们打算将图像的分辨率提高四倍，如下代码块所示：
- en: '[PRE22]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we will iterate through the folder containing the images, use the pretrained
    model to convert each image to high resolution, and save it, as shown in the following
    code block:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将遍历包含图像的文件夹，使用预训练模型将每个图像转换为高分辨率，并保存，如下面的代码块所示：
- en: '[PRE26]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: You can check the file size of one of the images to confirm that the images
    have been converted to high resolution. Once the images have been converted to
    high resolution, you can optionally duplicate the images to increase the number
    of files and finally upload them to S3 bucket. We will use the images uploaded
    to the S3 bucket for running a SageMaker training job.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过检查其中一张图像的文件大小来确认图像已被转换为高分辨率。一旦图像被转换为高分辨率，您可以可选地复制图像以增加文件数量，最后将它们上传到S3桶。我们将使用上传到S3桶的图像来运行SageMaker训练作业。
- en: Note
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In this example, we will walk you through the option of running a training job
    with **PyTorch** using the SageMaker training feature using the data stored in
    an S3 bucket. You can also choose to use other frameworks as well, such as **TensorFlow**
    and **MXNet**, which are also supported by SageMaker.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将向您展示如何使用SageMaker训练功能，通过存储在S3桶中的数据运行一个**PyTorch**训练作业。您也可以选择使用其他框架，例如也由SageMaker支持的**TensorFlow**和**MXNet**。
- en: 'In order to use PyTorch, we will first import the `sagemaker.pytorch` module,
    using which we will define the **SageMaker PyTorch Estimator**, as shown in the
    following code block:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用PyTorch，我们首先导入`sagemaker.pytorch`模块，使用它我们将定义**SageMaker PyTorch Estimator**，如下面的代码块所示：
- en: '[PRE37]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In the estimator object, as you can see from the code snippet, we need to provide
    configuration parameters. In this case, we need to define the following parameters:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在估计器对象中，如代码片段所示，我们需要提供配置参数。在这种情况下，我们需要定义以下参数：
- en: '`instance_count`: This is the number of instances'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`instance_count`: 这是实例数量'
- en: '`instance_type`: This is the type of instance on which the training job will
    be launched'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`instance_type`: 这是训练作业将启动的实例类型'
- en: '`framework_version`: This is the framework version of PyTorch to be used for
    training'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework_version`: 这是用于训练的PyTorch框架版本'
- en: '`py_version`: This is the Python version'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`py_version`: 这是Python版本'
- en: '`source_dir`: This is the folder path within the notebook, which contains the
    training script'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`source_dir`: 这是笔记本中的文件夹路径，其中包含训练脚本'
- en: '`entry_point`: This is the name of the Python training script'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entry_point`: 这是Python训练脚本的名称'
- en: '`hyperparameters`: This is the list of hyperparameters that will be used by
    the training script'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hyperparameters`: 这是训练脚本将使用的超参数列表'
- en: 'Once we have defined the PyTorch estimator, we will define the `TrainingInput`
    object, which will take the S3 location of the input data, content type, and input
    mode as parameters, as shown in the following code:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了PyTorch估计器，我们将定义`TrainingInput`对象，它将输入数据的S3位置、内容类型和输入模式作为参数，如下面的代码所示：
- en: '[PRE38]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The `input_mode` parameter can take the following values; in our case, we are
    using the `File` value:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_mode`参数可以取以下值；在我们的情况下，我们使用`File`值：'
- en: '`None`: Amazon SageMaker will use the input mode specified in the base `Estimator`
    class'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`None`: Amazon SageMaker将使用基础`Estimator`类中指定的输入模式'
- en: '`File`: Amazon SageMaker copies the training dataset from the S3 location to
    a local directory'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`File`: Amazon SageMaker将训练数据集从S3位置复制到本地目录'
- en: '`Pipe`: Amazon SageMaker streams data directly from S3 to the container via
    a *Unix-named pipe*'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pipe`: Amazon SageMaker通过*Unix命名管道*直接从S3流数据到容器'
- en: '`FastFile`: Amazon SageMaker streams data from S3 on demand instead of downloading
    the entire dataset before training begins'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FastFile`: Amazon SageMaker在训练开始前按需从S3流式传输数据，而不是下载整个数据集'
- en: Note
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can see the complete list of parameters for the PyTorch estimator at this
    link: [https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html).'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过此链接查看PyTorch估计器的完整参数列表：[https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html).
- en: 'Once we have configured the PyTorch `Estimator` and `TrainingInput` objects,
    we are now ready to start the training job, as shown in the following code:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们配置了PyTorch `Estimator`和`TrainingInput`对象，我们现在就可以开始训练作业，如下面的代码所示：
- en: '[PRE39]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: When we run `estimator.fit`, it will launch one training instance of the `ml.g4dn.8xlarge`
    type, install the `PyTorch 1.8` container, copy the training data from `S3 location`
    and the `train.py` script to the local directory on the training instance, and
    will finally run the training script that you have provided in the estimator configuration.
    Once the training job is completed, SageMaker will automatically terminate all
    the resources that it has launched, and you will only be charged for the amount
    of time the training job was running.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行`estimator.fit`时，它将启动一个`ml.g4dn.8xlarge`类型的训练实例，安装`PyTorch 1.8`容器，将训练数据从`S3
    位置`和`train.py`脚本复制到训练实例的本地目录，并最终运行你在估计器配置中提供的训练脚本。一旦训练作业完成，SageMaker 将自动终止它启动的所有资源，你只需为训练作业运行的时间付费。
- en: In this example, we used a simple training script that involved loading the
    data using the PyTorch `DataLoader` object and iterating through the images. In
    the following section, we’ll see how to process data at scale using AWS.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了一个简单的训练脚本，该脚本涉及使用 PyTorch `DataLoader` 对象加载数据并遍历图像。在下一节中，我们将看到如何使用
    AWS 进行大规模数据处理。
- en: Processing data at scale on AWS
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 AWS 上大规模处理数据
- en: In the previous section, *Analyzing large amounts of unstructured data*, the
    data was stored in an S3 bucket, which was used for training. There will be scenarios
    where you will need to load data faster for training instead of waiting for the
    training job to copy the data from S3 locally into your training instance. In
    these scenarios, you can store the data on a file system, such as `3_unstructured_data.ipynb`
    notebook. Refer to the **Optimize it with data on EFS** and **Optimize it with
    data on FSX** sections in the notebook.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节**分析大量非结构化数据**中，数据存储在 S3 存储桶中，用于训练。在有些情况下，你需要更快地加载数据进行训练，而不是等待训练作业将数据从 S3
    本地复制到训练实例。在这些情况下，你可以将数据存储在文件系统上，例如`3_unstructured_data.ipynb`笔记本。请参阅笔记本中的**使用
    EFS 上的数据优化**和**使用 FSX 上的数据优化**部分。
- en: Note
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Before you run the `template_filesystems.yaml` template, in a similar fashion
    as we did in the *Setting up EMR and SageMaker* *Studio* section.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行`template_filesystems.yaml`模板之前，以与我们在**设置 EMR 和 SageMaker Studio**部分所做的方式类似。
- en: Cleaning up
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清理
- en: Let’s terminate the EMR cluster, which we launched in the *Setting up EMR and
    SageMaker Studio* section, as it will not be used in the later chapters of the
    book.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们终止在**设置 EMR 和 SageMaker Studio**部分启动的 EMR 集群，因为它在本书的后续章节中不会被使用。
- en: 'Let’s start by logging into the AWS console and following the steps given here:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们登录 AWS 控制台并按照这里给出的步骤操作：
- en: Search `EMR` in the AWS console.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 控制台中搜索`EMR`。
- en: 'You will see the active **EMR-Cluster-sm-emr** cluster. Select the checkbox
    against the EMR cluster name and click on the **Terminate** button, as shown in
    *Figure 5**.11*:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将看到活动的**EMR-Cluster-sm-emr**集群。选择 EMR 集群名称旁边的复选框，然后点击**终止**按钮，如图**图 5**.11*所示：
- en: '![Figure 5.11 – List EMR cluster](img/B18493_05_011.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.11 – EMR 集群列表](img/B18493_05_011.jpg)'
- en: Figure 5.11 – List EMR cluster
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – EMR 集群列表
- en: 'Click on the red **Terminate** button in the pop-up window, as shown in *Figure
    5**.12*:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在弹出的窗口中点击红色**终止**按钮，如图**图 5**.12*所示：
- en: '![Figure 5.12 – Terminate EMR cluster](img/B18493_05_012.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.12 – 终止 EMR 集群](img/B18493_05_012.jpg)'
- en: Figure 5.12 – Terminate EMR cluster
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – 终止 EMR 集群
- en: It will take a few minutes to terminate the EMR cluster, and once completed,
    **Status** will change to **Terminated**.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 终止 EMR 集群可能需要几分钟，一旦完成，**状态**将变为**已终止**。
- en: Let’s summarize what we’ve learned in this chapter.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下本章学到的内容。
- en: Summary
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored various data analysis methods, reviewed some of
    the AWS services for analyzing data, and launched a CloudFormation template to
    create an EMR cluster, SageMaker Studio domain, and other useful resources. We
    then did a deep dive into code for analyzing both structured and unstructured
    data and suggested a few methods for optimizing its performance. This will help
    you to prepare your data for training ML models.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了各种数据分析方法，回顾了一些 AWS 数据分析服务，并启动了一个 CloudFormation 模板来创建 EMR 集群、SageMaker
    Studio 域和其他有用的资源。然后我们深入研究了分析结构化和非结构化数据的代码，并提出了一些优化其性能的方法。这将帮助你为训练机器学习模型准备数据。
- en: In the next chapter, we will see how we can train large models on large amounts
    of data in a distributed fashion to speed up the training process.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到如何以分布式的方式在大量数据上训练大型模型，以加快训练过程。
