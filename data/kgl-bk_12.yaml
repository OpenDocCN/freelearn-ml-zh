- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Modeling for Computer Vision
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉建模
- en: 'Computer vision tasks are among the most popular problems in practical applications
    of machine learning; they were the gateway into deep learning for many Kagglers,
    including yours truly (Konrad, that is). Over the last few years, there has been
    tremendous progress in the field and new SOTA libraries continue to be released.
    In this chapter, we will give you an overview of the most popular competition
    types in computer vision:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉任务是机器学习实际应用中最受欢迎的问题之一；它们是许多Kagglers（包括我本人，即Konrad）进入深度学习的门户。在过去的几年里，该领域取得了巨大的进步，新的SOTA库仍在不断发布。在本章中，我们将为您概述计算机视觉中最受欢迎的竞赛类型：
- en: Image classification
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分类
- en: Object detection
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测
- en: Image segmentation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分割
- en: We will begin with a short section on image augmentation, a group of task-agnostic
    techniques that can be applied to different problems to increase the generalization
    capability of our models.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从图像增强的简短部分开始，这是一组任务无关的技术，可以应用于不同的问题以提高我们模型的一般化能力。
- en: Augmentation strategies
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增强策略
- en: 'While deep learning techniques have been extremely successful in computer vision
    tasks like image recognition, segmentation, or object detection, the underlying
    algorithms are typically extremely data-intensive: they require large amounts
    of data to avoid overfitting. However, not all domains of interest satisfy that
    requirement, which is where **data augmentation**comes in. This is the name for
    a group of image processing techniques that create modified versions of images,
    thus enhancing the size and quality of training datasets, leading to better performance
    of deep learning models. The augmented data will typically represent a more comprehensive
    set of possible data points, thereby minimizing the distance between the training
    and validation set, as well as any future test sets.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然深度学习技术在图像识别、分割或目标检测等计算机视觉任务中取得了极大的成功，但底层算法通常非常数据密集：它们需要大量数据以避免过拟合。然而，并非所有感兴趣的领域都满足这一要求，这就是**数据增强**发挥作用的地方。这是指一组图像处理技术，它们创建图像的修改版本，从而增强训练数据集的大小和质量，导致深度学习模型性能的改善。增强数据通常代表一组更全面的可能数据点，从而最小化训练集和验证集之间的距离，以及任何未来的测试集。
- en: 'In this section, we will review some of the more common augmentation techniques,
    along with choices for their software implementations. The most frequently used
    transformations include:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾一些更常见的增强技术，以及它们软件实现的选项。最常用的变换包括：
- en: '**Flipping**: Flipping the image (along the horizontal or vertical axis)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**翻转**：沿水平或垂直轴翻转图像'
- en: '**Rotation**: Rotating the image by a given angle (clockwise or anti-clockwise)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**旋转**：按给定角度（顺时针或逆时针）旋转图像'
- en: '**Cropping**: A random subsection of the image is selected'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**裁剪**：随机选择图像的一个子区域'
- en: '**Brightness**: Modifying the brightness of the image'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**亮度**：修改图像的亮度'
- en: '**Scaling**: The image is increased or decreased to a higher (outward) or lower
    (inward) size'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩放**：将图像增加到更大的（向外）或更小的（向内）尺寸'
- en: 'Below, we demonstrate how those transformations work in practice using the
    image of an American acting legend and comedian, Betty White:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，我们将通过使用美国演员和喜剧传奇人物贝蒂·怀特的图像来展示这些变换在实际中的应用：
- en: '![Obraz zawierający osoba, niebieski  Opis wygenerowany automatycznie](img/B17574_10_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![包含人物、蓝色  自动生成的描述](img/B17574_10_01.png)'
- en: 'Figure 10.1: Betty White image'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：贝蒂·怀特图像
- en: 'We can flip the image along the vertical or horizontal axes:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以沿垂直或水平轴翻转图像：
- en: '![Obraz zawierający osoba, kobieta, niebieski  Opis wygenerowany automatycznie](img/B17574_10_02.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![包含人物、女性、蓝色  自动生成的描述](img/B17574_10_02.png)'
- en: 'Figure 10.2: Betty White image – flipped vertically (left) and horizontally
    (right)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：贝蒂·怀特图像 – 垂直翻转（左侧）和水平翻转（右侧）
- en: 'Rotations are fairly self-explanatory; notice the automatic padding of the
    image in the background:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转相当直观；注意背景中图像的自动填充：
- en: '![Obraz zawierający osoba  Opis wygenerowany automatycznie](img/B17574_10_03.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![包含人物  自动生成的描述](img/B17574_10_03.png)'
- en: 'Figure 10.3: Betty White image – rotated clockwise'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3：贝蒂·怀特图像 – 顺时针旋转
- en: 'We can also crop an image to the region of interest:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将图像裁剪到感兴趣的区域：
- en: '![Obraz zawierający osoba, zamknąć, oczy, wpatrywanie się  Opis wygenerowany
    automatycznie](img/B17574_10_04.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![包含人物、闭眼、注视  自动生成的描述](img/B17574_10_04.png)'
- en: 'Figure 10.4: Betty White image – cropped'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4：Betty White图像 – 裁剪
- en: 'On a high level, we can say that augmentations can be applied in one of two
    ways:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，我们可以这样说，增强可以以两种方式之一应用：
- en: '**Offline**: These are usually applied for smaller datasets (fewer images or
    smaller sizes, although the definition of “small” depends on the available hardware).
    The idea is to generate modified versions of the original images as a preprocessing
    step for your dataset, and then use those alongside the “original” ones.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**离线**：这些通常用于较小的数据集（较少的图像或较小的尺寸，尽管“小”的定义取决于可用的硬件）。想法是在数据集预处理步骤中生成原始图像的修改版本，然后与“原始”图像一起使用。'
- en: '**Online**: These are used for bigger datasets. The augmented images are not
    saved on disk; the augmentations are applied in mini-batches and fed to the model.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在线**：这些用于较大的数据集。增强后的图像不会保存到磁盘上；增强操作是在小批量中应用，并馈送到模型中。'
- en: 'In the next few sections, we will give you an overview of two of the most common
    methods for augmenting your image dataset: the built-in Keras functionality and
    the `albumentations` package. There are several other options available out there
    (`skimage`, OpenCV, `imgaug`, Augmentor, SOLT), but we will focus on the most
    popular ones.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将为您概述两种最常用的图像数据集增强方法：内置的Keras功能和`albumentations`包。还有其他一些选项可供选择（`skimage`、OpenCV、`imgaug`、Augmentor、SOLT），但我们将重点关注最受欢迎的几种。
- en: 'The methods discussed in this chapter focus on image analysis powered by GPU.
    The use of **tensor processing units** (**TPUs**) is an emerging, but still somewhat
    niche, application. Readers interested in image augmentation in combination with
    TPU-powered analysis are encouraged to check out the excellent work of *Chris
    Deotte* (**@cdeotte**):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的方法侧重于由GPU驱动的图像分析。**张量处理单元**（**TPUs**）的使用是一个新兴但仍然相对小众的应用。对图像增强与TPU驱动分析感兴趣的读者被鼓励查看Chris
    Deotte（**@cdeotte**）的出色工作：
- en: '[https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords](https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords](https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords)'
- en: Chris is a quadruple Kaggle Grandmaster and a fantastic educator through the
    Notebooks he creates and discussions he participates in; overall, a person definitely
    worth following for any Kaggler, irrespective of your level of experience.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Chris是一位四重Kaggle大师，也是一位出色的教育者，通过他创建的笔记本和参与的讨论；总的来说，任何Kaggler都值得关注的一个人，无论你的经验水平如何。
- en: 'We will be using data from the *Cassava Leaf Disease Classification* competition
    ([https://www.kaggle.com/c/cassava-leaf-disease-classification](https://www.kaggle.com/c/cassava-leaf-disease-classification)).
    As usual, we begin with the groundwork: first, loading the necessary packages:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自*甘薯叶病分类*竞赛（[https://www.kaggle.com/c/cassava-leaf-disease-classification](https://www.kaggle.com/c/cassava-leaf-disease-classification)）的数据。像往常一样，我们首先进行基础工作：首先，加载必要的包：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we define some helper functions that will streamline the presentation
    later. We need a way to load images into arrays:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一些辅助函数，以便稍后简化演示。我们需要一种将图像加载到数组中的方法：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We would like to display multiple images in a gallery style, so we create a
    function that takes as input an array containing the images along with the desired
    number of columns, and outputs the array reshaped into a grid with a given number
    of columns:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望以画廊风格显示多张图像，因此我们创建了一个函数，该函数接受一个包含图像以及所需列数的数组作为输入，并将该数组重塑为具有给定列数的网格：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With the boilerplate taken care of, we can load the images for augmentation:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理完模板后，我们可以加载用于增强的图像：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s load a single image so we know what our reference is:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载一张单独的图像，以便我们知道我们的参考是什么：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here it is:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下面就是：
- en: '![Obraz zawierający podłoże, roślina, zewnętrzne, zielony  Opis wygenerowany
    automatycznie](img/B17574_10_05.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![包含背景、植物、外部、绿色的图像 - 自动生成的描述](img/B17574_10_05.png)'
- en: 'Figure 10.5: Reference image'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5：参考图像
- en: In the following sections, we will demonstrate how to generate augmented images
    from this reference image using both built-in Keras functionality and the `albumentations`
    library.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将演示如何使用内置的Keras功能和`albumentations`库从参考图像生成增强图像。
- en: Keras built-in augmentations
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras内置增强
- en: The Keras library has a built-in functionality for augmentations. While not
    as extensive as dedicated packages, it has the advantage of easy integration with
    your code. We do not need a separate code block for defining the augmentation
    transformations but can incorporate them inside `ImageDataGenerator`, a functionality
    we are likely to be using anyway.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 库具有内置的增强功能。虽然不如专用包那么全面，但它具有易于与代码集成的优势。我们不需要单独的代码块来定义增强变换，而可以将它们包含在 `ImageDataGenerator`
    中，这是我们可能无论如何都会使用的一个功能。
- en: The first Keras approach we examine is based upon the `ImageDataGenerator` class.
    As the name suggests, it can be used to generate batches of image data with real-time
    data augmentations.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先检查的 Keras 方法是基于 `ImageDataGenerator` 类。正如其名所示，它可以用来生成带有实时数据增强的图像数据批次。
- en: ImageDataGenerator approach
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ImageDataGenerator 方法
- en: 'We begin by instantiating an object of class `ImageDataGenerator` in the following
    manner:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先以以下方式实例化 `ImageDataGenerator` 类的对象：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We define the desired augmentations as arguments to `ImageDataGenerator`. The
    official documentation does not seem to address the topic, but practical results
    indicate that the augmentations are applied in the order in which they are defined
    as arguments.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所需的增强作为 `ImageDataGenerator` 的参数。官方文档似乎没有涉及这个话题，但实际结果表明，增强是按照它们作为参数定义的顺序应用的。
- en: 'In the above example, we utilize only a limited subset of possible options;
    for a full list, the reader is encouraged to consult the official documentation:
    [https://keras.io/api/preprocessing/image/](https://keras.io/api/preprocessing/image/).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，我们只使用了可能选项的一个有限子集；对于完整的列表，建议读者查阅官方文档：[https://keras.io/api/preprocessing/image/](https://keras.io/api/preprocessing/image/).
- en: 'Next, we iterate through the images with the `.flow` method of the `ImageDataGenerator`
    object. The class provides three different functions to load the image dataset
    in memory and generate batches of augmented data:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `ImageDataGenerator` 对象的 `.flow` 方法遍历图像。该类提供了三种不同的函数来将图像数据集加载到内存中并生成增强数据批次：
- en: '`flow`'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flow`'
- en: '`flow_from_directory`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flow_from_directory`'
- en: '`flow_from_dataframe`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flow_from_dataframe`'
- en: 'They all achieve the same objective, but differ in the way the locations of
    the files are specified. In our example, the images are already in memory, so
    we can iterate using the simplest method:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 它们都达到相同的目标，但在指定文件位置的方式上有所不同。在我们的例子中，图像已经存储在内存中，因此我们可以使用最简单的方法进行迭代：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can examine the augmented images using the helper functions we defined earlier:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用之前定义的辅助函数来检查增强图像：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is the result:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '![Obraz zawierający roślina  Opis wygenerowany automatycznie](img/B17574_10_06.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![包含植物的图像 自动生成的描述](img/B17574_10_06.png)'
- en: 'Figure 10.6: A collection of augmented images'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6：增强图像的集合
- en: Augmentations are a very useful tool, but using them efficiently requires a
    judgment call. First, it is obviously a good idea to visualize them to get a feeling
    for the impact on the data. On the one hand, we want to introduce some variation
    in the data to increase the generalization of our model; on the other, if we change
    the images too radically, the input data will be less informative and the model
    performance is likely to suffer. In addition, the choice of which augmentations
    to use can also be problem-specific, as we can see by comparing different competitions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 增强是一个非常有用的工具，但有效地使用它们需要做出判断。首先，显然是一个好主意可视化它们，以了解对数据的影响。一方面，我们希望引入一些数据变化以增加我们模型的一般化；另一方面，如果我们过于激进地改变图像，输入数据将变得不那么有信息量，模型的性能可能会受到影响。此外，选择使用哪些增强也可能具有问题特异性，正如我们通过比较不同的竞赛所看到的那样。
- en: If you look at *Figure 10.6* above (the reference image from the *Cassava Leaf
    Disease Classification* competition), the leaves on which we are supposed to identify
    the disease can be of different sizes, pointing at different angles, and so on,
    due both to the shapes of the plants and differences in how the images are taken.
    This means transformations such as vertical or horizontal flips, cropping, and
    rotations all make sense in this context.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看上面的 *图10.6*（来自 *Cassava Leaf Disease Classification* 竞赛的参考图像），我们应识别疾病的叶子可能具有不同的尺寸，指向不同的角度等，这既是因为植物的形状，也是因为图像拍摄方式的不同。这意味着垂直或水平翻转、裁剪和旋转等变换在这个上下文中都是有意义的。
- en: 'By contrast, we can look at a sample image from the *Severstal: Steel Defect
    Detection* competition ([https://www.kaggle.com/c/severstal-steel-defect-detection](https://www.kaggle.com/c/severstal-steel-defect-detection)).
    In this competition, participants had to localize and classify defects on a steel
    sheet. All the images had the same size and orientation, which means that rotations
    or crops would have produced unrealistic images, adding to the noise and having
    an adverse impact on the generalization capabilities of an algorithm.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，我们可以查看来自*Severstal：钢铁缺陷检测*竞赛的样本图像（[https://www.kaggle.com/c/severstal-steel-defect-detection](https://www.kaggle.com/c/severstal-steel-defect-detection)）。在这个竞赛中，参与者必须在钢板上定位和分类缺陷。所有图像都具有相同的大小和方向，这意味着旋转或裁剪会产生不真实的图像，增加了噪声，并对算法的泛化能力产生不利影响。
- en: '![Obraz zawierający tekst, półka, zrzut ekranu  Opis wygenerowany automatycznie](img/B17574_10_07.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、货架、屏幕截图的图片，自动生成的描述](img/B17574_10_07.png)'
- en: 'Figure 10.7: Sample images from the Severstal competition'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7：Severstal竞赛的样本图像
- en: Preprocessing layers
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理层
- en: 'An alternative approach to data augmentation as a preprocessing step in a native
    Keras manner is to use the `preprocessing` layers API. The functionality is remarkably
    flexible: these pipelines can be used either in combination with Keras models
    or independently, in a manner similar to `ImageDataGenerator`.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 作为原生Keras中的预处理步骤的数据增强的另一种方法是使用`preprocessing`层API。该功能非常灵活：这些管道可以与Keras模型结合使用或独立使用，类似于`ImageDataGenerator`。
- en: 'Below we show briefly how a preprocessing layer can be set up. First, the imports:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们简要说明如何设置预处理层。首先，导入：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We load a pretrained model in the standard Keras manner:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以标准的Keras方式加载预训练模型：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preprocessing layers can be used in the same way as other layers are used
    inside the `Sequential` constructor; the only requirement is that they need to
    be specified before any others, at the beginning of our model definition:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理层可以使用与其他层在`Sequential`构造函数内部使用相同的方式；唯一的要求是它们需要在我们的模型定义的开始处指定，在所有其他层之前：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: albumentations
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: albumentations
- en: The `albumentations` package is a fast image augmentation library that is built
    as a wrapper of sorts around other libraries.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`albumentations`软件包是一个快速图像增强库，它作为其他库的某种包装构建。'
- en: The package is the result of intensive coding in quite a few Kaggle competitions
    (see [https://medium.com/@iglovikov/the-birth-of-albumentations-fe38c1411cb3](https://medium.com/@iglovikov/the-birth-of-albumentations-fe38c1411cb3)),
    and claims among its core developers and contributors quite a few notable Kagglers,
    including *Eugene Khvedchenya* ([https://www.kaggle.com/bloodaxe](https://www.kaggle.com/bloodaxe)),
    *Vladimir Iglovikov* ([https://www.kaggle.com/iglovikov](https://www.kaggle.com/iglovikov)),
    *Alex Parinov* ([https://www.kaggle.com/creafz](https://www.kaggle.com/creafz)),
    and *ZFTurbo* ([https://www.kaggle.com/zfturbo](https://www.kaggle.com/zfturbo)).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 该软件包是经过在多个Kaggle竞赛中密集编码的结果（参见[https://medium.com/@iglovikov/the-birth-of-albumentations-fe38c1411cb3](https://medium.com/@iglovikov/the-birth-of-albumentations-fe38c1411cb3)），其核心开发者和贡献者中包括一些知名的Kagglers，包括*尤金·赫维琴亚*
    ([https://www.kaggle.com/bloodaxe](https://www.kaggle.com/bloodaxe))、*弗拉基米尔·伊格洛维科夫*
    ([https://www.kaggle.com/iglovikov](https://www.kaggle.com/iglovikov))、*亚历克斯·帕里诺夫*
    ([https://www.kaggle.com/creafz](https://www.kaggle.com/creafz))和*ZFTurbo* ([https://www.kaggle.com/zfturbo](https://www.kaggle.com/zfturbo))。
- en: The full documentation can be found at [https://albumentations.readthedocs.io/en/latest/](https://albumentations.readthedocs.io/en/latest/).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的文档可以在[https://albumentations.readthedocs.io/en/latest/](https://albumentations.readthedocs.io/en/latest/)找到。
- en: 'Below we list the important characteristics:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们列出重要特性：
- en: A unified API for different data types
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对不同数据类型的统一API
- en: Support for all common computer vision tasks
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持所有常见的计算机视觉任务
- en: Integration both with TensorFlow and PyTorch
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与TensorFlow和PyTorch的集成
- en: 'Using `albumentations` functionality to transform an image is straightforward.
    We begin by initializing the required transformations:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`albumentations`功能转换图像非常简单。我们首先初始化所需的转换：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we apply the transformations to our reference image:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将转换应用于我们的参考图像：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can access the augmented images with the `''image''` key and visualize the
    results:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`'image'`键访问增强图像并可视化结果：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here are our results:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们的结果：
- en: '![Obraz zawierający roślina, zewnętrzne, ogród, trawa  Opis wygenerowany automatycznie](img/B17574_10_08.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![包含植物、外部、花园、草地的图片，自动生成的描述](img/B17574_10_08.png)'
- en: 'Figure 10.8: Image augmented using the albumentations library'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8：使用albumentations库增强的图像
- en: 'Having discussed augmentation as a crucial preprocessing step in approaching
    a computer vision problem, we are now in a position to apply this knowledge in
    the following sections, beginning with a very common task: image classification.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了增强作为处理计算机视觉问题的一个关键预处理步骤之后，我们现在可以应用这一知识到以下章节中，从一个非常常见的任务开始：图像分类。
- en: '![](img/Chris_Deotte.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Chris_Deotte.png)'
- en: Chris Deotte
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Chris Deotte
- en: '[https://www.kaggle.com/cdeotte](https://www.kaggle.com/cdeotte)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/cdeotte](https://www.kaggle.com/cdeotte)'
- en: Before we proceed, let’s look at a brief conversation we had with Chris Deotte,
    who we’ve mentioned quite a few times in this book (including earlier in this
    chapter), and for good reason. He is a quadruple Kaggle Grandmaster and Senior
    Data Scientist & Researcher at NVIDIA, who joined Kaggle in 2019.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们回顾一下我们与Chris Deotte的简短对话，我们在本书中多次提到他（包括在本章的早期），这有很好的理由。他是四届Kaggle大师级选手，也是NVIDIA的高级数据科学家和研究员，于2019年加入Kaggle。
- en: What’s your favorite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 您最喜欢的竞赛类型是什么？为什么？在技术和解决方法方面，您在Kaggle上的专长是什么？
- en: '*I enjoy competitions with fascinating data and competitions that require building
    creative novel models. My specialty is analyzing trained models to determine their
    strengths and weaknesses. Afterward, I enjoy improving the models and/or developing
    post-processing to boost CV LB.*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*我喜欢与有趣数据相关的竞赛，以及需要构建创新新颖模型的竞赛。我的专长是分析训练好的模型，以确定它们的优点和缺点。之后，我喜欢改进模型和/或开发后处理来提升CV
    LB。*'
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您是如何处理Kaggle竞赛的？这种处理方式与您日常工作的处理方式有何不同？
- en: '*I begin each competition by performing EDA (exploratory data analysis), creating
    a local validation, building some simple models, and submitting to Kaggle for
    leaderboard scores. This fosters an intuition of what needs to be done in order
    to build an accurate and competitive model.*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*我每次开始竞赛都会进行EDA（探索性数据分析），创建本地验证，构建一些简单的模型，并将它们提交到Kaggle以获取排行榜分数。这有助于培养一种直觉，了解为了构建一个准确且具有竞争力的模型需要做什么。*'
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请告诉我们您参加的一个特别具有挑战性的竞赛，以及您用来应对任务的见解。
- en: Kaggle’s Shopee – Price Match Guarantee *was a challenging competition that
    required both image models and natural language models. A key insight was extracting
    embeddings from the two types of models and then determining how to use both image
    and language information together to find product matches.*
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle的Shopee – 价格匹配保证*是一个具有挑战性的竞赛，需要图像模型和自然语言模型。一个关键的见解是从两种模型中提取嵌入，然后确定如何结合使用图像和语言信息来找到产品匹配。
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是否帮助了您的职业生涯？如果是的话，是如何帮助的？
- en: '*Yes. Kaggle helped me become a senior data scientist at NVIDIA by improving
    my skills and boosting my resume’s marketability.*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*是的。Kaggle帮助我通过提高我的技能和增强我的简历的市场价值，成为NVIDIA的高级数据科学家。*'
- en: '*Many employers peruse the work on Kaggle to find employees with specific skills
    to help solve their specific projects. In this way, I have been solicited about
    many job opportunities.*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*许多雇主浏览Kaggle上的作品，以寻找具有特定技能的员工来帮助他们解决特定的项目。这样，我收到了许多工作机会的邀请。*'
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的经验中，经验不足的Kagglers通常忽略了什么？您现在知道什么，而您希望在最初开始时就了解的呢？
- en: '*In my opinion, inexperienced Kagglers often overlook the importance of local
    validation. Seeing your name on the leaderboard is exciting. And it’s easy to
    focus on improving our leaderboard scores instead of our cross-validation scores.*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*在我看来，经验不足的Kagglers往往忽略了本地验证的重要性。看到自己的名字在排行榜上是很兴奋的。而且很容易专注于提高我们的排行榜分数，而不是交叉验证分数。*'
- en: What mistakes have you made in competitions in the past?
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的竞赛中，您犯过哪些错误？
- en: '*Many times, I have made the mistake of trusting my leaderboard score over
    my cross-validation score and selecting the wrong final submission.*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*很多时候，我犯了一个错误，就是过分相信我的排行榜分数，而不是交叉验证分数，从而选择了错误的最终提交。*'
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis or machine learning?
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据分析或机器学习，您会推荐使用哪些特定的工具或库？
- en: '*Absolutely. Feature engineering and quick experimentation are important when
    optimizing tabular data models. In order to accelerate the cycle of experimentation
    and validation, using NVIDIA RAPIDS cuDF and cuML on GPU are essential.*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*当然。在优化表格数据模型时，特征工程和快速实验非常重要。为了加速实验和验证的周期，使用NVIDIA RAPIDS cuDF和cuML在GPU上至关重要。*'
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们参加比赛时，他们应该记住或做最重要的事情是什么？
- en: '*The most important thing is to have fun and learn. Don’t worry about your
    final placement. If you focus on learning and having fun, then over time your
    final placements will become better and better.*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*最重要的是要享受乐趣并学习。不要担心你的最终排名。如果你专注于学习和享受乐趣，那么随着时间的推移，你的最终排名会越来越好。*'
- en: Do you use other competition platforms? How do they compare to Kaggle?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 您是否使用其他比赛平台？它们与Kaggle相比如何？
- en: '*Yes, I have entered competitions outside of Kaggle. Individual companies like
    Booking.com or Twitter.com will occasionally host a competition. These competitions
    are fun and involve high-quality, real-life data.*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*是的，我在Kaggle之外也参加过比赛。像Booking.com或Twitter.com这样的个别公司偶尔会举办比赛。这些比赛很有趣，涉及高质量的真实数据。*'
- en: Classification
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: In this section, we will demonstrate an end-to-end pipeline that can be used
    as a template for handling image classification problems. We will walk through
    the necessary steps, from data preparation, to model setup and estimation, to
    results visualization. Apart from being informative (and cool), this last step
    can also be very useful if you need to examine your code in-depth to get a better
    understanding of the performance.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示一个端到端流程，该流程可以用作处理图像分类问题的模板。我们将逐步介绍必要的步骤，从数据准备到模型设置和估计，再到结果可视化。除了提供信息（并且很酷）之外，这一最后步骤如果需要深入检查代码以更好地理解性能，也可以非常有用。
- en: We will continue using the data from the *Cassava Leaf Disease Classification*
    contest ([https://www.kaggle.com/c/cassava-leaf-disease-classification](https://www.kaggle.com/c/cassava-leaf-disease-classification)).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用来自*Cassava Leaf Disease Classification*比赛的数据（[https://www.kaggle.com/c/cassava-leaf-disease-classification](https://www.kaggle.com/c/cassava-leaf-disease-classification)）。
- en: 'As usual, we begin by loading the necessary libraries:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，我们首先加载必要的库：
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It is usually a good idea to define a few helper functions; it makes for code
    that is easier to both read and debug. If you are approaching a general image
    classification problem, a good starting point can be provided by a model from
    the **EfficientNet** family, introduced in 2019 in a paper from the Google Research
    Brain Team ([https://arxiv.org/abs/1905.11946](https://arxiv.org/abs/1905.11946)).
    The basic idea is to balance network depth, width, and resolution to enable more
    efficient scaling across all dimensions and subsequently better performance. For
    our solution, we will use the simplest member of the family, **EfficientNet B0**,
    which is a mobile-sized network with 11 million trainable parameters.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，定义几个辅助函数是个好主意；它使得代码更容易阅读和调试。如果您正在处理一个通用的图像分类问题，2019年由谷歌研究大脑团队在论文中引入的**EfficientNet**系列模型可以作为一个良好的起点（[https://arxiv.org/abs/1905.11946](https://arxiv.org/abs/1905.11946)）。基本思想是平衡网络深度、宽度和分辨率，以实现所有维度的更有效扩展，从而获得更好的性能。对于我们的解决方案，我们将使用该系列中最简单的成员，**EfficientNet
    B0**，这是一个具有1100万个可训练参数的移动尺寸网络。
- en: For a properly detailed explanation of the EfficientNet networks, you are encouraged
    to explore [https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html](https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html)
    as a starting point.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于EfficientNet网络的详细解释，您可以从[https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html](https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html)作为起点进行探索。
- en: 'We construct our model with B0 as the basis, followed by a pooling layer for
    improved translation invariance and a dense layer with an activation function
    suitable for our multiclass classification problem:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以B0为基础构建模型，随后是一个用于提高平移不变性的池化层和一个适合我们多类分类问题的激活函数的密集层：
- en: '[PRE15]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Some brief remarks on the parameters we pass to the `EfficientNetB0` function:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 关于我们传递给`EfficientNetB0`函数的参数的一些简要说明：
- en: The `include_top` parameter allows you to decide whether to include the final
    dense layers. As we want to use the pre-trained model as a feature extractor,
    a default strategy would be to skip them and then define the head ourselves.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_top` 参数允许你决定是否包含最终的密集层。由于我们想将预训练模型用作特征提取器，默认策略将是跳过它们，然后自己定义头部。'
- en: '`weights` can be set to `None` if we want to train the model from scratch,
    or to `''imagenet''`or `''noisy-student''` if we instead prefer to utilize the
    weights pre-trained on large image collections.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们想从头开始训练模型，可以将 `weights` 设置为 `None`，或者如果我们要利用在大图像集合上预训练的权重，可以设置为 `'imagenet'`
    或 `'noisy-student'`。
- en: 'The helper function below allows us to visualize the activation layer, so we
    can examine the network performance from a visual angle. This is frequently helpful
    in developing an intuition in a field notorious for its opacity:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的辅助函数允许我们可视化激活层，这样我们可以从视觉角度检查网络性能。这在开发一个在透明度方面臭名昭著的领域的直觉时非常有帮助：
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We generate the activations by creating predictions for a given model based
    on a “restricted” model, in other words, using the entire architecture up until
    the penultimate layer; this is the code up to the `activations` variable. The
    rest of the function ensures we show the right layout of activations, corresponding
    to the shape of the filter in the appropriate convolution layer.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过为给定的模型创建基于“受限”模型的预测来生成激活，换句话说，使用整个架构直到倒数第二层；这是到 `activations` 变量的代码。函数的其余部分确保我们展示正确的激活布局，对应于适当卷积层中过滤器的形状。
- en: 'Next, we process the labels and set up the validation scheme; there is no special
    structure in the data (for example, a time dimension or overlap across classes),
    so we can use a simple random split:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们处理标签并设置验证方案；数据中没有特殊结构（例如，时间维度或类别间的重叠），因此我们可以使用简单的随机分割：
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: For a refresher on more elaborate validation schemes, refer to *Chapter 6*,
    *Designing Good Validation*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 若想了解更多详细的验证方案，请参阅 *第6章*，*设计良好的验证*。
- en: We are now able to set up the data generators, which are necessary for our TF-based
    algorithm to loop through the image data.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以设置数据生成器，这对于我们的基于 TF 的算法循环图像数据是必要的。
- en: 'First, we instantiate two `ImageDataGenerator` objects; this is when we incorporate
    the image augmentations. For the purpose of this demonstration, we will go with
    the Keras built-in ones. After that, we create the generator using a `flow_from_dataframe()`
    method, which is used to generate batches of tensor image data with real-time
    data augmentation:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们实例化两个 `ImageDataGenerator` 对象；这是当我们引入图像增强的时候。为了演示目的，我们将使用 Keras 内置的增强。之后，我们使用
    `flow_from_dataframe()` 方法创建生成器，该方法用于生成具有实时数据增强的批处理张量图像数据：
- en: '[PRE18]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'With the data structures specified, we can create the model:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在指定了数据结构之后，我们可以创建模型：
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Once our model is created, we can quickly examine a summary. This is mostly
    useful for sanity checks, because unless you have a photographic memory, chances
    are you are not going to remember the layer composition batches of a sophisticated
    model like EffNetB0\. In practice, you can use the summary to check whether the
    dimensions of output filters are correct or whether the parameter counts (trainable
    on non-trainable) are in line with expectations. For the sake of compactness,
    we only demonstrate the first few lines of the output below; inspecting the architecture
    diagram for B0 will give you an idea of how long the complete output would be.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的模型创建完成后，我们可以快速查看一个摘要。这主要用于检查，因为除非你有 photographic memory，否则你不太可能记住像 EffNetB0
    这样复杂模型的层组成批次。在实践中，你可以使用摘要来检查输出过滤器的维度是否正确，或者参数计数（可训练的/不可训练的）是否符合预期。为了简洁起见，我们只展示了输出下面的前几行；检查
    B0 的架构图将给你一个完整输出将有多长的概念。
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With the above steps taken care of, we can proceed to fitting the model. In
    this step, we can also very conveniently define callbacks. The first one is `ModelCheckpoint`:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成上述步骤后，我们可以继续拟合模型。在这一步中，我们还可以非常方便地定义回调。第一个是 `ModelCheckpoint`：
- en: '[PRE21]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The checkpoint uses a few parameters worth elaborating on:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点使用了一些值得详细说明的参数：
- en: We can preserve the best set of model weights by setting `save_best_only = True`.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过设置 `save_best_only = True`，我们可以保留最佳模型权重集。
- en: We reduce the size of the model by only keeping the weights, instead of the
    complete set of optimizer state.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过只保留权重而不是完整的优化器状态来减小模型的大小。
- en: We decide on which model is optimal by locating a minimum for validation loss.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过找到验证损失的最低点来决定哪个模型是最优的。
- en: 'Next, we use one of the popular methods for preventing overfitting, **early
    stopping**. We monitor the performance of the model on the holdout set and stop
    the algorithm if the metric stops improving for a given number of epochs, in this
    case `5`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用防止过拟合的流行方法之一，**早期停止**。我们监控模型在保留集上的性能，如果给定数量的epoch内指标不再提升，则停止算法，在这个例子中是`5`：
- en: '[PRE22]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `ReduceLROnPlateau` callback monitors the loss on the holdout set and if
    no improvement is seen for a `patience` number of epochs, the learning rate is
    reduced, in this case by a factor of 0.3\. While not a universal solution, it
    can frequently help with convergence:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReduceLROnPlateau`回调监控保留集上的损失，如果在`patience`数量的epoch内没有看到改进，则降低学习率，在这个例子中是通过0.3的因子降低。虽然这不是万能的解决方案，但它可以经常帮助收敛：'
- en: '[PRE23]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We are now ready to fit the model:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好拟合模型：
- en: '[PRE24]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We will briefly explain the two parameters we have not encountered before:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要解释我们之前没有遇到的两个参数：
- en: The training generator yields `steps_per_epoch` batches per training epoch.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练生成器在每个训练epoch中产生`steps_per_epoch`批次的批次。
- en: When the epoch is finished, the validation generator produces `validation_steps`
    batches.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当epoch结束时，验证生成器产生`validation_steps`批次的批次。
- en: 'An example output after calling `model.fit()` is given below:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用`model.fit()`之后的一个示例输出如下：
- en: '[PRE25]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Once a model is fitted, we can examine the activations on a sample image using
    the helper function we wrote at the start. While this is not necessary for successful
    model execution, it can help determine what sort of features our model is extracting
    before applying the classification layer at the top:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 模型拟合后，我们可以使用我们在开头编写的辅助函数检查样本图像上的激活。虽然这对于模型的成功执行不是必需的，但它可以帮助确定我们的模型在应用顶部的分类层之前提取了哪些类型的特征：
- en: '[PRE26]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here is what we might see:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们可能会看到的情况：
- en: '![Obraz zawierający tekst, warzywo, kolorowy  Opis wygenerowany automatycznie](img/B17574_10_09.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、蔬菜、彩色  自动生成的描述](img/B17574_10_09.png)'
- en: 'Figure 10.9: Sample activations from a fitted model'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9：拟合模型的样本激活
- en: 'We can generate the predictions with `model.predict()`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`model.predict()`生成预测：
- en: '[PRE27]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We build the predictions by iterating through the list of images. For each of
    them, we reshape the image to the required dimensions and pick the channel with
    the strongest signal (the model predicts class probabilities, of which we pick
    the largest one with `argmax`). The final predictions are class numbers, in line
    with the metric utilized in the competition.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过遍历图像列表来构建预测。对于每一张图像，我们将图像重塑到所需的维度，并选择具有最强信号的通道（模型预测类别概率，我们通过`argmax`选择最大的一个）。最终的预测是类别编号，与竞赛中使用的指标一致。
- en: We have now demonstrated a minimal end-to-end pipeline for image classification.
    Numerous improvements are, of course, possible – for instance, more augmentations,
    bigger architecture, callback customization – but the basic underlying template
    should provide you with a good starting point going forward.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经演示了一个用于图像分类的最小化端到端流程。当然，许多改进都是可能的——例如，更多的增强、更大的架构、回调定制——但基本模板应该为你提供一个良好的起点。
- en: 'We move on now to a second popular problem in computer vision: object detection.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在继续讨论计算机视觉中的第二个流行问题：目标检测。
- en: Object detection
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标检测
- en: '**Object detection** is a computer vision/image processing task where we need
    to identify instances of semantic objects of a certain class in an image or video.
    In classification problems like those discussed in the previous section, we simply
    need to assign a class to each image, whereas in object detection tasks, we want
    to draw a **bounding box** around an object of interest to locate it within an
    image.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标检测**是计算机视觉/图像处理任务，我们需要在图像或视频中识别特定类别的语义对象实例。在前面章节讨论的分类问题中，我们只需要为每个图像分配一个类别，而在目标检测任务中，我们希望在感兴趣的对象周围绘制一个**边界框**来定位它。'
- en: In this section, we will use data from the *Global Wheat Detection* competition
    ([https://www.kaggle.com/c/global-wheat-detection](https://www.kaggle.com/c/global-wheat-detection)).
    In this competition, participants had to detect wheat heads, which are spikes
    atop plants containing grain. Detection of these in plant images is used to estimate
    the size and density of wheat heads across crop varieties. We will demonstrate
    how to train a model for solving this using **Yolov5**, a well-established model
    in object detection, and state-of-the-art until late 2021 when it was (based on
    preliminary results) surpassed by the YoloX architecture. Yolov5 gave rise to
    extremely competitive results in the competition and although it was eventually
    disallowed by the organizers due to licensing issues, it is very well suited for
    the purpose of this demonstration.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用来自 *全球小麦检测* 竞赛的数据（[https://www.kaggle.com/c/global-wheat-detection](https://www.kaggle.com/c/global-wheat-detection)）。在这个竞赛中，参与者必须检测小麦穗，即植物顶部含有谷物的穗状物。在植物图像中检测这些穗状物用于估计不同作物品种中小麦穗的大小和密度。我们将展示如何使用
    **Yolov5**，一个在目标检测中建立已久的模型，并在2021年底之前是最先进的模型，直到它（基于初步结果）被 YoloX 架构超越，来训练一个解决此问题的模型。Yolov5
    在竞赛中产生了极具竞争力的结果，尽管由于许可问题最终被组织者禁止使用，但它非常适合本演示的目的。
- en: '![](img/B17574_10_10.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_10_10.png)'
- en: 'Figure 10.10: Sample image visualizations of detected wheat heads'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10：检测到的小麦穗的样本图像可视化
- en: An important point worth mentioning before we begin is the different formats
    for bounding box annotations; there are different (but mathematically equivalent)
    ways of describing the coordinates of a rectangle.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，有一个重要的问题需要提及，那就是边界框注释的不同格式；有不同（但数学上等价）的方式来描述矩形的坐标。
- en: 'The most common types are coco, voc-pascal, and yolo. The differences between
    them are clear from the figure below:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的类型是 coco、voc-pascal 和 yolo。它们之间的区别可以从下面的图中清楚地看出：
- en: '![](img/B17574_10_11.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_10_11.png)'
- en: 'Figure 10.11: Annotation formats for bounding boxes'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11：边界框的注释格式
- en: 'One more part we need to define is the grid structure: Yolo detects objects
    by placing a grid over an image and checking for the presence of an object of
    interest (wheat head, in our case) in any of the cells. The bounding boxes are
    reshaped to be offset within the relevant cells of the image and the *(x, y, w,
    h)* parameters are scaled to the unit interval:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要定义的一个部分是网格结构：Yolo 通过在图像上放置一个网格并检查是否有感兴趣的对象（在我们的例子中是小麦穗）存在于任何单元格中来检测对象。边界框被重新塑形以在图像的相关单元格内偏移，并且
    *(x, y, w, h)* 参数被缩放到单位区间：
- en: '![](img/B17574_10_12.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_10_12.png)'
- en: 'Figure 10.12: Yolo annotation positioning'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12：Yolo 注释定位
- en: 'We start by loading the annotations for our training data:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载训练数据的注释：
- en: '[PRE28]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let’s inspect a few:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查几个：
- en: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B17574_10_13.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![包含桌子的图像 自动生成的描述](img/B17574_10_13.png)'
- en: 'Figure 10.13: Training data with annotations'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.13：带有注释的训练数据
- en: 'We extract the actual coordinates of the bounding boxes from the `bbox` column:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 `bbox` 列中提取边界框的实际坐标：
- en: '[PRE29]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s look at the array:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个数组：
- en: '[PRE30]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The next step is to extract the coordinates in Yolo format into separate columns:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是从 Yolo 格式中提取坐标到单独的列：
- en: '[PRE31]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The implementation from Ultralytics has some requirements on the structure of
    the dataset, specifically, where the annotations are stored and the folders for
    training/validation data.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Ultralytics 的实现对数据集的结构有一些要求，特别是注释存储的位置以及训练/验证数据的文件夹。
- en: 'The creation of the folders in the code below is fairly straightforward, but
    a more inquisitive reader is encouraged to consult the official documentation
    ([https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码中文件夹的创建相当简单，但鼓励更好奇的读者查阅官方文档（[https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)）：
- en: '[PRE32]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The next thing we do is install the Yolo package itself. If you are running
    this in a Kaggle Notebook or Colab, make sure to double-check GPU is enabled;
    Yolo installation will actually work without it, but you are likely to run into
    all sorts of timeouts and memory issues due to CPU versus GPU performance differences.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要做的是安装 Yolo 包本身。如果你在 Kaggle Notebook 或 Colab 中运行此代码，请确保已启用 GPU；实际上，即使没有
    GPU，Yolo 的安装也可以工作，但你可能会因为 CPU 与 GPU 性能差异而遇到各种超时和内存问题。
- en: '[PRE33]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We omit the output, as it is rather extensive. The last bit of preparation
    needed is the YAML configuration file, where we specify the training and validation
    data locations and the number of classes. We are only interested in detecting
    wheat heads and not distinguishing between different types, so we have one class
    (its name is only provided for notational consistency and can be an arbitrary
    string in this instance):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们省略了输出，因为它相当广泛。最后需要准备的是 YAML 配置文件，其中我们指定训练和验证数据的位置以及类别数。我们只对检测麦穗感兴趣，而不是区分不同类型，所以我们有一个类别（其名称仅用于符号一致性，在这种情况下可以是任意字符串）：
- en: '[PRE34]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'With that, we can start training our model:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们就可以开始训练我们的模型：
- en: '[PRE35]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Unless you are used to launching things from the command line, the incantation
    above is positively cryptic, so let’s discuss its composition in some detail:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 除非您习惯于从命令行启动事物，否则上述咒语肯定是晦涩难懂的，因此让我们详细讨论其组成：
- en: '`train.py` is the workhorse script for training a YoloV5 model, starting from
    pre-trained weights.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train.py` 是用于从预训练权重开始训练 YoloV5 模型的主脚本。'
- en: '`--img 512` means we want the original images (which, as you can see, we did
    not preprocess in any way) to be rescaled to 512x512\. For a competitive result,
    you should use a higher resolution, but this code was executed in a Kaggle Notebook,
    which has certain limitations on resources.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--img 512` 表示我们希望原始图像（如您所见，我们没有以任何方式预处理）被缩放到 512x512。为了获得有竞争力的结果，您应该使用更高的分辨率，但此代码是在
    Kaggle 笔记本中执行的，它对资源有一定的限制。'
- en: '`--batch` refers to the batch size in the training process.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--batch` 指的是训练过程中的批量大小。'
- en: '`--epochs 3` means we want to train the model for three epochs.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--epochs 3` 表示我们希望训练模型三个周期。'
- en: '`--workers 2` specifies the number of workers in the data loader. Increasing
    this number might help performance, but there is a known bug in version 6.0 (the
    most recent one available in the Kaggle Docker image, as of the time of this writing)
    when the number of workers is too high, even on a machine where more might be
    available.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--workers 2` 指定数据加载器中的工作线程数。增加此数字可能会帮助性能，但在版本 6.0（截至本文写作时在 Kaggle Docker 图像中可用的最新版本）中存在已知错误，当工作线程数过高时，即使在有更多可用资源的机器上也是如此。'
- en: '`--data wheat.yaml` is the file pointing to our data specification YAML file,
    defined above.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--data wheat.yaml` 是指向我们上面定义的数据规范 YAML 文件的文件。'
- en: '`--cfg "./yolov5/models/yolov5s.yaml"` specifies the model architecture and
    the corresponding set of weights to be used for initialization. You can use the
    ones provided with the installation (check the official documentation for details),
    or you can customize your own and keep them in the same `.yaml` format.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--cfg "./yolov5/models/yolov5s.yaml"` 指定模型架构和用于初始化的相应权重集。您可以使用与安装提供的版本（请参阅官方文档以获取详细信息），或者您可以自定义自己的并保持它们以相同的
    `.yaml` 格式。'
- en: '`--name` specifies where the resulting model is to be stored.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--name` 指定结果模型要存储的位置。'
- en: 'We break down the output of the training command below. First, the groundwork:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以下训练命令的输出分解。首先，基础工作：
- en: '[PRE36]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then comes the model. We see a summary of the architecture, the optimizer setup,
    and the augmentations used:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是模型。我们看到架构的摘要、优化器设置和使用的增强：
- en: '[PRE37]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This is followed by the actual training log:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这后面是实际的训练日志：
- en: '[PRE38]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The results from both training and validation stages can be examined; they
    are stored in the `yolov5` folder under `./yolov5/runs/train/yolov5x_fold0`:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和验证阶段的结果都可以进行检查；它们存储在 `yolov5` 文件夹下的 `./yolov5/runs/train/yolov5x_fold0`：
- en: '![](img/B17574_10_14.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_10_14.png)'
- en: 'Figure 10.14: Validation data with annotations'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.14：带有注释的验证数据
- en: 'Once we have trained the model, we can use the weights from the best performing
    model (Yolov5 has a neat functionality of automatically keeping both the best
    and last epoch model, storing them as `best.pt` and `last.pt`) to generate predictions
    on the test data:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了模型，我们可以使用表现最佳模型的权重（Yolov5 有一个很酷的功能，可以自动保留最佳和最后一个周期的模型，将它们存储为 `best.pt`
    和 `last.pt`）来生成测试数据上的预测：
- en: '[PRE39]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We will discuss the parameters that are specific to the inference stage:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论特定于推理阶段的参数：
- en: '`--weights` points to the location of the best weights from our model trained
    above.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--weights` 指向我们上面训练的模型中最佳权重的位置。'
- en: '`--conf 0.1` specifies which candidate bounding boxes generated by the model
    should be kept. As usual, it is a compromise between precision and recall (too
    low a threshold gives a high number of false positives, while moving the threshold
    too high means we might not find any wheat heads at all).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--conf 0.1`指定模型生成的候选边界框中哪些应该被保留。通常，这是一个在精确度和召回率之间的折衷（太低的阈值会导致大量误报，而将阈值调得太高则可能根本找不到任何麦穗头）。'
- en: '`--source` is the location of the test data.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--source`是测试数据的位置。'
- en: 'The labels created for our test images can be inspected locally:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 为我们的测试图像创建的标签可以在本地进行检查：
- en: '[PRE40]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This is what we might see:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们可能会看到的内容：
- en: '[PRE41]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let’s look at an individual prediction:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个单独的预测：
- en: '[PRE42]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'It has the following format:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有以下格式：
- en: '[PRE43]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This means that in image `2fd875eaa`, our trained model detected two bounding
    boxes (their coordinates are entries 2-5 in the row), with confidence scores above
    0.1 given at the end of the row.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在图像`2fd875eaa`中，我们的训练模型检测到了两个边界框（它们的坐标是行中的2-5项），并在行末给出了大于0.1的置信度分数。
- en: 'How do we go about combining the predictions into a submission in the required
    format? We start by defining a helper function that helps us convert the coordinates
    from the yolo format to coco (as required in this competition): it is a matter
    of simple rearrangement of the order and normalizing to the original range of
    values by multiplying the fractions by the image size:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将预测组合成所需格式的提交？我们首先定义一个辅助函数，帮助我们将坐标从yolo格式转换为coco（如本竞赛所需）：这是一个简单的顺序重排和通过乘以图像大小来归一化到原始值范围的问题：
- en: '[PRE44]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We then proceed to generate a submission file:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们继续生成一个提交文件：
- en: We loop over the files listed above.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们遍历上述列出的文件。
- en: For each file, all rows are converted into strings in the required format (one
    row represents one bounding box detected).
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个文件，所有行都被转换为所需格式的字符串（一行代表一个检测到的边界框）。
- en: The rows are then concatenated into a single string corresponding to this file.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些行随后被连接成一个字符串，对应于这个文件。
- en: 'The code is as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下：
- en: '[PRE45]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let’s see what it looks like:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它的样子：
- en: '[PRE46]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The generated `submission.csv` file completes our pipeline.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的`submission.csv`文件完成了我们的流程。
- en: 'In this section, we have demonstrated how to use a YoloV5 to solve the problem
    of object detection: how to handle annotations in different formats, how to customize
    a model for a specific task, train it, and evaluate the results.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了如何使用YoloV5解决目标检测问题：如何处理不同格式的注释，如何为特定任务定制模型，训练它，并评估结果。
- en: Based on this knowledge, you should be able to start working with object detection
    problems.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些知识，你应该能够开始处理目标检测问题。
- en: 'We now move on to the third popular class of computer vision tasks: semantic
    segmentation.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向计算机视觉任务中的第三大流行类别：语义分割。
- en: Semantic segmentation
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语义分割
- en: The easiest way to think about **segmentation** is that it classifies each pixel
    in an image, assigning it to a corresponding class; combined, those pixels form
    areas of interest, such as regions with disease on an organ in medical images.
    By contrast, object detection (discussed in the previous section) classifies patches
    of an image into different object classes and creates bounding boxes around them.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到**分割**，最简单的方式是它将图像中的每个像素分类，将其分配给相应的类别；这些像素组合在一起形成感兴趣的区域，例如医学图像中器官上的病变区域。相比之下，目标检测（在上一节中讨论）将图像的片段分类到不同的对象类别，并在它们周围创建边界框。
- en: We will demonstrate the modeling approach using data from the *Sartorius – Cell
    Instance Segmentation* competition ([https://www.kaggle.com/c/sartorius-cell-instance-segmentation](https://www.kaggle.com/c/sartorius-cell-instance-segmentation)).
    In this one, the participants were tasked to train models for instance segmentation
    of neural cells using a set of microscopy images.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自*Sartorius – Cell Instance Segmentation*竞赛（[https://www.kaggle.com/c/sartorius-cell-instance-segmentation](https://www.kaggle.com/c/sartorius-cell-instance-segmentation)）的数据展示建模方法。在这个竞赛中，参与者被要求使用一组显微镜图像训练用于神经细胞实例分割的模型。
- en: Our solution will be built around **Detectron2**, a library created by Facebook
    AI Research that supports multiple detection and segmentation algorithms.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将围绕**Detectron2**构建解决方案，这是一个由Facebook AI Research创建的库，支持多种检测和分割算法。
- en: Detectron2 is a successor to the original Detectron library ([https://github.com/facebookresearch/Detectron/](https://github.com/facebookresearch/Detectron/))
    and the Mask R-CNN project ([https://github.com/facebookresearch/maskrcnn-benchmark/](https://github.com/facebookresearch/maskrcnn-benchmark/)).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Detectron2 是原始 Detectron 库 ([https://github.com/facebookresearch/Detectron/](https://github.com/facebookresearch/Detectron/))
    和 Mask R-CNN 项目 ([https://github.com/facebookresearch/maskrcnn-benchmark/](https://github.com/facebookresearch/maskrcnn-benchmark/))
    的继任者。
- en: 'We begin by installing the extra packages:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先安装额外的包：
- en: '[PRE48]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We install `pycocotools` ([https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools](https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools)),
    which we will need to format the annotations, and Detectron2 ([https://github.com/facebookresearch/detectron2](https://github.com/facebookresearch/detectron2)),
    our workhorse in this task.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们安装 `pycocotools` ([https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools](https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools))，这是我们格式化注释和
    Detectron2（我们在这个任务中的工作马）所需的，以及 Detectron2 ([https://github.com/facebookresearch/detectron2](https://github.com/facebookresearch/detectron2))。
- en: 'Before we can train our model, we need a bit of preparation: the annotations
    need to be converted from the **run-length encoding** (**RLE**) format provided
    by the organizers to the COCO format required as input for Detectron2\. The basic
    idea behind RLE is saving space: creating a segmentation means marking a group
    of pixels in a certain manner. Since an image can be thought of as an array, this
    area can be denoted by a series of straight lines (row- or column-wise).'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够训练我们的模型之前，我们需要做一些准备工作：注释需要从组织者提供的 **run-length encoding** (**RLE**) 格式转换为
    Detectron2 所需的 COCO 格式。RLE 的基本思想是节省空间：创建一个分割意味着以某种方式标记一组像素。由于图像可以被视为一个数组，这个区域可以用一系列直线（行或列方向）表示。
- en: 'You can encode each of those lines by listing the indices, or by specifying
    a starting position and the length of the subsequent contiguous block. A visual
    example is given below:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过列出索引或指定起始位置和后续连续块长度来编码每一行。下面给出了一个视觉示例：
- en: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B17574_10_15.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![包含桌子的图像 自动生成的描述](img/B17574_10_15.png)'
- en: 'Figure 10.15: Visual representation of RLE'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.15：RLE 的视觉表示
- en: Microsoft’s **Common Objects in Context** (**COCO**) format is a specific JSON
    structure dictating how labels and metadata are saved for an image dataset. Below,
    we demonstrate how to convert RLE to COCO and combine it with a *k*-fold validation
    split, so we get the required train/validation pair of JSON files for each fold.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 微软的 **Common Objects in Context** (**COCO**) 格式是一种特定的 JSON 结构，它规定了图像数据集中标签和元数据的保存方式。下面，我们演示如何将
    RLE 转换为 COCO 并与 *k*-fold 验证分割结合，以便为每个分割获得所需的训练/验证 JSON 文件对。
- en: 'Let’s begin:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从这里开始：
- en: '[PRE49]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We need three functions to go from RLE to COCO. First, we need to convert from
    RLE to a binary mask:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要三个函数将 RLE 转换为 COCO。首先，我们需要将 RLE 转换为二值掩码：
- en: '[PRE50]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The second one converts a binary mask to RLE:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个将二值掩码转换为 RLE：
- en: '[PRE51]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Finally, we combine the two in order to produce the COCO output:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将两者结合起来以生成 COCO 输出：
- en: '[PRE52]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We split our data into non-overlapping folds:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的数据分割成非重叠的分割：
- en: '[PRE53]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We can now loop over the folds:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以遍历分割：
- en: '[PRE54]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The reason why the loop has to be executed in pieces is the size limit of the
    Kaggle environment: the maximum size of Notebook output is limited to 20 GB, and
    5 folds with 2 files (training/validation) for each fold meant a total of 10 JSON
    files, exceeding that limit.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 循环必须分块执行的原因是 Kaggle 环境的大小限制：笔记本输出的最大大小限制为 20 GB，5 个分割，每个分割有 2 个文件（训练/验证），总共
    10 个 JSON 文件，超过了这个限制。
- en: Such practical considerations are worth keeping in mind when running code in
    a Kaggle Notebook, although for such “preparatory” work, you can, of course, produce
    the results elsewhere, and upload them as Kaggle Datasets afterward.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 Kaggle 笔记本中运行代码时，这些实际考虑因素值得记住，尽管对于这种“预备”工作，您当然可以在其他地方产生结果，并在之后将其作为 Kaggle
    数据集上传。
- en: 'With the splits produced, we can move toward training a Detectron2 model for
    our dataset. As usual, we start by loading the necessary packages:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成分割后，我们可以开始为我们的数据集训练 Detectron2 模型。像往常一样，我们首先加载必要的包：
- en: '[PRE55]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'While the number of imports from Detectron2 can seem intimidating at first,
    their function will become clear as we progress with the task definition; we start
    by specifying paths to the input data folder, annotations folder, and a YAML file
    defining our preferred model architecture:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一开始从 Detectron2 导入的数量可能看起来令人畏惧，但随着我们对任务定义的深入，它们的功能将变得清晰；我们首先指定输入数据文件夹、注释文件夹和定义我们首选模型架构的
    YAML 文件路径：
- en: '[PRE56]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'One point worth mentioning here is the iterations parameter (`nof_iters` above).
    Usually, model training is parametrized in terms of the number of epochs, in other
    words, complete passes through the training data. Detectron2 is engineered differently:
    one iteration refers to one mini-batch and different mini-batch sizes are used
    in different parts of the model.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这里值得提一下的是迭代参数（`nof_iters` 上方）。通常，模型训练是以完整遍历训练数据的次数（即周期数）来参数化的。Detectron2 的设计不同：一次迭代指的是一个
    mini-batch，模型的不同部分使用不同的 mini-batch 大小。
- en: 'In order to ensure the results are reproducible, we fix random seeds used by
    different components of the model:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保结果可重复，我们固定了模型不同组件使用的随机种子：
- en: '[PRE57]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The competition metric was the mean average precision at different **intersection
    over union** (**IoU**) thresholds. As a refresher from *Chapter 5*, *Competition
    Tasks and Metrics*, the IoU of a proposed set of object pixels and a set of true
    object pixels is calculated as:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 竞赛指标是不同**交并比**（**IoU**）阈值下的平均平均精度。作为对 *第五章*，*竞赛任务和指标* 的回顾，建议的一组对象像素与一组真实对象像素的
    IoU 计算如下：
- en: '![](img/B17574_10_001.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图片 B17574_10_001](img/B17574_10_001.png)'
- en: The metric sweeps over a range of IoU thresholds, at each point calculating
    an average precision value. The threshold values range from 0.5 to 0.95, with
    increments of 0.05.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标遍历一系列 IoU 阈值，在每个点上计算平均精度值。阈值值从 0.5 到 0.95，增量为 0.05。
- en: At each threshold value, a precision value is calculated based on the number
    of **true positives** (**TP**), **false negatives** (**FN**), and **false positives**
    (**FP**) resulting from comparing the predicted object with all ground truth objects.
    Lastly, the score returned by the competition metric is the mean taken over the
    individual average precisions of each image in the test dataset.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个阈值值处，根据预测对象与所有地面真实对象比较后产生的**真正例**（**TP**）、**假阴性**（**FN**）和**假阳性**（**FP**）的数量计算一个精度值。最后，竞赛指标返回的分数是测试数据集中每个图像的个体平均精度的平均值。
- en: 'Below, we define the functions necessary to calculate the metric and use it
    directly inside the model as the objective function:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，我们定义计算该指标所需的函数，并将其直接用于模型内部作为目标函数：
- en: '[PRE58]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'With the metric defined, we can use it in the model:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 指标定义后，我们可以在模型中使用它：
- en: '[PRE59]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'This gives us the basis for creating a `Trainer` object, which is the workhorse
    of our solution built around Detectron2:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们创建 `Trainer` 对象提供了基础，它是围绕 Detectron2 构建的解决方案的核心：
- en: '[PRE60]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We now proceed to load the training/validation data in Detectron2 style:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在继续以 Detectron2 风格加载数据集的训练/验证数据：
- en: '[PRE61]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Before we instantiate a Detectron2 model, we need to take care of configuring
    it. Most of the values can be left at default values (at least, in a first pass);
    if you decide to tinker a bit more, start with `BATCH_SIZE_PER_IMAGE` (for increased
    generalization performance) and `SCORE_THRESH_TEST` (to limit false negatives):'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实例化 Detectron2 模型之前，我们需要注意配置它。大多数值都可以保留为默认值（至少在第一次尝试时是这样）；如果你决定进一步调整，从 `BATCH_SIZE_PER_IMAGE`（为了提高泛化性能）和
    `SCORE_THRESH_TEST`（为了限制假阴性）开始：
- en: '[PRE62]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Training a model is straightforward:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型很简单：
- en: '[PRE63]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'You will notice that the output during training is rich in information about
    the progress of the procedure:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到训练过程中的输出含有丰富的关于该过程进度的信息：
- en: '![Obraz zawierający tekst  Opis wygenerowany automatycznie](img/B17574_10_16.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本的图片 自动生成的描述](img/B17574_10_16.png)'
- en: 'Figure 10.16: Training output from Detectron2'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.16：Detectron2 的训练输出
- en: 'Once the model is trained, we can save the weights and use them for inference
    (potentially in a separate Notebook – see the discussion earlier in this chapter)
    and submission preparation. We start by adding new parameters that allow us to
    regularize the prediction, setting confidence thresholds and minimal mask sizes:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，我们可以保存权重并用于推理（可能在一个单独的笔记本中 – 参见本章前面的讨论）和提交准备。我们首先添加新的参数，允许我们正则化预测，设置置信度阈值和最小掩码大小：
- en: '[PRE64]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We need a helper function for encoding a single mask into RLE format:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个辅助函数来将单个掩码编码成 RLE 格式：
- en: '[PRE65]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Below is the main function for producing all masks per image, filtering out
    the dubious ones (with confidence scores below `THRESHOLDS`) with small areas
    (containing fewer pixels than `MIN_PIXELS`):'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是生成每张图像所有掩码的主要函数，过滤掉可疑的（置信度分数低于`THRESHOLDS`）和面积小的（包含的像素少于`MIN_PIXELS`）：
- en: '[PRE66]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We then prepare the lists where image IDs and masks will be stored:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后准备存储图像ID和掩码的列表：
- en: '[PRE67]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Competitions with large image sets – like the ones discussed in this section
    – often require training models for longer than 9 hours, which is the time limit
    imposed in Code competitions (see [https://www.kaggle.com/docs/competitions](https://www.kaggle.com/docs/competitions)).
    This means that training a model and running inference within the same Notebook
    becomes impossible. A typical workaround is to run a training Notebook/script
    first as a standalone Notebook in Kaggle, Google Colab, GCP, or locally. The output
    of this first Notebook (the trained weights) is used as input to the second one,
    in other words, to define the model used for predictions.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 大型图像集的竞赛——如本节中讨论的——通常需要训练模型超过9小时，这是代码竞赛中规定的时限（见[https://www.kaggle.com/docs/competitions](https://www.kaggle.com/docs/competitions)）。这意味着在同一笔记本中训练模型和运行推理变得不可能。一个典型的解决方案是首先作为一个独立的笔记本在Kaggle、Google
    Colab、GCP或本地运行训练笔记本/脚本。第一个笔记本的输出（训练好的权重）被用作第二个笔记本的输入，换句话说，用于定义用于预测的模型。
- en: 'We proceed in that manner by loading the weights of our trained model:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过加载我们训练好的模型的权重以这种方式继续进行：
- en: '[PRE68]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We can visualize some of the predictions:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过加载我们训练好的模型的权重来可视化一些预测：
- en: '[PRE69]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Here is an example:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子：
- en: '![](img/B17574_10_17.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![图片B17574_10_17.png](img/B17574_10_17.png)'
- en: 'Figure 10.17: Visualizing a sample prediction from Detectron2 alongside the
    source image'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.17：可视化Detectron2的样本预测与源图像
- en: 'With the helper functions defined above, producing the masks in RLE format
    for submission is straightforward:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上面定义的辅助函数，以RLE格式生成提交的掩码非常简单：
- en: '[PRE70]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Here are the first few rows of the final submission:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是最终提交的前几行：
- en: '![Obraz zawierający tekst  Opis wygenerowany automatycznie](img/B17574_10_18.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本的图像 自动生成的描述](img/B17574_10_18.png)'
- en: 'Figure 10.18: Formatted submission from a trained Detectron2 model'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.18：训练好的Detectron2模型的格式化提交
- en: We have reached the end of the section. The pipeline above demonstrates how
    to set up a semantic segmentation model and train it. We have used a small number
    of iterations, but in order to achieve competitive results, longer training is
    necessary.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经到达了本节的结尾。上面的流程演示了如何设置语义分割模型并进行训练。我们使用了少量迭代，但要达到有竞争力的结果，需要更长时间的训练。
- en: '![](img/Laura_Fink.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![Laura Fink的图片](img/Laura_Fink.png)'
- en: Laura Fink
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: Laura Fink
- en: '[https://www.kaggle.com/allunia](https://www.kaggle.com/allunia)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/allunia](https://www.kaggle.com/allunia)'
- en: To wrap up this chapter, let’s see what Kaggler Laura Fink has to say about
    her time on the platform. As well as being a Notebooks Grandmaster and producing
    many masterful Notebooks, she is also Head of Data Science at MicroMata.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结本章，让我们看看Kaggler Laura Fink对她在平台上的时间的看法。她不仅是Notebooks大师，制作了许多精湛的Notebooks，还是MicroMata的数据科学负责人。
- en: What’s your favorite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 你最喜欢的竞赛类型是什么？为什么？在技术和解决方法方面，你在Kaggle上的专长是什么？
- en: '*My favorite competitions are those that want to yield something good to humanity.
    I especially like all healthcare-related challenges. Nonetheless, each competition
    feels like an adventure for me with its own puzzles to be solved. I really enjoy
    learning new skills and exploring new kinds of datasets or problems. Consequently,
    I’m not focused on specific techniques but rather on learning something new. I
    think I’m known for my strengths in exploratory data analysis (EDA).*'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '*我最喜欢的竞赛是那些能为人类带来好处的竞赛。我特别喜欢所有与健康相关的挑战。然而，对我来说，每个竞赛都像一场冒险，有自己的谜题需要解决。我真的很享受学习新技能和探索新的数据集或问题。因此，我并不专注于特定的技术，而是专注于学习新事物。我认为我因在探索性数据分析（EDA）方面的优势而闻名。*'
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何参加Kaggle竞赛的？这种方法和你在日常工作中所做的工作有什么不同？
- en: '*When entering a competition, I start by reading the problem statement and
    the data description. After browsing through the forum and public Notebooks for
    collecting ideas, I usually start by developing my own solutions. In the initial
    phase, I spend some time on EDA to search for hidden groups and get some intuition.
    This helps quite a lot in setting up a proper validation strategy, which I believe
    is the foundation of all remaining steps. Then, I start to iterate through different
    parts of the machine learning pipeline like feature engineering or preprocessing,
    improving the model architecture, asking questions about the data collection,
    searching for leakages, doing more EDA, or building ensembles. I try to improve
    my solution in a greedy fashion. Kaggle competitions are very dynamic and one
    needs to try out diverse ideas and different solutions to survive in the end.*'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '*当参加比赛时，我首先阅读问题陈述和数据描述。浏览论坛和公开的笔记本以收集想法后，我通常开始开发自己的解决方案。在初始阶段，我花了一些时间进行EDA（探索性数据分析）以寻找隐藏的组并获取一些直觉。这有助于设置适当的验证策略，我认为这是所有后续步骤的基础。然后，我开始迭代机器学习管道的不同部分，如特征工程或预处理，改进模型架构，询问数据收集的问题，寻找泄漏，进行更多的EDA，或构建集成。我试图以贪婪的方式改进我的解决方案。Kaggle比赛非常动态，需要尝试不同的想法和不同的解决方案才能最终生存下来。*'
- en: '*This is definitely different from my day-to-day work, where the focus is more
    on gaining insights from data and finding simple but effective solutions to improve
    business processes. Here, the task is often more complex than the models used.
    The problem to be solved has to be defined very clearly, which means that one
    has to discuss with experts of different backgrounds which goals should be reached,
    which processes are involved, and how the data needs to be collected or fused.
    Compared to Kaggle competitions, my daily work needs much more communication than
    machine learning skills.*'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '*这肯定与我的日常工作不同，我的日常工作更侧重于从数据中获得见解，并找到简单但有效的解决方案来改进业务流程。在这里，任务通常比使用的模型更复杂。要解决的问题必须非常明确地定义，这意味着必须与不同背景的专家讨论应达到的目标，涉及哪些流程，以及数据需要如何收集或融合。与Kaggle比赛相比，我的日常工作需要更多的沟通而不是机器学习技能。*'
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 告诉我们您参加的一个特别具有挑战性的比赛，以及您使用了哪些见解来应对这项任务。
- en: '*The* G2Net Gravitational Wave Detection *competition was one of my favorites.
    The goal was to detect simulated gravitational wave signals that were hidden in
    noise originating from detector components and terrestrial forces. An important
    insight during this competition was that you should have a critical look at standard
    ways to analyze data and try out your own ideas. In the papers I read, the data
    was prepared mainly by using the Fourier or Constant-Q transform after whitening
    the data and applying a bandpass filter.*'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '*G2Net引力波探测比赛是我最喜欢的之一。目标是检测隐藏在来自探测器组件和地球力量的噪声中的模拟引力波信号。在这场比赛中的一个重要见解是，你应该批判性地看待分析数据的标准方法，并尝试自己的想法。在我阅读的论文中，数据主要是通过使用傅里叶变换或常Q变换，在数据白化并应用带通滤波器后准备的。*'
- en: '*It came out very quickly that whitening was not helpful, as it used spline
    interpolation of the Power Spectral Density, which was itself very noisy. Fitting
    polynomials to small subsets of noisy data adds another source of errors because
    of overfitting.*'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '*很快就很明显，白化没有帮助，因为它使用了功率谱密度的样条插值，这本身就很嘈杂。将多项式拟合到噪声数据的小子集会增加另一个错误来源，因为过度拟合。*'
- en: '*After dropping the whitening, I tried out different hyperparameters of the
    Constant-Q transform, which turned out to be the leading method in the forum and
    public* *Notebooks for a long time. As there were two sources of gravitational
    waves that can be covered by different ranges of Q-values, I tried out an ensemble
    of models that differed in these hyperparameters. This turned out to be helpful
    in improving my score, but then I reached a limit. The Constant-Q transform applies
    a series of filters to time series and transforms them into the frequency domain.
    I started to ask myself if there was a method that does these filtering tasks
    in a better, more flexible way. It was at the same time that the idea of using
    1 dim CNNs came up in the community, and I loved it. We all know that filters
    of 2 dim CNNs are able to detect edges, lines, and textures given image data.
    The same could be done with “classical” filters like the Laplace or Sobel filter.
    For this reason, I asked myself: can’t we use the 1dCNN to learn the most important
    filters on its own, instead of applying transformations that are already fixed
    somehow?*'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '*在去除白化之后，我尝试了不同超参数的Constant-Q变换，这长期以来一直是论坛和公共* *Notebooks中的领先方法。由于有两种引力波源可以覆盖不同的Q值范围，我尝试了一个在这些超参数上有所不同的模型集合。这证明在提高我的分数方面很有帮助，但后来我达到了一个极限。Constant-Q变换对时间序列应用一系列滤波器，并将它们转换到频域。我开始问自己，是否有一种方法可以以更好、更灵活的方式执行这些滤波任务。就在这时，社区中提出了使用1维卷积神经网络的想法，我非常喜欢它。我们都知道，2维卷积神经网络的滤波器能够根据图像数据检测边缘、线条和纹理。同样，可以使用“经典”滤波器，如拉普拉斯或索贝尔滤波器。因此，我自问：我们能否使用1维CNN来学习最重要的滤波器，而不是应用某种方式已经固定的变换？*'
- en: '*I was not able to get my 1 dim CNN solution to work, but it turned out that
    many top teams managed it well. The G2Net competition was one of my favorites
    even though I missed out on the goal of winning a medal. However, the knowledge
    I gained along the way and the lesson I learned about so-called standard approaches
    were very valuable.*'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '*我无法让我的1维卷积神经网络解决方案工作，但结果证明许多顶级团队都做得很好。G2Net比赛是我最喜欢的之一，尽管我错过了赢得奖牌的目标。然而，我在过程中获得的知识和关于所谓标准方法的教训都是非常宝贵的。*'
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是否帮助了你的职业生涯？如果是的话，是如何帮助的？
- en: '*I started my first job after university as a Java software developer even
    though I already had my first contact with machine learning during my master’s
    thesis. I was interested in doing more data analytics, but at that time, there
    were almost no data science jobs, or they were not named this way. When I heard
    about Kaggle the first time, I was trapped right from the start. Since then, I
    often found myself on Kaggle during the evenings to have some fun. It was not
    my intent to change my position at that time, but then a research project came
    up that needed machine learning skills. I was able to show that I was a suitable
    candidate for this project because of the knowledge I gained by participating
    on Kaggle. This turned out to be the entry point for my data science career.*'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在大学毕业后开始了我的第一份工作，成为一名Java软件开发者，尽管我在硕士论文期间已经接触过机器学习。我对进行更多数据分析很感兴趣，但在那时，几乎没有什么数据科学的工作，或者它们没有被这样命名。当我第一次听说Kaggle时，我立刻被它吸引。从那时起，我经常在晚上去Kaggle上玩。当时我并没有打算改变我的职位，但后来有一个研究项目需要机器学习技能。我能够通过在Kaggle上参与获得的知识证明我是这个项目的合适人选。这最终成为了我数据科学生涯的起点。*'
- en: '*Kaggle has always been a great place for me to try out ideas, learn new methods
    and tools, and gain practical experience. The skills I obtained this way have
    been quite helpful for data science projects at work. It’s like a boost of knowledge,
    as Kaggle provides a sandbox for you to try out different ideas and to be creative
    without risk. Failing in a competition means that there was at least one lesson
    to learn, but failing in a project can have a huge negative impact on yourself
    and other people.*'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kaggle一直是我尝试新想法、学习新方法和工具、获取实践经验的好地方。通过这种方式获得的能力对我在工作中的数据科学项目非常有帮助。这就像是一次知识上的提升，因为Kaggle为你提供了一个沙盒，让你可以尝试不同的想法，发挥创造力而无需承担风险。在比赛中失败意味着至少有一个教训可以学习，但项目中的失败可能会对自己和其他人产生巨大的负面影响。*'
- en: '*Besides taking part in competitions, another great way to build up your portfolio
    is to write Notebooks. In doing so, you can show the world how you approach problems
    and how to communicate insights and conclusions. The latter is very important
    when you have to work with management, clients, and experts from different backgrounds.*'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '*除了参加比赛外，另一种建立你个人作品集的绝佳方式是编写Notebooks。这样做，你可以向世界展示你如何解决问题，以及如何传达洞察力和结论。当你必须与管理人员、客户和来自不同背景的专家合作时，后者非常重要。*'
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的经验中，不经验的Kagglers通常忽略了什么？你现在知道什么，而当你最初开始时希望知道的呢？
- en: '*I think many beginners that enter competitions are seduced by the public leaderboard
    and build their models without having a good validation strategy. While measuring
    their success on the leaderboard, they are likely overfitting to the public test
    data. After the end of the competition, their models are not able to generalize
    to the unseen private test data, and they often fall down hundreds of places.
    I still remember how frustrated I was during the* Mercedes-Benz Greener Manufacturing
    *competition as I was not able to climb up the public leaderboard. But when the
    final standings came out, it was a big surprise how many people were shuffled
    up and down the leaderboard. Since then, I always have in mind that a proper validation
    scheme is very important for managing the challenges of under- and overfitting.*'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '*我认为许多参加比赛的初学者被公开排行榜所吸引，没有良好的验证策略就构建模型。当他们在排行榜上衡量自己的成功时，他们很可能会对公开测试数据进行过度拟合。比赛结束后，他们的模型无法泛化到未见的私有测试数据，他们通常会下降数百名。我仍然记得在*梅赛德斯-奔驰绿色制造*比赛中，由于我无法爬升公开排行榜，我有多么沮丧。但当最终排名公布时，人们在上榜和下榜之间的波动让我感到惊讶。从那时起，我总是牢记，一个适当的验证方案对于管理欠拟合和过拟合的挑战非常重要。*'
- en: What mistakes have you made in competitions in the past?
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 你在比赛中犯过哪些错误？
- en: '*My biggest mistake so far was to spend too much time and effort on the details
    of my solution at the beginning of a competition. Indeed, it’s much better to
    iterate fast through diverse and different ideas after building a proper validation
    strategy. That way, it’s easier and faster to find promising directions for improvements
    and the danger of getting stuck somewhere is much smaller.*'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '*我迄今为止犯的最大错误是在比赛开始时花费太多时间和精力在解决方案的细节上。实际上，在建立适当的验证策略之后，快速迭代不同的想法会更好。这样，找到改进的潜在方向更容易、更快，陷入某个地方的危险也小得多。*'
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis or machine learning?
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 您会推荐使用哪些特定的工具或库来进行数据分析或机器学习？
- en: '*There are a lot of common tools and libraries you can learn and practice when
    becoming active in the Kaggle community and I can only recommend them all. It’s
    important to stay flexible and to learn about their advantages and disadvantages.
    This way, your solutions don’t depend on your tools, but rather on your ideas
    and creativity.*'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '*在Kaggle社区活跃时，你可以学习和实践许多常见的工具和库，我唯一能做的就是推荐它们。保持灵活并了解它们的优缺点很重要。这样，你的解决方案不依赖于你的工具，而更多地依赖于你的想法和创造力。*'
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们参加比赛时，他们应该记住或做哪件事是最重要的？
- en: '*Data science is not about building models, but rather about understanding
    the data and the way it was collected. Many competitions I have entered so far
    showed leakages or had hidden groups in the test data that one could find with
    exploratory data analysis.*'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据科学不是关于构建模型，而是关于理解数据和收集数据的方式。我迄今为止参加的许多比赛都显示了数据泄露或测试数据中隐藏的组，这些组可以通过探索性数据分析找到。*'
- en: Summary
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we gave you an overview of the most important topics related
    to computer vision from a Kaggle competition angle. We introduced augmentations,
    an important class of techniques used for extending the generalization capabilities
    of an algorithm, and followed by demonstrating end-to-end pipelines for three
    of the most frequent problems: image classification, object detection, and semantic
    segmentation.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从Kaggle比赛的角度为您概述了与计算机视觉相关的重要主题。我们介绍了增强，这是一种用于扩展算法泛化能力的技巧的重要类别，随后展示了三个最常见问题的端到端管道：图像分类、目标检测和语义分割。
- en: In the next chapter, we switch our attention to natural language processing,
    another extremely broad and popular category of problems.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将注意力转向自然语言处理，这是另一个极其广泛且受欢迎的问题类别。
- en: Join our book’s Discord space
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 加入书籍的 Discord 工作空间，每月与作者进行一次 *问我任何问题* 的活动：
- en: '[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)'
- en: '![](img/QR_Code40480600921811704671.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code40480600921811704671.png)'
