- en: '*Chapter 2*: Machine Learning Basics'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第二章*：机器学习基础'
- en: This chapter covers some basic concepts of machine learning that will be used
    and referenced in this book. This is the bare minimum you need to know in order
    to use DataRobot effectively. Experienced data scientists can safely skip this
    chapter. It is not the intention of this chapter to give you a comprehensive understanding
    of statistics or machine learning, but just a refresher of some key ideas and
    concepts. Also, the focus is on practical aspects of what you need to know in
    order to understand the core ideas without going into too much detail. It might
    be tempting to jump in and let DataRobot automatically build the models, but doing
    that without a basic understanding could backfire. If you are leading a data science
    team, please make sure that you have experienced data scientists in your teams
    who are mentoring others and that there are other governance processes in place.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了本书中将使用和引用的一些机器学习基本概念。这是使用DataRobot有效所需的最基本知识。经验丰富的数据科学家可以安全地跳过本章。本章的目的不是让你对统计学或机器学习有一个全面的理解，而是对一些关键思想和概念进行复习。此外，重点是了解你需要了解的实践方面，以便理解核心思想，而不深入细节。你可能想跳进去，让DataRobot自动构建模型，但如果没有基本理解，这样做可能会适得其反。如果你是数据科学团队的领导者，请确保你的团队中有经验丰富的数据科学家，他们正在指导他人，并且已经建立了其他治理流程。
- en: Some of these concepts will come up again during the hands-on examples, but
    we are covering many concepts here that might not come up during a specific example,
    but might come up in relation to your project at some point. The topics listed
    here can be used as a guide to determine some of the basic knowledge that you
    require in order to start using powerful tools such as DataRobot.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一些这些概念将在实际操作示例中再次出现，但我们在这里涵盖了可能不会在特定示例中出现，但可能在某个时候与你的项目相关的许多概念。这里列出的主题可以用作指南，以确定你开始使用像DataRobot这样的强大工具所需的一些基本知识。
- en: 'By the end of this chapter, you will have learned some of the core concepts
    you need to know to use DataRobot effectively. In this chapter, we''re going to
    cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将学习到一些使用DataRobot有效所需的核心概念。在本章中，我们将涵盖以下主要主题：
- en: Data preparation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Data visualization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化
- en: Machine learning algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习算法
- en: Performance metrics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能指标
- en: Understanding the results
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解结果
- en: Data preparation
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: Before an algorithm can be applied to a dataset, the dataset needs to fit a
    certain pattern. The dataset also needs to be free of errors. Certain methods
    and techniques are used to ensure that the dataset is ready for the algorithms,
    and this will be the focus of this section.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在算法可以应用于数据集之前，数据集需要符合一定的模式。数据集还需要没有错误。某些方法和技术被用来确保数据集为算法做好准备，这将是本节的重点。
- en: Supervised learning dataset
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习数据集
- en: 'Since DataRobot mostly works with supervised learning problems, we will only
    focus on datasets for supervised machine learning (other types will be covered
    in a later section). In a supervised machine learning problem, we provide all
    the answers as part of the dataset. Imagine a table of data where each row represents
    a set of clues with their corresponding answers (*Figure 2.1*):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DataRobot主要处理监督学习问题，因此我们只会关注监督机器学习的数据集（其他类型将在后面的章节中介绍）。在监督机器学习问题中，我们将所有答案作为数据集的一部分提供。想象一下一张数据表，其中每一行代表一组线索及其相应的答案（*图2.1*）：
- en: '![Figure 2.1 – Supervised learning dataset'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.1 – 监督学习数据集'
- en: '](img/Figure_2.1_B17159.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.1_B17159.jpg)'
- en: Figure 2.1 – Supervised learning dataset
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 监督学习数据集
- en: This dataset is made up of columns that contain clues (these are called **features**),
    and there is a column with the answers (this is called **target**). Given a dataset
    that looks like this, the algorithm learns how to produce the right answer given
    a set of clues. No matter what form your data is in, your task is to first transform
    it to make it look like the table in *Figure 2.1*. Note that the clues that you
    have might be spread across multiple databases or Excel files. You will have to
    compile all of that information into one table. If the datasets you have are complex,
    you will need to use languages such as SQL, tools such as **Python** **Pandas**,
    or **Excel**, or tools such as **Paxata**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集由包含线索的列（这些被称为**特征**）和一个包含答案的列（这被称为**目标**）组成。给定一个类似这样的数据集，算法学习如何根据一组线索产生正确的答案。无论您的数据以何种形式存在，您的任务首先是将其转换成*图2.1*中的表格样子。请注意，您拥有的线索可能分布在多个数据库或Excel文件中。您将不得不将这些信息全部汇总到一个表格中。如果您拥有的数据集很复杂，您将需要使用SQL等语言，**Python
    Pandas**或**Excel**等工具，或**Paxata**等工具。
- en: Time series datasets
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列数据集
- en: 'Time series or forecasting problems have time as a key component of their datasets.
    They are similar to the supervised learning datasets, with slight differences,
    as shown in *Figure 2.2*:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列或预测问题的时间是其数据集的关键组成部分。它们与监督学习数据集类似，略有不同，如*图2.2*所示：
- en: '![Figure 2.2 – Time series dataset'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.2 – 时间序列数据集'
- en: '](img/Figure_2.2_B17159.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.2 – 时间序列数据集](img/Figure_2.2_B17159.jpg)'
- en: Figure 2.2 – Time series dataset
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – 时间序列数据集
- en: You need to make sure that your time series datasets appear as shown in the
    preceding diagram. It should have a date or time-based column, and a column with
    the series values you are trying to forecast, and a set of clues as needed. You
    can also add columns that help to categorize different series, if there are multiple
    time series that you need to forecast. For example, you might be interested in
    forecasting units sold for dates 5 and 6\. If your data is in some other form,
    it needs to be transformed to look like the preceding diagram.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要确保您的时间序列数据集看起来与前面的图表一样。它应该有一个基于日期或时间的列，一个包含您试图预测的序列值的列，以及所需的一组线索。如果您需要预测多个时间序列，您还可以添加帮助分类不同序列的列。例如，您可能对预测第5天和第6天的销量感兴趣。如果您的数据以其他形式存在，它需要被转换成前面图表的样子。
- en: Data cleansing
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清理
- en: 'The data that comes to you will typically have errors in it. For example, you
    might have text in a field that is supposed to contain numbers. You might see
    a price column where the values may contain a $ sign on occasion, but no sign
    at other times. DataRobot can catch some of these, but there are times when an
    automated tool will not catch these, so you need to look and analyze the dataset
    carefully. It is useful to sometimes upload your data to DataRobot to see what
    it finds, and then use its analysis to determine the next steps. Some of this
    cleansing will need to be performed outside DataRobot, so be prepared to iterate
    a few times to get the data set up correctly. Common issues to watch out for include
    the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您收到的数据通常会有错误。例如，您可能会在应该包含数字的字段中看到文本。您可能会看到价格列中的值有时包含美元符号，但有时没有符号。DataRobot可以捕捉到其中的一些错误，但有时自动化工具无法捕捉到这些错误，因此您需要仔细查看和分析数据集。有时将您的数据上传到DataRobot以查看它发现了什么，然后使用其分析结果来确定下一步操作是有用的。一些清理工作需要在DataRobot之外完成，因此请准备好多次迭代以正确设置数据集。需要注意的常见问题包括以下内容：
- en: Wrong data type in a column
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列中的数据类型错误
- en: Mixed data types in a column
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列中的混合数据类型
- en: Spaces or other characters in numeric columns that make them look like text
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数值列中的空格或其他字符使它们看起来像文本
- en: Synonyms or misspelled words
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同义词或拼写错误的单词
- en: Dates encoded as strings
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期以字符串形式编码
- en: Dates with differing formats
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期格式不同
- en: Data normalization and standardization
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据归一化和标准化
- en: When different data features have varying scales and ranges, it becomes harder
    to compare their impacts on the target values. Also, many algorithms have difficulty
    in dealing with different scales of values, sometimes leading to stability issues.
    One method for avoiding these problems is to normalize (not to be confused with
    database normal forms) or standardize the values.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当不同的数据特征具有不同的尺度和范围时，比较它们对目标值的影响变得更加困难。此外，许多算法在处理不同尺度的值时存在困难，有时会导致稳定性问题。避免这些问题的方法之一是对值进行归一化（不要与数据库范式混淆）或标准化。
- en: 'In normalization (also known as scaling), you scale the values such that they
    range from 0 to 1:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在归一化（也称为缩放）中，您将值缩放，使它们在 0 到 1 之间：
- en: Xnormalized = (X – Xmin) / (Xmax – Xmin)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Xnormalized = (X – Xmin) / (Xmax – Xmin)
- en: 'Standardization, on the other hand, centers the data such that the mean becomes
    zero and scales it such that the standard deviation becomes 1\. This is also known
    as **z-scoring** the data:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，标准化会将数据中心化，使得平均值变为零，并按比例缩放，使得标准差变为 1。这也被称为 **z 分数** 数据：
- en: Xstandardized = (X – Xmean) / XSD
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Xstandardized = (X – Xmean) / XSD
- en: Here, Xmean is the mean of all X values, and XSD is the standard deviation of
    X values.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，Xmean 是所有 X 值的平均值，XSD 是 X 值的标准差。
- en: In general, you will not need to worry about this because DataRobot automatically
    does this for the datasets as required.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您不需要担心这个问题，因为 DataRobot 会根据需要自动对数据集进行此操作。
- en: Outliers
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常值
- en: Outliers are values that seem to be out of place compared to the rest of the
    dataset. These values can be very large or very small. In general, values that
    are more than three standard deviations from the mean are considered outliers,
    but this only applies to features where values are expected to be normally distributed.
    Outliers typically come from data quality issues or some unusual situations that
    are not considered relevant enough to be trained on. The data points deemed to
    be outliers are typically removed from the dataset to prevent them from overpowering
    your models. The rules of thumb are only for highlighting the candidates. You
    will have to use your judgment to determine whether any values are outliers and
    whether they need to be removed. Once again, DataRobot will highlight potential
    outliers, but you will have to review those data points and determine whether
    to remove them or leave them in.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值是相对于数据集的其他部分似乎不合适的数据。这些值可能非常大或非常小。一般来说，与平均值相差超过三个标准差的数据被认为是异常值，但这仅适用于预期值呈正态分布的特征。异常值通常来自数据质量问题或一些不寻常的情况，这些情况被认为不足以作为训练数据。被认为异常的数据点通常从数据集中移除，以防止它们对您的模型产生过度影响。这些经验法则仅用于突出候选者。您将必须运用您的判断力来确定任何值是否为异常值以及是否需要删除它们。再次强调，DataRobot
    将突出潜在的异常值，但您必须审查这些数据点并确定是否删除它们或保留。
- en: Missing values
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺失值
- en: This is a very common problem in datasets. Your dataset may contain many missing
    values, marked as **NULL** or **NaN**. In some cases, you will see a **?**, or
    you might see an unusual value, such as **-999**, that an organization might be
    using to represent a missing or unknown value. How you choose to handle such values
    depends a lot on the problem you are trying to solve and what the dataset represents.
    Many times, you might choose to remove the row of data that contains a missing
    value. Sometimes, that is not possible because you might not have enough data,
    and removing such rows might lead to the removal of a significant portion of your
    dataset. Sometimes, you will see a large number of values in a feature (or column)
    that might be missing. In those situations, you might want to remove that feature
    from the dataset.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在数据集中非常常见的问题。您的数据集可能包含许多缺失值，标记为 **NULL** 或 **NaN**。在某些情况下，您可能会看到一个 **?**，或者您可能会看到一个不寻常的值，例如
    **-999**，这可能是一个组织用来表示缺失或未知值的值。您如何处理这些值在很大程度上取决于您试图解决的问题以及数据集代表的内容。很多时候，您可能会选择删除包含缺失值的行数据。有时，这可能不可行，因为您可能没有足够的数据，删除这些行可能会导致您数据集的重要部分被移除。有时，您可能会看到一个特征（或列）中的大量值可能缺失。在这些情况下，您可能希望从数据集中删除该特征。
- en: Another possible way of dealing with this situation is to fill the missing values
    with a reasonable guess. This could take the form of a zero value, or the mean
    value for that feature, or a median value of that feature. For categorical data,
    missing values are typically treated as their own separate category.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种处理这种情况的可能方法是使用合理的猜测来填充缺失值。这可能采取零值的形式，或者该特征的均值，或者该特征的中间值。对于分类数据，缺失值通常被视为一个单独的类别。
- en: More sophisticated methods use the k-nearest neighbor algorithm to compute missing
    values based on other similar data points. No one answer will be appropriate every
    time, so you will need to use your judgment and understanding of the problem to
    make a decision. One final option is to leave it as it is and let DataRobot figure
    out how to deal with the situation. DataRobot has many imputation strategies as
    well as algorithms to handle missing values. But you have to be careful, as that
    might not always lead to the best solution. Talk to an experienced data scientist
    and use your understanding of the business problem to plot a course of action.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的方法使用k最近邻算法根据其他相似数据点计算缺失值。没有一种答案在每次都适用，所以你需要使用你的判断和对问题的理解来做出决定。最后一个选项是保持原样，让DataRobot找出如何处理这种情况。DataRobot也有许多填充策略以及处理缺失值的算法。但你要小心，因为这可能并不总是导致最佳解决方案。与经验丰富的数据科学家交谈，并使用你对业务问题的理解来规划行动方案。
- en: Category encoding
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类别编码
- en: In many problems, you have to transform your features into numeric values. This
    is because many algorithms cannot handle categorical data. There are many ways
    to encode categorical values and DataRobot has many of these methods built in.
    Some of these techniques are one-hot encoding, leave one out encoding, and target
    encoding. We will not get into the details, as normally you would let DataRobot
    handle this for you, but there might be cases where you will want to encode it
    yourself in a specific way due to your understanding of the business problem.
    This feature of DataRobot is a great time saver and typically works very well
    for most problems.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多问题中，你必须将你的特征转换为数值。这是因为许多算法无法处理分类数据。有许多方法可以编码分类值，DataRobot内置了许多这些方法。其中一些技术包括独热编码、留一法编码和目标编码。我们不会深入细节，因为通常你会让DataRobot为你处理这些，但可能会有一些情况，由于你对业务问题的理解，你将想要以特定的方式自行编码。DataRobot的这个特性可以节省大量时间，并且通常对大多数问题都工作得很好。
- en: Consolidate categories
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并类别
- en: Sometimes, you have categorical data that contains a large number of categories.
    Although there are methods for dealing with large category counts (as discussed
    in the preceding section), many times, it is advisable to consolidate the categories.
    For example, you might have many categories that contain very few data points,
    but are very similar to one another. In this case, you can combine them into a
    single category. In other cases, it might just be that someone used a different
    spelling, a synonym, or an abbreviation. In such cases, it is better to combine
    them into a single category as well. Sometimes, you might want to split up a numerical
    feature into bins that have a business meaning for your users or stakeholders.
    This is an example of data preparation that you will need to do on your own based
    on your understanding of the problem. You should do this prior to uploading the
    data into DataRobot.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，你会有包含大量类别的分类数据。尽管有处理大量类别计数的方法（如前文所述），但很多时候，合并类别是明智的选择。例如，你可能有很多包含非常少数据点但彼此非常相似的类别。在这种情况下，你可以将它们合并成一个单独的类别。在其他情况下，可能只是有人使用了不同的拼写、同义词或缩写。在这种情况下，最好也将它们合并成一个单独的类别。有时，你可能想要将数值特征分割成对用户或利益相关者有业务意义的区间。这是你需要根据对问题的理解自行进行的数据准备的一个例子。你应该在将数据上传到DataRobot之前完成这项工作。
- en: Target leakage
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标泄露
- en: Sometimes, the dataset contains features that are derived from the target itself.
    These are not known in advance or are not known at the time of prediction. Inadvertently
    using these features to build a model causes problems downstream. This issue is
    called target leakage. The dataset should be inspected carefully and such features
    should be removed from the training features. DataRobot will also analyze the
    features automatically and try to flag any features that might lead to target
    leakage.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，数据集包含从目标本身派生的特征。这些特征在事先未知或在预测时未知。无意中使用这些特征来构建模型会导致下游问题。这个问题被称为目标泄露。应该仔细检查数据集，并将这些特征从训练特征中移除。DataRobot也会自动分析特征，并尝试标记可能导致目标泄露的任何特征。
- en: Term-document matrix
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项-文档矩阵
- en: Your dataset may contain features that contain text or notes. These notes frequently
    contain important information that is useful for making decisions. Many of the
    algorithms, however, cannot make use of this text directly. This text has to be
    parsed into numeric values for it to become useful to modeling algorithms. There
    are several methods for doing that, with the most common one being the term-document
    matrices. Document here refers to a single text or notes entry. Each of these
    documents can be parsed to split it up into terms. Now you can count how many
    times a term showed up in a document. This result can be stored in a matrix called
    a **Term Frequency** (**TF**) matrix. Some of this information can also be visualized
    in word clouds. DataRobot will automatically build these word clouds for you.
    While TF is useful, it can be limiting because some terms might be very common
    in all the documents, hence they are not very useful in distinguishing between
    them. This leads to another idea, whereby perhaps we should look for terms that
    are somewhat unique to a document. This concept of giving more weight to a term
    that is present in some documents only is called **Inverse Document Frequency**
    (**IDF**). The combination of a term showing up multiple times in a document (TF)
    and it being somewhat rare (IDF) is called **TFIDF**. TFIDF is something that
    DataRobot will compute automatically for you and gets applied to features that
    contain text.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据集中可能包含包含文本或注释的特征。这些注释通常包含对做出决策有用的重要信息。然而，许多算法却无法直接利用这些文本。这些文本必须被解析成数值，以便对建模算法变得有用。为此有几种方法，其中最常见的是术语-文档矩阵。这里的文档指的是单个文本或注释条目。每个文档都可以被解析以分割成术语。现在您可以计算一个术语在文档中出现的次数。这个结果可以存储在一个称为**词频（TF**）矩阵中。一些信息也可以在词云中进行可视化。DataRobot会自动为您构建这些词云。虽然TF很有用，但它可能有限制，因为某些术语可能在所有文档中都非常常见，因此它们在区分它们之间不是非常有用。这导致另一个想法，即我们可能应该寻找仅对某些文档独特的术语。这种给仅在某些文档中出现的术语赋予更多权重的概念被称为**逆文档频率（IDF**）。一个术语在文档中多次出现（TF）并且相对罕见（IDF）的组合被称为**TFIDF**。TFIDF是DataRobot会自动为您计算并应用于包含文本的特征。
- en: Data transformations
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据转换
- en: 'While DataRobot will do many data transformations for you (and it keeps adding
    more all the time), there are many transformations that will impact your model
    but that DataRobot will not be able to catch. You will have to do these on your
    own. Examples of these are mathematical transformations such as log, square, square
    root, absolute values, and differences. Some of the simple ones can be set up
    inside DataRobot, but for more complex ones, you will have to perform the operations
    outside of DataRobot or in tools such as Paxata. Sometimes, you will do a transformation
    to linearize your problem or to deal with features that have long-tailed data.
    Some of the transformations that DataRobot does automatically are as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然DataRobot会为您执行许多数据转换（并且它一直在添加更多），但有许多转换会影响您的模型，但DataRobot将无法捕捉到。您将不得不自己执行这些操作。这些操作的例子包括数学转换，如对数、平方、平方根、绝对值和差值。其中一些简单的可以在DataRobot内部设置，但对于更复杂的转换，您必须在DataRobot之外或使用Paxata等工具执行操作。有时，您会进行转换以线性化问题或处理具有长尾数据的特征。DataRobot自动执行的一些转换如下：
- en: Computing aggregates such as counts, min, max, average, median, most frequent,
    and entropy
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算聚合数据，如计数、最小值、最大值、平均值、中位数、最频繁值和熵
- en: An extensive list of time-based features, such as change over time, max over
    time, and averages over time
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个广泛的时间相关特征列表，例如随时间变化、随时间最大值和随时间平均值
- en: Some text extraction features, such as word counts, extracted tokens, and term-document
    matrices
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些文本提取特征，例如词数、提取的标记和术语-文档矩阵
- en: Geospatial features from geospatial data
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自地理空间数据的地理空间特征
- en: We will discuss this topic again in more detail in [*Chapter 4*](B17159_04_Final_NM_ePub.xhtml#_idTextAnchor087),
    *Preparing Data for DataRobot*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第4章*](B17159_04_Final_NM_ePub.xhtml#_idTextAnchor087)中更详细地讨论这个主题，*为DataRobot准备数据*。
- en: Collinearity checks
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共线性检查
- en: In any given dataset, there will be features that are highly correlated to other
    features. In essence, they carry the same information as some other features.
    It is generally desirable to remove such features that are highly duplicative
    of some other features in the dataset. DataRobot performs these checks automatically
    for you and will flag these collinear features. This is especially critical for
    linear models, but some of the newer methods can deal with this issue better.
    What thresholds to use varies based on the modeling algorithms and your business
    problem. It is fairly easy in DataRobot to remove these features from your feature
    sets to be used for modeling.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何给定的数据集中，都存在一些与其他特征高度相关的特征。本质上，它们携带与某些其他特征相同的信息。通常，我们希望删除这些与数据集中某些其他特征高度重复的特征。DataRobot会自动为您执行这些检查，并将这些共线性特征标记出来。这对于线性模型尤其关键，但一些较新的方法可以更好地处理这个问题。使用的阈值取决于建模算法和您的业务问题。在DataRobot中删除这些特征以用于建模是非常容易的。
- en: DataRobot also produces a correlation matrix that shows how the different features
    are correlated to one another. This helps identify collinear features as well
    as key candidate features to be used in the model. You can gain a lot of insight
    into your data and the problem by analyzing the correlation matrix. In [*Chapter
    5*](B17159_05_Final_NM_ePub.xhtml#_idTextAnchor097), *Exploratory Data Analysis
    with DataRobot*, we will discuss examples of how this is done.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot还会生成一个相关矩阵，显示不同特征之间是如何相互关联的。这有助于识别共线性特征以及模型中要使用的候选关键特征。通过分析相关矩阵，您可以深入了解您的数据和问题。在[*第5章*](B17159_05_Final_NM_ePub.xhtml#_idTextAnchor097)《使用DataRobot进行探索性数据分析》中，我们将讨论如何进行这种分析的例子。
- en: Data partitioning
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分区
- en: Before you start building the models, you need to partition your dataset into
    three parts. These parts are called training, validation, and holdout. These three
    parts are used for different purposes during the model building process. It is
    common to split 10-20% of the dataset into the holdout set. The remaining portion
    is split up further, with 70-80% going to training and 20-30% going to the validation
    set. This splitting is done to make sure that the models are not overfitted and
    that the expected results in deployment are in line with results seen during model
    building.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始构建模型之前，您需要将数据集分为三个部分。这些部分被称为训练集、验证集和保留集。在模型构建过程中，这三个部分用于不同的目的。通常将10-20%的数据集分为保留集。剩余的部分进一步分割，其中70-80%用于训练集，20-30%用于验证集。这种分割是为了确保模型不会过拟合，并且部署中预期的结果与模型构建期间看到的结果一致。
- en: Only the training dataset is used to train the model. The validation set is
    designed to tune the algorithms in order to optimize the results by performing
    multiple cross-validation tests. Finally, the holdout set is used after the models
    are built to test the model on data that it has never seen before. If the results
    on the holdout set are acceptable, then the model can be considered for deployment.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 只有训练集用于训练模型。验证集的设计是为了通过执行多次交叉验证测试来调整算法，以优化结果。最后，在构建模型后，使用保留集来测试模型在之前从未见过的数据上的表现。如果保留集上的结果是可以接受的，那么模型可以考虑部署。
- en: DataRobot automates most of this process, but it does allow the user to customize
    the split percentages, as well as how the partitioning should be done. It also
    performs a similar function for time series or forecasting problems by automatically
    splitting the data for time-based backtests.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot自动化了这一过程的大部分，但它允许用户自定义分割百分比以及分区应该如何进行。它还通过自动分割数据以进行基于时间的回溯测试，为时间序列或预测问题执行类似的功能。
- en: Data visualization
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据可视化
- en: One of the most important tasks a data analyst or data scientist needs to do
    is to understand the dataset. Data visualization is key to this understanding.
    DataRobot provides various ways to visualize the datasets to help you understand
    the dataset. These visualizations are built automatically for you so that you
    can spend your time analyzing them instead of preparing them. Let's look at what
    these are and how to use them.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析师或数据科学家需要完成的最重要任务之一是理解数据集。数据可视化是实现这一理解的关键。DataRobot提供了多种可视化数据集的方法，以帮助您理解数据集。这些可视化是自动为您构建的，这样您就可以花时间分析它们，而不是准备它们。让我们看看这些是什么以及如何使用它们。
- en: 'When you go to the data page (*Figure 1.20*) for your project, you will see
    high-level profile information for your dataset. Inspect this information carefully
    to understand your dataset in totality. If you click on the **Feature Association**
    menu (top left), you will see how the features are related to one another (*Figure
    2.3*):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当你访问你项目的数据页面（*图1.20*）时，你会看到你数据集的高级概要信息。仔细检查这些信息，以全面了解你的数据集。如果你点击左上角的**特征关联**菜单，你会看到特征之间是如何相互关联的（*图2.3*）：
- en: '![Figure 2.3 – Feature associations using mutual information'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.3 – 使用互信息进行特征关联'
- en: '](img/Figure_2.3_B17159.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_2.3_B17159.jpg)'
- en: Figure 2.3 – Feature associations using mutual information
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – 使用互信息进行特征关联
- en: This diagram shows the interrelationships using the mutual information metric.
    **Mutual Information** (**MI**) uses information theory to determine the amount
    of information you obtain about one feature from the other feature. The benefit
    of using MI compared to the Pearson correlation coefficient is that it can be
    used for any type of feature. The value goes from 0 (the two features are independent)
    to 1 (they carry the same information). This is useful in determining which features
    will be good candidates for the model and which features will not provide any
    useful information or are redundant. This view is extremely important to understand
    and use before model building starts, even though DataRobot automatically uses
    this information to make modeling decisions.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此图使用互信息指标显示了特征之间的相互关系。**互信息**（**MI**）使用信息理论来确定从另一个特征中获取关于一个特征的信息量。与皮尔逊相关系数相比，使用MI的优点是它可以用于任何类型的特征。其值从0（两个特征是独立的）到1（它们携带相同的信息）。这在确定哪些特征将是模型的良好候选者以及哪些特征不会提供任何有用的信息或冗余方面非常有用。在模型构建开始之前，理解并使用这种观点非常重要，尽管DataRobot会自动使用这些信息来做出建模决策。
- en: 'There is another metric that is also used in a similar capacity. If you click
    on the metric dropdown at the bottom of the preceding screenshot, you can select
    the other metric called **Cramer''s V**. Once you select Cramer''s V, you will
    see a similar graphical view (*Figure 2.4*):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 另有一个指标也以类似的方式使用。如果你点击前面截图底部的度量下拉菜单，你可以选择另一个称为**Cramer's V**的指标。一旦选择Cramer's
    V，你将看到类似的图形视图（*图2.4*）：
- en: '![Figure 2.4 – Feature associations using Cramer''s V'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.4 – 使用Cramer''s V进行特征关联'
- en: '](img/Figure_2.4_B17159.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_2.4_B17159.jpg)'
- en: Figure 2.4 – Feature associations using Cramer's V
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 使用Cramer's V进行特征关联
- en: Cramer's V is an alternative metric to MI, and it is used similarly. Its value
    also ranges from 0 (no relationship) to 1 (the features are highly correlated).
    Cramer's V is often used with categorical variables as an alternative to the Pearson
    correlation coefficient.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Cramer's V是MI的另一种指标，其使用方式类似。其值范围也从0（无关系）到1（特征高度相关）。Cramer's V通常与分类变量一起使用，作为皮尔逊相关系数的替代。
- en: Notice that DataRobot automatically found clusters of interrelated features.
    Each cluster is color-coded in a different color, and the features are sorted
    by clusters in *Figure 2.4*. You can zoom into specific clusters to inspect them
    further. This is an important feature of the DataRobot environment as very few
    data scientists know about this idea or make use of it. The clusters are important
    because they highlight groups of interrelated features. These complex interdependencies
    are typically very important for understanding the business problem. Normally,
    the only people who know about these complex interdependencies are people with
    a lot of domain experience. Most others will not even be aware of these complexities.
    If you are new to a domain, then understanding these will give you an equivalent
    of multiple years of experience. Study these carefully, discuss them with your
    business experts to fully understand what they are trying to highlight, and then
    use these insights to improve your models as well as your business processes.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到DataRobot自动找到了相互关联的特征簇。每个簇用不同的颜色编码，特征按簇排序在*图2.4*中。你可以放大到特定的簇以进一步检查它们。这是DataRobot环境的一个重要特性，因为非常少的数据科学家了解这个想法或利用它。这些簇很重要，因为它们突出了相互关联的特征组。这些复杂的相互依赖关系通常对于理解业务问题非常重要。通常，只有拥有大量领域经验的人才知道这些复杂的相互依赖关系。大多数人甚至不会意识到这些复杂性。如果你是某个领域的初学者，那么理解这些将给你相当于多年的经验。仔细研究这些，与你的业务专家讨论，以全面理解他们试图强调的内容，然后使用这些见解来改进你的模型以及你的业务流程。
- en: 'Also, note that DataRobot provides a list of the top 10 strongest associations.
    It is important to note these associations and spend some time thinking about
    what they mean for your problem. Are these consistent with what you know about
    your domain, or are there some surprises? It is the surprises that often result
    in key insights that could prove to be valuable insights for your business. In
    the following list, you see a **View Feature Association Pairs** button. If you
    click on that button, you will see *Figure 2.5*:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意DataRobot提供了一个包含前10个最强关联的列表。注意这些关联并花些时间思考它们对你的问题意味着什么。这些是否与你对领域的了解一致，或者有一些惊喜？往往是惊喜导致的关键洞察，这些洞察可能对你的业务具有价值。在以下列表中，你可以看到一个**查看特征关联对**按钮。如果你点击该按钮，你将看到*图2.5*：
- en: '![Figure 2.5 – Feature association details'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.5 – 特征关联细节'
- en: '](img/Figure_2.5_B17159.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.5_B17159.jpg)'
- en: Figure 2.5 – Feature association details
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – 特征关联细节
- en: This graphic shows the relationship between two selected features in detail.
    In this example, one feature is categorical while the other is numeric. The diagram
    shows how the two are related and could provide additional insights into the problem.
    Be sure to investigate the relationships, especially the ones that might be counterintuitive.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此图形详细显示了两个选定特征之间的关系。在这个例子中，一个特征是分类的，而另一个是数值的。该图显示了这两个特征之间的关系，并可能为问题提供额外的见解。务必调查这些关系，特别是那些可能具有反直觉性的关系。
- en: 'Now you can click on the specific features to see how they are distributed
    (*Figure 2.6*):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以点击特定的特征来查看它们的分布情况(*图2.6*)：
- en: '![Figure 2.6 – Feature details'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.6 – 特征细节'
- en: '](img/Figure_2.6_B17159.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.6_B17159.jpg)'
- en: Figure 2.6 – Feature details
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 – 特征细节
- en: This view shows a histogram of how the values are distributed and how they are
    related to the target values. Key things to focus on are ranges where you do not
    have enough data and where you have non-linearities. These could give you ideas
    about feature engineering. These are also areas where you ask the question why
    does the system exhibit this behavior?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此视图显示了值的分布情况以及它们与目标值之间的关系。需要关注的关键点是数据不足的区间以及非线性区间。这些可能会给你关于特征工程的想法。这些也是你询问系统为何表现出这种行为的区域？
- en: With this background work done, you are now ready to dive into modeling algorithms.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成这项背景工作后，你现在可以开始深入研究建模算法。
- en: Machine learning algorithms
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习算法
- en: 'There are now hundreds of machine learning algorithms available to be used
    for a machine learning project, and more are being invented every day. DataRobot
    supports a wide array of open source machine learning algorithms, including several
    deep learning algorithms – Prophet, SparkML-based algorithms, and H2O algorithms.
    Let''s now take a look at what types of algorithms exist and what they are used
    for (*Figure 2.7*):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经有数百种机器学习算法可供机器学习项目使用，并且每天都有新的算法被发明。DataRobot支持广泛的开源机器学习算法，包括几个深度学习算法——Prophet、基于SparkML的算法和H2O算法。现在让我们看看存在哪些类型的算法以及它们的应用(*图2.7*)：
- en: '![Figure 2.7 – Machine learning algorithms'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.7 – 机器学习算法'
- en: '](img/Figure_2.7_B17159-DESKTOP-C2VUV36.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.7_B17159-DESKTOP-C2VUV36.jpg)'
- en: Figure 2.7 – Machine learning algorithms
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 – 机器学习算法
- en: Our focus will mostly be on the algorithm types that DataRobot supports. These
    algorithm types are described in the following sub-sections.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的重点将主要放在DataRobot支持的算法类型上。这些算法类型将在以下子节中描述。
- en: Supervised learning
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'Supervised learning algorithms are used when you can provide an answer (also
    called a label) as part of the training dataset. For supervised learning, you
    have to assign a feature of your dataset to be the answer, and the algorithm tries
    to learn to predict the answer by seeing multiple examples and learning from these
    examples. See *Figure 2.8* for the different types of answers:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当你可以提供作为训练数据集一部分的答案（也称为标签）时，使用监督学习算法。对于监督学习，你必须将你的数据集的一个特征分配为答案，算法通过观察多个示例并从这些示例中学习来尝试预测答案。参见*图2.8*以了解不同类型的答案：
- en: '![Figure 2.8 – Targets for supervised learning algorithms'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.8 – 监督学习算法的目标'
- en: '](img/Figure_2.8_B17159.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.8_B17159.jpg)'
- en: Figure 2.8 – Targets for supervised learning algorithms
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 – 监督学习算法的目标
- en: 'DataRobot functionality is primarily focused on supervised learning algorithms.
    Included in the set are deep learning algorithms as well as big data algorithms
    from SparkML and H2O. DataRobot has built-in best practices to select the best-suited
    algorithms for your problem and dataset. There are four major types of supervised
    learning problems:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot的功能主要集中于监督学习算法。该套件包括深度学习算法以及来自SparkML和H2O的大数据算法。DataRobot内置了最佳实践来选择最适合您问题和数据集的算法。监督学习问题主要有以下四种类型：
- en: Regression
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回归
- en: Regression problems are the ones where the answer (target) takes a numeric form
    (see *Figure 2.8*). Regression models try to fit a curve such that the error between
    the prediction and the actual value is minimized for the entire training dataset.
    Sometimes, even a classification problem can be set up as a numeric regression
    problem. In such cases, the answer is a number that can then be turned into a
    bin by using thresholds. Logistic regression is one such method that produces
    a value between zero and one. You can mark all answers below a certain threshold
    to be zero, and all above as ones. There are linear as well as non-linear regression
    algorithms that are used based on the problem. The models are assessed based on
    how well the regression line matches the data. Typical metrics used are **RMSE**,
    **MAPE**, **LogLoss**, and **Rsquared**. Typical algorithms used are **XGBoost**,
    **Elastic Net**, **Random Forest**, and **GA2M**.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 回归问题是指答案（目标）以数值形式出现的情况（参见*图2.8*）。回归模型试图拟合一条曲线，使得预测值与实际值之间的误差在整个训练数据集上最小化。有时，甚至可以将分类问题设置为一个数值回归问题。在这种情况下，答案是数字，然后可以通过阈值将其转换为二进制。逻辑回归就是这样一种方法，它产生一个介于零和一之间的值。您可以将低于某个阈值的所有答案标记为零，而高于阈值的答案标记为一。根据问题，可以使用线性回归算法以及非线性回归算法。模型是根据回归线与数据匹配的程度来评估的。常用的指标包括**RMSE**、**MAPE**、**LogLoss**和**Rsquared**。常用的算法包括**XGBoost**、**Elastic
    Net**、**随机森林**和**GA2M**。
- en: Binary classification
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 二元分类
- en: Binary classification problems have answers that can only take two distinct
    values (called classes). These could be in the form of 0 or 1, Yes or No, and
    so on. Please refer to *Figure 2.8* for an example of the target feature for binary
    classification. A typical issue that you commonly face is the problem of class
    imbalance. This happens when most of the dataset is biased toward one class. These
    are typically addressed by downsampling the overrepresented class when sufficient
    training data is present. When this is not possible, you can try oversampling
    the underrepresented class or use other methods. None of these methods is perfect,
    and sometimes you have to try different approaches to see what works best. DataRobot
    provides mechanisms to specify downsampling if needed. Some of the algorithms
    that are commonly used for binary classification are **logistic regression**,
    **k-nearest neighbors**, **tree-based algorithms**, **SVM**, and **Naïve Bayes**.
    In the case of classification problems, it is best to avoid using accuracy as
    a metric to assess results. The results are often shown in the form of a confusion
    matrix (described later in this chapter). DataRobot will automatically select
    an appropriate metric to use in such cases.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类问题的答案只能取两个不同的值（称为类别）。这些可以是0或1、是或否等形式。请参阅*图2.8*以了解二元分类的目标特征的示例。您通常遇到的一个典型问题是类别不平衡问题。当大多数数据集偏向于一个类别时，这种情况就会发生。这些通常通过在存在足够训练数据的情况下对过度代表的类别进行下采样来解决。当这不可能时，您可以尝试对代表性不足的类别进行上采样或使用其他方法。这些方法都不是完美的，有时您不得不尝试不同的方法来查看哪种方法最有效。DataRobot提供了在需要时指定下采样的机制。一些常用的二元分类算法包括**逻辑回归**、**k最近邻**、**基于树的算法**、**SVM**和**朴素贝叶斯**。在分类问题的情况下，最好避免使用准确率作为评估结果的指标。结果通常以混淆矩阵的形式显示（本章后面将描述）。在需要时，DataRobot将自动选择合适的指标来使用。
- en: Multiclass classification
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多类分类
- en: Multiclass classification problems are the ones where you are trying to predict
    more than two classes or categories. For a simple example of what the target might
    look like, see *Figure 2.8*. Multiclass capability was added recently and many
    of the DataRobot features might not work with such problems. Since downsampling
    is not available, you might want to adjust your sampling prior to uploading your
    dataset into DataRobot. Also, note that you can frequently collapse your problem
    into a binary classification problem by collapsing the classes into two classes.
    That may or may not work for your use case, but it is an option if required. Also,
    not all algorithms are appropriate for multiclass problems. DataRobot will automatically
    select the appropriate algorithms to build the models for multiclass problems.
    Typical metrics to use are AUC, LogLoss, or Balanced Accuracy. The results are
    often shown in the form of a confusion matrix (described later in this chapter).
    Typical algorithms used are XGBoost, Random Forest, and TensorFlow.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 多分类分类问题是你试图预测两个以上类别或类别的那些问题。为了简单说明目标可能的样子，请参阅 *图 2.8*。多分类能力是最近添加的，许多 DataRobot
    功能可能不适用于此类问题。由于没有下采样可用，你可能想在将数据集上传到 DataRobot 之前调整你的采样。此外，请注意，你可以通过将类别合并为两个类别来频繁地将问题简化为二元分类问题。这可能或可能不适用于你的用例，但如果需要，这是一个选项。此外，并非所有算法都适用于多分类问题。DataRobot
    将自动选择合适的算法来构建多分类问题的模型。典型的指标包括 AUC、LogLoss 或平衡准确率。结果通常以混淆矩阵的形式显示（本章后面将描述）。典型的算法包括
    XGBoost、随机森林和 TensorFlow。
- en: Time series/forecasting
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间序列/预测
- en: 'Time series or forecasting models are also referred to as time-aware models
    in DataRobot. In these problems, you have data that is changing over time and
    you are interested in predicting/forecasting a target value in the future (*Figure
    2.2*). DataRobot not only supports the usual algorithms for time series such as
    ARIMA, but can also adapt these problems to machine learning regression problems
    and then apply algorithms such as XGBoost to solve them. These problems require
    that the series should be transformed into stationary series and require extensive
    feature engineering to create time-based features. The problems also require that
    you take into account important events in the past that may repeat (such as holidays
    or major shopping days). Time series models also require special ways of handling
    validation and testing via a method called backtests:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DataRobot 中，时间序列或预测模型也被称为时间感知模型。在这些问题中，你拥有随时间变化的数据，并且你对预测/预测未来某个目标值感兴趣（*图
    2.2*）。DataRobot 不仅支持时间序列的常用算法，如 ARIMA，还可以将这些问题适应为机器学习回归问题，然后应用 XGBoost 等算法来解决它们。这些问题要求将序列转换为平稳序列，并需要大量的特征工程来创建基于时间的特征。这些问题还要求你考虑过去可能重复发生的重要事件（如假日或大型购物日）。时间序列模型还需要通过一种称为回测的方法来处理验证和测试：
- en: '![Figure 2.9 – Backtesting for time series problems'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.9 – 时间序列问题的回测'
- en: '](img/Figure_2.9_B17159.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.9_B17159.jpg)'
- en: Figure 2.9 – Backtesting for time series problems
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9 – 时间序列问题的回测
- en: In backtesting, models are built using past data, and then tested using holdout
    data that is newer and has never been seen by the model. This time-based slicing
    of holdout data is also referred to as out-of-time validation. DataRobot automates
    many of these tasks for you, as we will see in more detail later.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在回测中，模型使用过去的数据构建，然后使用模型从未见过的较新和未知的保留数据进行测试。这种基于时间的保留数据切片也被称为时间外验证。DataRobot
    自动为你自动化了许多这些任务，我们将在后面更详细地看到。
- en: Algorithms
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法
- en: 'Let''s review some of the main algorithms used in DataRobot. Here, we only
    provide a high-level overview of these algorithms These algorithms can be tuned
    for a given problem by changing their hyperparameters. For a more detailed understanding
    of any specific algorithm, you can refer to a machine learning book or the DataRobot
    documentation. Some of the important algorithms are as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下 DataRobot 中使用的一些主要算法。在这里，我们只提供这些算法的高级概述。这些算法可以通过更改它们的超参数来针对特定问题进行调整。要更详细地了解任何特定算法，你可以参考机器学习书籍或
    DataRobot 文档。一些重要的算法如下：
- en: '**Random Forest**. A random forest model is built by creating multiple decision
    tree models and then uses the mean of the output. This is done by creating bootstrap
    samples of the training data and building decision trees (*Figure 2.10*) on these
    samples:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机森林**。随机森林模型是通过创建多个决策树模型，然后使用输出的平均值来构建的。这是通过创建训练数据的自助样本并在这些样本上构建决策树（*图2.10*）来实现的：'
- en: '![Figure 2.10 – Random forest'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.10 – 随机森林'
- en: '](img/Figure_2.10_B17159.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.10_B17159.jpg)'
- en: Figure 2.10 – Random forest
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 – 随机森林
- en: 'Random forest models handle missing data and non-linearities and have proven
    to work great in many situations. A random forest model can be used for regression
    as well as classification problems:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林模型可以处理缺失数据和非线性，并在许多情况下证明效果极佳。随机森林模型既可以用于回归问题，也可以用于分类问题：
- en: '**XGBoost**: Also known as **eXtreme** gradient boosted trees, are decision
    tree-based algorithms that have become very popular because they tend to produce
    very effective predictions and can handle missing values. They can handle non-linear
    problems and interactions between features. XGBoost builds upon random forest
    models by creating a random forest and then creating trees on the residuals of
    the previous trees. This way, every new set of trees is able to produce a better
    result. XGBoost can be used for regression as well as classification problems.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**XGBoost**：也称为**eXtreme**梯度提升树，是基于决策树的算法，因其通常能产生非常有效的预测并可以处理缺失值而变得非常流行。它们可以处理非线性问题和特征之间的交互。XGBoost通过创建一个随机森林并在先前树的残差上创建树来构建在随机森林模型之上。这样，每一组新的树都能产生更好的结果。XGBoost可以用于回归问题，也可以用于分类问题。'
- en: '**Rulefit**: Rulefit models are ensembles of simple rules. You can think of
    these rules as being chained together like a decision tree. Rulefit models are
    much easier to understand as most people can relate to a combination of rules
    being applied to solve a problem. DataRobot typically builds this model to help
    you understand a problem and provide insights. You can go to the insights section
    of your **Models** tab and see the insights generated from a Rulefit model and
    how effective a given rule is for the problem. They can be used for classification
    as well as regression problems.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Rulefit**：Rulefit模型是由简单规则组成的集成模型。你可以将这些规则想象成像决策树一样串联在一起。Rulefit模型更容易理解，因为大多数人都能理解将规则组合起来解决问题。DataRobot通常会构建这个模型来帮助你理解问题并提供洞察。你可以访问**模型**标签页的洞察部分，查看从Rulefit模型生成的洞察以及给定规则对问题的有效性。它们可以用于分类问题，也可以用于回归问题。'
- en: '**ElasticNet**, **Ridge regressor**, **Lasso regressor**: These models use
    regularization to make sure that the models are not overfitting and are not unnecessarily
    complex. Regularization is done by adding a penalty for adding more features,
    which in turn forces the models to either drop some features or reduce their relative
    impact. Lasso regressor (also known as **L1 regressor**) uses penalty weights
    that are the absolute values of the coefficients. The effect of using Lasso is
    that it tries to reduce the coefficients to zero, thereby selecting important
    features and removing the ones that do not contribute much. Ridge regressor (also
    known as **L2 regressor**) uses penalty weights that are squared coefficients.
    The impact of this is to reduce the magnitude of coefficients. **ElasticNet**
    is used to refer to linear models that use both Lasso and Ridge regularization
    to produce models that are simpler as well as regularized. This comes in handy
    when you have a lot of features that are correlated with each other.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ElasticNet**、**岭回归**、**Lasso回归**：这些模型使用正则化来确保模型不会过拟合且不会过于复杂。正则化是通过添加更多特征的惩罚来实现的，这反过来迫使模型要么丢弃一些特征，要么减少它们的相对影响。Lasso回归（也称为**L1回归**）使用的是系数的绝对值作为惩罚权重。使用Lasso的效果是尝试将系数减少到零，从而选择重要的特征并移除那些贡献不大的特征。岭回归（也称为**L2回归**）使用的是平方系数作为惩罚权重。这种影响是减少系数的幅度。**ElasticNet**用于指代同时使用Lasso和岭正则化的线性模型，以产生既简单又正则化的模型。当你有很多相互关联的特征时，这非常有用。'
- en: '**Logistic Regression**: Logistic regression is a non-linear regression model
    that is used for binary classification. The output is in the form of a probability
    with a value ranging from 0 to 1\. This is then typically used with a threshold
    to assign the value to be a 0 or a 1.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑回归**：逻辑回归是一种用于二元分类的非线性回归模型。输出形式为概率，值在0到1之间。这通常与一个阈值一起使用，以将值分配为0或1。'
- en: '**SVM** (**Support Vector Machine**): This is a classification algorithm that
    tries to find a vector that best separates classes. It is easy to see what this
    looks like in a two-dimensional space (*Figure 2.11*), but the algorithm is known
    to work well in high dimension spaces. Another benefit of SVM is its ability to
    handle non-linearity by using non-linear kernel functions, which can be used to
    linearize the problem:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SVM**（**支持向量机**）：这是一种试图找到最佳分离类别的分类算法。在二维空间中，这种算法的样子很容易理解（*图2.11*），但该算法在高维空间中表现良好。SVM的另一个优点是它能够通过使用非线性核函数来处理非线性，这可以将问题线性化：'
- en: '![Figure 2.11 – Targets for supervised learning algorithms'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.11 – 监督学习算法的目标'
- en: '](img/Figure_2.11_B17159.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_2.11_B17159.jpg)'
- en: Figure 2.11 – Targets for supervised learning algorithms
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11 – 监督学习算法的目标
- en: '**GA2M** (**Generalized Additive Model**): This is one of those rare algorithms
    that offers understandability, while also offering high accuracy even in a non-linear
    problem. The number "2" in the name represents its ability to model interactions
    between features. GAM model output is a summation of outputs of the effects of
    individual features that have been binned. Since GAM allows these effects to be
    non-linear, it can capture the non-linear nature of the problem. The results of
    the model can be represented as a simple table that shows you the contribution
    of each feature to the overall answer. This type of table representation is easily
    understandable by most people. For industries or use cases where understandability
    and explainability are very important, this is perhaps one of the best options
    you can choose.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GA2M**（**广义加性模型**）：这是那些罕见算法之一，它提供了可理解性，同时在非线性问题中也能提供高准确度。名字中的数字“2”代表其建模特征之间相互作用的能力。GAM模型的输出是单个特征效果的输出之和，这些效果已经被分箱。由于GAM允许这些效果非线性，它可以捕捉问题的非线性特征。模型的结果可以用一个简单的表格表示，该表格显示了每个特征对整体答案的贡献。这种表格表示方式容易被大多数人理解。对于理解和可解释性非常重要的行业或用例，这可能是一个最佳选择之一。'
- en: '**K-Nearest Neighbors**: This is a very straightforward algorithm that finds
    the k closest data points (based on a specific way of computing distance). Now
    it finds the classification answers for these k points. It then determines the
    answer with the most votes and then assigns that as the answer. The default distance
    metric used is **Euclidian** distance, but DataRobot chooses the appropriate metric
    based on the dataset. A user can also specify a specific distance metric to be
    used.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K-Nearest Neighbors**：这是一个非常直接的算法，它找到k个最近的数据点（基于一种特定的距离计算方式）。现在它为这些k个点找到分类答案。然后它确定得票最多的答案，并将其作为答案。默认的距离度量是**欧几里得**距离，但DataRobot会根据数据集选择合适的度量。用户也可以指定要使用的特定距离度量。'
- en: '**TensorFlow**. TensorFlow is a deep learning model that is based on deep neural
    networks. A deep neural network is one that has hidden deep layers made up of
    ensembles of artificial neurons. The neurons carry highly non-linear activation
    functions that allow them to fit highly non-linear problems. These models are
    very good at producing high accuracy without the need for feature engineering,
    but they do require a lot more training data as compared to other algorithms.
    These models are generally considered very opaque and are prone to overfitting
    and are therefore not suitable for some applications. They are especially successful
    for applications where the features and feature engineering are hard to extract,
    for example, image processing. These models can be used for regression as well
    as classification problems.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow**. TensorFlow是一种基于深度神经网络的深度学习模型。深度神经网络是指由人工神经元集合构成的深层隐藏层。这些神经元携带高度非线性的激活函数，使它们能够适应高度非线性问题。这些模型在不需要特征工程的情况下，能够产生非常高的准确度，但与其它算法相比，它们需要更多的训练数据。这些模型通常被认为非常不透明，容易过拟合，因此不适合某些应用。它们在特征和特征工程难以提取的应用中特别成功，例如图像处理。这些模型可以用于回归问题，也可以用于分类问题。'
- en: '**Keras Neural Network**: Keras is a high-level deep learning library built
    on top of TensorFlow that allows many types of deep learning models to be incorporated
    into DataRobot. Being a higher-level library, it makes building a TensorFlow model
    a lot easier. Everything described in the preceding section applies to Keras.
    The particular implementation in DataRobot is well suited for sparse datasets
    and is particularly useful for text processing and classification problems.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Keras神经网络**：Keras是一个基于TensorFlow构建的高级深度学习库，它允许将许多类型的深度学习模型集成到DataRobot中。作为一个高级库，它使得构建TensorFlow模型变得容易得多。前述章节中描述的一切都适用于Keras。在DataRobot中的特定实现非常适合稀疏数据集，并且特别适用于文本处理和分类问题。'
- en: Unsupervised learning
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Unsupervised learning problems are those where you are not provided with an
    answer or a label. Examples of such problems are clustering or anomaly detection.
    DataRobot does not offer much for these problems, but it does have some capability
    for anomaly or outlier detection. These are problems where you have data points
    that are unusual in a way that happens very rarely. Examples include fraud detection,
    cybersecurity breach detection, failure detection, and data outlier detection.
    DataRobot allows you to set up a project without a target and it will then attempt
    to identify anomalous data points. For any clustering problems, you should try
    to use Python or R to create clustering models.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习问题是指那些没有提供答案或标签的问题。这类问题的例子包括聚类或异常检测。DataRobot对于这些问题没有太多提供，但它确实有一些异常或离群值检测的能力。这些问题是指那些数据点在非常罕见的情况下表现出不寻常的情况。例子包括欺诈检测、网络安全漏洞检测、故障检测和数据离群值检测。DataRobot允许你设置一个没有目标的项目，然后它会尝试识别异常数据点。对于任何聚类问题，你应该尝试使用Python或R来创建聚类模型。
- en: Reinforcement learning
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习
- en: Reinforcement learning problems are where you want to learn a series of decisions
    to be taken by an agent such that you achieve a certain goal. This goal is associated
    with a reward that is given to the agent for achieving the goal either completely
    or partially. There is no dataset available for this training, so the agent must
    try multiple times (with different strategies) and learn something on each attempt.
    Over many attempts, the agent will learn the strategy or rules that produce the
    best reward. As you can now guess, these algorithms work best when you do not
    have data, but you can experiment repeatedly in the real world (or a synthetic
    world). As we discussed before, DataRobot is not a suitable tool for such problems.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习问题是你希望学习一系列由代理做出的决策，以便实现某个特定目标。这个目标与一个奖励相关联，这个奖励是给予代理的，以表彰它完全或部分地实现了目标。对于这种训练没有可用的数据集，因此代理必须尝试多次（使用不同的策略）并在每次尝试中学习一些东西。经过多次尝试，代理将学会产生最佳奖励的策略或规则。正如你现在可以猜到的，这些算法在没有数据但可以在现实世界（或合成世界）中反复实验的情况下工作得最好。正如我们之前讨论的，DataRobot不是这类问题的合适工具。
- en: Ensemble/blended models
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成/混合模型
- en: Ensembling is a technique for creating a model that aggregates or blends predictions
    of other models. Different algorithms are sometimes able to exploit different
    aspects of the problem or dataset better. This means that many times, you can
    increase prediction accuracy by combining several good models. This, of course,
    comes with increasing complexity and cost. DataRobot offers many blending approaches
    and, in most circumstances, builds the blended model automatically for your project.
    You can then evaluate whether the increase in accuracy is enough to justify the
    additional complexity.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 集成是一种创建聚合或混合其他模型预测的模型的技术。不同的算法有时能够更好地利用问题的不同方面或数据集。这意味着很多时候，通过结合几个好的模型，你可以提高预测精度。当然，这伴随着复杂性和成本的提高。DataRobot提供了许多混合方法，并且在大多数情况下，它会自动为你的项目构建混合模型。然后你可以评估精度提高是否足以证明额外的复杂性是合理的。
- en: Blueprints
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图
- en: 'In DataRobot, every model is associated with a blueprint. A blueprint is a
    step-by-step recipe used by DataRobot to train a specific model. See *Figure 2.12*
    for an example:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在DataRobot中，每个模型都与一个蓝图相关联。蓝图是DataRobot用来训练特定模型的一步步食谱。参见*图2.12*以获取示例：
- en: '![Figure 2.12 – Model blueprint'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.12 – 模型蓝图'
- en: '](img/Figure_2.12_B17159.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.12_B17159.jpg)'
- en: Figure 2.12 – Model blueprint
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 – 模型蓝图
- en: The blueprint shows all the steps taken by DataRobot to build that specific
    model, including any data preparation and feature engineering done by DataRobot.
    Clicking on any specific box will show more details on the actions taken, parameters
    used, and documentation of the particular algorithm used. This also serves as
    great documentation for your modeling project that is automatically created for
    you.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝图显示了DataRobot构建特定模型所采取的所有步骤，包括DataRobot所做的任何数据准备和特征工程。点击任何特定的框将显示采取的行动、使用的参数以及特定算法的文档。这也为您的建模项目提供了出色的文档，这些文档是自动为您创建的。
- en: Now, let's look at how to determine how well an algorithm did. For this, we
    will require some performance metrics.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何确定算法的表现如何。为此，我们需要一些性能指标。
- en: Performance metrics
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能指标
- en: 'DataRobot offers a wide range of performance metrics for the models. You have
    to specify the metric you want to use to optimize the models for your project.
    Typically, the best metric to use is the one recommended by DataRobot. DataRobot
    does compute the other metrics as well once the model is built, so you can review
    the results of your model across multiple metrics. Please keep in mind that no
    metric is perfect for every situation, and you should be careful in selecting
    the metric for evaluating your results. Listed here are some details regarding
    commonly used metrics:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot为模型提供了一系列性能指标。您必须指定您想要使用的指标来优化您项目的模型。通常，最佳指标是DataRobot推荐的指标。一旦构建了模型，DataRobot也会计算其他指标，因此您可以在多个指标上查看您模型的结果。请记住，没有哪个指标适合所有情况，您在选择评估结果的指标时应谨慎。以下是一些关于常用指标的详细信息：
- en: '**RMSE** (**Root Mean Squared Error**): RMSE is a metric that first computes
    the square of errors (the difference between actual and predicted). These are
    then averaged over the entire dataset and then we compute a square root of that
    average. Given that this metric is dependent on the scale of the values, its interpretation
    is dependent on the problem. You cannot compare RMSE for two different datasets.
    This metric is frequently used for regression problems when the data is not highly
    skewed.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RMSE（均方根误差）**：RMSE是一种首先计算误差平方（实际值与预测值之间的差异）的指标。然后，在整个数据集上对这些值进行平均，然后计算这个平均值的平方根。鉴于这个指标依赖于值的规模，其解释依赖于问题。您不能比较两个不同数据集的RMSE。这个指标在数据不是高度偏斜的回归问题中经常被使用。'
- en: '**MAPE** (**Mean Absolute Percentage Error**): MAPE is somewhat similar to
    RMSE in the sense that it first computes the absolute value of the percentage
    error. Then, these values are averaged over the dataset. Given that this metric
    is scaled in terms of percentage, it is easier to compare MAPE for different datasets.
    However, you have to be mindful of the fact that the percentage error for very
    small values (or zero values) tends to look very big.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MAPE（平均绝对百分比误差）**：MAPE在某种程度上与RMSE相似，因为它首先计算百分比误差的绝对值。然后，这些值在数据集上平均。鉴于这个指标按百分比缩放，比较不同数据集的MAPE更容易。然而，您必须注意，非常小的值（或零值）的百分比误差往往会看起来很大。'
- en: '**SMAPE** (**Symmetric MAPE**): SMAPE is similar to MAPE, but addresses some
    of the shortcomings discussed above. SMAPE bounds the upper percentage value so
    that errors from small values do not overpower the metric. This makes SMAPE a
    good metric that you can easily compare across different problems.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SMAPE（对称绝对百分比误差）**：SMAPE与MAPE类似，但解决了上述讨论的一些缺点。SMAPE限制了上限百分比值，这样小的值的误差就不会压倒指标。这使得SMAPE成为一个可以轻松跨不同问题比较的好指标。'
- en: '**Accuracy**: Accuracy is one of the metrics used for classification problems.
    It can be represented as follows:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**：准确率是用于分类问题的一种指标。它可以表示如下：'
- en: '*Accuracy = number of correct predictions/number of total predictions*'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*准确率 = 正确预测的数量/总预测数量*'
- en: It is essentially the ratio of the number of correct predictions and all predictions.
    For unbalanced problems, this metric can be misleading, hence it is never used
    by itself to determine how well a model did. It is typically used in combination
    with other metrics.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它本质上是指正确预测的数量与所有预测数量的比率。对于不平衡问题，这个指标可能会误导，因此它从不单独用来确定模型的表现如何。它通常与其他指标结合使用。
- en: '**Balanced Accuracy**: Balanced accuracy overcomes the issues with accuracy
    by normalizing the accuracy across the two classes being predicted. Let''s say
    that the two classes are A and B:'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Balanced Accuracy**: 平衡准确率通过在预测的两个类别中归一化准确率来克服准确率的问题。假设两个类别是A和B：'
- en: (a) *Accuracy rate for A = number of correct A predictions/total number of As*
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (a) *A的准确率 = 正确预测A的数量/总数A*
- en: (b) *Accuracy rate for B = number of correct B predictions/total number of Bs*
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (b) *B的准确率 = 正确预测B的数量/总数B*
- en: (c) *Balanced accuracy = accuracy rate for A + accuracy rate for B/2*
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (c) *平衡准确率 = A的准确率 + B的准确率/2*
- en: Balanced accuracy is essentially the average of the accuracy rate for A and
    the accuracy rate for B.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平衡准确率实际上是A的准确率和B的准确率的平均值。
- en: '**AUC** (**Area Under the ROC Curve**): AUC is the area under the **ROC** (**Received
    Operator Characteristic**) curve. This metric is frequently used for classification
    problems as this also overcomes the deficiencies associated with the accuracy
    metric. The ROC curve represents the relationship between the true positive rate
    and the false positive rate. The AUC goes from 0 to 1 and it shows how well the
    model discriminates between the two classes. A value of 0.5 represents a random
    model, so you would want the AUC for your model to be greater than 0.5.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AUC（ROC曲线下的面积**）: AUC是ROC（接收者操作特征）曲线下的面积。这个指标经常用于分类问题，因为它也克服了与准确率指标相关的缺陷。ROC曲线表示真正例率和假正例率之间的关系。AUC从0到1，它显示了模型区分两个类别的效果。0.5表示随机模型，因此您希望模型的AUC大于0.5。'
- en: '**Gamma Deviance**: Gamma deviance is used for regression problems when the
    target values are gamma-distributed. For such targets, gamma deviance measures
    twice the average deviance (using the log-likelihood function) of the predictions
    from the actuals. A model that fits perfectly will have a deviance of zero.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gamma Deviance**: 当目标值呈伽马分布时，使用Gamma偏差进行回归问题。对于此类目标，gamma偏差衡量预测值与实际值之间的平均偏差的两倍（使用对数似然函数）。一个拟合完美的模型将具有零偏差。'
- en: '**Poisson Deviance**: Poisson deviance is used for regression problems when
    the aim is to count data that is skewed. It works in a way that is very similar
    to gamma deviance.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**泊松偏差**: 泊松偏差用于回归问题，当目标是计数偏斜数据时。它的工作方式与伽马偏差非常相似。'
- en: '**LogLoss**: LogLoss (also known as cross-entropy loss) is a measure of the
    inaccuracy of predicted probabilities for a classification problem. A value of
    0 indicates a perfect model, and as the model becomes worse, the logloss value
    increases.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LogLoss**: LogLoss（也称为交叉熵损失）是衡量分类问题预测概率不准确性的指标。值为0表示模型完美，随着模型变差，logloss值增加。'
- en: '**Rsquared**: Rsquared is a metric used for regression problems that tells
    how well the fitted line represents the dataset. Its value ranges between 0 and
    1\. 0 indicates a poor model that explains none of the variation, while a value
    of 1 indicates a perfect model that explains 100% of the variation. It is one
    of the most commonly used metrics, but it can suffer from the problem that you
    can increase it by adding more variables without necessarily improving the model.
    It is also not suitable for non-linear problems.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R-squared**: R-squared是用于回归问题的指标，它说明了拟合线如何代表数据集。其值介于0和1之间。0表示模型较差，无法解释任何变化，而1表示模型完美，解释了100%的变化。这是最常用的指标之一，但它可能存在一个问题，即通过添加更多变量可以提高它，而不一定改善模型。它也不适用于非线性问题。'
- en: Now that we have discussed some of the commonly used metrics, let's look at
    how to look at other results to assess the quality of your model, and the effects
    of different features on your model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了一些常用的指标，让我们看看如何查看其他结果来评估模型的质量，以及不同特征对模型的影响。
- en: Understanding the results
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解结果
- en: In this section, we will discuss various visualizations of metrics and other
    information to understand the results of the modeling exercise. These are important
    visualizations that need to be inspected carefully in addition to looking at the
    model metrics discussed in the previous section. These visualizations are generated
    automatically by DataRobot for any model that it trains.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论各种指标和其他信息的可视化，以理解建模练习的结果。这些是重要的可视化，除了检查上一节中讨论的模型指标外，还需要仔细检查。这些可视化是DataRobot为它训练的任何模型自动生成的。
- en: Lift chart
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 升值图
- en: 'The lift chart shows how effective the model is at predicting the target values.
    As the number of data points is typically very large to show in one graphic, the
    lift chart sorts the output and aggregates the data into multiple bins. It then
    compares the averages of predictions and actuals in each bin (*Figure 2.13*):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 抬升图显示了模型在预测目标值方面的有效性。由于数据点通常非常多，无法在一个图表中显示，因此抬升图对输出进行排序，并将数据聚合到多个桶中。然后，它比较每个桶中预测值和实际值的平均值（*图2.13*）：
- en: '![Figure 2.13 – Lift chart'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.13 – Lift chart](img/Figure_2.13_B17159.jpg)'
- en: '](img/Figure_2.13_B17159.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.13_B17159.jpg](img/Figure_2.13_B17159.jpg)'
- en: Figure 2.13 – Lift chart
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 2.13 – Lift chart
- en: The preceding lift chart shows how the predictions have been sorted from low
    to high and then binned (60 bins in this case). You can now see the average prediction
    and average actual value in each bin. This gives you a sense of how well the model
    is doing across the entire spectrum. You can see whether there are ranges where
    the model is doing worse. If the model is not doing well in a range that is important
    to your business, you can then investigate further to see how you can improve
    the model in that range. You can also inspect different models to see whether
    there is a model that does better in the region that is more important. Lift charts
    are more meaningful for regression problems.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的抬升图显示了预测是如何从低到高排序并分桶的（本例中有60个桶）。现在你可以看到每个桶中的平均预测值和平均实际值。这让你对模型在整个范围内的表现有一个感觉。你可以看到模型表现较差的范围。如果模型在对你业务重要的范围内表现不佳，你可以进一步调查以了解如何在该范围内改进模型。你还可以检查不同的模型，看看是否有在更重要的区域内表现更好的模型。抬升图对于回归问题更有意义。
- en: Confusion matrix (binary and multiclass)
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混淆矩阵（二分类和多分类）
- en: 'For classification problems, one of the best ways to assess model results is
    by looking at the confusion matrix and its associated metrics (*Figure 2.14*).
    This tab is available for multiclass problems:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，评估模型结果的最佳方法之一是查看混淆矩阵及其相关指标（*图2.14*）。此选项卡适用于多分类问题：
- en: '![Figure 2.14 – Confusion matrix'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.14 – Confusion matrix](img/Figure_2.14_B17159.jpg)'
- en: '](img/Figure_2.14_B17159.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.14_B17159.jpg](img/Figure_2.14_B17159.jpg)'
- en: Figure 2.14 – Confusion matrix
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 2.14 – Confusion matrix
- en: The confusion matrix maps predicted versus actual counts (frequency) for each
    class. Let's look at the sedan column. The big green circle indicates how many
    times we correctly classified a sedan as a sedan. In that column, you will also
    see red dots where the model predicted it to be a sedan, but it is a different
    type. You can see these for all classes. The relative scales should give you an
    idea of how well your model did and where it is having difficulty.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵映射了每个类别的预测值与实际值的计数（频率）。让我们看看轿车列。大绿色圆圈表示我们正确地将轿车分类为轿车多少次。在该列中，你还会看到红色点，表示模型预测它为轿车，但实际上它是另一种类型。你可以看到所有类别的这些情况。相对尺度应该能给你一个关于你的模型表现如何以及它在哪些方面有困难的印象。
- en: 'If you select a specific class, you can look at the class-specific confusion
    matrix on the right. You can see two columns (+ for predicting a sedan, - for
    predicting something that isn''t a sedan). Similarly, you see two rows (+ where
    it is a sedan, and - for when it is not a sedan). You also see some critical definitions
    and metrics:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择一个特定的类别，你可以查看右侧的类别特定混淆矩阵。你可以看到两列（+表示预测为轿车，-表示预测为非轿车）。同样，你看到两行（+表示它是轿车，-表示它不是轿车）。你还可以看到一些关键的定义和指标：
- en: '**True Positives** (**TP**) = Where it is a sedan and is predicted as a sedan'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阳性**（**TP**）= 当它是轿车且预测结果显示是轿车时'
- en: '**False Positives** (**FP**) = Where it is not a sedan but is predicted as
    a sedan'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性**（**FP**）= 当它不是轿车但预测结果却显示是轿车时'
- en: '**True Negatives** (**TN**) = Where it is not a sedan and is predicted as not
    being a sedan'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阴性**（**TN**）= 当它不是轿车且预测结果显示不是轿车时'
- en: '**False Negatives** (**FN**) = Where it is a sedan but is predicted as not
    being a sedan'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性**（**FN**）= 当它是一辆轿车但预测结果却显示不是轿车时'
- en: 'Using these, we can now compute some specific metrics for this class:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些，我们现在可以计算这个类别的某些特定指标：
- en: '*Precision = correct fraction of predictions = TP/All Positive Predictions
    = TP/(TP+FP)*'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*精确度 = 预测正确的比例 = TP/所有正预测 = TP/(TP+FP)*'
- en: '*Recall = correct fraction of actuals = TP/All Positive Actuals = TP/(TP+FN)*'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*召回率 = 实际正确的比例 = TP/所有正实际值 = TP/(TP+FN)*'
- en: '*F1 Score = harmonic mean of precision and recall. So, 1/F1 = 1/Precision +
    1/Recall*'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*F1分数 = 精确度和召回率的调和平均。所以，1/F1 = 1/精确度 + 1/召回率*'
- en: ROC
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ROC
- en: 'This tab is available for binary classification problems. The **ROC** (**Receiver
    Operator Characteristic**) curve is the relationship between the true positive
    rate and the false positive rate. The area under this curve is known as AUC. It
    goes from 0 to 1 and it shows how well the model discriminates between the two
    classes (*Figure 2.15*):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 此标签适用于二元分类问题。**ROC**（**接收者操作特征**）曲线是真正例率和假正例率之间的关系。此曲线下的面积称为AUC。它从0到1，显示了模型在两个类别之间的区分能力（*图2.15*）：
- en: '![Figure 2.15 – ROC curve and confusion matrix'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.15 – ROC曲线和混淆矩阵'
- en: '](img/Figure_2.15_B17159.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.15_B17159.jpg)'
- en: Figure 2.15 – ROC curve and confusion matrix
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.15 – ROC曲线和混淆矩阵
- en: You can also see the confusion matrix (described earlier) and the associated
    metrics for the two classes. You can move the thresholds and assess the resulting
    trade-offs and cumulative gains. Since most problems are not symmetric in the
    sense that true positives have different business values compared to true negatives,
    you should select the threshold that makes sense for your business problem.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以看到（之前描述的）混淆矩阵和两个类别的相关指标。你可以移动阈值并评估产生的权衡和累积收益。由于大多数问题在真实正例与真实负例具有不同业务价值的情况下并不对称，你应该选择适合你业务问题的阈值。
- en: Accuracy over time
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随时间变化的准确率
- en: 'This tab is available for time series problems (*Figure 2.16*) and compares
    the actual versus predicted values over time for a series:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 此标签适用于时间序列问题（*图2.16*）并比较序列随时间变化的实际值与预测值：
- en: '![Figure 2.16 – Model accuracy over time'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.16 – 模型准确率随时间变化'
- en: '](img/Figure_2.16_B17159.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.16_B17159.jpg)'
- en: Figure 2.16 – Model accuracy over time
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16 – 模型准确率随时间变化
- en: You can view these values for the backtests or the holdout datasets. The diagram
    will clearly show where the model is not performing well and what you might want
    to focus on to improve your model.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看这些值对于回测或保留数据集。图表将清楚地显示模型表现不佳的地方以及你可能想要关注以改进模型的地方。
- en: Feature impacts
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征影响
- en: 'Besides model performance, one of the first things you want to understand is
    how impactful the features are in terms of your model''s performance. The **Feature
    Impacts** tab (*Figure 2.17*) is perhaps the most critical for understanding your
    model:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型性能之外，你首先想要了解的是特征在模型性能方面的影响程度。**特征影响**标签（*图2.17*）可能是理解你的模型最关键的部分：
- en: '![Figure 2.17 – Feature impacts'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.17 – 特征影响'
- en: '](img/Figure_2.17_B17159.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.17_B17159.jpg)'
- en: Figure 2.17 – Feature impacts
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17 – 特征影响
- en: The graphic shows a sorted list of the most important features. For each feature,
    you can see the relative impact that a feature has on this model. You can see
    which features contribute very little; this can be used to create new feature
    lists by removing some of the features that have very little impact.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图形显示了一个按重要性排序的最重要特征列表。对于每个特征，你可以看到该特征对模型相对影响的程度。你可以看到哪些特征贡献很小；这可以通过移除一些影响很小的特征来创建新的特征列表。
- en: Feature Fit
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征拟合
- en: 'The **Feature Fit** tab (*Figure 2.18*) shows an alternative view of the contribution
    of a feature. The graphic shows the features ranked by their importance:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征拟合**标签（*图2.18*）显示了特征的另一种贡献视图。图形显示了按重要性排序的特征：'
- en: '![Figure 2.18 – Feature Fit'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.18 – 特征拟合'
- en: '](img/Figure_2.18_B17159.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.18_B17159.jpg)'
- en: Figure 2.18 – Feature Fit
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.18 – 特征拟合
- en: For the selected feature, it shows how the predictions compare to actuals for
    the range of values of a feature. Reviewing these graphs for the key features
    can provide a lot of insight about how a feature impacts the results and range
    of values that perform better and ranges where it performs the worst. This could
    sometimes highlight the regions where you might need to collect more data to improve
    your model.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对于选定的特征，它显示了预测值与特征值范围的实际情况的比较。回顾这些关键特征的图表可以提供关于特征如何影响结果以及表现较好的值范围和表现最差的值范围的很多见解。这有时可能会突出你可能需要收集更多数据以改进模型的数据区域。
- en: Feature Effects
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征效应
- en: '**Feature Effects** show information that is very similar to **Feature Fit**
    (*Figure 2.19*). In this graphic, the features are sorted by **Feature Impacts**.
    Also, **Feature Effects** are focused on partial dependence:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征效应**显示的信息与**特征拟合**（*图2.19*）非常相似。在此图形中，特征按**特征影响**排序。此外，**特征效应**专注于部分依赖性：'
- en: '![Figure 2.19 – Feature Effects and Partial Dependence'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.19 – 特征效应和部分依赖性'
- en: '](img/Figure_2.19_B17159.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.19_B17159.jpg)'
- en: Figure 2.19 – Feature Effects and Partial Dependence
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.19 – 特征效应和局部依赖性
- en: Partial dependence plots are one of the most important plots that you want to
    study carefully. These plots tell you how a change in the value of a feature impacts
    the change in the average value of the target over a range of values for the other
    features. This insight is critical to understanding the business problem, understanding
    what the model is doing, and, more importantly, what aspects of the model are
    actionable and what range of values will produce the maximum impact.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 局部依赖性图是您想要仔细研究的最重要图表之一。这些图表告诉您特征值的变化如何影响其他特征值范围内目标平均值的改变。这种洞察力对于理解业务问题、理解模型正在做什么以及更重要的是，模型的哪些方面是可操作的以及哪些值范围会产生最大影响至关重要。
- en: Prediction Explanations
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测解释
- en: '**Prediction Explanations** describe the reasons for a specific prediction
    in terms of feature values for the specific instance or row that is being scored
    (*Figure 2.20*). Note that this is different from **Feature Impacts**, which tell
    you the importance of a feature at a global level:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测解释**描述了针对特定实例或正在评分的行的特征值来解释特定预测的原因（*图2.20*）。请注意，这与**特征影响**不同，它告诉你特征在全局级别的重要性：'
- en: '![Figure 2.20 – Prediction Explanations'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.20 – 预测解释'
- en: '](img/Figure_2.20_B17159.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_2.20_B17159.jpg](img/Figure_2.20_B17159.jpg)'
- en: Figure 2.20 – Prediction Explanations
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.20 – 预测解释
- en: '**Prediction Explanations** can be generated for an entire dataset or a subset
    of data, as shown in the preceding screenshot. For example, it will provide the
    top three reasons why the model predicted a specific value. These explanations
    are sometimes required for regulatory reasons in certain use cases, but it is
    a good idea to produce these explanations as they do help in understanding why
    a model predicts a certain way and can be very useful in validating or catching
    errors in a model. DataRobot uses two algorithms for computing the explanations:
    **XEMP** (**exemplar-based explanations**) or **Shapley values**. XEMP is supported
    for a broader range of models and is selected by default. Shapley values are described
    in the next section.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测解释**可以为整个数据集或数据子集生成，如前述截图所示。例如，它将提供模型预测特定值的前三个原因。在某些用例中，出于监管原因，这些解释有时是必需的，但生成这些解释是一个好主意，因为它们有助于理解模型为何以某种方式预测，并且在验证或捕捉模型中的错误时非常有用。DataRobot使用两种算法来计算解释：**XEMP**（**基于示例的解释**）或**Shapley值**。XEMP支持更广泛的模型，并默认选择。Shapley值将在下一节中描述。'
- en: Shapley values
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Shapley值
- en: '**Shapley** **values** (**SHAP**) are an alternative mechanism for producing
    prediction explanations (*Figure 2.21*). If you want to use SHAP for explanations,
    you have to specify this in the advanced options during the project setup before
    you press the **Start** button. Once DataRobot starts building the models, you
    cannot switch to SHAP. SHAP values are only available for linear or tree-based
    models and are not available for ensemble models:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**Shapley** **值**（**SHAP**）是生成预测解释的另一种机制（*图2.21*）。如果您想使用SHAP进行解释，您必须在按下**开始**按钮之前的项目设置的高级选项中指定这一点。一旦DataRobot开始构建模型，您就不能切换到SHAP。SHAP值仅适用于线性或基于树的模型，并且对于集成模型不可用：'
- en: '![Figure 2.21 – SHAP-based explanations'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.21 – 基于SHAP的解释'
- en: '](img/Figure_2.21_B17159.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_2.21_B17159.jpg](img/Figure_2.21_B17159.jpg)'
- en: Figure 2.21 – SHAP-based explanations
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.21 – 基于SHAP的解释
- en: SHAP values are based on cooperative game theory, which tries to assign values
    to contributions of a team member in a collaborative project. In the context of
    machine learning, it tries to assign the value contribution of a specific feature
    when there is a team of features collaborating to make a prediction. SHAP values
    are additive and you can easily see how much of the final answer is due to a specific
    feature value.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP值基于合作博弈论，它试图为协作项目中团队成员的贡献分配价值。在机器学习的背景下，它试图在特征团队协作进行预测时分配特定特征的价值贡献。SHAP值是可加的，您可以轻松地看到最终答案中有多少是由于特定特征值。
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered some of the basic machine learning concepts that
    will come in handy as we go through the remaining chapters, and they will also
    be useful in your data science journey. Please note that we have only covered
    concepts at a high level, and depending on your job role, you might want to explore
    some areas in more detail. We have also related this material to how DataRobot
    performs certain functions and where you need to pay closer attention.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一些基本的机器学习概念，这些概念在我们继续阅读剩余章节时将非常有用，并且在你的数据科学之旅中也将非常有用。请注意，我们只对概念进行了高层次概述，根据你的工作角色，你可能需要更深入地探索一些领域。我们还将这些材料与DataRobot执行某些功能的方式以及你需要更加关注的地方联系起来。
- en: 'Hopefully, this has given you some insights into what DataRobot will be displaying
    and where to focus your attention in different stages of your project. Since DataRobot
    automates a good chunk of model building and prediction tasks, it might be tempting
    to ignore many of the outputs that DataRobot is automatically producing for you.
    Please resist that temptation. DataRobot software is taking considerable pains
    and resources to produce those outputs for a very good reason. It is also doing
    much of the grunt work for you, so please take advantage of those capabilities.
    Specifically, we have covered the following: What are the things to watch out
    for during data preparation? What data visualizations are important for gaining
    an understanding of your dataset? What are the key machine learning algorithms,
    and when do you use them? How do you measure the goodness of your model results?
    How do you assess model performance and understand what the model is telling you
    about your problem?'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这已经让你对DataRobot将显示的内容以及在你项目不同阶段需要关注的地方有了更深的了解。由于DataRobot自动化了相当一部分模型构建和预测任务，你可能会有忽略DataRobot为你自动生成的大多数输出的诱惑。请抵制这种诱惑。DataRobot软件在产生这些输出时投入了大量的努力和资源，这是有很好的理由的。它还在为你做大量的基础工作，所以请充分利用这些功能。具体来说，我们涵盖了以下内容：在数据准备过程中需要注意哪些事项？哪些数据可视化对于理解你的数据集很重要？关键机器学习算法有哪些，你何时使用它们？你如何衡量模型结果的好坏？你如何评估模型性能并理解模型对你问题的描述？
- en: Now that we know the basics, we will start our data science journey in the next
    chapter by learning how to understand the business problem and how to turn it
    into a specification that can be solved by using machine learning.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了基础知识，我们将在下一章开始我们的数据科学之旅，学习如何理解业务问题以及如何将其转化为可以通过机器学习解决的问题的规范。
