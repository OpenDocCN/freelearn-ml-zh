- en: '*Chapter 5:*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第五章*：'
- en: Classification
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Define binary classification in supervised machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义监督机器学习中的二元分类
- en: 'Perform binary classification using white-box models: logistic regression and
    decision trees'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用白盒模型进行二元分类：逻辑回归和决策树
- en: Evaluate the performance of supervised classification models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估监督分类模型的表现
- en: Perform binary classification using black-box ensemble models – Random Forest
    and XGBoost
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用黑盒集成模型进行二元分类 - 随机森林和XGBoost
- en: Design and develop deep neural networks for classification
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计和开发用于分类的深度神经网络
- en: Select the best model for a given classification use case
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为给定的分类用例选择最佳模型
- en: In this chapter, we will focus on solving classification use cases for supervised
    learning. We will use a dataset designed for a classification use case, frame
    a business problem around it, and explore a few popular techniques to solve the
    problem.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于解决监督学习的分类用例。我们将使用一个为分类用例设计的数据库，围绕它构建一个业务问题，并探索一些流行的技术来解决该问题。
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简介
- en: Let's quickly brush up on the topics we learned in *Chapter 3*, *Introduction
    to Supervised Learning*. Supervised learning, as you already know by now, is the
    branch of machine learning and artificial intelligence that helps machines learn
    without explicit programming. A more simplified way of describing supervised learning
    would be developing algorithms that learn from labeled data. The broad categories
    in supervised learning are classification and regression, differentiated fundamentally
    by the type of label, that is, **continuous** or **categorical**. Algorithms that
    deal with continuous variables are known as **regression algorithms**, and those
    with categorical variables are called **classification algorithms**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速回顾一下我们在*第三章*中学到的内容，*监督学习简介*。正如你现在所知道的，监督学习是机器学习和人工智能的一个分支，它帮助机器在没有明确编程的情况下学习。描述监督学习的一种更简单的方式是开发从标记数据中学习的算法。监督学习的广泛类别包括分类和回归，它们的基本区别在于标签的类型，即**连续**或**分类**。处理连续变量的算法被称为**回归算法**，而处理分类变量的算法被称为**分类算法**。
- en: 'In classification algorithms, our target, dependent, or criterion variable
    is a **categorical variable**. Based on the number of classes, we can further
    divide them into the following groups:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类算法中，我们的目标、依赖或标准变量是一个**分类变量**。根据类别的数量，我们可以进一步将它们分为以下几组：
- en: Binary classification
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二元分类
- en: Multinomial classification
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式分类
- en: Multi-label classification
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多标签分类
- en: In this chapter, we will focus on **binary classification**. Discussing the
    specifics and practical examples of multinomial and multi-class classification
    is beyond the scope of this chapter; however, a few additional reading references
    for advanced topics will be listed before wrapping up the chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于**二元分类**。讨论多项式分类和多类分类的细节和实际例子超出了本章的范围；然而，在结束本章之前，我们将列出一些高级主题的额外阅读参考。
- en: Binary classification algorithms are the most popular class of algorithms within
    machine learning and have numerous applications in business, research, and academia.
    Simple models that classify a student's chances of passing a future exam based
    on their past performance as pass or fail, predict whether it will rain or not,
    predict whether a customer will default on a loan or not, predict whether a patient
    has cancer or not, and so on are all common use cases that are solved by classification
    algorithms.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类算法是机器学习中最受欢迎的算法类别，在商业、研究和学术界有众多应用。简单的模型可以根据学生的过去表现（通过或失败）来预测他们未来考试通过的可能性，预测是否会下雨，预测客户是否会违约，预测患者是否患有癌症等等，这些都是由分类算法解决的常见用例。
- en: Before diving deeper into algorithms, we will first get started with a use case
    that will help us solve a supervised learning classification problem with hands-on
    exercises.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究算法之前，我们将首先从一个小案例开始，这个小案例将帮助我们通过实际练习解决监督学习分类问题。
- en: Getting Started with the Use Case
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用用例
- en: In this chapter, we will refer to the `weather` dataset, obtained from the Australian
    Commonwealth Bureau of Meteorology and made available through R. The dataset has
    two target variables, `RainTomorrow`, a flag indicating whether it will rain tomorrow,
    and `RISK_MM`, which measures the amount of rainfall for the following day.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将参考来自澳大利亚联邦气象局并通过R提供的`weather`数据集。该数据集有两个目标变量，`RainTomorrow`，一个表示明天是否会下雨的标志，以及`RISK_MM`，它衡量的是下一天降雨量。
- en: In a nutshell, we can use this dataset for `RainTomorrow`, for our classification
    exercise. The metadata and additional details about the dataset are available
    to explore at https://www.rdocumentation.org/packages/rattle/versions/5.2.0/topics/weather.
    Since the dataset is readily available through R, we don't need to separately
    download it; instead, we can directly use the R function within the `rattle` library
    to load the data into system memory.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们可以使用这个数据集的`RainTomorrow`进行分类练习。有关数据集的元数据和附加详细信息可在https://www.rdocumentation.org/packages/rattle/versions/5.2.0/topics/weather上探索。由于数据集可以通过R直接使用，我们不需要单独下载它；相反，我们可以直接使用`rattle`库中的R函数将数据加载到系统内存中。
- en: Some Background on the Use Case
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于用例的一些背景信息
- en: Several weather parameters, such as temperature, direction, pressure, cloud
    cover, humidity, and sunshine, were recorded daily for one year. The rainfall
    for the next day is already engineered in the dataset as the target variable,
    `RainTomorrow`. We can leverage this data to define a machine learning model that
    learns from the present day's weather parameters and predicts the chances of rain
    for the next day.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 记录了温度、方向、气压、云量、湿度和日照等天气参数，持续一年。下一天的降雨量已经在数据集中作为目标变量`RainTomorrow`进行工程化。我们可以利用这些数据定义一个机器学习模型，该模型从当天的天气参数中学习，并预测下一天的降雨概率。
- en: Rainfall prediction is of paramount importance to many industries. Long-haul
    journeys by train and buses usually look at changing weather patterns, primarily
    rainfall, to estimate the arrival time and journey length. Similarly, most brick
    and mortar stores, small restaurants and food joints, and others are all heavily
    impacted by rainfall. Gaining visibility of the weather conditions for the next
    day can help businesses better prepare in several ways, to combat business losses
    and, in some cases, maximize business outcomes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 雨量预测对许多行业至关重要。火车和长途汽车的长途旅行通常关注天气模式的变化，主要是降雨，以估计到达时间和旅行长度。同样，大多数实体店、小型餐馆和食品摊位等也受到降雨的严重影响。了解明天的天气条件可以帮助企业从多个方面更好地准备，以应对业务损失，在某些情况下，甚至可以最大化业务成果。
- en: To build nice intuition around the problem-solving exercise, let's frame a business
    problem using the dataset and develop the problem statement for the use case.
    Since the data is about rainfall prediction, we will choose a popular business
    problem faced by today's hyper-local food-delivery services. Start-ups such as
    DoorDash, Skip the Dishes, FoodPanda, Swiggy, Foodora, and many others offer hyper-local
    food delivery services to customers in different countries. A common trend observed
    in most countries is the rise in food delivery orders with the onset of rain.
    In general, most delivery companies expect around a 30%-40% increase in the total
    number of deliveries on a given day. Given the limited number of delivery agents,
    the delivery time is impacted immensely due to increased orders on rainy days.
    To keep costs optimal, it is not viable for these companies to increase the number
    of full-time agents; therefore, a common strategy is to dynamically hire more
    agents for days when demand for the service is expected to be high. To plan better,
    visibility of rainfall predictions for the next day is of paramount importance.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在问题解决练习中建立良好的直觉，让我们使用数据集构建一个业务问题，并为用例制定问题陈述。由于数据是关于降雨预测的，我们将选择当今超本地食品配送服务面临的流行业务问题。DoorDash、Skip
    the Dishes、FoodPanda、Swiggy、Foodora等初创公司为不同国家的客户提供超本地食品配送服务。在大多数国家，一个普遍的趋势是随着雨季的到来，食品配送订单量增加。一般来说，大多数配送公司预计在给定的一天内总配送量会增加30%-40%。由于配送代理人数有限，雨天订单的增加对配送时间影响巨大。为了保持成本最优，这些公司不可能增加全职代理人数；因此，一个常见的策略是在预计服务需求高的日子里动态雇佣更多代理。为了更好地规划，了解下一天的降雨预测至关重要。
- en: Defining the Problem Statement
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义问题陈述
- en: With the context of the problem set up, let's try to define our problem statement
    for a hyper-local food-delivery service company to predict the rainfall for the
    next day. To keep things simple and consistent, let's frame the problem statement
    using the frameworks we studied previously, in *Chapter 2*, *Exploratory Analysis
    of Data*. This will help us distill the end goal we want to solve in a business-first
    approach while keeping the machine learning perspective at the forefront.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好问题背景后，让我们尝试为一家超本地食品配送服务公司定义我们的问题陈述，以预测明天的降雨量。为了保持简单和一致性，让我们使用之前研究的框架，即*第2章*，*数据探索分析*来构建问题陈述。这将帮助我们以业务优先的方法提炼出我们想要解决的最终目标，同时将机器学习视角放在首位。
- en: 'The following figure creates a simple visual for the **Situation** - **Complication**
    - **Question** (**SCQ**) framework for the previously defined use case:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了之前定义的使用案例的简单视觉框架——**情境**-**复杂性**-**问题**（**SCQ**）框架：
- en: '![Figure 5.1: SCQ for the classification use case'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.1：分类使用案例的SCQ'
- en: '](img/C12624_05_01.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12624_05_01.jpg)'
- en: 'Figure 5.1: SCQ for the classification use case'
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.1：分类使用案例的SCQ
- en: 'We can clearly answer the question from the SCQ: we would need a predictive
    model to predict the chances of rain for the next day as a solution to the problem.
    Let''s move on to the next step – gathering data to build a predictive model that
    will help us solve the business problem.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地从SCQ中回答问题：我们需要一个预测模型来预测明天的降雨概率，作为解决问题的解决方案。让我们继续下一步——收集数据以构建一个预测模型，这将帮助我们解决业务问题。
- en: Data Gathering
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据收集
- en: The `rattle.data` package provides us with the data for the use case, which
    can be accessed using the internal dataset methods of R. In case you have not
    already installed the packages, you can easily install them using the `install.packages("rattle.data")`
    command.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`rattle.data`包为我们提供了使用案例的数据，可以使用R的内部数据集方法访问。如果你还没有安装这些包，你可以使用`install.packages("rattle.data")`命令轻松安装它们。'
- en: 'Exercise 63: Exploring Data for the Use Case'
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习63：探索使用案例的数据
- en: In this exercise, we will perform the initial exploration of the dataset we
    have gathered for the use case. We will explore the shape of the data, that is,
    the number of rows and columns, and study the content within each column.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将对为使用案例收集的数据集进行初步探索。我们将探索数据的形状，即行数和列数，并研究每个列中的内容。
- en: 'To explore the shape (rows x columns) and content of the data, perform the
    following steps:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索数据的形状（行数x列数）和内容，执行以下步骤：
- en: 'First, load the `rattle` package using the following command:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令加载`rattle`包：
- en: '[PRE0]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load the data for our use case, which is available from the `rattle` package:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载我们使用案例的数据，这些数据可以从`rattle`包中获取：
- en: '[PRE1]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The `weatherAUS` dataset is a DataFrame containing more than 1,40,000 daily
    observations from over 45 Australian weather stations.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`weatherAUS`数据集是一个DataFrame，包含来自45个以上澳大利亚气象站的超过140,000条每日观测数据。'
- en: 'Now, load the weather data directly into a DataFrame called `df`:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将天气数据直接加载到名为`df`的DataFrame中：
- en: '[PRE2]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Explore the DataFrame''s content using the `str` command:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`str`命令探索DataFrame的内容：
- en: '[PRE3]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is as follows:'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.2: Final output'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.2：最终输出'
- en: '](img/C12624_05_02.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12624_05_02.jpg)'
- en: 'Figure 5.2: Final output'
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.2：最终输出
- en: We have almost 1,50,000 rows of data and 24 variables. We would need to drop
    the `RISK_MM` variable, as it will be the target variable for the regression use
    case (that is, predicting how much it will rain the next day). Therefore, we are
    left with 22 independent variables and 1 dependent variable, `RainTomorrow`, for
    our use case. We can also see a good mix of continuous and categorical variables.
    The `Location`, `WindDir`, `RainToday`, and many more variables are categorical,
    and the remainder are continuous.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有近150,000行数据，24个变量。我们需要删除`RISK_MM`变量，因为它将是回归使用案例（即预测明天下雨量）的目标变量。因此，我们剩下22个独立变量和1个因变量`RainTomorrow`，用于我们的使用案例。我们还可以看到连续变量和分类变量的良好混合。`Location`、`WindDir`、`RainToday`等多个变量是分类的，其余的是连续的。
- en: Note
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/2Vwgu8Q.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub上找到完整的代码：http://bit.ly/2Vwgu8Q。
- en: In the next exercise, we will calculate the total percentage of the null values
    in each column.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将计算每个列中缺失值的总百分比。
- en: 'Exercise 64: Calculating the Null Value Percentage in All Columns'
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习64：计算所有列的缺失值百分比
- en: The dataset we explored in *Exercise 1*, *Exploring Data for the Use Case* has
    quite a few null values. In this exercise, we will write a script to calculate
    the percentage of null values within each column.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*练习 1*，*探索数据集以用于用例*中探索的数据集有相当多的空值。在这个练习中，我们将编写一个脚本来计算每个列中空值的百分比。
- en: We can see the presence of null values in a few variables. Let's check the percentage
    of null values in each column within the `df` dataset.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到几个变量中存在空值。让我们检查`df`数据集中每个列的空值百分比。
- en: 'Perform the following steps to calculate the percentage of null values in each
    column of the dataset:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以计算数据集中每列的空值百分比：
- en: 'First, remove the column named `RISK_MM`, since it is supposed to be used as
    a target variable for regression use. (Adding this to our model will result in
    data leakage.):'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，移除名为`RISK_MM`的列，因为它打算用作回归用途的目标变量。（将其添加到我们的模型会导致数据泄露）：
- en: '[PRE4]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Create a `temp_df` DataFrame object and store the value in it:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`temp_df` DataFrame对象并将其值存储在其中：
- en: '[PRE5]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, use the `print` function to display the percentage null values in each
    column using the following command:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用`print`函数显示每列的空值百分比，使用以下命令：
- en: '[PRE6]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE7]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can see that the last four variables have more than *30%* missing or null
    values. This is a significantly huge drop. It would be best to drop these variables
    from our analysis. Also, we can see that there are a few other variables that
    have roughly *1%*-*2%*, and in some cases, up to *10%* missing or null values.
    We can treat these variables using various missing value treatment techniques,
    such as replacing them with mean or mode. In some important cases, we can also
    use additional techniques, such as clustering-based mean and mode replacement,
    for improved treatment. Additionally, in very critical scenarios, we can use a
    regression model to estimate the remainder of the missing values by defining a
    model where the column with the required missing value is treated as a function
    of the remaining variables.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到最后四个变量有超过*30%*的缺失或空值。这是一个相当大的下降。最好从我们的分析中删除这些变量。此外，我们还可以看到一些其他变量大约有*1%*到*2%，在某些情况下，高达*10%*的缺失或空值。我们可以使用各种缺失值处理技术来处理这些变量，例如用均值或众数替换它们。在某些重要情况下，我们还可以使用基于聚类的均值和众数替换等额外技术来提高处理效果。此外，在非常关键的场景中，我们可以使用回归模型来估计剩余缺失值的剩余部分，通过定义一个模型，其中所需的缺失值列被视为剩余变量的函数。
- en: Note
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/2ViZEp1.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到完整的代码：http://bit.ly/2ViZEp1。
- en: In the following exercise, we will remove null values. We will revisit data
    if we do not have a good model in place.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下练习中，我们将移除空值。如果没有合适的模型，我们将重新审视数据。
- en: 'Exercise 65: Removing Null Values from the Dataset'
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 65：从数据集中移除空值
- en: John is working on the newly created dataset, and while doing analysis, he has
    found out that the dataset contains significant null values. To make the dataset
    useful for further analysis, he must remove the null values from it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 约翰正在处理新创建的数据集，在进行分析时，他发现数据集中存在显著的空值。为了使数据集对进一步分析有用，他必须从其中移除空值。
- en: 'Perform the following steps to remove the null values from the `df` dataset:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以从`df`数据集中移除空值：
- en: 'First, select the last four columns to drop that have more than *30%* null
    values using the following command:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令选择最后四列，这些列的空值超过*30%*：
- en: '[PRE8]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Remove all the rows from the DataFrame that will have one or more columns with
    null values using the `na.omit` command, which removes all of the null rows from
    the DataFrame:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`na.omit`命令从DataFrame中移除所有包含一个或多个空值列的行，该命令会从DataFrame中移除所有空行：
- en: '[PRE9]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, print the newly formatted data using the following `print` commands:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下`print`命令打印新格式化的数据：
- en: '[PRE10]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE11]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Using the following command, verify whether the newly created dataset contains
    null values or not:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令，验证新创建的数据集中是否存在空值：
- en: '[PRE12]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, print the dataset using the following `print` command:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下`print`命令打印数据集：
- en: '[PRE13]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE14]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can now double check and see that the new dataset has no more missing values
    and the overall number of rows in the dataset also reduced to 112,000, which is
    around a *20%* loss of training data. We should use missing value treatment techniques
    such as replacing missing values with the mean, mode, or median to combat such
    high losses due to the omission of missing values. A rule of thumb would be to
    safely ignore anything less than a *5%* loss. Since, we have more than 1,00,000
    records (a reasonably high number of records for a simple use case), we are ignoring
    this rule of thumb.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以再次检查，看看新的数据集没有更多的缺失值，数据集的总行数也减少到112,000行，这大约是训练数据的*20%*损失。我们应该使用替换缺失值（如平均值、众数或中位数）等技术来对抗由于缺失值的省略而导致的高损失。一个经验法则是安全地忽略小于*5%*的损失。由于我们有超过100,000条记录（对于简单用例来说是一个相当高的记录数），我们正在忽略这个经验法则。
- en: Note
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/2Q3HIgT.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到完整的代码：http://bit.ly/2Q3HIgT。
- en: Additionally, we can also engineer date- and time-related features using the
    `Date` column. The following exercise creates numeric features such as day, month,
    day of the week, and quarter of the year as additional time-related features and
    drops the original `Date` variable.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以使用`Date`列来构建日期和时间相关的特征。以下练习创建了诸如日、月、星期和季度等数值特征作为额外的时相关特征，并删除了原始的`Date`变量。
- en: We will use the `lubridate` library in R to work with date and time-related
    features. It provides us with extremely easy-to-use functions to perform date
    and time operations. If you have not already installed the package, please install
    the library using the `install.packages('lubridate')` command.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用R中的`lubridate`库来处理日期和时间相关特征。它为我们提供了执行日期和时间操作的极其易于使用的函数。如果您尚未安装此包，请使用`install.packages('lubridate')`命令安装库。
- en: 'Exercise 66: Engineer Time-Based Features from the Date Variable'
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习66：从日期变量中构建基于时间的特征
- en: Time- and date-related attributes cannot be directly used in a supervised classification
    model. To extract meaningful properties from date- and time-related variables,
    it is a common practice to create month, year, week, and quarter from the date
    as features.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 时间和日期相关的属性不能直接用于监督分类模型。为了从日期和时间相关的变量中提取有意义的属性，通常的做法是从日期中创建月份、年份、星期和季度作为特征。
- en: 'Perform the following steps to work with the data and time function in R:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以在R中处理数据和时间函数：
- en: 'Import the `lubridate` library into RStudio using the following command:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将`lubridate`库导入RStudio：
- en: '[PRE15]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The `lubridate` library provides handy date- and time-related functions.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`lubridate`库提供了方便的日期和时间相关函数。'
- en: 'Extract `day`, `month`, `dayofweek`, and `quarter` as new features from the
    `Date` variable using the `lubridate` function:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`lubridate`函数从`Date`变量中提取`day`、`month`、`dayofweek`和`quarter`作为新特征：
- en: '[PRE16]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Examine the newly created variables:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查新创建的变量：
- en: '[PRE17]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now that we have created all of the date- and time-related features, we won''t
    need the actual `Date` variable. Therefore, delete the previous `Date` column:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经创建了所有日期和时间相关的特征，我们不再需要实际的`Date`变量。因此，删除之前的`Date`列：
- en: '[PRE18]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE19]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In this exercise, we have extracted meaningful features from date- and time-related
    attributes from the data and removed the actual date-related columns.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们从日期和时间相关的属性中提取了有意义的特征，并删除了实际的日期相关列。
- en: Note
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/2E4hOEU.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到完整的代码：http://bit.ly/2E4hOEU。
- en: 'Next, we need to process or clean another feature within the DataFrame: `location`.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要处理或清理DataFrame中的另一个特征：`location`。
- en: 'Exercise 67: Exploring the Location Frequency'
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习67：探索位置频率
- en: The `Location` variable defines the actual location where the weather data was
    captured for the specified time. Let's do a quick check on the number of distinct
    values that are captured within this variable and see whether there are any interesting
    patterns that might be of importance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`Location`变量定义了在指定时间实际捕获天气数据的实际位置。让我们快速检查这个变量中捕获的不同值的数量，看看是否有任何可能重要的有趣模式。'
- en: In the following exercise, we will be using the `Location` variable to define
    the actual location where the weather data was captured for the specified time.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下练习中，我们将使用`Location`变量来定义在指定时间实际捕获天气数据的实际位置。
- en: 'Perform the following steps:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'Calculate the frequency of rain across each location using the grouping functions
    from the `dplyr` package:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`dplyr`包中的分组函数计算每个位置的降雨频率：
- en: '[PRE20]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Examine the number of distinct locations for sanity:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查不同位置的数量以确保正确：
- en: '[PRE21]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output is as follows:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE22]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Print `summary` to examine the aggregation performed:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印`summary`以检查执行的聚合：
- en: '[PRE23]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE24]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We can see that there are 44 distinct locations in the data. The `cnt` variable,
    which defines the number of records (in the previous transformed data) for each
    location, has an average 2,566 records. The similar number distribution between
    the first quartile, median, and third quartile denote that the locations are evenly
    distributed in the data. However, if we investigate the percentage of records
    where rain was recorded (`pct`), we see an interesting trend. Here, we have locations
    with around a *6%* chance of rain and some with around a *36%* chance of rain.
    There is a huge difference in the possibility of rain, based on the location.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到数据中有44个不同的位置。定义每个位置记录数（在之前转换的数据中）的`cnt`变量，平均有2,566条记录。第一四分位数、中位数和第三四分位数的相似数量分布表明，位置在数据中分布均匀。然而，如果我们调查记录降雨的记录百分比（`pct`），我们会看到一个有趣的趋势。在这里，我们有一些位置大约有6%的降雨概率，还有一些位置大约有36%的降雨概率。根据位置的不同，降雨的可能性有很大的差异。
- en: Note
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/30aKUMx.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub上找到完整的代码：http://bit.ly/30aKUMx。
- en: Since we have around 44 distinct locations, it is difficult to utilize this
    variable directly as a categorical feature. In R, most supervised learning algorithms
    internally convert the categorical column into a numerical form that can be interpreted
    by the model. However, with an increased number of classes within the categorical
    variable, the complexity of the model increases with no additional value. To keep
    things simple, we can transform the `Location` variable as a new variable with
    a reduced number of levels. We will select the top five and the bottom five locations
    with chances of rain and tag all other locations as `Others`. This will reduce
    the number of distinct levels in the variable as 10+1 and will be more suitable
    for the model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有大约44个不同的位置，直接将此变量作为分类特征使用是有困难的。在R中，大多数监督学习算法内部将分类列转换为模型可以解释的数值形式。然而，随着分类变量中类别的数量增加，模型的复杂性也随之增加，但没有额外的价值。为了保持简单，我们可以将`Location`变量转换为一个具有较少级别的新的变量。我们将选择降雨概率最高的五个和最低的五个位置，并将所有其他位置标记为`Others`。这将减少变量中不同级别的数量为10+1，这将更适合模型。
- en: 'Exercise 68: Engineering the New Location with Reduced Levels'
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习68：创建具有较少级别的新的位置
- en: The `location` variable has too many distinct values (44 locations), and machine
    learning models in general do not perform well with categorical variables with
    a high frequency of distinct classes. We therefore need to prune the variable
    by reducing the number of distinct classes within it. We will select the top five
    and the bottom five locations with chances of rain and tag all other locations
    as `Others`. This will reduce the number of distinct levels in the variable as
    10+1 and will be more suitable for the model.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`location`变量有太多的不同值（44个位置），通常机器学习模型在具有高频率不同类别的分类变量上表现不佳。因此，我们需要通过减少变量中不同类别的数量来修剪变量。我们将选择降雨概率最高的五个和最低的五个位置，并将所有其他位置标记为`Others`。这将减少变量中不同级别的数量为10+1，这将更适合模型。'
- en: 'Perform the following steps to engineer a new variable for location with a
    reduced number of distinct levels:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以创建一个具有较少不同级别的位置新变量：
- en: 'Convert the `location` variable from a factor into a character:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`location`变量从因子转换为字符：
- en: '[PRE25]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Create a list with the top five and the bottom five locations with respect
    to the chances of rain. We can do this by using the `head` command for the top
    five and the `tail` command for the bottom five locations after ordering the DataFrame
    in ascending order:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含降雨概率最高和最低的五个位置的列表。我们可以通过在按升序排序的DataFrame中使用`head`命令获取前五个位置，以及使用`tail`命令获取后五个位置来实现这一点：
- en: '[PRE26]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Print the list to double-check that we have the locations correctly stored:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印列表以确认我们已经正确存储了位置：
- en: '[PRE27]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE28]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Convert the `Location` variable in the main `df_new` DataFrame into a `character`:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将主`df_new` DataFrame中的`Location`变量转换为`字符`：
- en: '[PRE29]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Reduce the number of distinct locations in the variable. This can be done by
    tagging all the locations that are not a part of the `location_list` list as `Others`:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少变量中不同位置的数量。这可以通过将所有不属于`location_list`列表的位置标记为`Others`来实现：
- en: '[PRE30]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Delete the old `Location` variable using the following command:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令删除旧的`Location`变量：
- en: '[PRE31]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To ensure that the fifth step was correctly performed, we can create a temporary
    DataFrame and summarize the frequency of records against the new `location` variable
    we created:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了确保第五步正确执行，我们可以创建一个临时DataFrame，并总结记录频率与我们所创建的新`location`变量之间的对比：
- en: '[PRE32]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Print the temporary test DataFrame and observe the results. We should see only
    11 distinct location values:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印临时测试DataFrame并观察结果。我们应该只看到11个不同的位置值：
- en: '[PRE33]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is as follows:'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE34]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We first convert the `Location` variable from a factor to a character to ease
    the string operation's tasks. The DataFrame is sorted in descending order according
    to the percentage chance of rain. The `head` and the `tail` commands are used
    to extract the top and bottom five locations in a list. This list is then used
    as a reference check to reduce the number of levels in the new feature. Finally,
    after engineering the new feature with the reduced levels, we do a simple check
    to ensure that our feature has been engineered in the way we expect.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将`Location`变量从因子转换为字符，以简化字符串操作任务。DataFrame根据降雨概率的百分比降序排序。`head`和`tail`命令用于提取列表中的前五个和后五个位置。然后，这个列表被用作参考检查，以减少新特征中的级别数量。最后，在工程化减少级别的新的特征后，我们进行简单的检查以确保我们的特征已经按照预期的方式工程化。
- en: Note
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/30fnR31.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到完整的代码：http://bit.ly/30fnR31。
- en: Let's now get into the most interesting topic of the chapter and explore classification
    techniques for supervised learning.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在进入本章最有趣的主题，并探讨监督学习的分类技术。
- en: Classification Techniques for Supervised Learning
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习的分类技术
- en: To approach a **supervised classification algorithm**, we first need to understand
    the basic functioning of the algorithm, explore a bit of the math in an abstract
    way, and then develop the algorithm using readily available packages in R. We
    will cover a few basic algorithms, such as white-box algorithms such as Logistic
    Regression and Decision Trees, and then we will move on to advanced modeling techniques,
    such as black-box models such as Random Forest, XGBoost, and neural networks.
    The list of algorithms we plan to cover is not exhaustive, but these five algorithms
    will help you gain a broad understanding of the topic.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 要接近**监督分类算法**，我们首先需要理解算法的基本功能，以抽象的方式探索一点数学，然后使用R中现成的包来开发算法。我们将介绍几个基本算法，例如透明算法如逻辑回归和决策树，然后我们将转向高级建模技术，例如黑盒模型如随机森林、XGBoost和神经网络。我们计划涵盖的算法列表并不全面，但这五个算法将帮助您对主题有一个广泛的理解。
- en: Logistic Regression
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: '**Logistic regression** is the most favorable white-box model used for binary
    classification. White-box models are defined as models where we have visibility
    of the entire reasoning used for the prediction. For each prediction made, we
    can leverage the model''s mathematical equation and decode the reasons for the
    prediction made. There are also a set of classification models that are completely
    black-box, that is, by no means can we understand the reasoning for the prediction
    leveraged by the model. In situations where we want to focus on only the end outcome,
    we should prefer black-box models, as they are more powerful.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑回归**是用于二元分类的最受欢迎的透明模型。透明模型定义为我们可以看到用于预测的整个推理过程的模型。对于每个做出的预测，我们可以利用模型的数学方程式来解码预测的原因。也存在一组完全黑盒的分类模型，也就是说，我们根本无法理解模型利用的预测推理。在我们只想关注最终结果的情况下，我们应该选择黑盒模型，因为它们更强大。'
- en: Though the name ends with *regression*, logistic regression is a technique used
    to predict binary categorical outcomes. We would need a different approach to
    model for a categorical outcome. This can be done by transforming the outcome
    into a log of odds ratio or the probability of the event happening.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管名称以*回归*结尾，但逻辑回归是一种用于预测二元分类结果的技巧。我们需要不同的方法来对分类结果进行建模。这可以通过将结果转换为优势比的对数或事件发生的概率来实现。
- en: Let's distill this approach into simpler constructs. Assume the probability
    of success for an event is 0.8\. Then, the probability of failure for the same
    event would be defined as *(1-0.8) = 0.2*. The odds of success are defined as
    the ratio of the probability of success over the probability of failure.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这种方法提炼成更简单的结构。假设一个事件成功的概率为0.8。那么，同一事件失败的概率将被定义为*(1-0.8) = 0.2*。成功的优势被定义为成功概率与失败概率的比率。
- en: 'In the following example, the odds of success would be *(0.8/0.2) = 4*. That
    is, the odds of success are four to one. If the probability of success is 0.5,
    that is, a 50-50 percent chance, then the odds of success are 0.5 to 1\. The logistic
    regression model can be mathematically represented as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，成功的优势将是*(0.8/0.2) = 4*。这意味着成功的优势是四比一。如果成功的概率是0.5，即50-50的机会，那么成功的优势是0.5比1。逻辑回归模型可以用以下方式数学表示：
- en: '![](img/C12624_05_10.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/C12624_05_10.jpg)'
- en: Where, ![](img/C12624_05_11.png) is the log of odds ratio.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![图片](img/C12624_05_11.png)是优势比的对数。
- en: 'Solving the math further, we can deduce the probability of the outcome as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步解决数学问题，我们可以推导出以下结果：
- en: '![](img/C12624_05_12.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/C12624_05_12.jpg)'
- en: Discussing the mathematical background and derivation of the equations is beyond
    the scope of the chapter. To summarize, the `logit` function, that is, the link
    function, helps logistic regression reframe the problem (predicted outcome) intuitively
    as the log of odds ratio. When solved, it helps us predict the probability of
    a binary dependent variable.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论方程的数学背景和推导超出了本章的范围。为了总结，`logit`函数，即连接函数，帮助逻辑回归直观地将问题（预测结果）重新表述为优势比的对数。当求解时，它帮助我们预测二元因变量的概率。
- en: How Does Logistic Regression Work?
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归是如何工作的？
- en: Just like linear regression, where the beta coefficients for the variables are
    estimated using the **Ordinary Least Squares** (**OLS**) method, a logistic regression
    model leverages the **maximum-likelihood estimation** (**MLE**). The MLE function
    estimates the best set of values of the model parameters or beta coefficients
    such that it maximizes the likelihood function, that is, the probability estimates,
    which can be also defined as the *agreement* of the selected model with the observed
    data. When the best set of parameter values are estimated, plugging these values
    or beta coefficients into the model equation as previously defined would help
    in estimating the probability of the outcome for a given sample. Akin to OLS,
    MLE is also an iterative process.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 就像线性回归中，变量的贝塔系数是通过**普通最小二乘法**（**OLS**）来估计的，逻辑回归模型利用**最大似然估计**（**MLE**）。MLE函数估计模型参数或贝塔系数的最佳值集，以最大化似然函数，即概率估计，这也可以定义为所选模型与观察数据的**一致性**。当最佳参数值集被估计出来后，将这些值或贝塔系数按先前定义的方式插入到模型方程中，将有助于估计给定样本的输出概率。类似于OLS，MLE也是一个迭代过程。
- en: Let's see a logistic regression model in action on our dataset. To get started,
    we will use only a small subset of variables for the model. Ideally, it is recommended
    to start with the most important variables based on the EDA exercise and then
    incrementally add remainder variables. For now, we will start with a temperature-related
    variable for the maximum and minimum values, a wind speed-related variable, pressure
    and humidity at 3 P.M., and the rainfall for the current day.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看逻辑回归模型在我们数据集上的实际应用。为了开始，我们将只使用模型的一小部分变量。理想情况下，建议根据EDA练习开始于最重要的变量，然后逐步添加剩余变量。现在，我们将从一个与最高和最低温度值相关的变量开始，一个与风速相关的变量，下午3点的气压和湿度，以及当天的降雨量。
- en: We will divide the entire dataset into train (70%) and test (30%). While fitting
    the data to the model, we will only use the train dataset and will later evaluate
    the performance of the model on the train, as well as the unseen test data. This
    approach will help us understand whether our model is overfitting and provide
    a more realistic model performance on unseen data.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将整个数据集分为训练集（70%）和测试集（30%）。在将数据拟合到模型时，我们只将使用训练集，稍后将在训练集以及未见过的测试数据上评估模型的表现。这种方法将帮助我们了解我们的模型是否过拟合，并在未见过的数据上提供更现实的模型性能。
- en: 'Exercise 69: Build a Logistic Regression Model'
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习69：构建逻辑回归模型
- en: We will build a binary classification model using logistic regression and the
    dataset we explored in the Exercises 1-6\. We will divide the data into train
    and test (70% and 30%, respectively) and leverage the training data to fit the
    model and the test data to evaluate the model's performance on unseen data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用逻辑回归和我们在练习1-6中探索的数据集构建一个二元分类模型。我们将数据分为训练集和测试集（分别为70%和30%），利用训练数据来拟合模型，并使用测试数据来评估模型在未见数据上的性能。
- en: 'Perform the following steps to complete the exercise:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来完成练习：
- en: 'First, set `seed` for reproducibility using the following command:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令设置 `seed` 以确保可重复性：
- en: '[PRE35]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, create a list of indexes for the training dataset (70%):'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，为训练数据集（70%）创建一个索引列表：
- en: '[PRE36]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, split the data into test and train datasets using the following commands:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令将数据分割为测试集和训练集：
- en: '[PRE37]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Build the logistic regression model with `RainTomorrow` as the dependent variable
    and a few independent variables (we selected `MinTemp`, `Rainfall`, `WindGustSpeed`,
    `WindSpeed3pm`, `Humidity3pm`. `Pressure3pm`, `RainToday`, `Temp3pm`, and `Temp9am`).
    We can add all the available independent variables in the DataFrame too:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `RainTomorrow` 作为因变量和几个自变量（我们选择了 `MinTemp`、`Rainfall`、`WindGustSpeed`、`WindSpeed3pm`、`Humidity3pm`、`Pressure3pm`、`RainToday`、`Temp3pm`
    和 `Temp9am`）构建逻辑回归模型。我们也可以在DataFrame中添加所有可用的自变量：
- en: '[PRE38]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Print the summary of the dataset using the `summary` function:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `summary` 函数打印数据集的摘要：
- en: '[PRE39]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is as follows:'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE40]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The `set.seed` command ensures that the random selections used for the train
    and test data split can be reproduced. We divide the data into 70% train and 30%
    test. The set seed function ensures that, for the same seed, we get the same split
    every time. The `glm` function is used in R to build generalized linear models.
    Logistic regression is defined in the model using the `family` parameter value
    set to `binomial(link ='logit')`. The `glm` function can be used to build several
    other models too (such as gamma, Poisson, and binomial). The formula defines the
    dependent, as well as the set of independent, variables. It takes the general
    form *Var1 ~ Var2 + Var3 + …*, which denotes `Var1` as the dependent or target
    variable and the remainder as the independent variables. If we want to use all
    of the variables in the DataFrame as independent variables, we can instead use
    `formula = Var1 ~ .`, which would indicate that `Var1` is the dependent variable
    and the rest are all independent variables.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`set.seed` 命令确保用于训练和测试数据集分割的随机选择可以重现。我们将数据分为70%的训练集和30%的测试集。设置种子函数确保，对于相同的种子，每次都能得到相同的分割。在R中，`glm`
    函数用于构建广义线性模型。在模型中使用 `family` 参数值设置为 `binomial(link =''logit'')` 定义了逻辑回归。`glm`
    函数还可以用于构建其他几种模型（如gamma、Poisson和binomial）。公式定义了因变量，以及一组自变量。它采用一般形式 *Var1 ~ Var2
    + Var3 + …*，表示 `Var1` 为因变量或目标变量，其余为自变量。如果我们想使用DataFrame中的所有变量作为自变量，我们可以使用 `formula
    = Var1 ~ .`，这表示 `Var1` 是因变量，其余都是自变量。'
- en: Note
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/2HwwUUX.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub上找到完整的代码：http://bit.ly/2HwwUUX。
- en: Interpreting the Results of Logistic Regression
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑回归结果解释
- en: We previously had a glimpse of logistic regression in *Chapter 2*, *Exploratory
    Analysis of Data*, but we didn't get into the specifics of the model results.
    The results demonstrated in the previous output snippet will look like what you
    observed in linear regression, but with some differences. Let's explore and interpret
    the results part by part.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *第二章*，*数据探索分析* 中曾经简要地了解过逻辑回归，但并未深入探讨模型结果的细节。前一个输出片段中展示的结果将类似于你在线性回归中观察到的结果，但也有一些不同之处。让我们逐部分探索和解释这些结果。
- en: Firstly, we have the `glm` function calculates two types of residuals, that
    is, **Null Deviance** and **Residual Deviance**. The difference between the two
    is that one reports the goodness of fit when only the intercept (that is, no dependent
    variables) is used and the other reports when all the provided independent variables
    are used. The reduction in deviance between null and residual deviance helps us
    understand the quantified value added by the independent variables in defining
    the variance or the predictive correctness. The distribution of deviance residuals
    is reported right after the formula.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有的`glm`函数计算两种类型的残差，即**零偏差**和**残差偏差**。两者的区别在于，一个报告了只使用截距（即没有因变量）时的拟合优度，而另一个报告了使用所有提供的自变量时的拟合优度。零偏差和残差偏差之间的偏差减少有助于我们理解独立变量在定义方差或预测正确性时增加的量化值。偏差残差的分布紧随公式之后报告。
- en: Next, we have the **beta coefficients** and the associated **standard error**,
    the *z-value* and the *p-value*, which is the probability of significance. For
    each variable provided, R internally calculates the coefficients and, along with
    the parameter value, it also reports additional test results to help us interpret
    how effective these coefficients are. The absolute value of the coefficient is
    a simple way to understand how important that variable is to the final predictive
    power, that is, how impactful the variable is in determining the end outcome of
    the prediction. We can see that all variables have a low value for the coefficient.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有**beta系数**和相关的**标准误差**，*z值*和*p值*，即显著性概率。对于每个提供的变量，R内部计算系数，并连同参数值一起报告额外的测试结果，帮助我们解释这些系数的有效性。系数的绝对值是理解该变量对最终预测能力重要性的简单方法，即该变量在确定预测结果最终结果中的影响程度。我们可以看到所有变量的系数值都很低。
- en: Next, the standard error helps us quantify how stable the value will be. A lower
    value for the standard error would indicate more consistent or stable values for
    the beta coefficients. The standard errors for all the variables in our exercise
    are low. The *z-value* and the probability of significance together help us take
    a call as to whether the results are statistically significant or just appear
    as they are due to random chance. This idea follows on from the same principle
    we learned about the null and alternate hypothesis in *Chapter 2*, *Exploratory
    Analysis of Data*, and is akin to linear regression parameter significance, which
    we learned about in *Chapter 4*, *Regression*.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，标准误差帮助我们量化值的稳定性。标准误差的值越低，表明beta系数的值越一致或稳定。在我们练习中的所有变量的标准误差都很低。*z值*和显著性概率共同帮助我们判断结果是否具有统计学意义，或者只是由于随机机会而看似如此。这个想法遵循我们在*第二章*，*数据探索分析*中学到的相同原理，类似于我们在*第四章*，*回归*中学到的线性回归参数显著性。
- en: The easiest way to interpret the significance would be to study the *asterix*
    besides each independent variable, that is, `*`. The number of `*` is defined
    by the actual probability value, as defined below the parameter values. In our
    exercise, notice that the `MinTemp` variable is not statistically significant,
    that is, *p-value > 0.05*. The rest are all statistically significant variables.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 解释显著性的最简单方法就是研究每个自变量旁边的*星号*，即`*`。星号的数量由实际概率值定义，如下所示。在我们的练习中，注意`MinTemp`变量不具有统计学意义，即*p值
    > 0.05*。其余的都是具有统计意义的变量。
- en: The **Akaike Information Criterion** (**AIC**) is again a metric reported by
    R to assess the goodness of fit of the model or the quality of the model. This
    number comes in handy to compare different models for the same use case. Say you
    fit several models using a combination of independent variables but the same dependent
    variable, the AIC can be used to study the best model by way of a simple comparison
    of the value in all models. The calculation of the metric is derived from the
    deviance between the model's prediction and the actual labels, but factors in
    the presence of variables that are not adding any value. Therefore, akin to **R
    Squared** and **adjusted R Squared** in linear regression, the AIC helps us to
    avoid building complicated models. To select the best model from a list of candidate
    models, we should select the model with the lowest AIC.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**赤池信息量准则**（**AIC**）是R报告的另一个指标，用于评估模型的拟合度或模型的质量。这个数字在比较同一用例的不同模型时非常有用。比如说，你使用一组独立变量但相同的因变量拟合了几个模型，AIC可以通过简单比较所有模型中的值来研究最佳模型。该指标的计算来源于模型预测与实际标签之间的偏差，但考虑了没有增加任何价值的变量。因此，类似于线性回归中的**R平方**和**调整R平方**，AIC帮助我们避免构建复杂的模型。要从候选模型列表中选择最佳模型，我们应该选择AIC最低的模型。'
- en: Toward the end of the previous output, we can see the results from **Fisher's
    Scoring** algorithm, which is a derivative of Newton's method for solving maximum
    likelihood problems numerically. We see that it required five iterations to fit
    the data to the model, but beyond that, this information is not of much value
    to us. It is a simple indication for us to conclude that the model did converge.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个输出的末尾，我们可以看到**费舍尔评分**算法的结果，这是一种牛顿法求解最大似然问题的衍生方法。我们看到它需要五次迭代来将数据拟合到模型中，但除此之外，这些信息对我们来说价值不大。这仅仅是我们得出模型已经收敛的简单指示。
- en: We now understand how logistic regression works and have interpreted the results
    reported by the model in R. However, we still need to evaluate the model results
    using our train and test dataset and ensure that the model performs well on unseen
    data. To study the performance of a classification model, we would need to leverage
    various metrics, such as accuracy, precision, and recall. Though we already explored
    them in *Chapter 4*, *Regression*, let's now study them in more detail.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经理解了逻辑回归的工作原理，并解释了模型在R中报告的结果。然而，我们仍然需要使用我们的训练集和测试集来评估模型结果，并确保模型在未见过的数据上表现良好。为了研究分类模型的表现，我们需要利用各种指标，例如准确率、精确率和召回率。尽管我们已经在*第4章*，*回归*中探讨了它们，但现在让我们更详细地研究它们。
- en: Evaluating Classification Models
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估分类模型
- en: Classification models require a bunch of different metrics to be thoroughly
    evaluated, unlike regression models. Here, we don't have something as intuitive
    as **R Squared**. Moreover, the performance requirements completely change based
    on a specific use case. Let's take a brief look at the various metrics that we
    already studied in *Chapter 3*, *Introduction to Supervised Learning*, for classification.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型需要一系列不同的指标来彻底评估，这与回归模型不同。在这里，我们没有像**R平方**这样直观的东西。此外，性能要求完全基于特定的用例。让我们简要地看看我们在*第3章*，*监督学习简介*中已经研究过的各种指标，用于分类。
- en: Confusion Matrix and Its Derived Metrics
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混淆矩阵及其衍生指标
- en: 'The first basis for studying model performance for classification algorithms
    starts with a **confusion matrix**. A confusion matrix is a simple representation
    of the distribution of predictions of each class across the actuals of each class:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 研究分类算法模型性能的第一个基础是从**混淆矩阵**开始。混淆矩阵是每个类别实际值中预测分布的简单表示：
- en: '![Figure 5.3: Confusion matrix'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.3：混淆矩阵'
- en: '](img/C12624_05_03.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12624_05_03.jpg)'
- en: 'Figure 5.3: Confusion matrix'
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.3：混淆矩阵
- en: The previous table is a simple representation of a confusion matrix. Here, we
    assume that the `1`) and `0`); when the result is correctly predicted, then we
    assign `1` correctly predicted as `1` and so on for the remaining outcomes.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的表格是混淆矩阵的简单表示。在这里，我们假设`1`和`0`；当结果被正确预测时，我们将其正确预测的`1`分配为`1`，依此类推，对于剩余的结果也是如此。
- en: 'Based on the confusion matrix and the values defined from it, we can further
    define a couple of metrics that will help us better understand the model''s performance.
    We will now use the abbreviations **TP** for **True Positive**, **FP** for **False
    Positive**, **TN** for **True Negative**, and **FN** for **False Negative** going
    forward:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 基于混淆矩阵及其定义的值，我们可以进一步定义一些指标，帮助我们更好地理解模型的表现。从现在开始，我们将使用缩写 **TP** 代表 **True Positive**（真正），**FP**
    代表 **False Positive**（假正），**TN** 代表 **True Negative**（真负），**FN** 代表 **False Negative**（假负）：
- en: '**Overall accuracy**: Overall accuracy is defined as the ratio of total correct
    predictions to the total number of predictions in the entire test sample. So,
    this would be simply the sum of **True Positives** and **True Negatives** divided
    by all the metrics in the confusion matrix:'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**整体准确率**：整体准确率定义为整个测试样本中正确预测总数与预测总数的比率。因此，这将是**真正**和**真负**的总和除以混淆矩阵中的所有指标：'
- en: '![](img/C12624_05_13.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/C12624_05_13.jpg)'
- en: '**Precision** or **Positive Predictive Value** (**PPV**): Precision is defined
    as the ratio of correctly predicted positive labels to the total number of positively
    predicted labels:'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度**或**阳性预测值**（**PPV**）：精确度定义为正确预测的正标签数与所有预测为正标签的总数的比率：'
- en: '![](img/C12624_05_14.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/C12624_05_14.jpg)'
- en: '**Recall** or **Sensitivity**: Recall measures how sensitive your model is
    by representing the ratio of the number of correctly predicted positive labels
    to the total number of actual positive labels:'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**或**灵敏度**：召回率通过表示正确预测的正标签数与实际正标签总数的比率来衡量你的模型灵敏度：'
- en: '![](img/C12624_05_15.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/C12624_05_15.jpg)'
- en: '**Specificity** or **True Negative Rate** (**TNR**): Specificity defines the
    ratio of correctly predicted negative labels to the total number of actual negative
    labels:'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特异性**或**真正负率**（**TNR**）：特异性定义为正确预测的负标签数与实际负标签总数的比率：'
- en: '![](img/C12624_05_16.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/C12624_05_16.jpg)'
- en: '**F1 Score**: The F1 score is the harmonic mean between precision and recall.
    It is a better metric to consider than overall accuracy for most cases:'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1 分数**：F1 分数是精确度和召回率的调和平均值。对于大多数情况来说，它是一个比整体准确率更好的指标：'
- en: '![](img/C12624_05_17.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/C12624_05_17.jpg)'
- en: What Metric Should You Choose?
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你应该选择哪个指标？
- en: Another important aspect to consider on a serious note, is which metric we should
    consider while evaluating a model. There is no straightforward answer, as the
    best combination of metrics completely depend on the type of classification use
    case we are dealing with. One situation that commonly arises in classification
    use cases is imbalanced classes. It is not necessary for us to always have an
    equal distribution of positive and negative labels in data. In fact, in most cases,
    we would be dealing with a scenario where the positive class would be less than
    *30%* of the data. In such cases, the overall accuracy would not be the ideal
    metric to consider.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在认真考虑的情况下，另一个重要的方面是我们在评估模型时应考虑哪个指标。没有直接的答案，因为最佳指标组合完全取决于我们处理的分类用例类型。在分类用例中，常见的一种情况是类别不平衡。我们并不总是需要在数据中保持正负标签的平等分布。事实上，在大多数情况下，我们会遇到正类别的数据少于
    *30%* 的情况。在这种情况下，整体准确率并不是一个理想的指标来考虑。
- en: Let's take a simple example to understand this better. Consider the example
    of predicting fraud in credit card transactions. In a realistic scenario, for
    every 100 transactions there may be just one or two fraud transactions. Now, if
    we use overall accuracy as the only metric to evaluate a model, even if we predict
    all the labels as **No**, that is, **Not Fraud**, we would have approximately
    *99%* accuracy, *0%* precision, and *0%* recall. The *99%* accuracy might seem
    a great number for model performance; however, in this case, it would not be the
    ideal metric to evaluate.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来更好地理解这一点。考虑预测信用卡交易中的欺诈的例子。在现实场景中，对于每100笔交易，可能只有一两次欺诈交易。现在，如果我们只用整体准确率作为评估模型的唯一指标，即使我们将所有标签预测为**否**，即**非欺诈**，我们会有大约
    *99%* 的准确率，*0%* 的精确度和 *0%* 的召回率。*99%* 的准确率可能看起来是一个很好的模型性能指标；然而，在这种情况下，它并不是一个理想的评估指标。
- en: To deal with such a situation, there is often additional business context required
    to make a tangible call, but in most cases (for this type of a scenario), the
    business would want a higher recall with a bit of compromise on the overall accuracy
    and precision. The rationale to use high recall as the metric for model evaluation
    is that it would still be fine to predict a transaction as fraud even if it is
    authentic; however, it would be a mistake to predict a fraud transaction as authentic;
    the business losses would be colossal.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这种情况，通常需要额外的业务背景来做出有形的决策，但在大多数情况下（对于此类场景），业务希望召回率更高，同时整体准确率和精度有所妥协。使用高召回率作为模型评估指标的合理性在于，即使交易是真实的，预测交易为欺诈仍然是可以接受的；然而，将欺诈交易预测为真实将是一个错误；业务损失将是巨大的。
- en: Often, the evaluation of a model is taken with a combination of metrics based
    on business demands. The biggest decision maker would be the trade-off between
    precision and recall. As indicated by the confusion matrix, whenever we try to
    improve precision, it hurts recall and vice versa.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，模型的评估是根据业务需求结合多种指标进行的。最大的决策者将是精度和召回率之间的权衡。如混淆矩阵所示，每当试图提高精度时，都会损害召回率，反之亦然。
- en: 'Here are some business situations in which we prioritize different metrics:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些我们优先考虑不同指标的业务场景：
- en: '**Predicting a rare event with catastrophic consequences**: When predicting
    whether a patient has cancer or not, whether a transaction is fraud, and so on,
    it is OK to predict a person without cancer as having cancer, but the other way
    around would result in the loss of life. Such scenarios demand high recall by
    compromising *precision* and *overall accuracy*.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测具有灾难性后果的罕见事件**：当预测患者是否有癌症或交易是否为欺诈等情况时，预测一个没有癌症的人患有癌症是可以接受的，但反过来预测会导致生命损失。这些场景需要通过妥协*精度*和*整体准确率*来保证高召回率。'
- en: '**Predicting a rare event with not such catastrophic consequences**: When predicting
    whether a customer will churn or whether a customer will positively respond to
    a marketing campaign, the business outcome is not jeopardized by an incorrect
    prediction, but would be the campaign. In such cases, based on the situation,
    it would make sense to have high precision with a bit of compromise on recall.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测具有非灾难性后果的罕见事件**：当预测客户是否会流失或客户是否会积极回应营销活动时，错误的预测不会危及业务结果，而是会危及活动本身。在这种情况下，根据具体情况，拥有高精度并稍微妥协召回率是有意义的。'
- en: '**Predicting a regular (non-rare) event with not such catastrophic consequences**:
    This would deal with most classification use cases, where the cost of correctly
    predicting a class is almost equal to the cost of incorrectly predicting the class.
    In such cases, we can use the F1 score, which represents a harmonic mean between
    precision and recall. It would be ideal to use overall accuracy in conjunction
    with the F1 score, as accuracy is more easily interpretable.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测具有非灾难性后果的常规（非罕见）事件**：这将处理大多数分类用例，其中正确预测一个类别的成本几乎等于错误预测该类别的成本。在这种情况下，我们可以使用F1分数，它代表精度和召回率之间的调和平均值。使用整体准确率与F1分数结合使用将是理想的，因为准确率更容易解释。'
- en: Evaluating Logistic Regression
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估逻辑回归
- en: Let's now evaluate the logistic regression model that we built previously.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来评估我们之前构建的逻辑回归模型。
- en: 'Exercise 70: Evaluate a Logistic Regression Model'
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习70：评估逻辑回归模型
- en: Machine learning models fitted on a training dataset cannot be evaluated using
    the same dataset. We would need to leverage a separate test dataset and compare
    the model's performance on a train as well as a test dataset. The `caret` package
    has some handy functions to compute the model evaluation metrics previously discussed.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据集上拟合的机器学习模型不能使用同一数据集进行评估。我们需要利用一个单独的测试数据集，并比较模型在训练集和测试集上的性能。`caret`包有一些方便的函数来计算之前讨论过的模型评估指标。
- en: 'Perform the following steps to evaluate the logistic regression model we built
    in *Exercise 7*, *Build a Logistic Regression Model*:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以评估我们在*练习7*，*构建逻辑回归模型*中构建的逻辑回归模型：
- en: 'Compute the distribution of records for the `RainTomorrow` target variable
    in the `df_new` DataFrame:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算DataFrame `df_new`中`RainTomorrow`目标变量的记录分布：
- en: '[PRE41]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is as follows:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE42]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Predict the `RainTomorrow` target variable on the train data using the `predict`
    function and cast observations with values (probability >0.5) as `Yes`, else `No`:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `predict` 函数在训练数据上预测 `RainTomorrow` 目标变量，并将概率值大于（>0.5）的观测值转换为 `Yes`，否则为 `No`：
- en: '[PRE43]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Create the confusion matrix and print the results for the train data:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为训练数据创建混淆矩阵并打印结果：
- en: '[PRE44]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is as follows:'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE45]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Predict the results on the test data, similar to the second step:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与第二步类似，在测试数据上预测结果：
- en: '[PRE46]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Create a confusion matrix for the test data predictions and print the results:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为测试数据预测创建混淆矩阵并打印结果：
- en: '[PRE47]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output is as follows:'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE48]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We first load the necessary `caret` library, which will provide the functions
    to compute the desired metrics, as discussed. We then use the `predict` function
    in R to predict the results using the previously fitted model on the train as
    well as the test data (separately). The `predict` function for logistic regression
    returns the value of the `link` function, by default. Using the `type= 'response'`
    parameter, we can override the function to return probabilities for the target.
    For simplicity, we use `0.5` as a threshold on the predictions. Therefore, anything
    above 0.5 would be `confusionMatrix` function from the `caret` library provides
    us with a simple way to construct the confusion matrix and calculate an exhaustive
    list of metrics. We would need to pass the actual, as well the predicted labels,
    to the function.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载必要的 `caret` 库，该库将提供计算所需指标的功能，如前所述。然后我们使用 R 中的 `predict` 函数，使用先前拟合的模型在训练数据以及测试数据（分别）上预测结果。`predict`
    函数对于逻辑回归默认返回 `link` 函数的值。使用 `type= 'response'` 参数，我们可以覆盖该函数以返回目标的概率。为了简单起见，我们在预测中使用
    `0.5` 作为阈值。因此，任何大于 0.5 的值都会被 `confusionMatrix` 函数从 `caret` 库提供给我们一个简单的方式来构建混淆矩阵并计算详尽的指标列表。我们需要将实际标签以及预测标签传递给该函数。
- en: Note
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/2Q6mYW0.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 GitHub 上找到完整的代码：http://bit.ly/2Q6mYW0。
- en: 'The distribution of the target label is imbalanced: *77%* no and *23%* yes.
    In such a scenario, we cannot rely only on the overall accuracy as a metric to
    evaluate the model''s performance. Also, the confusion matrix, as shown in the
    output for steps 3 and 5, is inverted when compared to the illustration shown
    in the previous section, *Confusion Matrix and Its Derived Metrics*. We have the
    predictions as rows and actual values as columns. However, the interpretation
    and results will remain the same. The next set of output reports the metrics of
    interest, along with a few others we have not explored. We have covered the most
    important ones (sensitivity and precision, that is, positive predictive value);
    however, it is recommended to explore the remaining metrics, such as negative
    predicted value, prevalence and detection rate. We can see that we are getting
    precision of around *73%* and *50%* recall and overall accuracy of *85%*. The
    results are similar on the train and test datasets; therefore, we can conclude
    that the model doesn''t overfit.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 目标标签的分布不平衡：*77%* 为无，*23%* 为有。在这种情况下，我们不能仅仅依靠整体准确率作为评估模型性能的指标。此外，与之前章节中所示的混淆矩阵相比，如图
    3 和 5 的输出所示，混淆矩阵是倒置的。我们有预测作为行，实际值作为列。然而，解释和结果将保持不变。下一组输出报告了感兴趣的指标，以及我们尚未探索的一些其他指标。我们已经涵盖了最重要的指标（敏感性、精确度，即阳性预测值）；然而，建议探索剩余的指标，例如阴性预测值、患病率和检测率。我们可以看到，我们得到的精确度约为
    *73%*，召回率约为 *50%，整体准确率为 *85%*。在训练和测试数据集上，结果相似；因此，我们可以得出结论，该模型没有过拟合。
- en: Note
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The results are not bad overall. Please don't be surprised to see the low recall
    rate; in scenarios where we have imbalanced datasets, the metrics that are used
    to assess model performance are business-driven.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，结果还不错。请不要对低召回率感到惊讶；在我们有不平衡数据集的场景中，用于评估模型性能的指标是业务驱动的。
- en: We can conclude that we would correctly predict at least half of the time whenever
    there is a possibility of rain, and whenever we predict, we are *73%* correct.
    From a business perspective, if we try to contemplate whether we should strive
    for high recall or precision, we would need to estimate the cost of misclassification.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得出结论，每次有下雨的可能性时，我们至少可以正确预测一半的时间，并且每次我们预测时，我们都是 *73%* 正确的。从业务角度来看，如果我们试图考虑是否应该努力提高召回率或精确度，我们需要估计误分类的成本。
- en: In our use case, whenever we predict that there is rainfall predicted for the
    next day, the operations management team would prepare the team with a higher
    number of agents to deliver faster. Since there isn't a pre-existing technique
    to combat rainfall-related problems, we have an opportunity to cover even if we
    recall only 50% of the times when there is rain. In this problem, since the cost
    of incorrectly predicting rain will be more expensive for the business, that is,
    if the chances of rainfall are predicted, the team would invest in pooling more
    agents for delivery, which comes at an additional cost. Therefore, we would want
    higher precision, while we are OK to compromise on recall.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的用例中，每当预测到第二天有降雨时，运营管理团队就会准备更多的代理人数以更快地交付。由于没有现成的技术来应对与降雨相关的问题，即使我们只有50%的召回率，我们也有机会覆盖。在这个问题中，由于错误预测降雨的成本对业务来说会更昂贵，也就是说，如果预测降雨的机会，团队将投资于更多的代理人数以交付，这会带来额外的成本。因此，我们希望有更高的精确度，同时我们可以接受在召回率上的妥协。
- en: Note
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The ideal scenario is to have high precision and high recall. However, there
    is always a trade-off in achieving one over the other. In most real-life machine
    learning use cases, a business-driven decision finalizes the priority to choose
    either precision or recall.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的情况是具有较高的精确度和召回率。然而，在两者之间总是存在权衡。在大多数现实生活中的机器学习用例中，由业务驱动的决策最终确定优先选择精确度或召回率。
- en: The previous model developed in *Exercise 8*, *Evaluate a Logistic Regression
    Model*, was developed only using a few variables that were available in the `df_new`
    dataset. Let's build an improved model with all the available variables in the
    dataset and check the performance on the test dataset.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在*练习 8*，*评估逻辑回归模型*中开发的先前模型仅使用了`df_new`数据集中可用的一些变量。让我们构建一个包含数据集中所有可用变量的改进模型，并检查在测试数据集上的性能。
- en: The best way to iterate for model improvements would be with feature selection
    and hyperparameter tuning. Feature selection involves selecting the best set of
    features from the available list through various validation approaches and finalizing
    a model with the best performance and the least number of features. Hyperparameter
    tuning deals with building generalized models that will not overfit, that is,
    a model that performs well on training as well as unseen test data. These topics
    will be covered in detail in *Chapter 6*, *Feature Selection and Dimensionality
    Reduction*, and *Chapter 7*, *Model Improvements*. For now, the scope of the chapter
    will be restricted to demonstrate model evaluation only. We will touch on the
    same use case for hyperparameter tuning and feature selection in upcoming chapters.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 为了迭代改进模型，最佳方式是进行特征选择和超参数调整。特征选择涉及通过各种验证方法从可用列表中选择最佳特征集，并最终确定一个具有最佳性能和最少特征数量的模型。超参数调整涉及构建泛化模型，这些模型不会过拟合，即模型在训练和未见过的测试数据上都能表现良好。这些主题将在*第6章*，*特征选择和降维*和*第7章*，*模型改进*中详细讨论。现在，本章的范围将仅限于演示模型评估。我们将在后续章节中涉及相同的用例进行超参数调整和特征选择。
- en: 'Exercise 71: Develop a Logistic Regression Model with All of the Independent
    Variables Available in Our Use Case'
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 71：使用我们用例中所有可用独立变量开发逻辑回归模型
- en: In the previous exercise, we limited the number of independent variables to
    only a few. In this example, we will use all the available independent variables
    in our `df_new` dataset and create an improved model. We will again use the train
    dataset to fit the model and test to evaluate the model's performance.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的练习中，我们限制了独立变量的数量，只限于几个。在这个例子中，我们将使用`df_new`数据集中所有可用的独立变量来创建一个改进的模型。我们再次使用训练数据集来拟合模型，并使用测试数据集来评估模型性能。
- en: 'Perform the following steps to build a logistic regression model with all of
    the independent variables available within the use case:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以构建一个包含用例中所有可用独立变量的逻辑回归模型：
- en: 'Fit the logistic regression model with all the available independent variables:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所有可用独立变量拟合逻辑回归模型：
- en: '[PRE49]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Predict on the train dataset:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据集上预测：
- en: '[PRE50]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Create the confusion matrix:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建混淆矩阵：
- en: '[PRE51]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output is as follows:'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE52]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Predict the results on the test data:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上预测结果：
- en: '[PRE53]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Create the confusion matrix:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建混淆矩阵：
- en: '[PRE54]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output is as follows:'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE55]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We leverage all the variables within the dataset to create a logistic regression
    model using the `glm` function. We then use the **fitted** model to predict the
    outcomes for the train and the test datasets; akin to the previous exercise.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用数据集中的所有变量，使用`glm`函数创建逻辑回归模型。然后我们使用**拟合**的模型来预测训练集和测试集的结果；类似于之前的练习。
- en: Note
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/2HgwjaU.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub上找到完整的代码：http://bit.ly/2HgwjaU。
- en: Notice how the overall accuracy, precision, and recall has improved a bit (though
    marginally). The results are fair and we can iterate with logistic regression
    to improving them further. For now, let's explore a few other classification techniques
    and study the performance of the model.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 注意整体准确率、精确率和召回率有所提高（尽管幅度很小）。结果尚可，我们可以通过逻辑回归进一步迭代改进它们。现在，让我们探索一些其他分类技术并研究模型的性能。
- en: Note
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In this exercise, we have not printed the model's summary statistics, akin to
    the first model, with a few variables. If printed, the results would consume less
    than two pages of the chapter. For now, we will ignore that since we are not exploring
    the model characteristics that are reported by R; instead, we are evaluating a
    model purely from the accuracy, precision, and recall metrics on the train and
    test dataset.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们没有打印模型的摘要统计信息，类似于第一个模型，只有几个变量。如果打印出来，结果将占用不到两页的章节内容。目前，我们将忽略这一点，因为我们不是在探索R报告的模型特征；相反，我们是从训练集和测试集上的准确率、精确率和召回率指标来评估模型的。
- en: The ideal way to get the best model would be to eliminate all statistically
    insignificant variables, remove multicollinearity, and treat the data for outliers,
    and so on. All these steps have been ignored for now, given the scope of the chapter.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 获取最佳模型的最理想方式是消除所有统计上不显著的变量，消除多重共线性，处理异常值数据等。鉴于本章的范围，现在我们将忽略所有这些步骤。
- en: 'Activity 8: Building a Logistic Regression Model with Additional Features'
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动8：构建具有额外特征的逻辑回归模型
- en: We built a simple model with few features in *Exercise 8*, *Evaluate a Logistic
    Regression Model*, and then with all the features in *Exercise 9*, *Develop a
    Logistic Regression Model with All of the Independent Variables Available in Our
    Use Case*. In this activity, we will build a logistic regression model with additional
    features that we can generate using simple mathematical transformations. It is
    good practice to add additional transformations of numeric features with log transformations,
    square and cube power transformations, square root transformations, and so on.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在*练习8*，*评估逻辑回归模型*中，我们构建了一个具有少量特征的简单模型，然后在*练习9*，*使用我们用例中所有可用独立变量的逻辑回归模型开发*中，我们使用了所有特征。在这个活动中，我们将构建一个逻辑回归模型，该模型具有我们可以使用简单数学变换生成的额外特征。添加额外的数值特征变换，如对数变换、平方和立方幂变换、平方根变换等，是一种良好的实践。
- en: 'Perform the following steps to develop a logistic regression model with additional
    features engineered:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以开发具有额外特征的逻辑回归模型：
- en: Create a copy of the `df_new` dataset in `df_copy` for the activity and select
    any three numeric features (for example, `MaxTemp`, `Rainfall` and `Humidity3pm`).
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为活动创建`df_new`数据集的副本到`df_copy`中，并选择任何三个数值特征（例如，`MaxTemp`，`Rainfall`和`Humidity3pm`）。
- en: Engineer new features with square and cube power and square root transformations
    for each of the selected features.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个选定的特征使用平方和立方幂以及平方根变换来生成新特征。
- en: Divide the `df_copy` dataset into train and test in a 70:30 ratio.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`df_copy`数据集分成70:30的训练集和测试集。
- en: Fit the model with the new train data, evaluate it on test data, and finally,
    compare the results.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新的训练数据拟合模型，在测试数据上评估它，最后比较结果。
- en: 'The output is as follows:'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE56]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Note
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can find the solution for this activity on page 451.
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在第451页找到这个活动的解决方案。
- en: Decision Trees
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树
- en: 'Like logistic regression, there is another popular classification technique
    that is very popular due to its simplicity and white-box nature. A decision tree
    is a simple flowchart that is represented in the form of a tree (an inverted tree).
    It starts with a root node and branches into several nodes, which can be traversed
    based on a decision, and ends with a leaf node where the *final outcome* is determined.
    Decision trees can be used for regression, as well as classification use cases.
    There are several variations of decision trees implemented in machine learning.
    A few popular choices are listed here:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 与逻辑回归一样，还有一种流行的分类技术因其简单性和白盒特性而非常受欢迎。决策树是一个以树（倒置树）的形式表示的简单流程图。它从一个根节点开始，分支到几个节点，可以根据决策进行遍历，并以叶节点结束，其中确定
    *最终结果*。决策树可以用于回归，也可以用于分类用例。机器学习中实现了多种决策树的变体。这里列出了几个流行的选择：
- en: '**Iterative Dichotomiser 3** (**ID3**)'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代二分器 3**（**ID3**）'
- en: '**Successor to ID3** (**C4.5**)'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ID3 的继承者**（**C4.5**）'
- en: '**Classification and Regression Tree** (**CART**)'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类和回归树**（**CART**）'
- en: '**CHi-squared Automatic Interaction Detector** (**CHAID**)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CHi-squared 自动交互检测器**（**CHAID**）'
- en: '**Conditional Inference Trees** (**C Trees**)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**条件推理树**（**C 树**）'
- en: The preceding list is not exhaustive. There are other alternatives, and each
    of them has small variations in how they approach the tree creation process. In
    this chapter, we will limit our exploration to **CART Decision Trees**, which
    are the most widely used. R provides a few packages that house the implementation
    of the CART algorithm. Before we delve into the implementation, let's explore
    a few important aspects of decision trees in the following sections.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的列表并不详尽。还有其他替代方案，它们在如何处理分类和数值变量、选择树中根节点和连续节点的方法、分支每个决策节点的规则等方面都有细微的差别。在本章中，我们将限制我们的探索范围到
    **CART 决策树**，这是最广泛使用的。R 提供了一些包含 CART 算法实现的包。在我们深入研究实现之前，让我们在以下几节中探讨决策树的一些重要方面。
- en: How Do Decision Trees Work?
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树是如何工作的？
- en: 'Each variation of decision trees has a slightly different approach. Overall,
    if we try to simplify the pseudocode for a generic decision tree, it can be summarized
    as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的每个变体都有略微不同的方法。总的来说，如果我们尝试简化通用决策树的伪代码，可以总结如下：
- en: Select the root node (the node corresponds to a variable).
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择根节点（节点对应一个变量）。
- en: Partition the data into groups.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据划分为组。
- en: 'For each group from the previous step:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于上一步中的每个组：
- en: Create a decision node or leaf node (based on the splitting criteria).
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建一个决策节点或叶节点（基于分割标准）。
- en: Repeat until node size <= threshold or features = empty.
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重复执行，直到节点大小 <= 阈值或特征 = 空集。
- en: Variations between different forms of tree implementations include the way categorical
    and numerical variables are handled, the approach used to select the root node
    and consecutive nodes in the tree, the rules to branch each decision node, and
    so on.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 不同形式的树实现之间的差异包括处理分类和数值变量的方式、选择树中根节点和连续节点的方法、分支每个决策节点的规则等。
- en: 'The following visual is a sample decision tree. The root node and the decision
    nodes are the independent variables we provide to the algorithm. The leaf nodes
    denote the final outcome, whereas the root node and the intermediate decision
    nodes help in traversing the data to the leaf node. The simplicity of a decision
    tree is what makes it so effective and easy to interpret. This helps in easily
    identifying rules for a prediction task. Often, many research and business initiatives
    leverage decision trees to design a set of rules for a simple classification system:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的视觉图是一个示例决策树。根节点和决策节点是我们提供给算法的独立变量。叶节点表示最终结果，而根节点和中间决策节点有助于遍历数据到叶节点。决策树的简单性使其非常有效且易于解释。这有助于轻松识别预测任务中的规则。通常，许多研究和商业倡议利用决策树来设计一套简单的分类系统规则：
- en: '![Figure 5.4: Sample decision tree'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.4：示例决策树'
- en: '](img/C12624_05_04.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12624_05_04.jpg)'
- en: 'Figure 5.4: Sample decision tree'
  id: totrans-317
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.4：示例决策树
- en: In a general sense, given a combination of dependent and several independent
    variables, the decision tree algorithm calculates a metric that represents the
    goodness of fit between the dependent target variable and all independent variables.
    For classification use cases, entropy and information gain are commonly used metrics
    in CART decision trees. The variable with the best fit for the metric is chosen
    as the root node and the next best is used as the decision nodes in the descending
    order of fit. The nodes are terminated into leaf nodes based on a defined threshold.
    The tree keeps growing till it exhausts the number of variables for decision nodes
    or when a predefined threshold for the number of nodes is reached.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般意义上，给定一组依赖变量和多个独立变量，决策树算法计算一个指标，该指标表示依赖目标变量与所有独立变量之间的拟合优度。对于分类用例，熵和信息增益是
    CART 决策树中常用的指标。对于该指标的最好拟合变量被选为根节点，下一个最佳拟合变量被用作决策节点，按拟合优度降序排列。节点根据定义的阈值终止为叶节点。树继续增长，直到耗尽决策节点的变量数量或达到预定义的节点数量阈值。
- en: To improve tree performance and reduce overfitting, a few strategies, such as
    restricting the depth or breadth of the tree or additional rules for leaf nodes
    or decision nodes help in generalizing a tree for prediction.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高树性能并减少过拟合，一些策略，如限制树的深度或宽度，或为叶节点或决策节点添加额外的规则，有助于对预测进行泛化。
- en: Let's implement the same use case using CART decision trees in R. The CART model
    is available through the `rpart` package in R. This algorithm was developed by
    Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone in 1984 and has
    been widely adopted in the industry.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 R 中的 CART 决策树实现相同的用例。CART 模型通过 R 中的`rpart`包提供。该算法由 Leo Breiman、Jerome
    Friedman、Richard Olshen 和 Charles Stone 在 1984 年开发，并在业界得到广泛应用。
- en: 'Exercise 72: Create a Decision Tree Model in R'
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 72：在 R 中创建决策树模型
- en: In this exercise, we will create a decision tree model in R using the same data
    and the use case we leveraged in *Exercise 9*, *Develop a Logistic Regression
    Model with All of the Independent Variables Available in Our Use Case*. We will
    try to study whether there are any differences in the performance of a decision
    tree model over a logistic regression model.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用与我们在 *练习 9* 中使用的相同数据和用例，在 R 中创建决策树模型，该用例是 *使用我们用例中所有可用独立变量开发逻辑回归模型*。我们将尝试研究决策树模型与逻辑回归模型在性能上是否存在任何差异。
- en: 'Perform the following steps to create a decision tree model in R:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R 中创建决策树模型的以下步骤：
- en: 'Import the `rpart` and `rpart.plot` packages using the following command:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令导入`rpart`和`rpart.plot`包：
- en: '[PRE57]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Build the CART model with all of the variables:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所有变量构建 CART 模型：
- en: '[PRE58]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Plot the cost parameter:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制成本参数：
- en: '[PRE59]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output is as follows:'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.5: Decision tree model'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.5：决策树模型'
- en: '](img/C12624_05_05.jpg)'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12624_05_05.jpg)'
- en: 'Figure 5.5: Decision tree model'
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.5：决策树模型
- en: 'Plot the tree using the following command:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令绘制树：
- en: '[PRE60]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The output is as follows:'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.6: Predicting rainfall'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.6：预测降雨'
- en: '](img/C12624_05_06.jpg)'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12624_05_06.jpg)'
- en: 'Figure 5.6: Predicting rainfall'
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.6：预测降雨
- en: 'Make predictions on the train data:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据上做出预测：
- en: '[PRE61]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output is as follows:'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE62]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Make predictions on the test data:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上做出预测：
- en: '[PRE63]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The output is as follows:'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE64]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The `rpart` library provides us with the CART implementation of decision trees.
    There are additional libraries that help us visualize the decision tree in R.
    We have used `rpart.plot` here. If the package is not already installed, please
    install it using the `install.packages` command. We use the `rpart` function to
    create the tree model and we use all the available independent variables. We then
    use the `plotcp` function to visualize the complexity parameter's corresponding
    validation error on different iterations. We also use the `plot.rpart` function
    to plot the decision tree.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '`rpart`库为我们提供了决策树的 CART 实现。还有其他库帮助我们可视化 R 中的决策树。我们在这里使用了`rpart.plot`。如果包尚未安装，请使用`install.packages`命令安装。我们使用`rpart`函数创建树模型，并使用所有可用的独立变量。然后我们使用`plotcp`函数可视化复杂度参数对应的不同迭代中的验证误差。我们还使用`plot.rpart`函数绘制决策树。'
- en: Finally, we make predictions on the train as well as the test data and build
    the confusion matrix and calculate the metrics of interest using the `confusionMatrix`
    function for the train and test datasets individually.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在训练集和测试集上做出预测，并使用`confusionMatrix`函数分别对训练集和测试集计算感兴趣的指标，构建混淆矩阵。
- en: Note
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/2WECLgZ.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到完整的代码：http://bit.ly/2WECLgZ。
- en: The CART decision tree implemented in R has several optimizations already in
    place. The function, by default, sets a ton of parameters for optimum results.
    In a decision tree, there are several parameters that we can manually set to tune
    the performance based on our requirements. However, the R implementation does
    a great job of setting a wide number of parameters with a relatively good value
    by default. These additional settings can be added to the `rpart` tree with the
    `control` parameter.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: R中实现的CART决策树已经内置了一些优化。默认情况下，该函数设置了许多参数以获得最佳结果。在决策树中，有几个参数我们可以手动设置，以根据我们的需求调整性能。然而，R实现通过默认值设置大量参数，并取得了相对较好的效果。这些额外的设置可以通过`control`参数添加到`rpart`树中。
- en: 'We can add the following parameter to the tree model:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将以下参数添加到树模型中：
- en: '[PRE65]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: One parameter of interest would be the `0.01`. We can further change this to
    a lower number that would make the tree grow deeper and become more complicated.
    The `plotcp` function visualizes the relative validation error for different values
    of `cp`, that is, the complexity parameter. The most ideal value for `cp` is the
    leftmost value below the dotted line in the plot in *Figure 5.4*. In this case
    (as shown in the plot), the best value is 0.017\. Since this value is not very
    different from the default value, we don't change it further.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的参数是`0.01`。我们可以将其进一步更改为一个更小的数字，这将使树生长得更深，变得更加复杂。`plotcp`函数可视化了不同`cp`值（即复杂度参数）的相对验证误差。在*图5.4*中，最理想的`cp`值是位于虚线以下的最左侧值。在这种情况下（如图所示），最佳值是0.017。由于这个值与默认值相差不大，我们没有进一步更改它。
- en: The next plot in *Figure 5.5* helps us visualize the actual decision tree constructed
    by the algorithm. We can see the simple set of rules being constructed using the
    available data. As you can see, only two independent variables, that is, `Humidity3pm`
    and `WindGustSpeed`, have been selected for the tree. If we change the complexity
    parameter to *0.001* instead of *0.01*, we can see a much deeper tree (which could
    overfit the model) would have been constructed. Finally, we can see the results
    from the confusion matrix (step 6) along with additional metrics of interest for
    the train and test dataset.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图表（*图5.5*）帮助我们可视化算法构建的实际决策树。我们可以看到正在使用可用数据构建的简单规则集。正如您所看到的，只有两个独立变量，即`Humidity3pm`和`WindGustSpeed`，被选入树中。如果我们将复杂度参数从*0.01*改为*0.001*，我们可以看到一个更深层次的树（这可能导致模型过拟合）将被构建。最后，我们可以看到混淆矩阵（步骤6）的结果，以及训练集和测试集的感兴趣的其他指标。
- en: We can see that the results are similar for the train and test dataset. We can
    therefore conclude that the model doesn't overfit. However, there is a significant
    drop in accuracy (*83%*) and recall (*35%*), while the precision has increased
    to a slightly higher value (*77%*).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，训练集和测试集的结果相似。因此，我们可以得出结论，该模型没有过拟合。然而，准确率（*83%*）和召回率（*35%*）有显著下降，而精确率则增加到略高的值（*77%*）。
- en: We have now worked with a few white-box modeling techniques. Given the simplicity
    and ease of interpretation of white-box models, they are the most preferred technique
    for classification use cases in business, where reasoning and driver analysis
    is of paramount importance. However, there are a few scenarios where a business
    might be more interested in the *net outcome* of the model rather than the entire
    interpretation of the outcome. In such cases, the end model performance is of
    more interest. In our use case, we want to achieve high precision. Let's explore
    a few black-box models that are superior (in most cases) to white-box models in
    terms of model performance and that can be achieved with far less effort and more
    training data.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经与几种白盒建模技术合作过。鉴于白盒模型的简单性和易于解释，它们是商业中分类用例中最受欢迎的技术，在这些用例中，推理和驱动分析至关重要。然而，在某些情况下，企业可能对模型的*净结果*更感兴趣，而不是对结果的整个解释。在这种情况下，最终模型的表现更为重要。在我们的用例中，我们希望实现高精度。让我们探索一些在模型性能上优于（在大多数情况下）白盒模型，并且可以用更少的努力和更多的训练数据实现的黑盒模型。
- en: 'Activity 9: Create a Decision Tree Model with Additional Control Parameters'
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 9：创建具有额外控制参数的决策树模型
- en: The decision tree model we created in *Exercise 10*, *Create a Decision Tree
    Model in R*, used the default control parameters for the tree. In this activity,
    we will override a few control parameters and study its impact on the overall
    tree-fitting process.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*练习 10*，*在 R 中创建决策树模型*中创建的决策树模型使用了树的默认控制参数。在这个活动中，我们将覆盖一些控制参数，并研究其对整体树拟合过程的影响。
- en: 'Perform the following steps to create a decision tree model with additional
    control parameters:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以创建具有额外控制参数的决策树模型：
- en: Load the `rpart` library.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`rpart`库。
- en: 'Create the control object for the decision tree with new values: `minsplit
    =15` and `cp = 0.00`.'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为决策树创建控制对象，并使用新值：`minsplit =15`和`cp = 0.00`。
- en: Fit the tree model with the train data and pass the control object to the `rpart`
    function.
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练数据拟合树模型，并将控制对象传递给`rpart`函数。
- en: Plot the complexity parameter plot to see how the tree performs at different
    values of `CP`.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制复杂性参数图，以查看树在不同`CP`值下的表现。
- en: Use the fitted model to make predictions on the train data and create the confusion
    matrix.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用拟合的模型对训练数据进行预测，并创建混淆矩阵。
- en: Use the fitted model to make predictions on the test data and create the confusion
    matrix.
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用拟合的模型对测试数据进行预测，并创建混淆矩阵。
- en: 'The output is as follows:'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE66]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Note
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can find the solution for this activity on page 454.
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在第 454 页找到这个活动的解决方案。
- en: Ensemble Modelling
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成建模
- en: 'Ensemble modeling is one of the most popular approaches used in classification
    and regression modeling techniques when there is a need for improved performance
    with a larger training sample. In simple words, ensemble modeling can be defined
    by breaking down the name into individual terms: **ensemble** and **modeling**.
    We have already studied modeling in this book; an ensemble in simple terms is
    a **group**. Therefore, the process of building several models for the same task
    instead of just one model and then combining the results into a single outcome
    through any means, such as averaging or voting, and many others, is called **ensemble
    modeling**.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要在大规模训练样本中提高性能时，集成建模是分类和回归建模技术中最常用的方法之一。简单来说，集成建模可以通过分解其名称为个别术语来定义：**集成**和**建模**。我们已经在本书中研究了建模；简单来说，集成就是一个**组**。因此，为同一任务构建多个模型而不是一个模型，然后将结果通过任何方式（如平均或投票）结合成一个单一结果的过程，称为**集成建模**。
- en: 'We can build ensembles of any models, such as linear models or tree models,
    and in fact can even build an ensemble of ensemble models. However, the most popular
    approach is using tree models as the base for ensembles. There are two broad types
    of ensemble models:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以构建任何模型的集成，例如线性模型或树模型，实际上甚至可以构建集成模型的集成。然而，最流行的方法是使用树模型作为集成的基座。集成模型主要有两种类型：
- en: '**Bagging**: Here, each model is built in parallel with some randomization
    introduced within each model, and the results of all models are combined using
    a simple voting mechanism. Say we built 100 tree models and 60 models predicted
    the outcome as *Yes* and 40 predicted it as *No*. The end result would be a *Yes*.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bagging**: 在这里，每个模型都是并行构建的，并在每个模型内部引入了一些随机化，所有模型的结果通过简单的投票机制进行组合。比如说我们构建了100个树模型，其中60个模型预测结果为*是*，40个模型预测为*否*。最终结果将是*是*。'
- en: '**Boosting**: Here, models are built sequentially and the results of the first
    model are used to tune the next model. Each model iteratively learns from errors
    made by the previous model and tries to improve with successive iterations. The
    result is usually a weighted average of all the individual outcomes.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Boosting**: 在这里，模型是按顺序构建的，第一个模型的结果被用来调整下一个模型。每个模型迭代地从先前模型犯的错误中学习，并试图在连续迭代中改进。结果是所有单个结果的加权平均值。'
- en: There are several implementations available in bagging as well as boosting.
    **Bagging** itself is an ensemble model available in R. By far the most popular
    bagging technique used is random forest. Another bagging technique along similar
    lines as random forest is **extra trees**. Similarly, a few examples of boosting
    techniques are AdaBoost, Stochastic Gradient Boosting, BrownBoost, and many others.
    However, the most popular boosting technique is **XGBoost**, which is derived
    from the name **EXtreme Gradient Boosting**. In most cases, for classification
    as well as regression use cases, data scientists prefer using random forests or
    XGBoost models. A recent survey on Kaggle (an online data science community) revealed
    the most popular technique used for most machine learning competitions were always
    random forest and XGBoost. In this chapter, we will take a closer look at both
    models.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在Bagging和Boosting中都有几种实现方式。**Bagging**本身是R中可用的一种集成模型。迄今为止，最流行的Bagging技术是随机森林。与随机森林类似，另一种Bagging技术是**额外树**。同样，一些Boosting技术的例子包括AdaBoost、随机梯度提升、BrownBoost等。然而，最流行的Boosting技术是**XGBoost**，它源自名称**EXtreme
    Gradient Boosting**。在大多数情况下，对于分类以及回归用例，数据科学家更喜欢使用随机森林或XGBoost模型。Kaggle（一个在线数据科学社区）的一项最近调查显示，在大多数机器学习竞赛中最常用的技术总是随机森林和XGBoost。在本章中，我们将更深入地研究这两种模型。
- en: Random Forest
  id: totrans-378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林
- en: '**Random forest** is the most popular bagging technique used in machine learning.
    It was developed by Leo Brieman, the author of CART. This simple technique is
    so effective that it is almost always the first choice of algorithm for a data
    scientist given a supervised use case. Random forest is a good choice for classification
    as well as regression use cases. It is a highly effective method for reducing
    overfitting with a bare minimum amount of effort. Let''s have a deeper understanding
    of how random forests work.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**是机器学习中使用的最流行的Bagging技术。它是由CART的作者Leo Brieman开发的。这种简单技术非常有效，以至于在给定的监督用例中，数据科学家几乎总是首先选择算法。随机森林是分类和回归用例的良好选择。它是一种高度有效的方法，可以以最小的努力减少过拟合。让我们更深入地了解随机森林是如何工作的。'
- en: As we already know, random forest is an ensemble modeling technique, where we
    build several models and combine their results using a simple voting technique.
    In random forests, we use decision trees as the base model. The inner workings
    of the algorithm can be fairly guessed from the name itself, that is, random (since
    it induces a layer of randomization in every model that is built) and forest (since
    there are several *tree* models we build). Before we get into the actual workings
    of the algorithm, we first need to understand the story of its predecessor, **bagging**,
    and study why we need ensembles.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，随机森林是一种集成建模技术，其中我们构建了多个模型，并使用简单的投票技术结合它们的结果。在随机森林中，我们使用决策树作为基础模型。算法的内部工作原理可以从其名称本身推测出来，即随机（因为它在构建的每个模型中引入了一层随机化）和森林（因为我们要构建多个*树*模型）。在我们深入了解算法的实际工作原理之前，我们首先需要了解其前身**Bagging**的故事，并研究为什么我们需要集成。
- en: Why Are Ensemble Models Used?
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么使用集成模型？
- en: The first question that would have surfaced in your thoughts may be, why do
    we need to build several models for the same task in the first place? Is it necessary?
    Well, yes! When we build ensembles, we don't build the exact same model several
    times; instead, every model we build will be different from the others in some
    way. The intuition behind this can be understood using a simple example from our
    day-to-day lives. It is built on the principle that several weak learners combined
    together build a stronger and more robust model.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的脑海中浮现的第一个问题可能就是，为什么我们一开始就需要为同一个任务构建多个模型？这是必要的吗？嗯，是的！当我们构建集成模型时，我们并不是多次构建完全相同的模型；相反，我们构建的每个模型都会以某种方式与其他模型不同。这个背后的直觉可以通过我们日常生活中的一个简单例子来理解。它是基于这样一个原则：将几个弱学习器结合起来可以构建一个更强、更健壮的模型。
- en: Let's understand this idea using a simple example. Say you reach a new city
    and want to know the chances of there being rain in the city the next day. Assuming
    technology is not an available option, the easiest way you could find this out
    would be to ask someone in the neighborhood who has been a dweller of the place
    for a while. Maybe the answer would not always be correct; if someone said that
    there was a very high chance of rain the next day, it doesn't necessarily mean
    that it would certainly rain. Therefore, to make an improved guess, you ask several
    people in the neighborhood. Now, if 7 out of the 10 people you asked mentioned
    that there was a high chance of rain the next day, then it almost certainly would
    rain the very next day. The reason this works effectively is because every person
    you reached out to would have some understanding about rain patterns and also
    every person's understanding about those patterns would be a bit different. Though
    the differences are not miles apart, some level of randomness among the people's
    understanding when aggregated for a collective answer would yield a better answer.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个简单的例子来理解这个想法。假设你到达一个新城市，想知道第二天这个城市下雨的概率。假设技术不是一个可用的选项，你能找到的最简单的方法就是询问在这个地方居住了一段时间的邻居。也许答案并不总是正确的；如果有人说第二天有很大的下雨概率，这并不一定意味着一定会下雨。因此，为了做出改进的猜测，你会询问几个邻居。现在，如果你询问的10个人中有7个人提到第二天有很高的下雨概率，那么第二天几乎肯定会下雨。这个方法之所以有效，是因为你接触到的每个人都会对降雨模式有一定的了解，而且每个人的了解也会有所不同。尽管这些差异并不大，但当这些了解汇总成一个集体答案时，就会产生更好的答案。
- en: Bagging – Predecessor to Random Forest
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bagging – 随机森林的前身
- en: Ensemble modeling works on the same principle. Here, in each model, we induce
    some level of randomness. The bagging algorithm brings in this randomness for
    each model on the training data. The name bagging is derived from **Bootstrap
    Aggregation**; a process where we sample two-thirds of the available data with
    replacement data for training and the remainder for testing and validation. Here,
    each model, that is, a decision tree model, trains on a slightly different dataset
    and therefore might have a slightly different outcome for the same test sample.
    Bagging, in a way, mimics the real-world example that we discussed and therefore
    combines several weak learners (decision tree models) into a strong learner.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 集成建模遵循相同的原理。在这里，每个模型中都会引入一定程度的随机性。Bagging算法为每个模型在训练数据上引入这种随机性。Bagging这个名字来源于**自助聚合**；这是一个过程，我们用替换数据从可用的数据中抽取三分之二的数据用于训练，其余的用于测试和验证。在这里，每个模型，即决策树模型，在略微不同的数据集上训练，因此对于相同的测试样本可能会有略微不同的结果。Bagging在某种程度上模仿了我们讨论的现实世界例子，因此将几个弱学习器（决策树模型）组合成一个强学习器。
- en: How Does Random Forest Work?
  id: totrans-386
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林是如何工作的？
- en: '**Random forest** is basically a successor to bagging. Here, apart from the
    randomness in the training data, random forest adds an additional layer of randomness
    with the feature set. Therefore, each decision tree not only has bootstrap aggregation,
    that is, two thirds of the training data with replacement, but also a subset of
    features randomly selected from the available list. Thus, each individual decision
    tree in the ensemble has a slightly different training dataset and a slightly
    different set of features to train. This additional layer of randomness works
    effectively in generalizing the model and reduces variance.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**基本上是 bagging 的继承者。在这里，除了训练数据的随机性外，随机森林还通过特征集添加了一个额外的随机层。因此，每个决策树不仅具有自助聚合，即三分之二的训练数据有放回地替换，而且还从可用列表中随机选择特征的一个子集。因此，集成中的每个单个决策树都有略微不同的训练数据集和略微不同的特征集进行训练。这额外的随机层在泛化模型方面非常有效，并减少了方差。'
- en: 'Exercise 73: Building a Random Forest Model in R'
  id: totrans-388
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 73：在 R 中构建随机森林模型
- en: In this exercise, we will build a random forest model on the same dataset we
    leveraged in Exercises 8, 9, and 10\. We will leverage ensemble modelling and
    test whether the overall model performance improves compared to decision trees
    and logistic regression.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将在练习 8、9 和 10 中使用的相同数据集上构建一个随机森林模型。我们将利用集成建模，并测试整体模型性能是否优于决策树和逻辑回归。
- en: Note
  id: totrans-390
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To get started, we can quickly build a random forest model using the same dataset
    we used earlier. The `randomForest` package in R provides the implementation for
    the model, along with a few additional functions to optimize the model.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，我们可以快速使用之前使用的相同数据集构建一个随机森林模型。R 中的 `randomForest` 包提供了模型的实现，以及一些额外的函数来优化模型。
- en: 'Let''s look at a basic random forest model. Perform the following steps:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个基本的随机森林模型。执行以下步骤：
- en: 'First, import the `randomForest` library using the following command:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令导入 `randomForest` 库：
- en: '[PRE67]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Build a random forest model with all of the independent features available:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所有可用的独立特征构建一个随机森林模型：
- en: '[PRE68]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Evaluate on the training data:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据上评估：
- en: '[PRE69]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Evaluate on the test data:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上评估：
- en: '[PRE70]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Plot the feature importance:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制特征重要性：
- en: '[PRE71]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The output is as follows:'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE72]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '![Figure 5.7: Random Forest model'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.7：随机森林模型'
- en: '](img/C12624_05_07.jpg)'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12624_05_07.jpg)'
- en: 'Figure 5.7: Random Forest model'
  id: totrans-407
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.7：随机森林模型
- en: Note
  id: totrans-408
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/2Q2xKwd.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 GitHub 上找到完整的代码：http://bit.ly/2Q2xKwd。
- en: 'Activity 10: Build a Random Forest Model with a Greater Number of Trees'
  id: totrans-410
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 10：构建具有更多树的随机森林模型
- en: In *Exercise 11*, *Building a Random Forest Model in R*, we created a random
    forest model with just 100 trees; we can build a more robust model with a higher
    number of trees. In this activity, we will create a random forest model with 500
    trees and study the impact of the model having only 100 trees. In general, we
    expect the model's performance to improve (at least marginally with an increased
    number of trees). This comes with higher computational time for the model to converge.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *练习 11*，*在 R 中构建随机森林模型* 中，我们创建了一个只有 100 树的随机森林模型；我们可以构建一个具有更多树的更健壮的模型。在这个活动中，我们将创建一个拥有
    500 树的随机森林模型，并研究模型只有 100 树的影响。一般来说，我们预计模型的性能会提高（至少随着树的数量增加而略有提高）。这伴随着模型收敛所需更高的计算时间。
- en: 'Perform the following steps to build a random forest model with 500 trees:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以构建一个拥有 500 树的随机森林模型：
- en: Develop a random forest model with a higher number of trees; say, 500\. Readers
    are encouraged to try higher numbers such as 1,000, 2,000, and so on, and study
    the incremental improvements in each version.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发一个具有更多树的随机森林模型；比如说，500 树。鼓励读者尝试更高的数字，如 1,000、2,000 等，并研究每个版本的增量改进。
- en: Leverage the fitted model to predict estimates on the train-and-test data and
    study whether there was any improvement compared to the model with 100 trees.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用拟合的模型在训练和测试数据上预测估计值，并研究与拥有 100 树的模型相比是否有任何改进。
- en: Note
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can find the solution for this activity on page 457.
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在第 457 页找到这个活动的解决方案。
- en: XGBoost
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost
- en: '**XGBoost** is the most popular boosting technique in recent times. Although
    there have been various new versions that have been developed by large corporations,
    XGBoost still remains the undisputed king. Let''s look at a brief history of boosting.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '**XGBoost** 是近年来最受欢迎的增强技术。尽管有大型公司开发了各种新版本，但 XGBoost 仍然稳居王座。让我们简要回顾一下增强技术的历史。'
- en: How Does the Boosting Process Work?
  id: totrans-419
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升过程是如何工作的？
- en: Boosting differs from bagging in its core principles; the learning process is,
    in fact, sequential. Every model built in an ensemble is ideally an improved version
    of the previous model. To understand boosting in simple terms, imagine you are
    playing a game where you must remember all the objects placed on the table that
    you are shown just once for 30 seconds. The moderator of the game arranges around
    50-100 different objects on a table, such as a bat, ball, clock, die, coins, and
    so on, and covers them with a large piece of cloth. When the game begins, he withdraws
    the cloth from the table and gives you exactly 30 seconds to see them and puts
    the curtain back. You now must recollect all the objects you can remember. The
    participant who can recollect the most, aces the game.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 提升法与装袋法在核心原则上有区别；学习过程实际上是顺序的。在集成中构建的每个模型理想上都是前一个模型的改进版本。用简单的话来说，想象你正在玩一个游戏，你必须记住你只被展示了一次、持续30秒的所有放在桌子上的物品。游戏的主持人将大约50-100种不同的物品放在桌子上，如蝙蝠、球、钟、骰子、硬币等等，然后用一块大布覆盖它们。当游戏开始时，他从桌子上取下布，给你30秒的时间看它们，然后再次拉上布。你现在必须回忆起你能记住的所有物品。能回忆起最多物品的参与者赢得了游戏。
- en: In this game, let's add one new dimension. Assume you are a team and the players
    take turns one by one to announce all the objects they can recollect, while the
    others listen to them. Say there are 10 participants; each participant steps forward
    and announces out loud the objects they can recollect from the table. By the time
    the second player steps forward, they have heard all the objects called out by
    the first player. They would have mentioned a few objects that the second player
    might not have recollected. To improve on the first player, the second player
    learns a few new objects from the first player, adds them to his list, and then
    announces them out loud. By the time the last player steps forward, they have
    already learned several objects that other players recollected, which they failed
    to recollect themselves.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个游戏中，让我们增加一个新维度。假设你是一个团队，玩家们一个接一个地轮流宣布他们能回忆起的所有物品，而其他人则聆听。假设有10名参与者；每个参与者走上前来，大声宣布他们从桌子上能回忆起的物品。当第二个玩家走上前来时，他们已经听到了第一个玩家宣布的所有物品。他们可能会提到一些第二个玩家可能没有回忆起的物品。为了改进第一个玩家的表现，第二个玩家从第一个玩家那里学习了一些新的物品，将它们添加到自己的列表中，然后大声宣布。当最后一名玩家走上前来时，他们已经学习了其他玩家回忆起的几个物品，而这些物品他们自己未能回忆起来。
- en: Putting those together, that player creates the most exhaustive list and aces
    the competition. The fact that each player announces the list sequentially helps
    the next player learn from their mistakes and improvise on it.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些放在一起，那个玩家创建了最详尽的列表，并赢得了比赛。每个玩家依次宣布列表的事实有助于下一个玩家从他们的错误中学习并改进。
- en: Boosting works in the same way. Each model trained sequentially is imparted
    with additional knowledge, such that the errors of the first model are learned
    better in the second model. Say the first model learns to classify well for most
    cases of a specific independent variable; however, it fails to correctly predict
    for just one specific category. The next model is imparted with a different training
    sample, such that the model learns better for the category where the previous
    model fails. A simple example would be oversampling based on the variable or category
    of interest. Boosting effectively reduces bias and therefore improves the model's
    performance.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 提升法以相同的方式工作。每个按顺序训练的模型都获得了额外的知识，使得第一个模型的错误在第二个模型中得到了更好的学习。比如说，第一个模型学会了在特定独立变量的大多数情况下进行良好的分类；然而，它未能正确预测一个特定的类别。下一个模型被赋予了不同的训练样本，使得模型在先前模型失败的类别中学习得更好。一个简单的例子是基于感兴趣变量或类别的过采样。提升法有效地减少了偏差，因此提高了模型的表现。
- en: What Are Some Popular Boosting Techniques?
  id: totrans-424
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有哪些流行的提升技术？
- en: The boosting techniques introduced earlier were not very popular, because they
    were easily overfit and often required, relatively, a lot of effort in tuning
    to achieve great performance. AdaBoost, BrownBoost, Gradient Boosting, and Stochastic
    Gradient Boosting are all boosting techniques that were popular for a long time.
    However, in 2014, when T Chen and others introduced XGBoost (**Extreme Gradient
    Boosting**), it ushered in a new height in the boosting performance.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 之前介绍的提升技术并不太受欢迎，因为它们很容易过拟合，并且通常需要相对较多的努力来调整以达到出色的性能。AdaBoost、BrownBoost、梯度提升和随机梯度提升都是长期流行的提升技术。然而，在2014年，当T
    Chen和其他人引入XGBoost（**极端梯度提升**）时，它为提升性能带来了新的高度。
- en: How Does XGBoost Work?
  id: totrans-426
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XGBoost是如何工作的？
- en: XGBoost natively introduced regularization, which helps models combat overfitting
    and thus delivered high performance. Compared to other available boosting techniques
    at the time, XGBoost reduced the overfitting problem significantly and with the
    least amount of effort. With current implementations of the model in R or any
    other language, XGBoost almost always performs great with the default parameter
    setting. (Though, this is not always true; in many cases, random forest outperforms
    XGBoost). XGBoost has been among the most popular choice of algorithms used in
    data science hackathons and enterprise projects.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost原生引入了正则化，这有助于模型对抗过拟合，从而实现了高性能。与其他当时可用的提升技术相比，XGBoost显著减少了过拟合问题，并且所需的工作量最少。在R或任何其他语言的当前模型实现中，XGBoost几乎总是使用默认参数设置表现出色。（尽管，这并不总是正确的；在许多情况下，随机森林的表现优于XGBoost）。XGBoost一直是数据科学黑客马拉松和企业项目中使用的最流行的算法之一。
- en: 'In a nutshell, XGBoost has regularization introduced in the objective function,
    which penalizes the model when it gets more complicated in a training iteration.
    Discussing the depth of mathematical constructs that goes into XGBoosting is beyond
    the scope of this chapter. You can refer to T Chen''s paper here (https://arxiv.org/abs/1603.02754)
    for further notes. Also, this blog will help you to understand the mathematical
    differences between GBM and XGBoost in a simple way: https://towardsdatascience.com/boosting-algorithm-xgboost-4d9ec0207d.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，XGBoost在目标函数中引入了正则化，当模型在训练迭代中变得更加复杂时，会对模型进行惩罚。讨论XGBoost中涉及的数学结构的深度超出了本章的范围。您可以参考T
    Chen的论文以获取更多信息（https://arxiv.org/abs/1603.02754）。此外，这篇博客以简单的方式帮助您理解GBM和XGBoost之间的数学差异：https://towardsdatascience.com/boosting-algorithm-xgboost-4d9ec0207d。
- en: Implementing XGBoost in R
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在R中实现XGBoost
- en: We can leverage the XGBoost package, which provides a neat implementation of
    the algorithm. There are a few differences in the implementation approach that
    we will need to take care of before getting started. Unlike other implementations
    of algorithms in R, XGBoost does not handle categorical data (others take care
    of converting it into numeric data internally). The internal functioning of XGBoost
    in R doesn't handle the automatic conversion of categorical columns into numeric
    columns. Therefore, we manually convert categorical columns into numeric or one-hot
    encoded form.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用XGBoost包，它提供了算法的整洁实现。在开始之前，我们需要注意一些实现方法上的差异。与其他R中算法的实现不同，XGBoost不处理分类数据（其他实现会将其内部转换为数值数据）。XGBoost在R中的内部工作方式不处理将分类列自动转换为数值列。因此，我们手动将分类列转换为数值或独热编码形式。
- en: A one-hot encoded form basically represents a single categorical column as a
    binary encoded form. Say we have a categorical column with values such as **Yes**/**No**/**Maybe**;
    then, we transform this single variable, where we have an individual variable
    for each value of the categorical variable indicating its value as **0** or **1**.
    So, the values for the columns **Yes**, **No**, and **Maybe** will take **0**
    and **1** based on the original value.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码形式基本上是将单个分类列表示为二进制编码形式。比如说我们有一个包含**是**/**否**/**可能**等值的分类列；然后，我们将这个单一变量转换为，其中我们为分类变量的每个值都有一个单独的变量，表示其值为**0**或**1**。因此，**是**、**否**和**可能**的值将根据原始值取**0**和**1**。
- en: 'One-hot encoding is demonstrated in the following table:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码在以下表格中演示：
- en: '![Figure 5.8: One-hot encoding'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.8：独热编码'
- en: '](img/C12624_05_08.jpg)'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12624_05_08.jpg)'
- en: 'Figure 5.8: One-hot encoding'
  id: totrans-435
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.8：独热编码
- en: Let's transform the data into the required form and build an XGBoost model on
    the dataset.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将数据转换为所需的形式，并在数据集上构建一个XGBoost模型。
- en: 'Exercise 74: Building an XGBoost Model in R'
  id: totrans-437
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习74：在R中构建XGBoost模型
- en: Just as we did in *Exercise 11*, *Building a Random Forest Model in R*, we will
    try to improve the performance of the classification model by building an XGBoost
    model for the same use case and dataset as in *Exercise 11*, *Building a Random
    Forest Model in R*.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*练习11*，*在R中构建随机森林模型*中做的那样，我们将尝试通过为与*练习11*，*在R中构建随机森林模型*相同的用例和数据集构建XGBoost模型来提高分类模型的性能。
- en: Perform the following steps to build an XGBoost model in R.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以在R中构建XGBoost模型。
- en: 'Create list placeholders for the target, categorical, and numeric variables:'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为目标、分类和数值变量创建列表占位符：
- en: '[PRE73]'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Convert the categorical factor variables into character. This will be useful
    for converting them into one-hot-encoded forms:'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分类因子变量转换为字符。这将在将它们转换为独热编码形式时很有用：
- en: '[PRE74]'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Convert the categorical variables into one-hot encoded forms using the `dummyVars`
    function from the `caret` package:'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`caret`包中的`dummyVars`函数将分类变量转换为独热编码形式：
- en: '[PRE75]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Combine numeric variables and the one-hot encoded variables from the third
    step into a single DataFrame named `df_final`:'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第三步中的数值变量和独热编码变量合并到一个名为`df_final`的单个DataFrame中：
- en: '[PRE76]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Convert the target variable into numeric form, as the XGBoost implementation
    in R doesn''t accept factor or character forms:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目标变量转换为数值形式，因为R中的XGBoost实现不接受因子或字符形式：
- en: '[PRE77]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Split the `df_final` dataset into train (70%) and test (30%) datasets:'
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`df_final`数据集分为训练集（70%）和测试集（30%）：
- en: '[PRE78]'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Build an XGBoost model using the `xgboost` function. Pass the train data and
    `y_train` target variable and define the `eta = 0.01`, `max_depth = 6`, `nrounds
    = 200`, and `colsample_bytree = 1` hyperparameters, define the evaluation metric
    as `logloss,` and the `objective` function as `binary:logistic`, since we are
    dealing with binary classification:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`xgboost`函数构建XGBoost模型。传递训练数据和`y_train`目标变量，并定义`eta = 0.01`、`max_depth = 6`、`nrounds
    = 200`和`colsample_bytree = 1`超参数，定义评估指标为`logloss`，目标函数为`binary:logistic`，因为我们处理的是二元分类：
- en: '[PRE79]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Make a prediction using the fitted model on the train dataset and create the
    confusion matrix to evaluate the model''s performance on the train data:'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练数据集上的拟合模型进行预测，并创建混淆矩阵以评估模型在训练数据上的性能：
- en: '[PRE80]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The output is as follows:'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE81]'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Now, as in the previous step, make predictions using the fitted model on the
    test dataset and create the confusion matrix to evaluate the model''s performance
    on the test data:'
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，就像在之前的步骤中一样，使用拟合模型在测试数据集上进行预测，并创建混淆矩阵以评估模型在测试数据上的性能：
- en: '[PRE82]'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The output is as follows:'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE83]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: If we take a closer look at the results from the model, we can see a slight
    improvement in the performance compared to random forest model results. The `0.54`
    instead of `0.5`, we can increase the precision (to match random forest) while
    still having slightly higher recall than random forest. The increase in recall
    for XGBoost is significantly higher than the decrease in precision. The threshold
    value for the probability cutoff is not a defined, hard cutoff. We can tweak the
    threshold based on our use case. The best number can be studied with empirical
    experiments or by studying the sensitivity, specificity distribution.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仔细观察模型的结果，我们可以看到与随机森林模型结果相比，性能略有提升。从`0.5`提升到`0.54`，我们可以在保持比随机森林略高的召回率的同时提高精确度（以匹配随机森林）。XGBoost的召回率提升幅度远大于精确度的下降。概率截止值的阈值不是一个固定的、硬性的截止值。我们可以根据我们的用例调整阈值。最佳数值可以通过经验实验或研究敏感性、特异性分布来研究。
- en: Note
  id: totrans-463
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/30gzSW0.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub上找到完整的代码：http://bit.ly/30gzSW0。
- en: The following exercise uses 0.54 instead of 0.5 as the probability cutoff to
    study the improvement in precision at the cost of recall.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 以下练习使用0.54而不是0.5作为概率截止值，以研究在牺牲召回率的情况下精确度的提升。
- en: 'Exercise 75: Improving the XGBoost Model''s Performance'
  id: totrans-466
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习75：提高XGBoost模型性能
- en: We can tweak the model performance of binary classification models by adjusting
    the threshold value of the output. By default, we select 0.5 as the default probability
    cutoff. So, all responses above 0.5 are tagged as `Yes`, else `No`. Adjusting
    the threshold can help us achieve more sensitive or more precise models.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调整输出的阈值来调整二元分类模型的性能。默认情况下，我们选择0.5作为默认概率截止值。因此，所有高于0.5的响应都被标记为`Yes`，否则为`No`。调整阈值可以帮助我们实现更敏感或更精确的模型。
- en: 'Perform the following steps to improve the XGBoost model''s performance by
    adjusting the threshold for the probability cutoff:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整概率截止阈值来提高 XGBoost 模型的性能，执行以下步骤：
- en: 'Increase the probability cutoff for the prediction on the train dataset from
    0.5 to 0.53 and print the results:'
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据集上预测的概率截止阈值从 0.5 提高到 0.53 并打印结果：
- en: '[PRE84]'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The output is as follows:'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE85]'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Increase the probability cutoff for the prediction on the test dataset from
    0.5 to 0.53 and print the results:'
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将测试数据集上预测的概率截止阈值从 0.5 提高到 0.53 并打印结果：
- en: '[PRE86]'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The output is as follows:'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE87]'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: We see that, at 44% recall, we have 80% precision on the test dataset, and the
    difference in performance between the train and test datasets is also negligible.
    We can therefore conclude that the model performance of XGBoost is a bit better
    than random forest, though only a bit.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在 44% 的召回率下，我们在测试数据集上达到了 80% 的精确率，训练集和测试集之间的性能差异也可以忽略不计。因此，我们可以得出结论，XGBoost
    的模型性能略优于随机森林，尽管只是略好。
- en: Note
  id: totrans-478
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/30c5DQ9.'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 GitHub 上找到完整的代码：http://bit.ly/30c5DQ9。
- en: Before wrapping up our chapter, let's experiment with the last supervised technique
    for classification, that is, deep neural networks.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本章之前，让我们通过实验最后一种用于分类的监督技术，即深度神经网络。
- en: Deep Neural Networks
  id: totrans-481
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: The last type of technique that we will be discussing before wrapping up our
    chapter is deep neural networks or deep learning. This is a long and complicated
    topic, which by no means will we be able to do justice in a short section of this
    chapter. A complete book may not even suffice to cover the surface of the topic!
    We will explore the topic from 100 feet and quickly study an easy implementation
    in R.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本章之前，我们将讨论最后一种技术，即深度神经网络或深度学习。这是一个漫长且复杂的话题，我们绝对无法在章节的简短部分中公正地处理它。一本书可能甚至不足以覆盖这个话题的表面！我们将从高处探讨这个话题，并快速研究
    R 中的简单实现。
- en: Deep neural networks, which are primarily used in the field of computer vision
    and natural language processing, have also found significance in machine learning
    use cases for regression and classification on tabular cross-sectional data. With
    large amounts of data, deep neural networks have been proved to be very effective
    at learning latent patterns and thus training models with better performance.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络主要应用于计算机视觉和自然语言处理领域，在机器学习的回归和分类用例中也具有重要意义，特别是在表格横截面数据上。随着大量数据的出现，深度神经网络已被证明在学习潜在模式并因此训练出性能更好的模型方面非常有效。
- en: A Deeper Look into Deep Neural Networks
  id: totrans-484
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度神经网络的深入探讨
- en: Deep neural networks were inspired by the neural structure of the human brain.
    The field of deep learning became popular for solving computer vision problems,
    that is, the area of problems that were easily solved by humans, but computers
    struggled with for a long time. The motivation for designing deep neural networks
    akin to a miniature and highly simplified human brain was to solve problems that
    were specifically easy for humans. Later, with the success of deep learning in
    the field of computer vision, it was embraced in several other fields, including
    traditional machine learning supervised use cases.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络受到人脑神经结构的启发。深度学习领域因解决计算机视觉问题而变得流行，即那些人类容易解决但计算机长期难以解决的问题领域。设计类似微型且高度简化的深度神经网络，旨在解决人类特别容易解决的问题。后来，随着深度学习在计算机视觉领域的成功，它被应用于其他几个领域，包括传统的机器学习监督用例。
- en: A neural network is organized as a hierarchy of neurons, just like the neurons
    in the human brain. Each neuron is connected to other neurons, which enables communication
    between them that traverses as a signal to other neurons and results in a large
    complex network that can learn with a feedback mechanism.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络组织成一个神经元层次结构，就像人脑中的神经元一样。每个神经元都与其他神经元相连，这使它们之间能够通过信号进行通信，形成一个可以学习并具有反馈机制的复杂网络。
- en: 'The following figure demonstrates a simple neural network:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个简单的神经网络：
- en: '![Figure 5.9: Simple neural network](img/C12624_05_09.jpg)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9：简单的神经网络](img/C12624_05_09.jpg)'
- en: 'Figure 5.9: Simple neural network'
  id: totrans-489
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.9：简单的神经网络
- en: The input data forms the 0th layer in the network. This layer then connects
    to the neurons within the next layer, which is hidden. It is called **hidden**
    as the network can be perceived as a black box where we provide input to the network
    and directly see the output. The intermediate layers are hidden. In a neural network,
    a layer can have any number of neurons and each network can have any number of
    layers. The larger the number of layers, the 'deeper' the network will be. Hence
    the name deep learning and deep neural networks. Every neuron in each hidden layer
    computes a mathematical function, which is called the activation function in deep
    learning. This function helps in mimicking the signal between two neurons. If
    the function (activation) computes a value greater than a threshold, it sends
    a signal to the immediate connected neuron in the next layer. The connection between
    these two neurons is moderated by a weight. The weight decides how important the
    incoming neuron's signal is for the receiving neuron. The learning method in the
    deep learning model updates the weights between neurons such that the end prediction,
    akin to machine learning models, is the most accurate one.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据构成了网络的第0层。这一层然后连接到下一层的神经元，下一层是隐藏的。它被称为**隐藏的**，因为网络可以被看作是一个黑盒，我们向网络提供输入并直接看到输出。中间层是隐藏的。在神经网络中，一层可以有任意数量的神经元，每个网络可以有任意数量的层。层的数量越多，网络就越“深”。因此得名深度学习和深度神经网络。每个隐藏层中的每个神经元都计算一个数学函数，在深度学习中这被称为激活函数。这个函数有助于模拟两个神经元之间的信号。如果函数（激活）计算出的值大于阈值，它就会向下一层直接连接的神经元发送信号。这两个神经元之间的连接由一个权重调节。权重决定了传入神经元的信号对接收神经元的重要性。深度学习模型中的学习方法通过更新神经元之间的权重，使得最终的预测，类似于机器学习模型，是最准确的。
- en: How Does the Deep Learning Model Work?
  id: totrans-491
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习模型是如何工作的？
- en: To understand how a neural network works and learns to make predictions on data,
    let's consider a simple task that is, relatively, very easy for humans. Consider
    the task of learning to identify different people by their faces. Most of us meet
    a few different people every day; say, at work, school, or on the street. Every
    person we meet is different from each other in some dimension. Though everyone
    would have a ton of similar features, such as two eyes, two ears, lips, two hands,
    and so on, our brain easily distinguishes between two individuals. The second
    time we meet a person, we would most probably recognize them and distinguish them
    as someone we met previously. Given the scale at which this happens and the fact
    that our brain effectively works to solve this mammoth problem with ease, it makes
    us wonder how exactly this happens.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解神经网络是如何工作以及如何学习在数据上做出预测的，让我们考虑一个对人类来说相对非常简单的任务。考虑通过人脸识别不同人的任务。我们大多数人每天都会遇到几个不同的人；比如说，在工作、学校或街道上。我们遇到的每个人在某些维度上都是不同的。尽管每个人都会有大量的相似特征，比如两只眼睛、两只耳朵、嘴唇、两只手等等，但我们的大脑可以轻易地区分两个人。当我们第二次遇到某个人时，我们很可能会认出他们，并将他们识别为我们之前遇到过的人。考虑到这种情况发生的规模以及我们的大脑能够轻松地解决这个巨大问题的现实，这让我们不禁想知道这究竟是如何发生的。
- en: To understand this and appreciate the beauty of our brain, we need to understand
    how the brain fundamentally learns. The brain is a large, complex structure of
    interconnected neurons. Each neuron gets activated when it senses something essential
    and passes a message or signal to other neurons it is connected to. The connection
    between neurons is strengthened by constant learning from the feedback they receive.
    Here, when we see a new face, rather than learning the structure of the face to
    identify people, the brain learns how different the given face is from a generic
    baseline face. This can be further simplified as calculating the difference between
    important facial features, such as eye shape, nose, lips, ears, and lip structure,
    color deviations of the skin and hair, and other attributes. These differences,
    which are quantified by different neurons, are then orchestrated in a systematic
    fashion for the brain to distinguish one face from another and recall a face from
    memory. This entire computation happens subconsciously, and we barely realize
    this as the results are instant for us to notice anything specific.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点并欣赏我们大脑的美丽，我们需要了解大脑是如何从根本上学习的。大脑是一个由相互连接的神经元组成的大型、复杂结构。当神经元感知到某些重要事物时，它会被激活，并将消息或信号传递给它连接的其他神经元。神经元之间的连接通过从反馈中不断学习而得到加强。在这里，当我们看到一张新面孔时，我们的大脑不是学习面孔的结构来识别人，而是学习给定的面孔与通用基准面孔的不同之处。这可以进一步简化为计算重要面部特征（如眼睛形状、鼻子、嘴唇、耳朵和唇部结构）之间的差异，以及皮肤和头发的颜色偏差和其他属性。这些差异由不同的神经元量化，然后以系统化的方式编排，以便大脑能够区分不同的面孔并从记忆中回忆起面孔。整个计算都是在潜意识中发生的，我们几乎意识不到这一点，因为结果对我们来说是一瞬间就能注意到的。
- en: A neural network essentially tries to mimic the learning functionality of the
    brain in an extremely simplified form. Neurons are connected to each other in
    a layer-wise fashion and initialized with random weights. A mathematical calculation
    across the network combines the inputs from all neurons layer-wise and finally
    reaches the end outcome. The deviation of the end outcome (the predicted value)
    is then quantified as an error and is given as feedback to the network. Based
    on the error, the network tries updating the weights of the connections and tries
    to reduce the error in the prediction iteratively. With several iterations, the
    network updates its weights in an ordered fashion and thus learns to recognize
    patterns to make a correct prediction.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络本质上试图以极其简化的形式模仿大脑的学习功能。神经元以分层的方式相互连接，并使用随机权重进行初始化。网络中的数学计算结合了所有神经元的输入，最终达到最终结果。最终结果的偏差（预测值）被量化为误差，并作为反馈提供给网络。基于误差，网络试图更新连接的权重，并尝试迭代地减少预测中的误差。经过几次迭代，网络以有序的方式更新其权重，从而学会识别模式以做出正确的预测。
- en: What Framework Do We Use for Deep Learning Models?
  id: totrans-495
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们使用什么框架进行深度学习模型？
- en: For now, we will experiment with deep neural networks for our classification
    use case, using Keras for R. For deep learning model development, we would need
    to write a ton of code, which would render the building blocks for the network.
    To speed up our process, we can leverage Keras, a deep learning framework that
    provides neat abstraction for deep learning components. Keras has an R interface
    and works on top of a low-level deep learning framework.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将使用Keras for R进行深度神经网络实验，以用于我们的分类用例。对于深度学习模型开发，我们需要编写大量的代码，这将构成网络的构建块。为了加快我们的进程，我们可以利用Keras，这是一个提供深度学习组件整洁抽象的深度学习框架。Keras有一个R接口，并且建立在低级深度学习框架之上。
- en: The deep learning frameworks available in today's AI community are either low-level
    or high-level. Frameworks such as TensorFlow, Theano, PyTorch, PaddlePaddle, and
    mxnet are low-level frameworks that provide the basic building blocks for deep
    learning models. Using low-level frameworks offers a ton of flexibility and customization
    to the end network design. However, we would still need to write quite a lot of
    code to get a relatively large network working. To simplify this further, there
    are a few high-level frameworks available that work on top of the low-level frameworks
    and provide a second layer of abstraction in the process of building deep learning
    models. Keras, Gluon, and Lasagne are a few frameworks that leverage the aforementioned
    low-level framework as a backend and provide a new API that makes the overall
    development process far easier. This reduces the flexibility when compared to
    directly using a low-level framework such as TensorFlow, and offers a robust solution
    for most networks. For our use case, we can directly leverage Keras with the R
    interface.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的人工智能社区中可用的深度学习框架要么是低级的，要么是高级的。TensorFlow、Theano、PyTorch、PaddlePaddle和mxnet等框架是低级框架，为深度学习模型提供基本构建块。使用低级框架为最终网络设计提供了大量的灵活性和定制性。然而，我们仍然需要编写相当多的代码才能使一个相对较大的网络工作。为了进一步简化，有一些高级框架可用，它们在低级框架之上工作，并在构建深度学习模型的过程中提供第二层抽象。Keras、Gluon和Lasagne是一些利用上述低级框架作为后端并提供新API的框架，这使得整体开发过程变得容易得多。与直接使用TensorFlow等低级框架相比，这减少了灵活性，并为大多数网络提供了一个稳健的解决方案。对于我们的用例，我们可以直接利用Keras的R接口。
- en: Using the `install.packages('keras')` command would install the R interface
    to Keras and would also automatically install TensorFlow as the low-level backend
    for Keras.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`install.packages('keras')`命令将安装Keras的R接口，并会自动安装TensorFlow作为Keras的低级后端。
- en: Building a Deep Neural Network in Keras
  id: totrans-499
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Keras中构建深度神经网络
- en: To leverage Keras in R, we would need additional data augmentations to our existing
    training dataset. In most machine learning functions available under R, we can
    pass the categorical column directly coded as a factor. However, we saw that XGBoost
    had a mandate that the data needs to be rendered into one-hot encoded form, as
    it does not internally transform the data into the required format. We therefore
    used the `dummyVars` function in R to transform the training and test dataset
    into a one-hot encoded version, such that we have only numerical data in the dataset.
    In Keras, we would need to feed a matrix instead of a DataFrame as the training
    dataset. Therefore, in addition to transforming the data into a one-hot encoded
    form, we would also need to convert the dataset into a matrix.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 要在R中利用Keras，我们需要对我们的现有训练数据集进行额外的数据增强。在R中大多数机器学习函数中，我们可以直接将分类列编码为因子传递。然而，我们注意到XGBoost强制要求数据需要被转换成one-hot编码形式，因为它不会将数据内部转换为所需的格式。因此，我们使用了R中的`dummyVars`函数将训练和测试数据集转换为one-hot编码版本，这样我们数据集中就只有数值数据。在Keras中，我们需要将训练数据集作为矩阵而不是DataFrame来提供。因此，除了将数据转换为one-hot编码形式外，我们还需要将数据集转换为矩阵。
- en: Moreover, it is also recommended that we standardize, normalize, or scale all
    our input dimensions. The process of normalization rescales data values into the
    range 0 to 1\. Similarly, standardization rescales data to have a mean (*μ*) of
    0 and standard deviation (*σ*) of 1 (unit variance). This transformation is a
    good feature to have in machine learning, as some algorithms tend to benefit and
    learn better. However, in deep learning, this transformation becomes crucial,
    as the model learning process suffers if we provide an input training dataset
    such that all dimensions are in a different range or scale. The reason behind
    this issue is the type of activation function used in neurons.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还建议我们标准化、归一化或缩放所有输入维度。归一化过程将数据值重新缩放到0到1的范围。同样，标准化将数据缩放到均值为（μ）0和标准差（σ）为1（单位方差）。这种转换在机器学习中是一个很好的特性，因为某些算法从中受益并更好地学习。然而，在深度学习中，这种转换变得至关重要，因为如果我们提供一个所有维度都在不同范围或尺度上的输入训练数据集，模型的学习过程就会受到影响。这个问题背后的原因是神经元中使用的激活函数类型。
- en: The following code snippet implements a basic neural network in Keras. Here,
    we use an architecture that has three layers with 250 neurons each. Finding the
    right architecture is an empirical process and does not have a definitive guide.
    The deeper network is designed, the more computation it will need to fit the data.
    The dataset used in the following snippet is the same as was used for XGBoost
    and already has the one-hot encoded forms.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段实现了Keras中的一个基本神经网络。在这里，我们使用一个具有三个层，每层有250个神经元的架构。找到正确的架构是一个经验过程，没有明确的指南。网络越深，拟合数据所需的计算就越多。以下代码片段中使用的数据集与XGBoost中使用的相同，并且已经有一元编码的形式。
- en: 'Exercise 76: Build a Deep Neural Network in R using R Keras'
  id: totrans-503
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习76：使用R Keras在R中构建深度神经网络
- en: In this exercise, we will leverage deep neural networks to build a classification
    model for the same use case as *Exercise 13*, *Improving XGBoost Model Performance*,
    and try to improve the performance. Deep neural networks will not always perform
    better than ensemble models. They are usually a preferred choice when we have
    a very high number of training samples, say 10 million. However, we will experiment
    and check whether we can achieve any better performance than the models we built
    in exercises 10-13.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将利用深度神经网络构建一个分类模型，用于与*练习13*，*提高XGBoost模型性能*相同的用例，并尝试提高性能。深度神经网络并不总是比集成模型表现更好。当我们有非常高的训练样本数量时，例如1000万，它们通常是一个更好的选择。然而，我们将进行实验并检查我们是否能实现比我们在练习10-13中构建的模型更好的性能。
- en: Perform the following steps to build a deep neural network in R.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤在R中构建深度神经网络。
- en: 'Scale the input dataset in the range 0 to 1\. We would first need to initiate
    a `preProcess` object on the training data. This will be later used to scale the
    train as well as the test data. Neural networks perform better with scaled data.
    The train data alone is used for creating the object to scale:'
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入数据集缩放到0到1的范围内。我们首先需要在训练数据上初始化一个`preProcess`对象。这将随后用于缩放训练数据以及测试数据。神经网络在缩放数据上表现更好。仅使用训练数据来创建缩放对象：
- en: '[PRE88]'
  id: totrans-507
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Use the `standardizer` object created in the previous step to scale the train
    and test data:'
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用之前步骤中创建的`standardizer`对象来缩放训练和测试数据：
- en: '[PRE89]'
  id: totrans-509
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Store the number of predictor variables in a variable called **predictors**.
    We will use this information to construct the network:'
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预测变量数量存储在一个名为**predictors**的变量中。我们将使用这些信息来构建网络：
- en: '[PRE90]'
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Define the structure for a deep neural network. We will use the `keras_model_sequential`
    method. We will create a network with three hidden layers, having 250 neurons
    each and `relu` as the activation function. The output layer will have one neuron
    with the `sigmoid` activation function (since we are developing a binary classification
    mode):'
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义深度神经网络的架构。我们将使用`keras_model_sequential`方法。我们将创建一个包含三个隐藏层，每个层有250个神经元，激活函数为`relu`的网络。输出层将有一个神经元，激活函数为`sigmoid`（因为我们正在开发一个二元分类模型）：
- en: '[PRE91]'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Define the model optimizer as `adam`, loss function, and the metrics to capture
    for the model''s training iteration:'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型优化器为`adam`，损失函数以及模型训练迭代中要捕获的指标：
- en: '[PRE92]'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'The output is as follows:'
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE93]'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Fit the model structure we created in steps 4-5 with the training and test
    data from steps 1-2:'
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用步骤4-5中创建的模型结构，以及步骤1-2中的训练和测试数据来拟合模型：
- en: '[PRE94]'
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'The output is as follows:'
  id: totrans-520
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE95]'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Predict the responses using the fitted model on the train dataset:'
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用拟合的模型在训练数据集上预测响应：
- en: '[PRE96]'
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'The output is as follows:'
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE97]'
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Predict the responses using the fitted model on the test dataset:'
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用拟合的模型在测试数据集上预测响应：
- en: '[PRE98]'
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'The output is as follows:'
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE99]'
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: Note
  id: totrans-530
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/2Vz8Omb.'
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在GitHub上找到完整的代码：http://bit.ly/2Vz8Omb。
- en: The preprocessor function helps to transform the data into the required scale
    or range. Here, we scale the data to a 0 to 1 scale. We should only consider using
    the train data as the input to the function generator and use the fitted method
    to scale the test data. This is essential, as we won't have access to the test
    data in a real-time scenario. Once the `preProcess` method is fit, we use it to
    transform the train and test data. We then define the architecture for the deep
    neural network model. R provides the easy to extend pipe operator with `%>%`,
    which enables the easy concatenation of the operators. We design a network with
    three layers and 250 neurons each. The input data will form the 0th layer and
    the last layer will be the predicted outcome. The activation function used in
    the network for the hidden layers is `relu`, the most recommended activation function
    for any deep learning use case. The final layer has the `sigmoid` activation function,
    as we have a binary classification use case. There are a ton of activation functions
    to choose from in Keras, such as `prelu`, `tanh`, `swish`, and so on. Once, the
    model architecture is defined, we define the loss function, `binary_crossentropy`,
    which is analogous to binary `logloss` (akin to XGBoost), the optimizer, that
    is, technique, used by the model to learn and backpropagate. The errors in the
    prediction are backpropagated to the network so that it can adjust the weights
    in the right direction and iteratively reduce the error.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理函数有助于将数据转换成所需的尺度或范围。在这里，我们将数据缩放到0到1的尺度。我们应仅考虑使用训练数据作为函数生成器的输入，并使用拟合方法来缩放测试数据。这在实际场景中非常重要，因为我们无法实时访问测试数据。一旦`preProcess`方法拟合完成，我们就用它来转换训练和测试数据。然后我们定义深度神经网络模型的架构。R提供了易于扩展的管道操作符`%>%`，它使得操作符的连接变得简单。我们设计了一个包含三个层次，每个层次有250个神经元的网络。输入数据将形成第0层，最后一层将是预测结果。网络中用于隐藏层的激活函数是`relu`，这是任何深度学习用例中最推荐的激活函数。最后一层使用`sigmoid`激活函数，因为我们有一个二元分类用例。在Keras中可以选择许多激活函数，例如`prelu`、`tanh`、`swish`等等。一旦模型架构定义完成，我们就定义损失函数`binary_crossentropy`，它与二元的`logloss`（类似于XGBoost）类似，是模型用来学习和反向传播的技术，即优化器。预测中的错误会被反向传播到网络中，以便它可以调整权重，并迭代地减少错误。
- en: The mathematical intuitiveness of this functionality can take various approaches.
    Adam optimization, which is based on adaptive estimates of lower-order moments,
    is the most popular choice, which we can almost blindly experiment with for most
    use cases in deep learning. Some of the other options are `rmsprop`, stochastic
    gradient descent, and `Adagrad`. We also define the metrics to calculate on the
    validation dataset after each epoch, that is, one complete presentation of training
    samples to the network. The `summary` function displays the resultant architecture
    we defined in the preceding section using the Keras constructs. The `summary`
    function gives us a brief idea of the number of parameters in each layer and additionally
    represents the network in a hierarchical structure to help us visualize the model
    architecture. Lastly, we use the `fit` function which trains or 'fits' the data
    to the network. We also define the number of epochs the model should iterate;
    the higher the number of epochs, the longer the training process will take to
    compute.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 这个功能的数学直观性可以采取多种方法。基于低阶矩自适应估计的Adam优化是最受欢迎的选择，对于大多数深度学习用例，我们几乎可以盲目地进行实验。其他一些选项包括`rmsprop`、随机梯度下降和`Adagrad`。我们还定义了在每个epoch后要在验证数据集上计算的指标，即网络对训练样本的一次完整展示。`summary`函数显示了使用Keras构造在前一节定义的架构的结果。`summary`函数给我们一个关于每个层中参数数量的简要概念，并且以层次结构的形式表示网络，帮助我们可视化模型架构。最后，我们使用`fit`函数来训练或“拟合”数据到网络中。我们还定义了模型应该迭代的epoch数；epoch数越高，训练过程将需要更长的时间来计算。
- en: The batch size indicates the number of training samples the network consumes
    in one single pass before updating the weights of the network; a lower number
    for the batch indicates more frequent weight updates and helps the RAM memory
    to be effectively utilized. The validation split defines the percentage of training
    samples to be used for validation at the end of each epoch. Finally, we validate
    the model's performance on the train and test data.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 批次大小表示网络在更新网络权重之前在一次单次传递中消耗的训练样本数量；批次较小的数字表示更频繁的权重更新，有助于有效地利用RAM内存。验证分割定义了每个epoch结束时用于验证的训练样本的百分比。最后，我们在训练数据和测试数据上验证模型的表现。
- en: Note
  id: totrans-535
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This explanation in the code snippet will by no means be a justification for
    the topic. A deep neural network is an extremely vast and complex topic that might
    need a complete book for a basic introduction. We have wrapped the context into
    a short paragraph for you to understand the constructs used in the model development
    process. Exploring the depth of the topic would be beyond the scope of this book.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段中的这种解释无论如何都不能成为主题的正当理由。深度神经网络是一个极其庞大且复杂的主题，可能需要一本书来介绍基础知识。我们已将上下文封装成一段简短的段落，以便您了解模型开发过程中使用的结构。探索这个主题的深度将超出本书的范围。
- en: Looking at the results, we can see similar results as for the previous models.
    The results are almost comparable with the XGBoost model we developed previously.
    We have around 48% recall and 75% precision on the test dataset. The results can
    be further tweaked to reduce recall and enhance precision (if necessary).
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果来看，我们可以看到与之前模型相似的结果。结果几乎可以与之前开发的XGBoost模型相媲美。在测试数据集上，我们大约有48%的召回率和75%的精确率。结果可以进一步调整以降低召回率并提高精确率（如果需要）。
- en: 'We can therefore conclude that we got fairly good results from our simple logistic
    regression model, XGBoost, and the deep neural network model. The differences
    between all three models were relatively slight. This might bring important questions
    into your mind: Is it worth iterating for various models on the same use case?
    Which model will ideally give the best results? Though there are no straightforward
    answers to these questions, we can say that, overall, simple models always do
    great; ensemble models perform better with lots of data; and deep learning models
    perform better with a ton of data. In the use case that we experimented with in
    this chapter, we will get improved results from all the models with hyperparameter
    tuning and; most importantly; feature engineering. We will explore hyperparameter
    tuning in *Chapter 7*, *Model Improvements*, and feature engineering on a light
    node in *Chapter 6*, *Feature Selection and Dimensionality Reduction*. The process
    of feature engineering is very domain-specific and can only be generalized to
    a certain extent. We will have a look at this in more detail in the next chapter.
    The primary agenda for this chapter was to introduce the range of modeling techniques
    that cover a substantial area in the field and can help you build the foundations
    for any machine learning technique to be developed for a classification use case.'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以得出结论，我们从简单的逻辑回归模型、XGBoost和深度神经网络模型中得到了相当好的结果。这三个模型之间的差异相对较小。这可能会让你产生重要的问题：在相同的使用案例上迭代各种模型是否值得？哪种模型理论上会给出最佳结果？尽管对这些问题的答案并不直接，但我们可以这样说，总的来说，简单模型总是表现良好；集成模型在大量数据的情况下表现更佳；而深度学习模型在大量数据的情况下表现更佳。在本章中，我们实验的使用案例中，通过超参数调整和；最重要的是；特征工程，我们将从所有模型中获得改进的结果。我们将在第7章“模型改进”中探讨超参数调整，在第6章“特征选择和降维”中探讨轻节点上的特征工程。特征工程的过程非常特定于领域，只能在一定程度上进行概括。我们将在下一章中更详细地探讨这一点。本章的主要议程是介绍涵盖该领域大量建模技术的范围，并帮助您为任何用于分类用例的机器学习技术打下基础。
- en: Choosing the Right Model for Your Use Case
  id: totrans-539
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为您的用例选择正确的模型
- en: So far, we have explored a set of white-box models and a couple of black-box
    machine learning models for the same classification use case. We also extended
    the same use case with a deep neural network in Keras and studied its performance.
    With the results from several models and various iterations, we need to decide
    which model would be the best for a classification use case. There isn't a simple
    and straightforward answer to this. In a more general sense, we can say that the
    best model would be a Random Forest or XGBoost for most use cases. However, this
    is not true for all types of data. There will be numerous scenarios where ensemble
    modeling may not be the right fit and a linear model would outperform it and vice
    versa. In most experiments conducted by data scientists for classification use
    cases, the approach would be an exploratory and iterative one. There is no one-size-fits-all
    model in machine learning. The process of designing and training a machine learning
    model is arduous and extremely iterative and will always depend on the type of
    data used to train it.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探索了一组白盒模型和几个用于相同分类用例的黑盒机器学习模型。我们还用 Keras 扩展了相同的用例，并研究了其性能。从几个模型和多次迭代的结果来看，我们需要决定哪个模型最适合分类用例。对此并没有简单直接的答案。在更广泛的意义上，我们可以认为对于大多数用例，最佳模型将是随机森林或XGBoost。然而，这并不适用于所有类型的数据。会有许多场景，集成建模可能并不合适，线性模型会优于它，反之亦然。在数据科学家为分类用例进行的多数实验中，方法将是探索性和迭代的。在机器学习中没有一种适合所有情况的模型。设计和训练机器学习模型的过程是艰巨且极其迭代的，并且始终取决于用于训练的数据类型。
- en: 'The best approach to proceed, given the task of building a supervised machine
    learning model, would be as follows:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定构建监督机器学习模型的任务的情况下，最佳的前进方法如下：
- en: '**Step 0**: **EDA, Data Treatment and Feature Engineering**: Study the data
    extensively using a combination of visualization techniques and then treat the
    data for missing values, remove outliers, engineer new features, and build the
    train and test datasets. (If necessary, create a validation dataset too.)'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 0**: **探索性数据分析、数据处理和特征工程**：使用可视化技术的组合对数据进行深入研究，然后处理缺失值、去除异常值、构建新特征，并建立训练集和测试集。（如果需要，也可以创建一个验证集。）'
- en: '**Step 1**: **Start with a simple white-box model such as logistic regression**:
    The best starting point in the modeling iterations is a simple white-box model
    that helps us study the impact of each predictor on the dependent variable in
    an easy-to-quantify way. A couple of model iterations will help with feature selection
    and getting a clear understanding of the best predictors and a model benchmark.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 1**: **从简单的白盒模型如逻辑回归开始**：在建模迭代中，最佳起点是一个简单的白盒模型，它可以帮助我们以易于量化的方式研究每个预测变量对因变量的影响。几次模型迭代将有助于特征选择，并清晰地理解最佳预测变量和模型基准。'
- en: '**Step 2**: **Repeat the modeling experiments with a decision tree model**:
    Leveraging decision tree models will always help us get a new perspective on the
    model and feature patterns. It might give us simple rules and thereby new ideas
    to engineer features for an improved model.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 2**: **使用决策树模型重复建模实验**：利用决策树模型将始终帮助我们获得对模型和特征模式的全新视角。它可能会给出简单的规则，从而为改进模型提供新的特征工程思路。'
- en: '**Step 3**: If there is enough data, experiment with ensemble modeling; otherwise,
    try alternative approaches, such as support vector machines.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 3**: 如果有足够的数据，尝试集成建模；否则，尝试其他方法，例如支持向量机。'
- en: 'Ensemble modeling with Random Forest and XGBoost is almost always a safe option
    to experiment with. But in cases where there is a scarcity of data to train, ensemble
    modeling might not be an effective approach to proceed. In such cases, a black
    box kernel-based model would be more effective at learning data patterns and,
    thus, would improve model performance. We have not covered **Support Vector Machines**
    (**SVM**) in this chapter, given the scope. However, with the wide range of topics
    covered in the chapter, getting started with SVMs would be a straightforward task
    for you. This blog provides a simple and easy to understand guide to SVMs: https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/.'
  id: totrans-546
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用随机森林和XGBoost进行集成建模几乎总是实验的一个安全选项。但在数据稀缺的情况下进行训练时，集成建模可能不是一种有效的推进方法。在这种情况下，基于黑盒核的模型在学习和数据模式方面可能更有效，从而提高模型性能。鉴于范围，我们没有在本章中涵盖**支持向量机**（**SVM**）。然而，考虑到本章涵盖的广泛主题，对于您来说，开始使用SVM将是一个简单直接的任务。本博客提供了一个简单易懂的SVM指南：https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/.
- en: Additionally, to understand whether the number of training samples is less or
    more, you can use a simple rule of thumb. If there are at least 100 rows of training
    samples for every feature in the dataset, then there is enough data for ensemble
    models; if the number of samples is lower than that, then ensemble models might
    not always be effective. It is still worth a try, though. For example, if there
    are 15 features (independent variables) and 1 dependent variable, and then if
    we have *15 x 100 = 1500* training samples, ensemble models might have better
    performance on a white-box model.
  id: totrans-547
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，为了了解训练样本的数量是更多还是更少，你可以使用一个简单的经验法则。如果数据集中每个特征至少有100个训练样本行，那么对于集成模型来说，数据就足够了；如果样本数量低于那个水平，那么集成模型可能并不总是有效的。尽管如此，尝试一下仍然值得。例如，如果有15个特征（自变量）和1个因变量，那么如果我们有*15
    x 100 = 1500*个训练样本，集成模型在白盒模型上可能会有更好的性能。
- en: '**Step 4**: If there is more than enough data, try deep neural networks. If
    there are at least 10,000 samples for every feature in the dataset, experimenting
    with deep neural networks might be a good idea. The problem with neural networks
    is mainly the huge training data and large number of iterations required to get
    good performance. In most generic cases for classification using tabular cross-sectional
    data (the type of use case we solved in this book), deep neural networks are just
    as effective as ensemble models but require significantly more effort in training
    and tuning to achieve the same results. They do outperform ensemble models when
    there is a significantly large number of samples to train. Investing the effort
    in deep neural networks only returns favorable results when there is a significantly
    higher number of training samples.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤4**：如果数据量足够多，尝试深度神经网络。如果数据集中每个特征至少有10,000个样本，那么尝试深度神经网络可能是个好主意。神经网络的问题主要是需要大量的训练数据和大量的迭代才能获得良好的性能。在大多数通用情况下，对于使用表格横截面数据进行分类（本书中我们解决的那种用例），深度神经网络与集成模型一样有效，但需要显著更多的努力来训练和调整以达到相同的结果。当有显著大量的样本进行训练时，它们确实优于集成模型。只有在有显著更多的训练样本时，在深度神经网络上的努力才会带来有利的结果。'
- en: Summary
  id: totrans-549
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored different types of classification algorithms for
    supervised machine learning. We leveraged the Australian weather data, designed
    a business problem around it, and explored various machine learning techniques
    on the same use case. We studied how to develop these models in R and studied
    the functioning of these algorithms in depth with mathematical abstractions. We
    summarized the results from each technique and studied a generalized approach
    to tackle common classification use cases.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了监督机器学习中不同类型的分类算法。我们利用澳大利亚天气数据，围绕它设计了一个商业问题，并探索了同一用例上的各种机器学习技术。我们研究了如何在R中开发这些模型，并深入研究了这些算法的数学抽象功能。我们总结了每种技术的结果，并研究了处理常见分类用例的通用方法。
- en: In the next chapter, we will study feature selection, dimensionality reduction,
    and feature engineering for machine learning models.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究机器学习模型的特征选择、降维和特征工程。
