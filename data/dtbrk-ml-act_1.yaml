- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Getting Started with This Book and Lakehouse Concepts
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用本书和湖仓概念
- en: “Give me six hours to chop down a tree, and I will spend the first four sharpening
    the axe.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “给我六个小时砍倒一棵树，我会花前四个小时磨斧头。”
- en: – Abraham Lincoln
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: – 亚伯拉罕·林肯
- en: We will start with a basic overview of how **Databrick**’s **Data Intelligence
    Platform** (**DI**) is an open platform on a **lakehouse** architecture and the
    advantages of this in developing **machine learning** (**ML**) applications. For
    brevity, we will use terms such as *Data Intelligence Platform* and *Databricks*
    interchangeably throughout the book. This chapter will introduce the different
    projects and associated datasets we’ll use throughout the book. Each project intentionally
    highlights a function or component of the DI Platform. Use the example projects
    as hands-on lessons for each platform element we cover. We progress through these
    projects in the last section of each chapter – namely, applying our learning.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个基本的概述开始，介绍 Databricks 的**数据智能平台**（**DI**）是一个基于**湖仓**架构的开放平台，以及这种架构在开发**机器学习**（**ML**）应用中的优势。为了简洁起见，本书中将交替使用“数据智能平台”和“Databricks”这两个术语。本章将介绍本书中我们将使用的不同项目和关联数据集。每个项目都有意突出
    DI 平台的一个功能或组件。请将示例项目作为每个平台元素的实际操作课程。我们将在每个章节的最后部分通过这些项目进行学习——即应用我们的学习。
- en: 'Here is what you will learn in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习以下内容：
- en: The components of the Data Intelligence Platform
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据智能平台组件
- en: Advantages of the Databricks Platform
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks 平台的优势
- en: Applying our learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用我们的学习
- en: The components of the Data Intelligence Platform
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据智能平台组件
- en: 'The Data Intelligence Platform allows your entire organization to leverage
    your data and AI. It’s built on a lakehouse architecture to provide an open, unified
    foundation for all data and governance layers. It is powered by a **Data Intelligence
    Engine**, which understands the context of your data. For practical purposes,
    let’s talk about the components of the Databricks Data Intelligence Platform:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据智能平台允许您的整个组织利用数据和 AI。它基于湖仓架构，为所有数据和管理层提供一个开放、统一的基石。它由一个**数据智能引擎**提供动力，该引擎理解您数据的环境。为了实际应用，让我们讨论
    Databricks 数据智能平台的组件：
- en: '![Figure 1.1 – The components of the Databricks Data Intelligence Platform](img/B16865_01_1.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – Databricks 数据智能平台组件](img/B16865_01_1.jpg)'
- en: Figure 1.1 – The components of the Databricks Data Intelligence Platform
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – Databricks 数据智能平台组件
- en: 'Let’s check out the following list with the descriptions of the items in the
    figure:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看以下列表，其中包含图中的项目描述：
- en: '**Delta Lake**: The data layout within the Data Intelligence Platform is automatically
    optimized based on common data usage patterns'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Delta Lake**：数据智能平台内的数据布局会根据常见的数据使用模式自动优化'
- en: '**Unity Catalog**: A unified governance model to secure, manage, and share
    your data assets'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Unity Catalog**：一个统一的管理模型，用于保护、管理和共享您的数据资产'
- en: '**Data Intelligence Engine**: This uses AI to enhance the platform’s capabilities'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据智能引擎**：该引擎使用 AI 来增强平台的功能'
- en: '**Databricks AI**: ML tools to support end-to-end ML solutions and **generative
    AI** capabilities, including creating, tuning, and serving LLMs'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Databricks AI**: 支持端到端机器学习解决方案和**生成式 AI**功能的机器学习工具，包括创建、调整和提供大型语言模型 (LLM)'
- en: '**Delta live tables**: Enables automated data ingestion and **data quality**'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Delta live tables**：使自动数据摄取和**数据质量**成为可能'
- en: '**Workflows**: A fully integrated orchestration service to automate, manage,
    and monitor multi-task workloads, queries, and pipelines'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作流**：一个完全集成的编排服务，用于自动化、管理和监控多任务工作负载、查询和管道'
- en: '**Databricks SQL (DBSQL)**: An SQL-first interface, similar to how you would
    interact with a data warehouse, and with functionality such as text-to-SQL, which
    lets you use natural language to generate queries'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Databricks SQL (DBSQL)**: 一种以 SQL 为首的界面，类似于您与数据仓库交互的方式，并具有文本到 SQL 等功能，允许您使用自然语言生成查询'
- en: Now that we have our elements defined, let’s discuss how they help us achieve
    our ML goals.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的元素，让我们讨论它们如何帮助我们实现我们的机器学习目标。
- en: The advantages of the Databricks Platform
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Databricks 平台的优势
- en: Databricks’ implementation of a lakehouse architecture is unique. Databricks’
    foundation is built on a Delta-formatted data lake that Unity Catalog governs.
    Therefore, it combines a data lake’s scalability and cost-effectiveness with a
    data warehouse’s governance. This means not only are table-level permissions managed
    through **access control lists** (**ACLs**) but file and object-level access are
    also regulated. This change in architecture from a data lake and/or a data warehouse
    to a unified platform is ideal – a lakehouse facilitates a wide range of new use
    cases for analytics, business intelligence, and data science projects across an
    organization. See the *Introduction to Data Lakes* blog post in the *Further reading*
    section for more information on lakehouse benefits.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 对湖屋架构的实施是独特的。Databricks 的基础建立在由 Unity Catalog 管理的 Delta 格式数据湖上。因此，它结合了数据湖的可扩展性和成本效益以及数据仓库的治理。这意味着不仅通过
    **访问控制列表**（**ACLs**）管理表级权限，而且文件和对象级访问也得到了规范。这种从数据湖和/或数据仓库到统一平台的架构变化是理想的——湖屋促进了组织在分析、商业智能和数据分析项目中的各种新用例。有关湖屋优势的更多信息，请参阅
    *进一步阅读* 部分的 *数据湖简介* 博客文章。
- en: This section will discuss the importance of open source frameworks and two critical
    advantages they provide – transparency and flexibility.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将讨论开源框架的重要性以及它们提供的两个关键优势——透明度和灵活性。
- en: Open source features
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开源特性
- en: How open source features relate to the Data Intelligence Platform is unique.
    This uniqueness lies in the concepts of openness and transparency, often referred
    to as the “glass box” approach by Databricks. It means that when you use the platform
    to create assets, there’s no inscrutable black box that forces you to depend on
    a specific vendor for usage, understanding, or storage. A genuinely open lakehouse
    architecture uses open data file formats to make accessing, sharing, and removing
    your data simple. Databricks has optimized the managed version of Apache Spark
    to leverage the open data format Delta (which we’ll cover in more detail shortly).
    This is one of the reasons why the Delta format is ideal for most use cases. However,
    nothing stops you from using something such as the CSV or Parquet format. Furthermore,
    Databricks introduced **Delta Lake Universal Format** (**Delta Lake UniForm**)
    to easily integrate with other file formats such as Iceberg or Hudi. For more
    details, check out the *Further reading* section at the end of this chapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 开源特性与数据智能平台的关系是独特的。这种独特性体现在开放性和透明度的概念上，通常被称为 Databricks 的“玻璃盒”方法。这意味着当你使用该平台创建资产时，没有不可理解的黑盒迫使你依赖特定供应商进行使用、理解或存储。真正开放的湖屋架构使用开放数据文件格式，使访问、共享和删除数据变得简单。Databricks
    对 Apache Spark 的托管版本进行了优化，以利用开放数据格式 Delta（我们将在稍后详细介绍）。这就是为什么 Delta 格式对于大多数用例来说都是理想的。然而，没有任何阻止你使用诸如
    CSV 或 Parquet 格式的东西。此外，Databricks 引入了 **Delta Lake 通用格式**（**Delta Lake UniForm**），以便轻松集成其他文件格式，如
    Iceberg 或 Hudi。有关更多详细信息，请参阅本章末尾的 *进一步阅读* 部分。
- en: '*Figure 1**.2* illustrates the coming together of data formats with UniForm.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1**.2* 展示了数据格式与 UniForm 的结合。'
- en: "![Figure 1.2 – Delta Lake UniForm makes consuming Hudi and Iceberg file formats\
    \ as easy as consumin\uFEFFg Delta](img/B16865_01_2.jpg)"
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2 – Delta Lake UniForm 使消费 Hudi 和 Iceberg 文件格式与消费 Delta 一样简单](img/B16865_01_2.jpg)'
- en: Figure 1.2 – Delta Lake UniForm makes consuming Hudi and Iceberg file formats
    as easy as consuming Delta
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – Delta Lake UniForm 使消费 Hudi 和 Iceberg 文件格式与消费 Delta 一样简单
- en: The ability to use third-party and open source software fuels rapid innovation.
    New advances in data processing and ML can be quickly tested and integrated into
    your workflow. In contrast, proprietary systems often have longer wait times for
    vendors to incorporate updates. Waiting for a vendor to capitalize on open source
    innovation may seem rare, but it is the rule rather than the exception. This is
    especially true for data science. The speed of software and algorithmic advances
    is incredible. Evidence of this frantic pace of innovation can be seen daily on
    the Hugging Face community website. Developers share libraries and models on Hugging
    Face; hundreds of libraries are updated daily on the site alone.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用第三方和开源软件的能力推动了快速创新。数据处理和机器学习的新进展可以迅速测试并集成到您的流程中。相比之下，专有系统通常需要较长的等待时间，以便供应商整合更新。等待供应商利用开源创新似乎很少见，但这是常态而不是例外。这在数据科学领域尤其如此。软件和算法进步的速度令人难以置信。这种疯狂的创新步伐的证据可以在
    Hugging Face 社区网站上每天看到。开发者们在 Hugging Face 上分享库和模型；仅该网站每天就有数百个库更新。
- en: Delta, Spark, the Pandas API on Spark (see *Figure 1**.3*), and MLflow are notable
    examples of consistent innovation, largely driven by their transparency as open
    source projects. We mention these specifically because they were all initially
    created by either the founders of Databricks or company members following its
    formation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Delta、Spark、Spark 上的 Pandas API（见 *图 1.3*）和 MLflow 是一致创新的重要例子，这主要得益于它们作为开源项目的透明度。我们特别提到这些，因为它们最初都是由
    Databricks 的创始人或公司成员在其成立后创建的。
- en: ML developers benefit significantly from this transparency, as it provides them
    with unparalleled flexibility, easy integration, and robust support from the open
    source community – all without the overhead of maintaining an open source full
    stack.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ML 开发者从这种透明度中受益匪浅，因为它为他们提供了无与伦比的灵活性、易于集成以及来自开源社区的强大支持——所有这些都不需要维护开源全栈的开销。
- en: Starting development as a contractor using Databricks is super-fast compared
    to when companies require a fresh development environment to be set up. Some companies
    require a service request to install Python libraries. This can be a productivity
    killer for data scientists. In Databricks, many of your favorite libraries are
    pre-installed and ready to use, and of course, you can easily install your own
    libraries as well.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与公司需要设置全新的开发环境相比，作为承包商使用 Databricks 进行开发速度极快。一些公司需要提交服务请求来安装 Python 库。这可能会成为数据科学家的生产力杀手。在
    Databricks 中，许多您喜欢的库都是预先安装并可供使用的，当然，您也可以轻松地安装自己的库。
- en: Additionally, there is a large and vibrant community of Databricks users. The
    Databricks community website is an excellent resource to ask and answer questions
    about anything related to Databricks. We’ve included a link in the *Further reading*
    section at the end of this chapter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Databricks 用户群体庞大且充满活力。Databricks 社区网站是一个极好的资源，可以询问和回答有关 Databricks 的任何问题。我们在本章末尾的
    *进一步阅读* 部分包含了一个链接。
- en: '![Figure 1.3 – The pandas API on Spark](img/B16865_01_3.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.3 – Spark 上的 pandas API](img/B16865_01_3.jpg)'
- en: Figure 1.3 – The pandas API on Spark
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – Spark 上的 pandas API
- en: The pandas API on Spark is nearly identical syntax to standard pandas, making
    distributed computing with Spark easier to learn for those who have written pandas
    code in Python
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 上的 pandas API 几乎与标准的 pandas 语法相同，这使得对于在 Python 中编写过 pandas 代码的人来说，学习使用
    Spark 进行分布式计算变得更加容易。
- en: While continuing with a focus on transparency, let’s move on to Databricks **AutoML**.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续关注透明度的同时，让我们转向 Databricks **AutoML**。
- en: Databricks AutoML
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Databricks AutoML
- en: Databricks refers to its AutoML solution as a **glass box**. This terminology
    highlights the fact that there is nothing hidden from the user. This feature in
    the Data Intelligence Platform leverages an open source library, Hyperopt, in
    conjunction with Spark for hyperparameter tuning. It intelligently explores different
    model types in addition to optimizing the parameters in a distributed fashion.
    The use of Hyperopt allows each run within the AutoML experiment to inform the
    next run, reducing the overall number of runs needed to reach an optimal solution
    compared to a grid search. Each run in the experiment has an associated notebook
    with the code for the model. This method increases productivity, reduces unnecessary
    computing, and lets scientists perform experiments instead of writing boilerplate
    code. Once AutoML has converged on the algorithmically optimal solution, there
    is a “best notebook” for the best scoring model. We’ll expand on AutoML in several
    chapters throughout this book.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks将其AutoML解决方案称为**玻璃盒**。这个术语突出了这样一个事实：对用户来说没有任何隐藏的东西。在数据智能平台中的这个特性利用了一个开源库Hyperopt，结合Spark进行超参数调整。它不仅智能地探索不同的模型类型，而且在分布式方式中优化参数。Hyperopt的使用使得AutoML实验中的每一次运行都能为下一次运行提供信息，与网格搜索相比，减少了达到最优解所需的运行次数。实验中的每一次运行都关联着一个包含模型代码的笔记本。这种方法提高了生产力，减少了不必要的计算，并让科学家能够进行实验而不是编写样板代码。一旦AutoML收敛到算法最优解，就会有“最佳笔记本”用于最佳评分的模型。本书的几个章节中我们将详细探讨AutoML。
- en: Reusability and reproducibility
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可重用性和可重现性
- en: As data scientists, transparency is especially important. We do not trust black
    box models. How do you use them without understanding them? A model is only as
    good as the data going in. In addition to not trusting the models, black boxes
    create concerns about our research’s reproducibility and model drivers’ explainability.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家，透明度尤为重要。我们不信任黑盒模型。在不理解它们的情况下如何使用它们呢？一个模型的好坏取决于输入的数据。除了不信任模型之外，黑盒还引发了我们对研究可重复性和模型驱动者可解释性的担忧。
- en: When we create a model, who does it belong to? Can we get access to it? Can
    we tweak, test, and, most importantly, reuse it? The amount of time put into the
    model’s creation is not negligible. Databricks AutoML gives you everything to
    explain, reproduce, and reuse the models it creates. In fact, you can take the
    model code or model object and run it on a laptop or wherever. This open source,
    glass-box, reproducible, and reusable methodology is our kind of open.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建一个模型时，它属于谁？我们能访问它吗？我们能调整、测试，最重要的是，重用它吗？投入模型创建的时间是不可忽视的。Databricks AutoML为你提供了解释、重现和重用它创建的模型所需的一切。实际上，你可以将模型代码或模型对象在笔记本电脑或任何地方运行。这种开源、玻璃盒、可重现和可重用的方法论正是我们所倡导的开放。
- en: Open file formats give you flexibility
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开放文件格式提供灵活性
- en: Flexibility is also an essential aspect of the Databricks platform, so let’s
    dive into the file format Delta, an open source project that makes it easy to
    adapt to many different use cases. For those familiar with Parquet, you can think
    of Delta as Parquet-plus – Delta files are Parquet files with a transaction log.
    The transaction log is a game changer. The increased reliability and optimizations
    make Delta the foundation of Databricks’ lakehouse architecture. The data lake
    side of the lakehouse is vital to data science, streaming, and unstructured and
    semi-structured data formats. Delta has also made the warehouse side possible.
    There are entire books on Delta; see the *Further reading* section for some examples.
    We are focusing on the fact that it is an open file format with key features that
    support building data products.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 灵活性也是Databricks平台的一个基本方面，因此让我们深入了解Delta文件格式，这是一个开源项目，它使得适应多种不同的用例变得容易。对于那些熟悉Parquet的人来说，可以将Delta视为Parquet的增强版——Delta文件是带有事务日志的Parquet文件。事务日志是一个变革性的因素。可靠性的提高和优化使得Delta成为Databricks湖屋架构的基础。湖屋的数据湖部分对数据科学、流处理以及非结构化和半结构化数据格式至关重要。Delta也使得仓库部分成为可能。关于Delta有整本书的讨论；在*进一步阅读*部分可以找到一些例子。我们关注的是它是一个开放文件格式，具有支持构建数据产品的关键特性。
- en: Integration and control
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成和控制
- en: Having an open file format is essential to maintain ownership of your data.
    Not only do you want to be able to read, alter, and open your data files, but
    you also want to keep them in your cloud tenant. Maintaining control over your
    data is possible in the Databricks Data Intelligence Platform. There is no need
    to put the data files into a proprietary format or lock them away in a vendor’s
    cloud. Take a look at *Figure 1**.4* to see how Delta is part of the larger ecosystem.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个开放文件格式对于保持对您数据的所有权至关重要。您不仅希望能够读取、修改和打开您的数据文件，还希望将它们保留在您的云租户中。在Databricks数据智能平台上，您可以保持对数据的控制。没有必要将数据文件放入专有格式或将其锁在供应商的云中。查看*图1.4*，了解Delta如何成为更大生态系统的一部分。
- en: '![Figure 1.4 – The Delta Kernel connection ecosystem](img/B16865_01_4.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图1.4 – Delta内核连接生态系统](img/B16865_01_4.jpg)'
- en: Figure 1.4 – The Delta Kernel connection ecosystem
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 – Delta内核连接生态系统
- en: The Delta Kernel introduces a fresh approach, offering streamlined, focused,
    and reliable APIs that abstract away the intricacies of the Delta protocol. By
    simply updating the Kernel version, connector developers can seamlessly access
    the latest Delta features without needing to modify any code.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Delta内核引入了一种全新的方法，提供简化的、专注的、可靠的API，这些API抽象了Delta协议的复杂性。通过简单地更新内核版本，连接器开发者可以无缝访问最新的Delta功能，而无需修改任何代码。
- en: Time-travel versioning in Delta
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Delta中的时间旅行版本控制
- en: The freedom and flexibility of open file formats make it possible to integrate
    with new and existing external tooling. Delta Lake, in particular, offers unique
    support to create data products thanks to features such as time-travel versioning,
    exceptional speed, and the ability to update and merge changes. Time travel, in
    this context, refers to the capability of querying different versions of your
    data table, allowing you to revisit the state of the table before your most recent
    changes or transformations (see *Figure 1**.5*). The more obvious use is to back
    up after making a mistake rather than writing out multiple copies of the table
    as a safety measure. A possibly less obvious use for time travel is reproducible
    research. You can access the data your model was trained on in the previous week
    without creating an additional copy of the data. Throughout the book, we will
    detail features of the Data Intelligence Platform you can use to facilitate reproducible
    research. The following figure shows you how the previous version of a table,
    relative to a timestamp or a version number, can be queried.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 开放文件格式的自由和灵活性使得它能够与新的和现有的外部工具集成。特别是Delta Lake，凭借时间旅行版本控制、卓越的速度以及更新和合并更改的能力等特性，为创建数据产品提供了独特的支持。在此背景下，时间旅行指的是查询您数据表不同版本的能力，让您能够回顾在最近更改或转换之前表的状态（见*图1.5*）。最明显的用途是在犯错后进行备份，而不是作为安全措施写出多个表的副本。时间旅行的一个可能不那么明显的用途是可重复研究。您可以在不创建数据副本的情况下访问模型在上一周训练时所使用的数据。在整个书中，我们将详细介绍您可以使用的数据智能平台的功能，以促进可重复研究。以下图示展示了如何查询相对于时间戳或版本号的表的前一个版本。
- en: "![Figure 1.5 – A code example of the querying techniques to view previous versi\uFEFF\
    ons of a table](img/B16865_01_5.jpg)"
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图1.5 – 查询技术代码示例，用于查看表的前一个版本](img/B16865_01_5.jpg)'
- en: Figure 1.5 – A code example of the querying techniques to view previous versions
    of a table
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 – 查询技术代码示例，用于查看表的前一个版本
- en: The speed of Databricks’ optimized combination
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Databricks优化组合的速度
- en: Next, let us discuss the speed of Databricks’ lakehouse architecture. In November
    2021, Databricks set a new world record for the gold standard performance benchmark
    for data warehousing. The Barcelona Computing Group shared their research supporting
    this finding. This record-breaking speed resulted from the Databricks’ engines
    (Spark and Photon) paired with Delta (see the *Databricks Sets Official Data Warehousing
    Performance Record* link in the *Further* *reading* section).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论Databricks湖屋架构的速度。2021年11月，Databricks为数据仓库的黄金标准性能基准设定了新的世界纪录。巴塞罗那计算小组分享了支持这一发现的研究。这一创纪录的速度得益于Databricks的引擎（Spark和Photon）与Delta的结合（参见*进一步阅读*部分的*Databricks
    Sets Official Data Warehousing Performance Record*链接）。
- en: The additional benefits of Delta
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Delta的额外优势
- en: Delta’s impressive features include **change data feed** (**CDF**), **change
    data capture** (**CDC**), and **schema evolution**. Each plays a specific role
    in data transformation in support of ML.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Delta的令人印象深刻的特性包括**变更数据馈送**（CDF）、**变更数据捕获**（CDC）和**模式演变**。每个都在支持机器学习的数据转换中扮演着特定的角色。
- en: Starting with Delta’s CDF capability, it is exactly what it sounds like – a
    feed of the changed data. Let’s say you have a model looking for fraud, and that
    model needs to know how many transaction requests have occurred in the last 10
    minutes. It is not feasible to rewrite the entire table each time a value for
    an account needs to be updated. The feature value, or in this case, the number
    of transactions that occurred in the last 10 minutes, needs to be updated only
    when the value has changed. The use of CDF in this example enables updates to
    be passed to an **online feature store**; see *Chapters 5* and *6* for more details.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从Delta的CDF（Change Data Feed，变更数据馈送）功能开始，它正是其名称所暗示的——变化数据的馈送。假设你有一个寻找欺诈行为的模型，该模型需要知道在过去10分钟内发生了多少交易请求。每次需要更新账户的值时，重写整个表是不切实际的。在这种情况下，特征值，即过去10分钟内发生的交易数量，只有在值发生变化时才需要更新。本例中CDF的使用使得更新可以传递给一个**在线特征存储**；详见*第
    5 章*和*第 6 章*以获取更多详细信息。
- en: 'Finally, let’s talk about change data capture, a game-changer in the world
    of data management. Unlike traditional filesystems, CDC in Delta has been purposefully
    designed to handle data updates efficiently. Let’s take a closer look at CDC and
    explore its capabilities through two practical scenarios:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们谈谈变更数据捕获（CDC），它是数据管理领域的一个颠覆者。与传统的文件系统不同，Delta中的CDC被有意设计来高效地处理数据更新。让我们更深入地了解CDC，并通过两个实际场景来探索其功能：
- en: '**Scenario 1 – effortless record updates**: Picture a scenario involving Rami,
    one of your customers. He initially made a purchase in Wisconsin but later relocated
    to Colorado, where he continued to make purchases. In your records, it’s essential
    to reflect Rami’s new address in Colorado. Here’s where Delta’s CDC shines. It
    effortlessly updates Rami’s customer record without treating him as a new customer.
    CDC excels at capturing and applying updates seamlessly, ensuring data integrity
    without any hassles.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**场景 1 – 无需努力的记录更新**：想象一个涉及你的客户拉米（Rami）的场景。他最初在威斯康星州购买商品，但后来搬到了科罗拉多州，并在那里继续购买。在你的记录中，反映拉米在科罗拉多州的新地址至关重要。这正是Delta的CDC（Change
    Data Capture，变更数据捕获）大放异彩的地方。它无需将拉米视为新客户，就能轻松更新他的客户记录。CDC擅长无缝地捕获和应用更新，确保数据完整性而无需任何麻烦。'
- en: '**Scenario 2 – adapting to evolving data sources**: Now, consider a situation
    where your data source experiences unexpected changes, resulting in adding a new
    column containing information about your customers. Let’s say this new column
    provides insights into the colors of items purchased by customers. This is valuable
    data that you wouldn’t want to lose. Delta’s CDC, combined with its schema evolution
    feature, comes to the rescue.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**场景 2 – 适应不断变化的数据源**：现在，考虑一种情况，你的数据源经历意外变化，导致添加了一个包含客户信息的新列。假设这个新列提供了关于客户购买物品颜色的洞察。这是你不想丢失的有价值数据。Delta的CDC结合其模式演变功能，正是解救之道。'
- en: Schema evolution, explored in depth in [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123),
    enables Delta to gracefully adapt to schema changes without causing any disruptions.
    When dealing with a new data column, Delta smoothly incorporates this information,
    ensuring your data remains up to date while retaining its full historical context.
    This ensures that you can leverage valuable insights for both present and future
    analyses.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 3 章*](B16865_03.xhtml#_idTextAnchor123)中深入探讨了模式演变，它使Delta能够优雅地适应模式变化，而不会造成任何中断。当处理新的数据列时，Delta能够顺利地整合这些信息，确保你的数据保持最新，同时保留其全部历史背景。这确保了你可以为现在和未来的分析利用有价值的见解。
- en: Applying our learning
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用我们的学习方法
- en: This book is heavily project-based. Each chapter starts with an overview of
    the important concepts and Data Intelligence Platform features that will prepare
    you for the main event – the *Applying our learning* sections. Every *Applying
    our learning* section has a *Technical requirements* section so that you know
    what technical resources you will need, in addition to your Databricks workspace
    and GitHub repository, to complete the project work in the respective chapter.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书以项目为基础。每一章都从概述重要概念和数据智能平台功能开始，为你准备主要活动——*应用我们的学习*部分。每个*应用我们的学习*部分都有一个*技术要求*部分，这样你知道完成相应章节的项目工作需要哪些技术资源，除了你的Databricks工作空间和GitHub仓库。
- en: Technical requirements
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Here are the technical requirements needed to get started with the hands-on
    examples used throughout this book:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是需要开始使用本书中使用的动手示例所需的技术要求：
- en: We use Kaggle for two of our datasets. If you do not already have an account,
    you will need to create one.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用Kaggle的两个数据集。如果你还没有账户，你需要创建一个。
- en: Throughout the book, we will refer to code in GitHub. Create an account if you
    do not already have one.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在整本书中，我们将参考GitHub中的代码。如果你还没有账户，请创建一个。
- en: Getting to know your data
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解你的数据
- en: There are four main projects that progress sequentially throughout the book.
    In each subsequent chapter, the code will expand upon the code in previous chapters.
    We chose these projects to highlight a variety of Data Intelligence Platform features
    across different ML projects. Specifically, we include streaming data into your
    lakehouse architecture, forecasting sales, building a **deep learning** (**DL**)
    model for computer vision, and building a chatbot using **Retrieval Augmented
    Generation** (**RAG**) techniques. Read through the descriptions of each project
    to get an idea of what it will cover. If some of the concepts and features are
    unfamiliar, don’t worry! We’ll explain them in the following chapters.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 书中包含四个主要项目，这些项目按顺序逐步展开。在每一章后续的章节中，代码将扩展前一章的代码。我们选择这些项目是为了突出不同机器学习项目中的各种数据智能平台功能。具体来说，我们包括将流数据引入你的湖仓架构、预测销售、构建用于计算机视觉的**深度学习**（**DL**）模型，以及使用**检索增强生成**（**RAG**）技术构建聊天机器人。阅读每个项目的描述，以了解它将涵盖的内容。如果一些概念和功能不熟悉，不要担心！我们将在接下来的章节中解释它们。
- en: Project – streaming transactions
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目 – 流式交易
- en: This first project is a data solution for the streaming transactions dataset
    we will generate. The transaction data will include information such as the customer
    ID and transaction time, which we’ll use to simulate transactions streaming in
    real time; see the sample data in *Figure 1**.6*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个第一个项目是为我们将生成的流式交易数据集提供的数据解决方案。交易数据将包括客户ID和交易时间等信息，我们将使用这些信息来模拟实时流式交易；请参阅*图1.6*中的示例数据。
- en: '![Figure 1.6 – A sample of the synthetic streaming transactions data](img/B16865_01_6.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图1.6 – 合成流式交易数据的示例](img/B16865_01_6.jpg)'
- en: Figure 1.6 – A sample of the synthetic streaming transactions data
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 – 合成流式交易数据的示例
- en: Our goal with this project is to demonstrate how flexible the Data Intelligence
    Platform is compared to proprietary data warehouses of the past, which were more
    rigid for data ingestion. Additionally, we want to highlight important Databricks
    capabilities such as **Spark Structured Streaming**, **Auto Loader**, schema evolution,
    Delta Live Tables, and **Lakehouse Monitoring**.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望通过这个项目展示数据智能平台与过去专有数据仓库相比的灵活性，后者在数据摄取方面更为僵化。此外，我们还想突出Databricks的重要功能，如**Spark
    Structured Streaming**、**Auto Loader**、模式演变、Delta Live Tables和**湖仓监控**。
- en: When we generate the transactions, we also generate a label based on statistical
    distributions (note that the label is random and only used for learning purposes).
    This is the label we will be predicting. Our journey includes generating transaction
    records as multiline JSON files, formatting the files to a Delta table, creating
    a streaming feature for our ML model, wrapping a `pyfunc`) with the preprocessing
    steps, and deploying the model wrapper via a workflow. Take a look through the
    project pipeline in *Figure 1**.7* to understand how we’ll progress through this
    project.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们生成交易时，我们也会根据统计分布生成一个标签（注意，标签是随机的，仅用于学习目的）。这是我们将会预测的标签。我们的旅程包括生成多行 JSON 文件的交易记录，将文件格式化为
    Delta 表，为我们的机器学习模型创建流式特征，使用 `pyfunc` 包裹预处理步骤，并通过工作流部署模型包装器。通过查看 *图 1.7* 了解我们将如何推进这个项目。
- en: '![Figure 1.7 – The project pipeline for the streaming transactions project'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.7 – 流式事务项目的项目流程'
- en: '](img/B16865_01_7.jpg)![Figure 1.7 – The project pipeline for the streaming
    transactions project'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_01_7.jpg)![图 1.7 – 流式事务项目的项目流程'
- en: '](img/B16865_01_8.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_01_8.jpg)'
- en: Figure 1.7 – The project pipeline for the streaming transactions project
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 – 流式事务项目的项目流程
- en: That concludes the streaming transaction project explanation. Next, we will
    look at the forecasting project.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了流式事务项目的解释。接下来，我们将查看预测项目。
- en: Project – Favorita sales forecasting
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目 – Favorita 销售预测
- en: This is a typical forecasting project. Our dataset is hosted on the Kaggle website
    (see the *Further reading* section). We will use the data to build a model to
    predict the total sales amount for a family of goods at a specific Favorita store
    in Ecuador. The data includes train, test, and supplementary data. This project
    will use Databricks’ AutoML for data exploration and to create a baseline model.
    Take a look through the project pipeline in *Figure 1**.8* to understand how we’ll
    progress through this project. The store sales dataset is a rich time-series dataset,
    and we encourage you to build on the project framework we provide using your favorite
    time-series library.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个典型的预测项目。我们的数据集托管在 Kaggle 网站上（参见 *进一步阅读* 部分）。我们将使用这些数据构建一个模型，以预测厄瓜多尔特定 Favorita
    店面一家商品的总销售额。数据包括训练数据、测试数据和补充数据。本项目将使用 Databricks 的 AutoML 进行数据探索并创建基线模型。通过查看 *图
    1.8* 了解我们将如何推进这个项目。店面销售数据集是一个丰富的时序数据集，我们鼓励您使用您喜欢的时序库在我们的项目框架上构建。
- en: '![Figure 1.8 – The project pipeline for the Favorita store sales project'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.8 – Favorita 店面销售项目的项目流程'
- en: '](img/B16865_01_9.jpg)![Figure 1.8 – The project pipeline for the Favorita
    store sales project'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_01_9.jpg)![图 1.8 – Favorita 店面销售项目的项目流程'
- en: '](img/B16865_01_10.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_01_10.jpg)'
- en: Figure 1.8 – The project pipeline for the Favorita store sales project
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 – Favorita 店面销售项目的项目流程
- en: That concludes the forecasting project explanation. Next, we will look at the
    DL project.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了预测项目的解释。接下来，我们将查看深度学习项目。
- en: Project – multilabel image classification
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目 – 多标签图像分类
- en: This project is a DL data solution that uses another Kaggle dataset. We will
    use these datasets images to fine-tune a deep-learning model, using PyTorch and
    Lightning to predict a corresponding label. We will implement MLflow code for
    experiment and model tracking, Spark for fast training and **inference**, and
    Delta for data version control. We will deploy the model as we would for a real-time
    scenario by creating a model wrapper, similar to the wrapper we use for the streaming
    transactions project. Take a look through the project pipeline in *Figure 1**.9*
    to understand how we’ll progress through this project.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个使用另一个 Kaggle 数据集的深度学习数据解决方案。我们将使用这些数据集的图像微调一个深度学习模型，使用 PyTorch 和 Lightning
    预测相应的标签。我们将实现 MLflow 代码进行实验和模型跟踪，使用 Spark 进行快速训练和 **推理**，以及使用 Delta 进行数据版本控制。我们将通过创建一个模型包装器来部署模型，类似于我们在流式事务项目中使用的包装器。通过查看
    *图 1.9* 了解我们将如何推进这个项目。
- en: '![Figure 1.9 – The project pipeline for the multilabel image classification
    project'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.9 – 多标签图像分类项目的项目流程'
- en: '](img/B16865_01_11.jpg)![Figure 1.9 – The project pipeline for the multilabel
    image classification project'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_01_11.jpg)![图 1.9 – 多标签图像分类项目的项目流程'
- en: '](img/B16865_01_12.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_01_12.jpg)'
- en: Figure 1.9 – The project pipeline for the multilabel image classification project
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9 – 多标签图像分类项目的项目流程
- en: That concludes the image classification project explanation. Next, we will look
    at the chatbot project.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了图像分类项目的解释。接下来，我们将探讨聊天机器人项目。
- en: Project – a retrieval augmented generation chatbot
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目 – 检索增强生成聊天机器人
- en: This project is a RAG chatbot. The dataset we use comes from the *arXiv* website.
    We have selected a few research articles about the impact of generative AI on
    humans and labor. We will download and store them in a volume in [*Chapter 2*](B16865_02.xhtml#_idTextAnchor073).
    After we download the PDF documents, we will extract and prepare the text through
    a process of chunking and tokenization, creating embeddings of the documents to
    be referenced in the chatbot. We will use Databricks Vector Search to store the
    embeddings. Then, we will use the new Foundation Model API to generate answers
    when text is retrieved. The final bot will be deployed as an application using
    **Databricks Model Serving**. This example allows you to build a chatbot from
    start to finish! Take a look through the project pipeline in *Figure 1**.10* to
    understand how we’ll progress through this project.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目是一个RAG聊天机器人。我们使用的数据集来自*arXiv*网站。我们选择了一些关于生成式人工智能对人类和劳动影响的研究文章。我们将下载并将它们存储在[*第2章*](B16865_02.xhtml#_idTextAnchor073)中的一个卷中。下载PDF文档后，我们将通过分块和标记化过程提取和准备文本，为聊天机器人创建要引用的文档嵌入。我们将使用Databricks向量搜索来存储嵌入。然后，我们将使用新的基础模型API在检索文本时生成答案。最终的机器人将作为使用**Databricks模型服务**的应用程序部署。这个示例允许你从头到尾构建一个聊天机器人！通过查看*图
    1.10*中的项目流程来了解我们将如何进行这个项目。
- en: '![Figure 1.10 – The project pipeline for the chatbot project'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.10 – 聊天机器人项目的项目流程'
- en: '](img/B16865_01_13.jpg)![Figure 1.10 – The project pipeline for the chatbot
    project'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.10 – 聊天机器人项目的项目流程](img/B16865_01_13.jpg)'
- en: '](img/B16865_01_14.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.14 – 项目流程图](img/B16865_01_14.jpg)'
- en: Figure 1.10 – The project pipeline for the chatbot project
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10 – 聊天机器人项目的项目流程
- en: The four projects are hands-on examples that can be implemented on Databricks.
    *Databricks ML in Action* provides best practices and recommendations from an
    ML perspective based on our experiences and supplements online documentation.
    All the code and solutions presented in this book have been developed and tested
    on the full version of Databricks. However, we understand that accessibility matters.
    There is also a free community version of the Databricks Data Intelligence Platform
    available, enabling everyone to follow along with the examples to a certain point
    before considering an upgrade.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 四个项目是可以在Databricks上实施的动手示例。*《Databricks机器学习实战》*基于我们的经验，从机器学习的角度提供了最佳实践和建议，并补充了在线文档。本书中展示的所有代码和解决方案都是在Databricks的全版本上开发和测试的。然而，我们理解可访问性很重要。Databricks数据智能平台还有一个免费的社区版本，使每个人都能在考虑升级之前跟随示例进行到一定程度。
- en: Summary
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced you to *Databricks ML in Action*. We emphasized
    that the Databricks Data Intelligence Platform is designed with openness, flexibility,
    and tooling freedom in mind, which greatly accelerates productivity. Additionally,
    we’ve given you a sneak peek at the projects and the associated datasets that
    will be central to this book.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们向您介绍了*《Databricks机器学习实战》*。我们强调，Databricks数据智能平台的设计考虑了开放性、灵活性和工具自由度，这极大地提高了生产力。此外，我们还向您展示了本书中将占中心地位的项目和相关数据集。
- en: Now that you’ve gained a foundational understanding of the Data Intelligence
    Platform, it’s time to take the next step. In the upcoming chapter, we’ll guide
    you through setting up your environment and provide instructions on downloading
    the project data. This will prepare you for the practical, hands-on ML experiences
    that lie ahead in this journey.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对数据智能平台有了基础的了解，是时候迈出下一步了。在接下来的章节中，我们将引导你设置环境，并提供下载项目数据的说明。这将为你准备在这个旅程中即将到来的实际、动手的机器学习体验。
- en: Questions
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let’s test ourselves on what we’ve learned by going through the following questions:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下问题来测试一下我们学到了什么：
- en: How will you use this book? Do you plan to go cover to cover or pick certain
    sections out? Have you chosen sections of interest?
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将如何使用这本书？你计划从头到尾阅读，还是挑选某些章节？你已经选择了感兴趣的章节吗？
- en: We covered why transparency in modeling is critical to success. How does Databricks’
    glass-box approach to AutoML support this?
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们讨论了为什么在建模中保持透明度对于成功至关重要。Databricks的自动机器学习（AutoML）的玻璃盒方法是如何支持这一点的？
- en: Databricks has developed a new way of uniting the open data formats, called
    UniForm. Which data formats does UniForm unite?
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Databricks 开发了一种新的方式来统一开放数据格式，称为 UniForm。UniForm 统一了哪些数据格式？
- en: Delta is the foundation of the lakehouse architecture. What is one of the benefits
    of using Delta?
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Delta 是湖屋架构的基础。使用 Delta 的一个好处是什么？
- en: What is the main advantage of using the Delta file format for large-scale data
    processing over simple Parquet files in Databricks?
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Databricks 中，使用 Delta 文件格式进行大规模数据处理相比简单的 Parquet 文件有哪些主要优势？
- en: Answers
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案
- en: 'After putting thought into the questions, compare your answers to ours:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在思考了这些问题之后，比较一下您的答案和我们的答案：
- en: We cannot answer this question, but we hope you learn something you can use
    in your career soon!
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们无法回答这个问题，但我们希望您很快就能学到在工作中可以用到的东西！
- en: The glass-box approach supports transparency by providing the code run for each
    run in the experiment and the best run, thus enabling reusability and reproducibility.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 玻璃盒方法通过提供实验中每次运行的代码和最佳运行，支持透明度，从而实现可重用性和可重复性。
- en: Apache Iceberg, Apache Hudi, and Linux Foundation Delta Lake (an open source/unmanaged
    version of Delta).
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Apache Iceberg、Apache Hudi 和 Linux Foundation Delta Lake（Delta 的开源/未管理版本）。
- en: 'There are several. Here are a few:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有几个。以下是一些：
- en: Open protocol (no vendor lock-in)
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开放协议（无供应商锁定）
- en: Speed
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 速度
- en: Change data capture
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据变更捕获
- en: Time travel
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间旅行
- en: While Parquet also provides columnar storage and has efficient read/write operations,
    its lack of ACID transaction capabilities distinguishes Delta Lake.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然 Parquet 也提供列式存储并具有高效的读写操作，但其缺乏 ACID 事务能力，这使得 Delta Lake 与之区分开来。
- en: Further reading
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'In this chapter, we introduced vital technologies. Look at these resources
    to go deeper into the areas that interest you most:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了关键技术。查看这些资源深入了解您最感兴趣的领域：
- en: 'YouTube video – *Introduction to Databricks Data Intelligence* *Platform*:
    [https://youtu.be/E885Ld3N2As?si=1NPg85phVH8RhayO](https://youtu.be/E885Ld3N2As?si=1NPg85phVH8RhayO)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YouTube 视频 - *Databricks 数据智能平台入门*：[https://youtu.be/E885Ld3N2As?si=1NPg85phVH8RhayO](https://youtu.be/E885Ld3N2As?si=1NPg85phVH8RhayO)
- en: '*Introduction to Data* *Lakes*: [https://www.databricks.com/discover/data-lakes](https://www.databricks.com/discover/data-lakes)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据湖入门*：[https://www.databricks.com/discover/data-lakes](https://www.databricks.com/discover/data-lakes)'
- en: '*5 Steps to a Successful Data Lakehouse* by Bill Inmon, father of the data
    warehouse: [https://www.databricks.com/resources/ebook/building-the-data-lakehouse](https://www.databricks.com/resources/ebook/building-the-data-lakehouse)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据仓库之父比尔·英蒙（Bill Inmon）的*5 步成功构建数据湖屋*：[https://www.databricks.com/resources/ebook/building-the-data-lakehouse](https://www.databricks.com/resources/ebook/building-the-data-lakehouse)
- en: '*Delta Lake: Up & Running* by O’Reilly: [https://www.databricks.com/resources/ebook/delta-lake-running-oreilly](https://www.databricks.com/resources/ebook/delta-lake-running-oreilly)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Delta Lake: Up & Running* by O’Reilly：[https://www.databricks.com/resources/ebook/delta-lake-running-oreilly](https://www.databricks.com/resources/ebook/delta-lake-running-oreilly)'
- en: '*Delta Lake: The Definitive* *Guide*: [https://www.oreilly.com/library/view/delta-lake-the/9781098151935/](https://www.oreilly.com/library/view/delta-lake-the/9781098151935/)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Delta Lake: The Definitive Guide*：[https://www.oreilly.com/library/view/delta-lake-the/9781098151935/](https://www.oreilly.com/library/view/delta-lake-the/9781098151935/)'
- en: '*Comparing Apache Spark and* *Databricks*: [https://www.databricks.com/spark/comparing-databricks-to-apache-spark](https://www.databricks.com/spark/comparing-databricks-to-apache-spark)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*比较 Apache Spark 和 Databricks*：[https://www.databricks.com/spark/comparing-databricks-to-apache-spark](https://www.databricks.com/spark/comparing-databricks-to-apache-spark)'
- en: '*Databricks* *MLflow*: [https://www.databricks.com/product/managed-mlflow](https://www.databricks.com/product/managed-mlflow)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks MLflow*：[https://www.databricks.com/product/managed-mlflow](https://www.databricks.com/product/managed-mlflow)'
- en: '*Databricks Community Edition* *FAQ*: [https://www.databricks.com/product/faq/community-edition#:~:text=What%20is%20the%20difference%20between,ODBC%20integrations%20for%20BI%20analysis](https://www.databricks.com/product/faq/community-edition#:~:text=What%20is%20the%20difference%20between,ODBC%20integrations%20for%20BI%20analysis)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks 社区版 FAQ*：[https://www.databricks.com/product/faq/community-edition#:~:text=What%20is%20the%20difference%20between,ODBC%20integrations%20for%20BI%20analysis](https://www.databricks.com/product/faq/community-edition#:~:text=What%20is%20the%20difference%20between,ODBC%20integrations%20for%20BI%20analysis)'
- en: '*Delta 2.0 - The Foundation of your Data Lakehouse is* *Open*: [https://delta.io/blog/2022-08-02-delta-2-0-the-foundation-of-your-data-lake-is-open/](https://delta.io/blog/2022-08-02-delta-2-0-the-foundation-of-your-data-lake-is-open/)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Delta 2.0 - 你的数据湖屋基础是* *开放的*：[https://delta.io/blog/2022-08-02-delta-2-0-the-foundation-of-your-data-lake-is-open/](https://delta.io/blog/2022-08-02-delta-2-0-the-foundation-of-your-data-lake-is-open/)'
- en: '*Delta Lake* *Integrations*: [https://delta.io/integrations/](https://delta.io/integrations/)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Delta Lake* *集成*：[https://delta.io/integrations/](https://delta.io/integrations/)'
- en: '*Delta vs* *Iceberg*: [https://databeans-blogs.medium.com/delta-vs-iceberg-performance-as-a-decisive-criteria-add7bcdde03d](https://databeans-blogs.medium.com/delta-vs-iceberg-performance-as-a-decisive-criteria-add7bcdde03d)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Delta 与 *Iceberg*：[https://databeans-blogs.medium.com/delta-vs-iceberg-performance-as-a-decisive-criteria-add7bcdde03d](https://databeans-blogs.medium.com/delta-vs-iceberg-performance-as-a-decisive-criteria-add7bcdde03d)'
- en: '*UniForm*: [https://www.databricks.com/blog/delta-uniform-universal-format-lakehouse-interoperability](https://www.databricks.com/blog/delta-uniform-universal-format-lakehouse-interoperability)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*UniForm*：[https://www.databricks.com/blog/delta-uniform-universal-format-lakehouse-interoperability](https://www.databricks.com/blog/delta-uniform-universal-format-lakehouse-interoperability)'
- en: '*Delta Kernel: Simplifying Building Connectors for* *Delta*: [https://www.databricks.com/dataaisummit/session/delta-kernel-simplifying-building-connectors-delta/](https://www.databricks.com/dataaisummit/session/delta-kernel-simplifying-building-connectors-delta/)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Delta 内核：简化为 *Delta* 构建连接器的*：[https://www.databricks.com/dataaisummit/session/delta-kernel-simplifying-building-connectors-delta/](https://www.databricks.com/dataaisummit/session/delta-kernel-simplifying-building-connectors-delta/)'
- en: '*Databricks Community* *Website*: [https://community.cloud.databricks.com](https://community.cloud.databricks.com)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks 社区* *网站*：[https://community.cloud.databricks.com](https://community.cloud.databricks.com)'
- en: '*Podcast: Delta Lake Discussions with Denny* *Lee*: [https://open.spotify.com/show/6YvPDkILtWfnJNTzJ9HsmW?si=214eb7d808d84aa4](https://open.spotify.com/show/6YvPDkILtWfnJNTzJ9HsmW?si=214eb7d808d84aa4)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*播客：与 Denny *Lee* 的 Delta Lake 讨论*：[https://open.spotify.com/show/6YvPDkILtWfnJNTzJ9HsmW?si=214eb7d808d84aa4](https://open.spotify.com/show/6YvPDkILtWfnJNTzJ9HsmW?si=214eb7d808d84aa4)'
- en: '*Databricks Sets Official Data Warehousing Performance* *Record*: [https://www.databricks.com/blog/2021/11/02/databricks-sets-official-data-warehousing-performance-record.html](https://www.databricks.com/blog/2021/11/02/databricks-sets-official-data-warehousing-performance-record.html)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks 创下官方数据仓库性能* *记录*：[https://www.databricks.com/blog/2021/11/02/databricks-sets-official-data-warehousing-performance-record.html](https://www.databricks.com/blog/2021/11/02/databricks-sets-official-data-warehousing-performance-record.html)'
- en: '*LightGBM*: [https://github.com/microsoft/LightGBM](https://github.com/microsoft/LightGBM)
    [https://lightgbm.readthedocs.io/en/latest/](https://lightgbm.readthedocs.io/en/latest/)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LightGBM*：[https://github.com/microsoft/LightGBM](https://github.com/microsoft/LightGBM)
    [https://lightgbm.readthedocs.io/en/latest/](https://lightgbm.readthedocs.io/en/latest/)'
- en: '*Kaggle* | *Store* *Sales*: [https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kaggle* | *商店* *销售*：[https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview)'
- en: '*Kaggle* | *Multi Label Image* *Classification*: [https://www.kaggle.com/datasets/meherunnesashraboni/multi-label-image-classification-dataset](https://www.kaggle.com/datasets/meherunnesashraboni/multi-label-image-classification-dataset)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kaggle* | *多标签图像* *分类*：[https://www.kaggle.com/datasets/meherunnesashraboni/multi-label-image-classification-dataset](https://www.kaggle.com/datasets/meherunnesashraboni/multi-label-image-classification-dataset)'
- en: '*Store Sales - Time Series* *Forecasting*: [Kaggle.com/competitions/store-sales-time-series-forecasting/overview](http://Kaggle.com/competitions/store-sales-time-series-forecasting/overview)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*存储销售 - 时间序列* *预测*：[Kaggle.com/competitions/store-sales-time-series-forecasting/overview](http://Kaggle.com/competitions/store-sales-time-series-forecasting/overview)'
- en: '*arXiv* *website*: [https://arxiv.org](https://arxiv.org)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*arXiv 网站*：[https://arxiv.org](https://arxiv.org)'
