- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Bias, Explainability, Fairness, and Lineage
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差、可解释性、公平性和血统
- en: Now that we have learned all of the steps required to build and deploy models
    in Google Cloud and to automate the entire **machine learning** (**ML**) model
    development life cycle, it’s time to dive into yet more advanced concepts that
    are fundamental to developing and maintaining high-quality models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了在谷歌云中构建和部署模型以及自动化整个机器学习（ML）模型开发生命周期的所有步骤，现在是时候深入探讨更多对开发和维护高质量模型至关重要的高级概念了。
- en: In addition to our models providing predictions that are as accurate as possible
    for a given use case, we need to ensure that the predictions provided by our models
    are as fair as possible and that they do not exhibit bias or prejudice against
    any individuals or demographic groups.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 除了确保我们的模型为特定用例提供尽可能准确的预测外，我们还需要确保模型提供的预测尽可能公平，并且它们不会对任何个人或人口群体表现出偏见或歧视。
- en: 'The topics of bias, fairness, and explainability are at the forefront of ML
    research today. This chapter discusses these concepts in detail and explains how
    to effectively incorporate these concepts into our ML workloads. Specifically,
    we will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差、公平性和可解释性是当前机器学习研究的前沿话题。本章详细讨论了这些概念，并解释了如何有效地将这些概念融入我们的机器学习工作中。具体来说，本章将涵盖以下主题：
- en: An overview of bias, fairness, and explainability in **artificial** **intelligence**
    (**AI**)/ML
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能/机器学习（AI/ML）中偏差、公平性和可解释性的概述
- en: How to detect and mitigate bias in datasets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何检测和减轻数据集中的偏差
- en: Using explainability to understand ML models and reduce bias
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用可解释性来理解机器学习模型并减少偏差
- en: The importance of lineage tracking in ML model development
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习模型开发中追踪血统的重要性
- en: Let’s begin by defining and describing the relevant concepts.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义和描述相关概念开始。
- en: An overview of bias, explainability, and fairness in AI/ML
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能/机器学习中的偏差、可解释性和公平性的概述
- en: While the terms “bias,” “explainability,” and “fairness” are not specific to
    ML, in this section, we will explore these terms as they apply to the development
    and use of ML models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然“偏差”、“可解释性”和“公平性”这些术语并不特指机器学习（ML），在本节中，我们将探讨这些术语在机器学习模型开发和应用中的具体应用。
- en: Bias
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏差
- en: Bias in AI/ML refers to tendencies or prejudices in data and algorithms that
    can lead to unfair outcomes. One of the most common sources of bias in ML model
    development is when biases exist in the training data; for example, when the data
    points in the training data do not fairly represent the reality or the population
    that the model’s predictions will serve, which we refer to as **data bias**. For
    example, using a dataset in which the data points predominantly represent only
    one demographic group to train a model can result in poorer performance when that
    model is required to make predictions based on data points that represent other
    demographic groups. More specifically, this is an example of something called
    **sampling bias**. To tie this to a real-world scenario, let’s imagine that we’re
    training a model to perform facial recognition. If we train the model with images
    of people mainly from one specific demographic group, the model may not perform
    facial recognition tasks well when it is later presented with images of people
    from other demographic groups. There are a number of different ways in which we
    may encounter bias during the development and use of ML models, which we outline
    in the following paragraphs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能/机器学习中的偏差指的是数据和分析算法中的倾向或偏见，可能导致不公平的结果。机器学习模型开发中最常见的偏差来源之一是训练数据中存在偏差；例如，当训练数据中的数据点没有公平地代表模型预测将服务的现实或人群时，我们称之为**数据偏差**。例如，使用一个数据集，其中数据点主要代表只有一个特定的人口群体来训练模型，当模型需要根据代表其他人口群体的数据点进行预测时，可能会导致性能较差。更具体地说，这是一个被称为**抽样偏差**的例子。为了将这个概念与现实世界联系起来，让我们想象我们正在训练一个进行面部识别的模型。如果我们用来自一个特定人口群体的主要图像来训练这个模型，那么当模型后来遇到来自其他人口群体的图像时，它可能无法很好地执行面部识别任务。在开发和使用机器学习模型的过程中，我们可能会遇到多种不同的偏差，我们将在以下段落中概述。
- en: Collection or measurement bias
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收集或测量偏差
- en: A major cause of data bias is the manner in which the data is collected or measured.
    Sometimes, bias can occur due to the way we measure or frame a problem, which
    might not correctly represent the underlying concept that we’re trying to measure.
    For example, let’s imagine that we own a small company and we want to expand our
    product offerings to attract new customers. We may decide to use a survey to gather
    data to train ML models to predict what kinds of new products we should offer,
    based on how popular those new products are likely to be. There are a number of
    ways in which we could distribute this survey, such as via email or via physical
    mail. In the case of email, we might decide to send the survey to all of our current
    customers, because it’s likely that we would already have their email addresses
    in our database. In the case of physical email, we may decide to send it out to
    everybody in the same postal code area, city, state, or country in which our company
    is physically located.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据偏差的主要原因在于数据收集或测量的方式。有时，偏差可能由于我们测量或构建问题的方式而产生，这可能无法正确代表我们试图测量的基本概念。例如，让我们想象我们拥有一家小型公司，我们希望扩大我们的产品种类以吸引新客户。我们可能会决定使用调查来收集数据，以训练机器学习模型来预测我们应该提供哪些类型的新产品，基于这些新产品可能受欢迎的程度。我们可以以多种方式分发这份调查，例如通过电子邮件或通过实体邮件。在电子邮件的情况下，我们可能会决定将调查发送给所有当前客户，因为我们很可能已经在数据库中拥有他们的电子邮件地址。在实体邮件的情况下，我们可能会决定向公司所在邮政编码、城市、州或国家内的所有人发送。
- en: Unfortunately, both of those methods may unexpectedly introduce bias into the
    datasets used to train our models. For example, perhaps we traditionally happened
    to appeal to only a specific type of customer in the past. If we use our current
    customer base as the survey group, we may not get good data points for products
    that would appeal to new customer demographics. Similarly, if we send out a survey
    to everybody in a specific geographical area, such as a postal code, city, state,
    or even country, the people living in that area may not represent diverse demographic
    groups, which could inadvertently introduce biases into the resulting dataset.
    This phenomenon is sometimes referred to as **response bias**, in which the people
    providing the required information may be biased in some way, and may therefore
    provide biased responses. To make this even more complicated, the way in which
    we phrase questions in the survey could accidentally bias the respondents.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这两种方法可能会意外地将偏差引入我们用于训练模型的训练数据集中。例如，也许我们过去传统上只吸引特定类型的客户。如果我们以当前客户群作为调查组，我们可能无法获得对吸引新客户群体有吸引力的产品的良好数据点。同样，如果我们向特定地理区域内的所有人发送调查问卷，例如邮政编码、城市、州，甚至国家，该区域居住的人可能不代表多样化的人口群体，这可能会无意中引入偏差到最终的数据集中。这种现象有时被称为**响应偏差**，在这种情况下，提供所需信息的人可能在某些方面存在偏见，因此可能会提供有偏见的回答。更复杂的是，我们在调查中提出问题的方式可能会无意中影响受访者的回答。
- en: In this scenario, we also need to be aware of **observer bias** or **subjectivity**,
    in which the people running the survey may be biased in some way. In fact, our
    example of deciding to email our current customers, or to send physical mail to
    people in a specific area, is a type of observer bias, in which we decided to
    survey specific groups of people based on factors such as currently available
    data (for example, our customers’ email addresses) or proximity to our company’s
    location.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们还需要意识到**观察者偏差**或**主观性**，在这种情况下，进行调查的人可能在某些方面存在偏见。事实上，我们决定通过电子邮件向我们的当前客户发送，或向特定区域的人发送实体邮件的例子，是一种观察者偏差，其中我们根据当前可用的数据（例如，我们客户的电子邮件地址）或与我们公司位置的邻近性等因素决定调查特定的人群群体。
- en: Pre-existing bias
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预先存在的偏差
- en: These are generally biases that already exist in society. Such biases can stem
    from factors such as cultural, historical, and societal norms, consisting of ingrained
    beliefs and practices that shape the way individuals view the world.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些通常是社会中已经存在的偏差。这种偏差可能源于文化、历史和社会规范等因素，包括根深蒂固的信念和实践，这些信念和实践塑造了个人看待世界的方式。
- en: One of the most common ways to train a model is to use historical data that
    has been recorded. However, data from the past might be tainted with biases that
    were inherent in those times — I think we can all agree that the societal norms
    of the 1950s or even the 1990s are quite different from today’s standards, especially
    in terms of fairness across different demographic groups. If AI/ML models are
    trained on such data without correction, they will likely perpetuate those biases.
    It’s important to understand that unaddressed pre-existing biases can not only
    perpetuate but sometimes even amplify stereotypes, leading to unfair or discriminatory
    outcomes. For example, a job recommendation system might be biased toward recommending
    higher-paying jobs to men if it is trained on historical data that exhibits this
    bias.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型最常见的方法之一是使用已经记录的历史数据。然而，过去的数据可能受到那些时代固有的偏见的影响——我想我们都可以同意，20世纪50年代或甚至20世纪90年代的社会规范与今天的标准有很大不同，特别是在不同人口群体之间的公平性方面。如果AI/ML模型在没有纠正的情况下使用这样的数据进行训练，它们很可能会持续这些偏见。重要的是要理解，未解决的预先存在的偏见不仅会持续，有时甚至还会放大刻板印象，导致不公平或歧视性的结果。例如，如果推荐系统在表现出这种偏见的历史数据上训练，它可能会倾向于向男性推荐更高薪酬的工作。
- en: Another common type of bias in this context is **confirmation bias**, in which
    people may subconsciously tend to select and interpret data in ways that confirm
    their pre-existing beliefs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，另一种常见的偏见类型是**确认偏见**，其中人们可能无意识地倾向于选择和解释数据，以证实他们先前的信念。
- en: If data collection processes do not check for pre-existing biases, they can
    lead to data that’s not truly representative of the reality or population it’s
    supposed to depict. Models trained on such data may learn and replicate those
    biases in their predictions or actions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据收集过程没有检查预先存在的偏见，它们可能导致的数据并不能真正代表现实或它应该描绘的人群。在这样数据上训练的模型可能会在它们的预测或行动中学习并复制这些偏见。
- en: Algorithmic bias
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法偏见
- en: This refers to biases that are introduced by the design of the algorithms themselves,
    not just the data they are trained on. This is an increasingly important topic,
    considering that algorithms are used to implement an ever-expanding plethora of
    important decisions in modern society, such as credit approvals, recruitment processes,
    and medical diagnoses.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这指的是由算法本身的设计引入的偏见，而不仅仅是它们训练的数据。考虑到算法在现代社会中用于实施越来越多的重要决策，如信用批准、招聘流程和医疗诊断，这是一个越来越重要的话题。
- en: Algorithmic bias can be more subtle and difficult to detect. For example, such
    bias could simply stem from the algorithm developers themselves. If the team developing
    the algorithm lacks diversity, it might not foresee or recognize potential biases
    in how the algorithm is implemented. We also need to recognize the possibility
    of accidentally developing biased feedback loops in ML systems. For example, in
    systems where algorithms are continuously trained with new data, biased outcomes
    can reinforce the input biases, and over time, this can lead to increasingly skewed
    results.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 算法偏见可能更为微妙且难以检测。例如，这种偏见可能仅仅源于算法开发者本身。如果开发算法的团队缺乏多样性，可能无法预见或识别算法实施过程中的潜在偏见。我们还需要认识到在机器学习系统中意外开发有偏见的反馈循环的可能性。例如，在算法持续用新数据训练的系统，有偏见的输出可能会加强输入偏见，随着时间的推移，这可能导致结果越来越偏斜。
- en: Just like the other types of biases discussed previously in this section, unaddressed
    algorithmic bias can lead to the perpetuation of stereotypes, misinformation,
    and unjust practices.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 就像本节之前讨论的其他类型的偏见一样，未解决的算法偏见可能导致刻板印象、错误信息和不公平做法的持续。
- en: It’s important to note that in this section, we have discussed only some of
    the most common types of biases that can affect ML model development. This is
    an active area of research, and additional types of bias exist that we may not
    be explicitly aware of as we develop and use ML models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，在本节中，我们只讨论了可能影响机器学习模型开发的一些最常见的偏见类型。这是一个活跃的研究领域，还存在一些我们可能没有明确意识到的其他类型的偏见，在我们开发和使用机器学习模型的过程中。
- en: Figuratively speaking, the concept of bias represents one side of a coin, and
    on the other side of that coin is the concept of fairness, which we explore in
    more detail next.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 比喻来说，偏见的概念代表了一枚硬币的一面，而硬币的另一面是公平的概念，我们将在下一节中更详细地探讨。
- en: Fairness
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公平性
- en: Fairness in AI and ML refers to the practice of developing algorithms in a way
    that prevents discrimination and promotes equity. While fairness is a rather straightforward
    concept to define, such as treating all people equally, in practice, it can be
    difficult to monitor and uphold. A little later in this chapter, we will look
    at mechanisms to monitor and enhance the fairness of ML models, but let’s first
    describe the concept in a bit more detail in the context of ML. Just as in the
    previous section related to bias, we will discuss a few different ways in which
    we can define and measure fairness with regard to ML models, which we outline
    in the following subsections.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在AI和机器学习（ML）中，公平性指的是以防止歧视和促进公平的方式开发算法。虽然公平性是一个相对直接的概念，例如平等对待所有人，但在实践中，它可能难以监控和维护。在本章稍后，我们将探讨监控和增强ML模型公平性的机制，但首先让我们在ML的背景下更详细地描述这一概念。正如前一个与偏差相关的部分，我们将讨论几种定义和衡量ML模型公平性的不同方法，这些方法将在以下小节中概述。
- en: Representational fairness
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代表性公平性
- en: This is the first line of defense against bias in ML model development, especially
    with regard to data bias. Representational fairness aims to ensure that the data
    used in model training and validation contains a fair representation of each of
    the various demographic groups that the resulting model’s predictions will affect;
    for example, ensuring that genders and ethnic groups are represented fairly in
    the dataset.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是防止ML模型开发中偏差的第一道防线，特别是在数据偏差方面。代表性公平性旨在确保用于模型训练和验证的数据包含了对结果模型预测将影响的各个不同人口群体的公平代表性；例如，确保性别和种族群体在数据集中得到公平的体现。
- en: Procedural fairness
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 程序公平性
- en: 'This involves ensuring that the processes for data collection, data handling,
    and algorithm development are equitable and do not favor any particular group.
    In [*Chapter 13*](B18143_13.xhtml#_idTextAnchor328), we will discuss the concept
    of governance in great detail, especially in relation to data governance, and
    the important role it plays in helping to ensure that potential biases are addressed
    or mitigated in datasets. Many enterprises employ entire teams or organizations
    dedicated to outlining and upholding governance requirements. In the case of procedural
    fairness, the focus is not so much on the contents of the data, but rather on
    the fairness of the processes that lead to the development, deployment, and operation
    of ML models. Some key components of procedural fairness are represented by the
    **TAIC** framework, which consists of the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及到确保数据收集、数据处理和算法开发的过程是公平的，不偏袒任何特定群体。在第13章[*](B18143_13.xhtml#_idTextAnchor328)中，我们将详细讨论治理的概念，特别是与数据治理相关的内容，以及它在帮助确保数据集中潜在的偏差得到解决或减轻的重要作用。许多企业雇佣了整个团队或组织来制定和执行治理要求。在程序公平性的情况下，重点不在于数据的内容，而在于导致ML模型开发、部署和运行的过程的公平性。程序公平性的几个关键组成部分由**TAIC**框架表示，该框架包括以下内容：
- en: '**Transparency**, such as clear documentation of the methods, data sources,
    and decisions made throughout the model’s life cycle'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明度**，例如在整个模型生命周期中对方法、数据来源和做出的决策进行清晰的文档记录'
- en: '**Accountability**, meaning that there should be clarity on who is responsible
    for various stages of model development, deployment, and monitoring'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可问责性**，意味着应明确谁对模型开发的各个阶段、部署和监控负责'
- en: '**Impartiality**, meaning that procedures should not favor or prejudice any
    individuals or groups'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公平性**，意味着程序不应偏袒或歧视任何个人或群体'
- en: '**Consistency**, meaning that the procedures used to develop a model should
    be reproducible in a consistent manner'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性**，意味着用于开发模型的过程应以一致的方式可重复'
- en: Google Cloud Vertex AI provides mechanisms that help to audit procedural fairness,
    which we will explore in more detail in later sections of this chapter, as well
    as in the practical activities that accompany this chapter.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Vertex AI提供了帮助审计程序公平性的机制，我们将在本章的后续部分以及伴随本章的实践活动中更详细地探讨。
- en: Outcome fairness
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结果公平性
- en: While representational fairness mainly focuses on the inputs used to train ML
    models (that is, the contents of the training data) and procedural fairness focuses
    on the procedures used to develop and deploy ML models, outcome fairness, as the
    name suggests, focuses on the results produced by ML models. Specifically, it
    aims to ensure that the outcomes produced by a model are equitable and do not
    disproportionately benefit or harm any group. Of course, one way to measure this
    is to monitor the predictions made by a model to determine whether any bias appears
    to be present. In this chapter, we will explore some mechanisms and metrics that
    can be used for this purpose.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当代表示性公平主要关注用于训练机器学习模型的输入（即训练数据的内容）时，程序性公平则关注用于开发和部署机器学习模型的程序，结果公平，正如其名所示，关注机器学习模型产生的结果。具体来说，它旨在确保模型产生的结果公平，不会不成比例地使任何群体受益或受损。当然，衡量这种公平性的方法之一是监控模型做出的预测，以确定是否存在任何偏见。在本章中，我们将探讨一些可用于此目的的机制和指标。
- en: As I mentioned when discussing bias in the previous section, the concepts of
    bias and fairness in ML are still very active and evolving areas of research.
    This is why fairness can be a complex topic in practice, and it’s important to
    understand that achieving one type of fairness can sometimes lead to a violation
    of another type. Moreover, what is considered “fair” can be context-dependent
    and might vary between communities, as well as over time.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在上一节讨论偏见时提到的，机器学习中的偏见和公平性概念仍然是研究非常活跃和不断发展的领域。这就是为什么在实践中，公平性可能是一个复杂的话题，并且重要的是要理解，实现一种类型的公平性有时可能导致另一种类型的公平性受到侵犯。此外，被认为“公平”的东西可能取决于具体情境，可能在不同的社区之间以及随时间推移而有所不同。
- en: Now that we’ve introduced the concepts that define the two-sided coin of bias
    and fairness, the next step will be to introduce a topic that is inherently linked
    to those two concepts, which is referred to as **explainability**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了定义偏见和公平性两面性的概念，下一步将介绍一个与这两个概念固有联系的课题，这被称为**可解释性**。
- en: Explainability
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释性
- en: Explainability in ML focuses on the ability of a human to understand and explain
    how the outputs of an ML model were produced. This is often also referred to as
    the **interpretability** of a model. These concepts continue to grow in importance
    as ML models become increasingly complex over time. For example, it’s pretty easy
    to interpret and explain how the outputs of a simple linear regression model were
    produced, because we have well-defined mathematical formulae that describe that
    process, such as *y = a + bx*, where *y* (the output) is a linear transformation
    of *x* (the input). Similarly, for decision tree models, we can logically trace
    the decision path through the tree. However, when we’re dealing with large **neural
    networks** (**NNs**) containing millions or even billions of parameters, and incorporating
    different types of architectures and activation functions, we need some additional
    tools to help us understand how those models are making their decisions, which
    we will explore in this chapter.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的可解释性关注人类理解和解释机器学习模型输出产生的能力。这通常也被称为模型的**可解释性**。随着机器学习模型随着时间的推移变得越来越复杂，这些概念的重要性持续增长。例如，解释和解释简单线性回归模型的输出是如何产生的相对容易，因为我们有定义良好的数学公式来描述这个过程，例如
    *y = a + bx*，其中 *y*（输出）是 *x*（输入）的线性变换。同样，对于决策树模型，我们可以逻辑上追踪通过树的决定路径。然而，当我们处理包含数百万甚至数十亿参数的大型**神经网络**（**NNs**），并采用不同类型的架构和激活函数时，我们需要一些额外的工具来帮助我们理解这些模型是如何做出决定的，我们将在本章中探讨这些工具。
- en: When we discussed procedural fairness in the previous section, we talked about
    the importance of transparency. Explainability links closely to the concept of
    transparency, as it seeks to ensure that people can understand the process by
    which predictions are produced from a given model. For example, if we own a bank
    and one of our models is responsible for granting or declining credit applications,
    we want to thoroughly understand how it makes those decisions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在上一节讨论程序性公平时，我们谈到了透明度的重要性。可解释性与透明度的概念紧密相连，因为它旨在确保人们可以理解从给定模型中产生预测的过程。例如，如果我们拥有一家银行，并且我们的某个模型负责批准或拒绝信贷申请，我们希望彻底了解它是如何做出这些决定的。
- en: We can talk about explainability in terms of **global explainability**, in which
    we try to understand the general logic the model applies to make predictions across
    all input instances, or **local explainability**, which involves explaining why
    a model made a specific decision for a particular instance (for example, understanding
    why a specific customer’s credit application was declined).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以就可解释性从**全局可解释性**的角度进行讨论，其中我们试图理解模型在所有输入实例上应用的一般逻辑以做出预测，或者从**局部可解释性**的角度进行讨论，这涉及到解释模型为何对特定实例做出了特定决策（例如，理解为什么某个特定客户的信用申请被拒绝）。
- en: Overall, explainability is critical for building trust in AI systems, complying
    with legal requirements, and ensuring that humans can intervene effectively in
    decision-making processes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，可解释性对于建立对AI系统的信任、遵守法律要求以及确保人类能够有效地干预决策过程至关重要。
- en: Now that we’ve introduced and explained the overall concepts, let’s start diving
    in deeper. Our first deep dive in this chapter will be on the topic of bias in
    datasets.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍并解释了整体概念，让我们开始深入探讨。本章的第一个深入探讨将是关于数据集中偏差的话题。
- en: How to detect and mitigate bias in datasets
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何检测和减轻数据集中的偏差
- en: In this section, we explore how to detect bias in our datasets, and there are
    various tools and methods we can use for this purpose. In fact, we’ve already
    covered some of them in previous chapters of this book, such as data exploration
    and visualization.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨如何检测数据集中的偏差，并且我们可以使用各种工具和方法来达到这个目的。实际上，我们已经在本书的前几章中介绍了一些这些工具，例如数据探索和可视化。
- en: Data exploration and visualization
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据探索和可视化
- en: When we explore our datasets using visualization, for example, charts such as
    histograms and scatter plots can help visualize disparities in data distribution
    for different demographic groups. Similarly, we’ve already explored descriptive
    statistics such as mean, median, mode, and variance to understand the contents
    of our datasets. If there are significant disparities in these statistics between
    subgroups, it may suggest the presence of bias in the dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用可视化来探索我们的数据集时，例如，直方图和散点图等图表可以帮助可视化不同人口群体之间的数据分布差异。同样，我们已经探讨了描述性统计，如均值、中位数、众数和方差，以了解我们数据集的内容。如果这些统计量在不同子组之间存在显著差异，这可能表明数据集中存在偏差。
- en: Specific tools for detecting dependencies between features
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于检测特征之间依赖关系的特定工具
- en: We also want to test for potential correlations or dependence between features
    in our dataset in order to understand whether the values of some features are
    significantly influenced by the values of others. While this is something that
    we generally want to do as part of our regular data exploration and feature engineering
    anyway (that is, we need to understand underlying patterns in our data as much
    as possible), it becomes even more important in the context of bias and fairness,
    especially with regard to potential links between the target variable and protected
    attributes such as gender or ethnicity.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望测试数据集中特征之间的潜在相关性或依赖性，以便了解某些特征的值是否受到其他特征值的显著影响。虽然这通常是我们作为常规数据探索和特征工程的一部分要做的事情（也就是说，我们需要尽可能多地了解我们数据中的潜在模式），但在偏差和公平性的背景下，这变得更加重要，特别是在目标变量与性别或种族等受保护属性之间潜在联系方面。
- en: There are specific tools for detecting dependencies among features, such as
    **Pearson’s Correlation Coefficient**, which measures the linear relationship
    between numeric variables, or the **Chi-Squared Test for Independence**, which
    can be used to determine if there is a significant association between two categorical
    variables.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些特定的工具可以检测特征之间的依赖关系，例如**皮尔逊相关系数**，它衡量数值变量之间的线性关系，或者**卡方检验**，它可以用来确定两个分类变量之间是否存在显著的关联。
- en: Mechanisms incorporating model prediction results
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 包含模型预测结果的机制
- en: In addition to the aforementioned methods, we can use mechanisms that go beyond
    just examining the dataset by also taking a model’s outputs into account, which
    can then help us to link any observed biases back to the training data and process.
    For example, we can use **disparate impact analysis** (**DIA**) to compare the
    ratio of favorable outcomes for different groups and measure whether any specific
    groups tend to get more favorable outcomes than any others. Let’s take a look
    at DIA in more detail.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述方法之外，我们还可以使用超越仅检查数据集的机制，同时考虑模型的输出，这可以帮助我们将任何观察到的偏见追溯到训练数据和过程。例如，我们可以使用**差异影响分析**（**DIA**）来比较不同组的有利结果比率，并衡量是否有任何特定组比其他组更有可能获得有利结果。让我们更详细地看看DIA。
- en: DIA
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DIA
- en: 'In the case of DIA, we generally identify what is referred to as a **privileged
    group** and an **unprivileged group** (also referred to as a **protected group**),
    based on some protected characteristic such as gender or race, and we compare
    the outputs of a given model for both of those groups to try to determine whether
    the model appears to bias in either direction (positively or negatively) with
    regard to those groups. We measure this disparity using a metric referred to as
    the **disparate impact ratio** (**DIR)**, which is defined by the following formula:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在DIA的情况下，我们通常根据某些受保护特征（如性别或种族）来识别所谓的**特权组**和**无特权组**（也称为**受保护组**），并比较给定模型为这两个组提供的输出，以试图确定模型是否在针对这些组表现出偏向（正面或负面）。我们使用称为**差异影响比率**（**DIR**）的指标来衡量这种差异，该指标由以下公式定义：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>D</mi><mi>I</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>P</mi><mo>(</mo><mi>F</mi><mi>U</mi><mi>P</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>F</mi><mi>P</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/12.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>D</mi><mi>I</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>P</mi><mo>(</mo><mi>F</mi><mi>U</mi><mi>P</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>F</mi><mi>P</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/12.png)'
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:mi>U</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/13.png)
    represents the probability of a favorable outcome for the unprivileged group,
    and P(FP) represents the probability of a favorable outcome for the privileged
    group.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:mi>U</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/13.png)代表无特权组获得有利结果的可能性，而P(FP)代表特权组获得有利结果的可能性。
- en: 'The DIR value can be interpreted in the following way:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: DIR值可以这样解释：
- en: When DIR is equal to 1, it indicates perfect fairness, where both groups receive
    favorable outcomes at the same rate
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当DIR等于1时，表示完美的公平性，两组以相同的比率获得有利结果
- en: When DIR is greater than 1, it indicates that the unprivileged group is more
    likely to receive favorable outcomes
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当DIR大于1时，表示无特权组获得有利结果的可能性更高
- en: When DIR is less than 1, it indicates that the unprivileged group is less likely
    to receive favorable outcomes
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当DIR小于1时，表示无特权组获得有利结果的可能性较低
- en: A commonly accepted threshold is a DIR value of between 0.8 and 1.25; values
    outside this range often indicate potential **disparate** **impact** (**DI**).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一个普遍接受的阈值是DIR值在0.8到1.25之间；超出这个范围的值通常表明潜在的**差异****影响**（**DI**）。
- en: Note that DIA can be performed directly on a dataset by using the target feature,
    or it can use a combination of the input features and a model’s predictions.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，DIA可以通过使用目标特征直接在数据集上执行，或者它可以使用输入特征和模型预测的组合。
- en: In the next section, we will dive into the concept of explainability in ML in
    more detail, and explore how we can use explainability frameworks to detect and
    address bias. We will also expand on the concepts covered in this section, and
    discuss how they relate to explainability and fairness.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更详细地探讨机器学习中可解释性的概念，并探讨我们如何使用可解释性框架来检测和解决偏见。我们还将扩展本节中涉及的概念，并讨论它们与可解释性和公平性的关系。
- en: Using explainability to understand ML models and reduce bias
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用可解释性来理解机器学习模型并减少偏见
- en: We introduced the concept of explainability at a high level in the previous
    section. This section dives further into this topic, introducing tools that can
    be used to gain insights into how ML models are working at inference time.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们以高层次介绍了可解释性的概念。本节将进一步探讨这个主题，介绍可用于在推理时深入了解机器学习模型工作的工具。
- en: Explainability techniques, methods, and tools
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释性技术、方法和工具
- en: Let’s begin by exploring some popular techniques, methods, and tools that we
    can use for implementing explainability in ML, which we describe in the following
    subsections.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从探索一些流行的技术、方法和工具开始，这些工具可以帮助我们在机器学习中实现可解释性，我们将在以下小节中描述。
- en: Performing data exploration
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行数据探索
- en: By now, it should hopefully be clear that understanding the data used to train
    our models is one of the first steps in explaining how the model makes decisions,
    and it is also one of the first lines of defense to identify and combat potential
    biases.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，希望已经清楚，理解用于训练我们模型的训练数据是解释模型如何做出决策的第一步之一，同时也是识别和对抗潜在偏差的第一道防线。
- en: In the practical activities associated with this chapter, we explore the “Adult
    Census Income” dataset ([https://archive.ics.uci.edu/dataset/2/adult](https://archive.ics.uci.edu/dataset/2/adult)),
    which is known to contain imbalances with regard to race and gender. The dataset
    comprises information pertaining to people, such as their race, gender, education
    they’ve received, and their current annual income, expressed as either “<=50K”
    (less than or equal to $50,000 per year) or “>50K” (more than $50,000 per year),
    which creates a binary classification use case when the income is used as the
    target variable.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章相关的实际活动中，我们探讨了“成人人口普查收入”数据集([https://archive.ics.uci.edu/dataset/2/adult](https://archive.ics.uci.edu/dataset/2/adult))，该数据集已知在种族和性别方面存在不平衡。该数据集包含有关人们的信息，例如他们的种族、性别、所受教育和他们当前的年收入，年收入以“<=50K”（每年不超过50,000美元）或“>50K”（每年超过50,000美元）表示，当使用收入作为目标变量时，这创建了一个二元分类用例。
- en: 'When exploring this data, we can ask questions such as the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索这些数据时，我们可以提出以下问题：
- en: Are feature values, such as race and gender, represented somewhat evenly or
    unevenly?
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征值，如种族和性别，是否在一定程度上均匀或不均匀地表示？
- en: Are there any correlations between some feature values and the income earned
    by that person?
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是否存在某些特征值与该人收入之间的相关性？
- en: We can easily see imbalances in the dataset by using data visualization techniques.
    Let’s take a look at some examples.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用数据可视化技术轻松地看到数据集中的不平衡。让我们看看一些例子。
- en: Income distribution by gender
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 按性别划分的收入分布
- en: 'The following code will show us the distributions of each gender in the dataset,
    with regard to the two different income categories (that is, people who earn less
    than or equal to $50,000 per year, or people who earn more than $50,000 per year).
    You can open the Jupyter notebook that accompanies this chapter if you’d like
    to follow along with the code examples that we will review. We can again use the
    same Vertex AI Workbench-Notebook Instance that we created in [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168)
    for this purpose. Please open JupyterLab on that notebook instance. In the directory
    explorer on the left side of the screen, navigate to the `Chapter-12` directory
    and open the `bias-explainability.ipynb` notebook. You can choose **Python (Local)**
    as the kernel. Again, you can run each cell in the notebook by selecting the cell
    and pressing *Shift* + *Enter* on your keyboard. In addition to the relevant code,
    the notebook contains markdown text that describes what the code is doing:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将向我们展示数据集中每个性别在两种不同的收入类别（即每年收入不超过50,000美元或超过50,000美元的人）中的分布情况。如果您想跟随我们即将审查的代码示例，可以打开本章附带的Jupyter笔记本。我们可以再次使用在[*第五章*](B18143_05.xhtml#_idTextAnchor168)中创建的相同的Vertex
    AI Workbench-Notebook实例来完成此目的。请在笔记本实例上打开JupyterLab。在屏幕左侧的目录浏览器中，导航到`Chapter-12`目录并打开`bias-explainability.ipynb`笔记本。您可以选择**Python
    (Local)**作为内核。同样，您可以通过选择单元格并按键盘上的*Shift* + *Enter*来运行笔记本中的每个单元格。除了相关代码外，笔记本还包含描述代码正在做什么的markdown文本：
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will display a graph like the one shown in *Figure 12**.1*:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示类似于*图12.1*的图表：
- en: '![Figure 12.1: Income distribution by gender](img/B18143_12_1.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图12.1：按性别划分的收入分布](img/B18143_12_1.jpg)'
- en: 'Figure 12.1: Income distribution by gender'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1：按性别划分的收入分布
- en: In *Figure 12**.1*, we can see that, overall, more people in the dataset earn
    more than $50,000 per year than those who do not. We can also see that the number
    of males in each group far exceeds the number of females in each group. This also
    tells us the entire dataset consists of more data points related to men than women.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图12.1*中，我们可以看到，总体而言，数据集中收入超过50,000美元的人比那些收入不超过50,000美元的人要多。我们还可以看到，每个组中的男性数量远超过女性数量。这也告诉我们整个数据集包含与男性相关的数据点比与女性相关的数据点要多。
- en: Income distribution by race
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 按种族划分的收入分布
- en: 'The following code will show us the distributions of each race in the dataset,
    with regard to income:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将向我们展示数据集中每个种族关于收入的分布：
- en: '[PRE1]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will display a graph like the one shown in *Figure 12**.2*:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示一个类似于*图12.2*所示的图表：
- en: '![Figure 12.2: Income distribution by race](img/B18143_12_2.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图12.2：按种族划分的收入分布](img/B18143_12_2.jpg)'
- en: 'Figure 12.2: Income distribution by race'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2：按种族划分的收入分布
- en: In *Figure 12**.2*, for both the “<=50K” category and the “>50K” category, we
    see that the data contains many more data points for White people than any other
    race. This can be seen as a type of bias in the dataset. As we introduced earlier
    in this chapter, this bias may exist for multiple potential reasons, such as bias
    in the collection of the data, or it may happen due to other factors such as geographic
    location. This particular dataset represents the population of a specific area,
    which may somewhat explain its apparent bias toward a particular race. For example,
    if the data were collected in Asia, then it would contain many more data points
    for Asian people than any other race, or if it were collected in central Africa,
    then it would contain many more data points for Black people than any other race.
    It’s important to note any imbalances in the data and determine how they may affect
    the training of an ML model and who that ML model is intended to serve. Generally,
    if features in the dataset have much higher numbers of instances of a specific
    value, then an ML model’s predictions will likely reflect that in some way.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图12.2*中，对于“<=50K”类别和“>50K”类别，我们可以看到，与任何其他种族相比，白人的数据点要多得多。这可以被视为数据集中的一种偏差。正如我们在本章前面所介绍的，这种偏差可能存在多个潜在原因，例如数据收集中的偏差，或者它可能由于地理位置等其他因素而发生。这个特定的数据集代表了特定地区的人口，这可以部分解释其明显倾向于特定种族的偏差。例如，如果数据是在亚洲收集的，那么它将包含比任何其他种族更多的亚洲人的数据点，或者如果它是在中非收集的，那么它将包含比任何其他种族更多的黑人的数据点。重要的是要注意数据中的任何不平衡，并确定它们可能如何影响机器学习模型的训练以及该模型旨在为谁服务。一般来说，如果数据集中的特征具有特定值的实例数量要高得多，那么机器学习模型的预测可能会以某种方式反映这一点。
- en: In the Jupyter notebook that accompanies this chapter, we also assess other
    types of distributions in the data, such as occupational distribution by gender
    and educational distribution by race. I encourage you to use the Jupyter notebook
    to explore the data in more detail. For now, let’s move on and look at implementing
    DIA in more detail.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在伴随本章的Jupyter笔记本中，我们还评估了数据中的其他类型分布，例如按性别划分的职业分布和按种族划分的教育分布。我鼓励您使用Jupyter笔记本来更详细地探索数据。现在，让我们继续并更详细地探讨实施DIA。
- en: Implementing DIA
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实施DIA
- en: 'The following is the code we use to implement DIA in our Jupyter notebook:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们用于在Jupyter笔记本中实施DIA的代码：
- en: '[PRE2]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The code first creates a pivot table of the `adult_data` DataFrame, grouped
    by gender and income, with the count of people in each group as the value. Then,
    it adds a new column to the pivot table called `rate`, which is the proportion
    of people in each group who earn more than $50,000\. Finally, it calculates the
    DI by dividing the rate for females by the rate for males.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先创建了一个`adult_data`数据框的交叉表，按性别和收入分组，每个组中的人数为值。然后，它向交叉表添加了一个名为`rate`的新列，这是每个组中收入超过50,000美元的人的比例。最后，它通过将女性的比率除以男性的比率来计算DI。
- en: This is a very simple example of implementing DIA on a dataset that is known
    to contain gender imbalances. DIA can be much more complex and may require some
    domain expertise to implement effectively, depending on the contents of the dataset
    and the intended function of an ML model trained on that data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个在已知包含性别不平衡的数据集上实施DIA的非常简单的例子。DIA可能更加复杂，并且可能需要一些领域专业知识才能有效实施，具体取决于数据集的内容以及在该数据上训练的机器学习模型的目的功能。
- en: Next, let’s discuss the topic of feature importance.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论特征重要性的主题。
- en: Feature importance
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征重要性
- en: Feature importance evaluates the impact that each feature has on the predictions
    made by a model. To explore this concept, let’s imagine we have a dataset that
    contains information about people, and the features in the dataset include height,
    age, eye color, and whether or not they like coffee. We want to use this data
    to train a model to predict the likelihood of each person being a successful basketball
    player. Do you think any of the input features in our dataset may be more important
    than any others in terms of influencing the outcome? Is it likely that height
    would be more important or less important than eye color in determining whether
    a person is likely to be a successful basketball player? Is age an important factor?
    In this case, we’re describing the concept of feature importance in a simple example.
    In reality, we may be dealing with datasets that contain thousands of features,
    and those features may not always represent easily interpretable concepts such
    as height, age, and eye color. For this reason, we can use tooling to help us
    to get these kinds of insights. The following subsections describe the tools we
    can use for this purpose.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性评估了每个特征对模型预测的影响。为了探讨这个概念，让我们想象我们有一个包含关于人们信息的数据集，数据集中的特征包括身高、年龄、眼睛颜色以及他们是否喜欢咖啡。我们想使用这些数据来训练一个模型，以预测每个人成为成功篮球运动员的可能性。你认为我们数据集中的任何输入特征可能比其他特征在影响结果方面更重要吗？身高在决定一个人是否可能成为成功的篮球运动员方面可能比眼睛颜色更重要或更不重要吗？年龄是一个重要因素吗？在这种情况下，我们通过一个简单的例子来描述特征重要性的概念。在现实中，我们可能正在处理包含数千个特征的数据库，而这些特征可能并不总是代表像身高、年龄和眼睛颜色这样容易解释的概念。因此，我们可以使用工具来帮助我们获得这些见解。以下小节描述了我们可以用于此目的的工具。
- en: Feature importance tools built into popular ML libraries
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常见机器学习库中内置的特征重要性工具
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Figure 12.3: Feature importance](img/B18143_12_3.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图12.3：特征重要性](img/B18143_12_3.jpg)'
- en: 'Figure 12.3: Feature importance'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3：特征重要性
- en: In *Figure 12**.3*, we can see that features such as `age`, `hours-per-week`,
    and `capital-gain` seem to be pretty important features with regard to predicting
    income. Does the influence of those features seem intuitive to you?
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在**图12**.3中，我们可以看到诸如`年龄`、`每周小时数`和`资本收益`等特征似乎在预测收入方面非常重要。这些特征的影响是否对你来说直观易懂？
- en: Note that feature importance does not imply causality. Just because a feature
    is deemed important doesn’t mean it causes the target variable to change; only
    that there’s an association.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，特征重要性并不表示因果关系。仅仅因为一个特征被认为很重要，并不意味着它会导致目标变量发生变化；只是表示存在关联。
- en: While we’re specifically using the `feature_importances_` attribute in scikit-learn
    in our Jupyter notebook, other popular ML libraries also provide similar mechanisms.
    For example, TensorFlow’s boosted trees (`tf.estimator.BoostedTreesClassifier`
    or `tf.estimator.BoostedTreesRegressor`) also provide `feature_importances_` as
    a property to get the importance of each feature. Similarly, LightGBM and CatBoost
    provide `feature_importances_` and `get_feature_importance()`, respectively, and
    XGBoost provides the `plot_importance()` function for visualization.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在Jupyter笔记本中具体使用scikit-learn的`feature_importances_`属性时，其他流行的机器学习库也提供了类似的机制。例如，TensorFlow的增强树（`tf.estimator.BoostedTreesClassifier`或`tf.estimator.BoostedTreesRegressor`）也提供了`feature_importances_`属性来获取每个特征的重要性。同样，LightGBM和CatBoost分别提供了`feature_importances_`和`get_feature_importance()`，而XGBoost提供了`plot_importance()`函数用于可视化。
- en: While `feature_importances_`, and similar mechanisms from other ML libraries,
    can be very useful, there are yet more advanced tools that we can use in order
    to assess feature importance, which I will describe next.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`feature_importances_`和其他机器学习库中的类似机制非常有用，但我们还可以使用更高级的工具来评估特征重要性，我将在下文中描述。
- en: Partial dependence plots
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 局部依赖性图
- en: '**Partial dependence plots** (**PDPs**) are graphical visualizations used to
    understand the relationship between an input feature (or a set of input features)
    and a model’s predicted outcome. With PDPs, we change the value of just one feature,
    while keeping the values of all other features constant, in order to determine
    how the different values of that particular feature impact the prediction. PDPs
    can also be used with more than one input feature at a time, which can reveal
    interactions among multiple features. There’s also an extended form of PDPs called
    **individual conditional expectation (ICE) plots**. While PDPs show the average
    effect of a feature on predictions, ICE plots display the effect of a feature
    on predictions for individual instances.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**部分依赖图**（**PDPs**）是用于理解输入特征（或一组输入特征）与模型预测结果之间关系的图形可视化。使用PDPs，我们只改变一个特征的值，同时保持所有其他特征的值不变，以确定该特定特征的各个值如何影响预测。PDPs也可以同时使用多个输入特征，这可以揭示多个特征之间的相互作用。还有PDPs的扩展形式，称为**个体条件期望（ICE）图**。虽然PDPs显示了特征对预测的平均影响，但ICE图显示了特征对单个实例预测的影响。'
- en: 'With PDPs, for every unique value of the feature of interest, the following
    high-level steps are performed:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PDPs，对于感兴趣特征的每个唯一值，都会执行以下高级步骤：
- en: Set the feature of interest to that value for every instance in the dataset.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集中每个实例的感兴趣特征设置为该值。
- en: Make predictions using the model.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型进行预测。
- en: Average the predictions.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 平均预测值。
- en: Plot the averaged predictions against the unique values of the feature.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将平均预测与特征的唯一值进行绘图。
- en: 'The following code uses the `PartialDependenceDisplay` attribute from scikit-learn
    to create and display a PDP graph:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用scikit-learn的`PartialDependenceDisplay`属性创建并显示PDP图形：
- en: '[PRE5]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This code will produce a graph similar to the one shown in *Figure 12**.4*:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将生成一个类似于*图12.4*所示的图形：
- en: '![Figure 12.4: PDP](img/B18143_12_4.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图12.4: PDP](img/B18143_12_4.jpg)'
- en: 'Figure 12.4: PDP'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4：PDP
- en: In the PDP shown in *Figure 12**.4*, the model appears to predict that people
    tend to start increasingly earning more money from the age of about 20 onward
    until they reach the age of around 60, after which their income begins to decrease.
    This is somewhat intuitive, considering that 20 years of age is considered early
    adulthood, and people often tend to retire in their 60s. Similarly, the model
    predicts that working more hours per week may result in higher income.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图12.4*所示的PDP中，模型似乎预测人们从大约20岁开始逐渐增加收入，直到他们达到大约60岁，之后他们的收入开始下降。这一点在一定程度上是直观的，因为20岁被认为是早期成年，人们通常在60多岁时退休。同样，模型预测每周工作更多小时可能会导致收入更高。
- en: Next, we move on to discuss more advanced feature importance and explanation
    mechanisms.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论更高级的特征重要性和解释机制。
- en: Local Interpretable Model-agnostic Explanations
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 局部可解释模型无关解释
- en: As I mentioned earlier in this chapter, some models are naturally more easily
    interpretable and understandable (and therefore explainable) than others. Examples
    provided were linear regression models and decision trees, versus large NNs with
    thousands, millions, or even billions of parameters (perhaps soon to be trillions!).
    **local interpretable model-agnostic explanations** (**LIME**) takes advantage
    of this fact, by training a simpler, **surrogate** model that can be more easily
    explained than the target model. The idea is that even if the overall model is
    complex and non-linear, it can be approximated well by a simpler, interpretable
    model.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如我在本章前面提到的，一些模型比其他模型更容易解释和理解（因此更容易解释）。提供的例子包括线性回归模型和决策树，与具有数千、数百万甚至数十亿参数的大型神经网络（可能很快就会达到万亿！）相比！**局部可解释模型无关解释**（**LIME**）利用这一事实，通过训练一个更简单、**代理**的模型，该模型比目标模型更容易解释。想法是，即使整体模型复杂且非线性，也可以通过一个更简单、可解释的模型很好地近似。
- en: 'LIME’s inner workings are quite complex, and I would recommend reading the
    original paper (*arXiv:1602.04938*) if you want to delve into that level of detail.
    Such algorithmic details are generally not required for the activities of a solutions
    architect, and you would mainly need to understand what LIME is used for without
    needing to dive into the academic details of its inner workings. Moving on, then,
    the following code provides an example of how to use LIME:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: LIME的内部工作原理相当复杂，如果您想深入了解该层面的细节，建议阅读原始论文(*arXiv:1602.04938*)。这类算法细节通常不是解决方案架构师活动所必需的，您只需了解LIME的用途，而不需要深入研究其内部工作的学术细节。那么，接下来，下面的代码提供了一个如何使用LIME的示例：
- en: '[PRE6]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The code will produce visualizations similar to those shown in *Figure 12**.5*:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 代码将生成与*图12.5*中所示类似的可视化效果：
- en: '![Figure 12.5: LIME outputs](img/B18143_12_5.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图12.5：LIME输出](img/B18143_12_5.jpg)'
- en: 'Figure 12.5: LIME outputs'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5：LIME输出
- en: At the top of *Figure 12**.5*, we see the model’s prediction for the instance
    that was used as input. In this case, since it’s a binary classification problem,
    it shows the class that the model predicted, along with the probability score
    associated with that prediction. We also see a horizontal bar chart representing
    the influence of various features on the prediction. Each bar represents a feature
    and its impact. The length of the bar indicates the significance of the influence,
    and its direction (left or right) indicates the direction of the influence (for
    example, toward the “<=50K” or “>50K” class). Each bar is labeled with the feature
    name and a small descriptor, which indicates how that feature was quantified for
    the specific instance being interpreted. This provides a clear indication of how
    the feature value for that specific instance influenced the prediction.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图12.5*的顶部，我们看到模型对输入实例的预测。在这种情况下，由于这是一个二元分类问题，它显示了模型预测的类别，以及与该预测相关的概率分数。我们还看到一个表示各种特征对预测影响的水平条形图。每个条形代表一个特征及其影响。条形的长度表示影响的重要性，其方向（左或右）表示影响的方向（例如，指向“<=50K”或“>50K”类别）。每个条形都标有特征名称和一个小描述符，这表明该特征是如何针对特定实例进行量化的。这提供了关于该特定实例的特征值如何影响预测的明确指示。
- en: It’s important to highlight that LIME’s explanations are local. They are specifically
    tailored to the instance in question and show how the model made its prediction
    for that one instance, not a general rule for all data. Next, we explore a mechanism
    that can help with both local and global model interpretations and is perhaps
    one of the most popular feature importance and explanation mechanisms in the industry.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 需要强调的是，LIME的解释是局部的。它们专门针对特定实例进行定制，展示了模型如何为该实例做出预测，而不是针对所有数据的通用规则。接下来，我们将探讨一种可以帮助解释局部和全局模型机制的机制，这可能是行业中最受欢迎的特征重要性和解释机制之一。
- en: SHapley Additive exPlanations
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SHapley Additive exPlanations
- en: '**Shapley Additive exPlanations** (**SHAP**) helps to explain ML model predictions
    using a concept called **Shapley values**. These values come from a field of mathematics
    referred to as **game theory** or, more specifically, **cooperative game theory**,
    and they were originally introduced in the 1950s by Lloyd Shapley. The study of
    game theory looks at competitive situations (called “games”) where the results
    of one player’s choices depend on what other players do. Cooperative game theory
    is a sub-branch that looks at games where players can make alliances or **coalitions**
    with other players and work together as a team to benefit the coalition as a whole,
    rather than just looking out for their own interests.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**Shapley Additive exPlanations** (**SHAP**)通过使用称为**Shapley值**的概念来帮助解释机器学习模型的预测。这些值来自被称为**博弈论**的数学领域，或者更具体地说，是**合作博弈论**，它们最初在20世纪50年代由Lloyd
    Shapley提出。博弈论的研究关注竞争性情况（称为“游戏”），其中一位玩家的选择结果取决于其他玩家的行为。合作博弈论是一个子分支，它关注玩家可以与其他玩家结盟或**联盟**，并作为一个团队共同努力，使联盟整体受益，而不仅仅是关注自己的利益。'
- en: Diving in further, let’s imagine a game in which the winner will receive a payout
    of $100\. People can either play the game individually, in which case an individual
    would simply receive $100 if they win, or they can form a team and work together
    to try to win the $100 payout. If three people form a team and they win the payout,
    in reality, it’s likely they would just split the payout into three equal parts.
    However, what if the payout should be **fairly** divided based on the contribution
    of each person? This is where Shapley values come into the picture.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步深入，让我们想象一个赢家将获得100美元奖金的游戏。人们可以单独玩游戏，在这种情况下，如果他们赢了，个人将直接获得100美元，或者他们可以组成团队共同努力争取100美元的奖金。如果三个人组成一个团队并赢得奖金，实际上，他们可能会将奖金平均分成三份。然而，如果奖金应该根据每个人的贡献**公平**地分配，那会怎样？这就是Shapley值发挥作用的地方。
- en: 'Shapley values represent the **average marginal contribution** of each player
    over all possible coalitions they could enter, as well as all possible orders
    in which they could enter each coalition. Again, striking a balance of how much
    detail to cover on this subject, I will not include the complex mathematical details
    here but would recommend reading the original paper if you would like to dive
    into those details, the formal reference for which is provided here:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Shapley值代表每个玩家在所有可能的联盟中以及他们可能进入每个联盟的所有可能顺序中的**平均边际贡献**。再次强调，关于这个主题的细节覆盖，我将不会在这里包括复杂的数学细节，但如果您想深入了解这些细节，可以阅读原始论文，其正式参考文献如下：
- en: '*Shapley, L.S. (1953). A Value for n-Person Games. In “Contributions to the
    Theory of Games volume II”, H.W. Kuhn and A.W. Tucker (eds.), Princeton* *University
    Press.*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*Shapley, L.S. (1953). A Value for n-Person Games. In “Contributions to the
    Theory of Games volume II”, H.W. Kuhn and A.W. Tucker (eds.), Princeton* *University
    Press.*'
- en: Here, we will focus on how Shapley values are used in the context of ML model
    explainability. We begin with the concept of an **average prediction**, which
    represents the average of all predictions our model makes over our entire dataset.
    For regression models, this is simply the mean of all predicted outputs. For classification
    models, it is the average predicted probability for a given class over the entire
    dataset.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将关注Shapley值在机器学习模型可解释性背景下的应用。我们首先从**平均预测**的概念开始，它代表我们的模型在整个数据集上做出的所有预测的平均值。对于回归模型，这仅仅是所有预测输出的平均值。对于分类模型，它是整个数据集上给定类别的平均预测概率。
- en: When computing Shapley values for a particular instance (that is, a row of input
    data), the contributions of each feature are measured with respect to how they
    move the prediction for that instance away from the average prediction. The Shapley
    value for a feature then captures its average marginal contribution over all possible
    combinations of features. For example, consider a binary classification model
    that predicts whether a bank loan will be defaulted on. If, on average, the model
    predicts a 5% probability of default over the entire dataset, this 5% would be
    the “average prediction.” Now, for a particular loan application, if the model
    predicts a 20% probability of default, Shapley values help attribute this 15%
    deviation from the average to each feature in the input (for example, the applicant’s
    income, employment status, credit score, and so on). Each feature’s Shapley value
    will indicate how much, on average, that feature contributes to the deviation
    of the prediction from the average prediction.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算特定实例（即输入数据的一行）的Shapley值时，每个特征的贡献是通过它们如何将该实例的预测移动到平均预测来衡量的。然后，该特征的Shapley值捕捉了它在所有可能的特征组合中的平均边际贡献。例如，考虑一个二元分类模型，该模型预测银行贷款是否会违约。如果平均而言，该模型在整个数据集上预测违约概率为5%，那么这个5%就是“平均预测”。现在，对于特定的贷款申请，如果模型预测违约概率为20%，Shapley值有助于将这个平均预测的15%偏差归因于输入中的每个特征（例如，申请人的收入、就业状况、信用评分等）。每个特征的Shapley值将表明该特征平均对预测偏差的贡献有多大。
- en: 'The following code provides an example of how to get feature importance insights
    using the SHAP library:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码提供了一个使用SHAP库获取特征重要性洞察的示例：
- en: '[PRE7]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the code, a `TreeExplainer` object from the SHAP library is being created.
    This specific explainer is optimized for tree-based models, such as decision trees,
    random forests, and gradient boosted trees. The `clf` instance that we pass to
    it is a tree-based model that we’ve trained. Once the explainer is created, we
    use the `.shap_values()` method to compute the SHAP values for each sample in
    our `X_test` dataset. We then visualize the average impact of each feature on
    the model’s predictions by using a bar plot (the longer the bar, the greater the
    feature’s importance). The code will produce a graph similar to the one shown
    in *Figure 12**.6*:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们正在创建 SHAP 库中的 `TreeExplainer` 对象。这个特定的解释器针对基于树的模型进行了优化，例如决策树、随机森林和梯度提升树。我们传递给它的
    `clf` 实例是我们训练的基于树的模型。一旦创建了解释器，我们就使用 `.shap_values()` 方法计算 `X_test` 数据集中每个样本的 SHAP
    值。然后，我们通过使用条形图（条形越长，特征的重要性就越大）来可视化每个特征对模型预测的平均影响。代码将生成类似于 *图 12**.6* 中所示的图形：
- en: '![Figure 12.6: SHAP outputs](img/B18143_12_6.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.6：SHAP 输出](img/B18143_12_6.jpg)'
- en: 'Figure 12.6: SHAP outputs'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6：SHAP 输出
- en: As we can see in *Figure 12**.6*, perhaps surprisingly, a person’s marital status
    appears to have the most impact on the model’s output. Considering that the SHAP
    value for a feature is the average contribution of that feature value to every
    possible prediction (averaged over all instances), it takes into account intricate
    interactions with other features, as well as the impact of the feature itself.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在 *图 12**.6 中所见，也许令人惊讶的是，一个人的婚姻状况似乎对模型的输出影响最大。考虑到一个特征的 SHAP 值是该特征值对每个可能预测的平均贡献（对所有实例进行平均），它考虑了与其他特征的复杂交互，以及特征本身的影响。
- en: While we can import tools such as SHAP into our notebooks, we can also use Vertex
    AI APIs to get explanations directly from models hosted in Vertex AI. The next
    section describes how to do this.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以将 SHAP 等工具导入我们的笔记本中，但我们也可以使用 Vertex AI API 直接从 Vertex AI 中托管的模型中获取解释。下一节将描述如何做到这一点。
- en: Getting explanations from a deployed model in Vertex AI
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从 Vertex AI 中部署的模型获取解释
- en: 'Conveniently, Vertex AI provides APIs and an SDK that we can use to get explanations
    from our models. In the Jupyter notebook that accompanies this chapter, we use
    the `projects.locations.endpoints.explain` API to get explanations from the model
    that we deployed in our MLOps pipeline in the previous chapter. The following
    is a snippet of the code we use to do so:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 便利的是，Vertex AI 提供了我们可以使用的 API 和 SDK，以便从我们的模型中获取解释。在本章所附的 Jupyter 笔记本中，我们使用 `projects.locations.endpoints.explain`
    API 从我们在上一章 MLOps 管道中部署的模型中获取解释。以下是我们用于此目的的代码片段：
- en: '[PRE8]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The code will produce an output similar to the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 代码将生成类似于以下内容的输出：
- en: '[PRE9]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The fields in the response can be interpreted as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 响应中的字段可以这样解释：
- en: '`baseline_output_value`: This is the model’s output value for the baseline
    instance. A baseline is a reference point (such as an average or neutral instance)
    against which the prediction for our instance of interest is compared. The difference
    in the model’s output between the instance of interest and the baseline helps
    us understand the contributions of each feature.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`baseline_output_value`: 这是模型对基线实例的输出值。基线是一个参考点（例如平均值或中性实例），我们将其与对我们感兴趣实例的预测进行比较。模型在感兴趣实例和基线之间的输出差异有助于我们理解每个特征的贡献。'
- en: '`instance_output_value`: This is the model’s output value for the instance
    we passed in for explanation. In the context of a binary classifier, this can
    be interpreted as the probability of the instance belonging to the positive class.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`instance_output_value`: 这是模型为我们传入的解释实例的输出值。在二元分类器的上下文中，这可以解释为实例属于正类的概率。'
- en: '`feature_attributions_dict`:'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_attributions_dict`:'
- en: '`''dense_input''`: This is the name of the input tensor to the model.'
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''dense_input''`: 这是模型输入张量的名称。'
- en: 'The list of numbers represents the importance or attribution of each corresponding
    feature in the input for the given prediction. The length of this list matches
    the number of features in our model’s input:'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这串数字表示了给定预测中每个对应特征在输入中的重要性或归因。此列表的长度与模型输入中的特征数量相匹配：
- en: Each number represents the marginal contribution of that feature toward the
    model’s prediction for the specific instance we’re explaining, relative to the
    baseline. In other words, how much did this feature move the prediction from the
    average/baseline prediction?
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个数代表该特征对特定实例的模型预测的边际贡献，相对于基线。换句话说，这个特征将预测从平均/基线预测移动了多少？
- en: Positive values indicate that the feature pushed the model’s output in the positive
    class’s direction. For binary classification, this usually means it made the model
    more confident in classifying the instance as the positive class.
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正值表示该特征推动了模型输出向正类方向。对于二分类，这通常意味着它使模型更有信心将实例分类为正类。
- en: Negative values indicate that the feature pushed the model’s output in the negative
    class’s direction.
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负值表示该特征推动了模型输出向负类方向。
- en: Zero or close to zero suggests that the feature didn’t have a significant impact
    on the prediction for this particular instance.
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零或接近零表示该特征对特定实例的预测没有产生显著影响。
- en: '`approximation_error`: This is the error in the approximation used to compute
    attribution values. Explanation methods often use approximations to compute attributions.
    The approximation error gives an idea of the confidence we can have in the attribution
    values (a smaller error typically indicates more reliable attributions).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`approximation_error`: 这是在计算归因值时使用的近似误差。解释方法通常使用近似来计算归因。近似误差给出了我们对归因值可以有多少信心（通常情况下，较小的误差表示更可靠的归因）。'
- en: '`output_name`: This is the name of the model’s output tensor.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_name`: 这是模型输出张量的名称。'
- en: 'Congratulations! You have successfully retrieved an explanation for an input
    sent to a model hosted in Vertex AI. To dive into more detail on Vertex Explainable
    AI, you can reference its documentation at the following link: [https://cloud.google.com/vertex-ai/docs/explainable-ai/overview](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功检索到发送到Vertex AI托管模型的输入的解释。要深入了解Vertex可解释AI，您可以参考以下链接中的文档：[https://cloud.google.com/vertex-ai/docs/explainable-ai/overview](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview)。
- en: So far, we’ve talked about getting and using explanations to understand our
    models. What if we found that some of our model’s predictions were unfair? What
    kinds of actions could we take to counteract that? One type of explanation mechanism
    we can use for this purpose is **counterfactual explanations**, which we explore
    next.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了获取和使用解释来理解我们的模型。如果我们发现我们的模型的一些预测是不公平的，那会怎样？我们可以采取哪些行动来对抗这种情况？我们可以用于此目的的一种解释机制是**反事实解释**，我们将在下一部分探讨。
- en: Counterfactual explanations
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反事实解释
- en: Counterfactual explanations revolve around the question, “What would need to
    change in my input data to alter the decision of a predictive model?” They describe
    a hypothetical alternative to an observed outcome that would have occurred if
    certain conditions were met. This could be a minimal change in input features
    that would change the prediction to a specified output.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实解释围绕的问题是，“我的输入数据需要改变什么才能改变预测模型的决策？”它们描述了一个假设的替代方案，如果满足某些条件，就会发生观察到的结果。这可能是在输入特征中发生的最小变化，从而将预测改变为指定的输出。
- en: 'Going back to our loan approval as an example, suppose an applicant, John,
    is denied a loan based on features such as his income, credit score, and employment
    history. A counterfactual explanation might tell John: “If your income was $10,000
    higher, you would have been approved for the loan.”'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的贷款批准的例子，假设一个申请人John因为他的收入、信用评分和就业历史等特征被拒绝贷款。一个反事实解释可能会告诉John：“如果你的收入高出10,000美元，你将获得贷款批准。”
- en: Counterfactual explanations are important for many reasons. They help individuals
    affected by a model’s prediction to understand why a decision was made. They help
    data scientists understand how to augment their models based on various criteria,
    and they are also important for regulatory compliance reasons.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实解释有多个重要原因。它们帮助受模型预测影响的个人理解为什么做出了某个决定。它们帮助数据科学家了解如何根据各种标准增强他们的模型，并且对于合规性也很重要。
- en: To find a counterfactual, we need to define a distance measure in the feature
    space. The goal is often to find the counterfactual instance that’s closest to
    the original instance but results in a different prediction. This is often framed
    as an optimization problem, where the objective is to minimize the distance between
    the original instance and its counterfactual, while still resulting in a different
    prediction.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到一个反事实，我们需要在特征空间中定义一个距离度量。目标通常是找到与原始实例最接近的反事实实例，但结果却产生不同的预测。这通常被表述为一个优化问题，其目标是使原始实例与其反事实之间的距离最小化，同时仍然产生不同的预测。
- en: Bear in mind that for some models, especially **deep NNs** (**DNNs**), finding
    counterfactuals can be computationally challenging. It’s also important to note
    that counterfactuals might suggest changes that are impossible or very hard to
    achieve in real life, so their real-world feasibility needs to be evaluated. In
    the Jupyter notebook that accompanies this chapter, we perform some simple counterfactual
    processing. Bear in mind that counterfactuals are a highly complex topic in a
    quickly evolving field.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，对于某些模型，特别是**深度神经网络**（**DNNs**），找到反事实可能具有计算上的挑战。还重要的是要注意，反事实可能建议在现实生活中不可能或非常难以实现的变化，因此需要评估其现实世界的可行性。在本章附带的Jupyter笔记本中，我们执行了一些简单的反事实处理。请记住，反事实是一个在快速发展的领域中高度复杂的话题。
- en: Next, let’s look at some additional mechanisms for reducing bias.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看一些减少偏差的额外机制。
- en: Reducing bias and enhancing fairness
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 减少偏差和增强公平性
- en: In this section, we discuss proactive steps we can use to reduce bias and enhance
    fairness in our datasets and ML models in each phase of the model development
    life cycle.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了我们可以采取的主动措施，以在每个模型开发生命周期阶段减少数据集和机器学习模型中的偏差并增强公平性。
- en: Starting with the data
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从数据开始
- en: As with anything in the field of data science, the data is usually a good place
    to start. If available, we should gather more data and ensure that the data is
    as balanced as possible. During data preprocessing, we can also mitigate bias
    by using resampling techniques to adjust the representation of underrepresented
    groups in the training data, either by oversampling minority groups or undersampling
    majority groups. During feature engineering, we can create or modify features
    to reduce their potential discriminatory effect.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 就像数据科学领域的任何事物一样，数据通常是开始的好地方。如果可能的话，我们应该收集更多的数据并确保数据尽可能平衡。在数据预处理期间，我们还可以通过使用重采样技术来调整训练数据中代表性不足的群体的表示，例如通过过采样少数群体或欠采样多数群体来减轻偏差。在特征工程期间，我们还可以创建或修改特征以减少其潜在的歧视性影响。
- en: During model training
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在模型训练期间
- en: 'During the model training process, we could introduce **fairness constraints**
    to ensure equal opportunity by making sure both protected and non-protected groups
    have equal positive and negative rates, for example. There are numerous fairness
    metrics and constraints for this purpose, such as the following:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练过程中，我们可以引入**公平约束**，通过确保受保护群体和非受保护群体具有相等的正率和负率来确保平等机会。为此目的，有大量的公平性指标和约束，例如以下内容：
- en: '**Demographic parity** or **statistical parity**, which requires that the probability
    of a positive outcome should be the same across the different groups'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人口统计学平等**或**统计平等**，要求不同群体中积极结果的概率应该相同'
- en: '**Equal opportunity**, which mandates equality in true positive rates across
    different groups'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平等机会**，要求不同群体中真实正率相等'
- en: '**Equalized odds**, which extends equal opportunity by requiring both true
    positive rates and false positive rates to be equal across groups'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均衡机会**，通过要求真实正率和假正率在各个群体中相等来扩展平等机会'
- en: '**Treatment equality**, which requires the ratio of false negatives to false
    positives to be equal across groups'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**治疗平等**，要求不同群体中假阴性率与假正率的比例相等'
- en: Fairness constraints can be implemented as a type of regularization during model
    training. There are even some fairness-aware algorithms we could use that are
    explicitly designed to handle fairness concerns, such as **Fair k-Means**.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 公平约束可以在模型训练期间作为正则化的一种类型来实现。甚至还有一些专门设计来处理公平问题的公平感知算法，例如**公平k均值**算法。
- en: Postprocessing
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后处理
- en: There are also some steps we can take during postprocessing, such as adjusting
    decision thresholds for different groups to ensure fairness metrics such as equal
    opportunity or demographic parity, and we could also adjust model predictions
    to ensure they are fair across groups.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在后处理期间，我们还可以采取一些步骤，例如调整不同群体的决策阈值，以确保公平性指标，如平等机会或人口统计平衡，我们还可以调整模型预测，以确保它们在各个群体中都是公平的。
- en: It is, of course, also important that we continuously monitor for fairness concerns
    in real-world predictions and retrain models as necessary. In critical decision-making
    scenarios, we could consider having a human in the loop to audit or override model
    decisions.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，持续监控现实世界预测中的公平性问题并在必要时重新训练模型也非常重要。在关键决策场景中，我们可以考虑让人类参与审计或覆盖模型决策。
- en: Note that fairness enhancement methods might lead to a trade-off with model
    accuracy. The challenge is often to find a balance between fairness and accuracy
    that’s acceptable for the given application. It’s also crucial to understand which
    fairness metric is most relevant to your specific problem, as different fairness
    metrics can sometimes be in conflict with each other.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，公平性增强方法可能会导致模型准确性的权衡。挑战通常在于找到公平性和准确性之间的平衡，这对于给定的应用来说是可接受的。了解哪个公平性指标与您具体问题最相关也非常关键，因为不同的公平性指标有时可能会相互冲突。
- en: To wrap up this section on explainability and fairness, let’s take a brief look
    at some other libraries we can use for assessing and implementing these concepts.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结本节关于可解释性和公平性的内容，让我们简要地看看一些我们可以用来评估和实施这些概念的库。
- en: Additional libraries
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他库
- en: Fortunately, there is an ever-growing list of libraries being developed for
    the purpose of assessing and promoting explainability and fairness. This section
    describes examples of such libraries.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一个不断增长的库列表正在开发中，目的是评估和促进可解释性和公平性。本节描述了此类库的示例。
- en: What-if Tool
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: What-if Tool
- en: The **What-if Tool** (**WIT**) was originally developed by Google and is an
    early explainability tool with a visual interface that allows the inspection of
    a model’s predictions, comparison across different models, and examination of
    potential bias. It is relatively easy to use and does not require much coding,
    and it includes support for many of the concepts we discussed in this chapter,
    such as counterfactuals and PDPs.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**What-if Tool**（**WIT**）最初由Google开发，是一个早期的可解释性工具，具有可视化界面，允许检查模型的预测、比较不同模型以及检查潜在的偏差。它相对容易使用，不需要太多编码，并且包括了对我们在本章中讨论的许多概念的支持，例如反事实和PDPs。'
- en: AI Fairness 360
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AI Fairness 360
- en: IBM’s **AI Fairness 360** (**AIF360**) is an open source library that includes
    a set of fairness metrics for datasets and models, and algorithms for mitigating
    bias. It can be used to provide detailed explanations to understand fairness metrics
    and their implications in a given context, and it enables users to visually explore
    bias in their datasets and models. It can also help in identifying if a trained
    model is producing biased outcomes and provides some tools to help mitigate bias
    in each phase of the model development life cycle, such as preprocessing, training,
    and postprocessing.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: IBM的**AI Fairness 360**（**AIF360**）是一个开源库，包括一组用于数据集和模型的公平性指标以及减轻偏差的算法。它可以用来提供详细的解释，以理解公平性指标及其在特定环境中的含义，并使用户能够可视化地探索数据集和模型中的偏差。它还可以帮助识别训练好的模型是否产生了偏差的结果，并提供一些工具来帮助在每个模型开发生命周期阶段（如预处理、训练和后处理）减轻偏差。
- en: EthicalML/XAI
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: EthicalML/XAI
- en: This is an open source Python library designed to support interpretable ML and
    responsible AI. It includes tools for preprocessing, model interpretability, bias
    detection, and visualization, and it supports concepts we discussed in this chapter,
    such as feature importance, Shapley values, LIME, and DIA.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个开源的Python库，旨在支持可解释的机器学习和负责任的AI。它包括预处理、模型可解释性、偏差检测和可视化的工具，并支持我们在本章中讨论的概念，例如特征重要性、Shapley值、LIME和DIA。
- en: Fairlearn
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Fairlearn
- en: Fairlearn is another open source Python-based project that aims to help data
    scientists improve the fairness of AI systems. It includes algorithms for mitigating
    unfairness in classification and regression models, and fairness metrics for comparison.
    Its primary goal is to help ML practitioners reduce unfair disparities in predictions
    by understanding metrics and algorithms, and it provides an interactive UI experience
    for model assessment and comparison, which includes fairness metrics and an assessment
    dashboard. It again supports mitigation techniques in various phases of the ML
    model development life cycle, such as preprocessing, training, and postprocessing.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Fairlearn 是另一个基于 Python 的开源项目，旨在帮助数据科学家提高人工智能系统的公平性。它包括用于减轻分类和回归模型不公平性的算法，以及用于比较的公平性指标。其主要目标是帮助机器学习从业者通过理解指标和算法来减少预测中的不公平差异，并提供用于模型评估和比较的交互式用户界面体验，这包括公平性指标和评估仪表板。它再次支持在机器学习模型开发生命周期的各个阶段使用缓解技术，例如预处理、训练和后处理。
- en: 'There are many more explainability and fairness libraries in addition to the
    ones mentioned here, but these ones are particularly popular libraries that are
    currently available in the industry. Google Cloud has recently launched more specific
    model evaluation mechanisms and metrics for fairness. At the time of writing this
    in October 2023, these mechanisms are still in preview mode and are not yet generally
    available. You can find out more about these features in the documentation at
    the following link: [https://cloud.google.com/vertex-ai/docs/evaluation/intro-evaluation-fairness](https://cloud.google.com/vertex-ai/docs/evaluation/intro-evaluation-fairness)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这里提到的之外，还有许多其他可解释性和公平性库，但这些是当前在行业中特别受欢迎的库。谷歌云最近推出了更具体的模型评估机制和公平性指标。在撰写本文的
    2023 年 10 月时，这些机制仍处于预览模式，尚未普遍可用。您可以在以下链接的文档中了解更多关于这些功能的信息：[https://cloud.google.com/vertex-ai/docs/evaluation/intro-evaluation-fairness](https://cloud.google.com/vertex-ai/docs/evaluation/intro-evaluation-fairness)
- en: A note on generative AI
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 关于生成式人工智能的说明
- en: '*Chapters 14* through *17* in this book are dedicated to **generative AI**
    (**GenAI**), which is a relatively new subset of AI/ML. In those chapters, we
    will explore the concepts of **large language models** (**LLMs**) and how they
    differ from other types of ML models that we’ve already covered in this book.
    LLMs are typically trained on extremely large datasets and therefore acquire vast
    amounts of knowledge that can be applied to many different kinds of use cases.
    In those later chapters, we will learn how LLMs can be used as auto-raters to
    open up new kinds of evaluation techniques for ML models, including specific evaluations
    focusing on bias, explainability, and fairness.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 本书第 14 章至第 17 章专门介绍**生成式人工智能**（**GenAI**），这是 AI/ML 的一个相对较新的子集。在这些章节中，我们将探讨**大型语言模型**（**LLMs**）的概念以及它们与我们已在本书中介绍的其他类型机器学习模型的不同之处。LLMs
    通常在极其庞大的数据集上训练，因此获得了可以应用于许多不同类型用例的大量知识。在这些后续章节中，我们将学习如何将 LLMs 用作自动评分员，为机器学习模型开辟新的评估技术，包括针对偏差、可解释性和公平性的特定评估。
- en: A final topic for us to explore in this chapter is the concept of lineage tracking.
    In the next section, we delve into this topic in detail and assess its importance
    in the context of explainability and fairness.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 本章最后要探讨的一个主题是谱系追踪的概念。在下一节中，我们将详细探讨这个主题，并评估其在可解释性和公平性背景下的重要性。
- en: The importance of lineage tracking in ML model development
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在机器学习模型开发中追踪谱系的重要性
- en: 'We’ve touched on the concept of lineage tracking in earlier chapters, and now
    we will explore it in more detail. When we talk about lineage tracking, we’re
    referring to tracking all of the steps and artifacts that were used to create
    a given ML model. This includes items such as the following:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面章节中提到了谱系追踪的概念，现在我们将更详细地探讨它。当我们谈论谱系追踪时，我们指的是追踪创建给定机器学习模型所使用的所有步骤和工件。这包括以下项目：
- en: The source datasets
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源数据集
- en: All transformations that were performed on those datasets
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对这些数据集执行的所有转换
- en: All intermediate datasets that were created
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建的所有中间数据集
- en: Which algorithm was used to train a model on the resulting data
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在结果数据上训练模型所使用的算法
- en: Which hyperparameters and values were used during model training
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型训练过程中使用了哪些超参数和值
- en: Which platform and tools were used in the training
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中使用了哪些平台和工具
- en: If a hyperparameter tuning job was used, details of that job
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果使用了超参数调整作业，该作业的详细信息
- en: Details of any evaluation steps performed on the resulting model
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对生成的模型执行的任何评估步骤的详细信息
- en: If the model is being served for online inference, details of the endpoint at
    which the model is hosted
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型正在为在线推理提供服务，则模型托管端点的详细信息
- en: The preceding list is not exhaustive. We generally want to track every step
    that was used to create a model and all of the inputs and outputs in each step.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 上述列表并不全面。我们通常希望追踪创建模型所使用的每个步骤以及每个步骤中的所有输入和输出。
- en: Why do we need to do this? Lineage tracking is important for many reasons. It
    complements the concept of explainability. While lineage tracking by itself will
    not necessarily explain why a model behaves in a particular manner, it is certainly
    important for researchers to understand how the model was created. It’s also important
    for reproducibility and collaboration. We’ve talked about the complexity some
    companies encounter when they need to manage thousands of ML models created by
    many different teams. If a model is behaving problematically, understanding its
    lineage will help in troubleshooting. If one team wants to build on the work another
    team has already performed, such as a model they have trained or datasets they
    have created, understanding the lineage of those artifacts will help the consuming
    team to be more productive in those endeavors. Also, in order to continually enhance
    a model’s performance, we need to know how that model was created. Lineage is
    also important, and sometimes required, for governance and compliance reasons.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要这样做？血缘追踪对于许多原因都很重要。它补充了可解释性的概念。虽然血缘追踪本身不一定能解释为什么模型以特定方式表现，但对于研究人员了解模型是如何创建的非常重要。它对于可重复性和协作也很重要。我们已经讨论了一些公司在需要管理由许多不同团队创建的数千个机器学习模型时遇到的复杂性。如果一个模型表现有问题，了解其血缘将有助于故障排除。如果一个团队想要基于另一个团队已经完成的工作构建，例如他们已经训练的模型或他们创建的数据集，了解这些工件的血缘将帮助消费团队在这些努力中更加高效。此外，为了持续提升模型的表现，我们需要知道该模型是如何创建的。血缘对于治理和合规也很重要，有时是必需的。
- en: Fortunately, Google Cloud provides tools that help us to track lineage. For
    example, Dataplex can be used to track data lineage, and the Vertex ML Metadata
    service can help us track all steps and artifacts in our ML model development
    life cycle. Next, we will take a look at Vertex ML Metadata in more detail; let’s
    first start with some terminology used by the Vertex ML Metadata service.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Google Cloud 提供了帮助我们追踪血缘的工具。例如，Dataplex 可以用来追踪数据血缘，而 Vertex ML 元数据服务可以帮助我们追踪我们机器学习模型开发生命周期中的所有步骤和工件。接下来，我们将更详细地了解
    Vertex ML 元数据；让我们首先从 Vertex ML 元数据服务使用的术语开始。
- en: ML metadata service terminology
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习元数据服务术语
- en: '**Executions** represent the steps or operations in an ML workflow, such as
    data preprocessing, model training, or evaluation. **Artifacts** represent the
    inputs and outputs of each step, such as datasets, models, or evaluation metrics.
    **Events** represent the relationships between executions and artifacts, such
    as “Artifact X was produced by Execution Y” or “Artifact X was used as an input
    by Execution Y.” Events help us to establish lineage data by associating artifacts
    and executions with each other. **Contexts** represent logical groupings that
    bundle related artifacts and executions together. An example of a context would
    be a specific pipeline run or a model version.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**执行**代表机器学习工作流中的步骤或操作，例如数据预处理、模型训练或评估。**工件**代表每个步骤的输入和输出，例如数据集、模型或评估指标。**事件**代表执行和工件之间的关系，例如“工件
    X 由执行 Y 生成”或“工件 X 被执行 Y 作为输入使用。”事件通过将工件和执行相互关联，帮助我们建立血缘数据。**上下文**代表将相关工件和执行捆绑在一起的逻辑分组。一个上下文的例子可能是一个特定的管道运行或模型版本。'
- en: Collectively, all of the aforementioned resources are referred to as **metadata
    resources**, and they are described by a **MetadataSchema**, which describes the
    schema for particular types of metadata resources. In addition to the predefined
    metadata resources, we can also store custom metadata in the Vertex ML Metadata
    service. All tracked metadata is stored in a **MetadataStore**, and all of this
    information can be used to create a **Lineage Graph**, which is a visual representation
    that connects artifacts, executions, and contexts, and shows the relationships
    and flow between them.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 所有上述资源统称为**元数据资源**，并由**MetadataSchema**描述，该Schema描述了特定类型的元数据资源的模式。除了预定义的元数据资源外，我们还可以在Vertex
    ML元数据服务中存储自定义元数据。所有跟踪的元数据都存储在**MetadataStore**中，所有这些信息都可以用来创建**谱系图**，这是一个连接工件、执行和上下文，并显示它们之间关系和流的视觉表示。
- en: Note that, as with most Google Cloud resources, access to the metadata resources
    can be controlled using Google Cloud **Identity and Access Management** (**IAM**),
    which is important for security and compliance reasons.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与大多数Google Cloud资源一样，可以通过Google Cloud **身份和访问管理**（**IAM**）来控制对元数据资源的访问，这对于安全和合规性非常重要。
- en: Now that we’ve covered the main terminology and concepts, let’s start reviewing
    some metadata in Vertex AI.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了主要术语和概念，让我们开始回顾一些Vertex AI中的元数据。
- en: Lineage tracking in Vertex AI
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Vertex AI中的谱系跟踪
- en: To explore the lineage tracking features in Vertex AI, we will use the MLOps
    pipeline we built in [*Chapter 11*](B18143_11.xhtml#_idTextAnchor288) as an example.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 要探索Vertex AI中的谱系跟踪功能，我们将使用我们在[*第11章*](B18143_11.xhtml#_idTextAnchor288)中构建的MLOps管道作为示例。
- en: 'In the Google Cloud console, perform the following steps:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在Google Cloud控制台中，执行以下步骤：
- en: Navigate to **Vertex AI** > **Pipelines**.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到**Vertex AI** > **Pipelines**。
- en: Click on the name of the pipeline run that was created in [*Chapter 11*](B18143_11.xhtml#_idTextAnchor288)
    (it should be the most recent pipeline run unless you have run other pipelines
    in this Google Cloud project in the meantime).
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击在[*第11章*](B18143_11.xhtml#_idTextAnchor288)中创建的管道运行名称（除非在此期间你在该Google Cloud项目中运行了其他管道，否则它应该是最近的管道运行）。
- en: You’ll see the execution graph for the pipeline run.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将看到管道运行的执行图。
- en: 'At the top of the screen, click the toggle button to the left of the words
    **Expand Artifacts** (see *Figure 12**.7* for reference):'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕顶部，点击**展开工件**（**Expand Artifacts**）左侧的切换按钮（参见**图12**.7以获取参考）：
- en: '![Figure 12.7: Expand Artifacts](img/B18143_12_7.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图12.7：展开工件](img/B18143_12_7.jpg)'
- en: 'Figure 12.7: Expand Artifacts'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7：展开工件
- en: We can now start exploring the metadata related to each of the steps and artifacts
    in our pipeline.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以开始探索与管道中每个步骤和工件相关的元数据。
- en: You’ll also notice that the **Pipeline run analysis** section on the right side
    of the screen contains lots of information about this pipeline run. The **SUMMARY**
    tab provides information about the pipeline run itself, including the parameters
    that were used as inputs. Those are the parameters we defined in our pipeline
    definition in [*Chapter 11*](B18143_11.xhtml#_idTextAnchor288).
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还会注意到，屏幕右侧的**管道运行分析**部分包含大量关于此管道运行的信息。**摘要**（**SUMMARY**）选项卡提供了有关管道运行本身的信息，包括用作输入的参数。这些是我们[*第11章*](B18143_11.xhtml#_idTextAnchor288)中管道定义中定义的参数。
- en: 'We can click on elements in the pipeline execution graph in order to see metadata
    related to that specific element. Let’s start right at the beginning. We want
    to know which dataset was used as an initial input to our pipeline. Considering
    that the first step in our pipeline is the data preprocessing step, and that step
    fetches the dataset, click on the preprocessing step, and its metadata will be
    shown on the right side of the screen, as depicted in *Figure 12**.8*:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以点击管道执行图中的元素，以查看与该特定元素相关的元数据。让我们从开始的地方开始。我们想知道哪个数据集被用作管道的初始输入。考虑到我们的管道中的第一步是数据预处理步骤，并且该步骤检索数据集，点击预处理步骤，其元数据将显示在屏幕右侧，如图**图12**.8所示：
- en: '![Figure 12.8: Preprocessing step details](img/B18143_12_8.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图12.8：预处理步骤详情](img/B18143_12_8.jpg)'
- en: 'Figure 12.8: Preprocessing step details'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8：预处理步骤详情
- en: The green arrow in *Figure 12**.8* is pointing to the `source_dataset` input
    parameter, which provides the path to the source dataset (the actual details are
    redacted in the image in order to obscure my bucket name).
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**图12**.8中，绿色的箭头指向`source_dataset`输入参数，它提供了源数据集的路径（为了隐藏我的存储桶名称，图像中的实际细节已被删除）。
- en: We can also see the `preprocessed_data_path` parameter value, which provides
    the path to the folder in which the preprocessing script will store the resulting
    processed data. If you scroll down (not shown in the screenshot), you will also
    see the `main_python_file_uri` parameter value, which provides the path to the
    PySpark script that we used in the preprocessing step in our pipeline. In fact,
    if we click on the **VIEW JOB** button, we can view the details of the actual
    Serverless Spark job that was used to execute our script in Google Cloud Dataproc,
    including its execution logs.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以看到`preprocessed_data_path`参数的值，它提供了预处理脚本将存储结果处理数据的文件夹路径。如果你向下滚动（截图未显示），你也会看到`main_python_file_uri`参数的值，它提供了我们用于管道预处理步骤的PySpark脚本的路径。实际上，如果我们点击**查看作业**按钮，我们可以查看用于在Google
    Cloud Dataproc上执行我们的脚本的实际无服务器Spark作业的详细信息，包括其执行日志。
- en: Now we’ve successfully tracked our source dataset, the script and job that performed
    transformations on that dataset, and the resulting processed dataset that was
    used to train our model, let’s move to the next step in our pipeline, which is
    the model training step.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经成功追踪了我们的源数据集、对数据集进行转换的脚本和作业，以及用于训练我们的模型的处理后的数据集，接下来让我们转向管道中的下一个步骤，即模型训练步骤。
- en: 'Click on the `custom-training-job` step in our pipeline execution graph. In
    the information panel on the right, perhaps the most important parameter is the
    `worker_pool_specs` parameter. As depicted in *Figure 12**.9*, this parameter
    provides a lot of information about how our model is trained, such as the dataset
    that was used for training (which is the output of the previous, preprocessing
    step), the location at which the trained model artifacts are saved, the container
    image that was used to run our custom training code, the hyperparameter values
    used during training, as well as the machine type and number of machines that
    were used by the training job:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的管道执行图中点击`custom-training-job`步骤。在右侧的信息面板中，可能最重要的参数是`worker_pool_specs`参数。如图*图12.9*所示，该参数提供了关于我们的模型如何训练的大量信息，例如用于训练的数据集（这是前一个预处理步骤的输出），训练模型工件保存的位置，用于运行自定义训练代码的容器镜像，训练期间使用的超参数值，以及训练作业使用的机器类型和机器数量：
- en: '![Figure 12.9: worker_pool_specs](img/B18143_12_9.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图12.9：worker_pool_specs](img/B18143_12_9.jpg)'
- en: 'Figure 12.9: worker_pool_specs'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.9：worker_pool_specs
- en: Again, if we click on the **VIEW JOB** button at the top of the screen, we can
    see the actual job that ran on the Vertex AI training service to train our model,
    as well as the execution logs for that job.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，如果我们点击屏幕顶部的**查看作业**按钮，我们可以看到在Vertex AI训练服务上运行的实际作业，以及该作业的执行日志。
- en: Because we used a custom script to train our model and simply saved the artifacts
    in Google Cloud Storage, at this point in the pipeline, our model is referred
    to as an `importer` job to import our model artifact for that purpose.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为我们使用自定义脚本来训练我们的模型，并将工件简单地保存在Google Cloud Storage中，在这个管道阶段，我们的模型被称为`importer`作业，用于导入我们的模型工件。
- en: The `model-upload` step in the pipeline is what registers our model in the Vertex
    AI Model Registry. If you click on that step in the execution graph and review
    its metadata, in the **Output Parameters** section, you will see the URI for the
    resulting resource in the Vertex AI Model Registry.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管道中的`model-upload`步骤是将我们的模型注册到Vertex AI模型注册表中的步骤。如果你在执行图中点击该步骤并查看其元数据，在**输出参数**部分，你会看到Vertex
    AI模型注册表中结果资源的URI。
- en: The remaining steps, `endpoint-create` and `model-deploy`, have similar formats.
    As their names suggest, the `endpoint-create` step creates an endpoint in the
    Vertex AI prediction service, and the `model-deploy` step deploys our model to
    that endpoint. Their output parameters will show the URIs for the resources created
    by those steps.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剩余的步骤`endpoint-create`和`model-deploy`具有类似的格式。正如它们的名称所暗示的，`endpoint-create`步骤在Vertex
    AI预测服务中创建一个端点，而`model-deploy`步骤将我们的模型部署到该端点。它们的输出参数将显示由这些步骤创建的资源URI。
- en: 'I want to draw your attention to the `endpoint` and `model` artifacts in the
    pipeline. If you click on those, and click the **View Lineage** button that appears
    in the information panel on the right-hand side of the screen, it will take you
    directly to the console for the Vertex AI Metadata service and will show you another
    view of how the steps and artifacts relate to each other, as depicted in *Figure
    12**.10*. Again, clicking on each element in the graph will display metadata for
    that element:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我想引起您的注意，在管道中`endpoint`和`model`工件。如果您点击这些工件，并在屏幕右侧的信息面板中点击出现的**查看谱系**按钮，它将直接带您到Vertex
    AI元数据服务控制台，并显示步骤和工件之间相互关系的另一种视图，如图*图12.10*所示。再次强调，点击图中每个元素都会显示该元素的元数据：
- en: '![Figure 12.10: Lineage graph in Vertex AI Metadata service console](img/B18143_12_10.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图12.10：Vertex AI元数据服务控制台中的谱系图](img/B18143_12_10.jpg)'
- en: 'Figure 12.10: Lineage graph in Vertex AI Metadata service console'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.10：Vertex AI元数据服务控制台中的谱系图
- en: 'In addition to getting metadata insights via the Google Cloud console, we can
    also use the Vertex AI SDK and API directly to query and manage metadata programmatically.
    For example, the following piece of code will list all artifacts in our Google
    Cloud project:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过Google Cloud控制台获取元数据洞察之外，我们还可以直接使用Vertex AI SDK和API以编程方式查询和管理元数据。例如，以下代码将列出我们Google
    Cloud项目中所有的工件：
- en: '[PRE10]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Similarly, the following lines will list all executions and contexts in our
    Google Cloud project:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，以下行将列出我们Google Cloud项目中的所有执行和上下文：
- en: '[PRE11]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We have now successfully tracked every step and artifact that was used to create
    our model. Next, let’s explore the **Experiments** feature in Vertex AI that is
    closely related to lineage tracking.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经成功追踪了创建我们的模型所使用的每一个步骤和工件。接下来，让我们探索Vertex AI中的**实验**功能，该功能与谱系追踪紧密相关。
- en: Vertex ML Experiments
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Vertex ML实验
- en: 'When we created our pipeline definition in [*Chapter 11*](B18143_11.xhtml#_idTextAnchor288),
    we specified an experiment name with which to associate our pipeline runs. This
    provides another view of how to group the steps and artifacts related to our pipeline
    runs and the model versions they create. This feature can be useful for sharing
    and collaboration, as well as for comparing different model versions against each
    other. To view the experiment associated with our pipeline runs, perform the following
    steps:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在[*第11章*](B18143_11.xhtml#_idTextAnchor288)中创建管道定义时，我们指定了一个实验名称来关联我们的管道运行。这为我们提供了另一种查看与我们的管道运行和它们创建的模型版本相关的步骤和工件的方式。此功能对于共享和协作以及比较不同的模型版本非常有用。要查看与我们的管道运行关联的实验，请执行以下步骤：
- en: In the Google Cloud console, navigate to **Vertex AI** > **Experiments**.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Cloud控制台中，导航到**Vertex AI** > **实验**。
- en: Click on the name of the experiment that we specified in our MLOps chapter (`aiml-sa-mlops-experiment`).
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击我们在MLOps章节中指定的实验名称（`aiml-sa-mlops-experiment`）。
- en: 'Click on the name of the most recent run, and explore the **ARTIFACTS** tab,
    as shown in *Figure 12**.11*:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击最近运行的名称，并探索如图*图12.11*所示的**工件**标签页：
- en: '![Figure 12.11: Vertex AI Experiments – Artifacts view](img/B18143_12_11.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图12.11：Vertex AI实验 – 工件视图](img/B18143_12_11.jpg)'
- en: 'Figure 12.11: Vertex AI Experiments – Artifacts view'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.11：Vertex AI实验 – 工件视图
- en: Click on the links that are shown under each artifact ID to see those artifacts,
    as well as their metadata in the Vertex ML Metadata service (it will bring you
    back to the screens we already explored in the previous section; this is just
    another way to access the same metadata).
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击每个工件ID下显示的链接，以查看这些工件以及它们在Vertex ML元数据服务中的元数据（它将带您回到我们在上一节中已经探索过的屏幕；这只是访问相同元数据的另一种方式）。
- en: We’ve covered a lot of information in this chapter – let’s summarize what we’ve
    discussed before we move on to the next chapter.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中涵盖了大量的信息——在我们继续下一章之前，让我们总结一下我们之前讨论的内容。
- en: Summary
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the concepts of bias, explainability, fairness,
    and lineage. We started off by examining some of the common types of bias that
    can occur at various steps in the ML model development life cycle. This included
    sources of bias such as pre-existing bias, algorithmic bias, and collection or
    measurement bias, which further included sub-categories such as sampling bias,
    response bias, and observer bias. We talked about how to inspect for bias, using
    techniques such as data exploration and DIA.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了偏差、可解释性、公平性和谱系的概念。我们首先检查了在机器学习模型开发生命周期中可能出现的常见偏差类型。这包括偏差的来源，如既存偏差、算法偏差以及收集或测量偏差，这些进一步包括子类别，如抽样偏差、响应偏差和观察者偏差。我们讨论了如何使用数据探索和DIA等技术来检查偏差。
- en: Next, we dived into the use of explainability techniques to understand how our
    models make their decisions at inference time and to assess their fairness, particularly
    with regard to understanding how the input features in our dataset could influence
    our models’ predictions. We used tools such as PDPs and SHAP for these purposes.
    We then looked at how to use Vertex AI to get explanations from models that were
    hosted on Vertex AI endpoints. Going beyond simply getting explanations, we then
    discussed how to proactively counteract bias by using counterfactual analysis
    to pose questions such as “What would need to change in my input data to alter
    the decision of a predictive model?”
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们深入探讨了可解释性技术的应用，以理解我们的模型在推理时如何做出决策，以及如何评估其公平性，特别是理解我们的数据集中的输入特征如何可能影响模型的预测。我们使用了PDPs和SHAP等工具来完成这些目的。然后，我们探讨了如何使用Vertex
    AI从托管在Vertex AI端点的模型中获取解释。在仅仅获取解释之外，我们还讨论了如何通过反事实分析来主动对抗偏差，提出诸如“为了改变预测模型的决策，我的输入数据需要做出哪些改变？”等问题。
- en: Finally, we covered the topic of lineage tracking and its importance in terms
    of explainability, as well as other factors such as collaboration, troubleshooting,
    and compliance. We walked through an ML pipeline that we had created in a previous
    chapter and looked at the metadata associated with each component of the pipeline,
    including all of the steps and artifacts that were used to create a specific model.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了谱系跟踪的主题及其在可解释性方面的重要性，以及其他因素，如协作、故障排除和合规性。我们回顾了在前一章中创建的机器学习管道，并检查了管道每个组件相关的元数据，包括创建特定模型所使用的所有步骤和工件。
- en: While the chapters before this one focused on everything that’s required to
    build and run an ML model, this chapter focused on more advanced topics such as
    ensuring that our models are explainable and fair. The next chapter in this book
    continues in a similar vein. We are no longer just looking at the mechanics of
    how to build and deploy ML models but are now incorporating broader ethical and
    architectural considerations. In the next chapter, we dive further into the topics
    of governance, compliance, and architectural best practices in the model development
    life cycle.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面的章节专注于构建和运行机器学习模型所需的一切，但本章则聚焦于更高级的主题，例如确保我们的模型是可解释和公平的。本书的下一章将继续这一趋势。我们不再仅仅关注构建和部署机器学习模型的机制，而是现在正在纳入更广泛的伦理和架构考量。在下一章中，我们将进一步深入探讨治理、合规和模型开发生命周期中的架构最佳实践。
