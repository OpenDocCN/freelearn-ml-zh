- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Predicting Continuous Target Variables with Regression Analysis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用回归分析预测连续目标变量
- en: 'Throughout the previous chapters, you learned a lot about the main concepts
    behind **supervised learning** and trained many different models for classification
    tasks to predict group memberships or categorical variables. In this chapter,
    we will dive into another subcategory of supervised learning: **regression analysis**.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，您学到了许多关于**监督学习**的主要概念，并训练了许多不同的分类任务模型，以预测组别成员或分类变量。在本章中，我们将深入探讨监督学习的另一个子类别：**回归分析**。
- en: Regression models are used to predict target variables on a continuous scale,
    which makes them attractive for addressing many questions in science. They also
    have applications in industry, such as understanding relationships between variables,
    evaluating trends, or making forecasts. One example is predicting the sales of
    a company in future months.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型用于预测连续范围的目标变量，这使得它们在解决许多科学问题时非常有吸引力。它们在工业中也有应用，例如理解变量之间的关系、评估趋势或进行预测。例如，预测公司未来几个月的销售额。
- en: 'In this chapter, we will discuss the main concepts of regression models and
    cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论回归模型的主要概念，并涵盖以下主题：
- en: Exploring and visualizing datasets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索和可视化数据集
- en: Looking at different approaches to implement linear regression models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看实现线性回归模型的不同方法
- en: Training regression models that are robust to outliers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练对异常值稳健的回归模型
- en: Evaluating regression models and diagnosing common problems
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估回归模型并诊断常见问题
- en: Fitting regression models to nonlinear data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将回归模型拟合到非线性数据
- en: Introducing linear regression
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍线性回归
- en: The goal of linear regression is to model the relationship between one or multiple
    features and a continuous target variable. In contrast to classification—a different
    subcategory of supervised learning—regression analysis aims to predict outputs
    on a continuous scale rather than categorical class labels.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的目标是建模一个或多个特征与连续目标变量之间的关系。与分类——监督学习的另一个子类别——不同，回归分析旨在预测连续范围的输出，而不是分类标签。
- en: In the following subsections, you will be introduced to the most basic type
    of linear regression, **simple linear regression**, and understand how to relate
    it to the more general, multivariate case (linear regression with multiple features).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下的小节中，您将了解最基本的线性回归类型——**简单线性回归**，并理解如何将其与更一般的多变量情况（具有多个特征的线性回归）联系起来。
- en: Simple linear regression
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单线性回归
- en: 'The goal of simple (**univariate**) linear regression is to model the relationship
    between a single feature (**explanatory variable**, *x*) and a continuous-valued
    **target** (**response variable**, *y*). The equation of a linear model with one
    explanatory variable is defined as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 简单（**单变量**）线性回归的目标是建模单个特征（**解释变量**，*x*）与连续值**目标**（**响应变量**，*y*）之间的关系。具有一个解释变量的线性模型方程定义如下：
- en: '![](img/B13208_10_001.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_001.png)'
- en: Here, the weight, ![](img/B13208_10_002.png), represents the *y* axis intercept
    and ![](img/B13208_10_003.png) is the weight coefficient of the explanatory variable.
    Our goal is to learn the weights of the linear equation to describe the relationship
    between the explanatory variable and the target variable, which can then be used
    to predict the responses of new explanatory variables that were not part of the
    training dataset.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，权重，![](img/B13208_10_002.png)，表示*y*轴截距，而![](img/B13208_10_003.png)是解释变量的权重系数。我们的目标是学习线性方程的权重，以描述解释变量与目标变量之间的关系，这些权重可以用来预测新的解释变量的响应，这些解释变量不属于训练数据集。
- en: 'Based on the linear equation that we defined previously, linear regression
    can be understood as finding the best-fitting straight line through the training
    examples, as shown in the following figure:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们之前定义的线性方程，线性回归可以理解为寻找一条最佳拟合的直线，通过训练样本，如下图所示：
- en: '![](img/B13208_10_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_01.png)'
- en: This best-fitting line is also called the **regression line**, and the vertical
    lines from the regression line to the training examples are the so-called **offsets**
    or **residuals**—the errors of our prediction.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这条最佳拟合线也叫做**回归线**，从回归线到训练样本的垂直线被称为**偏差**或**残差**——即我们的预测误差。
- en: Multiple linear regression
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多元线性回归
- en: 'The previous section introduced simple linear regression, a special case of
    linear regression with one explanatory variable. Of course, we can also generalize
    the linear regression model to multiple explanatory variables; this process is
    called **multiple linear regression**:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节介绍了简单线性回归，它是具有一个解释变量的线性回归的特例。当然，我们也可以将线性回归模型推广到多个解释变量；这个过程被称为**多元线性回归**：
- en: '![](img/B13208_10_004.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_004.png)'
- en: Here, ![](img/B13208_10_005.png) is the *y* axis intercept with ![](img/B13208_10_006.png).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_10_005.png)是*y*轴与![](img/B13208_10_006.png)的截距。
- en: 'The following figure shows how the two-dimensional, fitted hyperplane of a
    multiple linear regression model with two features could look:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了具有两个特征的多元线性回归模型的二维拟合超平面可能的样子：
- en: '![](img/B13208_10_02.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_02.png)'
- en: As you can see, visualizations of multiple linear regression hyperplanes in
    a three-dimensional scatterplot are already challenging to interpret when looking
    at static figures. Since we have no good means of visualizing hyperplanes with
    two dimensions in a scatterplot (multiple linear regression models fit to datasets
    with three or more features), the examples and visualizations in this chapter
    will mainly focus on the univariate case, using simple linear regression. However,
    simple and multiple linear regression are based on the same concepts and the same
    evaluation techniques; the code implementations that we will discuss in this chapter
    are also compatible with both types of regression model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，多个线性回归超平面在三维散点图中的可视化，在查看静态图像时已经很难理解。由于我们没有很好的方法在散点图中可视化具有两个维度的超平面（适用于具有三个或更多特征的数据集的多元线性回归模型），本章中的示例和可视化将主要聚焦于单变量情况，使用简单线性回归。然而，简单线性回归和多元线性回归基于相同的概念和评估技术；我们将在本章讨论的代码实现也兼容这两种类型的回归模型。
- en: Exploring the Housing dataset
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索住房数据集
- en: Before we implement the first linear regression model, we will discuss a new
    dataset, the Housing dataset, which contains information about houses in the suburbs
    of Boston collected by D. Harrison and D.L. Rubinfeld in 1978\. The Housing dataset
    has been made freely available and is included in the code bundle of this book.
    The dataset has recently been removed from the UCI Machine Learning Repository
    but is available online at [https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch10/housing.data.txt](https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/code/ch10/housing.data.txt)
    or scikit-learn ([https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/datasets/data/boston_house_prices.csv](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/datasets/data/boston_house_prices.csv)).
    As with each new dataset, it is always helpful to explore the data through a simple
    visualization, to get a better feeling of what we are working with.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现第一个线性回归模型之前，我们将讨论一个新的数据集——住房数据集，它包含由D. Harrison和D.L. Rubinfeld于1978年收集的波士顿郊区房屋信息。该住房数据集已经免费提供，并包含在本书的代码包中。尽管该数据集最近已从UCI机器学习库中删除，但它仍可在线访问，网址为[https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch10/housing.data.txt](https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/code/ch10/housing.data.txt)或通过scikit-learn
    ([https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/datasets/data/boston_house_prices.csv](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/datasets/data/boston_house_prices.csv))。和每一个新的数据集一样，使用简单的可视化探索数据，总是有助于更好地理解我们正在处理的内容。
- en: Loading the Housing dataset into a data frame
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将住房数据集加载到数据框中
- en: In this section, we will load the Housing dataset using the pandas `read_csv`
    function, which is fast and versatile and a recommended tool for working with
    tabular data stored in a plaintext format.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用pandas的`read_csv`函数加载住房数据集，该函数快速且多功能，是处理以纯文本格式存储的表格数据的推荐工具。
- en: 'The features of the 506 examples in the Housing dataset have been taken from
    the original source that was previously shared on [https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)
    and summarized here:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 住房数据集中506个示例的特征来自之前在[https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)上分享的原始数据源，现总结如下：
- en: '`CRIM`: Per capita crime rate by town'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CRIM`：按城镇计算的人均犯罪率'
- en: '`ZN`: Proportion of residential land zoned for lots over 25,000 sq. ft.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ZN`：划定为超过25,000平方英尺的住宅用地比例'
- en: '`INDUS`: Proportion of non-retail business acres per town'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`INDUS`：每个城镇非零售商业用地的比例'
- en: '`CHAS`: Charles River dummy variable (= 1 if tract bounds river and 0 otherwise)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CHAS`：查尔斯河虚拟变量（如果区域界限为河流，则为1，否则为0）'
- en: '`NOX`: Nitric oxide concentration (parts per 10 million)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NOX`：氮氧化物浓度（每千万分之一）'
- en: '`RM`: Average number of rooms per dwelling'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RM`：每个住宅的平均房间数'
- en: '`AGE`: Proportion of owner-occupied units built prior to 1940'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AGE`：1940年前建造的自有住房比例'
- en: '`DIS`: Weighted distances to five Boston employment centers'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DIS`：到波士顿五个就业中心的加权距离'
- en: '`RAD`: Index of accessibility to radial highways'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RAD`：到辐射状高速公路的可达性指数'
- en: '`TAX`: Full-value property tax rate per $10,000'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TAX`：每 $10,000 的全额物业税税率'
- en: '`PTRATIO`: Pupil-teacher ratio by town'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PTRATIO`：各城镇的师生比'
- en: '`B`: 1000(*Bk* – 0.63)², where *Bk* is the proportion of [people of African
    American descent] by town'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`B`：1000(*Bk* – 0.63)²，其中 *Bk* 是各城镇中[非洲裔美国人]人口的比例'
- en: '`LSTAT`: Percentage of lower status of the population'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LSTAT`：低社会经济地位人群的百分比'
- en: '`MEDV`: Median value of owner-occupied homes in $1000s'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MEDV`：自有住房的中位数价值（单位为 $1000）'
- en: 'For the rest of this chapter, we will regard the house prices (`MEDV`) as our
    target variable—the variable that we want to predict using one or more of the
    13 explanatory variables. Before we explore this dataset further, let''s load
    it into a pandas `DataFrame`:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将把房价（`MEDV`）视为目标变量——我们希望通过一个或多个解释变量来预测的变量。在进一步探索该数据集之前，让我们先将它加载到
    pandas 的 `DataFrame` 中：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To confirm that the dataset was loaded successfully, we can display the first
    five lines of the dataset, as shown in the following figure:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认数据集是否已成功加载，我们可以显示数据集的前五行，如下图所示：
- en: '![](img/B13208_10_03.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_03.png)'
- en: '**Obtaining the Housing dataset**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**获取房屋数据集**'
- en: 'You can find a copy of the Housing dataset (and all other datasets used in
    this book) in the code bundle of this book, which you can use if you are working
    offline or the web link [https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/code/ch10/housing.data.txt](https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/code/ch10/housing.data.txt)
    is temporarily unavailable. For instance, to load the Housing dataset from a local
    directory, you can replace these lines:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的代码包中找到一份房屋数据集的副本（以及本书中使用的所有其他数据集），如果你处于离线状态，或者 web 链接 [https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/code/ch10/housing.data.txt](https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/code/ch10/housing.data.txt)
    暂时不可用，你可以使用该副本。例如，要从本地目录加载房屋数据集，你可以替换以下几行：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'in the preceding code example with this line:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码示例中，使用这一行：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Visualizing the important characteristics of a dataset
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化数据集的重要特征
- en: '**Exploratory data analysis** (**EDA**) is an important and recommended first
    step prior to the training of a machine learning model. In the rest of this section,
    we will use some simple yet useful techniques from the graphical EDA toolbox that
    may help us to visually detect the presence of outliers, the distribution of the
    data, and the relationships between features.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**探索性数据分析**（**EDA**）是训练机器学习模型之前的重要且推荐的第一步。在本节的其余部分，我们将使用一些简单但有用的图形EDA工具箱中的技术，这些技术有助于我们从视觉上检测离群值的存在、数据的分布以及特征之间的关系。'
- en: First, we will create a **scatterplot matrix** that allows us to visualize the
    pair-wise correlations between the different features in this dataset in one place.
    To plot the scatterplot matrix, we will use the `scatterplotmatrix` function from
    the MLxtend library ([http://rasbt.github.io/mlxtend/](http://rasbt.github.io/mlxtend/)),
    which is a Python library that contains various convenience functions for machine
    learning and data science applications in Python.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个**散点图矩阵**，它可以让我们在一个地方可视化数据集中不同特征之间的成对相关性。为了绘制散点图矩阵，我们将使用来自 MLxtend
    库的`scatterplotmatrix`函数（[http://rasbt.github.io/mlxtend/](http://rasbt.github.io/mlxtend/)），这是一个包含多种机器学习和数据科学应用便捷函数的
    Python 库。
- en: 'You can install the `mlxtend` package via `conda install mlxtend` or `pip install
    mlxtend`. After the installation is complete, you can import the package and create
    the scatterplot matrix as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 `conda install mlxtend` 或 `pip install mlxtend` 安装 `mlxtend` 包。安装完成后，你可以导入该包并按如下方式创建散点图矩阵：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As you can see in the following figure, the scatterplot matrix provides us
    with a useful graphical summary of the relationships in a dataset:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，散点图矩阵为我们提供了一个关于数据集中关系的有用图形总结：
- en: '![](img/B13208_10_04.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_04.png)'
- en: 'Due to space constraints and in the interest of readability, we have only plotted
    five columns from the dataset: `LSTAT`, `INDUS`, `NOX`, `RM`, and `MEDV`. However,
    you are encouraged to create a scatterplot matrix of the whole `DataFrame` to
    explore the dataset further by choosing different column names in the previous
    `scatterplotmatrix` function call, or including all variables in the scatterplot
    matrix by omitting the column selector.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于篇幅限制和提高可读性的需要，我们仅绘制了数据集中的五列：`LSTAT`、`INDUS`、`NOX`、`RM`和`MEDV`。然而，建议你通过在前面的`scatterplotmatrix`函数调用中选择不同的列名，或者通过省略列选择器，来创建整个`DataFrame`的散点图矩阵，以便进一步探索数据集。
- en: Using this scatterplot matrix, we can now quickly eyeball how the data is distributed
    and whether it contains outliers. For example, we can see that there is a linear
    relationship between `RM` and house prices, `MEDV` (the fifth column of the fourth
    row). Furthermore, we can see in the histogram—the lower-right subplot in the
    scatterplot matrix—that the `MEDV` variable seems to be normally distributed but
    contains several outliers.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个散点图矩阵，我们现在可以快速地观察数据的分布情况，并查看是否存在异常值。例如，我们可以看到`RM`和房价`MEDV`之间有线性关系（这是散点图矩阵第四行第五列的内容）。此外，在散点图矩阵的右下子图中，我们可以看到`MEDV`变量似乎呈正态分布，但包含几个异常值。
- en: '**Normality assumption of linear regression**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性回归的正态性假设**'
- en: 'Note that in contrast to common belief, training a linear regression model
    does not require that the explanatory or target variables are normally distributed.
    The normality assumption is only a requirement for certain statistics and hypothesis
    tests that are beyond the scope of this book (for more information on this topic,
    please refer to *Introduction to Linear Regression Analysis*, *Montgomery*, *Douglas
    C. Montgomery*, *Elizabeth A. Peck*, and *G. Geoffrey Vining*, *Wiley*, *2012*,
    pages: 318-319).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与常见的看法相反，训练线性回归模型并不要求解释变量或目标变量必须符合正态分布。正态性假设仅是某些统计和假设检验的要求，而这些内容超出了本书的范围（有关此主题的更多信息，请参阅《线性回归分析导论》，*Montgomery*，*Douglas
    C. Montgomery*，*Elizabeth A. Peck*，*G. Geoffrey Vining*，*Wiley*，*2012*，第318-319页）。
- en: Looking at relationships using a correlation matrix
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用相关矩阵查看变量之间的关系
- en: In the previous section, we visualized the data distributions of the Housing
    dataset variables in the form of histograms and scatterplots. Next, we will create
    a correlation matrix to quantify and summarize linear relationships between variables.
    A correlation matrix is closely related to the covariance matrix that we covered
    in the section *Unsupervised dimensionality reduction via principal component
    analysis* in *Chapter 5*, *Compressing Data via Dimensionality Reduction*. We
    can interpret the correlation matrix as being a rescaled version of the covariance
    matrix. In fact, the correlation matrix is identical to a covariance matrix computed
    from standardized features.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们通过直方图和散点图可视化了住房数据集变量的分布。接下来，我们将创建一个相关矩阵，以量化和总结变量之间的线性关系。相关矩阵与我们在*第5章*《通过主成分分析进行无监督降维》中讲解的协方差矩阵密切相关。我们可以将相关矩阵解释为协方差矩阵的重新缩放版本。实际上，相关矩阵与从标准化特征计算的协方差矩阵是相同的。
- en: 'The correlation matrix is a square matrix that contains the **Pearson product-moment
    correlation coefficient** (often abbreviated as **Pearson''s r**), which measures
    the linear dependence between pairs of features. The correlation coefficients
    are in the range –1 to 1\. Two features have a perfect positive correlation if
    *r* = 1, no correlation if *r* = 0, and a perfect negative correlation if *r*
    = –1\. As mentioned previously, Pearson''s correlation coefficient can simply
    be calculated as the covariance between two features, *x* and *y* (numerator),
    divided by the product of their standard deviations (denominator):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 相关矩阵是一个方阵，包含**皮尔逊积矩相关系数**（通常缩写为**皮尔逊r**），它衡量特征对之间的线性依赖关系。相关系数的范围是 –1 到 1。两个特征如果*r*
    = 1，表示完美的正相关；如果*r* = 0，表示没有相关性；如果*r* = –1，表示完美的负相关。如前所述，皮尔逊相关系数可以简单地通过两个特征*x*和*y*之间的协方差（分子）除以它们标准差的乘积（分母）来计算：
- en: '![](img/B13208_10_007.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_007.png)'
- en: Here, ![](img/B13208_10_008.png) denotes the mean of the corresponding feature,
    ![](img/B13208_10_009.png) is the covariance between the features *x* and *y*,
    and ![](img/B13208_10_010.png) and ![](img/B13208_10_011.png) are the features'
    standard deviations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_10_008.png) 表示相应特征的均值，![](img/B13208_10_009.png) 是特征 *x* 和
    *y* 之间的协方差，而 ![](img/B13208_10_010.png) 和 ![](img/B13208_10_011.png) 是特征的标准差。
- en: '**Covariance versus correlation for standardized features**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**标准化特征的协方差与相关性**'
- en: 'We can show that the covariance between a pair of standardized features is,
    in fact, equal to their linear correlation coefficient. To show this, let''s first
    standardize the features *x* and *y* to obtain their z-scores, which we will denote
    as ![](img/B13208_10_012.png) and ![](img/B13208_10_013.png), respectively:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以证明，一对标准化特征之间的协方差，实际上等于它们的线性相关系数。为了证明这一点，让我们首先标准化特征 *x* 和 *y*，得到它们的 z 分数，分别表示为
    ![](img/B13208_10_012.png) 和 ![](img/B13208_10_013.png)：
- en: '![](img/B13208_10_014.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_014.png)'
- en: 'Remember that we compute the (population) covariance between two features as
    follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们计算两个特征之间的（总体）协方差的公式如下：
- en: '![](img/B13208_10_015.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_015.png)'
- en: 'Since standardization centers a feature variable at mean zero, we can now calculate
    the covariance between the scaled features as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于标准化将特征变量的均值中心化为零，我们现在可以按以下方式计算标准化特征之间的协方差：
- en: '![](img/B13208_10_016.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_016.png)'
- en: 'Through resubstitution, we then get the following result:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重新代入，我们得到以下结果：
- en: '![](img/B13208_10_017.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_017.png)'
- en: 'Finally, we can simplify this equation as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将此方程简化为以下形式：
- en: '![](img/B13208_10_018.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_018.png)'
- en: 'In the following code example, we will use NumPy''s `corrcoef` function on
    the five feature columns that we previously visualized in the scatterplot matrix,
    and we will use MLxtend''s `heatmap` function to plot the correlation matrix array
    as a heat map:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码示例中，我们将使用 NumPy 的 `corrcoef` 函数，作用于我们之前在散点图矩阵中可视化的五个特征列，并使用 MLxtend 的 `heatmap`
    函数将相关矩阵数组绘制为热图：
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As you can see in the resulting figure, the correlation matrix provides us
    with another useful summary graphic that can help us to select features based
    on their respective linear correlations:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如结果图所示，相关矩阵为我们提供了另一个有用的总结图表，帮助我们基于特征之间的线性相关性选择特征：
- en: '![](img/B13208_10_05.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_05.png)'
- en: To fit a linear regression model, we are interested in those features that have
    a high correlation with our target variable, `MEDV`. Looking at the previous correlation
    matrix, we can see that our target variable, `MEDV`, shows the largest correlation
    with the `LSTAT` variable (`-0.74`); however, as you might remember from inspecting
    the scatterplot matrix, there is a clear nonlinear relationship between `LSTAT`
    and `MEDV`. On the other hand, the correlation between `RM` and `MEDV` is also
    relatively high (`0.70`). Given the linear relationship between these two variables
    that we observed in the scatterplot, `RM` seems to be a good choice for an exploratory
    variable to introduce the concepts of a simple linear regression model in the
    following section.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了拟合线性回归模型，我们关注与目标变量 `MEDV` 相关性较高的特征。通过查看先前的相关矩阵，我们可以看到目标变量 `MEDV` 与 `LSTAT`
    变量（`-0.74`）的相关性最大；然而，正如你从散点图矩阵中观察到的，`LSTAT` 和 `MEDV` 之间存在明显的非线性关系。另一方面，`RM` 和
    `MEDV` 之间的相关性也相对较高（`0.70`）。鉴于我们在散点图中观察到这两个变量之间的线性关系，`RM` 似乎是引入简单线性回归模型概念的一个很好的探索变量选择。
- en: Implementing an ordinary least squares linear regression model
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个普通最小二乘线性回归模型
- en: At the beginning of this chapter, it was mentioned that linear regression can
    be understood as obtaining the best-fitting straight line through the examples
    of our training data. However, we have neither defined the term *best-fitting*
    nor have we discussed the different techniques of fitting such a model. In the
    following subsections, we will fill in the missing pieces of this puzzle using
    the **ordinary least squares** (**OLS**) method (sometimes also called **linear
    least squares**) to estimate the parameters of the linear regression line that
    minimizes the sum of the squared vertical distances (residuals or errors) to the
    training examples.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开始时提到，线性回归可以理解为通过训练数据集中的样本点得到最佳拟合直线。然而，我们既没有定义*最佳拟合*这一术语，也没有讨论拟合这种模型的不同技术。在接下来的小节中，我们将使用**普通最小二乘法**（**OLS**）（有时也称为**线性最小二乘法**）来估计线性回归直线的参数，从而最小化垂直距离（残差或误差）的平方和。
- en: Solving regression for regression parameters with gradient descent
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用梯度下降法求解回归参数
- en: 'Consider our implementation of the **Adaptive Linear Neuron** (**Adaline**)
    from *Chapter 2*, *Training Simple Machine Learning Algorithms for Classification*.
    You will remember that the artificial neuron uses a linear activation function.
    Also, we defined a cost function, *J*(*w*), which we minimized to learn the weights
    via optimization algorithms, such as **gradient descent** (**GD**) and **stochastic
    gradient descent** (**SGD**). This cost function in Adaline is the **sum of squared
    errors** (**SSE**), which is identical to the cost function that we use for OLS:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们在*第2章*《训练简单机器学习算法进行分类》中实现的**自适应线性神经元**（**Adaline**）。你一定还记得人工神经元使用的是线性激活函数。此外，我们定义了一个成本函数*J*(*w*)，并通过优化算法（如**梯度下降**（**GD**）和**随机梯度下降**（**SGD**））来最小化它，从而学习权重。Adaline中的成本函数是**平方误差和**（**SSE**），它与我们用于OLS的成本函数相同：
- en: '![](img/B13208_10_019.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_019.png)'
- en: 'Here, ![](img/B13208_10_020.png) is the predicted value ![](img/B13208_10_021.png)
    (note that the term ![](img/B13208_10_022.png) is just used for convenience to
    derive the update rule of GD). Essentially, OLS regression can be understood as
    Adaline without the unit step function so that we obtain continuous target values
    instead of the class labels `-1` and `1`. To demonstrate this, let''s take the
    GD implementation of Adaline from *Chapter 2*, *Training Simple Machine Learning
    Algorithms for Classification*, and remove the unit step function to implement
    our first linear regression model:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B13208_10_020.png) 是预测值 ![](img/B13208_10_021.png)（注意，术语 ![](img/B13208_10_022.png)
    只是为了方便推导GD的更新规则）。本质上，OLS回归可以理解为去掉单位阶跃函数后的Adaline，从而获得连续的目标值，而不是`-1`和`1`的类别标签。为了演示这一点，我们将从*第2章*《训练简单机器学习算法进行分类》中提取Adaline的GD实现，并去掉单位阶跃函数，来实现我们的第一个线性回归模型：
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Weight updates with gradient descent**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**权重更新与梯度下降**'
- en: If you need a refresher about how the weights are updated—taking a step in the
    opposite direction of the gradient—please revisit the *Adaptive linear neurons
    and the convergence of learning* section in *Chapter 2*, *Training Simple Machine
    Learning Algorithms for Classification*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要复习权重是如何更新的——即沿梯度的相反方向更新——请重新阅读*第2章*《训练简单机器学习算法进行分类》中的*自适应线性神经元与学习的收敛性*一节。
- en: 'To see our `LinearRegressionGD` regressor in action, let''s use the `RM` (number
    of rooms) variable from the Housing dataset as the explanatory variable and train
    a model that can predict `MEDV` (house prices). Furthermore, we will standardize
    the variables for better convergence of the GD algorithm. The code is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示我们的`LinearRegressionGD`回归器的应用，首先使用Housing数据集中的`RM`（房间数量）变量作为自变量，并训练一个能够预测`MEDV`（房价）的模型。此外，我们将对变量进行标准化，以便更好地实现GD算法的收敛。代码如下：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice the workaround regarding `y_std`, using `np.newaxis` and `flatten`. Most
    transformers in scikit-learn expect data to be stored in two-dimensional arrays.
    In the previous code example, the use of `np.newaxis` in `y[:, np.newaxis]` added
    a new dimension to the array. Then, after `StandardScaler` returned the scaled
    variable, we converted it back to the original one-dimensional array representation
    using the `flatten()` method for our convenience.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意关于`y_std`的变通方法，使用了`np.newaxis`和`flatten`。scikit-learn中的大多数变换器都期望数据存储为二维数组。在之前的代码示例中，`y[:,
    np.newaxis]`中的`np.newaxis`为数组添加了一个新的维度。然后，在`StandardScaler`返回缩放后的变量后，我们通过`flatten()`方法将其转换回原始的、具有一维数组表示的形式，方便我们使用。
- en: 'We discussed in *Chapter 2*, *Training Simple Machine Learning Algorithms for
    Classification*, that it is always a good idea to plot the cost as a function
    of the number of epochs (complete iterations) over the training dataset when we
    are using optimization algorithms, such as GD, to check that the algorithm converged
    to a cost minimum (here, a *global* cost minimum):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第2章*《*训练简单的机器学习算法进行分类*》中讨论过，当我们使用优化算法（如GD）时，绘制成本与训练数据集的迭代次数（epoch）之间的关系总是一个好主意，这样可以检查算法是否收敛到一个最小的成本（在这里是一个*全局*最小成本）：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As you can see in the following plot, the GD algorithm converged after the
    fifth epoch:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在下面的图中看到的那样，GD算法在第5次迭代后收敛：
- en: '![](img/B13208_10_06.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_06.png)'
- en: 'Next, let''s visualize how well the linear regression line fits the training
    data. To do so, we will define a simple helper function that will plot a scatterplot
    of the training examples and add the regression line:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们可视化线性回归线拟合训练数据的效果。为此，我们将定义一个简单的辅助函数，用于绘制训练样本的散点图，并添加回归线：
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we will use this `lin_regplot` function to plot the number of rooms against
    the house price:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用这个`lin_regplot`函数将房间数量与房价进行绘图：
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As you can see in the following plot, the linear regression line reflects the
    general trend that house prices tend to increase with the number of rooms:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在下面的图中看到的那样，线性回归线反映了房价随着房间数量的增加而普遍上升的趋势：
- en: '![](img/B13208_10_07.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_07.png)'
- en: 'Although this observation makes sense, the data also tells us that the number
    of rooms does not explain house prices very well in many cases. Later in this
    chapter, we will discuss how to quantify the performance of a regression model.
    Interestingly, we can also observe that several data points lined up at *y* =
    3, which suggests that the prices may have been clipped. In certain applications,
    it may also be important to report the predicted outcome variables on their original
    scale. To scale the predicted price outcome back onto the `Price in $1000s` axis,
    we can simply apply the `inverse_transform` method of the `StandardScaler`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个观察结果是有道理的，但数据也告诉我们，在许多情况下，房间数量并不能很好地解释房价。在本章稍后，我们将讨论如何量化回归模型的表现。有趣的是，我们还可以观察到几个数据点排列在*y*
    = 3的位置，这表明价格可能被截断了。在某些应用中，报告预测结果变量的原始尺度可能也很重要。为了将预测的价格结果缩放回`Price in $1000s`轴，我们可以简单地使用`StandardScaler`的`inverse_transform`方法：
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this code example, we used the previously trained linear regression model
    to predict the price of a house with five rooms. According to our model, such
    a house will be worth $10,840.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码示例中，我们使用了之前训练好的线性回归模型来预测一栋有五个房间的房子的价格。根据我们的模型，这样的房子价值$10,840。
- en: 'As a side note, it is also worth mentioning that we technically don''t have
    to update the weights of the intercept if we are working with standardized variables,
    since the *y* axis intercept is always 0 in those cases. We can quickly confirm
    this by printing the weights:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，值得一提的是，如果我们使用标准化变量，我们在技术上不需要更新截距项的权重，因为在这种情况下，*y*轴的截距始终为0。我们可以通过打印权重来快速确认这一点：
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Estimating the coefficient of a regression model via scikit-learn
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过scikit-learn估计回归模型的系数
- en: 'In the previous section, we implemented a working model for regression analysis;
    however, in a real-world application, we may be interested in more efficient implementations.
    For example, many of scikit-learn''s estimators for regression make use of the
    least squares implementation in SciPy (`scipy.linalg.lstsq`), which in turn uses
    highly optimized code optimizations based on the Linear Algebra Package (LAPACK).
    The linear regression implementation in scikit-learn also works (better) with
    unstandardized variables, since it does not use (S)GD-based optimization, so we
    can skip the standardization step:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们实现了一个用于回归分析的工作模型；然而，在实际应用中，我们可能会对更高效的实现感兴趣。例如，scikit-learn 的许多回归估计器使用了
    SciPy 中的最小二乘实现（`scipy.linalg.lstsq`），它基于线性代数库（LAPACK）进行了高度优化。scikit-learn 中的线性回归实现也能（更好地）处理未标准化的变量，因为它不使用基于（S）GD
    的优化，因此我们可以跳过标准化步骤：
- en: '[PRE12]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As you can see from executing this code, scikit-learn''s `LinearRegression`
    model, fitted with the unstandardized `RM` and `MEDV` variables, yielded different
    model coefficients, since the features have not been standardized. However, when
    we compare it to our GD implementation by plotting `MEDV` against `RM`, we can
    qualitatively see that it fits the data similarly well:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从执行这段代码可以看出，scikit-learn 的 `LinearRegression` 模型，在未标准化的 `RM` 和 `MEDV` 变量上拟合时，得出了不同的模型系数，因为这些特征没有进行标准化。然而，当我们通过绘制
    `MEDV` 与 `RM` 的关系来与我们的 GD 实现进行比较时，我们可以定性地看到它同样很好地拟合了数据：
- en: '[PRE13]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'For instance, we can see that the overall result looks identical to our GD
    implementation:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以看到整体结果与我们的 GD 实现非常相似：
- en: '![](img/B13208_10_08.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_08.png)'
- en: '**Analytical solutions of linear regression**'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性回归的解析解**'
- en: 'As an alternative to using machine learning libraries, there is also a closed-form
    solution for solving OLS involving a system of linear equations that can be found
    in most introductory statistics textbooks:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用机器学习库外，解决 OLS 的另一个方法是通过一个线性方程组的封闭解，这个解可以在大多数入门统计学教材中找到：
- en: '![](img/B13208_10_023.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_023.png)'
- en: 'We can implement it in Python as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式在 Python 中实现：
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The advantage of this method is that it is guaranteed to find the optimal solution
    analytically. However, if we are working with very large datasets, it can be computationally
    too expensive to invert the matrix in this formula (sometimes also called the
    **normal equation**), or the matrix containing the training examples may be singular
    (non-invertible), which is why we may prefer iterative methods in certain cases.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是它能够通过解析方法保证找到最优解。然而，如果我们处理的是非常大的数据集，求解这个公式中的矩阵逆可能会非常耗时（有时也称为**正态方程**），或者包含训练样本的矩阵可能是奇异矩阵（不可逆的），因此在某些情况下我们可能更倾向于使用迭代方法。
- en: 'If you are interested in more information on how to obtain normal equations,
    take a look at Dr. Stephen Pollock''s chapter *The Classical Linear Regression
    Model* from his lectures at the University of Leicester, which is available for
    free at: [http://www.le.ac.uk/users/dsgp1/COURSES/MESOMET/ECMETXT/06mesmet.pdf](http://www.le.ac.uk/users/dsgp1/COURSES/MESOMET/ECMETXT/06mesmet.pdf).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于如何得到正态方程的信息，可以查看斯蒂芬·波洛克博士在莱斯特大学讲座中的章节 *经典线性回归模型*，该章节可以免费访问：[http://www.le.ac.uk/users/dsgp1/COURSES/MESOMET/ECMETXT/06mesmet.pdf](http://www.le.ac.uk/users/dsgp1/COURSES/MESOMET/ECMETXT/06mesmet.pdf)。
- en: Also, if you want to compare linear regression solutions obtained via GD, SGD,
    the closed-form solution, QR factorization, and singular vector decomposition,
    you can use the `LinearRegression` class implemented in MLxtend ([http://rasbt.github.io/mlxtend/user_guide/regressor/LinearRegression/](http://rasbt.github.io/mlxtend/user_guide/regressor/LinearRegression/)),
    which lets users toggle between these options. Another great library to recommend
    for regression modeling in Python is Statsmodels, which implements more advanced
    linear regression models, as illustrated at [https://www.statsmodels.org/stable/examples/index.html#regression](https://www.statsmodels.org/stable/examples/index.html#regression).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你想比较通过 GD、SGD、封闭解、QR 分解和奇异值分解得到的线性回归解，可以使用 MLxtend 中实现的 `LinearRegression`
    类（[http://rasbt.github.io/mlxtend/user_guide/regressor/LinearRegression/](http://rasbt.github.io/mlxtend/user_guide/regressor/LinearRegression/)），该类允许用户在这些选项之间切换。另一个非常值得推荐的
    Python 回归建模库是 Statsmodels，它实现了更为先进的线性回归模型，详细内容请参见 [https://www.statsmodels.org/stable/examples/index.html#regression](https://www.statsmodels.org/stable/examples/index.html#regression)。
- en: Fitting a robust regression model using RANSAC
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RANSAC 拟合一个稳健的回归模型
- en: Linear regression models can be heavily impacted by the presence of outliers.
    In certain situations, a very small subset of our data can have a big effect on
    the estimated model coefficients. There are many statistical tests that can be
    used to detect outliers, which are beyond the scope of the book. However, removing
    outliers always requires our own judgment as data scientists as well as our domain
    knowledge.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型可能会受到异常值的严重影响。在某些情况下，数据中的一个非常小的子集可能对估计的模型系数产生很大影响。有许多统计测试可以用来检测异常值，但这些内容超出了本书的范围。然而，去除异常值总是需要我们作为数据科学家的判断和我们的领域知识。
- en: As an alternative to throwing out outliers, we will look at a robust method
    of regression using the **RANdom SAmple Consensus** (**RANSAC**) algorithm, which
    fits a regression model to a subset of the data, the so-called **inliers**.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 作为抛弃异常值的替代方法，我们将使用**随机样本一致性**（**RANSAC**）算法来进行回归分析，该算法将回归模型拟合到数据的子集上，即所谓的**内点**。
- en: 'We can summarize the iterative RANSAC algorithm as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将迭代的 RANSAC 算法总结如下：
- en: Select a random number of examples to be inliers and fit the model.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择一定数量的样本作为内点并拟合模型。
- en: Test all other data points against the fitted model and add those points that
    fall within a user-given tolerance to the inliers.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有其他数据点与拟合模型进行测试，并将那些落在用户给定容差范围内的点添加到内点中。
- en: Refit the model using all inliers.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所有内点重新拟合模型。
- en: Estimate the error of the fitted model versus the inliers.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 估算拟合模型与内点之间的误差。
- en: Terminate the algorithm if the performance meets a certain user-defined threshold
    or if a fixed number of iterations were reached; go back to step 1 otherwise.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果性能达到某个用户定义的阈值或达到固定的迭代次数，则终止算法；否则返回第 1 步。
- en: 'Let''s now use a linear model in combination with the RANSAC algorithm as implemented
    in scikit-learn''s `RANSACRegressor` class:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用线性模型并结合 scikit-learn 中实现的 RANSAC 算法，使用 `RANSACRegressor` 类：
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We set the maximum number of iterations of the `RANSACRegressor` to 100, and
    using `min_samples=50`, we set the minimum number of the randomly chosen training
    examples to be at least 50\. Using `'absolute_loss'` as an argument for the `loss`
    parameter, the algorithm computes absolute vertical distances between the fitted
    line and the training examples. By setting the `residual_threshold` parameter
    to `5.0`, we only allow training examples to be included in the inlier set if
    their vertical distance to the fitted line is within 5 distance units, which works
    well on this particular dataset.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `RANSACRegressor` 的最大迭代次数设置为 100，并且使用 `min_samples=50`，将随机选择的训练样本的最小数量设置为至少
    50。通过将 `'absolute_loss'` 作为 `loss` 参数的参数，算法计算拟合线与训练样本之间的绝对垂直距离。通过将 `residual_threshold`
    参数设置为 `5.0`，我们只允许当训练样本到拟合线的垂直距离在 5 个单位距离以内时才将其包含在内点集中，这在这个特定数据集上表现良好。
- en: 'By default, scikit-learn uses the **MAD** estimate to select the inlier threshold,
    where MAD stands for the **median absolute deviation** of the target values, `y`.
    However, the choice of an appropriate value for the inlier threshold is problem-specific,
    which is one disadvantage of RANSAC. Many different approaches have been developed
    in recent years to select a good inlier threshold automatically. You can find
    a detailed discussion in *Automatic Estimation of the Inlier Threshold in Robust
    Multiple Structures Fitting*, *R. Toldo*, *A. Fusiello''s*, *Springer*, *2009*
    (in *Image Analysis and Processing–ICIAP 2009*, pages: 123-131).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，scikit-learn 使用**MAD**估算方法来选择内点阈值，其中 MAD 代表目标值`y`的**中位绝对偏差**。然而，选择适当的内点阈值是与问题相关的，这是
    RANSAC 的一个缺点。近年来，已经开发了许多不同的方法来自动选择合适的内点阈值。你可以在*《鲁棒多重结构拟合中的内点阈值自动估计》*中找到详细讨论，*R.
    Toldo*，*A. Fusiello*，*Springer*，*2009*（见*《图像分析与处理–ICIAP 2009》*，第123-131页）。
- en: 'Once we have fitted the RANSAC model, let''s obtain the inliers and outliers
    from the fitted RANSAC-linear regression model and plot them together with the
    linear fit:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拟合了 RANSAC 模型，就让我们从拟合的 RANSAC 线性回归模型中获取内点和外点，并将它们与线性拟合一起绘制：
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As you can see in the following scatterplot, the linear regression model was
    fitted on the detected set of inliers, which are shown as circles:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，线性回归模型是在检测到的内点集上拟合的，这些内点以圆圈表示：
- en: '![](img/B13208_10_09.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_09.png)'
- en: 'When we print the slope and intercept of the model by executing the following
    code, the linear regression line will be slightly different from the fit that
    we obtained in the previous section without using RANSAC:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们通过执行以下代码打印模型的斜率和截距时，线性回归线将与我们在上一节中没有使用RANSAC时得到的拟合线稍有不同：
- en: '[PRE17]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Using RANSAC, we reduced the potential effect of the outliers in this dataset,
    but we don't know whether this approach will have a positive effect on the predictive
    performance for unseen data or not. Thus, in the next section, we will look at
    different approaches for evaluating a regression model, which is a crucial part
    of building systems for predictive modeling.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RANSAC，我们减少了数据集中异常值的潜在影响，但我们不知道这种方法是否会对未见数据的预测性能产生积极影响。因此，在下一节中，我们将探讨评估回归模型的不同方法，这是构建预测建模系统中至关重要的一部分。
- en: Evaluating the performance of linear regression models
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估线性回归模型的性能
- en: In the previous section, you learned how to fit a regression model on training
    data. However, you discovered in previous chapters that it is crucial to test
    the model on data that it hasn't seen during training to obtain a more unbiased
    estimate of its generalization performance.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你学习了如何在训练数据上拟合回归模型。然而，你在前几章中已经发现，至关重要的一点是要在模型没有在训练时见过的数据上进行测试，以获得更不偏倚的泛化性能估计。
- en: 'As you will remember from *Chapter 6*, *Learning Best Practices for Model Evaluation
    and Hyperparameter Tuning*, we want to split our dataset into separate training
    and test datasets, where we will use the former to fit the model and the latter
    to evaluate its performance on unseen data to estimate the generalization performance.
    Instead of proceeding with the simple regression model, we will now use all variables
    in the dataset and train a multiple regression model:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你会记得的，从*第6章*，*模型评估和超参数调优的最佳实践学习*，我们希望将数据集分成单独的训练集和测试集，我们将使用前者来拟合模型，后者用来评估其在未见数据上的性能，以估计泛化性能。我们将不再继续使用简单回归模型，而是使用数据集中的所有变量并训练一个多元回归模型：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Since our model uses multiple explanatory variables, we can't visualize the
    linear regression line (or hyperplane, to be precise) in a two-dimensional plot,
    but we can plot the residuals (the differences or vertical distances between the
    actual and predicted values) versus the predicted values to diagnose our regression
    model. **Residual plots** are a commonly used graphical tool for diagnosing regression
    models. They can help to detect nonlinearity and outliers, and check whether the
    errors are randomly distributed.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模型使用了多个解释变量，我们无法在二维图中可视化线性回归线（或者更准确地说，是超平面），但我们可以绘制残差图（实际值与预测值之间的差异或垂直距离）与预测值的关系图，从而诊断我们的回归模型。**残差图**是诊断回归模型时常用的图形工具。它们可以帮助检测非线性和异常值，并检查误差是否随机分布。
- en: 'Using the following code, we will now plot a residual plot where we simply
    subtract the true target variables from our predicted responses:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码，我们现在将绘制一个残差图，在该图中，我们简单地从预测响应中减去真实的目标变量：
- en: '[PRE19]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After executing the code, we should see a residual plot with a line passing
    through the *x* axis origin, as shown here:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后，我们应该看到一个通过 *x* 轴原点的残差图，如下所示：
- en: '![](img/B13208_10_10.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_10.png)'
- en: In the case of a perfect prediction, the residuals would be exactly zero, which
    we will probably never encounter in realistic and practical applications. However,
    for a good regression model, we would expect the errors to be randomly distributed
    and the residuals to be randomly scattered around the centerline. If we see patterns
    in a residual plot, it means that our model is unable to capture some explanatory
    information, which has leaked into the residuals, as you can slightly see in our
    previous residual plot. Furthermore, we can also use residual plots to detect
    outliers, which are represented by the points with a large deviation from the
    centerline.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在完美预测的情况下，残差应该恰好为零，但在实际应用中，我们可能永远不会遇到这种情况。然而，对于一个良好的回归模型，我们期望误差是随机分布的，残差应随机散布在中心线周围。如果在残差图中看到某种模式，这意味着我们的模型无法捕捉到一些解释性信息，这些信息已经渗入残差中，正如你在我们之前的残差图中可以略微看到的那样。此外，我们还可以利用残差图来检测异常值，这些异常值由偏离中心线较大的点表示。
- en: 'Another useful quantitative measure of a model''s performance is the so-called
    **mean squared error** (**MSE**), which is simply the averaged value of the SSE
    cost that we minimized to fit the linear regression model. The MSE is useful for
    comparing different regression models or for tuning their parameters via grid
    search and cross-validation, as it normalizes the SSE by the sample size:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种评估模型表现的有用定量指标是所谓的**均方误差**（**MSE**），它仅仅是我们在拟合线性回归模型时最小化的SSE成本的平均值。MSE对于比较不同的回归模型或通过网格搜索和交叉验证调节它们的参数非常有用，因为它通过样本大小对SSE进行归一化：
- en: '![](img/B13208_10_024.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_024.png)'
- en: 'Let''s compute the MSE of our training and test predictions:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算一下训练集和测试集预测的MSE：
- en: '[PRE20]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You can see that the MSE on the training dataset is 19.96, and the MSE on the
    test dataset is much larger, with a value of 27.20, which is an indicator that
    our model is overfitting the training data in this case. However, please be aware
    that the MSE is unbounded in contrast to the classification accuracy, for example.
    In other words, the interpretation of the MSE depends on the dataset and feature
    scaling. For example, if the house prices were presented as multiples of 1,000
    (with the K suffix), the same model would yield a lower MSE compared to a model
    that worked with unscaled features. To further illustrate this point, ![](img/B13208_10_025.png).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，训练数据集上的均方误差（MSE）为19.96，而测试数据集上的MSE要大得多，达到了27.20，这表明我们的模型在这种情况下出现了过拟合。然而，请注意，与分类准确率等指标不同，MSE是没有上限的。换句话说，MSE的解释依赖于数据集和特征缩放。例如，如果房价以千元为单位（带有K后缀）表示，相比于处理未缩放特征的模型，相同的模型会得到较低的MSE。为了进一步说明这一点，![](img/B13208_10_025.png)。
- en: 'Thus, it may sometimes be more useful to report the **coefficient of determination**
    (![](img/B13208_10_026.png)), which can be understood as a standardized version
    of the MSE, for better interpretability of the model''s performance. Or, in other
    words, ![](img/B13208_10_027.png) is the fraction of response variance that is
    captured by the model. The ![](img/B13208_10_028.png) value is defined as:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有时报告**决定系数**（![](img/B13208_10_026.png)）可能更为有用，它可以理解为MSE的标准化版本，有助于更好地解释模型的表现。换句话说，![](img/B13208_10_027.png)是模型捕获的响应方差的比例。![](img/B13208_10_028.png)的值定义为：
- en: '![](img/B13208_10_029.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_029.png)'
- en: 'Here, SSE is the sum of squared errors and SST is the total sum of squares:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，SSE是误差平方和，SST是总平方和：
- en: '![](img/B13208_10_030.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_030.png)'
- en: In other words, SST is simply the variance of the response.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，SST仅仅是响应变量的方差。
- en: 'Let''s quickly show that ![](img/B13208_10_031.png) is indeed just a rescaled
    version of the MSE:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速展示一下，![](img/B13208_10_031.png)实际上只是MSE的一个重新缩放版本：
- en: '![](img/B13208_10_032.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_032.png)'
- en: For the training dataset, the ![](img/B13208_10_033.png) is bounded between
    0 and 1, but it can become negative for the test dataset. If ![](img/B13208_10_034.png),
    the model fits the data perfectly with a corresponding *MSE* = 0.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练数据集，![](img/B13208_10_033.png)的值限定在0到1之间，但对于测试数据集，它可能会变成负值。如果![](img/B13208_10_034.png)，则模型完美拟合数据，对应的*MSE*为0。
- en: 'Evaluated on the training data, the ![](img/B13208_10_035.png) of our model
    is 0.765, which doesn''t sound too bad. However, the ![](img/B13208_10_036.png)
    on the test dataset is only 0.673, which we can compute by executing the following
    code:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据上评估时，模型的![](img/B13208_10_035.png)为0.765，听起来还不错。然而，在测试数据集上的![](img/B13208_10_036.png)只有0.673，我们可以通过执行以下代码来计算这个值：
- en: '[PRE21]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Using regularized methods for regression
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用正则化方法进行回归
- en: As we discussed in *Chapter 3*, *A Tour of Machine Learning Classifiers Using
    scikit-learn*, regularization is one approach to tackling the problem of overfitting
    by adding additional information, and thereby shrinking the parameter values of
    the model to induce a penalty against complexity. The most popular approaches
    to regularized linear regression are the so-called **Ridge Regression**, **least
    absolute shrinkage and selection operator** (**LASSO**), and **elastic Net**.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*第3章*中讨论的，*《使用scikit-learn的机器学习分类器巡礼》*，正则化是通过加入额外的信息来应对过拟合问题的一种方法，从而缩小模型参数值，给模型复杂度施加惩罚。最常见的正则化线性回归方法有**岭回归**、**最小绝对收缩与选择算子**（**LASSO**）和**弹性网**。
- en: 'Ridge Regression is an L2 penalized model where we simply add the squared sum
    of the weights to our least-squares cost function:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归是一个L2惩罚模型，我们只是将权重的平方和添加到最小二乘成本函数中：
- en: '![](img/B13208_10_037.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_037.png)'
- en: 'Here:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '![](img/B13208_10_038.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_038.png)'
- en: By increasing the value of hyperparameter ![](img/B13208_10_039.png), we increase
    the regularization strength and thereby shrink the weights of our model. Please
    note that we don't regularize the intercept term, ![](img/B13208_10_040.png).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增大超参数![](img/B13208_10_039.png)的值，我们增加了正则化强度，从而缩小了模型的权重。请注意，我们不会对截距项进行正则化，![](img/B13208_10_040.png)。
- en: 'An alternative approach that can lead to sparse models is LASSO. Depending
    on the regularization strength, certain weights can become zero, which also makes
    LASSO useful as a supervised feature selection technique:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可能导致稀疏模型的替代方法是LASSO。根据正则化强度，某些权重可以变为零，这也使得LASSO成为一种有监督的特征选择技术：
- en: '![](img/B13208_10_041.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_041.png)'
- en: 'Here, the L1 penalty for LASSO is defined as the sum of the absolute magnitudes
    of the model weights, as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，LASSO的L1惩罚定义为模型权重的绝对值之和，如下所示：
- en: '![](img/B13208_10_042.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_042.png)'
- en: However, a limitation of LASSO is that it selects at most *n* features if *m*
    > *n*, where *n* is the number of training examples. This may be undesirable in
    certain applications of feature selection. In practice, however, this property
    of LASSO is often an advantage because it avoids saturated models. Saturation
    of a model occurs if the number of training examples is equal to the number of
    features, which is a form of overparameterization. As a consequence, a saturated
    model can always fit the training data perfectly but is merely a form of interpolation
    and thus is not expected to generalize well.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LASSO的一个限制是，当*m* > *n*时，它最多选择*n*个特征，其中*n*是训练样本的数量。如果在某些特征选择应用中，这可能是不希望的。然而，在实践中，LASSO的这个特性往往是一个优势，因为它避免了模型的饱和。模型饱和是指训练样本的数量等于特征的数量，这是一种过度参数化的形式。因此，饱和模型可以总是完美地拟合训练数据，但仅仅是一种插值形式，因此不被期望能很好地推广。
- en: 'A compromise between Ridge Regression and LASSO is elastic net, which has an
    L1 penalty to generate sparsity and an L2 penalty such that it can be used for
    selecting more than *n* features if *m* > *n*:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Ridge回归和LASSO之间的折衷是弹性网，它具有L1惩罚以产生稀疏性，并且具有L2惩罚，因此在*m* > *n*时可以用于选择超过*n*个特征：
- en: '![](img/B13208_10_043.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_043.png)'
- en: Those regularized regression models are all available via scikit-learn, and
    their usage is similar to the regular regression model except that we have to
    specify the regularization strength via the parameter ![](img/B13208_10_044.png),
    for example, optimized via k-fold cross-validation.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这些正则化回归模型都可以通过scikit-learn获得，其使用方式与普通回归模型类似，不同之处在于我们必须通过参数![](img/B13208_10_044.png)指定正则化强度，例如，通过k折交叉验证进行优化。
- en: 'A Ridge Regression model can be initialized via:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下方式初始化Ridge回归模型：
- en: '[PRE22]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Note that the regularization strength is regulated by the parameter `alpha`,
    which is similar to the parameter ![](img/B13208_10_044.png). Likewise, we can
    initialize a LASSO regressor from the `linear_model` submodule:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，正则化强度由参数`alpha`控制，类似于参数![](img/B13208_10_044.png)。同样，我们可以从`linear_model`子模块初始化LASSO回归器：
- en: '[PRE23]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Lastly, the `ElasticNet` implementation allows us to vary the L1 to L2 ratio:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`ElasticNet`实现允许我们调整L1与L2的比率：
- en: '[PRE24]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: For example, if we set the `l1_ratio` to 1.0, the `ElasticNet` regressor would
    be equal to LASSO regression. For more detailed information about the different
    implementations of linear regression, please see the documentation at [http://scikit-learn.org/stable/modules/linear_model.html](http://scikit-learn.org/stable/modules/linear_model.html).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们将`l1_ratio`设置为1.0，则`ElasticNet`回归器将等于LASSO回归。有关不同线性回归实现的更多详细信息，请参阅文档：[http://scikit-learn.org/stable/modules/linear_model.html](http://scikit-learn.org/stable/modules/linear_model.html)。
- en: Turning a linear regression model into a curve – polynomial regression
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将线性回归模型转化为曲线——多项式回归
- en: 'In the previous sections, we assumed a linear relationship between explanatory
    and response variables. One way to account for the violation of linearity assumption
    is to use a polynomial regression model by adding polynomial terms:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们假设了解释变量与响应变量之间存在线性关系。为了解决线性假设的违背问题，一种方法是通过添加多项式项来使用多项式回归模型：
- en: '![](img/B13208_10_046.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_046.png)'
- en: Here, *d* denotes the degree of the polynomial. Although we can use polynomial
    regression to model a nonlinear relationship, it is still considered a multiple
    linear regression model because of the linear regression coefficients, *w*. In
    the following subsections, we will see how we can add such polynomial terms to
    an existing dataset conveniently and fit a polynomial regression model.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*d* 表示多项式的次数。虽然我们可以使用多项式回归来建模非线性关系，但由于使用的是线性回归系数 *w*，它仍然被视为一个多元线性回归模型。在接下来的子章节中，我们将看到如何方便地向现有数据集添加这些多项式项，并拟合多项式回归模型。
- en: Adding polynomial terms using scikit-learn
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 添加多项式项
- en: 'We will now learn how to use the `PolynomialFeatures` transformer class from
    scikit-learn to add a quadratic term (*d* = 2) to a simple regression problem
    with one explanatory variable. Then, we will compare the polynomial to the linear
    fit by following these steps:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将学习如何使用 scikit-learn 中的 `PolynomialFeatures` 变换器类，向一个简单回归问题中添加二次项（*d* =
    2）。然后，我们将通过以下步骤将多项式与线性拟合进行比较：
- en: 'Add a second-degree polynomial term:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加二次多项式项：
- en: '[PRE25]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Fit a simple linear regression model for comparison:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合一个简单的线性回归模型以进行比较：
- en: '[PRE26]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Fit a multiple regression model on the transformed features for polynomial
    regression:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对变换后的特征拟合多元回归模型以进行多项式回归：
- en: '[PRE27]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Plot the results:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制结果：
- en: '[PRE28]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In the resulting plot, you can see that the polynomial fit captures the relationship
    between the response and explanatory variables much better than the linear fit:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在结果图中，您可以看到多项式拟合比线性拟合更好地捕捉了响应变量与解释变量之间的关系：
- en: '![](img/B13208_10_11.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_11.png)'
- en: 'Next, we will compute the MSE and ![](img/B13208_10_047.png) evaluation metrics:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算 MSE 和 ![](img/B13208_10_047.png) 评估指标：
- en: '[PRE29]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As you can see after executing the code, the MSE decreased from 570 (linear
    fit) to 61 (quadratic fit); also, the coefficient of determination reflects a
    closer fit of the quadratic model (![](img/B13208_10_048.png)) as opposed to the
    linear fit (![](img/B13208_10_049.png)) in this particular toy problem.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在执行代码后，MSE 从 570（线性拟合）降低到 61（二次拟合）；此外，决定系数反映了二次模型（![](img/B13208_10_048.png)）相比于线性模型（![](img/B13208_10_049.png)）在这个特定的示例问题中更贴合。
- en: Modeling nonlinear relationships in the Housing dataset
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在房屋数据集中建模非线性关系
- en: 'In the preceding subsection, you learned how to construct polynomial features
    to fit nonlinear relationships in a toy problem; let''s now take a look at a more
    concrete example and apply those concepts to the data in the Housing dataset.
    By executing the following code, we will model the relationship between house
    prices and `LSTAT` (percentage of lower status of the population) using second-degree
    (quadratic) and third-degree (cubic) polynomials and compare that to a linear
    fit:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的子章节中，您学习了如何构建多项式特征以拟合玩具问题中的非线性关系；现在，让我们来看一个更实际的例子，并将这些概念应用于房屋数据集中的数据。通过执行以下代码，我们将使用二次（平方）和三次（立方）多项式建模房价与
    `LSTAT`（低收入群体百分比）之间的关系，并将其与线性拟合进行比较：
- en: '[PRE30]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The resulting plot is as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图如下所示：
- en: '![](img/B13208_10_12.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_12.png)'
- en: As you can see, the cubic fit captures the relationship between house prices
    and `LSTAT` better than the linear and quadratic fit. However, you should be aware
    that adding more and more polynomial features increases the complexity of a model
    and therefore increases the chance of overfitting. Thus, in practice, it is always
    recommended to evaluate the performance of the model on a separate test dataset
    to estimate the generalization performance.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，三次拟合比线性和二次拟合更好地捕捉了房价与 `LSTAT` 之间的关系。然而，您应该意识到，添加越来越多的多项式特征会增加模型的复杂性，从而增加过拟合的风险。因此，在实际应用中，始终建议在一个单独的测试数据集上评估模型的性能，以估计其泛化能力。
- en: 'In addition, polynomial features are not always the best choice for modeling
    nonlinear relationships. For example, with some experience or intuition, just
    looking at the `MEDV`-`LSTAT` scatterplot may lead to the hypothesis that a log-transformation
    of the `LSTAT` feature variable and the square root of `MEDV` may project the
    data onto a linear feature space suitable for a linear regression fit. For instance,
    my perception is that this relationship between the two variables looks quite
    similar to an exponential function:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，多项式特征并不总是建模非线性关系的最佳选择。例如，通过一些经验或直觉，仅查看`MEDV`-`LSTAT`散点图可能会导致一个假设，即对`LSTAT`特征变量进行对数转换，并对`MEDV`取平方根可能会将数据投射到适合线性回归拟合的线性特征空间。例如，我认为这两个变量之间的关系看起来非常类似于一个指数函数：
- en: '![](img/B13208_10_050.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_050.png)'
- en: 'Since the natural logarithm of an exponential function is a straight line,
    I assume that such a log-transformation can be usefully applied here:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 由于指数函数的自然对数是一条直线，我认为这样的对数转换在这里可能是有用的：
- en: '![](img/B13208_10_051.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_051.png)'
- en: 'Let''s test this hypothesis by executing the following code:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过执行以下代码来测试这一假设：
- en: '[PRE31]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'After transforming the explanatory onto the log space and taking the square
    root of the target variables, we were able to capture the relationship between
    the two variables with a linear regression line that seems to fit the data better
    (![](img/B13208_10_052.png)) than any of the previous polynomial feature transformations:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在将解释变量转换到对数空间并对目标变量取平方根后，我们能够通过线性回归线捕捉两个变量之间的关系，该线性回归线似乎比任何先前的多项式特征转换更适合数据（![](img/B13208_10_052.png)）：
- en: '![](img/B13208_10_13.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_13.png)'
- en: Dealing with nonlinear relationships using random forests
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林处理非线性关系
- en: In this section, we are going to take a look at **random forest** regression,
    which is conceptually different from the previous regression models in this chapter.
    A random forest, which is an ensemble of multiple **decision trees**, can be understood
    as the sum of piecewise linear functions, in contrast to the global linear and
    polynomial regression models that we discussed previously. In other words, via
    the decision tree algorithm, we subdivide the input space into smaller regions
    that become more manageable.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍**随机森林**回归，这与本章中先前介绍的回归模型在概念上有所不同。随机森林是多个**决策树**的集成，可以理解为分段线性函数的总和，与我们之前讨论的全局线性和多项式回归模型形成对比。换句话说，通过决策树算法，我们将输入空间细分为更小的区域，从而更易管理。
- en: Decision tree regression
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树回归
- en: 'An advantage of the decision tree algorithm is that it does not require any
    transformation of the features if we are dealing with nonlinear data, because
    decision trees analyze one feature at a time, rather than taking weighted combinations
    into account. (Likewise, normalizing or standardizing features is not required
    for decision trees.) You will remember from *Chapter 3*, *A Tour of Machine Learning
    Classifiers Using scikit-learn*, that we grow a decision tree by iteratively splitting
    its nodes until the leaves are pure or a stopping criterion is satisfied. When
    we used decision trees for classification, we defined entropy as a measure of
    impurity to determine which feature split maximizes the **information gain** (**IG**),
    which can be defined as follows for a binary split:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法的优点在于，如果处理非线性数据，它不需要对特征进行任何转换，因为决策树一次分析一个特征，而不考虑加权组合。（同样，决策树不需要归一化或标准化特征。）你会记得*第三章*，*使用scikit-learn进行机器学习分类器之旅*，我们通过迭代地分割其节点来生长决策树，直到叶子节点纯净或满足停止准则。当我们将决策树用于分类时，我们定义熵作为不纯度的度量，以确定哪个特征分割最大化**信息增益**（**IG**），可以如下定义用于二进制分割：
- en: '![](img/B13208_10_053.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_053.png)'
- en: 'Here, ![](img/B13208_10_054.png) is the feature to perform the split, ![](img/B13208_10_055.png)
    is the number of training examples in the parent node, *I* is the impurity function,
    ![](img/B13208_10_056.png) is the subset of training examples at the parent node,
    and ![](img/B13208_10_057.png) and ![](img/B13208_10_058.png) are the subsets
    of training examples at the left and right child nodes after the split. Remember
    that our goal is to find the feature split that maximizes the information gain;
    in other words, we want to find the feature split that reduces the impurities
    in the child nodes most. In *Chapter 3*, *A Tour of Machine Learning Classifiers
    Using scikit-learn*, we discussed Gini impurity and entropy as measures of impurity,
    which are both useful criteria for classification. To use a decision tree for
    regression, however, we need an impurity metric that is suitable for continuous
    variables, so we define the impurity measure of a node, *t*, as the MSE instead:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_10_054.png)是执行分裂的特征，![](img/B13208_10_055.png)是父节点中的训练样本数量，*I*是纯度函数，![](img/B13208_10_056.png)是父节点中的训练样本子集，而![](img/B13208_10_057.png)和![](img/B13208_10_058.png)分别是分裂后左右子节点中的训练样本子集。请记住，我们的目标是找到最大化信息增益的特征分裂；换句话说，我们希望找到能够最大程度减少子节点纯度的特征分裂。在*第3章*，*使用scikit-learn的机器学习分类器导览*中，我们讨论了基尼纯度和熵作为纯度度量，它们都是分类问题中的有用标准。然而，若要使用决策树进行回归，我们需要一个适合连续变量的纯度度量，因此我们将节点*t*的纯度度量定义为均方误差（MSE）：
- en: '![](img/B13208_10_059.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_059.png)'
- en: 'Here, ![](img/B13208_10_060.png) is the number of training examples at node
    *t*, ![](img/B13208_10_061.png) is the training subset at node *t*, ![](img/B13208_10_062.png)
    is the true target value, and ![](img/B13208_10_063.png) is the predicted target
    value (sample mean):'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_10_060.png)是节点*t*中的训练样本数量，![](img/B13208_10_061.png)是节点*t*中的训练子集，![](img/B13208_10_062.png)是真实目标值，而![](img/B13208_10_063.png)是预测的目标值（样本均值）：
- en: '![](img/B13208_10_064.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_064.png)'
- en: 'In the context of decision tree regression, the MSE is often referred to as
    **within-node variance**, which is why the splitting criterion is also better
    known as **variance reduction**. To see what the line fit of a decision tree looks
    like, let''s use the `DecisionTreeRegressor` implemented in scikit-learn to model
    the nonlinear relationship between the `MEDV` and `LSTAT` variables:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树回归的背景下，MSE通常被称为**节点内方差**，这也是为什么分裂标准更常被称为**方差减少**。为了看到决策树的拟合效果，让我们使用scikit-learn中实现的`DecisionTreeRegressor`来建模`MEDV`和`LSTAT`变量之间的非线性关系：
- en: '[PRE32]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As you can see in the resulting plot, the decision tree captures the general
    trend in the data. However, a limitation of this model is that it does not capture
    the continuity and differentiability of the desired prediction. In addition, we
    need to be careful about choosing an appropriate value for the depth of the tree
    so as to not overfit or underfit the data; here, a depth of three seemed to be
    a good choice:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在结果图中所看到的，决策树捕捉到了数据中的一般趋势。然而，这个模型的一个局限性是，它没有捕捉到期望预测的连续性和可微性。此外，我们还需要小心选择决策树的适当深度，以避免过拟合或欠拟合数据；在这里，深度为三似乎是一个不错的选择：
- en: '![](img/B13208_10_14.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_14.png)'
- en: 'In the next section, we will take a look at a more robust way of fitting regression
    trees: random forests.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍一种更强大的回归树拟合方法：随机森林。
- en: Random forest regression
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林回归
- en: As you learned in *Chapter 3*, *A Tour of Machine Learning Classifiers Using
    scikit-learn*, the random forest algorithm is an ensemble technique that combines
    multiple decision trees. A random forest usually has a better generalization performance
    than an individual decision tree due to randomness, which helps to decrease the
    model's variance. Other advantages of random forests are that they are less sensitive
    to outliers in the dataset and don't require much parameter tuning. The only parameter
    in random forests that we typically need to experiment with is the number of trees
    in the ensemble. The basic random forest algorithm for regression is almost identical
    to the random forest algorithm for classification that we discussed in *Chapter
    3*, *A Tour of Machine Learning Classifiers Using scikit-learn*. The only difference
    is that we use the MSE criterion to grow the individual decision trees, and the
    predicted target variable is calculated as the average prediction over all decision
    trees.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在*第3章*《使用scikit-learn的机器学习分类器概览》中学到的那样，随机森林算法是一种集成技术，它结合了多个决策树。由于随机性，随机森林通常比单独的决策树具有更好的泛化性能，这有助于降低模型的方差。随机森林的其他优点是它们对数据集中的异常值不太敏感，并且不需要太多的参数调优。随机森林中我们通常需要实验的唯一参数是集成中树的数量。回归的基本随机森林算法与我们在*第3章*《使用scikit-learn的机器学习分类器概览》中讨论的分类的随机森林算法几乎相同。唯一的区别是我们使用MSE标准来生长各个决策树，预测的目标变量是通过所有决策树的平均预测值来计算的。
- en: 'Now, let''s use all the features in the Housing dataset to fit a random forest
    regression model on 60 percent of the examples and evaluate its performance on
    the remaining 40 percent. The code is as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用住房数据集中的所有特征，在60%的样本上拟合一个随机森林回归模型，并在剩余的40%的样本上评估其性能。代码如下：
- en: '[PRE33]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Unfortunately, you can see that the random forest tends to overfit the training
    data. However, it's still able to explain the relationship between the target
    and explanatory variables relatively well (![](img/B13208_10_065.png) on the test
    dataset).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，你可以看到随机森林倾向于过拟合训练数据。然而，它仍然能够相对较好地解释目标变量和解释变量之间的关系（在测试数据集上见![](img/B13208_10_065.png)）。
- en: 'Lastly, let''s also take a look at the residuals of the prediction:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们也来看一下预测的残差：
- en: '[PRE34]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'As it was already summarized by the ![](img/B13208_10_066.png) coefficient,
    you can see that the model fits the training data better than the test data, as
    indicated by the outliers in the *y* axis direction. Also, the distribution of
    the residuals does not seem to be completely random around the zero center point,
    indicating that the model is not able to capture all the exploratory information.
    However, the residual plot indicates a large improvement over the residual plot
    of the linear model that we plotted earlier in this chapter:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 正如通过![](img/B13208_10_066.png)系数已经总结的那样，你可以看到模型在训练数据上的拟合效果优于测试数据，正如*y*轴方向上的异常值所示。此外，残差的分布似乎并非完全围绕零中心点随机分布，这表明模型无法捕捉到所有的探索性信息。然而，残差图显示出相较于我们在本章早些时候绘制的线性模型的残差图，模型有了很大的改进：
- en: '![](img/B13208_10_15.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_10_15.png)'
- en: Ideally, our model error should be random or unpredictable. In other words,
    the error of the predictions should not be related to any of the information contained
    in the explanatory variables; rather, it should reflect the randomness of the
    real-world distributions or patterns. If we find patterns in the prediction errors,
    for example, by inspecting the residual plot, it means that the residual plots
    contain predictive information. A common reason for this could be that explanatory
    information is leaking into those residuals.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们的模型误差应该是随机的或不可预测的。换句话说，预测的误差不应与任何解释变量中包含的信息相关，而应反映现实世界分布或模式的随机性。如果我们在预测误差中发现了模式，例如通过检查残差图，这意味着残差图中包含了预测信息。一个常见的原因可能是解释变量的信息泄漏到了这些残差中。
- en: Unfortunately, there is not a universal approach for dealing with non-randomness
    in residual plots, and it requires experimentation. Depending on the data that
    is available to us, we may be able to improve the model by transforming variables,
    tuning the hyperparameters of the learning algorithm, choosing simpler or more
    complex models, removing outliers, or including additional variables.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，目前没有一种通用方法来处理残差图中的非随机性问题，这需要通过实验来解决。根据我们所拥有的数据，我们可能能够通过转换变量、调优学习算法的超参数、选择简单或复杂的模型、移除离群值或增加额外的变量来改进模型。
- en: '**Regression with support vector machines**'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机回归**'
- en: 'In *Chapter 3*, *A Tour of Machine Learning Classifiers Using scikit-learn*,
    we also learned about the kernel trick, which can be used in combination with
    a **support vector machine** (**SVM**) for classification, and is useful if we
    are dealing with nonlinear problems. Although a discussion is beyond the scope
    of this book, SVMs can also be used in nonlinear regression tasks. The interested
    reader can find more information about SVMs for regression in an excellent report:
    *Support Vector Machines for Classification and Regression*, *S. R. Gunn* and
    others, *University of Southampton technical report*, 14, *1998* ([http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.579.6867&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.579.6867&rep=rep1&type=pdf)).
    An SVM regressor is also implemented in scikit-learn, and more information about
    its usage can be found at [http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR).'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3章*，*使用scikit-learn的机器学习分类器概览*中，我们还学习了核技巧，核技巧可以与**支持向量机**（**SVM**）结合使用进行分类，并且在处理非线性问题时非常有用。虽然讨论超出了本书的范围，但SVM也可以应用于非线性回归任务。感兴趣的读者可以参考一篇关于SVM回归的优秀报告：《用于分类和回归的支持向量机》，*S.
    R. Gunn*等人，*南安普敦大学技术报告*，14，*1998*（[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.579.6867&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.579.6867&rep=rep1&type=pdf)）。scikit-learn中也实现了SVM回归器，关于其使用的更多信息可以参考[http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR)。
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: At the beginning of this chapter, you learned about simple linear regression
    analysis to model the relationship between a single explanatory variable and a
    continuous response variable. We then discussed a useful explanatory data analysis
    technique to look at patterns and anomalies in data, which is an important first
    step in predictive modeling tasks.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开始时，您学习了简单线性回归分析，用于建模单一解释变量与连续响应变量之间的关系。接着，我们讨论了一种有用的解释性数据分析技术，用于观察数据中的模式和异常，这在预测建模任务中是一个重要的第一步。
- en: 'We built our first model by implementing linear regression using a gradient-based
    optimization approach. You then saw how to utilize scikit-learn''s linear models
    for regression and also implement a robust regression technique (RANSAC) as an
    approach for dealing with outliers. To assess the predictive performance of regression
    models, we computed the mean sum of squared errors and the related ![](img/B13208_10_067.png)
    metric. Furthermore, we also discussed a useful graphical approach to diagnose
    the problems of regression models: the residual plot.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过实现基于梯度优化方法的线性回归来构建了我们的第一个模型。然后，您看到了如何利用scikit-learn的线性模型进行回归，并实现了一种鲁棒回归技术（RANSAC）作为处理离群值的一种方法。为了评估回归模型的预测性能，我们计算了均方误差和相关的![](img/B13208_10_067.png)度量。此外，我们还讨论了一种有用的图形化方法来诊断回归模型的问题：残差图。
- en: After we explored how regularization can be applied to regression models to
    reduce the model complexity and avoid overfitting, we also covered several approaches
    to model nonlinear relationships, including polynomial feature transformation
    and random forest regressors.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索了如何将正则化应用于回归模型以减少模型复杂度并避免过拟合之后，我们还介绍了几种建模非线性关系的方法，包括多项式特征转换和随机森林回归器。
- en: We discussed supervised learning, classification, and regression analysis in
    great detail in the previous chapters. In the next chapter, we are going to learn
    about another interesting subfield of machine learning, unsupervised learning,
    and also how to use cluster analysis to find hidden structures in data in the
    absence of target variables.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们详细讨论了监督学习、分类和回归分析。在下一章中，我们将学习机器学习的另一个有趣的子领域——无监督学习，以及如何在没有目标变量的情况下使用聚类分析来发掘数据中的隐藏结构。
