- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Predicting Online Ad Click-Through with Logistic Regression
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用逻辑回归预测在线广告点击率
- en: In the previous chapter, we predicted ad click-through using tree algorithms.
    In this chapter, we will continue our journey of tackling the billion-dollar problem.
    We will focus on learning a very (probably the most) scalable classification model
    – logistic regression. We will explore what the logistic function is, how to train
    a logistic regression model, adding regularization to the model, and variants
    of logistic regression that are applicable to very large datasets. Besides its
    application in classification, we will also discuss how logistic regression and
    random forest models are used to pick significant features. You won’t get bored
    as there will be lots of implementations from scratch with scikit-learn and TensorFlow.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用基于树的算法预测了广告点击率。在本章中，我们将继续探索解决数十亿美元问题的旅程。我们将重点学习一种非常（可能是最）可扩展的分类模型——逻辑回归。我们将探讨逻辑函数是什么，如何训练逻辑回归模型，如何为模型添加正则化，以及适用于非常大数据集的逻辑回归变种。除了在分类中的应用外，我们还将讨论如何使用逻辑回归和随机森林模型来选择重要特征。你不会感到无聊，因为我们将有很多从零开始的实现，使用
    scikit-learn 和 TensorFlow。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Converting categorical features to numerical – one-hot encoding and original
    encoding
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将分类特征转换为数值型特征——独热编码和原始编码
- en: Classifying data with logistic regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用逻辑回归对数据进行分类
- en: Training a logistic regression model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练逻辑回归模型
- en: Training on large datasets with online learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用在线学习训练大规模数据集
- en: Handling multiclass classification
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理多类分类问题
- en: Implementing logistic regression using TensorFlow
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 实现逻辑回归
- en: Converting categorical features to numerical – one-hot encoding and ordinal
    encoding
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将分类特征转换为数值型特征——独热编码和原始编码
- en: In *Chapter 3*, *Predicting Online Ad Click-Through with Tree-Based Algorithms*,
    I mentioned how **one-hot encoding** transforms categorical features to numerical
    features in order to use them in the tree algorithms in scikit-learn and TensorFlow.
    If we transform categorical features into numerical ones using one-hot encoding,
    we don’t limit our choice of algorithms to the tree-based ones that can work with
    categorical features.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3章*，*使用基于树的算法预测在线广告点击率*中，我提到过**独热编码**如何将分类特征转换为数值特征，以便在 scikit-learn 和 TensorFlow
    的树算法中使用。如果我们使用独热编码将分类特征转换为数值特征，就不会将算法的选择局限于能够处理分类特征的基于树的算法。
- en: The simplest solution we can think of in terms of transforming a categorical
    feature with *k* possible values is to map it to a numerical feature with values
    from 1 to *k*. For example, `[Tech, Fashion, Fashion, Sports, Tech, Tech, Sports]`
    becomes `[1, 2, 2, 3, 1, 1, 3]`. However, this will impose an ordinal characteristic,
    such as `Sports` being greater than `Tech`, and a distance property, such as `Sports`
    being closer to `Fashion`than to `Tech`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能想到的最简单的解决方案是，将具有 *k* 个可能值的分类特征映射到一个数值特征，数值范围从 1 到 *k*。例如，`[Tech, Fashion,
    Fashion, Sports, Tech, Tech, Sports]` 变成 `[1, 2, 2, 3, 1, 1, 3]`。然而，这将引入顺序特性，例如
    `Sports` 大于 `Tech`，以及距离特性，例如 `Sports` 距离 `Fashion` 比 `Tech` 更近。
- en: 'Instead, one-hot encoding converts the categorical feature to *k* binary features.
    Each binary feature indicates the presence or absence of a corresponding possible
    value. Hence, the preceding example becomes the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，独热编码将分类特征转换为 *k* 个二进制特征。每个二进制特征表示是否存在相应的可能值。因此，前面的示例变成了以下内容：
- en: '![A picture containing text, screenshot, number, display  Description automatically
    generated](img/B21047_04_01.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text, screenshot, number, display  Description automatically
    generated](img/B21047_04_01.png)'
- en: 'Figure 4.1: Transforming user interest into numerical features with one-hot
    encoding'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：使用独热编码将用户兴趣转换为数值特征
- en: 'Previously, we used `OneHotEncoder` from scikit-learn to convert a matrix of
    strings into a binary matrix, but here, let’s take a look at another module, `DictVectorizer`,
    which also provides an efficient conversion. It transforms dictionary objects
    (categorical feature: value) into one-hot encoded vectors.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们使用了来自 scikit-learn 的 `OneHotEncoder` 将字符串矩阵转换为二进制矩阵，但在这里，让我们来看看另一个模块 `DictVectorizer`，它也提供了高效的转换。它将字典对象（分类特征：值）转换为独热编码向量。
- en: 'For example, take a look at the following code, which performs one-hot encoding
    on a list of dictionaries containing categorical features:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，看看以下代码，它对包含分类特征的字典列表执行独热编码：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can also see the mapping by executing the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过执行以下操作查看映射：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When it comes to new data, we can transform it with the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理新数据时，我们可以通过以下方式进行转换：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can inversely transform the encoded features back to the original features
    like this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像这样将编码后的特征逆向转换回原始特征：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'One important thing to note is that if a new (not seen in training data) category
    is encountered in new data, it should be ignored (otherwise, the encoder will
    complain about the unseen categorical value). `DictVectorizer` handles this implicitly
    (while `OneHotEncoder` needs to specify the `ignore` parameter):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，如果在新数据中遇到一个新的（训练数据中未出现过的）类别，它应该被忽略（否则，编码器会抱怨未见过的类别值）。`DictVectorizer`会隐式处理这个问题（而`OneHotEncoder`需要指定`ignore`参数）：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Sometimes, we prefer transforming a categorical feature with *k* possible values
    into a numerical feature with values ranging from *1* to *k*. This is **ordinal
    encoding** and we conduct it in order to employ ordinal or ranking knowledge in
    our learning; for example, large, medium, and small become 3, 2, and 1, respectively;
    good and bad become 1 and 0, while one-hot encoding fails to preserve such useful
    information. We can realize ordinal encoding easily through the use of `pandas`,
    for example:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们更倾向于将具有*k*个可能值的类别特征转换为一个数值特征，取值范围从*1*到*k*。这就是**顺序编码**，我们进行顺序编码是为了在学习中利用顺序或排名知识；例如，大、中、小分别变为3、2和1；好和坏分别变为1和0，而独热编码无法保留这样的有用信息。我们可以通过使用`pandas`轻松实现顺序编码，例如：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We convert the string feature into ordinal values based on the mapping we define.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据定义的映射将字符串特征转换为顺序值。
- en: '**Best practice**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'Handling high dimensionality resulting from one-hot encoding can be challenging.
    It may increase computational complexity or lead to overfitting. Here are some
    strategies to handle high dimensionality when using one-hot encoding:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 处理由独热编码导致的高维度可能是具有挑战性的。这可能会增加计算复杂性或导致过拟合。以下是一些在使用独热编码时处理高维度的策略：
- en: '**Feature selection**: This can reduce the number of one-hot encoded features
    while retaining the most informative ones.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择**：这可以减少独热编码特征的数量，同时保留最有信息量的特征。'
- en: '**Dimensionality reduction**: It transforms the high-dimensional feature space
    into a lower-dimensional representation.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：它将高维特征空间转换为低维表示。'
- en: '**Feature aggregation**: Instead of one-hot encoding every category individually,
    consider aggregating categories that share similar characteristics. For example,
    group rare categories into an “Other” category.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征聚合**：与其为每个类别单独进行独热编码，不如考虑将具有相似特征的类别进行聚合。例如，将罕见类别归为“其他”类别。'
- en: We’ve covered transforming categorical features into numerical ones. Next, we
    will talk about logistic regression, a classifier that only takes in numerical
    features.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了如何将类别特征转换为数值特征。接下来，我们将讨论逻辑回归，这是一种只接受数值特征的分类器。
- en: Classifying data with logistic regression
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用逻辑回归进行数据分类
- en: In the last chapter, we trained tree-based models only based on the first 300,000
    samples out of 40 million. We did so simply because training a tree on a large
    dataset is extremely computationally expensive and time consuming. Since we are
    not limited to algorithms directly taking in categorical features thanks to one-hot
    encoding, we should turn to a new algorithm with high scalability for large datasets.
    As mentioned, logistic regression is one of the most, or perhaps the most, scalable
    classification algorithms.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们仅使用了来自4000万样本中的前30万个样本来训练基于树的模型。之所以这样做，是因为在大数据集上训练树模型的计算代价和时间开销非常大。由于我们不再局限于直接接受类别特征的算法（这要感谢独热编码），我们应该转向一种适合大数据集的高可扩展性算法。如前所述，逻辑回归是最具可扩展性的分类算法之一，甚至可能是最具可扩展性的。
- en: Getting started with the logistic function
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用逻辑函数
- en: 'Let’s start with an introduction to the **logistic function** (which is more
    commonly referred to as the **sigmoid function**) as the algorithm’s core before
    we dive into the algorithm itself. It basically maps an input to an output of
    a value between *0* and *1*, and is defined as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解算法之前，我们先介绍一下**逻辑函数**（更常称为**sigmoid函数**），它是该算法的核心。它基本上将输入映射到0到1之间的输出值，其定义如下：
- en: '![](img/B21047_04_001.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_001.png)'
- en: 'We define the logistic function as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义逻辑函数如下：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we visualize what it looks like with input variables from -`8` to `8`,
    as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可视化输入变量在`-8`到`8`之间的变化，结果如下：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Refer to the following screenshot for the result:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考下方截图查看结果：
- en: '![A picture containing text, line, screenshot, plot  Description automatically
    generated](img/B21047_04_02.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含文本、线条、截图、图表的图片  描述自动生成](img/B21047_04_02.png)'
- en: 'Figure 4.2: The logistic function'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：逻辑函数
- en: In the S-shaped curve, all inputs are transformed into the range from 0 to 1\.
    For positive inputs, a greater value results in an output closer to 1; for negative
    inputs, a smaller value generates an output closer to 0; when the input is 0,
    the output is the midpoint, 0.5.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在S形曲线中，所有输入都会被转换到 0 到 1 的范围内。对于正输入，较大的值使得输出接近 1；对于负输入，较小的值使得输出接近 0；当输入为 0 时，输出是中点，即
    0.5。
- en: Jumping from the logistic function to logistic regression
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从逻辑函数跳转到逻辑回归
- en: 'Now that you have some knowledge of the logistic function, it is easy to map
    it to the algorithm that stems from it. In logistic regression, the function input
    *z* becomes the weighted sum of features. Given a data sample *x* with *n* features,
    *x*[1]*, x*[2]*, …, x*[n] (*x* represents a feature vector and *x* = (*x*[1]*,
    x*[2]*, …, x*[n]*)*), and **weights** (also called **coefficients**) of the model
    *w* (*w* represents a vector (*w*[1]*, w*[2]*, …, w*[n])), *z* is expressed as
    follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对逻辑函数有了一些了解，映射到源自它的算法就容易多了。在逻辑回归中，函数输入*z*是特征的加权和。给定一个具有*n*个特征的数据样本*x*，其中
    *x*[1]*, x*[2]*, …, x*[n]（*x*代表特征向量，*x* = (*x*[1]*, x*[2]*, …, x*[n]*)），以及模型的**权重**（也叫**系数**）*w*（*w*表示向量(*w*[1]*,
    w*[2]*, …, w*[n]）），*z*表示如下：
- en: '![](img/B21047_04_002.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_002.png)'
- en: Here, *T* is the transpose operator.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*T*是转置操作符。
- en: 'Occasionally, the model comes with an **intercept** (also called **bias**),
    *w*[0], which accounts for the inherent bias or baseline probability. In this
    instance, the preceding linear relationship becomes:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，模型会带有一个**截距**（也叫**偏置**），*w*[0]，它表示固有的偏差或基线概率。在这种情况下，前述的线性关系变为：
- en: '![](img/B21047_04_003.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_003.png)'
- en: 'As for the output *y*(*z*)in the range of 0 to 1, in the algorithm, it becomes
    the probability of the target being *1* or the positive class:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 至于输出*y*(*z*)的范围在 0 到 1 之间，在算法中，它表示目标为*1*或正类的概率：
- en: '![](img/B21047_04_004.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_004.png)'
- en: Hence, logistic regression is a probabilistic classifier, similar to the Naïve
    Bayes classifier.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，逻辑回归是一种概率分类器，类似于朴素贝叶斯分类器。
- en: 'A logistic regression model, or, more specifically, its weight vector *w*,
    is learned from the training data, with the goal of predicting a positive sample
    as close to *1* as possible and predicting a negative sample as close to 0 as
    possible. In mathematical language, the weights are trained to minimize the cost
    defined as the **Mean Squared Error** (**MSE**), which measures the average of
    squares of the difference between the truth and the prediction. Given *m* training
    samples:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型，或者更具体地说，其权重向量*w*，是通过训练数据学习得到的，目的是使得正样本尽可能接近*1*，负样本尽可能接近 0。在数学上，权重的训练目标是最小化定义为**均方误差**（**MSE**）的成本函数，该函数衡量真实值与预测值之间差值的平方的平均值。给定*m*个训练样本：
- en: '![](img/B21047_04_005.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_005.png)'
- en: 'Here, *y*^((i)) is either `1` (positive class) or `0` (negative class), and
    the cost function *J*(*w*) regarding the weights to be optimized is expressed
    as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*y*^((i))的值要么是`1`（正类），要么是`0`（负类），而需要优化的权重所对应的成本函数*J*(*w*)表示如下：
- en: '![](img/B21047_04_006.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_006.png)'
- en: However, the preceding cost function is **non-convex**, which means that, when
    searching for the optimal *w*, many local (suboptimal) optimums are found and
    the function does not converge to a global optimum.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，前述的成本函数是**非凸**的，这意味着在寻找最优的*w*时，会发现许多局部（次优）最优解，且该函数不会收敛到全局最优解。
- en: 'Examples of the **convex** and **non-convex** functions are plotted respectively
    in the following figure:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**凸**函数和**非凸**函数的示例分别如图所示：'
- en: '![A picture containing line, text, diagram, plot  Description automatically
    generated](img/B21047_04_03.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含线条、文本、图表、图形的图片  描述自动生成](img/B21047_04_03.png)'
- en: 'Figure 4.3: Examples of convex and non-convex functions'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：凸函数和非凸函数的示例
- en: In the convex example, there is only one global optimum, while there are two
    optimums in the non-convex example.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在凸函数示例中，只有一个全局最优解，而在非凸函数示例中，有两个最优解。
- en: For more about convex and non-convex functions, check out [https://web.stanford.edu/class/ee364a/lectures/functions.pdf](https://web.stanford.edu/class/ee364a/lectures/functions.pdf).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于凸函数和非凸函数的信息，请查看 [https://web.stanford.edu/class/ee364a/lectures/functions.pdf](https://web.stanford.edu/class/ee364a/lectures/functions.pdf)。
- en: 'To overcome this, in practice, we use the cost function that results in a convex
    optimization problem, which is defined as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题，在实践中，我们使用导致凸优化问题的成本函数，该函数定义如下：
- en: '![](img/B21047_04_007.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_007.png)'
- en: 'We can take a closer look at the cost of a single training sample:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更详细地查看单个训练样本的成本：
- en: '![](img/B21047_04_008.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_008.png)'
- en: '![](img/B21047_04_009.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_009.png)'
- en: 'When the ground truth *y*^((i)) *= 1*, if the model predicts correctly with
    full confidence (the positive class with 100% probability), the sample cost *j*
    is *0*; the cost *j* increases when the predicted probability ![](img/B21047_04_010.png)
    decreases. If the model incorrectly predicts that there is no chance of the positive
    class, the cost is infinitely high. We can visualize it as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当真实值 *y*^((i)) *= 1* 时，如果模型完全自信地正确预测（正类的概率为100%），样本成本 *j* 为 *0*；当预测概率 ![](img/B21047_04_010.png)
    下降时，成本 *j* 会增加。如果模型错误地预测正类没有任何机会，则成本无限高。我们可以通过以下方式进行可视化：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Refer to the following graph for the end result:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下图表查看最终结果：
- en: '![A picture containing screenshot, display, line, plot  Description automatically
    generated](img/B21047_04_04.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing screenshot, display, line, plot  Description automatically
    generated](img/B21047_04_04.png)'
- en: 'Figure 4.4: Cost function of logistic regression when y=1'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4：当 y=1 时逻辑回归的成本函数
- en: 'On the contrary, when the ground truth *y*^((i)) *= 0*, if the model predicts
    correctly with full confidence (the positive class with *0* probability, or the
    negative class with 100% probability), the sample cost *j* is *0*; the cost j
    increases when the predicted probability ![](img/B21047_04_010.png) increases.
    When it incorrectly predicts that there is no chance of the negative class, the
    cost becomes infinitely high. We can visualize it using the following code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，当真实值 *y*^((i)) *= 0* 时，如果模型完全自信地正确预测（正类的概率为 *0*，或负类的概率为100%），样本成本 *j* 为 *0*；当预测概率
    ![](img/B21047_04_010.png) 增加时，成本 *j* 会增加。当模型错误地预测没有负类的可能性时，成本将变得无限高。我们可以使用以下代码进行可视化：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following graph is the resultant output:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是结果输出：
- en: '![A picture containing screenshot, line, rectangle, plot  Description automatically
    generated](img/B21047_04_05.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing screenshot, line, rectangle, plot  Description automatically
    generated](img/B21047_04_05.png)'
- en: 'Figure 4.5: Cost function of logistic regression when y=0'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：当 y=0 时逻辑回归的成本函数
- en: 'Minimizing this alternative cost function is actually equivalent to minimizing
    the MSE-based cost function. The advantages of choosing it over the MSE version
    include the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化这个替代的成本函数实际上等同于最小化基于MSE的成本函数。选择它而不是MSE版本的优点包括：
- en: It is convex, so the optimal model weights can be found
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是凸的，因此可以找到最优的模型权重
- en: 'A summation of the logarithms of prediction, which are as follows, simplifies
    the calculation of its derivative with respect to the weights, which we will talk
    about later:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测的对数和如下所示，它简化了关于权重的导数计算，稍后我们将讨论这一点：
- en: '![](img/B21047_04_012.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_012.png)'
- en: 'Or:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 或者：
- en: '![](img/B21047_04_013.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_013.png)'
- en: 'Due to the logarithmic function, the cost function, which is as follows, is
    also called **logarithmic loss**, or simply **log loss**:'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于对数函数，以下成本函数也被称为**对数损失**，简称**log loss**：
- en: '![](img/B21047_04_014.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_014.png)'
- en: Now that the cost function is ready, how can we train the logistic regression
    model to minimize the cost function? Let’s see this in the next section.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在成本函数已经准备好，我们如何训练逻辑回归模型以最小化成本函数？让我们在下一节中看看。
- en: Training a logistic regression model
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练逻辑回归模型
- en: 'Now, the question is as follows: how can we obtain the optimal *w* such that
    *J*(*w*) is minimized? We can do so using gradient descent.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问题如下：我们如何获得最优的 *w* 使得 *J*(*w*) 最小化？我们可以通过梯度下降法来实现。
- en: Training a logistic regression model using gradient descent
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用梯度下降法训练逻辑回归模型
- en: '**Gradient descent** (also called **steepest descent**) is a procedure for
    minimizing a loss function by first-order iterative optimization. In each iteration,
    the model parameters move a small step that is proportional to the negative derivative
    of the objective function at the current point. This means the to-be-optimal point
    iteratively moves downhill toward the minimal value of the objective function.
    The proportion we just mentioned is called the **learning rate**, or **step size**.
    It can be summarized in a mathematical equation as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度下降**（也叫**最速下降法**）是一种通过一阶迭代优化最小化损失函数的方法。在每次迭代中，模型参数会根据目标函数在当前点的负导数移动一个小步长。这意味着待优化的点会迭代地沿着目标函数的最小值方向向下移动。我们刚才提到的步长比例称为**学习率**，也叫**步长**。它可以用一个数学方程总结如下：'
- en: '![](img/B21047_04_015.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_015.png)'
- en: Here, the left *w* is the weight vector after a learning step, and the right
    *w* is the one before moving, ![](img/B21047_04_016.png) is the learning rate,
    and ![](img/B21047_04_017.png) is the first-order derivative, the gradient.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，左边的 *w* 是学习一步后的权重向量，右边的 *w* 是学习前的权重，![](img/B21047_04_016.png) 是学习率，![](img/B21047_04_017.png)
    是一阶导数，梯度。
- en: 'To train a logistic regression model using gradient descent, let’s start with
    the derivative of the cost function *J*(*w*) with respect to *w*. It might require
    some knowledge of calculus but don’t worry, we will walk through it step by step:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用梯度下降训练逻辑回归模型，我们从成本函数 *J*(*w*) 对 *w* 的导数开始。这可能需要一些微积分的知识，但不用担心，我们将一步步讲解：
- en: 'We first calculate the derivative of ![](img/B21047_04_018.png) with respect
    to *w*. We herein take the *j-th* weight, *w*[j], as an example (note *z=w*^T*x*,
    and we omit the ^((i)) for simplicity):'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先计算 ![](img/B21047_04_018.png) 对 *w* 的导数。我们在这里以 *j-th* 权重 *w*[j] 为例（注意 *z=w*^T*x*，为简便起见我们省略了
    ^((i))）：
- en: '![](img/B21047_04_019.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_019.png)'
- en: '![](img/B21047_04_020.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_020.png)'
- en: '![](img/B21047_04_021.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_021.png)'
- en: 'Then, we calculate the derivative of the sample cost *J*(*w*) as follows:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们按如下方式计算样本成本 *J*(*w*) 的导数：
- en: '![](img/B21047_04_022.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_022.png)'
- en: '![](img/B21047_04_023.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_023.png)'
- en: '![](img/B21047_04_024.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_024.png)'
- en: '![](img/B21047_04_025.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_025.png)'
- en: 'Finally, we calculate the entire cost over *m* samples as follows:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们按以下方式计算 *m* 个样本的整体成本：
- en: '![](img/B21047_04_026.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_026.png)'
- en: 'We then generalize it to ![](img/B21047_04_027.png):'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将其推广到 ![](img/B21047_04_027.png)：
- en: '![](img/B21047_04_028.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_028.png)'
- en: 'Combined with the preceding derivations, the weights can be updated as follows:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结合前面的推导，权重可以如下更新：
- en: '![](img/B21047_04_029.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_029.png)'
- en: Here, *w* gets updated in each iteration.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*w* 在每次迭代中都会更新。
- en: 'After a substantial number of iterations, the learned parameter *w* is then
    used to classify a new sample *x*’ by means of the following equation:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经过大量迭代后，学习到的参数 *w* 将用于通过以下方程对新样本 *x*’ 进行分类：
- en: '![](img/B21047_04_030.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_030.png)'
- en: '![](img/B21047_04_031.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_031.png)'
- en: The decision threshold is `0.5` by default, but it definitely can be other values.
    In cases where a false negative is supposed to be avoided, for example, when predicting
    fire occurrence (the positive class) for alerts, the decision threshold can be
    lower than `0.5`, such as `0.3`, depending on how paranoid we are and how proactively
    we want to prevent the positive event from happening. On the other hand, when
    the false positive class is the one that should be evaded, for instance, when
    predicting the product success (the positive class) rate for quality assurance,
    the decision threshold can be greater than `0.5`, such as `0.7`, or lower than
    `0.5`, depending on how high a standard you set.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，决策阈值为 `0.5`，但它可以是其他值。例如，在避免假阴性时，比如预测火灾发生（正类）时的警报，决策阈值可以低于 `0.5`，如 `0.3`，具体取决于我们的警觉性以及我们希望多主动地防止正类事件的发生。另一方面，当假阳性类是需要避免的情况时，例如在质量保证中预测产品成功率（正类），决策阈值可以大于
    `0.5`，如 `0.7`，或者低于 `0.5`，具体取决于你设定的标准。
- en: 'With a thorough understanding of the gradient descent-based training and predicting
    process, we will now implement the logistic regression algorithm from scratch:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对基于梯度下降的训练和预测过程的深入理解，我们现在将从头实现逻辑回归算法：
- en: 'We begin by defining the function that computes the prediction ![](img/B21047_04_032.png)
    with the current weights:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义计算预测值的函数！[](img/B21047_04_032.png)，使用当前的权重：
- en: '[PRE10]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'With this, we are able to continue with the function updating the weights,
    which is as follows, by one step in a gradient descent manner:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以继续以梯度下降的方式更新权重函数，步骤如下：
- en: '![](img/B21047_04_033.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_033.png)'
- en: 'Take a look at the following code:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下代码：
- en: '[PRE11]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, the function calculating the cost *J*(*w*) is implemented as well:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，实现计算成本 *J*(*w*) 的函数：
- en: '[PRE12]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we connect all these functions to the model training function by executing
    the following:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过执行以下代码将所有这些函数连接到模型训练函数中：
- en: Updating the `weights` vector in each iteration
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每次迭代中更新`weights`向量
- en: Printing out the current cost for every `100` (this can be another value) iterations
    to ensure `cost` is decreasing and that things are on the right track
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每`100`次迭代（可以是其他值）打印当前的成本，以确保`cost`在减少，并且一切都在正确的轨道上。
- en: 'They are implemented in the following function:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 它们在以下函数中实现：
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we predict the results of new inputs using the trained model as follows:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用训练好的模型预测新输入的结果，方法如下：
- en: '[PRE14]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Implementing logistic regression is very simple, as you just saw. Let’s now
    examine it using a toy example:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 实现逻辑回归非常简单，就像你刚刚看到的那样。现在让我们通过一个玩具示例来进一步研究：
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We train a logistic regression model for `1000` iterations, at a learning rate
    of `0.1` based on intercept-included weights:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练一个逻辑回归模型，进行`1000`次迭代，学习率为`0.1`，基于包含截距的权重：
- en: '[PRE16]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The decreasing cost means that the model is being optimized over time. We can
    check the model’s performance on new samples as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 成本的下降意味着模型随着时间的推移在优化。我们可以通过以下方式检查模型在新样本上的表现：
- en: '[PRE17]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To visualize this, execute the following code using `0.5` as the classification
    decision threshold:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化这一点，使用`0.5`作为分类决策阈值执行以下代码：
- en: '[PRE18]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Blue-filled crosses are testing samples predicted from class 0, while black-filled
    dots are those predicted from class 1:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝色填充的交叉表示从类0预测的测试样本，而黑色填充的圆点表示从类1预测的测试样本：
- en: '[PRE19]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Refer to the following screenshot for the result:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下截图查看结果：
- en: '![](img/B21047_04_06.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_06.png)'
- en: 'Figure 4.6: Training and testing sets of the toy example'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：玩具示例的训练集和测试集
- en: The model we trained correctly predicts classes of new samples (filled crosses
    and filled dots).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练的模型能正确预测新样本的类别（填充的交叉和填充的圆点）。
- en: Predicting ad click-through with logistic regression using gradient descent
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用梯度下降进行逻辑回归预测广告点击率
- en: We will now deploy the algorithm we just developed in our click-through prediction
    project.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将在我们的点击预测项目中部署我们刚刚开发的算法。
- en: 'We will start with only 10,000 training samples (you will soon see why we don’t
    start with 270,000, as we did in the previous chapter):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从仅使用10,000个训练样本开始（你很快就会明白为什么我们不从270,000开始，就像在上一章中那样）：
- en: '[PRE20]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We train a logistic regression model over `10000` iterations, at a learning
    rate of `0.01` with bias:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`10000`次迭代中训练逻辑回归模型，学习率为`0.01`，并带有偏差：
- en: '[PRE21]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'It takes 184 seconds to optimize the model. The trained model performs on the
    testing set as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 优化模型花费了184秒。训练后的模型在测试集上的表现如下：
- en: '[PRE22]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now, let’s use 100,000 training samples (`n_train = 100000`) and repeat the
    same process. It will take more than an hour – 22 times longer to fit data of
    10 times the size. As I mentioned at the beginning of the chapter, the logistic
    regression classifier can be good at training on large datasets. But our testing
    results seem to contradict this. How could we handle even larger training datasets
    efficiently, not just 100,000 samples, but millions? Let’s look at a more efficient
    way to train a logistic regression model in the next section.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用100,000个训练样本（`n_train = 100000`）并重复相同的过程。它将需要超过一个小时——这比拟合10倍大小数据集的时间多22倍。正如我在本章开始时提到的，逻辑回归分类器在处理大型数据集时表现良好。但我们的测试结果似乎与此相矛盾。我们如何有效地处理更大的训练数据集，不仅是100,000个样本，而是数百万个样本？让我们在下一节中看看更高效的训练逻辑回归模型的方法。
- en: Training a logistic regression model using stochastic gradient descent (SGD)
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用随机梯度下降（SGD）训练逻辑回归模型
- en: In gradient descent-based logistic regression models, **all** training samples
    are used to update the weights in every single iteration. Hence, if the number
    of training samples is large, the whole training process will become very time
    consuming and computationally expensive, as you just witnessed in our last example.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于梯度下降的逻辑回归模型中，**所有**训练样本都用于每次迭代中更新权重。因此，如果训练样本数量很大，整个训练过程将变得非常耗时和计算昂贵，正如您在我们的最后一个例子中所见到的。
- en: 'Fortunately, a small tweak will make logistic regression suitable for large-sized
    datasets. For each weight update, **only one** training sample is consumed, instead
    of the **complete** training set. The model moves a step based on the error calculated
    by a single training sample. Once all samples are used, one iteration finishes.
    This advanced version of gradient descent is called **SGD**. Expressed in a formula,
    for each iteration, we do the following:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，一个小小的调整就可以使逻辑回归适用于大型数据集。每次权重更新，只消耗**一个**训练样本，而不是整个训练集。模型根据单个训练样本计算的误差前进一步。一旦所有样本都被使用，一次迭代就完成了。这种进阶版本的梯度下降被称为**SGD**。用公式表达，对于每次迭代，我们执行以下操作：
- en: '![](img/B21047_04_034.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_034.png)'
- en: '![](img/B21047_04_035.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_035.png)'
- en: SGD generally converges much faster than gradient descent where a large number
    of iterations is usually needed.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: SGD通常比梯度下降收敛速度快得多，后者通常需要大量迭代次数。
- en: 'To implement SGD-based logistic regression, we just need to slightly modify
    the `update_weights_gd` function:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现基于SGD的逻辑回归，我们只需要稍微修改`update_weights_gd`函数：
- en: '[PRE23]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In the `train_logistic_regression` function, SGD is applied:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在`train_logistic_regression`函数中，应用了SGD：
- en: '[PRE24]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, let’s see how powerful SGD is. We will work with 100,000 training samples
    and choose `10` as the number of iterations, `0.01` as the learning rate, and
    print out the current costs for every other iteration:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看SGD有多强大。我们将使用10万个训练样本，选择`10`作为迭代次数，`0.01`作为学习率，并打印出每两次迭代的当前成本：
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The training process finishes in just 25 seconds!
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程仅用时25秒完成！
- en: 'After successfully implementing the SGD-based logistic regression algorithm
    from scratch, we implement it using the `SGDClassifier` module of scikit-learn:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 成功从头开始实现基于SGD的逻辑回归算法后，我们使用scikit-learn的`SGDClassifier`模块来实现它：
- en: '[PRE26]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Here, `'``log_loss'` for the `loss` parameter indicates that the cost function
    is log loss, `penalty` is the regularization term to reduce overfitting, which
    we will discuss further in the next section, `max_iter` is the number of iterations,
    and the remaining two parameters mean the learning rate is `0.01` and unchanged
    during the course of training. It should be noted that the default `learning_rate`
    is `'optimal'`, where the learning rate slightly decreases as more and more updates
    are made. This can be beneficial for finding the optimal solution on large datasets.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`'``log_loss'`作为`loss`参数表明成本函数是对数损失，`penalty`是减少过拟合的正则化项，我们将在下一节中进一步讨论，`max_iter`是迭代次数，另外两个参数意味着学习率是`0.01`，在训练过程中保持不变。应注意，默认的`learning_rate`是`'optimal'`，随着更新的进行，学习率会稍微降低。这对于在大型数据集上找到最优解是有益的。
- en: 'Now, train the model and test it:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，训练模型并测试它：
- en: '[PRE27]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Quick and easy!
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 快速简单！
- en: Training a logistic regression model with regularization
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练带有正则化的逻辑回归模型
- en: 'As I briefly mentioned in the previous section, the `penalty` parameter in
    the logistic regression `SGDClassifier` is related to model **regularization**.
    There are two basic forms of regularization, **L1** (also called **Lasso**) and
    **L2** (also called **Ridge**). In either way, the regularization is an additional
    term on top of the original cost function:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在前一节简要提到的，逻辑回归的`SGDClassifier`中的`penalty`参数与模型的**正则化**有关。正则化有两种基本形式，**L1**（也称为**Lasso**）和**L2**（也称为**Ridge**）。无论哪种方式，正则化都是原始成本函数之外的一个额外项：
- en: '![](img/B21047_04_036.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_036.png)'
- en: 'Here, ![](img/B21047_04_037.png) is the constant that multiplies the regularization
    term, and *q* is either *1* or *2* representing L1 or L2 regularization where
    the following applies:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B21047_04_037.png)是乘以正则化项的常数，*q*可以是*1*或*2*，代表L1或L2正则化，其中以下适用：
- en: '![](img/B21047_04_038.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_038.png)'
- en: Training a logistic regression model is the process of reducing the cost as
    a function of weights *w*. If it gets to a point where some weights, such as *w*[i],
    *w*[j], and *w*[k] are considerably large, the whole cost will be determined by
    these large weights. In this case, the learned model may just memorize the training
    set and fail to generalize to unseen data. The regularization term is introduced
    in order to penalize large weights, as the weights now become part of the cost
    to minimize.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个逻辑回归模型是减少以权重 *w* 为函数的成本的过程。如果到达某个点时，某些权重（例如 *w*[i]、*w*[j] 和 *w*[k]）非常大，整个成本将由这些大权重决定。在这种情况下，学习到的模型可能只是记住了训练集，无法很好地推广到未见过的数据。正则化项的引入是为了惩罚大权重，因为权重现在成为了最小化成本的一部分。
- en: Regularization as a result eliminates overfitting. Finally, parameter α provides
    a trade-off between log loss and generalization. If *α* is too small, it is not
    able to compress large weights and the model may suffer from high variance or
    overfitting; on the other hand, if *α* is too large, the model may become over-generalized
    and perform poorly in terms of fitting the dataset, which is the syndrome of underfitting.
    *α* is an important parameter to tune in order to obtain the best logistic regression
    model with regularization.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化通过消除过拟合来起到作用。最后，参数 α 提供了对数损失和泛化之间的权衡。如果 *α* 太小，它不能压缩大的权重，模型可能会遭遇高方差或过拟合；另一方面，如果
    *α* 太大，模型可能会过度泛化，无法很好地拟合数据集，表现出欠拟合的症状。*α* 是调优的一个重要参数，用于获得最佳的带正则化的逻辑回归模型。
- en: As for choosing between the L1 and L2 forms, the rule of thumb is based on whether
    **feature selection** is expected. In **Machine** **Learning** (**ML**) classification,
    feature selection is the process of picking a subset of significant features for
    use in better model construction. In practice, not every feature in a dataset
    carries information that is useful for discriminating samples; some features are
    either redundant or irrelevant and hence can be discarded with little loss.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择 L1 和 L2 形式时，通常的经验法则是看是否预期进行 **特征选择**。在 **机器学习** (**ML**) 分类中，特征选择是选择一组重要特征以用于构建更好的模型的过程。在实践中，数据集中的并非每个特征都包含有助于区分样本的信息；有些特征是冗余的或无关的，因此可以在损失较小的情况下丢弃。
- en: 'In a logistic regression classifier, feature selection can only be achieved
    with L1 regularization. To understand this, let’s consider two weight vectors,
    *w*[1]= (*1, 0*) and *w*[2]= (*0.5, 0.5*); supposing they produce the same amount
    of log loss, the L1 and L2 regularization terms of each weight vector are as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归分类器中，特征选择只能通过 L1 正则化来实现。为了理解这一点，我们假设有两个权重向量，*w*[1]= (*1, 0*) 和 *w*[2]=
    (*0.5, 0.5*)；假设它们产生相同的对数损失，两个权重向量的 L1 和 L2 正则化项如下：
- en: '![](img/B21047_04_039.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_039.png)'
- en: '![](img/B21047_04_040.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_040.png)'
- en: The L1 term of both vectors is equivalent, while the L2 term of *w*[2] is less
    than that of *w*[1]. This indicates that L2 regularization penalizes weights composed
    of significantly large and small weights more than L1 regularization does. In
    other words, L2 regularization favors relatively small values for all weights,
    and avoids significantly large and small values for any weight, while L1 regularization
    allows some weights with a significantly small value and some with a significantly
    large value. Only with L1 regularization can some weights be compressed to close
    to or exactly *0*, which enables feature selection.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 两个向量的 L1 项是相等的，而 *w*[2] 的 L2 项小于 *w*[1] 的 L2 项。这表明，L2 正则化相比 L1 正则化对由极大和极小权重组成的权重施加了更多的惩罚。换句话说，L2
    正则化偏向于所有权重的相对较小值，避免任何权重出现极大或极小的值，而 L1 正则化允许某些权重具有显著较小的值，某些则具有显著较大的值。只有使用 L1 正则化，某些权重才能被压缩到接近或完全为
    *0*，这使得特征选择成为可能。
- en: In scikit-learn, the regularization type can be specified by the `penalty` parameter
    with the `none` (without regularization), `"l1"`, `"l2"`, and `"elasticnet"` (a
    mixture of L1 and L2) options, and the multiplier α can be specified by the alpha
    parameter.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，正则化类型可以通过 `penalty` 参数指定，选项包括 `none`（无正则化）、`"l1"`、`"l2"` 和
    `"elasticnet"`（L1 和 L2 的混合），而乘数 α 可以通过 alpha 参数指定。
- en: Feature selection using L1 regularization
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 L1 正则化进行特征选择
- en: We herein examine L1 regularization for feature selection.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在此讨论通过 L1 正则化进行特征选择。
- en: 'Initialize an SGD logistic regression model with L1 regularization, and train
    the model based on 10,000 samples:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化一个带有 L1 正则化的 SGD 逻辑回归模型，并基于 10,000 个样本训练模型：
- en: '[PRE28]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'With the trained model, we obtain the absolute values of its coefficients:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练好的模型，我们可以获得其系数的绝对值：
- en: '[PRE29]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The bottom `10` coefficients and their values are printed as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 底部的 `10` 个系数及其值如下所示：
- en: '[PRE30]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can see what these 10 features are using the following code:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码查看这 10 个特征：
- en: '[PRE31]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: They are `1001` from the `0` column (that is the `C1` column) in `X_train`,
    `84c2f017` from the `8` column (that is the `device_model` column), and so on
    and so forth.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 它们分别是来自 `X_train` 中第 `0` 列（即 `C1` 列）的 `1001`，来自第 `8` 列（即 `device_model` 列）的
    `84c2f017`，依此类推。
- en: 'Similarly, the top 10 coefficients and their values can be obtained as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，前 10 个系数及其值可以通过以下方式获得：
- en: '[PRE32]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: They are `28905ebd` from the `4` column (that is `site_category`) in `X_train`,
    `7687a86e` from the `3` column (that is `site_domain`), and so on and so forth.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 它们分别是来自 `X_train` 中第 `4` 列（即 `site_category`）的 `28905ebd`，来自第 `3` 列（即 `site_domain`）的
    `7687a86e`，依此类推。
- en: You have seen how feature selection works with L1-regularized logistic regression
    in this section, where weights of unimportant features are compressed to close
    to, or exactly, 0\. Besides L1-regularized logistic regression, random forest
    is another frequently used feature selection technique. Let’s see more in the
    next section.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你已经了解了如何使用 L1 正则化的逻辑回归进行特征选择，在该方法中，不重要特征的权重被压缩到接近 0 或者完全为 0。除了 L1 正则化的逻辑回归，随机森林是另一种常用的特征选择技术。我们将在下一节进一步探讨。
- en: Feature selection using random forest
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用随机森林进行特征选择
- en: 'To recap, random forest is bagging over a set of individual decision trees.
    Each tree considers a random subset of the features when searching for the best
    splitting point at each node. In a decision tree, only those significant features
    (along with their splitting values) are used to constitute tree nodes. Consider
    the forest as a whole: the more frequently a feature is used in a tree node, the
    more important it is.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，随机森林是对一组独立决策树的袋装方法。每棵树在每个节点寻找最佳分割点时都会考虑特征的随机子集。在决策树中，只有那些重要的特征（以及它们的分割值）被用来构建树节点。考虑到整个森林：一个特征在树节点中使用得越频繁，它的重要性就越大。
- en: In other words, we can rank the importance of features based on their occurrences
    in nodes among all trees, and select the top most important ones.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以根据特征在所有树的节点中出现的频率对特征重要性进行排名，并选择最重要的特征。
- en: 'The trained `RandomForestClassifier` module in scikit-learn comes with an attribute,
    `feature_importances_`, indicating the feature importance, which is calculated
    as the proportion of occurrences in tree nodes. Again, we will examine feature
    selection with random forest on the dataset with 100,000 ad click samples:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中训练好的 `RandomForestClassifier` 模块包含一个名为 `feature_importances_`
    的属性，表示特征重要性，计算方式为特征在树节点中出现的比例。同样，我们将在一个包含 100,000 个广告点击样本的数据集上使用随机森林进行特征选择：
- en: '[PRE33]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'After fitting the random forest model, we obtain the feature importance scores
    with the following:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合随机森林模型后，我们可以通过以下方式获取特征重要性分数：
- en: '[PRE34]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Take a look at the bottom 10 feature scores and the corresponding 10 least
    important features:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下底部 10 个特征分数以及相应的 10 个最不重要特征：
- en: '[PRE35]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, take a look at the top 10 feature scores and the corresponding 10 most
    important features:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，看看前 10 个特征分数以及相应的 10 个最重要特征：
- en: '[PRE36]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In this section, we covered how random forest is used for feature selection.
    We ranked the features of the ad click data using a random forest. Can you use
    the top 10 or 20 features to build another logistic regression model for ad click
    prediction?
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们讲解了如何使用随机森林进行特征选择。我们使用随机森林对广告点击数据进行了特征排名。你能否使用前 10 或 20 个特征来构建另一个用于广告点击预测的逻辑回归模型？
- en: Training on large datasets with online learning
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在大数据集上进行在线学习训练
- en: So far, we have trained our model on no more than 300,000 samples. If we go
    beyond this figure, memory might be overloaded since it holds too much data, and
    the program will crash. In this section, we will explore how to train on a large-scale
    dataset with **online learning**.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的模型已在不超过 300,000 个样本上进行了训练。如果超过这个数字，内存可能会被超载，因为它需要存储过多数据，程序也可能会崩溃。在本节中，我们将探讨如何使用
    **在线学习** 在大规模数据集上进行训练。
- en: SGD evolves from gradient descent by sequentially updating the model with individual
    training samples one at a time, instead of the complete training set at once.
    We can scale up SGD further with online learning techniques. In online learning,
    new data for training is available in sequential order or in real time, as opposed
    to all at once in an offline learning environment. A relatively small chunk of
    data is loaded and preprocessed for training at a time, which releases the memory
    used to hold the entire large dataset. Besides better computational feasibility,
    online learning is also used because of its adaptability to cases where new data
    is generated in real time and is needed for modernizing the model. For instance,
    stock price prediction models are updated in an online learning manner with timely
    market data; click-through prediction models need to include the most recent data
    reflecting users’ latest behaviors and tastes; spam email detectors have to be
    reactive to ever-changing spammers by considering new features that are dynamically
    generated.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: SGD 从梯度下降演变而来，通过依次使用单个训练样本逐步更新模型，而不是一次性使用完整的训练集。我们可以通过在线学习技术进一步扩展 SGD。在在线学习中，训练所需的新数据是按顺序或实时提供的，而不像离线学习环境中那样一次性提供。每次加载并预处理一小块数据进行训练，从而释放用于存储整个大数据集的内存。除了更好的计算可行性外，在线学习还因其对实时生成新数据并需要更新模型的适应性而被使用。例如，股票价格预测模型通过及时的市场数据以在线学习方式进行更新；点击率预测模型需要包括反映用户最新行为和兴趣的最新数据；垃圾邮件检测器必须对不断变化的垃圾邮件发送者做出反应，考虑动态生成的新特征。
- en: 'The existing model trained by previous datasets can now be updated based on
    the most recently available dataset only, instead of rebuilding it from scratch
    based on previous and recent datasets together, as is the case in offline learning:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 以前通过先前的数据集训练的现有模型，现在可以仅基于最新可用的数据集进行更新，而不是像在离线学习中那样，基于之前和当前的数据集一起从头开始重建模型：
- en: '![A diagram of a logistic process  Description automatically generated with
    low confidence](img/B21047_04_07.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![一个逻辑过程的示意图 说明自动生成，信心较低](img/B21047_04_07.png)'
- en: 'Figure 4.7: Online versus offline learning'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7：在线学习与离线学习
- en: In the preceding example, online learning allows the model to continue training
    with new arriving data. However, in offline learning, we have to retrain the whole
    model with the new arriving data along with the old data.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，在线学习允许模型继续使用新到达的数据进行训练。然而，在离线学习中，我们必须使用新到达的数据和旧数据一起重新训练整个模型。
- en: 'The `SGDClassifier` module in scikit-learn implements online learning with
    the `partial_fit` method (while the `fit` method is applied in offline learning,
    as you have seen). We will train the model with 1,000,000 samples, where we feed
    in 100,000 samples at one time to simulate an online learning environment. Also,
    we will test the trained model on another 100,000 samples as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 中的 `SGDClassifier` 模块通过 `partial_fit` 方法实现在线学习（而 `fit` 方法用于离线学习，正如你所见）。我们将使用
    1,000,000 个样本来训练模型，其中每次输入 100,000 个样本，以模拟在线学习环境。此外，我们还将用另外 100,000 个样本对训练好的模型进行测试，如下所示：
- en: '[PRE37]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Fit the encoder on the whole training set as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，将编码器应用于整个训练集：
- en: '[PRE38]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Initialize an SGD logistic regression model where we set the number of iterations
    to `1` in order to partially fit the model and enable online learning:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化一个 SGD 逻辑回归模型，并将迭代次数设置为 `1`，以便部分拟合模型并启用在线学习：
- en: '[PRE39]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Loop over every `100000` samples and partially fit the model:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 对每 `100000` 个样本进行循环，并部分拟合模型：
- en: '[PRE40]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Again, we use the `partial_fit` method for online learning. Also, we specify
    the `classes` parameter, which is required in online learning:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们使用 `partial_fit` 方法进行在线学习。此外，我们还指定了 `classes` 参数，这是在线学习中必需的：
- en: '[PRE41]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Apply the trained model on the testing set, the next 100,000 samples, as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练好的模型应用于测试集，即接下来的 100,000 个样本，如下所示：
- en: '[PRE42]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: With online learning, training based on a total of 1 million samples only takes
    87 seconds and yields better accuracy.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在在线学习中，仅用 100 万个样本进行训练只需 87 秒，并且能得到更好的准确度。
- en: We have been using logistic regression for binary classification so far. Can
    we use it for multiclass cases? Yes. However, we do need to make some small tweaks.
    Let’s look at this in the next section.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在使用逻辑回归进行二分类。我们可以用它处理多分类问题吗？可以。不过，我们确实需要做一些小的调整。让我们在下一节中看看。
- en: Handling multiclass classification
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理多类别分类
- en: One last thing worth noting is how logistic regression algorithms deal with
    multiclass classification. Although we interact with the scikit-learn classifiers
    in multiclass cases the same way as in binary cases, it is useful to understand
    how logistic regression works in multiclass classification.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得注意的事情是逻辑回归算法如何处理多类分类。尽管在多类情况下我们与 scikit-learn 分类器的交互方式与二分类时相同，但了解逻辑回归在多类分类中的工作原理是很有用的。
- en: 'Logistic regression for more than two classes is also called **multinomial
    logistic regression**, better known latterly as **softmax regression**. As you
    have seen in the binary case, the model is represented by one weight vector *w*,
    and the probability of the target being *1* or the positive class is written as
    follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 对于超过两个类别的逻辑回归也被称为**多项式逻辑回归**，后来更常被称为**softmax 回归**。正如你在二分类情况下看到的，模型由一个权重向量 *w*
    表示，目标属于 *1* 或正类的概率如下所示：
- en: '![](img/B21047_04_041.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_041.png)'
- en: 'In the *K* class case, the model is represented by *K* weight vectors, *w*[1],
    *w*[2], ..., *w*[K], and the probability of the target being class *k* is written
    as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *K* 类的情况下，模型由 *K* 个权重向量 *w*[1]、*w*[2]、...、*w*[K] 表示，目标属于类 *k* 的概率如下所示：
- en: '![](img/B21047_04_042.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_042.png)'
- en: 'See the following term:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下项：
- en: '![](img/B21047_04_043.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_043.png)'
- en: 'The preceding term normalizes the following probabilities (*k* from *1* to
    *K*) so that they total *1*:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 上述项规范化了以下概率（*k* 从 *1* 到 *K*），使其总和为 *1*：
- en: '![](img/B21047_04_044.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_044.png)'
- en: 'The cost function in the binary case is expressed as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类情况下的代价函数表示如下：
- en: '![](img/B21047_04_045.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_045.png)'
- en: 'Similarly, the cost function in the multiclass case becomes the following:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，多类情况下的代价函数变为如下：
- en: '![](img/B21047_04_046.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_046.png)'
- en: Here, the ![](img/B21047_04_047.png) function is *1* only if ![](img/B21047_04_048.png)
    is true, otherwise it’s 0.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B21047_04_047.png) 函数只有在 ![](img/B21047_04_048.png) 为真时才为 *1*，否则为
    0。
- en: 'With the cost function defined, we obtain the ![](img/B21047_04_049.png) step
    for the *j* weight vector in the same way as we derived the step ![](img/B21047_04_050.png)
    in the binary case:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了代价函数之后，我们以与在二分类情况下推导步长 ![](img/B21047_04_050.png) 相同的方式，得到 *j* 权重向量的步长 ![](img/B21047_04_049.png)：
- en: '![](img/B21047_04_051.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_051.png)'
- en: 'In a similar manner, all *K* weight vectors are updated in each iteration.
    After sufficient iterations, the learned weight vectors, *w*[1], *w*[2], ...,
    *w*[K], are then used to classify a new sample *x*’ by means of the following
    equation:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，所有 *K* 个权重向量在每次迭代中都会被更新。经过足够的迭代后，学习到的权重向量 *w*[1]、*w*[2]、...、*w*[K] 将用于通过以下方程对新的样本
    *x*’ 进行分类：
- en: '![](img/B21047_04_052.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_04_052.png)'
- en: 'To have a better sense, let’s experiment with it with a classic dataset, the
    handwritten digits for classification:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，我们来用一个经典数据集进行实验——手写数字分类：
- en: '[PRE43]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'As the image data is stored in 8*8 matrices, we need to flatten them, as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 由于图像数据存储在 8*8 矩阵中，我们需要将其展开，方法如下：
- en: '[PRE44]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We then split the data as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将数据分割如下：
- en: '[PRE45]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We then combine grid search and cross-validation to find the optimal multiclass
    logistic regression model, as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们结合网格搜索和交叉验证来找到最佳的多类逻辑回归模型，如下所示：
- en: '[PRE46]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We first define the hyperparameter grid we want to tune for the model. After
    initializing the classifier with some fixed parameters, we set up grid search
    cross-validation. We train on the training set and find the best set of hyperparameters.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义要为模型调优的超参数网格。在用一些固定参数初始化分类器后，我们设置网格搜索交叉验证。我们在训练集上训练并找到最佳的超参数组合。
- en: 'To predict using the optimal model, we apply the following:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用最佳模型进行预测，我们应用以下方法：
- en: '[PRE47]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: It doesn’t look much different from the previous example, since `SGDClassifier`
    handles multiclass internally. Feel free to compute the confusion matrix as an
    exercise. It will be interesting to see how the model performs on individual classes.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来与前一个例子差别不大，因为 `SGDClassifier` 在内部处理了多类情况。你可以自行计算混淆矩阵作为练习。观察模型在各个类上的表现会很有意思。
- en: The next section will be a bonus section where we will implement logistic regression
    with TensorFlow and use click prediction as an example.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分将是一个奖励部分，我们将使用 TensorFlow 实现逻辑回归，并以点击预测作为例子。
- en: Implementing logistic regression using TensorFlow
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 实现逻辑回归
- en: 'We’ll employ TensorFlow to implement logistic regression, utilizing click prediction
    as our illustrative example again. We use 90% of the first 100,000 samples for
    training and the remaining 10% for testing, and assume that `X_train_enc`, `Y_train`,
    `X_test_enc`, and `Y_test` contain the correct data:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用TensorFlow实现逻辑回归，再次以点击预测为示例。我们使用前100,000个样本中的90%进行训练，剩余的10%用于测试，并假设 `X_train_enc`、`Y_train`、`X_test_enc`
    和 `Y_test` 包含正确的数据：
- en: 'First, we import TensorFlow, transform `X_train_enc` and `X_test_enc` into
    a NumPy array, and cast `X_train_enc`, `Y_train`, `X_test_enc`, and `Y_test` to
    `float32`:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入TensorFlow，将 `X_train_enc` 和 `X_test_enc` 转换为NumPy数组，并将 `X_train_enc`、`Y_train`、`X_test_enc`
    和 `Y_test` 转换为 `float32`：
- en: '[PRE48]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In TensorFlow, it’s common to work with data in the form of NumPy arrays. Additionally,
    TensorFlow operates with float32 by default for computational efficiency.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，通常使用NumPy数组形式的数据进行操作。此外，TensorFlow默认使用float32进行计算，以提高计算效率。
- en: 'We use the `tf.data` module to shuffle and batch data:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 `tf.data` 模块对数据进行洗牌和分批：
- en: '[PRE49]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: For each weight update, only **one batch** of samples is consumed, instead of
    the one sample or the complete training set. The model moves a step based on the
    error calculated by a batch of samples. The batch size is 1,000 in this example.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次权重更新，仅消耗**一个批次**的样本，而不是单个样本或整个训练集。模型根据批次样本计算的误差进行一步更新。在本例中，批次大小为1,000。
- en: '`tf.data` provides a set of tools and utilities for efficiently loading and
    preprocessing data for ML modeling. It is designed to handle large datasets and
    enables efficient data pipeline construction for training and evaluation.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.data` 提供了一套高效加载和预处理机器学习数据的工具和实用程序。它旨在处理大规模数据集，并支持高效的数据管道构建，用于训练和评估。'
- en: 'Then, we define the weights and bias of the logistic regression model:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义逻辑回归模型的权重和偏差：
- en: '[PRE50]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We then create a gradient descent optimizer that searches for the best coefficients
    by minimizing the loss. We use Adam (Adam: *A method for stochastic optimization*,
    Kingma, D. P., & Ba, J. (2014)) as our optimizer, which is an advanced gradient
    descent with a learning rate (starting with `0.001`) that is adaptive to gradients:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个梯度下降优化器，通过最小化损失来寻找最佳系数。我们使用Adam（Adam：*一种随机优化方法*，Kingma，D. P.，和Ba，J.（2014））作为我们的优化器，它是一种改进的梯度下降方法，具有自适应学习率（起始学习率为`0.001`）：
- en: '[PRE51]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We define the optimization process where we compute the current prediction
    and cost and update the model coefficients following the computed gradients:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义优化过程，在该过程中计算当前的预测值和成本，并根据计算的梯度更新模型系数：
- en: '[PRE52]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Here, `tf.GradientTape` allows us to track TensorFlow computations and calculate
    gradients with respect to the given variables.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`tf.GradientTape` 允许我们跟踪TensorFlow计算，并计算相对于给定变量的梯度。
- en: 'We run the training for 5,000 steps (one step is with one batch of random samples):'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们运行训练5,000步（每步使用一个批次的随机样本）：
- en: '[PRE53]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: For every 500 steps, we compute and print out the current cost to check the
    training performance. As you can see, the training loss is decreasing overall.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 每500步，我们计算并打印当前的成本，以检查训练性能。如你所见，训练损失总体上在减少。
- en: 'After the model is trained, we use it to make predictions on the testing set
    and report the AUC metric:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练完成后，我们使用它对测试集进行预测，并报告AUC指标：
- en: '[PRE54]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We are able to achieve an AUC of `0.736` with the TensorFlow-based logistic
    regression model. You can also tweak the learning rate, the number of training
    steps, and other hyperparameters to obtain a better performance. This will be
    a fun exercise at the end of the chapter.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够通过基于TensorFlow的逻辑回归模型实现AUC值 `0.736`。你还可以调整学习率、训练步骤数和其他超参数，以获得更好的性能。这将在本章末尾作为一个有趣的练习。
- en: '**Best practice**'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'The choice of the batch size in SGD can significantly impact the training process
    and the performance of the model. Here are some best practices for choosing the
    batch size:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 批次大小的选择在SGD中可能会显著影响训练过程和模型的性能。以下是选择批次大小的一些最佳实践：
- en: '**Consider computational resources**: Larger batch sizes require more memory
    and computational resources, while smaller batch sizes may lead to slower convergence.
    Choose a batch size that fits within the memory constraints of your hardware while
    maximizing computational efficiency.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**考虑计算资源**：较大的批次大小需要更多的内存和计算资源，而较小的批次大小可能导致收敛速度较慢。选择一个适合硬件内存限制并最大化计算效率的批次大小。'
- en: '**Empirical testing**: Experiment with different batch sizes and evaluate model
    performance on a validation dataset. Choose the batch size that yields the best
    trade-off between convergence speed and model performance.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经验测试**：尝试不同的批量大小，并在验证数据集上评估模型性能。选择在收敛速度和模型性能之间取得最佳平衡的批量大小。'
- en: '**Batch size versus learning rate**: The choice of batch size can interact
    with the learning rate. Larger batch sizes may require higher learning rates to
    prevent slow convergence, while smaller batch sizes may benefit from smaller learning
    rates to avoid instability.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量大小与学习率**：批量大小的选择可能与学习率相互作用。较大的批量大小可能需要较高的学习率来防止收敛过慢，而较小的批量大小可能从较小的学习率中受益，以避免不稳定。'
- en: '**Consider the nature of the data**: The nature of the data can also influence
    the choice of batch size. For example, in tasks where the samples are highly correlated
    or exhibit temporal dependencies (e.g., time series data), smaller batch sizes
    may be more effective.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**考虑数据的性质**：数据的性质也会影响批量大小的选择。例如，在样本高度相关或存在时间依赖性的任务中（例如，时间序列数据），较小的批量大小可能更有效。'
- en: You might be curious about how we can efficiently train the model on the entire
    dataset of 40 million samples. You will utilize tools such as **Spark** ([https://spark.apache.org/](https://spark.apache.org/))
    and the `PySpark` module to scale up our solution.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会好奇我们是如何在包含4000万样本的整个数据集上高效地训练模型的。你将使用如**Spark** ([https://spark.apache.org/](https://spark.apache.org/))
    和 `PySpark` 模块等工具来扩展我们的解决方案。
- en: Summary
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小结
- en: In this chapter, we continued working on the online advertising click-through
    prediction project. This time, we overcame the categorical feature challenge by
    means of the one-hot encoding technique. We then resorted to a new classification
    algorithm, logistic regression, for its high scalability to large datasets. The
    in-depth discussion of the logistic regression algorithm started with the introduction
    of the logistic function, which led to the mechanics of the algorithm itself.
    This was followed by how to train a logistic regression model using gradient descent.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们继续进行在线广告点击率预测项目。这一次，我们通过使用独热编码技术克服了分类特征的挑战。随后，我们转向了新的分类算法——逻辑回归，因为它对大数据集具有较高的可扩展性。关于逻辑回归算法的深入讨论从介绍逻辑函数开始，进而引出了算法本身的机制。接着，我们讲解了如何使用梯度下降法训练逻辑回归模型。
- en: After implementing a logistic regression classifier by hand and testing it on
    our click-through dataset, you learned how to train the logistic regression model
    in a more advanced manner, using SGD, and we adjusted our algorithm accordingly.
    We also practiced how to use the SGD-based logistic regression classifier from
    scikit-learn and applied it to our project.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在手动实现逻辑回归分类器并在我们的点击率数据集上进行测试之后，你学习了如何使用更加先进的方式训练逻辑回归模型，采用了SGD，并相应地调整了我们的算法。我们还练习了如何使用基于SGD的scikit-learn逻辑回归分类器，并将其应用于我们的项目。
- en: We then continued to tackle the problems we might face in using logistic regression,
    including L1 and L2 regularization for eliminating overfitting, online learning
    techniques for training on large-scale datasets, and handling multi-class scenarios.
    You also learned how to implement logistic regression with TensorFlow. Finally,
    the chapter ended with applying the random forest model to feature selection,
    as an alternative to L1-regularized logistic regression.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们继续解决使用逻辑回归时可能遇到的问题，包括L1和L2正则化用于消除过拟合、大规模数据集的在线学习技术以及处理多分类场景。你还学习了如何使用TensorFlow实现逻辑回归。最后，本章以将随机森林模型应用于特征选择作为替代L1正则化逻辑回归的方法结束。
- en: Looking back on our learning journey, we have been working on classification
    problems since *Chapter 2*, *Building a Movie Recommendation Engine with Naïve
    Bayes*. We have now covered all the powerful and popular classification models
    in ML. We will move on to solving regression problems in the next chapter; regression
    is the sibling of classification in supervised learning. You will learn about
    regression models, including linear regression, and decision trees for regression.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们的学习历程，自*第2章*《使用朴素贝叶斯构建电影推荐引擎》开始，我们就一直在处理分类问题。现在我们已经覆盖了机器学习中所有强大且流行的分类模型。接下来，我们将进入回归问题的解决，回归是监督学习中与分类并列的任务。你将学习回归模型，包括线性回归和回归决策树。
- en: Exercises
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习题
- en: In the logistic regression-based click-through prediction project, can you also
    tweak hyperparameters such as `penalty`, `eta0`, and `alpha` in the `SGDClassifier`
    model? What is the highest testing AUC you are able to achieve?
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在基于逻辑回归的点击率预测项目中，你能否调整`penalty`、`eta0`和`alpha`等超参数，来优化`SGDClassifier`模型的表现？你能达到的最高测试AUC是多少？
- en: Can you try to use more training samples, for instance, 10 million samples,
    in the online learning solution?
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能否尝试在在线学习解决方案中使用更多的训练样本，比如1000万个样本？
- en: In the TensorFlow-based solution, can you tweak the learning rate, the number
    of training steps, and other hyperparameters to obtain better performance?
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在基于 TensorFlow 的解决方案中，你能否调整学习率、训练步数以及其他超参数，以获得更好的性能？
- en: Join our book’s Discord space
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者一起讨论：
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/yuxi](https://packt.link/yuxi)'
- en: '![](img/QR_Code1878468721786989681.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1878468721786989681.png)'
