- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Annotating Real Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标注真实数据
- en: The fuel of the **machine learning** (**ML**) engine is data. Data is available
    in almost every part of our technology-driven world. ML models usually need to
    be trained or evaluated on annotated data, not just data! Thus, data by itself
    is not very useful for ML but annotated data is what ML models need.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（**ML**）引擎的燃料是数据。数据几乎存在于我们技术驱动世界的每个部分。机器学习模型通常需要在标注数据上进行训练或评估，而不仅仅是数据！因此，数据本身对机器学习来说并不很有用，但标注数据是机器学习模型所需要的。
- en: In this chapter, we will learn why ML models need annotated data. We will see
    why the annotation process is expensive, error-prone, and biased. At the same
    time, you will be introduced to the annotation process for a number of ML tasks,
    such as **image classification**, **semantic segmentation**, and **instance segmentation**.
    We will highlight the main annotation problems. At the same time, we will understand
    why ideal ground truth generation is impossible or extremely difficult for tasks
    such as **optical flow estimation** and **depth estimation**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习为什么机器学习模型需要标注数据。我们将看到为什么标注过程成本高昂、易出错且存在偏见。同时，您将了解多个机器学习任务的标注过程，例如
    **图像分类**、**语义分割** 和 **实例分割**。我们将强调主要的标注问题。同时，我们将理解为什么对于 **光流估计** 和 **深度估计** 等任务，理想的真实情况生成是不可能的或极其困难的。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: The need to annotate real data for ML
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为机器学习标注真实数据的必要性
- en: Issues with the annotation process
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标注过程中的问题
- en: 'Optical flow and depth estimation: ground truth and annotation'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 光流和深度估计：真实情况和标注
- en: Annotating data for ML
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为机器学习标注数据
- en: In this section, you learn why ML models need annotated data and not simply
    data! Furthermore, you will be introduced to a diverse set of annotation tools.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将了解为什么机器学习模型需要标注数据，而不仅仅是数据！此外，您还将被介绍到一系列标注工具。
- en: Learning from data
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从数据中学习
- en: As humans, we learn differently from ML models. We just require *implicit* data
    annotation. However, ML models need *explicit* annotation of the data. For example,
    let’s say you want to train an ML model to classify cat and dog images; you cannot
    simply feed this model with many images of cats and dogs expecting the model to
    learn to differentiate between these two classes. Instead, you need to describe
    what each image is and then you can train your “cat-dog” classifier (see *Figure
    2**.1*).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，我们的学习方式与机器学习模型不同。我们只需要 *隐式* 数据标注。然而，机器学习模型需要 *显式* 数据标注。例如，假设您想训练一个机器学习模型来分类猫和狗的图像；您不能仅仅向这个模型提供许多猫和狗的图像，期望模型学会区分这两类。相反，您需要描述每张图像的内容，然后您才能训练您的“猫狗”分类器（见图
    *图 2**.1*）。
- en: '![Figure 2.1 – Training data for the cat-dog classifier](img/B18494_02_001.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – 猫狗分类器训练数据](img/B18494_02_001.jpg)'
- en: Figure 2.1 – Training data for the cat-dog classifier
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 猫狗分类器训练数据
- en: 'It should be noted that the amazing capabilities of ML models are closely related
    to and highly affected by the quality and quantity of the training data and ground
    truth. Generally, we need humans to annotate data for two main reasons: training
    and testing ML models. Next, we will be looking at these in more detail.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 应当注意的是，机器学习模型的惊人能力与训练数据和真实情况的质量和数量密切相关，并受到高度影响。通常，我们需要人类对数据进行标注主要有两个原因：训练和测试机器学习模型。接下来，我们将更详细地探讨这些内容。
- en: Training your ML model
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练您的机器学习模型
- en: We have four main steps for training an ML model. We will look at each of them
    next (see *Figure 2**.2*).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 训练机器学习模型有四个主要步骤。我们将在接下来逐一探讨它们（见图 *图 2**.2*）。
- en: '![Figure 2.2 – Training process of a typical ML model](img/B18494_02_002.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – 典型机器学习模型训练过程](img/B18494_02_002.jpg)'
- en: Figure 2.2 – Training process of a typical ML model
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 典型机器学习模型训练过程
- en: Initialization
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化
- en: At the beginning of the training process, the model’s parameters should be initialized.
    Usually, the parameters (weights and biases) of the **deep learning** (**DL**)
    model are set to random small numbers because this is what the **stochastic optimization
    process** expects at the beginning of the optimization process. The stochastic
    optimization process is a method of finding the best solution for a mathematical
    problem where randomness and uncertainty are involved to enhance the search procedure.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程的开始，模型的参数应该被初始化。通常，**深度学习**（**DL**）模型的参数（权重和偏差）被设置为随机的小数，因为这是**随机优化过程**在优化过程开始时所期望的。随机优化过程是一种寻找数学问题最佳解决方案的方法，其中涉及随机性和不确定性以增强搜索过程。
- en: Prediction
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测
- en: The model utilizes its past knowledge about the task and predicts the output
    given the input data. We can imagine that the model builds its own understanding
    of the problem by fitting a **hyperplane** (a decision boundary) to the training
    data in the training process, and then it projects any given input on this hyperplane
    to give us the model’s prediction for this specific input. Please remember that,
    at this step, we feed the model with *data only*, without any ground truth. If
    we go back to our “cat-dog” classifier, the model will be fed with cat and dog
    images and asked to predict the class or the label of these images.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型利用其对任务的过去知识，并根据输入数据预测输出。我们可以想象，在训练过程中，模型通过将一个**超平面**（决策边界）拟合到训练数据上来构建自己对问题的理解，然后它将任何给定的输入投影到这个超平面上，从而给出模型对这个特定输入的预测。请记住，在这一步，我们只向模型提供*数据*，而没有提供任何真实值。如果我们回到我们的“猫狗”分类器，模型将接受猫和狗的图像，并要求预测这些图像的类别或标签。
- en: Important note
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: '*Training*: The ML model develops its own comprehension of the problem by adjusting
    its parameters and evaluating its performance until it reaches a satisfactory
    understanding of the training data.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*训练*：ML模型通过调整其参数并评估其性能，直到对训练数据达到令人满意的了解。'
- en: '*Testing*: After the training, the ML model is evaluated on new data to assess
    its performance by using various metrics, such as F1 score, precision, and accuracy.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*测试*：在训练之后，ML模型在新数据上通过使用各种指标（如F1分数、精确度和准确度）来评估其性能。'
- en: Error calculation
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 错误计算
- en: 'Given what the model has predicted, we now need to assess the *correctness*
    of this prediction. This is exactly where we need the ground truth or the annotations.
    At this step, we have two inputs: the model’s prediction and the ground truth.
    Going back to our “cat-dog” classifier, the model may wrongly predict the class
    of a cat image as “dog.” Now, since we have the true class of the training cat
    image, we can tell the model that its understanding of the cat-dog problem was
    wrong this time (for this training sample). Furthermore, we can calculate how
    close the model was by using the loss function, which depends on the type of the
    problem. Please note that we essentially have two types of problems in ML: **classification**
    and **regression** problems. In classification problems, the ML model learns to
    categorize training data. For example, the “cat-dog” problem is a classification
    problem, and the error could be 0 or 1 since you have two categories: cat or dog.
    On the other hand, in regression problems, the ML model learns to leverage input
    data to predict a continuous value. For example, assume you are training a model
    to predict a house’s price based on some information about the house: location,
    number of rooms, age, and other information. Assume that the model predicted the
    house’s price to be £100,000 but the actual price (from the ground truth) is £105,000\.
    Then, the error in this case is £5,000.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 给定模型所做的预测，我们现在需要评估这个预测的*正确性*。这正是我们需要真实值或标注的地方。在这一步，我们有两个输入：模型的预测和真实值。回到我们的“猫狗”分类器，模型可能会错误地将猫图像的类别预测为“狗”。现在，由于我们有了训练猫图像的真实类别，我们可以告诉模型，它在这次（对于这个训练样本）对猫狗问题的理解是错误的。此外，我们可以通过损失函数来计算模型接近真实值的程度，损失函数取决于问题的类型。请注意，在机器学习中，我们本质上有两种类型的问题：**分类**问题和**回归**问题。在分类问题中，ML模型学习对训练数据进行分类。例如，“猫狗”问题是一个分类问题，错误可以是0或1，因为你有两个类别：猫或狗。另一方面，在回归问题中，ML模型学习利用输入数据来预测一个连续值。例如，假设你正在训练一个模型，根据房屋的一些信息（如位置、房间数量、年龄和其他信息）来预测房屋的价格。假设模型预测房屋的价格为10万英镑，但真实价格（来自真实值）为10.5万英镑。那么，在这种情况下，错误是5000英镑。
- en: The error is the essence of the learning process in ML; it provides guidance
    for training the model – for example, how much it needs to update its parameters
    and which parameters.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 误差是机器学习学习过程中的精髓；它为训练模型提供指导——例如，它需要更新多少参数以及哪些参数。
- en: Backpropagation
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播
- en: This is where the learning happens. Given the calculated error, we need to update
    the model’s parameters or weights based on the input and error. In other words,
    the model needs to “debug” the cause of this error in the prediction. If the error
    is small, the model will slightly update its weights or understanding of the problem.
    On the other hand, if the error is huge, the model will need to make major changes
    to the weights, thus, the understanding of the problem. Going back again to the
    “cat-dog” classifier, at the beginning of the training process, most predictions
    will be wrong, thus the model will be updating its weights drastically. In contrast,
    when the model is close to convergence (the best possible understanding of the
    training data), ideally, it starts to get most of its predictions right, thus
    making just slight updates on the weights.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是学习发生的地方。给定计算出的误差，我们需要根据输入和误差更新模型的参数或权重。换句话说，模型需要“调试”预测中误差的原因。如果误差很小，模型将稍微更新其权重或对问题的理解。另一方面，如果误差很大，模型将需要对权重进行重大调整，从而改变对问题的理解。回到“猫狗”分类器，在训练过程的开始阶段，大多数预测将是错误的，因此模型将大幅更新其权重。相反，当模型接近收敛（对训练数据的最佳理解）时，理想情况下，它开始正确预测大多数预测，从而只在权重上进行轻微更新。
- en: Testing your ML model
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试你的机器学习模型
- en: To assess the performance of your ML model, you need annotated data too. Thus,
    the annotations are not just needed for training but also for testing. Usually,
    qualitative results are good for having an overall understanding of how the model
    is performing in general or in some individual interesting scenarios. However,
    quantitative results are the most important way to understand the ML model’s robustness,
    accuracy, and precision.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估你的机器学习模型的表现，你也需要标注数据。因此，标注不仅对于训练是必要的，对于测试也是必要的。通常，定性结果有助于对模型整体表现或某些个别有趣场景有一个整体的理解。然而，定量结果是理解机器学习模型鲁棒性、准确性和精度的最重要方式。
- en: Using the ground truth, we can examine our trained model’s performance on a
    large number of examples. Thus, there is no need to look at predictions individually
    as the overall average, standard deviation, and other statistics will be a good
    description for that. In the next section, we will delve into common issues with
    the annotation of real data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用真实值，我们可以检查我们训练的模型在大量示例上的性能。因此，没有必要单独查看预测，因为整体平均数、标准差和其他统计数据将很好地描述这一点。在下一节中，我们将深入探讨真实数据标注中常见的常见问题。
- en: Issues with the annotation process
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标注过程中的问题
- en: As we have seen so far, annotations are critical to both training and testing.
    Thus, any mislabeling, biased annotations, or insufficient annotated data will
    drastically impact the learning and evaluation process of your ML model. As you
    can expect, the annotation process is time-consuming, expensive, and error-prone,
    and this is what we will see in this section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，标注对于训练和测试都是至关重要的。因此，任何标签错误、有偏的标注或标注数据不足都将极大地影响你的机器学习模型的训练和评估过程。正如你所预期的那样，标注过程是耗时、昂贵且易出错的，这就是我们将在本节中看到的内容。
- en: The annotation process is expensive
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标注过程是昂贵的
- en: 'To train state-of-the-art computer vision or **natural language processing**
    (**NLP**) models, you need large-scale training data. For example, *BERT* ([https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805))
    was trained on BooksCorpos (800 million words) and Wikipedia (2,500 million words).
    Similarly, *ViT* ([https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929))
    was trained on ImageNet (14 million images) and JFT (303 million images). Annotating
    such huge datasets is extremely difficult and challenging. Furthermore, it is
    time-consuming and expensive. It should be noted that the time required to annotate
    a dataset depends on three main elements: the task or problem, dataset size, and
    granularity level. Next, we will be looking at each of these in more detail.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练最先进的计算机视觉或**自然语言处理（NLP**）模型，你需要大规模的训练数据。例如，*BERT*（[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)）是在BooksCorpos（8亿单词）和维基百科（25亿单词）上训练的。同样，*ViT*（[https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)）是在ImageNet（1400万图像）和JFT（3亿图像）上训练的。标注如此庞大的数据集极其困难且具有挑战性。此外，它耗时且昂贵。需要注意的是，标注数据集所需的时间取决于三个主要因素：任务或问题、数据集大小和粒度级别。接下来，我们将更详细地探讨这些因素。
- en: Task
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 任务
- en: 'For example, annotating a dataset for a binary classification problem is easier
    and requires less time compared to annotating a dataset for semantic segmentation.
    Thus, the nature of the task also imposes clear difficulty on the annotation process.
    Even for the same task, let’s say semantic segmentation, annotating a single image
    under standard weather conditions and normal illumination takes approximately
    90 minutes for the *Cityscapes* dataset (Marius Cordts, et al. *The cityscapes
    dataset for semantic urban scene understanding*. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 3213–3223, 2016). However, doing
    similar annotation for images under adverse conditions such as snow, rain, and
    fog or at low illumination such as nighttime takes up to 3 hours for the *ACDC*
    dataset (Christos Sakaridis, et al. *ACDC: The adverse conditions dataset with
    correspondences for semantic driving scene understanding*. In Proceedings of the
    IEEE/CVF International Conference on Computer Vision, pages 10765–10775, 2021.).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于一个二元分类问题，标注数据集比标注语义分割数据集更容易，所需时间也更少。因此，任务的性质也对标注过程提出了明确的难度。即使是同一任务，比如语义分割，在标准天气条件和正常光照下标注单个图像，对于*Cityscapes*数据集（Marius
    Cordts等人*城市场景语义理解数据集*。在IEEE计算机视觉和模式识别会议论文集，第3213-3223页，2016年）大约需要90分钟。然而，在恶劣条件下（如雪、雨、雾）或低光照条件下（如夜间）进行类似的标注，对于*ACDC*数据集（Christos
    Sakaridis等人*ACDC：具有对应关系的语义驾驶场景理解恶劣条件数据集*。在IEEE/CVF国际计算机视觉会议论文集，第10765-10775页，2021年）则需要长达3小时。
- en: Dataset size
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集大小
- en: As expected, the larger the dataset, the harder it is to annotate. The complexity
    comes from managing such a huge dataset and ensuring the same annotation and data
    collection protocol is being followed by a large group of annotators. These annotators
    may have different languages, backgrounds, experiences, and skills. Indeed, guiding
    such a huge, diverse team, probably in different geographical locations, is not
    simple.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，数据集越大，标注就越困难。复杂性来自于管理如此庞大的数据集并确保大量标注员遵循相同的标注和数据收集协议。这些标注员可能拥有不同的语言、背景、经验和技能。实际上，指导这样一个庞大、多样化的团队，可能分布在不同的地理位置，并不简单。
- en: Granularity level
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 粒度级别
- en: The more detail you want your ground truth to capture, the more work for the
    annotators to perform. Let’s take **visual object tracking** as an example. Annotating
    images for single-object tracking is easier than multi-object tracking. We find
    the same thing for semantic segmentation, too. Annotating a semantic segmentation
    dataset with 3 classes is easier than 10 classes. Furthermore, the type of class
    also creates difficulty for the annotator. In other words, small objects may be
    harder to differentiate from the background and thus harder to annotate.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望你的真实标签捕捉的细节越多，标注员需要完成的工作就越多。以**视觉目标跟踪**为例。标注单目标跟踪的图像比多目标跟踪更容易。我们在语义分割中也发现了相同的情况。标注具有3个类别的语义分割数据集比10个类别的更容易。此外，类别的类型也会给标注员带来困难。换句话说，小物体可能更难与背景区分开来，因此标注起来也更困难。
- en: Next, we look at the main reasons behind noisy ground truth issues commonly
    seen in real datasets.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看一下在真实数据集中常见的噪声真实标签问题的主要原因。
- en: The annotation process is error-prone
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标注过程容易出错
- en: In this section, we shed light on the key reasons behind issues in manually
    annotated real data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们揭示了手动标注真实数据中问题背后的关键原因。
- en: Human factor
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人类因素
- en: The most important element in the annotation process is humans. However, we
    are limited by our perceptions of the world. Humans struggle to perceive with
    the naked eye the visual content in scenarios such as low illumination, cluttered
    scenes, or when objects are far from the camera, transparent, and so on. At the
    same time, miscommunication and misunderstanding of annotation protocol is another
    major issue. For example, assume you asked a team of annotators to annotate images
    for a visual object-tracking training dataset. You aim only to consider the *person
    object* for this task. Some annotators will annotate humans without objects while
    other annotators may consider other objects carried by humans as part of the object
    of interest (see *Figure 2**.3*). Furthermore, some annotators may consider only
    the unoccluded part of the human. This will cause a major inconsistency in the
    training data and the model will struggle to learn the task and will never converge.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 标注过程中最重要的元素是人类。然而，我们受限于对世界的感知。人类在低光照、杂乱场景或物体远离摄像头、透明等情况下的视觉内容难以用肉眼感知。同时，标注协议的误解和沟通不畅是另一个主要问题。例如，假设你要求一组标注者为视觉目标跟踪训练数据集标注图像。你的目标是只考虑*人物对象*。一些标注者会标注没有物体的人类，而其他标注者可能会将人类携带的其他物体视为感兴趣对象的一部分（参见*图2.3*）。此外，一些标注者可能只考虑人体未被遮挡的部分。这将导致训练数据中的主要不一致性，模型将难以学习任务，并且永远不会收敛。
- en: '![Figure 2.3 – Samples of annotation errors due to unclear annotation protocol](img/B18494_02_003.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图2.3 – 由于标注协议不明确导致的标注错误样本](img/B18494_02_003.jpg)'
- en: Figure 2.3 – Samples of annotation errors due to unclear annotation protocol
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – 由于标注协议不明确导致的标注错误样本
- en: Recording tools
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 记录工具
- en: If the recoding camera is shaky, the captured images will be blurred and thus
    the annotators will fail to accurately identify the actual pixels of the object
    from the background. Furthermore, the intrinsic and extrinsic parameters of the
    camera drastically change how the 3D scene will be projected into a 2D image.
    The focal length of the lens, shutter speed, lens distortion, and others all introduce
    certain errors in the annotation process. In other words, the objects annotated
    by annotators may not exactly correspond to the same object in the raw image or
    even in the 3D world.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果录制摄像头晃动，捕获的图像将变得模糊，因此标注者将无法准确识别对象从背景中的实际像素。此外，摄像头的内在和外在参数会极大地改变3D场景如何投影到2D图像中。镜头的焦距、快门速度、镜头畸变等都会在标注过程中引入一定的误差。换句话说，标注者标注的对象可能并不完全对应于原始图像或甚至3D世界中的同一对象。
- en: Scene attributes
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 场景属性
- en: Attributes such as weather conditions and time of the day all play an important
    role in the annotation process. As we have mentioned earlier, clear weather in
    the daytime may help the annotators to clearly identify objects as compared to
    adverse conditions at nighttime. In parallel to this, crowded and cluttered scenes
    are much more difficult to annotate and more subject to errors (see *Figure 2**.4*).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 气候条件和一天中的时间等属性在标注过程中都起着重要作用。正如我们之前提到的，白天的晴朗天气可能有助于标注者比夜间不利条件更清楚地识别对象。与此平行的是，拥挤和杂乱的场景标注更加困难，更容易出错（参见*图2.4*）。
- en: '![Figure 2.4 – Scene attribute: crowded scenes are more subject to annotation
    errors](img/B18494_02_004.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图2.4 – 场景属性：拥挤的场景更容易出现标注错误](img/B18494_02_004.jpg)'
- en: 'Figure 2.4 – Scene attribute: crowded scenes are more subject to annotation
    errors'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 场景属性：拥挤的场景更容易出现标注错误
- en: Annotation tools
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标注工具
- en: To enhance the annotation process, there are various annotation tools, such
    as Labelbox, Scale AI, Dataloop, HiveData, and LabelMe. Some of the annotation
    tools integrate AI components to optimize the annotation process by assisting
    the human annotator, such as Labelbox. While these AI-assisted methods are promising,
    they are not practical and reliable yet. In other words, the human annotator still
    needs to verify and correct the predictions. Additionally, some of these methods
    are slow and far from able to provide real-time assistance. In addition to this,
    if the problem is novel, the AI assistance will not work as expected because the
    AI model was not trained on similar scenarios.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高标注过程，有各种标注工具，例如Labelbox、Scale AI、Dataloop、HiveData和LabelMe。一些标注工具集成了AI组件，通过协助人工标注者优化标注过程，例如Labelbox。虽然这些AI辅助方法很有前景，但它们还不实用和可靠。换句话说，人工标注者仍然需要验证和纠正预测。此外，其中一些方法速度较慢，远远不能提供实时帮助。此外，如果问题是新颖的，AI辅助可能不会按预期工作，因为AI模型没有在类似的场景下进行过训练。
- en: Given the task, dataset size, and team specifications, a suitable annotation
    tool should be selected. The annotation tool should be the same for all annotators
    to ensure consistency among the annotators and the created ground truth.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 根据任务、数据集大小和团队规范，应选择合适的标注工具。标注工具应适用于所有标注者，以确保标注者之间以及创建的基线数据的一致性。
- en: The annotation process is biased
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标注过程存在偏见
- en: 'To understand the world and to reason about it efficiently, our brains build
    fast decisions and judgments based on our previous experience and systems of beliefs.
    For more details, see *Decision Making: Factors that Influence Decision Making,
    Heuristics Used, and Decision Outcomes* ([http://www.inquiriesjournal.com/a?id=180](http://www.inquiriesjournal.com/a?id=180)).
    ML models learn to reason and perceive the world using the training data. We try
    to collect and annotate the data objectively. However, unintentionally, we reflect
    our biases on the data we collect and annotate. Consequently, ML models also become
    biased and unfair. We will discuss the three common factors of annotation process
    bias next.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解世界并有效地进行推理，我们的大脑基于以往的经验和信念体系快速做出决策和判断。更多详情，请参阅*决策制定：影响决策制定的因素、使用的启发式方法和决策结果*([http://www.inquiriesjournal.com/a?id=180](http://www.inquiriesjournal.com/a?id=180))。机器学习模型通过训练数据学习推理和感知世界。我们试图客观地收集和标注数据。然而，无意中，我们在收集和标注的数据中反映了我们的偏见。因此，机器学习模型也变得有偏见和不公平。接下来，我们将讨论标注过程中常见的三个因素。
- en: Understanding the problem and task
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解问题和任务
- en: The annotator may not know the problem, may not understand the data, or understand
    why the data is collected and annotated. Thus, they may make wrong assumptions
    or misinterpret data. Furthermore, given the differences between the annotators,
    they may understand the problem differently.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 标注者可能不了解问题，可能不理解数据，或者不理解为什么收集和标注数据。因此，他们可能会做出错误的假设或误解数据。此外，鉴于标注者之间的差异，他们可能对问题的理解不同。
- en: Background, ideology, and culture
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 背景、意识形态和文化
- en: 'This is a critical factor behind inconsistency in the annotation process. Let’s
    imagine that you asked a group of 10 annotators to annotate a dataset for **action
    recognition**. You have only two actions: confirmation or negation. Your annotation
    team members are from the UK, Bulgaria, and India. The Bulgarian annotators will
    understand and annotate head shaking as “Yes” and nodding as “No.” The other annotators
    will do the opposite. Thus, you will have wrong training data and your model will
    not learn this task. There are also other scenarios where the bias is not clear
    and cannot be easily identified, and this is the hardest issue under this scope.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是标注过程中不一致性的一个关键因素。让我们想象一下，你要求一组10位标注者为**动作识别**标注数据集。你只有两个动作：确认或否定。你的标注团队成员来自英国、保加利亚和印度。保加利亚标注者会将摇头理解为“是”，点头理解为“否”。其他标注者则会相反。因此，你将得到错误的学习数据，你的模型将无法学习这个任务。还有其他场景，其中偏见并不明显，难以轻易识别，这是这个范围内最困难的问题。
- en: Subjectivity and emotions
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主观性和情绪
- en: For some problems such as **text sentiment analysis**, a well-known NLP technique
    used to understand textual data, a human annotator may be biased toward certain
    political parties, football teams, genders, and ethnicities. Thus, the annotations
    will be biased to the annotator’s point of view as well.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些问题，例如**文本情感分析**，这是一种用于理解文本数据的知名自然语言处理技术，人工标注者可能会对某些政党、足球队、性别和种族有偏见。因此，标注将偏向于标注者的观点。
- en: Common issues in the annotation process
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 标注过程中的常见问题
- en: 'Always avoid the following: using the wrong labeling tool, vague annotation
    protocol, miscommunication between annotators, adding new labels after starting
    the annotation process, and modifying the annotation protocol during the process.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 总是避免以下情况：使用错误的标注工具、模糊的标注协议、标注者之间的误解、在标注过程开始后添加新标签，以及在过程中修改标注协议。
- en: Optical flow and depth estimation
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 光流和深度估计
- en: In this section, we will look at different ML tasks and the followed procedures
    to generate their corresponding ground truth.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨不同的机器学习任务及其相应的地面真实值生成过程。
- en: Ground truth generation for computer vision
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算机视觉的地面真实值生成
- en: '**Computer vision** aims at enabling computers to see using digital images.
    It is not surprising to know that vision is one of the most complex functionalities
    performed by our brain. Thus, imitating vision is not simple, and it is rather
    complex for state-of-the-art computer vision models.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算机视觉**旨在使计算机能够通过数字图像进行视觉识别。知道视觉是我们大脑执行的最复杂功能之一并不奇怪。因此，模仿视觉并不简单，对于最先进的计算机视觉模型来说，这相当复杂。'
- en: Computer vision tasks include semantic segmentation, instance segmentation,
    optical flow estimation, depth estimation, normal map estimation, visual object
    tracking, and many more. Each task has its own unique way of generating the corresponding
    ground truth. Next, we will see samples of these tasks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉任务包括语义分割、实例分割、光流估计、深度估计、法线图估计、视觉目标跟踪等。每个任务都有其生成相应地面真实值的独特方法。接下来，我们将看到这些任务的样本。
- en: Image classification
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像分类
- en: The training images for this task usually contain one object, which is the object
    of interest. The annotation for this task is simply looking at each image and
    selecting one class or more describing the object in the image.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此任务的训练图像通常只包含一个对象，即感兴趣的对象。此任务的标注只是查看每张图像并选择一个或多个描述图像中对象的类别。
- en: Semantic and instance segmentation
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语义和实例分割
- en: For semantic and instance segmentation, the annotator needs to assign a class
    label for each pixel in the image. In other words, the annotator is asked to partition
    the image into different segments where each segment demonstrates one class for
    semantic segmentation and one instance for instance segmentation. Please refer
    to [https://github.com/mrgloom/awesome-semantic-segmentation](https://github.com/mrgloom/awesome-semantic-segmentation)
    for an exhaustive list of semantic segmentation methods, such as *U-Net*, *DeepLab*,
    and *D2Det*. For instance segmentation, please check [https://github.com/topics/instance-segmentation](https://github.com/topics/instance-segmentation).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语义分割和实例分割，标注者需要为图像中的每个像素分配一个类别标签。换句话说，标注者被要求将图像分割成不同的区域，其中每个区域展示语义分割中的一个类别和实例分割中的一个实例。请参阅[https://github.com/mrgloom/awesome-semantic-segmentation](https://github.com/mrgloom/awesome-semantic-segmentation)以获取语义分割方法的详尽列表，例如*U-Net*、*DeepLab*和*D2Det*。对于实例分割，请检查[https://github.com/topics/instance-segmentation](https://github.com/topics/instance-segmentation)。
- en: Object detection and tracking
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标检测和跟踪
- en: In object detection and tracking, the annotator draws a bounding box around
    each object in the image. Object tracking works using video to track an initially
    selected object throughout the video. On the other hand, **object detection**
    works on images to detect objects as required by the task. Please refer to [https://github.com/topics/object-detection](https://github.com/topics/object-detection)
    for a list of well-known and state-of-the-art object detection methods and useful
    Python libraries, such as *YOLOv5*, *Mask R-CNN*, and *OpenMMLab*. For object
    tracking, please refer to [https://github.com/topics/object-tracking](https://github.com/topics/object-tracking)
    for a list of models, such as *SiamMask*, *HQTrack*, and *CFNet*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标检测和跟踪中，注释员在图像中的每个目标周围绘制一个边界框。目标跟踪通过视频跟踪最初选择的目标在整个视频中的运动。另一方面，**目标检测**在图像上执行，以检测任务所需的对象。请参阅[https://github.com/topics/object-detection](https://github.com/topics/object-detection)获取知名和最先进的目标检测方法以及有用的Python库列表，例如*YOLOv5*、*Mask
    R-CNN*和*OpenMMLab*。对于目标跟踪，请参阅[https://github.com/topics/object-tracking](https://github.com/topics/object-tracking)获取模型列表，例如*SiamMask*、*HQTrack*和*CFNet*。
- en: Now, we will examine two specific tasks in computer vision that are extremely
    hard to generate ground truth for using a standard approach. This is basically
    just an example of the limitation of real data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将检查计算机视觉中两个极其难以使用标准方法生成地面真实数据的特定任务。这基本上只是真实数据限制的一个例子。
- en: Optical flow estimation
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 光流估计
- en: '**Optical flow** is the relative apparent motion of objects from one frame
    to another. The motion could be because of objects or camera motion. Optical flow
    has many key applications in tasks, such as **structure from motion**, **video
    compression**, and **video stabilization**. Structure from motion is widely used
    in 3D construction, and navigation and manipulation tasks in robotics, augmented
    reality, and games. Video compression is essential for video streaming, storage,
    and transmission; video stabilization, on the other hand, is crucial for timelapse
    videos, and videos recorded by drones or head-mounted cameras. Thus, optical flow
    has enormous applications in practice. For a comprehensive list of optical flow
    methods such as *SKFlow*, *GMFlow*, and *RAFT*, please refer to [https://github.com/hzwer/Awesome-Optical-Flow](https://github.com/hzwer/Awesome-Optical-Flow).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**光流**是物体从一个帧到另一个帧的相对明显运动。这种运动可能是由于物体或相机运动引起的。光流在许多任务中具有许多关键应用，例如**从运动中结构**、**视频压缩**和**视频稳定**。从运动中结构在3D构建、机器人的导航和操作任务、增强现实和游戏中的应用非常广泛。视频压缩对于视频流、存储和传输至关重要；另一方面，视频稳定对于延时摄影视频和无人机或头戴式相机记录的视频至关重要。因此，光流在实践中有巨大的应用。有关*SKFlow*、*GMFlow*和*RAFT*等光流方法的综合列表，请参阅[https://github.com/hzwer/Awesome-Optical-Flow](https://github.com/hzwer/Awesome-Optical-Flow)。'
- en: Please note that it is extremely hard to generate ground truth for optical flow.
    Some approaches apply complex procedures to achieve this under many assumptions,
    such as an indoor environment and a limited number of objects and motions.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，生成光流的地面真实数据极其困难。一些方法在许多假设下应用复杂程序来实现这一点，例如室内环境和有限数量的物体和运动。
- en: Depth estimation
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度估计
- en: '**Depth estimation** is the task of measuring the distance of each pixel in
    the scene to the camera. It is essential for 3D vision and has many applications,
    such as 3D scene reconstruction, autonomous cars and navigation, medical imaging,
    and augmented reality. Usually, there are two major approaches for depth estimation:
    one uses monocular images and the other is based on stereo images utilizing epipolar
    geometry. Similar to optical flow, generating ground truth for depth estimation
    is extremely hard in the real world. Please refer to [https://github.com/topics/depth-estimation](https://github.com/topics/depth-estimation)
    for a list of recent depth estimation methods, such as *AdaBins*, *SC-Depth*,
    and *Monodepth2*.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度估计**是指测量场景中每个像素到相机的距离的任务。这对于3D视觉至关重要，并且有许多应用，例如3D场景重建、自动驾驶汽车和导航、医学成像和增强现实。通常，深度估计有两种主要方法：一种使用单目图像，另一种基于基于极线几何的立体图像。与光流类似，在现实世界中生成深度估计的地面真实数据极其困难。请参阅[https://github.com/topics/depth-estimation](https://github.com/topics/depth-estimation)获取最近深度估计方法的列表，例如*AdaBins*、*SC-Depth*和*Monodepth2*。'
- en: Important note
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For optical flow and depth estimation, most of the standard datasets used are
    synthetic datasets. For optical flow, we can recognize synthetic datasets such
    as *FlyingChairs*, *FlyingThings3D*, and *Kubric* ([https://github.com/google-research/kubric](https://github.com/google-research/kubric)).
    For depth estimation, we can mention *Virtual KITTI*, *DENSE* ([https://github.com/uzh-rpg/rpg_e2depth](https://github.com/uzh-rpg/rpg_e2depth)),
    and *DrivingStereo* ([https://drivingstereo-dataset.github.io/](https://drivingstereo-dataset.github.io/)).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于光流和深度估计，大多数使用的标准数据集都是合成数据集。对于光流，我们可以识别出合成数据集，如 *FlyingChairs*、*FlyingThings3D*
    和 *Kubric* ([https://github.com/google-research/kubric](https://github.com/google-research/kubric))。对于深度估计，我们可以提到
    *Virtual KITTI*、*DENSE* ([https://github.com/uzh-rpg/rpg_e2depth](https://github.com/uzh-rpg/rpg_e2depth))
    和 *DrivingStereo* ([https://drivingstereo-dataset.github.io/](https://drivingstereo-dataset.github.io/))。
- en: Summary
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned why ML models need annotated real data. At the same
    time, we explored some of the common issues in the annotation process. Our exploration
    has led us to a deeper understanding of real data collection and annotation issues,
    such as being a time-consuming process and subject to annotator errors. Additionally,
    we covered the limitations of real data for tasks such as optical flow and depth
    estimation. In the next chapter, we will look specifically at the privacy issues
    with real data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了为什么机器学习模型需要标注的真实数据。同时，我们探讨了标注过程中的一些常见问题。我们的探索使我们更深入地理解了真实数据的收集和标注问题，例如这是一个耗时且容易受到标注者错误影响的过程。此外，我们还讨论了真实数据在光流和深度估计等任务中的局限性。在下一章中，我们将具体探讨真实数据带来的隐私问题。
- en: In the following chapters of the book, we will continue our exciting journey
    to understand the limitations of real data and the promising solutions of synthetic
    data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的后续章节中，我们将继续我们的激动人心的旅程，了解真实数据的局限性以及合成数据的潜在解决方案。
