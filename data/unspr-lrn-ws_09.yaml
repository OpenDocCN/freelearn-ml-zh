- en: 8\. Market Basket Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8\. 市场篮子分析
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we will explore market basket analysis, which is an algorithm
    originally designed to help retailers understand and improve their businesses.
    It is not, however, exclusive to retail, as we will discuss throughout the chapter.
    Market basket analysis unlocks the underlying relationships between the items
    that customers purchase. By the end of this chapter, you should have a solid grasp
    of transaction data, the basic metrics that define the relationship between two
    items, the Apriori algorithm, and association rules.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨市场篮子分析，这是一种最初设计用于帮助零售商了解和改善其业务的算法。然而，正如我们将在本章中讨论的，它并不仅限于零售业。市场篮子分析揭示了顾客购买的商品之间的潜在关系。本章结束时，你应该对交易数据、定义两件商品关系的基本指标、Apriori算法和关联规则有一个扎实的理解。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Most data science practitioners would agree that natural language processing,
    including topic modeling, is toward the cutting edge of data science and is an
    active research area. We now understand that topic models can, and should, be
    leveraged wherever text data could potentially drive insights or growth, including
    in social media analyses, recommendation engines, and news filtering. The preceding
    chapter featured an exploration of the fundamental features of topic models and
    two of the major algorithms. In this chapter, we are going to change direction
    entirely.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据科学从业者都认为，自然语言处理（包括主题建模）是数据科学的前沿领域，是一个活跃的研究方向。我们现在理解到，主题模型可以并且应该在任何潜在能够提供洞察或推动增长的文本数据中被应用，包括社交媒体分析、推荐引擎和新闻过滤等。上一章探讨了主题模型的基本特征和两种主要算法。在本章中，我们将完全改变方向。
- en: This chapter takes us into the retail space to explore a foundational and reliable
    algorithm for analyzing transaction data. While this algorithm might not be on
    the cutting edge or in the catalog of the most popular machine learning algorithms,
    it is ubiquitous and undeniably impactful in the retail space. The insights it
    drives are easily interpretable, immediately actionable, and instructive for determining
    analytical next steps. If you work in the retail space or with transaction data,
    you would be well-served to dive deep into market basket analysis. Market basket
    analysis is important because it provides insight into why people buy certain
    items together and whether those item combinations can be leveraged to hasten
    growth and or increase profitability.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章带我们进入零售领域，探索一种用于分析交易数据的基础且可靠的算法。虽然这种算法可能不是最前沿的，也不在最流行的机器学习算法目录中，但它在零售领域中无处不在，并且具有不可否认的影响力。它所提供的洞察易于解读、可以立即付诸行动，并且对确定分析的下一步至关重要。如果你在零售领域工作或涉及交易数据的分析，深入学习市场篮子分析将大有裨益。市场篮子分析的重要性在于，它提供了关于为什么人们会将某些商品一起购买的洞察，以及这些商品组合是否能被利用来加速增长或增加盈利。
- en: Market Basket Analysis
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 市场篮子分析
- en: 'Imagine you work for a retailer that sells dozens of products and your boss
    comes to you and asks the following questions:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你为一家零售商工作，卖着数十种产品，你的老板来找你，问你以下问题：
- en: What products are purchased together most frequently?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些产品最常一起购买？
- en: How should the products be organized and positioned in the store?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品应该如何在商店中组织和定位？
- en: How do we identify the best products to discount via coupons?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何识别最适合通过优惠券打折的产品？
- en: You might reasonably respond with complete bewilderment, as those questions
    are very diverse and do not immediately seem answerable using a single algorithm
    and dataset. However, the answer to all those questions and many more is **market
    basket analysis**. The general idea behind market basket analysis is to identify
    and quantify which items, or groups of items, are purchased together frequently
    enough to drive insight into customer behavior and product relationships.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会完全困惑地回应，因为这些问题非常多样，看起来似乎无法用单一的算法和数据集来回答。然而，所有这些问题以及更多问题的答案都是**市场篮子分析**。市场篮子分析的基本理念是识别和量化哪些商品或商品组合是经常一起购买的，从而深入了解顾客行为和产品关系。
- en: 'Before we dive into the analytics, it is worth defining the term *market basket*.
    A market basket is a permanent set of products in an economic system. Permanent
    does not necessarily mean permanent in the traditional sense. It means that until
    such time as the product is taken out of the catalog, it will consistently be
    available for purchase. The product referenced in the preceding definition is
    any good, service, or element of a group, including a bicycle, having your house
    painted, or a website. Lastly, an economic system could be a company, a collection
    of activities, or a country. The easiest example of a market basket is a grocery
    store, which is a system made up of a collection of food and drink items:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入分析之前，值得定义一下*市场篮子*这一术语。市场篮子是一个经济系统中的固定产品集合。固定并不一定意味着传统意义上的永久性。它意味着，直到产品从目录中移除之前，它将始终可以购买。上述定义中提到的产品可以是任何商品、服务或某一群体的元素，包括一辆自行车、给房子粉刷，或者一个网站。最后，经济系统可以是一个公司、一组活动或一个国家。市场篮子的最简单例子是一个超市，它是由一系列食品和饮品组成的系统：
- en: '![Figure 8.1: An example market basket'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.1: 一个示例市场篮子](img/B15923_08_02.jpg)'
- en: '](img/B15923_08_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_08_01.jpg)'
- en: 'Figure 8.1: An example market basket'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8.1: 一个示例市场篮子'
- en: Even without using any models or analyses, certain product relationships are
    obvious. Let's take the relationship between meat and vegetables. Typically, market
    basket analysis models return relationships more specific than meat and vegetables,
    but, for argument's sake, we will generalize to meat and vegetables. Okay, there
    is a relationship between meat and vegetables. So what? Well, we know these are
    staple items that are frequently purchased together. We can leverage this information
    by putting the vegetables and meats on opposite sides of the store, which you
    will notice is often the positioning of those two items, forcing customers to
    walk the full distance of the store, and thereby increasing the likelihood that
    they will buy additional items that they might not have bought had they not traversed
    the whole store.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 即使不使用任何模型或分析，某些产品之间的关系也是显而易见的。我们来看看肉类和蔬菜之间的关系。通常，市场篮子分析模型会返回比肉类和蔬菜更为具体的关系，但为了讨论的方便，我们将其泛化为肉类和蔬菜。好吧，肉类和蔬菜之间有关系。那又怎么样呢？嗯，我们知道这些是常见的主食，通常是一起购买的。我们可以利用这一信息，将蔬菜和肉类放置在商店的两侧，你会发现这两个物品通常就是这样被摆放的，迫使顾客走遍整个商店，这样会增加他们购买更多商品的可能性，哪怕他们原本并没有打算购买那些商品。
- en: 'One of the things retail companies struggle with is how to discount items effectively.
    Let''s consider another obvious relationship: peanut butter and jelly. In the
    United States, peanut butter and jelly sandwiches are incredibly popular, especially
    among children. When peanut butter is in a shopping basket, the chance jelly is
    also there can be assumed to be quite high. Since we know peanut butter and jelly
    are purchased together, it does not make sense to discount them both. If we want
    customers to buy both items, we can just discount one of the items, knowing that
    if we can get the customers to buy the discounted item, they will probably buy
    the other item too, even if it is full price. Just like the topic models in the
    preceding chapter, market basket analysis is all about identifying frequently
    occurring groups. The following figure presents an example of such groups:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 零售公司面临的一个挑战是如何有效地打折商品。我们再考虑一个显而易见的关系：花生酱和果冻。在美国，花生酱果冻三明治非常受欢迎，尤其是在孩子们中间。当购物篮里有花生酱时，可以假设果冻也会有很高的可能性。由于我们知道花生酱和果冻通常是一起购买的，因此同时对两者打折并没有意义。如果我们希望顾客购买这两样商品，我们只需对其中一项商品打折，因为如果我们能让顾客购买打折的商品，他们很可能也会购买另一项商品，尽管它是全价的。就像前一章中的主题模型一样，市场篮子分析的核心就是识别频繁出现的组合。下图展示了这样的组合的一个例子：
- en: '![Figure 8.2: A visualization of market basket analysis'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.2: 市场篮子分析的可视化](img/B15923_08_01.jpg)'
- en: '](img/B15923_08_02.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_08_02.jpg)'
- en: 'Figure 8.2: A visualization of market basket analysis'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8.2: 市场篮子分析的可视化'
- en: In market basket analysis, we are looking for frequently occurring groups of
    products, whereas in topic models, we were looking for frequently occurring groups
    of words. Thus, as it was to topic models, the word *clustering* could be applied
    to market basket analysis. The major differences are that the clusters in market
    basket analysis are micro – only a few products per cluster – and the order of
    the items in the cluster matters when it comes to computing probabilistic metrics.
    We will dive much deeper into these metrics and how they are calculated later
    in this chapter.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在市场篮子分析中，我们寻找的是频繁出现的产品组合，而在主题模型中，我们寻找的是频繁出现的词汇组合。因此，正如它适用于主题模型一样，*聚类*这个词也可以应用于市场篮子分析。主要的区别在于，市场篮子分析中的聚类是微型的——每个聚类中只有少数几种产品——并且当涉及到计算概率度量时，聚类中项目的顺序非常重要。我们将在本章稍后深入探讨这些度量及其计算方法。
- en: What has clearly been implied by the previous two examples is that, in market
    basket analysis, retailers can discover the relationships – obvious and surprising
    – between the products that customers buy. Once uncovered, the relationships can
    be used to inform and improve the decision-making process. A great aspect of market
    basket analysis is that while this analysis was developed in relation to, discussed
    in terms of, and mostly applied to, the retail world, it can be applied to many
    different types of businesses.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从前两个例子中可以明显看出，在市场篮子分析中，零售商可以发现顾客购买产品之间的关系——无论是显而易见的还是出乎意料的。一旦这些关系被揭示出来，就可以用来指导和改善决策过程。市场篮子分析的一个重要特点是，尽管这一分析方法最初是与零售领域相关开发、讨论和应用的，但它也可以应用于许多不同类型的商业。
- en: The only requirement for performing this type of analysis is that the data is
    a list of collections of items. In the retail case, this would be a list of transactions
    where each transaction is a group of purchased products. One example of an alternative
    application is analyzing website traffic. With website traffic, we consider the
    products to be websites, so each element of the list is the collection of websites
    visited by an individual over a specified time period. Needless to say, the applications
    of market basket analysis extend well beyond the initial retail application.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这种分析的唯一要求是数据必须是项集合的列表。在零售情况下，这将是一个交易列表，其中每个交易都是一组已购买的产品。另一个应用的例子是分析网站流量。在网站流量分析中，我们将产品视为网站，因此列表的每个元素就是一个人在指定时间段内访问的所有网站集合。不用说，市场篮子分析的应用远远超出了最初的零售应用。
- en: Use Cases
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用案例
- en: 'There are three principal use cases in the traditional retail application:
    pricing enhancement, coupon and discount recommendations, and store layout. As
    was briefly mentioned previously, by using the product associations uncovered
    by the model, retailers can strategically place products in their stores to get
    customers to buy more items and thus spend more money. If any relationship between
    two or more products is sufficiently strong, meaning the product grouping occurs
    often in the dataset and the individual products in the grouping appear separate
    from the group infrequently, then the products could be placed far away from one
    another in the store without significantly jeopardizing the odds of the customer
    purchasing both products. By forcing the customer to traverse the whole store
    to get both products, the retailer increases the chances that the customer will
    notice and purchase additional products. Likewise, retailers can increase the
    chances of customers purchasing two weakly related or non-staple products by placing
    the two items next to each other. Obviously, there are a lot of factors that drive
    store layout, but market basket analysis is definitely one of those factors:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 传统零售应用中的三个主要使用案例是：定价优化、优惠券和折扣推荐以及店铺布局。如前所述，通过使用模型揭示的产品关联，零售商可以有策略地将产品放置在商店中，促使顾客购买更多商品，从而花费更多的钱。如果两个或更多产品之间的关联足够强，意味着在数据集中该产品组合出现得很频繁，并且组合中的单独产品很少脱离该组合出现，那么这些产品就可以放置在商店的远离位置，而不会显著影响顾客购买两种产品的几率。通过迫使顾客横跨整个商店去购买这两种产品，零售商增加了顾客注意到并购买更多产品的机会。同样，零售商也可以通过将两种关联较弱或非日常产品放置在一起，增加顾客购买这两种产品的几率。显然，商店布局受到许多因素的影响，但市场篮子分析绝对是其中的一个因素。
- en: '![Figure 8.3: Product associations that can help inform efficient and lucrative
    store layouts'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.3：可以帮助制定高效且有利可图的店铺布局的产品关联](https://example.org)'
- en: '](img/B15923_08_03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_08_03.jpg)'
- en: 'Figure 8.3: Product associations that can help inform efficient and lucrative
    store layouts'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：帮助优化高效和有利可图商店布局的产品关联
- en: Pricing enhancement and coupon and discount recommendations are two sides of
    the same coin. They can simply be interpreted as where to raise and where to lower
    prices. Consider the case of two strongly related items. These two items are most
    likely going to be purchased in the same transaction, so one way to increase the
    profitability of that transaction would be to increase the price of one of the
    items. If the association between the two items is sufficiently strong, the price
    increase can be made with little to no risk of the customer not purchasing both
    items. In a similar way, retailers can encourage customers to purchase an item
    weakly associated with another through discounting or couponing.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 定价优化和优惠券及折扣推荐是同一枚硬币的两面。它们可以简单地解释为在哪里提高价格，在哪里降低价格。考虑到两个强关联的商品。这两件商品很可能会在同一交易中购买，所以增加交易的利润的一种方式就是提高其中一件商品的价格。如果这两件商品之间的关联足够强，那么价格的提升几乎没有客户不购买两者的风险。类似地，零售商可以通过折扣或优惠券鼓励客户购买一件与另一件商品弱相关的商品。
- en: For example, retailers could compare the purchase history of individual customers
    with the results of market basket analysis done on all transactions and find where
    some of the items certain customers are purchasing are weakly associated with
    items those customers are not currently purchasing. Using this comparison, retailers
    could offer discounts to the customers for the as-yet-unpurchased items the model
    suggested were related to the items previously purchased by those customers. If
    you have ever had coupons print out with your receipt at the end of a transaction,
    the chances are high that those items were found to be related to the items involved
    in your just-completed transaction.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，零售商可以将单个客户的购买历史与基于所有交易的市场篮子分析结果进行比较，找出某些客户购买的商品与他们当前未购买的商品之间的弱关联。通过这种对比，零售商可以为这些客户提供尚未购买商品的折扣，而这些商品是模型建议与他们之前购买的商品相关的。如果你曾在交易结束时收到与收据一同打印的优惠券，那么很可能这些商品就是通过这种方式发现与刚刚完成的交易中涉及的商品相关。
- en: A non-traditional, but viable, use of market basket analysis would be to enhance
    online advertising and search engine optimization. Imagine we had access to lists
    of websites visited by individuals. Using market basket analysis, we could find
    relationships between websites and use those relationships to both strategically
    order and group the websites resulting from a search engine query. In many ways,
    this is similar to the store layout use case.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 市场篮子分析的一个非传统但可行的应用是增强在线广告和搜索引擎优化。假设我们能够访问个人访问过的网站列表。利用市场篮子分析，我们可以找到网站之间的关系，并利用这些关系来战略性地排列和分组搜索引擎查询结果中的网站。在很多方面，这与商店布局的应用场景相似。
- en: With a general sense of what market basket analysis is all about and a clear
    understanding of its use cases, let's dig into the data used in these models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于市场篮子分析的基本概念和它的应用场景有了大致了解后，让我们深入研究这些模型所使用的数据。
- en: Important Probabilistic Metrics
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重要的概率度量
- en: Market basket analysis is built upon the computation of several probabilistic
    metrics. The five major metrics covered here are **support**, **confidence**,
    **lift**, **leverage**, and **conviction**. Before digging into transaction data
    and the specific market basket analysis models, including the **Apriori algorithm**
    and **association rules**, we should spend some time defining and exploring these
    metrics using a small, made-up set of transactions. We begin by making up some
    data to use.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 市场篮子分析建立在多个概率度量的计算基础上。这里涉及的五个主要度量是**支持度**、**置信度**、**提升度**、**杠杆度**和**定罪度**。在深入了解交易数据和具体的市场篮子分析模型（包括**Apriori算法**和**关联规则**）之前，我们应该花些时间使用一个小型的虚构交易集来定义和探索这些度量。我们首先构建一些数据来使用。
- en: 'Exercise 8.01: Creating Sample Transaction Data'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 8.01：创建示例交易数据
- en: 'Since this is the first exercise of the chapter, let''s set the environment.
    This chapter will use the same environment requirements that were used in *Chapter
    7*, *Topic Modeling*. If any of the packages do not load, as happened in the preceding
    chapter, use `pip` to install them via the command line. One of the libraries
    we will use is `mlxtend`, which may be unfamiliar to you. It is a machine learning
    extensions library that contains useful supplemental tools, including ensembling,
    stacking, and of course, market basket analysis models. This exercise does not
    have any real output. We will simply create a sample transaction dataset for use
    in subsequent exercises:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是本章的第一个练习，我们先设置环境。本章将使用与*第7章*、*主题建模*中相同的环境要求。如果任何包无法加载，就像前一章那样，请通过命令行使用`pip`进行安装。我们将使用的一个库是`mlxtend`，这对您来说可能比较陌生。它是一个机器学习扩展库，包含有用的补充工具，包括集成、堆叠，当然还有市场篮子分析模型。这个练习没有实际输出，我们只是创建一个示例交易数据集，以供后续练习使用：
- en: Open a Jupyter notebook with Python 3.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个带有Python 3的Jupyter笔记本。
- en: 'Install the following libraries: `matplotlib.pyplot`, which is used to plot
    the results of the models; `mlxtend.frequent_patterns`, which is used to run the
    models; `mlxtend.preprocessing`, which is used to encode and prep the data for
    the models; `numpy`, which is used to work with arrays; and `pandas`, which is
    used to work with DataFrames.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装以下库：`matplotlib.pyplot`，用于绘制模型的结果；`mlxtend.frequent_patterns`，用于运行模型；`mlxtend.preprocessing`，用于编码和准备数据以供模型使用；`numpy`，用于处理数组；`pandas`，用于处理数据框。
- en: '[PRE0]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create 10 fake transactions featuring grocery store items, and then print out
    the transactions. The data will take the form of a list of lists, a data structure
    that will be relevant later when discussing formatting transaction data for the
    models:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建10个包含杂货店商品的虚拟交易，然后打印出这些交易。数据将以列表的形式呈现，这种数据结构将在后续讨论格式化交易数据时与模型相关：
- en: '[PRE1]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output is as follows:'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.4: Printing the transactions'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.4：打印交易'
- en: '](img/B15923_08_04.jpg)'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_04.jpg)'
- en: 'Figure 8.4: Printing the transactions'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4：打印交易
- en: Now that we have created our dataset, we will explore several probabilistic
    metrics that quantify the relationship between pairs of items.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了数据集，接下来我们将探讨几种概率度量，量化项目对之间的关系。
- en: Note
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fhf9bS](https://packt.live/3fhf9bS).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/3fhf9bS](https://packt.live/3fhf9bS)。
- en: You can also run this example online at [https://packt.live/303vIBJ](https://packt.live/303vIBJ).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在[https://packt.live/303vIBJ](https://packt.live/303vIBJ)上在线运行此示例。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须执行整个笔记本，以便获得预期的结果。
- en: Support
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持度
- en: '**Support** is simply the probability that a given item set appears in the
    data, which can be calculated by counting the number of transactions in which
    the item set appears and dividing that count by the total number of transactions.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持度**只是给定项集在数据中出现的概率，可以通过计算项集出现的交易次数，并将该次数除以总交易次数来计算。'
- en: Note
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: An item set can be a single item or a group of items.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 项集可以是单个项或一组项。
- en: Support is an important metric, despite being very simple, as it is one of the
    primary metrics used to determine the believability and strength of association
    between groups of items. For example, it is possible to have two items that only
    occur with each other, suggesting that their association is very strong, but in
    a dataset containing 100 transactions, only appearing twice is not very impressive.
    Because the item set appears in only 2% of the transactions, and 2% is small in
    terms of the raw number of appearances, the association cannot be considered significant
    and, therefore, is probably unusable in decision making.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 支持度是一个重要的度量，尽管它非常简单，因为它是用来确定项集之间关联的可信度和强度的主要度量之一。例如，可能会有两个只在一起出现的项，表明它们之间的关联非常强，但在包含100个交易的数据集中，仅出现两次并不令人印象深刻。因为项集只出现在2%的交易中，而2%在出现次数上是较小的，所以该关联不能被认为是显著的，因此可能在决策中无法使用。
- en: 'Note that since support is a probability, it will fall in the range [0,1].
    The formula takes the following form if the item set is two items, X and Y, and
    N is the total number of transactions:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于支持度是一个概率，因此它的取值范围为 [0,1]。如果商品集是两个商品 X 和 Y，并且 N 是总交易数，则该公式具有以下形式：
- en: '![Figure 8.5: Formula for support'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.5：支持度公式'
- en: '](img/B15923_08_05.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_08_05.jpg)'
- en: 'Figure 8.5: Formula for support'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5：支持度公式
- en: While working with market basket analysis, if the support for an item or group
    of items is lower than a pre-defined threshold, then the purchase of that item
    or group of items is rare enough to not be actionable. Let's return momentarily
    to the made-up data from *Exercise 8.01*, *Creating Sample Transaction Data*,
    and define an item set as being milk and bread. We can easily look through the
    10 transactions and count the number of transactions in which this milk and bread
    item set occurs – that would be 4 times. Given that there are 10 transactions,
    the support of milk and bread is 4 divided by 10, or 0.4\. Whether this is large
    enough support depends on the dataset itself, which we will get into in a later
    section.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行市场篮分析时，如果某个商品或商品组合的支持度低于预定义的阈值，那么购买该商品或商品组合的情况就足够少，无法采取行动。让我们暂时回到来自*练习 8.01*，*创建样本交易数据*的虚构数据，并将一个商品集定义为牛奶和面包。我们可以轻松地查看这
    10 笔交易，并计算包含此牛奶和面包商品集的交易次数 - 共有 4 次。考虑到总共有 10 笔交易，牛奶和面包的支持度为 4 除以 10，即 0.4\. 支持度是否足够大取决于数据集本身，我们将在后面的章节中详细介绍。
- en: Confidence
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 置信度
- en: 'The **confidence** metric can be thought of in terms of conditional probability,
    as it is basically the probability that product B is purchased given the purchase
    of product A. Confidence is typically notated as A ![and expressed ](img/B15923_08_Formula_01.png)
    B, and expressed as the proportion of transactions containing A that also contain
    B. Hence, confidence is found by filtering the full set of transactions down to
    those containing A, and then computing the proportion of those transactions that
    contain B. Like support, confidence is a probability, so its range is [0,1]. Using
    the same variable definitions as in the *Support* section, the following is the
    formula for confidence:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**置信度**指标可以从条件概率的角度来理解，因为它基本上是指购买产品 A 后购买产品 B 的概率。置信度通常表示为 A ![and expressed
    ](img/B15923_08_Formula_01.png) B，并且表示包含 A 的交易中也包含 B 的比例。因此，置信度是通过将完整的交易集筛选到包含
    A 的交易中，然后计算这些交易中包含 B 的比例来确定的。与支持度一样，置信度是一个概率，因此其范围是 [0,1]。使用与*支持度*部分相同的变量定义，以下是置信度的公式：'
- en: '![Figure 8.6: Formula for confidence'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.6：置信度公式'
- en: '](img/B15923_08_06.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_08_06.jpg)'
- en: 'Figure 8.6: Formula for confidence'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6：置信度公式
- en: To demonstrate confidence, we will use the items beer and wine. Specifically,
    let's compute the confidence of Beer ![Formula](img/B15923_08_Formula_01.png)
    Wine. To begin, we need to identify the transactions that contain beer. There
    are three of them, and they are transactions 2, 6, and 7\. Now, of those transactions,
    how many contain wine? The answer is all of them. Thus, the confidence of Beer
    ![Formula](img/B15923_08_Formula_01.png) Wine is 1\. Every time a customer bought
    beer, they also bought wine. It might be obvious, but for identifying actionable
    associations, higher confidence values are better.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示置信度，我们将使用啤酒和葡萄酒这两种商品。具体来说，让我们计算啤酒 ![Formula](img/B15923_08_Formula_01.png)
    葡萄酒的置信度。首先，我们需要确定包含啤酒的交易。有三笔交易，它们是第 2、6 和 7 笔。现在，这些交易中有多少包含葡萄酒？答案是全部。因此，啤酒 ![Formula](img/B15923_08_Formula_01.png)
    葡萄酒的置信度为 1\. 每次客户购买啤酒时，他们也购买了葡萄酒。这可能是显而易见的，但对于识别可操作的关联，较高的置信度值更为有利。
- en: Lift and Leverage
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升度和杠杆
- en: 'We will discuss the next two metrics, lift and leverage, simultaneously, since
    despite being calculated differently, both seek to answer the same question. Like
    confidence, **lift** and **leverage** are notated as A ![Formula](img/B15923_08_Formula_01.png)
    B. The question to which we seek an answer is, can one item, say A, be used to
    determine anything about another item, say B? Stated another way, if product A
    is bought by an individual, can we say anything about whether they will or will
    not purchase product B with some level of confidence? These questions are answered
    by comparing the support of A and B under the standard case when A and B are not
    assumed to be independent with the case where the two products are assumed to
    be independent. Lift calculates the ratio of these two cases, so its range is
    [0, Infinity]. When lift equals one, the two products are independent and, hence,
    no conclusions can be made about product B when product A is purchased:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将同时讨论接下来的两个指标：提升（lift）和杠杆（leverage），因为尽管它们的计算方式不同，但都旨在回答相同的问题。像置信度一样，**提升**和**杠杆**的表示形式是
    A ![公式](img/B15923_08_Formula_01.png) B。我们要回答的问题是，一个项目，比如 A，能否用来推断另一个项目，比如 B？换句话说，如果某个人购买了产品
    A，我们能否在某种程度上有信心地说他们是否会购买产品 B？这些问题通过比较 A 和 B 在标准情况下（即假设 A 和 B 不独立时）与假设这两个产品独立时的支持度来回答。提升计算这两种情况的比率，因此它的取值范围是
    [0, 无穷大]。当提升值为 1 时，两个产品是独立的，因此在购买产品 A 时，无法得出关于产品 B 的任何结论：
- en: '![Figure 8.7: Formula for lift'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.7：提升公式](img/B15923_08_Formula_01.png)'
- en: '](img/B15923_08_07.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_08_07.jpg)'
- en: 'Figure 8.7: Formula for lift'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7：提升公式
- en: 'Leverage calculates the difference between the two cases, so its range is [-1,
    1]. Leverage equaling zero can be interpreted the same way as lift equaling one:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 杠杆计算这两种情况的差异，因此它的取值范围是 [-1, 1]。杠杆值为零的解释方式与提升值为 1 时相同：
- en: '![Figure 8.8: Formula for leverage'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.8：杠杆公式](img/B15923_08_Formula_01.png)'
- en: '](img/B15923_08_08.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_08_08.jpg)'
- en: 'Figure 8.8: Formula for leverage'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8：杠杆公式
- en: The values of the metrics measure the degree and orientation (in other words,
    positive or negative) of the relationship between the items. A value of lift other
    than 1 means that some dependency exists between the items. When the value is
    greater than 1, the second item is more likely to be purchased if the first item
    is purchased. Likewise, when the value is less than 1, the second item is less
    likely to be purchased if the first item is purchased. If the lift value is 0.1,
    we could say that the relationship between the two items is strong in the negative
    direction. That is, it could be said that when one product is purchased, the chance
    the second product is purchased is diminished. A lift of 1 indicates that the
    products are independent of one another. In the case of leverage, a positive value
    implies a positive association, while a negative value indicates a negative association.
    The positive and negative associations are separated by the points of independence,
    which, as stated earlier, are 1 for lift and 0 for leverage, and the further away
    the value gets from these points, the stronger the association.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标的值衡量了项目之间关系的程度和方向（换句话说，正向或负向）。当提升值不为 1 时，说明项目之间存在某种依赖关系。当提升值大于 1 时，购买第一个产品时，第二个产品更可能被购买。同样，当提升值小于
    1 时，购买第一个产品时，第二个产品的购买可能性较小。如果提升值为 0.1，我们可以说这两个项目之间的关系是强烈的负向关系。也就是说，可以说当购买了一个产品时，购买第二个产品的机会会减少。提升值为
    1 表示产品之间是独立的。在杠杆的情况下，正值表示正相关，负值表示负相关。正相关和负相关由独立点分开，正如前面所说，提升的独立点是 1，而杠杆的独立点是 0，值离这些独立点越远，关联越强。
- en: Conviction
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确信度
- en: 'The last metric to be discussed is conviction, which is a bit less intuitive
    than the other metrics. Conviction is the ratio of the expected frequency that
    X occurs without Y, given that X and Y are independent of the frequency of incorrect
    predictions. The frequency of incorrect predictions is defined as 1 minus the
    confidence of X ![Formula](img/B15923_08_Formula_01.png) Y. Remember that confidence
    can be defined as *P(Y|X)*, which means *1 – P(Y|X) = P(Not Y|X)*. The numerator
    could also be thought of as *1 – P(Y|X) = P(Not Y|X)*. The only difference between
    the two is that the numerator has the assumption of independence between X and
    Y, while the denominator does not. A value greater than 1 is ideal because that
    means the association between products or item sets X and Y is incorrect more
    often if the association between X and Y is random (in other words, X and Y are
    independent). To reiterate, this stipulates that the association between X and
    Y is meaningful. A value of 1 applies independence, and a value of less than 1
    signifies that the random chance X and Y relationship is correct more often than
    the X and Y relationship that has been defined as X ![Formula](img/B15923_08_Formula_06.png)
    Y. Under this situation, the relationship might go the other way (in other words,
    Y ![Formula](img/B15923_08_Formula_06.png) X). Conviction has the range [0, Infinity]
    and the following form:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来讨论的最后一个度量是信念（conviction），它比其他度量更不直观。信念是指在 X 和 Y 相互独立的情况下，X 发生而 Y 不发生的预期频率与错误预测频率的比率。错误预测频率定义为
    1 减去 X 的置信度 ![公式](img/B15923_08_Formula_01.png) Y。请记住，置信度可以定义为 *P(Y|X)*，这意味着 *1
    – P(Y|X) = P(Not Y|X)*。分子也可以被理解为 *1 – P(Y|X) = P(Not Y|X)*。这两者之间的唯一区别在于，分子假设 X
    和 Y 之间是独立的，而分母则没有这个假设。理想情况下，值大于 1，因为这意味着如果 X 和 Y 之间的关联是随机的（换句话说，X 和 Y 是独立的），则
    X 和 Y 之间的关系会更常出现错误。重申一下，这表明 X 和 Y 之间的关联是有意义的。值为 1 时表示独立，值小于 1 则表示随机的 X 和 Y 关系比已定义的
    X 和 Y 关系正确的情况更常见。此时，关系可能是反向的（换句话说，Y ![公式](img/B15923_08_Formula_06.png) X）。信念的取值范围是
    [0, 无穷大]，其公式如下：
- en: '![Figure 8.9: Formula for conviction'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.9：信念公式'
- en: '](img/B15923_08_09.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_08_09.jpg)'
- en: 'Figure 8.9: Formula for conviction'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9：信念公式
- en: Let's again return to the products beer and wine, but for this explanation,
    we will consider the opposite association of Wine ![Formula](img/B15923_08_Formula_06.png)
    Beer. Support(Y) or, in this case, Support(Beer) is 3/10, and Confidence X ![Formula](img/B15923_08_Formula_06.png)
    Y, or, in this case, Confidence(Wine ![Formula](img/B15923_08_Formula_06.png)
    Beer) is 3/4\. Thus, the Conviction(Wine ![Formula](img/B15923_08_Formula_06.png)
    Beer) is (1-3/10) / (1-3/4) = (7/10) * (4/1). We can conclude by saying that Wine
    ![Formula](img/B15923_08_Formula_06.png) Beer would be incorrect 2.8 times as
    often if wine and beer were independent. Thus, the previously articulated association
    between wine and beer is legitimate.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次回到啤酒和葡萄酒这两个产品，但在本次解释中，我们将考虑葡萄酒 ![公式](img/B15923_08_Formula_06.png) 啤酒的反向关联。Support(Y)，或者在这个例子中，Support(Beer)
    是 3/10，Confidence X ![公式](img/B15923_08_Formula_06.png) Y，或者在这个例子中，Confidence(Wine
    ![公式](img/B15923_08_Formula_06.png) Beer) 是 3/4。因此，Conviction(Wine ![公式](img/B15923_08_Formula_06.png)
    Beer) 是 (1-3/10) / (1-3/4) = (7/10) * (4/1)。我们可以得出结论，如果葡萄酒和啤酒是独立的，那么葡萄酒 ![公式](img/B15923_08_Formula_06.png)
    啤酒的关联会错误出现 2.8 次。因此，之前所述的葡萄酒和啤酒之间的关联是合法的。
- en: 'Exercise 8.02: Computing Metrics'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 8.02：计算度量
- en: 'In this exercise, we''ll use the fake data from *Exercise 8.01*, *Creating
    Sample Transaction Data*, to compute the five previously described metrics, which
    we will use again in the covering of the Apriori algorithm and association rules.
    The association on which these metrics will be evaluated is Milk ![Formula](img/B15923_08_Formula_06.png)
    Bread:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将使用 *练习 8.01* 中的虚拟数据，即 *创建样本事务数据*，来计算之前描述的五个度量，这些度量将再次用于 Apriori 算法和关联规则的讲解。将评估这些度量的关联是
    Milk ![公式](img/B15923_08_Formula_06.png) Bread：
- en: Note
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: All exercises in this chapter need to be performed in the same Jupyter notebook.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有练习都需要在同一个 Jupyter notebook 中进行。
- en: 'Define and print the frequencies that are the basis of all five metrics, which
    would be Frequency(Milk), Frequency(Bread), and Frequency(Milk, Bread). Also,
    define `N` as the total number of transactions in the dataset:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并打印所有五个度量的基础频率，包括 Frequency(Milk)、Frequency(Bread) 和 Frequency(Milk, Bread)。此外，定义
    `N` 为数据集中事务的总数：
- en: '[PRE2]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output is as follows:'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE3]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Calculate and print Support(Milk ![Formula](img/B15923_08_Formula_06.png) Bread):'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并打印支持度（Milk ![Formula](img/B15923_08_Formula_06.png) Bread）：
- en: '[PRE4]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The support of `x` to `y` is `0.4`. From experience, if we were working with
    a full transaction dataset, this support value would be considered very large
    in many cases.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`x`到`y`的支持度为`0.4`。根据经验，如果我们处理的是完整的交易数据集，在许多情况下，这个支持度值会被认为是非常大的。'
- en: 'Calculate and print Confidence(Milk ![Formula](img/B15923_08_Formula_06.png)
    Bread):'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并打印置信度（Milk ![Formula](img/B15923_08_Formula_06.png) Bread）：
- en: '[PRE5]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The confidence of `x` to `y` is `0.5714`. This means that the probability of
    Y being purchased given that `x` was purchased is just slightly higher than 50%.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`x`到`y`的置信度为`0.5714`。这意味着，给定`x`已购买，Y被购买的概率仅略高于50%。'
- en: 'Calculate and print Lift(Milk ![Formula](img/B15923_08_Formula_06.png) Bread):'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并打印提升度（Milk ![Formula](img/B15923_08_Formula_06.png) Bread）：
- en: '[PRE6]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The lift of `x` to `y` is `1.1429`.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`x`到`y`的提升度为`1.1429`。'
- en: 'Calculate and print Leverage(Milk ![Formula](img/B15923_08_Formula_06.png)
    Bread):'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并打印杠杆率（Milk ![Formula](img/B15923_08_Formula_06.png) Bread）：
- en: '[PRE7]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The leverage of `x` to `y` is `0.05`. Both lift and leverage can be used to
    say that the association `x` to `y` is positive (in other words, `x` implies `y`)
    but weak. The values for lift and leverage are close to 1 and 0, respectively.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`x`到`y`的杠杆率为`0.05`。提升度和杠杆率都可以用来表示`x`到`y`的关联是正向的（换句话说，`x`意味着`y`），但关联较弱。提升度和杠杆率的值分别接近1和0。'
- en: 'Calculate and print Conviction(Milk ![Formula](img/B15923_08_Formula_06.png)
    Bread):'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并打印确定度（Milk ![Formula](img/B15923_08_Formula_06.png) Bread）：
- en: '[PRE8]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The conviction value of `1.1667` can be interpreted by saying the Milk ![Formula](img/B15923_08_Formula_06.png)
    Bread association would be incorrect `1.1667` times as often if milk and bread
    were independent.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`1.1667`的确定度值可以解释为：如果牛奶和面包是独立的，Milk ![Formula](img/B15923_08_Formula_06.png)
    Bread关联会错误地出现`1.1667`倍。'
- en: In this exercise, we explored a series of probabilistic metrics designed to
    quantify the relationship between two items. The five metrics are support, confidence,
    lift, leverage, and conviction. We will use these metrics again as they are the
    foundation of both the Apriori algorithm and association rules.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个练习中，我们探索了一系列旨在量化两个项目之间关系的概率度量。五个度量分别是支持度、置信度、提升度、杠杆率和确定度。我们将在后续的Apriori算法和关联规则中再次使用这些度量，它们是这两者的基础。
- en: Note
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fhf9bS](https://packt.live/3fhf9bS).
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问本节的源代码，请参考[https://packt.live/3fhf9bS](https://packt.live/3fhf9bS)。
- en: You can also run this example online at [https://packt.live/303vIBJ](https://packt.live/303vIBJ).
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你还可以在网上运行这个示例，网址是[https://packt.live/303vIBJ](https://packt.live/303vIBJ)。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你必须执行整个Notebook才能获得预期的结果。
- en: Before diving into the Apriori algorithm and association rule learning on actual
    data, we will explore transaction data and get some retail data loaded and prepped
    for modeling.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际进行Apriori算法和关联规则学习之前，我们将探索交易数据，并加载一些零售数据，为建模做好准备。
- en: Characteristics of Transaction Data
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交易数据的特点
- en: 'The data used in market basket analysis is transaction data or any type of
    data that resembles transaction data. In its most basic form, transaction data
    has some sort of transaction identifier, such as an invoice or transaction number,
    and a list of products associated with said identifier. It just so happens that
    these two base elements are all that is needed to perform market basket analysis.
    However, transaction data rarely – it is probably even safe to say never – comes
    in this basic form. Transaction data typically includes pricing information, dates
    and times, and customer identifiers, among many other things. Here is how each
    product is mapped to multiple invoices:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在市场篮子分析中使用的数据是交易数据或任何类似交易数据的类型。最基本的形式下，交易数据包含某种交易标识符，比如发票或交易号，以及与该标识符关联的产品列表。恰好这两个基本元素就是执行市场篮子分析所需的全部。然而，交易数据很少——甚至可以说从未——以这种最基本的形式存在。交易数据通常包括定价信息、日期和时间、客户标识符等许多其他信息。以下是每个产品如何映射到多个发票的方式：
- en: '![Figure 8.10: Each available product is going to map back to multiple invoice
    numbers'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.10：每个可用的产品都将映射回多个发票号码'
- en: '](img/B15923_08_10.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_08_10.jpg)'
- en: 'Figure 8.10: Each available product is going to map back to multiple invoice
    numbers'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10：每个可用的产品都将映射回多个发票号码
- en: Due to the complexity of transaction data, data cleaning is crucial. The goal
    of data cleaning in the context of market basket analysis is to filter out all
    the unnecessary information, which includes removing variables in the data that
    are not relevant and filtering out problematic transactions. The techniques used
    to complete these two cleaning steps vary, depending on the particular transaction
    data file. In an attempt to not get bogged down in data cleaning, the exercises
    from here on out will use a subset of an online retail dataset from the UCI Machine
    Learning Repository, and the activities will use the entire dataset. This both
    limits the data cleaning discussion, but also gives us an opportunity to discuss
    how the results change when the size of the dataset changes. This is important
    because if you work for a retailer and run market basket analysis, it will be
    important to understand and be able to clearly articulate the fact that, as more
    data is received, product relationships can, and most likely will, shift. Before
    discussing the specific cleaning process required for this dataset, let's load
    the online retail dataset.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于交易数据的复杂性，数据清洗至关重要。在市场篮子分析的背景下，数据清洗的目标是过滤掉所有不必要的信息，包括删除数据中不相关的变量，并筛选掉有问题的交易。完成这两步清洗的技术方法因具体的交易数据文件而异。为了避免过度陷入数据清洗，本书从此开始使用UCI机器学习库中的一个在线零售数据集的子集，活动则会使用整个数据集。这既限制了数据清洗的讨论，也为我们提供了一个机会，讨论数据集大小变化时结果的变化。这一点非常重要，因为如果你在零售商工作并进行市场篮子分析，了解并能够清晰表达一个事实是非常关键的：随着更多数据的接收，产品之间的关系可能会发生变化，而且很可能会发生变化。在讨论此数据集所需的特定清洗过程之前，我们先来加载在线零售数据集。
- en: Note
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'In all subsequent exercises and activities, there could be slight differences
    in the output from what is shown in the following. This is due to one of two things:
    issues with data loading (in other words, rows getting shuffled) or the fact that
    `mlxtend` does not have a seed setting option to guarantee consistency across
    executions.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的所有练习和活动中，输出结果可能与下面展示的有所不同。这可能是由于以下两种原因之一：数据加载的问题（换句话说，行数据被打乱）或是`mlxtend`没有设置种子选项来确保执行结果的一致性。
- en: 'Exercise 8.03: Loading Data'
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 8.03：加载数据
- en: In this exercise, we will load and view an example online retail dataset. This
    dataset is originally sourced from the UCI Machine Learning Repository and can
    be found at [https://packt.live/2XeT6ft](https://packt.live/2XeT6ft). Once you
    have downloaded the dataset, save it and note the path. Now, let's proceed with
    the exercise. The output of this exercise is the transaction data that will be
    used in future modeling exercises and some exploratory figures to help us better
    understand the data with which we are working.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将加载并查看一个示例的在线零售数据集。该数据集最初来自UCI机器学习库，可以在[https://packt.live/2XeT6ft](https://packt.live/2XeT6ft)找到。下载数据集后，请保存并记录路径。现在，让我们继续进行练习。此练习的输出是将用于未来建模练习的交易数据，以及一些探索性图形，帮助我们更好地理解我们所使用的数据。
- en: Note
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is sourced from [http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail#)
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来源于[http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail#)（UCI机器学习库
    [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚州尔湾：加利福尼亚大学信息与计算机科学学院）。
- en: 'Citation: This is a subset of the online retail dataset obtained from the UCI
    Machine Learning repository. Daqing Chen, Sai Liang Sain, and Kun Guo, *Data mining
    for the online retail industry*: *A case study of RFM model-based customer segmentation
    using data mining*, Journal of Database Marketing and Customer Strategy Management,
    Vol. 19, No. 3, pp. 197-208, 2012.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 引用：这是从UCI机器学习库获取的在线零售数据集的一个子集。Daqing Chen, Sai Liang Sain 和 Kun Guo, *在线零售行业的数据挖掘*：*基于RFM模型的数据挖掘客户细分案例研究*，《数据库营销与客户战略管理杂志》，第19卷，第3期，页码197-208，2012年。
- en: It can be downloaded from [https://packt.live/2XeT6ft](https://packt.live/2XeT6ft).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以从[https://packt.live/2XeT6ft](https://packt.live/2XeT6ft)下载。
- en: 'Perform the following steps:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'Using the `read_excel` function from `pandas`, load the data. Note that we
    can define the first row as containing column names by adding `header=0` to the
    `read_excel` function:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`中的`read_excel`函数加载数据。注意，我们可以通过在`read_excel`函数中添加`header=0`来定义第一行作为列名：
- en: '[PRE9]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The path to `Online Retail.xlsx` should be changed as per the location of the
    file on your system.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Online Retail.xlsx`的路径应根据文件在你系统上的位置进行更改。'
- en: 'Print out the first 10 rows of the DataFrame. Notice that the data contains
    some columns that will not be relevant to market basket analysis:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出DataFrame的前10行。注意，数据包含一些与市场篮分析无关的列：
- en: '[PRE10]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.11: The raw online retail data'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.11：原始在线零售数据'
- en: '](img/B15923_08_11.jpg)'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_11.jpg)'
- en: 'Figure 8.11: The raw online retail data'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.11：原始在线零售数据
- en: 'Print out the data type for each column in the DataFrame. This information
    will come in handy when trying to perform specific cleaning tasks. Columns need
    to be of the correct type in order for filtering and computing to execute as expected:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出DataFrame中每列的数据类型。这些信息在执行特定清理任务时非常有用。列需要具备正确的数据类型，才能确保过滤和计算按预期执行：
- en: '[PRE11]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE12]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Get the dimensions of the DataFrame, as well as the number of unique invoice
    numbers and customer identifications:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取DataFrame的维度，以及唯一发票号码和客户标识的数量：
- en: '[PRE13]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE14]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: From the preceding output, we can say that we successfully loaded the data and
    obtained some key information which will be further used as we progress with exercises
    in this chapter.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，我们可以看到数据成功加载，并且获得了一些关键的信息，这些信息将在本章练习中进一步使用。
- en: Note
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fhf9bS](https://packt.live/3fhf9bS).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参见[https://packt.live/3fhf9bS](https://packt.live/3fhf9bS)。
- en: You can also run this example online at [https://packt.live/303vIBJ](https://packt.live/303vIBJ).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/303vIBJ](https://packt.live/303vIBJ)上在线运行此示例。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须执行整个Notebook，才能得到期望的结果。
- en: Data Cleaning and Formatting
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清理与格式化
- en: With the dataset now loaded, let's delve into the specific data cleaning processes
    to be performed. Since we are going to filter the data down to just the invoice
    numbers and items, we focus the data cleaning on these two columns of the dataset.
    Remember that market basket analysis looks to identify associations between the
    items purchased by all customers over time. As such, the main focus of data cleaning
    involves removing transactions with a non-positive number of items. This could
    happen when the transaction involves voiding another transaction, when items are
    returned, or when the transaction is an administrative task. These types of transactions
    will be filtered out in two ways. The first is that canceled transactions have
    invoice numbers that are prefixed with "C," so we will identify those specific
    invoice numbers and remove them from the data. The other approach is to remove
    all transactions with either zero or a negative number of items. After performing
    these two steps, the data will be subset down to just the invoice number and item
    description columns, and any row of the now two-column dataset with at least one
    missing value is removed.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据集已经加载完成，接下来让我们深入了解具体的数据清理过程。由于我们要将数据过滤到仅包含发票号码和商品，因此我们将数据清理重点放在这两列上。记住，市场篮分析旨在识别所有顾客在一段时间内购买的商品之间的关联。因此，数据清理的主要任务是去除包含非正数商品数量的交易。这种情况可能发生在交易作废时、商品退货时，或交易为行政操作时。这类交易将通过两种方式进行过滤。第一种方式是，取消的交易会在发票号码前加上“C”字母，因此我们会识别出这些特定的发票号码并将其从数据中删除。另一种方式是移除所有商品数量为零或负数的交易。执行完这两个步骤后，数据将被筛选至仅包含发票号码和商品描述两列，并且任何具有至少一个缺失值的两列数据行都将被删除。
- en: The next stage of the data cleaning exercise involves putting the data in the
    appropriate format for modeling. In this and subsequent exercises, we will use
    a subset of the full data. The subset will be created by taking the first 5,000
    unique invoice numbers. Once we have cut the data down to the first 5,000 unique
    invoice numbers, we change the data structure to the structure needed to run the
    models. Note that the data currently features one item per row, so transactions
    with multiple items take up multiple rows. The desired format is a list of lists,
    like the made-up data from earlier in the chapter. Each subset list represents
    a unique invoice number, so in this case, the outer list should contain 5,000
    sub-lists. The elements of the sub-lists are all the items belonging to the invoice
    number that that sub-list represents. With the cleaning process described, let's
    proceed to the exercise.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗练习的下一阶段是将数据转换为适合建模的格式。在本练习和后续练习中，我们将使用完整数据的一个子集。该子集将通过取前5,000个唯一发票号来创建。一旦我们将数据减少到前5,000个唯一发票号，我们将数据结构更改为运行模型所需的结构。请注意，目前数据每行一个商品，因此多件商品的交易占用多行。所需的格式是一个列表的列表，就像本章早些时候提到的虚构数据一样。每个子列表代表一个唯一的发票号，因此在这种情况下，外部列表应包含5,000个子列表。子列表的元素是属于该发票号的所有商品。按照描述的清洗过程，让我们继续进行练习。
- en: 'Exercise 8.04: Data Cleaning and Formatting'
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习8.04：数据清洗与格式化
- en: 'In this exercise, we will perform the cleaning steps described previously.
    As we work through the process, the evolution of the data will be monitored by
    printing out the current state of the data and computing some basic summary metrics.
    Be sure to perform data cleaning in the same notebook in which the data is loaded:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将执行之前描述的清洗步骤。在处理过程中，通过打印出数据的当前状态并计算一些基本的汇总指标来监控数据的演变。请确保在加载数据的同一本笔记本中执行数据清洗：
- en: 'Create an indicator column stipulating whether the invoice number begins with
    "`C`":'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个指示列，规定发票号是否以 "`C`" 开头：
- en: '[PRE15]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Filter out all transactions having either zero or a negative number of items
    (in other words, items were returned), remove all invoice numbers starting with
    "`C`" using the column created in step one, subset the DataFrame down to `InvoiceNo`
    and `Description`, and lastly, drop all rows with at least one missing value.
    Rename the DataFrame `online1`:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 筛选出所有包含零个或负数商品（换句话说，商品已退还）的交易，使用第一个步骤创建的列删除所有以"`C`"开头的发票号，将数据框子集化为`InvoiceNo`和`Description`，最后删除所有含有至少一个缺失值的行。将数据框重命名为`online1`：
- en: '[PRE16]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Print out the first 10 rows of the filtered DataFrame, `online1`:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出过滤后的`online1`数据框的前10行：
- en: '[PRE17]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.12: The cleaned online retail dataset'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.12：清洗后的在线零售数据集'
- en: '](img/B15923_08_12.jpg)'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_12.jpg)'
- en: 'Figure 8.12: The cleaned online retail dataset'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.12：清洗后的在线零售数据集
- en: 'Print out the dimensions of the cleaned DataFrame and the number of unique
    invoice numbers using the `nunique` function, which counts the number of unique
    values in a DataFrame column:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出清洗后数据框的维度以及通过`nunique`函数计算的唯一发票号数量，该函数计算数据框列中唯一值的数量：
- en: '[PRE18]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE19]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Notice that we have already removed approximately 10,000 rows and 5,800 invoice
    numbers.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们已经删除了大约10,000行和5,800个发票号。
- en: 'Extract the invoice numbers from the DataFrame as a list. Remove duplicate
    elements to create a list of unique invoice numbers. Confirm that the process
    was successful by printing the length of the list of unique invoice numbers. Compare
    with the output of *Step 4*:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将发票号从数据框中提取为列表。删除重复元素，创建一个唯一发票号的列表。通过打印唯一发票号列表的长度来确认该过程是否成功。与*第4步*的输出进行比较：
- en: '[PRE20]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is as follows:'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE21]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Take the list from *Step 5* and cut it to only include the first 5,000 elements.
    Print out the length of the new list to confirm that it is, in fact, the expected
    length of 5,000:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*第5步*中获取列表，仅保留前5,000个元素。打印出新列表的长度，以确认它确实是预期的5,000长度：
- en: '[PRE22]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output is as follows:'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE23]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Filter the `online1` DataFrame down by only keeping the invoice numbers in
    the list from the preceding step:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过只保留前一步骤中列出的发票号来过滤`online1`数据框：
- en: '[PRE24]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Print out the first 10 rows of `online1`:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出`online1`的前10行：
- en: '[PRE25]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is as follows:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.13: The cleaned dataset with only 5,000 unique invoice numbers'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.13：仅包含5,000个唯一发票号的清洗后数据集'
- en: '](img/B15923_08_13.jpg)'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_13.jpg)'
- en: 'Figure 8.13: The cleaned dataset with only 5,000 unique invoice numbers'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.13：清洗后的数据集，仅包含5000个唯一的发票编号
- en: 'Print out the dimensions of the DataFrame and the number of unique invoice
    numbers to confirm that the filtering and cleaning process was successful:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出数据框的维度以及唯一发票编号的数量，以确认过滤和清洗过程成功：
- en: '[PRE26]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is as follows:'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE27]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Transform the data in `online1` into the aforementioned list of lists called
    `invoice_item_list`. The process for doing this is to iterate over the unique
    invoice numbers and, at each iteration, extract the item descriptions as a list
    and append that list to the larger `invoice_item_list` list. Print out elements
    one through four of the list:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`online1`中的数据转换为前述的列表形式，称为`invoice_item_list`。执行此操作的过程是遍历唯一的发票编号，在每次迭代时，将物品描述提取为一个列表，并将该列表附加到更大的`invoice_item_list`列表中。打印出列表中的前四个元素：
- en: '[PRE28]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is as follows:'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.14: Four elements of the list of lists'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.14：列表中的四个元素'
- en: '](img/B15923_08_14.jpg)'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_14.jpg)'
- en: 'Figure 8.14: Four elements of the list of lists'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14：列表中的四个元素
- en: In the preceding list of lists, each sub-list contains all the items belonging
    to an individual invoice.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述的列表中，每个子列表包含属于单个发票的所有项目。
- en: Note
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This step can take some minutes to complete.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这个步骤可能需要几分钟才能完成。
- en: In this exercise, the DataFrame was filtered and subset to only the needed columns
    and the relevant rows. We then cut the full dataset down to the first 5,000 unique
    invoice numbers. The full dataset will be used in the forthcoming activities.
    The last step converted the DataFrame to a list of lists, which is the format
    the data needs to be in for the encoder that will be discussed next.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，数据框被过滤并子集化为仅包含所需的列和相关的行。接着我们将完整的数据集缩减为前5000个唯一的发票编号。完整的数据集将在接下来的活动中使用。最后一步将数据框转换为列表形式，这是数据需要为编码器准备的格式，接下来会讨论该编码器。
- en: Note
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fhf9bS](https://packt.live/3fhf9bS).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该部分的源代码，请参考[https://packt.live/3fhf9bS](https://packt.live/3fhf9bS)。
- en: You can also run this example online at [https://packt.live/303vIBJ](https://packt.live/303vIBJ).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，网址为[https://packt.live/303vIBJ](https://packt.live/303vIBJ)。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须执行整个笔记本才能获得期望的结果。
- en: Data Encoding
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据编码
- en: While cleaning the data is crucial, the most important part of the data preparation
    process is molding the data into the correct form. Before running the models,
    the data, currently in the list of lists form, needs to be encoded and recast
    as a DataFrame. To do this, we will leverage `TransactionEncoder` from the `preprocessing`
    module of `mlxtend`. The output from the encoder is a multidimensional array,
    where each row is the length of the total number of unique items in the transaction
    dataset and the elements are Boolean variables, indicating whether that particular
    item is linked to the invoice number that row represents. With the data encoded,
    we can recast it as a DataFrame where the rows are the invoice numbers and the
    columns are the unique items in the transaction dataset.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然清洗数据至关重要，但数据准备过程最重要的部分是将数据塑造成正确的格式。在运行模型之前，目前以列表形式存在的数据需要进行编码，并重新转换为数据框。为此，我们将利用`mlxtend`库中的`preprocessing`模块中的`TransactionEncoder`。编码器的输出是一个多维数组，其中每行的长度等于交易数据集中唯一项目的总数，元素是布尔变量，表示该特定项目是否与该行表示的发票编号相关联。编码后的数据可以重新转化为数据框，其中行是发票编号，列是交易数据集中的唯一项目。
- en: In the following exercise, the data encoding will be done using `mlxtend`, but
    it is very easy to encode the data without using a package. The first step is
    to unlist the list of lists and return one list with every value from the original
    list of lists. Next, the duplicate products are filtered out and, if preferred,
    the data is sorted in alphabetical order. Before doing the actual encoding, we
    initialize the final DataFrame by having all elements equal to false, the number
    of rows equal to the number of invoice numbers in the dataset, and column names
    equal to the non-duplicated list of product names.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，数据编码将使用 `mlxtend` 完成，但实际上不使用任何包也可以轻松编码数据。第一步是将列表中的列表拆开，并返回一个包含原始列表中所有值的单一列表。接下来，过滤掉重复的产品，并且如果需要的话，可以按字母顺序对数据进行排序。在进行实际编码之前，我们通过让所有元素初始值为
    false，行数等于数据集中的发票号数量，列名为去重后的产品名称列表，来初始化最终的 DataFrame。
- en: 'In this case, we have 5,000 transactions and over 3,100 unique products. Thus,
    the DataFrame has over 15,000,000 elements. The actual encoding is done by looping
    over each transaction and each item in each transaction. Change the row *i* and
    column *j* cell values in the initialized dataset from `false` to `true` if the
    *i*th transaction contains the *j*th product. This double loop is not fast as
    we need to iterate over 15,000,000 cells. There are ways to improve performance,
    including some that have been implemented in `mlxtend`, but to better understand
    the process, it is helpful to work through the double loop methodology. The following
    is an example function to do the encoding from scratch without the assistance
    of a package other than `pandas`:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，我们有 5,000 个交易和超过 3,100 个独特的产品。因此，DataFrame 中有超过 15,000,000 个元素。实际的编码是通过遍历每个交易和每个交易中的每个商品来完成的。如果第
    *i* 个交易包含第 *j* 个产品，则将初始化数据集中的第 *i* 行和第 *j* 列的单元格值从 `false` 改为 `true`。这个双重循环效率不高，因为我们需要遍历
    15,000,000 个单元格。虽然有一些方法可以提高性能，包括在 `mlxtend` 中实现的一些方法，但为了更好地理解这个过程，还是有助于通过双重循环的方法来进行操作。以下是一个示例函数，从零开始进行编码，除了
    `pandas` 外不使用其他包：
- en: '[PRE29]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Exercise 8.05: Data Encoding'
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 8.05：数据编码
- en: 'In this exercise, we''ll continue the data preparation process by taking the
    list of lists generated in the preceding exercise and encoding the data in the
    specific way required to run the models:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将继续数据准备过程，通过使用前一练习中生成的列表列表并按照特定方式对数据进行编码，以便运行模型：
- en: 'Initialize and fit the transaction encoder. Print out an example of the resulting
    data:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化并拟合交易编码器。打印结果数据的一个示例：
- en: '[PRE30]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output is as follows:'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE31]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The preceding array contains the Boolean variables indicating the product presence
    in each transaction.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述数组包含布尔变量，指示每个交易中是否包含某个产品。
- en: 'Recast the encoded array as a DataFrame named `online_encoder_df`. Print the
    predefined subset of the DataFrame that features both `True` and `False` values:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将编码后的数组重新构建为名为 `online_encoder_df` 的 DataFrame。打印该 DataFrame 的预定义子集，其中包含 `True`
    和 `False` 值：
- en: '[PRE32]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output will be similar to the following:'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '![Figure 8.15: A small section of the encoded data recast as a DataFrame'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.15：重新构建为 DataFrame 的编码数据的小部分'
- en: '](img/B15923_08_15.jpg)'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_15.jpg)'
- en: 'Figure 8.15: A small section of the encoded data recast as a DataFrame'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.15：重新构建为 DataFrame 的编码数据的一个小部分
- en: 'Print out the dimensions of the encoded DataFrame. It should have 5,000 rows
    because the data used to generate it was previously filtered down to 5,000 unique
    invoice numbers:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印编码后的 DataFrame 的维度。它应该有 5,000 行，因为用于生成它的数据之前已被过滤为 5,000 个唯一的发票号：
- en: '[PRE33]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output will be similar to the following:'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '[PRE34]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The data is now prepared for modeling, which we will perform in *Exercise 8.06*,
    *Executing the Apriori Algorithm*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 数据现在已经为建模做好准备，我们将在 *练习 8.06* 中执行 *Apriori 算法*。
- en: Note
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fhf9bS](https://packt.live/3fhf9bS).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅 [https://packt.live/3fhf9bS](https://packt.live/3fhf9bS)。
- en: You can also run this example online at [https://packt.live/303vIBJ](https://packt.live/303vIBJ).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 [https://packt.live/303vIBJ](https://packt.live/303vIBJ) 在线运行此示例。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须执行整个 Notebook 才能获得所需的结果。
- en: 'Activity 8.01: Loading and Preparing Full Online Retail Data'
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 8.01：加载并准备完整的在线零售数据
- en: In this activity, we are charged with loading and preparing a large transaction
    dataset for modeling. The final output will be an appropriately encoded dataset
    that has one row for each unique transaction in the dataset, and one column for
    each unique item in the dataset. If an item appears in an individual transaction,
    that element of the DataFrame will be marked `true`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项活动中，我们负责加载和准备大型交易数据集用于建模。最终输出将是一个适当编码的数据集，其中每个唯一交易在数据集中有一行，并且数据集中每个唯一项目有一列。如果一个项目出现在单个交易中，数据框的该元素将标记为`true`。
- en: This activity will largely repeat the last few exercises but will use the complete
    online retail dataset file. No new downloads need to be executed, but you will
    need the path to the file downloaded previously. Perform this activity in a separate
    Jupyter notebook.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动将主要重复前几个练习，但将使用完整的在线零售数据集文件。不需要执行新的下载，但需要先前下载文件的路径。请在单独的Jupyter笔记本中执行此活动。
- en: 'The following steps will help you to complete the activity:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 下列步骤将帮助您完成这项活动：
- en: Load the online retail dataset file.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载在线零售数据集文件。
- en: Note
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is sourced from [http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail#)
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science).'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此数据集来源于[http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail#)（UCI机器学习库[[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加州尔湾：加州大学信息与计算机科学学院）。
- en: 'Citation: Daqing Chen, Sai Liang Sain, and Kun Guo, *Data mining for the online
    retail industry: A case study of RFM model-based customer segmentation using data
    mining*, Journal of Database Marketing and Customer Strategy Management, Vol.
    19, No. 3, pp. 197-208, 2012.'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 引用：Daqing Chen，Sai Liang Sain 和 Kun Guo，《面向在线零售行业的数据挖掘：基于数据挖掘的RFM模型客户分割的案例研究》，《数据库营销和客户战略管理杂志》，第19卷，第3期，第197-208页，2012年。
- en: It can be downloaded from [https://packt.live/39Nx3iQ](https://packt.live/39Nx3iQ).
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以从[https://packt.live/39Nx3iQ](https://packt.live/39Nx3iQ)下载。
- en: Clean and prepare the data for modeling, including turning the cleaned data
    into a list of lists.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清理和准备建模数据，包括将清理后的数据转换为列表的列表。
- en: Encode the data and recast it as a DataFrame.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码数据并将其重新转换为数据框。
- en: Note
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 490.
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此活动的解决方案可以在第490页找到。
- en: 'The output will be similar to the following:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '![Figure 8.16: A subset of the cleaned, encoded, and recast DataFrame built
    from the complete online retail dataset'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.16：从完整在线零售数据集构建的已清理、编码和重新转换的数据框的子集'
- en: '](img/B15923_08_16.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_08_16.jpg)'
- en: 'Figure 8.16: A subset of the cleaned, encoded, and recast DataFrame built from
    the complete online retail dataset'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.16：从完整在线零售数据集构建的已清理、编码和重新转换的数据框的子集
- en: The Apriori Algorithm
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apriori 算法
- en: The **Apriori** algorithm is a data mining methodology for identifying and quantifying
    frequent item sets in transaction data and is the foundational component of association
    rule learning. Extending the results of the Apriori algorithm to association rule
    learning will be discussed in the next section. The threshold for an item set
    being frequent is an input (in other words, a hyperparameter) of the model and,
    as such, is adjustable. Frequency is quantified here as support, so the value
    input into the model is the minimum support acceptable for the analysis being
    done. The model then identifies all item sets whose support is greater than, or
    equal to, the minimum support provided to the model.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apriori** 算法是识别和量化交易数据中频繁项集的数据挖掘方法论，是关联规则学习的基础组件。将 Apriori 算法的结果扩展到关联规则学习将在下一节讨论。项集被认为频繁的阈值是模型的输入（即超参数），因此可调整。在这里频率被量化为支持度，因此输入到模型中的值是分析中所接受的最小支持度。然后，模型识别所有支持度大于或等于模型提供的最小支持度的项集。'
- en: Note
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The minimum support hyperparameter is not a value that can be optimized via
    grid search because there is no evaluation metric for the Apriori algorithm. Instead,
    the minimum support parameter is set based on the data, the use case, and domain
    expertise.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 最小支持超参数不是可以通过网格搜索优化的值，因为 Apriori 算法没有评估度量标准。相反，最小支持参数是基于数据、用例和领域专业知识设置的。
- en: 'The main idea behind the Apriori algorithm is the Apriori principle: any subset
    of a frequent item set must itself be frequent.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Apriori算法背后的主要思想是Apriori原理：任何频繁物品集的子集必须本身也是频繁的。
- en: 'Another aspect worth mentioning is the corollary: no superset of an infrequent
    item set can be frequent.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得提及的方面是推论：不频繁物品集的任何超集都不能是频繁的。
- en: Let's take some examples. If the item set {hammer, saw, and nail} is frequent,
    then, according to the Apriori principle, and what is hopefully obvious, any less
    complex item set derived from it, say {hammer, saw}, is also frequent. On the
    contrary, if that same item set, {hammer, saw, nail}, is infrequent, then adding
    complexity, such as incorporating wood into the item set {hammer, saw, nail, wood},
    is not going to result in the item set becoming frequent.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一些例子。如果物品集{锤子、锯子和钉子}是频繁的，那么根据Apriori原理，并且这应该是显而易见的，从中派生的任何更简单的物品集，例如{锤子、锯子}，也是频繁的。相反，如果同样的物品集{锤子、锯子、钉子}是不频繁的，那么通过增加复杂性，如将木材加入物品集{锤子、锯子、钉子、木材}，是不会使该物品集变得频繁的。
- en: 'It might seem straightforward to calculate the support value for every item
    set in a transactional database and only return those item sets whose support
    is greater than or equal to the pre-specified minimum support threshold, but it
    is not because of the number of computations that need to happen. For example,
    take an item set with 10 unique items. This would result in 1,023 individual item
    sets for which support would need to be calculated. Now, try to extrapolate out
    to our working dataset that has 3,135 unique items. That is going to be an enormous
    number of item sets for which we need to compute a support value. Computational
    efficiency is a major issue:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 对于事务数据库中的每个物品集计算支持度值，并仅返回那些支持度大于或等于预设最小支持度阈值的物品集，似乎很简单，但由于所需的计算量，这并非易事。例如，假设有一个包含10个独特物品的物品集，这将导致1,023个单独的物品集需要计算支持度值。现在，试着推算我们的工作数据集，它包含3,135个独特物品。这将是一个庞大的物品集数量，我们需要为这些物品集计算支持度值。计算效率是一个重大问题：
- en: '![Figure 8.17: Representation of the computational efficiency issue'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.17：计算效率问题的表现'
- en: '](img/B15923_08_17.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_08_17.jpg)'
- en: 'Figure 8.17: Representation of the computational efficiency issue'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17：计算效率问题的表现
- en: The preceding diagram shows a mapping of how item sets are built and how the
    Apriori principle can greatly decrease the computational requirements (all the
    grayed-out nodes are infrequent).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图示展示了物品集的构建方式以及Apriori原理如何显著降低计算需求（所有灰色节点都是不频繁的）。
- en: 'In order to address the computational demands, the Apriori algorithm is defined
    as a bottom-up model that has two steps. These steps involve generating candidate
    item sets by adding items to already existing frequent item sets and testing these
    candidate item sets against the dataset to determine whether these candidate item
    sets are also frequent. No support value is computed for item sets that contain
    infrequent item sets. This process repeats until no further candidate item sets
    exist:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决计算需求，Apriori算法被定义为一个自底向上的模型，包含两个步骤。这些步骤涉及通过向已有的频繁物品集添加物品来生成候选物品集，并将这些候选物品集与数据集进行测试，以确定这些候选物品集是否也频繁。对于包含不频繁物品集的物品集，不计算支持度值。该过程会重复，直到没有更多的候选物品集为止：
- en: '![Figure 8.18: General Apriori algorithm structure'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.18：一般Apriori算法结构'
- en: '](img/B15923_08_18.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_08_18.jpg)'
- en: 'Figure 8.18: General Apriori algorithm structure'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.18：一般Apriori算法结构
- en: Assuming a minimum support threshold of 0.4, the preceding diagram shows the
    general Apriori algorithm structure.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 假设最低支持度阈值为0.4，前面的图示展示了Apriori算法的一般结构。
- en: '*Figure 8.20* includes establishing an item set, computing support values,
    filtering out infrequent item sets, creating new item sets, and repeating the
    process.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.20*包括建立物品集、计算支持度值、筛选掉不频繁的物品集、创建新物品集，并重复这一过程。'
- en: There is a clear tree-like structure that serves as the path for identifying
    candidate item sets. The specific search technique used, which was built for traversing
    tree-like data structures, is called a breadth-first search, which means that
    each step of the search process focuses on completely searching one level of the
    tree before moving on instead of searching branch by branch.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个明确的树状结构，作为识别候选物品集的路径。所使用的具体搜索技术是广度优先搜索，它是为遍历树状数据结构而构建的，这意味着搜索过程的每一步都集中在完全搜索树的一个层级之后再继续，而不是逐个分支地进行搜索。
- en: 'The high-level steps of the algorithm are designed to do the following:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的高层步骤旨在完成以下任务：
- en: Define the set of frequent items (in other words, choose only those items that
    have support greater than the pre-defined threshold). To start, this is typically
    the set of individual items.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义频繁项集（换句话说，只选择那些支持度大于预定义阈值的项）。通常，首先选择的是单个项集。
- en: Derive candidate item sets by combining frequent item sets. Move up in size
    one item at a time. That is, go from item sets with one item to two, two to three,
    and so on.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过组合频繁项集来推导候选项集。每次只增加一个项的大小。也就是说，从包含一个项的项集开始，逐步扩展到包含两个项、三个项，以此类推。
- en: Compute the support value for each candidate item set.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个候选项集的支持度。
- en: Create a new frequent item set made up of the candidate item sets whose support
    value exceeds the specified threshold.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的频繁项集，由支持度超过指定阈值的候选项集组成。
- en: Repeat *Steps 1* to *4* until there are no more frequent item sets; that is,
    until we have worked through all the combinations.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 重复*步骤 1* 到 *步骤 4*，直到没有更多的频繁项集为止；也就是说，直到我们处理完所有的组合。
- en: 'The pseudo code for the Apriori algorithm is as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: Apriori算法的伪代码如下：
- en: '[PRE35]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Despite the Apriori principle, this algorithm can still face significant computational
    challenges depending on the size of the transaction dataset. There are several
    strategies currently accepted to further reduce the computational demands.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管遵循Apriori原则，该算法仍可能面临显著的计算挑战，这取决于事务数据集的大小。目前有几种公认的策略可以进一步减少计算需求。
- en: Computational Fixes
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算修正
- en: Transaction reduction is an easy way to reduce the computational load. Note
    that after each candidate set of items is generated, the entirety of the transaction
    data needs to be scanned in order to count the number of appearances of each candidate
    item set. If we could shrink the size of the transaction dataset, the size of
    the dataset scan would decrease dramatically. The shrinking of the transaction
    dataset is done by realizing that any transaction containing no frequent item
    sets in the *i*th iteration is not going to contain any frequent item sets in
    subsequent iterations. Therefore, once each transaction contains no frequent item
    sets, it can be removed from the transaction dataset used for future scans.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 事务简化是一种减少计算负载的简单方法。注意，在生成每个候选项集之后，需要扫描整个事务数据集，以统计每个候选项集的出现次数。如果我们能缩小事务数据集的大小，数据集扫描的规模将大大减少。事务数据集的缩小是通过实现以下方式：任何在第*i*次迭代中不包含频繁项集的事务，在后续迭代中也不会包含任何频繁项集。因此，一旦每个事务不包含频繁项集，就可以从用于未来扫描的事务数据集中移除。
- en: Sampling the transaction dataset and testing each candidate item set against
    it is another approach to reducing the computational requirements associated with
    scanning the transaction dataset to calculate the support of each item set. When
    this approach is implemented, it is important to lower the minimum support requirement
    to guarantee that no item sets that should be present in the final data are left
    out. Given that the sampled transaction dataset will naturally cause the support
    values to be smaller, leaving the minimum support at its original value will incorrectly
    remove what should be frequent item sets from the output of the model.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 对事务数据集进行抽样，并在其上测试每个候选项集，是另一种减少计算需求的方法，这种方法可以避免扫描事务数据集来计算每个项集的支持度。在实现这种方法时，重要的是降低最小支持度要求，以确保不会遗漏任何应包含在最终数据中的项集。由于抽样的事务数据集会自然导致支持度值较小，因此将最小支持度保持在原始值可能会错误地删除那些应该是频繁项集的项集。
- en: A similar approach is partitioning. In this case, the dataset is randomly partitioned
    into several individual datasets on which the evaluation of each candidate item
    set is executed. Item sets are deemed frequent in the full transaction dataset
    if frequent in one of the partitions. Each partition is scanned consecutively
    until the frequency for an item set is established. Like sampling, partitioning
    is just another way to avoid testing each item set on the full dataset, which
    could be very computationally expensive if the full dataset is really big. If
    the frequency is established on the first partition, then we have established
    it for the whole dataset without testing it against a majority of the partitions.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 一种类似的方法是分区。在这种情况下，数据集被随机划分为多个单独的数据集，每个数据集上都会执行对每个候选项集的评估。如果一个项集在其中一个分区中是频繁的，那么它就被认为在完整事务数据集中是频繁的。每个分区依次扫描，直到确定一个项集的频率。如果频率在第一个分区中得以确定，那么我们就不需要在大多数分区上测试它，就可以为整个数据集确立该频率。像抽样一样，分区也是另一种避免在完整数据集上测试每个项集的方法，如果完整数据集非常大，测试会非常耗费计算资源。
- en: Regardless of whether or not one of these techniques is employed, the computational
    requirements are always going to be fairly substantial when it comes to the Apriori
    algorithm. As should now be clear, the essence of the algorithm, the computation
    of support, is not as complex as other models discussed in this text.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是否采用这些技术，Apriori 算法的计算需求通常都会相当庞大。如现在所见，算法的核心——支持度的计算——并不像本书中讨论的其他模型那样复杂。
- en: 'Exercise 8.06: Executing the Apriori Algorithm'
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 8.06：执行 Apriori 算法
- en: 'The execution of the Apriori algorithm is made easy with `mlxtend`. As a result,
    this exercise will focus on how to manipulate the output dataset and to interpret
    the results. You will recall that the cleaned and encoded transaction data was
    defined as `online_encoder_df`:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `mlxtend` 执行 Apriori 算法变得非常简单。因此，本练习将重点关注如何处理输出数据集并解读结果。你会记得，清理并编码后的事务数据被定义为
    `online_encoder_df`：
- en: Note
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Perform this exercise in the same notebook that all previous exercises were
    run in as we will continue using the environment, data, and results already established
    in that notebook. (So, you should be using the notebook that contains the reduced
    dataset of 5,000 invoices, not the full dataset as was used in the activity.)
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在与之前所有练习相同的笔记本中执行此练习，因为我们将继续使用该笔记本中已经建立的环境、数据和结果。（因此，你应该使用包含 5,000 张发票的简化数据集的笔记本，而不是在活动中使用的完整数据集。）
- en: 'Run the Apriori algorithm using `mlxtend` without changing any of the default
    parameter values:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `mlxtend` 运行 Apriori 算法，而不改变任何默认的参数值：
- en: '[PRE36]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The output is an empty DataFrame. The default minimum support value is set to
    0.5, so since an empty DataFrame was returned, we know that all item sets have
    a support value of less than 0.5\. Depending on the number of transactions and
    the diversity of available items, having no item set with a plus 0.5 support value
    is not unusual.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出是一个空的 DataFrame。默认的最小支持度值设置为 0.5，因此由于返回的是空的 DataFrame，我们知道所有项集的支持度都低于 0.5。根据事务的数量和可用项的多样性，没有任何项集具有超过
    0.5 的支持度值并不奇怪。
- en: 'Rerun the Apriori algorithm, but with the minimum support set to 0.01:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新运行 Apriori 算法，但将最小支持度设置为 0.01：
- en: '[PRE37]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This minimum support value is the same as saying that when analyzing 5,000 transactions,
    we need an item set to appear 50 times to be considered frequent. As mentioned
    previously, the minimum support can be set to any value in the range [0,1]. There
    is no best minimum support value; the setting of this value is entirely subjective.
    Many businesses have their own specific thresholds for significance, but there
    is no industry standard or method for optimizing this value.
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个最小支持度值相当于在分析 5,000 个事务时，需要一个项集出现 50 次才会被认为是频繁的。如前所述，最小支持度可以设置为 [0,1] 范围内的任何值。没有最佳的最小支持度值；该值的设置完全是主观的。许多企业有自己特定的显著性阈值，但没有行业标准或优化此值的方法。
- en: 'The output will be similar to the following:'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '![Figure 8.19: Basic output of the Apriori algorithm run using mlxtend'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.19：使用 mlxtend 运行的 Apriori 算法的基本输出'
- en: '](img/B15923_08_19.jpg)'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_19.jpg)'
- en: 'Figure 8.19: Basic output of the Apriori algorithm run using mlxtend'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.19：使用 mlxtend 运行的 Apriori 算法的基本输出
- en: Notice that the item sets are designated numerically in the output, which makes
    the results hard to interpret.
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，输出中的项集以数字形式标识，这使得结果难以解读。
- en: 'Rerun the Apriori algorithm with the same minimum support as in *Step 2*, but
    this time set `use_colnames` to `True`. This will replace the numerical designations
    with the actual item names:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新运行 Apriori 算法，使用与 *步骤 2* 中相同的最小支持度，但这次将 `use_colnames` 设置为 `True`。这将用实际的项名替换数值标识：
- en: '[PRE38]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output will be similar to the following:'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '![Figure 8.20: The output of the Apriori algorithm with the actual item names
    instead of numerical designations'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.20：Apriori 算法输出，实际项名代替了数值标识'
- en: '](img/B15923_08_20.jpg)'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_20.jpg)'
- en: 'Figure 8.20: The output of the Apriori algorithm with the actual item names
    instead of numerical designations'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.20：Apriori 算法输出，实际项名代替了数值标识
- en: This DataFrame contains every item set whose support value is greater than the
    specified minimum support value. That is, these item sets occur with sufficient
    frequency to potentially be meaningful and therefore actionable.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个 DataFrame 包含了所有支持度大于指定最小支持度值的项集。也就是说，这些项集出现的频率足够高，有可能是有意义的，因此是可操作的。
- en: 'Add an additional column to the output of *Step 3* that contains the size of
    the item set (in other words, how many items are in the set), which will help
    with filtering and further analysis:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *步骤 3* 的输出中添加一个额外的列，包含项集的大小（换句话说，就是该项集中有多少项），这将有助于筛选和进一步分析：
- en: '[PRE39]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output will be similar to the following:'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '![Figure 8.21: The Apriori algorithm output plus an additional column containing
    the lengths of the item sets'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.21：Apriori 算法输出加上一个额外的列，包含项集的长度'
- en: '](img/B15923_08_21.jpg)'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_21.jpg)'
- en: 'Figure 8.21: The Apriori algorithm output plus an additional column containing
    the lengths of the item sets'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.21：Apriori 算法输出加上一个额外的列，包含项集的长度
- en: 'Find the support of the item set containing `10 COLOUR SPACEBOY PEN`:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找包含 `10 COLOUR SPACEBOY PEN` 的项集的支持度：
- en: '[PRE40]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output is as follows:'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.22: The output DataFrame filtered down to a single item set'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.22：筛选至单个项集的输出 DataFrame'
- en: '](img/B15923_08_22.jpg)'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_22.jpg)'
- en: 'Figure 8.22: The output DataFrame filtered down to a single item set'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.22：筛选至单个项集的输出 DataFrame
- en: This single-row DataFrame gives us the support value for this specific item
    set that contains one item. The support value says that this specific item set
    appears in 1.78% of the transactions.
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个单行 DataFrame 给出了包含一个项的特定项集的支持度值。支持度值表示该特定项集出现在 1.78% 的交易中。
- en: 'Return all item sets of length 2 whose support is in the range [0.02, 0.021]:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回所有支持度在 [0.02, 0.021] 范围内，且长度为 2 的项集：
- en: '[PRE41]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output will be similar to the following:'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '![Figure 8.23: The Apriori algorithm output DataFrame filtered by length and
    support'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.23：通过长度和支持度筛选后的 Apriori 算法输出 DataFrame'
- en: '](img/B15923_08_23.jpg)'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_23.jpg)'
- en: 'Figure 8.23: The Apriori algorithm output DataFrame filtered by length and
    support'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.23：通过长度和支持度筛选后的 Apriori 算法输出 DataFrame
- en: This DataFrame contains all the item sets (pairs of items bought together) whose
    support value is in the range specified at the start of the step. Each of these
    item sets appears in between 2.0% and 2.1% of transactions.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个 DataFrame 包含了所有支持度值在步骤开始时指定的范围内的项集（即一对一起购买的商品）。每个项集出现在 2.0% 到 2.1% 的交易中。
- en: Note that when filtering on `support`, it is wise to specify a range instead
    of a specific value since it is quite possible to pick a value for which there
    are no item sets. The preceding output has 32 item sets; only a subset is shown.
    Keep note of the particular items in the item sets because we will be running
    this same filter when we scale up to the full data and we will want to execute
    a comparison.
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，当按 `support` 进行筛选时，最好指定一个范围，而不是一个具体值，因为很可能会选择一个没有项集的值。前面的输出中有 32 个项集；这里只显示了一个子集。记住这些项集中的具体项，因为我们将在扩展到完整数据时使用相同的筛选条件，并且需要执行对比。
- en: 'Plot the support values. Note that this plot will have no support values less
    than `0.01` because that was the value used as the minimum support in *Step 2*:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制支持度值图。注意，该图不会包含支持度小于 `0.01` 的数据，因为该值作为 *步骤 2* 中的最小支持度值：
- en: '[PRE42]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output will be similar to the following plot:'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将类似于以下图表：
- en: '![Figure 8.24: Distribution of the support values returned by the Apriori algorithm'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.24：Apriori 算法返回的支持度值分布'
- en: '](img/B15923_08_24.jpg)'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_24.jpg)'
- en: 'Figure 8.24: Distribution of the support values returned by the Apriori algorithm'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.24：Apriori 算法返回的支持度值分布
- en: The maximum support value is approximately 0.14, which is approximately 700
    transactions. What might appear to be a small value may not be, given the number
    of products available. Larger numbers of products tend to result in lower support
    values because the variability of item combinations increases.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 最大支持值大约为0.14，约为700次交易。看似较小的值可能并不小，因为涉及的产品数量较多。产品数量越大，支持值往往越低，因为商品组合的变动性增加。
- en: Note
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fhf9bS](https://packt.live/3fhf9bS).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/3fhf9bS](https://packt.live/3fhf9bS)。
- en: You can also run this example online at [https://packt.live/303vIBJ](https://packt.live/303vIBJ).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在线运行此示例，网址为[https://packt.live/303vIBJ](https://packt.live/303vIBJ)。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须执行整个笔记本才能获得期望的结果。
- en: Hopefully, you can think of more ways in which this data could be used. We will
    generate even more useful information in the next section by using the Apriori
    algorithm results to generate association rules.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您能想到更多使用这些数据的方法。在下一节中，我们将通过使用Apriori算法结果生成关联规则，产生更多有用的信息。
- en: 'Activity 8.02: Running the Apriori Algorithm on the Complete Online Retail
    Dataset'
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动8.02：在完整的在线零售数据集上运行Apriori算法
- en: Imagine you work for an online retailer. You are given all the transaction data
    from the last month and told to find all the item sets appearing in at least 1%
    of the transactions. Once the qualifying item sets are identified, you are subsequently
    told to identify the distribution of the support values. The distribution of support
    values will tell all interested parties whether groups of items exist that are
    purchased together with high probability as well as the mean of the support values.
    Let's collect all the information for the company's leadership and strategists.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您为一家在线零售商工作。您获得了上个月所有的交易数据，并被告知找出至少出现在1%交易中的商品集。一旦识别出符合条件的商品集，接下来您需要找出支持度值的分布情况。支持度值的分布将告知所有相关方，哪些商品组合是以较高概率共同购买的，并给出支持度值的均值。让我们收集所有这些信息，供公司领导层和战略人员参考。
- en: In this activity, you will run the Apriori algorithm on the full online retail
    dataset.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在此活动中，您将对完整的在线零售数据集运行Apriori算法。
- en: Note
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is sourced from [http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail#)
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science).'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来源于[http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail#)（UCI机器学习库
    [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]）。加利福尼亚大学尔湾分校信息与计算机科学学院。
- en: 'Citation: Daqing Chen, Sai Liang Sain, and Kun Guo, *Data mining for the online
    retail industry: A case study of RFM model-based customer segmentation using data
    mining*, Journal of Database Marketing and Customer Strategy Management, Vol.
    19, No. 3, pp. 197-208, 2012.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 引用：Daqing Chen, Sai Liang Sain, 和 Kun Guo, *面向在线零售行业的数据挖掘：基于RFM模型的客户细分案例研究*，《数据库营销与客户战略管理期刊》，第19卷，第3期，197-208页，2012年。
- en: It can be downloaded from [https://packt.live/39Nx3iQ](https://packt.live/39Nx3iQ).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以从[https://packt.live/39Nx3iQ](https://packt.live/39Nx3iQ)下载。
- en: Ensure that you complete this activity in the same notebook as the preceding
    activity (in other words, the notebook that uses the full dataset, not the notebook
    that uses the subset of 5,000 invoices that you're using for the exercises).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在与前一个活动相同的笔记本中完成此活动（换句话说，即使用完整数据集的笔记本，而不是使用5,000个发票子集的笔记本，后者是用于练习的）。
- en: This will also provide you with an opportunity to compare the results with those
    generated using only 5,000 transactions. This is an interesting activity, as it
    provides some insight into the ways in which the data may change as more data
    is collected, as well as some insight into how support values change when the
    partitioning technique is employed. Note that what was done in the exercises is
    not a perfect representation of the partitioning technique because 5,000 was an
    arbitrary number of transactions to sample.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这也为您提供了一个机会，将结果与仅使用5,000笔交易生成的结果进行比较。这是一个有趣的活动，因为它能提供一些关于随着更多数据收集，数据变化的方式，以及在使用分区技术时支持度值变化的见解。请注意，在练习中所做的工作并不是分区技术的完美代表，因为5,000笔交易是随机抽取的一个样本量。
- en: Note
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: All the activities in this chapter need to be performed in the same notebook.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有活动需要在同一个笔记本中进行。
- en: 'The following steps will help you to complete the activity:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成这个活动：
- en: Run the Apriori algorithm on the full data with reasonable parameter settings.
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在整个数据集上运行Apriori算法，并设置合理的参数。
- en: Filter the results down to the item set containing `10 COLOUR SPACEBOY PEN`.
    Compare the support value to that of *Exercise 8.06*, *Executing the Apriori Algorithm*.
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果筛选到包含`10 COLOUR SPACEBOY PEN`的项集。将支持度值与*练习8.06*，*执行Apriori算法*中的支持度值进行比较。
- en: Add another column containing the item set length. Then, filter down to those
    item sets whose length is two and whose support is in the range [0.02, 0.021].
    Compare this to the result from *Exercise 8.06*, *Executing the Apriori Algorithm*.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一列包含项集长度的列。然后筛选出那些长度为二且支持度在[0.02, 0.021]范围内的项集。将此结果与*练习8.06*，*执行Apriori算法*中的结果进行比较。
- en: Plot the `support` values.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制`支持度`值。
- en: Note
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 492.
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此活动的解决方案可以在第492页找到。
- en: 'The output of this activity will be similar to the following:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的输出结果将类似于以下内容：
- en: '![Figure 8.25: Distribution of support values'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.25：支持度值的分布](img/B15923_08_25.jpg)'
- en: '](img/B15923_08_25.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_08_25.jpg)'
- en: 'Figure 8.25: Distribution of support values'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.25：支持度值的分布
- en: Association Rules
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关联规则
- en: Association rule learning is a machine learning model that seeks to unearth
    the hidden patterns (in other words, relationships) in transaction data that describe
    the shopping habits of the customers of any retailer. The definition of an association
    rule was hinted at when the common probabilistic metrics were defined and explained
    earlier in the chapter.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 关联规则学习是一种机器学习模型，旨在发掘交易数据中的隐藏模式（换句话说，就是描述任何零售商顾客购物习惯的关系）。关联规则的定义在本章早些时候定义并解释的常见概率度量中有所暗示。
- en: 'Consider the imaginary frequent item set {Milk, Bread}. Two association rules
    can be formed from that item set: Milk ![Formula](img/B15923_08_Formula_06.png)
    Bread and Bread ![Formula](img/B15923_08_Formula_06.png) Milk. For simplicity,
    the first item set in the association rule is referred to as the antecedent, while
    the second item set in the association rule is referred to as the consequent.
    Once the association rules have been identified, all the previously discussed
    metrics can be computed to evaluate the validity of the association rules, determining
    whether or not the rules can be leveraged in the decision-making process.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个虚拟的频繁项集{牛奶，面包}。可以从该项集形成两个关联规则：牛奶 ![公式](img/B15923_08_Formula_06.png) 面包和面包
    ![公式](img/B15923_08_Formula_06.png) 牛奶。为了简便起见，关联规则中的第一个项集称为前提，而第二个项集称为后果。一旦识别出关联规则，就可以计算所有之前讨论的度量，以评估关联规则的有效性，进而决定这些规则是否能在决策过程中加以利用。
- en: The establishment of an association rule is based on support and confidence.
    Support, as we discussed in the last section, identifies which item sets are frequent,
    while confidence measures the frequency of truthfulness for a particular rule.
    Confidence is typically referred to as one of the measures of interestingness,
    as it is one of the metrics that determines whether an association should be formed.
    Thus, the establishment of an association rule is a two-step process. Identify
    frequent datasets and then evaluate the confidence of a candidate association
    rule and, if that confidence value exceeds some arbitrary threshold, the result
    is an association rule.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 关联规则的建立基于支持度和置信度。正如我们在上一节中讨论的，支持度用于识别哪些项集是频繁的，而置信度衡量特定规则的真实性频率。置信度通常被视为有趣性度量之一，因为它是决定是否形成关联规则的度量之一。因此，建立关联规则是一个两步过程。首先识别频繁的数据集，然后评估候选关联规则的置信度，如果该置信度值超过某个临界值，则结果为一个关联规则。
- en: A major issue of association rule learning is the discovery of spurious associations,
    which are highly likely given the huge numbers of potential rules. Spurious associations
    are defined as associations that occur with surprising regularity in the data,
    given that the association occurs entirely by chance. To clearly articulate the
    idea, assume we are in a situation where we have 100 candidate rules. If we run
    a statistical test for independence at the 0.05 significance level, we are still
    faced with a 5% chance that an association is found when no association exists.
    Let's further assume that all 100 candidate rules are not valid associations.
    Given the 5% chance, we should still expect to find 5 valid association rules.
    Now, scale the imaginary candidate rule list up to millions or billions, so that
    5% amounts to an enormous number of associations. This problem is not unlike the
    issue of statistical significance and error faced by virtually every model. It
    is worth calling out that some techniques exist to combat the spurious association
    issue, but they are neither consistently incorporated into the frequently used
    association rule libraries nor in the scope of this chapter.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 关联规则学习的一个主要问题是发现虚假关联，考虑到潜在规则的巨大数量，这些虚假关联很可能存在。所谓虚假关联是指在数据中出现惊人的频繁性，考虑到这种关联完全是偶然发生的。为了清楚地阐明这个观点，假设我们处于这样一种情况：我们有
    100 条候选规则。如果我们以 0.05 的显著性水平运行独立性统计测试，我们仍然面临 5% 的机会，即当不存在关联时也发现关联的可能性。再进一步假设这 100
    条候选规则都不是有效的关联。考虑到这 5% 的机会，我们仍然期望找到 5 条有效的关联规则。现在，将想象的候选规则列表扩展到数百万或数十亿，以便 5% 对应大量的关联。这个问题与几乎每个模型都面临的统计显著性和误差问题类似。值得注意的是，一些技术用于应对虚假关联问题，但它们既没有被一贯地纳入常用的关联规则库中，也不在本章的范围内。
- en: Let's now apply our working knowledge of association rule learning to the online
    retail dataset.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将我们对关联规则学习的工作知识应用到在线零售数据集中。
- en: 'Exercise 8.07: Deriving Association Rules'
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 8.07：推导关联规则
- en: In this exercise, we will derive association rules for the online retail dataset
    and explore the associated metrics.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将为在线零售数据集推导关联规则并探索相关指标。
- en: Note
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Ensure that you complete this exercise in the same notebook as the previous
    exercises (in other words, the notebook that uses the 5,000-invoice subset, not
    the full dataset from the activities).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您在与之前练习相同的笔记本中完成此练习（换句话说，使用了 5,000 发票子集的笔记本，而不是来自活动的完整数据集）。
- en: 'Use the `mlxtend` library to derive association rules for the online retail
    dataset. Use confidence as the measure of interestingness, set the minimum threshold
    to `0.6`, and return all the metrics, not just support:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `mlxtend` 库为在线零售数据集推导关联规则。将置信度作为趣味度量，将最小阈值设置为 `0.6`，并返回所有指标，而不仅仅是支持度：
- en: '[PRE43]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output is similar to the following:'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出类似如下：
- en: '![Figure 8.26: The first seven rows of the association rules generated using
    only 5,000 transactions'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.26：仅使用 5,000 笔交易生成的前七行关联规则'
- en: '](img/B15923_08_26.jpg)'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_26.jpg)'
- en: 'Figure 8.26: The first seven rows of the association rules generated using
    only 5,000 transactions'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.26：仅使用 5,000 笔交易生成的前七行关联规则
- en: 'Print the number of associations:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印关联数量：
- en: '[PRE44]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`1,064` association rules were found.'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 找到了 `1,064` 条关联规则。
- en: Note
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The number of association rules may differ.
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关联规则的数量可能有所不同。
- en: 'Try running another version of the model. Choose any minimum threshold and
    any measure of interestingness. Explore the returned rules:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试运行模型的另一个版本。选择任意最小阈值和任意的趣味度度量。探索返回的规则：
- en: '[PRE45]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output is as follows:'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.27: The first seven rows of the association rules'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.27：关联规则的前七行'
- en: '](img/B15923_08_27.jpg)'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_27.jpg)'
- en: 'Figure 8.27: The first seven rows of the association rules'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.27：关联规则的前七行
- en: 'Print the number of associations:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印关联数量：
- en: '[PRE46]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The number of association rules found using the lift metric and the minimum
    threshold value of `50` is `176`, which is significantly lower than in *Step 2*.
    We will see in a future step that `50` is quite a high threshold value, so it
    is not surprising that we returned fewer association rules.
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用提升度度量和最小阈值 `50` 找到的关联规则数量为 `176`，这比 *步骤 2* 中的数量显著低。我们将在后续步骤中看到 `50` 是相当高的阈值，因此返回的关联规则较少并不令人意外。
- en: 'Plot confidence against support and identify specific trends in the data:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制置信度与支持度的图表，并识别数据中的特定趋势：
- en: '[PRE47]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output is as follows:'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.28: A plot of confidence against support'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.28：置信度与支持度的关系图'
- en: '](img/B15923_08_28.jpg)'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_28.jpg)'
- en: 'Figure 8.28: A plot of confidence against support'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.28：置信度与支持度的关系图
- en: Notice that there are no association rules with both extremely high confidence
    and extremely high support. This should hopefully make sense. If an item set has
    high support, the items are likely to appear with many other items, making the
    chances of high confidence very low.
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，既具有极高置信度又具有极高支持度的关联规则并不存在。这应该是有道理的。如果一个项集有高支持度，那么这些项很可能会与许多其他项一起出现，这使得置信度很难达到很高。
- en: 'Look at the distribution of confidence:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看看置信度的分布：
- en: '[PRE48]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output is as follows:'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.29: The distribution of confidence values'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.29：置信度值的分布'
- en: '](img/B15923_08_29.jpg)'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_29.jpg)'
- en: 'Figure 8.29: The distribution of confidence values'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.29：置信度值的分布
- en: 'Now, look at the distribution of lift:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，看看提升度的分布：
- en: '[PRE49]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output is as follows:'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.30: The distribution of lift values'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.30：提升度值的分布'
- en: '](img/B15923_08_30.jpg)'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_30.jpg)'
- en: 'Figure 8.30: The distribution of lift values'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.30：提升度值的分布
- en: As mentioned previously, this plot shows that `50` is a high threshold value
    in that there are not many points above that value.
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前所述，这个图表显示`50`是一个较高的阈值，因为很少有数据点超过这个值。
- en: 'Now, look at the distribution of leverage:'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，看看杠杆的分布：
- en: '[PRE50]'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is as follows:'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.31: The distribution of leverage values'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.31：杠杆值的分布'
- en: '](img/B15923_08_31.jpg)'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_31.jpg)'
- en: 'Figure 8.31: The distribution of leverage values'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.31：杠杆值的分布
- en: 'Now, look at the distribution of conviction:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，看看信念度的分布：
- en: '[PRE51]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output is as follows:'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.32: The distribution of conviction values'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.32：信念度值的分布'
- en: '](img/B15923_08_32.jpg)'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_08_32.jpg)'
- en: 'Figure 8.32: The distribution of conviction values'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.32：信念度值的分布
- en: What is interesting about the four distributions is that spikes of varying sizes
    appear at the upper ends of the plots, implying that there are a few very strong
    association rules. The distribution of confidence tails off as the confidence
    values get larger, but at the very end, around the highest values, the distribution
    jumps up a little. The lift distribution has the most obvious spike. The conviction
    distribution plot shows a small spike, perhaps more accurately described as a
    bump, around 50\. Lastly, the leverage distribution does not really show any spike
    in the higher values, but it does feature a long tail with some very high leverage
    values.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，四个分布的图表顶部出现了不同大小的尖峰，这意味着存在一些非常强的关联规则。置信度分布随着置信值增大而逐渐减少，但在最右端，接近最大值时，分布会稍微上升。提升度分布有最明显的尖峰。信念度分布图在50左右显示出一个小尖峰，也许更准确地说是一个小突起。最后，杠杆分布在较高的值处并没有显示出尖峰，但它具有一个较长的尾部，包含一些非常高的杠杆值。
- en: Take some time to explore the association rules found by the model. Do the product
    pairings make sense to you? What happened to the number of association rules when
    you changed the model parameter values? Do you appreciate the impact that these
    rules would have when attempting to improve any retail business?
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 花些时间探索模型发现的关联规则。这些产品配对对你来说合理吗？当你改变模型参数值时，关联规则的数量发生了什么变化？你是否意识到这些规则在试图改善任何零售业务时可能产生的影响？
- en: In the preceding exercise, we built and plotted association rules. Association
    rules can be difficult to interpret and are heavily dependent on the thresholds
    and metrics used to create them. The questions in the preceding paragraph are
    meant to get you thinking creatively about how the algorithm works and how the
    rules can be used. Let's go through the questions one by one. There are obviously
    many rules, so the question regarding whether or not the rules make sense is hard
    to answer as a whole. Spot-checking the pairs seems to suggest that the rules
    are reasonable. For example, three of the pairings include children's cups and
    bowls, teacups and plates, and a playhouse kitchen and living room, all of which
    make sense. When the metrics and parameters changed, so did the results. As is
    the case in almost all modeling exercises, the ideal course of action is to look
    at the results under various circumstances and leverage all the findings to make
    the best decisions.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的练习中，我们构建并绘制了关联规则。关联规则可能很难解释，并且在很大程度上依赖于用于创建它们的阈值和度量标准。上一段中的问题旨在激发您思考算法如何工作以及如何使用规则。我们逐一讨论这些问题。显然有很多规则，所以关于这些规则是否合理的问题很难一概而论。对一些规则对的抽查似乎表明这些规则是合理的。例如，三组对包括儿童杯子和碗、茶杯和盘子、以及游乐屋厨房和客厅，这些都很有道理。当度量标准和参数发生变化时，结果也会发生变化。正如几乎所有建模练习中的情况一样，理想的做法是查看在不同情况下的结果，并利用所有发现做出最佳决策。
- en: Note
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fhf9bS](https://packt.live/3fhf9bS).
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/3fhf9bS](https://packt.live/3fhf9bS)。
- en: You can also run this example online at [https://packt.live/303vIBJ](https://packt.live/303vIBJ).
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在[https://packt.live/303vIBJ](https://packt.live/303vIBJ)在线运行此示例。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须执行整个笔记本才能获得所需的结果。
- en: 'Activity 8.03: Finding the Association Rules on the Complete Online Retail
    Dataset'
  id: totrans-427
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 8.03：在完整的在线零售数据集上查找关联规则
- en: Let's pick up the scenario set out in *Activity 8.02*, *Running the Apriori
    Algorithm on the Complete Online Retail Dataset*. The company leadership comes
    back to you and says it is great that we know how frequently each item set occurs
    in the dataset, but which item sets can we act upon? Which item sets can we use
    to change the store layout or adjust pricing? To find these answers, we derive
    the full association rules.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进行*活动 8.02*中设定的场景，*在完整的在线零售数据集上运行Apriori算法*。公司领导回来说，知道每个商品集在数据集中出现的频率非常好，但我们可以采取哪些商品集进行操作呢？哪些商品集可以用于改变商店布局或调整定价？为了找到这些答案，我们推导出完整的关联规则。
- en: In this activity, let's derive association rules from the complete online retail
    transaction dataset. Ensure that you complete this activity in the notebook that
    uses the full dataset (in other words, the notebook with the complete retail dataset,
    not the notebook from the exercises that use the 5,000-invoice subset).
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在此活动中，让我们从完整的在线零售交易数据集中推导关联规则。确保在使用完整数据集的笔记本中完成此活动（换句话说，要在包含完整零售数据集的笔记本中进行，而不是使用5,000个发票子集的练习笔记本）。
- en: 'These steps will help us to perform the activity:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助我们执行此活动：
- en: Fit the association rule model on the full dataset. Use the confidence metric
    and a minimum threshold of `0.6`.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在完整的数据集上拟合关联规则模型。使用置信度度量标准和`0.6`的最低阈值。
- en: Count the number of association rules. Is the number different from that found
    in *Step 1* of *Exercise 8.07*, *Deriving Association Rules*?
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 统计关联规则的数量。这个数量与*练习 8.07*的*步骤 1*中的数量是否不同？*推导关联规则*？
- en: Plot confidence against support.
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制置信度与支持度的关系图。
- en: Look at the distributions of confidence, lift, leverage, and conviction.
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看置信度、提升度、杠杆度和置信度的分布。
- en: 'Expected association rules output the following:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 预期的关联规则输出如下：
- en: '![Figure 8.33: Expected association rules'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.33：预期的关联规则'
- en: '](img/B15923_08_33.jpg)'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_08_33.jpg)'
- en: 'Figure 8.33: Expected association rules'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.33：预期的关联规则
- en: Note
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 496.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解决方案可在第496页找到。
- en: By the end of this activity, you will have plots of lift, leverage, and conviction.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 在此活动结束时，您将获得提升度、杠杆度和置信度的图表。
- en: Summary
  id: totrans-442
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Market basket analysis is used to analyze and extract insights from transaction
    or transaction-like data that can be used to help drive growth in many industries,
    most famously the retail industry. These decisions can include how to lay out
    the retail space, what products to discount, and how to price products. One of
    the central pillars of market basket analysis is the establishment of association
    rules. Association rule learning is a machine learning approach to uncovering
    the associations between the products individuals purchase that are strong enough
    to be leveraged for business decisions. Association rule learning relies on the
    Apriori algorithm to find frequent item sets in a computationally efficient way.
    These models are atypical of machine learning models because no prediction is
    being done, the results cannot really be evaluated using any one metric, and the
    parameter values are selected not by grid search, but by domain requirements specific
    to the question of interest. That being said, the goal of pattern extraction that
    is at the heart of all machine learning models is most definitely present here.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 市场篮子分析用于分析和提取交易或类似交易的数据洞察，这些洞察可以帮助推动许多行业的发展，最著名的就是零售行业。这些决策可能包括如何布局零售空间、哪些产品进行折扣、以及如何定价产品。市场篮子分析的核心支柱之一是关联规则的建立。关联规则学习是一种机器学习方法，用于揭示个体购买产品之间的关联，这些关联足够强大，可以用来做商业决策。关联规则学习依赖于Apriori算法，以计算高效的方式找到频繁项集。这些模型不同于典型的机器学习模型，因为它们不进行预测，结果也无法用任何单一指标进行评估，参数值不是通过网格搜索选择的，而是根据特定问题的领域需求来选择的。尽管如此，所有机器学习模型核心的模式提取目标在这里依然是存在的。
- en: At the conclusion of this chapter, you should feel comfortable evaluating and
    interpreting probabilistic metrics, be able to run and adjust the Apriori algorithm
    and association rule learning models using `mlxtend` and know how these models
    are applied in business. Know that there is a decent chance that the positioning
    and pricing of items in your neighborhood grocery store were chosen based on the
    past actions made by you and many other customers in that store!
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你应该能够舒适地评估和解读概率性指标，能够使用`mlxtend`运行和调整Apriori算法及关联规则学习模型，并了解这些模型在商业中的应用。你应该知道，你所在社区的杂货店内物品的定位和定价，可能是基于你和许多其他顾客过去的购买行为来做出的选择！
- en: In the next chapter, we will explore hotspot analysis using kernel density estimation,
    arguably one of the most frequently used algorithms in all of statistics and machine
    learning.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨使用核密度估计的热点分析，核密度估计无疑是所有统计学和机器学习中最常用的算法之一。
