- en: Chapter 3. Lazy Learning – Classification Using Nearest Neighbors
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章：懒惰学习——使用最近邻进行分类
- en: An interesting new type of dining experience has been appearing in cities around
    the world. Patrons are served in a completely darkened restaurant by waiters who
    move carefully around memorized routes using only their sense of touch and sound.
    The allure of these establishments is the belief that depriving oneself of visual
    sensory input will enhance the sense of taste and smell, and foods will be experienced
    in new ways. Each bite provides a sense of wonder while discovering the flavors
    the chef has prepared.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 一种有趣的新型用餐体验正在世界各地的城市中出现。顾客在完全黑暗的餐厅中，由记住路线并仅凭触觉和听觉在餐厅中小心走动的服务员进行服务。这些餐厅的吸引力在于，人们相信剥夺视觉感官输入将增强味觉和嗅觉，使食物以新的方式被体验。每一口都充满惊奇，同时发现厨师所准备的美味。
- en: 'Can you imagine how a diner experiences the unseen food? Upon first bite, the
    senses are overwhelmed. What are the dominant flavors? Does the food taste savory
    or sweet? Does it taste similar to something eaten previously? Personally, I imagine
    this process of discovery in terms of a slightly modified adage: if it smells
    like a duck and tastes like a duck, then you are probably eating duck.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你能想象一下食客是如何体验那些看不见的食物吗？第一次咬下去时，感官会被完全压倒。主导的味道是什么？食物是咸的还是甜的？味道是否像以前吃过的东西？就个人而言，我将这个发现的过程想象成一种略微修改过的谚语：如果闻起来像鸭子，吃起来也像鸭子，那你很可能是在吃鸭子。
- en: 'This illustrates an idea that can be used for machine learning—as does another
    maxim involving poultry: "birds of a feather flock together." Stated differently,
    things that are alike are likely to have properties that are alike. Machine learning
    uses this principle to classify data by placing it in the same category as similar
    or "nearest" neighbors. This chapter is devoted to the classifiers that use this
    approach. You will learn:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明了一个可以用于机器学习的思想——另一个涉及家禽的格言也有类似的启示：“物以类聚”。换句话说，相似的事物很可能具有相似的特征。机器学习运用这一原则，通过将数据与相似或“最近”的邻居放在同一类别中来进行分类。本章将专门讲解使用这种方法的分类器。你将学习：
- en: The key concepts that define **nearest neighbor** classifiers, and why they
    are considered "lazy" learners
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义**最近邻**分类器的关键概念，以及它们为何被认为是“懒惰”学习者
- en: Methods to measure the similarity of two examples using distance
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用距离来衡量两个示例相似度的方法
- en: To apply a popular nearest neighbor classifier called k-NN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用一种流行的最近邻分类器，称为k-NN
- en: If all these talks about food is making you hungry, feel free to grab a snack.
    Our first task will be to understand the k-NN approach by putting it to use by
    settling a long-running culinary debate.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有这些关于食物的谈话让你感到饿了，可以随时去吃点零食。我们的第一个任务是通过将k-NN方法应用于一个长期存在的烹饪争论来理解这一方法。
- en: Understanding nearest neighbor classification
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解最近邻分类
- en: 'In a single sentence, **nearest neighbor** classifiers are defined by their
    characteristic of classifying unlabeled examples by assigning them the class of
    similar labeled examples. Despite the simplicity of this idea, nearest neighbor
    methods are extremely powerful. They have been used successfully for:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 用一句话来说，**最近邻**分类器的定义是通过将未标记的示例分类为与之相似的已标记示例的类别。尽管这个想法简单，最近邻方法却非常强大。它们已被成功应用于：
- en: Computer vision applications, including optical character recognition and facial
    recognition in both still images and video
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉应用，包括光学字符识别和静态图像及视频中的人脸识别
- en: Predicting whether a person will enjoy a movie or music recommendation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测一个人是否会喜欢电影或音乐推荐
- en: Identifying patterns in genetic data, perhaps to use them in detecting specific
    proteins or diseases
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在遗传数据中识别模式，也许可以用于检测特定的蛋白质或疾病
- en: In general, nearest neighbor classifiers are well-suited for classification
    tasks, where relationships among the features and the target classes are numerous,
    complicated, or extremely difficult to understand, yet the items of similar class
    type tend to be fairly homogeneous. Another way of putting it would be to say
    that if a concept is difficult to define, but you know it when you see it, then
    nearest neighbors might be appropriate. On the other hand, if the data is noisy
    and thus no clear distinction exists among the groups, the nearest neighbor algorithms
    may struggle to identify the class boundaries.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，最近邻分类器非常适用于分类任务，其中特征与目标类别之间的关系众多、复杂或极其难以理解，但相似类别的项目往往是相当同质的。换句话说，如果一个概念很难定义，但你一看就知道它是什么，那么最近邻可能是合适的选择。另一方面，如果数据很嘈杂，因此各组之间没有明确的区分，最近邻算法可能会难以识别类别边界。
- en: The k-NN algorithm
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-NN算法
- en: The nearest neighbors approach to classification is exemplified by the **k-nearest
    neighbors algorithm** (**k-NN**). Although this is perhaps one of the simplest
    machine learning algorithms, it is still used widely.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻分类方法以**k近邻算法**（**k-NN**）为代表。尽管这是可能最简单的机器学习算法之一，但它仍然被广泛使用。
- en: 'The strengths and weaknesses of this algorithm are as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的优缺点如下：
- en: '| Strengths | Weaknesses |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 优点 | 缺点 |'
- en: '| --- | --- |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Simple and effective
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单有效
- en: Makes no assumptions about the underlying data distribution
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对底层数据分布不做假设
- en: Fast training phase
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练阶段较快
- en: '|'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Does not produce a model, limiting the ability to understand how the features
    are related to the class
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不产生模型，限制了对特征与类别关系的理解
- en: Requires selection of an appropriate *k*
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要选择合适的*k*
- en: Slow classification phase
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类阶段较慢
- en: Nominal features and missing data require additional processing
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 名义特征和缺失数据需要额外处理
- en: '|'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The k-NN algorithm gets its name from the fact that it uses information about
    an example's k-nearest neighbors to classify unlabeled examples. The letter *k*
    is a variable term implying that any number of nearest neighbors could be used.
    After choosing *k*, the algorithm requires a training dataset made up of examples
    that have been classified into several categories, as labeled by a nominal variable.
    Then, for each unlabeled record in the test dataset, k-NN identifies *k* records
    in the training data that are the "nearest" in similarity. The unlabeled test
    instance is assigned the class of the majority of the k nearest neighbors.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN算法得名于它使用一个示例的k个最近邻的信息来分类未标记的示例。字母*k*是一个变量，意味着可以使用任意数量的最近邻。选择*k*之后，算法需要一个训练数据集，数据集中包含已分类为若干类别的示例，并由名义变量标记。然后，对于测试数据集中每个未标记的记录，k-NN会在训练数据中识别出最“接近”的*k*个记录。未标记的测试实例将被分配给k个最近邻的大多数类别。
- en: 'To illustrate this process, let''s revisit the blind tasting experience described
    in the introduction. Suppose that prior to eating the mystery meal we had created
    a dataset in which we recorded our impressions of a number of ingredients we tasted
    previously. To keep things simple, we rated only two features of each ingredient.
    The first is a measure from 1 to 10 of how crunchy the ingredient is and the second
    is a 1 to 10 score of how sweet the ingredient tastes. We then labeled each ingredient
    as one of the three types of food: fruits, vegetables, or proteins. The first
    few rows of such a dataset might be structured as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个过程，让我们回顾一下引言中描述的盲品体验。假设在品尝神秘餐前，我们创建了一个数据集，其中记录了我们之前品尝过的若干食材的印象。为了简化起见，我们仅对每个食材的两个特征进行了评分。第一个是衡量食材脆度的1到10的评分，第二个是食材甜度的1到10的评分。然后，我们将每个食材标记为三种食品类型之一：水果、蔬菜或蛋白质。这样的数据集的前几行可能是如下结构：
- en: '| Ingredient | Sweetness | Crunchiness | Food type |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 成分 | 甜度 | 脆度 | 食品类型 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| apple | 10 | 9 | fruit |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 苹果 | 10 | 9 | 水果 |'
- en: '| bacon | 1 | 4 | protein |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 培根 | 1 | 4 | 蛋白质 |'
- en: '| banana | 10 | 1 | fruit |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 香蕉 | 10 | 1 | 水果 |'
- en: '| carrot | 7 | 10 | vegetable |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 胡萝卜 | 7 | 10 | 蔬菜 |'
- en: '| celery | 3 | 10 | vegetable |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 芹菜 | 3 | 10 | 蔬菜 |'
- en: '| cheese | 1 | 1 | protein |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 奶酪 | 1 | 1 | 蛋白质 |'
- en: 'The k-NN algorithm treats the features as coordinates in a multidimensional
    feature space. As our dataset includes only two features, the feature space is
    two-dimensional. We can plot two-dimensional data on a scatter plot, with the
    *x* dimension indicating the ingredient''s sweetness and the *y* dimension, the
    crunchiness. After adding a few more ingredients to the taste dataset, the scatter
    plot might look similar to this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN 算法将特征视为多维特征空间中的坐标。由于我们的数据集只包含两个特征，因此特征空间是二维的。我们可以在散点图上绘制二维数据，其中 *x* 维表示食材的甜度，*y*
    维表示脆度。在向味道数据集中添加更多食材后，散点图可能会像这样：
- en: '![The k-NN algorithm](img/3905_03_01.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![k-NN 算法](img/3905_03_01.jpg)'
- en: 'Did you notice the pattern? Similar types of food tend to be grouped closely
    together. As illustrated in the next diagram, vegetables tend to be crunchy but
    not sweet, fruits tend to be sweet and either crunchy or not crunchy, while proteins
    tend to be neither crunchy nor sweet:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到这个模式了吗？相似类型的食物往往被聚集在一起。正如下图所示，蔬菜往往脆但不甜，水果通常是甜的，并且可能脆也可能不脆，而蛋白质则通常既不脆也不甜：
- en: '![The k-NN algorithm](img/3905_03_02.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![k-NN 算法](img/3905_03_02.jpg)'
- en: 'Suppose that after constructing this dataset, we decide to use it to settle
    the age-old question: is tomato a fruit or vegetable? We can use the nearest neighbor
    approach to determine which class is a better fit, as shown in the following diagram:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在构建完这个数据集后，我们决定用它来解决那个长期悬而未决的问题：番茄是水果还是蔬菜？我们可以使用最近邻方法来确定哪个类别更合适，如下图所示：
- en: '![The k-NN algorithm](img/3905_03_03.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![k-NN 算法](img/3905_03_03.jpg)'
- en: Measuring similarity with distance
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用距离度量相似性
- en: Locating the tomato's nearest neighbors requires a **distance function**, or
    a formula that measures the similarity between the two instances.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 找到番茄的最近邻需要一个**距离函数**，即一个衡量两个实例相似性的公式。
- en: There are many different ways to calculate distance. Traditionally, the k-NN
    algorithm uses **Euclidean distance**, which is the distance one would measure
    if it were possible to use a ruler to connect two points, illustrated in the previous
    figure by the dotted lines connecting the tomato to its neighbors.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 计算距离有许多不同的方法。传统上，k-NN 算法使用**欧几里得距离**，这是一种如果可以用尺子连接两点时所测量的距离，如前图中虚线连接番茄和邻居所示。
- en: Tip
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Euclidean distance is measured "as the crow flies," implying the shortest direct
    route. Another common distance measure is Manhattan distance, which is based on
    the paths a pedestrian would take by walking city blocks. If you are interested
    in learning more about other distance measures, you can read the documentation
    for R's distance function (a useful tool in its own right), using the `?dist`
    command.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离是“按直线飞行”的方式来度量的，意味着最短的直接路径。另一种常见的距离度量是曼哈顿距离，它是基于行人走过城市街区的路径。如果你想了解更多其他的距离度量，可以阅读
    R 的距离函数文档（这个工具本身也非常有用），使用`?dist`命令。
- en: 'Euclidean distance is specified by the following formula, where *p* and *q*
    are the examples to be compared, each having *n* features. The term *p[1]* refers
    to the value of the first feature of example *p*, while *q[1]* refers to the value
    of the first feature of example *q*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离由以下公式指定，其中 *p* 和 *q* 是要比较的示例，每个示例有 *n* 个特征。术语 *p[1]* 指示示例 *p* 的第一个特征的值，而
    *q[1]* 指示示例 *q* 的第一个特征的值：
- en: '![Measuring similarity with distance](img/3905_03_04.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![用距离度量相似性](img/3905_03_04.jpg)'
- en: 'The distance formula involves comparing the values of each feature. For example,
    to calculate the distance between the tomato (*sweetness = 6*, *crunchiness =
    4*), and the green bean (*sweetness = 3*, *crunchiness = 7*), we can use the formula
    as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 距离公式涉及比较每个特征的值。例如，要计算番茄（*甜度 = 6*，*脆度 = 4*）与青豆（*甜度 = 3*，*脆度 = 7*）之间的距离，我们可以使用如下公式：
- en: '![Measuring similarity with distance](img/3905_03_05.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![用距离度量相似性](img/3905_03_05.jpg)'
- en: 'In a similar vein, we can calculate the distance between the tomato and several
    of its closest neighbors as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以计算番茄与其几个最接近邻居之间的距离，如下所示：
- en: '| Ingredient | Sweetness | Crunchiness | Food type | Distance to the tomato
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 食材 | 甜度 | 脆度 | 食物类型 | 到番茄的距离 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| grape | 8 | 5 | fruit | *sqrt((6 - 8)^2 + (4 - 5)^2) = 2.2* |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 葡萄 | 8 | 5 | 水果 | *sqrt((6 - 8)^2 + (4 - 5)^2) = 2.2* |'
- en: '| green bean | 3 | 7 | vegetable | *sqrt((6 - 3)^2 + (4 - 7)^2) = 4.2* |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 青豆 | 3 | 7 | 蔬菜 | *sqrt((6 - 3)^2 + (4 - 7)^2) = 4.2* |'
- en: '| nuts | 3 | 6 | protein | *sqrt((6 - 3)^2 + (4 - 6)^2) = 3.6* |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 坚果 | 3 | 6 | 蛋白质 | *sqrt((6 - 3)^2 + (4 - 6)^2) = 3.6* |'
- en: '| orange | 7 | 3 | fruit | *sqrt((6 - 7)^2 + (4 - 3)^2) = 1.4* |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 橙子 | 7 | 3 | 水果 | *sqrt((6 - 7)^2 + (4 - 3)^2) = 1.4* |'
- en: To classify the tomato as a vegetable, protein, or fruit, we'll begin by assigning
    the tomato, the food type of its single nearest neighbor. This is called 1-NN
    classification because *k = 1*. The orange is the nearest neighbor to the tomato,
    with a distance of 1.4\. As orange is a fruit, the 1-NN algorithm would classify
    tomato as a fruit.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将番茄分类为蔬菜、蛋白质或水果，我们将首先根据番茄最近的单一邻居来分配其食物类型。这称为1-NN分类，因为*k = 1*。橙子是番茄的最近邻，距离为1.4。由于橙子是水果，因此1-NN算法会将番茄分类为水果。
- en: 'If we use the k-NN algorithm with *k = 3* instead, it performs a vote among
    the three nearest neighbors: orange, grape, and nuts. Since the majority class
    among these neighbors is fruit (two of the three votes), the tomato again is classified
    as a fruit.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们改用*k = 3*的k-NN算法，它会在三个最近邻居之间进行投票：橙子、葡萄和坚果。由于这些邻居中多数类为水果（这三票中的两票），因此番茄再次被分类为水果。
- en: Choosing an appropriate k
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择合适的k
- en: The decision of how many neighbors to use for k-NN determines how well the model
    will generalize to future data. The balance between overfitting and underfitting
    the training data is a problem known as **bias-variance tradeoff**. Choosing a
    large *k* reduces the impact or variance caused by noisy data, but can bias the
    learner so that it runs the risk of ignoring small, but important patterns.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 选择用于k-NN的邻居数量决定了模型对未来数据的泛化能力。训练数据的过拟合与欠拟合之间的平衡问题被称为**偏差-方差权衡**。选择较大的*k*值可以减少噪声数据带来的影响或方差，但也可能使学习器产生偏差，从而忽略一些微小但重要的模式。
- en: Suppose we took the extreme stance of setting a very large *k*, as large as
    the total number of observations in the training data. With every training instance
    represented in the final vote, the most common class always has a majority of
    the voters. The model would consequently always predict the majority class, regardless
    of the nearest neighbors.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们采取极端的做法，将*k*设定为非常大，甚至与训练数据中的观测总数相同。由于每个训练实例都被包含在最终投票中，最常见的类别总是会占多数选票。因此，模型将始终预测多数类，而不考虑最近邻居的情况。
- en: On the opposite extreme, using a single nearest neighbor allows the noisy data
    or outliers to unduly influence the classification of examples. For example, suppose
    some of the training examples were accidentally mislabeled. Any unlabeled example
    that happens to be nearest to the incorrectly labeled neighbor will be predicted
    to have the incorrect class, even if nine other nearest neighbors would have voted
    differently.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一种极端情况下，使用单一最近邻会让噪声数据或异常值不当地影响示例的分类。例如，假设一些训练样本被错误地标记。如果任何未标记的示例恰好是错误标记邻居的最近邻，它将被预测为错误的类别，即使其他九个最近邻可能会投不同的票。
- en: Obviously, the best *k* value is somewhere between these two extremes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，最佳的*k*值应该介于这两种极端之间。
- en: The following figure illustrates, more generally, how the decision boundary
    (depicted by a dashed line) is affected by larger or smaller *k* values. Smaller
    values allow more complex decision boundaries that more carefully fit the training
    data. The problem is that we do not know whether the straight boundary or the
    curved boundary better represents the true underlying concept to be learned.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下图更普遍地展示了决策边界（由虚线表示）如何受到较大或较小*k*值的影响。较小的*k*值允许更复杂的决策边界，这些边界能够更精细地拟合训练数据。问题在于，我们并不知道直线边界还是曲线边界更能准确地表示要学习的真实底层概念。
- en: '![Choosing an appropriate k](img/3905_03_06.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![选择合适的k](img/3905_03_06.jpg)'
- en: In practice, choosing *k* depends on the difficulty of the concept to be learned,
    and the number of records in the training data. One common practice is to begin
    with *k* equal to the square root of the number of training examples. In the food
    classifier we developed previously, we might set *k = 4* because there were 15
    example ingredients in the training data and the square root of 15 is 3.87.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，选择*k*的大小取决于待学习概念的难度以及训练数据中的记录数。一种常见的做法是将*k*设为训练样本数量的平方根。在我们之前开发的食物分类器中，可能将*k
    = 4*，因为训练数据中有15个示例食材，而15的平方根是3.87。
- en: However, such rules may not always result in the single best *k*. An alternative
    approach is to test several *k* values on a variety of test datasets and choose
    the one that delivers the best classification performance. That said, unless the
    data is very noisy, a large training dataset can make the choice of *k* less important.
    This is because even subtle concepts will have a sufficiently large pool of examples
    to vote as nearest neighbors.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些规则并不总是能得出最佳的单一*k*值。另一种方法是对多个*k*值进行测试，并选择在各种测试数据集上表现最佳的那个。也就是说，除非数据非常嘈杂，否则一个大的训练数据集可以使得*k*的选择变得不那么重要。因为即使是细微的概念，也会有足够多的示例来作为最近邻进行投票。
- en: Tip
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: A less common, but interesting solution to this problem is to choose a larger
    *k*, but apply a **weighted voting** process in which the vote of the closer neighbors
    is considered more authoritative than the vote of the far away neighbors. Many
    k-NN implementations offer this option.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一种不太常见，但有趣的解决方案是选择一个较大的*k*值，但采用**加权投票**过程，其中较近邻居的投票比远离邻居的投票更具权威性。许多k-NN实现提供了这个选项。
- en: Preparing data for use with k-NN
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为k-NN准备数据
- en: Features are typically transformed to a standard range prior to applying the
    k-NN algorithm. The rationale for this step is that the distance formula is highly
    dependent on how features are measured. In particular, if certain features have
    a much larger range of values than the others, the distance measurements will
    be strongly dominated by the features with larger ranges. This wasn't a problem
    for food tasting example as both sweetness and crunchiness were measured on a
    scale from 1 to 10.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用k-NN算法之前，特征通常会被转换为标准范围。这样做的理由是距离公式在很大程度上依赖于特征的度量方式。特别是，如果某些特征的值范围远大于其他特征，那么距离度量将会被那些具有更大范围的特征主导。对于食品品尝示例而言，这不是问题，因为甜度和脆度都是在1到10的范围内测量的。
- en: However, suppose we added an additional feature to the dataset for a food's
    spiciness, which was measured using the Scoville scale. If you are not familiar
    with this metric, it is a standardized measure of spice heat, ranging from zero
    (not at all spicy) to over a million (for the hottest chili peppers). Since the
    difference between spicy and non-spicy foods can be over a million, while the
    difference between sweet and non-sweet or crunchy and non-crunchy foods is at
    most 10, the difference in scale allows the spice level to impact the distance
    function much more than the other two factors. Without adjusting our data, we
    might find that our distance measures only differentiate foods by their spiciness;
    the impact of crunchiness and sweetness would be dwarfed by the contribution of
    spiciness.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，假设我们为数据集添加了一个额外的特征，用于表示食物的辣味，这个辣味是使用斯科维尔指数来衡量的。如果你不熟悉这个度量，它是一个标准化的辣味热度衡量标准，范围从零（完全不辣）到超过一百万（最辣的辣椒）。由于辣味与非辣味食物之间的差异可以超过一百万，而甜味与非甜味或脆度与非脆度食物之间的差异最多只有10，尺度上的差异导致辣味的水平对距离函数的影响远大于另外两个因素。如果不调整数据，我们可能会发现距离度量仅仅通过辣味来区分食物；脆度和甜度的影响会被辣味的贡献所掩盖。
- en: The solution is to rescale the features by shrinking or expanding their range
    such that each one contributes relatively equally to the distance formula. For
    example, if sweetness and crunchiness are both measured on a scale from 1 to 10,
    we would also like spiciness to be measured on a scale from 1 to 10\. There are
    several common ways to accomplish such scaling.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是通过缩小或扩展特征的范围来重新调整特征的尺度，使得每个特征对距离公式的贡献相对均衡。例如，如果甜度和脆度都在1到10的范围内测量，我们也希望辣味能在1到10的范围内测量。实现这种缩放有几种常见的方法。
- en: 'The traditional method of rescaling features for k-NN is **min-max** **normalization**.
    This process transforms a feature such that all of its values fall in a range
    between 0 and 1\. The formula for normalizing a feature is as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN中调整特征尺度的传统方法是**最小-最大** **归一化**。这个过程将一个特征转换为其所有值都落在0到1的范围内。归一化特征的公式如下：
- en: '![Preparing data for use with k-NN](img/3905_03_07.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![为k-NN准备数据](img/3905_03_07.jpg)'
- en: Essentially, the formula subtracts the minimum of feature *X* from each value
    and divides by the range of *X*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，该公式从每个值中减去特征*X*的最小值，并除以*X*的范围。
- en: Normalized feature values can be interpreted as indicating how far, from 0 percent
    to 100 percent, the original value fell along the range between the original minimum
    and maximum.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化特征值可以解释为指示原始值在原始最小值和最大值之间的范围内，距离0%到100%的位置。
- en: 'Another common transformation is called **z-score standardization**. The following
    formula subtracts the mean value of feature *X*, and divides the outcome by the
    standard deviation of *X*:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的转换方法叫做**z-score标准化**。以下公式减去特征*X*的均值，并将结果除以*X*的标准差：
- en: '![Preparing data for use with k-NN](img/3905_03_08.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![准备数据以供k-NN使用](img/3905_03_08.jpg)'
- en: This formula, which is based on the properties of the normal distribution covered
    in [Chapter 2](ch02.html "Chapter 2. Managing and Understanding Data"), *Managing
    and Understanding Data*, rescales each of the feature's values in terms of how
    many standard deviations they fall above or below the mean value. The resulting
    value is called a **z-score**. The z-scores fall in an unbound range of negative
    and positive numbers. Unlike the normalized values, they have no predefined minimum
    and maximum.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式基于[第2章](ch02.html "第2章. 管理与理解数据")中讲解的正态分布的特性，*管理与理解数据*，将每个特征的值重新缩放为它们相对于均值的标准差数量。结果值称为**z-score**。z-score值在负数和正数的无限范围内波动。与标准化值不同，它们没有预定义的最小值和最大值。
- en: Tip
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The same rescaling method used on the k-NN training dataset must also be applied
    to the examples the algorithm will later classify. This can lead to a tricky situation
    for min-max normalization, as the minimum or maximum of future cases might be
    outside the range of values observed in the training data. If you know the plausible
    minimum or maximum value ahead of time, you can use these constants rather than
    the observed values. Alternatively, you can use z-score standardization under
    the assumption that the future examples will have similar mean and standard deviation
    as the training examples.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 用于k-NN训练数据集的相同重缩放方法也必须应用于算法后续分类的示例。这可能导致最小-最大标准化的棘手情况，因为未来案例的最小值或最大值可能超出了训练数据中观察到的值范围。如果您提前知道合理的最小值或最大值，可以使用这些常数，而不是观察到的值。或者，您可以使用z-score标准化，假设未来的示例将具有与训练示例相似的均值和标准差。
- en: 'The Euclidean distance formula is not defined for nominal data. Therefore,
    to calculate the distance between nominal features, we need to convert them into
    a numeric format. A typical solution utilizes **dummy coding**, where a value
    of *1* indicates one category, and *0*, the other. For instance, dummy coding
    for a gender variable could be constructed as:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离公式对名义数据没有定义。因此，为了计算名义特征之间的距离，我们需要将其转换为数值格式。一种典型的解决方案是使用**虚拟编码**，其中*1*表示一个类别，*0*表示另一个类别。例如，性别变量的虚拟编码可以构建如下：
- en: '![Preparing data for use with k-NN](img/3905_03_09.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![准备数据以供k-NN使用](img/3905_03_09.jpg)'
- en: Notice how the dummy coding of the two-category (binary) gender variable results
    in a single new feature named male. There is no need to construct a separate feature
    for female; since the two sexes are mutually exclusive, knowing one or the other
    is enough.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，二类（即二进制）性别变量的虚拟编码结果是一个名为male的新特征。无需为female构建单独的特征；由于两性是互斥的，知道其中之一就足够了。
- en: 'This is true more generally as well. An *n*-category nominal feature can be
    dummy coded by creating the binary indicator variables for (*n - 1*) levels of
    the feature. For example, the dummy coding for a three-category temperature variable
    (for example, hot, medium, or cold) could be set up as *(3 - 1) = 2* features,
    as shown here:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 更普遍地说，*n*类名义特征可以通过为特征的(*n - 1*)个水平创建二进制指示变量来进行虚拟编码。例如，对于一个三类温度变量（例如，热、中等或冷），可以将虚拟编码设置为*(3
    - 1) = 2*个特征，如下所示：
- en: '![Preparing data for use with k-NN](img/3905_03_10.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![准备数据以供k-NN使用](img/3905_03_10.jpg)'
- en: Knowing that hot and medium are both *0* is enough to know that the temperature
    is cold. We, therefore, do not need a third feature for the cold category.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 知道热和中等的值都是*0*就足够知道温度是冷的。因此，我们不需要为冷类构建第三个特征。
- en: A convenient aspect of dummy coding is that the distance between dummy coded
    features is always one or zero, and thus, the values fall on the same scale as
    min-max normalized numeric data. No additional transformation is necessary.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟编码的一个方便之处在于，虚拟编码特征之间的距离始终为1或0，因此，这些值与最小-最大标准化的数值数据处于相同的尺度上。不需要额外的转换。
- en: Tip
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If a nominal feature is ordinal (one could make such an argument for temperature),
    an alternative to dummy coding is to number the categories and apply normalization.
    For instance, cold, warm, and hot could be numbered as 1, 2, and 3, which normalizes
    to 0, 0.5, and 1\. A caveat to this approach is that it should only be used if
    the steps between the categories are equivalent. For instance, although income
    categories for poor, middle class, and wealthy are ordered, the difference between
    the poor and middle class may be different than the difference between the middle
    class and wealthy. Since the steps between groups are not equal, dummy coding
    is a safer approach.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个名义特征是有序的（例如温度可以这样认为），那么除了虚拟编码之外，还可以通过对类别编号并应用归一化来实现。例如，冷、温暖和热可以被编号为 1、2
    和 3，然后归一化为 0、0.5 和 1。需要注意的是，这种方法只应在类别之间的差距是等价时使用。例如，尽管贫穷、中产阶级和富裕的收入类别是有序的，但贫穷与中产阶级之间的差距可能与中产阶级与富裕之间的差距不同。由于各组之间的差距不等，虚拟编码是更安全的方法。
- en: Why is the k-NN algorithm lazy?
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么 k-NN 算法是懒惰的？
- en: Classification algorithms based on the nearest neighbor methods are considered
    **lazy learning** algorithms because, technically speaking, no abstraction occurs.
    The abstraction and generalization processes are skipped altogether, and this
    undermines the definition of learning, proposed in [Chapter 1](ch01.html "Chapter 1. Introducing
    Machine Learning"), *Introducing Machine Learning*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 基于最近邻方法的分类算法被认为是**懒惰学习**算法，因为从技术上讲，没有发生抽象化。抽象和泛化过程完全跳过了，这削弱了[第 1 章](ch01.html
    "第 1 章：机器学习介绍")中提出的学习定义，*机器学习介绍*。
- en: Under the strict definition of learning, a lazy learner is not really learning
    anything. Instead, it merely stores the training data verbatim. This allows the
    training phase, which is not actually training anything, to occur very rapidly.
    Of course, the downside is that the process of making predictions tends to be
    relatively slow in comparison to training. Due to the heavy reliance on the training
    instances rather than an abstracted model, lazy learning is also known as **instance-based
    learning** or **rote learning**.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在严格的学习定义下，懒惰学习者实际上并没有学到任何东西。相反，它只是逐字地存储训练数据。这使得训练阶段（实际上并没有训练任何东西）可以非常快速地完成。当然，缺点是，与训练相比，做出预测的过程通常较慢。由于高度依赖于训练实例而非抽象模型，懒惰学习也被称为**基于实例的学习**或**死记硬背学习**。
- en: As instance-based learners do not build a model, the method is said to be in
    a class of **non-parametric** learning methods—no parameters are learned about
    the data. Without generating theories about the underlying data, non-parametric
    methods limit our ability to understand how the classifier is using the data.
    On the other hand, this allows the learner to find natural patterns rather than
    trying to fit the data into a preconceived and potentially biased functional form.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基于实例的学习者不建立模型，因此这种方法被归类为**非参数**学习方法——即没有关于数据的任何参数被学习。由于不生成关于底层数据的理论，非参数方法限制了我们理解分类器如何使用数据的能力。另一方面，这使得学习者能够发现自然的模式，而不是试图将数据拟合到一个预设的且可能存在偏差的功能形式中。
- en: '![Why is the k-NN algorithm lazy?](img/3905_03_11.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![为什么 k-NN 算法是懒惰的？](img/3905_03_11.jpg)'
- en: Although k-NN classifiers may be considered lazy, they are still quite powerful.
    As you will soon see, the simple principles of nearest neighbor learning can be
    used to automate the process of screening for cancer.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 k-NN 分类器可能被认为是懒惰的，但它们仍然非常强大。正如你很快会看到的，最近邻学习的简单原理可以用于自动化癌症筛查过程。
- en: Example – diagnosing breast cancer with the k-NN algorithm
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 – 使用 k-NN 算法诊断乳腺癌
- en: Routine breast cancer screening allows the disease to be diagnosed and treated
    prior to it causing noticeable symptoms. The process of early detection involves
    examining the breast tissue for abnormal lumps or masses. If a lump is found,
    a fine-needle aspiration biopsy is performed, which uses a hollow needle to extract
    a small sample of cells from the mass. A clinician then examines the cells under
    a microscope to determine whether the mass is likely to be malignant or benign.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 常规的乳腺癌筛查可以在疾病引发明显症状之前就诊断并治疗。早期检测过程涉及检查乳腺组织是否有异常的肿块或肿块。如果发现肿块，会进行细针穿刺活检，使用一个中空的针头从肿块中提取少量细胞样本。然后，临床医生在显微镜下检查细胞，以确定肿块是良性还是恶性。
- en: If machine learning could automate the identification of cancerous cells, it
    would provide considerable benefit to the health system. Automated processes are
    likely to improve the efficiency of the detection process, allowing physicians
    to spend less time diagnosing and more time treating the disease. An automated
    screening system might also provide greater detection accuracy by removing the
    inherently subjective human component from the process.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果机器学习能够自动化识别癌细胞，将为健康系统带来显著益处。自动化过程可能会提高检测效率，使医生能够花更少的时间诊断、更多的时间治疗疾病。自动筛查系统还可能通过去除过程中的人为主观成分，提高检测的准确性。
- en: We will investigate the utility of machine learning for detecting cancer by
    applying the k-NN algorithm to measurements of biopsied cells from women with
    abnormal breast masses.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过应用 k-NN 算法，对来自患有异常乳腺肿块的女性的活检细胞测量数据进行研究，以探讨机器学习在癌症检测中的应用。
- en: Step 1 – collecting data
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 1 步 – 收集数据
- en: We will utilize the Wisconsin Breast Cancer Diagnostic dataset from the UCI
    Machine Learning Repository at [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml).
    This data was donated by researchers of the University of Wisconsin and includes
    the measurements from digitized images of fine-needle aspirate of a breast mass.
    The values represent the characteristics of the cell nuclei present in the digital
    image.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用来自 UCI 机器学习库的威斯康星乳腺癌诊断数据集，网址为 [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)。该数据由威斯康星大学的研究人员捐赠，包含来自乳腺肿块的细针抽吸数字化图像的测量数据。这些数值代表了数字图像中细胞核的特征。
- en: Note
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'To read more about this dataset, refer to: Mangasarian OL, Street WN, Wolberg
    WH. Breast cancer diagnosis and prognosis via linear programming. *Operations
    Research*. 1995; 43:570-577.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解更多关于此数据集的信息，请参考：Mangasarian OL, Street WN, Wolberg WH. Breast cancer diagnosis
    and prognosis via linear programming. *Operations Research*. 1995; 43:570-577.
- en: The breast cancer data includes 569 examples of cancer biopsies, each with 32
    features. One feature is an identification number, another is the cancer diagnosis,
    and 30 are numeric-valued laboratory measurements. The diagnosis is coded as `"M"`
    to indicate malignant or `"B"` to indicate benign.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺癌数据集包括 569 个癌症活检样本，每个样本具有 32 个特征。其中一个特征是标识号，另一个是癌症诊断结果，30 个是数值型实验室测量值。诊断结果用
    `"M"` 表示恶性，`"B"` 表示良性。
- en: 'The other 30 numeric measurements comprise the mean, standard error, and worst
    (that is, largest) value for 10 different characteristics of the digitized cell
    nuclei. These include:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 另外的 30 个数值型测量包含了 10 个不同细胞核特征的均值、标准误差和最差（即最大）值。这些特征包括：
- en: Radius
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半径
- en: Texture
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纹理
- en: Perimeter
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周长
- en: Area
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面积
- en: Smoothness
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平滑度
- en: Compactness
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 紧凑度
- en: Concavity
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凹陷度
- en: Concave points
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凹点
- en: Symmetry
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对称性
- en: Fractal dimension
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分形维度
- en: Based on these names, all the features seem to relate to the shape and size
    of the cell nuclei. Unless you are an oncologist, you are unlikely to know how
    each relates to benign or malignant masses. These patterns will be revealed as
    we continue in the machine learning process.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些名称，所有特征似乎都与细胞核的形状和大小有关。除非你是肿瘤学家，否则你不太可能知道每个特征如何与良性或恶性肿块相关。这些模式将在我们继续进行机器学习过程时揭示出来。
- en: Step 2 – exploring and preparing the data
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 2 步 – 探索和准备数据
- en: Let's explore the data and see whether we can shine some light on the relationships.
    In doing so, we will prepare the data for use with the k-NN learning method.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索数据，看看是否能揭示一些关系。在此过程中，我们将为 k-NN 学习方法准备数据。
- en: Tip
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If you plan on following along, download the `wisc_bc_data.csv` file from the
    Packt website and save it to your R working directory. The dataset was modified
    very slightly from its original form for this book. In particular, a header line
    was added and the rows of data were randomly ordered.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划跟着一起操作，请从 Packt 网站下载 `wisc_bc_data.csv` 文件，并将其保存到你的 R 工作目录中。该数据集在原始形式的基础上略作修改，特别是添加了一个标题行，并且数据行已被随机排序。
- en: 'We''ll begin by importing the CSV data file, as we have done in previous chapters,
    saving the Wisconsin breast cancer data to the `wbcd` data frame:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像以前章节一样，首先导入 CSV 数据文件，并将威斯康星乳腺癌数据保存到 `wbcd` 数据框中：
- en: '[PRE0]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Using the `str(wbcd)` command, we can confirm that the data is structured with
    569 examples and 32 features as we expected. The first several lines of output
    are as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `str(wbcd)` 命令，我们可以确认数据结构与预期相符，包含 569 个样本和 32 个特征。输出的前几行如下所示：
- en: '[PRE1]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first variable is an integer variable named `id`. As this is simply a unique
    identifier (ID) for each patient in the data, it does not provide useful information,
    and we will need to exclude it from the model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个变量是一个名为`id`的整数变量。由于它仅仅是数据中每个病人的唯一标识符（ID），并不提供有用信息，因此我们需要将其排除在模型之外。
- en: Tip
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Regardless of the machine learning method, ID variables should always be excluded.
    Neglecting to do so can lead to erroneous findings because the ID can be used
    to uniquely "predict" each example. Therefore, a model that includes an identifier
    will suffer from overfitting, and is unlikely to generalize well to other data.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 无论使用哪种机器学习方法，ID变量都应始终被排除。忽略这一点可能会导致错误的结果，因为ID可以用于唯一地“预测”每个示例。因此，包含标识符的模型会出现过拟合的情况，而且不太可能在其他数据上具有良好的泛化能力。
- en: 'Let''s drop the `id` feature altogether. As it is located in the first column,
    we can exclude it by making a copy of the `wbcd` data frame without column `1`:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们完全删除`id`特征。由于它位于第一列，我们可以通过制作`wbcd`数据框的副本，排除第一列来去除它：
- en: '[PRE2]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The next variable, `diagnosis`, is of particular interest as it is the outcome
    we hope to predict. This feature indicates whether the example is from a benign
    or malignant mass. The `table()` output indicates that 357 masses are benign while
    212 are malignant:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个变量`diagnosis`尤其重要，因为它是我们希望预测的结果。这个特征表示示例是来自良性肿块还是恶性肿块。`table()`输出显示357个肿块是良性的，而212个是恶性的：
- en: '[PRE3]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Many R machine learning classifiers require that the target feature is coded
    as a factor, so we will need to recode the `diagnosis` variable. We will also
    take this opportunity to give the `"B"` and `"M"` values more informative labels
    using the `labels` parameter:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 许多R机器学习分类器要求目标特征被编码为因子，因此我们需要重新编码`diagnosis`变量。我们还将借此机会通过`labels`参数为`"B"`和`"M"`值提供更具信息性的标签：
- en: '[PRE4]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, when we look at the `prop.table()` output, we notice that the values have
    been labeled `Benign` and `Malignant` with 62.7 percent and 37.3 percent of the
    masses, respectively:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们查看`prop.table()`输出时，注意到值已被标记为`Benign`和`Malignant`，分别占62.7%和37.3%的肿块：
- en: '[PRE5]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The remaining 30 features are all numeric, and as expected, they consist of
    three different measurements of ten characteristics. For illustrative purposes,
    we will only take a closer look at three of these features:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的30个特征都是数值型的，正如预期的，它们由十个特征的三种不同测量组成。为了便于说明，我们将只对其中三个特征进行更深入的分析：
- en: '[PRE6]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Looking at the features side-by-side, do you notice anything problematic about
    the values? Recall that the distance calculation for k-NN is heavily dependent
    upon the measurement scale of the input features. Since smoothness ranges from
    0.05 to 0.16 and area ranges from `143.5` to `2501.0`, the impact of area is going
    to be much larger than the smoothness in the distance calculation. This could
    potentially cause problems for our classifier, so let's apply normalization to
    rescale the features to a standard range of values.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 看着这些特征并排放置，你是否注意到数值上有任何问题？请记住，k-NN的距离计算在很大程度上依赖于输入特征的测量尺度。由于平滑度范围从0.05到0.16，而面积范围从`143.5`到`2501.0`，面积在距离计算中的影响将远大于平滑度。这可能会对我们的分类器造成问题，因此我们需要应用规范化将特征重新缩放到一个标准的数值范围。
- en: Transformation – normalizing numeric data
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换——规范化数值数据
- en: 'To normalize these features, we need to create a `normalize()` function in
    R. This function takes a vector `x` of numeric values, and for each value in `x`,
    subtracts the minimum value in `x` and divides by the range of values in `x`.
    Finally, the resulting vector is returned. The code for this function is as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了规范化这些特征，我们需要在R中创建一个`normalize()`函数。这个函数接受一个数值型向量`x`，对于`x`中的每个值，减去`x`中的最小值，并除以`x`中值的范围。最后，返回规范化后的向量。这个函数的代码如下：
- en: '[PRE7]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After executing the preceding code, the `normalize()` function is available
    for use in R. Let''s test the function on a couple of vectors:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码后，`normalize()`函数可以在R中使用。让我们在几个向量上测试该函数：
- en: '[PRE8]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The function appears to be working correctly. Despite the fact that the values
    in the second vector are 10 times larger than the first vector, after normalization,
    they both appear exactly the same.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数似乎正常工作。尽管第二个向量中的值比第一个向量大10倍，但经过规范化后，它们看起来完全相同。
- en: We can now apply the `normalize()` function to the numeric features in our data
    frame. Rather than normalizing each of the 30 numeric variables individually,
    we will use one of R's functions to automate the process.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将`normalize()`函数应用于数据框中的数值特征。我们将不再单独对每一个30个数值变量进行标准化，而是使用R的一个函数来自动化这一过程。
- en: 'The `lapply()` function takes a list and applies a specified function to each
    list element. As a data frame is a list of equal-length vectors, we can use `lapply()`
    to apply `normalize()` to each feature in the data frame. The final step is to
    convert the list returned by `lapply()` to a data frame, using the `as.data.frame()`
    function. The full process looks like this:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`lapply()`函数接受一个列表，并将指定的函数应用于每个列表元素。由于数据框是一个等长向量的列表，我们可以使用`lapply()`将`normalize()`函数应用于数据框中的每个特征。最后一步是使用`as.data.frame()`函数将`lapply()`返回的列表转换为数据框。完整的过程如下所示：'
- en: '[PRE9]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In plain English, this command applies the `normalize()` function to columns
    2 through 31 in the `wbcd` data frame, converts the resulting list to a data frame,
    and assigns it the name `wbcd_n`. The `_n` suffix is used here as a reminder that
    the values in `wbcd` have been normalized.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 用简单的语言来说，这个命令将`normalize()`函数应用于`wbcd`数据框的第2到第31列，将结果列表转换为数据框，并将其命名为`wbcd_n`。这里使用`_n`后缀，提醒我们`wbcd`中的值已经被标准化。
- en: 'To confirm that the transformation was applied correctly, let''s look at one
    variable''s summary statistics:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认转换是否正确应用，让我们查看一个变量的汇总统计信息：
- en: '[PRE10]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As expected, the `area_mean` variable, which originally ranged from 143.5 to
    2501.0, now ranges from 0 to 1.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，`area_mean`变量，原始值范围从143.5到2501.0，现在的范围从0到1。
- en: Data preparation – creating training and test datasets
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备——创建训练集和测试集
- en: Although all the 569 biopsies are labeled with a benign or malignant status,
    it is not very interesting to predict what we already know. Additionally, any
    performance measures we obtain during the training may be misleading as we do
    not know the extent to which cases have been overfitted or how well the learner
    will generalize to unseen cases. A more interesting question is how well our learner
    performs on a dataset of unlabeled data. If we had access to a laboratory, we
    could apply our learner to the measurements taken from the next 100 masses of
    unknown cancer status, and see how well the machine learner's predictions compare
    to the diagnoses obtained using conventional methods.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所有的569个活检样本都已标记为良性或恶性状态，但预测我们已经知道的结果并不十分有趣。此外，在训练过程中获得的任何性能度量可能会误导我们，因为我们不知道这些案例过拟合的程度，或者学习者如何将其推广到未见过的案例。一个更有趣的问题是，学习者在未标记数据集上的表现如何。如果我们能够进入实验室，我们可以将学习者应用于下一批100个未知癌症状态的肿块测量值，并观察机器学习者的预测与常规方法获得的诊断结果的比较。
- en: 'In the absence of such data, we can simulate this scenario by dividing our
    data into two portions: a training dataset that will be used to build the k-NN
    model and a test dataset that will be used to estimate the predictive accuracy
    of the model. We will use the first 469 records for the training dataset and the
    remaining 100 to simulate new patients.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有这些数据，我们可以通过将数据分为两部分来模拟这种情况：一部分是训练数据集，用于构建k-NN模型；另一部分是测试数据集，用于估算模型的预测准确性。我们将使用前469条记录作为训练数据集，剩余的100条记录用于模拟新患者。
- en: 'Using the data extraction methods given in [Chapter 2](ch02.html "Chapter 2. Managing
    and Understanding Data"), *Managing and Understanding Data*, we will split the
    `wbcd_n` data frame into `wbcd_train` and `wbcd_test`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[第2章](ch02.html "第2章：管理与理解数据")中给出的数据提取方法，*管理与理解数据*，我们将`wbcd_n`数据框分割为`wbcd_train`和`wbcd_test`：
- en: '[PRE11]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If the preceding commands are confusing, remember that data is extracted from
    data frames using the `[row, column]` syntax. A blank value for the row or column
    value indicates that all the rows or columns should be included. Hence, the first
    line of code takes rows 1 to 469 and all columns, and the second line takes 100
    rows from 470 to 569 and all columns.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的命令让你感到困惑，请记住，数据是通过使用`[行, 列]`语法从数据框中提取的。行或列值为空表示应包含所有行或列。因此，第一行代码获取第1到469行以及所有列，第二行代码获取第470到569行的100行和所有列。
- en: Tip
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: When constructing training and test datasets, it is important that each dataset
    is a representative subset of the full set of data. The `wbcd` records were already
    randomly ordered, so we could simply extract 100 consecutive records to create
    a test dataset. This would not be appropriate if the data was ordered chronologically
    or in groups of similar values. In these cases, random sampling methods would
    be needed. Random sampling will be discussed in [Chapter 5](ch05.html "Chapter 5. Divide
    and Conquer – Classification Using Decision Trees and Rules"), *Divide and Conquer
    – Classification Using Decision Trees and Rules*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建训练和测试数据集时，重要的是每个数据集都是完整数据集的代表性子集。`wbcd`记录已经是随机排序的，因此我们可以简单地提取100个连续的记录来创建测试数据集。如果数据是按时间顺序或按类似值的分组排序的，这种做法就不合适。在这些情况下，需要使用随机抽样方法。随机抽样将在[第5章](ch05.html
    "第5章。分治法——使用决策树和规则进行分类")中讨论，*分治法——使用决策树和规则进行分类*。
- en: 'When we constructed our normalized training and test datasets, we excluded
    the target variable, `diagnosis`. For training the k-NN model, we will need to
    store these class labels in factor vectors, split between the training and test
    datasets:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建标准化的训练和测试数据集时，我们排除了目标变量`diagnosis`。在训练k-NN模型时，我们需要将这些类别标签存储在因子向量中，并将其分为训练集和测试集：
- en: '[PRE12]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This code takes the `diagnosis` factor in the first column of the `wbcd` data
    frame, and creates the vectors `wbcd_train_labels` and `wbcd_test_labels`. We
    will use these in the next steps of training and evaluating our classifier.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码提取`wbcd`数据框第一列中的`diagnosis`因子，并创建向量`wbcd_train_labels`和`wbcd_test_labels`。我们将在接下来的步骤中使用这些向量来训练和评估我们的分类器。
- en: Step 3 – training a model on the data
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步 —— 在数据上训练模型
- en: Equipped with our training data and labels vector, we are now ready to classify
    our unknown records. For the k-NN algorithm, the training phase actually involves
    no model building; the process of training a lazy learner like k-NN simply involves
    storing the input data in a structured format.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 配备了我们的训练数据和标签向量后，我们现在准备对未知记录进行分类。对于k-NN算法，训练阶段实际上不涉及模型构建；训练像k-NN这样的“懒惰学习者”的过程仅仅是将输入数据存储在结构化格式中。
- en: 'To classify our test instances, we will use a k-NN implementation from the
    `class` package, which provides a set of basic R functions for classification.
    If this package is not already installed on your system, you can install it by
    typing:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对我们的测试实例进行分类，我们将使用`class`包中的k-NN实现，该包提供了一组用于分类的基本R函数。如果你的系统上还没有安装该包，可以通过输入以下命令进行安装：
- en: '[PRE13]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To load the package during any session in which you wish to use the functions,
    simply enter the `library(class)` command.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何希望使用该函数的会话中，加载该包时，只需输入`library(class)`命令。
- en: The `knn()` function in the `class` package provides a standard, classic implementation
    of the k-NN algorithm. For each instance in the test data, the function will identify
    the k-Nearest Neighbors, using Euclidean distance, where *k* is a user-specified
    number. The test instance is classified by taking a "vote" among the k-Nearest
    Neighbors—specifically, this involves assigning the class of the majority of the
    *k* neighbors. A tie vote is broken at random.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`class`包中的`knn()`函数提供了k-NN算法的标准经典实现。对于测试数据中的每个实例，函数将使用欧几里得距离识别k个最近邻，其中*k*是用户指定的数字。通过在k个最近邻之间进行“投票”，对测试实例进行分类——具体来说，这包括为多数*
    k *邻居分配类别。如果出现平局投票，则随机打破平局。'
- en: Tip
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: There are several other k-NN functions in other R packages, which provide more
    sophisticated or more efficient implementations. If you run into limits with `knn()`,
    search for k-NN at the **Comprehensive R Archive Network** (**CRAN**).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他一些k-NN函数在其他R包中，它们提供了更复杂或更高效的实现。如果你在使用`knn()`时遇到限制，可以在**综合R档案网络**（**CRAN**）上搜索k-NN。
- en: 'Training and classification using the `knn()` function is performed in a single
    function call, using four parameters, as shown in the following table:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`knn()`函数进行训练和分类是通过一次函数调用完成的，使用四个参数，如下表所示：
- en: '![Step 3 – training a model on the data](img/3905_03_12.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![第3步 —— 在数据上训练模型](img/3905_03_12.jpg)'
- en: We now have nearly everything that we need to apply the k-NN algorithm to this
    data. We've split our data into training and test datasets, each with exactly
    the same numeric features. The labels for the training data are stored in a separate
    factor vector. The only remaining parameter is `k`, which specifies the number
    of neighbors to include in the vote.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们几乎具备了应用 k-NN 算法所需的一切。我们已经将数据分成了训练数据集和测试数据集，每个数据集都有完全相同的数值特征。训练数据的标签存储在一个单独的因子向量中。唯一剩下的参数是
    `k`，它指定了投票中要包含的邻居数量。
- en: As our training data includes 469 instances, we might try `k = 21`, an odd number
    roughly equal to the square root of 469\. With a two-category outcome, using an
    odd number eliminates the chance of ending with a tie vote.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的训练数据包含 469 个实例，我们可以尝试使用 `k = 21`，这是一个奇数，接近 469 的平方根。在二分类结果的情况下，使用奇数可以避免最终出现平局投票的情况。
- en: 'Now we can use the `knn()` function to classify the `test` data:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用 `knn()` 函数对 `test` 数据进行分类：
- en: '[PRE14]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `knn()` function returns a factor vector of predicted labels for each of
    the examples in the `test` dataset, which we have assigned to `wbcd_test_pred`.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`knn()` 函数返回一个预测标签的因子向量，对应 `test` 数据集中的每一个例子，我们将其赋值给 `wbcd_test_pred`。'
- en: Step 4 – evaluating model performance
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 4 – 评估模型性能
- en: The next step of the process is to evaluate how well the predicted classes in
    the `wbcd_test_pred` vector match up with the known values in the `wbcd_test_labels`
    vector. To do this, we can use the `CrossTable()` function in the `gmodels` package,
    which was introduced in [Chapter 2](ch02.html "Chapter 2. Managing and Understanding
    Data"), *Managing and Understanding Data*. If you haven't done so already, please
    install this package, using the `install.packages("gmodels")` command.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 过程的下一步是评估 `wbcd_test_pred` 向量中的预测类别与 `wbcd_test_labels` 向量中的已知值的匹配程度。为此，我们可以使用
    `gmodels` 包中的 `CrossTable()` 函数，该函数在 [第 2 章](ch02.html "第 2 章. 数据管理与理解")，*数据管理与理解*
    中介绍。如果你还没有安装该包，请使用 `install.packages("gmodels")` 命令进行安装。
- en: 'After loading the package with the `library(gmodels)` command, we can create
    a cross tabulation indicating the agreement between the two vectors. Specifying
    `prop.chisq = FALSE` will remove the unnecessary chi-square values from the output:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 `library(gmodels)` 命令加载包之后，我们可以创建一个交叉列联表，表示两个向量之间的一致性。指定 `prop.chisq = FALSE`
    将从输出中移除不必要的卡方值：
- en: '[PRE15]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The resulting table looks like this:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表格如下所示：
- en: '![Step 4 – evaluating model performance](img/3905_03_13.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![步骤 4 – 评估模型性能](img/3905_03_13.jpg)'
- en: The cell percentages in the table indicate the proportion of values that fall
    into four categories. The top-left cell indicates the **true negative** results.
    These 61 of 100 values are cases where the mass was benign and the k-NN algorithm
    correctly identified it as such. The bottom-right cell indicates the **true positive**
    results, where the classifier and the clinically determined label agree that the
    mass is malignant. A total of 37 of 100 predictions were true positives.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 表格中的单元格百分比表示四个类别中值的比例。左上角的单元格表示**真负**结果。这 100 个值中有 61 个是良性肿块，且 k-NN 算法正确地将其识别为良性。右下角的单元格表示**真阳性**结果，其中分类器和临床确定的标签一致，表明肿块为恶性。总共
    100 个预测中，有 37 个是真阳性。
- en: The cells falling on the other diagonal contain counts of examples where the
    k-NN approach disagreed with the true label. The two examples in the lower-left
    cell are **false negative** results; in this case, the predicted value was benign,
    but the tumor was actually malignant. Errors in this direction could be extremely
    costly as they might lead a patient to believe that she is cancer-free, but in
    reality, the disease may continue to spread. The top-right cell would contain
    the **false positive** results, if there were any. These values occur when the
    model classifies a mass as malignant, but in reality, it was benign. Although
    such errors are less dangerous than a false negative result, they should also
    be avoided as they could lead to additional financial burden on the health care
    system or additional stress for the patient as additional tests or treatment may
    have to be provided.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 其他对角线上的单元格包含的是k-NN方法与真实标签不一致的示例数量。左下角的两个示例是**假阴性**结果；在这种情况下，预测值为良性肿块，但实际上肿瘤是恶性的。此类错误可能极为代价高昂，因为它们可能导致患者误以为自己没有癌症，而实际上，疾病可能还在继续扩散。右上角的单元格将包含**假阳性**结果（如果有的话）。这些值发生在模型将肿块分类为恶性时，但实际上它是良性的。尽管此类错误比假阴性结果危害较小，但仍应避免，因为它们可能导致医疗系统额外的经济负担，或由于需要额外的检测或治疗而给患者带来更多压力。
- en: Tip
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If we desired, we could totally eliminate false negatives by classifying every
    mass as malignant. Obviously, this is not a realistic strategy. Still, it illustrates
    the fact that prediction involves striking a balance between the false positive
    rate and the false negative rate. In [Chapter 10](ch10.html "Chapter 10. Evaluating
    Model Performance"), *Evaluating Model Performance*, you will learn more sophisticated
    methods for measuring predictive accuracy that can be used to identify places
    where the error rate can be optimized depending on the costs of each type of error.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们愿意的话，完全可以通过将每个肿块分类为恶性肿瘤来消除假阴性。显然，这并不是一种现实的策略。尽管如此，这仍然说明了预测需要在假阳性率和假阴性率之间找到平衡。在[第10章](ch10.html
    "第10章：评估模型表现")，*评估模型表现*，你将学习到更复杂的预测准确性衡量方法，这些方法可以用来识别那些错误率可以根据每种类型错误的成本进行优化的地方。
- en: A total of 2 out of 100, or 2 percent of masses were incorrectly classified
    by the k-NN approach. While 98 percent accuracy seems impressive for a few lines
    of R code, we might try another iteration of the model to see whether we can improve
    the performance and reduce the number of values that have been incorrectly classified,
    particularly because the errors were dangerous false negatives.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有100个肿块中有2个被错误分类，即2%的肿块被k-NN方法错误分类。虽然98%的准确率看起来对于几行R代码来说相当令人印象深刻，但我们可能会尝试模型的另一个迭代版本，看看能否改善性能，减少错误分类的数量，特别是因为这些错误是危险的假阴性。
- en: Step 5 – improving model performance
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5步 – 提高模型性能
- en: We will attempt two simple variations on our previous classifier. First, we
    will employ an alternative method for rescaling our numeric features. Second,
    we will try several different values for *k*.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试对之前的分类器做两个简单的变动。首先，我们将采用一种替代方法对数值特征进行重缩放。其次，我们将尝试几个不同的*k*值。
- en: Transformation – z-score standardization
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换 – z分数标准化
- en: Although normalization is traditionally used for k-NN classification, it may
    not always be the most appropriate way to rescale features. Since the z-score
    standardized values have no predefined minimum and maximum, extreme values are
    not compressed towards the center. One might suspect that with a malignant tumor,
    we might see some very extreme outliers as the tumors grow uncontrollably. It
    might, therefore, be reasonable to allow the outliers to be weighted more heavily
    in the distance calculation. Let's see whether z-score standardization can improve
    our predictive accuracy.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管归一化通常用于k-NN分类，但它并不总是最合适的特征重缩放方法。由于z分数标准化后的值没有预定义的最小值和最大值，极端值不会被压缩到中心。因此，人们可能会怀疑在恶性肿瘤的情况下，肿瘤可能会
    uncontrollably 地生长，产生一些非常极端的异常值。因此，允许异常值在距离计算中有更高的权重可能是合理的。让我们看看z分数标准化是否能改善我们的预测准确性。
- en: 'To standardize a vector, we can use the R''s built-in `scale()` function, which,
    by default, rescales values using the z-score standardization. The `scale()` function
    offers the additional benefit that it can be applied directly to a data frame,
    so we can avoid the use of the `lapply()` function. To create a z-score standardized
    version of the `wbcd` data, we can use the following command:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 为了标准化一个向量，我们可以使用R的内置`scale()`函数，默认情况下，它使用z分数标准化值。`scale()`函数的额外优点是它可以直接应用于数据框，这样我们就可以避免使用`lapply()`函数。要创建`wbcd`数据的z分数标准化版本，我们可以使用以下命令：
- en: '[PRE16]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This command rescales all the features, with the exception of `diagnosis` and
    stores the result as the `wbcd_z` data frame. The `_z` suffix is a reminder that
    the values were z-score transformed.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令对所有特征进行标准化，`diagnosis`除外，并将结果存储为`wbcd_z`数据框。`_z`后缀是提醒值已经进行了z分数转换。
- en: 'To confirm that the transformation was applied correctly, we can look at the
    summary statistics:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认转换是否正确应用，我们可以查看汇总统计：
- en: '[PRE17]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The mean of a z-score standardized variable should always be zero, and the range
    should be fairly compact. A z-score greater than 3 or less than -3 indicates an
    extremely rare value. With this in mind, the transformation seems to have worked.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一个z分数标准化变量的均值应该始终为零，范围应该相对紧凑。大于3或小于-3的z分数表示极其罕见的值。考虑到这一点，转换似乎成功了。
- en: 'As we had done earlier, we need to divide the data into training and test sets,
    and then classify the test instances using the `knn()` function. We''ll then compare
    the predicted labels to the actual labels using `CrossTable()`:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所做的，我们需要将数据分为训练集和测试集，然后使用`knn()`函数对测试实例进行分类。接着，我们将使用`CrossTable()`将预测标签与实际标签进行比较：
- en: '[PRE18]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Unfortunately, in the following table, the results of our new transformation
    show a slight decline in accuracy. The instances where we had correctly classified
    98 percent of examples previously, we classified only 95 percent correctly this
    time. Making matters worse, we did no better at classifying the dangerous false
    negatives:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在下面的表格中，我们的新转换结果显示准确率略有下降。我们之前正确分类了98%的例子，而这次我们仅正确分类了95%。更糟糕的是，我们在分类危险的假阴性时也没有取得更好的成绩：
- en: '![Transformation – z-score standardization](img/3905_03_14.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![转换 – z分数标准化](img/3905_03_14.jpg)'
- en: Testing alternative values of k
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试不同的k值
- en: 'We may be able do even better by examining performance across various *k* values.
    Using the normalized training and test datasets, the same 100 records were classified
    using several different *k* values. The number of false negatives and false positives
    are shown for each iteration:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也许可以通过检查不同*k*值下的表现来做得更好。使用标准化后的训练集和测试集，使用多个不同的*k*值对相同的100条记录进行了分类。每次迭代的假阴性和假阳性的数量如下所示：
- en: '| k value | False negatives | False positives | Percent classified incorrectly
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| k 值 | 假阴性 | 假阳性 | 错误分类的百分比 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | 1 | 3 | 4 percent |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 3 | 4 百分比 |'
- en: '| 5 | 2 | 0 | 2 percent |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 2 | 0 | 2 百分比 |'
- en: '| 11 | 3 | 0 | 3 percent |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 3 | 0 | 3 百分比 |'
- en: '| 15 | 3 | 0 | 3 percent |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 3 | 0 | 3 百分比 |'
- en: '| 21 | 2 | 0 | 2 percent |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 21 | 2 | 0 | 2 百分比 |'
- en: '| 27 | 4 | 0 | 4 percent |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 27 | 4 | 0 | 4 百分比 |'
- en: Although the classifier was never perfect, the 1-NN approach was able to avoid
    some of the false negatives at the expense of adding false positives. It is important
    to keep in mind, however, that it would be unwise to tailor our approach too closely
    to our test data; after all, a different set of 100 patient records is likely
    to be somewhat different from those used to measure our performance.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管分类器从未完美，但1-NN方法能够避免一些假阴性，尽管代价是增加了假阳性。然而，需要记住的是，我们不应过于根据测试数据来调整我们的做法；毕竟，另一组100个患者记录很可能会与我们用来衡量性能的记录有所不同。
- en: Tip
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If you need to be certain that a learner will generalize to future data, you
    might create several sets of 100 patients at random and repeatedly retest the
    result. The methods to carefully evaluate the performance of machine learning
    models will be discussed further in [Chapter 10](ch10.html "Chapter 10. Evaluating
    Model Performance"), *Evaluating Model Performance*.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要确定一个学习算法能否对未来的数据进行泛化，你可以随机创建多个100个患者的集合，并反复重新测试结果。关于如何仔细评估机器学习模型性能的方法，将在[第10章](ch10.html
    "第10章：评估模型性能")，*评估模型性能*中进一步讨论。
- en: Summary
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about classification using k-NN. Unlike many classification
    algorithms, k-NN does not do any learning. It simply stores the training data
    verbatim. Unlabeled test examples are then matched to the most similar records
    in the training set using a distance function, and the unlabeled example is assigned
    the label of its neighbors.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了使用k-NN进行分类。与许多分类算法不同，k-NN不进行任何学习。它只是逐字存储训练数据。然后，使用距离函数将未标记的测试样本与训练集中最相似的记录进行匹配，并将未标记样本分配给其邻居的标签。
- en: In spite of the fact that k-NN is a very simple algorithm, it is capable of
    tackling extremely complex tasks, such as the identification of cancerous masses.
    In a few simple lines of R code, we were able to correctly identify whether a
    mass was malignant or benign 98 percent of the time.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管k-NN是一个非常简单的算法，但它能够处理极为复杂的任务，比如癌症肿块的识别。在几行简单的R代码中，我们能够以98%的准确率正确识别一个肿块是恶性还是良性。
- en: In the next chapter, we will examine a classification method that uses probability
    to estimate the likelihood that an observation falls into certain categories.
    It will be interesting to compare how this approach differs from k-NN. Later on,
    in [Chapter 9](ch09.html "Chapter 9. Finding Groups of Data – Clustering with
    k-means"), *Finding Groups of Data – Clustering with k-means*, we will learn about
    a close relative to k-NN, which uses distance measures for a completely different
    learning task.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将研究一种分类方法，它利用概率来估计一个观察值属于某些类别的可能性。将会很有趣地比较这种方法与k-NN的不同之处。稍后，在[第9章](ch09.html
    "第9章. 数据分组——使用k-means进行聚类")，*数据分组——使用k-means进行聚类*，我们将学习与k-NN关系密切的另一种方法，它使用距离度量进行完全不同的学习任务。
