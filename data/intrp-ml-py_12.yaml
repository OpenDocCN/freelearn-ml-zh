- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Monotonic Constraints and Model Tuning for Interpretability
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单调约束和模型调优以提高解释性
- en: Most model classes have hyperparameters that can be tuned for faster execution
    speed, increasing predictive performance, and reducing overfitting. One way of
    reducing overfitting is by introducing regularization into the model training.
    In *Chapter 3*, *Interpretation Challenges*, we called regularization a remedial
    interpretability property, which reduces complexity with a penalty or limitation
    that forces the model to learn sparser representations of the inputs. Regularized
    models generalize better, which is why it is highly recommended to tune models
    with regularization to avoid overfitting to the training data. As a side effect,
    regularized models tend to have fewer features and interactions, making the model
    easier to interpret—*less noise means a clearer signal*!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数模型类都有超参数，可以通过调整来提高执行速度、增强预测性能和减少过拟合。减少过拟合的一种方法是在模型训练中引入正则化。在*第3章*，*解释性挑战*中，我们将正则化称为一种补救的解释性属性，它通过惩罚或限制来降低复杂性，迫使模型学习输入的更稀疏表示。正则化模型具有更好的泛化能力，这就是为什么强烈建议使用正则化调整模型以避免对训练数据的过拟合。作为副作用，正则化模型通常具有更少的特征和交互，这使得模型更容易解释——*更少的噪声意味着更清晰的信号*！
- en: And even though there are many hyperparameters, we will only focus on those
    that improve interpretability by controlling overfitting. Also, to a certain extent,
    we will revisit bias mitigation through the class imbalance-related hyperparameters
    explored in previous chapters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多超参数，但我们只会关注那些通过控制过拟合来提高解释性的参数。在一定意义上，我们还将回顾通过前几章中探讨的类别不平衡相关超参数来减轻偏差。
- en: '*Chapter 2*, *Key Concepts of Interpretability*, explained three model properties
    that impact interpretability: non-linearity, interactivity, and non-monotonicity.
    Left to its own devices, a model can learn some spurious and counterintuitive
    non-linearities and interactivities. As discussed in *Chapter 10*, *Feature Selection
    and Engineering for Interpretability*, guardrails can be placed to prevent this
    through careful feature engineering. However, what can we do to place guardrails
    for monotonicity? In this chapter, we will learn how to do just this with monotonic
    constraints. And just as monotonic constraints can be the model counterpart to
    feature engineering, regularization can be the model counterpart to the feature
    selection methods we covered in *Chapter 10*!'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*第2章*，*解释性的关键概念*，解释了三个影响解释性的模型属性：非线性、交互性和非单调性。如果模型自行其是，它可能会学习到一些虚假的、反直觉的非线性和交互性。正如在第10章，*为解释性进行特征选择和工程*中讨论的那样，可以通过仔细的特征工程来设置限制以防止这种情况。然而，我们如何为单调性设置限制呢？在本章中，我们将学习如何使用单调约束来实现这一点。同样，单调约束可以是模型与特征工程的对应物，而正则化可以是我们在第10章中涵盖的特征选择方法的模型对应物！'
- en: 'These are the main topics we are going to cover in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖的主要主题包括：
- en: Placing guardrails with feature engineering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过特征工程设置限制
- en: Tuning models for interpretability
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整模型以提高解释性
- en: Implementing model constraints
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现模型约束
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter’s example uses the `mldatasets`, `pandas`, `numpy`, `sklearn`,
    `xgboost`, `lightgbm`, `catboost`, `tensorflow`, `bayes_opt`, `tensorflow_lattice`,
    `matplotlib`, `seaborn`, `scipy`, `xai`, and `shap` libraries. Instructions on
    how to install these libraries are in the preface.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例使用了`mldatasets`、`pandas`、`numpy`、`sklearn`、`xgboost`、`lightgbm`、`catboost`、`tensorflow`、`bayes_opt`、`tensorflow_lattice`、`matplotlib`、`seaborn`、`scipy`、`xai`和`shap`库。如何安装这些库的说明在序言中。
- en: 'The code for this chapter is located here: [https://packt.link/pKeAh](https://packt.link/pKeAh)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于此处：[https://packt.link/pKeAh](https://packt.link/pKeAh)
- en: The mission
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务
- en: The issue of algorithmic fairness is one with massive social implications, from
    the allocation of welfare resources to the prioritization of life-saving surgeries
    to screening job applications. These machine learning algorithms can determine
    a person’s livelihood or life, and it’s often the most marginalized and vulnerable
    populations that get the worst treatment from these algorithms because they perpetuate
    systemic biases learned from the data. Therefore, it’s poorer families that get
    misclassified for child abuse; it’s racial-minority people who get underprioritized
    for medical treatment; and it’s women who get screened out of high-paying tech
    jobs. Even in cases involving less immediate and individualized risks such as
    online searches, Twitter/X bots, and social media profiles, societal prejudices
    such as elitism, racism, sexism, and ageism are reinforced.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 算法公平性问题具有巨大的社会影响，从福利资源的分配到救命手术的优先级，再到求职申请的筛选。这些机器学习算法可以决定一个人的生计或生命，而且往往是边缘化和最脆弱的群体从这些算法中受到最恶劣的对待，因为这些算法持续传播从数据中学到的系统性偏见。因此，贫困家庭可能被错误地归类为虐待儿童；种族少数群体在医疗治疗中可能被优先级过低；而女性可能被排除在高薪技术工作之外。即使在涉及不那么直接和个性化的风险的情况下，如在线搜索、Twitter/X机器人账户和社交媒体档案，社会偏见如精英主义、种族主义、性别歧视和年龄歧视也会得到加强。
- en: This chapter will continue on the mission from *Chapter 6*, *Anchors and Counterfactual
    Explanations*. If you aren’t familiar with these techniques, please go back and
    read *Chapter 6* to get a solid understanding of the problem. The recidivism case
    from *Chapter 6* is one of algorithmic bias. The co-founder of the company that
    developed the **COMPAS algorithm** (where **COMPAS** stands for **Correctional
    Offender Management Profiling Alternative Sanctions**) admitted that it’s tough
    to make a score without questions that are correlated with race. This correlation
    is one of the main reasons that scores are biased against African Americans. The
    other reason is the likely overrepresentation of black defendants in the training
    data. We don’t know for sure because we don’t have the original training data,
    but we know that non-white minorities are overrepresented in the population of
    incarcerated individuals. We also know that black people are typically overrepresented
    in arrests because of codified discrimination in terms of minor drug-related offenses
    and over-policing in black communities.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将继续延续第六章的主题，即*锚点和反事实解释*。如果您不熟悉这些技术，请回过头去阅读*第六章*，以获得对问题的深入了解。第六章中的再犯案例是算法偏差的一个例子。开发**COMPAS算法**（其中**COMPAS**代表**矫正犯人管理配置文件替代制裁**）的公司的联合创始人承认，在没有与种族相关的问题的情况下很难给出分数。这种相关性是分数对非裔美国人产生偏见的主要原因之一。另一个原因是训练数据中黑人被告可能被过度代表。我们无法确定这一点，因为我们没有原始的训练数据，但我们知道非白人少数族裔在服刑人员群体中被过度代表。我们还知道，由于与轻微毒品相关罪行相关的编码歧视和黑人社区的过度执法，黑人通常在逮捕中被过度代表。
- en: So, what can we do to fix it?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们该如何解决这个问题呢？
- en: In *Chapter 6*, *Anchors and Counterfactual Explanations*, we managed to demonstrate
    via a *proxy model* that the COMPAS algorithm was biased. For this chapter, let’s
    say that the journalist published your findings, and an algorithmic justice advocacy
    group read the article and reached out. Companies that make criminal assessment
    tools are not taking responsibility for bias and claim that their tools simply
    reflect *reality*. The advocacy group has hired you to demonstrate that a machine
    learning model can be trained to be significantly less biased toward black defendants
    while ensuring that the model reflects only proven criminal justice *realities*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第六章*，*锚点和反事实解释*中，我们通过一个*代理模型*成功地证明了COMPAS算法存在偏见。对于本章，让我们假设记者发表了你的发现，一个算法正义倡导团体阅读了文章并联系了你。制作犯罪评估工具的公司没有对偏见承担责任，声称他们的工具只是反映了*现实*。该倡导团体雇佣你来证明机器学习模型可以被训练得对黑人被告的偏见显著减少，同时确保该模型仅反映经过验证的刑事司法*现实*。
- en: These proven realities include the monotone decrease of recidivism risk with
    age, and a strong correlation with priors, which increases strongly with age.
    Another fact supported by the academic literature is how females are significantly
    less prone to recidivism and criminality in general.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些被证实的现实包括随着年龄增长，再犯风险单调下降，以及与先前的强烈相关性，这种相关性随着年龄的增长而显著增强。学术文献支持的另一个事实是，女性在总体上显著不太可能再犯和犯罪。
- en: 'Before we move on, we must recognize that supervised learning models face several
    impediments in capturing domain knowledge from data. For instance, consider the
    following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我们必须认识到监督学习模型在从数据中捕获领域知识方面面临几个障碍。例如，考虑以下情况：
- en: '**Sample**, **exclusion**, **or** **prejudice bias**: What if your data doesn’t
    truly represent the environment your model intends to generalize? If that’s the
    case, the domain knowledge won’t align with what you observe in the data. What
    if the environment that produced the data has a built-in systemic or institutional
    bias? Then, the data will reflect these biases.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本、排除或偏见偏差**：如果您的数据并不能真正代表模型意图推广的环境，会怎样？如果是这样，领域知识将与您在数据中观察到的结果不一致。如果产生数据的那个环境具有固有的系统性或制度性偏见，那么数据将反映这些偏见。'
- en: '**Class imbalance**: As seen in *Chapter 11*, *Bias Mitigation and Causal Inference
    Methods*, class imbalance could favor some groups over others. While taking the
    most effective route toward high accuracy, a model will learn from this imbalance,
    contradicting domain knowledge.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别不平衡**：如第11章“偏差缓解和因果推断方法”中所述，类别不平衡可能会使某些群体相对于其他群体更有利。在追求最高准确率的最有效途径中，模型将从这个不平衡中学习，这与领域知识相矛盾。'
- en: '**Non-monotonicity**: Sparse areas in a features histogram or high-leverage
    outliers could cause a model to learn non-monotonicity when domain knowledge calls
    for otherwise, and any of the previously mentioned problems could contribute to
    this as well.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非单调性**：特征直方图中的稀疏区域或高杠杆异常值可能导致模型在领域知识要求单调性时学习到非单调性，任何之前提到的问题都可能促成这一点。'
- en: '**Uninfluential features**: An unregularized model will, by default, try to
    learn from all features as long as they carry some information, but this stands
    in the way of learning from relevant features or overfitting to noise in the training
    data. A more parsimonious model is more likely to prop up features supported by
    domain knowledge.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无影响力的特征**：一个未正则化的模型将默认尝试从所有特征中学习，只要它们携带一些信息，但这会阻碍从相关特征中学习或过度拟合训练数据中的噪声。一个更简约的模型更有可能支持由领域知识支持的特性。'
- en: '**Counterintuitive interactions**: As mentioned in *Chapter 10*, *Feature Selection
    and Engineering for Interpretability*, there could be counterintuitive interactions
    that a model favors over domain knowledge-supported interactions. As a side effect,
    these could end up favoring some groups that correlate with them. And in *Chapter
    6**, Anchors and Counterfactual Explanations*, we saw proof of this through an
    understanding of double standards.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反直觉的交互作用**：如第10章“用于可解释性的特征选择和工程”中提到的，模型可能会偏好与领域知识支持的交互作用相反的反直觉交互作用。作为一种副作用，这些交互作用可能会使一些与它们相关的群体受益。在第6章“锚点和反事实解释”中，我们通过理解双重标准证明了这一点。'
- en: '**Exceptions**: Our domain knowledge facts are based on an aggregate understanding,
    but when looking for patterns on a more granular scale, models will find exceptions
    such as pockets where female recidivism is of higher risk than that of males.
    Known phenomena might not support these models but they could be valid nonetheless,
    so we must be careful not to erase them with our tuning efforts.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**例外情况**：我们的领域知识事实基于总体理解，但在寻找更细粒度的模式时，模型会发现例外，例如女性再犯风险高于男性的区域。已知现象可能不支持这些模型，但它们可能是有效的，因此我们必须小心不要在我们的调整努力中抹去它们。'
- en: The advocacy group has validated the data as adequately representative of only
    one county in Florida, and they have provided you with a balanced dataset. The
    first impediment is a tough one to ascertain and control. The second one has been
    taken care of. It’s now up to you to deal with the remaining four!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 该倡导组织已验证数据仅足以代表佛罗里达州的一个县，并且他们已经向您提供了一个平衡的数据集。第一个障碍很难确定和控制。第二个问题已经得到解决。现在，剩下的四个问题就交给你来处理了！
- en: The approach
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: 'You have decided to take a three-fold approach, as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经决定采取三步走的方法，如下所示：
- en: '**Placing guardrails with feature engineering**: Leveraging lessons learned
    in *Chapter 6*, *Anchors and Counterfactual Explanations*, as well as the domain
    knowledge we already have about priors and age, in particular, we will engineer
    some features.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用特征工程设置护栏**：借鉴第6章“锚点和反事实解释”中学习到的经验，以及我们已有的关于先验和年龄的领域知识，我们将设计一些特征。'
- en: '**Tuning models for interpretability**: Once the data is ready, we will tune
    many models with different class weighting and overfitting prevention techniques.
    These methods will ensure that the models not only generalize better but are also
    easier to interpret.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调整模型以提高可解释性**：一旦数据准备就绪，我们将使用不同的类别权重和过拟合预防技术调整许多模型。这些方法将确保模型不仅泛化能力更好，而且更容易解释。'
- en: '**Implementing model constraints**: Last but not least, we will implement monotonic
    and interaction constraints on the best models to make sure that they don’t stray
    from trusted and fair interactions.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实施模型约束**：最后但同样重要的是，我们将对最佳模型实施单调性和交互约束，以确保它们不会偏离可信和公平的交互。'
- en: In the last two sections, we will make sure the models perform accurately and
    fairly. We will also compare recidivism risk distributions between the data and
    the model to ensure that they align.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后两个部分中，我们将确保模型准确且公平地执行。我们还将比较数据和模型之间的再犯风险分布，以确保它们一致。
- en: The preparations
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'You will find the code for this example here: [https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/12/Recidivism_part2.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/12/Recidivism_part2.ipynb)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到这个示例的代码：[https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/12/Recidivism_part2.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/12/Recidivism_part2.ipynb)
- en: Loading the libraries
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载库
- en: 'To run this example, you need to install the following libraries:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此示例，您需要安装以下库：
- en: '`mldatasets` to load the dataset'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mldatasets` 用于加载数据集'
- en: '`pandas` and `numpy` to manipulate it'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas` 和 `numpy` 用于操作'
- en: '`sklearn` (scikit-learn), `xgboost`, `lightgbm`, `catboost`, `tensorflow`,
    `bayes_opt`, and `tensorflow_lattice` to split the data and fit the models'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn`（scikit-learn）、`xgboost`、`lightgbm`、`catboost`、`tensorflow`、`bayes_opt`
    和 `tensorflow_lattice` 用于分割数据和拟合模型'
- en: '`matplotlib`, `seaborn`, `scipy`, `xai`, and `shap` to visualize the interpretations'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`、`seaborn`、`scipy`、`xai` 和 `shap` 以可视化解释'
- en: 'You should load all of them first, as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该首先加载所有这些库，如下所示：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s check that `tensorflow` has loaded the right version with `print(tf.__version__)`.
    This should be 2.8 and above.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查 `tensorflow` 是否加载了正确的版本，使用 `print(tf.__version__)`。这应该是 2.8 版本及以上。
- en: Understanding and preparing the data
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解和准备数据
- en: 'We load the data like this into a DataFrame we call `recidivism_df`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据以这种方式加载到我们称为 `recidivism_df` 的 DataFrame 中：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'There should be over 11,000 records and 11 columns. We can verify this was
    the case with `info()`, as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 应该有超过 11,000 条记录和 11 个列。我们可以使用 `info()` 验证这一点，如下所示：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding code outputs the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码输出了以下内容：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output checks out. There are no missing values, and all but three features
    are numeric (`sex`, `race`, and `charge_degree`). This is the same data we used
    in *Chapter 6*, *Anchors and Counterfactual Explanations*, so the data dictionary
    is exactly the same. However, the dataset has been balanced with sampling methods,
    and, this time, it hasn’t been prepared for us so we will need to do this, but
    before this, let’s gain an understanding of what the balancing did.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 输出检查无误。没有缺失值，除了三个特征（`sex`、`race` 和 `charge_degree`）外，所有特征都是数值型的。这是我们用于 *第 6
    章*，*锚点和反事实解释* 的相同数据，因此数据字典完全相同。然而，数据集已经通过采样方法进行了平衡，这次它没有为我们准备，因此我们需要这样做，但在这样做之前，让我们了解平衡做了什么。
- en: Verifying the sampling balance
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证采样平衡
- en: 'We can check how `race` and `is_recid` are distributed with XAI’s `imbalance_plot`.
    In other words, it will tally how many records exist for each `race`-`is_recid`
    combination. This plot will allow us to observe if there are imbalances in the
    number of defendants that recidivate for each `race`. The code can be seen in
    the following snippet:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 XAI 的 `imbalance_plot` 检查 `race` 和 `is_recid` 的分布情况。换句话说，它将统计每个 `race`-`is_recid`
    组合的记录数量。这个图将使我们能够观察每个 `race` 的被告中是否有再犯人数的不平衡。代码可以在以下片段中查看：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding code outputs *Figure 12.1*, which depicts how all races have equal
    amounts of `is_recid=0` and `is_recid=1`. However, **Other** is not at parity
    in numbers with the other races. Incidentally, this version of the dataset has
    bucketed all other races as **Other**, and the choice to not `upsample` **Other**
    or `downsample` the other two races to achieve total parity is made because they
    are less represented in the defendant population. This balancing choice is one
    of many that can be done in a situation such as this. Demographically, it all
    depends on what your data is supposed to represent. Defendants? Inmates? Civilians
    in the general population? And at what level? Of the county? The state? The country?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码输出了*图12.1*，它描述了所有种族的`is_recid=0`和`is_recid=1`的数量相等。然而，**其他**种族在数量上与其他种族不相等。顺便提一下，这个数据集版本将所有其他种族都归入了**其他**类别，选择不`upsample`
    **其他**或`downsample`其他两个种族以实现总数相等，是因为它们在被告人口中代表性较低。这种平衡选择是在这种情况下可以做的许多选择之一。从人口统计学角度看，这完全取决于你的数据应该代表什么。被告？囚犯？普通民众中的平民？以及在哪一层面？县一级？州一级？国家一级？
- en: 'The output can be seen here:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/B18406_12_01.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18406_12_01.png)'
- en: 'Figure 12.1: Distribution of 2-year recidivism (is_recid) by ethnicity'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1：按种族分布的两年再犯率（is_recid）
- en: Next, let’s compute how well each of our features monotonically correlates to
    the target. Spearman’s rank-order correlation will be instrumental in this chapter
    because it measures the monotonicity between two features. After all, one of the
    technical topics of this chapter is monotonic constraints, and the primary mission
    is to produce a significantly less biased model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们计算每个特征与目标变量单调相关性的程度。Spearman等级相关系数在本章中将起到关键作用，因为它衡量了两个特征之间的单调性。毕竟，本章的一个技术主题是单调约束，主要任务是产生一个显著减少偏差的模型。
- en: 'We first create a new DataFrame without `compas_score` (`recidivism_corr_df`).
    Using this DataFrame, we output a color-coded DataFrame with a `feature` column
    with the first 10 features’ names and another one with the Spearman coefficient
    (`correlation_to_target`) for all 10 features toward the 11th—the target variable.
    The code can be seen in the following snippet:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个新的DataFrame，其中不包含`compas_score`（`recidivism_corr_df`）。使用这个DataFrame，我们输出一个带有`feature`列的彩色DataFrame，其中包含前10个特征的名称，以及另一个带有所有10个特征与第11个特征（目标变量）的Spearman相关系数（`correlation_to_target`）。代码如下所示：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The preceding code outputs the DataFrame shown in *Figure 12.2*. The most correlated
    features are `priors_count` followed by `age`, the three juvenile counts, and
    `sex`. The coefficients for `c_charge_degree`, `days_b_screening_arrest`, `length_of_stay`,
    and `race` are negligible.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码输出了*图12.2*所示的DataFrame。最相关的特征是`priors_count`，其次是`age`、三个青少年计数和`sex`。`c_charge_degree`、`days_b_screening_arrest`、`length_of_stay`和`race`的系数可以忽略不计。
- en: 'The output can be seen here:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Table  Description automatically generated](img/B18406_12_02.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18406_12_02.png)'
- en: 'Figure 12.2: Spearman coefficients of all features toward the target, prior
    to feature engineering'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2：在特征工程之前，所有特征对目标变量的Spearman系数
- en: Next, we will learn how to use feature engineering to “bake in” some domain
    knowledge into the features.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何使用特征工程将一些领域知识“嵌入”到特征中。
- en: Placing guardrails with feature engineering
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用特征工程设置护栏
- en: In *Chapter 6*, *Anchors and Counterfactual Explanations*, we learned that besides
    `race`, the features most prominent in our explanations were `age`, `priors_count`,
    and `c_charge_degree`. Thankfully, the data is now balanced, so the racial bias
    attributed to this imbalance is now gone. However, through anchors and counterfactual
    explanations, we found some troubling inconsistencies. In the case of `age` and
    `priors_count`, these inconsistencies were due to how those features were distributed.
    We can correct issues with distribution through feature engineering, and, that
    way, ensure that a model doesn’t learn from uneven distributions. In `c_charge_degree`'s
    case, being categorical, it lacked a discernible order, and this lack of order
    created unintuitive explanations.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第6章*，*锚点和反事实解释*中，我们了解到除了`race`之外，在我们解释中最突出的特征是`age`、`priors_count`和`c_charge_degree`。幸运的是，数据现在已经平衡，因此这种不平衡导致的种族偏见现在已经消失。然而，通过锚点和反事实解释，我们发现了一些令人不安的不一致性。在`age`和`priors_count`的情况下，这些不一致性是由于这些特征的分布方式造成的。我们可以通过特征工程来纠正分布问题，从而确保模型不会从不均匀的分布中学习。在`c_charge_degree`的情况下，由于它是分类的，它缺乏可识别的顺序，这种缺乏顺序导致了不直观的解释。
- en: In this section, we will study **ordinalization**, **discretization**, and **interaction
    terms**, three ways in which you can place guardrails through feature engineering.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究**序列化**、**离散化**和**交互项**，这是通过特征工程设置护栏的三种方式。
- en: Ordinalization
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列化
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding code produced the following output:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了以下输出：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Each of the charge degrees corresponds to the charge’s gravity. There’s an order
    to these gravities, which is lost by using a categorical feature. We can easily
    fix this by replacing each category with a corresponding order.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 每个电荷度数对应电荷的重力。这些重力有一个顺序，使用分类特征时会丢失。我们可以通过用相应的顺序替换每个类别来轻松解决这个问题。
- en: 'We can put a lot of thought into what this order should be. For instance, we
    could look at sentencing laws or guidelines—there are minimum or maximum years
    of prison enforced for different degrees. We could also look at statistics on
    how violent these people are on average and assign this information to the charge
    degree. There’s potential for bias in every decision such as this, and if we don’t
    have substantial evidence to support it, it’s best to use a sequence of integers.
    So, that’s what we are going to do now. We will create a dictionary (`charge_degree_code_rank`)
    that maps the degrees to a number corresponding to a rank of gravity, from low
    to high. Then, we can use the `pandas` `replace` function to use the dictionary
    to perform the replacements. The code can be seen in the following snippet:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对此顺序进行很多思考。例如，我们可以查看判决法或指南——对于不同的程度，实施了最低或最高的监禁年数。我们还可以查看这些人的平均暴力统计数据，并将这些信息分配给电荷度数。每个此类决策都存在潜在的偏见，如果没有充分的证据支持它，最好使用整数序列。所以，我们现在要做的就是创建一个字典（`charge_degree_code_rank`），将度数映射到从低到高对应的重力等级的数字。然后，我们可以使用`pandas`的`replace`函数使用这个字典来进行替换。以下代码片段中可以看到代码：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'One way to assess how this order corresponds to recidivism probability is through
    a line plot that shows how it changes as the charge degree increases. We can use
    a function called `plot_prob_progression` for this, which takes a continuous feature
    in the first argument (`c_charge_degree`) to measure against probability for a
    binary feature in the second (`is_recid`). It can split the continuous feature
    by intervals (`x_intervals`), and even use quantiles (`use_quantiles`). Lastly,
    you can define axis labels and titles. The code can be seen in the following snippet:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 评估这种顺序如何对应再犯概率的一种方法是通过一条线图，显示随着电荷度数的增加，它如何变化。我们可以使用一个名为`plot_prob_progression`的函数来做这件事，它接受一个连续特征作为第一个参数（`c_charge_degree`），以衡量一个二元特征的概率（`is_recid`）。它可以按区间（`x_intervals`）分割连续特征，甚至可以使用分位数（`use_quantiles`）。最后，你可以定义轴标签和标题。以下代码片段中可以看到代码：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding code generates the plot in *Figure 12.3*. As the now-ranked charge
    degree increases, the tendency is that the probability of 2-year recidivism decreases,
    except for rank 1\. Below the probability, there are bar charts that show the
    distribution of the observations over every rank. Because it is so unevenly distributed,
    you should take the tendency with a grain of salt. You’ll notice that some ranks,
    such as 0, 8, and 13–15, aren’t in the plot because the charge-degree categories
    existed in the criminal justice system but weren’t in the data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了图12.3中的图表。随着现在排名的电荷度数的增加，趋势是2年再犯的概率降低，除了排名1。在概率下方有柱状图显示了每个排名的观测值的分布。由于分布非常不均匀，你应该谨慎对待这种趋势。你会注意到一些排名，如0、8和13-15，没有在图表中，因为电荷度数的类别存在于刑事司法系统中，但在数据中不存在。
- en: 'The output can be seen here:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Chart, line chart  Description automatically generated](img/B18406_12_03.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成的描述](img/B18406_12_03.png)'
- en: 'Figure 12.3: Probability progression plot by charge degree'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3：按电荷度数的概率进展图
- en: Feature engineering-wise, we can’t do much more to improve `c_charge_degree`
    because it already represents discrete categories now enhanced with an order.
    Any further transformations could produce a significant loss of information unless
    we have evidence to suggest otherwise. On the other hand, continuous features
    inherently have an order; however, a problem may arise from the level of precision
    they carry because small differences may not be meaningful but the data may tell
    the model otherwise. Uneven distributions and counterintuitive interactions only
    exacerbate this problem.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征工程方面，我们无法做更多的事情来改进 `c_charge_degree`，因为它已经代表了现在带有顺序的离散类别。除非我们有证据表明否则，任何进一步的转换都可能导致信息的大量丢失。另一方面，连续特征本质上具有顺序；然而，由于它们携带的精度水平，可能会出现问题。因为小的差异可能没有意义，但数据可能告诉模型否则。不均匀的分布和反直觉的交互只会加剧这个问题。
- en: Discretization
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 离散化
- en: To understand how to discretize our `age` continuous feature best, let’s try
    two different approaches. We can use equal-sized discretization, also known as
    fixed-width bins or intervals, which means the size of the bin is determined by
    ![](img/B18406_12_001.png), where *N* is the number of bins. Another way to do
    this is with equal-frequency discretization, also known as quantiles, which ensures
    that each bin has approximately the same number of observations. Although, sometimes,
    given the histogram’s skewed nature, it may be impossible to split them *N* ways,
    so you may end up with *N-1* or *N-2* quantiles.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解如何最佳地离散化我们的`年龄`连续特征，让我们尝试两种不同的方法。我们可以使用等宽离散化，也称为固定宽度箱或区间，这意味着箱的大小由 ![](img/B18406_12_001.png)
    决定，其中 *N* 是箱的数量。另一种方法是使用等频离散化，也称为分位数，这确保每个箱大约有相同数量的观测值。尽管如此，有时由于直方图的偏斜性质，可能无法以
    *N* 种方式分割它们，因此你可能最终得到 *N-1* 或 *N-2* 个分位数。
- en: 'It is easy to compare both approaches with `plot_prob_progression`, but this
    time, we produce two plots, one with fixed-width bins (`use_quantiles=False`)
    and another with quantiles (`use_quantiles=True`). The code can be seen in the
    following snippet:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `plot_prob_progression` 比较这两种方法很容易，但这次我们生成了两个图表，一个使用固定宽度箱（`use_quantiles=False`），另一个使用分位数（`use_quantiles=True`）。代码可以在下面的代码片段中看到：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output can be seen here:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/B18406_12_04.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_12_04.png)'
- en: 'Figure 12.4: Comparing two discretization approaches for age'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4：比较两种年龄离散化方法
- en: 'It is easy to observe why using quantiles to bin the feature is a better approach.
    We can take `age` and engineer a new feature called `age_group`. The `qcut` `pandas`
    function can perform quantile-based discretization. The code can be seen in the
    following snippet:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易观察到为什么使用分位数对特征进行箱化是一个更好的方法。我们可以将 `age` 工程化为一个新的特征，称为 `age_group`。`pandas`
    的 `qcut` 函数可以执行基于分位数的离散化。代码可以在下面的代码片段中看到：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: So, we now have discretized `age` into `age_group`. However, it must be noted
    that many model classes discretize automatically, so why bother? Because it allows
    you to control its effects. Otherwise, the model might decide on bins that don’t
    ensure monotonicity. For instance, maybe the model might always use 10 quantiles
    whenever possible. Still, if you attempt this level of granularity on `age` (`x_intervals=10`),
    you’ll end up with spikes in the probability progression. Our goal was to make
    sure that the models would learn that `age` and the incidence of `is_recid` have
    a monotonic relationship, and we cannot ascertain this if we allow the model to
    choose bins that may or may not achieve the same goal.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们现在已经将`age`离散化为`age_group`。然而，必须注意的是，许多模型类会自动进行离散化，那么为什么还要这么做呢？因为这允许你控制其影响。否则，模型可能会选择不保证单调性的桶。例如，模型可能会在可能的情况下始终使用10个分位数。尽管如此，如果你尝试在`age`上使用这种粒度（`x_intervals=10`），你最终会在概率进展中遇到峰值。我们的目标是确保模型会学习到`age`和`is_recid`的发病率之间存在单调关系，如果我们允许模型选择可能或可能不达到相同目标的桶，我们就无法确定这一点。
- en: We will remove `age` because `age_group` has everything we need. But wait—you
    ask—won’t we lose some important information by removing this variable? Yes, but
    only because of its interaction with `priors_count`. So, before we drop any features,
    let’s examine this relationship and realize how, through creating an interaction
    term, we can retain some of the information lost through the removal of `age`,
    while keeping the interaction.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将移除`age`，因为`age_group`包含了我们所需的所有信息。但是等等——你可能会问——移除这个变量会不会丢失一些重要信息？是的，但仅仅是因为它与`priors_count`的交互作用。所以，在我们丢弃任何特征之前，让我们检查这种关系，并意识到通过创建交互项，我们如何通过移除`age`来保留一些丢失的信息，同时保持交互。
- en: Interaction terms and non-linear transformations
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交互项和非线性变换
- en: 'We already know from *Chapter 6*, *Anchors and Counterfactual Explanations*,
    that `age` and `priors_count` are two of the most important predictors, and we
    can observe how, together, they impact the incidence of recidivism (`is_recid`)
    with `plot_prob_contour_map`. This function produces contour lines with color-coded
    contour regions, signifying different magnitudes. They are useful in topography,
    where they show elevation heights. In machine learning, they can show a two-dimensional
    plane representing feature interaction with a metric. In this case, the dimensions
    are `age` and `priors_count`, and the metric is the incidence of recidivism. The
    arguments received by this function are the same as `plot_prob_progression` except
    that it takes two features corresponding to the *x* axis and *y* axis. The code
    can be seen in the following snippet:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从*第6章*，*锚点和反事实解释*中已经知道，`age`和`priors_count`是最重要的预测因子之一，我们可以观察到它们如何一起影响再犯的发病率（`is_recid`），使用`plot_prob_contour_map`。这个函数产生带有彩色等高线区域的等高线，表示不同的幅度。它们在地理学中很有用，可以显示海拔高度。在机器学习中，它们可以显示一个二维平面，表示特征与度量之间的交互。在这种情况下，维度是`age`和`priors_count`，度量是再犯的发病率。这个函数接收到的参数与`plot_prob_progression`相同，只是它接受对应于*x*轴和*y*轴的两个特征。代码可以在下面的代码片段中看到：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output can be seen here:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/B18406_12_05.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18406_12_05.png)'
- en: 'Figure 12.5: Recidivism probability contour map for age and priors_count'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5：年龄和先前的计数再犯概率等高线图
- en: 'We can now engineer an interaction term that includes both features. Even though
    the contour map discretized the features to observe a smoother progression, we
    do not need to discretize this relationship. What makes sense is to make it a
    ratio of `priors_count` per year. But years since when? Years since the defendants
    were an adult, of course. But to obtain the years, we cannot use `age - 18` because
    this would lead to zero division, so we will use `17` instead. There are, of course,
    many ways to do this. The best way would be if we hypothetically had ages with
    decimals, and by deducting 18, we could compute a very precise `priors_per_year`
    ratio. Still, unfortunately, we don’t have that. You can see the code in the following
    snippet:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以构建一个包含两个特征的交互项。即使等高线图将特征离散化以观察更平滑的进展，我们也不需要将这种关系离散化。有意义的是将其作为每年`priors_count`的比率。但是从哪一年开始算起？当然是被告成年以来的年份。但是要获得这些年份，我们不能使用`age
    - 18`，因为这会导致除以零，所以我们将使用`17`代替。当然，有许多方法可以做到这一点。最好的方法是我们假设年龄有小数，通过减去18，我们可以计算出非常精确的`priors_per_year`比率。然而，不幸的是，我们并没有这样的数据。你可以在下面的代码片段中看到代码：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Black-box models typically find interaction terms automatically. For instance,
    hidden layers in a neural network have all the first-order interactions, but because
    of the non-linear activations, it is not limited to linear combinations. However,
    “manually” defining interaction terms and even non-linear transformation allows
    us to interpret these better once the model has been fitted. Furthermore, we can
    also use monotonic constraints on them, precisely what we will do later with `priors_per_year`.
    For now, let’s examine if its monotonicity holds with `plot_prob_progression`.
    Have a look at the following code snippet:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 黑盒模型通常会自动找到交互项。例如，神经网络中的隐藏层具有所有一阶交互项，但由于非线性激活，它并不仅限于线性组合。然而，“手动”定义交互项甚至非线性转换，一旦模型拟合完成，我们可以更好地解释这些交互项。此外，我们还可以对它们使用单调约束，这正是我们稍后将在`priors_per_year`上所做的。现在，让我们检查其单调性是否通过`plot_prob_progression`保持。查看以下代码片段：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preceding snippet outputs the progression in the following screenshot,
    which shows how the new feature is almost monotonic:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段输出以下截图，显示了新特征的几乎单调进展：
- en: '![Chart, line chart  Description automatically generated](img/B18406_12_06.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成的描述](img/B18406_12_06.png)'
- en: 'Figure 12.6: Probability progression for priors_per_year'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6：`priors_per_year`的先验概率进展
- en: 'The reason `priors_per_year` isn’t more monotonic is how sparse the over-3.0
    `priors_per_year` interval is. It would therefore be very unfair to these few
    defendants to enforce monotonicity on this feature because they present a 75%
    risk dip. One way to tackle this is to shift them over to the left, by setting
    `priors_per_year = -1` for these observations, as illustrated in the following
    code snippet:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`priors_per_year`不是更单调的原因是3.0以上的`priors_per_year`区间非常稀疏。因此，对这些少数被告强制执行该特征的单调性将非常不公平，因为他们呈现了75%的风险下降。解决这一问题的方法之一是将它们左移，将这些观察结果中的`priors_per_year`设置为`-1`，如下面的代码片段所示：'
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Of course, this shift changes the interpretation of the feature ever so slightly,
    knowing that the few values of `-1` really mean over `3`. Now, let’s generate
    another contour map, but this time, between `age_group` and `priors_per_year`.
    The latter will be discretized in quantiles (`y_intervals=6, use_quantiles=True`)
    so that the probability of recidivism is more easily observed. The code is shown
    in the following snippet:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这种移动会略微改变特征的解释，考虑到`-1`的少数值实际上意味着超过`3`。现在，让我们生成另一个等高线图，但这次是在`age_group`和`priors_per_year`之间。后者将按分位数（`y_intervals=6,
    use_quantiles=True`）进行离散化，以便更容易观察到再犯概率。以下代码片段显示了代码：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output can be seen here:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/B18406_12_07.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片，B18406_12_07.png]'
- en: 'Figure 12.7: Recidivism probability contour map for age_group and priors_per_year'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7：`age_group`和`priors_per_year`的再犯概率等高线图
- en: Almost everything is ready, but `age_group` is still categorical, so we have
    to encode it to take a numerical form.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎一切准备就绪，但`age_group`仍然是分类的，所以我们必须将其编码成数值形式。
- en: Categorical encoding
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类别编码
- en: The best categorical encoding method for `age_group` is **ordinal encoding**,
    also known as **label encoding**, because it will retain its order. We should
    also encode the other two categorical features in the dataset, `sex` and `race`.
    For `sex`, ordinal encoding converts it into binary form—equivalent to **dummy
    encoding**. On the other hand, `race` is a tougher call because it has three categories,
    and using ordinal encoding could lead to bias. However, whether to use **one-hot
    encoding** instead depends on which model classes you are using. Tree-based models
    have no bias issues with ordinal features but other models that operate with weights
    on a feature basis, such as neural networks and logistic regression, could be
    biased by this order.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`age_group`的最佳分类编码方法是**序数编码**，也称为**标签编码**，因为它会保留其顺序。我们还应该对数据集中的其他两个分类特征进行编码，即`sex`和`race`。对于`sex`，序数编码将其转换为二进制形式——相当于**虚拟编码**。另一方面，`race`是一个更具挑战性的问题，因为它有三个类别，使用序数编码可能会导致偏差。然而，是否使用**独热编码**取决于你使用的模型类别。基于树的模型对序数特征没有偏差问题，但其他基于特征权重的模型，如神经网络和逻辑回归，可能会因为这种顺序而产生偏差。
- en: Considering that the dataset has been balanced on `race`, there’s a lower risk
    of this happening and we will remove this feature later anyway, so we will go
    ahead and ordinal-encode it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到数据集已经在`种族`上进行了平衡，因此这种情况发生的风险较低，我们稍后无论如何都会移除这个特征，所以我们将继续对其进行序数编码。
- en: 'To ordinal-encode the three features, we will use scikit-learn’s `OrdinalEncoder`.
    We can use its `fit_transform` function to fit and transform the features in one
    fell swoop. Then, we should also delete unnecessary features while we are at it.
    Have a look at the following code snippet:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对三个特征进行序数编码，我们将使用scikit-learn的`OrdinalEncoder`。我们可以使用它的`fit_transform`函数一次性拟合和转换特征。然后，我们还可以趁机删除不必要的特征。请看下面的代码片段：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now, we aren’t entirely done yet. We still ought to initialize our random seeds
    and train/test split our data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们还没有完全完成。我们仍然需要初始化我们的随机种子并划分我们的数据为训练集和测试集。
- en: Other preparations
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他准备工作
- en: 'The next preparations are straightforward. To ensure reproducibility, let’s
    set a random seed everywhere it is needed, then set our `y` as `is_recid` and
    `X` as every other feature. We perform `train_test_split` on those two. Lastly,
    we reconstruct the `recidivism_df` DataFrame with the `X` followed by the `y`.
    The only reason for this is so that `is_recid` is the last column, which will
    help with the next step. The code can be seen here:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步的准备工作很简单。为了确保可重复性，让我们在需要的地方设置随机种子，然后将我们的`y`设置为`is_recid`，将`X`设置为其他所有特征。我们对这两个进行`train_test_split`。最后，我们使用`X`后跟`y`重建`recidivism_df`
    DataFrame。这样做只有一个原因，那就是`is_recid`是最后一列，这将有助于下一步。代码可以在这里看到：
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We will now verify that Spearman’s correlations have improved where needed
    and stay the same otherwise. Have a look at the following code snippet:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将验证Spearman的相关性是否在需要的地方有所提高，在其他地方保持不变。请看下面的代码片段：
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The preceding code outputs the DataFrame shown in *Figure 12.8*. Please compare
    it with *Figure 12.2*. Note that discretized in quantiles, `age` is slightly less
    monotonically correlated with the target. Once ordinalized, `c_charge_degree`
    is also much more correlated, and `priors_per_year` has also improved over `priors_count`.
    No other features should have been affected, including those that have the lowest
    coefficients.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码输出了*图12.8*中所示的DataFrame。请将其与*图12.2*进行比较。请注意，在分位数离散化后，`age`与目标变量的单调相关性略有降低。一旦进行序数编码，`c_charge_degree`的相关性也大大提高，而`priors_per_year`相对于`priors_count`也有所改善。其他特征不应受到影响，包括那些系数最低的特征。
- en: 'The output can be seen here:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Table  Description automatically generated](img/B18406_12_08.png)Figure 12.8:
    Spearman correlation coefficients of all features toward the target (after feature
    engineering)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18406_12_08.png)图12.8：所有特征与目标变量的Spearman相关系数（特征工程后）'
- en: Features with the lowest coefficients are likely also unnecessary in a model,
    but we will let the model decide if they are useful through regularization. That’s
    what we will do next.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 系数最低的特征在模型中可能也是不必要的，但我们将让模型通过正则化来决定它们是否有用。这就是我们接下来要做的。
- en: Tuning models for interpretability
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整模型以提高可解释性
- en: Traditionally, regularization was only achieved by imposing penalty terms such
    as **L1**, **L2**, or **elastic net** on the coefficients or weights, which shrink
    the impact of the least relevant features. As seen in the *Embedded methods* section
    of *Chapter 10*, *Feature Selection and Engineering for Interpretability*, this
    form of regularization results in feature selection while also reducing overfitting.
    And this brings us to another broader definition of regularization, which does
    not require a penalty term. Often, this comes as imposing a limitation, or a stopping
    criterion that forces the model to curb its complexity.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，正则化是通过在系数或权重上施加惩罚项（如**L1**、**L2**或**弹性网络**）来实现的，这会减少最不相关特征的影响。如第10章“可解释性特征选择和工程”部分的*嵌入式方法*中所示，这种正则化形式在特征选择的同时也减少了过拟合。这使我们来到了正则化的另一个更广泛的概念，它不需要惩罚项。通常，这相当于施加限制或停止标准，迫使模型限制其复杂性。
- en: In addition to regularization, both in its narrow (penalty-based) and broad
    sense (overfitting methods), there are other methods that tune a model for interpretability—that
    is, improve the fairness, accountability, and transparency of a model through
    adjustments to the training process. For instance, the class imbalance hyperparameters
    we discussed in *Chapter 10*, *Feature Selection and Engineering for Interpretability*,
    and the adversarial debiasing in *Chapter 11*, *Bias Mitigation and Causal Inference
    Methods*, enhance fairness. Also, the constraints we will study further in this
    chapter have potential benefits for fairness, accountability, and transparency.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 除了正则化，无论是其狭义（基于惩罚）还是广义（过拟合方法），还有其他方法可以调整模型以提高可解释性——也就是说，通过调整训练过程来提高模型的公平性、责任性和透明度。例如，我们在第10章*特征选择和可解释性工程*中讨论的类别不平衡超参数，以及第11章*偏差缓解和因果推断方法*中的对抗性偏差，都有助于提高公平性。此外，我们将在本章进一步研究的约束条件对公平性、责任性和透明度也有潜在的好处。
- en: There are so many different tuning possibilities and model classes. As stated
    at the beginning of the chapter, we will focus on interpretability-related options,
    but will also limit the model classes to a popular deep learning library (Keras),
    a handful of popular tree ensembles (XGBoost, Random Forest, and so on), **Support
    Vector Machines** (**SVMs**), and logistic regression. Except for the last one,
    these are all considered black-box models.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的调整可能性和模型类别。如本章开头所述，我们将关注与可解释性相关的选项，但也将模型类别限制在流行的深度学习库（Keras）、一些流行的树集成（XGBoost、随机森林等）、**支持向量机**（**SVMs**）和逻辑回归。除了最后一个，这些都被认为是黑盒模型。
- en: Tuning a Keras neural network
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整Keras神经网络
- en: 'For a Keras model, we will choose the best regularization parameters through
    hyperparameter tuning and **stratified K-fold cross-validation**. We will do this
    using the following steps:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Keras模型，我们将通过超参数调整和**分层K折交叉验证**来选择最佳正则化参数。我们将按照以下步骤进行：
- en: First, we need to define the model and the parameters to tune.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要定义模型和要调整的参数。
- en: Then, we run the tuning.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们进行调整。
- en: Next, we examine its results.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们检查其结果。
- en: Finally, we extract the best model and evaluate its predictive performance.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们提取最佳模型并评估其预测性能。
- en: Let’s look at each of these steps in detail.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看这些步骤。
- en: Defining the model and parameters to tune
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义模型和要调整的参数
- en: 'The first thing we ought to do is create a function (`build_nn_mdl`) to build
    and compile a regularizable Keras model. The function takes arguments that will
    help tune it. It takes a tuple with the number of neurons in hidden layers (`hidden_layer_sizes`),
    and a value of L1 (`l1_reg`) and L2 (`l1_reg`) regularization to apply on the
    layer’s kernel. Lastly, it takes the `dropout` parameter, which, unlike L1 and
    L2 penalties, is a **stochastic regularization method** because it employs random
    selection. Have a look at the following code snippet:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先应该创建一个函数（`build_nn_mdl`）来构建和编译一个可正则化的Keras模型。该函数接受一些参数，以帮助调整模型。它接受一个包含隐藏层中神经元数量的元组（`hidden_layer_sizes`），以及应用于层核的L1（`l1_reg`）和L2（`l1_reg`）正则化值。最后，它还接受`dropout`参数，与L1和L2惩罚不同，它是一种**随机正则化方法**，因为它采用随机选择。请看以下代码片段：
- en: '[PRE21]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The previous function initializes the model (`nn_model`) as a `Sequential` model
    with an input layer that corresponds to the number of features in training data,
    and a `Normalization()` layer that standardizes the input. Then, if either penalty
    term is over zero, it will set a dictionary (`reg_args`) with the `kernel_regularizer`
    assigned to `tf.keras.regularizers.l1_l2` initialized with these penalties. Once
    it adds the hidden (`Dense`) layers with the corresponding `hidden_layer_size`,
    it will pass the `reg_args` dictionary as extra arguments to each layer. After
    all hidden layers have been added, it will optionally add the `Dropout` layer
    and the final `Dense` layer with the `sigmoid` activation for the output. The
    model is then compiled with `binary_crossentropy` and an `Adam` optimizer with
    a slow learning rate and is set to monitor `accuracy` and `auc` metrics.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的功能将模型（`nn_model`）初始化为一个 `Sequential` 模型，其输入层与训练数据中的特征数量相对应，并添加一个 `Normalization()`
    层来标准化输入。然后，如果任一惩罚项超过零，它将设置一个字典（`reg_args`），将 `kernel_regularizer` 分配给 `tf.keras.regularizers.l1_l2`
    并用这些惩罚项初始化。一旦添加了相应的 `hidden_layer_size` 的隐藏（`Dense`）层，它将 `reg_args` 字典作为额外参数传递给每个层。在添加所有隐藏层之后，它可以选择添加
    `Dropout` 层和具有 `sigmoid` 激活的最终 `Dense` 层。然后，模型使用 `binary_crossentropy` 和具有较慢学习率的
    `Adam` 优化器编译，并设置为监控 `accuracy` 和 `auc` 指标。
- en: Running the hyperparameter tuning
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行超参数调整
- en: 'Now that we have defined the model and parameters to tune, we initialize the
    `RepeatedStratifiedKFold` cross-validator, which splits (`n_splits`) the training
    data in five a total of three times (`n_repeats`), using different randomization
    in each repetition. We then create a grid (`nn_grid`) for the grid-search hyperparameter
    tuning. It’s testing only two possible options for three of the parameters (`l1_reg`,
    `l2_reg`, and `dropout`), which will result in ![](img/B18406_12_002.png) combinations.
    We will use a scikit-learn wrapper (`KerasClassifier`) for our model to be compatible
    with the scikit-learn grid search. Speaking of which, we next initialize `GridSearchCV`,
    which, using the Keras model (`estimator`), performs a cross-validated (`cv`)
    grid search (`param_grid`). We want it to choose the best parameters based on
    precision (`scoring`) and not raise errors in the process (`error_score=0`). Finally,
    we fit `GridSearchCV` as we would with any Keras model, passing `X_train`, `y_train`,
    `epochs`, and `batch_size`. The code can be seen in the following snippet:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了模型和要调整的参数，我们初始化了 `RepeatedStratifiedKFold` 交叉验证器，它将训练数据分成五份，总共重复三次（`n_repeats`），每次重复使用不同的随机化。然后我们为网格搜索超参数调整创建一个网格（`nn_grid`）。它只测试三个参数（`l1_reg`、`l2_reg`
    和 `dropout`）的两个可能选项，这将产生 ![](img/B18406_12_002.png) 种组合。我们将使用 scikit-learn 包装器（`KerasClassifier`）来使我们的模型与
    scikit-learn 网格搜索兼容。说到这一点，我们接下来初始化 `GridSearchCV`，它使用 Keras 模型（`estimator`）执行交叉验证网格搜索（`param_grid`）。我们希望它根据精度（`scoring`）选择最佳参数，并且在过程中不抛出错误（`error_score=0`）。最后，我们像使用任何
    Keras 模型一样拟合 `GridSearchCV`，传递 `X_train`、`y_train`、`epochs` 和 `batch_size`。代码可以在以下代码片段中看到：
- en: '[PRE22]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Next, we can examine the results of our grid search.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以检查网格搜索的结果。
- en: Examining the results
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查结果
- en: 'Once the grid search has been completed, you can output the best parameters
    in a dictionary with this command: `print(nn_grid_result.best_params_)`. Or you
    can place all the results into a DataFrame, sort them by the highest precision
    (`sort_values`), and output them as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成网格搜索，你可以使用以下命令输出最佳参数：`print(nn_grid_result.best_params_)`。或者，你可以将所有结果放入一个
    DataFrame 中，按最高精度（`sort_values`）排序，并按以下方式输出：
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output can be seen here:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Table  Description automatically generated](img/B18406_12_09.png)Figure 12.9:
    Results for cross-validated grid search for a neural net model'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '![表格描述自动生成](img/B18406_12_09.png)图 12.9：神经网络模型交叉验证网格搜索的结果'
- en: Evaluating the best model
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估最佳模型
- en: 'Another important element that the grid search produced is the best-performing
    model (`nn_grid_result.best_estimator_`). We can create a dictionary to store
    all the models we will fit in this chapter (`fitted_class_mdls`) and then, using
    `evaluate_class_mdl`, evaluate this regularized Keras model and keep the evaluation
    in the dictionary at the same time. Have a look at the following code snippet:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索产生的另一个重要元素是表现最佳模型（`nn_grid_result.best_estimator_`）。我们可以创建一个字典来存储我们将在本章中拟合的所有模型（`fitted_class_mdls`），然后使用
    `evaluate_class_mdl` 评估这个正则化的 Keras 模型，并将评估结果同时保存在字典中。请查看以下代码片段：
- en: '[PRE24]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output can be seen here:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Chart, treemap chart  Description automatically generated](img/B18406_12_10.png)Figure
    12.10: Evaluation of the regularized Keras model'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图表，树状图  自动生成的描述](img/B18406_12_10.png)图12.10：正则化Keras模型的评估'
- en: Calibrating the class balance can be improved even further by employing a custom
    loss function or class weights, as we will do later. Next, we will cover how to
    tune other model classes.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用自定义损失函数或类权重，可以进一步校准类平衡，正如我们稍后将要做的。接下来，我们将介绍如何调整其他模型类。
- en: Tuning other popular model classes
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整其他流行模型类
- en: In this section, we will fit many different models, both unregularized and regularized.
    To this end, we will pick from a wide selection of parameters that perform penalized
    regularization, control overfitting through other means, and account for class
    imbalance.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将拟合许多不同的模型，包括未正则化和正则化的模型。为此，我们将从广泛的参数中选择，这些参数执行惩罚正则化，通过其他方式控制过拟合，并考虑类别不平衡。
- en: A quick introduction to relevant model parameters
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关模型参数的简要介绍
- en: For your reference, there are two tables with parameters used to tune many popular
    models. These have been split into two parts. Part A (*Figure 12.11*) has five
    scikit-learn models with penalty regularization. Part B (*Figure 12.12*) shows
    all the tree ensembles, including scikit-learn’s Random Forest models and models
    from the most popular boosted-tree libraries (XGBoost, LightGBM, and CatBoost).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 供您参考，有两个表格包含用于调整许多流行模型的参数。这些已经被分为两部分。Part A（图12.11）包含五个具有惩罚正则化的scikit-learn模型。Part
    B（图12.12）显示了所有树集成，包括scikit-learn的随机森林模型和来自最受欢迎的增强树库（XGBoost、LightGBM和CatBoost）的模型。
- en: 'Part A can be seen here:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Part A 可以在这里查看：
- en: '![Table, calendar  Description automatically generated](img/B18406_12_11.png)Figure
    12.11: Tuning parameters for penalty-regularized scikit-learn models'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '![表格，日历  自动生成的描述](img/B18406_12_11.png)图12.11：惩罚正则化scikit-learn模型的调整参数'
- en: 'In *Figure 12.11*, you can observe models in the columns and corresponding
    parameter names in the rows with their default values to the right. In between
    the parameter name and default value, there’s a plus or minus sign indicating
    whether changing the defaults in one direction or another should make the model
    more conservative. These parameters are also grouped by the following categories:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在图12.11中，您可以在列中观察到模型，在行中观察到相应的参数名称及其默认值在右侧。在参数名称和默认值之间，有一个加号或减号，表示是否改变默认值的一个方向或另一个方向应该使模型更加保守。这些参数还按以下类别分组：
- en: '**algorithm**: Some training algorithms are less prone to overfitting, but
    this often depends on the data.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法**：一些训练算法不太容易过拟合，但这通常取决于数据。'
- en: '**regularization**: Only in the stricter sense. In other words, parameters
    that control penalty-based regularization.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化**：仅在更严格的意义上。换句话说，控制基于惩罚的正则化的参数。'
- en: '**iterations**: This controls how many training rounds, iterations, or epochs
    are performed. Adjusting this in one direction or another can impact overfitting.
    In tree-based models, the number of estimators or trees is what’s analogous.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代**：这控制执行多少个训练轮次、迭代或epoch。调整这个方向或另一个方向可能会影响过拟合。在基于树的模型中，估计器或树的数量是类似的。'
- en: '**learning rate**: This controls how quickly the learning happens. It works
    in tandem with iterations. The lower the learning rate, the more iterations are
    needed to optimize the objective function.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**：这控制学习发生的速度。它与迭代一起工作。学习率越低，需要的迭代次数越多以优化目标函数。'
- en: '**early stopping**: These parameters control when to stop the training. This
    allows you to prevent your model from overfitting to training data.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提前停止**：这些参数控制何时停止训练。这允许您防止您的模型对训练数据过拟合。'
- en: '**class imbalance**: For most models, this penalizes misclassifications on
    smaller classes in the loss function, and for tree-based models, in particular,
    it is used to reweight the splitting criterion. Either way, it only works with
    classifiers.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别不平衡**：对于大多数模型，这在损失函数中惩罚了较小类别的误分类，对于基于树的模型，特别是这样，它被用来重新加权分割标准。无论如何，它只与分类器一起工作。'
- en: '**sample weight**: We leveraged this one in *Chapter 11*, *Bias Mitigation
    and Causal Inference Methods*, to assign weights on a sample basis to mitigate
    bias.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本权重**：我们在第11章“偏差缓解和因果推断方法”中利用了这一点，根据样本分配权重以减轻偏差。'
- en: There are both classification and regression models in the headings, and they
    share the same parameters. Please note that scikit-learn’s `LinearRegression`
    isn’t featured under `LogisticRegression` because it doesn’t have built-in regularization.
    In any case, we will use only classification models in this section.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 标题中既有分类模型也有回归模型，并且它们共享相同的参数。请注意，scikit-learn的`LinearRegression`在`LogisticRegression`下没有特色，因为它没有内置的正则化。无论如何，我们将在本节中仅使用分类模型。
- en: 'Part B can be seen here:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: B部分可以在这里看到：
- en: '![Table, calendar  Description automatically generated](img/B18406_12_12.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![表格，日历  自动生成的描述](img/B18406_12_12.png)'
- en: '![Table, calendar  Description automatically generated](img/B18406_12_12.1.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![表格，日历  自动生成的描述](img/B18406_12_12.1.png)'
- en: 'Figure 12.12: Tuning parameters for tree-ensemble models'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.12：树集成模型的调整参数
- en: '*Figure 12.12* is very similar to *Figure 12.11* except that it has a few more
    parameter categories that are only available in tree ensembles, such as the following:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.12*与*图12.11*非常相似，除了它有更多仅在树集成中可用的参数类别，如下所示：'
- en: '**feature sampling**: This works by considering fewer features in node splits,
    nodes, or tree training. It is a stochastic regularization method because features
    are randomly selected.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征采样**：这种方法通过在节点分裂、节点或树训练中考虑较少的特征来实现。因为它随机选择特征，所以它是一种随机正则化方法。'
- en: '**tree size**: This constrains the tree either by maximum depth or maximum
    leaves, or some other parameter that restricts its growth, which, in turn, curbs
    overfitting.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树的大小**：这通过最大深度、最大叶子数或其他限制其增长的参数来约束树，从而反过来抑制过拟合。'
- en: '**splitting**: Any parameter that controls how nodes in the tree are split
    can indirectly impact overfitting.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分裂**：任何控制树中节点如何分裂的参数都可以间接影响过拟合。'
- en: '**bagging**: Also known as **bootstrap aggregating**, this starts by bootstrapping,
    which involves randomly taking samples from the training data to fit weak learners.
    This method reduces variance and helps with overfitting, and by extension, the
    corresponding sampling parameters are usually prominent in hyperparameter tuning.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**袋装**：也称为**自助聚合**，它首先通过自助采样开始，这涉及到从训练数据中随机抽取样本来拟合弱学习器。这种方法减少了方差，有助于减少过拟合，并且相应地，采样参数通常在超参数调整中很突出。'
- en: '**constraints**: We will explain these in further detail in the next section,
    but this maps how the features should be constrained to decrease or increase against
    the output. It can reduce overfitting in areas where data is very sparse. However,
    reducing overfitting is not usually the main goal, while interaction constraints
    can limit which features are allowed to interact.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**约束**：我们将在下一节中进一步详细解释这些内容，但这是如何将特征约束以减少或增加对输出的影响。它可以在数据非常稀疏的领域减少过拟合。然而，减少过拟合通常不是主要目标，而交互约束可以限制哪些特征可以交互。'
- en: Please note that parameters with an asterisk (`*`) in *Figure 12.12* denote
    those set in the `fit` function as opposed to those initialized with the model.
    Also, except for scikit-learn’s `RandomForest` models, all other parameters typically
    have many aliases. For these, we are using the scikit-learn wrapper functions,
    but all the parameters also exist in the native versions. We can’t possibly explain
    every model parameter here, but it is recommended that you go directly to the
    documentation for more insight into what each one does. The point of the section
    was to serve as a guide or reference.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*图12.12*中带有星号（`*`）的参数表示在`fit`函数中设置的，而不是用模型初始化的。此外，除了scikit-learn的`RandomForest`模型外，所有其他参数通常有许多别名。对于这些，我们使用scikit-learn的包装函数，但所有参数也存在于原生版本中。我们不可能在这里解释每个模型参数，但建议您直接查阅文档以深入了解每个参数的作用。本节的目的在于作为指南或参考。
- en: Next, we will take steps similar to what we did with the Keras model but for
    many different models at once, and, lastly, we will assess the best model for
    fairness.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将采取与我们对Keras模型所做类似的步骤，但一次针对许多不同的模型，最后我们将评估最适合公平性的最佳模型。
- en: Batch hyperparameter tuning models
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量超参数调整模型
- en: 'OK—so, now that we have taken a quick crash course on which levers we can pull
    to tune the models, let’s define a dictionary with all the models, as we’ve done
    in other chapters. This time, we have included a `grid` with some parameter values
    for a grid search. Have a look at the following code snippet:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 好的——既然我们已经快速了解了我们可以拉动的哪些杠杆来调整模型，那么让我们定义一个包含所有模型的字典，就像我们在其他章节中所做的那样。这次，我们包括了一个用于网格搜索的参数值的`grid`。看看下面的代码片段：
- en: '[PRE25]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The next step is to add a `for` loop to every model in the dictionary, then
    `deepcopy` it and `fit` it to produce a “base” unregularized model. Next, we produce
    an evaluation for it with `evaluate_class_mdl` and save it into the `fitted_class_mdls`
    dictionary we had previously created for the Keras model. Now, we need to produce
    the regularized version of the model. So, we do another `deepcopy` and follow
    the same steps we took with Keras to do the `RepeatedStratifiedKFold` cross-validated
    grid search with `GridSearchCV`, and we also evaluate in the same way, saving
    the results in the fitted model dictionary. The code is shown in the following
    snippet:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是为字典中的每个模型添加一个`for`循环，然后`deepcopy`它并使用`fit`来生成一个“基础”的非正则化模型。接下来，我们使用`evaluate_class_mdl`对其进行评估，并将其保存到我们之前为Keras模型创建的`fitted_class_mdls`字典中。现在，我们需要生成模型的正则化版本。因此，我们再次进行`deepcopy`，并遵循与Keras相同的步骤进行`RepeatedStratifiedKFold`交叉验证网格搜索，并且我们也以相同的方式进行评估，将结果保存到拟合模型字典中。代码如下所示：
- en: '[PRE26]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Once the code has finished, we can rank models by precision.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦代码执行完毕，我们可以根据精确度对模型进行排名。
- en: Evaluating models by precision
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 根据精确度评估模型
- en: 'We can extract the fitted model dictionary’s metrics and place them into a
    DataFrame with `from_dict`. We can then sort the models by their highest test
    precision and color code the two columns that matter the most, which are `precision_test`
    and `recall_test`. The code can be seen in the following snippet:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提取拟合模型字典的指标，并将它们放入一个DataFrame中，使用`from_dict`。然后我们可以根据最高的测试精确度对模型进行排序，并为最重要的两个列着色编码，这两个列是`precision_test`和`recall_test`。代码可以在下面的代码片段中看到：
- en: '[PRE27]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The preceding code will output the DataFrame shown in *Figure 12.13*. You can
    tell that regularized tree-ensemble models mostly rule the ranks, followed by
    their unregularized counterparts. The one exception is regularized Nu-SVC, which
    is number one, and its unregularized version is dead last!
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将输出*图12.13*所示的DataFrame。你可以看出，正则化树集成模型在排名中占据主导地位，其次是它们的非正则化版本。唯一的例外是正则化Nu-SVC，它排名第一，而它的非正则化版本排名最后！
- en: 'The output can be seen here:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Table  Description automatically generated](img/B18406_12_13.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18406_12_13.png)'
- en: 'Figure 12.13: Top models according to the cross-validated grid search'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.13：根据交叉验证网格搜索的顶级模型
- en: You will find that the Keras regularized neural network model has lower precision
    than regularized logistic regression, but higher recall. It’s true that we want
    to optimize for high precision because it impacts false positives, which we want
    to minimize, but precision can be at 100% and recall at 0%, and if that’s the
    case, your model is no good. At the same time, there’s fairness, which is about
    having a low false-positive rate but being equally distributed across races. So,
    there’s a balancing act, and chasing one metric won’t get us there.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，Keras正则化神经网络模型的精确度低于正则化逻辑回归，但召回率更高。确实，我们希望优化高精确度，因为它会影响假阳性，这是我们希望最小化的，但精确度可以达到100%，而召回率可以是0%，如果那样的话，你的模型就不好了。同时，还有公平性，这关乎于保持低假阳性率，并且在种族间均匀分布。因此，这是一个权衡的问题，追求一个指标并不能让我们达到目标。
- en: Assessing fairness for the highest-performing model
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估最高性能模型的公平性
- en: 'To determine how to proceed, we must first assess how our highest-performing
    model does in terms of fairness. We can do this with `compare_confusion_matrices`.
    As you would do with scikit-learn’s `confusion_matrix`, the first argument is
    the ground truth or target values (often known as `y_true`), and the second is
    the model’s predictions (often known as `y_pred`). The difference here is it takes
    two sets of `y_true` and `y_pred`, one corresponding to one segment of the observations
    and one to another. After these first four arguments, you give each segment a
    name, so this is what the following two arguments tell you. Lastly, `compare_fpr=True`
    ensures that it will compare the **False Positive Rate** (**FPR**) between both
    confusion matrices. Have a look at the following code snippet:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定如何进行下一步，我们必须首先评估我们的最高性能模型在公平性方面的表现。我们可以使用`compare_confusion_matrices`来完成这项工作。正如你使用scikit-learn的`confusion_matrix`一样，第一个参数是真实值或目标值（通常称为`y_true`），第二个是模型的预测值（通常称为`y_pred`）。这里的区别是它需要两组`y_true`和`y_pred`，一组对应于观察的一个部分，另一组对应于另一个部分。在这四个参数之后，你给每个部分起一个名字，所以这就是以下两个参数告诉你的内容。最后，`compare_fpr=True`确保它将比较两个混淆矩阵之间的**假阳性率**（**FPR**）。看看下面的代码片段：
- en: '[PRE28]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![Chart, treemap chart  Description automatically generated](img/B18406_12_14.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图表，树状图图表，描述自动生成](img/B18406_12_14.png)'
- en: 'Figure 12.14: Confusion matrices between races for the regularized CatBoost
    model'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.14：正则化CatBoost模型之间的混淆矩阵
- en: '*Figure 12.15* tells us that the FPRs are significantly lower for the regularized
    model. You can see the output here:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.15*告诉我们，正则化模型的FPR显著低于基础模型。您可以看到输出如下：'
- en: '![Chart, waterfall chart, treemap chart  Description automatically generated](img/B18406_12_15.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图表，瀑布图，树状图图表，描述自动生成](img/B18406_12_15.png)'
- en: 'Figure 12.15: Confusion matrices between races for the base CatBoost model'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.15：基础CatBoost模型之间的混淆矩阵
- en: However, the base model in *Figure 12.15* has an FPR ratio of 1.11 compared
    to 1.47 for the regularized model, which is significantly more despite the similar
    overall metrics. But when trying to achieve several goals at once, it’s hard to
    evaluate and compare models, and that’s what we will do in the next section.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如图12.15所示的基础模型与正则化模型的FPR比率为1.11，而正则化模型的FPR比率为1.47，尽管整体指标相似，但差异显著。但在尝试同时实现几个目标时，很难评估和比较模型，这就是我们将在下一节中要做的。
- en: Optimizing for fairness with Bayesian hyperparameter tuning and custom metrics
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用贝叶斯超参数调整和自定义指标优化公平性
- en: Our mission is to produce a model with high precision and good recall while
    maintaining fairness across different races. So, achieving this mission will require
    a custom metric to be designed.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的使命是生产一个具有高精确度和良好召回率，同时在不同种族间保持公平性的模型。因此，实现这一使命将需要设计一个自定义指标。
- en: Designing a custom metric
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设计一个自定义指标
- en: 'We could use the F1 score, but it treats precision and recall equally, so we
    will have to create a weighted metric. We can also factor in how precision and
    recall are distributed for each race. One way to do this is by using the standard
    deviation, which quantifies the variation in this distribution. To that end, we
    will penalize precision with half the intergroup standard deviation for precision,
    and we can call this penalized precision. The formula is shown here:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用F1分数，但它对精确度和召回率的处理是平等的，因此我们不得不创建一个加权指标。我们还可以考虑每个种族的精确度和召回率的分布情况。实现这一目标的一种方法是通过使用标准差，它量化了这种分布的变化。为此，我们将用精确度的一半作为组间标准差来惩罚精确度，我们可以称之为惩罚后的精确度。公式如下：
- en: '![](img/B18406_12_003.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图片，B18406_12_003.png]'
- en: 'We can do the same for recall, as illustrated here:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对召回率做同样的处理，如图所示：
- en: '![](img/B18406_12_004.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图片，B18406_12_004.png]'
- en: 'Then, we make a weighted average for penalized precision and recall where precision
    is worth twice as much as recall, as illustrated here:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们为惩罚后的精确度和召回率做一个加权平均值，其中精确度是召回率的两倍，如图所示：
- en: '![](img/B18406_12_005.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图片，B18406_12_005.png]'
- en: 'To compute this new metric, we will need to create a function that we can call
    `weighted_penalized_pr_average`. It takes `y_true` and `y_pred` as the predictive
    performance metrics. However, it also includes `X_group` with a `pandas` series
    or array containing the values for the group, and `group_vals` with a list of
    values that it will subset the predictions by. In this case, the group is `race`,
    which can be values from 0 to 2\. The function includes a `for` loop that iterates
    through these possible values, subsetting the predictions by each group. That
    way, it can compute precision and recall for each group. After this, the rest
    of the function simply performs the three mathematical operations outlined previously.
    The code can be seen in the following snippet:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算这个新指标，我们需要创建一个可以调用`weighted_penalized_pr_average`的函数。它接受`y_true`和`y_pred`作为预测性能指标。然而，它还包括`X_group`，它是一个包含组值的`pandas`序列或数组，以及`group_vals`，它是一个列表，它将根据这些值对预测进行子集划分。在这种情况下，组是`race`，可以是0到2的值。该函数包括一个`for`循环，遍历这些可能的值，通过每个组对预测进行子集划分。这样，它可以计算每个组的精确度和召回率。之后，函数的其余部分只是简单地执行之前概述的三个数学运算。代码可以在以下片段中看到：
- en: '[PRE29]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now, to put this function to work, we will need to run the tuning.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了使这个函数发挥作用，我们需要运行调整。
- en: Running Bayesian hyperparameter tuning
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行贝叶斯超参数调整
- en: '**Bayesian optimization** is a *global optimization method* that uses the posterior
    distribution of black-box objective functions and their continuous parameters.
    In other words, it sequentially searches the best parameters to test next based
    on past results. Unlike grid search, it doesn’t try fixed combinations of parameters
    on a grid but exploits what it already knows and explores the unknown.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝叶斯优化**是一种 *全局优化方法*，它使用黑盒目标函数的后验分布及其连续参数。换句话说，它根据过去的结果顺序搜索下一个要测试的最佳参数。与网格搜索不同，它不会在网格上尝试固定参数组合，而是利用它已经知道的信息并探索未知领域。'
- en: The `bayesian-optimization` library is model-agnostic. All it needs is a function
    and parameters with their bounds. It will explore values for those parameters
    within those bounds. The function takes those parameters and returns a number.
    This is the number, or target, that the Bayesian optimization algorithm will maximize.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`bayesian-optimization` 库是模型无关的。它所需的所有东西是一个函数以及它们的界限参数。它将在这些界限内探索这些参数的值。该函数接受这些参数并返回一个数字。这个数字，或目标，是贝叶斯优化算法将最大化的。'
- en: 'The following code is for the `objective` function, which initializes a `RepeatedStratifiedKFold`
    cross-validation with four splits and three repeats. It then iterates across the
    splits and fits the `CatBoostClassifier` with them. Lastly, it computes the `weighted_penalized_pr_average`
    custom metric for each model training and appends it to a list. Finally, the function
    returns the `median` of the custom metric for all 12 training samples. The code
    is shown in the following snippet:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是用于 `objective` 函数的，它使用四个分割和三个重复初始化一个 `RepeatedStratifiedKFold` 交叉验证。然后，它遍历分割并使用它们拟合
    `CatBoostClassifier`。最后，它计算每个模型训练的 `weighted_penalized_pr_average` 自定义指标并将其追加到一个列表中。最后，该函数返回所有
    12 个训练样本的自定义指标的中位数。代码在以下片段中显示：
- en: '[PRE30]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now that the function has been defined, running the Bayesian optimization process
    is straightforward. First, set the parameter-bounds dictionary (`pbounds`), initialize
    `BayesianOptimization` with the `hyp_catboost` function, and then run it with
    `maximize`. The `maximize` function takes `init_points`, which sets how many iterations
    it should run initially using random exploration. Then, `n_iter` is the number
    of optimization iterations it should perform to find the maximum value. We will
    set `init_points` and `n_iter` to `3` and `7`, respectively, because it could
    take a long time, but the larger these numbers, the better. The code can be seen
    in the following snippet:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在函数已经定义，运行贝叶斯优化过程很简单。首先，设置参数界限字典（`pbounds`），使用 `hyp_catboost` 函数初始化 `BayesianOptimization`，然后使用
    `maximize` 运行它。`maximize` 函数接受 `init_points`，它设置初始使用随机探索运行的迭代次数。然后，`n_iter` 是它应该执行的优化迭代次数以找到最大值。我们将
    `init_points` 和 `n_iter` 分别设置为 `3` 和 `7`，因为可能需要很长时间，但这些数字越大越好。代码可以在以下片段中看到：
- en: '[PRE31]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Once it’s finished, you can access the best parameters, like this:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，你可以访问最佳参数，如下所示：
- en: '[PRE32]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'It will return a dictionary with the parameters, as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 它将返回一个包含参数的字典，如下所示：
- en: '[PRE33]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now, let’s fit a model with these parameters and evaluate it.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用这些参数拟合一个模型并评估它。
- en: Fitting and evaluating a model with the best parameters
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用最佳参数拟合和评估模型
- en: 'Initializing `CatBoostClassifier` with these parameters is as simple as passing
    the `best_params` dictionary as an argument. Then, all you need to do is `fit`
    the model and evaluate it (`evaluate_class_mdl`). The code is shown in the following
    snippet:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些参数初始化 `CatBoostClassifier` 与将 `best_params` 字典作为参数传递一样简单。然后，你所需要做的就是 `fit`
    模型并评估它（`evaluate_class_mdl`）。代码在以下片段中显示：
- en: '[PRE34]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The preceding snippet outputs the following predictive performance metrics:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段输出了以下预测性能指标：
- en: '[PRE35]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'They are the highest `Accuracy_test`, `Precision_test`, and `Recall_test` metrics
    we have achieved so far. Let’s now see how the model fares with fairness using
    `compare_confusion_matrices`. Have a look at the following code snippet:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们迄今为止达到的最高 `Accuracy_test`、`Precision_test` 和 `Recall_test` 指标。现在让我们看看模型使用
    `compare_confusion_matrices` 进行公平性测试的表现。请看以下代码片段：
- en: '[PRE36]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The preceding code outputs *Figure 12.16*, which shows some of the best fairness
    metrics we have obtained so far, as you can see here:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码输出了 *图 12.16*，它显示了迄今为止我们获得的一些最佳公平性指标，如你所见：
- en: '![Chart  Description automatically generated](img/B18406_12_16.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成](img/B18406_12_16.png)'
- en: 'Figure 12.16: Comparison of confusion matrices between races for the optimized
    CatBoost model'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.16：优化后的 CatBoost 模型不同种族之间的混淆矩阵比较
- en: These results are good, but we cannot be completely assured that the model is
    not racially biased because the feature is still there. One way to measure its
    impact is through feature importance methods.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果很好，但我们不能完全确信模型没有种族偏见，因为特征仍然存在。衡量其影响的一种方法是通过特征重要性方法。
- en: Examining racial bias through feature importance
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过特征重要性来检查种族偏见
- en: 'Although CatBoost is our best-performing model in most metrics, including accuracy,
    precision, and F1 score, we are moving forward with XGBoost because CatBoost doesn’t
    support interaction constraints, which we will implement in the next section.
    But first, we will compare them both in terms of what they found important. Also,
    **SHapley Additive exPlanations** (**SHAP**) values provide a robust means to
    measure and visualize feature importance, so let’s compute them for our optimized
    CatBoost and regularized XGBoost models. To do so, we need to initialize `TreeExplainer`
    with each model and then use `shap_values` to produce the values for each, as
    illustrated in the following code snippet:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CatBoost在大多数指标上，包括准确率、精确率和F1分数，都是我们表现最好的模型，但我们正在使用XGBoost前进，因为CatBoost不支持交互约束，我们将在下一节中实现。但首先，我们将比较它们在发现哪些特征重要方面的差异。此外，**SHapley
    Additive exPlanations**（**SHAP**）值提供了一种稳健的方法来衡量和可视化特征重要性，因此让我们为我们的优化CatBoost和正则化XGBoost模型计算它们。为此，我们需要用每个模型初始化`TreeExplainer`，然后使用`shap_values`为每个模型生成值，如下面的代码片段所示：
- en: '[PRE37]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, we can generate two `summary_plot` plots side by side, using Matplotlib’s
    `subplot`, as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用Matplotlib的`subplot`功能并排生成两个`summary_plot`图，如下所示：
- en: '[PRE38]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output can be seen here:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/B18406_12_17.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_12_17.png)'
- en: 'Figure 12.17: SHAP summary plot for the regularized XGBoost and optimized CatBoost
    models'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.17：XGBoost正则化和CatBoost优化模型的SHAP总结图
- en: 'In any case, it makes sense to remove `race` from the training data, but we
    must first ascertain why the model thinks this is a critical feature. Have a look
    at the following code snippet:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，从训练数据中移除`race`是有意义的，但我们必须首先确定模型为什么认为这是一个关键特征。请看以下代码片段：
- en: '[PRE39]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In *Chapter 4*, *Global Model-Agnostic Interpretation Methods*, we discussed
    assessing interaction effects. It’s time to revisit this topic, but this time,
    we will extract SHAP’s interaction values (`shap_interaction_values`) instead
    of using SHAP’s dependence plots. We can easily rank SHAP interactions with a
    `summary_plot` plot. A SHAP summary plot is very informative, but it’s not nearly
    as intuitive as a heatmap for interactions. To generate a heatmap with labels,
    we must place the `shap_xgb_interact_values` summed on the first axis in a DataFrame,
    then name the columns and rows (`index`) with the names of the features. The rest
    is simply using Seaborn’s `heatmap` function to plot the DataFrame as a heatmap.
    The code can be seen in the following snippet:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4章*，*全局模型无关解释方法*中，我们讨论了评估交互效应。现在是时候回顾这个话题了，但这次，我们将提取SHAP的交互值（`shap_interaction_values`）而不是使用SHAP的依赖图。我们可以很容易地使用`summary_plot`图对SHAP交互进行排序。SHAP总结图非常有信息量，但它并不像交互热图那样直观。为了生成带有标签的热图，我们必须将`shap_xgb_interact_values`的总和放在DataFrame的第一个轴上，然后使用特征的名称命名列和行（`index`）。其余的只是使用Seaborn的`heatmap`函数将DataFrame绘制为热图。代码可以在下面的代码片段中看到：
- en: '[PRE40]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The preceding code produced the heatmap shown in *Figure 12.18*. It demonstrates
    how `race` interacts most heavily with `length_of_stay`, `age_group`, and `priors
    per year`. These interactions would, of course, disappear once we removed `race`.
    However, given this finding, careful consideration ought to be given if these
    features don’t have racial bias built in. Research supports the need for `age_group`
    and `priors_per_year`, which leaves `length_of_stay` as a candidate for scrutiny.
    We won’t do this in this chapter, but it’s certainly food for thought:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了*图12.18*所示的热图。它展示了`race`与`length_of_stay`、`age_group`和`priors per year`之间的相互作用最为强烈。当然，一旦我们移除`race`，这些相互作用就会消失。然而，鉴于这一发现，如果这些特征中内置了种族偏见，我们应该仔细考虑。研究支持了`age_group`和`priors_per_year`的必要性，这使`length_of_stay`成为审查的候选者。我们不会在本章中这样做，但这确实值得思考：
- en: '![Graphical user interface, application  Description automatically generated](img/B18406_12_18.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序描述自动生成](img/B18406_12_18.png)'
- en: 'Figure 12.18: Heatmap with SHAP interaction values for the regularized XGBoost
    model'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.18：正则化XGBoost模型的SHAP交互值热图
- en: Another interesting insight from *Figure 12.18* is how features can be clustered.
    You can draw a box around the lower-right quadrant between `c_charge_degree` and
    `priors_per_year` because, once we remove `race`, most of the interaction will
    be located here. There are many benefits to limiting troubling interactions. For
    instance, why should all the juvenile delinquency features, such as `juv_fel_count`,
    interact with `age_group`? Why should `sex` interact with `length_of_stay`? Next,
    we will learn how to place a fence around the lower-right quadrant, limiting interactions
    between those features with **interaction constraints**. We will also ensure monotonicity
    for `priors_per_year` with **monotonic constraints**.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图12.18*中得到的另一个有趣的见解是特征如何被聚类。你可以在`c_charge_degree`和`priors_per_year`之间的右下象限画一个框，因为一旦我们移除`race`，大部分的交互都将位于这里。限制令人烦恼的交互有很多好处。例如，为什么所有青少年犯罪特征，如`juv_fel_count`，都应该与`age_group`交互？为什么`sex`应该与`length_of_stay`交互？接下来，我们将学习如何围绕右下象限设置一个围栏，通过**交互约束**限制这些特征之间的交互。我们还将确保`priors_per_year`的**单调约束**。
- en: Implementing model constraints
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现模型约束
- en: 'We will discuss how to implement constraints first with XGBoost and all popular
    tree ensembles, for that matter, because the parameters are named the same (see
    *Figure 12.12*). Then, we will do so with TensorFlow Lattice. But before we move
    forward with any of that, let’s remove `race` from the data, as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先讨论如何使用XGBoost以及所有流行的树集成实现约束，因为它们的参数名称相同（见*图12.12*）。然后，我们将使用TensorFlow Lattice进行操作。但在我们继续之前，让我们按照以下方式从数据中移除`race`：
- en: '[PRE41]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, with `race` out of the picture, the model may still have some bias. However,
    the feature engineering we performed and the constraints we will place can help
    align the model against them, given the double standards we found in *Chapter
    6**, Anchors and Counterfactual Explanations*. That being said, the resulting
    model might perform worse against the test data. There are two reasons for this,
    outlined here:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，随着`race`的消失，模型可能仍然存在一些偏见。然而，我们进行的特征工程和将要施加的约束可以帮助模型与这些偏见对齐，考虑到我们在*第6章*中发现的**锚点和反事实解释**的双重标准。话虽如此，生成的模型可能在对测试数据的性能上会较差。这里有两大原因，如下所述：
- en: '**Loss of information**: Race, especially through interaction with other features,
    impacted the outcome, so it unfortunately carried some information.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息丢失**：种族，尤其是与其他特征的交互，影响了结果，因此不幸地携带了一些信息。'
- en: '**Misalignment between reality and policy-driven ideals**: This occurs when
    the main reason to enforce these constraints is to ensure that the model not only
    complies with domain knowledge but also ideals, and these might not be evident
    in the data. We must remember that a whole host of institutional racism could
    have tainted the ground truth. The model reflects the data, but the data reflects
    reality on the ground, which is itself biased.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**现实与政策驱动理想的错位**：当实施这些约束的主要原因是确保模型不仅符合领域知识，而且符合理想，而这些理想可能不在数据中明显体现时，这种情况就会发生。我们必须记住，一整套制度化的种族主义可能已经玷污了真实情况。模型反映了数据，但数据反映了地面的现实，而现实本身是有偏见的。'
- en: With that in mind, let’s get started with constraint implementation!
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，让我们开始实施约束！
- en: Constraints for XGBoost
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost的约束
- en: We will take three simple steps in this section. We will first define our training
    parameters, then train and evaluate a constrained model, and, lastly, examine
    the effects of the constraints.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将采取三个简单的步骤。首先，我们将定义我们的训练参数，然后训练和评估一个约束模型，最后检查约束的效果。
- en: Setting regularization and constraint parameters
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置正则化和约束参数
- en: 'We take the best parameters for our regularized XGBoost model with `print(fitted_class_mdls[''xgb_reg''][''cv_best_params''])`.
    They are in the `best_xgb_params` dictionary, along with `eta` and `max_depth`.
    Then, to enforce monotonic constraints on `priors_per_year`, we must first know
    its position and the direction of the monotonic correlation. From *Figure 12.8*,
    we know the answers to both questions. It is the last feature, and the correlation
    is positive, so the `mono_con` tuple should have nine items, with the last one
    being a `1` and the rest `0`s. As for interaction constraints, we will only allow
    the last five features to interact with each other, and the same goes for the
    first four. The `interact_con` tuple is a list of lists that reflects these constraints.
    The code can be seen in the following snippet:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `print(fitted_class_mdls['xgb_reg']['cv_best_params'])` 来获取我们正则化 XGBoost
    模型的最佳参数。它们位于 `best_xgb_params` 字典中，包括 `eta` 和 `max_depth`。然后，为了对 `priors_per_year`
    应用单调约束，我们首先需要知道其位置和单调相关性的方向。从 *图12.8* 中，我们知道这两个问题的答案。它是最后一个特征，相关性是正的，所以 `mono_con`
    元组应该有九个项目，最后一个是一个 `1`，其余的是 `0`s。至于交互约束，我们只允许最后五个特征相互交互，前四个也是如此。`interact_con`
    元组是一个列表的列表，反映了这些约束。代码可以在下面的片段中看到：
- en: '[PRE42]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Next, we will train and evaluate the XGBoost model with these constraints.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用这些约束条件训练和评估 XGBoost 模型。
- en: Training and evaluating the constrained model
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练和评估约束模型
- en: 'We will now train and evaluate our constrained model. First, we initialize
    the `XGBClassifier` model with our constraint and regularization parameters and
    then fit it using training data that lacks the `race` feature (`X_train_con`).
    We then evaluate the predictive performance with `evaluate_class_mdl` and compare
    fairness with `compare_confusion_matrices`, as we have done before. The code can
    be seen in the following snippet:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用这些约束条件训练和评估我们的约束模型。首先，我们使用我们的约束和正则化参数初始化 `XGBClassifier` 模型，然后使用缺少 `race`
    特征的训练数据 (`X_train_con`) 来拟合它。然后，我们使用 `evaluate_class_mdl` 评估预测性能，并与 `compare_confusion_matrices`
    比较公平性，就像我们之前所做的那样。代码可以在下面的片段中看到：
- en: '[PRE43]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The confusion matrix output can be seen here:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在这里看到混淆矩阵的输出：
- en: '![Chart  Description automatically generated](img/B18406_12_19.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B18406_12_19.png)'
- en: 'Figure 12.19: Comparison of confusion matrices between races for the constrained
    XGBoost model'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.19：约束 XGBoost 模型不同种族之间的混淆矩阵比较
- en: One thing to consider is that, although racial inequity is a primary concern
    of this chapter, we also want to ensure that the model is optimal in other ways.
    As stated before, it’s a balancing act. For instance, it’s only fitting that defendants
    with the most `priors_per_year` are riskier than those with the least, and we
    ensured this with monotonic constraints. Let’s verify these outcomes!
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个需要考虑的事情是，尽管种族不平等是本章的主要关注点，但我们还希望确保模型在其他方面也是最优的。正如之前所述，这是一个权衡。例如，被告的 `priors_per_year`
    越多，风险越高，这是很自然的，我们通过单调约束确保了这一点。让我们验证这些结果！
- en: Examining constraints
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查约束
- en: 'An easy way to observe the constraints in action is to plot a SHAP `summary_plot`,
    as we did in *Figure 12.17*, but this time, we will only plot one. Have a look
    at the following ode snippet:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 观察约束条件在作用中的简单方法是将 SHAP `summary_plot` 绘制出来，就像我们在 *图12.17* 中所做的那样，但这次我们只绘制一个。请看下面的
    ode 程序片段：
- en: '[PRE44]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The preceding code produces *Figure 12.20*. This demonstrates how `priors_per_year`
    from left to right is a cleaner gradient, which means that lower values are consistently
    having a negative impact, and the higher ones a positive one—as they should!
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了 *图12.20*。这展示了从左到右的 `priors_per_year` 是一个更干净的梯度，这意味着较低的值持续产生负面影响，而较高的值产生正面影响——正如它们应该的那样！
- en: 'You can see the output here:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里看到输出：
- en: '![Chart  Description automatically generated](img/B18406_12_20.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B18406_12_20.png)'
- en: 'Figure 12.20: SHAP summary plot for the constrained XGBoost model'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.20：约束 XGBoost 模型的 SHAP 概述图
- en: 'Next, let’s examine the `age_group` versus `priors_per_year` interaction we
    saw through the lens of the data in *Figure 12.7*. We can also use `plot_prob_contour_map`
    for models by adding extra arguments, as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们通过 *图12.7* 中的数据视角检查我们看到的 `age_group` 与 `priors_per_year` 的交互。我们也可以通过添加额外的参数来为模型使用
    `plot_prob_contour_map`，如下所示：
- en: The fitted model (`fitted_xgb_con_mdl`)
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拟合的模型 (`fitted_xgb_con_mdl`)
- en: The DataFrame to use for inference with the model (`X_test_con`)
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于模型推理的 DataFrame (`X_test_con`)
- en: The names of the two columns in the DataFrame to compare on each axis (`x_col`
    and `y_col`)
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个轴上比较的DataFrame中两列的名称（`x_col`和`y_col`）
- en: 'The outcome is an interaction partial dependence plot, like those shown in
    *Chapter 4*, *Global Model-Agnostic Interpretation Methods*, except that it uses
    the dataset (`recidivism_df`) to create the histograms for each axis. We will
    create two such plots right now for comparison—one for the regularized XGBoost
    model and another for the constrained one. The code for this can be seen in the
    following snippet:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个交互部分依赖图，类似于*第4章*中展示的，*全局模型无关解释方法*，只不过它使用数据集（`recidivism_df`）为每个轴创建直方图。我们现在将创建两个这样的图进行比较——一个用于正则化的XGBoost模型，另一个用于约束模型。此代码的示例如下：
- en: '[PRE45]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The preceding code produces the plots shown in *Figure 12.21*. It shows that
    the regularized XGBoost model reflects the data (see *Figure 12.7*). On the other
    hand, the constrained XGBoost model smoothened and simplified the contours, as
    can be seen here:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了*图12.21*中显示的图表。它表明正则化的XGBoost模型反映了数据（参见*图12.7*）。另一方面，约束的XGBoost模型平滑并简化了等高线，如下所示：
- en: '![Chart, diagram  Description automatically generated](img/B18406_12_21.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![图表，自动生成描述](img/B18406_12_21.png)'
- en: 'Figure 12.21: Recidivism probability contour map for age_group and priors_per_year
    according to XGBoost regularized and constrained models'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.21：根据XGBoost正则化和约束模型，针对age_group和priors_per_year的再犯概率等高线图
- en: 'Next, we can generate the SHAP interaction values heatmap from *Figure 12.18*
    but for the constrained model. The code is the same but uses the `shap_xgb_con_explainer`
    SHAP explainer and `X_test_con` data. The code can be seen in the following snippet:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以从*图12.18*生成SHAP交互值热图，但针对的是约束模型。代码相同，但使用`shap_xgb_con_explainer` SHAP解释器和`X_test_con`数据。代码的示例如下：
- en: '[PRE46]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output can be seen here:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![A picture containing application  Description automatically generated](img/B18406_12_22.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![包含应用的图片，自动生成描述](img/B18406_12_22.png)'
- en: 'Figure 12.22: Heatmap with SHAP interaction values for the constrained XGBoost
    model'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.22：约束XGBoost模型的SHAP交互值热图
- en: Now, let’s see how TensorFlow implements monotonicity and other “shape constraints”
    via TensorFlow Lattice.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看TensorFlow是如何通过TensorFlow Lattice实现单调性和其他“形状约束”的。
- en: Constraints for TensorFlow Lattice
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Lattice的约束条件
- en: Neural networks can be very efficient in finding an optimal solution for the
    `loss` function. The loss is tied to a consequence we wish to predict. In this
    case, that would be 2-year recidivism. In ethics, a *utilitarian* (or *consequentialist*)
    view of fairness has no problem with this as long as the model’s training data
    isn’t biased. Yet a *deontological* view is that ethical principles or policies
    drive ethical questions and supersede consequences. Inspired by this, **TensorFlow
    Lattice** (**TFL**) can embody ethical principles in models as model shape constraints.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在寻找`loss`函数的最优解方面可以非常高效。损失与我们要预测的后果相关联。在这种情况下，那将是2年的再犯率。在伦理学中，*功利主义*（或*后果主义*）的公平观只要模型的训练数据没有偏见，就没有问题。然而，*义务论*的观点是，伦理原则或政策驱动着伦理问题，并超越后果。受此启发，**TensorFlow
    Lattice**（**TFL**）可以在模型中将伦理原则体现为模型形状约束。
- en: 'A lattice is an **interpolated lookup table**, which is a grid that approximates
    inputs to outputs through interpolation. In high-dimensional space, these grids
    become hypercubes. The mappings of each input to output are constrained through
    **calibration layers**, and they support many kinds of constraints—not just monotonicity.
    *Figure 12.23* shows this here:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 晶格是一种**插值查找表**，它通过插值近似输入到输出的网格。在高维空间中，这些网格成为超立方体。每个输入到输出的映射通过**校准层**进行约束，并且支持许多类型的约束——不仅仅是单调性。*图12.23*展示了这一点：
- en: '![Diagram  Description automatically generated](img/B18406_12_23.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图表，自动生成描述](img/B18406_12_23.png)'
- en: 'Figure 12.23: Some of the constraints supported by TensorFlow Lattice'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.23：TensorFlow Lattice支持的约束条件
- en: '*Figure 12.23* shows several shape constraints. The first three are applied
    to a single feature (*x*) constraining the ![](img/B18406_12_006.png) line, representing
    the output. The last two are applied to a pair of features (*x*[1] and *x*[2])
    constraining the color-coded contour map (![](img/B18406_12_007.png)). A brief
    explanation for each follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12.23*展示了几个形状约束。前三个应用于单个特征（*x*），约束了![](img/B18406_12_006.png)线，代表输出。最后两个应用于一对特征（*x*[1]和*x*[2]），约束了彩色等高线图（![](img/B18406_12_007.png)）。以下是对每个约束的简要说明：'
- en: '**Monotonicity**: This makes the function (![](img/B18406_12_008.png)) always
    increase (1) or decrease (-1) against the input (*x*).'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单调性**：这使得函数（![](img/B18406_12_008.png)）相对于输入（*x*）总是增加（1）或减少（-1）。'
- en: '**Convexity**: This forces the function (![](img/B18406_12_009.png)) to be
    convex (1) or concave (-1) against the input (*x*). Convexity can be mixed with
    monotonicity to have an effect like the one in *Figure 12.23*.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**凸性**：这迫使函数（![](img/B18406_12_009.png)）相对于输入（*x*）是凸的（1）或凹的（-1）。凸性可以与单调性结合，产生*图
    12.23*中的效果。'
- en: '**Unimodality**: This is like monotonicity, except that it goes in both directions,
    allowing the function (![](img/B18406_12_010.png)) to have a single valley (1)
    or peak (-1).'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单峰性**：这类似于单调性，不同之处在于它向两个方向延伸，允许函数（![](img/B18406_12_010.png)）有一个单一的谷底（1）或峰值（-1）。'
- en: '**Trust**: This forces one monotonic feature (*x*[1]) to rely on another one
    (*x*[2]). The example in *Figure 12.23* is **Edgeworth Trust**, but there’s also
    a **Trapezoid Trust** variation with a different shape constraint.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信任**：这迫使一个单调特征（*x*[1]）依赖于另一个特征（*x*[2]）。*图 12.23*中的例子是**爱德华兹信任**，但也有一个具有不同形状约束的**梯形信任**变体。'
- en: '**Dominance**: Monotonic dominance constrains one monotonic (*x*[1]) feature
    to define the direction of the slope or effects when compared to another (*x*[2]).
    An alternative, range dominance, is similar, except both features are monotonic.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支配性**：单调支配性约束一个单调特征（*x*[1]）定义斜率或效果的方向，当与另一个特征（*x*[2]）比较时。另一种选择，范围支配性，类似，但两个特征都是单调的。'
- en: Neural networks are particularly prone to overfitting, and the levers for controlling
    it are comparably more difficult. For instance, exactly what combination of hidden
    nodes, dropout, weight regularization, and epochs will lead to an acceptable level
    of overfitting is challenging to tell. On the other hand, moving a single parameter
    in a tree-based model, tree depth, in one direction will likely lower overfitting
    to an acceptable level, albeit it might require many different parameters to make
    it optimal.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络特别容易过拟合，控制它的杠杆相对更难。例如，确切地说，隐藏节点、dropout、权重正则化和 epoch 的哪种组合会导致可接受的过拟合水平是难以确定的。另一方面，在基于树的模型中移动单个参数，即树深度，朝一个方向移动，可能会将过拟合降低到可接受的水平，尽管可能需要许多不同的参数才能使其达到最佳状态。
- en: Enforcing shape constraints not only increases interpretability but also regularizes
    the model because it simplifies the function. TFL also supports different kinds
    of penalty-based regularization on a per-feature basis or to the calibration layer’s
    kernel, leveraging L1 and L2 penalties via **Laplacian**, **Hessian**, **Torsion**,
    and **Wrinkle** regularizers. These regularizers have the effect of making functions
    more flat, linear, or smooth. We won’t explain them but it suffices to say that
    there is regularization to cover many use cases.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 强制形状约束不仅增加了可解释性，还因为简化了函数而正则化了模型。TFL 还支持基于惩罚的正则化，针对每个特征或校准层的核，利用**拉普拉斯**、**海森**、**扭转**和**皱纹**正则化器通过
    L1 和 L2 惩罚。这些正则化器的作用是使函数更加平坦、线性或平滑。我们不会详细解释，但可以说，存在正则化来覆盖许多用例。
- en: There are also several ways to implement the framework—too many to elaborate
    here! Yet, it’s important to point out that this example is just one of a handful
    of ways of implementing it. TFL comes with built-in **canned estimators** that
    abstract some of the configurations. You can also create a **custom estimator**
    using the TFL layers. For Keras, you can either use **premade models** or build
    a Keras model with TensorFlow Lattice layers. This last one is what we will do
    next!
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 实现框架的方法也有几种——太多，这里无法一一详述！然而，重要的是指出，这个例子只是实现它的几种方法之一。TFL 内置了**预定义的估计器**，它们抽象了一些配置。您还可以使用
    TFL 层创建一个**自定义估计器**。对于 Keras，您可以使用**预制的模型**，或者使用 TensorFlow Lattice 层构建一个 Keras
    模型。接下来，我们将进行最后一项操作！
- en: Initializing the model and Lattice inputs
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化模型和 Lattice 输入
- en: We will now create a series of *input layers*, which each include a single feature.
    These connect to *calibration layers*, which make each input fit into a **Piece-Wise
    Linear** (**PWL**) function that complies with individual constraints and regularizations,
    except for `sex`, which will use categorical calibration. The calibration layers
    all feed into a multidimensional *Lattice layer*, producing output via a *Dense
    layer* with *sigmoid* activation. This description can be a lot to take in, so
    feel free to skip ahead to *Figure 12.24* to get some visual aid.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将创建一系列**输入层**，每个输入层包含一个特征。这些层连接到**校准层**，使每个输入适合符合个体约束和正则化的**分段线性**（**PWL**）函数，除了`sex`，它将使用分类校准。所有校准层都输入到一个多维**晶格层**，通过一个具有**sigmoid**激活的**密集层**产生输出。这个描述可能有点难以理解，所以您可以自由地跳到**图12.24**以获得一些视觉辅助。
- en: 'Incidentally, there are many kinds of layers available that you can connect
    to produce a **Deep Lattice Network** (**DLN**), including the following:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，有许多种类的层可供连接，以产生**深度晶格网络**（**DLN**），包括以下内容：
- en: '**Linear** for linear functions between more than one input, including those
    with dominance shape constraints.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性**用于多个输入之间的线性函数，包括具有支配形状约束的函数。'
- en: '**Aggregation** to perform an aggregation function on more than one input.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合**用于对多个输入执行聚合函数。'
- en: '**Parallel combination** to place many calibration layers within a single function,
    making it compatible with Keras `Sequential` layers.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行组合**将多个校准层放置在单个函数中，使其与Keras `Sequential`层兼容。'
- en: We won’t use any of these layers in this example, but perhaps knowing this will
    inspire you to explore the TensorFlow Lattice library further. Anyway, back to
    this example!
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们不会使用这些层，但也许了解这些会激发您进一步探索TensorFlow Lattice库。无论如何，回到这个例子！
- en: 'The first thing to define is `lattice_sizes`, which is a tuple that corresponds
    to a number of vertices per dimension. We have one dimension per feature in the
    chosen architecture, so we need to choose nine numbers greater than or equal to
    two. Features with less cardinality for categorical features or inflection points
    for continuous ones warrant fewer vertices. However, we might also want to restrict
    a feature’s expressiveness by purposely choosing an even smaller number of vertices.
    For instance, `juv_fel_count` has 10 unique values, but we will assign only two
    vertices to it. `lattice_sizes` is shown here:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要定义的是`lattice_sizes`，它是一个元组，对应于每个维度的顶点数。在所选架构中，每个特征都有一个维度，因此我们需要选择九个大于或等于2的数字。对于分类特征的基数较小的特征或连续特征的拐点，需要较少的顶点。然而，我们也可能想通过故意选择更少的顶点来限制特征的表达能力。例如，`juv_fel_count`有10个唯一值，但我们将只给它分配两个顶点。`lattice_sizes`如下所示：
- en: '[PRE47]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Next, we initialize two lists, one to place all the input layers (`model_inputs`)
    and another for the calibration layers (`lattice_inputs`). Then, for each feature,
    one by one, we define an input layer with `tf.keras.layers.Input` and a calibration
    layer with either categorical calibration (`tfl.layers.CategoricalCalibration`)
    or PWL calibration (`tfl.layers.PWLCalibration`). Both input and calibration layers
    will be appended to their respective lists for each feature. What happens inside
    the calibration layer depends on the feature. All PWL calibrations use `input_keypoints`,
    which asks where the PWL function should be segmented. Sometimes, this is best
    answered with fixed widths (`np.linspace`), and other times with fixed frequency
    (`np.quantile`). Categorical calibration instead uses buckets (`num_buckets`)
    that correspond to the number of categories. All calibrators have the following
    arguments:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们初始化两个列表，一个用于放置所有输入层（`model_inputs`）和另一个用于校准层（`lattice_inputs`）。然后，对于每个特征，我们逐一定义一个输入层使用`tf.keras.layers.Input`和一个校准层使用分类校准（`tfl.layers.CategoricalCalibration`）或PWL校准（`tfl.layers.PWLCalibration`）。每个特征的所有输入和校准层都将分别添加到各自的列表中。校准层内部发生的事情取决于特征。所有PWL校准都使用`input_keypoints`，它询问PWL函数应该在何处分段。有时，使用固定宽度（`np.linspace`）回答这个问题是最好的，而有时使用固定频率（`np.quantile`）。分类校准则使用桶（`num_buckets`），它对应于类别的数量。所有校准器都有以下参数：
- en: '`output_min`: The minimum output for the calibrator'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_min`：校准器的最小输出'
- en: '`output_max`: The maximum output for the calibrator—always has to match the
    output minimum + lattice size - 1'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_max`：校准器的最大输出——始终必须与输出最小值 + 晶格大小 - 1相匹配'
- en: '`monotonicity`: Whether it should monotonically constrain the PWL function,
    and if so, how'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`monotonicity`：是否应该单调约束PWL函数，如果是，如何约束'
- en: '`kernel_regularizer`: How to regularize the function'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel_regularizer`：如何正则化函数'
- en: 'In addition to these arguments, `convexity` and `is_cyclic` (for monotonic
    unimodal) can modify the constraint shape. Have a look at the following code snippet:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些参数之外，`convexity` 和 `is_cyclic`（对于单调单峰）可以修改约束形状。看看下面的代码片段：
- en: '[PRE48]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: So, we now have a list with `model_inputs` and another with calibration layers,
    which will be the input to the lattice (`lattice_inputs`). All we need to do now
    is tie these together to a lattice.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们现在有一个包含 `model_inputs` 的列表和另一个包含校准层的列表，这些校准层将成为 lattice 的输入（`lattice_inputs`）。我们现在需要做的就是将这些连接到一个
    lattice 上。
- en: Building a Keras model with TensorFlow Lattice layers
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Lattice 层构建 Keras 模型
- en: 'We already have the first two building blocks of this model connected. Now,
    let’s create the last two building blocks, starting with the lattice (`tfl.layers.Lattice`).
    As arguments, it takes `lattice_sizes`, output minimums and maximums, and `monotonicities`
    it should enforce. Note that the last item, `priors_per_year`, has monotonicity
    set as `increasing`. The lattice layer then feeds into the final piece, which
    is the `Dense` layer with `sigmoid` activation. The code can be seen in the following
    snippet:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将这个模型的前两个构建块连接起来。现在，让我们创建最后两个构建块，从 lattice (`tfl.layers.Lattice`) 开始。作为参数，它接受
    `lattice_sizes`、输出最小值和最大值以及它应该执行的 `monotonicities`。注意，最后一个参数 `priors_per_year`
    的单调性设置为 `increasing`。然后，lattice 层将输入到最终的部件，即具有 `sigmoid` 激活的 `Dense` 层。代码如下所示：
- en: '[PRE49]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The first two building blocks as `inputs` can now get connected with the last
    two as `outputs` with `tf.keras.models.Model`. And voilà! We now have a fully
    formed model, with the code shown here:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个构建块作为 `inputs` 现在可以与最后两个作为 `outputs` 通过 `tf.keras.models.Model` 连接起来。哇！我们现在有一个完整的模型，代码如下所示：
- en: '[PRE50]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'You can always run `tfl_mdl.summary()` to get an idea of how all the layers
    connect, but it’s not as intuitive as using `tf.keras.utils.plot_model`, which
    is illustrated in the following code snippet:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 你总是可以运行 `tfl_mdl.summary()` 来了解所有层是如何连接的，但使用 `tf.keras.utils.plot_model` 更直观，如下面的代码片段所示：
- en: '[PRE51]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The preceding code generates the model diagram shown here in *Figure 12.24*:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了 *图 12.24* 中显示的模型图：
- en: '![Diagram  Description automatically generated](img/B18406_12_24.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![图  描述自动生成](img/B18406_12_24.png)'
- en: 'Figure 12.24: A diagram of the Keras model with TFL layers'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.24：带有 TFL 层的 Keras 模型图
- en: 'Next, we need to compile the model. We will use a `binary_crossentropy` loss
    function and an `Adam` optimizer, and employ accuracy and **Area Under the Curve**
    (**AUC**) as metrics, as illustrated in the following code snippet:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要编译模型。我们将使用 `binary_crossentropy` 损失函数和 `Adam` 优化器，并使用准确率和 **曲线下面积**（**AUC**）作为指标，如下面的代码片段所示：
- en: '[PRE52]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We are almost ready to go now! What follows next is the very last step.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在几乎准备就绪了！接下来是最后一步。
- en: Training and evaluating the model
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练和评估模型
- en: 'If you take one hard look at *Figure 12.24*, you’ll notice that the model doesn’t
    have one input layer but nine, so this means that we must split our training and
    test data into nine parts. We can use `np.split` to do this, which will yield
    a list of nine NumPy arrays. As for the labels, TFL doesn’t accept arrays with
    a single dimension. With `expand_dims`, we convert their shapes from `(N,)` to
    `(N,1)`, as illustrated in the following code snippet:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察 *图 12.24*，你会注意到模型没有一层输入，而是有九层，这意味着我们必须将我们的训练和测试数据分成九部分。我们可以使用 `np.split`
    来做这件事，这将产生九个 NumPy 数组的列表。至于标签，TFL 不接受单维数组。使用 `expand_dims`，我们将它们的形状从 `(N,)` 转换为
    `(N,1)`，如下面的代码片段所示：
- en: '[PRE53]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now comes the training! To prevent overfitting, we can use `EarlyStopping`
    by monitoring the validation AUC (`val_auc`). And to account for class imbalance,
    in the `fit` function, we use `class_weight`, as illustrated in the following
    code snippet:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是训练！为了防止过拟合，我们可以通过监控验证 AUC (`val_auc`) 来使用 `EarlyStopping`。为了解决类别不平衡问题，在
    `fit` 函数中，我们使用 `class_weight`，如下面的代码片段所示：
- en: '[PRE54]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Once the model has been trained, we can use `evaluate_class_mdl` to output
    a quick summary of predictive performance, as we have before, and then `compare_confusion_matrices`
    to examine fairness, as we did previously. The code is shown in the following
    snippet:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们可以使用 `evaluate_class_mdl` 来输出预测性能的快速摘要，就像我们之前做的那样，然后使用 `compare_confusion_matrices`
    来检查公平性，就像我们之前所做的那样。代码如下所示：
- en: '[PRE55]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The output can be seen here:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Chart, treemap chart  Description automatically generated](img/B18406_12_25.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![图表，树状图  描述自动生成](img/B18406_12_25.png)'
- en: 'Figure 12.25: Comparison of confusion matrices between races for the constrained
    TensorFlow Lattice model'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.25：约束TensorFlow Lattice模型在种族之间的混淆矩阵比较
- en: Next, we will make some conclusions based on what was learned in this chapter
    and determine if we accomplished the mission.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将根据本章学到的内容得出一些结论，并确定我们是否完成了任务。
- en: Mission accomplished
  id: totrans-359
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务完成
- en: It’s often the data that takes the blame for a poor-performing, uninterpretable,
    or biased model, and that can be true, but many different things can be done in
    the preparation and model development stages to improve it. To offer an analogy,
    it’s like baking a cake. You need quality ingredients, yes. But seemingly small
    differences in the preparation of these ingredients and baking itself—such as
    the baking temperature, the container used, and time—can make a huge difference.
    Hell! Even things that are out of your control, such as atmospheric pressure or
    moisture, can impact baking! Even after it’s all finished, how many different
    ways can you assess the quality of a cake?
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，数据会因为表现不佳、不可解释或存在偏见而被责备，这可能是真的，但在准备和模型开发阶段可以采取许多不同的措施来改进它。为了提供一个类比，这就像烘焙蛋糕。你需要高质量的原料，是的。但似乎微小的原料准备和烘焙本身——如烘焙温度、使用的容器和时间——的差异可以产生巨大的影响。天哪！甚至是你无法控制的事情，如大气压力或湿度，也会影响烘焙！甚至在完成之后，你有多少种不同的方式可以评估蛋糕的质量？
- en: 'This chapter is about these many details, and, as with baking, they are **part
    exact science** and **part art form**. The concepts discussed in this chapter
    also have far-reaching consequences, especially regarding how to optimize a problem
    that doesn’t have a single goal and has profound societal implications. One possible
    approach is to combine metrics and account for imbalances. To that end, we have
    created a metric: a weighted average of precision recall that penalizes racial
    inequity, and we can efficiently compute it for all of our models and place it
    into the model dictionary (`fitted_class_mdls`). Then, as we have done before,
    we put it into a DataFrame and output it but, this time, sort by the custom metric
    (`wppra_test`). The code can be seen in the following snippet:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了这些许多细节，就像烘焙一样，它们既是**精确科学**的一部分，也是**艺术形式**的一部分。本章讨论的概念也具有深远的影响，特别是在如何优化没有单一目标且具有深远社会影响的问题方面。一种可能的方法是结合指标并考虑不平衡。为此，我们创建了一个指标：一个加权平均的精确率召回率，它惩罚种族不平等，并且我们可以为所有模型高效地计算它并将其放入模型字典（`fitted_class_mdls`）。然后，就像我们之前做的那样，我们将其放入DataFrame并输出，但这次是按照自定义指标（`wppra_test`）排序。代码可以在下面的代码片段中看到：
- en: '[PRE56]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The preceding code produced the DataFrame shown here in *Figure 12.26*:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码生成了*图12.26*中显示的DataFrame：
- en: '![Table  Description automatically generated](img/B18406_12_26.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18406_12_26.png)'
- en: 'Figure 12.26: Top models in this chapter when sorted by weighted penalized
    precision-recall average custom metric'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.26：按加权惩罚精确率-召回率平均值自定义指标排序的本章顶级模型
- en: In *Figure 12.26*, it’s tempting to propose one of the models at the very top.
    However, they were trained with `race` as a feature and didn’t account for proven
    criminal justice *realities*. However, the highest-performing constrained model—the
    XGBoost one (`xgb_con`)—lacked `race`, ensured that `priors_per_year` is monotonic
    and that `age_group` isn’t allowed to interact with juvenile delinquency features,
    and it did all this while significantly improving predictive performance when
    compared to the original model. It is fairer, too, because it reduced the ratio
    of the FPR between the privileged and underprivileged groups from 1.84x (*Figure
    6.2* from *Chapter 6*, *Anchors and Counterfactual Explanations*) to 1.39x (*Figure
    12.19*). It’s not perfect, but it’s a massive improvement!
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图12.26*中，很容易提出最上面的其中一个模型。然而，它们是用`race`作为特征进行训练的，并且没有考虑到证明的刑事司法*现实*。然而，性能最高的约束模型——XGBoost模型（`xgb_con`）——没有使用`race`，确保了`priors_per_year`是单调的，并且不允许`age_group`与青少年犯罪特征相互作用，而且与原始模型相比，它在显著提高预测性能的同时做到了这一切。它也更公平，因为它将特权群体和弱势群体之间的FPR比率从1.84x（*第6章*中的*图6.2*）降低到1.39x（*图12.19*）。它并不完美，但这是一个巨大的改进！
- en: The mission was to prove that accuracy and domain knowledge could coexist with
    progress toward fairness, and we have completed it successfully. That being said,
    there’s still room for improvement. Therefore, the plan of action would have to
    showcase the constrained XGBoost model to your client and continue improving and
    building more constrained models. The unconstrained ones should only serve as
    a benchmark.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是证明准确性和领域知识可以与公平性的进步共存，我们已经成功地完成了它。话虽如此，仍有改进的空间。因此，行动计划将不得不向您的客户展示受约束的XGBoost模型，并继续改进和构建更多受约束的模型。未受约束的模型应仅作为基准。
- en: You can make substantial fairness improvements if you combine the methods from
    this chapter with those learned in *Chapter 11*, *Bias Mitigation and Causal Inference
    Methods*. We didn’t incorporate them into this chapter, to focus solely on model
    (or in-processing) methods that are typically not seen as part of the bias-mitigation
    toolkit, but they very much can assist to that end, not to mention model-tuning
    methods that serve to make a model more reliable.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将本章的方法与第11章中学习的那些方法（*偏差缓解和因果推断方法*）相结合，你可以实现显著的公平性改进。我们没有将这些方法纳入本章，以专注于通常不被视为偏差缓解工具包一部分的模型（或内处理）方法，但它们在很大程度上可以协助达到这一目的，更不用说那些旨在使模型更可靠的模型调优方法了。
- en: Summary
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: After reading this chapter, you should now understand how to leverage data engineering
    to enhance interpretability, regularization to reduce overfitting, and constraints
    to comply with policies. The primary end goals are to place guardrails and curb
    the complexity that hinders interpretability.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章后，你现在应该了解如何利用数据工程来增强可解释性，正则化来减少过拟合，以及约束来符合政策。主要的目标是设置护栏和遏制阻碍可解释性的复杂性。
- en: In the next chapter, we will look at ways to enhance model reliability through
    adversarial robustness.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨通过对抗鲁棒性来增强模型可靠性的方法。
- en: Dataset sources
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集来源
- en: ProPublica Data Store (2019). *COMPAS Recidivism Risk Score Data and Analysis*.
    Originally retrieved from [https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ProPublica数据存储库 (2019). *COMPAS再犯风险评分数据和分析*. 原始数据检索自 [https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)
- en: Further reading
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Hastie, T. J., Tibshirani, R. J. and Friedman, J. H. (2001). *The elements of
    statistical learning*. Springer-Verlag, New York, USA
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hastie, T. J., Tibshirani, R. J. 和 Friedman, J. H. (2001). *统计学习的要素*. Springer-Verlag,
    纽约，美国
- en: Wang, S. & Gupta, M. (2020). *Deontological Ethics By Monotonicity Shape Constraints*.
    AISTATS. [https://arxiv.org/abs/2001.11990](https://arxiv.org/abs/2001.11990)
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang, S. & Gupta, M. (2020). *通过单调性形状约束的德性伦理*. AISTATS. [https://arxiv.org/abs/2001.11990](https://arxiv.org/abs/2001.11990)
- en: Cotter, A., Gupta, M., Jiang, H., Ilan, E. L., Muller, J., Narayan, T., Wang,
    S. & Zhu, T. (2019). *Shape Constraints for Set Functions*. ICML. [http://proceedings.mlr.press/v97/cotter19a.html](http://proceedings.mlr.press/v97/cotter19a.html)
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cotter, A., Gupta, M., Jiang, H., Ilan, E. L., Muller, J., Narayan, T., Wang,
    S. 和 Zhu, T. (2019). *集合函数的形状约束*. ICML. [http://proceedings.mlr.press/v97/cotter19a.html](http://proceedings.mlr.press/v97/cotter19a.html)
- en: Gupta, M. R., Cotter A., Pfeifer, J., Voevodski, K., Canini, K., Mangylov, A.,
    Moczydlowski, W. and van Esbroeck, A. (2016). *Monotonic Calibrated Interpolated
    Look-Up Tables. Journal of Machine Learning Research* 17(109):1−47\. [https://arxiv.org/abs/1505.06378](https://arxiv.org/abs/1505.06378)
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta, M. R., Cotter A., Pfeifer, J., Voevodski, K., Canini, K., Mangylov, A.,
    Moczydlowski, W. 和 van Esbroeck, A. (2016). *单调校准插值查找表. 机器学习研究杂志* 17(109):1−47\.
    [https://arxiv.org/abs/1505.06378](https://arxiv.org/abs/1505.06378)
- en: 'Noble, S. (2018). *Algorithms of oppression: data discrimination in the age
    of Google*. NYU Press'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Noble, S. (2018). *压迫算法：谷歌时代的数据歧视*. NYU Press
- en: Learn more on Discord
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Discord上了解更多
- en: 'To join the Discord community for this book – where you can share feedback,
    ask the author questions, and learn about new releases – follow the QR code below:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入本书的Discord社区——在那里您可以分享反馈，向作者提问，并了解新发布的内容——请扫描下面的二维码：
- en: '[https://packt.link/inml](Chapter_12.xhtml)'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/inml](Chapter_12.xhtml)'
- en: '![](img/QR_Code107161072033138125.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code107161072033138125.png)'
