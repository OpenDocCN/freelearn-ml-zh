- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Neural Networks and Deep Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络与深度学习
- en: In this chapter, we will discuss **neural networks** (**NNs**) in **machine
    learning** (**ML**), often referred to as **artificial NNs** or **ANNs**. We will
    introduce many important topics in this field of science, including fundamental
    concepts that led to the development of ANNs, and relevant use cases for their
    application. At this point, it’s important to note that the term **deep learning**
    (**DL**) refers to ML that is implemented with the use of **deep NNs** (**DNNs**).
    We will explain the term “DNN” later in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论**机器学习**（**ML**）中的**神经网络**（**NN**），通常被称为**人工神经网络**或**ANNs**。我们将介绍这个科学领域中的许多重要主题，包括导致人工神经网络发展的基本概念，以及它们应用的相应用例。在此阶段，重要的是要注意，术语**深度学习**（**DL**）指的是使用**深度神经网络**（**DNNs**）实现的机器学习。我们将在本章后面解释“DNN”这个术语。
- en: We will also introduce some tools and frameworks that make it easier for us
    to create NNs, such as TensorFlow and Keras, and we will use those tools to build
    an NN in our hands-on activities later in this chapter. Finally, we will expand
    the discussion to include different types of NN architectures, common challenges
    in NN implementations, and some practices for optimizing our NN architectures.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将介绍一些工具和框架，这些工具和框架使我们更容易创建神经网络，例如TensorFlow和Keras，我们将在本章后面的动手活动中使用这些工具来构建神经网络。最后，我们将讨论不同类型的神经网络架构、神经网络实现中的常见挑战以及优化我们神经网络架构的一些实践。
- en: As a side note, I first started learning about ANNs when I was in college, and
    I remember being absolutely fascinated by the concept because I also had an avid
    interest in understanding how the human brain works. Although the concept of NNs
    is indeed loosely based on the theorized workings of the human brain, in this
    chapter, we will separate hype from reality and will focus on the practical, mathematical
    descriptions of this technology. Let’s start by covering some important concepts
    in this space.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个旁注，我第一次开始学习关于人工神经网络（ANNs）是在大学期间，我记得我对这个概念非常着迷，因为我也有浓厚的兴趣了解人脑是如何工作的。尽管神经网络的概念确实是基于人类大脑理论运作的，但在本章中，我们将区分炒作和现实，并专注于这项技术的实际、数学描述。让我们首先覆盖这个领域的一些重要概念。
- en: 'This chapter covers the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: NN and DL concepts
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络和深度学习概念
- en: Libraries
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 库
- en: Implementing a **multilayer perceptron** (**MLP**) in TensorFlow
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在TensorFlow中实现**多层感知器**（**MLP**）
- en: NN architectures, challenges, and optimization
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络架构、挑战和优化
- en: NN and DL concepts
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络和深度学习概念
- en: In this section, we discuss concepts that are important to understand in the
    context of NNs and DL. We begin by discussing how ANNs are linked to our understanding
    of the human brain.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论在神经网络和深度学习背景下理解的重要概念。我们首先讨论人工神经网络是如何与我们理解人脑相联系的。
- en: Neurons and the perceptron
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经元和感知器
- en: 'While the link between artificial neurons and biological neurons (as found
    in the human brain) is often over-emphasized, there is a conceptual link between
    them that helps us form a mental model of how they work. Biological neurons generally
    consist of three main parts, as depicted in *Figure 9**.1*:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然人工神经元和生物神经元（如人类大脑中发现的）之间的联系经常被过分强调，但它们之间存在一个概念上的联系，这有助于我们形成它们如何工作的心理模型。生物神经元通常由三个主要部分组成，如图*图9.1*所示：
- en: '![Figure 9.1: Neuron (source: https://www.flickr.com/photos/187096960@N06/51173238594)](img/B18143_09_1.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1：神经元（来源：https://www.flickr.com/photos/187096960@N06/51173238594)](img/B18143_09_1.jpg)'
- en: 'Figure 9.1: Neuron (source: https://www.flickr.com/photos/187096960@N06/51173238594)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：神经元（来源：https://www.flickr.com/photos/187096960@N06/51173238594）
- en: The cell body is the core part of the neuron that contains the nucleus and other
    important components. The dendrites (coming from the Greek word “dendron,” meaning
    “tree”) are structures that branch out from the cell body. They receive information
    from other neurons and transmit this information to the cell body. Finally, the
    axons are long, tube-like structures that extend from the cell body and send information
    out to the dendrites of other neurons (across an interface called a synapse).
    That’s about as deep as we’re going to get with regard to the biology of a neuron;
    I’ve simplified it quite a bit here because we just need high-level context for
    the comparison we’re going to make with ANNs, but the next thing we need to understand
    is how they transmit information, and this works at a high level as follows (again,
    simplified for relevant context).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 细胞体是神经元的核心部分，其中包含细胞核和其他重要组件。树突（来自希腊语单词“dendron”，意为“树”）是从细胞体分支出来的结构。它们接收来自其他神经元的信息并将这些信息传输到细胞体。最后，轴突是从细胞体延伸出来的长管状结构，将信息发送到其他神经元的树突（通过称为突触的界面）。关于神经元的生物学，我们就讲到这里；我在这里简化了很多，因为我们只需要为与人工神经网络（ANNs）进行比较提供高级背景，但接下来我们需要了解的是它们是如何传输信息的，这在大体上如下（再次，为了相关背景简化）。
- en: When a neuron receives a signal from another neuron, this causes a change in
    what’s called the **electrical potential** across the neuron’s cell membrane (the
    difference in voltage between the inside and outside of the neuron), triggering
    what’s called an **action potential**, which is an electrical impulse that travels
    down the axon. When it reaches the end of the axon, it triggers the release of
    neurotransmitters, which are chemical messengers. These neurotransmitters cross
    something called the **synaptic gap** (the tiny space between neurons) and bind
    to receptors on the dendrites of the next neuron, and this binding can then either
    trigger or inhibit a new action potential in the second neuron.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个神经元从另一个神经元接收信号时，这会导致神经元细胞膜（神经元内外电压差）中所谓的**电势**发生变化，从而触发所谓的**动作电位**，这是一种沿着轴突传播的电气脉冲。当它到达轴突的末端时，它会触发神经递质的释放，这些神经递质是化学信使。这些神经递质穿过所谓的**突触间隙**（神经元之间的微小空间）并绑定到下一个神经元的树突上的受体，这种结合可以触发或抑制第二个神经元中的新动作电位。
- en: Okay – we’ve just introduced a lot of biological terminology in the past two
    paragraphs, but those concepts are important to understand when we want to draw
    a comparison with ANNs. With this in mind, let’s move on and discuss how ANNs
    are constructed, beginning with their most basic concept, the perceptron, which
    I briefly mentioned in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015) when I
    summarized various milestones in the evolution of ML.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 好的——在前两段中，我们刚刚介绍了很多生物学术语，但当我们想要将概念与人工神经网络（ANNs）进行比较时，这些概念是非常重要的。考虑到这一点，让我们继续前进，讨论人工神经网络是如何构建的，从它们最基本的概念，即感知器开始，我在[*第一章*](B18143_01.xhtml#_idTextAnchor015)中简要提到了感知器，当时我总结了机器学习（ML）演变的各个里程碑。
- en: A perceptron can be seen as one of the simplest types of ANNs and as a building
    block for larger, more complex networks. It was developed by Frank Rosenblatt
    in the late 1950s, and it’s basically a binary classifier that maps its input
    X (a vector) to an output value f(x) (a single binary value) using a set of weights
    that are applied to the input features.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器可以被视为最简单类型的人工神经网络之一，也是更大、更复杂网络的构建块。它在20世纪50年代末由弗兰克·罗森布拉特（Frank Rosenblatt）开发，基本上是一个二元分类器，它使用一组应用于输入特征的权重将输入X（一个向量）映射到一个输出值f(x)（一个单一的二元值）。
- en: 'To understand this process in more detail, let’s dive deeper into how a perceptron
    works, which can be summarized using the following set of steps:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更详细地了解这个过程，让我们深入探讨感知器是如何工作的，这可以通过以下步骤来概括：
- en: The perceptron receives input values, which could be features or attributes
    from a dataset.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 感知器接收输入值，这些值可以是数据集中的特征或属性。
- en: Each input has an associated **weight** that represents its importance. The
    weights are often just given random values at the beginning of training, and the
    values are then refined during the training process. We will discuss this in more
    detail shortly.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个输入都有一个与之相关的**权重**，它表示其重要性。权重通常在训练开始时随机给出，然后在训练过程中进行细化。我们将在稍后详细讨论这一点。
- en: A **bias unit** is also added to the perceptron model to increase the model’s
    flexibility. This is not related to the topic of bias that we will discuss in
    the context of fairness in later chapters of this book; it’s simply a mathematical
    trick that provides an additional control mechanism for refining our model’s performance
    when attempting to produce the desired output.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**偏置单元**也被添加到感知器模型中，以增加模型的灵活性。这与我们在本书后续章节中讨论的公平性背景下的偏差主题无关；这仅仅是一个数学技巧，它为在尝试产生所需输出时改进我们模型性能提供了一个额外的控制机制。'
- en: Next, each input is multiplied by its corresponding weight, and all of the results
    of those multiplications are added together (as well as the bias), which results
    in a **weighted sum**, so this is simply a **linear transformation** of the inputs,
    based on the weights and bias values.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，每个输入乘以其相应的权重，并将所有这些乘法的结果（以及偏置）相加（结果是一个**加权总和**），所以这仅仅是基于权重和偏置值的输入的**线性变换**。
- en: This weighted sum is then passed through an **activation function** that produces
    a binary output. We will explain activation functions in more detail shortly,
    but at a high level, a non-linear transformation is performed on the weighted
    sum, and the results of that transformation are produced as an output from the
    perceptron. In the case of a single perceptron, a simple example would be that
    if the weighted sum of the inputs is greater than a threshold value, the perceptron
    would output a value of 1, or if the weighted sum is less than or equal to the
    threshold, it would output a value of 0, so this is basically an implementation
    of the process referred to as **logistic regression**.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个加权的总和随后通过一个**激活函数**，该函数产生一个二进制输出。我们将在稍后更详细地解释激活函数，但从高层次来看，对加权总和执行非线性变换，并将该变换的结果作为感知器的输出。对于单个感知器来说，一个简单的例子是，如果输入的加权总和大于一个阈值值，感知器将输出1，或者如果加权总和小于或等于阈值，它将输出0，所以这基本上是**逻辑回归**过程的实现。
- en: 'Mathematically, this can be written as the following:'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从数学上讲，这可以写成以下形式：
- en: If ∑ (weights * inputs) + bias > 0, output 1
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 ∑ (权重 * 输入) + 偏置 > 0，输出 1
- en: If ∑ (weights * inputs) + bias ≤ 0, output 0
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 ∑ (权重 * 输入) + 偏置 ≤ 0，输出 0
- en: '*Figure 9**.2* provides a visual representation of how a perceptron works:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.2*提供了感知器工作原理的视觉表示：'
- en: '![Figure 9.2: Perceptron (source: https://commons.wikimedia.org/wiki/File:Perceptron-unit.svg#file)](img/B18143_09_2.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2：感知器（来源：https://commons.wikimedia.org/wiki/File:Perceptron-unit.svg#file)](img/B18143_09_2.jpg)'
- en: 'Figure 9.2: Perceptron (source: https://commons.wikimedia.org/wiki/File:Perceptron-unit.svg#file)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：感知器（来源：https://commons.wikimedia.org/wiki/File:Perceptron-unit.svg#file）
- en: In *Figure 9**.2*, the *x* values on the far left of the diagram represent inputs
    to the perceptron. The *x*0 input is the bias, and *x*1 through *x*n represent
    the input features from our dataset. The *w* values represent the weights, the
    Greek characters (sigma and phi) within the green circle represent the activation
    function, and the Greek character (omicron) on the far right represents the output.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图9.2* 中，图的最左侧的 *x* 值代表感知器的输入。*x*0 输入是偏置，而 *x*1 到 *x*n 代表来自我们数据集的输入特征。*w*
    值代表权重，绿色圆圈内的希腊字符（sigma 和 phi）代表激活函数，而最右侧的希腊字符（omicron）代表输出。
- en: The important concept to understand is that the values of the weights and bias
    are what our perceptron model is trying to learn. That is, our model is trying
    to figure out what the best weights are for each feature, which results in a pattern
    that gets us as close as possible to the target outcome after we perform the linear
    and non-linear transformations we described (in combination with the bias). If
    we think back to the more traditional ML models we created in earlier chapters,
    such as linear regression, you may remember that our models were trying to figure
    out the optimal values of the coefficients for each of our features that would
    result in the desired outcomes. In perceptrons, and in ANNs in general, the weights
    (and the bias) are the coefficients that our models are trying to optimize.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 需要理解的重要概念是，权重和偏置的值是我们感知器模型试图学习的。也就是说，我们的模型试图找出每个特征的最佳权重，从而在执行我们描述的线性和非线性变换（结合偏置）后，得到一个尽可能接近目标结果的模式。如果我们回想一下我们在前几章中创建的更传统的ML模型，例如线性回归，我们可能会记得我们的模型试图找出每个特征的系数的最优值，以实现预期的结果。在感知器和ANN中，权重（以及偏置）是我们模型试图优化的系数。
- en: With this understanding of how perceptrons work, let’s discuss how they can
    be used to build more complex NNs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了感知器的工作原理之后，让我们讨论如何使用它们来构建更复杂的NN。
- en: MLPs and NNs
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MLPs和NNs
- en: While the perceptron is a simple and powerful algorithm, it can only model linearly
    separable functions. This means that if our data isn’t linearly separable (that
    is, we can’t draw a straight line to separate the classes), the perceptron won’t
    be able to accurately distinguish between the classes in the dataset. To overcome
    this limitation, multiple perceptrons can be combined in layers to form an MLP,
    which has the potential to solve non-linear problems. An MLP is a form of NN,
    so basically, when we combine multiple perceptrons in a sequential manner (that
    is, where the outputs of some perceptrons become the inputs for other perceptrons),
    we form a type of ANN.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然感知器是一个简单而强大的算法，但它只能模拟线性可分函数。这意味着如果我们的数据不是线性可分的（也就是说，我们无法画一条直线来分离类别），感知器将无法准确地区分数据集中的类别。为了克服这一限制，可以通过层将多个感知器组合起来形成MLP，它有可能解决非线性问题。MLP是NN的一种形式，所以基本上，当我们以顺序方式（即某些感知器的输出成为其他感知器的输入）组合多个感知器时，我们形成了一种ANN。
- en: Considering that the perceptron can be considered a type of artificial neuron,
    we will use the terms “perceptron” and “artificial neuron” (or sometimes just
    “neuron”) interchangeably from this point onward.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到感知器可以被看作是一种人工神经元，从现在开始我们将“感知器”和“人工神经元”（有时简称“神经元”）这两个术语交替使用。
- en: Summarizing the comparison to biological neural activity
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 总结与生物神经活动的比较
- en: As we’ve already discussed, a neuron receives inputs from sensory organs or
    from other neurons, and, depending on the resulting electrical potential, an action
    potential causes the neuron to fire (or not fire) a message to other neurons.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所讨论的，神经元从感觉器官或其他神经元接收输入，并且根据产生的电势，动作电位会导致神经元向其他神经元发送（或不发送）消息。
- en: Similarly, a perceptron (or artificial neuron) receives inputs from our dataset,
    or from other artificial neurons if we are chaining the perceptrons together in
    an NN. Then, depending on the linear combination of these inputs and their weights
    and biases, the activation function will influence the output of the perceptron,
    which can be used as an input to another perceptron in the network.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，感知器（或人工神经元）从我们的数据集接收输入，或者如果我们是在NN中将感知器串联起来，则从其他人工神经元接收输入。然后，根据这些输入及其权重和偏置的线性组合，激活函数将影响感知器的输出，该输出可以用作网络中另一个感知器的输入。
- en: Next, let’s dive into more detail on how NNs are typically structured, and introduce
    the important concept of **layers** in NNs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们更详细地探讨NN的典型结构，并介绍NN中重要的**层**概念。
- en: Layers in an NN
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NN中的层
- en: 'When artificial neurons are combined together to form an NN, they are not just
    connected randomly but instead are connected in a structured manner using the
    concept of layers, as depicted in *Figure 9**.3*:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当人工神经元组合在一起形成NN时，它们不是随机连接的，而是使用层这一概念以结构化的方式进行连接，如图*图9.3*所示：
- en: '![Figure 9.3: NN layers](img/B18143_09_3.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3：NN层](img/B18143_09_3.jpg)'
- en: 'Figure 9.3: NN layers'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：NN层
- en: 'As shown in *Figure 9**.3*, the layers of an NN are generally categorized into
    three different types:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图9**.3*所示，神经网络的层通常分为三种不同类型：
- en: '**The input layer**, which, as the name suggests, is how our inputs enter our
    NN. It is, of course, the first layer in the network.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入层**，正如其名称所暗示的，是我们输入进入神经网络的方式。它是网络中的第一层，当然是。'
- en: '**Hidden layers**, which sit between the input and output layers. Their job
    is to transform the inputs into something the output layer can use. The term “hidden”
    just means that they do not interface with the outside world (they are neither
    the input nor the output of the network). We usually do not have control over
    or direct interaction with them; they learn to represent the data on their own.
    The number of hidden layers and the number of neurons in each hidden layer define
    the complexity and structure of the NN.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**隐藏层**，位于输入层和输出层之间。它们的工作是将输入转换为输出层可以使用的东西。术语“隐藏”只是意味着它们不与外界接口（它们既不是网络的输入也不是输出）。我们通常无法控制或直接与它们交互；它们学会独立表示数据。隐藏层的数量以及每个隐藏层中的神经元数量定义了神经网络的复杂性和结构。'
- en: '**The output layer**, which, again suggested by the name, presents the output
    of our NN, which is usually some kind of prediction.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输出层**，正如其名称所暗示的，展示了我们的神经网络的输出，这通常是某种预测。'
- en: In addition to the number of hidden layers and the number of neurons in each
    hidden layer, exactly how the layers are connected together depends on the architecture
    of the NN. We will discuss different types of common NN architectures later in
    this chapter, but what generally happens is that after our input data is fed into
    the NN’s input layer, each neuron in the subsequent layers of the network behaves
    similarly to how we described the perceptron earlier in this chapter.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 除了隐藏层的数量和每个隐藏层中的神经元数量外，层之间的确切连接方式取决于神经网络的架构。我们将在本章后面讨论不同类型的常见神经网络架构，但通常发生的情况是，我们的输入数据被输入到神经网络的输入层后，网络后续层中的每个神经元的行为类似于我们在本章前面描述的感知器。
- en: For example, each input is assigned a weight that represents its importance.
    These weights are typically initialized with random values and then refined during
    the learning process. The inputs are multiplied by their corresponding weights
    and the results are summed together, in addition to a bias value. The summed result
    is then used as the input to the activation function in a neuron in the subsequent
    layer (that is, beginning with the first hidden layer) of the NN. Generally, this
    process is performed by every neuron in the subsequent layers. The important thing
    to remember is that the weights and biases will be different for each neuron.
    Therefore, even though the exact same input data is seen by each neuron in the
    first hidden layer, how each neuron reacts to the data will vary because the various
    weights and biases will influence each neuron’s activation function differently.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，每个输入都被分配一个表示其重要性的权重。这些权重通常用随机值初始化，然后在学习过程中进行细化。输入乘以其相应的权重，并将结果相加，再加上一个偏差值。然后将总和作为后续层（即从第一个隐藏层开始）中神经元的激活函数的输入。重要的是要记住，每个神经元的权重和偏差将是不同的。因此，尽管每个神经元在第一个隐藏层看到的是完全相同的数据输入，但每个神经元对数据的反应将因各种权重和偏差的不同影响而有所不同。
- en: The next thing to understand is that the outputs from the activation functions
    in each layer serve as inputs to the same process that will be performed again
    in subsequent layers of the network. So, the same process that we just described
    will be performed in each subsequent layer, but rather than our original dataset
    being used as an input in every layer, each subsequent layer will use the activation
    values from the previous layer as the input. This means that multiple transformations
    are being implemented as information travels through our network, and this is
    what makes NNs so powerful.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 需要理解的是，每一层的激活函数的输出将作为网络后续层中再次执行的过程的输入。因此，我们刚才描述的过程将在每一后续层中执行，但与我们的原始数据集在每一层作为输入不同，每一后续层将使用前一层的激活值作为输入。这意味着在信息通过我们的网络传递时，正在实施多个转换，这也是神经网络之所以强大的原因。
- en: 'Let’s just be sure we have a clear understanding of this process. If we take
    the second hidden layer as an example, the process works as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确保我们对这个过程有清晰的理解。以第二个隐藏层为例，过程如下：
- en: Each activation function output from the first layer is assigned a weight that
    represents its importance. These weights are typically initialized with random
    values and then refined during the learning process. The activation function outputs
    are multiplied by their corresponding weights and the results are summed together,
    in addition to the bias values. The summed result is then used as the input to
    the activation function in the neurons in the next layer of the NN. This process
    is repeated in each layer until we get to the final, output layer of the network.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层的每个激活函数输出都被分配一个表示其重要性的权重。这些权重通常用随机值初始化，然后在学习过程中进行优化。激活函数的输出乘以相应的权重，并将结果相加，再加上偏差值。然后将这个总和作为下一层神经网络中神经元的激活函数的输入。这个过程在每个层中重复，直到我们到达网络的最终输出层。
- en: Note
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Different NN architectures can use slightly different ways of propagating information
    through the network. We will discuss some common types of NN architectures in
    more detail in this chapter, but what we’ve described so far can be considered
    the building blocks of how ANNs work.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的神经网络架构可以使用不同的方式在网络中传播信息。在本章中，我们将更详细地讨论一些常见的神经网络架构类型，但我们之前描述的内容可以被认为是人工神经网络工作原理的基石。
- en: You may also hear the term “DNN” being used. Traditionally, any NN with at least
    two hidden layers is considered a DNN.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可能听到“DNN”这个术语。传统上，任何至少有两个隐藏层的神经网络都被认为是深度神经网络（DNN）。
- en: The fact that activations in neurons in one layer of the network influence the
    activations of neurons in the next layer brings us back to the loose analogy with
    the human brain, in which certain neurons firing can cause other neurons to fire,
    resulting in varied combinations of interactions that can produce much more complex
    higher-level functions. We must take this analogy with a pinch of salt, however,
    because even the most complex ANNs contain thousands of neurons, whereas the human
    brain has billions of neurons, and each neuron is capable of much more complex
    functionality than the relatively simple mathematical transformations performed
    by artificial neurons.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 网络中某一层的神经元激活对下一层神经元激活的影响，使我们回到了与人类大脑的类比，其中某些神经元的放电可以引起其他神经元的放电，产生各种不同的交互组合，从而产生更复杂的高级功能。然而，我们必须对这个类比持保留态度，因为即使是最复杂的ANN也包含成千上万的神经元，而人类大脑有数十亿个神经元，每个神经元能够执行比人工神经元相对简单的数学变换更复杂的函数。
- en: Now that we’ve discussed how information travels through an ANN, let’s dive
    into more detail on how an ANN learns, and to do that, we must introduce the concept
    of **backpropagation**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了信息如何在人工神经网络（ANN）中传播，让我们更深入地探讨ANN是如何学习的，为此，我们必须引入**反向传播**的概念。
- en: Backpropagation
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播
- en: What we described in the previous section can be referred to as **forward propagation**,
    whereby information is being propagated forward from one layer to another in our
    NN. In order to discuss backpropagation, let’s think back on what we learned earlier
    in this book with regard to how **supervised ML** (**SML**) algorithms work. Remember
    that we use labels to describe what each data point is in our dataset. When we
    train a model, the model tries to learn patterns between the features in our dataset
    that will help it accurately predict the label for each data point. We then use
    a loss function to calculate how far off our model’s predictions were from the
    correct label; the primary purpose of the model training activity is to minimize
    the loss function (that is, to minimize the errors produced by our model), and
    we can use techniques such as gradient descent to minimize the loss function.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节中描述的内容可以被称为**前向传播**，其中信息在我们的神经网络（NN）中从一层传播到另一层。为了讨论反向传播，让我们回顾一下在这本书中我们之前学到的关于**监督机器学习（SML**）算法是如何工作的内容。记住，我们使用标签来描述数据集中每个数据点的特征。当我们训练一个模型时，模型试图学习数据集中特征之间的模式，这将帮助它准确地预测每个数据点的标签。然后我们使用损失函数来计算我们的模型预测与正确标签之间的差距；模型训练活动的首要目的是最小化损失函数（即最小化模型产生的错误），我们可以使用梯度下降等技术来最小化损失函数。
- en: Let’s take a basic linear regression model trained on tabular data as an example.
    You may remember that in such a case, each row in the table of our dataset represents
    a data point or observation, and each column in the table is a feature. The linear
    regression model tries to guess which coefficients it could use for each feature,
    such that multiplying each feature by its coefficient and adding all of the results
    together would result in something as close as possible to the target label.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以在表格数据上训练的基本线性回归模型为例。你可能记得，在这种情况下，我们的数据集表中的每一行代表一个数据点或观察值，而表中的每一列代表一个特征。线性回归模型试图猜测它可以为每个特征使用的系数，这样将每个特征乘以其系数并将所有结果相加，可以得到尽可能接近目标标签的结果。
- en: In the case of linear regression, each time the model predicted incorrectly,
    we would use the loss function to calculate the error, then calculate the gradients
    of the loss function with respect to each coefficient, and then use gradient descent
    to determine how to adjust the coefficients accordingly, and the process would
    repeat many times until the model improved or was stopped for some reason.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归的情况下，每次模型预测错误时，我们会使用损失函数来计算误差，然后计算损失函数相对于每个系数的梯度，然后使用梯度下降来确定如何相应地调整系数，这个过程会重复多次，直到模型改进或由于某种原因停止。
- en: This is where NNs become more complex than the simple regression models we implemented
    earlier in this book. In the case of NNs, we don’t have a one-to-one mapping of
    input features to coefficients. Instead, our data propagates through a complex
    network consisting of multiple layers, which each contain multiple neurons, and
    each neuron in each layer has a different set of weights (and bias values). Therefore,
    when our model makes a prediction and we use a loss function to calculate the
    error, it’s no longer simply a case of calculating the loss function’s gradient
    with respect to the coefficient of each input feature and then updating each feature’s
    coefficient and trying again. Instead, we must perform that process for all of
    the weights in each layer of the NN. One way to do this is referred to as “backward
    propagation of errors” or “backpropagation.”
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是神经网络比我们在本书前面实现的简单回归模型更复杂的地方。在神经网络的情况下，我们没有输入特征到系数的一对一映射。相反，我们的数据通过一个由多个层组成的复杂网络传播，每个层包含多个神经元，每个层的每个神经元都有不同的权重集（以及偏差值）。因此，当我们的模型做出预测并使用损失函数来计算误差时，它不再仅仅是计算损失函数相对于每个输入特征系数的梯度，然后更新每个特征的系数并再次尝试的情况。相反，我们必须为神经网络每一层的所有权重执行此过程。完成此过程的一种方法被称为“误差反向传播”或“反向传播”。
- en: With backpropagation, we update the weights in each layer, starting with the
    last layer (that is, the one closest to our output layer, which represents our
    model’s prediction), and then moving back through our network, layer by layer.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用反向传播，我们从最后一层开始更新每一层的权重（即最接近我们的输出层的那一层，它代表我们的模型预测），然后逐层反向通过我们的网络。
- en: Since the loss function for our NN is composed of several nested functions (due
    to the layers in the network), the calculation of gradients in the backpropagation
    step uses something called the **chain rule**, which is a technique in calculus
    to compute the derivatives of the loss function with respect to the weights in
    each layer. These results are then used to determine how to update the weights
    in each pass through the network.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的神经网络损失函数由几个嵌套函数组成（由于网络中的层），反向传播步骤中的梯度计算使用了一种称为**链式法则**的技术，这是微积分中计算损失函数相对于每一层权重的导数的技术。然后，这些结果用于确定如何在网络中的每一遍中更新权重。
- en: We will come back to the topic of backpropagation and the chain rule later in
    this chapter, but first, let’s dive into more detail on what kinds of algorithms
    we can use to optimize our cost function in each training pass.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面回到反向传播和链式法则的话题，但首先，让我们更详细地探讨一下我们可以在每次训练过程中使用哪些算法来优化我们的成本函数。
- en: Cost function optimization algorithms
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 成本函数优化算法
- en: We’ve already discussed using mechanisms such as gradient descent for optimizing
    our cost function during model training. In this section, we will briefly discuss
    some other common types of optimization algorithms that we can use.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了在模型训练期间使用梯度下降等机制来优化我们的成本函数。在本节中，我们将简要讨论一些我们可以使用的其他常见优化算法。
- en: Momentum
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动量
- en: This can be considered an upgrade to the basic gradient descent algorithm. When
    we discuss gradient descent in cost function optimization, we often use the analogy
    of walking downhill in a mountainous landscape until we reach the bottom of the
    mountain (or at least the bottom of a valley, which could be a “local minimum”).
    In the case of **stochastic gradient descent** (**SGD**), the analogy is more
    akin to jumping around somewhat randomly, in which case we may sometimes jump
    a little bit uphill (that is, in the incorrect direction), but overall, we usually
    end up going downhill (that is, in the correct direction). This case of jumping
    around in slightly different directions is referred to as **oscillating**. The
    Momentum algorithm accelerates SGD by navigating more prominently in the correct
    directions and reducing oscillations in incorrect directions. It does this by
    averaging the gradients of the updates in each step, which results in a smoother
    descent down the error gradient and often leads to reaching the bottom more quickly.
    The analogy used in this case is that of a ball rolling down the hill, in which
    case the movements are smoother than jumping around sporadically. Note that the
    ball can also gain momentum, which can help it to perform better even when the
    slope of the gradient is small (small gradient slopes result in slower learning
    for traditional gradient descent). In practice, Momentum almost always outperforms
    basic gradient descent.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以被视为基本梯度下降算法的升级。当我们讨论成本函数优化中的梯度下降时，我们经常使用在山区下山（或至少到达山谷底部，这可能是“局部最小值”）的类比。在**随机梯度下降**（**SGD**）的情况下，类比更像是随机跳跃，在这种情况下，我们有时会向上跳一点（即，朝错误的方向），但总体上，我们通常最终会向下走（即，朝正确的方向）。这种在不同方向跳跃的情况被称为**振荡**。动量算法通过在正确的方向上导航更明显并减少错误方向上的振荡来加速SGD。它是通过平均每一步的更新梯度来实现的，这导致沿着误差梯度的下降更加平滑，通常会导致更快地到达底部。在这种情况下使用的类比是一个球沿着山滚动，在这种情况下，运动比偶尔跳跃更平滑。请注意，球也可以获得动量，这有助于它在梯度斜率较小时（小梯度斜率会导致传统梯度下降的学习速度较慢）表现得更好。在实践中，动量几乎总是优于基本梯度下降。
- en: Adaptive Gradient Algorithm (Adagrad)
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自适应梯度算法（Adagrad）
- en: Adagrad, as the name suggests, is an adaptive optimization algorithm. That is,
    in each optimization cycle, Adagrad adapts the learning rate to each individual
    parameter. It performs smaller updates for parameters that have large gradients
    and larger updates for parameters with small gradients, which makes it particularly
    useful for dealing with sparse data and DL models with millions of parameters.
    Although it can be a useful algorithm, it can cause the learning rate to get too
    small too quickly, and effectively stop learning. This can be a problem in long
    training scenarios (such as in DL) where the learning process could prematurely
    stop. More recent variants such as **Root Mean Square Propagation** (**RMSProp**)
    and **Adaptive Moment Estimation** (**Adam**) were developed to tackle this issue,
    and we will discuss those next.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，Adagrad是一种自适应优化算法。也就是说，在每次优化周期中，Adagrad会根据每个单独的参数调整学习率。对于梯度大的参数，它执行较小的更新；对于梯度小的参数，它执行较大的更新，这使得它在处理稀疏数据和具有数百万个参数的深度学习模型时特别有用。尽管它可能是一个有用的算法，但它可能导致学习率迅速变得过小，从而有效停止学习。这在长时间训练场景（如深度学习）中可能是一个问题，因为学习过程可能会过早停止。为了解决这个问题，最近开发了如**均方根传播**（**RMSProp**）和**自适应矩估计**（**Adam**）等更近期的变体，我们将在下一节讨论这些内容。
- en: RMSProp
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RMSProp
- en: RMSProp resolves Adagrad’s rapidly diminishing learning rates by dividing the
    learning rate over a **moving average** (**MA**) of the squared gradients. Basically,
    it’s faster and often better than Adagrad.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: RMSProp通过将学习率除以平方梯度的**移动平均**（**MA**）来解决Adagrad的学习率迅速减小的问题。基本上，它比Adagrad更快，通常也更好。
- en: Adam
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Adam
- en: Adam combines the benefits of Momentum and RMSProp. It averages the gradients
    (like Momentum) and uses squared gradients (like RMSProp). It’s often the best
    choice of optimizer, especially for DL.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Adam结合了动量和RMSProp的优点。它平均梯度（像动量一样）并使用平方梯度（像RMSProp一样）。它通常是优化器的最佳选择，尤其是在深度学习中。
- en: There are many more optimization algorithms in addition to the ones we have
    covered in this section, and we may use other optimizers in later chapters, but
    we will use Adam in this chapter, so for now, we’re just introducing the popular
    algorithms that Adam builds upon.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 除了本节中我们已介绍的那些优化算法之外，还有许多其他的优化算法，我们可能在后面的章节中使用其他优化器，但本章我们将使用 Adam，所以现在我们只是介绍
    Adam 所基于的流行算法。
- en: We will dive deeper into one more important concept before we begin our hands-on
    activities, and that is the concept of activation functions.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始动手活动之前，我们将更深入地探讨一个重要概念，即激活函数的概念。
- en: Activation functions
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: So far, we’ve touched upon the topic of activation functions and how they work
    at a high level. In this section, we dive into more details on this topic and
    discuss some common types of activation functions that we can use in our NNs.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经触及了激活函数及其在高级别上如何工作的主题。在本节中，我们将更深入地探讨这个主题，并讨论一些我们可以在我们的神经网络中使用的一些常见的激活函数类型。
- en: Linear (identity) activation function
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性（恒等）激活函数
- en: This activation function simply returns whatever input we provide to it, unchanged,
    and it’s typically used for simple tasks, or it’s often used as the output layer
    in regression use cases.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个激活函数简单地返回我们提供给它的任何输入，不改变，它通常用于简单任务，或者它经常用作回归用例的输出层。
- en: It is represented mathematically as f(x) = x.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以用数学公式表示为 f(x) = x。
- en: This is generally not something we would use for the hidden layers in our network
    because it doesn’t allow for any kind of complex relationships to be learned.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常不是我们会用于我们网络中的隐藏层的，因为它不允许学习任何类型的复杂关系。
- en: Note
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It’s important to specifically call out the significance of non-linear transformations
    in NNs because the main power of NNs is their ability to combine multiple non-linear
    transformations in order to learn complex relationships in the data. Non-linear
    activation functions are, therefore, an important ingredient in complex NNs.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中明确指出非线性变换的重要性是很重要的，因为神经网络的主要力量在于它们能够组合多个非线性变换，以学习数据中的复杂关系。因此，非线性激活函数是复杂神经网络的一个重要组成部分。
- en: Even if we combined many layers together in our network, if all of them just
    implemented a linear transformation, our whole network would just perform one
    big linear transformation, which we could implement without the need for NNs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们将许多层组合到我们的网络中，如果它们都只是实现了线性变换，我们的整个网络也只会执行一个大的线性变换，这我们可以不使用神经网络来实现。
- en: Sigmoid activation function
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Sigmoid 激活函数
- en: The sigmoid function is an implementation of logistic regression, which maps
    any input into a range between 0 and 1.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数是逻辑回归的一种实现，它将任何输入映射到 0 和 1 之间的范围。
- en: It is represented mathematically as f(x) = 1 / (1 + exp(-x)).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以用数学公式表示为 f(x) = 1 / (1 + exp(-x))。
- en: 'See *Figure 9**.4* for a visual representation of this function:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 见 *图 9.4* 以了解此函数的视觉表示：
- en: '![Figure 9.4: Sigmoid function (source: https://commons.wikimedia.org/wiki/File:Sigmoid-function-2.svg)](img/B18143_09_4.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4：Sigmoid 函数（来源：https://commons.wikimedia.org/wiki/File:Sigmoid-function-2.svg）](img/B18143_09_4.jpg)'
- en: 'Figure 9.4: Sigmoid function (source: https://commons.wikimedia.org/wiki/File:Sigmoid-function-2.svg)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4：Sigmoid 函数（来源：https://commons.wikimedia.org/wiki/File:Sigmoid-function-2.svg）
- en: The sigmoid function is one of the simpler activation functions, and it has
    generally been superseded by newer functions that we will discuss next. One of
    its limitations is that it is susceptible to a problem known as the vanishing
    gradient problem, which we will describe later in this chapter. However, it can
    still be useful in the context of output neurons in binary classification problems,
    where we interpret the output as the probability of the input being in one class
    or the other.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数是较简单的激活函数之一，它通常已被我们接下来将要讨论的新函数所取代。它的一个局限性是它容易受到一个称为梯度消失问题的困扰，我们将在本章后面描述这个问题。然而，在二分类问题的输出神经元中，它仍然可以是有用的，其中我们将输出解释为输入属于某一类或另一类的概率。
- en: Hyperbolic tangent (tanh) activation function
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 双曲正切（tanh）激活函数
- en: The tanh function is like the sigmoid function but it maps any input to a value
    in the range between -1 and 1.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: tanh 函数类似于 sigmoid 函数，但它将任何输入映射到 -1 和 1 之间的值。
- en: It is represented mathematically as f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以用数学公式表示为 f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))。
- en: 'See *Figure 9**.5* for a visual representation of this function:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 见 *图 9.5* 以了解此函数的视觉表示：
- en: '![Figure 9.5: tanh function (source: https://commons.wikimedia.org/wiki/File:Mplwp_tanh.svg)](img/B18143_09_5.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5：tanh 函数（来源：https://commons.wikimedia.org/wiki/File:Mplwp_tanh.svg)](img/B18143_09_5.jpg)'
- en: 'Figure 9.5: tanh function (source: https://commons.wikimedia.org/wiki/File:Mplwp_tanh.svg)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：tanh 函数（来源：https://commons.wikimedia.org/wiki/File:Mplwp_tanh.svg）
- en: Like the sigmoid function, tanh also suffers from the vanishing gradient problem,
    but it can still be useful in practice.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 与 sigmoid 函数类似，tanh 也存在梯度消失问题，但在实际应用中仍然有用。
- en: Rectified Linear Unit (ReLU) activation function
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩形线性单元（ReLU）激活函数
- en: The ReLU function has become very popular in the last few years. Quite simply,
    it maps any positive number to itself and any negative number to zero.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 函数在最近几年变得非常流行。简单来说，它将任何正数映射到自身，任何负数映射到零。
- en: It is represented mathematically as f(x) = max(0, x).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以用数学公式表示为 f(x) = max(0, x)。
- en: 'See *Figure 9**.6* for a visual representation of this function:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 见 *图 9**.6* 中该函数的直观表示：
- en: '![Figure 9.6: ReLU function](img/B18143_09_6.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6：ReLU 函数](img/B18143_09_6.jpg)'
- en: 'Figure 9.6: ReLU function'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：ReLU 函数
- en: A major benefit of ReLU is that it is computationally efficient because the
    function is essentially just checking whether the input is greater than zero and
    simply returning the input directly if it is greater than zero, or returning zero
    if it isn’t. This is an easy mathematical computation to perform, and this simplicity
    leads to much faster training.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 的一个主要优点是它计算效率高，因为该函数本质上只是检查输入是否大于零，如果大于零则直接返回输入，如果不大于零则返回零。这是一个简单的数学计算，这种简单性导致训练速度大大加快。
- en: Another major benefit is that it doesn’t suffer from the vanishing gradient
    problem. However, it suffers from another problem known as the dying ReLU problem,
    which is a phenomenon where neurons effectively become useless due to consistently
    outputting zero. This happens because when the inputs are zero, or negative, then
    the gradient of the function becomes zero. This means that during backpropagation,
    when the weights get updated, the weights of that neuron will not be adjusted.
    This situation leads to the neuron becoming “stuck” and continually outputting
    zero – effectively causing the neuron to “die” and play no role in discriminating
    the input.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个主要优点是它不受梯度消失问题的影响。然而，它还受到另一个称为“死亡 ReLU”问题的困扰，这是一种神经元由于持续输出零而变得无用的现象。这是因为当输入为零或负数时，函数的梯度变为零。这意味着在反向传播过程中，当权重更新时，该神经元的权重将不会调整。这种情况导致神经元“卡住”，并持续输出零——实际上导致神经元“死亡”，在区分输入时不起任何作用。
- en: Leaky ReLU activation function
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Leaky ReLU 激活函数
- en: This function attempts to solve the dying ReLU problem by outputting small negative
    values when the input is less than zero.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数试图通过在输入小于零时输出小的负值来解决“死亡 ReLU”问题。
- en: It is represented mathematically as f(x) = max(0.01x, x).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以用数学公式表示为 f(x) = max(0.01x, x)。
- en: In this case, the value of 0.01 represents a small, nonzero gradient for x,
    and it’s a hyperparameter that can be changed.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，0.01 的值代表 x 的小的非零梯度，它是一个可以改变的超参数。
- en: 'See *Figure 9**.7* for a visual representation of this function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 见 *图 9**.7* 中该函数的直观表示：
- en: '![Figure 9.7: Leaky ReLU](img/B18143_09_7.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7：Leaky ReLU](img/B18143_09_7.jpg)'
- en: 'Figure 9.7: Leaky ReLU'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：Leaky ReLU
- en: As we can see in *Figure 9**.7*, Leaky ReLU avoids the problem of the outputs
    becoming zero, even when the input is negative. There is also an extension of
    Leaky ReLU called **Parametric ReLU** (**PReLU**), which allows the small, nonzero
    gradient for x to be learned by backpropagation during learning, rather than specifying
    it as a static number via a hyperparameter.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 9**.7* 所示，Leaky ReLU 避免了输出变为零的问题，即使输入是负数。还有一个 Leaky ReLU 的扩展，称为 **参数 ReLU**（**PReLU**），它允许在训练过程中通过反向传播学习
    x 的小的非零梯度，而不是通过超参数指定为静态数字。
- en: Softmax activation function
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Softmax 激活函数
- en: The softmax function is often used for the output layer of an NN in multiclass
    classification use cases, where it transforms the raw outputs of the network into
    a vector of probabilities (that is, creating a probability distribution for the
    classes).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 函数通常用于多类分类用例中的神经网络输出层，它将网络的原始输出转换为概率向量（即为类别创建概率分布）。
- en: It is represented mathematically as f(xi) = exp(xi) / Σ(exp(xj)), where j runs
    over the set of neurons in the output layer.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以用数学公式表示为 f(xi) = exp(xi) / Σ(exp(xj))，其中 j 遍历输出层中的神经元集合。
- en: It’s an extension of the sigmoid function; while the sigmoid function can be
    used to provide the probability that the input is a member of one class or another
    (in a selection between two classes), the softmax function can provide the range
    of probabilities of the input being a member of multiple classes. For example,
    if our network tries to identify images of numbers between 1 and 10, then there
    are 10 possible classes to choose from. If we provide an image of the number 1
    and use softmax in the output layer of our network, it would hopefully determine
    that the number in the image has a high probability of being 1 and lower probabilities
    for each of the other potential classes (that is, 2 through 10).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 它是sigmoid函数的扩展；sigmoid函数可以用来提供输入属于某一类或另一类的概率（在两个类别之间的选择中），而softmax函数可以提供输入属于多个类别的概率范围。例如，如果我们的网络试图识别1到10之间的数字图像，那么有10个可能的类别可以选择。如果我们提供一个数字1的图像，并在网络的输出层使用softmax，那么它可能会确定图像中的数字有很高的概率是1，而其他潜在类别的概率较低（即2到10）。
- en: There are more activation functions in addition to the ones we have covered
    in this section, but the ones we’ve covered here are among the most well known
    and widely used. Our choice of activation function can depend on the specific
    use case and other factors.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 除了本节中提到的激活函数之外，还有更多的激活函数，但我们在这里提到的这些是最为知名和广泛使用的。我们选择的激活函数可以取决于特定的用例和其他因素。
- en: Now that we’ve covered many of the important theoretical concepts in the field
    of DL, let’s put what we’ve learned into practice by building our first NN. To
    do that, we’re going to introduce some important libraries.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经覆盖了深度学习领域中的许多重要理论概念，让我们通过构建我们的第一个神经网络来将所学知识付诸实践。为此，我们将介绍一些重要的库。
- en: Libraries
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 库
- en: In this section, we describe the libraries we will use in this chapter, such
    as TensorFlow and Keras.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了我们将在本章中使用的库，例如TensorFlow和Keras。
- en: TensorFlow
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow
- en: 'TensorFlow is an **open source software** (**OSS**) library developed by the
    Google Brain team for ML and DNN research. However, it is also possible to run
    it on a wide range of systems, from mobile devices to multi-GPU setups, and it
    can have many applications beyond just ML. In this section, we will discuss some
    of its important aspects, such as the following:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是由Google Brain团队开发的用于机器学习和深度神经网络研究的开源软件库（**OSS**）。然而，它也可以在从移动设备到多GPU设置的广泛系统上运行，并且它可以在机器学习之外有众多应用。在本节中，我们将讨论其一些重要方面，例如以下内容：
- en: Tensors, which are a generalization of vectors and matrices in multiple dimensions
    (we can think of them as multi-dimensional arrays or lists). They are the basic
    building blocks in TensorFlow.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量是向量矩阵在多维度的推广（我们可以把它们想象成多维数组或列表）。它们是TensorFlow的基本构建块。
- en: '**Data-flow graphs** (**DFGs**), in which nodes in the graph represent mathematical
    operations and edges represent the data (tensors) transmitted between these nodes.
    This approach enables parallel computation across multiple devices, making TensorFlow
    suitable for training large NNs.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据流图**（**DFGs**），其中图中的节点代表数学运算，边代表在这些节点之间传输的数据（张量）。这种方法使得TensorFlow能够在多个设备上实现并行计算，使其适合训练大型神经网络。'
- en: Multiple options for model deployment, such as TensorFlow Serving for server-side
    deployments or TensorFlow Lite for mobile and IoT devices.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型部署有多种选择，例如TensorFlow Serving用于服务器端部署或TensorFlow Lite用于移动和物联网设备。
- en: Backpropagation optimization by automatically computing the gradient of the
    loss with respect to the model weights.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过自动计算损失相对于模型权重的梯度来优化反向传播。
- en: Anyone interested in ML and DL should be familiar with TensorFlow because it
    is widely used in the industry.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对机器学习和深度学习感兴趣的人应该熟悉TensorFlow，因为它在业界得到了广泛的应用。
- en: Keras
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras
- en: 'Keras is a high-level NN API written in Python that can run on top of lower-level
    frameworks such as TensorFlow, Theano, and **Cognitive Toolkit** (**CNTK**). It
    was developed to enable fast experimentation and has become the official high-level
    API of TensorFlow (as of TensorFlow 2.0). Some of its main features include the
    following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是一个用Python编写的用于在TensorFlow、Theano和**认知工具包**（**CNTK**）等底层框架之上运行的神经网络高级API。它是为了实现快速实验而开发的，并已成为TensorFlow的官方高级API（截至TensorFlow
    2.0）。它的一些主要特性包括以下内容：
- en: '**User-friendliness**: It has a simple and consistent interface that has been
    optimized for common use cases, and it provides clear error messages, as well
    as useful documentation and developer guides.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户友好性**：它有一个简单且一致的界面，针对常见用例进行了优化，并提供清晰的错误消息、有用的文档和开发者指南。'
- en: '**Modularity**: A Keras model is assembled by connecting configurable building
    blocks together. For example, we can easily construct an NN by stacking multiple
    layers together.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模块化**：Keras模型是通过连接可配置的构建块组装而成的。例如，我们可以通过堆叠多个层轻松构建神经网络。'
- en: '**Extensibility**: We can write custom building blocks to express new ideas
    for research, and create new layers, loss functions, and models.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：我们可以编写自定义构建块来表达新的研究想法，并创建新的层、损失函数和模型。'
- en: The core data structure of Keras is a model, which is a way to organize layers.
    The main type of model is the Sequential model, which is a linear stack of layers,
    but for more complex architectures, we can use the Keras functional API, which
    allows us to build our own custom graphs of layers. A layer in Keras is a class
    that implements common NN operations, and Keras includes a wide range of pre-defined
    layers that we can use to build our models. It also allows us to specify the loss
    function and the metrics we want to evaluate during the training phase, and it
    provides many pre-defined loss functions such as `mean_squared_error` and metrics
    such as `accuracy`. Keras also includes many optimization algorithms, such as
    SGD. Overall, it includes lots of useful tools that make it easy for us to create
    NNs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的核心数据结构是模型，这是一种组织层的方式。主要模型类型是Sequential模型，它是一系列层的线性堆叠，但对于更复杂的架构，我们可以使用Keras功能API，这允许我们构建自己的自定义层图。Keras中的层是一个实现常见神经网络操作的类，Keras包括一系列预定义的层，我们可以使用它们来构建模型。它还允许我们在训练阶段指定我们想要评估的损失函数和度量标准，并提供许多预定义的损失函数，如`mean_squared_error`和度量标准如`accuracy`。Keras还包括许多优化算法，如SGD。总的来说，它包括许多有用的工具，使我们能够轻松创建神经网络。
- en: Now that we’ve introduced the relevant libraries, let’s dive in and build our
    first NN!
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了相关的库，让我们深入探讨并构建我们的第一个神经网络！
- en: Implementing an MLP in TensorFlow
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在TensorFlow中实现MLP
- en: 'In this section, we will build an MLP using TensorFlow. We will use Keras as
    the high-level API for interacting with TensorFlow. We can use the same Vertex
    AI Workbench notebook we created in [*Chapter* *5*](B18143_05.xhtml#_idTextAnchor168)
    for this purpose. In that notebook, perform the following steps:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用TensorFlow构建一个MLP。我们将使用Keras作为与TensorFlow交互的高级API。我们可以使用在[*第5章*](B18143_05.xhtml#_idTextAnchor168)中创建的相同的Vertex
    AI Workbench笔记本来完成此目的。在那个笔记本中，执行以下步骤：
- en: Navigate into the folder named `Google-Machine-Learning-for-Solutions-Architects`.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到名为`Google-Machine-Learning-for-Solutions-Architects`的文件夹。
- en: Double-click on the `Chapter-09` folder within it and then double-click on the
    `Chapter-9-TF-Keras.ipynb` file to open it.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在其中双击`Chapter-09`文件夹，然后双击`Chapter-9-TF-Keras.ipynb`文件以打开它。
- en: When prompted to select a kernel, select **TensorFlow**.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当提示选择内核时，选择**TensorFlow**。
- en: The notebook we have opened contains some Python code that creates and tests
    an MLP using Keras and TensorFlow.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打开的笔记本中包含一些Python代码，使用Keras和TensorFlow创建和测试了一个MLP。
- en: Run each of the cells in the notebook by clicking on each cell and pressing
    *Shift* + *Enter* on your keyboard. If you see any errors related to CUDA, you
    can ignore them because we are not using GPUs in this notebook.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击每个单元格并在键盘上按*Shift* + *Enter*来运行笔记本中的每个单元格。如果您看到任何与CUDA相关的错误，请忽略它们，因为我们在这个笔记本中不使用GPU。
- en: Our code in the first cell of the notebook imports the necessary libraries and
    modules, loads a dataset using the `make_moons` function from `sklearn.datasets`,
    and then visualizes the data using `matplotlib`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本的第一单元格中的代码导入必要的库和模块，使用`sklearn.datasets`中的`make_moons`函数加载数据集，然后使用`matplotlib`可视化数据。
- en: 'In this case, we’re using the `moons` dataset, which is a mathematically generated
    dataset for binary classification that is often used as a simple test case for
    ML algorithms, especially those designed to handle non-linear data (such as NNs).
    The dataset consists of a two-dimensional array of two features (usually visualized
    on an X-Y plane) and a binary label (0 or 1) for each sample, and the samples
    are generated in such a way that they form two crescent moon-like shapes when
    plotted (hence the name “moons”), with each “moon” corresponding to one class.
    See *Figure 9**.**8* for reference:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用`moons`数据集，这是一个用于二分类的数学生成数据集，常被用作ML算法的简单测试案例，尤其是那些设计用于处理非线性数据（如NNs）的算法。该数据集由两个特征的两维数组组成（通常在X-Y平面上可视化）以及每个样本的二元标签（0或1），样本生成的方式使得当绘制时形成两个新月形形状（因此得名“moons”），每个“月亮”对应一个类别。参见*图9**.**8*以获取参考：
- en: '![Figure 9.8: The moons dataset](img/B18143_09_9.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图9.8：moons数据集](img/B18143_09_9.jpg)'
- en: 'Figure 9.8: The moons dataset'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8：moons数据集
- en: Note that the main characteristic of the dataset is its non-linearity (that
    is, the decision boundary separating the two classes is not a straight line).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，数据集的主要特征是其非线性（即，分隔两个类别的决策边界不是一条直线）。
- en: 'The code in the second cell of our Jupyter notebook then does the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们Jupyter笔记本第二个单元格中的代码执行以下操作：
- en: Splits the dataset into a training set and a test set
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集分为训练集和测试集
- en: Defines a Sequential model (which means that the layers are stacked on top of
    each other)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个Sequential模型（这意味着层是堆叠在一起的）
- en: Adds an input layer and the first hidden layer with 32 neurons and the `relu`
    activation function
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个输入层和第一个隐藏层，包含32个神经元和`relu`激活函数
- en: Adds a second hidden layer with 32 neurons and the `relu` activation function
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个第二个隐藏层，包含32个神经元和`relu`激活函数
- en: Adds an output layer with one neuron (for binary classification) and the `sigmoid`
    activation function
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个输出层，包含一个神经元（用于二分类）和`sigmoid`激活函数
- en: Compiles the model with the `adam` optimizer and the `binary_crossentropy` loss
    function (suitable for binary classification)
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`adam`优化器和`binary_crossentropy`损失函数（适用于二分类）编译模型
- en: Trains the model for 50 epochs
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对模型进行50个周期的训练
- en: 'When the code is running, you should see outputs from each epoch, as depicted
    in *Figure 9**.10*:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当代码运行时，你应该会看到每个周期的输出，如图*图9**.10*所示：
- en: '![Figure 9.9: Training epoch outputs](img/B18143_09_10.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图9.9：训练周期输出](img/B18143_09_10.jpg)'
- en: 'Figure 9.9: Training epoch outputs'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9：训练周期输出
- en: Note that `loss` and `val_loss` (validation loss) should decrease as the training
    progresses, and `accuracy` and `val_accuracy` (validation accuracy) should increase
    as the training progresses.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，随着训练的进行，`loss`和`val_loss`（验证损失）应该减少，而`accuracy`和`val_accuracy`（验证准确率）应该增加。
- en: 'Next, our code in the third cell does the following:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们第三个单元格中的代码执行以下操作：
- en: Evaluates our model using the `model.evaluate` method, which returns the loss
    value and metric values (in this case, `accuracy`) for the model, in test mode.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`model.evaluate`方法评估我们的模型，该方法在测试模式下返回模型的损失值和度量值（在这种情况下，`accuracy`）。
- en: Gets some predictions from our model using the `model.predict` method, which
    outputs the probabilities that each input sample belongs to the positive class.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`model.predict`方法从我们的模型中获得一些预测，该方法输出每个输入样本属于正类的概率。
- en: In order to treat this as a binary classification use case, our code then converts
    these probabilities into binary class labels based on a threshold of 0.5 (that
    is, anything with a probability of more than 0.5 is deemed to be a member of the
    positive class).
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了将其视为二分类用例，我们的代码随后将这些概率转换为基于0.5阈值的二元类别标签（即，任何概率超过0.5的东西都被认为是正类成员）。
- en: Finally, we print out the first 10 predictions for a quick check. These outputs
    will be in the form of 0s and 1s, denoting the predicted class labels.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们打印出前10个预测结果以进行快速检查。这些输出将以0和1的形式呈现，表示预测的类别标签。
- en: That’s it! You’ve just created an NN! It might not be quite as smart as a human,
    but this is a basic example of an MLP in TensorFlow with Keras. We could extend
    this to more advanced DL use cases by adjusting things such as the number of layers,
    neurons, types of activation functions, types of optimizers, loss functions, and
    training configurations (such as the number of epochs, batch size, and so on).
    Great job!
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你已经创建了一个神经网络！它可能没有人类那么聪明，但这在TensorFlow中使用Keras的MLP（多层感知器）的基本示例。我们可以通过调整诸如层数、神经元类型、激活函数类型、优化器类型、损失函数以及训练配置（如epoch数量、批量大小等）等方式，将其扩展到更高级的深度学习用例。做得好！
- en: Next, we will dive into additional DL concepts, such as different types of NN
    architectures, challenges in applications of DNN, and optimization considerations.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入研究额外的深度学习概念，例如不同类型的神经网络架构、深度神经网络应用中的挑战，以及优化考虑。
- en: NN architectures, challenges, and optimization
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络架构、挑战和优化
- en: We’ve primarily covered the basics of NNs so far in this chapter, and in this
    section, we will expand our discussion to include different types of NN architectures
    that can be used for different types of real-world use cases, as well as some
    challenges that are often encountered when training them. Finally, we will discuss
    how to optimize our NNs to address those challenges.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本章我们已经主要介绍了神经网络的基础知识，在本节中，我们将扩展我们的讨论，包括不同类型的神经网络架构，这些架构可以用于不同类型的实际应用场景，以及训练它们时经常遇到的挑战。最后，我们将讨论如何优化我们的神经网络以解决这些挑战。
- en: Common NN architectures
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见的神经网络架构
- en: The “architecture” of an NN refers to its structure in terms of the number of
    layers it contains and the number of neurons in each layer, as well as any special
    characteristics that influence how information is propagated through the network.
    The NN architectures we’ve described so far in this chapter are the simplest forms
    of ANNs, which are referred to as **feed-forward NNs** (**FFNNs**). Information
    in these networks travels in one direction only, from the input layer, through
    the hidden layers, to the output layer. Next, let’s take a look at some other
    commonly used NN architectures. We will introduce them at a high level here, and
    we will dive into much more detail in later chapters.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的“架构”指的是其结构，包括它包含的层数、每层的神经元数量，以及任何影响信息通过网络传播的特殊特性。在本章中我们描述的神经网络架构是ANN（人工神经网络）的最简单形式，被称为**前馈神经网络**（FFNN）。这些网络中的信息仅沿一个方向传播，从输入层，通过隐藏层，到输出层。接下来，让我们看看一些其他常用的神经网络架构。在这里，我们将从高层次介绍它们，并在后面的章节中深入探讨更多细节。
- en: Note
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When we talk about information traveling through the network in this section,
    we are not referring to the backpropagation step, because that is a separate step
    implemented during the iterative learning process. We are simply referring to
    how data is processed through our network in each training pass, or at inference
    time.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在本节中谈论信息通过网络传播时，我们并不是指反向传播步骤，因为那是在迭代学习过程中实现的单独步骤。我们只是简单地指数据在每次训练遍历或推理时间通过我们的网络的方式。
- en: Convolutional NNs (CNNs)
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）
- en: CNNs are commonly used in **computer vision** (**CV**) for use cases such as
    object recognition and picture categorization. Consider the scenario where we
    wish to teach our model to recognize images of cats, keeping in mind that these
    images might take many different forms, such as being captured at various distances
    and perspectives. Our model would need to establish a kind of visual understanding
    of what a cat is, such as the shape of its face, ears, body, and tail, in order
    to correctly identify cats in the images.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: CNN在**计算机视觉**（CV）中常用，用于诸如物体识别和图像分类等用例。考虑这样一个场景，我们希望训练我们的模型来识别猫的图像，同时考虑到这些图像可能以多种不同的形式出现，例如在不同的距离和视角下被捕捉。我们的模型需要建立一种对猫的视觉理解，例如它的面部形状、耳朵、身体和尾巴，以便在图像中正确识别猫。
- en: CNNs do this by breaking pictures down into smaller components or “features”
    and learning each one separately. In this way, the network learns to detect small
    details in a larger image, such as an edge or a curve, and then combines those
    into larger features such as a single whisker or a portion of a cat’s ear, regardless
    of where they appear in the image, and then combining those characteristics to
    identify a cat. The concepts of **convolutional layers**, **pooling layers**,
    and **fully connected (FC) layers** are used by CNNs to do this. In later chapters
    of this book, we will dive further into those ideas and how they function.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）通过将图片分解成更小的组件或“特征”并分别学习每个特征来实现这一点。这样，网络学会在更大的图像中检测小细节，例如边缘或曲线，然后将这些细节组合成更大的特征，如单根胡须或猫耳朵的一部分，无论它们在图像中的位置如何，然后将这些特征组合起来以识别猫。卷积神经网络（CNNs）使用**卷积层**、**池化层**和**全连接（FC）层**的概念来完成这项工作。在这本书的后续章节中，我们将进一步探讨这些概念及其工作原理。
- en: Recurrent NNs (RNNs) and long short-term memory (LSTM) networks
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）和长短期记忆（LSTM）网络
- en: RNNs are designed to find patterns in sequential data such as language or time
    series. In RNNs, the network contains loops, which enable information to be persisted
    from one step to the next, and this is what allows RNNs to create a kind of memory,
    unlike basic NNs that assume all inputs (and outputs) are independent of each
    other. In this way, the network blends current data with inputs from earlier phases
    in each step, which is important for activities such as language comprehension,
    in which the model needs to understand each word in relation to the other words
    in the input (that is, the words are not entirely independent).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）被设计用来在序列数据中寻找模式，如语言或时间序列。在RNNs中，网络包含循环，这使得信息可以从一步持续到下一步，这也是RNNs能够创建一种记忆的原因，与假设所有输入（和输出）都是相互独立的基本神经网络不同。通过这种方式，网络在每个步骤中将当前数据与早期阶段输入的数据混合，这对于语言理解等活动非常重要，在这些活动中，模型需要理解每个词与输入中其他词的关系（即，词不是完全独立的）。
- en: However, one of the problems with RNNs is that, for reasons we’ll cover later,
    they “forget” prior inputs when dealing with long sequences. To address these
    issues, variations such as LSTM networks and **gated recurrent unit** (**GRU**)
    networks have been invented, which use gates and other techniques to maintain
    memory.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，循环神经网络（RNNs）的一个问题是，由于我们将在后面讨论的原因，它们在处理长序列时会“忘记”先前的输入。为了解决这些问题，已经发明了诸如长短期记忆（LSTM）网络和**门控循环单元**（**GRU**）网络等变体，它们使用门和其他技术来保持记忆。
- en: Autoencoders (AEs)
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自编码器（AEs）
- en: 'AEs are used to learn efficient encodings of unlabeled data, usually to reduce
    the dimensionality of the data. An AE’s basic idea is pretty straightforward:
    it is trained to try to duplicate its input to its output. Although it might seem
    like a simple (and redundant) operation, the restrictions we place on the network
    force it to discover interesting aspects of the data. Most commonly, we limit
    the number of nodes in the hidden layers, forcing the network to learn a compressed
    view of the data.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器（AEs）用于学习未标记数据的有效编码，通常用于降低数据的维度。自编码器（AEs）的基本思想相当简单：它被训练来尝试复制其输入到输出。尽管这可能看起来像是一个简单（且冗余）的操作，但我们施加给网络的限制迫使它发现数据中的有趣方面。最常见的是，我们限制隐藏层中的节点数量，迫使网络学习数据的压缩视图。
- en: AEs consist of an encoder, which encodes the input data as a compressed representation
    in a reduced-dimensional space, and a decoder, which attempts to reconstruct the
    input from the reduced-dimensional representation (that is, it “decodes” the compressed
    representation). The goal during training is to create a reconstructed output
    that is as close as possible to the input so that the network can learn to rebuild
    the input from the compressed representation.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器（AEs）由一个编码器和一个解码器组成。编码器将输入数据编码为在低维空间中的压缩表示，解码器则尝试从低维表示中重建输入（即“解码”压缩表示）。在训练过程中的目标是创建一个重建输出，使其尽可能接近输入，以便网络能够学会从压缩表示中重建输入。
- en: In the real world, AEs are used for applications such as anomaly detection and
    recommendation systems. One particularly popular application of AEs is in **generative
    AI** (**GenAI**) models. In this case, after the AE has been trained, the decoder
    can generate new data that mimics the training data.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，自编码器（AEs）被用于异常检测和推荐系统等应用。自编码器（AEs）的一个特别流行的应用是在**生成式人工智能**（**GenAI**）模型中。在这种情况下，一旦自编码器被训练，解码器就可以生成新的数据，这些数据模仿了训练数据。
- en: Generative adversarial networks (GANs)
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）
- en: The basic objective of GANs, which are made up of two competing NNs called a
    generator and a discriminator, is to create new fake data that closely matches
    “real” data (as determined by the training data). The two networks are trained
    in tandem using a **minimax** game, which is a type of game where the generator
    tries to trick the discriminator, and the discriminator tries to reliably distinguish
    real data from the generated data. During training, the generator becomes ever
    more accurate at providing data that appears real, while the discriminator becomes
    increasingly skillful at identifying forgeries.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: GANs（生成对抗网络）的基本目标是创建与“真实”数据（由训练数据确定）非常接近的新伪造数据。GANs由两个相互竞争的神经网络组成，一个称为生成器，另一个称为判别器。这两个网络通过一种**最小-最大**游戏协同训练，这是一种生成器试图欺骗判别器，而判别器试图可靠地区分真实数据和生成数据的游戏。在训练过程中，生成器在提供看似真实的数据方面变得越来越准确，而判别器在识别伪造方面变得越来越熟练。
- en: Transformer networks
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Transformer网络
- en: Transformer networks are a type of model architecture invented by Google, in
    which the breakthrough innovation is the use of **self-attention** mechanisms,
    or **multi-head attention**, which enables the model to consider the relative
    significance of various words in a phrase while generating an output. For example,
    in a sentence such as “the dog tried to jump over the pond, but it was too wide,”
    the word “it” could refer to either the pond or the dog. To humans, it seems pretty
    obvious that it refers to the pond, but that’s because we’re using contextual
    awareness to intuit what makes the most sense. However, this is not inherently
    obvious to an ML model, and self-attention is the mechanism that allows the model
    to get a better understanding of the contextual meaning of each word in the sentence.
    The Transformer architecture also includes the concept of an encoder and a decoder,
    which are both made up of a stack of identical layers. And, because the self-attention
    mechanism alone does not account for the location of words in the input sequence,
    the Transformer design additionally contains a **positional encoding** system
    to track the positions of the words. We will dive into all of these components
    in much greater detail in a later chapter.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer网络是Google发明的一种模型架构，其突破性创新是使用**自注意力**机制或**多头注意力**，这使得模型在生成输出时能够考虑短语中各种词语的相对重要性。例如，在句子“the
    dog tried to jump over the pond, but it was too wide”中，“it”这个词可能指的是池塘或狗。对人类来说，这似乎非常明显，因为它指的是池塘，这是因为我们正在使用上下文意识来直观地判断什么最有意义。然而，这对机器学习模型来说并不明显，自注意力机制是允许模型更好地理解句子中每个词语的上下文意义的机制。Transformer架构还包括编码器和解码器的概念，它们都由一系列相同的层组成。此外，由于自注意力机制本身并不考虑输入序列中词语的位置，因此Transformer设计还包含一个**位置编码**系统来跟踪词语的位置。我们将在后面的章节中更详细地探讨所有这些组件。
- en: Another advantage provided by Transformer models is that they handle all inputs
    in parallel, as opposed to sequence-based models such as RNNs or LSTM networks,
    and this can significantly speed up training and inference.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型提供的另一个优点是，它们可以并行处理所有输入，而不同于基于序列的模型，如RNN或LSTM网络，这可以显著加快训练和推理速度。
- en: 'Transformers have proven to be very useful for **natural language processing**
    (**NLP**) tasks including **sentiment analysis** (**SA**), text summarization,
    and machine translation, and the Transformer architecture serves as the foundation
    for models such as **Generative Pre-trained Transformer** (**GPT**), **Bidirectional
    Encoder Representations from Transformers** (**BERT**), and T5\. If you’re interested
    in learning more about this ground-breaking technology, I recommend reading the
    pivotal research paper that first introduced the concept of Transformers (Vaswani,
    A. et al., 2017), which can be found at the following URL: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer网络已被证明在自然语言处理（NLP）任务中非常有用，包括情感分析（SA）、文本摘要和机器翻译，Transformer架构是**生成预训练Transformer**（GPT）、**双向编码器表示来自Transformer**（BERT）和T5等模型的基础。如果您想了解更多关于这项突破性技术的信息，我建议阅读首次介绍Transformer概念的标志性研究论文（Vaswani,
    A. et al., 2017），该论文可以在以下网址找到：[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)。
- en: There are more types of NN architectures in addition to the ones we have covered
    in this section, but the ones we’ve covered here are among the most well known
    and widely used. Researchers are constantly creating and experimenting with new
    NN configurations, and the choice of network configuration relies on the issue
    we’re seeking to resolve because each network type has advantages and disadvantages.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 除了本节中提到的神经网络架构之外，还有更多类型的架构，但我们在这里讨论的这些是其中最知名和最广泛使用的。研究人员不断创造和实验新的神经网络配置，网络配置的选择取决于我们试图解决的问题，因为每种网络类型都有其优势和劣势。
- en: Next, let’s discuss some common challenges that people run into when training
    and using NNs.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论一些人们在训练和使用神经网络时遇到的一些常见挑战。
- en: Common NN challenges
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见神经网络挑战
- en: In addition to all of the challenges we covered in earlier chapters with regard
    to traditional ML implementations, DNNs introduce their own set of challenges,
    such as interpretability, cost, and vanishing or exploding gradients.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们在前面章节中讨论的传统机器学习实现的所有挑战之外，DNNs还引入了它们自己的一套挑战，例如可解释性、成本以及梯度消失或爆炸。
- en: Interpretability
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可解释性
- en: Interpretability refers to how easily we can understand the inner workings of
    our models, and the reasons behind the decisions they produce. Models such as
    linear regression are generally quite easy to understand and explain because their
    outputs are just a straightforward mathematical transformation of whatever input
    is provided to the model.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性指的是我们理解模型内部工作原理以及它们做出决策背后的原因的难易程度。例如，线性回归模型通常很容易理解和解释，因为它们的输出只是对模型提供的任何输入的简单数学变换。
- en: However, DNNs can be extremely complex, with thousands of neurons and billions
    of parameters that influence their outputs. Also, their outputs are usually not
    just linear transformations of their inputs but instead are non-linear in nature.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，深度神经网络（DNNs）可能极其复杂，拥有数千个神经元和数十亿个参数，这些参数会影响它们的输出。此外，它们的输出通常不仅仅是输入的线性变换，而是本质上是非线性的。
- en: In a later chapter, we will discuss the importance of interpretability in much
    more detail and will introduce mechanisms that can help us to better understand
    how our models work.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在后面的章节中，我们将更详细地讨论可解释性的重要性，并介绍可以帮助我们更好地理解模型工作原理的机制。
- en: Cost
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 成本
- en: DNNs can require a lot of computing resources to train and host, which can result
    in monetary expenses. If we consider the example of highly complex models with
    billions of parameters, those models can take weeks or even months to train, using
    large numbers of very powerful servers with the latest generation of cutting-edge
    GPUs. Those kinds of resources are not cheap, so we need to ensure that our models
    are optimized to use computing resources as efficiently as possible.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: DNNs在训练和托管时可能需要大量的计算资源，这可能导致货币支出。如果我们考虑具有数十亿参数的高度复杂模型，这些模型可能需要数周甚至数月的时间来训练，使用大量最新一代的尖端GPU的非常强大的服务器。这些资源并不便宜，因此我们需要确保我们的模型尽可能高效地使用计算资源。
- en: Vanishing gradient problem
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度消失问题
- en: We briefly touched on this topic in earlier sections of this chapter, so let’s
    take a look at this concept in more detail. The vanishing gradient problem develops
    when the gradients of the loss function are so tiny that they essentially vanish,
    leading to sluggish weight updates in the network’s first layers. Remember that
    backpropagation uses the chain rule of differentiation, which involves multiplying
    a sequence of derivatives (or gradients). This means that if the values of the
    derivatives are less than 1, we are essentially dividing them in an exponential
    manner into smaller and smaller values as we propagate back through the network.
    Because of this, early layers learn much more slowly than later layers.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章的前几节简要提到了这个话题，所以让我们更详细地探讨这个概念。当损失函数的梯度变得非常小，以至于几乎消失时，就会产生梯度消失问题，导致网络第一层的权重更新缓慢。记住，反向传播使用微分链式法则，这涉及到乘以一系列的导数（或梯度）。这意味着如果导数的值小于1，我们在通过网络反向传播时，实际上是以指数方式将它们分成越来越小的值。正因为如此，早期层的学习速度比后期层慢得多。
- en: When utilizing activation functions such as the sigmoid or tanh functions, which
    condense their input into a small range, this problem becomes more prominent.
    The sigmoid function, for example, compresses input values into a range between
    zero and one, which means that even when the inputs are large in size (either
    positive or negative), the output of the sigmoid function is between zero and
    one, and the result is that the gradients reduce to tiny values, causing backpropagation-based
    learning to slow down significantly.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用sigmoid或tanh等将输入压缩到小范围的激活函数时，这个问题变得更加明显。例如，sigmoid函数将输入值压缩到零到一的范围，这意味着即使输入值很大（无论是正还是负），sigmoid函数的输出也介于零和一之间，结果是梯度减少到极小的值，导致基于反向传播的学习速度显著减慢。
- en: Exploding gradient problem
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度爆炸问题
- en: On the other hand, the exploding gradient problem happens when the gradient
    becomes too large, causing the weights in the network to be updated by excessive
    increments. This can result in the network’s performance oscillating wildly, causing
    the model training process to fail.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当梯度变得过大时，会导致网络中的权重通过过大的增量更新，这会导致网络的性能剧烈波动，从而使模型训练过程失败。
- en: The exploding gradient problem is more common in RNNs, especially with long
    sequences, but it can occur in any type of network.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度爆炸问题在RNN中更为常见，尤其是在长序列中，但它可以发生在任何类型的网络中。
- en: Optimizations to help prevent vanishing or exploding gradients
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 防止梯度消失或梯度爆炸的优化
- en: Now that we have a better understanding of how vanishing and exploding gradient
    issues take place, the following considerations can help reduce their likelihood
    of occurring.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对梯度消失和梯度爆炸问题发生的方式有了更好的理解，以下考虑因素可以帮助降低它们发生的可能性。
- en: Weight initialization
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 权重初始化
- en: Effective weight initialization can help mitigate problems of vanishing and
    exploding gradients. For example, techniques such as Xavier (Glorot) initialization
    and He initialization can help set the initial weights to values that prevent
    the gradients from becoming too small or too large early in training.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的权重初始化可以帮助减轻梯度消失和梯度爆炸的问题。例如，Xavier（Glorot）初始化和He初始化等技术可以帮助将初始权重设置为防止梯度在训练初期变得过小或过大的值。
- en: Choice of activation functions
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 激活函数的选择
- en: As we discussed, using activation functions that squash their inputs, such as
    sigmoid or tanh, can increase the likelihood of encountering vanishing and exploding
    gradient issues. Therefore, it’s best to avoid those activation functions in cases
    that are prone to those issues. We can instead use activation functions such as
    Leaky ReLU or PReLU, as these functions do not squash their inputs.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论的，使用压缩其输入的激活函数，如sigmoid或tanh，会增加遇到梯度消失和梯度爆炸问题的可能性。因此，在容易遇到这些问题的场合最好避免使用这些激活函数。我们可以改用Leaky
    ReLU或PReLU等激活函数，因为这些函数不会压缩它们的输入。
- en: Batch normalization
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 批标准化
- en: Using this technique, we can normalize the output of a layer to stabilize the
    means and variances of each layer’s inputs. This helps control the scale of gradients,
    mitigating both vanishing and exploding gradient problems.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种技术，我们可以将层的输出归一化，以稳定每个层输入的均值和方差。这有助于控制梯度的规模，减轻梯度消失和梯度爆炸问题。
- en: Gradient clipping
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 梯度裁剪
- en: This technique puts a pre-defined limit or threshold on the gradients to prevent
    them from getting too large, which is especially useful for tackling the exploding
    gradient problem.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术对梯度设置了一个预定义的限制或阈值，以防止它们变得过大，这对于解决梯度爆炸问题特别有用。
- en: Architectural methods
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 架构方法
- en: When we discussed RNNs earlier in this section, we mentioned that they “forget”
    prior inputs when dealing with long sequences. The vanishing gradient problem
    is one factor that causes this to happen, and this is one of the reasons why certain
    architectures such as LSTM and GRU were designed to address these issues in the
    context of RNNs by using a form of gating in their structure. Therefore, sometimes
    the choice of NN architecture can reduce the likelihood of encountering vanishing
    and exploding gradient problems.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节前面讨论RNN时，我们提到它们在处理长序列时会“忘记”先前的输入。梯度消失问题是导致这种情况的一个因素，这也是为什么某些架构，如LSTM和GRU，被设计用来在RNN的上下文中通过在其结构中使用一种门控机制来解决这些问题的原因之一。因此，有时选择NN架构可以降低遇到梯度消失和梯度爆炸问题的可能性。
- en: These problems and their solutions are major factors in understanding how DL
    models work and how to effectively train DNNs.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题和它们的解决方案是理解深度学习模型如何工作以及如何有效地训练深度神经网络的主要因素。
- en: We’ve covered a lot of new concepts and terminology in this chapter. Let’s take
    a moment to summarize everything we’ve learned.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们介绍了许多新的概念和术语。让我们花点时间总结一下我们所学的所有内容。
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we started by exploring the comparison between artificial neurons
    (in the form of the perceptron) and biological neurons in the human brain. We
    then extended this idea to describe the activity of multiple neurons in an NN,
    both in terms of combining multiple perceptrons together and in terms of how the
    tiny neurons in our brain work together to produce extremely complex higher-level
    functions.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先探讨了人工神经元（以感知器形式）与人类大脑中的生物神经元之间的比较。然后，我们将这一想法扩展到描述神经网络中多个神经元的活动，包括将多个感知器组合在一起以及我们大脑中的微小神经元如何协同工作以产生极其复杂的高级功能。
- en: We then dived deeper into the inner workings and components of ANNs, including
    concepts such as activation functions and backpropagation. We discussed many different
    types of activation functions, including how they work and what use cases are
    most appropriate for them.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们更深入地探讨了人工神经网络的内幕和工作组件，包括诸如激活函数和反向传播等概念。我们讨论了许多不同类型的激活函数，包括它们的工作原理以及最适合它们的用例。
- en: 'In the context of backpropagation, we learned about various types of commonly
    used cost function optimization algorithms, such as Momentum and Adam, and then
    we introduced two very important libraries for DL: TensorFlow and Keras.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播的背景下，我们学习了各种常用的成本函数优化算法，例如动量法和Adam法，然后我们介绍了两个非常重要的深度学习库：TensorFlow和Keras。
- en: Next, we built our first NN using those libraries, and we tested that network
    by successfully getting predictions based on the `moons` dataset, which we also
    explored in some detail.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用这些库构建了我们第一个神经网络，并通过基于`moons`数据集成功获得预测来测试了这个网络，我们也在本章中对其进行了详细探讨。
- en: After building our first simple NN, we expanded our discussion to cover more
    advanced kinds of NN architectures and their use cases, and we explored common
    challenges that people often run into when training and using NNs, as well as
    some approaches we can use to optimize our networks in order to reduce the likelihood
    of running into those issues.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建了我们第一个简单的神经网络之后，我们扩展了讨论范围，涵盖了更高级的神经网络架构及其用例，并探讨了人们在训练和使用神经网络时经常遇到的常见挑战，以及我们可以用来优化网络以减少遇到这些问题的可能性的方法。
- en: These are some of the more advanced concepts in ML, so if you have understood
    the content we’ve covered in this chapter, then you have built an important foundation
    for the deeper dives we will perform on these concepts in later chapters.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是机器学习领域的一些更高级的概念，所以如果你已经理解了本章所涵盖的内容，那么你已经为我们在后续章节中对这些概念进行更深入探讨奠定了重要的基础。
- en: In the next chapter, let’s explore how we can bring trained models into production
    to host them for serving real-world use cases.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，让我们探讨如何将训练好的模型投入生产，以便为现实世界的用例提供服务。
