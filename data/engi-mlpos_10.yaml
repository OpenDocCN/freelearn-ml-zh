- en: 'Chapter 8: APIs and Microservice Management'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章：API和微服务管理
- en: In this chapter, you will learn about APIs and microservice management. So far,
    we have deployed ML applications that are served as APIs. Now we will look into
    how to develop, organize, manage, and serve APIs. You will learn the principles
    of API and microservice design for ML inference so that you can design your own
    custom ML solutions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解API和微服务管理。到目前为止，我们已经部署了作为API提供服务的ML应用程序。现在我们将探讨如何开发、组织、管理和提供API。你将学习API和微服务设计的原理，以便你可以设计自己的定制ML解决方案。
- en: 'In this chapter, we will learn by doing as we build a microservice using FastAPI
    and Docker and serve it as an API. For this, we will go through the fundamentals
    of designing an API and microservice for an ML model trained previously (in [*Chapter
    4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074), *Machine Learning Pipelines*).
    Lastly, we will reflect on some key principles, challenges, and tips to design
    a robust and scalable microservice and API for test and production environments.
    The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过构建使用FastAPI和Docker的微服务并将其作为API提供服务来实践学习。为此，我们将学习设计用于之前训练的ML模型（在[*第4章*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074)，*机器学习管道*）的API和微服务的基础知识。最后，我们将反思一些关键原则、挑战和技巧，以设计一个健壮且可扩展的微服务和API，用于测试和生产环境。本章将涵盖以下主题：
- en: Introduction to APIs and microservices
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API和微服务的介绍
- en: The need for microservices for ML
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML对微服务的需求
- en: Old is gold – REST API-based microservices
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旧的是金子 - 基于REST API的微服务
- en: Hands-on implementation of serving an ML model as an API
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实践实现将ML模型作为API提供服务
- en: Developing a microservice using Docker
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Docker开发微服务
- en: Testing the API service
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试API服务
- en: Introduction to APIs and microservices
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: API和微服务的介绍
- en: APIs and microservices are powerful tools that help to enable your **ML** (**ML**)
    models to become useful in production or legacy systems for serving the models
    or communicating with other components of the system. Using APIs and microservices,
    you can design a robust and scalable ML solution to cater to your business needs.
    Let's take a look at what APIs and microservices are and how they realize your
    model's potential in the real world.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: API和微服务是强大的工具，有助于使你的**ML**（**ML**）模型在生产或遗留系统中变得有用，用于提供模型或与系统的其他组件进行通信。使用API和微服务，你可以设计一个健壮且可扩展的ML解决方案，以满足你的业务需求。让我们看看API和微服务是什么，以及它们如何实现你的模型在现实世界中的潜力。
- en: What is an Application Programming Interface (API)?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是应用程序编程接口（API）？
- en: 'An **API** is the gateway that enables developers to communicate with an application.
    APIs enable two things:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**API**是开发者与应用程序通信的网关。API使开发者能够实现两件事：'
- en: Access to an application's data
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问应用程序的数据
- en: The use of an application's functionality
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用应用程序的功能
- en: By accessing and communicating with application data and functionalities, APIs
    have enabled the world's electronics, applications, and web pages to communicate
    with each other in order to work together to accomplish business or operations-centric
    tasks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过访问和与应用程序数据和功能进行通信，API已经使世界的电子设备、应用程序和网页能够相互通信，以便共同完成以业务或运营为中心的任务。
- en: '![Figure 8.1 – Workings of an API'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.1 – API的工作原理'
- en: '](img/B16572_08_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16572_08_01.jpg]'
- en: Figure 8.1 – Workings of an API
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – API的工作原理
- en: 'In *Figure 8.1*, we can see the role of an API as it enables access to application
    data (from the database) and communication with third parties or other applications
    such as mobile applications (for mobile users), weather applications (on mobile
    or the web), electric cars, and so on. APIs have been in operation since the dawn
    of computers, intending to enable inter-application communication. Over time,
    we have seen developers come to a consensus with protocols such as **Simple Object
    Access Protocol** (**SOAP**) and **Representational State Transfer** (**REST**)
    in the early 2000s. In recent years, a generation of new types of API protocols
    have been developed, such as **Remote Procedure Call** (**RPC**) and GraphQL as
    seen in the following table:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图8.1*中，我们可以看到API的作用，它使得访问应用程序数据（来自数据库）以及与第三方或其他应用程序（如移动应用程序[为移动用户]，天气应用程序[在移动或网页上]，电动汽车等）进行通信成为可能。API自计算机诞生以来一直在运行，旨在实现应用程序间的通信。随着时间的推移，我们在21世纪初看到了开发者就**简单对象访问协议**（**SOAP**）和**表示状态转换**（**REST**）等协议达成共识。近年来，一代新的API协议类型被开发出来，如**远程过程调用**（**RPC**）和GraphQL，如下表所示：
- en: '![Table 8.1 – API protocols comparison'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![表8.1 – API协议比较](img/Table_011.jpg)'
- en: '](img/Table_011.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![表8.1 – API协议比较](img/Table_011.jpg)'
- en: Table 8.1 – API protocols comparison
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1 – API协议比较
- en: It is valuable to understand the mainstream API protocols if you are a developer
    of applications (hosted on the cloud or communicating with other services). It
    helps you design your APIs as per your business or functionality needs. As a programmer,
    count yourself fortunate to have many API protocols at your disposal, as 20 years
    ago, only SOAP and REST were available. Now a variety of choices are at your disposal
    depending on your needs, for example, GraphQL, Thrift, and JSON-RPC. These protocols
    have various advantages and drawbacks, making it easy to find the best suited
    to your situation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是应用开发者（托管在云端或与其他服务进行通信），了解主流的API协议是非常有价值的。这有助于你根据你的业务或功能需求来设计你的API。作为一名程序员，你应该感到幸运，因为现在你有许多API协议可供选择，而在20年前，只有SOAP和REST可用。现在，根据你的需求，你可以选择各种协议，例如GraphQL、Thrift和JSON-RPC。这些协议各有优缺点，使得找到最适合你情况的协议变得容易。
- en: Microservices
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微服务
- en: Microservices are a modern way of designing and deploying apps to run a service.
    Microservices enable distributed applications rather than one big monolithic application
    where functionalities are broken up into smaller fragments (called microservices).
    A microservice is an individual application in a microservice architecture. This
    is contrary to centralized or monolithic architectures, where all functionalities
    are tied up together in one big app. Microservices have grown in popularity due
    to **Service-Oriented Architecture** (**SOA**), an alternative to developing traditional
    monolithic (singular and self-sufficient applications).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务是一种现代的设计和部署应用程序以运行服务的方式。微服务使分布式应用程序成为可能，而不是一个大的单体应用程序，其中功能被分解成更小的片段（称为微服务）。微服务是微服务架构中的一个独立应用程序。这与集中式或单体架构相反，在集中式或单体架构中，所有功能都绑定在一个大应用程序中。由于**面向服务的架构**（**SOA**）的出现，微服务因其作为传统单体（单一且自给自足）应用程序的替代方案而越来越受欢迎。
- en: 'Microservices gained massive adoption as they enable developers to develop,
    integrate, and maintain applications with ease. Eventually, this comes down to
    the fact that individual functionalities are treated independently, at first permitting
    you to develop an individual functionality of a service step by step. Lastly,
    it allows you to work on each functionality independently while integrating the
    whole system to orchestrate the service. This way, you can add, improve, or fix
    it without risking breaking the entire application. Microservices are valuable
    for bigger companies since they allow teams to work on isolated things without
    any complicated organization. In *Figure 8.2*, we can see the difference between
    monoliths and microservices. Microservices enable distributed applications compared
    to monoliths, which are non-distributed applications:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务因其使开发者能够轻松地开发、集成和维护应用程序而得到了广泛的应用。最终，这归结于这样一个事实：单个功能被独立对待，最初允许你逐步开发服务的一个单独功能。最后，它允许你在整合整个系统以协调服务的同时，独立地工作在每个功能上。这样，你可以添加、改进或修复它，而不会风险破坏整个应用程序。对于大型公司来说，微服务非常有价值，因为它们允许团队在没有复杂组织的情况下独立工作。在*图8.2*中，我们可以看到单体和微服务之间的区别。与单体（非分布式应用程序）相比，微服务使分布式应用程序成为可能：
- en: '![Figure 8.2 – Microservices versus monoliths'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.2 – 微服务与单体架构](img/B16572_08_02.jpg)'
- en: '](img/B16572_08_02.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.2 – 微服务与单体架构](img/B16572_08_02.jpg)'
- en: Figure 8.2 – Microservices versus monoliths
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 微服务与单体架构
- en: Software development teams are empowered to work independently and within well-understood
    service responsibilities. Microservices-based architecture encourages software
    development teams to take ownership of their services or modules. One possible
    downside to microservices-based architecture is if you break an application up
    into parts, there is a severe need for those parts to communicate effectively
    in order to keep the service running.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 软件开发团队被赋予了独立工作和在明确的服务责任范围内工作的能力。基于微服务的架构鼓励软件开发团队对其服务或模块负责。基于微服务的架构的一个可能的缺点是，如果你将应用程序分解成部分，那么这些部分之间需要有效地沟通，以便保持服务的运行。
- en: The relationship between APIs and microservices is fascinating as it has two
    sides. As a result of microservices-based architecture, an API is a direct outcome
    of implementing that architecture in your application. Whereas at the same time,
    an API is an essential tool for communicating between services in a microservices-based
    architecture to function efficiently. Let's have a look at the next section, where
    we will glance through some examples of ML applications.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: API与微服务之间的关系非常有趣，因为它有两个方面。由于基于微服务的架构，API是你在应用程序中实施该架构的直接结果。同时，API是微服务架构中服务之间高效通信的必要工具。让我们看看下一节，我们将浏览一些机器学习应用的示例。
- en: The need for microservices for ML
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习对微服务的需求
- en: To understand the need for microservices-based architecture for ML applications,
    let's look at a hypothetical use case and go through various phases of developing
    a ML application for the use case.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解基于微服务的架构对于机器学习应用的需求，让我们看看一个假设用例，并探讨开发该用例的机器学习应用的各个阶段。
- en: Hypothetical use case
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 假设用例
- en: A large car repair facility needs a solution to estimate the number of cars
    in the facility and their accurate positions. A bunch of IP cameras is installed
    in the repair stations for monitoring the facility. Design an ML system to monitor
    and manage the car repair facility.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个大型汽车维修设施需要一个解决方案来估计设施中的汽车数量及其准确位置。在维修站安装了一堆IP摄像头以监控设施。设计一个机器学习系统来监控和管理汽车维修设施。
- en: Stage 1 – Proof of concept (a monolith)
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1阶段 – 概念验证（单体）
- en: A quick PoC is developed in a typical case using available data points and applying
    ML to showcase and validate the use case and prove to the business stakeholders
    that ML can solve their problems or improve their business.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型情况下，通过使用可用数据点和应用机器学习来展示和验证用例，并向业务利益相关者证明机器学习可以解决他们的问题或改善他们的业务，快速开发了一个PoC。
- en: 'In our hypothetical use case, a monolith Python app is developed that does
    the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的假设用例中，开发了一个单体Python应用程序，执行以下操作：
- en: Fetches streams from all cameras
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从所有摄像头获取流
- en: Determines the positions of cars (head or tail) from each camera
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从每个摄像头确定汽车的位置（头部或尾部）
- en: Aggregates all estimations into a facility state estimator
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有估计汇总到设施状态估计器
- en: 'We can see in *Figure 8.3*, the app is dockerized and deployed to the server:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图8.3*中看到，应用程序已经容器化并部署到服务器上：
- en: '![Figure 8.3 – Monolith ML application (PoC for hypothetical use case)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.3 – 单体机器学习应用（假设用例的PoC）'
- en: '](img/B16572_08_03.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B16572_08_03.jpg)'
- en: Figure 8.3 – Monolith ML application (PoC for hypothetical use case)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.3 – 单体机器学习应用（假设用例的PoC）'
- en: All cameras are connected to this server via the local network. The algorithms
    for car position estimation and the facility state estimator work but need further
    improvements, and overall the PoC works. This monolith app is highly prone to
    crashing due to the instability of the cameras, the local network, and other errors.
    Such instabilities can be handled better by microservices. Let's see this in practice
    in stage 2.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所有摄像头都通过本地网络连接到这台服务器。汽车位置估计算法和设施状态估计器正在工作，但需要进一步改进，总体上PoC是有效的。由于摄像头、本地网络和其他错误的不稳定性，这个单体应用程序非常容易崩溃。微服务可以更好地处理这种不稳定性。让我们在第二阶段看看实际情况。
- en: Stage 2 – Production (microservices)
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2阶段 – 生产（微服务）
- en: 'In this stage, an application that is less prone to crashing is essential to
    run the car repair facility''s monitoring operations continuously. For this reason,
    a monolith application is replaced with microservices-based architecture as shown
    in *Figure 8.4*:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，一个不太容易崩溃的应用程序对于持续运行汽车维修设施的监控操作至关重要。因此，将单体应用程序替换为如*图8.4*所示基于微服务的架构：
- en: '![Figure 8.4 – Microservices (production-ready application for hypothetical
    use case)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.4 – 微服务（假设用例的生产就绪应用程序）'
- en: '](img/B16572_08_04.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B16572_08_04.jpg)'
- en: Figure 8.4 – Microservices (production-ready application for hypothetical use
    case)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 微服务（假设用例的生产就绪应用程序）
- en: 'The application is fragmented into multiple services in the following manner:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序以下列方式分解为多个服务：
- en: Video stream collector.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频流收集器。
- en: '**Image processor**: This aggregates images – it receives, processes, and caches
    images, and generates packets for further processing.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像处理器**：此处理器汇总图像 – 它接收、处理和缓存图像，并为进一步处理生成数据包。'
- en: '**Position classifier**: Estimates a car''s position (head or tail) parked
    in the repair facility.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置分类器**：估计停放在维修设施中的汽车的位置（头部或尾部）。'
- en: '**Facility setup estimator**: This asynchronously receives car position estimations
    and calibrates the facility setup and sends real-time data to the cloud.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设施设置估算器**：它异步接收汽车位置估算并校准设施设置，然后将实时数据发送到云端。'
- en: The cloud collects and stores data using MQTT (a standard lightweight, publish-subscribe
    network protocol that transports messages between devices). The data is portrayed
    on a dashboard for the car facility operators to analyze operations.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云使用MQTT（一种标准轻量级、发布/订阅网络协议，用于在设备之间传输消息）收集和存储数据。数据在仪表板上呈现，供汽车设施操作员分析操作。
- en: All of the communication between each microservice is facilitated using APIs.
    The advantages of microservice architecture are that if any of the services crash
    or errors take place, that particular microservice is spawned to replace the failed
    one to keep the whole service running. Secondly, each microservice can be maintained
    and improved continuously by a dedicated team (of data scientists, developers,
    and DevOps engineers), unlike coordinating teams, to work on a monolithic system.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 所有微服务之间的通信都通过API进行。微服务架构的优势在于，如果任何服务崩溃或发生错误，将启动特定的微服务来替换失败的服务，以保持整个服务的运行。其次，每个微服务都可以由一个专门的团队（数据科学家、开发人员和DevOps工程师）持续维护和改进，而不是协调团队来工作于一个单体系统。
- en: Old is gold – REST API-based microservices
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 旧的就是好的 – 基于REST API的微服务
- en: Old is gold. Plus, it's better to start somewhere where there are various API
    protocols. The **Representational State Transfer** (**REST**) protocol has become
    a gold standard for many applications over the years, and it's not so very different
    for ML applications today. The majority of companies prefer developing their ML
    applications based on the REST API protocol.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 旧的就是好的。此外，最好从有各种API协议的地方开始。多年来，**表示状态转移**（**REST**）协议已成为许多应用的黄金标准，对于今天的机器学习应用来说也并不那么不同。大多数公司更倾向于基于REST
    API协议开发他们的机器学习应用。
- en: A REST API or RESTful API is based on REST, an architectural method used to
    communicate mainly in web services development.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: REST API或RESTful API基于REST，这是一种用于主要是Web服务开发的架构方法。
- en: 'RESTful APIs are widely used; companies such as Amazon, Google, LinkedIn, and
    Twitter use them. Serving our ML models via RESTful APIs has many benefits, such
    as the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: RESTful API被广泛使用；例如，亚马逊、谷歌、领英和推特等公司都在使用它们。通过RESTful API提供我们的机器学习模型有许多好处，例如以下：
- en: Serve predictions on the fly to multiple users.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为多个用户提供即时预测服务。
- en: Add more instances to scale up the application behind a load balancer.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向负载均衡器后面的应用程序添加更多实例以进行扩展。
- en: Possibly combine multiple models using different API endpoints.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能需要使用不同的API端点来组合多个模型。
- en: Separate our model operating environment from the user-facing environment.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将我们的模型操作环境与用户界面环境分开。
- en: Enable microservices-based architecture. Hence, teams can work independently
    to develop and enhance the services.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用基于微服务的架构。因此，团队可以独立工作以开发和增强服务。
- en: A RESTful API uses existing HTTP methodologies that are defined by the RFC 2616
    protocol. *Table 8.2* summarizes the HTTP methods in combination with their CRUD
    operations and purpose in ML applications.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: RESTful API使用由RFC 2616协议定义的现有HTTP方法。*表8.2*总结了HTTP方法及其在机器学习应用中的CRUD操作和目的。
- en: '![Table 8.2 – REST API HTTP methods'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![表8.2 – REST API HTTP方法'
- en: '](img/Table_02.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Table_02.jpg)'
- en: Table 8.2 – REST API HTTP methods
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.2 – REST API HTTP方法
- en: 'The fundamental HTTP methods are `GET`, `POST`, `PUT`, `PATCH`, and `DELETE`.
    These methods correspond to `create`, `read`, `update`, and `delete`. Using these
    methods, we can develop RESTful APIs to serve ML models. RESTful APIs have gained
    significant adoption due to drivers such as OpenAPI. The OpenAPI Specification
    is a standardized REST API description format. It has become a standardized format
    for humans and machines; it enables REST API understandability and provides extended
    tooling such as API validation, testing, and an interactive documentation generator.
    In practice, the OpenAPI file enables you to describe an entire API with critical
    information such as the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的HTTP方法有`GET`、`POST`、`PUT`、`PATCH`和`DELETE`。这些方法对应于`创建`、`读取`、`更新`和`删除`。使用这些方法，我们可以开发RESTful
    API来服务机器学习模型。由于OpenAPI等驱动因素，RESTful API得到了广泛的应用。OpenAPI规范是一种标准化的REST API描述格式。它已成为人类和机器的标准格式；它使REST
    API易于理解，并提供了扩展的工具，如API验证、测试和交互式文档生成器。在实践中，OpenAPI文件使您能够描述整个API，包括以下关键信息：
- en: Available endpoints (`/names`) and operations on each endpoint (`GET /names`,
    `POST /names`)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用端点（`/names`）以及每个端点的操作（`GET /names`，`POST /names`）
- en: Input and output for each operation (operation parameters)
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个操作的输入和输出（操作参数）
- en: Authentication methods
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认证方法
- en: Developer documentation
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发者文档
- en: Terms of use, license, and other information
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用条款、许可和其他信息
- en: 'You can find more about OpenAPI on this site: [https://swagger.io/specification/](https://swagger.io/specification/).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此网站上了解更多关于OpenAPI的信息：[https://swagger.io/specification/](https://swagger.io/specification/).
- en: In the next section, we will develop a RESTful API to serve an ML model and
    test it using an OpenAPI based interface called Swagger UI.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将开发一个RESTful API来提供ML模型，并使用基于OpenAPI的界面Swagger UI进行测试。
- en: Hands-on implementation of serving an ML model as an API
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践操作：将ML模型作为API提供服务
- en: In this section, we will apply the principles of APIs and microservices that
    we have learned previously (in the section *Introduction to APIs and microservices*)
    and develop a RESTful API service to serve the ML model. The ML model we'll serve
    will be for the business problem (weather prediction using ML) we worked on previously.
    We will use the FastAPI framework to serve the model as an API and Docker to containerize
    the API service into a microservice.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将应用之前学到的API和微服务原则（在*API和微服务简介*部分中），并开发一个RESTful API服务来提供ML模型。我们将提供的ML模型是之前我们工作的业务问题（使用ML进行天气预测）。我们将使用FastAPI框架将模型作为API提供服务，并使用Docker将API服务容器化为微服务。
- en: 'FastAPI is a framework for deploying ML models. It is easy and fast to code
    and enables high performance with features such as asynchronous calls and data
    integrity checks. FastAPI is easy to use and follows the OpenAPI Specification,
    making it easy to test and validate APIs. Find out more about FastAPI here: [https://fastapi.tiangolo.com/.](https://fastapi.tiangolo.com/.)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI是一个用于部署ML模型的框架。它易于编码且速度快，具有异步调用和数据完整性检查等特性，可实现高性能。FastAPI易于使用，遵循OpenAPI规范，使其易于测试和验证API。更多关于FastAPI的信息请访问：[https://fastapi.tiangolo.com/.](https://fastapi.tiangolo.com/.)
- en: API design and development
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: API设计和开发
- en: We will develop the API service and run it on a local computer. (This could
    also be developed on the VM we created earlier in the Azure Machine learning workspace.
    For learning, it is recommended to practice it locally for ease.)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开发API服务并在本地计算机上运行它。（这也可以在我们在Azure机器学习工作区中创建的虚拟机上开发。为了学习，建议在本地练习以便于操作。）
- en: 'To get started, clone the book repository on your PC or laptop and go to the
    `08_API_Microservices` folder. We will use these files to build the API service:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，请在您的PC或笔记本电脑上克隆本书的仓库，并转到`08_API_Microservices`文件夹。我们将使用这些文件来构建API服务：
- en: '[PRE0]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The files listed in the directory tree for the folder `08_API_Microservices`
    include a Dockerfile (used to build a Docker image and container from the `FASTAPI`
    service) and a folder named `app`. The `app` folder contains the files `weather_api.py`
    (contains the code for API endpoint definitions), `variables.py` (contains the
    input variables definition), and `requirements.txt` (contains Python packages
    needed for running the API service), and a folder with model artifacts such as
    a model scaler (used to scale incoming data) and a serialized model file (`svc.onnx`).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 文件夹`08_API_Microservices`中的目录树列出的文件包括一个Dockerfile（用于从`FASTAPI`服务构建Docker镜像和容器）和一个名为`app`的文件夹。`app`文件夹包含`weather_api.py`文件（包含API端点定义的代码），`variables.py`文件（包含输入变量定义），以及`requirements.txt`文件（包含运行API服务所需的Python包），以及包含模型工件（如用于缩放传入数据的模型缩放器）和序列化模型文件（`svc.onnx`）的文件夹。
- en: 'The model was serialized previously, in the model training and evaluation stage,
    as seen in [*Chapter 5*](B16572_05_Final_JM_ePub.xhtml#_idTextAnchor093), *Model
    Evaluation and Packaging*. The model is downloaded and placed in the folder from
    the model registry in the Azure Machine learning workspace (`Learn_MLOps`) as
    shown in *Figure 8.3*:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 模型之前已在模型训练和评估阶段进行了序列化，如[*第5章*](B16572_05_Final_JM_ePub.xhtml#_idTextAnchor093)中所述，“模型评估和打包”。模型从Azure机器学习工作区（`Learn_MLOps`）中的模型注册表中下载并放置在文件夹中，如图*图8.3*所示：
- en: '![Figure 8.5 – Download the serialized model file'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.5 – 下载序列化的模型文件'
- en: '](img/B16572_08_05.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16572_08_05.jpg)'
- en: Figure 8.5 – Download the serialized model file
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 下载序列化的模型文件
- en: You can replace the `svc.onnx` and `model-scalar.pkl` files with the files you
    have trained in your Azure Machine learning workspace or else just continue using
    these files for quick experimentation. Now we will look into the code of each
    file. Let's start with `variables.py`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将 `svc.onnx` 和 `model-scalar.pkl` 文件替换为您在 Azure Machine learning 工作区中训练的文件，或者继续使用这些文件进行快速实验。现在我们将查看每个文件中的代码。让我们从
    `variables.py` 开始。
- en: variables.py
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: variables.py
- en: 'We use only one package for defining input variables. The package we use is
    called `pydantic`; it is a data validation and settings management package using
    Python-type annotations. Using `pydantic`, we will define input variables in the
    class named `WeatherVariables` used for the `fastAPI` service:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅使用一个包来定义输入变量。我们使用的包名为 `pydantic`；它是一个使用 Python 类型注解进行数据验证和设置管理的包。使用 `pydantic`，我们将在用于
    `fastAPI` 服务的名为 `WeatherVariables` 的类中定义输入变量：
- en: '[PRE1]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the `WeatherVariables` class, define variables and their types as shown in
    the preceding code. The same variables that were used for training the model in
    [*Chapter 4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074), *Machine Learning
    Pipelines*, will be used for inference. We define those input variables here as
    `temp_c`, `humidity`, `wind_speed_kmph`, `wind_bearing_degree`, `visibility_km`,
    `pressure_millibars`, and `current_weather_condition`. Data types for these variables
    are defined as `float`. We will import the `WeatherVariables` class and use the
    defined input variables in the `fastAPI` service. Let's look at how we can use
    the variables defined in the `WeatherVariables` class in the `fastAPI` service
    using the `Weather_api.py` file.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `WeatherVariables` 类中，定义变量及其类型，如前述代码所示。用于训练模型相同的变量将用于推理。我们在这里定义这些输入变量为 `temp_c`、`humidity`、`wind_speed_kmph`、`wind_bearing_degree`、`visibility_km`、`pressure_millibars`
    和 `current_weather_condition`。这些变量的数据类型定义为 `float`。我们将导入 `WeatherVariables` 类并在
    `fastAPI` 服务中使用定义的输入变量。让我们看看如何使用 `WeatherVariables` 类中定义的变量在 `fastAPI` 服务中使用 `Weather_api.py`
    文件。
- en: Weather_api.py
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Weather_api.py
- en: 'This file is used to define the `fastAPI` service. The needed model artifacts
    are imported and used to serve API endpoints to infer the model for making predictions
    in real time or in production:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件用于定义 `fastAPI` 服务。所需的模型工件被导入并用于提供API端点以推理模型，用于实时或生产中的预测：
- en: 'We start by importing the required packages as follows:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先按照以下方式导入所需的包：
- en: '[PRE2]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We imported the required packages, such as `uvicorn` (an ASGI server implementation
    package), `fastapi`, `numpy`, `pickle`, `pandas`, and `onnxruntime` (used to deserialize
    and infer `onnx` models).
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们导入了所需的包，例如 `uvicorn`（一个 ASGI 服务器实现包）、`fastapi`、`numpy`、`pickle`、`pandas` 和
    `onnxruntime`（用于反序列化和推理 `onnx` 模型）。
- en: Note
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: We imported the `WeatherVariables` class previously created in the `variables.py`
    file. We will use the variables defined in this file for procuring input data
    for the `fastAPI` service.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们之前在 `variables.py` 文件中导入了 `WeatherVariables` 类。我们将使用此文件中定义的变量为 `fastAPI` 服务获取输入数据。
- en: Next, we create an `app` object. You will notice some syntactic similarities
    of `fastAPI` with the Flask web framework (if you have ever used Flask).
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个 `app` 对象。您将注意到 `fastAPI` 与 Flask 网络框架（如果您曾经使用过 Flask）在语法上有一些相似之处。
- en: 'For instance, in the next step, we create the `app` object using the function
    `FastAPI()` to create the `app` object. Creating an `app` object is similar to
    how we do it via the `Flask` example: from Flask, import `Flask` and then we use
    the `Flask` function to create the `app` object in the manner `app = Flask ()`.
    You will notice such similarities as we build API endpoints using `fastAPI`:'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，在下一步中，我们使用 `FastAPI()` 函数创建 `app` 对象。创建 `app` 对象的方式类似于我们在 `Flask` 示例中做的那样：从
    Flask 导入 `Flask`，然后使用 `Flask` 函数以 `app = Flask()` 的方式创建 `app` 对象。您将会注意到我们在使用 `fastAPI`
    构建API端点时存在这样的相似性：
- en: '[PRE3]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After creating the `app` object, we will import the necessary model artifacts
    for inference in the endpoints. `Pickle` is used to deserialize the data scaler
    file `model-scaler.pkl`. This file was used to train the model (in [*Chapter 4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074),
    *Machine Learning Pipelines*), and now we'll use it to scale the incoming data
    before model inference. We will use the previously trained support vector classifier
    model, which was serialized into the file named `scv.onnx` (we can access and
    download the file as shown in *Figure 8.3*).
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建 `app` 对象后，我们将导入在端点中进行推理所需的必要模型工件。使用 `Pickle` 反序列化数据缩放文件 `model-scaler.pkl`。此文件用于训练模型（在
    [*第 4 章*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074)，*机器学习管道*），现在我们将使用它来在模型推理之前缩放传入的数据。我们将使用之前训练的支持向量机分类器模型，该模型被序列化到名为
    `scv.onnx` 的文件中（我们可以像 *图 8.3* 所示那样访问和下载该文件）。
- en: '`ONNX` Runtime is used to load the serialized model into inference sessions
    (`input_name` and `label_name`) for making ML model predictions. Next, we can
    move to the core part of defining the API endpoints to infer the ML model. To
    begin, we make a `GET` request to the index route using the wrapper function `@app.get(''/'')`:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `ONNX` 运行时将序列化的模型加载到推理会话（`input_name` 和 `label_name`）中，以进行机器学习模型的预测。接下来，我们可以转向定义
    API 端点的核心部分，以推断机器学习模型。首先，我们使用包装函数 `@app.get('/')` 向索引路由发出一个 `GET` 请求：
- en: '[PRE4]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A function named `index()` is defined for the index route. It returns the welcome
    message, pointing to the docs link. This message is geared toward guiding the
    users to the docs link to access and test the API endpoints.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为索引路由定义了一个名为 `index()` 的函数。它返回指向文档链接的欢迎信息。此信息旨在指导用户访问和测试 API 端点。
- en: 'Next, we will define the core API endpoint, `/predict`, which is used to infer
    the ML model. A wrapper function, `@app.post(''/predict'')`, is used to make a
    `POST` request:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义核心 API 端点 `/predict`，该端点用于推断机器学习模型。使用包装函数 `@app.post('/predict')` 发出
    `POST` 请求：
- en: '[PRE5]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: A function named `predict_weather()` is initiated for the endpoint `/predict`.
    Inside the function, we have created a variable called `data` that will capture
    the input data; this variable captures the `JSON` data we are getting through
    the `POST` request and points to `WeatherVariables`. As soon as we do the `POST`
    request, all the variables in the incoming data will be mapped to variables in
    the `WeatherVariables` class from the `variables.py` file.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为 `/predict` 端点启动了一个名为 `predict_weather()` 的函数。在函数内部，我们创建了一个名为 `data` 的变量，该变量将捕获输入数据；这个变量捕获通过
    `POST` 请求获取的 `JSON` 数据，并指向 `WeatherVariables`。一旦我们发出 `POST` 请求，传入数据中的所有变量都将映射到
    `variables.py` 文件中的 `WeatherVariables` 类中的变量。
- en: 'Next, we convert the data into a dictionary, fetch each input variable from
    the dictionary, and compress them into a `numpy` array variable, `data_to_pred`.
    We will use this variable to scale the data and infer the ML model:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将数据转换为字典，从字典中获取每个输入变量，并将它们压缩成一个名为 `data_to_pred` 的 `numpy` 数组变量。我们将使用这个变量来缩放数据并推断机器学习模型：
- en: '[PRE6]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The data (`data_to_pred`) is reshaped and scaled using the scaler loaded previously
    using the `fit_transform()` function.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用之前加载的缩放器通过 `fit_transform()` 函数对数据 (`data_to_pred`) 进行重塑和缩放。
- en: 'Next, the model inference step, which is the key step, is performed by inferencing
    scaled data to the model, as shown in the preceding code. The prediction inferred
    from the model is then returned as the output to the `prediction` variable:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，执行模型推理步骤，这是关键步骤，通过将缩放数据推断到模型中完成，如前面的代码所示。从模型推断出的预测随后作为输出返回到 `prediction`
    变量：
- en: '[PRE7]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Lastly, we will convert the model inference into a human-readable format by
    suggesting `rain` or `no_rain` based on the ML model's predictions and return
    the `prediction` for the `POST` call to the `/predict` endpoint. This brings us
    to the end of the `weather_api.py` file. When a `POST` request is made by passing
    input data, the service returns the model prediction in the form of `0` or `1`.
    The service will return `rain` or `not_rain` based on the model prediction. When
    you get such a prediction, your service is working and is robust enough to serve
    production needs.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们将模型推理转换为人类可读的格式，根据机器学习模型的预测建议 `rain` 或 `no_rain`，并将 `prediction` 返回给 `/predict`
    端点的 `POST` 调用。这标志着 `weather_api.py` 文件的结束。当通过传递输入数据发出 `POST` 请求时，服务以 `0` 或 `1`
    的形式返回模型预测。根据模型预测，服务将返回 `rain` 或 `not_rain`。当你得到这样的预测时，你的服务正在运行，并且足够健壮，可以满足生产需求。
- en: Requirement.txt
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Requirement.txt
- en: 'This text file contains all the packages needed to run the `fastAPI` service:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此文本文件包含运行 `fastAPI` 服务所需的所有软件包：
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: These packages should be installed in the environment where you would like to
    run the API service. We will use `numpy`, `fastapi` (an ML framework for creating
    robust APIs), `uvicorn` (an AGSI server), `scikit-learn`, `pandas`, `onnx`, and
    `onnxruntime` (to deserialize and infer `onnx` models) to run the FastAPI service.
    To deploy and run the API service in a standardized way, we will use Docker to
    run the FastAPI service in a Docker container.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这些软件包应该安装在你希望运行 API 服务的环境中。我们将使用 `numpy`、`fastapi`（一个用于创建健壮 API 的机器学习框架）、`uvicorn`（一个
    ASGI 服务器）、`scikit-learn`、`pandas`、`onnx` 和 `onnxruntime`（用于反序列化和推理 `onnx` 模型）来运行
    FastAPI 服务。为了以标准化的方式部署和运行 API 服务，我们将使用 Docker 在 Docker 容器中运行 FastAPI 服务。
- en: Next, let's look at how to create a Dockerfile for the service.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何为服务创建 Dockerfile。
- en: Developing a microservice using Docker
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Docker 开发微服务
- en: In this section, we will package the FastAPI service in a standardized way using
    Docker. This way, we can deploy the Docker image or container on the deployment
    target of your choice within around 5 minutes.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 Docker 以标准化的方式打包 FastAPI 服务。这样，我们可以在大约 5 分钟内将 Docker 镜像或容器部署到您选择的部署目标。
- en: 'Docker has several advantages, such as replicability, security, development
    simplicity, and so on. We can use the official Docker image of `fastAPI` (`tiangolo/uvicorn-gunicorn-fastapi`)
    from Docker Hub. Here is a snippet of the Dockerfile:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 有许多优点，例如可复制性、安全性、开发简单性等。我们可以使用 Docker Hub 上的官方 `fastAPI` 镜像（`tiangolo/uvicorn-gunicorn-fastapi`）。以下是
    Dockerfile 的一个片段：
- en: '[PRE9]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Firstly, we use an official `fastAPI` Docker image from Docker Hub by using
    the `FROM` command and pointing to the image – `tiangolo/uvicorn-gunicorn-fastapi:python3.7`.
    The image uses Python 3.7, which is compatible with `fastAPI`. Next, we copy the
    `app` folder into a directory named `app` inside `docker image/container`. After
    the folder `app` is copied inside the Docker image/container, we will install
    the necessary packages listed in the file `requirements.txt` by using the `RUN`
    command.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 Docker Hub 上的官方 `fastAPI` Docker 镜像，通过使用 `FROM` 命令并指向镜像 – `tiangolo/uvicorn-gunicorn-fastapi:python3.7`。该镜像使用
    Python 3.7，与 `fastAPI` 兼容。接下来，我们将 `app` 文件夹复制到 Docker 镜像/容器内的 `app` 目录中。在将文件夹
    `app` 复制到 Docker 镜像/容器内之后，我们将使用 `RUN` 命令安装文件 `requirements.txt` 中列出的必要软件包。
- en: As the `uvicorn` server (AGSI server) for `fastAPI` uses port `80` by default,
    we will `EXPOSE` port `80` for the Docker image/container. Lastly, we will spin
    up the server inside the Docker image/container using the command `CMD "uvicorn
    weather_api:app –host 0.0.0.0 –port 80"`. This command points to the `weather_api.py`
    file to access the `fastAPI` app object for the service and host it on port `80`
    of the image/container.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `uvicorn` 服务器（ASGI 服务器）默认使用端口 `80`，我们将为 Docker 镜像/容器 `EXPOSE` 端口 `80`。最后，我们将使用命令
    `CMD "uvicorn weather_api:app –host 0.0.0.0 –port 80"` 在 Docker 镜像/容器内启动服务器。此命令指向
    `weather_api.py` 文件以访问服务的 `fastAPI` 应用程序对象，并在镜像/容器的端口 `80` 上托管。
- en: Congrats, you are almost there. Now we will test the microservice for readiness
    and see whether and how it works.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，你几乎完成了。现在我们将测试微服务的就绪状态，并查看它是否以及如何工作。
- en: Testing the API
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试 API
- en: 'To test the API for readiness, we will perform the following steps:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试 API 的就绪状态，我们将执行以下步骤：
- en: 'Let''s start by building the Docker image. For this, a prerequisite is to have
    Docker installed. Go to your terminal or Command Prompt and clone the repository
    to your desired location and access the folder `08_API_Microservices`. Execute
    the following Docker command to build the Docker image:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从构建 Docker 镜像开始。为此，先决条件是安装 Docker。转到您的终端或命令提示符，将存储库克隆到您希望的位置，并访问文件夹 `08_API_Microservices`。执行以下
    Docker 命令以构建 Docker 镜像：
- en: '[PRE10]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: (base) user ~ docker images
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (base) 用户 ~ docker images
- en: REPOSITORY   TAG       IMAGE ID       CREATED          SIZE
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: REPOSITORY   标签       镜像 ID       创建时间          大小
- en: fastapi      latest    1745e964f57f   56 seconds ago   1.31GB
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: fastapi      latest    1745e964f57f   56秒前   1.31GB
- en: '[PRE11]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Run a Docker container locally. Now, we can spawn a running Docker container
    from the Docker image created previously. To run a Docker container, we use the
    `RUN` command:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地运行 Docker 容器。现在，我们可以从之前创建的 Docker 镜像启动一个正在运行的 Docker 容器。要运行 Docker 容器，我们使用
    `RUN` 命令：
- en: '[PRE12]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Test the API service using sample data. We will check whether the container
    is running successfully or not. To check this, use the following command:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用样本数据测试 API 服务。我们将检查容器是否成功运行。要检查这一点，请使用以下命令：
- en: '[PRE13]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, test the `/predict` endpoint (by selecting the endpoint and clicking the
    **Try it out** button) using input data of your choice, as shown in *Figure 8.6*:![Figure
    8.7 – Input for the request body of the FastAPI service
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用您选择的数据测试 `/predict` 端点（通过选择端点并点击**尝试它**按钮），如图 *图8.6* 所示：![图8.7 – FastAPI服务的请求体输入
- en: '](img/B16572_08_07.jpg)'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/B16572_08_07.jpg)'
- en: Figure 8.7 – Input for the request body of the FastAPI service
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.7 – FastAPI服务的请求体输入
- en: 'Click `POST` call and test the endpoint. The input is inferred with the model
    in the service and the model prediction `Rain` or `No_Rain` is the output of the
    `POST` call, as shown in *Figure 8.7*:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`POST`调用并测试端点。输入由服务中的模型推断，模型预测`Rain`或`No_Rain`是`POST`调用的输出，如图 *图8.7* 所示：
- en: '![Figure 8.8 – Output for the POST call (/predict endpoint)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.8 – POST调用（/predict端点）的输出'
- en: '](img/B16572_08_08.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B16572_08_08.jpg)'
- en: Figure 8.8 – Output for the POST call (/predict endpoint)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – POST调用的输出（/predict端点）
- en: The successful execution of the `POST` call for the `/predict` API will result
    in the output model prediction as shown in *Figure 8.6*. The model running in
    the Docker container outputs the weather condition as `Rain` for the `POST` call.
    Congratulations, you have successfully spawned a `fastAPI` container and tested
    it. This exercise should have equipped you with the skills to build, deploy, and
    test ML-based API services for your use cases.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`/predict` API的`POST`调用成功执行将导致输出模型预测，如图 *图8.6* 所示。运行在Docker容器中的模型在`POST`调用中将天气条件输出为`Rain`。恭喜你，你已经成功启动了一个`fastAPI`容器并对其进行了测试。这个练习应该已经让你具备了构建、部署和测试基于ML的API服务以供你使用的能力。'
- en: Summary
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have learned the key principles of API design and microservice
    deployment in production. We touched upon the basics of API design methods and
    learned about FastAPI. For our business problem, we have learned by doing a practical
    implementation of developing an API service in the *Hands-on implementation of
    serving an ML model as an API* section using FastAPI and Docker. Using the practical
    knowledge gained in this chapter, you can design and develop robust API services
    to serve your ML models. Developing API services for ML models is a stepping stone
    to take ML models to production.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了API设计和生产中微服务部署的关键原则。我们简要介绍了API设计方法的基础，并了解了FastAPI。对于我们的业务问题，我们在*动手实现将ML模型作为API提供服务*部分通过FastAPI和Docker进行了实际实施。利用本章获得的实际知识，你可以设计和开发健壮的API服务来服务于你的ML模型。为ML模型开发API服务是将ML模型推向生产的一个步骤。
- en: In the next chapter, we will delve into the concepts of testing and security.
    We will implement a testing method to test the robustness of an API service using
    Locust. Let's go!
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨测试和安全性的概念。我们将使用Locust实现一种测试方法来测试API服务的健壮性。让我们开始吧！
