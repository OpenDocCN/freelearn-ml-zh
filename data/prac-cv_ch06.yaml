- en: Feature-Based Object Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于特征的对象检测
- en: In the previous chapter, we understood the importance of and how to model deep
    layered feature extraction using **Convolutional Neural Networks** (**CNNs**).
    In this chapter, we will learn how to model a CNN to detect where the object in
    the image is and also classify the object in one of our pre-decided categories.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解了如何使用**卷积神经网络**（**CNNs**）建模深度层特征提取的重要性。在本章中，我们将学习如何建模CNN以检测图像中的对象位置，并分类为预先决定的类别之一。
- en: 'In this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中：
- en: We will begin with a general discussion on image recognition and what is object
    detection
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将从一个关于图像识别以及什么是对象检测的通用讨论开始
- en: A working example of the popular techniques for face detection using OpenCV
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用OpenCV进行人脸检测的流行技术的一个工作示例
- en: Object detection using two-stage models such as Faster-RCNN
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用两阶段模型如Faster-RCNN进行对象检测
- en: Object detection using one-stage model such as SSD
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单阶段模型如SSD进行对象检测
- en: The major part of this chapter will be discussing deep learning-based object
    detectors and explaining them using a code for the demo
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的主要内容将讨论基于深度学习的对象检测器，并使用演示代码进行解释
- en: Introduction to object detection
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对象检测简介
- en: 'To begin with object detection, we will first see an overview of image recognition
    as detection is one part of it. In the following figure, an overview of object
    recognition is described using an image from `Pascal VOC` dataset. The input is
    passes through a model which then produces information in four different styles:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始对象检测，我们首先将看到图像识别的概述，因为检测是其中的一部分。在下面的图中，使用`Pascal VOC`数据集的图像描述了对象识别的概述。输入通过一个模型传递，然后以四种不同的风格产生信息：
- en: '![](img/e87720aa-caa0-4632-b7b7-633f61ec8f82.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e87720aa-caa0-4632-b7b7-633f61ec8f82.png)'
- en: 'The model in the previous image performs generic image recognition where we
    can predict the following  information:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 上一张图中的模型执行的是通用图像识别，我们可以预测以下信息：
- en: A class name for the object in the image
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像中对象的类名
- en: Object center pixel location
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象中心像素位置
- en: A bounding box surrounding the object as output
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为输出围绕对象的边界框
- en: In instance image where each pixel is classified into a class. The classes are
    for object as well as background
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实例图像中，每个像素被分类为一个类别。这些类别包括对象以及背景
- en: When we say object detection, we are usually referring to the first and third
    type of image recognition. Our goal is to estimate class names as well as bounding
    box surrounding target objects. Before we begin our discussion on object detection
    techniques, in the next section we shall see why detecting objects is a difficult
    computer vision task.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提到对象检测时，我们通常指的是图像识别的第一种和第三种类型。我们的目标是估计目标对象的类名以及围绕目标对象的边界框。在我们开始讨论对象检测技术之前，在下一节中我们将看到为什么检测对象是一个困难的计算机视觉任务。
- en: Challenges in object detection
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对象检测的挑战
- en: In the past, several approaches for object detection were proposed. However,
    these either perform well in a controlled environment or look for special objects
    in images like a human face. Even in the case of faces, the approaches suffer
    from issues like low light conditions, a highly occluded face or tiny face size
    compared to the image size.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，提出了几种对象检测的方法。然而，这些方法要么在受控环境中表现良好，要么在图像中寻找特殊对象，如人脸。即使在人脸的情况下，这些方法也面临着诸如低光照条件、高度遮挡的人脸或与图像大小相比人脸尺寸过小等问题。
- en: 'Following are several challenges that are faced by an object detector in real-world
    applications:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些对象检测器在实际应用中面临的挑战：
- en: '**Occlusion**: Objects like dogs or cats can be hidden behind one another,
    as a result, the features that can be extracted from them are not strong enough
    to say that they are an object.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遮挡**：像狗或猫这样的对象可能被彼此遮挡，因此从它们中提取的特征不足以说明它们是对象。'
- en: '**Viewpoint changes**: In cases of different viewpoints of an object, the shape
    may change drastically and hence the features of the object will also change drastically.
    This causes a detector which is trained to see a given object from one viewpoint
    to fail on seeing it from other viewpoints. For example, in the case of person
    detection, if the detector is looking for a head, hands, and legs combination
    to find a person, will fail if we put the camera overhead to take vertical downward
    facing images. The only thing that the detector will see are heads and hence the
    results are drastically reduced.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视点变化**：在物体不同视点的情况下，形状可能会发生剧烈变化，因此物体的特征也会发生剧烈变化。这导致了一个训练有素的检测器，从特定视点看到给定物体时可能会在其他视点失败。例如，在人体检测的情况下，如果检测器正在寻找头部、手和腿的组合来找到一个人，当我们把相机放在上方拍摄垂直向下面对图像时，它将失败。检测器将看到的东西只有头部，因此结果会大大减少。'
- en: '**Variation in sizes**: The same object can be far from a camera or near. As
    a result, the size of objects varies. The detector is therefore required to be
    size invariant as well as rotation invariant.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**尺寸变化**：同一个物体可以离相机很远或很近。因此，物体的尺寸会有所变化。因此，检测器需要具有尺寸不变性和旋转不变性。'
- en: '**Non-rigid objects**: If the shape of the object splits into parts or there
    is a fluid object, it becomes even more challenging to describe them using features.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非刚性物体**：如果物体的形状分成几部分或者存在流体物体，那么使用特征来描述它们就变得更加具有挑战性。'
- en: '**Motion-blur**: If we are detecting a moving body like a car, there might
    be cases where the camera captured image is blurred. This is another challenge
    for the object detectors, to provide a correct estimation, and making a detector
    robust is crucial when deployed in moving robots like self-driving cars or drones.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运动模糊**：如果我们正在检测一个移动的物体，如汽车，那么可能会出现相机捕获的图像模糊的情况。这对物体检测器来说又是一个挑战，提供正确的估计，以及在移动机器人（如自动驾驶汽车或无人机）中部署时使检测器鲁棒至关重要。'
- en: Dataset and libraries used
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集和使用的库
- en: In this chapter, we will be using TensorFlow (v1.4.0) and OpenCV as our main
    library for detection. We show results on custom images. However, any colored
    image can be used as input for various models. Wherever required, there are links
    to pre-trained model files in the sections.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用TensorFlow（v1.4.0）和OpenCV作为我们的主要库进行检测。我们在自定义图像上展示结果。然而，任何彩色图像都可以作为各种模型的输入。在需要的地方，章节中提供了预训练模型文件的链接。
- en: Methods for object detection
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体检测方法
- en: Object detection is the problem of two steps. First, it should localize an object
    or multiple objects inside an image. Secondly, it gives out a predicted class
    for each of the localized objects. There have been several object detection methods
    that use a sliding window-based approach. One of the popular detection techniques
    is face detection approach, developed by Viola and Jones[1]. The paper exploited
    the fact that the human face has strong descriptive features such as regions near
    eyes which are darker than near the mouth. So there may be a significant difference
    between the rectangle area surrounding the eyes with respect to the rectangular
    area near the nose. Using this as one of the several pre-defined patterns of rectangle
    pairs, their method computed area difference between rectangles in each pattern.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测是两步的问题。首先，它应该在图像内部定位一个或多个对象。其次，它为每个定位的对象给出一个预测类别。已经有一些使用滑动窗口方法的物体检测方法。其中一种流行的检测技术是Viola和Jones开发的**人脸检测方法**[1]。该论文利用了人类面部具有强烈的描述性特征这一事实，例如眼睛附近区域比嘴巴附近区域更暗。因此，眼睛周围的矩形区域与鼻子附近的矩形区域之间可能存在显著差异。利用这一点作为几个预定义的矩形对模式之一，他们的方法计算了每个模式中矩形之间的面积差异。
- en: 'Detecting faces is a two-step process:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 检测人脸是两步的过程：
- en: 'First is to create a classifier with parameters for specific object detection.
    In our case, it is face detection:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一是要创建一个具有特定物体检测参数的分类器。在我们的例子中，它是人脸检测：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In second step, for each image, it face detection is done using previously
    loaded classifier parameters:'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二步中，对于每个图像，使用之前加载的分类器参数进行人脸检测：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In OpenCV we can code this to detect the face, shown as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV中，我们可以编写代码来检测人脸，如下所示：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, we used a file `haarcascade_frontalface_default.xml` which contains classifier
    parameters available at [https://github.com/opencv/opencv/tree/master/data/haarcascades](https://github.com/opencv/opencv/tree/master/data/haarcascades).
    We have to download these cascade classifier files in order to run face detection.
    Also for detecting other objects like eyes, smiles, and so on, we require similar
    files for use with OpenCV.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了文件`haarcascade_frontalface_default.xml`，它包含可在[https://github.com/opencv/opencv/tree/master/data/haarcascades](https://github.com/opencv/opencv/tree/master/data/haarcascades)找到的分类器参数。我们必须下载这些级联分类器文件才能运行人脸检测。同样，为了检测其他对象，如眼睛、微笑等，我们也需要类似的文件与OpenCV一起使用。
- en: The preceding face detector we saw became popular in several devices ranging
    from smartphones to digital cameras. However, recent advances in deep learning
    are creating better face detectors. We will see this in the next few sections
    on deep learning-based general object detectors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到的面部检测器在从智能手机到数码相机等多种设备上变得流行。然而，深度学习的最新进展正在创造更好的面部检测器。我们将在下一节中看到基于深度学习的通用目标检测器的几个部分。
- en: Deep learning-based object detection
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的目标检测
- en: With recent advancements in CNNs and their performance in image classification,
    it was becoming intuitive to use the similar model style for object detection.
    This has been proven right, as in the last few years there are better object detectors
    proposed every year which increases overall accuracy on standard benchmarks. Some
    of the styles of detectors are already in use in smartphones, robot self-driving
    cars, and so on.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 随着卷积神经网络（CNNs）的最近进展及其在图像分类中的表现，使用类似模型风格进行目标检测变得越来越直观。这一点已经被证明是正确的，因为在过去几年中，每年都有更好的目标检测器被提出，这提高了在标准基准测试上的整体准确性。其中一些检测器风格已经被用于智能手机、自动驾驶汽车等设备中。
- en: A generic CNN outputs class probabilities, as in the case of image recognition.
    But in order to detect objects, these must be modified to output both the class
    probability as well as bounding box rectangle coordinates and shape. Early CNN-based
    object detection, computes possible windows from an input image and then computes
    features using a CNN model for each window. This output of the CNN feature extractor
    will then tell us if the chosen window is the target object or not. This is slow
    due to a large computation of each window through the CNN feature extractor. Intuitively,
    we would like to extract features from images and use those features for object
    detection. This not only enhances speed for detection but also filters unwanted
    noise in the image.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用的CNN输出类别概率，就像图像识别的情况一样。但是，为了检测对象，这些必须修改为输出类别概率以及边界框的矩形坐标和形状。早期的基于CNN的目标检测，从输入图像中计算可能的窗口，然后使用CNN模型为每个窗口计算特征。CNN特征提取器的输出将告诉我们所选窗口是否为目标对象。这由于CNN特征提取器对每个窗口的大量计算而变得缓慢。直观地讲，我们希望从图像中提取特征，并使用这些特征进行目标检测。这不仅提高了检测速度，还过滤了图像中的不需要的噪声。
- en: 'There have been several methods proposed to tackle such issues of speed and
    accuracy in object detection. These are in general divided into two major categories:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了几种方法来解决目标检测中的速度和准确性问题。这些方法通常分为两大类：
- en: '**Two-stage detectors**: Here, the overall process is divided into two major
    steps, hence the name two-stage detectors. The most popular among these is **Faster
    R-CNN**. In the next section, we will see a detailed explanation of this method.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**两阶段检测器**：在这里，整个过程被分为两个主要步骤，因此得名两阶段检测器。其中最受欢迎的是**Faster R-CNN**。在下一节中，我们将看到对这个方法的详细解释。'
- en: '**One-stage detectors**: While two-stage detectors increased accuracy for detection,
    they were still hard to train and they were slower for several real-time operations.
    One-stage detectors rectified these issues by making a network in single architecture
    which predicts faster. One of the popular models of this style is **Single Shot
    Multibox Detector** (**SSD**).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单阶段检测器**：虽然两阶段检测器提高了检测的准确性，但它们仍然难以训练，并且在几个实时操作中速度较慢。单阶段检测器通过构建一个单架构网络来预测更快地解决了这些问题。这种风格中流行的模型之一是**单次多框检测器**（**SSD**）。'
- en: In the following sections, we will see both of these types of detectors with
    a demo that shows the quality of results from each.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下几节中，我们将通过演示来展示这两种类型检测器的质量，演示将展示每个检测器产生的结果质量。
- en: Two-stage detectors
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 两阶段检测器
- en: As CNN show their performance in general image classification, researchers used
    the same CNNs to do better object detection. The initial approaches using deep
    learning for object detection can be described as two-stage detectors and one
    of the popular ones is Faster R-CNN by Shaoqing Ren, Kaiming He, Ross Girshick,
    and Jian Sun 2015 [https://arxiv.org/pdf/1506.01497.pdf](https://arxiv.org/pdf/1506.01497.pdf).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如CNN在一般图像分类中显示其性能，研究人员使用了相同的CNN来进行更好的目标检测。使用深度学习进行目标检测的初始方法可以描述为两阶段检测器，其中之一是2015年由Shaoqing
    Ren、Kaiming He、Ross Girshick和Jian Sun提出的Faster R-CNN [https://arxiv.org/pdf/1506.01497.pdf](https://arxiv.org/pdf/1506.01497.pdf)。
- en: 'The method is divided into two stages:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法分为两个阶段：
- en: In the first stage, the features are extracted from an image and **Region of
    Interests** (**ROI**) are proposed. ROIs consists of a possible box where an object
    might be in the image.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一阶段，从图像中提取特征，并提议**感兴趣区域（ROI）**。ROI包括一个可能包含对象的矩形框。
- en: The second stage uses features and ROIs to compute final bounding boxes and
    class probabilities for each of the boxes. These together constitute the final
    output.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二阶段使用特征和ROI来计算每个框的最终边界框和类别概率。这些共同构成了最终输出。
- en: 'An overview of Faster-RCNN is as shown in the following figure. An input image
    is used to extract features and a region proposals. These extracted features and
    proposals are used together to compute predicted bounding boxes and class probabilities
    for each box:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Faster-RCNN的概述如图所示。输入图像用于提取特征和区域提议。这些提取的特征和提议一起用于计算每个矩形的预测边界框和类别概率：
- en: '![](img/f61d607d-20c7-4394-9ed3-c8ba39a8e4d0.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f61d607d-20c7-4394-9ed3-c8ba39a8e4d0.png)'
- en: 'As shown in the previous figure, overall method is considered two-stage because
    during training the model will first learn to produce ROIs using a sub-model called
    **Region Proposal Network (RPN)**. It will then learn to produce correct class
    probabilities and bounding box locations using ROIs and features. An overview
    of RPN is as shown in the following figure . RPN layer uses feature layer as input
    creates a proposal for bounding boxes and corresponding probabilities:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，整体方法被认为是两阶段，因为在训练过程中，模型首先会学习使用称为**区域提议网络（RPN）**的子模型来产生区域（ROIs）。然后，它将学习使用ROIs和特征来产生正确的类别概率和边界框定位。RPN的概述如图所示。RPN层使用特征层作为输入，为边界框创建提议和相应的概率：
- en: '![](img/bcfcf94d-01e0-433a-ad98-d22603384731.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bcfcf94d-01e0-433a-ad98-d22603384731.png)'
- en: The bounding box locations are usually normalized values for the top left coordinate
    of the box with width and height values, though this can change depending on the
    way the model is learnt. During prediction, the model outputs a set of class probabilities,
    class categories as well as the bounding box location in (x, y, w, h) format.
    This set is again passed through a threshold to filter out the bounding boxes
    with confidence scores less than the threshold.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 矩形的定位通常是宽度和高度值的归一化值，对于矩形的左上角坐标，尽管这取决于模型的学习方式。在预测过程中，模型输出一系列类别概率、类别类别以及以(x, y,
    w, h)格式表示的矩形定位。这个集合再次通过阈值过滤掉置信度得分低于阈值的矩形。
- en: 'The major advantage of using this style of the detector is that it gives better
    accuracy than one-stage detectors. These usually achieve state-of-the-art detection
    accuracy. However, they suffer from slower speeds during predictions. If for an
    application prediction, time plays a crucial role, then it is advised to either
    provide these networks with a high-performance system or use one-stage detectors.
    On the other hand, if the requirement is to get the best accuracy, it is highly
    recommended to use such a method for object detection. An example output of object
    detection is as shown in the following figure with the bounding box around detected
    objects. Each box has a label showing predicted class name and confidence for
    the box:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种检测器风格的主要优点是它比单阶段检测器提供了更高的精度。这些通常实现了最先进的检测精度。然而，在预测过程中它们速度较慢。如果对于预测应用，时间起着至关重要的作用，那么建议为这些网络提供高性能系统或使用单阶段检测器。另一方面，如果要求是获得最佳精度，则强烈建议使用这种方法进行目标检测。以下图显示了目标检测的示例输出，其中检测到的对象周围有矩形框。每个框都有一个标签，显示预测的类别名称和框的置信度：
- en: '![](img/fc24432c-3230-4ddb-a84e-27232070db7f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fc24432c-3230-4ddb-a84e-27232070db7f.png)'
- en: The detection in the previous screenshot uses Faster RCNN model and even for
    small objects, like a person on the right bottom, the model detects with a good
    confidence score. Overall detected objects are bus, car and person. The model
    doesn't detect other objects, such as trees, pole, traffic light, and so on because
    it has not been trained to detect those objects.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 上一张截图中的检测使用了Faster RCNN模型，即使是像右下角的人这样的小物体，模型也能以良好的置信度进行检测。总体上检测到的物体有公交车、汽车和人。模型没有检测到其他物体，如树木、电线杆、交通灯等，因为它没有被训练来检测这些物体。
- en: Demo – Faster R-CNN with ResNet-101
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 – 使用ResNet-101的Faster R-CNN
- en: 'It can be seen from the previous screenshot that even in the case of varying
    object sizes and also objects with small sizes, the two-stage model of Faster
    R-CNN predicts accurately. Now, we will show how to run a similar prediction using
    TensorFlow. Let''s begin by cloning a repository, as it will contain most of the
    required codes:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一张截图可以看出，即使在物体大小变化的情况下，以及物体尺寸较小的情况下，Faster R-CNN的两阶段模型也能准确预测。现在，我们将展示如何使用TensorFlow运行类似的预测。让我们首先克隆一个存储库，因为它将包含大部分所需的代码：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After we have cloned, we will set up the environment. We will first download
    a pre-trained model from TensorFlow `model-zoo`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们克隆之后，我们将设置环境。我们首先将从TensorFlow的`model-zoo`下载一个预训练模型：
- en: 'For macOS X:'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于macOS X：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For Linux:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Linux：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Keep the extracted folder by the name `faster_rcnn_resnet101_coco_2017_11_08` in `models/research/object_detection`*. *This
    completes the downloading of the pre-trained model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请将名为`faster_rcnn_resnet101_coco_2017_11_08`的提取文件夹保留在`models/research/object_detection`中。*这完成了预训练模型的下载。
- en: 'These two steps have to be performed each time we launch a Terminal shell:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个步骤每次启动终端外壳时都必须执行：
- en: 'At first, we will compile `protobuf` files, as TensorFlow uses them to serialize
    structured data:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们将编译`protobuf`文件，因为TensorFlow使用它们来序列化结构化数据：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Also, run in the research folder:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还需要在研究文件夹中运行：
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The environment and pre-trained models are set, now we will start with the
    prediction code. The following code stays and runs inside `models/research/object_detection` and
    the code style is like a Jupyter notebook. As we progress in this section, each
    of the further code blocks can be run inside a Jupyter notebook cell. If you are
    not familiar with Jupyter, you can still run complete Python scripts:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 环境和预训练模型已设置，现在我们将开始预测代码。以下代码将保留并运行在`models/research/object_detection`中，代码风格类似于Jupyter笔记本。随着本节的进展，每个后续的代码块都可以在Jupyter笔记本单元格中运行。如果您不熟悉Jupyter，您仍然可以运行完整的Python脚本：
- en: 'Let''s begin with loading libs that will be used here:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从加载这里将要用到的库开始：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In order to load a pre-trained model for prediction:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了加载用于预测的预训练模型：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'It can be used to load the model Faster R-CNN with the ResNet-101 feature extractor
    pre-trained on `MSCOCO` dataset:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用它来加载在`MSCOCO`数据集上预训练的ResNet-101特征提取器的Faster R-CNN模型：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let''s set up labels to display in our figure using `MSCOCO` labels:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们设置用于在图中显示的标签，使用`MSCOCO`标签：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Before final predictions, we will set up the utility function as:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在进行最终预测之前，我们将设置实用函数如下：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Following is utility function to display bounding boxes using `matplotib`:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是一个使用`matplotlib`显示边界框的实用函数：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Using this setup, we can do predictions on the input image. In the following
    snippet, we are doing predictions on the input image as well as displaying the
    results. We will launch a `Tensorflow` session and run the graph in `sess.run`
    to compute bounding boxes, scores for each box, the class prediction for boxes
    and number of detections:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个设置，我们可以对输入图像进行预测。在以下代码片段中，我们正在对输入图像进行预测并显示结果。我们将启动一个`Tensorflow`会话，并在`sess.run`中运行图来计算边界框、每个框的分数、框的类别预测和检测数量：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Using previous code, an example of prediction is as shown in the following
    screenshot. Each detected object is displayed with the bounding box. Each bounding
    box has a name of the predicted class as well as the confidence score for the
    object inside the box:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前的代码，以下截图展示了预测的一个示例。每个检测到的物体都显示有边界框。每个边界框都有一个预测类别的名称以及框内物体的置信度分数：
- en: '![](img/172440dd-3f05-4611-b21c-5edfcd084ecb.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/172440dd-3f05-4611-b21c-5edfcd084ecb.png)'
- en: One-stage detectors
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单阶段检测器
- en: 'In the previous section, we saw that two-stage detectors suffer from the issue
    of slower prediction time and harder training by splitting the network into two.
    In recently proposed networks like **Single Shot Multibox Detectors (SSD)**[3],
    the prediction time is reduced by removing the intermediate stage and the training
    is always end-to-end. These networks have shown effectiveness by running on smartphones
    as well as low-end computation units:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了两阶段检测器由于将网络分成两部分而存在预测时间较慢和训练难度较大的问题。在最近提出的网络，如**单阶段多框检测器 (SSD)**[3]中，通过移除中间阶段，预测时间得到了减少，并且训练始终是端到端的。这些网络通过在智能手机以及低端计算单元上运行，已经展示了其有效性：
- en: '![](img/2dc74647-a537-43e6-b0cc-d9463a8ccdd1.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2dc74647-a537-43e6-b0cc-d9463a8ccdd1.png)'
- en: 'An abstract view of the network is shown in the preceding figure. The overall
    output of the network is same as two-stage, the class probability for the object
    and bounding box coordinates of the form **(x, y, w, h)**, where (x,y) is the
    top-left corner of the rectangle and (w, h) are the width and height of the box
    respectively. In order to use multiple resolutions, the model not only uses the
    final layer of feature extraction but also several intermediate feature layers.
    An abstract view is shown in the following screenshot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的抽象视图如图所示。网络的总体输出与两阶段相同，即对象的类别概率和**（x, y, w, h）**形式的边界框坐标，其中（x,y）是矩形的左上角，（w,
    h）分别是框的宽度和高度。为了使用多个分辨率，模型不仅使用特征提取的最终层，还使用几个中间特征层。以下截图显示了抽象视图：
- en: '![](img/7db77e68-b1ef-40c9-a317-72247f97d1f3.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7db77e68-b1ef-40c9-a317-72247f97d1f3.png)'
- en: To further increase the speed for detection, the model also uses a technique
    called **non-maximal suppression**. This will suppress all the **Bounding Box**
    which do not have a maximum score in a given region and for a given category.
    As a result, the total output boxes from the **MultiBox Layer** are reduced significantly
    and thus we have only high scored detections per class in an image.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高检测速度，该模型还使用了一种称为**非极大值抑制**的技术。这将抑制给定区域和给定类别中所有没有最大得分的**边界框**。因此，从**MultiBox
    层**输出的总框数显著减少，从而我们只在每个类别中获取高得分的检测。
- en: In the next section, we will see TensorFlow-based SSD object detection. We will
    use some of the code from the previous section;  Reader does not need to install
    again if there is already an installation of the previous section.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到基于 TensorFlow 的 SSD 物体检测。我们将使用上一节中的一些代码；如果已经安装了上一节的安装，则读者无需再次安装。
- en: Demo
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演示
- en: In the following codes, we will load a pre-trained model and perform an object
    detection task on pre-defined 90 categories. Before we begin, check that there
    is a working TensorFlow (Version = 1.4.0) Python environment.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们将加载一个预训练模型并在预定义的 90 个类别上执行物体检测任务。在我们开始之前，请检查是否存在一个可工作的 TensorFlow（版本
    = 1.4.0）Python 环境。
- en: 'In this section, our input is as shown in the image with people:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们的输入如图所示，包含人物：
- en: '![](img/86cae04e-26c3-4f4d-a0df-4be87d55022c.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/86cae04e-26c3-4f4d-a0df-4be87d55022c.png)'
- en: 'We will follow similar instructions as that of two-stage detectors and begin
    by cloning TensorFlow/models repo:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循与两阶段检测器类似的说明，并首先克隆 TensorFlow/models 仓库：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s download a pre-trained model from TensorFlow model-zoo. These are for
    one-stage detectors:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 TensorFlow 模型库中下载一个预训练模型。这些模型适用于单阶段检测器：
- en: 'For macOS X:'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对于 macOS X:'
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For Linux:'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对于 Linux:'
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Similarly, keep the extracted folder by the name `ssd_inception_v2_coco_2017_11_17
    in models/research/object_detection`*.* We will set up the environment now. If
    this has already been done from the previous section, please skip this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，保留名为 `ssd_inception_v2_coco_2017_11_17` 的提取文件夹在 `models/research/object_detection`
    中*.* 我们现在将设置环境。如果这已经在上一节中完成，请跳过此步骤：
- en: 'First, we will compile the `protobuf` files:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们将编译 `protobuf` 文件：
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Also, run in the research folder:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，在研究文件夹中运行：
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s begin with loading libraries:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载库开始：
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following code reads pre-trained model. In TensorFlow, these models are
    usually saved as `protobuf` in `.pb` format. Also, note that if there are other
    formats of pre-trained model files, then we may have to read accordingly:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码读取预训练模型。在 TensorFlow 中，这些模型通常以 `.pb` 格式保存为 `protobuf`。另外，请注意，如果有其他格式的预训练模型文件，我们可能需要相应地读取：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'For using our input image, the following block reads an image from a given
    path to a file:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于使用我们的输入图像，以下代码块从给定路径读取图像到一个文件中：
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The last utility function is for the output display of the bounding box around
    the predicted object with the class name and detection score for each box:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一个实用函数是用于显示预测对象的边界框的输出，包括每个框的类别名称和检测分数：
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We will be using an SSD model for object detection that uses Inception-v2 model
    for feature extraction. This model is pre-trained on the `MSCOCO` dataset. We
    saw earlier the code snippet to download the model and also to load. So let''s
    go ahead and read the model:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用一个SSD模型进行物体检测，该模型使用Inception-v2模型进行特征提取。这个模型在`MSCOCO`数据集上进行了预训练。我们之前已经看到了下载模型和加载模型的代码片段。所以，让我们继续读取模型：
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Before we start using the model to do predictions on the input image, we need
    our output to make sense. We will create a dictionary map of the class index to
    pre-defined class names. The following code will read a file `data/mscoco_label_map.pbtxt`
    which contains this index to class name mapping. The final index can be used to
    read our output as class names:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们开始使用模型对输入图像进行预测之前，我们需要确保我们的输出是有意义的。我们将创建一个字典映射，将类别索引映射到预定义的类别名称。以下代码将读取文件`data/mscoco_label_map.pbtxt`，该文件包含索引到类别名称的映射。最终的索引可以用来读取我们的输出作为类别名称：
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We have set up everything necessary for prediction. In TensorFlow, the model
    is represented as a computational graph and is often referred to as graph in code
    snippets. This consists of various layers and operation on layers represented
    as a node the and connection between them is how the data will flow. For performing
    predictions, we need to know the input node name and output node names. There
    can be more than one nodes of a type. To start performing the computation, we
    will first create a session. A graph can only perform computation inside a session
    and we can create a session as we need it in the program. In the following code
    snippet, we create a session and get pre-defined input node and output nodes:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为预测设置了所有必要的东西。在TensorFlow中，模型表示为一个计算图，在代码片段中通常被称为图。这由各种层和层上的操作组成，这些操作以节点表示，节点之间的连接表示数据如何流动。为了执行预测，我们需要知道输入节点名称和输出节点名称。可能存在多个同类型的节点。为了开始执行计算，我们将首先创建一个会话。一个图只能在会话内部执行计算，我们可以在程序中按需创建会话。在以下代码片段中，我们创建了一个会话并获取了预定义的输入节点和输出节点：
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In the previous code, the input node is `image_tensor:0` and four output nodes
    are `detection_boxes:0`, `detection_scores:0`, `detection_classes:0`, and `num_detections:0`.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，输入节点是`image_tensor:0`，四个输出节点分别是`detection_boxes:0`、`detection_scores:0`、`detection_classes:0`和`num_detections:0`。
- en: 'When we run inference on a given image, the inference is as shown in the following
    figure. Each box color is according to the class, and the predicted class name,
    as well as the score for class prediction, is displayed in the top-left corner.
    Ideally, score one shows the model is 100% sure about the category of an object
    inside the box:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们对给定的图像进行推理时，推理结果如图所示。每个框的颜色根据类别而定，预测的类别名称以及类别预测的分数显示在左上角。理想情况下，分数为1表示模型对框内物体的类别有100%的把握：
- en: This score is not for how correct the box is but only for the confidence for
    the category of the object inside.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分数并不是用来衡量框是否正确，而是仅用于判断框内物体的类别置信度。
- en: '![](img/6a2601de-45f3-43b3-85a3-f6314b0eda24.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a2601de-45f3-43b3-85a3-f6314b0eda24.png)'
- en: 'Here we used only one image as input. We can use a list of images as input
    and correspondingly we will get a list of outputs for each image. To display the
    results, iterate simultaneously on images and outputs as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只使用了一张图像作为输入。我们可以使用图像列表作为输入，相应地，我们将为每个图像得到一个输出列表。为了显示结果，我们可以同时迭代图像和输出，如下所示：
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To show the comparison with the two-stage detector, for the same input the
    following are the output prediction with the one-stage detector. We can easily
    notice that the one-stage detectors such as SSD is good for large objects but
    fail to recognize small objects such as people. Also, the prediction scores vary
    a lot between the two detectors:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与两阶段检测器进行比较，对于相同的输入，以下是一阶段检测器的输出预测。我们可以很容易地注意到，一阶段检测器如SSD对于大物体很好，但无法识别如人这样的小物体。此外，两个检测器之间的预测分数差异很大：
- en: '![](img/bbceb912-d626-4585-89b5-293a2796d0e2.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bbceb912-d626-4585-89b5-293a2796d0e2.png)'
- en: Summary
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter gives an overview of object detection and several challenges in
    modeling a good detector. While there are many methods for detection using deep
    learning, common categories are one-stage and two-stage detectors. Each of the
    detectors has its own advantages, such as one-stage detectors are good for real-time
    applications while two-stage detectors are good for high accuracy output. The
    difference in accuracy between the models is shown using example figures. We can
    now understand the choice of object detector and run a pre-trained model using
    TensorFlow. The various output samples for each show the effectiveness of models
    in complex images.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了目标检测以及建模良好检测器时的一些挑战。虽然有许多使用深度学习的检测方法，但常见的类别是一阶段和两阶段检测器。每种检测器都有其自身的优点，例如一阶段检测器适合实时应用，而两阶段检测器适合高精度输出。模型之间准确度的差异通过示例图展示。我们现在可以理解目标检测器的选择，并使用
    TensorFlow 运行预训练模型。每个输出的各种样本展示了模型在复杂图像中的有效性。
- en: In the next chapter, we will learn more about the image recognition problems
    of segmentation as well as tracking using deep learning methods.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习更多关于使用深度学习方法进行图像识别中的分割以及跟踪问题。
- en: References
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Viola Paul and Michael J. Jones. *Robust real-time face detection*. International
    journal of computer vision 57, no. 2 (2004): 137-154.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Viola Paul 和 Michael J. Jones. *鲁棒的实时人脸检测*。国际计算机视觉杂志 57, 第 2 期 (2004): 137-154.'
- en: 'Ren Shaoqing, Kaiming He, Ross Girshick, and Jian Sun. *Faster R-CNN: Towards
    real-time object detection with region proposal networks*. In Advances in neural
    information processing systems, pp. 91-99\. 2015.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任少强，何凯明，Ross Girshick，以及 姜山。*Faster R-CNN：使用区域建议网络实现实时目标检测*。在神经信息处理系统进展中，第 91-99
    页。2015。
- en: 'Liu Wei, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang
    Fu, and Alexander C. Berg. S*SD: Single Shot Multibox Detector*. In European conference
    on computer vision, pp. 21-37\. Springer, Cham, 2016.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '刘伟，Dragomir Anguelov，Dumitru Erhan，Christian Szegedy，Scott Reed，Cheng-Yang
    Fu，以及 Alexander C. Berg。S*SD: 单次多框检测器*。在欧洲计算机视觉会议上，第 21-37 页。Springer，Cham，2016。'
- en: 'Lin et al., *Microsoft COCO: Common Objects in Context*, [https://arxiv.org/pdf/1405.0312.pdf](https://arxiv.org/pdf/1405.0312.pdf).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '林等人，*Microsoft COCO: 上下文中的常见物体*，[https://arxiv.org/pdf/1405.0312.pdf](https://arxiv.org/pdf/1405.0312.pdf).'
