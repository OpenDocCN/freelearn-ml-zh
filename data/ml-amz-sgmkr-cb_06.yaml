- en: '*Chapter 4*: Preparing Data for DataRobot'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第四章*：为 DataRobot 准备数据'
- en: This chapter covers tasks relating to preparing data for modeling. While the
    tasks themselves are relatively straightforward, they can take up a lot of time
    and can sometimes cause frustration. Just know that if you feel this way, you
    are not alone. This is pretty normal. This is also where you will begin to notice
    that things are a bit different from your experience in an academic setting. Data
    will almost never arrive in a form that's suitable for modeling, and it is a mistake
    to assume that the data you have received is in good condition and of good quality.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了与准备建模数据相关的任务。虽然这些任务本身相对直接，但它们可能需要花费大量时间，有时还可能引起挫败感。只需知道，如果你有这样的感觉，你并不孤单。这是相当正常的。这也是你开始注意到事情与你在学术环境中的经验略有不同的地方。数据几乎永远不会以适合建模的形式出现，假设你收到的数据状况良好且质量上乘是错误的。
- en: Most real-world problems do not come with a ready-made dataset that you can
    start processing and use to build models. Most likely you will need to stitch
    data together from multiple disparate sources. Depending on the data, **DataRobot**
    might perform data preparation and cleansing tasks automatically, or you might
    have to do some of these on your own. This chapter covers concepts and examples
    to show how to cleanse and prepare your data and the features that DataRobot provides
    to help with these tasks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现实世界的问题并没有附带现成的数据集，你可以开始处理并用于构建模型。很可能会需要从多个不同的来源拼接数据。根据数据的不同，**DataRobot**
    可能会自动执行数据准备和清洗任务，或者你可能需要自己完成其中的一些。本章涵盖了概念和示例，展示了如何清洗和准备你的数据，以及 DataRobot 提供的用于这些任务的功能。
- en: 'By the end of this chapter, you will know how to set up data to hand it off
    to DataRobot and begin modeling. In the chapter, we''re going to cover the following
    main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将知道如何设置数据以便将其传递给 DataRobot 并开始建模。在本章中，我们将涵盖以下主要主题：
- en: Connecting to data sources
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接到数据源
- en: Aggregating data for modeling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为建模聚合数据
- en: Cleansing the dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清洗数据集
- en: Working with different types of data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理不同类型的数据
- en: Engineering features for modeling
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于建模的工程特征
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Some parts of this chapter require access to the DataRobot software, and some
    tools for data manipulation. Most of the examples deal with small datasets and
    therefore can be handled via Excel. The datasets that we will be using in the
    rest of this book are described in the following sections.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的一些部分需要访问 DataRobot 软件，以及一些数据操作工具。大多数示例处理的是小型数据集，因此可以通过 Excel 来处理。本书余下的部分我们将使用的数据集将在以下章节中描述。
- en: Automobile Dataset
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 汽车数据集
- en: 'The Automobile Dataset (source: Dua, D. and Graff, C. (2019). UCI Machine Learning
    Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]. Irvine,
    CA: University of California, School of Information and Computer Science) can
    be accessed at the UCI Machine Learning Repository ([https://archive.ics.uci.edu/ml/datasets/Automobile](https://archive.ics.uci.edu/ml/datasets/Automobile)).
    Each row in this dataset represents a specific automobile. The features (columns)
    describe its characteristics, risk rating, and associated normalized losses. Even
    though it is a small dataset, it has many features that are numerical as well
    as categorical. Features are described on the web page, and the data is provided
    in `.csv` format.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 汽车数据集（来源：Dua, D. 和 Graff, C. (2019)。UCI 机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加州大学欧文分校，信息与计算机科学学院）可以在
    UCI 机器学习库中访问（[https://archive.ics.uci.edu/ml/datasets/Automobile](https://archive.ics.uci.edu/ml/datasets/Automobile)）。该数据集中的每一行代表一辆特定的汽车。特征（列）描述了其特征、风险评级和相关的归一化损失。尽管这是一个小型数据集，但它具有许多数值和分类特征。特征在网页上进行了描述，数据以
    `.csv` 格式提供。
- en: Appliances Energy Prediction Dataset
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 家用电器能源预测数据集
- en: 'This dataset (source: Luis M. Candanedo, Veronique Feldheim, Dominique Deramaix,
    *Data driven prediction models of energy use of appliances in a low-energy house*,
    Energy and Buildings, Volume 140, 1 April 2017, Pages 81-97, ISSN 0378-7788) can
    be accessed at the UCI Machine Learning Repository ([https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction#](https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction#)).
    This dataset captures temperature and humidity data in various rooms in a house
    and in the outside environment, along with energy consumption by various devices
    over time. The data is captured every 10 minutes. This is a typical example of
    a time series dataset. Data is provided in `.csv` format, and the site also provides
    descriptions of the various features. All features in this dataset are numeric
    features. The dataset also includes two random variables to make the problem interesting.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集（来源：Luis M. Candanedo，Veronique Feldheim，Dominique Deramaix，*低能耗房屋中家电能源使用的数据驱动预测模型*，Energy
    and Buildings，第 140 卷，2017 年 4 月 1 日，第 81-97 页，ISSN 0378-7788）可在 UCI 机器学习库中访问（[https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction#](https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction#)）。此数据集捕捉了房屋中各个房间和室外环境中的温度和湿度数据，以及各种设备随时间推移的能源消耗。数据每
    10 分钟捕获一次。这是一个典型的时间序列数据集示例。数据以 `.csv` 格式提供，网站还提供了各种特征的描述。此数据集中的所有特征都是数值特征。数据集还包括两个随机变量以使问题更有趣。
- en: SQL
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: SQL
- en: For some parts of this chapter, it will be helpful to know SQL, although you
    do not need to know SQL to go through the example problems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章的一些部分，了解 SQL 将很有帮助，尽管您不需要了解 SQL 就能通过示例问题。
- en: Connecting to data sources
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接到数据源
- en: 'By this point, you should have a list of data sources and an idea of what data
    is stored there. Depending on your use case, these sources could be real-time
    data streaming sources you need to tap into. Here are some typical sources of
    data:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您应该有一份数据源列表以及存储在那里的数据的概念。根据您的用例，这些源可能是您需要挖掘的实时数据流源。以下是一些典型的数据源：
- en: Filesystems
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件系统
- en: Excel files
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Excel 文件
- en: SQL databases
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQL 数据库
- en: Amazon S3 buckets
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊 S3 存储桶
- en: '**Hadoop Distributed File System** (**HDFS**)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop 分布式文件系统**（**HDFS**）'
- en: NoSQL databases
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NoSQL 数据库
- en: Data warehouses
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据仓库
- en: Data lakes
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖
- en: Graph databases
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图形数据库
- en: Data streams
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据流
- en: Depending on the type of data source, you will use different mechanisms to access
    this data. These could be on-premises or in the cloud. Depending on the condition
    of the data, you can bring it directly into DataRobot, or you might have to do
    some preparation before you bring it into DataRobot. DataRobot has recently added
    capabilities in the form of **Paxata** to help with this process, but you might
    not have access to that add-on. Most of the processing work is done via **SQL**,
    **Python**, **pandas**, and **Excel**. For the purpose of this book, we will only
    focus on Excel.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据源的类型，您将使用不同的机制来访问这些数据。这些可能是本地或云端的。根据数据的状态，您可以直接将数据带入 DataRobot，或者您可能需要在将数据带入
    DataRobot 之前做一些准备工作。DataRobot 最近添加了 **Paxata** 功能来帮助这个过程，但您可能无法访问该附加组件。大部分的处理工作是通过
    **SQL**、**Python**、**pandas** 和 **Excel** 完成的。为了本书的目的，我们将只关注 Excel。
- en: 'If you are not already familiar with SQL and pandas, then it will be helpful
    for you to start learning about them as soon as you get an opportunity:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对 SQL 和 pandas 不熟悉，那么在有机会的时候开始学习它们将很有帮助：
- en: You can connect to a data source by going to the **Create New Project** menu,
    as shown in the following figure:![Figure 4.1 – Connecting to a data source
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过访问**创建新项目**菜单来连接到数据源，如图所示：![图 4.1 – 连接到数据源
- en: '](img/Figure_4.1_B17159.jpg)'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_4.1_B17159.jpg)'
- en: Figure 4.1 – Connecting to a data source
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.1 – 连接到数据源
- en: You can search for an existing data source that has been defined, or you can
    add a new data connection. If you select the **add new data connection** option
    (shown in the preceding figure), you will see the following connection choices:![Figure
    4.2 – Types of data connection
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以搜索已定义的现有数据源，或者添加新的数据连接。如果您选择**添加新数据连接**选项（如图所示），您将看到以下连接选项：![图 4.2 – 数据连接类型
- en: '](img/Figure_4.2_B17159.jpg)'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_4.2_B17159.jpg)'
- en: Figure 4.2 – Types of data connection
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.2 – 数据连接类型
- en: 'You will see the connection choices available for your organization. What you
    see here could be different from the preceding figure. Most databases with JDBC
    drivers are supported, but you might have to check with your administrator. As
    an example, let''s select the **MySQL** option, as shown in the following figure:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将看到您组织可用的连接选项。这里显示的内容可能与前面的图不同。大多数具有JDBC驱动的数据库都受支持，但您可能需要咨询您的管理员。例如，让我们选择**MySQL**选项，如图下所示：
- en: '![Figure 4.3 – Configuring a data connection'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.3 – 配置数据连接'
- en: '](img/Figure_4.3_B17159.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.3_B17159.jpg)'
- en: Figure 4.3 – Configuring a data connection
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – 配置数据连接
- en: In the preceding figure, you will see the configuration parameters for configuring
    a MySQL data source. Other data sources are similar in nature. Here, you will
    enter the configuration settings that can be obtained from your database administrator.
    You will need to create a similar connection if you are connecting to a database
    to get data into Python or Excel.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，您将看到配置MySQL数据源的配置参数。其他数据源在本质上类似。在这里，您将输入可以从数据库管理员那里获得的配置设置。如果您要将数据导入Python或Excel中的数据库，则需要创建类似的连接。
- en: Note
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You will need to have some working knowledge of SQL or work with someone who
    knows SQL to make use of these options.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要具备一些SQL的实际知识或与了解SQL的人合作，才能使用这些选项。
- en: Aggregating data for modeling
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为建模聚合数据
- en: 'From the previous chapters, you might remember that machine learning algorithms
    expect the dataset to be in a specific form and it needs to be in one table. The
    data needed for this table, however, could reside in multiple sources. Hence,
    one of the first things you need to do is to aggregate data from multiple sources.
    This is often done using SQL or Python. Recently, DataRobot has added the capability
    to add multiple datasets into a project and then aggregate this data within DataRobot.
    Please note that there are still some data cleansing operations that you might
    have to do outside of DataRobot, so if you want to use the aggregation capabilities
    of DataRobot, you need to do cleansing operations prior to bringing this data
    into DataRobot. We cover data cleansing in the following section. If you choose
    to do data aggregation inside DataRobot, you have to make sure to do this at the
    very start of the project (*Figure 4.4*):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的章节中，您可能还记得机器学习算法期望数据集以特定的形式存在，并且它需要在一个表中。然而，为这个表所需的数据可能存在于多个来源。因此，您需要做的第一件事之一就是从多个来源聚合数据。这通常使用SQL或Python来完成。最近，DataRobot增加了将多个数据集添加到项目中，并在DataRobot内部聚合这些数据的能力。请注意，您可能仍需要在DataRobot之外执行一些数据清洗操作，因此如果您想使用DataRobot的聚合功能，您需要在将数据带入DataRobot之前执行清洗操作。我们将在下一节中介绍数据清洗。如果您选择在DataRobot内部进行数据聚合，您必须确保在项目的非常开始阶段就进行此操作（*图4.4*）：
- en: '![Figure 4.4 – Add secondary datasets'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.4 – 添加二级数据集'
- en: '](img/Figure_4.4_B17159.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.4_B17159.jpg)'
- en: Figure 4.4 – Add secondary datasets
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – 添加二级数据集
- en: 'In the preceding figure, just below the **Start** button, you can click on
    **Add datasets**. Once you click on it, you will see a window that lets you specify
    the additional dataset, as shown in *Figure 4.5*:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，在**开始**按钮下方，您可以点击**添加数据集**。一旦点击，您将看到一个窗口，允许您指定额外的数据集，如图*图4.5*所示：
- en: '![Figure 4.5 – Secondary datasets'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.5 – 二级数据集'
- en: '](img/Figure_4.5_B17159.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.5_B17159.jpg)'
- en: Figure 4.5 – Secondary datasets
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – 二级数据集
- en: Here, you can add a new dataset and define the relationships between your main
    dataset and the secondary datasets. For time series problems, you can also use
    this capability to aggregate your data to the right timescale and join it with
    the main dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以添加新的数据集并定义主数据集和二级数据集之间的关系。对于时间序列问题，您还可以使用此功能将数据聚合到正确的时间尺度，并将其与主数据集连接起来。
- en: Please note that this does require some understanding of how relational tables
    work and some SQL concepts. If you are not familiar with these ideas and you are
    not sure what indexes to use, work with someone who understands databases to help
    you set this up.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这确实需要您对关系表的工作原理和某些SQL概念有一定的了解。如果您不熟悉这些概念，并且不确定要使用哪些索引，请与了解数据库的人合作，以帮助您设置此配置。
- en: Cleansing the dataset
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集清洗
- en: 'This step can come before or after the data aggregation we talked about in
    the previous section. We introduced some concepts around data cleansing in [*Chapter
    2*](B17159_02_Final_NM_ePub.xhtml#_idTextAnchor039), *Machine Learning Basics*,
    so let''s look at how to actually do it on a dataset. For this, let''s start with
    the Automobile Dataset. Please refer to the *Technical requirements* section to
    access the UCI repository for this dataset:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步可以在我们之前章节中提到的数据聚合之前或之后进行。在[*第 2 章*](B17159_02_Final_NM_ePub.xhtml#_idTextAnchor039)“机器学习基础”中，我们介绍了一些关于数据清洗的概念，所以让我们看看如何在数据集上实际操作。为此，让我们从汽车数据集开始。请参考*技术要求*部分以访问此数据集的
    UCI 仓库：
- en: 'Let''s download two files: `imports-85.data` and `imports-85.names`. The data
    file is in `.csv` format, so let''s rename the file with the `.csv` extension
    and open it using Excel (you can use any text editor). You will now see the data
    (*Figure 4.6*):![Figure 4.6 – Automobile data'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们下载两个文件：`imports-85.data` 和 `imports-85.names`。数据文件是 `.csv` 格式，所以让我们将文件重命名为
    `.csv` 扩展名，并使用 Excel（您可以使用任何文本编辑器）打开它。现在您将看到数据（*图 4.6*）：![图 4.6 – 汽车数据
- en: '](img/Figure_4.6_B17159.jpg)'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_4.6_B17159.jpg)'
- en: Figure 4.6 – Automobile data
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.6 – 汽车数据
- en: 'You will notice in the preceding screenshot that it is missing the header information.
    To retrieve the header information, open the `.names` file in any text editor.
    You will see the names of attributes as well as their definitions. Create an empty
    row at the top of your `.csv` file and you will have to manually type the names
    of these attributes as the first row of your file. Now let''s save this file as
    `autodata.csv`. It should now look as shown in *Figure 4.7*:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您会在前面的屏幕截图中注意到缺少标题信息。要检索标题信息，请在任何文本编辑器中打开 `.names` 文件。您将看到属性名称以及它们的定义。在您的 `.csv`
    文件顶部创建一个空行，然后您将必须手动输入这些属性名称作为文件的第一行。现在让我们将此文件保存为 `autodata.csv`。现在它应该看起来像*图 4.7*所示：
- en: '![Figure 4.7 – Automobile data with headers'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7 – 带标题的汽车数据'
- en: '](img/Figure_4.7_B17159.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.7_B17159.jpg)'
- en: Figure 4.7 – Automobile data with headers
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 带标题的汽车数据
- en: 'Please review all the cells in this data file. You will have already noticed
    that many cells in the preceding figure have a `normalized-losses` where 20% of
    the total values are missing. Given that our dataset is very small, we do not
    want to drop the rows with missing data. Also, DataRobot has mechanisms to account
    for missing values, so we are going to leave most of them as is. The only one
    that we want to consider is `normalized-losses`. If `normalized-losses` is our
    target variable, then we have no choice but to drop those rows. If not, we can
    first try to go as is and let DataRobot build a model. We can then try an alternative
    strategy of using the average value of `normalized-losses` per **Symboling** value
    to see if that makes any difference. I will use Excel''s pivot table functionality
    to compute these averages (*Figure 4.8*):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请检查此数据文件中的所有单元格。您已经注意到，在前面的图中，许多单元格都有 `normalized-losses`，其中 20% 的总值是缺失的。鉴于我们的数据集非常小，我们不希望删除包含缺失数据的行。此外，DataRobot
    有处理缺失值的机制，所以我们打算将大部分保留原样。我们唯一想要考虑的是 `normalized-losses`。如果 `normalized-losses`
    是我们的目标变量，那么我们别无选择，只能删除这些行。如果不是，我们首先可以尝试保留原样，让 DataRobot 构建模型。然后我们可以尝试使用每个**符号**值的
    `normalized-losses` 平均值作为替代策略，看看这会有什么不同。我将使用 Excel 的交叉表功能来计算这些平均值（*图 4.8*）：
- en: '![Figure 4.8 – Pivot table'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.8 – 交叉表'
- en: '](img/Figure_4.8_B17159.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.8_B17159.jpg)'
- en: Figure 4.8 – Pivot table
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – 交叉表
- en: The reason for using **Symboling** is that it is an indicator of risk. Depending
    on the problem and what you are trying to accomplish, you can choose some other
    feature for this purpose. For now, we will use **Symboling** to illustrate how
    to do it. There are more sophisticated imputation methods available, such as a
    K-Nearest Neighbor-based imputation method, that you can explore if desired ([https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html)).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**符号**的原因是它是风险的指标。根据问题和您试图达成的目标，您可以为此目的选择其他一些特征。目前，我们将使用**符号**来说明如何操作。还有更复杂的插补方法可用，例如基于
    K-最近邻的插补方法，如果您愿意可以探索（[https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html)）。
- en: 'In reviewing the Appliances Energy Prediction Dataset, we see that the data
    looks very clean and no further cleansing is required. In real-world projects,
    you will almost never find a dataset that is free of problems. Typical problems
    in time series datasets to watch out for are as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在审查“家用电器能源预测数据集”时，我们发现数据看起来非常干净，不需要进一步清洗。在现实世界的项目中，您几乎找不到没有任何问题的数据集。在时间序列数据集中需要注意的典型问题如下：
- en: '**Very little data**: You need at least 35 or so datapoints for regression
    and 100 datapoints for classification problems to allow DataRobot to do something
    useful with your data.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非常少的数据**：您至少需要35个或更多的数据点来进行回归分析，以及100个数据点来解决分类问题，以便DataRobot能够对您的数据做出有用的处理。'
- en: '**Data gaps**: Sometimes data might be missing for certain timesteps. In these
    cases, you can use values from the timesteps before or after to assign values
    for the missing time step. You can also let DataRobot do this for you.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据缺失**：有时某些时间步的数据可能会缺失。在这种情况下，您可以使用之前或之后时间步的值来为缺失的时间步分配值。您也可以让DataRobot为您完成这项工作。'
- en: '**Interrelated series**: Often you will have multiple timeseries that you are
    trying to forecast. If the series are similar and are interrelated, then you can
    combine them into a single model. This can often improve the forecast accuracy.
    In these cases, you have to create a feature that tells DataRobot that these series
    are part of the same cluster.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相互关联的序列**：通常您会尝试预测多个时间序列。如果这些序列相似且相互关联，则可以将它们合并为单个模型。这通常可以提高预测精度。在这些情况下，您必须创建一个特征来告诉DataRobot这些序列属于同一个簇。'
- en: We will revisit the data quality based on what DataRobot finds. Now that the
    dataset looks reasonably clean (which is very unusual by the way), let's investigate
    this data further.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将根据DataRobot发现的结果重新审视数据质量。现在，数据集看起来相当干净（顺便说一句，这在现实中是非常不寻常的），让我们进一步调查这些数据。
- en: Working with different types of data
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理不同类型的数据
- en: You will have noticed that some of the features have numeric values while others
    have categorical values. For example, the `standard` as well as `std` in your
    datasets. In this case, DataRobot will treat them as different values, even though
    they are the same.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，一些特征具有数值，而其他特征具有分类值。例如，数据集中的`standard`以及`std`。在这种情况下，即使它们是相同的，DataRobot也会将它们视为不同的值。
- en: 'There are some features that can be treated as categorical or as numerical.
    For example, `num-of-cylinders`; here, the values are expressed as text. Given
    that there is a numerical order here, it might be beneficial to turn this into
    a numeric variable, as shown in *Figure 4.9*:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有些特征可以被视为分类或数值。例如，`num-of-cylinders`；在这里，值以文本形式表示。鉴于这里存在数值顺序，将其转换为数值变量可能是有益的，如图*4.9*所示：
- en: '![Figure 4.9 – Categorical to numerical feature conversion'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.9 – 将分类特征转换为数值特征]'
- en: '](img/Figure_4.9_B17159.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_4.9_B17159.jpg)'
- en: Figure 4.9 – Categorical to numerical feature conversion
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 – 将分类特征转换为数值特征
- en: Here, we have created (in `cylinder-count`, that carries the numerical values
    for the number of cylinders. In this example, we are using Excel for the data
    manipulation, but this can be achieved via many methods, such as SQL, Python,
    and Paxata. You can do similar data manipulation and create a new column for `num-of-doors`
    as well.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了（在`cylinder-count`中，它包含了气缸数量的数值。在这个例子中，我们使用Excel进行数据处理，但这也可以通过许多方法实现，例如SQL、Python和Paxata。您也可以进行类似的数据处理，并为`num-of-doors`创建一个新的列。
- en: 'Let''s take a look at the `make` feature in the following figure. This seems
    to have 22 possible values, but we have very limited data available. If we count
    the number of rows for each make, we can see how much data is available for each
    make:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下以下图中`make`特征。这似乎有22个可能的值，但我们可用的数据非常有限。如果我们计算每个制造商的行数，我们可以看到每个制造商可用的数据量：
- en: '![Figure 4.10 – Data for each make'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.10 – 每个制造商的数据]'
- en: '](img/Figure_4.10_B17159.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_4.10_B17159.jpg)'
- en: Figure 4.10 – Data for each make
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 – 每个制造商的数据
- en: We notice that some car types have very little data available, so it might be
    useful to combine some of them. For example, we can combine (using Excel) the
    highlighted rows into a make called `other`. Where you draw the line depends upon
    your understanding of the business problem or discussions with domain experts.
    Even with that knowledge, you might have to try out a few different options to
    see what works best. This is what makes machine learning an iterative and exploratory
    process. Also keep in mind that you have limited time available, so don't over-explore
    either. There is certainly a point of diminishing returns where additional tinkering
    will not produce many benefits.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到某些车型可用的数据非常少，因此将它们中的某些组合起来可能很有用。例如，我们可以（使用Excel）将突出显示的行合并成一个名为`other`的制造商。你如何划分界限取决于你对业务问题的理解或与领域专家的讨论。即使有了这些知识，你也可能需要尝试几种不同的选项，看看哪种效果最好。这正是机器学习是一个迭代和探索过程的原因。同时，请记住，你的时间有限，因此不要过度探索。当然，存在一个收益递减的点，额外的调整不会带来很多好处。
- en: DataRobot also allows special processing for images and geo-spatial data. We
    will cover them in [*Chapter 11*](B17159_11_Final_NM_ePub.xhtml#_idTextAnchor161),
    *Working with GeoSpatial Data, NLP, and Image Processing*. Now let's look at other
    transformations that can be done on data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot 还允许对图像和地理空间数据进行特殊处理。我们将在 [*第11章*](B17159_11_Final_NM_ePub.xhtml#_idTextAnchor161)
    中介绍，*处理地理空间数据、NLP和图像处理*。现在让我们看看可以对数据进行的其他转换。
- en: Engineering features for modeling
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模特征工程
- en: 'As part of the system''s understanding you would have gained some insights
    into your problem and dataset that can be used to create new features in your
    dataset by combining the existing features in various ways. For example, we can
    create a new feature called `volume` by multiplying length, width, and height.
    Similarly, we can create a feature called `mpg-ratio` by dividing `highway-mpg`
    by `city-mpg`. Let''s also create a feature called `cylinder-size` by dividing
    `engine-size` by `cylinder-count`. The equations for these features are as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 作为系统理解的一部分，你会对问题和数据集有了一些洞察，这些洞察可以通过以各种方式组合现有特征来创建数据集中的新特征。例如，我们可以通过乘以长度、宽度和高度来创建一个名为`volume`的新特征。同样，我们可以通过将`highway-mpg`除以`city-mpg`来创建一个名为`mpg-ratio`的特征。让我们还创建一个名为`cylinder-size`的特征，通过将`engine-size`除以`cylinder-count`。以下是一些特征的方程：
- en: '`volume = length * width * height`'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`volume = length * width * height`'
- en: '`mpg-ratio = highway-mpg / city-mpg`'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mpg-ratio = highway-mpg / city-mpg`'
- en: '`cylinder-size = engine-size / cylinder-count`'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cylinder-size = engine-size / cylinder-count`'
- en: '*Figure 4.11* shows an example of what these feature values look like:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.11* 展示了这些特征值看起来是什么样的：'
- en: '![Figure 4.11 – Engineered features for the Automobile Dataset'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.11 – 车辆数据集的工程化特征'
- en: '](img/Figure_4.11_B17159.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.11_B17159.jpg)'
- en: Figure 4.11 – Engineered features for the Automobile Dataset
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 – 车辆数据集的工程化特征
- en: As you can now see, many possibilities exist to create new features that could
    prove helpful in solving your problem. Many of these new features may not be useful,
    and it is OK to drop them later. Sometimes, such features will have meaning for
    the customers or stakeholders, and you might want to keep them instead of some
    other features that are redundant.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你现在所看到的，存在许多可能性来创建新的特征，这些特征可能有助于解决你的问题。许多这些新特征可能并不有用，稍后删除它们是可以接受的。有时，这些特征对客户或利益相关者可能有意义，你可能希望保留它们而不是其他冗余的特征。
- en: 'Let''s take a look at the Appliances Energy Prediction Dataset file. With this
    dataset, we can create the following features:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下电器能源预测数据集文件。使用这个数据集，我们可以创建以下特征：
- en: '`total-energy = Appliances + lights`'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`total-energy = Appliances + lights`'
- en: '`avg-temp-inside = (T1 + T2 + T3 + T4 + T5 + T7 + T8 + T9 ) / 8`'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`avg-temp-inside = (T1 + T2 + T3 + T4 + T5 + T7 + T8 + T9) / 8`'
- en: '`avg-rh-inside = (RH_1 + RH_2 + RH_3 + RH_4 + RH_5 + RH_7 + RH_8 + RH_9 ) /
    8`'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`avg-rh-inside = (RH_1 + RH_2 + RH_3 + RH_4 + RH_5 + RH_7 + RH_8 + RH_9) /
    8`'
- en: '`temp-inout-diff = T6 – avg-temp-inside`'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temp-inout-diff = T6 – avg-temp-inside`'
- en: '`rh-inout-diff = RH_6 – avg-rh-inside`'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rh-inout-diff = RH_6 – avg-rh-inside`'
- en: '`windchill-factor` (I am creating an approximate windchill factor based on
    [https://www.weather.gov/media/epz/wxcalc/windChill.pdf](https://www.weather.gov/media/epz/wxcalc/windChill.pdf))
    `= T_out * (Windspeed0.16 )`'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`windchill-factor`（我正在根据[https://www.weather.gov/media/epz/wxcalc/windChill.pdf](https://www.weather.gov/media/epz/wxcalc/windChill.pdf)创建一个近似的体感温度）`=
    T_out * (Windspeed^0.16)`'
- en: 'The new data features will appear as shown in *Figure 4.12*:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 新的数据特征将如图 *图4.12* 所示：
- en: '![Figure 4.12 – Engineered features for the Appliances Energy Prediction Dataset'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.12 – Appliances Energy Prediction Dataset 的工程特征'
- en: '](img/Figure_4.12_B17159.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.12_B17159.jpg)'
- en: Figure 4.12 – Engineered features for the Appliances Energy Prediction Dataset
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 – Appliances Energy Prediction Dataset 的工程特征
- en: As you can see, these features use our knowledge about the domain that we can
    find by talking to domain experts or doing some research on the internet. You
    might be able to find even more such features by doing some research about dew
    points, pressure, and visibility. It will be hard for the automation to catch
    all of these on its own, but on the other hand, the automation might be able to
    find some additional interesting features based on them. Recently, DataRobot has
    also been adding capabilities to automatically do some feature engineering, but
    these capabilities are somewhat limited. One area where these capabilities are
    very useful is time series problems. In this particular area, these capabilities
    are extremely helpful in trying out a wide range of features that will be hard
    to match on your own. Having said that, it is still your responsibility to inject
    your domain knowledge into the model via engineered features.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，这些功能利用了我们通过与领域专家交谈或在网上进行研究所能找到的领域知识。你可能通过研究露点、压力和能见度等主题，找到更多这样的功能。自动化系统可能很难自己捕捉到所有这些特征，但另一方面，自动化系统可能能够基于这些特征找到一些额外的有趣特征。最近，DataRobot
    也开始添加一些自动进行特征工程的能力，但这些能力相对有限。这些能力非常有用的一个领域是时间序列问题。在这个特定领域，这些能力在尝试大量难以自己匹配的特征时极为有帮助。话虽如此，将你的领域知识通过工程特征注入模型的责任仍然在你。
- en: Summary
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered methods to help you prepare the dataset for building
    the models. Many of these methods have to be applied outside of DataRobot, although
    DataRobot is beginning to provide support for many of the data preparation tasks.
    As we discussed, many of these tasks cannot be automated at this point in time,
    and they require domain understanding to make appropriate decisions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了帮助您为构建模型准备数据集的方法。虽然 DataRobot 正在开始提供对许多数据准备任务的支持，但许多这些方法仍需要在 DataRobot
    之外应用。正如我们讨论的，许多这些任务目前无法自动化，并且需要领域理解来做出适当的决策。
- en: Specifically, in this chapter we have learned how to connect to various data
    sources and how to aggregate data from these sources. We looked at examples to
    address missing data issues and other data manipulation that should be done prior
    to modeling. We also covered several methods for creating new features that can
    be very important for improving the model's performance.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在本章中，我们学习了如何连接到各种数据源以及如何从这些源汇总数据。我们研究了如何处理缺失数据问题以及其他在建模前应该进行的数据操作。我们还介绍了创建新特征的方法，这些新特征对于提高模型性能可能非常重要。
- en: We are now at a stage where we will be working almost completely inside the
    DataRobot environment to analyze the data and build models. In the next chapter,
    we will use DataRobot to analyze the datasets.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在处于一个阶段，我们将几乎完全在 DataRobot 环境中工作，以分析数据和构建模型。在下一章中，我们将使用 DataRobot 来分析数据集。
