- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Vertex AI Deployment and Automation Tools – Orchestration through Managed Kubeflow
    Pipelines
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Vertex AI部署和自动化工具 – 通过托管Kubeflow管道进行编排
- en: In a typical **machine learning** (**ML**) solution, we often have lots of applications
    and services as part of the end-to-end workflow. If we try to stitch these services
    and applications together using some custom scripts with cron jobs, it becomes
    super tricky to manage the workflows. Thus, it becomes important to make use of
    some orchestration services to carefully manage, scale, and monitor complex workflows.
    Orchestration is the process of stitching multiple applications or services together
    to build an end-to-end solution workflow. Google Cloud provides multiple orchestration
    services, such as Cloud Scheduler, Workflows, and Cloud Composer, to manage complex
    workflows at scale. Cloud Scheduler is ideal for single, repetitive tasks, Workflows
    is more suitable for complex multi-service orchestration, and Cloud Composer is
    ideal for data-driven workloads.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的**机器学习**（**ML**）解决方案中，我们通常会有很多应用程序和服务作为端到端工作流程的一部分。如果我们尝试使用一些定制的脚本和cron作业将这些服务和应用程序拼接在一起，管理工作流程就会变得非常棘手。因此，利用一些编排服务来仔细管理、扩展和监控复杂的工作流程变得非常重要。编排是将多个应用程序或服务拼接在一起以构建端到端解决方案工作流程的过程。Google
    Cloud提供了多个编排服务，例如Cloud Scheduler、Workflows和Cloud Composer，以大规模管理复杂的工作流程。Cloud
    Scheduler非常适合单一、重复的任务，Workflows更适合复杂的多服务编排，而Cloud Composer非常适合数据驱动的负载。
- en: 'ML workflows have a lot of steps, from data preparation to model training,
    evaluation, and more. On top of that, monitoring and version tracking become even
    more challenging. In this chapter, we will learn about GCP tooling for orchestrating
    ML workflows effectively. The main topics covered in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ML工作流程有很多步骤，从数据准备到模型训练、评估等。除此之外，监控和版本跟踪变得更加具有挑战性。在本章中，我们将学习关于GCP工具如何有效地编排ML工作流程。本章涵盖的主要主题如下：
- en: Orchestrating ML workflows using Vertex AI Pipelines (managed Kubeflow pipelines)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Vertex AI管道（托管Kubeflow管道）编排ML工作流程
- en: Orchestrating ML workflows using Cloud Composer (managed Airflow)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Cloud Composer（托管Airflow）编排ML工作流程
- en: Vertex AI Pipelines versus Cloud Composer
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vertex AI管道与Cloud Composer的比较
- en: Getting predictions on Vertex AI
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Vertex AI上获取预测
- en: Managing deployed models on Vertex AI
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理Vertex AI上部署的模型
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code examples shown in this chapter can be found in the following GitHub
    repo:[https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter10](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter10)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中展示的代码示例可以在以下GitHub仓库中找到：[https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter10](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter10)
- en: Orchestrating ML workflows using Vertex AI Pipelines (managed Kubeflow pipelines)
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Vertex AI管道（托管Kubeflow管道）编排ML工作流程
- en: ML solutions are complex and involve lots of steps, including data preparation,
    feature engineering, model selection, model training, testing, evaluation, and
    deployment. On top of these, it is really important to track and version control
    lots of aspects related to the ML model while in production. Vertex AI Pipelines
    on GCP lets us codify our ML workflows in such a way that they are easily composable,
    shareable, and reproducible. Vertex AI Pipelines can run Kubeflow as well as **TensorFlow
    Extended** (**TFX**)-based ML pipelines in a fully managed way. In this section,
    we will learn about developing Kubeflow pipelines for ML development as Vertex
    AI Pipelines.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 'ML解决方案复杂且涉及许多步骤，包括数据准备、特征工程、模型选择、模型训练、测试、评估和部署。除此之外，在生产过程中跟踪和版本控制与ML模型相关的许多方面也非常重要。GCP上的Vertex
    AI管道让我们能够以易于组合、共享和重现的方式对ML工作流程进行编码。Vertex AI管道可以以完全托管的方式运行Kubeflow以及基于**TensorFlow
    Extended**（**TFX**）的ML管道。在本节中，我们将学习如何开发用于ML开发的Kubeflow管道作为Vertex AI管道。 '
- en: Kubeflow is a Kubernetes-native solution that simplifies the orchestration of
    ML pipelines and makes experimentation easy and reproducible. Also, the pipelines
    are sharable. It comes with framework support for things such as execution monitoring,
    workflow scheduling, metadata logging, and versioning. A Kubeflow pipeline is
    a description of an ML workflow that combines multiple small components of the
    workflow into a **directed acyclic graph** (**DAG**). Behind the scenes, it runs
    the pipeline components on containers, which provide portability, reproducibility,
    and encapsulation. Each pipeline component is one step in the ML workflow that
    does a specific task. The output of one component may become the input of another
    component and so forth. Each pipeline component is made up of code, packaged as
    a Docker image that performs one step in the pipeline and runs on one or more
    Kubernetes Pods. Kubeflow pipelines can be leveraged for ETL and CI/CD tasks but
    they are more popularly used to run ML workflows.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow是一个Kubernetes原生解决方案，它简化了ML管道的编排，使得实验变得简单且可重复。此外，管道是可共享的。它提供了对执行监控、工作流程调度、元数据记录和版本控制等框架的支持。Kubeflow管道是ML工作流程的描述，它将工作流程的多个小组件组合成一个**有向无环图**（DAG）。在幕后，它将在容器上运行管道组件，这提供了可移植性、可重复性和封装。每个管道组件是ML工作流程中的一步，执行特定的任务。一个组件的输出可能成为另一个组件的输入，依此类推。每个管道组件由代码组成，打包成执行管道中一步的Docker镜像，并在一个或多个Kubernetes
    Pod上运行。Kubeflow管道可以用于ETL和CI/CD任务，但它们更常用于运行ML工作流程。
- en: The Vertex AI SDK lets us create and upload Kubeflow pipelines programmatically
    from within the Jupyter Notebook itself, but we can also use the console UI to
    work on pipelines. The Vertex AI UI lets us visualize the pipeline execution graph.
    It also lets us track, monitor, and compare different pipeline executions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI SDK允许我们从Jupyter Notebook内部以编程方式创建和上传Kubeflow管道，但我们也可以使用控制台UI来处理管道。Vertex
    AI UI允许我们可视化管道执行图。它还允许我们跟踪、监控和比较不同的管道执行。
- en: Developing Vertex AI Pipeline using Python
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Python开发Vertex AI管道
- en: In this section, we will develop and launch a simple Kubeflow-based Vertex Pipeline
    using the Vertex AI SDK within a Jupyter Notebook. In this example, we will work
    on an open source wine quality dataset. Let’s get started!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Vertex AI SDK在Jupyter Notebook中开发和启动一个简单的基于Kubeflow的Vertex管道。在这个例子中，我们将使用开源的葡萄酒质量数据集。让我们开始吧！
- en: 'Open a Jupyter Notebook and install some useful libraries:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 打开一个Jupyter Notebook并安装一些有用的库：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In a new cell, import useful libraries for Vertex Pipeline development:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个新的单元格中，导入用于Vertex管道开发的库：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create a timestamp variable. It will be useful in creating unique names for
    pipeline objects:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个时间戳变量。它将有助于为管道对象创建唯一的名称：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we will set some project-related configurations, such as `project_id`,
    region, staging bucket, and service account:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将设置一些与项目相关的配置，例如`project_id`、区域、暂存桶和服务账户：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this section we will use the Wine Quality dataset. The Wine Quality dataset
    was created by Cortez,Paulo, Cerdeira,A., Almeida,F., Matos,T., and Reis,J.. (2009).
    You can check it out at OR You can download the dataset from the following link:
    [https://doi.org/10.24432/C56S3T](https://doi.org/10.24432/C56S3T). (UCI Machine
    Learning Repository.)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用葡萄酒质量数据集。葡萄酒质量数据集由Cortez, Paulo, Cerdeira, A., Almeida, F., Matos,
    T. 和 Reis, J. (2009) 创建。您可以在以下链接查看：[https://doi.org/10.24432/C56S3T](https://doi.org/10.24432/C56S3T)。
    (UCI机器学习仓库。)
- en: 'Next, we load and check the wine quality dataset in a notebook cell to understand
    the data and columns:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在笔记本单元格中加载并检查葡萄酒质量数据集，以了解数据和列：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The output of this snippet is shown in *Figure 10**.1*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段的输出显示在**图10.1**中。
- en: '![Figure 10.1 – Overview of the wine quality dataset](img/B17792_10_1.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1 – 葡萄酒质量数据集概述](img/B17792_10_1.jpg)'
- en: Figure 10.1 – Overview of the wine quality dataset
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – 葡萄酒质量数据集概述
- en: 'Here is a quick overview of the feature columns:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是特征列的快速概述：
- en: '`volatile acidity`: The `volatile acidity` column represents the amount of
    gaseous acids'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`挥发性酸度`：`挥发性酸度`列表示气态酸的含量'
- en: '`fixed acidity`: The amount of fixed acids found in wine, which can be tartaric,
    succinic, citric, malic, and so on'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`固定酸度`：葡萄酒中发现的固定酸量，可以是酒石酸、琥珀酸、柠檬酸、苹果酸等等'
- en: '`residual sugar`: This column represents the amount of sugar left after the
    fermentation of wine'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`残糖`：此列表示葡萄酒发酵后剩余的糖量'
- en: '`citric acid`: The amount of citric acid, which is naturally found in fruits'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`柠檬酸`：柠檬酸的含量，它天然存在于水果中'
- en: '`chlorides`: The amount of salt in the wine'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`氯化物`：葡萄酒中的盐含量'
- en: '`free sulfur dioxide`: Sulpher dioxide, or SO2, prevents wine oxidation and
    spoilage'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`游离二氧化硫`：二氧化硫，或SO2，可以防止葡萄酒氧化和变质'
- en: '`total sulfur dioxide`: The total amount of SO2 in a wine'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`总二氧化硫`：葡萄酒中SO2的总含量'
- en: '`pH`: pH is used for checking acidity in a wine'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pH值`：pH值用于检查葡萄酒的酸度'
- en: '`density`: Represents the density of the wine'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`密度`：表示葡萄酒的密度'
- en: '`sulphates`: Sulphates help preserve the freshness of wine and also protect
    it from oxidation and bacteria'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`硫酸盐`：硫酸盐有助于保持葡萄酒的新鲜度，并保护其免受氧化和细菌侵害'
- en: '`alcohol`: The percentage of alcohol present in the wine'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`酒精`：葡萄酒中酒精的百分比'
- en: The idea is to predict the wine quality given all the preceding parameters.
    We will convert it into a classification problem and call a wine *best quality*
    if its quality indicator value is >=7.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 策略是预测葡萄酒质量，给定所有前面的参数。我们将将其转换为分类问题，如果葡萄酒的质量指标值 >=7，则称其为*最佳质量*。
- en: Pipeline components
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道组件
- en: 'In this exercise, we will define four pipeline components for our task:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将为我们的任务定义四个管道组件：
- en: Data loading component
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据加载组件
- en: Model training component
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练组件
- en: Model evaluation component
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估组件
- en: Model deploying component
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型部署组件
- en: Here, the first component loads the data and the second component uses that
    data to train a model. The third component evaluates the trained model on the
    test dataset. The fourth component automatically deploys the trained model as
    a Vertex AI endpoint. We will put a condition on automatic model deployment, such
    as if model ROC >= 0.8, then deploy the model, otherwise don’t.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，第一个组件加载数据，第二个组件使用这些数据来训练模型。第三个组件在测试数据集上评估训练好的模型。第四个组件将训练好的模型自动部署为Vertex
    AI端点。我们将对自动模型部署设置条件，例如，如果模型ROC >= 0.8，则部署模型，否则不部署。
- en: Now, let’s define these components one by one. The following is the first component
    that loads and splits the data into training and testing partitions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们逐一定义这些组件。以下是一个加载并拆分数据到训练和测试分区的第一个组件。
- en: 'To create a Kubeflow component, we can wrap our function with an `@component`
    decorator. Here, we can define the base image, and also the dependencies to install:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个Kubeflow组件，我们可以用`@component`装饰器包装我们的函数。在这里，我们可以定义基本镜像，以及需要安装的依赖项：
- en: Note
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: In a real project or production pipeline, it is advisable to write package versions
    along with their names to avoid any version realated conflicts.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际项目或生产管道中，建议在包名称旁边写出版本号，以避免任何版本相关的冲突。
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, we define the function that loads and splits the data into train and
    test sets:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义一个函数，用于加载数据并将其拆分为训练集和测试集：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We will keep about 30% of the data for testing and the remaining for training
    and save them as CSV files:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保留大约30%的数据用于测试，其余的用于训练，并将它们保存为CSV文件：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To define a component, we can wrap our Python functions with an `@component`
    decorator. It allows us to pass the base image path, packages to install, and
    a YAML file path if we wish to write the component into a file. The YAML file
    definition of a component makes it portable and reusable. We can simply create
    a YAML file with the component definition and load this component anywhere in
    the project. Note that we can use our custom container image with all our custom
    dependencies as well.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义一个组件，我们可以用`@component`装饰器包装我们的Python函数。它允许我们传递基本镜像路径、要安装的包，如果需要将组件写入文件，还可以指定YAML文件路径。组件的YAML文件定义使其具有可移植性和可重用性。我们可以简单地创建一个包含组件定义的YAML文件，并在项目的任何位置加载此组件。请注意，我们还可以使用包含所有自定义依赖项的自定义容器镜像。
- en: The first component essentially loads the wine quality dataset table, creates
    the binary classification output, as discussed previously, drops unnecessary columns,
    and finally divides it into train and test files. Here, the train and test dataset
    files are output artifacts of this component that can be reused by subsequently
    running components.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个组件实际上加载了葡萄酒质量数据集表，创建了之前讨论的二进制分类输出，删除了不必要的列，并将其最终分为训练文件和测试文件。在这里，训练和测试数据集文件是该组件的输出工件，可以在随后运行的组件中重用。
- en: Now, let’s define the second component, which trains a random forest classifier
    over the training dataset generated by the first component.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义第二个组件，该组件在第一个组件生成的训练数据集上训练一个随机森林分类器。
- en: 'The first step is the decorator, with dependencies:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是装饰器，包括依赖项：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next, we define our training function, which fits our model on training data
    and saves it as a Pickle file. Here, our output artifact would be a model and
    we can associate it with some metadata as well, as shown in the following function.
    Inside this function, we can associate the model artifact with metadata by putting
    the metadata key and value within `model.metadata` dictionary.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的训练函数，它将模型拟合到训练数据上，并将其保存为 Pickle 文件。在这里，我们的输出工件将是一个模型，我们还可以将其与一些元数据相关联，如下面的函数所示。在这个函数内部，我们可以通过将元数据键和值放入
    `model.metadata` 字典中来将模型工件与元数据关联。
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This component trains a random forest classifier model on the training dataset
    and saves the model as a Pickle file.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此组件在训练数据集上训练随机森林分类器模型，并将模型保存为 Pickle 文件。
- en: 'Next, let’s define the third component for model evaluation. We start with
    the `@``component` decorator:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义模型评估的第三个组件。我们以 `@component` 装饰器开始：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we define the actual Python function for model evaluation:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义实际的 Python 函数用于模型评估：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here is a small function that controls the deployment of the model. We only
    deploy a new model if its accuracy is above a certain threshold:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个小的函数，用于控制模型的部署。只有当新模型的准确度高于某个阈值时，我们才部署新模型：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now that we have the model outputs, we can calculate accuracy scores and `roc_curve`,
    and log them as metadata:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了模型输出，我们可以计算准确度分数和 `roc_curve`，并将它们作为元数据记录：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we check the model accuracy and see whether it satisfies the deployment
    condition. We return the deployment condition flag from here:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们检查模型准确度，看它是否满足部署条件。我们从这里返回部署条件标志：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This component uses the outputs of component 1 (test dataset) and component
    2 (trained model) as input and performs model evaluation. This component performs
    the following operations:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此组件使用组件 1（测试数据集）和组件 2（训练模型）的输出作为输入，并执行模型评估。此组件执行以下操作：
- en: Loads the test dataset
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载测试数据集
- en: Loads the trained model from a Pickle file
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Pickle 文件中加载训练好的模型
- en: Logs the ROC curve and confusion matrix as an output artifact
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 ROC 曲线和混淆矩阵作为输出工件记录
- en: Checks whether the model accuracy is greater than the threshold
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查模型准确度是否大于阈值
- en: 'Finally, we define the model deployment component. This component automatically
    deploys the trained model as a Vertex AI endpoint if the deployment condition
    is `true`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义模型部署组件。如果部署条件为 `true`，则该组件会自动将训练好的模型作为 Vertex AI 端点部署：
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we define the function that deploys the wine quality model when the deployment
    condition is true. This function will be wrapped around by the previously defined
    `@component` decorator so that we can later use it in the final pipeline definition:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数，当部署条件为真时，将部署葡萄酒质量模型。这个函数将被之前定义的 `@component` 装饰器包装起来，这样我们就可以在最终的管道定义中使用它：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here, we define a function to create the endpoint for our model so that we
    can use it for inference:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义一个函数来创建我们模型的端点，以便我们可以用它进行推理：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here, we import our saved model programmatically:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们以编程方式导入我们保存的模型：
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we deploy the uploaded model on the desired machine type with the
    desired traffic split:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将上传的模型部署到所需的机器类型上，并按照所需的流量分配：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now that the core components of our pipeline are ready, we can go ahead and
    define our Vertex Pipeline.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们管道的核心组件已经准备好了，我们可以继续定义我们的 Vertex Pipeline。
- en: 'First, we need to provide a unique name for our pipe:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要为我们的管道提供一个唯一的名称：
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Pipeline definition is the part where we stitch these components together to
    define our ML workflow (or execution graph). Here, we can control which components
    run first and the output of which component should be fed to another component.
    The following scripts define a simple pipeline for our experiment.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 管道定义是我们将这些组件拼接在一起以定义我们的机器学习工作流程（或执行图）的部分。在这里，我们可以控制哪些组件先运行，以及哪个组件的输出应该被馈送到另一个组件。以下脚本定义了我们实验的一个简单管道。
- en: 'We can use the `@dsl.pipeline` decorator to define a Kubeflow pipeline. We
    can pass here a `pipeline_root` parameter inside the decorator, as shown in the
    following code:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `@dsl.pipeline` 装饰器来定义一个 Kubeflow 管道。我们可以在装饰器中传递一个 `pipeline_root` 参数，如下面的代码所示：
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can create the execution DAG here and define the order of execution for
    our predefined components. Some components can be dependent, where the output
    of one component is the input for another. Dependent components execute sequentially,
    while independent ones can be executed in parallel:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这里创建执行 DAG 并定义预定义组件的执行顺序。某些组件可能是相互依赖的，其中一个组件的输出是另一个组件的输入。依赖组件按顺序执行，而独立的组件可以并行执行：
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here is our condition that decides whether to deploy this model or not:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是决定是否部署此模型的条件：
- en: '[PRE23]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Here, we use the `@dsl.pipeline` decorator to define our pipeline. Note that
    in the preceding definition, the first three components are simple, but the fourth
    component has been defined using `dsl.Condition()`. We only run the model deployment
    component if this condition is satisfied. So, this is how we can control when
    to deploy the model. If our model meets the business criteria, we can choose to
    auto-deploy it.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 `@dsl.pipeline` 装饰器来定义我们的流水线。请注意，在先前的定义中，前三个组件很简单，但第四个组件是使用 `dsl.Condition()`
    定义的。只有当这个条件满足时，我们才会运行模型部署组件。因此，这就是我们控制何时部署模型的方法。如果我们的模型符合业务标准，我们可以选择自动部署它。
- en: 'Next, we can compile our pipeline:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以编译我们的流水线：
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, we can submit our pipeline job to Vertex AI:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将我们的流水线作业提交到 Vertex AI：
- en: '[PRE25]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This script will launch our pipeline in Vertex AI. It will also provide us with
    a console URL to monitor the pipeline job.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本将在 Vertex AI 中启动我们的流水线。它还将为我们提供一个控制台 URL，以便监控流水线作业。
- en: We can also locate the pipeline run by going to the **Vertex AI** tab in the
    console and clicking on the **pipelines** tab. *Figure 10**.2* is a screenshot
    of the execution graph present in Vertex AI for our example job.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过在控制台中转到 **Vertex AI** 选项卡并点击 **流水线** 选项卡来定位流水线运行。*图 10.2* 是 Vertex AI
    中我们示例作业的执行图截图。
- en: '![Figure 10.2 – Execution graph of our example Vertex Pipeline from the Google
    Cloud console UI](img/B17792_10_2.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.2 – 从 Google Cloud 控制台 UI 观看的示例 Vertex 流水线的执行图](img/B17792_10_2.jpg)'
- en: Figure 10.2 – Execution graph of our example Vertex Pipeline from the Google
    Cloud console UI
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – 从 Google Cloud 控制台 UI 观看的示例 Vertex 流水线的执行图
- en: As we can see, this execution graph has all four components defined by us. It
    also has all the artifacts generated by the components. If we click on the **metrics**
    artifact, we can see the output values in the right pane of the console UI. It
    looks something similar to *Figure 10**.3*.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这个执行图包含了我们定义的所有四个组件。它还包括由组件生成的一切工件。如果我们点击 **指标** 工件，我们可以在控制台 UI 的右侧面板中看到输出值。它看起来类似于
    *图 10.3*。
- en: '![Figure 10.3 – Metadata and artifacts related to our pipeline execution](img/B17792_10_3.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.3 – 与我们的流水线执行相关的元数据和工件](img/B17792_10_3.jpg)'
- en: Figure 10.3 – Metadata and artifacts related to our pipeline execution
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – 与我们的流水线执行相关的元数据和工件
- en: This is how we can use the Google Cloud console UI to track the execution and
    metrics of our ML-related workflows. Once we have our pipeline ready, we can also
    schedule its execution using services such as the native scheduler for Vertex
    AI Pipelines, Cloud Scheduler (we can define a schedule), Cloud Functions (event-based
    trigger), and so on.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们可以使用 Google Cloud 控制台 UI 跟踪我们的 ML 相关工作流的执行和指标的方法。一旦我们的流水线准备就绪，我们还可以使用 Vertex
    AI Pipelines 的本地调度器、Cloud Scheduler（我们可以定义一个计划）、Cloud Functions（基于事件的触发器）等服务来安排其执行。
- en: Now, we have a good understanding of how Kubeflow pipelines can be developed
    on Google Cloud as Vertex AI Pipelines. We should be able to develop and launch
    our custom pipelines from scratch now. In the next section, we will learn about
    Cloud Composer as another solution for workflow orchestration.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对如何在 Google Cloud 上作为 Vertex AI Pipelines 开发 Kubeflow 流水线有了很好的理解。我们现在应该能够从头开始开发和启动我们的自定义流水线。在下一节中，我们将了解
    Cloud Composer 作为工作流编排的另一种解决方案。
- en: Orchestrating ML workflows using Cloud Composer (managed Airflow)
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Cloud Composer（托管 Airflow）编排 ML 工作流
- en: Cloud Composer is a workflow orchestration service on Google Cloud that is built
    upon the open source project of Apache Airflow. The key difference is that Composer
    is fully managed and also integrates with other GCP tooling very easily. With
    Cloud Composer, we can write, execute, schedule, or monitor our workflows that
    are also supported across multi-cloud and hybrid environments. Composer pipelines
    are DAGs that can be easily defined and configured using Python. It comes with
    a rich library of connectors that let us deploy our workflows instantly with one
    click. Graphical representations of workflows on the Google Cloud console make
    monitoring and troubleshooting quite convenient. Automatic synchronization of
    our DAGs ensures that our jobs always stay on schedule.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud Composer 是建立在 Apache Airflow 开源项目之上的 Google Cloud 上的工作流程编排服务。主要区别在于 Composer
    是完全托管，并且可以非常容易地与其他 GCP 工具集成。使用 Cloud Composer，我们可以编写、执行、调度或监控我们的跨多云和混合环境也得到支持的工作流程。Composer
    管道是 DAG，可以使用 Python 轻松定义和配置。它附带丰富的连接器库，让我们可以一键部署我们的工作流程。在 Google Cloud 控制台上的工作流程图形表示使得监控和故障排除非常方便。我们的
    DAG 自动同步确保我们的作业始终按计划进行。
- en: Cloud Composer is commonly used by data scientists and data engineers to build
    complex data pipelines (ETL or ELT pipelines). It can also be used as an orchestrator
    for ML workflows. Cloud Composer is pretty convenient for data-related workflows
    as the Apache project comes with hundreds of operators and sensors that make it
    easy to communicate across multiple cloud environments with very little code.
    It also lets us define failure handling mechanisms such as sending emails or Slack
    notifications on pipeline failure.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud Composer 通常被数据科学家和数据工程师用于构建复杂的数据管道（ETL 或 ELT 管道）。它也可以用作 ML 工作流程的编排器。由于
    Apache 项目附带数百个操作员和传感器，Cloud Composer 对于数据相关的工作流程来说非常方便，这使得我们可以用很少的代码轻松地在多个云环境中进行通信。它还允许我们定义故障处理机制，例如在管道失败时发送电子邮件或
    Slack 通知。
- en: Now let’s understand how to develop Cloud Composer-based pipelines.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解如何开发基于 Cloud Composer 的管道。
- en: Creating a Cloud Composer environment
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 Cloud Composer 环境
- en: 'We can follow these steps to create a Cloud Composer environment using the
    Google Cloud console UI:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按照以下步骤使用 Google Cloud 控制台 UI 创建 Cloud Composer 环境：
- en: Enable the Cloud Composer API.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用 Cloud Composer API。
- en: From the left pane of the console, select **Composer** and click on **Create**
    to start creating a Composer environment (see *Figure 10**.4*).
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从控制台左侧面板中选择 **Composer**，然后点击 **创建** 以开始创建 Composer 环境（见 *图 10.4*）。
- en: '![ Figure 10.4 – Creating a Composer environment on the Google Cloud console](img/B17792_10_4.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.4 – 在 Google Cloud 控制台上创建 Composer 环境](img/B17792_10_4.jpg)'
- en: Figure 10.4 – Creating a Composer environment on the Google Cloud console
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – 在 Google Cloud 控制台上创建 Composer 环境
- en: Click on **Create**. It will take about 15–20 minutes to create the environment.
    Once it is complete, the environment page will look like the following (see *Figure
    10**.5*).
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **创建**。创建环境大约需要 15-20 分钟。一旦完成，环境页面将如下所示（见 *图 10.5*）。
- en: '![Figure 10.5 – Ready-to-use Cloud Composer environment](img/B17792_10_5.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.5 – 即可使用的 Cloud Composer 环境](img/B17792_10_5.jpg)'
- en: Figure 10.5 – Ready-to-use Cloud Composer environment
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 – 即可使用的 Cloud Composer 环境
- en: Click on **Airflow** to see the Airflow web UI. The Airflow web UI is shown
    in *Figure 10**.6*.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **Airflow** 以查看 Airflow 网页 UI。Airflow 网页 UI 如 *图 10.6* 所示。
- en: '![ Figure 10.6 – Airflow web UI with our workflows](img/B17792_10_6.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.6 – 带有我们的工作流程的 Airflow 网页 UI](img/B17792_10_6.jpg)'
- en: Figure 10.6 – Airflow web UI with our workflows
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – 使用我们的工作流程的 Airflow 网页 UI
- en: As we can see in the preceding screenshot, there is already one DAG running
    – `airflow_monitoring.py` file. See *Figure 10**.7*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个屏幕截图所示，已经有一个 DAG 在运行 – `airflow_monitoring.py` 文件。见 *图 10.7*。
- en: '![Figure 10.7 – GCS location where we can put our Python-based DAGs for execution](img/B17792_10_7.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.7 – 我们可以放置基于 Python 的 DAG 以进行执行的 GCS 位置](img/B17792_10_7.jpg)'
- en: Figure 10.7 – GCS location where we can put our Python-based DAGs for execution
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 – 我们可以放置基于 Python 的 DAG 以进行执行的 GCS 位置
- en: Now that our Composer setup is ready, we can quickly check whether it is working
    as expected. To test things fast, we will use one demo DAG from the Airflow tutorials
    and put it inside the **dags** folder of this bucket. If everything is working
    fine, any DAG that we put inside this bucket should automatically get synced with
    Airflow.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们Composer的设置已经准备好了，我们可以快速检查它是否按预期工作。为了快速测试，我们将使用Airflow教程中的一个演示DAG，并将其放入这个桶的**dags**文件夹中。如果一切正常，我们放入这个桶中的任何DAG都应该会自动与Airflow同步。
- en: 'The following is the code for a demo DAG from the Airflow tutorials:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码是从Airflow教程中的一个演示DAG：
- en: '[PRE26]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following dictionary with some default arguments will be used when creating
    the operators later. By default, these arguments will be passed to each operator,
    but we can also override some of these arguments in some operators as per the
    requirements:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建操作符时，将使用以下具有一些默认参数的字典。默认情况下，这些参数将传递给每个操作符，但根据需要，我们也可以在操作符中覆盖一些这些参数：
- en: '[PRE27]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This is where we define our DAG, with execution steps in the desired or required
    order:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们定义我们的DAG的地方，执行步骤按照所需或要求的顺序：
- en: '[PRE28]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here, we define different tasks that our code will be performing:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了我们的代码将要执行的不同任务：
- en: '[PRE29]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You can document your task using the following attributes: `doc_md` (Markdown),
    `doc` (plain text), `doc_rst`, `doc_json`, and `doc_yaml`, which gets rendered
    on the UI’s **Task Instance** **Details** page:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下属性来记录您的任务：`doc_md`（Markdown）、`doc`（纯文本）、`doc_rst`、`doc_json`和`doc_yaml`，这些属性将在UI的**任务实例****详情**页面上显示：
- en: '[PRE30]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now let’s define the `t3` task:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义`t3`任务：
- en: '[PRE31]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Here, we define the execution order of our tasks. `t1` needs to be executed
    before `t2` and `t3`, but `t2` and `t3` can execute in parallel:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了任务的执行顺序。`t1`需要在`t2`和`t3`之前执行，但`t2`和`t3`可以并行执行：
- en: '[PRE32]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As soon as we upload this `.py` file to a GCS bucket inside the dags folder,
    Airflow will automatically sync it. If you refresh the Airflow web UI, it should
    show another DAG, as shown in *Figure 10**.8*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将这个`.py`文件上传到dags文件夹中的GCS桶，Airflow将自动同步它。如果您刷新Airflow web UI，它应该显示另一个DAG，如图*图10*.*8*所示。
- en: '![Figure 10.8 – Airflow web UI with all the DAGs that are present in the GCS
    location](img/B17792_10_8.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图10.8 – Airflow web UI中GCS位置中存在的所有DAG](img/B17792_10_8.jpg)'
- en: Figure 10.8 – Airflow web UI with all the DAGs that are present in the GCS location
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 – Airflow web UI中GCS位置中存在的所有DAG
- en: If we are able to see our DAG running in the Airflow UI, it verifies that our
    installation is working fine. Now, let’s open this DAG to check the actual execution
    graph. It should look something similar to what is shown in *Figure 10**.9*.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能在Airflow UI中看到我们的DAG正在运行，这验证了我们的安装运行正常。现在，让我们打开这个DAG来检查实际的执行图。它应该看起来与*图10*.*9*中显示的类似。
- en: '![Figure 10.9 – Execution graph of our workflow within the Airflow web UI](img/B17792_10_9.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图10.9 – Airflow web UI中我们工作流程的执行图](img/B17792_10_9.jpg)'
- en: Figure 10.9 – Execution graph of our workflow within the Airflow web UI
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9 – Airflow web UI中我们工作流程的执行图
- en: Although it is a very simple DAG, it gives an idea of how easy it is to work
    with Airflow using Cloud Composer. The level of logging and monitoring we get
    with Cloud Composer is quite amazing. Cloud Composer makes the lives of data engineers
    really easy so that they can focus on defining complex data pipelines without
    worrying about infrastructure and Airflow management.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是一个非常简单的DAG，但它让我们了解到使用Cloud Composer与Airflow一起工作是多么容易。我们通过Cloud Composer获得的日志记录和监控水平相当惊人。Cloud
    Composer让数据工程师的生活变得非常容易，这样他们就可以专注于定义复杂的数据管道，而不用担心基础设施和Airflow管理。
- en: We now have a good idea of how Vertex AI Pipelines and Cloud Composer (managed
    Airflow service) can be used as an orchestrator for ML workflows. Now let’s summarize
    some of the similarities and differences between these two.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对如何使用Vertex AI Pipelines和Cloud Composer（托管Airflow服务）作为ML工作流程的编排器有了很好的了解。现在让我们总结一下这两者之间的一些相似之处和不同之处。
- en: Vertex AI Pipelines versus Cloud Composer
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Vertex AI Pipelines与Cloud Composer
- en: 'In this section, we will talk about some of the key similarities and differences
    between Vertex AI Pipelines and Cloud Composer when it comes to orchestrating
    ML workflows. Based on this comparison, we can choose the best solution for our
    next ML project. The following is a list of points that summarize the important
    aspects of both orchestrators for ML-related tasks:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论Vertex AI Pipelines和Cloud Composer在编排ML工作流程方面的关键相似之处和不同之处。基于这种比较，我们可以为我们的下一个ML项目选择最佳解决方案。以下是一个总结两个编排器在ML相关任务重要方面的要点列表：
- en: Both are easy to use and divide the overall ML workflow into smaller execution
    units in terms of tasks (Composer) or containerized components (Vertex AI Pipelines).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在任务（作曲家）或容器化组件（Vertex AI Pipelines）方面，它们都易于使用，并将整体ML工作流程划分为更小的执行单元。
- en: Passing data between components is similar, and it may require an intermediate
    storage system if the data size is large.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在组件之间传递数据的方式相似，如果数据量很大，可能需要一个中间存储系统。
- en: Vertex AI Pipelines have an extensive list of prebuilt components available
    open source and thus developers can avoid writing a lot of boilerplate code. On
    the other hand, in the case of a Composer-based pipeline, we need to write the
    entire workflow.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vertex AI Pipelines提供了一系列可用的预构建组件，这些组件是开源的，因此开发者可以避免编写大量的模板代码。另一方面，在基于Composer的管道的情况下，我们需要编写整个工作流程。
- en: Based on the ease of setting up environments, Vertex AI Pipelines is a little
    bit easier.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据设置环境的简便性，Vertex AI Pipelines稍微容易一些。
- en: Both run on Kubernetes, but in the case of Vertex AI Pipelines, there is no
    need to worry about clusters, Pods, and so on.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们都运行在Kubernetes上，但在Vertex AI Pipelines的情况下，无需担心集群、Pods等。
- en: Cloud Composer is ideal for data-related tasks. We can also implement ML pipelines
    as a data task but we lose a lot of ML-related functionalities, such as lineage
    tracking, metrics, experiment comparisons, and distributed training. These features
    come out of the box with Vertex AI Pipelines.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloud Composer非常适合数据相关任务。我们还可以将ML管道作为数据任务实现，但我们会失去许多ML相关功能，例如血缘跟踪、指标、实验比较和分布式训练。这些功能是Vertex
    AI Pipelines的默认功能。
- en: Data engineers might feel more comfortable with Composer pipelines, while ML
    engineers might be more comfortable with Vertex AI Pipelines.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据工程师可能会更习惯于Composer管道，而ML工程师可能会更习惯于Vertex AI Pipelines。
- en: In many cases, Vertex AI Pipelines can be cheaper to use as here we pay for
    what we use. On the other hand, in the case of Composer, some Pods are always
    running.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在许多情况下，Vertex AI Pipelines的使用成本可能更低，因为我们只为我们使用的部分付费。另一方面，在Composer的情况下，一些Pods总是处于运行状态。
- en: If needed, some of the Vertex AI Pipelines capabilities can be used with Composer
    as well.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要，一些Vertex AI Pipelines的功能也可以与Composer一起使用。
- en: Working with Vertex AI Pipelines requires zero knowledge about Kubernetes, but
    with Cloud Composer, it is important to know common aspects of Kubernetes.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Vertex AI Pipelines不需要了解Kubernetes，但使用Cloud Composer时，了解Kubernetes的常见方面很重要。
- en: After reading these comparison points, we might find it easy to choose the best
    orchestrator for our next ML use case. Nevertheless, both orchestrators are easy
    to use and are commonly used across organizations to manage their complex data/ML-related
    workflows.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读了这些比较点之后，我们可能会觉得选择我们下一个机器学习（ML）用例的最佳编排器变得容易。然而，这两个编排器都易于使用，并且在各个组织中普遍用于管理它们复杂的数据/ML相关工作流程。
- en: Now that we have a good understanding of ML orchestration tools on Google Cloud
    with their pros and cons, we are ready to start developing production-grade ML
    pipelines. Next, let’s learn how to get predictions on Vertex AI.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对Google Cloud上的ML编排工具及其优缺点有了很好的理解，我们准备开始开发生产级的ML管道。接下来，让我们学习如何在Vertex
    AI上获取预测。
- en: Getting predictions on Vertex AI
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Vertex AI上获取预测
- en: In this section, we will learn how to get predictions from our ML models on
    Vertex AI. Depending on the use case, prediction requests can be of two types
    – online predictions (real time) and batch predictions. Online predictions are
    synchronous requests made to a model endpoint. Online predictions are needed by
    applications that keep requesting outputs for given inputs in a timely manner
    via an API call in order to update information for end users in near real time.
    For example, the Google Maps API gives us near real-time traffic updates and requires
    online prediction requests. Batch predictions, on the other hand, are asynchronous
    requests. If our use case only requires batch prediction, we might not need to
    deploy the model to an endpoint as the Vertex AI `batchprediciton` service also
    allows us to perform batch prediction from a saved model that is present in a
    GCS location without even needing to create an endpoint. Batch predictions are
    suitable for use cases where the response is not time sensitive and we can afford
    to get a delayed response (for example, an e-commerce company may wish to forecast
    sales for the next six months or so). Using batch predictions, we can make predictions
    of a large amount of data with just a single request.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何在 Vertex AI 上从我们的机器学习模型中获取预测。根据用例，预测请求可以是两种类型之一——在线预测（实时）和批量预测。在线预测是对模型端点发出的同步请求。在线预测是那些需要通过
    API 调用及时请求输出以更新最终用户信息的应用程序所需要的。例如，Google Maps API 提供了近实时的交通更新，并需要在线预测请求。另一方面，批量预测是异步请求。如果我们的用例只需要批量预测，我们可能不需要将模型部署到端点，因为
    Vertex AI 的 `batchprediction` 服务也允许我们从 GCS 位置中的保存模型执行批量预测，甚至不需要创建端点。批量预测适用于那些对响应时间不敏感且可以接受延迟响应的用例（例如，电子商务公司可能希望预测未来六个月左右的销售额）。使用批量预测，我们可以通过单个请求对大量数据进行预测。
- en: Getting online predictions
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取在线预测
- en: We must deploy our model to an endpoint before that model can be used to serve
    online prediction requests. Model deployment essentially means keeping the model
    in memory with the required infrastructure (memory and compute) so that it can
    serve predictions with low latency. We can deploy multiple models to a single
    endpoint as well as a single model to multiple endpoints based on the use case
    and scaling requirements.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型可以被用来处理在线预测请求之前，我们必须将其部署到端点。模型部署本质上意味着将模型及其所需的基础设施（内存和计算）保持在内存中，以便能够以低延迟提供预测。根据用例和扩展需求，我们也可以将多个模型部署到单个端点，或者将单个模型部署到多个端点。
- en: 'When you deploy a model using the Vertex AI API, you complete the following
    steps:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用 Vertex AI API 部署模型时，你需要完成以下步骤：
- en: Create an endpoint.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建端点。
- en: Get the endpoint ID.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取端点 ID。
- en: Deploy the model to the endpoint.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型部署到端点。
- en: 'We can use the following Python sample function to create a Vertex AI endpoint.
    This function is taken from official documentation ([https://cloud.google.com/vertex-ai/docs/general/deployment#api](https://cloud.google.com/vertex-ai/docs/general/deployment#api)):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下 Python 示例函数来创建 Vertex AI 端点。这个函数取自官方文档（[https://cloud.google.com/vertex-ai/docs/general/deployment#api](https://cloud.google.com/vertex-ai/docs/general/deployment#api)）：
- en: '[PRE33]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The second step is to get the endpoint ID so that we can use it to deploy our
    model. The following shell command will give us a list of all the endpoints within
    our project and location. We can filter it with the endpoint name if we have it:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是获取端点 ID，这样我们就可以用它来部署我们的模型。以下 shell 命令将给出我们项目位置内所有端点的列表。如果我们有端点名称，我们可以通过端点名称进行过滤：
- en: '[PRE34]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now that we have the endpoint ID, we can deploy our model to this endpoint.
    While deploying the model, we can specify parameters for a number of replicas,
    the accelerator count, accelerator types, and so on. The following is a sample
    Python function that can be used to deploy the model to a given endpoint. This
    sample has been taken from the Google Cloud documentation ([https://cloud.google.com/vertex-ai/docs/general/deployment#api](https://cloud.google.com/vertex-ai/docs/general/deployment#api)):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得了端点 ID，我们可以将我们的模型部署到这个端点。在部署模型的过程中，我们可以指定多个副本的参数、加速器数量、加速器类型等。以下是一个示例
    Python 函数，可以用来将模型部署到指定的端点。这个示例取自 Google Cloud 文档（[https://cloud.google.com/vertex-ai/docs/general/deployment#api](https://cloud.google.com/vertex-ai/docs/general/deployment#api)）：
- en: '[PRE35]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here, we initialize the Vertex AI SDK and deploy our model to an endpoint:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们初始化 Vertex AI SDK 并将我们的模型部署到端点：
- en: '[PRE36]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Once our model is deployed to an endpoint, it is ready to serve online predictions.
    We can now make online prediction requests to this endpoint. See the following
    sample request:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的模型部署到端点，它就准备好提供在线预测。现在我们可以向此端点发出在线预测请求。请参见以下示例请求：
- en: '[PRE37]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The `instances[]` object is required and must contain the list of instances
    to get predictions for. See the following example:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`instances[]`对象是必需的，并且必须包含要获取预测的实例列表。请参见以下示例：'
- en: '[PRE38]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The response body is also similar. It may look something like the following
    example. This example is not related to the earlier model; it is just for understanding
    purposes:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 响应体也类似。它可能看起来像以下示例。此示例与先前的模型无关；只是为了理解目的：
- en: '[PRE39]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The response when there is an error in processing the input looks as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理输入时出现错误时的响应如下：
- en: '[PRE40]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We now have a good idea of how to get online predictions using Vertex AI endpoints.
    But not every use case requires on-demand or online predictions. There are times
    when we want to make predictions on a large amount of data but the results are
    not immediately required. In such cases, we can utilize batch predictions. Let’s
    discuss more about getting batch predictions using Vertex AI.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对如何使用Vertex AI端点获取在线预测有了很好的了解。但并非每个用例都需要按需或在线预测。有时我们想要对大量数据进行预测，但结果并不需要立即得到。在这种情况下，我们可以利用批量预测。让我们进一步讨论如何使用Vertex
    AI获取批量预测。
- en: Getting batch predictions
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取批量预测
- en: 'As discussed before, batch prediction requests are asynchronous and do not
    require a model to be deployed to an endpoint all the time. To make a batch prediction
    request, we specify an input source and an output location (either Cloud Storage
    or BigQuery), where Vertex AI stores prediction results. The input source location
    must contain our input instances in one of the accepted formats: TFRecord, JSON
    Lines, CSV, BigQuery, and so on.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，批量预测请求是异步的，不需要将模型始终部署到端点。要发出批量预测请求，我们指定一个输入源和一个输出位置（可以是云存储或BigQuery），其中Vertex
    AI存储预测结果。输入源位置必须包含我们的输入实例，格式之一为：TFRecord、JSON Lines、CSV、BigQuery等。
- en: 'TFRecord input instances may look something like the following example:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: TFRecord输入实例可能看起来像以下示例：
- en: '[PRE41]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Batch prediction can be requested through Vertex AI API programatically or also
    with Google Cloud console UI. As we can pass lots of data to batch prediction
    requests, they may take a long time to complete depending upon the size of data
    and model.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过Vertex AI API编程方式或使用Google Cloud控制台UI请求批量预测。由于我们可以向批量预测请求传递大量数据，因此它们可能需要很长时间才能完成，具体取决于数据量和模型大小。
- en: 'A sample batch prediction request using the Vertex AI API with Python may look
    something like the following Python function. This sample code has been taken
    from the official documentation ([https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions)):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python的Vertex AI API的一个示例批量预测请求可能看起来像以下Python函数。此示例代码取自官方文档（[https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions)）：
- en: '[PRE42]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Here, we initialize the Vertex AI SDK and call batch predictions on our deployed
    model:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们初始化Vertex AI SDK，并在我们的部署模型上调用批量预测：
- en: '[PRE43]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Once the batch prediction request is complete, the output is saved in the specified
    Cloud Storage or BigQuery location.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦批量预测请求完成，输出将保存在指定的云存储或BigQuery位置。
- en: 'A `jsonl` output file might look something like the following example output:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`jsonl`输出文件可能看起来像以下示例输出：
- en: '[PRE44]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We now have a fair idea of how online and batch prediction work on Vertex AI.
    The idea of separating batch prediction from online prediction (eliminating the
    need for deployment) saves a lot of resources and costs. Next, let’s discuss some
    important considerations related to deployed models on Google Vertex AI.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对Vertex AI上的在线和批量预测工作原理有了相当的了解。将批量预测与在线预测（消除部署需求）分开的想法可以节省大量的资源和成本。接下来，让我们讨论一些与Google
    Vertex AI上部署的模型相关的重要考虑因素。
- en: Managing deployed models on Vertex AI
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理Vertex AI上的部署模型
- en: When we deploy an ML model to an endpoint, we associate it with physical resources
    (compute) so that it can serve online predictions at low latency. Depending on
    the requirements, we might want to deploy multiple models to a single endpoint
    or a single model to multiple endpoints as well. Let’s learn about these two scenarios.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将机器学习模型部署到端点时，我们将它与物理资源（计算）关联起来，以便它可以在低延迟下提供在线预测。根据需求，我们可能希望将多个模型部署到单个端点，或者将单个模型部署到多个端点。让我们了解这两种场景。
- en: Multiple models – single endpoint
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多个模型 - 单个端点
- en: Suppose we already have one model deployed to an endpoint in production and
    we have found some interesting ideas to improve that model. Now, suppose we have
    already trained an improved model that we want to deploy but we also don’t want
    to make any sudden changes to our application. In this situation, we can add our
    latest model to the existing endpoint and start serving a very small percentage
    of traffic with the new model. If everything looks great, we can gradually increase
    the traffic until it is serving the full 100% of the traffic.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经在生产环境中将一个模型部署到端点，并且我们已经找到了一些改进该模型的好主意。现在，假设我们已经训练了一个改进的模型，我们想要部署，但我们也不希望对我们的应用程序进行任何突然的更改。在这种情况下，我们可以将我们的最新模型添加到现有端点，并开始用新模型服务非常小的流量百分比。如果一切看起来都很完美，我们可以逐渐增加流量，直到它服务全部
    100% 的流量。
- en: Single model – multiple endpoints
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单个模型 - 多个端点
- en: This is useful when we want to deploy our model with different resources for
    different application environments, such as testing and production. Secondly,
    if one of our applications has high-performance needs, we can serve it using an
    endpoint with high-performance machines, while we can serve other applications
    with lower-performance machines to optimize operationalization costs.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要为不同的应用环境（如测试和生产）部署具有不同资源的模型时，这很有用。其次，如果我们的某个应用程序有高性能需求，我们可以使用高性能机器的端点来提供服务，而我们可以使用低性能机器为其他应用程序提供服务，以优化运营成本。
- en: Compute resources and scaling
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算资源和扩展
- en: Vertex AI allocates compute nodes to handle online and batch predictions. When
    we deploy our ML model to an endpoint, we can customize the type of virtual machines
    to be used for serving the model. We can choose accelerators such as GPUs or TPUs
    if needed. A machine configuration with more computing resources can serve predictions
    with lower latency, hence handling more prediction requests at the same time.
    But such a machine will cost more than a machine with low compute resources. Thus,
    it is important to choose the best-suited machine depending on the use case and
    requirements.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 会分配计算节点来处理在线和批量预测。当我们部署我们的机器学习模型到端点时，我们可以自定义用于服务模型的虚拟机类型。如果需要，我们可以选择使用
    GPU 或 TPUs 等加速器。具有更多计算资源的机器配置可以以更低的延迟处理预测，因此可以同时处理更多的预测请求。但是，这样的机器将比计算资源较低的机器成本更高。因此，根据用例和需求选择最合适的机器非常重要。
- en: When we deploy a model for online predictions, we can also configure a prediction
    node to automatically scale. But the prediction nodes for batch prediction do
    not automatically scale. By default, if we deploy a model with or without dedicated
    GPU resources, Vertex AI will automatically scale the number of replicas up or
    down so that CPU or GPU usage (whichever is higher) matches the default 60% target
    value. Given these conditions, Vertex AI will scale up, even if this may not have
    been needed to achieve **queries per second** (**QPS**) and latency targets. We
    can monitor the endpoint to track metrics such as CPU and accelerator usage, the
    number of requests, and latency, as well as the current and target number of replicas.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们部署用于在线预测的模型时，我们还可以配置一个预测节点以自动扩展。但是，批量预测的预测节点不会自动扩展。默认情况下，如果我们部署带有或没有专用 GPU
    资源的模型，Vertex AI 将自动调整副本的数量，以便 CPU 或 GPU 使用率（以较高的为准）匹配默认的 60% 目标值。在这些条件下，Vertex
    AI 将进行扩展，即使这可能不是达到每秒查询数（**QPS**）和延迟目标所必需的。我们可以监控端点以跟踪 CPU 和加速器使用率、请求数量、延迟以及当前和目标副本数量等指标。
- en: To determine the ideal machine type for a prediction container from a cost perspective,
    we can deploy it to a virtual machine instance and benchmark the instance by making
    prediction requests until the virtual machine hits about 90% of the CPU usage.
    By doing this experiment a few times on different machines, we can identify the
    cost of the prediction service based on the QPS values.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从成本角度确定预测容器的理想机器类型，我们可以将其部署到虚拟机实例上，并通过发送预测请求来基准测试实例，直到虚拟机达到大约90%的CPU使用率。通过在多个不同的机器上重复进行这个实验，我们可以根据QPS值确定预测服务的成本。
- en: Summary
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have learned about two popular ML workflow orchestration
    tools – Vertex AI Pipelines (managed Kubeflow) and Cloud Composer (managed Airflow).
    We have also implemented a Vertex Pipeline for an example use case, and similarly,
    we have also developed and executed an example DAG with Cloud Composer. Both Vertex
    AI Pipelines and Cloud Composer are managed services on GCP and make it really
    easy to set up and launch complex ML and data-related workflows. Finally, we have
    learned about getting online and batch predictions on Vertex AI for our custom
    models, including some best practices related to model deployments.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了两种流行的机器学习工作流程编排工具——Vertex AI Pipelines（托管Kubeflow）和Cloud Composer（托管Airflow）。我们还为示例用例实现了一个Vertex
    Pipeline，并且类似地，我们也使用Cloud Composer开发并执行了一个示例DAG。Vertex AI Pipelines和Cloud Composer都是GCP上的托管服务，使得设置和启动复杂的机器学习和数据相关的工作流程变得非常容易。最后，我们学习了在Vertex
    AI上为我们的自定义模型进行在线和批量预测，包括一些与模型部署相关的最佳实践。
- en: After reading this chapter, you should have a good understanding of different
    ways of carrying out ML workflow orchestration on GCP and their similarities and
    differences. Now, you should be able to write your own ML workflows and orchestrate
    them on GCP via either Vertex AI Pipelines or Cloud Composer. Finally, you should
    also be confident in getting online and batch predictions using Vertex AI.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章后，你应该对在GCP上执行机器学习工作流程编排的不同方式及其相似之处和不同之处有一个很好的理解。现在，你应该能够编写自己的机器学习工作流程，并通过Vertex
    AI Pipelines或Cloud Composer在GCP上编排它们。最后，你应该对使用Vertex AI进行在线和批量预测也充满信心。
- en: Now that we have a good understanding of deploying ML models on GCP, and also
    orchestrating ML workflows, we can start developing production-grade pipelines
    for different use cases. Along similar lines, we will learn about some ML governance
    best practices and tools in the upcoming chapter.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对在GCP上部署机器学习模型以及编排机器学习工作流程有了很好的理解，我们可以开始为不同的用例开发生产级别的管道。沿着相似的方向，我们将在下一章学习一些机器学习治理的最佳实践和工具。
