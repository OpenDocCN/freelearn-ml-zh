- en: Chapter 9. Learning Object Tracking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章. 学习目标跟踪
- en: In the previous chapter, we learned about video surveillance, background modeling,
    and morphological image processing. We discussed how we can use different morphological
    operators to apply cool visual effects to input images. In this chapter, we will
    learn how to track an object in a live video. We will discuss the different characteristics
    of an object that can be used to track it. We will also learn about different
    methods and techniques used for object tracking. Object tracking is used extensively
    in robotics, self-driving cars, vehicle tracking, player tracking in sports, video
    compression, and so on.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了视频监控、背景建模和形态学图像处理。我们讨论了如何使用不同的形态学算子将酷炫的视觉效果应用到输入图像上。在本章中，我们将学习如何在实时视频中跟踪一个物体。我们将讨论可用于跟踪物体的不同物体特征。我们还将了解用于物体跟踪的不同方法和技巧。物体跟踪在机器人技术、自动驾驶汽车、车辆跟踪、体育中的运动员跟踪、视频压缩等领域得到了广泛的应用。
- en: 'By the end of this chapter, you will learn:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将学习：
- en: How to track colored objects
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何跟踪彩色物体
- en: How to build an interactive object tracker
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建一个交互式目标跟踪器
- en: What is a corner detector
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是角点检测器
- en: How to detect good features to track
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何检测用于跟踪的良好特征
- en: How to build an optical flow-based feature tracker
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建基于光流的特征跟踪器
- en: Tracking objects of a specific color
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪特定颜色的物体
- en: In order to build a good object tracker, we need to understand what characteristics
    can be used to make our tracking robust and accurate. So, let's take a baby step
    in this direction, and see how we can use colorspaces to come up with a good visual
    tracker. One thing to keep in mind is that the color information is sensitive
    to lighting conditions. In real-world applications, you need to do some preprocessing
    to take care of this. But for now, let's assume that somebody else is doing this
    and we are getting clean color images.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个良好的目标跟踪器，我们需要了解哪些特征可以用来使我们的跟踪既稳健又准确。因此，让我们迈出小小的一步，看看我们如何利用色彩空间来设计一个良好的视觉跟踪器。有一点需要记住的是，色彩信息对光照条件很敏感。在实际应用中，你需要进行一些预处理来处理这个问题。但就目前而言，让我们假设有人在做这件事，而我们正在获取干净的彩色图像。
- en: There are many different colorspaces and picking up a good one will depend on
    what people use for different applications. While RGB is the native representation
    on the computer screen, it's not necessarily ideal for humans. When it comes to
    humans, we give names to colors that are based on their hue. This is why **HSV**
    (**Hue Saturation Value**) is probably one of the most informative colorspaces.
    It closely aligns with how we perceive colors. Hue refers to the color spectrum,
    saturation refers to the intensity of a particular color, and value refers to
    the brightness of that pixel. This is actually represented in a cylindrical format.
    You can refer to a simple explanation about this at [http://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html](http://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html).
    We can take the pixels of an image to the HSV space and then use colorspace distances
    and threshold in this space thresholding to track a given object.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多不同的色彩空间，选择一个好的取决于人们在不同应用中的使用。虽然RGB是计算机屏幕上的原生表示，但它对于人类来说并不一定是理想的。当涉及到人类时，我们给基于色调的颜色命名。这就是为什么**HSV**（**色调饱和度值**）可能是最有信息量的色彩空间之一。它与我们的颜色感知非常接近。色调指的是颜色光谱，饱和度指的是特定颜色的强度，而值指的是该像素的亮度。这实际上是以圆柱格式表示的。你可以参考关于这个的简单解释[http://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html](http://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html)。我们可以将图像的像素转换到HSV空间，然后使用该空间中的色彩空间距离和阈值来进行阈值处理以跟踪特定物体。
- en: 'Consider the following frame in the video:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑视频中的以下帧：
- en: '![Tracking objects of a specific color](img/B04283_09_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![跟踪特定颜色的物体](img/B04283_09_01.jpg)'
- en: 'If you run it through the colorspace filter and track the object, you will
    see something like this:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过色彩空间过滤器运行它并跟踪物体，你会看到类似这样的东西：
- en: '![Tracking objects of a specific color](img/B04283_09_02.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![跟踪特定颜色的物体](img/B04283_09_02.jpg)'
- en: 'As you can see here, our tracker recognizes a particular object in the video
    based on its color characteristics. In order to use this tracker, we need to know
    the color distribution of our target object. The following code is used to track
    a colored object that selects only pixels that have a certain given hue. The code
    is well commented, so read the explanation mentioned previously for each line
    to see what''s happening:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在此处所见，我们的跟踪器根据视频中的颜色特征识别特定对象。为了使用此跟踪器，我们需要知道目标对象的颜色分布。以下代码用于跟踪一个只选择具有特定给定色调的像素的彩色对象。代码注释详尽，因此请阅读之前提到的每行解释以了解发生了什么：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Building an interactive object tracker
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建交互式对象跟踪器
- en: A colorspace-based tracker gives us the freedom to track a colored object, but
    we are also constrained to a predefined color. What if we just want to randomly
    pick an object? How do we build an object tracker that can learn the characteristics
    of the selected object and track it automatically? This is where the CAMShift
    algorithm, which stands for **Continuously Adaptive Meanshift**, comes into the
    picture. It's basically an improved version of the Meanshift algorithm.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于颜色空间的跟踪器为我们提供了跟踪彩色对象的自由，但我们也被限制在预定义的颜色上。如果我们只想随机选择一个对象怎么办？我们如何构建一个可以学习所选对象特征并自动跟踪它的对象跟踪器？这就是CAMShift算法出现的地方，它代表**连续自适应均值漂移**。它基本上是Meanshift算法的改进版本。
- en: The concept of Meanshift is actually nice and simple. Let's say we select a
    region of interest, and we want our object tracker to track that object. In this
    region, we select a bunch of points based on the color histogram and compute the
    centroid of spatial points. If the centroid lies at the center of this region,
    we know that the object hasn't moved. But if the centroid is not at the center
    of this region, then we know that the object is moving in some direction. The
    movement of the centroid controls the direction in which the object is moving.
    So, we move the bounding box of the object to a new location so that the new centroid
    becomes the center of this bounding box. Hence, this algorithm is called Meanshift
    because the mean (that is, the centroid) is shifting. This way, we keep ourselves
    updated with the current location of the object.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Meanshift的概念实际上很好且简单。假设我们选择一个感兴趣的区域，并希望我们的对象跟踪器跟踪该对象。在这个区域中，我们根据颜色直方图选择一些点，并计算空间点的质心。如果质心位于该区域的中心，我们知道对象没有移动。但如果质心不在该区域的中心，那么我们知道对象正在某个方向上移动。质心的移动控制着对象移动的方向。因此，我们将对象的边界框移动到新的位置，使新的质心成为边界框的中心。因此，这个算法被称为Meanshift，因为均值（即质心）在移动。这样，我们就能保持对对象当前位置的了解。
- en: However, the problem with Meanshift is that the size of the bounding box is
    not allowed to change. When you move the object away from the camera, the object
    will appear smaller to the human eye, but Meanshift will not take this into account.
    The size of the bounding box will remain the same throughout the tracking session.
    Hence, we need to use CAMShift. The advantage of CAMShift is that it can adapt
    the size of the bounding box to the size of the object. Along with this, it can
    also keep track of the orientation of the object.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Meanshift的问题在于边界框的大小不允许改变。当你将对象从摄像机移开时，人眼会看到对象变得更小，但Meanshift不会考虑这一点。在整个跟踪过程中，边界框的大小将保持不变。因此，我们需要使用CAMShift。CAMShift的优势在于它可以调整边界框的大小以适应对象的大小。此外，它还可以跟踪对象的方向。
- en: 'Let''s consider the following figure in which the object is highlighted:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下图中被突出显示的对象：
- en: '![Building an interactive object tracker](img/B04283_09_03.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![构建交互式对象跟踪器](img/B04283_09_03.jpg)'
- en: 'Now that we have selected the object, the algorithm computes the histogram
    backprojection and extracts all the information. What is histogram backprojection?
    It''s just a way of identifying how well the image fits into our histogram model.
    We compute the histogram model of a particular thing, and then use this model
    to find that thing in an image. Let''s move the object and see how it gets tracked:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经选择了对象，算法计算直方图反向投影并提取所有信息。什么是直方图反向投影？它只是识别图像如何适合我们的直方图模型的一种方法。我们计算特定事物的直方图模型，然后使用此模型在图像中找到该事物。让我们移动对象并看看它是如何被跟踪的：
- en: '![Building an interactive object tracker](img/B04283_09_04.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![构建交互式对象跟踪器](img/B04283_09_04.jpg)'
- en: 'Looks like the object is getting tracked fairly well. Let''s change the orientation,
    and check whether the tracking is maintained:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来物体被跟踪得相当好。让我们改变方向，并检查跟踪是否保持：
- en: '![Building an interactive object tracker](img/B04283_09_05.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![构建交互式对象跟踪器](img/B04283_09_05.jpg)'
- en: 'As you can see, the bounding ellipse has changed its location as well as its
    orientation. Let''s change the perspective of the object, and see whether it''s
    still able to track it:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，边界椭圆已经改变了其位置和方向。让我们改变物体的视角，看看它是否仍然能够跟踪它：
- en: '![Building an interactive object tracker](img/B04283_09_06.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![构建交互式对象跟踪器](img/B04283_09_06.jpg)'
- en: 'We are still good! The bounding ellipse has changed the aspect ratio to reflect
    the fact that the object looks skewed now (because of the perspective transformation).
    Let''s take a look at the user interface functionality in the following code:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然做得很好！边界椭圆已经改变了宽高比，以反映物体现在看起来是倾斜的（由于透视变换）。让我们看看以下代码中的用户界面功能：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This function basically captures the coordinates of the rectangle that were
    selected in the window. The user just needs to click on them and drag them with
    the mouse. There are a set of inbuilt functions in OpenCV that help us detect
    these different mouse events.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数基本上捕获了在窗口中选定的矩形的坐标。用户只需点击它们并用鼠标拖动即可。OpenCV中有一系列内置函数帮助我们检测这些不同的鼠标事件。
- en: 'Here is the code used to perform object tracking based on CAMShift:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是用于基于CAMShift进行对象跟踪的代码：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We now have the HSV image waiting to be processed at this point. Let''s go
    ahead and see how we can use our thresholds to process this image:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经有了一个等待处理的HSV图像。让我们继续看看我们如何使用我们的阈值来处理这个图像：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As you can see here, we use the HSV image to compute the histogram of the region.
    We use our thresholds to locate the required color in the HSV spectrum and then
    filter out the image based on that. Let''s go ahead and see how we can compute
    the histogram backprojection:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在此处所见，我们使用HSV图像来计算区域的直方图。我们使用我们的阈值在HSV光谱中定位所需颜色，然后根据该颜色过滤图像。让我们继续看看我们如何计算直方图反向投影：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We are now ready to display the results. Using the rotated rectangle, let''s
    draw an ellipse around our region of interest:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好显示结果。使用旋转矩形，让我们在我们的感兴趣区域周围画一个椭圆：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Detecting points using the Harris corner detector
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Harris角点检测器检测点
- en: Corner detection is a technique used to detect *interest points* in the image.
    These interest points are also called *feature points* or simply *features* in
    Computer Vision terminology. A corner is basically an intersection of two edges.
    An *interest point* is basically something that can be uniquely detected in an
    image. A corner is a particular case of an interest point. These interest points
    help us characterize an image. These points are used extensively in applications,
    such as object tracking, image classification, visual search, and so on. Since
    we know that the corners are *interesting*, let's see how we can detect them.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 角点检测是一种用于在图像中检测*兴趣点*的技术。这些兴趣点在计算机视觉术语中也被称为*特征点*或简单地称为*特征*。一个角基本上是两条边的交点。一个*兴趣点*基本上是在图像中可以唯一检测到的东西。一个角是兴趣点的特例。这些兴趣点帮助我们描述图像。这些点在许多应用中得到了广泛的应用，例如对象跟踪、图像分类、视觉搜索等等。既然我们知道角是*有趣的*，让我们看看我们如何检测它们。
- en: In Computer Vision, there is a popular corner detection technique called the
    Harris corner detector. We construct a 2 x 2 matrix based on partial derivatives
    of the grayscale image, and then analyze the eigenvalues. Now what does this mean?
    Well, let's dissect it so that we can understand it better. Let's consider a small
    patch in the image. Our goal is to check whether this patch has a corner in it.
    So, we consider all the neighboring patches and compute the intensity difference
    between our patch and all those neighboring patches. If the difference is high
    in all directions, then we know that our patch has a corner in it. This is actually
    an oversimplification of the actual algorithm, but it covers the gist. If you
    want to understand the underlying mathematical details, you can take a look at
    the original paper by Harris and Stephens at [http://www.bmva.org/bmvc/1988/avc-88-023.pdf](http://www.bmva.org/bmvc/1988/avc-88-023.pdf).
    A corner point is a point where both the eigenvalues would have large values.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，有一个流行的角点检测技术称为 Harris 角点检测器。我们根据灰度图像的偏导数构建一个 2x2 矩阵，然后分析特征值。那么这究竟意味着什么呢？好吧，让我们来剖析一下，以便我们更好地理解它。让我们考虑图像中的一个小的区域。我们的目标是检查这个区域中是否有角点。因此，我们考虑所有相邻的区域，并计算我们的区域与所有这些相邻区域之间的强度差异。如果在所有方向上差异都很大，那么我们知道我们的区域中有一个角点。这实际上是实际算法的一个过度简化，但它涵盖了核心内容。如果你想要了解背后的数学细节，你可以查看
    Harris 和 Stephens 的原始论文，链接为 [http://www.bmva.org/bmvc/1988/avc-88-023.pdf](http://www.bmva.org/bmvc/1988/avc-88-023.pdf)。角点是一个特征值都会有大值的点。
- en: 'If we run the Harris corner detector, it will look like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行 Harris 角点检测器，它看起来会是这样：
- en: '![Detecting points using the Harris corner detector](img/B04283_09_07.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Harris 角点检测器检测点](img/B04283_09_07.jpg)'
- en: 'As you can see, the green circles on the TV remote are the detected corners.
    This will change based on the parameters you choose for the detector. If you modify
    the parameters, you can see that more points might get detected. If you make it
    strict, then you might not be able to detect soft corners. Let''s take a look
    at the following code to detect Harris corners:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，电视遥控器上的绿色圆圈是检测到的角点。这会根据你为检测器选择的参数而改变。如果你修改参数，你可能会看到检测到更多的点。如果你让它更严格，那么你可能无法检测到软角点。让我们看一下以下代码来检测
    Harris 角点：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We converted the image to grayscale and detected corners using our parameters.
    You can find the complete code in the `.cpp` files. These parameters play an important
    role in the number of points that will be detected. You can check out the OpenCV
    documentation of the Harris corner detector at [http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=cornerharris#void
    cornerHarris(InputArray src, OutputArray dst, int blockSize, int ksize, double
    k, int borderType](http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=cornerharris#voidcornerHarris%28InputArraysrc,OutputArraydst,intblockSize,intksize,doublek,intborderType%29)).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将图像转换为灰度，并使用我们的参数检测角点。你可以在 `.cpp` 文件中找到完整的代码。这些参数在将要检测到的点的数量中起着重要作用。你可以在 OpenCV
    的 Harris 角点检测器文档中查看详细信息，链接为 [http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=cornerharris#void
    cornerHarris(InputArray src, OutputArray dst, int blockSize, int ksize, double
    k, int borderType](http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=cornerharris#voidcornerHarris%28InputArraysrc,OutputArraydst,intblockSize,intksize,doublek,intborderType%29))。
- en: 'We now have all the information that we need. Let''s go ahead and draw circles
    around our corners to display the results:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了所有需要的信息。让我们继续在角点周围画圆圈以显示结果：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see, this code takes a `blockSize` input argument. Depending on the
    size you choose, the performance will vary. Start with a value of `4` and play
    around with it to see what happens.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这段代码接受一个 `blockSize` 输入参数。根据你选择的大小，性能会有所不同。从 `4` 开始，尝试不同的值以查看会发生什么。
- en: Shi-Tomasi Corner Detector
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Shi-Tomasi 角点检测器
- en: 'The Harris corner detector performs well in many cases, but it can still be
    improved. Around six years after the original paper by Harris and Stephens, Shi-Tomasi
    came up with something better and they called it *Good Features To Track*. You
    can read the original paper at: [http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf](http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf).
    They used a different scoring function to improve the overall quality. Using this
    method, we can find the *N* strongest corners in the given image. This is very
    useful when we don''t want to use every single corner to extract information from
    the image. As discussed earlier, a good interest point detector is very useful
    in applications, such as object tracking, object recognition, image search, and
    so on.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Harris角点检测器在许多情况下表现良好，但仍有改进空间。在Harris和Stephens的原始论文发表后的六年左右，Shi-Tomasi提出了一种更好的方法，他们称之为*Good
    Features To Track*。您可以在[http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf](http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf)阅读原始论文。他们使用不同的评分函数来提高整体质量。使用这种方法，我们可以在给定的图像中找到*N*个最强的角点。当我们不想使用图像中的每一个角点来提取信息时，这非常有用。如前所述，一个好的兴趣点检测器在诸如对象跟踪、对象识别、图像搜索等应用中非常有用。
- en: 'If you apply the Shi-Tomasi corner detector to an image, you will see something
    like this:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将Shi-Tomasi角点检测器应用于图像，您将看到类似这样的效果：
- en: '![Shi-Tomasi Corner Detector](img/B04283_09_08.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![Shi-Tomasi角点检测器](img/B04283_09_08.jpg)'
- en: 'As you can see here, all the important points in the frame are captured. Let''s
    take a look at the following code to track these features:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，帧中的所有重要点都被捕捉到了。让我们看一下以下代码来跟踪这些特征：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We extracted the frame and used `goodFeaturesToTrack` to detect the corners.
    It''s important to understand that the number of corners detected will depend
    on our choice of parameters. You can find a detailed explanation at [http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=goodfeaturestotrack#goodfeaturestotrack](http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=goodfeaturestotrack#goodfeaturestotrack).
    Let''s go ahead and draw circles on these points to display the output image:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提取了帧并使用`goodFeaturesToTrack`来检测角点。重要的是要理解检测到的角点数量将取决于我们选择的参数。您可以在[http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=goodfeaturestotrack#goodfeaturestotrack](http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=goodfeaturestotrack#goodfeaturestotrack)找到详细的解释。让我们继续在这些点上画圆圈以显示输出图像：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This program takes a `numCorners` input argument. This value indicates the maximum
    number of corners you want to track. Start with a value of `100` and play around
    with it to see what happens. If you increase this value, you will see more feature
    points getting detected.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此程序接受一个`numCorners`输入参数。此值表示您想要跟踪的最大角点数。从`100`开始，尝试调整这个值以观察会发生什么。如果您增加这个值，您将看到更多特征点被检测到。
- en: Feature-based tracking
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于特征的跟踪
- en: Feature-based tracking refers to tracking individual feature points across successive
    frames in the video. The advantage here is that we don't have to detect feature
    points in every single frame. We can just detect them once and keep tracking them
    after that. This is more efficient as compared to running the detector on every
    frame. We use a technique called optical flow to track these features. Optical
    flow is one of the most popular techniques in Computer Vision. We choose a bunch
    of feature points, and track them through the video stream. When we detect the
    feature points, we compute the displacement vectors and show the motion of those
    keypoints between consecutive frames. These vectors are called motion vectors.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 基于特征的跟踪指的是在视频的连续帧中跟踪单个特征点。这里的优势是我们不需要在每一帧中检测特征点。我们只需检测一次，然后继续跟踪。与在每一帧上运行检测器相比，这更有效率。我们使用一种称为光流的技术来跟踪这些特征。光流是计算机视觉中最流行的技术之一。我们选择一些特征点，并通过视频流跟踪它们。当我们检测到特征点时，我们计算位移矢量并显示这些关键点在连续帧之间的运动。这些矢量被称为运动矢量。
- en: A motion vector for a particular point is just a directional line that indicates
    where that point has moved as compared to the previous frame. Different methods
    are used to detect these motion vectors. The two most popular algorithms are the
    Lucas-Kanade method and Farneback algorithm.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 特定点的运动矢量只是一个指示该点相对于前一帧移动方向的线条。不同的方法被用来检测这些运动矢量。最流行的两种算法是Lucas-Kanade方法和Farneback算法。
- en: The Lucas-Kanade method
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lucas-Kanade方法
- en: The Lucas-Kanade method is used for sparse optical flow tracking. By sparse,
    we mean that the number of feature points is relatively low. You can refer to
    their original paper at [http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf](http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf).
    We start the process by extracting the feature points. For each feature point,
    we create 3 x 3 patches with the feature point at the center. We assume that all
    the points within each patch will have a similar motion. We can adjust the size
    of this window, depending on the problem at hand.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Lucas-Kanade 方法用于稀疏光流跟踪。通过稀疏，我们指的是特征点的数量相对较低。你可以参考他们的原始论文[http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf](http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf)。我们通过提取特征点开始这个过程。对于每个特征点，我们以特征点为中心创建
    3 x 3 的面。我们假设每个面内的所有点将具有相似的运动。我们可以根据手头的问题调整这个窗口的大小。
- en: For each feature point in the current frame, we take the surrounding 3 x 3 patch
    as our reference point. For this patch, we take a look at its neighborhood in
    the previous frame to get the best match. This neighborhood is usually bigger
    than 3 x 3 because we want to get the patch that's closest to the patch under
    consideration. Now, the path from the center pixel of the matched patch in the
    previous frame to the center pixel of the patch under consideration in the current
    frame will become the motion vector. We do this for all the feature points, and
    extract all the motion vectors.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于当前帧中的每个特征点，我们将其周围的 3 x 3 面积作为参考点。对于这个面，我们查看前一个帧中的邻域以获取最佳匹配。这个邻域通常比 3 x 3 大，因为我们想要获取与考虑中的面最接近的面。现在，从前一个帧中匹配面的中心像素到当前帧中考虑的面的中心像素的路径将成为运动向量。我们对所有特征点都这样做，并提取所有运动向量。
- en: 'Let''s consider the following frame:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下帧：
- en: '![The Lucas-Kanade method](img/B04283_09_09.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![Lucas-Kanade 方法](img/B04283_09_09.jpg)'
- en: 'We need to add some points that we want to track. Just go ahead and click on
    a bunch of points on this window with your mouse:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要添加一些我们想要跟踪的点。只需用鼠标点击这个窗口上的几个点即可：
- en: '![The Lucas-Kanade method](img/B04283_09_10.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![Lucas-Kanade 方法](img/B04283_09_10.jpg)'
- en: 'If I move to a different position, you will see that the points are still being
    tracked correctly within a small margin of error:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我移动到不同的位置，你会看到点仍然在小的误差范围内被正确跟踪：
- en: '![The Lucas-Kanade method](img/B04283_09_11.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![Lucas-Kanade 方法](img/B04283_09_11.jpg)'
- en: 'Let''s add a lot of points and see what happens:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加很多点来看看会发生什么：
- en: '![The Lucas-Kanade method](img/B04283_09_12.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![Lucas-Kanade 方法](img/B04283_09_12.jpg)'
- en: As you can see, it will keep tracking those points. However, you will notice
    that some of the points will be dropped in between because of factors, such as
    prominence, speed of the movement, and so on. If you want to play around with
    it, you can just keep adding more points to it. You can also allow the user to
    select a region of interest in the input video. You can then extract feature points
    from this region of interest and track the object by drawing the bounding box.
    It will be a fun exercise!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它将一直跟踪这些点。然而，你会注意到由于突出度、运动速度等因素，其中一些点会在中间丢失。如果你想玩玩，你可以继续添加更多的点。你也可以允许用户在输入视频中选择感兴趣的区域。然后你可以从这个感兴趣的区域中提取特征点，并通过绘制边界框来跟踪对象。这将是一个有趣的练习！
- en: 'Here is the code used to perform Lucas-Kanade-based tracking:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是用于执行基于 Lucas-Kanade 跟踪的代码：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We use the current image and the previous image to compute the optical flow
    information. Needless to say that the quality of the output will depend on the
    parameters you have chosen. You can find more details about the parameters at
    [http://docs.opencv.org/2.4/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowpyrlk](http://docs.opencv.org/2.4/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowpyrlk).
    To increase the quality and robustness, we need to filter out the points that
    are very close to each other because they do not add the new information. Let''s
    go ahead and do that:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用当前图像和前一个图像来计算光流信息。不用说，输出的质量将取决于你选择的参数。你可以在[http://docs.opencv.org/2.4/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowpyrlk](http://docs.opencv.org/2.4/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowpyrlk)找到更多关于参数的详细信息。为了提高质量和鲁棒性，我们需要过滤掉彼此非常接近的点，因为它们不会添加新的信息。让我们继续这样做：
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We now have the tracking points. The next step is to refine the location of
    these points. What exactly does "refine" mean in this context? To increase the
    speed of computation, there is some level of quantization involved. In layman''s
    terms, you can think of it as "rounding off". Now that we have the approximate
    region, we can refine the location of the point within that region to get a more
    accurate outcome. Let''s go ahead and do this:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了跟踪点。下一步是细化这些点的位置。在这个上下文中，“细化”究竟意味着什么？为了提高计算速度，涉及一定程度的量化。用通俗易懂的话来说，你可以把它想成“四舍五入”。现在我们有了近似区域，我们可以细化点在该区域内的位置，以获得更准确的结果。让我们继续这样做：
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The Farneback algorithm
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Farneback算法
- en: Gunnar Farneback proposed this optical flow algorithm and it's used for dense
    tracking. Dense tracking is used extensively in robotics, augmented reality, 3D
    mapping, and so on. You can check out the original paper at [http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf](http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf).
    The Lucas-Kanade method is a sparse technique, which means that we only need to
    process some pixels in the entire image. The Farneback algorithm, on the other
    hand, is a dense technique that requires us to process all the pixels in the given
    image. So, obviously, there is a trade-off here. Dense techniques are more accurate,
    but they are slower. Sparse techniques are less accurate, but they are faster.
    For real-time applications, people tend to prefer sparse techniques. For applications
    where time and complexity is not a factor, people prefer dense techniques to extract
    finer details.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 冈纳·法尔内巴克提出了这个光流算法，它用于密集跟踪。密集跟踪在机器人技术、增强现实、3D制图等领域被广泛使用。你可以在[http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf](http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf)查看原始论文。Lucas-Kanade方法是一种稀疏技术，这意味着我们只需要处理整个图像中的一些像素。另一方面，Farneback算法是一种密集技术，它要求我们处理给定图像中的所有像素。因此，显然这里有一个权衡。密集技术更准确，但速度较慢。稀疏技术不太准确，但速度较快。对于实时应用，人们倾向于更喜欢稀疏技术。对于时间和复杂度不是因素的场合，人们更喜欢密集技术来提取更精细的细节。
- en: In his paper, Farneback describes a method for dense optical flow estimation
    based on polynomial expansion for two frames. Our goal is to estimate the motion
    between these two frames, and it's basically a three-step process. In the first
    step, each neighborhood in both the frames is approximated by polynomials. In
    this case, we are only interested in quadratic polynomials. The next step is to
    construct a new signal by global displacement. Now that each neighborhood is approximated
    by a polynomial, we need to see what happens if this polynomial undergoes an ideal
    translation. The last step is to compute the global displacement by equating the
    coefficients in the yields of these quadratic polynomials.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的论文中，法尔内巴克描述了一种基于多项式展开的密集光流估计方法，用于两个帧。我们的目标是估计这两个帧之间的运动，这基本上是一个三步过程。在第一步中，两个帧中的每个邻域都通过多项式进行近似。在这种情况下，我们只对二次多项式感兴趣。下一步是通过全局位移构建一个新的信号。现在，每个邻域都通过一个多项式近似，我们需要看看如果这个多项式经历一个理想的平移会发生什么。最后一步是通过将二次多项式的系数相等来计算全局位移。
- en: Now, how is this feasible? If you think about it, we are assuming that an entire
    signal is a single polynomial and there is a global translation relating the two
    signals. This is not a realistic scenario. So, what are we looking for? Well,
    our goal is to find out whether these errors are small enough so that we can build
    a useful algorithm that can track the features.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这如何可行呢？如果你仔细想想，我们假设整个信号是一个单一的多项式，并且存在一个全局平移来关联这两个信号。这不是一个现实的场景。那么，我们在寻找什么呢？好吧，我们的目标是找出这些错误是否足够小，以至于我们可以构建一个有用的算法来跟踪特征。
- en: 'Let''s take a look at the following static image:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看下面的静态图像：
- en: '![The Farneback algorithm](img/B04283_09_13.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![Farneback算法](img/B04283_09_13.jpg)'
- en: 'If I move sideways, you can see that the motion vectors point in the horizontal
    direction. They simply track the movement of my head:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我向侧面移动，你可以看到运动矢量指向水平方向。它们只是跟踪我头部的移动：
- en: '![The Farneback algorithm](img/B04283_09_14.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![Farneback算法](img/B04283_09_14.jpg)'
- en: 'If I move away from the webcam, you can see that the motion vectors point in
    a direction that is perpendicular to the image plane:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我远离摄像头，你可以看到运动矢量指向与图像平面垂直的方向：
- en: '![The Farneback algorithm](img/B04283_09_15.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![Farneback算法](img/B04283_09_15.jpg)'
- en: 'Here is the code used to perform optical flow-based tracking using the Farneback
    algorithm:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用Farneback算法执行基于光流跟踪的代码：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As you can see, we use the Farneback algorithm to compute the optical flow
    vectors. The `calcOpticalFlowFarneback` input parameters are important when it
    comes to the quality of tracking. You can find the details about these parameters
    at [http://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html](http://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html).
    Let''s go ahead and draw those vectors on the output image:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们使用Farneback算法来计算光流向量。`calcOpticalFlowFarneback`的输入参数对于跟踪质量至关重要。您可以在[http://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html](http://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html)找到这些参数的详细信息。让我们继续在输出图像上绘制这些向量：
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We used a function called `drawOpticalFlow` to draw these optical flow vectors.
    These vectors indicate the direction of the motion. Let''s take a look at the
    function to see how we can draw these vectors:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个名为`drawOpticalFlow`的函数来绘制这些光流向量。这些向量指示了运动的方向。让我们看看这个函数，看看我们如何绘制这些向量：
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Summary
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about object tracking. We learned how to use the
    HSV colorspace to track colored objects. We discussed clustering techniques used
    for object tracking and how we can build an interactive object tracker using the
    CAMShift algorithm. We learned about corner detectors and how to track corners
    in a live video. We discussed how to track features in a video using optical flow.
    We also learned the concepts behind Lucas-Kanade and Farneback algorithms and
    implemented them as well.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了目标跟踪。我们学习了如何使用HSV颜色空间来跟踪彩色物体。我们讨论了用于目标跟踪的聚类技术，以及如何使用CAMShift算法构建一个交互式目标跟踪器。我们还学习了角点检测器以及如何在实时视频中跟踪角点。我们讨论了如何使用光流在视频中跟踪特征。我们还学习了Lucas-Kanade和Farneback算法背后的概念，并将它们实现出来。
- en: In the next chapter, we will discuss segmentation algorithms and see how we
    can use them for text recognition.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论分割算法，并了解我们如何将它们用于文本识别。
