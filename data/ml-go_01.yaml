- en: Gathering and Organizing Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集和组织数据
- en: Polls have shown that 90% or more of a data scientist's time is spent gathering
    data, organizing it, and cleaning it, not training/tuning their sophisticated
    machine learning models. Why is this? Isn't the machine learning part the fun
    part? Why do we need to care so much about the state of our data? Firstly, without
    data, our machine learning models can't learn. This might seem obvious. However,
    we need to realize that part of the strength of the models that we build is in
    the data that we feed them. As the common phrase goes, *garbage in, garbage out*.
    We need to make sure that we gather relevant, clean data to power our machine
    learning models, such that they can operate on the data as expected and produce
    valuable results.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 调查显示，90%或更多的数据科学家时间花在收集数据、组织数据和清洗数据上，而不是在训练/调整复杂的机器学习模型上。这是为什么？机器学习部分不是最有意思的部分吗？为什么我们需要如此关注我们数据的状态？首先，没有数据，我们的机器学习模型就无法学习。这看起来可能很明显。然而，我们需要意识到我们构建的模型的部分优势在于我们提供给它们的那些数据。正如常见的说法，“垃圾输入，垃圾输出”。我们需要确保收集相关、干净的数据来为我们的机器学习模型提供动力，这样它们才能按预期操作并产生有价值的结果。
- en: Not all types of data are appropriate when using certain types of models. For
    example, certain models do not perform well when we have high-dimensional data
    (for example, text data), and other models assume that variables are normally
    distributed, which is definitely not always the case. Thus, we must take care
    in gathering data that fits our use case and make sure that we understand how
    our data and models will interact.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有类型的数据都适用于使用某些类型的模型。例如，当我们有高维数据（例如文本数据）时，某些模型的表现不佳，而其他模型则假设变量是正态分布的，这显然并不总是如此。因此，我们必须小心收集适合我们用例的数据，并确保我们理解我们的数据和模型将如何交互。
- en: Another reason why gathering and organizing data consumes so much of a data
    scientist's time is that data is often messy and hard to aggregate. In most organizations,
    data might be housed in various systems and formats, and have various access control
    policies. We can't assume that supplying a training set to our model will be as
    easy as specifying a file path; this is often not the case.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 收集和组织数据消耗了数据科学家大量时间的原因之一是数据通常很混乱且难以聚合。在大多数组织中，数据可能存储在不同的系统和格式中，并具有不同的访问控制策略。我们不能假设向我们的模型提供训练集就像指定一个文件路径那样简单；这通常并非如此。
- en: To form a training/test set or to supply variables to a model for predictions,
    we will likely need to deal with various formats of data, such as CSV, JSON, database
    tables, and so on, and we will likely need to transform individual values. Common
    transformations include parsing date times, converting categorical data to numerical
    data, normalizing values, and applying some function across values. However, we
    can't always assume that all values of a certain variable are present or able
    to be parsed in a similar manner.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了形成训练/测试集或向模型提供预测变量，我们可能需要处理各种数据格式，如CSV、JSON、数据库表等，并且我们可能需要转换单个值。常见的转换包括解析日期时间、将分类数据转换为数值数据、归一化值以及应用一些函数到值上。然而，我们并不能总是假设某个变量的所有值都存在或能够以类似的方式进行解析。
- en: Often data includes missing values, mixed types, or corrupted values. How we
    handle each of these scenarios will directly influence the quality of the models
    that we build, and thus, we have to be willing to carefully gather, organize,
    and understand our data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通常数据中包含缺失值、混合类型或损坏值。我们如何处理这些情况将直接影响我们构建的模型的质量，因此，我们必须愿意仔细收集、组织和理解我们的数据。
- en: Even though much of this book will be focused on various modeling techniques,
    you should always consider data gathering, parsing, and organization as a (or
    maybe the) key component of a successful data science project. If this part of
    your project is not carefully developed with a high level of integrity, you are
    setting yourself up for trouble in the long run.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这本书的大部分内容将专注于各种建模技术，但你应始终将数据收集、解析和组织视为成功数据科学项目的关键组成部分（或可能是最重要的部分）。如果你的项目这部分没有经过精心开发且具有高度诚信，那么你将给自己在长远发展中埋下隐患。
- en: Handling data - Gopher style
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理数据 - Gopher风格
- en: In comparison to many other languages that are used for data science/analysis,
    Go provides a very strong foundation for data manipulation and parsing. Although
    other languages (for example, Python or R) may allow users to quickly explore
    data interactively, they often promote integrity-breaking convenience, that is,
    dynamic and interactive data exploration often results in code that behaves strangely
    when applied more generally.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多用于数据科学/分析的其它语言相比，Go 为数据操作和解析提供了一个非常强大的基础。尽管其他语言（例如 Python 或 R）可能允许用户快速交互式地探索数据，但它们通常促进破坏完整性的便利性，即动态和交互式数据探索通常会导致在更广泛的应用中行为异常的代码。
- en: 'Take, for instance, this simple CSV file:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以这个简单的 CSV 文件为例：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It is true that, very quickly, we can write some Python code to parse this
    CSV and output the maximum value from the integer column without even knowing
    what types are in the data:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 诚然，我们很快就能编写一些 Python 代码来解析这个 CSV 文件，并从整数列中输出最大值，即使我们不知道数据中有什么类型：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This simple program will print the correct result:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的程序将打印出正确的结果：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We now remove one of the integer values to produce a missing value, as shown
    here:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在删除一个整数值以产生一个缺失值，如下所示：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The Python program consequently has a complete breakdown in integrity; specifically,
    the program still runs, doesn''t tell us that anything went differently, still
    produces a value, and produces a value of a different type:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Python 程序因此完全失去了完整性；具体来说，程序仍然运行，没有告诉我们任何事情有所不同，仍然产生了一个值，并且产生了一个不同类型的值：
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is unacceptable. All but one of our integer values could disappear, and
    we wouldn't have any insight into the changes. This could produce profound changes
    in our modeling, but they would be extremely hard to track down. Generally, when
    we opt for the conveniences of dynamic types and abstraction, we are accepting
    this sort of variability in behavior.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这是不可接受的。除了一个整数值外，我们的所有整数值都可能消失，而我们不会对变化有任何洞察。这可能会对我们的建模产生深远的影响，但它们将非常难以追踪。通常，当我们选择动态类型和抽象的便利性时，我们正在接受这种行为的变化性。
- en: The important thing here is not that you cannot handle such behavior in Python,
    because Python, experts will quickly recognize that you can properly handle such
    behavior. The point is that such conveniences do not promote integrity by default,
    and thus, it is very easy to shoot yourself in the foot.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里重要的是，你并不是不能在 Python 中处理这种行为，因为专家会很快认识到你可以正确处理这种行为。关键是这种便利性并不默认促进完整性，因此很容易自食其果。
- en: 'On the other hand, we can leverage Go''s static typing and explicit error handling
    to ensure that our data is parsed as expected. In this small example, we can also
    write some Go code, without too much trouble, to parse our CSV (don''t worry about
    the details right now):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们可以利用 Go 的静态类型和显式错误处理来确保我们的数据以预期的方式被解析。在这个小例子中，我们也可以编写一些 Go 代码，而不会遇到太多麻烦来解析我们的
    CSV（现在不用担心细节）：
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will produce the same correct result for the CSV file with all the integer
    values present:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生一个正确的结果，对于所有整数值都存在的 CSV 文件：
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'But in contrast to our previous Python code, our Go code will inform us when
    we encounter something that we don''t expect in the input CSV (for the case when
    we remove the value 3):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 但与之前的 Python 代码相比，我们的 Go 代码将在我们遇到输入 CSV 中不期望遇到的内容时通知我们（对于删除值 3 的情况）：
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, we have maintained integrity, and we can ensure that we can handle missing
    values in a manner that is appropriate for our use case.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们保持了完整性，并且我们可以确保我们可以以适合我们用例的方式处理缺失值。
- en: Best practices for gathering and organizing data with Go
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Go 收集和组织数据的最佳实践
- en: As you can see in the preceding section, Go itself provides us with an opportunity
    to maintain high levels of integrity in our data gathering, parsing, and organization.
    We want to ensure that we leverage Go's unique properties whenever we are preparing
    our data for machine learning workflows.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述部分所示，Go 本身为我们提供了在数据收集、解析和组织中保持高完整性水平的机会。我们希望确保在为机器学习工作流程准备数据时，我们能够利用 Go
    的独特属性。
- en: 'Generally, Go data scientists/analysts should follow the following best practices
    when gathering and organizing data. These best practices are meant to help you
    maintain integrity in your applications, and been able you to reproduce any analysis:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，Go 数据科学家/分析师在收集和组织数据时应遵循以下最佳实践。这些最佳实践旨在帮助您在应用程序中保持完整性，并能够重现任何分析：
- en: '**Check for and enforce expected types**: This might seem obvious, but it is
    too often overlooked when using dynamically typed languages. Although it is slightly
    verbose, explicitly parsing data into expected types and handling related errors
    can save you big headaches down the road.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检查并强制执行预期的类型**：这看起来可能很显然，但在使用动态类型语言时，它往往被忽视。尽管这稍微有些冗长，但将数据显式解析为预期类型并处理相关错误可以在将来为你节省很多麻烦。'
- en: '**Standardize and simplify your data ingress/egress**: There are many third-party
    packages for handling certain types of data or interactions with certain sources
    of data (some of which we will cover in this book). However, if you standardize
    the ways you are interacting with data sources, particularly centered around the
    use of `stdlib`, you can develop predictable patterns and maintain consistency
    within your team. A good example of this is a choice to utilize `database/sql`
    for database interactions rather than using various third-party APIs and DSLs.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**标准化和简化你的数据输入/输出**：有许多第三方包用于处理某些类型的数据或与某些数据源交互（其中一些我们将在本书中介绍）。然而，如果你标准化与数据源交互的方式，特别是围绕使用
    `stdlib` 的使用，你可以开发可预测的模式并在团队内部保持一致性。一个很好的例子是选择使用 `database/sql` 进行数据库交互，而不是使用各种第三方
    API 和 DSL。'
- en: '**Version your data**: Machine learning models produce extremely different
    results depending on the training data you use, your choice of parameters, and
    input data. Thus, it is impossible to reproduce results without versioning both
    your code and data. We will discuss the appropriate techniques for data versioning
    later in this chapter.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**版本化你的数据**：机器学习模型产生的结果极其不同，这取决于你使用的训练数据、参数选择和输入数据。因此，如果不版本化你的代码和数据，就无法重现结果。我们将在本章后面讨论数据版本化的适当技术。'
- en: If you start to stray from these general principles, you should stop immediately.
    You are likely to sacrifice integrity for the sake of convenience, which is a
    dangerous road. We will let these principles guide us through the book and as
    we consider various data formats/sources in the following section.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你开始偏离这些基本原则，你应该立即停止。你可能会为了方便而牺牲完整性，这是一条危险的道路。我们将让这些原则引导我们在本书中的学习，并在下一节考虑各种数据格式/来源时，我们将遵循这些原则。
- en: CSV files
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CSV 文件
- en: CSV files might not be a go-to format for big data, but as a data scientist
    or developer working in machine learning, you are sure to encounter this format.
    You might need a mapping of zip codes to latitude/longitude and find this as a
    CSV file on the internet, or you may be given sales figures from your sales team
    in a CSV format. In any event, we need to understand how to parse these files.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 文件可能不是大数据的首选格式，但作为一名机器学习领域的数据科学家或开发者，你肯定会遇到这种格式。你可能需要将邮政编码映射到经纬度，并在互联网上找到这个
    CSV 文件，或者你的销售团队可能会以 CSV 格式提供销售数据。无论如何，我们需要了解如何解析这些文件。
- en: The main package that we will utilize in parsing CSV files is `encoding/csv`
    from Go's standard library. However, we will also discuss a couple of packages
    that allow us to quickly manipulate or transform CSV data--`github.com/kniren/gota/dataframe`
    and `go-hep.org/x/hep/csvutil`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在解析 CSV 文件时利用的主要包是 Go 标准库中的 `encoding/csv`。然而，我们还将讨论几个允许我们快速操作或转换 CSV 数据的包--`github.com/kniren/gota/dataframe`
    和 `go-hep.org/x/hep/csvutil`。
- en: Reading in CSV data from a file
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文件中读取 CSV 数据
- en: 'Let''s consider a simple CSV file, which we will return to later, named `iris.csv`
    (available here: [https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris)).
    This CSV file includes four float columns of flower measurements and a string
    column with the corresponding flower species:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的 CSV 文件，我们将在稍后返回，命名为 `iris.csv`（可在以下链接找到：[https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris))。这个
    CSV 文件包括四个表示花朵测量的浮点列和一个表示相应花朵种类的字符串列：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With `encoding/csv` imported, we first open the CSV file and create a CSV reader
    value:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 `encoding/csv` 后，我们首先打开 CSV 文件并创建一个 CSV 读取器值：
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then we can read in all of the records (corresponding to rows) of the CSV file.
    These records are imported as `[][]string`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以读取 CSV 文件的所有记录（对应于行），这些记录被导入为 `[][]string`：
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can also read in records one at a time in an infinite loop. Just make sure
    that you check for the end of the file (`io.EOF`) so that the loop ends after
    reading in all of your data:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过无限循环逐个读取记录。只需确保检查文件末尾（`io.EOF`），以便在读取所有数据后循环结束：
- en: '[PRE11]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If your CSV file is not delimited by commas and/or if your CSV file contains
    commented rows, you can utilize the `csv.Reader.Comma` and `csv.Reader.Comment`
    fields to properly handle uniquely formatted CSV files. In cases where the fields
    in your CSV file are single-quoted, you may need to add in a helper function to
    trim the single quotes and parse the values.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的CSV文件不是以逗号分隔的，或者如果你的CSV文件包含注释行，你可以利用`csv.Reader.Comma`和`csv.Reader.Comment`字段来正确处理格式独特的CSV文件。在字段在CSV文件中用单引号包围的情况下，你可能需要添加一个辅助函数来删除单引号并解析值。
- en: Handling unexpected fields
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理意外的字段
- en: 'The preceding methods work fine with clean CSV data, but, in general, we don''t
    encounter clean data. We have to parse messy data. For example, you might find
    unexpected fields or numbers of fields in your CSV records. This is why `reader.FieldsPerRecord`
    exists. This field of the reader value lets us easily handle messy data, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方法对干净的CSV数据工作得很好，但通常我们不会遇到干净的数据。我们必须解析混乱的数据。例如，你可能会在你的CSV记录中找到意外的字段或字段数量。这就是为什么`reader.FieldsPerRecord`存在的原因。这个读取值字段让我们能够轻松地处理混乱的数据，如下所示：
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This version of the `iris.csv` file has an extra field in one of the rows.
    We know that each record should have five fields, so let''s set our `reader.FieldsPerRecord`
    value to `5`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个版本的`iris.csv`文件在一行中有一个额外的字段。我们知道每个记录应该有五个字段，所以让我们将我们的`reader.FieldsPerRecord`值设置为`5`：
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then as we are reading in records from the CSV file, we can check for unexpected
    fields and maintain the integrity of our data:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 那么当我们从CSV文件中读取记录时，我们可以检查意外的字段并保持我们数据的一致性：
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, we have chosen to handle the error by logging the error, and we only collect
    successfully parsed records into `rawCSVData`. The reader will note that this
    error could be handled in many different ways. The important thing is that we
    are forcing ourselves to check for an expected property of the data and increasing
    the integrity of our application.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们选择通过记录错误来处理错误，并且我们只将成功解析的记录收集到`rawCSVData`中。读者会注意到这种错误可以以许多不同的方式处理。重要的是我们正在强迫自己检查数据的一个预期属性，并提高我们应用程序的完整性。
- en: Handling unexpected types
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理意外的类型
- en: 'We just saw that CSV data is read into Go as `[][]string`. However, Go is statically
    typed, which allows us to enforce strict checks for each of the CSV fields. We
    can do this as we parse each field for further processing. Consider some messy
    data that has random fields that don''t match the type of the other values in
    a column:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到CSV数据被读取为`[][]string`。然而，Go是静态类型的，这允许我们对每个CSV字段执行严格的检查。我们可以在解析每个字段以进行进一步处理时这样做。考虑一些混乱的数据，其中包含与列中其他值类型不匹配的随机字段：
- en: '[PRE15]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To check the types of the fields in our CSV records, let''s create a `struct`
    variable to hold successfully parsed values:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查我们CSV记录中字段的类型，让我们创建一个`struct`变量来保存成功解析的值：
- en: '[PRE16]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, before we loop over the records, let''s initialize a slice of these values:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在我们遍历记录之前，让我们初始化这些值的一个切片：
- en: '[PRE17]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now as we loop over the records, we can parse into the relevant type for that
    record, catch any errors, and log as needed:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们遍历记录时，我们可以解析为该记录的相关类型，捕获任何错误，并按需记录：
- en: '[PRE18]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Manipulating CSV data with data frames
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据框操作CSV数据
- en: As you can see, manually parsing many different fields and performing row-by-row
    operations can be rather verbose and tedious. This is definitely *not* an excuse
    to increase complexity and import a bunch of non standard functionalities. You
    should still default to the use of `encoding/csv` in most cases.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，手动解析许多不同的字段并逐行执行操作可能会相当冗长且繁琐。这绝对不是增加复杂性和导入大量非标准功能的借口。在大多数情况下，你应该仍然默认使用`encoding/csv`。
- en: 'However, manipulation of data frames has proven to be a successful and somewhat
    standardized way (in the data science community) of dealing with tabular data.
    Thus, in some cases, it is worth employing some third-party functionality to manipulate
    tabular data, such as CSV data. For example, data frames and the corresponding
    functionality can be very useful when you are trying to filter, subset, and select
    portions of tabular datasets. In this section, we will introduce `github.com/kniren/gota/dataframe`,
    a wonderful `dataframe` package for Go:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据框的操作已被证明是处理表格数据的一种成功且相对标准化的方式（在数据科学社区中）。因此，在某些情况下，使用一些第三方功能来操作表格数据，如CSV数据，是值得的。例如，数据框及其对应的功能在你尝试过滤、子集化和选择表格数据集的部分时非常有用。在本节中，我们将介绍`github.com/kniren/gota/dataframe`，这是一个为Go语言提供的优秀的`dataframe`包：
- en: '[PRE19]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To create a data frame from a CSV file, we open a file with `os.Open()` and
    then supply the returned pointer to the `dataframe.ReadCSV()` function:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要从CSV文件创建数据框，我们使用`os.Open()`打开一个文件，然后将返回的指针提供给`dataframe.ReadCSV()`函数：
- en: '[PRE20]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If we compile and run this Go program, we will see a nice, pretty-printed version
    of our data with the types that were inferred during parsing:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们编译并运行这个Go程序，我们将看到一个漂亮的、格式化的数据版本，其中包含了在解析过程中推断出的类型：
- en: '[PRE21]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Once we have the data parsed into a `dataframe`, we can filter, subset, and
    select our data easily:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将数据解析到`dataframe`中，我们就可以轻松地进行过滤、子集化和选择我们的数据：
- en: '[PRE22]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This is really only scratching the surface of the `github.com/kniren/gota/dataframe`
    package. You can merge datasets, output to other formats, and even process JSON
    data. For more information about this package, you should visit the auto generated
    GoDocs at [https://godoc.org/github.com/kniren/gota/dataframe](https://godoc.org/github.com/kniren/gota/dataframe),
    which is good practice, in general, for any packages we discuss in the book.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上只是对`github.com/kniren/gota/dataframe`包表面的探索。你可以合并数据集，输出到其他格式，甚至处理JSON数据。关于这个包的更多信息，你应该访问自动生成的GoDocs，网址为[https://godoc.org/github.com/kniren/gota/dataframe](https://godoc.org/github.com/kniren/gota/dataframe)，这在一般情况下，对于我们在书中讨论的任何包来说都是好的实践。
- en: JSON
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JSON
- en: In a world in which the majority of data is accessed via the web, and most engineering
    organizations implement some number of microservices, we are going to encounter
    data in JSON format fairly frequently. We may only need to deal with it when pulling
    some random data from an API, or it might actually be the primary data format
    that drives our analytics and machine learning workflows.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个大多数数据都是通过网络访问的世界里，大多数工程组织实施了一定数量的微服务，我们将非常频繁地遇到JSON格式的数据。我们可能只需要在从API中拉取一些随机数据时处理它，或者它实际上可能是驱动我们的分析和机器学习工作流程的主要数据格式。
- en: Typically, JSON is used when ease of use is the primary goal of data interchange.
    Since JSON is human readable, it is easy to debug if something breaks. Remember
    that we want to maintain the integrity of our data handling as we process data
    with Go, and part of that process is ensuring that, when possible, our data is
    interpretable and readable. JSON turns out to be very useful in achieving these
    goals (which is why it is also used for logging, in many cases).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当易用性是数据交换的主要目标时，会使用JSON。由于JSON是可读的，如果出现问题，它很容易调试。记住，我们希望在用Go处理数据时保持我们数据处理的一致性，这个过程的一部分是确保，当可能时，我们的数据是可解释和可读的。JSON在实现这些目标方面非常有用（这也是为什么它也常用于日志记录）。
- en: Go offers really great JSON functionality in its standard library with `encoding/json`.
    We will utilize this standard library functionality throughout the book.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Go在其标准库中提供了非常好的JSON功能，使用`encoding/json`。我们将在整个书中利用这个标准库功能。
- en: Parsing JSON
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析JSON
- en: 'To understand how to parse (that is, unmarshal) JSON data in Go, we will be
    using some data from the Citi Bike API ([https://www.citibikenyc.com/system-data](https://www.citibikenyc.com/system-data)),
    a bike-sharing service operating in New York City. Citi Bike provides frequently
    updated operational information about its network of bike sharing stations in
    JSON format at [https://gbfs.citibikenyc.com/gbfs/en/station_status.json](https://gbfs.citibikenyc.com/gbfs/en/station_status.json):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何在Go中解析（即反序列化）JSON数据，我们将使用来自Citi Bike API（[https://www.citibikenyc.com/system-data](https://www.citibikenyc.com/system-data)）的一些数据，这是一个在纽约市运营的自行车共享服务。Citi
    Bike以JSON格式提供其自行车共享站点的频繁更新的运营信息，网址为[https://gbfs.citibikenyc.com/gbfs/en/station_status.json](https://gbfs.citibikenyc.com/gbfs/en/station_status.json)：
- en: '[PRE23]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To parse the import and this type of data in Go, we first need to import `encoding/json`
    (along with a couple of other things from a standard library, such as `net/http`,
    because we are going to pull this data off of the previously mentioned website).
    We will also define `struct` that mimics the structure of the JSON shown in the
    preceding code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在Go中解析导入和这种类型的数据时，我们首先需要导入`encoding/json`（以及从标准库中的一些其他东西，如`net/http`，因为我们将从之前提到的网站上拉取这些数据）。我们还将定义`struct`，它模仿了前面代码中显示的JSON结构：
- en: '[PRE24]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Note a couple of things here: (i) we have followed Go idioms by avoiding the
    `struct` field name with underscores, but (ii) we have utilized the `json` struct
    tags to label the `struct` fields with the corresponding expected fields in the
    JSON data.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里的一些事情：（i）我们遵循了Go的惯例，避免了使用下划线的`struct`字段名，但（ii）我们使用了`json`结构标签来标记`struct`字段，以对应JSON数据中的预期字段。
- en: Note, to properly parse JSON data, the struct fields need to be exported fields.
    That is, the fields need to begin with a capital letter. `encoding/json` does
    cannot view fields using reflect unless they are exported.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了正确解析JSON数据，结构体字段必须是导出字段。也就是说，字段需要以大写字母开头。`encoding/json`无法使用反射查看未导出的字段。
- en: 'Now we can get the JSON data from the URL and unmarshal it into a new `stationData`
    value. This will produce a `struct` variable with the respective fields filled
    with the data in the tagged JSON data fields. We can check it by printing out
    some data associated with one of the stations:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以从URL获取JSON数据并将其反序列化到一个新的`stationData`值中。这将产生一个`struct`变量，其相应字段填充了标记的JSON数据字段中的数据。我们可以通过打印与某个站点相关的一些数据来检查它：
- en: '[PRE25]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'When we run this, we can see that our `struct` contains the parsed data from
    the URL:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行此操作时，我们可以看到我们的`struct`包含了从URL解析的数据：
- en: '[PRE26]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: JSON output
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JSON输出
- en: 'Now let''s say that we have the Citi Bike station data in our `stationData`
    struct value and we want to save that data out to a file. We can do this with
    `json.marshal`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们已经在`stationData`结构体值中有了Citi Bike站点的数据，并希望将数据保存到文件中。我们可以使用`json.marshal`来完成此操作：
- en: '[PRE27]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: SQL-like databases
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类似SQL的数据库
- en: Although there is a good bit of hype around interesting NoSQL databases and
    key-value stores, SQL-like databases are still ubiquitous. Every data scientist
    will, at some point, be processing data from an SQL-like database, such as Postgres,
    MySQL, or SQLite.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管围绕有趣的NoSQL数据库和键值存储有很多炒作，但类似SQL的数据库仍然无处不在。每个数据科学家在某个时候都会处理来自类似SQL的数据库的数据，例如Postgres、MySQL或SQLite。
- en: For example, we may be required to query one or more tables in a Postgres database
    to generate a set of features for model training. After using that model to make
    predictions or identify anomalies, we may send results to another database table
    that drives a dashboard or other reporting tool.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可能需要查询Postgres数据库中的一个或多个表来生成用于模型训练的一组特征。在用该模型进行预测或识别异常之后，我们可能将结果发送到另一个数据库表，该表驱动仪表板或其他报告工具。
- en: Go, of course, interacts nicely with all the popular data stores, such as SQL,
    NoSQL, key-value, and so on, but here, we will focus on SQL-like interactions.
    We will utilize `database/sql` for these interactions throughout the book.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Go与所有流行的数据存储都很好地交互，例如SQL、NoSQL、键值存储等，但在这里，我们将专注于类似SQL的交互。在整个书中，我们将使用`database/sql`进行这些交互。
- en: Connecting to an SQL database
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接到SQL数据库
- en: 'The first thing we need do before connecting to an SQL-like database is identify
    the particular database that we will be interacting with and import a corresponding
    driver. In the following examples, we will be connecting to a Postgres database
    and will utilize the `github.com/lib/pq` database driver for `database/sql`. This
    driver can be loaded via an empty import (with a corresponding comment):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在连接类似SQL的数据库之前，我们需要做的第一件事是确定我们将与之交互的特定数据库，并导入相应的驱动程序。在以下示例中，我们将连接到Postgres数据库，并将使用`github.com/lib/pq`数据库驱动程序来处理`database/sql`。此驱动程序可以通过空导入（带有相应的注释）来加载：
- en: '[PRE28]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now let''s assume that you have exported the Postgres connection string to
    an environmental variable `PGURL`. We can easily create an `sql.DB` value for
    our connection via the follow code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设您已经将Postgres连接字符串导出到环境变量`PGURL`中。我们可以通过以下代码轻松地为我们的连接创建一个`sql.DB`值：
- en: '[PRE29]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note that we need to defer the `close` method on this value. Also, note that
    creating this value does not mean that you have made a successful connection to
    the database. This is merely a value used by `database/sql` to connect to the
    database when triggered to do so by certain operations (such as a query).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们需要延迟此值的`close`方法。另外，请注意，创建此值并不意味着您已成功连接到数据库。这只是一个由`database/sql`在触发某些操作（如查询）时用于连接数据库的值。
- en: 'To ensure that we can make a successful connection to the database, we can
    use the `Ping` method:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们可以成功连接到数据库，我们可以使用`Ping`方法：
- en: '[PRE30]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Querying the database
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询数据库
- en: 'Now that we know how to connect to the database, let''s see how we can get
    data out of the database. We won''t cover the specifics of SQL queries and statements
    in this book. If you are not familiar with SQL, I would highly recommend that
    you learn how to query, insert, and so on, but for our purposes here, you should
    know that there are basically two types of operations we want to perform as related
    to SQL databases:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何连接到数据库，让我们看看我们如何从数据库中获取数据。在这本书中，我们不会涵盖SQL查询和语句的细节。如果您不熟悉SQL，我强烈建议您学习如何查询、插入等，但就我们这里的目的而言，您应该知道我们基本上有两种类型的操作与SQL数据库相关：
- en: A `Query` operation selects, groups, or aggregates data in the database and
    returns rows of data to us
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Query`操作在数据库中选取、分组或聚合数据，并将数据行返回给我们'
- en: An `Exec` operation updates, inserts, or otherwise modifies the state of the
    database without an expectation that portions of the data stored in the database
    should be returned
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Exec`操作更新、插入或以其他方式修改数据库的状态，而不期望数据库中存储的数据的部分应该被返回'
- en: 'As you might expect, to get data out of our database, we will use a `Query`
    operation. To do this, we need to query the database with an SQL statement string.
    For example, imagine we have a database storing a bunch of iris flower measurements
    (petal length, petal width, and so on), we could query some of that data related
    to a particular iris species as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所预期的那样，为了从我们的数据库中获取数据，我们将使用`Query`操作。为此，我们需要使用SQL语句字符串查询数据库。例如，假设我们有一个存储大量鸢尾花测量数据（花瓣长度、花瓣宽度等）的数据库，我们可以查询与特定鸢尾花物种相关的数据如下：
- en: '[PRE31]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Note that this returns a pointer to an `sql.Rows` value, and we need to defer
    the closing of this rows value. Then we can loop over our rows and parse the data
    into values of expected type. We utilize the `Scan` method on rows to parse out
    the columns returned by the SQL query and print them to standard out:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这返回了一个指向`sql.Rows`值的指针，我们需要延迟关闭这个行值。然后我们可以遍历我们的行并将数据解析为预期的类型。我们利用`Scan`方法在行上解析SQL查询返回的列并将它们打印到标准输出：
- en: '[PRE32]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, we need to check for any errors that might have occurred while processing
    our rows. We want to maintain the integrity of our data handling, and we cannot
    assume that we looped over all the rows without encountering an error:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要检查在处理我们的行时可能发生的任何错误。我们希望保持我们数据处理的一致性，我们不能假设我们在没有遇到错误的情况下遍历了所有的行：
- en: '[PRE33]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Modifying the database
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修改数据库
- en: As mentioned earlier, there is another flavor of interaction with the database
    called `Exec`. With these types of statements, we are concerned with updating,
    adding to, or otherwise modifying the state of one or more tables in the database.
    We use the same type of database connection, but instead of calling `db.Query`,
    we will call `db.Exec`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，还有另一种与数据库的交互方式称为`Exec`。使用这些类型的语句，我们关注的是更新、添加或以其他方式修改数据库中的一个或多个表的状态。我们使用相同类型的数据库连接，但不是调用`db.Query`，我们将调用`db.Exec`。
- en: 'For example, let''s say we want to update some of the values in our iris database
    table:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想要更新我们iris数据库表中的某些值：
- en: '[PRE34]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'But how do we know whether we were successful and changed something? Well,
    the `res` function returned here allows us to see how many rows of our table were
    affected by our update:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何知道我们是否成功并改变了某些内容呢？嗯，这里返回的`res`函数允许我们查看我们的表中有多少行受到了我们更新的影响：
- en: '[PRE35]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Caching
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存
- en: Sometimes, our machine learning algorithms will be trained by and/or given input
    for prediction via data from external sources (for example, APIs), that is, data
    that isn't local to the application running our modeling or analysis. Further,
    we might have various sets of data that are being accessed frequently, may be
    accessed again soon, or may need to be made available while the application is
    running.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们的机器学习算法将通过外部来源（例如，API）的数据进行训练和/或提供预测输入，即不是运行我们的建模或分析的应用程序本地的数据。此外，我们可能有一些经常访问的数据集，可能很快会再次访问，或者可能需要在应用程序运行时提供。
- en: In at least some of these cases, it might make sense to cache data in memory
    or embed the data locally where the application is running. For example, if you
    are reaching out to a government API (typically having high latency) for census
    data frequently, you may consider maintaining a local or in-memory cache of the
    census data being used so that you can avoid constantly reaching out to the API.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在至少这些情况中，缓存数据在内存中或嵌入到应用程序运行的地方可能是合理的。例如，如果你经常访问政府 API（通常具有高延迟）以获取人口普查数据，你可能会考虑维护一个本地或内存中的缓存，以便你可以避免不断调用
    API。
- en: Caching data in memory
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在内存中缓存数据
- en: To cache a series of values in memory, we will use `github.com/patrickmn/go-cache`.
    With this package, we can create an in-memory cache of keys and corresponding
    values. We can even specify things, such as the time to live, in the cache for
    specific key-value pairs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要在内存中缓存一系列值，我们将使用 `github.com/patrickmn/go-cache`。使用这个包，我们可以创建一个包含键和相应值的内存缓存。我们甚至可以指定缓存中特定键值对的时间生存期。
- en: 'To create a new in-memory cache and set a key-value pair in the cache, we do
    the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个新的内存缓存并在缓存中设置键值对，我们执行以下操作：
- en: '[PRE36]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To then retrieve the value for `mykey` out of the cache, we just need to use
    the `Get` method:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要从缓存中检索 `mykey` 的值，我们只需使用 `Get` 方法：
- en: '[PRE37]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Caching data locally on disk
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在磁盘上本地缓存数据
- en: The caching we just saw is in memory. That is, the cached data exists and is
    accessible while your application is running, but as soon as your application
    exits, your data disappears. In some cases, you may want your cached data to stick
    around when your application restarts or exits. You may also want to back up your
    cache such that you don't have to start applications from scratch without a cache
    of relevant data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才看到的缓存是在内存中的。也就是说，缓存的数据在应用程序运行时存在并可访问，但一旦应用程序退出，数据就会消失。在某些情况下，你可能希望当你的应用程序重新启动或退出时，缓存的数据仍然保留。你也可能想要备份你的缓存，这样你就不需要在没有相关数据缓存的情况下从头开始启动应用程序。
- en: 'In these scenarios, you may consider using a local, embedded cache, such as
    `github.com/boltdb/bolt`. BoltDB, as it is referred to, is a very popular project
    for these sorts of applications, and basically consists of a local key-value store.
    To initialize one of these local key-value stores, do the following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，你可能考虑使用本地的嵌入式缓存，例如 `github.com/boltdb/bolt`。BoltDB，正如其名，是这类应用中非常受欢迎的项目，基本上由一个本地的键值存储组成。要初始化这些本地键值存储之一，请执行以下操作：
- en: '[PRE38]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: You can, of course, have multiple different buckets of data in your BoltDB and
    use a filename other than `embedded.db`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以在 BoltDB 中拥有多个不同的数据桶，并使用除 `embedded.db` 之外的其他文件名。
- en: 'Next, let''s say you had a map of string values in memory that you need to
    cache in BoltDB. To do this, you would range over the keys and values in the map,
    updating your BoltDB:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，假设你有一个内存中的字符串值映射，你需要将其缓存到 BoltDB 中。为此，你需要遍历映射中的键和值，更新你的 BoltDB：
- en: '[PRE39]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Then, to get values out of BoltDB, you can view your data:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，要从 BoltDB 中获取值，你可以查看你的数据：
- en: '[PRE40]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Data versioning
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据版本控制
- en: 'As mentioned, machine learning models produce extremely different results depending
    on the training data you use, the choices of parameters, and the input data. It
    is essential to be able to reproduce results for collaborative, creative, and
    compliance reasons:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，机器学习模型产生的结果极其不同，这取决于你使用的训练数据、参数的选择和输入数据。为了协作、创造性和合规性原因，能够重现结果是至关重要的：
- en: '**Collaboration**: Despite what you see on social media, there are no data
    science and machine learning unicorns (that is, people with knowledge and capabilities
    in every area of data science and machine learning). We need to have our colleagues''
    reviews and improve on our work, and this is impossible if they aren''t able to
    reproduce our model results and analyses.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协作**：尽管你在社交媒体上看到的是，没有数据科学和机器学习独角兽（即在每个数据科学和机器学习领域都有知识和能力的人）。我们需要同事的审查并改进我们的工作，而如果他们无法重现我们的模型结果和分析，这是不可能的。'
- en: '**Creativity**: I don''t know about you, but I have trouble remembering even
    what I did yesterday. We can''t trust ourselves to always remember our reasoning
    and logic, especially when we are dealing with machine learning workflows. We
    need to track exactly what data we are using, what results we created, and how
    we created them. This is the only way we will be able to continually improve our
    models and techniques.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创造力**：我不知道你怎么样，但即使是我也难以记住昨天做了什么。我们无法信任自己总是能记住我们的推理和逻辑，尤其是在处理机器学习工作流程时。我们需要精确跟踪我们使用的数据、我们创建的结果以及我们是如何创建它们的。这是我们能够不断改进我们的模型和技术的方式。'
- en: '**Compliance**: Finally, we may not have a choice regarding data versioning
    and reproducibility in machine learning very soon. Laws are being passed around
    the world (for example, the **General Data Protection Regulation** (**GDPR**)
    in the European Union) that give users a right to an explanation for algorithmically
    made decisions. We simply cannot hope to comply with these rulings if we don''t
    have a robust way of tracking what data we are processing and what results we
    are producing.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合规性**：最后，我们可能很快就会在机器学习中没有选择地进行数据版本化和可重现性。世界各地正在通过法律（例如，欧盟的**通用数据保护条例**（GDPR））赋予用户对算法决策的解释权。如果我们没有一种稳健的方式来跟踪我们正在处理的数据和产生的结果，我们根本无法希望遵守这些裁决。'
- en: 'There are multiple open source data versioning projects. Some of these are
    focused on security and peer-to-peer distributed storage of data. Others are focused
    on data science workflows. In this book, we will focus on and utilize Pachyderm
    ([http://pachyderm.io/](http://pachyderm.io/)), an open source framework for data
    versioning and data pipelining. Some of the reasons for this will be clear later
    in the book when we talk about production deploys and managing ML pipelines. For
    now, I will just summarize some of the features of Pachyderm that make it an attractive
    choice for data versioning in Go-based (and other) ML projects:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多个开源数据版本控制项目。其中一些专注于数据的安全性和对等分布式存储。其他一些则专注于数据科学工作流程。在这本书中，我们将重点关注并利用 Pachyderm
    ([http://pachyderm.io/](http://pachyderm.io/))，这是一个开源的数据版本控制和数据管道框架。其中一些原因将在本书后面关于生产部署和管理
    ML 管道时变得清晰。现在，我将仅总结一些使 Pachyderm 成为基于 Go（和其他）ML 项目数据版本控制吸引力的特性：
- en: It has an convenient Go client, `github.com/pachyderm/pachyderm/src/client`
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有一个方便的 Go 客户端，`github.com/pachyderm/pachyderm/src/client`
- en: The ability to version any type and format of data
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够对任何类型和格式的数据进行版本控制
- en: A flexible object store backing for the versioned data
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为版本化数据提供灵活的对象存储后端
- en: Integration with a data pipelining system for driving versioned ML workflows
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与数据管道系统集成以驱动版本化的 ML 工作流程
- en: Pachyderm jargon
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pachyderm 术语
- en: 'Think about versioning data in Pachyderm kind of like versioning code in Git.
    The primitives are similar:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Pachyderm 中的数据版本化想象成在 Git 中版本化代码。基本原理是相似的：
- en: '**Repositories**: These are versioned collections of data, similar to having
    versioned collections of code in Git repositories'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仓库**：这些是版本化的数据集合，类似于在 Git 仓库中拥有版本化的代码集合'
- en: '**Commits**: Data is versioned in Pachyderm by making commits of that data
    into data repositories'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提交**：在 Pachyderm 中，通过将数据提交到数据仓库来对数据进行版本控制'
- en: '**Branches**: These lightweight points to certain commits or sets of commits
    (for example, master points to the latest HEAD commit)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分支**：这些轻量级指针指向特定的提交或一系列提交（例如，master 指向最新的 HEAD 提交）'
- en: '**Files**: Data is versioned at the file level in Pachyderm, and Pachyderm
    automatically employs strategies, such as de-duplication, to keep your versioned
    data space efficient'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文件**：在 Pachyderm 中，数据在文件级别进行版本控制，并且 Pachyderm 自动采用去重等策略来保持你的版本化数据空间高效'
- en: Even though versioning data with Pachyderm feels similar to versioning code
    with Git, there are some major differences. For example, merging data doesn't
    exactly make sense. If there are merge conflicts on petabytes of data, no human
    could resolve these. Furthermore, the Git protocol would not be space efficient
    in general for large sets of data. Pachyderm uses its own internal logic to perform
    the versioning and work with versioned data, and the logic is both space efficient
    and processing efficient in terms of caching.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用 Pachyderm 对数据进行版本控制的感觉与使用 Git 对代码进行版本控制相似，但也有一些主要区别。例如，合并数据并不完全有意义。如果存在数PB（皮字节）数据的合并冲突，没有人能够解决这些问题。此外，Git
    协议在处理大量数据时通常不会很节省空间。Pachyderm 使用其自身的内部逻辑来执行版本控制和处理版本化数据，这种逻辑在缓存方面既节省空间又高效。
- en: Deploying/installing Pachyderm
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署/安装Pachyderm
- en: We will be using Pachyderm in various other places in the book to both version
    data and create distributed ML workflows. Pachyderm itself is an app that runs
    on top of Kubernetes ([https://kubernetes.io/](https://kubernetes.io/)), and is
    backed by an object store of your choice. For the purposes of this book, development,
    and experimentation, you can easily install and run Pachyderm locally. It should
    take 5-10 minutes to install and doesn't require much effort. The instructions
    for the local installation can be found in the Pachyderm documentation at [http://docs.pachyderm.io](http://docs.pachyderm.io).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的多个地方使用Pachyderm来对数据进行版本控制并创建分布式机器学习工作流程。Pachyderm本身是一个运行在Kubernetes([https://kubernetes.io/](https://kubernetes.io/))之上的应用程序，并支持你选择的任何对象存储。为了本书的开发和实验目的，你可以轻松地安装并本地运行Pachyderm。安装应该需要5-10分钟，并且不需要太多努力。本地安装的说明可以在Pachyderm文档中找到，网址为[http://docs.pachyderm.io](http://docs.pachyderm.io)。
- en: When you are ready to run your workflows in production or your deploy model,
    you can easily deploy a production-ready Pachyderm cluster that will behave the
    same exact way as your local installation. Pachyderm can be deployed to any cloud,
    or even on premises.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当你准备好在生产环境中运行你的工作流程或部署模型时，你可以轻松地部署一个生产就绪的Pachyderm集群，该集群将与你本地安装的行为完全相同。Pachyderm可以部署到任何云中，甚至可以在本地部署。
- en: As mentioned, Pachyderm is an open source project and has an active group of
    users. If you have questions or need help, you can join the public Pachyderm Slack
    channel by visiting [http://slack.pachyderm.io/](http://slack.pachyderm.io/).
    The active Pachyderm users and the Pachyderm team itself will be able to respond
    very quickly to your questions there.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Pachyderm是一个开源项目，并且有一个活跃的用户群体。如果你有问题或需要帮助，你可以通过访问[http://slack.pachyderm.io/](http://slack.pachyderm.io/)加入公共Pachyderm
    Slack频道。活跃的Pachyderm用户和Pachyderm团队本身将能够快速回答你的问题。
- en: Creating data repositories for data versioning
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为数据版本化创建数据仓库
- en: 'If you followed the local installation of Pachyderm specified in the Pachyderm
    documentation, you should have the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遵循了Pachyderm文档中指定的本地安装说明，你应该有以下内容：
- en: Kubernetes running in a Minikube VM on your machine
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的机器上的Minikube VM上运行的Kubernetes
- en: The `pachctl` command line tool installed and connected to your Pachyderm cluster
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已安装并连接到你的Pachyderm集群的`pachctl`命令行工具
- en: Of course, if you have a production cluster running in a cloud, the following
    steps still apply. Your `pachctl` would just be connected to the remote cluster.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果你在云中运行一个生产集群，以下步骤仍然适用。你的`pachctl`将连接到远程集群。
- en: We will be demonstrating data versioning functionality with the `pachctl` **Command-line
    Interface** (**CLI**) tool below (which is a Go program). However, as mentioned
    above, Pachyderm has a full-fledged Go client. You can create repositories, commit
    data, and much more directly from your Go programs. This functionality will be
    demonstrated later in Chapter 9, *Deploying and distributing Analyses and Models*.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下面的示例中使用`pachctl` **命令行界面**（**CLI**）（这是一个Go程序）来演示数据版本化功能。然而，如上所述，Pachyderm有一个完整的Go客户端。你可以直接从你的Go程序中创建仓库、提交数据等等。这一功能将在第9章*部署和分发分析和模型*中演示。
- en: 'To create a repository of data called `myrepo`, you can run this code:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个名为`myrepo`的数据仓库，你可以运行以下代码：
- en: '[PRE41]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You can then confirm that the repository exists with `list-repo`:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`list-repo`来确认仓库是否存在：
- en: '[PRE42]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This `myrepo` repository is a collection of data that we have defined and is
    ready for housing-versioned data. Right now, there is no data in the repository,
    because we haven't put any data there yet.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`myrepo`仓库是我们定义的数据集合，已准备好存放版本化的数据。目前，仓库中没有数据，因为我们还没有放入任何数据。
- en: Putting data into data repositories
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据放入数据仓库
- en: 'Let''s say that we have a simple text file:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个简单的文本文件：
- en: '[PRE43]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'If this file is part of the data we are utilizing in our ML workflow, we should
    version it. To version this file in our repository, `myrepo`, we just need to
    commit it into that repository:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个文件是我们正在利用的机器学习工作流程中的数据的一部分，我们应该对其进行版本控制。要在我们的仓库`myrepo`中对此文件进行版本控制，我们只需将其提交到该仓库：
- en: '[PRE44]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The `-c` flag specifies that we want Pachyderm to open a new commit, insert
    the file we are referencing, and close the commit all in one shot. The `-f` flag
    specifies that we are providing a file.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`-c`标志指定我们希望Pachyderm打开一个新提交，插入我们引用的文件，然后一次性关闭提交。`-f`标志指定我们提供了一个文件。'
- en: Note that we are committing a single file to the master branch of a single repository
    here. However, the Pachyderm API is incredibly flexible. We can commit, delete,
    or otherwise modify many versioned files in a single commit or over multiple commits.
    Further, these files could be versioned via a URL, object store link, database
    dump, and so on.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在这里是将单个文件提交到单个仓库的master分支。然而，Pachyderm API非常灵活。我们可以在单个提交或多个提交中提交、删除或以其他方式修改许多版本化文件。此外，这些文件可以通过URL、对象存储链接、数据库转储等方式进行版本化。
- en: 'As a sanity check, we can confirm that our file was versioned in the repository:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种合理性检查，我们可以确认我们的文件已在仓库中进行了版本化：
- en: '[PRE45]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Getting data out of versioned data repositories
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从版本化数据仓库中获取数据
- en: Now that we have versioned data in Pachyderm, we probably want to know how to
    interact with that data. The primary way is via Pachyderm data pipelines (which
    will be discussed later in this book). The mechanism for interacting with versioned
    data when using pipelines is a simple file I/O.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在Pachyderm中有了版本化的数据，我们可能想知道如何与这些数据交互。主要的方式是通过Pachyderm数据管道（本书后面将讨论）。在管道中使用时与版本化数据交互的机制是一个简单的文件I/O。
- en: 'However, if we manually want to pull certain sets of versioned data out of
    Pachyderm, analyze them interactively, then we can use the `pachctl` CLI to get
    data:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们想手动从Pachyderm中提取某些版本的版本化数据，进行交互式分析，那么我们可以使用`pachctl` CLI来获取数据：
- en: '[PRE46]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: References
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'CSV data:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: CSV数据：
- en: '`encoding/csv` docs: [https://golang.org/pkg/encoding/csv/](https://golang.org/pkg/encoding/csv/)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoding/csv` 文档：[https://golang.org/pkg/encoding/csv/](https://golang.org/pkg/encoding/csv/)'
- en: '`github.com/kniren/gota/dataframe` docs: [https://godoc.org/github.com/kniren/gota/dataframe](https://godoc.org/github.com/kniren/gota/dataframe)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`github.com/kniren/gota/dataframe` 文档：[https://godoc.org/github.com/kniren/gota/dataframe](https://godoc.org/github.com/kniren/gota/dataframe)'
- en: 'JSON data:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: JSON数据：
- en: '`encoding/json` docs: [https://golang.org/pkg/encoding/json/](https://golang.org/pkg/encoding/json/)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoding/json` 文档：[https://golang.org/pkg/encoding/json/](https://golang.org/pkg/encoding/json/)'
- en: 'Bill Kennedy''s blog post of JSON decoding: [https://www.goinggo.net/2014/01/decode-json-documents-in-go.html](https://www.goinggo.net/2014/01/decode-json-documents-in-go.html)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bill Kennedy的博客文章JSON解码：[https://www.goinggo.net/2014/01/decode-json-documents-in-go.html](https://www.goinggo.net/2014/01/decode-json-documents-in-go.html)
- en: 'Ben Johnson''s blog post Go Walkthrough: `encoding/json` package: [https://medium.com/go-walkthrough/go-walkthrough-encoding-json-package-9681d1d37a8f](https://medium.com/go-walkthrough/go-walkthrough-encoding-json-package-9681d1d37a8f)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ben Johnson的博客文章Go Walkthrough：`encoding/json`包：[https://medium.com/go-walkthrough/go-walkthrough-encoding-json-package-9681d1d37a8f](https://medium.com/go-walkthrough/go-walkthrough-encoding-json-package-9681d1d37a8f)
- en: 'Caching:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存：
- en: '`github.com/patrickmn/go-cache` docs: [https://godoc.org/github.com/patrickmn/go-cache](https://godoc.org/github.com/patrickmn/go-cache)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`github.com/patrickmn/go-cache` 文档：[https://godoc.org/github.com/patrickmn/go-cache](https://godoc.org/github.com/patrickmn/go-cache)'
- en: '`github.com/boltdb/bolt` docs: [https://godoc.org/github.com/boltdb/bolt](https://godoc.org/github.com/boltdb/bolt)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`github.com/boltdb/bolt` 文档：[https://godoc.org/github.com/boltdb/bolt](https://godoc.org/github.com/boltdb/bolt)'
- en: 'Information about and motivation for BoltDB: [https://npf.io/2014/07/intro-to-boltdb-painless-performant-persistence/](https://npf.io/2014/07/intro-to-boltdb-painless-performant-persistence/)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BoltDB的相关信息和动机：[https://npf.io/2014/07/intro-to-boltdb-painless-performant-persistence/](https://npf.io/2014/07/intro-to-boltdb-painless-performant-persistence/)
- en: 'Pachyderm:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Pachyderm：
- en: 'General documentation: [http://docs.pachyderm.io](http://docs.pachyderm.io)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用文档：[http://docs.pachyderm.io](http://docs.pachyderm.io)
- en: 'Go client docs: [https://godoc.org/github.com/pachyderm/pachyderm/src/client](https://godoc.org/github.com/pachyderm/pachyderm/src/client)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Go客户端文档：[https://godoc.org/github.com/pachyderm/pachyderm/src/client](https://godoc.org/github.com/pachyderm/pachyderm/src/client)
- en: 'Public users Slack team registration: [http://docs.pachyderm.io](http://docs.pachyderm.io)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公共用户Slack团队注册：[http://docs.pachyderm.io](http://docs.pachyderm.io)
- en: Summary
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to gather, organize, and parse data. This is
    the first step, and one of the most important step, in developing machine learning
    models, but having data does not get us very far if we do not gain some intuition
    about our data and put it into a standard form for processing. Next, we will tackle
    some techniques for further structuring our data (matrices) and for understanding
    our data (statistics and probability).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何收集、组织和解析数据。这是开发机器学习模型的第一步，也是最重要的一步，但如果我们不对数据进行一些直观的理解并将其放入标准形式进行处理，那么拥有数据也不会让我们走得很远。接下来，我们将探讨一些进一步结构化我们的数据（矩阵）和理解我们的数据（统计学和概率）的技术。
