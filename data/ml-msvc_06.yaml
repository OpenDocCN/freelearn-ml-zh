- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Stabilizing the Machine Learning System
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稳定机器学习系统
- en: In the last two chapters, we went over the different concepts in machine learning
    and how we can create a comprehensive machine learning system pipeline that can
    work and adapt to our needs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后两章中，我们介绍了机器学习的不同概念以及我们如何创建一个综合的机器学习系统管道，该管道可以工作并适应我们的需求。
- en: While our pipeline can address our expectations, it is important for us to be
    able to maintain our system in the face of external factors to which it may be
    hard for the system to self-adjust.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的管道可以满足我们的期望，但在面对系统可能难以自我调整的外部因素时，对我们来说，能够维持我们的系统是非常重要的。
- en: In this chapter, we will discuss the phenomenon of dataset shifts and how we
    can optimize our machine learning system to help address these issues while maintaining
    its functional goal without having to rebuild our system from scratch.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论数据集变化的现象以及我们如何优化我们的机器学习系统来帮助解决这些问题，同时保持其功能目标，而无需从头开始重建我们的系统。
- en: 'We will be going over the following concepts:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍以下概念：
- en: Machine learning parameterization and dataset shifts
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习参数化和数据集变化
- en: The causes of dataset shifts
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集变化的原因
- en: Identifying dataset shifts
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别数据集变化
- en: Handling and stabilizing dataset shifts
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理和稳定数据集变化
- en: Machine learning parameterization and dataset shifts
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习参数化和数据集变化
- en: Maintaining our machine learning models is an integral part of creating a robust
    model. As time progresses, our data begins to morph and shift based on our environment,
    and while most models can detect and self-repair, sometimes, human intervention
    will be required to guide them back on track.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 维护我们的机器学习模型是创建一个健壮模型的重要组成部分。随着时间的推移，我们的数据开始根据我们的环境发生变化和移动，尽管大多数模型可以检测和自我修复，但有时需要人为干预来引导它们回到正确的轨道。
- en: 'In this section, we will briefly go over two main concepts that will help us
    understand the impact on our model:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要介绍两个主要概念，这些概念将帮助我们了解它们对我们模型的影响：
- en: '**Parameterization**'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数化**'
- en: '**Dataset shifts**'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集变化**'
- en: 'Our machine learning model is represented by certain specifications that help
    define the learning process of our model. These include the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们机器学习模型的表示由某些规格定义，这些规格有助于定义我们模型的学习过程。这些包括以下内容：
- en: '**Parameters**'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数**'
- en: '**Hyperparameters**'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数**'
- en: We will first look at parameters. These specifications are internal within the
    model. During the training process, these parameters are updated and learned while
    the model is trying to learn the mapping between the input features and the target
    values.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先来看参数。这些规格在模型内部。在训练过程中，这些参数在模型试图学习输入特征与目标值之间的映射关系时被更新和学习。
- en: Most of the time, these parameters are set to an initial value of either zeros
    or random values. As the training process happens, the values are continuously
    updated by an optimization method, such as gradient descent. At the end of the
    training process, the final weights of the values are what constitute the model
    itself. These weights can even be used for other models, especially those with
    similar applications.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，这些参数被设置为初始值为零或随机值。随着训练过程的进行，这些值通过优化方法（如梯度下降）不断更新。在训练过程结束时，这些值的最终权重构成了模型本身。这些权重甚至可以用于其他模型，特别是那些具有相似应用场景的模型。
- en: 'Some examples of parameters include the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的一些例子包括以下内容：
- en: Node weights and bias values for artificial neural networks
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经网络中的节点权重和偏置值
- en: Coefficients of linear and logistic regression models
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性和逻辑回归模型的系数
- en: Cluster centroids for clustering models
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类模型的聚类中心
- en: While parameters play a core role in determining the performance of a model,
    they are mostly out of our control since the model itself is what updates the
    weights. This leads us to hyperparameters.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然参数在确定模型性能方面起着核心作用，但由于模型本身更新权重，它们大多不受我们的控制。这导致我们转向超参数。
- en: Hyperparameters are parameters that control the learning process of our machine
    learning model, which, in turn, affects the output weights that our model learns.
    These values are set from the beginning and stay fixed throughout the learning
    process.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数是控制我们机器学习模型学习过程的参数，反过来，它又影响模型学习的输出权重。这些值从开始设置并保持在整个学习过程中固定。
- en: We, as users, determine which values to set in the beginning for our model to
    use during the training process. As a result, it takes time and experience to
    figure out which values produce the best results. There is effort involved in
    testing and training multiple variations of hyperparameters to see which performs
    the best.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作为用户，我们决定在训练过程中为我们的模型设置哪些初始值。因此，找出哪些值能产生最佳结果需要时间和经验。测试和训练多个超参数变体以查看哪个表现最佳需要付出努力。
- en: 'There are many hyperparameters and each model has its own unique set of hyperparameters
    that the user can modify. These hyperparameters can include the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多超参数，每个模型都有其独特的超参数集，用户可以修改。这些超参数可能包括以下内容：
- en: The split ratio between the training and testing datasets
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集和测试集之间的分割比例
- en: The learning rate used in optimization algorithms
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化算法中使用的学习率
- en: The choice of optimization algorithm
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化算法的选择
- en: The batch size
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批大小
- en: The number of epochs or iterations
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代次数或周期数
- en: The number of hidden layers
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层的数量
- en: The number of nodes in each hidden layer
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个隐藏层中的节点数量
- en: The choice of cost or loss function
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本或损失函数的选择
- en: The choice of activation function
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数的选择
- en: The number of ![](img/Formula_06_001.png) clusters
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要调整的聚类数量 ![img/Formula_06_001.png]
- en: Since there can be many hyperparameters to adjust and many different combinations
    to try, it can be very time-consuming to test these changes one by one. As discussed
    in the last chapter, it can be useful to have a section in our pipeline that automates
    this process by running multiple models with different combinations of hyperparameters
    to speed up the testing process and find the most optimal combination of hyperparameters.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可能有许多超参数需要调整，以及许多不同的组合需要尝试，逐一测试这些变化可能会非常耗时。正如上一章所讨论的，在我们的流程中有一个部分自动化这个过程，通过运行具有不同超参数组合的多个模型来加速测试过程并找到最优的超参数组合，这可能是有用的。
- en: '![Figure 6.1: Hyperparameter and parameter tuning](img/B18934_06_1.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1：超参数和参数调整](img/B18934_06_1.jpg)'
- en: 'Figure 6.1: Hyperparameter and parameter tuning'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：超参数和参数调整
- en: There may be cases where adjusting our parameters and hyperparameters is not
    enough for us to prevent our model from degrading.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 可能存在调整我们的参数和超参数不足以防止我们的模型退化的情况。
- en: For example, let’s say we create a machine learning model with a model accuracy
    of 85%. This model continues to perform well for some time. We then begin to see
    our model accuracy deteriorate until it becomes unusable, as the model is unable
    to properly predict the new test data we collect.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们创建了一个模型准确率为85%的机器学习模型。这个模型在一段时间内继续表现良好。然后我们开始看到我们的模型准确率下降，直到变得无法使用，因为模型无法正确预测我们收集的新测试数据。
- en: As we analyze our model, we can begin to see that our training data does not
    reflect the testing data we have recently collected. Here, we can see that there
    is a shift between the data distribution for our training and test datasets.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析我们的模型时，我们可以开始看到我们的训练数据并不反映我们最近收集的测试数据。在这里，我们可以看到我们的训练和测试数据集的数据分布之间存在变化。
- en: Before we work on resolving dataset shifts, we must first understand the background
    of dataset shifts, how they occur, and how we can adjust our machine learning
    system to help prevent dataset shifts from impacting our model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们着手解决数据集变化问题之前，我们首先必须了解数据集变化的背景、它们是如何发生的，以及我们如何调整我们的机器学习系统以帮助防止数据集变化影响我们的模型。
- en: Machine learning systems are built under the assumption that the data distribution
    between the training and test sets is similar. Since the real world is ever-changing,
    new data distributions emerge and there may be a significant difference between
    the training and test sets.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统是在假设训练集和测试集之间的数据分布相似的情况下构建的。由于现实世界不断变化，新的数据分布出现，训练集和测试集之间可能存在显著差异。
- en: 'The major difference in data distribution between the training and test sets
    is considered a dataset shift. This drastic difference will eventually degrade
    the model, as the model is biased to the training set and is unable to adapt to
    the test set:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和测试集之间数据分布的主要差异被认为是数据集变化。这种剧烈的差异最终会降低模型性能，因为模型偏向于训练集，无法适应测试集：
- en: '![Figure 6.2: Outcome of a machine learning model due to a dataset shift](img/B18934_06_2.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2：由于数据集变化导致的机器学习模型结果](img/B18934_06_2.jpg)'
- en: 'Figure 6.2: Outcome of a machine learning model due to a dataset shift'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：由于数据集变化导致的机器学习模型结果
- en: Some examples of this occurring include a shift in consumer habits, a socioeconomic
    shift, or a global influence, such as a pandemic. These events can heavily impact
    the data we collect and observe, which, in turn, can sway our model’s performance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况的一些例子包括消费者习惯的变化、社会经济变化或全球影响，如大流行。这些事件可以严重影响我们收集和观察到的数据，进而影响我们模型的性能。
- en: Important Note
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: First, try adjusting the hyperparameters of your machine learning model and
    see whether the newly learned parameters can improve your model significantly.
    If you still encounter major issues, it may be best to analyze the data and see
    whether a dataset shift has occurred.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，尝试调整你的机器学习模型的超参数，看看新学习的参数是否能显著提高你的模型。如果你仍然遇到主要问题，那么最好分析数据，看看是否发生了数据集偏移。
- en: The causes of dataset shifts
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集偏移的原因
- en: 'Now that we have learned what dataset shifts are, we can start to investigate
    the different causes of dataset shifts. While there are many different reasons
    dataset shifts can occur, we can split them into two categories:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了数据集偏移是什么，我们可以开始调查数据集偏移的不同原因。虽然数据集偏移可能发生的原因有很多，但我们可以将它们分为两类：
- en: '**Sample** **selection bias**'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本选择偏差**'
- en: '**Non-stationary environments**'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非平稳环境**'
- en: Sample selection bias is self-explanatory in that there is a bias or issue when
    it comes to labeling or collecting the training data used for the model. Collecting
    biased data will result in a non-uniform sample selection for the training set.
    That bias, in essence, will fail to represent the actual sample distribution.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 样本选择偏差在标签或收集用于模型的训练数据时是显而易见的，存在偏差或问题。收集有偏差的数据会导致训练集的非均匀样本选择。这种偏差本质上无法代表实际的样本分布。
- en: Non-stationary environments are another cause for dataset shifts – we will go
    into further detail about the different types later in the chapter. Let’s assume
    that we have a model with a set of input features, ![](img/Formula_06_002.png),
    a target or output variable ![](img/Formula_06_003.png). From there, we can also
    define the prior probability as ![](img/Formula_06_004.png), the conditional probability
    as ![](img/Formula_06_005.png), and the joint distribution as ![](img/Formula_06_006.png).
    This dataset shift is caused by temporal or spatial changes, defined as ![](img/Formula_06_007.png),
    which reflect very much how the real world operates.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 非平稳环境是数据集偏移的另一个原因——我们将在本章后面进一步详细讨论不同类型。假设我们有一个包含一组输入特征![](img/Formula_06_002.png)，目标或输出变量![](img/Formula_06_003.png)的模型。从那里，我们还可以定义先验概率为![](img/Formula_06_004.png)，条件概率为![](img/Formula_06_005.png)，联合分布为![](img/Formula_06_006.png)。这种数据集偏移是由时间或空间变化引起的，定义为![](img/Formula_06_007.png)，这非常反映现实世界的运作方式。
- en: 'This causal effect can lead to different types of shifts:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这种因果效应可能导致不同类型的偏移：
- en: For ![](img/Formula_06_008.png) problems, non-stationary environments can make
    changes to either ![](img/Formula_06_009.png) or ![](img/Formula_06_010.png),
    giving us a covariate or concept shift
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 ![公式_06_008.png](img/Formula_06_008.png) 问题，非平稳环境可以改变![](img/Formula_06_009.png)
    或 ![](img/Formula_06_010.png)，给我们带来协变量或概念上的偏移
- en: For ![](img/Formula_06_011.png) problems, a change in ![](img/Formula_06_012.png)
    or ![](img/Formula_06_013.png) can give us a prior probability or concept shift
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 ![公式_06_011.png](img/Formula_06_011.png) 问题，![](img/Formula_06_012.png) 或
    ![](img/Formula_06_013.png) 的变化可以给我们提供先验概率或概念上的转变
- en: In the next section, we will look into the different types of shifts and how
    we can identify them.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨不同类型的偏移以及我们如何识别它们。
- en: Identifying dataset shifts
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别数据集偏移
- en: After looking into the different causes of dataset shifts, we can begin to classify
    certain shifts into different groups that can help us easily identify the type
    of dataset shift we are dealing with.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究了数据集偏移的不同原因之后，我们可以开始将某些偏移分类到不同的组中，这有助于我们轻松识别我们正在处理的数据集偏移类型。
- en: 'Among the different dataset shifts we can encounter, we can classify data shifts
    into these categories:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可能遇到的不同数据集偏移中，我们可以将数据偏移分为以下类别：
- en: '**Covariate shifts**'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协变量偏移**'
- en: '**Prior** **probability shifts**'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**先验概率偏移**'
- en: '**Concept shifts**'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概念偏移**'
- en: We will first look at covariate shifts. This is the most common dataset shift,
    as a covariate shift occurs when there is a change in the distribution of one
    or more of the input features of the training or test data. Despite the change,
    the target value remains the same.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将研究协变量偏移。这是最常见的数据集偏移，因为协变量偏移发生在训练或测试数据的输入特征之一或多个的分布发生变化时。尽管发生了变化，但目标值保持不变。
- en: In mathematical terms, this dataset shift occurs only in X > Y problems. Whenever
    the input distribution, ![](img/Formula_06_014.png), changes between the training
    and testing datasets, ![](img/Formula_06_015.png), but the conditional probability
    of the training and testing dataset stays the same, ![](img/Formula_06_016.png),
    this will cause a covariate shift.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学的角度来看，这种数据集偏移只发生在X > Y问题中。每当输入分布，![](img/Formula_06_014.png)，在训练集和测试集之间发生变化，![](img/Formula_06_015.png)，但训练集和测试集的条件概率保持不变，![](img/Formula_06_016.png)，这将会导致协变量偏移。
- en: For example, we can create a model that predicts the salary of the employees
    of a certain city. Let’s say that the majority of the employees in your training
    set consist of younger individuals. After time passes, the employees get older.
    If you were to try to predict the salary of the older employees, you would begin
    to see a significant error. This is due to the model being heavily biased toward
    the training set, which consisted of mostly younger employees and is unable to
    find the relationship among the older employees.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以创建一个模型来预测某个城市员工的薪水。假设你的训练集中大多数员工是年轻人。随着时间的推移，员工会变老。如果你尝试预测老员工的薪水，你会开始看到显著的误差。这是由于模型对主要由年轻人组成的训练集存在严重偏差，无法找到老年员工之间的关系。
- en: '![Figure 6.3: Covariate dataset shifts](img/B18934_06_3.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3：协变量数据集偏移](img/B18934_06_3.jpg)'
- en: 'Figure 6.3: Covariate dataset shifts'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：协变量数据集偏移
- en: Next, we will be looking into prior probability shifts, also known as label
    shifts. This is the opposite of a covariate shift, as this shift occurs when the
    output distribution changes for a given output but the input distribution remains
    the same.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨先验概率偏移，也称为标签偏移。这与协变量偏移相反，因为这种偏移发生在给定输出分布发生变化，但输入分布保持不变的情况下。
- en: 'In mathematical terms, this occurs only in Y -> X problems. When the prior
    probability changes, ![](img/Formula_06_017.png), but the conditional probability
    remains the same, ![](img/Formula_06_018.png), a prior probability shift occurs:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学的角度来看，这种情况只发生在Y -> X问题中。当先验概率发生变化时，![](img/Formula_06_017.png)，但条件概率保持不变，![](img/Formula_06_018.png)，就会发生先验概率偏移：
- en: '![Figure 6.4: Prior probability shifts](img/B18934_06_4.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4：先验概率偏移](img/B18934_06_4.jpg)'
- en: 'Figure 6.4: Prior probability shifts'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：先验概率偏移
- en: Finally, we will discuss concept shifts, also known as concept drifts. This
    shift occurs when the distribution of the training data remains the same but the
    conditional distribution for the output given the training data changes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将讨论概念偏移，也称为概念漂移。这种偏移发生在训练数据的分布保持不变，但给定训练数据的输出条件分布发生变化时。
- en: 'In mathematical terms, this can occur both in X -> Y or Y -> X problems:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学的角度来看，这种情况可以在X -> Y或Y -> X问题中发生：
- en: For X -> Y problems, this occurs when the prior probability of the input variables
    remains the same in the training and testing datasets, (![](img/Formula_06_019.png),
    but the conditional probability changes, ![](img/Formula_06_020.png).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于X -> Y问题，这种情况发生在输入变量的先验概率在训练集和测试集中保持不变时，(![](img/Formula_06_019.png)，但条件概率发生变化，![](img/Formula_06_020.png)）。
- en: For Y -> X problems, this occurs when the prior probability of the target variables
    remains the same in the training and testing datasets, ![](img/Formula_06_021.png),
    but the conditional probability changes, ![](img/Formula_06_022.png).
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Y -> X问题，这种情况发生在目标变量的先验概率在训练集和测试集中保持不变时，![](img/Formula_06_021.png)，但条件概率发生变化，![](img/Formula_06_022.png)。
- en: As an example, a user’s purchasing behavior is affected due to the economy,
    but neither our training nor our test data contains any information regarding
    the economy’s performance. As a result, our model’s performance will degrade.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，由于经济原因，一个用户的购买行为受到影响，但我们的训练数据和测试数据中都没有包含关于经济表现的任何信息。因此，我们模型的性能将下降。
- en: '![Figure 6.5: Concept shifts](img/B18934_06_5.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5：概念偏移](img/B18934_06_5.jpg)'
- en: 'Figure 6.5: Concept shifts'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：概念偏移
- en: This can be a tricky dataset shift since the distribution shift is not related
    to the data that we train on, but rather external information that our model may
    not have. Most of the time, these dataset shifts are cyclical and/or seasonal.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一个棘手的数据集偏移，因为分布偏移与我们训练的数据无关，而是与我们的模型可能没有的外部信息有关。大多数情况下，这些数据集偏移是周期性的和/或季节性的。
- en: Important Note
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Visualizing your data and calculating the different probabilities with regard
    to your data is the best way to help determine and identify which dataset shift
    you are dealing with. From there, you can decide how you will address your dataset
    shift.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化你的数据并计算与你的数据相关的不同概率是帮助确定和识别你所处理的数据集偏移的最佳方式。从那里，你可以决定你将如何解决你的数据集偏移。
- en: 'When it comes to identifying most dataset shifts, there is a process that we
    can follow to help us. It includes the following steps:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到识别大多数数据集偏移时，我们可以遵循一个过程来帮助我们。它包括以下步骤：
- en: Preprocessing the data
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Creating random samples of your training and test sets on their own
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在各自的训练和测试集上创建随机样本
- en: Combining the random samples into one dataset
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将随机样本合并成一个数据集
- en: Create a model using one feature at a time while using the origin as the output
    value
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个特征一次创建一个模型，同时使用原始值作为输出值
- en: Predicting on the test set and calculating the **Area Under Curve – Receiver
    Operating Characteristics** **Curve** (**AUC-ROC**)
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在测试集上预测并计算**曲线下面积-接收者操作特征** **曲线**（**AUC-ROC**）
- en: If the AUC-ROC is greater than a certain threshold, for example, 80%, we can
    classify the data as having experienced a dataset shift
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果AUC-ROC大于某个特定阈值，例如，80%，我们可以将数据分类为经历了数据集偏移
- en: '![Figure 6.6: An example of an AUC-ROC graph (a value close to 1 indicates
    a strong model)](img/B18934_06_6.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6：AUC-ROC图示例（接近1的值表示模型强大）](img/B18934_06_6.jpg)'
- en: 'Figure 6.6: An example of an AUC-ROC graph (a value close to 1 indicates a
    strong model)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：AUC-ROC图示例（接近1的值表示模型强大）
- en: Handling and stabilizing dataset shifts
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理和稳定数据集偏移
- en: Now that we have established the methods for identifying the different types
    of dataset shifts, we can discuss the different ways of addressing these shifts
    and stabilizing our machine learning models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经建立了识别不同类型数据集偏移的方法，我们可以讨论解决这些偏移和稳定我们的机器学习模型的不同方式。
- en: 'While there are many ways to address dataset shifts, we will be looking at
    the three main methods. They consist of the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多方法可以解决数据集偏移，但我们将关注三种主要方法。它们包括以下内容：
- en: '**Feature dropping**'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征丢弃**'
- en: '**Adversarial search**'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对抗搜索**'
- en: '**Density** **ratio estimation**'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密度** **比率估计**'
- en: 'We will first look at feature dropping. This is the simplest form of adjusting
    dataset shifts. As we determine which features are classified as drifting, we
    can simply drop them from the machine learning model. We can also define a simple
    rule where any features with a drift value greater than a certain threshold, for
    example, 80%, can be dropped:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将查看特征丢弃。这是调整数据集偏移的最简单形式。当我们确定哪些特征被分类为漂移时，我们可以简单地从机器学习模型中丢弃它们。我们还可以定义一个简单的规则，即任何漂移值大于某个特定阈值（例如，80%）的特征都可以被丢弃：
- en: '![Figure 6.7: Feature Dropping Process](img/B18934_06_7.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7：特征丢弃过程](img/B18934_06_7.jpg)'
- en: 'Figure 6.7: Feature Dropping Process'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：特征丢弃过程
- en: While this is a simple change, this is something that needs to be considered
    carefully. If this feature is considered important when training your machine
    learning model, then it is worth reconsidering whether this feature needs to be
    dropped. Also, if the majority of your features pass the threshold for being dropped,
    you may want to revisit your data as a whole and consider a different approach
    when addressing your dataset shift.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是一个简单的变化，但这需要仔细考虑。如果在训练你的机器学习模型时认为这个特征很重要，那么重新考虑这个特征是否需要被丢弃是值得的。此外，如果你的大多数特征都通过了被丢弃的阈值，你可能需要重新审视你的整体数据，并在解决数据集偏移时考虑不同的方法。
- en: Next, we will look at adversarial search. This is a technique that requires
    training a binary classifier to predict whether the sample data is within the
    training or test datasets. We can then evaluate the performance of the classifier
    to determine whether there has been a dataset shift. If the performance of our
    classifier is close to that of a random guess (~50%), we can confidently determine
    that our training and test dataset distribution is consistent. On the other hand,
    if our classifier performs better than a random guess, then that will indicate
    an inconsistency between the distribution of the training and test datasets.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨对抗搜索。这是一种需要训练二元分类器来预测样本数据是否在训练或测试数据集中的技术。然后我们可以评估分类器的性能，以确定是否发生了数据集偏移。如果我们的分类器性能接近随机猜测（约50%），我们可以自信地确定我们的训练和测试数据集分布是一致的。另一方面，如果我们的分类器表现优于随机猜测，那么这表明训练和测试数据集的分布之间存在不一致。
- en: 'The adversarial search can be split into three parts:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗搜索可以分为三个部分：
- en: From the original dataset, we will remove the target value column and replace
    it with a new column that indicates the source of data (train = 0 and test = 1).
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从原始数据集中，我们将移除目标值列，并用一个新列来表示数据的来源（训练 = 0 和测试 = 1）。
- en: We will create and train the new classifier with the new dataset. The output
    of the classifier is the probability that the sample data is part of the test
    dataset.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用新的数据集创建和训练新的分类器。分类器的输出是样本数据属于测试数据集的概率。
- en: Finally, we can observe the results and measure the performance of our classifier.
    If our classifier performance is close to 50%, then this indicates that the model
    is unable to differentiate whether the data is coming from the training or test
    set. This can tell us that the data distribution between the training and test
    datasets is consistent. On the flip side, if our performance is close to 100%,
    then the model is confident enough to find the difference between the training
    and test datasets, which then indicates a major difference between the distribution
    of the training and test datasets.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以观察结果并衡量分类器的性能。如果我们的分类器性能接近50%，那么这表明模型无法区分数据是来自训练集还是测试集。这可以告诉我们训练和测试数据集之间的数据分布是一致的。另一方面，如果我们的性能接近100%，那么模型有足够的信心找到训练和测试数据集之间的差异，这表明训练和测试数据集的分布存在重大差异。
- en: '![Figure 6.8: Adversarial search process](img/B18934_06_8.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8：对抗搜索过程](img/B18934_06_8.jpg)'
- en: 'Figure 6.8: Adversarial search process'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8：对抗搜索过程
- en: 'Using adversarial search, we can establish three methods to address the dataset
    shifts we encounter:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对抗搜索，我们可以建立三种方法来解决我们遇到的数据集偏移问题：
- en: Using the results, we can use them as sample weights for the training process.
    The weights correspond to the nature of how the data is distributed. The data
    that is similar in the actual distribution will be assigned a larger weight while
    that with inconsistent distribution will be given a lower weight. This will help
    the model emphasize the data that actually represents the real distribution it
    is trying to learn.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用结果，我们可以将它们用作训练过程的样本权重。权重对应于数据分布的性质。在实际情况中分布相似的数据将被分配更大的权重，而分布不一致的数据将被分配更低的权重。这将有助于模型强调代表它试图学习的真实分布的实际数据。
- en: We can use only the top-ranked adversarial validation results. Rather than mitigating
    the weights of inconsistent samples in the testing dataset, we can remove them
    altogether.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用仅排名靠前的对抗验证结果。而不是减轻测试数据集中不一致样本的权重，我们可以完全删除它们。
- en: All data is used for training except for the top-ranked adversarial validation
    results. This method can address the issues that can arise from the second method
    by using all the data rather than dropping features. Rather than discarding unimportant
    data, we can incorporate some of the data in the training data for each fold when
    using K-fold cross-validation during training. This helps maintain consistency
    while using all the data.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了排名靠前的对抗验证结果外，所有数据都用于训练。这种方法通过使用所有数据而不是丢弃特征来解决可能由第二种方法引起的问题。在训练过程中使用K折交叉验证时，我们可以将一些数据纳入每个折的训练数据中，而不是丢弃不重要的数据。这样可以帮助我们在使用所有数据的同时保持一致性。
- en: The final method used to address dataset shifts is called the density ratio
    estimation method. This method is still under research and not a commonly used
    method to address dataset shifts.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 解决数据集偏移的最终方法被称为密度比估计法。这种方法仍在研究之中，并不是解决数据集偏移的常用方法。
- en: With this approach, we would first estimate the training and test dataset densities
    separately. Once we have done this, we will then estimate the importance of the
    dataset by taking the ratio of the estimated densities of the training and test
    datasets. Using this density ratio, we can use it as the weight for each data
    entry in our training dataset.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种方法，我们首先分别估计训练集和测试集的数据集密度。一旦我们完成了这个步骤，我们就会通过计算训练集和测试集估计密度的比率来估计数据集的重要性。使用这个密度比率，我们可以将其用作训练数据集中每个数据条目的权重。
- en: The reason this method is not preferred and is still under research is that
    it is computationally expensive, especially for higher dimensional datasets. Even
    then, the improvements it can bring to addressing dataset shifts are negligible
    and not worth the effort of pursuing this method.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法不受青睐且仍在研究中的原因是它计算成本高昂，尤其是对于高维数据集。即便如此，它对解决数据集偏移的改进微乎其微，不值得追求这种方法。
- en: Important Note
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Feature dropping is the easiest and simplest way to address dataset shifts.
    Consider using this approach before using the adversarial search approach, as
    that option, while effective, can be a little involved and may require more effort
    and resources to help mitigate the effect of dataset shifts.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 特征删除是解决数据集偏移最简单、最直接的方法。在考虑使用对抗搜索方法之前，考虑使用这种方法，因为虽然这种方法有效，但它可能比较复杂，可能需要更多的努力和资源来帮助减轻数据集偏移的影响。
- en: Summary
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we went over the general concepts of dataset shifts and how
    they can negatively impact our machine learning model.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了数据集偏移的一般概念以及它们如何对我们机器学习模型产生负面影响。
- en: From there, we delved in deeper into what causes these dataset shifts to occur
    and what different characteristics dataset shifts can exhibit. Using these characteristics,
    we can better identify the type of dataset shift – whether it was a covariate
    shift, prior probability shift, or concept shift.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，我们进一步深入研究了导致这些数据集偏移发生的原因以及数据集偏移可能展现的不同特征。利用这些特征，我们可以更好地识别数据集偏移的类型——是协变量偏移、先验概率偏移还是概念偏移。
- en: Once we were able to analyze our data and identify the type of dataset shift,
    we looked at different methods to help us handle and stabilize these dataset shifts
    so that we could maintain our machine learning model. We went over some techniques,
    such as feature searching, adversarial search, and density ratio estimation, that
    can assist us when dealing with dataset shifts.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们能够分析我们的数据并识别数据集偏移的类型，我们就研究了不同的方法来帮助我们处理和稳定这些数据集偏移，以便我们能够维持我们的机器学习模型。我们回顾了一些技术，如特征搜索、对抗搜索和密度比估计，这些技术可以帮助我们在处理数据集偏移时。
- en: Using these processes and methods, we can prevent our model from suffering from
    common dataset shifts that occur in the real world and continuously maintain our
    machine learning model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些过程和方法，我们可以防止我们的模型遭受在现实世界中发生的常见数据集偏移，并持续维护我们的机器学习模型。
- en: Now that we have a firm understanding of machine learning and how to maintain
    a robust model, we can start looking into how we can incorporate our machine learning
    models into our **Microservices** **Architecture** (**MSA**).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对机器学习和如何维护一个健壮的模型有了牢固的理解，我们可以开始研究如何将我们的机器学习模型集成到我们的**微服务架构**（**MSA**）中。
