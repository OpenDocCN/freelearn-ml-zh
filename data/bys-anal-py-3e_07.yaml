- en: ChapterÂ 8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬8ç« 
- en: Gaussian Processes
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ–¯è¿‡ç¨‹
- en: Lonely? You have yourself. Your infinite selves. - Rick Sanchez (at least the
    one from dimension C-137)
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å­¤å•å—ï¼Ÿä½ æœ‰ä½ è‡ªå·±ã€‚ä½ æ— é™çš„è‡ªå·±ã€‚- Rick Sanchezï¼ˆè‡³å°‘æ˜¯C-137ç»´åº¦ä¸­çš„é‚£ä¸ªï¼‰
- en: In the last chapter, we learned about the Dirichlet process, an infinite-dimensional
    generalization of the Dirichlet distribution that can be used to set a prior on
    an unknown continuous distribution. In this chapter, we will learn about the Gaussian
    process, an infinite-dimensional generalization of the Gaussian distribution that
    can be used to set a prior on unknown functions. Both the Dirichlet process and
    the Gaussian process are used in Bayesian statistics to build flexible models
    where the number of parameters is allowed to increase with the size of the data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ç‹„åˆ©å…‹é›·è¿‡ç¨‹ï¼Œè¿™æ˜¯ä¸€ç§ç‹„åˆ©å…‹é›·åˆ†å¸ƒçš„æ— é™ç»´æ¨å¹¿ï¼Œå¯ä»¥ç”¨æ¥å¯¹æœªçŸ¥çš„è¿ç»­åˆ†å¸ƒè®¾å®šå…ˆéªŒã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ é«˜æ–¯è¿‡ç¨‹ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ–¯åˆ†å¸ƒçš„æ— é™ç»´æ¨å¹¿ï¼Œå¯ä»¥ç”¨æ¥å¯¹æœªçŸ¥çš„å‡½æ•°è®¾å®šå…ˆéªŒã€‚ç‹„åˆ©å…‹é›·è¿‡ç¨‹å’Œé«˜æ–¯è¿‡ç¨‹éƒ½ç”¨äºè´å¶æ–¯ç»Ÿè®¡ä¸­ï¼Œæ„å»ºçµæ´»çš„æ¨¡å‹ï¼Œåœ¨è¿™äº›æ¨¡å‹ä¸­ï¼Œå‚æ•°çš„æ•°é‡å¯ä»¥éšç€æ•°æ®é‡çš„å¢åŠ è€Œå¢åŠ ã€‚
- en: 'We will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è®¨è®ºä»¥ä¸‹ä¸»é¢˜ï¼š
- en: Functions as probabilistic objects
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½œä¸ºæ¦‚ç‡å¯¹è±¡çš„å‡½æ•°
- en: Kernels
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ¸å‡½æ•°
- en: Gaussian processes with Gaussian likelihoods
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…·æœ‰é«˜æ–¯ä¼¼ç„¶çš„é«˜æ–¯è¿‡ç¨‹
- en: Gaussian processes with non-Gaussian likelihoods
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éé«˜æ–¯ä¼¼ç„¶ä¸‹çš„é«˜æ–¯è¿‡ç¨‹
- en: Hilbert space Gaussian process
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¸Œå°”ä¼¯ç‰¹ç©ºé—´ä¸­çš„é«˜æ–¯è¿‡ç¨‹
- en: 8.1 Linear models and non-linear data
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 çº¿æ€§æ¨¡å‹ä¸éçº¿æ€§æ•°æ®
- en: 'In *Chapter [4](CH04.xhtml#x1-760004)* and *Chapter [6](CH06.xhtml#x1-1200006)*
    we learned how to build models of the general form:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*ç¬¬[4ç« ](CH04.xhtml#x1-760004)*å’Œ*ç¬¬[6ç« ](CH06.xhtml#x1-1200006)*ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•æ„å»ºä»¥ä¸‹å½¢å¼çš„æ¨¡å‹ï¼š
- en: '![Î¸ = ğœ“ (Ï•(X )ğ›½ ) ](img/file216.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![Î¸ = ğœ“ (Ï•(X )ğ›½ ) ](img/file216.jpg)'
- en: Here, *Î¸* is a parameter for some probability distribution, for example, the
    mean of a Gaussian, the *p* parameter of the binomial, the rate of a Poisson,
    and so on. We call ![](img/phi.png) the inverse link function and ![](img/phi.png)
    is some other function we use to potentially transform the data, like a square
    root, a polynomial function, or something else.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œ*Î¸*æ˜¯æŸä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å‚æ•°ï¼Œä¾‹å¦‚ï¼Œé«˜æ–¯åˆ†å¸ƒçš„å‡å€¼ï¼ŒäºŒé¡¹åˆ†å¸ƒçš„*p*å‚æ•°ï¼Œæ³Šæ¾åˆ†å¸ƒçš„é€Ÿç‡ï¼Œç­‰ç­‰ã€‚æˆ‘ä»¬ç§°![](img/phi.png)ä¸ºé€†é“¾æ¥å‡½æ•°ï¼Œ![](img/phi.png)æ˜¯æˆ‘ä»¬ç”¨æ¥æ½œåœ¨åœ°å˜æ¢æ•°æ®çš„å…¶ä»–å‡½æ•°ï¼Œæ¯”å¦‚å¹³æ–¹æ ¹ã€å¤šé¡¹å¼å‡½æ•°ï¼Œæˆ–è€…å…¶ä»–å½¢å¼ã€‚
- en: Fitting, or learning, a Bayesian model can be seen as finding the posterior
    distribution of the weights *Î²*, and thus this is known as the weight view of
    approximating functions. As we already saw with polynomial and splines regression,
    by letting ![](img/phi.png) be a non-linear function, we can map the inputs onto
    a *feature space*. We also saw that by using a polynomial of the proper degree,
    we can perfectly fit any function. But unless we apply some form of regularization,
    for example, using prior distributions, this will lead to models that memorize
    the data, or in other words models with very poor generalizing properties. We
    also mention that splines can be as flexible as polynomials but with better statistical
    properties. We will now discuss Gaussian processes, which provide a principled
    solution to modeling arbitrary functions by effectively letting the data decide
    on the complexity of the function, while avoiding, or at least minimizing, the
    chance of overfitting.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ‹Ÿåˆæˆ–å­¦ä¹ ä¸€ä¸ªè´å¶æ–¯æ¨¡å‹å¯ä»¥çœ‹ä½œæ˜¯å¯»æ‰¾æƒé‡*Î²*çš„åéªŒåˆ†å¸ƒï¼Œå› æ­¤è¿™è¢«ç§°ä¸ºè¿‘ä¼¼å‡½æ•°çš„æƒé‡è§†è§’ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨å¤šé¡¹å¼å›å½’å’Œæ ·æ¡å›å½’ä¸­çœ‹åˆ°çš„é‚£æ ·ï¼Œé€šè¿‡è®©![](img/phi.png)æˆä¸ºä¸€ä¸ªéçº¿æ€§å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¾“å…¥æ˜ å°„åˆ°*ç‰¹å¾ç©ºé—´*ä¸­ã€‚æˆ‘ä»¬è¿˜çœ‹åˆ°ï¼Œé€šè¿‡ä½¿ç”¨é€‚å½“é˜¶æ•°çš„å¤šé¡¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥å®Œç¾åœ°æ‹Ÿåˆä»»ä½•å‡½æ•°ã€‚ä½†é™¤éæˆ‘ä»¬åº”ç”¨æŸç§å½¢å¼çš„æ­£åˆ™åŒ–ï¼Œä¾‹å¦‚ï¼Œä½¿ç”¨å…ˆéªŒåˆ†å¸ƒï¼Œå¦åˆ™è¿™å°†å¯¼è‡´è®°å¿†æ•°æ®çš„æ¨¡å‹ï¼Œæ¢å¥è¯è¯´ï¼Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›éå¸¸å·®ã€‚æˆ‘ä»¬è¿˜æåˆ°ï¼Œæ ·æ¡å›å½’å¯ä»¥åƒå¤šé¡¹å¼ä¸€æ ·çµæ´»ï¼Œä½†å…·æœ‰æ›´å¥½çš„ç»Ÿè®¡å±æ€§ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†è®¨è®ºé«˜æ–¯è¿‡ç¨‹ï¼Œå®ƒä¸ºé€šè¿‡æœ‰æ•ˆåœ°è®©æ•°æ®å†³å®šå‡½æ•°å¤æ‚åº¦æ¥å»ºæ¨¡ä»»æ„å‡½æ•°æä¾›äº†ä¸€ä¸ªæœ‰åŸåˆ™çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶é¿å…æˆ–è‡³å°‘æœ€å°åŒ–è¿‡æ‹Ÿåˆçš„å¯èƒ½æ€§ã€‚
- en: The following sections discuss Gaussian processes from a very practical point
    of view; we have avoided covering almost all the mathematics surrounding them.
    For a more formal explanation, you may read *Gaussian Processes for Machine Learning*
    by [Rasmussen and Williams](Bibliography.xhtml#Xrasmussen_2005)Â [[2005](Bibliography.xhtml#Xrasmussen_2005)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ç« èŠ‚ä»ä¸€ä¸ªéå¸¸å®é™…çš„è§’åº¦è®¨è®ºé«˜æ–¯è¿‡ç¨‹ï¼›æˆ‘ä»¬é¿å…äº†æ¶‰åŠå‡ ä¹æ‰€æœ‰ç›¸å…³çš„æ•°å­¦å†…å®¹ã€‚å¯¹äºæ›´æ­£å¼çš„è§£é‡Šï¼Œä½ å¯ä»¥é˜…è¯»[Rasmussenå’ŒWilliams](Bibliography.xhtml#Xrasmussen_2005)çš„ã€Š*Gaussian
    Processes for Machine Learning*ã€‹[[2005](Bibliography.xhtml#Xrasmussen_2005)]ã€‚
- en: 8.2 Modeling functions
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 å‡½æ•°å»ºæ¨¡
- en: 'We will begin our discussion of Gaussian processes by first describing a way
    to represent functions as probabilistic objects. We may think of a function *f*
    as a mapping from a set of inputs *X* to a set of outputs *Y* . Thus, we can write:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é€šè¿‡é¦–å…ˆæè¿°ä¸€ç§å°†å‡½æ•°è¡¨ç¤ºä¸ºæ¦‚ç‡å¯¹è±¡çš„æ–¹æ³•æ¥å¼€å§‹è®¨è®ºé«˜æ–¯è¿‡ç¨‹ã€‚æˆ‘ä»¬å¯ä»¥å°†ä¸€ä¸ªå‡½æ•° *f* è§†ä¸ºä»è¾“å…¥é›†åˆ *X* åˆ°è¾“å‡ºé›†åˆ *Y* çš„æ˜ å°„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å†™æˆï¼š
- en: '![Y = f(X ) ](img/file217.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Y = f(X ) ](img/file217.jpg)'
- en: One very crude way to represent functions is by listing for each *x*[*i*] value
    its corresponding *y*[*i*] value as in *Table [8.1](#x1-158002r1)*. You may remember
    this way of representing functions from elementary school.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ç¤ºå‡½æ•°çš„ä¸€ç§éå¸¸ç²—ç•¥çš„æ–¹å¼æ˜¯åˆ—å‡ºæ¯ä¸ª *x*[*i*] å€¼å¯¹åº”çš„ *y*[*i*] å€¼ï¼Œå¦‚ *è¡¨ [8.1](#x1-158002r1)* æ‰€ç¤ºã€‚ä½ å¯èƒ½è¿˜è®°å¾—è¿™ç§è¡¨ç¤ºå‡½æ•°çš„æ–¹å¼ï¼Œå®ƒæ¥è‡ªå°å­¦é˜¶æ®µã€‚
- en: '| *x* | *y* |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| *x* | *y* |'
- en: '| 0.00 | 0.46 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 0.00 | 0.46 |'
- en: '| 0.33 | 2.60 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 0.33 | 2.60 |'
- en: '| 0.67 | 5.90 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 0.67 | 5.90 |'
- en: '| 1.00 | 7.91 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 1.00 | 7.91 |'
- en: '**TableÂ 8.1**: A tabular representation of a function (sort of)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¡¨ 8.1**ï¼šä¸€ç§å‡½æ•°çš„è¡¨æ ¼è¡¨ç¤ºï¼ˆæŸç§ç¨‹åº¦ä¸Šï¼‰'
- en: As a general case, the values of *X* and *Y* will live on the real line; thus,
    we can see a function as a (potentially) infinite and ordered list of paired values
    (*x*[*i*]*,y*[*i*]). The order is important because, if we shuffle the values,
    we will get different functions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€èˆ¬æƒ…å†µï¼Œ*X* å’Œ *Y* çš„å€¼å°†ä½äºå®æ•°çº¿ä¸Šï¼›å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°†å‡½æ•°è§†ä¸ºä¸€ä¸ªï¼ˆå¯èƒ½ï¼‰æ— é™ä¸”æœ‰åºçš„å€¼å¯¹åˆ—è¡¨ (*x*[*i*]*,y*[*i*])ã€‚é¡ºåºå¾ˆé‡è¦ï¼Œå› ä¸ºå¦‚æœæˆ‘ä»¬æ‰“ä¹±è¿™äº›å€¼çš„é¡ºåºï¼Œå¾—åˆ°çš„å°†æ˜¯ä¸åŒçš„å‡½æ•°ã€‚
- en: Following this description, we can represent, numerically, any specific function
    we want. But what if we want to represent functions probabilistically? Well, we
    then need to encode a probabilitics mapping. Let me explain this; we can let each
    value be a random variable with some associated distribution. As working with
    Gaussians is usually convenient, letâ€™s say that they are distributed as a Gaussian
    with a given mean and variance. In this way, we no longer have the description
    of a single specific function, but the description of a family of distributions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®è¿™ä¸ªæè¿°ï¼Œæˆ‘ä»¬å¯ä»¥æ•°å€¼åœ°è¡¨ç¤ºæˆ‘ä»¬æƒ³è¦çš„ä»»ä½•ç‰¹å®šå‡½æ•°ã€‚ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦æ¦‚ç‡æ€§åœ°è¡¨ç¤ºå‡½æ•°æ€ä¹ˆåŠå‘¢ï¼Ÿé‚£ä¹ˆï¼Œæˆ‘ä»¬å°±éœ€è¦ç¼–ç ä¸€ä¸ªæ¦‚ç‡æ˜ å°„ã€‚è®©æˆ‘è§£é‡Šä¸€ä¸‹ï¼›æˆ‘ä»¬å¯ä»¥è®©æ¯ä¸ªå€¼éƒ½æ˜¯ä¸€ä¸ªå…·æœ‰æŸç§å…³è”åˆ†å¸ƒçš„éšæœºå˜é‡ã€‚ç”±äºä½¿ç”¨é«˜æ–¯åˆ†å¸ƒé€šå¸¸å¾ˆæ–¹ä¾¿ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾å®ƒä»¬æœä»å…·æœ‰ç»™å®šå‡å€¼å’Œæ–¹å·®çš„é«˜æ–¯åˆ†å¸ƒã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±ä¸å†æè¿°å•ä¸€çš„ç‰¹å®šå‡½æ•°ï¼Œè€Œæ˜¯æè¿°ä¸€ä¸ªåˆ†å¸ƒæ—ã€‚
- en: 'To make this discussion concrete, letâ€™s use some Python code to build and plot
    two examples of such functions:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿è®¨è®ºæ›´åŠ å…·ä½“ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€äº› Python ä»£ç æ¥æ„å»ºå¹¶ç»˜åˆ¶ä¸¤ä¸ªè¿™æ ·çš„å‡½æ•°ç¤ºä¾‹ï¼š
- en: '**CodeÂ 8.1**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.1**'
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![PIC](img/file218.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file218.png)'
- en: '**FigureÂ 8.1**: Two dummy functions sampled from Gaussian distributions'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 8.1**ï¼šä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·çš„ä¸¤ä¸ªè™šæ‹Ÿå‡½æ•°'
- en: '*Figure [8.1](#x1-158016r1)* shows that encoding functions using samples from
    Gaussian distributions is not that crazy or foolish, so we may be on the right
    track. Nevertheless, the approach used to generate *Figure [8.1](#x1-158016r1)*
    is limited and not sufficiently flexible.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ [8.1](#x1-158016r1)* æ˜¾ç¤ºäº†ä½¿ç”¨æ¥è‡ªé«˜æ–¯åˆ†å¸ƒçš„æ ·æœ¬æ¥ç¼–ç å‡½æ•°å¹¶éé‚£ä¹ˆç–¯ç‹‚æˆ–æ„šè ¢ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯èƒ½èµ°åœ¨æ­£ç¡®çš„é“è·¯ä¸Šã€‚ç„¶è€Œï¼Œç”¨äºç”Ÿæˆ
    *å›¾ [8.1](#x1-158016r1)* çš„æ–¹æ³•æ˜¯æœ‰é™çš„ï¼Œå¹¶ä¸”ä¸å¤Ÿçµæ´»ã€‚'
- en: While we expect real functions to have some structure or pattern, the way we
    express `the first one` function does not let us encode any relation between data
    points. In fact, each point is completely independent of the others, as we just
    get them as 10 independent samples from a common one-dimensional Gaussian distribution.
    For `the second one` function, we introduce some dependency. The mean of the point
    *y*[*i*+1] is the value *y*[*i*], thus we have some structure here. Nevertheless,
    we will see next that there is a more general approach to capturing dependencies,
    and not only between consecutive points.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æˆ‘ä»¬æœŸæœ›å®é™…çš„å‡½æ•°å…·æœ‰æŸç§ç»“æ„æˆ–æ¨¡å¼ï¼Œä½†æˆ‘ä»¬è¡¨ç¤º `ç¬¬ä¸€ä¸ª` å‡½æ•°çš„æ–¹å¼å¹¶æ²¡æœ‰è®©æˆ‘ä»¬ç¼–ç æ•°æ®ç‚¹ä¹‹é—´çš„ä»»ä½•å…³ç³»ã€‚å®é™…ä¸Šï¼Œæ¯ä¸ªç‚¹éƒ½æ˜¯å®Œå…¨ç‹¬ç«‹çš„ï¼Œå› ä¸ºæˆ‘ä»¬åªæ˜¯ä»ä¸€ä¸ªå…±åŒçš„ä¸€ç»´é«˜æ–¯åˆ†å¸ƒä¸­éšæœºæŠ½å–äº†
    10 ä¸ªç‹¬ç«‹çš„æ ·æœ¬ã€‚å¯¹äº `ç¬¬äºŒä¸ª` å‡½æ•°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€äº›ä¾èµ–å…³ç³»ã€‚ç‚¹ *y*[*i*+1] çš„å‡å€¼æ˜¯ *y*[*i*]ï¼Œå› æ­¤æˆ‘ä»¬è¿™é‡Œæœ‰ä¸€å®šçš„ç»“æ„ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„å†…å®¹ä¸­çœ‹åˆ°ï¼Œæ•æ‰ä¾èµ–å…³ç³»æœ‰ä¸€ç§æ›´ä¸ºé€šç”¨çš„æ–¹æ³•ï¼Œè€Œä¸ä»…ä»…æ˜¯è¿ç»­ç‚¹ä¹‹é—´çš„ä¾èµ–ã€‚
- en: Before continuing, let me stop for a moment and consider why weâ€™re using Gaussians
    and not any other probability distribution. First, by restricting ourselves to
    working with Gaussians, we do not lose any flexibility in specifying functions
    of different shapes, as each point has potentially its own mean and variance.
    Second, working with Gaussians is nice from a mathematical point of view.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»§ç»­ä¹‹å‰ï¼Œè®©æˆ‘åœä¸‹æ¥æ€è€ƒä¸€ä¸‹ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒè€Œä¸æ˜¯å…¶ä»–æ¦‚ç‡åˆ†å¸ƒã€‚é¦–å…ˆï¼Œé€šè¿‡å°†è‡ªå·±é™åˆ¶ä¸ºåªä½¿ç”¨é«˜æ–¯åˆ†å¸ƒï¼Œæˆ‘ä»¬åœ¨æŒ‡å®šä¸åŒå½¢çŠ¶çš„å‡½æ•°æ—¶ä¸ä¼šå¤±å»ä»»ä½•çµæ´»æ€§ï¼Œå› ä¸ºæ¯ä¸ªç‚¹å¯èƒ½æœ‰è‡ªå·±çš„å‡å€¼å’Œæ–¹å·®ã€‚å…¶æ¬¡ï¼Œä»æ•°å­¦çš„è§’åº¦æ¥çœ‹ï¼Œä½¿ç”¨é«˜æ–¯åˆ†å¸ƒæ˜¯éå¸¸æ–¹ä¾¿çš„ã€‚
- en: 8.3 Multivariate Gaussians and functions
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 å¤šå…ƒé«˜æ–¯å’Œå‡½æ•°
- en: In *Figure [8.1](#x1-158016r1)*, we represented a function as a collection of
    samples from 1-dimensional Gaussian distributions. One alternative is to use an
    n-dimensional multivariate Gaussian distribution to get a sample vector of length
    *n*. Actually, you may want to try to reproduce *Figure [8.1](#x1-158016r1)* but
    replacing `np.random.normal(0, 1, len(x))` with `np.random.multivariate_normal`,
    with a mean of `np.zeros_like(x)` and a standard deviation of `np.eye(len(x)`.
    The advantage of working with a Multivariate Normal is that we can use the covariance
    matrix to encode information about the function. For instance, by setting the
    covariance matrix to `np.eye(len(x))`, we are saying that each of the 10 points,
    where we are evaluating the function, has a variance of 1\. We are also saying
    that the variance between them, that is, their covariances, is 0\. In other words,
    they are independent. If we replace those zeros with other numbers, we could get
    covariances telling a different story.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*å›¾ [8.1](#x1-158016r1)*ä¸­ï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªå‡½æ•°è¡¨ç¤ºä¸ºä»ä¸€ç»´é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·å¾—åˆ°çš„é›†åˆã€‚å¦ä¸€ç§é€‰æ‹©æ˜¯ä½¿ç”¨nç»´å¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼Œå¾—åˆ°ä¸€ä¸ªé•¿åº¦ä¸º*n*çš„æ ·æœ¬å‘é‡ã€‚å®é™…ä¸Šï¼Œä½ å¯ä»¥å°è¯•é‡ç°*å›¾
    [8.1](#x1-158016r1)*ï¼Œä½†å°†`np.random.normal(0, 1, len(x))`æ›¿æ¢ä¸º`np.random.multivariate_normal`ï¼Œå‡å€¼ä¸º`np.zeros_like(x)`ï¼Œæ ‡å‡†å·®ä¸º`np.eye(len(x))`ã€‚ä½¿ç”¨å¤šå…ƒæ­£æ€åˆ†å¸ƒçš„ä¼˜åŠ¿åœ¨äºï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨åæ–¹å·®çŸ©é˜µæ¥ç¼–ç æœ‰å…³å‡½æ•°çš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œé€šè¿‡å°†åæ–¹å·®çŸ©é˜µè®¾ç½®ä¸º`np.eye(len(x))`ï¼Œæˆ‘ä»¬è¡¨æ˜æˆ‘ä»¬åœ¨è¯„ä¼°å‡½æ•°çš„10ä¸ªç‚¹ä¸­ï¼Œæ¯ä¸ªç‚¹çš„æ–¹å·®ä¸º1ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜å®ƒä»¬ä¹‹é—´çš„æ–¹å·®ï¼Œå³åæ–¹å·®ä¸º0ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒä»¬æ˜¯ç‹¬ç«‹çš„ã€‚å¦‚æœæˆ‘ä»¬å°†è¿™äº›é›¶æ›¿æ¢ä¸ºå…¶ä»–æ•°å€¼ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æè¿°ä¸åŒæ•…äº‹çš„åæ–¹å·®ã€‚
- en: I hope you are starting to get convinced that it is possible to use a multivariate
    Gaussian in order to represent functions. If thatâ€™s the case, then we just need
    to find a suitable covariance matrix. And thatâ€™s the topic of the next section.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›ä½ å·²ç»å¼€å§‹ç›¸ä¿¡ä½¿ç”¨å¤šå…ƒé«˜æ–¯æ¥è¡¨ç¤ºå‡½æ•°æ˜¯å¯èƒ½çš„ã€‚å¦‚æœæ˜¯è¿™æ ·ï¼Œé‚£ä¹ˆæˆ‘ä»¬åªéœ€è¦æ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„åæ–¹å·®çŸ©é˜µã€‚è¿™ä¹Ÿæ˜¯ä¸‹ä¸€èŠ‚çš„ä¸»é¢˜ã€‚
- en: 8.3.1 Covariance functions and kernels
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 åæ–¹å·®å‡½æ•°å’Œæ ¸å‡½æ•°
- en: In practice, covariance matrices are specified using functions known as kernels.
    Unfortunately, the term kernel is a very polysemic one, even in the statistical
    literature. An easy way to define a kernel is any function that returns a valid
    covariance matrix. But this is a tautological and not very intuitive definition.
    A more conceptual and useful definition is that a kernel defines a measure of
    similarity between data points in the input space, and this similarity determines
    how much influence one data point should have on predicting the value of another
    data point.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®è·µä¸­ï¼Œåæ–¹å·®çŸ©é˜µæ˜¯é€šè¿‡ç§°ä¸ºæ ¸å‡½æ•°çš„å‡½æ•°æ¥æŒ‡å®šçš„ã€‚ä¸å¹¸çš„æ˜¯ï¼Œæœ¯è¯­â€œæ ¸â€æ˜¯ä¸€ä¸ªå¤šä¹‰è¯ï¼Œç”šè‡³åœ¨ç»Ÿè®¡å­¦æ–‡çŒ®ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å®šä¹‰æ ¸å‡½æ•°çš„ä¸€ç§ç®€å•æ–¹æ³•æ˜¯ï¼Œä»»ä½•è¿”å›æœ‰æ•ˆåæ–¹å·®çŸ©é˜µçš„å‡½æ•°éƒ½å¯ä»¥ç§°ä¸ºæ ¸ã€‚ä½†è¿™ä¸ªå®šä¹‰æ˜¯è‡ªæˆ‘é‡å¤çš„ï¼Œä¸”ä¸å¤ªç›´è§‚ã€‚ä¸€ä¸ªæ›´å…·æ¦‚å¿µæ€§å’Œå®ç”¨æ€§çš„å®šä¹‰æ˜¯ï¼Œæ ¸å‡½æ•°å®šä¹‰äº†è¾“å…¥ç©ºé—´ä¸­æ•°æ®ç‚¹ä¹‹é—´çš„ç›¸ä¼¼åº¦åº¦é‡ï¼Œè€Œè¿™ç§ç›¸ä¼¼åº¦å†³å®šäº†ä¸€ä¸ªæ•°æ®ç‚¹å¯¹é¢„æµ‹å¦ä¸€ä¸ªæ•°æ®ç‚¹å€¼çš„å½±å“ç¨‹åº¦ã€‚
- en: 'There are many useful kernels, a popular one being the exponentiated quadratic
    kernel:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è®¸å¤šæœ‰ç”¨çš„æ ¸å‡½æ•°ï¼Œä¸€ä¸ªæµè¡Œçš„æ ¸å‡½æ•°æ˜¯æŒ‡æ•°äºŒæ¬¡æ ¸ï¼š
- en: '![ ( â€² 2) ğœ…(X,X â€²) = exp âˆ’ âˆ¥X--âˆ’-X-âˆ¥- 2â„“2 ](img/file219.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![ ( â€² 2) ğœ…(X,X â€²) = exp âˆ’ âˆ¥X--âˆ’-X-âˆ¥- 2â„“Â² ](img/file219.jpg)'
- en: 'Here, âˆ¥**X** âˆ’ **X**â€²âˆ¥Â² is the squared Euclidean distance:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œâˆ¥**X** âˆ’ **X**â€²âˆ¥Â²æ˜¯å¹³æ–¹æ¬§å‡ é‡Œå¾—è·ç¦»ï¼š
- en: '![âˆ¥X âˆ’ Xâ€²âˆ¥2 = (X1 âˆ’ X1â€²)2 + (X2 âˆ’ Xâ€²2)2 + â‹…â‹…â‹…+ (Xn âˆ’ Xâ€²n)2 ](img/file220.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![âˆ¥X âˆ’ Xâ€²âˆ¥Â² = (X1 âˆ’ X1â€²)Â² + (X2 âˆ’ Xâ€²2)Â² + â‹…â‹…â‹…+ (Xn âˆ’ Xâ€²n)Â²](img/file220.jpg)'
- en: For this kernel, we can see that we have a symmetric function that takes two
    inputs and returns a value of 0 if the inputs are the same, or positive otherwise.
    And thus we can interpret the output of the exponentiated quadratic kernel as
    a measure of similarity between the two inputs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªæ ¸å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒæ˜¯ä¸€ä¸ªå¯¹ç§°å‡½æ•°ï¼Œæ¥å—ä¸¤ä¸ªè¾“å…¥ï¼Œå¦‚æœè¾“å…¥ç›¸åŒåˆ™è¿”å›0ï¼Œå¦åˆ™è¿”å›æ­£å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°†æŒ‡æ•°äºŒæ¬¡æ ¸çš„è¾“å‡ºè§£é‡Šä¸ºä¸¤ä¸ªè¾“å…¥ä¹‹é—´çš„ç›¸ä¼¼åº¦åº¦é‡ã€‚
- en: It may not be obvious at first sight, but the exponentiated quadratic kernel
    has a similar formula to the Gaussian distribution. For this reason, this kernel
    is also called the Gaussian kernel. The term *â„“* is known as the length scale
    (or bandwidth or variance) and controls the width of the kernel. In other words,
    it controls at what scale the *X* values are considered similar.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹ä¸€çœ‹å¯èƒ½ä¸å¤ªæ˜æ˜¾ï¼Œä½†æŒ‡æ•°äºŒæ¬¡æ ¸çš„å…¬å¼ä¸é«˜æ–¯åˆ†å¸ƒéå¸¸ç›¸ä¼¼ã€‚æ­£å› ä¸ºå¦‚æ­¤ï¼Œè¿™ä¸ªæ ¸å‡½æ•°ä¹Ÿè¢«ç§°ä¸ºé«˜æ–¯æ ¸å‡½æ•°ã€‚æœ¯è¯­*â„“*è¢«ç§°ä¸ºé•¿åº¦å°ºåº¦ï¼ˆæˆ–å¸¦å®½æˆ–æ–¹å·®ï¼‰ï¼Œå®ƒæ§åˆ¶ç€æ ¸å‡½æ•°çš„å®½åº¦ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒæ§åˆ¶ç€*X*å€¼åœ¨ä»€ä¹ˆå°ºåº¦ä¸‹è¢«è®¤ä¸ºæ˜¯ç›¸ä¼¼çš„ã€‚
- en: 'To better understand the role of kernels, I recommend you play with them. For
    instance, letâ€™s define a Python function to compute the exponentiated quadratic
    kernel:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£æ ¸å‡½æ•°çš„ä½œç”¨ï¼Œæˆ‘å»ºè®®ä½ åŠ¨æ‰‹å°è¯•å®ƒä»¬ã€‚ä¾‹å¦‚ï¼Œå®šä¹‰ä¸€ä¸ª Python å‡½æ•°æ¥è®¡ç®—æŒ‡æ•°äºŒæ¬¡æ ¸å‡½æ•°ï¼š
- en: '**CodeÂ 8.2**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.2**'
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Figure [8.2](#x1-160009r2)* shows how a 4 Ã— 4 covariance matrix looks for
    different inputs. The input I chose is rather simple and consists of the values
    [âˆ’1*,*0*,*1*,*2].'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ [8.2](#x1-160009r2)* å±•ç¤ºäº†åœ¨ä¸åŒè¾“å…¥ä¸‹4 Ã— 4åæ–¹å·®çŸ©é˜µçš„æ ·å­ã€‚æˆ‘é€‰æ‹©çš„è¾“å…¥éå¸¸ç®€å•ï¼ŒåŒ…å«äº†å€¼ [âˆ’1*,*0*,*1*,*2]ã€‚'
- en: '![PIC](img/file221.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file221.png)'
- en: '**FigureÂ 8.2**: Input values (left), covariance matrix (right)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 8.2**ï¼šè¾“å…¥å€¼ï¼ˆå·¦ï¼‰ï¼Œåæ–¹å·®çŸ©é˜µï¼ˆå³ï¼‰'
- en: On the left panel of *Figure [8.2](#x1-160009r2)*, we have the input values.
    These are the values on the x-axis, and we have labeled the points from 0 to 3\.
    Thus, point 0 takes the value -1, point 1 takes 0, and so on. On the right panel,
    we have a heatmap representing the covariance matrix that we computed using the
    exponentiated quadratic kernel. The lighter the color, the larger the value of
    the covariance. As you can see, the heatmap is symmetric, with the diagonal taking
    the largest values. This makes sense when we realize that the value of each element
    in the covariance matrix is inversely proportional to the distance between the
    points, and the diagonal is the result of comparing each data point with itself.
    The smallest value is the one for the points 0 and 3, as they are the most distant
    points.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ *å›¾ [8.2](#x1-160009r2)* çš„å·¦é¢æ¿ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¾“å…¥å€¼ã€‚è¿™äº›æ˜¯ x è½´ä¸Šçš„å€¼ï¼Œæˆ‘ä»¬å°†ç‚¹ä» 0 æ ‡è®°åˆ° 3ã€‚å› æ­¤ï¼Œç‚¹ 0 å–å€¼
    -1ï¼Œç‚¹ 1 å–å€¼ 0ï¼Œä»¥æ­¤ç±»æ¨ã€‚åœ¨å³é¢æ¿ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨æŒ‡æ•°äºŒæ¬¡æ ¸å‡½æ•°è®¡ç®—å¾—åˆ°çš„åæ–¹å·®çŸ©é˜µçš„çƒ­åŠ›å›¾ã€‚é¢œè‰²è¶Šæµ…ï¼Œåæ–¹å·®å€¼è¶Šå¤§ã€‚å¦‚ä½ æ‰€è§ï¼Œçƒ­åŠ›å›¾æ˜¯å¯¹ç§°çš„ï¼Œä¸”å¯¹è§’çº¿ä¸Šçš„å€¼æœ€å¤§ã€‚å½“æˆ‘ä»¬æ„è¯†åˆ°åæ–¹å·®çŸ©é˜µä¸­æ¯ä¸ªå…ƒç´ çš„å€¼ä¸ç‚¹ä¹‹é—´çš„è·ç¦»æˆåæ¯”æ—¶ï¼Œè¿™ä¸€ç‚¹æ˜¯æœ‰æ„ä¹‰çš„ï¼Œå¯¹è§’çº¿æ˜¯é€šè¿‡å°†æ¯ä¸ªæ•°æ®ç‚¹ä¸è‡ªèº«è¿›è¡Œæ¯”è¾ƒå¾—åˆ°çš„ã€‚æœ€å°çš„å€¼å‡ºç°åœ¨ç‚¹
    0 å’Œç‚¹ 3ï¼Œå› ä¸ºå®ƒä»¬æ˜¯æœ€è¿œçš„ä¸¤ä¸ªç‚¹ã€‚
- en: Once you understand this example, you should try it with other inputs. See exercise
    1 at the end of this chapter and the accompanying notebook ( [https://github.com/aloctavodia/BAP3](https://github.com/aloctavodia/BAP3))
    for further practice.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä½ ç†è§£äº†è¿™ä¸ªä¾‹å­ï¼Œä½ åº”è¯¥å°è¯•ä½¿ç”¨å…¶ä»–è¾“å…¥ã€‚è¯·å‚è€ƒæœ¬ç« æœ«çš„ç»ƒä¹  1 ä»¥åŠé™„å¸¦çš„ç¬”è®°æœ¬ï¼ˆ[https://github.com/aloctavodia/BAP3](https://github.com/aloctavodia/BAP3)ï¼‰è¿›è¡Œè¿›ä¸€æ­¥ç»ƒä¹ ã€‚
- en: Now that we have a better grasp of how to use a kernel to generate a covariance
    matrix, letâ€™s move one step further and use the covariance matrix to sample functions.
    As you can see in *Figure [8.3](#x1-160010r3)*, a Gaussian kernel implies a wide
    variety of functions with the parameter *â„“* controlling the smoothness of the
    functions. The larger the value of *â„“*, the smoother the function.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æ›´å¥½åœ°ç†è§£äº†å¦‚ä½•ä½¿ç”¨æ ¸å‡½æ•°ç”Ÿæˆåæ–¹å·®çŸ©é˜µï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬æ›´è¿›ä¸€æ­¥ï¼Œä½¿ç”¨åæ–¹å·®çŸ©é˜µæ¥é‡‡æ ·å‡½æ•°ã€‚å¦‚ä½ åœ¨ *å›¾ [8.3](#x1-160010r3)*
    ä¸­æ‰€è§ï¼Œé«˜æ–¯æ ¸å‡½æ•°æ„å‘³ç€ä¸€ç³»åˆ—ä¸åŒçš„å‡½æ•°ï¼Œè€Œå‚æ•° *â„“* æ§åˆ¶å‡½æ•°çš„å¹³æ»‘åº¦ã€‚*â„“* çš„å€¼è¶Šå¤§ï¼Œå‡½æ•°è¶Šå¹³æ»‘ã€‚
- en: '![PIC](img/file222.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file222.png)'
- en: '**FigureÂ 8.3**: Realizations of a Gaussian kernel for four values of *â„“* (two
    realization per value of *â„“*)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 8.3**ï¼šå››ä¸ª *â„“* å€¼çš„é«˜æ–¯æ ¸å‡½æ•°çš„å®ç°ï¼ˆæ¯ä¸ª *â„“* å€¼å¯¹åº”ä¸¤ä¸ªå®ç°ï¼‰'
- en: Show me Your Friends and Iâ€™ll Show you your Future
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å‘Šè¯‰æˆ‘ä½ çš„æœ‹å‹ï¼Œæˆ‘å°±èƒ½å‘Šè¯‰ä½ ä½ çš„æœªæ¥
- en: The kernel translates the distance of the data points along the x axis to values
    of covariances for values of the expected function (on the y axis). Thus, the
    closest two points are on the x axis; the most similar we expect their values
    to be on the y axis.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¸å‡½æ•°å°†æ•°æ®ç‚¹æ²¿ x è½´çš„è·ç¦»è½¬åŒ–ä¸ºæœŸæœ›å‡½æ•°å€¼ï¼ˆy è½´ï¼‰çš„åæ–¹å·®å€¼ã€‚å› æ­¤ï¼Œæ•°æ®ç‚¹åœ¨ x è½´ä¸Šè¶Šæ¥è¿‘ï¼Œæˆ‘ä»¬æœŸæœ›å®ƒä»¬åœ¨ y è½´ä¸Šçš„å€¼è¶Šç›¸ä¼¼ã€‚
- en: 8.4 Gaussian processes
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 é«˜æ–¯è¿‡ç¨‹
- en: 'Now we are ready to understand what Gaussian processes (GPs) are and how they
    are used in practice. A somewhat formal definition of GPs, taken from Wikipedia,
    is as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½ç†è§£ä»€ä¹ˆæ˜¯é«˜æ–¯è¿‡ç¨‹ï¼ˆGPsï¼‰ä»¥åŠå®ƒä»¬åœ¨å®è·µä¸­çš„åº”ç”¨ã€‚æ ¹æ®ç»´åŸºç™¾ç§‘ï¼Œé«˜æ–¯è¿‡ç¨‹çš„ä¸€ä¸ªæ­£å¼å®šä¹‰å¦‚ä¸‹ï¼š
- en: â€The collection of random variables indexed by time or space, such that every
    finite collection of those random variables has a MultivariateNormal distribution,
    i.e. every finite linear combination of them is normally distributed.â€
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: â€œç”±æ—¶é—´æˆ–ç©ºé—´ç´¢å¼•çš„éšæœºå˜é‡çš„é›†åˆï¼Œä½¿å¾—è¿™äº›éšæœºå˜é‡çš„æ¯ä¸ªæœ‰é™é›†åˆéƒ½å…·æœ‰å¤šå…ƒæ­£æ€åˆ†å¸ƒï¼Œå³å®ƒä»¬çš„æ¯ä¸ªæœ‰é™çº¿æ€§ç»„åˆæœä»æ­£æ€åˆ†å¸ƒã€‚â€
- en: This is probably not a very useful definition, at least not at this stage of
    your learning path. The trick to understanding Gaussian processes is to realize
    that the concept of GP is a mental (and mathematical) scaffold, since, in practice,
    we do not need to directly work with this infinite mathematical object. Instead,
    we only evaluate the GPs at the points where we have data. By doing this, we collapse
    the infinite-dimensional GP into a finite multivariate Gaussian distribution with
    as many dimensions as data points. Mathematically, this collapse is achieved by
    marginalization over the infinitely unobserved dimensions. The theory assures
    us that it is OK to omit (actually marginalize over) all points, except the ones
    we are observing. It also guarantees that we will always get a multivariate Gaussian
    distribution. Thus, we can rigorously interpret *Figure [8.3](#x1-160010r3)* as
    actual samples from a Gaussian process!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå®šä¹‰å¯èƒ½å¹¶ä¸æ˜¯å¾ˆæœ‰ç”¨ï¼Œè‡³å°‘åœ¨ä½ å½“å‰çš„å­¦ä¹ é˜¶æ®µä¸å¤ªæœ‰ç”¨ã€‚ç†è§£é«˜æ–¯è¿‡ç¨‹çš„è¯€çªåœ¨äºæ„è¯†åˆ°ï¼Œé«˜æ–¯è¿‡ç¨‹çš„æ¦‚å¿µæ˜¯ä¸€ä¸ªå¿ƒç†ï¼ˆå’Œæ•°å­¦ï¼‰æ¡†æ¶ï¼Œå› ä¸ºåœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬å¹¶ä¸éœ€è¦ç›´æ¥å¤„ç†è¿™ä¸ªæ— é™ç»´çš„æ•°å­¦å¯¹è±¡ã€‚ç›¸åï¼Œæˆ‘ä»¬åªåœ¨æœ‰æ•°æ®çš„ç‚¹ä¸Šè¯„ä¼°é«˜æ–¯è¿‡ç¨‹ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬å°†æ— é™ç»´çš„é«˜æ–¯è¿‡ç¨‹å‹ç¼©æˆä¸€ä¸ªæœ‰é™ç»´çš„å¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼Œç»´åº¦æ•°ä¸æ•°æ®ç‚¹çš„æ•°é‡ç›¸ç­‰ã€‚ä»æ•°å­¦ä¸Šè®²ï¼Œè¿™ç§å‹ç¼©æ˜¯é€šè¿‡å¯¹æ— é™æœªè§‚å¯Ÿåˆ°çš„ç»´åº¦è¿›è¡Œè¾¹é™…åŒ–æ¥å®ç°çš„ã€‚ç†è®ºå‘Šè¯‰æˆ‘ä»¬ï¼Œé™¤äº†æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„ç‚¹ï¼Œå¿½ç•¥ï¼ˆå®é™…ä¸Šæ˜¯è¾¹é™…åŒ–ï¼‰æ‰€æœ‰å…¶ä»–ç‚¹æ˜¯å¯ä»¥çš„ã€‚å®ƒè¿˜ä¿è¯æˆ‘ä»¬æ€»æ˜¯ä¼šå¾—åˆ°ä¸€ä¸ªå¤šå…ƒé«˜æ–¯åˆ†å¸ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä¸¥æ ¼åœ°è§£é‡Š*å›¾
    [8.3](#x1-160010r3)* ä¸ºé«˜æ–¯è¿‡ç¨‹çš„å®é™…æ ·æœ¬ï¼
- en: So far we have focused on the covariance matrix of the MultivariateNormal and
    we have not discussed the mean. Setting the mean of a multivariate Gaussian at
    0 is common practice when working with GPs, since they are flexible enough to
    model the mean arbitrarily well. But notice that there is no restriction in doing
    so. Actually, for some problems, you may want to model the mean parametrically
    and leave the GP to model the residuals.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå¤šå…ƒæ­£æ€åˆ†å¸ƒçš„åæ–¹å·®çŸ©é˜µï¼Œå¹¶æ²¡æœ‰è®¨è®ºå‡å€¼ã€‚åœ¨ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹æ—¶ï¼Œå°†å¤šå…ƒé«˜æ–¯çš„å‡å€¼è®¾ç½®ä¸º0æ˜¯å¸¸è§åšæ³•ï¼Œå› ä¸ºé«˜æ–¯è¿‡ç¨‹è¶³å¤Ÿçµæ´»ï¼Œå¯ä»¥å°†å‡å€¼å»ºæ¨¡å¾—éå¸¸å¥½ã€‚ä½†è¯·æ³¨æ„ï¼Œè¿™æ ·åšå¹¶æ²¡æœ‰é™åˆ¶ã€‚å®é™…ä¸Šï¼Œå¯¹äºæŸäº›é—®é¢˜ï¼Œä½ å¯èƒ½å¸Œæœ›å‚æ•°åŒ–åœ°å»ºæ¨¡å‡å€¼ï¼Œè€Œå°†é«˜æ–¯è¿‡ç¨‹ç”¨æ¥å»ºæ¨¡æ®‹å·®ã€‚
- en: GPs are Prior Over Functions
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ–¯è¿‡ç¨‹æ˜¯å¯¹å‡½æ•°çš„å…ˆéªŒ
- en: Gaussian processes are prior distributions over functions in such a way that
    at each point that you evaluate a function, it places a Gaussian distribution
    with a given mean and variance. In practice, GPs are usually built using kernels,
    which turn distance on an x axis into similarities on the y axis.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ–¯è¿‡ç¨‹æ˜¯å¯¹å‡½æ•°çš„å…ˆéªŒåˆ†å¸ƒï¼Œä½¿å¾—åœ¨ä½ è¯„ä¼°å‡½æ•°çš„æ¯ä¸ªç‚¹æ—¶ï¼Œå®ƒä¼šåœ¨è¯¥ç‚¹æ”¾ç½®ä¸€ä¸ªå…·æœ‰ç»™å®šå‡å€¼å’Œæ–¹å·®çš„é«˜æ–¯åˆ†å¸ƒã€‚åœ¨å®è·µä¸­ï¼Œé«˜æ–¯è¿‡ç¨‹é€šå¸¸æ˜¯é€šè¿‡ä½¿ç”¨æ ¸å‡½æ•°æ¥æ„å»ºçš„ï¼Œæ ¸å‡½æ•°å°†xè½´ä¸Šçš„è·ç¦»è½¬åŒ–ä¸ºyè½´ä¸Šçš„ç›¸ä¼¼åº¦ã€‚
- en: 8.5 Gaussian process regression
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 é«˜æ–¯è¿‡ç¨‹å›å½’
- en: 'Letâ€™s assume we can model a variable *Y* as a function *f* of *X* plus some
    Gaussian noise:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬å¯ä»¥å°†ä¸€ä¸ªå˜é‡*Y*å»ºæ¨¡ä¸º*X*çš„å‡½æ•°*f*åŠ ä¸Šä¸€äº›é«˜æ–¯å™ªå£°ï¼š
- en: '![Y âˆ¼ ğ’© (Î¼ = f(X ),Ïƒ = ğœ–) ](img/file223.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![Y âˆ¼ ğ’© (Î¼ = f(X ),Ïƒ = ğœ–) ](img/file223.jpg)'
- en: 'If *f* is a linear function of *X*, then this assumption is essentially the
    same one we used in *Chapter [4](CH04.xhtml#x1-760004)* when we discussed simple
    linear regression. In this chapter, instead, we are going to use a more general
    expression for *f* by setting a prior over it. In that way, we will be able to
    get more complex functions than linear. If we decided to use Gaussian processes
    as this prior, then we can write:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ*f*æ˜¯*X*çš„çº¿æ€§å‡½æ•°ï¼Œé‚£ä¹ˆè¿™ä¸ªå‡è®¾æœ¬è´¨ä¸Šä¸æˆ‘ä»¬åœ¨*ç¬¬[4ç« ](CH04.xhtml#x1-760004)*è®¨è®ºç®€å•çº¿æ€§å›å½’æ—¶ä½¿ç”¨çš„å‡è®¾ç›¸åŒã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡å¯¹*f*è®¾ç½®å…ˆéªŒæ¥ä½¿ç”¨ä¸€ä¸ªæ›´ä¸€èˆ¬çš„è¡¨è¾¾å¼ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å°†èƒ½å¤Ÿå¾—åˆ°æ¯”çº¿æ€§æ›´å¤æ‚çš„å‡½æ•°ã€‚å¦‚æœæˆ‘ä»¬å†³å®šä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ä½œä¸ºè¿™ä¸ªå…ˆéªŒï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å†™æˆï¼š
- en: '![ â€² f(X ) = ğ’¢ğ’« (Î¼X,ğœ…(X, X )) ](img/file224.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![ â€² f(X ) = ğ’¢ğ’« (Î¼X,ğœ…(X, X )) ](img/file224.jpg)'
- en: Here, ![](img/GP.PNG) represents a Gaussian process with the mean function *Î¼*[*X*]
    and covariance function *K*(*X,X*â€²). Even though in practice, we always work with
    finite objects, we used the word **function** to indicate that mathematically,
    the mean and covariance are infinite objects.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œ![](img/GP.PNG)è¡¨ç¤ºä¸€ä¸ªå‡å€¼å‡½æ•°*Î¼*[*X*]å’Œåæ–¹å·®å‡½æ•°*K*(*X,X*â€²)çš„é«˜æ–¯è¿‡ç¨‹ã€‚å°½ç®¡åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬æ€»æ˜¯å¤„ç†æœ‰é™å¯¹è±¡ï¼Œä½†æˆ‘ä»¬ä½¿ç”¨**å‡½æ•°**è¿™ä¸ªè¯æ¥è¡¨ç¤ºä»æ•°å­¦ä¸Šè®²ï¼Œå‡å€¼å’Œåæ–¹å·®æ˜¯æ— é™å¯¹è±¡ã€‚
- en: I mentioned before that working with Gaussians is nice. For instance, if the
    prior distribution is a GP and the likelihood is a Gaussian distribution, then
    the posterior is also a GP and we can compute it analytically. Additionally, its
    nice to have a Gaussian likelihood because we can marginalize out the GP, which
    hugely reduces the size of the parameter space we need to sample from. The GP
    module in PyMC takes advantage of this and then it has different implementations
    for Gaussian and non-Gaussian likelihoods. In the next sections, we will explore
    both.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¹‹å‰æåˆ°è¿‡ï¼Œä½¿ç”¨é«˜æ–¯åˆ†å¸ƒéå¸¸æ–¹ä¾¿ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå…ˆéªŒåˆ†å¸ƒæ˜¯ä¸€ä¸ªé«˜æ–¯è¿‡ç¨‹ï¼Œè€Œä¼¼ç„¶æ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œé‚£ä¹ˆåéªŒåˆ†å¸ƒä¹Ÿæ˜¯é«˜æ–¯è¿‡ç¨‹ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯ä»¥è§£æåœ°è®¡ç®—å®ƒã€‚æ­¤å¤–ï¼Œä½¿ç”¨é«˜æ–¯ä¼¼ç„¶çš„å¥½å¤„æ˜¯æˆ‘ä»¬å¯ä»¥è¾¹é™…åŒ–é«˜æ–¯è¿‡ç¨‹ï¼Œè¿™å¤§å¤§å‡å°‘äº†æˆ‘ä»¬éœ€è¦ä»ä¸­æŠ½æ ·çš„å‚æ•°ç©ºé—´çš„å¤§å°ã€‚PyMCä¸­çš„é«˜æ–¯è¿‡ç¨‹æ¨¡å—åˆ©ç”¨äº†è¿™ä¸€ç‚¹ï¼Œå¹¶ä¸ºé«˜æ–¯å’Œéé«˜æ–¯ä¼¼ç„¶æä¾›äº†ä¸åŒçš„å®ç°æ–¹å¼ã€‚åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢è¿™ä¸¤è€…ã€‚
- en: 8.6 Gaussian process regression with PyMC
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.6 ä½¿ç”¨PyMCè¿›è¡Œé«˜æ–¯è¿‡ç¨‹å›å½’
- en: The gray line in *Figure [8.4](#x1-163002r4)* is a sin function. We are going
    to assume we donâ€™t know this function and instead, all we have is a set of data
    points (dots). Then we use a Gaussian process to approximate the function that
    generated those data points.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ [8.4](#x1-163002r4)*ä¸­çš„ç°è‰²çº¿æ˜¯ä¸€ä¸ªæ­£å¼¦å‡½æ•°ã€‚æˆ‘ä»¬å‡è®¾æˆ‘ä»¬ä¸çŸ¥é“è¿™ä¸ªå‡½æ•°ï¼Œè€Œæ˜¯åªæœ‰ä¸€ç»„æ•°æ®ç‚¹ï¼ˆç‚¹ï¼‰ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹æ¥é€¼è¿‘ç”Ÿæˆè¿™äº›æ•°æ®ç‚¹çš„å‡½æ•°ã€‚'
- en: '![PIC](img/file225.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file225.png)'
- en: '**FigureÂ 8.4**: Synthetic data (dots) generated from a known function (line)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 8.4**ï¼šä»å·²çŸ¥å‡½æ•°ï¼ˆçº¿ï¼‰ç”Ÿæˆçš„åˆæˆæ•°æ®ï¼ˆç‚¹ï¼‰'
- en: GPs are implemented in PyMC as a series of Python classes that deviate a little
    bit from what we have seen in previous models; nevertheless, the code is still
    very *PyMConic*. I have added a few comments in the following code to guide you
    through the key steps of defining a GP with PyMC.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ–¯è¿‡ç¨‹åœ¨PyMCä¸­å®ç°ä¸ºä¸€ç³»åˆ—Pythonç±»ï¼Œè¿™äº›ç±»ä¸æˆ‘ä»¬ä¹‹å‰è§è¿‡çš„æ¨¡å‹ç¨æœ‰ä¸åŒï¼›ç„¶è€Œï¼Œä»£ç ä¾ç„¶éå¸¸*PyMConic*ã€‚æˆ‘åœ¨æ¥ä¸‹æ¥çš„ä»£ç ä¸­æ·»åŠ äº†ä¸€äº›æ³¨é‡Šï¼Œå¸®åŠ©ä½ ç†è§£åœ¨PyMCä¸­å®šä¹‰é«˜æ–¯è¿‡ç¨‹çš„å…³é”®æ­¥éª¤ã€‚
- en: '**CodeÂ 8.3**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.3**'
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice that instead of a Gaussian likelihood, we have used the `gp.marginal_likelihood`
    method. This method takes advantage of the fact that the posterior has a closed
    form, as explained in the previous section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨é«˜æ–¯ä¼¼ç„¶ï¼Œè€Œæ˜¯ä½¿ç”¨äº†`gp.marginal_likelihood`æ–¹æ³•ã€‚è¿™ä¸ªæ–¹æ³•åˆ©ç”¨äº†åéªŒåˆ†å¸ƒæœ‰å°é—­å½¢å¼è¿™ä¸€äº‹å®ï¼Œæ­£å¦‚å‰é¢éƒ¨åˆ†æ‰€è§£é‡Šçš„é‚£æ ·ã€‚
- en: OK, now that we have computed the posterior, letâ€™s see how to get predictions
    of the mean fitted function. We can do this by computing the conditional distribution
    evaluated over new input locations using `gp.conditional`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œç°åœ¨æˆ‘ä»¬å·²ç»è®¡ç®—äº†åéªŒåˆ†å¸ƒï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å¾—åˆ°å‡å€¼æ‹Ÿåˆå‡½æ•°çš„é¢„æµ‹ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨`gp.conditional`è®¡ç®—åœ¨æ–°è¾“å…¥ä½ç½®ä¸Šè¯„ä¼°çš„æ¡ä»¶åˆ†å¸ƒæ¥å®ç°è¿™ä¸€ç‚¹ã€‚
- en: '**CodeÂ 8.4**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.4**'
- en: '[PRE3]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As a result, we get a new PyMC random variable, `f_pred`, which we can use
    to get samples from the posterior predictive distribution:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªæ–°çš„PyMCéšæœºå˜é‡`f_pred`ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥ä»åéªŒé¢„æµ‹åˆ†å¸ƒä¸­è·å–æ ·æœ¬ï¼š
- en: '**CodeÂ 8.5**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.5**'
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now we can plot the fitted functions over the original data, to visually inspect
    how well they fit the data and the associated uncertainty in our predictions.
    As we did with linear models in *Chapter [4](CH04.xhtml#x1-760004)*, we are going
    to show different ways to plot the same results. *Figure [8.5](#x1-163035r5)*
    shows lines from the fitted function.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥åœ¨åŸå§‹æ•°æ®ä¸Šç»˜åˆ¶æ‹Ÿåˆå‡½æ•°ï¼Œä»¥ç›´è§‚æ£€æŸ¥å®ƒä»¬ä¸æ•°æ®çš„æ‹Ÿåˆç¨‹åº¦ä»¥åŠæˆ‘ä»¬é¢„æµ‹çš„ç›¸å…³ä¸ç¡®å®šæ€§ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨*ç¬¬[4](CH04.xhtml#x1-760004)ç« *ä¸­å¯¹çº¿æ€§æ¨¡å‹æ‰€åšçš„é‚£æ ·ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå‡ ç§ä¸åŒçš„æ–¹å¼æ¥ç»˜åˆ¶ç›¸åŒçš„ç»“æœã€‚*å›¾
    [8.5](#x1-163035r5)*å±•ç¤ºäº†æ‹Ÿåˆå‡½æ•°çš„çº¿æ¡ã€‚
- en: '![PIC](img/file226.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file226.png)'
- en: '**FigureÂ 8.5**: Lines represent samples from the posterior mean of `model_reg`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 8.5**ï¼šçº¿æ¡è¡¨ç¤ºä»`model_reg`çš„åéªŒå‡å€¼ä¸­æŠ½å–çš„æ ·æœ¬'
- en: Alternatively, we can use the auxiliary function `pm.gp.util.plot_gp_dist` to
    get some nice plots as in *Figure [8.6](#x1-163036r6)*. In this plot, each band
    represents a different percentile, ranging from percentile 99 (lighter gray) to
    percentile 51 (darker gray).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¾…åŠ©å‡½æ•°`pm.gp.util.plot_gp_dist`æ¥è·å¾—å¦‚*å›¾ [8.6](#x1-163036r6)*æ‰€ç¤ºçš„æ¼‚äº®å›¾è¡¨ã€‚åœ¨è¿™ä¸ªå›¾è¡¨ä¸­ï¼Œæ¯æ¡å¸¦çŠ¶çº¿è¡¨ç¤ºä¸åŒçš„ç™¾åˆ†ä½æ•°ï¼Œä»99ç™¾åˆ†ä½ï¼ˆæµ…ç°è‰²ï¼‰åˆ°51ç™¾åˆ†ä½ï¼ˆæ·±ç°è‰²ï¼‰ã€‚
- en: '![PIC](img/file227.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file227.png)'
- en: '**FigureÂ 8.6**: Samples from the posterior of `model_reg` plotted using `plot_gp_dist`
    function'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 8.6**ï¼šä½¿ç”¨`plot_gp_dist`å‡½æ•°ç»˜åˆ¶çš„ä»`model_reg`çš„åéªŒæ ·æœ¬'
- en: Yet another alternative is to compute the mean vector and standard deviation
    of the conditional distribution evaluated at a given point in the parameter space.
    In *Figure [8.7](#x1-163037r7)*, we use the mean (over the samples in the trace)
    for *â„“* and ![](img/e.png). We can compute the mean and variance using the `gp.predict`
    method.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§é€‰æ‹©æ˜¯è®¡ç®—åœ¨å‚æ•°ç©ºé—´ä¸­ç»™å®šç‚¹å¤„çš„æ¡ä»¶åˆ†å¸ƒçš„å‡å€¼å‘é‡å’Œæ ‡å‡†å·®ã€‚åœ¨*å›¾ [8.7](#x1-163037r7)*ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å‡å€¼ï¼ˆåŸºäºè½¨è¿¹ä¸­çš„æ ·æœ¬ï¼‰æ¥è¡¨ç¤º*â„“*å’Œ
    ![](img/e.png)ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`gp.predict`æ–¹æ³•æ¥è®¡ç®—å‡å€¼å’Œæ–¹å·®ã€‚
- en: '![PIC](img/file228.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file228.png)'
- en: '**FigureÂ 8.7**: Posterior mean of `model_reg` with bands for 1 and 2 standard
    deviations'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 8.7**ï¼š`model_reg`çš„åéªŒå‡å€¼ï¼Œå¸¦æœ‰1å’Œ2æ ‡å‡†å·®çš„å¸¦å®½'
- en: As we saw in *Chapter [4](CH04.xhtml#x1-760004)*, we can use a linear model
    with a non-Gaussian likelihood and a proper inverse link function to extend the
    range of useful linear models. We can do the same for GPs. We can, for example,
    use a Poisson likelihood with an exponential inverse link function. For a model
    like this, the posterior is no longer analytically tractable, but, nevertheless,
    we can use numerical methods to approximate it. In the following sections, we
    will discuss these types of models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨*ç¬¬ [4](CH04.xhtml#x1-760004)*ç« ä¸­çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå…·æœ‰éé«˜æ–¯ä¼¼ç„¶åº¦å’Œé€‚å½“é€†é“¾æ¥å‡½æ•°çš„çº¿æ€§æ¨¡å‹æ¥æ‰©å±•æœ‰ç”¨çº¿æ€§æ¨¡å‹çš„èŒƒå›´ã€‚å¯¹äºé«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥åšåŒæ ·çš„äº‹æƒ…ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå¸¦æœ‰æŒ‡æ•°é€†é“¾æ¥å‡½æ•°çš„æ³Šæ¾ä¼¼ç„¶åº¦ã€‚å¯¹äºè¿™æ ·çš„æ¨¡å‹ï¼ŒåéªŒåˆ†å¸ƒä¸å†æ˜¯è§£æå¯è§£çš„ï¼Œä½†æˆ‘ä»¬ä»ç„¶å¯ä»¥ä½¿ç”¨æ•°å€¼æ–¹æ³•æ¥è¿‘ä¼¼è®¡ç®—ã€‚åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºè¿™äº›ç±»å‹çš„æ¨¡å‹ã€‚
- en: 8.6.1 Setting priors for the length scale
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6.1 è®¾ç½®é•¿åº¦å°ºåº¦çš„å…ˆéªŒ
- en: 'For length-scale parameters, priors avoiding zero usually work better. As we
    already saw, *â„“* controls the smoothness of the function, thus a value of 0 for
    *â„“* implies a non-smooth function; we will get a function like â€the first oneâ€
    from *Figure [8.1](#x1-158016r1)*. But a far more important reason is that for
    values of *â„“* that are larger than 0 but still below the minimum spacing of the
    covariates, we can get some nasty effects. Essentially, below that point, the
    likelihood has no way to distinguish between different length scales, so all of
    them are equally good. This is a type of non-identifiability issue. As a result,
    we will have a GP that will tend to overfit and exactly interpolate between the
    input data. Additionally, the MCMC sampler will have a harder time, and we could
    get longer sampling times or simple unreliable samples. Something similar happens
    for values beyond the range of the data. If the range of your data is 10 and the
    value of *â„“ >*= 10, this implies a flat function. And again beyond that point,
    you (and the likelihood) have no way of distinguishing between different values
    of the parameter. Thus even if you have no idea how smooth or wiggly your function
    is, you can still set a prior that avoids very low and very high values of *â„“*.
    For instance, to get the prior `pm.InverseGamma("`*â„“*`", 7, 17)` we ask PreliZ
    for the maximum entropy prior that has 0.95 of the mass between 1 and 5:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºé•¿åº¦å°ºåº¦å‚æ•°ï¼Œé¿å…ä¸ºé›¶çš„å…ˆéªŒé€šå¸¸æ•ˆæœæ›´å¥½ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼Œ*â„“*æ§åˆ¶ç€å‡½æ•°çš„å¹³æ»‘åº¦ï¼Œå› æ­¤ï¼Œ*â„“*ä¸º0æ„å‘³ç€å‡½æ•°ä¸å¹³æ»‘ï¼›æˆ‘ä»¬ä¼šå¾—åˆ°ç±»ä¼¼äº*å›¾ [8.1](#x1-158016r1)*ä¸­çš„â€œç¬¬ä¸€ä¸ªâ€å‡½æ•°ã€‚ä½†æ›´é‡è¦çš„åŸå› æ˜¯ï¼Œå¯¹äºå¤§äº0ä½†ä»ä½äºåå˜é‡æœ€å°é—´è·çš„*â„“*å€¼ï¼Œå¯èƒ½ä¼šå‡ºç°ä¸€äº›ä¸è‰¯æ•ˆåº”ã€‚æœ¬è´¨ä¸Šï¼Œåœ¨è¯¥ç‚¹ä»¥ä¸‹ï¼Œä¼¼ç„¶åº¦æ— æ³•åŒºåˆ†ä¸åŒçš„é•¿åº¦å°ºåº¦ï¼Œå› æ­¤å®ƒä»¬éƒ½åŒæ ·å¥½ã€‚è¿™æ˜¯ä¸€ä¸ªä¸å¯è¯†åˆ«æ€§é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ä¸€ä¸ªå€¾å‘äºè¿‡æ‹Ÿåˆå¹¶ä¸”å‡†ç¡®æ’å€¼è¾“å…¥æ•°æ®çš„é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰ã€‚æ­¤å¤–ï¼ŒMCMCé‡‡æ ·å™¨ä¼šé‡åˆ°æ›´å¤šå›°éš¾ï¼Œå¯èƒ½ä¼šå‡ºç°æ›´é•¿çš„é‡‡æ ·æ—¶é—´æˆ–ç®€å•çš„ä¸å¯é æ ·æœ¬ã€‚ç±»ä¼¼çš„æƒ…å†µå‘ç”Ÿåœ¨æ•°æ®èŒƒå›´ä¹‹å¤–ã€‚å¦‚æœæ•°æ®èŒƒå›´ä¸º10ä¸”*â„“*>
    = 10ï¼Œè¿™æ„å‘³ç€å‡½æ•°æ˜¯å¹³å¦çš„ã€‚å†å¾€åï¼Œæ‚¨ï¼ˆä»¥åŠä¼¼ç„¶åº¦ï¼‰æ— æ³•åŒºåˆ†ä¸åŒçš„å‚æ•°å€¼ã€‚å› æ­¤ï¼Œå³ä½¿æ‚¨ä¸ç¡®å®šå‡½æ•°æ˜¯å¤šä¹ˆå¹³æ»‘æˆ–èµ·ä¼ï¼Œæ‚¨ä»ç„¶å¯ä»¥è®¾ç½®ä¸€ä¸ªå…ˆéªŒï¼Œé¿å…æä½æˆ–æé«˜çš„*â„“*å€¼ã€‚ä¾‹å¦‚ï¼Œä¸ºäº†è·å¾—å…ˆéªŒ`pm.InverseGamma("`*â„“*`",
    7, 17)`ï¼Œæˆ‘ä»¬è¯·æ±‚PreliZæä¾›ä¸€ä¸ªæœ€å¤§ç†µå…ˆéªŒï¼Œå…¶ä¸­0.95çš„è´¨é‡åˆ†å¸ƒåœ¨1åˆ°5ä¹‹é—´ï¼š
- en: '**CodeÂ 8.6**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.6**'
- en: '[PRE5]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The InverseGamma is a common choice. Like the Gamma, it allows us to set a prior
    that avoids 0, but unlike the Gamma, the InverseGamma has a lighter tail toward
    0, or in other words, it allocates less mass for small values.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: é€†ä¼½é©¬åˆ†å¸ƒï¼ˆInverseGammaï¼‰æ˜¯ä¸€ä¸ªå¸¸è§çš„é€‰æ‹©ã€‚ä¸ä¼½é©¬åˆ†å¸ƒï¼ˆGammaï¼‰ç±»ä¼¼ï¼Œå®ƒå…è®¸æˆ‘ä»¬è®¾ç½®ä¸€ä¸ªé¿å…ä¸º0çš„å…ˆéªŒï¼Œä½†ä¸ä¼½é©¬åˆ†å¸ƒä¸åŒçš„æ˜¯ï¼Œé€†ä¼½é©¬åˆ†å¸ƒåœ¨æ¥è¿‘0æ—¶æœ‰ä¸€ä¸ªè¾ƒè½»çš„å°¾éƒ¨ï¼Œæ¢å¥è¯è¯´ï¼Œå®ƒä¸ºå°å€¼åˆ†é…çš„è´¨é‡è¾ƒå°‘ã€‚
- en: For the rest of this chapter, we will use the function `get_ig_params` to obtain
    weakly informative priors from the scale of the covariates. You will find the
    details in the accompanying code ([https://github.com/aloctavodia/BAP3](https://github.com/aloctavodia/BAP3)),
    but essentially we are using the `maxent` function from PreliZ to set most of
    the prior mass in a range compatible with the range of the covariates.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å‰©ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‡½æ•°`get_ig_params`ä»åå˜é‡çš„å°ºåº¦ä¸­è·å–å¼±ä¿¡æ¯å…ˆéªŒã€‚ä½ å¯ä»¥åœ¨é™„å¸¦çš„ä»£ç ä¸­æ‰¾åˆ°è¯¦ç»†ä¿¡æ¯ï¼ˆ[https://github.com/aloctavodia/BAP3](https://github.com/aloctavodia/BAP3)ï¼‰ï¼Œä½†æœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨PreliZä¸­çš„`maxent`å‡½æ•°ï¼Œå°†å¤§éƒ¨åˆ†å…ˆéªŒè´¨é‡è®¾ç½®åœ¨ä¸€ä¸ªä¸åå˜é‡èŒƒå›´å…¼å®¹çš„åŒºé—´å†…ã€‚
- en: 8.7 Gaussian process classification
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7 é«˜æ–¯è¿‡ç¨‹åˆ†ç±»
- en: In *Chapter [4](CH04.xhtml#x1-760004)*, we saw how a linear model can be used
    to classify data. We used a Bernoulli likelihood with a logistic inverse link
    function. Then, we applied a boundary decision rule. In this section, we are going
    to do the same, but this time using a GP instead of a linear model. As we did
    with `model_lrs` from *Chapter [4](CH04.xhtml#x1-760004)*, we are going to use
    the iris dataset with two classes, `setosa` and `versicolor`, and one predictor
    variable, the `sepal length`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*ç¬¬[4ç« ](CH04.xhtml#x1-760004)*ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¦‚ä½•ä½¿ç”¨çº¿æ€§æ¨¡å‹æ¥åˆ†ç±»æ•°æ®ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¼¯åŠªåˆ©ä¼¼ç„¶å’Œé€»è¾‘é€†é“¾æ¥å‡½æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨äº†è¾¹ç•Œå†³ç­–è§„åˆ™ã€‚åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åšåŒæ ·çš„äº‹æƒ…ï¼Œä½†è¿™æ¬¡ä½¿ç”¨GPè€Œä¸æ˜¯çº¿æ€§æ¨¡å‹ã€‚å°±åƒæˆ‘ä»¬åœ¨*ç¬¬[4ç« ](CH04.xhtml#x1-760004)*ä¸­çš„`model_lrs`ä¸€æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åŒ…å«ä¸¤ç±»`setosa`å’Œ`versicolor`ã€ä¸€ä¸ªé¢„æµ‹å˜é‡`sepal
    length`çš„é¸¢å°¾èŠ±æ•°æ®é›†ã€‚
- en: For this model, we cannot use the `pm.gp.Marginal` class, because that class
    is restricted to Gaussian likelihoods as it takes advantage of the mathematical
    tractability of the combination of a GP prior with a Gaussian likelihood. Instead,
    we need to use the more general class `pm.gp.Latent`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬ä¸èƒ½ä½¿ç”¨`pm.gp.Marginal`ç±»ï¼Œå› ä¸ºè¯¥ç±»ä»…é™äºé«˜æ–¯ä¼¼ç„¶ï¼Œå› ä¸ºå®ƒåˆ©ç”¨äº†GPå…ˆéªŒä¸é«˜æ–¯ä¼¼ç„¶ç»„åˆçš„æ•°å­¦å¯å¤„ç†æ€§ã€‚ç›¸åï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨æ›´é€šç”¨çš„`pm.gp.Latent`ç±»ã€‚
- en: '**CodeÂ 8.7**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.7**'
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As we can see, *Figure [8.8](#x1-165012r8)* looks pretty similar to *Figure
    [4.11](CH04.xhtml#x1-85023r11)*. Please take some time to compare these figures.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œ*å›¾[8.8](#x1-165012r8)*çœ‹èµ·æ¥ä¸*å›¾[4.11](CH04.xhtml#x1-85023r11)*éå¸¸ç›¸ä¼¼ã€‚è¯·èŠ±äº›æ—¶é—´å¯¹æ¯”è¿™ä¸¤å¼ å›¾ã€‚
- en: '![PIC](img/file229.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file229.png)'
- en: '**FigureÂ 8.8**: Logistic regression, result of `model_lrs`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾8.8**ï¼šé€»è¾‘å›å½’ï¼Œ`model_lrs`çš„ç»“æœ'
- en: You probably have already noticed that the inferred function looks similar to
    a sigmoid curve, except for the tails that go up at lower values of `sepal_length`,
    and down at higher values of `sepal_length`. Why are we seeing this? Because when
    there is little or no data available, a GP posterior tends to revert to the GP
    prior. This makes sense if we think that in the absence of data, your posterior
    essentially becomes the prior.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œæ¨æ–­å‡ºçš„å‡½æ•°çœ‹èµ·æ¥ç±»ä¼¼äºsigmoidæ›²çº¿ï¼Œé™¤äº†åœ¨è¾ƒä½çš„`sepal_length`å€¼æ—¶å°¾éƒ¨å‘ä¸Šï¼Œè¾ƒé«˜çš„`sepal_length`å€¼æ—¶å°¾éƒ¨å‘ä¸‹ã€‚ä¸ºä»€ä¹ˆä¼šè¿™æ ·å‘¢ï¼Ÿå› ä¸ºå½“æ•°æ®å¾ˆå°‘æˆ–æ²¡æœ‰æ•°æ®æ—¶ï¼ŒGPåéªŒå€¾å‘äºæ¢å¤åˆ°GPå…ˆéªŒã€‚å¦‚æœæˆ‘ä»¬è€ƒè™‘åˆ°åœ¨æ²¡æœ‰æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒåéªŒåŸºæœ¬ä¸Šå˜æˆäº†å…ˆéªŒï¼Œè¿™å°±å¾ˆæœ‰æ„ä¹‰ã€‚
- en: 'If our only concern is the decision boundary, then the behavior at the tails
    may be irrelevant. But if we want to model the probabilities of belonging to setosa
    or versicolor at different values of `sepal_length`, we should do something to
    improve the model at the tails. One way to achieve this is to add more structure
    to the Gaussian process. One very nice feature of GP is that we can combine covariance
    functions. Hence, for the next model, we are going to combine three kernels: the
    exponential quadratic kernel, a linear kernel, and a white noise kernel.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å”¯ä¸€å…³æ³¨çš„æ˜¯å†³ç­–è¾¹ç•Œï¼Œé‚£ä¹ˆå°¾éƒ¨çš„è¡Œä¸ºå¯èƒ½æ˜¯æ— å…³ç´§è¦çš„ã€‚ä½†å¦‚æœæˆ‘ä»¬æƒ³è¦å»ºæ¨¡åœ¨ä¸åŒ`sepal_length`å€¼ä¸‹å±äºsetosaæˆ–versicolorçš„æ¦‚ç‡ï¼Œæˆ‘ä»¬åº”è¯¥åšä¸€äº›äº‹æƒ…æ¥æ”¹å–„å°¾éƒ¨çš„æ¨¡å‹ã€‚å®ç°è¿™ä¸€ç›®æ ‡çš„ä¸€ç§æ–¹æ³•æ˜¯ä¸ºé«˜æ–¯è¿‡ç¨‹æ·»åŠ æ›´å¤šç»“æ„ã€‚é«˜æ–¯è¿‡ç¨‹çš„ä¸€ä¸ªéå¸¸å¥½çš„ç‰¹ç‚¹æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥ç»„åˆåæ–¹å·®å‡½æ•°ã€‚å› æ­¤ï¼Œå¯¹äºä¸‹ä¸€ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬å°†ç»„åˆä¸‰ä¸ªæ ¸å‡½æ•°ï¼šæŒ‡æ•°äºŒæ¬¡æ ¸ã€çº¿æ€§æ ¸å’Œç™½å™ªå£°æ ¸ã€‚
- en: 'The linear kernel will have the effect of making the tails go to 0 or 1 at
    the boundaries of the data. Additionally, we use the white noise kernel just as
    a trick to stabilize the computation of the covariance matrix. Kernels for Gaussian
    processes are restricted to guarantee the resulting covariance matrix is positive
    definite. Nevertheless, numerical errors can lead to violating this condition.
    One manifestation of this problem is that we get NaNs when computing posterior
    predictive samples of the fitted function. One way to mitigate this error is to
    stabilize the computation by adding some noise. As a matter of fact, PyMC already
    does something similar to this under the hood, but sometimes a little bit more
    noise is needed, as shown in the following code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§æ ¸çš„æ•ˆæœæ˜¯åœ¨æ•°æ®è¾¹ç•Œå¤„ä½¿å°¾éƒ¨è¶‹è¿‘äº0æˆ–1ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ç™½å™ªå£°æ ¸ä½œä¸ºç¨³å®šåæ–¹å·®çŸ©é˜µè®¡ç®—çš„ä¸€ç§æŠ€å·§ã€‚é«˜æ–¯è¿‡ç¨‹çš„æ ¸å‡½æ•°å—åˆ°é™åˆ¶ï¼Œä»¥ä¿è¯ç”Ÿæˆçš„åæ–¹å·®çŸ©é˜µæ˜¯æ­£å®šçš„ã€‚ç„¶è€Œï¼Œæ•°å€¼è¯¯å·®å¯èƒ½å¯¼è‡´è¿åè¿™ä¸€æ¡ä»¶ã€‚è¿™ç§é—®é¢˜çš„è¡¨ç°ä¹‹ä¸€æ˜¯ï¼Œåœ¨è®¡ç®—æ‹Ÿåˆå‡½æ•°çš„åéªŒé¢„æµ‹æ ·æœ¬æ—¶ä¼šå¾—åˆ°NaNã€‚ç¼“è§£æ­¤é”™è¯¯çš„ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡æ·»åŠ å™ªå£°æ¥ç¨³å®šè®¡ç®—ã€‚äº‹å®ä¸Šï¼ŒPyMCåœ¨åå°å·²ç»åšäº†ç±»ä¼¼çš„å¤„ç†ï¼Œä½†æœ‰æ—¶éœ€è¦æ›´å¤šçš„å™ªå£°ï¼Œå¦‚ä¸‹é¢çš„ä»£ç æ‰€ç¤ºï¼š
- en: '**CodeÂ 8.8**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.8**'
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can see the result of this model in *Figure [8.9](#x1-165027r9)*. Notice
    how this figure looks much more similar now to *Figure [4.11](CH04.xhtml#x1-85023r11)*
    than *Figure [8.8](#x1-165012r8)*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨*å›¾ [8.9](#x1-165027r9)*ä¸­çœ‹åˆ°è¿™ä¸ªæ¨¡å‹çš„ç»“æœã€‚æ³¨æ„ï¼Œè¿™ä¸ªå›¾çœ‹èµ·æ¥ç°åœ¨ä¸*å›¾ [4.11](CH04.xhtml#x1-85023r11)*ç›¸æ¯”ï¼Œæ›´åŠ ç›¸ä¼¼ï¼Œè€Œä¸æ˜¯*å›¾
    [8.8](#x1-165012r8)*ã€‚
- en: '![PIC](img/file230.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file230.png)'
- en: '**FigureÂ 8.9**: Logistic regression, result of `model_lrs`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 8.9**ï¼šé€»è¾‘å›å½’ï¼Œ`model_lrs`çš„ç»“æœ'
- en: 'The example discussed in this section has two main aims:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚è®¨è®ºçš„ç¤ºä¾‹æœ‰ä¸¤ä¸ªä¸»è¦ç›®çš„ï¼š
- en: Showing how we can easily combine kernels to get a more expressive model
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ˜¾ç¤ºå¦‚ä½•è½»æ¾åœ°å°†æ ¸å‡½æ•°ç»„åˆåœ¨ä¸€èµ·ï¼Œä»¥è·å¾—æ›´å…·è¡¨ç°åŠ›çš„æ¨¡å‹
- en: Showing how we can *recover* a logistic regression using a Gaussian process
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ˜¾ç¤ºå¦‚ä½•ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹*æ¢å¤*é€»è¾‘å›å½’
- en: 'Regarding the second point, logistic regression is indeed a special case of
    Gaussian processes, because a simple linear regression is just a particular case
    of a Gaussian process. In fact, many known models can be seen as special cases
    of GPs, or at least they are somehow connected to GPs. If you want to learn more
    about this, you can read Chapter 15 from Kevin Murphyâ€™s Machine Learning: A Probabilistic
    Perspective (first edition) [[Murphy](Bibliography.xhtml#Xpml0Book),Â [2012](Bibliography.xhtml#Xpml0Book)],
    and also Chapter 18 from the second edition [[Murphy](Bibliography.xhtml#Xpml2Book),Â [2023](Bibliography.xhtml#Xpml2Book)].'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºç¬¬äºŒç‚¹ï¼Œé€»è¾‘å›å½’ç¡®å®æ˜¯é«˜æ–¯è¿‡ç¨‹çš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œå› ä¸ºç®€å•çš„çº¿æ€§å›å½’åªæ˜¯é«˜æ–¯è¿‡ç¨‹çš„ä¸€ä¸ªç‰¹ä¾‹ã€‚äº‹å®ä¸Šï¼Œè®¸å¤šå·²çŸ¥çš„æ¨¡å‹å¯ä»¥çœ‹ä½œæ˜¯é«˜æ–¯è¿‡ç¨‹çš„ç‰¹ä¾‹ï¼Œæˆ–è€…è‡³å°‘å®ƒä»¬ä¸é«˜æ–¯è¿‡ç¨‹æœ‰æŸç§è”ç³»ã€‚å¦‚æœä½ æƒ³äº†è§£æ›´å¤šï¼Œå¯ä»¥é˜…è¯»Kevin
    Murphyçš„ã€Šæœºå™¨å­¦ä¹ ï¼šä¸€ç§æ¦‚ç‡è§†è§’ã€‹ç¬¬ä¸€ç‰ˆç¬¬15ç« [[Murphy](Bibliography.xhtml#Xpml0Book)ï¼Œ[2012](Bibliography.xhtml#Xpml0Book)]ï¼Œä»¥åŠç¬¬äºŒç‰ˆçš„ç¬¬18ç« [[Murphy](Bibliography.xhtml#Xpml2Book)ï¼Œ[2023](Bibliography.xhtml#Xpml2Book)]ã€‚
- en: 8.7.1 GPs for space flu
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1 é«˜æ–¯è¿‡ç¨‹ç”¨äºç©ºé—´æµæ„Ÿ
- en: In practice, it does not make too much sense to use a GP to model a problem
    we can just solve with a logistic regression. Instead, we want to use a GP to
    model more complex data that is not well captured with less flexible models. For
    instance, suppose we want to model the probability of getting a disease as a function
    of age. It turns out that very young and very old people have a higher risk than
    people of middle age. The dataset `space_flu.csv` is a synthetic dataset inspired
    by the previous description. *Figure [8.10](#x1-166011r10)* shows a plot of it.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œä½¿ç”¨é«˜æ–¯è¿‡ç¨‹æ¥å»ºæ¨¡æˆ‘ä»¬å¯ä»¥é€šè¿‡é€»è¾‘å›å½’è§£å†³çš„é—®é¢˜å¹¶æ²¡æœ‰å¤ªå¤§æ„ä¹‰ã€‚ç›¸åï¼Œæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹æ¥å»ºæ¨¡é‚£äº›æ— æ³•é€šè¿‡ä¸å¤ªçµæ´»çš„æ¨¡å‹å¾ˆå¥½æ•æ‰çš„æ›´å¤æ‚çš„æ•°æ®ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æƒ³è¦å°†æ‚£ç—…çš„æ¦‚ç‡ä½œä¸ºå¹´é¾„çš„å‡½æ•°è¿›è¡Œå»ºæ¨¡ã€‚ç»“æœè¡¨æ˜ï¼Œå¹´è½»äººå’Œå¹´è€çš„äººæ¯”ä¸­å¹´äººæœ‰æ›´é«˜çš„é£é™©ã€‚æ•°æ®é›†`space_flu.csv`æ˜¯ä¸€ä¸ªå—ä¸Šè¿°æè¿°å¯å‘çš„åˆæˆæ•°æ®é›†ã€‚*å›¾
    [8.10](#x1-166011r10)*å±•ç¤ºäº†å®ƒçš„å›¾å½¢ã€‚
- en: 'Letâ€™s fit the following model and plot the results:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ‹Ÿåˆä»¥ä¸‹æ¨¡å‹å¹¶ç»˜åˆ¶ç»“æœï¼š
- en: '**CodeÂ 8.9**'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.9**'
- en: '[PRE8]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice, as illustrated in *Figure [8.10](#x1-166011r10)*, that the GP can fit
    this space flu dataset very well, even when the data demands the function to be
    more complex than a logistic one. Fitting this data well will be impossible for
    a simple logistic regression, unless we introduce some ad hoc modifications to
    help it a little bit (see exercise 6 at the end of the chapter for a discussion
    of such modifications).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¦‚*å›¾ [8.10](#x1-166011r10)*æ‰€ç¤ºï¼Œé«˜æ–¯è¿‡ç¨‹å¯ä»¥å¾ˆå¥½åœ°æ‹Ÿåˆè¿™ä¸ªç©ºé—´æµæ„Ÿæ•°æ®é›†ï¼Œå³ä½¿æ•°æ®è¦æ±‚å‡½æ•°æ¯”é€»è¾‘å›å½’æ›´å¤æ‚ã€‚å¯¹äºç®€å•çš„é€»è¾‘å›å½’æ¥è¯´ï¼Œæ‹Ÿåˆè¿™äº›æ•°æ®å‡ ä¹æ˜¯ä¸å¯èƒ½çš„ï¼Œé™¤éæˆ‘ä»¬åšä¸€äº›ç‰¹æ®Šçš„ä¿®æ”¹æ¥å¸®åŠ©å®ƒï¼ˆæœ‰å…³è¿™ç§ä¿®æ”¹çš„è®¨è®ºè¯·å‚è§æœ¬ç« æœ«çš„ç»ƒä¹ 6ï¼‰ã€‚
- en: '![PIC](img/file231.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file231.png)'
- en: '**FigureÂ 8.10**: Logistic regression, result of `model_space_flu`'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 8.10**ï¼šé€»è¾‘å›å½’ï¼Œ`model_space_flu`çš„ç»“æœ'
- en: 8.8 Cox processes
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.8 Cox è¿‡ç¨‹
- en: Now we are going to model count data. We will see two examples; one with a time-varying
    rate and one with a 2D spatially varying rate. To do this, we will use a Poisson
    likelihood and the rate will be modeled using a Gaussian process. Because the
    rate of the Poisson distribution is limited to positive values, we will use an
    exponential as the inverse link function, as we did for the NegativeBinomial regression
    from *Chapter [4](CH04.xhtml#x1-760004)*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬è¦å»ºæ¨¡è®¡æ•°æ•°æ®ã€‚æˆ‘ä»¬å°†çœ‹åˆ°ä¸¤ä¸ªä¾‹å­ï¼›ä¸€ä¸ªæ˜¯å…·æœ‰æ—¶é—´å˜åŒ–ç‡çš„ï¼Œå¦ä¸€ä¸ªæ˜¯å…·æœ‰äºŒç»´ç©ºé—´å˜åŒ–ç‡çš„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ³Šæ¾ä¼¼ç„¶ï¼Œå¹¶é€šè¿‡é«˜æ–¯è¿‡ç¨‹æ¥å»ºæ¨¡è¯¥æ¯”ç‡ã€‚ç”±äºæ³Šæ¾åˆ†å¸ƒçš„æ¯”ç‡é™åˆ¶ä¸ºæ­£å€¼ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æŒ‡æ•°ä½œä¸ºé€†é“¾æ¥å‡½æ•°ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨*ç¬¬
    [4 ç« ](CH04.xhtml#x1-760004)*ä¸­çš„è´ŸäºŒé¡¹å›å½’ä¸­æ‰€åšçš„é‚£æ ·ã€‚
- en: We can think of a Poisson process as a distribution over collections of points
    in a given space where every finite collection of those random variables has a
    Poisson distribution. When the rate of the Poisson process is itself a stochastic
    process, such as, for example, a Gaussian process, then we have a Cox process.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†æ³Šæ¾è¿‡ç¨‹çœ‹ä½œæ˜¯åœ¨ç»™å®šç©ºé—´ä¸­ç‚¹é›†çš„åˆ†å¸ƒï¼Œå…¶ä¸­æ¯ä¸ªæœ‰é™çš„ç‚¹é›†éƒ½æ˜¯æ³Šæ¾åˆ†å¸ƒã€‚å½“æ³Šæ¾è¿‡ç¨‹çš„æ¯”ç‡æœ¬èº«æ˜¯ä¸€ä¸ªéšæœºè¿‡ç¨‹æ—¶ï¼Œä¾‹å¦‚ä¸€ä¸ªé«˜æ–¯è¿‡ç¨‹ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±æœ‰äº†
    Cox è¿‡ç¨‹ã€‚
- en: 8.8.1 Coal mining disasters
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.8.1 ç…¤çŸ¿ç¾éš¾
- en: 'The first example is known as the coal mining disasters. This example consists
    of a record of coal-mining disasters in the UK from 1851 to 1962\. The number
    of disasters is thought to have been affected by changes in safety regulations
    during this period. We want to model the rate of disasters as a function of time.
    Our dataset consists of a single column and each entry corresponds to the time
    a disaster happened. The model we will use to fit the data has the form:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªä¾‹å­è¢«ç§°ä¸ºç…¤çŸ¿ç¾éš¾ã€‚è¿™ä¸ªä¾‹å­è®°å½•äº†è‹±å›½ä» 1851 å¹´åˆ° 1962 å¹´çš„ç…¤çŸ¿ç¾éš¾æ•°æ®ã€‚ç¾éš¾çš„æ•°é‡è¢«è®¤ä¸ºå—åˆ°äº†è¿™ä¸€æ—¶æœŸå®‰å…¨æ³•è§„å˜åŒ–çš„å½±å“ã€‚æˆ‘ä»¬å¸Œæœ›å°†ç¾éš¾çš„å‘ç”Ÿç‡å»ºæ¨¡ä¸ºæ—¶é—´çš„å‡½æ•°ã€‚æˆ‘ä»¬çš„æ•°æ®é›†åŒ…å«ä¸€åˆ—æ•°æ®ï¼Œæ¯æ¡æ•°æ®å¯¹åº”ä¸€æ¬¡ç¾éš¾å‘ç”Ÿçš„æ—¶é—´ã€‚æˆ‘ä»¬å°†ç”¨äºæ‹Ÿåˆæ•°æ®çš„æ¨¡å‹å½¢å¼å¦‚ä¸‹ï¼š
- en: '![](img/Formula_01.PNG)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_01.PNG)'
- en: 'As you can see, this is a Poisson regression problem. You may be wondering
    at this point how weâ€™re going to perform a regression if we only have a single
    column with just the date of the disasters. The answer is to discretize the data,
    just as if we were building a histogram. We are going to use the centers of the
    bins as the *X* variable and the counts per bin as the *Y* variable:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æ‚¨æ‰€è§ï¼Œè¿™æ˜¯ä¸€ä¸ªæ³Šæ¾å›å½’é—®é¢˜ã€‚æ­¤æ—¶ï¼Œæ‚¨å¯èƒ½ä¼šæƒ³ï¼Œæˆ‘ä»¬å¦‚ä½•ä»…å‡­ä¸€ä¸ªåªæœ‰ç¾éš¾æ—¥æœŸçš„å•åˆ—æ•°æ®æ¥è¿›è¡Œå›å½’åˆ†æã€‚ç­”æ¡ˆæ˜¯å°†æ•°æ®ç¦»æ•£åŒ–ï¼Œå°±åƒæˆ‘ä»¬æ„å»ºç›´æ–¹å›¾ä¸€æ ·ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç®±å­çš„ä¸­å¿ƒä½œä¸º*X*å˜é‡ï¼Œç®±å­å†…çš„è®¡æ•°ä½œä¸º*Y*å˜é‡ï¼š
- en: '**CodeÂ 8.10**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.10**'
- en: '[PRE9]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now we define and solve the model with PyMC:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ä½¿ç”¨ PyMC æ¥å®šä¹‰å¹¶æ±‚è§£æ¨¡å‹ï¼š
- en: '**CodeÂ 8.11**'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.11**'
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '*Figure [8.11](#x1-168023r11)* shows the median disaster rate as a function
    of time (white line). The bands describe the 50% HDI (darker) and the 94% HDI
    (lighter). At the bottom, the black markers indicate the moment of each disaster.
    As we can see, the rate of accidents decreases with time, except for a brief initial
    increase. The PyMC documentation includes the coal mining disaster but is modeled
    from a different perspective. I strongly recommend that you check that example
    as it is very useful on its own and is also useful to compare it with the approach
    we just implemented here.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ [8.11](#x1-168023r11)* æ˜¾ç¤ºäº†ç¾éš¾ç‡éšæ—¶é—´å˜åŒ–çš„ä¸­ä½æ•°ï¼ˆç™½çº¿ï¼‰ã€‚å¸¦çŠ¶åŒºåŸŸæè¿°äº†50%çš„HDIï¼ˆè¾ƒæš—ï¼‰å’Œ94%çš„HDIï¼ˆè¾ƒäº®ï¼‰ã€‚åº•éƒ¨çš„é»‘è‰²æ ‡è®°è¡¨ç¤ºæ¯æ¬¡ç¾éš¾å‘ç”Ÿçš„æ—¶åˆ»ã€‚æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œäº‹æ•…ç‡éšç€æ—¶é—´çš„æ¨ç§»è€Œä¸‹é™ï¼Œé™¤äº†æœ€åˆçš„çŸ­æš‚ä¸Šå‡ã€‚PyMC
    æ–‡æ¡£åŒ…æ‹¬äº†ç…¤çŸ¿ç¾éš¾çš„æ¡ˆä¾‹ï¼Œä½†ä»ä¸åŒçš„è§’åº¦è¿›è¡Œå»ºæ¨¡ã€‚æˆ‘å¼ºçƒˆå»ºè®®æ‚¨æŸ¥çœ‹è¿™ä¸ªä¾‹å­ï¼Œå› ä¸ºå®ƒæœ¬èº«éå¸¸æœ‰ç”¨ï¼Œå¹¶ä¸”ä¸æˆ‘ä»¬åˆšåˆšå®ç°çš„æ–¹æ³•åšå¯¹æ¯”ä¹Ÿå¾ˆæœ‰ä»·å€¼ã€‚'
- en: '![PIC](img/file232.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file232.png)'
- en: '**FigureÂ 8.11**: Logistic regression, result of `model_coal`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 8.11**ï¼šé€»è¾‘å›å½’ï¼Œ`model_coal`çš„ç»“æœ'
- en: Notice that even when we binned the data, we obtained, as a result, a smooth
    curve. In this sense, we can see `model_coal` (and, in general, this type of model)
    as building a histogram and then smoothing it.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå³ä½¿æˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œäº†åˆ†ç®±ï¼Œç»“æœä»ç„¶æ˜¯ä¸€ä¸ªå¹³æ»‘çš„æ›²çº¿ã€‚ä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥å°† `model_coal`ï¼ˆä»¥åŠä¸€èˆ¬è€Œè¨€ï¼Œè¿™ç§ç±»å‹çš„æ¨¡å‹ï¼‰è§†ä¸ºæ„å»ºç›´æ–¹å›¾åè¿›è¡Œå¹³æ»‘å¤„ç†ã€‚
- en: 8.8.2 Red wood
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.8.2 çº¢æœ¨
- en: Letâ€™s apply the same approach we just did to a 2D spatial problem. We are going
    to use the redwood data as shown in *Figure [8.12](#x1-169015r12)*. This dataset
    (distributed with a GPL license) is from the GPstuff package. The dataset consists
    of the location of redwood trees over a given area. The motivation of the inference
    is to obtain a map of a rate, the number of trees in a given area.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°†åˆšæ‰ä½¿ç”¨çš„æ–¹æ³•åº”ç”¨äºä¸€ä¸ªäºŒç»´ç©ºé—´é—®é¢˜ã€‚æˆ‘ä»¬å°†ä½¿ç”¨å¦‚*å›¾ [8.12](#x1-169015r12)*æ‰€ç¤ºçš„çº¢æœ¨æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†ï¼ˆé‡‡ç”¨GPLè®¸å¯è¯åˆ†å‘ï¼‰æ¥è‡ªGPstuffåŒ…ã€‚æ•°æ®é›†åŒ…å«äº†æŸä¸€åœ°åŒºçº¢æœ¨æ ‘çš„ä½ç½®ã€‚æ¨æ–­çš„åŠ¨æœºæ˜¯è·å¾—ä¸€ä¸ªé€Ÿç‡å›¾ï¼Œè¡¨ç¤ºæŸä¸€åœ°åŒºå†…æ ‘æœ¨çš„æ•°é‡ã€‚
- en: 'As with the coal-mining disaster example, we need to discretize the data:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ç…¤çŸ¿ç¾éš¾ç¤ºä¾‹ç±»ä¼¼ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œç¦»æ•£åŒ–å¤„ç†ï¼š
- en: '**CodeÂ 8.12**'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.12**'
- en: '[PRE11]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![PIC](img/file233.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file233.png)'
- en: '**FigureÂ 8.12**: Redwood data'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 8.12**ï¼šçº¢æœ¨æ•°æ®'
- en: 'Notice that instead of doing a mesh grid, we treat `x1` and `x2` data as being
    distinct arrays. This allows us to build a covariance matrix independently for
    each coordinate, effectively reducing the size of the matrix needed to compute
    the GP. We then combine both matrices using the `LatentKron` class. It is important
    to note that this is not a numerical trick, but a mathematical property of the
    structure of this type of matrix, so we are not introducing any approximation
    or error in our model. We are just expressing it in a way that allows faster computations:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä¸å…¶åšç½‘æ ¼åŒ–å¤„ç†ï¼Œæˆ‘ä»¬å°†`x1`å’Œ`x2`æ•°æ®è§†ä¸ºç‹¬ç«‹çš„æ•°ç»„ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¸ºæ¯ä¸ªåæ ‡ç‹¬ç«‹åœ°æ„å»ºåæ–¹å·®çŸ©é˜µï¼Œä»è€Œæœ‰æ•ˆåœ°å‡å°‘è®¡ç®—é«˜æ–¯è¿‡ç¨‹æ‰€éœ€çš„çŸ©é˜µå¤§å°ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨`LatentKron`ç±»å°†ä¸¤ä¸ªçŸ©é˜µç»“åˆèµ·æ¥ã€‚éœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œè¿™ä¸æ˜¯ä¸€ç§æ•°å€¼æŠ€å·§ï¼Œè€Œæ˜¯æ­¤ç±»çŸ©é˜µç»“æ„çš„æ•°å­¦ç‰¹æ€§ï¼Œå› æ­¤æˆ‘ä»¬å¹¶æ²¡æœ‰åœ¨æ¨¡å‹ä¸­å¼•å…¥ä»»ä½•è¿‘ä¼¼æˆ–è¯¯å·®ã€‚æˆ‘ä»¬åªæ˜¯ä»¥ä¸€ç§æ–¹å¼è¡¨è¾¾å®ƒï¼Œä»è€Œå®ç°æ›´å¿«çš„è®¡ç®—ï¼š
- en: '**CodeÂ 8.13**'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.13**'
- en: '[PRE12]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In *Figure [8.13](#x1-169028r13)*, the darker the shade of gray, the higher
    the rate of trees. We may imagine that we are interested in finding high-growing
    rate zones, because we may be interested in how a wood is recovering from a fire,
    or maybe we are interested in some properties of the soil and we use the trees
    as a proxy.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*å›¾ [8.13](#x1-169028r13)*ä¸­ï¼Œç°è‰²çš„é˜´å½±è¶Šæ·±ï¼Œæ ‘æœ¨çš„ç”Ÿé•¿é€Ÿç‡è¶Šé«˜ã€‚æˆ‘ä»¬å¯ä»¥æƒ³è±¡ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°é«˜ç”Ÿé•¿é€Ÿç‡çš„åŒºåŸŸï¼Œå› ä¸ºæˆ‘ä»¬å¯èƒ½å¯¹æœ¨æä»ç«ç¾ä¸­æ¢å¤çš„æƒ…å†µæ„Ÿå…´è¶£ï¼Œæˆ–è€…æˆ‘ä»¬å¯èƒ½å¯¹åœŸå£¤çš„ä¸€äº›ç‰¹æ€§æ„Ÿå…´è¶£ï¼Œå¹¶ç”¨æ ‘æœ¨ä½œä¸ºä»£ç†ã€‚
- en: '![PIC](img/file234.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file234.png)'
- en: '**FigureÂ 8.13**: Logistic regression, result of `model_rw`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 8.13**ï¼šé€»è¾‘å›å½’ï¼Œ`model_rw`çš„ç»“æœ'
- en: 8.9 Regression with spatial autocorrelation
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.9 å¸¦æœ‰ç©ºé—´è‡ªç›¸å…³çš„å›å½’åˆ†æ
- en: 'The following example is taken from *Statistical Rethinking: A Bayesian Course
    with Examples in R and STAN, Second Edition by Richard McElreath, Copyright (2020)
    by Chapman and Hall/CRC. Reproduced by permission of Taylor & Francis Group*.
    I strongly recommend reading this book, as you will find many good examples like
    this and very good explanations. The only *caveat* is that the book examples are
    in R/Stan, but donâ€™t worry and keep sampling; you will find the Python/PyMC version
    of those examples in the [https://github.com/pymc-devs/pymc-resources](https://github.com/pymc-devs/pymc-resources)
    resources.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä»¥ä¸‹ç¤ºä¾‹æ‘˜è‡ª*Statistical Rethinking: A Bayesian Course with Examples in R and STAN,
    Second Edition by Richard McElreath, Copyright (2020) by Chapman and Hall/CRC.
    ç»Taylor & Francis Groupæˆæƒå¤åˆ¶*ã€‚æˆ‘å¼ºçƒˆæ¨èé˜…è¯»è¿™æœ¬ä¹¦ï¼Œå› ä¸ºä½ ä¼šå‘ç°å¾ˆå¤šåƒè¿™æ ·çš„å¥½ä¾‹å­ä»¥åŠéå¸¸å¥½çš„è§£é‡Šã€‚å”¯ä¸€çš„*è­¦å‘Š*æ˜¯ï¼Œè¿™äº›ä¹¦ä¸­çš„ä¾‹å­æ˜¯ç”¨R/Stanå®ç°çš„ï¼Œä½†åˆ«æ‹…å¿ƒå¹¶ç»§ç»­é‡‡æ ·ï¼›ä½ å¯ä»¥åœ¨[https://github.com/pymc-devs/pymc-resources](https://github.com/pymc-devs/pymc-resources)èµ„æºä¸­æ‰¾åˆ°è¿™äº›ä¾‹å­çš„Python/PyMCç‰ˆæœ¬ã€‚'
- en: For this example we have 10 different island societies; for each one of them,
    we have the number of tools they use. Some theories predict that larger populations
    develop and sustain more tools than smaller populations. Thus, we have a regression
    problem where the dependent variable is the number of tools and the independent
    variable is the population. Because the number of tools is a count variable, we
    can use a Poisson distribution. Additionally, we have good theoretical reasons
    to think the logarithm of the population is a better variable than absolute size
    because what really matters (according to the theory) is the order of magnitude
    of the population.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬æœ‰10ä¸ªä¸åŒçš„å²›å±¿ç¤¾ä¼šï¼›å¯¹äºæ¯ä¸€ä¸ªç¤¾ä¼šï¼Œæˆ‘ä»¬éƒ½æœ‰å®ƒä»¬ä½¿ç”¨çš„å·¥å…·æ•°é‡ã€‚ä¸€äº›ç†è®ºé¢„æµ‹ï¼Œè¾ƒå¤§çš„äººå£æ¯”å°äººå£èƒ½å‘å±•å’Œç»´æŒæ›´å¤šçš„å·¥å…·ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå›å½’é—®é¢˜ï¼Œå…¶ä¸­å› å˜é‡æ˜¯å·¥å…·çš„æ•°é‡ï¼Œç‹¬ç«‹å˜é‡æ˜¯äººå£ã€‚å› ä¸ºå·¥å…·æ•°é‡æ˜¯è®¡æ•°å˜é‡ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ³Šæ¾åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æœ‰å……åˆ†çš„ç†è®ºä¾æ®è®¤ä¸ºï¼Œäººå£çš„å¯¹æ•°æ¯”ç»å¯¹äººå£å¤§å°æ›´åˆé€‚ï¼Œå› ä¸ºçœŸæ­£é‡è¦çš„ï¼ˆæ ¹æ®ç†è®ºï¼‰æ˜¯äººå£çš„æ•°é‡çº§ã€‚
- en: So far, the model we have in mind is a Poisson regression, but here comes the
    interesting part. Another important factor affecting the number of tools is the
    contact rates among the island societies. One way to include the contact rate
    in our model is to gather information on how frequent these societies were in
    contact throughout history and to create a categorical variable such as low/high
    rate. Yet another way is to use the distance between societies as a proxy of the
    contact rate, since it is reasonable to assume that geographically close societies
    come into contact more often than distant ones.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ‰€è€ƒè™‘çš„æ¨¡å‹æ˜¯æ³Šæ¾å›å½’ï¼Œä½†è¿™é‡Œæœ‰ä¸ªæœ‰è¶£çš„éƒ¨åˆ†ã€‚å¦ä¸€ä¸ªå½±å“å·¥å…·æ•°é‡çš„é‡è¦å› ç´ æ˜¯å²›å±¿ç¤¾ä¼šä¹‹é—´çš„æ¥è§¦ç‡ã€‚å°†æ¥è§¦ç‡çº³å…¥æˆ‘ä»¬æ¨¡å‹çš„ä¸€ç§æ–¹æ³•æ˜¯æ”¶é›†è¿™äº›ç¤¾ä¼šåœ¨å†å²ä¸Šæ¥è§¦çš„é¢‘ç‡ä¿¡æ¯ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªåˆ†ç±»å˜é‡ï¼Œå¦‚ä½/é«˜æ¥è§¦ç‡ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨ç¤¾ä¼šä¹‹é—´çš„è·ç¦»ä½œä¸ºæ¥è§¦ç‡çš„ä»£ç†ï¼Œå› ä¸ºåˆç†çš„å‡è®¾æ˜¯ï¼Œåœ°ç†ä¸Šè¾ƒè¿‘çš„ç¤¾ä¼šæ¯”è¿œç¦»çš„ç¤¾ä¼šæ›´é¢‘ç¹æ¥è§¦ã€‚
- en: The number of tools, the population size, and the coordinates are stored in
    the file `islands.csv` in the GitHub repo of this book ([https://github.com/aloctavodia/BAP3](https://github.com/aloctavodia/BAP3)).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: å·¥å…·æ•°é‡ã€äººå£è§„æ¨¡å’Œåæ ‡å­˜å‚¨åœ¨æœ¬ä¹¦GitHubä»“åº“ä¸­çš„`islands.csv`æ–‡ä»¶é‡Œï¼ˆ[https://github.com/aloctavodia/BAP3](https://github.com/aloctavodia/BAP3)ï¼‰ã€‚
- en: 'Omitting the priors, the model we are going to build is:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: å¿½ç•¥å…ˆéªŒåˆ†å¸ƒï¼Œæˆ‘ä»¬å°†è¦æ„å»ºçš„æ¨¡å‹æ˜¯ï¼š
- en: '![](img/Formula_02.PNG)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_02.PNG)'
- en: This model is a linear model plus a GP term. We use the linear part to model
    the effect of the logarithm of the population and the GP term to model the effect
    of the distance/contact rate. In this way, we will be effectively incorporating
    a measure of similarity in technology exposure (estimated from the distance matrix).
    Thus, instead of assuming the total number is just a consequence of population
    alone and independent from one society to the next, we will be modeling the number
    of tools in each society as a function of their spatial distribution.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ªçº¿æ€§æ¨¡å‹åŠ ä¸Šä¸€ä¸ªé«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰é¡¹ã€‚æˆ‘ä»¬ç”¨çº¿æ€§éƒ¨åˆ†æ¥å»ºæ¨¡äººå£å¯¹æ•°çš„å½±å“ï¼Œç”¨é«˜æ–¯è¿‡ç¨‹é¡¹æ¥å»ºæ¨¡è·ç¦»/æ¥è§¦ç‡çš„å½±å“ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å®é™…ä¸Šåœ¨æœ‰æ•ˆåœ°çº³å…¥æŠ€æœ¯æš´éœ²çš„ç›¸ä¼¼æ€§åº¦é‡ï¼ˆä»è·ç¦»çŸ©é˜µä¼°ç®—ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸ä¼šå‡è®¾æ€»æ•°é‡ä»…ä»…æ˜¯äººå£çš„ç»“æœï¼Œä¸”å„ç¤¾ä¼šé—´ç›¸äº’ç‹¬ç«‹ï¼Œè€Œæ˜¯å°†æ¯ä¸ªç¤¾ä¼šçš„å·¥å…·æ•°é‡å»ºæ¨¡ä¸ºå…¶ç©ºé—´åˆ†å¸ƒçš„å‡½æ•°ã€‚
- en: The information about the spatial distribution is in terms of latitudes and
    longitudes, but the kernels in PyMC assume the distances are all Euclidean. This
    can be problematic. Probably the cleanest way to circumvent this issue is to work
    with a distance that takes into account that the islands are on an approximately
    spherical planet. For instance, we can use the haversine distance, which determines
    the great-circle distance between two points on a sphere given their longitudes
    and latitudes. The great-circle distance is the shortest distance between two
    points on the surface of a sphere, measured along the surface of the sphere. To
    use this distance, we need to create a new kernel as shown in the next code block.
    If you are not very familiar with classes in Python, you just need to know that
    what I did is copy the code for the `ExpQuad` from the PyMC code base and tweak
    it a little bit to create a new class, `ExpQuadHaversine`. The largest change
    is the addition of the function/method `haversine_distance`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºç©ºé—´åˆ†å¸ƒçš„ä¿¡æ¯æ˜¯ä»¥çº¬åº¦å’Œç»åº¦çš„å½¢å¼å­˜åœ¨çš„ï¼Œä½†PyMCä¸­çš„æ ¸å‡½æ•°å‡è®¾æ‰€æœ‰è·ç¦»éƒ½æ˜¯æ¬§å‡ é‡Œå¾—è·ç¦»ã€‚è¿™å¯èƒ½ä¼šå¸¦æ¥é—®é¢˜ã€‚ç»•è¿‡è¿™ä¸ªé—®é¢˜çš„æœ€ç®€æ´æ–¹æ³•å¯èƒ½æ˜¯ä½¿ç”¨è€ƒè™‘å²›å±¿å¤„äºå¤§è‡´çƒå½¢åœ°çƒä¸Šçš„è·ç¦»ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å“ˆå¼—è¾›è·ç¦»ï¼Œå®ƒåŸºäºç»çº¬åº¦è®¡ç®—ä¸¤ä¸ªç‚¹ä¹‹é—´çš„å¤§åœ†è·ç¦»ã€‚å¤§åœ†è·ç¦»æ˜¯çƒé¢ä¸Šä¸¤ç‚¹ä¹‹é—´çš„æœ€çŸ­è·ç¦»ï¼Œæ˜¯æ²¿ç€çƒé¢æµ‹é‡çš„ã€‚ä¸ºäº†ä½¿ç”¨è¿™ç§è·ç¦»ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªæ–°çš„æ ¸å‡½æ•°ï¼Œå¦‚ä¸‹ä¸€ä¸ªä»£ç å—æ‰€ç¤ºã€‚å¦‚æœä½ å¯¹Pythonä¸­çš„ç±»ä¸å¤ªç†Ÿæ‚‰ï¼Œä½ åªéœ€è¦çŸ¥é“æˆ‘æ‰€åšçš„æ˜¯å¤åˆ¶äº†PyMCä»£ç åº“ä¸­çš„`ExpQuad`ä»£ç ï¼Œå¹¶ç¨å¾®ä¿®æ”¹å®ƒï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ç±»`ExpQuadHaversine`ã€‚æœ€å¤§å˜åŒ–æ˜¯æ·»åŠ äº†å‡½æ•°/æ–¹æ³•`haversine_distance`ã€‚
- en: '**CodeÂ 8.14**'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.14**'
- en: '[PRE13]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that we have defined the class `ExpQuadHaversine` we can use it to define
    the covariance matrix as we did with the previous models with the built-in kernels.
    For this model, we are going to introduce another change. We are going to define
    a parameter *Î·*. The role of this parameter is to scale the GP in the y-axis direction.
    It is pretty common to define GPs with both *â„“* and *Î·*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å®šä¹‰äº†ç±»`ExpQuadHaversine`ï¼Œå¯ä»¥åƒä½¿ç”¨ä¹‹å‰çš„å†…ç½®æ ¸å‡½æ•°é‚£æ ·ï¼Œä½¿ç”¨å®ƒæ¥å®šä¹‰åæ–¹å·®çŸ©é˜µã€‚å¯¹äºè¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬å°†å¼•å…¥å¦ä¸€ä¸ªå˜åŒ–ã€‚æˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ªå‚æ•°*Î·*ã€‚è¿™ä¸ªå‚æ•°çš„ä½œç”¨æ˜¯å¯¹é«˜æ–¯è¿‡ç¨‹åœ¨yè½´æ–¹å‘è¿›è¡Œç¼©æ”¾ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬ä¼šä¸ºé«˜æ–¯è¿‡ç¨‹å®šä¹‰*â„“*å’Œ*Î·*è¿™ä¸¤ä¸ªå‚æ•°ã€‚
- en: '**CodeÂ 8.15**'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.15**'
- en: '[PRE14]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: To understand the posterior distribution of covariance functions in terms of
    distances, we can plot a few samples from the posterior distribution as in *Figure
    [8.14](#x1-170037r14)*. The black curve represents the posterior median covariance
    at each distance and the gray curves sample functions from the joint posterior
    distribution of *â„“* and *Î·*.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£åæ–¹å·®å‡½æ•°ç›¸å¯¹äºè·ç¦»çš„åéªŒåˆ†å¸ƒï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶ä¸€äº›æ¥è‡ªåéªŒåˆ†å¸ƒçš„æ ·æœ¬ï¼Œå¦‚ *å›¾ [8.14](#x1-170037r14)* æ‰€ç¤ºã€‚é»‘è‰²æ›²çº¿è¡¨ç¤ºæ¯ä¸ªè·ç¦»çš„åéªŒä¸­ä½æ•°åæ–¹å·®ï¼Œç°è‰²æ›²çº¿è¡¨ç¤ºä»
    *â„“* å’Œ *Î·* çš„è”åˆåéªŒåˆ†å¸ƒä¸­é‡‡æ ·çš„å‡½æ•°ã€‚
- en: '![PIC](img/file235.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file235.png)'
- en: '**FigureÂ 8.14**: Posterior distribution of the spatial covariance'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 8.14**ï¼šç©ºé—´åæ–¹å·®çš„åéªŒåˆ†å¸ƒ'
- en: The thick black line in *Figure [8.14](#x1-170037r14)* is the posterior median
    of the covariance between pairs of societies as a function of distance. We use
    the median because the distributions for *â„“* and *Î·* are very skewed. We can see
    that the covariance is, on average, not that high and also drops to almost 0 at
    about 2,000 kilometers. The thin lines represent the uncertainty, and we can see
    that there is a lot of uncertainty.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ [8.14](#x1-170037r14)* ä¸­çš„ç²—é»‘çº¿è¡¨ç¤ºç¤¾ä¼šå¯¹ä¹‹é—´çš„åæ–¹å·®çš„åéªŒä¸­ä½æ•°ä¸è·ç¦»çš„å…³ç³»ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸­ä½æ•°æ˜¯å› ä¸º *â„“* å’Œ *Î·*
    çš„åˆ†å¸ƒéå¸¸åæ–œã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåæ–¹å·®çš„å¹³å‡å€¼å¹¶ä¸é«˜ï¼Œè€Œä¸”åœ¨å¤§çº¦ 2,000 å…¬é‡Œæ—¶å‡ ä¹é™åˆ° 0ã€‚ç»†çº¿è¡¨ç¤ºä¸ç¡®å®šæ€§ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å­˜åœ¨å¾ˆå¤§çš„ä¸ç¡®å®šæ€§ã€‚'
- en: Now letâ€™s take a look at how strongly correlated the island societies are according
    to the model and data. To do this, we have to turn the covariance matrix into
    a correlation matrix. See the accompanying code for details. *Figure [8.15](#x1-170038r15)*
    shows a heatmap of the mean correlation matrix.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹æ ¹æ®æ¨¡å‹å’Œæ•°æ®ï¼Œå²›å±¿ç¤¾ä¼šä¹‹é—´çš„ç›¸å…³æ€§æœ‰å¤šå¼ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¿…é¡»å°†åæ–¹å·®çŸ©é˜µè½¬æ¢ä¸ºç›¸å…³çŸ©é˜µã€‚è¯¦æƒ…è¯·å‚è§é™„å¸¦çš„ä»£ç ã€‚*å›¾ [8.15](#x1-170038r15)*
    æ˜¾ç¤ºäº†å‡å€¼ç›¸å…³çŸ©é˜µçš„çƒ­å›¾ã€‚
- en: '![PIC](img/file236.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file236.png)'
- en: '**FigureÂ 8.15**: Posterior mean correlation matrix'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 8.15**ï¼šåéªŒå‡å€¼ç›¸å…³çŸ©é˜µ'
- en: Two observations that stand out from the rest is, first, that Hawaii is very
    lonely. This makes sense, as Hawaii is very far away from the rest of the island
    societies. Also, we can see that Malekula (Ml), Tikopia (Ti), and Santa Cruz (SC)
    are highly correlated with one another. This also makes sense, as these societies
    are very close together, and they also have a similar number of tools.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å…¶ä»–è§‚å¯Ÿä¸­è„±é¢–è€Œå‡ºçš„ä¸¤ä¸ªè§‚ç‚¹æ˜¯ï¼Œé¦–å…ˆï¼Œå¤å¨å¤·éå¸¸å­¤ç‹¬ã€‚è¿™æ˜¯æœ‰é“ç†çš„ï¼Œå› ä¸ºå¤å¨å¤·è·ç¦»å…¶ä»–å²›å±¿ç¤¾ä¼šéå¸¸é¥è¿œã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°é©¬è±åº“æ‹‰ï¼ˆMlï¼‰ã€æç§‘çš®äºšï¼ˆTiï¼‰å’Œåœ£å…‹é²å…¹ï¼ˆSCï¼‰ä¹‹é—´é«˜åº¦ç›¸å…³ã€‚è¿™ä¹Ÿæ˜¯åˆç†çš„ï¼Œå› ä¸ºè¿™äº›ç¤¾ä¼šå½¼æ­¤éå¸¸æ¥è¿‘ï¼Œå¹¶ä¸”å®ƒä»¬ä¹Ÿæœ‰ç›¸ä¼¼æ•°é‡çš„å·¥å…·ã€‚
- en: The left panel of *Figure [8.16](#x1-170039r16)* is essentially a map. The island
    societies are represented in their relative positions. The lines are the posterior
    median correlations among societies. The opacity of the lines is proportional
    to the value of the correlations.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ [8.16](#x1-170039r16)* çš„å·¦é¢æ¿æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåœ°å›¾ã€‚å²›å±¿ç¤¾ä¼šè¢«è¡¨ç¤ºä¸ºå®ƒä»¬ç›¸å¯¹çš„ä½ç½®ã€‚çº¿æ¡è¡¨ç¤ºç¤¾ä¼šä¹‹é—´çš„åéªŒä¸­ä½æ•°ç›¸å…³æ€§ã€‚çº¿æ¡çš„é€æ˜åº¦ä¸ç›¸å…³æ€§çš„å€¼æˆæ­£æ¯”ã€‚'
- en: '![PIC](img/file237.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file237.png)'
- en: '**FigureÂ 8.16**: Posterior distribution of the spatial covariance'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 8.16**ï¼šç©ºé—´åæ–¹å·®çš„åéªŒåˆ†å¸ƒ'
- en: On the right panel of *Figure [8.16](#x1-170039r16)* , we have again the posterior
    median correlations, but this time plotted in terms of the log population versus
    the total number of tools. The dashed lines represent the median number of tools
    and the HDI of 94% as a function of log population. In both panels of *Figure
    [8.16](#x1-170039r16)*, the size of the dots is proportional to the population
    of each island society. Notice how the correlations among Malekula, Tikopia, and
    Santa Cruz describe the fact that they have a rather low number of tools close
    to the median or lower than the expected number of tools for their populations.
    Something similar is happening with Trobriand and Manus; they are geographically
    close and have fewer tools than expected for their population sizes. Tonga has
    way more tools than expected for its population and a relatively high correlation
    with Fiji. In a way, the model is telling us that Tonga has a positive effect
    on Lua Fiji, increasing the total number of tools and counteracting the effect
    of it on its close neighbors, Malekula, Tikopia, and Santa Cruz.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*å›¾[8.16](#x1-170039r16)*çš„å³ä¾§é¢æ¿ä¸­ï¼Œæˆ‘ä»¬å†æ¬¡å±•ç¤ºäº†åéªŒä¸­ä½æ•°ç›¸å…³æ€§ï¼Œä½†è¿™æ¬¡æ˜¯æ ¹æ®å¯¹æ•°äººå£ä¸å·¥å…·æ€»æ•°çš„å…³ç³»æ¥ç»˜åˆ¶çš„ã€‚è™šçº¿è¡¨ç¤ºå·¥å…·çš„ä¸­ä½æ•°å’Œ94%çš„é«˜å¯†åº¦åŒºé—´ï¼ˆHDIï¼‰ä½œä¸ºå¯¹æ•°äººå£çš„å‡½æ•°ã€‚åœ¨*å›¾[8.16](#x1-170039r16)*çš„ä¸¤ä¸ªé¢æ¿ä¸­ï¼Œç‚¹çš„å¤§å°ä¸æ¯ä¸ªå²›å±¿ç¤¾ä¼šçš„äººå£æˆæ­£æ¯”ã€‚æ³¨æ„ï¼Œé©¬åˆ—åº“æ‹‰ã€æç§‘çš®äºšå’Œåœ£å…‹é²æ–¯ä¹‹é—´çš„ç›¸å…³æ€§æè¿°äº†å®ƒä»¬å·¥å…·æ•°é‡ç›¸å¯¹è¾ƒå°‘ï¼Œæ¥è¿‘ä¸­ä½æ•°æˆ–ä½äºæ ¹æ®å…¶äººå£é¢„æœŸçš„å·¥å…·æ•°é‡ã€‚ç±»ä¼¼çš„æƒ…å†µå‘ç”Ÿåœ¨ç‰¹ç½—å¸ƒé‡Œå®‰ç¾¤å²›å’Œé©¬åŠªæ–¯å²›ï¼›å®ƒä»¬åœ°ç†ä½ç½®æ¥è¿‘ï¼Œä¸”å·¥å…·æ•°é‡ä½äºé¢„æœŸã€‚æ±¤åŠ çš„å·¥å…·æ•°é‡è¿œé«˜äºå…¶äººå£é¢„æœŸï¼Œå¹¶ä¸”ä¸æ–æµæœ‰è¾ƒé«˜çš„ç›¸å…³æ€§ã€‚ä»æŸç§ç¨‹åº¦ä¸Šè®²ï¼Œæ¨¡å‹å‘Šè¯‰æˆ‘ä»¬ï¼Œæ±¤åŠ å¯¹æ–æµçš„å½±å“æ˜¯ç§¯æçš„ï¼Œå¢åŠ äº†å·¥å…·çš„æ€»æ•°ï¼ŒåŒæ—¶æŠµæ¶ˆäº†å¯¹å…¶é‚»è¿‘åœ°åŒºé©¬åˆ—åº“æ‹‰ã€æç§‘çš®äºšå’Œåœ£å…‹é²æ–¯çš„å½±å“ã€‚
- en: 8.10 Hilbert space GPs
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.10 å¸Œå°”ä¼¯ç‰¹ç©ºé—´é«˜æ–¯è¿‡ç¨‹ï¼ˆHSGPï¼‰
- en: Gaussian processes can be slow. The main reason is that their computation requires
    us to invert a matrix, whose size grows with the number of observations. This
    operation is computationally costly and does not scale very nicely. For that reason,
    a large portion of the research around GPs has been to find approximations to
    compute them faster and allow scaling them to large data.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ–¯è¿‡ç¨‹å¯èƒ½ä¼šæ¯”è¾ƒæ…¢ã€‚ä¸»è¦åŸå› æ˜¯å®ƒä»¬çš„è®¡ç®—éœ€è¦æˆ‘ä»¬å¯¹ä¸€ä¸ªçŸ©é˜µè¿›è¡Œæ±‚é€†ï¼Œè€Œè¯¥çŸ©é˜µçš„å¤§å°ä¼šéšç€è§‚å¯Ÿæ•°æ®çš„å¢å¤šè€Œå¢é•¿ã€‚è¿™ä¸ªæ“ä½œè®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œè€Œä¸”æ‰©å±•æ€§ä¸å¥½ã€‚å› æ­¤ï¼Œå›´ç»•é«˜æ–¯è¿‡ç¨‹çš„ç ”ç©¶å¤§éƒ¨åˆ†é›†ä¸­åœ¨å¯»æ‰¾è¿‘ä¼¼æ–¹æ³•ï¼Œä»¥ä¾¿æ›´å¿«åœ°è®¡ç®—å®ƒä»¬å¹¶ä½¿å…¶èƒ½å¤Ÿæ‰©å±•åˆ°å¤§è§„æ¨¡æ•°æ®ã€‚
- en: We are going to discuss only one of those approximations, namely the **Hilbert
    Space Gaussian Process** (**HSGP**), without going into the details of how this
    approximation is achieved. Conceptually, we can think of it as a basis function
    expansion similar, in spirit, to how splines are constructed (see *Chapter [6](CH06.xhtml#x1-1200006)*).
    The consequence of this approximation is that it turns the matrix inversion into
    just matrix multiplication, a much faster operation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åªè®¨è®ºå…¶ä¸­ä¸€ç§è¿‘ä¼¼æ–¹æ³•ï¼Œå³**å¸Œå°”ä¼¯ç‰¹ç©ºé—´é«˜æ–¯è¿‡ç¨‹**ï¼ˆ**HSGP**ï¼‰ï¼Œè€Œä¸æ·±å…¥æ¢è®¨è¿™ç§è¿‘ä¼¼æ˜¯å¦‚ä½•å®ç°çš„ã€‚ä»æ¦‚å¿µä¸Šè®²ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶è§†ä¸ºä¸€ç§åŸºå‡½æ•°å±•å¼€ï¼Œç±»ä¼¼äºæ ·æ¡å‡½æ•°çš„æ„å»ºæ–¹å¼ï¼ˆå‚è§*ç¬¬[6ç« ](CH06.xhtml#x1-1200006)*ï¼‰ã€‚è¿™ç§è¿‘ä¼¼çš„ç»“æœæ˜¯ï¼Œå®ƒå°†çŸ©é˜µæ±‚é€†æ“ä½œè½¬åŒ–ä¸ºçŸ©é˜µä¹˜æ³•ï¼Œè¿™æ˜¯ä¸€ç§é€Ÿåº¦æ›´å¿«çš„æ“ä½œã€‚
- en: But When Will It Work?
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä»€ä¹ˆæ—¶å€™å®ƒèƒ½èµ·ä½œç”¨ï¼Ÿ
- en: We can only use HSGPs for low dimensions (1 to maybe 3 or 4), and only for some
    kernels like the exponential quadratic or Matern. The reason is that for the HSGP
    approximation to work, the kernel has to be written in a special form known as
    power spectral density, and not all kernels can be written in this form.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªèƒ½åœ¨ä½ç»´åº¦ï¼ˆå¤§çº¦1åˆ°3æˆ–4ç»´ï¼‰ä¸­ä½¿ç”¨HSGPï¼Œå¹¶ä¸”ä»…é™äºæŸäº›æ ¸å‡½æ•°ï¼Œå¦‚æŒ‡æ•°äºŒæ¬¡æ ¸æˆ–é©¬ç‰¹æ©æ ¸ã€‚åŸå› æ˜¯ï¼Œä¸ºäº†ä½¿HSGPè¿‘ä¼¼æ³•ç”Ÿæ•ˆï¼Œæ ¸å‡½æ•°å¿…é¡»ä»¥ä¸€ç§ç‰¹æ®Šçš„å½¢å¼è¡¨ç¤ºï¼Œå³åŠŸç‡è°±å¯†åº¦å½¢å¼ï¼Œè€Œå¹¶éæ‰€æœ‰æ ¸å‡½æ•°éƒ½å¯ä»¥ç”¨è¿™ç§å½¢å¼è¡¨ç¤ºã€‚
- en: Using the HSGP approximation in PyMC is straightforward, as we will demonstrate
    with the bikes dataset. We want to model the number of rented bikes as a function
    of the time of the day in hours. The following code block shows the PyMC implementation
    of such a model.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨PyMCä¸­ä½¿ç”¨HSGPè¿‘ä¼¼æ–¹æ³•æ˜¯éå¸¸ç›´æ¥çš„ï¼Œæˆ‘ä»¬å°†åœ¨è‡ªè¡Œè½¦æ•°æ®é›†ä¸Šæ¼”ç¤ºè¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬å¸Œæœ›å°†å‡ºç§Ÿè‡ªè¡Œè½¦çš„æ•°é‡å»ºæ¨¡ä¸ºä¸€å¤©ä¸­å°æ—¶æ•°çš„å‡½æ•°ã€‚ä»¥ä¸‹ä»£ç å—å±•ç¤ºäº†æ­¤æ¨¡å‹çš„PyMCå®ç°ã€‚
- en: '**CodeÂ 8.16**'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.16**'
- en: '[PRE15]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The main difference from previous GP models is the use of the `pm.gp.HSGP(.)`
    class instead of the `pm.gp.Latent(.)` class, which we should have used for non-Gaussian
    likelihoods and standard GPs. The class `pm.gp.HSGP(.)` has two parameters:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¹‹å‰çš„é«˜æ–¯è¿‡ç¨‹æ¨¡å‹çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œä½¿ç”¨äº†`pm.gp.HSGP(.)`ç±»ï¼Œè€Œä¸æ˜¯ç”¨äºéé«˜æ–¯ä¼¼ç„¶å’Œæ ‡å‡†é«˜æ–¯è¿‡ç¨‹çš„`pm.gp.Latent(.)`ç±»ã€‚`pm.gp.HSGP(.)`ç±»æœ‰ä¸¤ä¸ªå‚æ•°ï¼š
- en: '`m` is the number of basic functions we use to approximate the GP. The larger
    the value of `m`, the better the approximation will be and the more costly the
    computation.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`m` æ˜¯æˆ‘ä»¬ç”¨æ¥é€¼è¿‘ GP çš„åŸºæœ¬å‡½æ•°æ•°é‡ã€‚`m` å€¼è¶Šå¤§ï¼Œé€¼è¿‘æ•ˆæœè¶Šå¥½ï¼Œä½†è®¡ç®—æˆæœ¬ä¹Ÿè¶Šé«˜ã€‚'
- en: '`c` is a boundary factor. For a fixed and sufficiently large value of `m`,
    `c` affects the approximation of the mean function mainly near the boundaries.
    It should not be smaller than 1.2 (PyMC will give you a warning if you use a value
    lower than this), and usually 1.5 is a good choice. Changing this parameter does
    not affect the speed of the computations.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`c` æ˜¯ä¸€ä¸ªè¾¹ç•Œå› å­ã€‚å¯¹äºä¸€ä¸ªå›ºå®šä¸”è¶³å¤Ÿå¤§çš„ `m` å€¼ï¼Œ`c` ä¸»è¦å½±å“å‡å€¼å‡½æ•°åœ¨è¾¹ç•Œé™„è¿‘çš„é€¼è¿‘ã€‚å®ƒä¸åº”å°äº 1.2ï¼ˆå¦‚æœä½¿ç”¨è¾ƒå°çš„å€¼ï¼ŒPyMC
    ä¼šç»™å‡ºè­¦å‘Šï¼‰ï¼Œé€šå¸¸ 1.5 æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚æ”¹å˜æ­¤å‚æ•°ä¸ä¼šå½±å“è®¡ç®—é€Ÿåº¦ã€‚'
- en: We set `m=10` partially because we are fans of the decimal system and partially
    based on the recommendations in the paper *Practical Hilbert space approximate
    Bayesian Gaussian processes for probabilistic programming* written by [Riutort-Mayol
    etÂ al.](Bibliography.xhtml#Xriutortmayol_2022)Â [[2022](Bibliography.xhtml#Xriutortmayol_2022)].
    In practice, the results are robust to the exact values of `m` and `c`, as long
    as they are within a certain range based on what your prior for the length scale
    is. For details on how HSGP works and some advice on how to use it in practice,
    you can read [Riutort-Mayol etÂ al.](Bibliography.xhtml#Xriutortmayol_2022)Â [[2022](Bibliography.xhtml#Xriutortmayol_2022)].
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¾ç½® `m=10`ï¼Œéƒ¨åˆ†åŸå› æ˜¯æˆ‘ä»¬å–œæ¬¢åè¿›åˆ¶ç³»ç»Ÿï¼Œéƒ¨åˆ†åŸå› æ˜¯åŸºäº [Riutort-Mayol et al.](Bibliography.xhtml#Xriutortmayol_2022)
    å‘è¡¨çš„è®ºæ–‡ã€ŠPractical Hilbert space approximate Bayesian Gaussian processes for probabilistic
    programmingã€‹ä¸­çš„å»ºè®® [[2022](Bibliography.xhtml#Xriutortmayol_2022)]ã€‚å®é™…ä¸Šï¼Œåªè¦ `m` å’Œ
    `c` çš„å€¼åœ¨æŸä¸ªèŒƒå›´å†…ï¼Œä¸é•¿åº¦å°ºåº¦çš„å…ˆéªŒä¸€è‡´ï¼Œç»“æœå¯¹å®ƒä»¬çš„ç²¾ç¡®å€¼æ˜¯ç¨³å¥çš„ã€‚å…³äº HSGP å¦‚ä½•å·¥ä½œä»¥åŠå¦‚ä½•åœ¨å®è·µä¸­ä½¿ç”¨å®ƒçš„ä¸€äº›å»ºè®®ï¼Œå¯ä»¥å‚è€ƒ [Riutort-Mayol
    et al.](Bibliography.xhtml#Xriutortmayol_2022) [[2022](Bibliography.xhtml#Xriutortmayol_2022)]ã€‚
- en: Now letâ€™s see the results. *Figure [8.17](#x1-171015r17)* shows the mean posterior
    GP in black and 100 samples (realizations) from the GP posterior (gray lines).
    You can compare these results to the ones obtained using splines (see *Figure
    [6.8](CH06.xhtml#x1-124013r8)*).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ç»“æœã€‚*å›¾ [8.17](#x1-171015r17)* æ˜¾ç¤ºäº†é»‘è‰²çš„å‡å€¼åéªŒ GP å’Œæ¥è‡ª GP åéªŒçš„ 100 ä¸ªæ ·æœ¬ï¼ˆç°è‰²çº¿ï¼‰ã€‚ä½ å¯ä»¥å°†è¿™äº›ç»“æœä¸ä½¿ç”¨æ ·æ¡å¾—åˆ°çš„ç»“æœè¿›è¡Œæ¯”è¾ƒï¼ˆè§*å›¾
    [6.8](CH06.xhtml#x1-124013r8)*ï¼‰ã€‚
- en: '![PIC](img/file238.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file238.png)'
- en: '**FigureÂ 8.17**: Posterior mean for the HSGP model for rented bikes as a function
    of the time of the day'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 8.17**ï¼šHSGP æ¨¡å‹å¯¹ç§Ÿèµè‡ªè¡Œè½¦çš„åéªŒå‡å€¼ï¼Œä½œä¸ºä¸€å¤©ä¸­æ—¶é—´çš„å‡½æ•°'
- en: The HSGP approximation is also implemented in Bambi. Letâ€™s see how we can use
    it.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: HSGP è¿‘ä¼¼ä¹Ÿå·²åœ¨ Bambi ä¸­å®ç°ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨å®ƒã€‚
- en: 8.10.1 HSGP with Bambi
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.10.1 HSGP ä¸ Bambi
- en: 'To fit the previous model with Bambi, we need to write the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨ Bambi æ‹Ÿåˆä¹‹å‰çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ç¼–å†™ä»¥ä¸‹ä»£ç ï¼š
- en: '**CodeÂ 8.17**'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.17**'
- en: '[PRE16]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This will work, but instead, we will provide priors to Bambi, as we did with
    PyMC. This will result in a much faster sampling and more reliable samples.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·æ˜¯å¯è¡Œçš„ï¼Œä½†æˆ‘ä»¬å°†ä¸º Bambi æä¾›å…ˆéªŒï¼Œå°±åƒæˆ‘ä»¬åœ¨ PyMC ä¸­åšçš„é‚£æ ·ã€‚è¿™å°†å¯¼è‡´æ›´å¿«çš„é‡‡æ ·å’Œæ›´å¯é çš„æ ·æœ¬ã€‚
- en: 'As we saw in *Chapter [6](CH06.xhtml#x1-1200006)*, to define priors in Bambi,
    we just need to pass a dictionary to the `priors` argument of `bmb.Model`. But
    we must be aware that HSGP terms do not receive priors. Instead, we need to define
    priors for *â„“* (called `ell` in Bambi) and *Î·* (called `sigma` in Bambi) and pass
    those priors to the HSGP terms. One more thing: as in the previous model, we did
    not use *Î·* but since Bambi is expecting it, we use a dirty trick to define a
    prior that is essentially 1.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨*ç¬¬ [6](CH06.xhtml#x1-1200006)* ç« ä¸­çœ‹åˆ°çš„é‚£æ ·ï¼Œè¦åœ¨ Bambi ä¸­å®šä¹‰å…ˆéªŒï¼Œæˆ‘ä»¬åªéœ€å°†å­—å…¸ä¼ é€’ç»™ `bmb.Model`
    çš„ `priors` å‚æ•°ã€‚ä½†æˆ‘ä»¬å¿…é¡»æ³¨æ„ï¼ŒHSGP é¡¹ä¸ä¼šæ¥æ”¶å…ˆéªŒã€‚ç›¸åï¼Œæˆ‘ä»¬éœ€è¦ä¸º *â„“*ï¼ˆåœ¨ Bambi ä¸­ç§°ä¸º `ell`ï¼‰å’Œ *Î·*ï¼ˆåœ¨ Bambi
    ä¸­ç§°ä¸º `sigma`ï¼‰å®šä¹‰å…ˆéªŒï¼Œå¹¶å°†è¿™äº›å…ˆéªŒä¼ é€’ç»™ HSGP é¡¹ã€‚è¿˜æœ‰ä¸€ä»¶äº‹ï¼šå’Œä¹‹å‰çš„æ¨¡å‹ä¸€æ ·ï¼Œæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨ *Î·*ï¼Œä½†ç”±äº Bambi éœ€è¦å®ƒï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå°æŠ€å·§æ¥å®šä¹‰ä¸€ä¸ªåŸºæœ¬ä¸Šæ˜¯
    1 çš„å…ˆéªŒã€‚
- en: '**CodeÂ 8.18**'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  8.18**'
- en: '[PRE17]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: I invite you to check that the parameters computed by Bambi are very similar
    to those we got with PyMC. *Figure [8.18](#x1-172023r18)* shows the mean posterior
    GP in black and a band for the HDI of 94%. The figure was generated with `bmb.interpret.plot_predictions`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘é‚€è¯·ä½ æ£€æŸ¥ Bambi è®¡ç®—çš„å‚æ•°ï¼Œå®ƒä»¬ä¸æˆ‘ä»¬é€šè¿‡ PyMC å¾—åˆ°çš„éå¸¸ç›¸ä¼¼ã€‚*å›¾ [8.18](#x1-172023r18)* æ˜¾ç¤ºäº†é»‘è‰²çš„å‡å€¼åéªŒ
    GP å’Œ 94% HDI çš„å¸¦çŠ¶åŒºåŸŸã€‚è¯¥å›¾æ˜¯é€šè¿‡ `bmb.interpret.plot_predictions` ç”Ÿæˆçš„ã€‚
- en: '![PIC](img/file239.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file239.png)'
- en: '**FigureÂ 8.18**: Posterior mean for the HSGP model for rented bikes as a function
    of the time of the day, using Bambi'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 8.18**ï¼šHSGP æ¨¡å‹å¯¹ç§Ÿèµè‡ªè¡Œè½¦çš„åéªŒå‡å€¼ï¼Œä½œä¸ºä¸€å¤©ä¸­æ—¶é—´çš„å‡½æ•°ï¼Œä½¿ç”¨ Bambi'
- en: In this section, we have explored the concept of HSGP as a powerful approximation
    to scale Gaussian processes to large datasets. By combining the flexibility of
    PyMC and Bambi with the scalability offered by HSGPs, researchers and practitioners
    can more effectively tackle complex modeling tasks, paving the way for the application
    of Gaussian processes on increasingly large and intricate datasets.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†HSGPçš„æ¦‚å¿µï¼Œä½œä¸ºä¸€ç§å¼ºå¤§çš„è¿‘ä¼¼æ–¹æ³•ï¼Œå¯ä»¥å°†é«˜æ–¯è¿‡ç¨‹æ‰©å±•åˆ°å¤§æ•°æ®é›†ã€‚é€šè¿‡å°†PyMCå’ŒBambiçš„çµæ´»æ€§ä¸HSGPæä¾›çš„å¯æ‰©å±•æ€§ç›¸ç»“åˆï¼Œç ”ç©¶äººå‘˜å’Œå®è·µè€…å¯ä»¥æ›´æœ‰æ•ˆåœ°è§£å†³å¤æ‚çš„å»ºæ¨¡ä»»åŠ¡ï¼Œä¸ºåœ¨æ—¥ç›Šåºå¤§å’Œå¤æ‚çš„æ•°æ®é›†ä¸Šåº”ç”¨é«˜æ–¯è¿‡ç¨‹é“ºå¹³é“è·¯ã€‚
- en: 8.11 Summary
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.11 æ€»ç»“
- en: A Gaussian process is a generalization of the multivariate Gaussian distribution
    to infinitely many dimensions and is fully specified by a mean function and a
    covariance function. Since we can conceptually think of functions as infinitely
    long vectors, we can use Gaussian processes as priors over functions. In practice,
    we do not work with infinite objects but with multivariate Gaussian distributions
    with as many dimensions as data points. To define their corresponding covariance
    function, we used properly parameterized kernels; and by learning about those
    hyperparameters, we ended up learning about arbitrary complex functions.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ–¯è¿‡ç¨‹æ˜¯å¤šå…ƒé«˜æ–¯åˆ†å¸ƒçš„ä¸€ç§æ¨å¹¿ï¼Œé€‚ç”¨äºæ— é™å¤šä¸ªç»´åº¦ï¼Œå®Œå…¨ç”±å‡å€¼å‡½æ•°å’Œåæ–¹å·®å‡½æ•°æŒ‡å®šã€‚ç”±äºæˆ‘ä»¬å¯ä»¥æ¦‚å¿µä¸Šå°†å‡½æ•°è§†ä¸ºæ— é™é•¿çš„å‘é‡ï¼Œå› æ­¤å¯ä»¥å°†é«˜æ–¯è¿‡ç¨‹ä½œä¸ºå‡½æ•°çš„å…ˆéªŒã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å¹¶ä¸å¤„ç†æ— é™å¯¹è±¡ï¼Œè€Œæ˜¯å¤„ç†ä¸æ•°æ®ç‚¹æ•°é‡ç›¸ç­‰ç»´åº¦çš„å¤šå…ƒé«˜æ–¯åˆ†å¸ƒã€‚ä¸ºäº†å®šä¹‰å…¶ç›¸åº”çš„åæ–¹å·®å‡½æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†é€‚å½“å‚æ•°åŒ–çš„æ ¸ï¼›é€šè¿‡å­¦ä¹ è¿™äº›è¶…å‚æ•°ï¼Œæˆ‘ä»¬æœ€ç»ˆå­¦ä¼šäº†å…³äºä»»æ„å¤æ‚å‡½æ•°çš„çŸ¥è¯†ã€‚
- en: In this chapter, we have given a short introduction to GPs. We have covered
    regression, semi-parametric models (the islands example), combining two or more
    kernels to better describe the unknown function, and how a GP can be used for
    classification tasks. There are many other topics we could have discussed. Nevertheless,
    I hope this introduction to GPs has motivated you sufficiently to keep using,
    reading, and learning about Gaussian processes and Bayesian non-parametric models.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬ç®€è¦ä»‹ç»äº†é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰ã€‚æˆ‘ä»¬æ¶µç›–äº†å›å½’ã€åŠå‚æ•°æ¨¡å‹ï¼ˆå²›å±¿ç¤ºä¾‹ï¼‰ã€å°†ä¸¤ä¸ªæˆ–å¤šä¸ªæ ¸ç»„åˆä»¥æ›´å¥½åœ°æè¿°æœªçŸ¥å‡½æ•°ï¼Œä»¥åŠé«˜æ–¯è¿‡ç¨‹å¦‚ä½•ç”¨äºåˆ†ç±»ä»»åŠ¡ã€‚è¿˜æœ‰è®¸å¤šå…¶ä»–ä¸»é¢˜æˆ‘ä»¬å¯ä»¥è®¨è®ºã€‚ç„¶è€Œï¼Œæˆ‘å¸Œæœ›è¿™æ®µé«˜æ–¯è¿‡ç¨‹çš„ä»‹ç»è¶³ä»¥æ¿€åŠ±ä½ ç»§ç»­ä½¿ç”¨ã€é˜…è¯»å’Œå­¦ä¹ é«˜æ–¯è¿‡ç¨‹åŠè´å¶æ–¯éå‚æ•°æ¨¡å‹ã€‚
- en: 8.12 Exercises
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.12 ç»ƒä¹ 
- en: For the example in the *Covariance functions and kernels* section, make sure
    you understand the relationship between the input data and the generated covariance
    matrix. Try using other input such as `data = np.random.normal(size=4)`.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äº*åæ–¹å·®å‡½æ•°å’Œæ ¸*éƒ¨åˆ†ä¸­çš„ç¤ºä¾‹ï¼Œç¡®ä¿ç†è§£è¾“å…¥æ•°æ®ä¸ç”Ÿæˆçš„åæ–¹å·®çŸ©é˜µä¹‹é—´çš„å…³ç³»ã€‚å°è¯•ä½¿ç”¨å…¶ä»–è¾“å…¥ï¼Œå¦‚`data = np.random.normal(size=4)`ã€‚
- en: Rerun the code generating *Figure [8.3](#x1-160010r3)* and increase the number
    of samples obtained from the GP prior to around 200\. In the original figure,
    the number of samples is 2\. What is the range of the generated values?
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é‡æ–°è¿è¡Œç”Ÿæˆ*å›¾ 8.3*çš„ä»£ç ï¼Œå¹¶å°†ä»é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰å…ˆéªŒä¸­è·å¾—çš„æ ·æœ¬æ•°é‡å¢åŠ åˆ°çº¦200ã€‚åŸå§‹å›¾ä¸­çš„æ ·æœ¬æ•°é‡ä¸º2ã€‚ç”Ÿæˆå€¼çš„èŒƒå›´æ˜¯ä»€ä¹ˆï¼Ÿ
- en: 'For the generated plot in the previous exercise, compute the standard deviation
    for the values at each point. Do this in the following form:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºå‰ä¸€ä¸ªç»ƒä¹ ä¸­ç”Ÿæˆçš„å›¾è¡¨ï¼Œè®¡ç®—æ¯ä¸ªç‚¹çš„æ ‡å‡†å·®ã€‚æŒ‰ç…§ä»¥ä¸‹å½¢å¼è¿›è¡Œæ“ä½œï¼š
- en: Visually, just observing the plots
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»è§†è§‰ä¸Šçœ‹ï¼Œä»…ä»…è§‚å¯Ÿå›¾è¡¨
- en: Directly from the values generated from `pz.MVNormal(.).rvs`
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›´æ¥æ¥è‡ªäº`pz.MVNormal(.).rvs`ç”Ÿæˆçš„å€¼
- en: By inspecting the covariance matrix (if you have doubts go back to exercise
    1)
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡æ£€æŸ¥åæ–¹å·®çŸ©é˜µï¼ˆå¦‚æœæœ‰ç–‘é—®ï¼Œè¯·å›åˆ°ç»ƒä¹ 1ï¼‰
- en: Did the values you get from these three methods match?
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»è¿™ä¸‰ç§æ–¹æ³•ä¸­å¾—åˆ°çš„å€¼æ˜¯å¦åŒ¹é…ï¼Ÿ
- en: Use test points `np.linspace(np.floor(x.min()), 20, 100)[:,None]` and re-run
    `model_reg`. Plot the results. What did you observe? How is this related to the
    specification of the GP prior?
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æµ‹è¯•ç‚¹`np.linspace(np.floor(x.min()), 20, 100)[:,None]`å¹¶é‡æ–°è¿è¡Œ`model_reg`ã€‚ç»˜åˆ¶ç»“æœã€‚ä½ è§‚å¯Ÿåˆ°äº†ä»€ä¹ˆï¼Ÿè¿™ä¸GPå…ˆéªŒçš„è§„èŒƒæœ‰ä½•å…³ç³»ï¼Ÿ
- en: Repeat exercise 1, but this time use a linear kernel (see the accompanying code
    for a linear kernel).
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é‡å¤ç»ƒä¹ 1ï¼Œä½†è¿™æ¬¡ä½¿ç”¨çº¿æ€§æ ¸ï¼ˆå‚è§é™„å¸¦çš„çº¿æ€§æ ¸ä»£ç ï¼‰ã€‚
- en: Check out [https://www.pymc.io/projects/examples/en/latest/gaussian_processes/GP-MeansAndCovs.html](https://www.pymc.io/projects/examples/en/latest/gaussian_processes/GP-MeansAndCovs.html)
    in PyMCâ€™s documentation.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¯·æŸ¥çœ‹PyMCæ–‡æ¡£ä¸­çš„[https://www.pymc.io/projects/examples/en/latest/gaussian_processes/GP-MeansAndCovs.html](https://www.pymc.io/projects/examples/en/latest/gaussian_processes/GP-MeansAndCovs.html)ã€‚
- en: Run a logistic regression model for the `space_flu` data. What do you see? Can
    you explain the result?
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹`space_flu`æ•°æ®è¿è¡Œé€»è¾‘å›å½’æ¨¡å‹ã€‚ä½ çœ‹åˆ°äº†ä»€ä¹ˆï¼Ÿä½ èƒ½è§£é‡Šç»“æœå—ï¼Ÿ
- en: 'Change the logistic regression model in order to fit the data. Tip: Use an
    order 2 polynomial.'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¿®æ”¹é€»è¾‘å›å½’æ¨¡å‹ä»¥é€‚åº”æ•°æ®ã€‚æç¤ºï¼šä½¿ç”¨äºŒé˜¶å¤šé¡¹å¼ã€‚
- en: Compare the model for the coal mining disaster with the one from the PyMC documentation
    ( [https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#case-study-2-coal-mining-disasters](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#case-study-2-coal-mining-disasters)).
    Describe the differences between both models in terms of model specification and
    results.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†ç…¤çŸ¿ç¾éš¾çš„æ¨¡å‹ä¸PyMCæ–‡æ¡£ä¸­çš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼ˆ[https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#case-study-2-coal-mining-disasters](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#case-study-2-coal-mining-disasters)ï¼‰ã€‚æè¿°è¿™ä¸¤ä¸ªæ¨¡å‹åœ¨æ¨¡å‹è§„èŒƒå’Œç»“æœæ–¹é¢çš„å·®å¼‚ã€‚
- en: Join our community Discord space
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘ä»¬çš„ç¤¾åŒºDiscordç©ºé—´
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at: [https://packt.link/bayesian](https://packt.link/bayesian)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘ä»¬çš„Discordç¤¾åŒºï¼Œç»“è¯†å¿—åŒé“åˆçš„äººï¼Œå¹¶ä¸5000å¤šåæˆå‘˜ä¸€èµ·å­¦ä¹ ï¼š[https://packt.link/bayesian](https://packt.link/bayesian)
- en: '![PIC](img/file1.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1.png)'
