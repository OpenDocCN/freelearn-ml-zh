- en: Chapter 8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章
- en: Gaussian Processes
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯过程
- en: Lonely? You have yourself. Your infinite selves. - Rick Sanchez (at least the
    one from dimension C-137)
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 孤单吗？你有你自己。你无限的自己。- Rick Sanchez（至少是C-137维度中的那个）
- en: In the last chapter, we learned about the Dirichlet process, an infinite-dimensional
    generalization of the Dirichlet distribution that can be used to set a prior on
    an unknown continuous distribution. In this chapter, we will learn about the Gaussian
    process, an infinite-dimensional generalization of the Gaussian distribution that
    can be used to set a prior on unknown functions. Both the Dirichlet process and
    the Gaussian process are used in Bayesian statistics to build flexible models
    where the number of parameters is allowed to increase with the size of the data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了狄利克雷过程，这是一种狄利克雷分布的无限维推广，可以用来对未知的连续分布设定先验。在本章中，我们将学习高斯过程，这是一种高斯分布的无限维推广，可以用来对未知的函数设定先验。狄利克雷过程和高斯过程都用于贝叶斯统计中，构建灵活的模型，在这些模型中，参数的数量可以随着数据量的增加而增加。
- en: 'We will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论以下主题：
- en: Functions as probabilistic objects
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为概率对象的函数
- en: Kernels
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核函数
- en: Gaussian processes with Gaussian likelihoods
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有高斯似然的高斯过程
- en: Gaussian processes with non-Gaussian likelihoods
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非高斯似然下的高斯过程
- en: Hilbert space Gaussian process
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 希尔伯特空间中的高斯过程
- en: 8.1 Linear models and non-linear data
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 线性模型与非线性数据
- en: 'In *Chapter [4](CH04.xhtml#x1-760004)* and *Chapter [6](CH06.xhtml#x1-1200006)*
    we learned how to build models of the general form:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第[4章](CH04.xhtml#x1-760004)*和*第[6章](CH06.xhtml#x1-1200006)*中，我们学习了如何构建以下形式的模型：
- en: '![θ = 𝜓 (ϕ(X )𝛽 ) ](img/file216.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![θ = 𝜓 (ϕ(X )𝛽 ) ](img/file216.jpg)'
- en: Here, *θ* is a parameter for some probability distribution, for example, the
    mean of a Gaussian, the *p* parameter of the binomial, the rate of a Poisson,
    and so on. We call ![](img/phi.png) the inverse link function and ![](img/phi.png)
    is some other function we use to potentially transform the data, like a square
    root, a polynomial function, or something else.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*θ*是某个概率分布的参数，例如，高斯分布的均值，二项分布的*p*参数，泊松分布的速率，等等。我们称![](img/phi.png)为逆链接函数，![](img/phi.png)是我们用来潜在地变换数据的其他函数，比如平方根、多项式函数，或者其他形式。
- en: Fitting, or learning, a Bayesian model can be seen as finding the posterior
    distribution of the weights *β*, and thus this is known as the weight view of
    approximating functions. As we already saw with polynomial and splines regression,
    by letting ![](img/phi.png) be a non-linear function, we can map the inputs onto
    a *feature space*. We also saw that by using a polynomial of the proper degree,
    we can perfectly fit any function. But unless we apply some form of regularization,
    for example, using prior distributions, this will lead to models that memorize
    the data, or in other words models with very poor generalizing properties. We
    also mention that splines can be as flexible as polynomials but with better statistical
    properties. We will now discuss Gaussian processes, which provide a principled
    solution to modeling arbitrary functions by effectively letting the data decide
    on the complexity of the function, while avoiding, or at least minimizing, the
    chance of overfitting.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合或学习一个贝叶斯模型可以看作是寻找权重*β*的后验分布，因此这被称为近似函数的权重视角。正如我们在多项式回归和样条回归中看到的那样，通过让![](img/phi.png)成为一个非线性函数，我们可以将输入映射到*特征空间*中。我们还看到，通过使用适当阶数的多项式，我们可以完美地拟合任何函数。但除非我们应用某种形式的正则化，例如，使用先验分布，否则这将导致记忆数据的模型，换句话说，模型的泛化能力非常差。我们还提到，样条回归可以像多项式一样灵活，但具有更好的统计属性。现在，我们将讨论高斯过程，它为通过有效地让数据决定函数复杂度来建模任意函数提供了一个有原则的解决方案，同时避免或至少最小化过拟合的可能性。
- en: The following sections discuss Gaussian processes from a very practical point
    of view; we have avoided covering almost all the mathematics surrounding them.
    For a more formal explanation, you may read *Gaussian Processes for Machine Learning*
    by [Rasmussen and Williams](Bibliography.xhtml#Xrasmussen_2005) [[2005](Bibliography.xhtml#Xrasmussen_2005)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节从一个非常实际的角度讨论高斯过程；我们避免了涉及几乎所有相关的数学内容。对于更正式的解释，你可以阅读[Rasmussen和Williams](Bibliography.xhtml#Xrasmussen_2005)的《*Gaussian
    Processes for Machine Learning*》[[2005](Bibliography.xhtml#Xrasmussen_2005)]。
- en: 8.2 Modeling functions
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 函数建模
- en: 'We will begin our discussion of Gaussian processes by first describing a way
    to represent functions as probabilistic objects. We may think of a function *f*
    as a mapping from a set of inputs *X* to a set of outputs *Y* . Thus, we can write:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过首先描述一种将函数表示为概率对象的方法来开始讨论高斯过程。我们可以将一个函数 *f* 视为从输入集合 *X* 到输出集合 *Y* 的映射。因此，我们可以写成：
- en: '![Y = f(X ) ](img/file217.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Y = f(X ) ](img/file217.jpg)'
- en: One very crude way to represent functions is by listing for each *x*[*i*] value
    its corresponding *y*[*i*] value as in *Table [8.1](#x1-158002r1)*. You may remember
    this way of representing functions from elementary school.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 表示函数的一种非常粗略的方式是列出每个 *x*[*i*] 值对应的 *y*[*i*] 值，如 *表 [8.1](#x1-158002r1)* 所示。你可能还记得这种表示函数的方式，它来自小学阶段。
- en: '| *x* | *y* |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| *x* | *y* |'
- en: '| 0.00 | 0.46 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 0.00 | 0.46 |'
- en: '| 0.33 | 2.60 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 0.33 | 2.60 |'
- en: '| 0.67 | 5.90 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 0.67 | 5.90 |'
- en: '| 1.00 | 7.91 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 1.00 | 7.91 |'
- en: '**Table 8.1**: A tabular representation of a function (sort of)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 8.1**：一种函数的表格表示（某种程度上）'
- en: As a general case, the values of *X* and *Y* will live on the real line; thus,
    we can see a function as a (potentially) infinite and ordered list of paired values
    (*x*[*i*]*,y*[*i*]). The order is important because, if we shuffle the values,
    we will get different functions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一般情况，*X* 和 *Y* 的值将位于实数线上；因此，我们可以将函数视为一个（可能）无限且有序的值对列表 (*x*[*i*]*,y*[*i*])。顺序很重要，因为如果我们打乱这些值的顺序，得到的将是不同的函数。
- en: Following this description, we can represent, numerically, any specific function
    we want. But what if we want to represent functions probabilistically? Well, we
    then need to encode a probabilitics mapping. Let me explain this; we can let each
    value be a random variable with some associated distribution. As working with
    Gaussians is usually convenient, let’s say that they are distributed as a Gaussian
    with a given mean and variance. In this way, we no longer have the description
    of a single specific function, but the description of a family of distributions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个描述，我们可以数值地表示我们想要的任何特定函数。但是，如果我们想要概率性地表示函数怎么办呢？那么，我们就需要编码一个概率映射。让我解释一下；我们可以让每个值都是一个具有某种关联分布的随机变量。由于使用高斯分布通常很方便，我们可以假设它们服从具有给定均值和方差的高斯分布。这样，我们就不再描述单一的特定函数，而是描述一个分布族。
- en: 'To make this discussion concrete, let’s use some Python code to build and plot
    two examples of such functions:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使讨论更加具体，我们将使用一些 Python 代码来构建并绘制两个这样的函数示例：
- en: '**Code 8.1**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.1**'
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![PIC](img/file218.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file218.png)'
- en: '**Figure 8.1**: Two dummy functions sampled from Gaussian distributions'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.1**：从高斯分布中采样的两个虚拟函数'
- en: '*Figure [8.1](#x1-158016r1)* shows that encoding functions using samples from
    Gaussian distributions is not that crazy or foolish, so we may be on the right
    track. Nevertheless, the approach used to generate *Figure [8.1](#x1-158016r1)*
    is limited and not sufficiently flexible.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [8.1](#x1-158016r1)* 显示了使用来自高斯分布的样本来编码函数并非那么疯狂或愚蠢，所以我们可能走在正确的道路上。然而，用于生成
    *图 [8.1](#x1-158016r1)* 的方法是有限的，并且不够灵活。'
- en: While we expect real functions to have some structure or pattern, the way we
    express `the first one` function does not let us encode any relation between data
    points. In fact, each point is completely independent of the others, as we just
    get them as 10 independent samples from a common one-dimensional Gaussian distribution.
    For `the second one` function, we introduce some dependency. The mean of the point
    *y*[*i*+1] is the value *y*[*i*], thus we have some structure here. Nevertheless,
    we will see next that there is a more general approach to capturing dependencies,
    and not only between consecutive points.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们期望实际的函数具有某种结构或模式，但我们表示 `第一个` 函数的方式并没有让我们编码数据点之间的任何关系。实际上，每个点都是完全独立的，因为我们只是从一个共同的一维高斯分布中随机抽取了
    10 个独立的样本。对于 `第二个` 函数，我们引入了一些依赖关系。点 *y*[*i*+1] 的均值是 *y*[*i*]，因此我们这里有一定的结构。然而，我们将在接下来的内容中看到，捕捉依赖关系有一种更为通用的方法，而不仅仅是连续点之间的依赖。
- en: Before continuing, let me stop for a moment and consider why we’re using Gaussians
    and not any other probability distribution. First, by restricting ourselves to
    working with Gaussians, we do not lose any flexibility in specifying functions
    of different shapes, as each point has potentially its own mean and variance.
    Second, working with Gaussians is nice from a mathematical point of view.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我停下来思考一下，为什么我们使用高斯分布而不是其他概率分布。首先，通过将自己限制为只使用高斯分布，我们在指定不同形状的函数时不会失去任何灵活性，因为每个点可能有自己的均值和方差。其次，从数学的角度来看，使用高斯分布是非常方便的。
- en: 8.3 Multivariate Gaussians and functions
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 多元高斯和函数
- en: In *Figure [8.1](#x1-158016r1)*, we represented a function as a collection of
    samples from 1-dimensional Gaussian distributions. One alternative is to use an
    n-dimensional multivariate Gaussian distribution to get a sample vector of length
    *n*. Actually, you may want to try to reproduce *Figure [8.1](#x1-158016r1)* but
    replacing `np.random.normal(0, 1, len(x))` with `np.random.multivariate_normal`,
    with a mean of `np.zeros_like(x)` and a standard deviation of `np.eye(len(x)`.
    The advantage of working with a Multivariate Normal is that we can use the covariance
    matrix to encode information about the function. For instance, by setting the
    covariance matrix to `np.eye(len(x))`, we are saying that each of the 10 points,
    where we are evaluating the function, has a variance of 1\. We are also saying
    that the variance between them, that is, their covariances, is 0\. In other words,
    they are independent. If we replace those zeros with other numbers, we could get
    covariances telling a different story.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 [8.1](#x1-158016r1)*中，我们将一个函数表示为从一维高斯分布中采样得到的集合。另一种选择是使用n维多元高斯分布，得到一个长度为*n*的样本向量。实际上，你可以尝试重现*图
    [8.1](#x1-158016r1)*，但将`np.random.normal(0, 1, len(x))`替换为`np.random.multivariate_normal`，均值为`np.zeros_like(x)`，标准差为`np.eye(len(x))`。使用多元正态分布的优势在于，我们可以利用协方差矩阵来编码有关函数的信息。例如，通过将协方差矩阵设置为`np.eye(len(x))`，我们表明我们在评估函数的10个点中，每个点的方差为1。我们还表明它们之间的方差，即协方差为0。换句话说，它们是独立的。如果我们将这些零替换为其他数值，我们可以得到描述不同故事的协方差。
- en: I hope you are starting to get convinced that it is possible to use a multivariate
    Gaussian in order to represent functions. If that’s the case, then we just need
    to find a suitable covariance matrix. And that’s the topic of the next section.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你已经开始相信使用多元高斯来表示函数是可能的。如果是这样，那么我们只需要找到一个合适的协方差矩阵。这也是下一节的主题。
- en: 8.3.1 Covariance functions and kernels
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 协方差函数和核函数
- en: In practice, covariance matrices are specified using functions known as kernels.
    Unfortunately, the term kernel is a very polysemic one, even in the statistical
    literature. An easy way to define a kernel is any function that returns a valid
    covariance matrix. But this is a tautological and not very intuitive definition.
    A more conceptual and useful definition is that a kernel defines a measure of
    similarity between data points in the input space, and this similarity determines
    how much influence one data point should have on predicting the value of another
    data point.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，协方差矩阵是通过称为核函数的函数来指定的。不幸的是，术语“核”是一个多义词，甚至在统计学文献中也是如此。定义核函数的一种简单方法是，任何返回有效协方差矩阵的函数都可以称为核。但这个定义是自我重复的，且不太直观。一个更具概念性和实用性的定义是，核函数定义了输入空间中数据点之间的相似度度量，而这种相似度决定了一个数据点对预测另一个数据点值的影响程度。
- en: 'There are many useful kernels, a popular one being the exponentiated quadratic
    kernel:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多有用的核函数，一个流行的核函数是指数二次核：
- en: '![ ( ′ 2) 𝜅(X,X ′) = exp − ∥X--−-X-∥- 2ℓ2 ](img/file219.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![ ( ′ 2) 𝜅(X,X ′) = exp − ∥X--−-X-∥- 2ℓ² ](img/file219.jpg)'
- en: 'Here, ∥**X** − **X**′∥² is the squared Euclidean distance:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，∥**X** − **X**′∥²是平方欧几里得距离：
- en: '![∥X − X′∥2 = (X1 − X1′)2 + (X2 − X′2)2 + ⋅⋅⋅+ (Xn − X′n)2 ](img/file220.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![∥X − X′∥² = (X1 − X1′)² + (X2 − X′2)² + ⋅⋅⋅+ (Xn − X′n)²](img/file220.jpg)'
- en: For this kernel, we can see that we have a symmetric function that takes two
    inputs and returns a value of 0 if the inputs are the same, or positive otherwise.
    And thus we can interpret the output of the exponentiated quadratic kernel as
    a measure of similarity between the two inputs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个核函数，我们可以看到它是一个对称函数，接受两个输入，如果输入相同则返回0，否则返回正值。因此，我们可以将指数二次核的输出解释为两个输入之间的相似度度量。
- en: It may not be obvious at first sight, but the exponentiated quadratic kernel
    has a similar formula to the Gaussian distribution. For this reason, this kernel
    is also called the Gaussian kernel. The term *ℓ* is known as the length scale
    (or bandwidth or variance) and controls the width of the kernel. In other words,
    it controls at what scale the *X* values are considered similar.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看可能不太明显，但指数二次核的公式与高斯分布非常相似。正因为如此，这个核函数也被称为高斯核函数。术语*ℓ*被称为长度尺度（或带宽或方差），它控制着核函数的宽度。换句话说，它控制着*X*值在什么尺度下被认为是相似的。
- en: 'To better understand the role of kernels, I recommend you play with them. For
    instance, let’s define a Python function to compute the exponentiated quadratic
    kernel:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解核函数的作用，我建议你动手尝试它们。例如，定义一个 Python 函数来计算指数二次核函数：
- en: '**Code 8.2**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.2**'
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Figure [8.2](#x1-160009r2)* shows how a 4 × 4 covariance matrix looks for
    different inputs. The input I chose is rather simple and consists of the values
    [−1*,*0*,*1*,*2].'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [8.2](#x1-160009r2)* 展示了在不同输入下4 × 4协方差矩阵的样子。我选择的输入非常简单，包含了值 [−1*,*0*,*1*,*2]。'
- en: '![PIC](img/file221.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file221.png)'
- en: '**Figure 8.2**: Input values (left), covariance matrix (right)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.2**：输入值（左），协方差矩阵（右）'
- en: On the left panel of *Figure [8.2](#x1-160009r2)*, we have the input values.
    These are the values on the x-axis, and we have labeled the points from 0 to 3\.
    Thus, point 0 takes the value -1, point 1 takes 0, and so on. On the right panel,
    we have a heatmap representing the covariance matrix that we computed using the
    exponentiated quadratic kernel. The lighter the color, the larger the value of
    the covariance. As you can see, the heatmap is symmetric, with the diagonal taking
    the largest values. This makes sense when we realize that the value of each element
    in the covariance matrix is inversely proportional to the distance between the
    points, and the diagonal is the result of comparing each data point with itself.
    The smallest value is the one for the points 0 and 3, as they are the most distant
    points.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 [8.2](#x1-160009r2)* 的左面板中，我们展示了输入值。这些是 x 轴上的值，我们将点从 0 标记到 3。因此，点 0 取值
    -1，点 1 取值 0，以此类推。在右面板中，我们展示了使用指数二次核函数计算得到的协方差矩阵的热力图。颜色越浅，协方差值越大。如你所见，热力图是对称的，且对角线上的值最大。当我们意识到协方差矩阵中每个元素的值与点之间的距离成反比时，这一点是有意义的，对角线是通过将每个数据点与自身进行比较得到的。最小的值出现在点
    0 和点 3，因为它们是最远的两个点。
- en: Once you understand this example, you should try it with other inputs. See exercise
    1 at the end of this chapter and the accompanying notebook ( [https://github.com/aloctavodia/BAP3](https://github.com/aloctavodia/BAP3))
    for further practice.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你理解了这个例子，你应该尝试使用其他输入。请参考本章末的练习 1 以及附带的笔记本（[https://github.com/aloctavodia/BAP3](https://github.com/aloctavodia/BAP3)）进行进一步练习。
- en: Now that we have a better grasp of how to use a kernel to generate a covariance
    matrix, let’s move one step further and use the covariance matrix to sample functions.
    As you can see in *Figure [8.3](#x1-160010r3)*, a Gaussian kernel implies a wide
    variety of functions with the parameter *ℓ* controlling the smoothness of the
    functions. The larger the value of *ℓ*, the smoother the function.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们更好地理解了如何使用核函数生成协方差矩阵，接下来让我们更进一步，使用协方差矩阵来采样函数。如你在 *图 [8.3](#x1-160010r3)*
    中所见，高斯核函数意味着一系列不同的函数，而参数 *ℓ* 控制函数的平滑度。*ℓ* 的值越大，函数越平滑。
- en: '![PIC](img/file222.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file222.png)'
- en: '**Figure 8.3**: Realizations of a Gaussian kernel for four values of *ℓ* (two
    realization per value of *ℓ*)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.3**：四个 *ℓ* 值的高斯核函数的实现（每个 *ℓ* 值对应两个实现）'
- en: Show me Your Friends and I’ll Show you your Future
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你告诉我你的朋友，我就能告诉你你的未来
- en: The kernel translates the distance of the data points along the x axis to values
    of covariances for values of the expected function (on the y axis). Thus, the
    closest two points are on the x axis; the most similar we expect their values
    to be on the y axis.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数将数据点沿 x 轴的距离转化为期望函数值（y 轴）的协方差值。因此，数据点在 x 轴上越接近，我们期望它们在 y 轴上的值越相似。
- en: 8.4 Gaussian processes
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 高斯过程
- en: 'Now we are ready to understand what Gaussian processes (GPs) are and how they
    are used in practice. A somewhat formal definition of GPs, taken from Wikipedia,
    is as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好理解什么是高斯过程（GPs）以及它们在实践中的应用。根据维基百科，高斯过程的一个正式定义如下：
- en: ”The collection of random variables indexed by time or space, such that every
    finite collection of those random variables has a MultivariateNormal distribution,
    i.e. every finite linear combination of them is normally distributed.”
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: “由时间或空间索引的随机变量的集合，使得这些随机变量的每个有限集合都具有多元正态分布，即它们的每个有限线性组合服从正态分布。”
- en: This is probably not a very useful definition, at least not at this stage of
    your learning path. The trick to understanding Gaussian processes is to realize
    that the concept of GP is a mental (and mathematical) scaffold, since, in practice,
    we do not need to directly work with this infinite mathematical object. Instead,
    we only evaluate the GPs at the points where we have data. By doing this, we collapse
    the infinite-dimensional GP into a finite multivariate Gaussian distribution with
    as many dimensions as data points. Mathematically, this collapse is achieved by
    marginalization over the infinitely unobserved dimensions. The theory assures
    us that it is OK to omit (actually marginalize over) all points, except the ones
    we are observing. It also guarantees that we will always get a multivariate Gaussian
    distribution. Thus, we can rigorously interpret *Figure [8.3](#x1-160010r3)* as
    actual samples from a Gaussian process!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义可能并不是很有用，至少在你当前的学习阶段不太有用。理解高斯过程的诀窍在于意识到，高斯过程的概念是一个心理（和数学）框架，因为在实际应用中，我们并不需要直接处理这个无限维的数学对象。相反，我们只在有数据的点上评估高斯过程。通过这样做，我们将无限维的高斯过程压缩成一个有限维的多元高斯分布，维度数与数据点的数量相等。从数学上讲，这种压缩是通过对无限未观察到的维度进行边际化来实现的。理论告诉我们，除了我们观察到的点，忽略（实际上是边际化）所有其他点是可以的。它还保证我们总是会得到一个多元高斯分布。因此，我们可以严格地解释*图
    [8.3](#x1-160010r3)* 为高斯过程的实际样本！
- en: So far we have focused on the covariance matrix of the MultivariateNormal and
    we have not discussed the mean. Setting the mean of a multivariate Gaussian at
    0 is common practice when working with GPs, since they are flexible enough to
    model the mean arbitrarily well. But notice that there is no restriction in doing
    so. Actually, for some problems, you may want to model the mean parametrically
    and leave the GP to model the residuals.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们专注于多元正态分布的协方差矩阵，并没有讨论均值。在使用高斯过程时，将多元高斯的均值设置为0是常见做法，因为高斯过程足够灵活，可以将均值建模得非常好。但请注意，这样做并没有限制。实际上，对于某些问题，你可能希望参数化地建模均值，而将高斯过程用来建模残差。
- en: GPs are Prior Over Functions
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯过程是对函数的先验
- en: Gaussian processes are prior distributions over functions in such a way that
    at each point that you evaluate a function, it places a Gaussian distribution
    with a given mean and variance. In practice, GPs are usually built using kernels,
    which turn distance on an x axis into similarities on the y axis.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯过程是对函数的先验分布，使得在你评估函数的每个点时，它会在该点放置一个具有给定均值和方差的高斯分布。在实践中，高斯过程通常是通过使用核函数来构建的，核函数将x轴上的距离转化为y轴上的相似度。
- en: 8.5 Gaussian process regression
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 高斯过程回归
- en: 'Let’s assume we can model a variable *Y* as a function *f* of *X* plus some
    Gaussian noise:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们可以将一个变量*Y*建模为*X*的函数*f*加上一些高斯噪声：
- en: '![Y ∼ 𝒩 (μ = f(X ),σ = 𝜖) ](img/file223.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![Y ∼ 𝒩 (μ = f(X ),σ = 𝜖) ](img/file223.jpg)'
- en: 'If *f* is a linear function of *X*, then this assumption is essentially the
    same one we used in *Chapter [4](CH04.xhtml#x1-760004)* when we discussed simple
    linear regression. In this chapter, instead, we are going to use a more general
    expression for *f* by setting a prior over it. In that way, we will be able to
    get more complex functions than linear. If we decided to use Gaussian processes
    as this prior, then we can write:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*f*是*X*的线性函数，那么这个假设本质上与我们在*第[4章](CH04.xhtml#x1-760004)*讨论简单线性回归时使用的假设相同。在本章中，我们将通过对*f*设置先验来使用一个更一般的表达式。通过这种方式，我们将能够得到比线性更复杂的函数。如果我们决定使用高斯过程作为这个先验，那么我们可以写成：
- en: '![ ′ f(X ) = 𝒢𝒫 (μX,𝜅(X, X )) ](img/file224.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![ ′ f(X ) = 𝒢𝒫 (μX,𝜅(X, X )) ](img/file224.jpg)'
- en: Here, ![](img/GP.PNG) represents a Gaussian process with the mean function *μ*[*X*]
    and covariance function *K*(*X,X*′). Even though in practice, we always work with
    finite objects, we used the word **function** to indicate that mathematically,
    the mean and covariance are infinite objects.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/GP.PNG)表示一个均值函数*μ*[*X*]和协方差函数*K*(*X,X*′)的高斯过程。尽管在实践中，我们总是处理有限对象，但我们使用**函数**这个词来表示从数学上讲，均值和协方差是无限对象。
- en: I mentioned before that working with Gaussians is nice. For instance, if the
    prior distribution is a GP and the likelihood is a Gaussian distribution, then
    the posterior is also a GP and we can compute it analytically. Additionally, its
    nice to have a Gaussian likelihood because we can marginalize out the GP, which
    hugely reduces the size of the parameter space we need to sample from. The GP
    module in PyMC takes advantage of this and then it has different implementations
    for Gaussian and non-Gaussian likelihoods. In the next sections, we will explore
    both.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提到过，使用高斯分布非常方便。例如，如果先验分布是一个高斯过程，而似然是高斯分布，那么后验分布也是高斯过程，并且我们可以解析地计算它。此外，使用高斯似然的好处是我们可以边际化高斯过程，这大大减少了我们需要从中抽样的参数空间的大小。PyMC中的高斯过程模块利用了这一点，并为高斯和非高斯似然提供了不同的实现方式。在接下来的部分中，我们将探索这两者。
- en: 8.6 Gaussian process regression with PyMC
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.6 使用PyMC进行高斯过程回归
- en: The gray line in *Figure [8.4](#x1-163002r4)* is a sin function. We are going
    to assume we don’t know this function and instead, all we have is a set of data
    points (dots). Then we use a Gaussian process to approximate the function that
    generated those data points.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [8.4](#x1-163002r4)*中的灰色线是一个正弦函数。我们假设我们不知道这个函数，而是只有一组数据点（点）。然后我们使用高斯过程来逼近生成这些数据点的函数。'
- en: '![PIC](img/file225.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file225.png)'
- en: '**Figure 8.4**: Synthetic data (dots) generated from a known function (line)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.4**：从已知函数（线）生成的合成数据（点）'
- en: GPs are implemented in PyMC as a series of Python classes that deviate a little
    bit from what we have seen in previous models; nevertheless, the code is still
    very *PyMConic*. I have added a few comments in the following code to guide you
    through the key steps of defining a GP with PyMC.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯过程在PyMC中实现为一系列Python类，这些类与我们之前见过的模型稍有不同；然而，代码依然非常*PyMConic*。我在接下来的代码中添加了一些注释，帮助你理解在PyMC中定义高斯过程的关键步骤。
- en: '**Code 8.3**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.3**'
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice that instead of a Gaussian likelihood, we have used the `gp.marginal_likelihood`
    method. This method takes advantage of the fact that the posterior has a closed
    form, as explained in the previous section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们没有使用高斯似然，而是使用了`gp.marginal_likelihood`方法。这个方法利用了后验分布有封闭形式这一事实，正如前面部分所解释的那样。
- en: OK, now that we have computed the posterior, let’s see how to get predictions
    of the mean fitted function. We can do this by computing the conditional distribution
    evaluated over new input locations using `gp.conditional`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们已经计算了后验分布，接下来让我们看看如何得到均值拟合函数的预测。我们可以通过使用`gp.conditional`计算在新输入位置上评估的条件分布来实现这一点。
- en: '**Code 8.4**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.4**'
- en: '[PRE3]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As a result, we get a new PyMC random variable, `f_pred`, which we can use
    to get samples from the posterior predictive distribution:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，我们得到了一个新的PyMC随机变量`f_pred`，我们可以用它来从后验预测分布中获取样本：
- en: '**Code 8.5**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.5**'
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now we can plot the fitted functions over the original data, to visually inspect
    how well they fit the data and the associated uncertainty in our predictions.
    As we did with linear models in *Chapter [4](CH04.xhtml#x1-760004)*, we are going
    to show different ways to plot the same results. *Figure [8.5](#x1-163035r5)*
    shows lines from the fitted function.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在原始数据上绘制拟合函数，以直观检查它们与数据的拟合程度以及我们预测的相关不确定性。正如我们在*第[4](CH04.xhtml#x1-760004)章*中对线性模型所做的那样，我们将展示几种不同的方式来绘制相同的结果。*图
    [8.5](#x1-163035r5)*展示了拟合函数的线条。
- en: '![PIC](img/file226.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file226.png)'
- en: '**Figure 8.5**: Lines represent samples from the posterior mean of `model_reg`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.5**：线条表示从`model_reg`的后验均值中抽取的样本'
- en: Alternatively, we can use the auxiliary function `pm.gp.util.plot_gp_dist` to
    get some nice plots as in *Figure [8.6](#x1-163036r6)*. In this plot, each band
    represents a different percentile, ranging from percentile 99 (lighter gray) to
    percentile 51 (darker gray).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用辅助函数`pm.gp.util.plot_gp_dist`来获得如*图 [8.6](#x1-163036r6)*所示的漂亮图表。在这个图表中，每条带状线表示不同的百分位数，从99百分位（浅灰色）到51百分位（深灰色）。
- en: '![PIC](img/file227.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file227.png)'
- en: '**Figure 8.6**: Samples from the posterior of `model_reg` plotted using `plot_gp_dist`
    function'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.6**：使用`plot_gp_dist`函数绘制的从`model_reg`的后验样本'
- en: Yet another alternative is to compute the mean vector and standard deviation
    of the conditional distribution evaluated at a given point in the parameter space.
    In *Figure [8.7](#x1-163037r7)*, we use the mean (over the samples in the trace)
    for *ℓ* and ![](img/e.png). We can compute the mean and variance using the `gp.predict`
    method.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是计算在参数空间中给定点处的条件分布的均值向量和标准差。在*图 [8.7](#x1-163037r7)*中，我们使用均值（基于轨迹中的样本）来表示*ℓ*和
    ![](img/e.png)。我们可以使用`gp.predict`方法来计算均值和方差。
- en: '![PIC](img/file228.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file228.png)'
- en: '**Figure 8.7**: Posterior mean of `model_reg` with bands for 1 and 2 standard
    deviations'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.7**：`model_reg`的后验均值，带有1和2标准差的带宽'
- en: As we saw in *Chapter [4](CH04.xhtml#x1-760004)*, we can use a linear model
    with a non-Gaussian likelihood and a proper inverse link function to extend the
    range of useful linear models. We can do the same for GPs. We can, for example,
    use a Poisson likelihood with an exponential inverse link function. For a model
    like this, the posterior is no longer analytically tractable, but, nevertheless,
    we can use numerical methods to approximate it. In the following sections, we
    will discuss these types of models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*第 [4](CH04.xhtml#x1-760004)*章中看到的，我们可以使用一个具有非高斯似然度和适当逆链接函数的线性模型来扩展有用线性模型的范围。对于高斯过程（GP），我们也可以做同样的事情。例如，我们可以使用一个带有指数逆链接函数的泊松似然度。对于这样的模型，后验分布不再是解析可解的，但我们仍然可以使用数值方法来近似计算。在接下来的部分中，我们将讨论这些类型的模型。
- en: 8.6.1 Setting priors for the length scale
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6.1 设置长度尺度的先验
- en: 'For length-scale parameters, priors avoiding zero usually work better. As we
    already saw, *ℓ* controls the smoothness of the function, thus a value of 0 for
    *ℓ* implies a non-smooth function; we will get a function like ”the first one”
    from *Figure [8.1](#x1-158016r1)*. But a far more important reason is that for
    values of *ℓ* that are larger than 0 but still below the minimum spacing of the
    covariates, we can get some nasty effects. Essentially, below that point, the
    likelihood has no way to distinguish between different length scales, so all of
    them are equally good. This is a type of non-identifiability issue. As a result,
    we will have a GP that will tend to overfit and exactly interpolate between the
    input data. Additionally, the MCMC sampler will have a harder time, and we could
    get longer sampling times or simple unreliable samples. Something similar happens
    for values beyond the range of the data. If the range of your data is 10 and the
    value of *ℓ >*= 10, this implies a flat function. And again beyond that point,
    you (and the likelihood) have no way of distinguishing between different values
    of the parameter. Thus even if you have no idea how smooth or wiggly your function
    is, you can still set a prior that avoids very low and very high values of *ℓ*.
    For instance, to get the prior `pm.InverseGamma("`*ℓ*`", 7, 17)` we ask PreliZ
    for the maximum entropy prior that has 0.95 of the mass between 1 and 5:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于长度尺度参数，避免为零的先验通常效果更好。正如我们之前看到的，*ℓ*控制着函数的平滑度，因此，*ℓ*为0意味着函数不平滑；我们会得到类似于*图 [8.1](#x1-158016r1)*中的“第一个”函数。但更重要的原因是，对于大于0但仍低于协变量最小间距的*ℓ*值，可能会出现一些不良效应。本质上，在该点以下，似然度无法区分不同的长度尺度，因此它们都同样好。这是一个不可识别性问题。因此，我们将得到一个倾向于过拟合并且准确插值输入数据的高斯过程（GP）。此外，MCMC采样器会遇到更多困难，可能会出现更长的采样时间或简单的不可靠样本。类似的情况发生在数据范围之外。如果数据范围为10且*ℓ*>
    = 10，这意味着函数是平坦的。再往后，您（以及似然度）无法区分不同的参数值。因此，即使您不确定函数是多么平滑或起伏，您仍然可以设置一个先验，避免极低或极高的*ℓ*值。例如，为了获得先验`pm.InverseGamma("`*ℓ*`",
    7, 17)`，我们请求PreliZ提供一个最大熵先验，其中0.95的质量分布在1到5之间：
- en: '**Code 8.6**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.6**'
- en: '[PRE5]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The InverseGamma is a common choice. Like the Gamma, it allows us to set a prior
    that avoids 0, but unlike the Gamma, the InverseGamma has a lighter tail toward
    0, or in other words, it allocates less mass for small values.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 逆伽马分布（InverseGamma）是一个常见的选择。与伽马分布（Gamma）类似，它允许我们设置一个避免为0的先验，但与伽马分布不同的是，逆伽马分布在接近0时有一个较轻的尾部，换句话说，它为小值分配的质量较少。
- en: For the rest of this chapter, we will use the function `get_ig_params` to obtain
    weakly informative priors from the scale of the covariates. You will find the
    details in the accompanying code ([https://github.com/aloctavodia/BAP3](https://github.com/aloctavodia/BAP3)),
    but essentially we are using the `maxent` function from PreliZ to set most of
    the prior mass in a range compatible with the range of the covariates.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 本章剩余部分，我们将使用函数`get_ig_params`从协变量的尺度中获取弱信息先验。你可以在附带的代码中找到详细信息（[https://github.com/aloctavodia/BAP3](https://github.com/aloctavodia/BAP3)），但本质上，我们使用PreliZ中的`maxent`函数，将大部分先验质量设置在一个与协变量范围兼容的区间内。
- en: 8.7 Gaussian process classification
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7 高斯过程分类
- en: In *Chapter [4](CH04.xhtml#x1-760004)*, we saw how a linear model can be used
    to classify data. We used a Bernoulli likelihood with a logistic inverse link
    function. Then, we applied a boundary decision rule. In this section, we are going
    to do the same, but this time using a GP instead of a linear model. As we did
    with `model_lrs` from *Chapter [4](CH04.xhtml#x1-760004)*, we are going to use
    the iris dataset with two classes, `setosa` and `versicolor`, and one predictor
    variable, the `sepal length`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第[4章](CH04.xhtml#x1-760004)*中，我们看到了如何使用线性模型来分类数据。我们使用了伯努利似然和逻辑逆链接函数。然后，我们应用了边界决策规则。在这一节中，我们将做同样的事情，但这次使用GP而不是线性模型。就像我们在*第[4章](CH04.xhtml#x1-760004)*中的`model_lrs`一样，我们将使用包含两类`setosa`和`versicolor`、一个预测变量`sepal
    length`的鸢尾花数据集。
- en: For this model, we cannot use the `pm.gp.Marginal` class, because that class
    is restricted to Gaussian likelihoods as it takes advantage of the mathematical
    tractability of the combination of a GP prior with a Gaussian likelihood. Instead,
    we need to use the more general class `pm.gp.Latent`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个模型，我们不能使用`pm.gp.Marginal`类，因为该类仅限于高斯似然，因为它利用了GP先验与高斯似然组合的数学可处理性。相反，我们需要使用更通用的`pm.gp.Latent`类。
- en: '**Code 8.7**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.7**'
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As we can see, *Figure [8.8](#x1-165012r8)* looks pretty similar to *Figure
    [4.11](CH04.xhtml#x1-85023r11)*. Please take some time to compare these figures.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，*图[8.8](#x1-165012r8)*看起来与*图[4.11](CH04.xhtml#x1-85023r11)*非常相似。请花些时间对比这两张图。
- en: '![PIC](img/file229.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file229.png)'
- en: '**Figure 8.8**: Logistic regression, result of `model_lrs`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**图8.8**：逻辑回归，`model_lrs`的结果'
- en: You probably have already noticed that the inferred function looks similar to
    a sigmoid curve, except for the tails that go up at lower values of `sepal_length`,
    and down at higher values of `sepal_length`. Why are we seeing this? Because when
    there is little or no data available, a GP posterior tends to revert to the GP
    prior. This makes sense if we think that in the absence of data, your posterior
    essentially becomes the prior.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，推断出的函数看起来类似于sigmoid曲线，除了在较低的`sepal_length`值时尾部向上，较高的`sepal_length`值时尾部向下。为什么会这样呢？因为当数据很少或没有数据时，GP后验倾向于恢复到GP先验。如果我们考虑到在没有数据的情况下，后验基本上变成了先验，这就很有意义。
- en: 'If our only concern is the decision boundary, then the behavior at the tails
    may be irrelevant. But if we want to model the probabilities of belonging to setosa
    or versicolor at different values of `sepal_length`, we should do something to
    improve the model at the tails. One way to achieve this is to add more structure
    to the Gaussian process. One very nice feature of GP is that we can combine covariance
    functions. Hence, for the next model, we are going to combine three kernels: the
    exponential quadratic kernel, a linear kernel, and a white noise kernel.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们唯一关注的是决策边界，那么尾部的行为可能是无关紧要的。但如果我们想要建模在不同`sepal_length`值下属于setosa或versicolor的概率，我们应该做一些事情来改善尾部的模型。实现这一目标的一种方法是为高斯过程添加更多结构。高斯过程的一个非常好的特点是，我们可以组合协方差函数。因此，对于下一个模型，我们将组合三个核函数：指数二次核、线性核和白噪声核。
- en: 'The linear kernel will have the effect of making the tails go to 0 or 1 at
    the boundaries of the data. Additionally, we use the white noise kernel just as
    a trick to stabilize the computation of the covariance matrix. Kernels for Gaussian
    processes are restricted to guarantee the resulting covariance matrix is positive
    definite. Nevertheless, numerical errors can lead to violating this condition.
    One manifestation of this problem is that we get NaNs when computing posterior
    predictive samples of the fitted function. One way to mitigate this error is to
    stabilize the computation by adding some noise. As a matter of fact, PyMC already
    does something similar to this under the hood, but sometimes a little bit more
    noise is needed, as shown in the following code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 线性核的效果是在数据边界处使尾部趋近于0或1。此外，我们使用白噪声核作为稳定协方差矩阵计算的一种技巧。高斯过程的核函数受到限制，以保证生成的协方差矩阵是正定的。然而，数值误差可能导致违反这一条件。这种问题的表现之一是，在计算拟合函数的后验预测样本时会得到NaN。缓解此错误的一种方法是通过添加噪声来稳定计算。事实上，PyMC在后台已经做了类似的处理，但有时需要更多的噪声，如下面的代码所示：
- en: '**Code 8.8**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.8**'
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can see the result of this model in *Figure [8.9](#x1-165027r9)*. Notice
    how this figure looks much more similar now to *Figure [4.11](CH04.xhtml#x1-85023r11)*
    than *Figure [8.8](#x1-165012r8)*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图 [8.9](#x1-165027r9)*中看到这个模型的结果。注意，这个图看起来现在与*图 [4.11](CH04.xhtml#x1-85023r11)*相比，更加相似，而不是*图
    [8.8](#x1-165012r8)*。
- en: '![PIC](img/file230.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file230.png)'
- en: '**Figure 8.9**: Logistic regression, result of `model_lrs`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.9**：逻辑回归，`model_lrs`的结果'
- en: 'The example discussed in this section has two main aims:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论的示例有两个主要目的：
- en: Showing how we can easily combine kernels to get a more expressive model
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示如何轻松地将核函数组合在一起，以获得更具表现力的模型
- en: Showing how we can *recover* a logistic regression using a Gaussian process
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示如何使用高斯过程*恢复*逻辑回归
- en: 'Regarding the second point, logistic regression is indeed a special case of
    Gaussian processes, because a simple linear regression is just a particular case
    of a Gaussian process. In fact, many known models can be seen as special cases
    of GPs, or at least they are somehow connected to GPs. If you want to learn more
    about this, you can read Chapter 15 from Kevin Murphy’s Machine Learning: A Probabilistic
    Perspective (first edition) [[Murphy](Bibliography.xhtml#Xpml0Book), [2012](Bibliography.xhtml#Xpml0Book)],
    and also Chapter 18 from the second edition [[Murphy](Bibliography.xhtml#Xpml2Book), [2023](Bibliography.xhtml#Xpml2Book)].'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 关于第二点，逻辑回归确实是高斯过程的一个特例，因为简单的线性回归只是高斯过程的一个特例。事实上，许多已知的模型可以看作是高斯过程的特例，或者至少它们与高斯过程有某种联系。如果你想了解更多，可以阅读Kevin
    Murphy的《机器学习：一种概率视角》第一版第15章[[Murphy](Bibliography.xhtml#Xpml0Book)，[2012](Bibliography.xhtml#Xpml0Book)]，以及第二版的第18章[[Murphy](Bibliography.xhtml#Xpml2Book)，[2023](Bibliography.xhtml#Xpml2Book)]。
- en: 8.7.1 GPs for space flu
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1 高斯过程用于空间流感
- en: In practice, it does not make too much sense to use a GP to model a problem
    we can just solve with a logistic regression. Instead, we want to use a GP to
    model more complex data that is not well captured with less flexible models. For
    instance, suppose we want to model the probability of getting a disease as a function
    of age. It turns out that very young and very old people have a higher risk than
    people of middle age. The dataset `space_flu.csv` is a synthetic dataset inspired
    by the previous description. *Figure [8.10](#x1-166011r10)* shows a plot of it.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，使用高斯过程来建模我们可以通过逻辑回归解决的问题并没有太大意义。相反，我们希望使用高斯过程来建模那些无法通过不太灵活的模型很好捕捉的更复杂的数据。例如，假设我们想要将患病的概率作为年龄的函数进行建模。结果表明，年轻人和年老的人比中年人有更高的风险。数据集`space_flu.csv`是一个受上述描述启发的合成数据集。*图
    [8.10](#x1-166011r10)*展示了它的图形。
- en: 'Let’s fit the following model and plot the results:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拟合以下模型并绘制结果：
- en: '**Code 8.9**'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.9**'
- en: '[PRE8]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice, as illustrated in *Figure [8.10](#x1-166011r10)*, that the GP can fit
    this space flu dataset very well, even when the data demands the function to be
    more complex than a logistic one. Fitting this data well will be impossible for
    a simple logistic regression, unless we introduce some ad hoc modifications to
    help it a little bit (see exercise 6 at the end of the chapter for a discussion
    of such modifications).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如*图 [8.10](#x1-166011r10)*所示，高斯过程可以很好地拟合这个空间流感数据集，即使数据要求函数比逻辑回归更复杂。对于简单的逻辑回归来说，拟合这些数据几乎是不可能的，除非我们做一些特殊的修改来帮助它（有关这种修改的讨论请参见本章末的练习6）。
- en: '![PIC](img/file231.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file231.png)'
- en: '**Figure 8.10**: Logistic regression, result of `model_space_flu`'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.10**：逻辑回归，`model_space_flu`的结果'
- en: 8.8 Cox processes
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.8 Cox 过程
- en: Now we are going to model count data. We will see two examples; one with a time-varying
    rate and one with a 2D spatially varying rate. To do this, we will use a Poisson
    likelihood and the rate will be modeled using a Gaussian process. Because the
    rate of the Poisson distribution is limited to positive values, we will use an
    exponential as the inverse link function, as we did for the NegativeBinomial regression
    from *Chapter [4](CH04.xhtml#x1-760004)*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们要建模计数数据。我们将看到两个例子；一个是具有时间变化率的，另一个是具有二维空间变化率的。为此，我们将使用泊松似然，并通过高斯过程来建模该比率。由于泊松分布的比率限制为正值，我们将使用指数作为逆链接函数，正如我们在*第
    [4 章](CH04.xhtml#x1-760004)*中的负二项回归中所做的那样。
- en: We can think of a Poisson process as a distribution over collections of points
    in a given space where every finite collection of those random variables has a
    Poisson distribution. When the rate of the Poisson process is itself a stochastic
    process, such as, for example, a Gaussian process, then we have a Cox process.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将泊松过程看作是在给定空间中点集的分布，其中每个有限的点集都是泊松分布。当泊松过程的比率本身是一个随机过程时，例如一个高斯过程，那么我们就有了
    Cox 过程。
- en: 8.8.1 Coal mining disasters
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.8.1 煤矿灾难
- en: 'The first example is known as the coal mining disasters. This example consists
    of a record of coal-mining disasters in the UK from 1851 to 1962\. The number
    of disasters is thought to have been affected by changes in safety regulations
    during this period. We want to model the rate of disasters as a function of time.
    Our dataset consists of a single column and each entry corresponds to the time
    a disaster happened. The model we will use to fit the data has the form:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个例子被称为煤矿灾难。这个例子记录了英国从 1851 年到 1962 年的煤矿灾难数据。灾难的数量被认为受到了这一时期安全法规变化的影响。我们希望将灾难的发生率建模为时间的函数。我们的数据集包含一列数据，每条数据对应一次灾难发生的时间。我们将用于拟合数据的模型形式如下：
- en: '![](img/Formula_01.PNG)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_01.PNG)'
- en: 'As you can see, this is a Poisson regression problem. You may be wondering
    at this point how we’re going to perform a regression if we only have a single
    column with just the date of the disasters. The answer is to discretize the data,
    just as if we were building a histogram. We are going to use the centers of the
    bins as the *X* variable and the counts per bin as the *Y* variable:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这是一个泊松回归问题。此时，您可能会想，我们如何仅凭一个只有灾难日期的单列数据来进行回归分析。答案是将数据离散化，就像我们构建直方图一样。我们将使用箱子的中心作为*X*变量，箱子内的计数作为*Y*变量：
- en: '**Code 8.10**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.10**'
- en: '[PRE9]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now we define and solve the model with PyMC:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用 PyMC 来定义并求解模型：
- en: '**Code 8.11**'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.11**'
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '*Figure [8.11](#x1-168023r11)* shows the median disaster rate as a function
    of time (white line). The bands describe the 50% HDI (darker) and the 94% HDI
    (lighter). At the bottom, the black markers indicate the moment of each disaster.
    As we can see, the rate of accidents decreases with time, except for a brief initial
    increase. The PyMC documentation includes the coal mining disaster but is modeled
    from a different perspective. I strongly recommend that you check that example
    as it is very useful on its own and is also useful to compare it with the approach
    we just implemented here.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [8.11](#x1-168023r11)* 显示了灾难率随时间变化的中位数（白线）。带状区域描述了50%的HDI（较暗）和94%的HDI（较亮）。底部的黑色标记表示每次灾难发生的时刻。正如我们所看到的，事故率随着时间的推移而下降，除了最初的短暂上升。PyMC
    文档包括了煤矿灾难的案例，但从不同的角度进行建模。我强烈建议您查看这个例子，因为它本身非常有用，并且与我们刚刚实现的方法做对比也很有价值。'
- en: '![PIC](img/file232.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file232.png)'
- en: '**Figure 8.11**: Logistic regression, result of `model_coal`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.11**：逻辑回归，`model_coal`的结果'
- en: Notice that even when we binned the data, we obtained, as a result, a smooth
    curve. In this sense, we can see `model_coal` (and, in general, this type of model)
    as building a histogram and then smoothing it.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，即使我们对数据进行了分箱，结果仍然是一个平滑的曲线。从这个角度来看，我们可以将 `model_coal`（以及一般而言，这种类型的模型）视为构建直方图后进行平滑处理。
- en: 8.8.2 Red wood
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.8.2 红木
- en: Let’s apply the same approach we just did to a 2D spatial problem. We are going
    to use the redwood data as shown in *Figure [8.12](#x1-169015r12)*. This dataset
    (distributed with a GPL license) is from the GPstuff package. The dataset consists
    of the location of redwood trees over a given area. The motivation of the inference
    is to obtain a map of a rate, the number of trees in a given area.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将刚才使用的方法应用于一个二维空间问题。我们将使用如*图 [8.12](#x1-169015r12)*所示的红木数据集。该数据集（采用GPL许可证分发）来自GPstuff包。数据集包含了某一地区红木树的位置。推断的动机是获得一个速率图，表示某一地区内树木的数量。
- en: 'As with the coal-mining disaster example, we need to discretize the data:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 与煤矿灾难示例类似，我们需要对数据进行离散化处理：
- en: '**Code 8.12**'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.12**'
- en: '[PRE11]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![PIC](img/file233.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file233.png)'
- en: '**Figure 8.12**: Redwood data'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.12**：红木数据'
- en: 'Notice that instead of doing a mesh grid, we treat `x1` and `x2` data as being
    distinct arrays. This allows us to build a covariance matrix independently for
    each coordinate, effectively reducing the size of the matrix needed to compute
    the GP. We then combine both matrices using the `LatentKron` class. It is important
    to note that this is not a numerical trick, but a mathematical property of the
    structure of this type of matrix, so we are not introducing any approximation
    or error in our model. We are just expressing it in a way that allows faster computations:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与其做网格化处理，我们将`x1`和`x2`数据视为独立的数组。这使我们能够为每个坐标独立地构建协方差矩阵，从而有效地减少计算高斯过程所需的矩阵大小。然后我们使用`LatentKron`类将两个矩阵结合起来。需要强调的是，这不是一种数值技巧，而是此类矩阵结构的数学特性，因此我们并没有在模型中引入任何近似或误差。我们只是以一种方式表达它，从而实现更快的计算：
- en: '**Code 8.13**'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.13**'
- en: '[PRE12]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In *Figure [8.13](#x1-169028r13)*, the darker the shade of gray, the higher
    the rate of trees. We may imagine that we are interested in finding high-growing
    rate zones, because we may be interested in how a wood is recovering from a fire,
    or maybe we are interested in some properties of the soil and we use the trees
    as a proxy.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 [8.13](#x1-169028r13)*中，灰色的阴影越深，树木的生长速率越高。我们可以想象，我们的目标是找到高生长速率的区域，因为我们可能对木材从火灾中恢复的情况感兴趣，或者我们可能对土壤的一些特性感兴趣，并用树木作为代理。
- en: '![PIC](img/file234.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file234.png)'
- en: '**Figure 8.13**: Logistic regression, result of `model_rw`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.13**：逻辑回归，`model_rw`的结果'
- en: 8.9 Regression with spatial autocorrelation
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.9 带有空间自相关的回归分析
- en: 'The following example is taken from *Statistical Rethinking: A Bayesian Course
    with Examples in R and STAN, Second Edition by Richard McElreath, Copyright (2020)
    by Chapman and Hall/CRC. Reproduced by permission of Taylor & Francis Group*.
    I strongly recommend reading this book, as you will find many good examples like
    this and very good explanations. The only *caveat* is that the book examples are
    in R/Stan, but don’t worry and keep sampling; you will find the Python/PyMC version
    of those examples in the [https://github.com/pymc-devs/pymc-resources](https://github.com/pymc-devs/pymc-resources)
    resources.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '以下示例摘自*Statistical Rethinking: A Bayesian Course with Examples in R and STAN,
    Second Edition by Richard McElreath, Copyright (2020) by Chapman and Hall/CRC.
    经Taylor & Francis Group授权复制*。我强烈推荐阅读这本书，因为你会发现很多像这样的好例子以及非常好的解释。唯一的*警告*是，这些书中的例子是用R/Stan实现的，但别担心并继续采样；你可以在[https://github.com/pymc-devs/pymc-resources](https://github.com/pymc-devs/pymc-resources)资源中找到这些例子的Python/PyMC版本。'
- en: For this example we have 10 different island societies; for each one of them,
    we have the number of tools they use. Some theories predict that larger populations
    develop and sustain more tools than smaller populations. Thus, we have a regression
    problem where the dependent variable is the number of tools and the independent
    variable is the population. Because the number of tools is a count variable, we
    can use a Poisson distribution. Additionally, we have good theoretical reasons
    to think the logarithm of the population is a better variable than absolute size
    because what really matters (according to the theory) is the order of magnitude
    of the population.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们有10个不同的岛屿社会；对于每一个社会，我们都有它们使用的工具数量。一些理论预测，较大的人口比小人口能发展和维持更多的工具。因此，我们有一个回归问题，其中因变量是工具的数量，独立变量是人口。因为工具数量是计数变量，所以我们可以使用泊松分布。此外，我们有充分的理论依据认为，人口的对数比绝对人口大小更合适，因为真正重要的（根据理论）是人口的数量级。
- en: So far, the model we have in mind is a Poisson regression, but here comes the
    interesting part. Another important factor affecting the number of tools is the
    contact rates among the island societies. One way to include the contact rate
    in our model is to gather information on how frequent these societies were in
    contact throughout history and to create a categorical variable such as low/high
    rate. Yet another way is to use the distance between societies as a proxy of the
    contact rate, since it is reasonable to assume that geographically close societies
    come into contact more often than distant ones.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所考虑的模型是泊松回归，但这里有个有趣的部分。另一个影响工具数量的重要因素是岛屿社会之间的接触率。将接触率纳入我们模型的一种方法是收集这些社会在历史上接触的频率信息，并创建一个分类变量，如低/高接触率。另一种方法是使用社会之间的距离作为接触率的代理，因为合理的假设是，地理上较近的社会比远离的社会更频繁接触。
- en: The number of tools, the population size, and the coordinates are stored in
    the file `islands.csv` in the GitHub repo of this book ([https://github.com/aloctavodia/BAP3](https://github.com/aloctavodia/BAP3)).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 工具数量、人口规模和坐标存储在本书GitHub仓库中的`islands.csv`文件里（[https://github.com/aloctavodia/BAP3](https://github.com/aloctavodia/BAP3)）。
- en: 'Omitting the priors, the model we are going to build is:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略先验分布，我们将要构建的模型是：
- en: '![](img/Formula_02.PNG)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_02.PNG)'
- en: This model is a linear model plus a GP term. We use the linear part to model
    the effect of the logarithm of the population and the GP term to model the effect
    of the distance/contact rate. In this way, we will be effectively incorporating
    a measure of similarity in technology exposure (estimated from the distance matrix).
    Thus, instead of assuming the total number is just a consequence of population
    alone and independent from one society to the next, we will be modeling the number
    of tools in each society as a function of their spatial distribution.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是一个线性模型加上一个高斯过程（GP）项。我们用线性部分来建模人口对数的影响，用高斯过程项来建模距离/接触率的影响。通过这种方式，我们实际上在有效地纳入技术暴露的相似性度量（从距离矩阵估算）。因此，我们不会假设总数量仅仅是人口的结果，且各社会间相互独立，而是将每个社会的工具数量建模为其空间分布的函数。
- en: The information about the spatial distribution is in terms of latitudes and
    longitudes, but the kernels in PyMC assume the distances are all Euclidean. This
    can be problematic. Probably the cleanest way to circumvent this issue is to work
    with a distance that takes into account that the islands are on an approximately
    spherical planet. For instance, we can use the haversine distance, which determines
    the great-circle distance between two points on a sphere given their longitudes
    and latitudes. The great-circle distance is the shortest distance between two
    points on the surface of a sphere, measured along the surface of the sphere. To
    use this distance, we need to create a new kernel as shown in the next code block.
    If you are not very familiar with classes in Python, you just need to know that
    what I did is copy the code for the `ExpQuad` from the PyMC code base and tweak
    it a little bit to create a new class, `ExpQuadHaversine`. The largest change
    is the addition of the function/method `haversine_distance`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 关于空间分布的信息是以纬度和经度的形式存在的，但PyMC中的核函数假设所有距离都是欧几里得距离。这可能会带来问题。绕过这个问题的最简洁方法可能是使用考虑岛屿处于大致球形地球上的距离。例如，我们可以使用哈弗辛距离，它基于经纬度计算两个点之间的大圆距离。大圆距离是球面上两点之间的最短距离，是沿着球面测量的。为了使用这种距离，我们需要创建一个新的核函数，如下一个代码块所示。如果你对Python中的类不太熟悉，你只需要知道我所做的是复制了PyMC代码库中的`ExpQuad`代码，并稍微修改它，创建了一个新的类`ExpQuadHaversine`。最大变化是添加了函数/方法`haversine_distance`。
- en: '**Code 8.14**'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.14**'
- en: '[PRE13]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that we have defined the class `ExpQuadHaversine` we can use it to define
    the covariance matrix as we did with the previous models with the built-in kernels.
    For this model, we are going to introduce another change. We are going to define
    a parameter *η*. The role of this parameter is to scale the GP in the y-axis direction.
    It is pretty common to define GPs with both *ℓ* and *η*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了类`ExpQuadHaversine`，可以像使用之前的内置核函数那样，使用它来定义协方差矩阵。对于这个模型，我们将引入另一个变化。我们将定义一个参数*η*。这个参数的作用是对高斯过程在y轴方向进行缩放。通常，我们会为高斯过程定义*ℓ*和*η*这两个参数。
- en: '**Code 8.15**'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.15**'
- en: '[PRE14]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: To understand the posterior distribution of covariance functions in terms of
    distances, we can plot a few samples from the posterior distribution as in *Figure
    [8.14](#x1-170037r14)*. The black curve represents the posterior median covariance
    at each distance and the gray curves sample functions from the joint posterior
    distribution of *ℓ* and *η*.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解协方差函数相对于距离的后验分布，我们可以绘制一些来自后验分布的样本，如 *图 [8.14](#x1-170037r14)* 所示。黑色曲线表示每个距离的后验中位数协方差，灰色曲线表示从
    *ℓ* 和 *η* 的联合后验分布中采样的函数。
- en: '![PIC](img/file235.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file235.png)'
- en: '**Figure 8.14**: Posterior distribution of the spatial covariance'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.14**：空间协方差的后验分布'
- en: The thick black line in *Figure [8.14](#x1-170037r14)* is the posterior median
    of the covariance between pairs of societies as a function of distance. We use
    the median because the distributions for *ℓ* and *η* are very skewed. We can see
    that the covariance is, on average, not that high and also drops to almost 0 at
    about 2,000 kilometers. The thin lines represent the uncertainty, and we can see
    that there is a lot of uncertainty.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [8.14](#x1-170037r14)* 中的粗黑线表示社会对之间的协方差的后验中位数与距离的关系。我们使用中位数是因为 *ℓ* 和 *η*
    的分布非常偏斜。我们可以看到，协方差的平均值并不高，而且在大约 2,000 公里时几乎降到 0。细线表示不确定性，我们可以看到存在很大的不确定性。'
- en: Now let’s take a look at how strongly correlated the island societies are according
    to the model and data. To do this, we have to turn the covariance matrix into
    a correlation matrix. See the accompanying code for details. *Figure [8.15](#x1-170038r15)*
    shows a heatmap of the mean correlation matrix.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看根据模型和数据，岛屿社会之间的相关性有多强。为此，我们必须将协方差矩阵转换为相关矩阵。详情请参见附带的代码。*图 [8.15](#x1-170038r15)*
    显示了均值相关矩阵的热图。
- en: '![PIC](img/file236.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file236.png)'
- en: '**Figure 8.15**: Posterior mean correlation matrix'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.15**：后验均值相关矩阵'
- en: Two observations that stand out from the rest is, first, that Hawaii is very
    lonely. This makes sense, as Hawaii is very far away from the rest of the island
    societies. Also, we can see that Malekula (Ml), Tikopia (Ti), and Santa Cruz (SC)
    are highly correlated with one another. This also makes sense, as these societies
    are very close together, and they also have a similar number of tools.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 从其他观察中脱颖而出的两个观点是，首先，夏威夷非常孤独。这是有道理的，因为夏威夷距离其他岛屿社会非常遥远。其次，我们可以看到马莱库拉（Ml）、提科皮亚（Ti）和圣克鲁兹（SC）之间高度相关。这也是合理的，因为这些社会彼此非常接近，并且它们也有相似数量的工具。
- en: The left panel of *Figure [8.16](#x1-170039r16)* is essentially a map. The island
    societies are represented in their relative positions. The lines are the posterior
    median correlations among societies. The opacity of the lines is proportional
    to the value of the correlations.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [8.16](#x1-170039r16)* 的左面板本质上是一个地图。岛屿社会被表示为它们相对的位置。线条表示社会之间的后验中位数相关性。线条的透明度与相关性的值成正比。'
- en: '![PIC](img/file237.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file237.png)'
- en: '**Figure 8.16**: Posterior distribution of the spatial covariance'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.16**：空间协方差的后验分布'
- en: On the right panel of *Figure [8.16](#x1-170039r16)* , we have again the posterior
    median correlations, but this time plotted in terms of the log population versus
    the total number of tools. The dashed lines represent the median number of tools
    and the HDI of 94% as a function of log population. In both panels of *Figure
    [8.16](#x1-170039r16)*, the size of the dots is proportional to the population
    of each island society. Notice how the correlations among Malekula, Tikopia, and
    Santa Cruz describe the fact that they have a rather low number of tools close
    to the median or lower than the expected number of tools for their populations.
    Something similar is happening with Trobriand and Manus; they are geographically
    close and have fewer tools than expected for their population sizes. Tonga has
    way more tools than expected for its population and a relatively high correlation
    with Fiji. In a way, the model is telling us that Tonga has a positive effect
    on Lua Fiji, increasing the total number of tools and counteracting the effect
    of it on its close neighbors, Malekula, Tikopia, and Santa Cruz.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图[8.16](#x1-170039r16)*的右侧面板中，我们再次展示了后验中位数相关性，但这次是根据对数人口与工具总数的关系来绘制的。虚线表示工具的中位数和94%的高密度区间（HDI）作为对数人口的函数。在*图[8.16](#x1-170039r16)*的两个面板中，点的大小与每个岛屿社会的人口成正比。注意，马列库拉、提科皮亚和圣克鲁斯之间的相关性描述了它们工具数量相对较少，接近中位数或低于根据其人口预期的工具数量。类似的情况发生在特罗布里安群岛和马努斯岛；它们地理位置接近，且工具数量低于预期。汤加的工具数量远高于其人口预期，并且与斐济有较高的相关性。从某种程度上讲，模型告诉我们，汤加对斐济的影响是积极的，增加了工具的总数，同时抵消了对其邻近地区马列库拉、提科皮亚和圣克鲁斯的影响。
- en: 8.10 Hilbert space GPs
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.10 希尔伯特空间高斯过程（HSGP）
- en: Gaussian processes can be slow. The main reason is that their computation requires
    us to invert a matrix, whose size grows with the number of observations. This
    operation is computationally costly and does not scale very nicely. For that reason,
    a large portion of the research around GPs has been to find approximations to
    compute them faster and allow scaling them to large data.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯过程可能会比较慢。主要原因是它们的计算需要我们对一个矩阵进行求逆，而该矩阵的大小会随着观察数据的增多而增长。这个操作计算成本较高，而且扩展性不好。因此，围绕高斯过程的研究大部分集中在寻找近似方法，以便更快地计算它们并使其能够扩展到大规模数据。
- en: We are going to discuss only one of those approximations, namely the **Hilbert
    Space Gaussian Process** (**HSGP**), without going into the details of how this
    approximation is achieved. Conceptually, we can think of it as a basis function
    expansion similar, in spirit, to how splines are constructed (see *Chapter [6](CH06.xhtml#x1-1200006)*).
    The consequence of this approximation is that it turns the matrix inversion into
    just matrix multiplication, a much faster operation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只讨论其中一种近似方法，即**希尔伯特空间高斯过程**（**HSGP**），而不深入探讨这种近似是如何实现的。从概念上讲，我们可以将其视为一种基函数展开，类似于样条函数的构建方式（参见*第[6章](CH06.xhtml#x1-1200006)*）。这种近似的结果是，它将矩阵求逆操作转化为矩阵乘法，这是一种速度更快的操作。
- en: But When Will It Work?
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 但什么时候它能起作用？
- en: We can only use HSGPs for low dimensions (1 to maybe 3 or 4), and only for some
    kernels like the exponential quadratic or Matern. The reason is that for the HSGP
    approximation to work, the kernel has to be written in a special form known as
    power spectral density, and not all kernels can be written in this form.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只能在低维度（大约1到3或4维）中使用HSGP，并且仅限于某些核函数，如指数二次核或马特恩核。原因是，为了使HSGP近似法生效，核函数必须以一种特殊的形式表示，即功率谱密度形式，而并非所有核函数都可以用这种形式表示。
- en: Using the HSGP approximation in PyMC is straightforward, as we will demonstrate
    with the bikes dataset. We want to model the number of rented bikes as a function
    of the time of the day in hours. The following code block shows the PyMC implementation
    of such a model.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyMC中使用HSGP近似方法是非常直接的，我们将在自行车数据集上演示这一点。我们希望将出租自行车的数量建模为一天中小时数的函数。以下代码块展示了此模型的PyMC实现。
- en: '**Code 8.16**'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.16**'
- en: '[PRE15]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The main difference from previous GP models is the use of the `pm.gp.HSGP(.)`
    class instead of the `pm.gp.Latent(.)` class, which we should have used for non-Gaussian
    likelihoods and standard GPs. The class `pm.gp.HSGP(.)` has two parameters:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的高斯过程模型的主要区别在于，使用了`pm.gp.HSGP(.)`类，而不是用于非高斯似然和标准高斯过程的`pm.gp.Latent(.)`类。`pm.gp.HSGP(.)`类有两个参数：
- en: '`m` is the number of basic functions we use to approximate the GP. The larger
    the value of `m`, the better the approximation will be and the more costly the
    computation.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`m` 是我们用来逼近 GP 的基本函数数量。`m` 值越大，逼近效果越好，但计算成本也越高。'
- en: '`c` is a boundary factor. For a fixed and sufficiently large value of `m`,
    `c` affects the approximation of the mean function mainly near the boundaries.
    It should not be smaller than 1.2 (PyMC will give you a warning if you use a value
    lower than this), and usually 1.5 is a good choice. Changing this parameter does
    not affect the speed of the computations.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`c` 是一个边界因子。对于一个固定且足够大的 `m` 值，`c` 主要影响均值函数在边界附近的逼近。它不应小于 1.2（如果使用较小的值，PyMC
    会给出警告），通常 1.5 是一个不错的选择。改变此参数不会影响计算速度。'
- en: We set `m=10` partially because we are fans of the decimal system and partially
    based on the recommendations in the paper *Practical Hilbert space approximate
    Bayesian Gaussian processes for probabilistic programming* written by [Riutort-Mayol
    et al.](Bibliography.xhtml#Xriutortmayol_2022) [[2022](Bibliography.xhtml#Xriutortmayol_2022)].
    In practice, the results are robust to the exact values of `m` and `c`, as long
    as they are within a certain range based on what your prior for the length scale
    is. For details on how HSGP works and some advice on how to use it in practice,
    you can read [Riutort-Mayol et al.](Bibliography.xhtml#Xriutortmayol_2022) [[2022](Bibliography.xhtml#Xriutortmayol_2022)].
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置 `m=10`，部分原因是我们喜欢十进制系统，部分原因是基于 [Riutort-Mayol et al.](Bibliography.xhtml#Xriutortmayol_2022)
    发表的论文《Practical Hilbert space approximate Bayesian Gaussian processes for probabilistic
    programming》中的建议 [[2022](Bibliography.xhtml#Xriutortmayol_2022)]。实际上，只要 `m` 和
    `c` 的值在某个范围内，与长度尺度的先验一致，结果对它们的精确值是稳健的。关于 HSGP 如何工作以及如何在实践中使用它的一些建议，可以参考 [Riutort-Mayol
    et al.](Bibliography.xhtml#Xriutortmayol_2022) [[2022](Bibliography.xhtml#Xriutortmayol_2022)]。
- en: Now let’s see the results. *Figure [8.17](#x1-171015r17)* shows the mean posterior
    GP in black and 100 samples (realizations) from the GP posterior (gray lines).
    You can compare these results to the ones obtained using splines (see *Figure
    [6.8](CH06.xhtml#x1-124013r8)*).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看结果。*图 [8.17](#x1-171015r17)* 显示了黑色的均值后验 GP 和来自 GP 后验的 100 个样本（灰色线）。你可以将这些结果与使用样条得到的结果进行比较（见*图
    [6.8](CH06.xhtml#x1-124013r8)*）。
- en: '![PIC](img/file238.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file238.png)'
- en: '**Figure 8.17**: Posterior mean for the HSGP model for rented bikes as a function
    of the time of the day'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.17**：HSGP 模型对租赁自行车的后验均值，作为一天中时间的函数'
- en: The HSGP approximation is also implemented in Bambi. Let’s see how we can use
    it.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: HSGP 近似也已在 Bambi 中实现。让我们看看如何使用它。
- en: 8.10.1 HSGP with Bambi
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.10.1 HSGP 与 Bambi
- en: 'To fit the previous model with Bambi, we need to write the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Bambi 拟合之前的模型，我们需要编写以下代码：
- en: '**Code 8.17**'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.17**'
- en: '[PRE16]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This will work, but instead, we will provide priors to Bambi, as we did with
    PyMC. This will result in a much faster sampling and more reliable samples.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这样是可行的，但我们将为 Bambi 提供先验，就像我们在 PyMC 中做的那样。这将导致更快的采样和更可靠的样本。
- en: 'As we saw in *Chapter [6](CH06.xhtml#x1-1200006)*, to define priors in Bambi,
    we just need to pass a dictionary to the `priors` argument of `bmb.Model`. But
    we must be aware that HSGP terms do not receive priors. Instead, we need to define
    priors for *ℓ* (called `ell` in Bambi) and *η* (called `sigma` in Bambi) and pass
    those priors to the HSGP terms. One more thing: as in the previous model, we did
    not use *η* but since Bambi is expecting it, we use a dirty trick to define a
    prior that is essentially 1.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*第 [6](CH06.xhtml#x1-1200006)* 章中看到的那样，要在 Bambi 中定义先验，我们只需将字典传递给 `bmb.Model`
    的 `priors` 参数。但我们必须注意，HSGP 项不会接收先验。相反，我们需要为 *ℓ*（在 Bambi 中称为 `ell`）和 *η*（在 Bambi
    中称为 `sigma`）定义先验，并将这些先验传递给 HSGP 项。还有一件事：和之前的模型一样，我们没有使用 *η*，但由于 Bambi 需要它，我们使用了一个小技巧来定义一个基本上是
    1 的先验。
- en: '**Code 8.18**'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 8.18**'
- en: '[PRE17]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: I invite you to check that the parameters computed by Bambi are very similar
    to those we got with PyMC. *Figure [8.18](#x1-172023r18)* shows the mean posterior
    GP in black and a band for the HDI of 94%. The figure was generated with `bmb.interpret.plot_predictions`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我邀请你检查 Bambi 计算的参数，它们与我们通过 PyMC 得到的非常相似。*图 [8.18](#x1-172023r18)* 显示了黑色的均值后验
    GP 和 94% HDI 的带状区域。该图是通过 `bmb.interpret.plot_predictions` 生成的。
- en: '![PIC](img/file239.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file239.png)'
- en: '**Figure 8.18**: Posterior mean for the HSGP model for rented bikes as a function
    of the time of the day, using Bambi'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.18**：HSGP 模型对租赁自行车的后验均值，作为一天中时间的函数，使用 Bambi'
- en: In this section, we have explored the concept of HSGP as a powerful approximation
    to scale Gaussian processes to large datasets. By combining the flexibility of
    PyMC and Bambi with the scalability offered by HSGPs, researchers and practitioners
    can more effectively tackle complex modeling tasks, paving the way for the application
    of Gaussian processes on increasingly large and intricate datasets.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了HSGP的概念，作为一种强大的近似方法，可以将高斯过程扩展到大数据集。通过将PyMC和Bambi的灵活性与HSGP提供的可扩展性相结合，研究人员和实践者可以更有效地解决复杂的建模任务，为在日益庞大和复杂的数据集上应用高斯过程铺平道路。
- en: 8.11 Summary
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.11 总结
- en: A Gaussian process is a generalization of the multivariate Gaussian distribution
    to infinitely many dimensions and is fully specified by a mean function and a
    covariance function. Since we can conceptually think of functions as infinitely
    long vectors, we can use Gaussian processes as priors over functions. In practice,
    we do not work with infinite objects but with multivariate Gaussian distributions
    with as many dimensions as data points. To define their corresponding covariance
    function, we used properly parameterized kernels; and by learning about those
    hyperparameters, we ended up learning about arbitrary complex functions.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯过程是多元高斯分布的一种推广，适用于无限多个维度，完全由均值函数和协方差函数指定。由于我们可以概念上将函数视为无限长的向量，因此可以将高斯过程作为函数的先验。在实践中，我们并不处理无限对象，而是处理与数据点数量相等维度的多元高斯分布。为了定义其相应的协方差函数，我们使用了适当参数化的核；通过学习这些超参数，我们最终学会了关于任意复杂函数的知识。
- en: In this chapter, we have given a short introduction to GPs. We have covered
    regression, semi-parametric models (the islands example), combining two or more
    kernels to better describe the unknown function, and how a GP can be used for
    classification tasks. There are many other topics we could have discussed. Nevertheless,
    I hope this introduction to GPs has motivated you sufficiently to keep using,
    reading, and learning about Gaussian processes and Bayesian non-parametric models.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们简要介绍了高斯过程（GP）。我们涵盖了回归、半参数模型（岛屿示例）、将两个或多个核组合以更好地描述未知函数，以及高斯过程如何用于分类任务。还有许多其他主题我们可以讨论。然而，我希望这段高斯过程的介绍足以激励你继续使用、阅读和学习高斯过程及贝叶斯非参数模型。
- en: 8.12 Exercises
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.12 练习
- en: For the example in the *Covariance functions and kernels* section, make sure
    you understand the relationship between the input data and the generated covariance
    matrix. Try using other input such as `data = np.random.normal(size=4)`.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于*协方差函数和核*部分中的示例，确保理解输入数据与生成的协方差矩阵之间的关系。尝试使用其他输入，如`data = np.random.normal(size=4)`。
- en: Rerun the code generating *Figure [8.3](#x1-160010r3)* and increase the number
    of samples obtained from the GP prior to around 200\. In the original figure,
    the number of samples is 2\. What is the range of the generated values?
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新运行生成*图 8.3*的代码，并将从高斯过程（GP）先验中获得的样本数量增加到约200。原始图中的样本数量为2。生成值的范围是什么？
- en: 'For the generated plot in the previous exercise, compute the standard deviation
    for the values at each point. Do this in the following form:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于前一个练习中生成的图表，计算每个点的标准差。按照以下形式进行操作：
- en: Visually, just observing the plots
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从视觉上看，仅仅观察图表
- en: Directly from the values generated from `pz.MVNormal(.).rvs`
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接来自于`pz.MVNormal(.).rvs`生成的值
- en: By inspecting the covariance matrix (if you have doubts go back to exercise
    1)
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过检查协方差矩阵（如果有疑问，请回到练习1）
- en: Did the values you get from these three methods match?
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从这三种方法中得到的值是否匹配？
- en: Use test points `np.linspace(np.floor(x.min()), 20, 100)[:,None]` and re-run
    `model_reg`. Plot the results. What did you observe? How is this related to the
    specification of the GP prior?
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用测试点`np.linspace(np.floor(x.min()), 20, 100)[:,None]`并重新运行`model_reg`。绘制结果。你观察到了什么？这与GP先验的规范有何关系？
- en: Repeat exercise 1, but this time use a linear kernel (see the accompanying code
    for a linear kernel).
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复练习1，但这次使用线性核（参见附带的线性核代码）。
- en: Check out [https://www.pymc.io/projects/examples/en/latest/gaussian_processes/GP-MeansAndCovs.html](https://www.pymc.io/projects/examples/en/latest/gaussian_processes/GP-MeansAndCovs.html)
    in PyMC’s documentation.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请查看PyMC文档中的[https://www.pymc.io/projects/examples/en/latest/gaussian_processes/GP-MeansAndCovs.html](https://www.pymc.io/projects/examples/en/latest/gaussian_processes/GP-MeansAndCovs.html)。
- en: Run a logistic regression model for the `space_flu` data. What do you see? Can
    you explain the result?
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`space_flu`数据运行逻辑回归模型。你看到了什么？你能解释结果吗？
- en: 'Change the logistic regression model in order to fit the data. Tip: Use an
    order 2 polynomial.'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改逻辑回归模型以适应数据。提示：使用二阶多项式。
- en: Compare the model for the coal mining disaster with the one from the PyMC documentation
    ( [https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#case-study-2-coal-mining-disasters](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#case-study-2-coal-mining-disasters)).
    Describe the differences between both models in terms of model specification and
    results.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将煤矿灾难的模型与PyMC文档中的模型进行比较（[https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#case-study-2-coal-mining-disasters](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#case-study-2-coal-mining-disasters)）。描述这两个模型在模型规范和结果方面的差异。
- en: Join our community Discord space
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的社区Discord空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at: [https://packt.link/bayesian](https://packt.link/bayesian)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区，结识志同道合的人，并与5000多名成员一起学习：[https://packt.link/bayesian](https://packt.link/bayesian)
- en: '![PIC](img/file1.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1.png)'
