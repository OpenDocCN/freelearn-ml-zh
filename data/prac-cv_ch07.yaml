- en: Segmentation and Tracking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分割和跟踪
- en: In the previous chapter, we studied different methods for feature extraction
    and image classification using **Convolutional Neural Networks** (**CNNs**) to
    detect objects in an image. Those methods work well in creating a bounding box
    around the target object. However, if our application requires a precise boundary,
    called an **instance**, around the object, we need to apply a different approach.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们研究了使用**卷积神经网络**（**CNNs**）进行特征提取和图像分类的不同方法，以检测图像中的对象。这些方法在围绕目标对象创建边界框方面表现良好。然而，如果我们的应用程序需要围绕对象的一个精确边界，即**实例**，我们需要采用不同的方法。
- en: In this chapter, we will be focusing on object instance detection, which is
    also termed image segmentation. In the second part of the chapter, we will first
    see MOSSE tracker with OpenCV see various approaches to tracking objects in a
    sequence of image
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注对象实例检测，这也可以称为图像分割。在章节的第二部分，我们将首先看到使用OpenCV的MOSSE跟踪器，并了解在一系列图像中跟踪对象的各种方法。
- en: Segmentation and tracking are, however, not quite interlinked problems, but
    they depend heavily on the previous approaches of feature extraction and object
    detection. The application's range is quite vast, including image editing, image
    denoising, surveillance, motion capture, and so on. The chosen methods for segmentation
    and tracking are suitable for specific applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 分割和跟踪并不是完全相关的两个问题，但它们在很大程度上依赖于之前特征提取和对象检测的方法。应用范围相当广泛，包括图像编辑、图像去噪、监控、动作捕捉等。所选择的方法适合特定的应用。
- en: Datasets and libraries
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集和库
- en: We will be continuing the use of OpenCV and NumPy for image processing. For
    deep learning, we will use Keras with the TensorFlow backend. For segmentation,
    we will be using the `Pascal VOC` dataset. This has annotations for object detection,
    as well as segmentation. For tracking, we will use the `MOT16` dataset, which
    consists of an annotated sequence of images from video. We will mention how to
    use the code in the sections where it is used.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用OpenCV和NumPy进行图像处理。对于深度学习，我们将使用Keras和TensorFlow后端。对于分割，我们将使用`Pascal VOC`数据集。这个数据集包含对象检测和分割的注释。对于跟踪，我们将使用`MOT16`数据集，它由视频中的注释图像序列组成。我们将在使用代码的章节中提及如何使用代码。
- en: Segmentation
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分割
- en: 'Segmentation is often referred to as the clustering of pixels of a similar
    category. An example is as shown in the following screenshot. Here, we see that
    inputs are on the left and the segmentation results are on the right. The colors
    of an object are according to pre-defined object categories. These examples are
    taken from the `Pascal VOC` dataset:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 分割通常被称为相似类别像素的聚类。以下截图是一个例子。在这里，我们看到输入在左边，分割结果在右边。物体的颜色根据预定义的物体类别。这些例子是从`Pascal
    VOC`数据集中选取的：
- en: '![](img/69476ba8-0d38-46cf-83cd-6c583e42c4a1.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/69476ba8-0d38-46cf-83cd-6c583e42c4a1.png)'
- en: In the top picture on the left, there are several small aeroplanes in the background
    and, therefore, we see small pixels colored accordingly in the corresponding image
    on the right. In the bottom-left picture, there are two pets laying together,
    therefore, their segmented image on the right has different colors for the pixels
    belonging to the cat and dog respectively. In this figure, the boundary is differently
    colored for convenience and does not imply a different category.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在左上方的图片中，背景中有几架小型飞机，因此我们在右边的相应图像中看到相应的小像素被相应地着色。在左下方的图片中，有两只宠物躺在一起，因此它们分割后的图像在右边为猫和狗的像素分别有不同的颜色。在这个图中，边界以不同的颜色表示，以方便查看，并不表示不同的类别。
- en: In traditional segmentation techniques, the key property used is image intensity
    levels. First, different smaller regions of similar intensity values are found,
    and later they are merged into larger regions. To get the best performance, an
    initial point is chosen by the user for algorithms. Recent approaches using deep
    learning have shown better performance without the need for initialization. In
    further sections, we will see an extension of previously seen CNNs for image segmentation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的分割技术中，使用的关键属性是图像强度级别。首先，找到具有相似强度值的不同较小区域，然后它们被合并成较大的区域。为了获得最佳性能，用户为算法选择一个初始点。最近使用深度学习的方法已经显示出更好的性能，而不需要初始化。在接下来的章节中，我们将看到之前看到的CNNs在图像分割中的应用扩展。
- en: Before starting our discussion on segmentation methods, let's look at the challenges.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始我们关于分割方法的讨论之前，让我们看看所面临的挑战。
- en: Challenges in segmentation
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分割的挑战
- en: 'The challenges in a segmentation task are greater than the previous object
    detection task, as the complexity of detection is increased:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 分割任务中的挑战大于之前的对象检测任务，因为检测的复杂性增加了：
- en: '**Noisy boundaries**: Grouping pixels that belong to a category may not be
    as accurate due to the fuzzy edges of an object. As a result, objects from different
    categories are clustered together.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声边界**：由于对象的模糊边缘，将属于某一类别的像素分组可能不会那么准确。因此，来自不同类别的对象可能会聚集在一起。'
- en: '**Cluttered scene**: With several objects in the image frame, it becomes harder
    to classify pixels correctly. With more clutter, the chances of false positive
    classification also increase.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**杂乱场景**：当图像框架中有多个对象时，正确分类像素变得更加困难。随着杂乱的增多，错误分类的可能性也增加。'
- en: CNNs for segmentation
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于分割的CNN
- en: 'Deep learning based segmentation approaches have recently grown, both in terms
    of accuracy as well as effectiveness, in more complex domains. One of the popular
    models using CNN for segmentation is a **fully convolutional network** (**FCN**)[5],
    which we will explore in this section. This method has the advantage of training
    an end-to-end CNN to perform pixel-wise semantic segmentation. The output is an
    image with each pixel classified as either background or into one of the predefined
    categories of objects. The overall architecture is shown in the following screenshot:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的分割方法最近在更复杂的领域中，无论是在准确性还是在有效性方面，都得到了增长。使用CNN进行分割的流行模型之一是**全卷积网络**（**FCN**）[5]，我们将在本节中探讨。这种方法的优势在于训练一个端到端的CNN以执行像素级语义分割。输出是一个图像，每个像素都被分类为背景或预定义对象类别之一。整体架构如下截图所示：
- en: '![](img/49f5436a-4b06-4495-bee9-349d2a6c25cc.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49f5436a-4b06-4495-bee9-349d2a6c25cc.png)'
- en: As the layers are stacked hierarchically, the output from each layer gets downsampled
    yet is feature rich. In the last layer, as shown in the figure, the downsampled
    output is upsampled using a deconvolutional layer, resulting in the final output
    being the same size as that of the input.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随着层的层次堆叠，每一层的输出都会下采样，但特征丰富。在最后一层，如图所示，使用反卷积层将下采样输出上采样，从而使得最终输出与输入大小相同。
- en: The deconvolutional layer is used to transform the input feature to the upsampled
    feature, however, the name is a bit misleading, as the operation is not exactly
    the inverse of convolution. This acts as transposed convolution, where the input
    is convolved after a transpose, as compared to a regular convolution operation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 反卷积层用于将输入特征转换为上采样特征，然而，这个名称有点误导，因为操作并不完全是卷积的逆操作。这相当于转置卷积，与常规卷积操作相比，输入在转置后进行卷积。
- en: 'In the previous model, the upsampling of the feature layer was done with a
    single layer. This can, however, be extended over to a hierarchical structure,
    as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的模型中，特征层的上采样是通过单个层完成的。然而，这可以扩展到层次结构，如下所示：
- en: '![](img/29bbcdc6-0f64-4016-9e24-9288c303e85b.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29bbcdc6-0f64-4016-9e24-9288c303e85b.png)'
- en: In the preceding screenshot, the feature extractor is kept the same, while upsampling
    is updated with more deconvolutional layers where each of these layers upsamples
    features from the previous layer and generates an overall richer prediction.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，特征提取器保持不变，而上采样通过添加更多的反卷积层进行更新，其中每个这些层都从上一层上采样特征并生成更丰富的预测。
- en: Implementation of FCN
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FCN的实现
- en: In this section, we will learn to model one of the basic segmentation models
    in Keras.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习在Keras中建模基本分割模型之一。
- en: 'Let''s begin by importing Keras required modules:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入Keras所需的模块开始：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following code will create an FCN model, which takes in VGG16 features
    as input and adds further layers for fine tuning them. These are then upsampled
    to give resulting output:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将创建一个FCN模型，该模型以VGG16特征作为输入，并添加更多层以进行微调。然后这些层被上采样以产生结果输出：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this section, we saw segmentation methods to compute object precise region
    in an image. The FCN method shown here uses only convolutional layers to compute
    these regions. The `upsampling` method is key to compute pixel-wise categories
    and hence different choices of upsampling methods will result in a different quality
    of results.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了计算图像中对象精确区域的分割方法。这里展示的FCN方法仅使用卷积层来计算这些区域。`上采样`方法是计算像素级类别的关键，因此不同的上采样方法选择将导致结果质量的不同。
- en: Tracking
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪
- en: Tracking is the problem of estimating the position of an object over consecutive
    image sequences. This is also further divided into single object tracking and
    multiple object tracking, however, both single and multi-object tracking require
    slightly different approaches. In this section, we will see the methods for multi-object
    tracking, as well as single-object tracking.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪是估计物体在连续图像序列中的位置的问题。这进一步分为单目标跟踪和多目标跟踪，然而，单目标和多目标跟踪都需要稍微不同的方法。在本节中，我们将看到多目标跟踪的方法，以及单目标跟踪。
- en: The methods for image-based tracking are used in several applications, such
    as action recognition, self-driving cars, security and surveillance, augmented
    reality apps, motion capture systems, and video compression techniques. In **Augmented
    Reality** (**AR**) apps, for example, if we want to draw a virtual three-dimensional
    object on a planar surface, we would want to keep track of the planar surface
    for a feasible output.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图像的跟踪方法被应用于多个应用中，例如动作识别、自动驾驶汽车、安全和监控、增强现实应用、动作捕捉系统和视频压缩技术。例如，在增强现实（**AR**）应用中，如果我们想在平面表面上绘制一个虚拟的三维物体，我们希望跟踪平面表面以实现可行的输出。
- en: In surveillance or traffic monitoring, tracking vehicles and keeping records
    of number plates helps to manage traffic and keeps security in check. Also, in
    video compression applications, if we already know that a single object is the
    only thing changing in frames, we can perform better compression by using only
    those pixels that change, thereby optimizing video transmission and receiving.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在监控或交通监控中，跟踪车辆并记录车牌号码有助于管理交通并保持安全。此外，在视频压缩应用中，如果我们已经知道单个物体是帧中唯一变化的东西，我们可以通过仅使用那些变化的像素来执行更好的压缩，从而优化视频传输和接收。
- en: In the setup of tracking, we will first see challenges in the next section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在跟踪设置中，我们将在下一节看到挑战。
- en: Challenges in tracking
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪挑战
- en: 'It is always crucial to know which challenges we need to take care of before
    building apps. As a standard computer vision method, a lot of the challenges here
    are common:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建应用程序之前了解我们需要关注哪些挑战始终至关重要。作为一个标准的计算机视觉方法，这里的大多数挑战都是常见的：
- en: '**Object occlusion**: If the target object is hidden behind other objects in
    a sequence of images, then it becomes not only hard to detect the object but also
    to update future images if it becomes visible again.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物体遮挡**：如果目标物体在一系列图像中隐藏在其他物体后面，那么不仅难以检测到该物体，而且如果它再次可见，更新未来图像也会变得困难。'
- en: '**Fast movement**: Cameras, such as on smartphones, often suffers from jittery
    movement. This causes a blurring effect and, sometimes, the complete absence of
    an object from the frame. Therefore, sudden changes in the motion of cameras also
    lead to problems in tracking applications.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**快速移动**：例如智能手机上的相机，常常受到抖动的影响。这会导致模糊效果，有时甚至会导致物体完全从帧中消失。因此，相机运动的突然变化也会导致跟踪应用中的问题。'
- en: '**Change of shape**: If we are targeting non-rigid objects, changes in shape
    or the complete deformation of an object will often lead to being unable to detect
    the object and also tracking failure.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**形状变化**：如果我们针对的是非刚性物体，物体的形状变化或完全变形通常会导致无法检测到物体，以及跟踪失败。'
- en: '**False positives**: In a scene with multiple similar objects, it is hard to
    match which object is targeted in subsequent images. The tracker may lose the
    current object in terms of detection and start tracking a similar object.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**误报**：在包含多个相似物体的场景中，很难匹配后续图像中目标物体是哪一个。跟踪器可能在检测过程中丢失当前物体，并开始跟踪一个相似的物体。'
- en: These challenges can make our applications crash suddenly or give a completely
    incorrect estimate of an object's location.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战可能导致我们的应用程序突然崩溃或给出完全错误的目标物体位置估计。
- en: Methods for object tracking
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体跟踪方法
- en: An intuitive method for tracking is to use the object detection method from
    the previous chapter and compute detection in each frame. This will result in
    a bounding box detection for every frame, but we would also like to know if a
    particular object stays in the image sequence and for how many frames, that is,
    to keep track of K-frames for the object in the scene. We would also need a matching
    strategy to say that the object found in the previous image is the same as the
    one in the current image frame.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一种直观的跟踪方法是使用前一章中的对象检测方法并在每个帧中进行检测。这将导致每个帧都有一个边界框检测，但我们还想知道特定对象是否保持在图像序列中以及持续了多少帧，即跟踪场景中对象的
    K 帧。我们还需要一个匹配策略来说明前一个图像中找到的对象与当前图像帧中的对象相同。
- en: Continuing with this intuition, we add a predictor for the bounding box motion.
    We assume a state for the bounding box, which consists of coordinates for the
    box center as well as its velocities. This state changes as we see more boxes
    in the sequence.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 继续这种直觉，我们添加了一个用于边界框运动的预测器。我们假设一个边界框的状态，它由框中心的坐标及其速度组成。随着我们在序列中看到更多的框，这个状态会发生变化。
- en: Given the current state of the box, we can predict a possible region for where
    it will be in the next frame by assuming some noise in our measurement. The object
    detector can search for an object similar to the previous object in the next possible
    region. The location of the newly found object box and the previous box state
    will help us to update the new state of the box. This will be used for the next
    frame. As a result, iterating this process over all of the frames will result
    in not only the tracking of the object bounding box but keeping a location check
    on particular objects over the whole sequence. This method of tracking is also
    termed as **tracking by detection. **
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 给定当前框的状态，我们可以通过假设测量中存在一些噪声来预测它将在下一帧中可能出现的区域。对象检测器可以在下一个可能的区域中搜索与先前对象相似的对象。新找到的对象框的位置和先前框的状态将帮助我们更新框的新状态。这将用于下一帧。因此，在所有帧上迭代此过程不仅会导致对象边界框的跟踪，而且还会在整个序列中对特定对象的位置进行检查。这种跟踪方法也被称为**检测跟踪**。
- en: In tracking by detection, each frame uses an object detector to find possible
    instances of objects and matches those detections with corresponding objects in
    the previous frame.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在检测跟踪中，每一帧都使用对象检测器来找到可能的对象实例，并将这些检测与前一帧中的相应对象进行匹配。
- en: On the other hand, if no object detector is to be used, we can initialize the
    target object and track it by matching it and finding a similar object in each
    frame.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果不需要使用对象检测器，我们可以通过匹配和在每个帧中找到相似对象来初始化目标对象并跟踪它。
- en: In the following section, we will see two popular methods for tracking. The
    first method is quite fast, yet simple, while the latter is quite accurate, even
    in the case of multiple-object tracking.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到两种流行的跟踪方法。第一种方法相当快，而且简单，而后者相当准确，即使在多对象跟踪的情况下也是如此。
- en: MOSSE tracker
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MOSSE 跟踪器
- en: 'This is proposed by for fast object tracking using correlation filter methods.
    Correlation filter-based tracking comprises the following steps:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由提出的一种使用相关滤波器方法进行快速对象跟踪的方法。基于相关滤波器的跟踪包括以下步骤：
- en: Assuming a template of a target object *T* and an input image *I*, we first
    take the **Fast Fourier Transform** (**FFT**) of both the template (*T*) and the
    image (*I*).
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设目标对象 *T* 的模板和输入图像 *I*，我们首先对模板 (*T*) 和图像 (*I*) 进行**快速傅里叶变换**（**FFT**）。
- en: A convolution operation is performed between template *T* and image *I*.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模板 *T* 和图像 *I* 之间执行卷积操作。
- en: The result from step 2 is inverted to the spatial domain using **Inverse Fast
    Fourier Transform** (**IFFT**). The position of the template object in the image
    *I* is the max value of the IFFT response we get.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第2步的结果通过**逆快速傅里叶变换**（**IFFT**）转换到空间域。图像 *I* 中模板对象的位置是获得的 IFFT 响应的最大值。
- en: 'This correlation filter-based technique has limitations in the choice of *T*.
    As a single template image match may not observe all the variations of an object,
    such as rotation in the image sequence, Bolme, and its co-authors[1] proposed
    a more robust tracker-based correlation filter, termed as **Minimum Output Sum
    of Squared Error** (**MOSSE**) filter. In this method, the template *T* for matching
    is first learned by minimizing a sum of squared error as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于相关滤波器的技术在选择*T*时有局限性。由于单个模板图像匹配可能无法观察到对象的所有变化，例如图像序列中的旋转，Bolme及其合作者[1]提出了一种更鲁棒的基于跟踪器的相关滤波器，称为**最小输出平方误差**（MOSSE）滤波器。在此方法中，匹配的模板*T*首先通过最小化平方误差和来学习：
- en: '![](img/a0ceec79-6109-42d4-b7e0-e1f27ddcebaa.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0ceec79-6109-42d4-b7e0-e1f27ddcebaa.png)'
- en: Here, *i* is the training samples and the resulting learned template is *T**.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*i*是训练样本，得到的学到的模板是*T**。
- en: We will see the implementation of MOSSE tracker from OpenCV, as it already has
    good implementation here: [https://github.com/opencv/opencv/blob/master/samples/python/mosse.py](https://github.com/opencv/opencv/blob/master/samples/python/mosse.py)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看OpenCV中MOSSE跟踪器的实现，因为它在这里已经有了很好的实现：[https://github.com/opencv/opencv/blob/master/samples/python/mosse.py](https://github.com/opencv/opencv/blob/master/samples/python/mosse.py)
- en: 'We will look at the key parts of the following code:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看以下代码的关键部分：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `update` function gets a frame from video or image sequence iteratively
    and updates the state of the tracker:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`update`函数迭代地从视频或图像序列中获取一帧并更新跟踪器的状态：'
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A major advantage of using the MOSSE filter is that it is quite fast for real-time
    tracking systems. The overall algorithm is simple to implement and can be used
    in the hardware without special image processing libraries, such as embedded platforms.
    There have been several modifications to this filter and, as such, readers are
    requested to explore more about these filters.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MOSSE滤波器的一个主要优点是它对于实时跟踪系统来说非常快。整体算法简单易实现，可以在没有特殊图像处理库的硬件上使用，例如嵌入式平台。对此滤波器已经进行了几次修改，因此读者被要求探索更多关于这些滤波器的信息。
- en: Deep SORT
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Deep SORT
- en: Previously, we looked at one of the simplest trackers. In this section, we will
    use richer features from CNNs to perform tracking. **Deep SORT**[2] is a recent
    algorithm for tracking that extends **Simple Online and Real-time Tracking**[3]
    and has shown remarkable results in the **Multiple Object Tracking** (**MOT**)
    problem.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们查看了一种最简单的跟踪器。在本节中，我们将使用CNN的更丰富特征来进行跟踪。**Deep SORT**[2]是一种跟踪算法，它扩展了**简单在线实时跟踪**[3]，并在**多目标跟踪**（MOT）问题中取得了显著成果。
- en: 'In the problem setting of MOT, each frame has more than one object to track.
    A generic method to solve this has two steps:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在MOT的问题设置中，每一帧都有多个需要跟踪的对象。解决这个问题的通用方法有两个步骤：
- en: '**Detection**: First, all the objects are detected in the frame. There can
    be single or multiple detections.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检测**：首先，在帧中检测所有对象。可能存在单个或多个检测。'
- en: '**Association**: Once we have detections for the frame, a matching is performed
    for similar detections with respect to the previous frame. The matched frames
    are followed through the sequence to get the tracking for an object.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关联**：一旦我们为帧获得了检测，就会对与前一帧相似的检测进行匹配。匹配的帧会跟随序列以获取对象的跟踪。'
- en: 'In Deep SORT, this generic method is further divided into three steps:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在Deep SORT中，这种方法进一步分为三个步骤：
- en: To compute detections, a popular CNN-based object detection method is used.
    In the paper[2], Faster-RCNN[4] is used to perform the initial detection per frame.
    As explained in the previous chapter, this method is two-stage object detection,
    which performs well for object detection, even in cases of object transformations
    and occlusions.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了计算检测，使用了一种基于CNN的流行目标检测方法。在论文[2]中，使用Faster-RCNN[4]对每一帧进行初始检测。正如前一章所解释的，这种方法是两阶段目标检测，对于目标检测效果良好，即使在目标变换和遮挡的情况下也是如此。
- en: The intermediate step before data association consists of an estimation model.
    This uses the state of each track as a vector of eight quantities, that is, box
    center (*x*, *y*), box scale (*s*), box aspect ratio (*a*), and their derivatives
    with time as velocities. The Kalman filter is used to model these states as a
    dynamical system. If there is no detection of a tracking object for a threshold
    of consecutive frames, it is considered to be out of frame or lost. For a newly
    detected box, the new track is started.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据关联之前的中间步骤包括一个估计模型。该模型使用每个轨迹的状态作为包含八个量的向量，即框中心（*x*，*y*），框尺度（*s*），框宽高比（*a*），以及它们随时间变化的导数作为速度。卡尔曼滤波用于将这些状态建模为动态系统。如果没有检测到跟踪对象连续帧的阈值，则认为该对象已出帧或丢失。对于新检测到的框，将开始新的轨迹。
- en: In the final step, given the predicted states from Kalman filtering using the
    previous information and the newly detected box in the current frame, an association
    is made for the new detection with old object tracks in the previous frame. This
    is computed using Hungarian algorithm on bipartite graph matching. This is made
    even more robust by setting the weights of the matching with distance formulation.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最后一步，给定使用先前信息和当前帧中新检测到的框进行卡尔曼滤波预测的状态，对新检测与前一帧中的旧对象轨迹进行关联。这是通过在二分图匹配上使用匈牙利算法来计算的。通过设置匹配的权重与距离公式，这使匹配更加稳健。
- en: 'This is further explained in the following diagram. The tracker uses a vector
    of states to store the historical information for previous detections. If a new
    frame comes, we can either use pre-stores bounding box detections or compute them
    using object detection methods discussed in *chapter 6.*  Finally, using current
    observation of bounding box detections and previous states, the current tracking
    is estimated:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这在以下图中进一步解释。跟踪器使用状态向量来存储先前检测的历史信息。如果新帧到来，我们可以使用预先存储的边界框检测或使用第*6*章中讨论的对象检测方法来计算它们。最后，使用当前观察到的边界框检测和先前状态，估计当前跟踪：
- en: '![](img/52fec427-9359-4a8c-92ca-d2e9ea01434e.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/52fec427-9359-4a8c-92ca-d2e9ea01434e.png)'
- en: We will see an effective demo of Deep SORT using its official repository at  [https://github.com/nwojke/deep_sort](https://github.com/nwojke/deep_sort)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用其官方仓库在[https://github.com/nwojke/deep_sort](https://github.com/nwojke/deep_sort)中看到Deep
    SORT的有效演示
- en: 'At first, clone the following repository:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，克隆以下仓库：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Since we already have TensorFlow and Keras installed, we will not be going
    through their installation. As we saw previously, it uses CNN-based object detection
    for initial detection. We can run the network and get detection or use pre-generated
    detections. To do so, let''s get pre-trained models here in the `deep_sort` folder:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已安装了TensorFlow和Keras，因此我们将不会介绍它们的安装过程。正如我们之前所看到的，它使用基于CNN的对象检测进行初始检测。我们可以运行网络以获取检测或使用预先生成的检测。为此，让我们在`deep_sort`文件夹中获取预训练模型：
- en: 'On macOS (if `wget` is not available):'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在macOS上（如果`wget`不可用）：
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'On Linux:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Linux上：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: These downloaded files consist of pre-detected boxes using CNN-based models
    for the `MOT challenge` dataset CC BY-NC-SA 3.0\. We need one more thing to use
    the downloaded model, that is, a dataset on which these detections were created.
    Let's get the dataset from [https://motchallenge.net/data/MOT16.zip](https://motchallenge.net/data/MOT16.zip):[](https://motchallenge.net/data/MOT16.zip)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些下载的文件包括使用CNN模型对`MOT challenge`数据集进行预检测的框，CC BY-NC-SA 3.0。为了使用下载的模型，我们还需要一个数据集，这些检测是在该数据集上创建的。让我们从[https://motchallenge.net/data/MOT16.zip](https://motchallenge.net/data/MOT16.zip)获取数据集：[https://motchallenge.net/data/MOT16.zip](https://motchallenge.net/data/MOT16.zip)
- en: 'On macOS:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在macOS上：
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'On Linux:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Linux上：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now that we have finished setting up the code structure, we can run a demo:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了代码结构的设置，我们可以运行一个演示：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this case:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下：
- en: '`--sequence_dir` is the path to the MOT challenge test image sequence'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--sequence_dir`是MOT挑战测试图像序列的路径'
- en: '`--detection_file` is our downloaded pre-generated detection corresponding
    to the sequence directory we chose previously'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--detection_file`是我们下载的与先前选择的序列目录相对应的预生成检测文件'
- en: '`--min_confidence` is the threshold to filter any detection less than this
    value'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--min_confidence`是过滤任何小于此值的检测的阈值'
- en: 'For test sequence MOT16-06, we can see the window which shows video output
    frame-by-frame. Each frame consists of the bounding box around person tracked
    and the number is the ID of the person being tracked. The number updates if a
    new person is detected and follows until the tracking stops.  In the following
    figure, a sample output is explained from the tracking window. For ease of explanation,
    background image is not shown and only tracking boxes are shown:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于测试序列MOT16-06，我们可以看到显示视频输出帧帧的窗口。每一帧都包含跟踪到的人的边界框和数字，该数字是正在跟踪的人的ID。如果检测到新的人，数字会更新，直到跟踪停止。在下面的图中，从跟踪窗口中解释了一个示例输出。为了便于解释，未显示背景图像，只显示了跟踪框：
- en: '![](img/1e15452e-1e3e-4fa2-a7bc-d508958426fb.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1e15452e-1e3e-4fa2-a7bc-d508958426fb.png)'
- en: Readers are encourages to run other test sequences too, like MOT16-07, to further
    understand effectiveness of the model with varying environments.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励读者运行其他测试序列，如MOT16-07，以进一步了解模型在不同环境下的有效性。
- en: In this section, we saw a demo of the Deep SORT method for MOT. One of the crucial
    parts of this method is detection and the use of Faster RCNN as a good detector.
    However, to increase the speed of the overall algorithm, Faster RCNN can also
    be replaced by other fast object detectors such as the Single Shot detector, because
    the rest of the method uses detected box states and not on the feature extraction
    method and features itself.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了用于多目标跟踪（MOT）的Deep SORT方法的演示。这个方法的关键部分之一是检测，以及使用Faster RCNN作为优秀的检测器。然而，为了提高整个算法的速度，Faster
    RCNN也可以被其他快速目标检测器如单次检测器所替代，因为该方法的其他部分使用的是检测到的框状态，而不是特征提取方法和特征本身。
- en: Summary
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, two different computer vision problems were shown. In segmentation,
    both the pixel level as well as convolutional neural net-based methods were shown.
    FCN shows the effectiveness of segmenting an image using the feature extraction
    method and, as a result, several current applications can be based on it. In track,
    two different approaches were discussed. Tracking by detection and tracking by
    matching can both be used for applications to track objects in the video. MOSSE
    tracker is a simple tracker for fast-paced applications and can be implemented
    on small computing devices. The Deep SORT method explained in this chapter can
    be used for multi-object tracking that uses deep CNN object detectors.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，展示了两个不同的计算机视觉问题。在分割方面，展示了像素级以及基于卷积神经网络的方法。FCN展示了使用特征提取方法分割图像的有效性，因此，几个当前的应用程序可以基于它。在跟踪方面，讨论了两种不同的方法。检测跟踪和匹配跟踪都可以用于视频中的对象跟踪应用。MOSSE跟踪器是一个适用于快速应用的简单跟踪器，可以实现在小型计算设备上。本章中解释的Deep
    SORT方法可以用于使用深度CNN对象检测器的多目标跟踪。
- en: In the next chapter, we will begin with another branch of computer vision that
    focuses on understanding geometry of the scene explicitly. We will see methods
    to compute camera position and track its trajectory using only images.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将从计算机视觉的另一个分支开始，该分支专注于显式理解场景的几何结构。我们将看到仅使用图像来计算相机位置和跟踪其轨迹的方法。
- en: References
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bolme David S. J. Ross Beveridge, Bruce A. Draper, and Yui Man Lui. *Visual
    object tracking using adaptive correlation filters*. In Computer Vision and Pattern
    Recognition (CVPR), 2010 IEEE Conference on, pp. 2544-2550\. IEEE, 2010.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bolme David S. J., Ross Beveridge, Bruce A. Draper, 和 Yui Man Lui. *使用自适应相关滤波进行视觉目标跟踪*.
    在《计算机视觉和模式识别》（CVPR）2010 IEEE会议论文集中，第2544-2550页。IEEE，2010。
- en: Wojke, Nicolai, Alex Bewley, and Dietrich Paulus. *Simple Online and Realtime
    Tracking with a Deep Association Metric*. arXiv preprint arXiv:1703.07402 (2017).
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wojke, Nicolai, Alex Bewley, 和 Dietrich Paulus. *使用深度关联度量进行简单在线和实时跟踪*. arXiv预印本
    arXiv:1703.07402（2017）。
- en: Bewley, Alex, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. *Simple
    online and realtime tracking*. In Image Processing (ICIP), 2016 IEEE International
    Conference on, pp. 3464-3468\. IEEE, 2016.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bewley, Alex, Zongyuan Ge, Lionel Ott, Fabio Ramos, 和 Ben Upcroft. *简单在线和实时跟踪*.
    在《图像处理》（ICIP）2016 IEEE国际会议上，第3464-3468页。IEEE，2016。
- en: 'Ren, Shaoqing, Kaiming He, Ross Girshick, and Jian Sun. *Faster R-CNN: Towards
    real-time object detection with region proposal networks*. In Advances in neural
    information processing systems, pp. 91-99\. 2015.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren, Shaoqing, Kaiming He, Ross Girshick, 和 Jian Sun. *Faster R-CNN：使用区域提议网络实现实时目标检测*.
    在《神经信息处理系统进展》中，第91-99页。2015。
- en: Long, Jonathan, Evan Shelhamer, and Trevor Darrell. *Fully convolutional networks
    for semantic segmentation*. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 3431-3440\. 2015.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长江，乔纳森，伊万·谢尔汉默，和特雷弗·达雷尔。*全卷积网络在语义分割中的应用*。载于《IEEE计算机视觉与模式识别会议论文集》，第3431-3440页。2015年。
