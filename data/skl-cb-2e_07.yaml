- en: Cross-Validation and Post-Model Workflow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证和后模型工作流
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Selecting a model with cross-validation
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交叉验证选择模型
- en: K-fold cross-validation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K 折交叉验证
- en: Balanced cross-validation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡交叉验证
- en: Cross-validation with ShuffleSplit
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 ShuffleSplit 的交叉验证
- en: Time series cross-validation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列交叉验证
- en: Grid search with scikit-learn
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 进行网格搜索
- en: Randomized search with scikit-learn
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 进行随机搜索
- en: Classification metrics
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类度量
- en: Regression metrics
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归度量
- en: Clustering metrics
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类度量
- en: Using dummy estimators to compare results
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用虚拟估计器比较结果
- en: Feature selection
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征选择
- en: Feature selection on L1 norms
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1 范数上的特征选择
- en: Persisting models with joblib or pickle
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 joblib 或 pickle 持久化模型
- en: Introduction
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'This is perhaps the most important chapter. The fundamental question addressed
    in this chapter is as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这也许是最重要的章节。本章所探讨的基本问题如下：
- en: How do we select a model that predicts well?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何选择一个预测良好的模型？
- en: This is the purpose of cross-validation, regardless of what the model is. This
    is slightly different from traditional statistics, which is perhaps more concerned
    with how we understand a phenomenon better. (Why would I limit my quest for understanding?
    Well, because there is more and more data, we cannot necessarily look at it all,
    reflect upon it, and create a theoretical model.)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是交叉验证的目的，不管模型是什么。这与传统统计略有不同，传统统计更关心我们如何更好地理解现象。（为什么要限制我对理解的追求？好吧，因为有越来越多的数据，我们不一定能够全部查看、反思并创建理论模型。）
- en: Machine learning is concerned with prediction and how a machine learning algorithm
    processes new unseen data and arrives at predictions. Even if it does not seem
    like traditional statistics, you can use interpretation and domain understanding
    to create new columns (features) and make even better predictions. You can use
    traditional statistics to create new columns.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习关注预测以及机器学习算法如何处理新的未见数据并得出预测。即使它看起来不像传统统计，你可以使用解释和领域理解来创建新列（特征）并做出更好的预测。你可以使用传统统计来创建新列。
- en: Very early in the book, we started with training/testing splits. Cross-validation
    is the iteration of many crucial training and testing splits to maximize prediction
    performance.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 书的早期，我们从训练/测试拆分开始。交叉验证是许多关键训练和测试拆分的迭代，以最大化预测性能。
- en: 'This chapter examines the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨以下内容：
- en: Cross-validation schemes
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证方案
- en: Grid searches—what are the best parameters within an estimator?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网格搜索——在估计器中找到最佳参数是什么？
- en: Metrics that compare `y_test` with `y_pred`—the real target set versus the predicted
    target set
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估指标比较 `y_test` 与 `y_pred` ——真实目标集与预测目标集
- en: 'The following line contains the cross-validation scheme `cv = 10` for the scoring
    mechanism `neg_log_lost`, which is built from the `log_loss` metric:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下面一行包含交叉验证方案 `cv = 10`，用于 `neg_log_lost` 评分机制，该机制由 `log_loss` 指标构建：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Part of the power of scikit-learn is including so much information in a single
    line. Additionally, we will also see a dummy estimator, have a look at feature
    selection, and save trained models. These methods are what really make machine
    learning what it is.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 的一部分力量在于在一行代码中包含如此多的信息。此外，我们还将看到一个虚拟估计器，查看特征选择，并保存训练好的模型。这些方法真正使得机器学习成为它所是的东西。
- en: Selecting a model with cross-validation
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用交叉验证选择模型
- en: We saw automatic cross-validation, the `cross_val_score` function, in [Chapter
    1](9a5af114-e518-47ef-ac63-edf9ae69384c.xhtml), *High-Performance Machine Learning
    – NumPy*. This will be very similar, except we will use the last two columns of
    the iris dataset as the data. The purpose of this section is to select the best
    model we can.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了自动交叉验证，在 [第1章](9a5af114-e518-47ef-ac63-edf9ae69384c.xhtml)，*高性能机器学习 – NumPy*
    中的 `cross_val_score` 函数。这将非常相似，除了我们将使用鸢尾花数据集的最后两列作为数据。本节的目的是选择我们可以选择的最佳模型。
- en: Before starting, we will define the best model as the one that scores the highest.
    If there happens to be a tie, we will choose the model that has the best score
    with the least volatility.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们将定义最佳模型为得分最高的模型。如果出现并列，我们将选择波动最小的模型。
- en: Getting ready
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'In this recipe we will do the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将执行以下操作：
- en: Load the last two features (columns) of the iris dataset
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载鸢尾花数据集的最后两个特征（列）
- en: Split the data into training and testing data
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据拆分为训练数据和测试数据
- en: Instantiate two **k-nearest neighbors** (**KNN**) algorithms, with three and
    five neighbors
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例化两个**k近邻**（**KNN**）算法，分别设置为三个和五个邻居。
- en: Score both algorithms
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对两个算法进行评分
- en: Select the model that scores the best
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择得分最好的模型
- en: 'Start by loading the dataset:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从加载数据集开始：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Split the data into training and testing. The samples are stratified, the default
    throughout the book. Stratified means that the proportions of the target variable
    are the same in both the training and testing sets (also, `random_state` is set
    to `7`):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集和测试集。样本是分层抽样的，书中默认使用这种方法。分层抽样意味着目标变量在训练集和测试集中的比例相同（此外，`random_state`被设置为`7`）：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How to do it...
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'First, instantiate two nearest-neighbor algorithms:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，实例化两个最近邻算法：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, score both algorithms using `cross_val_score`. View `kn_3_scores`, a list
    of scores:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用`cross_val_score`对两个算法进行评分。查看`kn_3_scores`，这是得分的列表：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'View `kn_5_scores`, the other list of scores:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看`kn_5_scores`，这是另一个得分列表：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'View basic statistics of both lists. View the means:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看两个列表的基本统计信息。查看均值：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'View the spreads, looking at the standard deviations:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看分布，查看标准差：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Overall, `kn_5`, when the algorithm is set to five neighbors, performs a little
    better than three neighbors, yet it is less stable (its scores are a bit more
    all over the place).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，当算法设置为五个邻居时，`kn_5`的表现比三个邻居稍好，但它的稳定性较差（它的得分有点分散）。
- en: 'Let''s now do the final step: select the model that scores the highest. We
    select `kn_5` because it scores the highest. (This model has the highest score
    under cross-validation. Note that the scores involved are the nearest neighbors
    default accuracy score: the proportion of correct classifications divided by all
    of the classifications attempted.)'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们进行最后一步：选择得分最高的模型。我们选择`kn_5`，因为它的得分最高。（该模型在交叉验证下得分最高。请注意，涉及的得分是最近邻的默认准确率得分：正确分类的比例除以所有尝试分类的数量。）
- en: How it works...
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'This is an example of 4-fold cross-validation because in the `cross_val_score`
    function, `cv = 4`. We split the training data, or **CV Set** (`X_train`), into
    four parts, or folds. We iterate by rotating each fold as the testing set. At
    first, fold 1 is the testing set while folds 2, 3, and 4 are together the training
    set. Then fold 2 is the testing set while folds 1, 3, and 4 are the training set.
    We do this procedure with folds 3 and 4 as well:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个4折交叉验证的示例，因为在`cross_val_score`函数中，`cv = 4`。我们将训练数据，或**交叉验证集**（`X_train`），拆分为四个部分，或称折叠。我们通过轮流将每个折叠作为测试集来迭代。首先，折叠1是测试集，而折叠2、3和4一起构成训练集。接下来，折叠2是测试集，而折叠1、3和4是训练集。我们还对折叠3和折叠4进行类似的操作：
- en: '![](img/1f4fc2e4-4e98-4165-9467-bafb11dff24c.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f4fc2e4-4e98-4165-9467-bafb11dff24c.png)'
- en: 'Once we split the dataset into folds, we score the algorithm four times:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将数据集拆分为折叠，我们就对算法进行四次评分：
- en: We train one of the nearest neighbors algorithm on folds 2, 3, and 4.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在折叠2、3和4上训练其中一个最近邻算法。
- en: Then we predict on fold 1, the test fold.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们对折叠1进行预测，即测试折。
- en: 'We measure the classification accuracy: compare the test fold with the predicted
    results on that fold. This is the first of the classification scores on the list.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们衡量分类准确性：将测试折与该折的预测结果进行比较。这是列表中第一个分类分数。
- en: The process is performed four times. The final output is a list of four scores.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程执行了四次。最终输出是一个包含四个分数的列表。
- en: Overall, we did the whole process twice, once for `kn_3` and once for `kn_5`,
    and produced two lists to select the best model. The module we imported from is
    called `model_selection` because it is helping us select the best model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，我们进行了整个过程两次，一次用于`kn_3`，一次用于`kn_5`，并生成了两个列表以选择最佳模型。我们从中导入的模块叫做`model_selection`，因为它帮助我们选择最佳模型。
- en: K-fold cross validation
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K折交叉验证
- en: In the quest to find the best model, you can view the indices of cross-validation
    folds and see what data is in each fold.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在寻找最佳模型的过程中，你可以查看交叉验证折叠的索引，看看每个折叠中有哪些数据。
- en: Getting ready
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Create a toy dataset that is very small:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个非常小的玩具数据集：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: How to do it..
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Import `KFold` and select the number of splits:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`KFold`并选择拆分的数量：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can iterate through the generator and print out the indices:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以遍历生成器并打印出索引：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can see, for example, how in the first round there are two testing indices,
    `0` and `1`. `[0 1]` constitutes the first fold. `[2 3 4 5 6 7]` are folds 2,
    3, and 4 put together.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，例如，在第一轮中有两个测试索引，`0`和`1`。`[0 1]`构成了第一个折叠。`[2 3 4 5 6 7]`是折叠2、3和4的组合。
- en: 'You can also view the number of splits:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以查看拆分的次数：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The number of splits is `4`, which we set when we instantiated the `KFold` class.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 分割数为 `4`，这是我们实例化 `KFold` 类时设置的。
- en: There's more...
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'If you want, you can view the data in the folds themselves. Store the generator
    as a list:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果愿意，可以查看折叠数据本身。将生成器存储为列表：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, `indices_list` is a list of tuples. View the information for the fourth
    fold:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`indices_list` 是一个元组的列表。查看第四个折叠的信息：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This information matches the information from the preceding printout, except
    it is in the form of a tuple of two NumPy arrays. View the actual data from the
    fourth fold. View the training data for the fourth fold:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此信息与前面的打印输出信息相匹配，但它以两个 NumPy 数组的元组形式给出。查看第四个折叠的实际数据。查看第四个折叠的训练数据：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'View the test data:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 查看测试数据：
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Balanced cross-validation
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平衡交叉验证
- en: 'While splitting the different folds in various datasets, you might wonder:
    couldn''t the different sets in each fold of k-fold cross-validation be very different?
    The distributions could be very different in each fold, and these differences
    could lead to volatility in the scores.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在将不同折叠中的不同数据集分割时，您可能会想知道：k 折交叉验证中每个折叠中的不同集合可能会非常不同吗？每个折叠中的分布可能会非常不同，这些差异可能导致得分的波动。
- en: There is a solution for this, using stratified cross-validation. The subsets
    of the dataset will look like smaller versions of the whole dataset (at least
    in the target variable).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对此有一个解决方案，使用分层交叉验证。数据集的子集看起来像整个数据集的较小版本（至少在目标变量方面）。
- en: Getting ready
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Create a toy dataset as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个玩具数据集如下：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How to do it...
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'If we perform 4-fold cross-validation on this miniature toy dataset, each of
    the four testing folds will have only one value for the target. This can be remedied
    using `StratifiedKFold`:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们在这个小型玩具数据集上执行4折交叉验证，每个测试折叠将只有一个目标值。可以使用 `StratifiedKFold` 来解决这个问题：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Print out the indices of the folds:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出折叠的索引：
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Observe that the `split` method of the `skf` class, the stratified k-fold split,
    has two arguments, `X` and `y`. It tries to distribute the target `y` with the
    same distribution in each of the fold sets. In this case, every subset has 50%
    `1` and 50% `2`, just like the whole target set `y`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 观察 `skf` 类的 `split` 方法，即分层 k 折叠分割，具有两个参数 `X` 和 `y`。它试图在每个折叠集中以相同的分布分配目标 `y`。在这种情况下，每个子集都有50%
    的 `1` 和 50% 的 `2`，就像整个目标集 `y` 一样。
- en: There's more...
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'You can use `StratifiedShuffleSplit` to reshuffle the stratified fold. Note
    that this does not try to make four folds with testing sets that are mutually
    exclusive:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `StratifiedShuffleSplit` 重新洗牌分层折叠。请注意，这并不会尝试制作具有互斥测试集的四个折叠：
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The splits are not splits of the dataset but iterations of a random procedure,
    each one with a training set size of 75% of the whole dataset and a testing set
    size of 25%. All of the iterations are stratified.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分割不是数据集的分割，而是随机过程的迭代，每个迭代的训练集大小为整个数据集的75%，测试集大小为25%。所有迭代都是分层的。
- en: Cross-validation with ShuffleSplit
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ShuffleSplit 的交叉验证
- en: The ShuffleSplit is one of the simplest cross-validation techniques. Using this
    cross-validation technique will simply take a sample of the data for the number
    of iterations specified.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ShuffleSplit 是最简单的交叉验证技术之一。使用这种交叉验证技术只需取数据的样本，指定的迭代次数。
- en: Getting ready
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: The ShuffleSplit is a simple validation technique. We'll specify the total elements
    in the dataset, and it will take care of the rest. We'll walk through an example
    of estimating the mean of a univariate dataset. This is similar to resampling,
    but it'll illustrate why we want to use cross-validation while showing cross-validation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ShuffleSplit 是一种简单的验证技术。我们将指定数据集中的总元素数量，其余由它来处理。我们将通过估计单变量数据集的均值来示例化。这类似于重新采样，但它将说明为什么我们要在展示交叉验证时使用交叉验证。
- en: How to do it...
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'First, we need to create the dataset. We''ll use NumPy to create a dataset
    in which we know the underlying mean. We''ll sample half of the dataset to estimate
    the mean and see how close it is to the underlying mean. Generate a normally distributed
    random sample with a mean of 1,000 and a scale (standard deviation) of 10:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要创建数据集。我们将使用 NumPy 创建一个数据集，其中我们知道底层均值。我们将对数据集的一半进行采样以估计均值，并查看它与底层均值的接近程度。生成一个均值为1000，标准差为10的正态分布随机样本：
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/201288c4-5c4a-4e64-bb84-2d2395182bc7.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/201288c4-5c4a-4e64-bb84-2d2395182bc7.png)'
- en: 'Estimate the mean of half of the dataset:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 估计数据集的一半的平均值：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You can also get the mean of the whole dataset:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你也可以获取整个数据集的均值：
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'It is not 1,000 because random points were selected to create the dataset.
    To observe the behavior of `ShuffleSplit`, write the following and make a plot:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它不是 1,000，因为随机选择了点来创建数据集。为了观察 `ShuffleSplit` 的行为，写出以下代码并绘制图表：
- en: '[PRE23]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](img/7de5d845-f9e0-4854-ba09-c8bc4998efea.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7de5d845-f9e0-4854-ba09-c8bc4998efea.png)'
- en: The estimated mean keeps getting closer to the data's mean of 999.55177343767843
    and then plateaus at being 0.1 away from the data's mean. It is a bit closer than
    the estimate of the mean, with half the dataset, to the mean of the data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 估计的均值不断接近数据的均值 999.55177343767843，并在距离数据均值 0.1 时停滞。它比仅用一半数据估算出的均值更接近数据的均值。
- en: Time series cross-validation
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列交叉验证
- en: scikit-learn can perform cross-validation for time series data such as stock
    market data. We will do so with a time series split, as we would like the model
    to predict the future, not have an information data leak from the future.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 可以对时间序列数据（例如股市数据）进行交叉验证。我们将使用时间序列拆分，因为我们希望模型能够预测未来，而不是从未来泄漏信息。
- en: Getting ready
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will create the indices for a time series split. Start by creating a small
    toy dataset:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为时间序列拆分创建索引。首先创建一个小的玩具数据集：
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: How to do it...
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Now create a time series split object:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在创建一个时间序列拆分对象：
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Iterate through it:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历它：
- en: '[PRE26]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You can also save the indices by creating a list of tuples from the generator:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你也可以通过创建一个包含元组的列表来保存索引：
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: There's more...
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: You can create rolling windows with NumPy or pandas as well. The main requirement
    of time series cross-validation is that the test set appears after the training
    set in time; otherwise, you would be predicting the past from the future.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用 NumPy 或 pandas 创建滚动窗口。时间序列交叉验证的主要要求是测试集必须出现在训练集之后；否则，你就会从未来预测过去。
- en: Time series cross-validation is interesting because depending on the dataset,
    the influence of time varies. Sometimes, you do not have to put data rows in time-sequential
    order, yet you can never assume you know the future in the past.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列交叉验证很有趣，因为根据数据集的不同，时间的影响是不同的。有时，你不需要将数据行按时间顺序排列，但你永远不能假设你知道过去的未来。
- en: Grid search with scikit-learn
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 进行网格搜索
- en: At the beginning of the model selection and cross-validation chapter we tried
    to select the best nearest-neighbor model for the two last features of the iris
    dataset. We will refocus on that now with `GridSearchCV` in scikit-learn.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型选择和交叉验证章节的开头，我们尝试为鸢尾花数据集的最后两个特征选择最佳的最近邻模型。现在，我们将使用 `GridSearchCV` 在 scikit-learn
    中重新聚焦这一点。
- en: Getting ready
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'First, load the last two features of the iris dataset. Split the data into
    training and testing sets:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载鸢尾花数据集的最后两个特征。将数据拆分为训练集和测试集：
- en: '[PRE28]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: How to do it...
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Instantiate a nearest neighbors classifier:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个最近邻分类器：
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Prepare a parameter grid, which is necessary for a grid search. A parameter
    grid is a dictionary with the parameter setting you would like to try:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备一个参数网格，这是网格搜索所必需的。参数网格是一个字典，包含你希望尝试的参数设置：
- en: '[PRE30]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Instantiate a grid search passing the following as arguments:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个网格搜索，传入以下参数：
- en: The estimator
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计器
- en: The parameter grid
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数网格
- en: A type of cross-validation, `cv=10`
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种交叉验证方法，`cv=10`
- en: '[PRE31]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Fit the grid search estimator:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合网格搜索估计器：
- en: '[PRE32]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'View the results:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看结果：
- en: '[PRE33]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: How it works...
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In the first chapter, we tried the brute force method, that is, scanning for
    the best score with Python:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们尝试了蛮力法，即使用 Python 扫描最佳得分：
- en: '[PRE34]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The problem with this approach is that it is more time consuming and error prone,
    especially if there are more parameters involved or additional transformations,
    such as those involved in using pipelines.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的问题是，它更加耗时且容易出错，尤其是当涉及更多参数或额外的转换（如使用管道时）时。
- en: Note that the grid search and brute force approaches both scan all of the possible
    values of the parameters.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，网格搜索和蛮力法方法都会扫描所有可能的参数值。
- en: Randomized search with scikit-learn
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 进行随机搜索
- en: From a practical standpoint, `RandomizedSearchCV` is more important than a regular
    grid search. This is because with a medium amount of data, or with a model involving
    a few parameters, it is too computationally expensive to try every parameter combination
    involved in a complete grid search.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际角度来看，`RandomizedSearchCV` 比常规网格搜索更为重要。因为对于中等大小的数据，或者涉及少量参数的模型，进行完整网格搜索的所有参数组合计算开销太大。
- en: Computational resources are probably better spent stratifying sampling very
    well, or improving randomization procedures.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 计算资源最好用于非常好地分层采样，或者改进随机化过程。
- en: Getting ready
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'As before, load the last two features of the iris dataset. Split the data into
    training and testing sets:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，加载鸢尾花数据集的最后两个特征。将数据拆分为训练集和测试集：
- en: '[PRE35]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: How to do it...
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Instantiate a nearest neighbors classifier:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个最近邻分类器：
- en: '[PRE36]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Prepare a parameter distribution, which is necessary for a randomized grid
    search. A parameter distribution is a dictionary with the parameter setting you
    would like to try randomly:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备一个参数分布，这是进行随机网格搜索时必需的。参数分布是一个字典，包含你希望随机尝试的参数设置：
- en: '[PRE37]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Instantiate a randomized grid search passing the following as arguments:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个随机网格搜索并传入以下参数：
- en: The estimator
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估算器
- en: The parameter distribution
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数分布
- en: A type of cross-validation, `cv=10`
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种交叉验证类型，`cv=10`
- en: The number of times to run the procedure, `n_iter`
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行此过程的次数，`n_iter`
- en: '[PRE38]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Fit the randomized grid search estimator:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合随机网格搜索估算器：
- en: '[PRE39]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'View the results:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看结果：
- en: '[PRE40]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In this case, we actually ran a grid search through all six of the parameters.
    You could have scanned a larger parameter space, however:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种情况下，我们实际上对所有六个参数进行了网格搜索。你本可以扫描更大的参数空间：
- en: '[PRE41]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Try timing this procedure with IPython:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用 IPython 计时此过程：
- en: '[PRE42]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Time the grid search procedure:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计时网格搜索过程：
- en: '[PRE43]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Look at the grid search''s best parameters:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看网格搜索的最佳参数：
- en: '[PRE44]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'It turns out that 3-nearest neighbors scores the same as 16-nearest neighbors:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果表明，3-最近邻的得分与16-最近邻相同：
- en: '[PRE45]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Thus, we got the same score in one-third of the time.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在三分之一的时间内得到了相同的分数。
- en: Whether to use randomized search or not is a decision you have to make on a
    case-by-case basis. You should use a randomized search to try to get a feel for
    an algorithm. It is possible that it performs poorly no matter what the parameters
    are, so then you can move on to a different algorithm. If the algorithm performs
    very well, you can use a complete grid search to find the best parameters.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 是否使用随机搜索，这是你需要根据具体情况做出的决定。你应该使用随机搜索来尝试了解某个算法的表现。可能无论参数如何，算法的表现都很差，这时你可以换一个算法。如果算法表现非常好，可以使用完整的网格搜索来寻找最佳参数。
- en: Additionally, instead of focusing on exhaustive searches, you could bag, stack,
    or mix a set of reasonably good algorithms.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，除了专注于穷举搜索，你还可以通过集成、堆叠或混合一组合理表现良好的算法来进行尝试。
- en: Classification metrics
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类指标
- en: 'Earlier in the chapter, we explored choosing the best of a few nearest neighbors
    instances based on the number of neighbors, `n_neighbors`, parameter. This is
    the main parameter in nearest neighbors classification: classify a point based
    on the label of KNN. So, for 3-nearest neighbors, classify a point based on the
    label of the three nearest points. Take a majority vote of the three nearest points.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章早些时候，我们探讨了基于邻居数量 `n_neighbors` 参数选择几个最近邻实例的最佳方法。这是最近邻分类中的主要参数：基于 KNN 的标签对一个点进行分类。所以，对于
    3-最近邻，根据三个最近点的标签对一个点进行分类。对这三个最近点进行多数投票。
- en: The classification metric in this case was the internal metric `accuracy_score`,
    which is defined as the number of classifications that were correct divided by
    the total number of classifications. There are alternate metrics, and we will
    explore them here.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 该分类指标在此案例中是内部指标 `accuracy_score`，定义为正确分类的数量除以分类总数。还有其他指标，我们将在这里进行探讨。
- en: Getting ready
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'To start, load the Pima diabetes dataset from the UCI repository:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从 UCI 数据库加载 Pima 糖尿病数据集：
- en: '[PRE46]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Split the data into training and testing sets:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集和测试集：
- en: '[PRE47]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'To review the previous section, run a randomized search using the KNN algorithm:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回顾上一部分，使用 KNN 算法运行随机搜索：
- en: '[PRE48]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Then display the best accuracy score:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后显示最佳准确率得分：
- en: '[PRE49]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Additionally, look at the confusion matrix on the test set:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，查看测试集上的混淆矩阵：
- en: '[PRE50]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The confusion matrix gives more specific information about how the model performed.
    There were 27 times when the model predicted someone did not have diabetes even
    though they did. This is a more serious mistake than the 16 people thought to
    have diabetes that really did not.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵提供了更具体的关于模型表现的信息。有 27 次模型预测某人没有糖尿病，尽管他们实际上有。这比 16 个被认为有糖尿病的人实际上没有糖尿病的错误更为严重。
- en: 'In this situation, we want to maximize sensitivity or recall. While examining
    linear models, we looked at the definition of recall, or sensitivity:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们希望最大化灵敏度或召回率。在检查线性模型时，我们查看了召回率或灵敏度的定义：
- en: '![](img/f1a676a1-db39-4460-b560-18ba206d5bd6.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1a676a1-db39-4460-b560-18ba206d5bd6.png)'
- en: Thus, the sensitivity score in this case is 27/ (27 + 27) = 0.5\. With scikit-learn,
    we can conveniently compute this as follows.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，灵敏度得分为 27/ (27 + 27) = 0.5。使用 scikit-learn，我们可以方便地按如下方式计算此值。
- en: How to do it...
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Import from the metrics module `recall_score`. Measure the sensitivity of the
    set using `y_test` and `y_pred`:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 metrics 模块导入 `recall_score`。使用 `y_test` 和 `y_pred` 测量集合的灵敏度：
- en: '[PRE51]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We recovered the recall score we computed by hand beforehand. In the randomized
    search, we could have used the `recall_score` to find the nearest neighbor instance
    with the highest recall.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们恢复了之前手动计算的召回得分。在随机搜索中，我们本可以使用 `recall_score` 来找到具有最高召回率的最近邻实例。
- en: 'Import `make_scorer` and use the function with two arguments, `recall_score`
    and `greater_is_better`:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `make_scorer` 并使用带有两个参数的函数 `recall_score` 和 `greater_is_better`：
- en: '[PRE52]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now perform a randomized grid search:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在执行随机网格搜索：
- en: '[PRE53]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now look at the highest score:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在查看最高得分：
- en: '[PRE54]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Look at the recall score:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看召回得分：
- en: '[PRE55]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'It is the same as before. In the randomized search you could have tried the
    `roc_auc_score`, the ROC area under the curve:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果与之前相同。在随机搜索中，你本可以尝试 `roc_auc_score`，即曲线下面积（ROC AUC）：
- en: '[PRE56]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: There's more...
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'You could design your own scorer for classification. Suppose that you are an
    insurance company and that you have associated costs for each cell in the confusion
    matrix. The relative costs are as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以为分类设计自己的评分器。假设你是一个保险公司，并且你为混淆矩阵中的每个单元格分配了成本。相对成本如下：
- en: '![](img/b8334234-cca1-421f-8d45-e157d3765ba8.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8334234-cca1-421f-8d45-e157d3765ba8.png)'
- en: 'The cost for the confusion matrix we were looking at can be computed as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在查看的混淆矩阵的成本可以按如下方式计算：
- en: '[PRE57]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now add up the total cost:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在加总总成本：
- en: '[PRE58]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now place it in a scorer and run the grid search. The argument within the scorer
    `greater_is_better` is set to `False`, because costs should be as low as possible:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将其放入评分器中并运行网格搜索。评分器中的参数 `greater_is_better` 设置为 `False`，因为成本应尽可能低：
- en: '[PRE59]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The score is negative because when the `greater_is_better` argument in the `make_scorer`
    function is false, the score is multiplied by `-1`. The grid search attempts to
    maximize this score, thereby minimizing the absolute value of the score.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 得分为负，因为当 `make_scorer` 函数中的 `greater_is_better` 参数为 false 时，得分会乘以 `-1`。网格搜索试图最大化该得分，从而最小化得分的绝对值。
- en: 'The cost on the test set is as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集的成本如下：
- en: '[PRE60]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: While looking at this number, do not forget to look at the number of individuals
    involved in the test set, which is 154\. The average cost per person is about
    $21.8.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看这个数字时，别忘了查看测试集中涉及的个体数量，共有 154 人。每个人的平均成本约为 21.8 美元。
- en: Regression metrics
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归指标
- en: Cross-validation with a regression metric is straightforward with scikit-learn.
    Either import a score function from `sklearn.metrics` and place it within a `make_scorer`
    function, or you could create a custom scorer for a particular data science problem.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 使用回归指标的交叉验证在 scikit-learn 中非常简单。可以从 `sklearn.metrics` 导入评分函数并将其放入 `make_scorer`
    函数中，或者你可以为特定的数据科学问题创建自定义评分器。
- en: Getting ready
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Load a dataset that utilizes a regression metric. We will load the Boston housing
    dataset and split it into training and test sets:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 加载一个使用回归指标的数据集。我们将加载波士顿房价数据集并将其拆分为训练集和测试集：
- en: '[PRE61]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We do not know much about the dataset. We can try a quick grid search using
    a high variance algorithm:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对数据集了解不多。我们可以尝试使用高方差算法进行快速网格搜索：
- en: '[PRE62]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Try a different model, this time a linear model:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试一个不同的模型，这次是一个线性模型：
- en: '[PRE63]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Both regressors, by default, measure `r2_score`, R-squared, so the linear model
    is far better. Try a different complex model, an ensemble of trees:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，两个回归器都衡量 `r2_score`，即 R 平方，因此线性模型更好。尝试一个不同的复杂模型，一个树的集成：
- en: '[PRE64]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The ensemble performs even better. You can try a random forest as well:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型的表现更好。你也可以尝试随机森林：
- en: '[PRE65]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now we could focus on making gradient boosting better with the current score
    mechanism by maximizing the internal R-squared gradient boosting scorer. Try one
    or two randomized searches. This is a second one:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过最大化内部 R-squared 梯度提升评分器，专注于利用当前评分机制来改进梯度提升。尝试进行一两次随机搜索。这是第二次搜索：
- en: '[PRE66]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Optimizing for R-squared returned the following:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 为 R-squared 优化返回了以下结果：
- en: '[PRE67]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The trees in the gradient boost have a depth of three.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升中的树的深度为三。
- en: How to do it...
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'Now we will do the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将执行以下操作：
- en: Make a scoring function.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个评分函数。
- en: Make a scikit-scorer with that function.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用该函数创建一个 scikit-scorer。
- en: Run a grid search to find the best gradient boost parameters to minimize the
    error function.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行网格搜索以找到最佳的梯度提升参数，最小化误差函数。
- en: 'Let''s start:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始：
- en: 'Make the mean percentage error scoring function with the Numba **just-in-time**
    (**JIT**) compiler. The original NumPy function looks like this:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Numba **即时编译**（**JIT**）编译器创建平均百分比误差评分函数。原始的 NumPy 函数如下：
- en: '[PRE68]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Let''s rewrite the function using the Numba JIT compiler to make things a bit
    faster. You can write C-like code, indexing arrays by location using Numba:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用 Numba JIT 编译器重写这个函数，稍微加速一些。你可以用类似 C 的代码，通过 Numba 按位置索引数组：
- en: '[PRE69]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now make a scorer. The lower the score the better, unlike R-squared, in which
    higher is better:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在创建一个评分器。得分越低越好，不像 R-squared，那里的得分越高越好：
- en: '[PRE70]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Now run a grid search:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在进行网格搜索：
- en: '[PRE71]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Using this metric, the best score corresponds to a gradient boost with trees
    of depth one.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此度量，最佳得分对应于深度为 1 的梯度提升树。
- en: Clustering metrics
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类度量
- en: Measuring the performance of a clustering algorithm is a little trickier than
    classification or regression, because clustering is unsupervised machine learning.
    Thankfully, scikit-learn comes equipped to help us with this as well in a very
    straightforward manner.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量聚类算法的性能比分类或回归要复杂一些，因为聚类是无监督机器学习。幸运的是，scikit-learn 已经非常直接地为我们提供了帮助。
- en: Getting ready
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'To measure clustering performance, start by loading the iris dataset. We will
    relabel the iris flowers as two types: type 0 is whenever the target is 0 and
    type 1 is when the target is 1 or 2:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量聚类性能，首先加载鸢尾花数据集。我们将鸢尾花重新标记为两种类型：当目标是 0 时为类型 0，当目标是 1 或 2 时为类型 1：
- en: '[PRE72]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: How to do it...
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'Instantiate a k-means algorithm and train it. Since the algorithm is a clustering
    one, do not use the target in the training:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个 k-means 算法并训练它。由于该算法是聚类算法，因此在训练时不要使用目标值：
- en: '[PRE73]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Now import everything necessary to score k-means through cross-validation.
    We will use the `adjusted_rand_score` clustering performance metric:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在导入所有必要的库，通过交叉验证对 k-means 进行评分。我们将使用 `adjusted_rand_score` 聚类性能指标：
- en: '[PRE74]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Scoring a clustering algorithm is very similar to scoring a classification algorithm.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 评估聚类算法与评估分类算法非常相似。
- en: Using dummy estimators to compare results
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用虚拟估算器进行结果比较
- en: This recipe is about creating fake estimators; this isn't the pretty or exciting
    stuff, but it is worthwhile having a reference point for the model you'll eventually
    build.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱是关于创建虚拟估算器的；这不是很华丽或令人兴奋的部分，但它为你最终构建的模型提供了一个参考点，值得一做。
- en: Getting ready
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'In this recipe, we''ll perform the following tasks:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将执行以下任务：
- en: Create some random data.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一些随机数据。
- en: Fit the various dummy estimators.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合各种虚拟估算器。
- en: We'll perform these two steps for regression data and classification data.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对回归数据和分类数据执行这两步操作。
- en: How to do it...
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'First, we''ll create the random data:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将创建随机数据：
- en: '[PRE75]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'By default, the estimator will predict by just taking the mean of the values
    and outputting it multiple times::'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，估算器将通过取值的均值并多次输出它来进行预测：
- en: '[PRE76]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: There are other two other strategies we can try. We can predict a supplied constant
    (refer to `constant=None` in the first command block in this recipe). We can also
    predict the median value. Supplying a constant will only be considered if strategy
    is constant.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 还有两种其他策略可以尝试。我们可以预测一个提供的常量（参考此食谱中的第一条命令块中的 `constant=None`）。我们还可以预测中位数值。仅当策略为常量时才会考虑提供常量。
- en: 'Let''s have a look:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们来看看：
- en: '[PRE77]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'We actually have four options for classifiers. These strategies are similar
    to the continuous case, it''s just slanted toward classification problems:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们实际上有四种分类器的选择。这些策略与连续情况类似，只不过更加倾向于分类问题：
- en: '[PRE78]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: How it works...
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: It's always good to test your models against the simplest models, and that's
    exactly what the dummy estimators give you. For example, imagine a fraud model.
    In this model, only 5% of the dataset is fraudulent. Therefore, we can probably
    fit a pretty good model just by never guessing that the data is fraudulent.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 测试你的模型是否能够比最简单的模型表现更好总是个不错的做法，这正是虚拟估计器所能提供的。例如，假设你有一个欺诈检测模型。在这个模型中，数据集中只有5%是欺诈行为。因此，我们很可能通过仅仅不猜测数据是欺诈的，就能拟合出一个相当不错的模型。
- en: 'We can create this model by using the stratified strategy using the following
    command. We can also get a good example of why class imbalance causes problems:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用分层策略并执行以下命令来创建此模型。我们还可以得到一个很好的例子，说明类别不平衡是如何导致问题的：
- en: '[PRE79]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: We were actually correct very often, but that's not the point. The point is
    that this is our baseline. If we cannot create a model for fraud that is more
    accurate than this, then it isn't worth our time.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上经常是正确的，但这并不是重点。重点是这是我们的基准。如果我们不能创建一个比这个更准确的欺诈检测模型，那么就不值得花费时间。
- en: Feature selection
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择
- en: This recipe, along with the two following it, will be centered around automatic
    feature selection. I like to think of this as the feature analog of parameter
    tuning. In the same way that we cross-validate to find an appropriately general
    parameter, we can find an appropriately general subset of features. This will
    involve several different methods.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱以及接下来的两个将围绕自动特征选择展开。我喜欢将其视为参数调优的特征类比。就像我们通过交叉验证来寻找合适的参数一样，我们也可以找到一个合适的特征子集。这将涉及几种不同的方法。
- en: The simplest idea is univariate selection. The other methods involve working
    with a combination of features.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的想法是单变量选择。其他方法则涉及特征的组合使用。
- en: An added benefit of feature selection is that it can ease the burden on the
    data collection. Imagine that you have built a model on a very small subset of
    the data. If all goes well, you might want to scale up to predict the model on
    the entire subset of data. If this is the case, you can ease the engineering effort
    of data collection at that scale.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择的一个附加好处是，它可以减轻数据收集的负担。假设你已经基于一个非常小的数据子集建立了一个模型。如果一切顺利，你可能想扩大规模，在整个数据子集上预测模型。如果是这种情况，你可以在这个规模上减轻数据收集的工程负担。
- en: Getting ready
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: With univariate feature selection, scoring functions will come to the forefront
    again. This time, they will define the comparable measure by which we can eliminate
    features.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单变量特征选择时，评分函数将再次成为焦点。这一次，它们将定义我们可以用来消除特征的可比度量。
- en: 'In this recipe, we''ll fit a regression model with around 10,000 features,
    but only 1,000 points. We''ll walk through the various univariate feature selection
    methods:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将拟合一个包含大约10,000个特征的回归模型，但只有1,000个数据点。我们将逐步了解各种单变量特征选择方法：
- en: '[PRE80]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Now that we have the data, we will compare the features that are included with
    the various methods. This is actually a very common situation when you're dealing
    with text analysis or some areas of bioinformatics.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了数据，我们将比较通过各种方法包含的特征。这实际上是你在处理文本分析或某些生物信息学领域时非常常见的情况。
- en: How to do it...
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'First, we need to import the `feature_selection` module:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要导入`feature_selection`模块：
- en: '[PRE81]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Here, `f` is the `f` score associated with each linear model fit with just
    one of the features. We can then compare these features and based on this comparison,
    we can cull features. `p` is the `p` value associated with the `f` value. In statistics,
    the `p` value is the probability of a value more extreme than the current value
    of the test statistic. Here, the `f` value is the test statistic:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里，`f`是与每个线性模型拟合相关的`f`分数，该模型仅使用一个特征。然后我们可以比较这些特征，并根据这种比较来剔除特征。`p`是与`f`值相关的`p`值。在统计学中，`p`值是指在给定的检验统计量值下，出现比当前值更极端的结果的概率。在这里，`f`值是检验统计量：
- en: '[PRE82]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'As we can see, many of the `p` values are quite large. We want the `p` values
    to be quite small. So, we can grab NumPy out of our toolbox and choose all the
    `p` values less than `.05`. These will be the features we''ll use for our analysis:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们所见，许多`p`值相当大。我们希望`p`值尽可能小。因此，我们可以从工具箱中取出NumPy，选择所有小于`.05`的`p`值。这些将是我们用于分析的特征：
- en: '[PRE83]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: As you can see, we're actually keeping a relatively large number of features.
    Depending on the context of the model, we can tighten this `p` value. This will
    lessen the number of features kept.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们实际上保留了相对较多的特征。根据模型的上下文，我们可以缩小这个`p`值。这将减少保留的特征数量。
- en: Another option is using the `VarianceThreshold` object. We've learned a bit
    about it, but it's important to understand that our ability to fit models is largely
    based on the variance created by features. If there is no variance, then our features
    cannot describe the variation in the dependent variable. A nice feature of this,
    as per the documentation, is that because it does not use the outcome variable,
    it can be used for unsupervised cases.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是使用`VarianceThreshold`对象。我们已经了解了一些它的内容，但需要明白的是，我们拟合模型的能力在很大程度上依赖于特征所产生的方差。如果没有方差，那么我们的特征就无法描述因变量的变化。根据文档的说法，它的一个优点是由于它不使用结果变量，因此可以用于无监督的情况。
- en: 'We will need to set the threshold for which we eliminate features. In order
    to do that, we just take the median of the feature variances and supply that:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要设定一个阈值，以决定去除哪些特征。为此，我们只需要取特征方差的中位数并提供它：
- en: '[PRE84]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: As we can see, we have eliminated roughly half the features, more or less what
    we would expect.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们已经去除了大约一半的特征，这也大致符合我们的预期。
- en: How it works...
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In general, all these methods work by fitting a basic model with a single feature.
    Depending on whether we have a classification problem or a regression problem,
    we can use the appropriate scoring function.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，这些方法都是通过拟合一个只有单一特征的基本模型来工作的。根据我们是分类问题还是回归问题，我们可以使用相应的评分函数。
- en: 'Let''s look at a smaller problem and visualize how feature selection will eliminate
    certain features. We''ll use the same scoring function from the first example,
    but just 20 features:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个较小的问题，并可视化特征选择如何去除某些特征。我们将使用第一个示例中的相同评分函数，但只使用20个特征：
- en: '[PRE85]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Now let''s plot the p values of the features. We can see which features will
    be eliminated and which will be kept:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们绘制特征的p值。我们可以看到哪些特征将被去除，哪些特征将被保留：
- en: '[PRE86]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '![](img/a8f4c0f9-e43a-4714-8c0b-05890091b2d1.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8f4c0f9-e43a-4714-8c0b-05890091b2d1.png)'
- en: As we can see, many of the features won't be kept, but several will be.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，许多特征将不会被保留，但有一些特征会被保留。
- en: Feature selection on L1 norms
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于L1范数的特征选择
- en: We're going to work with some ideas that are similar to those we saw in the
    recipe on LASSO regression. In that recipe, we looked at the number of features
    that had zero coefficients. Now we're going to take this a step further and use
    the sparseness associated with L1 norms to pre-process the features.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一些与LASSO回归配方中看到的类似的思想。在那个配方中，我们查看了具有零系数的特征数量。现在我们将更进一步，利用与L1范数相关的稀疏性来预处理特征。
- en: Getting ready
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: We'll use the diabetes dataset to fit a regression. First, we'll fit a basic
    linear regression model with a ShuffleSplit cross-validation. After we do that,
    we'll use LASSO regression to find the coefficients that are zero when using an
    L1 penalty. This hopefully will help us to avoid overfitting (when the model is
    too specific to the data it was trained on). To put this another way, the model,
    if it overfits, does not generalize well to outside data.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用糖尿病数据集来进行回归拟合。首先，我们将使用ShuffleSplit交叉验证拟合一个基本的线性回归模型。完成后，我们将使用LASSO回归来找到系数为零的特征，这些特征在使用L1惩罚时会被去除。这有助于我们避免过拟合（即模型过于专门化，无法适应它未训练过的数据）。换句话说，如果模型过拟合，它对外部数据的泛化能力较差。
- en: 'We''re going to perform the following steps:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行以下步骤：
- en: Load the dataset.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集。
- en: Fit a basic linear regression model.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合一个基本的线性回归模型。
- en: Use feature selection to remove uninformative features.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用特征选择去除不具信息量的特征。
- en: Refit the linear regression and check to see how well it fits compared with
    the fully featured model.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新拟合线性回归模型，并检查它与完全特征模型相比的拟合效果。
- en: How to do it...
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'First, let''s get the dataset:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们获取数据集：
- en: '[PRE87]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Let''s create the `LinearRegression` object:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建`LinearRegression`对象：
- en: '[PRE88]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Let''s also import from the metrics module the `mean_squared_error` function
    and `make_scorer` wrapper. From the `model_selection` module, import the `ShuffleSplit`
    cross-validation scheme and the `cross_val_score` cross-validation scorer. Go
    ahead and score the function with the `mean_squared_error` metric:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从metrics模块导入`mean_squared_error`函数和`make_scorer`包装器。从`model_selection`模块，导入`ShuffleSplit`交叉验证方案和`cross_val_score`交叉验证评分器。接下来，使用`mean_squared_error`度量来评分该函数：
- en: '[PRE89]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'So now that we have the regular fit, let''s check it after eliminating any
    features with a zero for the coefficient. Let''s fit the LASSO regression:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经有了常规拟合，让我们在去除系数为零的特征后检查一下。让我们拟合LASSO回归：
- en: '[PRE90]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'We''ll remove the first feature. I''ll use a NumPy array to represent the columns
    that are to be included in the model:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将删除第一个特征。我将使用NumPy数组来表示要包含在模型中的列：
- en: '[PRE91]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Okay, so now we''ll fit the model with the specific features (see the columns
    in the following code block):'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 好的，现在我们将使用特定的特征来拟合模型（请参见以下代码块中的列）：
- en: '[PRE92]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: The score afterwards is not much better than the score before, even though we
    eliminated an uninformative feature. We will see an additional example in the
    *There's more...* section.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 之后的得分并没有比之前好多少，尽管我们已经消除了一个无信息特征。在*还有更多内容...*部分，我们将看到一个额外的示例。
- en: There's more...
  id: totrans-356
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'First, we''re going to create a regression dataset with many uninformative
    features:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个具有许多无信息特征的回归数据集：
- en: '[PRE93]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Create a `ShuffleSplit` instance with 10 iterations, `n_splits=10`. Measure
    the cross-validation score of plain linear regression:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`ShuffleSplit`实例，进行10次迭代，`n_splits=10`。测量普通线性回归的交叉验证得分：
- en: '[PRE94]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Instantiate `LassoCV` to eliminate uninformative columns:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化`LassoCV`来消除无信息的列：
- en: '[PRE95]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Eliminate uninformative columns. Look at the final scores:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 消除无信息的列。查看最终得分：
- en: '[PRE96]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: The fit is a lot better at the end after we removed uninformative features.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们移除无信息特征后，最后的拟合效果要好得多。
- en: Persisting models with joblib or pickle
  id: totrans-366
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用joblib或pickle持久化模型
- en: In this recipe, we're going to show how you can keep your model around for later
    use. For example, you might want to actually use a model to predict an outcome
    and automatically make a decision.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将展示如何将模型保留以供以后使用。例如，你可能希望使用一个模型来预测结果并自动做出决策。
- en: Getting ready
  id: totrans-368
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 做好准备
- en: 'Create a dataset and train a classifier:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 创建数据集并训练分类器：
- en: '[PRE97]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: How to do it...
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Save the training work the classifier has done with joblib:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用joblib保存分类器所做的训练工作：
- en: '[PRE98]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: Opening the saved model
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 打开已保存的模型
- en: 'Load the model with joblib. Make a prediction with a set of inputs:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用joblib加载模型。使用一组输入进行预测：
- en: '[PRE99]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: We did not have to train the model again, and have saved a lot of training time.
    We simply reloaded it with joblib and made a prediction.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要重新训练模型，并且节省了大量训练时间。我们只是使用joblib重新加载了模型并进行了预测。
- en: There's more...
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'You can also use the `cPickle` module in Python 2.x or the `pickle` module
    in Python 3.x. Personally, I use this module for several types of Python classes
    and objects:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在Python 2.x中使用`cPickle`模块，或者在Python 3.x中使用`pickle`模块。就个人而言，我使用这个模块处理几种类型的Python类和对象：
- en: 'Begin by importing `pickle`:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先导入`pickle`：
- en: '[PRE100]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Use the `dump()` module method. It has three arguments: the data being saved,
    the file it is being saved to, and the pickle protocol. The following saves the
    trained tree to the `dtree.save` file:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`dump()`模块方法。它有三个参数：要保存的数据、保存目标文件和pickle协议。以下代码将训练好的树保存到`dtree.save`文件：
- en: '[PRE101]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Open `dtree.save` as follows:'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式打开`dtree.save`文件：
- en: '[PRE102]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'View the tree:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看树：
- en: '[PRE103]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
