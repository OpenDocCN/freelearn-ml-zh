- en: '*Chapter 6*: Advanced Model Building – Part II'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第六章*：高级建模 – 第二部分'
- en: 'In the previous chapter, [*Chapter 5*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Advanced Model Building – Part I*, we detailed the process for building an enterprise-grade
    **supervised learning** model on the H2O platform. In this chapter, we round out
    our advanced model-building topics by doing the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章[*第五章*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082)，*高级建模 – 第一部分*中，我们详细介绍了在
    H2O 平台上构建企业级**监督学习**模型的过程。在本章中，我们通过以下方式完成我们的高级建模主题：
- en: Demonstrating how to build H2O supervised learning models within an Apache Spark
    pipeline
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示如何在 Apache Spark 管道内构建 H2O 监督学习模型
- en: Introducing H2O's **unsupervised learning** method
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 H2O 的**无监督学习**方法
- en: Discussing best practices for updating H2O models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论更新 H2O 模型的最佳实践
- en: Documenting requirements to ensure H2O model reproducibility
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录需求以确保 H2O 模型的可重复性
- en: We begin this chapter by introducing Sparkling Water pipelines, a method for
    embedding H2O models natively within a Spark pipeline. In enterprise settings
    where Spark is heavily utilized, we have found this to be a popular method for
    building and deploying H2O models. We demonstrate by building a Sparkling Water
    pipeline for **sentiment analysis** using data from online reviews of Amazon food
    products.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章开始时介绍 Sparkling Water 管道，这是一种将 H2O 模型原生嵌入 Spark 管道的方法。在企业环境中，Spark 被广泛使用，我们发现这是一种构建和部署
    H2O 模型的流行方法。我们通过构建一个用于**情感分析**的 Sparkling Water 管道来演示，该管道使用亚马逊食品产品的在线评论数据。
- en: We then introduce the unsupervised learning methods available in H2O. Using
    credit card transaction data, we build an anomaly detection model using isolation
    forests. In this context, the unsupervised model would be used to flag suspicious
    credit card transactions in a financial fraud-prevention effort.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们介绍 H2O 中可用的无监督学习方法。使用信用卡交易数据，我们使用隔离森林构建了一个异常检测模型。在这种情况下，无监督模型将被用于在金融欺诈预防工作中标记可疑的信用卡交易。
- en: We conclude this chapter by addressing issues pertinent to models built in this
    chapter, as well as in [*Chapter 5*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082)*,
    Advanced Model Building – Part I*. These are best practices for updating H2O models
    and ensuring reproducibility of H2O model results.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章结束时解决与本章以及[*第五章*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082)*高级建模
    – 第一部分*中构建的模型相关的问题。这些是更新 H2O 模型的最佳实践，以及确保 H2O 模型结果的可重复性。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Modeling in Sparkling Water
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sparkling Water 中的建模
- en: UL methods in H2O
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: H2O 中的 UL 方法
- en: Best practices for updating H2O models
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新 H2O 模型的最佳实践
- en: Ensuring H2O model reproducibility
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保H2O模型的可重复性
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code and datasets we introduce in this chapter can be found in the GitHub
    repository at [https://github.com/PacktPublishing/Machine-Learning-at-Scale-with-H2O](https://github.com/PacktPublishing/Machine-Learning-at-Scale-with-H2O).
    If you have not set up your H2O environment at this point, see [*Appendix*](B16721_Appendix_Final_SK_ePub.xhtml#_idTextAnchor268)
    *– Alternative Methods to Launch H2O Clusters for This Book,* to do so.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中介绍的代码和数据集可以在 GitHub 仓库[https://github.com/PacktPublishing/Machine-Learning-at-Scale-with-H2O](https://github.com/PacktPublishing/Machine-Learning-at-Scale-with-H2O)中找到。如果您在此阶段尚未设置
    H2O 环境，请参阅[*附录*](B16721_Appendix_Final_SK_ePub.xhtml#_idTextAnchor268) *– 为本书启动
    H2O 集群的替代方法*以进行设置。
- en: Modeling in Sparkling Water
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sparkling Water 中的建模
- en: 'We saw in [*Chapter 2*](B16721_02_Final_SK_ePub.xhtml#_idTextAnchor024), *Platform
    Components and Key Concepts,* that Sparkling Water is simply H2O-3 in an Apache
    Spark environment. From the Python coder''s point of view, H2O-3 code is virtually
    identical to Sparkling Water code. If the code is the same, why have a separate
    section for modeling in Sparkling Water? There are two important reasons, as outlined
    here:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第二章*](B16721_02_Final_SK_ePub.xhtml#_idTextAnchor024)，*平台组件和关键概念*中，我们了解到
    Sparkling Water 在 Apache Spark 环境中仅仅是 H2O-3。从 Python 开发者的角度来看，H2O-3 代码与 Sparkling
    Water 代码几乎相同。如果代码相同，为什么 Sparkling Water 中还要有单独的建模部分？这里有两个重要的原因，如下所述：
- en: Sparkling Water enables data scientists to leverage Spark's extensive data processing
    capabilities.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sparkling Water 使数据科学家能够利用 Spark 的广泛数据处理能力。
- en: Sparkling Water provides access to production Spark pipelines. We expand upon
    these reasons next.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sparkling Water 提供了对生产 Spark 管道的访问。我们将在下面进一步阐述这些原因。
- en: Spark is rightly known for its data operations that effortlessly scale with
    increasing data volume. Since the presence of Spark in an enterprise setting is
    now almost a given, data scientists should add Spark to their skills toolbelt.
    This is not nearly as hard as it seems, since Spark can be operated from Python
    (using PySpark) with data operations written primarily in Spark SQL. For experienced
    Python and **Structured Query Language** (**SQL**) coders, this is a very easy
    transition indeed.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spark因其数据操作能够随着数据量的增加而轻松扩展而闻名。由于Spark在企业环境中的存在现在几乎是一个既定的事实，数据科学家应该将Spark添加到他们的技能工具箱中。这并不像看起来那么困难，因为Spark可以通过Python（使用PySpark）操作，数据操作主要使用Spark
    SQL编写。对于经验丰富的Python和**结构化查询语言**（**SQL**）编码者来说，这确实是一个非常容易的过渡。
- en: In the Lending Club example from [*Chapter 5*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Advanced Model Building – Part I*, data munging and **feature engineering** tasks
    were carried out using native H2O commands on the H2O cluster. These H2O data
    commands work in Sparkling Water as well. However, in an enterprise that has invested
    in a Spark data infrastructure, replacing H2O data commands with their Spark equivalents
    makes a lot of sense. It would then pass the cleaned dataset to H2O to handle
    the subsequent modeling steps. This is our recommended workflow in Sparkling Water.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第5章*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082)“高级模型构建 – 第一部分”中的Lending
    Club示例中，使用H2O集群上的原生H2O命令执行了数据整理和**特征工程**任务。这些H2O数据命令在Sparkling Water中同样适用。然而，在一个已经投资Spark数据基础设施的企业中，用Spark的等效命令替换H2O数据命令是非常有意义的。然后，将清洗后的数据集传递给H2O以处理后续的建模步骤。这是我们推荐在Sparkling
    Water中的工作流程。
- en: In addition, Spark pipelines are frequently used in enterprise production settings
    for **extract, transform, and load** (**ETL**) and other data processing tasks.
    Sparkling Water's integration of H2O algorithms into Spark pipelines allows for
    seamless training and deployment of H2O models in a Spark environment. In the
    remainder of this section, we show how Spark pipelines can be combined with H2O
    modeling to create a Sparkling Water pipeline. This pipeline is easily promoted
    into production, a topic that we return to in detail in [*Chapter 10*](B16721_10_Final_SK_ePub.xhtml#_idTextAnchor178),
    *H2O Model Deployment Patterns*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Spark流水线在企业生产环境中经常用于**提取、转换和加载**（**ETL**）以及其他数据处理任务。Sparkling Water将H2O算法集成到Spark流水线中，使得在Spark环境中无缝训练和部署H2O模型成为可能。在本节的剩余部分，我们将展示如何将Spark流水线与H2O建模相结合，创建一个Sparkling
    Water流水线。这个流水线可以轻松地推广到生产环境中，我们将在[*第10章*](B16721_10_Final_SK_ePub.xhtml#_idTextAnchor178)“H2O模型部署模式”中详细讨论这个话题。
- en: Introducing Sparkling Water pipelines
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍Sparkling Water流水线
- en: '*Figure 6.1* illustrates the Sparkling Water pipeline training and deployment
    process. The pipeline starts with an input data source for model training. Data
    cleaning and feature engineering steps are built sequentially from Spark transformers,
    with the outputs of one transformer becoming the inputs of the subsequent transformer.
    Once the dataset is in a modeling-ready format, H2O takes over to specify and
    build a model. We wrap all the transformer and model steps into a pipeline that
    is trained and then exported for production.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.1*展示了Sparkling Water流水线的训练和部署过程。流水线从模型训练的输入数据源开始。数据清洗和特征工程步骤通过Spark转换器依次构建，一个转换器的输出成为后续转换器的输入。一旦数据集以建模准备好的格式存在，H2O将接管以指定和构建模型。我们将所有转换器和模型步骤封装到一个流水线中，该流水线经过训练然后导出用于生产。'
- en: 'In the production environment, we import the pipeline and introduce new data
    to it (in the following diagram, we assume this happens via a data stream, but
    the data could be arriving in batches as well). The pipeline outputs H2O model
    predictions:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，我们导入流水线并向其中引入新的数据（在以下图中，我们假设这是通过数据流完成的，但数据也可以批量到达）。流水线输出H2O模型预测：
- en: '![Figure 6.1 – Sparkling Water pipeline train and deploy illustration'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.1 – Sparkling Water流水线训练和部署示意图'
- en: '](img/B16721_06_01.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16721_06_01.jpg)'
- en: Figure 6.1 – Sparkling Water pipeline train and deploy illustration
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – Sparkling Water流水线训练和部署示意图
- en: Next, let's create a pipeline to implement sentiment analysis.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个流水线来实现情感分析。
- en: Implementing a sentiment analysis pipeline
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现情感分析流水线
- en: We next create a Sparkling Water pipeline for an sentiment analysis classification
    problem. Sentiment analysis is used to model whether a customer has positive or
    negative feelings toward a product or company. It typically requires **natural
    language processing** (**NLP**) to create predictors from text. For our example,
    we use a preprocessed version of the *Amazon Fine Food reviews* dataset from the
    **Stanford Network Analysis Platform** (**SNAP**) repository. (See [https://snap.stanford.edu/data/web-FineFoods.html](https://snap.stanford.edu/data/web-FineFoods.html)
    for the original data.)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为情感分析分类问题创建一个Sparkling Water流水线。情感分析用于建模客户对产品或公司的正面或负面感受。它通常需要**自然语言处理**（**NLP**）从文本中创建预测器。在我们的例子中，我们使用**斯坦福网络分析平台**（**SNAP**）存储库中预处理的*亚马逊美食评论*数据集。有关原始数据，请参阅[https://snap.stanford.edu/data/web-FineFoods.html](https://snap.stanford.edu/data/web-FineFoods.html)。
- en: Let's first verify that Spark is available on our system.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先验证Spark是否在我们的系统上可用。
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following screenshot shows the output:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了输出：
- en: '![Figure 6.2 – Spark startup in Jupyter notebook with PySparkling kernel'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.2 – 使用PySparkling内核在Jupyter笔记本中启动Spark'
- en: '](img/B16721_06_02.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16721_06_02.jpg)'
- en: Figure 6.2 – Spark startup in Jupyter notebook with PySparkling kernel
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 使用PySparkling内核在Jupyter笔记本中启动Spark
- en: You can see in the Spark output that the **SparkSession** has been started and
    the **SparkContext** initiated.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在Spark输出中看到**SparkSession**已经启动，并且**SparkContext**已经初始化。
- en: PySpark and PySparkling
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark和PySparkling
- en: '**PySpark** is Apache''s Python interface for Spark. It provides a shell for
    interactive Spark sessions and access to Spark components such as Spark SQL, DataFrames,
    and Streaming. **PySparkling** is the H2O extension of **PySpark**, enabling H2O
    services to be started on a Spark cluster from Python. Our Jupyter notebook uses
    a PySpark shell.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**PySpark**是Apache的Spark的Python接口。它提供了一个交互式Spark会话的shell，并可以访问Spark组件，如Spark
    SQL、DataFrames和Streaming。**PySparkling**是**PySpark**的H2O扩展，允许从Python在Spark集群上启动H2O服务。我们的Jupyter笔记本使用PySpark
    shell。'
- en: 'In the *internal backend* mode of Sparkling Water, H2O resources piggyback
    on their Spark counterparts, all within the same **Java virtual machine** (**JVM**).
    As illustrated inthe following diagram, an **H2OContext** that sits on top of
    the **SparkContext** is launched and H2O is initialized in each worker node of
    the Spark cluster:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在Sparkling Water的*内部后端*模式下，H2O资源在其Spark对应物上运行，所有这些都在同一个**Java虚拟机**（**JVM**）中。如图所示，在**SparkContext**之上启动了一个**H2OContext**，并在Spark集群的每个工作节点上初始化了H2O：
- en: '![Figure 6.3 – Sparkling Water internal backend mode'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.3 – Sparkling Water内部后端模式'
- en: '](img/B16721_06_03.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16721_06_03.jpg)'
- en: Figure 6.3 – Sparkling Water internal backend mode
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – Sparkling Water内部后端模式
- en: 'PySparkling is used to create an H2OContext and initialize worker nodes, as
    follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: PySparkling用于创建H2OContext并初始化工作节点，如下所示：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This results in the following output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '![Figure 6.4 – Sparkling Water cluster immediately after launch'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.4 – Sparkling Water集群启动后立即'
- en: '](img/B16721_06_04.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16721_06_04.jpg)'
- en: Figure 6.4 – Sparkling Water cluster immediately after launch
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – Sparkling Water集群启动后立即
- en: After the H2O server is launched, we interact with it using Python commands.
    We will start by importing the raw data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在H2O服务器启动后，我们使用Python命令与之交互。我们首先导入原始数据。
- en: Importing the raw Amazon data
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入原始的亚马逊数据
- en: 'We import the Amazon training data into a `reviews_spark` Spark DataFrame,
    as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将亚马逊训练数据导入到`reviews_spark` Spark DataFrame中，如下所示：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As an alternative, we could have imported the data using H2O and then converted
    the `reviews_h2o` H2O frame to the `reviews_spark` Spark DataFrame, like so:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 作为替代方案，我们本可以使用H2O导入数据，然后将`reviews_h2o` H2O框架转换为`reviews_spark` Spark DataFrame，如下所示：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This approach has the advantage of allowing us to use H2O Flow for interactive
    data exploration, as demonstrated in [*Chapter 5*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Advanced Model Building – Part I*, before converting to a Spark DataFrame.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优势在于，它允许我们使用H2O Flow进行交互式数据探索，如[*第5章*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082)中所示，*高级模型构建
    – 第一部分*，然后再将其转换为Spark DataFrame。
- en: 'Next, we print out the data schema to show the input variables and variable
    types. This is done with the following code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们打印数据模式以显示输入变量和变量类型。这是通过以下代码完成的：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The resulting data schema is shown in the following screenshot:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 结果数据模式在以下屏幕截图中显示：
- en: '![Figure 6.5 – Schema for Amazon Fine Food raw data'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.5 – 亚马逊美食原始数据模式'
- en: '](img/B16721_06_05.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16721_06_05.jpg)'
- en: Figure 6.5 – Schema for Amazon Fine Food raw data
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 亚马逊美食原始数据的模式
- en: For simplicity, we use only the `Time`, `Summary`, and overall `Score` columns
    in this analysis. `Time` is a date-time string, `Score` is an integer value between
    1 and 5, from which sentiment is derived, and `Summary` is a short text summary
    of the product review. Note that the `Text` column contains the actual product
    review. A better model choice would include `Text` in place of—or perhaps in addition
    to—`Summary`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们在这个分析中只使用了`Time`、`Summary`和整体的`Score`列。`Time`是一个日期时间字符串，`Score`是一个介于1到5之间的整数，从它可以导出情感，而`Summary`是产品评论的简短文本摘要。请注意，`Text`列包含实际的产品评论。更好的模型选择将包括`Text`，而不是或可能还包括`Summary`。
- en: 'Save the input data schema into the `schema.json` file using the following
    code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码将输入数据模式保存到`schema.json`文件中：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Saving the input data schema will make the deployment of the Sparkling Water
    pipeline a very simple matter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 保存输入数据模式将使Sparkling Water管道的部署变得非常简单。
- en: Input Data and Production Data Structure
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据和生产数据结构
- en: Saving the data schema for deployment presumes that production data will use
    the same schema. As a data scientist building a Sparkling Water pipeline, we strongly
    recommend that your training input data exactly follows the production data schema.
    It is worth the extra effort to track this information down prior to model build
    rather than having to reengineer something at the deployment stage.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 保存数据模式以供部署假定生产数据将使用相同的模式。作为构建Sparkling Water管道的数据科学家，我们强烈建议您的训练输入数据与生产数据模式完全一致。在模型构建之前追踪这些信息是值得的，而不是在部署阶段重新设计某些内容。
- en: Defining Spark pipeline stages
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义Spark管道阶段
- en: Spark pipelines are created by stringing individual data operations or transformers
    together. Each transformer takes as its input the output data from the preceding
    stage, which makes development for a data scientist very simple. A large job can
    be broken down into individual tasks that are daisy-chained together.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Spark管道是通过将单个数据操作或转换器连接在一起来创建的。每个转换器将其输入数据作为前一阶段的输出数据，这使得数据科学家的开发变得非常简单。一个大型作业可以被分解成一系列相互连接的单独任务。
- en: Apache Spark operates through lazy evaluation. This means that computations
    do not execute immediately; rather, operations are cached and execution occurs
    when an action of some sort is triggered. This approach has many advantages, including
    allowing Spark to optimize its compute.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark通过延迟评估来操作。这意味着计算不会立即执行；相反，操作被缓存，当触发某种操作的动作时才会执行。这种方法具有许多优点，包括允许Spark优化其计算。
- en: In our example, all the data cleaning and feature engineering steps will be
    created through Spark transformers. The pipeline is finalized by training an H2O
    XGBoost model. For clarity, we will define a stage number for each transformer
    as we proceed in building the pipeline.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，所有数据清洗和特征工程步骤将通过Spark转换器创建。通过训练一个H2O XGBoost模型来最终确定管道。为了清晰起见，我们在构建管道的过程中将为每个转换器定义一个阶段编号。
- en: Stage 1 – Creating a transformer to select required columns
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第1阶段 – 创建一个转换器以选择所需的列
- en: 'The Spark `SQLTransformer` class allows us to use SQL to munge data. The fact
    that most data scientists are already experienced with SQL makes for smooth adoption
    of Spark for data operations. `SQLTransformer` will be widely used in this pipeline.
    Run the following code to import the class:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`SQLTransformer`类允许我们使用SQL来处理数据。由于大多数数据科学家已经熟悉SQL，这使得Spark在数据操作方面的采用变得顺利。`SQLTransformer`将在本管道中得到广泛使用。运行以下代码以导入该类：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Define a `colSelect` transformer, like so:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个`colSelect`转换器，如下所示：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, we select the `Score`, `Time`, and `Summary` columns,
    converting the timestamp to a readable date-time string. `__THIS__` in the `FROM`
    statement references the output from the previous transformer stage. Since this
    is the first stage, `__THIS__` refers to the input data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们选择了`Score`、`Time`和`Summary`列，将时间戳转换为可读的日期时间字符串。`FROM`语句中的`__THIS__`引用了前一转换器阶段的输出。由于这是第一个阶段，`__THIS__`指的是输入数据。
- en: 'During development, it is helpful to check results at each stage by calling
    the transformer directly. This makes it easy to debug transformer code and understand
    which inputs will be available for the next stage. Calling the transformer will
    cause Spark to execute it along with all unevaluated upstream code. The following
    code snippet illustrates how to call the transformer:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发过程中，通过直接调用转换器来检查每个阶段的结果是有帮助的。这使得调试转换器代码和理解下一阶段可用的输入变得容易。调用转换器将导致Spark执行它以及所有未评估的上游代码。以下代码片段说明了如何调用转换器：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The first few rows are shown in the following screenshot:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的屏幕截图显示了前几行：
- en: '![Figure 6.6 – Results from the colSelect stage 1 transformer'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.6 – colSelect阶段1转换器的结果'
- en: '](img/B16721_06_06.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16721_06_06.jpg)'
- en: Figure 6.6 – Results from the colSelect stage 1 transformer
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – colSelect阶段1转换器的结果
- en: This first transformer has taken the original data and boiled it down to three
    columns. We will operate on each column separately to create our modeling-ready
    dataset. Let's begin with the `Time` column.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个第一个转换器已经将原始数据简化为三列。我们将分别对每一列进行操作，以创建我们的建模准备数据集。让我们从`时间`列开始。
- en: Stage 2 – Defining a transformer to create multiple time features
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阶段2 – 定义一个转换器以创建多个时间特征
- en: 'The goal of this model is to predict sentiment: was the review positive or
    negative? Date and time are factors that could arguably influence sentiment. Perhaps
    people give better reviews on Friday evenings because there is a weekend upcoming.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型的目标是预测情感：评论是正面还是负面？日期和时间可能是影响情感的因素。也许人们在周五晚上给出更好的评论，因为周末即将到来。
- en: 'The `Time` column is stored internally as a timestamp. To be useful in modeling,
    we need to extract the date-and-time information in a format that is understandable
    by the predictive algorithms we employ. We define an `expandTime` transformer
    using SparkSQL data methods (such as `hour`, `month`, and `year`) to engineer
    multiple new features from the raw timestamp information, as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`时间`列以时间戳的形式内部存储。为了在建模中变得有用，我们需要以预测算法可以理解的形式提取日期和时间信息。我们定义了一个`expandTime`转换器，使用SparkSQL数据方法（如`hour`、`month`和`year`）从原始时间戳信息中构建多个新特征，如下所示：'
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Note that `Score` and `Summary` are selected in the `expandTime` code, but
    we do not operate on them. This simply passes those columns along to subsequent
    transformers. We engineer several features from the `Time` column: `Day`, `Month`,
    `Year`, `WeekNum`, `Weekday`, `HourOfDay`, `Weekend`, and `Season`. And once more,
    `__THIS__` refers to the output from the `colSelect` stage 1 transformer.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在`expandTime`代码中选择了`评分`和`摘要`，但我们没有对它们进行操作。这仅仅是将这些列传递给后续的转换器。我们从`时间`列中构建了几个特征：`日`、`月`、`年`、`周数`、`星期几`、`小时`、`周末`和`季节`。而且，`__THIS__`再次指的是`colSelect`阶段1转换器的输出。
- en: 'To check our progress in development and perhaps debug our code, we inspect
    the output of the second stage, which uses as its input the first-stage results
    stored in `selected`, as illustrated in the following code snippet:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查我们的开发进度并可能调试我们的代码，我们检查第二阶段的输出，该阶段使用存储在`selected`中的第一阶段结果作为输入，如下面的代码片段所示：
- en: '[PRE41]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output is shown in the following screenshot:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的屏幕截图显示了输出。
- en: '![Figure 6.7 – Results from the expandTime stage 2 transformer'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.7 – expandTime阶段2转换器的结果'
- en: '](img/B16721_06_07.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16721_06_07.jpg)'
- en: Figure 6.7 – Results from the expandTime stage 2 transformer
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – expandTime阶段2转换器的结果
- en: The output confirms that we have successfully replaced the `Time` column with
    a collection of newly created features.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 输出确认我们已经成功将`时间`列替换为一系列新创建的特征。
- en: Stage 3 – Creating a response from Score while removing neutral reviews
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阶段3 – 从评分创建响应，同时移除中性评论
- en: In this stage, we create our `Sentiment` response variable using values from
    the `Score` column. We could model *positive* versus *not positive* as the response,
    but we choose to remove neutral reviews (`Score=3`) and compare `Positive` with
    `Negative`. This is a standard approach in **net promoter score** (**NPS**) analyses
    and is common in sentiment analysis. It makes sense because we assume that records
    with a neutral response contain little information the model could learn from.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们使用`评分`列的值创建我们的`情感`响应变量。我们可以将`正面`与`非正面`建模为响应，但选择移除中性评论（`评分=3`），并将`正面`与`负面`进行比较。这是**净推荐者得分**（**NPS**）分析中的标准方法，并且在情感分析中也很常见。这样做是有道理的，因为我们假设具有中性响应的记录包含的信息很少，模型难以从中学习。
- en: 'We create our `createResponse` transformer like so:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建`createResponse`转换器的方式如下：
- en: '[PRE43]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The `IF` statement assigns scores of 1 or 2 to `Negative` sentiment and all
    others to `Positive`, filtering out neutral reviews with the `WHERE` clause. Now,
    inspect the results of this intermediate step by running the following code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`IF`语句将1或2分给`Negative`情感，并将所有其他情感归为`Positive`，通过`WHERE`子句过滤掉中性的评论。现在，通过运行以下代码来检查这个中间步骤的结果：'
- en: '[PRE49]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This results in the following output:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 6.8 – Results from the createResponse stage 3 transformer'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.8 – createResponse阶段3的输出结果](img/B16721_06_08.jpg)'
- en: '](img/B16721_06_08.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16721_06_08.jpg](img/B16721_06_08.jpg)'
- en: Figure 6.8 – Results from the createResponse stage 3 transformer
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – createResponse阶段3的输出结果
- en: The only remaining feature engineering steps are those replacing the text in
    the `Summary` column with appropriately representative numerical values. Stages
    4 through 8 will leverage Spark's built-in NLP data transformation capabilities
    to create features based on the text in `Summary`. While this is not a formal
    deep dive into NLP, we will describe each transformation step in enough detail
    to make our model understandable.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的唯一特征工程步骤是将`Summary`列中的文本替换为适当的代表性数值。第4到第8阶段将利用Spark内置的NLP数据转换功能，根据`Summary`中的文本创建特征。虽然这不是一个正式的深度NLP研究，但我们将详细描述每个转换步骤，以便使我们的模型易于理解。
- en: Stage 4 – Tokenizing the summary
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第4阶段 – 分词摘要
- en: 'Tokenization breaks a text sequence into individual terms. Spark provides a
    simple `Tokenizer` class and a more flexible `RegexTokenizer` class, which we
    use here. The `pattern` parameter specifies a `"[!,\" ]"` `\"` escaped quote in
    the regex), and we specify `This` and `this` will be considered identical terms
    upon later processing. The code is illustrated in the following snippet:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 分词将文本序列分解成单个术语。Spark提供了一个简单的`Tokenizer`类和一个更灵活的`RegexTokenizer`类，我们在这里使用后者。`pattern`参数指定了一个正则表达式中的转义引号`"[!,\"
    ]"`，并且我们指定`This`和`this`在后续处理中将被视为相同的术语。代码在下面的代码片段中展示：
- en: '[PRE51]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Inspect the tokenized values from `Summary`, as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 检查`Summary`中的分词值，如下所示：
- en: '[PRE56]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output is shown in the following screenshot:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 输出在下面的屏幕截图中显示：
- en: '![Figure 6.9 – Results from the regexTokenizer stage 4 transformer'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.9 – regexTokenizer阶段4的输出结果](img/B16721_06_09.jpg)'
- en: '](img/B16721_06_09.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16721_06_09.jpg](img/B16721_06_09.jpg)'
- en: Figure 6.9 – Results from the regexTokenizer stage 4 transformer
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – regexTokenizer阶段4的输出结果
- en: Phrases have now been broken up into lists of individual terms or tokens. Since
    our goal is to extract information from these tokens, we next filter out words
    that carry little information.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在短语已经被分解成单个术语或标记的列表。由于我们的目标是提取这些标记中的信息，我们接下来过滤掉那些信息量较小的单词。
- en: Stage 5 – Removing stop words
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第5阶段 – 去除停用词
- en: 'Some words occur so frequently in language that they have very little predictive
    value. These are termed *stop words* and we use Spark''s `StopWordsRemover` transformer
    to delete them, as illustrated in the following screenshot:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 有些单词在语言中出现的频率很高，它们几乎没有预测价值。这些被称为*停用词*，我们使用Spark的`StopWordsRemover`转换器来删除它们，如下面的截图所示：
- en: '[PRE59]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Let''s compare the tokenized results before and after removing stop words,
    as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较去除停用词前后的分词结果，如下所示：
- en: '[PRE63]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The results are displayed in the following screenshot:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 结果在下面的屏幕截图中显示：
- en: '![Figure 6.10 – Results from the removeStopWords stage 5 transformer'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.10 – removeStopWords阶段5的输出结果](img/B16721_06_10.jpg)'
- en: '](img/B16721_06_10.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16721_06_10.jpg](img/B16721_06_10.jpg)'
- en: Figure 6.10 – Results from the removeStopWords stage 5 transformer
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – removeStopWords阶段5的输出结果
- en: 'Inspecting the results in *Figure 6.10* is illustrative. For the most part,
    removing stop words such as `as`, `it`, or `the` has little effect on meaning:
    the *Great! Just as good as the expensive brands!* statement being reduced to
    tokens `[great, good, expensive, brands]` seems reasonable. But what about *Not
    as advertised!* being reduced to `[advertised]`? The `not` in the statement would
    seem to carry important information that is lost by its removal. This is a valid
    concern that could be addressed by NLP concepts such as n-grams (bigrams, trigrams,
    and so on). For an example demonstrating Sparkling Water pipelines, we will acknowledge
    this as a potential issue but move on for simplicity.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 检查*图6.10*的结果是很有说明性的。就大部分而言，移除诸如`as`、`it`或`the`之类的停用词对意义的影响很小：将*Great! Just as
    good as the expensive brands!*的陈述缩减为标记`[great, good, expensive, brands]`似乎是合理的。但*Not
    as advertised!*缩减为`[advertised]`呢？陈述中的`not`似乎承载着重要信息，而移除它会导致信息丢失。这是一个有效的问题，可以通过NLP概念如n-gram（双词组、三词组等）来解决。为了演示Sparkling
    Water管道的例子，我们将承认这是一个潜在问题，但为了简单起见，我们将继续前进。
- en: NLP in predictive modeling represents information in text as numbers. A popular
    approach is **term frequency-inverse document frequency** (**TF-IDF**). TF is
    simply the number of times a term appears in a document divided by the number
    of words in a document. In a corpus (collection of documents), IDF measures how
    rare a term is across its constituent documents. A term such as *linear* may have
    high frequency, but its information value decreases as the number of documents
    it appears in increases. On the other hand, a word such as *motorcycle* may have
    lower frequency but also be found in fewer documents in a corpus, making its information
    content higher. Multiplying TF by IDF gives a rescaled TF value that has proven
    quite useful. TF-IDF values are maximized when a term is found frequently but
    only in one document (*Which article reviewed motorcycles?*).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测建模中，自然语言处理（NLP）将文本信息表示为数字。一种流行的方法是**词频-逆文档频率**（**TF-IDF**）。TF简单地是指一个词在文档中出现的次数除以文档中的单词数。在一个语料库（文档集合）中，IDF衡量一个词在其构成文档中的稀有程度。例如，*线性*这样的词可能频率很高，但随着它在文档中出现的次数增加，其信息价值会降低。另一方面，像*摩托车*这样的词可能频率较低，但在语料库中出现的文档较少，使其信息含量更高。将TF乘以IDF得到一个经过缩放的TF值，这已被证明非常有用。当一个词频繁出现在一个文档中但只在一个文档中出现时，TF-IDF值达到最大（*哪篇文章回顾了摩托车？*）。
- en: TF-IDF is widely used in information retrieval, text mining, recommender systems,
    and search engines, as well as in predictive modeling. The next two pipeline stages
    will compute the TF and IDF values, respectively.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF在信息检索、文本挖掘、推荐系统和搜索引擎以及预测建模中得到广泛应用。接下来的两个管道阶段将分别计算TF和IDF值。
- en: Stage 6 – Hashing words for TF
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第6阶段 – 为TF哈希单词
- en: Our preferred way to compute TF in Spark is `CountVectorizer`, which preserves
    the mapping from the index back to the word using an internal vocabulary. That
    is, `countVectorizerModel.vocabulary[5]` looks up the word stored in index 5.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Spark中计算TF的首选方法是`CountVectorizer`，它使用内部词汇表将索引映射回单词。也就是说，`countVectorizerModel.vocabulary[5]`查找存储在索引5中的单词。
- en: 'A trick for building better TF-IDF models is to remove terms that are too infrequent
    by setting the `minDF` parameter as an integer or proportion, as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 构建更好的TF-IDF模型的一个技巧是通过将`minDF`参数设置为整数或比例来移除过于罕见的词，如下所示：
- en: '`minDF = 100`: Omit terms that appear in fewer than 100 documents'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minDF = 100`：省略在不到100个文档中出现的词'
- en: '`minDF = 0.05`: Omit terms that appear in fewer than 5% of documents'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minDF = 0.05`：省略在不到5%的文档中出现的词'
- en: A `maxDF` parameter is also available for removing terms that occur too frequently
    across a corpus. For instance, setting `maxDF = 0.95` in NLP for document retrieval
    might improve model performance.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个`maxDF`参数可以用来移除在语料库中过于频繁出现的词。例如，在NLP文档检索中设置`maxDF = 0.95`可能会提高模型性能。
- en: 'We create a `countVectorizer` transformer, as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个`countVectorizer`转换器，如下所示：
- en: '[PRE67]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Note that our corpus is the output column of the `removeStopWords` transformer,
    with each row as a document. We output the frequencies and set `minDF` to 100\.
    Because `countVectorizer` is a model, it is a good idea to manually train it before
    executing it in the pipeline. This is a good practice for any model that is a
    pipeline component as it allows us to determine its behavior and perhaps fine-tune
    it before pipeline execution commences. The code is illustrated in the following
    snippet:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们的语料库是`removeStopWords`转换器的输出列，每行作为一个文档。我们输出频率并设置`minDF`为100。因为`countVectorizer`是一个模型，所以在执行管道之前手动训练它是好主意。这对于任何作为管道组件的模型来说都是一种好习惯，因为它允许我们在管道执行开始之前确定其行为，并可能对其进行微调。以下代码片段展示了这一点：
- en: '[PRE72]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'We can explore this model by inspecting its vocabulary size and individual
    terms, as well as any other appropriate due diligence. Here''s the code we''d
    need to accomplish this:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查其词汇表大小和单个术语，以及任何其他适当的尽职调查来探索这个模型。以下是完成此操作所需的代码：
- en: '[PRE73]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The vocabulary results are shown here:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表结果如下所示：
- en: '![Figure 6.11 – Vocabulary size and vocabulary of the countVecModel transformer'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.11 – countVecModel转换器的词汇表大小和词汇]'
- en: '](img/B16721_06_11.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '![图B16721_06_11.jpg]'
- en: Figure 6.11 – Vocabulary size and vocabulary of the countVecModel transformer
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – countVecModel转换器的词汇表大小和词汇
- en: 'Our total vocabulary size shown in *Figure 6.11* is 1,431 terms. Inspect the
    data with the following code:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.11*中显示的总词汇表大小是1,431个术语。使用以下代码检查数据：
- en: '[PRE76]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The vectorized result is shown in the following screenshot:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化结果如下截图所示：
- en: '![Figure 6.12 – Intermediate results from the countVecModel transformer'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.12 – countVecModel转换器的中间结果]'
- en: '](img/B16721_06_12.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '![图B16721_06_12.jpg]'
- en: Figure 6.12 – Intermediate results from the countVecModel transformer
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – countVecModel转换器的中间结果
- en: '*Figure 6.12* shows the cleaned summary tokens with TF vectors side by side
    for each row. To describe the output for the first row, the 1431 value is the
    vocabulary size. The next sequence of values—`[1,10,11,38]`—refers to the indices
    of the `[good, quality, dog, food]` terms in the vocabulary vector. The last series
    of values— `[1.0,1.0,1.0,1.0]`—are the TFs for their respective terms. Thus, `dog`
    is referenced by index `11` and occurs once in the `CleanedSummary` column.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.12* 展示了每行旁边带有TF向量的清洗后的摘要标记。为了描述第一行的输出，1431的值是词汇表大小。接下来的值序列——`[1,10,11,38]`——指的是词汇向量中`[good,
    quality, dog, food]`术语的索引。最后的值序列——`[1.0,1.0,1.0,1.0]`——是它们各自术语的TF值。因此，`dog`通过索引`11`引用，并在`CleanedSummary`列中出现了1次。'
- en: Stage 7 – Creating an IDF model
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第7阶段 – 创建IDF模型
- en: 'We use Spark''s `IDF` estimator to scale frequencies from `countVectorizer`,
    yielding TF-IDF values. We execute the following code to accomplish this:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Spark的`IDF`估计器从`countVectorizer`缩放频率，得到TF-IDF值。我们执行以下代码来完成此操作：
- en: '[PRE79]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Manually train the IDF model to see the results before we execute the pipeline,
    like so:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们执行管道之前，手动训练IDF模型以查看结果，如下所示：
- en: '[PRE83]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Inspect the data again, noting especially the scaled TF-IDF frequencies, as
    follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 再次检查数据，特别注意缩放的TF-IDF频率，如下所示：
- en: '[PRE84]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The first five rows of the resulting TF-IDF model are shown in the following
    screenshot:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了结果TF-IDF模型的头五行：
- en: '![Figure 6.13 – TF-IDF frequencies from the Spark transformer'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.13 – 来自Spark转换器的TF-IDF频率]'
- en: '](img/B16721_06_13.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '![图B16721_06_13.jpg]'
- en: Figure 6.13 – TF-IDF frequencies from the Spark transformer
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – 来自Spark转换器的TF-IDF频率
- en: Stage 8 – Selecting modeling dataset columns
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第8阶段 – 选择建模数据集列
- en: 'In addition to the `Sentiment` response variable and all the features engineered
    from the `Time` variable, the output of `idf` includes the original `Summary`
    column as well as `Tokenized`, `CleanedSummary`, `frequencies`, and `TFIDF`. Of
    these, we wish to keep only `TFIDF`. The following code selects the desired columns:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`Sentiment`响应变量以及从`Time`变量中构建的所有特征之外，`idf`的输出还包括原始的`Summary`列以及`Tokenized`、`CleanedSummary`、`frequencies`和`TFIDF`。在这些中，我们只想保留`TFIDF`。以下代码选择了所需的列：
- en: '[PRE87]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Now that we are finished building our model-ready data, the next step is to
    build a predictive model using one of H2O's supervised learning algorithms.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了模型准备数据的构建，下一步是使用H2O的监督学习算法之一构建一个预测模型。
- en: Stage 9 – Creating an XGBoost model using H2O
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第9阶段 – 使用H2O创建XGBoost模型
- en: 'Up to this point, all our data wrangling and feature engineering efforts have
    used Spark methods exclusively. Now, we turn to H2O to train an XGBoost model
    on the `Sentiment` column. For simplicity, we train using default settings. The
    code is illustrated in the following snippet:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所有的数据处理和特征工程工作都仅使用了Spark方法。现在，我们转向H2O在`Sentiment`列上训练XGBoost模型。为了简单起见，我们使用默认设置进行训练。代码如下所示：
- en: '[PRE92]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Note – Training Models in Sparkling Water
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 注意——在Sparkling Water中训练模型
- en: In [*Chapter 5*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082), *Advanced
    Model Building – Part I*, we demonstrated in detail a process for building and
    tuning a high-quality XGBoost model. We stop at a simple baseline model here to
    emphasize the utility of the overall pipeline. In a real application, much more
    effort should be spent on the modeling component of this pipeline.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第五章*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082)《高级模型构建——第一部分》中，我们详细演示了构建和调整高质量XGBoost模型的过程。在这里，我们停留在简单的基线模型上，以强调整体管道的实用性。在实际应用中，应该在这条管道的建模组件上投入更多的努力。
- en: Creating a Sparkling Water pipeline
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建Sparkling Water管道
- en: 'Now that we have all the transformers defined, we are ready to create a pipeline.
    Doing so is simple—we just name each transformer in order in the `stages` list
    parameter of `Pipeline`, as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了所有转换器，我们准备创建一个管道。这样做很简单——我们只需按照顺序在`Pipeline`的`stages`列表参数中命名每个转换器，如下所示：
- en: '[PRE95]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Training the pipeline model is made simple by using the `fit` method. We pass
    as a parameter the Spark DataFrame containing the raw data, as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`fit`方法简化了管道模型的训练。我们传递一个包含原始数据的Spark DataFrame作为参数，如下所示：
- en: '[PRE100]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: During the `pipeline.fit` process, the data munging and feature engineering
    stages are all applied to the raw data in the order defined before the XGBoost
    model is fit. These pipeline stages operate identically after deployment in production
    with the XGBoost stage, producing predictions.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在`pipeline.fit`过程中，数据预处理和特征工程阶段按照在XGBoost模型拟合之前定义的顺序应用于原始数据。这些管道阶段在生产部署后与XGBoost阶段操作相同，生成预测。
- en: Looking ahead – a production preview
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 展望未来——生产预览
- en: 'Putting the Sparkling Water pipeline into production is simply a matter of
    *saving* the `pipeline` model, *loading* it onto the production system, then calling
    the following:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 将Sparkling Water管道投入生产只是简单地保存`pipeline`模型，将其加载到生产系统中，然后调用以下操作：
- en: '[PRE101]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: In [*Chapter 10*](B16721_10_Final_SK_ePub.xhtml#_idTextAnchor178), *H2O Model
    Deployment Patterns*, we show how to deploy this pipeline as a Spark streaming
    application, with the pipeline receiving raw streaming data and outputting predictions
    in real time.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第十章*](B16721_10_Final_SK_ePub.xhtml#_idTextAnchor178)《H2O模型部署模式》中，我们展示了如何将此管道作为Spark流应用程序部署，该管道接收原始流数据并在实时输出预测。
- en: UL methods in H2O
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: H2O中的UL方法
- en: H2O includes several unsupervised learning algorithms including **Generalized
    Low Rank Models** (**GLRM**), **Principal Component Analysis** (**PCA**), and
    an aggregator for dimensionality reduction. Clustering use cases can utilize k-means
    clustering, H2O aggregator, GLRM, or PCA. Unsupervised learning also underlies
    a set of useful feature transformers used in predictive modeling applications—for
    example, the distance of an observation to a specific data cluster identified
    by an unsupervised method. In addition, H2O provides an isolation forest algorithm
    for anomaly detection.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: H2O包括几个无监督学习算法，包括**广义低秩模型**（**GLRM**）、**主成分分析**（**PCA**）以及用于降维的聚合器。聚类用例可以利用k-means聚类、H2O聚合器、GLRM或PCA。无监督学习还基于一组在预测建模应用中使用的有用特征转换器——例如，一个观测点到由无监督方法识别的特定数据簇的距离。此外，H2O还提供了一种用于异常检测的隔离森林算法。
- en: What is anomaly detection?
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是异常检测？
- en: 'Most **machine learning** (**ML**) algorithms attempt, in some manner, to find
    patterns in data. These patterns are leveraged to make predictions in supervised
    learning models. Many unsupervised learning algorithms try to uncover patterns
    through clustering similar data or estimating boundaries between data segments.
    Unsupervised anomaly detection algorithms take the opposite approach: data points
    that do not follow known patterns are what we want to discover.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数**机器学习**（**ML**）算法以某种方式试图在数据中找到模式。这些模式被用于监督学习模型中的预测。许多无监督学习算法试图通过聚类相似数据或估计数据段之间的边界来揭示模式。无监督异常检测算法采取相反的方法：不遵循已知模式的数据点是我们要发现的。
- en: 'In this context, the term *anomaly* is value-free. It may refer to an unusual
    observation because it is the first of its kind; more data could yield additional
    similar observations. Anomalies might be indicative of unexpected events and serve
    as a diagnostic. For instance, a failed sensor in a manufacturing data-collection
    application could yield atypical measurements. Anomalies may also indicate malicious
    actors or actions: security breaches and fraud are two classic examples resulting
    in anomalous data points.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，术语“异常”是中性的。它可能指的是一种不寻常的观察，因为它是最先出现的；更多的数据可能会产生更多类似的观察。异常可能表明意外事件，并可作为诊断工具。例如，在制造数据收集应用中，一个失败的传感器可能会产生不典型的测量值。异常也可能表明恶意行为者或行为：安全漏洞和欺诈是导致异常数据点的两个经典例子。
- en: Anomaly detection approaches may include supervised, semi-supervised, or unsupervised
    methods. Supervised models are the gold standard in fraud detection. However,
    obtaining labels for each observation can be costly and is often not feasible.
    An unsupervised approach is required when labels are absent. Semi-supervised approaches
    refer to situations where only some of the data records are labeled, usually a
    small minority of records.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测方法可能包括监督的、半监督的或无监督的方法。监督模型在欺诈检测中是金标准。然而，为每个观察结果获取标签可能成本高昂，并且通常不可行。当没有标签时，需要无监督方法。半监督方法指的是只有一些数据记录被标记的情况，通常是一小部分记录。
- en: Isolation forest is a unsupervised learning algorithm for anomaly detection—we'll
    introduce this next.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 隔离森林是一种无监督学习算法，用于异常检测——我们将在下一节介绍。
- en: Isolation forests in H2O
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: H2O中的隔离森林
- en: 'The isolation forest algorithm is based on decision trees and a clever observation:
    outliers tend to be split out very early in the building of a decision tree. But
    decision trees are a supervised method, so how is this unsupervised? The trick
    is to create a target column of random values and train a decision tree on it.
    We repeat this many times and record the average depth at which observations are
    split into their own leaf. The earlier an observation is isolated, the more likely
    it is to be anomalous. Depending on the use case, these anomalous points may be
    filtered out or escalated for further investigation.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 隔离森林算法基于决策树和一个巧妙的观察：异常值往往在决策树构建的早期就被分割出来。但是决策树是一种监督方法，那么如何实现无监督呢？诀窍是创建一个随机值的标签列，并在其上训练一个决策树。我们重复多次，并记录观察结果被分割到其自己的叶节点中的平均深度。观察结果越早被隔离，它就越可能是异常的。根据用例，这些异常点可能被过滤掉或升级以进行进一步调查。
- en: 'You can see a representation of an isolation forest in the following screenshot:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在下面的屏幕截图中看到隔离森林的表示：
- en: '![Figure 6.14 – An isolation forest'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.14 – An isolation forest](img/B16721_06_14.jpg)'
- en: '](img/B16721_06_14.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16721_06_14.jpg](img/B16721_06_14.jpg)'
- en: Figure 6.14 – An isolation forest
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 – 一个隔离森林
- en: We show how to build an isolation forest in H2O using the Kaggle credit card
    transaction data ([https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud)).
    There are 492 fraudulent and 284,807 non-fraudulent transactions in this dataset,
    which makes the target class highly imbalanced. Because we are demonstrating an
    unsupervised anomaly detection approach, we will drop the labeled target during
    the model build.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了如何在H2O中使用Kaggle信用卡交易数据（[https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud)）构建隔离森林。在这个数据集中有492起欺诈交易和284,807起非欺诈交易，这使得目标类高度不平衡。因为我们正在演示无监督异常检测方法，所以在模型构建过程中我们将丢弃标记的目标。
- en: 'The H2O code for loading the data is shown here:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据的H2O代码如下所示：
- en: '[PRE102]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'We fit our isolation forest using the `H2OIsolationForestEstimator` method.
    We set the number of trees to `100` and omit the last column, which contains the
    target class label, as shown in the following code snippet:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`H2OIsolationForestEstimator`方法来拟合我们的隔离森林。我们将树的数量设置为`100`，并省略了最后一列，该列包含目标类标签，如下面的代码片段所示：
- en: '[PRE103]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Once the model is trained, prediction is straightforward, as we can see here:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，预测就很简单，正如我们在这里可以看到的：
- en: '[PRE106]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'The output is shown in the following screenshot:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 6.15 – Isolation forest predictions'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.15 – Isolation forest predictions](img/B16721_06_15.jpg)'
- en: '](img/B16721_06_15.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16721_06_15.jpg](img/B16721_06_15.jpg)'
- en: Figure 6.15 – Isolation forest predictions
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 – 隔离森林预测
- en: 'The predictions in *Figure 6.15* consist of two columns: the normalized anomaly
    score and the average number of splits across all trees to isolate the observation.
    Note that the anomaly score is perfectly correlated with mean length, increasing
    as mean length decreases.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.15* 中的预测结果包含两列：归一化的异常分数和所有树中隔离观察的平均分割数。请注意，异常分数与平均长度完全相关，随着平均长度的减小而增加。'
- en: 'How do we go from either an anomaly score or mean length to an actual prediction?
    One of the best ways is through a quantile-based threshold. If we have an idea
    about the prevalence of fraud, we can find the corresponding quantile value of
    the score and use it as a threshold for our predictions. Suppose we know that
    5% of our transactions are fraudulent. Then, we estimate the correct quantile
    using the following H2O code:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何从异常分数或平均长度到一个实际的预测？最好的方法之一是通过基于分位数的阈值。如果我们对欺诈的普遍性有所了解，我们可以找到相应的分数分位数值并将其用作预测的阈值。假设我们知道我们
    5% 的交易是欺诈的。然后，我们使用以下 H2O 代码估计正确的分位数：
- en: '[PRE108]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'The resulting quantile output is shown in the following screenshot:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的分位数输出如下截图所示：
- en: '![Figure 6.16 – Choosing a quantile-based threshold'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.16 – 选择基于分位数阈值的阈值'
- en: '](img/B16721_06_16.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16721_06_16.jpg)'
- en: Figure 6.16 – Choosing a quantile-based threshold
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16 – 选择基于分位数阈值的阈值
- en: 'We now can use the threshold to predict the anomalous class using the following
    code:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用以下代码使用阈值来预测异常类别：
- en: '[PRE111]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'The first 10 observations of the `predictions` frame are shown in the following
    screenshot:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了 `predictions` 帧的前 10 个观察值：
- en: '![Figure 6.17 – Identifying anomalous values'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.17 – 识别异常值'
- en: '](img/B16721_06_17.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16721_06_17.jpg)'
- en: Figure 6.17 – Identifying anomalous values
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17 – 识别异常值
- en: The `predict` column in *Figure 6.17* has only one observation greater than
    0.198324, the threshold for the 95th percentile shown in *Figure 6.16*. The `predicted_class`
    column indicates this with a value of `1`. Also, note that the `mean_length` value
    for this observation of `6.14` is less than the mean length values for the other
    nine observations.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.17* 中的 `predict` 列只有一条观察值大于 0.198324，这是 *图 6.16* 中显示的 95 百分位数的阈值。`predicted_class`
    列用 `1` 的值表示这一点。此外，请注意，该观察值的 `mean_length` 值为 `6.14`，小于其他九个观察值的平均长度值。'
- en: The `class` column contains the transaction fraud indicator that we omitted
    in building the unsupervised isolation forest model. For the anomalous observation,
    a class value of `0` indicates that the transaction was not fraudulent. When we
    have access to an actual target value as in this example, we could use the `predicted_class`
    and `class` columns to study the effectiveness of the anomaly detection algorithm
    in detecting fraud. We should note that the definition of fraud and anomalous
    in this context are not equivalent. In other words, not all frauds are anomalous
    and not all anomalies will indicate fraud. These two models have separate, albeit
    complementary, purposes.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '`class` 列包含我们在构建无监督隔离森林模型时省略的交易欺诈指示符。对于异常观察值，`0` 类值表示交易不是欺诈的。当我们像在这个例子中一样能够访问实际的目标值时，我们可以使用
    `predicted_class` 和 `class` 列来研究异常检测算法在检测欺诈方面的有效性。我们应该注意，在这个上下文中，欺诈和异常的定义并不相同。换句话说，并非所有欺诈都是异常的，并非所有异常都会表明欺诈。这两个模型有各自的目的，尽管是互补的。'
- en: We now turn our attention to updating models.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将注意力转向更新模型。
- en: Best practices for updating H2O models
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新 H2O 模型的最佳实践
- en: As the famous British statistician George Box stated, *All models are wrong,
    but some are useful*. Good modelers are aware of the purpose as well as the limitations
    of their models. This should be especially true for those who build enterprise
    models that go into production.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 正如著名的英国统计学家乔治·博克斯所说，*所有模型都是错误的，但有些是有用的*。好的模型构建者了解他们模型的目的以及局限性。这对于那些构建投入生产的企业的模型的人来说尤其如此。
- en: One such limitation is that predictive models as a rule degrade over time. This
    is largely because, in the real world, things change. Perhaps what we are modeling—customer
    behavior, for example—itself changes, and the data we collect reflects that change.
    Even if customer behavior is static but our mix of business changes (think more
    teenagers and fewer retirees), our model's predictions will likely degrade. In
    both cases but for different reasons, the population that was sampled to create
    our predictive model is not the same now as it was before.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一种限制是，预测模型通常随着时间的推移而退化。这主要是因为在现实世界中，事物是不断变化的。也许我们所建模的——比如客户行为——本身也在变化，而我们收集的数据反映了这种变化。即使客户行为是静态的，但我们的业务组合发生了变化（比如更多的青少年和更少的退休人员），我们的模型预测很可能会退化。在这两种情况下，尽管原因不同，但用于创建我们的预测模型的样本人群现在与之前不同。
- en: Detecting model degradation and searching for its root cause is the subject
    of diagnostics and model monitoring, which we do not address here. Rather, once
    a model is no longer satisfactory, what should a data scientist do? We address
    retraining and checkpointing models in the following sections.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 诊断模型退化并寻找其根本原因是诊断和模型监控的主题，这里我们不涉及。相反，一旦模型不再令人满意，数据科学家应该做什么？我们在以下章节中讨论了模型的重新训练和检查点。
- en: Retraining models
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新训练模型
- en: 'Developing a parametric model entails:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 开发参数模型包括：
- en: Finding the correct structural form for the process being modeled and then
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到正在建模的过程的正确结构形式，然后
- en: Using data to estimate the parameters of that structure. Over time, if the structure
    of the model remains the same but the data changes, then we could *refit* (or
    *retrain* or *re-estimate* or *update*) the model's parameter estimates. This
    is a simple and relatively straightforward procedure.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据来估计该结构的参数。随着时间的推移，如果模型的架构保持不变但数据发生变化，那么我们可以*重新拟合*（或*重新训练*、*重新估计*或*更新*）模型的参数估计。这是一个简单且相对直接的程序。
- en: However, if the underlying process changes in a way that means the structural
    form of the model is no longer valid, then modeling consists of both discovering
    the correct form of the model and estimating parameters. This is almost, but not
    quite, the same thing as starting from scratch. *Rebuilding* or *updating the
    model* (as opposed to *updating the parameter estimates*) are better terms for
    this larger activity.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果底层过程发生变化，以至于模型的架构形式不再有效，那么建模包括发现模型的正确形式和估计参数。这几乎，但并不完全，等同于从头开始。*重建*或*更新模型*（与*更新参数估计*相对）是描述这一更大活动的更好术语。
- en: 'In the case of ML or other nonparametric models, the structural form of the
    model is determined by the data along with any parameter estimates. This is one
    of the selling points of nonparametric models: they are incredibly data-driven
    and nearly assumption-free. The differences between refitting or retraining and
    rebuilding have little meaning in this context; these terms become, in effect,
    synonyms.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习或其他非参数模型的情况下，模型的架构形式由数据以及任何参数估计决定。这是非参数模型的一个卖点：它们非常数据驱动，几乎无假设。在这个背景下，重新拟合或重新训练与重建之间的差异几乎没有意义；这些术语实际上变成了同义词。
- en: Checkpointing models
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查点模型
- en: The **Checkpoint** option in H2O lets you save the state of a model build, allowing
    a new model to be built as a *continuation* of a previously generated model rather
    than building one from scratch. This can be used to update a model in production
    with additional, more current data.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: H2O中的**检查点**选项允许您保存模型构建的状态，使得新的模型可以作为先前生成的模型的*延续*而不是从头开始构建。这可以用于使用额外的、更当前的数据更新生产中的模型。
- en: The checkpoint option is available for **Distributed Random Forest** (**DRF**),
    **Gradient Boosting Machine** (**GBM**), XGBoost, and **deep learning** (**DL**)
    algorithms. For tree-based algorithms, the number of trees specified must be greater
    than the number of trees in the referenced model. That is, if the original model
    included 20 trees and you specify 30 trees, then 10 new trees will be built. The
    same concept is true for DL using epochs rather than trees.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点选项适用于**分布式随机森林**（**DRF**）、**梯度提升机**（**GBM**）、XGBoost和**深度学习**（**DL**）算法。对于基于树的算法，指定的树的数量必须大于引用模型中的树的数量。也就是说，如果原始模型包含20棵树，而你指定了30棵树，那么将构建10棵新树。对于使用epoch而不是树的深度学习，这个概念同样适用。
- en: 'Checkpointing is feasible for these algorithms *only* when the following are
    the same as the checkpointed model:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在以下条件与检查点模型相同的情况下，这些算法的检查点才是可行的：
- en: The training data model type, response type, columns, categorical factor levels,
    and the total number of predictors
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据模型类型、响应类型、列、分类因素水平以及预测因子总数
- en: The identical validation dataset if one was used in the checkpointed model (cross-validation
    is not currently supported for checkpointing)
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在检查点模型中使用了，则使用相同的验证数据集（目前不支持检查点的交叉验证）。
- en: Additional parameters that you can specify with checkpointing vary based on
    the algorithm that was used for model training.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用检查点指定的附加参数取决于用于模型训练的算法。
- en: Checkpointing Caveats
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点注意事项
- en: Although it is technically feasible, we do not recommend checkpointing on new
    data for GBM or XGBoost algorithms. Recall that boosting works by fitting sequential
    models to the residuals of previous models. The early splits are thus the most
    important. By the time new data has been introduced, the structure of the model
    has been largely determined in its absence.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在技术上可行，但我们不建议对GBM或XGBoost算法的新数据进行检查点。回想一下，提升是通过拟合先前模型的残差来工作的。因此，早期的分割是最重要的。在新数据被引入之前，模型的结构在很大程度上已经确定。
- en: Checkpointing **random forest** models does not suffer from these concerns due
    to differences between boosting and bagging.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 由于提升和袋装之间的差异，检查点**随机森林**模型不受这些问题的困扰。
- en: Ensuring H2O model reproducibility
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保H2O模型的可重复性
- en: In a laboratory or experimental setting, repeating a process under the same
    protocols and conditions should lead to similar results. Natural variability may
    of course occur, but this can be measured and attributed to appropriate factors.
    This is termed *repeatability*. The enterprise data scientist should ensure that
    their model builds are well coded and sufficiently documented to make the process
    repeatable.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验室或实验环境中，在相同的协议和条件下重复一个过程应该导致类似的结果。当然，自然变异性可能发生，但这可以被测量并归因于适当的因素。这被称为*可重复性*。企业数据科学家应确保他们的模型构建具有良好的编码和充分的文档，以便使过程可重复。
- en: '**Reproducibility** in the context of model building is a much stronger condition:
    the results when a process is repeated must be identical. From a regulatory or
    compliance perspective, reproducibility may be required.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型构建的上下文中，**可重复性**是一个更强的条件：重复一个过程时，结果必须相同。从监管或合规的角度来看，可能需要可重复性。
- en: At a high level, reproducibility requires the same hardware, software, data,
    and settings. Let's review this specifically for H2O setups. We begin with two
    cases depending on the H2O cluster type.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，可重复性需要相同的硬件、软件、数据和设置。让我们具体回顾H2O设置。我们开始于根据H2O集群类型的不同，考虑两个案例。
- en: Case 1 – Reproducibility in single-node clusters
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例一 – 单节点集群中的可重复性
- en: 'A single-node cluster is the simplest H2O hardware configuration. Reproducibility
    can be attained if the following conditions are met:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 单节点集群是H2O硬件配置中最简单的配置。如果满足以下条件，可以实现可重复性：
- en: '**Software requirements**: The same version of H2O-3 or Sparkling Water is
    used.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软件要求**：使用相同的H2O-3或Sparkling Water版本。'
- en: '**Data requirements**: The same training data is used (note that H2O requires
    files to be imported individually rather than as an entire directory to guarantee
    reproducibility).'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据要求**：使用相同的训练数据（请注意，H2O需要单独导入文件而不是整个目录，以确保可重复性）。'
- en: '`sample_rate`, `sample_rate_per_class`, `col_sample_rate`, `col_sample_rate_per_level`,
    `col_sample_rate_per_tree`.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_rate`、`sample_rate_per_class`、`col_sample_rate`、`col_sample_rate_per_level`、`col_sample_rate_per_tree`。'
- en: If early stopping is enabled, reproducibility is only guaranteed when the `score_tree_interval` parameter
    is explicitly set and the same validation dataset is used.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果启用了早期停止，只有当显式设置了`score_tree_interval`参数并且使用了相同的验证数据集时，才能保证可重复性。
- en: Case 2 – Reproducibility in multi-node clusters
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例二 – 多节点集群中的可重复性
- en: 'Adding nodes to a cluster creates additional hardware conditions that must
    be met in order to achieve reproducibility. The software, data, and settings requirements
    are the same as in single-node clusters detailed previously in *Case 1*. These
    requirements are outlined here:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 向集群添加节点会创建额外的硬件条件，这些条件必须满足才能实现可重复性。软件、数据和设置的要求与之前在*案例1*中详细说明的单节点集群相同。这些要求在此概述：
- en: The hardware cluster must be configured identically. Specifically, clusters
    must have the same number of nodes with the same number of CPU cores per node
    or—alternatively—the same restriction on the number of threads.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件集群必须配置相同。具体来说，集群必须具有相同数量的节点，每个节点具有相同数量的CPU核心，或者——作为替代方案——对线程数量的相同限制。
- en: The cluster's leader node must initiate model training. In Hadoop, the leader
    node is automatically returned to the user. In standalone deployments, the leader
    node must be manually identified. See the H2O documentation for more details.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群的领导者节点必须启动模型训练。在Hadoop中，领导者节点会自动返回给用户。在独立部署中，必须手动识别领导者节点。请参阅H2O文档以获取更多详细信息。
- en: For reproducibility, you must ensure that the cluster configuration is identical.
    The parallelization level (number of nodes and CPU cores/threads) controls how
    a dataset is partitioned in memory. H2O runs its tasks in a predictable order
    on these partitions. If the number of partitions is different, the results will
    not be reproducible.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保可重复性，你必须确保集群配置是相同的。并行化级别（节点数和每个节点的CPU核心/线程数）控制着数据集在内存中的分区方式。H2O在这些分区上以可预测的顺序运行其任务。如果分区数不同，结果将不可重复。
- en: In cases where the cluster configuration is not identical, it may be possible
    to constrain the resources of computations being reproduced. This process involves
    replicating data partitions in the original environment. We refer you to the H2O
    documentation for more information.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群配置不相同的情况下，可能可以限制正在复制的计算资源。这个过程涉及在原始环境中复制数据分区。我们建议您查阅H2O文档以获取更多信息。
- en: Reproducibility for specific algorithms
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特定算法的可重复性
- en: The complexity of DL, GBM, and **automated ML** (**AutoML**) algorithms introduces
    additional constraints that must be met in order to ensure reproducibility. We
    will review these requirements in this section.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）、梯度提升机（GBM）和**自动机器学习**（AutoML）算法的复杂性引入了额外的约束，这些约束必须满足以确保可重复性。我们将在本节中回顾这些要求。
- en: DL
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DL
- en: H2O DL models are not reproducible by default for performance reasons. There
    is a `reproducible` option that can be enabled, but we recommend doing this only
    for small data. The model takes significantly more time to generate because only
    one thread is used for computation.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 由于性能原因，H2O DL模型默认情况下不可重复。有一个可以启用的`reproducible`选项，但我们建议只在小型数据上这样做。由于仅使用一个线程进行计算，模型生成所需的时间会显著增加。
- en: GBM
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GBM
- en: GBM is deterministic up to floating-point rounding errors when reproducibility
    criteria are met for single- or multi-node clusters.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 当满足单节点或多节点集群的可重复性标准时，GBM在浮点数舍入误差范围内是确定的。
- en: AutoML
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AutoML
- en: 'To ensure reproducibility in AutoML, the following criteria must be met:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保AutoML中的可重复性，必须满足以下标准：
- en: DL must be excluded.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL必须排除。
- en: The `max_models` constraint rather than `max_runtime_secs` must be used.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应使用`max_models`约束而不是`max_runtime_secs`。
- en: As a rule, time-based constraints are resource-limited. This means that AutoML
    may be able to train more models on one run than another if available compute
    resources differ between runs. Specifying the number of models to build will ensure
    reproducibility.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，基于时间的约束是资源受限的。这意味着如果运行之间的可用计算资源不同，AutoML可能在一次运行中训练比另一次运行更多的模型。指定要构建的模型数量将确保可重复性。
- en: Best practices for reproducibility
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可重复性的最佳实践
- en: 'To ensure reproducibility, think of the four requirement categories emphasized
    earlier: hardware, software, data, and settings. These categories are explained
    in more detail here:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保可重复性，考虑之前强调的四个要求类别：硬件、软件、数据和设置。这些类别在此处有更详细的解释：
- en: '**Hardware**: You should always document the hardware resources the H2O cluster
    is running on—this includes the number of nodes, CPU cores, and threads. (This
    information can be found in the log files.)'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件**：你应该始终记录H2O集群运行的硬件资源——这包括节点数、CPU核心和线程数。（此信息可以在日志文件中找到。）'
- en: '**Software**: You should document the version of H2O-3 or Sparkling Water used.
    (This information can be found in the log files.)'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软件**：你应该记录使用的H2O-3或Sparkling Water的版本。（此信息可以在日志文件中找到。）'
- en: '**Data**: Obviously, you must use the same input data. You should save all
    scripts that were used to process the data prior to model training. All data column
    modifications should be documented (for example, if you converted a numeric column
    to a categorical one).'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据**：显然，你必须使用相同的输入数据。你应该保存所有在模型训练之前用于处理数据的脚本。所有数据列的修改都应记录在案（例如，如果你将数值列转换为分类列）。'
- en: '**Settings**: Save the H2O logs and the H2O binary model. The logs contain
    a wealth of information. More importantly, the binary model contains the H2O version
    (software) and the parameters used to train the model (settings).'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设置**: 保存 H2O 的日志和二进制模型。日志包含大量信息。更重要的是，二进制模型包含了 H2O 版本（软件）以及用于训练模型（设置）的参数。'
- en: Summary
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have rounded out our advanced modeling topics by showing
    how to build H2O models in Spark pipelines with a hands-on sentiment analysis
    modeling example. We summarized the unsupervised learning methods available in
    H2O and showed how to build an anomaly detection model using the isolation forest
    algorithm for a credit card fraud transaction use case. We also reviewed how to
    update models, including refitting versus checkpointing, and showed requirements
    to ensure model reproducibility.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过展示如何在 Spark 管道中使用 H2O 模型以及一个实际的情感分析建模示例，完善了我们的高级建模主题。我们总结了 H2O 中可用的无监督学习方法，并展示了如何使用隔离森林算法构建一个用于信用卡欺诈交易用例的异常检测模型。我们还回顾了如何更新模型，包括重新拟合与检查点之间的区别，并展示了确保模型可复现性的要求。
- en: In [*Chapter 7*](B16721_07_Final_SK_ePub.xhtml#_idTextAnchor127), *Understanding
    ML Models,* we discuss approaches for understanding and reviewing our ML models.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第七章*](B16721_07_Final_SK_ePub.xhtml#_idTextAnchor127)，“理解机器学习模型”，我们讨论了理解和审查我们机器学习模型的方法。
