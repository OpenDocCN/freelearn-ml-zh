- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Building a Data Science Environment Using AWS ML Services
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AWS机器学习服务构建数据科学环境
- en: While some organizations opt to build their own ML platforms using open-source
    technologies, many other organizations prefer to leverage fully managed ML services
    as the foundation for their ML platforms. In this chapter, we will delve into
    the fully managed ML services offered by AWS. Specifically, you will learn about
    **Amazon SageMaker**, and other related services for building a data science environment
    for data scientists. We will examine various components of SageMaker, such as
    SageMaker Studio, SageMaker Training, and SageMaker Hosting. Additionally, we
    will delve into the architectural framework for constructing a data science environment
    and provide a hands-on exercise to guide you through the process.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些组织选择使用开源技术构建自己的机器学习平台，但许多其他组织更喜欢利用完全托管的机器学习服务作为其机器学习平台的基础。在本章中，我们将深入了解AWS提供的完全托管的机器学习服务。具体来说，您将了解**Amazon
    SageMaker**以及为数据科学家构建数据科学环境的相关服务。我们将检查SageMaker的各个组件，如SageMaker Studio、SageMaker
    Training和SageMaker Hosting。此外，我们将深入研究构建数据科学环境的架构框架，并提供一个动手练习，指导您完成该过程。
- en: 'In a nutshell, this chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，本章将涵盖以下主题：
- en: SageMaker overview
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SageMaker概述
- en: Data science environment architecture using SageMaker
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SageMaker构建的数据科学环境架构
- en: Best practices for building a data science environment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建数据科学环境的最佳实践
- en: Hands-on lab – building a data science environment using AWS services
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动手实验室 - 使用AWS服务构建数据科学环境
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, you will need access to an AWS account and have the following
    AWS services for the hands-on lab:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您需要访问AWS账户，并拥有以下AWS服务以进行动手实验室：
- en: Amazon S3
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon S3
- en: Amazon SageMaker
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon SageMaker
- en: Amazon ECR
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon ECR
- en: You will also need to download the dataset from [https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news](https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要从[https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news](https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news)下载数据集。
- en: The sample source code used in this chapter can be found at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter08](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter08).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的示例源代码可在[https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter08](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter08)找到。
- en: SageMaker overview
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SageMaker概述
- en: 'Amazon SageMaker offers ML functionalities that cover the entire ML lifecycle,
    spanning from initial experimentation to production deployment and ongoing monitoring.
    It caters to various roles, such as data scientists, data analysts, and MLOps
    engineers. The following diagram showcases the key SageMaker features that support
    the complete data science journey for different personas:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker提供覆盖整个机器学习生命周期的机器学习功能，从初始实验到生产部署和持续监控。它满足各种角色，如数据科学家、数据分析师和MLOps工程师。以下图表展示了SageMaker的关键功能，这些功能支持不同角色的完整数据科学之旅：
- en: '![A screenshot of a computer  Description automatically generated](img/B20836_08_01.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](img/B20836_08_01.png)'
- en: 'Figure 8.1: SageMaker capabilities'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：SageMaker功能
- en: Within SageMaker, data scientists have access to an array of features and services
    to support different ML tasks. These include Studio notebooks for model building,
    Data Wrangler for visual data preparation, the Processing service for large-scale
    data processing and transformation, the Training service, the Tuning service for
    model tuning, and the Hosting service for model hosting. With these tools, data
    scientists can handle various ML responsibilities, such as data preparation, model
    building and training, model tuning, and conducting model integration testing.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在SageMaker中，数据科学家可以访问一系列功能和服务的组合，以支持不同的机器学习任务。这些包括用于模型构建的Studio笔记本，用于可视数据准备的Data
    Wrangler，用于大规模数据处理和转换的Processing服务，训练服务，用于模型调优的Tuning服务，以及用于模型托管的Hosting服务。有了这些工具，数据科学家可以处理各种机器学习职责，如数据准备、模型构建和训练、模型调优以及进行模型集成测试。
- en: On the other hand, data analysts can utilize SageMaker Canvas, a user-friendly
    model-building service that requires little to no coding. This visual interface
    empowers analysts to train models effortlessly. Additionally, they can use Studio
    notebooks for lightweight data analysis and processing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，数据分析师可以利用SageMaker Canvas，这是一个用户友好的模型构建服务，几乎不需要编写代码。这个可视化界面使分析师能够轻松训练模型。此外，他们还可以使用Studio笔记本进行轻量级的数据分析和处理。
- en: MLOps engineers play a crucial role in managing and governing the ML environment.
    They are responsible for automating ML workflows and can leverage SageMaker Pipelines,
    Model Registry, and endpoint monitoring to achieve this. Furthermore, MLOps engineers
    configure the processing, training, and hosting infrastructure to ensure smooth
    operations for both interactive usage by data scientists and automated operations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps工程师在管理和治理ML环境中扮演着至关重要的角色。他们负责自动化ML工作流程，并可以利用SageMaker Pipelines、模型注册和端点监控来实现这一目标。此外，MLOps工程师配置处理、训练和托管基础设施，以确保数据科学家交互式使用和自动化操作都能顺利进行。
- en: In this chapter, our focus will center on data science environments catered
    specifically to data scientists. Subsequently, in the following chapter, we will
    delve into the administration, governance, and automation of ML infrastructure.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们的重点将集中在专门针对数据科学家的数据科学环境中。随后，在下一章中，我们将深入探讨ML基础设施的治理、管理和自动化。
- en: Data science environment architecture using SageMaker
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker的数据科学环境架构
- en: Data scientists use data science environments to iterate different data science
    experiments with various datasets and algorithms. These environments require essential
    tools like Jupyter Notebook to author and execute code, data processing engines
    for handling large-scale data processing and feature engineering, and model training
    services for training models at scale. Additionally, an effective data science
    environment should include utilities for managing and tracking different experimentation
    runs, enabling researchers to organize and monitor their experiments effectively.
    To manage artifacts such as source code and Docker images, the data scientists
    also need a code repository and a Docker container repository.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家使用数据科学环境，通过不同的数据集和算法迭代不同的数据科学实验。这些环境需要诸如Jupyter Notebook等基本工具来编写和执行代码，数据处理引擎来处理大规模数据处理和特征工程，以及模型训练服务来大规模训练模型。此外，一个有效的数据科学环境还应包括用于管理和跟踪不同实验运行的工具，使研究人员能够有效地组织和监控他们的实验。为了管理诸如源代码和Docker镜像等工件，数据科学家还需要代码仓库和Docker容器仓库。
- en: 'The following diagram illustrates a basic data science environment architecture
    that uses Amazon SageMaker and other supporting services:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了使用Amazon SageMaker和其他支持服务的基本数据科学环境架构：
- en: '![Figure 8.1 – Data science environment architecture ](img/B20836_08_02.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – 数据科学环境架构](img/B20836_08_02.png)'
- en: 'Figure 8.2: Data science environment architecture'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2：数据科学环境架构
- en: SageMaker has multiple data science environments, including Studio, which is
    the primary data science development environment for data scientists, RStudio
    for R users, and Canvas for users who want a no-code/low-code development environment
    for building ML models. You also have access to TensorBoard, a popular tool for
    monitoring and visualizing model metrics such as loss and accuracy.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker拥有多个数据科学环境，包括Studio，这是数据科学家的主要开发环境，RStudio为R用户设计，Canvas为希望构建ML模型的无代码/低代码开发环境用户设计。您还可以访问TensorBoard，这是一个用于监控和可视化模型指标（如损失和准确率）的流行工具。
- en: Now, let’s explore how data scientists can leverage the different components
    of SageMaker to accomplish data science tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探讨数据科学家如何利用SageMaker的不同组件来完成数据科学任务。
- en: Onboarding SageMaker users
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指导SageMaker用户
- en: 'The primary SageMaker user interface for data scientists is SageMaker Studio,
    a data science **integrated development environment** (**IDE**). It provides core
    features such as hosted notebooks for running experiments, as well as access to
    different backend services such as data wrangling, model training, and model hosting
    services from a single user interface. It is the main interface for data scientists
    to interact with most of SageMaker’s functionality. It also provides a Python
    SDK for interacting with its backend services programmatically from Python notebooks
    or scripts. The following diagram shows the key components of SageMaker Studio:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 的主要用户界面是 SageMaker Studio，这是一个数据科学 **集成开发环境**（**IDE**）。它提供了核心功能，如托管笔记本以运行实验，以及从单一用户界面访问不同的后端服务，如数据整理、模型训练和模型托管服务。它是数据科学家与
    SageMaker 大多数功能交互的主要界面。它还提供了一个 Python SDK，用于从 Python 笔记本或脚本中以编程方式与其后端服务交互。以下图显示了
    SageMaker Studio 的关键组件：
- en: '![Figure 8.2 – SageMaker Studio architecture ](img/B20836_08_03.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – SageMaker Studio 架构](img/B20836_08_03.png)'
- en: 'Figure 8.3: SageMaker Studio architecture'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：SageMaker Studio 架构
- en: SageMaker Studio implements the concept of domains to segregate user environments.
    A domain represents a collection of user profiles, each equipped with specific
    configurations, such as the AWS IAM role used when running Jupyter notebook, tags,
    and access permission to SageMaker Canvas. Within each domain, a Studio user can
    access different Studio applications such as JupyterLab, Code Editor, or Canvas
    through a user profile.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Studio 实现了域的概念来隔离用户环境。域代表一组用户配置文件，每个配置文件都配备了特定的配置，例如运行 Jupyter 笔记本时使用的
    AWS IAM 角色、标签以及访问 SageMaker Canvas 的权限。在域内，每个 Studio 用户都可以通过用户配置文件访问不同的 Studio
    应用程序，如 JupyterLab、代码编辑器或 Canvas。
- en: To run certain Studio applications such as JupyterLab and Code Editor, you need
    to create SageMaker Studio spaces. SageMaker Studio spaces are used to manage
    the storage and resource needs of these applications. Each space has a 1:1 relationship
    with an instance of an application, and is composed of resources such as a storage
    volume, application type, and image the application is based on. A space can be
    either private or shared.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行某些 Studio 应用程序，如 JupyterLab 和代码编辑器，您需要创建 SageMaker Studio 空间。SageMaker Studio
    空间用于管理这些应用程序的存储和资源需求。每个空间与一个应用程序实例有一个 1:1 的关系，并包含诸如存储卷、应用程序类型和应用程序基于的镜像等资源。空间可以是私有的也可以是共享的。
- en: To begin using SageMaker, the initial step is to onboard users into SageMaker
    Studio. The onboarding process starts by creating a SageMaker domain, if one doesn’t
    already exist. For the single-user scenario, SageMaker provides a quick domain
    setup option. With the quick setup option, SageMaker automatically configures
    the new domain with default settings for fast setup. For advanced multi-user scenarios,
    SageMaker provides an advanced domain setup option. With the advanced setting,
    you can configure various aspects of the domain for enterprise adoption, such
    as the authentication method, access to different services, networking configuration,
    and data encryption keys. Following that, user profiles are created within the
    domain, and users are granted access to these profiles. Once the user profiles
    are set up, users can launch a Studio environment by selecting their respective
    user profile in a domain. This allows them to start working within SageMaker Studio.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 SageMaker，第一步是将用户引入 SageMaker Studio。如果尚未存在，则通过创建 SageMaker 域来启动注册过程。对于单用户场景，SageMaker
    提供了快速域设置选项。使用快速设置选项，SageMaker 会自动使用默认设置配置新域以实现快速设置。对于高级多用户场景，SageMaker 提供了高级域设置选项。使用高级设置，您可以配置域的各个方面以适应企业采用，例如认证方法、访问不同服务、网络配置和数据加密密钥。随后，在域内创建用户配置文件，并授予用户对这些配置文件的访问权限。一旦用户配置文件设置完成，用户可以通过在域中选择各自的用户配置文件来启动
    Studio 环境。这使他们能够在 SageMaker Studio 中开始工作。
- en: Launching Studio applications
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动 Studio 应用程序
- en: There are multiple applications available inside Studio, including JupyterLab,
    RStudio, Canvas, and Code Editor. You also have the option to launch the previous
    Studio Classic experience.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Studio 内部有多种应用程序可供使用，包括 JupyterLab、RStudio、Canvas 和代码编辑器。您还可以选择启动之前的 Studio
    经典体验。
- en: In the SageMaker Studio environment, initiating a Jupyter notebook is a straightforward
    process. Begin by selecting the JupyterLab application, followed by the creation
    of a JupyterLab space. A space, serving as a named, self-contained, and durable
    storage container, can be attached to the JupyterLab application. During the space
    creation, you have the flexibility to choose the server instance type and the
    Docker image for your JupyterLab. Once the space is successfully created, you
    can proceed to launch the JupyterLab application and initiate a notebook within
    it. The JupyterLab notebook now has an AI-powered programming assistant (Jupyternaut)
    feature that you can use to ask programming questions. For example, you can ask
    questions such as “How do I import a Python library?” or “How do I train a model
    using SageMaker?”.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SageMaker Studio 环境中，启动 Jupyter notebook 是一个简单的过程。首先选择 JupyterLab 应用程序，然后创建一个
    JupyterLab 空间。空间作为一个命名、自包含且持久的存储容器，可以附加到 JupyterLab 应用程序上。在空间创建过程中，您可以灵活地选择服务器实例类型和
    JupyterLab 的 Docker 镜像。一旦空间成功创建，您就可以启动 JupyterLab 应用程序并在其中启动笔记本。JupyterLab 笔记本现在具有一个
    AI 驱动的编程助手（Jupyternaut）功能，您可以使用它来提问编程问题。例如，您可以提问“如何导入 Python 库？”或“如何使用 SageMaker
    训练模型？”。
- en: Similarly, launching the Canvas application involves selecting it within the
    SageMaker Studio environment. As a no-code ML tool, Canvas provides features for
    data preparation, model training, inference, and workflow automation. While primarily
    designed for data analysts with a limited background in data science and ML, it
    also proves to be valuable for experienced data scientists aiming to quickly establish
    baseline models for diverse datasets. Users can leverage Canvas’s ready-to-use
    models for predictions without model building or choose to create custom models
    tailored to specific business problems. Ready-to-use models cover various use
    cases like language detection and document analysis. For custom models, Canvas
    supports building diverse model types using tabular and image data for personalized
    predictions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在 SageMaker Studio 环境中启动 Canvas 应用程序也涉及在其中选择它。作为一个无代码机器学习工具，Canvas 提供了数据准备、模型训练、推理和工作流程自动化的功能。Canvas
    主要面向数据分析师，他们可能在数据科学和机器学习方面背景有限，但它也证明对经验丰富的数据科学家很有价值，他们希望快速为各种数据集建立基线模型。用户可以利用
    Canvas 的现成模型进行预测，而无需构建模型，或者选择创建针对特定业务问题的定制模型。现成模型涵盖了各种用例，如语言检测和文档分析。对于定制模型，Canvas
    支持使用表格和图像数据构建多种模型类型，以进行个性化预测。
- en: Importing data, whether from local or external sources such as S3, Amazon Redshift,
    or Databricks, is supported. Additionally, Canvas facilitates data preparation
    through Data Wrangler and offers generative AI foundation models for initiating
    conversational chats.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 支持从本地或外部源导入数据，如 S3、Amazon Redshift 或 Databricks。此外，Canvas 通过 Data Wrangler 促进数据准备，并提供用于启动对话聊天的生成式
    AI 基础模型。
- en: The Code Editor, built on Code-OSS and Visual Studio Code Open Source, facilitates
    writing, testing, debugging, and running analytics and ML code. To launch the
    Code Editor, simply select the application within Studio and create a dedicated
    private space. This space operates on a single **Amazon Elastic Compute Cloud**
    (**Amazon EC2**) instance for computing and a corresponding **Amazon Elastic Block
    Store** (**Amazon EBS**) volume for storage. All elements within the space, including
    code, Git profiles, and environment variables, are stored on this unified Amazon
    EBS volume.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Code-OSS 和 Visual Studio Code Open Source 构建的代码编辑器，便于编写、测试、调试以及运行分析和机器学习代码。要启动代码编辑器，只需在
    Studio 中选择应用程序并创建一个专用私有空间。此空间在单个 **Amazon Elastic Compute Cloud** (**Amazon EC2**)
    实例上进行计算，并对应一个 **Amazon Elastic Block Store** (**Amazon EBS**) 存储卷。空间内的所有元素，包括代码、Git
    配置文件和环境变量，都存储在这个统一的 Amazon EBS 存储卷上。
- en: Preparing data
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据
- en: Preparing and processing data for model training is an essential step in the
    ML lifecycle to optimize model performance. To do this in the Studio environment,
    you can install and use your preferred library packages directly inside your Studio
    notebook. SageMaker also provides several data wrangling and processing services
    to assist with data preparation, including SageMaker Data Wrangler, and integrated
    data preparation with Amazon EMR and Glue for large-scale data preparation and
    processing.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 准备和加工数据以用于模型训练是机器学习生命周期中优化模型性能的一个关键步骤。在 Studio 环境中，您可以直接在 Studio 笔记本中安装和使用您首选的库包来完成这项工作。SageMaker
    还提供了一些数据整理和加工服务，以协助数据准备，包括 SageMaker Data Wrangler，以及与 Amazon EMR 和 Glue 集成的数据准备，用于大规模数据准备和处理。
- en: Preparing data interactively with SageMaker Data Wrangler
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker Data Wrangler 交互式准备数据
- en: SageMaker Data Wrangler is a fully managed service that helps data scientists
    and engineers prepare and analyze their data for ML. With a graphical user interface,
    it facilitates data preparation tasks, such as data cleaning, feature engineering,
    feature selection, and visualization.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Data Wrangler 是一种完全托管的服务，帮助数据科学家和工程师准备和分析他们的数据以用于机器学习。它提供了一个图形用户界面，简化了数据准备任务，例如数据清洗、特征工程、特征选择和可视化。
- en: To use Data Wrangler, you construct a data flow, a pipeline that connects the
    dataset, transformation, and analysis. When you create and run a data flow, Data
    Wrangler spins up an EC2 instance to run the transformation and analysis. Each
    data flow is associated with an EC2 instance. A data flow normally starts with
    a data import step. Data Wrangler allows you to import data from multiple data
    sources, including Amazon S3, Athena, Amazon Redshift, Amazon EMR, Databricks,
    Snowflake, as well as **Software-as-a-Service** (**SaaS**) platforms such as Datadog,
    GitHub, and Stripe.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Data Wrangler，您需要构建一个数据流，这是一个连接数据集、转换和分析的管道。当您创建并运行一个数据流时，Data Wrangler
    会启动一个 EC2 实例来运行转换和分析。每个数据流都与一个 EC2 实例相关联。数据流通常从数据导入步骤开始。Data Wrangler 允许您从多个数据源导入数据，包括
    Amazon S3、Athena、Amazon Redshift、Amazon EMR、Databricks、Snowflake，以及 **软件即服务**
    (**SaaS**) 平台，如 Datadog、GitHub 和 Stripe。
- en: After the data is imported, you can use Data Wrangler to clean and explore the
    data and perform feature engineering with a built-in transform. You can also use
    the preconfigured visualization templates to understand data and detect outliers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据导入后，您可以使用 Data Wrangler 清洗和探索数据，并使用内置的转换进行特征工程。您还可以使用预配置的可视化模板来理解数据和检测异常值。
- en: 'You can assess the data quality with built-in reports. The following diagram
    shows the flow and architecture of Data Wrangler:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用内置的报告来评估数据质量。以下图显示了 Data Wrangler 的流程和架构：
- en: '![A diagram of a data processing process  Description automatically generated](img/B20836_08_04.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![数据处理流程图  描述自动生成](img/B20836_08_04.png)'
- en: 'Figure 8.4: SageMaker Data Wrangler architecture'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4：SageMaker Data Wrangler 架构
- en: When utilizing Data Wrangler, the first step involves importing sample data
    from various data sources to perform data processing and transformations. After
    completing the necessary transformation steps, you have the option to export a
    recipe. This recipe can then be executed by SageMaker Processing, which processes
    the entire dataset based on the defined transformations.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Data Wrangler 时，第一步涉及从各种数据源导入样本数据以执行数据处理和转换。完成必要的转换步骤后，您可以选择导出一份食谱。然后，SageMaker
    Processing 可以执行这份食谱，根据定义的转换处理整个数据集。
- en: Additionally, you have the option to export the transformation steps into a
    notebook file. This notebook file can be used to initiate a SageMaker Processing
    job, allowing for the data to be processed and transformed. The resulting output
    can be either directed to SageMaker Feature Store or stored in Amazon S3 for further
    usage and analysis.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以选择将转换步骤导出到笔记本文件中。这个笔记本文件可以用来启动 SageMaker Processing 作业，允许对数据进行处理和转换。结果输出可以指向
    SageMaker Feature Store 或存储在 Amazon S3 以供进一步使用和分析。
- en: Preparing data at scale interactively
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交互式大规模数据准备
- en: When dealing with large-scale data analysis, transformation, and preparation
    tasks, SageMaker offers built-in integration with Amazon EMR and AWS Glue. This
    built-in integration allows you to manage and handle large-scale interactive data
    preparation. By leveraging Amazon EMR, you can process and analyze massive datasets,
    while AWS Glue provides a serverless capability to prepare data at scale, as well
    as providing easy access to Glue data catalogs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大规模数据分析、转换和准备任务时，SageMaker提供了与Amazon EMR和AWS Glue的内置集成。这种内置集成允许您管理和处理大规模交互式数据准备。通过利用Amazon
    EMR，您可以处理和分析大量数据集，同时AWS Glue提供了一种无服务器能力，可以在大规模上准备数据，并提供对Glue数据目录的便捷访问。
- en: 'Within the Studio notebook environment, you have the capability to discover
    and establish connections with their existing Amazon EMR clusters. This enables
    them to interactively explore, visualize, and prepare large-scale data for ML
    tasks, leveraging powerful tools such as Apache Spark, Apache Hive, and Presto.
    The following diagram shows how SageMaker integration with EMR works:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在Studio笔记本环境中，您有发现和建立与现有Amazon EMR集群连接的能力。这使得他们能够交互式地探索、可视化和准备用于机器学习任务的大规模数据，利用Apache
    Spark、Apache Hive和Presto等强大工具。以下图显示了SageMaker与EMR的集成方式：
- en: '![A diagram of a software development process  Description automatically generated](img/B20836_08_05.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![软件开发流程图  自动生成的描述](img/B20836_08_05.png)'
- en: 'Figure 8.5: SageMaker integration with EMR'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5：SageMaker与EMR的集成
- en: As a user, you connect to an existing EMR cluster from your Studio notebooks.
    If there is no EMR cluster available, you can self-provision one directly from
    the Studio environment by choosing a predefined template created by system administrators.
    System administrators use AWS Service Catalog to define parameterized templates
    for data scientists to use. Once your notebook is connected to an EMR cluster,
    you can run Spark commands or code inside your notebook cells.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 作为用户，您可以从您的Studio笔记本连接到现有的EMR集群。如果没有可用的EMR集群，您可以直接从Studio环境中通过选择系统管理员创建的预定义模板来自行配置一个。系统管理员使用AWS服务目录为数据科学家定义参数化模板。一旦您的笔记本连接到EMR集群，您就可以在笔记本单元格中运行Spark命令或代码。
- en: AWS Glue interactive sessions is a serverless service that provides you with
    the tools to collect, transform, cleanse, and prepare the data. The built-in integration
    between SageMaker and Glue interactive sessions allows you to run interactive
    sessions for data preparation interactively using Glue as the backend.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue交互式会话是一种无服务器服务，它为您提供了收集、转换、清洗和准备数据的工具。SageMaker与Glue交互式会话之间的内置集成允许您使用Glue作为后端，交互式地运行数据准备会话。
- en: '![A diagram of a diagram  Description automatically generated](img/B20836_08_06.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![一个图例的图  自动生成的描述](img/B20836_08_06.png)'
- en: 'Figure 8.6: SageMaker integration with AWS Glue interactive sessions'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6：SageMaker与AWS Glue交互式会话的集成
- en: To use Glue interactive sessions inside the Studio notebook, you choose the
    built-in Glue PySpark or Glue Spark kernel when you create your Studio notebook.
    After initialization, you can browse the Glue data catalog, run large queries,
    and interactively analyze and prepare data using Spark, all within your Studio
    notebook.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Studio笔记本中使用Glue交互式会话，您在创建Studio笔记本时选择内置的Glue PySpark或Glue Spark内核。初始化后，您可以在Studio笔记本中浏览Glue数据目录、运行大型查询，并使用Spark交互式分析和准备数据。
- en: Both EMR and Glue offer similar capabilities for large-scale interactive data
    processing. Glue interactive sessions are a good choice if you are looking for
    a quick and serverless way to run Spark sessions. EMR provides more robust capabilities
    and the flexibility to configure your compute cluster for optimization.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: EMR和Glue都为大规模交互式数据处理提供了类似的功能。如果您正在寻找一种快速且无服务器的Spark会话运行方式，Glue交互式会话是一个不错的选择。EMR提供了更强大的功能和配置计算集群以进行优化的灵活性。
- en: Processing data as separate jobs
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据处理为单独的任务
- en: 'SageMaker Processing provides a separate infrastructure for large-scale data
    processing such as data cleaning and feature engineering for large datasets as
    separate backend jobs. It can be accessed directly from a notebook environment
    via the SageMaker Python SDK or Boto3 SDK. SageMaker Processing uses Docker container
    images to run data processing jobs. Several built-in containers, such as scikit-learn
    containers and Spark containers, are provided out of the box. You also have the
    option to use your custom containers for processing. The following diagram shows
    the SageMaker Processing architecture:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Processing 为大规模数据处理提供独立的架构，例如为大型数据集提供数据清洗和特征工程作为独立的后端作业。它可以通过 SageMaker
    Python SDK 或 Boto3 SDK 直接从笔记本环境中访问。SageMaker Processing 使用 Docker 容器镜像来运行数据处理作业。提供了一些内置容器，例如
    scikit-learn 容器和 Spark 容器。您还可以选择使用自己的自定义容器进行数据处理。以下图显示了 SageMaker Processing 架构：
- en: '![Figure 8.3 – SageMaker Processing architecture ](img/B20836_08_07.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – SageMaker Processing 架构](img/B20836_08_07.png)'
- en: 'Figure 8.7: SageMaker Processing architecture'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7：SageMaker Processing 架构
- en: When a SageMaker Processing job is initiated, the processing container is pulled
    from Amazon ECR and loaded into the EC2 compute cluster. The data in S3 is copied
    over to the storage attached to the compute nodes for the data processing scripts
    to access and process. Once the processing procedure is completed, the output
    data is copied back to the S3 output location.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当 SageMaker Processing 作业启动时，处理容器从 Amazon ECR 中拉取并加载到 EC2 计算集群中。S3 中的数据被复制到连接到计算节点的存储中，以便数据处理脚本可以访问和处理。一旦处理过程完成，输出数据将被复制回
    S3 输出位置。
- en: SageMaker Processing provides several processors for data processing, including
    a Spark processor, scikit-learn processor, and your own customer processor by
    bringing your containers. SageMaker Processing also supports processors for different
    ML frameworks, including PyTorch, TensorFlow, MXNet, Hugging Face, and XGBoost.
    You can use one of the processors if you need to use library packages as part
    of the processing script.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Processing 提供了多个处理器用于数据处理，包括 Spark 处理器、scikit-learn 处理器和通过引入您的容器实现的客户处理器。SageMaker
    Processing 还支持不同机器学习框架的处理器，包括 PyTorch、TensorFlow、MXNet、Hugging Face 和 XGBoost。如果您需要在处理脚本中使用库包，可以使用这些处理器之一。
- en: Creating, storing, and sharing features
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建、存储和共享特征
- en: When data scientists perform feature engineering for training data preparation,
    they often need to reuse the same features for different model tasks. Further,
    features generated could be used for both training and inference to help reduce
    the training-serving skew.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据科学家进行训练数据准备的特征工程时，他们通常需要为不同的模型任务重用相同的特征。此外，生成的特征可以用于训练和推理，以帮助减少训练-服务偏差。
- en: Amazon SageMaker Feature Store is a service for sharing and managing ML features
    for ML development and serving. Feature Store is a centralized store for features
    and associated metadata so features can be discovered and reused. It has an online
    component as well as an offline component. The online store is used for low-latency
    real-time inference use cases, and the offline store is used for training and
    batch inference. The following diagram shows how Feature Store works. As you can
    see, it is quite similar to the architecture of other open-source alternatives,
    such as Feast, with the key difference being it is fully managed.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker Feature Store 是一个用于分享和管理机器学习特征以用于机器学习开发和服务的服务。Feature Store
    是一个集中存储特征及其相关元数据的地方，以便可以发现和重用特征。它具有在线组件和离线组件。在线存储用于低延迟实时推理用例，离线存储用于训练和批量推理。以下图显示了
    Feature Store 的工作方式。如您所见，它与其他开源替代方案（如 Feast）的架构非常相似，主要区别在于它是完全托管的。
- en: '![A diagram of a software store  Description automatically generated](img/B20836_08_08.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![软件商店的图示  自动生成的描述](img/B20836_08_08.png)'
- en: 'Figure 8.8: SageMaker Feature Store'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8：SageMaker Feature Store
- en: To begin, you first read and process the raw data. The data is then ingested
    into online and offline stores via streaming, or directly to the offline store
    via batch. Feature Store uses the concept called `FeatureGroup` to store and manage
    features. A `FeatureGroup` is a collection of features that are defined via a
    schema in Feature Store, describing the structure and metadata associated with
    a record. You can visualize a feature group as a table in which each column is
    a feature, with a unique identifier for each row.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要阅读并处理原始数据。然后，数据通过流式传输或直接通过批量传输被摄入到在线和离线存储中。特征存储使用名为`FeatureGroup`的概念来存储和管理特征。`FeatureGroup`是一组通过特征存储中的模式定义的特征，描述了与记录相关的结构和元数据。你可以将特征组视为一个表格，其中每一列是一个特征，每一行都有一个唯一的标识符。
- en: The online store is specifically optimized for real-time predictions, offering
    low-latency reads and high-throughput writes. It is ideal for scenarios that require
    quick access to feature data for real-time inference. On the other hand, the offline
    store is designed for batch predictions and model training purposes. It operates
    as an append-only store, allowing historical feature data to be stored and accessed.
    The offline store is particularly useful for storing and serving features during
    exploration and model training processes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在线存储专门针对实时预测进行了优化，提供低延迟读取和高吞吐量写入。它非常适合需要快速访问特征数据进行实时推理的场景。另一方面，离线存储是为批量预测和模型训练而设计的。它作为一个只追加存储操作，允许存储和访问历史特征数据。离线存储在探索和模型训练过程中存储和提供特征特别有用。
- en: Training ML models
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练机器学习模型
- en: Once you have prepared the training data, you are all set to train the model.
    Data scientists can use the SageMaker Training service to handle model training
    that requires dedicated training infrastructure and instance types. The Training
    service is also ideal for large-scale distributed training using multiple nodes.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你准备好了训练数据，你就可以开始训练模型。数据科学家可以使用SageMaker训练服务来处理需要专用训练基础设施和实例类型的模型训练。训练服务也适用于使用多个节点进行的大规模分布式训练。
- en: 'To start training, you first store your training data in storage such as Amazon
    S3, Amazon EFS, or Amazon FSx. You choose different storage options based on your
    specific requirements, such as cost and latency. S3 is the most common one, suitable
    for the majority of model training needs. With S3, you have multiple modes to
    ingest data from S3 to the training infrastructure:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始训练，你首先需要将你的训练数据存储在Amazon S3、Amazon EFS或Amazon FSx等存储中。你根据自己的具体需求选择不同的存储选项，例如成本和延迟。S3是最常见的一种，适合大多数模型训练需求。使用S3，你有多种模式将数据从S3摄入到训练基础设施中：
- en: '**File mode**: This is the default input model whereby SageMaker downloads
    the training data from S3 to a local directory in the training instance. The training
    starts when the full dataset has been downloaded. With this mode, the training
    instance must have sufficient local storage space to fit the entire dataset.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文件模式**：这是默认输入模式，SageMaker将训练数据从S3下载到训练实例的本地目录。当完整数据集下载完毕后开始训练。使用此模式，训练实例必须有足够的本地存储空间来容纳整个数据集。'
- en: '**Pipe mode**: With this mode, data is streamed directly from an S3 data source.
    This can provide faster start times and better throughput than the file mode.
    This model also reduces the size of the storage volumes attached to the training
    instances. It only needs enough space to store the final model artifacts.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道模式**：使用此模式，数据直接从S3数据源流式传输。这可以提供比文件模式更快的启动时间和更好的吞吐量。此模式还可以减小附加到训练实例的存储卷的大小。它只需要足够的空间来存储最终模型工件。'
- en: '**Fast file mode**: This mode is the newer and simpler-to-use replacement of
    pipe mode. At the start of the training, the data files are identified but not
    downloaded. Training can start without waiting for the entire dataset to download.
    Fast file mode exposes S3 objects using a POSIX-compliant file system interface,
    as if the files are available on the local disk of the training instances. It
    streams the S3 data on demand as the training script consumes it, meaning that
    you don’t need to fit the training data into the training instance storage.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**快速文件模式**：这种模式是管道模式的较新且更易于使用的替代品。在训练开始时，数据文件被识别但尚未下载。训练可以在整个数据集下载完毕之前开始。快速文件模式使用POSIX兼容的文件系统接口公开S3对象，就像文件在训练实例的本地磁盘上可用一样。它按需流式传输S3数据，这意味着你不需要将训练数据拟合到训练实例存储中。'
- en: In addition to S3, you can also store your training data in Amazon FSx for Lustre.
    You want to use FSx when you have high-throughput and low-latency data retrieval
    requirements for your training data. With FSx, you can scale to hundreds of gigabytes
    of throughput and millions of **Inputs/Outputs Operations Per Second** (**IOPS**)
    with low-latency file retrieval. When a training job is started, it mounts FSx
    to the training instance file system as a local drive. FSx allows the training
    job to run faster as it takes less time to read the files and it does not need
    to copy data to the training instance’s local storage like S3 file mode. EFS provides
    similar functionality as FSx, but at a lower throughput and higher latency.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 除了S3，您还可以将训练数据存储在Amazon FSx for Lustre中。当您对训练数据有高吞吐量和低延迟的数据检索需求时，您可以使用FSx。使用FSx，您可以通过低延迟的文件检索扩展到数百GB的吞吐量和数百万**每秒输入/输出操作**（**IOPS**）。当启动训练作业时，它将FSx挂载到训练实例文件系统作为本地驱动器。FSx允许训练作业运行得更快，因为它读取文件所需的时间更少，并且不需要像S3文件模式那样将数据复制到训练实例的本地存储。EFS提供了与FSx类似的功能，但吞吐量较低，延迟较高。
- en: If you already have data in your EFS system, you can directly launch a training
    job using data in EFS without data movement. This reduces the training start time.
    Compared to FSx, EFS is less costly but has lower throughput and higher latency.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经在EFS系统中有了数据，您可以直接使用EFS中的数据启动训练作业，无需数据迁移。这减少了训练启动时间。与FSx相比，EFS成本较低，但吞吐量较低，延迟较高。
- en: Once the training data is ready in the storage system of your choice, you can
    kick off the training job using the AWS Boto3 SDK or the SageMaker Python SDK.
    To run the training job, you need to provide configuration details such as the
    training Docker image’s URL from ECR, the training script location, the framework
    version, the training dataset location, and the S3 model output location, as well
    as infrastructure details such as the compute instance type and number, as well
    as networking details. The following sample code shows how to configure and kick
    off a training job using the SageMaker SDK.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练数据已准备好在您选择的存储系统中，您可以使用AWS Boto3 SDK或SageMaker Python SDK启动训练作业。要运行训练作业，您需要提供配置细节，例如来自ECR的训练Docker镜像的URL、训练脚本位置、框架版本、训练数据集位置以及S3模型输出位置，以及基础设施细节，例如计算实例类型和数量，以及网络细节。以下示例代码展示了如何使用SageMaker
    SDK配置并启动一个训练作业。
- en: 'The SageMaker Training service uses containers as a core technology for training
    management. All training jobs are executed inside containers hosted on SageMaker
    training infrastructure. The following diagram shows the architecture of the SageMaker
    Training service:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker训练服务使用容器作为训练管理的关键技术。所有训练作业都在SageMaker训练基础设施上托管的容器内执行。以下图示展示了SageMaker训练服务的架构：
- en: '![Figure 8.4 – SageMaker Training Service architecture ](img/B20836_08_09.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – SageMaker训练服务架构](img/B20836_08_09.png)'
- en: 'Figure 8.9: SageMaker Training Service architecture'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9：SageMaker训练服务架构
- en: SageMaker provides various managed containers for model training using different
    ML algorithms and ML frameworks. Firstly, there is a list of built-in containerized
    algorithms for different ML tasks such as computer vision, NLP, forecasting, and
    common tabular regression and classifications. With these built-in algorithms,
    you only need to provide the training data location. SageMaker also provides a
    list of managed framework containers, such as containers for scikit-learn, TensorFlow,
    and PyTorch. With a managed framework container, in addition to providing data
    sources and infrastructure specifications, you also need to provide a training
    script that runs the model training loop.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker为使用不同机器学习算法和机器学习框架进行模型训练提供了各种托管容器。首先，有一系列内置的容器化算法，用于不同的机器学习任务，如计算机视觉、自然语言处理、预测以及常见的表格回归和分类。使用这些内置算法，您只需提供训练数据位置。SageMaker还提供了一系列托管框架容器，例如scikit-learn、TensorFlow和PyTorch的容器。使用托管框架容器，除了提供数据源和基础设施规范外，您还需要提供一个运行模型训练循环的训练脚本。
- en: If the built-in algorithms and framework containers do not meet your needs,
    you can bring your own custom container for model training. This container needs
    to contain the model training scripts, as well as all the dependencies required
    to run the training loop.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果内置算法和框架容器不能满足您的需求，您可以为模型训练带来自己的自定义容器。这个容器需要包含模型训练脚本以及运行训练循环所需的所有依赖项。
- en: By default, SageMaker tracks all training jobs and their associated metadata,
    such as algorithms, input training dataset URLs, hyperparameters, and model output
    locations. Training jobs also emit system metrics and algorithm metrics to AWS
    CloudWatch for monitoring. Training logs are also sent to CloudWatch Logs for
    inspection and analysis needs. This metadata is critical for lineage tracking
    and reproducibility.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，SageMaker跟踪所有训练作业及其关联的元数据，例如算法、输入训练数据集URL、超参数和模型输出位置。训练作业还会向AWS CloudWatch发送系统指标和算法指标以进行监控。训练日志也会发送到CloudWatch日志以供检查和分析。这些元数据对于谱系追踪和可重复性至关重要。
- en: Tuning ML models
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整机器学习模型
- en: 'To optimize the model’s performance, you also need to try different hyperparameters,
    such as a learning rate for gradient descent and model training. An algorithm
    can contain a large number of hyperparameters, and tuning them manually would
    be a highly labor-intensive task. The SageMaker Tuning service works with SageMaker
    training jobs to tune model training hyperparameters automatically. The following
    four types of hyperparameter tuning strategies are supported by the service:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化模型性能，您还需要尝试不同的超参数，例如梯度下降和模型训练的学习率。一个算法可能包含大量超参数，手动调整它们将是一项劳动密集型任务。SageMaker调优服务与SageMaker训练作业协同工作，以自动调整模型训练超参数。该服务支持以下四种类型的超参数调优策略：
- en: '**Grid search**: Grid search is an exhaustive search technique that systematically
    explores a predefined set of hyperparameter values over a specified range for
    each hyperparameter. It creates a grid of all possible combinations and evaluates
    the model’s performance for each configuration using cross-validation or a validation
    set. Grid search is highly focused, but it can be highly inefficient due to the
    large number of combinations, especially when the hyperparameter dimension is
    high.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网格搜索**：网格搜索是一种穷举搜索技术，它系统地探索每个超参数在指定范围内的预定义超参数值集。它创建了一个所有可能组合的网格，并使用交叉验证或验证集评估每个配置的模型性能。网格搜索非常专注，但由于组合数量庞大，尤其是当超参数维度较高时，它可能非常低效。'
- en: '**Random search**: Random search is a popular and effective hyperparameter
    optimization technique that offers an alternative to exhaustive methods like grid
    search. Unlike grid search, which evaluates all possible combinations of hyperparameters
    within predefined ranges, random search takes a more stochastic approach. Instead
    of covering the entire search space systematically, it randomly samples hyperparameter
    values from defined distributions for each hyperparameter. Random search can be
    more efficient, compared to grid search; however, it might not always find the
    best combination of hyperparameters.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机搜索**：随机搜索是一种流行且有效的超参数优化技术，它为像网格搜索这样的穷举方法提供了一种替代方案。与网格搜索不同，后者在预定义的范围内评估所有可能的超参数组合，而随机搜索采取了一种更随机的策略。它不是系统地覆盖整个搜索空间，而是为每个超参数从定义的分布中随机采样超参数值。与网格搜索相比，随机搜索可能更有效率；然而，它可能并不总是能找到最佳的超参数组合。'
- en: '**Bayesian search**: This is where the hyperparameter search is treated like
    a regression problem, where the inputs for regression are the values of the hyperparameters
    and the output is the model’s performance metric once the model has been trained
    using the input values. The tuning service uses the values that have been collected
    from the training jobs to predict the next set of values that would produce model
    improvement. Compared to random search, Bayesian search is more efficient as it
    uses a model to focus on the most promising search spaces of hyperparameters.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贝叶斯搜索**：在这里，超参数搜索被处理为一个回归问题，其中回归的输入是超参数的值，输出是在使用输入值训练模型后模型性能指标。调优服务使用从训练作业中收集的值来预测下一组将产生模型改进的值。与随机搜索相比，贝叶斯搜索更有效率，因为它使用模型来关注最有希望的超参数搜索空间。'
- en: '**Hyperband**: Hyperband leverages the concept of bandit algorithms and successive
    halving to make the search process more effective and resource-efficient. It begins
    by randomly sampling a large number of hyperparameter configurations and then
    dividing them into multiple “bands” or sets. In each band, the configurations
    are evaluated through a predefined number of iterations, eliminating poorly performing
    ones at regular intervals. The surviving configurations are then promoted to the
    next band, where they are given additional iterations to fine-tune their performance.
    This process continues, gradually increasing the resources allocated to promising
    configurations while efficiently discarding underperforming ones. Hyperband is
    known to be more efficient than other methods, and it can find good combinations
    of hyperparameters with fewer iterations.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hyperband**：Hyperband利用了bandit算法和连续减半的概念，使搜索过程更加有效和资源高效。它首先随机采样大量超参数配置，然后将它们分成多个“波段”或集合。在每个波段中，配置通过预定义的迭代次数进行评估，定期消除表现不佳的配置。然后，幸存下来的配置被提升到下一个波段，在那里它们将获得额外的迭代次数以微调其性能。这个过程持续进行，逐渐增加分配给有希望的配置的资源，同时有效地丢弃表现不佳的配置。Hyperband被认为比其他方法更有效率，并且可以在更少的迭代中找到良好的超参数组合。'
- en: 'The SageMaker tuning service works with SageMaker training jobs to optimize
    the hyperparameters. It works by sending different input hyperparameter values
    to the training jobs and picking the hyperparameter values that return the best
    model metrics. The following diagram shows how the SageMaker tuning service works:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker调优服务与SageMaker训练作业协同工作以优化超参数。它通过向训练作业发送不同的输入超参数值并选择返回最佳模型指标的超参数值来实现。以下图表展示了SageMaker调优服务的工作原理：
- en: '![A diagram of a training process  Description automatically generated](img/B20836_08_10.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![训练过程的图  自动生成的描述](img/B20836_08_10.png)'
- en: 'Figure 8.10: SageMaker tuning architecture'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10：SageMaker调优架构
- en: To use the SageMaker tuning service, you create a tuning job and specify configuration
    details such as tuning strategy, objective metric to optimize, hyperparameters
    to tune and their ranges, max number of training jobs to run, and the number of
    jobs to run in parallel. Subsequently, the tuning job will kick off a number of
    training jobs. Depending on the tuning strategy, the tuning job will pass different
    hyperparameters to the training jobs to execute. The training metrics from the
    training jobs will be used by the tuning job to determine what hyperparameters
    to use in order to optimize the model performance.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用SageMaker调优服务，你需要创建一个调优作业并指定配置细节，例如调优策略、要优化的目标指标、要调整的超参数及其范围、要运行的最大训练作业数以及并行运行的作业数。随后，调优作业将启动多个训练作业。根据调优策略，调优作业将传递不同的超参数到训练作业以执行。训练作业的训练指标将被调优作业用来确定使用哪些超参数以优化模型性能。
- en: Deploying ML models for testing
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署ML模型进行测试
- en: 'Data scientists normally do not deploy models for client application consumption
    directly. However, data scientists sometimes need to test the performance of models
    trained with the SageMaker training service, and they will need to deploy these
    models to an API endpoint for testing. This is especially needed for models of
    large sizes where they can’t be evaluated in a notebook instance. SageMaker provides
    a dedicated service for model hosting and its architecture is illustrated as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家通常不会直接部署模型供客户端应用使用。然而，数据科学家有时需要测试使用SageMaker训练服务训练的模型的表现，并且他们需要将这些模型部署到API端点进行测试。这对于大型模型尤其必要，因为它们无法在笔记本实例中进行评估。SageMaker提供了一个专门的服务用于模型托管，其架构如下所示：
- en: '![A diagram of a server  Description automatically generated](img/B20836_08_11.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![服务器的图  自动生成的描述](img/B20836_08_11.png)'
- en: 'Figure 8.11: SageMaker hosting architecture'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11：SageMaker托管架构
- en: 'The SageMaker Hosting service offers multiple model inference options for different
    needs, from real-time inference to batch inference. The following are some options
    available for data scientists to use for model hosting evaluation and testing:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker托管服务为不同的需求提供了多种模型推理选项，从实时推理到批量推理。以下是一些数据科学家用于模型托管评估和测试的可用选项：
- en: One of the most common model serving use cases is low-latency prediction in
    real time on a sustained basis. For this use case, you should consider the **real-time
    inference** option from the SageMaker hosting service. SageMaker real-time inference
    provides multiple options for model hosting, including single-model hosting, multiple-model
    hosting in a single container behind one endpoint, and multiple-model hosting
    using different containers behind one endpoint.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最常见的模型服务用例之一是在持续的基础上进行低延迟的实时预测。对于这个用例，你应该考虑SageMaker托管服务中的**实时推理**选项。SageMaker实时推理提供了多种模型托管选项，包括单个模型托管、单个端点后单个容器中的多个模型托管，以及单个端点后使用不同容器的多个模型托管。
- en: Sometimes, you have models that are used for intermittent predictions with idle
    periods between traffic bursts. For this type of requirement, you can consider
    the **Serverless** option from the SageMaker hosting service. The key benefit
    of using Serverless Inference is the removal of infrastructure configuration and
    management overhead. It is also more cost effective since you don’t need to pay
    for infrastructure when it is not used, unlike real-time inference, where you
    need to pay for the infrastructure whether there is inference traffic or not.
    However, there might be a delay caused by cold-starts with Serverless Inference
    when the model is invoked for the first time or there is extended idle time between
    invocations to allow SageMaker Serverless Inference to launch instances.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时，你有一些模型用于间歇性预测，在流量高峰之间的空闲期间。对于这种需求，你可以考虑SageMaker托管服务的**无服务器**选项。使用无服务器推理的关键好处是消除了基础设施配置和管理开销。它也更经济高效，因为你不需要在未使用时支付基础设施费用，与实时推理不同，无论是否有推理流量，都需要支付基础设施费用。然而，当模型首次被调用或调用之间存在较长的空闲时间以允许SageMaker无服务器推理启动实例时，可能会因为冷启动而出现延迟。
- en: Sometimes, the payload size of an inference can be very large, and it takes
    a long time to generate the prediction. In this case, a real-time endpoint won’t
    work due to the large payload size and extended time for the inference. For this
    type of use case, you can consider **the asynchronous inference** option of SageMaker
    hosting. Asynchronous inference queues the incoming requests and processes them
    asynchronously (as the name suggests). When using asynchronous inference, the
    input data and prediction output are stored in S3 instead of sending the payload
    and getting the response directly from the endpoint API. When the prediction is
    complete, you get a notification from the AWS SNS service.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时，推理的负载大小可能非常大，生成预测需要很长时间。在这种情况下，由于负载大小大和推理时间延长，实时端点将无法工作。对于这种类型的用例，你可以考虑SageMaker托管的**异步推理**选项。异步推理将传入的请求排队，并异步处理它们（正如其名称所暗示的）。当使用异步推理时，输入数据和预测输出存储在S3中，而不是直接从端点API发送负载并获取响应。当预测完成时，你会从AWS
    SNS服务收到通知。
- en: Another model inference pattern is **batch inference**. This is when you have
    a large number of inferences to do, and they don’t need individual predictions
    to be generated and returned. For example, you might need to run a propensity-to-buy
    model for a large number of users and store the output in a database for downstream
    consumption. For this usage pattern, you can consider the SageMaker Batch Transform
    feature for model inference. Batch inference is also more cost effective as you
    only need to spin up the infrastructure when running the batch job.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种模型推理模式是**批量推理**。这是当你有大量推理要做，并且不需要为每个预测生成和返回单独预测时。例如，你可能需要为大量用户运行购买倾向模型，并将输出存储在数据库中以供下游使用。对于这种使用模式，你可以考虑SageMaker批量转换功能进行模型推理。批量推理也更经济高效，因为你只需要在运行批量作业时启动基础设施。
- en: Having discussed the various tasks involved in completing an ML project, ranging
    from data preparation to model deployment using the different SageMaker features,
    now let’s briefly talk about the need for automation. Data scientists frequently
    engage in iterative experimentation and model development, involving different
    datasets, new features, and various training scripts. Keeping track of configurations
    and model metrics for each run becomes essential. To streamline and automate these
    repetitive tasks, automation pipelines can be constructed, supporting various
    ML processes like data processing, model training, and model testing.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论了完成机器学习项目所涉及的各种任务后，从数据准备到使用不同的SageMaker功能进行模型部署，现在让我们简要谈谈自动化的必要性。数据科学家经常进行迭代实验和模型开发，涉及不同的数据集、新特征和多种训练脚本。跟踪每次运行的配置和模型指标变得至关重要。为了简化并自动化这些重复性任务，可以构建自动化管道，支持各种机器学习过程，如数据处理、模型训练和模型测试。
- en: There are several tools available for automation and orchestration, including
    SageMaker’s Pipelines feature. It allows the creation of a **Directed Acyclic
    Graph** (**DAG**) to efficiently orchestrate and automate ML workflows. SageMaker
    Pipelines shares similarities with Airflow, an open-source workflow orchestration
    tool discussed in *Chapter 7*, *Open-Source ML Platforms*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种自动化和编排工具可用，包括SageMaker的管道功能。它允许创建一个**有向无环图**（**DAG**），以有效地编排和自动化机器学习工作流程。SageMaker管道与第7章中讨论的开源工作流编排工具Airflow有相似之处。
- en: Additionally, AWS Step Functions serves as an alternative option for building
    automated workflow orchestration, providing flexibility and scalability to accommodate
    diverse ML tasks. By leveraging these tools, data scientists can enhance efficiency,
    reproducibility, and organization in their ML workflows.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，AWS Step Functions作为构建自动化工作流编排的替代选项，提供了灵活性和可扩展性，以适应各种机器学习任务。通过利用这些工具，数据科学家可以提高他们在机器学习工作流程中的效率、可重复性和组织性。
- en: Best practices for building a data science environment
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建数据科学环境的最佳实践
- en: 'Data science environments are meant for data scientists to perform quick experimentations
    using a wide range of ML frameworks and libraries. The following are some best
    practices to follow when providing such an environment for your data scientists:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学环境旨在让数据科学家使用广泛的机器学习框架和库进行快速实验。以下是在为数据科学家提供此类环境时需要遵循的一些最佳实践：
- en: '**Run large-scale model training using the SageMaker Training service instead
    of Studio notebooks**: SageMaker Studio notebooks are meant for quick experimentation
    with small datasets. While it is possible to provision large EC2 instances for
    certain large model training jobs, it is not cost effective to always keep a large
    EC2 instance running for a notebook all the time.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用SageMaker训练服务而不是Studio笔记本进行大规模模型训练**：SageMaker Studio笔记本旨在对小型数据集进行快速实验。虽然为某些大型模型训练作业配置大型EC2实例是可能的，但始终保持大型EC2实例运行以供笔记本使用并不经济。'
- en: '**Abstract infrastructure configuration details from data scientists**: There
    are many infrastructure configurations to consider when using SageMaker, such
    as networking configuration, IAM roles, encryption keys, EC2 instance types, and
    storage options. To make the lives of data scientists easier, abstract these details
    away from the data scientists. For example, instead of having the data scientists
    enter specific networking configurations, have those details stored as environment
    variables or custom SDK options for data scientists to choose from.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从数据科学家那里抽象出基础设施配置细节**：在使用SageMaker时，有许多基础设施配置需要考虑，例如网络配置、IAM角色、加密密钥、EC2实例类型和存储选项。为了使数据科学家的生活更轻松，将这些细节抽象出来。例如，而不是让数据科学家输入特定的网络配置，将这些细节存储为环境变量或数据科学家可以选择的自定义SDK选项。'
- en: '**Create self-service provisioning**: To prevent provisioning bottlenecks,
    consider building a self-service provisioning capability to streamline user onboarding.
    For example, use AWS Service Catalog to create an ML product for automated user
    onboarding.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建自助服务配置**：为了防止配置瓶颈，考虑构建自助服务配置能力以简化用户入职流程。例如，使用AWS服务目录创建一个用于自动化用户入职的机器学习产品。'
- en: '**Use Studio notebook local mode for quick model training job testing**: SageMaker
    has support for local mode, meaning you can mimic running training jobs locally
    in your Studio notebook. With SageMaker Training jobs, there is an overhead of
    spinning up separate infrastructure. Running these tests locally can help speed
    up experimentation.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 Studio 笔记本本地模式进行快速模型训练作业测试**：SageMaker 支持本地模式，这意味着你可以在 Studio 笔记本中模拟本地运行训练作业。使用
    SageMaker 训练作业时，启动单独基础设施会有额外的开销。在本地运行这些测试可以帮助加快实验速度。'
- en: '**Set up guardrails**: This helps prevent data scientists from making mistakes
    such as using the wrong instance types for model training or forgetting to use
    data encryption keys. You can use AWS service control policies to help with guardrail
    management.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设置安全措施**：这有助于防止科学家犯诸如为模型训练使用错误的实例类型或忘记使用数据加密密钥等错误。你可以使用 AWS 服务控制策略来帮助进行安全措施管理。'
- en: '**Clean up unused resources**: Regularly review and clean up unused notebooks,
    endpoints, and other resources to avoid unnecessary costs.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清理未使用的资源**：定期审查并清理未使用的笔记本、端点和其它资源，以避免不必要的费用。'
- en: '**Use Spot instances**: For cost optimization, consider using Amazon EC2 Spot
    Instances for training jobs if possible. Spot instances can significantly reduce
    training costs while maintaining high performance. However, since Spot instances
    can be taken away unexpectedly, it is important to enable training checkpoints,
    so training can resume at the last checkpoint instead of restarting training from
    the beginning.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 Spot 实例**：为了成本优化，如果可能的话，考虑使用 Amazon EC2 Spot 实例进行训练作业。Spot 实例可以显著降低训练成本，同时保持高性能。然而，由于
    Spot 实例可能会意外被回收，因此启用训练检查点非常重要，这样训练可以从最后一个检查点恢复，而不是从头开始重新训练。'
- en: '**Use built-in algorithms and managed containers for training**: SageMaker
    provides a list of built-in algorithms for different ML tasks and managed training
    containers for different ML frameworks. Taking advantage of these pre-existing
    resources can substantially reduce the engineering effort required, eliminating
    the need to build your own algorithms from scratch.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用内置算法和托管容器进行训练**：SageMaker 为不同的机器学习任务提供了一系列内置算法，并为不同的机器学习框架提供了托管训练容器。利用这些现有资源可以大幅减少所需的工程工作量，消除从头开始构建自己的算法的需求。'
- en: '**Build automated pipelines for repeatable ML experimentation, model building,
    and model testing**: Having an automated pipeline can greatly reduce the manual
    effort and improve the tracking of different experiments. Consider different orchestration
    technology options based on your technology standards and preferences.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建可重复的机器学习实验、模型构建和模型测试的自动化管道**：拥有自动化管道可以大大减少手动工作量并提高不同实验的跟踪。根据你的技术标准和偏好，考虑不同的编排技术选项。'
- en: By adhering to these best practices, you can make the most of SageMaker Studio’s
    capabilities, streamline your ML workflows, and ensure cost-effective and secure
    usage of the platform.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这些最佳实践，你可以充分利用 SageMaker Studio 的功能，简化你的机器学习工作流程，并确保平台的使用既经济高效又安全。
- en: Hands-on exercise – building a data science environment using AWS services
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践练习 - 使用 AWS 服务构建数据科学环境
- en: The primary goal of this lab is to offer practical, hands-on experience with
    the various SageMaker tools. Once you are familiar with the core functionality
    in this lab, you should independently explore other features such as Code Editor
    and RStudio.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 本实验的主要目标是提供使用各种 SageMaker 工具的实用、动手经验。一旦你熟悉了本实验中的核心功能，你应该独立探索其他功能，例如代码编辑器和 RStudio。
- en: Problem statement
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题陈述
- en: As an ML solutions architect, you have been tasked with building a data science
    environment on AWS for the data scientists in the equity research department.
    The data scientists in the equity research department have several NLP problems,
    such as detecting the sentiment of financial phrases. Once you have created the
    environment for the data scientists, you also need to build a proof of concept
    to show the data scientists how to build and train an NLP model using the environment.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名机器学习解决方案架构师，你被分配在 AWS 上为股票研究部门的科学家们构建数据科学环境。股票研究部门的科学家们有几个自然语言处理问题，例如检测金融短语的情感。一旦为科学家们创建了环境，你还需要构建一个概念验证，向科学家们展示如何使用该环境构建和训练
    NLP 模型。
- en: Dataset description
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集描述
- en: 'The data scientists have indicated that they like to use the BERT model to
    solve sentiment analysis problems, and they plan to use the financial phrases
    dataset to establish some initial benchmarks for the model: [https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news](https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家表示，他们喜欢使用BERT模型来解决情感分析问题，并计划使用金融短语数据集为模型建立一些初始基准：[https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news](https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news)。
- en: Lab instructions
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验说明
- en: In this lab, you will begin by establishing a SageMaker domain and user profile
    to facilitate user onboarding to SageMaker Studio. Additionally, the lab encompasses
    learning to train a deep learning model through both the JupyterLab notebook directly
    and the SageMaker training job service.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，您将首先建立SageMaker域和用户资料，以促进用户加入SageMaker工作室。此外，实验还包括通过JupyterLab笔记本直接和SageMaker训练作业服务学习如何训练深度学习模型。
- en: The final step will involve deploying the trained model utilizing the SageMaker
    hosting service. You will also explore SageMaker Canvas to learn how to train
    an ML model without any coding. After the lab, you will be able to use SageMaker
    as a data science tool for various experimentation, model training, and model
    deployment tasks. SageMaker has many other features.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步将涉及使用SageMaker托管服务部署训练好的模型。您还将探索SageMaker Canvas，以了解如何在不编写任何代码的情况下训练机器学习模型。实验结束后，您将能够将SageMaker用作数据科学工具，用于各种实验、模型训练和模型部署任务。SageMaker还有许多其他功能。
- en: Let’s dive in!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Setting up SageMaker Studio
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置SageMaker工作室
- en: 'Follow these steps to set up a SageMaker Studio environment:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤设置SageMaker工作室环境：
- en: To create a SageMaker Studio environment, we need to set up a domain and a user
    profile in the respective AWS Region. Navigate to the SageMaker management console,
    once you’ve logged in to the AWS Management Console, and click on the **Studio**
    link on the left.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建SageMaker工作室环境，我们需要在相应的AWS区域设置一个域和一个用户资料。导航到SageMaker管理控制台，登录AWS管理控制台后，点击左侧的**工作室**链接。
- en: On the right-hand side of the screen, click on the **Create a SageMaker Domain**
    button. Choose the **Setup for Single User** option and click on **Set up.** It
    will take a few minutes for the domain and a default user profile to be created.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕右侧，点击**创建SageMaker域**按钮。选择**单用户设置**选项，然后点击**设置**。创建域和默认用户资料可能需要几分钟时间。
- en: 'To start the Studio environment for the newly created user, click on the **Studio**
    link again, select the user profile you just created, and click on **Open Studio**
    to launch Studio. It will take a few minutes for the Studio environment to appear.
    Once everything is ready, you will see a screen that is similar to the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动为新创建的用户创建的工作室环境，请再次点击**工作室**链接，选择您刚刚创建的用户资料，然后点击**打开工作室**以启动工作室。工作室环境出现可能需要几分钟时间。一旦一切准备就绪，您将看到一个类似于以下屏幕的界面：
- en: '![](img/B20836_08_12.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20836_08_12.png)'
- en: 'Figure 8.12: Studio UI'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12：工作室UI
- en: Launching a JupyterLab notebook
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启动JupyterLab笔记本
- en: 'Now, we need to launch a JupyterLab application within the SageMaker Studio
    UI so we can have a Jupyter Notebook environment for authoring model-building
    scripts and training ML models. Continue with the following steps:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要在SageMaker工作室UI中启动一个JupyterLab应用，以便我们有一个用于编写模型构建脚本和训练机器学习模型的Jupyter Notebook环境。继续以下步骤：
- en: Select the **JupterLab** application under the **Applications** section in the
    left navigation pane.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧导航面板的**应用程序**部分下选择**JupyterLab**应用。
- en: Click on **Create JupyterLab Space** on the right-hand side to create a space
    for the JupyterLab. Provide a name for the space on the subsequent pop-up screen
    and click **Create space**.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在右侧点击**创建JupyterLab空间**以创建JupyterLab的空间。在随后的弹出屏幕上为空间提供一个名称，然后点击**创建空间**。
- en: On the next screen, change the storage to **20 GB**, select **ml.g5.xLarge**,
    and keep all other configurations the same, and then click on **Run Space**.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一屏幕上，将存储空间更改为**20 GB**，选择**ml.g5.xLarge**，并保持所有其他配置不变，然后点击**运行空间**。
- en: It will take some time for the space to be created. Once it is ready, click
    on **Open JupyterLab** to launch it, and it will be launched in a separate tab.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建空间可能需要一些时间。一旦就绪，点击**打开JupyterLab**以启动它，它将在一个单独的标签页中启动。
- en: Training the BERT model in the Jupyter notebook
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Jupyter笔记本中训练BERT模型
- en: In this part of the hands-on exercise, we will train a financial sentiment analysis
    NLP model using the BERT transformer, which we learned about in *Chapter 3*, *Exploring
    ML Algorithms*. To get started, create a new notebook to author our code by selecting
    **File** > **New** > **Notebook** from the menu dropdown. When prompted to select
    a kernel, pick **Python 3 (ipykernel)**. You can rename the file so that it has
    a more meaningful name by selecting **File** > **Rename Notebook** from the menu.
    Download the dataset from Kaggle at [https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news](https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news).
    Note that you will need a Kaggle account to download it. Once it’s been downloaded,
    you should see an `archive.zip` file. Unzip the file using unzip tool on your
    local machine.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节动手练习中，我们将使用BERT转换器训练一个金融情感分析NLP模型，这是我们在*第3章*，*探索机器学习算法*中学到的。要开始，请从菜单下拉中选择**文件**
    > **新建** > **笔记本**来创建一个新的笔记本以编写我们的代码。当提示选择内核时，选择**Python 3 (ipykernel)**。您可以通过选择**文件**
    > **重命名笔记本**从菜单中重命名文件，使其具有更有意义的名称。从Kaggle下载数据集，网址为[https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news](https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news)。请注意，您需要Kaggle账户才能下载。下载完成后，您应该看到一个`archive.zip`文件。使用您本地机器上的unzip工具解压文件。
- en: Next, let’s upload the `data` file to the Studio notebook. Create a new folder
    called `data` in the same folder where the new notebook is located and upload
    it to the `data` directory using the **File Upload** utility (the up arrow icon)
    in Studio UI. Select `all-data.csv` to upload.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将`data`文件上传到Studio笔记本。在新的笔记本所在文件夹中创建一个名为`data`的新文件夹，并使用Studio UI中的**文件上传**实用程序（向上箭头图标）将文件上传到`data`目录。选择`all-data.csv`进行上传。
- en: 'Now, let’s install some additional packages for our exercise. Run the following
    code block inside the notebook cell to install the transformer package. The transformer
    package provides a list of pre-trained transformers such as BERT. You will use
    these transformers to fine-tune an ML task. Note that some of the code block samples
    are not complete. You can find the complete code samples at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/bert-financial-sentiment.ipynb](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/bert-financial-sentiment.ipynb):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为我们的练习安装一些额外的包。在笔记本单元中运行以下代码块来安装转换器包。转换器包提供了一系列预训练的转换器，如BERT。您将使用这些转换器来微调一个机器学习任务。请注意，一些代码块示例并不完整。您可以在[https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/bert-financial-sentiment.ipynb](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/bert-financial-sentiment.ipynb)找到完整的代码示例：
- en: '[PRE0]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Restart the kernel of the notebook after installing `ipywidgets`. Next, import
    some libraries into the notebook and set up the logger for logging purposes:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装`ipywidgets`后重启笔记本内核。接下来，将一些库导入到笔记本中，并设置日志记录器以进行日志记录：
- en: '[PRE1]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we are ready to load the `data` file and process it. The following code
    block loads the `data` file and splits the data into train and test datasets.
    We will select the first two columns from the file and name them `sentiment` and
    `article`. The `sentiment` column is the label column. It contains three different
    unique values (`negative`, `neutral`, and `positive`). Since they are string values,
    we will convert them into integers (`0, 1, 2`) using `OrdinalEncoder` from the
    scikit-learn library. We also need to determine the max length of the article
    column. The max length is used to prepare the input for the transformer since
    the transformer requires a fixed length:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好加载`data`文件并处理它。以下代码块加载`data`文件并将数据分为训练集和测试集。我们将从文件中选择前两列，并命名为`sentiment`和`article`。`sentiment`列是标签列，它包含三个不同的唯一值（`negative`，`neutral`和`positive`）。由于它们是字符串值，我们将使用scikit-learn库中的`OrdinalEncoder`将它们转换为整数（`0,
    1, 2`）。我们还需要确定文章列的最大长度。最大长度用于为转换器准备输入，因为转换器需要一个固定长度：
- en: '[PRE2]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we will build a list of utility functions to support the data loading
    and model training. We need to feed data to the transformer model in batches.
    The following `get_data_loader()` function loads the dataset into the PyTorch
    `DataLoader` class with a specified batch size. Note that we also encode the articles
    into tokens with the `BertTokenizer` class:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构建一个实用函数列表来支持数据加载和模型训练。我们需要以批量的形式向转换器模型提供数据。以下 `get_data_loader()` 函数将数据集加载到具有指定批量大小的
    PyTorch `DataLoader` 类中。请注意，我们还会使用 `BertTokenizer` 类将文章编码为标记：
- en: '[PRE3]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following `train()` function will run the training loop using the `BertForSequenceClassification`
    class. We will use the pre-trained BERT model for fine-tuning, instead of training
    from scratch. We will feed one batch of data to the BERT model at a time. Note
    that we will also check if there is a GPU device on the server. If there is one,
    we will use the `cuda` device for GPU training, instead of `cpu` for CPU training.
    We need to manually move the data and BERT model to the same target device using
    the `.to(device)` function so that the training can happen on the target device
    with the data residing in memory on the same device. The optimizer we’re using
    here is AdamW, which is a variant of the gradient descent optimization algorithm.
    The training loop will run through the number of epochs specified. One epoch runs
    through the entire training dataset once:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 `train()` 函数将使用 `BertForSequenceClassification` 类运行训练循环。我们将使用预训练的 BERT 模型进行微调，而不是从头开始训练。我们将一次向
    BERT 模型提供一批数据。请注意，我们也会检查服务器上是否有 GPU 设备。如果有，我们将使用 `cuda` 设备进行 GPU 训练，而不是使用 `cpu`
    进行 CPU 训练。我们需要手动使用 `.to(device)` 函数将数据和 BERT 模型移动到同一目标设备，以便在目标设备上发生训练，同时数据驻留在同一设备上的内存中。我们在这里使用的优化器是
    AdamW，它是梯度下降优化算法的一个变体。训练循环将运行指定数量的 epoch。一个 epoch 会遍历整个训练数据集一次：
- en: '[PRE4]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We also want to test the model’s performance using a separate test dataset
    during training. To do this, we will implement the following `test()` function,
    which is called by the `train()` function:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望在训练期间使用单独的测试数据集来测试模型的性能。为此，我们将实现以下 `test()` 函数，该函数由 `train()` 函数调用：
- en: '[PRE5]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we have all the functions needed to load and process data, run the training
    loop, and measure the model metrics using a test dataset. With that, we can kick
    off the training process. We will use the `args` variable to set up various values,
    such as batch size, data location, and learning rate, to be used by the training
    loop and the testing loop:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经拥有了加载和处理数据、运行训练循环以及使用测试数据集测量模型指标所需的所有函数。有了这些，我们就可以启动训练过程。我们将使用 `args`
    变量设置各种值，例如批量大小、数据位置和学习率，这些值将由训练循环和测试循环使用：
- en: '[PRE6]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once you have run the preceding code, you should see training stats for each
    batch and epoch. The model will also be saved in the specified directory.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦运行了前面的代码，你应该会看到每个批次和每个 epoch 的训练统计信息。模型也将被保存在指定的目录中。
- en: 'Next, let’s see how the trained model can be used to make predictions directly.
    To do this, we must implement several utility functions. The following `input_fn()`
    function takes input in JSON format and outputs an input vector that represents
    the string input and its associated mask. The output will be sent to the model
    for prediction:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何使用训练好的模型直接进行预测。为此，我们必须实现几个实用函数。以下 `input_fn()` 函数接收 JSON 格式的输入，并输出一个表示字符串输入及其相关掩码的输入向量。输出将被发送到模型进行预测：
- en: '[PRE7]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following `predict_fn()` function takes `input_data` returned by `input_fn()`
    and uses the trained model to generate the prediction. Note that we will also
    use a GPU if a GPU device is available on the server:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 `predict_fn()` 函数接收由 `input_fn()` 返回的 `input_data`，并使用训练好的模型生成预测。请注意，如果服务器上可用
    GPU 设备，我们也会使用 GPU：
- en: '[PRE8]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, run the following code to generate a prediction. Replace the value of
    the article with different financial text to see the result:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，运行以下代码以生成预测。将文章的值替换为不同的金融文本以查看结果：
- en: '[PRE9]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With SageMaker, you have different options to train ML models. For quick experiments
    and light model building, the Jupyter Notebook environment is sufficient for many
    model training tasks. For more resource-demanding ML training tasks, we need to
    consider dedicated training resources for model training. In the next section,
    let us look at an alternative way to train the BERT model using the SageMaker
    training service.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SageMaker，您有不同选项来训练ML模型。对于快速实验和轻量级模型构建，Jupyter Notebook环境对于许多模型训练任务来说已经足够。对于更资源密集型的ML训练任务，我们需要考虑为模型训练配置专用训练资源。在下一节中，让我们看看使用SageMaker训练服务训练BERT模型的另一种方法。
- en: Training the BERT model with the SageMaker Training service
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用SageMaker训练服务训练BERT模型
- en: 'In the previous section, you trained the BERT model directly inside a GPU-based
    Jupyter notebook. Instead of provisioning a GPU-based notebook instance, you can
    provision a less costly CPU-based instance and send the model training task to
    the SageMaker Training service. To use the SageMaker Training service, you need
    to make some minor changes to the training script and create a separate launcher
    script to kick off the training. As we discussed in the *Training ML models* section,
    there are three main approaches to training a model in SageMaker. Since SageMaker
    provides a managed container for PyTorch, we will use the managed container approach
    to train the model. With this approach, you will need to provide the following
    inputs:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，您直接在基于GPU的Jupyter笔记本中训练了BERT模型。您不必配置基于GPU的笔记本实例，而是可以配置一个成本更低的基于CPU的实例，并将模型训练任务发送到SageMaker训练服务。要使用SageMaker训练服务，您需要对训练脚本进行一些小的修改，并创建一个单独的启动器脚本来启动训练。正如我们在*训练ML模型*部分所讨论的，在SageMaker中训练模型有三种主要方法。由于SageMaker为PyTorch提供了托管容器，我们将使用托管容器方法来训练模型。使用这种方法，您需要提供以下输入：
- en: A training script as the entry point, as well as dependencies
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以训练脚本作为入口点，以及依赖项
- en: An IAM role to be used by the training job
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练作业将使用的IAM角色
- en: Infrastructure details such as the instance type and number
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础设施细节，如实例类型和数量
- en: A data (training/validation/testing) location in S3
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S3中的数据（训练/验证/测试）位置
- en: A model output location in S3
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S3中的模型输出位置
- en: Hyperparameters for training the model
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练的超参数
- en: 'When a training job is started, the SageMaker Training service will perform
    the following tasks in sequence:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练作业启动时，SageMaker训练服务将按顺序执行以下任务：
- en: Launch the EC2 instances needed for the training job.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动训练作业所需的EC2实例。
- en: Download the data from S3 to the training host.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从S3下载数据到训练主机。
- en: Download the appropriate managed container from the SageMaker ECR registry and
    run the container.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从SageMaker ECR注册表中下载适当的托管容器并运行容器。
- en: Copy the training script and dependencies to the training container.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练脚本和依赖项复制到训练容器。
- en: Run the training script and pass the hyperparameters as command-line arguments
    to the training script. The training script will load the training/validation/testing
    data from specific directories in the container, run the training loop, and save
    the model to a specific directory in the container. Several environment variables
    will be set in the container to provide configuration details, such as directories
    for the data and model output, to the training script.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行训练脚本并将超参数作为命令行参数传递给训练脚本。训练脚本将从容器中的特定目录加载训练/验证/测试数据，运行训练循环，并将模型保存到容器中的特定目录。容器中将设置几个环境变量，以提供配置详细信息，例如数据和模型输出目录，给训练脚本。
- en: Once the training script exits with success, the SageMaker Training service
    will copy the saved model artifacts from the container to the model output location
    in S3.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦训练脚本成功退出，SageMaker训练服务将从容器中复制保存的模型工件到S3中的模型输出位置。
- en: 'Now, let’s create the following training script, name it `train.py`, and save
    it in a new directory called `code`. Note that the training script is almost the
    same as the code in the *Training the BERT model in the Jupyter notebook* section.
    We have added an `if __name__ == "__main__":` section at the end. This section
    contains the code for reading the values of the command-line arguments and the
    values of the system environment variables such as SageMaker’s data directory
    (`SM_CHANNEL_TRAINING`), the model output directory (`SM_MODEL_DIR`), and the
    number of GPUs (`SM_NUM_GPUS`) available on the host. The following code sample
    is not complete. You can find the complete code sample at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/code/train.py](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/code/train.py):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建以下训练脚本，命名为`train.py`，并将其保存在名为`code`的新目录中。请注意，训练脚本几乎与*在Jupyter笔记本中训练BERT模型*部分中的代码相同。我们在末尾添加了一个`if
    __name__ == "__main__":`部分。此部分包含读取命令行参数值和系统环境变量值的代码，例如SageMaker的数据目录（`SM_CHANNEL_TRAINING`）、模型输出目录（`SM_MODEL_DIR`）以及主机上可用的GPU数量（`SM_NUM_GPUS`）。以下代码示例并不完整。您可以在[https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/code/train.py](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/code/train.py)找到完整的代码示例：
- en: '[PRE10]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding script requires library packages that are not available in the
    managed training container. You can install custom library packages using the
    `requirement.txt` file. Create a `requirement.txt` file with the following code
    and save it in the `code` directory:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的脚本需要一些在托管训练容器中不可用的库包。您可以使用`requirement.txt`文件安装自定义库包。创建一个包含以下代码的`requirement.txt`文件，并将其保存在`code`目录中：
- en: '[PRE11]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, let’s create a launcher notebook for kicking off the training job using
    the SageMaker Training service. The launcher notebook will do the following:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个启动笔记本，用于使用SageMaker训练服务启动训练作业。启动笔记本将执行以下操作：
- en: Upload the training and test datasets to the S3 bucket and folders.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将训练集和测试集上传到S3存储桶和文件夹中。
- en: Set up the SageMaker PyTorch estimator using the SageMaker SDK to configure
    the training job.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SageMaker SDK设置SageMaker PyTorch估计器以配置训练作业。
- en: Kick off the SageMaker training job.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动SageMaker训练作业。
- en: Create a new notebook called `bert-financial-sentiment-launcher.ipynb` in the
    folder where the `code` folder is located and copy the following code block into
    the notebook one cell at a time. When you’re prompted to choose a kernel, pick
    the **Python 3 (ipykernel)** kernel.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在`code`文件夹所在的文件夹中创建一个新的笔记本，命名为`bert-financial-sentiment-launcher.ipynb`，并将以下代码块逐个单元格地复制到笔记本中。当您被提示选择内核时，请选择**Python
    3 (ipykernel**)内核。
- en: 'The following code specifies the S3 bucket to be used for saving the training
    and testing dataset, as well as the model artifacts. You can use the bucket that
    was created earlier in the *Setting up SageMaker Studio* section, when the Studio
    domain was configured. The training and test datasets we created earlier will
    be uploaded to the bucket. The `get_execution_role()` function returns the IAM
    role associated with the notebook, which we will use to run the training job later:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码指定了用于保存训练和测试数据集以及模型工件要使用的S3存储桶。您可以使用在*设置SageMaker Studio*部分中创建的存储桶，当时配置了Studio域名。我们之前创建的训练集和测试集将被上传到该存储桶。`get_execution_role()`函数返回与笔记本关联的IAM角色，我们将使用它来稍后运行训练作业：
- en: '[PRE12]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we must set up the SageMaker PyTorch estimator and kick off the training
    job. Note that you can also specify the PyTorch framework version and Python version
    to set up the container. For simplicity, we are passing the name of the training
    file and test file, as well as the max length, as hyperparameters. The `train.py`
    file can also be modified to look them up dynamically:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们必须设置SageMaker PyTorch估计器和启动训练作业。请注意，您还可以指定PyTorch框架版本和Python版本来设置容器。为了简单起见，我们正在传递训练文件和测试文件的名字，以及最大长度作为超参数。`train.py`文件也可以修改为动态查找它们：
- en: '[PRE13]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Once the training job has been completed, you can go to the SageMaker management
    console to access the training job’s details and metadata. Training jobs also
    send outputs to CloudWatch Logs and CloudWatch metrics. You can navigate to these
    logs by clicking on the respective links on the training job details page.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练作业完成，您可以通过SageMaker管理控制台访问训练作业的详细信息和元数据。训练作业还会将输出发送到CloudWatch日志和CloudWatch指标。您可以通过点击训练作业详情页面上的相应链接来导航到这些日志。
- en: Deploying the model
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型部署
- en: 'In this step, we will deploy the trained model to a SageMaker RESTful endpoint
    so that it can be integrated with downstream applications. We will use the managed
    PyTorch serving container to host the model. With the managed PyTorch serving
    container, you can provide an inference script to process the request data before
    it is sent to the model for inference, as well as controlling how to call the
    model for inference. Let’s create a new script called `inference.py` in the `code`
    folder that contains the following code block. As you have probably noticed, we
    have used the same functions that we used in *Training the BERT model in the Jupyter
    notebook* section for the predictions. Note that you need to use the same function
    signatures for these two functions as SageMaker will be looking for the exact
    function name and parameter lists. You can find the complete source code at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/code/inference.py](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/code/inference.py):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们将部署训练好的模型到SageMaker RESTful端点，以便它可以与下游应用程序集成。我们将使用托管的PyTorch服务容器来托管模型。使用托管的PyTorch服务容器，您可以在将请求数据发送到模型进行推理之前提供推理脚本以处理数据，以及控制如何调用模型进行推理。让我们在`code`文件夹中创建一个新的脚本`inference.py`，其中包含以下代码块。如您所注意到的，我们使用了与*在Jupyter笔记本中训练BERT模型*部分相同的函数来生成预测。请注意，您需要使用这两个函数相同的函数签名，因为SageMaker将寻找确切的函数名和参数列表。您可以在[https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/code/inference.py](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/code/inference.py)找到完整的源代码：
- en: '[PRE14]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we need to modify the `bert-financial-sentiment-launcher.ipynb` file
    to create the endpoint. You can deploy trained models from the SageMaker `estimator`
    class directly. Here, however, we want to show you how to deploy a model that’s
    been trained previously, as this is the most likely deployment scenario:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要修改`bert-financial-sentiment-launcher.ipynb`文件以创建端点。您可以直接从SageMaker的`estimator`类部署训练好的模型。然而，在这里，我们想向您展示如何部署一个之前训练好的模型，因为这可能是最常见的部署场景：
- en: '[PRE15]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After the model has been deployed, we can call the model endpoint to generate
    some predictions:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署后，我们可以调用模型端点来生成一些预测：
- en: '[PRE16]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Try out different phrases and see if the model predicts the sentiment correctly.
    You can also access the endpoint’s details by navigating to the SageMaker management
    console and clicking on the endpoint.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试不同的短语，看看模型是否正确预测情感。您还可以通过导航到SageMaker管理控制台并点击端点来访问端点的详细信息。
- en: 'To avoid any ongoing costs for the endpoint, let’s delete it. Run the following
    command in a new cell to delete the endpoint:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免端点的持续成本，让我们将其删除。在新的单元格中运行以下命令以删除端点：
- en: '[PRE17]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Congratulations – you have finished building a basic data science environment
    and used it to train and deploy an NLP model to detect its sentiment! If you don’t
    want to keep this environment to avoid any associated costs, make sure that you
    shut down any instances of the SageMaker Studio notebooks.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜您——您已经完成了基本数据科学环境的构建，并使用它来训练和部署了一个用于检测情感的NLP模型！如果您不想保留这个环境以避免任何相关成本，请确保关闭SageMaker
    Studio笔记本的任何实例。
- en: Next, let’s explore SageMaker Canvas and see how to use it to build custom ML
    models without any coding.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探索SageMaker Canvas，看看如何使用它来构建无需任何编码的定制机器学习模型。
- en: Building ML models with SageMaker Canvas
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用SageMaker Canvas构建机器学习模型
- en: In this lab, we will train a customer churn classification model using Canvas’s
    custom model feature. We will go through the full cycle from dataset creation/selection,
    model training, and model analysis to prediction generation and model deployment.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验室中，我们将使用Canvas的定制模型功能来训练一个客户流失分类模型。我们将完成从数据集创建/选择、模型训练、模型分析到预测生成和模型部署的全过程。
- en: To get started, we first need to launch a SageMaker Canvas environment. To do
    that, go back to your Studio environment, select **Canvas** under the **Applications**
    section, and click on the **Run Canvas** button in the right-hand pane. It is
    going to take 8-10 minutes for the Canvas environment to become available. When
    the Canvas status changes to **Running**, click on **Open Canvas**, and you will
    see a screen similar to *Figure 8.13*.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，我们首先需要启动 SageMaker Canvas 环境。为此，返回您的 Studio 环境，在**应用程序**部分下选择**Canvas**，然后在右侧面板中点击**运行
    Canvas**按钮。Canvas 环境变得可用需要 8-10 分钟。当 Canvas 状态变为**运行中**时，点击**打开 Canvas**，您将看到一个类似于*图
    8.13*的屏幕。
- en: '![](img/B20836_08_13.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20836_08_13.png)'
- en: 'Figure 8.13: SageMaker Canvas'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13：SageMaker Canvas
- en: 'The following steps will guide you through the rest of the lab:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将引导您完成剩余的实验：
- en: Select the **My models** icon on the left-hand pane to start building a custom
    model using a custom dataset. You should see a screen similar to *Figure 8.14*:![](img/B20836_08_14.png)
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧面板中选择**我的模型**图标，以使用自定义数据集开始构建自定义模型。您应该看到一个类似于*图 8.14*的屏幕：![图片](img/B20836_08_14.png)
- en: 'Figure 8.14: Canvas My models screen'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.14：Canvas 我的模型屏幕
- en: Click on the **New model** button,provide a name for the model, select **Predictive
    analysis** as the problem type, and click on **Create**.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**新建模型**按钮，为模型提供一个名称，选择**预测分析**作为问题类型，然后点击**创建**。
- en: Download the dataset from [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/churn.csv](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/churn.csv)
    and save it to your local machine.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/churn.csv](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter08/churn.csv)下载数据集，并将其保存到您的本地计算机上。
- en: On the **Select dataset** screen, click on the **Create data** link in the top-right
    corner. Provide a name for the dataset.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**选择数据集**屏幕上，点击右上角的**创建数据**链接。为数据集提供一个名称。
- en: On the next screen, click on **Select files from your local computer** and navigate
    to the `churn.csv` file you downloaded in *step 4* to upload the file. After the
    file is uploaded, click on **Create dataset** to continue.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一屏，点击**从您的本地计算机选择文件**，导航到您在*步骤 4*中下载的`churn.csv`文件以上传文件。文件上传后，点击**创建数据集**以继续。
- en: On the next screen, check the dataset you have just created, and click on **Select
    datase**t to continue.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一屏，检查您刚刚创建的数据集，并点击**选择数据集**以继续。
- en: On the next screen, you will be asked to select a target column to predict.
    Select the **Exited** column from the dataset. Also, you should uncheck some of
    the source columns, such as surname and row number, as they are not relevant to
    model training. Finally, select **Quick build** to build a model.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一屏，您将被要求选择一个预测的目标列。从数据集中选择**已退出**列。您还应该取消选中一些源列，例如姓氏和行号，因为它们与模型训练不相关。最后，选择**快速构建**来构建模型。
- en: On the next screen, you will see some information about the duration of the
    build and the build type. Now you will need to wait for the build process to complete.
    When it is complete, you will see a screen similar to the following figure. You
    will be able to review the various training metrics, such as accuracy, precision,
    and recall. You will also be able to see the impact of different source columns
    on the target columns.![](img/B20836_08_15.png)
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一屏，您将看到有关构建持续时间和构建类型的一些信息。现在您需要等待构建过程完成。完成后，您将看到一个类似于以下图所示的屏幕。您将能够查看各种训练指标，例如准确率、精确率和召回率。您还将能够看到不同源列对目标列的影响。![图片](img/B20836_08_15.png)
- en: 'Figure 8.15: Model training results'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.15：模型训练结果
- en: Since model performance optimization is not our primary goal here, we will click
    on **Predict** to generate some predictions on the next screen.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于模型性能优化不是我们这里的主要目标，我们将点击**预测**以在下一屏生成一些预测。
- en: On the next screen, select the **Single prediction** option, and change the
    values for some of the fields, such as age, salary, and credit score, to see how
    they impact the results by clicking on **Update**.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一屏，选择**单次预测**选项，并更改一些字段的值，例如年龄、薪资和信用评分，通过点击**更新**来查看它们如何影响结果。
- en: Lastly, we will deploy the model to an endpoint so it can be used by other applications.
    To deploy, you simply click on the **Deploy** button to deploy the model. You
    will have the option to select an instance type and the number of instances for
    the model. Upon successful deployment, a deployment URL will be made available
    for other applications to use.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将模型部署到端点，以便其他应用程序可以使用。要部署，您只需点击**部署**按钮来部署模型。您可以选择实例类型和模型的实例数量。部署成功后，将为其他应用程序提供部署URL。
- en: Congratulations on completing the lab! You have effectively trained and deployed
    a binary classification model using your custom dataset without any code, using
    SageMaker Canvas. This no-code ML tool facilitates a swift initiation into ML
    projects, even for individuals without prior ML knowledge. You now have first-hand
    experience with how Canvas automates numerous tasks for you, ranging from algorithm
    selection and model training to model deployment.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜您完成实验室任务！您已经有效地使用SageMaker Canvas，在没有编写任何代码的情况下，使用自定义数据集训练和部署了一个二元分类模型。这个无代码ML工具使得即使是缺乏先前ML知识的人也能迅速启动ML项目。您现在亲身体验了Canvas如何自动化为您执行众多任务，从算法选择和模型训练到模型部署。
- en: Summary
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored how a data science environment can provide a scalable
    infrastructure for experimentation, model training, and model deployment for testing
    purposes. You learned about the core architecture components for building a fully
    managed data science environment using AWS services such as Amazon SageMaker,
    Amazon ECR, and Amazon S3\. You practiced setting up a data science environment
    and trained and deployed an NLP model using both SageMaker Studio notebooks and
    the SageMaker Training service. You have also developed hands-on experience with
    SageMaker Canvas to automate ML tasks from model building to model deployment.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了数据科学环境如何提供可扩展的基础设施，用于实验、模型训练和测试部署。您学习了使用AWS服务如Amazon SageMaker、Amazon
    ECR和Amazon S3构建完全托管的数据科学环境的核心架构组件。您练习了设置数据科学环境，并使用SageMaker Studio笔记本和SageMaker训练服务训练和部署了一个NLP模型。您还通过SageMaker
    Canvas获得了实际操作经验，以自动化从模型构建到模型部署的ML任务。
- en: At this point, you should be able to talk about the key components of a data
    science environment, as well as how to build one using AWS services and use it
    for model building, training, and deployment. In the next chapter, we will talk
    about how to build an enterprise ML platform for scale through automation.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您应该能够讨论数据科学环境的关键组件，以及如何使用AWS服务构建一个并用于模型构建、训练和部署。在下一章中，我们将讨论如何通过自动化构建一个企业级ML平台以实现规模扩展。
- en: Join our community on Discord
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的社区Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/mlsah](https://packt.link/mlsah )'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mlsah](https://packt.link/mlsah)'
- en: '![](img/QR_Code70205728346636561.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code70205728346636561.png)'
