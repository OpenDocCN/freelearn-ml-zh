- en: ChapterÂ 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬1ç« 
- en: Thinking Probabilistically
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: æ¦‚ç‡æ€ç»´
- en: Probability theory is nothing but common sense reduced to calculation. â€“ Pierre
    Simon Laplace
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¦‚ç‡è®ºæ— éæ˜¯å°†å¸¸è¯†ç®€åŒ–ä¸ºè®¡ç®—ã€‚â€”â€”çš®åŸƒå°”Â·è¥¿è’™Â·æ‹‰æ™®æ‹‰æ–¯
- en: In this chapter, we will learn about the core concepts of Bayesian statistics
    and some of the instruments in the Bayesian toolbox. We will use some Python code,
    but this chapter will be mostly theoretical; most of the concepts we will see
    here will be revisited many times throughout this book. This chapter, being heavy
    on the theoretical side, is perhaps a little anxiogenic for the coder in you,
    but I think it will ease the path to effectively applying Bayesian statistics
    to your problems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ è´å¶æ–¯ç»Ÿè®¡çš„æ ¸å¿ƒæ¦‚å¿µä»¥åŠè´å¶æ–¯å·¥å…·ç®±ä¸­çš„ä¸€äº›å·¥å…·ã€‚æˆ‘ä»¬ä¼šä½¿ç”¨ä¸€äº› Python ä»£ç ï¼Œä½†è¿™ä¸€ç« å¤§éƒ¨åˆ†å†…å®¹å°†æ˜¯ç†è®ºæ€§çš„ï¼›æˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°çš„å¤§éƒ¨åˆ†æ¦‚å¿µå°†åœ¨æœ¬ä¹¦çš„è®¸å¤šéƒ¨åˆ†ä¸­åå¤å‡ºç°ã€‚è¿™ä¸€ç« ç†è®ºæ€§è¾ƒå¼ºï¼Œå¯èƒ½ä¼šè®©ä½ è¿™ä¸ªç¼–ç è€…æœ‰äº›ç„¦è™‘ï¼Œä½†æˆ‘è®¤ä¸ºå®ƒä¼šä¸ºæœ‰æ•ˆåœ°å°†è´å¶æ–¯ç»Ÿè®¡åº”ç”¨äºä½ çš„é—®é¢˜é“ºå¹³é“è·¯ã€‚
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: Statistical modeling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»Ÿè®¡å»ºæ¨¡
- en: Probabilities and uncertainty
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¦‚ç‡ä¸ä¸ç¡®å®šæ€§
- en: Bayesâ€™ theorem and statistical inference
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è´å¶æ–¯å®šç†ä¸ç»Ÿè®¡æ¨æ–­
- en: Single-parameter inference and the classic coin-flip problem
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å•å‚æ•°æ¨æ–­å’Œç»å…¸çš„æŠ›ç¡¬å¸é—®é¢˜
- en: Choosing priors and why people often donâ€™t like them but should
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€‰æ‹©å…ˆéªŒåˆ†å¸ƒåŠä¸ºä½•äººä»¬é€šå¸¸ä¸å–œæ¬¢å®ƒä»¬ï¼Œä½†å…¶å®åº”è¯¥å–œæ¬¢
- en: Communicating a Bayesian analysis
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¼ è¾¾è´å¶æ–¯åˆ†æç»“æœ
- en: 1.1 Statistics, models, and this bookâ€™s approach
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 ç»Ÿè®¡å­¦ã€æ¨¡å‹å’Œæœ¬ä¹¦çš„æ–¹æ³•
- en: 'Statistics is about collecting, organizing, analyzing, and interpreting data,
    and hence statistical knowledge is essential for data analysis. Two main statistical
    methods are used in data analysis:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç»Ÿè®¡å­¦æ˜¯å…³äºæ”¶é›†ã€æ•´ç†ã€åˆ†æå’Œè§£é‡Šæ•°æ®çš„ï¼Œå› æ­¤ç»Ÿè®¡çŸ¥è¯†å¯¹æ•°æ®åˆ†æè‡³å…³é‡è¦ã€‚åœ¨æ•°æ®åˆ†æä¸­ä½¿ç”¨äº†ä¸¤ç§ä¸»è¦çš„ç»Ÿè®¡æ–¹æ³•ï¼š
- en: '**Exploratory Data Analysis (EDA)**: This is about numerical summaries, such
    as the mean, mode, standard deviation, and interquartile ranges. EDA is also about
    visually inspecting the data, using tools you may be already familiar with, such
    as histograms and scatter plots.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆEDAï¼‰**ï¼šè¿™æ¶‰åŠåˆ°æ•°å€¼æ±‡æ€»ï¼Œä¾‹å¦‚å‡å€¼ã€ä¼—æ•°ã€æ ‡å‡†å·®å’Œå››åˆ†ä½æ•°èŒƒå›´ã€‚EDA è¿˜æ¶‰åŠé€šè¿‡å¯è§†åŒ–æ£€æŸ¥æ•°æ®ï¼Œä½¿ç”¨ä½ å¯èƒ½å·²ç»ç†Ÿæ‚‰çš„å·¥å…·ï¼Œæ¯”å¦‚ç›´æ–¹å›¾å’Œæ•£ç‚¹å›¾ã€‚'
- en: '**Inferential statistics**: This is about making statements beyond the current
    data. We may want to understand some particular phenomenon, maybe we want to make
    predictions for future (yet unobserved) data points, or we need to choose among
    several competing explanations for the same set of observations. In summary, inferential
    statistics allow us to draw meaningful insights from a limited set of data and
    make informed decisions based on the results of our analysis.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¨æ–­ç»Ÿè®¡**ï¼šè¿™æ˜¯å…³äºè¶…è¶Šå½“å‰æ•°æ®è¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬å¯èƒ½æƒ³è¦äº†è§£æŸç§ç‰¹å®šç°è±¡ï¼Œæˆ–è€…æˆ‘ä»¬æƒ³è¦å¯¹æœªæ¥ï¼ˆå°šæœªè§‚å¯Ÿåˆ°çš„ï¼‰æ•°æ®ç‚¹è¿›è¡Œé¢„æµ‹ï¼Œæˆ–è€…æˆ‘ä»¬éœ€è¦åœ¨å¤šä¸ªç«äº‰æ€§è§£é‡Šä¹‹é—´åšå‡ºé€‰æ‹©ï¼Œé’ˆå¯¹åŒä¸€ç»„è§‚å¯Ÿæ•°æ®ã€‚æ€»ä¹‹ï¼Œæ¨æ–­ç»Ÿè®¡è®©æˆ‘ä»¬èƒ½å¤Ÿä»æœ‰é™çš„æ•°æ®ä¸­æå–æœ‰æ„ä¹‰çš„è§è§£ï¼Œå¹¶åŸºäºåˆ†æç»“æœåšå‡ºæ˜æ™ºçš„å†³ç­–ã€‚'
- en: A Match Made in Heaven
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¤©ä½œä¹‹åˆ
- en: The focus of this book is on how to perform Bayesian inferential statistics,
    but we will also use ideas from EDA to summarize, interpret, check, and communicate
    the results of Bayesian inference.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ä¹¦çš„é‡ç‚¹æ˜¯å¦‚ä½•è¿›è¡Œè´å¶æ–¯æ¨æ–­ç»Ÿè®¡ï¼Œä½†æˆ‘ä»¬ä¹Ÿä¼šå€Ÿç”¨æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆEDAï¼‰ä¸­çš„ä¸€äº›æ€æƒ³æ¥æ€»ç»“ã€è§£é‡Šã€æ£€æŸ¥å¹¶ä¼ è¾¾è´å¶æ–¯æ¨æ–­çš„ç»“æœã€‚
- en: 'Most introductory statistical courses, at least for non-statisticians, are
    taught as a collection of recipes that go like this: go to the statistical pantry,
    pick one tin can and open it, add data to taste, and stir until you obtain a consistent
    p-value, preferably under 0.05\. The main goal of these courses is to teach you
    how to pick the proper can. I never liked this approach, mainly because the most
    common result is a bunch of confused people unable to grasp, even at the conceptual
    level, the unity of the different learned methods. We will take a different approach:
    we will learn some recipes, but they will be homemade rather than canned food;
    we will learn how to mix fresh ingredients that will suit different statistical
    occasions and, more importantly, that will let you apply concepts far beyond the
    examples in this book.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°å…¥é—¨çº§ç»Ÿè®¡è¯¾ç¨‹ï¼Œè‡³å°‘å¯¹äºéç»Ÿè®¡å­¦ä¸“ä¸šçš„äººæ¥è¯´ï¼Œé€šå¸¸è¢«æ•™æˆä¸ºä¸€ç³»åˆ—çš„â€œé…æ–¹â€ï¼Œå¤§è‡´å¦‚ä¸‹ï¼šèµ°è¿›ç»Ÿè®¡å­¦çš„å‚¨è—å®¤ï¼ŒæŒ‘é€‰ä¸€ç½ç½å¤´æ‰“å¼€ï¼ŒåŠ å…¥æ•°æ®ï¼ŒæŒ‰ä¸ªäººå£å‘³è°ƒå‘³ï¼Œæ…æ‹Œç›´åˆ°å¾—åˆ°ä¸€ä¸ªä¸€è‡´çš„
    p å€¼ï¼Œæœ€å¥½å°äº 0.05ã€‚ è¿™äº›è¯¾ç¨‹çš„ä¸»è¦ç›®æ ‡æ˜¯æ•™ä½ å¦‚ä½•é€‰æ‹©åˆé€‚çš„ç½å¤´ã€‚æˆ‘ä»ä¸å–œæ¬¢è¿™ç§æ–¹æ³•ï¼Œä¸»è¦æ˜¯å› ä¸ºæœ€å¸¸è§çš„ç»“æœæ˜¯ä¸€ç¾¤å›°æƒ‘çš„äººï¼Œç”šè‡³åœ¨æ¦‚å¿µå±‚é¢ä¹Ÿæ— æ³•ç†è§£ä¸åŒå­¦ä¹ æ–¹æ³•çš„ç»Ÿä¸€æ€§ã€‚æˆ‘ä»¬å°†é‡‡å–ä¸åŒçš„æ–¹æ³•ï¼šæˆ‘ä»¬å°†å­¦ä¹ ä¸€äº›é…æ–¹ï¼Œä½†å®ƒä»¬æ˜¯è‡ªåˆ¶çš„ï¼Œè€Œéç½å¤´é£Ÿå“ï¼›æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•æ··åˆæ–°é²œçš„åŸæ–™ï¼Œé€‚ç”¨äºä¸åŒçš„ç»Ÿè®¡åœºåˆï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œè¿™å°†è®©ä½ èƒ½å°†è¿™äº›æ¦‚å¿µåº”ç”¨åˆ°æœ¬ä¹¦ä¸­çš„ä¾‹å­ä¹‹å¤–çš„åœºæ™¯ä¸­ã€‚
- en: 'Taking this approach is possible for two reasons:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: é‡‡ç”¨è¿™ç§æ–¹æ³•æ˜¯å› ä¸ºä¸¤ä¸ªåŸå› ï¼š
- en: '**Ontological**: Statistics is a form of modeling unified under the mathematical
    framework of probability theory. Using a probabilistic approach provides a unified
    view of what may seem like very disparate methods; statistical methods and machine
    learning methods look much more similar under the probabilistic lens.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æœ¬ä½“è®º**ï¼šç»Ÿè®¡å­¦æ˜¯ä¸€ç§å»ºæ¨¡æ–¹æ³•ï¼Œç»Ÿä¸€äºæ¦‚ç‡è®ºçš„æ•°å­¦æ¡†æ¶ä¸‹ã€‚é‡‡ç”¨æ¦‚ç‡æ–¹æ³•èƒ½å¤Ÿæä¾›ä¸€ä¸ªç»Ÿä¸€çš„è§†è§’æ¥çœ‹å¾…é‚£äº›çœ‹ä¼¼æˆªç„¶ä¸åŒçš„æ–¹æ³•ï¼›ç»Ÿè®¡æ–¹æ³•å’Œæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨æ¦‚ç‡è§†è§’ä¸‹æ˜¾å¾—æ›´åŠ ç›¸ä¼¼ã€‚'
- en: '**Technical**: Modern software, such as PyMC, allows practitioners, just like
    you and me, to define and solve models in a relatively easy way. Many of these
    models were unsolvable just a few years ago or required a high level of mathematical
    and technical sophistication.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŠ€æœ¯æ€§**ï¼šç°ä»£è½¯ä»¶ï¼Œå¦‚ PyMCï¼Œä½¿å¾—ä»ä¸šè€…â€”â€”å°±åƒä½ æˆ‘ä¸€æ ·â€”â€”èƒ½å¤Ÿç›¸å¯¹è½»æ¾åœ°å®šä¹‰å’Œè§£å†³æ¨¡å‹ã€‚å‡ å¹´å‰ï¼Œè®¸å¤šè¿™æ ·çš„æ¨¡å‹æ˜¯æ— æ³•æ±‚è§£çš„ï¼Œæˆ–è€…éœ€è¦é«˜æ°´å¹³çš„æ•°å­¦å’ŒæŠ€æœ¯ç²¾æ¹›ã€‚'
- en: 1.2 Working with data
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 å¤„ç†æ•°æ®
- en: Data is an essential ingredient in statistics and data science. Data comes from
    several sources, such as experiments, computer simulations, surveys, and field
    observations. If we are the ones in charge of generating or gathering the data,
    it is always a good idea to first think carefully about the questions we want
    to answer and which methods we will use, and only then proceed to get the data.
    There is a whole branch of statistics dealing with data collection, known as experimental
    design. In the era of the data deluge, we can sometimes forget that gathering
    data is not always cheap. For example, while it is true that the **Large Hadron
    Collider** (**LHC**) produces hundreds of terabytes a day, its construction took
    years of manual and intellectual labor.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®æ˜¯ç»Ÿè®¡å­¦å’Œæ•°æ®ç§‘å­¦çš„æ ¸å¿ƒè¦ç´ ã€‚æ•°æ®æ¥æºäºå¤šä¸ªæ¸ é“ï¼Œä¾‹å¦‚å®éªŒã€è®¡ç®—æœºæ¨¡æ‹Ÿã€è°ƒæŸ¥å’Œå®åœ°è§‚å¯Ÿã€‚å¦‚æœæˆ‘ä»¬è´Ÿè´£ç”Ÿæˆæˆ–æ”¶é›†æ•°æ®ï¼Œé¦–å…ˆä»”ç»†æ€è€ƒæˆ‘ä»¬æƒ³è¦å›ç­”çš„é—®é¢˜ä»¥åŠæˆ‘ä»¬å°†ä½¿ç”¨å“ªäº›æ–¹æ³•æ˜¯éå¸¸é‡è¦çš„ï¼Œåªæœ‰åœ¨æ­¤ä¹‹åï¼Œæˆ‘ä»¬æ‰åº”è¯¥å¼€å§‹æ”¶é›†æ•°æ®ã€‚ç»Ÿè®¡å­¦ä¸­æœ‰ä¸€ä¸ªä¸“é—¨ç ”ç©¶æ•°æ®æ”¶é›†çš„åˆ†æ”¯ï¼Œç§°ä¸ºå®éªŒè®¾è®¡ã€‚åœ¨æ•°æ®æ³›æ»¥çš„æ—¶ä»£ï¼Œæˆ‘ä»¬æœ‰æ—¶ä¼šå¿˜è®°ï¼Œæ”¶é›†æ•°æ®å¹¶ä¸æ€»æ˜¯ä¾¿å®œçš„ã€‚ä¾‹å¦‚ï¼Œè™½ç„¶**å¤§å‹å¼ºå­å¯¹æ’æœº**ï¼ˆ**LHC**ï¼‰æ¯å¤©èƒ½äº§ç”Ÿæ•°ç™¾
    TB çš„æ•°æ®ï¼Œä½†å®ƒçš„å»ºé€ è¿‡ç¨‹èŠ±è´¹äº†æ•°å¹´çš„äººå·¥å’Œæ™ºåŠ›åŠ³åŠ¨ã€‚
- en: As a general rule, we can think of the process of generating the data as stochastic,
    because there is ontological, technical, and/or epistemic uncertainty, that is,
    the system is intrinsically stochastic, there are technical issues adding noise
    or restricting us from measuring with arbitrary precision, and/or there are conceptual
    limitations veiling details from us. For all these reasons, we always need to
    interpret data in the context of models, including mental and formal ones. Data
    does not speak but through models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥è®¤ä¸ºæ•°æ®ç”Ÿæˆçš„è¿‡ç¨‹æ˜¯éšæœºçš„ï¼Œå› ä¸ºå­˜åœ¨æœ¬ä½“è®ºã€æŠ€æœ¯æ€§å’Œ/æˆ–è®¤è¯†è®ºçš„ä¸ç¡®å®šæ€§ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç³»ç»Ÿæœ¬è´¨ä¸Šæ˜¯éšæœºçš„ï¼ŒæŠ€æœ¯æ€§é—®é¢˜ä¼šå¢åŠ å™ªå£°æˆ–é™åˆ¶æˆ‘ä»¬ä»¥ä»»æ„ç²¾åº¦è¿›è¡Œæµ‹é‡ï¼Œå’Œ/æˆ–å­˜åœ¨æ¦‚å¿µæ€§å±€é™é®è”½äº†æˆ‘ä»¬æ— æ³•çœ‹åˆ°çš„ç»†èŠ‚ã€‚åŸºäºè¿™äº›åŸå› ï¼Œæˆ‘ä»¬æ€»æ˜¯éœ€è¦åœ¨æ¨¡å‹çš„æ¡†æ¶ä¸‹è§£è¯»æ•°æ®ï¼ŒåŒ…æ‹¬å¿ƒæ™ºæ¨¡å‹å’Œå½¢å¼åŒ–æ¨¡å‹ã€‚æ•°æ®ä¸ç›´æ¥å‘å£°ï¼Œåªæœ‰é€šè¿‡æ¨¡å‹æ‰æœ‰æ„ä¹‰ã€‚
- en: In this book, we will assume that we already have collected the data. Our data
    will also be clean and tidy, something thatâ€™s rarely true in the real world. We
    will make these assumptions to focus on the subject of this book. I just want
    to emphasize, especially for newcomers to data analysis, that even when not covered
    in this book, there are important skills that you should learn and practice to
    successfully work with data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬å‡è®¾æˆ‘ä»¬å·²ç»æ”¶é›†å¥½äº†æ•°æ®ã€‚æˆ‘ä»¬çš„æ•°æ®ä¹Ÿå°†æ˜¯å¹²å‡€ä¸”æ•´æ´çš„ï¼Œè€Œè¿™åœ¨ç°å®ä¸–ç•Œä¸­æ˜¯æå°‘è§çš„ã€‚æˆ‘ä»¬ä½œå‡ºè¿™äº›å‡è®¾æ˜¯ä¸ºäº†é›†ä¸­è®¨è®ºæœ¬ä¹¦çš„ä¸»é¢˜ã€‚æˆ‘ç‰¹åˆ«æƒ³å¼ºè°ƒï¼Œå°¤å…¶æ˜¯å¯¹äºæ•°æ®åˆ†æçš„æ–°äººæ¥è¯´ï¼Œå³ä½¿æœ¬ä¹¦æ²¡æœ‰æ¶‰åŠï¼Œä»ç„¶æœ‰ä¸€äº›é‡è¦çš„æŠ€èƒ½éœ€è¦ä½ å»å­¦ä¹ å’Œå®è·µï¼Œä»¥ä¾¿èƒ½å¤ŸæˆåŠŸåœ°å¤„ç†æ•°æ®ã€‚
- en: A very useful skill when analyzing data is knowing how to write code in a programming
    language, such as Python. Manipulating data is usually necessary given that we
    live in a messy world with even messier data, and coding helps to get things done.
    Even if you are lucky and your data is very clean and tidy, coding will still
    be very useful since modern Bayesian statistics is done mostly through programming
    languages such as Python or R. If you want to learn how to use Python for cleaning
    and manipulating data, you can find a good introduction in *Python for Data Analysis*
    by [McKinney](Bibliography.xhtml#Xmckinney_2022)Â [[2022](Bibliography.xhtml#Xmckinney_2022)].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ææ•°æ®æ—¶ï¼Œä¸€ä¸ªéå¸¸æœ‰ç”¨çš„æŠ€èƒ½æ˜¯çŸ¥é“å¦‚ä½•åœ¨ç¼–ç¨‹è¯­è¨€ä¸­ç¼–å†™ä»£ç ï¼Œä¾‹å¦‚ Pythonã€‚ç”±äºæˆ‘ä»¬ç”Ÿæ´»åœ¨ä¸€ä¸ªæ‚ä¹±çš„ä¸–ç•Œä¸­ï¼Œæ•°æ®æ›´åŠ æ‚ä¹±ï¼Œå› æ­¤æ“æ§æ•°æ®é€šå¸¸æ˜¯å¿…è¦çš„ï¼Œç¼–ç¨‹æœ‰åŠ©äºå®Œæˆä»»åŠ¡ã€‚å³ä½¿ä½ å¾ˆå¹¸è¿ï¼Œæ•°æ®éå¸¸å¹²å‡€æ•´æ´ï¼Œç¼–ç¨‹ä»ç„¶éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºç°ä»£è´å¶æ–¯ç»Ÿè®¡ä¸»è¦é€šè¿‡åƒ
    Python æˆ– R è¿™æ ·çš„ç¼–ç¨‹è¯­è¨€è¿›è¡Œã€‚å¦‚æœä½ æƒ³å­¦ä¹ å¦‚ä½•ä½¿ç”¨ Python æ¥æ¸…ç†å’Œæ“æ§æ•°æ®ï¼Œå¯ä»¥å‚è€ƒ [McKinney](Bibliography.xhtml#Xmckinney_2022)Â çš„
    *Python for Data Analysis* ä¸€ä¹¦ [[2022](Bibliography.xhtml#Xmckinney_2022)]ã€‚
- en: 1.3 Bayesian modeling
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 è´å¶æ–¯å»ºæ¨¡
- en: 'Models are simplified descriptions of a given system or process that, for some
    reason, we are interested in. Those descriptions are deliberately designed to
    capture only the most relevant aspects of the system and not to explain every
    minor detail. This is one reason a more complex model is not always a better one.
    There are many different kinds of models; in this book, we will restrict ourselves
    to Bayesian models. We can summarize the Bayesian modeling process using three
    steps:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ˜¯å¯¹æŸä¸ªç³»ç»Ÿæˆ–è¿‡ç¨‹çš„ç®€åŒ–æè¿°ï¼Œå‡ºäºæŸäº›åŸå› ï¼Œæˆ‘ä»¬å¯¹æ­¤ç³»ç»Ÿæˆ–è¿‡ç¨‹æ„Ÿå…´è¶£ã€‚è¿™äº›æè¿°æ˜¯åˆ»æ„è®¾è®¡çš„ï¼Œåªæ•æ‰ç³»ç»Ÿä¸­æœ€ç›¸å…³çš„æ–¹é¢ï¼Œè€Œä¸è§£é‡Šæ¯ä¸€ä¸ªå¾®å°çš„ç»†èŠ‚ã€‚è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæ›´å¤æ‚çš„æ¨¡å‹ä¸ä¸€å®šæ˜¯æ›´å¥½çš„æ¨¡å‹çš„åŸå› ä¹‹ä¸€ã€‚æœ‰è®¸å¤šä¸åŒç§ç±»çš„æ¨¡å‹ï¼›åœ¨æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬å°†åªè®¨è®ºè´å¶æ–¯æ¨¡å‹ã€‚æˆ‘ä»¬å¯ä»¥ç”¨ä¸‰ä¸ªæ­¥éª¤æ€»ç»“è´å¶æ–¯å»ºæ¨¡è¿‡ç¨‹ï¼š
- en: Given some data and some assumptions on how this data could have been generated,
    we design a model by combining building blocks known as **probability distributions**.
    Most of the time these models are crude approximations, but most of the time thatâ€™s
    all we need.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€äº›æ•°æ®å’Œå…³äºè¿™äº›æ•°æ®å¦‚ä½•ç”Ÿæˆçš„å‡è®¾ï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆç§°ä¸º**æ¦‚ç‡åˆ†å¸ƒ**çš„æ„å»ºæ¨¡å—æ¥è®¾è®¡ä¸€ä¸ªæ¨¡å‹ã€‚å¤§å¤šæ•°æ—¶å€™ï¼Œè¿™äº›æ¨¡å‹æ˜¯ç²—ç•¥çš„è¿‘ä¼¼ï¼Œä½†å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æ‰€éœ€è¦çš„ã€‚
- en: We use Bayesâ€™ theorem to add data to our models and derive the logical consequences
    of combining the data and our assumptions. We say we are **conditioning** the
    model on our data.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨è´å¶æ–¯å®šç†å°†æ•°æ®æ·»åŠ åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­ï¼Œå¹¶æ¨å¯¼å‡ºç»“åˆæ•°æ®å’Œå‡è®¾çš„é€»è¾‘åæœã€‚æˆ‘ä»¬è¯´æˆ‘ä»¬æ­£åœ¨**å¯¹æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–**ã€‚
- en: We evaluate the model, and its predictions, under different criteria, including
    the data, our expertise on the subject, and sometimes by comparing it to other
    models.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ ¹æ®ä¸åŒçš„æ ‡å‡†è¯„ä¼°æ¨¡å‹åŠå…¶é¢„æµ‹ï¼ŒåŒ…æ‹¬æ•°æ®ã€æˆ‘ä»¬å¯¹è¯¥ä¸»é¢˜çš„ä¸“ä¸šçŸ¥è¯†ï¼Œæœ‰æ—¶è¿˜ä¼šé€šè¿‡ä¸å…¶ä»–æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚
- en: 'In general, we will find ourselves performing these three steps in an iterative
    non-linear fashion. We will retrace our steps at any given point: maybe we made
    a silly coding mistake, or we found a way to change the model and improve it,
    or we realized that we need to add more data or collect a different kind of data.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œæˆ‘ä»¬ä¼šå‘ç°è‡ªå·±åœ¨ä¸€ä¸ªè¿­ä»£çš„éçº¿æ€§æ–¹å¼ä¸­æ‰§è¡Œè¿™ä¸‰ä¸ªæ­¥éª¤ã€‚æˆ‘ä»¬ä¼šåœ¨ä»»ä½•æ—¶å€™é‡æ–°è¿½æº¯æˆ‘ä»¬çš„æ­¥éª¤ï¼šä¹Ÿè®¸æˆ‘ä»¬çŠ¯äº†ä¸€ä¸ªæ„šè ¢çš„ç¼–ç é”™è¯¯ï¼Œæˆ–è€…æˆ‘ä»¬æ‰¾åˆ°äº†ä¸€ç§æ–¹æ³•æ¥æ”¹å˜æ¨¡å‹å¹¶æ”¹è¿›å®ƒï¼Œæˆ–è€…æˆ‘ä»¬æ„è¯†åˆ°éœ€è¦æ·»åŠ æ›´å¤šçš„æ•°æ®æˆ–æ”¶é›†ä¸åŒç§ç±»çš„æ•°æ®ã€‚
- en: Bayesian models are also known as **probabilistic models** because they are
    built using probabilities. Why probabilities? Because probabilities are a very
    useful tool to model uncertainty; we even have good arguments to state they are
    the correct mathematical concept. So letâ€™s take a walk through *the garden of
    forking paths* [[Borges](Bibliography.xhtml#Xborges),Â [1944](Bibliography.xhtml#Xborges)].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯æ¨¡å‹ä¹Ÿè¢«ç§°ä¸º**æ¦‚ç‡æ¨¡å‹**ï¼Œå› ä¸ºå®ƒä»¬æ˜¯é€šè¿‡æ¦‚ç‡æ„å»ºçš„ã€‚ä¸ºä»€ä¹ˆæ˜¯æ¦‚ç‡ï¼Ÿå› ä¸ºæ¦‚ç‡æ˜¯å»ºæ¨¡ä¸ç¡®å®šæ€§çš„éå¸¸æœ‰ç”¨çš„å·¥å…·ï¼›æˆ‘ä»¬ç”šè‡³æœ‰å……åˆ†çš„ç†ç”±è®¤ä¸ºå®ƒä»¬æ˜¯æ­£ç¡®çš„æ•°å­¦æ¦‚å¿µã€‚æ‰€ä»¥è®©æˆ‘ä»¬ä¸€èµ·èµ°è¿›
    *å‰è·¯èŠ±å›­* [[Borges](Bibliography.xhtml#Xborges),Â [1944](Bibliography.xhtml#Xborges)]ã€‚
- en: 1.4 A probability primer for Bayesian practitioners
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 è´å¶æ–¯å®è·µè€…çš„æ¦‚ç‡å…¥é—¨
- en: In this section, we are going to discuss a few general and important concepts
    that are key for better understanding Bayesian methods. Additional probability-related
    concepts will be introduced or elaborated on in future chapters, as we need them.
    For a detailed study of probability theory, however, I highly recommend the book
    *Introduction to Probability* by [Blitzstein](Bibliography.xhtml#Xblitzstein_2019)Â [[2019](Bibliography.xhtml#Xblitzstein_2019)].
    Those already familiar with the basic elements of probability theory can skip
    this section or skim it.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸€äº›å¯¹äºæ›´å¥½ç†è§£è´å¶æ–¯æ–¹æ³•è‡³å…³é‡è¦çš„é€šç”¨æ¦‚å¿µå’Œé‡è¦æ¦‚å¿µã€‚æœªæ¥çš„ç« èŠ‚å°†æ ¹æ®éœ€è¦ä»‹ç»æˆ–è¯¦ç»†è¯´æ˜å…¶ä»–ä¸æ¦‚ç‡ç›¸å…³çš„æ¦‚å¿µã€‚ç„¶è€Œï¼Œå¯¹äºæ¦‚ç‡è®ºçš„è¯¦ç»†å­¦ä¹ ï¼Œæˆ‘å¼ºçƒˆæ¨è[Blitzstein](Bibliography.xhtml#Xblitzstein_2019)çš„ã€Š*Introduction
    to Probability*ã€‹[[2019](Bibliography.xhtml#Xblitzstein_2019)]ä¸€ä¹¦ã€‚å·²ç»ç†Ÿæ‚‰æ¦‚ç‡è®ºåŸºæœ¬è¦ç´ çš„è¯»è€…å¯ä»¥è·³è¿‡æœ¬èŠ‚æˆ–å¿«é€Ÿæµè§ˆã€‚
- en: 1.4.1 Sample space and events
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.1 æ ·æœ¬ç©ºé—´ä¸äº‹ä»¶
- en: 'Letâ€™s say we are surveying to see how people feel about the weather in their
    area. We asked three individuals whether they enjoy sunny weather, with possible
    responses being â€œyesâ€ or â€œno.â€ The sample space of all possible outcomes can be
    denoted by *S* and consists of eight possible combinations:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æ­£åœ¨è°ƒæŸ¥äººä»¬å¯¹è‡ªå·±æ‰€åœ¨åœ°åŒºå¤©æ°”çš„çœ‹æ³•ã€‚æˆ‘ä»¬é—®äº†ä¸‰ä¸ªäººæ˜¯å¦å–œæ¬¢æ™´å¤©ï¼Œå¯èƒ½çš„å›ç­”æ˜¯â€œæ˜¯â€æˆ–â€œå¦â€ã€‚æ‰€æœ‰å¯èƒ½ç»“æœçš„æ ·æœ¬ç©ºé—´å¯ä»¥ç”¨*S*è¡¨ç¤ºï¼Œå¹¶åŒ…å«å…«ç§å¯èƒ½çš„ç»„åˆï¼š
- en: '*S* = {(yes, yes, yes), (yes, yes, no), (yes, no, yes), (no, yes, yes), (yes,
    no, no), (no, yes, no), (no, no, yes), (no, no, no)}'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*S* = {(æ˜¯, æ˜¯, æ˜¯), (æ˜¯, æ˜¯, å¦), (æ˜¯, å¦, æ˜¯), (å¦, æ˜¯, æ˜¯), (æ˜¯, å¦, å¦), (å¦, æ˜¯, å¦), (å¦,
    å¦, æ˜¯), (å¦, å¦, å¦)}'
- en: Here, each element of the sample space represents the responses of the three
    individuals in the order they were asked. For example, (yes, no, yes) means the
    first and third people answered â€œyesâ€ while the second person answered â€œno.â€
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæ ·æœ¬ç©ºé—´ä¸­çš„æ¯ä¸ªå…ƒç´ ä»£è¡¨ä¸‰ä¸ªäººæ ¹æ®è¢«é—®åˆ°çš„é¡ºåºæ‰€åšå‡ºçš„å›ç­”ã€‚ä¾‹å¦‚ï¼Œ(æ˜¯, å¦, æ˜¯)è¡¨ç¤ºç¬¬ä¸€å’Œç¬¬ä¸‰ä¸ªäººå›ç­”äº†â€œæ˜¯â€ï¼Œè€Œç¬¬äºŒä¸ªäººå›ç­”äº†â€œå¦â€ã€‚
- en: 'We can define events as subsets of the sample space. For example, event *A*
    is when all three individuals answered â€œyesâ€:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†äº‹ä»¶å®šä¹‰ä¸ºæ ·æœ¬ç©ºé—´çš„å­é›†ã€‚ä¾‹å¦‚ï¼Œäº‹ä»¶*A*å°±æ˜¯æ‰€æœ‰ä¸‰ä¸ªäººéƒ½å›ç­”â€œæ˜¯â€æ—¶å‘ç”Ÿçš„äº‹ä»¶ï¼š
- en: '*A* = {(yes, yes, yes)}'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*A* = {(æ˜¯, æ˜¯, æ˜¯)}'
- en: 'Similarly, we can define event *B* as when at least one person answered â€œno,â€
    and then we will have:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰äº‹ä»¶*B*ä¸ºè‡³å°‘æœ‰ä¸€ä¸ªäººå›ç­”â€œå¦â€çš„æƒ…å†µï¼Œç„¶åæˆ‘ä»¬å°†å¾—åˆ°ï¼š
- en: '*B* = {(yes, yes, no), (yes, no, yes), (no, yes, yes), (yes, no, no), (no,
    yes, no), (no, no, yes), (no, no, no)}'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*B* = {(æ˜¯, æ˜¯, å¦), (æ˜¯, å¦, æ˜¯), (å¦, æ˜¯, æ˜¯), (æ˜¯, å¦, å¦), (å¦, æ˜¯, å¦), (å¦, å¦, æ˜¯), (å¦,
    å¦, å¦)}'
- en: 'We can use probabilities as a measure of how likely these events are. Assuming
    all events are equally likely, the probability of event *A*, which is the event
    that all three individuals answered â€œyes,â€ is:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¦‚ç‡æ¥è¡¡é‡è¿™äº›äº‹ä»¶å‘ç”Ÿçš„å¯èƒ½æ€§ã€‚å‡è®¾æ‰€æœ‰äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ç›¸ç­‰ï¼Œé‚£ä¹ˆäº‹ä»¶*A*çš„æ¦‚ç‡ï¼Œå³æ‰€æœ‰ä¸‰ä¸ªäººéƒ½å›ç­”â€œæ˜¯â€çš„äº‹ä»¶æ¦‚ç‡ä¸ºï¼š
- en: '![ number of outcomes in A P (A) = ---------------------------- total number
    of outcomes in S ](img/file4.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![ A äº‹ä»¶çš„ç»“æœæ•° P (A) = ---------------------------- æ ·æœ¬ç©ºé—´ S çš„æ€»ç»“æœæ•° ](img/file4.jpg)'
- en: 'In this case, there is only one outcome in *A*, and there are eight outcomes
    in *S*. Therefore, the probability of *A* is:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ*A*ä¸­åªæœ‰ä¸€ä¸ªç»“æœï¼Œè€Œ*S*ä¸­æœ‰å…«ä¸ªç»“æœã€‚å› æ­¤ï¼Œ*A*çš„æ¦‚ç‡ä¸ºï¼š
- en: '![ 1 P(A ) = 8 = 0.125 ](img/file5.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![ 1 P(A ) = 8 = 0.125 ](img/file5.jpg)'
- en: 'Similarly, we can calculate the probability of event *B*, which is the event
    that at least one person answered â€œno.â€ Since there are seven outcomes in *B*
    and eight outcomes in *S*, the probability of *B* is:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—äº‹ä»¶*B*çš„æ¦‚ç‡ï¼Œè¿™ä¸ªäº‹ä»¶è¡¨ç¤ºè‡³å°‘æœ‰ä¸€ä¸ªäººå›ç­”â€œå¦â€ã€‚ç”±äº*B*ä¸­æœ‰ä¸ƒä¸ªç»“æœï¼Œè€Œ*S*ä¸­æœ‰å…«ä¸ªç»“æœï¼Œäº‹ä»¶*B*çš„æ¦‚ç‡ä¸ºï¼š
- en: '![P(B ) = 7-= 0.875 8 ](img/file6.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![P(B ) = 7-= 0.875 8 ](img/file6.jpg)'
- en: 'Considering all events equally likely is just a particular case that makes
    calculating probabilities easier. This is something called the naive definition
    of probability since it is restrictive and relies on strong assumptions. However,
    it is still useful if we are cautious when using it. For instance, it is not true
    that all yes-no questions have a 50-50 chance. Another example. What is the probability
    of seeing a purple horse? The right answer can vary a lot depending on whether
    weâ€™re talking about the natural color of a real horse, a horse from a cartoon,
    a horse dressed in a parade, etc. Anyway, no matter if the events are equally
    likely or not, the probability of the entire sample space is always equal to 1\.
    We can see that this is true by computing:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰äº‹ä»¶è§†ä¸ºåŒæ ·å¯èƒ½çš„äº‹ä»¶åªæ˜¯ä¸€ä¸ªç‰¹æ®Šæƒ…å†µï¼Œå®ƒä½¿å¾—è®¡ç®—æ¦‚ç‡æ›´ä¸ºç®€ä¾¿ã€‚è¿™è¢«ç§°ä¸ºæœ´ç´ çš„æ¦‚ç‡å®šä¹‰ï¼Œå› ä¸ºå®ƒå…·æœ‰å±€é™æ€§å¹¶ä¾èµ–äºå¼ºå‡è®¾ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬è°¨æ…ä½¿ç”¨ï¼Œå®ƒä»ç„¶æ˜¯æœ‰ç”¨çš„ã€‚ä¾‹å¦‚ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„â€œæ˜¯-å¦â€é—®é¢˜éƒ½æœ‰50-50çš„æ¦‚ç‡ã€‚å†ä¸¾ä¸ªä¾‹å­ï¼Œçœ‹åˆ°ä¸€åŒ¹ç´«è‰²çš„é©¬çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿæ­£ç¡®ç­”æ¡ˆå¯ä»¥æœ‰å¾ˆå¤§çš„ä¸åŒï¼Œå…·ä½“å–å†³äºæˆ‘ä»¬æ˜¯åœ¨è°ˆè®ºä¸€åŒ¹çœŸå®é©¬çš„è‡ªç„¶é¢œè‰²ã€å¡é€šä¸­çš„é©¬ã€ä¸€åŒ¹ç©¿ç€æ¸¸è¡Œæœè£…çš„é©¬ï¼Œç­‰ç­‰ã€‚æ— è®ºäº‹ä»¶æ˜¯å¦ç­‰å¯èƒ½ï¼Œæ•´ä¸ªæ ·æœ¬ç©ºé—´çš„æ¦‚ç‡æ€»æ˜¯ç­‰äº1ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¡ç®—æ¥éªŒè¯è¿™ä¸€ç‚¹ï¼š
- en: '![ number of outcomes in S P (S) = ---------------------------- total number
    of outcomes in S ](img/file7.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![ ç»“æœæ•°ç›® S P (S) = ---------------------------- S ä¸­ç»“æœçš„æ€»æ•° ](img/file7.jpg)'
- en: 1 is the highest value a probability can take. Saying that *P*(*S*) = 1 is saying
    that *S* is not only very likely, it is certain. If everything that can happen
    is defined by *S*, then *S* will happen.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 1æ˜¯æ¦‚ç‡èƒ½è¾¾åˆ°çš„æœ€é«˜å€¼ã€‚è¯´*P*(*S*) = 1å°±æ„å‘³ç€*S*ä¸ä»…éå¸¸å¯èƒ½ï¼Œå®ƒæ˜¯ç¡®å®šçš„ã€‚å¦‚æœ*S*å®šä¹‰äº†æ‰€æœ‰å¯èƒ½å‘ç”Ÿçš„äº‹æƒ…ï¼Œé‚£ä¹ˆ*S*ä¸€å®šä¼šå‘ç”Ÿã€‚
- en: 'If an event is impossible, then its probability is 0\. Letâ€™s define the event
    *C* as the event of three persons saying â€œbananaâ€:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸€ä¸ªäº‹ä»¶æ˜¯ä¸å¯èƒ½çš„ï¼Œé‚£ä¹ˆå®ƒçš„æ¦‚ç‡å°±æ˜¯0ã€‚æˆ‘ä»¬å®šä¹‰äº‹ä»¶*C*ä¸ºä¸‰ä¸ªäººéƒ½è¯´â€œé¦™è•‰â€çš„äº‹ä»¶ï¼š
- en: '*C* = {(banana, banana, banana)}'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*C* = {(é¦™è•‰, é¦™è•‰, é¦™è•‰)}'
- en: As *C* is not part of *S*, by definition, it cannot happen. Think of this as
    the questionnaire from our survey only having two boxes, *yes* and *no*. By design,
    our survey is restricting all other possible options.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº*C*ä¸æ˜¯*S*çš„ä¸€éƒ¨åˆ†ï¼Œæ ¹æ®å®šä¹‰ï¼Œå®ƒæ˜¯æ— æ³•å‘ç”Ÿçš„ã€‚å¯ä»¥æŠŠå®ƒçœ‹ä½œæ˜¯æˆ‘ä»¬çš„è°ƒæŸ¥é—®å·åªæœ‰ä¸¤ä¸ªé€‰é¡¹ï¼Œ*yes*å’Œ*no*ã€‚æ ¹æ®è®¾è®¡ï¼Œæˆ‘ä»¬çš„è°ƒæŸ¥é™åˆ¶äº†æ‰€æœ‰å…¶ä»–å¯èƒ½çš„é€‰é¡¹ã€‚
- en: 'We can take advantage of the fact that Python includes sets and define a Python
    function to compute probabilities following their naive definition:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åˆ©ç”¨PythonåŒ…å«é›†åˆçš„äº‹å®ï¼Œå¹¶å®šä¹‰ä¸€ä¸ªPythonå‡½æ•°æ¥æŒ‰ç…§å®ƒä»¬çš„æœ´ç´ å®šä¹‰è®¡ç®—æ¦‚ç‡ï¼š
- en: '**CodeÂ 1.1**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  1.1**'
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: I left for the reader the joy of playing with this function.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æŠŠç©è¿™ä¸ªå‡½æ•°çš„ä¹è¶£ç•™ç»™è¯»è€…äº†ã€‚
- en: One useful way to conceptualize probabilities is as conserved quantities distributed
    throughout the sample space. This means that if the probability of one event increases,
    the probability of some other event or events must decrease so that the total
    probability remains equal to 1\. This can be illustrated with a simple example.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§æœ‰ç”¨çš„ç†è§£æ¦‚ç‡çš„æ–¹å¼æ˜¯å°†æ¦‚ç‡è§†ä¸ºåˆ†å¸ƒåœ¨æ ·æœ¬ç©ºé—´ä¸­çš„å®ˆæ’é‡ã€‚è¿™æ„å‘³ç€å¦‚æœä¸€ä¸ªäº‹ä»¶çš„æ¦‚ç‡å¢åŠ ï¼Œå…¶ä»–ä¸€äº›äº‹ä»¶çš„æ¦‚ç‡å¿…é¡»å‡å°‘ï¼Œä»¥ä¾¿æ€»æ¦‚ç‡ä¿æŒä¸º1ã€‚å¯ä»¥é€šè¿‡ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥è¯´æ˜è¿™ä¸€ç‚¹ã€‚
- en: Suppose we ask one person whether it will rain tomorrow, with possible responses
    of â€œyesâ€ and â€œno.â€ The sample space for possible responses is given by *S* = {yes,
    no}. An event that will rain tomorrow is represented by *A* = {yes}. If *P*(*A*),
    is 0.5, then the probability of the complement of event *A*, denoted by *P*![(Ac)](img/file8.jpg),
    must also be 0.5\. If for some reason *P*(*A*) increases to 0.8, then *P*![ c
    (A )](img/file9.jpg) must decrease to 0.2\. This property holds for disjoint events,
    which are events that cannot occur simultaneously. For instance, it cannot *rain*
    and *not rain* at the same time tomorrow. You may object that it can rain during
    the morning and not rain during the afternoon. That is true, but thatâ€™s a different
    sample space!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬é—®æŸäººæ˜å¤©æ˜¯å¦ä¼šä¸‹é›¨ï¼Œå¯èƒ½çš„å›ç­”æ˜¯â€œæ˜¯â€æˆ–â€œå¦â€ã€‚å¯èƒ½çš„å›ç­”çš„æ ·æœ¬ç©ºé—´æ˜¯*S* = {æ˜¯, å¦}ã€‚è¡¨ç¤ºæ˜å¤©ä¼šä¸‹é›¨çš„äº‹ä»¶æ˜¯*A* = {æ˜¯}ã€‚å¦‚æœ*P*(*A*)æ˜¯0.5ï¼Œé‚£ä¹ˆäº‹ä»¶*A*çš„è¡¥é›†äº‹ä»¶çš„æ¦‚ç‡ï¼Œå³*P*![(Ac)](img/file8.jpg)ï¼Œä¹Ÿå¿…é¡»æ˜¯0.5ã€‚å¦‚æœç”±äºæŸç§åŸå› *P*(*A*)å¢åŠ åˆ°0.8ï¼Œé‚£ä¹ˆ*P*![
    c (A )](img/file9.jpg)å¿…é¡»å‡å°‘åˆ°0.2ã€‚è¿™ä¸ªç‰¹æ€§é€‚ç”¨äºäº’æ–¥äº‹ä»¶ï¼Œå³ä¸èƒ½åŒæ—¶å‘ç”Ÿçš„äº‹ä»¶ã€‚ä¾‹å¦‚ï¼Œæ˜å¤©ä¸å¯èƒ½åŒæ—¶â€œä¸‹é›¨â€å’Œâ€œä¸ä¸‹é›¨â€ã€‚ä½ å¯èƒ½ä¼šåé©³ï¼Œæ—©ä¸Šå¯èƒ½ä¸‹é›¨ï¼Œä¸‹åˆä¸ä¸‹é›¨ã€‚æ²¡é”™ï¼Œä½†é‚£æ˜¯ä¸åŒçš„æ ·æœ¬ç©ºé—´ï¼
- en: 'So far, we have avoided directly defining probabilities, and instead, we have
    just shown some of their properties and ways to compute them. A general definition
    of probability that works for non-equally likely events is as follows. Given a
    sample space *S*, and the event *A*, which is a subset of *S*, a probability is
    a function *P*, which takes *A* as input and returns a real number between 0 and
    1, as output. The function *P* has some restrictions, defined by the following
    3 axioms. Keep in mind that an axiom is a statement that is taken to be true and
    that we use as the starting point in our reasoning:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬é¿å…äº†ç›´æ¥å®šä¹‰æ¦‚ç‡ï¼Œè€Œæ˜¯å±•ç¤ºäº†ä¸€äº›æ¦‚ç‡çš„æ€§è´¨ä»¥åŠè®¡ç®—æ–¹æ³•ã€‚é€‚ç”¨äºéç­‰å¯èƒ½äº‹ä»¶çš„æ¦‚ç‡çš„ä¸€ä¸ªä¸€èˆ¬å®šä¹‰å¦‚ä¸‹ã€‚ç»™å®šä¸€ä¸ªæ ·æœ¬ç©ºé—´*S*ï¼Œä»¥åŠäº‹ä»¶*A*ï¼Œå®ƒæ˜¯*S*çš„ä¸€ä¸ªå­é›†ï¼Œæ¦‚ç‡æ˜¯ä¸€ä¸ªå‡½æ•°*P*ï¼Œå®ƒä»¥*A*ä¸ºè¾“å…¥ï¼Œè¿”å›ä¸€ä¸ªä»‹äº0å’Œ1ä¹‹é—´çš„å®æ•°ä½œä¸ºè¾“å‡ºã€‚å‡½æ•°*P*æœ‰ä¸€äº›é™åˆ¶ï¼Œè¿™äº›é™åˆ¶ç”±ä»¥ä¸‹ä¸‰ä¸ªå…¬ç†å®šä¹‰ã€‚è¯·è®°ä½ï¼Œå…¬ç†æ˜¯è¢«è®¤ä¸ºä¸ºçœŸçš„é™ˆè¿°ï¼Œæˆ‘ä»¬ç”¨å®ƒä½œä¸ºæ¨ç†çš„èµ·ç‚¹ï¼š
- en: The probability of an event is a non-negative real number
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: äº‹ä»¶çš„æ¦‚ç‡æ˜¯ä¸€ä¸ªéè´Ÿå®æ•°ã€‚
- en: '*P*(*S*) = 1'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*P*(*S*) = 1'
- en: If *A*1*,A*2*,â€¦* are disjoint events, meaning they cannot occur simultaneously
    then *P*(*A*1*,A*2*,â€¦*) = *P*(*A*1) + *P*(*A*2) + *â€¦*
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœ*A*1*, A*2*, â€¦*æ˜¯äº’æ–¥äº‹ä»¶ï¼Œæ„å‘³ç€å®ƒä»¬ä¸èƒ½åŒæ—¶å‘ç”Ÿï¼Œé‚£ä¹ˆ*P*(*A*1*, A*2*, â€¦*) = *P*(*A*1) + *P*(*A*2)
    + *â€¦*
- en: If this were a book on probability theory, we would likely dedicate a few pages
    to demonstrating the consequences of these axioms and provide exercises for manipulating
    probabilities. That would help us to become proficient in manipulating probabilities.
    However, our main focus is not on those topics. My motivation to present these
    axioms is just to show that probabilities are well-defined mathematical concepts
    with rules that govern their operations. They are a particular type of function,
    and there is no mystery surrounding them.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¿™æ˜¯ä¸€æœ¬å…³äºæ¦‚ç‡è®ºçš„ä¹¦ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šä¸“é—¨ç”¨å‡ é¡µæ¥å±•ç¤ºè¿™äº›å…¬ç†çš„åæœï¼Œå¹¶æä¾›ä¸€äº›ç»ƒä¹ æ¥æ“ä½œæ¦‚ç‡ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬ç†Ÿç»ƒåœ°æ“ä½œæ¦‚ç‡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ä¸»è¦å…³æ³¨ç‚¹ä¸åœ¨è¿™äº›è¯é¢˜ä¸Šã€‚æˆ‘å±•ç¤ºè¿™äº›å…¬ç†çš„åŠ¨æœºä»…ä»…æ˜¯ä¸ºäº†è¯´æ˜æ¦‚ç‡æ˜¯ä¸€ä¸ªå®šä¹‰æ˜ç¡®çš„æ•°å­¦æ¦‚å¿µï¼Œå¹¶ä¸”æœ‰è§„åˆ™æ¥æ”¯é…å®ƒä»¬çš„è¿ç®—ã€‚å®ƒä»¬æ˜¯ç‰¹å®šç±»å‹çš„å‡½æ•°ï¼Œå¹¶ä¸”å¹¶ä¸ç¥ç§˜ã€‚
- en: 1.4.2 Random variables
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.2 éšæœºå˜é‡
- en: A random variable is a function that maps the sample space into the real numbers
    â„ (see *Figure [1.1](#x1-22003r1)*). Letâ€™s assume the events of interest are the
    number of a die, the mapping is very simple, we associate ![PIC](img/dice_1.png)
    with the number 1, ![PIC](img/dice_2.png) with 2, etc. Another simple example
    is the answer to the question, will it rain tomorrow? We can map â€œyesâ€ to 1 and
    â€œnoâ€ to 0\. It is common, but not always the case, to use a capital letter for
    random variables like *X* and a lowercase letter for their outcomes *x*. For example,
    if *X* represents a single roll of a die, then *x* represents some specific integer
    {1*,*2*,*3*,*4*,*5*,*6}. Thus, we can write *P*(*X* = 3) to indicate the probability
    of getting the value 3, when rolling a die. We can also leave *x* unspecified,
    for instance, we can write *P*(*X* = *x*) to indicate the probability of getting
    some value *x*, or *P*(*X* â‰¤ *x*), to indicate the probability of getting a value
    less than or equal to *x*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºå˜é‡æ˜¯ä¸€ä¸ªå°†æ ·æœ¬ç©ºé—´æ˜ å°„åˆ°å®æ•°â„çš„å‡½æ•°ï¼ˆè§*å›¾ [1.1](#x1-22003r1)*ï¼‰ã€‚å‡è®¾æˆ‘ä»¬å…³æ³¨çš„äº‹ä»¶æ˜¯éª°å­çš„ç‚¹æ•°ï¼Œæ˜ å°„éå¸¸ç®€å•ï¼Œæˆ‘ä»¬å°†![PIC](img/dice_1.png)ä¸æ•°å­—1å…³è”ï¼Œ![PIC](img/dice_2.png)ä¸2ï¼Œä¾æ­¤ç±»æ¨ã€‚å¦ä¸€ä¸ªç®€å•çš„ä¾‹å­æ˜¯å›ç­”é—®é¢˜â€œæ˜å¤©ä¼šä¸‹é›¨å—ï¼Ÿâ€ï¼Œæˆ‘ä»¬å¯ä»¥å°†â€œæ˜¯â€æ˜ å°„ä¸º1ï¼Œå°†â€œå¦â€æ˜ å°„ä¸º0ã€‚é€šå¸¸ï¼Œéšæœºå˜é‡ä½¿ç”¨å¤§å†™å­—æ¯è¡¨ç¤ºï¼Œå¦‚*X*ï¼Œè€Œå…¶ç»“æœä½¿ç”¨å°å†™å­—æ¯è¡¨ç¤ºï¼Œå¦‚*x*ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ*X*è¡¨ç¤ºä¸€æ¬¡éª°å­æŠ•æ·ï¼Œé‚£ä¹ˆ*x*è¡¨ç¤ºæŸä¸ªç‰¹å®šçš„æ•´æ•°{1,*2*,*3*,*4*,*5*,*6*}ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å†™*P*(*X*
    = 3)æ¥è¡¨ç¤ºæŠ•æ·éª°å­å¾—åˆ°3çš„æ¦‚ç‡ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ä¸æŒ‡å®š*x*ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å†™*P*(*X* = *x*)æ¥è¡¨ç¤ºå¾—åˆ°æŸä¸ªå€¼*x*çš„æ¦‚ç‡ï¼Œæˆ–è€…å†™*P*(*X*
    â‰¤ *x*)ï¼Œè¡¨ç¤ºå¾—åˆ°å°äºæˆ–ç­‰äº*x*çš„æ¦‚ç‡ã€‚
- en: Being able to map symbols like ![PIC](img/dice_1.png) or strings like â€œyesâ€
    to numbers makes analysis simpler as we already know how to do math with numbers.
    Random variables are also useful because we can operate with them without directly
    thinking in terms of the sample space. This feature becomes more and more relevant
    as the sample space becomes more complex. For example, when simulating molecular
    systems, we need to specify the position and velocity of each atom; for complex
    molecules like proteins this means that we will need to track thousands, millions,
    or even larger numbers. Instead, we can use random variables to summarize certain
    properties of the system, such as the total energy or the relative angles between
    certain atoms of the system.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: èƒ½å¤Ÿå°†ç¬¦å·å¦‚![PIC](img/dice_1.png)æˆ–å­—ç¬¦ä¸²å¦‚â€œyesâ€æ˜ å°„åˆ°æ•°å­—ä¸Šï¼Œä½¿å¾—åˆ†æå˜å¾—æ›´åŠ ç®€å•ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»çŸ¥é“å¦‚ä½•ç”¨æ•°å­—è¿›è¡Œæ•°å­¦è¿ç®—ã€‚éšæœºå˜é‡ä¹Ÿå¾ˆæœ‰ç”¨ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥åœ¨ä¸ç›´æ¥è€ƒè™‘æ ·æœ¬ç©ºé—´çš„æƒ…å†µä¸‹å¯¹å®ƒä»¬è¿›è¡Œæ“ä½œã€‚éšç€æ ·æœ¬ç©ºé—´å˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œè¿™ä¸€ç‰¹ç‚¹å˜å¾—æ„ˆåŠ é‡è¦ã€‚ä¾‹å¦‚ï¼Œåœ¨æ¨¡æ‹Ÿåˆ†å­ç³»ç»Ÿæ—¶ï¼Œæˆ‘ä»¬éœ€è¦æŒ‡å®šæ¯ä¸ªåŸå­çš„ä½ç½®ä¿¡æ¯å’Œé€Ÿåº¦ï¼›å¯¹äºåƒè›‹ç™½è´¨è¿™æ ·å¤æ‚çš„åˆ†å­ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦è¿½è¸ªæˆåƒä¸Šä¸‡ç”šè‡³æ›´å¤šçš„æ•°å­—ã€‚ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨éšæœºå˜é‡æ¥æ€»ç»“ç³»ç»Ÿçš„æŸäº›å±æ€§ï¼Œæ¯”å¦‚æ€»èƒ½é‡æˆ–ç³»ç»Ÿä¸­æŸäº›åŸå­ä¹‹é—´çš„ç›¸å¯¹è§’åº¦ã€‚
- en: If you are still confused, thatâ€™s fine. The concept of a random variable may
    sound too abstract at the beginning, but we will see plenty of examples throughout
    the book that will help you cement these ideas. Before moving on, let me try one
    analogy that I hope you find useful. Random variables are useful in a similar
    way to how Python functions are useful. We often encapsulate code within functions,
    so we can store, reuse, and *hide* complex manipulations of data into a single
    call. Even more, once we have a few functions, we can sometimes combine them in
    many ways, like adding the output of two functions or using the output of one
    function as the input of the other. We can do all this without functions, but
    abstracting away the inner workings not only makes the code cleaner, it also helps
    with understanding and fostering new ideas. Random variables play a similar role
    in statistics.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä»ç„¶æ„Ÿåˆ°å›°æƒ‘ï¼Œé‚£ä¹Ÿæ²¡å…³ç³»ã€‚éšæœºå˜é‡çš„æ¦‚å¿µåˆšå¼€å§‹å¯èƒ½æ˜¾å¾—è¿‡äºæŠ½è±¡ï¼Œä½†æˆ‘ä»¬å°†åœ¨å…¨ä¹¦ä¸­çœ‹åˆ°è®¸å¤šä¾‹å­ï¼Œå¸®åŠ©ä½ å·©å›ºè¿™äº›æ¦‚å¿µã€‚åœ¨ç»§ç»­ä¹‹å‰ï¼Œæˆ‘æƒ³ä¸¾ä¸€ä¸ªç±»æ¯”ï¼Œå¸Œæœ›å¯¹ä½ æœ‰å¸®åŠ©ã€‚éšæœºå˜é‡çš„ä½œç”¨ç±»ä¼¼äºPythonå‡½æ•°çš„ä½œç”¨ã€‚æˆ‘ä»¬é€šå¸¸å°†ä»£ç å°è£…åœ¨å‡½æ•°ä¸­ï¼Œè¿™æ ·å°±å¯ä»¥å°†å¤æ‚çš„æ•°æ®æ“ä½œå­˜å‚¨ã€é‡ç”¨ï¼Œå¹¶é€šè¿‡ä¸€æ¬¡è°ƒç”¨æ¥*éšè—*ã€‚æ›´è¿›ä¸€æ­¥ï¼Œå½“æˆ‘ä»¬æ‹¥æœ‰å¤šä¸ªå‡½æ•°æ—¶ï¼Œæœ‰æ—¶å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼ç»„åˆå®ƒä»¬ï¼Œæ¯”å¦‚å°†ä¸¤ä¸ªå‡½æ•°çš„è¾“å‡ºç›¸åŠ ï¼Œæˆ–å°†ä¸€ä¸ªå‡½æ•°çš„è¾“å‡ºä½œä¸ºå¦ä¸€ä¸ªå‡½æ•°çš„è¾“å…¥ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨æ²¡æœ‰å‡½æ•°çš„æƒ…å†µä¸‹å®Œæˆè¿™äº›æ“ä½œï¼Œä½†å°†å†…éƒ¨å·¥ä½œæŠ½è±¡åŒ–ä¸ä»…è®©ä»£ç æ›´ç®€æ´ï¼Œè¿˜å¸®åŠ©ç†è§£å’Œæ¿€å‘æ–°çš„åˆ›æ„ã€‚éšæœºå˜é‡åœ¨ç»Ÿè®¡å­¦ä¸­èµ·ç€ç±»ä¼¼çš„ä½œç”¨ã€‚
- en: '![PIC](img/file10.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file10.png)'
- en: '**FigureÂ 1.1**: A random variable *X* defined on a sample space with 5 elements
    {*S*[1]*,*![â‹…â‹…â‹…](img/file11.jpg)*S*[5]}, and possible values -1, 2, and *Ï€*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 1.1**ï¼šåœ¨ä¸€ä¸ªåŒ…å«5ä¸ªå…ƒç´ çš„æ ·æœ¬ç©ºé—´ä¸Šå®šä¹‰çš„éšæœºå˜é‡*X*ï¼Œå…¶ä¸­å…ƒç´ åŒ…æ‹¬{*S*[1]*ï¼Œ*![â‹…â‹…â‹…](img/file11.jpg)*S*[5]}ï¼Œå…¶å¯èƒ½çš„å€¼ä¸º-1ã€2
    å’Œ *Ï€*ã€‚'
- en: The mapping between the sample space and â„ is deterministic. There is no randomness
    involved. So why do we call it a *random* variable? Because we can *ask* the variable
    for values, and every time we ask, we will get a different number. The randomness
    comes from the probability associated with the events. In *Figure [1.1](#x1-22003r1)*,
    we have represented *P* as the size of the circles.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ ·æœ¬ç©ºé—´ä¸â„ä¹‹é—´çš„æ˜ å°„æ˜¯ç¡®å®šæ€§çš„ã€‚è¿™é‡Œæ²¡æœ‰æ¶‰åŠéšæœºæ€§ã€‚é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬ç§°å…¶ä¸º*éšæœº*å˜é‡å‘¢ï¼Ÿå› ä¸ºæˆ‘ä»¬å¯ä»¥*è¯·æ±‚*è¯¥å˜é‡çš„å€¼ï¼Œæ¯æ¬¡è¯·æ±‚æ—¶ï¼Œå¾—åˆ°çš„æ•°å­—éƒ½ä¼šä¸åŒã€‚éšæœºæ€§æ¥æºäºä¸äº‹ä»¶ç›¸å…³çš„æ¦‚ç‡ã€‚åœ¨*å›¾
    [1.1](#x1-22003r1)*ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åœ†åœˆçš„å¤§å°è¡¨ç¤ºäº†*P*ã€‚
- en: The two most common types of random variables are discrete and continuous ones.
    Without going into a proper definition, we are going to say that discrete variables
    take only discrete values and we usually use integers to represent them, like
    1, 5, 42\. And continuous variables take real values, so we use floats to work
    with them, like 3.1415, 1.01, 23.4214, and so on. When we use one or the other
    is problem-dependent. If we ask people about their favorite color, we will get
    answers like â€œred,â€ â€œblue,â€ and â€œgreen.â€ This is an example of a discrete random
    variable. The answers are categories â€“ there are no intermediate values between
    â€œredâ€ and â€œgreen.â€ But if we are studying the properties of light absorption,
    then discrete values like â€œredâ€ and â€œgreenâ€ may not be adequate and instead working
    with wavelength could be more appropriate. In that case, we will expect to get
    values like 650 nm and 510 nm and any number in between, including 579.1\.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ç§æœ€å¸¸è§çš„éšæœºå˜é‡ç±»å‹æ˜¯ç¦»æ•£å‹å’Œè¿ç»­å‹ã€‚è™½ç„¶ä¸åšæ­£å¼å®šä¹‰ï¼Œä½†æˆ‘ä»¬å¯ä»¥è¯´ç¦»æ•£å‹å˜é‡åªå–ç¦»æ•£å€¼ï¼Œé€šå¸¸ä½¿ç”¨æ•´æ•°è¡¨ç¤ºï¼Œä¾‹å¦‚ 1ã€5ã€42ã€‚è¿ç»­å‹å˜é‡åˆ™å–å®æ•°å€¼ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨æµ®ç‚¹æ•°æ¥è¡¨ç¤ºå®ƒä»¬ï¼Œä¾‹å¦‚
    3.1415ã€1.01ã€23.4214 ç­‰ç­‰ã€‚æˆ‘ä»¬ä½¿ç”¨å“ªç§ç±»å‹å–å†³äºå…·ä½“é—®é¢˜ã€‚å¦‚æœæˆ‘ä»¬è¯¢é—®äººä»¬æœ€å–œæ¬¢çš„é¢œè‰²ï¼Œç­”æ¡ˆå¯èƒ½æ˜¯â€œçº¢è‰²â€ã€â€œè“è‰²â€å’Œâ€œç»¿è‰²â€ã€‚è¿™æ˜¯ä¸€ä¸ªç¦»æ•£éšæœºå˜é‡çš„ä¾‹å­ã€‚ç­”æ¡ˆæ˜¯ç±»åˆ«é—´çš„â€”â€”â€œçº¢è‰²â€å’Œâ€œç»¿è‰²â€ä¹‹é—´æ²¡æœ‰ä¸­é—´å€¼ã€‚ä½†å¦‚æœæˆ‘ä»¬ç ”ç©¶å…‰çš„å¸æ”¶ç‰¹æ€§ï¼Œé‚£ä¹ˆåƒâ€œçº¢è‰²â€å’Œâ€œç»¿è‰²â€è¿™æ ·çš„ç¦»æ•£å€¼å¯èƒ½ä¸å¤Ÿå‡†ç¡®ï¼Œè½¬è€Œä½¿ç”¨æ³¢é•¿å¯èƒ½æ›´ä¸ºåˆé€‚ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ç±»ä¼¼
    650 çº³ç±³å’Œ 510 çº³ç±³çš„å€¼ï¼Œå¹¶ä¸”ä»»ä½•ä¸­é—´å€¼ä¹Ÿéƒ½å¯èƒ½å‡ºç°ï¼ŒåŒ…æ‹¬ 579.1 çº³ç±³ã€‚
- en: 1.4.3 Discrete random variables and their distributions
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.3 ç¦»æ•£éšæœºå˜é‡åŠå…¶åˆ†å¸ƒ
- en: Instead of calculating the probability that all three individuals answered â€œyes,â€
    or the probability of getting a 3 when rolling a die, we may be more interested
    in finding out the *list of probabilities* for all possible answers or all possible
    numbers from a die. Once this list is computed, we can inspect it visually or
    use it to compute other quantities like the probability of getting at least one
    â€œno,â€ the probability of getting an odd number, or the probability of getting
    a number equal to or larger than 5\. The formal name of this *list* is **probability**
    **distribution**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯èƒ½ä¸ä»…ä»…æƒ³è®¡ç®—æ‰€æœ‰ä¸‰ä¸ªäººéƒ½å›ç­”â€œæ˜¯â€çš„æ¦‚ç‡ï¼Œæˆ–è€…æ·éª°å­å¾—åˆ°3çš„æ¦‚ç‡ï¼Œæˆ‘ä»¬å¯èƒ½æ›´æ„Ÿå…´è¶£çš„æ˜¯æ‰¾åˆ°æ‰€æœ‰å¯èƒ½ç­”æ¡ˆæˆ–éª°å­ä¸Šæ‰€æœ‰å¯èƒ½æ•°å­—çš„*æ¦‚ç‡åˆ—è¡¨*ã€‚ä¸€æ—¦è¿™ä¸ªåˆ—è¡¨è®¡ç®—å‡ºæ¥ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¯è§†åŒ–æŸ¥çœ‹å®ƒï¼Œæˆ–è€…åˆ©ç”¨å®ƒæ¥è®¡ç®—å…¶ä»–é‡ï¼Œæ¯”å¦‚è‡³å°‘å¾—åˆ°ä¸€ä¸ªâ€œå¦â€çš„æ¦‚ç‡ã€å¾—åˆ°å¥‡æ•°çš„æ¦‚ç‡ï¼Œæˆ–è€…å¾—åˆ°å¤§äºæˆ–ç­‰äº5çš„æ•°å­—çš„æ¦‚ç‡ã€‚è¿™ä¸ª*åˆ—è¡¨*çš„æ­£å¼åç§°æ˜¯**æ¦‚ç‡**
    **åˆ†å¸ƒ**ã€‚
- en: We can get the empirical probability distribution of a die, by rolling it a
    few times and tabulating how many times we got each number. To turn each value
    into a probability and the entire list into a valid probability distribution,
    we need to *normalize* the counts. We can do this by dividing the value we got
    for each number by the number of times we roll the die.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡æ·éª°å­å‡ æ¬¡å¹¶è®°å½•æ¯ä¸ªæ•°å­—å‡ºç°çš„æ¬¡æ•°æ¥è·å¾—éª°å­çš„ç»éªŒæ¦‚ç‡åˆ†å¸ƒã€‚ä¸ºäº†å°†æ¯ä¸ªå€¼è½¬åŒ–ä¸ºæ¦‚ç‡ï¼Œå¹¶ä½¿æ•´ä¸ªåˆ—è¡¨æˆä¸ºæœ‰æ•ˆçš„æ¦‚ç‡åˆ†å¸ƒï¼Œæˆ‘ä»¬éœ€è¦*å½’ä¸€åŒ–*è¿™äº›è®¡æ•°ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å°†æ¯ä¸ªæ•°å­—å‡ºç°çš„æ¬¡æ•°é™¤ä»¥æ·éª°å­çš„æ¬¡æ•°æ¥å®ç°è¿™ä¸€ç‚¹ã€‚
- en: Empirical distributions are very useful, and we are going to extensively use
    them. But instead of rolling dice by hand, we are going to use advanced computational
    methods to do the hard work for us; this will not only save us time and boredom
    but it will allow us to get samples from really complicated distributions effortlessly.
    But we are getting ahead of ourselves. Our priority is to concentrate on theoretical
    distributions, which are central in statistics because, among other reasons, they
    allow the construction of probabilistic models.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ç»éªŒåˆ†å¸ƒéå¸¸æœ‰ç”¨ï¼Œæˆ‘ä»¬å°†å¹¿æ³›ä½¿ç”¨å®ƒä»¬ã€‚ä½†æˆ‘ä»¬ä¸ä¼šå†æ‰‹åŠ¨æ·éª°å­ï¼Œè€Œæ˜¯å°†ä½¿ç”¨å…ˆè¿›çš„è®¡ç®—æ–¹æ³•æ¥ä¸ºæˆ‘ä»¬å®Œæˆè¿™é¡¹ç¹é‡çš„å·¥ä½œï¼›è¿™ä¸ä»…èƒ½èŠ‚çœæˆ‘ä»¬çš„æ—¶é—´å’Œé¿å…æ— èŠï¼Œè¿˜èƒ½è®©æˆ‘ä»¬è½»æ¾åœ°ä»éå¸¸å¤æ‚çš„åˆ†å¸ƒä¸­è·å–æ ·æœ¬ã€‚ä¸è¿‡æˆ‘ä»¬ç°åœ¨æœ‰ç‚¹æ€¥äºæ±‚æˆã€‚æˆ‘ä»¬çš„ä¼˜å…ˆä»»åŠ¡æ˜¯é›†ä¸­ç²¾åŠ›ç ”ç©¶ç†è®ºåˆ†å¸ƒï¼Œå› ä¸ºå®ƒä»¬åœ¨ç»Ÿè®¡å­¦ä¸­å æ®æ ¸å¿ƒåœ°ä½ï¼ŒåŸå› ä¹‹ä¸€æ˜¯å®ƒä»¬èƒ½å¤Ÿæ„å»ºæ¦‚ç‡æ¨¡å‹ã€‚
- en: 'As we saw, there is nothing random or mysterious about random variables; they
    are just a type of mathematical function. The same goes for theoretical probability
    distributions. I like to compare probability distributions with circles. Because
    we are all familiar with circles even before we get into school, we are not afraid
    of them and they donâ€™t look mysterious to us. We can define a circle as the geometric
    space of points on a plane that is equidistant from another point called the center.
    We can go further and provide a mathematical expression for this definition. If
    we assume the location of the center is irrelevant, then the circle of radius
    *r* can simply be described as the set of all points (*x,y*) such that:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œéšæœºå˜é‡å¹¶æ²¡æœ‰ä»€ä¹ˆéšæœºæˆ–ç¥ç§˜ä¹‹å¤„ï¼›å®ƒä»¬åªæ˜¯æ•°å­¦å‡½æ•°çš„ä¸€ç§ç±»å‹ã€‚ç†è®ºæ¦‚ç‡åˆ†å¸ƒä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘å–œæ¬¢å°†æ¦‚ç‡åˆ†å¸ƒä¸åœ†å½¢è¿›è¡Œæ¯”è¾ƒã€‚å› ä¸ºæˆ‘ä»¬åœ¨ä¸Šå­¦ä¹‹å‰å°±å·²ç»ç†Ÿæ‚‰åœ†å½¢äº†ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯¹å®ƒä»¬ä¸æ„Ÿåˆ°å®³æ€•ï¼Œå®ƒä»¬ä¹Ÿä¸ä¼šè®©æˆ‘ä»¬è§‰å¾—ç¥ç§˜ã€‚æˆ‘ä»¬å¯ä»¥å°†åœ†å®šä¹‰ä¸ºå¹³é¢ä¸Šä¸å¦ä¸€ä¸ªç‚¹ï¼ˆç§°ä¸ºåœ†å¿ƒï¼‰ç­‰è·çš„æ‰€æœ‰ç‚¹çš„å‡ ä½•ç©ºé—´ã€‚æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥ç»™å‡ºè¿™ä¸ªå®šä¹‰çš„æ•°å­¦è¡¨è¾¾å¼ã€‚å¦‚æœæˆ‘ä»¬å‡è®¾åœ†å¿ƒçš„ä½ç½®æ— å…³ç´§è¦ï¼Œé‚£ä¹ˆåŠå¾„ä¸º*r*çš„åœ†å¯ä»¥ç®€å•åœ°æè¿°ä¸ºæ‰€æœ‰æ»¡è¶³ä»¥ä¸‹æ¡ä»¶çš„ç‚¹é›†ï¼ˆ*x,y*ï¼‰ï¼š
- en: '![x2 + y2 = r2 ](img/file12.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![x2 + y2 = r2 ](img/file12.jpg)'
- en: From this expression, we can see that given the **parameter** *r*, the circle
    is completely defined. This is all we need to plot it and all we need to compute
    properties such as the perimeter, which is 2*Ï€r*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™ä¸ªè¡¨è¾¾å¼ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç»™å®š**å‚æ•°** *r*ï¼Œåœ†å°±è¢«å®Œå…¨å®šä¹‰äº†ã€‚è¿™å°±æ˜¯æˆ‘ä»¬ç»˜åˆ¶å®ƒæ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬è®¡ç®—å¦‚å‘¨é•¿ï¼ˆ2*Ï€r*ï¼‰ç­‰æ€§è´¨æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ã€‚
- en: Now notice that all circles look very similar to each other and that any two
    circles with the same value of *r* are essentially the same objects. Thus we can
    think of the family of circles, where each member is set apart from the rest precisely
    by the value of *r*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¯·æ³¨æ„ï¼Œæ‰€æœ‰çš„åœ†çœ‹èµ·æ¥éƒ½éå¸¸ç›¸ä¼¼ï¼Œå¹¶ä¸”ä»»ä½•ä¸¤ä¸ªåŠå¾„ç›¸åŒçš„åœ†åŸºæœ¬ä¸Šæ˜¯ç›¸åŒçš„å¯¹è±¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠåœ†çš„å®¶æ—çœ‹ä½œæ˜¯å…¶ä¸­æ¯ä¸ªæˆå‘˜éƒ½æ°å¥½é€šè¿‡åŠå¾„ *r* çš„å€¼ä¸å…¶ä»–æˆå‘˜åŒºåˆ†å¼€æ¥çš„ã€‚
- en: So far, so good, but why are we talking about circles? Because all this can
    be directly applied to probability distributions. Both circles and probability
    distributions have mathematical expressions that define them, and these expressions
    have parameters that we can change to define all members of a family of probability
    distributions. *Figure [1.2](#x1-23006r2)* shows four members of one probability
    distribution known as BetaBinomial. In *Figure [1.2](#x1-23006r2)*, the height
    of the bars represents the probability of each *x* value. The values of *x* below
    1 or above 6 have a probability of 0 as they are out of the support of the distribution.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä¸€åˆ‡é¡ºåˆ©ï¼Œä½†æˆ‘ä»¬ä¸ºä»€ä¹ˆè¦è°ˆè®ºåœ†å‘¢ï¼Ÿå› ä¸ºè¿™ä¸€åˆ‡éƒ½å¯ä»¥ç›´æ¥åº”ç”¨äºæ¦‚ç‡åˆ†å¸ƒã€‚åœ†å’Œæ¦‚ç‡åˆ†å¸ƒéƒ½æœ‰å®šä¹‰å®ƒä»¬çš„æ•°å­¦è¡¨è¾¾å¼ï¼Œè¿™äº›è¡¨è¾¾å¼æœ‰æˆ‘ä»¬å¯ä»¥æ”¹å˜çš„å‚æ•°ï¼Œç”¨ä»¥å®šä¹‰æ¦‚ç‡åˆ†å¸ƒå®¶æ—ä¸­çš„æ‰€æœ‰æˆå‘˜ã€‚*å›¾
    [1.2](#x1-23006r2)* æ˜¾ç¤ºäº†ä¸€ä¸ªåä¸ºBetaBinomialçš„æ¦‚ç‡åˆ†å¸ƒçš„å››ä¸ªæˆå‘˜ã€‚åœ¨*å›¾ [1.2](#x1-23006r2)*ä¸­ï¼Œæ¡å½¢çš„é«˜åº¦è¡¨ç¤ºæ¯ä¸ª
    *x* å€¼çš„æ¦‚ç‡ã€‚ä½äº1æˆ–é«˜äº6çš„ *x* å€¼çš„æ¦‚ç‡ä¸º0ï¼Œå› ä¸ºå®ƒä»¬è¶…å‡ºäº†åˆ†å¸ƒçš„æ”¯æŒèŒƒå›´ã€‚
- en: '![PIC](img/file13.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file13.png)'
- en: '**FigureÂ 1.2**: Four members of the BetaBinomial distribution with parameters
    *Î±* and *Î²*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 1.2**ï¼šå…·æœ‰å‚æ•° *Î±* å’Œ *Î²* çš„BetaBinomialåˆ†å¸ƒçš„å››ä¸ªæˆå‘˜'
- en: 'This is the mathematical expression for the BetaBinomial distribution:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯BetaBinomialåˆ†å¸ƒçš„æ•°å­¦è¡¨è¾¾å¼ï¼š
- en: '![ ( ) pmf (x ) = n- B-(x-+-ğ›¼,n-âˆ’-x-+-ğ›½)- x B(ğ›¼,ğ›½ ) ](img/file14.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![ ( ) pmf (x ) = n- B-(x-+-ğ›¼,n-âˆ’-x-+-ğ›½)- x B(ğ›¼,ğ›½ ) ](img/file14.jpg)'
- en: pmf stands for **probability mass function**. For discrete random variables,
    the pmf is the function that returns probabilities. In mathematical notation,
    if we have a random variable *X*, then pmf(*x*) = *P*(*X* = *x*).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: pmfä»£è¡¨**æ¦‚ç‡è´¨é‡å‡½æ•°**ã€‚å¯¹äºç¦»æ•£éšæœºå˜é‡ï¼Œpmfæ˜¯è¿”å›æ¦‚ç‡çš„å‡½æ•°ã€‚åœ¨æ•°å­¦ç¬¦å·ä¸­ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªéšæœºå˜é‡ *X*ï¼Œé‚£ä¹ˆpmf(*x*) = *P*(*X*
    = *x*)ã€‚
- en: Understanding or remembering the pmf of the BetaBinomial has zero importance
    for us. Iâ€™m just showing it here so you can see that this is just another function;
    you put in one number and you get out another number. Nothing weird, at least
    not in principle. I must concede that to fully understand the details of the BetaBinomial
    distribution, we need to know what ![( ) nx](img/file15.jpg) is, known as the
    binomial coefficient, and what *B* is, the Beta function. But thatâ€™s not fundamentally
    different from showing *x*Â² + *y*Â² = *r*Â².
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£æˆ–è®°ä½BetaBinomialçš„pmfå¯¹æˆ‘ä»¬æ¥è¯´å¹¶ä¸é‡è¦ã€‚æˆ‘åªæ˜¯å±•ç¤ºå®ƒï¼Œä»¥ä¾¿ä½ èƒ½çœ‹åˆ°è¿™ä»…ä»…æ˜¯å¦ä¸€ä¸ªå‡½æ•°ï¼›ä½ è¾“å…¥ä¸€ä¸ªæ•°å­—ï¼Œè¾“å‡ºå¦ä¸€ä¸ªæ•°å­—ã€‚æ²¡ä»€ä¹ˆå¥‡æ€ªçš„ï¼Œè‡³å°‘åŸåˆ™ä¸Šæ˜¯è¿™æ ·ã€‚æˆ‘å¿…é¡»æ‰¿è®¤ï¼Œè¦å®Œå…¨ç†è§£BetaBinomialåˆ†å¸ƒçš„ç»†èŠ‚ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“ä»€ä¹ˆæ˜¯
    ![( ) nx](img/file15.jpg)ï¼Œå³äºŒé¡¹ç³»æ•°ï¼Œä»¥åŠ*B*æ˜¯ä»€ä¹ˆï¼Œå³Betaå‡½æ•°ã€‚ä½†è¿™ä¸å±•ç¤º *x*Â² + *y*Â² = *r*Â²å¹¶æ²¡æœ‰æ ¹æœ¬åŒºåˆ«ã€‚
- en: 'Mathematical expressions can be super useful, as they are concise and we can
    use them to derive properties from them. But sometimes that can be too much work,
    even if we are good at math. Visualization can be a good alternative (or complement)
    to help us understand probability distributions. I cannot fully show this on paper,
    but if you run the following, you will get an interactive plot that will update
    every time you move the sliders for the parameters `alpha`, `beta`, and `n`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°å­¦è¡¨è¾¾å¼éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒä»¬ç®€æ´ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨å®ƒä»¬æ¨å¯¼å‡ºæ€§è´¨ã€‚ä½†æœ‰æ—¶è¿™å¯èƒ½ä¼šå¤ªå¤æ‚ï¼Œå³ä½¿æˆ‘ä»¬æ“…é•¿æ•°å­¦ã€‚å¯è§†åŒ–å¯èƒ½æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ›¿ä»£ï¼ˆæˆ–è¡¥å……ï¼‰ï¼Œå¸®åŠ©æˆ‘ä»¬ç†è§£æ¦‚ç‡åˆ†å¸ƒã€‚æˆ‘æ— æ³•åœ¨çº¸é¢ä¸Šå®Œå…¨å±•ç¤ºï¼Œä½†å¦‚æœä½ è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œä½ å°†è·å¾—ä¸€ä¸ªäº¤äº’å¼å›¾ï¼Œæ¯æ¬¡è°ƒæ•´`alpha`ã€`beta`å’Œ`n`çš„æ»‘å—æ—¶éƒ½ä¼šæ›´æ–°ï¼š
- en: '**CodeÂ 1.2**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  1.2**'
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Figure [1.3](#x1-23013r3)* shows a static version of this interactive plot.
    The black dots represent the probabilities for each value of the random variable,
    while the dotted black line is just a visual aid.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ [1.3](#x1-23013r3)*å±•ç¤ºäº†è¯¥äº¤äº’å¼å›¾çš„é™æ€ç‰ˆæœ¬ã€‚é»‘è‰²çš„ç‚¹ä»£è¡¨æ¯ä¸ªéšæœºå˜é‡å€¼çš„æ¦‚ç‡ï¼Œè€Œè™šçº¿é»‘è‰²çº¿æ¡ä»…ä½œä¸ºè§†è§‰è¾…åŠ©çº¿ã€‚'
- en: '![PIC](img/file16.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file16.png)'
- en: '**FigureÂ 1.3**: The output of `pz.BetaBinomial(alpha=10,Â beta=10,Â n=6).plot_interactive()`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 1.3**ï¼š`pz.BetaBinomial(alpha=10, beta=10, n=6).plot_interactive()`çš„è¾“å‡º'
- en: On the x-axis, we have the support of the BetaBinomial distribution, i.e.,Â the
    values it can take, *x* âˆˆ{0*,*1*,*2*,*3*,*4*,*5}. On the y-axis, the probabilities
    associated with each of those values. The full list is shown in *Table [1.1](#x1-23014r1)*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨xè½´ä¸Šï¼Œæˆ‘ä»¬æœ‰BetaBinomialåˆ†å¸ƒçš„æ”¯æŒï¼Œå³å®ƒå¯ä»¥å–çš„å€¼ï¼Œ*x* âˆˆ{0*,*1*,*2*,*3*,*4*,*5}ã€‚åœ¨yè½´ä¸Šï¼Œä¸è¿™äº›å€¼ç›¸å…³çš„æ¦‚ç‡ã€‚å®Œæ•´åˆ—è¡¨è¯·å‚è§*è¡¨æ ¼
    [1.1](#x1-23014r1)*ã€‚
- en: '| **x value** | **probability** |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| **x å€¼** | **æ¦‚ç‡** |'
- en: '| 0 | 0.047 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.047 |'
- en: '| 1 | 0.168 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.168 |'
- en: '| 2 | 0.285 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.285 |'
- en: '| 3 | 0.285 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.285 |'
- en: '| 4 | 0.168 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.168 |'
- en: '| 5 | 0.047 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.047 |'
- en: '**TableÂ 1.1**: Probabilities for `pz.BetaBinomial(alpha=10, beta=10, n=6)`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¡¨æ ¼ 1.1**ï¼š`pz.BetaBinomial(alpha=10, beta=10, n=6)`çš„æ¦‚ç‡'
- en: Notice that for a `BetaBinomial(alpha=10,Â beta=10,Â n=6)` distribution, the probability
    of values not in {0*,*1*,*2*,*3*,*4*,*5}, including values such as âˆ’1*,*0*.*5*,Ï€,*42,
    is 0.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¯¹äº`BetaBinomial(alpha=10, beta=10, n=6)`åˆ†å¸ƒï¼Œ{0*,*1*,*2*,*3*,*4*,*5}ä¹‹å¤–çš„å€¼ï¼ˆä¾‹å¦‚
    âˆ’1*,*0*.*5*,Ï€,*42ï¼‰çš„æ¦‚ç‡ä¸º0ã€‚
- en: 'We previously mentioned that we can *ask* a random variable for values and
    every time we ask, we will get a different number. We can simulate this with PreliZ
    [[Icazatti etÂ al.](Bibliography.xhtml#Xicazatti2023),Â [2023](Bibliography.xhtml#Xicazatti2023)],
    a Python library for prior elicitation. Take the following code snippet for instance:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹‹å‰æåˆ°è¿‡ï¼Œæˆ‘ä»¬å¯ä»¥*è¯¢é—®*éšæœºå˜é‡è·å–å€¼ï¼Œæ¯æ¬¡è¯¢é—®æ—¶ï¼Œæˆ‘ä»¬éƒ½ä¼šå¾—åˆ°ä¸åŒçš„æ•°å­—ã€‚æˆ‘ä»¬å¯ä»¥ç”¨PreliZ [[Icazatti etÂ al.](Bibliography.xhtml#Xicazatti2023),Â [2023](Bibliography.xhtml#Xicazatti2023)]æ¥æ¨¡æ‹Ÿè¿™ä¸€è¿‡ç¨‹ï¼ŒPreliZæ˜¯ä¸€ä¸ªç”¨äºå…ˆéªŒå¼•å¯¼çš„Pythonåº“ã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘ä»¥ä¸‹ä»£ç ç‰‡æ®µï¼š
- en: '**CodeÂ 1.3**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  1.3**'
- en: '[PRE2]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will give us an integer between 0 and 5\. Which one? We donâ€™t know! But
    letâ€™s run the following code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†ç»™æˆ‘ä»¬ä¸€ä¸ª0åˆ°5ä¹‹é—´çš„æ•´æ•°ã€‚æ˜¯å“ªä¸€ä¸ªï¼Ÿæˆ‘ä»¬ä¸çŸ¥é“ï¼ä½†è®©æˆ‘ä»¬è¿è¡Œä»¥ä¸‹ä»£ç ï¼š
- en: '**CodeÂ 1.4**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  1.4**'
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will get something similar to *Figure [1.4](#x1-23026r4)*. Even when we cannot
    predict the next value from a random variable, we can predict the probability
    of getting any particular value and by the same token, if we get many values,
    we can predict their overall distribution.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å¾—åˆ°ç±»ä¼¼äº*å›¾ [1.4](#x1-23026r4)*çš„ç»“æœã€‚å³ä½¿æˆ‘ä»¬æ— æ³•ä»éšæœºå˜é‡ä¸­é¢„æµ‹ä¸‹ä¸€ä¸ªå€¼ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é¢„æµ‹è·å¾—ä»»ä½•ç‰¹å®šå€¼çš„æ¦‚ç‡ï¼Œåè¿‡æ¥ï¼Œå¦‚æœæˆ‘ä»¬è·å–äº†è®¸å¤šå€¼ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é¢„æµ‹å®ƒä»¬çš„æ•´ä½“åˆ†å¸ƒã€‚
- en: '![PIC](img/file17.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file17.png)'
- en: '**FigureÂ 1.4**: The gray dots represent the pmf of the BetaBinomial sample.
    In light gray, a histogram of 1,000 draws from that distribution'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 1.4**ï¼šç°è‰²çš„ç‚¹è¡¨ç¤ºBetaBinomialæ ·æœ¬çš„æ¦‚ç‡è´¨é‡å‡½æ•°ï¼ˆpmfï¼‰ã€‚æµ…ç°è‰²ä¸ºä»è¯¥åˆ†å¸ƒä¸­æŠ½å–çš„1,000æ¬¡æ ·æœ¬çš„ç›´æ–¹å›¾ã€‚'
- en: 'In this book, we will sometimes know the parameters of a given distribution
    and we will want to get random samples from it. Other times, we are going to be
    in the opposite scenario: we will have a set of samples and we will want to estimate
    the parameters of a distribution. Playing back and forth between these two scenarios
    will become second nature as we move forward through the pages.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬æœ‰æ—¶ä¼šçŸ¥é“ç»™å®šåˆ†å¸ƒçš„å‚æ•°ï¼Œå¹¶å¸Œæœ›ä»ä¸­è·å–éšæœºæ ·æœ¬ã€‚å…¶ä»–æ—¶å€™ï¼Œæˆ‘ä»¬å°†é‡åˆ°ç›¸åçš„æƒ…å†µï¼šæˆ‘ä»¬ä¼šæœ‰ä¸€ç»„æ ·æœ¬ï¼Œå¹¶å¸Œæœ›ä¼°è®¡è¯¥åˆ†å¸ƒçš„å‚æ•°ã€‚åœ¨è¿™ä¸¤ç§æƒ…å¢ƒä¹‹é—´çš„æ¥å›åˆ‡æ¢å°†éšç€æˆ‘ä»¬æ·±å…¥ä¹¦ä¸­çš„å†…å®¹è€Œå˜å¾—å¾—å¿ƒåº”æ‰‹ã€‚
- en: 1.4.4 Continuous random variables and their distributions
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.4 è¿ç»­éšæœºå˜é‡åŠå…¶åˆ†å¸ƒ
- en: 'Probably the most widely known continuous probability distribution is the **Normal
    distribution**, also known as the **Gaussian distribution**. Its **probability
    density function** is:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å¯èƒ½æœ€å¹¿ä¸ºäººçŸ¥çš„è¿ç»­æ¦‚ç‡åˆ†å¸ƒæ˜¯**æ­£æ€åˆ†å¸ƒ**ï¼Œä¹Ÿç§°ä¸º**é«˜æ–¯åˆ†å¸ƒ**ã€‚å…¶**æ¦‚ç‡å¯†åº¦å‡½æ•°**ä¸ºï¼š
- en: '![ { } 1 1( x âˆ’ Î¼)2 pdf(x) = -âˆš----exp âˆ’ -- ----- Ïƒ 2Ï€ 2 Ïƒ ](img/file18.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![ { } 1 1( x âˆ’ Î¼)2 pdf(x) = -âˆš----exp âˆ’ -- ----- Ïƒ 2Ï€ 2 Ïƒ ](img/file18.jpg)'
- en: 'Again, we only show this expression to remove the mystery veil. No need to
    pay too much attention to its details, other than to the fact that this distribution
    has two parameters *Î¼*, which controls the location of the peak of the curve,
    and *Ïƒ*, which controls the spread of the curve. *Figure [1.5](#x1-24006r5)* shows
    3 examples from the Gaussian family. If you want to learn more about this distribution,
    I recommend you watch this video: [https://www.youtube.com/watch?v=cy8r7WSuT1I](https://www.youtube.com/watch?v=cy8r7WSuT1I).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡å¼ºè°ƒï¼Œæˆ‘ä»¬ä»…å±•ç¤ºè¿™ä¸ªè¡¨è¾¾å¼æ˜¯ä¸ºäº†æ­å¼€å…¶ä¸­çš„ç¥ç§˜é¢çº±ã€‚æ— éœ€è¿‡å¤šå…³æ³¨å…¶ç»†èŠ‚ï¼Œé™¤äº†è¯¥åˆ†å¸ƒæœ‰ä¸¤ä¸ªå‚æ•° *Î¼*ï¼Œå®ƒæ§åˆ¶æ›²çº¿çš„å³°å€¼ä½ç½®ï¼Œå’Œ *Ïƒ*ï¼Œå®ƒæ§åˆ¶æ›²çº¿çš„æ‰©å±•åº¦ä»¥å¤–ã€‚*Figure
    [1.5](#x1-24006r5)* å±•ç¤ºäº†æ¥è‡ªé«˜æ–¯å®¶æ—çš„ 3 ä¸ªç¤ºä¾‹ã€‚å¦‚æœä½ æƒ³æ·±å…¥äº†è§£è¿™ä¸ªåˆ†å¸ƒï¼Œæˆ‘å»ºè®®ä½ è§‚çœ‹è¿™ä¸ªè§†é¢‘ï¼š[https://www.youtube.com/watch?v=cy8r7WSuT1I](https://www.youtube.com/watch?v=cy8r7WSuT1I)ã€‚
- en: '![PIC](img/file19.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file19.png)'
- en: '**FigureÂ 1.5**: Three members of the Gaussian family'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**FigureÂ 1.5**ï¼šé«˜æ–¯å®¶æ—çš„ä¸‰ä¸ªæˆå‘˜'
- en: If you have been paying attention, you may have noticed that we said **probability
    density function** (**pdf**) instead of **probability mass** **function** (**pmf**).
    This was no typo â€“ they are actually two different objects. Letâ€™s take one step
    back and think about this; the output of a discrete probability distribution is
    a probability. The height of the bars in *Figure [1.2](#x1-23006r2)* or the height
    of the dots in *Figure [1.3](#x1-23013r3)* are probabilities. Each bar or dot
    will never be higher than 1 and if you sum all the bars or dots, you will always
    get 1\. Letâ€™s do the same but with the curve in *Figure [1.5](#x1-24006r5)*. The
    first thing to notice is that we donâ€™t have bars or dots; we have a continuous,
    smooth curve. So maybe we can think that the curve is made up of super thin bars,
    so thin that we assign one bar for every real value in the support of the distributions,
    we measure the height of each bar, and we perform an infinite sum. This is a sensible
    thing to do, right?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä¸€ç›´åœ¨å…³æ³¨ï¼Œä½ å¯èƒ½ä¼šæ³¨æ„åˆ°æˆ‘ä»¬ä½¿ç”¨äº†**æ¦‚ç‡å¯†åº¦å‡½æ•°**ï¼ˆ**pdf**ï¼‰è€Œä¸æ˜¯**æ¦‚ç‡è´¨é‡å‡½æ•°**ï¼ˆ**pmf**ï¼‰ã€‚è¿™ä¸æ˜¯ç¬”è¯¯â€”â€”å®ƒä»¬å®é™…ä¸Šæ˜¯ä¸¤ä¸ªä¸åŒçš„æ¦‚å¿µã€‚æˆ‘ä»¬å…ˆé€€åä¸€æ­¥ï¼Œæ€è€ƒä¸€ä¸‹ï¼šç¦»æ•£æ¦‚ç‡åˆ†å¸ƒçš„è¾“å‡ºæ˜¯æ¦‚ç‡ã€‚*Figure
    [1.2](#x1-23006r2)* ä¸­çš„æ¡å½¢é«˜åº¦æˆ– *Figure [1.3](#x1-23013r3)* ä¸­çš„ç‚¹çš„é«˜åº¦å°±æ˜¯æ¦‚ç‡ã€‚æ¯ä¸ªæ¡å½¢æˆ–ç‚¹çš„é«˜åº¦æ°¸è¿œä¸ä¼šè¶…è¿‡
    1ï¼Œå¹¶ä¸”å¦‚æœä½ å°†æ‰€æœ‰çš„æ¡å½¢æˆ–ç‚¹ç›¸åŠ ï¼Œä½ æ€»ä¼šå¾—åˆ° 1ã€‚è®©æˆ‘ä»¬åšåŒæ ·çš„äº‹æƒ…ï¼Œä½†æ¢æˆ *Figure [1.5](#x1-24006r5)* ä¸­çš„æ›²çº¿ã€‚é¦–å…ˆéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æ²¡æœ‰æ¡å½¢æˆ–ç‚¹ï¼›æˆ‘ä»¬æœ‰ä¸€ä¸ªè¿ç»­ã€å¹³æ»‘çš„æ›²çº¿ã€‚æ‰€ä»¥ï¼Œä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥è®¤ä¸ºè¿™ä¸ªæ›²çº¿æ˜¯ç”±éå¸¸ç»†çš„æ¡å½¢ç»„æˆçš„ï¼Œè¿™äº›æ¡å½¢ç»†å¾—æˆ‘ä»¬ä¸ºåˆ†å¸ƒæ”¯æŒä¸­çš„æ¯ä¸€ä¸ªå®æ•°å€¼åˆ†é…ä¸€æ¡æ¡å½¢ï¼Œæµ‹é‡æ¯æ¡æ¡å½¢çš„é«˜åº¦ï¼Œç„¶åè¿›è¡Œæ— é™æ±‚å’Œã€‚è¿™æ˜¯åˆç†çš„å—ï¼Ÿ
- en: Well yes, but it is not immediately obvious what are we going to get from this.
    Will this sum give us exactly 1? Or are we going to get a large number instead?
    Is the sum finite? Or does the result depend on the parameters of the distribution?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œä½†æˆ‘ä»¬ä»ä¸­è·å¾—çš„ç»“æœå¹¶ä¸ç«‹åˆ»æ˜¾ç°å‡ºæ¥ã€‚è¿™ä¸ªæ±‚å’Œä¼šå¾—åˆ°ç²¾ç¡®çš„ 1 å—ï¼Ÿè¿˜æ˜¯ä¼šå¾—åˆ°ä¸€ä¸ªè¾ƒå¤§çš„æ•°å­—å‘¢ï¼Ÿè¿™ä¸ªæ±‚å’Œæ˜¯æœ‰é™çš„å—ï¼Ÿç»“æœæ˜¯å¦ä¾èµ–äºåˆ†å¸ƒçš„å‚æ•°ï¼Ÿ
- en: 'A proper answer to these questions requires measure theory, and this is a very
    informal introduction to probability, so we are not going into that rabbit hole.
    But the answer essentially is that for a continuous random variable, we can only
    assign a probability of 0 to every individual value it may take; instead, we can
    assign densities to them and then we can calculate probabilities for a range of
    values. Thus, for a Gaussian, the probability of getting exactly the number -2,
    i.e. the number -2 followed by an infinite number of zeros after the decimal point,
    is 0\. But the probability of getting a number between -2 and 0 is some number
    larger than 0 and smaller than 1\. To find out the exact answer, we need to compute
    the following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ­£ç¡®å›ç­”è¿™äº›é—®é¢˜éœ€è¦æµ‹åº¦ç†è®ºï¼Œè€Œè¿™åªæ˜¯ä¸€ä¸ªéå¸¸éæ­£å¼çš„æ¦‚ç‡å­¦å…¥é—¨ï¼Œå› æ­¤æˆ‘ä»¬ä¸ä¼šæ·±å…¥æ¢è®¨è¿™ä¸ªé—®é¢˜ã€‚ä½†æœ¬è´¨ä¸Šï¼Œç­”æ¡ˆæ˜¯ï¼Œå¯¹äºè¿ç»­éšæœºå˜é‡ï¼Œæˆ‘ä»¬åªèƒ½ä¸ºå®ƒå¯èƒ½å–çš„æ¯ä¸ªå•ç‹¬å€¼åˆ†é…
    0 çš„æ¦‚ç‡ï¼›ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºå®ƒä»¬åˆ†é…å¯†åº¦å€¼ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥è®¡ç®—ä¸€ä¸ªå€¼èŒƒå›´å†…çš„æ¦‚ç‡ã€‚å› æ­¤ï¼Œå¯¹äºé«˜æ–¯åˆ†å¸ƒï¼Œå¾—åˆ°ç²¾ç¡®æ•°å€¼ -2 çš„æ¦‚ç‡ï¼Œå³ -2 åé¢è·Ÿç€æ— é™å¤šä¸ªé›¶çš„å°æ•°éƒ¨åˆ†ï¼Œæ¦‚ç‡ä¸º
    0ã€‚ä½†æ˜¯å¾—åˆ°ä»‹äº -2 å’Œ 0 ä¹‹é—´çš„æ•°å­—çš„æ¦‚ç‡æ˜¯ä¸€ä¸ªå¤§äº 0 ä¸”å°äº 1 çš„æ•°ã€‚ä¸ºäº†æ‰¾å‡ºå‡†ç¡®ç­”æ¡ˆï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—ä»¥ä¸‹å†…å®¹ï¼š
- en: '![ âˆ« b P(a < X < b) = pdf(x)dx a ](img/file20.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![ âˆ« b P(a < X < b) = pdf(x)dx a ](img/file20.jpg)'
- en: And to compute that, we need to replace the symbols for a concrete quantity.
    If we replace the pdf by Normal(0*,*1), and *a* = âˆ’2, *b* = 0, we will get that
    *P*(âˆ’2 *< X <* 0) â‰ˆ 0*.*477, which is the shaded area in *Figure [1.6](#x1-24010r6)*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®¡ç®—è¿™ä¸ªï¼Œæˆ‘ä»¬éœ€è¦ç”¨å…·ä½“çš„æ•°é‡æ¥æ›¿ä»£ç¬¦å·ã€‚å¦‚æœæˆ‘ä»¬å°† pdf æ›¿æ¢ä¸º Normal(0*,*1)ï¼Œå¹¶ä¸” *a* = âˆ’2ï¼Œ*b* = 0ï¼Œæˆ‘ä»¬å°†å¾—åˆ°
    *P*ï¼ˆâˆ’2 *< X <* 0ï¼‰â‰ˆ 0*.*477ï¼Œè¿™å°±æ˜¯ *Figure [1.6](#x1-24010r6)* ä¸­çš„é˜´å½±åŒºåŸŸã€‚
- en: '![PIC](img/file21.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file21.png)'
- en: '**FigureÂ 1.6**: The black line represents the pdf of a Gaussian with parameters
    mu=0 and sigma=1, the gray area is the probability of a value being larger than
    -2 and smaller than 0'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 1.6**ï¼šé»‘çº¿è¡¨ç¤ºå‚æ•°ä¸º mu=0 å’Œ sigma=1 çš„é«˜æ–¯åˆ†å¸ƒçš„ pdfï¼Œç°è‰²åŒºåŸŸè¡¨ç¤ºä¸€ä¸ªå€¼å¤§äº -2 ä¸”å°äº 0 çš„æ¦‚ç‡'
- en: 'You may remember that we can approximate an integral by summing areas of rectangles
    and the approximation becomes more and more accurate as we reduce the length of
    the base of the rectangles (see the Wikipedia entry for [Riemann integral](Bibliography.xhtml#Xwikipedia_riemann_2023)).
    Based on this idea and using PreliZ, we can estimate *P*(âˆ’2 *< X <* 0) as:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½è¿˜è®°å¾—ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ±‚çŸ©å½¢é¢ç§¯çš„å’Œæ¥è¿‘ä¼¼ä¸€ä¸ªç§¯åˆ†ï¼Œéšç€çŸ©å½¢åº•è¾¹é•¿åº¦çš„ç¼©å°ï¼Œè¿™ç§è¿‘ä¼¼ä¼šå˜å¾—è¶Šæ¥è¶Šå‡†ç¡®ï¼ˆè¯·å‚é˜…ç»´åŸºç™¾ç§‘ä¸Šçš„ [Riemann ç§¯åˆ†](Bibliography.xhtml#Xwikipedia_riemann_2023)
    æ¡ç›®ï¼‰ã€‚åŸºäºè¿™ä¸ªæ€è·¯ï¼Œå¹¶ä½¿ç”¨ PreliZï¼Œæˆ‘ä»¬å¯ä»¥ä¼°ç®— *P*(âˆ’2 *< X <* 0) ä¸ºï¼š
- en: '**CodeÂ 1.5**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  1.5**'
- en: '[PRE4]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If we increase the value of `num`, we will get a better approximation.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å¢åŠ  `num` çš„å€¼ï¼Œç»“æœä¼šå¾—åˆ°æ›´ç²¾ç¡®çš„è¿‘ä¼¼å€¼ã€‚
- en: 1.4.5 Cumulative distribution function
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.5 ç´¯ç§¯åˆ†å¸ƒå‡½æ•°
- en: 'We have seen the pmf and the pdf, but these are not the only ways to characterize
    distributions. An alternative is the **cumulative distribution** **function**
    (**cdf**). The cdf of a random variable *X* is the function *F*[*X*] given by
    *F*[*X*](*x*) = *P*(*X* â‰¤ *x*). In words, the cdf is the answer to the question:
    what is the probability of getting a number lower than or equal to *x*? On the
    first column of *Figure [1.7](#x1-25003r7)*, we can see the pmf and cdf of a BetaBinomial,
    and in the second column, the pdf and cdf of a Gaussian. Notice how the cdf *jumps*
    for the discrete variable but it is smooth for the continuous variable. The height
    of each jump represents a probability â€“ just compare them with the height of the
    dots. We can use the plot of the cdf of a continuous variable as visual proof
    that probabilities are zero for any value of the continuous variable. Just notice
    how there are no *jumps* for continuous variables, which is equivalent to saying
    that the height of the jumps is exactly zero.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»çœ‹è¿‡äº†æ¦‚ç‡è´¨é‡å‡½æ•°ï¼ˆpmfï¼‰å’Œæ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆpdfï¼‰ï¼Œä½†è¿™äº›å¹¶ä¸æ˜¯æè¿°åˆ†å¸ƒçš„å”¯ä¸€æ–¹å¼ã€‚å¦ä¸€ç§é€‰æ‹©æ˜¯**ç´¯ç§¯åˆ†å¸ƒå‡½æ•°**ï¼ˆ**cdf**ï¼‰ã€‚éšæœºå˜é‡
    *X* çš„ cdf æ˜¯å‡½æ•° *F*[*X*]ï¼Œç”± *F*[*X*](*x*) = *P*(*X* â‰¤ *x*) ç»™å‡ºã€‚æ¢å¥è¯è¯´ï¼Œcdf æ˜¯å¯¹è¿™ä¸ªé—®é¢˜çš„å›ç­”ï¼šå¾—åˆ°å°äºæˆ–ç­‰äº
    *x* çš„æ•°å€¼çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿåœ¨*å›¾ [1.7](#x1-25003r7)* çš„ç¬¬ä¸€åˆ—ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° BetaBinomial åˆ†å¸ƒçš„ pmf å’Œ cdfï¼Œåœ¨ç¬¬äºŒåˆ—ä¸­ï¼Œåˆ™å±•ç¤ºäº†é«˜æ–¯åˆ†å¸ƒçš„
    pdf å’Œ cdfã€‚è¯·æ³¨æ„ï¼Œç¦»æ•£å˜é‡çš„ cdf ä¼šâ€œè·³è·ƒâ€ï¼Œè€Œè¿ç»­å˜é‡çš„ cdf åˆ™æ˜¯å¹³æ»‘çš„ã€‚æ¯æ¬¡è·³è·ƒçš„é«˜åº¦ä»£è¡¨ä¸€ä¸ªæ¦‚ç‡â€”â€”åªéœ€å°†å…¶ä¸ç‚¹çš„é«˜åº¦è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ç»˜åˆ¶è¿ç»­å˜é‡çš„
    cdf æ¥ä½œä¸ºæ¦‚ç‡ä¸ºé›¶çš„è§†è§‰è¯æ˜ï¼Œä»»ä½•è¿ç»­å˜é‡çš„æ•°å€¼éƒ½æ²¡æœ‰â€œè·³è·ƒâ€ï¼Œè¿™ç­‰åŒäºè¯´è¿™äº›è·³è·ƒçš„é«˜åº¦æ­£å¥½ä¸ºé›¶ã€‚
- en: '![PIC](img/file22.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file22.png)'
- en: '**FigureÂ 1.7**: The pmf of the BetaBinomial distribution with its corresponding
    cdf and the pdf of the Normal distribution with its corresponding cdf'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 1.7**ï¼šBetaBinomial åˆ†å¸ƒçš„ pmf åŠå…¶å¯¹åº”çš„ cdf å’Œæ­£æ€åˆ†å¸ƒçš„ pdf åŠå…¶å¯¹åº”çš„ cdf'
- en: Just by looking at a cdf, it is easier to find what is the probability of getting
    a number smaller than, letâ€™s say, 1\. We just need to go to the value 1 on the
    x-axis, move up until we cross the black line, and then check the value of the
    y-axis. For instance, in *Figure [1.7](#x1-25003r7)* and for the Normal distribution,
    we can see that the value lies between 0.75 and 1\. Letâ€™s say it is â‰ˆ 0*.*85\.
    This is way harder to do with the pdf because we would need to compare the entire
    area below 1 to the total area to get the answer. Humans are worse at judging
    areas than judging heights or lengths.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…é€šè¿‡æŸ¥çœ‹ cdfï¼Œå°±æ›´å®¹æ˜“æ‰¾åˆ°æŸä¸ªæ•°å­—å°äºï¼Œæ¯”å¦‚ 1 çš„æ¦‚ç‡ã€‚æˆ‘ä»¬åªéœ€è¦åœ¨ x è½´ä¸Šæ‰¾åˆ° 1 çš„å€¼ï¼Œå‘ä¸Šç§»åŠ¨ç›´åˆ°ç©¿è¿‡é»‘çº¿ï¼Œç„¶åæŸ¥çœ‹ y è½´çš„å€¼ã€‚ä¾‹å¦‚ï¼Œåœ¨*å›¾
    [1.7](#x1-25003r7)* ä¸­ï¼Œå¯¹äºæ­£æ€åˆ†å¸ƒï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¯¥å€¼ä½äº 0.75 å’Œ 1 ä¹‹é—´ã€‚å‡è®¾å®ƒå¤§çº¦æ˜¯ â‰ˆ 0*.*85ã€‚è¿™æ¯”ä½¿ç”¨ pdf è¦éš¾å¾—å¤šï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦å°†
    1 ä»¥ä¸‹çš„æ•´ä¸ªåŒºåŸŸä¸æ€»åŒºåŸŸè¿›è¡Œæ¯”è¾ƒæ‰èƒ½å¾—åˆ°ç­”æ¡ˆã€‚äººç±»åœ¨åˆ¤æ–­é¢ç§¯æ–¹é¢ä¸å¦‚åœ¨åˆ¤æ–­é«˜åº¦æˆ–é•¿åº¦æ—¶å‡†ç¡®ã€‚
- en: 1.4.6 Conditional probability
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.6 æ¡ä»¶æ¦‚ç‡
- en: 'Given two events *A* and *B* with *P*(*B*) *>* 0, the probability of *A* given
    *B*, which we write as *P*(*A*|*B*) is defined as:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸¤ä¸ªäº‹ä»¶ *A* å’Œ *B*ï¼Œä¸” *P*(*B*) *>* 0ï¼Œæ¡ä»¶æ¦‚ç‡ *P*(*A*|*B*) å®šä¹‰ä¸ºï¼š
- en: '![P(A | B ) = P-(A,-B-) P (B) ](img/file23.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![P(A | B ) = P-(A,-B-) P (B) ](img/file23.jpg)'
- en: '*P*(*A,B*) is the probability that both the event *A* and event *B* occur.
    *P*(*A*|*B*) is known as conditional probability, and it is the probability that
    event *A* occurs, **conditioned** by the fact that we know (or assume, imagine,
    hypothesize, etc.) that *B* has occurred. For example, the probability that the
    pavement is wet is different from the probability that the pavement is wet if
    we know itâ€™s raining.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*A,B*) æ˜¯äº‹ä»¶ *A* å’Œäº‹ä»¶ *B* åŒæ—¶å‘ç”Ÿçš„æ¦‚ç‡ã€‚*P*(*A*|*B*) è¢«ç§°ä¸ºæ¡ä»¶æ¦‚ç‡ï¼Œå®ƒè¡¨ç¤ºåœ¨å·²çŸ¥ï¼ˆæˆ–å‡è®¾ã€æƒ³è±¡ã€å‡å®šç­‰ï¼‰*B*
    å‘ç”Ÿçš„å‰æä¸‹ï¼Œäº‹ä»¶ *A* å‘ç”Ÿçš„æ¦‚ç‡ã€‚ä¾‹å¦‚ï¼Œè·¯é¢æ¹¿äº†çš„æ¦‚ç‡ä¸å·²çŸ¥ä¸‹é›¨æ—¶è·¯é¢æ¹¿äº†çš„æ¦‚ç‡æ˜¯ä¸åŒçš„ã€‚'
- en: A conditional probability can be larger than, smaller than, or equal to the
    unconditional probability. If knowing *B* does not provide us with information
    about *A*, then *P*(*A*|*B*) = *P*(*A*). This will be true only if *A* and *B*
    are independent of each other. On the contrary, if knowing *B* gives us useful
    information about *A*, then the conditional probability could be larger or smaller
    than the unconditional probability, depending on whether knowing *B* makes *A*
    less or more likely. Letâ€™s see a simple example using a fair six-sided die. What
    is the probability of getting the number 3 if we roll the die? *P*(die = 3) =
    ![16](img/file25.jpg) since each of the six numbers has the same chance for a
    fair six-sided die. And what is the probability of getting the number 3 given
    that we have obtained an odd number? *P*(die = 3 | die = {1,3,5}) = ![1 3](img/file27.jpg),
    because if we know we have an odd number, the only possible numbers are {1*,*3*,*5}
    and each of them has the same chance. Finally, what is the probability of getting
    3 if we have obtained an even number? This is *P*(die = 3 | die = {2,4,6}) = 0,
    because if we know the number is even, then the only possible ones are {2*,*4*,*6}
    and thus getting a 3 is not possible.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æ¡ä»¶æ¦‚ç‡å¯ä»¥å¤§äºã€å°äºæˆ–ç­‰äºæ— æ¡ä»¶æ¦‚ç‡ã€‚å¦‚æœçŸ¥é“ *B* å¯¹æˆ‘ä»¬ç†è§£ *A* æ²¡æœ‰æä¾›ä»»ä½•ä¿¡æ¯ï¼Œé‚£ä¹ˆ *P*(*A*|*B*) = *P*(*A*)ã€‚åªæœ‰å½“
    *A* å’Œ *B* äº’ç›¸ç‹¬ç«‹æ—¶ï¼Œè¿™ä¸ªå…³ç³»æ‰æˆç«‹ã€‚ç›¸åï¼Œå¦‚æœçŸ¥é“ *B* ç»™æˆ‘ä»¬æä¾›äº†å…³äº *A* çš„æœ‰ç”¨ä¿¡æ¯ï¼Œé‚£ä¹ˆæ¡ä»¶æ¦‚ç‡å¯èƒ½å¤§äºæˆ–å°äºæ— æ¡ä»¶æ¦‚ç‡ï¼Œå…·ä½“å–å†³äºçŸ¥é“
    *B* æ˜¯å¦è®© *A* æ›´åŠ å¯èƒ½æˆ–æ›´ä¸å¯èƒ½ã€‚è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥çœ‹çœ‹ï¼Œå‡è®¾æˆ‘ä»¬æ·ä¸€ä¸ªå…¬å¹³çš„å…­é¢éª°å­ã€‚æ·åˆ°æ•°å­— 3 çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿ*P*(éª°å­ = 3)
    = ![16](img/file25.jpg)ï¼Œå› ä¸ºå¯¹äºä¸€ä¸ªå…¬å¹³çš„å…­é¢éª°å­ï¼Œæ¯ä¸ªæ•°å­—çš„æœºä¼šæ˜¯ä¸€æ ·çš„ã€‚é‚£ä¹ˆï¼Œå‡å¦‚æˆ‘ä»¬å·²ç»çŸ¥é“æ·åˆ°çš„æ˜¯å¥‡æ•°ï¼Œæ·åˆ°æ•°å­— 3 çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿ*P*(éª°å­
    = 3 | éª°å­ = {1,3,5}) = ![1 3](img/file27.jpg)ï¼Œå› ä¸ºå¦‚æœæˆ‘ä»¬çŸ¥é“ç»“æœæ˜¯å¥‡æ•°ï¼Œé‚£ä¹ˆå¯èƒ½çš„æ•°å­—åªæœ‰ {1*,*3*,*5}ï¼Œè€Œä¸”å®ƒä»¬çš„æœºä¼šæ˜¯ç›¸ç­‰çš„ã€‚æœ€åï¼Œå¦‚æœæˆ‘ä»¬å·²ç»çŸ¥é“æ·åˆ°çš„æ˜¯å¶æ•°ï¼Œæ·åˆ°
    3 çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿè¿™æ˜¯ *P*(éª°å­ = 3 | éª°å­ = {2,4,6}) = 0ï¼Œå› ä¸ºå¦‚æœæˆ‘ä»¬çŸ¥é“ç»“æœæ˜¯å¶æ•°ï¼Œé‚£ä¹ˆå”¯ä¸€å¯èƒ½çš„æ•°å­—æ˜¯ {2*,*4*,*6}ï¼Œæ‰€ä»¥æ·åˆ°
    3 çš„æ¦‚ç‡ä¸º 0ã€‚
- en: As we can see from these simple examples, by conditioning on observed data,
    we are changing the sample space. When asking about *P*(die = 3), we need to evaluate
    the sample space *S* = {1*,*2*,*3*,*4*,*5*,*6}, but when we **condition** **on**
    having got an even number, then the new sample space becomes *T* = {2*,*4*,*6}.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬ä»è¿™äº›ç®€å•çš„ä¾‹å­ä¸­å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡å¯¹è§‚æµ‹æ•°æ®è¿›è¡Œæ¡ä»¶åŒ–ï¼Œæˆ‘ä»¬æ­£åœ¨æ”¹å˜æ ·æœ¬ç©ºé—´ã€‚å½“æˆ‘ä»¬è¯¢é—® *P*(éª°å­ = 3) æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è¯„ä¼°æ ·æœ¬ç©ºé—´ *S*
    = {1*,*2*,*3*,*4*,*5*,*6}ï¼Œä½†å½“æˆ‘ä»¬åœ¨å·²çŸ¥æ·åˆ°çš„æ˜¯å¶æ•°çš„æƒ…å†µä¸‹è¿›è¡Œæ¡ä»¶åŒ–æ—¶ï¼Œæ–°çš„æ ·æœ¬ç©ºé—´å˜ä¸º *T* = {2*,*4*,*6}ã€‚
- en: Conditional probabilities are at the heart of statistics, irrespective of whether
    your problem is rolling dice or building self-driving cars.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: æ¡ä»¶æ¦‚ç‡æ˜¯ç»Ÿè®¡å­¦çš„æ ¸å¿ƒï¼Œæ— è®ºä½ é¢å¯¹çš„é—®é¢˜æ˜¯æ·éª°å­è¿˜æ˜¯æ„å»ºè‡ªåŠ¨é©¾é©¶æ±½è½¦ã€‚
- en: The central panel of *Figure [1.8](#x1-26005r8)* represents *p*(*A,B*) using
    a grayscale with darker colors for higher probability densities. We see the joint
    distribution is elongated, indicating that the higher the value of *A*, the higher
    the one of *B*, and vice versa. Knowing the value of *A* tells us something about
    the values of *B* and the other way around. On the top and right *margins* of
    *Figure [1.8](#x1-26005r8)* we have the **marginal distributions** *p*(*A*) and
    *p*(*B*) respectively. To compute the marginal of *A*, we take *p*(*A,B*) and
    we average overall values of *B*, intuitively this is like taking a 2D object,
    the joint distribution, and projecting it into one dimension. The marginal distribution
    of *B* is computed similarly. The dashed lines represent the **conditional probability**
    *p*(*A*|*B*) for 3 different values of *B*. We get them by slicing the joint *p*(*A,B*)
    at a given value of *B*. We can think of this as the distribution of *A* given
    that we have observed a particular value of *B*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ [1.8](#x1-26005r8)* çš„ä¸­å¤®é¢æ¿ä½¿ç”¨ç°åº¦æ˜¾ç¤ºäº†è”åˆåˆ†å¸ƒ*p*(*A,B*)ï¼Œå…¶ä¸­è¾ƒæ·±çš„é¢œè‰²è¡¨ç¤ºè¾ƒé«˜çš„æ¦‚ç‡å¯†åº¦ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è”åˆåˆ†å¸ƒå‘ˆç°æ‹‰é•¿çš„å½¢æ€ï¼Œè¡¨æ˜*A*çš„å€¼è¶Šé«˜ï¼Œ*B*çš„å€¼ä¹Ÿè¶Šé«˜ï¼Œåä¹‹äº¦ç„¶ã€‚çŸ¥é“äº†*A*çš„å€¼ï¼Œå°±èƒ½æ¨æµ‹å‡º*B*çš„å€¼ï¼Œåä¹‹äº¦ç„¶ã€‚åœ¨*å›¾
    [1.8](#x1-26005r8)* çš„é¡¶éƒ¨å’Œå³ä¾§è¾¹ç¼˜åˆ†åˆ«å±•ç¤ºäº†**è¾¹é™…åˆ†å¸ƒ** *p*(*A*)å’Œ*p*(*B*)ã€‚è¦è®¡ç®—*A*çš„è¾¹é™…åˆ†å¸ƒï¼Œæˆ‘ä»¬éœ€è¦å¯¹*p*(*A,B*)è¿›è¡Œå¯¹æ‰€æœ‰*B*å€¼çš„å¹³å‡ï¼Œç›´è§‚åœ°è¯´ï¼Œè¿™å°±åƒæŠŠäºŒç»´å¯¹è±¡ï¼ˆè”åˆåˆ†å¸ƒï¼‰æŠ•å½±åˆ°ä¸€ç»´ã€‚*B*çš„è¾¹é™…åˆ†å¸ƒä¹Ÿä»¥ç±»ä¼¼çš„æ–¹å¼è®¡ç®—ã€‚è™šçº¿è¡¨ç¤º3ä¸ªä¸åŒ*B*å€¼ä¸‹çš„**æ¡ä»¶æ¦‚ç‡**
    *p*(*A*|*B*)ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ç»™å®š*B*å€¼æ—¶åˆ‡å‰²è”åˆåˆ†å¸ƒ*p*(*A,B*)æ¥å¾—åˆ°å®ƒä»¬ã€‚æˆ‘ä»¬å¯ä»¥æŠŠè¿™çœ‹ä½œæ˜¯åœ¨å·²è§‚å¯Ÿåˆ°ç‰¹å®šçš„*B*å€¼æ—¶ï¼Œ*A*çš„åˆ†å¸ƒã€‚'
- en: '![PIC](img/file30.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file30.png)'
- en: '**FigureÂ 1.8**: Representation of the relationship between the joint *p*(*A,B*),
    the marginals *p*(*A*) and *p*(*B*), and the conditional *p*(*A*|*B*) probabilities'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 1.8**ï¼šè”åˆæ¦‚ç‡*p*(*A,B*)ã€è¾¹ç¼˜æ¦‚ç‡*p*(*A*)å’Œ*p*(*B*)ï¼Œä»¥åŠæ¡ä»¶æ¦‚ç‡*p*(*A*|*B*)ä¹‹é—´å…³ç³»çš„è¡¨ç¤º'
- en: 1.4.7 Expected values
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.7 æœŸæœ›å€¼
- en: 'If *X* is a discrete random variable, we can compute its expected value as:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ*X*æ˜¯ä¸€ä¸ªç¦»æ•£çš„éšæœºå˜é‡ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å…¶æœŸæœ›å€¼ï¼Œå…¬å¼å¦‚ä¸‹ï¼š
- en: '![ âˆ‘ ğ”¼ (X ) = xP (X = x) x ](img/file31.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![ âˆ‘ ğ”¼ (X ) = xP (X = x) x ](img/file31.jpg)'
- en: This is just the mean or average value.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯å‡å€¼æˆ–å¹³å‡å€¼ã€‚
- en: You are probably used to computing means or averages of samples or collections
    of numbers, either by hand, on a calculator, or using Python. But notice that
    here we are not talking about the mean of a bunch of numbers; we are talking about
    the mean of a distribution. Once we have defined the parameters of a distribution,
    we can, in principle, compute its expected values. Those are properties of the
    distribution in the same way that the perimeter is a property of a circle that
    gets defined once we set the value of the radius.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½å·²ç»ä¹ æƒ¯äºè®¡ç®—æ ·æœ¬æˆ–ä¸€ç»„æ•°å­—çš„å‡å€¼æˆ–å¹³å‡å€¼ï¼Œæ— è®ºæ˜¯æ‰‹åŠ¨ã€ç”¨è®¡ç®—å™¨ï¼Œè¿˜æ˜¯ä½¿ç”¨Pythonã€‚ä½†è¯·æ³¨æ„ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬è®¨è®ºçš„ä¸æ˜¯ä¸€å †æ•°å­—çš„å‡å€¼ï¼Œè€Œæ˜¯åˆ†å¸ƒçš„å‡å€¼ã€‚ä¸€æ—¦æˆ‘ä»¬å®šä¹‰äº†åˆ†å¸ƒçš„å‚æ•°ï¼ŒåŸåˆ™ä¸Šå¯ä»¥è®¡ç®—å…¶æœŸæœ›å€¼ã€‚å®ƒä»¬æ˜¯åˆ†å¸ƒçš„ç‰¹æ€§ï¼Œå°±åƒåœ†çš„å‘¨é•¿æ˜¯åœ†çš„ä¸€ä¸ªç‰¹æ€§ï¼Œå®šä¹‰åœ†çš„åŠå¾„åå°±å¯ä»¥ç¡®å®šã€‚
- en: Another expected value is the variance, which we can use to describe the spread
    of a distribution. The variance appears *naturally* in many computations in statistics,
    but in practice, it is often more useful to use the standard deviation, which
    is the square root of the variance. The reason is that the standard deviation
    is in the same units as the random variable.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæœŸæœ›å€¼æ˜¯æ–¹å·®ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥æè¿°åˆ†å¸ƒçš„ç¦»æ•£ç¨‹åº¦ã€‚æ–¹å·®åœ¨è®¸å¤šç»Ÿè®¡è®¡ç®—ä¸­*è‡ªç„¶*å‡ºç°ï¼Œä½†åœ¨å®è·µä¸­ï¼Œé€šå¸¸ä½¿ç”¨æ ‡å‡†å·®ï¼Œå®ƒæ˜¯æ–¹å·®çš„å¹³æ–¹æ ¹ã€‚åŸå› æ˜¯æ ‡å‡†å·®çš„å•ä½ä¸éšæœºå˜é‡ç›¸åŒã€‚
- en: The mean and variance are often called the **moments** of a distribution. Other
    moments are skewness, which tells us about the asymmetry of a distribution, and
    the kurtosis, which tells us about the behavior of the tails or the *extreme values*
    [[Westfall](Bibliography.xhtml#Xwestfall2014),Â [2014](Bibliography.xhtml#Xwestfall2014)].
    *Figure [1.9](#x1-27006r9)* shows examples of different distributions and their
    mean *Î¼*, standard deviation *Ïƒ*, skew *Î³*, and kurtosis ![](img/K.png). Notice
    that for some distributions, some moments may not be defined or they may be inf.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å€¼å’Œæ–¹å·®é€šå¸¸è¢«ç§°ä¸ºåˆ†å¸ƒçš„**çŸ©**ã€‚å…¶ä»–çš„çŸ©åŒ…æ‹¬ååº¦ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬åˆ†å¸ƒçš„åæ–œç¨‹åº¦ï¼Œä»¥åŠå³°åº¦ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬åˆ†å¸ƒå°¾éƒ¨æˆ–*æå€¼*çš„è¡Œä¸º[[Westfall](Bibliography.xhtml#Xwestfall2014)ï¼Œ
    [2014](Bibliography.xhtml#Xwestfall2014)]ã€‚*å›¾ [1.9](#x1-27006r9)* å±•ç¤ºäº†ä¸åŒåˆ†å¸ƒåŠå…¶å‡å€¼*Î¼*ã€æ ‡å‡†å·®*Ïƒ*ã€ååº¦*Î³*å’Œå³°åº¦çš„ä¾‹å­
    ![](img/K.png)ã€‚è¯·æ³¨æ„ï¼Œå¯¹äºæŸäº›åˆ†å¸ƒï¼ŒæŸäº›çŸ©å¯èƒ½æ²¡æœ‰å®šä¹‰ï¼Œæˆ–è€…å®ƒä»¬å¯èƒ½æ˜¯æ— ç©·å¤§ã€‚
- en: '![PIC](img/file32.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file32.png)'
- en: '**FigureÂ 1.9**: Four distributions with their first four moments'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 1.9**ï¼šå››ä¸ªåˆ†å¸ƒåŠå…¶å‰å››ä¸ªçŸ©'
- en: Now that we have learned about some of the basic concepts and jargon from probability
    theory, we can move on to the moment everyone was waiting for.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»äº†è§£äº†ä¸€äº›æ¦‚ç‡è®ºçš„åŸºæœ¬æ¦‚å¿µå’Œæœ¯è¯­ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­è¿›å…¥å¤§å®¶æœŸå¾…çš„æ—¶åˆ»äº†ã€‚
- en: 1.4.8 Bayesâ€™ theorem
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.8 è´å¶æ–¯å®šç†
- en: 'Without further ado, letâ€™s contemplate, in all its majesty, Bayesâ€™ theorem:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯«ä¸æ‹–å»¶ï¼Œè®©æˆ‘ä»¬ä»¥å®ƒçš„å¨ä¸¥ï¼Œæ€è€ƒè´å¶æ–¯å®šç†ï¼š
- en: '![ p(Y-| Î¸)p(Î¸) p(Î¸ | Y ) = p(Y) ](img/file33.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![ p(Y-| Î¸)p(Î¸) p(Î¸ | Y ) = p(Y) ](img/file33.jpg)'
- en: 'Well, itâ€™s not that impressive, is it? It looks like an elementary school formula,
    and yet, paraphrasing Richard Feynman, this is all you need to know about Bayesian
    statistics. Learning where Bayesâ€™ theorem comes from will help us understand its
    meaning. According to the product rule, we have:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œè¿™å¹¶ä¸æ˜¯ä»€ä¹ˆä»¤äººå°è±¡æ·±åˆ»çš„ä¸œè¥¿ï¼Œå¯¹å§ï¼Ÿå®ƒçœ‹èµ·æ¥åƒæ˜¯å°å­¦çš„å…¬å¼ï¼Œä½†å¼•ç”¨ç†æŸ¥å¾·Â·è´¹æ›¼çš„è¯ï¼Œè¿™å°±æ˜¯ä½ éœ€è¦äº†è§£çš„è´å¶æ–¯ç»Ÿè®¡çš„å…¨éƒ¨å†…å®¹ã€‚äº†è§£è´å¶æ–¯å®šç†çš„æ¥æºå°†å¸®åŠ©æˆ‘ä»¬ç†è§£å®ƒçš„æ„ä¹‰ã€‚æ ¹æ®ä¹˜ç§¯è§„åˆ™ï¼Œæˆ‘ä»¬æœ‰ï¼š
- en: '![p (Î¸,Y ) = p(Î¸ | Y ) p(Y ) ](img/file34.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![p (Î¸,Y ) = p(Î¸ | Y ) p(Y ) ](img/file34.jpg)'
- en: 'This can also be written as:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¹Ÿå¯ä»¥å†™æˆï¼š
- en: '![p(Î¸,Y) = p(Y | Î¸) p(Î¸) ](img/file35.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![p(Î¸,Y) = p(Y | Î¸) p(Î¸) ](img/file35.jpg)'
- en: 'Given that the terms on the left are equal for both equations, we can combine
    them and write:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå·¦ä¾§çš„é¡¹å¯¹äºä¸¤ä¸ªæ–¹ç¨‹æ˜¯ç›¸ç­‰çš„ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬åˆå¹¶å¹¶å†™å‡ºï¼š
- en: '![p(Î¸ | Y) p(Y) = p(Y | Î¸) p(Î¸) ](img/file36.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![p(Î¸ | Y) p(Y) = p(Y | Î¸) p(Î¸) ](img/file36.jpg)'
- en: 'On reordering, we get Bayesâ€™ theorem:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: é‡æ–°æ’åˆ—åï¼Œæˆ‘ä»¬å¾—åˆ°è´å¶æ–¯å®šç†ï¼š
- en: '![ p(Y | Î¸)p(Î¸) p(Î¸ | Y ) =---p(Y)---- ](img/file37.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![ p(Y | Î¸)p(Î¸) p(Î¸ | Y ) =---p(Y)---- ](img/file37.jpg)'
- en: Why is Bayesâ€™ theorem that important? Letâ€™s see.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè´å¶æ–¯å®šç†å¦‚æ­¤é‡è¦ï¼Ÿè®©æˆ‘ä»¬çœ‹çœ‹ã€‚
- en: First, it says that *p*(*Î¸*|*Y* ) is not necessarily the same as *p*(*Y* |*Î¸*).
    This is a very important fact â€“ one that is easy to miss in daily situations,
    even for people trained in statistics and probability. Letâ€™s use a simple example
    to clarify why these quantities are not necessarily the same. The probability
    of a person being the Pope given that this person is Argentinian is not the same
    as the probability of being Argentinian given that this person is the Pope. As
    there are around 47,000,000 Argentinians alive and a single one of them is the
    current Pope, we have *p*(Pope | Argentinian ) â‰ˆ![470100000](img/file39.jpg) and
    we also have *p*(Argentinian | Pope ) = 1.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œå®ƒè¯´*p*(*Î¸*|*Y*)ä¸ä¸€å®šç­‰äº*p*(*Y*|*Î¸*)ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„äº‹å®â€”â€”ä¸€ä¸ªå³ä½¿æ˜¯å—è¿‡ç»Ÿè®¡å­¦å’Œæ¦‚ç‡å­¦è®­ç»ƒçš„äººä¹Ÿå®¹æ˜“åœ¨æ—¥å¸¸æƒ…å†µä¸‹å¿½è§†çš„äº‹å®ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥æ¾„æ¸…ä¸ºä»€ä¹ˆè¿™äº›é‡ä¸ä¸€å®šç›¸åŒã€‚ä¸€ä¸ªäººæ˜¯æ•™çš‡çš„æ¦‚ç‡ï¼Œç»™å®šè¿™ä¸ªäººæ˜¯é˜¿æ ¹å»·äººï¼Œå¹¶ä¸ç­‰åŒäºï¼Œç»™å®šè¿™ä¸ªäººæ˜¯æ•™çš‡ï¼Œæˆä¸ºé˜¿æ ¹å»·äººçš„æ¦‚ç‡ã€‚ç”±äºå¤§çº¦æœ‰4700ä¸‡é˜¿æ ¹å»·äººï¼Œè€Œå…¶ä¸­åªæœ‰ä¸€ä¸ªæ˜¯ç°ä»»æ•™çš‡ï¼Œæˆ‘ä»¬æœ‰*p*(æ•™çš‡
    | é˜¿æ ¹å»·äºº) â‰ˆ![470100000](img/file39.jpg)ï¼ŒåŒæ—¶æˆ‘ä»¬ä¹Ÿæœ‰*p*(é˜¿æ ¹å»·äºº | æ•™çš‡) = 1ã€‚
- en: If we replace *Î¸* with â€œhypothesisâ€ and *Y* with â€œdata,â€ Bayesâ€™ theorem tells
    us how to compute the probability of a hypothesis, *Î¸*, given the data, *Y* ,
    and thatâ€™s the way you will find Bayesâ€™ theorem is explained in a lot of places.
    But, how do we turn a hypothesis into something that we can put inside Bayesâ€™
    theorem? Well, we do it by using probability distributions. So, in general, our
    hypothesis is a hypothesis in a very, very, very narrow sense; we will be more
    precise if we talk about finding a suitable value for parameters in our models,
    that is, parameters of probability distributions. By the way, donâ€™t try to set
    *Î¸* to statements such as â€unicorns are real,â€ unless you are willing to build
    a realistic probabilistic model of unicorn existence!
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°†*Î¸*æ›¿æ¢ä¸ºâ€œå‡è®¾â€ï¼Œå°†*Y*æ›¿æ¢ä¸ºâ€œæ•°æ®â€ï¼Œè´å¶æ–¯å®šç†å‘Šè¯‰æˆ‘ä»¬å¦‚ä½•è®¡ç®—åœ¨ç»™å®šæ•°æ®*Y*çš„æƒ…å†µä¸‹ï¼Œå‡è®¾*Î¸*çš„æ¦‚ç‡ï¼Œè¿™ä¹Ÿæ˜¯ä½ ä¼šåœ¨å¾ˆå¤šåœ°æ–¹çœ‹åˆ°çš„è´å¶æ–¯å®šç†çš„è§£é‡Šã€‚ä½†æ˜¯ï¼Œå¦‚ä½•å°†ä¸€ä¸ªå‡è®¾è½¬åŒ–ä¸ºå¯ä»¥æ”¾å…¥è´å¶æ–¯å®šç†ä¸­çš„ä¸œè¥¿å‘¢ï¼Ÿå—¯ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ¦‚ç‡åˆ†å¸ƒæ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚æ‰€ä»¥ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬çš„å‡è®¾æ˜¯ä¸€ä¸ªéå¸¸éå¸¸éå¸¸ç‹­ä¹‰çš„å‡è®¾ï¼›å¦‚æœæˆ‘ä»¬è°ˆè®ºçš„æ˜¯æ‰¾åˆ°é€‚åˆæˆ‘ä»¬æ¨¡å‹çš„å‚æ•°çš„å€¼ï¼Œå³æ¦‚ç‡åˆ†å¸ƒçš„å‚æ•°ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¼šæ›´ç²¾ç¡®ã€‚é¡ºä¾¿è¯´ä¸€å¥ï¼Œä¸è¦è¯•å›¾å°†*Î¸*è®¾å®šä¸ºâ€œç‹¬è§’å…½å­˜åœ¨â€çš„é™ˆè¿°ï¼Œé™¤éä½ æ„¿æ„æ„å»ºä¸€ä¸ªç°å®çš„ç‹¬è§’å…½å­˜åœ¨çš„æ¦‚ç‡æ¨¡å‹ï¼
- en: 'Bayesâ€™ theorem is central to Bayesian statistics. As we will see in *Chapter
    [2](CH02.xhtml#x1-440002)*, using tools such as PyMC frees us of the need to explicitly
    write Bayesâ€™ theorem every time we build a Bayesian model. Nevertheless, it is
    important to know the name of its parts because we will constantly refer to them
    and it is important to understand what each part means because this will help
    us to conceptualize models. So, let me rewrite Bayesâ€™ theorem now with labels:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯å®šç†æ˜¯è´å¶æ–¯ç»Ÿè®¡çš„æ ¸å¿ƒã€‚æ­£å¦‚æˆ‘ä»¬åœ¨*ç¬¬[2ç« ](CH02.xhtml#x1-440002)*ä¸­çœ‹åˆ°çš„ï¼Œä½¿ç”¨åƒPyMCè¿™æ ·çš„å·¥å…·è§£æ”¾äº†æˆ‘ä»¬æ¯æ¬¡æ„å»ºè´å¶æ–¯æ¨¡å‹æ—¶éƒ½å¿…é¡»æ˜¾å¼ä¹¦å†™è´å¶æ–¯å®šç†çš„éœ€è¦ã€‚ç„¶è€Œï¼Œäº†è§£å…¶å„ä¸ªéƒ¨åˆ†çš„åç§°æ˜¯å¾ˆé‡è¦çš„ï¼Œå› ä¸ºæˆ‘ä»¬å°†ä¸æ–­å¼•ç”¨å®ƒä»¬ï¼Œè€Œä¸”ç†è§£æ¯ä¸ªéƒ¨åˆ†çš„å«ä¹‰ä¹Ÿå¾ˆé‡è¦ï¼Œå› ä¸ºè¿™æœ‰åŠ©äºæˆ‘ä»¬æ„å»ºæ¨¡å‹çš„æ¦‚å¿µã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ç°åœ¨å¸¦ç€æ ‡ç­¾é‡å†™è´å¶æ–¯å®šç†ï¼š
- en: '![ posterior â—œlikeliâ—hâ—Ÿoodâ—pâ—œrâ—ioâ—Ÿrâ— â—œ--â—â—Ÿ-â— p (Y | Î¸)p(Î¸) p(Î¸ | Y) =--------------
    pâ—Ÿ(â—Yâ—œ)â— marginal likelihood ](img/file41.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![ posterior â—œlikeliâ—hâ—Ÿoodâ—pâ—œrâ—ioâ—Ÿrâ— â—œ--â—â—Ÿ-â— p (Y | Î¸)p(Î¸) p(Î¸ | Y) =--------------
    pâ—Ÿ(â—Yâ—œ)â— marginal likelihood ](img/file41.jpg)'
- en: The **prior distribution** should reflect what we know about the value of the
    parameter *Î¸* before seeing the data, *Y* . If we know nothing, like Jon Snow,
    we could use flat priors that do not convey too much information. In general,
    we can do better than flat priors, as we will learn in this book. The use of priors
    is why some people still talk about Bayesian statistics as subjective, even when
    priors are just another assumption that we made when modeling and hence are just
    as subjective (or objective) as any other assumption, such as likelihoods.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…ˆéªŒåˆ†å¸ƒ**åº”å½“åæ˜ æˆ‘ä»¬åœ¨çœ‹åˆ°æ•°æ®ä¹‹å‰å¯¹å‚æ•°*Î¸*å€¼çš„äº†è§£ï¼Œ*Y*ã€‚å¦‚æœæˆ‘ä»¬ä»€ä¹ˆéƒ½ä¸çŸ¥é“ï¼Œå°±åƒä¹”æ©Â·é›ªè¯ºé‚£æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¹³å¦çš„å…ˆéªŒï¼Œè¿™ä¸ä¼šä¼ è¾¾å¤ªå¤šä¿¡æ¯ã€‚é€šå¸¸æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥æ¯”å¹³å¦çš„å…ˆéªŒåšå¾—æ›´å¥½ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨æœ¬ä¹¦ä¸­å°†è¦å­¦åˆ°çš„é‚£æ ·ã€‚å…ˆéªŒçš„ä½¿ç”¨æ˜¯ä¸ºä»€ä¹ˆä¸€äº›äººä»ç„¶è®¤ä¸ºè´å¶æ–¯ç»Ÿè®¡æ˜¯ä¸»è§‚çš„ï¼Œå°½ç®¡å…ˆéªŒåªæ˜¯æˆ‘ä»¬åœ¨å»ºæ¨¡æ—¶æ‰€åšçš„å¦ä¸€ç§å‡è®¾ï¼Œå› æ­¤å®ƒå’Œå…¶ä»–å‡è®¾ï¼ˆå¦‚ä¼¼ç„¶ï¼‰ä¸€æ ·ä¸»è§‚ï¼ˆæˆ–å®¢è§‚ï¼‰ã€‚'
- en: The **likelihood** is how we will introduce data in our analysis. It is an expression
    of the plausibility of the data given the parameters. In some texts, you will
    find people call this term sampling model, statistical model, or just model. We
    will stick to the name likelihood and we will model the combination of priors
    and likelihood.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¼¼ç„¶**æ˜¯æˆ‘ä»¬åœ¨åˆ†æä¸­å¼•å…¥æ•°æ®çš„æ–¹å¼ã€‚å®ƒæ˜¯ç»™å®šå‚æ•°ä¸‹æ•°æ®çš„åˆç†æ€§è¡¨è¾¾ã€‚åœ¨æŸäº›æ–‡æœ¬ä¸­ï¼Œä½ ä¼šå‘ç°æœ‰äººç§°è¿™ä¸ªæœ¯è¯­ä¸ºé‡‡æ ·æ¨¡å‹ã€ç»Ÿè®¡æ¨¡å‹æˆ–ä»…ä»…æ˜¯æ¨¡å‹ã€‚æˆ‘ä»¬å°†åšæŒä½¿ç”¨â€œä¼¼ç„¶â€è¿™ä¸ªåç§°ï¼Œå¹¶å°†å»ºæ¨¡å…ˆéªŒå’Œä¼¼ç„¶çš„ç»„åˆã€‚'
- en: 'The **posterior distribution** is the result of the Bayesian analysis and reflects
    all that we know about a problem (given our data and model). The posterior is
    a probability distribution for the parameters in our model and not a single value.
    This distribution is a balance between the prior and the likelihood. There is
    a well-known joke: a Bayesian is one who, vaguely expecting a horse, and catching
    a glimpse of a donkey, strongly believes they have seen a mule. One excellent
    way to kill the mood after hearing this joke is to explain that if the likelihood
    and priors are both vague, you will get a posterior reflecting vague beliefs about
    seeing a mule rather than strong ones. Anyway, I like the joke, and I like how
    it captures the idea of a posterior being somehow a compromise between prior and
    likelihood. Conceptually, we can think of the posterior as the updated prior in
    light of (new) data. In theory, the posterior from one analysis can be used as
    the prior for a new analysis (in practice, life can be harder). This makes Bayesian
    analysis particularly suitable for analyzing data that becomes available in sequential
    order. One example could be an early warning system for natural disasters that
    processes online data coming from meteorological stations and satellites. For
    more details, read about online machine-learning methods.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**åéªŒåˆ†å¸ƒ**æ˜¯è´å¶æ–¯åˆ†æçš„ç»“æœï¼Œåæ˜ äº†æˆ‘ä»¬å…³äºä¸€ä¸ªé—®é¢˜çš„æ‰€æœ‰å·²çŸ¥ä¿¡æ¯ï¼ˆåŸºäºæˆ‘ä»¬çš„æ•°æ®å’Œæ¨¡å‹ï¼‰ã€‚åéªŒåˆ†å¸ƒæ˜¯æ¨¡å‹å‚æ•°çš„æ¦‚ç‡åˆ†å¸ƒï¼Œè€Œä¸æ˜¯å•ä¸€çš„å€¼ã€‚è¿™ä¸ªåˆ†å¸ƒæ˜¯å…ˆéªŒå’Œä¼¼ç„¶ä¹‹é—´çš„å¹³è¡¡ã€‚æœ‰ä¸€ä¸ªè‘—åçš„ç¬‘è¯ï¼šè´å¶æ–¯å­¦å®¶æ˜¯é‚£ç§æ¨¡ç³Šåœ°æœŸæœ›çœ‹åˆ°ä¸€åŒ¹é©¬ï¼Œå´åªçœ‹åˆ°äº†ä¸€åªé©´ï¼Œå¹¶åšä¿¡è‡ªå·±çœ‹åˆ°äº†éª¡å­çš„äººã€‚å¬åˆ°è¿™ä¸ªç¬‘è¯åï¼Œè§£é‡Šè¯´å¦‚æœä¼¼ç„¶å’Œå…ˆéªŒéƒ½å¾ˆæ¨¡ç³Šï¼Œé‚£ä¹ˆå¾—åˆ°çš„åéªŒåæ˜ çš„å°±æ˜¯å¯¹çœ‹åˆ°éª¡å­çš„æ¨¡ç³Šä¿¡å¿µï¼Œè€Œä¸æ˜¯å¼ºçƒˆçš„ä¿¡å¿µï¼Œè¿™ä¼šå¤§å¤§ç ´åæ°”æ°›ã€‚æ— è®ºå¦‚ä½•ï¼Œæˆ‘å–œæ¬¢è¿™ä¸ªç¬‘è¯ï¼Œä¹Ÿå–œæ¬¢å®ƒä¼ è¾¾äº†åéªŒä½œä¸ºå…ˆéªŒå’Œä¼¼ç„¶ä¹‹é—´æŸç§å¦¥åçš„æƒ³æ³•ã€‚ä»æ¦‚å¿µä¸Šè®²ï¼Œæˆ‘ä»¬å¯ä»¥å°†åéªŒè§†ä¸ºæ ¹æ®ï¼ˆæ–°ï¼‰æ•°æ®æ›´æ–°åçš„å…ˆéªŒã€‚ç†è®ºä¸Šï¼Œä¸€ä¸ªåˆ†æçš„åéªŒå¯ä»¥ä½œä¸ºæ–°åˆ†æçš„å…ˆéªŒï¼ˆä½†å®é™…ä¸Šï¼Œç”Ÿæ´»å¯èƒ½ä¼šæ›´å¤æ‚ï¼‰ã€‚è¿™ä½¿å¾—è´å¶æ–¯åˆ†æç‰¹åˆ«é€‚ç”¨äºåˆ†ææŒ‰é¡ºåºæä¾›çš„æ•°æ®ã€‚ä¸€ä¸ªä¾‹å­å¯èƒ½æ˜¯è‡ªç„¶ç¾å®³çš„æ—©æœŸé¢„è­¦ç³»ç»Ÿï¼Œå®ƒå¤„ç†æ¥è‡ªæ°”è±¡ç«™å’Œå«æ˜Ÿçš„åœ¨çº¿æ•°æ®ã€‚æœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·é˜…è¯»å…³äºåœ¨çº¿æœºå™¨å­¦ä¹ æ–¹æ³•çš„èµ„æ–™ã€‚'
- en: The last term is the **marginal likelihood**, sometimes referred to as the **evidence**.
    Formally, the marginal likelihood is the probability of observing the data averaged
    over all the possible values the parameters can take (as prescribed by the prior).
    We can write this as âˆ« [Î˜]^(*p*(*Y* |*Î¸*)*p*(*Î¸*)d*Î¸*. We will not really care
    about the marginal likelihood until *Chapter [5](CH05.xhtml#x1-950005)*. But for
    the moment, we can think of it as a normalization factor that ensures the posterior
    is a proper pmf or pdf. If we ignore the marginal likelihood, we can write Bayesâ€™
    theorem as a proportionality, which is also a common way to write Bayesâ€™ theorem:)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€é¡¹æ˜¯**è¾¹é™…ä¼¼ç„¶æ€§**ï¼Œæœ‰æ—¶ä¹Ÿç§°ä¸º**è¯æ®**ã€‚ä¸¥æ ¼æ¥è¯´ï¼Œè¾¹é™…ä¼¼ç„¶æ€§æ˜¯è§‚å¯Ÿæ•°æ®çš„æ¦‚ç‡ï¼Œå–å†³äºæ‰€æœ‰å‚æ•°å¯èƒ½å–å€¼çš„å¹³å‡å€¼ï¼ˆæŒ‰ç…§å…ˆéªŒåˆ†å¸ƒè§„å®šï¼‰ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶è¡¨ç¤ºä¸º
    âˆ« [Î˜]^(*p*(*Y* |*Î¸*)*p*(*Î¸*)d*Î¸*ã€‚æˆ‘ä»¬ç›´åˆ°*ç¬¬5ç« *[5](CH05.xhtml#x1-950005)æ‰ä¼šçœŸæ­£å…³å¿ƒè¾¹é™…ä¼¼ç„¶æ€§ã€‚ä½†ç›®å‰ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶è§†ä¸ºä¸€ä¸ªå½’ä¸€åŒ–å› å­ï¼Œç¡®ä¿åéªŒåˆ†å¸ƒæ˜¯ä¸€ä¸ªåˆé€‚çš„pmfæˆ–pdfã€‚å¦‚æœå¿½ç•¥è¾¹é™…ä¼¼ç„¶æ€§ï¼Œæˆ‘ä»¬å¯ä»¥å°†è´å¶æ–¯å®šç†å†™ä¸ºä¸€ä¸ªæ¯”ä¾‹å…³ç³»ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ç§å¸¸è§çš„è´å¶æ–¯å®šç†è¡¨ç¤ºæ–¹å¼ã€‚
- en: '![p(Î¸ | Y ) âˆ p(Y | Î¸)p(Î¸) ](img/file42.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![p(Î¸ | Y ) âˆ p(Y | Î¸)p(Î¸) ](img/file42.jpg)'
- en: Understanding the exact role of each term in Bayesâ€™ theorem will take some time
    and practice, and it will require a few examples, but thatâ€™s what the rest of
    this book is for.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£è´å¶æ–¯å®šç†ä¸­æ¯ä¸ªé¡¹çš„ç¡®åˆ‡ä½œç”¨éœ€è¦ä¸€äº›æ—¶é—´å’Œå®è·µï¼Œå¹¶ä¸”éœ€è¦é€šè¿‡å‡ ä¸ªä¾‹å­æ¥å¸®åŠ©ç†è§£ï¼Œä½†è¿™å°±æ˜¯æœ¬ä¹¦å…¶ä½™éƒ¨åˆ†çš„ç›®çš„æ‰€åœ¨ã€‚
- en: 1.5 Interpreting probabilities
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 æ¦‚ç‡è§£é‡Š
- en: Probabilities can be interpreted in various useful ways. For instance, we can
    think that *P*(*A*) = 0*.*125 means that if we repeat the survey many times, we
    would expect all three individuals to answer â€œyesâ€ about 12.5% of the time. We
    are interpreting probabilities as the outcome of long-run experiments. This is
    a very common and useful interpretation. It not only can help us think about probabilities
    but can also provide an empirical method to estimate probabilities. Do we want
    to know the probability of a car tire exploding if filled with air beyond the
    manufacturerâ€™s recommendation? Just inflate 120 tires or so, and you may get a
    good approximation. This is usually called the frequentist interpretation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: æ¦‚ç‡å¯ä»¥é€šè¿‡å„ç§æœ‰ç”¨çš„æ–¹å¼æ¥è§£é‡Šã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è®¤ä¸º *P*(*A*) = 0*.*125 æ„å‘³ç€å¦‚æœæˆ‘ä»¬å¤šæ¬¡é‡å¤è°ƒæŸ¥ï¼Œæˆ‘ä»¬æœŸæœ›è¿™ä¸‰ä¸ªäººå¤§çº¦12.5%çš„æ—¶é—´ä¼šå›ç­”â€œæ˜¯â€ã€‚æˆ‘ä»¬æ­£åœ¨å°†æ¦‚ç‡è§£é‡Šä¸ºé•¿æœŸå®éªŒçš„ç»“æœã€‚è¿™æ˜¯ä¸€ç§éå¸¸å¸¸è§ä¸”æœ‰ç”¨çš„è§£é‡Šã€‚å®ƒä¸ä»…å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ€è€ƒæ¦‚ç‡ï¼Œè¿˜å¯ä»¥æä¾›ä¸€ç§ç»éªŒæ–¹æ³•æ¥ä¼°è®¡æ¦‚ç‡ã€‚æˆ‘ä»¬æƒ³çŸ¥é“ï¼Œå¦‚æœæ±½è½¦è½®èƒå……æ°”è¶…è¿‡åˆ¶é€ å•†æ¨èçš„æ ‡å‡†ï¼Œçˆ†èƒçš„æ¦‚ç‡æ˜¯å¤šå°‘å—ï¼Ÿåªéœ€å……æ°”å¤§çº¦120ä¸ªè½®èƒï¼Œä½ å°±èƒ½å¾—åˆ°ä¸€ä¸ªä¸é”™çš„è¿‘ä¼¼å€¼ã€‚è¿™é€šå¸¸è¢«ç§°ä¸ºé¢‘ç‡ä¸»ä¹‰è§£é‡Šã€‚
- en: Another interpretation of probability, usually called subjective or Bayesian
    interpretation, states that probabilities can be interpreted as measures of an
    individualâ€™s uncertainty about events. In this interpretation, probabilities are
    about our state of knowledge of the world and are not necessarily based on repeated
    trials. Under this definition of probability, it is valid and natural to ask about
    the probability of life on Mars, the probability of the mass of an electron being
    9*.*1 Ã— 10^(âˆ’31) kg, or the probability that the 9^(th) of July of 1816 was a
    sunny day in Buenos Aires. All these are one-time events. We cannot re-create
    1 million universes, each with one Mars, and check how many of them develop life.
    Of course, we can do this as a mental experiment, so long-term frequencies can
    still be a valid mental scaffold.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: æ¦‚ç‡çš„å¦ä¸€ç§è§£é‡Šï¼Œé€šå¸¸ç§°ä¸ºä¸»è§‚æˆ–è´å¶æ–¯è§£é‡Šï¼Œè®¤ä¸ºæ¦‚ç‡å¯ä»¥è¢«è§£é‡Šä¸ºä¸ªä½“å¯¹äº‹ä»¶ä¸ç¡®å®šæ€§çš„åº¦é‡ã€‚åœ¨è¿™ç§è§£é‡Šä¸‹ï¼Œæ¦‚ç‡ä¸æˆ‘ä»¬å¯¹ä¸–ç•Œçš„çŸ¥è¯†çŠ¶æ€ç›¸å…³ï¼Œå¹¶ä¸ä¸€å®šåŸºäºé‡å¤è¯•éªŒã€‚åœ¨è¿™ç§æ¦‚ç‡å®šä¹‰ä¸‹ï¼Œæå‡ºå…³äºç«æ˜Ÿä¸Šæ˜¯å¦æœ‰ç”Ÿå‘½çš„æ¦‚ç‡ã€ç”µå­è´¨é‡ä¸º9*.*1
    Ã— 10^(âˆ’31) kgçš„æ¦‚ç‡ï¼Œæˆ–è€…1816å¹´7æœˆ9æ—¥å¸ƒå®œè¯ºæ–¯è‰¾åˆ©æ–¯æ˜¯å¦æ˜¯æ™´å¤©çš„æ¦‚ç‡ï¼Œéƒ½æ˜¯æœ‰æ•ˆä¸”è‡ªç„¶çš„ã€‚è¿™äº›éƒ½æ˜¯ä¸€æ¬¡æ€§äº‹ä»¶ã€‚æˆ‘ä»¬ä¸èƒ½é‡æ–°åˆ›é€ 1ç™¾ä¸‡ä¸ªå®‡å®™ï¼Œæ¯ä¸ªå®‡å®™éƒ½æœ‰ä¸€ä¸ªç«æ˜Ÿï¼Œå¹¶æ£€æŸ¥å…¶ä¸­æœ‰å¤šå°‘ä¸ªå‘å±•å‡ºäº†ç”Ÿå‘½ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥åšè¿™ä¸ªå¿ƒç†å®éªŒï¼Œåªè¦é•¿æœŸé¢‘ç‡ä»ç„¶æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å¿ƒç†æ¡†æ¶ã€‚
- en: Sometimes the Bayesian interpretation of probabilities is described in terms
    of personal beliefs; I donâ€™t like that. I think it can lead to unnecessary confusion
    as beliefs are generally associated with the notion of faith or unsupported claims.
    This association can easily lead people to think that Bayesian probabilities,
    and by extension Bayesian statistics, is less objective or less scientific than
    alternatives. I think it also helps to generate confusion about the role of prior
    knowledge in statistics and makes people think that being objective or rational
    means not using prior information.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶è´å¶æ–¯å¯¹æ¦‚ç‡çš„è§£é‡Šè¢«æè¿°ä¸ºä¸ªäººä¿¡å¿µï¼›æˆ‘ä¸å–œæ¬¢è¿™æ ·ã€‚æˆ‘è®¤ä¸ºè¿™å¯èƒ½ä¼šå¯¼è‡´ä¸å¿…è¦çš„æ··æ·†ï¼Œå› ä¸ºä¿¡å¿µé€šå¸¸ä¸ä¿¡ä»°æˆ–æ²¡æœ‰ä¾æ®çš„ä¸»å¼ æœ‰å…³ã€‚è¿™ç§å…³è”å¾ˆå®¹æ˜“è®©äººè®¤ä¸ºè´å¶æ–¯æ¦‚ç‡ï¼Œè¿›è€Œè´å¶æ–¯ç»Ÿè®¡å­¦ï¼Œæ¯”å…¶ä»–æ–¹æ³•ä¸é‚£ä¹ˆå®¢è§‚æˆ–ä¸é‚£ä¹ˆç§‘å­¦ã€‚æˆ‘è¿˜è®¤ä¸ºï¼Œè¿™ç§è¯´æ³•æœ‰åŠ©äºäº§ç”Ÿå¯¹ç»Ÿè®¡å­¦ä¸­å…ˆéªŒçŸ¥è¯†è§’è‰²çš„æ··æ·†ï¼Œè®©äººä»¬è¯¯ä»¥ä¸ºå®¢è§‚æˆ–ç†æ€§å°±æ„å‘³ç€ä¸ä½¿ç”¨å…ˆéªŒä¿¡æ¯ã€‚
- en: 'Bayesian methods are as subjective (or objective) as any other well-established
    scientific method we have. Let me explain myself with an example: life on Mars
    exists or does not exist; the outcome is binary, a yes-no question. But given
    that we are not sure about that fact, a sensible course of action is trying to
    find out how likely life on Mars is. To answer this question any honest and scientific-minded
    person will use all the relevant geophysical data about Mars, all the relevant
    biochemical knowledge about necessary conditions for life, and so on. The response
    will be necessarily about our epistemic state of knowledge, and others could disagree
    and even get different probabilities. But at least, in principle, they all will
    be able to provide arguments in favor of their data, their methods, their modeling
    decisions, and so on. A scientific and rational debate about life on Mars does
    not admit *arguments* such as â€an angel told me about tiny green creatures.â€ Bayesian
    statistics, however, is just a procedure to make scientific statements using probabilities
    as building blocks.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯æ–¹æ³•ä¸æˆ‘ä»¬æ‹¥æœ‰çš„ä»»ä½•å…¶ä»–æˆç†Ÿçš„ç§‘å­¦æ–¹æ³•ä¸€æ ·ä¸»è§‚ï¼ˆæˆ–å®¢è§‚ï¼‰ã€‚è®©æˆ‘ç”¨ä¸€ä¸ªä¾‹å­æ¥è§£é‡Šï¼šç«æ˜Ÿä¸Šæ˜¯å¦å­˜åœ¨ç”Ÿå‘½ï¼Œç­”æ¡ˆæ˜¯äºŒå…ƒçš„ï¼Œç±»ä¼¼æ˜¯æˆ–ä¸æ˜¯çš„é—®é¢˜ã€‚ä½†è€ƒè™‘åˆ°æˆ‘ä»¬æ— æ³•ç¡®è®¤è¿™ä¸€äº‹å®ï¼Œåˆç†çš„åšæ³•æ˜¯å°è¯•æ‰¾å‡ºç«æ˜Ÿä¸Šç”Ÿå‘½å­˜åœ¨çš„å¯èƒ½æ€§ã€‚ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œä»»ä½•è¯šå®ä¸”å…·æœ‰ç§‘å­¦æ€ç»´çš„äººéƒ½ä¼šä½¿ç”¨æ‰€æœ‰ç›¸å…³çš„ç«æ˜Ÿåœ°çƒç‰©ç†æ•°æ®ã€æ‰€æœ‰å…³äºç”Ÿå‘½æ‰€éœ€æ¡ä»¶çš„ç”Ÿç‰©åŒ–å­¦çŸ¥è¯†ç­‰ç­‰ã€‚è¿™ä¸ªå›ç­”å¿…ç„¶æ˜¯å…³äºæˆ‘ä»¬çŸ¥è¯†çŠ¶æ€çš„ï¼Œä¸åŒçš„äººå¯èƒ½ä¼šæœ‰ä¸åŒçš„çœ‹æ³•ï¼Œç”šè‡³å¾—å‡ºä¸åŒçš„æ¦‚ç‡ã€‚ä½†è‡³å°‘ï¼Œä»åŸåˆ™ä¸Šè®²ï¼Œä»–ä»¬éƒ½ä¼šèƒ½å¤Ÿä¸ºè‡ªå·±çš„æ•°æ®ã€æ–¹æ³•ã€å»ºæ¨¡å†³ç­–ç­‰æä¾›æ”¯æŒè®ºæ®ã€‚å…³äºç«æ˜Ÿç”Ÿå‘½çš„ç§‘å­¦ç†æ€§è¾©è®ºä¸å®¹è®¸åƒâ€œå¤©ä½¿å‘Šè¯‰æˆ‘æœ‰å°ç»¿äººâ€è¿™æ ·çš„*è®ºæ®*ã€‚ç„¶è€Œï¼Œè´å¶æ–¯ç»Ÿè®¡å­¦åªæ˜¯ä¸€ç§ä½¿ç”¨æ¦‚ç‡ä½œä¸ºæ„å»ºæ¨¡å—æ¥åšç§‘å­¦é™ˆè¿°çš„ç¨‹åºã€‚
- en: 1.6 Probabilities, uncertainty, and logic
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 æ¦‚ç‡ã€ä¸ç¡®å®šæ€§å’Œé€»è¾‘
- en: Probabilities can help us to quantify uncertainty. If we do not have information
    about a problem, it is reasonable to state that every possible event is equally
    likely. This is equivalent to assigning the same probability to every possible
    event. In the absence of information, our uncertainty is maximum, and I am not
    saying this colloquially; this is something we can compute using probabilities.
    If we know instead that some events are more likely, then this can be formally
    represented by assigning a higher probability to those events and less to the
    others. Notice that when we talk about events in stats-speak, we are not restricting
    ourselves to things that can happen, such as an asteroid crashing into Earth or
    my auntieâ€™s 60^(th) birthday party. An event is just any of the possible values
    (or a subset of values) a variable can take, such as the event that you are older
    than 30, the price of a Sachertorte, or the number of bikes that will be sold
    next year around the world.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: æ¦‚ç‡å¯ä»¥å¸®åŠ©æˆ‘ä»¬é‡åŒ–ä¸ç¡®å®šæ€§ã€‚å¦‚æœæˆ‘ä»¬å¯¹ä¸€ä¸ªé—®é¢˜æ²¡æœ‰ä¿¡æ¯ï¼Œé‚£ä¹ˆå¯ä»¥åˆç†åœ°è¯´æ¯ä¸€ä¸ªå¯èƒ½çš„äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡æ˜¯ç›¸ç­‰çš„ã€‚è¿™ç›¸å½“äºå¯¹æ¯ä¸€ä¸ªå¯èƒ½çš„äº‹ä»¶èµ‹äºˆç›¸åŒçš„æ¦‚ç‡ã€‚åœ¨æ²¡æœ‰ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ä¸ç¡®å®šæ€§æ˜¯æœ€å¤§çš„ï¼Œæˆ‘å¹¶ä¸æ˜¯éšä¾¿è¿™ä¹ˆè¯´ï¼›è¿™æ˜¯æˆ‘ä»¬å¯ä»¥é€šè¿‡æ¦‚ç‡æ¥è®¡ç®—çš„ã€‚å¦‚æœæˆ‘ä»¬çŸ¥é“æŸäº›äº‹ä»¶æ›´å¯èƒ½å‘ç”Ÿï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡ç»™è¿™äº›äº‹ä»¶èµ‹äºˆæ›´é«˜çš„æ¦‚ç‡ï¼Œè€Œå…¶ä»–äº‹ä»¶èµ‹äºˆè¾ƒä½çš„æ¦‚ç‡æ¥æ­£å¼è¡¨ç¤ºè¿™ä¸€ç‚¹ã€‚è¯·æ³¨æ„ï¼Œå½“æˆ‘ä»¬åœ¨ç»Ÿè®¡å­¦ä¸­è°ˆè®ºäº‹ä»¶æ—¶ï¼Œå¹¶ä¸ä»…ä»…å±€é™äºå¯èƒ½å‘ç”Ÿçš„äº‹æƒ…ï¼Œæ¯”å¦‚å°è¡Œæ˜Ÿæ’å‡»åœ°çƒæˆ–æˆ‘å§¨å¦ˆçš„60å²ç”Ÿæ—¥æ´¾å¯¹ã€‚äº‹ä»¶åªæ˜¯ä¸€ä¸ªå˜é‡å¯ä»¥å–çš„ä»»ä½•å¯èƒ½å€¼ï¼ˆæˆ–å€¼çš„å­é›†ï¼‰ï¼Œæ¯”å¦‚ä½ å¹´é¾„è¶…è¿‡30å²ã€è¨èµ«æ‰˜å°”ç‰¹çš„ä»·æ ¼ï¼Œæˆ–æ˜å¹´å…¨çƒå°†å”®å‡ºçš„è‡ªè¡Œè½¦æ•°é‡ã€‚
- en: 'The concept of probability is also related to the subject of logic. Under classical
    logic, we can only have statements that take the values of true or false. Under
    the Bayesian definition of probability, certainty is just a special case: a true
    statement has a probability of 1, and a false statement has a probability of 0\.
    We would assign a probability of 1 to the statement that there is Martian life
    only after having conclusive data indicating something is growing, reproducing,
    and doing other activities we associate with living organisms.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: æ¦‚ç‡çš„æ¦‚å¿µä¹Ÿä¸é€»è¾‘å­¦æœ‰å…³ã€‚åœ¨ç»å…¸é€»è¾‘ä¸‹ï¼Œæˆ‘ä»¬åªèƒ½æœ‰çœŸæˆ–å‡çš„å‘½é¢˜ã€‚åœ¨è´å¶æ–¯æ¦‚ç‡å®šä¹‰ä¸‹ï¼Œç¡®å®šæ€§åªæ˜¯ä¸€ä¸ªç‰¹æ®Šçš„æƒ…å†µï¼šä¸€ä¸ªçœŸå®çš„å‘½é¢˜çš„æ¦‚ç‡æ˜¯1ï¼Œè€Œä¸€ä¸ªè™šå‡çš„å‘½é¢˜çš„æ¦‚ç‡æ˜¯0ã€‚åªæœ‰åœ¨æ‹¥æœ‰å†³å®šæ€§æ•°æ®è¡¨æ˜æœ‰ä¸œè¥¿åœ¨ç”Ÿé•¿ã€ç¹æ®–ä»¥åŠè¿›è¡Œå…¶ä»–æˆ‘ä»¬è®¤ä¸ºä¸ç”Ÿç‰©ä½“ç›¸å…³çš„æ´»åŠ¨æ—¶ï¼Œæˆ‘ä»¬æ‰ä¼šç»™â€œç«æ˜Ÿä¸Šæœ‰ç”Ÿå‘½â€è¿™ä¸€å‘½é¢˜åˆ†é…æ¦‚ç‡ä¸º1ã€‚
- en: Notice, however, that assigning a probability of 0 is harder because we could
    always think that there is some Martian spot that is unexplored, or that we have
    made mistakes with some experiments, or there are several other reasons that could
    lead us to falsely believe life is absent on Mars even if it is not. This is related
    to Cromwellâ€™s rule, which states that we should reserve the probabilities of 0
    or 1 to logically true or false statements. Interestingly enough, it can be shown
    that if we want to extend the logic to include uncertainty, we must use probabilities
    and probability theory.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¯·æ³¨æ„ï¼Œåˆ†é…ä¸€ä¸ªæ¦‚ç‡ä¸º0æ›´éš¾ï¼Œå› ä¸ºæˆ‘ä»¬æ€»æ˜¯å¯ä»¥è®¤ä¸ºç«æ˜Ÿä¸Šè¿˜æœ‰ä¸€äº›æœªæ¢ç´¢çš„åŒºåŸŸï¼Œæˆ–è€…æˆ‘ä»¬åœ¨æŸäº›å®éªŒä¸­çŠ¯äº†é”™è¯¯ï¼Œæˆ–è€…æœ‰å…¶ä»–å‡ ä¸ªåŸå› å¯èƒ½å¯¼è‡´æˆ‘ä»¬é”™è¯¯åœ°è®¤ä¸ºç«æ˜Ÿä¸Šæ²¡æœ‰ç”Ÿå‘½ï¼Œå³ä½¿å®é™…ä¸Šæ˜¯æœ‰çš„ã€‚è¿™ä¸å…‹åŠ³æ¢…å°”æ³•åˆ™æœ‰å…³ï¼Œè¯¥æ³•åˆ™æŒ‡å‡ºæˆ‘ä»¬åº”è¯¥å°†æ¦‚ç‡ä¸º0æˆ–1çš„å‘½é¢˜ä¿ç•™ç»™é€»è¾‘ä¸ŠçœŸå®æˆ–è™šå‡çš„å‘½é¢˜ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå¯ä»¥è¯æ˜ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦å°†é€»è¾‘æ‰©å±•ä»¥åŒ…æ‹¬ä¸ç¡®å®šæ€§ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨æ¦‚ç‡å’Œæ¦‚ç‡ç†è®ºã€‚
- en: As we will soon see, Bayesâ€™ theorem is just a logical consequence of the rules
    of probability. Thus, we can think of Bayesian statistics as an extension of logic
    that is useful whenever we are dealing with uncertainty. Thus, one way to justify
    using the Bayesian method is to recognize that uncertainty is commonplace. We
    generally have to deal with incomplete and or noisy data, we are intrinsically
    limited by our evolution-sculpted primate brain, and so on.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬å¾ˆå¿«å°±ä¼šçœ‹åˆ°çš„ï¼Œè´å¶æ–¯å®šç†åªæ˜¯æ¦‚ç‡è§„åˆ™çš„é€»è¾‘ç»“æœã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°†è´å¶æ–¯ç»Ÿè®¡çœ‹ä½œæ˜¯é€»è¾‘çš„æ‰©å±•ï¼Œå®ƒåœ¨æˆ‘ä»¬å¤„ç†ä¸ç¡®å®šæ€§æ—¶éå¸¸æœ‰ç”¨ã€‚å› æ­¤ï¼Œé‡‡ç”¨è´å¶æ–¯æ–¹æ³•çš„ä¸€ä¸ªç†ç”±æ˜¯æ‰¿è®¤ä¸ç¡®å®šæ€§æ˜¯æ™®éå­˜åœ¨çš„ã€‚æˆ‘ä»¬é€šå¸¸ä¸å¾—ä¸å¤„ç†ä¸å®Œæ•´æˆ–å˜ˆæ‚çš„æ•°æ®ï¼Œæˆ‘ä»¬æœ¬è´¨ä¸Šå—åˆ°è¿›åŒ–å¡‘é€ çš„çµé•¿ç±»å¤§è„‘çš„å±€é™ï¼Œç­‰ç­‰ã€‚
- en: The Bayesian Ethos
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯ç²¾ç¥
- en: Probabilities are used to measure the uncertainty we have about parameters,
    and Bayesâ€™ theorem is a mechanism to correctly update those probabilities in light
    of new data, hopefully reducing our uncertainty.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: æ¦‚ç‡ç”¨äºè¡¡é‡æˆ‘ä»¬å¯¹å‚æ•°çš„ä¸ç¡®å®šæ€§ï¼Œè€Œè´å¶æ–¯å®šç†æ˜¯ä¸€ä¸ªåœ¨æœ‰æ–°æ•°æ®çš„æƒ…å†µä¸‹æ­£ç¡®æ›´æ–°è¿™äº›æ¦‚ç‡çš„æœºåˆ¶ï¼Œå¸Œæœ›èƒ½å¤Ÿå‡å°‘æˆ‘ä»¬çš„ä¸ç¡®å®šæ€§ã€‚
- en: 1.7 Single-parameter inference
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7 å•ä¸€å‚æ•°æ¨æ–­
- en: Now that we know what Bayesian statistics is, letâ€™s learn how to do Bayesian
    statistics with a simple example. We are going to begin inferring a single, unknown
    parameter.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çŸ¥é“äº†è´å¶æ–¯ç»Ÿè®¡æ˜¯ä»€ä¹ˆï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥å­¦ä¹ å¦‚ä½•è¿›è¡Œè´å¶æ–¯ç»Ÿè®¡ã€‚æˆ‘ä»¬å°†ä»æ¨æ–­ä¸€ä¸ªå•ä¸€çš„æœªçŸ¥å‚æ•°å¼€å§‹ã€‚
- en: 1.7.1 The coin-flipping problem
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7.1 æŠ•ç¡¬å¸é—®é¢˜
- en: 'The coin-flipping problem, or the BetaBinomial model if you want to sound fancy
    at parties, is a classical problem in statistics and goes like this: we toss a
    coin several times and record how many heads and tails we get. Based on this data,
    we try to answer questions such as, is the coin fair? Or, more generally, how
    biased is the coin? While this problem may sound dull, we should not underestimate
    it.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: æŠ•ç¡¬å¸é—®é¢˜ï¼Œæˆ–è€…å¦‚æœä½ æƒ³åœ¨èšä¼šä¸Šæ˜¾å¾—æ›´ä¸“ä¸šçš„è¯å¯ä»¥ç§°å…¶ä¸ºBetaBinomialæ¨¡å‹ï¼Œæ˜¯ç»Ÿè®¡å­¦ä¸­çš„ä¸€ä¸ªç»å…¸é—®é¢˜ï¼Œé—®é¢˜æ˜¯è¿™æ ·çš„ï¼šæˆ‘ä»¬å¤šæ¬¡æ·ç¡¬å¸å¹¶è®°å½•æ­£åé¢æœä¸Šçš„æ¬¡æ•°ã€‚åŸºäºè¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬è¯•å›¾å›ç­”è¿™æ ·çš„é—®é¢˜ï¼šè¿™æšç¡¬å¸å…¬å¹³å—ï¼Ÿæˆ–è€…ï¼Œæ›´ä¸€èˆ¬åœ°è¯´ï¼Œè¿™æšç¡¬å¸æœ‰å¤šåï¼Ÿè™½ç„¶è¿™ä¸ªé—®é¢˜å¯èƒ½å¬èµ·æ¥å¾ˆæ— èŠï¼Œä½†æˆ‘ä»¬ä¸åº”è¯¥ä½ä¼°å®ƒã€‚
- en: The coin-flipping problem is a great example to learn the basics of Bayesian
    statistics because it is a simple model that we can solve and compute with ease.
    Besides, many real problems consist of binary, mutually exclusive outcomes such
    as 0 or 1, positive or negative, odds or evens, spam or ham, hotdog or not a hotdog,
    cat or dog, safe or unsafe, and healthy or unhealthy. Thus, even when we are talking
    about coins, this model applies to any of those problems. To estimate the bias
    of a coin, and in general, to answer any questions in a Bayesian setting, we will
    need data and a probabilistic model. For this example, we will assume that we
    have already tossed a coin several times and we have a record of the number of
    observed heads, so the data-gathering part is already done. Getting the model
    will take a little bit more effort. Since this is our first model, we will explicitly
    write Bayesâ€™ theorem and do all the necessary math (donâ€™t be afraid, I promise
    it will be painless) and we will proceed very slowly. From [2](CH02.xhtml#x1-440002)
    onward, we will use PyMC and our computer to do the math for us.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: æŠ•ç¡¬å¸é—®é¢˜æ˜¯å­¦ä¹ è´å¶æ–¯ç»Ÿè®¡åŸºç¡€çš„ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°è§£å†³å’Œè®¡ç®—ã€‚æ­¤å¤–ï¼Œè®¸å¤šå®é™…é—®é¢˜ç”±äºŒå…ƒçš„ã€äº’æ–¥çš„ç»“æœç»„æˆï¼Œä¾‹å¦‚ 0
    æˆ– 1ã€æ­£æˆ–è´Ÿã€å¥‡æ•°æˆ–å¶æ•°ã€åƒåœ¾é‚®ä»¶æˆ–éåƒåœ¾é‚®ä»¶ã€çƒ­ç‹—æˆ–ä¸æ˜¯çƒ­ç‹—ã€çŒ«æˆ–ç‹—ã€å®‰å…¨æˆ–ä¸å®‰å…¨ã€å¥åº·æˆ–ä¸å¥åº·ã€‚å› æ­¤ï¼Œå³ä½¿æˆ‘ä»¬åœ¨è°ˆè®ºç¡¬å¸æ—¶ï¼Œè¿™ä¸ªæ¨¡å‹ä¹Ÿé€‚ç”¨äºä»»ä½•è¿™äº›é—®é¢˜ã€‚ä¸ºäº†ä¼°è®¡ç¡¬å¸çš„åå‘æ€§ï¼Œå¹¶ä¸”é€šå¸¸æ¥è¯´ï¼Œå›ç­”è´å¶æ–¯ç¯å¢ƒä¸­çš„ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä»¬å°†éœ€è¦æ•°æ®å’Œä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ã€‚å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å‡è®¾æˆ‘ä»¬å·²ç»æŠ›æ·äº†å‡ æ¬¡ç¡¬å¸ï¼Œå¹¶ä¸”æˆ‘ä»¬æœ‰è§‚å¯Ÿåˆ°çš„æ­£é¢æœä¸Šçš„æ¬¡æ•°è®°å½•ï¼Œæ‰€ä»¥æ•°æ®æ”¶é›†éƒ¨åˆ†å·²ç»å®Œæˆã€‚è·å¾—æ¨¡å‹å°†éœ€è¦æ›´å¤šçš„åŠªåŠ›ã€‚ç”±äºè¿™æ˜¯æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬å°†æ˜ç¡®åœ°å†™å‡ºè´å¶æ–¯å®šç†å¹¶å®Œæˆæ‰€æœ‰å¿…è¦çš„æ•°å­¦è¿ç®—ï¼ˆä¸ç”¨æ‹…å¿ƒï¼Œæˆ‘ä¿è¯è¿™ä¸ä¼šç—›è‹¦ï¼‰ï¼Œå¹¶ä¸”æˆ‘ä»¬å°†éå¸¸æ…¢åœ°è¿›è¡Œã€‚ä»[2](CH02.xhtml#x1-440002)å¼€å§‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨
    PyMC å’Œæˆ‘ä»¬çš„è®¡ç®—æœºæ¥ä¸ºæˆ‘ä»¬åšæ•°å­¦è¿ç®—ã€‚
- en: The first thing we will do is generalize the concept of bias. We will say that
    a coin with a bias of 1 will always land heads, one with a bias of 0 will always
    land tails, and one with a bias of 0.5 will land heads half of the time and tails
    half of the time. To represent the bias, we will use the parameter *Î¸*, and to
    represent the total number of heads for several tosses, we will use the variable
    *Y* . According to Bayesâ€™ theorem, we have to specify the prior, *p*(*Î¸*), and
    likelihood, *p*(*Y* |*Î¸*), we will use. Letâ€™s start with the likelihood.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯æ¨å¹¿åå‘çš„æ¦‚å¿µã€‚æˆ‘ä»¬å°†è¯´ï¼Œåå‘ä¸º1çš„ç¡¬å¸æ€»æ˜¯ä¼šæ­£é¢æœä¸Šï¼Œåå‘ä¸º0çš„ç¡¬å¸æ€»æ˜¯ä¼šåé¢æœä¸Šï¼Œè€Œåå‘ä¸º0.5çš„ç¡¬å¸åœ¨ä¸€åŠçš„æ—¶é—´é‡Œä¼šæ­£é¢æœä¸Šï¼Œå¦ä¸€åŠæ—¶é—´ä¼šåé¢æœä¸Šã€‚ä¸ºäº†è¡¨ç¤ºåå‘ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‚æ•°*Î¸*ï¼Œè€Œä¸ºäº†è¡¨ç¤ºå¤šæ¬¡æŠ•æ·ä¸­æ­£é¢æœä¸Šçš„æ€»æ¬¡æ•°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å˜é‡*Y*ã€‚æ ¹æ®è´å¶æ–¯å®šç†ï¼Œæˆ‘ä»¬å¿…é¡»æŒ‡å®šå…ˆéªŒåˆ†å¸ƒ*p*(*Î¸*)å’Œä¼¼ç„¶å‡½æ•°*p*(*Y*
    | *Î¸*)ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿™äº›ã€‚è®©æˆ‘ä»¬ä»ä¼¼ç„¶å‡½æ•°å¼€å§‹ã€‚
- en: 1.7.2 Choosing the likelihood
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7.2 é€‰æ‹©ä¼¼ç„¶å‡½æ•°
- en: 'Letâ€™s assume that only two outcomes are possibleâ€”heads or tailsâ€”and letâ€™s also
    assume that a coin toss does not affect other tosses, that is, we are assuming
    coin tosses are independent of each other. We will further assume all coin tosses
    come from the same distribution. Thus the random variable coin toss is an example
    of an **independent and identically distributed** (**iid**) variable. I hope you
    agree that these are very reasonable assumptions to make for our problem. Given
    these assumptions, a good candidate for the likelihood is the Binomial distribution:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾åªæœ‰ä¸¤ç§å¯èƒ½çš„ç»“æœâ€”â€”æ­£é¢æˆ–åé¢â€”â€”åŒæ—¶å‡è®¾ä¸€æ¬¡ç¡¬å¸æŠ•æ·ä¸ä¼šå½±å“å…¶ä»–æŠ•æ·ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å‡è®¾ç¡¬å¸æŠ•æ·æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å‡è®¾æ‰€æœ‰ç¡¬å¸æŠ•æ·æ¥è‡ªåŒä¸€åˆ†å¸ƒã€‚å› æ­¤ï¼Œéšæœºå˜é‡ç¡¬å¸æŠ•æ·æ˜¯**ç‹¬ç«‹åŒåˆ†å¸ƒ**ï¼ˆ**iid**ï¼‰å˜é‡çš„ä¸€ä¸ªä¾‹å­ã€‚æˆ‘å¸Œæœ›ä½ åŒæ„è¿™äº›å‡è®¾å¯¹äºæˆ‘ä»¬çš„é—®é¢˜æ¥è¯´æ˜¯éå¸¸åˆç†çš„ã€‚åŸºäºè¿™äº›å‡è®¾ï¼Œä¼¼ç„¶å‡½æ•°çš„ä¸€ä¸ªåˆé€‚å€™é€‰æ˜¯äºŒé¡¹åˆ†å¸ƒï¼š
- en: '![ ----N-!--- y Nâˆ’ y p(Y | Î¸) = y!(N âˆ’ y)! Î¸ (1 âˆ’ Î¸) â—Ÿ---â—â—œ---â— normalizing
    constant ](img/file43.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![ ----N-!--- y Nâˆ’ y p(Y | Î¸) = y!(N âˆ’ y)! Î¸ (1 âˆ’ Î¸) â—Ÿ---â—â—œ---â— normalizing
    constant ](img/file43.jpg)'
- en: This is a discrete distribution returning the probability of getting *y* heads
    (or, in general, successes) out of *N* coin tosses (or, in general, trials or
    experiments) given a fixed value of *Î¸*.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§ç¦»æ•£åˆ†å¸ƒï¼Œè¿”å›åœ¨*N*æ¬¡ç¡¬å¸æŠ•æ·ï¼ˆæˆ–ä¸€èˆ¬çš„è¯•éªŒæˆ–å®éªŒï¼‰ä¸­å¾—åˆ°*y*æ¬¡æ­£é¢ï¼ˆæˆ–ä¸€èˆ¬çš„æˆåŠŸï¼‰çš„æ¦‚ç‡ï¼Œå‰ææ˜¯å›ºå®šçš„*Î¸*å€¼ã€‚
- en: '*Figure [1.10](#x1-33003r10)* shows nine distributions from the Binomial family;
    each subplot has its legend indicating the values of the parameters. Notice that
    for this plot, I did not omit the values on the y-axis. I did this so you can
    check for yourself that if you sum the height of all bars, you will get 1, that
    is, for discrete distributions, the height of the bars represents actual probabilities.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ [1.10](#x1-33003r10)* æ˜¾ç¤ºäº†æ¥è‡ªäºŒé¡¹åˆ†å¸ƒæ—çš„ä¹ä¸ªåˆ†å¸ƒï¼›æ¯ä¸ªå­å›¾éƒ½æœ‰ä¸€ä¸ªå›¾ä¾‹ï¼Œè¡¨ç¤ºå‚æ•°çš„å€¼ã€‚è¯·æ³¨æ„ï¼Œå¯¹äºè¿™ä¸ªå›¾ï¼Œæˆ‘æ²¡æœ‰çœç•¥yè½´ä¸Šçš„å€¼ã€‚æˆ‘è¿™æ ·åšæ˜¯ä¸ºäº†è®©ä½ è‡ªå·±æ£€æŸ¥ï¼Œå¦‚æœä½ å°†æ‰€æœ‰æŸ±å­çš„é«˜åº¦ç›¸åŠ ï¼Œä½ å°†å¾—åˆ°1ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹äºç¦»æ•£åˆ†å¸ƒï¼ŒæŸ±å­çš„é«˜åº¦ä»£è¡¨çš„æ˜¯å®é™…æ¦‚ç‡ã€‚'
- en: '![PIC](img/file44.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file44.png)'
- en: '**FigureÂ 1.10**: Nine members of the Binomial family'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**FigureÂ 1.10**: äºŒé¡¹å®¶æ—çš„ä¹ä½æˆå‘˜'
- en: The Binomial distribution is a reasonable choice for the likelihood. We can
    see that *Î¸* indicates how likely it is to obtain a head when tossing a coin.
    This is easier to see when *N* = 1 but is valid for any value of *N*, just compare
    the value of *Î¸* with the height of the bar for *y* = 1 (heads).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: äºŒé¡¹åˆ†å¸ƒæ˜¯ä¼¼ç„¶æ€§çš„åˆç†é€‰æ‹©ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°*Î¸*è¡¨ç¤ºæŠ›ç¡¬å¸æ—¶å¾—åˆ°å¤´çš„å¯èƒ½æ€§æœ‰å¤šå¤§ã€‚å½“*N*=1æ—¶æ›´å®¹æ˜“çœ‹åˆ°ï¼Œä½†å¯¹äºä»»ä½•*N*çš„å€¼ï¼Œåªéœ€æ¯”è¾ƒ*y*=1ï¼ˆå¤´ï¼‰æ—¶*Î¸*çš„å€¼ä¸æŸ±çŠ¶å›¾çš„é«˜åº¦å³å¯ã€‚
- en: 1.7.3 Choosing the prior
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7.3 é€‰æ‹©å…ˆéªŒåˆ†å¸ƒ
- en: 'As a prior, we will use a Beta distribution, which is a very common distribution
    in Bayesian statistics and looks as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºå…ˆéªŒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è´å¡”åˆ†å¸ƒï¼Œè¿™åœ¨è´å¶æ–¯ç»Ÿè®¡ä¸­éå¸¸å¸¸è§ï¼Œå¤–è§‚å¦‚ä¸‹ï¼š
- en: '![p(Î¸) = --Î“ (ğ›¼-+-ğ›½)- Î¸ğ›¼âˆ’1(1âˆ’ Î¸)ğ›½âˆ’1 Î“â—Ÿ-(ğ›¼-)+â—â—œÎ“ (ğ›½)â— normalizing constant ](img/file45.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![p(Î¸) = --Î“ (ğ›¼-+-ğ›½)- Î¸ğ›¼âˆ’1(1âˆ’ Î¸)ğ›½âˆ’1 Î“â—Ÿ-(ğ›¼-)+â—â—œÎ“ (ğ›½)â— å½’ä¸€åŒ–å¸¸æ•°](img/file45.jpg)'
- en: If we look carefully, we will see that the Beta distribution looks similar to
    the Binomial except for the first term. Î“ is the Greek uppercase gamma letter,
    which represents the gamma function, but thatâ€™s not really important. What is
    relevant for us is that the first term is a normalizing constant that ensures
    the distribution integrates to 1\. We can see from the preceding formula that
    the Beta distribution has two parameters, *Î±* and *Î²*. *Figure [1.11](#x1-34002r11)*
    shows nine members of the Beta family.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ä»”ç»†è§‚å¯Ÿï¼Œä¼šå‘ç°è´å¡”åˆ†å¸ƒä¸äºŒé¡¹åˆ†å¸ƒçœ‹èµ·æ¥å¾ˆç›¸ä¼¼ï¼Œé™¤äº†ç¬¬ä¸€é¡¹ã€‚ Î“æ˜¯å¸Œè…Šå¤§å†™gammaå­—æ¯ï¼Œä»£è¡¨ä¼½ç›å‡½æ•°ï¼Œä½†è¿™å¹¶ä¸æ˜¯çœŸæ­£é‡è¦çš„ã€‚å¯¹æˆ‘ä»¬è€Œè¨€é‡è¦çš„æ˜¯ï¼Œç¬¬ä¸€é¡¹æ˜¯ä¸€ä¸ªå½’ä¸€åŒ–å¸¸æ•°ï¼Œç¡®ä¿åˆ†å¸ƒç§¯åˆ†ä¸º1ã€‚ä»å‰è¿°å…¬å¼ä¸­å¯ä»¥çœ‹å‡ºï¼Œè´å¡”åˆ†å¸ƒæœ‰ä¸¤ä¸ªå‚æ•°ï¼Œ*Î±*å’Œ*Î²*ã€‚*Figure
    [1.11](#x1-34002r11)* å±•ç¤ºäº†è´å¡”å®¶æ—çš„ä¹ä½æˆå‘˜ã€‚
- en: '![PIC](img/file46.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file46.png)'
- en: '**FigureÂ 1.11**: Nine members of the Beta family'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**FigureÂ 1.11**: è´å¡”å®¶æ—çš„ä¹ä½æˆå‘˜'
- en: I like the Beta distribution and all the shapes we can get from it, but why
    are we using it for our model? There are many reasons to use a Beta distribution
    for this and other problems. One of them is that the Beta distribution is restricted
    to be between 0 and 1, in the same way our *Î¸* parameter is. In general, we use
    the Beta distribution when we want to model the proportions of a Binomial variable.
    Another reason is its versatility. As we can see in *Figure [1.11](#x1-34002r11)*,
    the distribution adopts several shapes (all restricted to the [0*,*1] interval),
    including a Uniform distribution, Gaussian-like distributions, and U-like distributions.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å–œæ¬¢è´å¡”åˆ†å¸ƒåŠå…¶æ‰€æœ‰å¯èƒ½çš„å½¢çŠ¶ï¼Œä½†ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­ä½¿ç”¨å®ƒå‘¢ï¼Ÿä½¿ç”¨è´å¡”åˆ†å¸ƒæ¥å¤„ç†æ­¤ç±»é—®é¢˜æœ‰è®¸å¤šç†ç”±ã€‚å…¶ä¸­ä¹‹ä¸€æ˜¯è´å¡”åˆ†å¸ƒé™åˆ¶åœ¨0åˆ°1ä¹‹é—´ï¼Œä¸æˆ‘ä»¬çš„*Î¸*å‚æ•°ç›¸åŒã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æƒ³è¦å»ºæ¨¡äºŒé¡¹å˜é‡çš„æ¯”ä¾‹æ—¶ä½¿ç”¨è´å¡”åˆ†å¸ƒã€‚å¦ä¸€ä¸ªåŸå› æ˜¯å…¶å¤šåŠŸèƒ½æ€§ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨*Figure
    [1.11](#x1-34002r11)* ä¸­çœ‹åˆ°çš„ï¼Œè¯¥åˆ†å¸ƒé‡‡ç”¨å¤šç§å½¢çŠ¶ï¼ˆå‡é™åˆ¶åœ¨[0,1]åŒºé—´å†…ï¼‰ï¼ŒåŒ…æ‹¬å‡åŒ€åˆ†å¸ƒã€ç±»ä¼¼æ­£æ€åˆ†å¸ƒå’ŒUå½¢åˆ†å¸ƒã€‚
- en: As a third reason, the Beta distribution is the conjugate prior to the Binomial
    distribution (which we are using as the likelihood). A conjugate prior of a likelihood
    is a prior that, when used in combination with a given likelihood, returns a posterior
    with the same functional form as the prior. Untwisting the tongue, every time
    we use a Beta distribution as the prior and a Binomial distribution as the likelihood,
    we will get a Beta as the posterior distribution. There are other pairs of conjugate
    priors; for example, the Normal distribution is the conjugate prior to itself.
    For many years, Bayesian analysis was restricted to the use of conjugate priors.
    Conjugacy ensures mathematical tractability of the posterior, which is important
    given that a common problem in Bayesian statistics ends up with a posterior we
    cannot solve analytically. This was a deal breaker before the development of suitable
    computational methods to solve probabilistic methods. From *Chapter [2](CH02.xhtml#x1-440002)*
    onwards, we will learn how to use modern computational methods to solve Bayesian
    problems, whether we choose conjugate priors or not.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºç¬¬ä¸‰ä¸ªåŸå› ï¼ŒBeta åˆ†å¸ƒæ˜¯äºŒé¡¹åˆ†å¸ƒï¼ˆæˆ‘ä»¬ä½œä¸ºä¼¼ç„¶ä½¿ç”¨çš„åˆ†å¸ƒï¼‰çš„å…±è½­å…ˆéªŒã€‚å…±è½­å…ˆéªŒæ˜¯æŒ‡ï¼Œå½“ä¸ç»™å®šçš„ä¼¼ç„¶ç»“åˆä½¿ç”¨æ—¶ï¼Œè¿”å›çš„åéªŒå…·æœ‰ä¸å…ˆéªŒç›¸åŒçš„å‡½æ•°å½¢å¼ã€‚ç®€å•æ¥è¯´ï¼Œæ¯æ¬¡æˆ‘ä»¬ä½¿ç”¨
    Beta åˆ†å¸ƒä½œä¸ºå…ˆéªŒï¼ŒäºŒé¡¹åˆ†å¸ƒä½œä¸ºä¼¼ç„¶æ—¶ï¼Œæˆ‘ä»¬å°†å¾—åˆ° Beta ä½œä¸ºåéªŒåˆ†å¸ƒã€‚è¿˜æœ‰å…¶ä»–å…±è½­å…ˆéªŒçš„é…å¯¹ï¼›ä¾‹å¦‚ï¼Œæ­£æ€åˆ†å¸ƒæ˜¯å…¶è‡ªèº«çš„å…±è½­å…ˆéªŒã€‚å¤šå¹´æ¥ï¼Œè´å¶æ–¯åˆ†æä¸€ç›´å±€é™äºä½¿ç”¨å…±è½­å…ˆéªŒã€‚å…±è½­æ€§ç¡®ä¿äº†åéªŒçš„æ•°å­¦å¯è§£æ€§ï¼Œè¿™ä¸€ç‚¹éå¸¸é‡è¦ï¼Œå› ä¸ºè´å¶æ–¯ç»Ÿè®¡ä¸­ä¸€ä¸ªå¸¸è§çš„é—®é¢˜æ˜¯æˆ‘ä»¬æœ€ç»ˆå¾—åˆ°ä¸€ä¸ªæ— æ³•è§£ææ±‚è§£çš„åéªŒã€‚åœ¨å¼€å‘é€‚åˆçš„è®¡ç®—æ–¹æ³•è§£å†³æ¦‚ç‡æ–¹æ³•ä¹‹å‰ï¼Œè¿™æ›¾æ˜¯ä¸€ä¸ªå…³é”®çš„éšœç¢ã€‚ä»*ç¬¬
    [2](CH02.xhtml#x1-440002)* ç« å¼€å§‹ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ç°ä»£è®¡ç®—æ–¹æ³•è§£å†³è´å¶æ–¯é—®é¢˜ï¼Œæ— è®ºæˆ‘ä»¬é€‰æ‹©æ˜¯å¦ä½¿ç”¨å…±è½­å…ˆéªŒã€‚
- en: 1.7.4 Getting the posterior
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7.4 è·å–åéªŒ
- en: 'Letâ€™s remember that Bayesâ€™ theorem says the posterior is proportional to the
    likelihood times the prior. So, for our problem, we have to multiply the Binomial
    and the Beta distributions:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è®°ä½ï¼Œè´å¶æ–¯å®šç†è¡¨æ˜åéªŒä¸ä¼¼ç„¶å’Œå…ˆéªŒçš„ä¹˜ç§¯æˆæ­£æ¯”ã€‚å› æ­¤ï¼Œå¯¹äºæˆ‘ä»¬çš„é—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦å°†äºŒé¡¹åˆ†å¸ƒå’Œ Beta åˆ†å¸ƒç›¸ä¹˜ï¼š
- en: '![ likelihood prior â—œ---------â—â—Ÿ---------â— â—œ----------â—â—Ÿ-----------â— p(Î¸ |
    Y ) =---N-!---Î¸y(1âˆ’ Î¸)N âˆ’y -Î“-(ğ›¼+-ğ›½-)-Î¸ğ›¼âˆ’ 1(1 âˆ’ Î¸)ğ›½âˆ’1 y!(N âˆ’ y )! Î“ (ğ›¼) + Î“ (ğ›½)
    ](img/file47.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![ likelihood prior â—œ---------â—â—Ÿ---------â— â—œ----------â—â—Ÿ-----------â— p(Î¸ |
    Y ) =---N-!---Î¸y(1âˆ’ Î¸)N âˆ’y -Î“-(ğ›¼+-ğ›½-)-Î¸ğ›¼âˆ’ 1(1 âˆ’ Î¸)ğ›½âˆ’1 y!(N âˆ’ y )! Î“ (ğ›¼) + Î“ (ğ›½)
    ](img/file47.jpg)'
- en: 'We can simplify this expression by dropping all the terms that do not depend
    on *Î¸* and our results will still be valid. Accordingly, we can write:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡å»æ‰æ‰€æœ‰ä¸ *Î¸* æ— å…³çš„é¡¹æ¥ç®€åŒ–è¿™ä¸ªè¡¨è¾¾å¼ï¼Œç»“æœä»ç„¶æœ‰æ•ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å†™æˆï¼š
- en: '![ -likelihood--- -----prior---- â—œy â—â—Ÿ N âˆ’â—y â—œğ›¼âˆ’1 â—â—Ÿ ğ›½âˆ’â—1 p(Î¸ | Y) âˆ Î¸) ](img/file48.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![ -likelihood--- -----prior---- â—œy â—â—Ÿ N âˆ’â—y â—œğ›¼âˆ’1 â—â—Ÿ ğ›½âˆ’â—1 p(Î¸ | Y) âˆ Î¸) ](img/file48.jpg)'
- en: 'Reordering it, and noticing this has the form of a Beta distribution, we get:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: é‡æ–°æ’åˆ—å®ƒï¼Œå¹¶æ³¨æ„åˆ°è¿™å…·æœ‰ Beta åˆ†å¸ƒçš„å½¢å¼ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š
- en: '![p(Î¸ | Y ) = Beta (ğ›¼prior + y,ğ›½prior+N âˆ’y) ](img/file49.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![p(Î¸ | Y ) = Beta (ğ›¼prior + y,ğ›½prior+N âˆ’y) ](img/file49.jpg)'
- en: Based on this analytical expression, we can compute the posterior. *Figure [1.12](#x1-35014r12)*
    shows the results for 3 priors and different numbers of trials. The following
    block of code shows the gist to generate *Figure [1.12](#x1-35014r12)* (omitting
    the code necessary for plotting).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè¿™ä¸ªè§£æè¡¨è¾¾å¼ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—åéªŒã€‚*å›¾ [1.12](#x1-35014r12)* æ˜¾ç¤ºäº† 3 ä¸ªå…ˆéªŒå’Œä¸åŒè¯•éªŒæ¬¡æ•°ä¸‹çš„ç»“æœã€‚ä»¥ä¸‹ä»£ç å—å±•ç¤ºäº†ç”Ÿæˆ
    *å›¾ [1.12](#x1-35014r12)* çš„è¦ç‚¹ï¼ˆçœç•¥äº†ç»˜å›¾æ‰€éœ€çš„ä»£ç ï¼‰ã€‚
- en: '**CodeÂ 1.6**'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  1.6**'
- en: '[PRE5]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![PIC](img/file50.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file50.png)'
- en: '**FigureÂ 1.12**: The first subplot shows 3 priors. The rest show successive
    updates as we get new data'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 1.12**ï¼šç¬¬ä¸€ä¸ªå­å›¾æ˜¾ç¤ºäº† 3 ä¸ªå…ˆéªŒã€‚å…¶ä½™çš„æ˜¾ç¤ºäº†éšç€æ–°æ•°æ®çš„åˆ°æ¥ï¼Œæ›´æ–°åçš„ç»“æœã€‚'
- en: 'On the first subplot of *Figure [1.12](#x1-35014r12)*, we have zero trials,
    thus the three curves represent our priors:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*å›¾ [1.12](#x1-35014r12)* çš„ç¬¬ä¸€ä¸ªå­å›¾ä¸­ï¼Œæˆ‘ä»¬æœ‰é›¶æ¬¡è¯•éªŒï¼Œå› æ­¤ä¸‰æ¡æ›²çº¿è¡¨ç¤ºæˆ‘ä»¬çš„å…ˆéªŒï¼š
- en: 'The Uniform prior (black): This represents all the possible values for the
    bias being equally probable a priori.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡åŒ€å…ˆéªŒï¼ˆé»‘è‰²ï¼‰ï¼šè¿™è¡¨ç¤ºåœ¨å…ˆéªŒä¸­æ‰€æœ‰åå·®çš„å¯èƒ½å€¼éƒ½æ˜¯ç­‰æ¦‚ç‡çš„ã€‚
- en: 'The Gaussian-like prior (dark gray): This is centered and concentrated around
    0.5, so this prior is compatible with information indicating that the coin has
    more or less about the same chance of landing heads or tails. We could also say
    this prior is compatible with the knowledge that coins are fair.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é«˜æ–¯å‹å…ˆéªŒï¼ˆæ·±ç°è‰²ï¼‰ï¼šè¿™å›´ç»• 0.5 è¿›è¡Œé›†ä¸­ï¼Œè¡¨ç¤ºè¯¥å…ˆéªŒä¸ä¿¡æ¯å…¼å®¹ï¼Œè¡¨æ˜ç¡¬å¸æ­£åé¢çš„æ¦‚ç‡å¤§è‡´ç›¸åŒã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥è¯´ï¼Œè¿™ä¸ªå…ˆéªŒä¸ç¡¬å¸å…¬å¹³çš„çŸ¥è¯†æ˜¯å…¼å®¹çš„ã€‚
- en: 'The skewed prior (light gray): This puts most of the weight on a tail-biased
    outcome.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å€¾æ–œçš„å…ˆéªŒï¼ˆæµ…ç°è‰²ï¼‰ï¼šè¿™å°†å¤§éƒ¨åˆ†æƒé‡æ”¾åœ¨å°¾éƒ¨åå‘çš„ç»“æœä¸Šã€‚
- en: 'The rest of the subplots show posterior distributions for successive trials.
    The number of trials (or coin tosses) and the number of heads are indicated in
    each subplotâ€™s legend. There is also a black dot at 0.35 representing the true
    value for *Î¸*. Of course, in real problems, we do not know this value, and it
    is here just for pedagogical reasons. *Figure [1.12](#x1-35014r12)*, can teach
    us a lot about Bayesian analysis, so grab your coffee, tea, or favorite drink,
    and letâ€™s take a moment to understand it:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä½™çš„å­å›¾å±•ç¤ºäº†åç»­è¯•éªŒçš„åéªŒåˆ†å¸ƒã€‚æ¯ä¸ªå­å›¾çš„å›¾ä¾‹ä¸­æ ‡æ˜äº†è¯•éªŒæ¬¡æ•°ï¼ˆæˆ–æ·ç¡¬å¸æ¬¡æ•°ï¼‰å’Œæ­£é¢æœä¸Šçš„æ¬¡æ•°ã€‚è¿˜æœ‰ä¸€ä¸ªé»‘ç‚¹åœ¨0.35å¤„ï¼Œè¡¨ç¤º*Î¸*çš„çœŸå®å€¼ã€‚å½“ç„¶ï¼Œåœ¨å®é™…é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬å¹¶ä¸çŸ¥é“è¿™ä¸ªå€¼ï¼Œå®ƒåœ¨è¿™é‡Œä»…ç”¨äºæ•™å­¦ç›®çš„ã€‚*å›¾
    [1.12](#x1-35014r12)*ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ·±å…¥ç†è§£è´å¶æ–¯åˆ†æï¼Œå› æ­¤æ‹¿èµ·ä½ çš„å’–å•¡ã€èŒ¶æˆ–æœ€å–œæ¬¢çš„é¥®å“ï¼Œæˆ‘ä»¬æ¥èŠ±ç‚¹æ—¶é—´ç†è§£å®ƒï¼š
- en: The result of a Bayesian analysis is a posterior distribution â€“ not a single
    value but a distribution of plausible values given the data and our model.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è´å¶æ–¯åˆ†æçš„ç»“æœæ˜¯ä¸€ä¸ªåéªŒåˆ†å¸ƒâ€”â€”å®ƒä¸æ˜¯ä¸€ä¸ªå•ä¸€å€¼ï¼Œè€Œæ˜¯ç»™å®šæ•°æ®å’Œæ¨¡å‹ä¸‹çš„ä¸€ä¸ªå¯èƒ½å€¼çš„åˆ†å¸ƒã€‚
- en: The most probable value is given by the mode of the posterior (the peak of the
    distribution).
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€å¯èƒ½çš„å€¼ç”±åéªŒåˆ†å¸ƒçš„ä¼—æ•°ï¼ˆåˆ†å¸ƒçš„å³°å€¼ï¼‰ç»™å‡ºã€‚
- en: The spread of the posterior is proportional to the uncertainty about the value
    of a parameter; the more spread out the distribution, the less certain we are.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åéªŒåˆ†å¸ƒçš„æ‰©å±•ä¸å‚æ•°å€¼çš„ä¸ç¡®å®šæ€§æˆæ­£æ¯”ï¼›åˆ†å¸ƒè¶Šå¹¿æ³›ï¼Œæˆ‘ä»¬çš„ç¡®å®šæ€§å°±è¶Šä½ã€‚
- en: Intuitively, we are more confident in a result when we have observed more data
    supporting that result. Thus, even when numerically ![1 2](img/file51.jpg) = ![48](img/file52.jpg)
    = 0*.*5, seeing four heads out of eight trials gives us more confidence that the
    bias is 0.5 than observing one head out of two trials. This intuition is reflected
    in the posterior, as you can check for yourself if you pay attention to the (black)
    posterior in the third and sixth subplots; while the mode is the same, the spread
    (uncertainty) is larger in the third subplot than in the sixth subplot.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›´è§‚ä¸Šï¼Œå½“æˆ‘ä»¬è§‚å¯Ÿåˆ°æ›´å¤šæ”¯æŒæŸä¸€ç»“æœçš„æ•°æ®æ—¶ï¼Œæˆ‘ä»¬å¯¹ç»“æœçš„ä¿¡å¿ƒå°±æ›´å¼ºã€‚å› æ­¤ï¼Œå³ä½¿æ•°å€¼ä¸Š![1 2](img/file51.jpg) = ![48](img/file52.jpg)
    = 0*.*5ï¼Œåœ¨å…«æ¬¡è¯•éªŒä¸­çœ‹åˆ°å››æ¬¡æ­£é¢æœä¸Šæ¯”åœ¨ä¸¤æ¬¡è¯•éªŒä¸­çœ‹åˆ°ä¸€æ¬¡æ­£é¢æœä¸Šèƒ½æ›´æœ‰ä¿¡å¿ƒåœ°è®¤ä¸ºåå·®æ˜¯0.5ã€‚è¿™ä¸ªç›´è§‰åœ¨åéªŒåˆ†å¸ƒä¸­æœ‰æ‰€ä½“ç°ï¼Œæ‚¨å¯ä»¥è‡ªå·±æ£€æŸ¥ï¼Œå¦‚æœæ³¨æ„è§‚å¯Ÿç¬¬ä¸‰å’Œç¬¬å…­ä¸ªå­å›¾ä¸­çš„ï¼ˆé»‘è‰²ï¼‰åéªŒåˆ†å¸ƒï¼›è™½ç„¶ä¼—æ•°ç›¸åŒï¼Œä½†ç¬¬ä¸‰ä¸ªå­å›¾çš„æ‰©å±•ï¼ˆä¸ç¡®å®šæ€§ï¼‰æ¯”ç¬¬å…­ä¸ªå­å›¾æ›´å¤§ã€‚
- en: Given a sufficiently large amount of data, two or more Bayesian models with
    different priors will tend to converge to the same result. In the limit of infinite
    data, no matter which prior we use, all of them will provide the same posterior.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»™å®šè¶³å¤Ÿå¤šçš„æ•°æ®ï¼Œä¸¤ä¸ªæˆ–å¤šä¸ªå…·æœ‰ä¸åŒå…ˆéªŒçš„è´å¶æ–¯æ¨¡å‹å°†è¶‹å‘äºæ”¶æ•›åˆ°ç›¸åŒçš„ç»“æœã€‚åœ¨æ— é™æ•°æ®çš„æé™ä¸‹ï¼Œæ— è®ºæˆ‘ä»¬ä½¿ç”¨ä»€ä¹ˆå…ˆéªŒï¼Œæ‰€æœ‰çš„æ¨¡å‹éƒ½å°†æä¾›ç›¸åŒçš„åéªŒåˆ†å¸ƒã€‚
- en: Remember that infinite is a limit and not a number, so from a practical point
    of view, we could get practically equivalent posteriors for a finite and relatively
    small number of data points.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œæ— é™æ˜¯ä¸€ä¸ªæé™ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ•°å­—ï¼Œå› æ­¤ä»å®é™…è§’åº¦æ¥çœ‹ï¼Œå¯¹äºæœ‰é™ä¸”ç›¸å¯¹è¾ƒå°çš„æ•°æ®ç‚¹ï¼Œæˆ‘ä»¬å¯èƒ½å¾—åˆ°å‡ ä¹ç­‰åŒçš„åéªŒåˆ†å¸ƒã€‚
- en: How fast posteriors converge to the same distribution depends on the data and
    the model. We can see that the posteriors arising from the black prior (Uniform)
    and gray prior (biased towards tails) converge faster to almost the same distribution,
    while it takes longer for the dark gray posterior (the one arising from the concentrated
    prior). Even after 150 trials, it is somehow easy to recognize the dark gray posterior
    as a different distribution from the two others.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åéªŒæ”¶æ•›åˆ°ç›¸åŒåˆ†å¸ƒçš„é€Ÿåº¦å–å†³äºæ•°æ®å’Œæ¨¡å‹ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä»é»‘è‰²å…ˆéªŒï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰å’Œç°è‰²å…ˆéªŒï¼ˆåå‘å°¾éƒ¨ï¼‰å¾—åˆ°çš„åéªŒåˆ†å¸ƒæ”¶æ•›å¾—æ›´å¿«ï¼Œå‡ ä¹ç›¸åŒï¼Œè€Œä»æ·±ç°è‰²å…ˆéªŒï¼ˆåå‘é›†ä¸­åˆ†å¸ƒï¼‰å¾—åˆ°çš„åéªŒåˆ†å¸ƒåˆ™æ”¶æ•›å¾—è¾ƒæ…¢ã€‚å³ä½¿ç»è¿‡150æ¬¡è¯•éªŒï¼Œä»ç„¶å¾ˆå®¹æ˜“è¯†åˆ«å‡ºæ·±ç°è‰²åéªŒåˆ†å¸ƒä¸å¦å¤–ä¸¤ä¸ªåˆ†å¸ƒçš„åŒºåˆ«ã€‚
- en: Something not obvious from the figure is that we will get the same result if
    we update the posterior sequentially as if we do it all at once. We can compute
    the posterior 150 times, each time adding one more observation and using the obtained
    posterior as the new prior, or we can just compute one posterior for the 150 tosses
    at once. The result will be exactly the same. This feature not only makes perfect
    sense, but it also leads to a natural way of updating our estimations when we
    get new data, a situation common in many data-analysis problems.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»å›¾ä¸­ä¸å®¹æ˜“çœ‹å‡ºæ¥çš„ä¸€ç‚¹æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æŒ‰é¡ºåºæ›´æ–°åéªŒï¼Œå°±åƒä¸€æ¬¡æ€§è®¡ç®—åéªŒä¸€æ ·ï¼Œæœ€ç»ˆä¼šå¾—åˆ°ç›¸åŒçš„ç»“æœã€‚æˆ‘ä»¬å¯ä»¥è®¡ç®—150æ¬¡åéªŒï¼Œæ¯æ¬¡åŠ å…¥ä¸€ä¸ªæ–°è§‚å¯Ÿå€¼ï¼Œå¹¶å°†è·å¾—çš„åéªŒä½œä¸ºæ–°çš„å…ˆéªŒï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥ä¸€æ¬¡æ€§è®¡ç®—150æ¬¡æ·ç¡¬å¸çš„åéªŒã€‚ç»“æœå°†å®Œå…¨ç›¸åŒã€‚è¿™ä¸ªç‰¹æ€§ä¸ä»…éå¸¸åˆç†ï¼Œè€Œä¸”ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§åœ¨è·å–æ–°æ•°æ®æ—¶æ›´æ–°ä¼°è®¡å€¼çš„è‡ªç„¶æ–¹æ³•ï¼Œè¿™ç§æƒ…å†µåœ¨è®¸å¤šæ•°æ®åˆ†æé—®é¢˜ä¸­éƒ½å¾ˆå¸¸è§ã€‚
- en: 1.7.5 The influence of the prior
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7.5 å…ˆéªŒçš„å½±å“
- en: From the preceding example, it is clear that priors can influence inferences.
    Thatâ€™s fine â€“ priors are supposed to do that. Maybe it would be better to not
    have priors at all. That would make modeling easier, right? Well, not necessarily.
    If you are not setting the prior, someone else will be doing it for you. Sometimes
    this is fine â€“ *default priors* can be useful and have their place â€“ but sometimes
    it is better to have more control. Let me explain.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å‰é¢çš„ä¾‹å­å¯ä»¥æ¸…æ¥šåœ°çœ‹å‡ºï¼Œå…ˆéªŒå¯ä»¥å½±å“æ¨æ–­ã€‚è¿™æ˜¯æ­£å¸¸çš„â€”â€”å…ˆéªŒæœ¬æ¥å°±åº”è¯¥è¿™æ ·åšã€‚ä¹Ÿè®¸æœ€å¥½æ ¹æœ¬ä¸è®¾ç½®å…ˆéªŒï¼Œé‚£æ ·å»ºæ¨¡ä¸å°±æ›´ç®€å•äº†å—ï¼Ÿå—¯ï¼Œä¸ä¸€å®šã€‚å¦‚æœä½ ä¸è®¾ç½®å…ˆéªŒï¼Œåˆ«äººä¼šä¸ºä½ è®¾ç½®ã€‚æœ‰æ—¶è¿™æ²¡é—®é¢˜â€”â€”*é»˜è®¤å…ˆéªŒ*æ˜¯æœ‰ç”¨çš„ï¼Œä¹Ÿæœ‰å®ƒçš„ä½œç”¨â€”â€”ä½†æœ‰æ—¶æœ€å¥½èƒ½æœ‰æ›´å¤šçš„æ§åˆ¶æƒã€‚è®©æˆ‘æ¥è§£é‡Šä¸€ä¸‹ã€‚
- en: 'We can think that every (statistical) model, Bayesian or not, has some kind
    of prior, even if the prior is not set explicitly. For instance, many procedures
    typically used in frequentist statistics can be seen as special cases of a Bayesian
    model under certain conditions, such as flat priors. One common way to estimate
    parameters is known as maximum likelihood; this method avoids setting a prior
    and works just by finding the single value maximizing the likelihood. This value
    is usually notated by adding a little hat on top of the name of the parameter
    we are estimating, such as ![](img/hat_theta.png). Contrary to the posterior estimate,
    which is a distribution, ![](img/hat_theta.png) is a point estimate, a number.
    For the coin-flipping problem, we can compute it analytically:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è®¤ä¸ºæ¯ä¸€ä¸ªï¼ˆç»Ÿè®¡ï¼‰æ¨¡å‹ï¼Œä¸è®ºæ˜¯å¦æ˜¯è´å¶æ–¯æ¨¡å‹ï¼Œéƒ½æœ‰æŸç§ç±»å‹çš„å…ˆéªŒï¼Œå³ä½¿å…ˆéªŒæ²¡æœ‰æ˜ç¡®è®¾ç½®ã€‚ä¾‹å¦‚ï¼Œè®¸å¤šåœ¨é¢‘ç‡æ´¾ç»Ÿè®¡ä¸­å¸¸ç”¨çš„ç¨‹åºå¯ä»¥çœ‹ä½œæ˜¯åœ¨æŸäº›æ¡ä»¶ä¸‹ï¼ˆå¦‚å¹³å¦å…ˆéªŒï¼‰è´å¶æ–¯æ¨¡å‹çš„ç‰¹ä¾‹ã€‚ä¸€ç§å¸¸è§çš„å‚æ•°ä¼°è®¡æ–¹æ³•è¢«ç§°ä¸ºæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼›è¿™ç§æ–¹æ³•é¿å…è®¾ç½®å…ˆéªŒï¼Œåªé€šè¿‡æ‰¾åˆ°æœ€å¤§åŒ–ä¼¼ç„¶çš„å•ä¸€å€¼æ¥å·¥ä½œã€‚è¿™ä¸ªå€¼é€šå¸¸é€šè¿‡åœ¨æˆ‘ä»¬ä¼°è®¡çš„å‚æ•°åç§°ä¸Šæ–¹åŠ ä¸€ä¸ªå°å¸½å­æ¥è¡¨ç¤ºï¼Œä¾‹å¦‚![](img/hat_theta.png)ã€‚ä¸åéªŒä¼°è®¡ä¸åŒï¼ŒåéªŒä¼°è®¡æ˜¯ä¸€ä¸ªåˆ†å¸ƒï¼Œè€Œ![](img/hat_theta.png)æ˜¯ä¸€ä¸ªç‚¹ä¼°è®¡ï¼Œæ˜¯ä¸€ä¸ªæ•°å€¼ã€‚å¯¹äºæŠ›ç¡¬å¸é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è§£ææ–¹æ³•è®¡ç®—å‡ºæ¥ï¼š
- en: '![ y Ë†Î¸ = -- N ](img/file53.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![ y Ë†Î¸ = -- N ](img/file53.jpg)'
- en: If you go back to *Figure [1.12](#x1-35014r12)*, you will be able to check for
    yourself that the mode of the black posterior (the one corresponding to the uniform/flat
    prior) agrees with the values of ![](img/hat_theta.png), computed for each subplot.
    This is not a coincidence; it is a consequence of the fact that setting a Uniform
    prior and then taking the mode of the posterior is equivalent to maximum likelihood.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å›åˆ°*å›¾ [1.12](#x1-35014r12)*ï¼Œä½ å°†èƒ½è‡ªå·±æ£€æŸ¥å‡ºé»‘è‰²åéªŒçš„æ¨¡æ€ï¼ˆå³ä¸å‡åŒ€/å¹³å¦å…ˆéªŒå¯¹åº”çš„é‚£ä¸ªæ¨¡æ€ï¼‰ä¸æ¯ä¸ªå­å›¾ä¸­è®¡ç®—å¾—åˆ°çš„![](img/hat_theta.png)å€¼ä¸€è‡´ã€‚è¿™ä¸æ˜¯å·§åˆï¼›è¿™æ˜¯å› ä¸ºè®¾ç½®å‡åŒ€å…ˆéªŒåï¼Œå†å–åéªŒçš„æ¨¡æ€ç›¸å½“äºæœ€å¤§ä¼¼ç„¶ä¼°è®¡ã€‚
- en: We cannot avoid priors, but if we include them in our analysis, we can get some
    potential benefits. The most direct benefit is that we get a posterior distribution,
    which is a distribution of plausible values and not only the most probable ones.
    Having a distribution can be more informative than a single-point estimate, as
    we saw the width of the distribution is related to the uncertainty we have for
    the estimate. Another benefit is that computing the posteriors means to average
    over the prior. This can lead to models that are more difficult to overfit and
    more robust predictions [[Wilson and Izmailov](Bibliography.xhtml#Xwilson_2022),Â [2022](Bibliography.xhtml#Xwilson_2022)].
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ— æ³•é¿å…å…ˆéªŒï¼Œä½†å¦‚æœå°†å…¶çº³å…¥åˆ†æä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—ä¸€äº›æ½œåœ¨çš„å¥½å¤„ã€‚æœ€ç›´æ¥çš„å¥½å¤„æ˜¯æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªåéªŒåˆ†å¸ƒï¼Œå®ƒæ˜¯ä¸€ä¸ªåˆç†å€¼çš„åˆ†å¸ƒï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€å¯èƒ½çš„å€¼ã€‚æ‹¥æœ‰ä¸€ä¸ªåˆ†å¸ƒæ¯”å•ä¸€çš„ç‚¹ä¼°è®¡æ›´å…·ä¿¡æ¯æ€§ï¼Œæ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œåˆ†å¸ƒçš„å®½åº¦ä¸æˆ‘ä»¬å¯¹ä¼°è®¡çš„ä¸ç¡®å®šæ€§æœ‰å…³ã€‚å¦ä¸€ä¸ªå¥½å¤„æ˜¯ï¼Œè®¡ç®—åéªŒæ„å‘³ç€å¯¹å…ˆéªŒè¿›è¡Œå¹³å‡ã€‚è¿™å¯ä»¥å¯¼è‡´æ›´éš¾ä»¥è¿‡æ‹Ÿåˆçš„æ¨¡å‹å’Œæ›´ç¨³å¥çš„é¢„æµ‹[[Wilson
    å’Œ Izmailov](Bibliography.xhtml#Xwilson_2022)ï¼Œ [2022](Bibliography.xhtml#Xwilson_2022)]ã€‚
- en: Priors can bring us other benefits. Starting in the next chapter, we are going
    to use numerical methods to get posteriors. These methods feel like magic, until
    they donâ€™t. The folk theorem of statistical computing states, â€When you have computational
    problems, often thereâ€™s a problem with your modelâ€ [[Gelman](Bibliography.xhtml#Xgelman_folk_2008),Â [2008](Bibliography.xhtml#Xgelman_folk_2008)].
    Sometimes a wise choice of prior can make inference easier or faster. It is important
    to remark that we are not advocating for setting priors specifically to make inference
    faster, but it is often the case that by thinking about priors, we can get faster
    models.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: å…ˆéªŒåˆ†å¸ƒèƒ½ä¸ºæˆ‘ä»¬å¸¦æ¥å…¶ä»–å¥½å¤„ã€‚ä»ä¸‹ä¸€ç« å¼€å§‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ•°å€¼æ–¹æ³•æ¥è·å¾—åéªŒåˆ†å¸ƒã€‚è¿™äº›æ–¹æ³•çœ‹èµ·æ¥åƒé­”æ³•ï¼Œç›´åˆ°å®ƒä»¬ä¸å†æœ‰æ•ˆã€‚ç»Ÿè®¡è®¡ç®—çš„æ°‘é—´å®šç†æŒ‡å‡ºï¼šâ€œå½“ä½ é‡åˆ°è®¡ç®—é—®é¢˜æ—¶ï¼Œé€šå¸¸æ˜¯æ¨¡å‹å‡ºäº†é—®é¢˜â€[[Gelman](Bibliography.xhtml#Xgelman_folk_2008)ï¼Œ[2008](Bibliography.xhtml#Xgelman_folk_2008)]ã€‚æœ‰æ—¶å€™ï¼Œæ˜æ™ºçš„å…ˆéªŒé€‰æ‹©å¯ä»¥ä½¿æ¨æ–­å˜å¾—æ›´å®¹æ˜“æˆ–æ›´å¿«é€Ÿã€‚éœ€è¦æŒ‡å‡ºçš„æ˜¯ï¼Œæˆ‘ä»¬å¹¶ä¸æå€¡ä¸ºäº†åŠ é€Ÿæ¨æ–­è€Œç‰¹æ„è®¾ç½®å…ˆéªŒï¼Œä½†é€šå¸¸æƒ…å†µä¸‹ï¼Œé€šè¿‡è€ƒè™‘å…ˆéªŒï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æ›´å¿«é€Ÿçš„æ¨¡å‹ã€‚
- en: One advantage of priors, one that is sometimes overlooked, is that having to
    think about priors can *force us* to think a little bit deeper about the problem
    we are trying to solve and the data we have. Sometimes the modeling process leads
    to a better understanding by itself irrespective of how well we end and fit the
    data or make predictions. By being explicit about priors, we get more transparent
    models, meaning theyâ€™re easier to criticize, debug (in a broad sense of the word),
    explain to others, and hopefully improve.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: å…ˆéªŒçš„ä¸€ä¸ªä¼˜ç‚¹ï¼Œæœ‰æ—¶è¢«å¿½è§†äº†ï¼Œå°±æ˜¯å¿…é¡»è€ƒè™‘å…ˆéªŒå¯èƒ½*è¿«ä½¿æˆ‘ä»¬*æ›´æ·±å…¥åœ°æ€è€ƒæˆ‘ä»¬è¦è§£å†³çš„é—®é¢˜ä»¥åŠæˆ‘ä»¬æ‰€æ‹¥æœ‰çš„æ•°æ®ã€‚æœ‰æ—¶å€™ï¼Œå»ºæ¨¡è¿‡ç¨‹æœ¬èº«å°±èƒ½å¸¦æ¥æ›´å¥½çš„ç†è§£ï¼Œä¸ç®¡æˆ‘ä»¬æœ€ç»ˆå¦‚ä½•æ‹Ÿåˆæ•°æ®æˆ–åšå‡ºé¢„æµ‹ã€‚é€šè¿‡æ˜ç¡®å…ˆéªŒï¼Œæˆ‘ä»¬èƒ½å¤Ÿå¾—åˆ°æ›´é€æ˜çš„æ¨¡å‹ï¼Œè¿™æ„å‘³ç€è¿™äº›æ¨¡å‹æ›´å®¹æ˜“è¢«æ‰¹è¯„ã€è°ƒè¯•ï¼ˆå¹¿ä¹‰ä¸Šè®²ï¼‰ã€å‘ä»–äººè§£é‡Šï¼Œå¹¶ä¸”æœ‰å¯èƒ½å¾—åˆ°æ”¹å–„ã€‚
- en: 1.8 How to choose priors
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.8 å¦‚ä½•é€‰æ‹©å…ˆéªŒ
- en: Newcomers to Bayesian analysis (as well as detractors of this paradigm) are
    generally a little nervous about how to choose priors. Usually, they are afraid
    that the prior distribution will not let the data speak for itself! Thatâ€™s OK,
    but we have to remember that data does not speak; at best, data murmurs. We can
    only make sense of data in the context of our models, including mathematical and
    mental models. There are plenty of examples in the history of science where the
    same data led people to think differently about the same topics, and this can
    happen even if you base your opinions on formal models.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå­¦è´å¶æ–¯åˆ†æçš„äººï¼ˆä»¥åŠè¯¥èŒƒå¼çš„åå¯¹è€…ï¼‰é€šå¸¸å¯¹å¦‚ä½•é€‰æ‹©å…ˆéªŒæ„Ÿåˆ°æœ‰äº›ç´§å¼ ã€‚é€šå¸¸ï¼Œä»–ä»¬æ‹…å¿ƒå…ˆéªŒåˆ†å¸ƒä¼šä½¿æ•°æ®æ— æ³•è‡ªä¸»è¡¨è¾¾ï¼æ²¡å…³ç³»ï¼Œä½†æˆ‘ä»¬å¿…é¡»è®°ä½ï¼Œæ•°æ®å¹¶ä¸ä¼šâ€œè¯´è¯â€ï¼›å……å…¶é‡ï¼Œæ•°æ®åªæ˜¯ä½å£°ç»†è¯­ã€‚æˆ‘ä»¬åªèƒ½åœ¨æ¨¡å‹çš„ä¸Šä¸‹æ–‡ä¸­ç†è§£æ•°æ®ï¼ŒåŒ…æ‹¬æ•°å­¦æ¨¡å‹å’Œå¿ƒç†æ¨¡å‹ã€‚ç§‘å­¦å²ä¸Šæœ‰å¾ˆå¤šä¾‹å­è¡¨æ˜ï¼Œç›¸åŒçš„æ•°æ®æ›¾è®©äººä»¬å¯¹ç›¸åŒçš„è¯é¢˜äº§ç”Ÿä¸åŒçš„çœ‹æ³•ï¼Œå³ä¾¿ä½ åŸºäºæ­£å¼æ¨¡å‹æ¥å½¢æˆè§‚ç‚¹ï¼Œä¹Ÿä¼šå‘ç”Ÿè¿™ç§æƒ…å†µã€‚
- en: Some people like the idea of using non-informative priors (also known as flat,
    vague, or diffuse priors). These priors have the least possible amount of impact
    on the analysis. While it is possible to use them for some problems deriving truly
    non-informative priors can be hard or just impossible. Additionally, we generally
    can do better as we usually have some prior information.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº›äººå–œæ¬¢ä½¿ç”¨éä¿¡æ¯æ€§å…ˆéªŒï¼ˆä¹Ÿç§°ä¸ºå¹³å¦çš„ã€æ¨¡ç³Šçš„æˆ–æ‰©æ•£çš„å…ˆéªŒï¼‰è¿™ä¸€æƒ³æ³•ã€‚è¿™äº›å…ˆéªŒå¯¹åˆ†æçš„å½±å“æœ€å°ã€‚è™½ç„¶åœ¨æŸäº›é—®é¢˜ä¸­ä½¿ç”¨å®ƒä»¬æ˜¯å¯è¡Œçš„ï¼Œä½†çœŸæ­£æ¨å¯¼å‡ºéä¿¡æ¯æ€§å…ˆéªŒæ˜¯å¾ˆå›°éš¾çš„ï¼Œç”šè‡³æ˜¯ä¸å¯èƒ½çš„ã€‚æ­¤å¤–ï¼Œé€šå¸¸æˆ‘ä»¬èƒ½å¤Ÿåšå¾—æ›´å¥½ï¼Œå› ä¸ºæˆ‘ä»¬é€šå¸¸æ‹¥æœ‰ä¸€äº›å…ˆéªŒä¿¡æ¯ã€‚
- en: Throughout this book, we will follow the recommendations of Gelman, McElreath,
    Kruschke, and many others, and we will prefer weakly informative priors. For many
    problems, we often know something about the values a parameter can take. We may
    know that a parameter is restricted to being positive, or we may know the approximate
    range it can take, or whether we expect the value to be close to zero or below/above
    some value. In such cases, we can use priors to put some weak information in our
    models without being afraid of being too pushy. Because these priors work to keep
    the posterior distribution within certain reasonable bounds, they are also known
    as regularizing priors.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬å°†éµå¾ªGelmanã€McElreathã€Kruschkeç­‰äººçš„å»ºè®®ï¼Œå¹¶åå¥½å¼±ä¿¡æ¯å…ˆéªŒã€‚å¯¹äºè®¸å¤šé—®é¢˜ï¼Œæˆ‘ä»¬é€šå¸¸å¯¹ä¸€ä¸ªå‚æ•°å¯èƒ½å–çš„å€¼æœ‰æ‰€äº†è§£ã€‚æˆ‘ä»¬å¯èƒ½çŸ¥é“æŸä¸ªå‚æ•°åªèƒ½å–æ­£å€¼ï¼Œæˆ–è€…çŸ¥é“å®ƒå¯èƒ½çš„èŒƒå›´ï¼Œæˆ–è€…é¢„æœŸå®ƒæ¥è¿‘é›¶æˆ–åœ¨æŸä¸ªå€¼çš„ä¸Šä¸‹æ–¹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å…ˆéªŒæ¥åœ¨æ¨¡å‹ä¸­å¼•å…¥ä¸€äº›å¼±ä¿¡æ¯ï¼Œè€Œä¸å¿…æ‹…å¿ƒè¿‡äºå¼ºåŠ¿ã€‚å› ä¸ºè¿™äº›å…ˆéªŒæœ‰åŠ©äºä¿æŒåéªŒåˆ†å¸ƒåœ¨åˆç†çš„èŒƒå›´å†…ï¼Œæ‰€ä»¥å®ƒä»¬ä¹Ÿè¢«ç§°ä¸ºæ­£åˆ™åŒ–å…ˆéªŒã€‚
- en: Informative priors are very strong priors that convey a lot of information.
    Using them is also a valid option. Depending on your problem, it could be easy
    or not to find good-quality information from your domain knowledge and turn it
    into priors. I used to work on structural bioinformatics. In this field, people
    have been using, in Bayesian and non-Bayesian ways, all the prior information
    they could get to study and predict the structure of proteins. This is reasonable
    because we have been collecting data from thousands of carefully designed experiments
    for decades and hence we have a great amount of trustworthy prior information
    at our disposal. Not using it would be absurd! There is nothing â€œobjectiveâ€ or
    â€œscientificâ€ about throwing away valuable information. If you have reliable prior
    information, you should use it. Imagine if every time an automotive engineer had
    to design a new car, they had to start from scratch and reinvent the combustion
    engine, the wheel, and for that matter, the whole concept of a car.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿¡æ¯æ€§å…ˆéªŒæ˜¯éå¸¸å¼ºçš„å…ˆéªŒï¼Œèƒ½ä¼ é€’å¤§é‡ä¿¡æ¯ã€‚ä½¿ç”¨å®ƒä»¬ä¹Ÿæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„é€‰é¡¹ã€‚æ ¹æ®ä½ çš„é—®é¢˜ï¼Œä»é¢†åŸŸçŸ¥è¯†ä¸­æ‰¾åˆ°ä¼˜è´¨çš„ä¿¡æ¯å¹¶å°†å…¶è½¬åŒ–ä¸ºå…ˆéªŒå¯èƒ½å®¹æ˜“ï¼Œä¹Ÿå¯èƒ½ä¸å®¹æ˜“ã€‚æˆ‘æ›¾ç»ä»äº‹ç»“æ„ç”Ÿç‰©ä¿¡æ¯å­¦å·¥ä½œã€‚åœ¨è¿™ä¸ªé¢†åŸŸï¼Œå¤§å®¶ä¸€ç›´åœ¨ä½¿ç”¨è´å¶æ–¯å’Œéè´å¶æ–¯æ–¹æ³•ï¼Œåˆ©ç”¨æ‰€æœ‰èƒ½å¤Ÿè·å–çš„å…ˆéªŒä¿¡æ¯æ¥ç ”ç©¶å’Œé¢„æµ‹è›‹ç™½è´¨çš„ç»“æ„ã€‚è¿™æ˜¯åˆç†çš„ï¼Œå› ä¸ºå‡ åå¹´æ¥ï¼Œæˆ‘ä»¬é€šè¿‡æ•°åƒä¸ªç²¾å¿ƒè®¾è®¡çš„å®éªŒæ”¶é›†äº†å¤§é‡æ•°æ®ï¼Œå› æ­¤æˆ‘ä»¬æ‰‹å¤´æœ‰å¤§é‡å¯ä¿¡çš„å…ˆéªŒä¿¡æ¯ã€‚ä¸ä½¿ç”¨è¿™äº›ä¿¡æ¯ç®€ç›´è’è°¬ï¼æŠ›å¼ƒæœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œç»å¯¹æ²¡æœ‰ä»€ä¹ˆâ€œå®¢è§‚â€æˆ–â€œç§‘å­¦â€å¯è¨€ã€‚å¦‚æœä½ æœ‰å¯é çš„å…ˆéªŒä¿¡æ¯ï¼Œå°±åº”è¯¥ä½¿ç”¨å®ƒã€‚è¯•æƒ³ä¸€ä¸‹ï¼Œå¦‚æœæ¯æ¬¡æ±½è½¦å·¥ç¨‹å¸ˆéœ€è¦è®¾è®¡ä¸€è¾†æ–°è½¦æ—¶ï¼Œéƒ½å¿…é¡»ä»å¤´å¼€å§‹ï¼Œé‡æ–°å‘æ˜å†…ç‡ƒæœºã€è½¦è½®ï¼Œç”šè‡³æ˜¯æ±½è½¦çš„åŸºæœ¬æ¦‚å¿µï¼Œé‚£è¯¥å¤šä¹ˆæµªè´¹æ—¶é—´ï¼
- en: 'PreliZ is a very new Python library for prior elicitation [[Mikkola etÂ al.](Bibliography.xhtml#Xmikkola_2021),Â [2023](Bibliography.xhtml#Xmikkola_2021),Â [Icazatti
    etÂ al.](Bibliography.xhtml#Xicazatti2023),Â [2023](Bibliography.xhtml#Xicazatti2023)].
    Its mission is to help you to elicit, represent, and visualize your prior knowledge.
    For instance, we can ask PreliZ to compute the parameters of a distribution satisfying
    a set of constraints. Letâ€™s say we want to find the Beta distribution with 90%
    of the mass between 0.1 and 0.7, then we can write:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: PreliZ æ˜¯ä¸€ä¸ªå…¨æ–°çš„ Python åº“ï¼Œç”¨äºå…ˆéªŒçŸ¥è¯†çš„å¼•å‡º [[Mikkola et al.](Bibliography.xhtml#Xmikkola_2021),
    [2023](Bibliography.xhtml#Xmikkola_2021), [Icazatti et al.](Bibliography.xhtml#Xicazatti2023),
    [2023](Bibliography.xhtml#Xicazatti2023)]ã€‚å®ƒçš„ä½¿å‘½æ˜¯å¸®åŠ©ä½ å¼•å‡ºã€è¡¨ç¤ºå’Œå¯è§†åŒ–ä½ çš„å…ˆéªŒçŸ¥è¯†ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è®© PreliZ
    è®¡ç®—ä¸€ä¸ªæ»¡è¶³ä¸€ç»„çº¦æŸæ¡ä»¶çš„åˆ†å¸ƒçš„å‚æ•°ã€‚å‡è®¾æˆ‘ä»¬æƒ³æ‰¾åˆ°ä¸€ä¸ª Beta åˆ†å¸ƒï¼Œå…¶ä¸­ 90% çš„è´¨é‡ä½äº 0.1 å’Œ 0.7 ä¹‹é—´ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å†™ï¼š
- en: '**CodeÂ 1.7**'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  1.7**'
- en: '[PRE6]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The result is a Beta distribution with parameters *Î±* = 2*.*5 and *Î²* = 3*.*6
    (rounded to the first decimal point). The `pz.maxent` function computes the **maximum**
    **entropy** distribution given the constraints we specified. Why maximum entropy
    distribution? Because that is equivalent to computing the least informative distribution
    under those constraints. By default, PreliZ will plot the distribution as shown
    here:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¯ä¸€ä¸ªå…·æœ‰å‚æ•°*Î±* = 2*.*5 å’Œ *Î²* = 3*.*6ï¼ˆå››èˆäº”å…¥åˆ°å°æ•°ç‚¹åä¸€ä½ï¼‰çš„ Beta åˆ†å¸ƒã€‚`pz.maxent` å‡½æ•°è®¡ç®—äº†åœ¨æˆ‘ä»¬æŒ‡å®šçº¦æŸæ¡ä»¶ä¸‹çš„**æœ€å¤§**
    **ç†µ**åˆ†å¸ƒã€‚ä¸ºä»€ä¹ˆæ˜¯æœ€å¤§ç†µåˆ†å¸ƒï¼Ÿå› ä¸ºè¿™ç›¸å½“äºåœ¨è¿™äº›çº¦æŸæ¡ä»¶ä¸‹è®¡ç®—æœ€ä¸å…·ä¿¡æ¯é‡çš„åˆ†å¸ƒã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒPreliZ ä¼šç»˜åˆ¶å¦‚ä¸‹æ‰€ç¤ºçš„åˆ†å¸ƒï¼š
- en: '![PIC](img/file54.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file54.png)'
- en: '**FigureÂ 1.13**: Maximum entropy Beta distribution with 90% of the mass between
    0.1 and 0.7'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 1.13**ï¼šæœ€å¤§ç†µ Beta åˆ†å¸ƒï¼Œ90% çš„è´¨é‡ä½äº 0.1 å’Œ 0.7 ä¹‹é—´'
- en: As eliciting prior has many facets, PreliZ offers many other ways to elicit
    priors. If you are interested in learning more about PreliZ, you can check the
    documentation at [https://preliz.readthedocs.io](https://preliz.readthedocs.io).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå…ˆéªŒå¼•å‡ºæœ‰å¾ˆå¤šæ–¹é¢ï¼ŒPreliZ æä¾›äº†è®¸å¤šå…¶ä»–æ–¹æ³•æ¥å¼•å‡ºå…ˆéªŒã€‚å¦‚æœä½ æœ‰å…´è¶£äº†è§£æ›´å¤šå…³äº PreliZ çš„ä¿¡æ¯ï¼Œå¯ä»¥æŸ¥çœ‹ [https://preliz.readthedocs.io](https://preliz.readthedocs.io)
    ä¸Šçš„æ–‡æ¡£ã€‚
- en: Building models is an iterative process; sometimes the iteration takes a few
    minutes, and sometimes it could take years. Reproducibility matters and transparent
    assumptions in a model contribute to it. We are free to use more than one prior
    (or likelihood) for a given analysis if we are not sure about any special one;
    exploring the effect of different priors can also bring valuable information to
    the table. Part of the modeling process is about questioning assumptions, and
    priors (and likelihoods) are just that. Different assumptions will lead to different
    models and probably different results. By using data and our domain knowledge
    of the problem, we will be able to compare models and, if necessary, decide on
    a winner. *Chapter [5](CH05.xhtml#x1-950005)* will be devoted to this issue. Since
    priors have a central role in Bayesian statistics, we will keep discussing them
    as we face new problems. So if you have doubts and feel a little bit confused
    about this discussion, just keep calm and donâ€™t worry, people have been confused
    for decades and the discussion is still going on.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºæ¨¡å‹æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼›æœ‰æ—¶è¿­ä»£åªéœ€å‡ åˆ†é’Ÿï¼Œæœ‰æ—¶åˆ™å¯èƒ½éœ€è¦å‡ å¹´ã€‚å¯é‡å¤æ€§å¾ˆé‡è¦ï¼Œæ¨¡å‹ä¸­çš„é€æ˜å‡è®¾æœ‰åŠ©äºæé«˜å…¶å¯é‡å¤æ€§ã€‚å¦‚æœæˆ‘ä»¬å¯¹æŸä¸ªç‰¹å®šçš„å…ˆéªŒï¼ˆæˆ–ä¼¼ç„¶ï¼‰æ²¡æœ‰æŠŠæ¡ï¼Œæˆ‘ä»¬å¯ä»¥è‡ªç”±åœ°ä¸ºç»™å®šçš„åˆ†æä½¿ç”¨å¤šä¸ªå…ˆéªŒï¼ˆæˆ–ä¼¼ç„¶ï¼‰ï¼›æ¢ç´¢ä¸åŒå…ˆéªŒçš„æ•ˆæœä¹Ÿèƒ½å¸¦æ¥æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚å»ºæ¨¡è¿‡ç¨‹çš„ä¸€éƒ¨åˆ†æ˜¯è´¨ç–‘å‡è®¾ï¼Œå…ˆéªŒï¼ˆå’Œä¼¼ç„¶ï¼‰æ­£æ˜¯å…¶ä¸­çš„ä¸€éƒ¨åˆ†ã€‚ä¸åŒçš„å‡è®¾å°†å¯¼è‡´ä¸åŒçš„æ¨¡å‹ï¼Œå¯èƒ½è¿˜ä¼šå¾—å‡ºä¸åŒçš„ç»“æœã€‚é€šè¿‡ä½¿ç”¨æ•°æ®å’Œæˆ‘ä»¬å¯¹é—®é¢˜çš„é¢†åŸŸçŸ¥è¯†ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ¯”è¾ƒä¸åŒçš„æ¨¡å‹ï¼Œå¹¶åœ¨å¿…è¦æ—¶å†³å®šä¸€ä¸ªä¼˜èƒœè€…ã€‚*ç¬¬
    [5](CH05.xhtml#x1-950005) ç« *å°†ä¸“é—¨è®¨è®ºè¿™ä¸ªé—®é¢˜ã€‚ç”±äºå…ˆéªŒåœ¨è´å¶æ–¯ç»Ÿè®¡ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œæˆ‘ä»¬å°†åœ¨é¢å¯¹æ–°é—®é¢˜æ—¶ç»§ç»­è®¨è®ºå®ƒä»¬ã€‚å› æ­¤ï¼Œå¦‚æœä½ å¯¹è¿™ä¸ªè®¨è®ºæœ‰ç–‘é—®å¹¶æ„Ÿåˆ°æœ‰äº›å›°æƒ‘ï¼Œä¸ç”¨æ‹…å¿ƒï¼Œä¿æŒå†·é™ï¼Œåˆ«ç€æ€¥ï¼Œå¾ˆå¤šäººä¹Ÿå›°æƒ‘äº†å‡ åå¹´ï¼Œè¿™ä¸ªè®¨è®ºä»åœ¨ç»§ç»­ã€‚
- en: 1.9 Communicating a Bayesian analysis
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.9 è´å¶æ–¯åˆ†æçš„æ²Ÿé€š
- en: Creating reports and communicating results is central to the practice of statistics
    and data science. In this section, we will briefly discuss some of the peculiarities
    of this task when working with Bayesian models. In future chapters, we will keep
    looking at examples of this important matter.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºæŠ¥å‘Šå’Œä¼ è¾¾ç»“æœæ˜¯ç»Ÿè®¡å­¦å’Œæ•°æ®ç§‘å­¦å®è·µçš„æ ¸å¿ƒå†…å®¹ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ç®€è¦è®¨è®ºåœ¨ä½¿ç”¨è´å¶æ–¯æ¨¡å‹æ—¶ï¼Œè¿›è¡Œè¿™é¡¹ä»»åŠ¡çš„ä¸€äº›ç‰¹æ®Šä¹‹å¤„ã€‚åœ¨æœªæ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ç»§ç»­è®¨è®ºè¿™ä¸ªé‡è¦é—®é¢˜çš„ä¾‹å­ã€‚
- en: 1.9.1 Model notation and visualization
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.9.1 æ¨¡å‹ç¬¦å·ä¸å¯è§†åŒ–
- en: 'If you want to communicate the results of an analysis, you should also communicate
    the model you used. A common notation to succinctly represent probabilistic models
    is:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³ä¼ è¾¾åˆ†æç»“æœï¼Œä¹Ÿåº”è¯¥ä¼ è¾¾ä½ æ‰€ä½¿ç”¨çš„æ¨¡å‹ã€‚è¡¨ç¤ºæ¦‚ç‡æ¨¡å‹çš„ä¸€ç§å¸¸è§ç¬¦å·æ˜¯ï¼š
- en: '|  | *Î¸* âˆ¼ Beta(**Î±*,*Î²**) |  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  | *Î¸* âˆ¼ Beta(**Î±*,*Î²**) |  |'
- en: '|  | *y* âˆ¼ Bin(*n* = 1*,p* = *Î¸*) |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | *y* âˆ¼ Bin(*n* = 1*,p* = *Î¸*) |  |'
- en: This is just the model we use for the coin-flip example. As you may remember,
    the âˆ¼ symbol indicates that the variable on the left of it is a random variable
    distributed according to the distribution on the right. In many contexts, this
    symbol is used to indicate that a variable takes *approximately* some value, but
    when talking about probabilistic models, we will read this symbol out loud, saying
    *is distributed as*. Thus, we can say *Î¸* is distributed as a Beta with parameters
    *Î±* and *Î²*, and *y* is distributed as a Binomial with parameters *n* = 1 and
    *p* = *Î¸*. The very same model can be represented graphically using Kruschke diagrams
    as in *Figure [1.14](#x1-39001r14)*.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬ç”¨äºæŠ›ç¡¬å¸ç¤ºä¾‹çš„æ¨¡å‹ã€‚å¦‚ä½ æ‰€è®°å¾—ï¼Œâˆ¼ ç¬¦å·è¡¨ç¤ºå®ƒå·¦è¾¹çš„å˜é‡æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼ŒæŒ‰ç…§å³è¾¹çš„åˆ†å¸ƒè¿›è¡Œåˆ†å¸ƒã€‚åœ¨è®¸å¤šä¸Šä¸‹æ–‡ä¸­ï¼Œè¿™ä¸ªç¬¦å·ç”¨æ¥è¡¨ç¤ºæŸä¸ªå˜é‡*å¤§è‡´*å–æŸä¸ªå€¼ï¼Œä½†åœ¨è°ˆè®ºæ¦‚ç‡æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬ä¼šæŠŠè¿™ä¸ªç¬¦å·è¯»ä½œ
    *æŒ‰â€¦â€¦åˆ†å¸ƒ*ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è¯´ *Î¸* æŒ‰ Beta åˆ†å¸ƒï¼Œå‚æ•°ä¸º *Î±* å’Œ *Î²*ï¼Œè€Œ *y* æŒ‰ Binomial åˆ†å¸ƒï¼Œå‚æ•°ä¸º *n* = 1 å’Œ
    *p* = *Î¸*ã€‚åŒæ ·çš„æ¨¡å‹å¯ä»¥é€šè¿‡å…‹é²ä»€å…‹å›¾å½¢è±¡åœ°è¡¨ç¤ºï¼Œå¦‚ *å›¾ [1.14](#x1-39001r14)* æ‰€ç¤ºã€‚
- en: '![PIC](img/file55.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file55.png)'
- en: '**FigureÂ 1.14**: A Kruschke diagram of a BetaBinomial model'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 1.14**ï¼šBetaBinomial æ¨¡å‹çš„å…‹é²ä»€å…‹å›¾'
- en: On the first level, we have the prior that generates the values for *Î¸*, then
    the likelihood, and on the last line, the data, *y*. Arrows indicate the relationship
    between variables and the symbol âˆ¼ indicates the stochastic nature of the variables.
    All Kruschke diagrams in the book were made using the templates provided by Rasmus
    BÃ¥Ã¥th ( [http://www.sumsar.net/blog/2013/10/diy-kruschke-style-diagrams/](http://www.sumsar.net/blog/2013/10/diy-kruschke-style-diagrams/)).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ä¸€å±‚ï¼Œæˆ‘ä»¬æœ‰ç”Ÿæˆ *Î¸* å€¼çš„å…ˆéªŒï¼Œç„¶åæ˜¯ä¼¼ç„¶ï¼Œæœ€åä¸€è¡Œæ˜¯æ•°æ® *y*ã€‚ç®­å¤´è¡¨ç¤ºå˜é‡ä¹‹é—´çš„å…³ç³»ï¼Œç¬¦å· âˆ¼ è¡¨ç¤ºå˜é‡çš„éšæœºæ€§è´¨ã€‚ä¹¦ä¸­çš„æ‰€æœ‰å…‹é²ä»€å…‹å›¾éƒ½æ˜¯ä½¿ç”¨
    Rasmus BÃ¥Ã¥th æä¾›çš„æ¨¡æ¿åˆ¶ä½œçš„ï¼ˆ[http://www.sumsar.net/blog/2013/10/diy-kruschke-style-diagrams/](http://www.sumsar.net/blog/2013/10/diy-kruschke-style-diagrams/)ï¼‰ã€‚
- en: 1.9.2 Summarizing the posterior
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.9.2 æ€»ç»“åéªŒ
- en: The result of a Bayesian analysis is a posterior distribution, and all the information
    about the parameters (given a model and dataset) is contained in the posterior
    distribution. Thus, by summarizing the posterior, we are summarizing the logical
    consequences of a model and data. A common practice is to report, for each parameter,
    the mean (or mode or median) to have an idea of the location of the distribution
    and some measure of dispersion, such as the standard deviation, to have an idea
    of uncertainty in our estimates. The standard deviation works well for Normal-like
    distributions but can be misleading for other types of distributions, such as
    skewed ones.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯åˆ†æçš„ç»“æœæ˜¯åéªŒåˆ†å¸ƒï¼Œæ‰€æœ‰å…³äºå‚æ•°çš„ä¿¡æ¯ï¼ˆåœ¨ç»™å®šæ¨¡å‹å’Œæ•°æ®é›†çš„æƒ…å†µä¸‹ï¼‰éƒ½åŒ…å«åœ¨åéªŒåˆ†å¸ƒä¸­ã€‚å› æ­¤ï¼Œé€šè¿‡æ€»ç»“åéªŒï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨æ€»ç»“æ¨¡å‹å’Œæ•°æ®çš„é€»è¾‘ç»“æœã€‚ä¸€ç§å¸¸è§åšæ³•æ˜¯æŠ¥å‘Šæ¯ä¸ªå‚æ•°çš„å‡å€¼ï¼ˆæˆ–ä¼—æ•°æˆ–ä¸­ä½æ•°ï¼‰ï¼Œä»¥äº†è§£åˆ†å¸ƒçš„ä½ç½®ï¼ŒåŒæ—¶æŠ¥å‘Šä¸€äº›ç¦»æ•£åº¦é‡ï¼ˆå¦‚æ ‡å‡†å·®ï¼‰ï¼Œä»¥äº†è§£æˆ‘ä»¬ä¼°è®¡çš„ä¸ç¡®å®šæ€§ã€‚æ ‡å‡†å·®å¯¹äºç±»ä¼¼æ­£æ€åˆ†å¸ƒçš„åˆ†å¸ƒæ•ˆæœè‰¯å¥½ï¼Œä½†å¯¹äºå…¶ä»–ç±»å‹çš„åˆ†å¸ƒï¼ˆå¦‚åæ–œåˆ†å¸ƒï¼‰å¯èƒ½ä¼šäº§ç”Ÿè¯¯å¯¼ã€‚
- en: A commonly used device to summarize the spread of a posterior distribution is
    to use a **Highest-Density Interval** (**HDI**). An HDI is the shortest interval
    containing a given portion of the probability density. If we say that the 95%
    HDI for some analysis is [2*,*5], we mean that according to our data and model,
    the parameter in question is between 2 and 5 with a probability of 0.95\. There
    is nothing special about choosing 95%, 50%, or any other value. We are free to
    choose the 82% HDI interval if we like. Ideally, justifications should be context-dependent
    and not automatic, but it is okay to settle on some common value like 95%. As
    a friendly reminder of the arbitrary nature of this choice, the ArviZ default
    is 94%.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§å¸¸ç”¨çš„è®¾å¤‡æ¥æ€»ç»“åéªŒåˆ†å¸ƒçš„å¹¿åº¦æ˜¯ä½¿ç”¨**æœ€é«˜å¯†åº¦åŒºé—´**ï¼ˆ**HDI**ï¼‰ã€‚HDI æ˜¯åŒ…å«ç»™å®šæ¦‚ç‡å¯†åº¦éƒ¨åˆ†çš„æœ€çŸ­åŒºé—´ã€‚å¦‚æœæˆ‘ä»¬è¯´æŸä¸ªåˆ†æçš„ 95%
    HDI æ˜¯ [2*,*5]ï¼Œæˆ‘ä»¬æ„å‘³ç€æ ¹æ®æˆ‘ä»¬çš„æ•°æ®å’Œæ¨¡å‹ï¼Œç›¸å…³å‚æ•°çš„å€¼ä»‹äº 2 åˆ° 5 ä¹‹é—´ï¼Œä¸”å…¶æ¦‚ç‡ä¸º 0.95ã€‚é€‰æ‹© 95%ã€50% æˆ–å…¶ä»–ä»»ä½•å€¼å¹¶æ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„ã€‚æˆ‘ä»¬å¯ä»¥è‡ªç”±é€‰æ‹©
    82% çš„ HDI åŒºé—´ã€‚å¦‚æœæ„¿æ„ï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œé€‰æ‹©çš„ä¾æ®åº”æ ¹æ®ä¸Šä¸‹æ–‡è€Œå®šï¼Œè€Œä¸æ˜¯è‡ªåŠ¨çš„ï¼Œä½†é€‰æ‹©ä¸€ä¸ªå¸¸è§å€¼ï¼ˆå¦‚ 95%ï¼‰ä¹Ÿæ²¡é—®é¢˜ã€‚ä¸ºäº†æé†’è¿™ç§é€‰æ‹©çš„ä»»æ„æ€§ï¼ŒArviZ
    çš„é»˜è®¤å€¼æ˜¯ 94%ã€‚
- en: 'ArviZ is a Python package for exploratory analysis of Bayesian models, and
    it has many functions to help us summarize the posterior. One of those functions
    is `az.plot_posterior`, which we can use to generate a plot with the mean and
    HDI of *Î¸*. The distribution does not need to be a posterior distribution; any
    distribution will work. *Figure [1.15](#x1-40007r15)* shows the result for a random
    sample from a Beta distribution:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ArviZ æ˜¯ä¸€ä¸ªç”¨äºè´å¶æ–¯æ¨¡å‹æ¢ç´¢æ€§åˆ†æçš„ Python åŒ…ï¼Œæä¾›äº†è®¸å¤šå¸®åŠ©æˆ‘ä»¬æ€»ç»“åéªŒçš„åŠŸèƒ½ã€‚å…¶ä¸­ä¸€ä¸ªåŠŸèƒ½æ˜¯ `az.plot_posterior`ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒç”Ÿæˆä¸€ä¸ªåŒ…å«*Î¸*çš„å‡å€¼å’Œ
    HDI çš„å›¾è¡¨ã€‚åˆ†å¸ƒä¸å¿…æ˜¯åéªŒåˆ†å¸ƒï¼Œä»»ä½•åˆ†å¸ƒéƒ½å¯ä»¥ä½¿ç”¨ã€‚*å›¾ [1.15](#x1-40007r15)* æ˜¾ç¤ºäº†æ¥è‡ª Beta åˆ†å¸ƒçš„éšæœºæ ·æœ¬çš„ç»“æœï¼š
- en: '**CodeÂ 1.8**'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  1.8**'
- en: '[PRE7]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![PIC](img/file56.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file56.png)'
- en: '**FigureÂ 1.15**: A KDE of a sample from a Beta distribution with its mean and
    94% HDI'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 1.15**ï¼šæ¥è‡ª Beta åˆ†å¸ƒçš„æ ·æœ¬çš„ KDE å›¾ï¼ŒåŒ…å«å…¶å‡å€¼å’Œ 94% HDI'
- en: Not Confidence Intervals
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ˜¯ç½®ä¿¡åŒºé—´
- en: If you are familiar with the frequentist paradigm, please note that HDIs are
    not the same as confidence intervals. In the frequentist framework, parameters
    are fixed by design; a frequentist confidence interval either contains or does
    not contain the true value of a parameter. In the Bayesian framework, parameters
    are random variables, and thus we can talk about the probability of a parameter
    having specific values or being inside some interval. The unintuitive nature of
    confident intervals makes them easily misinterpreted and people often talk about
    frequentist confidence intervals as if they were Bayesian credible intervals.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ç†Ÿæ‚‰é¢‘ç‡å­¦æ´¾çš„èŒƒå¼ï¼Œè¯·æ³¨æ„ï¼ŒHDI ä¸ç½®ä¿¡åŒºé—´ä¸åŒã€‚åœ¨é¢‘ç‡å­¦æ´¾æ¡†æ¶ä¸‹ï¼Œå‚æ•°æ˜¯ç”±è®¾è®¡å›ºå®šçš„ï¼›é¢‘ç‡å­¦æ´¾çš„ç½®ä¿¡åŒºé—´è¦ä¹ˆåŒ…å«çœŸå®å‚æ•°å€¼ï¼Œè¦ä¹ˆä¸åŒ…å«ã€‚è€Œåœ¨è´å¶æ–¯æ¡†æ¶ä¸­ï¼Œå‚æ•°æ˜¯éšæœºå˜é‡ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥è°ˆè®ºæŸä¸ªå‚æ•°å…·æœ‰ç‰¹å®šå€¼æˆ–ä½äºæŸä¸ªåŒºé—´å†…çš„æ¦‚ç‡ã€‚ç½®ä¿¡åŒºé—´çš„ç›´è§‰æ€§è¾ƒå·®ï¼Œå®¹æ˜“è¢«è¯¯è§£ï¼Œäººä»¬å¸¸å¸¸æŠŠé¢‘ç‡å­¦æ´¾çš„ç½®ä¿¡åŒºé—´å½“ä½œè´å¶æ–¯å¯ä¿¡åŒºé—´æ¥è®¨è®ºã€‚
- en: 1.10 Summary
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.10 æ€»ç»“
- en: We began our Bayesian journey with a very brief discussion of statistical modeling,
    probabilities, conditional probabilities, random variables, probability distributions
    and Bayesâ€™ theorem. We then used the coin-flipping problem as an excuse to introduce
    basic aspects of Bayesian modeling and data analysis. We used this classic toy
    example to convey some of the most important ideas of Bayesian statistics, such
    as using probability distributions to build models and represent uncertainties.
    We tried to demystify the use of priors and put them on an equal footing with
    other elements that are part of the modeling process, such as the likelihood,
    or even more meta-questions, such as why we are trying to solve a particular problem
    in the first place.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»¥ç®€çŸ­çš„ç»Ÿè®¡å»ºæ¨¡è®¨è®ºå¼€å§‹äº†è´å¶æ–¯ä¹‹æ—…ï¼Œå†…å®¹åŒ…æ‹¬æ¦‚ç‡ã€æ¡ä»¶æ¦‚ç‡ã€éšæœºå˜é‡ã€æ¦‚ç‡åˆ†å¸ƒå’Œè´å¶æ–¯å®šç†ã€‚ç„¶åæˆ‘ä»¬ç”¨æŠ›ç¡¬å¸é—®é¢˜ä½œä¸ºå€Ÿå£ï¼Œå¼•å…¥äº†è´å¶æ–¯å»ºæ¨¡å’Œæ•°æ®åˆ†æçš„åŸºæœ¬æ¦‚å¿µã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ä¸ªç»å…¸çš„ç©å…·ç¤ºä¾‹ä¼ è¾¾äº†è´å¶æ–¯ç»Ÿè®¡å­¦ä¸­ä¸€äº›æœ€é‡è¦çš„æ€æƒ³ï¼Œæ¯”å¦‚ä½¿ç”¨æ¦‚ç‡åˆ†å¸ƒæ¥æ„å»ºæ¨¡å‹å¹¶è¡¨ç¤ºä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬è¯•å›¾æ­å¼€å…ˆéªŒçš„ç¥ç§˜é¢çº±ï¼Œå¹¶å°†å…¶ä¸æ¨¡å‹è¿‡ç¨‹ä¸­çš„å…¶ä»–å…ƒç´ ï¼ˆå¦‚ä¼¼ç„¶æ€§ï¼‰å¹³ç­‰å¯¹å¾…ï¼Œç”šè‡³æ¶‰åŠåˆ°æ›´å¤šçš„å…ƒé—®é¢˜ï¼Œæ¯”å¦‚æˆ‘ä»¬ä¸ºä½•è¦è§£å†³ç‰¹å®šé—®é¢˜ã€‚
- en: 'We ended the chapter by discussing the interpretation and communication of
    the results of a Bayesian analysis. We assume there is a true distribution that
    in general is unknown (and in principle also unknowable), from which we get a
    finite sample, either by doing an experiment, a survey, an observation, or a simulation.
    To learn something from the true distribution, given that we have only observed
    a sample, we build a probabilistic model. A probabilistic model has two basic
    ingredients: a prior and a likelihood. Using the model and the sample, we perform
    Bayesian inference and obtain a posterior distribution; this distribution encapsulates
    all the information about a problem, given our model and data. From a Bayesian
    perspective, the posterior distribution is the main object of interest and everything
    else is derived from it, including predictions in the form of a posterior predictive
    distribution. As the posterior distribution (and any other derived quantity from
    it) is a consequence of the model and data, the usefulness of Bayesian inferences
    is restricted by the quality of models and data. Finally, we briefly summarized
    the main aspects of doing Bayesian data analysis. Throughout the rest of this
    book, we will revisit these ideas to absorb them and use them as the scaffold
    of more advanced concepts.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨æœ¬ç« æœ€åè®¨è®ºäº†è´å¶æ–¯åˆ†æç»“æœçš„è§£é‡Šå’Œæ²Ÿé€šã€‚æˆ‘ä»¬å‡è®¾å­˜åœ¨ä¸€ä¸ªçœŸå®çš„åˆ†å¸ƒï¼Œè¿™ä¸ªåˆ†å¸ƒé€šå¸¸æ˜¯æœªçŸ¥çš„ï¼ˆåŸåˆ™ä¸Šä¹Ÿæ— æ³•çŸ¥é“ï¼‰ï¼Œæˆ‘ä»¬ä»ä¸­è·å–ä¸€ä¸ªæœ‰é™çš„æ ·æœ¬ï¼Œå¯èƒ½æ˜¯é€šè¿‡å®éªŒã€è°ƒæŸ¥ã€è§‚å¯Ÿæˆ–æ¨¡æ‹Ÿè·å¾—çš„ã€‚ä¸ºäº†ä»çœŸå®åˆ†å¸ƒä¸­å­¦ä¹ ä¸€äº›ä¸œè¥¿ï¼Œé‰´äºæˆ‘ä»¬åªèƒ½è§‚å¯Ÿåˆ°ä¸€ä¸ªæ ·æœ¬ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ã€‚æ¦‚ç‡æ¨¡å‹æœ‰ä¸¤ä¸ªåŸºæœ¬ç»„æˆéƒ¨åˆ†ï¼šå…ˆéªŒå’Œä¼¼ç„¶ã€‚ä½¿ç”¨æ¨¡å‹å’Œæ ·æœ¬ï¼Œæˆ‘ä»¬è¿›è¡Œè´å¶æ–¯æ¨æ–­å¹¶å¾—åˆ°åéªŒåˆ†å¸ƒï¼›è¿™ä¸ªåˆ†å¸ƒå°è£…äº†å…³äºé—®é¢˜çš„æ‰€æœ‰ä¿¡æ¯ï¼ŒåŸºäºæˆ‘ä»¬çš„æ¨¡å‹å’Œæ•°æ®ã€‚ä»è´å¶æ–¯çš„è§’åº¦çœ‹ï¼ŒåéªŒåˆ†å¸ƒæ˜¯æœ€é‡è¦çš„å¯¹è±¡ï¼Œä¸€åˆ‡å…¶ä»–å†…å®¹éƒ½ä»å®ƒä¸­æ¨å¯¼å‡ºæ¥ï¼ŒåŒ…æ‹¬ä»¥åéªŒé¢„æµ‹åˆ†å¸ƒå½¢å¼å‘ˆç°çš„é¢„æµ‹ã€‚ç”±äºåéªŒåˆ†å¸ƒï¼ˆä»¥åŠä»ä¸­æ¨å¯¼çš„ä»»ä½•å…¶ä»–é‡ï¼‰æ˜¯æ¨¡å‹å’Œæ•°æ®çš„ç»“æœï¼Œå› æ­¤è´å¶æ–¯æ¨æ–­çš„æœ‰ç”¨æ€§å—åˆ°æ¨¡å‹å’Œæ•°æ®è´¨é‡çš„é™åˆ¶ã€‚æœ€åï¼Œæˆ‘ä»¬ç®€è¦æ€»ç»“äº†è´å¶æ–¯æ•°æ®åˆ†æçš„ä¸»è¦æ–¹é¢ã€‚åœ¨æœ¬ä¹¦çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å†æ¬¡å›é¡¾è¿™äº›æ€æƒ³ï¼Œå°†å…¶å¸æ”¶å¹¶ä½œä¸ºæ›´é«˜çº§æ¦‚å¿µçš„æ¡†æ¶ã€‚
- en: In the next chapter, we will introduce PyMC, which is a Python library for Bayesian
    modeling and probabilistic machine learning and will use more features from ArviZ,
    a Python library for the exploratory analysis of Bayesian models, and PreliZ a
    Python library for prior elicitation.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†ä»‹ç»PyMCï¼Œå®ƒæ˜¯ä¸€ä¸ªç”¨äºè´å¶æ–¯å»ºæ¨¡å’Œæ¦‚ç‡æœºå™¨å­¦ä¹ çš„Pythonåº“ï¼Œè¿˜ä¼šä½¿ç”¨æ›´å¤šæ¥è‡ªArviZçš„ç‰¹æ€§ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè´å¶æ–¯æ¨¡å‹æ¢ç´¢æ€§åˆ†æçš„Pythonåº“ï¼Œä»¥åŠPreliZï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå…ˆéªŒå¼•å¯¼çš„Pythonåº“ã€‚
- en: 1.11 Exercises
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.11 ç»ƒä¹ 
- en: 'We do not know whether the brain works in a Bayesian way, in an approximately
    Bayesian fashion, or maybe some evolutionary (more or less) optimized heuristics.
    Nevertheless, we know that we learn by exposing ourselves to data, examples, and
    exercisesâ€¦ Well you may say that humans never learn, given our record as a species
    on subjects such as wars or economic systems that prioritize profit and not peopleâ€™s
    well-being... Anyway, I recommend you do the proposed exercises at the end of
    each chapter:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸çŸ¥é“å¤§è„‘æ˜¯å¦ä»¥è´å¶æ–¯æ–¹å¼å·¥ä½œï¼Œæˆ–ä»¥å¤§è‡´è´å¶æ–¯æ–¹å¼å·¥ä½œï¼Œäº¦æˆ–æ˜¯æŸç§è¿›åŒ–ï¼ˆæˆ–å¤šæˆ–å°‘ï¼‰ä¼˜åŒ–çš„å¯å‘å¼æ–¹æ³•ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬çŸ¥é“é€šè¿‡æ¥è§¦æ•°æ®ã€ç¤ºä¾‹å’Œç»ƒä¹ æ¥å­¦ä¹ â€¦â€¦ä½ å¯èƒ½ä¼šè¯´ï¼Œäººç±»ä»æœªçœŸæ­£å­¦ä¹ ï¼Œçœ‹çœ‹æˆ‘ä»¬ä½œä¸ºç‰©ç§åœ¨æˆ˜äº‰æˆ–ç»æµä½“ç³»ç­‰æ–¹é¢çš„è¡¨ç°ï¼Œè¿™äº›ä½“ç³»ä¼˜å…ˆè€ƒè™‘åˆ©æ¶¦è€Œéäººæ°‘çš„ç¦ç¥‰â€¦â€¦ä¸ç®¡æ€æ ·ï¼Œæˆ‘å»ºè®®ä½ åœ¨æ¯ç« ç»“æŸæ—¶åšä¸€ä¸‹æ‰€æçš„ç»ƒä¹ ï¼š
- en: 'Suppose you have a jar with 4 jelly beans: 2 are strawberry-flavored, 1 is
    blueberry-flavored, and 1 is cinnamon-flavored. You draw one jelly bean at random
    from the jar.'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‡è®¾ä½ æœ‰ä¸€ä¸ªç½å­ï¼Œé‡Œé¢æœ‰4é¢—æœå†»è±†ï¼š2é¢—æ˜¯è‰è“å‘³çš„ï¼Œ1é¢—æ˜¯è“è“å‘³çš„ï¼Œ1é¢—æ˜¯è‚‰æ¡‚å‘³çš„ã€‚ä½ ä»ç½å­é‡ŒéšæœºæŠ½å–ä¸€é¢—æœå†»è±†ã€‚
- en: What is the sample space for this experiment?
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå®éªŒçš„æ ·æœ¬ç©ºé—´æ˜¯ä»€ä¹ˆï¼Ÿ
- en: We define event *A* as *the jelly bean drawn is strawberry-flavored* and event
    *B* as *The jelly bean drawn is not cinnamon-flavored*. What are the probabilities
    of events *A* and *B*?
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†äº‹ä»¶*A*å®šä¹‰ä¸º*æŠ½åˆ°çš„æœå†»è±†æ˜¯è‰è“å‘³çš„*ï¼Œå°†äº‹ä»¶*B*å®šä¹‰ä¸º*æŠ½åˆ°çš„æœå†»è±†ä¸æ˜¯è‚‰æ¡‚å‘³çš„*ã€‚äº‹ä»¶*A*å’Œ*B*çš„æ¦‚ç‡åˆ†åˆ«æ˜¯å¤šå°‘ï¼Ÿ
- en: Are events *A* and *B* mutually exclusive? Why or why not?
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: äº‹ä»¶*A*å’Œ*B*æ˜¯äº’æ–¥äº‹ä»¶å—ï¼Ÿä¸ºä»€ä¹ˆæˆ–è€…ä¸ºä»€ä¹ˆä¸ï¼Ÿ
- en: 'Previously, we defined a Python function `P` to compute the probability of
    an event using the naive definition of probability. Generalize that function to
    compute the probability of events when they are not all equally likely. Use this
    new function to compute the probability of events *A* and *B* from the previous
    exercise. Hint: you can pass a third argument with the probability of each event.'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¹‹å‰ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªPythonå‡½æ•°`P`æ¥ä½¿ç”¨æ¦‚ç‡çš„ç®€å•å®šä¹‰è®¡ç®—äº‹ä»¶çš„æ¦‚ç‡ã€‚å°†è¯¥å‡½æ•°æ¨å¹¿ï¼Œä»¥è®¡ç®—å½“äº‹ä»¶çš„æ¦‚ç‡ä¸å®Œå…¨ç›¸ç­‰æ—¶çš„äº‹ä»¶æ¦‚ç‡ã€‚ä½¿ç”¨è¿™ä¸ªæ–°å‡½æ•°è®¡ç®—å‰é¢ç»ƒä¹ ä¸­äº‹ä»¶*A*å’Œ*B*çš„æ¦‚ç‡ã€‚æç¤ºï¼šä½ å¯ä»¥ä¼ é€’ä¸€ä¸ªç¬¬ä¸‰ä¸ªå‚æ•°æ¥è¡¨ç¤ºæ¯ä¸ªäº‹ä»¶çš„æ¦‚ç‡ã€‚
- en: Use PreliZ to explore different parameters for the BetaBinomial and Gaussian
    distributions. Use the methods `plot_pdf`, `plot_cdf`, and `plot_interactive`.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨PreliZæ¢ç´¢BetaBinomialå’Œé«˜æ–¯åˆ†å¸ƒçš„ä¸åŒå‚æ•°ã€‚ä½¿ç”¨`plot_pdf`ã€`plot_cdf`å’Œ`plot_interactive`æ–¹æ³•ã€‚
- en: We discussed the probability mass/density functions and the cumulative density
    function. But there are other ways to represent functions like the percentile
    point function ppf. Using the `plot_ppf` method of PreliZ, plot the percentile
    point function for the BetaBinomial and Gaussian distributions. Can you explain
    how the ppf is related to the cdf and pmf/pdf?
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¨è®ºäº†æ¦‚ç‡è´¨é‡/å¯†åº¦å‡½æ•°å’Œç´¯ç§¯åˆ†å¸ƒå‡½æ•°ã€‚ä½†ä¹Ÿæœ‰å…¶ä»–æ–¹å¼è¡¨ç¤ºå‡½æ•°ï¼Œæ¯”å¦‚ç™¾åˆ†ä½ç‚¹å‡½æ•°ppfã€‚ä½¿ç”¨PreliZçš„`plot_ppf`æ–¹æ³•ï¼Œç»˜åˆ¶BetaBinomialå’Œé«˜æ–¯åˆ†å¸ƒçš„ç™¾åˆ†ä½ç‚¹å‡½æ•°ã€‚ä½ èƒ½è§£é‡Šppfæ˜¯å¦‚ä½•ä¸cdfå’Œpmf/pdfç›¸å…³çš„å—ï¼Ÿ
- en: 'From the following expressions, which one corresponds to: the probability of
    being sunny given that it is 9^(th) of July of 1816?'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹è¡¨è¾¾å¼ä¸­ï¼Œå“ªä¸€ä¸ªå¯¹åº”äºï¼šåœ¨1816å¹´7æœˆ9æ—¥å·²çŸ¥çš„æƒ…å†µä¸‹ï¼Œæ™´å¤©çš„æ¦‚ç‡ï¼Ÿ
- en: '*p*(sunny)'
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*p*(sunny)'
- en: '*p*(sunny|July)'
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*p*(sunny|July)'
- en: '*p*(sunny|9 of July of 1816)'
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*p*(sunny|9 of July of 1816)'
- en: '*p*(9^(th) of July of 1816|sunny)'
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*p*(9^(th) of July of 1816|sunny)'
- en: '![p(sunny,9th of July-of 1816) p(9th of July of 1816)](img/file57.jpg)'
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_IMG
  zh: '![p(sunny,9th of July-of 1816) p(9th of July of 1816)](img/file57.jpg)'
- en: We showed that the probability of choosing a human at random and picking the
    Pope is not the same as the probability of the Pope being human. In the animated
    series Futurama, the (Space) Pope is a reptile. How does this change your previous
    calculations?
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å±•ç¤ºäº†éšæœºé€‰æ‹©ä¸€ä¸ªäººå¹¶é€‰å‡ºæ•™çš‡çš„æ¦‚ç‡ä¸æ•™çš‡æ˜¯äººç±»çš„æ¦‚ç‡æ˜¯ä¸åŒçš„ã€‚åœ¨åŠ¨ç”»ç³»åˆ—ã€Šæœªæ¥éƒ½å¸‚ã€‹ä¸­ï¼Œ(å¤ªç©º)æ•™çš‡æ˜¯çˆ¬è¡ŒåŠ¨ç‰©ã€‚è¿™ä¼šå¦‚ä½•æ”¹å˜ä½ ä¹‹å‰çš„è®¡ç®—ç»“æœï¼Ÿ
- en: Following the example in *Figure [1.9](#x1-27006r9)*, use PreliZ to compute
    the moments for the SkewNormal distribution for a different combination of parameters.
    Generate random samples of different sizes, like 10, 100, and 1,000, and see if
    you can recover the values of the first two moments (mean and variance) from the
    samples. What do you observe?
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æŒ‰ç…§*å›¾ [1.9](#x1-27006r9)*ä¸­çš„ç¤ºä¾‹ï¼Œä½¿ç”¨PreliZè®¡ç®—SkewNormalåˆ†å¸ƒçš„çŸ©ï¼Œå‚æ•°ç»„åˆä¸åŒã€‚ç”Ÿæˆä¸åŒå¤§å°çš„éšæœºæ ·æœ¬ï¼Œä¾‹å¦‚10ã€100å’Œ1,000ï¼Œçœ‹çœ‹æ˜¯å¦èƒ½ä»æ ·æœ¬ä¸­æ¢å¤å‡ºå‰ä¸¤ä¸ªçŸ©çš„å€¼ï¼ˆå‡å€¼å’Œæ–¹å·®ï¼‰ã€‚ä½ è§‚å¯Ÿåˆ°ä»€ä¹ˆï¼Ÿ
- en: Repeat the previous exercise for the Studentâ€™s T distribution. Try values of
    *Î½* like 2, 3, 500\. What do you observe?
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹å­¦ç”Ÿçš„Tåˆ†å¸ƒé‡å¤ä¹‹å‰çš„ç»ƒä¹ ã€‚å°è¯•*Î½*çš„å€¼ï¼Œæ¯”å¦‚2ã€3ã€500ã€‚ä½ è§‚å¯Ÿåˆ°ä»€ä¹ˆï¼Ÿ
- en: 'In the following definition of a probabilistic model, identify the prior and
    the likelihood:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹çš„æ¦‚ç‡æ¨¡å‹å®šä¹‰ä¸­ï¼Œè¯†åˆ«å…ˆéªŒå’Œä¼¼ç„¶ï¼š
- en: '![Y âˆ¼ Normal (Î¼,Ïƒ) Î¼ âˆ¼ Normal (0,2) Ïƒ âˆ¼ HalfNormal (0.75 ) ](img/file58.jpg)'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Y âˆ¼ Normal (Î¼,Ïƒ) Î¼ âˆ¼ Normal (0,2) Ïƒ âˆ¼ HalfNormal (0.75 ) ](img/file58.jpg)'
- en: In the previous model, how many parameters will the posterior have? Compare
    it with the model for the coin-flipping problem.
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨å‰é¢çš„æ¨¡å‹ä¸­ï¼ŒåéªŒå°†æœ‰å¤šå°‘ä¸ªå‚æ•°ï¼Ÿä¸æ·ç¡¬å¸é—®é¢˜çš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚
- en: Write Bayesâ€™ theorem for the model in exercise 9.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸ºç¬¬9é¢˜ä¸­çš„æ¨¡å‹å†™å‡ºè´å¶æ–¯å®šç†ã€‚
- en: Letâ€™s suppose that we have two coins; when we toss the first coin, half of the
    time it lands on tails and half of the time on heads. The other coin is a loaded
    coin that always lands on heads. If we take one of the coins at random and get
    a head, what is the probability that this coin is the unfair one?
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æœ‰ä¸¤æšç¡¬å¸ï¼›å½“æˆ‘ä»¬æ·ç¬¬ä¸€æšç¡¬å¸æ—¶ï¼Œæ­£é¢æœä¸Šçš„æ¦‚ç‡ä¸ºä¸€åŠï¼Œåé¢æœä¸Šçš„æ¦‚ç‡ä¹Ÿä¸ºä¸€åŠã€‚å¦ä¸€æšç¡¬å¸æ˜¯ä¸€æšåŠ é‡ç¡¬å¸ï¼Œæ€»æ˜¯æœä¸Šæ­£é¢ã€‚å¦‚æœæˆ‘ä»¬éšæœºé€‰æ‹©å…¶ä¸­ä¸€æšç¡¬å¸å¹¶ä¸”æ·å‡ºæ­£é¢ï¼Œé‚£ä¹ˆè¿™æšç¡¬å¸æ˜¯ä¸å…¬å¹³çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿ
- en: Try re-plotting *Figure [1.12](#x1-35014r12)* using other priors (`beta_params`)
    and other data (`trials` and `data`).
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°è¯•ä½¿ç”¨å…¶ä»–å…ˆéªŒï¼ˆ`beta_params`ï¼‰å’Œå…¶ä»–æ•°æ®ï¼ˆ`trials` å’Œ `data`ï¼‰é‡æ–°ç»˜åˆ¶*å›¾ [1.12](#x1-35014r12)*ã€‚
- en: 'Read about the Cromwell rule on Wikipedia: [https://en.wikipedia.org/wiki/Cromwell%27s_rule](https://en.wikipedia.org/wiki/Cromwell%27s_rule).'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é˜…è¯»å…³äºå…‹ä¼¦å¨å°”æ³•åˆ™çš„ Wikipedia æ–‡ç« ï¼š[https://en.wikipedia.org/wiki/Cromwell%27s_rule](https://en.wikipedia.org/wiki/Cromwell%27s_rule)ã€‚
- en: 'Read about probabilities and the Dutch book on Wikipedia: [https://en.wikipedia.org/wiki/Dutch_book](https://en.wikipedia.org/wiki/Dutch_book).'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é˜…è¯»å…³äºæ¦‚ç‡å’Œè·å…°ä¹¦çš„ Wikipedia æ–‡ç« ï¼š[https://en.wikipedia.org/wiki/Dutch_book](https://en.wikipedia.org/wiki/Dutch_book)ã€‚
- en: Join our community Discord space
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘ä»¬çš„ç¤¾åŒº Discord ç©ºé—´
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at: [https://packt.link/bayesian](https://packt.link/bayesian)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘ä»¬çš„ Discord ç¤¾åŒºï¼Œä¸å¿—åŒé“åˆçš„äººä¸€èµ·å­¦ä¹ ï¼Œå’Œè¶…è¿‡ 5000 åæˆå‘˜å…±åŒè¿›æ­¥ï¼š [https://packt.link/bayesian](https://packt.link/bayesian)
- en: '![PIC](img/file1.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file1.png)'
