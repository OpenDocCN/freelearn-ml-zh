- en: Chapter 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章
- en: Thinking Probabilistically
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概率思维
- en: Probability theory is nothing but common sense reduced to calculation. – Pierre
    Simon Laplace
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 概率论无非是将常识简化为计算。——皮埃尔·西蒙·拉普拉斯
- en: In this chapter, we will learn about the core concepts of Bayesian statistics
    and some of the instruments in the Bayesian toolbox. We will use some Python code,
    but this chapter will be mostly theoretical; most of the concepts we will see
    here will be revisited many times throughout this book. This chapter, being heavy
    on the theoretical side, is perhaps a little anxiogenic for the coder in you,
    but I think it will ease the path to effectively applying Bayesian statistics
    to your problems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将学习贝叶斯统计的核心概念以及贝叶斯工具箱中的一些工具。我们会使用一些 Python 代码，但这一章大部分内容将是理论性的；我们在这里看到的大部分概念将在本书的许多部分中反复出现。这一章理论性较强，可能会让你这个编码者有些焦虑，但我认为它会为有效地将贝叶斯统计应用于你的问题铺平道路。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将涵盖以下主题：
- en: Statistical modeling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计建模
- en: Probabilities and uncertainty
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率与不确定性
- en: Bayes’ theorem and statistical inference
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯定理与统计推断
- en: Single-parameter inference and the classic coin-flip problem
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单参数推断和经典的抛硬币问题
- en: Choosing priors and why people often don’t like them but should
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择先验分布及为何人们通常不喜欢它们，但其实应该喜欢
- en: Communicating a Bayesian analysis
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传达贝叶斯分析结果
- en: 1.1 Statistics, models, and this book’s approach
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 统计学、模型和本书的方法
- en: 'Statistics is about collecting, organizing, analyzing, and interpreting data,
    and hence statistical knowledge is essential for data analysis. Two main statistical
    methods are used in data analysis:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学是关于收集、整理、分析和解释数据的，因此统计知识对数据分析至关重要。在数据分析中使用了两种主要的统计方法：
- en: '**Exploratory Data Analysis (EDA)**: This is about numerical summaries, such
    as the mean, mode, standard deviation, and interquartile ranges. EDA is also about
    visually inspecting the data, using tools you may be already familiar with, such
    as histograms and scatter plots.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索性数据分析（EDA）**：这涉及到数值汇总，例如均值、众数、标准差和四分位数范围。EDA 还涉及通过可视化检查数据，使用你可能已经熟悉的工具，比如直方图和散点图。'
- en: '**Inferential statistics**: This is about making statements beyond the current
    data. We may want to understand some particular phenomenon, maybe we want to make
    predictions for future (yet unobserved) data points, or we need to choose among
    several competing explanations for the same set of observations. In summary, inferential
    statistics allow us to draw meaningful insights from a limited set of data and
    make informed decisions based on the results of our analysis.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推断统计**：这是关于超越当前数据进行推理。我们可能想要了解某种特定现象，或者我们想要对未来（尚未观察到的）数据点进行预测，或者我们需要在多个竞争性解释之间做出选择，针对同一组观察数据。总之，推断统计让我们能够从有限的数据中提取有意义的见解，并基于分析结果做出明智的决策。'
- en: A Match Made in Heaven
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 天作之合
- en: The focus of this book is on how to perform Bayesian inferential statistics,
    but we will also use ideas from EDA to summarize, interpret, check, and communicate
    the results of Bayesian inference.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的重点是如何进行贝叶斯推断统计，但我们也会借用探索性数据分析（EDA）中的一些思想来总结、解释、检查并传达贝叶斯推断的结果。
- en: 'Most introductory statistical courses, at least for non-statisticians, are
    taught as a collection of recipes that go like this: go to the statistical pantry,
    pick one tin can and open it, add data to taste, and stir until you obtain a consistent
    p-value, preferably under 0.05\. The main goal of these courses is to teach you
    how to pick the proper can. I never liked this approach, mainly because the most
    common result is a bunch of confused people unable to grasp, even at the conceptual
    level, the unity of the different learned methods. We will take a different approach:
    we will learn some recipes, but they will be homemade rather than canned food;
    we will learn how to mix fresh ingredients that will suit different statistical
    occasions and, more importantly, that will let you apply concepts far beyond the
    examples in this book.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数入门级统计课程，至少对于非统计学专业的人来说，通常被教授为一系列的“配方”，大致如下：走进统计学的储藏室，挑选一罐罐头打开，加入数据，按个人口味调味，搅拌直到得到一个一致的
    p 值，最好小于 0.05。 这些课程的主要目标是教你如何选择合适的罐头。我从不喜欢这种方法，主要是因为最常见的结果是一群困惑的人，甚至在概念层面也无法理解不同学习方法的统一性。我们将采取不同的方法：我们将学习一些配方，但它们是自制的，而非罐头食品；我们将学习如何混合新鲜的原料，适用于不同的统计场合，更重要的是，这将让你能将这些概念应用到本书中的例子之外的场景中。
- en: 'Taking this approach is possible for two reasons:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种方法是因为两个原因：
- en: '**Ontological**: Statistics is a form of modeling unified under the mathematical
    framework of probability theory. Using a probabilistic approach provides a unified
    view of what may seem like very disparate methods; statistical methods and machine
    learning methods look much more similar under the probabilistic lens.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本体论**：统计学是一种建模方法，统一于概率论的数学框架下。采用概率方法能够提供一个统一的视角来看待那些看似截然不同的方法；统计方法和机器学习方法在概率视角下显得更加相似。'
- en: '**Technical**: Modern software, such as PyMC, allows practitioners, just like
    you and me, to define and solve models in a relatively easy way. Many of these
    models were unsolvable just a few years ago or required a high level of mathematical
    and technical sophistication.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**技术性**：现代软件，如 PyMC，使得从业者——就像你我一样——能够相对轻松地定义和解决模型。几年前，许多这样的模型是无法求解的，或者需要高水平的数学和技术精湛。'
- en: 1.2 Working with data
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 处理数据
- en: Data is an essential ingredient in statistics and data science. Data comes from
    several sources, such as experiments, computer simulations, surveys, and field
    observations. If we are the ones in charge of generating or gathering the data,
    it is always a good idea to first think carefully about the questions we want
    to answer and which methods we will use, and only then proceed to get the data.
    There is a whole branch of statistics dealing with data collection, known as experimental
    design. In the era of the data deluge, we can sometimes forget that gathering
    data is not always cheap. For example, while it is true that the **Large Hadron
    Collider** (**LHC**) produces hundreds of terabytes a day, its construction took
    years of manual and intellectual labor.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是统计学和数据科学的核心要素。数据来源于多个渠道，例如实验、计算机模拟、调查和实地观察。如果我们负责生成或收集数据，首先仔细思考我们想要回答的问题以及我们将使用哪些方法是非常重要的，只有在此之后，我们才应该开始收集数据。统计学中有一个专门研究数据收集的分支，称为实验设计。在数据泛滥的时代，我们有时会忘记，收集数据并不总是便宜的。例如，虽然**大型强子对撞机**（**LHC**）每天能产生数百
    TB 的数据，但它的建造过程花费了数年的人工和智力劳动。
- en: As a general rule, we can think of the process of generating the data as stochastic,
    because there is ontological, technical, and/or epistemic uncertainty, that is,
    the system is intrinsically stochastic, there are technical issues adding noise
    or restricting us from measuring with arbitrary precision, and/or there are conceptual
    limitations veiling details from us. For all these reasons, we always need to
    interpret data in the context of models, including mental and formal ones. Data
    does not speak but through models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们可以认为数据生成的过程是随机的，因为存在本体论、技术性和/或认识论的不确定性，也就是说，系统本质上是随机的，技术性问题会增加噪声或限制我们以任意精度进行测量，和/或存在概念性局限遮蔽了我们无法看到的细节。基于这些原因，我们总是需要在模型的框架下解读数据，包括心智模型和形式化模型。数据不直接发声，只有通过模型才有意义。
- en: In this book, we will assume that we already have collected the data. Our data
    will also be clean and tidy, something that’s rarely true in the real world. We
    will make these assumptions to focus on the subject of this book. I just want
    to emphasize, especially for newcomers to data analysis, that even when not covered
    in this book, there are important skills that you should learn and practice to
    successfully work with data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们假设我们已经收集好了数据。我们的数据也将是干净且整洁的，而这在现实世界中是极少见的。我们作出这些假设是为了集中讨论本书的主题。我特别想强调，尤其是对于数据分析的新人来说，即使本书没有涉及，仍然有一些重要的技能需要你去学习和实践，以便能够成功地处理数据。
- en: A very useful skill when analyzing data is knowing how to write code in a programming
    language, such as Python. Manipulating data is usually necessary given that we
    live in a messy world with even messier data, and coding helps to get things done.
    Even if you are lucky and your data is very clean and tidy, coding will still
    be very useful since modern Bayesian statistics is done mostly through programming
    languages such as Python or R. If you want to learn how to use Python for cleaning
    and manipulating data, you can find a good introduction in *Python for Data Analysis*
    by [McKinney](Bibliography.xhtml#Xmckinney_2022) [[2022](Bibliography.xhtml#Xmckinney_2022)].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 分析数据时，一个非常有用的技能是知道如何在编程语言中编写代码，例如 Python。由于我们生活在一个杂乱的世界中，数据更加杂乱，因此操控数据通常是必要的，编程有助于完成任务。即使你很幸运，数据非常干净整洁，编程仍然非常有用，因为现代贝叶斯统计主要通过像
    Python 或 R 这样的编程语言进行。如果你想学习如何使用 Python 来清理和操控数据，可以参考 [McKinney](Bibliography.xhtml#Xmckinney_2022) 的
    *Python for Data Analysis* 一书 [[2022](Bibliography.xhtml#Xmckinney_2022)]。
- en: 1.3 Bayesian modeling
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 贝叶斯建模
- en: 'Models are simplified descriptions of a given system or process that, for some
    reason, we are interested in. Those descriptions are deliberately designed to
    capture only the most relevant aspects of the system and not to explain every
    minor detail. This is one reason a more complex model is not always a better one.
    There are many different kinds of models; in this book, we will restrict ourselves
    to Bayesian models. We can summarize the Bayesian modeling process using three
    steps:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是对某个系统或过程的简化描述，出于某些原因，我们对此系统或过程感兴趣。这些描述是刻意设计的，只捕捉系统中最相关的方面，而不解释每一个微小的细节。这也是为什么更复杂的模型不一定是更好的模型的原因之一。有许多不同种类的模型；在本书中，我们将只讨论贝叶斯模型。我们可以用三个步骤总结贝叶斯建模过程：
- en: Given some data and some assumptions on how this data could have been generated,
    we design a model by combining building blocks known as **probability distributions**.
    Most of the time these models are crude approximations, but most of the time that’s
    all we need.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一些数据和关于这些数据如何生成的假设，我们通过结合称为**概率分布**的构建模块来设计一个模型。大多数时候，这些模型是粗略的近似，但大多数情况下，这正是我们所需要的。
- en: We use Bayes’ theorem to add data to our models and derive the logical consequences
    of combining the data and our assumptions. We say we are **conditioning** the
    model on our data.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用贝叶斯定理将数据添加到我们的模型中，并推导出结合数据和假设的逻辑后果。我们说我们正在**对模型进行条件化**。
- en: We evaluate the model, and its predictions, under different criteria, including
    the data, our expertise on the subject, and sometimes by comparing it to other
    models.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们根据不同的标准评估模型及其预测，包括数据、我们对该主题的专业知识，有时还会通过与其他模型进行比较。
- en: 'In general, we will find ourselves performing these three steps in an iterative
    non-linear fashion. We will retrace our steps at any given point: maybe we made
    a silly coding mistake, or we found a way to change the model and improve it,
    or we realized that we need to add more data or collect a different kind of data.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们会发现自己在一个迭代的非线性方式中执行这三个步骤。我们会在任何时候重新追溯我们的步骤：也许我们犯了一个愚蠢的编码错误，或者我们找到了一种方法来改变模型并改进它，或者我们意识到需要添加更多的数据或收集不同种类的数据。
- en: Bayesian models are also known as **probabilistic models** because they are
    built using probabilities. Why probabilities? Because probabilities are a very
    useful tool to model uncertainty; we even have good arguments to state they are
    the correct mathematical concept. So let’s take a walk through *the garden of
    forking paths* [[Borges](Bibliography.xhtml#Xborges), [1944](Bibliography.xhtml#Xborges)].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯模型也被称为**概率模型**，因为它们是通过概率构建的。为什么是概率？因为概率是建模不确定性的非常有用的工具；我们甚至有充分的理由认为它们是正确的数学概念。所以让我们一起走进
    *叉路花园* [[Borges](Bibliography.xhtml#Xborges), [1944](Bibliography.xhtml#Xborges)]。
- en: 1.4 A probability primer for Bayesian practitioners
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 贝叶斯实践者的概率入门
- en: In this section, we are going to discuss a few general and important concepts
    that are key for better understanding Bayesian methods. Additional probability-related
    concepts will be introduced or elaborated on in future chapters, as we need them.
    For a detailed study of probability theory, however, I highly recommend the book
    *Introduction to Probability* by [Blitzstein](Bibliography.xhtml#Xblitzstein_2019) [[2019](Bibliography.xhtml#Xblitzstein_2019)].
    Those already familiar with the basic elements of probability theory can skip
    this section or skim it.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一些对于更好理解贝叶斯方法至关重要的通用概念和重要概念。未来的章节将根据需要介绍或详细说明其他与概率相关的概念。然而，对于概率论的详细学习，我强烈推荐[Blitzstein](Bibliography.xhtml#Xblitzstein_2019)的《*Introduction
    to Probability*》[[2019](Bibliography.xhtml#Xblitzstein_2019)]一书。已经熟悉概率论基本要素的读者可以跳过本节或快速浏览。
- en: 1.4.1 Sample space and events
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.1 样本空间与事件
- en: 'Let’s say we are surveying to see how people feel about the weather in their
    area. We asked three individuals whether they enjoy sunny weather, with possible
    responses being “yes” or “no.” The sample space of all possible outcomes can be
    denoted by *S* and consists of eight possible combinations:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在调查人们对自己所在地区天气的看法。我们问了三个人是否喜欢晴天，可能的回答是“是”或“否”。所有可能结果的样本空间可以用*S*表示，并包含八种可能的组合：
- en: '*S* = {(yes, yes, yes), (yes, yes, no), (yes, no, yes), (no, yes, yes), (yes,
    no, no), (no, yes, no), (no, no, yes), (no, no, no)}'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*S* = {(是, 是, 是), (是, 是, 否), (是, 否, 是), (否, 是, 是), (是, 否, 否), (否, 是, 否), (否,
    否, 是), (否, 否, 否)}'
- en: Here, each element of the sample space represents the responses of the three
    individuals in the order they were asked. For example, (yes, no, yes) means the
    first and third people answered “yes” while the second person answered “no.”
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，样本空间中的每个元素代表三个人根据被问到的顺序所做出的回答。例如，(是, 否, 是)表示第一和第三个人回答了“是”，而第二个人回答了“否”。
- en: 'We can define events as subsets of the sample space. For example, event *A*
    is when all three individuals answered “yes”:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将事件定义为样本空间的子集。例如，事件*A*就是所有三个人都回答“是”时发生的事件：
- en: '*A* = {(yes, yes, yes)}'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*A* = {(是, 是, 是)}'
- en: 'Similarly, we can define event *B* as when at least one person answered “no,”
    and then we will have:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以定义事件*B*为至少有一个人回答“否”的情况，然后我们将得到：
- en: '*B* = {(yes, yes, no), (yes, no, yes), (no, yes, yes), (yes, no, no), (no,
    yes, no), (no, no, yes), (no, no, no)}'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*B* = {(是, 是, 否), (是, 否, 是), (否, 是, 是), (是, 否, 否), (否, 是, 否), (否, 否, 是), (否,
    否, 否)}'
- en: 'We can use probabilities as a measure of how likely these events are. Assuming
    all events are equally likely, the probability of event *A*, which is the event
    that all three individuals answered “yes,” is:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用概率来衡量这些事件发生的可能性。假设所有事件发生的概率相等，那么事件*A*的概率，即所有三个人都回答“是”的事件概率为：
- en: '![ number of outcomes in A P (A) = ---------------------------- total number
    of outcomes in S ](img/file4.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![ A 事件的结果数 P (A) = ---------------------------- 样本空间 S 的总结果数 ](img/file4.jpg)'
- en: 'In this case, there is only one outcome in *A*, and there are eight outcomes
    in *S*. Therefore, the probability of *A* is:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，*A*中只有一个结果，而*S*中有八个结果。因此，*A*的概率为：
- en: '![ 1 P(A ) = 8 = 0.125 ](img/file5.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![ 1 P(A ) = 8 = 0.125 ](img/file5.jpg)'
- en: 'Similarly, we can calculate the probability of event *B*, which is the event
    that at least one person answered “no.” Since there are seven outcomes in *B*
    and eight outcomes in *S*, the probability of *B* is:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以计算事件*B*的概率，这个事件表示至少有一个人回答“否”。由于*B*中有七个结果，而*S*中有八个结果，事件*B*的概率为：
- en: '![P(B ) = 7-= 0.875 8 ](img/file6.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![P(B ) = 7-= 0.875 8 ](img/file6.jpg)'
- en: 'Considering all events equally likely is just a particular case that makes
    calculating probabilities easier. This is something called the naive definition
    of probability since it is restrictive and relies on strong assumptions. However,
    it is still useful if we are cautious when using it. For instance, it is not true
    that all yes-no questions have a 50-50 chance. Another example. What is the probability
    of seeing a purple horse? The right answer can vary a lot depending on whether
    we’re talking about the natural color of a real horse, a horse from a cartoon,
    a horse dressed in a parade, etc. Anyway, no matter if the events are equally
    likely or not, the probability of the entire sample space is always equal to 1\.
    We can see that this is true by computing:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有事件视为同样可能的事件只是一个特殊情况，它使得计算概率更为简便。这被称为朴素的概率定义，因为它具有局限性并依赖于强假设。然而，如果我们谨慎使用，它仍然是有用的。例如，并不是所有的“是-否”问题都有50-50的概率。再举个例子，看到一匹紫色的马的概率是多少？正确答案可以有很大的不同，具体取决于我们是在谈论一匹真实马的自然颜色、卡通中的马、一匹穿着游行服装的马，等等。无论事件是否等可能，整个样本空间的概率总是等于1。我们可以通过计算来验证这一点：
- en: '![ number of outcomes in S P (S) = ---------------------------- total number
    of outcomes in S ](img/file7.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![ 结果数目 S P (S) = ---------------------------- S 中结果的总数 ](img/file7.jpg)'
- en: 1 is the highest value a probability can take. Saying that *P*(*S*) = 1 is saying
    that *S* is not only very likely, it is certain. If everything that can happen
    is defined by *S*, then *S* will happen.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 1是概率能达到的最高值。说*P*(*S*) = 1就意味着*S*不仅非常可能，它是确定的。如果*S*定义了所有可能发生的事情，那么*S*一定会发生。
- en: 'If an event is impossible, then its probability is 0\. Let’s define the event
    *C* as the event of three persons saying “banana”:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个事件是不可能的，那么它的概率就是0。我们定义事件*C*为三个人都说“香蕉”的事件：
- en: '*C* = {(banana, banana, banana)}'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*C* = {(香蕉, 香蕉, 香蕉)}'
- en: As *C* is not part of *S*, by definition, it cannot happen. Think of this as
    the questionnaire from our survey only having two boxes, *yes* and *no*. By design,
    our survey is restricting all other possible options.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*C*不是*S*的一部分，根据定义，它是无法发生的。可以把它看作是我们的调查问卷只有两个选项，*yes*和*no*。根据设计，我们的调查限制了所有其他可能的选项。
- en: 'We can take advantage of the fact that Python includes sets and define a Python
    function to compute probabilities following their naive definition:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用Python包含集合的事实，并定义一个Python函数来按照它们的朴素定义计算概率：
- en: '**Code 1.1**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 1.1**'
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: I left for the reader the joy of playing with this function.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我把玩这个函数的乐趣留给读者了。
- en: One useful way to conceptualize probabilities is as conserved quantities distributed
    throughout the sample space. This means that if the probability of one event increases,
    the probability of some other event or events must decrease so that the total
    probability remains equal to 1\. This can be illustrated with a simple example.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一种有用的理解概率的方式是将概率视为分布在样本空间中的守恒量。这意味着如果一个事件的概率增加，其他一些事件的概率必须减少，以便总概率保持为1。可以通过一个简单的例子来说明这一点。
- en: Suppose we ask one person whether it will rain tomorrow, with possible responses
    of “yes” and “no.” The sample space for possible responses is given by *S* = {yes,
    no}. An event that will rain tomorrow is represented by *A* = {yes}. If *P*(*A*),
    is 0.5, then the probability of the complement of event *A*, denoted by *P*![(Ac)](img/file8.jpg),
    must also be 0.5\. If for some reason *P*(*A*) increases to 0.8, then *P*![ c
    (A )](img/file9.jpg) must decrease to 0.2\. This property holds for disjoint events,
    which are events that cannot occur simultaneously. For instance, it cannot *rain*
    and *not rain* at the same time tomorrow. You may object that it can rain during
    the morning and not rain during the afternoon. That is true, but that’s a different
    sample space!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们问某人明天是否会下雨，可能的回答是“是”或“否”。可能的回答的样本空间是*S* = {是, 否}。表示明天会下雨的事件是*A* = {是}。如果*P*(*A*)是0.5，那么事件*A*的补集事件的概率，即*P*![(Ac)](img/file8.jpg)，也必须是0.5。如果由于某种原因*P*(*A*)增加到0.8，那么*P*![
    c (A )](img/file9.jpg)必须减少到0.2。这个特性适用于互斥事件，即不能同时发生的事件。例如，明天不可能同时“下雨”和“不下雨”。你可能会反驳，早上可能下雨，下午不下雨。没错，但那是不同的样本空间！
- en: 'So far, we have avoided directly defining probabilities, and instead, we have
    just shown some of their properties and ways to compute them. A general definition
    of probability that works for non-equally likely events is as follows. Given a
    sample space *S*, and the event *A*, which is a subset of *S*, a probability is
    a function *P*, which takes *A* as input and returns a real number between 0 and
    1, as output. The function *P* has some restrictions, defined by the following
    3 axioms. Keep in mind that an axiom is a statement that is taken to be true and
    that we use as the starting point in our reasoning:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们避免了直接定义概率，而是展示了一些概率的性质以及计算方法。适用于非等可能事件的概率的一个一般定义如下。给定一个样本空间*S*，以及事件*A*，它是*S*的一个子集，概率是一个函数*P*，它以*A*为输入，返回一个介于0和1之间的实数作为输出。函数*P*有一些限制，这些限制由以下三个公理定义。请记住，公理是被认为为真的陈述，我们用它作为推理的起点：
- en: The probability of an event is a non-negative real number
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 事件的概率是一个非负实数。
- en: '*P*(*S*) = 1'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*P*(*S*) = 1'
- en: If *A*1*,A*2*,…* are disjoint events, meaning they cannot occur simultaneously
    then *P*(*A*1*,A*2*,…*) = *P*(*A*1) + *P*(*A*2) + *…*
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果*A*1*, A*2*, …*是互斥事件，意味着它们不能同时发生，那么*P*(*A*1*, A*2*, …*) = *P*(*A*1) + *P*(*A*2)
    + *…*
- en: If this were a book on probability theory, we would likely dedicate a few pages
    to demonstrating the consequences of these axioms and provide exercises for manipulating
    probabilities. That would help us to become proficient in manipulating probabilities.
    However, our main focus is not on those topics. My motivation to present these
    axioms is just to show that probabilities are well-defined mathematical concepts
    with rules that govern their operations. They are a particular type of function,
    and there is no mystery surrounding them.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是一本关于概率论的书，我们可能会专门用几页来展示这些公理的后果，并提供一些练习来操作概率。这将帮助我们熟练地操作概率。然而，我们的主要关注点不在这些话题上。我展示这些公理的动机仅仅是为了说明概率是一个定义明确的数学概念，并且有规则来支配它们的运算。它们是特定类型的函数，并且并不神秘。
- en: 1.4.2 Random variables
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.2 随机变量
- en: A random variable is a function that maps the sample space into the real numbers
    ℝ (see *Figure [1.1](#x1-22003r1)*). Let’s assume the events of interest are the
    number of a die, the mapping is very simple, we associate ![PIC](img/dice_1.png)
    with the number 1, ![PIC](img/dice_2.png) with 2, etc. Another simple example
    is the answer to the question, will it rain tomorrow? We can map “yes” to 1 and
    “no” to 0\. It is common, but not always the case, to use a capital letter for
    random variables like *X* and a lowercase letter for their outcomes *x*. For example,
    if *X* represents a single roll of a die, then *x* represents some specific integer
    {1*,*2*,*3*,*4*,*5*,*6}. Thus, we can write *P*(*X* = 3) to indicate the probability
    of getting the value 3, when rolling a die. We can also leave *x* unspecified,
    for instance, we can write *P*(*X* = *x*) to indicate the probability of getting
    some value *x*, or *P*(*X* ≤ *x*), to indicate the probability of getting a value
    less than or equal to *x*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量是一个将样本空间映射到实数ℝ的函数（见*图 [1.1](#x1-22003r1)*）。假设我们关注的事件是骰子的点数，映射非常简单，我们将![PIC](img/dice_1.png)与数字1关联，![PIC](img/dice_2.png)与2，依此类推。另一个简单的例子是回答问题“明天会下雨吗？”，我们可以将“是”映射为1，将“否”映射为0。通常，随机变量使用大写字母表示，如*X*，而其结果使用小写字母表示，如*x*。例如，如果*X*表示一次骰子投掷，那么*x*表示某个特定的整数{1,*2*,*3*,*4*,*5*,*6*}。因此，我们可以写*P*(*X*
    = 3)来表示投掷骰子得到3的概率。我们也可以不指定*x*，例如，我们可以写*P*(*X* = *x*)来表示得到某个值*x*的概率，或者写*P*(*X*
    ≤ *x*)，表示得到小于或等于*x*的概率。
- en: Being able to map symbols like ![PIC](img/dice_1.png) or strings like “yes”
    to numbers makes analysis simpler as we already know how to do math with numbers.
    Random variables are also useful because we can operate with them without directly
    thinking in terms of the sample space. This feature becomes more and more relevant
    as the sample space becomes more complex. For example, when simulating molecular
    systems, we need to specify the position and velocity of each atom; for complex
    molecules like proteins this means that we will need to track thousands, millions,
    or even larger numbers. Instead, we can use random variables to summarize certain
    properties of the system, such as the total energy or the relative angles between
    certain atoms of the system.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 能够将符号如![PIC](img/dice_1.png)或字符串如“yes”映射到数字上，使得分析变得更加简单，因为我们已经知道如何用数字进行数学运算。随机变量也很有用，因为我们可以在不直接考虑样本空间的情况下对它们进行操作。随着样本空间变得越来越复杂，这一特点变得愈加重要。例如，在模拟分子系统时，我们需要指定每个原子的位置信息和速度；对于像蛋白质这样复杂的分子，这意味着我们需要追踪成千上万甚至更多的数字。相反，我们可以使用随机变量来总结系统的某些属性，比如总能量或系统中某些原子之间的相对角度。
- en: If you are still confused, that’s fine. The concept of a random variable may
    sound too abstract at the beginning, but we will see plenty of examples throughout
    the book that will help you cement these ideas. Before moving on, let me try one
    analogy that I hope you find useful. Random variables are useful in a similar
    way to how Python functions are useful. We often encapsulate code within functions,
    so we can store, reuse, and *hide* complex manipulations of data into a single
    call. Even more, once we have a few functions, we can sometimes combine them in
    many ways, like adding the output of two functions or using the output of one
    function as the input of the other. We can do all this without functions, but
    abstracting away the inner workings not only makes the code cleaner, it also helps
    with understanding and fostering new ideas. Random variables play a similar role
    in statistics.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仍然感到困惑，那也没关系。随机变量的概念刚开始可能显得过于抽象，但我们将在全书中看到许多例子，帮助你巩固这些概念。在继续之前，我想举一个类比，希望对你有帮助。随机变量的作用类似于Python函数的作用。我们通常将代码封装在函数中，这样就可以将复杂的数据操作存储、重用，并通过一次调用来*隐藏*。更进一步，当我们拥有多个函数时，有时可以通过多种方式组合它们，比如将两个函数的输出相加，或将一个函数的输出作为另一个函数的输入。我们也可以在没有函数的情况下完成这些操作，但将内部工作抽象化不仅让代码更简洁，还帮助理解和激发新的创意。随机变量在统计学中起着类似的作用。
- en: '![PIC](img/file10.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file10.png)'
- en: '**Figure 1.1**: A random variable *X* defined on a sample space with 5 elements
    {*S*[1]*,*![⋅⋅⋅](img/file11.jpg)*S*[5]}, and possible values -1, 2, and *π*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.1**：在一个包含5个元素的样本空间上定义的随机变量*X*，其中元素包括{*S*[1]*，*![⋅⋅⋅](img/file11.jpg)*S*[5]}，其可能的值为-1、2
    和 *π*。'
- en: The mapping between the sample space and ℝ is deterministic. There is no randomness
    involved. So why do we call it a *random* variable? Because we can *ask* the variable
    for values, and every time we ask, we will get a different number. The randomness
    comes from the probability associated with the events. In *Figure [1.1](#x1-22003r1)*,
    we have represented *P* as the size of the circles.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 样本空间与ℝ之间的映射是确定性的。这里没有涉及随机性。那么，为什么我们称其为*随机*变量呢？因为我们可以*请求*该变量的值，每次请求时，得到的数字都会不同。随机性来源于与事件相关的概率。在*图
    [1.1](#x1-22003r1)*中，我们通过圆圈的大小表示了*P*。
- en: The two most common types of random variables are discrete and continuous ones.
    Without going into a proper definition, we are going to say that discrete variables
    take only discrete values and we usually use integers to represent them, like
    1, 5, 42\. And continuous variables take real values, so we use floats to work
    with them, like 3.1415, 1.01, 23.4214, and so on. When we use one or the other
    is problem-dependent. If we ask people about their favorite color, we will get
    answers like “red,” “blue,” and “green.” This is an example of a discrete random
    variable. The answers are categories – there are no intermediate values between
    “red” and “green.” But if we are studying the properties of light absorption,
    then discrete values like “red” and “green” may not be adequate and instead working
    with wavelength could be more appropriate. In that case, we will expect to get
    values like 650 nm and 510 nm and any number in between, including 579.1\.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 两种最常见的随机变量类型是离散型和连续型。虽然不做正式定义，但我们可以说离散型变量只取离散值，通常使用整数表示，例如 1、5、42。连续型变量则取实数值，因此我们使用浮点数来表示它们，例如
    3.1415、1.01、23.4214 等等。我们使用哪种类型取决于具体问题。如果我们询问人们最喜欢的颜色，答案可能是“红色”、“蓝色”和“绿色”。这是一个离散随机变量的例子。答案是类别间的——“红色”和“绿色”之间没有中间值。但如果我们研究光的吸收特性，那么像“红色”和“绿色”这样的离散值可能不够准确，转而使用波长可能更为合适。在这种情况下，我们会得到类似
    650 纳米和 510 纳米的值，并且任何中间值也都可能出现，包括 579.1 纳米。
- en: 1.4.3 Discrete random variables and their distributions
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.3 离散随机变量及其分布
- en: Instead of calculating the probability that all three individuals answered “yes,”
    or the probability of getting a 3 when rolling a die, we may be more interested
    in finding out the *list of probabilities* for all possible answers or all possible
    numbers from a die. Once this list is computed, we can inspect it visually or
    use it to compute other quantities like the probability of getting at least one
    “no,” the probability of getting an odd number, or the probability of getting
    a number equal to or larger than 5\. The formal name of this *list* is **probability**
    **distribution**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能不仅仅想计算所有三个人都回答“是”的概率，或者掷骰子得到3的概率，我们可能更感兴趣的是找到所有可能答案或骰子上所有可能数字的*概率列表*。一旦这个列表计算出来，我们可以通过可视化查看它，或者利用它来计算其他量，比如至少得到一个“否”的概率、得到奇数的概率，或者得到大于或等于5的数字的概率。这个*列表*的正式名称是**概率**
    **分布**。
- en: We can get the empirical probability distribution of a die, by rolling it a
    few times and tabulating how many times we got each number. To turn each value
    into a probability and the entire list into a valid probability distribution,
    we need to *normalize* the counts. We can do this by dividing the value we got
    for each number by the number of times we roll the die.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过掷骰子几次并记录每个数字出现的次数来获得骰子的经验概率分布。为了将每个值转化为概率，并使整个列表成为有效的概率分布，我们需要*归一化*这些计数。我们可以通过将每个数字出现的次数除以掷骰子的次数来实现这一点。
- en: Empirical distributions are very useful, and we are going to extensively use
    them. But instead of rolling dice by hand, we are going to use advanced computational
    methods to do the hard work for us; this will not only save us time and boredom
    but it will allow us to get samples from really complicated distributions effortlessly.
    But we are getting ahead of ourselves. Our priority is to concentrate on theoretical
    distributions, which are central in statistics because, among other reasons, they
    allow the construction of probabilistic models.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 经验分布非常有用，我们将广泛使用它们。但我们不会再手动掷骰子，而是将使用先进的计算方法来为我们完成这项繁重的工作；这不仅能节省我们的时间和避免无聊，还能让我们轻松地从非常复杂的分布中获取样本。不过我们现在有点急于求成。我们的优先任务是集中精力研究理论分布，因为它们在统计学中占据核心地位，原因之一是它们能够构建概率模型。
- en: 'As we saw, there is nothing random or mysterious about random variables; they
    are just a type of mathematical function. The same goes for theoretical probability
    distributions. I like to compare probability distributions with circles. Because
    we are all familiar with circles even before we get into school, we are not afraid
    of them and they don’t look mysterious to us. We can define a circle as the geometric
    space of points on a plane that is equidistant from another point called the center.
    We can go further and provide a mathematical expression for this definition. If
    we assume the location of the center is irrelevant, then the circle of radius
    *r* can simply be described as the set of all points (*x,y*) such that:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，随机变量并没有什么随机或神秘之处；它们只是数学函数的一种类型。理论概率分布也是如此。我喜欢将概率分布与圆形进行比较。因为我们在上学之前就已经熟悉圆形了，所以我们对它们不感到害怕，它们也不会让我们觉得神秘。我们可以将圆定义为平面上与另一个点（称为圆心）等距的所有点的几何空间。我们可以进一步给出这个定义的数学表达式。如果我们假设圆心的位置无关紧要，那么半径为*r*的圆可以简单地描述为所有满足以下条件的点集（*x,y*）：
- en: '![x2 + y2 = r2 ](img/file12.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![x2 + y2 = r2 ](img/file12.jpg)'
- en: From this expression, we can see that given the **parameter** *r*, the circle
    is completely defined. This is all we need to plot it and all we need to compute
    properties such as the perimeter, which is 2*πr*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个表达式中，我们可以看到，给定**参数** *r*，圆就被完全定义了。这就是我们绘制它所需的所有信息，也是我们计算如周长（2*πr*）等性质所需的所有信息。
- en: Now notice that all circles look very similar to each other and that any two
    circles with the same value of *r* are essentially the same objects. Thus we can
    think of the family of circles, where each member is set apart from the rest precisely
    by the value of *r*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在请注意，所有的圆看起来都非常相似，并且任何两个半径相同的圆基本上是相同的对象。因此，我们可以把圆的家族看作是其中每个成员都恰好通过半径 *r* 的值与其他成员区分开来的。
- en: So far, so good, but why are we talking about circles? Because all this can
    be directly applied to probability distributions. Both circles and probability
    distributions have mathematical expressions that define them, and these expressions
    have parameters that we can change to define all members of a family of probability
    distributions. *Figure [1.2](#x1-23006r2)* shows four members of one probability
    distribution known as BetaBinomial. In *Figure [1.2](#x1-23006r2)*, the height
    of the bars represents the probability of each *x* value. The values of *x* below
    1 or above 6 have a probability of 0 as they are out of the support of the distribution.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利，但我们为什么要谈论圆呢？因为这一切都可以直接应用于概率分布。圆和概率分布都有定义它们的数学表达式，这些表达式有我们可以改变的参数，用以定义概率分布家族中的所有成员。*图
    [1.2](#x1-23006r2)* 显示了一个名为BetaBinomial的概率分布的四个成员。在*图 [1.2](#x1-23006r2)*中，条形的高度表示每个
    *x* 值的概率。低于1或高于6的 *x* 值的概率为0，因为它们超出了分布的支持范围。
- en: '![PIC](img/file13.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file13.png)'
- en: '**Figure 1.2**: Four members of the BetaBinomial distribution with parameters
    *α* and *β*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.2**：具有参数 *α* 和 *β* 的BetaBinomial分布的四个成员'
- en: 'This is the mathematical expression for the BetaBinomial distribution:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是BetaBinomial分布的数学表达式：
- en: '![ ( ) pmf (x ) = n- B-(x-+-𝛼,n-−-x-+-𝛽)- x B(𝛼,𝛽 ) ](img/file14.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![ ( ) pmf (x ) = n- B-(x-+-𝛼,n-−-x-+-𝛽)- x B(𝛼,𝛽 ) ](img/file14.jpg)'
- en: pmf stands for **probability mass function**. For discrete random variables,
    the pmf is the function that returns probabilities. In mathematical notation,
    if we have a random variable *X*, then pmf(*x*) = *P*(*X* = *x*).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: pmf代表**概率质量函数**。对于离散随机变量，pmf是返回概率的函数。在数学符号中，如果我们有一个随机变量 *X*，那么pmf(*x*) = *P*(*X*
    = *x*)。
- en: Understanding or remembering the pmf of the BetaBinomial has zero importance
    for us. I’m just showing it here so you can see that this is just another function;
    you put in one number and you get out another number. Nothing weird, at least
    not in principle. I must concede that to fully understand the details of the BetaBinomial
    distribution, we need to know what ![( ) nx](img/file15.jpg) is, known as the
    binomial coefficient, and what *B* is, the Beta function. But that’s not fundamentally
    different from showing *x*² + *y*² = *r*².
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 理解或记住BetaBinomial的pmf对我们来说并不重要。我只是展示它，以便你能看到这仅仅是另一个函数；你输入一个数字，输出另一个数字。没什么奇怪的，至少原则上是这样。我必须承认，要完全理解BetaBinomial分布的细节，我们需要知道什么是
    ![( ) nx](img/file15.jpg)，即二项系数，以及*B*是什么，即Beta函数。但这与展示 *x*² + *y*² = *r*²并没有根本区别。
- en: 'Mathematical expressions can be super useful, as they are concise and we can
    use them to derive properties from them. But sometimes that can be too much work,
    even if we are good at math. Visualization can be a good alternative (or complement)
    to help us understand probability distributions. I cannot fully show this on paper,
    but if you run the following, you will get an interactive plot that will update
    every time you move the sliders for the parameters `alpha`, `beta`, and `n`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 数学表达式非常有用，因为它们简洁，我们可以利用它们推导出性质。但有时这可能会太复杂，即使我们擅长数学。可视化可能是一个很好的替代（或补充），帮助我们理解概率分布。我无法在纸面上完全展示，但如果你运行以下代码，你将获得一个交互式图，每次调整`alpha`、`beta`和`n`的滑块时都会更新：
- en: '**Code 1.2**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 1.2**'
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Figure [1.3](#x1-23013r3)* shows a static version of this interactive plot.
    The black dots represent the probabilities for each value of the random variable,
    while the dotted black line is just a visual aid.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [1.3](#x1-23013r3)*展示了该交互式图的静态版本。黑色的点代表每个随机变量值的概率，而虚线黑色线条仅作为视觉辅助线。'
- en: '![PIC](img/file16.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file16.png)'
- en: '**Figure 1.3**: The output of `pz.BetaBinomial(alpha=10, beta=10, n=6).plot_interactive()`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.3**：`pz.BetaBinomial(alpha=10, beta=10, n=6).plot_interactive()`的输出'
- en: On the x-axis, we have the support of the BetaBinomial distribution, i.e., the
    values it can take, *x* ∈{0*,*1*,*2*,*3*,*4*,*5}. On the y-axis, the probabilities
    associated with each of those values. The full list is shown in *Table [1.1](#x1-23014r1)*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在x轴上，我们有BetaBinomial分布的支持，即它可以取的值，*x* ∈{0*,*1*,*2*,*3*,*4*,*5}。在y轴上，与这些值相关的概率。完整列表请参见*表格
    [1.1](#x1-23014r1)*。
- en: '| **x value** | **probability** |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| **x 值** | **概率** |'
- en: '| 0 | 0.047 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.047 |'
- en: '| 1 | 0.168 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.168 |'
- en: '| 2 | 0.285 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.285 |'
- en: '| 3 | 0.285 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.285 |'
- en: '| 4 | 0.168 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.168 |'
- en: '| 5 | 0.047 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.047 |'
- en: '**Table 1.1**: Probabilities for `pz.BetaBinomial(alpha=10, beta=10, n=6)`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**表格 1.1**：`pz.BetaBinomial(alpha=10, beta=10, n=6)`的概率'
- en: Notice that for a `BetaBinomial(alpha=10, beta=10, n=6)` distribution, the probability
    of values not in {0*,*1*,*2*,*3*,*4*,*5}, including values such as −1*,*0*.*5*,π,*42,
    is 0.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于`BetaBinomial(alpha=10, beta=10, n=6)`分布，{0*,*1*,*2*,*3*,*4*,*5}之外的值（例如
    −1*,*0*.*5*,π,*42）的概率为0。
- en: 'We previously mentioned that we can *ask* a random variable for values and
    every time we ask, we will get a different number. We can simulate this with PreliZ
    [[Icazatti et al.](Bibliography.xhtml#Xicazatti2023), [2023](Bibliography.xhtml#Xicazatti2023)],
    a Python library for prior elicitation. Take the following code snippet for instance:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，我们可以*询问*随机变量获取值，每次询问时，我们都会得到不同的数字。我们可以用PreliZ [[Icazatti et al.](Bibliography.xhtml#Xicazatti2023), [2023](Bibliography.xhtml#Xicazatti2023)]来模拟这一过程，PreliZ是一个用于先验引导的Python库。例如，考虑以下代码片段：
- en: '**Code 1.3**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 1.3**'
- en: '[PRE2]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will give us an integer between 0 and 5\. Which one? We don’t know! But
    let’s run the following code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们一个0到5之间的整数。是哪一个？我们不知道！但让我们运行以下代码：
- en: '**Code 1.4**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 1.4**'
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will get something similar to *Figure [1.4](#x1-23026r4)*. Even when we cannot
    predict the next value from a random variable, we can predict the probability
    of getting any particular value and by the same token, if we get many values,
    we can predict their overall distribution.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到类似于*图 [1.4](#x1-23026r4)*的结果。即使我们无法从随机变量中预测下一个值，我们也可以预测获得任何特定值的概率，反过来，如果我们获取了许多值，我们也可以预测它们的整体分布。
- en: '![PIC](img/file17.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file17.png)'
- en: '**Figure 1.4**: The gray dots represent the pmf of the BetaBinomial sample.
    In light gray, a histogram of 1,000 draws from that distribution'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.4**：灰色的点表示BetaBinomial样本的概率质量函数（pmf）。浅灰色为从该分布中抽取的1,000次样本的直方图。'
- en: 'In this book, we will sometimes know the parameters of a given distribution
    and we will want to get random samples from it. Other times, we are going to be
    in the opposite scenario: we will have a set of samples and we will want to estimate
    the parameters of a distribution. Playing back and forth between these two scenarios
    will become second nature as we move forward through the pages.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们有时会知道给定分布的参数，并希望从中获取随机样本。其他时候，我们将遇到相反的情况：我们会有一组样本，并希望估计该分布的参数。在这两种情境之间的来回切换将随着我们深入书中的内容而变得得心应手。
- en: 1.4.4 Continuous random variables and their distributions
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.4 连续随机变量及其分布
- en: 'Probably the most widely known continuous probability distribution is the **Normal
    distribution**, also known as the **Gaussian distribution**. Its **probability
    density function** is:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最广为人知的连续概率分布是**正态分布**，也称为**高斯分布**。其**概率密度函数**为：
- en: '![ { } 1 1( x − μ)2 pdf(x) = -√----exp − -- ----- σ 2π 2 σ ](img/file18.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![ { } 1 1( x − μ)2 pdf(x) = -√----exp − -- ----- σ 2π 2 σ ](img/file18.jpg)'
- en: 'Again, we only show this expression to remove the mystery veil. No need to
    pay too much attention to its details, other than to the fact that this distribution
    has two parameters *μ*, which controls the location of the peak of the curve,
    and *σ*, which controls the spread of the curve. *Figure [1.5](#x1-24006r5)* shows
    3 examples from the Gaussian family. If you want to learn more about this distribution,
    I recommend you watch this video: [https://www.youtube.com/watch?v=cy8r7WSuT1I](https://www.youtube.com/watch?v=cy8r7WSuT1I).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们仅展示这个表达式是为了揭开其中的神秘面纱。无需过多关注其细节，除了该分布有两个参数 *μ*，它控制曲线的峰值位置，和 *σ*，它控制曲线的扩展度以外。*Figure
    [1.5](#x1-24006r5)* 展示了来自高斯家族的 3 个示例。如果你想深入了解这个分布，我建议你观看这个视频：[https://www.youtube.com/watch?v=cy8r7WSuT1I](https://www.youtube.com/watch?v=cy8r7WSuT1I)。
- en: '![PIC](img/file19.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file19.png)'
- en: '**Figure 1.5**: Three members of the Gaussian family'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**Figure 1.5**：高斯家族的三个成员'
- en: If you have been paying attention, you may have noticed that we said **probability
    density function** (**pdf**) instead of **probability mass** **function** (**pmf**).
    This was no typo – they are actually two different objects. Let’s take one step
    back and think about this; the output of a discrete probability distribution is
    a probability. The height of the bars in *Figure [1.2](#x1-23006r2)* or the height
    of the dots in *Figure [1.3](#x1-23013r3)* are probabilities. Each bar or dot
    will never be higher than 1 and if you sum all the bars or dots, you will always
    get 1\. Let’s do the same but with the curve in *Figure [1.5](#x1-24006r5)*. The
    first thing to notice is that we don’t have bars or dots; we have a continuous,
    smooth curve. So maybe we can think that the curve is made up of super thin bars,
    so thin that we assign one bar for every real value in the support of the distributions,
    we measure the height of each bar, and we perform an infinite sum. This is a sensible
    thing to do, right?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直在关注，你可能会注意到我们使用了**概率密度函数**（**pdf**）而不是**概率质量函数**（**pmf**）。这不是笔误——它们实际上是两个不同的概念。我们先退后一步，思考一下：离散概率分布的输出是概率。*Figure
    [1.2](#x1-23006r2)* 中的条形高度或 *Figure [1.3](#x1-23013r3)* 中的点的高度就是概率。每个条形或点的高度永远不会超过
    1，并且如果你将所有的条形或点相加，你总会得到 1。让我们做同样的事情，但换成 *Figure [1.5](#x1-24006r5)* 中的曲线。首先需要注意的是，我们没有条形或点；我们有一个连续、平滑的曲线。所以，也许我们可以认为这个曲线是由非常细的条形组成的，这些条形细得我们为分布支持中的每一个实数值分配一条条形，测量每条条形的高度，然后进行无限求和。这是合理的吗？
- en: Well yes, but it is not immediately obvious what are we going to get from this.
    Will this sum give us exactly 1? Or are we going to get a large number instead?
    Is the sum finite? Or does the result depend on the parameters of the distribution?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，但我们从中获得的结果并不立刻显现出来。这个求和会得到精确的 1 吗？还是会得到一个较大的数字呢？这个求和是有限的吗？结果是否依赖于分布的参数？
- en: 'A proper answer to these questions requires measure theory, and this is a very
    informal introduction to probability, so we are not going into that rabbit hole.
    But the answer essentially is that for a continuous random variable, we can only
    assign a probability of 0 to every individual value it may take; instead, we can
    assign densities to them and then we can calculate probabilities for a range of
    values. Thus, for a Gaussian, the probability of getting exactly the number -2,
    i.e. the number -2 followed by an infinite number of zeros after the decimal point,
    is 0\. But the probability of getting a number between -2 and 0 is some number
    larger than 0 and smaller than 1\. To find out the exact answer, we need to compute
    the following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要正确回答这些问题需要测度理论，而这只是一个非常非正式的概率学入门，因此我们不会深入探讨这个问题。但本质上，答案是，对于连续随机变量，我们只能为它可能取的每个单独值分配
    0 的概率；相反，我们可以为它们分配密度值，然后我们可以计算一个值范围内的概率。因此，对于高斯分布，得到精确数值 -2 的概率，即 -2 后面跟着无限多个零的小数部分，概率为
    0。但是得到介于 -2 和 0 之间的数字的概率是一个大于 0 且小于 1 的数。为了找出准确答案，我们需要计算以下内容：
- en: '![ ∫ b P(a < X < b) = pdf(x)dx a ](img/file20.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![ ∫ b P(a < X < b) = pdf(x)dx a ](img/file20.jpg)'
- en: And to compute that, we need to replace the symbols for a concrete quantity.
    If we replace the pdf by Normal(0*,*1), and *a* = −2, *b* = 0, we will get that
    *P*(−2 *< X <* 0) ≈ 0*.*477, which is the shaded area in *Figure [1.6](#x1-24010r6)*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算这个，我们需要用具体的数量来替代符号。如果我们将 pdf 替换为 Normal(0*,*1)，并且 *a* = −2，*b* = 0，我们将得到
    *P*（−2 *< X <* 0）≈ 0*.*477，这就是 *Figure [1.6](#x1-24010r6)* 中的阴影区域。
- en: '![PIC](img/file21.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file21.png)'
- en: '**Figure 1.6**: The black line represents the pdf of a Gaussian with parameters
    mu=0 and sigma=1, the gray area is the probability of a value being larger than
    -2 and smaller than 0'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.6**：黑线表示参数为 mu=0 和 sigma=1 的高斯分布的 pdf，灰色区域表示一个值大于 -2 且小于 0 的概率'
- en: 'You may remember that we can approximate an integral by summing areas of rectangles
    and the approximation becomes more and more accurate as we reduce the length of
    the base of the rectangles (see the Wikipedia entry for [Riemann integral](Bibliography.xhtml#Xwikipedia_riemann_2023)).
    Based on this idea and using PreliZ, we can estimate *P*(−2 *< X <* 0) as:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，我们可以通过求矩形面积的和来近似一个积分，随着矩形底边长度的缩小，这种近似会变得越来越准确（请参阅维基百科上的 [Riemann 积分](Bibliography.xhtml#Xwikipedia_riemann_2023)
    条目）。基于这个思路，并使用 PreliZ，我们可以估算 *P*(−2 *< X <* 0) 为：
- en: '**Code 1.5**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 1.5**'
- en: '[PRE4]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If we increase the value of `num`, we will get a better approximation.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们增加 `num` 的值，结果会得到更精确的近似值。
- en: 1.4.5 Cumulative distribution function
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.5 累积分布函数
- en: 'We have seen the pmf and the pdf, but these are not the only ways to characterize
    distributions. An alternative is the **cumulative distribution** **function**
    (**cdf**). The cdf of a random variable *X* is the function *F*[*X*] given by
    *F*[*X*](*x*) = *P*(*X* ≤ *x*). In words, the cdf is the answer to the question:
    what is the probability of getting a number lower than or equal to *x*? On the
    first column of *Figure [1.7](#x1-25003r7)*, we can see the pmf and cdf of a BetaBinomial,
    and in the second column, the pdf and cdf of a Gaussian. Notice how the cdf *jumps*
    for the discrete variable but it is smooth for the continuous variable. The height
    of each jump represents a probability – just compare them with the height of the
    dots. We can use the plot of the cdf of a continuous variable as visual proof
    that probabilities are zero for any value of the continuous variable. Just notice
    how there are no *jumps* for continuous variables, which is equivalent to saying
    that the height of the jumps is exactly zero.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过了概率质量函数（pmf）和概率密度函数（pdf），但这些并不是描述分布的唯一方式。另一种选择是**累积分布函数**（**cdf**）。随机变量
    *X* 的 cdf 是函数 *F*[*X*]，由 *F*[*X*](*x*) = *P*(*X* ≤ *x*) 给出。换句话说，cdf 是对这个问题的回答：得到小于或等于
    *x* 的数值的概率是多少？在*图 [1.7](#x1-25003r7)* 的第一列中，我们可以看到 BetaBinomial 分布的 pmf 和 cdf，在第二列中，则展示了高斯分布的
    pdf 和 cdf。请注意，离散变量的 cdf 会“跳跃”，而连续变量的 cdf 则是平滑的。每次跳跃的高度代表一个概率——只需将其与点的高度进行比较。我们可以通过绘制连续变量的
    cdf 来作为概率为零的视觉证明，任何连续变量的数值都没有“跳跃”，这等同于说这些跳跃的高度正好为零。
- en: '![PIC](img/file22.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file22.png)'
- en: '**Figure 1.7**: The pmf of the BetaBinomial distribution with its corresponding
    cdf and the pdf of the Normal distribution with its corresponding cdf'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.7**：BetaBinomial 分布的 pmf 及其对应的 cdf 和正态分布的 pdf 及其对应的 cdf'
- en: Just by looking at a cdf, it is easier to find what is the probability of getting
    a number smaller than, let’s say, 1\. We just need to go to the value 1 on the
    x-axis, move up until we cross the black line, and then check the value of the
    y-axis. For instance, in *Figure [1.7](#x1-25003r7)* and for the Normal distribution,
    we can see that the value lies between 0.75 and 1\. Let’s say it is ≈ 0*.*85\.
    This is way harder to do with the pdf because we would need to compare the entire
    area below 1 to the total area to get the answer. Humans are worse at judging
    areas than judging heights or lengths.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过查看 cdf，就更容易找到某个数字小于，比如 1 的概率。我们只需要在 x 轴上找到 1 的值，向上移动直到穿过黑线，然后查看 y 轴的值。例如，在*图
    [1.7](#x1-25003r7)* 中，对于正态分布，我们可以看到该值位于 0.75 和 1 之间。假设它大约是 ≈ 0*.*85。这比使用 pdf 要难得多，因为我们需要将
    1 以下的整个区域与总区域进行比较才能得到答案。人类在判断面积方面不如在判断高度或长度时准确。
- en: 1.4.6 Conditional probability
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.6 条件概率
- en: 'Given two events *A* and *B* with *P*(*B*) *>* 0, the probability of *A* given
    *B*, which we write as *P*(*A*|*B*) is defined as:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个事件 *A* 和 *B*，且 *P*(*B*) *>* 0，条件概率 *P*(*A*|*B*) 定义为：
- en: '![P(A | B ) = P-(A,-B-) P (B) ](img/file23.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![P(A | B ) = P-(A,-B-) P (B) ](img/file23.jpg)'
- en: '*P*(*A,B*) is the probability that both the event *A* and event *B* occur.
    *P*(*A*|*B*) is known as conditional probability, and it is the probability that
    event *A* occurs, **conditioned** by the fact that we know (or assume, imagine,
    hypothesize, etc.) that *B* has occurred. For example, the probability that the
    pavement is wet is different from the probability that the pavement is wet if
    we know it’s raining.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*A,B*) 是事件 *A* 和事件 *B* 同时发生的概率。*P*(*A*|*B*) 被称为条件概率，它表示在已知（或假设、想象、假定等）*B*
    发生的前提下，事件 *A* 发生的概率。例如，路面湿了的概率与已知下雨时路面湿了的概率是不同的。'
- en: A conditional probability can be larger than, smaller than, or equal to the
    unconditional probability. If knowing *B* does not provide us with information
    about *A*, then *P*(*A*|*B*) = *P*(*A*). This will be true only if *A* and *B*
    are independent of each other. On the contrary, if knowing *B* gives us useful
    information about *A*, then the conditional probability could be larger or smaller
    than the unconditional probability, depending on whether knowing *B* makes *A*
    less or more likely. Let’s see a simple example using a fair six-sided die. What
    is the probability of getting the number 3 if we roll the die? *P*(die = 3) =
    ![16](img/file25.jpg) since each of the six numbers has the same chance for a
    fair six-sided die. And what is the probability of getting the number 3 given
    that we have obtained an odd number? *P*(die = 3 | die = {1,3,5}) = ![1 3](img/file27.jpg),
    because if we know we have an odd number, the only possible numbers are {1*,*3*,*5}
    and each of them has the same chance. Finally, what is the probability of getting
    3 if we have obtained an even number? This is *P*(die = 3 | die = {2,4,6}) = 0,
    because if we know the number is even, then the only possible ones are {2*,*4*,*6}
    and thus getting a 3 is not possible.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 条件概率可以大于、小于或等于无条件概率。如果知道 *B* 对我们理解 *A* 没有提供任何信息，那么 *P*(*A*|*B*) = *P*(*A*)。只有当
    *A* 和 *B* 互相独立时，这个关系才成立。相反，如果知道 *B* 给我们提供了关于 *A* 的有用信息，那么条件概率可能大于或小于无条件概率，具体取决于知道
    *B* 是否让 *A* 更加可能或更不可能。让我们通过一个简单的例子来看看，假设我们掷一个公平的六面骰子。掷到数字 3 的概率是多少？*P*(骰子 = 3)
    = ![16](img/file25.jpg)，因为对于一个公平的六面骰子，每个数字的机会是一样的。那么，假如我们已经知道掷到的是奇数，掷到数字 3 的概率是多少？*P*(骰子
    = 3 | 骰子 = {1,3,5}) = ![1 3](img/file27.jpg)，因为如果我们知道结果是奇数，那么可能的数字只有 {1*,*3*,*5}，而且它们的机会是相等的。最后，如果我们已经知道掷到的是偶数，掷到
    3 的概率是多少？这是 *P*(骰子 = 3 | 骰子 = {2,4,6}) = 0，因为如果我们知道结果是偶数，那么唯一可能的数字是 {2*,*4*,*6}，所以掷到
    3 的概率为 0。
- en: As we can see from these simple examples, by conditioning on observed data,
    we are changing the sample space. When asking about *P*(die = 3), we need to evaluate
    the sample space *S* = {1*,*2*,*3*,*4*,*5*,*6}, but when we **condition** **on**
    having got an even number, then the new sample space becomes *T* = {2*,*4*,*6}.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们从这些简单的例子中可以看到，通过对观测数据进行条件化，我们正在改变样本空间。当我们询问 *P*(骰子 = 3) 时，我们需要评估样本空间 *S*
    = {1*,*2*,*3*,*4*,*5*,*6}，但当我们在已知掷到的是偶数的情况下进行条件化时，新的样本空间变为 *T* = {2*,*4*,*6}。
- en: Conditional probabilities are at the heart of statistics, irrespective of whether
    your problem is rolling dice or building self-driving cars.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 条件概率是统计学的核心，无论你面对的问题是掷骰子还是构建自动驾驶汽车。
- en: The central panel of *Figure [1.8](#x1-26005r8)* represents *p*(*A,B*) using
    a grayscale with darker colors for higher probability densities. We see the joint
    distribution is elongated, indicating that the higher the value of *A*, the higher
    the one of *B*, and vice versa. Knowing the value of *A* tells us something about
    the values of *B* and the other way around. On the top and right *margins* of
    *Figure [1.8](#x1-26005r8)* we have the **marginal distributions** *p*(*A*) and
    *p*(*B*) respectively. To compute the marginal of *A*, we take *p*(*A,B*) and
    we average overall values of *B*, intuitively this is like taking a 2D object,
    the joint distribution, and projecting it into one dimension. The marginal distribution
    of *B* is computed similarly. The dashed lines represent the **conditional probability**
    *p*(*A*|*B*) for 3 different values of *B*. We get them by slicing the joint *p*(*A,B*)
    at a given value of *B*. We can think of this as the distribution of *A* given
    that we have observed a particular value of *B*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [1.8](#x1-26005r8)* 的中央面板使用灰度显示了联合分布*p*(*A,B*)，其中较深的颜色表示较高的概率密度。我们可以看到联合分布呈现拉长的形态，表明*A*的值越高，*B*的值也越高，反之亦然。知道了*A*的值，就能推测出*B*的值，反之亦然。在*图
    [1.8](#x1-26005r8)* 的顶部和右侧边缘分别展示了**边际分布** *p*(*A*)和*p*(*B*)。要计算*A*的边际分布，我们需要对*p*(*A,B*)进行对所有*B*值的平均，直观地说，这就像把二维对象（联合分布）投影到一维。*B*的边际分布也以类似的方式计算。虚线表示3个不同*B*值下的**条件概率**
    *p*(*A*|*B*)。我们通过在给定*B*值时切割联合分布*p*(*A,B*)来得到它们。我们可以把这看作是在已观察到特定的*B*值时，*A*的分布。'
- en: '![PIC](img/file30.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file30.png)'
- en: '**Figure 1.8**: Representation of the relationship between the joint *p*(*A,B*),
    the marginals *p*(*A*) and *p*(*B*), and the conditional *p*(*A*|*B*) probabilities'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.8**：联合概率*p*(*A,B*)、边缘概率*p*(*A*)和*p*(*B*)，以及条件概率*p*(*A*|*B*)之间关系的表示'
- en: 1.4.7 Expected values
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.7 期望值
- en: 'If *X* is a discrete random variable, we can compute its expected value as:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*X*是一个离散的随机变量，我们可以计算其期望值，公式如下：
- en: '![ ∑ 𝔼 (X ) = xP (X = x) x ](img/file31.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑ 𝔼 (X ) = xP (X = x) x ](img/file31.jpg)'
- en: This is just the mean or average value.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是均值或平均值。
- en: You are probably used to computing means or averages of samples or collections
    of numbers, either by hand, on a calculator, or using Python. But notice that
    here we are not talking about the mean of a bunch of numbers; we are talking about
    the mean of a distribution. Once we have defined the parameters of a distribution,
    we can, in principle, compute its expected values. Those are properties of the
    distribution in the same way that the perimeter is a property of a circle that
    gets defined once we set the value of the radius.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经习惯于计算样本或一组数字的均值或平均值，无论是手动、用计算器，还是使用Python。但请注意，在这里我们讨论的不是一堆数字的均值，而是分布的均值。一旦我们定义了分布的参数，原则上可以计算其期望值。它们是分布的特性，就像圆的周长是圆的一个特性，定义圆的半径后就可以确定。
- en: Another expected value is the variance, which we can use to describe the spread
    of a distribution. The variance appears *naturally* in many computations in statistics,
    but in practice, it is often more useful to use the standard deviation, which
    is the square root of the variance. The reason is that the standard deviation
    is in the same units as the random variable.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个期望值是方差，我们可以用它来描述分布的离散程度。方差在许多统计计算中*自然*出现，但在实践中，通常使用标准差，它是方差的平方根。原因是标准差的单位与随机变量相同。
- en: The mean and variance are often called the **moments** of a distribution. Other
    moments are skewness, which tells us about the asymmetry of a distribution, and
    the kurtosis, which tells us about the behavior of the tails or the *extreme values*
    [[Westfall](Bibliography.xhtml#Xwestfall2014), [2014](Bibliography.xhtml#Xwestfall2014)].
    *Figure [1.9](#x1-27006r9)* shows examples of different distributions and their
    mean *μ*, standard deviation *σ*, skew *γ*, and kurtosis ![](img/K.png). Notice
    that for some distributions, some moments may not be defined or they may be inf.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 均值和方差通常被称为分布的**矩**。其他的矩包括偏度，它告诉我们分布的偏斜程度，以及峰度，它告诉我们分布尾部或*极值*的行为[[Westfall](Bibliography.xhtml#Xwestfall2014)，
    [2014](Bibliography.xhtml#Xwestfall2014)]。*图 [1.9](#x1-27006r9)* 展示了不同分布及其均值*μ*、标准差*σ*、偏度*γ*和峰度的例子
    ![](img/K.png)。请注意，对于某些分布，某些矩可能没有定义，或者它们可能是无穷大。
- en: '![PIC](img/file32.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file32.png)'
- en: '**Figure 1.9**: Four distributions with their first four moments'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.9**：四个分布及其前四个矩'
- en: Now that we have learned about some of the basic concepts and jargon from probability
    theory, we can move on to the moment everyone was waiting for.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了一些概率论的基本概念和术语，我们可以继续进入大家期待的时刻了。
- en: 1.4.8 Bayes’ theorem
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.8 贝叶斯定理
- en: 'Without further ado, let’s contemplate, in all its majesty, Bayes’ theorem:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不拖延，让我们以它的威严，思考贝叶斯定理：
- en: '![ p(Y-| θ)p(θ) p(θ | Y ) = p(Y) ](img/file33.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![ p(Y-| θ)p(θ) p(θ | Y ) = p(Y) ](img/file33.jpg)'
- en: 'Well, it’s not that impressive, is it? It looks like an elementary school formula,
    and yet, paraphrasing Richard Feynman, this is all you need to know about Bayesian
    statistics. Learning where Bayes’ theorem comes from will help us understand its
    meaning. According to the product rule, we have:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这并不是什么令人印象深刻的东西，对吧？它看起来像是小学的公式，但引用理查德·费曼的话，这就是你需要了解的贝叶斯统计的全部内容。了解贝叶斯定理的来源将帮助我们理解它的意义。根据乘积规则，我们有：
- en: '![p (θ,Y ) = p(θ | Y ) p(Y ) ](img/file34.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![p (θ,Y ) = p(θ | Y ) p(Y ) ](img/file34.jpg)'
- en: 'This can also be written as:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以写成：
- en: '![p(θ,Y) = p(Y | θ) p(θ) ](img/file35.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![p(θ,Y) = p(Y | θ) p(θ) ](img/file35.jpg)'
- en: 'Given that the terms on the left are equal for both equations, we can combine
    them and write:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 由于左侧的项对于两个方程是相等的，我们可以将它们合并并写出：
- en: '![p(θ | Y) p(Y) = p(Y | θ) p(θ) ](img/file36.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![p(θ | Y) p(Y) = p(Y | θ) p(θ) ](img/file36.jpg)'
- en: 'On reordering, we get Bayes’ theorem:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排列后，我们得到贝叶斯定理：
- en: '![ p(Y | θ)p(θ) p(θ | Y ) =---p(Y)---- ](img/file37.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![ p(Y | θ)p(θ) p(θ | Y ) =---p(Y)---- ](img/file37.jpg)'
- en: Why is Bayes’ theorem that important? Let’s see.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么贝叶斯定理如此重要？让我们看看。
- en: First, it says that *p*(*θ*|*Y* ) is not necessarily the same as *p*(*Y* |*θ*).
    This is a very important fact – one that is easy to miss in daily situations,
    even for people trained in statistics and probability. Let’s use a simple example
    to clarify why these quantities are not necessarily the same. The probability
    of a person being the Pope given that this person is Argentinian is not the same
    as the probability of being Argentinian given that this person is the Pope. As
    there are around 47,000,000 Argentinians alive and a single one of them is the
    current Pope, we have *p*(Pope | Argentinian ) ≈![470100000](img/file39.jpg) and
    we also have *p*(Argentinian | Pope ) = 1.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它说*p*(*θ*|*Y*)不一定等于*p*(*Y*|*θ*)。这是一个非常重要的事实——一个即使是受过统计学和概率学训练的人也容易在日常情况下忽视的事实。我们通过一个简单的例子来澄清为什么这些量不一定相同。一个人是教皇的概率，给定这个人是阿根廷人，并不等同于，给定这个人是教皇，成为阿根廷人的概率。由于大约有4700万阿根廷人，而其中只有一个是现任教皇，我们有*p*(教皇
    | 阿根廷人) ≈![470100000](img/file39.jpg)，同时我们也有*p*(阿根廷人 | 教皇) = 1。
- en: If we replace *θ* with “hypothesis” and *Y* with “data,” Bayes’ theorem tells
    us how to compute the probability of a hypothesis, *θ*, given the data, *Y* ,
    and that’s the way you will find Bayes’ theorem is explained in a lot of places.
    But, how do we turn a hypothesis into something that we can put inside Bayes’
    theorem? Well, we do it by using probability distributions. So, in general, our
    hypothesis is a hypothesis in a very, very, very narrow sense; we will be more
    precise if we talk about finding a suitable value for parameters in our models,
    that is, parameters of probability distributions. By the way, don’t try to set
    *θ* to statements such as ”unicorns are real,” unless you are willing to build
    a realistic probabilistic model of unicorn existence!
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将*θ*替换为“假设”，将*Y*替换为“数据”，贝叶斯定理告诉我们如何计算在给定数据*Y*的情况下，假设*θ*的概率，这也是你会在很多地方看到的贝叶斯定理的解释。但是，如何将一个假设转化为可以放入贝叶斯定理中的东西呢？嗯，我们通过使用概率分布来做到这一点。所以，一般来说，我们的假设是一个非常非常非常狭义的假设；如果我们谈论的是找到适合我们模型的参数的值，即概率分布的参数，那么我们会更精确。顺便说一句，不要试图将*θ*设定为“独角兽存在”的陈述，除非你愿意构建一个现实的独角兽存在的概率模型！
- en: 'Bayes’ theorem is central to Bayesian statistics. As we will see in *Chapter
    [2](CH02.xhtml#x1-440002)*, using tools such as PyMC frees us of the need to explicitly
    write Bayes’ theorem every time we build a Bayesian model. Nevertheless, it is
    important to know the name of its parts because we will constantly refer to them
    and it is important to understand what each part means because this will help
    us to conceptualize models. So, let me rewrite Bayes’ theorem now with labels:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理是贝叶斯统计的核心。正如我们在*第[2章](CH02.xhtml#x1-440002)*中看到的，使用像PyMC这样的工具解放了我们每次构建贝叶斯模型时都必须显式书写贝叶斯定理的需要。然而，了解其各个部分的名称是很重要的，因为我们将不断引用它们，而且理解每个部分的含义也很重要，因为这有助于我们构建模型的概念。所以，让我现在带着标签重写贝叶斯定理：
- en: '![ posterior ◜likeli◞h◟ood◝p◜r◞io◟r◝ ◜--◞◟-◝ p (Y | θ)p(θ) p(θ | Y) =--------------
    p◟(◝Y◜)◞ marginal likelihood ](img/file41.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![ posterior ◜likeli◞h◟ood◝p◜r◞io◟r◝ ◜--◞◟-◝ p (Y | θ)p(θ) p(θ | Y) =--------------
    p◟(◝Y◜)◞ marginal likelihood ](img/file41.jpg)'
- en: The **prior distribution** should reflect what we know about the value of the
    parameter *θ* before seeing the data, *Y* . If we know nothing, like Jon Snow,
    we could use flat priors that do not convey too much information. In general,
    we can do better than flat priors, as we will learn in this book. The use of priors
    is why some people still talk about Bayesian statistics as subjective, even when
    priors are just another assumption that we made when modeling and hence are just
    as subjective (or objective) as any other assumption, such as likelihoods.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**先验分布**应当反映我们在看到数据之前对参数*θ*值的了解，*Y*。如果我们什么都不知道，就像乔恩·雪诺那样，我们可以使用平坦的先验，这不会传达太多信息。通常来说，我们可以比平坦的先验做得更好，正如我们在本书中将要学到的那样。先验的使用是为什么一些人仍然认为贝叶斯统计是主观的，尽管先验只是我们在建模时所做的另一种假设，因此它和其他假设（如似然）一样主观（或客观）。'
- en: The **likelihood** is how we will introduce data in our analysis. It is an expression
    of the plausibility of the data given the parameters. In some texts, you will
    find people call this term sampling model, statistical model, or just model. We
    will stick to the name likelihood and we will model the combination of priors
    and likelihood.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**似然**是我们在分析中引入数据的方式。它是给定参数下数据的合理性表达。在某些文本中，你会发现有人称这个术语为采样模型、统计模型或仅仅是模型。我们将坚持使用“似然”这个名称，并将建模先验和似然的组合。'
- en: 'The **posterior distribution** is the result of the Bayesian analysis and reflects
    all that we know about a problem (given our data and model). The posterior is
    a probability distribution for the parameters in our model and not a single value.
    This distribution is a balance between the prior and the likelihood. There is
    a well-known joke: a Bayesian is one who, vaguely expecting a horse, and catching
    a glimpse of a donkey, strongly believes they have seen a mule. One excellent
    way to kill the mood after hearing this joke is to explain that if the likelihood
    and priors are both vague, you will get a posterior reflecting vague beliefs about
    seeing a mule rather than strong ones. Anyway, I like the joke, and I like how
    it captures the idea of a posterior being somehow a compromise between prior and
    likelihood. Conceptually, we can think of the posterior as the updated prior in
    light of (new) data. In theory, the posterior from one analysis can be used as
    the prior for a new analysis (in practice, life can be harder). This makes Bayesian
    analysis particularly suitable for analyzing data that becomes available in sequential
    order. One example could be an early warning system for natural disasters that
    processes online data coming from meteorological stations and satellites. For
    more details, read about online machine-learning methods.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**后验分布**是贝叶斯分析的结果，反映了我们关于一个问题的所有已知信息（基于我们的数据和模型）。后验分布是模型参数的概率分布，而不是单一的值。这个分布是先验和似然之间的平衡。有一个著名的笑话：贝叶斯学家是那种模糊地期望看到一匹马，却只看到了一只驴，并坚信自己看到了骡子的人。听到这个笑话后，解释说如果似然和先验都很模糊，那么得到的后验反映的就是对看到骡子的模糊信念，而不是强烈的信念，这会大大破坏气氛。无论如何，我喜欢这个笑话，也喜欢它传达了后验作为先验和似然之间某种妥协的想法。从概念上讲，我们可以将后验视为根据（新）数据更新后的先验。理论上，一个分析的后验可以作为新分析的先验（但实际上，生活可能会更复杂）。这使得贝叶斯分析特别适用于分析按顺序提供的数据。一个例子可能是自然灾害的早期预警系统，它处理来自气象站和卫星的在线数据。有关更多细节，请阅读关于在线机器学习方法的资料。'
- en: The last term is the **marginal likelihood**, sometimes referred to as the **evidence**.
    Formally, the marginal likelihood is the probability of observing the data averaged
    over all the possible values the parameters can take (as prescribed by the prior).
    We can write this as ∫ [Θ]^(*p*(*Y* |*θ*)*p*(*θ*)d*θ*. We will not really care
    about the marginal likelihood until *Chapter [5](CH05.xhtml#x1-950005)*. But for
    the moment, we can think of it as a normalization factor that ensures the posterior
    is a proper pmf or pdf. If we ignore the marginal likelihood, we can write Bayes’
    theorem as a proportionality, which is also a common way to write Bayes’ theorem:)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一项是**边际似然性**，有时也称为**证据**。严格来说，边际似然性是观察数据的概率，取决于所有参数可能取值的平均值（按照先验分布规定）。我们可以将其表示为
    ∫ [Θ]^(*p*(*Y* |*θ*)*p*(*θ*)d*θ*。我们直到*第5章*[5](CH05.xhtml#x1-950005)才会真正关心边际似然性。但目前，我们可以将其视为一个归一化因子，确保后验分布是一个合适的pmf或pdf。如果忽略边际似然性，我们可以将贝叶斯定理写为一个比例关系，这也是一种常见的贝叶斯定理表示方式。
- en: '![p(θ | Y ) ∝ p(Y | θ)p(θ) ](img/file42.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![p(θ | Y ) ∝ p(Y | θ)p(θ) ](img/file42.jpg)'
- en: Understanding the exact role of each term in Bayes’ theorem will take some time
    and practice, and it will require a few examples, but that’s what the rest of
    this book is for.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 理解贝叶斯定理中每个项的确切作用需要一些时间和实践，并且需要通过几个例子来帮助理解，但这就是本书其余部分的目的所在。
- en: 1.5 Interpreting probabilities
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 概率解释
- en: Probabilities can be interpreted in various useful ways. For instance, we can
    think that *P*(*A*) = 0*.*125 means that if we repeat the survey many times, we
    would expect all three individuals to answer “yes” about 12.5% of the time. We
    are interpreting probabilities as the outcome of long-run experiments. This is
    a very common and useful interpretation. It not only can help us think about probabilities
    but can also provide an empirical method to estimate probabilities. Do we want
    to know the probability of a car tire exploding if filled with air beyond the
    manufacturer’s recommendation? Just inflate 120 tires or so, and you may get a
    good approximation. This is usually called the frequentist interpretation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 概率可以通过各种有用的方式来解释。例如，我们可以认为 *P*(*A*) = 0*.*125 意味着如果我们多次重复调查，我们期望这三个人大约12.5%的时间会回答“是”。我们正在将概率解释为长期实验的结果。这是一种非常常见且有用的解释。它不仅可以帮助我们思考概率，还可以提供一种经验方法来估计概率。我们想知道，如果汽车轮胎充气超过制造商推荐的标准，爆胎的概率是多少吗？只需充气大约120个轮胎，你就能得到一个不错的近似值。这通常被称为频率主义解释。
- en: Another interpretation of probability, usually called subjective or Bayesian
    interpretation, states that probabilities can be interpreted as measures of an
    individual’s uncertainty about events. In this interpretation, probabilities are
    about our state of knowledge of the world and are not necessarily based on repeated
    trials. Under this definition of probability, it is valid and natural to ask about
    the probability of life on Mars, the probability of the mass of an electron being
    9*.*1 × 10^(−31) kg, or the probability that the 9^(th) of July of 1816 was a
    sunny day in Buenos Aires. All these are one-time events. We cannot re-create
    1 million universes, each with one Mars, and check how many of them develop life.
    Of course, we can do this as a mental experiment, so long-term frequencies can
    still be a valid mental scaffold.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 概率的另一种解释，通常称为主观或贝叶斯解释，认为概率可以被解释为个体对事件不确定性的度量。在这种解释下，概率与我们对世界的知识状态相关，并不一定基于重复试验。在这种概率定义下，提出关于火星上是否有生命的概率、电子质量为9*.*1
    × 10^(−31) kg的概率，或者1816年7月9日布宜诺斯艾利斯是否是晴天的概率，都是有效且自然的。这些都是一次性事件。我们不能重新创造1百万个宇宙，每个宇宙都有一个火星，并检查其中有多少个发展出了生命。当然，我们可以做这个心理实验，只要长期频率仍然是一个有效的心理框架。
- en: Sometimes the Bayesian interpretation of probabilities is described in terms
    of personal beliefs; I don’t like that. I think it can lead to unnecessary confusion
    as beliefs are generally associated with the notion of faith or unsupported claims.
    This association can easily lead people to think that Bayesian probabilities,
    and by extension Bayesian statistics, is less objective or less scientific than
    alternatives. I think it also helps to generate confusion about the role of prior
    knowledge in statistics and makes people think that being objective or rational
    means not using prior information.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 有时贝叶斯对概率的解释被描述为个人信念；我不喜欢这样。我认为这可能会导致不必要的混淆，因为信念通常与信仰或没有依据的主张有关。这种关联很容易让人认为贝叶斯概率，进而贝叶斯统计学，比其他方法不那么客观或不那么科学。我还认为，这种说法有助于产生对统计学中先验知识角色的混淆，让人们误以为客观或理性就意味着不使用先验信息。
- en: 'Bayesian methods are as subjective (or objective) as any other well-established
    scientific method we have. Let me explain myself with an example: life on Mars
    exists or does not exist; the outcome is binary, a yes-no question. But given
    that we are not sure about that fact, a sensible course of action is trying to
    find out how likely life on Mars is. To answer this question any honest and scientific-minded
    person will use all the relevant geophysical data about Mars, all the relevant
    biochemical knowledge about necessary conditions for life, and so on. The response
    will be necessarily about our epistemic state of knowledge, and others could disagree
    and even get different probabilities. But at least, in principle, they all will
    be able to provide arguments in favor of their data, their methods, their modeling
    decisions, and so on. A scientific and rational debate about life on Mars does
    not admit *arguments* such as ”an angel told me about tiny green creatures.” Bayesian
    statistics, however, is just a procedure to make scientific statements using probabilities
    as building blocks.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯方法与我们拥有的任何其他成熟的科学方法一样主观（或客观）。让我用一个例子来解释：火星上是否存在生命，答案是二元的，类似是或不是的问题。但考虑到我们无法确认这一事实，合理的做法是尝试找出火星上生命存在的可能性。为了回答这个问题，任何诚实且具有科学思维的人都会使用所有相关的火星地球物理数据、所有关于生命所需条件的生物化学知识等等。这个回答必然是关于我们知识状态的，不同的人可能会有不同的看法，甚至得出不同的概率。但至少，从原则上讲，他们都会能够为自己的数据、方法、建模决策等提供支持论据。关于火星生命的科学理性辩论不容许像“天使告诉我有小绿人”这样的*论据*。然而，贝叶斯统计学只是一种使用概率作为构建模块来做科学陈述的程序。
- en: 1.6 Probabilities, uncertainty, and logic
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 概率、不确定性和逻辑
- en: Probabilities can help us to quantify uncertainty. If we do not have information
    about a problem, it is reasonable to state that every possible event is equally
    likely. This is equivalent to assigning the same probability to every possible
    event. In the absence of information, our uncertainty is maximum, and I am not
    saying this colloquially; this is something we can compute using probabilities.
    If we know instead that some events are more likely, then this can be formally
    represented by assigning a higher probability to those events and less to the
    others. Notice that when we talk about events in stats-speak, we are not restricting
    ourselves to things that can happen, such as an asteroid crashing into Earth or
    my auntie’s 60^(th) birthday party. An event is just any of the possible values
    (or a subset of values) a variable can take, such as the event that you are older
    than 30, the price of a Sachertorte, or the number of bikes that will be sold
    next year around the world.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 概率可以帮助我们量化不确定性。如果我们对一个问题没有信息，那么可以合理地说每一个可能的事件发生的概率是相等的。这相当于对每一个可能的事件赋予相同的概率。在没有信息的情况下，我们的不确定性是最大的，我并不是随便这么说；这是我们可以通过概率来计算的。如果我们知道某些事件更可能发生，那么可以通过给这些事件赋予更高的概率，而其他事件赋予较低的概率来正式表示这一点。请注意，当我们在统计学中谈论事件时，并不仅仅局限于可能发生的事情，比如小行星撞击地球或我姨妈的60岁生日派对。事件只是一个变量可以取的任何可能值（或值的子集），比如你年龄超过30岁、萨赫托尔特的价格，或明年全球将售出的自行车数量。
- en: 'The concept of probability is also related to the subject of logic. Under classical
    logic, we can only have statements that take the values of true or false. Under
    the Bayesian definition of probability, certainty is just a special case: a true
    statement has a probability of 1, and a false statement has a probability of 0\.
    We would assign a probability of 1 to the statement that there is Martian life
    only after having conclusive data indicating something is growing, reproducing,
    and doing other activities we associate with living organisms.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 概率的概念也与逻辑学有关。在经典逻辑下，我们只能有真或假的命题。在贝叶斯概率定义下，确定性只是一个特殊的情况：一个真实的命题的概率是1，而一个虚假的命题的概率是0。只有在拥有决定性数据表明有东西在生长、繁殖以及进行其他我们认为与生物体相关的活动时，我们才会给“火星上有生命”这一命题分配概率为1。
- en: Notice, however, that assigning a probability of 0 is harder because we could
    always think that there is some Martian spot that is unexplored, or that we have
    made mistakes with some experiments, or there are several other reasons that could
    lead us to falsely believe life is absent on Mars even if it is not. This is related
    to Cromwell’s rule, which states that we should reserve the probabilities of 0
    or 1 to logically true or false statements. Interestingly enough, it can be shown
    that if we want to extend the logic to include uncertainty, we must use probabilities
    and probability theory.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，分配一个概率为0更难，因为我们总是可以认为火星上还有一些未探索的区域，或者我们在某些实验中犯了错误，或者有其他几个原因可能导致我们错误地认为火星上没有生命，即使实际上是有的。这与克劳梅尔法则有关，该法则指出我们应该将概率为0或1的命题保留给逻辑上真实或虚假的命题。有趣的是，可以证明，如果我们想要将逻辑扩展以包括不确定性，我们必须使用概率和概率理论。
- en: As we will soon see, Bayes’ theorem is just a logical consequence of the rules
    of probability. Thus, we can think of Bayesian statistics as an extension of logic
    that is useful whenever we are dealing with uncertainty. Thus, one way to justify
    using the Bayesian method is to recognize that uncertainty is commonplace. We
    generally have to deal with incomplete and or noisy data, we are intrinsically
    limited by our evolution-sculpted primate brain, and so on.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们很快就会看到的，贝叶斯定理只是概率规则的逻辑结果。因此，我们可以将贝叶斯统计看作是逻辑的扩展，它在我们处理不确定性时非常有用。因此，采用贝叶斯方法的一个理由是承认不确定性是普遍存在的。我们通常不得不处理不完整或嘈杂的数据，我们本质上受到进化塑造的灵长类大脑的局限，等等。
- en: The Bayesian Ethos
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯精神
- en: Probabilities are used to measure the uncertainty we have about parameters,
    and Bayes’ theorem is a mechanism to correctly update those probabilities in light
    of new data, hopefully reducing our uncertainty.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 概率用于衡量我们对参数的不确定性，而贝叶斯定理是一个在有新数据的情况下正确更新这些概率的机制，希望能够减少我们的不确定性。
- en: 1.7 Single-parameter inference
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7 单一参数推断
- en: Now that we know what Bayesian statistics is, let’s learn how to do Bayesian
    statistics with a simple example. We are going to begin inferring a single, unknown
    parameter.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了贝叶斯统计是什么，接下来让我们通过一个简单的例子来学习如何进行贝叶斯统计。我们将从推断一个单一的未知参数开始。
- en: 1.7.1 The coin-flipping problem
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7.1 投硬币问题
- en: 'The coin-flipping problem, or the BetaBinomial model if you want to sound fancy
    at parties, is a classical problem in statistics and goes like this: we toss a
    coin several times and record how many heads and tails we get. Based on this data,
    we try to answer questions such as, is the coin fair? Or, more generally, how
    biased is the coin? While this problem may sound dull, we should not underestimate
    it.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 投硬币问题，或者如果你想在聚会上显得更专业的话可以称其为BetaBinomial模型，是统计学中的一个经典问题，问题是这样的：我们多次掷硬币并记录正反面朝上的次数。基于这些数据，我们试图回答这样的问题：这枚硬币公平吗？或者，更一般地说，这枚硬币有多偏？虽然这个问题可能听起来很无聊，但我们不应该低估它。
- en: The coin-flipping problem is a great example to learn the basics of Bayesian
    statistics because it is a simple model that we can solve and compute with ease.
    Besides, many real problems consist of binary, mutually exclusive outcomes such
    as 0 or 1, positive or negative, odds or evens, spam or ham, hotdog or not a hotdog,
    cat or dog, safe or unsafe, and healthy or unhealthy. Thus, even when we are talking
    about coins, this model applies to any of those problems. To estimate the bias
    of a coin, and in general, to answer any questions in a Bayesian setting, we will
    need data and a probabilistic model. For this example, we will assume that we
    have already tossed a coin several times and we have a record of the number of
    observed heads, so the data-gathering part is already done. Getting the model
    will take a little bit more effort. Since this is our first model, we will explicitly
    write Bayes’ theorem and do all the necessary math (don’t be afraid, I promise
    it will be painless) and we will proceed very slowly. From [2](CH02.xhtml#x1-440002)
    onward, we will use PyMC and our computer to do the math for us.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 投硬币问题是学习贝叶斯统计基础的一个很好的例子，因为它是一个简单的模型，我们可以轻松地解决和计算。此外，许多实际问题由二元的、互斥的结果组成，例如 0
    或 1、正或负、奇数或偶数、垃圾邮件或非垃圾邮件、热狗或不是热狗、猫或狗、安全或不安全、健康或不健康。因此，即使我们在谈论硬币时，这个模型也适用于任何这些问题。为了估计硬币的偏向性，并且通常来说，回答贝叶斯环境中的任何问题，我们将需要数据和一个概率模型。对于这个例子，我们假设我们已经抛掷了几次硬币，并且我们有观察到的正面朝上的次数记录，所以数据收集部分已经完成。获得模型将需要更多的努力。由于这是我们的第一个模型，我们将明确地写出贝叶斯定理并完成所有必要的数学运算（不用担心，我保证这不会痛苦），并且我们将非常慢地进行。从[2](CH02.xhtml#x1-440002)开始，我们将使用
    PyMC 和我们的计算机来为我们做数学运算。
- en: The first thing we will do is generalize the concept of bias. We will say that
    a coin with a bias of 1 will always land heads, one with a bias of 0 will always
    land tails, and one with a bias of 0.5 will land heads half of the time and tails
    half of the time. To represent the bias, we will use the parameter *θ*, and to
    represent the total number of heads for several tosses, we will use the variable
    *Y* . According to Bayes’ theorem, we have to specify the prior, *p*(*θ*), and
    likelihood, *p*(*Y* |*θ*), we will use. Let’s start with the likelihood.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将做的第一件事是推广偏向的概念。我们将说，偏向为1的硬币总是会正面朝上，偏向为0的硬币总是会反面朝上，而偏向为0.5的硬币在一半的时间里会正面朝上，另一半时间会反面朝上。为了表示偏向，我们将使用参数*θ*，而为了表示多次投掷中正面朝上的总次数，我们将使用变量*Y*。根据贝叶斯定理，我们必须指定先验分布*p*(*θ*)和似然函数*p*(*Y*
    | *θ*)，我们将使用这些。让我们从似然函数开始。
- en: 1.7.2 Choosing the likelihood
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7.2 选择似然函数
- en: 'Let’s assume that only two outcomes are possible—heads or tails—and let’s also
    assume that a coin toss does not affect other tosses, that is, we are assuming
    coin tosses are independent of each other. We will further assume all coin tosses
    come from the same distribution. Thus the random variable coin toss is an example
    of an **independent and identically distributed** (**iid**) variable. I hope you
    agree that these are very reasonable assumptions to make for our problem. Given
    these assumptions, a good candidate for the likelihood is the Binomial distribution:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 假设只有两种可能的结果——正面或反面——同时假设一次硬币投掷不会影响其他投掷，也就是说，我们假设硬币投掷是相互独立的。我们进一步假设所有硬币投掷来自同一分布。因此，随机变量硬币投掷是**独立同分布**（**iid**）变量的一个例子。我希望你同意这些假设对于我们的问题来说是非常合理的。基于这些假设，似然函数的一个合适候选是二项分布：
- en: '![ ----N-!--- y N− y p(Y | θ) = y!(N − y)! θ (1 − θ) ◟---◝◜---◞ normalizing
    constant ](img/file43.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![ ----N-!--- y N− y p(Y | θ) = y!(N − y)! θ (1 − θ) ◟---◝◜---◞ normalizing
    constant ](img/file43.jpg)'
- en: This is a discrete distribution returning the probability of getting *y* heads
    (or, in general, successes) out of *N* coin tosses (or, in general, trials or
    experiments) given a fixed value of *θ*.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种离散分布，返回在*N*次硬币投掷（或一般的试验或实验）中得到*y*次正面（或一般的成功）的概率，前提是固定的*θ*值。
- en: '*Figure [1.10](#x1-33003r10)* shows nine distributions from the Binomial family;
    each subplot has its legend indicating the values of the parameters. Notice that
    for this plot, I did not omit the values on the y-axis. I did this so you can
    check for yourself that if you sum the height of all bars, you will get 1, that
    is, for discrete distributions, the height of the bars represents actual probabilities.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [1.10](#x1-33003r10)* 显示了来自二项分布族的九个分布；每个子图都有一个图例，表示参数的值。请注意，对于这个图，我没有省略y轴上的值。我这样做是为了让你自己检查，如果你将所有柱子的高度相加，你将得到1，也就是说，对于离散分布，柱子的高度代表的是实际概率。'
- en: '![PIC](img/file44.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file44.png)'
- en: '**Figure 1.10**: Nine members of the Binomial family'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**Figure 1.10**: 二项家族的九位成员'
- en: The Binomial distribution is a reasonable choice for the likelihood. We can
    see that *θ* indicates how likely it is to obtain a head when tossing a coin.
    This is easier to see when *N* = 1 but is valid for any value of *N*, just compare
    the value of *θ* with the height of the bar for *y* = 1 (heads).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 二项分布是似然性的合理选择。我们可以看到*θ*表示抛硬币时得到头的可能性有多大。当*N*=1时更容易看到，但对于任何*N*的值，只需比较*y*=1（头）时*θ*的值与柱状图的高度即可。
- en: 1.7.3 Choosing the prior
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7.3 选择先验分布
- en: 'As a prior, we will use a Beta distribution, which is a very common distribution
    in Bayesian statistics and looks as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 作为先验，我们将使用贝塔分布，这在贝叶斯统计中非常常见，外观如下：
- en: '![p(θ) = --Γ (𝛼-+-𝛽)- θ𝛼−1(1− θ)𝛽−1 Γ◟-(𝛼-)+◝◜Γ (𝛽)◞ normalizing constant ](img/file45.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![p(θ) = --Γ (𝛼-+-𝛽)- θ𝛼−1(1− θ)𝛽−1 Γ◟-(𝛼-)+◝◜Γ (𝛽)◞ 归一化常数](img/file45.jpg)'
- en: If we look carefully, we will see that the Beta distribution looks similar to
    the Binomial except for the first term. Γ is the Greek uppercase gamma letter,
    which represents the gamma function, but that’s not really important. What is
    relevant for us is that the first term is a normalizing constant that ensures
    the distribution integrates to 1\. We can see from the preceding formula that
    the Beta distribution has two parameters, *α* and *β*. *Figure [1.11](#x1-34002r11)*
    shows nine members of the Beta family.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仔细观察，会发现贝塔分布与二项分布看起来很相似，除了第一项。 Γ是希腊大写gamma字母，代表伽玛函数，但这并不是真正重要的。对我们而言重要的是，第一项是一个归一化常数，确保分布积分为1。从前述公式中可以看出，贝塔分布有两个参数，*α*和*β*。*Figure
    [1.11](#x1-34002r11)* 展示了贝塔家族的九位成员。
- en: '![PIC](img/file46.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file46.png)'
- en: '**Figure 1.11**: Nine members of the Beta family'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**Figure 1.11**: 贝塔家族的九位成员'
- en: I like the Beta distribution and all the shapes we can get from it, but why
    are we using it for our model? There are many reasons to use a Beta distribution
    for this and other problems. One of them is that the Beta distribution is restricted
    to be between 0 and 1, in the same way our *θ* parameter is. In general, we use
    the Beta distribution when we want to model the proportions of a Binomial variable.
    Another reason is its versatility. As we can see in *Figure [1.11](#x1-34002r11)*,
    the distribution adopts several shapes (all restricted to the [0*,*1] interval),
    including a Uniform distribution, Gaussian-like distributions, and U-like distributions.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢贝塔分布及其所有可能的形状，但为什么我们要在我们的模型中使用它呢？使用贝塔分布来处理此类问题有许多理由。其中之一是贝塔分布限制在0到1之间，与我们的*θ*参数相同。一般来说，我们在想要建模二项变量的比例时使用贝塔分布。另一个原因是其多功能性。正如我们在*Figure
    [1.11](#x1-34002r11)* 中看到的，该分布采用多种形状（均限制在[0,1]区间内），包括均匀分布、类似正态分布和U形分布。
- en: As a third reason, the Beta distribution is the conjugate prior to the Binomial
    distribution (which we are using as the likelihood). A conjugate prior of a likelihood
    is a prior that, when used in combination with a given likelihood, returns a posterior
    with the same functional form as the prior. Untwisting the tongue, every time
    we use a Beta distribution as the prior and a Binomial distribution as the likelihood,
    we will get a Beta as the posterior distribution. There are other pairs of conjugate
    priors; for example, the Normal distribution is the conjugate prior to itself.
    For many years, Bayesian analysis was restricted to the use of conjugate priors.
    Conjugacy ensures mathematical tractability of the posterior, which is important
    given that a common problem in Bayesian statistics ends up with a posterior we
    cannot solve analytically. This was a deal breaker before the development of suitable
    computational methods to solve probabilistic methods. From *Chapter [2](CH02.xhtml#x1-440002)*
    onwards, we will learn how to use modern computational methods to solve Bayesian
    problems, whether we choose conjugate priors or not.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第三个原因，Beta 分布是二项分布（我们作为似然使用的分布）的共轭先验。共轭先验是指，当与给定的似然结合使用时，返回的后验具有与先验相同的函数形式。简单来说，每次我们使用
    Beta 分布作为先验，二项分布作为似然时，我们将得到 Beta 作为后验分布。还有其他共轭先验的配对；例如，正态分布是其自身的共轭先验。多年来，贝叶斯分析一直局限于使用共轭先验。共轭性确保了后验的数学可解性，这一点非常重要，因为贝叶斯统计中一个常见的问题是我们最终得到一个无法解析求解的后验。在开发适合的计算方法解决概率方法之前，这曾是一个关键的障碍。从*第
    [2](CH02.xhtml#x1-440002)* 章开始，我们将学习如何使用现代计算方法解决贝叶斯问题，无论我们选择是否使用共轭先验。
- en: 1.7.4 Getting the posterior
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7.4 获取后验
- en: 'Let’s remember that Bayes’ theorem says the posterior is proportional to the
    likelihood times the prior. So, for our problem, we have to multiply the Binomial
    and the Beta distributions:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们记住，贝叶斯定理表明后验与似然和先验的乘积成正比。因此，对于我们的问题，我们需要将二项分布和 Beta 分布相乘：
- en: '![ likelihood prior ◜---------◞◟---------◝ ◜----------◞◟-----------◝ p(θ |
    Y ) =---N-!---θy(1− θ)N −y -Γ-(𝛼+-𝛽-)-θ𝛼− 1(1 − θ)𝛽−1 y!(N − y )! Γ (𝛼) + Γ (𝛽)
    ](img/file47.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![ likelihood prior ◜---------◞◟---------◝ ◜----------◞◟-----------◝ p(θ |
    Y ) =---N-!---θy(1− θ)N −y -Γ-(𝛼+-𝛽-)-θ𝛼− 1(1 − θ)𝛽−1 y!(N − y )! Γ (𝛼) + Γ (𝛽)
    ](img/file47.jpg)'
- en: 'We can simplify this expression by dropping all the terms that do not depend
    on *θ* and our results will still be valid. Accordingly, we can write:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过去掉所有与 *θ* 无关的项来简化这个表达式，结果仍然有效。因此，我们可以写成：
- en: '![ -likelihood--- -----prior---- ◜y ◞◟ N −◝y ◜𝛼−1 ◞◟ 𝛽−◝1 p(θ | Y) ∝ θ) ](img/file48.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![ -likelihood--- -----prior---- ◜y ◞◟ N −◝y ◜𝛼−1 ◞◟ 𝛽−◝1 p(θ | Y) ∝ θ) ](img/file48.jpg)'
- en: 'Reordering it, and noticing this has the form of a Beta distribution, we get:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排列它，并注意到这具有 Beta 分布的形式，我们得到：
- en: '![p(θ | Y ) = Beta (𝛼prior + y,𝛽prior+N −y) ](img/file49.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![p(θ | Y ) = Beta (𝛼prior + y,𝛽prior+N −y) ](img/file49.jpg)'
- en: Based on this analytical expression, we can compute the posterior. *Figure [1.12](#x1-35014r12)*
    shows the results for 3 priors and different numbers of trials. The following
    block of code shows the gist to generate *Figure [1.12](#x1-35014r12)* (omitting
    the code necessary for plotting).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个解析表达式，我们可以计算后验。*图 [1.12](#x1-35014r12)* 显示了 3 个先验和不同试验次数下的结果。以下代码块展示了生成
    *图 [1.12](#x1-35014r12)* 的要点（省略了绘图所需的代码）。
- en: '**Code 1.6**'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 1.6**'
- en: '[PRE5]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![PIC](img/file50.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file50.png)'
- en: '**Figure 1.12**: The first subplot shows 3 priors. The rest show successive
    updates as we get new data'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.12**：第一个子图显示了 3 个先验。其余的显示了随着新数据的到来，更新后的结果。'
- en: 'On the first subplot of *Figure [1.12](#x1-35014r12)*, we have zero trials,
    thus the three curves represent our priors:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 [1.12](#x1-35014r12)* 的第一个子图中，我们有零次试验，因此三条曲线表示我们的先验：
- en: 'The Uniform prior (black): This represents all the possible values for the
    bias being equally probable a priori.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均匀先验（黑色）：这表示在先验中所有偏差的可能值都是等概率的。
- en: 'The Gaussian-like prior (dark gray): This is centered and concentrated around
    0.5, so this prior is compatible with information indicating that the coin has
    more or less about the same chance of landing heads or tails. We could also say
    this prior is compatible with the knowledge that coins are fair.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯型先验（深灰色）：这围绕 0.5 进行集中，表示该先验与信息兼容，表明硬币正反面的概率大致相同。我们也可以说，这个先验与硬币公平的知识是兼容的。
- en: 'The skewed prior (light gray): This puts most of the weight on a tail-biased
    outcome.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 倾斜的先验（浅灰色）：这将大部分权重放在尾部偏向的结果上。
- en: 'The rest of the subplots show posterior distributions for successive trials.
    The number of trials (or coin tosses) and the number of heads are indicated in
    each subplot’s legend. There is also a black dot at 0.35 representing the true
    value for *θ*. Of course, in real problems, we do not know this value, and it
    is here just for pedagogical reasons. *Figure [1.12](#x1-35014r12)*, can teach
    us a lot about Bayesian analysis, so grab your coffee, tea, or favorite drink,
    and let’s take a moment to understand it:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的子图展示了后续试验的后验分布。每个子图的图例中标明了试验次数（或掷硬币次数）和正面朝上的次数。还有一个黑点在0.35处，表示*θ*的真实值。当然，在实际问题中，我们并不知道这个值，它在这里仅用于教学目的。*图
    [1.12](#x1-35014r12)*，可以帮助我们深入理解贝叶斯分析，因此拿起你的咖啡、茶或最喜欢的饮品，我们来花点时间理解它：
- en: The result of a Bayesian analysis is a posterior distribution – not a single
    value but a distribution of plausible values given the data and our model.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯分析的结果是一个后验分布——它不是一个单一值，而是给定数据和模型下的一个可能值的分布。
- en: The most probable value is given by the mode of the posterior (the peak of the
    distribution).
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最可能的值由后验分布的众数（分布的峰值）给出。
- en: The spread of the posterior is proportional to the uncertainty about the value
    of a parameter; the more spread out the distribution, the less certain we are.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后验分布的扩展与参数值的不确定性成正比；分布越广泛，我们的确定性就越低。
- en: Intuitively, we are more confident in a result when we have observed more data
    supporting that result. Thus, even when numerically ![1 2](img/file51.jpg) = ![48](img/file52.jpg)
    = 0*.*5, seeing four heads out of eight trials gives us more confidence that the
    bias is 0.5 than observing one head out of two trials. This intuition is reflected
    in the posterior, as you can check for yourself if you pay attention to the (black)
    posterior in the third and sixth subplots; while the mode is the same, the spread
    (uncertainty) is larger in the third subplot than in the sixth subplot.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直观上，当我们观察到更多支持某一结果的数据时，我们对结果的信心就更强。因此，即使数值上![1 2](img/file51.jpg) = ![48](img/file52.jpg)
    = 0*.*5，在八次试验中看到四次正面朝上比在两次试验中看到一次正面朝上能更有信心地认为偏差是0.5。这个直觉在后验分布中有所体现，您可以自己检查，如果注意观察第三和第六个子图中的（黑色）后验分布；虽然众数相同，但第三个子图的扩展（不确定性）比第六个子图更大。
- en: Given a sufficiently large amount of data, two or more Bayesian models with
    different priors will tend to converge to the same result. In the limit of infinite
    data, no matter which prior we use, all of them will provide the same posterior.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定足够多的数据，两个或多个具有不同先验的贝叶斯模型将趋向于收敛到相同的结果。在无限数据的极限下，无论我们使用什么先验，所有的模型都将提供相同的后验分布。
- en: Remember that infinite is a limit and not a number, so from a practical point
    of view, we could get practically equivalent posteriors for a finite and relatively
    small number of data points.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记住，无限是一个极限，而不是一个数字，因此从实际角度来看，对于有限且相对较小的数据点，我们可能得到几乎等同的后验分布。
- en: How fast posteriors converge to the same distribution depends on the data and
    the model. We can see that the posteriors arising from the black prior (Uniform)
    and gray prior (biased towards tails) converge faster to almost the same distribution,
    while it takes longer for the dark gray posterior (the one arising from the concentrated
    prior). Even after 150 trials, it is somehow easy to recognize the dark gray posterior
    as a different distribution from the two others.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后验收敛到相同分布的速度取决于数据和模型。我们可以看到，从黑色先验（均匀分布）和灰色先验（偏向尾部）得到的后验分布收敛得更快，几乎相同，而从深灰色先验（偏向集中分布）得到的后验分布则收敛得较慢。即使经过150次试验，仍然很容易识别出深灰色后验分布与另外两个分布的区别。
- en: Something not obvious from the figure is that we will get the same result if
    we update the posterior sequentially as if we do it all at once. We can compute
    the posterior 150 times, each time adding one more observation and using the obtained
    posterior as the new prior, or we can just compute one posterior for the 150 tosses
    at once. The result will be exactly the same. This feature not only makes perfect
    sense, but it also leads to a natural way of updating our estimations when we
    get new data, a situation common in many data-analysis problems.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从图中不容易看出来的一点是，如果我们按顺序更新后验，就像一次性计算后验一样，最终会得到相同的结果。我们可以计算150次后验，每次加入一个新观察值，并将获得的后验作为新的先验，或者我们可以一次性计算150次掷硬币的后验。结果将完全相同。这个特性不仅非常合理，而且为我们提供了一种在获取新数据时更新估计值的自然方法，这种情况在许多数据分析问题中都很常见。
- en: 1.7.5 The influence of the prior
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7.5 先验的影响
- en: From the preceding example, it is clear that priors can influence inferences.
    That’s fine – priors are supposed to do that. Maybe it would be better to not
    have priors at all. That would make modeling easier, right? Well, not necessarily.
    If you are not setting the prior, someone else will be doing it for you. Sometimes
    this is fine – *default priors* can be useful and have their place – but sometimes
    it is better to have more control. Let me explain.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的例子可以清楚地看出，先验可以影响推断。这是正常的——先验本来就应该这样做。也许最好根本不设置先验，那样建模不就更简单了吗？嗯，不一定。如果你不设置先验，别人会为你设置。有时这没问题——*默认先验*是有用的，也有它的作用——但有时最好能有更多的控制权。让我来解释一下。
- en: 'We can think that every (statistical) model, Bayesian or not, has some kind
    of prior, even if the prior is not set explicitly. For instance, many procedures
    typically used in frequentist statistics can be seen as special cases of a Bayesian
    model under certain conditions, such as flat priors. One common way to estimate
    parameters is known as maximum likelihood; this method avoids setting a prior
    and works just by finding the single value maximizing the likelihood. This value
    is usually notated by adding a little hat on top of the name of the parameter
    we are estimating, such as ![](img/hat_theta.png). Contrary to the posterior estimate,
    which is a distribution, ![](img/hat_theta.png) is a point estimate, a number.
    For the coin-flipping problem, we can compute it analytically:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以认为每一个（统计）模型，不论是否是贝叶斯模型，都有某种类型的先验，即使先验没有明确设置。例如，许多在频率派统计中常用的程序可以看作是在某些条件下（如平坦先验）贝叶斯模型的特例。一种常见的参数估计方法被称为最大似然估计；这种方法避免设置先验，只通过找到最大化似然的单一值来工作。这个值通常通过在我们估计的参数名称上方加一个小帽子来表示，例如![](img/hat_theta.png)。与后验估计不同，后验估计是一个分布，而![](img/hat_theta.png)是一个点估计，是一个数值。对于抛硬币问题，我们可以通过解析方法计算出来：
- en: '![ y ˆθ = -- N ](img/file53.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![ y ˆθ = -- N ](img/file53.jpg)'
- en: If you go back to *Figure [1.12](#x1-35014r12)*, you will be able to check for
    yourself that the mode of the black posterior (the one corresponding to the uniform/flat
    prior) agrees with the values of ![](img/hat_theta.png), computed for each subplot.
    This is not a coincidence; it is a consequence of the fact that setting a Uniform
    prior and then taking the mode of the posterior is equivalent to maximum likelihood.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回到*图 [1.12](#x1-35014r12)*，你将能自己检查出黑色后验的模态（即与均匀/平坦先验对应的那个模态）与每个子图中计算得到的![](img/hat_theta.png)值一致。这不是巧合；这是因为设置均匀先验后，再取后验的模态相当于最大似然估计。
- en: We cannot avoid priors, but if we include them in our analysis, we can get some
    potential benefits. The most direct benefit is that we get a posterior distribution,
    which is a distribution of plausible values and not only the most probable ones.
    Having a distribution can be more informative than a single-point estimate, as
    we saw the width of the distribution is related to the uncertainty we have for
    the estimate. Another benefit is that computing the posteriors means to average
    over the prior. This can lead to models that are more difficult to overfit and
    more robust predictions [[Wilson and Izmailov](Bibliography.xhtml#Xwilson_2022), [2022](Bibliography.xhtml#Xwilson_2022)].
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法避免先验，但如果将其纳入分析中，我们可以获得一些潜在的好处。最直接的好处是我们得到一个后验分布，它是一个合理值的分布，而不仅仅是最可能的值。拥有一个分布比单一的点估计更具信息性，正如我们所看到的，分布的宽度与我们对估计的不确定性有关。另一个好处是，计算后验意味着对先验进行平均。这可以导致更难以过拟合的模型和更稳健的预测[[Wilson
    和 Izmailov](Bibliography.xhtml#Xwilson_2022)， [2022](Bibliography.xhtml#Xwilson_2022)]。
- en: Priors can bring us other benefits. Starting in the next chapter, we are going
    to use numerical methods to get posteriors. These methods feel like magic, until
    they don’t. The folk theorem of statistical computing states, ”When you have computational
    problems, often there’s a problem with your model” [[Gelman](Bibliography.xhtml#Xgelman_folk_2008), [2008](Bibliography.xhtml#Xgelman_folk_2008)].
    Sometimes a wise choice of prior can make inference easier or faster. It is important
    to remark that we are not advocating for setting priors specifically to make inference
    faster, but it is often the case that by thinking about priors, we can get faster
    models.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 先验分布能为我们带来其他好处。从下一章开始，我们将使用数值方法来获得后验分布。这些方法看起来像魔法，直到它们不再有效。统计计算的民间定理指出：“当你遇到计算问题时，通常是模型出了问题”[[Gelman](Bibliography.xhtml#Xgelman_folk_2008)，[2008](Bibliography.xhtml#Xgelman_folk_2008)]。有时候，明智的先验选择可以使推断变得更容易或更快速。需要指出的是，我们并不提倡为了加速推断而特意设置先验，但通常情况下，通过考虑先验，我们可以得到更快速的模型。
- en: One advantage of priors, one that is sometimes overlooked, is that having to
    think about priors can *force us* to think a little bit deeper about the problem
    we are trying to solve and the data we have. Sometimes the modeling process leads
    to a better understanding by itself irrespective of how well we end and fit the
    data or make predictions. By being explicit about priors, we get more transparent
    models, meaning they’re easier to criticize, debug (in a broad sense of the word),
    explain to others, and hopefully improve.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 先验的一个优点，有时被忽视了，就是必须考虑先验可能*迫使我们*更深入地思考我们要解决的问题以及我们所拥有的数据。有时候，建模过程本身就能带来更好的理解，不管我们最终如何拟合数据或做出预测。通过明确先验，我们能够得到更透明的模型，这意味着这些模型更容易被批评、调试（广义上讲）、向他人解释，并且有可能得到改善。
- en: 1.8 How to choose priors
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.8 如何选择先验
- en: Newcomers to Bayesian analysis (as well as detractors of this paradigm) are
    generally a little nervous about how to choose priors. Usually, they are afraid
    that the prior distribution will not let the data speak for itself! That’s OK,
    but we have to remember that data does not speak; at best, data murmurs. We can
    only make sense of data in the context of our models, including mathematical and
    mental models. There are plenty of examples in the history of science where the
    same data led people to think differently about the same topics, and this can
    happen even if you base your opinions on formal models.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 初学贝叶斯分析的人（以及该范式的反对者）通常对如何选择先验感到有些紧张。通常，他们担心先验分布会使数据无法自主表达！没关系，但我们必须记住，数据并不会“说话”；充其量，数据只是低声细语。我们只能在模型的上下文中理解数据，包括数学模型和心理模型。科学史上有很多例子表明，相同的数据曾让人们对相同的话题产生不同的看法，即便你基于正式模型来形成观点，也会发生这种情况。
- en: Some people like the idea of using non-informative priors (also known as flat,
    vague, or diffuse priors). These priors have the least possible amount of impact
    on the analysis. While it is possible to use them for some problems deriving truly
    non-informative priors can be hard or just impossible. Additionally, we generally
    can do better as we usually have some prior information.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人喜欢使用非信息性先验（也称为平坦的、模糊的或扩散的先验）这一想法。这些先验对分析的影响最小。虽然在某些问题中使用它们是可行的，但真正推导出非信息性先验是很困难的，甚至是不可能的。此外，通常我们能够做得更好，因为我们通常拥有一些先验信息。
- en: Throughout this book, we will follow the recommendations of Gelman, McElreath,
    Kruschke, and many others, and we will prefer weakly informative priors. For many
    problems, we often know something about the values a parameter can take. We may
    know that a parameter is restricted to being positive, or we may know the approximate
    range it can take, or whether we expect the value to be close to zero or below/above
    some value. In such cases, we can use priors to put some weak information in our
    models without being afraid of being too pushy. Because these priors work to keep
    the posterior distribution within certain reasonable bounds, they are also known
    as regularizing priors.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将遵循Gelman、McElreath、Kruschke等人的建议，并偏好弱信息先验。对于许多问题，我们通常对一个参数可能取的值有所了解。我们可能知道某个参数只能取正值，或者知道它可能的范围，或者预期它接近零或在某个值的上下方。在这种情况下，我们可以使用先验来在模型中引入一些弱信息，而不必担心过于强势。因为这些先验有助于保持后验分布在合理的范围内，所以它们也被称为正则化先验。
- en: Informative priors are very strong priors that convey a lot of information.
    Using them is also a valid option. Depending on your problem, it could be easy
    or not to find good-quality information from your domain knowledge and turn it
    into priors. I used to work on structural bioinformatics. In this field, people
    have been using, in Bayesian and non-Bayesian ways, all the prior information
    they could get to study and predict the structure of proteins. This is reasonable
    because we have been collecting data from thousands of carefully designed experiments
    for decades and hence we have a great amount of trustworthy prior information
    at our disposal. Not using it would be absurd! There is nothing “objective” or
    “scientific” about throwing away valuable information. If you have reliable prior
    information, you should use it. Imagine if every time an automotive engineer had
    to design a new car, they had to start from scratch and reinvent the combustion
    engine, the wheel, and for that matter, the whole concept of a car.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 信息性先验是非常强的先验，能传递大量信息。使用它们也是一个有效的选项。根据你的问题，从领域知识中找到优质的信息并将其转化为先验可能容易，也可能不容易。我曾经从事结构生物信息学工作。在这个领域，大家一直在使用贝叶斯和非贝叶斯方法，利用所有能够获取的先验信息来研究和预测蛋白质的结构。这是合理的，因为几十年来，我们通过数千个精心设计的实验收集了大量数据，因此我们手头有大量可信的先验信息。不使用这些信息简直荒谬！抛弃有价值的信息，绝对没有什么“客观”或“科学”可言。如果你有可靠的先验信息，就应该使用它。试想一下，如果每次汽车工程师需要设计一辆新车时，都必须从头开始，重新发明内燃机、车轮，甚至是汽车的基本概念，那该多么浪费时间！
- en: 'PreliZ is a very new Python library for prior elicitation [[Mikkola et al.](Bibliography.xhtml#Xmikkola_2021), [2023](Bibliography.xhtml#Xmikkola_2021), [Icazatti
    et al.](Bibliography.xhtml#Xicazatti2023), [2023](Bibliography.xhtml#Xicazatti2023)].
    Its mission is to help you to elicit, represent, and visualize your prior knowledge.
    For instance, we can ask PreliZ to compute the parameters of a distribution satisfying
    a set of constraints. Let’s say we want to find the Beta distribution with 90%
    of the mass between 0.1 and 0.7, then we can write:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: PreliZ 是一个全新的 Python 库，用于先验知识的引出 [[Mikkola et al.](Bibliography.xhtml#Xmikkola_2021),
    [2023](Bibliography.xhtml#Xmikkola_2021), [Icazatti et al.](Bibliography.xhtml#Xicazatti2023),
    [2023](Bibliography.xhtml#Xicazatti2023)]。它的使命是帮助你引出、表示和可视化你的先验知识。例如，我们可以让 PreliZ
    计算一个满足一组约束条件的分布的参数。假设我们想找到一个 Beta 分布，其中 90% 的质量位于 0.1 和 0.7 之间，那么我们可以写：
- en: '**Code 1.7**'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 1.7**'
- en: '[PRE6]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The result is a Beta distribution with parameters *α* = 2*.*5 and *β* = 3*.*6
    (rounded to the first decimal point). The `pz.maxent` function computes the **maximum**
    **entropy** distribution given the constraints we specified. Why maximum entropy
    distribution? Because that is equivalent to computing the least informative distribution
    under those constraints. By default, PreliZ will plot the distribution as shown
    here:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个具有参数*α* = 2*.*5 和 *β* = 3*.*6（四舍五入到小数点后一位）的 Beta 分布。`pz.maxent` 函数计算了在我们指定约束条件下的**最大**
    **熵**分布。为什么是最大熵分布？因为这相当于在这些约束条件下计算最不具信息量的分布。默认情况下，PreliZ 会绘制如下所示的分布：
- en: '![PIC](img/file54.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file54.png)'
- en: '**Figure 1.13**: Maximum entropy Beta distribution with 90% of the mass between
    0.1 and 0.7'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.13**：最大熵 Beta 分布，90% 的质量位于 0.1 和 0.7 之间'
- en: As eliciting prior has many facets, PreliZ offers many other ways to elicit
    priors. If you are interested in learning more about PreliZ, you can check the
    documentation at [https://preliz.readthedocs.io](https://preliz.readthedocs.io).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 由于先验引出有很多方面，PreliZ 提供了许多其他方法来引出先验。如果你有兴趣了解更多关于 PreliZ 的信息，可以查看 [https://preliz.readthedocs.io](https://preliz.readthedocs.io)
    上的文档。
- en: Building models is an iterative process; sometimes the iteration takes a few
    minutes, and sometimes it could take years. Reproducibility matters and transparent
    assumptions in a model contribute to it. We are free to use more than one prior
    (or likelihood) for a given analysis if we are not sure about any special one;
    exploring the effect of different priors can also bring valuable information to
    the table. Part of the modeling process is about questioning assumptions, and
    priors (and likelihoods) are just that. Different assumptions will lead to different
    models and probably different results. By using data and our domain knowledge
    of the problem, we will be able to compare models and, if necessary, decide on
    a winner. *Chapter [5](CH05.xhtml#x1-950005)* will be devoted to this issue. Since
    priors have a central role in Bayesian statistics, we will keep discussing them
    as we face new problems. So if you have doubts and feel a little bit confused
    about this discussion, just keep calm and don’t worry, people have been confused
    for decades and the discussion is still going on.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型是一个迭代过程；有时迭代只需几分钟，有时则可能需要几年。可重复性很重要，模型中的透明假设有助于提高其可重复性。如果我们对某个特定的先验（或似然）没有把握，我们可以自由地为给定的分析使用多个先验（或似然）；探索不同先验的效果也能带来有价值的信息。建模过程的一部分是质疑假设，先验（和似然）正是其中的一部分。不同的假设将导致不同的模型，可能还会得出不同的结果。通过使用数据和我们对问题的领域知识，我们能够比较不同的模型，并在必要时决定一个优胜者。*第
    [5](CH05.xhtml#x1-950005) 章*将专门讨论这个问题。由于先验在贝叶斯统计中的核心作用，我们将在面对新问题时继续讨论它们。因此，如果你对这个讨论有疑问并感到有些困惑，不用担心，保持冷静，别着急，很多人也困惑了几十年，这个讨论仍在继续。
- en: 1.9 Communicating a Bayesian analysis
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.9 贝叶斯分析的沟通
- en: Creating reports and communicating results is central to the practice of statistics
    and data science. In this section, we will briefly discuss some of the peculiarities
    of this task when working with Bayesian models. In future chapters, we will keep
    looking at examples of this important matter.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 创建报告和传达结果是统计学和数据科学实践的核心内容。在本节中，我们将简要讨论在使用贝叶斯模型时，进行这项任务的一些特殊之处。在未来的章节中，我们将继续讨论这个重要问题的例子。
- en: 1.9.1 Model notation and visualization
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.9.1 模型符号与可视化
- en: 'If you want to communicate the results of an analysis, you should also communicate
    the model you used. A common notation to succinctly represent probabilistic models
    is:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想传达分析结果，也应该传达你所使用的模型。表示概率模型的一种常见符号是：
- en: '|  | *θ* ∼ Beta(**α*,*β**) |  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  | *θ* ∼ Beta(**α*,*β**) |  |'
- en: '|  | *y* ∼ Bin(*n* = 1*,p* = *θ*) |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | *y* ∼ Bin(*n* = 1*,p* = *θ*) |  |'
- en: This is just the model we use for the coin-flip example. As you may remember,
    the ∼ symbol indicates that the variable on the left of it is a random variable
    distributed according to the distribution on the right. In many contexts, this
    symbol is used to indicate that a variable takes *approximately* some value, but
    when talking about probabilistic models, we will read this symbol out loud, saying
    *is distributed as*. Thus, we can say *θ* is distributed as a Beta with parameters
    *α* and *β*, and *y* is distributed as a Binomial with parameters *n* = 1 and
    *p* = *θ*. The very same model can be represented graphically using Kruschke diagrams
    as in *Figure [1.14](#x1-39001r14)*.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们用于抛硬币示例的模型。如你所记得，∼ 符号表示它左边的变量是一个随机变量，按照右边的分布进行分布。在许多上下文中，这个符号用来表示某个变量*大致*取某个值，但在谈论概率模型时，我们会把这个符号读作
    *按……分布*。因此，我们可以说 *θ* 按 Beta 分布，参数为 *α* 和 *β*，而 *y* 按 Binomial 分布，参数为 *n* = 1 和
    *p* = *θ*。同样的模型可以通过克鲁什克图形象地表示，如 *图 [1.14](#x1-39001r14)* 所示。
- en: '![PIC](img/file55.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file55.png)'
- en: '**Figure 1.14**: A Kruschke diagram of a BetaBinomial model'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.14**：BetaBinomial 模型的克鲁什克图'
- en: On the first level, we have the prior that generates the values for *θ*, then
    the likelihood, and on the last line, the data, *y*. Arrows indicate the relationship
    between variables and the symbol ∼ indicates the stochastic nature of the variables.
    All Kruschke diagrams in the book were made using the templates provided by Rasmus
    Bååth ( [http://www.sumsar.net/blog/2013/10/diy-kruschke-style-diagrams/](http://www.sumsar.net/blog/2013/10/diy-kruschke-style-diagrams/)).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一层，我们有生成 *θ* 值的先验，然后是似然，最后一行是数据 *y*。箭头表示变量之间的关系，符号 ∼ 表示变量的随机性质。书中的所有克鲁什克图都是使用
    Rasmus Bååth 提供的模板制作的（[http://www.sumsar.net/blog/2013/10/diy-kruschke-style-diagrams/](http://www.sumsar.net/blog/2013/10/diy-kruschke-style-diagrams/)）。
- en: 1.9.2 Summarizing the posterior
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.9.2 总结后验
- en: The result of a Bayesian analysis is a posterior distribution, and all the information
    about the parameters (given a model and dataset) is contained in the posterior
    distribution. Thus, by summarizing the posterior, we are summarizing the logical
    consequences of a model and data. A common practice is to report, for each parameter,
    the mean (or mode or median) to have an idea of the location of the distribution
    and some measure of dispersion, such as the standard deviation, to have an idea
    of uncertainty in our estimates. The standard deviation works well for Normal-like
    distributions but can be misleading for other types of distributions, such as
    skewed ones.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯分析的结果是后验分布，所有关于参数的信息（在给定模型和数据集的情况下）都包含在后验分布中。因此，通过总结后验，我们实际上是在总结模型和数据的逻辑结果。一种常见做法是报告每个参数的均值（或众数或中位数），以了解分布的位置，同时报告一些离散度量（如标准差），以了解我们估计的不确定性。标准差对于类似正态分布的分布效果良好，但对于其他类型的分布（如偏斜分布）可能会产生误导。
- en: A commonly used device to summarize the spread of a posterior distribution is
    to use a **Highest-Density Interval** (**HDI**). An HDI is the shortest interval
    containing a given portion of the probability density. If we say that the 95%
    HDI for some analysis is [2*,*5], we mean that according to our data and model,
    the parameter in question is between 2 and 5 with a probability of 0.95\. There
    is nothing special about choosing 95%, 50%, or any other value. We are free to
    choose the 82% HDI interval if we like. Ideally, justifications should be context-dependent
    and not automatic, but it is okay to settle on some common value like 95%. As
    a friendly reminder of the arbitrary nature of this choice, the ArviZ default
    is 94%.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常用的设备来总结后验分布的广度是使用**最高密度区间**（**HDI**）。HDI 是包含给定概率密度部分的最短区间。如果我们说某个分析的 95%
    HDI 是 [2*,*5]，我们意味着根据我们的数据和模型，相关参数的值介于 2 到 5 之间，且其概率为 0.95。选择 95%、50% 或其他任何值并没有什么特别之处。我们可以自由选择
    82% 的 HDI 区间。如果愿意，理想情况下，选择的依据应根据上下文而定，而不是自动的，但选择一个常见值（如 95%）也没问题。为了提醒这种选择的任意性，ArviZ
    的默认值是 94%。
- en: 'ArviZ is a Python package for exploratory analysis of Bayesian models, and
    it has many functions to help us summarize the posterior. One of those functions
    is `az.plot_posterior`, which we can use to generate a plot with the mean and
    HDI of *θ*. The distribution does not need to be a posterior distribution; any
    distribution will work. *Figure [1.15](#x1-40007r15)* shows the result for a random
    sample from a Beta distribution:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ArviZ 是一个用于贝叶斯模型探索性分析的 Python 包，提供了许多帮助我们总结后验的功能。其中一个功能是 `az.plot_posterior`，我们可以使用它生成一个包含*θ*的均值和
    HDI 的图表。分布不必是后验分布，任何分布都可以使用。*图 [1.15](#x1-40007r15)* 显示了来自 Beta 分布的随机样本的结果：
- en: '**Code 1.8**'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 1.8**'
- en: '[PRE7]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![PIC](img/file56.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file56.png)'
- en: '**Figure 1.15**: A KDE of a sample from a Beta distribution with its mean and
    94% HDI'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.15**：来自 Beta 分布的样本的 KDE 图，包含其均值和 94% HDI'
- en: Not Confidence Intervals
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 不是置信区间
- en: If you are familiar with the frequentist paradigm, please note that HDIs are
    not the same as confidence intervals. In the frequentist framework, parameters
    are fixed by design; a frequentist confidence interval either contains or does
    not contain the true value of a parameter. In the Bayesian framework, parameters
    are random variables, and thus we can talk about the probability of a parameter
    having specific values or being inside some interval. The unintuitive nature of
    confident intervals makes them easily misinterpreted and people often talk about
    frequentist confidence intervals as if they were Bayesian credible intervals.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉频率学派的范式，请注意，HDI 与置信区间不同。在频率学派框架下，参数是由设计固定的；频率学派的置信区间要么包含真实参数值，要么不包含。而在贝叶斯框架中，参数是随机变量，因此我们可以谈论某个参数具有特定值或位于某个区间内的概率。置信区间的直觉性较差，容易被误解，人们常常把频率学派的置信区间当作贝叶斯可信区间来讨论。
- en: 1.10 Summary
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.10 总结
- en: We began our Bayesian journey with a very brief discussion of statistical modeling,
    probabilities, conditional probabilities, random variables, probability distributions
    and Bayes’ theorem. We then used the coin-flipping problem as an excuse to introduce
    basic aspects of Bayesian modeling and data analysis. We used this classic toy
    example to convey some of the most important ideas of Bayesian statistics, such
    as using probability distributions to build models and represent uncertainties.
    We tried to demystify the use of priors and put them on an equal footing with
    other elements that are part of the modeling process, such as the likelihood,
    or even more meta-questions, such as why we are trying to solve a particular problem
    in the first place.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以简短的统计建模讨论开始了贝叶斯之旅，内容包括概率、条件概率、随机变量、概率分布和贝叶斯定理。然后我们用抛硬币问题作为借口，引入了贝叶斯建模和数据分析的基本概念。我们利用这个经典的玩具示例传达了贝叶斯统计学中一些最重要的思想，比如使用概率分布来构建模型并表示不确定性。我们试图揭开先验的神秘面纱，并将其与模型过程中的其他元素（如似然性）平等对待，甚至涉及到更多的元问题，比如我们为何要解决特定问题。
- en: 'We ended the chapter by discussing the interpretation and communication of
    the results of a Bayesian analysis. We assume there is a true distribution that
    in general is unknown (and in principle also unknowable), from which we get a
    finite sample, either by doing an experiment, a survey, an observation, or a simulation.
    To learn something from the true distribution, given that we have only observed
    a sample, we build a probabilistic model. A probabilistic model has two basic
    ingredients: a prior and a likelihood. Using the model and the sample, we perform
    Bayesian inference and obtain a posterior distribution; this distribution encapsulates
    all the information about a problem, given our model and data. From a Bayesian
    perspective, the posterior distribution is the main object of interest and everything
    else is derived from it, including predictions in the form of a posterior predictive
    distribution. As the posterior distribution (and any other derived quantity from
    it) is a consequence of the model and data, the usefulness of Bayesian inferences
    is restricted by the quality of models and data. Finally, we briefly summarized
    the main aspects of doing Bayesian data analysis. Throughout the rest of this
    book, we will revisit these ideas to absorb them and use them as the scaffold
    of more advanced concepts.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章最后讨论了贝叶斯分析结果的解释和沟通。我们假设存在一个真实的分布，这个分布通常是未知的（原则上也无法知道），我们从中获取一个有限的样本，可能是通过实验、调查、观察或模拟获得的。为了从真实分布中学习一些东西，鉴于我们只能观察到一个样本，我们构建了一个概率模型。概率模型有两个基本组成部分：先验和似然。使用模型和样本，我们进行贝叶斯推断并得到后验分布；这个分布封装了关于问题的所有信息，基于我们的模型和数据。从贝叶斯的角度看，后验分布是最重要的对象，一切其他内容都从它中推导出来，包括以后验预测分布形式呈现的预测。由于后验分布（以及从中推导的任何其他量）是模型和数据的结果，因此贝叶斯推断的有用性受到模型和数据质量的限制。最后，我们简要总结了贝叶斯数据分析的主要方面。在本书的其余部分，我们将再次回顾这些思想，将其吸收并作为更高级概念的框架。
- en: In the next chapter, we will introduce PyMC, which is a Python library for Bayesian
    modeling and probabilistic machine learning and will use more features from ArviZ,
    a Python library for the exploratory analysis of Bayesian models, and PreliZ a
    Python library for prior elicitation.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将介绍PyMC，它是一个用于贝叶斯建模和概率机器学习的Python库，还会使用更多来自ArviZ的特性，这是一个用于贝叶斯模型探索性分析的Python库，以及PreliZ，这是一个用于先验引导的Python库。
- en: 1.11 Exercises
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.11 练习
- en: 'We do not know whether the brain works in a Bayesian way, in an approximately
    Bayesian fashion, or maybe some evolutionary (more or less) optimized heuristics.
    Nevertheless, we know that we learn by exposing ourselves to data, examples, and
    exercises… Well you may say that humans never learn, given our record as a species
    on subjects such as wars or economic systems that prioritize profit and not people’s
    well-being... Anyway, I recommend you do the proposed exercises at the end of
    each chapter:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不知道大脑是否以贝叶斯方式工作，或以大致贝叶斯方式工作，亦或是某种进化（或多或少）优化的启发式方法。尽管如此，我们知道通过接触数据、示例和练习来学习……你可能会说，人类从未真正学习，看看我们作为物种在战争或经济体系等方面的表现，这些体系优先考虑利润而非人民的福祉……不管怎样，我建议你在每章结束时做一下所提的练习：
- en: 'Suppose you have a jar with 4 jelly beans: 2 are strawberry-flavored, 1 is
    blueberry-flavored, and 1 is cinnamon-flavored. You draw one jelly bean at random
    from the jar.'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你有一个罐子，里面有4颗果冻豆：2颗是草莓味的，1颗是蓝莓味的，1颗是肉桂味的。你从罐子里随机抽取一颗果冻豆。
- en: What is the sample space for this experiment?
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个实验的样本空间是什么？
- en: We define event *A* as *the jelly bean drawn is strawberry-flavored* and event
    *B* as *The jelly bean drawn is not cinnamon-flavored*. What are the probabilities
    of events *A* and *B*?
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将事件*A*定义为*抽到的果冻豆是草莓味的*，将事件*B*定义为*抽到的果冻豆不是肉桂味的*。事件*A*和*B*的概率分别是多少？
- en: Are events *A* and *B* mutually exclusive? Why or why not?
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 事件*A*和*B*是互斥事件吗？为什么或者为什么不？
- en: 'Previously, we defined a Python function `P` to compute the probability of
    an event using the naive definition of probability. Generalize that function to
    compute the probability of events when they are not all equally likely. Use this
    new function to compute the probability of events *A* and *B* from the previous
    exercise. Hint: you can pass a third argument with the probability of each event.'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之前，我们定义了一个Python函数`P`来使用概率的简单定义计算事件的概率。将该函数推广，以计算当事件的概率不完全相等时的事件概率。使用这个新函数计算前面练习中事件*A*和*B*的概率。提示：你可以传递一个第三个参数来表示每个事件的概率。
- en: Use PreliZ to explore different parameters for the BetaBinomial and Gaussian
    distributions. Use the methods `plot_pdf`, `plot_cdf`, and `plot_interactive`.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用PreliZ探索BetaBinomial和高斯分布的不同参数。使用`plot_pdf`、`plot_cdf`和`plot_interactive`方法。
- en: We discussed the probability mass/density functions and the cumulative density
    function. But there are other ways to represent functions like the percentile
    point function ppf. Using the `plot_ppf` method of PreliZ, plot the percentile
    point function for the BetaBinomial and Gaussian distributions. Can you explain
    how the ppf is related to the cdf and pmf/pdf?
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们讨论了概率质量/密度函数和累积分布函数。但也有其他方式表示函数，比如百分位点函数ppf。使用PreliZ的`plot_ppf`方法，绘制BetaBinomial和高斯分布的百分位点函数。你能解释ppf是如何与cdf和pmf/pdf相关的吗？
- en: 'From the following expressions, which one corresponds to: the probability of
    being sunny given that it is 9^(th) of July of 1816?'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下表达式中，哪一个对应于：在1816年7月9日已知的情况下，晴天的概率？
- en: '*p*(sunny)'
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*p*(sunny)'
- en: '*p*(sunny|July)'
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*p*(sunny|July)'
- en: '*p*(sunny|9 of July of 1816)'
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*p*(sunny|9 of July of 1816)'
- en: '*p*(9^(th) of July of 1816|sunny)'
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*p*(9^(th) of July of 1816|sunny)'
- en: '![p(sunny,9th of July-of 1816) p(9th of July of 1816)](img/file57.jpg)'
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_IMG
  zh: '![p(sunny,9th of July-of 1816) p(9th of July of 1816)](img/file57.jpg)'
- en: We showed that the probability of choosing a human at random and picking the
    Pope is not the same as the probability of the Pope being human. In the animated
    series Futurama, the (Space) Pope is a reptile. How does this change your previous
    calculations?
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们展示了随机选择一个人并选出教皇的概率与教皇是人类的概率是不同的。在动画系列《未来都市》中，(太空)教皇是爬行动物。这会如何改变你之前的计算结果？
- en: Following the example in *Figure [1.9](#x1-27006r9)*, use PreliZ to compute
    the moments for the SkewNormal distribution for a different combination of parameters.
    Generate random samples of different sizes, like 10, 100, and 1,000, and see if
    you can recover the values of the first two moments (mean and variance) from the
    samples. What do you observe?
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照*图 [1.9](#x1-27006r9)*中的示例，使用PreliZ计算SkewNormal分布的矩，参数组合不同。生成不同大小的随机样本，例如10、100和1,000，看看是否能从样本中恢复出前两个矩的值（均值和方差）。你观察到什么？
- en: Repeat the previous exercise for the Student’s T distribution. Try values of
    *ν* like 2, 3, 500\. What do you observe?
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对学生的T分布重复之前的练习。尝试*ν*的值，比如2、3、500。你观察到什么？
- en: 'In the following definition of a probabilistic model, identify the prior and
    the likelihood:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下的概率模型定义中，识别先验和似然：
- en: '![Y ∼ Normal (μ,σ) μ ∼ Normal (0,2) σ ∼ HalfNormal (0.75 ) ](img/file58.jpg)'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Y ∼ Normal (μ,σ) μ ∼ Normal (0,2) σ ∼ HalfNormal (0.75 ) ](img/file58.jpg)'
- en: In the previous model, how many parameters will the posterior have? Compare
    it with the model for the coin-flipping problem.
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的模型中，后验将有多少个参数？与掷硬币问题的模型进行比较。
- en: Write Bayes’ theorem for the model in exercise 9.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为第9题中的模型写出贝叶斯定理。
- en: Let’s suppose that we have two coins; when we toss the first coin, half of the
    time it lands on tails and half of the time on heads. The other coin is a loaded
    coin that always lands on heads. If we take one of the coins at random and get
    a head, what is the probability that this coin is the unfair one?
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们有两枚硬币；当我们掷第一枚硬币时，正面朝上的概率为一半，反面朝上的概率也为一半。另一枚硬币是一枚加重硬币，总是朝上正面。如果我们随机选择其中一枚硬币并且掷出正面，那么这枚硬币是不公平的概率是多少？
- en: Try re-plotting *Figure [1.12](#x1-35014r12)* using other priors (`beta_params`)
    and other data (`trials` and `data`).
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用其他先验（`beta_params`）和其他数据（`trials` 和 `data`）重新绘制*图 [1.12](#x1-35014r12)*。
- en: 'Read about the Cromwell rule on Wikipedia: [https://en.wikipedia.org/wiki/Cromwell%27s_rule](https://en.wikipedia.org/wiki/Cromwell%27s_rule).'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读关于克伦威尔法则的 Wikipedia 文章：[https://en.wikipedia.org/wiki/Cromwell%27s_rule](https://en.wikipedia.org/wiki/Cromwell%27s_rule)。
- en: 'Read about probabilities and the Dutch book on Wikipedia: [https://en.wikipedia.org/wiki/Dutch_book](https://en.wikipedia.org/wiki/Dutch_book).'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读关于概率和荷兰书的 Wikipedia 文章：[https://en.wikipedia.org/wiki/Dutch_book](https://en.wikipedia.org/wiki/Dutch_book)。
- en: Join our community Discord space
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的社区 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at: [https://packt.link/bayesian](https://packt.link/bayesian)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人一起学习，和超过 5000 名成员共同进步： [https://packt.link/bayesian](https://packt.link/bayesian)
- en: '![PIC](img/file1.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file1.png)'
