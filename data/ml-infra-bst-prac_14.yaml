- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Training and Evaluation of Advanced ML Algorithms – GPT and Autoencoders
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级机器学习算法的训练和评估——GPT和自动编码器
- en: Classical **machine learning** (**ML**) and **neural networks** (**NNs**) are
    very good for classical problems – prediction, classification, and recognition.
    As we learned in the previous chapter, training them requires a moderate amount
    of data, and we train them for specific tasks. However, breakthroughs in ML and
    **artificial intelligence** (**AI**) in the late 2010s and the beginning of 2020s
    were about completely different types of models – **deep learning** (**DL**),
    **Generative Pre-Trained Transformers** (**GPTs**), and **generative** **AI**
    (**GenAI**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的**机器学习**（**ML**）和**神经网络**（**NNs**）非常适合处理经典问题——预测、分类和识别。正如我们在上一章所学，训练它们需要适量的数据，并且我们针对特定任务进行训练。然而，在2010年代末和2020年代初，机器学习（ML）和**人工智能**（**AI**）的突破是关于完全不同类型的模型——**深度学习**（**DL**）、**生成预训练转换器**（**GPTs**）和**生成**
    **AI**（**GenAI**）。
- en: GenAI models provide two advantages – they can generate new data and they can
    provide us with an internal representation of the data that captures the context
    of the data and, to some extent, its semantics. In the previous chapters, we saw
    how we can use existing models for inference and generating simple pieces of text.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 生成AI模型提供了两个优势——它们可以生成新数据，并且可以为我们提供数据的内部表示，该表示捕捉了数据的上下文，并在一定程度上捕捉了其语义。在前几章中，我们看到了如何使用现有模型进行推理和生成简单的文本片段。
- en: In this chapter, we explore how GenAI models work based on GPT and Bidirectional
    Encoder Representations from Transformers (BERT) models. These models are designed
    to generate new data based on the patterns that they were trained on. We also
    look at the concept of autoencoders (AEs), where we train an AE to generate new
    images based on previously trained data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨基于GPT和双向编码器表示转换器（BERT）模型的生成AI模型是如何工作的。这些模型旨在根据它们训练的模式生成新数据。我们还探讨了自动编码器（AEs）的概念，其中我们训练一个AE根据先前训练的数据生成新图像。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: From classical ML models to GenAI
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从经典机器学习模型到生成AI
- en: The theory behind GenAI models – AEs and transformers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成AI模型背后的理论——自动编码器（AEs）和转换器
- en: Training and evaluation of a **Robustly Optimized BERT Approach** (**RoBERTa**)
    model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Robustly Optimized BERT Approach**（**RoBERTa**）模型的训练和评估'
- en: Training and evaluation of an AE
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器（AE）的训练和评估
- en: Developing safety cages to prevent models from breaking the entire system
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发安全笼子以防止模型破坏整个系统
- en: From classical ML to GenAI
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从经典机器学习到生成AI
- en: Classical AI, also known as symbolic AI or rule-based AI, emerged as one of
    the earliest schools of thought in the field. It is rooted in the concept of explicitly
    encoding knowledge and using logical rules to manipulate symbols and derive intelligent
    behavior. Classical AI systems are designed to follow predefined rules and algorithms,
    enabling them to solve well-defined problems with precision and determinism. We
    delve into the underlying principles of classical AI, exploring its reliance on
    rule-based systems, expert systems, and logical reasoning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 经典人工智能，也称为符号人工智能或基于规则的AI，是该领域最早的思想流派之一。它基于明确编码知识和使用逻辑规则来操纵符号并推导出智能行为的理念。经典人工智能系统旨在遵循预定义的规则和算法，使它们能够以精确和确定性解决定义明确的问题。我们深入探讨经典人工智能的潜在原则，探索其对基于规则的系统、专家系统和逻辑推理的依赖。
- en: In contrast, GenAI represents a paradigm shift in AI development, capitalizing
    on the power of ML and NNs to create intelligent systems that can generate new
    content, recognize patterns, and make informed decisions. Rather than relying
    on explicit rules and handcrafted knowledge, GenAI leverages data-driven approaches
    to learn from vast amounts of information and infer patterns and relationships.
    We examine the core concepts of GenAI, including DL, NNs, and probabilistic models,
    to unravel its ability to create original content and foster creative problem-solving.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，生成AI代表了人工智能发展的范式转变，利用机器学习（ML）和神经网络（NNs）的力量来创建能够生成新内容、识别模式和做出明智决策的智能系统。生成AI不是依赖于显式规则和手工知识，而是利用数据驱动的方法从大量信息中学习，并推断模式和关系。我们探讨生成AI的核心概念，包括深度学习（DL）、神经网络（NNs）和概率模型，以揭示其创造原创内容并促进创造性问题解决的能力。
- en: One of the examples of a GenAI model is the GPT-3 model. GPT-3 is a state-of-the-art
    language model developed by OpenAI. It is based on the transformer architecture.
    GPT-3 is trained using a technique called **unsupervised learning** (**UL**),
    which enables it to generate coherent and contextually relevant text.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能（GenAI）模型的一个例子是GPT-3模型。GPT-3是由OpenAI开发的最先进的语言模型。它基于转换器架构。GPT-3使用一种称为**无监督学习**（UL）的技术进行训练，这使得它能够生成连贯且上下文相关的文本。
- en: The theory behind advanced models – AEs and transformers
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 先进模型（AE和转换器）背后的理论
- en: One of the large limitations of classical ML models is the access to annotated
    data. Large NNs contain millions (if not billions) of parameters, which means
    that they require equally many labeled data points to be trained correctly. Data
    labeling, also known as annotation, is the most expensive activity in ML, and
    therefore it is the labeling process that becomes the de facto limit of ML models.
    In the early 2010s, the solution to that problem was to use crowdsourcing.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 经典机器学习（ML）模型的一个大局限是访问标注数据。大型神经网络包含数百万（如果不是数十亿）个参数，这意味着它们需要同样数量的标记数据点来正确训练。数据标注，也称为注释，是ML中最昂贵的活动，因此标注过程成为了ML模型的实际限制。在2010年代初，解决这个问题的方法是使用众包。
- en: Crowdsourcing, which is a process of collective data collection (among other
    things), means that we use users of our services to label the data. A CAPTCHA
    is one of the most prominent examples. A CAPTCHA is used when we need to recognize
    images in order to log in to a service. When we introduce new images, every time
    a user needs to recognize these images, we can label a lot of data in a relatively
    short time.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 众包，这是一种集体数据收集的过程（以及其他），意味着我们使用我们服务的用户来标注数据。CAPTCHA是最突出的例子之一。当我们需要识别图像以登录服务时，会使用CAPTCHA。当我们引入新图像时，每次用户需要识别这些图像时，我们可以在相对较短的时间内标注大量数据。
- en: There is, nevertheless, an inherent problem with that process. Well, there are
    a few problems, but the most prominent one is that this process works mostly with
    images or similar kinds of data. It is also a relatively limited process – we
    can only ask users to recognize an image, but not add a semantic map and not draw
    a bounding box over an image. We cannot ask users to assess the similarity of
    images or any other, bit more advanced, task.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个过程本身存在一个固有的问题。好吧，有几个问题，但最突出的问题是这个过程主要与图像或类似类型的数据一起工作。它也是一个相对有限的过程——我们只能要求用户识别图像，但不能添加语义图，也不能在图像上绘制边界框。我们不能要求用户评估图像或任何其他，稍微复杂一些的任务。
- en: Here enter more advanced methods – GenAI and networks such as **generative adversarial
    networks** (**GANs**). These networks are designed to generate data and learn
    which data is like the original data. These networks are very powerful and have
    been used in such applications as the generation of images; for example, in the
    so-called “deep fakes.”
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里引入了更高级的方法——生成式人工智能（GenAI）和如**生成对抗网络**（GANs）之类的网络。这些网络被设计用来生成数据并学习哪些数据类似于原始数据。这些网络非常强大，并被用于如图像生成等应用；例如，在所谓的“深度伪造”中。
- en: AEs
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AEs
- en: One of the main components of such a model is the AE, which is designed to learn
    a compressed representation (encoding) of the input data and then reconstruct
    the original data (decoding) from this compressed representation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的模型的主要组成部分是自动编码器（AE），它被设计用来学习输入数据的压缩表示（编码），然后从这个压缩表示中重建原始数据（解码）。
- en: 'The architecture of an AE (*Figure 11**.1*) consists of two main components:
    an encoder and a decoder. The encoder takes the input data and maps it to a lower-dimensional
    latent space representation, often referred to as the encoding/embedding or latent
    representation. The decoder takes this encoded representation and reconstructs
    the original input data:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器（AE）的架构（**图11.1**.1）由两个主要组件组成：编码器和解码器。编码器接收输入数据并将其映射到一个低维的潜在空间表示，通常被称为编码/嵌入或潜在表示。解码器接收这个编码表示并将其重建为原始输入数据：
- en: '![Figure 11.1 – High-level architecture of AEs](img/B19548_11_1.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图11.1 – 自动编码器的高级架构](img/B19548_11_1.jpg)'
- en: Figure 11.1 – High-level architecture of AEs
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – 自动编码器的高级架构
- en: The objective of an AE is to minimize reconstruction errors, which is the difference
    between the input data and the output of the decoder. By doing so, the AE learns
    to capture the most important features of the input data in the latent representation,
    effectively compressing the information. The most interesting part is the latent
    space or encoding. This part allows this model to learn the representation of
    a complex data point (for example, an image) in a small vector of just a few numbers.
    The latent representation learned by the AE can be considered a compressed representation
    or a low-dimensional embedding of the input data. This compressed representation
    can be used for various purposes, such as data visualization, dimensionality reduction,
    anomaly detection, or as a starting point for other downstream tasks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器（AE）的目标是最小化重建误差，即输入数据与解码器输出之间的差异。通过这样做，AE学习在潜在表示中捕获输入数据的最重要特征，从而有效地压缩信息。最有趣的部分是潜在空间或编码。这一部分允许模型在只有几个数字的小向量中学习复杂数据点（例如，图像）的表示。AE学习的潜在表示可以被视为输入数据的压缩表示或低维嵌入。这种压缩表示可用于各种目的，例如数据可视化、降维、异常检测，或作为其他下游任务的起点。
- en: The encoder part calculates the latent vector, and the decoder part can expand
    it to an image. There are different types of AEs; the most interesting one is
    the **variational AE** (**VAE**), which encodes the parameters of a function that
    can generate new data rather than the representation of the data itself. In this
    way, it can create new data based on the distribution. In fact, it can even create
    completely new types of data by combining different functions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器部分计算潜在向量，解码器部分可以将它扩展成图像。自动编码器有多种类型；最有趣的一种是**变分自动编码器**（**VAE**），它编码的是可以生成新数据的函数的参数，而不是数据的表示本身。这样，它可以根据分布创建新数据。实际上，它甚至可以通过组合不同的函数来创建完全新的数据类型。
- en: Transformers
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换器
- en: In **natural language processing** (**NLP**) tasks, we usually employ a bit
    different type of GenAI – transformers. Transformers revolutionized the field
    of machine translation but have been applied to many other tasks, including language
    understanding and text generation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在**自然语言处理**（**NLP**）任务中，我们通常使用一种略有不同的生成人工智能类型——转换器。转换器彻底改变了机器翻译领域，但已被应用于许多其他任务，包括语言理解和文本生成。
- en: At its core, a transformer employs a self-attention mechanism that allows the
    model to weigh the importance of different words or tokens in a sequence when
    processing them. This attention mechanism enables the model to capture long-range
    dependencies and contextual relationships between words more effectively than
    traditional **recurrent NNs** (**RNNs**) or **convolutional** **NNs** (**CNNs**).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，转换器采用了一种自注意力机制，允许模型在处理序列中的不同单词或标记时，权衡它们的重要性。这种注意力机制使得模型能够比传统的**循环神经网络**（**RNNs**）或**卷积神经网络**（**CNNs**）更有效地捕捉单词之间的长距离依赖关系和上下文关系。
- en: 'Transformers consist of an encoder-decoder structure. The encoder processes
    the input sequence, such as a sentence, and the decoder generates an output sequence,
    often based on the input and a target sequence. Two elements are unique for transformers:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器由编码器-解码器结构组成。编码器处理输入序列，如句子，解码器生成输出序列，通常基于输入和目标序列。转换器有两个独特的元素：
- en: '**Multi-head self-attention (MHSA)**: A mechanism that allows the model to
    attend to different positions in the input sequence simultaneously, capturing
    different types of dependencies. This is an extension to the RNN architecture,
    which was able to connect neurons in the same layer, thus capturing temporal dependencies.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多头自注意力（MHSA**）：一种允许模型同时关注输入序列中不同位置的机制，捕捉不同类型的依赖关系。这是对RNN架构的扩展，它能够连接同一层中的神经元，从而捕捉时间依赖关系。'
- en: '**Positional encoding**: To incorporate positional information into the model,
    positional encoding vectors are added to the input embeddings. These positional
    encodings are based on the tokens and their relative position to one another.
    This mechanism allows us to capture the context of a specific token and therefore
    to capture the basic contextual semantics of the text.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置编码**：为了将位置信息纳入模型，添加了位置编码向量到输入嵌入中。这些位置编码基于标记及其相互之间的相对位置。这种机制允许我们捕捉特定标记的上下文，因此捕捉文本的基本上下文语义。'
- en: '*Figure 11**.2* presents the high-level architecture of transformers:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.2*展示了转换器的高级架构：'
- en: '![Figure 11.2 – High-level architecture of transformers](img/B19548_11_2.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图11.2 – Transformer的高级架构](img/B19548_11_2.jpg)'
- en: Figure 11.2 – High-level architecture of transformers
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2 – Transformer的高级架构
- en: In this architecture, self-attention is a key mechanism that allows the model
    to weigh the importance of different words or tokens in a sequence when processing
    them. The self-attention mechanism is applied independently to each word in the
    input sequence, and it helps capture contextual relationships and dependencies
    between words. The term *head* refers to an independent attention mechanism that
    operates in parallel. Multiple self-attention heads can be used in the transformer
    model to capture different types of relationships (although we do not know what
    these relationships are).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个架构中，自注意力是模型在处理序列中的单词或标记时，权衡不同单词或标记重要性的关键机制。自注意力机制独立应用于输入序列中的每个单词，并有助于捕捉单词之间的上下文关系和依赖。术语*头*指的是并行操作的独立注意力机制。在Transformer模型中可以使用多个自注意力头来捕捉不同类型的关系（尽管我们不知道这些关系是什么）。
- en: Each self-attention head operates by computing attention scores between query
    representations and key representations. These attention scores indicate the importance
    or relevance of each word in the sequence with respect to the others. Attention
    scores are obtained by taking the dot product between the query and key representations,
    followed by applying a softmax function to normalize the scores.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 每个自注意力头通过计算查询表示和键表示之间的注意力分数来操作。这些注意力分数表示序列中每个单词相对于其他单词的重要性或相关性。通过将查询和键表示之间的点积，然后应用softmax函数来归一化分数，获得注意力分数。
- en: The attention scores are then used to weigh the value representations. The weighted
    values are summed together to obtain the output representation for each word in
    the sequence.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用注意力分数来权衡值表示。将加权值相加，以获得序列中每个单词的输出表示。
- en: 'The feed-forward networks in transformers serve two main purposes: feature
    extraction and position-wise representation. The feature extraction extracts higher-level
    features from the self-attention outputs, in a way that is very similar to the
    word-embeddings extraction that we learned previously. By applying non-linear
    transformations, the model can capture complex patterns and dependencies in the
    input sequence. The position-wise representation ensures that the model can learn
    different transformations for each position. It allows the model to learn complex
    representations of the sentences and therefore capture the more complex context
    of each word and sentence.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer中的前馈网络有两个主要作用：特征提取和位置表示。特征提取从自注意力输出中提取高级特征，其方式与我们之前学习过的词嵌入提取非常相似。通过应用非线性变换，模型可以捕捉输入序列中的复杂模式和依赖关系。位置表示确保模型可以学习每个位置的不同变换。它允许模型学习句子的复杂表示，因此捕捉每个单词和句子的更复杂上下文。
- en: The transformer architecture is the basis for modern models such as GPT-3, which
    is a pre-trained generative transformer; that is, a transformer that has been
    pre-trained on a large mass of text. However, it is based on models such as BERT
    and its relatives.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构是现代模型如GPT-3的基础，GPT-3是一个预训练的生成Transformer；也就是说，它已经在大量文本上进行了预训练。然而，它基于BERT及其相关模型。
- en: Training and evaluation of a RoBERTa model
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RoBERTa模型的训练和评估
- en: In general, the training process for GPT-3 involves exposing the model to a
    massive amount of text data from diverse sources, such as books, articles, websites,
    and more. By analyzing the patterns, relationships, and language structures within
    this data, the model learns to predict the likelihood of a word or phrase appearing
    based on the surrounding context. This learning objective is achieved through
    a process known as **masked language modeling** (**MLM**), where certain words
    are randomly masked in the input, and the model is tasked with predicting the
    correct word based on the context.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，GPT-3的训练过程涉及将模型暴露于来自不同来源的大量文本数据，如书籍、文章、网站等。通过分析这些数据中的模式、关系和语言结构，模型学习根据周围上下文预测单词或短语出现的可能性。这种学习目标是通过称为**掩码语言模型**（**MLM**）的过程实现的，其中随机掩码输入中的某些单词，模型的任务是根据上下文预测正确的单词。
- en: In this chapter, we train the RoBERTa model, which is a variation of the now-classical
    BERT model. Instead of using generic sources such as books and *Wikipedia* articles,
    we use programs. To make our training task a bit more specific, let us train a
    model that is capable of “understanding” code from a networking domain – WolfSSL,
    which is an open source implementation of the SSL protocol, used in many embedded
    software devices.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们训练RoBERTa模型，这是现在经典的BERT模型的一个变体。我们不是使用如书籍和*维基百科*文章等通用来源，而是使用程序。为了使我们的训练任务更加具体，让我们训练一个能够“理解”来自网络域的代码的模型——WolfSSL，这是一个SSL协议的开源实现，用于许多嵌入式软件设备。
- en: Once the training is complete, BERT models are capable of generating text by
    leveraging their learned knowledge and the context provided in a given prompt.
    When a user provides a prompt or a partial sentence, the model processes the input
    and generates a response by probabilistically predicting the most likely next
    word based on the context it has learned from the training data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，BERT模型能够通过利用其学习到的知识和给定提示中提供的上下文来生成文本。当用户提供一个提示或部分句子时，模型会处理输入，并通过基于从训练数据中学习到的上下文概率预测最可能的下一个词来生成响应。
- en: When it comes to GPT-3 (and similar) models, it is an extension of the BERT
    model. The generation process in GPT-3 involves multiple layers of attention mechanisms
    within the transformer architecture. These attention mechanisms allow the model
    to focus on relevant parts of the input text and make connections between different
    words and phrases, ensuring coherence and contextuality in the generated output.
    The model generates text by sampling or selecting the most probable next word
    at each step, taking into account previously generated words.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到GPT-3（以及类似的）模型时，它是BERT模型的扩展。GPT-3的生成过程涉及transformer架构内的多层注意力机制。这些注意力机制允许模型关注输入文本的相关部分，并在不同的单词和短语之间建立联系，确保生成输出的连贯性和上下文性。模型通过在每个步骤中采样或选择最可能的下一个词来生成文本，同时考虑到之前生成的单词。
- en: 'So, let us start our training process by preparing the data for training. First,
    we read the dataset:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们通过准备训练数据来开始我们的训练过程。首先，我们读取数据集：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This provides us with the raw training set. In this set, the text file contains
    all the source code from the WolfSSL protocol in one file. We do not have to prepare
    it like this, but it certainly makes the process easier as we only deal with one
    source file. Now, we can train the tokenizer, very similar to what we saw in the
    previous chapters:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了原始训练集。在这个集合中，文本文件包含了一个文件中的所有WolfSSL协议的源代码。我们不必这样准备，但这样做确实使过程更容易，因为我们只需处理一个源文件。现在，我们可以训练分词器，这与我们在前几章中看到的过程非常相似：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first line initializes an instance of the `ByteLevelBPETokenizer` tokenizer
    class. This tokenizer is based on a byte-level version of the **Byte-Pair Encoding**
    (**BPE**) algorithm, which is a popular subword tokenization method. We discussed
    it in the previous chapters.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行初始化`ByteLevelBPETokenizer`分词器类的实例。这个分词器基于**字节对编码**（**BPE**）算法的字节级版本，这是一种流行的子词分词方法。我们已在前几章中讨论过它。
- en: The next line prints a message indicating that the tokenizer training process
    is starting.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行打印一条消息，表明分词器训练过程开始。
- en: 'The `tokenizer.train()` function is called to train the tokenizer. The training
    process takes a few parameters:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`tokenizer.train()`函数来训练分词器。训练过程需要几个参数：
- en: '`files=paths`: This parameter specifies the input files or paths containing
    the text data to train the tokenizer. It expects a list of file paths.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`files=paths`: 此参数指定包含要训练分词器的文本数据的输入文件或路径。它期望一个文件路径列表。'
- en: '`vocab_size=52_000`: This parameter sets the size of the vocabulary; that is,
    the number of unique tokens the tokenizer will generate. In this case, the tokenizer
    will create a vocabulary of 52,000 tokens.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size=52_000`: 此参数设置词汇表的大小；也就是说，分词器将生成的唯一标记的数量。在这种情况下，分词器将创建一个包含52,000个标记的词汇表。'
- en: '`min_frequency=2`: This parameter specifies the minimum frequency a token must
    have in the training data to be included in the vocabulary. Tokens that occur
    less frequently than this threshold will be treated as **out-of-vocabulary** (**OOV**)
    tokens.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_frequency=2`: 此参数指定一个标记必须在训练数据中出现的最小频率，才能包含在词汇表中。低于此阈值的标记将被视为**词汇表外**（**OOV**）标记。'
- en: '`special_tokens=["<s>","<pad>","</s>","<unk>","<mask>"]`: This parameter defines
    a list of special tokens that will be added to the vocabulary. Special tokens
    are commonly used to represent specific meanings or special purposes. In this
    case, the special tokens are `<s>`, `<pad>`, `</s>`, `<unk>`, and `<mask>`. These
    tokens are often used in tasks such as machine translation, text generation, or
    language modeling.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens=["<s>","<pad>","</s>'
- en: 'Once the training process is completed, the tokenizer will have learned the
    vocabulary and will be able to encode and decode text using the trained subword
    units. We can now save the tokenizer using this piece of code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练过程完成，分词器将学习词汇表，并能够使用训练的子词单元对文本进行编码和解码。现在，我们可以使用以下代码保存分词器：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We also test this tokenizer using the following line: `tokenizer.encode("int
    main(int argc,` `void **argv)").tokens`.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用以下行测试此分词器：`tokenizer.encode("int main(int argc,` `void **argv)").tokens`。
- en: 'Now, let us make sure that the tokenizer is comparable with our model in the
    next step. To do that, we need to make sure that the output of the tokenizer never
    exceeds the number of tokens that the model can accept:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们确保在下一步中分词器与我们的模型可比较。为此，我们需要确保分词器的输出永远不会超过模型可以接受的标记数：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we can move over to preparing the model. We do this by importing the predefined
    class from the HuggingFace hub:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始准备模型。我们通过从HuggingFace hub导入预定义的类来完成此操作：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The first line, `from transformers import RobertaConfig`, imports the `RobertaConfig`
    class from the `transformers` library. The `RobertaConfig` class is used to configure
    the RoBERTa model. Next, the code initializes the configuration of the RoBERTa
    model. The parameters passed to the `RobertaConfig` constructor are as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行，`from transformers import RobertaConfig`，从`transformers`库中导入`RobertaConfig`类。`RobertaConfig`类用于配置RoBERTa模型。接下来，代码初始化RoBERTa模型的配置。传递给`RobertaConfig`构造函数的参数如下：
- en: '`vocab_size=52_000`: This parameter sets the size of the vocabulary used by
    the RoBERTa model. It should match the vocabulary size used during the tokenizer
    training. In this case, the tokenizer and the model both have a vocabulary size
    of 52,000, ensuring they are compatible.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size=52_000`: 此参数设置RoBERTa模型使用的词汇表大小。它应与分词器训练期间使用的词汇表大小相匹配。在这种情况下，分词器和模型都具有52,000的词汇表大小，确保它们兼容。'
- en: '`max_position_embeddings=514`: This parameter sets the maximum sequence length
    that the RoBERTa model can handle. It defines the maximum number of tokens in
    a sequence that the model can process. Longer sequences may need to be truncated
    or split into smaller segments. Please note that the input is 514, not 512 as
    the output of the tokenizer. This is caused by the fact that we leave the place
    from the starting and ending tokens.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings=514`: 此参数设置RoBERTa模型可以处理的最大序列长度。它定义了模型可以处理的序列中的最大标记数。较长的序列可能需要截断或分成更小的段。请注意，输入是514，而不是分词器输出的512。这是由于我们从起始和结束标记中留出了位置。'
- en: '`num_attention_heads=12`: This parameter sets the number of attention heads
    in the **multi-head attention** (**MHA**) mechanism of the RoBERTa model. Attention
    heads allow the model to focus on different parts of the input sequence simultaneously.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads=12`: 此参数设置RoBERTa模型中**多头注意力**（**MHA**）机制中的注意力头数量。注意力头允许模型同时关注输入序列的不同部分。'
- en: '`num_hidden_layers=6`: This parameter sets the number of hidden layers in the
    RoBERTa model. These layers contain the learnable parameters of the model and
    are responsible for processing the input data.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers=6`: 此参数设置RoBERTa模型中的隐藏层数量。这些层包含模型的可学习参数，并负责处理输入数据。'
- en: '`type_vocab_size=1`: This parameter sets the size of the token type vocabulary.
    In models such as RoBERTa, which do not use the token type (also called a segment)
    embeddings, this value is typically set to 1.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type_vocab_size=1`: 此参数设置标记类型词汇表的大小。在RoBERTa等不使用标记类型（也称为段）嵌入的模型中，此值通常设置为1。'
- en: The configuration object config stores all these settings and will be used later
    when initializing the actual RoBERTa model. Having the same configuration parameters
    as the tokenizer ensures that the model and tokenizer are compatible and can be
    used together to process text data properly.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象 config 存储了所有这些设置，将在初始化实际的 RoBERTa 模型时使用。与分词器具有相同的配置参数确保了模型和分词器是兼容的，并且可以一起使用来正确处理文本数据。
- en: It is worth noting that this model is rather small, compared to the 175 billion
    parameters of GPT-3\. It has (only) 85 million parameters. However, it can be
    trained on a laptop with a moderately powerful GPU (any NVIDIA GPU with 6 GB of
    VRAM will do). The model is, nevertheless, much larger than the original BERT
    model from 2017, which had only six attention heads and a handful of millions
    of parameters.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，与拥有 1750 亿个参数的 GPT-3 相比，这个模型相当小，它只有（只有）8500万个参数。然而，它可以在配备中等性能 GPU 的笔记本电脑上训练（任何具有
    6GB VRAM 的 NVIDIA GPU 都可以）。尽管如此，该模型仍然比 2017 年的原始 BERT 模型大得多，后者只有六个注意力头和数百万个参数。
- en: 'Once the model is created, we need to initiate it:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 模型创建完成后，我们需要初始化它：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The last two lines print out the number of parameters in the model (a bit over
    85 million) and then the model itself. The output of that model is quite large,
    so we do not present it here.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两行打印出模型中的参数数量（略超过 8500 万）以及模型本身。该模型的输出相当大，因此我们在此不展示。
- en: 'Now that the model is ready, we need to go back to the dataset and prepare
    it for training. The simplest way is to reuse the previously trained tokenizer
    by reading it back from the folder, but with the changed class of that tokenizer
    so that it fits the model:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经准备好了，我们需要回到数据集并为其准备训练。最简单的方法是重用之前训练好的分词器，通过从文件夹中读取它，但需要更改分词器的类别，以便它适合模型：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once this is done, we can read the dataset:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些操作后，我们可以读取数据集：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The previous code fragment reads the same dataset that we used to train the
    tokenizer. Now, we will use the tokenizer to transform the dataset into a set
    of tokens:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码片段读取了我们用于训练分词器的相同数据集。现在，我们将使用分词器将数据集转换成一组标记：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This takes a moment, but it gives us a moment to also reflect on the fact that
    this code takes advantage of the so-called map-reduce algorithm, which became
    a golden standard for processing large files at the beginning of the 2010s when
    the concept of big data was very popular. It is the `map()` function that utilizes
    that algorithm.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要一点时间，但它也让我们有机会反思这个代码利用了所谓的 map-reduce 算法，该算法在 2010 年代初大数据概念非常流行时成为了处理大型文件的黄金标准。是
    `map()` 函数利用了该算法。
- en: 'Now, we need to prepare the dataset for training by creating so-called masked
    input. Masked input is a set of sentences where words are replaced by the mask
    token (`<mask>` in our case). It can look something like the example in *Figure
    11**.3*:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要通过创建所谓的掩码输入来准备数据集以进行训练。掩码输入是一组句子，其中单词被掩码标记（在我们的例子中是 `<mask>`）所替换。它可以看起来像
    *图 11.3* 中的示例：
- en: '![Figure 11.3 – Masked input for MLMs](img/B19548_11_3.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.3 – MLMs 的掩码输入](img/B19548_11_3.jpg)'
- en: Figure 11.3 – Masked input for MLMs
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – MLMs 的掩码输入
- en: 'It’s easy to guess that the `<mask>` token can appear at any place and that
    it should appear several times in similar places in order for the model to actually
    learn the masked token’s context. It would be very cumbersome to do it manually,
    and therefore, the HuggingFace library has a dedicated class for it – `DataCollatorForLanguageModeling`.
    The following code demonstrates how to instantiate that class and how to use its
    parameters:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易猜测 `<mask>` 标记可以出现在任何位置，并且为了模型能够真正学习掩码标记的上下文，它应该在相似位置出现多次。手动操作会非常繁琐，因此 HuggingFace
    库为此提供了一个专门的类 – `DataCollatorForLanguageModeling`。以下代码演示了如何实例化该类以及如何使用其参数：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `from transformers import DataCollatorForLanguageModeling` line imports
    the `DataCollatorForLanguageModeling` class, which is used for preparing data
    for language modeling tasks. The code initializes a `DataCollatorForLanguageModeling`
    object named `data_collator`. This object takes several parameters:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`from transformers import DataCollatorForLanguageModeling` 这一行导入了 `DataCollatorForLanguageModeling`
    类，该类用于准备语言建模任务的数据。代码初始化了一个名为 `data_collator` 的 `DataCollatorForLanguageModeling`
    对象。此对象接受多个参数：'
- en: '`tokenizer=tokenizer`: This parameter specifies the tokenizer to be used for
    encoding and decoding the text data. It expects an instance of a `tokenizer` object.
    In this case, it appears that the `tokenizer` object has been previously defined
    and assigned to the `tokenizer` variable.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer=tokenizer`: 此参数指定用于编码和解码文本数据时要使用的标记器。它期望一个`tokenizer`对象的实例。在这种情况下，似乎`tokenizer`对象已被预先定义并分配给`tokenizer`变量。'
- en: '`mlm=True`: This parameter indicates that the language modeling task is an
    MLM task.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlm=True`: 此参数指示语言建模任务是MLM任务。'
- en: '`mlm_probability=0.15`: This parameter sets the probability of masking a token
    in the input text. Each token has a 15% chance of being masked during data preparation.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlm_probability=0.15`: 此参数设置在输入文本中掩码标记的概率。每个标记在数据准备期间有15%的概率被掩码。'
- en: 'The `data_collator` object is now ready to be used for preparing data for language
    modeling tasks. It takes care of tasks such as tokenization and masking of the
    input data to be compatible with the RoBERTa model. Now, we can instantiate another
    helper class – `Trainer` – which manages the training process of the MLM model:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`data_collator`对象现在已准备好用于准备语言建模任务的数据。它负责诸如标记化和掩码输入数据等任务，以确保与RoBERTa模型兼容。现在，我们可以实例化另一个辅助类——`Trainer`——它管理MLM模型的训练过程：'
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `from transformers import Trainer, TrainingArguments` line imports the
    `Trainer` class and the `TrainingArguments` class from the `transformers` library.
    Then it initializes a `TrainingArguments` object, `training_args`. This object
    takes several parameters to configure the training process:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`from transformers import Trainer, TrainingArguments`这一行从`transformers`库中导入了`Trainer`类和`TrainingArguments`类。然后它初始化了一个`TrainingArguments`对象，`training_args`。此对象接受多个参数以配置训练过程：'
- en: '`output_dir="./wolfBERTa"`: This parameter specifies the directory where the
    trained model and other training artifacts will be saved.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_dir="./wolfBERTa"`: 此参数指定训练模型和其他训练工件将被保存的目录。'
- en: '`overwrite_output_dir=True`: This parameter determines whether to overwrite
    `output_dir` if it already exists. If set to `True`, it will overwrite the directory.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overwrite_output_dir=True`: 此参数确定是否在存在的情况下覆盖`output_dir`。如果设置为`True`，它将覆盖目录。'
- en: '`num_train_epochs=10`: This parameter sets the number of training epochs; that
    is, the number of times the training data will be iterated during training. In
    our example, it is enough with a few epochs only, such as 10\. It takes a lot
    of time to train these models, so that’s why we go with a small number of epochs.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_train_epochs=10`: 此参数设置训练的轮数；即在训练过程中训练数据将被迭代的次数。在我们的例子中，只需要几轮就足够了，例如10轮。训练这些模型需要花费很多时间，这就是为什么我们选择较小的轮数。'
- en: '`per_device_train_batch_size=32`: This parameter sets the batch size per GPU
    for training. It determines how many training examples are processed together
    in parallel during each training step. If you do not have a lot of VRAM in your
    GPU, decrease this number.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`per_device_train_batch_size=32`: 此参数设置每个GPU的训练批大小。它确定在每次训练步骤中并行处理多少个训练示例。如果您GPU的VRAM不多，请减少此数值。'
- en: '`save_steps=10_000`: This parameter specifies the number of training steps
    before saving a checkpoint of the model.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_steps=10_000`: 此参数指定在保存模型检查点之前要进行的训练步骤数。'
- en: '`save_total_limit=2`: This parameter limits the total number of saved checkpoints.
    If the limit is exceeded, older checkpoints will be deleted.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_total_limit=2`: 此参数限制保存检查点的总数。如果超过限制，将删除较旧的检查点。'
- en: 'After initializing the trainer arguments, the code initializes a `Trainer`
    object with the following arguments:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化训练参数后，代码使用以下参数初始化一个`Trainer`对象：
- en: '`model=model`: This parameter specifies the model to be trained. In this case,
    the pre-initialized RoBERTa model from our previous steps is assigned to the model
    variable.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model=model`: 此参数指定要训练的模型。在这种情况下，从我们之前的步骤中预初始化的RoBERTa模型被分配给模型变量。'
- en: '`args=training_args`: This parameter specifies the training arguments, which
    we prepared in our previous steps.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args=training_args`: 此参数指定训练参数，这是我们之前步骤中准备的。'
- en: '`data_collator=data_collator`: This parameter specifies the data collator to
    be used during training. This object was prepared previously in our code.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_collator=data_collator`: 此参数指定在训练期间要使用的数据合并器。此对象已在我们的代码中预先准备。'
- en: '`train_dataset=tokenized_dataset[''train'']`: This parameter specifies the
    training dataset. It appears that a tokenized dataset has been prepared and stored
    in a dictionary called `tokenized_dataset`, and the training portion of that dataset
    is assigned to `train_dataset`. In our case, since we did not define the train-test
    split, it take the entire dataset.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_dataset=tokenized_dataset[''train'']`：此参数指定了训练数据集。看起来已经准备并存储在一个名为 `tokenized_dataset`
    的字典中的标记化数据集，并且该数据集的训练部分被分配给了 `train_dataset`。在我们的案例中，因为我们没有定义训练-测试分割，所以它采用了整个数据集。'
- en: The `Trainer` object is now ready to be used for training the RoBERTa model
    using the specified training arguments, data collator, and training dataset. We
    do this by simply writing `trainer.train()`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`Trainer` 对象现在已准备好使用指定的训练参数、数据收集器和训练数据集来训练 RoBERTa 模型。我们只需简单地编写 `trainer.train()`
    即可。'
- en: 'Once the model finishes training, we can save it using the following command:
    `trainer.save_model("./wolfBERTa")`. After that, we can use the model just as
    we learned in [*Chapter 10*](B19548_10.xhtml#_idTextAnchor121).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型完成训练，我们可以使用以下命令保存它：`trainer.save_model("./wolfBERTa")`。之后，我们可以像在 [*第 10
    章*](B19548_10.xhtml#_idTextAnchor121) 中学习的那样使用该模型。
- en: It takes a while to train the model; on a consumer-grade GPU such as NVIDIA
    4090, it can take about one day for 10 epochs, but if we want to use a larger
    dataset or more epochs, it can take much longer. I do not advise executing this
    code on a computer without a GPU as it takes ca. 5-10 times longer than on a GPU.
    Hence my next best practice.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型需要一段时间；在 NVIDIA 4090 这样的消费级 GPU 上，10 个周期的训练可能需要大约一天时间，但如果我们想使用更大的数据集或更多的周期，可能需要更长的时间。我不建议在没有
    GPU 的计算机上执行此代码，因为它比在 GPU 上慢约 5-10 倍。因此，我的下一个最佳实践是。
- en: 'Best practice #57'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #57'
- en: Use NVIDIA **Compute Unified Device Architecture** (**CUDA**; accelerated computing)
    for training advanced models such as BERT, GPT-3, and AEs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NVIDIA **Compute Unified Device Architecture** (**CUDA**；加速计算）来训练如 BERT、GPT-3
    和 AEs 等高级模型。
- en: 'For classical ML, and even for simple NNs, a modern CPU is more than enough.
    The number of calculations is large, but not extreme. However, when it comes to
    training BERT models, AEs, and similar, we need acceleration for handling tensors
    (vectors) and making calculations on entire vectors at once. CUDA is NVIDIA’s
    acceleration framework. It allows developers to utilize the power of NVIDIA GPUs
    to accelerate computational tasks, including training DL models. It provides a
    few benefits:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于经典机器学习，甚至对于简单的神经网络，现代 CPU 已经足够。计算量很大，但并不极端。然而，当涉及到训练 BERT 模型、AEs 以及类似模型时，我们需要加速来处理张量（向量）以及一次性在整个向量上执行计算。CUDA
    是 NVIDIA 的加速框架。它允许开发者利用 NVIDIA GPU 的强大能力来加速计算任务，包括深度学习模型的训练。它提供了一些好处：
- en: '**GPU parallelism**, designed to handle many parallel computations simultaneously.
    DL models, especially large models such as RoBERTa, consist of millions or even
    billions of parameters. Training these models involves performing numerous mathematical
    operations, such as matrix multiplications and convolutions, on these parameters.
    CUDA enables these computations to be parallelized across the thousands of cores
    present in a GPU, greatly speeding up the training process compared to a traditional
    CPU.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU 并行处理**，旨在同时处理许多并行计算。深度学习模型，尤其是像 RoBERTa 这样的大型模型，包含数百万甚至数十亿个参数。训练这些模型涉及到对这些参数执行大量的数学运算，如矩阵乘法和卷积。CUDA
    使得这些计算可以在 GPU 的数千个核心上并行化，与传统的 CPU 相比，大大加快了训练过程。'
- en: '**Optimized tensor operations for PyTorch or TensorFlow**, which are designed
    to work seamlessly with CUDA. These frameworks provide GPU-accelerated libraries
    that implement optimized tensor operations specifically designed for GPUs. Tensors
    are multi-dimensional arrays used to store and manipulate data in DL models. With
    CUDA, these tensor operations can be efficiently executed on the GPU, leveraging
    its high memory bandwidth and parallel processing capabilities.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**针对 PyTorch 或 TensorFlow 优化的张量操作**，这些操作旨在与 CUDA 无缝工作。这些框架提供了 GPU 加速库，实现了专门为
    GPU 设计的优化张量操作。张量是多维数组，用于在深度学习模型中存储和处理数据。有了 CUDA，这些张量操作可以在 GPU 上高效执行，利用其高内存带宽和并行处理能力。'
- en: '**High memory bandwidth**, which enables data to be transferred to and from
    the GPU memory at a much faster rate, enabling faster data processing during training.
    DL models often require large amounts of data to be loaded and processed in batches.
    CUDA allows these batches to be efficiently transferred and processed on the GPU,
    reducing training time.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高内存带宽**，这使数据能够以更快的速度在 GPU 内存之间传输，从而在训练期间实现更快的数据处理。深度学习模型通常需要大量数据以批量形式加载和处理。CUDA
    允许这些批次在 GPU 上有效地传输和处理，从而减少训练时间。'
- en: By utilizing CUDA, DL frameworks can effectively leverage the parallel computing
    capabilities and optimized operations of NVIDIA GPUs, resulting in significant
    acceleration of the training process for large-scale models such as RoBERTa.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 CUDA，深度学习框架可以有效地利用 NVIDIA GPU 的并行计算能力和优化操作，从而显著加速大规模模型如 RoBERTa 的训练过程。
- en: Training and evaluation of an AE
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AE 的训练和评估
- en: We mentioned AEs in [*Chapter 7*](B19548_07.xhtml#_idTextAnchor085) when we
    discussed the process of feature engineering for images. AEs, however, are used
    to do much more than just image feature extraction. One of the major aspects of
    them is to be able to recreate images. This means that we can create images based
    on the placement of the image in the latent space.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在讨论图像特征工程过程时提到了 AEs，见 [*第 7 章*](B19548_07.xhtml#_idTextAnchor085)。然而，AEs 的用途远不止图像特征提取。它们的一个主要方面是能够重新创建图像。这意味着我们可以根据图像在潜在空间中的位置创建图像。
- en: 'So, let us train the AE model for a dataset that is pretty standard in ML –
    Fashion MNIST. We got to see what the dataset looks like in our previous chapters.
    We start our training by preparing the data in the following code fragment:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们为在机器学习（ML）中相当标准的 Fashion MNIST 数据集训练 AE 模型。我们在前面的章节中看到了数据集的样子。我们通过以下代码片段准备数据开始我们的训练：
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It imports the necessary modules from the PyTorch library.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 它从 PyTorch 库中导入必要的模块。
- en: It defines a transformation called `tensor_transform` using `transforms.ToTensor()`.
    This transformation is used to convert images in the dataset to PyTorch tensors.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用 `transforms.ToTensor()` 定义了一个名为 `tensor_transform` 的转换。这个转换用于将数据集中的图像转换为
    PyTorch 张量。
- en: The code fragment downloads the dataset using the `datasets.FashionMNIST()`
    function. The `train` parameter is set to `True` to indicate that the downloaded
    dataset is for training purposes. The `download` parameter is set to `True` to
    automatically download the dataset if it is not already present in the specified
    directory.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段使用 `datasets.FashionMNIST()` 函数下载数据集。`train` 参数设置为 `True`，表示下载的数据集用于训练目的。`download`
    参数设置为 `True`，以自动下载数据集，如果它尚未存在于指定的目录中。
- en: Since we use the PyTorch framework with accelerated computing, we need to make
    sure that the image is transformed into a tensor. The `transform` parameter is
    set to `tensor_transform`, which is a transformer defined in the first line of
    the code fragment.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用具有加速计算的 PyTorch 框架，我们需要确保图像被转换成张量。`transform` 参数设置为 `tensor_transform`，这是在代码片段的第一行定义的转换器。
- en: Then, we create a `DataLoader` object used to load the dataset in batches for
    training. The `dataset` parameter is set to the previously downloaded dataset.
    The `batch_size` parameter is set to `32`, indicating that each batch of the dataset
    will contain 32 images.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个 `DataLoader` 对象，用于批量加载数据集进行训练。`dataset` 参数设置为之前下载的数据集。`batch_size`
    参数设置为 `32`，表示数据集的每一批次将包含 32 张图像。
- en: The `shuffle` parameter is set to `True` to shuffle the order of the samples
    in each epoch of training, ensuring randomization and reducing any potential bias
    during training.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`shuffle` 参数设置为 `True`，以在每个训练周期的样本顺序中打乱，确保随机化并减少训练过程中的任何潜在偏差。'
- en: 'Once we have the dataset prepared, we can create our AE, which we do like this:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们准备好了数据集，我们就可以创建我们的 AE，具体做法如下：
- en: '[PRE12]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: First, we define a class named `AE` that inherits from the `torch.nn.Module`
    class, which is the base class for all NN modules in PyTorch. The `super().__init__()`
    line ensures that the initialization of the base class (`torch.nn.Module`) is
    called. Since AEs are a special kind of NN class with backpropagation learning,
    we can just inherit a lot of basic functionality from the library.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一个名为 `AE` 的类，它继承自 `torch.nn.Module` 类，这是 PyTorch 中所有神经网络模块的基类。`super().__init__()`
    行确保调用基类（`torch.nn.Module`）的初始化。由于 AEs 是一种具有反向传播学习的特殊神经网络类，我们可以从库中继承很多基本功能。
- en: Then, we define the encoder part of the AE. The encoder consists of several
    linear (fully connected) layers with ReLU activation functions. Each `torch.nn.Linear`
    layer represents a linear transformation of the input data followed by an activation
    function. In this case, the input size is 28 * 28 (which corresponds to the dimensions
    of an image in the Fashion MNIST dataset), and the output size gradually decreases
    until it reaches 9, which is our latent vector size.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义AE的编码器部分。编码器由几个具有ReLU激活函数的线性（全连接）层组成。每个`torch.nn.Linear`层代表输入数据的线性变换，后面跟着一个激活函数。在这种情况下，输入大小为28
    * 28（这对应于Fashion MNIST数据集中图像的维度），输出大小逐渐减小，直到达到9，这是我们潜在向量的大小。
- en: Then, we define the decoder part of the AE. The decoder is responsible for reconstructing
    the input data from the encoded representation. It consists of several linear
    layers with ReLU activation functions, followed by a final linear layer with a
    sigmoid activation function. The input size of the decoder is 9, which corresponds
    to the size of the latent vector space in the bottleneck of the encoder. The output
    size is 28 * 28, which matches the dimensions of the original input data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义AE的解码器部分。解码器负责从编码表示中重建输入数据。它由几个具有ReLU激活函数的线性层组成，后面跟着一个具有sigmoid激活函数的最终线性层。解码器的输入大小为9，这对应于编码器瓶颈中潜在向量空间的大小。输出大小为28
    * 28，这与原始输入数据的维度相匹配。
- en: 'The `forward` method defines the forward pass of the AE. It takes an `x` input
    and passes it through the encoder to obtain an encoded representation. Then, it
    passes the encoded representation through the decoder to reconstruct the input
    data. The reconstructed output is returned as the result. We are now ready to
    instantiate our AE:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward`方法定义了AE的前向传递。它接受一个`x`输入，并通过编码器传递以获得编码表示。然后，它通过解码器将编码表示传递以重建输入数据。重建的输出作为结果返回。我们现在可以实例化我们的AE：'
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In this code, we first instantiate our AE as our model. Then, we create an
    instance of the **Mean Squared Error** (**MSE**) loss function provided by PyTorch.
    MSE is a commonly used loss function for regression tasks. We need it to calculate
    the mean squared difference between the predicted values and the target values
    – which are the individual pixels in our dataset, providing a measure of how well
    the model is performing. *Figure 11**.4* shows the role of the learning function
    in the process of training the AE:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们首先将我们的AE实例化为我们自己的模型。然后，我们创建了一个由PyTorch提供的**均方误差**（**MSE**）损失函数的实例。MSE是回归任务中常用的损失函数。我们需要它来计算预测值和目标值之间的平均平方差异——这些目标值是我们数据集中的单个像素，提供了衡量模型性能好坏的指标。*图11*.*4*显示了学习函数在训练AE过程中的作用：
- en: '![Figure 11.4 – Loss function (MSE) in the AE training process](img/B19548_11_4.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图11.4 – AE训练过程中的损失函数（均方误差）](img/B19548_11_4.jpg)'
- en: Figure 11.4 – Loss function (MSE) in the AE training process
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 – AE训练过程中的损失函数（均方误差）
- en: 'Then, we initialize the optimizer used to update the model’s parameters during
    training. In this case, the code creates an Adam optimizer, which is a popular
    optimization algorithm for training NNs. It takes three important arguments:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们初始化用于在训练过程中更新模型参数的优化器。在这种情况下，代码创建了一个Adam优化器，这是一种用于训练神经网络（NNs）的流行优化算法。它接受三个重要参数：
- en: '`model.parameters()`: This specifies the parameters that will be optimized.
    In this case, it includes all the parameters of the model (the AE) that we created
    earlier.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.parameters()`：这指定了将要优化的参数。在这种情况下，它包括我们之前创建的模型（自动编码器，AE）的所有参数。'
- en: '`lr=1e-1`: This sets the learning rate, which determines the step size at which
    the optimizer updates the parameters. A higher learning rate can lead to faster
    convergence but may risk overshooting the optimal solution, while a lower learning
    rate may converge more slowly but with potentially better accuracy.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr=1e-1`：这设置了学习率，它决定了优化器更新参数的步长大小。较高的学习率可能导致更快收敛，但可能风险超过最佳解，而较低的学习率可能收敛较慢，但可能具有更好的精度。'
- en: '`weight_decay=1e-8`: This parameter adds a weight decay regularization term
    to the optimizer. Weight decay helps prevent overfitting by adding a penalty term
    to the loss function that discourages large weights. The `1e-8` value represents
    the weight decay coefficient.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight_decay=1e-8`：此参数向优化器添加一个权重衰减正则化项。权重衰减通过向损失函数添加一个惩罚项来防止过拟合，该惩罚项会阻止权重过大。`1e-8`的值表示权重衰减系数。'
- en: 'With this code, we have now an instance of an AE to train. Now, we can start
    the process of training. We train the model for 10 epochs, but we can try more
    if needed:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此代码，我们现在有一个自动编码器（AE）的实例用于训练。现在，我们可以开始训练过程。我们训练模型10个epoch，如果需要的话，可以尝试更多：
- en: '[PRE14]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We start by iterating over the specified number of epochs for the training.
    Within each epoch, we iterate over the loader, which provides batches of image
    data and their corresponding labels. We do not use the labels, because the AE
    is a network to recreate images and not to learn what the images show – in that
    sense, it is an unsupervised model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先遍历指定的epoch数量进行训练。在每个epoch中，我们遍历加载器，它提供图像数据批次及其相应的标签。我们不使用标签，因为AE是一个用于重建图像的网络，而不是学习图像显示的内容——从这个意义上说，它是一个无监督模型。
- en: For each image, we reshape the input image data by flattening each image, originally
    in the shape of `(batch_size, 28, 28)`, into a 2D tensor of shape `(batch_size,
    784)`, where each row represents a flattened image. The flattened image is created
    when we take each row of pixels and concatenate it to create one large vector.
    It is needed as the images are two-dimensional, while our tensor input needs to
    be of a single dimension.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每张图像，我们通过将原始形状为`(batch_size, 28, 28)`的输入图像数据展平，形成一个形状为`(batch_size, 784)`的2D张量，其中每一行代表一个展平的图像。展平图像是在我们将像素的每一行连接起来创建一个大型向量时创建的。这是必需的，因为图像是二维的，而我们的张量输入需要是一维的。
- en: 'Then, we obtain the reconstructed image using `reconstructed = model(image)`.
    Once we get the reconstructed image, we can calculate the MSE loss function and
    use that information to manage the next step of the learning (`optimizer.zero_grad()`).
    In the last line, we add this information to the list of losses per iteration
    so that we can create a learning diagram. We do it by using the following code
    fragment:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`reconstructed = model(image)`获取重建图像。一旦我们得到重建图像，我们可以计算均方误差（MSE）损失函数，并使用该信息来管理学习的下一步（`optimizer.zero_grad()`）。在最后一行，我们将此信息添加到每次迭代的损失列表中，以便我们可以创建学习图。我们通过以下代码片段来完成：
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This results in a learning diagram, shown in *Figure 11**.5*:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了学习图，如*图11.5*所示：
- en: '![Figure 11.5 – Learning rate diagram from training our AE](img/B19548_11_5.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图11.5 – 从训练我们的AE得到的学习率图](img/B19548_11_5.jpg)'
- en: Figure 11.5 – Learning rate diagram from training our AE
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5 – 从训练我们的AE得到的学习率图
- en: 'The learning rate diagram shows that the AE is not really great yet and that
    we should train it a bit more. However, we can always check what the recreated
    images look like. We can do that using this code:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率图显示AE还不是很好，我们应该再训练一段时间。然而，我们总是可以检查重建图像的外观。我们可以使用以下代码来做这件事：
- en: '[PRE16]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The code results in the output shown in *Figure 11**.6*:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成了如*图11.6*所示的输出：
- en: '![Figure 11.6 – Recreated image from our AE](img/B19548_11_6.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图11.6 – 我们AE重建的图像](img/B19548_11_6.jpg)'
- en: Figure 11.6 – Recreated image from our AE
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6 – 我们AE重建的图像
- en: Despite the learning rate, which is OK, we still can get very good results from
    our AEs.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管学习率是OK的，我们仍然可以从我们的AE中获得非常好的结果。
- en: 'Best practice #58'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践#58
- en: In addition to monitoring the loss, make sure to visualize the actual results
    of the generation.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 除了监控损失，确保可视化生成的实际结果。
- en: Monitoring the loss function is a good way to understand when the AE stabilizes.
    However, just the loss function is not enough. I usually plot the actual output
    to understand whether the AE has been trained correctly.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 监控损失函数是理解AE何时稳定的好方法。然而，仅仅损失函数是不够的。我通常绘制实际输出以了解AE是否已经正确训练。
- en: 'Finally, we can visualize the learning process when we use this code:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用此代码可视化学习过程：
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This code visualizes the learning process of the entire network. It creates
    a large image, and we can only show a small excerpt of it. *Figure 11**.7* shows
    this excerpt:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码可视化整个网络的学习过程。它创建了一个大图像，我们只能显示其中的一小部分。*图11.7*显示了这一部分：
- en: '![Figure 11.7 – The first three steps in training an AE, visualized as the
    AE architecture](img/B19548_11_7.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图11.7 – 训练AE的前三个步骤，以AE架构的形式可视化](img/B19548_11_7.jpg)'
- en: Figure 11.7 – The first three steps in training an AE, visualized as the AE
    architecture
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7 – 训练AE的前三个步骤，以AE架构的形式可视化
- en: 'We can even visualize the entire architecture in a text form by using the following
    code:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以使用以下代码以文本形式可视化整个架构：
- en: '[PRE18]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This results in the following model:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下模型：
- en: '[PRE19]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The bottleneck layer is in boldface, to illustrate the place where the encode
    and decode parts are linked to one another.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 瓶颈层用粗体表示，以说明编码和解码部分是如何相互连接的。
- en: Developing safety cages to prevent models from breaking the entire system
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发安全笼以防止模型破坏整个系统
- en: As GenAI systems such as MLMs and AEs create new content, there is a risk that
    they generate content that can either break the entire software system or become
    unethical.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 随着像 MLM 和 AE 这样的 GenAI 系统创建新的内容，存在它们生成的内容可能会破坏整个软件系统或变得不道德的风险。
- en: 'Therefore, software engineers often use the concept of a safety cage to guard
    the model itself from inappropriate input and output. For an MLM such as RoBERTa,
    this can be a simple preprocessor that checks whether the content generated is
    problematic. Conceptually, this is illustrated in *Figure 11**.8*:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，软件工程师经常使用安全笼的概念来保护模型本身免受不适当的输入和输出。对于像 RoBERTa 这样的 MLM，这可能是一个简单的预处理程序，用于检查生成的内容是否存在问题。从概念上讲，这如图
    *11**.8* 所示：
- en: '![Figure 11.8 – Safety-cage concept for MLMs](img/B19548_11_8.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.8 – MLM 的安全笼概念](img/B19548_11_8.jpg)'
- en: Figure 11.8 – Safety-cage concept for MLMs
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 – MLM 的安全笼概念
- en: In the example of the `wolfBERTa` model, this can mean that we check whether
    the generated code does not contain cybersecurity vulnerabilities, which can potentially
    allow hackers to take over our system. This means that all programs generated
    by the `wolfBERTa` model should be checked using tools such as SonarQube or CodeSonar
    to check for cybersecurity vulnerabilities, hence my next best practice.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `wolfBERTa` 模型的例子中，这可能意味着我们检查生成的代码是否不包含网络安全漏洞，这些漏洞可能允许黑客接管我们的系统。这意味着由 `wolfBERTa`
    模型生成的所有程序都应该使用 SonarQube 或 CodeSonar 等工具进行检查，以查找网络安全漏洞，因此我的下一个最佳实践。
- en: 'Best practice #59'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #59'
- en: Check the output of GenAI models so that it does not break the entire system
    or provide unethical responses.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 GenAI 模型的输出，以确保它不会破坏整个系统或提供不道德的回应。
- en: My recommendation to create such safety cages is to start from the requirements
    of the system. The first step is to understand what the system is going to do
    and understand which dangers and risks this task entails. The safety cage’s output
    processor should ensure that these dangerous situations do not occur and that
    they are handled properly.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议创建这样的安全笼，应从系统的需求开始。第一步是了解系统将要做什么，以及这项任务涉及哪些危险和风险。安全笼的输出处理器应确保这些危险情况不会发生，并且得到妥善处理。
- en: Once we understand how to prevent dangers, we can move over to conceptualizing
    how to prevent these risks on the language-model level. For example, when we train
    the model, we can select code that is known to be secure and does not contain
    security vulnerabilities. Although it does not guarantee that the model generates
    secure code, it certainly reduces the risk for it.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们了解了如何预防危险，我们就可以转向思考如何在语言模型层面上预防这些风险。例如，当我们训练模型时，我们可以选择已知是安全的且不包含安全漏洞的代码。尽管这并不能保证模型生成的代码是安全的，但它确实降低了风险。
- en: Summary
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to train advanced models and saw that their
    training is not much more difficult than training classical ML models, which were
    described in [*Chapter 10*](B19548_10.xhtml#_idTextAnchor121). Even though the
    models that we trained are much more complex than the models in [*Chapter 10*](B19548_10.xhtml#_idTextAnchor121),
    we can use the same principles and expand this kind of activity to train even
    more complex models.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何训练高级模型，并看到它们的训练并不比训练第 10 章中描述的经典 ML 模型更困难。尽管我们训练的模型比第 10 章中的模型复杂得多，但我们可以使用相同的原则，并将此类活动扩展到训练更复杂的模型。
- en: We focused on GenAI in the form of BERT models (fundamental GPT models) and
    AEs. Training these models is not very difficult, and we do not need huge computing
    power to train them. Our `wolfBERTa` model has ca. 80 million parameters, which
    seems like a lot, but the really good models, such as GPT-3, have billions of
    parameters – GPT-3 has 175 billion parameters, NVIDIA Turing has over 350 billion
    parameters, and GPT-4 is 1,000 times larger than GPT-3\. The training process
    is the same, but we need a supercomputing architecture in order to train these
    models.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们专注于以BERT模型（基础GPT模型）和AEs形式存在的GenAI。训练这些模型并不困难，我们不需要巨大的计算能力来训练它们。我们的`wolfBERTa`模型大约有8000万个参数，这看起来很多，但真正优秀的模型，如GPT-3，有数十亿个参数——GPT-3有1750亿个参数，NVIDIA
    Turing有超过3500亿个参数，而GPT-4是GPT-3的1000倍大。训练过程是相同的，但我们需要超级计算架构来训练这些模型。
- en: We have also learned that these models are only parts of larger software systems.
    In the next chapter, we learn how to create such a larger system.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还了解到，这些模型只是更大软件系统的一部分。在下一章中，我们将学习如何创建这样一个更大的系统。
- en: References
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Kratsch, W. et al., Machine learning in business process monitoring: a comparison
    of deep learning and classical approaches used for outcome prediction. Business
    & Information Systems Engineering, 2021, 63:* *p. 261-276.*'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kratsch, W. 等人，《Machine learning in business process monitoring: a comparison
    of deep learning and classical approaches used for outcome prediction. Business
    & Information Systems Engineering》，2021年，第63卷：* *p. 261-276.*'
- en: '*Vaswani, A. et al., Attention is all you need. Advances in neural information
    processing systems,* *2017, 30.*'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Vaswani, A. 等人，《Attention is all you need. Advances in neural information
    processing systems》，2017年，第30卷。*'
- en: '*Aggarwal, A., M. Mittal, and G. Battineni, Generative adversarial network:
    An overview of theory and applications. International Journal of Information Management
    Data Insights, 2021\. 1(1):* *p. 100004.*'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Aggarwal, A.，M. Mittal，和G. Battineni，《Generative adversarial network: An overview
    of theory and applications. International Journal of Information Management Data
    Insights》，2021年，第1卷：* *p. 100004.*'
- en: '*Creswell, A., et al., Generative adversarial networks: An overview. IEEE signal
    processing magazine, 2018\. 35(1):* *p. 53-65.*'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Creswell, A. 等人，《Generative adversarial networks: An overview. IEEE signal
    processing magazine》，2018年，第35卷（第1期）：* *p. 53-65.*'
