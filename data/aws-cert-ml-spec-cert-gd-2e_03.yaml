- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: AWS Services for Data Migration and Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS数据迁移和处理服务
- en: 'In the previous chapter, you learned about several ways of storing data in
    AWS. In this chapter, you will explore the techniques for using that data and
    gaining some insight from the data. There are use cases where you have to process
    your data or load the data to a hive data warehouse to query and analyze the data.
    If you are on AWS and your data is in S3, then you have to create a table in hive
    on AWS EMR to query the data in the hive table. To provide the same functionality
    as a managed service, AWS has a product called Athena, where you create a data
    catalog and query your data on S3\. If you need to transform the data, then AWS
    Glue is the best option to transform and restore it to S3\. Imagine a use case
    where you need to stream data and create analytical reports on that data. For
    this, you can opt for AWS Kinesis Data Streams to stream data and store it in
    S3\. Using Glue, the same data can be copied to Redshift for further analytical
    utilization. AWS **Database Migration Service** (**DMS**) provides seamless migration
    of heterogeneous and homogeneous databases. This chapter will cover the following
    topics that are required for the purpose of the certification:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了在AWS中存储数据的几种方法。在本章中，你将探索使用这些数据并获得一些洞察的技术。在某些用例中，你必须处理你的数据或将数据加载到数据仓库中以便查询和分析。如果你在AWS上，并且你的数据在S3中，那么你必须创建一个在AWS
    EMR上的hive表来查询hive表中的数据。为了提供与托管服务相同的功能，AWS有一个名为Athena的产品，你可以创建一个数据目录并在S3上查询你的数据。如果你需要转换数据，那么AWS
    Glue是转换并恢复到S3的最佳选择。想象一个用例，你需要流式传输数据并在该数据上创建分析报告。为此，你可以选择AWS Kinesis Data Streams来流式传输数据并将其存储在S3中。使用Glue，相同的数据可以被复制到Redshift以进行进一步的分析利用。AWS
    **数据库迁移服务**（**DMS**）提供了异构和同构数据库的无缝迁移。本章将涵盖以下认证所需的主题：
- en: Using Glue to design ETL jobs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Glue设计ETL作业
- en: Querying S3 data using Athena
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Athena查询S3数据
- en: Streaming data through AWS Kinesis Data Streams and storing it using Kinesis
    Firehose
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过AWS Kinesis Data Streams进行数据流并通过Kinesis Firehose存储
- en: Ingesting data from on-premises locations to AWS
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从本地位置摄取数据到AWS
- en: Migrating data to AWS and extending on-premises data centers to AWS
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据迁移到AWS并扩展本地数据中心到AWS
- en: Processing data on AWS
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在AWS上处理数据
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can download the data used in the examples from GitHub, available here:
    [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter03](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter03).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从GitHub下载示例中使用的数据，链接如下：[https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter03](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter03)。
- en: '**Creating ETL jobs o****n AWS Glue**'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**在AWS Glue上创建ETL作业**'
- en: 'In a modern data pipeline, there are multiple stages, such as generating data,
    collecting data, storing data, performing ETL, analyzing, and visualizing. In
    this section, you will cover each of these at a high level and understand the
    **extract, transform, load (ETL)** process in depth:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '在现代数据管道中，有多个阶段，例如生成数据、收集数据、存储数据、执行ETL、分析和可视化。在本节中，你将概述这些阶段，并深入了解**提取、转换、加载（ETL）**过程：  '
- en: Data can be generated from several devices, including mobile devices or IoT,
    weblogs, social media, transactional data, and online games.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可以来自多个设备，包括移动设备或物联网、Web日志、社交媒体、交易数据和在线游戏。
- en: This huge amount of generated data can be collected by using polling services,
    through API gateways integrated with AWS Lambda to collect the data, or via streams
    such as AWS Kinesis, AWS-managed Kafka, or Kinesis Firehose. If you have an on-premises
    database and you want to bring that data to AWS, then you would choose AWS DMS
    for that. You can sync your on-premises data to Amazon S3, Amazon EFS, or Amazon
    FSx via AWS DataSync. AWS Snowball is used to collect/transfer data into and out
    of AWS.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这巨大的数据量可以通过使用轮询服务、通过集成AWS Lambda的API网关来收集数据，或者通过AWS Kinesis、AWS管理的Kafka或Kinesis
    Firehose等流来收集。如果你有一个本地数据库，并且想把那些数据带到AWS，那么你会选择AWS DMS。你可以通过AWS DataSync将本地数据同步到Amazon
    S3、Amazon EFS或Amazon FSx。AWS Snowball用于收集/传输数据到和从AWS。
- en: The next step involves storing data. You learned about some of the services
    to do this in the previous chapter, such as S3, EBS, EFS, RDS, Redshift, and DynamoDB.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一步涉及存储数据。您在前一章中学习了有关此方面的某些服务，例如 S3、EBS、EFS、RDS、Redshift 和 DynamoDB。
- en: Once you know your data storage requirements, an ETL job can be designed to
    extract-transform-load or extract-load-transform your structured or unstructured
    data into the format you desire for further analysis. For example, you can use
    AWS Lambda to transform the data on the fly and store the transformed data in
    S3, or you can run a Spark application on an EMR cluster to transform the data
    and store it in S3 or Redshift or RDS.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦您了解您的数据存储需求，就可以设计一个 ETL 作业来提取-转换-加载或提取-加载-转换您的结构化或非结构化数据，以满足您进一步分析所需的格式。例如，您可以使用
    AWS Lambda 在线转换数据并将其存储在 S3 中，或者您可以在 EMR 集群上运行 Spark 应用程序来转换数据并将其存储在 S3、Redshift
    或 RDS 中。
- en: There are many services available in AWS for performing an analysis on transformed
    data, for example, EMR, Athena, Redshift, Redshift Spectrum, and Kinesis Analytics.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS 中提供了许多服务用于对转换后的数据进行分析，例如 EMR、Athena、Redshift、Redshift Spectrum 和 Kinesis
    Analytics。
- en: Once the data is analyzed, you can visualize it using AWS QuickSight to understand
    the patterns or trends. Data scientists or machine learning professionals would
    want to apply statistical analysis to understand data distribution in a better
    way. Business users use statistical analysis to prepare reports. You will learn
    and explore various ways to present and visualize data in [*Chapter 5*](B21197_05.xhtml#_idTextAnchor638)*,
    Data Understanding* *and Visualization.*
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦数据被分析，您可以使用 AWS QuickSight 进行可视化，以了解模式或趋势。数据科学家或机器学习专业人士会希望应用统计分析来更好地理解数据分布。商业用户使用统计分析来准备报告。您将在
    [*第5章*](B21197_05.xhtml#_idTextAnchor638) *数据理解和可视化* 中学习和探索各种展示和可视化数据的方法。
- en: What you understood from the traditional data pipeline is that ETL is all about
    coding and maintaining code on the servers so that everything runs smoothly. If
    the data format changes in any way, then the code needs to be changed, and that
    results in a change to the target schema. If the data source changes, then the
    code must be able to handle that too, and it’s an overhead. *Should you write
    code to recognize these changes in data sources? Do you need a system to adapt
    to the change and discover the data for you?* The answer to these questions is
    yes, and to do so, you can use **AWS Glue**. Now, you will learn why AWS Glue
    is so popular.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 您从传统数据管道中理解到的是，ETL 全部关于在服务器上编码和维护代码，以确保一切顺利运行。如果数据格式有任何变化，则代码需要更改，这将导致目标模式发生变化。如果数据源发生变化，则代码必须能够处理这种情况，这会增加开销。*您是否需要编写代码来识别数据源中的这些变化？您是否需要一个能够适应变化并为您发现数据的系统？*
    这些问题的答案是肯定的，为此，您可以使用 **AWS Glue**。现在，您将了解为什么 AWS Glue 如此受欢迎。
- en: '**Features of AWS Glue**'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**AWS Glue 的功能**'
- en: 'AWS Glue is a completely managed serverless ETL service on AWS. It has the
    following features:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue 是 AWS 上完全托管的无服务器 ETL 服务。它具有以下功能：
- en: It automatically discovers and categorizes your data by connecting to the data
    sources and generates a data catalog.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过连接到数据源自动发现和分类您的数据，并生成数据目录。
- en: Services such as Amazon Athena, Amazon Redshift, and Amazon EMR can use the
    data catalog to query the data.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如 Amazon Athena、Amazon Redshift 和 Amazon EMR 这样的服务可以使用数据目录来查询数据。
- en: AWS Glue generates the ETL code, which is an extension to Spark in Python or
    Scala, which can be modified, too.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Glue 生成 ETL 代码，这是 Spark 在 Python 或 Scala 中的扩展，也可以进行修改。
- en: It scales out automatically to match your Spark application requirements for
    running the ETL job and loading the data into the destination.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它会自动扩展以匹配您的 Spark 应用程序运行 ETL 作业和将数据加载到目标位置的需求。
- en: 'AWS Glue has the **Data Catalog**, and that’s the secret to its success. It
    helps with discovering data from data sources and understanding a bit about it:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue 拥有 **数据目录**，这正是其成功的关键。它有助于从数据源发现数据并了解一些相关信息：
- en: The Data Catalog automatically discovers new data and extracts schema definitions.
    It detects schema changes and version tables. It detects Apache Hive-style partitions
    on Amazon S3.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据目录自动发现新数据并提取模式定义。它检测模式更改和版本表。它检测 Amazon S3 上的 Apache Hive 风格分区。
- en: The Data Catalog comes with built-in classifiers for popular data types. Custom
    classifiers can be written using **Grok expressions**. The classifiers help to
    detect the schema.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据目录自带用于流行数据类型的内置分类器。可以使用 **Grok 表达式**编写自定义分类器。分类器有助于检测模式。
- en: Glue crawlers can be run ad hoc or in a scheduled fashion to update the metadata
    in the Glue Data Catalog. Glue crawlers must be associated with an IAM role with
    sufficient access to read the data sources, such as Amazon RDS, Redshift, and
    S3.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glue爬虫可以按需或按计划运行以更新Glue数据目录中的元数据。Glue爬虫必须与具有足够权限读取数据源（如Amazon RDS、Redshift和S3）的IAM角色相关联。
- en: As you now have a brief idea of what AWS Glue is used for, move on to run the
    following example to get your hands dirty.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 既然您对AWS Glue的用途有了初步的了解，接下来运行以下示例以亲自动手实践。
- en: '**Getting hands-on with AWS Glue Data Catalog components**'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**动手实践AWS Glue数据目录组件**'
- en: 'In this example, you will create a job to copy data from S3 to Redshift by
    using AWS Glue. All my components were created in the `us-east-1` region. Start
    by creating a bucket:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，您将创建一个作业，使用AWS Glue将数据从S3复制到Redshift。我所有的组件都是在`us-east-1`区域创建的。首先创建一个存储桶：
- en: Navigate to the AWS S3 console and create a bucket. I have named the bucket
    `aws-glue-example-01`.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到AWS S3控制台并创建一个存储桶。我将其命名为`aws-glue-example-01`。
- en: Click on `input-data`.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`input-data`。
- en: 'Navigate inside the folder and click on the `sales-records.csv` dataset. The
    data is available in the following GitHub location: [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter03/AWS-Glue-Demo/input-data](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter03/AWS-Glue-Demo/input-data).'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在文件夹内导航并点击`sales-records.csv`数据集。数据可在以下GitHub位置找到：[https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter03/AWS-Glue-Demo/input-data](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter03/AWS-Glue-Demo/input-data)。
- en: As you have the data uploaded in the S3 bucket, now create a VPC in which you
    will create your Redshift cluster.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于您已将数据上传到S3存储桶，现在创建一个VPC，您将在其中创建Redshift集群。
- en: Navigate to the VPC console by accessing the [https://console.aws.amazon.com/vpc/home?region=us-east-1#](https://console.aws.amazon.com/vpc/home?region=us-east-1#)
    URL and click on `AWS services`
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过访问[https://console.aws.amazon.com/vpc/home?region=us-east-1#](https://console.aws.amazon.com/vpc/home?region=us-east-1#)
    URL进入VPC控制台，并点击`AWS services`
- en: '`com.amazonaws.us-east-1.s3` (gateway type)'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`com.amazonaws.us-east-1.s3`（网关类型）'
- en: '**VPC**: Select **the Default VPC** (use this default VPC in which your Redshift
    cluster will be created)'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**VPC**：选择**默认VPC**（在您的Redshift集群将创建的默认VPC中使用此默认VPC）'
- en: Leave the other fields as is and click on **Create Endpoint**.*   Click on `redshift-self`,
    and choose the default VPC drop-down menu. Provide an appropriate description,
    such as `Redshift Security Group`. Click on **Create** **security group**.*   Click
    on the `All traffic`*   `Custom`*   In the search field, select the same security
    group (`redshift-self`)*   Click on **Save Rules**.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持其他字段不变，并点击**创建端点**。*   点击`redshift-self`，并选择默认VPC下拉菜单。提供适当的描述，例如`Redshift安全组`。点击**创建****安全组**。*   点击`所有流量`*   `自定义`*   在搜索字段中，选择相同的组（`redshift-self`）*   点击**保存规则**。
- en: Now, create your Redshift cluster.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，创建您的Redshift集群。
- en: Navigate to the Amazon Redshift console. Click on **Create Cluster** and complete
    the highlighted fields, as shown in *Figure 3**.1*:![Figure 3.1 – A screenshot
    of Amazon Redshift’s Create cluster screen](img/B21197_03_01.jpg)
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进入Amazon Redshift控制台。点击**创建集群**并填写高亮字段，如图*图3.1*所示。![图3.1 – Amazon Redshift创建集群屏幕截图](img/B21197_03_01.jpg)
- en: Figure 3.1 – A screenshot of Amazon Redshift’s Create cluster screen
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – Amazon Redshift创建集群屏幕截图
- en: Scroll down and fill in the highlighted fields shown in *Figure 3**.2* with
    your own values:![Figure 3.2 – A screenshot of an Amazon Redshift cluster’s Database
    configurations section](img/B21197_03_02.jpg)
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动并使用自己的值填写图*图3.2*中显示的高亮字段：![图3.2 – Amazon Redshift集群数据库配置部分的截图](img/B21197_03_02.jpg)
- en: Figure 3.2 – A screenshot of an Amazon Redshift cluster’s Database configurations
    section
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – Amazon Redshift集群数据库配置部分的截图
- en: Scroll down and change the **Additional configurations** settings, as shown
    in *Figure 3**.3*:![Figure 3.3 – A screenshot of an Amazon Redshift cluster’s
    Additional configurations section](img/B21197_03_03.jpg)
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动并更改**附加配置**设置，如图*图3.3*所示。![图3.3 – Amazon Redshift集群的附加配置部分截图](img/B21197_03_03.jpg)
- en: Figure 3.3 – A screenshot of an Amazon Redshift cluster’s Additional configurations
    section
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – Amazon Redshift集群的附加配置部分截图
- en: Change the IAM permissions too, as shown in *Figure 3**.4*:![Figure 3.4 – A
    screenshot of an Amazon Redshift cluster’s Cluster permissions section](img/B21197_03_04.jpg)
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如图3.4所示，更改IAM权限：![Figure 3.4 – Amazon Redshift集群的集群权限截图](img/B21197_03_04.jpg)
- en: Figure 3.4 – A screenshot of an Amazon Redshift cluster’s Cluster permissions
    section
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – Amazon Redshift集群的集群权限截图
- en: Scroll down and click on **Create Cluster**. It will take a minute or two to
    get the cluster in the available state.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动并点击**创建集群**。将花费一分钟左右的时间将集群置于可用状态。
- en: Next, you will create an IAM role.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，你将创建一个IAM角色。
- en: Navigate to the AWS IAM console and select **Roles** in the **Access Management**
    section on the screen.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到AWS IAM控制台，并在屏幕上的**访问管理**部分选择**角色**。
- en: 'Click on the **Create role** button and choose **Glue** from the services.
    Click on the **Next: permissions** button to navigate to the next page.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建角色**按钮，并从服务中选择**Glue**。点击**下一步：权限**按钮进入下一页。
- en: 'Search for `AmazonS3FullAccess` and select it. Then, search for `AWSGlueServiceRole`
    and select it. As you are writing your data to Redshift as part of this example,
    select **AmazonRedshiftFullAccess**. Click on **Next: Tags**, followed by the
    **Next:** **Review** button.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索栏中搜索`AmazonS3FullAccess`并选择它。然后，搜索`AWSGlueServiceRole`并选择它。由于你正在将数据写入Redshift作为本例的一部分，请选择**AmazonRedshiftFullAccess**。点击**下一步：标签**，然后点击**下一步：****审查**按钮。
- en: Provide a name, `Glue-IAM-Role`, and then click on the **Create role** button.
    The role appears as shown in *Figure 3**.5*:![Figure 3.5 – A screenshot of the
    IAM role](img/B21197_03_05.jpg)
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供一个名称，`Glue-IAM-Role`，然后点击**创建角色**按钮。角色如图3.5所示。5*:![Figure 3.5 – IAM角色的截图](img/B21197_03_05.jpg)
- en: Figure 3.5 – A screenshot of the IAM role
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 – IAM角色的截图
- en: Now, you have the input data source and the output data storage handy. The next
    step is to create the Glue crawler from the AWS Glue console.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经有输入数据源和输出数据存储在手头。下一步是从AWS Glue控制台创建Glue爬虫。
- en: Select `glue-redshift-connection`
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择`glue-redshift-connection`
- en: '`Amazon Redshift`'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Amazon Redshift`'
- en: Click on `redshift-glue-example`*   `glue-dev`*   `awsuser`*   `********` (enter
    the value chosen in *step 10*)*   Click on **Next** and then **Finish**. To verify
    that it’s working, click on **Test Connection**, select **Glue-IAM-Role** in the
    IAM role section, and then click **Test Connection**.*   Go to Crawler and select
    Add Crawler. Provide a name for the crawler, S3-glue-crawler, and then click Next.
    On the Specify crawler source type page, leave everything as their default settings
    and then click Next.*   On the`s3://aws-glue-example-01/input-data/sales-records.csv`.*   Click
    **Next**.*   Set`No`. Click **Next**.*   For `Glue-IAM-Role`. Then, click **Next**.*   Set
    **Frequency** to **Run on demand**. Click **Next**.*   No database has been created,
    so click on `s3-data`, click **Next**, and then click **Finish**.*   Select the
    crawler, `s3-data`, has been created, as mentioned in the previous step, and a
    table has been added. Click on **Tables** and select the newly created table,
    **sales_records_csv**. You can see that the schema has been discovered now. You
    can change the data type if the inferred data type does not meet your requirements.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击`redshift-glue-example`*   `glue-dev`*   `awsuser`*   `********`（输入第10步中选择的值）*   点击**下一步**然后**完成**。为了验证其是否正常工作，点击**测试连接**，在IAM角色部分选择**Glue-IAM-Role**，然后点击**测试连接**。*   前往爬虫并选择添加爬虫。为爬虫提供一个名称，S3-glue-crawler，然后点击下一步。在指定爬虫源类型页面，保持所有默认设置，然后点击下一步。*   在`s3://aws-glue-example-01/input-data/sales-records.csv`。*   点击**下一步**。*   设置`否`。点击**下一步**。*   对于`Glue-IAM-Role`。然后，点击**下一步**。*   将**频率**设置为**按需运行**。点击**下一步**。*   没有创建数据库，因此点击`s3-data`，点击**下一步**，然后点击**完成**。*   选择爬虫，`s3-data`，如前一步所述已创建，并已添加一个表。点击**表**并选择新创建的表，**sales_records_csv**。你现在可以看到模式已经被发现。如果推断的数据类型不符合你的要求，你可以更改数据类型。
- en: In this hands-on section, you learned about database tables, database connections,
    crawlers in S3, and the creation of a Redshift cluster. In the next hands-on section,
    you will learn about creating ETL jobs using Glue.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节动手实践中，你学习了数据库表、数据库连接、S3中的爬虫以及Redshift集群的创建。在下一节动手实践中，你将学习如何使用Glue创建ETL作业。
- en: '**Getting hands-on with AWS Glue ETL components**'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**AWS Glue ETL组件的实际操作**'
- en: 'In this section, you will use the Data Catalog components created earlier to
    build a job. You will start by creating a job:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将使用之前创建的数据目录组件来构建一个作业。你将首先创建一个作业：
- en: Navigate to the AWS Glue console and click on **Jobs** under the **ETL** section.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到AWS Glue控制台，并在**ETL**部分下点击**作业**。
- en: Click on the `s3-glue-redshift`
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**s3-glue-redshift**
- en: '`Spark`'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Spark`'
- en: '**Glue version**: **Spark 2.4, Python 3 with improved job start up times (Glue**
    **version 2.0)**'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Glue版本**：**Spark 2.4，Python 3，改进了作业启动时间（Glue版本2.0**）'
- en: Leave the other fields as they are and then click on **Next**.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保持其他字段不变，然后点击**下一步**。
- en: Select **sales_records_csv** and click on **Next**.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**sales_records_csv**并点击**下一步**。
- en: Select **Change Schema** by default and then click **Next** (at the time of
    writing this book, machine learning transformations are not supported for Glue
    2.0).
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认选择**更改模式**，然后点击**下一步**（在撰写本书时，Glue 2.0不支持机器学习转换）。
- en: Select `glue-dev` as the database name (as created in the previous section)
    and then click **Next**.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据库名设置为`glue-dev`（如前节中创建），然后点击**下一步**。
- en: Next comes the **Output Schema Definition** page, where you can choose the desired
    columns to be removed from the target schema. Scroll down and click on **Save
    job and** **edit script**.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是**输出模式定义**页面，您可以选择要从目标模式中删除的所需列。向下滚动并点击**保存作业并编辑脚本**。
- en: You can now see the pipeline being created on the left-hand side of the screen
    and the suggested code on the right-hand side, as shown in *Figure 3**.6*. You
    can modify the code based on your requirements. Click on the **Run job** button.
    A pop-up window appears, asking you to edit any details that you wish to change.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您现在可以在屏幕左侧看到正在创建的管道，以及右侧的推荐代码，如图*图3.6*所示。您可以根据需求修改代码。点击**运行作业**按钮。会出现一个弹出窗口，要求您编辑您希望更改的任何详细信息。
- en: 'This is optional. Then, click on the **Run** **job** button:'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是个可选步骤。然后，点击**运行作业**按钮：
- en: '![Figure 3.6 – A screenshot of the AWS Glue ETL job](img/B21197_03_06.jpg)'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图3.6 – AWS Glue ETL作业的屏幕截图](img/B21197_03_06.jpg)'
- en: Figure 3.6 – A screenshot of the AWS Glue ETL job
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 – AWS Glue ETL作业的屏幕截图
- en: Once the job is successful, navigate to Amazon Redshift and click on **Query
    editor**.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作业成功后，导航到Amazon Redshift并点击**查询编辑器**。
- en: Set the database name as `glue-dev` and then provide the username and password
    to create a connection.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据库名设置为`glue-dev`，然后提供用户名和密码以创建连接。
- en: Select the `public` schema, and now you can query the table to see the records,
    as shown in *Figure 3**.7*:![Figure 3.7 – A screenshot of Amazon Redshift’s Query
    editor](img/B21197_03_07.jpg)
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择`public`模式，现在您可以查询表以查看记录，如图*图3.7*所示：![图3.7 – Amazon Redshift查询编辑器的屏幕截图](img/B21197_03_07.jpg)
- en: Figure 3.7 – A screenshot of Amazon Redshift’s Query editor
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 – Amazon Redshift查询编辑器的屏幕截图
- en: You now understand how to create an ETL job using AWS Glue to copy the data
    from an S3 bucket to Amazon Redshift. You also queried data in Amazon Redshift
    using the query editor from the UI console. It is recommended to delete the Redshift
    cluster and AWS Glue job once you have completed the steps successfully. AWS creates
    two buckets in your account to store AWS Glue scripts and temporary results from
    AWS Glue. Delete these as well to save costs. You will use the data catalog created
    on S3 data in the next section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已了解如何使用AWS Glue创建ETL作业，将数据从S3存储桶复制到Amazon Redshift。您还使用UI控制台中的查询编辑器查询了Amazon
    Redshift中的数据。建议在成功完成步骤后删除Redshift集群和AWS Glue作业。AWS在您的账户中创建了两个存储桶来存储AWS Glue脚本和AWS
    Glue的临时结果。也请删除这些以节省成本。您将在下一节中使用在S3数据上创建的数据目录。
- en: In the following section, you will learn about querying S3 data using Athena.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将了解如何使用Athena查询S3数据。
- en: '**Querying S3 data us****ing Athena**'
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**使用Athena查询S3数据**'
- en: 'Athena is a serverless service designed for querying data stored in S3\. It
    is serverless because the client doesn’t manage the servers that are used for
    computation:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Athena是一种无服务器服务，旨在查询存储在S3中的数据。它是无服务器的，因为客户端不管理用于计算的服务器：
- en: Athena uses a schema to present the results against a query on the data stored
    in S3\. You define how (the way or the structure) you want your data to appear
    in the form of a schema and Athena reads the raw data from S3 to show the results
    as per the defined schema.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Athena使用模式来呈现对存储在S3中的数据的查询结果。您定义您希望数据以模式的形式呈现的方式（方式或结构），Athena从S3读取原始数据，根据定义的模式显示结果。
- en: 'The output can be used by other services for visualization, storage, or various
    analytics purposes. The source data in S3 can be in any of the following structured,
    semi-structured, or unstructured data formats: XML, JSON, CSV/TSV, AVRO, Parquet,
    or ORC (as well as others). CloudTrail, ELB logs, and VPC flow logs can also be
    stored in S3 and analyzed by Athena.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出可以被其他服务用于可视化、存储或各种分析目的。S3 中的源数据可以是以下任何一种结构化、半结构化或非结构化数据格式：XML、JSON、CSV/TSV、AVRO、Parquet
    或 ORC（以及其他格式）。CloudTrail、ELB 日志和 VPC 流日志也可以存储在 S3 中，并由 Athena 进行分析。
- en: This follows the schema-on-read technique. Unlike traditional techniques, tables
    are defined in advance in a data catalog, and the data’s structure is validated
    against the table’s schema while reading the data from the tables. SQL-like queries
    can be carried out on data without transforming the source data.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这遵循了读取时定义模式的技术。与传统技术不同，表在数据目录中预先定义，并且在从表中读取数据时，数据结构将与表的架构进行验证。可以在不转换源数据的情况下对数据进行类似
    SQL 的查询。
- en: 'Now, to help you understand this, here’s an example, where you will use **AWSDataCatalog**
    created in AWS Glue on the S3 data and query them using Athena:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了帮助您理解这一点，这里有一个示例，您将使用在 AWS Glue 上创建的 **AWSDataCatalog** 对 S3 数据进行操作，并使用
    Athena 查询它们：
- en: Navigate to the AWS Athena console. Select `AWSDataCatalog` from `sampledb`
    database will be created with a table, `elb_logs`, in the AWS Glue Data Catalog).
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到 AWS Athena 控制台。从 `sampledb` 数据库中选择 `AWSDataCatalog` 将在 AWS Glue 数据目录中创建一个表，名为
    `elb_logs`）。
- en: Select `s3-data` as the database.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 `s3-data` 作为数据库。
- en: Click on **Settings** in the top-right corner and fill in the details as shown
    in *Figure 3**.8* (I have used the same bucket as in the previous example and
    a different folder):![Figure 3.8 – A screenshot of Amazon Athena’s settings](img/B21197_03_08.jpg)
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击右上角的**设置**，并填写如图 3.8 所示的详细信息：（我在此示例中使用了与上一个示例相同的存储桶和不同的文件夹）：![图 3.8 – Amazon
    Athena 设置的截图](img/B21197_03_08.jpg)
- en: Figure 3.8 – A screenshot of Amazon Athena’s settings
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – Amazon Athena 设置的截图
- en: The next step is to write your query in the query editor and execute it. Once
    your execution is complete, please delete your S3 buckets and AWS Glue data catalogs.
    This will save you money.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是在查询编辑器中编写您的查询并执行它。一旦执行完成，请删除您的 S3 存储桶和 AWS Glue 数据目录。这将为您节省费用。
- en: In this section, you learned how to query S3 data using Amazon Athena through
    the AWS Glue Data Catalog. You also learned how to create a schema and query data
    from S3\. In the next section, you will learn about Amazon Kinesis Data Streams.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您学习了如何通过 AWS Glue 数据目录使用 Amazon Athena 查询 S3 数据。您还学习了如何创建模式并从 S3 查询数据。在下一节中，您将了解
    Amazon Kinesis Data Streams。
- en: '**Processing real-time data using Kinesis Da****ta Streams**'
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**使用 Kinesis Data Streams 处理实时数据**'
- en: 'Kinesis is Amazon’s streaming service and can be scaled based on requirements.
    It has a level of persistence that retains data for 24 hours by default or optionally
    up to 365 days. Kinesis Data Streams is used for large-scale data ingestion, analytics,
    and monitoring:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis 是亚马逊的流服务，可以根据需求进行扩展。它默认保留数据 24 小时，或者可选地最多保留 365 天。Kinesis Data Streams
    用于大规模数据摄取、分析和监控：
- en: Kinesis streams can be ingested by multiple producers and multiple consumers
    can also read data from the streams. The following is an example to help you understand
    this. Suppose you have a producer ingesting data to a Kinesis stream and the default
    retention period is 24 hours, which means data ingested at 05:00:00 A.M. today
    will be available in the stream until 04:59:59 A.M. tomorrow. This data won’t
    be available beyond that point, and ideally, it should be consumed before it expires;
    otherwise, it can be stored somewhere if it’s critical. The retention period can
    be extended to a maximum of 365 days, at an extra cost.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kinesis 流可以被多个生产者摄取，同时多个消费者也可以从流中读取数据。以下是一个示例，以帮助您理解这一点。假设您有一个生产者将数据摄取到 Kinesis
    流中，默认的保留期是 24 小时，这意味着今天早上 05:00:00 提取的数据将在流中可用，直到明天早上 04:59:59。在此之后，这些数据将不再可用，理想情况下，应该在它们过期之前被消费；否则，如果它们是关键数据，可以存储在别处。保留期可以延长至最多
    365 天，但需要额外付费。
- en: Kinesis can be used for real-time analytics or dashboard visualization. Producers
    can be imagined as a piece of code pushing data into the Kinesis stream, and it
    can be an EC2 instance, a Lambda function, an IoT device, on-premises servers,
    mobile applications or devices, and so on running the code.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kinesis可用于实时分析或仪表板可视化。生产者可以想象成将数据推入Kinesis流的一段代码，它可以是一个EC2实例、Lambda函数、IoT设备、本地服务器、移动应用程序或设备等，这些设备正在运行代码。
- en: Similarly, the consumer can also be a piece of code running on an EC2 instance,
    Lambda function, or on-premises servers that know how to connect to a Kinesis
    stream, read the data, and apply some action to the data. AWS provides triggers
    to invoke a Lambda consumer as soon as data arrives in the Kinesis stream.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，消费者也可以是一段在EC2实例、Lambda函数或本地服务器上运行的代码，这些服务器知道如何连接到Kinesis流、读取数据并对数据进行一些操作。AWS提供触发器，以便在Kinesis流中数据到达时立即调用Lambda消费者。
- en: Kinesis is scalable due to its shard architecture, which is the fundamental
    throughput unit of a Kinesis stream. *What is a shard?* A shard is a logical structure
    that partitions the data based on a partition key. A shard supports a writing
    capacity of *1 MB/sec* and a reading capacity of *2 MB/sec*. 1,000 `PUT` records
    per second are supported by a single shard. If you have created a stream with
    *three shards*, then *3 MB/sec write throughput* and *6 MB/sec read throughput*
    can be achieved, and this allows 3,000 `PUT` records. So, with more shards, you
    have to pay an extra amount to get higher performance.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kinesis由于其分片架构而具有可伸缩性，这是Kinesis流的基本吞吐量单元。*什么是分片？* 分片是根据分区键对数据进行分区的一个逻辑结构。分片支持每秒*1
    MB*的写入容量和每秒*2 MB*的读取容量。单个分片每秒支持1,000条`PUT`记录。如果您创建了一个包含*三个分片*的流，那么可以实现*3 MB/sec*的写入吞吐量和*6
    MB/sec*的读取吞吐量，这允许3,000条`PUT`记录。因此，随着分片数量的增加，您需要额外支付费用以获得更高的性能。
- en: The data in a shard is stored via a Kinesis data record and can be a maximum
    of 1 MB. Kinesis data records are stored across the shard based on the partition
    key. They also have a sequence number. A sequence number is assigned by Kinesis
    as soon as a `putRecord` or `putRecords` API operation is performed so as to uniquely
    identify a record. The partition key is specified by the producer while adding
    the data to the Kinesis data stream, and the partition key is responsible for
    segregating and routing the record to different shards in the stream to balance
    the load.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分片中的数据通过Kinesis数据记录进行存储，最大可达1 MB。Kinesis数据记录根据分区键跨分片存储。它们还有一个序列号。序列号在执行`putRecord`或`putRecords`
    API操作时由Kinesis分配，以便唯一标识一条记录。分区键由生产者在将数据添加到Kinesis数据流时指定，分区键负责将记录隔离和路由到流中的不同分片中以平衡负载。
- en: 'There are two ways to encrypt the data in a Kinesis stream: server-side encryption
    and client-side encryption. Client-side encryption makes it hard to implement
    and manage the keys because the client has to encrypt the data before putting
    it into the stream and decrypt the data after reading it from the stream. With
    server-side encryption enabled via **AWS KMS**, the data is automatically encrypted
    and decrypted as you put the data and get it from a stream.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kinesis流中加密数据有两种方式：服务器端加密和客户端加密。客户端加密使得密钥的实施和管理变得困难，因为客户端必须在将数据放入流之前对其进行加密，并在从流中读取数据之后对其进行解密。通过**AWS
    KMS**启用服务器端加密后，数据在您将数据放入流和从流中获取数据时会自动加密和解密。
- en: '**Note**'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: Amazon Kinesis shouldn’t be confused with Amazon SQS. Amazon SQS supports one
    production group and one consumption group. If your use case demands multiple
    users sending data and receiving data, then Kinesis is the solution.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 不要将Amazon Kinesis与Amazon SQS混淆。Amazon SQS支持一个生产组和消费组。如果您的用例需要多个用户发送和接收数据，那么Kinesis是解决方案。
- en: For decoupling and asynchronous communications, SQS is the solution, because
    the sender and receiver do not need to be aware of one another.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于解耦和异步通信，SQS是解决方案，因为发送者和接收者不需要相互了解。
- en: In SQS, there is no concept of persistence. Once the message is read, the next
    step is deletion. There’s no concept of a retention time window for Amazon SQS.
    If your use case demands large-scale ingestion, then Kinesis should be used.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQS中，没有持久性的概念。一旦读取消息，下一步就是删除。Amazon SQS没有保留时间窗口的概念。如果您的用例需要大规模摄取，则应使用Kinesis。
- en: In the next section, you will learn about storing the streamed data for further
    analysis.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将了解如何存储流数据以进行进一步分析。
- en: '**Storing and transforming real-time data using Kinesis Dat****a Firehose**'
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**使用Kinesis Data Firehose存储和转换实时数据**'
- en: 'There are a lot of use cases that require data to be streamed and stored for
    future analytics purposes. To overcome such problems, you can write a Kinesis
    consumer to read the Kinesis stream and store the data in S3\. This solution needs
    an instance or a machine to run the code with the required access to read from
    the stream and write to S3\. The other possible option would be to run a Lambda
    function that gets triggered on the `putRecord` or `putRecords` API made to the
    stream and reads the data from the stream to store in the S3 bucket:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多用例需要将数据流式传输并存储以供未来的分析。为了克服这些问题，你可以编写一个Kinesis消费者来读取Kinesis流并将数据存储在S3中。这个解决方案需要一个实例或机器来运行代码，并需要从流中读取和写入S3所需的访问权限。另一个可能的选择是运行一个Lambda函数，该函数在向流发出的`putRecord`或`putRecords`
    API触发时执行，并从流中读取数据以存储在S3桶中：
- en: To make this easy, Amazon provides a separate service called Kinesis Data Firehose.
    This can easily be plugged into a Kinesis data stream and will require essential
    IAM roles to write data into S3\. It is a fully managed service to reduce the
    load of managing servers and code. It also supports loading the streamed data
    into Amazon Redshift, Elasticsearch, and Splunk. Kinesis Data Firehose scales
    automatically to match the throughput of the data.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了使这个过程变得简单，Amazon提供了一个名为Kinesis Data Firehose的独立服务。它可以轻松地连接到Kinesis数据流，并需要基本的IAM角色来将数据写入S3。这是一个完全托管的服务，旨在减轻管理服务器和代码的负担。它还支持将流式数据加载到Amazon
    Redshift、Elasticsearch和Splunk。Kinesis Data Firehose会自动扩展以匹配数据的吞吐量。
- en: Data can be transformed via an AWS Lambda function before storing or delivering
    it to the destination. If you want to build a raw data lake with the untransformed
    data, then by enabling source record backup, you can store it in another S3 bucket
    prior to the transformation.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可以在存储或发送到目的地之前通过AWS Lambda函数进行转换。如果你想使用未转换的数据构建原始数据湖，那么通过启用源记录备份，你可以在转换之前将其存储在另一个S3桶中。
- en: With the help of AWS KMS, data can be encrypted following delivery to the S3
    bucket. It has to be enabled while creating the delivery stream. Data can also
    be compressed in supported formats, such as gzip, ZIP, or Snappy.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据发送到S3桶后，借助AWS KMS，可以对数据进行加密。必须在创建交付流时启用它。数据也可以以支持的格式（如gzip、ZIP或Snappy）进行压缩。
- en: In the next section, you will learn about different AWS services used for ingesting
    data from on-premises servers to AWS.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将了解用于从本地服务器摄取数据到AWS的不同AWS服务。
- en: '**Different ways of ingesting data from on****-premises** **into AWS**'
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**从本地到AWS的数据摄取的不同方式**'
- en: With the increasing demand for data-driven use cases, managing data on on-premises
    servers is pretty tough at the moment. Taking backups is not easy when you deal
    with a huge amount of data. This data in data lakes is used to build deep neural
    networks, create a data warehouse to extract meaningful information from it, run
    analytics, and generate reports.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对数据驱动用例需求的增加，目前管理本地服务器上的数据相当困难。当你处理大量数据时，备份并不容易。这些数据湖中的数据用于构建深度神经网络、创建数据仓库以从中提取有意义的信息、运行分析和生成报告。
- en: Now, if you look at the available options to migrate data into AWS, this comes
    with various challenges too. For example, if you want to send data to S3, then
    you have to write a few lines of code to send your data to AWS. You will have
    to manage the code and servers to run the code. It has to be ensured that the
    data is commuting via the HTTPS network. You need to verify whether the data transfer
    was successful. This adds complexity as well as time and effort challenges to
    the process. To avoid such scenarios, AWS provides services to match or solve
    your use cases by designing a hybrid infrastructure that allows data sharing between
    the on-premises data centers and AWS. You will learn about these in the following
    sections.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你查看将数据迁移到AWS的可选方案，这也带来了各种挑战。例如，如果你想将数据发送到S3，那么你必须编写几行代码将你的数据发送到AWS。你将需要管理代码和服务器以运行代码。必须确保数据通过HTTPS网络传输。你需要验证数据传输是否成功。这增加了过程的复杂性，同时也带来了时间和精力上的挑战。为了避免此类情况，AWS提供了一些服务，通过设计混合基础设施允许本地数据中心与AWS之间的数据共享，以匹配或解决你的用例。你将在接下来的章节中了解这些内容。
- en: '**AWS Storage Gateway**'
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**AWS存储网关**'
- en: 'Storage Gateway is a hybrid storage virtual appliance. It can run in three
    different modes – **File Gateway**, **Tape Gateway**, and **Volume Gateway**.
    It can be used for the extension, migration, and backups of an on-premises data
    center to AWS:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 存储网关是一种混合存储虚拟设备。它可以在三种不同的模式下运行——**文件网关**、**磁带网关**和**卷网关**。它可以用于将本地数据中心扩展、迁移和备份到AWS：
- en: In Tape Gateway mode, Storage Gateway stores virtual tapes on S3, and when ejected
    and archived, the tapes are moved from S3 to Glacier. Active tapes are stored
    in S3 for storage and retrieval. Archived or exported tapes are stored in **Virtual
    Tape Shelf** (**VTS**) in Glacier. Virtual tapes can be created and can range
    in size from 100 GiB to 5 TiB. A total of 1 petabyte of storage can be configured
    locally and an unlimited number of tapes can be archived to Glacier. This is ideal
    for an existing backup system on tape and where there is a need to migrate backup
    data into AWS. You can decommission the physical tape hardware later.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在磁带网关模式下，存储网关将虚拟磁带存储在S3上，当磁带被弹出并归档时，磁带将从S3移动到冰川。活动磁带存储在S3上进行存储和检索。归档或导出的磁带存储在冰川的**虚拟磁带架**（**VTS**）中。虚拟磁带可以创建，大小可以从100
    GiB到5 TiB不等。总共可以配置1个PB的本地存储，并且可以将无限数量的磁带归档到冰川。这对于现有的磁带备份系统和需要将备份数据迁移到AWS的场景来说非常理想。您可以在以后停用物理磁带硬件。
- en: In File Gateway mode, Storage Gateway maps files onto S3 objects, which can
    be stored using one of the available storage classes. This helps you to extend
    the data center into AWS. You can load more files to your file gateway and these
    are stored as S3 objects. It can run on your on-premises virtual server, which
    connects to various devices using **Server Message Block (SMB)** or **Network
    File System (NFS)**. File Gateway connects to AWS using an HTTPS public endpoint
    to store the data on S3 objects. Life cycle policies can be applied to those S3
    objects. You can easily integrate your **Active** **Directory** (**AD**) with
    File Gateway to control access to the files on the file share.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文件网关模式下，存储网关将文件映射到S3对象上，可以使用可用的存储类之一进行存储。这有助于您将数据中心扩展到AWS。您可以将更多文件加载到文件网关中，这些文件将作为S3对象存储。它可以在您的本地虚拟服务器上运行，该服务器通过**服务器消息块（SMB）**或**网络文件系统（NFS）**连接到各种设备。文件网关通过HTTPS公共端点连接到AWS，以在S3对象上存储数据。可以对那些S3对象应用生命周期策略。您可以将您的**活动**
    **目录**（**AD**）轻松集成到文件网关中，以控制对文件共享中文件的访问。
- en: 'In Volume Gateway mode, the storage gateway presents block storage. There are
    two ways of using this; one is **Gateway Cached** and the other is **Gateway Stored**:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在卷网关模式下，存储网关提供块存储。有两种使用方式；一种是**网关缓存**，另一种是**网关存储**：
- en: '**Gateway Stored** is a volume storage gateway running locally on-premises.
    It has local storage and an upload buffer. A total of 32 volumes can be created,
    and each volume can be up to 16 TB in size for a total capacity of 512 TB. Primary
    data is stored on-premises and backup data is asynchronously replicated to AWS
    in the background. Volumes are made available via **Internet Small Computer Systems
    Interface (iSCSI)** for network-based servers to access. It connects to a Storage
    Gateway endpoint via an HTTPS public endpoint and creates EBS snapshots from backup
    data. These snapshots can be used to create standard EBS volumes. This option
    is ideal for migration to AWS, disaster recovery, or business continuity. The
    local system will still use the local volume, but the EBS snapshots are in AWS,
    which can be used instead of backups. It’s not the best option for data center
    extensions because you require a huge amount of local storage.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网关存储**是一种本地运行的卷存储网关。它具有本地存储和上传缓冲区。总共可以创建32个卷，每个卷的大小可以达到16 TB，总容量为512 TB。主数据存储在本地，备份数据在后台异步复制到AWS。卷通过网络服务器访问，通过**互联网小型计算机系统接口（iSCSI）**提供。它通过HTTPS公共端点连接到存储网关端点，并从备份数据创建EBS快照。这些快照可以用来创建标准的EBS卷。此选项非常适合迁移到AWS、灾难恢复或业务连续性。本地系统将继续使用本地卷，但EBS快照存储在AWS中，可以用作备份的替代品。这不是数据中心扩展的最佳选择，因为您需要大量的本地存储。'
- en: '**Gateway Cached** is a volume storage gateway running locally on-premises.
    It has cache storage and an upload buffer. The difference is that the data that
    is added to Storage Gateway is not local but uploaded to AWS. Primary data is
    stored in AWS. Frequently accessed data is cached locally. This is an ideal option
    for extending an on-premises data center to AWS. It connects to a Storage Gateway
    endpoint via an HTTPS public endpoint and creates S3-backed volume (AWS-managed
    bucket) snapshots that are stored as standard EBS snapshots.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gateway Cached** 是一种本地运行的卷存储网关。它具有缓存存储和上传缓冲区。区别在于添加到存储网关的数据不是本地的，而是上传到AWS。主数据存储在AWS。频繁访问的数据在本地缓存。这是一个将本地数据中心扩展到AWS的理想选择。它通过HTTPS公共端点连接到存储网关端点，并创建由S3支持的卷快照（AWS管理的存储桶），这些快照作为标准EBS快照存储。'
- en: '**Snowball, Snowball Edge, and Snowmobile**'
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Snowball、Snowball Edge和Snowmobile**'
- en: 'These belong to the same product category or family for the physical transfer
    of data between business operating locations and AWS. To move a large amount of
    data into and out of AWS, you can use any of the three:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这些属于同一产品类别或家族，用于在业务运营地点和AWS之间物理传输数据。为了将大量数据移动到AWS或从AWS中移出，你可以使用以下三种中的任何一种：
- en: '**Snowball**: This physical device can be ordered from AWS by logging a job.
    AWS delivers a device for you to load your data onto before sending it back. Data
    in Snowball is encrypted using KMS. It comes with two capacity ranges: 50 TB and
    80 TB. It is economical to order one or more Snowball devices for data between
    10 TB and 10 PB. The device can be sent to different premises. It does not have
    any compute capability; it only comes with storage capability.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Snowball**: 这是一个可以通过提交工作订单从AWS订购的物理设备。AWS会为你提供一个设备，用于在发送回AWS之前加载你的数据。Snowball中的数据使用KMS加密。它提供两种容量范围：50
    TB和80 TB。对于10 TB到10 PB之间的数据，订购一个或多个Snowball设备是经济实惠的。该设备可以发送到不同的地点。它没有任何计算能力；它只提供存储能力。'
- en: '**Snowball Edge**: This is like Snowball, but it comes with both storage and
    compute capability. It has a larger capacity than Snowball. It offers fastened
    networking, such as 10 Gbps over RJ45, 10/25 Gb over SFP28, and 40/100 Gb+ over
    QSFP+ copper. This is ideal for the secure and quick transfer of terabytes to
    petabytes of data into AWS.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Snowball Edge**: 这就像Snowball，但它同时具备存储和计算能力。它的容量比Snowball大。它提供快速网络，例如通过RJ45的10
    Gbps、10/25 Gb通过SFP28以及40/100 Gb+通过QSFP+铜。这对于将千兆到太字节的数据安全快速传输到AWS是理想的。'
- en: '**Snowmobile**: This is a portable data center within a shipping container
    on a truck. It allows you to move exabytes of data from on-premises to AWS. If
    your data size exceeds 10 PB, then Snowmobile is preferred. Essentially, upon
    requesting to use the Snowmobile service, a truck is driven to your location and
    you plug your data center into the truck and transfer the data. If you have multiple
    sites, choosing Snowmobile for data transfer is not an ideal option.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Snowmobile**: 这是一个在卡车上的运输集装箱内的便携式数据中心。它允许你将埃字节的数据从本地移动到AWS。如果你的数据量超过10 PB，则Snowmobile是首选。本质上，在请求使用Snowmobile服务时，一辆卡车会被开到你的位置，你将数据中心连接到卡车上并传输数据。如果你有多个地点，选择Snowmobile进行数据传输不是一个理想的选择。'
- en: '**AWS DataSync**'
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**AWS DataSync**'
- en: 'AWS DataSync is designed to move data from on-premises storage to AWS, or vice
    versa:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: AWS DataSync旨在将数据从本地存储移动到AWS，或反之亦然：
- en: It is an ideal product from AWS for data processing transfers, archival or cost-effective
    storage, disaster recovery, business continuity, and data migrations.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是AWS为数据处理传输、归档或成本效益存储、灾难恢复、业务连续性和数据迁移提供的理想产品。
- en: It has a special data validation feature that verifies the original data with
    the data in AWS, as soon as the data arrives in AWS. In other words, it checks
    the integrity of the data.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它具有特殊的数据验证功能，可以在数据到达AWS后立即验证原始数据与AWS中的数据，换句话说，它检查数据的完整性。
- en: To understand this product in depth, consider an example of an on-premises data
    center that has SAN/NAS storage. When you run the AWS DataSync agent on a VMWare
    platform, this agent is capable of communicating with the NAS/SAN storage via
    an NFS/SMB protocol. Once it is on, it communicates with the AWS DataSync endpoint,
    and from there, it can connect with several different types of locations, including
    various S3 storage classes or VPC-based resources, such as **Elastic** **File**
    **System** (**EFS**) and FSx for Windows Server.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要深入了解此产品，可以考虑一个拥有 SAN/NAS 存储的本地数据中心示例。当您在 VMWare 平台上运行 AWS DataSync 代理时，此代理能够通过
    NFS/SMB 协议与 NAS/SAN 存储进行通信。一旦启动，它就会与 AWS DataSync 端点通信，然后可以从那里连接到多种不同类型的地点，包括各种
    S3 存储类别或基于 VPC 的资源，例如 **弹性文件系统**（**EFS**）和 Windows Server 的 FSx。
- en: It allows you to schedule data transfers during specific periods. By configuring
    the built-in bandwidth throttle, you can limit the amount of network bandwidth
    that DataSync uses.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许您在特定时间段内安排数据传输。通过配置内置的带宽限制，您可以限制 DataSync 使用的网络带宽量。
- en: '**AWS Database Migration Service**'
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**AWS 数据迁移服务**'
- en: There are several situations when an organization might decide to migrate their
    databases from one to another, such as the need for better performance, enhanced
    security, or advanced features or to avoid licensing costs from vendors. If an
    organization wants to expand its business to a different geolocation, it will
    need to carry out database migration, disaster recovery improvements, and database
    sync in a cost-effective manner. AWS DMS allows you to leverage the benefits of
    scalability, flexibility, and cost-efficiency when migrating databases from an
    `on-premises/EC2 instance/Amazon RDS` to Amazon RDS or Amazon Aurora.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，组织可能会决定将其数据库从一个迁移到另一个，例如需要更好的性能、增强的安全性、高级功能或避免来自供应商的许可费用。如果组织希望将其业务扩展到不同的地理位置，它将需要以经济高效的方式执行数据库迁移、灾难恢复改进和数据库同步。AWS
    DMS 允许您在将数据库从本地/EC2 实例/Amazon RDS 迁移到 Amazon RDS 或 Amazon Aurora 时利用可扩展性、灵活性和成本效益的优势。
- en: 'In scenarios where multiple databases need to be consolidated into a single
    database or data needs to be integrated across multiple databases, AWS DMS can
    be a valuable tool. AWS DMS is designed to move data from a source to a target
    provided one of the endpoints is on AWS:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要将多个数据库合并到单个数据库或需要在多个数据库之间集成数据的情况下，AWS DMS 可以成为一个有价值的工具。AWS DMS 设计为从一个源移动到目标，前提是其中一个端点位于
    AWS：
- en: DMS supports both homogenous and heterogeneous database migrations, allowing
    you to migrate between different database engines, such as Oracle, MySQL, PostgreSQL,
    and Microsoft SQL Server.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DMS 支持同构和异构数据库迁移，允许您在不同数据库引擎之间进行迁移，例如 Oracle、MySQL、PostgreSQL 和 Microsoft SQL
    Server。
- en: DMS simplifies the database migration process by handling schema conversion,
    data replication, and ongoing synchronization between the source and target databases.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DMS 通过处理模式转换、数据复制和源数据库与目标数据库之间的持续同步来简化数据库迁移过程。
- en: DMS supports both full-load and ongoing **Change Data Capture** (**CDC**) replication
    methods. Full-load migration copies the entire source database to the target,
    while CDC captures and replicates only the changes made after the initial load.
    For example, for a database migration with minimal downtime, DMS performs an initial
    full-load migration, followed by CDC replication to keep the target database up
    to date with changes in the source database.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DMS 支持全量加载和持续 **变更数据捕获**（**CDC**）复制方法。全量迁移会将整个源数据库复制到目标数据库，而 CDC 仅捕获并复制初始加载后所做的更改。例如，对于需要最小停机时间的数据库迁移，DMS
    首先执行初始全量迁移，然后通过 CDC 复制来保持目标数据库与源数据库更改的一致性。
- en: DMS provides a user-friendly console and API for the easy configuration, monitoring,
    and management of database migrations. It offers detailed logging and error handling
    to help diagnose and resolve migration issues. For example, during a migration
    from Oracle to Amazon Aurora, DMS can automatically convert Oracle-specific data
    types and modify table structures to align with the Aurora database schema.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DMS 提供了一个用户友好的控制台和 API，用于轻松配置、监控和管理数据库迁移。它提供详细的日志记录和错误处理，以帮助诊断和解决迁移问题。例如，在从
    Oracle 迁移到 Amazon Aurora 的过程中，DMS 可以自动转换 Oracle 特定的数据类型并修改表结构以与 Aurora 数据库模式保持一致。
- en: DMS supports continuous data replication, allowing you to keep the source and
    target databases in sync even after the initial migration. This is particularly
    useful in scenarios requiring ongoing data synchronization or database replication.
    An example is if a company maintains an active-active database setup for high
    availability, where DMS replicates data changes between multiple database instances
    located in different regions for real-time synchronization.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DMS支持持续数据复制，即使在初始迁移之后也能保持源数据库和目标数据库的同步。这在需要持续数据同步或数据库复制的场景中特别有用。例如，如果一家公司维护一个用于高可用性的活动-活动数据库设置，DMS将在不同地区的多个数据库实例之间复制数据更改，以实现实时同步。
- en: DMS offers built-in validation and testing capabilities to ensure the integrity
    and consistency of the migrated data. It performs data validation checks and generates
    reports to verify the success of the migration process. For example, after migrating
    a large database from Microsoft SQL Server to Amazon RDS for PostgreSQL, DMS validates
    the migrated data by comparing row counts, data types, and other metrics to ensure
    data accuracy.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DMS提供了内置的验证和测试功能，以确保迁移数据的完整性和一致性。它执行数据验证检查并生成报告，以验证迁移过程的成功。例如，在将大型数据库从Microsoft
    SQL Server迁移到Amazon RDS for PostgreSQL之后，DMS通过比较行数、数据类型和其他指标来验证迁移数据，以确保数据准确性。
- en: DMS supports both one-time migrations and continuous replication for database
    consolidation and integration scenarios. It enables organizations to consolidate
    data from multiple databases into a single target database or distribute data
    across multiple databases as needed. For example, say a company with several subsidiary
    databases wants to consolidate all the data into a centralized database for unified
    reporting and analysis. DMS facilitates the migration and ongoing synchronization
    of data from multiple sources to the target database.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DMS支持一次性迁移和持续复制，适用于数据库整合和集成场景。它使组织能够将来自多个数据库的数据合并到单个目标数据库，或根据需要跨多个数据库分发数据。例如，假设一家拥有多个子公司数据库的公司希望将所有数据合并到一个集中式数据库中，以便进行统一的报告和分析。DMS简化了从多个来源到目标数据库的数据迁移和持续同步。
- en: '**Processing stored d****ata on AWS**'
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**在AWS上处理存储的数据**'
- en: There are several services for processing the data stored in AWS. You will learn
    about AWS Batch and AWS `MapReduce` jobs and Spark applications in a managed way.
    AWS Batch is used for long-running, compute-heavy workloads.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种服务用于处理存储在AWS中的数据。您将了解AWS Batch和AWS `MapReduce`作业以及以托管方式运行的Spark应用程序。AWS Batch用于长时间运行、计算密集型的工作负载。
- en: '**AWS EMR**'
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**AWS EMR**'
- en: 'EMR is a managed implementation of Apache Hadoop provided as a service by AWS.
    It includes other components of the Hadoop ecosystem, such as Spark, HBase, Flink,
    Presto, Hive, and Pig. You will not need to learn about these in detail for the
    certification exam, but here’s some information about EMR:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: EMR是AWS提供的一项服务，作为Apache Hadoop的托管实现。它包括Hadoop生态系统的其他组件，如Spark、HBase、Flink、Presto、Hive和Pig。您不需要为认证考试详细了解这些内容，但这里有一些关于EMR的信息：
- en: EMR clusters can be launched from the AWS console or via the AWS CLI with a
    specific number of nodes. The cluster can be a long-term cluster or an ad hoc
    cluster. In a long-running traditional cluster, you have to configure the machines
    and manage them yourself. If you have jobs that need to be executed faster, then
    you need to manually add a cluster. In the case of EMR, these admin overheads
    disappear. You can request any number of nodes from EMR and it manages and launches
    the nodes for you. If you have autoscaling enabled on the cluster, EMR regulates
    nodes according to the requirement. That means, EMR launches new nodes in the
    cluster when the load is high and decommissions the nodes once the load is reduced.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMR集群可以从AWS控制台或通过AWS CLI以特定数量的节点启动。集群可以是长期集群或临时集群。在长时间运行的传统集群中，您必须自行配置机器并管理它们。如果您有需要更快执行的任务，那么您需要手动添加集群。在EMR的情况下，这些管理开销消失了。您可以从EMR请求任意数量的节点，并且它将为您管理和启动节点。如果您在集群上启用了自动扩展，EMR将根据需求调节节点。这意味着，当负载高时，EMR将在集群中启动新的节点，一旦负载减少，就会取消节点。
- en: EMR uses EC2 instances in the background and runs in one Availability Zone in
    a VPC. This enables faster network speeds between the nodes. AWS Glue uses EMR
    clusters in the background, where users do not need to worry about having an operational
    understanding of AWS EMR.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMR在后台使用EC2实例运行，在一个VPC中的可用区。这使节点之间的网络速度更快。AWS Glue在后台使用EMR集群，用户无需担心对AWS EMR的操作理解。
- en: From a use case standpoint, EMR can be used to process or transform the data
    stored in S3 and output data to be stored in S3\. EMR uses nodes (EC2 instances)
    as the computing units for data processing. EMR nodes come in different variants,
    including master nodes, core nodes, and task nodes.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从使用场景的角度来看，EMR可以用于处理或转换存储在S3中的数据，并将输出数据存储在S3中。EMR使用节点（EC2实例）作为数据处理单元。EMR节点有多种变体，包括主节点、核心节点和任务节点。
- en: The EMR master node acts as a Hadoop NameNode and manages the cluster and its
    health. It is responsible for distributing the job workload among the other core
    nodes and task nodes. If you have SSH enabled, then you can connect to the master
    node instance and access the cluster.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMR主节点充当Hadoop NameNode，管理集群及其健康。它负责在其他核心节点和任务节点之间分配作业工作负载。如果你启用了SSH，则可以连接到主节点实例并访问集群。
- en: An EMR cluster can have one or more core nodes. If you relate it to the Hadoop
    ecosystem, then core nodes are similar to Hadoop data nodes for HDFS and they
    are responsible for running the tasks within them.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMR集群可以有一个或多个核心节点。如果你将其与Hadoop生态系统联系起来，那么核心节点类似于Hadoop数据节点用于HDFS，它们负责在它们内部运行任务。
- en: Task nodes are optional and they don’t have HDFS storage. They are responsible
    for running tasks. If a task node fails for some reason, then this does not impact
    HDFS storage, but a core node failure causes HDFS storage interruptions.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务节点是可选的，它们没有HDFS存储。它们负责运行任务。如果任务节点因某种原因失败，则不会影响HDFS存储，但核心节点故障会导致HDFS存储中断。
- en: EMR has a filesystem called EMRFS. It is backed by S3, which makes it regionally
    resilient. If a core node fails, the data is still safe in S3\. HDFS is efficient
    in terms of I/O and faster than EMRFS.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMR有一个名为EMRFS的文件系统。它由S3支持，这使得它在区域上具有弹性。如果核心节点失败，数据仍然安全存储在S3中。在I/O效率和速度方面，HDFS比EMRFS更高效。
- en: In the following section, you will learn about AWS Batch, which is a managed
    batch-processing compute service that can be used for long-running services.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将了解AWS Batch，这是一个托管的批量处理计算服务，可用于长时间运行的服务。
- en: '**AWS Batch**'
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**AWS Batch**'
- en: 'This is a managed batch-processing product. If you are using AWS Batch, then
    jobs can be run without end user interaction or can be scheduled to run:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个托管批量处理产品。如果你使用AWS Batch，则可以无需终端用户交互运行作业，或者可以安排作业运行：
- en: Imagine an event-driven application that launches a Lambda function to process
    the data stored in S3\. If the processing time goes beyond 15 minutes, then Lambda
    stops the execution and fails. For such scenarios, AWS Batch is a better solution,
    where computation-heavy workloads can be scheduled or driven through API events.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想象一个事件驱动的应用程序，它启动Lambda函数来处理存储在S3中的数据。如果处理时间超过15分钟，则Lambda停止执行并失败。对于此类场景，AWS
    Batch是一个更好的解决方案，其中计算密集型工作负载可以通过API事件进行调度或驱动。
- en: AWS Batch is a good fit for use cases where a longer processing time is required
    or more computation resources are needed.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Batch适用于需要较长时间处理时间或更多计算资源的使用场景。
- en: AWS Batch jobs can be a script or an executable. One job can depend on another
    job. A job needs to be defined, such as who can run the job (with IAM permissions),
    where the job can be run (resources to be used), mount points, and other metadata.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Batch作业可以是脚本或可执行文件。一个作业可以依赖于另一个作业。需要定义作业，例如谁可以运行作业（使用IAM权限）、作业可以在哪里运行（要使用的资源）、挂载点和其他元数据。
- en: Jobs are submitted to queues, where they wait for compute environment capacity.
    These queues are associated with one or more compute environments.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务提交到队列中，在那里它们等待计算环境容量。这些队列与一个或多个计算环境相关联。
- en: Compute environments do the actual work of executing the jobs. These can be
    ECS or EC2 instances, or any computing resources. You can define their sizes and
    capacities too.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算环境执行实际的工作，执行作业。这些可以是ECS或EC2实例，或者任何计算资源。你还可以定义它们的大小和容量。
- en: Environments receive jobs from the queues based on their priority and execute
    them. They can be managed or unmanaged compute environments.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境根据其优先级从队列中接收作业并执行它们。它们可以是管理的或非管理的计算环境。
- en: AWS Batch can store the metadata in DynamoDB for further use and can also store
    the output in an S3 bucket.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Batch可以将元数据存储在DynamoDB中以便进一步使用，也可以将输出存储在S3桶中。
- en: '**Note**'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: If you get a question in the exam on an event-style workload that requires flexible
    compute, a higher disk space, no time limit (more than 15 minutes), or an effective
    resource limit, then the answer is likely to be AWS Batch.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在考试中遇到一个需要灵活计算、更高磁盘空间、无时间限制（超过15分钟）或有效资源限制的事件式工作负载的问题，那么答案很可能是AWS Batch。
- en: '**Summary**'
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**摘要**'
- en: In this chapter, you learned about different ways of processing data in AWS.
    You also learned the capabilities in terms of extending your data centers to AWS,
    migrating data to AWS, and the ingestion process. You learned about the various
    ways of using data to process it and make it ready for analysis. You understood
    the magic of using a data catalog, which helps you to query your data via AWS
    Glue and Athena.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了在AWS中处理数据的不同方式。你还学习了将数据中心扩展到AWS、将数据迁移到AWS以及数据摄取过程的能力。你学习了使用数据来处理数据并将其准备好进行分析的各种方法。你了解了使用数据目录的魔力，这可以帮助你通过AWS
    Glue和Athena查询你的数据。
- en: In the next chapter, you will learn about various machine learning algorithms
    and their usage.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习各种机器学习算法及其用法。
- en: Exam Readiness Drill – Chapter Review Questions
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 考试准备练习 – 章节复习题
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对关键概念有扎实的理解外，能够在时间压力下快速思考是一项有助于你通过认证考试的能力。这就是为什么在学习的早期阶段就培养这些技能是关键。
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 章节复习题旨在通过学习每个章节并复习章节中的关键概念来逐步提高你的应试技巧。你将在每个章节的末尾找到这些内容。
- en: How To Access These Resources
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如何访问这些资源
- en: To learn how to access these resources, head over to the chapter titled [*Chapter
    11*](B21197_11.xhtml#_idTextAnchor1477), *Accessing the Online* *Practice Resources*.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何访问这些资源，请转到名为[*第11章*](B21197_11.xhtml#_idTextAnchor1477)的章节，*访问在线练习资源*。
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要打开本章的章节复习题，请执行以下步骤：
- en: Click the link – [https://packt.link/MLSC01E2_CH03](https://packt.link/MLSC01E2_CH03).
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击链接 – [https://packt.link/MLSC01E2_CH03](https://packt.link/MLSC01E2_CH03)。
- en: 'Alternatively, you can scan the following **QR code** (*Figure 3**.9*):'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，你可以扫描以下**二维码**（*图3.9*）：
- en: '![Figure 3.9 – QR code that opens Chapter Review Questions for logged-in users](img/B21197_03_09.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图3.9 – 打开登录用户章节复习题的二维码](img/B21197_03_09.jpg)'
- en: Figure 3.9 – QR code that opens Chapter Review Questions for logged-in users
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 – 打开登录用户章节复习题的二维码
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 3**.10*:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你登录，你会看到一个类似于*图3.10*所示的页面：
- en: '![Figure 3.10 – Chapter Review Questions for Chapter 3](img/B21197_03_10.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图3.10 – 第3章的章节复习题](img/B21197_03_10.jpg)'
- en: Figure 3.10 – Chapter Review Questions for Chapter 3
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 – 第3章的章节复习题
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备好之后，开始以下练习，多次重新尝试测验。
- en: Exam Readiness Drill
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 考试准备练习
- en: For the first three attempts, don’t worry about the time limit.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前三次尝试，不要担心时间限制。
- en: ATTEMPT 1
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试1
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次尝试，目标至少达到**40%**。查看你答错的答案，并再次阅读章节中相关的部分，以修复你的学习差距。
- en: ATTEMPT 2
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试2
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次尝试，目标至少达到**60%**。查看你答错的答案，并再次阅读章节中相关的部分，以修复任何剩余的学习差距。
- en: ATTEMPT 3
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试3
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 第三次尝试，目标至少达到**75%**。一旦得分达到75%或更高，你就可以开始练习时间管理。
- en: Tip
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要超过**三次**尝试才能达到75%。这没关系。只需复习章节中的相关部分，直到你达到目标。
- en: Working On Timing
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正在练习时间管理
- en: 'Target: Your aim is to keep the score the same while trying to answer these
    questions as quickly as possible. Here’s an example of how your next attempts
    should look like:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 目标：你的目标是保持得分不变，同时尽可能快速地回答这些问题。以下是你下一次尝试应该看起来像的样子：
- en: '| **Attempt** | **Score** | **Time Taken** |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| **尝试** | **得分** | **用时** |'
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 尝试5次 | 77% | 21分30秒 |'
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 尝试6次 | 78% | 18分34秒 |'
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 尝试7次 | 76% | 14分44秒 |'
- en: Table 3.1 – Sample timing practice drills on the online platform
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1 – 在线平台上的样本时间练习练习
- en: Note
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 上表中显示的时间限制只是示例。根据网站上的测验时间限制，每次尝试时为自己设定时间限制。
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 每次新的尝试，你的得分应保持在**75%**以上，同时完成所需的时间“应减少”。你可以重复尝试任意次数，直到你觉得自己能够自信地应对时间压力。
