- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Modeling for NLP
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP建模
- en: '**Natural language processing** (**NLP**) is a field operating at the intersection
    of linguistics, computer science, and AI. Its primary focus is algorithms to process
    and analyze large amounts of natural language data. Over the last few years, it
    has become an increasingly popular topic of Kaggle competitions. While the domain
    itself is very broad and encompasses very popular topics such as chatbots and
    machine translation, in this chapter we will focus on specific subsets that Kaggle
    contests frequently deal with.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）是一个在语言学、计算机科学和人工智能交叉领域的学科。其主要关注点是算法，用于处理和分析大量自然语言数据。在过去的几年里，它已经成为Kaggle竞赛中越来越受欢迎的话题。虽然该领域本身非常广泛，包括非常受欢迎的话题，如聊天机器人和机器翻译，但在本章中，我们将专注于Kaggle竞赛经常涉及的具体子集。'
- en: 'Sentiment analysis as a simple classification problem is extremely popular
    and discussed all over, so we’ll begin with a somewhat more interesting variation
    on the problem: identifying sentiment-supporting phrases in a tweet. We’ll proceed
    to describe an example solution to the problem of open domain question answering
    and conclude with a section on augmentation for NLP problems, which is a topic
    that receives significantly less attention than its computer vision counterpart.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 将情感分析视为一个简单的分类问题非常受欢迎，并且被广泛讨论，因此我们将从对问题的某种更有趣的变体开始：在推文中识别情感支持短语。我们将继续描述一个开放域问答问题的示例解决方案，并以一个关于NLP问题增强的章节结束，这是一个比其计算机视觉对应物受到更多关注的主题。
- en: 'To summarize, we will cover:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们将涵盖：
- en: Sentiment analysis
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析
- en: Open domain Q&A
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开放域问答
- en: Text augmentation strategies
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本增强策略
- en: Sentiment analysis
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析
- en: Twitter is one of the most popular social media platforms and an important communication
    tool for many, individuals and companies alike.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter是最受欢迎的社会媒体平台之一，也是许多个人和公司的重要沟通工具。
- en: 'Capturing sentiment in language is particularly important in the latter context:
    a positive tweet can go viral and spread the word, while a particularly negative
    one can be harmful. Since human language is complicated, it is important not to
    just decide on the sentiment, but also to be able to investigate the *how*: which
    words actually led to the sentiment description?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在后一种情况下，在语言中捕捉情感尤为重要：一条积极的推文可以迅速传播，而一条特别消极的推文可能会造成伤害。由于人类语言很复杂，因此不仅需要决定情感，而且还需要能够调查“如何”：哪些单词实际上导致了情感描述？
- en: We will demonstrate an approach to this problem by using data from the *Tweet
    Sentiment Extraction* competition ([https://www.kaggle.com/c/tweet-sentiment-extraction](https://www.kaggle.com/c/tweet-sentiment-extraction)).
    For brevity, we have omitted the imports from the following code, but you can
    find them in the corresponding Notebook in the GitHub repo for this chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用来自*推文情感提取*竞赛（[https://www.kaggle.com/c/tweet-sentiment-extraction](https://www.kaggle.com/c/tweet-sentiment-extraction)）的数据来展示解决这个问题的一种方法。为了简洁起见，我们省略了以下代码中的导入，但您可以在GitHub上本章相应笔记本中找到它们。
- en: 'To get a better feel for the problem, let’s start by looking at the data:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解这个问题，让我们先看看数据：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here are the first few rows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是前几行：
- en: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B17574_11_01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![包含桌子的图像 自动生成的描述](img/B17574_11_01.png)'
- en: 'Figure 11.1: Sample rows from the training data'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：训练数据中的样本行
- en: The actual tweets are stored in the `text` column. Each of them has an associated
    `sentiment`, along with the **support phrase** stored in the `selected_text` column
    (the part of the tweet that was the basis for the decision on sentiment assignment).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的推文存储在`text`列中。每个推文都有一个相关的`sentiment`，以及存储在`selected_text`列中的**支持短语**（基于情感分配决策的推文部分）。
- en: 'We start by defining basic cleanup functions. First, we want to get rid of
    website URLs and non-characters and replace the stars people use in place of swear
    words with a single token, `"swear"`. We use some regular expressions to help
    us do this:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义基本的清理函数。首先，我们想要去除网站URL和非字符，并将人们用来代替脏话的星号替换为单个标记，“swear”。我们使用一些正则表达式来帮助我们完成这项工作：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we remove HTML from the content of the tweets, as well as emojis:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从推文的正文中移除HTML以及表情符号：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Lastly, we want to be able to remove repeated characters (for example, so we
    have “way” instead of “waaaayyyyy”):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们希望能够移除重复的字符（例如，这样我们就有“way”而不是“waaaayyyyy”）：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For convenience, we combine the four functions into a single cleanup function:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，我们将四个函数组合成一个单一的清理函数：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The last bit of preparation involves writing functions for creating the embeddings
    based on a pre-trained model (the `tokenizer` argument):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的准备涉及编写基于预训练模型（`tokenizer`参数）创建嵌入的函数：
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we create a pre-processing function enabling us to work with the entire
    corpus:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个预处理函数，使我们能够处理整个语料库：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Using our previously prepared functions, we can clean and prepare the training
    data. The `sentiment` column is our target, and we convert it to dummy variables
    (one-hot encoding) for performance:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们之前准备好的函数，我们可以清理和准备训练数据。`sentiment`列是我们的目标，我们将其转换为虚拟变量（独热编码）以提高性能：
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'A necessary next step is **tokenization** of the input texts, as well as conversion
    into sequences (along with padding, to ensure equal lengths across the dataset):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个必要的步骤是对输入文本进行**分词**，以及将它们转换为序列（包括填充，以确保数据集跨数据集的长度相等）：
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will create the embeddings for our model using **DistilBERT** and use them
    as-is. DistilBERT is a lightweight version of BERT: the tradeoff is 3% performance
    loss at 40% fewer parameters. We could train the embedding layer and gain performance
    – at the cost of massively increased training time.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用**DistilBERT**为我们的模型创建嵌入，并直接使用它们。DistilBERT是BERT的一个轻量级版本：权衡是参数减少40%时的性能损失3%。我们可以训练嵌入层并提高性能——但代价是大幅增加的训练时间。
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can use the previously defined `fast_encode` function, along with the `fast_tokenizer`
    defined above, to encode the tweets:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用之前定义的`fast_encode`函数，以及上面定义的`fast_tokenizer`，来编码推文：
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'With the data prepared, we can construct the model. For the sake of this demonstration,
    we will go with a fairly standard architecture for these applications: a combination
    of LSTM layers, normalized by global pooling and dropout, and a dense layer on
    top. In order to achieve a truly competitive solution, some tweaking of the architecture
    would be needed: a “heavier” model, bigger embeddings, more units in the LSTM
    layers, and so on.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备就绪后，我们可以构建模型。为了这次演示，我们将采用这些应用中相当标准的架构：结合LSTM层、通过全局池化和dropout归一化，以及顶部的密集层。为了实现真正有竞争力的解决方案，需要对架构进行一些调整：一个“更重”的模型，更大的嵌入，LSTM层中的更多单元，等等。
- en: '[PRE11]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'There is no special need to pay attention to a temporal dimension of the data,
    so we are fine with a random split into training and validation, which can be
    achieved inside a call to the `fit` method:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据的时序维度没有特殊需要关注，所以我们满足于将数据随机分为训练集和验证集，这可以在调用`fit`方法内部实现：
- en: '[PRE12]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Below is some sample output:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些示例输出：
- en: '[PRE13]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Generating a prediction from the fitted model proceeds in a straightforward
    manner. In order to utilize all the available data, we begin by re-training our
    model on all available data (so no validation):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从拟合的模型生成预测的过程是直接的。为了利用所有可用数据，我们首先在所有可用数据上重新训练我们的模型（因此没有验证）：
- en: '[PRE14]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We refit the model on the entire dataset before generating the predictions:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成预测之前，我们在整个数据集上重新拟合了模型：
- en: '[PRE15]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Our next step is to process the test data into the same format we are using
    for training data fed into the model:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一步是将测试数据处理成与用于模型训练数据相同的格式：
- en: '[PRE16]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we generate the predictions:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们生成预测：
- en: '[PRE17]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The final model shows **0.74** accuracy on the test set. Below we show a sample
    of what the output looks like; as you can see already from these few rows, there
    are some instances where the sentiment is obvious to a human reader, but the model
    fails to capture it:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最终模型在测试集上的准确率为**0.74**。以下我们展示了一些输出样例；正如您从这些几行中已经可以看到的，有些情况下情感对人类读者来说很明显，但模型未能捕捉到：
- en: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B17574_11_02.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![包含桌子的图像 自动生成的描述](img/B17574_11_02.png)'
- en: 'Figure 11.2: Example rows from the predicted results'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2：预测结果中的示例行
- en: 'We have now demonstrated a sample pipeline for solving sentiment attribution
    problems (identifying parts of the text that lead to annotator decisions on sentiment
    classification). There are some improvements that can be made if you want to achieve
    competitive performance, given below in order of likely impact:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经演示了一个用于解决情感归属问题的样本管道（识别导致标注者在情感分类决策中做出决定的文本部分）。如果您想实现有竞争力的性能，以下是一些可以进行的改进，按可能的影响顺序排列：
- en: '**Larger embeddings**: This allows us to capture more information already at
    the (processed) input data level'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更大的嵌入**：这使我们能够在（处理过的）输入数据级别上捕获更多信息'
- en: '**Bigger model**: More units in the LSTM layers'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更大的模型**：LSTM层中的单元更多'
- en: '**Longer training**: In other words, more epochs'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更长的训练**：换句话说，更多的epoch'
- en: 'While the improvements listed above will undoubtedly boost the performance
    of the model, the core elements of our pipeline are reusable:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上述改进无疑会提高模型的性能，但我们管道的核心元素是可重用的：
- en: Data cleaning and pre-processing
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据清洗和预处理
- en: Creating text embeddings
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建文本嵌入
- en: Incorporating recurrent layers and regularization in the target model architecture
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在目标模型架构中结合循环层和正则化
- en: We’ll now move on to a discussion of open domain question answering, a frequent
    problem encountered in NLP competitions.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将讨论开放域问答，这是在NLP竞赛中经常遇到的问题。
- en: '![](img/Abhishek_Thakur.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Abhishek_Thakur.png)'
- en: Abhishek Thakur
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Abhishek Thakur
- en: '[https://www.kaggle.com/abhishek](https://www.kaggle.com/abhishek)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/abhishek](https://www.kaggle.com/abhishek)'
- en: We caught up with Abhishek Thakur, the world’s first quadruple Kaggle Grandmaster.
    He currently works at Hugging Face, where he is building AutoNLP; he also wrote
    pretty much the only book on Kaggle in English (aside from this one!), *Approaching
    (Almost) Any Machine Learning Problem*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采访了Abhishek Thakur，他是世界上第一位四重Kaggle大师。他目前在Hugging Face工作，在那里他正在构建AutoNLP；他还写了几乎唯一一本关于Kaggle的英文书籍（除了这本书之外！）*接近（几乎）任何机器学习问题*。
- en: What’s your specialty on Kaggle?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你在Kaggle上的专长是什么？
- en: '*None. Every competition is different and there is so much to learn from each
    one of them. If I were to have a specialty, I would win all competitions in that
    domain.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*没有。每个竞赛都不同，从每个竞赛中都可以学到很多东西。如果我要有一个专长，我会在那个领域的所有竞赛中获胜。*'
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何处理Kaggle竞赛的？这种处理方式与你在日常工作中所做的是否不同？
- en: '*The first thing I do is to take a look at the data and try to understand it
    a bit. If I’m late to the competition, I take the help of public EDA kernels.*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*我首先会查看数据，并试图理解它。如果我在竞赛中落后了，我会寻求公共EDA核的帮助。*'
- en: '*The first thing I do when approaching a problem on (or off) Kaggle is to build
    a benchmark. Building a benchmark is very important as it provides you with a
    baseline you can compare your future models to. If I’m late to the game, for building
    the baseline, I try not to take the help of public Notebooks. If we do that, we
    think only in a single direction. At least, that’s what I feel.*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*当我接近Kaggle（或非Kaggle）上的问题时，我首先会建立一个基准。建立基准非常重要，因为它为你提供了一个可以比较未来模型的基准。如果我在建立基准方面落后了，我会尽量避免使用公共Notebooks。如果我们那样做，我们只会朝一个方向思考。至少，我感觉是这样的。*'
- en: '*When I am done with a benchmark, I try to squeeze as much as possible without
    doing anything complicated like stacking or blending. Then I go over the data
    and models again and try to improve on the baseline, one step at a time.*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*当我完成一个基准测试后，我会尽量在不做任何复杂操作，比如堆叠或混合的情况下，尽可能多地提取信息。然后我会再次检查数据和模型，并尝试逐步改进基准。*'
- en: '*Day-to-day work sometimes has a lot of similarities. Most of the time there
    is a benchmark and then you have to come up with techniques, features, models
    that beat the benchmark.*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*日常工作中有时有很多相似之处。大多数时候都有一个基准，然后你必须提出技术、特征、模型来击败基准。*'
- en: What was the most interesting competition you entered? Did you have any special
    insights?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你参加的最有趣的竞赛是什么？你有什么特别的见解吗？
- en: '*Every competition is interesting.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*每个竞赛都很吸引人。*'
- en: Has Kaggle helped you in your career?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是否帮助你在职业生涯中取得进展？
- en: '*Sure, it has helped. In the last few years, Kaggle has gained a very good
    reputation when it comes to hiring data scientists and machine learning engineers.
    Kaggle rank and experience with many datasets is something that surely helps in
    the industry in one way or another. The more experienced you are with approaching
    different types of problems, the faster you will be able to iterate. And that’s
    something very useful in industries. No one wants to spend several months doing
    something that doesn’t bring any value to the business.*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*当然，它有帮助。在过去的几年里，Kaggle在招聘数据科学家和机器学习工程师方面赢得了非常好的声誉。Kaggle排名和与许多数据集的经验无疑在行业中以某种方式有所帮助。你越熟悉处理不同类型的问题，你迭代的速度就越快。这在行业中是非常有用的。没有人愿意花几个月的时间做对业务没有任何价值的事情。*'
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的经验中，不经验丰富的Kagglers通常忽略什么？您现在知道什么，而您希望自己在最初开始时就知道？
- en: '*Most beginners give up quite easily. It’s very easy to join a Kaggle competition
    and get intimidated by top scorers. If beginners want to succeed on Kaggle, they
    have to have perseverance. In my opinion, perseverance is the key. Many beginners
    also fail to start on their own and stick to public kernels. This makes them think
    like the authors of public kernels. My advice would be to start with competitions
    on your own, look at data, build features, build models, and then dive into kernels
    and discussions to see what others might be doing differently. Then incorporate
    what you have learned into your own solution.*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*大多数初学者很容易放弃。加入Kaggle竞赛并受到顶尖选手的威胁是非常容易的。如果初学者想在Kaggle上成功，他们必须要有毅力。在我看来，毅力是关键。许多初学者也未能独立开始，而是一直坚持使用公共内核。这使得他们像公共内核的作者一样思考。我的建议是先从自己的竞赛开始，查看数据，构建特征，构建模型，然后深入内核和讨论，看看其他人可能有什么不同的做法。然后，将您所学到的知识融入到自己的解决方案中。*'
- en: Open domain Q&A
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开放域问答
- en: In this section, we will be looking at the *Google QUEST Q&A Labeling* competition
    ([https://www.kaggle.com/c/google-quest-challenge/overview/description](https://www.kaggle.com/c/google-quest-challenge/overview/description)).
    In this competition, question-answer pairs were evaluated by human raters on a
    diverse set of criteria, such as “question conversational,” “question fact-seeking,”
    or “answer helpful.” The task was to predict a numeric value for each of the target
    columns (corresponding to the criteria); since the labels were aggregated across
    multiple raters, the objective was effectively a multivariate regression output,
    with target columns normalized to the unit range.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨*Google QUEST Q&A Labeling*竞赛([https://www.kaggle.com/c/google-quest-challenge/overview/description](https://www.kaggle.com/c/google-quest-challenge/overview/description))。在这个竞赛中，问题-答案对由人类评分员根据一系列标准进行评估，例如“问题对话性”、“问题求证事实”或“答案有帮助”。任务是预测每个目标列（对应于标准）的数值；由于标签是跨多个评分员汇总的，因此目标列实际上是一个多元回归输出，目标列被归一化到单位范围。
- en: Before engaging in modeling with advanced techniques (like transformer-based
    models for NLP), it is frequently a good idea to establish a baseline with simpler
    methods. As with the previous section, we will omit the imports for brevity, but
    you can find them in the Notebook in the GitHub repo.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用高级技术（如基于transformer的NLP模型）进行建模之前，通常使用更简单的方法建立一个基线是一个好主意。与上一节一样，为了简洁，我们将省略导入部分，但您可以在GitHub仓库中的笔记本中找到它们。
- en: 'We begin by defining several helper functions, which can help us extract different
    aspects of the text. First, a function that will output a word count given a string:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义几个辅助函数，这些函数可以帮助我们提取文本的不同方面。首先，一个函数将输出给定字符串的词数：
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The metric used in the competition was **Spearman correlation** (linear correlation
    computed on ranks: [https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 竞赛中使用的指标是**斯皮尔曼相关系数**（在排名上计算的线性相关系数：[https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient))。
- en: 'Since we intend to build a Scikit-learn pipeline, it is useful to define the
    metric as a scorer (the `make_scorer` method is a wrapper in Scikit-learn that
    takes a scoring function – like accuracy or MSE – and returns a callable that
    scores an output of the estimator):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们打算构建一个Scikit-learn管道，定义一个指标作为评分器是有用的（`make_scorer`方法是一个Scikit-learn的包装器，它接受一个评分函数——如准确度或MSE——并返回一个可调用的对象，该对象对估计器的输出进行评分）：
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, a small helper function to extract successive chunks of size `n` from
    `l`. This will help us later with generating embeddings for our body of text without
    running into memory problems:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，一个小型的辅助函数，用于从`l`中提取大小为`n`的连续块。这将帮助我们稍后在不遇到内存问题时为我们的文本生成嵌入：
- en: '[PRE20]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Part of the feature set we will use is embeddings from pre-trained models. Recall
    that the idea of this section is the construction of a baseline without training
    elaborate models, but this need not prevent us from using existing ones.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的特征集的一部分是来自预训练模型的嵌入。回想一下，本节的想法是构建一个不训练复杂模型的基线，但这并不妨碍我们使用现有的模型。
- en: 'We begin by importing the tokenizer and model, and then we process the corpus
    in chunks, encoding each question/answer into a fixed-size embedding:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入分词器和模型，然后以块的形式处理语料库，将每个问题/答案编码为固定大小的嵌入：
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can now proceed to load the data:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以继续加载数据：
- en: '[PRE22]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here are the first few rows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是前几行：
- en: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B17574_11_03.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![包含桌子的图片 自动生成的描述](img/B17574_11_03.png)'
- en: 'Figure 11.3: Sample rows from the training data'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：训练数据中的样本行
- en: 'We specify our 30 target columns of interest:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定了我们感兴趣的30个目标列：
- en: '[PRE23]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: For a discussion of their meaning and interpretation, the reader is referred
    to the competition’s **Data** page, at [https://www.kaggle.com/c/google-quest-challenge/data](https://www.kaggle.com/c/google-quest-challenge/data).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 关于它们的意义和解释的讨论，读者可以参考竞赛的**数据**页面，在[https://www.kaggle.com/c/google-quest-challenge/data](https://www.kaggle.com/c/google-quest-challenge/data)。
- en: 'Next, we proceed with **feature engineering**. We start by counting the words
    in the title and body of the question, as well as the answer. This is a simple
    yet surprisingly useful feature in many applications:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进行**特征工程**。我们首先计算问题标题和正文以及答案中的单词数量。这是一个简单但出人意料的有用特征，在许多应用中都很受欢迎：
- en: '[PRE24]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The next feature we create is **lexical diversity**, counting the proportion
    of unique words in a chunk of text:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的下一个特征是**词汇多样性**，计算文本块中独特单词的比例：
- en: '[PRE25]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'When dealing with information sourced from online, we can extract potentially
    informative features by examining the components of a website address (where we
    define components as elements of the address separated by dots); we count the
    number of components, and store individual ones as features:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理来自在线的信息时，我们可以通过检查网站地址的组成部分（我们将组成部分定义为由点分隔的地址元素）来提取可能的信息性特征；我们计算组成部分的数量，并将单个组成部分作为特征存储：
- en: '[PRE26]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Numerous target columns deal with how relevant the answer is for a given question.
    One possible way of quantifying this relationship is evaluating **shared words**
    within a pair of strings:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 许多目标列处理的是答案对于给定问题的相关性。一种量化这种关系的方法是评估字符串对中的**共享单词**：
- en: '[PRE27]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Stopwords and punctuation occurrence patterns can tell us something about the
    style and intent:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词和标点符号的出现模式可以告诉我们一些关于风格和意图的信息：
- en: '[PRE28]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: With the “vintage” features prepared – where our focus is on simple summary
    statistics of the text, without paying heed to semantic structure – we can move
    on to creating **embeddings** for the questions and answers. We could theoretically
    train a separate word2vec-type model on our data (or fine-tune an existing one),
    but for the sake of this presentation we will use a pre-trained model as-is. A
    useful choice is the **Universal Sentence Encoder** from Google ([https://tfhub.dev/google/universal-sentence-encoder/4](https://tfhub.dev/google/universal-sentence-encoder/4)).
    This model is trained on a variety of data sources. It takes as input a piece
    of text in English and outputs a 512-dimensional vector.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备“复古”特征时——我们的重点是文本的简单汇总统计，而不关注语义结构——我们可以继续为问题和答案创建**嵌入**。理论上，我们可以在我们的数据上训练一个单独的word2vec类型的模型（或者微调现有的一个），但为了这次演示，我们将直接使用预训练的模型。一个有用的选择是来自Google的**通用句子编码器**([https://tfhub.dev/google/universal-sentence-encoder/4](https://tfhub.dev/google/universal-sentence-encoder/4))。这个模型在多种数据源上进行了训练。它接受一段英文文本作为输入，并输出一个512维度的向量。
- en: '[PRE29]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The code for turning the text fields into embeddings is presented below: we
    loop through the entries in the training/test sets in batches, embed each batch
    (for memory efficiency reasons), and then append them to the original list.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本字段转换为嵌入的代码如下：我们按批处理遍历训练/测试集中的条目，为每个批次嵌入（出于内存效率的考虑），然后将它们附加到原始列表中。
- en: 'The final data frames are constructed by stacking each list of batch-level
    embeddings vertically:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的数据框是通过垂直堆叠每个批次级别的嵌入列表构建的：
- en: '[PRE30]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Given the vector representations for both questions and answers, we can calculate
    the semantic similarity between the fields by using different distance metrics
    on the pairs of vectors. The idea behind trying different metrics is the desire
    to capture diverse types of characteristics; an analogy in the context of classification
    would be to use both accuracy and entropy to get a complete picture of the situation:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 给定问题和答案的向量表示，我们可以通过在向量对上使用不同的距离度量来计算字段之间的语义相似度。尝试不同度量的背后的想法是希望捕捉到各种类型的特征；在分类的背景下，这可以类比为使用准确性和熵来全面了解情况：
- en: '[PRE31]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let’s gather the distance features in separate columns:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分别收集距离特征到不同的列中：
- en: '[PRE32]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Finally, we can also create **TF-IDF** representations of the text fields; the
    general idea is to create multiple features based on diverse transformations of
    the input text, and then feed them to a relatively simple model.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还可以创建文本字段的**TF-IDF**表示；一般想法是创建基于输入文本多种变换的多个特征，然后将它们输入到一个相对简单的模型中。
- en: This way, we can capture the characteristics of the data without the need to
    fit a sophisticated deep learning model.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以在不拟合复杂深度学习模型的情况下捕获数据的特征。
- en: 'We can achieve it by analyzing the text at the word as well as the character
    level. To limit the memory consumption, we put an upper bound on the maximum number
    of both kinds of features (your mileage might vary; with more memory, these limits
    can be upped):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过分析文本的词和字符级别来实现这一点。为了限制内存消耗，我们对这两种类型的特征的最大数量设置上限（你的里程可能会有所不同；如果有更多的内存，这些限制可以增加）：
- en: '[PRE33]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We instantiate character- and word-level vectorizers. The setup of our problem
    lends itself to a convenient usage of the `Pipeline` functionality from Scikit-learn,
    allowing a combination of multiple steps in the model fitting procedure. We begin
    by creating two separate transformers for the title column (word- and character-level):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实例化了字符级和词级向量器。我们问题的设置使得Scikit-learn的`Pipeline`功能的使用变得方便，允许在模型拟合过程中组合多个步骤。我们首先为标题列创建两个单独的转换器（词级和字符级）：
- en: '[PRE34]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We use the same logic (two different pipelined transformers) for the body:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对正文使用相同的逻辑（两个不同的管道转换器）：
- en: '[PRE35]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And finally for the answer column:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于答案列：
- en: '[PRE36]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We wrap up the feature engineering part by processing the numerical features.
    We use simple methods only: missing value imputation to take care of N/A values
    and a power transformer to stabilize the distribution and make it closer to Gaussian
    (which is frequently helpful if you are using a numerical feature inside a neural
    network):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过处理数值特征来结束特征工程部分。我们只使用简单的方法：用缺失值插补处理N/A值，并使用幂转换器来稳定分布并使其更接近高斯分布（如果你在神经网络中使用数值特征，这通常很有帮助）：
- en: '[PRE37]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'A useful feature of Pipelines is they can be combined and nested. Next, we
    add functionality to handle categorical variables, and then put it all together
    in a `ColumnTransformer` object to streamline the data pre-processing and feature
    engineering logic. Each part of the input can be handled in its own appropriate
    manner:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Pipelines的一个有用特性是它们可以被组合和嵌套。接下来，我们添加处理分类变量的功能，然后将所有这些整合到一个`ColumnTransformer`对象中，以简化数据预处理和特征工程逻辑。输入的每个部分都可以以适当的方式处理：
- en: '[PRE38]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Finally, we are ready to use a `Pipeline` object combining pre-processing and
    model fitting:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备好使用一个结合预处理和模型拟合的`Pipeline`对象：
- en: '[PRE39]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'It is always a good idea to evaluate the performance of your model out of sample:
    a convenient way to go about this is to create **out-of-fold predictions**, which
    we discussed in *Chapter 6*. The procedure involves the following steps:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 总是评估你的模型在样本外的性能是一个好主意：一个方便的做法是创建**折叠外预测**，这在第6章中已经讨论过。该过程涉及以下步骤：
- en: Split the data into folds. In our case we use `GroupKFold`, since one question
    can have multiple answers (in separate rows of the data frame). In order to prevent
    information leakage, we want to ensure each question only appears in one fold.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分成折叠。在我们的案例中，我们使用`GroupKFold`，因为一个问题可以有多个答案（在数据框的单独行中）。为了防止信息泄露，我们想要确保每个问题只出现在一个折叠中。
- en: For each fold, train the model using the data in the other folds, and generate
    the predictions for the fold of choice, as well as the test set.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个折叠，使用其他折叠中的数据训练模型，并为选择的折叠以及测试集生成预测。
- en: Average the predictions on the test set.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对测试集上的预测进行平均。
- en: 'We start with preparing the “storage” matrices in which we will store the predictions.
    `mvalid` will contain the out-of-fold predictions, while `mfull` is a placeholder
    for the predictions on the entire test set, averaged across folds. Since several
    questions contain more than one candidate answer, we stratify our `KFold` split
    on `question_body`:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始准备“存储”矩阵，我们将在这里存储预测。`mvalid`将包含折叠外的预测，而`mfull`是整个测试集预测的占位符，这些预测在折叠间平均。由于几个问题包含多个候选答案，我们在`question_body`上对`KFold`分割进行分层：
- en: '[PRE40]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We loop through the folds and build the separate models:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历折叠并构建单独的模型：
- en: '[PRE41]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Once the fitting part is done, we can evaluate the performance in accordance
    with the metric specified in the competition:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦拟合部分完成，我们就可以根据竞赛中指定的指标来评估性能：
- en: '[PRE42]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The final score is **0.34**, which is fairly acceptable as a starting point.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最终得分是 **0.34**，作为一个起点来说相当可以接受。
- en: In this section, we have demonstrated how to build descriptive features on a
    body of text. While this is not a winning formula for an NLP competition (the
    score is OK, but not a guarantee for landing in the medal zone), it is a useful
    tool to keep in your toolbox. We close this chapter with a section providing an
    overview of text augmentation techniques.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了如何在文本体上构建描述性特征。虽然这并不是NLP竞赛的获胜公式（得分尚可，但并不能保证进入奖牌区），但它是一个值得保留在工具箱中的有用工具。我们以一个概述文本增强技术的章节来结束这一章。
- en: '![](img/Shotaro_Ishihara.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Shotaro_Ishihara.png)'
- en: Shotaro Ishihara
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Shotaro Ishihara
- en: '[https://www.kaggle.com/sishihara](https://www.kaggle.com/sishihara)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/sishihara](https://www.kaggle.com/sishihara)'
- en: Our second interview of the chapter is with Shotaro Ishihara, aka u++, a Competitions
    and Notebooks Master who was a member of the winning team in the *PetFinder.my
    Adoption Prediction* competition. He is currently a Data Scientist and Researcher
    at a Japanese news media company, and has also published books in Japanese on
    Kaggle, including a translation of Abhishek Thakur’s book. He maintains a weekly
    newsletter in Japanese on Kaggle initiatives ([https://www.getrevue.co/profile/upura](https://www.getrevue.co/profile/upura)).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这一章节的第二位访谈对象是Shotaro Ishihara，别名u++，他是*PetFinder.my Adoption Prediction*竞赛获胜团队的成员，同时也是一位竞赛和笔记大师。他目前是一家日本新闻媒体公司的数据科学家和研究员，并在日本出版了关于Kaggle的书籍，包括Abhishek
    Thakur书籍的翻译。他还维护着一个关于Kaggle活动的每周日文通讯 ([https://www.getrevue.co/profile/upura](https://www.getrevue.co/profile/upura))).
- en: Where can we find the Kaggle books you’ve written/translated?
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在哪里可以找到你写的/翻译的Kaggle书籍？
- en: '[https://www.kspub.co.jp/book/detail/5190067.html](https://www.kspub.co.jp/book/detail/5190067.html)
    *is a Kaggle primer for beginners based on the* Titanic *GettingStarted competition.*'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kspub.co.jp/book/detail/5190067.html](https://www.kspub.co.jp/book/detail/5190067.html)
    是基于*Titanic GettingStarted*竞赛的Kaggle入门指南。'
- en: '[https://book.mynavi.jp/ec/products/detail/id=123641](https://book.mynavi.jp/ec/products/detail/id=123641)
    *is the Japanese translation of Abhishek Thakur’s* Approaching (Almost) Any Machine
    Learning Problem.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://book.mynavi.jp/ec/products/detail/id=123641](https://book.mynavi.jp/ec/products/detail/id=123641)
    是Abhishek Thakur的《Approaching (Almost) Any Machine Learning Problem》的日文翻译。'
- en: What’s your favorite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你最喜欢的竞赛类型是什么？为什么？在技术和解决方法方面，你在Kaggle上的专长是什么？
- en: '*In Kaggle, I love joining competitions with tabular or text datasets. These
    types of datasets are familiar to me because they are widely used in news media
    companies. I have a good knowledge of the approaches used to handle these datasets.*'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*在Kaggle，我喜欢参加那些包含表格或文本数据集的竞赛。这类数据集对我来说很熟悉，因为它们在新闻媒体公司中广泛使用。我对处理这些数据集的方法有很好的了解。*'
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何参加Kaggle竞赛的？这种方法和你在日常工作中所做的方法有何不同？
- en: '*The first process is the same: thinking about how to tackle the problem through
    exploratory data analysis. Kaggle assumes the use of advanced machine learning,
    but this is not the case in business. In practice, I try to find ways to avoid
    using machine learning. Even when I do use it, I prefer working with classical
    methods such as TF-IDF and linear regression rather than advanced methods such
    as BERT.*'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*第一个过程是相同的：通过探索性数据分析来思考如何解决这个问题。Kaggle假设会使用高级机器学习，但在商业中并非如此。在实践中，我试图找到避免使用机器学习的方法。即使我确实使用了它，我也更愿意使用TF-IDF和线性回归等经典方法，而不是BERT等高级方法。*'
- en: We are interested in learning more about how to avoid using machine learning
    in real-world problems. Can you give us some examples?
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对了解如何避免在现实世界问题中使用机器学习感兴趣。你能给我们举一些例子吗？
- en: '*When working on automated article summaries at work, we adopt a more straightforward
    extractive approach (*[https://www.jstage.jst.go.jp/article/pjsai/JSAI2021/0/JSAI2021_1D2OS3a03/_article/-char/en](https://www.jstage.jst.go.jp/article/pjsai/JSAI2021/0/JSAI2021_1D2OS3a03/_article/-char/en)*)
    rather than a neural network-based method (*[https://www.jstage.jst.go.jp/article/pjsai/JSAI2021/0/JSAI2021_1D4OS3c02/_article/-char/en](https://www.jstage.jst.go.jp/article/pjsai/JSAI2021/0/JSAI2021_1D4OS3c02/_article/-char/en)*).*'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*在工作上处理自动文章摘要时，我们采用了一种更直接的提取方法（[https://www.jstage.jst.go.jp/article/pjsai/JSAI2021/0/JSAI2021_1D2OS3a03/_article/-char/en](https://www.jstage.jst.go.jp/article/pjsai/JSAI2021/0/JSAI2021_1D2OS3a03/_article/-char/en)），而不是基于神经网络的方法（[https://www.jstage.jst.go.jp/article/pjsai/JSAI2021/0/JSAI2021_1D4OS3c02/_article/-char/en](https://www.jstage.jst.go.jp/article/pjsai/JSAI2021/0/JSAI2021_1D4OS3c02/_article/-char/en)）。*'
- en: '*It is difficult to guarantee 100% performance with machine learning, and simple
    methods that are easy for humans to understand and engage with are sometimes preferred.*'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*使用机器学习很难保证100%的性能，有时人们更倾向于选择简单的方法，这些方法易于人类理解和参与。*'
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 请告诉我们你参加的一个特别具有挑战性的竞赛，以及你用来解决这个任务的见解。
- en: '*In the* PetFinder.my Adoption Prediction *competition, a multi-modal dataset
    was provided. Many participants tried to explore and use all types of data, and
    the main approach was to extract features from images and texts, concatenate them,
    and train LightGBM. I also employed the same approach. Surprisingly, one of my
    teammates, takuoko (*[https://www.kaggle.com/takuok](https://www.kaggle.com/takuok)*),
    developed a great neural network that handles all datasets end to end. Well-designed
    neural networks have the potential to outperform LightGBM in multi-modal competitions.
    This is a lesson I learned in 2019.*'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*在*PetFinder.my Adoption Prediction*竞赛中，提供了一个多模态数据集。许多参赛者试图探索和使用所有类型的数据，主要方法是从图像和文本中提取特征，将它们连接起来，并训练LightGBM。我也采用了同样的方法。令人惊讶的是，我的一个队友takuoko（[https://www.kaggle.com/takuok](https://www.kaggle.com/takuok)）开发了一个处理所有数据集端到端的大神经网络。设计良好的神经网络有可能在多模态竞赛中优于LightGBM。这是我在2019年学到的一课。*'
- en: Is that lesson still valid today?
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教训今天仍然有效吗？
- en: '*I think the answer is yes. Compared to 2019, neural networks are getting better
    and better at handling multimodal data.*'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*我认为答案是肯定的。与2019年相比，神经网络在处理多模态数据方面越来越好。*'
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是否对你的职业生涯有所帮助？如果是的话，是如何帮助的？
- en: '*Yes. Kaggle gave me a lot of experience in data analysis. The machine learning
    knowledge I’ve gained from Kaggle has significantly helped me to work more successfully.
    My achievements in Kaggle and business work were one of the main reasons why I
    received the 30 Under 30 Awards and Grand Prize in 2020 from the International
    News Media Association. Kaggle has also allowed me to get to know a lot of people.
    These relationships have definitely contributed to my career development.*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*是的。Kaggle给了我很多数据分析的经验。我从Kaggle获得的人工智能知识极大地帮助我更成功地工作。我在Kaggle和商业工作中的成就是我获得2020年国际新闻媒体协会颁发的30
    Under 30奖项和大奖的主要原因之一。Kaggle还让我结识了许多人。这些关系无疑对我的职业发展做出了贡献。*'
- en: How have you built up your portfolio thanks to Kaggle?
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何通过Kaggle建立起你的作品集的？
- en: '*Learned skills, achieved competition results, and published Notebooks, books,
    newsletters, and so on.*'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*学习到的技能、取得的竞赛结果以及发布的Notebooks、书籍、通讯等。*'
- en: How do you promote your publishing?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何推广你的发布的？
- en: '*I have various communication channels and I use the appropriate tools for
    promotion. For example, Twitter, personal blogs, and YouTube.*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*我拥有多种沟通渠道，并使用适当的工具进行推广。例如，Twitter、个人博客和YouTube。*'
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的经验中，没有经验的Kagglers通常忽略了什么？你现在知道什么，而你在最初开始时希望知道的？
- en: '*The importance of exploratory data analysis. In the field of machine learning,
    there is a concept of the No Free Lunch theorem. We should not only learn algorithms,
    but also learn how to address challenges. The No Free Lunch theorem is a statement
    that there is no universal model that performs well on all problems.*'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '*探索性数据分析的重要性。在机器学习领域，有一个无免费午餐定理的概念。我们不仅应该学习算法，还应该学习如何应对挑战。无免费午餐定理是一个声明，即没有一种通用的模型能够在所有问题上都表现良好。*'
- en: '*In machine learning competitions, it is essential to find a model that is
    appropriate to the characteristics of the dataset and the task in order to improve
    your score.*'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*在机器学习竞赛中，找到适合数据集和任务特性的模型对于提高你的分数至关重要。*'
- en: What mistakes have you made in competitions in the past?
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你在过去比赛中犯过哪些错误？
- en: '*Overfitting to the public leaderboard. In the* LANL Earthquake Prediction
    *competition, I scored pretty well on the public leaderboard and finished the
    competition at the rank of fifth. However, my final ranking was 211*^(st)*, which
    means I believed too much in a limited dataset. Overfitting is a very popular
    concept in machine learning, and I realized the importance of this with pain through
    Kaggle.*'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*过度拟合到公共排行榜。在* LANL 地震预测 *竞赛中，我在公共排行榜上得分很高，最终以第五名的成绩完成了比赛。然而，我的最终排名是211*^(th)*，这意味着我过于相信了一个有限的数据集。过度拟合是机器学习中一个非常流行的概念，我在Kaggle的痛苦经历中意识到了这一点的重要性。*'
- en: Do you suggest any particular way to avoid overfitting?
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你建议采取什么特定的方法来避免过度拟合？
- en: '*It is important to observe carefully how the training and evaluation datasets
    are divided. I try to build a validation set that reproduces this partitioning.*'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*仔细观察训练集和评估集是如何划分的非常重要。我试图构建一个验证集，使其能够重现这种划分。*'
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis or machine learning?
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你会推荐使用哪些特定的工具或库来进行数据分析或机器学习？
- en: '*I love Pandas, which is an essential library for handling tabular datasets.
    I use it for exploratory data analysis by extracting, aggregating, and visualizing.*'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*我喜欢Pandas，这是一个处理表格数据集的必备库。我通过提取、聚合和可视化来使用它进行数据探索性分析。*'
- en: What do you suggest readers do to master Pandas?
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你建议读者如何掌握Pandas？
- en: '*You can look at some community tutorials. Kaggle also provides some learning
    tutorial courses on Pandas and feature engineering.*'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以查看一些社区教程。Kaggle还提供了一些关于Pandas和特征工程的教程课程。*'
- en: Do you use other competition platforms? How do they compare to Kaggle?
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用其他竞赛平台吗？它们与Kaggle相比如何？
- en: '*I sometimes use Japanese platforms like Signate, Nishika, etc. (*[https://upura.github.io/projects/data_science_competitions/](https://upura.github.io/projects/data_science_competitions/)*).
    These are obviously inferior to Kaggle in terms of functionality and UX/UX, but
    it’s interesting to see familiar subjects like the Japanese language.*'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*我有时会使用像Signate、Nishika等日本平台（[https://upura.github.io/projects/data_science_competitions/](https://upura.github.io/projects/data_science_competitions/)）。这些平台在功能和UX/UX方面显然不如Kaggle，但看到熟悉的主题，如日语，还是很有趣的。*'
- en: Text augmentation strategies
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本增强策略
- en: We discussed augmentation strategies for computer vision problems extensively
    in the previous chapter. By contrast, similar approaches for textual data are
    a less well-explored topic (as evidenced by the fact there is no single package
    like `albumentations`). In this section, we demonstrate some of the possible approaches
    to addressing the problem.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一章中广泛讨论了计算机视觉问题的增强策略。相比之下，针对文本数据的类似方法是一个不太被探索的领域（正如没有像`albumentations`这样的单一包所证明的那样）。在本节中，我们展示了处理该问题的可能方法之一。
- en: Basic techniques
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本技术
- en: As usual, it is informative to examine the basic approaches first, focusing
    on random changes and synonym handling. A systematic study of the basic approaches
    is provided in *Wei* and *Zou* (2019) at [https://arxiv.org/abs/1901.11196](https://arxiv.org/abs/1901.11196).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，首先检查基本方法是有益的，重点关注随机变化和同义词处理。Wei和Zou（2019）在[https://arxiv.org/abs/1901.11196](https://arxiv.org/abs/1901.11196)提供了对基本方法的有系统研究。
- en: 'We begin with **synonym replacement**. Replacing certain words with their synonyms
    produces text that is close in meaning to the original, but slightly perturbed
    (see the project page at [https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)
    if you are interested in more details, like where the synonyms are actually coming
    from):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从**同义词替换**开始。用同义词替换某些单词会产生与原文意思相近但略有扰动的文本（如果你对同义词的来源等更多细节感兴趣，可以查看[https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)上的项目页面）。
- en: '[PRE43]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We create a simple wrapper around the workhorse function defined above, specifying
    a chunk of text (a string containing multiple words) and replace at most *n* of
    the words:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上述定义的工作函数周围创建了一个简单的包装器，指定一段文本（包含多个单词的字符串），并最多替换其中的*n*个单词：
- en: '[PRE44]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let’s see how the function works in practice:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个函数在实际中的应用：
- en: '[PRE45]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Not quite what you would call Shakespearean, but it does convey the same message
    while changing the style markedly. We can extend this approach by creating multiple
    new sentences per tweet:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不完全符合莎士比亚的风格，但它确实传达了相同的信息，同时显著改变了风格。我们可以通过为每条推文创建多个新句子来扩展这种方法：
- en: '[PRE47]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: As you can see, generating variations of a text chunk using synonyms is quite
    straightforward.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，使用同义词生成文本片段的变体相当直接。
- en: Next, **swapping** is a simple and efficient method; we create a modified sentence
    by randomly swapping the order of words in the text.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，**交换**是一种简单而有效的方法；我们通过随机交换文本中单词的顺序来创建一个修改后的句子。
- en: 'Carefully applied, this can be viewed as a potentially useful form of **regularization**,
    as it disturbs the sequential nature of the data that models like LSTM rely on.
    The first step is to define a function swapping words:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细应用，这可以被视为一种可能有用的**正则化**形式，因为它干扰了像LSTM这样的模型所依赖的数据的顺序性。第一步是定义一个交换单词的函数：
- en: '[PRE49]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Then, we write a wrapper around this function:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们围绕这个函数编写一个包装器：
- en: '[PRE50]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Synonyms and swapping do not affect the length of the sentence we are modifying.
    If in a given application it is useful to modify that attribute, we can remove
    or add words to the sentence.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 同义词和交换不会影响我们修改的句子的长度。如果在一个特定的应用中修改该属性是有用的，我们可以在句子中删除或添加单词。
- en: 'The most common way to implement the former is to delete words at random:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 实现前者的最常见方法是随机删除单词：
- en: '[PRE51]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let’s look at some examples:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些例子：
- en: '[PRE52]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'If we can remove, we can also add, of course. Random insertion of words to
    a sentence can be viewed as the NLP equivalent of adding noise or blur to an image:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以删除，当然也可以添加。在句子中随机插入单词可以被视为NLP中向图像添加噪声或模糊的等效操作：
- en: '[PRE54]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Here is the function in action:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这是函数在行动中的样子：
- en: '[PRE55]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We can combine all the transformations discussed above into a single function,
    producing four variants of the same sentence:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将上述讨论的所有变换组合成一个单一的功能，生成相同句子的四个变体：
- en: '[PRE57]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The augmentation methods discussed above do not exploit the structure of text
    data - to give one example, even analyzing a simple characteristic like “part
    of speech” can help us construct more useful transformations of the original text.
    This is the approach we will now focus on.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 上文讨论的增强方法没有利用文本数据的结构——以一个例子来说，分析像“词性”这样的简单特征可以帮助我们构建更有用的原始文本变换。这是我们接下来要关注的方法。
- en: nlpaug
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: nlpaug
- en: We conclude this section by demonstrating the capabilities provided by the `nlpaug`
    package ([https://github.com/makcedward/nlpaug](https://github.com/makcedward/nlpaug)).
    It aggregates different methods for text augmentation and is designed to be lightweight
    and easy to incorporate into a workflow. We demonstrate some examples of the functionality
    contained therein below.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过展示`nlpaug`包（[https://github.com/makcedward/nlpaug](https://github.com/makcedward/nlpaug)）提供的功能来结束本节。它聚合了文本增强的不同方法，并设计得轻量级且易于集成到工作流程中。以下是一些包含其中的功能示例。
- en: '[PRE59]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We import the character- and word-level augmenters, which we will use to plug
    in specific methods:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入字符级和词级增强器，我们将使用它们来插入特定方法：
- en: '[PRE60]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'What happens when we apply a **simulated typo** to our test sentence? This
    transformation can be parametrized in a number of ways; for a full list of parameters
    and their explanations, the reader is encouraged to examine the official documentation:
    [https://nlpaug.readthedocs.io/en/latest/augmenter/char/keyboard.html](https://nlpaug.readthedocs.io/en/latest/augmenter/char/keyboard.html).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将**模拟的打字错误**应用于测试句子时会发生什么？这种转换可以用多种方式参数化；有关参数及其解释的完整列表，鼓励读者查阅官方文档：[https://nlpaug.readthedocs.io/en/latest/augmenter/char/keyboard.html](https://nlpaug.readthedocs.io/en/latest/augmenter/char/keyboard.html)。
- en: '[PRE61]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This is the output:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '[PRE62]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We can simulate an **OCR error** creeping into our input:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以模拟一个**OCR错误**逐渐渗透到我们的输入中：
- en: '[PRE63]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We get:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到：
- en: '[PRE64]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'While useful, character-level transformations have a limited scope when it
    comes to creative changes in the data. Let us examine what possibilities `nlpaug`
    offers when it comes to word-level modifications. Our first example is replacing
    a fixed percentage of words with their antonyms:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有用，但在对数据进行创造性更改时，字符级变换的适用范围有限。让我们看看`nlpaug`在词级修改方面提供了哪些可能性。我们的第一个例子是用反义词替换固定百分比的单词：
- en: '[PRE65]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We get:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到：
- en: '[PRE66]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '`nlpaug` also offers us a possibility for, for example, replacing synonyms;
    such transformations can also be achieved with the more basic techniques discussed
    above. For completeness’ sake, we demonstrate a small sample below, which uses
    a BERT architecture under the hood:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`nlpaug` 还为我们提供了例如替换同义词的可能性；这些转换也可以使用上面讨论的更基本的技术来实现。为了完整性，我们下面展示了一个小样本，它使用了一个底层的
    BERT 架构：'
- en: '[PRE67]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Here is the result:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE68]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: As you can see, `nlpaug` offers a broad range of options for modifying your
    textual input to generate augmentations. Which ones should actually be chosen
    is very much context-dependent and the decision requires a little bit of domain
    knowledge, suited to a particular application.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`nlpaug` 为修改您的文本输入以生成增强提供了广泛的选择。实际上应该选择哪一些很大程度上取决于上下文，并且这个决定需要一点领域知识，适合特定的应用。
- en: Some places for further exploration would be beginner competitions such as *Natural
    Language Processing with Disaster Tweets* ([https://www.kaggle.com/c/nlp-getting-started](https://www.kaggle.com/c/nlp-getting-started)),
    as well as more intermediate or advanced ones like *Jigsaw Rate Severity of Toxic
    Comments* ([https://www.kaggle.com/c/jigsaw-toxic-severity-rating](https://www.kaggle.com/c/jigsaw-toxic-severity-rating))
    or *Google QUEST Q&A Labeling* ([https://www.kaggle.com/c/google-quest-challenge](https://www.kaggle.com/c/google-quest-challenge)).
    In all of these cases, `nlpaug` has been widely used – including in the winning
    solutions.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 一些可供进一步探索的地方包括入门级比赛，例如 *使用灾难推文的自然语言处理* ([https://www.kaggle.com/c/nlp-getting-started](https://www.kaggle.com/c/nlp-getting-started))，以及更中级或高级的比赛，如
    *Jigsaw 毒性评论严重程度评级* ([https://www.kaggle.com/c/jigsaw-toxic-severity-rating](https://www.kaggle.com/c/jigsaw-toxic-severity-rating))
    或 *Google QUEST Q&A 标注* ([https://www.kaggle.com/c/google-quest-challenge](https://www.kaggle.com/c/google-quest-challenge))。在这些所有情况下，`nlpaug`
    都已被广泛使用——包括在获奖方案中。
- en: Summary
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed modeling for NLP competitions. We demonstrate
    both vintage and state-of-the-art methods applicable to a diverse range of problems
    appearing in Kaggle competitions. In addition, we touched upon the frequently
    ignored topic of text augmentation.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了 NLP 竞赛的建模。我们展示了适用于 Kaggle 竞赛中出现的各种问题的传统和最先进的方法。此外，我们还触及了经常被忽视的主题——文本增强。
- en: In the next chapter, we will discuss simulation competitions, a new class of
    contests that has been gaining popularity over the last few years.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论模拟比赛，这是一种近年来逐渐流行起来的新型竞赛类别。
- en: Join our book’s Discord space
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们本书的 Discord 空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 加入本书的 Discord 工作空间，参加每月一次的作者 *问我任何问题* 活动：
- en: '[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)'
- en: '![](img/QR_Code40480600921811704671.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code40480600921811704671.png)'
