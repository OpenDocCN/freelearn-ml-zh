- en: '*Chapter 9*: Training ML Models at Scale in SageMaker Studio'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*：在SageMaker Studio中大规模训练机器学习模型'
- en: A typical ML life cycle starts with prototyping and will transition to a production
    scale where the data gets larger, models get more complicated, and the runtime
    environment gets more complex. Getting a training job done requires the right
    set of tools. Distributed training using multiple computers to share the load
    addresses situations that involve large datasets and large models. However, as
    complex ML training jobs use more compute resources, and more costly infrastructure
    (such as **Graphical Processing Units** (**GPUs**)), being able to effectively
    train a complex ML model on large data is important for a data scientist and an
    ML engineer. Being able to see and monitor how a training script interacts with
    data and compute instances is critical to optimizing the model training strategy
    in the training script so that it is time- and cost-effective. Speaking of cost
    when training at a large scale, did you know you can easily save more than 70%
    when training models in SageMaker? SageMaker Studio makes training ML models at
    scale easier and cost-effective.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的机器学习生命周期从原型设计开始，将过渡到生产规模，数据量增大，模型变得更加复杂，运行环境也更加复杂。完成训练作业需要正确的一套工具。使用多台计算机进行分布式训练以分担负载，可以解决涉及大量数据集和大型模型的情况。然而，随着复杂的机器学习训练作业使用更多的计算资源，以及更昂贵的硬件基础设施（如**图形处理单元**（**GPU**）），对于数据科学家和机器学习工程师来说，能够在大量数据上有效地训练复杂的机器学习模型非常重要。能够查看和监控训练脚本如何与数据和计算实例交互，对于优化训练脚本中的模型训练策略，使其既节省时间又节省成本至关重要。当在大规模训练时谈到成本，您知道您可以在SageMaker中轻松地节省超过70%的费用吗？SageMaker
    Studio使得大规模训练机器学习模型变得更加容易且成本效益更高。
- en: 'In this chapter, we will be learning about the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下内容：
- en: Performing distributed training in SageMaker Studio
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在SageMaker Studio中执行分布式训练
- en: Monitoring model training and compute resources with SageMaker Debugger
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SageMaker Debugger监控模型训练和计算资源
- en: Managing long-running jobs with check-pointing and spot training
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用检查点和spot训练管理长时间运行的作业
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, you need to access the code provided at [https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter09](https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter09).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您需要访问提供的代码，链接为[https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter09](https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter09)。
- en: Performing distributed training in SageMaker Studio
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在SageMaker Studio中执行分布式训练
- en: 'As the field of deep learning advances, ML models and training data are growing
    to a point that one single device is no longer sufficient for conducting effective
    model training. The neural networks are getting deeper and deeper, and gaining
    more and more parameters for training:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习领域的进步，机器学习模型和训练数据增长到一个点，单个设备已不再足以进行有效的模型训练。神经网络变得越来越深，训练参数也越来越多：
- en: '**LeNet-5**, one of the first **Convolutional Neural Network** (**CNN**) models
    proposed in 1989 that uses 2 convolutional layers and 3 dense layers, has around
    60,000 trainable parameters.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LeNet-5**，这是1989年提出的第一个**卷积神经网络**（**CNN**）模型之一，使用了2个卷积层和3个密集层，大约有6万个可训练参数。'
- en: '**AlexNet**, a deeper CNN architecture with 5 layers of convolutional layers
    and 3 dense layers proposed in 2012, has around 62 million trainable parameters.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AlexNet**，这是一种在2012年提出的具有5层卷积层和3个密集层的更深层CNN架构，大约有6200万个可训练参数。'
- en: '**Bidirectional Transformers for Language Understanding** (**BERT**), a language
    representation model using a transformer proposed in 2018, has 110 million and
    340 million trainable parameters in the base and large models respectively.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双向Transformer语言理解模型**（**BERT**），这是一种在2018年提出的基于transformer的语言表示模型，其基础模型和大模型分别有1.1亿和3.4亿可训练参数。'
- en: '**Generative Pre-trained Transformer 2** (**GPT-2**), a large transformer-based
    generative model proposed in 2019, has 1.5 billion trainable parameters.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成式预训练Transformer 2**（**GPT-2**），这是一种在2019年提出的基于大型transformer的生成模型，拥有15亿可训练参数。'
- en: '**GPT-3** is the next version, proposed in 2020, that reaches 175 billion trainable
    parameters.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT-3**是下一个版本，于2020年提出，其可训练参数达到1750亿。'
- en: Having more parameters to train means that there is a larger memory footprint
    during training. Additionally, the training data size needed to fit a complex
    model has also gone up significantly. For computer vision, one of the most commonly
    used training datasets, ImageNet, has 1.2 million images. For **Natural Language
    Processing** (**NLP**), GPT-3 is trained with 499 billion tokens, for example.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有更多参数需要训练意味着在训练期间有更大的内存占用。此外，适应复杂模型所需训练数据的大小也显著增加。对于计算机视觉，最常用的训练数据集之一ImageNet有120万张图片。例如，对于**自然语言处理**（**NLP**），GPT-3的训练使用了4990亿个标记。
- en: However, the latest and greatest GPU device would still struggle to hold up
    for such training requirements. The latest GPU device from NVIDIA, the A100 Tensor
    Core GPU available on AWS P4d.24xlarge instances, has 40 GB of GPU memory, but
    it would not be sufficient to hold the GPT-3 model, which has 175 billion parameters,
    as such a network would need *175 x 109 x 4 bytes = 700 GB* when using the *FP32*
    precision. Therefore, developers are going beyond single GPU device training and
    resorting to distributed training – that is, training using multiple GPU devices
    and multiple compute instances.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最新的最强大的GPU设备在满足这样的训练需求时仍会感到吃力。来自NVIDIA的最新GPU设备，AWS P4d.24xlarge实例上的A100 Tensor
    Core GPU，有40GB的GPU内存，但它不足以容纳拥有1750亿个参数的GPT-3模型，因为这样的网络在使用*FP32*精度时需要*175 x 10^9
    x 4 bytes = 700GB*。因此，开发者正在超越单GPU设备训练，并求助于分布式训练——即使用多个GPU设备和多个计算实例进行训练。
- en: Let's understand why and how distributed training helps.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解为什么以及如何分布式训练有助于。
- en: Understanding the concept of distributed training
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解分布式训练的概念
- en: In ML model training, the training data is fed into the loss optimization process
    in order to compute the gradients and weights for the next step. When data and
    parameters are much larger, as in the case of deep learning, having a full dataset
    that fits into the optimization becomes less feasible due to the GPU memory available
    on the device. It is common to use the **stochastic gradient descent optimization**
    approach, which estimates the gradients with a subset (**batch size**) of the
    full training dataset in each step, to overcome the GPU memory limitation. However,
    when a model or each data point is too large to have a meaningful batch size for
    the model training, we will not be able to converge to an optimal, accurate model
    in a reasonable timeframe.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型训练中，训练数据被输入到损失优化过程中，以计算下一步的梯度和权重。当数据和相关参数很大，如深度学习的情况时，由于设备上的GPU内存限制，拥有一个适合优化的完整数据集变得不太可行。通常使用**随机梯度下降优化**方法，该方法在每一步中用完整训练数据集的子集（**批大小**）估计梯度，以克服GPU内存限制。然而，当模型或每个数据点太大，以至于无法为模型训练提供一个有意义的批大小时，我们无法在合理的时间内收敛到一个最优、准确的模型。
- en: 'Distributed training is a practice to distribute parts of the computation to
    multiple GPU devices and multiple compute instances (also called nodes), and synchronize
    the computation from all devices before proceeding to the next iteration. There
    are two strategies in distributed training: **data parallelism** and **model parallelism**.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练是一种将计算的部分分布到多个GPU设备和多个计算实例（也称为节点）的实践，并在进行下一次迭代之前同步所有设备的计算。分布式训练中有两种策略：**数据并行**和**模型并行**。
- en: Data parallelism distributes the training dataset during epochs from disk to
    multiple devices and instances while each device contains a portion of data and
    a *complete replica* of the model. Each node performs a forward and backward propagation
    pass using different batches of data and shares trainable weight updates with
    other nodes for synchronization at the end of a pass. With data parallelism, you
    can increase the batch size by *n*-fold, where *n* is the number of GPU devices
    across nodes. An appropriately large batch size allows better generalization during
    the estimation of gradients and also reduces the number of steps needed to run
    through the entire pass (**an epoch**).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行在训练周期中将训练数据集从磁盘分布到多个设备和实例，每个设备包含数据的一部分和模型的*完整副本*。每个节点使用不同的数据批次执行正向和反向传播，并在传播结束时与其他节点共享可训练权重的更新以进行同步。使用数据并行，你可以将批大小增加*n*倍，其中*n*是节点间GPU设备的数量。一个适当大的批大小在估计梯度期间允许更好的泛化，并且也减少了运行整个传播（**一个周期**）所需的步骤数量。
- en: Note
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It has also been observed in practice that an overly large batch size will hurt
    the quality and generalization of a model. This is model- and dataset-dependent
    and requires experimentations and tuning to find out an appropriate batch size.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 实践中观察到，过大的批次大小会损害模型的质量和泛化能力。这取决于模型和数据集，需要实验和调整来找到合适的批次大小。
- en: 'Data parallelism is illustrated in *Figure 9.1*:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行在*图9.1*中展示：
- en: '![Figure 9.1 – The training data is distributed across GPU devices in data
    parallelism. A complete replica of the model is placed on each GPU device'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.1 – The training data is distributed across GPU devices in data
    parallelism. A complete replica of the model is placed on each GPU device](img/B17447_09_001.jpg)'
- en: '](img/B17447_09_001.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17447_09_001.jpg]'
- en: Figure 9.1 – The training data is distributed across GPU devices in data parallelism.
    A complete replica of the model is placed on each GPU device
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – 在数据并行中，训练数据被分布到GPU设备上。模型的完整副本放置在每个GPU设备上
- en: 'Alternatively, model parallelism distributes a large model across nodes. Partitioning
    of a model is performed at a layers and a weights level. Each node possesses a
    partition of the model. Forward and backward propagations take place as a pipeline,
    with the data batches going through the model partitions on all nodes before the
    weight updates. To be more specific, each data batch is split into micro-batches
    and feeds into each part of the model, located on devices for forward and backward
    passes. With model parallelism, you can more effectively train a large model that
    needs a higher GPU memory footprint than a single GPU device using memory collectively
    from multiple GPU devices. Model parallelism is illustrated in *Figure 9.2*:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，模型并行将大型模型分布到节点上。模型的分割在层和权重级别上进行。每个节点拥有模型的一部分。前向和反向传播作为一个流水线进行，数据批次在所有节点上的模型分割中通过，然后在权重更新之前。更具体地说，每个数据批次被分成微批次，并输入到每个模型部分，这些部分位于用于前向和反向传播的设备上。使用模型并行，你可以更有效地训练需要比单个GPU设备更高的GPU内存占用的大型模型，通过从多个GPU设备中集体使用内存。模型并行在*图9.2*中展示：
- en: '![Figure 9.2 – The model is partitioned across GPU devices in model parallelism.
    The training data is split into micro-batches and fed into the GPUs, each of which
    has a part of the model as a pipeline'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.2 – The model is partitioned across GPU devices in model parallelism.
    The training data is split into micro-batches and fed into the GPUs, each of which
    has a part of the model as a pipeline](img/B17447_09_002.jpg)'
- en: '](img/B17447_09_002.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17447_09_002.jpg]'
- en: Figure 9.2 – The model is partitioned across GPU devices in model parallelism.
    The training data is split into micro-batches and fed into the GPUs, each of which
    has a part of the model as a pipeline
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 – 在模型并行中，模型被分割到多个GPU设备上。训练数据被分成微批次并输入到GPU中，每个GPU拥有模型的一部分作为流水线
- en: '*When should we use data parallelism or model parallelism?* It depends on the
    data size, batch, and model sizes in training. Data parallelism is suitable for
    situations when a single data point is too large to have a desirable batch size
    during training. The immediate trade-off of having a small batch size is having
    a longer runtime to finish an epoch. You may want to increase the batch size so
    that you can complete an epoch under a reasonable timeframe. You can use data
    parallelism to distribute a larger batch size to multiple GPU devices. However,
    if your model is large and takes up most GPU memory in a single device, you will
    not enjoy the scale benefit of data parallelism much. This is because, in data
    parallelism, an ML model is fully replicated onto each of the GPU devices, leaving
    little space for any data. You should use model parallelism when you have a large
    model in relation to the GPU memory.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们应该在什么情况下使用数据并行或模型并行？* 这取决于训练中的数据大小、批次和模型大小。当单个数据点太大，在训练期间无法达到期望的批次大小时，数据并行是合适的。小批次大小的直接权衡是完成一个epoch需要更长的时间。你可能希望增加批次大小，以便在合理的时间内完成一个epoch。你可以使用数据并行将更大的批次大小分布到多个GPU设备上。然而，如果你的模型很大，并且在一个设备上占据了大部分GPU内存，你将不会从数据并行中获得太多的规模效益。这是因为，在数据并行中，ML模型被完全复制到每个GPU设备上，留给数据的空间很少。当你的模型相对于GPU内存很大时，你应该使用模型并行。'
- en: SageMaker makes running distributed training for large datasets and large models
    easy in the cloud. SageMaker's **distributed training libraries** support data
    parallelism and model parallelism for the two most popular deep learning frameworks,
    **TensorFlow** and **PyTorch**, when used in SageMaker. SageMaker's **distributed
    data parallel library** scales your model training with near-linear scaling efficiency,
    meaning that the reduction in training time in relation to the number of nodes
    is close to linear. SageMaker's **distributed model parallel library** automatically
    analyzes your neural network architecture and splits the model across GPU devices
    and orchestrates the pipeline execution efficiently.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 使得在云中运行大型数据集和大型模型的分布式训练变得简单。当在 SageMaker 中使用时，SageMaker 的 **分布式训练库**
    支持数据并行和模型并行，适用于最流行的两个深度学习框架，**TensorFlow** 和 **PyTorch**。SageMaker 的 **分布式数据并行库**
    以接近线性的扩展效率扩展您的模型训练，这意味着与节点数量相关的训练时间减少接近线性。SageMaker 的 **分布式模型并行库** 自动分析您的神经网络架构，并将模型分割到
    GPU 设备上，并高效地编排管道执行。
- en: In the following sections, we'll learn how we can implement data parallelism
    and model parallelism in SageMaker Studio for our training scripts written in
    TensorFlow and PyTorch.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将学习如何在 SageMaker Studio 中实现数据并行和模型并行，适用于我们用 TensorFlow 和 PyTorch 编写的训练脚本。
- en: Note
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Both TensorFlow and PyTorch are supported by the two distributed training libraries.
    The distributed training concepts remain the same between the two deep learning
    frameworks. We will focus on TensorFlow for the data parallel library and PyTorch
    for the model parallel library.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 和 PyTorch 都由两个分布式训练库支持。这两个深度学习框架之间的分布式训练概念是相同的。我们将重点关注 TensorFlow
    的数据并行库和 PyTorch 的模型并行库。
- en: The data parallel library with TensorFlow
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow 的数据并行库
- en: SageMaker's distributed data parallel library implements simple APIs that look
    similar to TensorFlow's way of performing model training in a distributed fashion
    but conduct distributed training that is optimized with AWS's compute infrastructure.
    This means that you can easily adopt SageMaker's API without making sophisticated
    changes to your existing distributed training code written in TensorFlow. If this
    is your first model training with distribution, we will demonstrate the modification
    needed to adapt SageMaker's distributed data parallel library to your existing
    model training script.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 的分布式数据并行库实现了类似于 TensorFlow 以分布式方式执行模型训练的简单 API，但进行了针对 AWS 计算基础设施优化的分布式训练。这意味着您可以在不修改现有用
    TensorFlow 编写的分布式训练代码的情况下轻松采用 SageMaker 的 API。如果您这是第一次进行分布式模型训练，我们将演示如何修改以适应 SageMaker
    的分布式数据并行库到现有的模型训练脚本。
- en: 'Let''s go to SageMaker Studio and start working with the `Getting-Started-with-Amazon-SageMaker-Studio/chapter09/01-smdp_tensorflow_sentiment_analysis.ipynb`
    notebook. This example is built on top of the training example we walked through
    in [*Chapter 5*](B17447_05_ePub_RK.xhtml#_idTextAnchor077), *Building and Training
    ML Models with the SageMaker Studio IDE* ([Getting-Started-with-Amazon-SageMaker-Studio/chapter05/02-tensorflow_sentiment_analysis.ipynb](http://Getting-Started-with-Amazon-SageMaker-Studio/chapter05/02-tensorflow_sentiment_analysis.ipynb)),
    where we trained a deep learning model using the TensorFlow Keras API on an IMDB
    review dataset. Back in [*Chapter 5*](B17447_05_ePub_RK.xhtml#_idTextAnchor077),
    *Building and Training ML Models with SageMaker Studio IDE*, we ran the training
    script on one `ml.p3.2xlarge` instance, which only has one NVIDIA Tesla V100 GPU.
    Now, in this chapter, we will use SageMaker''s distributed data parallel library
    to extend the code to work with multiple GPU devices, either from an instance
    or from multiple instances. And remember that we can always easily specify the
    number of instances and the type of the instances in the `sagemaker.tensorflow.TensorFlow`
    estimator. Let''s open the notebook and select the `%%writefile code/smdp_tensorflow_sentiment.py`
    is where modification to adopt the distributed training script will go. Follow
    the next steps to see the changes that need to be made to enable the distributed
    data parallel library:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们去 SageMaker Studio 并开始使用 `Getting-Started-with-Amazon-SageMaker-Studio/chapter09/01-smdp_tensorflow_sentiment_analysis.ipynb`
    笔记本。这个示例是基于我们在 [*第五章*](B17447_05_ePub_RK.xhtml#_idTextAnchor077)，*使用 SageMaker
    Studio IDE 构建和训练 ML 模型*中走过的训练示例构建的，在那里我们使用 TensorFlow Keras API 在 IMDB 评论数据集上训练了一个深度学习模型。回到
    [*第五章*](B17447_05_ePub_RK.xhtml#_idTextAnchor077)，*使用 SageMaker Studio IDE 构建和训练
    ML 模型*，我们在一个 `ml.p3.2xlarge` 实例上运行了训练脚本，该实例只有一个 NVIDIA Tesla V100 GPU。现在，在本章中，我们将使用
    SageMaker 的分布式数据并行库来扩展代码，使其能够与多个 GPU 设备一起工作，无论是来自单个实例还是多个实例。记住，我们总可以在 `sagemaker.tensorflow.TensorFlow`
    估算器中轻松指定实例数量和实例类型。让我们打开笔记本，选择 `%%writefile code/smdp_tensorflow_sentiment.py`，这是修改以采用分布式训练脚本的地方。按照以下步骤查看需要进行的更改，以启用分布式数据并行库：
- en: 'First, import the TensorFlow module of the data parallel library:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入数据并行库的 TensorFlow 模块：
- en: '[PRE0]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After the library is imported, we need to initialize the SageMaker distributed
    data parallel library in order to use it during runtime. We can implement it right
    after the `import` statements or in `main` (`if __name__ == "__main__"`):'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在导入库之后，我们需要初始化 SageMaker 分布式数据并行库以便在运行时使用。我们可以在 `import` 语句之后或 `main` 函数（`if
    __name__ == "__main__"`）中实现它：
- en: '[PRE1]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we discover all the GPU devices available in the compute instance fleet
    and configure the GPUs so that they are aware of the ranking within an instance.
    If an instance has eight GPU devices, each of them will get assigned a rank from
    zero to seven. The way to think about this is that each GPU device establishes
    a process to run the script and gets a unique ranking from `sdp.local_rank()`:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们发现了计算实例集群中所有可用的 GPU 设备，并配置了 GPU，使它们了解实例内的排名。如果一个实例有八个 GPU 设备，每个设备都将被分配一个从零到七的排名。可以这样理解：每个
    GPU 设备都会建立一个进程来运行脚本，并通过 `sdp.local_rank()` 获得一个唯一的排名：
- en: '[PRE2]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We also configure the GPUs to allow memory growth. This is specific to running
    TensorFlow with the SageMaker distributed data parallel library:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还配置了 GPU 以允许内存增长。这是在运行 TensorFlow 与 SageMaker 分布式数据并行库时特有的：
- en: '[PRE3]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The compute environment is now ready to perform distributed training.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 计算环境现在已准备好执行分布式训练。
- en: 'We scale the learning rate by the number of devices. Because of data parallelism,
    we will be able to fit in a larger batch size. With a larger batch size, it is
    recommended to scale the learning rate proportionally:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过设备数量来缩放学习率。由于数据并行，我们将能够适应更大的批量大小。随着批量大小的增加，建议按比例缩放学习率：
- en: '[PRE4]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Previously in [*Chapter 5*](B17447_05_ePub_RK.xhtml#_idTextAnchor077), *Building
    and Training ML Models with SageMaker Studio IDE*, we trained the model using
    Keras'' `model.fit()` API, but we have to make some changes to the model training.
    SageMaker''s distributed data parallel library does not yet support Keras'' `.fit()`
    API and only works with TensorFlow core modules. To use SageMaker''s distributed
    data parallel library, we can use the automatic differentiation (`tf.GradientTape`)
    and eager execution from TensorFlow 2.x. After defining the model using Keras
    layers in the `get_model()` function, instead of compiling it with an optimizer,
    we write the forward and backward pass explicitly with the `loss` function, the
    optimizer, and also the accuracy metrics defined explicitly:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在之前的[*第五章*](B17447_05_ePub_RK.xhtml#_idTextAnchor077)，“使用SageMaker Studio IDE构建和训练ML模型”中，我们使用Keras的`model.fit()`
    API训练了模型，但我们必须对模型训练做一些修改。SageMaker的分布式数据并行库尚不支持Keras的`.fit()` API，只与TensorFlow核心模块一起工作。要使用SageMaker的分布式数据并行库，我们可以使用TensorFlow
    2.x的自动微分（`tf.GradientTape`）和急切执行。在`get_model()`函数中使用Keras层定义模型后，我们不再用优化器编译它，而是用`loss`函数、优化器和显式定义的准确率度量来明确编写正向和反向传递：
- en: '[PRE5]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We then wrap `tf.GradientTape` with SMDataParallel''s `DistributedGradientTape`
    to optimize the `AllReduce` operation during the multi-GPU training. `AllReduce`
    is an operation that reduces the matrixes from all distributed processes:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将`tf.GradientTape`包装在SMDataParallel的`DistributedGradientTape`中，以优化多GPU训练期间的`AllReduce`操作。`AllReduce`是一个将所有分布式进程的矩阵进行归约的操作：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that the `sparse_as_dense` argument is set to `True` because we have an
    embedding layer in the model that will generate a spare matrix.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`sparse_as_dense`参数设置为`True`，因为我们模型中有一个嵌入层，它将生成稀疏矩阵。
- en: 'At the start of the training, broadcast the initial model variables from the
    head node (`rank 0`) to all other worker nodes (`rank 1` onward). We use a `first_batch`
    variable to denote the start of the training epochs:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练开始时，从主节点（`rank 0`）广播初始模型变量到所有其他工作节点（`rank 1`及以后）。我们使用`first_batch`变量表示训练epoch的开始：
- en: '[PRE7]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Average the loss and accuracy across devices; this process is called **all-reduce**:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在设备间平均损失和准确率；这个过程称为**all-reduce**：
- en: '[PRE8]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Put these steps in a `training_step()` function to perform a forward and backward
    pass, decorated with `@tf.function`. Run this training step in a nested `for`
    loop to go over epochs and batches of training data. We need to make sure that
    all GPU devices are getting an equal amount of data during a pass. We do this
    by taking data that is divisible by the total number of GPU devices in the inner
    `for` loop:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些步骤放入`training_step()`函数中，以执行正向和反向传递，并用`@tf.function`装饰。在嵌套的`for`循环中运行此训练步骤，遍历训练数据的epoch和批次。我们需要确保在传递过程中所有GPU设备都获得相等的数据量。我们通过在内层`for`循环中取可被GPU设备总数整除的数据来实现这一点：
- en: '[PRE9]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After the training `epoch` loop, we save the model, only using the leader device:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练`epoch`循环之后，我们只使用主设备保存模型：
- en: '[PRE10]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Last but not least in the training script, we convert the training data into
    a `tf.data.Dataset` object and set up the batching in the `get_train_data()` function
    so that it will work with our eager execution implementation. Note that we need
    `drop_remainder` to prevent the dataset from being of an equal batch_size across
    devices:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练脚本中，最后但同样重要的是，我们将训练数据转换为`tf.data.Dataset`对象，并在`get_train_data()`函数中设置批处理，以便与我们的急切执行实现一起工作。请注意，我们需要`drop_remainder`来防止数据集在设备间具有相等的批处理大小：
- en: '[PRE11]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We then move on to SageMaker''s TensorFlow estimator construct. To enable the
    SageMaker distributed data parallel library in a training job, we need to provide
    a dictionary:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们转向SageMaker的TensorFlow估算器构建。为了在训练作业中启用SageMaker分布式数据并行库，我们需要提供一个字典：
- en: '[PRE12]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This is given to the estimator, as follows.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这被分配给估算器，如下所示。
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Also, we need to choose a SageMaker instance from the following instance types
    that supports SageMaker''s distributed data parallel library: **ml.p4d.24xlarge**,
    **ml.p3dn.24xlarge**, and **ml.p3.16xlarge**:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要从以下实例类型中选择一个支持SageMaker分布式数据并行库的SageMaker实例：**ml.p4d.24xlarge**、**ml.p3dn.24xlarge**和**ml.p3.16xlarge**：
- en: The `ml.p4d.24xlarge` instance equips with 8 NVIDIA A100 Tensor Core GPUs, each
    with 40 GB of GPU memory.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ml.p4d.24xlarge`实例配备了8个NVIDIA A100 Tensor Core GPU，每个GPU拥有40 GB的GPU内存。'
- en: The `ml.p3dn.24xlarge` instance comes with 8 NVIDIA Tesla V100 GPUs, each with
    32 GB of GPU memory.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ml.p3dn.24xlarge`实例配备了8个NVIDIA Tesla V100 GPU，每个GPU拥有32 GB的GPU内存。'
- en: The `ml.p3.16xlarge` instance also comes with 8 NVIDIA Tesla V100 GPUs, each
    with 16 GB of GPU memory.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ml.p3.16xlarge`实例还配备了8个NVIDIA Tesla V100 GPU，每个GPU具有16 GB的GPU内存。'
- en: For demonstration purposes, we will choose ml.p3.16xlarge, which is the least
    expensive one among the three options. One single ml.p3.16xlarge is sufficient
    to run distributed data parallel training in SageMaker, as there will be 8 GPU
    devices to perform the training.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，我们将选择ml.p3.16xlarge，这是三种选项中最便宜的一个。一个单独的ml.p3.16xlarge就足以在SageMaker中运行分布式数据并行训练，因为将有8个GPU设备来执行训练。
- en: As there are more GPU devices and GPU memory to carry out the batching in an
    epoch, we can now increase `batch_size`. We scale `batch_size` 8 times from what
    we used in [*Chapter 5*](B17447_05_ePub_RK.xhtml#_idTextAnchor077), *Building
    and Training ML Models with SageMaker Studio IDE* – that is, *64 x 8 = 512*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有更多的GPU设备和GPU内存可以在一个epoch中进行批处理，我们现在可以增加`batch_size`。我们将`batch_size`从我们在[*第5章*](B17447_05_ePub_RK.xhtml#_idTextAnchor077)中使用的值扩大了8倍，即*64
    x 8 = 512*。
- en: With the estimator, we can proceed to call `estimator.fit()` to start the training.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用估计器，我们可以继续调用`estimator.fit()`来开始训练。
- en: 'To verify that the training is run with multiple GPU devices, the simplest
    way to tell is from the standard output. You can see a prefix of `[x, y]<stdout>:
    message` being added to indicate the process ranking from which the message is
    produced, as shown in *Figure 9.3*. We will learn more about this topic in the
    *Monitoring model training and compute resource with SageMaker Debugger* section:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '要验证训练是否在多个GPU设备上运行，最简单的方法是查看标准输出。您可以看到添加了前缀`[x, y]<stdout>: message`，以指示产生消息的进程等级，如图*9.3*所示。我们将在*使用SageMaker
    Debugger监控模型训练和计算资源*部分中了解更多关于这个主题的内容：'
- en: '![Figure 9.3 – The standard output from the cell, showing messages printed
    from process ranks – [1,0] to [1,7]. In our example, we use one ml.p3.16xlarge
    instance that has eight GPU devices'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.3 – 单元的标准输出，显示从进程等级[1,0]到[1,7]打印的消息。在我们的示例中，我们使用了一个具有八个GPU设备的ml.p3.16xlarge实例'
- en: '](img/B17447_09_003.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17447_09_003.jpg)'
- en: Figure 9.3 – The standard output from the cell, showing messages printed from
    process ranks – [1,0] to [1,7]. In our example, we use one ml.p3.16xlarge instance
    that has eight GPU devices
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 单元的标准输出，显示从进程等级[1,0]到[1,7]打印的消息。在我们的示例中，我们使用了一个具有八个GPU设备的ml.p3.16xlarge实例
- en: Even though here I am not using PyTorch to demonstrate SageMaker's distributed
    data parallel library, PyTorch is indeed supported by the library under the `smdistributed.dataparallel.torch`
    module. This module has a set of APIs that are similar to PyTorch's native distributed
    data parallel library. This means that you do not require many coding changes
    to adopt SageMaker's distributed data parallel library for PyTorch, which is optimized
    for training using SageMaker's infrastructure. You can find more details on how
    to adopt it in your PyTorch scripts at [https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-modify-sdp-pt.html](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-modify-sdp-pt.html).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在这里我没有使用PyTorch来演示SageMaker的分布式数据并行库，但PyTorch确实在`smdistributed.dataparallel.torch`模块下得到了库的支持。此模块提供了一套与PyTorch原生分布式数据并行库相似的API。这意味着您不需要进行很多编码更改，就可以将SageMaker的分布式数据并行库用于PyTorch，该库针对使用SageMaker基础设施的训练进行了优化。您可以在[https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-modify-sdp-pt.html](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-modify-sdp-pt.html)上找到有关如何在PyTorch脚本中采用它的更多详细信息。
- en: In the next section, we will run a PyTorch example and adopt model parallelism.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将运行一个PyTorch示例并采用模型并行性。
- en: Model parallelism with PyTorch
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch的模型并行性
- en: 'Model parallelism is particularly useful when you have a large network model
    that does not fit into the memory of a single GPU device. SageMaker''s distributed
    model parallel library implements two features that enable efficient training
    for large models so that you can easily adapt the library to your existing training
    scripts:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行性在您有一个大型网络模型且无法适应单个GPU设备内存时特别有用。SageMaker的分布式模型并行库实现了两个功能，这些功能使得对大型模型的训练变得高效，因此您可以轻松地将库适配到现有的训练脚本中：
- en: '**Automated model partitioning**, which maximizes GPU utilization, balances
    the memory footprint, and minimizes communication among GPU devices. In contrast,
    you can also manually partition the model using the library.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动模型分区**，这最大化了GPU利用率，平衡了内存占用，并最小化了GPU设备之间的通信。相比之下，您也可以使用库手动分区模型。'
- en: '**Pipeline execution**, which determines the order of computation and data
    movement across parts of the model that are on different GPU devices. There are
    two pipeline implementations: **interleaved** and **simple**. An interleaved pipeline
    prioritizes the backward passes whenever possible. It uses GPU memory more efficiently
    and minimizes the idle time of any GPU device in the fleet without waiting for
    the forward pass to complete to start the backward pass, as shown in *Figure 9.4*:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流水线执行**，它决定了不同GPU设备上的模型不同部分之间的计算和数据移动的顺序。有两种流水线实现：**交错**和**简单**。交错流水线尽可能优先处理反向传递。它更有效地使用GPU内存，并最小化整个GPU设备队列中任何GPU设备的空闲时间，无需等待前向传递完成就开始反向传递，如图9.4所示：'
- en: '![Figure 9.4 – An interleaved pipeline over two GPUs (GPU0 and GPU1). F0 represents
    a forward pass for the first micro-batch and B1 represents a backward pass for
    the second micro-batch. Backward passes are prioritized whenever possible'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.4 – 在两个GPU（GPU0和GPU1）上的交错流水线。F0代表第一个微批次的正向传递，B1代表第二个微批次的反向传递。尽可能优先处理反向传递'
- en: '](img/B17447_09_004.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17447_09_004.jpg)'
- en: Figure 9.4 – An interleaved pipeline over two GPUs (GPU0 and GPU1). F0 represents
    a forward pass for the first micro-batch and B1 represents a backward pass for
    the second micro-batch. Backward passes are prioritized whenever possible
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 在两个GPU（GPU0和GPU1）上的交错流水线。F0代表第一个微批次的正向传递，B1代表第二个微批次的反向传递。尽可能优先处理反向传递
- en: 'A simple pipeline, on the other hand, waits for the forward pass to complete
    before starting the backward pass, resulting in a simpler execution schedule,
    as shown in *Figure 9.5*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，简单的流水线会在开始反向传递之前等待前向传递完成，从而产生一个更简单的执行计划，如图9.5所示：
- en: '![Figure 9.5 – A simple pipeline over two GPUs. Backward passes are run only
    after'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.5 – 在两个GPU上的简单流水线。反向传递仅在完成前向传递后运行'
- en: the forward passes finish
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传递完成后
- en: '](img/B17447_09_005.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17447_09_005.jpg)'
- en: Figure 9.5 – A simple pipeline over two GPUs. Backward passes are run only after
    the forward passes finish
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – 在两个GPU上的简单流水线。只有在前向传递完成后才会运行反向传递
- en: Note
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Images in *Figure 9.4* and *9.5* are from: [https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.4*和*9.5*中的图像来自：[https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html)'
- en: 'Let''s start an example with the notebook in `chapter09/02-smmp-pytorch_mnist.ipynb`,
    where we are going to apply SageMaker''s distributed model parallel library to
    train a PyTorch model to classify digit handwriting using the famous MNIST digit
    dataset. Open the notebook in SageMaker Studio and use the `ml.t3.medium` instance:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`chapter09/02-smmp-pytorch_mnist.ipynb`中的笔记本开始一个示例，我们将应用SageMaker的分布式模型并行库来训练一个PyTorch模型，使用著名的MNIST数字数据集对数字手写体进行分类。在SageMaker
    Studio中打开笔记本并使用`ml.t3.medium`实例：
- en: As usual, set up the SageMaker session and import the dependencies in the first
    cell.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如同往常，在第一个单元中设置SageMaker会话并导入依赖项。
- en: Then, create a model training script written in PyTorch. This is a new training
    script. Essentially, it is training a convolutional neural network model on the
    MNIST handwriting digit dataset from the `torchvision` library. The model is defined
    using the `torch.nn` module. The optimizer used is the AdamW optimization algorithm.
    We implement the training epochs and batching, as it allows us to have the most
    flexibility to adopt SageMaker's distributed model parallel library.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建一个用PyTorch编写的模型训练脚本。这是一个新的训练脚本。本质上，它是在`torchvision`库的MNIST手写数字数据集上训练卷积神经网络模型。模型使用`torch.nn`模块定义。使用的优化器是AdamW优化算法。我们实现了训练周期和批处理，因为它允许我们最大限度地灵活地采用SageMaker的分布式模型并行库。
- en: 'SageMaker''s distributed model parallel library for PyTorch can be imported
    from `smdistributed.modelparallel.torch`:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SageMaker的PyTorch分布式模型并行库可以从`smdistributed.modelparallel.torch`导入：
- en: '[PRE14]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After the library is imported, initialize the SageMaker distributed model parallel
    library in order to use it during runtime. We can implement it right after the
    import statements or in `main` (`if __name__ == "__main__"`):'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库后，初始化SageMaker分布式模型并行库以便在运行时使用。我们可以在导入语句之后或`main`（`if __name__ == "__main__"`）中实现它：
- en: '[PRE15]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We will then ping and set the GPU devices with their local ranks:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将ping并设置GPU设备及其本地排名：
- en: '[PRE16]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The data downloading process from `torchvision` should only take place in the
    leader node (`local_rank` = `0`), while all the other processes (on other GPUs)
    should wait until the leader node completes the download:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`torchvision`下载数据的过程应该只在主节点（`local_rank` = `0`）上进行，而所有其他进程（在其他GPU上）应该等待主节点完成下载：
- en: '[PRE17]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, wrap the model and the optimizer with SageMaker''s distributed model
    parallel library''s implementations:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用SageMaker的分布式模型并行库的实现来包装模型和优化器：
- en: '[PRE18]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Up to now, the implementation between SageMaker's distributed data parallel
    library and model parallel library has been quite similar. The following is where
    things get different for the model parallel library.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，SageMaker的分布式数据并行库和模型并行库之间的实现相当相似。以下是在模型并行库中有所不同之处。
- en: 'We create a `train_step()` for function forward and backward passes and decorate
    it with `@smp.step`:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为函数的前向和反向传递创建一个`train_step()`，并用`@smp.step`装饰它：
- en: '[PRE19]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Create another `train()` function to implement the batching within an epoch.
    This is where we call `train_step()` to perform the forward and backward passes
    for a batch of data. Importantly, the data-related `to.(device)` calls need to
    be placed before `train_step()` while the typical `model.to(device)` is not required.
    Placing the model to a device is done automatically by the library.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 创建另一个`train()`函数以在epoch内实现批处理。这是我们调用`train_step()`以对一批数据进行前向和反向传递的地方。重要的是，需要在`train_step()`之前放置与数据相关的`to.(device)`调用，而通常不需要`model.to(device)`。将模型放置到设备上是由库自动完成的。
- en: 'Before stepping to the next batch, we need to average the loss across micro-batches
    with `.reduce_mean()`. Also, note that `optimizer.step()` needs to take place
    outside of `train_step()`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一个批次之前，我们需要使用`.reduce_mean()`对微批次中的损失进行平均。此外，请注意`optimizer.step()`需要在`train_step()`外部执行：
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Implement `test_step()`, decorated with `@smp.step`, and `test()` similarly
    for model evaluation. This allows model parallelism in model evaluation too.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现`test_step()`，用`@smp.step`装饰，并类似地为模型评估实现`test()`。这允许在模型评估中也实现模型并行。
- en: 'After the epochs loop, save the model with `smp.dp_rank()==0` to avoid data
    racing and ensure the gathering happens properly. Note that we set `partial=True`
    if we want to be able to load the model later and further train it:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在epochs循环之后，使用`smp.dp_rank()==0`保存模型以避免数据竞争并确保正确收集。注意，如果我们想以后能够加载模型并进一步训练它，我们需要设置`partial=True`：
- en: '[PRE21]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We then move on to the SageMaker PyTorch estimator construct. To enable SageMaker''s
    distributed model parallel library in a training job, we need to provide a dictionary
    to configure SageMaker''s distributed model parallel library and the `''partitions'':
    2`, to optimize for speed when partitioning the model `''optimize'': ''speed''`,
    to use a micro-batch of four `''microbatches'': 4`, to employ an interleaved pipeline
    schedule (''pipeline'': ''interleaved''), and to disable distribute data parallel
    `''ddp'': False`. The MPI is enabled with four processes per host ''`mpi'':{''enabled'':
    True`, `''processes_per_host'': 2}}`, which should be smaller than or equal to
    the number of GPU devices:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '然后，我们继续到SageMaker PyTorch估计器构建。为了在训练作业中启用SageMaker的分布式模型并行库，我们需要提供一个配置SageMaker的分布式模型并行库的字典，以及`''partitions'':
    2`以优化模型分区速度，`''optimize'': ''speed''`，使用包含四个样本的微批处理`''microbatches'': 4`，采用交错流水线调度（`''pipeline'':
    ''interleaved''`），并禁用分布式数据并行`''ddp'': False`。MPI通过每个主机四个进程启用`''mpi'':{''enabled'':
    True, ''processes_per_host'': 2}}`，这应该小于或等于GPU设备数量：'
- en: '[PRE22]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You can find the full list of parameters for `distribution` at [https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html#smdistributed-parameters](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html#smdistributed-parameters).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html#smdistributed-parameters](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html#smdistributed-parameters)找到`distribution`的完整参数列表。
- en: We then apply the `distribution` dictionary to the PyTorch estimator and use
    one ml.p3.8xlarge instance, which has four NVIDIA Tesla V100 GPUs. Unlike SageMaker's
    distributed data parallel library, SageMaker's distributed model parallel library
    is supported by all instances with multiple GPU devices.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将`distribution`字典应用于PyTorch估计器，并使用一个ml.p3.8xlarge实例，该实例具有四个NVIDIA Tesla V100
    GPU。与SageMaker的分布式数据并行库不同，SageMaker的分布式模型并行库支持所有具有多个GPU设备的实例。
- en: We can then proceed to call `estimator.fit()` to start the training.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以继续调用`estimator.fit()`以开始训练。
- en: Adopting a TensorFlow training script with SageMaker's distributed model parallel
    library employs similar concepts that we can just walk through. You can find out
    more about how to use the `smdistributed.modelparallel.tensorflow` module at [https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 采用SageMaker的分布式模型并行库的TensorFlow训练脚本采用了类似的概念，我们可以轻松地了解这些概念。你可以在[https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23)了解更多关于如何使用`smdistributed.modelparallel.tensorflow`模块的信息。
- en: When training with multiple GPU devices, one of the main challenges is to understand
    how expensive GPU resources are utilized. In the next section, we will discuss
    SageMaker Debugger, a feature that helps us analyze the utilization of compute
    resources during a SageMaker training job.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用多个GPU设备进行训练时，主要挑战之一是理解GPU资源是如何被利用的。在下一节中，我们将讨论SageMaker Debugger，这是一个帮助我们分析SageMaker训练作业期间计算资源利用情况的特性。
- en: Monitoring model training and compute resources with SageMaker Debugger
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker Debugger监控模型训练和计算资源
- en: Training ML models using `sagemaker.estimator.Estimator` and related classes,
    such as `sagemaker.pytorch.estimator.PyTorch` and `sagemaker.tensorflow.estimator.TensorFlow`,
    gives us the flexibility and scalability we need when developing in SageMaker
    Studio. However, due to the use of remote compute resources, it is rather different
    debugging and monitoring training jobs on a local machine or a single EC2 machine
    to how you would on a SageMaker Studio notebook. Being an IDE for ML, SageMaker
    Studio provides a comprehensive view of the managed training jobs through **SageMaker
    Debugger**. SageMaker Debugger helps developers monitor the compute resource utilization,
    detect modeling-related issues, profile deep learning operations, and identify
    bottlenecks during the runtime of your training jobs.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sagemaker.estimator.Estimator`和相关类，如`sagemaker.pytorch.estimator.PyTorch`和`sagemaker.tensorflow.estimator.TensorFlow`进行ML模型训练，为我们提供了在SageMaker
    Studio中开发所需的灵活性和可扩展性。然而，由于使用了远程计算资源，与在本地机器或单个EC2机器上调试和监控训练作业相比，SageMaker Studio笔记本上的调试和监控训练作业有很大的不同。作为一个ML
    IDE，SageMaker Studio通过**SageMaker Debugger**提供了对托管训练作业的全面视图。SageMaker Debugger帮助开发者监控计算资源利用率，检测建模相关的问题，分析深度学习操作，并在训练作业运行时识别瓶颈。
- en: SageMaker Debugger supports TensorFlow, PyTorch, MXNet, and XGBoost. By default,
    SageMaker Debugger is enabled in every SageMaker estimator. It collects instance
    metrics such as GPU, CPU, and memory utilization every 500 milliseconds and basic
    tensor output such as loss and accuracy every 500 steps. The data is saved in
    your S3 bucket. You can inspect the monitoring results live or after the job finishes
    in the SageMaker Studio IDE. You can also retrieve the monitoring results from
    S3 into a notebook and run additional analyses and custom visualization. If the
    default setting is not sufficient, you can configure the SageMaker Debugger programmatically
    for your `Estimator` to get the level of information you need.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Debugger支持TensorFlow、PyTorch、MXNet和XGBoost。默认情况下，SageMaker Debugger在每一个SageMaker估计器中都是启用的。它每500毫秒收集实例指标，如GPU、CPU和内存利用率，每500步收集基本的张量输出，如损失和准确率。数据保存在你的S3桶中。你可以在SageMaker
    Studio IDE中实时检查监控结果或作业完成后进行检查。你还可以将监控结果从S3检索到笔记本中，并运行额外的分析和自定义可视化。如果默认设置不够用，你可以通过编程方式配置SageMaker
    Debugger，为你的`Estimator`获取所需的信息级别。
- en: 'To get started, we can first inspect the information from the default Debugger
    configuration for the job we ran in `Machine-Learning-Development-with-Amazon-SageMaker-Studio/chapter09/01-smdp_tensorflow_sentiment_analysis.ipynb`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，我们可以首先检查我们在`Machine-Learning-Development-with-Amazon-SageMaker-Studio/chapter09/01-smdp_tensorflow_sentiment_analysis.ipynb`中运行的作业的默认Debugger配置信息：
- en: Find the job name you have run. It is in the `jobname` variable, in the form
    of `imdb-smdp-tf-YYYY-mm-DD-HH-MM-SS`. You can also find it in the output of the
    last cell.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到你已运行的作业名称。它在`jobname`变量中，形式为`imdb-smdp-tf-YYYY-mm-DD-HH-MM-SS`。你也可以在最后一个单元的输出中找到它。
- en: 'Navigate to the `jobname`; double-click the entry. You will see a trial component
    named **Training**, as shown in *Figure 9.6*. Right-click on the entry and select
    **Open Debugger for insights**:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到`jobname`；双击条目。您将看到一个名为**训练**的试验组件，如图*图9.6*所示。右键单击条目并选择**打开调试器以获取洞察**：
- en: '![Figure 9.6 – Opening the SageMaker Debugger UI from Experiments and trials'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.6 – 从实验和试验中打开SageMaker调试器UI'
- en: '](img/B17447_09_006.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17447_09_006.jpg]'
- en: Figure 9.6 – Opening the SageMaker Debugger UI from Experiments and trials
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – 从实验和试验中打开SageMaker调试器UI
- en: 'A new window in the main working area will pop up. The window will become available
    in a couple of minutes, as SageMaker Studio is launching a dedicated instance
    to process and render the data in the UI. This is called the **SageMaker Debugger
    insights dashboard**. Once available, you can see the results in the **Overview**
    and **Nodes** tabs, as shown in *Figure 9.7*:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在主工作区域将弹出一个新窗口。该窗口将在几分钟内可用，因为SageMaker Studio正在启动一个专用实例来处理和渲染UI中的数据。这被称为**SageMaker调试器洞察仪表板**。一旦可用，您可以在**概览**和**节点**选项卡中查看结果，如图*图9.7*所示：
- en: '![Figure 9.7 – The SageMaker Debugger insights dashboard showing the CPU and
    network utilization over the course of the training'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.7 – 显示训练过程中CPU和网络利用率的SageMaker调试器洞察仪表板'
- en: '](img/B17447_09_007.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17447_09_007.jpg]'
- en: Figure 9.7 – The SageMaker Debugger insights dashboard showing the CPU and network
    utilization over the course of the training
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – 显示训练过程中CPU和网络利用率的SageMaker调试器洞察仪表板
- en: 'In the **Nodes** tab, the mean utilization of the CPU, the network, the GPU,
    and the GPU memory are shown in the charts. You can narrow down the chart to a
    specific CPU or GPU to see whether there is any uneven utilization over the devices,
    as shown in *Figure 9.8*. From these charts, we can tell the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在**节点**选项卡中，图表显示了CPU、网络、GPU和GPU内存的平均利用率。您可以将图表缩小到特定的CPU或GPU，以查看设备之间是否存在不均匀的利用率，如图*图9.8*所示。从这些图表中，我们可以得出以下结论：
- en: The average CPU utilization peaked at around 60%, 3 minutes after the start
    of the job. This indicates that the training was taking place, and there was much
    activity on the CPU side to read in the data batches and feed into the GPU devices.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均CPU利用率在作业开始后大约3分钟达到峰值，约为60%。这表明训练正在进行，CPU端有大量活动来读取数据批次并将其输入到GPU设备中。
- en: The average GPU utilization over eight devices peaked at around 25%, also at
    3 minutes after the start of the job. At the same time, there was around 5% of
    GPU memory used on average. This is considered low GPU utilization, potentially
    due to the small batch size compared to the now much larger compute capacity from
    an ml.p3.16xlarge instance.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在八台设备上的平均GPU利用率在作业开始后大约3分钟达到峰值，约为25%。同时，平均使用了大约5%的GPU内存。这被认为是低GPU利用率，可能是由于与现在更大的计算能力相比，批次大小较小。
- en: 'On the other hand, there was some network utilization in the first 3 minutes.
    This is the period when SageMaker''s fully managed training downloaded the training
    data from the S3 bucket:'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，在前3分钟内有一些网络利用率。这是SageMaker的完全管理训练从S3存储桶下载训练数据的时期：
- en: '![Figure 9.8 – The SageMaker Debugger insights dashboard showing the GPU utilization
    over the course of the training'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.8 – 显示训练过程中GPU利用率的SageMaker调试器洞察仪表板'
- en: '](img/B17447_09_008.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17447_09_008.jpg]'
- en: Figure 9.8 – The SageMaker Debugger insights dashboard showing the GPU utilization
    over the course of the training
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – 显示训练过程中GPU利用率的SageMaker调试器洞察仪表板
- en: At the bottom of the page, a heatmap of the CPU/GPU utilization in a holistic
    view is displayed. As an exercise, feel free to open the Debugger for the training
    job submitted at `Getting-Started-with-Amazon-SageMaker-Studio/chapter06/02-tensorflow_sentiment_analysis.ipynb`
    and compare the difference in the CPU/GPU utilization between single-device training
    and distributed training.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在页面底部，显示了一个CPU/GPU利用率的整体热图。作为练习，您可以随意打开在`Getting-Started-with-Amazon-SageMaker-Studio/chapter06/02-tensorflow_sentiment_analysis.ipynb`提交的训练作业的调试器，并比较单设备训练和分布式训练之间CPU/GPU利用率的差异。
- en: Next, we'll move on to learn how to lower the cost of training ML models in
    SageMaker Studio with fully managed spot training and how to create checkpointing
    for long-running jobs and spot jobs.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何在SageMaker Studio中使用完全管理的spot训练降低训练ML模型的成本，以及如何为长时间运行的工作和spot工作创建检查点。
- en: Managing long-running jobs with checkpointing and spot training
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用检查点和 spot 训练管理长时间运行的任务
- en: Training ML models at scale can be costly. Even with SageMaker's pay-as-you-go
    pricing model on the training instances, performing long-running deep learning
    training and using multiple expensive instances can add up quickly. SageMaker's
    fully managed spot training and checkpointing features allow us to manage and
    resume long-running jobs easily, helping us reduce costs up to 90% on training
    instances over on-demand instances.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模上训练机器学习模型可能会很昂贵。即使在 SageMaker 的按需付费定价模型下，进行长时间运行的深度学习训练和使用多个昂贵的实例也会迅速增加成本。SageMaker
    的完全托管式 spot 训练和检查点功能使我们能够轻松管理和恢复长时间运行的任务，帮助我们将训练实例的成本降低高达 90%，与按需实例相比。
- en: SageMaker-managed Spot training uses the concept of spot instances from Amazon
    EC2\. EC2 spot instances let you take advantage of any unused instance capacity
    in an AWS Region at a much lower cost compared to regular on-demand instances.
    The spot instances are cheaper but can be interrupted when there is a higher demand
    for instances from other users on AWS. SageMaker-managed spot training manages
    the use of spot instances, including safe interruption and timely resumption of
    your training when the spot instances are available again.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 管理的 Spot 训练使用了 Amazon EC2 的 spot 实例概念。EC2 的 spot 实例允许你以比常规按需实例低得多的成本利用
    AWS 区域中任何未使用的实例容量。spot 实例更便宜，但可以在 AWS 上其他用户对实例有更高需求时被中断。SageMaker 管理的 spot 训练管理
    spot 实例的使用，包括在 spot 实例再次可用时安全中断和及时恢复你的训练。
- en: Along with the spot training feature, managed checkpointing is a key to managing
    your long-running job. Checkpoints in ML refer to intermediate ML models saved
    during training. Data scientists regularly create checkpoints and keep track of
    the best accuracy during the epochs. They compare accuracy against the best one
    during progression and use the checkpoint model that has the highest accuracy,
    rather than the model from the last epoch.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 spot 训练功能外，托管式检查点也是管理长时间运行任务的关键。在机器学习中，检查点是指在训练过程中保存的中间机器学习模型。数据科学家通常会创建检查点并跟踪每个时期内的最佳准确率。他们在进展过程中将准确率与最佳值进行比较，并使用具有最高准确率的检查点模型，而不是最后一个时期的模型。
- en: Data scientists can also resume and continue the training from any particular
    checkpoint if they want to fine-tune a model. As SageMaker trains a model on remote
    compute instances using containers, the checkpoints are saved in a local directory
    in the container. SageMaker automatically uploads the checkpoints from the local
    bucket to your S3 bucket. You can reuse the checkpoints in another training job
    easily by specifying their location in S3\. In the context of SageMaker-managed
    spot training, you do not need to worry about uploading and downloading the checkpoint
    files in case there is any interruption and resumption of a training job. SageMaker
    handles it for us.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据科学家想要微调模型，他们也可以从任何特定的检查点恢复并继续训练。由于 SageMaker 使用容器在远程计算实例上训练模型，检查点被保存在容器中的本地目录中。SageMaker
    会自动将检查点从本地存储桶上传到你的 S3 存储桶。你可以通过指定它们在 S3 中的位置轻松地在另一个训练作业中重用这些检查点。在 SageMaker 管理的
    spot 训练的上下文中，你不需要担心在训练作业的中断和恢复时上传和下载检查点文件。SageMaker 会为我们处理这些。
- en: 'Let''s run an example to see how things work. Open `Getting-Started-with-Amazon-SageMaker-Studio/chapter09/03-spot_training_checkpointing.ipynb`
    using the **Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)** kernel and an
    **ml.t3.medium** instance. In this notebook, we will be reusing our TensorFlow
    model training for the IMDB review dataset from [*Chapter 5*](B17447_05_ePub_RK.xhtml#_idTextAnchor077),
    *Building and Training ML Models with SageMaker Studio IDE*, and make some changes
    to the code to demonstrate how you can enable the checkpointing and managed spot
    training using SageMaker:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一个示例来看看事情是如何工作的。使用 **Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized**)
    内核和 **ml.t3.medium** 实例打开 `Getting-Started-with-Amazon-SageMaker-Studio/chapter09/03-spot_training_checkpointing.ipynb`。在这个笔记本中，我们将重用我们在
    [*第 5 章*](B17447_05_ePub_RK.xhtml#_idTextAnchor077) 中为 IMDB 评论数据集进行的 TensorFlow
    模型训练，即 *使用 SageMaker Studio IDE 构建和训练机器学习模型*，并对代码进行一些修改以展示如何使用 SageMaker 启用检查点和托管式
    spot 训练：
- en: Run the first five cells to set up the SageMaker session, and prepare the dataset.
    If you ran the first `chapter09/01-smdp_tensorflow_sentiment_analysis.ipynb` notebook,
    the dataset should be available already.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行前五个单元格以设置 SageMaker 会话，并准备数据集。如果你已经运行了第一个 `chapter09/01-smdp_tensorflow_sentiment_analysis.ipynb`
    笔记本，数据集应该已经可用。
- en: The cell leading with `%%writefile code/tensorflow_sentiment_with_checkpoint.py`
    is where we will make changes to the TensorFlow/Keras code. First of all, we are
    adding a new `--checkpoint_dir` argument in the `parse_args()` function to assign
    a default `/opt/ml/checkpoints` location set by SageMaker.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `%%writefile code/tensorflow_sentiment_with_checkpoint.py` 这一行代码中，我们将对 TensorFlow/Keras
    代码进行修改。首先，我们在 `parse_args()` 函数中添加了一个新的 `--checkpoint_dir` 参数，用于指定由 SageMaker
    设置的默认 `/opt/ml/checkpoints` 位置。
- en: 'In `__name__ == ''__main__''`, we will add a check to see whether `checkpoint_dir`
    exists locally in the container or not. If it does, list the directory to see
    whether there are any existing checkpoint files:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `__name__ == '__main__'` 中，我们将添加一个检查，以查看 `checkpoint_dir` 是否在容器中本地存在。如果存在，列出目录以查看是否存在任何现有的检查点文件：
- en: '[PRE23]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If `checkpoint_dir` does not contain valid checkpoint files, it means that there
    is no prior training job and checkpoints attached to the container and that `checkpoint_dir`
    is newly created for brand-new model training. If it does contain files, it means
    that previous checkpoint files are plugged into this training job and should be
    used as a starting point of the training, implemented in the `load_model_from_checkpoints()`
    function.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `checkpoint_dir` 不包含有效的检查点文件，这意味着没有先前的训练作业和附加到容器中的检查点，并且 `checkpoint_dir`
    是为新模型训练而新创建的。如果它包含文件，这意味着之前的检查点文件被插入到这个训练作业中，并且应该作为 `load_model_from_checkpoints()`
    函数中训练的起点。
- en: Implement `load_model_from_checkpoints()` to list all the checkpoint files,
    ending with `.h5`, as this is how Keras saved the model, in a given directory
    and use `regex` from the `re` library to filter the epoch number in the filename.
    We can then identify the latest checkpoint to load and continue the training with
    such a model. We assume the epoch number ranges from `0` to `999` in the regular
    expression operation.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现 `load_model_from_checkpoints()` 函数以列出给定目录中所有以 `.h5` 结尾的检查点文件，因为这是 Keras 保存模型的方式。使用
    `re` 库中的 `regex` 来过滤文件名中的 epoch 数。然后我们可以识别出最新的检查点来加载，并使用这样的模型继续训练。我们假设在正则表达式操作中，epoch
    数的范围是 `0` 到 `999`。
- en: After the model is loaded, either a new one or from a checkpoint, implement
    a `tf.keras.callbacks.ModelCheckpoint` callback in Keras to save a model checkpoint
    to `args.checkpoint_dir` after every epoch.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型加载后，无论是新模型还是从检查点加载，在 Keras 中实现一个 `tf.keras.callbacks.ModelCheckpoint` 回调，以便在每个
    epoch 后将模型检查点保存到 `args.checkpoint_dir`。
- en: 'When setting up the `sagemaker.tensorflow.TensorFlow` estimator, provide the
    following additional arguments to the estimator:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在设置 `sagemaker.tensorflow.TensorFlow` 估算器时，向估算器提供以下附加参数：
- en: '`use_spot_instances`: A Boolean to elect to use SageMaker spot instances for
    training.'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`use_spot_instances`：一个布尔值，用于选择是否在训练中使用 SageMaker Spot 实例。'
- en: '`max_wait`: A required argument when `use_spot_instances` is `True`. This is
    a timeout in seconds waiting for the spot training job. After this timeout, the
    job will be stopped.'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`max_wait`：当 `use_spot_instances` 为 `True` 时是必需的参数。这是一个等待 Spot 训练作业的超时时间（以秒为单位）。在此超时后，作业将被停止。'
- en: '`checkpoint_s3_uri`: The S3 bucket location to save the checkpoint files persistently.
    If you pass an S3 bucket location that already has checkpoint models and pass
    a higher epoch number, the script will pick up the latest checkpoint and resume
    training. For example, by providing `checkpoint_s3_uri`, which has checkpoints
    from a previous 50-epoch run and an `epochs` hyperparameter of 60, our script
    will resume the training from the fiftieth checkpoint and continue for another
    10 epochs.'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`checkpoint_s3_uri`：用于持久保存检查点文件的 S3 存储桶位置。如果你传递一个已经包含检查点模型的 S3 存储桶位置，并传递一个更高的
    epoch 数，脚本将选择最新的检查点并继续训练。例如，通过提供 `checkpoint_s3_uri`，它包含来自之前 50 个 epoch 运行的检查点，以及一个
    `epochs` 超参数为 60，我们的脚本将从第五十个检查点继续训练，再进行另外 10 个 epoch。'
- en: '`max_run`: The maximum runtime in seconds allowed for training. After this
    timeout, the job will be stopped. This value needs to be smaller than or equal
    to `max_wait`.'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`max_run`：允许训练的最大运行时间（以秒为单位）。在此超时后，作业将被停止。此值需要小于或等于 `max_wait`。'
- en: 'The following code snippet will construct an estimator to train a model with
    managed spot instances and checkpointing:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段将构建一个估算器，用于使用托管 Spot 实例和检查点来训练模型：
- en: '[PRE24]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The rest of the steps remain the same. We specify the hyperparameters, data
    input, and experiment configuration before we invoke `.fit()` to start the training
    job.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其余步骤保持不变。我们在调用 `.fit()` 开始训练作业之前，指定超参数、数据输入和实验配置。
- en: 'Wonder how much we save by using spot instances? From **Experiments and trials**
    in the left sidebar, we can bring up the AWS settings details of the trial, as
    shown in *Figure 9.9*, and see a **70%** saving by simply using managed spot training
    instances:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 想知道使用即时实例能节省多少吗？从左侧边栏的**实验和试验**中，我们可以调出试验的AWS设置详情，如图*图9.9*所示，仅通过使用托管即时训练实例就能看到**70%**的节省：
- en: '![Figure 9.9 – A 70% saving using managed spot training, as seen in the trial
    details'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.9 – 使用托管即时训练节省70%，如试验详情所示'
- en: '](img/B17447_09_009.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17447_09_009.jpg)'
- en: Figure 9.9 – A 70% saving using managed spot training, as seen in the trial
    details
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 – 使用托管即时训练节省70%，如试验详情所示
- en: A 70% saving is quite significant. This is especially beneficial to large-scale
    model training use cases that need expensive compute instances and have a long
    training time. Just four additional arguments to the estimator and some changes
    in the training script earn us a 70% saving.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 节省70%是非常显著的。这对于需要昂贵的计算实例且具有长时间训练的大型模型训练用例特别有益。只需向估计器添加四个额外的参数以及在训练脚本中做一些更改，我们就能节省70%。
- en: Summary
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we walked through how to train deep learning models using
    SageMaker distributed training libraries: data parallel and model parallel. We
    ran a TensorFlow example to show how you can modify a script to use SageMaker''s
    distributed data parallel library with eight GPU devices, instead of one from
    what we learned previously. This enables us to increase the batch size and reduce
    the iterations needed to go over the entire dataset in an epoch, improving the
    model training runtime. We then showed how you can adapt SageMaker''s distributed
    model parallel library to model training written in PyTorch. This enables us to
    train a much larger neural network model by partitioning the large model to all
    GPU devices. We further showed you how you can easily monitor the compute resource
    utilization in a training job using SageMaker Debugger and visualize the metrics
    in the SageMaker Debugger insights dashboard. Lastly, we explained how to adapt
    your training script to use the fully managed spot training and checkpointing
    to save costs when training models in SageMaker.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了如何使用SageMaker分布式训练库来训练深度学习模型：数据并行和模型并行。我们运行了一个TensorFlow示例，展示了如何修改脚本以使用SageMaker的分布式数据并行库，使用八个GPU设备，而不是之前我们所学的一个。这使得我们能够增加批量大小并减少在整个数据集上迭代所需的迭代次数，从而提高模型训练的运行时间。然后我们展示了如何将SageMaker的分布式模型并行库适配到用PyTorch编写的模型训练中。这使得我们能够通过将大型模型分割到所有GPU设备上来训练一个更大的神经网络模型。我们还展示了如何使用SageMaker
    Debugger轻松监控训练作业中的计算资源利用率，并在SageMaker Debugger洞察仪表板中可视化指标。最后，我们解释了如何调整训练脚本以使用完全托管的即时训练和检查点来在SageMaker中训练模型时节省成本。
- en: In the next chapter, we will be switching gear to learn how to monitor ML models
    in production. ML models in production taking unseen inference data may or may
    not produce quality predictions as expected from evaluations conducted prior to
    deployment. It is crucial in an ML life cycle to set up a monitoring strategy
    to ensure that your models are operating at a satisfactory level. SageMaker Studio
    has the functionality needed to help you set up model monitoring easily to monitor
    ML models in production. We will learn how to configure SageMaker Model Monitor
    and how to use it as part of our ML life cycle.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将转换方向，学习如何监控生产中的ML模型。在生产中，ML模型对未见过的推理数据进行推理可能或可能不会产生预期的质量预测，这些预测是在部署之前进行的评估中得出的。在ML生命周期中设置监控策略以确保模型在令人满意的水平上运行是至关重要的。SageMaker
    Studio具有帮助您轻松设置模型监控功能，以监控生产中的ML模型。我们将学习如何配置SageMaker Model Monitor以及如何将其作为我们ML生命周期的一部分来使用。
