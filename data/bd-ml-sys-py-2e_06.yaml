- en: Chapter 6. Classification II – Sentiment Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 6 章：分类 II – 情感分析
- en: For companies, it is vital to closely monitor the public reception of key events,
    such as product launches or press releases. With its real-time access and easy
    accessibility of user-generated content on Twitter, it is now possible to do sentiment
    classification of tweets. Sometimes also called opinion mining, it is an active
    field of research, in which several companies are already selling such services.
    As this shows that there obviously exists a market, we have motivation to use
    our classification muscles built in the last chapter, to build our own home-grown
    sentiment classifier.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 对于公司而言，密切监控重大事件的公众反应至关重要，如产品发布或新闻稿。借助 Twitter 的实时访问和用户生成内容的易获取性，现在可以对推文进行情感分类。情感分析有时也称为意见挖掘，它是一个活跃的研究领域，许多公司已经在销售此类服务。由于这表明市场显然存在，我们有动力使用在上一章中构建的分类技术，来构建我们自己的情感分类器。
- en: Sketching our roadmap
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制我们的路线图
- en: Sentiment analysis of tweets is particularly hard, because of Twitter's size
    limitation of 140 characters. This leads to a special syntax, creative abbreviations,
    and seldom well-formed sentences. The typical approach of analyzing sentences,
    aggregating their sentiment information per paragraph, and then calculating the
    overall sentiment of a document does not work here.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 推文的情感分析特别困难，因为 Twitter 对字符数的限制为 140 个字符。这导致了特殊的语法、创造性的缩写，并且句子通常不完整。分析句子的典型方法是汇总段落中的情感信息，然后计算文档的整体情感，这种方法在这里行不通。
- en: 'Clearly, we will not try to build a state-of-the-art sentiment classifier.
    Instead, we want to:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 很显然，我们并不会尝试构建一个最先进的情感分类器。相反，我们的目标是：
- en: Use this scenario as a vehicle to introduce yet another classification algorithm,
    **Naïve Bayes**
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这个场景作为引入另一个分类算法 **朴素贝叶斯** 的载体
- en: Explain how **Part Of Speech** (**POS**) tagging works and how it can help us
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释 **词性标注** (**POS**) 如何工作以及它如何帮助我们
- en: Show some more tricks from the scikit-learn toolbox that come in handy from
    time to time
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示一些来自 scikit-learn 工具箱的其他小技巧，这些技巧不时会派上用场
- en: Fetching the Twitter data
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取 Twitter 数据
- en: Naturally, we need tweets and their corresponding labels that tell whether a
    tweet is containing a positive, negative, or neutral sentiment. In this chapter,
    we will use the corpus from Niek Sanders, who has done an awesome job of manually
    labeling more than 5,000 tweets and has granted us permission to use it in this
    chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，我们需要推文及其相应的标签，以判断一条推文是包含积极、消极还是中性情感。在本章中，我们将使用 Niek Sanders 提供的语料库，他手动标注了超过
    5,000 条推文，并已授权我们在本章中使用这些数据。
- en: To comply with Twitter's terms of services, we will not provide any data from
    Twitter nor show any real tweets in this chapter. Instead, we can use Sander's
    hand-labeled data, which contains the tweet IDs and their hand-labeled sentiment,
    and use his script, `install.py`, to fetch the corresponding Twitter data. As
    the script is playing nice with Twitter's servers, it will take quite some time
    to download all the data for more than 5,000 tweets. So it is a good idea to start
    it right away.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了遵守 Twitter 的服务条款，我们不会提供任何来自 Twitter 的数据，也不会展示任何真实的推文。相反，我们可以使用 Sanders 的手动标注数据，其中包含推文
    ID 及其手动标注的情感，并使用他的脚本 `install.py` 获取相应的 Twitter 数据。由于该脚本与 Twitter 服务器兼容，因此下载超过
    5,000 条推文的数据需要相当长的时间。所以，最好立刻开始运行它。
- en: 'The data comes with four sentiment labels:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包含四个情感标签：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Inside `load_sanders_data()`, we are treating irrelevant and neutral labels
    together as neutral and drop ping all non-English tweets, resulting in 3,362 tweets.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `load_sanders_data()` 函数中，我们将不相关和中性标签一起处理为中性，并删除所有非英语推文，最终得到 3,362 条推文。
- en: In case you get different counts here, it is because, in the meantime, tweets
    might have been deleted or set to be private. In that case, you might also get
    slightly different numbers and graphs than the ones shown in the upcoming sections.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这里获得不同的计数，那是因为在此期间，推文可能被删除或设置为私人状态。在这种情况下，你也可能会看到与接下来的章节所展示的数字和图表略有不同。
- en: Introducing the Naïve Bayes classifier
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍朴素贝叶斯分类器
- en: Naïve Bayes is probably one of the most elegant machine learning algorithms
    out there that is of practical use. And despite its name, it is not that naïve
    when you look at its classification performance. It proves to be quite robust
    to irrelevant features, which it kindly ignores. It learns fast and predicts equally
    so. It does not require lots of storage. So, why is it then called naïve?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯可能是最优雅的机器学习算法之一，并且具有实际应用价值。尽管它的名字里有“朴素”二字，但从它的分类表现来看，它并不那么“朴素”。它对无关特征表现出强大的鲁棒性，能够巧妙地忽略它们。它学习速度快，预测也同样迅速。它不需要大量存储。那么，为什么它被称为“朴素”呢？
- en: The *Naïve* was added to account for one assumption that is required for Naïve
    Bayes to work optimally. The assumption is that the features do not impact each
    other. This, however, is rarely the case for real-world applications. Nevertheless,
    it still returns very good accuracy in practice even when the independence assumption
    does not hold.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*朴素*一词是为了表示朴素贝叶斯所依赖的一个假设，这个假设是特征之间相互独立。实际上，特征之间很少完全独立，这也是现实世界应用中的常见问题。然而，即便假设不成立，朴素贝叶斯在实践中仍能提供很高的准确性。'
- en: Getting to know the Bayes' theorem
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解贝叶斯定理
- en: 'At its core, Naïve Bayes classification is nothing more than keeping track
    of which feature gives evidence to which class. The way the features are designed
    determines the model that is used to learn. The so-called Bernoulli model only
    cares about Boolean features: whether a word occurs only once or multiple times
    in a tweet does not matter. In contrast, the Multinomial model uses word counts
    as features. For the sake of simplicity, we will use the Bernoulli model to explain
    how to use Naïve Bayes for sentiment analysis. We will then use the Multinomial
    model later on to set up and tune our real-world classifiers.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类的核心不过是记录每个特征给出哪一类的证据。特征的设计决定了使用哪种模型进行学习。所谓的伯努利模型只关注布尔特征：一个单词是否在推文中出现一次或多次并不重要。相反，多项式模型使用单词计数作为特征。为了简化起见，我们将使用伯努利模型来解释如何使用朴素贝叶斯进行情感分析。接下来，我们将使用多项式模型来设置和调优我们的实际分类器。
- en: 'Let''s assume the following meanings for the variables that we will use to
    explain Naïve Bayes:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设以下变量含义，用以解释朴素贝叶斯：
- en: '| Variable | Meaning |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 变量 | 含义 |'
- en: '| --- | --- |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ![Getting to know the Bayes'' theorem](img/2772OS_06_08.jpg) | This is the
    class of a tweet (positive or negative) |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| ![了解贝叶斯定理](img/2772OS_06_08.jpg) | 这是推文的类别（正面或负面） |'
- en: '| ![Getting to know the Bayes'' theorem](img/2772OS_06_09.jpg) | The word "awesome"
    occurs at least once in the tweet |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| ![了解贝叶斯定理](img/2772OS_06_09.jpg) | 单词“awesome”至少出现在推文中一次 |'
- en: '| ![Getting to know the Bayes'' theorem](img/2772OS_06_10.jpg) | The word "crazy"
    occurs at least once in the tweet |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| ![了解贝叶斯定理](img/2772OS_06_10.jpg) | 单词“crazy”至少出现在推文中一次 |'
- en: During training, we learned the Naïve Bayes model, which is the probability
    for a class ![Getting to know the Bayes' theorem](img/2772OS_06_08.jpg) when we
    already know features ![Getting to know the Bayes' theorem](img/2772OS_06_09.jpg)
    and ![Getting to know the Bayes' theorem](img/2772OS_06_10.jpg). This probability
    is written as ![Getting to know the Bayes' theorem](img/2772OS_06_11.jpg).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们学习了朴素贝叶斯模型，这是在已知特征的情况下，某个类别的概率！[了解贝叶斯定理](img/2772OS_06_08.jpg)。这个概率可以写作！[了解贝叶斯定理](img/2772OS_06_11.jpg)。
- en: 'Since we cannot estimate ![Getting to know the Bayes'' theorem](img/2772OS_06_11.jpg)
    directly, we apply a trick, which was found out by Bayes:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们无法直接估计！[了解贝叶斯定理](img/2772OS_06_11.jpg)，我们采用了一个技巧，这是贝叶斯发现的：
- en: '![Getting to know the Bayes'' theorem](img/2772OS_06_12.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![了解贝叶斯定理](img/2772OS_06_12.jpg)'
- en: 'If we substitute ![Getting to know the Bayes'' theorem](img/2772OS_06_13.jpg)
    with the probability of both words "awesome" and "crazy", and think of ![Getting
    to know the Bayes'' theorem](img/2772OS_06_14.jpg) as being our class ![Getting
    to know the Bayes'' theorem](img/2772OS_06_08.jpg), we arrive at the relationship
    that helps us to later retrieve the probability for the data instance belonging
    to the specified class:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将！[了解贝叶斯定理](img/2772OS_06_13.jpg)替换为“awesome”和“crazy”两个单词的概率，并将！[了解贝叶斯定理](img/2772OS_06_14.jpg)视为我们的类别！[了解贝叶斯定理](img/2772OS_06_08.jpg)，我们可以得到这个关系，它帮助我们后来推导数据实例属于指定类别的概率：
- en: '![Getting to know the Bayes'' theorem](img/2772OS_06_17.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![了解贝叶斯定理](img/2772OS_06_17.jpg)'
- en: 'This allows us to express ![Getting to know the Bayes'' theorem](img/2772OS_06_11.jpg)
    by means of the other probabilities:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得我们可以通过其他概率来表达 ![了解贝叶斯定理](img/2772OS_06_11.jpg)：
- en: '![Getting to know the Bayes'' theorem](img/2772OS_06_18.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![了解贝叶斯定理](img/2772OS_06_18.jpg)'
- en: 'We could also describe this as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将其描述为：
- en: '![Getting to know the Bayes'' theorem](img/2772OS_06_19.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![了解贝叶斯定理](img/2772OS_06_19.jpg)'
- en: 'The *prior* and the *evidence* are easily determined:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*先验* 和 *证据* 容易确定：'
- en: '![Getting to know the Bayes'' theorem](img/2772OS_06_20.jpg) is the prior probability
    of class ![Getting to know the Bayes'' theorem](img/2772OS_06_08.jpg) without
    knowing about the data. We can estimate this quantity by simply calculating the
    fraction of all training data instances belonging to that particular class.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![了解贝叶斯定理](img/2772OS_06_20.jpg) 是类别 ![了解贝叶斯定理](img/2772OS_06_08.jpg) 的先验概率，在不知道数据的情况下。我们可以通过简单地计算所有训练数据实例中属于该类别的比例来估计这个量。'
- en: '![Getting to know the Bayes'' theorem](img/2772OS_06_21.jpg) is the evidence
    or the probability of features ![Getting to know the Bayes'' theorem](img/2772OS_06_09.jpg)
    and ![Getting to know the Bayes'' theorem](img/2772OS_06_10.jpg).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![了解贝叶斯定理](img/2772OS_06_21.jpg) 是特征的证据或概率 ![了解贝叶斯定理](img/2772OS_06_09.jpg)
    和 ![了解贝叶斯定理](img/2772OS_06_10.jpg)。'
- en: The tricky part is the calculation of the likelihood ![Getting to know the Bayes'
    theorem](img/2772OS_06_22.jpg). It is the value describing how likely it is to
    see feature values ![Getting to know the Bayes' theorem](img/2772OS_06_09.jpg)
    and ![Getting to know the Bayes' theorem](img/2772OS_06_10.jpg) if we know that
    the class of the data instance is ![Getting to know the Bayes' theorem](img/2772OS_06_08.jpg).
    To estimate this, we need to do some thinking.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 难点在于似然度的计算 ![了解贝叶斯定理](img/2772OS_06_22.jpg)。它是描述在已知数据实例的类别是 ![了解贝叶斯定理](img/2772OS_06_08.jpg)
    的情况下，看到特征值 ![了解贝叶斯定理](img/2772OS_06_09.jpg) 和 ![了解贝叶斯定理](img/2772OS_06_10.jpg)
    的可能性。要估计这个，我们需要做一些思考。
- en: Being naïve
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 天真假设
- en: 'From probability theory, we also know the following relationship:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从概率理论中，我们还知道以下关系：
- en: '![Being naïve](img/2772OS_06_23.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![天真假设](img/2772OS_06_23.jpg)'
- en: This alone, however, does not help much, since we treat one difficult problem
    (estimating ![Being naïve](img/2772OS_06_22.jpg)) with another one (estimating
    ![Being naïve](img/2772OS_06_24.jpg)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，单靠这一点并没有太大帮助，因为我们用另一个困难问题（估计 ![天真假设](img/2772OS_06_22.jpg)）来处理一个问题（估计 ![天真假设](img/2772OS_06_24.jpg)）。
- en: 'However, if we naïvely assume that ![Being naïve](img/2772OS_06_09.jpg) and
    ![Being naïve](img/2772OS_06_10.jpg) are independent from each other, ![Being
    naïve](img/2772OS_06_24.jpg) simplifies to ![Being naïve](img/2772OS_06_25.jpg)
    and we can write it as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们天真地假设 ![天真假设](img/2772OS_06_09.jpg) 和 ![天真假设](img/2772OS_06_10.jpg) 彼此独立，那么
    ![天真假设](img/2772OS_06_24.jpg) 简化为 ![天真假设](img/2772OS_06_25.jpg)，我们可以将其写为：
- en: '![Being naïve](img/2772OS_06_26.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![天真假设](img/2772OS_06_26.jpg)'
- en: 'Putting everything together, we get the quite manageable formula:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 将一切合并在一起，我们得到一个相当简洁的公式：
- en: '![Being naïve](img/2772OS_06_27.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![天真假设](img/2772OS_06_27.jpg)'
- en: The interesting thing is that although it is not theoretically correct to simply
    tweak our assumptions when we are in the mood to do so, in this case, it proves
    to work astonishingly well in real-world applications.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，尽管从理论上讲，在我们有心情时随意调整假设并不正确，但在这种情况下，实际应用中它的效果出奇地好。
- en: Using Naïve Bayes to classify
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯进行分类
- en: 'Given a new tweet, the only part left is to simply calculate the probabilities:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个新的推文，剩下的唯一任务就是简单地计算概率：
- en: '![Using Naïve Bayes to classify](img/2772OS_06_28.jpg)![Using Naïve Bayes to
    classify](img/2772OS_06_29.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯进行分类](img/2772OS_06_28.jpg)![使用朴素贝叶斯进行分类](img/2772OS_06_29.jpg)'
- en: Then choose the class ![Using Naïve Bayes to classify](img/2772OS_06_30.jpg)
    having higher probability.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后选择概率较高的类别 ![使用朴素贝叶斯进行分类](img/2772OS_06_30.jpg)。
- en: As for both classes the denominator, ![Using Naïve Bayes to classify](img/2772OS_06_21.jpg),
    is the same, we can simply ignore it without changing the winner class.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对于两个类别，分母 ![使用朴素贝叶斯进行分类](img/2772OS_06_21.jpg) 是相同的，我们可以忽略它而不改变最终类别。
- en: 'Note, however, that we don''t calculate any real probabilities any more. Instead,
    we are estimating which class is more likely, given the evidence. This is another
    reason why Naïve Bayes is so robust: It is not so much interested in the real
    probabilities, but only in the information, which class is more likely. In short,
    we can write:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，我们现在不再计算任何真实的概率。相反，我们在估算给定证据下，哪个类别更有可能。这也是朴素贝叶斯如此稳健的另一个原因：它更关心的是哪个类别更有可能，而不是实际的概率。简而言之，我们可以写：
- en: '![Using Naïve Bayes to classify](img/2772OS_06_31.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯分类](img/2772OS_06_31.jpg)'
- en: This is simply telling that we are calculating the part after *argmax* for all
    classes of ![Using Naïve Bayes to classify](img/2772OS_06_08.jpg) (*pos* and *neg*
    in our case) and returning the class that results in the highest value.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是说明我们正在计算*argmax*之后的部分，针对所有类别![使用朴素贝叶斯分类](img/2772OS_06_08.jpg)（在我们的例子中是*pos*和*neg*），并返回得到最高值的类别。
- en: 'But, for the following example, let''s stick to real probabilities and do some
    calculations to see how Naïve Bayes works. For the sake of simplicity, we will
    assume that Twitter allows only for the two aforementioned words, "awesome" and
    "crazy", and that we had already manually classified a handful of tweets:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，对于以下示例，我们将坚持使用真实的概率并进行一些计算，以便看看朴素贝叶斯是如何工作的。为了简化起见，我们将假设Twitter只允许使用之前提到的两个单词：“awesome”和“crazy”，并且我们已经手动分类了一些推文：
- en: '| Tweet | Class |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 推文 | 类别 |'
- en: '| --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| awesome | Positive tweet |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| awesome | 正面推文 |'
- en: '| awesome | Positive tweet |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| awesome | 正面推文 |'
- en: '| awesome crazy | Positive tweet |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| awesome crazy | 正面推文 |'
- en: '| crazy | Positive tweet |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| crazy | 正面推文 |'
- en: '| crazy | Negative tweet |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| crazy | 负面推文 |'
- en: '| crazy | Negative tweet |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| crazy | 负面推文 |'
- en: In this example, we have the tweet "crazy" both in a positive and negative tweet
    to emulate some ambiguities you will often find in the real world (for example,
    "being soccer crazy" versus "a crazy idiot").
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将“crazy”这条推文同时放在正面和负面推文中，以模拟现实世界中经常会遇到的一些模糊情况（例如，“热衷足球”与“疯狂的傻瓜”）。
- en: 'In this case, we have six total tweets, out of which four are positive and
    two negative, which results in the following priors:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们共有六条推文，其中四条是正面的，二条是负面的，得出的先验概率如下：
- en: '![Using Naïve Bayes to classify](img/2772OS_06_32.jpg)![Using Naïve Bayes to
    classify](img/2772OS_06_33.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯分类](img/2772OS_06_32.jpg)![使用朴素贝叶斯分类](img/2772OS_06_33.jpg)'
- en: This means, without knowing anything about the tweet itself, it would be wise
    to assume the tweet to be positive.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，在不了解推文本身的任何信息的情况下，假设这条推文是正面推文是明智的。
- en: A still missing piece is the calculation of ![Using Naïve Bayes to classify](img/2772OS_06_34.jpg)
    and ![Using Naïve Bayes to classify](img/2772OS_06_25.jpg), which are the probabilities
    for the two features ![Using Naïve Bayes to classify](img/2772OS_06_09.jpg) and
    ![Using Naïve Bayes to classify](img/2772OS_06_10.jpg) conditioned in class ![Using
    Naïve Bayes to classify](img/2772OS_06_08.jpg).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然缺少的部分是计算![使用朴素贝叶斯分类](img/2772OS_06_34.jpg)和![使用朴素贝叶斯分类](img/2772OS_06_25.jpg)，这些是条件概率，分别针对两个特征![使用朴素贝叶斯分类](img/2772OS_06_09.jpg)和![使用朴素贝叶斯分类](img/2772OS_06_10.jpg)，并且是基于类别![使用朴素贝叶斯分类](img/2772OS_06_08.jpg)计算的。
- en: 'This is calculated as the number of tweets, in which we have seen the concrete
    feature divided by the number of tweets that have been labeled with the class
    of ![Using Naïve Bayes to classify](img/2772OS_06_08.jpg). Let''s say we want
    to know the probability of seeing "awesome" occurring in a tweet, knowing that
    its class is positive, we will have:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过计算我们见过具体特征的推文数，再除以已被标记为类别![使用朴素贝叶斯分类](img/2772OS_06_08.jpg)的推文总数来得出的。假设我们想知道在已知推文类别为正面的情况下，出现“awesome”的概率，我们将得到：
- en: '![Using Naïve Bayes to classify](img/2772OS_06_35.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯分类](img/2772OS_06_35.jpg)'
- en: 'Because out of the four positive tweets three contained the word "awesome".
    Obviously, the probability for not having "awesome" in a positive tweet is its
    inverse:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因为四条正面推文中有三条包含“awesome”这个词。显然，正面推文中不包含“awesome”的概率就是它的逆：
- en: '![Using Naïve Bayes to classify](img/2772OS_06_36.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯分类](img/2772OS_06_36.jpg)'
- en: 'Similarly, for the rest (omitting the case that a word is not occurring in
    a tweet):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于其余情况（省略没有出现该词的推文）：
- en: '![Using Naïve Bayes to classify](img/2772OS_06_37.jpg)![Using Naïve Bayes to
    classify](img/2772OS_06_38.jpg)![Using Naïve Bayes to classify](img/2772OS_06_39.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯分类](img/2772OS_06_37.jpg)![使用朴素贝叶斯分类](img/2772OS_06_38.jpg)![使用朴素贝叶斯分类](img/2772OS_06_39.jpg)'
- en: 'For the sake of completeness, we will also compute the evidence so that we
    can see real probabilities in the following example tweets. For two concrete values
    of ![Using Naïve Bayes to classify](img/2772OS_06_09.jpg) and ![Using Naïve Bayes
    to classify](img/2772OS_06_10.jpg), we can calculate the evidence as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我们还将计算证据，以便在接下来的示例推文中看到实际概率。对于![使用朴素贝叶斯进行分类](img/2772OS_06_09.jpg)和![使用朴素贝叶斯进行分类](img/2772OS_06_10.jpg)这两个具体值，我们可以按如下方式计算证据：
- en: '![Using Naïve Bayes to classify](img/2772OS_06_40.jpg)![Using Naïve Bayes to
    classify](img/2772OS_06_41.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯进行分类](img/2772OS_06_40.jpg)![使用朴素贝叶斯进行分类](img/2772OS_06_41.jpg)'
- en: 'This leads to the following values:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下值：
- en: '![Using Naïve Bayes to classify](img/2772OS_06_42.jpg)![Using Naïve Bayes to
    classify](img/2772OS_06_43.jpg)![Using Naïve Bayes to classify](img/2772OS_06_44.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯进行分类](img/2772OS_06_42.jpg)![使用朴素贝叶斯进行分类](img/2772OS_06_43.jpg)![使用朴素贝叶斯进行分类](img/2772OS_06_44.jpg)'
- en: 'Now we have all the data to classify new tweets. The only work left is to parse
    the tweet and featurize it:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了所有的数据来分类新的推文。剩下的工作就是解析推文并提取特征：
- en: '| Tweet | ![Using Naïve Bayes to classify](img/2772OS_06_09.jpg) | ![Using
    Naïve Bayes to classify](img/2772OS_06_10.jpg) | Class probabilities | Classification
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 推文 | ![使用朴素贝叶斯进行分类](img/2772OS_06_09.jpg) | ![使用朴素贝叶斯进行分类](img/2772OS_06_10.jpg)
    | 类别概率 | 分类 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| "awesome" | 1 | 0 | ![Using Naïve Bayes to classify](img/2772OS_06_47.jpg)![Using
    Naïve Bayes to classify](img/2772OS_06_48.jpg) | Positive |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| "极棒" | 1 | 0 | ![使用朴素贝叶斯进行分类](img/2772OS_06_47.jpg)![使用朴素贝叶斯进行分类](img/2772OS_06_48.jpg)
    | 正面 |'
- en: '| "crazy" | 0 | 1 | ![Using Naïve Bayes to classify](img/2772OS_06_49.jpg)![Using
    Naïve Bayes to classify](img/2772OS_06_50.jpg) | Negative |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| "疯狂" | 0 | 1 | ![使用朴素贝叶斯进行分类](img/2772OS_06_49.jpg)![使用朴素贝叶斯进行分类](img/2772OS_06_50.jpg)
    | 负面 |'
- en: '| "awesome crazy" | 1 | 1 | ![Using Naïve Bayes to classify](img/2772OS_06_45.jpg)![Using
    Naïve Bayes to classify](img/2772OS_06_46.jpg) | Positive |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| "极棒 疯狂" | 1 | 1 | ![使用朴素贝叶斯进行分类](img/2772OS_06_45.jpg)![使用朴素贝叶斯进行分类](img/2772OS_06_46.jpg)
    | 正面 |'
- en: So far, so good. The classification of trivial tweets seems to assign correct
    labels to the tweets. The question remains, however, how we should treat words
    that did not occur in our training corpus. After all, with the preceding formula,
    new words will always be assigned a probability of zero.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。对于简单的推文分类，似乎能够给推文分配正确的标签。然而，问题仍然是，我们应该如何处理那些在训练语料库中没有出现过的词？毕竟，使用前面的公式，新词总是会被分配零的概率。
- en: Accounting for unseen words and other oddities
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 考虑到未见过的词和其他异常情况
- en: When we calculated the probabilities earlier, we actually cheated ourselves.
    We were not calculating the real probabilities, but only rough approximations
    by means of the fractions. We assumed that the training corpus will tell us the
    whole truth about the real probabilities. It did not. A corpus of only six tweets
    obviously cannot give us all the information about every tweet that has ever been
    written. For example, there certainly are tweets containing the word "text" in
    them. It is only that we have never seen them. Apparently, our approximation is
    very rough and we should account for that. This is often done in practice with
    the so-called **add-one smoothing**.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们之前计算概率时，实际上我们是在自欺欺人。我们并没有计算真正的概率，而只是通过分数来进行粗略的估算。我们假设训练语料库会告诉我们关于真实概率的全部真相，但事实并非如此。仅仅六条推文的语料库显然不能告诉我们所有关于曾经写过的推文的信息。例如，肯定有包含“文本”一词的推文，只是我们从未见过它们。显然，我们的估算非常粗略，我们应该对此加以考虑。在实践中，这通常通过所谓的**加一平滑**来实现。
- en: Tip
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Add-one smoothing is sometimes also referred to as **additive smoothing** or
    **Laplace smoothing**. Note that Laplace smoothing has nothing to do with Laplacian
    smoothing, which is related to the smoothing of polygon meshes. If we do not smooth
    by `1` but by an adjustable parameter `alpha<0`, it is called Lidstone smoothing.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 加一平滑有时也被称为**加性平滑**或**拉普拉斯平滑**。注意，拉普拉斯平滑与拉普拉斯算子平滑无关，后者是与多边形网格的平滑相关的。如果我们不是通过`1`来平滑，而是通过可调参数`alpha<0`来平滑，那就叫做Lidstone平滑。
- en: It is a very simple technique that adds one to all feature occurrences. It has
    the underlying assumption that even if we have not seen a given word in the whole
    corpus, there is still a chance that it is just that our sample of tweets happened
    to not include that word. So, with add-one smoothing we pretend that we have seen
    every occurrence once more than we actually did. That means that instead of calculating
    ![Accounting for unseen words and other oddities](img/2772OS_06_54.jpg), we now
    do ![Accounting for unseen words and other oddities](img/2772OS_06_55.jpg).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种非常简单的技术，它为所有特征出现次数加 1。其基本假设是，即使我们在整个语料库中没有见过某个单词，也有可能是我们的推文样本刚好没有包含这个单词。所以，通过加一平滑，我们假装每个出现的单词比实际出现的次数多见了一次。这意味着，我们现在计算的不是![计算未见过的单词和其他异常情况](img/2772OS_06_54.jpg)，而是![计算未见过的单词和其他异常情况](img/2772OS_06_55.jpg)。
- en: 'Why do we add 2 in the denominator? Because we have two features: the occurrence
    of "awesome" and "crazy". Since we add 1 for each feature, we have to make sure
    that the end result is again a probability. And indeed, we get 1 as the total
    probability:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们在分母中加 2？因为我们有两个特征：“awesome”和“crazy”的出现。由于每个特征加 1，我们必须确保最终结果仍然是一个概率。事实上，我们得到的总概率为
    1：
- en: '![Accounting for unseen words and other oddities](img/2772OS_06_56.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![计算未见过的单词和其他异常情况](img/2772OS_06_56.jpg)'
- en: Accounting for arithmetic underflows
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算算术下溢
- en: 'There is yet another road block. In reality, we work with probabilities much
    smaller than the ones we have dealt with in the toy example. Typically, we also
    have many more than only two features, which we multiply with each other. This
    will quickly lead to the point where the accuracy provided by NumPy does not suffice
    any more:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个障碍。实际上，我们处理的概率要比在玩具示例中遇到的要小得多。通常，我们也有比仅仅两个特征更多的特征，而这些特征需要相互相乘。这将很快导致 NumPy
    提供的精度不再足够：
- en: '[PRE1]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'So, how probable is it that we will ever hit a number like `2.47E-324`? To
    answer this, we just need to imagine a likelihood for the conditional probabilities
    of 0.0001, and then multiply 65 of them together (meaning that we have 65 low
    probable feature values) and you''ve been hit by the arithmetic underflow:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，`2.47E-324` 这样的数字到底有多可能出现呢？为了回答这个问题，我们只需要想象一个条件概率为 0.0001，然后将其中 65 个概率相乘（意味着我们有
    65 个低概率特征值），你就会遇到算术下溢：
- en: '[PRE2]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'A float in Python is typically implemented using double in C. To find out whether
    this is the case for your platform you can check it as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Python 中的浮点数通常使用 C 中的 double 类型实现。要检查你的平台是否是这样，你可以通过以下方式进行确认：
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To mitigate this, one could switch to math libraries such as `mpmath` ([http://code.google.com/p/mpmath/](http://code.google.com/p/mpmath/))
    that allow for arbitrary accuracy. However, they are not fast enough to work as
    a NumPy replacement.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这个问题，可以切换到诸如`mpmath`（[http://code.google.com/p/mpmath/](http://code.google.com/p/mpmath/)）这样的数学库，它们支持任意精度。然而，它们的速度不够快，无法替代
    NumPy。
- en: 'Fortunately, there is a better way to take care of this, and it has to do with
    a nice relationship that we might still remember from school:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一个更好的方法可以解决这个问题，这与我们可能还记得的一个学校里的好关系有关：
- en: '![Accounting for arithmetic underflows](img/2772OS_06_58.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![计算算术下溢](img/2772OS_06_58.jpg)'
- en: 'If we apply it to our case, we get the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将其应用到我们的案例中，我们得到如下结果：
- en: '![Accounting for arithmetic underflows](img/2772OS_06_59.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![计算算术下溢](img/2772OS_06_59.jpg)'
- en: As the probabilities are in the interval between 0 and 1, the log of the probabilities
    lies in the interval -∞ and 0\. Don't be bothered with that. Higher numbers are
    still a stronger indicator for the correct class—it is only that they are negative
    now.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于概率位于 0 和 1 之间，概率的对数则位于-∞和 0 之间。别为这个困扰。较大的数值仍然是正确类别的更强指示——只是现在它们变成了负数。
- en: '![Accounting for arithmetic underflows](img/2772OS_06_01.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![计算算术下溢](img/2772OS_06_01.jpg)'
- en: 'There is one caveat though: we actually don''t have the log in the formula''s
    nominator (the part above the fraction). We only have the product of the probabilities.
    In our case, luckily, we are not interested in the actual value of the probabilities.
    We simply want to know which class has the highest posterior probability. We are
    lucky, because if we find that ![Accounting for arithmetic underflows](img/2772OS_06_60.jpg),
    then we will always also have ![Accounting for arithmetic underflows](img/2772OS_06_61.jpg).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 但有一个注意事项：公式的分子部分（即分数上方的部分）实际上并没有对数。我们只有概率的乘积。幸运的是，在我们的案例中，我们并不关心概率的实际值。我们只是想知道哪个类别具有最高的后验概率。幸运的是，如果我们发现![处理算术下溢](img/2772OS_06_60.jpg)，那么我们也总是能得到![处理算术下溢](img/2772OS_06_61.jpg)。
- en: 'A quick look at the preceding graph shows that the curve is monotonically increasing,
    that is, it never goes down, when we go from left to right. So let''s stick this
    into the aforementioned formula:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 快速查看前面的图表可以发现曲线是单调递增的，即从左到右时，曲线永远不会下降。所以让我们把这个带入前面提到的公式：
- en: '![Accounting for arithmetic underflows](img/2772OS_06_61a.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![处理算术下溢](img/2772OS_06_61a.jpg)'
- en: 'This will finally retrieve the formula for two features that will give us the
    best class also for the real-world data that we will see in practice:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这将最终得到适用于两个特征的公式，它将为我们提供最佳类别，适用于我们在实践中看到的实际数据：
- en: '![Accounting for arithmetic underflows](img/2772OS_06_62.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![处理算术下溢](img/2772OS_06_62.jpg)'
- en: 'Of course, we will not be very successful with only two features, so, let''s
    rewrite it to allow for an arbitrary number of features:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，只有两个特征的话，我们不会非常成功，所以，让我们重写代码以允许任意数量的特征：
- en: '![Accounting for arithmetic underflows](img/2772OS_06_63.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![处理算术下溢](img/2772OS_06_63.jpg)'
- en: There we are, ready to use our first classifier from the scikit-learn toolkit.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们准备好使用来自 scikit-learn 工具包的第一个分类器。
- en: As mentioned earlier, we just learned the Bernoulli model of Naïve Bayes. Instead
    of having Boolean features, we can also use the number of word occurrences, also
    known as the Multinomial model. As this provides more information, and often also
    results in better performance, we will use this for our real-world data. Note,
    however, that the underlying formulas change a bit. However, no worries, as the
    general idea how Naïve Bayes works, is still the same.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们刚刚学习了 Naïve Bayes 的伯努利模型。与布尔特征不同，我们也可以使用单词出现次数，即多项式模型。由于这个方法提供了更多的信息，通常也能带来更好的性能，因此我们会在实际数据中使用这个模型。不过，注意的是，底层的公式会有一些变化。不过，不用担心，Naïve
    Bayes 的基本原理依然不变。
- en: Creating our first classifier and tuning it
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建我们的第一个分类器并进行调优
- en: 'The Naïve Bayes classifiers resides in the `sklearn.naive_bayes` package. There
    are different kinds of Naïve Bayes classifiers:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Naïve Bayes 分类器位于 `sklearn.naive_bayes` 包中。这里有不同种类的 Naïve Bayes 分类器：
- en: '`GaussianNB`: This classifier assumes the features to be normally distributed
    (Gaussian). One use case for it could be the classification of sex given the height
    and width of a person. In our case, we are given tweet texts from which we extract
    word counts. These are clearly not Gaussian distributed.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GaussianNB`：这个分类器假设特征呈正态分布（高斯分布）。它的一个应用场景可能是根据身高和体宽来判断性别。在我们的例子中，我们有推文文本，从中提取词频。这些显然不是高斯分布的。'
- en: '`MultinomialNB`: This classifier assumes the features to be occurrence counts,
    which is our case going forward, since we will be using word counts in the tweets
    as features. In practice, this classifier also works well with TF-IDF vectors.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MultinomialNB`：这个分类器假设特征是出现次数，这正是我们接下来使用的情况，因为我们将在推文中使用词频作为特征。实际上，这个分类器也能很好地处理
    TF-IDF 向量。'
- en: '`BernoulliNB`: This classifier is similar to `MultinomialNB`, but more suited
    when using binary word occurrences and not word counts.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BernoulliNB`：这个分类器与 `MultinomialNB` 类似，但更适合处理二进制单词出现与否，而非词频。'
- en: As we will mainly look at the word occurrences, for our purpose the `MultinomialNB`
    classifier is best suited.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将主要关注词频，因此对于我们的目的来说，`MultinomialNB` 分类器最为合适。
- en: Solving an easy problem first
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先解决一个简单的问题
- en: 'As we have seen, when we looked at our tweet data, the tweets are not only
    positive or negative. The majority of tweets actually do not contain any sentiment,
    but are neutral or irrelevant, containing, for instance, raw information (for
    example, "New book: Building Machine Learning … http://link"). This leads to four
    classes. To not complicate the task too much, let''s only focus on the positive
    and negative tweets for now.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在查看推文数据时所看到的，推文不仅仅是正面或负面的。实际上，大多数推文不包含任何情感，而是中立或无关的，包含例如原始信息（例如，“新书：构建机器学习……http://link”）。这导致了四个类别。为了避免任务过于复杂，现在我们只关注正面和负面的推文。
- en: '[PRE4]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, we have in `X` the raw tweet texts and in `Y` the binary classification,
    `0` for negative and `1` for positive tweets.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在`X`中有原始推文文本，在`Y`中有二元分类，`0`表示负面推文，`1`表示正面推文。
- en: 'We just said that we will use word occurrence counts as features. We will not
    use them in their raw form, though. Instead, we will use our power horse `TfidfVectorizer`
    to convert the raw tweet text into TF-IDF feature values, which we then use together
    with the labels to train our first classifier. For convenience, we will use the
    `Pipeline` class, which allows us to hook the vectorizer and the classifier together
    and provides the same interface:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚提到过，我们将使用单词出现次数作为特征。但我们不会直接使用它们的原始形式，而是使用我们的强力工具`TfidfVectorizer`，将原始推文文本转换为TF-IDF特征值，然后将其与标签一起用于训练我们的第一个分类器。为了方便，我们将使用`Pipeline`类，它允许我们将向量化器和分类器连接在一起，并提供相同的接口：
- en: '[PRE5]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `Pipeline` instance returned by `create_ngram_model()` can now be used to
    fit and predict as if we had a normal classifier.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_ngram_model()`返回的`Pipeline`实例现在可以像普通分类器一样用于拟合和预测。'
- en: Since we do not have that much data, we should do cross-validation. This time,
    however, we will not use `KFold`, which partitions the data in consecutive folds,
    but instead, we use `ShuffleSplit`. It shuffles the data for us, but does not
    prevent the same data instance to be in multiple folds. For each fold, then, we
    keep track of the area under the Precision-Recall curve and for accuracy.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有那么多数据，应该进行交叉验证。然而，这一次，我们不会使用`KFold`（它将数据划分为连续的折叠），而是使用`ShuffleSplit`。它会将数据打乱，但不会阻止相同的数据实例出现在多个折叠中。对于每个折叠，我们会跟踪精准率-召回率曲线下的面积和准确度。
- en: To keep our experimentation agile, let's wrap everything together in a `train_model()`function,
    which takes a function as a parameter that creates the classifier.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持实验的灵活性，我们将一切封装在一个`train_model()`函数中，该函数接受一个创建分类器的函数作为参数。
- en: '[PRE6]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Putting everything together, we can train our first model:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容组合起来，我们可以训练我们的第一个模型：
- en: '[PRE7]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With our first try using Naïve Bayes on vectorized TF-IDF trigram features,
    we get an accuracy of 78.8 percent and an average P/R AUC of 88.2 percent. Looking
    at the P/R chart of the median (the train/test split that is performing most similar
    to the average), it shows a much more encouraging behavior than the plots we have
    seen in the previous chapter.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯和向量化的TF-IDF三元组特征，我们的第一次尝试得到了78.8%的准确率和88.2%的平均P/R AUC。当我们查看中位数的P/R图表（即表现最接近平均水平的训练/测试拆分）时，它表现出比我们在上一章看到的图表更加令人鼓舞的行为。
- en: '![Solving an easy problem first](img/2772OS_06_02.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![首先解决简单问题](img/2772OS_06_02.jpg)'
- en: For a start, the results are quite encouraging. They get even more impressive
    when we realize that 100 percent accuracy is probably never achievable in a sentiment
    classification task. For some tweets, even humans often do not really agree on
    the same classification label.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，结果相当令人鼓舞。当我们意识到在情感分类任务中，100%的准确率可能永远无法实现时，结果就变得更加令人印象深刻。对于某些推文，甚至人类在分类标签上也往往达不到一致。
- en: Using all classes
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用所有类别
- en: Once again, we simplified our task a bit, since we used only positive or negative
    tweets. That means, we assumed a perfect classifier that upfront classified whether
    the tweet contains a sentiment and forwarded that to our Naïve Bayes classifier.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次简化了任务，因为我们只使用了正面或负面的推文。这意味着，我们假设有一个完美的分类器，首先对推文是否包含情感进行分类，然后将结果传递给我们的朴素贝叶斯分类器。
- en: 'So, how well do we perform if we also classify whether a tweet contains any
    sentiment at all? To find that out, let''s first write a convenience function
    that returns a modified class array providing a list of sentiments that we would
    like to interpret as positive:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果我们还对推文是否包含任何情感进行分类，我们的表现如何呢？为了找出答案，我们首先编写一个便捷函数，返回一个修改后的类别数组，提供我们希望解释为正面的情感列表：
- en: '[PRE8]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Note that we are talking about two different positives now. The sentiment of
    a tweet can be positive, which is to be distinguished from the class of the training
    data. If, for example, we want to find out how good we can separate tweets having
    sentiment from neutral ones, we could do:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们现在谈论的是两个不同的正面情感。推文的情感可以是正面的，这与训练数据的类别不同。例如，如果我们想要了解我们如何区分具有情感的推文和中立推文，我们可以这样做：
- en: '[PRE9]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In `Y` we have now `1` (positive class) for all tweets that are either positive
    or negative and `0` (negative class) for neutral and irrelevant ones.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Y`中，对于所有正面或负面的推文，我们现在有`1`（正类），对于中立和无关的推文，我们有`0`（负类）。
- en: '[PRE10]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Have a look at the following plot:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下的图表：
- en: '![Using all classes](img/2772OS_06_03.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![使用所有类别](img/2772OS_06_03.jpg)'
- en: As expected, P/R AUC drops considerably, being only 66 percent now. The accuracy
    is still high, but that is only due to the fact that we have a highly imbalanced
    dataset. Out of 3,362 total tweets, only 920 are either positive or negative,
    which is about 27 percent. This means, if we create a classifier that always classifies
    a tweet as not containing any sentiment, we will already have an accuracy of 73
    percent. This is another case to always look at precision and recall if the training
    and test data is unbalanced.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，P/R AUC显著下降，目前只有66%。准确率仍然很高，但这仅仅是因为我们有一个高度不平衡的数据集。在3,362条推文中，只有920条是正面或负面的，占约27%。这意味着，如果我们创建一个总是将推文分类为不包含任何情感的分类器，我们就已经有73%的准确率了。这是另一个在训练和测试数据不平衡时，总是查看精度和召回率的案例。
- en: 'So, how will the Naïve Bayes classifier perform on classifying positive tweets
    versus the rest and negative tweets versus the rest? One word: bad.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，朴素贝叶斯分类器在将正面推文与其他推文以及负面推文与其他推文进行分类时表现如何呢？一句话：差。
- en: '[PRE11]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Pretty unusable if you ask me. Looking at the P/R curves in the following plots,
    we will also find no usable precision/recall trade-off, as we were able to do
    in the last chapter:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你问我，这几乎是不可用的。看下面的P/R曲线，我们也会发现没有可用的精度/召回率权衡，就像我们在上一章做的那样：
- en: '![Using all classes](img/2772OS_06_04a.jpg)![Using all classes](img/2772OS_06_04b.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![使用所有类别](img/2772OS_06_04a.jpg)![使用所有类别](img/2772OS_06_04b.jpg)'
- en: Tuning the classifier's parameters
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整分类器的参数
- en: 'Certainly, we have not explored the current setup enough and should investigate
    more. There are roughly two areas, where we can play with the knobs: `TfidfVectorizer`
    and `MultinomialNB`. As we have no real intuition in which area we should explore,
    let''s try to distribute the parameters'' values.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们还没有充分探索当前的设置，应该进行更多的调查。大致有两个领域，我们可以调整参数：`TfidfVectorizer`和`MultinomialNB`。由于我们对哪个领域的探索没有直觉，让我们尝试分配参数值。
- en: 'We will see the `TfidfVectorizer` parameter first:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先查看`TfidfVectorizer`参数：
- en: 'Using different settings for NGrams:'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的NGram设置：
- en: unigrams (1,1)
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单字（1,1）
- en: unigrams and bigrams (1,2)
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单字和双字（1,2）
- en: unigrams, bigrams, and trigrams (1,3)
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单字、双字和三字（1,3）
- en: 'Playing with `min_df`: 1 or 2'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整`min_df`：1或2
- en: 'Exploring the impact of IDF within TF-IDF using `use_idf` and `smooth_idf`:
    `False` or `True`'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`use_idf`和`smooth_idf`探索IDF在TF-IDF中的影响：`False`或`True`
- en: Whether to remove stop words or not, by setting `stop_words` to `english` or
    `None`
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否移除停用词，通过将`stop_words`设置为`english`或`None`
- en: Whether to use the logarithm of the word counts (`sublinear_tf`)
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否使用词频的对数值（`sublinear_tf`）
- en: Whether to track word counts or simply track whether words occur or not, by
    setting `binary` to `True` or `False`
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否跟踪词频，还是仅跟踪词是否出现，通过将`binary`设置为`True`或`False`
- en: 'Now we will see the `MultinomialNB` classifier:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将查看`MultinomialNB`分类器：
- en: 'Which smoothing method to use by setting `alpha`:'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过设置`alpha`来选择平滑方法：
- en: 'Add-one or Laplace smoothing: 1'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加一平滑或拉普拉斯平滑：1
- en: 'Lidstone smoothing: 0.01, 0.05, 0.1, or 0.5'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lidstone平滑：0.01，0.05，0.1，或0.5
- en: 'No smoothing: 0'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无平滑：0
- en: A simple approach could be to train a classifier for all those reasonable exploration
    values, while keeping the other parameters constant and check the classifier's
    results. As we do not know whether those parameters affect each other, doing it
    right will require that we train a classifier for every possible combination of
    all parameter values. Obviously, this is too tedious for us.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是为所有合理的探索值训练一个分类器，同时保持其他参数不变并检查分类器的结果。由于我们不知道这些参数是否相互影响，正确的方法是为所有可能的参数值组合训练一个分类器。显然，这对我们来说太繁琐了。
- en: Because this kind of parameter exploration occurs frequently in machine learning
    tasks, scikit-learn has a dedicated class for it, called `GridSearchCV`. It takes
    an estimator (instance with a classifier-like interface), which will be the `Pipeline`
    instance in our case, and a dictionary of parameters with their potential values.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这种参数探索在机器学习任务中经常发生，scikit-learn为此提供了一个专门的类，叫做`GridSearchCV`。它接受一个估算器（具有分类器类似接口的实例），在我们这个案例中将是`Pipeline`实例，并且接受一个包含参数及其潜在值的字典。
- en: '`GridSearchCV` expects the dictionary''s keys to obey a certain format so that
    it is able to set the parameters of the correct estimator. The format is as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV`期望字典的键遵循特定的格式，以便能够为正确的估算器设置参数。格式如下：'
- en: '[PRE12]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For example, if we want to specify the desired values to explore for the `min_df`
    parameter of `TfidfVectorizer` (named `vect` in the `Pipeline` description), we
    would have to say:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想指定要探索的`TfidfVectorizer`（在`Pipeline`描述中称为`vect`）的`min_df`参数的期望值，我们需要这样说：
- en: '[PRE13]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This will tell `GridSearchCV` to try out unigrams to trigrams as parameter values
    for the `ngram_range` parameter of `TfidfVectorizer`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这将告诉`GridSearchCV`尝试将一元组到三元组作为`TfidfVectorizer`的`ngram_range`参数的候选值。
- en: Then, it trains the estimator with all possible parameter value combinations.
    Here, we make sure that it trains on random samples of the training data using
    `ShuffleSplit`, which generates an iterator of random train/test splits. Finally,
    it provides the best estimator in the form of the member variable, `best_estimator_`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它使用所有可能的参数值组合训练估算器。在这里，我们确保它在训练数据的随机样本上进行训练，使用`ShuffleSplit`，它生成随机的训练/测试分割迭代器。最后，它提供最佳估算器，作为成员变量`best_estimator_`。
- en: As we want to compare the returned best classifier with our current best one,
    we need to evaluate it in the same way. Therefore, we can pass the `ShuffleSplit`
    instance using the `cv` parameter (therefore, `CV` in `GridSearchCV`).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望将返回的最佳分类器与当前的最佳分类器进行比较，我们需要以相同的方式评估它。因此，我们可以通过`cv`参数传递`ShuffleSplit`实例（因此，`GridSearchCV`中的`CV`）。
- en: 'The last missing piece is to define how `GridSearchCV` should determine the
    best estimator. This can be done by providing the desired score function to (surprise!)
    the `score_func` parameter. We can either write one ourselves, or pick one from
    the `sklearn.metrics` package. We should certainly not take `metric.accuracy`
    because of our class imbalance (we have a lot less tweets containing sentiment
    than neutral ones). Instead, we want to have good precision and recall on both
    classes, tweets with sentiment and tweets without positive or negative opinions.
    One metric that combines both precision and recall is the so-called **F-measure**,
    which is implemented as `metrics.f1_score`:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一块缺失的部分是定义`GridSearchCV`应如何确定最佳估算器。这可以通过提供期望的评分函数来完成（惊讶！）传递给`score_func`参数。我们可以自己编写一个，或者从`sklearn.metrics`包中选择一个。我们肯定不应该使用`metric.accuracy`，因为我们的数据存在类别不平衡（包含情感的推文远少于中立推文）。相反，我们希望对两个类别都有良好的精度和召回率，即具有情感的推文和没有正面或负面观点的推文。一种结合精度和召回率的评价指标是所谓的**F值**，它在`metrics.f1_score`中实现：
- en: '![Tuning the classifier''s parameters](img/2772OS_06_64.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![Tuning the classifier''s parameters](img/2772OS_06_64.jpg)'
- en: 'After putting everything together, we get the following code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容整合后，我们得到以下代码：
- en: '[PRE14]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We have to be patient while executing this:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此操作时我们需要耐心：
- en: '[PRE15]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Since we have just requested a parameter, sweep over ![Tuning the classifier''s
    parameters](img/2772OS_06_65.jpg) parameter combinations, each being trained on
    10 folds:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们刚刚请求了一个参数，覆盖![[调整分类器的参数](img/2772OS_06_65.jpg)]的参数组合，每种组合都将在10折交叉验证中进行训练：
- en: '[PRE16]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The best estimator indeed improves the P/R AUC by nearly 3.3 percent to now
    70.2, with the settings shown in the previous code.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳估算器确实将P/R AUC提高了近3.3个百分点，现在为70.2，设置如前面代码所示。
- en: 'Also, the devastating results for positive tweets against the rest and negative
    tweets against the rest improve if we configure the vectorizer and classifier
    with those parameters we have just found out:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，针对正面推文与其他推文的比较，负面推文与其他推文的比较，若我们配置向量化器和分类器使用刚刚发现的参数，结果会得到显著改善：
- en: '[PRE17]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Have a look at the following plots:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下图表：
- en: '![Tuning the classifier''s parameters](img/2772OS_06_06a.jpg)![Tuning the classifier''s
    parameters](img/2772OS_06_06b.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![Tuning the classifier''s parameters](img/2772OS_06_06a.jpg)![Tuning the classifier''s
    parameters](img/2772OS_06_06b.jpg)'
- en: Indeed, the P/R curves look much better (note that the plots are from the medium
    of the fold classifiers, thus, slightly diverging AUC values). Nevertheless, we
    probably still wouldn't use those classifiers. Time for something completely different…
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，P/R曲线看起来好多了（注意，这些图是来自于不同折分类器的中位数，因此AUC值略有差异）。不过，我们可能仍然不会使用这些分类器。是时候换点新的东西了…
- en: Cleaning tweets
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清洗推文
- en: New constraints lead to new forms. Twitter is no exception in this regard. Because
    the text has to fit into 140 characters, people naturally develop new language
    shortcuts to say the same in less characters. So far, we have ignored all the
    diverse emoticons and abbreviations. Let's see how much we can improve by taking
    that into account. For this endeavor, we will have to provide our own `preprocessor()`
    to `TfidfVectorizer`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 新的约束导致了新的形式。Twitter在这方面也不例外。因为文本必须适应140个字符，人们自然会发展出新的语言简写，以用更少的字符表达相同的意思。到目前为止，我们忽略了各种各样的表情符号和缩写。让我们看看在考虑到这些因素后，我们能做出多大的改进。为了这个目标，我们将需要提供我们自己的`preprocessor()`来处理`TfidfVectorizer`。
- en: 'First, we define a range of frequent emoticons and their replacements in a
    dictionary. Although we can find more distinct replacements, we go with obvious
    positive or negative words to help the classifier:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在字典中定义一系列常见的表情符号及其替换词。虽然我们可以找到更多不同的替换词，但我们选择明显的正面或负面词汇来帮助分类器：
- en: '[PRE18]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we define abbreviations as regular expressions together with their expansions
    (`\b` marks the word boundary):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将缩写定义为正则表达式，并给出它们的扩展（`\b`表示单词边界）：
- en: '[PRE19]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Certainly, there are many more abbreviations that can be used here. But already
    with this limited set, we get an improvement for sentiment versus not sentiment
    of half a point, being now 70.7 percent:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这里可以使用更多的缩写。但仅凭这一有限的集合，我们已经能在情感与非情感的分类上提高了半个点，目前准确率为70.7%：
- en: '[PRE20]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Taking the word types into account
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 考虑单词类型
- en: So far, our hope was that simply using the words independent of each other with
    the bag-of-words approach would suffice. Just from our intuition, however, neutral
    tweets probably contain a higher fraction of nouns, while positive or negative
    tweets are more colorful, requiring more adjectives and verbs. What if we use
    this linguistic information of the tweets as well? If we could find out how many
    words in a tweet were nouns, verbs, adjectives, and so on, the classifier could
    probably take that into account as well.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的期望是仅仅使用单词本身（采用袋装词汇方法）就足够了。然而，仅凭我们的直觉，可能中立的推文包含更多名词，而积极或消极的推文则更加多彩，包含更多形容词和动词。那么，如果我们也利用推文中的语言学信息呢？如果我们能找出推文中有多少单词是名词、动词、形容词等，分类器也许能够将这一信息纳入考虑。
- en: Determining the word types
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确定单词类型
- en: This is what part-of-speech tagging, or POS tagging, is all about. A POS tagger
    parses a full sentence with the goal to arrange it into a dependence tree, where
    each node corresponds to a word and the parent-child relationship determines which
    word it depends on. With this tree, it can then make more informed decisions,
    for example, whether the word "book" is a noun ("This is a good book.") or a verb
    ("Could you please book the flight?").
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是词性标注，或POS标注的核心。POS标注器解析一个完整的句子，目的是将其排列成依存树，其中每个节点对应一个单词，父子关系决定了该单词依赖哪个单词。通过这棵树，它可以做出更为明智的决策，例如判断“book”是名词（“这是一本好书。”）还是动词（“你能帮我订机票吗？”）。
- en: You might have already guessed that NLTK will play its role in this area as
    well. And indeed, it comes readily packaged with all sorts of parsers and taggers.
    The POS tagger we will use, `nltk.pos_tag()`, is actually a full blown classifier
    trained using manually annotated sentences from the Penn Treebank Project ([http://www.cis.upenn.edu/~treebank](http://www.cis.upenn.edu/~treebank)).
    It takes as input a list of word tokens and outputs a list of tuples, where each
    element contains the part of the original sentence and its part-of-speech tag.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经猜到，NLTK在这个领域也将发挥作用。的确，它提供了各种各样的解析器和标注器。我们将使用的POS标注器`nltk.pos_tag()`实际上是一个完整的分类器，经过使用Penn
    Treebank项目中的人工标注句子训练而成（[http://www.cis.upenn.edu/~treebank](http://www.cis.upenn.edu/~treebank)）。它的输入是一个单词标记列表，输出是一个包含元组的列表，每个元组包含原句的部分内容及其词性标注。
- en: '[PRE21]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The POS tag abbreviations are taken from the Penn Treebank (adapted from [http://www.anc.org/OANC/penn.html](http://www.anc.org/OANC/penn.html)):'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注缩写来自于Penn Treebank（改编自[http://www.anc.org/OANC/penn.html](http://www.anc.org/OANC/penn.html)）：
- en: '| POS tag | Description | Example |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| POS标签 | 描述 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| CC | coordinating conjunction | or |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| CC | 并列连词 | 或者 |'
- en: '| CD | cardinal number | 2, second |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| CD | 基数词 | 2，第二 |'
- en: '| DT | determiner | the |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| DT | 限定词 | 这 |'
- en: '| EX | existential there | *there* are |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| EX | 存在性there | *那里* 有 |'
- en: '| FW | foreign word | kindergarten |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| FW | 外来词 | 幼儿园 |'
- en: '| IN | preposition/subordinating conjunction | on, of, like |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| IN | 介词/从属连词 | 在，的，像 |'
- en: '| JJ | adjective | cool |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| JJ | 形容词 | 酷 |'
- en: '| JJR | adjective, comparative | cooler |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| JJR | 形容词，比较级 | 更酷 |'
- en: '| JJS | adjective, superlative | coolest |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| JJS | 形容词，最高级 | 最酷的 |'
- en: '| LS | list marker | 1) |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| LS | 列表标记 | 1) |'
- en: '| MD | modal | could, will |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| MD | 情态动词 | 可以，将 |'
- en: '| NN | noun, singular or mass | book |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| NN | 名词，单数或不可数名词 | 书 |'
- en: '| NNS | noun plural | books |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| NNS | 名词复数 | 书籍 |'
- en: '| NNP | proper noun, singular | Sean |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| NNP | 专有名词，单数 | Sean |'
- en: '| NNPS | proper noun, plural | Vikings |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| NNPS | 专有名词，复数 | 维京人 |'
- en: '| PDT | predeterminer | both the boys |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| PDT | 预定限定词 | 两个男孩 |'
- en: '| POS | possessive ending | friend''s |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| POS | 所有格结尾 | 朋友的 |'
- en: '| PRP | personal pronoun | I, he, it |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| PRP | 人称代词 | 我，他，它 |'
- en: '| PRP$ | possessive pronoun | my, his |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| PRP$ | 所有格代词 | 我的，他的 |'
- en: '| RB | adverb | however, usually, naturally, here, good |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| RB | 副词 | 然而，通常，当然，这里，好 |'
- en: '| RBR | adverb, comparative | better |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| RBR | 副词，比较级 | 更好 |'
- en: '| RBS | adverb, superlative | best |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| RBS | 副词，最高级 | 最好 |'
- en: '| RP | particle | give *up* |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| RP | 小品词 | 放*弃* |'
- en: '| TO | to | *to* go, *to* him |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| TO | 到 | *到* 去，*到* 他那里 |'
- en: '| UH | interjection | uhhuhhuhh |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| UH | 感叹词 | 嗯嗯 |'
- en: '| VB | verb, base form | take |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| VB | 动词，基本形式 | 拿 |'
- en: '| VBD | verb, past tense | took |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| VBD | 动词，过去时 | 拿 |'
- en: '| VBG | verb, gerund/present participle | taking |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| VBG | 动词，动名词/现在分词 | 拿 |'
- en: '| VBN | verb, past participle | taken |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| VBN | 动词，过去分词 | 已拿 |'
- en: '| VBP | verb, sing. present, non-3d | take |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| VBP | 动词，单数现在时，非第三人称 | 拿 |'
- en: '| VBZ | verb, 3rd person sing. present | takes |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| VBZ | 动词，第三人称单数现在时 | 拿 |'
- en: '| WDT | wh-determiner | which |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| WDT | 疑问限定词 | 哪个 |'
- en: '| WP | wh-pronoun | who, what |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| WP | 疑问代词 | 谁，什么 |'
- en: '| WP$ | possessive wh-pronoun | whose |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| WP$ | 所有格疑问代词 | 谁的 |'
- en: '| WRB | wh-abverb | where, when |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| WRB | 疑问副词 | 哪里，什么时候 |'
- en: With these tags, it is pretty easy to filter the desired tags from the output
    of `pos_tag()`. We simply have to count all words whose tags start with `NN` for
    nouns, `VB` for verbs, `JJ` for adjectives, and `RB` for adverbs.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些标签，从`pos_tag()`的输出中过滤出所需的标签是相当容易的。我们只需数出所有标签以`NN`开头的名词，`VB`开头的动词，`JJ`开头的形容词，和`RB`开头的副词。
- en: Successfully cheating using SentiWordNet
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SentiWordNet 成功作弊
- en: 'While linguistic information, as mentioned in the preceding section, will most
    likely help us, there is something better we can do to harvest it: SentiWordNet
    ([http://sentiwordnet.isti.cnr.it](http://sentiwordnet.isti.cnr.it)). Simply put,
    it is a 13 MB file that assigns most of the English words a positive and negative
    value. More complicated put, for every synonym set, it records both the positive
    and negative sentiment values. Some examples are as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面提到的语言学信息最有可能帮助我们，但我们可以做得更好来收获它：SentiWordNet ([http://sentiwordnet.isti.cnr.it](http://sentiwordnet.isti.cnr.it))。简单来说，它是一个13MB的文件，为大多数英语单词分配了正负值。更复杂的说，对于每个同义词集合，它记录了正面和负面的情感值。以下是一些例子：
- en: '| POS | ID | PosScore | NegScore | SynsetTerms | Description |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| POS | ID | PosScore | NegScore | 同义词集合 | 描述 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| a | 00311354 | 0.25 | 0.125 | studious#1 | Marked by care and effort; "made
    a studious attempt to fix the television set" |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| a | 00311354 | 0.25 | 0.125 | 勤奋的#1 | 以细心和努力为特征；"做了一个勤奋的尝试来修理电视机" |'
- en: '| a | 00311663 | 0 | 0.5 | careless#1 | Marked by lack of attention or consideration
    or forethought or thoroughness; not careful… |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| a | 00311663 | 0 | 0.5 | 粗心#1 | 特征是缺乏关注、考虑、预见或彻底性；不小心… |'
- en: '| n | 03563710 | 0 | 0 | implant#1 | A prosthesis placed permanently in tissue
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| n | 03563710 | 0 | 0 | 移植物#1 | 永久性地植入组织的假体 |'
- en: '| v | 00362128 | 0 | 0 | kink#2 curve#5 curl#1 | Form a curl, curve, or kink;
    "the cigar smoke curled up at the ceiling" |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| v | 00362128 | 0 | 0 | kink#2 曲线#5 卷曲#1 | 形成弯曲、曲线或扭结；"雪茄烟雾在天花板上卷曲" |'
- en: With the information in the **POS** column, we will be able to distinguish between
    the noun "book" and the verb "book". `PosScore` and `NegScore` together will help
    us to determine the neutrality of the word, which is 1-PosScore-NegScore. `SynsetTerms`
    lists all words in the set that are synonyms. We can safely ignore the **ID**
    and **Description** columns for our purposes.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 利用**POS**列中的信息，我们能够区分名词"book"和动词"book"。`PosScore`和`NegScore`一起帮助我们确定单词的中立性，即1-PosScore-NegScore。`SynsetTerms`列出该集合中所有的同义词。对于我们的任务来说，我们可以忽略**ID**和**Description**列。
- en: 'The synset terms have a number appended, because some occur multiple times
    in different synsets. For example, "fantasize" conveys two quite different meanings,
    which also leads to different scores:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 同义词集术语后面会有一个数字，因为有些词会出现在多个同义词集中。例如，"fantasize"表达了两种截然不同的意思，因此会导致不同的分数：
- en: '| POS | ID | PosScore | NegScore | SynsetTerms | Description |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| POS | ID | PosScore | NegScore | SynsetTerms | Description |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| v | 01636859 | 0.375 | 0 | fantasize#2 fantasise#2 | Portray in the mind;
    "he is fantasizing the ideal wife" |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| v | 01636859 | 0.375 | 0 | fantasize#2 fantasise#2 | 在脑海中描绘；"他在幻想理想的妻子" |'
- en: '| v | 01637368 | 0 | 0.125 | fantasy#1 fantasize#1 fantasise#1 | Indulge in
    fantasies; "he is fantasizing when he says he plans to start his own company"
    |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| v | 01637368 | 0 | 0.125 | fantasy#1 fantasize#1 fantasise#1 | 沉溺于幻想；"他说他计划创办自己的公司时就是在幻想"
    |'
- en: To find out which of the synsets to take, we will need to really understand
    the meaning of the tweets, which is beyond the scope of this chapter. The field
    of research that is focusing on this challenge is called word-sense-disambiguation.
    For our task, we take the easy route and simply average the scores over all the
    synsets, in which a term is found. For "fantasize", `PosScore` will be 0.1875
    and `NegScore` will be 0.0625.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出应该使用哪些同义词集，我们需要真正理解推文的含义，这超出了本章的范围。专注于这一挑战的研究领域叫做词义消歧。在我们的任务中，我们走捷径，简单地计算所有同义词集中的分数平均值，某个词项出现在这些同义词集中。例如，"fantasize"的`PosScore`为0.1875，`NegScore`为0.0625。
- en: 'The following function, `load_sent_word_net()`, does all that for us and returns
    a dictionary where the keys are strings of the form *word type/word*, for example,
    n/implant, and the values are the positive and negative scores:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数`load_sent_word_net()`会为我们完成所有这些工作，并返回一个字典，其中键是形如*词性/词*的字符串，例如n/implant，值为正负分数：
- en: '[PRE22]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Our first estimator
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的第一个估算器
- en: 'Now, we have everything in place to create our own first vectorizer. The most
    convenient way to do it is to inherit it from `BaseEstimator`. It requires us
    to implement the following three methods:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好创建我们自己的第一个特征提取器。最方便的方式是从`BaseEstimator`继承。它要求我们实现以下三个方法：
- en: '`get_feature_names()`: This returns a list of strings of the features that
    we will return in `transform()`.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_feature_names()`: 该函数返回一个字符串列表，表示我们将在`transform()`中返回的特征。'
- en: '`fit(document, y=None)`: As we are not implementing a classifier, we can ignore
    this one and simply return self.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fit(document, y=None)`: 由于我们没有实现分类器，因此可以忽略这一点，直接返回self。'
- en: '`transform(documents)`: This returns `numpy.array()`, containing an array of
    shape (`len(documents), len(get_feature_names)`). This means, for every document
    in `documents`, it has to return a value for every feature name in `get_feature_names()`.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transform(documents)`: 该方法返回`numpy.array()`，包含形状为(`len(documents), len(get_feature_names)`)的数组。也就是说，对于`documents`中的每个文档，它必须为`get_feature_names()`中的每个特征名称返回一个值。'
- en: 'Here is the implementation:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是实现：
- en: '[PRE23]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Putting everything together
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 整合所有内容
- en: Nevertheless, using these linguistic features in isolation without the words
    themselves will not take us very far. Therefore, we have to combine the `TfidfVectorizer`
    parameter with the linguistic features. This can be done with scikit-learn's `FeatureUnion`
    class. It is initialized in the same manner as `Pipeline`; however, instead of
    evaluating the estimators in a sequence each passing the output of the previous
    one to the next one, `FeatureUnion` does it in parallel and joins the output vectors
    afterwards.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅仅使用这些语言特征而不考虑单词本身，是无法带我们走得很远的。因此，我们必须将`TfidfVectorizer`参数与语言特征结合起来。可以使用scikit-learn的`FeatureUnion`类来实现这一点。它的初始化方式与`Pipeline`相同；但是，`FeatureUnion`不同于按顺序评估估算器并将每个估算器的输出传递给下一个估算器，它是并行处理的，并在之后将输出向量合并。
- en: '[PRE24]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Training and testing on the combined featurizers, gives another 0.4 percent
    improvement on average P/R AUC for positive versus negative:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在结合特征提取器上进行训练和测试，可以使正负样本的平均P/R AUC提高0.4个百分点：
- en: '[PRE25]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: With these results, we probably do not want to use the positive versus rest
    and negative versus rest classifiers, but instead use first the classifier determining
    whether the tweet contains sentiment at all (pos/neg versus irrelevant/neutral)
    and then, in case it does, use the positive versus negative classifier to determine
    the actual sentiment.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些结果，我们可能不想使用正向对比休息和负向对比休息的分类器，而是首先使用一个分类器来确定推文是否包含情感（正向/负向与无关/中立），然后，如果包含情感，再使用正向与负向分类器来确定具体情感。
- en: Summary
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Congratulations for sticking with us until the end! Together we have learned
    how Naïve Bayes works and why it is not that naïve at all. Especially, for training
    sets, where we don't have enough data to learn all the niches in the class probability
    space, Naïve Bayes does a great job of generalizing. We learned how to apply it
    to tweets and that cleaning the rough tweets' texts helps a lot. Finally, we realized
    that a bit of "cheating" (only after we have done our fair share of work) is okay.
    Especially when it gives another improvement of the classifier's performance,
    as we have experienced with the use of `SentiWordNet`.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你坚持到最后！我们一起学习了朴素贝叶斯如何工作，以及它为什么并不那么“天真”。尤其是在训练集数据不足以学习类别概率空间中的所有细分时，朴素贝叶斯在泛化方面表现出色。我们学习了如何将其应用于推文，并且清理粗糙的推文文本帮助很大。最后，我们意识到适当的“作弊”（前提是我们已经做了足够的工作）是可以接受的，尤其是当它带来分类器性能的提升时，正如我们在使用`SentiWordNet`时所经历的那样。
- en: In the next chapter, we will look at regressions.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论回归分析。
