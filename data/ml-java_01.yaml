- en: Applied Machine Learning Quick Start
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用机器学习快速入门
- en: 'This chapter introduces the basics of machine learning, laying down common
    themes and concepts and making it easy to follow the logic and familiarize yourself
    with the topic. The goal is to quickly learn the step-by-step process of applied
    machine learning and grasp the main machine learning principles. In this chapter,
    we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了机器学习的基础知识，阐述了常见的主题和概念，使逻辑易于理解，并使你能够熟悉这个主题。目标是快速学习应用机器学习的逐步过程并掌握主要的机器学习原则。在本章中，我们将涵盖以下主题：
- en: Machine learning and data science
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习和数据科学
- en: Data and problem definition
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据和问题定义
- en: Data collection
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据收集
- en: Data preprocessing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Unsupervised learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Supervised learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Generalization and evaluation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泛化和评估
- en: If you are already familiar with machine learning and are eager to start coding,
    then quickly jump to the chapters that follow this one. However, if you need to
    refresh your memory or clarify some concepts, then it is strongly recommend revisiting
    the topics presented in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经熟悉机器学习并且渴望开始编码，那么请快速跳转到本节之后的章节。然而，如果你需要刷新记忆或澄清一些概念，那么强烈建议重新回顾本章中介绍的主题。
- en: Machine learning and data science
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习和数据科学
- en: 'Nowadays, everyone talks about machine learning and data science. So, what
    exactly is machine learning, anyway? How does it relate to data science? These
    two terms are commonly confused, as they often employ the same methods and overlap
    significantly. Therefore, let''s first clarify what they are. Josh Wills tweeted
    this:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每个人都谈论机器学习和数据科学。那么，机器学习究竟是什么呢？它与数据科学有什么关系？这两个术语通常被混淆，因为它们经常使用相同的方法并且有显著的重叠。因此，让我们首先明确它们是什么。Josh
    Wills在推特上写道：
- en: '"A data scientist is a person who is better at statistics than any software
    engineer and better at software engineering than any statistician."'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: “数据科学家是一个比任何软件工程师都擅长统计学，比任何统计学家都擅长软件工程的人。”
- en: – Josh Wills
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: – Josh Wills
- en: More specifically, data science encompasses the entire process of obtaining
    knowledge by integrating methods from statistics, computer science, and other
    fields to gain insight from data. In practice, data science encompasses an iterative
    process of data harvesting, cleaning, analysis, visualization, and deployment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，数据科学包括从统计、计算机科学和其他领域整合方法以从数据中获得洞察力的整个过程。在实践中，数据科学包括数据采集、清洗、分析、可视化和部署的迭代过程。
- en: Machine learning, on the other hand, is mainly concerned with generic algorithms
    and techniques that are used in analysis and modelling phases of the data science
    process.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，另一方面，主要关注在数据科学流程的分析和建模阶段使用的通用算法和技术。
- en: Solving problems with machine learning
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用机器学习解决问题
- en: 'Among the different machine learning approaches, there are three main ways
    of learning, as shown in the following list:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的机器学习方法中，有三种主要的学习方法，如下列所示：
- en: Supervised learning
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised learning
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Reinforcement learning
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'Given a set of example inputs X, and their outcomes Y, supervised learning
    aims to learn a general mapping function f, which transforms inputs into outputs,
    as f: (X,Y).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '给定一组示例输入X和它们的输出Y，监督学习的目标是学习一个通用的映射函数f，它将输入转换为输出，即f: (X,Y)。'
- en: An example of supervised learning is credit card fraud detection, where the
    learning algorithm is presented with credit card transactions (matrix X) marked
    as normal or suspicious (vector Y). The learning algorithm produces a decision
    model that marks unseen transactions as normal or suspicious (this is the f function).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的例子是信用卡欺诈检测，其中学习算法被提供标记为正常或可疑的信用卡交易（矩阵X）和向量Y。学习算法产生一个决策模型，将未见交易标记为正常或可疑（这就是f函数）。
- en: In contrast, unsupervised learning algorithms do not assume given outcome labels,
    as they focus on learning the structure of the data, such as grouping similar
    inputs into clusters. Unsupervised learning can, therefore, discover hidden patterns
    in the data. An example of unsupervised learning is an item-based recommendation
    system, where the learning algorithm discovers similar items bought together;
    for example, people who bought book A also bought book B.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，无监督学习算法不假设有给定的输出标签，因为它们专注于学习数据的结构，例如将相似输入分组到簇中。因此，无监督学习可以揭示数据中的隐藏模式。无监督学习的一个例子是基于物品的推荐系统，其中学习算法发现一起购买的相似物品；例如，购买书籍A的人也购买了书籍B。
- en: Reinforcement learning addresses the learning process from a completely different
    angle. It assumes that an agent, which can be a robot, bot, or computer program,
    interacts with a dynamic environment to achieve a specific goal. The environment
    is described with a set of states and the agent can take different actions to
    move from one state to another. Some states are marked as goal states, and if
    the agent achieves this state, it receives a large reward. In other states, the
    reward is smaller, non-existent, or even negative. The goal of reinforcement learning
    is to find an optimal policy or a mapping function that specifies the action to
    take in each of the states, without a teacher explicitly telling whether this
    leads to the goal state or not. An example of reinforcement learning would be
    a program for driving a vehicle, where the states correspond to the driving conditions,
    for example, current speed, road segment information, surrounding traffic, speed
    limits, and obstacles on the road; and the actions could be driving maneuvers,
    such as turn left or right, stop, accelerate, and continue. The learning algorithm
    produces a policy that specifies the action that is to be taken in specific configurations
    of driving conditions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习从完全不同的角度处理学习过程。它假设一个智能体，可以是机器人、机器人或计算机程序，与动态环境互动以实现特定目标。环境由一组状态描述，智能体可以采取不同的行动从一个状态移动到另一个状态。一些状态被标记为目标状态，如果智能体达到这个状态，它会收到大量奖励。在其他状态下，奖励较小，不存在，甚至可能是负的。强化学习的目标是找到一个最优策略或映射函数，该函数指定在每个状态下采取的动作，而不需要教师明确指出这会导致目标状态与否。强化学习的一个例子是驾驶车辆的程序，其中状态对应于驾驶条件，例如当前速度、路段信息、周围交通、速度限制和道路上的障碍物；而动作可以是驾驶操作，如左转或右转、停车、加速和继续。学习算法产生一个策略，该策略指定在特定驾驶条件配置下应采取的动作。
- en: 'In this book, we will focus on supervised and unsupervised learning only, as
    they share many concepts. If reinforcement learning sparked your interest, a good
    book to start with is *Reinforcement Learning: An Introduction* by Richard S.
    Sutton and Andrew Barto, MIT Press (2018).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将仅关注监督学习和无监督学习，因为它们共享许多概念。如果您对强化学习感兴趣，可以从理查德·S·萨顿和安德鲁·巴托的《强化学习：入门》（MIT
    Press，2018年）这本书开始。
- en: Applied machine learning workflow
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用机器学习工作流程
- en: This book's emphasis is on applied machine learning. We want to provide you
    with the practical skills needed to get learning algorithms to work in different
    settings. Instead of math and theory in machine learning, we will spend more time
    on the practical, hands-on skills (and dirty tricks) to get this stuff to work
    well on an application. We will focus on supervised and unsupervised machine learning
    and learn the essential steps in data science to build the applied machine learning
    workflow.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本书强调应用机器学习。我们希望为您提供在不同环境中使学习算法工作所需的实用技能。在机器学习中，我们不会花太多时间在数学和理论上，而是会花更多时间在实用、动手技能（和一些技巧）上，以便使这些内容在实际应用中运行良好。我们将重点关注监督学习和无监督学习，并学习数据科学中构建应用机器学习工作流程的基本步骤。
- en: 'A typical workflow in applied machine learning applications consists of answering
    a series of questions that can be summarized in the following steps:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 应用机器学习应用中的典型工作流程包括回答一系列可以总结为以下步骤的问题：
- en: '![](img/91882d54-ec06-4b55-926d-128197b56788.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/91882d54-ec06-4b55-926d-128197b56788.png)'
- en: '**Data and problem definition**: The first step is to ask interesting questions,
    such as: *What is the problem you are trying solve*? *Why is it important*? *Which
    format of result answers your question*? *Is this a simple yes/no answer*? *Do
    you need to pick one of the available questions*?'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据和问题定义**：第一步是提出有趣的问题，例如：*你正在尝试解决的问题是什么*？ *为什么它很重要*？ *哪种结果格式可以回答你的问题*？ *这是一个简单的是/否答案吗*？
    *你需要从可用的问题中选择一个吗*？'
- en: '**Data collection**: Once you have a problem to tackle, you will need the data.
    Ask yourself what kind of data will help you answer the question. *Can you get
    the data from the available sources*? *Will you have to combine multiple sources*?
    *Do you have to generate the data*? *Are there any sampling biases*? *How much
    data will be required*?'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据收集**：一旦你有一个问题要解决，你将需要数据。问问自己你需要什么样的数据来回答问题。*你能从可用的来源获取数据吗*？ *你需要结合多个来源吗*？
    *你需要生成数据吗*？ *是否存在任何抽样偏差*？ *需要多少数据*？'
- en: '**Data preprocessing**: The first data preprocessing task is **data cleaning**.
    Some of the examples include filling missing values, smoothing noisy data, removing
    outliers, and resolving consistencies. This is usually followed by integration
    of multiple data sources and data transformation to a specific range (normalization),
    to value bins (discretized intervals), and to reduce the number of dimensions.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据预处理**：第一个数据预处理任务是**数据清洗**。一些例子包括填充缺失值、平滑噪声数据、移除异常值和解决一致性。这通常随后是多个数据源的集成和数据转换到特定的范围（归一化）、到值区间（离散化区间）以及减少维度数量。'
- en: '**Data analysis and modelling**: Data analysis and modelling includes unsupervised
    and supervised machine learning, statistical inference, and prediction. A wide
    variety of machine learning algorithms are available, including k-nearest neighbors,
    Naive Bayes classifier, decision trees, **Support Vector Machines** (**SVMs**),
    logistic regression, k-means, and so on. The method to be deployed depends on
    the problem definition, as discussed in the first step, and the type of collected
    data. The final product of this step is a model inferred from the data.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据分析与建模**：数据分析与建模包括无监督和监督机器学习、统计推断和预测。有各种各样的机器学习算法可供选择，包括k近邻算法、朴素贝叶斯分类器、决策树、**支持向量机**（**SVMs**）、逻辑回归、k均值等。要部署的方法取决于问题定义，正如第一步所讨论的，以及收集到的数据类型。这一步的最终产品是从数据中推断出的模型。'
- en: '**Evaluation**: The last step is devoted to model assessment. The main issue
    that the models built with machine learning face is how well they model the underlying
    data; for example, if a model is too specific or it overfits to the data used
    for training, it is quite possible that it will not perform well on new data.
    The model can be too generic, meaning that it underfits the training data. For
    example, when asked how the weather is in California, it always answers sunny,
    which is indeed correct most of the time. However, such a model is not really
    useful for making valid predictions. The goal of this step is to correctly evaluate
    the model and make sure it will work on new data as well. Evaluation methods include
    separate test and train sets, cross-validation, and leave-one-out cross-validation.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估**：最后一步是模型评估。使用机器学习构建的模型面临的主要问题是它们如何很好地模拟底层数据；例如，如果一个模型过于具体或者它过度拟合了用于训练的数据，那么它在新的数据上可能表现不佳。模型可能过于通用，这意味着它对训练数据欠拟合。例如，当被问及加利福尼亚的天气时，它总是回答晴天，这确实在大多数时候是正确的。然而，这样的模型实际上并不适用于做出有效的预测。这一步的目标是正确评估模型，并确保它在新的数据上也能正常工作。评估方法包括单独的测试和训练集，交叉验证和留一法交叉验证。'
- en: We will take a closer look at each of the steps in the following sections. We
    will try to understand the type of questions we must answer during the applied
    machine learning workflow, and look at the accompanying concepts of data analysis
    and evaluation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中更详细地研究每个步骤。我们将尝试理解在应用机器学习工作流程中我们必须回答的问题类型，并查看数据分析和评估的相关概念。
- en: Data and problem definition
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据和问题定义
- en: 'When presented with a problem definition, we need to ask questions that will
    help in understanding the objective and target information from the data. We could
    ask very common questions, such as: *what is the expected finding once the data
    is explored?* *What kind of information can be extracted after data exploration?* Or, *what
    kind of format is required so the question can be answered?* Asking the right
    question will give a clearer understanding of how to proceed further. Data is
    simply a collection of measurements in the form of numbers, words, observations,
    descriptions of things, images, and more.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当面对一个问题定义时，我们需要提出有助于理解数据目标和目标信息的问题。我们可以问一些非常常见的问题，例如：*一旦数据被探索，预期的发现是什么？* *在数据探索之后可以提取哪些信息？*
    或者，*需要什么格式的信息才能回答这个问题？* 提出正确的问题将有助于更清楚地了解如何进一步进行。数据仅仅是数字、文字、观察、事物描述、图像等形式的一系列测量。
- en: Measurement scales
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量尺度
- en: 'The most common way to represent data is using a set of attribute-value pairs.
    Consider the following example:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表示数据最常见的方式是使用一组属性-值对。考虑以下示例：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For example, `Bob` has attributes named `height`, `eye color`, and `hobbies`
    with the values `185cm`, `blue`, `climbing`, and `sky diving` respectively.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`Bob`具有名为`height`、`eye color`和`hobbies`的属性，其值分别为`185cm`、`blue`、`climbing`和`sky
    diving`。
- en: 'A set of data can be presented simply as a table, where columns correspond
    to attributes or features and rows correspond to particular data examples or instances.
    In supervised machine learning, the attribute whose value we want to predict the
    outcome, Y, from the values of the other attributes, X, is denoted as the class
    or target variable, as shown in the following table:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一组数据可以简单地以表格的形式呈现，其中列对应属性或特征，行对应特定的数据示例或实例。在监督机器学习中，我们想要从其他属性的值X中预测结果Y的属性称为类别或目标变量，如下表所示：
- en: '| **Name** | **Height [cm]** | **Eye color** | **Hobbies** |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| **Name** | **Height [cm]** | **Eye color** | **Hobbies** |'
- en: '| Bob | 185.0 | Blue | Climbing, sky diving |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| Bob | 185.0 | Blue | Climbing, sky diving |'
- en: '| Anna | 163.0 | Brown | Reading |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Anna | 163.0 | Brown | Reading |'
- en: '| ... | ... | ... | ... |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... | ... | ... |'
- en: 'The first thing we notice is how much the attribute values vary. For instance,
    height is a number, eye color is text, and hobbies are a list. To gain a better
    understanding of the value types, let''s take a closer look at the different types
    of data or measurement scales. Stanley Smith Stevens (1946) defined the following
    four scales of measurement with increasingly expressive properties:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先注意到属性值的变化有多大。例如，身高是一个数字，眼睛颜色是文本，爱好是一个列表。为了更好地理解值类型，让我们更仔细地看看不同的数据类型或测量尺度。斯坦利·史密斯·史蒂文斯（1946年）定义了以下四种测量尺度，它们具有越来越丰富的表达属性：
- en: '**Nominal data** consists of data that is mutually exclusive, but not ordered.
    Examples include eye color, marital status, type of car owned, and so on.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**名义数据**由互斥但无序的数据组成。例如，眼睛颜色、婚姻状况、拥有的车型等。'
- en: '**Ordinal data** correspond to categories where order matters, but not the
    difference between the values, such as pain level, student letter grades, service
    quality ratings, IMDb movie ratings, and so on.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有序数据**对应于顺序重要但值之间的差异不重要的类别，例如疼痛程度、学生成绩、服务质量评分、IMDb电影评分等。'
- en: '**Interval data** consists of data where the difference between two values
    is meaningful, but there is no concept of zero, for instance, standardized exam
    scores, temperature in Fahrenheit, and so on.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**区间数据**由两个值之间的差异有意义的但没有任何零的概念的数据组成，例如标准化考试分数、华氏温度等。'
- en: '**Ratio data** has all of the properties of an interval variable and also a
    clear definition of zero; when the variable is equal to zero, this variable would be
    missing. Variables such as height, age, stock prices, and weekly food spending
    are ratio variables.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**比率数据**具有区间变量的所有属性，并且有一个明确的零的定义；当变量等于零时，这个变量将是缺失的。例如，身高、年龄、股价和每周食品支出等变量都是比率变量。'
- en: Why should we care about measurement scales? Well, machine learning depends heavily on
    the statistical properties of the data; hence, we should be aware of the limitations
    that each data type possesses. Some machine learning algorithms can only be applied
    to a subset of measurement scales.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么应该关心测量尺度呢？嗯，机器学习在很大程度上依赖于数据的统计属性；因此，我们应该了解每种数据类型所具有的限制。一些机器学习算法只能应用于测量尺度的子集。
- en: 'The following table summarizes the main operations and statistics properties
    for each of the measurement types:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了每种测量类型的主要操作和统计属性：
- en: '| **Property** | **Nominal** | **Ordinal** | **Interval** | **Ratio** |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **属性** | **名义** | **有序** | **区间** | **比率** |'
- en: '| 1 | Frequency of distribution | True | True | True | True |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 分布频率 | 是 | 是 | 是 | 是 |'
- en: '| 2 | Mode and median |  | True | True | True |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 模式和众数 |  | 是 | 是 | 是 |'
- en: '| 3 | Order of values is known |  | True | True | True |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 值的顺序是已知的 |  | 是 | 是 | 是 |'
- en: '| 4 | Can quantify difference between each value |  |  | True | True |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 可以量化每个值之间的差异 |  |  | 是 | 是 |'
- en: '| 5 | Can add or subtract values |  |  | True | True |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 可以加减值 |  |  | 是 | 是 |'
- en: '| 6 | Can multiply and divide values |  |  |  | True |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 可以乘除值 |  |  |  | 是 |'
- en: '| 7 | Has true zero |  |  |  | True |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 有真正的零 |  |  |  | 是 |'
- en: Furthermore, nominal and ordinal data correspond to discrete values, while interval
    and ratio data can correspond to continuous values as well. In supervised learning,
    the measurement scale of the attribute values that we want to predict dictates
    the kind of machine algorithm that can be used. For instance, predicting discrete
    values from a limited list is called classification and can be achieved using
    decision trees, while predicting continuous values is called regression, which
    can be achieved using model trees.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，名义和有序数据对应于离散值，而区间和比率数据也可以对应于连续值。在监督学习中，我们想要预测的属性值的测量尺度决定了可以使用哪种机器算法。例如，从有限列表中预测离散值称为分类，可以使用决策树实现，而预测连续值称为回归，可以使用模型树实现。
- en: Data collection
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据收集
- en: 'Once questions are asked in the right direction, the target of data exploration
    is clear. So, the next step is to see where the data comes from. Data collected
    can be much unorganized and in very diverse formats, which may involve reading
    from a database, internet, file system, or other documents. Most of the tools
    for machine learning require data to be presented in a specific format in order
    to generate the proper result. We have two choices: observe the data from existing
    sources or generate the data via surveys, simulations, and experiments. Let''s
    take a closer look at both approaches.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦以正确的方向提出问题，数据探索的目标就变得清晰。因此，下一步是查看数据来源。收集到的数据可能非常无序，并且格式非常多样，这可能涉及从数据库、互联网、文件系统或其他文档中读取。大多数机器学习工具都需要数据以特定格式呈现，以便生成正确的结果。我们有两种选择：观察现有来源的数据或通过调查、模拟和实验生成数据。让我们更详细地看看这两种方法。
- en: Finding or observing data
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找或观察数据
- en: Data can be found or observed in many places. An obvious data source is the
    internet. With an increase in social media usage, and with mobile phones penetrating
    deeper as mobile data plans become cheaper or even offer unlimited data, there
    has been an exponential rise in data consumed by users.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以在许多地方找到或观察到。一个明显的数据来源是互联网。随着社交媒体使用的增加，以及随着移动数据计划变得更便宜或甚至提供无限数据，用户消耗的数据呈指数级增长。
- en: 'Now, online streaming platforms have emerged—the following diagram shows that
    the hours spent on consuming video data is also growing rapidly:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在线流媒体平台已经出现——以下图表显示，用于消费视频数据的小时数也在迅速增长：
- en: '![](img/57611773-a4b5-405c-b1ad-0934a9a81475.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57611773-a4b5-405c-b1ad-0934a9a81475.png)'
- en: 'To get data from the internet, there are multiple options, as shown in the
    following list:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要从互联网获取数据，有多种选择，如下所示列表所示：
- en: 'Bulk downloads from websites such as Wikipedia, IMDb, and the *Million Song
    Dataset* (which can be found here: [https://labrosa.ee.columbia.edu/millionsong/](https://labrosa.ee.columbia.edu/millionsong/)).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自维基百科、IMDb 和 *百万歌曲数据集*（可在以下链接找到：[https://labrosa.ee.columbia.edu/millionsong/](https://labrosa.ee.columbia.edu/millionsong/)）的批量下载。
- en: Accessing the data through APIs (such as Google, Twitter, Facebook, and YouTube).
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 API（如 Google、Twitter、Facebook 和 YouTube）访问数据。
- en: It is okay to scrape public, non-sensitive, and anonymized data. Be sure to
    check the terms and conditions and to fully reference the information.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爬取公开的、非敏感的和匿名化的数据是可以的。务必检查条款和条件，并完全引用信息。
- en: The main drawbacks of the data collected is that it takes time and space to
    accumulate the data, and it covers only what happened; for instance, intentions
    and internal and external motivations are not collected. Finally, such data might
    be noisy, incomplete, inconsistent, and may even change over time.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 收集到的数据的主要缺点是积累数据需要时间和空间，并且它只覆盖了已经发生的事情；例如，意图和内部及外部动机没有被收集。最后，这样的数据可能存在噪声、不完整、不一致，甚至可能随时间变化。
- en: Another option is to collect measurements from sensors such as inertial and
    location sensors in mobile devices, environmental sensors, and software agents
    monitoring key performance indicators.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是从移动设备中的惯性传感器和位置传感器、环境传感器以及监控关键性能指标软件代理收集测量数据。
- en: Generating data
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据生成
- en: An alternative approach is to generate the data by you, for example, with a
    survey. In survey design, we have to pay attention to data sampling; that is,
    who the respondents are that are answering the survey. We only get data from the
    respondents who are accessible and willing to respond. Also, respondents can provide
    answers that are in line with their self-image and researcher's expectations.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是自行生成数据，例如，通过调查。在调查设计中，我们必须注意数据采样；也就是说，谁在回答调查。我们只从可以接触并愿意回答的受访者那里获取数据。此外，受访者可以提供符合他们自我形象和研究者期望的答案。
- en: Alternatively, the data can be collected with simulations, where a domain expert
    specifies the behavior model of users at a micro level. For instance, crowd simulation
    requires specifying how different types of users will behave in a crowd. Some
    of the examples could be following the crowd, looking for an escape, and so on.
    The simulation can then be run under different conditions to see what happens
    (Tsai et al., 2011). Simulations are appropriate for studying macro phenomena
    and emergent behavior; however, they are typically hard to validate empirically.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，数据可以通过模拟来收集，其中领域专家在微观层面上指定用户的行为模型。例如，人群模拟需要指定不同类型的用户在人群中会如何行为。一些例子可能是跟随人群、寻找逃生路线等。然后，可以在不同的条件下运行模拟，以观察会发生什么（Tsai等人，2011）。模拟适合研究宏观现象和涌现行为；然而，它们通常很难从经验上进行验证。
- en: Furthermore, you can design experiments to thoroughly cover all of the possible
    outcomes, where you keep all of the variables constant and only manipulate one
    variable at a time. This is the most costly approach, but usually provides the
    best quality.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以设计实验来彻底覆盖所有可能的后果，其中你保持所有变量不变，一次只操纵一个变量。这是最昂贵的方法，但通常提供最好的质量。
- en: Sampling traps
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 采样陷阱
- en: Data collection may involve many traps. To demonstrate one, let me share a story.
    There is supposed to be a global, unwritten rule for sending regular mail between
    students for free. If you write student to student in the place where the stamp
    should be, the mail is delivered to the recipient for free. Now, suppose Jacob
    sends a set of postcards to Emma, and given that Emma indeed receives some of
    the postcards, she concludes that all of the postcards are delivered and that
    the rule indeed holds true. Emma reasons that, as she received the postcards,
    all of the postcards are delivered. However, she does not know of the postcards
    that were sent by Jacob, but were undelivered; hence, she is unable to account
    for this in her inference. What Emma experienced is survivorship bias; that is,
    she drew the conclusion based on the data that survived. For your information,
    postcards that are sent with a student to student stamp get a circled black letter
    T stamp on them, which mean postage is due and the receiver should pay it, including
    a small fine. However, mail services often have higher costs on applying such
    fees and hence do not do it. (Magalhães, 2010).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集可能会涉及许多陷阱。为了说明一个，让我分享一个故事。在学生之间免费发送平信应该有一个全球性的、不成文的规则。如果你在邮票应该贴的地方写学生给学生的信，邮件就可以免费递送给收件人。现在，假设雅各布给艾玛寄了一套明信片，并且鉴于艾玛确实收到了一些明信片，她得出结论认为所有明信片都已被递送，而且这个规则确实是真的。艾玛推理，因为她收到了明信片，所以所有明信片都已被递送。然而，她不知道雅各布寄出的但未被递送的明信片；因此，她无法在她的推理中解释这一点。艾玛所经历的是幸存者偏差；也就是说，她是基于幸存的数据得出结论的。顺便说一句，带有学生给学生邮票的明信片上会有一个圆圈黑字母T的邮票，这意味着邮资已付，收件人应支付邮资，包括一小笔罚款。然而，邮件服务通常在应用此类费用时成本较高，因此通常不会这样做。（Magalhães，2010）
- en: Another example is a study that found that the profession with the lowest average
    age of death was student. Being a student does not cause you to die at an early
    age; rather, being a student means you are young. This is why the average is so
    low. (Gelman and Nolan, 2002).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是一项研究发现，平均寿命最低的职业是学生。成为学生并不会导致你过早死亡；相反，成为学生意味着你年轻。这就是为什么平均寿命如此之低的原因。（Gelman
    和 Nolan，2002）
- en: Furthermore, a study that found that only 1.5% of drivers in accidents reported
    they were using a cell phone, whereas 10.9% reported another occupant in the car
    distracted them. Can we conclude that using a cell phone is safer than speaking
    with another occupant? (Uts, 2003) To answer this question, we need to know the
    prevalence of the cell phone use. It is likely that a higher number of people
    talked to another occupant in the car while driving than talked on a cell phone
    during the period when the data was collected.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一项研究发现，只有1.5%的交通事故司机报告说他们在使用手机，而10.9%的人报告说车内有其他乘客分散了他们的注意力。我们能得出使用手机比与车内其他乘客交谈更安全的结论吗？（Uts，2003）要回答这个问题，我们需要知道手机使用的普遍性。很可能是，在收集数据期间，开车时与车内其他乘客交谈的人数比在手机上交谈的人数要多。
- en: Data preprocessing
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: The goal of data preprocessing tasks is to prepare the data for a machine learning
    algorithm in the best possible way, as not all algorithms are capable of addressing
    issues with missing data, extra attributes, or denormalized values.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理任务的目标是以最佳方式准备数据，以便用于机器学习算法，因为并非所有算法都能处理缺失数据、额外属性或非规范化值的问题。
- en: Data cleaning
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据清理
- en: 'Data cleaning, also known as data cleansing or data scrubbing, is a process
    consisting of the following steps:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理，也称为数据净化或数据擦洗，是一个包含以下步骤的过程：
- en: Identifying inaccurate, incomplete, irrelevant, or corrupted data to remove
    it from further processing
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别不准确、不完整、不相关或损坏的数据，并将其从进一步处理中移除
- en: Parsing data, extracting information of interest, or validating whether a string
    of data is in an acceptable format
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析数据，提取感兴趣的信息，或验证数据字符串是否处于可接受的格式
- en: Transforming data into a common encoding format, for example, UTF-8 or int32,
    time scale, or a normalized range
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据转换为通用的编码格式，例如，UTF-8 或 int32，时间尺度或归一化范围
- en: Transforming data into a common data schema; for instance, if we collect temperature
    measurements from different types of sensors, we might want them to have the same
    structure
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据转换为通用的数据模式；例如，如果我们从不同类型的传感器收集温度测量值，我们可能希望它们具有相同的结构
- en: Filling missing values
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 填充缺失值
- en: 'Machine learning algorithms generally do not work well with missing values.
    Rare exceptions include decision trees, Naive Bayes classifier, and some rule-based
    learners. It is very important to understand why a value is missing. It can be
    missing due to many reasons, such as random error, systematic error, and sensor
    noise. Once we identify the reason, there are multiple ways to deal with the missing
    values, as shown in the following list:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法通常不擅长处理缺失值。罕见的例外包括决策树、朴素贝叶斯分类器和一些基于规则的学习者。了解值缺失的原因非常重要。它可能由于许多原因而缺失，例如随机错误、系统错误和传感器噪声。一旦我们确定了原因，就有多种方法可以处理缺失值，如下列所示：
- en: '**Remove the instance**: If there is enough data, and only a couple of non-relevant
    instances have some missing values, then it is safe to remove these instances.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移除实例**：如果有足够的数据，并且只有少数几个非相关实例有一些缺失值，那么移除这些实例是安全的。'
- en: '**Remove the attribute**: Removing an attribute makes sense when most of the
    values are missing, values are constant, or an attribute is strongly correlated
    with another attribute.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移除属性**：当大多数值缺失、值恒定或属性与另一个属性高度相关时，移除属性是有意义的。'
- en: '**Assign a special value** (**N/A**): Sometimes a value is missing due to valid
    reasons, such as the value is out of scope, the discrete attribute value is not
    defined, or it is not possible to obtain or measure the value. For example, if
    a person never rates a movie, their rating on this movie is nonexistent.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分配特殊值**（**N/A**）：有时值缺失是由于有效的原因，例如值超出范围、离散属性值未定义，或无法获得或测量该值。例如，如果一个人从未评价过一部电影，那么他对这部电影的评分就不存在。'
- en: '**Take the average attribute value**: If we have a limited number of instances,
    we might not be able to afford removing instances or attributes. In that case,
    we can estimate the missing values by assigning the average attribute value.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**取平均属性值**：如果我们有有限数量的实例，我们可能无法承担移除实例或属性。在这种情况下，我们可以通过分配平均属性值来估计缺失值。'
- en: '**Predict the value from other attributes**: Predict the value from previous
    entries if the attribute possesses time dependencies.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从其他属性预测值**：如果属性具有时间依赖性，则从之前的条目预测值。'
- en: As we have seen, the value can be missing for many reasons, and hence, it is
    important to understand why the value is missing, absent, or corrupted.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，值可能由于许多原因而缺失，因此了解值缺失、不存在或损坏的原因非常重要。
- en: Remove outliers
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除异常值
- en: 'Outliers in data are values that are unlike any other values in the series
    and affect all learning methods to various degrees. These can be extreme values,
    which could be detected with confidence intervals and removed by using a threshold.
    The best approach is to visualize the data and inspect the visualization to detect
    irregularities. An example is shown in the following diagram. Visualization applies
    to low-dimensional data only:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中的异常值是与系列中其他任何值都不同的值，并且会以不同程度影响所有学习方法。这些可能是极端值，可以通过置信区间检测并使用阈值移除。最佳方法是可视化数据并检查可视化以检测不规则性。以下是一个示例图。可视化仅适用于低维数据：
- en: '![](img/83097aac-9415-4756-aef1-e80e8567a157.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/83097aac-9415-4756-aef1-e80e8567a157.png)'
- en: Data transformation
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据转换
- en: 'Data transformation techniques tame the dataset to a format that a machine
    learning algorithm expects as input and may even help the algorithm to learn faster
    and achieve better performance. It is also known as data munging or data wrangling.
    Standardization, for instance, assumes that data follows Gaussian distribution
    and transforms the values in such a way that the mean value is 0 and the deviation
    is 1, as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换技术将数据集转换为机器学习算法期望的输入格式，甚至可能帮助算法更快地学习并实现更好的性能。这也被称为数据整理或数据清洗。例如，标准化假设数据遵循高斯分布，并按以下方式转换值，使得平均值是0，标准差是1：
- en: '![](img/137411ff-835c-4d8b-9472-74b038795d30.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/137411ff-835c-4d8b-9472-74b038795d30.png)'
- en: 'Normalization, on the other hand, scales the values of attributes to a small,
    specified range, usually between 0 and 1:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，归一化会将属性值缩放到一个小的、指定的范围，通常在0到1之间：
- en: '![](img/61ec0095-4708-41b2-9675-a4c37483604c.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61ec0095-4708-41b2-9675-a4c37483604c.png)'
- en: Many machine learning toolboxes automatically normalize and standardize the
    data for you.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习工具箱会自动为您归一化和标准化数据。
- en: 'The last transformation technique is discretization, which divides the range
    of a continuous attribute into intervals. Why should we care? Some algorithms,
    such as decision trees and Naive Bayes prefer discrete attributes. The most common
    ways to select the intervals are as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种转换技术是离散化，它将连续属性的取值范围划分为区间。为什么我们应该关心这个问题呢？一些算法，如决策树和朴素贝叶斯，更喜欢离散属性。选择区间的最常见方法如下：
- en: '**Equal width**: The interval of continuous variables is divided into *k* equal
    width intervals'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**等宽**：连续变量的区间被划分为*k*个等宽的区间'
- en: '**Equal frequency**: Supposing there are *N* instances, each of the *k* intervals
    contains approximately *N* or *k* instances'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**等频率**：假设有*N*个实例，每个*k*个区间大约包含*N*或*k*个实例'
- en: '**Min entropy**: This approach recursively splits the intervals until the entropy,
    which measures disorder, decreases more than the entropy increase, introduced
    by the interval split (Fayyad and Irani, 1993)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小熵**：这种方法递归地分割区间，直到熵（衡量无序）的减少超过由区间分割引入的熵增加（Fayyad和Irani，1993）。'
- en: The first two methods require us to specify the number of intervals, while the
    last method sets the number of intervals automatically; however, it requires the
    class variable, which means it won't work for unsupervised machine learning tasks.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 前两种方法需要我们指定区间的数量，而最后一种方法会自动设置区间的数量；然而，它需要类变量，这意味着它不适用于无监督机器学习任务。
- en: Data reduction
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据降维
- en: Data reduction deals with abundant attributes and instances. The number of attributes
    corresponds to the number of dimensions in our dataset. Dimensions with low prediction
    power contribute very little to the overall model, and cause a lot of harm. For
    instance, an attribute with random values can introduce some random patterns that
    will be picked up by a machine learning algorithm. It may happen that data contains
    a large number of missing values, wherein we have to find the reason for missing
    values in large numbers, and on that basis, it may fill it with some alternate
    value or impute or remove the attribute altogether. If 40% or more values are
    missing, then it may be advisable to remove such attributes, as this will impact
    the model performance.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 数据降维处理的是大量的属性和实例。属性的数量对应于我们的数据集维度数。预测能力低的维度对整体模型贡献很小，并造成很多危害。例如，具有随机值的属性可能会引入一些随机模式，这些模式会被机器学习算法捕捉到。可能数据中包含大量缺失值，其中我们必须找到大量缺失值的原因，在此基础上，可能用一些替代值填充它或进行插补或完全删除该属性。如果40%或更多值缺失，那么可能建议删除此类属性，因为这会影响模型性能。
- en: The other factor is variance, where the constant variable may have low variance,
    which means the data is very close to each other or there is not very much variation
    in the data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个因素是方差，其中常数变量可能具有低方差，这意味着数据彼此非常接近，或者数据中的变化不是很大。
- en: To deal with this problem, the first set of techniques removes such attributes
    and selects the most promising ones. This process is known as feature selection,
    or attributes selection, and includes methods such as ReliefF, information gain,
    and the Gini index. These methods are mainly focused on discrete attributes.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，第一组技术删除了这些属性并选择了最有希望的属性。这个过程被称为特征选择或属性选择，包括ReliefF、信息增益和基尼指数等方法。这些方法主要关注离散属性。
- en: 'Another set of tools, focused on continuous attributes, transforms the dataset
    from the original dimensions into a lower-dimensional space. For example, if we
    have a set of points in three-dimensional space, we can make a projection into
    a two-dimensional space. Some information is lost, but in a situation where the
    third dimension is irrelevant, we don''t lose much, as the data structure and
    relationships are almost perfectly preserved. This can be performed by the following
    methods:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 另一组工具专注于连续属性，将数据集从原始维度转换到低维空间。例如，如果我们有三个空间中的一组点，我们可以将其投影到二维空间。一些信息会丢失，但在第三维无关紧要的情况下，我们不会丢失太多，因为数据结构和关系几乎完美地保留了下来。这可以通过以下方法实现：
- en: '**Singular value decomposition** (**SVD**)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奇异值分解**（**SVD**）'
- en: '**Principal component analysis** (**PCA**)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）'
- en: Backward/forward feature elimination
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后向/前向特征消除
- en: Factor analysis
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因子分析
- en: '**Linear discriminant analysis** (**LDA**)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性判别分析**（**LDA**）'
- en: Neural network autoencoders
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络自编码器
- en: The second problem in data reduction is related to too many instances; for example,
    they can be duplicates or come from a very frequent data stream. The main idea
    is to select a subset of instances in such a way that distribution of the selected
    data still resembles the original data distribution, and more importantly, the
    observed process. Techniques to reduce the number of instances involve random
    data sampling, stratification, and others. Once the data is prepared, we can start
    with the data analysis and modeling.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 数据降维的第二个问题与实例过多有关；例如，它们可能是重复的，或者来自非常频繁的数据流。主要思想是以一种方式选择实例的子集，使得所选数据的分布仍然类似于原始数据分布，更重要的是，与观察到的过程相似。减少实例数量的技术包括随机数据抽样、分层等。一旦数据准备就绪，我们就可以开始数据分析建模。
- en: Unsupervised learning
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Unsupervised learning is about analyzing the data and discovering hidden structures
    in unlabeled data. As no notion of the right labels is given, there is also no
    error measure to evaluate a learned model; however, unsupervised learning is an
    extremely powerful tool. Have you ever wondered how Amazon can predict what books
    you'll like? Or how Netflix knows what you want to watch before you do? The answer
    can be found in unsupervised learning. We will look at a similar example of unsupervised
    learning in the following section.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是关于分析数据并在未标记数据中发现隐藏结构的过程。由于没有给出正确的标签概念，因此也没有误差度量来评估学习模型；然而，无监督学习是一个极其强大的工具。你是否曾经想过亚马逊是如何预测你会喜欢哪些书的？或者Netflix是如何在你之前就知道你想看什么？答案可以在无监督学习中找到。我们将在下一节中查看一个类似的无监督学习示例。
- en: Finding similar items
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找相似项目
- en: Many problems can be formulated as finding similar sets of elements, for example,
    customers who purchased similar products, web pages with similar content, images
    with similar objects, users who visited similar websites, and so on.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 许多问题可以表述为寻找相似元素集，例如，购买相似产品的客户、内容相似的网页、具有相似对象的图像、访问相似网站的用户等等。
- en: 'Two items are considered similar if they are a small distance apart. The main
    questions are how each item is represented and how the distance between the items
    is defined. There are two main classes of distance measures:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个项目之间的距离很小，则认为它们是相似的。主要问题是每个项目是如何表示的，以及项目之间的距离是如何定义的。距离度量主要有两大类：
- en: Euclidean distances
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: Non-Euclidean distances
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非欧几里得距离
- en: Euclidean distances
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: In Euclidean space, with the *n* dimension, the distance between two elements
    is based on the locations of the elements in such a space, which is expressed
    as **p-norm distance**. Two commonly used distance measures are L2- and L1-norm
    distances.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在欧几里得空间中，具有 *n* 维，两个元素之间的距离基于该空间中元素的位置，这被称为**p-norm距离**。两种常用的距离度量是L2-和L1-norm距离。
- en: '**L2-norm**, also known as Euclidean distance, is the most frequently applied
    distance measure that measures how far apart two items in a two-dimensional space
    are. It is calculated as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**L2-norm**，也称为欧几里得距离，是应用最广泛的距离度量，用于衡量二维空间中两个项目之间的距离。其计算方法如下：'
- en: '![](img/bb336e5c-d229-4f2c-ae61-a9cb703abf71.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bb336e5c-d229-4f2c-ae61-a9cb703abf71.png)'
- en: '**L1-norm**, also known as Manhattan distance, city block distance, and taxicab
    norm, simply sums the absolute differences in each dimension, as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**L1-norm**，也称为曼哈顿距离、城市街区距离和出租车规范，简单地将每个维度上的绝对差异相加，如下所示：'
- en: '![](img/1217c0c1-2d6a-4128-8a33-79643686255f.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1217c0c1-2d6a-4128-8a33-79643686255f.png)'
- en: Non-Euclidean distances
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非欧几里得距离
- en: A non-Euclidean distance is based on the properties of the elements, but not
    on their location in space. Some well known distances are Jaccard distance, cosine
    distance, edit distance, and Hamming distance.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 非欧几里得距离是基于元素的性质，而不是它们在空间中的位置。一些著名的距离包括Jaccard距离、余弦距离、编辑距离和汉明距离。
- en: '**Jaccard distance** is used to compute the distance between two sets. First,
    we compute the Jaccard similarity of two sets as the size of their intersection
    divided by the size of their union, as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jaccard距离**用于计算两个集合之间的距离。首先，我们计算两个集合的Jaccard相似度，即它们的交集大小除以它们的并集大小，如下所示：'
- en: '![](img/2bf37964-2cd6-4012-893a-41677d13ba7c.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2bf37964-2cd6-4012-893a-41677d13ba7c.png)'
- en: 'The Jaccard distance is then defined as per the following formula:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Jaccard距离随后根据以下公式定义：
- en: '![](img/16af1b11-46fd-423b-906f-4fe6bd23f4f9.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/16af1b11-46fd-423b-906f-4fe6bd23f4f9.png)'
- en: '**Cosine distance** between two vectors focuses on the orientation and not
    magnitude, therefore, two vectors with the same orientation have a cosine similarity
    of 1, while two perpendicular vectors have a cosine similarity of 0\. Supposing
    that we have two multidimensional points, think of a point as a vector from origin
    (*0, 0, ..., 0*) to its location. Two vectors make an angle, whose cosine distance
    is a normalized dot-product of the vectors, as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**余弦距离**关注的是两个向量之间的方向而不是大小，因此，具有相同方向的两个向量具有余弦相似度为1，而垂直的两个向量具有余弦相似度为0。假设我们有两个多维点，可以将一个点视为从原点（*0,
    0, ..., 0*）到其位置的向量。两个向量形成一个角度，其余弦距离是向量的归一化点积，如下所示：'
- en: '![](img/1e1238be-2ba1-41ef-a36a-56d784af1638.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1e1238be-2ba1-41ef-a36a-56d784af1638.png)'
- en: Cosine distance is commonly used in a high-dimensional feature space; for instance,
    in text mining, where a text document represents an instance, features that correspond
    to different words, and their values correspond to the number of times the word
    appears in the document. By computing cosine similarity, we can measure how likely
    two documents match in describing similar content.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦距离在高维特征空间中常用；例如，在文本挖掘中，其中文本文档代表一个实例，特征对应不同的单词，它们的值对应于单词在文档中出现的次数。通过计算余弦相似度，我们可以衡量两个文档在描述相似内容时匹配的可能性。
- en: '**Edit distance** makes sense when we compare two strings. The distance between
    the a=a1,a2,a3,...an and b=b1,b2,b3,...bn strings is the smallest number of the
    insert/delete operation of single characters required to convert the string from
    a to b, for example, a = abcd and b = abbd. To convert a into b, we have to delete
    the second b and insert c in its place. No smallest number of operations would
    convert a into b, hence the distance is d(a, b) =2.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**编辑距离**在比较两个字符串时是有意义的。字符串a=a1,a2,a3,...an和b=b1,b2,b3,...bn之间的距离是转换字符串a到b所需的最小单字符插入/删除操作的数量，例如，a
    = abcd和b = abbd。要将a转换为b，我们必须删除第二个b并在其位置插入c。没有最小的操作数量可以将a转换为b，因此距离是d(a, b) = 2。'
- en: '**Hamming distance** compares two vectors of the same size and counts the number
    of dimensions in which they differ. In other words, it measures the number of
    substitutions required to convert one vector into another.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**汉明距离**比较两个相同大小的向量，并计算它们在哪些维度上不同。换句话说，它衡量将一个向量转换为另一个向量所需的替换次数。'
- en: 'There are many distance measures focusing on various properties, for instance,
    correlation measures the linear relationship between two elements; **Mahalanobis
    distance** measures the distance between a point and distribution of other points
    and **SimRank**, which is based on graph theory, measures similarity of the structure
    in which elements occur, and so on. As you can already imagine, selecting and
    designing the right similarity measure for your problem is more than half of the
    battle. An impressive overview and evaluation of similarity measures is collected
    in Chapter 2, *Similarity and Dissimilarity Measures**,* in the book *Image Registration:
    Principles, Tools and Methods* by A. A. Goshtasby, Springer Science and Business
    Media (2012).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多距离度量专注于各种属性，例如，相关度衡量两个元素之间的线性关系；**马氏距离**衡量一个点与其他点的分布之间的距离，**SimRank**，基于图论，衡量元素出现的结构相似性，等等。正如你所能想象的，为你的问题选择和设计合适的相似性度量超过了一半的战斗。在A.
    A. Goshtasby所著的《图像配准：原理、工具和方法》一书中，收集了关于相似性度量的令人印象深刻的概述和评估，该书由Springer Science
    and Business Media（2012年）出版。
- en: The curse of dimensionality
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度诅咒
- en: The curse of dimensionality refers to a situation where we have a large number
    of features, often hundreds or thousands, which lead to an extremely large space
    with sparse data and, consequently, to distance anomalies. For instance, in high
    dimensions, almost all pairs of points are equally distant from each other; in
    fact, almost all of the pairs have distance close to the average distance. Another
    manifestation of the curse is that any two vectors are almost orthogonal, which
    means all of the angles are close to 90 degrees. This practically makes any distance
    measurement useless.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 维度诅咒指的是我们拥有大量特征的情况，通常有数百或数千个，这导致了一个具有稀疏数据且距离异常的极大空间。例如，在高维中，几乎所有的点对彼此的距离都相等；事实上，几乎所有的点对都有接近平均距离的距离。维度诅咒的另一种表现是任何两个向量几乎正交，这意味着所有角度都接近90度。这实际上使得任何距离测量都变得无用。
- en: A cure for the curse of dimensionality might be found in one of the data reduction
    techniques, where we want to reduce the number of features; for instance, we can
    run a feature selection algorithm, such as ReliefF, or a feature extraction or
    reduction algorithm, such as PCA.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 解决维度诅咒的方法可能隐藏在数据降维技术中，我们希望减少特征的数量；例如，我们可以运行一个特征选择算法，如ReliefF，或者一个特征提取或降维算法，如PCA。
- en: Clustering
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: 'Clustering is a technique for grouping similar instances into clusters according
    to some distance measures. The main idea is to put instances that are similar
    (that is, close to each other) into the same cluster, while keeping the dissimilar
    points (that is, the ones further apart from each other) in different clusters.
    An example of how clusters might look like is shown in the following diagram:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种根据某些距离度量将相似实例分组到聚类的技术。主要思想是将相似的实例（即彼此接近的实例）放入同一个聚类中，同时将不相似的点（即彼此距离较远的点）保留在不同的聚类中。以下图表显示了聚类可能的外观示例：
- en: '![](img/72273512-ea06-44d6-aae4-ffc7bd4f70a4.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/72273512-ea06-44d6-aae4-ffc7bd4f70a4.png)'
- en: The clustering algorithms follow two fundamentally different approaches. The
    first is a hierarchical or agglomerative approach that first considers each point
    as its own cluster, and then iteratively merges the most similar clusters together.
    It stops when further merging reaches a predefined number of clusters, or if the
    clusters to be merged are spread over a large region.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法遵循两种根本不同的方法。第一种是层次或聚合方法，它首先将每个点视为自己的聚类，然后迭代地将最相似的聚类合并在一起。当进一步的合并达到预定义的聚类数量或要合并的聚类分布在很大区域时，它停止。
- en: The other approach is based on point assignment. First, initial cluster centers
    (that is, centroids) are estimated, for instance, randomly, and then, each point
    is assigned to the closest cluster, until all of the points are assigned. The
    most well known algorithm in this group is k-means clustering.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法基于点分配。首先，估计初始聚类中心（即质心），例如随机地，然后，每个点被分配到最近的聚类，直到所有点都被分配。这个群体中最著名的算法是k-means聚类。
- en: The k-means clustering either picks initial cluster centers as points that are
    as far as possible from one another, or (hierarchically) clusters a sample of
    data and picks a point that is the closest to the center of each of the k-clusters.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: k-means聚类算法要么选择尽可能远的点作为初始聚类中心，要么（层次化地）聚类数据样本并选择距离每个k聚类中心最近的点。
- en: Supervised learning
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'Supervised learning is the key concept behind such amazing things as voice
    recognition, email spam filtering, and face recognition in photos, and detecting
    credit card frauds. More formally, given a set, D, of learning examples described
    with features, X, the goal of supervised learning is to find a function that predicts
    a target variable, Y. The function, f ,that describes the relation between features
    X and class Y is called a model:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习是语音识别、电子邮件垃圾邮件过滤、照片中的人脸识别以及信用卡欺诈检测等令人惊叹的事情背后的关键概念。更正式地说，给定一个由特征X描述的学习示例集合D，监督学习的目标是找到一个预测目标变量Y的函数。描述特征X和类别Y之间关系的函数f被称为模型：
- en: '![](img/2b91c10c-c155-47f2-97fd-39675c71c1d5.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2b91c10c-c155-47f2-97fd-39675c71c1d5.png)'
- en: 'The general structure of supervised learning algorithms is defined by the following
    decisions (Hand et al., 2001):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法的一般结构由以下决策定义（Hand等人，2001年）：
- en: Define the task
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义任务
- en: Decide on the machine learning algorithm, which introduces specific inductive
    bias; that is, and a priori assumptions that it makes regarding the target concept
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定机器学习算法，它引入特定的归纳偏差；即它对目标概念做出的先验假设
- en: Decide on the score or cost function, for instance, information gain, root mean
    square error, and so on
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定得分或成本函数，例如信息增益、均方根误差等
- en: Decide on the optimization/search method to optimize the score function
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定优化/搜索方法以优化得分函数
- en: Find a function that describes the relation between X and Y
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到一个描述X和Y之间关系的函数
- en: Many decisions are already made for us by the type of the task and dataset that
    we have. In the following sections, we will take a closer look at the classification
    and regression methods and the corresponding score functions.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们具有的任务类型和数据集，许多决策已经为我们做出了。在以下章节中，我们将更详细地探讨分类和回归方法以及相应的得分函数。
- en: Classification
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: Classification can be applied when we deal with a discrete class, where the
    goal is to predict one of the mutually exclusive values in the target variable.
    An example would be credit scoring, where the final prediction is whether the
    person is credit liable or not. The most popular algorithms include decision trees,
    Naive Bayes classifiers, SVMs, neural networks, and ensemble methods.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理离散类时，可以应用分类，目标是在目标变量的互斥值中预测一个值。一个例子是信用评分，最终的预测是个人是否有信用责任。最流行的算法包括决策树、朴素贝叶斯分类器、SVMs、神经网络和集成方法。
- en: Decision tree learning
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树学习
- en: Decision tree learning builds a classification tree, where each node corresponds
    to one of the attributes; edges correspond to a possible value (or intervals)
    of the attribute from which the node originates; and each leaf corresponds to
    a class label. A decision tree can be used to visually and explicitly represent
    the prediction model, which makes it a very transparent (white box) classifier.
    Notable algorithms are ID3 and C4.5, although many alternative implementations
    and improvements exist (for example, J48 in Weka).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树学习构建了一个分类树，其中每个节点对应一个属性；边对应于从节点起源的属性的可能的值（或区间）；每个叶子节点对应一个类标签。决策树可以用来直观和明确地表示预测模型，这使得它成为一个非常透明的（白盒）分类器。著名的算法包括ID3和C4.5，尽管存在许多替代实现和改进（例如，Weka中的J48）。
- en: Probabilistic classifiers
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率分类器
- en: Given a set of attribute values, a probabilistic classifier is able to predict
    a distribution over a set of classes, rather than an exact class. This can be
    used as a degree of certainty; that is, how sure the classifier is about its prediction.
    The most basic classifier is Naive Bayes, which happens to be the optimal classifier
    if, and only if, the attributes are conditionally independent. Unfortunately,
    this is extremely rare in practice.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组属性值，概率分类器能够预测一个类集的分布，而不是一个确切的类。这可以用作确定性的程度；也就是说，分类器对其预测有多大的信心。最基本的分类器是朴素贝叶斯，它恰好是当且仅当属性条件独立时，最优的分类器。不幸的是，这在实践中极为罕见。
- en: There is an enormous subfield denoted as probabilistic graphical models, comprising
    hundreds of algorithms for example, Bayesian networks, dynamic Bayesian networks,
    hidden Markov models, and conditional random fields that can handle not only specific
    relationships between attributes, but also temporal dependencies. Kiran R Karkera
    wrote an excellent introductory book on this topic, *Building Probabilistic Graphical
    Models with Python*, Packt Publishing (2014), while Koller and Friedman published
    a comprehensive theory bible, *Probabilistic Graphical Models, *MIT Press (2009).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一个巨大的子领域，称为概率图模型，包括数百个算法，例如贝叶斯网络、动态贝叶斯网络、隐马尔可夫模型和条件随机字段，它们不仅可以处理属性之间的特定关系，还可以处理时间依赖性。Kiran
    R Karkera撰写了一本关于这个主题的优秀入门书籍，《使用Python构建概率图模型》，Packt Publishing（2014年），而Koller和Friedman出版了一本全面的理论圣经，《概率图模型》，MIT
    Press（2009年）。
- en: Kernel methods
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核方法
- en: Any linear model can be turned into a non-linear model by applying the kernel
    trick to the model—replacing its features (predictors) by a kernel function. In
    other words, the kernel implicitly transforms our dataset into higher dimensions.
    The kernel trick leverages the fact that it is often easier to separate the instances
    in more dimensions. Algorithms capable of operating with kernels include the kernel
    perceptron, SVMs, Gaussian processes, PCA, canonical correlation analysis, ridge
    regression, spectral clustering, linear adaptive filters, and many others.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 任何线性模型都可以通过将核技巧应用于模型来转化为非线性模型——用核函数替换其特征（预测器）。换句话说，核隐式地将我们的数据集转换到更高维。核技巧利用了这样一个事实，即在更高维中分离实例通常更容易。能够操作核的算法包括核感知器、SVMs、高斯过程、PCA、典型相关分析、岭回归、谱聚类、线性自适应滤波器以及许多其他算法。
- en: Artificial neural networks
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: Artificial neural networks are inspired by the structure of biological neural
    networks and are capable of machine learning, as well as pattern recognition.
    They are commonly used for both regression and classification problems, comprising
    a wide variety of algorithms and variations for all manner of problem types. Some
    popular classification methods are **perceptron**, **restricted Boltzmann machine**
    (**RBM**), and **deep belief networks**.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络受到生物神经网络结构的启发，能够进行机器学习和模式识别。它们通常用于回归和分类问题，包括各种算法和变体，适用于各种问题类型。一些流行的分类方法包括**感知器**、**受限玻尔兹曼机**（**RBM**）和**深度信念网络**。
- en: Ensemble learning
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习
- en: Ensemble methods compose of a set of diverse weaker models to obtain better
    predictive performance. The individual models are trained separately and their
    predictions are then combined in some way to make the overall prediction. Ensembles,
    hence, contain multiple ways of modeling the data, which hopefully leads to better
    results. This is a very powerful class of techniques, and as such, it is very
    popular. This class includes boosting, bagging, AdaBoost, and random forest. The
    main differences among them are the type of weak learners that are to be combined
    and the ways in which to combine them.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法由一组不同的较弱模型组成，以获得更好的预测性能。这些模型分别进行训练，然后以某种方式结合它们的预测来做出整体预测。因此，集成包含多种建模数据的方式，这有望带来更好的结果。这是一类非常强大的技术，因此非常受欢迎。这类技术包括提升、装袋、AdaBoost和随机森林。它们之间的主要区别在于要组合的弱学习者的类型以及组合它们的方式。
- en: Evaluating classification
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估分类
- en: 'Is our classifier doing well? Is this better than the other one? In classification,
    we count how many times we classify something right and wrong. Suppose there are
    two possible classification labels of yes and no, then there are four possible
    outcomes, as shown in the following table:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分类器表现如何？这比另一个好吗？在分类中，我们计算我们正确和错误分类的次数。假设有两种可能的分类标签：是和否，那么就有四种可能的结果，如下表所示：
- en: '|  | **Predicted as positive?** |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | **预测为正吗？** |'
- en: '| **Yes** | **No** |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| **是** | **否** |'
- en: '| **Really positive?** | **Yes** | TP-True Positive | FN- False Negative |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| **真的积极吗？** | **是** | TP-真阳性 | FN-假阴性 |'
- en: '| **No** | FP- False Positive | TN-True Negative |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| **否** | FP-假阳性 | TN-真阴性 |'
- en: 'The four variables:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 四个变量：
- en: '**True positive** (**hit**): This indicates a yes instance correctly predicted
    as yes'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阳性**（**命中**）：这表示一个被正确预测为是的“是”实例'
- en: '**True negative** (**correct rejection**): This indicates a no instance correctly
    predicted as no'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阴性**（**正确拒绝**）：这表示没有实例被正确预测为否'
- en: '**False positive** (**false alarm**): This indicates a no instance predicted
    as yes'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性**（**误报**）：这表示没有实例被预测为是'
- en: '**False negative** (**miss**): This indicates a yes instance predicted as no'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性**（**漏报**）：这表示一个被预测为否的“是”实例'
- en: 'The basic two performance measures of a classifier are, firstly, classification
    error:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的两个基本性能指标是，首先，分类错误：
- en: '![](img/74b24dfa-575e-4108-937c-5c4df718e6cb.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/74b24dfa-575e-4108-937c-5c4df718e6cb.png)'
- en: 'And, secondly, classification accuracy is another performance measure, as shown
    here:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，分类准确率是另一个性能指标，如下所示：
- en: '![](img/6965f2b6-d7d9-4d2e-81f4-9f985d4525f4.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6965f2b6-d7d9-4d2e-81f4-9f985d4525f4.png)'
- en: 'The main problem with these two measures is that they cannot handle unbalanced
    classes. Classifying whether a credit card transaction is an abuse or not is an
    example of a problem with unbalanced classes: there are 99.99% normal transactions
    and just a tiny percentage of abuses. The classifier that says that every transaction
    is a normal one is 99.99% accurate, but we are mainly interested in those few
    classifications that occur very rarely.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个指标的主要问题是它们无法处理不平衡的类别。例如，将信用卡交易分类为滥用或非滥用是一个具有不平衡类别的例子：有99.99%的正常交易和极小比例的滥用。声称每笔交易都是正常交易的分类器准确率为99.99%，但我们主要对那些非常罕见的分类感兴趣。
- en: Precision and recall
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确度和召回率
- en: 'The solution is to use measures that don''t involve true negatives. Two such
    measures are as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是使用不涉及真阴性的指标。以下有两种这样的指标：
- en: '**Precision**: This is the proportion of positive examples correctly predicted
    as positive (*TP*) out of all examples predicted as positive (*TP + FP*):'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度**：这是所有预测为正的例子中，正确预测为正的例子（*TP*）的比例，占所有预测为正的例子（*TP + FP*）的比例：'
- en: '![](img/af34d88a-09a6-425b-8077-666a435e7de7.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/af34d88a-09a6-425b-8077-666a435e7de7.png)'
- en: '**Recall**: This is the proportion of positives examples correctly predicted
    as positive (*TP*) out of all positive examples (*TP + FN*):'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**：这是所有正例中正确预测为正例（*TP*）的比例，占所有正例（*TP + FN*）的比例：'
- en: '![](img/05bfa43a-8747-4ac9-901a-d60e5b23a5ba.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05bfa43a-8747-4ac9-901a-d60e5b23a5ba.png)'
- en: 'It is common to combine the two and report the *F-measure*, which considers
    both precision and recall to calculate the score as a weighted average, where
    the score reaches its best value at 1 and worst at 0, as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 通常会将这两个指标结合起来，并报告 *F度量*，它考虑了精确度和召回率来计算得分，得分作为加权平均，其中得分的最佳值为1，最差值为0，如下所示：
- en: '![](img/a90480a0-db38-444c-b45b-9dcc2853f71d.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a90480a0-db38-444c-b45b-9dcc2853f71d.png)'
- en: Roc curves
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ROC曲线
- en: 'Most classification algorithms return a classification confidence denoted as
    *f(X)*, which is, in turn, used to calculate the prediction. Following the credit
    card abuse example, a rule might look similar to the following:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数分类算法返回一个表示分类置信度的值，记为 *f(X)*，该值反过来用于计算预测。以信用卡滥用为例，一条规则可能看起来类似于以下内容：
- en: '![](img/f0de84bb-14de-4b98-9008-8dfb40a47150.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0de84bb-14de-4b98-9008-8dfb40a47150.png)'
- en: 'The threshold determines the error rate and the true positive rate. The outcomes
    of all the possible threshold values can be plotted as **receiver operating characteristics**
    (**ROC**) as shown in the following diagram:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值决定了错误率和真正例率。所有可能阈值值的输出可以绘制为**接收者操作特征**（**ROC**），如下所示：
- en: '![](img/63acddca-37ab-4e66-8ebe-38c4afd55caf.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63acddca-37ab-4e66-8ebe-38c4afd55caf.png)'
- en: A random predictor is plotted with a red dashed line and a perfect predictor
    is plotted with a green dashed line. To compare whether the **A** classifier is
    better than **C**, we compare the area under the curve.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 随机预测器用红色虚线绘制，完美预测器用绿色虚线绘制。为了比较**A**分类器是否优于**C**分类器，我们比较曲线下的面积。
- en: Most of the toolboxes provide all of the previous measures out of the box.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数工具箱都提供所有这些先前度量。
- en: Regression
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归
- en: Regression deals with a continuous target variable, unlike classification, which
    works with a discrete target variable. For example, in order to forecast the outside
    temperature of the following few days, we would use regression, while classification
    will be used to predict whether it will rain or not. Generally speaking, regression
    is a process that estimates the relationship among features, that is, how varying
    a feature changes the target variable.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 回归处理连续的目标变量，与分类不同，分类处理离散的目标变量。例如，为了预测未来几天的外部温度，我们会使用回归，而分类将用于预测是否会下雨。一般来说，回归是一个估计特征之间关系的过程，即一个特征的变化如何影响目标变量。
- en: Linear regression
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: 'The most basic regression model assumes linear dependency between features
    and target variable. The model is often fitted using least squares approach, that
    is, the best model minimizes the squares of the errors. In many cases, linear
    regression is not able to model complex relations; for example, the following
    diagram shows four different sets of points having the same linear regression
    line. The upper-left model captures the general trend and can be considered as
    a proper model, whereas the bottom-left model fits points much better (except
    for one outlier, which should be carefully checked), and the upper and lower-right
    side linear models completely miss the underlying structure of the data and cannot
    be considered proper models:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的回归模型假设特征和目标变量之间存在线性依赖关系。模型通常使用最小二乘法进行拟合，即最佳模型最小化误差的平方。在许多情况下，线性回归无法建模复杂的关系；例如，以下图表显示了具有相同线性回归线的四组不同点。左上角的模型捕捉了总体趋势，可以被认为是一个合适的模型，而左下角的模型拟合点更好（除了一个异常值，应仔细检查），而上右和下右的线性模型完全错过了数据的潜在结构，不能被认为是合适的模型：
- en: '![](img/5cf2c635-8ac0-4f34-ae6f-0bac01f6e6f1.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5cf2c635-8ac0-4f34-ae6f-0bac01f6e6f1.png)'
- en: Logistic regression
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Linear regression works when the dependent variable is continuous. If, however,
    the dependent variable is binary in nature, that is, 0 or 1, success or failure,
    yes or no, true or false, survived or died, and so on, then logistic regression
    is used instead. One such example is a clinical trial of drugs where the subject
    under study either responds to the drugs or does not respond. It is also used
    in fraud detection where the transaction is either a fraud or not fraud. Normally,
    a logistic function is used to measure the relationship between dependent and
    independent variables. It is seen as a Bernoulli distribution and, when plotted,
    looks similar to a curve in the shape of characters.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归适用于因变量是连续的情况。然而，如果因变量本质上是二元的，即0或1，成功或失败，是或否，真或假，存活或死亡等，那么就使用逻辑回归。一个这样的例子是药物的临床试验，研究对象要么对药物有反应，要么没有反应。它也用于欺诈检测，交易要么是欺诈，要么不是欺诈。通常，使用逻辑函数来衡量因变量和自变量之间的关系。它被视为伯努利分布，当绘制时，看起来类似于字符形状的曲线。
- en: Evaluating regression
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估回归
- en: 'In regression, we predict numbers, Y, from input, X, and the predictions are
    usually wrong or not exact. The main question that we have to ask is: by how much?
    In other words, we want to measure the distance between the predicted and true
    values.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归中，我们从输入X预测数字Y，预测通常是不准确的或不是精确的。我们必须问的主要问题是：偏差有多大？换句话说，我们想要衡量预测值和真实值之间的距离。
- en: Mean squared error
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均方误差
- en: '**Mean squared error **(**MSE**) is an average of the squared difference between
    the predicted and true values, as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方误差**（**MSE**）是预测值与真实值之间平方差的平均值，如下所示：'
- en: '![](img/6f1df0a0-f696-4754-b732-517b9b1aff7f.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6f1df0a0-f696-4754-b732-517b9b1aff7f.png)'
- en: The measure is very sensitive to the outliers, for example, 99 exact predictions
    and 1 prediction off by 10 is scored the same as all predictions wrong by 1\.
    Moreover, the measure is sensitive to the mean. Therefore, a relative squared
    error that compares the MSE of our predictor to the MSE of the mean predictor
    (which always predicts the mean value) is often used instead.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这个度量对异常值非常敏感，例如，99次精确预测和1次预测偏差10分与所有预测偏差1分的情况得分相同。此外，这个度量对平均值也很敏感。因此，通常使用一个相对均方误差来比较我们的预测器的均方误差与均值预测器的均方误差（它总是预测平均值）。
- en: Mean absolute error
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平均绝对误差
- en: '**Mean absolute error **(**MAS**) is an average of the absolute difference
    between the predicted and the true values, as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAS**）是预测值与真实值之间绝对差的平均值，如下所示：'
- en: '![](img/43999978-fc51-4401-92e7-a768b394d343.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/43999978-fc51-4401-92e7-a768b394d343.png)'
- en: The MAS is less sensitive to the outliers, but it is also sensitive to the mean
    and scale.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: MAS对异常值不太敏感，但它对平均值和尺度也很敏感。
- en: Correlation coefficient
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关系数
- en: '**Correlation coefficient **(**CC**) compares the average of prediction relative
    to the mean, multiplied by training values relative to the mean. If the number
    is negative, it means weak correlation; a positive number means strong correlation;
    and zero means no correlation. The correlation between true values *X* and predictions
    *Y* is defined as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关系数**（**CC**）比较预测的平均值相对于平均值，乘以训练值相对于平均值。如果这个数字是负数，意味着弱相关；正数表示强相关；零表示无相关。真实值X和预测值Y之间的相关系数定义为如下：'
- en: '![](img/9458c0ac-02a7-49dc-95fc-834bf433eb37.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9458c0ac-02a7-49dc-95fc-834bf433eb37.png)'
- en: The CC measure is completely insensitive to the mean and scale and less sensitive
    to the outliers. It is able to capture the relative ordering, which makes it useful
    for ranking tasks, such as document relevance and gene expression.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: CC度量对平均值和尺度完全不敏感，对异常值也不太敏感。它能够捕捉相对顺序，这使得它在排名任务中很有用，例如文档相关性和基因表达。
- en: Generalization and evaluation
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 泛化和评估
- en: Once the model is built, how do we know it will perform on new data? Is this
    model any good? To answer these questions, we'll first look into the model generalization,
    and then see how to get an estimate of the model performance on new data.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦构建了模型，我们如何知道它在新的数据上会表现如何？这个模型是否好？为了回答这些问题，我们首先将研究模型泛化，然后看看如何对新数据上的模型性能进行估计。
- en: Underfitting and overfitting
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欠拟合和过拟合
- en: 'Predictor training can lead to models that are too complex or too simple. The
    model with low complexity (the leftmost models in the following diagram) can be
    as simple as predicting the most frequent or mean class value, while the model
    with high complexity (the rightmost models) can represent the training instances.
    Modes that are too rigid, shown on the left-hand side, cannot capture complex
    patterns; while models that are too flexible, shown on the right-hand side, fit
    to the noise in the training data. The main challenge is to select the appropriate
    learning algorithm and its parameters, so that the learned model will perform
    well on the new data (for example, the middle column):'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 预测器训练可能导致模型过于复杂或过于简单。具有低复杂度的模型（下图中最左侧的模型）可以简单到预测最频繁或平均的类别值，而具有高复杂度的模型（最右侧的模型）可以表示训练实例。过于刚性的模型（左侧显示），无法捕捉复杂模式；而过于灵活的模型（右侧显示），会拟合训练数据中的噪声。主要挑战是选择适当的学习算法及其参数，以便学习到的模型在新数据上表现良好（例如，中间列）：
- en: '![](img/8dae28b2-e179-4037-b85c-08fe12352d0d.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8dae28b2-e179-4037-b85c-08fe12352d0d.png)'
- en: 'The following diagram shows how errors in the training set decreases with model
    complexity. Simple rigid models underfit the data and have large errors. As model
    complexity increases, it describes the underlying structure of the training data
    better and, consequentially, the error decreases. If the model is too complex,
    it overfits the training data and its prediction error increases again:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了随着模型复杂度的增加，训练集中的误差如何降低。简单的刚性模型欠拟合数据，误差较大。随着模型复杂度的增加，它更好地描述了训练数据的潜在结构，因此误差降低。如果模型过于复杂，它就会过拟合训练数据，其预测误差再次增加：
- en: '![](img/d717d235-8dfa-4f24-b797-114e1db578d7.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d717d235-8dfa-4f24-b797-114e1db578d7.png)'
- en: 'Depending on the task complexity and data availability, we want to tune our
    classifiers toward more or less complex structures. Most learning algorithms allow
    such tuning, as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 根据任务复杂度和数据可用性，我们希望调整我们的分类器以适应更复杂或更简单的结构。大多数学习算法允许这样的调整，如下所示：
- en: '**Regression**: This is the order of the polynomial'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**: 这是指多项式的阶数'
- en: '**Naive Bayes**: This is the number of the attributes'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯**: 这是指属性的个数'
- en: '**Decision trees**: This is the number of nodes in the tree—pruning confidence'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策树**: 这是指树中的节点数量——剪枝置信度'
- en: '**K-nearest neighbors**: This is the number of neighbors—distance-based neighbor
    weights'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K近邻法**: 这是指邻居的数量——基于距离的邻居权重'
- en: '**SVM**: This is the kernel type; cost parameter'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SVM**: 这是指核类型；成本参数'
- en: '**Neural network**: This is the number of neurons and hidden layers'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络**: 这是指神经元和隐藏层的数量'
- en: With tuning, we want to minimize the generalization error; that is, how well
    the classifier performs on future data. Unfortunately, we can never compute the
    true generalization error; however, we can estimate it. Nevertheless, if the model
    performs well on the training data but performance is much worse on the test data,
    then the model most likely overfits.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整，我们希望最小化泛化误差；即分类器在未来的数据上的表现如何。不幸的是，我们永远无法计算真实的泛化误差；然而，我们可以估计它。尽管如此，如果一个模型在训练数据上表现良好，但在测试数据上的表现却很差，那么这个模型很可能是过拟合的。
- en: Train and test sets
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练集和测试集
- en: 'To estimate the generalization error, we split our data into two parts: training
    data and testing data. A general rule of thumb is to split them by the training:
    testing ratio, that is, 70:30\. We first train the predictor on the training data,
    then predict the values for the test data, and finally, compute the error, that
    is, the difference between the predicted and the true values. This gives us an
    estimate of the true generalization error.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计泛化误差，我们将数据分为两部分：训练数据和测试数据。一个一般规则是按照训练：测试的比例来分割，即70:30。我们首先在训练数据上训练预测器，然后预测测试数据的值，最后计算误差，即预测值和真实值之间的差异。这为我们提供了对真实泛化误差的估计。
- en: 'The estimation is based on the two following assumptions: first, we assume
    that the test set is an unbiased sample from our dataset; and second, we assume
    that the actual new data will reassemble the distribution as our training and
    testing examples. The first assumption can be mitigated by cross-validation and
    stratification. Also, if it is scarce, one can''t afford to leave out a considerable
    amount of data for a separate test set, as learning algorithms do not perform
    well if they don''t receive enough data. In such cases, cross-validation is used
    instead.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 估计基于以下两个假设：首先，我们假设测试集是我们数据集的无偏样本；其次，我们假设实际的新数据将重新组合成与我们的训练和测试示例相同的分布。第一个假设可以通过交叉验证和分层来缓解。此外，如果数据稀缺，就不能为了单独的测试集而放弃大量数据，因为学习算法在没有足够数据的情况下表现不佳。在这种情况下，可以使用交叉验证代替。
- en: Cross-validation
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证
- en: 'Cross-validation splits the dataset into *k* sets of approximately the same
    size—for example, in the following diagram, into five sets. First, we use sets
    2 to 5 for learning and set 1 for training. We then repeat the procedure five
    times, leaving out one set at a time for testing, and average the error over the
    five repetitions:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证将数据集分成大约相同大小的*k*个集合——例如，在以下图中分成五个集合。首先，我们使用集合2到5进行学习，集合1进行训练。然后我们重复此过程五次，每次留出一个集合进行测试，并平均五次重复的错误：
- en: '![](img/55371877-1535-4fd0-9ae7-02cc6291e082.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/55371877-1535-4fd0-9ae7-02cc6291e082.png)'
- en: This way, we use all of the data for learning and testing as well, while avoiding
    using the same data to train and test a model.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们既使用了所有数据用于学习和测试，又避免了使用相同的数据来训练和测试模型。
- en: Leave-one-out validation
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 留一法验证
- en: An extreme example of cross-validation is the leave-one-out validation. In this
    case, the number of folds is equal to the number of instances; we learn on all
    but one instance, and then test the model on the omitted instance. We repeat this
    for all instances, so that each instance is used exactly once for the validation.
    This approach is recommended when we have a limited set of learning examples,
    for example, less than 50.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证的一个极端例子是留一法验证。在这种情况下，折叠的数量等于实例的数量；我们在所有实例上学习，除了一个实例，然后在被省略的实例上测试模型。我们为所有实例重复此操作，以便每个实例正好用于验证一次。当学习示例有限时，例如少于50个，这种方法是推荐的。
- en: Stratification
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分层
- en: Stratification is a procedure to select a subset of instances in such a way
    that each fold roughly contains the same proportion of class values. When a class
    is continuous, the folds are selected so that the mean response value is approximately
    equal in all of the folds. Stratification can be applied along with cross-validation
    or separate training and test sets.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 分层是一种选择实例子集的过程，使得每个折叠大致包含相同比例的类别值。当一个类别是连续的，折叠的选择使得所有折叠中的平均响应值大致相等。分层可以与交叉验证或单独的训练和测试集一起应用。
- en: Summary
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we refreshed our knowledge of machine learning basics. We revisited
    the workflow of applied machine learning and clarified the main tasks, methods,
    and algorithms. We learned about different types for regression and how to evaluate
    them. We also explored cross-validation and where it is applied.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们更新了我们对机器学习基础知识的了解。我们回顾了应用机器学习的流程，并明确了主要任务、方法和算法。我们学习了回归的不同类型以及如何评估它们。我们还探讨了交叉验证及其应用。
- en: In the next chapter, we will learn about Java libraries, the tasks that they
    can perform, and different platforms for machine learning.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习Java库，它们可以执行的任务，以及机器学习的不同平台。
