- en: Supervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'As we learned in the first chapter, supervised learning is one of two major
    branches of machine learning. In a way, it is similar to how humans learn a new
    skill: someone else shows us what to do, and we are then able to learn by following
    their example. In the case of supervised learning algorithms, we usually need
    lots of examples, that is, lots of data providing the **input** to our algorithm
    and what the **expected output** should be. The algorithm will learn from this
    data, and then be able to **predict** the output based on new inputs that it has
    not seen before.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第一章中学到的，监督学习是机器学习的两个主要分支之一。从某种意义上说，它与人类学习新技能的方式相似：有人向我们展示该怎么做，然后我们通过模仿他们的例子来学习。在监督学习算法的情况下，我们通常需要大量的例子，即大量的数据提供算法的**输入**以及**预期输出**应该是什么。算法将从这些数据中学习，然后能够根据它之前未见过的新输入**预测**输出。
- en: 'A surprising number of problems can be addressed using supervised learning.
    Many email systems use it to classify emails as either important or unimportant
    automatically whenever a new message arrives in the inbox. More complex examples
    include image recognition systems, which can identify what an image contains purely
    from the input pixel values^([1]). These systems start by learning from huge datasets
    of images that have been labelled manually by humans, but are then able to categorize
    completely new images automatically. It is even possible to use supervised learning
    to steer a car automatically around a racing track: the algorithm starts by learning
    how a human driver controls the vehicle, and is eventually able to replicate this
    behavior^([2]).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 使用监督学习可以解决大量问题。许多电子邮件系统会自动将新消息分类为重要或不重要，每当新消息到达收件箱时就会使用它。更复杂的例子包括图像识别系统，这些系统可以仅从输入像素值中识别图像内容^([1])。这些系统最初是通过学习大量由人类手动标记的图像数据集来学习的，但随后能够自动对全新的图像进行分类。甚至可以使用监督学习来自动驾驶赛车：算法首先学习人类驾驶员如何控制车辆，最终能够复制这种行为^([2])。
- en: 'By the end of this chapter, you will be able to use Go to implement two types
    of supervised learning:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将能够使用Go实现两种类型的监督学习：
- en: '**Classification**, where an algorithm must learn to classify the input into
    two or more discrete categories. We will build a simple image recognition system
    to demonstrate how this works.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**，其中算法必须学习将输入分类到两个或多个离散类别中。我们将构建一个简单的图像识别系统来展示这是如何工作的。'
- en: '**Regression**, in which the algorithm must learn to predict a continuous variable,
    for example, the price of an item for sale on a website. For our example, we will
    predict house prices based on inputs, such as the location, size, and age of the
    house.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**，其中算法必须学习预测一个连续变量，例如，在网站上出售的商品的价格。在我们的例子中，我们将根据输入预测房价，例如房屋的位置、大小和年龄。'
- en: 'In this chapter, we will be covering the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: When to use regression and classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何时使用回归和分类
- en: How to implement regression and classification using Go machine learning libraries
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用Go机器学习库实现回归和分类
- en: How to measure the performance of an algorithm
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何衡量算法的性能
- en: 'We will cover the two stages involved in building a supervised learning system:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖构建监督学习系统涉及的两个阶段：
- en: '**Training**, which is the learning phase where we use labelled data to calibrate
    an algorithm'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练**，这是使用标记数据校准算法的学习阶段'
- en: '**Inference** or **prediction**, where we use the trained algorithm for its
    intended purpose: to make predictions from input data'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理**或**预测**，即我们使用训练好的算法来实现其预期目的：从输入数据中进行预测'
- en: Classification
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: When starting any supervised learning problem, the first step is to load and
    prepare the data. We are going to start by loading the **MNIST Fashion** **dataset**^([3]),
    a collection of small, grayscale images showing different items of clothing. Our
    job is to build a system that can recognize what is in each image; that is, does
    it contain a dress, a shoe, a coat, and so on?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始任何监督学习问题之前，第一步是加载数据并准备数据。我们将从加载**MNIST Fashion** **数据集**^([3])开始，这是一个包含不同服装的小型、灰度图像集合。我们的任务是构建一个能够识别每张图像内容的系统；也就是说，它是否包含连衣裙、鞋子、外套等？
- en: 'First, we need to download the dataset by running the `download-fashion-mnist.sh`
    script in the code repository. Then, we will load it into Go:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要通过在代码仓库中运行`download-fashion-mnist.sh`脚本来下载数据集。然后，我们将将其加载到Go中：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s start by taking a look at a sample of the images. Each one is 28 x 28
    pixels, and each pixel has a value between 0 and 255\. We are going to use these
    pixel values as the inputs to our algorithm: our system will accept 784 inputs
    from an image and use them to classify the image according to which item of clothing
    it contains. In Jupyter, you can view an image as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看图像的样本。每个图像都是28 x 28像素，每个像素的值在0到255之间。我们将使用这些像素值作为算法的输入：我们的系统将从图像中接受784个输入，并使用它们来根据包含的衣物项目对图像进行分类。在Jupyter中，您可以按以下方式查看图像：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will display one of the 28 x 28 images from the dataset, as shown in the
    following image:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示数据集中28 x 28像素的图像之一，如下面的图像所示：
- en: '![](img/a4ddbc0b-b7bd-4a7e-b2e0-4e8730942184.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4ddbc0b-b7bd-4a7e-b2e0-4e8730942184.png)'
- en: 'To make this data suitable for a machine learning algorithm, we need to convert
    it into a dataframe format, as we learned in [Chapter 2](532d8304-b31d-41ef-81c1-b13f4c692824.xhtml),
    *Setting Up the Development Environment*. To start, we will load the first 1,000
    images from the dataset:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些数据适合机器学习算法，我们需要将其转换为我们在[第2章](532d8304-b31d-41ef-81c1-b13f4c692824.xhtml)，“设置开发环境”中学到的dataframe格式。首先，我们将从数据集中加载前1000张图像：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We also need a string array that contains the possible labels for each image:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个包含每个图像可能标签的字符串数组：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It is very important to start by reserving a small proportion of your data in
    order to test the finished algorithm. This allows us to measure how well the algorithm
    works on new data that was not used during training. If you do not do this, you
    will most likely build a system that works really well during training but performs
    badly when faced with new data. To start with, we are going to use 75% of the
    images to train our model and 25% of the images to test it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先保留一小部分数据以测试最终算法非常重要。这使我们能够衡量算法在训练过程中未使用的新数据上的表现。如果您不这样做，您很可能会构建一个在训练期间表现良好但在面对新数据时表现不佳的系统。首先，我们将使用75%的图像来训练我们的模型，25%的图像来测试它。
- en: Splitting your data into a **training set** and a **test set** is crucial step
    when using supervised learning. It is normal to reserve 20-30% of the data for
    testing, but if your dataset is very large, you may be able to use less than this.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用监督学习时，将数据分成**训练集**和**测试集**是一个关键步骤。通常，我们会保留20-30%的数据用于测试，但如果您的数据集非常大，您可能可以使用更少的比例。
- en: 'Use the `Split(df dataframe.DataFrame, valFraction float64)` function from
    the last chapter to prepare these two datasets:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上一章中的`Split(df dataframe.DataFrame, valFraction float64)`函数来准备这两个数据集：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A simple model – the logistic classifier
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单的模型——逻辑分类器
- en: 'One of the simplest algorithms that solves our problem is a logistic classifier.
    This is what mathematicians call a **linear model**, which we can understand by
    thinking about a simple example where we are trying to classify the points on
    the following two charts as either circles or squares. A linear model will try
    to do this by drawing a straight line to separate the two types of point. This
    works very well on the left-hand chart, where the relationship between the inputs
    (on the chart axes) and the output (circle or square) is simple. However, it does
    not work on the right-hand chart, where it is not possible to split the points
    into two correct groups using a straight line:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 解决我们问题的最简单算法之一是逻辑分类器。这是数学家所说的**线性模型**，我们可以通过考虑一个简单的例子来理解它，在这个例子中，我们试图将以下两个图表上的点分类为圆圈或正方形。线性模型将尝试通过画一条直线来分隔这两种类型的点。这在左边的图表上效果很好，其中输入（图表轴上的）与输出（圆圈或正方形）之间的关系很简单。然而，它不适用于右边的图表，在右边的图表中，无法使用直线将点分成两个正确的组：
- en: '![](img/28aa78cc-b28a-4a64-bb0d-815d13704964.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28aa78cc-b28a-4a64-bb0d-815d13704964.png)'
- en: When faced with a new machine learning problem, it is advised that you start
    with a linear model as a **baseline**, and then compare other models to it. Although
    linear models ca not capture complex relationships in the input data, they are
    easy to understand and normally quick to implement and train. You might find that
    a linear model is good enough for the problem you are working on and save yourself
    time by not having to implement anything more complex. If not, you can try different
    algorithms and use the linear model to understand how much better they work.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 面对一个新的机器学习问题时，建议你从一个线性模型作为**基线**开始，然后将其与其他模型进行比较。尽管线性模型无法捕捉输入数据中的复杂关系，但它们易于理解，通常实现和训练速度也很快。你可能发现线性模型对于你正在解决的问题已经足够好，从而节省了时间，无需实现更复杂的模型。如果不是这样，你可以尝试不同的算法，并使用线性模型来了解它们的效果有多好。
- en: A **baseline** is a simple model that you can use as a point of reference when
    comparing different machine learning algorithms.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**基线**是一个简单的模型，你可以将其用作比较不同机器学习算法时的参考点。'
- en: 'Going back to our image dataset, we are going to use a logistic classifier
    to decide whether an image contains trousers or not. First, let''s do some final
    data preparation: simplify the labels to be either trousers (`true`) or not-trousers
    (`false`):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的图像数据集，我们将使用逻辑分类器来决定一张图片是否包含裤子。首先，让我们做一些最终的数据准备：将标签简化为“裤子”（`true`）或“非裤子”（`false`）：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We are also going to normalize the pixel data so that, instead of being stored
    as integers between 0 and 255, it will be represented by floats between 0 and
    1:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将对像素数据进行归一化，使其不再是存储在0到255之间的整数，而是表示为0到1之间的浮点数：
- en: Many supervised machine learning algorithms only work properly if the data is
    normalized, that is, rescaled so that it is between 0 and 1\. If you are having
    trouble getting an algorithm to train properly, make sure that you have normalized
    the data properly.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 许多监督式机器学习算法只有在数据归一化（即缩放，使其在0到1之间）的情况下才能正常工作。如果你在训练算法时遇到困难，请确保你已经正确归一化了数据。
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After preparing the data properly, it is finally time to create a logistic
    classifier and train it:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在正确准备数据之后，现在终于到了创建逻辑分类器并对其进行训练的时候了：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Measuring performance
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 衡量性能
- en: Now that we have our trained model, we need to measure how well it is performing
    by comparing the predictions it makes on each image with the ground truth (whether
    or not the image is a pair of trousers). A simple way to do this is to measure
    **accuracy**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练好了模型，我们需要通过将模型对每张图片的预测与真实情况（图片是否是一双裤子）进行比较来衡量其表现的好坏。一个简单的方法是测量**准确率**。
- en: '**Accuracy** measures what proportion of the input data can be classified correctly
    by the algorithm, for example, 90%, if 90 out of 100 predictions from the algorithm
    are correct.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**准确率**衡量算法能够正确分类输入数据的比例，例如，如果算法的100个预测中有90个是正确的，那么准确率为90%。'
- en: 'In our Go code example, we can test the model by looping over the validation
    dataset and counting how many images are classified correctly. This will output
    a model accuracy of 98.8%:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的Go代码示例中，我们可以通过遍历验证数据集并计算正确分类的图片数量来测试模型。这将输出模型准确率为98.8%：
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Precision and recall
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确率和召回率
- en: Measuring accuracy can be very misleading. Suppose you are building a system
    to classify whether medical patients will test positive for a rare disease, and
    in the dataset only 0.1% of examples are in fact positive. A really bad algorithm
    might predict that nobody will test positive, and yet it has an accuracy of 99.9%
    simply because the disease is rare.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 测量准确率可能会非常误导。假设你正在构建一个系统来分类医疗患者是否会测试出罕见疾病，而在数据集中只有0.1%的例子实际上是阳性的。一个非常差的算法可能会预测没有人会测试出阳性，然而它仍然有99.9%的准确率，仅仅因为这种疾病很罕见。
- en: A dataset that has many more examples of one classification versus another is
    known as **unbalanced**. Unbalanced datasets need to be treated carefully when
    measuring algorithm performance.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分类比另一个分类有更多示例的数据集被称为**不平衡**。在衡量算法性能时，需要仔细处理不平衡数据集。
- en: 'A better way to measure performance starts by putting each prediction from
    the algorithm into one of the following four categories:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更好的衡量性能的方法是从将算法的每个预测放入以下四个类别之一开始：
- en: '![](img/115c28b2-7254-4bea-b5c2-10cb3bdd4daa.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/115c28b2-7254-4bea-b5c2-10cb3bdd4daa.png)'
- en: 'We can now define some new performance metrics:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义一些新的性能指标：
- en: '**Precision** measures what fraction of the models true predictions are actually
    correct. In the following diagram, it is the true positives that are predicted
    from the model (the left-hand side of the circle) divided by all of the models
    positive predictions (everything in the circle).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度**衡量的是模型真实预测中正确预测的比例。在下面的图中，这是从模型预测出的真实阳性（圆圈的左侧）除以模型所有阳性预测（圆圈中的所有内容）。'
- en: '**Recall** measures how good the model is at identifying all the positive examples.
    In other words, the true positives (left-hand side of the circle) divided by all
    the datapoints that are actually positive (the entire left-hand side):'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**衡量模型在识别所有正例方面的好坏。换句话说，真实阳性（圆圈的左侧）除以所有实际为正的数据点（圆圈的整个左侧）：'
- en: '![](img/5ff88538-985a-4a66-9574-ea896bd17ba2.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5ff88538-985a-4a66-9574-ea896bd17ba2.png)'
- en: The preceding diagram shows datapoints that have been predicted as true by the
    model in the central circle. The points that are actually true are on the left
    half of the diagram.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图显示了模型在中心圆中预测为真实的数据点。实际上为真的点位于图的左侧一半。
- en: '**Precision** and **recall** are more robust performance metrics when working
    with unbalanced datasets. Both range between 0 and 1, where 1 indicates perfect
    performance.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**精确度**和**召回率**是在处理不平衡数据集时更稳健的性能指标。它们的范围在0到1之间，其中1表示完美性能。'
- en: 'Following is the code for the total count of true positives and false negatives:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码是计算真实阳性和假阴性的总数：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can now calculate precision and recall with the following code:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用以下代码计算精确度和召回率：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: For our linear model, we get 100% precision, meaning that there are no false
    positives, and a recall of 90.3%.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的线性模型，我们得到了100%的精确度，这意味着没有假阳性，召回率为90.3%。
- en: ROC curves
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ROC曲线
- en: 'Another way to measure performance involves looking at how the classifier works
    in more detail. Inside our model, two things happen:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种衡量性能的方法是更详细地观察分类器的工作方式。在我们的模型内部，发生两件事：
- en: First, the model calculates a value between 0 and 1, indicating how likely it
    is that a given image should be classified as a pair of trousers.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，模型计算一个介于0到1之间的值，表示给定图像被分类为裤子对的可能性有多大。
- en: A threshold is set, so that only images scoring more than the threshold get
    classified as trousers. Setting different thresholds can improve precision at
    the expense of recall and vice versa.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置一个阈值，只有得分超过阈值的图像才被分类为裤子。设置不同的阈值可以在牺牲召回率的同时提高精确度，反之亦然。
- en: 'If we look at the model output *across all the different thresholds from 0
    to 1*, we can understand more about how useful it is. We do this using something
    called the **receiver operating characteristic** (**ROC**) curve, which is a plot
    of the true positive rate versus the false positive rate across the dataset for
    different threshold values. The following three examples show ROC curves for a
    bad, moderate, and very good classifier:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看模型输出在所有不同的阈值从0到1之间的情况，我们可以更了解它的有用性。我们使用称为**接收者操作特征**（**ROC**）曲线的东西来做这件事，这是一个在不同阈值值下，数据集中真实阳性率与假阳性率的图表。以下三个示例显示了不良、中等和非常好的分类器的ROC曲线：
- en: '![](img/34189d2a-5b38-4024-8e0c-0eaa87b148d0.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/34189d2a-5b38-4024-8e0c-0eaa87b148d0.png)'
- en: By measuring the shaded area under these ROC curves, we get a simple metric
    of how good the model is, which is known as **area under curve** (**AUC)**. For
    the bad model, this is close to **0.5**, but for the very good model, it is close
    to **1.0**, indicating that the model can achieve *both* a high true positive
    rate and a low false positive rate.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通过测量这些ROC曲线下的阴影区域，我们得到一个衡量模型好坏的简单指标，这被称为**曲线下面积**（**AUC**）。对于不良模型，这个值接近**0.5**，但对于非常好的模型，这个值接近**1.0**，表明模型可以同时实现高真实阳性率和低假阳性率。
- en: The `gonum`/`stat` package provides a useful function for computing ROC curves,
    which we will use once we have extended the model to work with each of the different
    items of clothing in the dataset.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`gonum`/`stat`包提供了一个用于计算ROC曲线的有用函数，一旦我们将模型扩展到处理数据集中的每个不同物品，我们就会使用它。'
- en: The **receiver operating characteristic**, or **ROC curve**, is a plot of true
    positive rate versus false positive rate for different threshold values. It allows
    us to visualize how good the model is at classification. The AUC gives a simple
    measure of how good the classifier is.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收者操作特征**，或**ROC曲线**，是不同阈值值下真实阳性率与假阳性率的图表。它使我们能够可视化模型在分类方面的好坏。AUC提供了一个简单的衡量分类器好坏的指标。'
- en: Multi-class models
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多分类模型
- en: Up until now, we have been using **binary classification**; that is, it should
    output `true` if the image shows a pair of trousers, and `false` otherwise. For
    some problems, such as detecting whether an email is important or not, this is
    all we need. But in this example, what we really want is a model that can identify
    all the different types of clothing in our dataset, that is, shirt, boot, dress,
    and so on.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在使用**二元分类**；也就是说，如果图像显示一条裤子，则应输出`true`，否则输出`false`。对于某些问题，例如检测电子邮件是否重要，这可能就足够了。但在本例中，我们真正想要的是一个可以识别我们数据集中所有不同类型衣物的模型，即衬衫、靴子、连衣裙等。
- en: 'With some algorithm implementations, you will need to start by applying one-hot
    encoding to the output, as demonstrated in [Chapter 2](532d8304-b31d-41ef-81c1-b13f4c692824.xhtml),
    *Setting Up the Development Environment*. However, for our example, we will use
    **softmax regression** in **goml/linear**, which does this step automatically.
    We can train the model by simply feeding it with the input (pixel values) and
    the integer output (0, 1, 2, ... representing t-shirt, trouser, pullover, and
    so on):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些算法实现，你可能需要首先对输出应用one-hot编码，如[第2章](532d8304-b31d-41ef-81c1-b13f4c692824.xhtml)中所示，*设置开发环境*。然而，在我们的例子中，我们将使用**softmax回归**在**goml/linear**中，这会自动完成这一步。我们可以通过简单地给它输入（像素值）和整数输出（0，1，2等，代表T恤、裤子、开衫等）来训练模型：
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'When using this model for inference, it will output a vector of probabilities
    for each class; that is, it tells us what the probability that an input image
    is a t-shirt, trouser, and so on. This is exactly what we need for the ROC analysis,
    but, if we want a single prediction for each image, we can use the following func
    to find the class that has the *highest* probability:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用此模型进行推理时，它将为每个类别输出一个概率向量；也就是说，它告诉我们输入图像是T恤、裤子等的概率。这正是我们进行ROC分析所需要的，但如果我们要为每张图像提供一个单一的预测，我们可以使用以下函数来找到具有*最高*概率的类别：
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we can plot the ROC curve and the AUC for each individual class. The
    following code will loop over each example in the validation dataset and predict
    probabilities for each class using the new model:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以为每个单独的类别绘制ROC曲线和AUC。以下代码将遍历验证数据集中的每个示例，并使用新模型为每个类别预测概率：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can now compute AUC values for each class, which shows that our model performs
    better on some classes than others:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以计算每个类别的AUC值，这表明我们的模型在某些类别上的表现优于其他类别：
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For trousers, the AUC value is `0.96`, showing that even a simple linear model
    works really well in this case. However, shirt and pullover both score close to
    `0.6`. This makes intuitive sense: shirts and pullovers look very similar, and
    are therefore much harder for the model to recognize correctly. We can see this
    more clearly by plotting the ROC curve for each class as separate lines: the model
    clearly performs the worst on shirts and pullovers, and the best on the clothes
    that have a very distinctive shape (boots, trousers, sandals, and so on).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于裤子，AUC值为`0.96`，这表明即使是一个简单的线性模型在这种情况下也工作得非常好。然而，衬衫和开衫的得分都接近`0.6`。这从直观上是有道理的：衬衫和开衫看起来非常相似，因此模型正确识别它们要困难得多。我们可以通过为每个类别绘制ROC曲线作为单独的线条来更清楚地看到这一点：模型在衬衫和开衫上的表现最差，而在形状非常独特的衣物（如靴子、裤子、凉鞋等）上的表现最好。
- en: 'The following code loads gonums plotting libraries, creates the ROC plot, and
    saves it as a JPEG image:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码加载gonums绘图库，创建ROC图，并将其保存为JPEG图像：
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If we view the plot in Jupyter, we can see that the the worst classes follow
    the lines close to the diagonal, again indicating an AUC close to `0.5`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在Jupyter中查看图表，我们可以看到最差的类别紧贴着对角线，再次表明AUC接近`0.5`：
- en: '![](img/c384cecc-fbc7-4f0a-9daf-7377d5e36731.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c384cecc-fbc7-4f0a-9daf-7377d5e36731.png)'
- en: A non-linear model – the support vector machine
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非线性模型——支持向量机
- en: 'To move forward, we need to use a different machine learning algorithm: one
    that is able to model more complex, non-linear relationships between the pixel
    inputs and the output classes. While some of the mainstream Go machine learning
    libraries such as Golearn have support for basic algorithms like local least squares,
    there is not a single library that supports as broad a set of algorithms as Python''s
    scikit-learn or R''s standard library. For this reason, it is often necessary
    to search for alternative libraries that implement bindings to a widely used C
    library, or that contain a configurable implementation of an algorithm that is
    suited for a particular problem. For this example, we are going to use an algorithm
    called the **support vector machine** (**SVM**).SVMs can be more difficult to
    use than linear models—they have more parameters to tune—but have the advantage
    of being able to model much more complex patterns in the data.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续前进，我们需要使用不同的机器学习算法：一种能够对像素输入和输出类别之间的更复杂、非线性关系进行建模的算法。虽然一些主流的围棋机器学习库，如 Golearn，支持基本算法，如局部最小二乘法，但没有一个库支持像
    Python 的 scikit-learn 或 R 的标准库那样广泛的算法集。因此，通常需要寻找实现绑定到广泛使用的 C 库的替代库，或者包含适用于特定问题的算法的可配置实现。对于这个例子，我们将使用一个称为**支持向量机**（**SVM**）的算法。与线性模型相比，SVM
    可能更难使用——它们有更多的参数需要调整——但它们的优势在于能够对数据中的更复杂模式进行建模。
- en: An SVMis a more advanced machine learning method that can be used both for classification
    and regression. They allow us to apply **kernels** to the input data, which means
    that they can model non-linear relationships between the inputs/outputs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 是一种更高级的机器学习方法，可用于分类和回归。它们允许我们对输入数据应用**核**，这意味着它们可以建模输入/输出之间的非线性关系。
- en: 'An important feature of SVM models is their ability to use a **kernel function**.
    Put simply, this means that the algorithm can apply a transformation to the input
    data so that non-linear patterns can be found. For our example, we will use the **LIBSVM**
    library to train an SVM on the image data. LIBSVM is an open source library with
    bindings for many different languages, meaning that it is also useful if you want
    to port a model that has been built in Python''s popular scikit-learn library.
    First, we need to do some data preparation to make our input/output data suitable
    for feeding into the Go library:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 模型的一个重要特性是它们能够使用**核函数**。简单来说，这意味着算法可以对输入数据进行变换，以便找到非线性模式。在我们的例子中，我们将使用**LIBSVM**库在图像数据上训练
    SVM。LIBSVM 是一个开源库，具有多种语言的绑定，这意味着如果你想在 Python 的流行 scikit-learn 库中移植模型，它也非常有用。首先，我们需要做一些数据准备，使我们的输入/输出数据适合输入到
    Go 库中：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we can set up the SVM model and configure it with a **radial basis function**
    (**RBF**) **kernel**. RBF kernels are a common choice when using SVMs, but do
    take longer to train than linear models:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以设置 SVM 模型，并使用**径向基函数**（**RBF**）**核**对其进行配置。RBF 核在 SVM 中是一个常见的选择，但训练时间比线性模型要长：
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we can fit our model to the training data of 750 images, and then
    use `svm.SVMPredictProbability` to predict probabilities, like we did with the
    linear multi-class model:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将我们的模型拟合到 750 张图像的训练数据上，然后使用 `svm.SVMPredictProbability` 来预测概率，就像我们之前对线性多类模型所做的那样：
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As we did previously, we compute the AUC and ROC curves, which demonstrate
    that this model performs much better across the board, including the difficult
    classes, like shirt and pullover:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所做的那样，我们计算了 AUC 和 ROC 曲线，这表明该模型在各个方面的表现都更好，包括像衬衫和套头衫这样的困难类别：
- en: '![](img/91329032-d01f-413b-8771-7773e3317475.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/91329032-d01f-413b-8771-7773e3317475.png)'
- en: Overfitting and underfitting
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过度拟合和欠拟合
- en: 'The SVM model is performing much better on our validation dataset than the
    linear model, but, in order to understand what to do next, we need to introduce
    two important concepts in machine learning: **overfitting** and **underfitting**.
    These both refer to problems that can occur when training a model.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 模型在我们的验证数据集上的表现比线性模型要好得多，但为了了解下一步该做什么，我们需要介绍机器学习中的两个重要概念：**过度拟合**和**欠拟合**。这两个概念都指的是在训练模型时可能发生的问题。
- en: If a model **underfits** the data, it is *too simple* to explain the patterns
    in the input data, and therefore performs poorly when evaluated against the training
    dataset and the validation dataset. Another term for this problem is that the
    model has **high bias**.If a model **overfits** the data, it is *too complex*,
    and will not generalize well to new data points that were not included as part
    of training. This means that the model will perform well when evaluated against
    the training data, but poorly when evaluated against the validation dataset. Another
    term for this problem is that the model has **high variance**.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个模型**欠拟合**数据，它对输入数据中的模式解释得太简单，因此在评估训练数据集和验证数据集时表现不佳。这个问题还有另一个术语，即模型有**高偏差**。如果一个模型**过拟合**数据，它太复杂了，不能很好地推广到训练中没有包含的新数据点。这意味着当评估训练数据时，模型表现良好，但当评估验证数据集时表现不佳。这个问题还有另一个术语，即模型有**高方差**。
- en: 'An easy way to understand the difference between overfitting and underfitting
    is to look at the following simple example: when building a model, our aim is
    to build something that is just right for the dataset. The example on the left
    underfits because a straight line model can not accurately divide the circles
    and squares. The model on the right is too complex: it separates all the circles
    and squares correctly, but is unlikely to work well on new data:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 理解过拟合和欠拟合之间的区别的一个简单方法是看看以下简单的例子：在构建模型时，我们的目标是构建适合数据集的东西。左边的例子欠拟合，因为直线模型无法准确地将圆和正方形分开。右边的模型太复杂了：它正确地分离了所有的圆和正方形，但不太可能在新的数据上工作得很好：
- en: '![](img/6397e42b-9573-4cf2-b611-c61ad5259c07.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6397e42b-9573-4cf2-b611-c61ad5259c07.png)'
- en: 'Our linear model suffered from underfitting: it was too simplistic to model
    the difference between all the classes. Looking at the accuracy of the SVM, we
    can see that it scores 100% on the training data, but only 82% on validation.
    This is a clear sign that it is overfitting: it is much worse at classifying new
    images compared with those on which it was trained.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的线性模型受到了欠拟合的影响：它太简单，无法模拟所有类别的差异。查看SVM的准确率，我们可以看到它在训练数据上得分为100%，但在验证数据上只有82%。这是一个明显的迹象表明它过拟合了：与训练数据相比，它在分类新图像方面表现得更差。
- en: 'One way of dealing with overfitting is to use more training data: even a complex
    model will not be able to overfit if the training dataset is large enough. Another
    way to do this is to introduce regularization: many machine learning models have
    a parameter that you can adjust to reduce overfitting.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 处理过拟合的一种方法是用更多的训练数据：即使是一个复杂的模型，如果训练数据集足够大，也不会过拟合。另一种方法是引入正则化：许多机器学习模型都有一个可以调整的参数，以减少过拟合。
- en: Deep learning
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习
- en: 'So far, we have improved our model''s performance using an SVM, but still face
    two problems:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用支持向量机（SVM）提高了我们模型的性能，但仍然面临两个问题：
- en: Our SVM is overfitting the training data.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的SVM过度拟合了训练数据。
- en: 'It is also difficult to scale to the full dataset of 60,000 images: try training
    the last example with more images and you will find that it gets *much slower*.
    If we double the number of datapoints, the SVM algorithm takes *more than double*
    the amount of time.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 也很难扩展到包含60,000张图像的全数据集：尝试用更多的图像训练最后一个示例，你会发现它变得**慢得多**。如果我们将数据点的数量加倍，SVM算法所需的时间将**超过加倍**。
- en: In this section, we are going to tackle this problem using a **deep neural network**.
    These types of model have been able to achieve state-of-the-art performance on
    image classification tasks, as well many other machine learning problems. They
    are able to model complex non-linear patterns, and also scale well to large datasets.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用**深度神经网络**来解决这个问题。这类模型已经在图像分类任务上实现了最先进的性能，以及许多其他机器学习问题。它们能够模拟复杂的非线性模式，并且在大数据集上扩展良好。
- en: Data scientists will often use Python to develop and train neural networks because
    it has access to extremely well-supported deep learning frameworks such as **TensorFlow**
    and **Keras**. These frameworks make it easier than ever to build complex neural
    networks and train them on large datasets. They are usually the best choice for
    building sophisticated deep learning models. In [Chapter 5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml), *Using
    Pre-Trained Models*, we will look at how to export a trained model from Python
    and then call it from Go for inference. In this section, we will build a much
    simpler neural network from scratch using the `go-deep` library to demonstrate
    the key concepts.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家通常会使用Python来开发和训练神经网络，因为它可以访问如**TensorFlow**和**Keras**这样的深度学习框架，这些框架提供了极好的支持。这些框架使得构建复杂神经网络并在大型数据集上训练它们变得比以往任何时候都更容易。它们通常是构建复杂深度学习模型的最佳选择。在[第5章](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml)，*使用预训练模型*中，我们将探讨如何从Python导出训练好的模型，然后从Go中进行推理。在本节中，我们将使用`go-deep`库从头开始构建一个更简单的神经网络，以演示关键概念。
- en: Neural networks
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'The basic building block of a neural network is a **neuron** (also known as
    a **perceptron**). This is actually just the same as our simple linear model:
    it combines all of its inputs, that is, *x[1],x[2],x[3]... *and so on into a single
    output, *y*, according to the following formula:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的基本构建块是一个**神经元**（也称为**感知器**）。这实际上与我们的简单线性模型相同：它将所有输入结合在一起，即 *x[1],x[2],x[3]...* 等等，根据以下公式生成一个单一的输出，即 *y*：
- en: '![](img/02e2f192-2e94-4abe-8e73-bdabca624535.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/02e2f192-2e94-4abe-8e73-bdabca624535.png)'
- en: 'The magic of neural networks comes from what happens when we combine these
    simple neurons:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的魔力来自于当我们组合这些简单的神经元时会发生什么：
- en: First, we create a **layer** of many neurons into which we feed the input data.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建一个包含许多神经元的**层**，我们将输入数据馈送到这个层中。
- en: At the output of each neuron, we introduce an **activation function**.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个神经元的输出处，我们引入一个**激活函数**。
- en: The output of this **input layer** is then fed to another layer of neurons and
    activations, known as a **hidden layer**.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，这个**输入层**的输出被馈送到另一个包含神经元和激活的层，称为**隐藏层**。
- en: This gets repeated for multiple hidden layers—the more layers there are, the
    **deeper** the network is said to be.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这种过程会重复多次隐藏层——层的数量越多，网络就被说成是越**深**。
- en: A final **output** layer of neurons combines the result of the network into
    the final output.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个最终的**输出层**的神经元将网络的输出结果组合成最终的输出。
- en: Using a technique known as **backpropagation**, we can train the network by
    finding the weights, *w[0],w[1],w[2]...*, for each neural network that allows
    the whole network to fit the training data.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用称为**反向传播**的技术，我们可以通过找到每个神经网络的权重，即 *w[0],w[1],w[2]...*，来训练网络，使整个网络能够适应训练数据。
- en: 'The following diagram shows this layout: the arrows represent the output of
    each neuron, which are feeding into the input of the neurons in the next layer:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图显示了这种布局：箭头代表每个神经元的输出，这些输出被馈送到下一层的神经元的输入中：
- en: '![](img/d68fe958-c58f-4f05-b0d9-e351c81c13da.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d68fe958-c58f-4f05-b0d9-e351c81c13da.png)'
- en: The neurons in this network are said to be arranged **fully-connected** or **dense**
    layers. Recent advances in both computing power and software have allowed researchers
    to build and train more complex neural network architectures than ever before.
    For instance, a state-of-the-art image recognition system might contain millions
    of individual weights, and require many days of computing time to train all of
    these parameters to fit a large dataset. They often contain different arrangements
    of neurons, for instance, in **convolutional layers**, which perform more specialized
    learning in these types of systems.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络中的神经元被称为**全连接**或**密集**层。计算能力和软件的最近进步使得研究人员能够构建和训练比以往任何时候都更复杂的神经网络架构。例如，一个最先进的图像识别系统可能包含数百万个单独的权重，并且需要多天的计算时间来训练所有这些参数以适应大量数据集。它们通常包含不同类型的神经元排列，例如在**卷积层**中，这些层在这些类型的系统中执行更专业的学习。
- en: Much of the skill that is required to use deep learning successfully in practice
    involves a broad understanding of how to select and tune a network to get good
    performance. There are many blogs and online resources that provide more detail
    on how these networks work and the types of problems that they have been applied
    to.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中成功使用深度学习所需的大部分技能涉及对如何选择和调整网络以获得良好性能的广泛理解。有许多博客和在线资源提供了更多关于这些网络如何工作以及它们应用到的各种问题的细节。
- en: A **fully-connected** layer in a neural network is one where the inputs of each
    neuron are connected to the outputs of all the neurons in the previous layer.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的一个**全连接**层是指每个神经元的输入都连接到前一层中所有神经元的输出。
- en: A simple deep learning model architecture
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单的深度学习模型架构
- en: 'Much of the skill in building a successful deep learning model involves choosing
    the correct model architecture: the number/size/type of layers, and the activation
    functions for each neuron. Before starting, it is worth researching to see if
    someone else has already tackled a similar problem to yours using deep learning
    and published an architecture that works well. As always, it is best to start
    with something simple and then modify the network iteratively to improve its performance.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建一个成功的深度学习模型中，大部分的技能在于选择正确的模型架构：层的数量/大小/类型，以及每个神经元的激活函数。在开始之前，值得研究一下是否有人已经使用深度学习解决了与你类似的问题，并发布了一个效果良好的架构。一如既往，最好从简单的东西开始，然后迭代地修改网络以提高其性能。
- en: 'For our example, we will start with the following architecture:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的例子，我们将从以下架构开始：
- en: An input layer
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层
- en: Two hidden layers containing 128 neurons each
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含两个各含128个神经元的隐藏层
- en: An output layer of 10 neurons (one for each output class in the dataset)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含10个神经元的输出层（每个输出类在数据集中都有一个）
- en: Each neuron in the hidden layer will use a **rectified linear unit** (**ReLU**)
    as its output function
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层中的每个神经元将使用**线性整流单元**（**ReLU**）作为其输出函数
- en: ReLUs are a common choice of activation function in neural networks. They are
    a very simple way to introduce non-linearity into a model. Other common activation
    functions include the **logistic** function and the **tanh** function.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ReLUs是神经网络中常用的激活函数。它们是向模型中引入非线性的一种非常简单的方式。其他常见的激活函数包括**对数**函数和**双曲正切**函数。
- en: 'The `go-deep` library lets us build this architecture very quickly:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`go-deep`库让我们能够非常快速地构建这个架构：'
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Neural network training
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络训练
- en: Training a neural network is another area in which you need to make skillful
    adjustments in order to get good results. The training algorithm works by calculating
    how well the model fits a small **batch** of training data (known as the **loss**),
    and then making small adjustments to the weights to improve the fit. This process
    then gets repeated over and over again on different batches of training data.
    The **learning rate** is an important parameter that controls how quickly the
    algorithm will adjust the neuron weights.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络是另一个需要巧妙调整以获得良好结果的地方。训练算法通过计算模型与一小批训练数据（称为**损失**）的拟合程度，然后对权重进行小幅度调整以改善拟合。这个过程在不同的训练数据批次上反复进行。**学习率**是一个重要的参数，它控制算法调整神经元权重速度的快慢。
- en: When training a neural network, the algorithm will feed all of the input data
    into the network repeatedly, and adjust the network weights as it goes. Each full
    pass through the data is known as an **epoch**.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，算法会反复将所有输入数据输入到网络中，并在过程中调整网络权重。每次完整的数据遍历被称为一个**epoch**。
- en: 'When training a neural network, monitor the **accuracy** and **loss** of the
    network after each epoch (accuracy should increase, while loss should decrease).
    If the accuracy is not improving, try lowering the learning rate. Keep training
    the network until accuracy stops improving: at this point, the network is said
    to have **converged**.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，监控每个epoch后网络的**准确率**和**损失**（准确率应该提高，而损失应该降低）。如果准确率没有提高，尝试降低学习率。继续训练网络，直到准确率停止提高：此时，网络被认为是**收敛**了。
- en: 'The following code trains our model using a learning rate of `0.006` for `500`
    iterations and prints out the accuracy after each epoch:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用`0.006`的学习率对模型进行`500`次迭代训练，并在每个epoch后打印出准确率：
- en: '[PRE20]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This neural network provides an accuracy of 80% on both the training and validation
    datasets, a good sign that the model is not overfitting. See if you can improve
    its performance by adjusting the network architecture and retraining. In [Chapter
    5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml), *Using Pre-Trained Models*, we
    will revisit this example by building a more sophisticated neural network in Python
    and then exporting it to Go.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络在训练集和验证集上都提供了80%的准确率，这是一个好迹象，表明模型没有过拟合。看看你是否可以通过调整网络架构和重新训练来提高其性能。在[第5章](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml)，“使用预训练模型”中，我们将通过在Python中构建一个更复杂的神经网络并导出到Go来重新审视这个例子。
- en: Regression
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归
- en: Having mastered many of the key machine learning concepts in the *Classification*
    section, in this section, we will apply what we have learned to a regression problem.
    We will be using a dataset containing information about groups of houses in different
    locations in California^([4]). Our goal will be to predict the median house price
    in each group using input data such as the latitude/longitude location, median
    house size, age, and so on.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握了*分类*部分中的许多关键机器学习概念之后，在本节中，我们将应用所学知识来解决回归问题。我们将使用包含加利福尼亚不同地区房屋群体信息的数据库^([4])。我们的目标将是使用如纬度/经度位置、中位数房屋大小、年龄等输入数据来预测每个群体的中位数房价。
- en: 'Use the `download-housing.sh` script to download the dataset and then load
    it into Go:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`download-housing.sh`脚本下载数据集，然后将其加载到Go中：
- en: '[PRE21]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We need to carry out some data preparation to create columns in the dataframe
    that represent the average number of rooms and bedrooms for houses in each area,
    along with the average occupancy. We will also rescale the median house value
    into units of $100,000:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要进行一些数据准备，在数据框中创建代表每个区域房屋平均房间数和卧室数的列，以及平均入住率。我们还将将中位数房价重新缩放为$100,000为单位：
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Like we did previously, we need to split this data into training and validation
    sets:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所做的那样，我们需要将此数据分为训练集和验证集：
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Linear regression
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: Like the classification example, we are going to start by using a linear model
    as a baseline. This time, though, we are predicting a **continuous output variable**,
    so we need a different performance metric. A common metric to use for regression
    is the **mean squared error** (**MSE**), that is, the sum of the squared differences
    between the model predictions and the true values. By using a *squared* error,
    we are making sure that the value increases for underestimates and overestimates
    are of the true value.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类示例类似，我们将首先使用线性模型作为基线。不过，这次我们预测的是一个**连续输出变量**，因此我们需要一个不同的性能指标。回归中常用的指标是**均方误差**（**MSE**），即模型预测值与真实值之间平方差的和。通过使用*平方*误差，我们确保当低估和超估真实值时，值会增加。
- en: A common alternative to MSE for regression problems is the **mean absolute error**
    (**MAE**). This can be useful when your input data contains outliers.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，MSE（均方误差）的一个常见替代方法是**平均绝对误差**（**MAE**）。当你的输入数据包含异常值时，这可能很有用。
- en: 'Using a Golang regression library, we can train the model as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Golang回归库，我们可以按以下方式训练模型：
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, we can calculate the mean squared error from the validation set as
    `0.51`. This provides a benchmark level of performance that we can refer to when
    comparing other models:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以从验证集中计算出均方误差为`0.51`。这为我们提供了一个基准性能水平，我们可以将其作为比较其他模型的参考：
- en: '[PRE25]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Random forest regression
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林回归
- en: We know that house prices vary according to location, often in complicated ways
    that our linear model is unlikely to be able to capture. Therefore, we are going
    to introduce **random forest regression** as an alternative model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道房价会根据位置的不同而变化，通常以我们线性模型难以捕捉的复杂方式变化。因此，我们将引入**随机森林回归**作为替代模型。
- en: '**Random forest regression** is an example of an **ensemble model**: it works
    by training a large number of simple **base models** and then uses statistical
    averaging to output a final prediction. With random forests, the base models are
    decision trees, and, by adjusting the parameters of these trees and the number
    of models in the ensemble, you can control overfitting.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林回归**是**集成模型**的一个例子：它通过训练大量简单的**基础模型**，然后使用统计平均来输出最终预测。在随机森林中，基础模型是决策树，通过调整这些树和集成中模型的数量参数，你可以控制过拟合。'
- en: 'Using the `RF.go` library, we can train a random forest on the house price
    data. First, let''s do some data preparation on the training and validation sets:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`RF.go`库，我们可以在房价数据上训练一个随机森林。首先，让我们对训练集和验证集进行一些数据准备：
- en: '[PRE26]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, we can fit a random forest containing 25 underlying decision trees:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以拟合一个包含25个底层决策树的随机森林：
- en: '[PRE27]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This gives a much improved MSE of `0.29` on the validation set, but shows signs
    of overfitting with an error of only `0.05` on the training data.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这在验证集上给出了一个大幅改进的MSE为`0.29`，但在训练数据上仅显示`0.05`的错误，表明了过拟合的迹象。
- en: Other regression models
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他回归模型
- en: There are many other regression models you can try out on this dataset. In fact,
    the SVM and deep learning models that we used in the previous example can also
    be adapted for use on regression problems. See if you can improve on the performance
    of the random forest by using a different model. Remember that some of these models
    will require the data to be normalized so that they can be trained properly.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以尝试在这个数据集上使用许多其他回归模型。实际上，我们在前一个示例中使用的SVM和深度学习模型也可以用于回归问题。看看你是否能通过使用不同的模型来提高随机森林的性能。记住，这些模型中的某些将需要数据归一化，以便正确训练。
- en: Summary
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'We have covered a lot of ground in this chapter, and introduced many important
    machine learning concepts. The first step in tackling a supervised learning problem
    is to collect and preprocess the data, making sure that it is normalized, and
    split into training and validation sets. We covered a range of different algorithms
    for both classification and regression. In each example, there were two phases:
    training the algorithm, followed by inference; that is, using the trained model
    to make predictions from new input data. Whenever you try a new machine learning
    technique on your data, it is important to keep track of its performance against
    the training and validation datasets. This serves two main purposes: it helps
    you diagnose underfitting/overfitting and also provides an indication of how well
    your model is working.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了大量的内容，并介绍了许多重要的机器学习概念。解决监督学习问题的第一步是收集和预处理数据，确保数据已归一化，并将其分为训练集和验证集。我们涵盖了用于分类和回归的多种不同算法。在每个示例中，都有两个阶段：训练算法，然后进行推理；也就是说，使用训练好的模型对新输入数据进行预测。每次你在数据上尝试新的机器学习技术时，跟踪其与训练集和验证集的性能对比都是非常重要的。这有两个主要目的：它帮助你诊断欠拟合/过拟合，同时也提供了你模型工作效果的指示。
- en: It is usually best to choose the simplest model that provides good enough performance
    for the task that you are working on. Simple models are usually faster and easier
    to implement and use. In each example, we started with a simple linear model,
    and then evaluated more sophisticated techniques against this baseline.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，选择一个足够简单但能提供良好性能的模型是最佳选择。简单模型通常运行更快，更容易实现和使用。在每个示例中，我们从一个简单的线性模型开始，然后评估更复杂的技术与这个基线。
- en: There are many different implementations of machine learning models for Go that
    are available online. As we have done in this chapter, it is usually quicker to
    find and use an existing library rather than implementing an algorithm completely
    from scratch. Often, these libraries have slightly different requirements in terms
    of data preparation and tuning parameters, so be sure to read the documentation
    carefully in each case.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在线有许多针对围棋的机器学习模型的不同实现。正如我们在本章中所做的，通常更快的是找到并使用现有的库，而不是从头开始完全实现算法。通常，这些库在数据准备和调整参数方面有略微不同的要求，所以请务必仔细阅读每个案例的文档。
- en: The next chapter will reuse many of the techniques for data loading and preparation
    that we have implemented here, but, instead, will focus on unsupervised machine
    learning.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将重用我们在本章中实现的数据加载和准备技术，但将专注于无监督机器学习。
- en: Further readings
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/). Retrieved
    March 24, 2019.'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/). 获取日期：2019年3月24日。'
- en: '[https://blogs.nvidia.com/blog/2016/05/06/self-driving-cars-3/](https://blogs.nvidia.com/blog/2016/05/06/self-driving-cars-3/).
    Retrieved March 24, 2019.'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://blogs.nvidia.com/blog/2016/05/06/self-driving-cars-3/](https://blogs.nvidia.com/blog/2016/05/06/self-driving-cars-3/).
    获取日期：2019年3月24日。'
- en: '[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist). Retrieved
    March 24, 2019.'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist).
    获取日期：2019年3月24日。'
- en: '[http://colah.github.io/](http://colah.github.io/). Retrieved May 15, 2019.'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://colah.github.io/](http://colah.github.io/). 获取日期：2019年5月15日。'
- en: '[https://karpathy.github.io/](https://karpathy.github.io/). Retrieved May 15,
    2019.'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://karpathy.github.io/](https://karpathy.github.io/). 获取日期：2019年5月15日。'
- en: '[http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html](http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html). Retrieved
    March 24,  2019.'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html](http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html).
    获取日期：2019年3月24日。'
