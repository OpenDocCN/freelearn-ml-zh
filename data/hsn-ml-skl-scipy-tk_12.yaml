- en: Imbalanced Learning – Not Even 1% Win the Lottery
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡学习 - 连 1% 的人都未能中彩票
- en: Cases where your classes are neatly balanced are more of an exception than the
    rule. In most of the interesting problems we'll come across, the classes are extremely
    imbalanced. Luckily, a small fraction of online payments are fraudulent, just
    like a small fraction of the population catch rare diseases. Conversely, few contestants
    win the lottery and fewer of your acquaintances become your close friends. That's
    why we are usually interested in capturing those rare cases.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的类别平衡的情况下，更多的是例外而不是规则。在我们将遇到的大多数有趣的问题中，类别极不平衡。幸运的是，网络支付的一小部分是欺诈的，就像人口中少数人感染罕见疾病一样。相反，少数竞争者中彩票，你的熟人中少数成为你的密友。这就是为什么我们通常对捕捉这些罕见案例感兴趣的原因。
- en: In this chapter, we will learn how to deal with imbalanced classes. We will
    start by giving different weights to our training samples to mitigate the class
    imbalance problem. Afterward, we will learn about other techniques, such as undersampling
    and oversampling. We will see the effect of these techniques in practice. We will
    also learn how to combine concepts such as ensemble learning with resampling,
    and also introduce new scores to validate if our learners are meeting our needs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何处理不平衡的类别。我们将从给训练样本分配不同权重开始，以减轻类别不平衡问题。此后，我们将学习其他技术，如欠采样和过采样。我们将看到这些技术在实践中的效果。我们还将学习如何将集成学习与重采样等概念结合起来，并引入新的评分来验证我们的学习器是否符合我们的需求。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Reweighting the training samples
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新加权训练样本
- en: Random oversampling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机过采样
- en: Random undersampling
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机欠采样
- en: Combing sampling with ensembles
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将采样与集成结合使用
- en: Equal opportunity score
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平等机会分数
- en: Let's get started!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Getting the click prediction dataset
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取点击预测数据集
- en: Usually, a small percentage of people who see an advertisement click on it.
    In other words, the percentage of samples in a positive class in such an instance
    can be just 1% or even less. This makes it hard to predict the **click-through
    rate** (**CTR**) since the training data is highly imbalanced. In this section,
    we are going to use a highly imbalanced dataset from the **Knowledge Discovery
    in Databases** (**KDD**) Cup.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，看到广告并点击的人只占很小一部分。换句话说，在这种情况下，正类样本的百分比可能只有1%甚至更少。这使得预测**点击率**（**CTR**）变得困难，因为训练数据极度不平衡。在本节中，我们将使用来自**知识发现数据库**（**KDD**）杯赛的高度不平衡数据集。
- en: The KDD Cup is an annual competition organized by the ACM Special Interest Group
    on Knowledge Discovery and Data Mining. In 2012, they released a dataset for the
    advertisements shown alongside the search results in a search engine. The aim
    of the competitors was to predict whether a user will click on each ad or not.
    A modified version of the data has been published on the OpenML platform ([https://www.openml.org/d/1220](https://www.openml.org/d/1220)).
    The CTR in the modified dataset is 16.8%. This is our positive class. We can also
    call it the minority class since the majority of the cases did not lead to an
    ad being clicked on.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: KDD杯是由ACM知识发现与数据挖掘特别兴趣小组每年组织的比赛。2012年，他们发布了一个数据集，用于预测搜索引擎中显示的广告是否会被用户点击。经修改后的数据已发布在OpenML平台上（[https://www.openml.org/d/1220](https://www.openml.org/d/1220)）。修改后数据集的CTR为16.8%。这是我们的正类。我们也可以称之为少数类，因为大多数情况下广告未被点击。
- en: 'Here, we are going to download the data and put it into a DataFrame, as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将下载数据并将其放入DataFrame中，如下所示：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can display `5` random rows of the dataset using the following line of code:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码显示数据集的`5`个随机行：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can make sure we get the same random lines if we both set `random_state`
    to the same value. In *The Hitchhiker''s Guide to the Galaxy* by *Douglas Adams*,
    the number `42` was deemed as the answer to the ultimate question of life, the
    universe, and everything. So, we will stick to setting `random_state`to`42`throughout
    this chapter. Here is our five-line sample:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将`random_state`设为相同的值，我们可以确保得到相同的随机行。在《银河系漫游指南》中，道格拉斯·亚当斯认为数字`42`是生命、宇宙和一切的终极问题的答案。因此，我们将在本章节中始终将`random_state`设为`42`。这是我们的五行样本：
- en: '![](img/af9bb2e9-7f3c-4ff5-a9b7-72034f38b679.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af9bb2e9-7f3c-4ff5-a9b7-72034f38b679.png)'
- en: 'There are two things we need to keep in mind about this data:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些数据，我们需要记住两件事：
- en: The classes are imbalanced, as mentioned earlier. You can check this by running
    `df['target'].mean()`, which will give you `16.8%`.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前所述，类别不平衡。你可以通过运行`df['target'].mean()`来检查这一点，这将返回`16.8%`。
- en: 'Despite the fact that all the features are numerical, it is clear that all
    the features ending with the `id` suffix are supposed to be treated as categorical
    features. For example, the relationship between `ad_id` and the CTR is not expected
    to be linear, and thus when using a linear model, we may need to encode these
    features using a *one-hot encoder*. Nevertheless, due to their high cardinality,
    a one-hot encoding strategy will result in too many features for our classifier
    to deal with. Therefore, we need to come up with another scalable solution. For
    now, let''s learn how to check the cardinality of each feature:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管所有特征都是数值型的，但显然，所有以`id`后缀结尾的特征应该作为分类特征来处理。例如，`ad_id`与CTR之间的关系并不预期是线性的，因此在使用线性模型时，我们可能需要使用*one-hot编码器*对这些特征进行编码。然而，由于这些特征具有高基数，one-hot编码策略会导致产生过多的特征，使我们的分类器难以处理。因此，我们需要想出另一种可扩展的解决方案。现在，让我们学习如何检查每个特征的基数：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will give us the following results:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下结果：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we will convert our data into `x_train`, `x_test`, `y_train`, and
    `y_test` sets, as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将把数据转换成`x_train`、`x_test`、`y_train`和`y_test`数据集，如下所示：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this section, we downloaded the necessary data and added it to a DataFrame.
    In the next section, we will install the `imbalanced-learn` library.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们下载了必要的数据并将其添加到DataFrame中。在下一节中，我们将安装`imbalanced-learn`库。
- en: Installing the imbalanced-learn library
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装imbalanced-learn库
- en: Due to class imbalance, we will need to resample our training data or apply
    different techniques to get better classification results. Thus, we are going
    to rely on the`imbalanced-learn`library here. The project was started in 2014
    by *Fernando Nogueira*. It now offers multiple resampling data techniques, as
    well as metrics for evaluating imbalanced classification problems. The library's
    interface is compatible with scikit-learn.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于类别不平衡，我们需要重新采样训练数据或应用不同的技术以获得更好的分类结果。因此，我们将在这里依赖`imbalanced-learn`库。该项目由*Fernando
    Nogueira*于2014年启动，现提供多种重采样数据技术，以及用于评估不平衡分类问题的度量标准。该库的接口与scikit-learn兼容。
- en: 'You can download the library via `pip` by running the following command in
    your Terminal:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在终端中运行以下命令来使用`pip`下载该库：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, you can import and use its different modules in your code, as we will see
    in the following sections. One of the metrics provided by the library is the **g****eometric
    mean score**. In[Chapter 8](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=30&action=edit)*,
    Ensembles – When One Model is Not Enough*, we learned about the **true positive
    rate****(**TPR**),**or sensitivity, and the **false positive rate** (**FPR**),
    and we used them to draw the area under the curve. We also learned about the**true
    negative rate** (**TNR**), or specificity, which is basically 1 minus the FPR.
    The geometric mean score, for binary classification problems, is the square root
    of the product of the sensitivity (TPR) and specificity (TNR). By combining these
    two metrics, we try to maximize the accuracy of each of the classes while taking
    their imbalances into account. The interface for`geometric_mean_score`is similar
    to the other scikit-learn metrics. It takes the true and predicted values and
    returns the calculated score, as follows:****
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以在代码中导入并使用它的不同模块，正如我们在接下来的章节中所看到的。该库提供的度量标准之一是**几何均值分数**。在[第8章](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=30&action=edit)*，集成方法
    – 当一个模型不足够时*，我们了解了**真正例率**（**TPR**），即灵敏度，以及**假正例率**（**FPR**），并用它们绘制了曲线下面积。我们还学习了**真负例率**（**TNR**），即特异度，它基本上是1减去FPR。几何均值分数，对于二分类问题来说，是灵敏度（TPR）和特异度（TNR）乘积的平方根。通过结合这两个度量标准，我们试图在考虑类别不平衡的情况下，最大化每个类别的准确性。`geometric_mean_score`的接口与其他scikit-learn度量标准类似。它接受真实值和预测值，并返回计算出的分数，如下所示：****
- en: '****[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '****[PRE6]'
- en: We will be using this metric in addition to the precision and recall scores
    throughout this chapter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用这个度量标准，除了精确度和召回率分数外。
- en: In the next section, we are going to alter the weights of our training samples
    and see if this helps us deal with our imbalanced classes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将调整训练样本的权重，看看这是否有助于处理类别不平衡问题。
- en: Predicting the CTR
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测CTR
- en: 'We have our data and installed the `imbalanced-learn` library. Now, we are
    ready to build our classifier. As we mentioned earlier, the one-hot encoding techniques
    we are familiar with will not scale well with the high cardinality of our categorical
    features. In [Chapter 8](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=30&action=edit),
    *Ensembles – When One Model is Not Enough*, we briefly mentioned**random trees
    embedding** as a technique for transforming our features. It is an ensemble of
    totally random trees, where each sample of our data will be represented according
    to the leaves of each tree it ends upon. Here, we are going to build a pipeline
    where the data will be transformed into a random trees embedding and scaled. Finally,
    a **logistic regression** classifier will be used to predict whether a click has
    occurred or not:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好了数据并安装了`imbalanced-learn`库。现在，我们可以开始构建我们的分类器了。正如我们之前提到的，由于类别特征的高基数，传统的独热编码技术并不适合大规模应用。在[第8章](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=30&action=edit)，《集成方法——当一个模型不足以应对时》，我们简要介绍了**随机树嵌入**作为一种特征转换技术。它是完全随机树的集成，每个数据样本将根据它落在每棵树的叶子节点来表示。这里，我们将构建一个管道，将数据转换为随机树嵌入并进行缩放。最后，使用**逻辑回归**分类器来预测是否发生了点击：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We wrapped the whole process into a function so that we can reuse it later in
    this chapter. The `predict_and_evalutate()` function takes the x's and the y's,
    as well as the sample weights. We are going to use the sample weights in a moment,
    but you can ignore them for now. Once you're done predicting, the function will
    also print the different scores and return an instance of the pipeline that was
    used.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将整个过程封装到一个函数中，这样我们就可以在本章后面重复使用它。`predict_and_evalutate()`函数接受x和y，以及样本权重。我们稍后会使用样本权重，但现在可以忽略它们。一旦预测完成，函数还会打印不同的得分，并返回使用的管道实例。
- en: 'We can use the function we have just created as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像下面这样使用我们刚刚创建的函数：
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: By default, the precision and recall that are calculated are for the positive
    class. The previous code gave us a recall of `0.3%`, a precision of `62.5%`, and
    a geometric mean score of `5.45%`. The recall is less than `1%`, which means that
    the classifier won't be able to capture the vast majority of the positive/minority
    class. This is an expected scenario when dealing with imbalanced data. One way
    to fix this is to give more weights to the samples in the minority class. This
    is like asking the classifier to give more attention to these samples since we
    care about capturing them, despite their rareness. In the next section, we are
    going to see the effect of sample weighting on our classifier.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，计算出的精确度和召回率是针对正类的。前面的代码给出了`0.3%`的召回率，`62.5%`的精确度，以及`5.45%`的几何均值得分。召回率低于`1%`，这意味着分类器将无法捕捉到绝大多数正类/少数类样本。这是处理不平衡数据时常见的情况。解决方法之一是为少数类样本分配更多的权重。这就像是要求分类器更加关注这些样本，因为我们关心的是捕捉到它们，尽管它们相对稀少。在下一节中，我们将看到样本加权对分类器的影响。
- en: Weighting the training samples differently
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对训练样本进行不同的加权
- en: 'The number of samples in the majority class is about five times those in the
    minority class. You can double-check this by running the following line of code:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 多数类样本的数量大约是少数类样本的五倍。你可以通过运行以下代码来验证这一点：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Thus, it makes sense to give the samples in the minority class five times the
    weight of the other samples. We can use the same `predict_and_evalutate()` function
    from the previous section and change the sample weights, as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将少数类样本的权重设置为其他样本的五倍是有意义的。我们可以使用前一节中的`predict_and_evalutate()`函数，并调整样本权重，具体如下：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, the recall jumps to `13.4%` at the expense of the precision, which went
    down to `24.8%`. The geometric mean score went down from`5.5%` to `34%`, thanks
    to the new weights.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，召回率跳升到`13.4%`，但精确度下降至`24.8%`。几何均值得分从`5.5%`降至`34%`，这得益于新的权重设置。
- en: 'The `predict_and_evalutate()` function returns an instance of the pipeline
    that was used. We can get the last component of the pipeline, the logistic regression
    classifier, via`clf[-1]`. Then, we can access the coefficients of the classifier
    that were assigned to each feature as we intercept it. Due to the embedding step,
    we may end up with up to 200 features; 10 estimators x up to 20 leaf nodes. The
    following function prints the last nine features, as well as the intercept, along
    with their coefficients:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict_and_evalutate()`函数返回使用的管道实例。我们可以通过`clf[-1]`获取管道的最后一个组件，即逻辑回归分类器。然后，我们可以访问分配给每个特征的分类器系数。在嵌入步骤中，我们可能最终会得到多达200个特征；10个估算器×最多20个叶节点。以下函数打印出最后九个特征及其系数以及截距：'
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output of `calculate_feature_coeff(clf).round(2)` can also be rounded to
    two decimal points so that it looks as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`calculate_feature_coeff(clf).round(2)`的输出也可以四舍五入到两位小数，如下所示：'
- en: '![](img/11752e5a-6f7a-4f3e-acc6-68b93b6678b5.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/11752e5a-6f7a-4f3e-acc6-68b93b6678b5.png)'
- en: 'Now, let''s compare three weighting strategies side by side. With a weight
    of one, both the minority and the majority classes get the same weights. Then,
    we give the minority class double the weight of the majority class, as well as
    five times its weight, as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们并排比较三种加权策略。权重为1时，少数类和多数类的权重相同。然后，我们将少数类的权重设置为多数类的两倍，再设置为五倍，如下所示：
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This gives us the following results:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了以下结果：
- en: '![](img/77a78a51-3757-49c9-bb16-f13565334100.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77a78a51-3757-49c9-bb16-f13565334100.png)'
- en: 'It is easy to see how the weighting affects the precision and the recall. It
    is as if one of them always improves at the expense of the other. This behavior
    is the result of moving the classifier''s boundaries. As we know, the class boundaries
    are defined by the coefficients of the different features, as well as the intercept.
    I bet you are tempted to see the coefficients of the three previous models side
    by side. Luckily, we have saved the coefficients in`df_coef_list` so that we can
    display them using the following code snippet:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看到加权如何影响精度和召回率。就好像其中一个总是在牺牲另一个的情况下改进。这种行为是由于移动了分类器的边界。如我们所知，类别边界是由不同特征的系数以及截距定义的。我敢打赌你很想看到这三种先前模型的系数并排显示。幸运的是，我们已经将系数保存在`df_coef_list`中，以便我们可以使用以下代码片段显示它们：
- en: '[PRE13]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This gives us the following visual comparison between the three classifiers:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了三种分类器之间的以下视觉比较：
- en: '![](img/c341204c-5d34-4d3a-ac90-b307d4d5aa21.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c341204c-5d34-4d3a-ac90-b307d4d5aa21.png)'
- en: The coefficients of the features did change slightly, but the changes in the
    intercept are more noticeable. In summary, the weighting affects the intercept
    the most and moves the class boundaries as a result.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 特征的系数确实发生了轻微变化，但截距的变化更为显著。总之，加权最影响截距，并因此移动了类别边界。
- en: A sample is classified as a member of the positive class if the predicted probability
    is above `50%`. The movement of the intercept, without any changes in the other
    coefficients, is equivalent to changing the probability threshold so that it's
    above or below that `50%`. If the weighting only affected the intercept, we might
    suggest that we should try different probability thresholds until we get the desired
    precision-recall tradeoff. To check whether the weighting offered any additional
    benefit on top of altering the intercept, we have to check the area under the
    **Receiver Operating Characteristic** (**ROC**) curve.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果预测的概率超过`50%`，则将样本分类为正类成员。截距的变化（在其他系数不变的情况下）相当于改变概率阈值，使其高于或低于`50%`。如果加权只影响截距，我们可能会建议尝试不同的概率阈值，直到获得期望的精度-召回平衡。为了检查加权是否提供了超出仅改变截距的额外好处，我们必须检查**接收者操作特征**（**ROC**）曲线下面积。
- en: The effect of the weighting on the ROC
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加权对ROC的影响
- en: 'Did the weighting improve the area under the ROC curve? To answer this question,
    let''s start by creating a function that will display the ROC curve and print
    the **area under the curve** (**AUC**):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 加权是否改善了ROC曲线下面积？为了回答这个问题，让我们从创建一个显示ROC曲线并打印**曲线下面积**（**AUC**）的函数开始：
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we can loop over the three weighting options and render their corresponding
    curves, as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以循环遍历三种加权选项，并渲染它们相应的曲线，如下所示：
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'These three curves are displayed here:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这三条曲线在这里展示：
- en: '![](img/dff06623-8599-4f34-89d4-a0fcbeadf72c.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dff06623-8599-4f34-89d4-a0fcbeadf72c.png)'
- en: The ROC curve is meant to show the tradeoff between the TPR and the FPR for
    the different probability thresholds. If the area under the ROC curve is more
    or less than the same for the three weighting strategies, then the weighting did
    not offer much value beyond altering the classifier's intercept. Thus, it is up
    to us if we want to increase the recall at the expense of the precision to either
    reweight our training samples or to try different probability thresholds for our
    classification decision.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线旨在显示不同概率阈值下的真正率（TPR）与假正率（FPR）之间的权衡。如果ROC曲线下的面积对于三种加权策略大致相同，那么加权除了改变分类器的截距外，并没有提供太多价值。因此，是否要以牺牲精度为代价来提高召回率，就取决于我们是否想重新加权训练样本，或者尝试不同的分类决策概率阈值。
- en: In addition to the sample weighting, we can resample the training data so that
    we train on a more balanced set. In the next section, we are going to see the
    different sampling techniques offered by the `imbalanced-learn` library.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 除了样本加权之外，我们还可以重新采样训练数据，以便在一个更加平衡的数据集上进行训练。在下一节中，我们将看到`imbalanced-learn`库提供的不同采样技术。
- en: Sampling the training data
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练数据的采样
- en: '"It''s not denial. I''m just selective about the reality I accept."'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '"这不是否认。我只是对我接受的现实有选择性。"'
- en: '- Bill Watterson'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '- 比尔·沃特森'
- en: If the machine learning models were humans, they would have believed that the
    end justifies the means. When 99% of their training data belongs to one class,
    and their aim is to optimize their objective function, we cannot blame them if
    they focus on getting that single class right since it contributes to 99% of the
    solution. In the previous section, we tried to change this behavior by giving
    more weights to the minority class, or classes. Another strategy might entail
    removing some samples from the majority class or adding new samples to the minority
    class until the two classes are balanced.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果机器学习模型是人类，它们可能会认为目的证明手段是合理的。当99%的训练数据属于同一类时，它们的目标是优化目标函数。如果它们专注于正确处理那一类，也不能怪它们，因为它为解决方案贡献了99%的数据。在上一节中，我们通过给少数类或多类更多的权重来尝试改变这种行为。另一种策略可能是从多数类中移除一些样本，或向少数类中添加新样本，直到两个类达到平衡。
- en: Undersampling the majority class
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对多数类进行下采样
- en: '"Truth, like gold, is to be obtained not by its growth, but by washing away
    from it all that is not gold."'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '"真理，就像黄金，不是通过它的增长得到的，而是通过洗净其中所有非黄金的部分来获得的。"'
- en: '- Leo Tolstoy'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '- 列夫·托尔斯泰'
- en: 'We can randomly remove samples from the majority class until it becomes the
    same size as the minority class. When dealing with non-binary classification tasks,
    we can remove samples from all the classes until they all become the same size
    as the minority class. This technique is known as **Random Undersampling**. The
    following code shows how `RandomUnderSampler()` can be used to downsample the
    majority class:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以随机移除多数类的样本，直到它与少数类的大小相同。在处理非二分类任务时，我们可以从所有类别中移除样本，直到它们都变成与少数类相同的大小。这个技术被称为**随机下采样**。以下代码展示了如何使用`RandomUnderSampler()`来对多数类进行下采样：
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Rather than keeping the classes balanced, you can just reduce their imbalance
    by setting the`sampling_strategy`hyperparameter. Its value dictates the final
    ratio of the minority class versus the majority class. In the following example,
    we kept the final size of the majority class so that it''s twice that of the minority
    class:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 与其保持类别平衡，你可以通过设置`sampling_strategy`超参数来减少类别的不平衡。其值决定了少数类与多数类的最终比例。在下面的示例中，我们保持了多数类的最终大小，使其是少数类的两倍：
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The downsampling process doesn''t have to be random. For example, we can use
    the nearest neighbors algorithm to remove the samples that do not agree with their
    neighbors. The`EditedNearestNeighbours`module allows you to set the number of
    neighbors to check via its `n_neighbors`hyperparameter, as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下采样过程不一定是随机的。例如，我们可以使用最近邻算法来移除那些与邻居不一致的样本。`EditedNearestNeighbours`模块允许你通过其`n_neighbors`超参数来设置检查邻居的数量，代码如下：
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The previous techniques belong to what is known as **prototype selection**.
    In this situation, we select samples from already existing ones. In contrast to
    Prototype Selection, the **prototype generation** approach generates new samples
    to summarize the existing ones. The *ClusterCentroids* algorithm puts the majority
    class samples into clusters and uses the cluster centroids instead of the original
    samples. More on clustering and cluster centroids will be provided in[Chapter
    11](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=34&action=edit)*,
    Clustering – Making Sense of Unlabeled Data*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的技术属于**原型选择**。在这种情况下，我们从已经存在的样本中选择样本。与原型选择不同，**原型生成**方法生成新的样本来概括现有样本。*ClusterCentroids*算法将多数类样本放入聚类中，并使用聚类中心代替原始样本。有关聚类和聚类中心的更多内容，将在[第11章](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=34&action=edit)*《聚类——理解无标签数据》*中提供。
- en: 'To compare the aforementioned algorithms, let''s create a function that takes
    the x''s and y''s, in addition to the sampler instance, and then trains them and
    returns the predicted values for the test set:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较前述算法，让我们创建一个函数，该函数接收x和y以及采样器实例，然后训练它们并返回测试集的预测值：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we can use the `sample_and_predict()` function we have just created and
    plot the resulting ROC curve for the following two sampling techniques:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用刚刚创建的`sample_and_predict()`函数，并为以下两种采样技术绘制结果的ROC曲线：
- en: '[PRE20]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The resulting ROC curve will look as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的ROC曲线将如下所示：
- en: '![](img/c08eccba-8880-4140-8e4a-1b22976d5968.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c08eccba-8880-4140-8e4a-1b22976d5968.png)'
- en: Here, we can see the value of the sampling techniques on the resulting area
    under the ROC curve in comparison to training on the original unsampled set. The
    three graphs may be too close for us to tell them apart, as is the case here,
    so it makes sense to check the resulting AUC number instead.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到与训练原始未采样数据集相比，采样技术对ROC曲线下面积的影响。三张图可能太过接近，导致我们难以区分它们，就像这里一样，因此检查最终的AUC值更为有意义。
- en: Oversampling the minority class
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对少数类进行过采样
- en: Besides undersampling, we can also increase the data points of the minority
    class. `RandomOverSampler` naively clones random samples of the minority class
    until it becomes the same size as the majority class. `SMOTE` and `ADASYN`, on
    the other hand, generate new synthetic samples by interpolation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 除了欠采样，我们还可以增加少数类的数据点。`RandomOverSampler`简单地复制少数类的随机样本，直到它的大小与多数类相同。而`SMOTE`和`ADASYN`则通过插值生成新的合成样本。
- en: 'Here, we are comparing `RandomOverSampler` to the`SMOTE` oversampling algorithm:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将`RandomOverSampler`与`SMOTE`过采样算法进行比较：
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The resulting ROC curve helps us compare the performance of the two techniques
    being used on the dataset at hand:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的ROC曲线帮助我们比较当前数据集上两种技术的性能：
- en: '![](img/b2af8b59-d845-4ad6-b36e-4940766ef48d.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2af8b59-d845-4ad6-b36e-4940766ef48d.png)'
- en: As we can see, the `SMOTE`algorithm did not perform on our current dataset,
    while`RandomOverSampler`pushed the curve upward. So far, the classifiers we've
    used have been agnostic to the sampling techniques we've applied. We can simply
    remove the logistic regression classifier and plug in any other classifier here
    without changing the data sampling code. In contrast to the algorithms we've used,
    the data sampling process is an integral part of some ensemble algorithms. In
    the next section, we'll learn how to make use of this fact to get the best of
    both worlds.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，`SMOTE`算法在当前数据集上的表现不好，而`RandomOverSampler`则使曲线向上移动。到目前为止，我们使用的分类器对于我们应用的采样技术是无关的。我们可以简单地移除逻辑回归分类器，并在不更改数据采样代码的情况下插入任何其他分类器。与我们使用的算法不同，数据采样过程是一些集成算法的核心组成部分。在下一节中，我们将学习如何利用这一点，做到两者兼得。
- en: Combining data sampling with ensembles
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据采样与集成方法结合
- en: In [Chapter 8](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=30&action=edit),
    *Ensembles – When One Model is Not Enough,* we learned about bagging algorithms.
    They basically allow multiple estimators to learn from different subsets of the
    dataset, in the hope that these diverse training subsets will allow the different
    estimators to come to a better decision when combined. Now that we've undersampled
    the majority class to keep our training data balanced, it is natural that we combine
    the two ideas together; that is, the bagging and the under-sampling techniques.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=30&action=edit)中，*集成方法——当一个模型不足以解决问题时*，我们学习了包外算法。它们基本上允许多个估计器从数据集的不同子集进行学习，期望这些多样化的训练子集能够帮助不同的估计器在结合时做出更好的决策。现在我们已经对多数类进行了欠采样，以保持训练数据的平衡，结合这两个思路是很自然的；也就是说，结合包外和欠采样技术。
- en: '`BalancedBaggingClassifier` builds several estimators on different randomly
    selected subsets of data, where the classes are balanced during the sampling process.
    Similarly,`BalancedRandomForestClassifier`builds its trees on balanced samples.
    In the following code, we''re plotting the ROC curves for the two ensembles:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`BalancedBaggingClassifier`在不同的随机选择的数据子集上构建多个估计器，在采样过程中，类别是平衡的。同样，`BalancedRandomForestClassifier`在平衡样本上构建其决策树。以下代码绘制了这两个集成方法的ROC曲线：'
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Some formatting lines have been omitted for brevity. Running the previous code
    gives us the following graph:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 出于简洁考虑，某些格式化行已被省略。运行前面的代码会给我们以下图表：
- en: '![](img/c0e7e068-c2ee-49c2-b736-f2aeaa10a8f7.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0e7e068-c2ee-49c2-b736-f2aeaa10a8f7.png)'
- en: From this, it's clear that the combination of undersampling and ensembles achieved
    better results than our earlier models.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一点可以看出，欠采样和集成方法的结合比我们之前的模型取得了更好的效果。
- en: In addition to the bagging algorithms, `RUSBoostClassifier`combines the random
    undersampling technique with the `adaBoost`classifier.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 除了包外算法，`RUSBoostClassifier`将随机欠采样技术与`adaBoost`分类器相结合。
- en: Equal opportunity score
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平等机会得分
- en: So far, we've only focused on the imbalances in the class labels. In some situations,
    the imbalance in a particular feature may also be problematic. Say, historically,
    that the vast majority of the engineers in your company were men. Now, if you
    build an algorithm to filter the new applicants based on your existing data, would
    it discriminate against the female candidates?
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只关注了类别标签的不平衡。在某些情况下，某个特征的不平衡也可能是一个问题。假设历史上，公司大多数工程师是男性。如果现在你基于现有数据构建一个算法来筛选新申请人，那么这个算法是否会对女性候选人产生歧视？
- en: 'The **equal opportunity score** tries to evaluate how dependent a model is
    of a certain feature. Simply put, a model is considered to give an equal opportunity
    to the different value of a certain feature if the relationship between the model''s
    predictions and the actual targets is the same, regardless of the value of this
    feature. Formally, this means that the conditional probability of the predicted
    target, which is conditional on the actual target, and the applicant''s gender
    should be the same, regardless of gender. These conditional probabilities are
    shown in the following equation:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**平等机会得分**试图评估模型在某个特征上的依赖程度。简单来说，如果模型的预测和实际目标之间的关系无论该特征的值如何都相同，则认为模型对该特征的不同值给予了平等的机会。形式上，这意味着在实际目标和申请人性别条件下，预测目标的条件概率应该是相同的，无论性别如何。以下方程显示了这些条件概率：'
- en: '![](img/800db37b-c415-4c0e-90db-8bb5858ee163.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/800db37b-c415-4c0e-90db-8bb5858ee163.png)'
- en: 'The previous equation only gives a binary outcome. Therefore, we can turn it
    into a ratio where we have a value between 0 and 1\. Since we do not know which
    gender gets a better opportunity, we take the minimum value of the two possible
    fractions using the following equation:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的方程只给出了二元结果。因此，我们可以将其转化为一个比率，值在0和1之间。由于我们不知道哪个性别获得更好的机会，我们使用以下方程取两种可能分数中的最小值：
- en: '![](img/692b5a92-ae04-4609-ba2c-3b6a99f50088.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/692b5a92-ae04-4609-ba2c-3b6a99f50088.png)'
- en: 'To demonstrate this metric, let''s assume we have a model trained on the applicant''s
    `IQ` and `Gender`. This following code shows its predictions on the test set,
    where both the true label and the predictions are listed side by side:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这一指标，假设我们有一个基于申请人的`IQ`和`Gender`训练的模型。以下代码展示了该模型在测试集上的预测结果，其中真实标签和预测值并排列出：
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we can create a function to calculate the equal opportunity score for
    us, as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个函数来计算等机会得分，代码如下：
- en: '[PRE24]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'When called with our `df_engineers` DataFrame, it will give us `0.5`. Having
    a value that''s less than one tells us that the female applicants have less of
    an opportunity to get hired by our model:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用我们的`df_engineers`数据框时，它将给我们`0.5`。一个小于1的值告诉我们，女性申请者在我们的模型中获得聘用的机会较少：
- en: '[PRE25]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Obviously, we can exclude the gender feature from this model altogether, yet
    this score is still useful if there are any remaining features that depend on
    the applicant's gender. Additionally, we need to alter this score when dealing
    with a non-binary classifier and/or a non-binary feature. You can read about this
    score in more detail in the original paper by *Moritz Hardt**et al*.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们可以完全从模型中排除性别特征，但如果有任何剩余的特征依赖于申请者的性别，那么这个得分仍然很有用。此外，在处理非二元分类器和/或非二元特征时，我们需要调整这个得分。你可以在*Moritz
    Hardt*等人的原始论文中更详细地阅读有关此得分的内容。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to deal with class imbalances. This is a recurrent
    problem in machine learning, where most of the value lies in the minority class.
    This phenomenon is common enough that the *black swan* metaphor was coined to
    explain it. When the machine learning algorithms try to blindly optimize their
    out-of-the-box objective functions, they usually miss those black swans. Hence,
    we have to use techniques such as sample weighting, sample removal, and sample
    generation to force the algorithms to meet our own objectives.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了如何处理类别不平衡问题。这是机器学习中的一个常见问题，其中大部分价值都集中在少数类中。这种现象足够常见，以至于*黑天鹅*隐喻被用来解释它。当机器学习算法试图盲目地优化其开箱即用的目标函数时，它们通常会忽略这些黑天鹅。因此，我们必须使用诸如样本加权、样本删除和样本生成等技术，迫使算法实现我们的目标。
- en: This was the last chapter in this book about supervised learning algorithms.
    There is a rough estimate that 80% of the machine learning problems in business
    setups and academia are supervised learning ones, which is why about 80% of this
    book focused on that paradigm. From the next chapter onward, we will start covering
    the other machine learning paradigms, which is where about 20% of the real-life
    value resides. We will start by looking at clustering algorithms, and then move
    on and look at other problems where the data is also unlabeled.****
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本书关于监督学习算法的最后一章。有一个粗略估计，大约80%的商业和学术环境中的机器学习问题是监督学习问题，这也是本书约80%的内容聚焦于这一范式的原因。从下一章开始，我们将开始介绍其他机器学习范式，这是现实生活中大约20%价值所在。我们将从聚类算法开始，然后继续探讨其他数据也是未标记的情况下的问题。****
