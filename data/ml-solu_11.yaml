- en: Chapter 11. Building Gaming Bot
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章：构建游戏机器人
- en: In previous chapters, we covered applications that belong to the computer vision
    domain. In this chapter, we will be making a gaming bot. We will cover different
    approaches to build the gaming bot. These gaming bots can be used to play a variety
    of Atari games.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们介绍了属于计算机视觉领域的一些应用。在本章中，我们将制作一个游戏机器人。我们将介绍构建游戏机器人的不同方法。这些游戏机器人可以用来玩各种Atari游戏。
- en: 'Let''s do a quick recap of the past two years. Let''s begin with 2015\. A small
    London-based company called DeepMind published a research paper titled Playing
    Atari with Deep Reinforcement Learning, available at [https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)
    In this paper, they demonstrated how a computer can learn and play Atari 2600
    video games. A computer can play the game just by observing the screen pixels.
    Our computer game agent (the computer game player) will receive rewards when the
    game score increases. The result presented in this paper is remarkable. The paper
    created a lot of buzz, and that was because each game has different scoring mechanisms
    and these games are designed in such a way that humans find it difficult to achieve
    the highest score. The beauty of this research paper is that we can use the concept
    and given model architecture without any changes to learn different games. This
    model architecture and algorithm are applied to seven games, and in three of them,
    the algorithm performed way better than a human! This is a big leap in the field
    of AI because the hope is that we can build a single algorithm that can master
    many tasks, as well as build a General Artificial Intelligence or Artificial General
    Intelligence (AGI) system at some point in the next few decades. You can read
    more about AGI at: [https://en.wikipedia.org/wiki/Artificial_general_intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence).
    We all know that DeepMind was immediately acquired by Google.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速回顾一下过去两年。让我们从2015年开始。一家名为DeepMind的小型伦敦公司发布了一篇名为《用深度强化学习玩Atari》的研究论文，可在[https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)找到。在这篇论文中，他们展示了计算机如何学习和玩Atari
    2600视频游戏。计算机只需观察屏幕像素就能玩游戏。我们的计算机游戏代理（即游戏玩家）在游戏得分增加时会获得奖励。这篇论文中展示的结果非常引人注目。这篇论文引起了很大的轰动，那是因为每个游戏都有不同的得分机制，而且这些游戏被设计成人类难以获得最高分。这篇研究论文的美丽之处在于，我们可以使用该概念和给定的模型架构，无需任何修改来学习不同的游戏。这个模型架构和算法应用于七款游戏，其中三款游戏中，算法的表现远远超过了人类！这在人工智能领域是一个巨大的飞跃，因为希望我们能够构建一个可以掌握许多任务的单一算法，并在未来几十年内的某个时刻构建一个通用人工智能或人工通用智能（AGI）系统。你可以在[https://en.wikipedia.org/wiki/Artificial_general_intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence)上了解更多关于AGI的信息。我们都知道DeepMind很快就被谷歌收购了。
- en: 'In 2017, Google DeepMind and OpenAI achieved a major milestone, which gives
    us hope that AGI will happen soon. Let''s start with Google DeepMind first; you
    must have heard that Google DeepMind ''s AlphaGo AI (a gaming bot) won a three-match
    series against the world''s best Go player. Go is a complex game because it has
    a huge number of permutations and combinations for a single move. You can watch
    the video for this game by clicking on this YouTube video: [https://www.youtube.com/watch?v=vFr3K2DORc8](https://www.youtube.com/watch?v=vFr3K2DORc8).
    Now let''s talk about OpenAI. If this is the first time you have heard about OpenAI,
    this is a short introduction. OpenAI is a non-profit AI research organization,
    cofounded by Elon Musk, which is trying to build AI that will be safe and ensure
    that the benefits of Artificial Intelligence (AI) systems are widely and evenly
    distributed as far as possible. In 2017, OpenAI''s gaming bot beat the world''s
    best Dota 2 players. You can watch this YouTube video for reference: [https://www.youtube.com/watch?v=7U4-wvhgx0w](https://www.youtube.com/watch?v=7U4-wvhgx0w).
    All this was achieved by AGI system environments created by tech giants. The goal
    of making an AGI system is that a single system can perform a variety of complex
    tasks. The ideal AGI system can help us solve lots of complex tasks in the fields
    of healthcare, agriculture, robotics, and so on without any changes to its algorithm.
    So, it is better for us if we can understand the basic concepts in order to develop
    the AGI system.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，谷歌DeepMind和OpenAI取得了重大里程碑，这让我们有理由相信通用人工智能（AGI）很快就会实现。让我们先从谷歌DeepMind开始；你一定听说过谷歌DeepMind的AlphaGo人工智能（一个游戏机器人）在一场三局比赛中击败了世界上最优秀的围棋选手。围棋是一项复杂的游戏，因为它对于每一个走法都有大量的排列组合。你可以通过点击这个YouTube视频观看这场比赛：[https://www.youtube.com/watch?v=vFr3K2DORc8](https://www.youtube.com/watch?v=vFr3K2DORc8)。现在让我们来谈谈OpenAI。如果你第一次听说OpenAI，这是一个简要的介绍。OpenAI是一个非营利性的人工智能研究组织，由埃隆·马斯克共同创立，致力于构建既安全又能确保人工智能（AI）系统的利益尽可能广泛且均匀地分布的AI。2017年，OpenAI的游戏机器人击败了世界上最优秀的Dota
    2选手。你可以观看这个YouTube视频作为参考：[https://www.youtube.com/watch?v=7U4-wvhgx0w](https://www.youtube.com/watch?v=7U4-wvhgx0w)。所有这些成就都是通过科技巨头创建的AGI系统环境实现的。创建AGI系统的目标是让一个系统能够执行各种复杂任务。理想的AGI系统可以帮助我们在医疗保健、农业、机器人技术等领域解决大量复杂任务，而无需对其算法进行任何更改。因此，如果我们能理解基本概念，以便开发AGI系统，那就更好了。
- en: In this chapter, just to start with, we will be trying to make a gaming bot
    that can play simple Atari games. We will achieve this by using reinforcement
    learning.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先尝试制作一个能够玩简单Atari游戏的机器人。我们将通过使用强化学习来实现这一点。
- en: 'In general, we will be covering the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Introducing the problem statement
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: Setting up the coding environment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置编码环境
- en: Understanding reinforcement learning (RL)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解强化学习（RL）
- en: Basic Atari gaming bot for pathfinder
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本Atari游戏机器人用于路径搜索
- en: Understanding the key concepts
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解关键概念
- en: Implementing the basic version of the gaming bot
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现游戏机器人的基本版本
- en: Building the Space Invaders gaming bot
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建Space Invaders游戏机器人
- en: Understanding the key concepts
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解关键概念
- en: Implementing the Space Invaders gaming bot
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现Space Invaders游戏机器人
- en: Building the Pong gaming bot
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建Pong游戏机器人
- en: Understanding the key concepts
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解关键概念
- en: Implementing the Pong gaming bot
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现Pong游戏机器人
- en: Just for fun - implementing the Flappy Bird gaming bot
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅为了乐趣 - 实现Flappy Bird游戏机器人
- en: Summary
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结
- en: Introducing the problem statement
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: 'We know we are trying to develop a gaming bot: a program that can play simple
    Atari games. If we provide enough time and computation resources, then it can
    outperform humans who are experts at playing certain games. I will list down some
    famous Atari games so that you can see which types of games I''m talking about.
    You must have played one of these games for sure. Some of the famous Atari games
    are Casino, Space Invaders, Pac-man, Space War, Pong (ping-pong), and so on. In
    short, the problem statement that we are trying to solve is how can we build a
    bot that can learn to play Atari games?'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道我们正在尝试开发一个游戏机器人：一个能够玩简单Atari游戏的程序。如果我们提供足够的时间和计算资源，那么它就能超越那些在玩某些游戏方面专家级的人类。我将列出一些著名的Atari游戏，以便你能看到我所说的游戏类型。你肯定玩过这些游戏中的至少一个。一些著名的Atari游戏包括Casino、Space
    Invaders、Pac-man、Space War、Pong（乒乓球）等等。简而言之，我们试图解决的问题是如何构建一个能够学习玩Atari游戏的机器人？
- en: In this chapter, we will be using already built-in gaming environments using
    `gym` and `dqn` libraries. So, we don't need to create a gaming visual environment
    and we can focus on the approach of making the best possible gaming bot. First,
    we need to set up the coding environment.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用已经内置的游戏环境，利用`gym`和`dqn`库。因此，我们不需要创建游戏视觉环境，我们可以专注于制作最佳游戏机器人的方法。首先，我们需要设置编码环境。
- en: Setting up the coding environment
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置编码环境
- en: 'In this section, we will cover how to set up a coding environment that can
    help us implement our applications. We need to install the gym library. These
    are the steps that you can follow. I''m using `Ubuntu 16.04 LTS` as my operating
    system:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何设置一个编码环境，以帮助我们实现我们的应用程序。我们需要安装gym库。以下是您可以遵循的步骤。我使用`Ubuntu 16.04
    LTS`作为我的操作系统：
- en: 'Step 1: Clone the gym repository from GitHub by executing this command: `$
    sudo git clone` `https://github.com/openai/gym.gi``t`'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一步：通过执行以下命令从GitHub克隆gym仓库：`$ sudo git clone` `https://github.com/openai/gym.git`
- en: 'Step 2: Jump to the gym directory by executing this command: `$ cd gym`'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二步：通过执行以下命令跳转到gym目录：`$ cd gym`
- en: 'Step 3: Execute this command to install the minimum number of required libraries
    for `gym`: `$ sudo pip install -e`'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三步：执行以下命令安装`gym`所需的最小数量库：`$ sudo pip install -e`
- en: 'Step 4: Install the gaming environment for Atari games by executing this command:
    `$ sudo pip install gym[atari]`'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四步：通过执行以下命令安装Atari游戏的游戏环境：`$ sudo pip install gym[atari]`
- en: 'Step 5: This step is optional. If you want to install all the gaming environments,
    then you can execute the following commands:'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第五步：这一步是可选的。如果您想安装所有游戏环境，则可以执行以下命令：
- en: '`$ sudo apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev
    xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig`'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$ sudo apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev
    xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig`'
- en: '`$ sudo pip install gym[all]`'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$ sudo pip install gym[all]`'
- en: 'This is how you can install the `gym` machine learning library, which we will
    be using to develop the gaming bot. We will be using the TensorFlowimplementation
    of the `dqn` library, so there will be no need to install `dqn` separately, but
    you can definitely refer to this installation note: [https://github.com/deepmind/dqn](https://github.com/deepmind/dqn).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是您安装`gym`机器学习库的方法，我们将使用它来开发游戏机器人。我们将使用`dqn`库的TensorFlow实现，因此不需要单独安装`dqn`，但您当然可以参考这个安装说明：[https://github.com/deepmind/dqn](https://github.com/deepmind/dqn)。
- en: As we are ready with the environment setup, we need to move on to our next section,
    which will help us understand the techniques that will be useful in order to develop
    the gaming bots. So let's begin!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经完成了环境设置，我们需要继续到下一个部分，这将帮助我们了解在开发游戏机器人时有用的技术。那么，让我们开始吧！
- en: Understanding Reinforcement Learning (RL)
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解强化学习（RL）
- en: In this chapter, we are making a gaming bot with the help of reinforcement learning
    techniques. The motivation behind reinforcement learning is simple. RL gives the
    machine or any software agent a chance to learn its behavior based on the feedback
    this agent receives from the environment. This behavior can be learned once, or
    you can keep on adapting with time.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将借助强化学习技术制作游戏机器人。强化学习的动机很简单。RL给机器或任何软件代理一个机会，根据该代理从环境中收到的反馈来学习其行为。这种行为可以一次性学习，或者您可以随着时间的推移不断适应。
- en: 'Let''s understand RL with a fun example of a child learning to speak. These
    are the steps a child will take when they are learning how to speak:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个孩子学习说话的有趣例子来理解RL。当孩子学习如何说话时，他们会采取以下步骤：
- en: 'Step 1: The first thing is that the child starts to observe you; how you are
    speaking and how you are interacting with him or her. The child listens to the
    basic words and sentences from you and learns that they can make a similar sound
    too. So, the child tries to imitate you.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一步：首先，孩子开始观察您；您是如何说话的，以及您是如何与他或她互动的。孩子从您那里听基本单词和句子，并了解到他们也可以发出类似的声音。因此，孩子试图模仿您。
- en: 'Step 2: The child wants to speak full sentences or words but they may not understand
    that even before speaking sentences, they need to learn simple words! This is
    a challenge that comes while they are trying to speak. Now the child attempts
    to make sounds, some sounds are funny or weird, but they are still determined
    to speak words and sentences.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2步：孩子想要说出完整的句子或单词，但他们可能不理解，甚至在说话句子之前，他们需要学习简单的单词！这是他们在尝试说话时遇到的挑战。现在孩子试图发出声音，有些声音听起来很滑稽或奇怪，但他们仍然决心说出单词和句子。
- en: 'Step 3: There is another challenge that the child faces, which is that they
    need to understand and remember the meaning behind the words they are trying to
    speak. But the child manages to overcome this challenge and learns to speak their
    first few words, which are very simple words, such as *mama, papa, dadda, paa,
    maa*, and so on. They learn this task by constantly observing their surroundings.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第3步：孩子面临的另一个挑战是他们需要理解并记住他们试图说出的单词背后的含义。但孩子设法克服了这个挑战，学会了说出他们最初的一些单词，这些单词非常简单，例如*mama,
    papa, dadda, paa, maa*等等。他们通过不断观察周围的环境来学习这项任务。
- en: 'Step 4: The real challenge begins with how to use a particular word, when to
    use which word, and remembering all the words they hear for the first time. Try
    to feed the meaning of all the words and the context in which the child needs
    to use them. Sounds like a challenging task, doesn''t it?'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第4步：真正的挑战始于如何使用特定的单词，何时使用哪个单词，以及记住他们第一次听到的所有单词。尝试向孩子提供所有单词的意义以及他们需要使用的情境。听起来像是一项挑战性的任务，不是吗？
- en: For a child, it is a difficult task, but once it starts understanding the language
    and practices the sentences, then it will become a part of the child's life. Within
    2-3 years, the child could have enough practice to start interacting easily. If
    we think of ourselves speaking, it is an easy task for us because we have learned
    enough about how to interact within our environment.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个孩子来说，这是一项困难的任务，但一旦开始理解语言并练习句子，它就会成为孩子生活的一部分。在2-3年内，孩子可能已经足够练习，可以轻松地开始互动。如果我们考虑我们自己说话，对于我们来说这是一项简单的任务，因为我们已经学会了足够关于如何在我们的环境中互动的知识。
- en: 'Now, let''s try to connect the dots. With the help of the preceding example,
    we will try to understand the concept of Reinforcement Learning. The problem statement
    of the given example is speaking, where the child is the agent who is trying to
    manipulate the environment (which word the child speaks first) by taking an action
    (here, the action is speaking), and they try to speak one word or the other. The
    child gets a reward—say, a chocolate—when they accomplish a submodule of the task,
    which means speaking some words in a day, and will not receive any chocolate when
    they are not able to speak anything. This is a simplified description of reinforcement
    learning. You can refer to the following diagram:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试将这些点联系起来。借助前面的例子，我们将尝试理解强化学习（RL）的概念。给定例子的问题陈述是说话，其中孩子是代理，他们试图通过采取行动（在这里，行动是说话）来操纵环境（孩子首先说出哪个单词），并尝试说出一个单词或另一个单词。当孩子完成任务的子模块时，他们会得到奖励——比如说，一块巧克力——这意味着他们在一天内说出了一些单词，而如果他们无法说出任何东西，则不会得到巧克力。这是强化学习的简化描述。您可以参考以下图表：
- en: '![Understanding Reinforcement Learning (RL)](img/B08394_11_01.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![理解强化学习（RL）](img/B08394_11_01.jpg)'
- en: 'Figure 11.1: Pictorial representation of the basic concept of RL'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：强化学习基本概念的形象表示
- en: So basically, RL allows machines and software agents to automatically determine
    the ideal and best possible behavior within a specific task or within a specific
    context in order to maximize the performance of software agents. Simple reward
    feedback is required for the agent to learn its behavior, and this is known as
    the reinforcement signal. Every time the software agent tries to take the kind
    of actions that lead it to gain maximum rewards. Eventually, it learns all the
    actions or moves that lead the agent to the optimum solution of the task so that
    it becomes the master of it. The algorithms for RL learn to react to an environment.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上，强化学习（RL）允许机器和软件代理在特定任务或特定情境中自动确定理想和最佳的行为，以最大化软件代理的性能。代理需要简单的奖励反馈来学习其行为，这被称为强化信号。每次软件代理尝试采取导致其获得最大奖励的行动时，它都会尝试。最终，它学会了所有导致代理达到任务最优解的行动或移动，从而成为该领域的专家。强化学习的算法学会对环境做出反应。
- en: In order to build the gaming bot, RL algorithms are the perfect choice, and
    there is a reason behind it. Suppose there are many slot machines with random
    payouts and you want to win the maximum amount of money. How do you win the maximum
    amount of money? One naive approach is to just select a single machine and pull
    its lever all day long, and it might give you some payouts. If you are lucky enough,
    then you may hit the jackpot. There are chances that in order to try this approach,
    you may lose some money. This approach is called a *pure exploitation* *approach*.
    It is not an optimal approach.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建游戏机器人，强化学习算法是完美的选择，这背后有原因。假设有许多带有随机回报的老虎机，你想要赢得最大金额。你该如何赢得最大金额？一个简单的方法是只选择一台机器，整天拉动它的杠杆，它可能会给你一些回报。如果你足够幸运，那么你可能会中大奖。为了尝试这种方法，你可能会损失一些钱。这种方法被称为*纯利用*方法。这不是一个最优的方法。
- en: Let's take another approach. In this approach, we will pull the lever of every
    single slot machine and pray that at least one of them hits the jackpot. This
    too is a naive approach. In this approach, we need to keep pulling the lever all
    day long. This approach is called a *pure exploration approach*. This approach
    is not optimal as well, so we need to find a proper balance between these two
    approaches in order to get maximum rewards. This is referred to as the exploration
    versus exploitation dilemma of RL. Now we need to solve this issue. Well, for
    that, we need a mathematical framework that can help us achieve the optimal solution,
    and that mathematical approach is *Markov Decision Process (MDP)*. Let's explore
    this.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们采取另一种方法。在这个方法中，我们将拉动每台老虎机的杠杆，并祈祷至少有一台能够中大奖。这同样是一个简单的方法。在这个方法中，我们需要整天拉动杠杆。这种方法被称为*纯探索*方法。这种方法也不是最优的，因此我们需要在这两种方法之间找到一个适当的平衡，以获得最大的奖励。这被称为强化学习中的探索与利用的困境。现在我们需要解决这个问题。好吧，为了做到这一点，我们需要一个数学框架，可以帮助我们实现最优解，而这个数学方法就是*马尔可夫决策过程（MDP）*。让我们来探讨一下。
- en: Markov Decision Process (MDP)
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（MDP）
- en: 'Markov Decision Process uses the following parameters:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程使用以下参数：
- en: Set of states, *S*
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态集合，*S*
- en: Set of actions, *A*
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行动集合，*A*
- en: Reward function, *R*
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励函数，*R*
- en: Policy, *π*
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略，*π*
- en: Value, *V*
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 价值，*V*
- en: 'In order to perform transition for one state to the end state *(S)*, we have
    to take an action *(A)* or a series of actions. We will get rewards *(R)* for
    each action we take. Our action can provide us either a positive reward or a negative
    reward. The set of actions that we take define our policy *(π)*. The rewards that
    we get in return after performing each action define our value *(V).* Our goal
    is to maximize the rewards by choosing the correct policy. We can do that by performing
    the best possible action. Mathematically, we can express this as shown in the
    following screenshot:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将一个状态转换到最终状态*(S)*，我们必须采取一个动作*(A)*或一系列动作。我们将为每个采取的动作获得奖励*(R)*。我们的行动可以提供正奖励或负奖励。我们采取的行动集合定义了我们的策略*(π)*。执行每个行动后获得的奖励定义了我们的价值*(V)*。我们的目标是通过对策略的正确选择来最大化奖励。我们可以通过执行最佳可能的行动来实现这一点。从数学上讲，我们可以像以下截图所示那样表达这一点：
- en: '![Markov Decision Process (MDP)](img/B08394_11_02.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![马尔可夫决策过程 (MDP)](img/B08394_11_02.jpg)'
- en: 'Figure 11.2: Mathematical representation of Markov Decision Process'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2：马尔可夫决策过程的数学表示
- en: 'We will be applying the preceding equation for all possible values of *S* for
    a time t. We have a set of states and actions. We need to consider these states,
    actions, and rules for transitioning the agent from one state to another. When
    we perform an action that changes the state of the gaming agent, the the agent
    will get rewards for doing that. This entire process of state, action, and getting
    rewards makes up Markov Decision Process (MDP). One round of a game is considered
    *one episode* of MDP. This process includes a finite sequence of states, actions,
    and rewards. Take a look at the following equation for a representation of the
    process: :'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用前面的方程式来计算时间t时所有可能的*状态*值。我们有一组状态和动作。我们需要考虑这些状态、动作以及将智能体从一个状态转换到另一个状态的规则。当我们执行改变游戏智能体状态的行动时，智能体将因该行动而获得奖励。这个状态、行动和获得奖励的整个过程构成了马尔可夫决策过程（MDP）。一场游戏的一轮被认为是MDP的一个*回合*。这个过程包括有限的状态、动作和奖励序列。请看以下方程式，以表示这个过程：：
- en: S0, a0, r1, s1, a1, r2, s2, a2, r3 ,…, sn-1, an-1, rn, sn
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: S0, a0, r1, s1, a1, r2, s2, a2, r3 ，…，sn-1, an-1, rn, sn
- en: Here, s[i] represents the state, ai is the action, and r[i+1] is the reward
    that we will get after performing the action. sn indicates that a particular episode
    ends with a terminal state, and this happens when the **game over** screen appears.
    A Markov Decision Process is based on the Markov assumption, the probability of
    the next state s[i+1] depends on the current state s[i] and the performed action
    ai and does not depend on the preceding states or actions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，s[i] 代表状态，ai 是动作，r[i+1] 是执行动作后我们将获得的奖励。sn 表示一个特定的场景以终端状态结束，这发生在“游戏结束”屏幕出现时。马尔可夫决策过程基于马尔可夫假设，下一个状态
    s[i+1] 的概率取决于当前状态 s[i] 和执行的动作 ai，而不取决于前面的状态或动作。
- en: Discounted Future Reward
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折现未来奖励
- en: In the long term, if we want our gaming agent to do well, then we need to take
    into account the immediate rewards, but we also need to consider the future awards
    that our agent will get. How should we approach this scenario? Well, the answer
    lies in the concept of discounted future rewards.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从长远来看，如果我们想让我们的游戏代理做得好，那么我们需要考虑即时奖励，但我们也需要考虑我们的代理将获得的未来奖励。我们应该如何处理这种情况呢？嗯，答案在于折现未来奖励的概念。
- en: 'Given one run of MDP, we can calculate the *total rewards* for one episode
    by using the following equation:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个 MDP 的运行，我们可以通过以下方程计算一个场景的总奖励：
- en: R = r1 + r2 + r3 + … + rn
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: R = r1 + r2 + r3 + … + rn
- en: 'Based on the preceding equation, we can calculate the *total future rewards*
    from time stamp *t* onward, and that can be expressed by the given equation:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面的方程，我们可以计算出从时间戳 *t* 开始的*总未来奖励*，这可以用给定的方程表示：
- en: Rt = rt + rt+1 + rt+2 + rt+3 + … + rn
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Rt = rt + rt+1 + rt+2 + rt+3 + … + rn
- en: 'Here, we are dealing with a gaming environment that is random, and we cannot
    be sure whether we will get the same rewards the next time we perform the same
    actions to play a specific game. The more you think about the future, the more
    it will get diverged. For that reason, it is better that we use *discounted future
    rewards* instead of total rewards:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们处理的是一个随机的游戏环境，我们无法确定我们是否会在执行相同的动作来玩特定游戏时获得相同的奖励。你考虑未来得越多，它就会越偏离。因此，我们最好使用*折现未来奖励*而不是总奖励：
- en: Rt = rt+ γrt+1 + γ2rt+2+ … + γn-1 rn
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Rt = rt+ γrt+1 + γ2rt+2+ … + γn-1 rn
- en: 'Here, γ is the discount factor. Its value is between *0 to 1*. It is easy to
    understand that the discounted future reward at particular time step t can be
    expressed with the help of the rewards of the current stare plus rewards at time
    step *t+1*:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，γ 是折现系数。它的值在 *0 到 1* 之间。很容易理解，在特定的时间步 t 的折现未来奖励可以用当前状态的奖励加上时间步 *t+1* 的奖励来表示：
- en: Rt = rt + γ (rt+1 + γ (rt+2 + …)) = rt + γRt+1
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Rt = rt + γ (rt+1 + γ (rt+2 + …)) = rt + γRt+1
- en: 'Now let me tell you what the practical meaning of tuning this discount factor
    is: if we set the value of the discount factor γ = 0, then our strategy of plying
    will be short-sighted and we take our gaming decision just based on the immediate
    rewards. We need to find a balance between immediate rewards and future rewards,
    so we should set the value of the discount factor to something more than 0.7.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我告诉你调整这个折现系数的实际意义：如果我们把折现系数 γ 的值设为 0，那么我们的游戏策略将是短视的，我们只是基于即时奖励来做游戏决策。我们需要在即时奖励和未来奖励之间找到一个平衡，所以我们应该把折现系数的值设为大于
    0.7。
- en: For example, we can set the value as γ = 0.9\. If our gaming environment is
    deterministic and we know that the same actions always lead us to the same reward,
    then we can set the value of the discount factor γ =1\. A good strategy for a
    gaming agent would be to always *choose an action that maximizes the discounted
    future reward.*
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将值设为 γ = 0.9。如果我们知道我们的游戏环境是确定性的，并且相同的动作总是带给我们相同的奖励，那么我们可以将折现系数 γ 的值设为
    1。对于游戏代理来说，一个好的策略是始终*选择一个最大化折现未来奖励的动作*。
- en: We have covered the basics of RL. From now onward, we will start implementing
    our gaming bot. So let's get ready for some fun!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了强化学习的基础。从现在开始，我们将开始实现我们的游戏机器人。所以，让我们为一些乐趣做好准备！
- en: Basic Atari gaming bot
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本Atari游戏机器人
- en: In this chapter, we are trying a hands-on approach to building some basic gaming
    bots. We are choosing some famous Atari games that nearly everybody has played
    at some point in their lives. We choose Atari games because we know how to play
    them, and that makes our life easy because we can understand what kind of action
    our bot should perform in order to get better over a period of time.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们尝试了一种动手的方式来构建一些基本的游戏机器人。我们选择了几乎每个人在生命中某个时刻都玩过的著名雅达利游戏。我们选择雅达利游戏是因为我们知道如何玩它们，这使得我们的生活变得简单，因为我们能够理解我们的机器人应该执行什么样的动作，以便在一段时间内变得更好。
- en: In this section, we are building our own game. This game is simple, so we can
    look at how we can apply the Q-Learning algorithms. Here, we will be designing
    the game world on our own. Let's begin!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建自己的游戏。这个游戏很简单，因此我们可以看看我们如何应用 Q-Learning 算法。在这里，我们将自己设计游戏世界。让我们开始吧！
- en: Understanding the key concepts
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解关键概念
- en: 'In this section, we will be looking at a lot of important aspects that will
    help us while coding, so here, we will be covering the following topics:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨许多重要的方面，这些方面将帮助我们进行编码，因此在这里，我们将涵盖以下主题：
- en: Rules for the game
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏规则
- en: Understanding the Q-Learning algorithm
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Q-Learning 算法
- en: Rules for the game
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 游戏规则
- en: 'Before we begin with the basic concepts or algorithms, we need to understand
    the rules of the game that we are building. The game is simple and easy to play.
    The rules for this game are as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始基本概念或算法之前，我们需要了解我们正在构建的游戏规则。这个游戏很简单，很容易玩。这个游戏的规则如下：
- en: '*Rules of the game:* The gaming agent means a yellow box has to reach one of
    the goals to end the game: it can be either a green cell or a red cell. This means
    the yellow box should reach either the green cell or the red cell.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*游戏规则：* 游戏代理意味着一个黄色方块必须达到一个目标以结束游戏：它可以是绿色单元格或红色单元格。这意味着黄色方块应该到达绿色单元格或红色单元格。'
- en: '*Rewards:* Each step gives us a negative reward of - 0.04\. If our gaming agent
    reaches the red cell, then the red cell gives us a negative reward of - 1\. If
    our gaming agent reaches the green cell, then the green cell gives us a positive
    reward of +1.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*奖励：* 每一步都会给我们一个负奖励 -0.04。如果我们的游戏代理到达红色单元格，那么红色单元格会给我们一个负奖励 -1。如果我们的游戏代理到达绿色单元格，那么绿色单元格会给我们一个正奖励
    +1。'
- en: '*States:* Each cell is a state for the agent that it takes to find its goal.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*状态：* 每个单元格都是代理的一个状态，它需要找到其目标。'
- en: '*Actions:* There are only four actions for this game: Up direction, Down direction,
    Right direction, Left direction.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*动作：* 这个游戏只有四个动作：向上方向、向下方向、向右方向、向左方向。'
- en: 'We need the `tkinter` library to implement this approach. I have already provided
    a description about how to install it at this GitHub link: [https://github.com/jalajthanaki/Q_learning_for_simple_atari_game/blob/master/README.md](https://github.com/jalajthanaki/Q_learning_for_simple_atari_game/blob/master/README.md).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要 `tkinter` 库来实现这个方法。我已经在这个 GitHub 链接中提供了如何安装它的描述：[https://github.com/jalajthanaki/Q_learning_for_simple_atari_game/blob/master/README.md](https://github.com/jalajthanaki/Q_learning_for_simple_atari_game/blob/master/README.md)。
- en: Now let's look at the Q learning algorithm that we will use during this chapter
    to build the gaming bot.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在本章构建游戏机器人时我们将使用的 Q 学习算法。
- en: Understanding the Q-Learning algorithm
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解 Q-Learning 算法
- en: This algorithm was originally published by DeepMind in two papers. The first
    one was published with the title *Playing Atari with Deep Reinforcement Learning*
    on NIPS 2013\. The link for the paper is [https://arxiv.org/pdf/1312.5602.pdf](https://arxiv.org/pdf/1312.5602.pdf).
    The second one was published with the title *Human-level control through deep
    reinforcement Learning* on Nature in 2015\. The link for this paper is [http://www.davidqiu.com:8888/research/nature14236.pdf](http://www.davidqiu.com:8888/research/nature14236.pdf).
    You should definitely read these papers. I have simplified the main concepts of
    these papers for you.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法最初由 DeepMind 在两篇论文中发表。第一篇论文在 2013 年 NIPS 上以 *Playing Atari with Deep Reinforcement
    Learning* 为标题发表。论文的链接是 [https://arxiv.org/pdf/1312.5602.pdf](https://arxiv.org/pdf/1312.5602.pdf)。第二篇论文在
    2015 年《自然》杂志上以 *Human-level control through deep reinforcement Learning* 为标题发表。这篇论文的链接是
    [http://www.davidqiu.com:8888/research/nature14236.pdf](http://www.davidqiu.com:8888/research/nature14236.pdf)。你绝对应该阅读这些论文。我已经为你简化了这些论文的主要概念。
- en: 'In Q-learning, we need to define a *Q (s, a)* function that represents the
    discount factor reward when we perform action *a* in state *s*, and it continues
    optimally from that point onward. You can see the equation that helps us choose
    the maximum reward in the followingscreenshot:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在Q学习中，我们需要定义一个*Q(s, a)*函数，它表示我们在状态*s*执行动作*a*时的折扣因子奖励，并从该点开始进行最优操作。你可以在下面的截图中看到帮助我们选择最大奖励的方程：
- en: '![Understanding the Q-Learning algorithm](img/B08394_11_03.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![理解Q学习算法](img/B08394_11_03.jpg)'
- en: 'Figure 11.3: Equation for Q-function'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：Q函数的方程
- en: We can think of the Q (s, a) function as giving us the best possible score at
    the end of the game after performing action *a* in the particular state *s*. This
    function is the Q function because it indicates the *quality* of a certain action
    in a certain given state.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将Q(s, a)函数视为在特定状态*s*执行动作*a*后，在游戏结束时给我们提供最佳可能得分的函数。这个函数是Q函数，因为它表明了在特定状态下某个动作的质量。
- en: 'Let me simplify this for you. Suppose you are in state *s* and are thinking
    about whether you should perform action *a* or *b*. You really want to win the
    game with a high score. So, in order to achieve your goal, you want to select
    the action that gives you the highest score at the end of the game. If you have
    this Q-function with you, then the selection of actions become quite easy because
    you just need to pick the action that has the highest Q-value. You can see the
    equation that you can use to obtain the highest Q-value in the following screenshot:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我来简化一下。假设你处于状态*s*，正在考虑是否应该执行动作*a*或*b*。你真的想以高分赢得游戏。所以，为了实现你的目标，你想要选择在游戏结束时给你带来最高得分的动作。如果你有这个Q函数，那么动作的选择就变得相当简单，因为你只需要选择具有最高Q值的动作。你可以在下面的截图中看到你可以使用的方程来获得最高的Q值：
- en: '![Understanding the Q-Learning algorithm](img/B08394_11_04.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![理解Q学习算法](img/B08394_11_04.jpg)'
- en: 'Figure 11.4: Equation for choosing the maximum rewards using the Q-function'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4：使用Q函数选择最大奖励的方程
- en: 'Here, π represents the policy. The policy indicates the rules of the game and
    the action. With the help of the policy, we can choose what kind of typical actions
    are available in each state. Our next step is to obtain this Q-function. For that,
    we need to concentrate on just one transition. This transition is made of four
    states: *< s, a, r, s'' >*. Remember the discount factor reward, where we can
    express the Q-value of the current state *s* and the current action *a* in terms
    of the Q-value of the next state *s''*. The equation for calculating rewards is
    provided in the following screenshot:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，π代表策略。策略表明游戏的规则和动作。借助策略，我们可以选择在每个状态下可用的典型动作。我们的下一步是获取这个Q函数。为此，我们需要专注于仅一个转换。这个转换由四个状态组成：*<
    s, a, r, s' >*。记住折扣因子奖励，其中我们可以用下一个状态*s'的Q值来表示当前状态*s*和当前动作*a*的Q值。计算奖励的方程式在下面的截图提供：
- en: '![Understanding the Q-Learning algorithm](img/B08394_11_05.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![理解Q学习算法](img/B08394_11_05.jpg)'
- en: 'Figure 11.5: The bellman equation for calculation rewards'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5：计算奖励的Bellman方程
- en: The preceding equation is called the Bellman equation, and it is the main idea
    behind the Q-learning algorithm. This equation is quite logical, and it indicates
    that the maximum future rewards for this state and action are the summation of
    the immediate rewards and the maximum future reward for the next state.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程被称为Bellman方程，它是Q学习算法背后的主要思想。这个方程相当合理，它表明这个状态和动作的最大未来奖励是即时奖励和下一个状态的最大未来奖励的总和。
- en: 'The main intuition is that with the help of the *n number of iterations followed
    by approximation* step, we can generate the values for the Q-function. We will
    achieve this by using the *Bellman equation*. In the simplest case, the Q-function
    is implemented in the form of a table where states are its rows and actions are
    its columns. The pseudo steps of this Q-learning algorithm are simple. You can
    take a look at them, as follows: at them, as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的直觉是，通过跟随*迭代次数*和近似步骤，我们可以生成Q函数的值。我们将通过使用*Bellman方程*来实现这一点。在最简单的情况下，Q函数以表格的形式实现，其中状态是其行，动作是其列。这个Q学习算法的伪步骤很简单。你可以看看它们，如下所示：
- en: 'Step 1: Initialize Q [number of states, number of actions] arbitrarily'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第1步：任意初始化Q [状态数量，动作数量]
- en: 'Step 2: Observe initial states'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2步：观察初始状态
- en: 'Step 3: Repeat'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第3步：重复
- en: '[PRE0]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Until terminated
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直到终止
- en: 'We need to follow these steps, where α is the learning rate. The learning rate
    verifies the difference between the previous Q-value and the newly proposed Q-value.
    This difference value is taken into account so that we can check when our model
    will converge. With the help of the learning rate, we can regulate the speed of
    training in such a way that our model won''t become too slow to converge or too
    fast to converge in a way that it cannot learn anything. We will be using *maxa''Q
    [s'', a'']* to update *Q [s, a]*In order to maximize the reward. This is the only
    operation that we need to perform. This estimation operation will give us the
    updated Q-value. In the early stages of training, when our agent is learning,
    there could be a situation where our estimations may go completely wrong, but
    the estimations and updated Q-values get more and more accurate with every iteration.
    If we perform this process enough times, then the Q-function will converge. It
    represents the true and optimized Q-value. For better understanding, we will implement
    the preceding algorithm. Refer to the code snippet given in the following screenshot:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要遵循以下步骤，其中α是学习率。学习率验证了先前Q值和新提出的Q值之间的差异。这个差异值被考虑在内，以便我们可以检查我们的模型何时会收敛。借助学习率，我们可以调节训练速度，使我们的模型不会变得太慢以至于无法收敛，或者太快以至于无法学习任何东西。我们将使用*maxa'Q
    [s', a']*来更新*Q [s, a]*以最大化奖励。这是我们唯一需要执行的操作。这个估计操作将给我们提供更新的Q值。在训练的早期阶段，当我们的智能体在学习时，可能会出现我们的估计完全错误的情况，但随着每一次迭代的进行，估计和更新的Q值会越来越准确。如果我们进行这个过程足够多次，那么Q函数将收敛。它代表了真实和优化的Q值。为了更好地理解，我们将实现前面的算法。请参考以下截图中的代码片段：
- en: '![Understanding the Q-Learning algorithm](img/B08394_11_06.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![理解Q学习算法](img/B08394_11_06.jpg)'
- en: 'Figure 11.6: Code snippet for building and updating the Q-table'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6：构建和更新Q表的代码片段
- en: 'You can see the output in the form of a Q-table in the following screenshot:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下截图中以Q表的形式看到输出：
- en: '![Understanding the Q-Learning algorithm](img/B08394_11_07.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![理解Q学习算法](img/B08394_11_07.jpg)'
- en: 'Figure 11.7: Q-table value'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7：Q表值
- en: 'You can see the implementation of the preceding algorithm by referring to this
    GitHub link: [https://github.com/jalajthanaki/Q_learning_for_simple_atari_game/blob/master/Demo_Q_table.ipynb](https://github.com/jalajthanaki/Q_learning_for_simple_atari_game/blob/master/Demo_Q_table.ipynb).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过参考以下GitHub链接来查看前面算法的实现：[https://github.com/jalajthanaki/Q_learning_for_simple_atari_game/blob/master/Demo_Q_table.ipynb](https://github.com/jalajthanaki/Q_learning_for_simple_atari_game/blob/master/Demo_Q_table.ipynb)。
- en: Now let's start implementing the game.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始实现游戏。
- en: Implementing the basic version of the gaming bot
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现游戏机器人的基本版本
- en: 'In this section, we will be implementing a simple game. I have already defined
    the rules of this game. Just to remind you quickly, our agent, yellow block tries
    to reach either the red block or the green block. If the agent reaches the green
    block, we will receive + 1 as a reward. If it reaches the red block, we get -1\.
    Each step the agent will take will be considered a - 0.04 reward. You can turn
    back the pages and refer to the section Rules for the game if you want. You can
    refer to the code for this basic version of a gaming bot by referring to this
    GitHub link: [https://github.com/jalajthanaki/Q_learning_for_simple_atari_game](https://github.com/jalajthanaki/Q_learning_for_simple_atari_game).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现一个简单的游戏。我已经定义了这个游戏的规则。为了快速提醒你，我们的智能体，黄色方块试图到达红色方块或绿色方块。如果智能体到达绿色方块，我们将获得+1作为奖励。如果它到达红色方块，我们得到-1。智能体每走一步都将被视为-0.04的奖励。如果你想查看游戏的规则部分，可以翻回前面的页面。你可以通过参考这个GitHub链接来查看这个基本版本的游戏机器人代码：[https://github.com/jalajthanaki/Q_learning_for_simple_atari_game](https://github.com/jalajthanaki/Q_learning_for_simple_atari_game)。
- en: 'For this game, the gaming world or the gaming environment is already built,
    so we do not need to worry about it. We need to include this gaming world by just
    using the import statement. The main script that we are running is `Lerner.py`.
    The code snippet for this code is given in the following screenshot:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个游戏，游戏世界或游戏环境已经构建好了，所以我们不需要担心它。我们只需要通过导入语句来包含这个游戏世界。我们正在运行的脚本主程序是`Lerner.py`。该代码的代码片段如下所示：
- en: '![Implementing the basic version of the gaming bot](img/B08394_11_08.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![实现游戏机器人的基本版本](img/B08394_11_08.jpg)'
- en: 'Figure 11.8: Code snippet for the basic version of the gaming bot - I'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8：游戏机器人基本版本的代码片段 - I
- en: 'As you can see in the preceding code, we are keeping track of the agent''s
    states and actions with the help of the code given in loops. After that, we will
    define the four possible actions for this game, and based on that, we will calculate
    the reward values. We have also defined the `max_Q` function, which calculates
    the maximum Q value for us. You can also refer to the following screenshot:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，我们通过循环中给出的代码帮助跟踪智能体的状态和动作。之后，我们将定义这个游戏的四种可能动作，并基于此计算奖励值。我们还定义了`max_Q`函数，它为我们计算最大的Q值。您也可以参考以下截图：
- en: '![Implementing the basic version of the gaming bot](img/B08394_11_09.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![实现游戏机器人的基本版本](img/B08394_11_09.jpg)'
- en: 'Figure 11.9: Code snippet for basic version of gaming bot - II'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9：游戏机器人基本版本代码片段 - II
- en: 'As you can see in the preceding code snippet, the helper function uses the
    `inc_Q` method in order to update Q. By using the `run` function, we can update
    Q values so that our bot will learn how to achieve the best solution. You can
    run this script by executing this command:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码片段所示，辅助函数使用`inc_Q`方法来更新Q值。通过使用`run`函数，我们可以更新Q值，使我们的机器人学会如何找到最佳解决方案。您可以通过执行以下命令来运行此脚本：
- en: '[PRE1]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When you run the script, you can see the following output window, and within
    1-2 minutes, this bot will find the optimal solution. You can find the bot''s
    initial state and final state output in the following screenshot:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行脚本时，您可以看到以下输出窗口，在1-2分钟内，这个机器人将找到最佳解决方案。您可以在以下截图中找到机器人的初始状态和最终状态输出：
- en: '![Implementing the basic version of the gaming bot](img/B08394_11_10.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![实现游戏机器人的基本版本](img/B08394_11_10.jpg)'
- en: 'Figure 11.10: Output of the basic version of the gaming bot'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10：游戏机器人基本版本的输出
- en: 'You can track the progress of the bot by using the reward score. You can refer
    to the following screenshot:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用奖励分数来跟踪机器人的进度。您可以参考以下截图：
- en: '![Implementing the basic version of the gaming bot](img/B08394_11_11.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![实现游戏机器人的基本版本](img/B08394_11_11.jpg)'
- en: 'Figure 11.11: Tracking the progress of the gaming bot'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11：跟踪游戏机器人的进度
- en: As you can see, during the initial iteration, the gaming bot didn't perform
    well after some iterations bot started learning how to take action based on the
    experience it gained. We stopped the code when there was no significant improvement
    in the reward scores. That is because our gaming bot was able to achieve the best
    solution.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在初始迭代中，游戏机器人在经过几次迭代后表现不佳，但随后开始根据其获得的经验学习如何采取行动。我们在奖励分数没有显著提高时停止了代码。这是因为我们的游戏机器人能够找到最佳解决方案。
- en: Now let's build a more complex gaming bot; we will be using a deep Q-network
    for training. So let's begin.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建一个更复杂的游戏机器人；我们将使用深度Q网络进行训练。那么，让我们开始吧。
- en: Building the Space Invaders gaming bot
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建太空侵略者游戏机器人
- en: 'We are going to build a gaming bot that can play Space Invaders. Most of you
    may have played this game or at least heard of it. If you haven''t played it or
    you can''t remember it at this moment, then take a look at the following screenshot:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个能够玩太空侵略者的游戏机器人。你们大多数人可能都玩过这个游戏，或者至少听说过它。如果您没有玩过，或者现在想不起来，那么请看看以下截图：
- en: '![Building the Space Invaders gaming bot](img/B08394_11_12.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![构建太空侵略者游戏机器人](img/B08394_11_12.jpg)'
- en: 'Figure 11.12: Snippet of the Space Invaders game'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12：太空侵略者游戏片段
- en: Hopefully you remember the game now and how it was played. First, we will look
    at the concepts that we will be using to build this version of the gaming bot.
    Let's begin!
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您现在还记得这个游戏以及它是如何玩的。首先，我们将看看我们将用于构建这个版本游戏机器人的概念。让我们开始吧！
- en: Understanding the key concepts
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解关键概念
- en: 'In this version of the gaming bot, we will be using the deep Q-network and
    training our bot. So before implementing this algorithm, we need to understand
    the concepts. Take a look at the following concepts:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个版本的游戏机器人中，我们将使用深度Q网络并训练我们的机器人。因此，在实现此算法之前，我们需要理解这些概念。看看以下概念：
- en: Understanding a deep Q-network (DQN)
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解深度Q网络（DQN）
- en: Understanding Experience Replay
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解经验回放
- en: Understanding a deep Q-network (DQN)
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解深度Q网络（DQN）
- en: The deep Q-network algorithm is basically a combination of two concepts. It
    uses the Q-learning logic for a deep neural network. That is the reason why it
    is called a deep Q-network (DQN).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 深度Q网络算法基本上是两个概念的组合。它使用深度神经网络中的Q学习逻辑。这就是为什么它被称为深度Q网络（DQN）。
- en: Every gaming world has a different environment. So, say, Super Mario looks different
    from Space Invaders. We can't feed the entire gaming environment for an individual
    game every time, so first of all, we need to decide on the universal representation
    of all games so that we use them as input for the DQN algorithm. The screen pixels
    are the obvious choice for input because clearly they contain all the relevant
    information about the game world and its situation. Without the help of the screen
    pixels we cannot capture the speed and direction of the gaming agent.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 每个游戏世界都有不同的环境。例如，超级马里奥与太空侵略者看起来不同。我们不能每次都为单个游戏提供整个游戏环境，因此首先我们需要决定所有游戏的通用表示，以便我们将它们作为DQN算法的输入。屏幕像素是明显的输入选择，因为它们显然包含了关于游戏世界及其状况的所有相关信息。没有屏幕像素的帮助，我们无法捕捉游戏代理的速度和方向。
- en: 'If we apply the same preprocessing steps to the game screens as mentioned in
    the DeepMind paper, then we need to follow these steps:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将与DeepMind论文中提到的相同的前处理步骤应用于游戏屏幕，那么我们需要遵循以下步骤：
- en: 'Step 1: We need to consider the last four screen images of the game as the
    input.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步：我们需要考虑游戏的最后四张屏幕图像作为输入。
- en: 'Step 2: We need to resize them to 84 x 84 and convert them into grayscale with
    256 gray levels. That means we would have 256^(84x84x4), which is approximately
    10^(67970) possible gaming states. This means we have 10^(67970) rows in our imaginary
    Q-table, and that is a big number. You could argue that many pixel combinations
    or states never occur so we can possibly represent it as a sparse matrix. This
    sparse matrix contains only visited states. However, most of the states are rarely
    visited. So, it would take a long time for the Q-table to converge. Honestly,
    we would also like to take a good guess for Q-values for states we have never
    seen before by the agent so that we can generate a reasonably good action for
    the gaming agent. This is the point where deep learning enters the picture.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步：我们需要将它们调整到84 x 84的大小，并将它们转换为256级灰度。这意味着我们将有256^(84x84x4)，大约是10^(67970)种可能的游戏状态。这意味着在我们的想象中的Q表中将有10^(67970)行，这是一个很大的数字。你可以争论说，许多像素组合或状态从未发生，所以我们可能将其表示为稀疏矩阵。这个稀疏矩阵只包含已访问的状态。然而，大多数状态很少被访问。因此，Q表的收敛需要很长时间。坦白说，我们也希望为代理从未见过的状态提供一个好的Q值猜测，这样我们就可以为游戏代理生成一个合理的动作。这就是深度学习介入的地方。
- en: 'Step 3: Neural networks are quite good for generating good features for highly
    structured data. With the help of the neural network, we can represent our Q-function.
    This neural network takes the states, which means four game screens and actions,
    as input and generates the corresponding Q-value as output. *Alternatively, we
    could take only game screens as the input and generate the Q-value for each possible
    action as output.* This approach has a great advantage. Let me explain. There
    are two major things that we are doing here. First, we need to obtain the updated
    Q-value. Second, we need to pick up the action with the highest Q-value.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步：神经网络非常适合为高度结构化的数据生成良好的特征。借助神经网络，我们可以表示我们的Q函数。这个神经网络将状态（即四个游戏屏幕和动作）作为输入，并生成相应的Q值作为输出。*或者，我们也可以只将游戏屏幕作为输入，并为每个可能的动作生成Q值作为输出。*
    这种方法有一个很大的优点。让我解释一下。我们在这里做了两件主要的事情。首先，我们需要获得更新的Q值。其次，我们需要选择具有最高Q值的动作。
- en: So if we have Q-values for all possible actions, then we can update the Q-value
    easily. We can also pick the action with the highest Q-value with a lot of ease.
    The interesting part is that we can generate the Q-values for all actions by performing
    a forward pass through the network. After a single forward pass, we can have a
    list of Q-values for all possible actions with us. This forward pass will save
    a lot of time and give the gaming agent good rewards.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们有所有可能动作的Q值，那么我们可以很容易地更新Q值。我们也可以很轻松地选择具有最高Q值的动作。有趣的部分是，我们可以通过执行网络的前向传递来生成所有动作的Q值。经过单次前向传递，我们可以拥有所有可能动作的Q值列表。这次前向传递将节省大量时间，并为游戏代理提供良好的奖励。
- en: Architecture of DQN
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DQN架构
- en: 'You can find the optimal architecture of a deep Q-network represented in the
    following diagram:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下图表中找到深度Q网络的理想架构：
- en: '![Architecture of DQN](img/B08394_11_13.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![DQN架构](img/B08394_11_13.jpg)'
- en: 'Figure 11.13: Architecture of DQN'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13：DQN架构
- en: 'The preceding architecture is used and published in a DeepMind paper. The architecture
    for the neural network is shown in the following screenshot:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 上述架构已在DeepMind论文中使用并发表。神经网络架构在下面的屏幕截图中显示：
- en: '![Architecture of DQN](img/B08394_11_14.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![DQN架构](img/B08394_11_14.jpg)'
- en: 'Figure 12.14: The DQN architecture'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.14：DQN架构
- en: The provided architecture uses a classic convolutional neural network (CNN).
    There are three convolutional layers followed by two fully connected layers that
    we have seen in the CNN architecture for object detection and face recognition
    CNN with pooling layers. Here, there are no pooling layers. That is because the
    main motive behind using pooling layers is that they make the neural network insensitive
    to the location. This means that if we use the pooling layer, then the placement
    of the objects in the image is not considered by the neural network. This kind
    of location insensitivity makes sense for a classification task, but for games,
    the location of the objects in a gaming environment is important. They help us
    determine the action as well as potential rewards, and we wouldn't want to discard
    this information. So, we are not using pooling layers here.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的架构使用了一个经典的卷积神经网络（CNN）。它包含三个卷积层，随后是两个全连接层，这与我们在对象检测和面部识别CNN架构中看到的带有池化层的CNN架构相同。在这里，没有使用池化层。这是因为使用池化层的主要目的是使神经网络对位置不敏感。这意味着如果我们使用池化层，那么图像中对象的放置就不会被神经网络考虑。对于分类任务来说，这种位置不敏感性是有意义的，但对于游戏来说，游戏环境中对象的放置位置很重要。它们帮助我们确定动作以及潜在的奖励，我们不想丢弃这些信息。因此，我们在这里没有使用池化层。
- en: Steps for the DQN algorithm
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DQN算法的步骤
- en: 'Let''s see the steps for DQN algorithm:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看DQN算法的步骤：
- en: '*Input of network:* Four 84 x 84 grayscale game screen pixels.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*网络输入:* 四个84 x 84的灰度游戏屏幕像素。'
- en: '*Output of network:* As output, we will generate Q-values for each possible
    action. Q- values take any real number, which means it can be any real number
    you can possibly imagine, and that makes it a regression task. We know we can
    optimize the regression function with a simple squared error loss. The equation
    of the error loss is shown in the following screenshot:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*网络输出:* 作为输出，我们将为每个可能的行为生成Q值。Q值可以是任何实数，这意味着它可以是你能想象到的任何实数，这使得它成为一个回归任务。我们知道我们可以通过简单的平方误差损失来优化回归函数。误差损失的方程式在下面的屏幕截图中显示：'
- en: '![Steps for the DQN algorithm](img/B08394_11_15.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![DQN算法的步骤](img/B08394_11_15.jpg)'
- en: 'Figure 11.15: Equation for the error loss function'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15：误差损失函数的方程式
- en: '*Q-table update step:* There''s the transition *< s, a, r, s'' >*, but this
    time, the rules for updating the Q-table are not the same as Q-learning. There
    are some changes. So, the steps for updating the Q-table are as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q表更新步骤:* 存在着过渡 *< s, a, r, s'' >*，但这次，更新Q表的规则与Q-learning不同。有一些变化。因此，更新Q表的步骤如下：'
- en: 'Step 1: We need to perform a feedforward pass for the current state *s* in
    order to get predicted Q-values for all actions.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第1步：我们需要对当前状态 *s* 执行前向传播，以获得所有动作的预测Q值。
- en: 'Step 2: Perform a feedforward pass for the next sate *s''* and calculate the
    maximum over all network output *maxa''Q(s'', a'').*'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2步：对下一个状态 *s'* 执行前向传播，并计算所有网络输出 *maxa'Q(s', a').*
- en: 'Step 3: Set a Q-value target for action *a* to *r + γmaxa''Q(s'', a'')*. Here,
    we can use the *maxa''Q(s'', a'')* value that we have already calculated in step
    2\. For all other actions, set the Q-values that are originally from step 1, making
    the error zero for those outputs.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第3步：为动作 *a* 设置Q值目标为 *r + γmaxa'Q(s', a')*。在这里，我们可以使用我们在第2步中已经计算出的 *maxa'Q(s',
    a')* 值。对于所有其他动作，设置从第1步继承的Q值，使得那些输出的误差为零。
- en: 'Step 4: We need to update the weights of the neural network using backpropagation.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第4步：我们需要使用反向传播来更新神经网络的权重。
- en: Now let's look at the concept of experience replay.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下经验回放的概念。
- en: Understanding Experience Replay
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解经验回放
- en: 'We are estimating the future rewards in each state using two concepts. We use
    Q-learning and approximate the Q-function using a convolutional neural network.
    Here, the approximation of Q-values is done using a nonlinear function, and this
    function is not very stable for converging the model. So, we need to experiment
    with various hyperparameters. This takes a long time: almost a week on a single
    GPU to train the gaming bot.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用两个概念来估计每个状态的未来奖励。我们使用Q-learning，并使用卷积神经网络来近似Q函数。在这里，Q值的近似是通过一个非线性函数完成的，而这个函数对于模型收敛来说并不非常稳定。因此，我们需要尝试各种超参数。这需要很长时间：在单个GPU上几乎需要一周的时间来训练游戏机器人。
- en: We will be using a concept called experience replay. During the training, all
    the experiences *< s, a, r, s' >* are stored in a replay memory. When we perform
    training, the network will use random samples from the replay memory instead of
    the most recent transition. This way, the training time will be less, plus there
    is another advantage. With the help of the experience replay, our training task
    will become more similar to the usual supervised learning. Now we can easily perform
    debugging and testing operations for the algorithm. With the help of the replay
    memory, we can store all our human experiences of gameplay and then train the
    model based on this dataset.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个称为经验回放的概念。在训练过程中，所有经验 *< s, a, r, s' >* 都存储在回放记忆中。当我们进行训练时，网络将使用回放记忆中的随机样本而不是最近的转换。这样，训练时间会更短，而且还有一个优点。借助经验回放，我们的训练任务将更类似于常规的监督学习。现在我们可以轻松地对算法进行调试和测试操作。借助回放记忆，我们可以存储所有我们的游戏人类经验，然后根据这个数据集来训练模型。
- en: 'So, the steps for the final Q-learning algorithm used in DQN will be as follows.
    This algorithm takes from the original DQN paper, which is available at [https://arxiv.org/pdf/1312.5602.pdf](https://arxiv.org/pdf/1312.5602.pdf):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最终在DQN中使用的Q-learning算法的步骤如下。此算法来自原始的DQN论文，该论文可在[https://arxiv.org/pdf/1312.5602.pdf](https://arxiv.org/pdf/1312.5602.pdf)找到：
- en: 'Step 1: We need to initialize the replay memory D'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第1步：我们需要初始化回放记忆D
- en: 'Step 2: We need to initialize the action-value function Q with random weights'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2步：我们需要用随机权重初始化动作值函数Q
- en: 'Step 3: Observe value of the initial states'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第3步：观察初始状态值
- en: 'Step 4: Repeat'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第4步：重复
- en: '[PRE2]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We are using Q-learning and DQN to implement the Space Invaders gaming bot.
    So let's start coding.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用Q-learning和DQN来实现空间入侵者游戏机器人。所以，让我们开始编码。
- en: Implementing the Space Invaders gaming bot
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现空间入侵者游戏机器人
- en: 'In this section, we will be coding the Space Invaders game using DQN and Q-learning.
    For coding, we will be using the `gym`, `TensorFlow`, and `virtualenv` libraries.
    You can refer to the entire code by using this GitHub link: [https://github.com/jalajthanaki/SpaceInvaders_gamingbot](https://github.com/jalajthanaki/SpaceInvaders_gamingbot).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用DQN和Q-learning来编写空间入侵者游戏。对于编码，我们将使用`gym`、`TensorFlow`和`virtualenv`库。你可以通过使用这个GitHub链接查看整个代码：[https://github.com/jalajthanaki/SpaceInvaders_gamingbot](https://github.com/jalajthanaki/SpaceInvaders_gamingbot)。
- en: 'We are using a convolutional neural network (CNN). Here, we have defined the
    CNN in a separate file. The name of this file is `convnet.py`. Take a look at
    the following screenshot: at the following figure:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用卷积神经网络（CNN）。在这里，我们已经在单独的文件中定义了CNN。这个文件的名称是`convnet.py`。请看下面的截图：以下图：
- en: '![Implementing the Space Invaders gaming bot](img/B08394_11_16.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![实现空间入侵者游戏机器人](img/B08394_11_16.jpg)'
- en: 'Figure 11.16: Code snippet for Convnrt.py'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.16：`convnet.py`的代码片段
- en: 'You can refer to the code using this GitHub link: [https://github.com/jalajthanaki/SpaceInvaders_gamingbot/blob/master/convnet.py](https://github.com/jalajthanaki/SpaceInvaders_gamingbot/blob/master/convnet.py).'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过这个GitHub链接查看代码：[https://github.com/jalajthanaki/SpaceInvaders_gamingbot/blob/master/convnet.py](https://github.com/jalajthanaki/SpaceInvaders_gamingbot/blob/master/convnet.py)。
- en: 'We are defining the DQN algorithm in the `dqn.py` script. You can refer to
    the code snippet shown in the following screenshot:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`dqn.py`脚本中定义了DQN算法。你可以参考以下截图中的代码片段：
- en: '![Implementing the Space Invaders gaming bot](img/B08394_11_17.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![实现空间入侵者游戏机器人](img/B08394_11_17.jpg)'
- en: 'Figure 11.17: Code snippet for dqn.py'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.17：`dqn.py`的代码片段
- en: 'For training, we have defined our training logic in `train.py`. You can refer
    to the code snippet shown in the following screenshot:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，我们已经在`train.py`中定义了我们的训练逻辑。你可以参考以下截图中的代码片段：
- en: '![Implementing the Space Invaders gaming bot](img/B08394_11_18.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![实现空间入侵者游戏机器人](img/B08394_11_18.jpg)'
- en: 'Figure 11.18: Code snippet for train.py'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.18：train.py的代码片段
- en: 'At last, we import all these separate scripts to the main `atari.py` script,
    and in that script, we define all the parameter values. You can refer to the code
    snippet given in the following screenshot:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将所有这些独立的脚本导入到主`atari.py`脚本中，在该脚本中，我们定义了所有参数值。您可以参考以下截图中的代码片段：
- en: '![Implementing the Space Invaders gaming bot](img/B08394_11_19.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![实现空间入侵者游戏机器人](img/B08394_11_19.jpg)'
- en: 'Figure 11.19: Code snippet for atari.py'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.19：atari.py的代码片段
- en: 'You can start training by executing the following command:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过执行以下命令开始训练：
- en: '`$ python atari.py --game SpaceInvaders-v0 --display true`'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ python atari.py --game SpaceInvaders-v0 --display true`'
- en: 'Training this bot to pass human level performance requires at least 3-4 days
    of training. I have not provided that amount of training, but you can definitely
    do that. You can take a look at the output of the training in the following screenshot:
    the following figure:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这个机器人以达到人类水平的表现至少需要3-4天的训练。我没有提供那么多的训练，但您肯定可以做到。您可以查看以下截图中的训练输出：以下图：
- en: '![Implementing the Space Invaders gaming bot](img/B08394_11_20.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![实现空间入侵者游戏机器人](img/B08394_11_20.jpg)'
- en: 'Figure 11.20: Output snippet of training step – I'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.20：训练步骤的输出片段
- en: 'You can refer to the code snippet for the gaming environment initial score
    by referring to the following screenshot:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过参考以下截图来查看游戏环境初始得分的代码片段：
- en: '![Implementing the Space Invaders gaming bot](img/B08394_11_21.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![实现空间入侵者游戏机器人](img/B08394_11_21.jpg)'
- en: 'Figure 11.21: Code snippet of the score for the initial few games from the
    gaming bot'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.21：游戏机器人前几场比赛得分的代码片段
- en: 'To stop the training, there will be two parameters: either we can end our training
    when our loss function value becomes constant for a few iterations, or we complete
    all the training steps. Here, we have defined 50,000 training steps. You can refer
    to the code snippet of the output of training in the following screenshot :'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止训练，将有两个参数：要么当我们的损失函数值在几次迭代中变为常数时结束训练，要么完成所有训练步骤。在这里，我们定义了50,000个训练步骤。您可以参考以下截图中的训练输出代码片段：
- en: '![Implementing the Space Invaders gaming bot](img/B08394_11_22.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![实现空间入侵者游戏机器人](img/B08394_11_22.jpg)'
- en: 'Figure 11.22: Code snippet for the training log'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.22：训练日志的代码片段
- en: 'You can see the score of the gaming bot after 1,000 iterations by taking a
    look at the following screenshot:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过查看以下截图来查看游戏机器人经过1,000次迭代后的得分：
- en: '![Implementing the Space Invaders gaming bot](img/B08394_11_23.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![实现空间入侵者游戏机器人](img/B08394_11_23.jpg)'
- en: 'Figure 11.23: Code snippet for the gaming bot after 1,000 iterations'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.23：经过1,000次迭代的游戏机器人的代码片段
- en: 'I have already upload the pre-trained model for you. You can download it by
    using this GitHub link: [https://github.com/jalajthanaki/SpaceInvaders_gamingbot/tree/master/model](https://github.com/jalajthanaki/SpaceInvaders_gamingbot/tree/master/model).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经为您上传了预训练模型。您可以通过使用此GitHub链接下载它：[https://github.com/jalajthanaki/SpaceInvaders_gamingbot/tree/master/model](https://github.com/jalajthanaki/SpaceInvaders_gamingbot/tree/master/model)。
- en: Now it's time to build the gaming bot for the Pong game. If you train this bot
    for a week using a single GPU, it can beat the AI rules that are written by the
    gaming manufacture team. So, our agent will surely act better than the computer
    agent.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候构建乒乓球游戏的机器人了。如果您使用单个GPU训练这个机器人一周，它可以击败游戏制造商团队编写的AI规则。因此，我们的智能体肯定会比计算机智能体表现得更好。
- en: Building the Pong gaming bot
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建乒乓球游戏机器人
- en: In this section, we will be looking at how we can build a gaming bot that can
    learn the game of Pong. Before we start, we will look at the approach and concepts
    that we will be using for building the Pong gaming bot.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何构建一个能够学习乒乓球游戏的游戏机器人。在我们开始之前，我们将查看构建乒乓球游戏机器人所使用的方法和概念。
- en: Understanding the key concepts
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解关键概念
- en: 'In this section, we will be covering some aspects of building the Pong game
    bot, which are as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍构建乒乓球游戏机器人的一些方面，具体如下：
- en: Architecture of the gaming bot
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏机器人的架构
- en: Approach for the gaming bot
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏机器人的方法
- en: Architecture of the gaming bot
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 游戏机器人的架构
- en: 'In order to develop the Pong gaming bot, we are choosing a neural-network-based
    approach. The architecture of our neural network is crucial. Let''s look at the
    architectural components step by step:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发乒乓球游戏机器人，我们选择了一个基于神经网络的方案。我们神经网络的架构至关重要。让我们一步一步地查看架构组件：
- en: We take the gaming screen as the input and preprocess it as per the DQN algorithm.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将游戏屏幕作为输入，并按照 DQN 算法对其进行预处理。
- en: We pass this preprocessed screen to an neural network (NN.)
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将预处理后的屏幕传递给一个神经网络 (NN)。
- en: We use a gradient descent to update the weights of the NN.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用梯度下降来更新 NN 的权重。
- en: 'Weight [1]: This matrix holds the weights of pixels passing into the hidden
    layer. The dimension will be [200 x 80 x 80] – [200 x 6400].'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '权重 [1]: 此矩阵持有传递到隐藏层的像素权重。维度将是 [200 x 80 x 80] – [200 x 6400]。'
- en: 'Weight [2]: This matrix holds the weights of the hidden layer passing into
    the output. The dimension will be [1 x 200].'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '权重 [2]: 此矩阵持有传递到输出的隐藏层权重。维度将是 [1 x 200]。'
- en: 'You can refer to the following diagram:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下图表：
- en: '![Architecture of the gaming bot](img/B08394_11_24.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![游戏机器人架构](img/B08394_11_24.jpg)'
- en: 'Figure 11.24: Architecture of NN for the Pong gaming bot'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.24：Pong 游戏机器人的神经网络架构
- en: The tasks for each component of the NN make more sense when we see the detailed
    approach for this gaming bot.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们看到这个游戏机器人的详细方法时，NN 的每个组件的任务更有意义。
- en: Approach for the gaming bot
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 游戏机器人的方法
- en: 'In order to build the Pong gaming bot, we will be using the following approach:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建 Pong 游戏机器人，我们将采用以下方法：
- en: For implementation, we are using the preprocessed image vector, which is a [6400
    x 1] dimension array.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于实现，我们使用预处理后的图像向量，它是一个 [6400 x 1] 维度数组。
- en: With the help of NN, we can compute a probability of moving up.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在神经网络 (NN) 的帮助下，我们可以计算向上移动的概率。
- en: With the help of that probability distribution, we will decide whether the agent
    is moving up or not.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在那个概率分布的帮助下，我们将决定智能体是否向上移动。
- en: If the gaming round is over, it means that the gaming agent as well as the opponent
    missed the ball. In this case, we need to find out whether our gaming agent won
    or lost.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果游戏回合结束，这意味着游戏智能体以及对手都错过了球。在这种情况下，我们需要找出我们的游戏智能体是赢了还是输了。
- en: When the episode finishes, which means if either of the players scores 21 points,
    we need to pass the result. With the help of the loss function, we can find out
    the error values. We applied the gradient descent algorithm to find out the direction
    in which our neural network's weight should be updated. Based on the backpropagation
    algorithm, we propagate the error back to the network so that our network can
    update the weights.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当剧集结束时，这意味着如果任何一名玩家得分达到 21 分，我们需要传递结果。借助损失函数，我们可以找到错误值。我们应用梯度下降算法来找出我们的神经网络权重应该更新的方向。基于反向传播算法，我们将错误传播回网络，以便我们的网络可以更新权重。
- en: Once 10 episodes have finished, we need to sum up the gradient, and after that,
    we update the weights in the direction of the gradient.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦完成 10 个剧集，我们需要汇总梯度，然后，我们沿着梯度的方向更新权重。
- en: Repeat this process until our networks weights are tuned and we can beat the
    computer.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复此过程，直到我们的网络权重调整完毕，我们可以击败电脑。
- en: Now let's cover the coding steps.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来介绍编码步骤。
- en: Implementing the Pong gaming bot
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 Pong 游戏机器人
- en: 'These are the implementation steps that we need to follow:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们需要遵循的实现步骤：
- en: Initialization of the parameters
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数初始化
- en: Weights stored in the form of matrices
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以矩阵形式存储的权重
- en: Updating weights
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新权重
- en: How to move the agent
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何移动智能体
- en: Understanding the process using NN
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络理解过程
- en: 'You can refer to the entire code by using this GitHub link: [https://github.com/jalajthanaki/Atari_Pong_gaming_bot](https://github.com/jalajthanaki/Atari_Pong_gaming_bot).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用此 GitHub 链接来参考整个代码：[https://github.com/jalajthanaki/Atari_Pong_gaming_bot](https://github.com/jalajthanaki/Atari_Pong_gaming_bot)。
- en: Initialization of the parameters
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数初始化
- en: 'First, we define and initialize our parameters:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义并初始化我们的参数：
- en: '`batch_size:` This parameter indicates how many rounds of games we should play
    before updating the weights of our network.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size:` 此参数表示在更新我们网络的权重之前应该玩多少轮游戏。'
- en: '`gamma:` This is the discount factor. We use this to discount the effect of
    old actions of the game on the final result.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma:` 这是折扣因子。我们使用它来折扣游戏旧动作对最终结果的影响。'
- en: '`decay_rate:` This parameter is used to update the weight.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decay_rate:` 此参数用于更新权重。'
- en: '`num_hidden_layer_neurons:` This parameter indicates how many neurons we should
    put in the hidden layer.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layer_neurons:` 此参数表示我们应该在隐藏层中放置多少个神经元。'
- en: '`learning_rate:` This is the speed at which our gaming agent learns from the
    results so that we can compute new weights. A higher learning rate means we react
    more strongly to results, and a lower rate means we don''t react much to each
    result.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`学习率:` 这是我们游戏代理从结果中学习的速度，以便我们可以计算新的权重。学习率越高，我们对外部结果的反应越强烈，而学习率越低，我们对外部结果的反应就越不强烈。'
- en: 'You can refer to the code snippet shown in the following screenshot:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下截图所示的代码片段：
- en: '![Initialization of the parameters](img/B08394_11_25.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![参数初始化](img/B08394_11_25.jpg)'
- en: 'Figure 11.25: Initialization of parameters'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.25：参数初始化
- en: Weights stored in the form of matrices
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 以矩阵形式存储的权重
- en: The weights of the neural network are stored in the form of matrices. The first
    layer of NN is a 200 x 6400 matrix that represents the weights for our hidden
    layer. If we use the notation *w1_ij*, then that would mean that we are representing
    the weight of the *i^(th)* neuron for the input pixel *j* in layer 1\. The second
    layer is a 200 x 1 matrix representing the weights. These weights are the output
    of the hidden layer. For layer 2, element *w2_i* indicates the weights placed
    on the activation of the *i^(th)* neuron in the hidden layer.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的权重以矩阵的形式存储。NN的第一层是一个200 x 6400的矩阵，表示隐藏层的权重。如果我们使用符号*w1_ij*，那么这意味着我们正在表示第*i^(th)*个神经元在层1中对输入像素*j*的权重。第二层是一个200
    x 1的矩阵，表示权重。这些权重是隐藏层的输出。对于层2，元素*w2_i*表示放置在隐藏层第*i^(th)*个神经元激活上的权重。
- en: 'You can refer to the code snippet given in the following screenshot:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下截图给出的代码片段：
- en: '![Weights stored in the form of matrices](img/B08394_11_26.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![以矩阵形式存储的权重](img/B08394_11_26.jpg)'
- en: 'Figure 11.26: Weight matrices'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.26：权重矩阵
- en: Updating weights
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新权重
- en: 'For updating the weight, we will be using RMSprop. You can refer to this paper
    in order to understand more details about this function:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更新权重，我们将使用RMSprop。您可以参考以下论文以了解有关此函数的更多详细信息：
- en: '[http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop](http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop).
    Refer to the following figure for the following figure.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop](http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop)。请参考以下图示。'
- en: '![Updating weights](img/B08394_11_27.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![更新权重](img/B08394_11_27.jpg)'
- en: 'Figure 11.27: Equation for RMSprop'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.27：RMSprop方程
- en: 'The code is shown in the following screenshot:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 代码显示在以下截图：
- en: '![Updating weights](img/B08394_11_28.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![更新权重](img/B08394_11_28.jpg)'
- en: 'Figure 11.28: Code snippet for updating weights'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.28：更新权重的代码片段
- en: How to move the agent
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何移动代理
- en: 'With the help of the preprocessed input, we pass the weight matrix to the neural
    network. We need to generate the probability of telling our agent to move up.
    You can refer to the code snippet shown in the following screenshot:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在预处理输入的帮助下，我们将权重矩阵传递给神经网络。我们需要生成告诉代理向上移动的概率。您可以参考以下截图所示的代码片段：
- en: '![How to move the agent](img/B08394_11_29.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![如何移动代理](img/B08394_11_29.jpg)'
- en: 'Figure 11.29: Code snippet to move the agent'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.29：移动代理的代码片段
- en: We are done with all the major helper functions. We need to apply all this logic
    to the neural network so that it can take the observation and generate the probability
    of our gaming agent for going in upward direction.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了所有主要的辅助函数。我们需要将这些逻辑应用到神经网络中，以便它能够根据观察结果生成游戏代理向上移动的概率。
- en: Understanding the process using NN
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用神经网络理解过程
- en: These are the steps that can help us generate the probability for our agent
    so that it can decide when they should move in upward direction
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤可以帮助我们生成代理向上移动的概率，以便它们可以决定何时向上移动
- en: We need to compute hidden layer values by applying the dot product between weights
    [1] and `observation_matrix`. Weight [1] is a 200 x 6400 matrix and `observation_matrix`
    is a 6400 x 1 matrix. The dimension of the output matrix is 200 x 1\. Here, we
    are using 200 neurons. Each row of Q-function represents the output of one neuron.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要通过将权重[1]和`observation_matrix`之间的点积来计算隐藏层值。权重[1]是一个200 x 6400的矩阵，而`observation_matrix`是一个6400
    x 1的矩阵。输出矩阵的维度是200 x 1。在这里，我们使用了200个神经元。Q函数的每一行代表一个神经元的输出。
- en: We apply a nonlinear function ReLU to the hidden layer values.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将对隐藏层值应用非线性函数ReLU。
- en: We are using hidden layer activation values in order to calculate the values
    for the output layer. Again, we performed dot product between `hidden_layer_values`
    [200 x 1] and weights [2] [1 x 200]. This dot product gives us single value [1
    x 1].
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在使用隐藏层的激活值来计算输出层的值。再次，我们在`hidden_layer_values` [200 x 1]和权重[2] [1 x 200]之间执行点积。这个点积给我们一个单一值[1
    x 1]。
- en: Finally, we apply the sigmoid function to the output value. This will give us
    the answer in terms of probability. The value of the output is between 0 and 1.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将sigmoid函数应用于输出值。这将给出一个关于概率的答案。输出值介于0和1之间。
- en: 'You can refer to the code snippet shown in the following screenshot:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下屏幕截图中的代码片段：
- en: '![Understanding the process using NN](img/B08394_11_30.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![使用神经网络理解过程](img/B08394_11_30.jpg)'
- en: 'Figure 11.30: Code snippet for the process happens using NN'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.30：使用神经网络的过程代码片段
- en: 'To run this code, you need to execute the following command:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此代码，你需要执行以下命令：
- en: '[PRE3]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you want to build a bot that can beat the computer, then you need to train
    it for at least three to four days on a single GPU. You can refer to the output
    of the bot in the following screenshot:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要构建一个能够打败电脑的机器人，那么你至少需要在单个GPU上训练三天到四天。你可以在以下屏幕截图中参考机器人的输出：
- en: '![Understanding the process using NN](img/B08394_11_31.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![使用神经网络理解过程](img/B08394_11_31.jpg)'
- en: 'Figure 11.31: The Pong gaming bot output'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.31：Pong游戏机器人的输出
- en: 'You can see the training log in the following screenshot:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下屏幕截图中看到训练日志：
- en: '![Understanding the process using NN](img/B08394_11_32.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![使用神经网络理解过程](img/B08394_11_32.jpg)'
- en: 'Figure 11.32: Training log for Pong gaming bot'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.32：Pong游戏机器人的训练日志
- en: Now let's build a gaming bot just for fun. This bot uses the Flappy Bird gaming
    environment.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们仅作娱乐地构建一个游戏机器人。这个机器人使用Flappy Bird游戏环境。
- en: Just for fun - implementing the Flappy Bird gaming bot
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仅作娱乐 - 实现Flappy Bird游戏机器人
- en: 'In this section, we will be building the Flappy Bird gaming bot. This gaming
    bot has been built using DQN. You can find the entire code at this GitHub link:
    [https://github.com/jalajthanaki/DQN_FlappyBird](https://github.com/jalajthanaki/DQN_FlappyBird).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建Flappy Bird游戏机器人。这个游戏机器人是使用DQN构建的。你可以在以下GitHub链接中找到完整的代码：[https://github.com/jalajthanaki/DQN_FlappyBird](https://github.com/jalajthanaki/DQN_FlappyBird)。
- en: 'This bot has a pre-trained model, so you test it using the pre-trained model.
    In order to run this bot, you need to execute this command:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这个机器人有一个预训练的模型，所以你可以使用预训练的模型来测试它。为了运行这个机器人，你需要执行以下命令：
- en: '[PRE4]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can see the output in the following screenshot:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下屏幕截图中看到输出：
- en: '![Just for fun - implementing the Flappy Bird gaming bot](img/B08394_11_33.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![仅作娱乐 - 实现Flappy Bird游戏机器人](img/B08394_11_33.jpg)'
- en: 'Figure 11.33: Output of the Flappy Bird gaming bot'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.33：Flappy Bird游戏机器人的输出
- en: You can see the combination of all the concepts that we have studied so far
    in this implementation, so make sure you explore this code. Consider this your
    exercise for the chapter.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这次实现中看到我们迄今为止所研究的所有概念的组合，所以请确保你探索这段代码。这将是本章的练习。
- en: Summary
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Congratulations, readers; you have made it to the end! We covered basic concepts
    related to reinforcement learning in this chapter. You learned about the various
    concepts and algorithms of building the gaming bot. You also learned how the Deep
    Q Learner algorithm works. Using the `gym` library, we loaded the gaming world.
    By using the `dqn` library, we will be able to train the model. Training a gaming
    bot that can defeat human level experts takes a lot of time. So, I trained it
    for a few hours only. If you want to train for more hours, you can definitely
    do that. We tried to build a variety of simple Atari games, such as a simple pathfinder
    gaming bot, Space Invaders, Pong, and Flappy Bird. You can expand this basic approach
    to the bigger gaming environment. If you want to get yourself updated and contribute,
    then you can take a look at the OpenAI GitHub repository at: [https://github.com/openai](https://github.com/openai).
    Deep Mind news and the blog section are at this link: [https://deepmind.com/blog/](https://deepmind.com/blog/)
    .'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜读者们；你们已经到达了本章的结尾！我们在这章中介绍了与强化学习相关的基本概念。你学习了构建游戏机器人的各种概念和算法。你还了解了深度Q学习算法是如何工作的。使用`gym`库，我们加载了游戏世界。通过使用`dqn`库，我们将能够训练模型。训练一个能够击败人类级别专家的游戏机器人需要花费大量时间。所以，我只训练了几个小时。如果你想训练更长的时间，你绝对可以做到。我们尝试构建了各种简单的Atari游戏，例如简单的路径寻找游戏机器人、太空侵略者、乒乓球和Flappy
    Bird。你可以将这种基本方法扩展到更大的游戏环境中。如果你想更新自己并做出贡献，那么你可以查看OpenAI的GitHub仓库：[https://github.com/openai](https://github.com/openai)。深度学习的新闻和博客部分可以在以下链接找到：[https://deepmind.com/blog/](https://deepmind.com/blog/)。
- en: In the following section, you will find an appendix that can help you gain some
    extra information. This extra information will help you when you are building
    Machine Learning (ML) applications or taking part in a hackathon or other competitions.
    I have also provided some cheat sheets that can help you when you are building
    ML applications.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，你将找到一个附录，可以帮助你获得一些额外信息。这些额外信息将帮助你在构建机器学习（ML）应用或参加黑客马拉松或其他竞赛时。我还提供了一些可以帮助你在构建ML应用时的速查表。
