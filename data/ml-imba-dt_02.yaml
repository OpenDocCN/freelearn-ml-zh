- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Oversampling Methods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿‡é‡‡æ ·æ–¹æ³•
- en: In machine learning, we often donâ€™t have enough samples of the minority class.
    One solution might be to gather more samples of such a class. For example, in
    the problem of detecting whether a patient has cancer or not, if we donâ€™t have
    enough samples of the cancer class, we can wait for some time to gather more samples.
    However, such a strategy is not always feasible or sensible and can be time-consuming.
    In such cases, we can augment our data by using various techniques. One such technique
    is oversampling.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å¾€å¾€æ²¡æœ‰è¶³å¤Ÿçš„å°‘æ•°ç±»æ ·æœ¬ã€‚ä¸€ä¸ªå¯èƒ½çš„è§£å†³æ–¹æ¡ˆå¯èƒ½æ˜¯æ”¶é›†æ­¤ç±»æ›´å¤šçš„æ ·æœ¬ã€‚ä¾‹å¦‚ï¼Œåœ¨æ£€æµ‹æ‚£è€…æ˜¯å¦æ‚£æœ‰ç™Œç—‡çš„é—®é¢˜ä¸­ï¼Œå¦‚æœæˆ‘ä»¬æ²¡æœ‰è¶³å¤Ÿçš„ç™Œç—‡ç±»æ ·æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥ç­‰å¾…ä¸€æ®µæ—¶é—´æ¥æ”¶é›†æ›´å¤šæ ·æœ¬ã€‚ç„¶è€Œï¼Œè¿™ç§ç­–ç•¥å¹¶ä¸æ€»æ˜¯å¯è¡Œæˆ–æ˜æ™ºï¼Œå¹¶ä¸”å¯èƒ½è€—æ—¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨å„ç§æŠ€æœ¯æ¥å¢å¼ºæˆ‘ä»¬çš„æ•°æ®ã€‚å…¶ä¸­ä¸€ç§æŠ€æœ¯å°±æ˜¯è¿‡é‡‡æ ·ã€‚
- en: In this chapter, we will introduce the concept of oversampling, discuss when
    to use it, and the various techniques to perform it. We will also demonstrate
    how to utilize these techniques through the `imbalanced-learn` library APIs and
    compare their performance using some classical machine learning models. Finally,
    we will conclude with some practical advice on which techniques tend to work best
    under specific real-world conditions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»è¿‡é‡‡æ ·çš„æ¦‚å¿µï¼Œè®¨è®ºä½•æ—¶ä½¿ç”¨å®ƒï¼Œä»¥åŠæ‰§è¡Œå®ƒçš„å„ç§æŠ€æœ¯ã€‚æˆ‘ä»¬è¿˜å°†é€šè¿‡`imbalanced-learn`åº“çš„APIæ¼”ç¤ºå¦‚ä½•åˆ©ç”¨è¿™äº›æŠ€æœ¯ï¼Œå¹¶ä½¿ç”¨ä¸€äº›ç»å…¸çš„æœºå™¨å­¦ä¹ æ¨¡å‹æ¯”è¾ƒå®ƒä»¬çš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ€»ç»“ä¸€äº›å®é™…å»ºè®®ï¼Œè¯´æ˜å“ªäº›æŠ€æœ¯åœ¨ç‰¹å®šç°å®ä¸–ç•Œæ¡ä»¶ä¸‹æ•ˆæœæœ€ä½³ã€‚
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: Random oversampling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éšæœºè¿‡é‡‡æ ·
- en: SMOTE
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SMOTE
- en: SMOTE variants
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SMOTEå˜ä½“
- en: ADASYN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ADASYN
- en: Model performance comparison of various oversampling methods
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å„ç§è¿‡é‡‡æ ·æ–¹æ³•çš„æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ
- en: Guidance for using various oversampling techniques
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å„ç§è¿‡é‡‡æ ·æŠ€æœ¯çš„æŒ‡å—
- en: Oversampling in multi-class classification
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤šç±»åˆ†ç±»ä¸­çš„è¿‡é‡‡æ ·
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŠ€æœ¯è¦æ±‚
- en: In this chapter, we will utilize common libraries such as `numpy`, `scikit-learn`,
    and `imbalanced-learn`. The code and notebooks for this chapter are available
    on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter02](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter02).
    You can just fire up the GitHub notebook using Google Colab by clicking on the
    **Open in Colab** icon at the top of this chapterâ€™s notebook or by launching it
    from [https://colab.research.google.com](https://colab.research.google.com) using
    the GitHub URL of the notebook.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¦‚`numpy`ã€`scikit-learn`å’Œ`imbalanced-learn`ç­‰å¸¸ç”¨åº“ã€‚æœ¬ç« çš„ä»£ç å’Œç¬”è®°æœ¬å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼Œç½‘å€ä¸º[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter02](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter02)ã€‚æ‚¨åªéœ€ç‚¹å‡»æœ¬ç« ç¬”è®°æœ¬é¡¶éƒ¨çš„**åœ¨Colabä¸­æ‰“å¼€**å›¾æ ‡ï¼Œæˆ–é€šè¿‡[https://colab.research.google.com](https://colab.research.google.com)ä½¿ç”¨ç¬”è®°æœ¬çš„GitHub
    URLå¯åŠ¨ï¼Œå³å¯å¯åŠ¨GitHubç¬”è®°æœ¬ã€‚
- en: What is oversampling?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯è¿‡é‡‡æ ·ï¼Ÿ
- en: '**Sampling** involves selecting a subset of observations from a larger set
    of observations. In this chapter, weâ€™ll initially focus on binary classification
    problems with two classes: the positive class and the negative class. The minority
    class has significantly fewer instances than the majority class. Later in this
    chapter, we will explore multi-class classification problems. Toward the end of
    this chapter, we will look into oversampling for multi-class classification problems.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**é‡‡æ ·**æ¶‰åŠä»æ›´å¤§çš„è§‚å¯Ÿé›†ä¸­é€‰æ‹©è§‚å¯Ÿå­é›†ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬æœ€åˆå°†å…³æ³¨å…·æœ‰ä¸¤ä¸ªç±»åˆ«çš„äºŒåˆ†ç±»é—®é¢˜ï¼šæ­£ç±»å’Œè´Ÿç±»ã€‚å°‘æ•°ç±»çš„å®ä¾‹æ•°é‡æ˜¾è‘—å°‘äºå¤šæ•°ç±»ã€‚åœ¨æœ¬ç« çš„åé¢éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¤šç±»åˆ†ç±»é—®é¢˜ã€‚åœ¨æœ¬ç« çš„ç»“å°¾ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¤šç±»åˆ†ç±»é—®é¢˜çš„è¿‡é‡‡æ ·ã€‚'
- en: '**Oversampling** is a data balancing technique that generates more samples
    of the minority class. However, this can be easily scaled to work for any number
    of classes where there are multiple classes with an imbalance. *Figure 2**.1*
    shows how samples of minority and majority classes are imbalanced (**a**) initially
    and balanced (**b**) after applying an oversampling technique:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¿‡é‡‡æ ·**æ˜¯ä¸€ç§æ•°æ®å¹³è¡¡æŠ€æœ¯ï¼Œå®ƒä¸ºå°‘æ•°ç±»ç”Ÿæˆæ›´å¤šçš„æ ·æœ¬ã€‚ç„¶è€Œï¼Œè¿™å¯ä»¥å¾ˆå®¹æ˜“åœ°æ‰©å±•åˆ°é€‚ç”¨äºä»»ä½•æœ‰å¤šä¸ªç±»åˆ«ä¸”å­˜åœ¨ä¸å¹³è¡¡çš„ç±»åˆ«ã€‚*å›¾2.1*æ˜¾ç¤ºäº†åœ¨åº”ç”¨è¿‡é‡‡æ ·æŠ€æœ¯ä¹‹å‰ï¼Œå°‘æ•°ç±»å’Œå¤šæ•°ç±»çš„æ ·æœ¬æ˜¯ä¸å¹³è¡¡çš„ï¼ˆ**a**ï¼‰ä»¥åŠä¹‹åå¹³è¡¡çš„æƒ…å†µï¼ˆ**b**ï¼‰ï¼š'
- en: '![](img/B17259_02_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_01.jpg)'
- en: Figure 2.1 â€“ An increase in the number of minority class samples after oversampling
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.1 â€“ è¿‡é‡‡æ ·åå°‘æ•°ç±»æ ·æœ¬æ•°é‡çš„å¢åŠ 
- en: '*Why is oversampling needed*, you ask? It is required so that we give the model
    enough samples of the minority class to learn from it. If we offer too few instances
    of the minority class, the model may choose to ignore these minority class examples
    and focus solely on the majority class examples. This, in turn, would lead to
    the model not being able to learn the decision boundary well.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ä¼šé—®ï¼šâ€œä¸ºä»€ä¹ˆéœ€è¦è¿‡é‡‡æ ·ï¼Ÿâ€è¿™æ˜¯å¿…è¦çš„ï¼Œä»¥ä¾¿æˆ‘ä»¬ç»™æ¨¡å‹è¶³å¤Ÿçš„å°‘æ•°ç±»æ ·æœ¬æ¥ä»ä¸­å­¦ä¹ ã€‚å¦‚æœæˆ‘ä»¬æä¾›çš„å°‘æ•°ç±»å®ä¾‹å¤ªå°‘ï¼Œæ¨¡å‹å¯èƒ½ä¼šé€‰æ‹©å¿½ç•¥è¿™äº›å°‘æ•°ç±»ç¤ºä¾‹ï¼Œè€Œåªå…³æ³¨å¤šæ•°ç±»ç¤ºä¾‹ã€‚è¿™åè¿‡æ¥åˆä¼šå¯¼è‡´æ¨¡å‹æ— æ³•å¾ˆå¥½åœ°å­¦ä¹ å†³ç­–è¾¹ç•Œã€‚
- en: 'Letâ€™s generate a two-class imbalanced dataset with a 1:99 ratio using the `sklearn`
    libraryâ€™s `make_classification` API, which creates a normally distributed set
    of points for each class. This will generate an imbalanced dataset of two classes:
    one being the minority class with label 1 and the other being the majority class
    with label 0\. We will apply various oversampling techniques throughout this chapter
    to balance this dataset:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨`sklearn`åº“çš„`make_classification` APIç”Ÿæˆä¸€ä¸ª1:99æ¯”ä¾‹çš„ä¸¤ç±»ä¸å¹³è¡¡æ•°æ®é›†ï¼Œè¯¥APIä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºä¸€ä¸ªæ­£æ€åˆ†å¸ƒçš„ç‚¹é›†ã€‚è¿™å°†ç”Ÿæˆä¸¤ä¸ªç±»çš„ä¸å¹³è¡¡æ•°æ®é›†ï¼šä¸€ä¸ªæ˜¯æœ‰æ ‡ç­¾1çš„å°‘æ•°ç±»ï¼Œå¦ä¸€ä¸ªæ˜¯æœ‰æ ‡ç­¾0çš„å¤šæ•°ç±»ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†åº”ç”¨å„ç§è¿‡é‡‡æ ·æŠ€æœ¯æ¥å¹³è¡¡è¿™ä¸ªæ•°æ®é›†ï¼š
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This code generates 100 examples of class 1 and 9,900 examples of class 0 with
    an imbalance ratio of 1:99\. By plotting the dataset, we can see how the examples
    are distributed:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ®µä»£ç ç”Ÿæˆäº†100ä¸ªç±»åˆ«1çš„ä¾‹å­å’Œ9,900ä¸ªç±»åˆ«0çš„ä¾‹å­ï¼Œä¸å¹³è¡¡æ¯”ç‡ä¸º1:99ã€‚é€šè¿‡ç»˜åˆ¶æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¾‹å­æ˜¯å¦‚ä½•åˆ†å¸ƒçš„ï¼š
- en: '![](img/B17259_02_02.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_02.jpg)'
- en: Figure 2.2 â€“ The dataset with an imbalance ratio of 1:99
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.2 â€“ ä¸å¹³è¡¡æ¯”ç‡ä¸º1:99çš„æ•°æ®é›†
- en: In this section, we understood the need for oversampling. We also generated
    a synthetic imbalanced binary classification dataset to demonstrate the application
    of various oversampling techniques.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬äº†è§£äº†è¿‡é‡‡æ ·çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬è¿˜ç”Ÿæˆäº†ä¸€ä¸ªåˆæˆçš„éå¹³è¡¡äºŒåˆ†ç±»æ•°æ®é›†ï¼Œä»¥å±•ç¤ºå„ç§è¿‡é‡‡æ ·æŠ€æœ¯çš„åº”ç”¨ã€‚
- en: Random oversampling
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: éšæœºè¿‡é‡‡æ ·
- en: The simplest strategy to balance the imbalance in a dataset is to randomly choose
    samples of the minority class and repeat or duplicate them. This is also called
    **random oversampling** **with replacement**.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¹³è¡¡æ•°æ®é›†ä¸­ä¸å¹³è¡¡çš„æœ€ç®€å•ç­–ç•¥æ˜¯éšæœºé€‰æ‹©å°‘æ•°ç±»çš„æ ·æœ¬å¹¶é‡å¤æˆ–å¤åˆ¶å®ƒä»¬ã€‚è¿™ä¹Ÿè¢«ç§°ä¸º**éšæœºè¿‡é‡‡æ ·****å¸¦æ›¿æ¢**ã€‚
- en: To increase the number of minority class observations, we can replicate the
    minority class data observations enough times to balance the two classes. Does
    this sound too trivial? Yes, but it works. By increasing the number of minority
    class samples, random oversampling reduces the bias toward the majority class.
    This helps the model learn the patterns and characteristics of the minority class
    more effectively.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¢åŠ å°‘æ•°ç±»è§‚å¯Ÿçš„æ•°é‡ï¼Œæˆ‘ä»¬å¯ä»¥å¤åˆ¶å°‘æ•°ç±»æ•°æ®è§‚å¯Ÿç»“æœè¶³å¤Ÿå¤šæ¬¡ä»¥å¹³è¡¡ä¸¤ä¸ªç±»ã€‚è¿™å¬èµ·æ¥å¤ªç®€å•äº†å—ï¼Ÿæ˜¯çš„ï¼Œä½†å®ƒæ˜¯æœ‰æ•ˆçš„ã€‚é€šè¿‡å¢åŠ å°‘æ•°ç±»æ ·æœ¬çš„æ•°é‡ï¼Œéšæœºè¿‡é‡‡æ ·å‡å°‘äº†å‘å¤šæ•°ç±»çš„åå·®ã€‚è¿™æœ‰åŠ©äºæ¨¡å‹æ›´æœ‰æ•ˆåœ°å­¦ä¹ å°‘æ•°ç±»çš„æ¨¡å¼å’Œç‰¹å¾ã€‚
- en: We will use random oversampling from the `imbalanced-learn` library. The `fit_resample`
    API from the `RandomOverSampler` class resamples the original dataset and balances
    it. The `sampling_strategy` parameter is used to specify the new ratio of various
    classes. For example, we could say `sampling_strategy=1.0` to have an equal number
    of the two classes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª`imbalanced-learn`åº“çš„éšæœºè¿‡é‡‡æ ·ã€‚`RandomOverSampler`ç±»çš„`fit_resample` APIé‡æ–°é‡‡æ ·åŸå§‹æ•°æ®é›†å¹¶ä½¿å…¶å¹³è¡¡ã€‚`sampling_strategy`å‚æ•°ç”¨äºæŒ‡å®šå„ç§ç±»çš„æ–°æ¯”ç‡ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å°†`sampling_strategy=1.0`æŒ‡å®šä¸ºä¸¤ä¸ªç±»å…·æœ‰ç›¸åŒæ•°é‡çš„ä¾‹å­ã€‚
- en: 'There are various ways to specify `sampling_strategy`, such as a float value,
    string value, or `dict` â€“ for example, {0: 50, 1: 50}:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 'æœ‰å¤šç§æ–¹å¼å¯ä»¥æŒ‡å®š`sampling_strategy`ï¼Œä¾‹å¦‚æµ®ç‚¹å€¼ã€å­—ç¬¦ä¸²å€¼æˆ–`dict` â€“ ä¾‹å¦‚ï¼Œ{0: 50, 1: 50}ï¼š'
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here is the output:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯è¾“å‡ºï¼š
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: So, we went from a ratio of 1:99 to 1:1, which is what we expected with `sampling_strategy=1.0`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬ä»1:99çš„æ¯”ä¾‹å˜ä¸ºäº†1:1ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æœŸæœ›çš„`sampling_strategy=1.0`çš„ç»“æœã€‚
- en: 'Letâ€™s plot the oversampled dataset:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»˜åˆ¶è¿‡é‡‡æ ·åçš„æ•°æ®é›†ï¼š
- en: '![](img/B17259_02_03.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_03.jpg)'
- en: Figure 2.3 â€“ Dataset oversampled using RandomOverSampler (label 1 examples appear
    unchanged due to overlap)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.3 â€“ ä½¿ç”¨RandomOverSamplerï¼ˆRandomOverSamplerï¼‰è¿‡é‡‡æ ·åçš„æ•°æ®é›†ï¼ˆæ ‡ç­¾1çš„ç¤ºä¾‹ç”±äºé‡å è€Œæœªæ”¹å˜ï¼‰
- en: After applying random oversampling, the examples with label 1 overlap each other,
    creating the impression that nothing has changed. Repeating the same data point
    over and over can cause the model to memorize the specific data points and not
    be able to generalize to new, unseen examples. The `shrinkage` parameter in `RandomOverSampler`
    lets us perturb or shift each point by a small amount.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨éšæœºè¿‡é‡‡æ ·åï¼Œæ ‡ç­¾ä¸º1çš„ç¤ºä¾‹ä¼šç›¸äº’é‡å ï¼Œç»™äººä¸€ç§æ²¡æœ‰å˜åŒ–çš„æ„Ÿè§‰ã€‚åå¤é‡å¤ç›¸åŒçš„æ•°æ®ç‚¹å¯èƒ½å¯¼è‡´æ¨¡å‹è®°ä½ç‰¹å®šçš„æ•°æ®ç‚¹ï¼Œè€Œæ— æ³•æ¨å¹¿åˆ°æ–°çš„ã€æœªè§è¿‡çš„ç¤ºä¾‹ã€‚`RandomOverSampler`ä¸­çš„`shrinkage`å‚æ•°è®©æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ªå°é‡æ‰°åŠ¨æˆ–ç§»åŠ¨æ¯ä¸ªç‚¹ã€‚
- en: The value of the `shrinkage` parameter has to be greater than or equal to 0
    and can be `float` or `dict`. If a `float` data type is used, the same shrinkage
    factor will be used for all classes. If a `dict` data type is used, the shrinkage
    factor will be specific for each class.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`shrinkage`å‚æ•°çš„å€¼å¿…é¡»å¤§äºæˆ–ç­‰äº0ï¼Œå¯ä»¥æ˜¯`float`æˆ–`dict`ç±»å‹ã€‚å¦‚æœä½¿ç”¨`float`æ•°æ®ç±»å‹ï¼Œç›¸åŒçš„æ”¶ç¼©å› å­å°†ç”¨äºæ‰€æœ‰ç±»åˆ«ã€‚å¦‚æœä½¿ç”¨`dict`æ•°æ®ç±»å‹ï¼Œæ”¶ç¼©å› å­å°†é’ˆå¯¹æ¯ä¸ªç±»åˆ«å…·ä½“æŒ‡å®šã€‚'
- en: 'In *Figure 2**.4*, we can observe the impact of random oversampling with `shrinkage=0.2`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨**å›¾2**.4ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°`shrinkage=0.2`çš„éšæœºè¿‡é‡‡æ ·çš„å½±å“ï¼š
- en: '![](img/B17259_02_04.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_02_04.jpg)'
- en: Figure 2.4 â€“ Result of applying random oversampling with shrinkage=0.2
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.4 â€“ åº”ç”¨éšæœºè¿‡é‡‡æ ·ï¼ˆshrinkage=0.2ï¼‰çš„ç»“æœ
- en: Toward the end of this chapter, we will compare the performance of random oversampling
    with various other oversampling techniques across multiple models and datasets.
    This will provide insights into their effectiveness in real-world applications.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« çš„ç»“å°¾ï¼Œæˆ‘ä»¬å°†æ¯”è¾ƒéšæœºè¿‡é‡‡æ ·ä¸å…¶ä»–å¤šç§è¿‡é‡‡æ ·æŠ€æœ¯åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚è¿™å°†ä¸ºæˆ‘ä»¬æä¾›å…³äºå®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§çš„è§è§£ã€‚
- en: ğŸš€ Random oversampling in production at Grab
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš€ Grabåœ¨ç”Ÿäº§ä¸­ä½¿ç”¨éšæœºè¿‡é‡‡æ ·
- en: Grab, a ride-hailing and food delivery service in Southeast Asia, developed
    an image collection platform [1] for storing and retrieving imagery and map data.
    A key feature of this platform was its ability to automatically detect and blur
    **Personally Identifiable Information** (**PII**), such as faces and license plates,
    in street-level images. This was essential for maintaining user privacy. The dataset
    that was used for this purpose had a significant imbalance, with far more negative
    samples (images without PII) than positive ones (images with PII). Manual annotation
    was not feasible, so they turned to machine learning to solve this problem.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Grabæ˜¯ä¸€å®¶ä¸œå—äºšçš„æ‰“è½¦å’Œé£Ÿå“é…é€æœåŠ¡å…¬å¸ï¼Œå¼€å‘äº†ä¸€ä¸ªç”¨äºå­˜å‚¨å’Œæ£€ç´¢å›¾åƒå’Œåœ°å›¾æ•°æ®çš„å›¾åƒæ”¶é›†å¹³å°[1]ã€‚è¯¥å¹³å°çš„ä¸€ä¸ªå…³é”®ç‰¹æ€§æ˜¯èƒ½å¤Ÿè‡ªåŠ¨æ£€æµ‹å’Œæ¨¡ç³Šè¡—æ™¯å›¾åƒä¸­çš„**ä¸ªäººèº«ä»½ä¿¡æ¯**ï¼ˆ**PII**ï¼‰ï¼Œå¦‚äººè„¸å’Œè½¦ç‰Œã€‚è¿™å¯¹äºç»´æŠ¤ç”¨æˆ·éšç§è‡³å…³é‡è¦ã€‚ç”¨äºæ­¤ç›®çš„çš„æ•°æ®é›†å­˜åœ¨æ˜¾è‘—çš„ä¸å¹³è¡¡ï¼Œè´Ÿæ ·æœ¬ï¼ˆæ²¡æœ‰PIIçš„å›¾åƒï¼‰è¿œå¤šäºæ­£æ ·æœ¬ï¼ˆæœ‰PIIçš„å›¾åƒï¼‰ã€‚æ‰‹åŠ¨æ ‡æ³¨ä¸å¯è¡Œï¼Œæ‰€ä»¥ä»–ä»¬è½¬å‘æœºå™¨å­¦ä¹ æ¥è§£å†³æ­¤é—®é¢˜ã€‚
- en: To address the data imbalance, Grab employed the random oversampling technique
    to increase the number of positive samples, thereby enhancing the performance
    of their machine learning model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³æ•°æ®ä¸å¹³è¡¡ï¼ŒGrabé‡‡ç”¨äº†éšæœºè¿‡é‡‡æ ·æŠ€æœ¯æ¥å¢åŠ æ­£æ ·æœ¬çš„æ•°é‡ï¼Œä»è€Œæé«˜äº†ä»–ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚
- en: Problems with random oversampling
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éšæœºè¿‡é‡‡æ ·çš„é—®é¢˜
- en: Random oversampling can often lead to overfitting of the model since the generated
    synthetic observations get repeated, and the model sees the same observations
    again and again. Shrinkage tries to handle that in some sense, but it may be challenging
    to come up with an apt value of shrinkage, and shrinkage doesnâ€™t care if the generated
    synthetic samples overlap with the majority class samples, which can lead to other
    problems.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºè¿‡é‡‡æ ·å¾€å¾€ä¼šå¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œå› ä¸ºç”Ÿæˆçš„åˆæˆè§‚å¯Ÿå€¼ä¼šé‡å¤ï¼Œæ¨¡å‹ä¼šä¸€æ¬¡åˆä¸€æ¬¡åœ°çœ‹åˆ°ç›¸åŒçš„è§‚å¯Ÿå€¼ã€‚æ”¶ç¼©è¯•å›¾åœ¨æŸç§ç¨‹åº¦ä¸Šå¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œä½†å¯èƒ½å¾ˆéš¾æ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„æ”¶ç¼©å€¼ï¼Œè€Œä¸”æ”¶ç¼©å¹¶ä¸å…³å¿ƒç”Ÿæˆçš„åˆæˆæ ·æœ¬æ˜¯å¦ä¸å¤šæ•°ç±»æ ·æœ¬é‡å ï¼Œè¿™å¯èƒ½å¯¼è‡´å…¶ä»–é—®é¢˜ã€‚
- en: In the previous section, we learned about the most basic and practical technique
    for applying oversampling to balance a dataset and reduce bias toward the majority
    class. Many times, random oversampling itself might give us such a high boost
    to our modelâ€™s performance that we may not even need to apply more advanced techniques.
    In production settings, it would also be beneficial to keep things plain and simple
    until we are ready to introduce more complexity in the pipeline. As they say,
    â€œpremature optimization is the root of all evil,â€ so we start with something simple,
    so long as it does improve our modelâ€™s performance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†åº”ç”¨è¿‡é‡‡æ ·å¹³è¡¡æ•°æ®é›†å’Œå‡å°‘å¯¹å¤šæ•°ç±»åå·®çš„æœ€åŸºæœ¬å’Œå®ç”¨çš„æŠ€æœ¯ã€‚å¾ˆå¤šæ—¶å€™ï¼Œéšæœºè¿‡é‡‡æ ·æœ¬èº«å¯èƒ½ä¼šæå¤§åœ°æé«˜æˆ‘ä»¬æ¨¡å‹çš„è¡¨ç°ï¼Œä»¥è‡³äºæˆ‘ä»¬å¯èƒ½ç”šè‡³ä¸éœ€è¦åº”ç”¨æ›´é«˜çº§çš„æŠ€æœ¯ã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œåœ¨å‡†å¤‡å¼•å…¥æ›´å¤šå¤æ‚æ€§ä¹‹å‰ï¼Œä¿æŒäº‹æƒ…ç®€å•æ˜äº†ä¹Ÿæ˜¯æœ‰åˆ©çš„ã€‚æ­£å¦‚ä»–ä»¬æ‰€è¯´ï¼Œâ€œè¿‡æ—©ä¼˜åŒ–æ˜¯ä¸‡æ¶ä¹‹æºâ€ï¼Œæ‰€ä»¥æˆ‘ä»¬ä»ç®€å•çš„äº‹æƒ…å¼€å§‹ï¼Œåªè¦å®ƒèƒ½æé«˜æˆ‘ä»¬æ¨¡å‹çš„è¡¨ç°ã€‚
- en: In the subsequent sections, we will explore some alternative techniques, such
    as SMOTE and ADASYN, which adopt a different approach to oversampling and alleviate
    some of the problems associated with the random oversampling technique.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨éšåçš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä¸€äº›æ›¿ä»£æŠ€æœ¯ï¼Œä¾‹å¦‚SMOTEå’ŒADASYNï¼Œå®ƒä»¬é‡‡ç”¨ä¸åŒçš„è¿‡é‡‡æ ·æ–¹æ³•ï¼Œå¹¶ç¼“è§£äº†ä¸éšæœºè¿‡é‡‡æ ·æŠ€æœ¯ç›¸å…³çš„ä¸€äº›é—®é¢˜ã€‚
- en: SMOTE
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SMOTE
- en: The main problem with random oversampling is that it duplicates the observations
    from the minority class. This can often cause overfitting. **Synthetic Minority
    Oversampling Technique** (**SMOTE**) [2] solves this problem of duplication by
    using a technique called **interpolation**.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºè¿‡é‡‡æ ·çš„ä¸»è¦é—®é¢˜æ˜¯å®ƒä¼šé‡å¤å°‘æ•°ç±»çš„è§‚å¯Ÿç»“æœã€‚è¿™é€šå¸¸ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆã€‚**åˆæˆå°‘æ•°è¿‡é‡‡æ ·æŠ€æœ¯**ï¼ˆ**SMOTE**ï¼‰[2]é€šè¿‡ä½¿ç”¨ç§°ä¸º**æ’å€¼**çš„æŠ€æœ¯æ¥è§£å†³è¿™ç§é‡å¤é—®é¢˜ã€‚
- en: Interpolation involves creating new data points in the range of known data points.
    Think of interpolation as being similar to the process of reproduction in biology.
    In reproduction, two individuals come together to produce a new individual with
    traits of both of them. Similarly, in interpolation, we pick two observations
    from the dataset and create a new observation by choosing a random point on the
    line joining the two selected points.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ’å€¼æ¶‰åŠåœ¨å·²çŸ¥æ•°æ®ç‚¹çš„èŒƒå›´å†…åˆ›å»ºæ–°çš„æ•°æ®ç‚¹ã€‚å°†æ’å€¼æƒ³è±¡æˆç±»ä¼¼äºç”Ÿç‰©å­¦ä¸­çš„ç¹æ®–è¿‡ç¨‹ã€‚åœ¨ç¹æ®–ä¸­ï¼Œä¸¤ä¸ªä¸ªä½“ç»“åˆåœ¨ä¸€èµ·äº§ç”Ÿä¸€ä¸ªå…·æœ‰ä¸¤è€…ç‰¹å¾çš„æ–°ä¸ªä½“ã€‚åŒæ ·ï¼Œåœ¨æ’å€¼ä¸­ï¼Œæˆ‘ä»¬ä»æ•°æ®é›†ä¸­é€‰æ‹©ä¸¤ä¸ªè§‚å¯Ÿç»“æœï¼Œå¹¶é€šè¿‡é€‰æ‹©ä¸¤ä¸ªé€‰å®šç‚¹ä¹‹é—´çº¿ä¸Šçš„éšæœºç‚¹æ¥åˆ›å»ºä¸€ä¸ªæ–°çš„è§‚å¯Ÿç»“æœã€‚
- en: 'We oversample the minority class by interpolating synthetic examples. That
    prevents the duplication of minority samples while generating new synthetic observations
    similar to the known points. *Figure 2**.5* depicts how SMOTE works:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡æ’å€¼åˆæˆç¤ºä¾‹æ¥è¿‡é‡‡æ ·å°‘æ•°ç±»ã€‚è¿™é˜²æ­¢äº†å°‘æ•°æ ·æœ¬çš„é‡å¤ï¼ŒåŒæ—¶ç”Ÿæˆä¸å·²çŸ¥ç‚¹ç›¸ä¼¼çš„æ–°çš„åˆæˆè§‚å¯Ÿç»“æœã€‚*å›¾2.5*å±•ç¤ºäº†SMOTEæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼š
- en: '![](img/B17259_02_05.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_05.jpg)'
- en: Figure 2.5 â€“ Working of SMOTE
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.5 â€“ SMOTEçš„å·¥ä½œåŸç†
- en: 'Here, we can see the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä»¥ä¸‹å†…å®¹ï¼š
- en: The majority and minority class samples are plotted (left)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°ç±»å’Œå°‘æ•°ç±»æ ·æœ¬è¢«ç»˜åˆ¶å‡ºæ¥ï¼ˆå·¦ä¾§ï¼‰
- en: The synthetic samples are generated by taking a random point on the line joining
    a minority sample to two nearest neighbor majority class samples (right)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡åœ¨è¿æ¥å°‘æ•°æ ·æœ¬å’Œä¸¤ä¸ªæœ€è¿‘é‚»å¤šæ•°ç±»æ ·æœ¬çš„çº¿ä¸Šçš„éšæœºç‚¹ç”Ÿæˆåˆæˆæ ·æœ¬ï¼ˆå³ä¾§ï¼‰
- en: SMOTE was originally designed for continuous inputs. To keep the explanations
    simple, weâ€™ll start with continuous inputs and discuss other kinds of inputs later.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTEæœ€åˆæ˜¯ä¸ºè¿ç»­è¾“å…¥è®¾è®¡çš„ã€‚ä¸ºäº†ä¿æŒè§£é‡Šçš„ç®€å•æ€§ï¼Œæˆ‘ä»¬å°†ä»è¿ç»­è¾“å…¥å¼€å§‹ï¼Œç¨åå†è®¨è®ºå…¶ä»–ç±»å‹çš„è¾“å…¥ã€‚
- en: First, we will examine the functioning of SMOTE and explore any potential disadvantages
    associated with this technique.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ£€æŸ¥SMOTEçš„åŠŸèƒ½ï¼Œå¹¶æ¢è®¨ä¸æ­¤æŠ€æœ¯ç›¸å…³çš„ä»»ä½•æ½œåœ¨ç¼ºç‚¹ã€‚
- en: How SMOTE works
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SMOTEæ˜¯å¦‚ä½•å·¥ä½œçš„
- en: 'The SMOTE algorithm works as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTEç®—æ³•çš„å·¥ä½œåŸç†å¦‚ä¸‹ï¼š
- en: It considers only the samples from the minority class.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ƒåªè€ƒè™‘å°‘æ•°ç±»çš„æ ·æœ¬ã€‚
- en: It trains KNN on the minority samples. A typical value of `k` is 5.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ƒåœ¨å°‘æ•°æ ·æœ¬ä¸Šè®­ç»ƒKNNã€‚`k`çš„å…¸å‹å€¼æ˜¯5ã€‚
- en: For each minority sample, a line is drawn between the point and each of its
    KNN examples.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªå°‘æ•°æ ·æœ¬ï¼Œä»è¯¥ç‚¹åˆ°å…¶KNNç¤ºä¾‹ä¹‹é—´ç”»ä¸€æ¡çº¿ã€‚
- en: For each such line segment, a point on the segment is randomly picked to create
    a new synthetic example.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™æ ·çš„çº¿æ®µï¼Œéšæœºé€‰æ‹©çº¿æ®µä¸Šçš„ä¸€ä¸ªç‚¹æ¥åˆ›å»ºä¸€ä¸ªæ–°çš„åˆæˆç¤ºä¾‹ã€‚
- en: 'Letâ€™s use SMOTE using APIs from the `imbalanced-learn` library:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨`imbalanced-learn`åº“çš„APIæ¥åº”ç”¨SMOTEï¼š
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here is the output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯è¾“å‡ºï¼š
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The oversampled dataset looks like this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‡é‡‡æ ·åçš„æ•°æ®é›†çœ‹èµ·æ¥åƒè¿™æ ·ï¼š
- en: '![](img/B17259_02_06.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_06.jpg)'
- en: Figure 2.6 â€“ Oversampling using SMOTE
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.6 â€“ ä½¿ç”¨SMOTEè¿›è¡Œè¿‡é‡‡æ ·
- en: ğŸš€ Oversampling techniques in production at Microsoft
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš€ å¾®è½¯ç”Ÿäº§ä¸­çš„è¿‡é‡‡æ ·æŠ€æœ¯
- en: In a real-world application at Microsoft [3], machine learning was employed
    to forecast **Live Site Incidents** (**LSIs**) for early detection and escalation
    of incidents for engineering teams. Every day, a high volume of incidents was
    being generated, most of which started as low-severity issues. Due to limited
    resources, it was impractical for engineering teams to investigate all incidents,
    leading to potential delays in mitigating critical issues until they had a significant
    customer impact.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¾®è½¯çš„ä¸€ä¸ªå®é™…åº”ç”¨[3]ä¸­ï¼Œæœºå™¨å­¦ä¹ è¢«ç”¨äºé¢„æµ‹**ç°åœºäº‹ä»¶**ï¼ˆ**LSIs**ï¼‰ï¼Œä»¥å®ç°äº‹ä»¶çš„æ—©æœŸæ£€æµ‹å’Œå‡çº§ï¼Œè¿™å¯¹äºå·¥ç¨‹å›¢é˜Ÿæ¥è¯´è‡³å…³é‡è¦ã€‚æ¯å¤©éƒ½ä¼šäº§ç”Ÿå¤§é‡çš„äº‹ä»¶ï¼Œå…¶ä¸­å¤§éƒ¨åˆ†æœ€åˆæ˜¯ä½ä¸¥é‡æ€§é—®é¢˜ã€‚ç”±äºèµ„æºæœ‰é™ï¼Œå·¥ç¨‹å›¢é˜Ÿè°ƒæŸ¥æ‰€æœ‰äº‹ä»¶æ˜¯ä¸åˆ‡å®é™…çš„ï¼Œè¿™å¯èƒ½å¯¼è‡´åœ¨äº‹ä»¶å¯¹å®¢æˆ·äº§ç”Ÿé‡å¤§å½±å“ä¹‹å‰ï¼Œç¼“è§£å…³é”®é—®é¢˜çš„æ½œåœ¨å»¶è¿Ÿã€‚
- en: 'To address this, Microsoft employed machine learning to forecast which LSIs
    could escalate into severe problems, aiming for proactive identification and early
    resolution. The challenge was the data imbalance in the training set: out of approximately
    40,000 incidents, fewer than 2% escalated to high severity. Microsoft used two
    different oversampling techniquesâ€” bagged classification (covered in [*Chapter
    4*](B17259_04.xhtml#_idTextAnchor120), *Ensemble Methods*), and SMOTE, which were
    the most effective in improving the modelâ€™s performance. They used a two-step
    pipeline for balancing classes: first, oversampling with **SMOTE** and then undersampling
    with **RandomUnderSampler** (covered in [*Chapter 3*](B17259_03.xhtml#_idTextAnchor079),
    *Undersampling Methods*). The pipeline automatically selected the optimal sampling
    ratios for both steps, and SMOTE performed better when combined with undersampling.
    The resulting end-to-end automated model was designed to be generic, making it
    applicable across different teams within or outside Microsoft, provided historical
    incidents were available for learning. The LSI insight tool used this model, which
    was adopted by various engineering teams.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¾®è½¯é‡‡ç”¨äº†æœºå™¨å­¦ä¹ æ¥é¢„æµ‹å“ªäº›LSIså¯èƒ½ä¼šå‡çº§ä¸ºä¸¥é‡é—®é¢˜ï¼Œç›®æ ‡æ˜¯è¿›è¡Œä¸»åŠ¨è¯†åˆ«å’Œæ—©æœŸè§£å†³ã€‚æŒ‘æˆ˜åœ¨äºè®­ç»ƒé›†ä¸­çš„æ•°æ®ä¸å¹³è¡¡ï¼šåœ¨çº¦40,000ä¸ªäº‹ä»¶ä¸­ï¼Œä¸åˆ°2%å‡çº§ä¸ºé«˜ä¸¥é‡æ€§ã€‚å¾®è½¯ä½¿ç”¨äº†ä¸¤ç§ä¸åŒçš„è¿‡é‡‡æ ·æŠ€æœ¯â€”â€”è¢‹åˆ†ç±»ï¼ˆåœ¨ç¬¬[*ç¬¬4ç« *](B17259_04.xhtml#_idTextAnchor120)ï¼Œ*é›†æˆæ–¹æ³•*ï¼‰å’ŒSMOTEï¼Œè¿™äº›æŠ€æœ¯åœ¨æé«˜æ¨¡å‹æ€§èƒ½æ–¹é¢æœ€ä¸ºæœ‰æ•ˆã€‚ä»–ä»¬ä½¿ç”¨äº†ä¸¤æ­¥æµç¨‹æ¥å¹³è¡¡ç±»åˆ«ï¼šé¦–å…ˆï¼Œä½¿ç”¨**SMOTE**è¿›è¡Œè¿‡é‡‡æ ·ï¼Œç„¶åä½¿ç”¨**RandomUnderSampler**ï¼ˆåœ¨ç¬¬[*ç¬¬3ç« *](B17259_03.xhtml#_idTextAnchor079)ï¼Œ*æ¬ é‡‡æ ·æ–¹æ³•*ï¼‰è¿›è¡Œæ¬ é‡‡æ ·ã€‚è¯¥æµç¨‹è‡ªåŠ¨é€‰æ‹©äº†ä¸¤æ­¥çš„æœ€ä¼˜é‡‡æ ·æ¯”ç‡ï¼Œå¹¶ä¸”å½“ä¸æ¬ é‡‡æ ·ç»“åˆæ—¶ï¼ŒSMOTEè¡¨ç°æ›´ä½³ã€‚æ‰€å¾—åˆ°çš„ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–æ¨¡å‹è¢«è®¾è®¡ä¸ºé€šç”¨å‹ï¼Œä½¿å…¶é€‚ç”¨äºå¾®è½¯å†…éƒ¨æˆ–å¤–éƒ¨çš„ä¸åŒå›¢é˜Ÿï¼Œå‰ææ˜¯å¯ç”¨å†å²äº‹ä»¶è¿›è¡Œå­¦ä¹ ã€‚LSIæ´å¯Ÿå·¥å…·ä½¿ç”¨äº†è¿™ä¸ªæ¨¡å‹ï¼Œå¹¶è¢«å„ä¸ªå·¥ç¨‹å›¢é˜Ÿé‡‡ç”¨ã€‚
- en: Next, we will look at the limitations of using SMOTE.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä½¿ç”¨SMOTEçš„å±€é™æ€§ã€‚
- en: Problems with SMOTE
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SMOTEçš„é—®é¢˜
- en: 'SMOTE has its pitfalls â€“ for example, it can add noise to an already noisy
    dataset. It can also lead to class overlap issues as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTEæœ‰å…¶é™·é˜±â€”â€”ä¾‹å¦‚ï¼Œå®ƒå¯èƒ½ä¼šå‘å·²ç»å˜ˆæ‚çš„æ•°æ®é›†ä¸­æ·»åŠ å™ªå£°ã€‚å®ƒä¹Ÿå¯èƒ½å¯¼è‡´ä»¥ä¸‹ç±»é‡å é—®é¢˜ï¼š
- en: 'SMOTE generates minority class samples without considering the majority class
    distribution, which may increase the overlap between the classes. In *Figure 2**.7*,
    weâ€™re plotting the binary classification imbalanced dataset before and after applying
    SMOTE. We can see a lot of overlap between the two classes after applying SMOTE:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SMOTEåœ¨ç”Ÿæˆå°‘æ•°ç±»æ ·æœ¬æ—¶æ²¡æœ‰è€ƒè™‘å¤šæ•°ç±»çš„åˆ†å¸ƒï¼Œè¿™å¯èƒ½ä¼šå¢åŠ ç±»åˆ«ä¹‹é—´çš„é‡å ã€‚åœ¨*å›¾2*.7ä¸­ï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†åº”ç”¨SMOTEå‰åçš„äºŒåˆ†ç±»ä¸å¹³è¡¡æ•°æ®é›†ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°åº”ç”¨SMOTEåä¸¤ä¸ªç±»åˆ«ä¹‹é—´æœ‰å¾ˆå¤šé‡å ï¼š
- en: '![](img/B17259_02_07.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_02_07.jpg)'
- en: Figure 2.7 â€“ Binary classification dataset before (left) and after (right) applying
    SMOTE (see the overlap between two classes on the right)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.7 â€“ åº”ç”¨SMOTEå‰ï¼ˆå·¦ï¼‰å’Œåï¼ˆå³ï¼‰çš„äºŒåˆ†ç±»æ•°æ®é›†ï¼ˆå³å›¾ä¸­ä¸¤ä¸ªç±»åˆ«çš„é‡å éƒ¨åˆ†ï¼‰
- en: The other case may be that you have a huge amount of data, and running SMOTE
    may increase the runtime of your pipeline.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§æƒ…å†µå¯èƒ½æ˜¯ä½ æ‹¥æœ‰å¤§é‡æ•°æ®ï¼Œè¿è¡ŒSMOTEå¯èƒ½ä¼šå¢åŠ ä½ ç®¡é“çš„è¿è¡Œæ—¶é—´ã€‚
- en: Problem 1 can be solved by using the SMOTE variant Borderline-SMOTE (discussed
    in the next section).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜1å¯ä»¥é€šè¿‡ä½¿ç”¨SMOTEå˜ä½“Borderline-SMOTEï¼ˆå°†åœ¨ä¸‹ä¸€èŠ‚ä¸­è®¨è®ºï¼‰æ¥è§£å†³ã€‚
- en: In this section, we learned about SMOTE, which uses the nearest neighbor technique
    to generate synthetic samples of the minority class. Sometimes, SMOTE may perform
    better than random oversampling since it exploits the proximity to other minority
    class samples to generate new samples.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†SMOTEï¼Œå®ƒä½¿ç”¨æœ€è¿‘é‚»æŠ€æœ¯æ¥ç”Ÿæˆå°‘æ•°ç±»çš„åˆæˆæ ·æœ¬ã€‚æœ‰æ—¶ï¼ŒSMOTEå¯èƒ½æ¯”éšæœºè¿‡é‡‡æ ·è¡¨ç°å¾—æ›´å¥½ï¼Œå› ä¸ºå®ƒåˆ©ç”¨äº†ä¸å…¶ä»–å°‘æ•°ç±»æ ·æœ¬çš„é‚»è¿‘æ€§æ¥ç”Ÿæˆæ–°çš„æ ·æœ¬ã€‚
- en: SMOTE variants
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SMOTEå˜ä½“
- en: Now, letâ€™s look at some of the SMOTE variants, such as Borderline-SMOTE, SMOTE-NC,
    and SMOTEN. These variants apply the SMOTE algorithm to samples of a certain kind
    and may not always be applicable.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ä¸€äº›SMOTEçš„å˜ä½“ï¼Œä¾‹å¦‚Borderline-SMOTEã€SMOTE-NCå’ŒSMOTENã€‚è¿™äº›å˜ä½“å°†SMOTEç®—æ³•åº”ç”¨äºç‰¹å®šç±»å‹çš„æ ·æœ¬ï¼Œå¹¶ä¸”ä¸ä¸€å®šæ€»æ˜¯é€‚ç”¨ã€‚
- en: Borderline-SMOTE
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Borderline-SMOTE
- en: Borderline-SMOTE [4] is a variation of SMOTE that generates synthetic samples
    from the minority class samples that are near the classification boundary, which
    divides the majority class from the minority class.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Borderline-SMOTE [4]æ˜¯SMOTEçš„ä¸€ç§å˜ä½“ï¼Œå®ƒä»é è¿‘åˆ†ç±»è¾¹ç•Œçš„å°‘æ•°ç±»æ ·æœ¬ä¸­ç”Ÿæˆåˆæˆæ ·æœ¬ï¼Œè¯¥è¾¹ç•Œå°†å¤šæ•°ç±»ä¸å°‘æ•°ç±»åˆ†å¼€ã€‚
- en: Why consider samples on the classification boundary?
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¦è€ƒè™‘åˆ†ç±»è¾¹ç•Œçš„æ ·æœ¬ï¼Ÿ
- en: The idea is that the examples near the classification boundary are more prone
    to misclassification than those far away from the decision boundary. Producing
    more such minority samples along the boundary would help the model learn better
    about the minority class. Intuitively, it is also true that the points away from
    the classification boundary likely wonâ€™t make the model a better classifier.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ç†å¿µæ˜¯é è¿‘åˆ†ç±»è¾¹ç•Œçš„ä¾‹å­æ¯”è¿œç¦»å†³ç­–è¾¹ç•Œçš„ä¾‹å­æ›´å®¹æ˜“è¯¯åˆ†ç±»ã€‚åœ¨è¾¹ç•Œé™„è¿‘äº§ç”Ÿæ›´å¤šçš„è¿™ç§å°‘æ•°æ ·æœ¬å°†æœ‰åŠ©äºæ¨¡å‹æ›´å¥½åœ°å­¦ä¹ å°‘æ•°ç±»ã€‚ç›´è§‚ä¸Šï¼Œè¿œç¦»åˆ†ç±»è¾¹ç•Œçš„ç‚¹å¯èƒ½ä¸ä¼šä½¿æ¨¡å‹æˆä¸ºä¸€ä¸ªæ›´å¥½çš„åˆ†ç±»å™¨ã€‚
- en: 'Hereâ€™s a step-by-step algorithm for Borderline-SMOTE:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯Borderline-SMOTEçš„é€æ­¥ç®—æ³•ï¼š
- en: We run a KNN algorithm over the whole dataset.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè¿è¡ŒKNNç®—æ³•ã€‚
- en: 'Then, we divide the minority class points into three categories:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å°†å°‘æ•°ç±»ç‚¹åˆ†ä¸ºä¸‰ç±»ï¼š
- en: '*Noise* points are minority class examples that have all the neighbors from
    the majority class. These points are buried among majority-class neighbors. They
    are likely outliers and can safely be ignored as â€œnoise.â€'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å™ªå£°**ç‚¹æ˜¯æ‰€æœ‰é‚»å±…éƒ½æ˜¯å¤šæ•°ç±»çš„å°‘æ•°ç±»ä¾‹å­ã€‚è¿™äº›ç‚¹è¢«åŸ‹åœ¨å¤šæ•°ç±»é‚»å±…ä¸­ã€‚å®ƒä»¬å¯èƒ½æ˜¯å¼‚å¸¸å€¼ï¼Œå¯ä»¥å®‰å…¨åœ°è¢«å¿½ç•¥ä½œä¸ºâ€œå™ªå£°â€ã€‚'
- en: '*Safe* points have more minority-class neighbors than majority-class neighbors.
    Such observations donâ€™t contain much information and can be safely ignored.'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å®‰å…¨**ç‚¹æ¯”å¤šæ•°ç±»é‚»å±…æœ‰æ›´å¤šçš„å°‘æ•°ç±»é‚»å±…ã€‚è¿™æ ·çš„è§‚å¯Ÿç»“æœåŒ…å«çš„ä¿¡æ¯ä¸å¤šï¼Œå¯ä»¥å®‰å…¨åœ°å¿½ç•¥ã€‚'
- en: '*Danger* points have more majority-class neighbors than minority-class neighbors.
    This implies that such observations are on or close to the boundary between the
    two classes.'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å±é™©**ç‚¹æ¯”å°‘æ•°ç±»é‚»å±…æœ‰æ›´å¤šçš„å¤šæ•°ç±»é‚»å±…ã€‚è¿™è¡¨æ˜è¿™æ ·çš„è§‚å¯Ÿç»“æœä½äºæˆ–æ¥è¿‘ä¸¤ç±»ä¹‹é—´çš„è¾¹ç•Œã€‚'
- en: Then, we train a KNN model only on the minority class examples.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬ä»…åœ¨å°‘æ•°ç±»ä¾‹å­ä¸Šè®­ç»ƒKNNæ¨¡å‹ã€‚
- en: Finally, we apply the SMOTE algorithm to the `Danger` points. Note that the
    neighbors of these `Danger` points may or may not be marked as `Danger`.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å°†SMOTEç®—æ³•åº”ç”¨äº`å±é™©`ç‚¹ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›`å±é™©`ç‚¹çš„é‚»å±…å¯èƒ½è¢«æ ‡è®°ä¸º`å±é™©`ï¼Œä¹Ÿå¯èƒ½ä¸æ˜¯ã€‚
- en: 'As shown in *Figure 2**.8*, Borderline-SMOTE focuses on the danger class points
    for synthetic data generation:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚**å›¾2.8**æ‰€ç¤ºï¼ŒBorderline-SMOTEåœ¨ç”Ÿæˆåˆæˆæ•°æ®æ—¶ä¸“æ³¨äºå±é™©ç±»ç‚¹ï¼š
- en: '![](img/B17259_02_08.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_08.jpg)'
- en: Figure 2.8 â€“ The Borderline-SMOTE algorithm uses only danger points to generate
    synthetic samples. Danger points have more majority-class neighbors than minority-class
    ones
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.8 â€“ Borderline-SMOTEç®—æ³•ä»…ä½¿ç”¨å±é™©ç‚¹æ¥ç”Ÿæˆåˆæˆæ ·æœ¬ã€‚å±é™©ç‚¹æ¯”å°‘æ•°ç±»é‚»å±…æœ‰æ›´å¤šçš„å¤šæ•°ç±»é‚»å±…
- en: '*Figure 2**.9* shows how Borderline-SMOTE focuses on the minority class samples
    that are near the classification boundary, which separates the majority and minority
    classes:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾2.9**å±•ç¤ºäº†Borderline-SMOTEå¦‚ä½•ä¸“æ³¨äºé è¿‘åˆ†ç±»è¾¹ç•Œçš„å°‘æ•°ç±»æ ·æœ¬ï¼Œè¯¥è¾¹ç•Œå°†å¤šæ•°ç±»å’Œå°‘æ•°ç±»åˆ†å¼€ï¼š'
- en: '![](img/B17259_02_09.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_09.jpg)'
- en: Figure 2.9 â€“ Illustrating Borderline-SMOTE
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.9 â€“ Borderline-SMOTEçš„è¯´æ˜
- en: 'Here, we can see the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä»¥ä¸‹æƒ…å†µï¼š
- en: a) Plots of majority and minority class samples
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: a) å¤šæ•°ç±»å’Œå°‘æ•°ç±»æ ·æœ¬çš„å›¾
- en: b) Synthetic samples generated using neighbors near the classification boundary
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: b) ä½¿ç”¨é è¿‘åˆ†ç±»è¾¹ç•Œçš„é‚»å±…ç”Ÿæˆçš„åˆæˆæ ·æœ¬
- en: 'Letâ€™s see how we can use Borderline-SMOTE from the `imbalanced-learn` library
    to perform oversampling of the data:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨`imbalanced-learn`åº“ä¸­çš„Borderline-SMOTEæ¥æ‰§è¡Œæ•°æ®è¿‡é‡‡æ ·ï¼š
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here is the output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯è¾“å‡ºï¼š
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Can you guess the problem with focusing solely on data points on the decision
    boundary of the two classes?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ èƒ½çŒœå‡ºåªå…³æ³¨ä¸¤ç±»å†³ç­–è¾¹ç•Œä¸Šçš„æ•°æ®ç‚¹çš„é—®é¢˜å—ï¼Ÿ
- en: 'Since this technique focuses so heavily on a very small number of points on
    the boundary, the points inside the minority class clusters are not sampled at
    all:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™ç§æŠ€æœ¯å¦‚æ­¤ä¾§é‡äºè¾¹ç•Œä¸Šéå¸¸å°‘æ•°çš„ç‚¹ï¼Œå°‘æ•°ç±»ç°‡å†…çš„ç‚¹æ ¹æœ¬å°±æ²¡æœ‰è¢«é‡‡æ ·ï¼š
- en: '![](img/B17259_02_10.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_10.jpg)'
- en: Figure 2.10 â€“ The Borderline-SMOTE algorithm utilizing danger points, with more
    majority- than minority-class neighbors, to generate synthetic samples
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.10 â€“ åˆ©ç”¨å±é™©ç‚¹ï¼ˆå¤šæ•°ç±»é‚»å±…å¤šäºå°‘æ•°ç±»ï¼‰çš„Borderline-SMOTEç®—æ³•ç”Ÿæˆåˆæˆæ ·æœ¬
- en: In this section, we learned about Borderline-SMOTE, which generates synthetic
    minority class samples by focusing on the samples that are close to the classification
    boundary of the majority and minority classes, which, in turn, may help in improving
    the discrimination power of the model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†è¾¹ç•ŒSMOTEï¼Œå®ƒé€šè¿‡å…³æ³¨æ¥è¿‘å¤šæ•°å’Œå°‘æ•°ç±»åˆ«åˆ†ç±»è¾¹ç•Œçš„æ ·æœ¬æ¥ç”Ÿæˆåˆæˆå°‘æ•°ç±»åˆ«æ ·æœ¬ï¼Œè¿™åè¿‡æ¥åˆå¯èƒ½æœ‰åŠ©äºæé«˜æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›ã€‚
- en: ğŸš€ Oversampling techniques in production at Amazon
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš€ äºšé©¬é€Šç”Ÿäº§ä¸­çš„è¿‡é‡‡æ ·æŠ€æœ¯
- en: In a real-world application, Amazon used machine learning to optimize packaging
    types for products, aiming to reduce waste while ensuring product safety [5].
    In their training dataset, which featured millions of product and package combinations,
    Amazon faced a significant class imbalance, with as few as 1% of the examples
    representing unsuitable product-package pairings (minority class).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®é™…åº”ç”¨ä¸­ï¼Œäºšé©¬é€Šä½¿ç”¨æœºå™¨å­¦ä¹ ä¼˜åŒ–äº§å“çš„åŒ…è£…ç±»å‹ï¼Œæ—¨åœ¨å‡å°‘æµªè´¹åŒæ—¶ç¡®ä¿äº§å“å®‰å…¨[5]ã€‚åœ¨ä»–ä»¬çš„è®­ç»ƒæ•°æ®é›†ä¸­ï¼ŒåŒ…å«äº†æ•°ç™¾ä¸‡ç§äº§å“å’ŒåŒ…è£…ç»„åˆï¼Œäºšé©¬é€Šé¢ä¸´æ˜¾è‘—çš„ç±»åˆ«ä¸å¹³è¡¡ï¼Œå…¶ä¸­åªæœ‰1%çš„ç¤ºä¾‹ä»£è¡¨ä¸é€‚åˆçš„äº§å“-åŒ…è£…é…å¯¹ï¼ˆå°‘æ•°ç±»åˆ«ï¼‰ã€‚
- en: 'To tackle this imbalance, Amazon used various oversampling techniques:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ç§ä¸å¹³è¡¡ï¼Œäºšé©¬é€Šä½¿ç”¨äº†å„ç§è¿‡é‡‡æ ·æŠ€æœ¯ï¼š
- en: '- Borderline-SMOTE oversampling, which resulted in a 4%-7% increase in PR-AUC
    but increased the training time by 25%-35%.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '- è¾¹ç•ŒSMOTEè¿‡é‡‡æ ·ï¼Œå¯¼è‡´PR-AUCæé«˜äº†4%-7%ï¼Œä½†è®­ç»ƒæ—¶é—´å¢åŠ äº†25%-35%ã€‚'
- en: '- A hybrid of random oversampling and random undersampling, where they randomly
    oversampled the minority class and undersampled the majority class. It led to
    a 6%-10% improvement in PR-AUC and increased the training time by up to 25%.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '- éšæœºè¿‡é‡‡æ ·å’Œéšæœºæ¬ é‡‡æ ·çš„æ··åˆï¼Œå…¶ä¸­å®ƒä»¬éšæœºè¿‡é‡‡æ ·å°‘æ•°ç±»ï¼Œå¹¶æ¬ é‡‡æ ·å¤šæ•°ç±»ã€‚è¿™å¯¼è‡´äº†PR-AUCæé«˜äº†6%-10%ï¼Œä½†è®­ç»ƒæ—¶é—´å¢åŠ äº†é«˜è¾¾25%ã€‚'
- en: The best-performing technique was two-phase learning with random undersampling
    (discussed in [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205), *Data-Level Deep
    Learning Methods)*, which improved PR-AUC by 18%-24% with no increase in training
    time.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ç°æœ€å¥½çš„æŠ€æœ¯æ˜¯ä¸¤é˜¶æ®µå­¦ä¹ ä¸éšæœºæ¬ é‡‡æ ·ï¼ˆåœ¨ç¬¬[*ç¬¬7ç« *](B17259_07.xhtml#_idTextAnchor205)ï¼Œ*æ•°æ®çº§æ·±åº¦å­¦ä¹ æ–¹æ³•)*ä¸­è®¨è®ºï¼‰ï¼Œå®ƒå°†PR-AUCæé«˜äº†18%-24%ï¼Œè€Œæ²¡æœ‰å¢åŠ è®­ç»ƒæ—¶é—´ã€‚
- en: They mentioned that the effectiveness of a technique in dealing with dataset
    imbalance is both domain- and dataset-specific. This real-world example underscores
    the effectiveness of oversampling techniques in tackling class imbalance issues.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬æåˆ°ï¼Œå¤„ç†æ•°æ®é›†ä¸å¹³è¡¡çš„æŠ€æœ¯æ•ˆæœæ—¢ä¸é¢†åŸŸç›¸å…³ï¼Œä¹Ÿä¸æ•°æ®é›†ç‰¹å®šã€‚è¿™ä¸ªç°å®ä¸–ç•Œçš„ä¾‹å­å¼ºè°ƒäº†è¿‡é‡‡æ ·æŠ€æœ¯åœ¨è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ã€‚
- en: Next, we will learn about another oversampling technique, called ADASYN, that
    oversamples examples near boundaries and in other low-density regions without
    completely ignoring data points that do not lie on the boundary.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦ä¸€ç§è¿‡é‡‡æ ·æŠ€æœ¯ï¼Œç§°ä¸ºADASYNï¼Œå®ƒé€šè¿‡å¯¹è¾¹ç•Œé™„è¿‘å’Œå…¶ä»–ä½å¯†åº¦åŒºåŸŸçš„ç¤ºä¾‹è¿›è¡Œè¿‡é‡‡æ ·ï¼Œè€Œä¸ä¼šå®Œå…¨å¿½ç•¥ä¸åœ¨è¾¹ç•Œä¸Šçš„æ•°æ®ç‚¹ã€‚
- en: ADASYN
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ADASYN
- en: 'While SMOTE doesnâ€™t distinguish between the density distribution of minority
    class samples, **Adaptive Synthetic Sampling** (**ADASYN**) [6] focuses on harder-to-classify
    minority class samples since they are in a low-density area. ADASYN uses a weighted
    distribution of the minority class based on the difficulty of classifying the
    observations. This way, more synthetic data is generated from harder samples:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ SMOTE ä¸åŒºåˆ†å°‘æ•°ç±»åˆ«æ ·æœ¬çš„å¯†åº¦åˆ†å¸ƒï¼Œ**è‡ªé€‚åº”åˆæˆé‡‡æ ·**ï¼ˆ**ADASYN**ï¼‰[6]åˆ™ä¸“æ³¨äºéš¾ä»¥åˆ†ç±»çš„å°‘æ•°ç±»åˆ«æ ·æœ¬ï¼Œå› ä¸ºå®ƒä»¬ä½äºä½å¯†åº¦åŒºåŸŸã€‚ADASYNæ ¹æ®åˆ†ç±»è§‚å¯Ÿçš„éš¾åº¦ï¼Œä½¿ç”¨å°‘æ•°ç±»åˆ«çš„åŠ æƒåˆ†å¸ƒã€‚è¿™æ ·ï¼Œä»æ›´éš¾æ ·æœ¬ä¸­ç”Ÿæˆæ›´å¤šçš„åˆæˆæ•°æ®ï¼š
- en: '![](img/B17259_02_11.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_11.jpg)'
- en: Figure 2.11 â€“ Illustration of how ADASYN works
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.11 â€“ ADASYNå·¥ä½œåŸç†çš„è¯´æ˜
- en: 'Here, we can see the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä»¥ä¸‹å†…å®¹ï¼š
- en: a) The majority and minority class samples are plotted
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: a) ç»˜åˆ¶äº†å¤šæ•°ç±»å’Œå°‘æ•°ç±»æ ·æœ¬
- en: b) Synthetic samples are generated depending on the hardness factor (explained
    later)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: b) æ ¹æ®ç¡¬åº¦å› å­ï¼ˆç¨åè§£é‡Šï¼‰ç”Ÿæˆåˆæˆæ ·æœ¬
- en: While SMOTE uses all samples from the minority class for oversampling uniformly,
    in ADASYN, the observations that are harder to classify are used more often.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ SMOTE ä½¿ç”¨å°‘æ•°ç±»ä¸­çš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œå‡åŒ€è¿‡é‡‡æ ·ï¼Œä½†åœ¨ ADASYN ä¸­ï¼Œæ›´éš¾åˆ†ç±»çš„è§‚å¯Ÿç»“æœè¢«æ›´é¢‘ç¹åœ°ä½¿ç”¨ã€‚
- en: Another difference between the two techniques is that, unlike SMOTE, ADASYN
    also uses the majority class observations while training KNN. It then decides
    the hardness of samples based on how many majority observations are its neighbors.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ç§æŠ€æœ¯ä¹‹é—´çš„å¦ä¸€ä¸ªåŒºåˆ«æ˜¯ï¼Œä¸SMOTEä¸åŒï¼ŒADASYNåœ¨è®­ç»ƒKNNæ—¶ä¹Ÿä½¿ç”¨å¤šæ•°ç±»è§‚æµ‹å€¼ã€‚ç„¶åï¼Œå®ƒæ ¹æ®æœ‰å¤šå°‘å¤šæ•°è§‚æµ‹å€¼æ˜¯å…¶é‚»å±…æ¥å†³å®šæ ·æœ¬çš„ç¡¬åº¦ã€‚
- en: Working of ADASYN
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ADASYNçš„å·¥ä½œåŸç†
- en: 'ADASYN follows a simple algorithm. Here is the step-by-step working of ADASYN:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ADASYNéµå¾ªä¸€ä¸ªç®€å•çš„ç®—æ³•ã€‚ä»¥ä¸‹æ˜¯ADASYNçš„é€æ­¥å·¥ä½œåŸç†ï¼š
- en: First, it trains a KNN on the entire dataset.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œå®ƒåœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªKNNã€‚
- en: For each observation of the minority class, we find the hardness factor. This
    factor tells us how difficult it is to classify that data point. The hardness
    factor, denoted by r, is the ratio of the number of majority class neighbors with
    the total number of neighbors. Here, r = MÂ _Â KÂ , where M is the count of majority
    class neighbors and K is the total number of nearest neighbors.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºå°‘æ•°ç±»çš„æ¯ä¸ªè§‚æµ‹å€¼ï¼Œæˆ‘ä»¬æ‰¾åˆ°ç¡¬åº¦å› å­ã€‚è¿™ä¸ªå› å­å‘Šè¯‰æˆ‘ä»¬åˆ†ç±»è¯¥æ•°æ®ç‚¹æœ‰å¤šå›°éš¾ã€‚ç¡¬åº¦å› å­ï¼Œç”¨rè¡¨ç¤ºï¼Œæ˜¯å¤šæ•°ç±»é‚»å±…æ•°ä¸é‚»å±…æ€»æ•°çš„æ¯”ç‡ã€‚åœ¨è¿™é‡Œï¼Œr = M
    / Kï¼Œå…¶ä¸­Mæ˜¯å¤šæ•°ç±»é‚»å±…çš„æ•°é‡ï¼ŒKæ˜¯æœ€è¿‘é‚»çš„æ€»æ•°ã€‚
- en: For each minority observation, we generate synthetic samples proportional to
    the hardness factor by drawing a line between the minority observation and its
    neighbors (neighbors could be from the majority class or minority class). The
    harder it is to classify a data point, the more synthetic samples will be created
    for it.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªå°‘æ•°ç±»è§‚æµ‹å€¼ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨å°‘æ•°ç±»è§‚æµ‹å€¼åŠå…¶é‚»å±…ï¼ˆé‚»å±…å¯ä»¥æ˜¯å¤šæ•°ç±»æˆ–å°‘æ•°ç±»ï¼‰ä¹‹é—´ç”»çº¿æ¥ç”Ÿæˆä¸ç¡¬åº¦å› å­æˆæ¯”ä¾‹çš„åˆæˆæ ·æœ¬ã€‚æ•°æ®ç‚¹åˆ†ç±»è¶Šå›°éš¾ï¼Œä¸ºå…¶åˆ›å»ºçš„åˆæˆæ ·æœ¬å°±è¶Šå¤šã€‚
- en: 'Letâ€™s see how we can use the ADASYN API from the `imbalanced-learn` library
    to perform oversampling of the data:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨æ¥è‡ª`imbalanced-learn`åº“çš„ADASYN APIè¿›è¡Œæ•°æ®çš„è¿‡é‡‡æ ·ï¼š
- en: '[PRE7]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is the output:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯è¾“å‡ºï¼š
- en: '[PRE8]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/B17259_02_12.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_12.jpg)'
- en: Figure 2.12 â€“ ADASYN prioritizes harder samples and incorporates majority class
    examples in KNN to assess sample hardness
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.12 â€“ ADASYNä¼˜å…ˆè€ƒè™‘è¾ƒéš¾åˆ†ç±»çš„æ ·æœ¬ï¼Œå¹¶åœ¨KNNä¸­åŒ…å«å¤šæ•°ç±»ç¤ºä¾‹ä»¥è¯„ä¼°æ ·æœ¬ç¡¬åº¦
- en: '![](img/B17259_02_13.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_13.jpg)'
- en: Figure 2.13 â€“ A memory aid summarizing various oversampling techniques
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.13 â€“ æ€»ç»“å„ç§è¿‡é‡‡æ ·æŠ€æœ¯çš„è®°å¿†è¾…åŠ©å›¾
- en: In this section, we learned about ADASYN. Next, letâ€™s see how we can deal with
    cases when our data contains categorical features.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å…³äºADASYNçš„å†…å®¹ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å½“æˆ‘ä»¬çš„æ•°æ®åŒ…å«åˆ†ç±»ç‰¹å¾æ—¶ï¼Œæˆ‘ä»¬è¯¥å¦‚ä½•å¤„ç†è¿™äº›æƒ…å†µã€‚
- en: Categorical features and SMOTE variants (SMOTE-NC and SMOTEN)
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†ç±»ç‰¹å¾å’ŒSMOTEå˜ä½“ï¼ˆSMOTE-NCå’ŒSMOTENï¼‰
- en: 'What if your data contains categorical features? A categorical feature can
    take one of a limited or fixed number of possible values, and itâ€™s a parallel
    to enumerations (enums) in computer science. These could be nominal categorical
    features that lack a natural order (for example, hair color, ethnicity, and so
    on) or ordinal categorical features that have an inherent order (for example,
    low, medium, and high):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ çš„æ•°æ®åŒ…å«åˆ†ç±»ç‰¹å¾å‘¢ï¼Ÿåˆ†ç±»ç‰¹å¾å¯ä»¥å–æœ‰é™æˆ–å›ºå®šæ•°é‡çš„å¯èƒ½å€¼ï¼Œè¿™ä¸è®¡ç®—æœºç§‘å­¦ä¸­çš„æšä¸¾ï¼ˆenumsï¼‰ç±»ä¼¼ã€‚è¿™äº›å¯èƒ½æ˜¯æ— åºçš„åˆ†ç±»ç‰¹å¾ï¼Œä¾‹å¦‚å¤´å‘é¢œè‰²ã€ç§æ—ç­‰ï¼Œæˆ–è€…æ˜¯æœ‰åºçš„åˆ†ç±»ç‰¹å¾ï¼Œä¾‹å¦‚ä½ã€ä¸­ã€é«˜ï¼š
- en: '![](img/B17259_02_14.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_14.jpg)'
- en: Figure 2.14 â€“ Categorical data and its types with examples
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.14 â€“ å¸¦æœ‰ç¤ºä¾‹çš„åˆ†ç±»æ•°æ®å’Œå…¶ç±»å‹
- en: For ordinal features, we can just encode them via sklearnâ€™s `OrdinalEncoder`,
    which assigns the categories to the values 0, 1, 2, and so on.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæœ‰åºç‰¹å¾ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡sklearnçš„`OrdinalEncoder`å¯¹å…¶è¿›è¡Œç¼–ç ï¼Œå®ƒå°†ç±»åˆ«åˆ†é…ç»™å€¼0ã€1ã€2ç­‰ã€‚
- en: 'For nominal features, none of the SMOTE variants we have learned so far will
    work. However, `RandomOverSampler` can handle nominal features too:'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºåä¹‰ç‰¹å¾ï¼Œæˆ‘ä»¬ä¹‹å‰å­¦åˆ°çš„æ‰€æœ‰SMOTEå˜ä½“éƒ½ä¸ä¼šèµ·ä½œç”¨ã€‚ç„¶è€Œï¼Œ`RandomOverSampler`ä¹Ÿå¯ä»¥å¤„ç†åä¹‰ç‰¹å¾ï¼š
- en: '[PRE9]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is the output:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯è¾“å‡ºï¼š
- en: '[PRE10]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: However, SMOTE, by default, works only on continuous data and cannot be directly
    used on categorical data. *Why?* Thatâ€™s because SMOTE works by generating a random
    point on the line joining two different data points of the minority class (also
    called interpolation). If our data is categorical and has values of â€œyesâ€ and
    â€œno,â€ we would first need to transform such values into numbers. Even when we
    do so, say â€œyesâ€ is mapped to 1 and â€œnoâ€ is mapped to 0, the interpolation via
    SMOTE may end up producing a new point of 0.3, which does not map to any real
    category.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼ŒSMOTEä»…åœ¨è¿ç»­æ•°æ®ä¸Šå·¥ä½œï¼Œä¸èƒ½ç›´æ¥ç”¨äºåˆ†ç±»æ•°æ®ã€‚*ä¸ºä»€ä¹ˆï¼Ÿ* è¿™æ˜¯å› ä¸ºSMOTEé€šè¿‡ç”Ÿæˆè¿æ¥å°‘æ•°ç±»ä¸¤ä¸ªä¸åŒæ•°æ®ç‚¹çš„çº¿ä¸Šçš„éšæœºç‚¹ï¼ˆä¹Ÿç§°ä¸ºæ’å€¼ï¼‰æ¥å·¥ä½œã€‚å¦‚æœæˆ‘ä»¬çš„æ•°æ®æ˜¯åˆ†ç±»çš„ï¼Œå¹¶ä¸”æœ‰â€œæ˜¯â€å’Œâ€œå¦â€è¿™æ ·çš„å€¼ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å°†è¿™äº›å€¼è½¬æ¢ä¸ºæ•°å­—ã€‚å³ä½¿æˆ‘ä»¬è¿™æ ·åšï¼Œæ¯”å¦‚â€œæ˜¯â€æ˜ å°„åˆ°1ï¼Œâ€œå¦â€æ˜ å°„åˆ°0ï¼ŒSMOTEçš„æ’å€¼å¯èƒ½ä¼šäº§ç”Ÿä¸€ä¸ª0.3çš„æ–°ç‚¹ï¼Œè¿™å¹¶ä¸æ˜ å°„åˆ°ä»»ä½•çœŸå®ç±»åˆ«ã€‚
- en: Also, we cannot use the `shrinkage` parameter in `RandomOverSampler` with categorical
    data because this parameter is designed only for continuous values.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œç”±äº`RandomOverSampler`ä¸­çš„`shrinkage`å‚æ•°ä»…è®¾è®¡ç”¨äºè¿ç»­å€¼ï¼Œå› æ­¤æˆ‘ä»¬æ— æ³•åœ¨åˆ†ç±»æ•°æ®ä¸­ä½¿ç”¨è¯¥å‚æ•°ã€‚
- en: 'However, two variants of SMOTE can deal with categorical features:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼ŒSMOTEçš„ä¸¤ä¸ªå˜ä½“å¯ä»¥å¤„ç†åˆ†ç±»ç‰¹å¾ï¼š
- en: '`imbalanced-learn` to oversample our dataset. The first item in the dataset
    is categorical, and the second item is continuous:'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`imbalanced-learn`å¯¹æ•°æ®è¿›è¡Œè¿‡é‡‡æ ·ã€‚æ•°æ®é›†çš„ç¬¬ä¸€é¡¹æ˜¯åˆ†ç±»çš„ï¼Œç¬¬äºŒé¡¹æ˜¯è¿ç»­çš„ï¼š
- en: '[PRE11]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here is the output:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯è¾“å‡ºï¼š
- en: '[PRE12]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Synthetic Minority Oversampling Technique for Nominal** (**SMOTEN**) is used
    for nominal categorical data. SMOTEN performs the majority vote similar to SMOTE-NC
    for all the features. It considers all features as nominal categorical, and the
    feature value of new samples is decided by taking the most frequent category of
    the nearest neighbors. The distance metric thatâ€™s used for calculating the nearest
    neighbors is called the **Value Distance Metric** (**VDM**). VDM computes the
    distance between two attribute values by considering the distribution of class
    labels associated with each value. It is based on the idea that two attribute
    values are more similar if they have similar distributions of class labels. This
    way, VDM can capture the underlying relationships between categorical attributes
    and their corresponding class labels.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç”¨äºåä¹‰æ•°æ®çš„åˆæˆå°‘æ•°è¿‡é‡‡æ ·æŠ€æœ¯**ï¼ˆ**SMOTEN**ï¼‰ç”¨äºåä¹‰åˆ†ç±»æ•°æ®ã€‚SMOTENå¯¹æ‰€æœ‰ç‰¹å¾æ‰§è¡Œä¸SMOTE-NCç±»ä¼¼çš„å¤šæ•°æŠ•ç¥¨ã€‚å®ƒå°†æ‰€æœ‰ç‰¹å¾è§†ä¸ºåä¹‰åˆ†ç±»ï¼Œæ–°æ ·æœ¬çš„ç‰¹å¾å€¼é€šè¿‡é€‰æ‹©æœ€è¿‘é‚»ä¸­æœ€é¢‘ç¹çš„ç±»åˆ«æ¥å†³å®šã€‚ç”¨äºè®¡ç®—æœ€è¿‘é‚»çš„è·ç¦»åº¦é‡ç§°ä¸º**å€¼è·ç¦»åº¦é‡**ï¼ˆ**VDM**ï¼‰ã€‚VDMé€šè¿‡è€ƒè™‘ä¸æ¯ä¸ªå€¼å…³è”çš„ç±»åˆ«æ ‡ç­¾åˆ†å¸ƒæ¥è®¡ç®—ä¸¤ä¸ªå±æ€§å€¼ä¹‹é—´çš„è·ç¦»ã€‚åŸºäºè¿™æ ·çš„æƒ³æ³•ï¼Œå¦‚æœä¸¤ä¸ªå±æ€§å€¼çš„ç±»åˆ«æ ‡ç­¾åˆ†å¸ƒç›¸ä¼¼ï¼Œåˆ™è¿™ä¸¤ä¸ªå±æ€§å€¼æ›´ç›¸ä¼¼ã€‚è¿™æ ·ï¼ŒVDMå¯ä»¥æ•æ‰åˆ†ç±»å±æ€§åŠå…¶å¯¹åº”ç±»åˆ«æ ‡ç­¾ä¹‹é—´çš„æ½œåœ¨å…³ç³»ã€‚'
- en: 'Letâ€™s look at some example code that uses SMOTEN:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹ä½¿ç”¨SMOTENçš„ä¸€äº›ç¤ºä¾‹ä»£ç ï¼š
- en: '[PRE13]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is the output:'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯è¾“å‡ºï¼š
- en: '[PRE14]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In *Table 2.1*, we can see SMOTE, SMOTEN, and SMOTENC, with a few examples
    for each technique to demonstrate the difference between them:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*è¡¨2.1*ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°SMOTEã€SMOTENå’ŒSMOTENCï¼Œä»¥åŠæ¯ç§æŠ€æœ¯çš„ä¸€äº›ç¤ºä¾‹ï¼Œä»¥å±•ç¤ºå®ƒä»¬ä¹‹é—´çš„å·®å¼‚ï¼š
- en: '| **Type** **of SMOTE** | **Features Supported** | **Example Data** |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| **SMOTEç±»å‹** | **æ”¯æŒçš„ç‰¹æ€§** | **ç¤ºä¾‹æ•°æ®** |'
- en: '| --- | --- | --- |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| SMOTE | Only numerical | features: [2.3, 4.5, 1.2], label: 0features: [3.4,
    2.2, 5.1], label: 1 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| SMOTE | ä»…æ•°å€¼ | ç‰¹å¾ï¼š[2.3, 4.5, 1.2]ï¼Œæ ‡ç­¾ï¼š0ç‰¹å¾ï¼š[3.4, 2.2, 5.1]ï¼Œæ ‡ç­¾ï¼š1 |'
- en: '| SMOTEN | Categorical(nominal or ordinal) | features: [â€˜greenâ€™, â€˜squareâ€™],
    label: 0features: [â€˜redâ€™, â€˜circleâ€™], label: 1 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| SMOTEN | åˆ†ç±»ï¼ˆåä¹‰æˆ–æœ‰åºï¼‰ | ç‰¹å¾ï¼š[â€˜greenâ€™, â€˜squareâ€™]ï¼Œæ ‡ç­¾ï¼š0ç‰¹å¾ï¼š[â€˜redâ€™, â€˜circleâ€™]ï¼Œæ ‡ç­¾ï¼š1
    |'
- en: '| SMOTENC | Numerical or categorical(nominal or ordinal) | features: [2.3,
    â€˜greenâ€™, â€˜smallâ€™, â€˜squareâ€™], label: 0features: [3.4, â€˜redâ€™, â€˜largeâ€™, â€˜circleâ€™],
    label: 1 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| SMOTENC | æ•°å€¼æˆ–åˆ†ç±»ï¼ˆåä¹‰æˆ–æœ‰åºï¼‰ | ç‰¹å¾ï¼š[2.3, â€˜greenâ€™, â€˜smallâ€™, â€˜squareâ€™]ï¼Œæ ‡ç­¾ï¼š0ç‰¹å¾ï¼š[3.4,
    â€˜redâ€™, â€˜largeâ€™, â€˜circleâ€™]ï¼Œæ ‡ç­¾ï¼š1 |'
- en: Table 2.1 â€“ SMOTE and some of its common variants with example data
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨2.1 â€“ SMOTEåŠå…¶ä¸€äº›å¸¸è§å˜ä½“ä¸ç¤ºä¾‹æ•°æ®
- en: In summary, we should use SMOTENC when we have a mix of categorical and continuous
    data types, while SMOTEN can only be used when all the columns are categorical.
    You might be curious about how the various oversampling methods compare with each
    other in terms of model performance. Weâ€™ll explore this topic in the next section.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œå½“æˆ‘ä»¬æœ‰åˆ†ç±»å’Œè¿ç»­æ•°æ®ç±»å‹çš„æ··åˆæ—¶ï¼Œåº”ä½¿ç”¨SMOTENCï¼Œè€ŒSMOTENåªèƒ½ç”¨äºæ‰€æœ‰åˆ—éƒ½æ˜¯åˆ†ç±»çš„æƒ…å†µã€‚ä½ å¯èƒ½å¯¹å„ç§è¿‡é‡‡æ ·æ–¹æ³•åœ¨æ¨¡å‹æ€§èƒ½æ–¹é¢çš„æ¯”è¾ƒæ„Ÿåˆ°å¥½å¥‡ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚æ¢è®¨è¿™ä¸ªè¯é¢˜ã€‚
- en: Model performance comparison of various oversampling methods
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å„ç§è¿‡é‡‡æ ·æ–¹æ³•çš„æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ
- en: 'Letâ€™s examine how some popular models perform with the different oversampling
    techniques weâ€™ve discussed. Weâ€™ll use two datasets for this comparison: one synthetic
    and one real-world dataset. Weâ€™ll evaluate the performance of four oversampling
    techniques, as well as no sampling, using logistic regression and random forest
    models.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ£€æŸ¥ä¸€äº›æµè¡Œçš„æ¨¡å‹åœ¨ä¸åŒè¿‡é‡‡æ ·æŠ€æœ¯ä¸‹çš„è¡¨ç°ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸¤ä¸ªæ•°æ®é›†è¿›è¡Œæ¯”è¾ƒï¼šä¸€ä¸ªæ˜¯åˆæˆæ•°æ®é›†ï¼Œå¦ä¸€ä¸ªæ˜¯çœŸå®ä¸–ç•Œæ•°æ®é›†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨é€»è¾‘å›å½’å’Œéšæœºæ£®æ—æ¨¡å‹è¯„ä¼°å››ç§è¿‡é‡‡æ ·æŠ€æœ¯ä»¥åŠæ— é‡‡æ ·çš„æ€§èƒ½ã€‚
- en: 'You can find all the related code in this bookâ€™s GitHub repository. In *Figure
    2**.15* and *Figure 2**.16*, we can see the average precision score values for
    both models on the two datasets:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨æœ¬ä¹¦çš„GitHubä»“åº“ä¸­æ‰¾åˆ°æ‰€æœ‰ç›¸å…³ä»£ç ã€‚åœ¨*å›¾2.15*å’Œ*å›¾2.16*ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªæ•°æ®é›†ä¸Šä¸¤ç§æ¨¡å‹çš„å¹³å‡ç²¾ç¡®åº¦å¾—åˆ†å€¼ï¼š
- en: '![](img/B17259_02_15.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_02_15.jpg)'
- en: Figure 2.15 â€“ Performance comparison of various oversampling techniques on a
    synthetic dataset
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.15 â€“ åœ¨åˆæˆæ•°æ®é›†ä¸Šå„ç§è¿‡é‡‡æ ·æŠ€æœ¯çš„æ€§èƒ½æ¯”è¾ƒ
- en: '![](img/B17259_02_16.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_02_16.jpg)'
- en: Figure 2.16 â€“ Performance comparison of various oversampling techniques on the
    thyroid_sick dataset
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.16 â€“ åœ¨thyroid_sickæ•°æ®é›†ä¸Šå„ç§è¿‡é‡‡æ ·æŠ€æœ¯çš„æ€§èƒ½æ¯”è¾ƒ
- en: 'Based on these plots, we can draw some useful conclusions:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®è¿™äº›å›¾è¡¨ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºä¸€äº›æœ‰ç”¨çš„ç»“è®ºï¼š
- en: '**Effectiveness of oversampling**: In general, using oversampling techniques
    seems to improve the average precision score compared to not using any sampling
    (NoSampling).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¿‡é‡‡æ ·çš„æœ‰æ•ˆæ€§**ï¼šæ€»çš„æ¥è¯´ï¼Œä½¿ç”¨è¿‡é‡‡æ ·æŠ€æœ¯ä¼¼ä¹æ¯”ä¸ä½¿ç”¨ä»»ä½•é‡‡æ ·ï¼ˆNoSamplingï¼‰æé«˜äº†å¹³å‡ç²¾ç¡®åº¦å¾—åˆ†ã€‚'
- en: '**Algorithm sensitivity**: The effectiveness of oversampling techniques varies
    depending on the machine learning algorithm used. For example, random forest seems
    to benefit more from oversampling techniques than logistic regression, especially
    on synthetic data.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç®—æ³•æ•æ„Ÿæ€§**ï¼šè¿‡é‡‡æ ·æŠ€æœ¯çš„æœ‰æ•ˆæ€§å–å†³äºæ‰€ä½¿ç”¨çš„æœºå™¨å­¦ä¹ ç®—æ³•ã€‚ä¾‹å¦‚ï¼Œéšæœºæ£®æ—ä¼¼ä¹æ¯”é€»è¾‘å›å½’ä»è¿‡é‡‡æ ·æŠ€æœ¯ä¸­å—ç›Šæ›´å¤šï¼Œå°¤å…¶æ˜¯åœ¨åˆæˆæ•°æ®ä¸Šã€‚'
- en: '`thyroid_sick` dataset but showed variations in the synthetic data.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`thyroid_sick`æ•°æ®é›†ä½†åœ¨åˆæˆæ•°æ®ä¸­æ˜¾ç¤ºäº†å˜åŒ–ã€‚'
- en: '`thyroid_sick` data'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`thyroid_sick`æ•°æ®'
- en: For random forest, Borderline-SMOTE had the highest average precision score
    on synthetic data
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºéšæœºæ£®æ—ï¼ŒBorderline-SMOTEåœ¨åˆæˆæ•°æ®ä¸Šå…·æœ‰æœ€é«˜çš„å¹³å‡ç²¾ç¡®åº¦å¾—åˆ†ã€‚
- en: '`thyroid_sick` data.*   **No clear winner**: There is no single oversampling
    technique that outperforms all others across all conditions. The choice of technique
    may depend on the specific algorithm and dataset being used.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`thyroid_sick`æ•°æ®.*   **æ²¡æœ‰æ˜æ˜¾çš„èƒœè€…**ï¼šæ²¡æœ‰ä¸€ç§è¿‡é‡‡æ ·æŠ€æœ¯åœ¨æ‰€æœ‰æ¡ä»¶ä¸‹éƒ½ä¼˜äºå…¶ä»–æŠ€æœ¯ã€‚æŠ€æœ¯çš„é€‰æ‹©å¯èƒ½å–å†³äºæ‰€ä½¿ç”¨çš„ç‰¹å®šç®—æ³•å’Œæ•°æ®é›†ã€‚'
- en: Please note that the models used here are not tuned with the best hyperparameters.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œè¿™é‡Œä½¿ç”¨çš„æ¨¡å‹æ²¡æœ‰ä½¿ç”¨æœ€ä½³è¶…å‚æ•°è¿›è¡Œè°ƒæ•´ã€‚
- en: Tuning the hyperparameters of random forest and logistic regression models may
    improve the models' performance further.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒæ•´éšæœºæ£®æ—å’Œé€»è¾‘å›å½’æ¨¡å‹çš„è¶…å‚æ•°å¯ä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚
- en: In general, there is no single technique that will always do better than the
    rest. We have multiple variables at play here, namely the â€œmodelâ€ and the â€œdata.â€
    Most of the time, the only way to know is to try out a bunch of these techniques
    and find the one that works the best for our model and data. You may find yourself
    curious about how to choose from the numerous oversampling options available.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸æƒ…å†µä¸‹ï¼Œæ²¡æœ‰ä¸€ç§æŠ€æœ¯æ€»æ˜¯æ¯”å…¶ä»–æŠ€æœ¯è¡¨ç°å¾—æ›´å¥½ã€‚è¿™é‡Œæœ‰å‡ ä¸ªå˜é‡åœ¨èµ·ä½œç”¨ï¼Œå³â€œæ¨¡å‹â€å’Œâ€œæ•°æ®â€ã€‚å¤§å¤šæ•°æ—¶å€™ï¼Œå”¯ä¸€çŸ¥é“çš„æ–¹æ³•æ˜¯å°è¯•è¿™äº›æŠ€æœ¯ä¸­çš„ä¸€ç³»åˆ—ï¼Œå¹¶æ‰¾åˆ°æœ€é€‚åˆæˆ‘ä»¬æ¨¡å‹å’Œæ•°æ®çš„é‚£ä¸ªã€‚ä½ å¯èƒ½ä¼šå¯¹å¦‚ä½•ä»ä¼—å¤šè¿‡é‡‡æ ·é€‰é¡¹ä¸­è¿›è¡Œé€‰æ‹©æ„Ÿåˆ°å¥½å¥‡ã€‚
- en: Guidance for using various oversampling techniques
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å„ç§è¿‡é‡‡æ ·æŠ€æœ¯çš„æŒ‡å—
- en: 'Now, letâ€™s review some guidelines on how to navigate through the various oversampling
    techniques we went over and how these techniques differ from each other:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹å¦‚ä½•å¯¼èˆªæˆ‘ä»¬è®¨è®ºè¿‡çš„å„ç§è¿‡é‡‡æ ·æŠ€æœ¯ä»¥åŠè¿™äº›æŠ€æœ¯å¦‚ä½•å½¼æ­¤ä¸åŒçš„ä¸€äº›æŒ‡å—ï¼š
- en: Train a model without applying any sampling techniques. This will be our model
    with baseline performance. Any oversampling technique we apply is expected to
    give a boost to this performance.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸åº”ç”¨ä»»ä½•é‡‡æ ·æŠ€æœ¯æ¥è®­ç»ƒæ¨¡å‹ã€‚è¿™å°†æ˜¯æˆ‘ä»¬å…·æœ‰åŸºçº¿æ€§èƒ½çš„æ¨¡å‹ã€‚æˆ‘ä»¬åº”ç”¨çš„ä»»ä½•è¿‡é‡‡æ ·æŠ€æœ¯éƒ½é¢„æœŸä¼šæé«˜è¿™ç§æ€§èƒ½ã€‚
- en: Start with random oversampling and add some shrinkage too. We may have to play
    with some values of shrinkage to see if the modelâ€™s performance improves.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»éšæœºè¿‡é‡‡æ ·å¼€å§‹ï¼Œå¹¶æ·»åŠ ä¸€äº›æ”¶ç¼©ã€‚æˆ‘ä»¬å¯èƒ½éœ€è¦è°ƒæ•´ä¸€äº›æ”¶ç¼©çš„å€¼ï¼Œçœ‹çœ‹æ¨¡å‹æ€§èƒ½æ˜¯å¦æœ‰æ‰€æ”¹å–„ã€‚
- en: 'When we have categorical features, we have a couple of options:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬å…·æœ‰åˆ†ç±»ç‰¹å¾æ—¶ï¼Œæˆ‘ä»¬æœ‰å‡ ç§é€‰æ‹©ï¼š
- en: Convert all categorical features into numerical features first using one-hot
    encoding, label encoding, feature hashing, or other feature transformation techniques.
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: é¦–å…ˆå°†æ‰€æœ‰åˆ†ç±»ç‰¹å¾è½¬æ¢ä¸ºæ•°å€¼ç‰¹å¾ï¼Œä½¿ç”¨ç‹¬çƒ­ç¼–ç ã€æ ‡ç­¾ç¼–ç ã€ç‰¹å¾å“ˆå¸Œæˆ–å…¶ä»–ç‰¹å¾è½¬æ¢æŠ€æœ¯ã€‚
- en: (Only for nominal categorical features) Use SMOTENC and SMOTEN directly on the
    data.
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: ï¼ˆä»…é€‚ç”¨äºåä¹‰åˆ†ç±»ç‰¹å¾ï¼‰ç›´æ¥åœ¨æ•°æ®ä¸Šä½¿ç”¨SMOTENCå’ŒSMOTENã€‚
- en: Apply various oversampling techniques â€“ random oversampling, SMOTE, Borderline-SMOTE,
    and ADASYN â€“ and measure the modelâ€™s performance on metrics applicable to your
    problem, such as the average precision score, ROC-AUC, precision, recall, F1 score,
    and more.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åº”ç”¨å„ç§è¿‡é‡‡æ ·æŠ€æœ¯â€”â€”éšæœºè¿‡é‡‡æ ·ã€SMOTEã€Borderline-SMOTEå’ŒADASYNâ€”â€”å¹¶åœ¨é€‚ç”¨äºæ‚¨é—®é¢˜çš„æŒ‡æ ‡ä¸Šè¡¡é‡æ¨¡å‹çš„æ€§èƒ½ï¼Œä¾‹å¦‚å¹³å‡ç²¾åº¦å¾—åˆ†ã€ROC-AUCã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰ã€‚
- en: Since oversampling alters the distribution of the training dataset, which is
    not the case for the test set or the real world, using oversampling can potentially
    generate biased predictions. After using oversampling, it can be essential to
    recalibrate our modelâ€™s probability scores depending on the application. Recalibration
    of the model corrects any bias introduced by altering the class distribution,
    ensuring more reliable decision-making when deployed. Similarly, adjusting the
    classification threshold is key for accurate model interpretation, especially
    with imbalanced datasets. For more details on recalibration and threshold adjustment,
    please see [*Chapter 10*](B17259_10.xhtml#_idTextAnchor279), *Model Calibration*,
    and [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive* *Learning*,
    respectively.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç”±äºè¿‡é‡‡æ ·æ”¹å˜äº†è®­ç»ƒæ•°æ®é›†çš„åˆ†å¸ƒï¼Œè€Œæµ‹è¯•é›†æˆ–ç°å®ä¸–ç•Œå¹¶éå¦‚æ­¤ï¼Œä½¿ç”¨è¿‡é‡‡æ ·å¯èƒ½ä¼šäº§ç”Ÿæ½œåœ¨çš„åå·®é¢„æµ‹ã€‚ä½¿ç”¨è¿‡é‡‡æ ·åï¼Œæ ¹æ®åº”ç”¨é‡æ–°æ ¡å‡†æ¨¡å‹çš„æ¦‚ç‡å¾—åˆ†å¯èƒ½è‡³å…³é‡è¦ã€‚æ¨¡å‹çš„é‡æ–°æ ¡å‡†çº æ­£äº†ç”±äºæ”¹å˜ç±»åˆ«åˆ†å¸ƒè€Œå¼•å…¥çš„ä»»ä½•åå·®ï¼Œç¡®ä¿åœ¨éƒ¨ç½²æ—¶åšå‡ºæ›´å¯é çš„å†³ç­–ã€‚åŒæ ·ï¼Œè°ƒæ•´åˆ†ç±»é˜ˆå€¼å¯¹äºå‡†ç¡®è§£é‡Šæ¨¡å‹è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨ä¸å¹³è¡¡æ•°æ®é›†çš„æƒ…å†µä¸‹ã€‚æœ‰å…³é‡æ–°æ ¡å‡†å’Œé˜ˆå€¼è°ƒæ•´çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[*ç¬¬åç« *](B17259_10.xhtml#_idTextAnchor279)ã€Šæ¨¡å‹æ ¡å‡†ã€‹å’Œ[*ç¬¬äº”ç« *](B17259_05.xhtml#_idTextAnchor151)ã€Šæˆæœ¬æ•æ„Ÿå­¦ä¹ ã€‹ã€‚
- en: When to avoid oversampling
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¿å…è¿‡é‡‡æ ·çš„æ—¶æœº
- en: 'In [*Chapter 1*](B17259_01.xhtml#_idTextAnchor015), *Introduction to Data Imbalance
    in Machine Learning*, we discussed scenarios where data imbalance may not be a
    concern. Those considerations should be revisited before you opt for oversampling
    techniques. Despite criticisms, the applicability of oversampling should be evaluated
    on a case-by-case basis. Here are some additional technical considerations to
    keep in mind when choosing to apply oversampling techniques:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[*ç¬¬ä¸€ç« *](B17259_01.xhtml#_idTextAnchor015)ã€Šæœºå™¨å­¦ä¹ ä¸­çš„æ•°æ®ä¸å¹³è¡¡ä»‹ç»ã€‹ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†æ•°æ®ä¸å¹³è¡¡å¯èƒ½ä¸æ˜¯é—®é¢˜çš„åœºæ™¯ã€‚åœ¨æ‚¨é€‰æ‹©è¿‡é‡‡æ ·æŠ€æœ¯ä¹‹å‰ï¼Œåº”è¯¥é‡æ–°å®¡è§†è¿™äº›è€ƒè™‘å› ç´ ã€‚å°½ç®¡å­˜åœ¨æ‰¹è¯„ï¼Œä½†è¿‡é‡‡æ ·çš„é€‚ç”¨æ€§åº”è¯¥æ ¹æ®å…·ä½“æƒ…å†µè¯„ä¼°ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›åœ¨é€‰æ‹©åº”ç”¨è¿‡é‡‡æ ·æŠ€æœ¯æ—¶éœ€è¦è€ƒè™‘çš„é¢å¤–æŠ€æœ¯å› ç´ ï¼š
- en: '**Computational cost**: Oversampling increases the datasetâ€™s size, leading
    to higher computational demands in terms of processing time and hardware resources.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è®¡ç®—æˆæœ¬**ï¼šè¿‡é‡‡æ ·å¢åŠ äº†æ•°æ®é›†çš„å¤§å°ï¼Œå¯¼è‡´å¤„ç†æ—¶é—´å’Œç¡¬ä»¶èµ„æºæ–¹é¢çš„è®¡ç®—éœ€æ±‚æ›´é«˜ã€‚'
- en: '**Data quality**: If the minority class data is noisy or has many outliers,
    oversampling can introduce more noise, reducing model reliability.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ•°æ®è´¨é‡**ï¼šå¦‚æœå°‘æ•°ç±»æ•°æ®æœ‰å™ªå£°æˆ–è®¸å¤šå¼‚å¸¸å€¼ï¼Œè¿‡é‡‡æ ·å¯èƒ½ä¼šå¼•å…¥æ›´å¤šå™ªå£°ï¼Œé™ä½æ¨¡å‹å¯é æ€§ã€‚'
- en: '**Classifier limitations**: In scenarios with system constraints, such as extremely
    low latency, or when dealing with legacy systems, the use of strong classifiers
    (complex and more accurate models) may not be feasible. In these cases, we may
    be limited to using weak classifiers. Weak classifiers are simpler and less accurate
    but require fewer computational resources and have lower runtime latency. In such
    situations, oversampling can be beneficial [7]. For strong classifiers, oversampling
    may offer diminishing returns, and optimizing the decision threshold could sometimes
    serve as a simpler, less resource-intensive alternative.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åˆ†ç±»å™¨é™åˆ¶**ï¼šåœ¨ç³»ç»Ÿçº¦æŸåœºæ™¯ä¸­ï¼Œä¾‹å¦‚æä½å»¶è¿Ÿæˆ–å¤„ç†é—ç•™ç³»ç»Ÿæ—¶ï¼Œä½¿ç”¨å¼ºå¤§çš„åˆ†ç±»å™¨ï¼ˆå¤æ‚ä¸”æ›´å‡†ç¡®çš„æ¨¡å‹ï¼‰å¯èƒ½ä¸å¯è¡Œã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½åªèƒ½ä½¿ç”¨å¼±åˆ†ç±»å™¨ã€‚å¼±åˆ†ç±»å™¨æ›´ç®€å•ã€ç²¾åº¦è¾ƒä½ï¼Œä½†éœ€è¦è¾ƒå°‘çš„è®¡ç®—èµ„æºï¼Œå¹¶ä¸”å…·æœ‰æ›´ä½çš„è¿è¡Œæ—¶å»¶è¿Ÿã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿‡é‡‡æ ·å¯èƒ½æ˜¯æœ‰ç›Šçš„[7]ã€‚å¯¹äºå¼ºå¤§çš„åˆ†ç±»å™¨ï¼Œè¿‡é‡‡æ ·å¯èƒ½å¸¦æ¥é€’å‡çš„å›æŠ¥ï¼Œæœ‰æ—¶ä¼˜åŒ–å†³ç­–é˜ˆå€¼å¯èƒ½æ˜¯ä¸€ä¸ªæ›´ç®€å•ã€èµ„æºæ¶ˆè€—æ›´å°‘çš„æ›¿ä»£æ–¹æ¡ˆã€‚'
- en: Consider these factors when deciding whether to use oversampling methods for
    imbalanced datasets.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å†³å®šæ˜¯å¦ä½¿ç”¨è¿‡é‡‡æ ·æ–¹æ³•æ¥è§£å†³ä¸å¹³è¡¡æ•°æ®é›†æ—¶ï¼Œè€ƒè™‘ä»¥ä¸‹å› ç´ ã€‚
- en: '*Table 2.2* summarizes the key ideas, pros, and cons of various oversampling
    techniques. This can help you better evaluate which oversampling method to choose:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¡¨ 2.2* æ€»ç»“äº†å„ç§è¿‡é‡‡æ ·æŠ€æœ¯çš„å…³é”®æ€æƒ³ã€ä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚è¿™å¯ä»¥å¸®åŠ©ä½ æ›´å¥½åœ°è¯„ä¼°é€‰æ‹©å“ªç§è¿‡é‡‡æ ·æ–¹æ³•ï¼š'
- en: '|  | **SMOTE** | **Borderline-SMOTE** | **ADASYN** | **SMOTE-NC** **and SMOTEN**
    |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | **SMOTE** | **Borderline-SMOTE** | **ADASYN** | **SMOTE-NC** å’Œ **SMOTEN**
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Key idea | Choose random points on the line joining the nearest neighbors
    of minority class examples. | Choose the minority samples on the boundary between
    the majority and minority classes. Perform SMOTE for such samples on the boundary.
    | Automatically decides the number of minority class samples to generate according
    to density distribution. More points are generated where the density distribution
    is low. | It performs a majority vote for the categorical features. |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| æ ¸å¿ƒæ€æƒ³ | åœ¨è¿æ¥å°‘æ•°ç±»æ ·æœ¬æœ€è¿‘é‚»çš„çº¿ä¸Šé€‰æ‹©éšæœºç‚¹ã€‚ | åœ¨å¤šæ•°ç±»å’Œå°‘æ•°ç±»ä¹‹é—´çš„è¾¹ç•Œä¸Šé€‰æ‹©å°‘æ•°æ ·æœ¬ã€‚å¯¹è¿™æ ·çš„æ ·æœ¬åœ¨è¾¹ç•Œä¸Šæ‰§è¡Œ SMOTEã€‚ |
    æ ¹æ®å¯†åº¦åˆ†å¸ƒè‡ªåŠ¨å†³å®šè¦ç”Ÿæˆçš„å°‘æ•°ç±»æ ·æœ¬æ•°é‡ã€‚åœ¨å¯†åº¦åˆ†å¸ƒä½çš„åœ°æ–¹ç”Ÿæˆæ›´å¤šç‚¹ã€‚ | å®ƒå¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œå¤šæ•°æŠ•ç¥¨ã€‚ |'
- en: '| Pro | Usually reduces false negatives. | Creates synthetic samples that are
    not naÃ¯ve copies of the known data. | It cares about the density distribution
    of different classes. | It works with categorical data. |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| ä¼˜ç‚¹ | é€šå¸¸å‡å°‘å‡é˜´æ€§ã€‚ | åˆ›å»ºçš„åˆæˆæ ·æœ¬ä¸æ˜¯å·²çŸ¥æ•°æ®çš„ç®€å•å¤åˆ¶ã€‚ | å®ƒå…³æ³¨ä¸åŒç±»åˆ«çš„å¯†åº¦åˆ†å¸ƒã€‚ | å®ƒé€‚ç”¨äºåˆ†ç±»æ•°æ®ã€‚ |'
- en: '| Con | Overlapping classes may occur and can introduce more noise to data.
    This may not work well with high-dimensional data or multi-class classification
    problems. | It does not care about the distribution of minority class examples.
    | It focuses on areas where there is overlap between classes. It may focus too
    much on outliers, resulting in poor model performance. | The same as SMOTE. |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| ç¼ºç‚¹ | å¯èƒ½ä¼šå‘ç”Ÿé‡å çš„ç±»åˆ«ï¼Œå¹¶å¯èƒ½å‘æ•°æ®ä¸­å¼•å…¥æ›´å¤šå™ªå£°ã€‚è¿™å¯èƒ½ä¸é€‚åˆé«˜ç»´æ•°æ®æˆ–å¤šç±»åˆ«åˆ†ç±»é—®é¢˜ã€‚ | å®ƒä¸å…³å¿ƒå°‘æ•°ç±»æ ·æœ¬çš„åˆ†å¸ƒã€‚ | å®ƒå…³æ³¨ç±»åˆ«ä¹‹é—´é‡å çš„åŒºåŸŸã€‚å®ƒå¯èƒ½è¿‡åˆ†å…³æ³¨å¼‚å¸¸å€¼ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸ä½³ã€‚
    | ä¸ SMOTE ç›¸åŒã€‚ |'
- en: Table 2.2 â€“ Summarizing the various oversampling techniques that were discussed
    in this chapter
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 2.2 â€“ æ€»ç»“æœ¬ç« è®¨è®ºçš„å„ç§è¿‡é‡‡æ ·æŠ€æœ¯
- en: In this section, we looked at some general guidelines to apply the various oversampling
    techniques we learned about in this chapter and the pros and cons of using them.
    Next, we will look at how to extend the various oversampling methods to multi-class
    classification problems.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•åº”ç”¨æœ¬ç« ä¸­å­¦ä¹ åˆ°çš„å„ç§è¿‡é‡‡æ ·æŠ€æœ¯ä»¥åŠä½¿ç”¨å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•å°†å„ç§è¿‡é‡‡æ ·æ–¹æ³•æ‰©å±•åˆ°å¤šç±»åˆ«åˆ†ç±»é—®é¢˜ã€‚
- en: Oversampling in multi-class classification
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šç±»åˆ«åˆ†ç±»ä¸­çš„è¿‡é‡‡æ ·
- en: 'In multi-class classification problems, we have more than two classes or labels
    to be predicted, and hence more than one class may be imbalanced. This adds some
    more complexity to the problem. However, we can apply the same techniques to multi-class
    classification problems as well. The `imbalanced-learn` library provides the option
    to deal with multi-class classification in almost all the supported methods. We
    can choose from various sampling strategies using the `sampling_strategy` parameter.
    For multi-class classification, we can pass some fixed string values (called built-in
    strategies) to the `sampling_strategy` parameter in the SMOTE API. We can also
    pass a dictionary with the following:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤šç±»åˆ«åˆ†ç±»é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬æœ‰è¶…è¿‡ä¸¤ä¸ªç±»åˆ«æˆ–æ ‡ç­¾éœ€è¦é¢„æµ‹ï¼Œå› æ­¤å¯èƒ½å­˜åœ¨å¤šä¸ªç±»åˆ«ä¸å¹³è¡¡ã€‚è¿™ç»™é—®é¢˜å¢åŠ äº†æ›´å¤šå¤æ‚æ€§ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†ç›¸åŒçš„æŠ€å·§åº”ç”¨äºå¤šç±»åˆ«åˆ†ç±»é—®é¢˜ã€‚`imbalanced-learn`
    åº“æä¾›äº†åœ¨å‡ ä¹æ‰€æœ‰æ”¯æŒçš„æ–¹æ³•ä¸­å¤„ç†å¤šç±»åˆ«åˆ†ç±»çš„é€‰é¡¹ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ `sampling_strategy` å‚æ•°é€‰æ‹©å„ç§é‡‡æ ·ç­–ç•¥ã€‚å¯¹äºå¤šç±»åˆ«åˆ†ç±»ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨
    SMOTE API ä¸­çš„ `sampling_strategy` å‚æ•°ä¼ é€’ä¸€äº›å›ºå®šçš„å­—ç¬¦ä¸²å€¼ï¼ˆç§°ä¸ºå†…ç½®ç­–ç•¥ï¼‰ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä¼ é€’ä¸€ä¸ªåŒ…å«ä»¥ä¸‹å†…å®¹çš„å­—å…¸ï¼š
- en: Keys as the class labels
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»¥ç±»åˆ«æ ‡ç­¾ä½œä¸ºé”®
- en: Values as the number of samples of that class
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»¥è¯¥ç±»åˆ«çš„æ ·æœ¬æ•°é‡ä½œä¸ºå€¼
- en: 'Here are the built-in strategies for `sampling_strategy` when using the parameter
    as a string:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨å‚æ•°ä½œä¸ºå­—ç¬¦ä¸²æ—¶ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å†…ç½®çš„ `sampling_strategy` æ ·æœ¬ç­–ç•¥ï¼š
- en: The `minority` strategy resamples only the minority class.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minority` ç­–ç•¥ä»…é‡æ–°é‡‡æ ·å°‘æ•°ç±»ã€‚'
- en: The `not minority` strategy resamples all classes except the minority class.
    This may be helpful in the case of multi-class imbalance, where we have more than
    two classes and multiple classes are imbalanced, but we donâ€™t want to touch the
    minority class.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`not minority` ç­–ç•¥é‡æ–°é‡‡æ ·é™¤äº†å°‘æ•°ç±»ä»¥å¤–çš„æ‰€æœ‰ç±»åˆ«ã€‚åœ¨å¤šç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µä¸‹ï¼Œè¿™å¯èƒ½æ˜¯æœ‰å¸®åŠ©çš„ï¼Œå› ä¸ºæˆ‘ä»¬æœ‰è¶…è¿‡ä¸¤ä¸ªç±»åˆ«ï¼Œå¹¶ä¸”å¤šä¸ªç±»åˆ«ä¸å¹³è¡¡ï¼Œä½†æˆ‘ä»¬ä¸æƒ³è§¦åŠå°‘æ•°ç±»ã€‚'
- en: The `not majority` strategy resamples all classes except the majority class.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`not majority`ç­–ç•¥é‡æ–°é‡‡æ ·æ‰€æœ‰ç±»åˆ«ï¼Œé™¤äº†å¤šæ•°ç±»åˆ«ã€‚'
- en: The `all` strategy resamples all classes.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`all`ç­–ç•¥é‡æ–°é‡‡æ ·æ‰€æœ‰ç±»åˆ«ã€‚'
- en: The `auto` strategy is the same as the `not` `majority` strategy.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auto`ç­–ç•¥ä¸`not``majority`ç­–ç•¥ç›¸åŒã€‚'
- en: The following code shows the usage of SMOTE for multi-class classification using
    various sampling strategies.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç å±•ç¤ºäº†ä½¿ç”¨å„ç§é‡‡æ ·ç­–ç•¥è¿›è¡Œå¤šç±»åˆ†ç±»æ—¶SMOTEçš„ä½¿ç”¨æ–¹æ³•ã€‚
- en: 'First, letâ€™s create a dataset containing 100 samples with three classes that
    have weights of 0.1, 0.4, and 0.5:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…å«100ä¸ªæ ·æœ¬çš„æ•°æ®é›†ï¼Œå…¶ä¸­ä¸‰ä¸ªç±»åˆ«çš„æƒé‡åˆ†åˆ«ä¸º0.1ã€0.4å’Œ0.5ï¼š
- en: '[PRE15]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here is the output:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯è¾“å‡ºï¼š
- en: '[PRE16]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As expected, our dataset contains the three classes in the ratio 10:40:50 for
    classes 0, 1, and 2, respectively.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚é¢„æœŸï¼Œæˆ‘ä»¬çš„æ•°æ®é›†åŒ…å«ä¸‰ä¸ªç±»åˆ«ï¼Œåˆ†åˆ«ä¸ºç±»åˆ«0ã€1å’Œ2ï¼Œå…¶æ¯”ä¾‹ä¸º10:40:50ã€‚
- en: 'Now, letâ€™s apply SMOTE with the â€œ*minority*â€ sampling strategy. This will oversample
    the class with the least number of samples:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬åº”ç”¨å¸¦æœ‰â€œ*å°‘æ•°*â€é‡‡æ ·ç­–ç•¥çš„SMOTEã€‚è¿™å°†å¢åŠ æœ€å°‘æ ·æœ¬æ•°çš„ç±»åˆ«ï¼š
- en: '[PRE17]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here is the output:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯è¾“å‡ºï¼š
- en: '[PRE18]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Since class 0 previously had the least number of samples, the â€œ*minority*â€ sampling
    strategy only oversampled class 0, making the number of samples equal to the number
    of samples in the majority class.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºç±»åˆ«0ä¹‹å‰æ ·æœ¬æ•°æœ€å°‘ï¼Œå› æ­¤â€œ*å°‘æ•°*â€é‡‡æ ·ç­–ç•¥åªå¯¹ç±»åˆ«0è¿›è¡Œäº†è¿‡é‡‡æ ·ï¼Œä½¿å¾—æ ·æœ¬æ•°ç­‰äºå¤šæ•°ç±»åˆ«çš„æ ·æœ¬æ•°ã€‚
- en: 'In the following code, weâ€™re using a dictionary for oversampling. Here, for
    each class label (0, 1, or 2) as `key` in the `sampling_strategy` dictionary,
    we have the number of desired samples for each targeted class as `value`:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å­—å…¸è¿›è¡Œè¿‡é‡‡æ ·ã€‚åœ¨è¿™é‡Œï¼Œå¯¹äº`sampling_strategy`å­—å…¸ä¸­çš„æ¯ä¸ªç±»åˆ«æ ‡ç­¾ï¼ˆ0ã€1æˆ–2ï¼‰ä½œä¸º`key`ï¼Œæˆ‘ä»¬éƒ½æœ‰æ¯ä¸ªç›®æ ‡ç±»åˆ«çš„æœŸæœ›æ ·æœ¬æ•°ä½œä¸º`value`ï¼š
- en: '[PRE19]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here is the output:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯è¾“å‡ºï¼š
- en: '[PRE20]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Tip
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: å°è´´å£«
- en: Please note that when using `dict` within `sampling_strategy`, the number of
    desired samples for each class should be greater than or equal to the original
    number of samples. Otherwise, the `fit_resample` API will throw an exception.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå½“åœ¨`sampling_strategy`ä¸­ä½¿ç”¨`dict`æ—¶ï¼Œæ¯ä¸ªç±»åˆ«çš„æœŸæœ›æ ·æœ¬æ•°åº”å¤§äºæˆ–ç­‰äºåŸå§‹æ ·æœ¬æ•°ã€‚å¦åˆ™ï¼Œ`fit_resample`APIå°†æŠ›å‡ºå¼‚å¸¸ã€‚
- en: In this section, we saw how to extend oversampling strategies to handle cases
    when we have imbalanced datasets with more than two classes. Most of the time,
    the â€œautoâ€ `sampling_strategy` would be good enough and would balance all the
    classes.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¦‚ä½•å°†è¿‡é‡‡æ ·ç­–ç•¥æ‰©å±•åˆ°å¤„ç†å…·æœ‰ä¸¤ä¸ªä»¥ä¸Šç±»åˆ«çš„æ•°æ®ä¸å¹³è¡¡æƒ…å†µã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œâ€œautoâ€`é‡‡æ ·ç­–ç•¥`å°±è¶³å¤Ÿå¥½äº†ï¼Œå¹¶ä¸”ä¼šå¹³è¡¡æ‰€æœ‰ç±»åˆ«ã€‚
- en: Summary
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: In this chapter, we went through various oversampling techniques for dealing
    with imbalanced datasets and applied them using Pythonâ€™s `imbalanced-learn` library
    (also called `imblearn`). We also saw the internal workings of some of the techniques
    by implementing them from scratch. While random oversampling generates new minority
    class samples by duplicating them, SMOTE-based techniques work by choosing random
    samples in the direction of nearest neighbors of the minority class samples. Though
    oversampling can potentially overfit the model on your data, it usually has more
    pros than cons, depending on the data and model.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†çš„å„ç§è¿‡é‡‡æ ·æŠ€æœ¯ï¼Œå¹¶ä½¿ç”¨Pythonçš„`imbalanced-learn`åº“ï¼ˆä¹Ÿç§°ä¸º`imblearn`ï¼‰è¿›è¡Œäº†åº”ç”¨ã€‚æˆ‘ä»¬è¿˜é€šè¿‡ä»å¤´å®ç°ä¸€äº›æŠ€æœ¯æ¥äº†è§£è¿™äº›æŠ€æœ¯çš„å†…éƒ¨å·¥ä½œåŸç†ã€‚è™½ç„¶è¿‡é‡‡æ ·å¯èƒ½ä¼šæ½œåœ¨åœ°ä½¿æ¨¡å‹å¯¹æ•°æ®è¿›è¡Œè¿‡æ‹Ÿåˆï¼Œä½†å®ƒé€šå¸¸æ¯”ç¼ºç‚¹å¤šï¼Œå…·ä½“å–å†³äºæ•°æ®å’Œæ¨¡å‹ã€‚
- en: We applied them to some of the synthesized and publicly available datasets and
    benchmarked their performance and effectiveness. We saw how different oversampling
    techniques may lead to model performance on a varying scale, so it becomes crucial
    to try a few different oversampling techniques to decide on the one thatâ€™s most
    optimal for our data.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å®ƒä»¬åº”ç”¨äºä¸€äº›åˆæˆå’Œå…¬å¼€å¯ç”¨çš„æ•°æ®é›†ï¼Œå¹¶å¯¹å…¶æ€§èƒ½å’Œæœ‰æ•ˆæ€§è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çœ‹åˆ°äº†ä¸åŒçš„è¿‡é‡‡æ ·æŠ€æœ¯å¦‚ä½•å¯èƒ½å¯¼è‡´æ¨¡å‹æ€§èƒ½åœ¨å„ä¸ªå°ºåº¦ä¸Šå˜åŒ–ï¼Œå› æ­¤å°è¯•å‡ ç§ä¸åŒçš„è¿‡é‡‡æ ·æŠ€æœ¯ä»¥å†³å®šæœ€é€‚åˆæˆ‘ä»¬æ•°æ®çš„æ–¹æ³•å˜å¾—è‡³å…³é‡è¦ã€‚
- en: If you feel intrigued by the prospect of discovering oversampling approaches
    relevant to deep learning models, we invite you to check out [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205),
    *Data-Level Deep Learning Methods*, where weâ€™ll discuss data-level techniques
    within the realm of deep learning.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¢«å‘ç°ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸å…³çš„è¿‡é‡‡æ ·æ–¹æ³•æ‰€å¸å¼•ï¼Œæˆ‘ä»¬é‚€è¯·ä½ æŸ¥çœ‹[*ç¬¬7ç« *](B17259_07.xhtml#_idTextAnchor205)ï¼Œ*æ•°æ®çº§æ·±åº¦å­¦ä¹ æ–¹æ³•*ï¼Œæˆ‘ä»¬å°†è®¨è®ºæ·±åº¦å­¦ä¹ é¢†åŸŸå†…çš„æ•°æ®çº§æŠ€æœ¯ã€‚
- en: In the next chapter, we will go over various undersampling techniques.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å„ç§æ¬ é‡‡æ ·æŠ€æœ¯ã€‚
- en: Exercises
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»ƒä¹ 
- en: Explore the two variants of SMOTE, namely KMeans-SMOTE and SVM-SMOTE, from the
    `imbalanced-learn` library, not discussed in this chapter. Compare their performance
    with vanilla SMOTE, Borderline-SMOTE, and ADASYN using the logistic regression
    and random forest models.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¢ç´¢`imbalanced-learn`åº“ä¸­çš„SMOTEçš„ä¸¤ä¸ªå˜ä½“ï¼Œå³KMeans-SMOTEå’ŒSVM-SMOTEï¼Œæœ¬ç« æœªè®¨è®ºã€‚ä½¿ç”¨é€»è¾‘å›å½’å’Œéšæœºæ£®æ—æ¨¡å‹æ¯”è¾ƒå®ƒä»¬ä¸vanilla
    SMOTEã€Borderline-SMOTEå’ŒADASYNçš„æ€§èƒ½ã€‚
- en: For a classification problem with two classes, letâ€™s say the minority class
    to majority class ratio is 1:20\. How should we balance this dataset? Should we
    apply the balancing technique at test or evaluation time? Please provide a reason
    for your answer.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºä¸€ä¸ªæœ‰ä¸¤ä¸ªç±»åˆ«çš„åˆ†ç±»é—®é¢˜ï¼Œå‡è®¾å°‘æ•°ç±»ä¸å¤šæ•°ç±»çš„æ¯”ä¾‹æ˜¯1:20ã€‚æˆ‘ä»¬åº”è¯¥å¦‚ä½•å¹³è¡¡è¿™ä¸ªæ•°æ®é›†ï¼Ÿæˆ‘ä»¬åº”è¯¥åœ¨æµ‹è¯•æˆ–è¯„ä¼°æ—¶é—´åº”ç”¨å¹³è¡¡æŠ€æœ¯å—ï¼Ÿè¯·æä¾›ä½ çš„ç­”æ¡ˆçš„ç†ç”±ã€‚
- en: Letâ€™s say we are trying to build a model that can estimate whether a person
    can be granted a bank loan or not. Out of the 5,000 observations we have, only
    500 people got the loan approved. To balance the dataset, we duplicate the approved
    people data and then split it into train, test, and validation datasets. Are there
    any issues with using this approach?
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æ­£åœ¨å°è¯•æ„å»ºä¸€ä¸ªæ¨¡å‹ï¼Œç”¨äºä¼°è®¡ä¸€ä¸ªäººæ˜¯å¦èƒ½è·å¾—é“¶è¡Œè´·æ¬¾ã€‚åœ¨æˆ‘ä»¬æ‹¥æœ‰çš„5,000ä¸ªè§‚æµ‹å€¼ä¸­ï¼Œåªæœ‰500äººçš„è´·æ¬¾è·å¾—äº†æ‰¹å‡†ã€‚ä¸ºäº†å¹³è¡¡æ•°æ®é›†ï¼Œæˆ‘ä»¬å°†æ‰¹å‡†çš„äººçš„æ•°æ®è¿›è¡Œå¤åˆ¶ï¼Œç„¶åå°†å…¶åˆ†ä¸ºè®­ç»ƒé›†ã€æµ‹è¯•é›†å’ŒéªŒè¯é›†ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•æ˜¯å¦å­˜åœ¨ä»»ä½•é—®é¢˜ï¼Ÿ
- en: Data normalization helps in dealing with data imbalance. Is this true? Why or
    why not?
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ•°æ®å½’ä¸€åŒ–æœ‰åŠ©äºå¤„ç†æ•°æ®ä¸å¹³è¡¡ã€‚è¿™æ˜¯çœŸçš„å—ï¼Ÿä¸ºä»€ä¹ˆæˆ–ä¸ºä»€ä¹ˆä¸ï¼Ÿ
- en: 'Explore the various oversampling APIs available from the `imbalanced-learn`
    library here: [https://imbalanced-learn.org/stable/references/over_sampling.html](https://imbalanced-learn.org/stable/references/over_sampling.html).
    Pay attention to the various parameters of each of the APIs.'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæ¢ç´¢`imbalanced-learn`åº“ä¸­å¯ç”¨çš„å„ç§è¿‡é‡‡æ ·APIï¼š[https://imbalanced-learn.org/stable/references/over_sampling.html](https://imbalanced-learn.org/stable/references/over_sampling.html)ã€‚è¯·æ³¨æ„æ¯ä¸ªAPIçš„å„ç§å‚æ•°ã€‚
- en: References
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '*Protecting Personal Data in Grabâ€™s Imagery* (2021), [https://engineering.grab.com/protecting-personal-data-in-grabs-imagery](https://engineering.grab.com/protecting-personal-data-in-grabs-imagery).'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*ã€ŠGrabçš„å›¾åƒä¸­ä¿æŠ¤ä¸ªäººæ•°æ®ã€‹*ï¼ˆ2021å¹´ï¼‰ï¼Œ[https://engineering.grab.com/protecting-personal-data-in-grabs-imagery](https://engineering.grab.com/protecting-personal-data-in-grabs-imagery)ã€‚'
- en: 'N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, *SMOTE: Synthetic
    Minority Over-sampling Technique*, jair, vol. 16, pp. 321â€“357, Jun. 2002, doi:
    10.1613/jair.953.'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'N. V. Chawlaï¼ŒK. W. Bowyerï¼ŒL. O. Hallï¼Œå’Œ W. P. Kegelmeyerï¼Œ*SMOTEï¼šåˆæˆå°‘æ•°ç±»è¿‡é‡‡æ ·æŠ€æœ¯*ï¼Œjairï¼Œç¬¬16å·ï¼Œç¬¬321-357é¡µï¼Œ2002å¹´6æœˆï¼Œdoi:
    10.1613/jair.953ã€‚'
- en: '*Live Site Incident escalation forecast* (2023), [https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178](https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178).'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*ã€Šå®æ—¶ç½‘ç«™äº‹ä»¶å‡çº§é¢„æµ‹ã€‹*ï¼ˆ2023å¹´ï¼‰ï¼Œ[https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178](https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178)ã€‚'
- en: 'H. Han, W.-Y. Wang, and B.-H. Mao, *Borderline-SMOTE: A New Over-Sampling Method
    in Imbalanced Data Sets Learning*, in Advances in Intelligent Computing, D.-S.
    Huang, X.-P. Zhang, and G.-B. Huang, Eds., in Lecture Notes in Computer Science,
    vol. 3644\. Berlin, Heidelberg: Springer Berlin Heidelberg, 2005, pp. 878â€“887\.
    doi: 10.1007/11538059_91.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'H. Hanï¼ŒW.-Y. Wangï¼Œå’ŒB.-H. Maoï¼Œ*Borderline-SMOTEï¼šä¸å¹³è¡¡æ•°æ®é›†å­¦ä¹ ä¸­çš„æ–°è¿‡é‡‡æ ·æ–¹æ³•*ï¼Œåœ¨ã€Šæ™ºèƒ½è®¡ç®—è¿›å±•ã€‹ä¸­ï¼ŒD.-S.
    Huangï¼ŒX.-P. Zhangï¼Œå’ŒG.-B. Huangï¼Œç¼–ï¼Œåœ¨ã€Šè®¡ç®—æœºç§‘å­¦è®²åº§ç¬”è®°ã€‹ç¬¬3644å·ã€‚æŸæ—ï¼Œæµ·å¾·å ¡ï¼šSpringer Berlin Heidelbergï¼Œ2005å¹´ï¼Œç¬¬878-887é¡µã€‚doi:
    10.1007/11538059_91ã€‚'
- en: 'P. Meiyappan and M. Bales, *Position Paper: Reducing Amazonâ€™s packaging waste
    using multimodal deep learning*, (2021), article: [https://www.amazon.science/latest-news/deep-learning-machine-learning-computer-vision-applications-reducing-amazon-package-waste](https://www.amazon.science/latest-news/deep-learning-machine-learning-computer-vision-applications-reducing-amazon-package-waste),
    paper: [https://www.amazon.science/publications/position-paper-reducing-amazons-packaging-wasteusing-multimodal-deep-learning](https://www.amazon.science/publications/position-paper-reducing-amazons-packaging-wasteusing-multimodal-deep-learning).'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: P. Meiyappanå’ŒM. Balesï¼Œ*ç«‹åœºæ–‡ä»¶ï¼šä½¿ç”¨å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ å‡å°‘äºšé©¬é€Šçš„åŒ…è£…æµªè´¹*ï¼Œï¼ˆ2021å¹´ï¼‰ï¼Œæ–‡ç« ï¼š[https://www.amazon.science/latest-news/deep-learning-machine-learning-computer-vision-applications-reducing-amazon-package-waste](https://www.amazon.science/latest-news/deep-learning-machine-learning-computer-vision-applications-reducing-amazon-package-waste)ï¼Œè®ºæ–‡ï¼š[https://www.amazon.science/publications/position-paper-reducing-amazons-packaging-wasteusing-multimodal-deep-learning](https://www.amazon.science/publications/position-paper-reducing-amazons-packaging-wasteusing-multimodal-deep-learning)ã€‚
- en: 'Haibo He, Yang Bai, E. A. Garcia, and Shutao Li, *ADASYN: Adaptive synthetic
    sampling approach for imbalanced learning*, in 2008 IEEE International Joint Conference
    on Neural Networks (IEEE World Congress on Computational Intelligence), Hong Kong,
    China: IEEE, Jun. 2008, pp. 1322â€“1328\. doi: 10.1109/IJCNN.2008.4633969.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Haibo He, Yang Bai, E. A. Garcia å’Œ Shutao Liï¼Œ*ADASYNï¼šç”¨äºä¸å¹³è¡¡å­¦ä¹ çš„è‡ªé€‚åº”åˆæˆé‡‡æ ·æ–¹æ³•*ï¼Œè½½äº2008å¹´IEEEå›½é™…ç¥ç»ç½‘ç»œè”åˆä¼šè®®ï¼ˆIEEEè®¡ç®—æ™ºèƒ½ä¸–ç•Œå¤§ä¼šï¼‰ï¼Œä¸­å›½é¦™æ¸¯ï¼šIEEEï¼Œ2008å¹´6æœˆï¼Œç¬¬1322â€“1328é¡µã€‚doi:
    10.1109/IJCNN.2008.4633969ã€‚'
- en: 'Y. Elor and H. Averbuch-Elor, *To SMOTE, or not to SMOTE?*, arXiv, May 11,
    2022\. Accessed: Feb. 19, 2023\. [Online]. Available at [http://arxiv.org/abs/2201.08528](http://arxiv.org/abs/2201.08528).'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Y. Elor å’Œ H. Averbuch-Elor, *SMOTE æ˜¯å¦å¿…è¦ï¼Ÿ*ï¼ŒarXivï¼Œ2022å¹´5æœˆ11æ—¥ã€‚è®¿é—®æ—¶é—´ï¼š2023å¹´2æœˆ19æ—¥ã€‚[åœ¨çº¿]ã€‚å¯åœ¨[http://arxiv.org/abs/2201.08528](http://arxiv.org/abs/2201.08528)è·å–ã€‚
