- en: Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: After we reviewed the recent developments in deep learning, we are now reaching
    the cutting-edge of machine learning, and we are now adding a very special dimension
    to our model (time, and hence sequences of inputs) through a recent series of
    algorithms called **recurrent neural networks (RNNs)**.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们回顾了深度学习的最新发展之后，我们现在已经达到了机器学习的尖端，我们现在通过一系列称为 **循环神经网络（RNNs）** 的新算法，给我们的模型增加了一个非常特殊的维度（时间，因此是输入序列）。
- en: Solving problems with order — RNNs
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决有序问题——RNNs
- en: 'In the previous chapters, we have examined a number of models, from simple
    ones to more sophisticated ones, with some common properties:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们考察了许多模型，从简单的到更复杂的，它们有一些共同的特性：
- en: They accept unique and isolated input
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们接受独特且独立的输入
- en: They have unique and fixed size output
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们有独特且固定大小的输出
- en: The outputs will depend exclusively on the current input characteristics, without
    dependency on past or previous input
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出将完全取决于当前输入的特征，而不依赖于过去或之前的输入
- en: In real life, the pieces of information that the brain processes have an inherent
    structure and order, and the organization and sequence of every phenomenon we
    perceive has an influence on how we treat them. Examples of this include speech
    comprehension (the order of the words in a sentence), video sequence (the order
    of the frames in a video), and language translation. This prompted the creation
    of new models. The most important ones are grouped under the RNN umbrella.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，大脑处理的信息具有固有的结构和顺序，我们感知到的每一个现象的组织和序列都会影响我们如何处理它们。这包括语音理解（句子中单词的顺序）、视频序列（视频中的帧顺序）和语言翻译。这促使新的模型被创造出来。其中最重要的模型被归类在
    RNN 的范畴下。
- en: RNN definition
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 定义
- en: 'RNNs are **Artificial Neural Network** (**ANN**) models whose inputs and outputs
    are sequences. A more formal definition can be expressed as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 是 **人工神经网络**（**ANN**）模型，其输入和输出都是序列。一个更正式的定义可以表达如下：
- en: '"An RNN represents a sequence with a high-dimensional vector (called the hidden
    state) of a fixed dimensionality that incorporates new observations using a complex
    non-linear function."'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: “一个 RNN 通过一个固定维度的多维向量（称为隐藏状态）来表示一个序列，该向量使用复杂的非线性函数结合新的观察结果。”
- en: RNNs are highly expressive and can implement an arbitrary memory-bounded computation,
    and as a result, they can be configured to achieve non-trivial performance on
    difficult sequence tasks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 具有高度的表达能力，可以实现任意内存限制的计算，因此，它们可以被配置以在困难的序列任务上实现非平凡的性能。
- en: Types of sequence to be modeled
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要建模的序列类型
- en: 'RNNs work with sequences models, both in the input and the output realm. Based
    on this, we can have all the possible combinations to solve different kinds of
    problems. In the following diagram, we illustrate the main architectures used
    in this field, and then a reference of the recursive ones:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 在输入和输出领域都使用序列模型。基于这一点，我们可以有所有可能的组合来解决不同类型的问题。在下面的图中，我们展示了该领域使用的主要架构，然后是递归架构的参考：
- en: '![](img/21c36793-0b24-4644-be7f-f1d24452af93.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/21c36793-0b24-4644-be7f-f1d24452af93.png)'
- en: Type of sequences modes
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 序列模式类型
- en: Development of RNN
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 的发展
- en: The origin of RNNs is surprisingly common to the other modern neural network
    architectures, dating back to Hopfield networks from the 1980s, but with counterparts
    in the 1970s.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 的起源与其他现代神经网络架构惊人地相似，可以追溯到 20 世纪 80 年代的 Hopfield 网络，但在 20 世纪 70 年代就有对应的网络。
- en: 'The common structure for the first iterations of the recurrent architecture
    can be represented in the following way:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 循环架构第一次迭代的常见结构可以表示如下：
- en: '![](img/f28d6f32-e9b2-43f0-85d9-cbb5d2e8fa56.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f28d6f32-e9b2-43f0-85d9-cbb5d2e8fa56.png)'
- en: Recurrent cell unrolling
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 循环单元展开
- en: Classic RNN nodes have recurrent connections to themselves, and so they can
    evolve their weights as the input sequence progresses. Additionally, on the right
    of the diagram, you can see how the network can be *unrolled* to generate a set
    of outputs based on the stochastic model it has saved internally. It stores representations
    of recent input events in the form of activation (**short-term memory**, as opposed
    to **long-term memory**, embodied by slowly changing weights). This is potentially
    significant for many applications, including speech processing, music composition
    (such as *Mozer, 1992*), **Natural Language Processing** (**NLP**), and many other
    fields.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的RNN节点具有与自身的循环连接，因此它们可以随着输入序列的进展而调整它们的权重。此外，在图右边的部分，你可以看到网络如何被*展开*以生成基于它内部保存的随机模型的一组输出。它以激活的形式存储最近输入事件的表示（**短期记忆**，与**长期记忆**相对，由缓慢变化的权重体现）。这对许多应用都具有重要意义，包括语音处理、音乐创作（如*Mozer,
    1992*）、**自然语言处理**（**NLP**）以及许多其他领域。
- en: Training method — backpropagation through time
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练方法——时间反向传播
- en: After the considerable number of model types we've been studying, it's possible
    that you can already see a pattern in the implementation of the training steps.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们研究过的大量模型类型之后，你可能已经能够在训练步骤的实施中看到一种模式。
- en: For recurrent neural networks, the most well-known error minimization technique
    is a variation of the well-known (for us) backpropagation methods, with a simple
    element – **backpropagation through time** (**BPTT**) works by unrolling all input
    timesteps. Each timestep has one input timestep, one copy of the whole network,
    and one output. Errors are calculated and accumulated for each timestep, and finally
    the network is rolled back up and the weights are updated.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于循环神经网络来说，最著名的误差最小化技术是众所周知的（对我们来说）反向传播方法的变体，其中有一个简单的元素——**时间反向传播**（**BPTT**）通过展开所有输入时间步来实现。每个时间步有一个输入时间步，整个网络的一个副本和一个输出。每个时间步计算并累积误差，最后网络被卷起，权重被更新。
- en: Spatially, each timestep of the unrolled recurrent neural network can be seen
    as an additional layer, given the dependence from one timestep to another and
    how every timestep output is taken as an input for the subsequent timestep. This
    can lead to really complex training performance requirements, and thus, **truncated
    backpropagation through time** was born.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在空间上，展开的循环神经网络中的每个时间步可以看作是一个额外的层，考虑到从一个时间步到另一个时间步的依赖性以及每个时间步的输出如何作为后续时间步的输入。这可能导致非常复杂的训练性能要求，因此，**时间反向传播**应运而生。
- en: 'The following pseudocode represents the whole process:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下伪代码表示整个过程：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Main problems of the traditional RNNs — exploding and vanishing gradients
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统RNNs的主要问题——梯度爆炸和消失
- en: However, RNNs have turned out to be difficult to train, especially on problems
    with complicated long-range temporal structures – precisely the setting where
    RNNs ought to be most useful. Since their potential has not been realized, methods
    that address the difficulty of training RNNs are of great importance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，循环神经网络（RNNs）的训练证明是困难的，尤其是在具有复杂长距离时间结构的难题上——这正是RNNs应该最有用的设置。由于它们的潜力尚未得到实现，解决RNNs训练困难的方法非常重要。
- en: The most widely used algorithms for learning what to put in short-term memory,
    however, take too much time or do not work well at all, especially when minimal
    time lags between inputs and corresponding teacher signals are long. Although
    theoretically fascinating, the existing methods did not provide clear practical
    advantages over traditional feedforward networks.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，用于学习短期记忆内容的最广泛使用的算法，却花费了太多时间或者根本不起作用，尤其是在输入和相应的教师信号之间的最小时间延迟很长的情况下。尽管从理论上来说非常吸引人，但现有方法并没有在传统前馈网络之上提供明显的实际优势。
- en: One of the main problems with RNNs happens in the backpropagation stage. Given
    its recurrent nature, the number of steps that the backpropagation of the errors
    has corresponds to a very deep network. This cascade of gradient calculations
    could lead to a very insignificant value in the last stages, or on the contrary,
    to ever-increasing and unbounded parameters. Those phenomena receive the names
    of vanishing and exploding gradients. This is one of the reasons for which LSTM
    was created.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 中的一个主要问题出现在反向传播阶段。由于其循环性质，错误反向传播的步数对应一个非常深的网络。这种梯度计算的级联可能导致最后阶段非常微小的值，或者相反，参数不断增长且无界。这些现象被称为梯度消失和梯度爆炸。这也是
    LSTM 被创造出来的原因之一。
- en: The problem with conventional BPTT is that error signals going backwards in
    time tend to either blow up or vanish – the temporal evolution of the backpropagated
    error exponentially depends on the size of the weights. It could lead to oscillating
    weights, or taking a prohibitive amount of time, or it could not work at all.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 传统 BPTT 的问题在于，随时间向后传播的错误信号往往会爆炸或消失——反向传播错误的时序演化指数依赖于权重的尺寸。它可能导致权重振荡，或者花费过多时间，或者根本不起作用。
- en: As a consequence of many different attempts to solve the problem with vanishing
    and exploding gradients, finally in 1997, *Schmidhuber* and *Sepp* published a
    fundamental paper on RNNs, and LSTM, which paved the way for all modern developments
    in the area.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作为解决梯度消失和梯度爆炸问题的一系列不同尝试的结果，最终在 1997 年，*Schmidhuber* 和 *Sepp* 发表了一篇关于 RNNs 和
    LSTM 的基础论文，为该领域所有现代发展铺平了道路。
- en: LSTM
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM
- en: '**LSTMs** are a fundamental step in RNNs, because they introduce long-term
    dependencies into the cells. The unrolled cells contain two different parameter
    lines: one long-term status, and the other representing short-term memory.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**LSTMs** 是 RNNs 的一个基本步骤，因为它们将长期依赖引入到细胞中。展开的细胞包含两条不同的参数线：一条代表长期状态，另一条代表短期记忆。'
- en: Between steps, the long-term forgets less important information, and adds filtered
    information from short-term events, incorporating them into the future.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤之间，长期遗忘不那么重要的信息，并添加来自短期事件的过滤信息，将它们纳入未来。
- en: LSTMs are really versatile in their possible applications, and they are the
    most commonly employed recurrent models, along with GRUs, which we will explain
    later. Let's try to break down an LSTM into its components to get a better understanding
    of how they work.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: LSTMs 在其可能的用途上非常灵活，它们是最常用的循环模型之一，与稍后我们将解释的 GRUs 一起。让我们尝试将 LSTM 分解为其组成部分，以更好地理解它们是如何工作的。
- en: The gate and multiplier operation
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 门和乘法操作
- en: 'LSTMs have two fundamental values: remembering important things from the present,
    and slowly forgetting unimportant things from the past. What kind of mechanism
    can we use to apply this kind of filtering? It''s called the **gate** operation.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LSTMs 有两个基本值：记住现在的重要事情，并缓慢忘记过去的不重要事情。我们可以使用什么机制来应用这种过滤？这被称为 **门** 操作。
- en: The gate operation basically has a multivariate vector input and a filter vector,
    which will be dot multiplied with the input, allowing or rejecting the elements
    from the inputs to be transferred. How do we adjust this gate's filters? This multivariate
    control vector (marked with an arrow on the diagram) is connected with a neural
    network layer with a sigmoid activation function. If we apply the control vector
    and pass through the sigmoid function, we will get a binary-like output vector.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 门操作基本上有一个多元向量输入和一个过滤器向量，该向量将与输入进行点乘，允许或拒绝从输入中传输的元素。我们如何调整这个门的过滤器？这个多元控制向量（在图中用箭头标记）与一个具有
    sigmoid 激活函数的神经网络层相连。如果我们应用控制向量并通过 sigmoid 函数，我们将得到一个类似二进制的输出向量。
- en: 'In the following diagram, the gate will be represented by a series of switches:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，门将由一系列开关表示：
- en: '![](img/d40d6035-e0b8-420a-80ed-48b8095d5d64.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d40d6035-e0b8-420a-80ed-48b8095d5d64.png)'
- en: LSTM gate
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 门
- en: 'Another important part of the process is the multiplications, which formalize
    the trained filters, effectively multiplying the inputs by an included gate. The
    arrow icon indicates the direction in which the filtered information will flow:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 过程中的另一个重要部分是乘法，它将训练好的过滤器形式化，有效地通过一个包含的门来乘以输入。箭头图标表示过滤信息将流动的方向：
- en: '![](img/31203714-478a-44ff-9488-df48560da1d1.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/31203714-478a-44ff-9488-df48560da1d1.png)'
- en: Gate multiplication step
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 门乘法步骤
- en: Now, it's time to describe an LSTM cell in more detail.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候更详细地描述一个LSTM细胞了。
- en: 'An LSTM has three gates to protect and control the cell state: one at the start
    of the data flow, another in the middle, and the last at the end of the cell''s
    informational boundaries. This operation will allow both discarding (hopefully
    not important) low-important state data and incorporating (hopefully important)
    new data to the state.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM有三个门来保护和控制细胞状态：一个在数据流的开始，另一个在中间，最后一个在细胞信息边界的末端。这个操作将允许丢弃（希望不是重要的）低重要性的状态数据，并将（希望重要的）新数据纳入状态。
- en: 'The following diagram shows all the concepts in the operation of one LSTM cell.
    As inputs, we have the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了单个LSTM细胞操作中的所有概念。作为输入，我们有以下内容：
- en: The cell state, which will store long-term information, because it carries the
    optimized weights dating from the origin of the cell training
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 细胞状态，它将存储长期信息，因为它携带从细胞训练起源的优化权重
- en: 'The short-term state, *h(t)*, which will be directly combined with the current
    input on each iteration, and so it will have a much bigger influence on the latest
    values of the inputs:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短期状态，*h(t)*，它将在每次迭代中直接与当前输入结合，因此它将对输入的最新值有更大的影响：
- en: '![](img/4874cf1c-76ae-4cb1-95e9-3af19d8f6a7e.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4874cf1c-76ae-4cb1-95e9-3af19d8f6a7e.png)'
- en: Depiction of an LSTM cell, with all its components
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM细胞的描绘，包括所有其组成部分
- en: Now, let's explore the data flow on this LSTM unit in order to better understand
    how the different gates and operations work together in a single cell.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索这个LSTM单元上的数据流，以便更好地理解不同的门和操作如何在单个细胞中协同工作。
- en: Part 1 — set values to forget (input gate)
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一部分 — 设置要遗忘的值（输入门）
- en: 'In this stage, we take the values coming from the short-term memory combined
    with the input itself, and these values will then output a binary function, represented
    by a multivariable sigmoid. Depending on the input and short-term memory values,
    the sigmoid output will filter the long-term knowledge, represented by the weights
    of the cell''s state:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们将来自短期记忆的值与输入本身结合，然后这些值将输出一个二进制函数，由一个多变量Sigmoid表示。根据输入和短期记忆的值，Sigmoid输出将过滤长期知识，由细胞状态的权重表示：
- en: '![](img/529af80b-efe4-4d92-9612-27619fdd7734.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/529af80b-efe4-4d92-9612-27619fdd7734.png)'
- en: State forget parameters setup
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 状态遗忘参数设置
- en: Part 2 — set values to keep
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分 — 设置要保留的值
- en: Now, it's time to set the filter that will allow or reject the incorporation
    of new and short-term memory to the cell's semi-permanent state. Then it is time
    to set the filter that will allow or reject the incorporation of new and short-term
    memory to the cell's semi-permanent state.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候设置过滤器，允许或拒绝将新的和短期记忆纳入细胞的半永久状态。然后是设置过滤器，允许或拒绝将新的和短期记忆纳入细胞的半永久状态。
- en: 'So, at this stage, we will determine how much of the new and semi-new information
    will be incorporated in the cell''s new state:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这个阶段，我们将确定多少新的和半新的信息将被纳入细胞的新状态：
- en: '![](img/71b9d57f-6861-46bf-9ce0-1a318b602600.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/71b9d57f-6861-46bf-9ce0-1a318b602600.png)'
- en: Short-term values selection step
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 短期值选择步骤
- en: Part 3 — apply changes to cell
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分 — 应用到细胞的变化
- en: In this part of the sequence, we will finally pass through the information filter
    we have been configuring, and as a result, we will have an updated long-term state.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个序列的这一部分，我们最终将通过我们一直在配置的信息过滤器，结果我们将有一个更新的长期状态。
- en: 'In order to normalize the new and short-term information, we pass the new input
    and the short-term state via a neural network with **tanh** activation. This will
    allow us to feed the new information in a normalized *[-1,1]* range:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使新的和短期信息归一化，我们通过具有**tanh**激活的神经网络传递新的输入和短期状态。这将使我们能够以归一化的*[-1,1]*范围提供新的信息：
- en: '![](img/0b3418fd-359f-4ac4-b1bc-548766fb5c97.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0b3418fd-359f-4ac4-b1bc-548766fb5c97.png)'
- en: State changes persistence step
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 状态变化持久步骤
- en: Part 4 — output filtered cell state
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四部分 — 输出过滤后的细胞状态
- en: 'Now, it''s the turn of the short-term state. It will also use the new and previous
    short-term state to set up the filter that will pass the long-term state, dot
    multiplied by a tanh function, again to normalize the information to incorporate
    a *(-1,1)* range:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，轮到短期状态了。它也将使用新的和之前的短期状态来设置过滤长期状态的过滤器，再次通过tanh函数点乘，以将信息归一化到*(-1,1)*范围：
- en: '![](img/4a048a2f-5c05-433d-8584-de049c86d8de.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4a048a2f-5c05-433d-8584-de049c86d8de.png)'
- en: New short-term generation step
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 新短期生成步骤
- en: Univariate time series prediction with energy consumption data
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用能源消耗数据进行单变量时间序列预测
- en: In this example, we will be solving a problem in the domain of regression. For
    this reason, we will build a multi-layer RNN with two LSTMs. The type of regression
    we will do is of the *many to one* type, because the network will receive a sequence
    of energy consumption values and will try to output the next value based on the
    previous four registers.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将解决回归领域的问题。因此，我们将构建一个包含两个LSTMs的多层RNN。我们将进行的回归类型是*多对一*类型，因为网络将接收一系列能源消耗值，并尝试根据前四个记录输出下一个值。
- en: The dataset we will be working on is a compendium of many measurements of the
    power consumption of one home over a period of time. As we might infer, this kind
    of behavior can easily follow patterns (it increases when the occupants use the
    microwave to prepare breakfast and use computers during the day, it decreases
    a bit in the afternoon, and then increases in the evening with all the lights,
    finally decreasing to zero when the occupants are asleep).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要工作的数据集是关于一个家庭在一段时间内电力消耗的许多测量的汇编。正如我们可能推断的那样，这种行为很容易遵循模式（当居民使用微波炉准备早餐并在白天使用电脑时，它会增加，下午稍微减少，然后在晚上随着所有灯光的增加而增加，最后当居民入睡时减少到零）。
- en: 'Let''s start by setting the appropriate environment variables and loading the
    required libraries:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先设置适当的环境变量并加载所需的库：
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Dataset description and loading
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集描述和加载
- en: 'In this example, we will be using the **Electricity Load Diagrams Data Sets**,
    from *Artur Trindade* ([https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014)).
    This is the description of the original dataset:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用**电力负荷图数据集**，来自*Artur Trindade* ([https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014))。这是原始数据集的描述：
- en: '"Data set has no missing values.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '"数据集没有缺失值。'
- en: Values are in kW of each 15 min. To convert values in kWh values must be divided
    by 4.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数值以每15分钟的kW为单位。要将数值转换为kWh，必须将数值除以4。
- en: Each column represent one client. Some clients were created after 2011\. In
    these cases consumption were considered zero.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 每一列代表一个客户。有些客户是在2011年之后创建的。在这些情况下，消费被视为零。
- en: All time labels report to Portuguese hour. However all days present 96 measures
    (24*15). Every year in March time change day (which has only 23 hours) the values
    between 1:00 am and 2:00 am are zero for all points. Every year in October time
    change day (which has 25 hours) the values between 1:00 am and 2:00 am aggregate
    the consumption of two hours."
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所有时间标签都报告葡萄牙小时。然而，所有日子都有96个测量值（24*15）。每年3月时间变化日（只有23小时）凌晨1:00至2:00之间的值对所有点都是零。每年10月时间变化日（有25小时）凌晨1:00至2:00之间的值汇总了两个小时的消耗。"
- en: In order to simplify our model description, we took just one client's complete
    measurements and converted its format to standard CSV. It is located in the `data`
    subfolder of this chapter's code folder.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化我们的模型描述，我们只取了一个客户的完整测量数据，并将其格式转换为标准CSV。它位于本章代码文件夹的`data`子文件夹中。
- en: 'So, we will load the first 1,500 values of the consumption of a sample home
    from the dataset:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将从数据集中加载一个样本家庭消费的前1500个值：
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following graph shows the subset of data that we need to model:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了我们需要建模的数据子集：
- en: '![](img/70e61ce5-14e6-4465-a264-08bd834e1a3a.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70e61ce5-14e6-4465-a264-08bd834e1a3a.png)'
- en: If we take a look at this representation (we took the first 1,500 samples) we
    can see an initial transient state, probably when the measurements were put in
    place, and then we see a really clear cycle of high and low consumption levels.
    From simple observation, we can also see that the cycles are of more or less 100
    samples, pretty close to the 96 samples per day this dataset has.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看一下这个表示（我们取了前1500个样本），我们可以看到一个初始瞬态状态，可能是当测量被放置时，然后我们看到一个非常清晰的消费水平高低的周期。从简单的观察中，我们还可以看到周期大约是100个样本，非常接近这个数据集每天96个样本。
- en: Dataset preprocessing
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集预处理
- en: 'In order to assure a better convergence of the backpropagation methods, we
    should try to normalize the input data. So, we will be applying the classic scale
    and centering technique, subtracting the mean value, and scaling by the `floor()`
    of the maximum value. To get the required values, we use the pandas `describe()`
    method:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保反向传播方法的更好收敛，我们应该尝试对输入数据进行归一化。因此，我们将应用经典的缩放和居中技术，减去平均值，并按最大值的`floor()`进行缩放。为了获取所需值，我们使用pandas的`describe()`方法：
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This is the graph of our normalized data:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的归一化数据的图表：
- en: '![](img/cbb950bc-0fcc-49ad-aaa6-b31334d715b9.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cbb950bc-0fcc-49ad-aaa6-b31334d715b9.png)'
- en: 'In this step, we will prepare our input dataset, because we need an input `x` (the
    previous 5 values) with a corresponding input `y` (the value after 5 timesteps).
    Then, we will assign the first 13,000 elements to the training set, and then we
    will assign the following 1,000 samples to the testing set:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们将准备我们的输入数据集，因为我们需要一个输入`x`（前5个值）以及相应的输入`y`（5个时间步后的值）。然后，我们将前13,000个元素分配给训练集，然后我们将接下来的1,000个样本分配给测试集：
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we will build the model, which will be a dual LSTM with a dropout layer
    at the end of each:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将构建模型，它将是一个具有每个末尾都有dropout层的双LSTM：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now it''s time to run the model and adjust the weights. The model fitter will
    use 8% of the dataset values as the validation set:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是运行模型并调整权重的时候了。模型适配器将使用数据集的8%作为验证集：
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After rescaling, it''s time to see how our model predicts the values compared
    with the actual test values, which didn''t participate in the training of the
    models, to understand how the model generalizes the behavior of the sample home:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新缩放后，是时候看看我们的模型如何预测值与实际测试值相比，这些值没有参与模型的训练，以了解模型如何泛化样本住宅的行为：
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/279595ac-742c-4790-a2de-d131878c6892.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/279595ac-742c-4790-a2de-d131878c6892.png)'
- en: Final regressed data
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最终回归数据
- en: Summary
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, our scope has expanded even more, adding the important dimension
    of time to the set of elements to be included in our generalization. Also, we
    learned how to solve a practical problem with RNNs, based on real data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们的范围进一步扩大，将时间维度添加到我们想要包含在泛化中的元素集合中。此外，我们还学习了如何基于真实数据使用RNN解决实际问题。
- en: But if you think you have covered all the possible options, there are many more
    model types to see!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你认为你已经涵盖了所有可能的选项，还有许多更多的模型类型可以查看！
- en: In the next chapter, we will talk about cutting edge architectures that can
    be trained to produce very clever elements, for example, transfer the style of
    famous painters to a picture, and even play video games! Keep reading for reinforcement
    learning and generative adversarial networks.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论可以训练以产生非常聪明元素的尖端架构，例如，将著名画家的风格转移到图片上，甚至玩电子游戏！继续阅读以了解强化学习和生成对抗网络。
- en: References
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Hopfield, John J, *Neural networks and physical systems with emergent collective
    computational abilities.* Proceedings of the national academy of sciences 79.8
    (1982): 2554-2558.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hopfield, John J，*具有涌现集体计算能力的神经网络和物理系统*。美国国家科学院院刊79.8（1982）：2554-2558。
- en: 'Bengio, Yoshua, Patrice Simard, and Paolo Frasconi, *Learning long-term dependencies
    with gradient descent is difficult.* IEEE transactions on neural networks 5.2
    (1994): 157-166.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio, Yoshua，Patrice Simard和Paolo Frasconi，*使用梯度下降学习长期依赖性是困难的*。IEEE神经网络杂志5.2（1994）：157-166。
- en: 'Hochreiter, Sepp, and Jürgen Schmidhuber, *long short-term memory*. Neural
    Computation 9.8 (1997): 1735-1780.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter, Sepp和Jürgen Schmidhuber，*长短期记忆*。神经计算9.8（1997）：1735-1780。
- en: 'Hochreiter, Sepp. *Recurrent neural net learning and vanishing gradient.* International
    Journal Of Uncertainity, Fuzziness and Knowledge-Based Systems 6.2 (1998): 107-116.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter, Sepp。*循环神经网络学习和梯度消失*。国际不确定性和模糊知识系统杂志6.2（1998）：107-116。
- en: Sutskever, Ilya, *Training recurrent neural networks.* University of Toronto,
    Toronto, Ont., Canada (2013).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever, Ilya，*训练循环神经网络*。多伦多大学，多伦多，安大略省，加拿大（2013）。
- en: Chung, Junyoung, et al, *Empirical evaluation of gated recurrent neural networks
    on sequence modeling.* arXiv preprint arXiv:1412.3555 (2014).
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung, Junyoung, 等人，*门控循环神经网络在序列建模中的实证评估*。arXiv预印本arXiv:1412.3555（2014）。
