- en: Reinforcement Learning Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习技术
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下食谱：
- en: Weather forecasting with MDP
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MDP进行天气预报
- en: Optimizing a financial portfolio using DP
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DP优化金融投资组合
- en: Finding the shortest path
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找最短路径
- en: Deciding the discount factor using Q-learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Q学习决定折现因子
- en: Implementing a deep Q-learning algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现深度Q学习算法
- en: Developing an AI-based dynamic modeling system
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发基于AI的动态建模系统
- en: Deep reinforcement learning with Double Q-learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于Double Q学习的深度强化学习
- en: Deep Q-Network algorithm with dueling Q-learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有对抗性Q学习的深度Q网络算法
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To address the recipes in this chapter, you will need the following files (available
    on GitHub):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理本章中的食谱，你需要以下文件（可在GitHub上找到）：
- en: '`MarkovChain.py`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MarkovChain.py`'
- en: '`KPDP.py`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KPDP.py`'
- en: '`DijkstraNX.py`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DijkstraNX.py`'
- en: '`FrozenQlearning.py`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FrozenQlearning.py`'
- en: '`FrozenDeepQLearning.py`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FrozenDeepQLearning.py`'
- en: '`dqn_cartpole.py`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dqn_cartpole.py`'
- en: '`DoubleDQNCartpole.py`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DoubleDQNCartpole.py`'
- en: '`DuelingDQNCartpole.py`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DuelingDQNCartpole.py`'
- en: Introduction
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Reinforcement learning represents a family of algorithms that are able to learn
    and adapt to environmental changes. It is based on the concept of receiving external
    stimuli based on the choices of the algorithm. A correct choice will result in
    a reward, while a wrong choice will lead to a penalty. The goal of the system
    is to achieve the best possible result.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习代表了一类能够学习和适应环境变化的算法。它基于算法选择的外部刺激的概念。正确的选择将导致奖励，而错误的选择将导致惩罚。系统的目标是实现最佳结果。
- en: In supervised learning, the correct output is clearly specified (learning with
    a teacher). But it is not always possible to do so. Often, we only have qualitative
    information. The information that's available is called a **reinforcement signal**.
    In these cases, the system does not provide any information on how to update the
    agent's behavior (for example, weights). You cannot define a cost function or
    a gradient. The goal of the system is to create the smart agents that are able
    to learn from their experience.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，正确的输出是明确指定的（有教师指导的学习）。但并非总是可能这样做。通常，我们只有定性信息。可用的信息被称为**强化信号**。在这些情况下，系统不会提供有关如何更新智能体行为的任何信息（例如，权重）。无法定义成本函数或梯度。系统的目标是创建能够从经验中学习的智能体。
- en: 'In the following screenshot, we can see a flowchart that displays the reinforcement
    learning interaction with the environment:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的屏幕截图中，我们可以看到一个流程图，显示了强化学习与环境之间的交互：
- en: '![](img/e7ba0357-9922-4ceb-8752-3787f92d9767.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7ba0357-9922-4ceb-8752-3787f92d9767.png)'
- en: 'Here are the steps to follow to correctly apply a reinforcement learning algorithm:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是正确应用强化学习算法的步骤：
- en: Preparation of the agent
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 智能体的准备
- en: Observation of the environment
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 环境观察
- en: Selection of the optimal strategy
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择最优策略
- en: Execution of actions
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作
- en: Calculation of the corresponding reward (or penalty)
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算相应的奖励（或惩罚）
- en: Development of updating strategies (if necessary)
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新策略的开发（如有必要）
- en: Repetition of steps 2 to 5 iteratively until the agent learns the optimal strategies
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2到5，直到智能体学习到最优策略
- en: Reinforcement learning tries to maximize the rewards that are received for the
    execution of the action or set of actions that allow a goal to be achieved.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习试图最大化执行动作或动作集以实现目标所获得的奖励。
- en: Weather forecasting with MDP
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MDP进行天气预报
- en: To avoid load problems and computational difficulties, the agent-environment
    interaction is considered a **Markov decision process** (**MDP**). An MDP is a
    discrete time stochastic control process.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免负载问题和计算困难，将智能体-环境交互视为一个**马尔可夫决策过程**（**MDP**）。MDP是一个离散时间随机控制过程。
- en: '**Stochastic processes** are mathematical models that are used to study the
    evolution of phenomena following random or probabilistic laws. It is known that
    in all natural phenomena, both by their very nature and by observational errors,
    a random or accidental component is present.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机过程**是用于研究遵循随机或概率定律的现象演化的数学模型。众所周知，在所有自然现象中，无论是由于其本质还是由于观测误差，都存在一个随机或偶然的成分。'
- en: 'This component causes the following: at every instance of *t*, the result of
    the observation of the phenomenon is a random number or random variable, *st*.
    It is not possible to predict with certainty what the result will be; you can
    only state that it will take one of several possible values, each of which has
    a given probability.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个组件导致以下情况：在*t*的每一个实例中，对现象的观察结果是随机数或随机变量，*st*。不可能确定结果会是什么；你只能声明它将取几个可能值中的一个，每个值都有给定的概率。
- en: A stochastic process is called **Markovian** when, having chosen a certain instance
    of *t* for observation, the evolution of the process, starting with *t*, depends
    only on *t* and does not depend in any way on the previous instances. Thus, a
    process is Markovian when, given the moment of observation, only this instance
    determines the future evolution of the process, while this evolution does not
    depend on the past.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当观察到一个特定的*t*实例时，如果随机过程的演变，从*t*开始，只依赖于*t*而不依赖于任何先前的实例，则称该随机过程为**马尔可夫**。因此，当给定观察时刻时，只有这个实例决定了过程的未来演变，而这种演变不依赖于过去。
- en: Getting ready
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this recipe, we want to build a statistical model to predict the weather.
    To simplify the model, we will assume that there are only two states: sunny and
    rainy. Let''s further assume that we have made some calculations and discovered
    that tomorrow''s time is somehow based on today''s time.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们想要构建一个统计模型来预测天气。为了简化模型，我们假设只有两种状态：晴天和雨天。让我们进一步假设我们已经进行了一些计算，并发现明天的时刻某种程度上基于今天的时间。
- en: How to do it…
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s see how we can perform weather forecasting with MDP:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用MDP进行天气预报：
- en: 'We will use the `MarkovChain.py` file that is already provided for you as a
    reference. To start, we import the `numpy`, `time`, and `matplotlib.pyplot` packages:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用已经为你提供的`MarkovChain.py`文件作为参考。首先，我们导入`numpy`、`time`和`matplotlib.pyplot`包：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s set the seed of a random number generator and the state of the weather:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们设置随机数生成器的种子和天气状态：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'At this point, we have to define the possible transitions of weather conditions:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，我们必须定义天气条件的可能转移：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we insert the following check to verify that we did not make mistakes
    in defining the transition matrix:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们插入以下检查以验证我们没有在定义转移矩阵时出错：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s set the initial condition:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们设置初始条件：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can now predict the weather conditions for each of the days set by the `NumberDays`
    variable. To do this, we will use a `while` loop, as follows:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以预测`NumberDays`变量设置的每一天的天气条件。为此，我们将使用以下`while`循环：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It consists of a control condition and a loop body. At the entrance of the cycle
    and every time that all the instructions contained in the body are executed, the
    validity of the control condition is verified. The cycle ends when the condition,
    consisting of a Boolean expression, returns `false`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 它由一个控制条件和循环体组成。在循环的入口处以及每次执行循环体内的所有指令后，都要验证控制条件的有效性。当由布尔表达式组成的条件返回`false`时，循环结束。
- en: 'At this point, we have generated forecasts for the next 200 days. Let''s plot
    the chart using the following code:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，我们已经为未来200天生成了预报。让我们使用以下代码绘制图表：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following graph shows the weather conditions for the next 200 days, starting
    from the sunny condition:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图显示了从晴天开始的未来200天的天气条件：
- en: '![](img/08afee2a-205a-413c-b485-7eebe23692ea.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08afee2a-205a-413c-b485-7eebe23692ea.png)'
- en: At first sight, it seems that sunny days prevail over the rainy ones.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，似乎晴天比雨天多。
- en: How it works…
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: A Markov chain is a mathematical model of a random phenomenon that evolves over
    time in such a way that the past influences the future only through the present.
    In other words, a stochastic model describes a sequence of possible events where
    the probability of each event depends only on the state that was attained in the
    previous event. So, Markov chains have the property of memorylessness.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链是随机现象的数学模型，该现象随时间演变，过去只通过现在影响未来。换句话说，随机模型描述了一系列可能的事件，其中每个事件的概率只依赖于前一个事件达到的状态。因此，马尔可夫链具有无记忆性。
- en: 'The structure of a Markov chain is therefore completely represented by the
    following transition matrix:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，马尔可夫链的结构完全由以下转移矩阵表示：
- en: '![](img/2d8d7be0-63a8-4818-ae7b-717417ce5420.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d8d7be0-63a8-4818-ae7b-717417ce5420.png)'
- en: The properties of transition probability matrices derive directly from the nature
    of the elements that compose them.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 转移概率矩阵的性质直接来源于组成它们的元素的本质。
- en: There's more…
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: A very intuitive alternative to the description of a Markov chain through a
    transition matrix is associating an oriented graph (transition diagram) to a Markov
    chain. The transition matrix and transition diagram provide the same information
    regarding the same Markov chain.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 将马尔可夫链通过转移矩阵描述的一个非常直观的替代方法是将其与一个有向图（转移图）关联。转移矩阵和转移图提供了关于同一马尔可夫链的相同信息。
- en: See also
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: '*Keras Reinforcement Learning Projects*, Giuseppe Ciaburro, Packt Publishing'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Keras强化学习项目*，Giuseppe Ciaburro，Packt出版社'
- en: '*INTRODUCTION TO MARKOV MODELS* (from Clemson University): [http://cecas.clemson.edu/~ahoover/ece854/refs/Ramos-Intro-HMM.pdf](http://cecas.clemson.edu/~ahoover/ece854/refs/Ramos-Intro-HMM.pdf)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*马尔可夫模型导论*（来自克莱姆森大学）: [http://cecas.clemson.edu/~ahoover/ece854/refs/Ramos-Intro-HMM.pdf](http://cecas.clemson.edu/~ahoover/ece854/refs/Ramos-Intro-HMM.pdf)'
- en: '*Markov Decision Processes* (from Carnegie Mellon University): [http://egon.cheme.cmu.edu/ewo/docs/SchaeferMDP.pdf](http://egon.cheme.cmu.edu/ewo/docs/SchaeferMDP.pdf)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*马尔可夫决策过程*（来自卡内基梅隆大学）: [http://egon.cheme.cmu.edu/ewo/docs/SchaeferMDP.pdf](http://egon.cheme.cmu.edu/ewo/docs/SchaeferMDP.pdf)'
- en: Optimizing a financial portfolio using DP
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用动态规划优化金融投资组合
- en: The management of financial portfolios is an activity that aims to combine financial
    products in a manner that best represents the investor's needs. This requires
    an overall assessment of various characteristics, such as risk appetite, expected
    returns, and investor consumption, as well as an estimate of future returns and
    risk. **Dynamic programming** (**DP**) represents a set of algorithms that can
    be used to calculate an optimal policy given a perfect model of the environment
    in the form of an MDP. The fundamental idea of DP, as well as reinforcement learning
    in general, is the use of state values and actions to look for good policies.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 金融投资组合的管理是一种旨在以最能代表投资者需求的方式组合金融产品的活动。这需要评估各种特征的整体评估，例如风险偏好、预期回报和投资者消费，以及未来回报和风险的估计。**动态规划**（**DP**）代表了一组算法，可以在形式为MDP的环境完美模型下计算最优策略。DP的基本思想，以及强化学习的一般思想，是使用状态值和动作来寻找好的策略。
- en: Getting ready
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this recipe, we will address the **knapsack problem**: a thief goes into
    a house and wants to steal valuables. They put them in their knapsack, but they
    are limited by the weight. Each object has its own value and weight. They must
    choose the objects that are of value, but that do not have excessive weight. The
    thief must not exceed the weight limit in the knapsack, but, at the same time,
    they must optimize their gain.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将解决**背包问题**：一个小偷进入一栋房子，想要偷走贵重物品。他们将它们放入背包中，但受到重量的限制。每个对象都有自己的价值和重量。他们必须选择有价值但重量不过重的对象。小偷不能超过背包的重量限制，同时，他们必须优化他们的收益。
- en: How to do it…
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'Let''s see how we can optimize a financial portfolio using DP:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何使用动态规划优化金融投资组合：
- en: 'We will use the `KPDP.py` file that is already provided for you as a reference.
    This algorithm starts with the definition of a `KnapSackTable()` function that
    will choose the optimal combination of the objects respecting the two constraints
    imposed by the problem: the total weight of the objects equal to 10, and the maximum
    value of the chosen objects, as shown in the following code:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用已经为你提供的`KPDP.py`文件作为参考。此算法从定义一个`KnapSackTable()`函数开始，该函数将选择满足问题所提出的两个约束条件的对象的最优组合：对象的总重量等于10，以及所选对象的最大值，如下面的代码所示：
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we set an iterative loop on all objects and on all weight values, as
    follows:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们在所有对象和所有重量值上设置一个迭代循环，如下所示：
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we can memorize the result that was obtained, which represents the maximum
    value of the objects that can be carried in the knapsack, as follows:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以记住获得的结果，这代表了可以装入背包的对象的最大价值，如下所示：
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The procedure we''ve followed so far does not indicate which subset provides
    the optimal solution. We must extract this information using a set procedure:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们迄今为止遵循的程序并没有表明哪个子集提供了最优解。我们必须使用一种集合程序来提取这个信息：
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If the current element is the same as the previous one, we will move on to
    the next one, as follows:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果当前元素与上一个元素相同，我们将继续下一个，如下所示：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If it is not the same, then the current object will be included in the knapsack,
    and this item will be printed, as follows:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果它们不相同，那么当前对象将被包含在背包中，并将打印出此项目，如下所示：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, the total included weight is printed, as follows:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，打印出包含的总重量，如下所示：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this way, we have defined the function that allows us to build the table.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就定义了一个函数，允许我们构建表格。
- en: 'Now, we have to define the input variables and pass them to the function, as
    follows:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须定义输入变量并将它们传递给函数，如下所示：
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'At this point, we need to extract the weight and variable values from the objects.
    We put them in a separate array to better understand the steps, as follows:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，我们需要从对象中提取权重和变量值。我们将它们放入一个单独的数组中，以便更好地理解步骤，如下所示：
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, the total weight that can be carried by the knapsack and the number
    of available items is set, as follows:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，设置背包可以携带的总重量和可用物品的数量，如下所示：
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we print out the results:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们打印出结果：
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The DP algorithm allowed us to obtain the optimal solution, saving on computational
    costs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划算法使我们能够获得最优解，从而节省了计算成本。
- en: How it works…
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Consider, for example, the problem of finding the best path that joins two locations.
    The principle of optimality states that each sub path included in it, between
    any intermediate location and the final location, must in turn be optimal. Based
    on this principle, DP solves a problem by taking one decision at a time. At every
    step, the best policy for the future is determined, regardless of the past choices
    (it is a Markov process), assuming that the latter choices are also optimal.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑找到连接两个位置的最佳路径的问题。最优性原理指出，它包含的每个子路径，在任意中间位置和最终位置之间，必须依次是最优的。基于这个原理，动态规划通过一次做出一个决策来解决问题。在每一步，都会确定未来的最佳策略，而不考虑过去的决策（它是一个马尔可夫过程），假设后者也是最优的。
- en: There's more…
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'DP is a technique for solving recursive problems more efficiently. Why is this
    the case? Oftentimes, in recursive procedures, we solve sub problems repeatedly.
    In DP, this does not happen: we memorize the solution of these sub problems so
    that we do not have to solve them again. This is called **memoization**. If the
    value of a variable at a given step depends on the results of previous calculations,
    and if the same calculations are repeated over and over, then it is convenient
    to store the intermediate results so as to avoid repeating computationally expensive
    calculations.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划是一种更有效地解决递归问题的技术。为什么是这样呢？在递归过程中，我们通常会反复解决子问题。在动态规划中，这种情况不会发生：我们记住这些子问题的解决方案，这样我们就不必再次解决它们。这被称为**备忘录化**。如果变量的值在给定步骤上依赖于先前计算的结果，并且如果相同的计算反复进行，那么存储中间结果以避免重复计算昂贵的计算是有利的。
- en: See also
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: Refer to *Keras Reinforcement Learning Projects*, Giuseppe Ciaburro, Packt Publishing
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考Giuseppe Ciaburro所著的《Keras强化学习项目》，Packt出版社
- en: 'Refer to *Dynamic Programming* (from Stanford University): [https://web.stanford.edu/class/cs97si/04-dynamic-programming.pdf](https://web.stanford.edu/class/cs97si/04-dynamic-programming.pdf)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考斯坦福大学的*动态规划*：[https://web.stanford.edu/class/cs97si/04-dynamic-programming.pdf](https://web.stanford.edu/class/cs97si/04-dynamic-programming.pdf)
- en: 'Refer to *The Knapsack Problem* (from Eindhoven University): [http://www.es.ele.tue.nl/education/5MC10/Solutions/knapsack.pdf](http://www.es.ele.tue.nl/education/5MC10/Solutions/knapsack.pdf)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考蒂尔堡大学的*背包问题*：[http://www.es.ele.tue.nl/education/5MC10/Solutions/knapsack.pdf](http://www.es.ele.tue.nl/education/5MC10/Solutions/knapsack.pdf)
- en: 'Refer to *Memoization* (from Radford University): [https://www.radford.edu/~nokie/classes/360/dp-memoized.html](https://www.radford.edu/~nokie/classes/360/dp-memoized.html)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考拉德福德大学的*备忘录化*：[https://www.radford.edu/~nokie/classes/360/dp-memoized.html](https://www.radford.edu/~nokie/classes/360/dp-memoized.html)
- en: Finding the shortest path
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找最短路径
- en: Given a weighted graph and a designated vertex *X*, we will often need to find
    the path from *X* to each of the other vertices in the graph. Identifying a path
    connecting two or more nodes of a graph appears as a sub problem of many other
    problems of discrete optimization and has, in addition, numerous applications
    in the real world.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个加权图和一个指定的顶点 *X*，我们通常会需要找到从 *X* 到图中每个其他顶点的路径。识别连接图中的两个或多个节点的路径在许多其他离散优化问题中表现为子问题，并且在现实世界中也有许多应用。
- en: Getting ready
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will find the shortest path between two points using the
    **Dijkstra** algorithm. We will also use the `networkx` package to represent graphs
    in Python.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将使用**Dijkstra**算法找到两点之间的最短路径。我们还将使用`networkx`包在Python中表示图。
- en: How to do it…
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'Let''s see how we can find the shortest path:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何找到最短路径：
- en: 'We will use the `DijkstraNX.py` file that is already provided for you as a
    reference. First, we import the libraries we will use here:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用已经提供的`DijkstraNX.py`文件作为参考。首先，我们导入我们将在这里使用的库：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, a graph object is created and the vertices are added:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建了一个图对象并添加了顶点：
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Subsequently, the weighted edges are added:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随后，添加了加权边：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'At this point, we have drawn the graph by adding labels to the edges with the
    indication of weight:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，我们已经通过给边添加带有权重指示的标签来绘制了图：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To do this, the `draw_networkx_edge_labels` function was used. The following
    diagram shows the results of this:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，使用了`draw_networkx_edge_labels`函数。以下图表显示了这一结果：
- en: '![](img/afea934e-5e66-439a-b69e-b2c5676c5619.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/afea934e-5e66-439a-b69e-b2c5676c5619.png)'
- en: 'Finally, the shortest path from one to four nodes has been calculated:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，计算从节点一到四的最短路径：
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `shortest_path` function computes the shortest paths and path lengths between
    nodes in the graph. The following are the results:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`shortest_path`函数计算图中节点之间的最短路径和路径长度。以下结果是：'
- en: '[PRE23]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, the length of the shortest paths has been calculated:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，计算了最短路径的长度：
- en: '[PRE24]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following is the result:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果是：
- en: '[PRE25]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As we can verify, we have obtained the same result.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所验证的，我们已经获得了相同的结果。
- en: How it works…
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The Dijkstra algorithm is able to solve the problem of finding the shortest
    path from the source, *s,* to all of the nodes. The algorithm maintains a label
    *d(i)* to the nodes representing an upper bound on the length of the shortest
    path of the node i.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Dijkstra算法能够解决从源点`s`到所有节点的最短路径问题。算法维护一个标签*d(i)*到节点，表示节点i的最短路径长度的上界。
- en: 'At each step, the algorithm partitions the nodes in *V* into two sets: the
    set of permanently labeled nodes and the set of nodes that are still temporarily
    labeled. The distance of permanently labeled nodes represents the shortest path
    distance from the source to these nodes, whereas the temporary labels contain
    a value that can be greater than or equal to the shortest path length.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步中，算法将节点集*V*划分为两个集合：永久标记节点的集合和仍然临时标记的节点的集合。永久标记节点的距离代表从源点到这些节点的最短路径距离，而临时标签包含一个可以大于或等于最短路径长度的值。
- en: There's more…
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'The basic idea of the algorithm is to start from the source and try to permanently
    label the successor nodes. At the beginning, the algorithm places the value of
    the source distance to zero and initializes the other distances to an arbitrarily
    high value (by convention, we will set the initial value of the distances: *d[i]
    = + ∞, ∀i ∈* *V*).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的基本思想是从源点开始，尝试永久标记后续节点。一开始，算法将源点的距离值设为零，并将其他距离初始化为任意高值（按照惯例，我们将距离的初始值设为：*d[i]
    = + ∞, ∀i ∈* *V*）。
- en: At each iteration, the node label i is the value of the minimum distance along
    a path from the source that contains, apart from i, only permanently labeled nodes.
    The algorithm selects the node whose label has the lowest value among those labeled
    temporarily, labels it permanently, and updates all the labels of the nodes adjacent
    to it. The algorithm terminates when all the nodes have been permanently labeled.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，节点标签i是包含除了i之外只有永久标记节点的源点到路径上的最小距离的值。算法选择标签值最低的临时标记节点，将其永久标记，并更新其相邻节点的所有标签。当所有节点都被永久标记时，算法终止。
- en: See also
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: Check out *Keras Reinforcement Learning Projects*, Giuseppe Ciaburro, Packt
    Publishing
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看书籍《Keras 强化学习项目》，作者 Giuseppe Ciaburro，出版社 Packt Publishing
- en: 'Check out *Solving Shortest Path Problem: Dijkstra''s Algorithm* (from Illinois
    University): [http://www.ifp.illinois.edu/~angelia/ge330fall09_dijkstra_l18.pdf](http://www.ifp.illinois.edu/~angelia/ge330fall09_dijkstra_l18.pdf)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看书籍《解决最短路径问题：Dijkstra算法》（来自伊利诺伊大学）：[http://www.ifp.illinois.edu/~angelia/ge330fall09_dijkstra_l18.pdf](http://www.ifp.illinois.edu/~angelia/ge330fall09_dijkstra_l18.pdf)
- en: 'Check out *Graph Theory Tutorials* (from the University of Tennessee at Martin):
    [https://primes.utm.edu/graph/index.html](https://primes.utm.edu/graph/index.html)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看书籍《图论教程》（来自田纳西大学马丁分校）：[https://primes.utm.edu/graph/index.html](https://primes.utm.edu/graph/index.html)
- en: Deciding the discount factor using Q-learning
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Q学习决定折扣因子
- en: '**Q-learning** is one of the most used reinforcement learning algorithms. This
    is due to its ability to compare the expected utility of the available actions
    without requiring an environment model. Thanks to this technique, it is possible
    to find an optimal action for every given state in a finished MDP.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q学习**是使用最广泛的强化学习算法之一。这得益于它能够比较可用动作的预期效用，而无需环境模型。多亏了这项技术，我们可以在完成MDP的每个给定状态下找到最优动作。'
- en: A general solution to the reinforcement learning problem is to estimate, thanks
    to the learning process, an evaluation function. This function must be able to
    evaluate, through the sum of the rewards, the convenience or otherwise of a particular
    policy. In fact, Q-learning tries to maximize the value of the Q function (the
    action-value function), which represents the maximum discounted future reward
    when we perform actions, *a*, in the state, *s*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习问题的一般解决方案是通过学习过程估计一个评估函数。这个函数必须能够通过奖励的总和来评估特定策略的便利性或不利性。实际上，Q学习试图最大化Q函数（动作值函数）的值，它表示当我们执行动作*a*在状态*s*时，最大化的折现未来奖励。
- en: Getting ready
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will deal with the problem of controlling a character's movement
    in a grid world by offering a first solution based on Q-learning.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将通过提供一个基于Q学习的第一个解决方案来处理在网格世界中控制角色移动的问题。
- en: How to do it…
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Let''s see how we can decide on the discount factor using Q-learning:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何使用Q学习来决定折扣因子：
- en: 'We will use the `FrozenQlearning.py` file that is already provided for you
    as reference. Let''s start by importing the libraries:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用已经提供的`FrozenQlearning.py`文件作为参考。让我们先导入库：
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then, we will move on and create the environment by calling the `make` method:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将继续创建环境，通过调用`make`方法：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This method creates the environment that our agent will run in.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法创建我们的智能体将运行的 环境。
- en: 'Now, let''s initialize the parameters, starting with `QTable`:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们初始化参数，从`QTable`开始：
- en: '[PRE28]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s define some parameters:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义一些参数：
- en: '[PRE29]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Here, `alpha` is the learning rate, `gamma` is the discount factor, and `NumEpisodes`
    is the number of episodes.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`alpha`是学习率，`gamma`是折扣因子，`NumEpisodes`是剧集数。
- en: 'Now, we will create a list to contain the total rewards:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个列表来包含总奖励：
- en: '[PRE30]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'At this point, after setting the parameters, it is possible to start the Q-learning
    cycle:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，在设置参数后，可以开始Q学习周期：
- en: '[PRE31]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: At the end of each episode, the list of rewards is enriched with a new value.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个剧集结束时，奖励列表将增加一个新值。
- en: 'Finally, we print the results:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们打印结果：
- en: '[PRE32]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following screenshot shows the final Q-Table:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图显示了最终的Q表：
- en: '![](img/21b062e8-2b5a-4052-8cf2-9d02539736a6.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21b062e8-2b5a-4052-8cf2-9d02539736a6.png)'
- en: To improve the result, the retuning of the configuration parameters is required.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高结果，需要调整配置参数的回退。
- en: How it works…
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'The `FrozenLake` environment is a 4 × 4 grid that contains four possible areas:
    **Safe** (**S**), **Frozen** (**F**), **Hole** (**H**), and **Goal** (**G**).
    The agent controls the movement of a character in a grid world, and moves around
    the grid until it reaches the goal or the hole. Some tiles of the grid are walkable,
    and others lead to the agent falling into the water. If it falls into the hole,
    it has to start from the beginning and is rewarded with the value 0\. Additionally,
    the direction in which the agent will move is uncertain and only partially depends
    on the chosen direction. If the agent finds a walkable path to a goal tile, it
    is rewarded. The agent has four possible moves: up, down, left, and right. The
    process continues until it learns from every mistake and reaches the goal.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`FrozenLake`环境是一个4 × 4的网格，包含四个可能区域：**安全**（**S**）、**冰冻**（**F**）、**洞**（**H**）和**目标**（**G**）。智能体控制网格世界中角色的移动，并在网格中移动直到达到目标或洞。网格中的一些方格是可通行的，而其他方格会导致智能体掉入水中。如果掉入洞中，它必须从头开始，并得到0的奖励。此外，智能体将移动的方向是不确定的，并且仅部分取决于所选方向。如果智能体找到一个可通行的路径到达目标方格，它将得到奖励。智能体有四种可能的移动：上、下、左和右。这个过程会持续进行，直到它从每个错误中学习并达到目标。'
- en: There's more…
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容...
- en: 'Q-learning estimates the function value *q (s, a)* incrementally, updating
    the value of the state-action pair at each step of the environment, following
    the logic of updating the general formula for estimating the values for the temporal
    difference methods. Q-learning has off-policy characteristics; that is, while
    the policy is improved according to the values estimated by *q (s, a)*, the value
    function updates the estimates following a strictly greedy secondary policy: given
    a state, the chosen action is always the one that maximizes the *max q (s, a)*
    value. However, the π policy has an important role in estimating values, because
    through it the state-action pairs to be visited and updated are determined.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 通过增量估计函数值 *q(s, a)*，在环境的每个步骤中更新状态-动作对的值，遵循更新时间差分方法估计值的一般公式的逻辑。Q-learning
    具有离线策略特性；也就是说，虽然策略是根据 *q(s, a)* 估计的值来改进的，但值函数会根据一个严格贪婪的次级策略来更新估计：给定一个状态，选择的行为总是最大化
    *max q(s, a)* 值的那个行为。然而，π 策略在估计值方面起着重要作用，因为通过它确定了要访问和更新的状态-动作对。
- en: See also
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: Check out *Keras Reinforcement Learning Projects*, Giuseppe Ciaburro, Packt
    Publishing
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看《Keras 强化学习项目》，作者 Giuseppe Ciaburro，Packt 出版
- en: 'Refer to *Reinforcement Learning: A Tutorial* (from University of Toronto):
    [http://www.cs.toronto.edu/~zemel/documents/411/rltutorial.pdf](http://www.cs.toronto.edu/~zemel/documents/411/rltutorial.pdf)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考《强化学习：教程》（多伦多大学）：[http://www.cs.toronto.edu/~zemel/documents/411/rltutorial.pdf](http://www.cs.toronto.edu/~zemel/documents/411/rltutorial.pdf)
- en: 'Check out the official site of the `gym` library: [https://gym.openai.com/](https://gym.openai.com/)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看gym库的官方网站：[https://gym.openai.com/](https://gym.openai.com/)
- en: 'Check out the *FrozenLake-v0* environment: [https://gym.openai.com/envs/FrozenLake-v0/](https://gym.openai.com/envs/FrozenLake-v0/)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看FrozenLake-v0环境：[https://gym.openai.com/envs/FrozenLake-v0/](https://gym.openai.com/envs/FrozenLake-v0/)
- en: Implementing the deep Q-learning algorithm
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现深度 Q-learning 算法
- en: '**Deep Q-learning** represents an evolution of the basic Q-learning method.
    The state-action is replaced by a neural network, with the aim of approximating
    the optimal value function. Compared to the Q-learning approaches, where it was
    used to structure the network in order to request both input and action and providing
    its expected return, deep Q-learning revolutionizes the structure to request only
    the state of the environment and supply as many status-action values as there
    are actions that can be performed in the environment.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度 Q-learning** 代表了基本 Q-learning 方法的演变。状态-动作被神经网络所取代，目的是逼近最优值函数。与 Q-learning
    方法相比，它被用来构建网络以请求输入和动作，并提供其期望回报，深度 Q-learning 革新了结构，只请求环境的状态，并提供尽可能多的状态-动作值，这些值对应于环境中可以执行的动作数量。'
- en: Getting ready
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will use the deep Q-learning approaches to controls a character's
    movement in a grid world. In this recipe, the `keras-rl` library will be used;
    to learn about it further, refer to the *Developing AI-based dynamic modeling
    system* recipe.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用深度 Q-learning 方法来控制网格世界中角色的移动。在这个菜谱中，将使用 `keras-rl` 库；要了解更多信息，请参考《开发基于
    AI 的动态建模系统》菜谱。
- en: How to do it…
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s see how we can implement a deep Q-learning algorithm:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何实现深度 Q-learning 算法：
- en: 'We will use the `FrozenDeepQLearning.py` file that is already provided for
    you as a reference. Let''s start by importing the libraries:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用提供的 `FrozenDeepQLearning.py` 文件作为参考。让我们首先导入库：
- en: '[PRE33]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then, we will define the environment and set the seed:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将定义环境和设置种子：
- en: '[PRE34]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, we will extract the actions that are available to the agent:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将提取智能体可用的动作：
- en: '[PRE35]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The `Actions` variable now contains all the actions that are available in the
    selected environment. Gym will not always tell you the meaning of those actions,
    but only about which ones are available.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`Actions` 变量现在包含在所选环境中可用的所有动作。Gym 不会总是告诉你这些动作的含义，但只会告诉你哪些动作是可用的。'
- en: 'Now, we will build a simple neural network model using the `keras` library:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用 `keras` 库构建一个简单的神经网络模型：
- en: '[PRE36]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now, the neural network model is ready to use, so let's configure and compile
    our agent. One problem with using the DQN is that the neural network that was
    used in the algorithm tends to forget previous experiences because it overwrites
    them with new experiences.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，神经网络模型已经准备好使用，所以让我们配置和编译我们的智能体。使用 DQN 的问题之一是，算法中使用的神经网络倾向于忘记先前的经验，因为它用新的经验覆盖了它们。
- en: 'So, we need a list of previous experiences and observations to reform the model
    with previous experiences. For this reason, a memory variable is defined that
    will contain the previous experiences, and a policy will be set:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们需要一个包含先前经验和观察结果列表来用先前经验重新构建模型。为此，定义了一个将包含先前经验的内存变量，并设置了一个策略：
- en: '[PRE37]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We just have to define the agent:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只需要定义智能体：
- en: '[PRE38]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let''s proceed to compile and fit the model:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续编译和拟合模型：
- en: '[PRE39]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'At the end of the training, it is necessary to save the obtained weights:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练结束时，有必要保存获得的权重：
- en: '[PRE40]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Finally, we will evaluate our algorithm for 20 episodes:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将对算法进行20个回合的评估：
- en: '[PRE41]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Our agent is now able to identify the path that allows them to reach the goal.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的智能体现在能够识别出通往目标的路径。
- en: How it works…
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: A general solution to the reinforcement learning problem is to estimate, thanks
    to the learning process, an evaluation function. This function must be able to
    evaluate, through the sum of the rewards, the convenience or otherwise of a particular
    policy. In fact, Q-learning tries to maximize the value of the `Q` function (action-value
    function), which represents the maximum discounted future reward when we perform
    actions, *a*, in the state, *s*. DQN represents an evolution of the basic Q-learning
    method, where the state-action is replaced by a neural network, with the aim of
    approximating the optimal value function.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习问题的一般解决方案是，通过学习过程估计一个评估函数。这个函数必须能够通过奖励的总和来评估特定策略的便利性或不利性。实际上，Q学习试图最大化`Q`函数（动作值函数）的值，它表示在状态`s`中执行动作`a`时的最大折现未来奖励。DQN代表了基本Q学习方法的演变，其中状态-动作被神经网络取代，目的是逼近最优值函数。
- en: There's more…
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: OpenAI Gym is a library that helps us implement algorithms based on reinforcement
    learning. It includes a growing collection of benchmark issues that expose a common
    interface, and a website where people can share their results and compare algorithm
    performance.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym是一个帮助我们实现基于强化学习算法的库。它包括一个不断增长的基准问题集合，这些问题提供了一个公共接口，以及一个网站，人们可以在那里分享他们的结果并比较算法性能。
- en: OpenAI Gym focuses on the episodic setting of reinforced learning. In other
    words, the agent's experience is divided into a series of episodes. The initial
    state of the agent is randomly sampled by a distribution, and the interaction
    proceeds until the environment reaches a terminal state. This procedure is repeated
    for each episode, with the aim of maximizing the total reward expectation per
    episode and achieving a high level of performance in the fewest possible episodes.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym专注于强化学习的回合设置。换句话说，智能体的经验被划分为一系列回合。智能体的初始状态由一个分布随机采样，交互过程一直进行，直到环境达到终端状态。这个程序在每个回合中重复进行，目的是最大化每个回合的总奖励期望，并在尽可能少的回合内达到高水平的表现。
- en: See also
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: Refer to *Keras Reinforcement Learning Projects*, Giuseppe Ciaburro, Packt Publishing
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请参考 *Keras强化学习项目*，作者Giuseppe Ciaburro，Packt Publishing
- en: 'Refer to *Learning 2048 with Deep Reinforcement Learning* (from the University
    of Waterloo): [https://cs.uwaterloo.ca/~mli/zalevine-dqn-2048.pdf](https://cs.uwaterloo.ca/~mli/zalevine-dqn-2048.pdf)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请参考 *使用深度强化学习学习2048*（来自滑铁卢大学）：[https://cs.uwaterloo.ca/~mli/zalevine-dqn-2048.pdf](https://cs.uwaterloo.ca/~mli/zalevine-dqn-2048.pdf)
- en: 'Refer to *Deep RL with Q-Functions* (from UC Berkeley): [http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请参考 *深度强化学习中的Q函数*（来自加州大学伯克利分校）：[http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf)
- en: Developing an AI-based dynamic modeling system
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发基于AI的动态建模系统
- en: A **Segway** is a personal transport device that exploits an innovative combination
    of computer science, electronics, and mechanics. It functions as an extension
    of the body; as with a partner in a dance, it is able to anticipate every move.
    The operating principle is based on the **reverse pendulum** system. The reverse
    pendulum system is an example that's commonly found in textbooks on control and
    research literature. Its popularity derives in part from the fact that it is unstable
    without control and has a non-linear dynamic, but, above all, because it has several
    practical applications, such as controlling a rocket's take-off or a Segway.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**Segway**是一种利用计算机科学、电子学和机械学创新组合的个人交通工具。它作为身体的延伸；就像舞伴一样，能够预测每一个动作。其工作原理基于**反向摆**系统。反向摆系统是控制理论和研究文献中常见的例子。它的流行部分原因在于它没有控制是不稳定的，并且具有非线性动态，但更重要的是，它有多个实际应用，例如控制火箭的起飞或Segway。'
- en: Getting ready
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will analyze the functioning of a physical system that's
    made by connecting a rigid rod to a cart, modeling the system using different
    approaches. The rod is connected through a pivot that's hinged on the carriage
    and is free to rotate around it. This mechanical system, which is called the reverse
    pendulum, is a classic problem in control theory.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将分析由连接到车上的刚性杆组成的物理系统的功能，使用不同的方法来模拟系统。杆通过一个铰链连接到车架上，可以自由地围绕它旋转。这个被称为反向摆的机械系统是控制理论中的经典问题。
- en: How to do it…
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Let''s see how we can develop an AI-based dynamic modeling system:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何开发一个基于AI的动态建模系统：
- en: 'We will use the `dqn_cartpole.py` file that is already provided for you as
    a reference. Let''s start by importing the libraries:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用已经为你提供的`dqn_cartpole.py`文件作为参考。让我们先导入库：
- en: '[PRE42]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, we will define and load the environment:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将定义和加载环境：
- en: '[PRE43]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'To set the `seed` value, the NumPy library''s `random.seed()` function is used,
    as follows:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了设置`seed`值，使用了NumPy库的`random.seed()`函数，如下所示：
- en: '[PRE44]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, we will extract the actions that are available to the agent:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将提取代理可用的动作：
- en: '[PRE45]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We will build a simple neural network model using the `keras` library:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`keras`库构建一个简单的神经网络模型：
- en: '[PRE46]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'A `memory` variable and a `policy` will be set:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将设置一个`memory`变量和一个`policy`：
- en: '[PRE47]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We just have to define the agent:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只需要定义代理：
- en: '[PRE48]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let''s move on to compile and fit the model:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续编译和拟合模型：
- en: '[PRE49]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'At the end of the training, it is necessary to save the obtained weights:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练结束时，有必要保存获得的权重：
- en: '[PRE50]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Saving the weight of a network or an entire structure takes place in an `HDF5`
    file, an efficient and flexible storage system that supports complex multidimensional
    datasets.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 网络或整个结构的权重保存发生在`HDF5`文件中，这是一个高效且灵活的存储系统，支持复杂的多维数据集。
- en: 'Finally, we will evaluate our algorithm for 10 episodes:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将对算法进行10个回合的评估：
- en: '[PRE51]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: How it works…
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In this recipe, we used the `keras–rl` package to develop an AI-based dynamic
    modeling system. This package implements some deep reinforcement learning algorithms
    in Python, and integrates seamlessly with Keras' in-depth learning library.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们使用了`keras–rl`包来开发一个基于AI的动态建模系统。这个包在Python中实现了某些深度强化学习算法，并且与Keras的深度学习库无缝集成。
- en: Furthermore, `keras-rl` works immediately with OpenAI Gym. OpenAI Gym includes
    a growing collection of benchmark issues that shows a common interface and a website
    where people can share their results and compare algorithm performance. This library
    will be adequately addressed in the next chapter—for now, we will limit ourselves
    to using it.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`keras-rl`可以立即与OpenAI Gym一起工作。OpenAI Gym包括一个不断增长的基准问题集合，展示了通用的接口和网站，人们可以在那里分享他们的结果并比较算法性能。这个库将在下一章中适当介绍——现在，我们将限制自己使用它。
- en: There's more…
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: These choices do not limit the use of the `keras-rl` package, in the sense that
    the uses of `keras-rl` can be easily adapted to our needs. You can use the built-in
    Keras callbacks and metrics, or define others. For this reason, it is easy to
    implement your own environments, and even algorithms, simply by extending some
    simple abstract classes.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这些选择并不限制`keras-rl`包的使用，从`keras-rl`的使用可以很容易地适应我们的需求。你可以使用内置的Keras回调和指标，或者定义其他的。因此，通过扩展一些简单的抽象类，很容易实现自己的环境，甚至算法。
- en: See also
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: Check out *Keras Reinforcement Learning Projects*, Giuseppe Ciaburro, Packt
    Publishing
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看《Keras强化学习项目》，作者Giuseppe Ciaburro，Packt Publishing出版社
- en: 'Check out *Tutorial: Deep Reinforcement Learning* (from Google DeepMind): [https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf](https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf)'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看教程：《深度强化学习》（来自Google DeepMind）：[https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf](https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf)
- en: 'Refer to *Deep Reinforcement Learning* (by Xu Wang): [https://pure.tue.nl/ws/files/46933213/844320-1.pdf](https://pure.tue.nl/ws/files/46933213/844320-1.pdf)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考书籍《深度强化学习》（作者徐王）：[https://pure.tue.nl/ws/files/46933213/844320-1.pdf](https://pure.tue.nl/ws/files/46933213/844320-1.pdf)
- en: Deep reinforcement learning with double Q-learning
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双Q-learning的深度强化学习
- en: 'In the Q-learning algorithm, the future maximum approximated action value is
    evaluated using the same Q function as the current stock selection policy. In
    some cases, this can overestimate the action values, slowing down learning. A
    variation called **Double Q-learning** was proposed by DeepMind researchers in
    the following paper: *Deep reinforcement learning with Double Q-learning*, H van
    Hasselt, A Guez, and D Silver, March, 2016, at the Thirtieth AAAI Conference on
    Artificial Intelligence. As a solution to this problem, the authors proposed to
    modify the Bellman update.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在Q-learning算法中，使用与当前股票选择策略相同的Q函数评估未来的最大近似动作值。在某些情况下，这可能会高估动作值，从而减慢学习速度。DeepMind研究人员在以下论文中提出了一个名为**Double
    Q-learning**的变体：*Deep reinforcement learning with Double Q-learning*，H van Hasselt，A
    Guez，和D Silver，2016年3月，在第三十届AAAI人工智能会议上。作为解决这个问题的方案，作者们提出了修改Bellman更新的方法。
- en: Getting ready
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: In this recipe, we will control an inverted pendulum system using the Double
    Q-learning algorithm.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用Double Q-learning算法控制一个倒立摆系统。
- en: How to do it…
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Let''s see how we can perform deep reinforcement learning with Double Q-learning:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用Double Q-learning进行深度强化学习：
- en: 'We will use the `DoubleDQNCartpole.py` file that is already provided for you
    as a reference. Let''s start by importing the libraries:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用已经为你提供的`DoubleDQNCartpole.py`文件作为参考。让我们首先导入库：
- en: '[PRE52]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now, we will define and load the environment:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将定义和加载环境：
- en: '[PRE53]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'To set the `seed` value, the NumPy library''s `random.seed()` function is used,
    as follows:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要设置`seed`值，使用NumPy库的`random.seed()`函数，如下所示：
- en: '[PRE54]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, we will extract the actions that are available to the agent:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将提取代理可用的动作：
- en: '[PRE55]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We will build a simple neural network model using the `keras` library:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`keras`库构建一个简单的神经网络模型：
- en: '[PRE56]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'A `memory` variable and a `policy` will be set:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将设置一个`memory`变量和一个`policy`：
- en: '[PRE57]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We just have to define the agent:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只需要定义代理：
- en: '[PRE58]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: To enable the double network, we have to set the `enable_double_dqn` option
    to `True`.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用双网络，我们必须将`enable_double_dqn`选项设置为`True`。
- en: 'Let''s move on to compile and fit the model:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续编译和拟合模型：
- en: '[PRE59]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'At the end of the training, it is necessary to save the obtained weights:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练结束时，有必要保存获得的权重：
- en: '[PRE60]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Saving the weight of a network or an entire structure takes place in an `HDF5`
    file, an efficient and flexible storage system that supports complex multidimensional
    datasets.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 网络或整个结构的权重保存发生在`HDF5`文件中，这是一个高效且灵活的存储系统，支持复杂的多元数据集。
- en: 'Finally, we will evaluate our algorithm for 10 episodes:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将对算法进行10个回合的评估：
- en: '[PRE61]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: How it works…
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The overestimation of the action value is due to the maximum operator that is
    used in the Bellman equation. The max operator uses the same value for both selecting
    and evaluating an action. Now, if we select the best action as the one that has
    the maximum value, we will end up selecting a sub-optimal action (which assumes
    the maximum value by mistake) instead of the optimal action. We can solve this
    problem by having two separate Q functions, each of which learns independently.
    A Q1 function is used to select an action, and the other Q2 function is used to
    evaluate an action. To do this, simply change the objective function.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 动作值的高估是由于在Bellman方程中使用的最大运算符。最大运算符在选择和评估动作时使用相同的值。现在，如果我们选择具有最大值的最佳动作，我们最终会选择一个次优动作（错误地假设了最大值）而不是最佳动作。我们可以通过有两个独立的Q函数来解决这个问题，每个Q函数都独立学习。一个Q1函数用于选择动作，另一个Q2函数用于评估动作。为此，只需更改目标函数。
- en: There's more…
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'Essentially, the following two networks are used:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，以下两个网络被使用：
- en: The DQN network to select what is the best action to take for the next state
    (the action with the highest Q value)
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DQN网络用于选择在下一个状态中采取的最佳动作（具有最高Q值的动作）
- en: The target network, to calculate the target Q value of taking that action at
    the next state
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标网络，用于计算在下一个状态下执行该动作的目标Q值
- en: See also
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: Check out *Keras Reinforcement Learning Projects*, Giuseppe Ciaburro, Packt
    Publishing
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看*Keras强化学习项目*，作者Giuseppe Ciaburro，Packt Publishing
- en: 'Refer to *Deep Reinforcement Learning with Double Q-learning*: [https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847)'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考文献请见*使用双重Q学习的深度强化学习*：[https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847)
- en: Deep Q-network algorithm with dueling Q-learning
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q网络算法与对抗Q学习
- en: 'To improve convergence speed by making our network''s architecture closer represent
    one of the last challenges of reinforcement learning, a definite improvement in
    the performance of a DQN model has been proposed by Wang and others in the following
    paper: *Dueling network architectures for deep reinforcement learning, Z *Wang,
    T Schaul, M Hessel, H van Hasselt, M Lanctot, and N de Freitas, 2015, arXiv preprint
    arXiv:1511.06581.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过使我们的网络架构更接近强化学习的最后挑战之一来提高收敛速度，王等人提出了以下论文中DQN模型性能的明显改进：*对抗网络架构用于深度强化学习，Z
    Wang, T Schaul, M Hessel, H van Hasselt, M Lanctot, and N de Freitas, 2015, arXiv预印本arXiv:1511.06581。
- en: Getting ready
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will control an inverted pendulum system using the dueling
    Q-learning algorithm.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用对抗Q学习算法控制一个倒立摆系统。
- en: How to do it…
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Let''s see how we can perform deep Q-network algorithm with dueling Q-learning:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用对抗Q学习执行深度Q网络算法：
- en: 'We will use the `DuelingDQNCartpole.py` file that is already provided for you
    as a reference. Let''s start by importing the libraries:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用已提供的`DuelingDQNCartpole.py`文件作为参考。让我们首先导入库：
- en: '[PRE62]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Now, we will define and load the environment:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将定义和加载环境：
- en: '[PRE63]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'To set the `seed` value, the NumPy library''s `random.seed()` function is used,
    as follows:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要设置`seed`值，使用NumPy库的`random.seed()`函数，如下所示：
- en: '[PRE64]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Now, we will extract the actions, available to the agent:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将提取智能体可用的动作：
- en: '[PRE65]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We will build a simple neural network model using the Keras library:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用Keras库构建一个简单的神经网络模型：
- en: '[PRE66]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'A `memory` variable and a `policy` will be set:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将设置一个`memory`变量和一个`policy`：
- en: '[PRE67]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We just have to define the agent:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需定义智能体：
- en: '[PRE68]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: To enable the dueling network, we have to specify the `dueling_type` to one
    of the following:`'avg'`, `'max'`, or `'naive'`.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用对抗网络，我们必须指定`dueling_type`为以下之一：`'avg'`、`'max'`或`'naive'`。
- en: 'Let''s move on to compile and fit the model:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续编译和拟合模型：
- en: '[PRE69]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'At the end of the training, it is necessary to save the obtained weights:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练结束时，有必要保存获得的权重：
- en: '[PRE70]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Saving the weight of a network or an entire structure takes place in an `HDF5`
    file, an efficient and flexible storage system that supports complex multidimensional
    datasets.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 网络或整个结构的权重保存发生在`HDF5`文件中，这是一个高效且灵活的存储系统，支持复杂的多元数据集。
- en: 'Finally, we will evaluate our algorithm for 10 episodes:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将对算法进行10个回合的评估：
- en: '[PRE71]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: How it works…
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'In reinforcement learning, the function Q and the value function play a fundamental
    role:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，函数Q和值函数扮演着基本角色：
- en: The Q function specifies how good an agent is to perform an action in the *s*
    state
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q函数指定了智能体在状态`s`中执行动作的好坏
- en: The value function specifies how good it is for an agent to be in a state, *s*
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值函数指定了智能体处于状态`s`时的好坏
- en: To introduce a further improvement in the performance of a DQN, we introduce
    a new function called an `advantage` function, which can be defined as the difference
    between the `value` function and the `benefit` function. The `benefit` function
    specifies how good an agent is at performing an action compared to other actions.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高DQN的性能，我们引入了一个新的函数，称为`advantage`函数，它可以定义为`value`函数和`benefit`函数之间的差异。`benefit`函数指定了智能体在执行动作方面相对于其他动作的好坏。
- en: Therefore, the `value` function specifies the goodness of a state and the `advantage`
    function specifies the goodness of an action. Then, the combination of these two
    functions tells us how good it is for an agent to perform an action in a state
    that is actually our `Q` function. So, we can define our `Q` function as the sum
    of a `value` function and an `advantage` function.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`价值`函数指定了状态的好坏，而`优势`函数指定了动作的好坏。然后，这两个函数的组合告诉我们代理在某个状态下执行动作的好坏，这就是我们的`Q`函数。因此，我们可以将我们的`Q`函数定义为`价值`函数和`优势`函数的总和。
- en: 'The dueling DQN is essentially a DQN, in which the fully connected final layer
    is divided into two streams:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗式DQN本质上是一个DQN，其中完全连接的最终层被分为两个流：
- en: One calculates the `value` function
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个计算`价值`函数
- en: The other calculates the `advantage` function
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个计算`优势`函数
- en: Finally, the two streams are combined using the aggregate level for obtaining
    the `Q` function.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用聚合级别将两个流合并以获得`Q`函数。
- en: There's more…
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: The approximation of the `value` function via the neural network is anything
    but stable. To achieve convergence, the basic algorithm should be modified by
    introducing techniques to avoid oscillations and divergences.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 通过神经网络对`价值`函数的近似远非稳定。为了实现收敛，基本算法应通过引入避免振荡和发散的技术进行修改。
- en: The most important technique is called `experience replay`. During the episodes,
    at each step, the agent's experience is stored in a dataset, called `replay memory`.
    In the internal cycle of the algorithm, instead of performing the training on
    the network based on the only transition just performed, a subset of transitions
    is selected randomly from the replay memory, and the training takes place according
    to the loss calculated on the subset of transitions.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的技术称为`经验重放`。在剧集期间，在每一步，代理的经验被存储在一个数据集中，称为`重放记忆`。在算法的内部循环中，不是基于刚刚执行的唯一转换在网络上进行训练，而是从重放记忆中随机选择转换的一个子集，并根据这个转换子集计算出的损失进行训练。
- en: The experience of the `replay` technique, that is, randomly selecting transitions
    from `replay memory`, eliminates the problem of correlation between consecutive
    transitions and reduces variance among different updates.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '`重放`技术的经验，即从`重放记忆`中随机选择转换，消除了连续转换之间的相关性问题，并减少了不同更新之间的方差。'
- en: See also
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: Check out *Keras Reinforcement Learning Projects*, Giuseppe Ciaburro, Packt
    Publishing
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看Giuseppe Ciaburro的《Keras强化学习项目》，Packt Publishing
- en: 'Refer to *Dueling Network Architectures for Deep Reinforcement Learning* for
    more information: [https://arxiv.org/abs/1511.06581](https://arxiv.org/abs/1511.06581)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多信息请参阅 *《深度强化学习的对抗网络架构》*：[https://arxiv.org/abs/1511.06581](https://arxiv.org/abs/1511.06581)
