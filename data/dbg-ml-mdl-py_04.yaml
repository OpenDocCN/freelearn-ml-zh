- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Detecting Performance and Efficiency Issues in Machine Learning Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测机器学习模型中的性能和效率问题
- en: One of the main objectives we must keep in mind is how to build a high-performance
    machine learning model with minimal errors on new data we want to use the model
    for. In this chapter, you will learn how to properly assess the performance of
    your models and identify opportunities for decreasing their errors.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须牢记的主要目标之一是，如何在新的数据上构建一个高性能的机器学习模型，这些数据是我们希望使用模型的情况。在本章中，你将学习如何正确评估你的模型性能，并识别减少它们错误的机会。
- en: This chapter includes many figures and code examples to help you better understand
    these concepts and start benefiting from them in your projects.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含许多图表和代码示例，以帮助你更好地理解这些概念，并在你的项目中开始从中受益。
- en: 'We will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Performance and error assessment measures
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能和错误评估措施
- en: Visualization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化
- en: Bias and variance diagnosis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差和方差诊断
- en: Model validation strategy
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型验证策略
- en: Error analysis
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误分析
- en: Beyond performance
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除此之外
- en: By the end of this chapter, you will have learned about how to assess the performance
    of machine learning models and the benefits, limitations, and wrong usage of visualization
    in different machine learning problems. You will have also learned about bias
    and variance diagnosis and error analysis to help you identify opportunities so
    that you can improve your models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解如何评估机器学习模型的性能，以及可视化在不同机器学习问题中的好处、局限性和错误使用。你还将了解偏差和方差诊断以及错误分析，以帮助你识别机会，以便你可以改进你的模型。
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following requirements should be considered for this chapter as they will
    help you better understand the concepts, use them in your projects, and practice
    with the provided code:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，以下要求应予以考虑，因为它们将帮助你更好地理解概念，在你的项目中使用它们，并使用提供的代码进行实践：
- en: 'Python library requirements:'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python库要求：
- en: '`sklearn` >= 1.2.2'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn` >= 1.2.2'
- en: '`numpy` >= 1.22.4'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy` >= 1.22.4'
- en: '`pandas` >= 1.4.4'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas` >= 1.4.4'
- en: '`matplotlib` >= 3.5.3'
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib` >= 3.5.3'
- en: '`collections` >= 3.8.16'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`collections` >= 3.8.16'
- en: '`xgboost` >= 1.7.5'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xgboost` >= 1.7.5'
- en: You should have basic knowledge of model validation and testing, as well as
    classification, regression, and clustering in machine learning
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该具备模型验证和测试的基本知识，以及机器学习中的分类、回归和聚类
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter04](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter04).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub上找到本章的代码文件，网址为[https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter04](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter04)。
- en: Performance and error assessment measures
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能和错误评估措施
- en: The metrics we use to assess the performance and calculate errors in our models,
    and how we interpret their values, determine the models we select, the decisions
    we make to improve a component of our machine learning life cycle, and determine
    if we have a reliable model to bring into production. Although many performance
    metrics can be used in one line of Python code to calculate errors and performance,
    we shouldn’t blindly use them or try to improve our performance reports by implementing
    many of them together without knowing their limitations and how to correctly interpret
    them. In this section, we will talk about metrics for assessing the performance
    of classification, regression, and clustering models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来评估模型性能和计算错误的指标，以及我们如何解释它们的值，决定了我们选择的模型，我们做出的改进机器学习生命周期组件的决定，以及我们是否有一个可靠的生产模型。尽管许多性能指标可以用一行Python代码来计算错误和性能，但我们不应该盲目使用它们，或者试图通过实现许多指标来提高我们的性能报告，而不了解它们的限制和如何正确解释它们。在本节中，我们将讨论用于评估分类、回归和聚类模型性能的指标。
- en: Classification
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类
- en: 'Each classification model, either binary or multi-class, returns the probability
    of predictions, a number between 0 and 1, which then gets transformed into class
    labels. There are two major categories of performance metrics: **label-based performance
    metrics**, which rely on predicted labels, and **probability-based performance
    metrics**, which use the probability of predictions for performance or error calculation.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分类模型，无论是二分类还是多分类，都会返回预测的概率，一个介于 0 和 1 之间的数字，然后将其转换为类别标签。性能指标主要有两大类：**基于标签的性能指标**，它依赖于预测标签，以及**基于概率的性能指标**，它使用预测的概率进行性能或错误计算。
- en: Label-based performance metrics
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于标签的性能指标
- en: 'The predicted probabilities of classification models get transformed into class
    labels by the Python classes we use for modeling. We can then use a confusion
    matrix, as shown in *Figure 4**.1*, to identify four groups of data points, including
    **true positives** (**TPs**), **false positives** (**FPs**), **false negatives**
    (**FNs**), and **true negatives** (**TNs**) for binary classification problems:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型的预测概率通过我们用于建模的 Python 类转换为类别标签。然后，我们可以使用如图 *图 4.1* 所示的混淆矩阵来识别四组数据点，包括**真正例**（**TPs**）、**假正例**（**FPs**）、**假反例**（**FNs**）和**真反例**（**TNs**）对于二分类问题：
- en: '![Figure 4.1 – Confusion matrix for binary classification](img/B16369_04_01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – 二分类的混淆矩阵](img/B16369_04_01.jpg)'
- en: Figure 4.1 – Confusion matrix for binary classification
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 二分类的混淆矩阵
- en: 'We can use `sklearn.metrics.confusion_matrix()` to extract these four groups
    of data points and then calculate performance metrics such as specificity according
    to the following mathematical definition:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `sklearn.metrics.confusion_matrix()` 提取这四组数据点，然后根据以下数学定义计算性能指标，如特异性：
- en: '![](img/B16369_04_001.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16369_04_001.jpg)'
- en: 'Here is the Python implementation of extracting specificity, precision, and
    recall from a confusion matrix:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用混淆矩阵提取特异性、精确率和召回率的 Python 实现：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can calculate other performance metrics, such as precision and recall, using
    TP, TN, FP, and FN, which have been extracted from the confusion matrix, or directly
    use functions available in Python (*Table 4.1*). In addition to the Python functions
    to calculate some of the common performance metrics for classification models,
    you can also find the mathematical definitions of the metrics and their interpretations
    in *Table 4.1*. This extra information will help you understand how to interpret
    each of these metrics and when to use them:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用从混淆矩阵中提取的 TP、TN、FP 和 FN 计算其他性能指标，如精确率和召回率，或者直接使用 Python 中可用的函数（*表 4.1*）。除了用于计算分类模型的一些常见性能指标的
    Python 函数外，您还可以在 *表 4.1* 中找到指标的数学定义及其解释。这些额外信息将帮助您理解如何解释这些指标以及何时使用它们：
- en: '| **Metric** | **Python** **Function** | **Formula** | **Description** |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **指标** | **Python** **函数** | **公式** | **描述** |'
- en: '| Accuracy | `metrics.accuracy_score()` |  TP + TN _ n n: Number of data points
    | Number of correct predictions over the total number of data pointsRange: [0,
    1]Higher values mean higher performance |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | `metrics.accuracy_score()` |  TP + TN _ n n: 数据点数量 | 在总数据点中正确预测的数量范围：[0,
    1]值越高表示性能越好 |'
- en: '| **Precision or positive predictive** **value** (**PPV**) | `metrics.precision_score()`
    |  TP _ TP + FP  | Fraction of predicted positives that are positiveRange:[0,
    1]Higher values mean higher performance |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **精确率或阳性预测值**（**PPV**） | `metrics.precision_score()` |  TP _ TP + FP  | 预测为阳性的预测中实际为阳性的比例范围：[0,
    1]值越高表示性能越好 |'
- en: '| Recall, sensitivity, or **true positive** **rate** (**TPR**) | `metrics.recall_score()`
    |  TP _ TP + FN  | Fraction of positives that are predicted as positiveRange:[0,
    1]Higher values mean higher performance |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 召回率、灵敏度或**真正例率**（**TPR**） | `metrics.recall_score()` |  TP _ TP + FN  | 被预测为阳性的正例的比例范围：[0,
    1]值越高表示性能越好 |'
- en: '| F1 score and its derivatives | `metrics.f1_score()` |  Precision * Recall  ____________ Precision
    + Recall _ 2   | The harmonic mean of precision and recallRange:[0, 1]Higher values
    mean higher performance |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| F1 分数及其衍生指标 | `metrics.f1_score()` |  精确率 * 召回率  ____________ 精确率 + 召回率 _ 2  
    | 精确率和召回率的调和平均值范围：[0, 1]值越高表示性能越好 |'
- en: '| Balanced accuracy | `metrics.balanced_accuracy_score()` |  Recall + Specificity  _____________ 2 
    | Average of the fraction of positives and negatives that are truly predictedRange:[0,
    1]Higher values mean higher performance |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 平衡准确率 | `metrics.balanced_accuracy_score()` |  召回率 + 特异性  _____________ 2 
    | 真正预测的正负比例的平均值范围：[0, 1]值越高表示性能越好 |'
- en: '| **Matthews correlation** **coefficient** (**MCC**) | `sklearn.metrics.matthews_corrcoef()`
    |  TP * TN − FP * FN  ______________________________   √ __________________________________    (TP
    + FP)(FP + TN)(TN + FN)(FN + TP)   | The numerator aims to maximize diagonal and
    minimize off-diagonal elements of a confusion matrixRange: [ − 1, 1]Higher values
    mean higher performance |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| **马修斯相关系数** **系数**（**MCC**） | `sklearn.metrics.matthews_corrcoef()` |  TP
    * TN − FP * FN  ______________________________   √ __________________________________    (TP
    + FP)(FP + TN)(TN + FN)(FN + TP)   | 分子旨在最大化混淆矩阵的对角线元素并最小化非对角线元素范围：[ − 1, 1]值越高表示性能越好
    |'
- en: Table 4.1 – Common metrics for assessing the performance of classification models
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.1 – 评估分类模型性能的常用指标
- en: One aspect of selecting performance metrics for model selection and reporting
    is their relevance to the target problem. For example, if you are building a model
    for cancer detection, you could aim to maximize recall by maximizing the identification
    of all positive class members (that is, cancer patients) while controlling them
    for precision. This strategy helps you make sure patients with cancer will not
    remain undiagnosed with a deadly disease, although it would be ideal to have a
    model with high precision and recall at the same time.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性能指标用于模型选择和报告的一个方面是它们与目标问题的相关性。例如，如果你正在构建一个用于癌症检测的模型，你可以通过最大化识别所有正类成员（即癌症患者）来最大化召回率，同时控制精确度。这种策略可以帮助你确保患有癌症的患者不会因致命疾病而未得到诊断，尽管同时拥有高精确度和召回率的模型会更理想。
- en: Selecting performance metrics depends on whether we care about the true prediction
    of all classes with the same level of importance or whether there are one or more
    classes that would be more important. There are algorithmic ways to enforce the
    model to care more about one or multiple classes. Also, in reporting performance
    and model selection, we need to consider this imbalance between the classes and
    not solely rely on performance metrics that summarize the prediction performance
    of all classes with equal weights.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性能指标取决于我们是否关心所有类别的真实预测具有相同的重要性水平，或者是否有一个或多个类别更为重要。有一些算法方法可以强制模型更加关注一个或多个类别。此外，在报告性能和模型选择时，我们需要考虑类别之间的这种不平衡，而不仅仅依赖于总结所有类别预测性能的等权重性能指标。
- en: We also have to note that we define positive and negative classes in the case
    of binary classification. The data we generate or collect usually does not have
    such labeling. For example, your dataset could have “fraud” versus “not fraud,”
    “cancer” versus “healthy,” or digit names in strings such as “one,” “two,” and
    “three.” So, we need to select the performance metrics according to our definition
    of classes if there are one or more we care more or less about.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须注意，在二元分类的情况下，我们需要定义正类和负类。我们生成或收集的数据通常没有这样的标签。例如，你的数据集可能有“欺诈”与“非欺诈”、“癌症”与“健康”，或者字符串中的数字名称，如“一”、“二”和“三”。因此，如果有我们更关心或更少关心的一个或多个类别，我们需要根据我们对类别的定义来选择性能指标。
- en: The other aspect of selecting performance metrics is their reliability, and
    if they have biases that depend on the data, we use them for training, validation,
    or testing. For example, accuracy, one of the widely used performance metrics
    for classification models, should not be used on an imbalanced dataset. Accuracy
    is defined as the total number of correct predictions over the total number of
    data points (*Table 4.1*). Hence, if a model predicts all data points as the majority
    class, it returns a high value, even if it might not be a good model. *Figure
    4**.2* shows the values of different performance metrics, including accuracy,
    for a model that predicts all data points as negatives. The accuracy of this bad
    model is 0.8 if 80% of the data points in the dataset are negative (*Figure 4**.2*).
    However, alternative performance metrics such as balanced accuracy or **Matthews
    correlation coefficient** (**MCC**) remain unchanged for such a bad model across
    datasets with different positive data point fractions. Data balance is only one
    of the parameters, although an important one, in selecting performance metrics
    for classification models.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性能指标的其他方面是它们的可靠性，如果它们有依赖于数据的偏差，我们就会在训练、验证或测试中使用它们。例如，准确率，作为分类模型广泛使用的性能指标之一，不应在不平衡的数据集上使用。准确率定义为正确预测的总数除以数据点的总数（*表4.1*）。因此，如果一个模型将所有数据点预测为多数类，即使它可能不是一个好的模型，它也会返回一个高值。*图4.2*显示了不同性能指标，包括准确率，对于一个将所有数据点预测为负数的模型的值。如果数据集中有80%的数据点是负数，那么这个糟糕模型的准确率是0.8（*图4.2*）。然而，平衡准确率或**马修斯相关系数**（**MCC**）等替代性能指标在具有不同正数据点分数的数据集上对这样一个糟糕的模型来说保持不变。数据平衡只是选择分类模型性能指标时考虑的参数之一，尽管它很重要。
- en: 'Some of the performance metrics have derivatives that better behave in situations
    such as imbalanced data classification. For example, F1 is a widely used metric
    that is not the best choice when dealing with imbalanced data classification (*Figure
    4**.2*):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一些性能指标具有更好的行为，适用于不平衡数据分类等情境。例如，F1是一个广泛使用的指标，但在处理不平衡数据分类时并不是最佳选择（*图4.2*）：
- en: '![Figure 4.2 – Values of common classification metrics across different real
    positive fractions for a model that returns all predictions as negatives](img/B16369_04_02.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图4.2 – 对于一个将所有预测返回为负数的模型，在不同真实正分数下的常见分类指标值](img/B16369_04_02.jpg)'
- en: Figure 4.2 – Values of common classification metrics across different real positive
    fractions for a model that returns all predictions as negatives
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 – 对于一个将所有预测返回为负数的模型，在不同真实正分数下的常见分类指标值
- en: 'However, it has a general form of F β where a parameter, β, is used as a weight
    for increasing the effect of precision according to its mathematical definition.
    You can use the `sklearn.metrics.fbeta_score()` function to calculate this metric
    using true and predicted labels of a list of data points:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它有一个通用的形式Fβ，其中参数β用作根据其数学定义增加精度的效果的权重。你可以使用`sklearn.metrics.fbeta_score()`函数来计算这个指标，使用数据点的真实标签和预测标签：
- en: '![](img/B16369_04_018.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16369_04_018.jpg)'
- en: Probability-based performance metrics
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于概率的性能指标
- en: 'The probability outputs of classification models can be directly used to assess
    the performance of models, without the need for transformation to predict labels.
    An example of such a performance measure is **logistic loss**, known as **log-loss**
    or **cross-entropy loss**, which calculates the total loss over a dataset using
    probabilities of prediction for each data point and its true label, as follows.
    Log-loss is also a loss function that’s used to train classification models:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型的概率输出可以直接用来评估模型性能，无需将预测标签进行转换。这种性能度量的一种例子是**逻辑损失**，也称为**对数损失**或**交叉熵损失**，它使用每个数据点的预测概率及其真实标签来计算数据集上的总损失，如下所示。对数损失也是一个用于训练分类模型的损失函数：
- en: L log(y, p) = − (ylog(p) + (1 − y)log(1 − p))
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: L log(y, p) = − (ylog(p) + (1 − y)log(1 − p))
- en: 'There are other types of probability-based performance assessment methods such
    as the **receiver operating characteristic** (**ROC**) curve and the **precision
    recall** (**PR**) curve that consider different cutoffs for transforming probabilities
    into labels to predict the true positive rate, false positive rate, precision,
    and recall. Then, these values, across different cutoffs, get used to generate
    ROC and PR curves (*Figure 4**.3*):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他基于概率的性能评估方法，如**接收者操作特征**（**ROC**）曲线和**精确率召回率**（**PR**）曲线，它们考虑了将概率转换为标签的不同截止点，以预测真正例率、假正例率、精确率和召回率。然后，这些值在不同截止点被用来生成ROC和PR曲线（**图4.3**）：
- en: '![Figure 4.3 – Schematic illustration of ROC and PR curves](img/B16369_04_03.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3 – ROC和PR曲线的示意图](img/B16369_04_03.jpg)'
- en: Figure 4.3 – Schematic illustration of ROC and PR curves
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – ROC和PR曲线的示意图
- en: It is common to use the area under these curves, referred to as ROC-AUC and
    PR-AUC, to assess the performance of classification models. ROC-AUC and PR-AUC
    range from 0 to 1, with 1 being the performance of a perfect model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些曲线下的面积，称为ROC-AUC和PR-AUC，来评估分类模型的性能是很常见的。ROC-AUC和PR-AUC的范围从0到1，其中1表示完美模型的性能。
- en: In *Figure 4**.2*, you saw how some performance metrics return high-performance
    values for a bad model that predicts everything as negative due to data imbalance.
    We can see the extension of this analysis in *Figure 4**.4* for different fractions
    of positive data points among true labels and predicted labels. There is no training
    here and the data points are randomly generated to result in the specified fraction
    of positive data points in each panel of *Figure 4**.4*. The randomly generated
    probabilities are then transformed into labels so that they can be compared with
    true labels using different performance metrics.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图4.2*中，你看到了一些性能指标如何为预测所有数据点为负的坏模型返回高性能值，这是由于数据不平衡。我们可以在*图4.4*中看到这种分析的扩展，它展示了真正例标签和预测标签中不同正数据点的比例。这里没有训练，数据点是随机生成的，以在*图4.4*的每个面板中产生指定的正数据点比例。然后，随机生成的概率被转换为标签，以便可以使用不同的性能指标与真正例进行比较。
- en: '*Figures 4.4* and *4.5* show different biases in the performance metrics of
    classification models. For example, the median precision of random predictions
    is equal to the fraction of true positive data points, while the median recall
    of random predictions is equal to the fraction of positive predicted labels. You
    can also check the behavior of other performance metrics in *Figures 4.4* and
    *4.5* for different fractions of true or predicted positives:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.4*和*图4.5*显示了分类模型性能指标中的不同偏差。例如，随机预测的中位精确率等于真正例数据点的比例，而随机预测的中位召回率等于预测标签中正标签的比例。你还可以检查*图4.4*和*图4.5*中其他性能指标在不同真正例或预测正例比例下的行为：'
- en: '![Figure 4.4 – Distribution of performance of 1,000 random binary predictions
    on 1,000 data points (part 1)](img/Image97668.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – 1,000个随机二元预测在1,000个数据点上的性能分布（第一部分）](img/Image97668.jpg)'
- en: Figure 4.4 – Distribution of performance of 1,000 random binary predictions
    on 1,000 data points (part 1)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – 1,000个随机二元预测在1,000个数据点上的性能分布（第一部分）
- en: '![Figure 4.5 – Distribution of performance of 1,000 random binary predictions
    on 1,000 data points (part 2)](img/B16369_04_05.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – 1,000个随机二元预测在1,000个数据点上的性能分布（第二部分）](img/B16369_04_05.jpg)'
- en: Figure 4.5 – Distribution of performance of 1,000 random binary predictions
    on 1,000 data points (part 2)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – 1,000个随机二元预测在1,000个数据点上的性能分布（第二部分）
- en: A combination of ROC-AUC and PR-AUC, or the use of MCC or balanced accuracy,
    are common approaches to have a low bias in performance assessment for classification
    models. But if you know your objectives, such as if you care more about precision
    than recall, then you can choose the performance metrics that would add the necessary
    information for decision-making. But avoid reporting 10 performance metrics for
    your models just for the sake of counting how many of them are better in one model
    versus another.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ROC-AUC和PR-AUC的组合，或使用MCC或平衡准确率，是降低分类模型性能评估偏差的常见方法。但如果你知道你的目标，例如如果你更关心精确率而不是召回率，那么你可以选择添加决策所需必要信息的性能指标。但避免仅仅为了计数模型中哪些性能指标更好而报告10个性能指标。
- en: Regression
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归
- en: 'You can assess the performance of your regression models using metrics that
    evaluate either the difference between the continuous predictions of your models
    and true values, such as **Root Mean Squared Error** (**RMSE**), or the agreement
    between the predictions and true values, such as the coefficient of determination
    R 2 (*Table 4.2*). Each of the metrics for regression model performance assessment
    has its assumptions, interpretation, and limitations. For example, R 2 doesn’t
    take into account data dimensionality (that is, the number of features, inputs,
    or independent variables). So, if you have a regression model with multiple features,
    you should use adjusted R 2 instead of R 2\. By adding new features, R 2 could
    increase but might not necessarily correspond to a better model. However, adjusted
    R 2 increases when the new inputs improve model performance more than expectation
    by chance. This is an important consideration, especially if you want to compare
    models with different numbers of inputs for the sample problem:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用评估模型连续预测值与真实值之间差异的度量，例如**均方根误差**（**RMSE**），或者评估预测值与真实值之间一致性的度量，如决定系数 R²（*表
    4.2*）。每个回归模型性能评估的度量都有其假设、解释和局限性。例如，R² 不考虑数据维度（即特征、输入或独立变量的数量）。因此，如果您有一个具有多个特征的回归模型，您应该使用调整后的
    R² 而不是 R²。通过添加新特征，R² 可能会增加，但并不一定代表更好的模型。然而，当新输入通过偶然机会比预期更好地提高模型性能时，调整后的 R² 会增加。这是一个重要的考虑因素，尤其是如果您想比较具有不同输入数量的样本问题的模型：
- en: '| **Metric** | **Python Function** | **Formula** | **Description** |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| **度量** | **Python 函数** | **公式** | **描述** |'
- en: '| **Root Mean Squared** **Error** (**RMSE**)**Mean Squared** **Error** (**MSE**)
    | `sklearn.metrics.mean_squared_error()` | MSE =  1 _ n  ∑ i=1 n (y i −  ˆ y  i) 2RMSE
    = √ _ MSE n: Number of data pointsy i: The true value of the data point, i ˆ y  i:
    The predicted value of the data point, i | Range: [0, ∞)Lower values mean higher
    performance |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| **均方根误差**（**RMSE**）**均方误差**（**MSE**） | `sklearn.metrics.mean_squared_error()`
    | MSE = 1/n ∑(i=1 to n) (y_i - ˆy_i)^2, RMSE = √(MSE/n) n: 数据点数量y_i: 数据点的真实值ˆy_i:
    数据点的预测值 | 范围：[0, ∞)，数值越低表示性能越高 |'
- en: '| **Mean Absolute** **Error** (**MAE**) | `sklearn.metrics.mean_absolute_error()`
    | MAE =  1 _ n  ∑ i=1 n &#124;y i − ˆ y i&#124; | Range: [0, ∞)Lower values mean
    higher performance |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| **平均绝对误差**（**MAE**） | `sklearn.metrics.mean_absolute_error()` | MAE = 1/n
    ∑(i=1 to n) |y_i - ˆy_i| | 范围：[0, ∞)，数值越低表示性能越高 |'
- en: '| Coefficient of determination (R 2) | `sklearn.metrics.r2_score()` | R 2 =
    1 −  ∑ i=1 n  (y i −  ˆ y  i) 2 _ ∑ i=1 n  (y i − y _) 2  ;y _ =  1 _ n  ∑ i=1 n y iy _:
    Mean of the true valuesn: Number of data pointsy i: The true value of the data
    point, i ˆ y  i: The predicted value of the data point, i | Range: [0, 1]Higher
    values mean higher performanceThe proportion of the dependent variable that can
    be explained by the independent variables |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 决定系数（R²） | `sklearn.metrics.r2_score()` | R² = 1 - ∑(i=1 to n) (y_i - ˆy_i)^2
    / ∑(i=1 to n) (y_i - y_)^2 ; y_ = 1/n ∑(i=1 to n) y_i y_: 真实值的平均值n: 数据点数量y_i:
    数据点的真实值ˆy_i: 数据点的预测值 | 范围：[0, 1]，数值越高表示性能越高，表示独立变量可以解释的因变量的比例 |'
- en: '| Adjusted R 2 | Use `sklearn.metrics.r2_score()` to calculate R 2, then calculate
    the adjusted version using its formula. | Adj R 2 = 1 −  (1 − R 2)(n − 1) ___________ n
    − m − 1 n: Number of data pointsm: Number of features | Adjusts to the number
    of featuresCould be greater than 1 or less than 0 if m is close to nHigher values
    mean higher performance |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 调整后的 R² | 使用 `sklearn.metrics.r2_score()` 计算原始 R²，然后使用其公式计算调整后的版本。 | Adj
    R² = 1 - (1 - R²)(n - 1) / (n - m - 1) n: 数据点数量m: 特征数量 | 调整以适应特征数量，如果 m 接近 n，则可能大于
    1 或小于 0。数值越高表示性能越高 |'
- en: Table 4.2 – Common metrics for assessing the performance of regression models
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.2 – 评估回归模型性能的常见度量
- en: 'Correlation coefficients are also used to report on the performance of regression
    models. Correlation coefficients use the predicted and true continuous values,
    or a transformation of those, and report values commonly between -1 and 1, with
    1 corresponding to an ideal prediction with 100% agreement and -1 with full disagreement
    (*Table 4.3*). Correlation coefficients also have their own assumptions and cannot
    be selected randomly for reporting on the performance of regression models. For
    example, Pearson correlation is a parametric test that assumes a linear relationship
    between predicted and true continuous values, which does not always hold. Alternatively,
    the Spearman and Kendall rank correlations are non-parametric without such assumptions
    behind the relationship of variables or the distribution of each variable in comparison.
    Both the Spearman and Kendall rank correlations rely on the rank of predicted
    and true outputs instead of their actual values:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 相关系数也用于报告回归模型的性能。相关系数使用预测值和真实连续值，或这些值的变换，并报告介于 -1 和 1 之间的值，其中 1 表示理想的预测，即 100%
    的一致性，-1 表示完全的不一致性（*表 4.3*）。相关系数也有其自身的假设，不能随机选择用于报告回归模型的性能。例如，Pearson 相关系数是一种参数化测试，假设预测值和真实连续值之间存在线性关系，这并不总是成立。另一方面，Spearman
    和 Kendall 排序相关系数是非参数化的，没有变量关系或每个变量的分布背后的假设。Spearman 和 Kendall 排序相关系数都依赖于预测值和真实输出的排名，而不是它们的实际值：
- en: '| **Correlation Coefficient** | **Python Function** | **Formula** | **Description**
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| **相关系数** | **Python 函数** | **公式** | **描述** |'
- en: '| Pearson correlation coefficient or Pearson’s *r* | `scipy.stats.pearsonr()`
    | r =  ∑ i=1 n  ( ˆ y  i −  ˆ y  _)(y i − y _)  ________________  √ _________________  ∑ i=1 n  ( ˆ y  i
    −  ˆ y  _) 2 (y i − y _) 2  n: Number of data pointsy i: The true value of the
    data point, iy _: Mean of the true values ˆ y  i: The predicted value of the data
    point, i ˆ y  _: Mean of the predicted values | ParametricLooks for a linear relationship
    between predictions and true valuesRange:[ − 1, 1] |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Pearson 相关系数或 Pearson 的 *r* | `scipy.stats.pearsonr()` | r = ∑ i=1 n (ˆy_i
    − ˆy_) (y_i − y_) __________________ √ ___________________ ∑ i=1 n (ˆy_i − ˆy_)^2
    (y_i − y_)^2 n: 数据点数量 y_i: 数据点的真实值 iy_: 真实值的平均值 ˆy_i: 数据点的预测值 i ˆy_: 预测值的平均值 |
    参数化 寻找预测值和真实值之间的线性关系 范围：[ − 1, 1] |'
- en: '| Spearman’s rank correlation coefficient or Spearman correlation coefficient
    | `scipy.stats.spearmanr()` | ρ = 1 −  6∑ i=1 n  d i 2 _ n(n 2 − 1) n: Number
    of data pointsd i: The difference between the rank of the data point, i, among
    true values and predicted values | Non-parametricLooks for a monotonic relationship
    between predictions and true valuesRange:[ − 1, 1] |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Spearman 排序相关系数或 Spearman 相关系数 | `scipy.stats.spearmanr()` | ρ = 1 − 6∑ i=1
    n d_i^2 ___________________ n(n^2 − 1) n: 数据点数量 d_i: 真实值和预测值中数据点 i 排名的差异 | 非参数化
    寻找预测值和真实值之间的单调关系 范围：[ − 1, 1] |'
- en: '| Kendall rank correlation coefficient or Kendall’s τ coefficient | `scipy.stats.kendalltau()`
    | τ =  C − D __________________  √ ___________________  (C + D + T)(C + D + c)  C:
    Number of concordant pairs (for example, y i > y j and  ˆ y  i >  ˆ y  j; or y i
    < y j and  ˆ y  i <  ˆ y  j)D: Number of discordant pairs (for example, y i >
    y j and  ˆ y  i <  ˆ y  j; or y i < y j and  ˆ y  i >  ˆ y  j)T: Number of ties
    only in predicted valuesU: Number of ties only in true values | Non-parametricLooks
    for a monotonic relationship between predictions and true valuesRange:[ − 1, 1]
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Kendall 排序相关系数或 Kendall 的 τ 系数 | `scipy.stats.kendalltau()` | τ = C − D ____________________
    √ _____________________ (C + D + T)(C + D + c) C: 一致对数（例如，y_i > y_j 且 ˆy_i > ˆy_j；或
    y_i < y_j 且 ˆy_i < ˆy_j）D: 不一致对数（例如，y_i > y_j 且 ˆy_i < ˆy_j；或 y_i < y_j 且 ˆy_i
    > ˆy_j）T: 仅在预测值中存在相同排名的情况 U: 仅在真实值中存在相同排名的情况 | 非参数化 寻找预测值和真实值之间的单调关系 范围：[ − 1,
    1] |'
- en: Table 4.3 – Common correlation coefficients used for assessing the performance
    of regression models
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.3 – 评估回归模型性能的常用相关系数
- en: Clustering
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类
- en: 'Clustering is an unsupervised learning approach to identify groupings of data
    points using their feature values. However, to assess the performance of a clustering
    model, we need to have a dataset or example data points with available true labels.
    We don’t use these labels when training the clustering model, as in supervised
    learning; instead, we use them to assess how well similar data points are grouped
    and separated from dissimilar data points. You can find some of the common metrics
    for assessing the performance of clustering models in *Table 4.4*. These metrics
    do not inform you about the quality of the clustering. For example, homogeneity
    tells you if the data points that are clustered together are similar to each other
    while completeness informs you if similar data points in your dataset are clustered
    together. There are also metrics such as V-measure and adjusted mutual information
    that try to assess both qualities at the same time:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种无监督学习方法，用于通过数据点的特征值来识别数据点的分组。然而，为了评估聚类模型的性能，我们需要有一个数据集或具有可用真实标签的示例数据点。在监督学习中，我们不使用这些标签来训练聚类模型；相反，我们使用它们来评估相似数据点被分组以及与不相似数据点分离的程度。你可以在*表4.4*中找到一些用于评估聚类模型性能的常见指标。这些指标不会告诉你聚类的质量。例如，同质性告诉你聚在一起的数据点是否彼此相似，而完整性告诉你数据集中相似的数据点是否被聚在一起。还有一些指标，如V度量、调整后的互信息，试图同时评估这两个质量：
- en: '| **Metric** | **Python Function** | **Formula** | **Description** |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| **指标** | **Python函数** | **公式** | **描述** |'
- en: '| Homogeneity | `sklearn.metrics.homogeneity_score()` | Formula (*1*) provided
    in Rosenberg et al., EMNLP-CoNLL 2007 | Measures how many data points within the
    same clusters are similar to each otherRange: [0, 1]Higher values mean higher
    performance |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 同质性 | `sklearn.metrics.homogeneity_score()` | 来自Rosenberg等人，EMNLP-CoNLL 2007提供的公式(*1*)
    | 衡量同一聚类内的数据点之间有多少是彼此相似的范围：[0, 1]值越高表示性能越好 |'
- en: '| Completeness | `sklearn.metrics.completeness_score()` | Formula (*2*) provided
    in Rosenberg et al., EMNLP-CoNLL 2007 | Measures how similar the data points that
    are clustered together areRange: [0, 1]Higher values mean higher performance |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 完整性 | `sklearn.metrics.completeness_score()` | 来自Rosenberg等人，EMNLP-CoNLL
    2007提供的公式(*2*) | 衡量聚在一起的数据点之间的相似程度范围：[0, 1]值越高表示性能越好 |'
- en: '| V-measure or normalized mutual information score | `sklearn.metrics.v_measure_score()`
    | v =  (1 + β) × h × c ___________ (β × h + c) h: Homogeneityc: Completenessβ:
    Ratio of weight attributed to homogeneity versus completeness | Measures both
    homogeneity and completeness at the same timeRange: [0, 1]Higher values mean higher
    performance |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| V度量或归一化互信息得分 | `sklearn.metrics.v_measure_score()` | v = (1 + β) × h × c
    / [(β × h + c) h: Homogeneity c: Completeness β: 同质性与完整性所赋予的权重比率] | 同时衡量同质性和完整性范围：[0,
    1]值越高表示性能越好 |'
- en: '| Mutual information | `sklearn.metrics.mutual_info_score()` | MI(U, V) = ∑ i=1 &#124;U&#124; ∑ j=1 &#124;V&#124;  &#124;U i
    ∩ V j&#124; _ N  log  &#124;U i ∩ V j&#124; _ &#124;U i&#124;&#124;V j&#124; 
    | Range:[0, 1]Higher values mean higher performance |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 互信息 | `sklearn.metrics.mutual_info_score()` | MI(U, V) = ∑ i=1 |U| ∑ j=1
    |V| |U_i ∩ V_j| _ N log |U_i ∩ V_j| _ |U_i| |V_j| | | 范围：[0, 1]值越高表示性能越好 |'
- en: '| Adjusted mutual information | `sklearn.metrics.adjusted_mutual_info_score()`
    | AMI(U, V)=  [MI(U, V) − E(MI(U, V))]  ________________________   [avg(H(U),
    H(V)) − E(MI(U, V))]  | Range:[0, 1]Higher values mean higher performance |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 调整后的互信息 | `sklearn.metrics.adjusted_mutual_info_score()` | AMI(U, V)= [MI(U,
    V) − E(MI(U, V))] / [avg(H(U), H(V)) − E(MI(U, V))] | 范围：[0, 1]值越高表示性能越好 |'
- en: Table 4.4 – Common metrics for assessing the performance of clustering models
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.4 – 评估聚类模型性能的常见指标
- en: In this section, we discussed the different performance measures for assessing
    the performance of machine learning models. But there are other important aspects
    of performance assessment to consider, such as data visualization, which we will
    discuss next.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了用于评估机器学习模型性能的不同性能度量。但还有其他重要的性能评估方面需要考虑，例如数据可视化，我们将在下一节讨论。
- en: Visualization for performance assessment
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于性能评估的可视化
- en: Visualization is an important tool that helps us not only understand the characteristics
    of our data for modeling but also better assess the performance of our models.
    Visualization could provide complementary information to the aforementioned model
    performance metrics.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化是一个重要的工具，它不仅帮助我们理解建模数据的特点，还能更好地评估我们模型的性能。可视化可以为上述模型性能指标提供补充信息。
- en: Summary metrics are not enough
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 汇总指标是不够的
- en: 'There are summary statistics such as ROC-AUC and PR-AUC that provide a one-number
    summary of their corresponding curves for assessing the performance of classification
    models. Although these summaries are more reliable than many other metrics such
    as accuracy, they do not completely capture the characteristics of their corresponding
    curves. For example, two different models with different ROC curves can have the
    same or very close ROC-AUCs (*Figure 4**.6*):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些汇总统计量，如ROC-AUC和PR-AUC，提供了对应曲线的一个数值总结，用于评估分类模型的性能。尽管这些汇总比许多其他指标（如准确率）更可靠，但它们并不能完全捕捉其对应曲线的特征。例如，具有不同ROC曲线的两个不同模型可以具有相同的或非常接近的ROC-AUC值（*图4**.6*）：
- en: '![Figure 4.6 – Comparison of two arbitrary models with the same ROC-AUCs and
    different ROC curves](img/B16369_04_06.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图4.6 – 比较具有相同ROC-AUC值和不同ROC曲线的两个任意模型](img/B16369_04_06.jpg)'
- en: Figure 4.6 – Comparison of two arbitrary models with the same ROC-AUCs and different
    ROC curves
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 – 比较具有相同ROC-AUC值和不同ROC曲线的两个任意模型
- en: Comparing ROC-AUCs alone could result in deciding the equivalence of these models.
    However, they have different ROC curves and in most applications, a red curve
    is preferred over a blue one as it results in a higher true positive rate for
    low false positive rates such as *FPR*1.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 仅比较ROC-AUC值可能会导致判断这些模型的等效性。然而，它们的ROC曲线不同，在大多数应用中，红色曲线比蓝色曲线更受欢迎，因为它在低假阳性率（如*FPR*1）的情况下会产生更高的真正阳性率。
- en: Visualizations could be misleading
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化可能会产生误导
- en: 'Using the proper visualization technique for your results is the key to analyzing
    the results of your models and reporting their performances. Plotting your data
    without having the model objective in mind could be misleading. For example, you
    might see time series plots such as the one shown in *Figure 4**.7* that overlay
    predictions and real values over time in many blog posts. For such time series
    models, we want predictions and real values to be as close to each other as possible
    for each time point. Although the lines might seem to agree with each other in
    *Figure 4**.7*, there is a two-time unit delay in predictions shown in orange
    compared to the true values shown in blue. This lag in predictions could have
    serious consequences in many applications such as stock price prediction:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用适合您结果的正确可视化技术是分析模型结果和报告其性能的关键。没有考虑模型目标就绘制数据可能会导致误导。例如，你可能会看到时间序列图，如*图4**.7*所示，在许多博客文章中，预测值和真实值随时间叠加。对于此类时间序列模型，我们希望每个时间点的预测值和真实值尽可能接近。尽管*图4**.7*中的线条似乎彼此一致，但与蓝色显示的真实值相比，橙色显示的预测值存在两个时间单位的延迟。这种预测延迟在许多应用（如股票价格预测）中可能产生严重后果：
- en: "![Figure 4.7 – Laying two time series diagrams on top of each other is misleading\
    \ – the orange and blue curves represent predictions and true values for arbitrary\
    \ time series da\uFEFFta](img/B16369_04_07.jpg)"
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图4.7 – 将两个时间序列图叠加在一起是误导性的 – 橙色和蓝色曲线代表任意时间序列数据的预测值和真实值](img/B16369_04_07.jpg)'
- en: Figure 4.7 – Laying two time series diagrams on top of each other is misleading
    – the orange and blue curves represent predictions and true values for arbitrary
    time series data
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 – 将两个时间序列图叠加在一起是误导性的 – 橙色和蓝色曲线代表任意时间序列数据的预测值和真实值
- en: Don’t interpret your plots as you wish
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不要随意解释你的图表
- en: 'Each visualization has its assumptions and right way of interpretation. For
    example, if you want to compare the numerical values of data points in a 2D plot,
    you need to pay attention to the units of the *x* and *y* axes. Or when we use
    **t-distributed Stochastic Neighbor Embedding** (**t-SNE**), a dimensionality
    reduction method designed to help in visualizing high dimensional data in low
    dimensional space, we have to remind ourselves that large distances between data
    points and densities of each group are not representative of the distances and
    densities in the original high-dimensional space (*Figure 4**.8*):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 每个可视化都有其假设和正确的解释方式。例如，如果你想比较二维图中数据点的数值，你需要注意*x*轴和*y*轴的单位。或者当我们使用**t分布随机邻域嵌入**（**t-SNE**），这是一种旨在帮助在低维空间中可视化高维数据的降维方法时，我们必须提醒自己，数据点之间的大距离和每个组的密度并不代表原始高维空间中的距离和密度（*图4**.8*）：
- en: '![Figure 4.8 – Schematic t-SNE plots showing (A) three groups of data points
    with different distances and (B) two groups with different densities in two dimensions](img/B16369_04_08.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8 – 示意 t-SNE 折线图显示（A）具有不同距离的三组数据点以及（B）在二维空间中具有不同密度的两组数据点](img/B16369_04_08.jpg)'
- en: Figure 4.8 – Schematic t-SNE plots showing (A) three groups of data points with
    different distances and (B) two groups with different densities in two dimensions
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – 示意 t-SNE 折线图显示（A）具有不同距离的三组数据点以及（B）在二维空间中具有不同密度的两组数据点
- en: You can use different performance measures to assess if your models are trained
    well and generalizable to new data points, which is the next topic in this chapter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用不同的性能指标来评估你的模型是否训练良好并且可以推广到新的数据点，这是本章的下一个主题。
- en: Bias and variance diagnosis
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差和方差诊断
- en: 'We aim to have a model with high performance, or low error, in the training
    set (that is, a low bias model) while keeping the performance high, or error low,
    for new data points (that is, a low variance model). As we don’t have access to
    unseen new data points, we must use validation and test sets to assess the variance
    or generalizability of our models. Model complexity is one of the important factors
    in determining the bias and variance of machine learning models. By increasing
    complexity, we let a model learn more complex patterns in training data that could
    reduce training errors or model bias (*Figure 4**.9*):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是在训练集（即低偏差模型）中实现高性能或低误差，同时保持对新数据点的性能或误差保持在高水平（即低方差模型）。由于我们没有访问未见过的新的数据点，我们必须使用验证集和测试集来评估我们模型的方差或泛化能力。模型复杂性是确定机器学习模型偏差和方差的重要因素之一。通过增加复杂性，我们让模型在训练数据中学习更复杂的模式，这可能会减少训练误差或模型偏差（*图
    4**.9*）：
- en: '![Figure 4.9 – Error versus model complexity for (A) high bias, (B) high variance,
    and (C, D) two different cases of low bias and low variance models](img/B16369_04_09.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9 – 对于（A）高偏差、（B）高方差以及（C, D）两种低偏差和低方差模型的情况，误差与模型复杂度的关系](img/B16369_04_09.jpg)'
- en: Figure 4.9 – Error versus model complexity for (A) high bias, (B) high variance,
    and (C, D) two different cases of low bias and low variance models
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – 对于（A）高偏差、（B）高方差以及（C, D）两种低偏差和低方差模型的情况，误差与模型复杂度的关系
- en: This decrease in error helps build a better model, even for new data points.
    However, this trend changes after a point, and higher complexities could cause
    overfitting or higher variance and lower performance in validation and test sets
    compared to the training set (*Figure 4**.9*). Assessing bias and variance concerning
    parameters such as model complexity or dataset size could help us identify opportunities
    for model performance improvements in training, validation, and test sets.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这种误差的降低有助于构建更好的模型，即使是对于新的数据点。然而，这种趋势在某个点之后会发生变化，更高的复杂性可能导致过拟合或验证集和测试集相对于训练集有更高的方差和更低的性能（*图
    4**.9*）。评估与模型复杂性或数据集大小等参数相关的偏差和方差可以帮助我们识别在训练、验证和测试集中提高模型性能的机会。
- en: Four of the possible dependencies of model error in training and validation
    sets to model complexity are shown in *Figure 4**.9*. Although the validation
    error is usually higher than the training error, you might experience a lower
    error in validation sets because of the data points you have in your training
    and validation sets. For example, a multiclass classifier could have a lower error
    in the validation set because of being better at predicting classes that form
    the majority of the data points in the validation set. In such cases, you need
    to investigate the distribution of data points in the training and validation
    sets before reporting performance assessments on training and validation datasets
    and deciding which model to select for production.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4**.9* 展示了训练和验证集中模型误差与模型复杂度之间可能的四种依赖关系。尽管验证误差通常高于训练误差，但你可能会因为训练和验证集中存在的数据点而经历较低的验证误差。例如，一个多类分类器可能因为更擅长预测验证集中数据点中占多数的类别而具有较低的验证误差。在这种情况下，在报告训练和验证数据集的性能评估并决定选择哪个模型用于生产之前，你需要调查训练和验证集中数据点的分布。'
- en: 'Let’s practice a bias and variance analysis. You can find the results of training
    random forest models with different maximum depths on the breast cancer dataset
    from `scikit-learn` (*Figure 4**.10*). The breast cancer data from `scikit-learn`
    is used for training and validating model performance, with 30% of the data randomly
    separated as the validation set and the rest kept as the training set. By increasing
    the maximum depth of the random forest models, log-loss error in the training
    set decreases while balanced accuracy as a measure of model performance increases.
    Validation errors also decrease up to a maximum depth of three and start increasing
    after that as a sign of overfitting. Although error decreases after the maximum
    depth of three, balanced accuracy can still be increased by increasing the maximum
    depth to four and five. The reason is the difference in the definition of log-loss
    based on the probability of predictions and the balanced accuracy on predicted
    labels:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们练习一下偏差和方差分析。你可以在 `scikit-learn` 的乳腺癌数据集上找到使用不同最大深度的随机森林模型训练的结果（*图 4.10*）。`scikit-learn`
    的乳腺癌数据用于训练和验证模型性能，其中 30% 的数据随机分离作为验证集，其余的保留为训练集。通过增加随机森林模型的最大深度，训练集的对数损失错误减少，而作为模型性能指标的平衡准确率增加。验证错误也减少到最大深度为三，之后开始增加，这是过拟合的迹象。尽管在最大深度为三之后错误减少，但通过将最大深度增加到四和五，平衡准确率仍然可以增加。原因是基于预测概率的对数损失定义与基于预测标签的平衡准确率定义之间的差异：
- en: '![Figure 4.10 – Balanced accuracy (top) and log-loss (bottom) in training and
    validation sets separated from the breast cancer dataset of scikit-learn for a
    random forest model](img/B16369_04_10.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10 – 从 scikit-learn 的乳腺癌数据集中分离的训练集和验证集的平衡准确率（顶部）和对数损失（底部）](img/B16369_04_10.jpg)'
- en: Figure 4.10 – Balanced accuracy (top) and log-loss (bottom) in training and
    validation sets separated from the breast cancer dataset of scikit-learn for a
    random forest model
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – 从 scikit-learn 的乳腺癌数据集中分离的训练集和验证集的平衡准确率（顶部）和对数损失（底部）
- en: 'Here is the code for the results shown in *Figure 4**.10*. First, we must import
    the necessary Python libraries and load the breast cancer dataset:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 *图 4.10* 中显示结果的代码。首先，我们必须导入必要的 Python 库并加载乳腺癌数据集：
- en: '[PRE1]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we must split the data into train and test sets and train multiple random
    forest models with different maximum depths allowed for their decision trees:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须将数据分成训练集和测试集，并使用不同最大深度的随机森林模型进行训练：
- en: '[PRE2]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that you’ve learned about the concepts of bias and variance, we will introduce
    different techniques that you can use to validate your models.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了偏差和方差的概念，我们将介绍不同的技术，你可以使用这些技术来验证你的模型。
- en: Model validation strategy
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型验证策略
- en: 'To validate our models, we can use separate datasets or split the dataset we
    have into training and validation sets using different techniques, as explained
    in *Table 4.5* and illustrated in *Figure 4**.11*. In cross-validation strategies,
    we split the data into different subsets, then the performance score or error
    for each subset, since the validation set is calculated using the predictions
    of the model trained on the rest of the data. Then, we can use the mean of the
    performance across the subsets as the cross-validation performance:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的模型，我们可以使用单独的数据集，或者使用不同的技术将我们拥有的数据集分成训练集和验证集，如 *表 4.5* 中所述，并在 *图 4.11*
    中展示。在交叉验证策略中，我们将数据分成不同的子集，然后计算每个子集的性能分数或错误，因为验证集是使用其余数据训练的模型的预测来计算的。然后，我们可以使用子集间的性能平均值作为交叉验证性能：
- en: '![Figure 4.11 – Techniques for separating the validation and training sets
    within one dataset](img/B16369_04_11.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.11 – 在一个数据集中分离验证集和训练集的技术](img/B16369_04_11.jpg)'
- en: Figure 4.11 – Techniques for separating the validation and training sets within
    one dataset
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – 在一个数据集中分离验证集和训练集的技术
- en: Each of these validation techniques has its advantages and limitations. Using
    cross-validation techniques instead of hold-out validation has the benefit of
    covering all or the majority of the data in at least one validation subset. Stratified
    k-fold **cross-validation** (**CV**) is also a better choice compared to k-fold
    CV or leave-one-out CV as it keeps the same balance across the validation subsets
    as in the whole dataset.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这些验证技术各有其优点和局限性。使用交叉验证技术而不是保留法验证的好处是，至少在一个验证子集中涵盖了所有或大部分数据。与k折交叉验证或留一法交叉验证相比，分层k折交叉验证（**CV**）也是一个更好的选择，因为它保持了与整个数据集相同的平衡。
- en: 'The classification or regression hold-out or CV methods don’t work for time
    series data. As the order of data points is important in time series data, shuffling
    the data or randomly selecting data points is not suitable in the process of training
    and validation subset selection. Randomly selecting data points for validation
    and training sets results in models trained on some future data points to predict
    the outcome in the past, which is not the intention of time series models. Rolling
    or time series CV is an appropriate validation technique for time series models
    as it rolls the validation set over time instead of randomly selecting the data
    point (*Table 4.5*):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 分类或回归的保留法或交叉验证方法不适用于时间序列数据。由于时间序列数据中数据点的顺序很重要，因此在训练和验证子集选择过程中对数据进行洗牌或随机选择是不合适的。随机选择数据点用于验证和训练集会导致在未来的某些数据点上训练模型来预测过去的结果，这与时间序列模型的目的不符。滚动或时间序列交叉验证是时间序列模型的一个合适的验证技术，因为它随着时间的推移滚动验证集而不是随机选择数据点（*表4.5*）：
- en: '| **Validation Method** | **Python Function** | **Description** |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| **验证方法** | **Python 函数** | **描述** |'
- en: '| Hold-out validation | `sklearn.model_selection.train_test_split()` | This
    splits all the data into one training and one validation set. 20-40% of the data
    commonly gets selected as a validation set but this percentage could be lower
    for large datasets. |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 保留法验证 | `sklearn.model_selection.train_test_split()` | 这会将所有数据分为一个训练集和一个验证集。通常选择20-40%的数据作为验证集，但对于大型数据集，这个百分比可能更低。
    |'
- en: '| k-fold cross-validation | `sklearn.model_selection.KFold()` | This method
    splits the data into *k* different subsets and uses each as a validation set and
    the remaining data points as a training set. |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| k折交叉验证 | `sklearn.model_selection.KFold()` | 此方法将数据分为*k*个不同的子集，并使用每个子集作为验证集，剩余的数据点作为训练集。
    |'
- en: '| Stratified k-fold cross-validation | `sklearn.model_selection.StratifiedKFold()`
    | This is similar to k-fold CV but preserves the percentage of samples for each
    class, as in the whole dataset, in each of the *k* subsets. |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 分层k折交叉验证 | `sklearn.model_selection.StratifiedKFold()` | 这与k折交叉验证类似，但保留了每个类别在*k*个子集中的样本百分比，正如在整个数据集中一样。
    |'
- en: '| **Leave-p-out** **cross-validation** (**LOCV**) | `sklearn.model_selection.LeavePOut()`
    | This is similar to k-fold CV, with each subset having *p* data points instead
    of splitting the dataset into *k* subsets. |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| **留出-p个数据点的交叉验证**（**LOCV**） | `sklearn.model_selection.LeavePOut()` | 这与k折交叉验证类似，每个子集有*p*个数据点，而不是将数据集分为*k*个子集。
    |'
- en: '| **Leave-one-out cross-validation** (**LOOCV**) | `sklearn.model_selection.LeaveOneOut()`
    | This works exactly as k-fold CV, with *k* being equal to the total number of
    data points. Each validation subset has one data point that uses LOOCV. |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| **留一法交叉验证**（**LOOCV**） | `sklearn.model_selection.LeaveOneOut()` | 这与k折交叉验证完全相同，其中*k*等于数据点的总数。每个验证子集有一个数据点使用LOOCV。
    |'
- en: '| Monte Carlo or random permutation cross-validation | `sklearn.model_selection.ShuffleSplit()`
    | This splits the data randomly into a training and a validation set, similar
    to hold-out validation, and repeats this process many times. More iterations result
    in a better assessment of performance, although it increases the computational
    cost of validation. |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 随机蒙特卡洛或随机排列交叉验证 | `sklearn.model_selection.ShuffleSplit()` | 这会将数据随机分为训练集和验证集，类似于保留法验证，并重复此过程多次。更多的迭代次数会导致对性能的更好评估，尽管它增加了验证的计算成本。
    |'
- en: '| Rolling or time-based cross-validation | `sklearn.model_selection.TimeSeriesSplit()`
    | A small subset of data gets selected as the training set and a smaller subset
    gets selected as the validation set. The validation set gets shifted in time and
    the data points that were previously considered for validation get added to the
    training set. |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 滚动或基于时间的交叉验证 | `sklearn.model_selection.TimeSeriesSplit()` | 选择一小部分数据作为训练集，更小的一部分数据作为验证集。验证集在时间上移动，之前用于验证的数据点被添加到训练集中。|'
- en: Table 4.5 – Common validation techniques that use one dataset
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.5 – 使用一个数据集的常见验证技术
- en: Here is the Python implementation of hold-out, k-fold CV, and stratified k-fold
    CV to help you start using these methods in your projects.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是Python实现保留法、k折交叉验证和分层k折交叉验证的示例，以帮助你在项目中开始使用这些方法。
- en: 'First, we must import the necessary libraries, load the breast cancer dataset,
    and initialize a random forest model:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须导入必要的库，加载乳腺癌数据集，并初始化一个随机森林模型：
- en: '[PRE3]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, we must train and validate different random forest models using each
    validation technique:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须使用每种验证技术训练和验证不同的随机森林模型：
- en: '[PRE4]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Error analysis is another technique you can benefit from when seeking to develop
    reliable machine learning models, which we will introduce next.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 错误分析是你在寻求开发可靠的机器学习模型时可以受益的另一种技术，我们将在下面介绍。
- en: Error analysis
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 错误分析
- en: You can use error analysis to find common characteristics between data points
    with incorrectly predicted outputs. For example, the majority of images that are
    misclassified in image classification models might have darker backgrounds, or
    a disease diagnostic model might have lower performance for men compared to women.
    Although manually investigating the data points with incorrect predictions could
    be insightful, this process could cost you a lot of time. Instead, you can try
    to reduce the cost programmatically.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用错误分析来找出具有错误预测输出的数据点之间的共同特征。例如，在图像分类模型中被错误分类的大多数图像可能背景较暗，或者疾病诊断模型可能男性比女性的性能低。虽然手动调查错误预测的数据点可能会有所启发，但这个过程可能会花费你大量时间。相反，你可以尝试以编程方式减少成本。
- en: Here, we want to practice with a simple case of error analysis in which the
    number of misclassified data points from each class is counted for a random forest
    model that’s been trained and validated using a 5-fold CV. For error analysis,
    only predictions for validation subsets are used.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们想通过一个简单的错误分析案例进行练习，即计算使用5折交叉验证训练和验证的随机森林模型中每个类别的错误分类数据点的数量。对于错误分析，仅使用验证子集的预测。
- en: 'First, we must import the necessary Python libraries and load the wine dataset:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须导入必要的Python库并加载葡萄酒数据集：
- en: '[PRE5]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we must initialize a random forest model and 5-fold CV object:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须初始化一个随机森林模型和5折交叉验证对象：
- en: '[PRE6]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, for each fold, we must train a random forest model using all the data,
    excluding that fold, and validate the model on the chunk of data considered in
    that fold:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于每个折，我们必须使用除该折之外的所有数据训练一个随机森林模型，并在该折考虑的数据块上验证模型：
- en: '[PRE7]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This analysis shows that class 1 has nine misclassified data points, while classes
    2 and 0 have only three and two misclassified examples, respectively. This simple
    example helps you start practicing with error analysis. But error analysis is
    not only about identifying misclassification count per class. You can also identify
    patterns in feature values for misclassified examples by comparing feature values
    between misclassified data points and the whole dataset.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这项分析表明，类别1有九个被错误分类的数据点，而类别2和0分别只有三个和两个错误分类的例子。这个简单的例子可以帮助你开始练习错误分析。但错误分析不仅仅是识别每个类别的错误分类数量。你还可以通过比较错误分类数据点和整个数据集的特征值来识别错误分类示例的特征值中的模式。
- en: There are other important factors, such as computational cost and time, that
    also need to be considered when developing machine learning models. Here, we will
    briefly talk about this important topic, but the details are beyond the scope
    of this book.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发机器学习模型时，还需要考虑其他重要因素，例如计算成本和时间。在这里，我们将简要讨论这个重要话题，但详细内容超出了本书的范围。
- en: Beyond performance
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不仅仅是性能
- en: 'Paying any price for improving the performance of machine learning models is
    not the objective of modeling as part of bigger pipelines at the industrial level.
    Increasing the performance of models by a tenth of a percent could help you win
    machine learning competitions or publish papers by beating state-of-the-art models.
    But not all improvements result in models worth deploying to production. An example
    of such efforts, which has been common in machine learning competitions, is model
    stacking. Model stacking is about using the output of multiple models to train
    a secondary model, which could increase the cost of inference by orders of magnitude.
    Python’s implementation of stacking of the logistic regression, k-nearest neighbor,
    random forest, support vector machine, and XGBoost classification models on the
    breast cancer dataset from `scikit-learn` is shown here. A secondary logistic
    regression model uses predictions of each of these primary models as input to
    come up with the final prediction of the stacked model:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在工业级更大管道中建模以提高机器学习模型的性能并不是目的。通过提高模型性能的十分之一可能有助于你在机器学习竞赛中获胜或通过击败最先进的模型来发表论文。但并非所有改进都能导致值得部署到生产中的模型。在机器学习竞赛中常见的此类努力的例子是模型堆叠。模型堆叠是关于使用多个模型的输出来训练一个次级模型，这可能会将推理成本提高数个数量级。这里展示了Python对`scikit-learn`中的乳腺癌数据集上逻辑回归、k-最近邻、随机森林、支持向量机和XGBoost分类模型进行堆叠的实现。一个次级逻辑回归模型使用每个这些主要模型的预测作为输入，以得出堆叠模型的最终预测：
- en: '[PRE8]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this example, the performance of the stacked model is less than 1% better
    than the best individual model, while the inference time could be more than 20
    times higher, depending on the hardware and software configurations you have.
    Although inference time could be less important, such as in the case of disease
    diagnosis or scientific discoveries, it could be of critical importance if your
    model needs to provide the output in real time, such as in recommending products
    to consumers. So, you need to consider other factors, such as inference or prediction
    time, when you’re deciding to bring a model into production or planning for new
    expensive computational experiments or data collection.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，堆叠模型的性能比最佳单个模型低不到1%，而推理时间可能会比你的硬件和软件配置高20倍。尽管推理时间可能不太重要，例如在疾病诊断或科学发现的情况下，但如果你的模型需要实时提供输出，例如在向消费者推荐产品时，它可能至关重要。因此，当你决定将模型投入生产或计划新的昂贵计算实验或数据收集时，你需要考虑其他因素，例如推理或预测时间。
- en: Although inference time or other factors need to be considered in your model
    building and selection, it doesn’t mean that you cannot use complex models for
    real-time output generation. Depending on the application and your budget, you
    can use better configurations, for example, on your cloud-based system, to eliminate
    the issues that arise due to higher performance but slower models.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在构建和选择模型时需要考虑推理时间或其他因素，但这并不意味着你不能使用复杂模型进行实时输出生成。根据应用和你的预算，你可以使用更好的配置，例如在你的基于云的系统上，以消除由于性能更高但速度较慢的模型而产生的问题。
- en: Summary
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about different performance and error metrics for
    supervised and unsupervised learning models. We discussed the limitations of each
    metric and the right way of interpreting them. We also reviewed bias and variance
    analysis and different validation and cross-validation techniques for assessing
    the generalizability of models. We also presented error analysis as an approach
    for detecting the components of a model that contribute to model overfitting.
    We went through Python code examples for these topics to help you practice with
    them and be able to quickly use them in your projects.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了监督学习和无监督学习模型的性能和误差指标。我们讨论了每个指标的限制以及正确解释它们的方法。我们还回顾了偏差和方差分析以及用于评估模型泛化能力的不同验证和交叉验证技术。我们还介绍了错误分析作为检测模型中导致模型过拟合的组件的方法。我们通过这些主题的Python代码示例来帮助你练习，并能够快速在你的项目中使用它们。
- en: In the next chapter, we will review techniques to improve the generalizability
    of machine learning models, such as synthetic data addition to training data,
    removing data inconsistencies, and regularization methods.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将回顾提高机器学习模型泛化性的技术，例如向训练数据添加合成数据、去除数据不一致性和正则化方法。
- en: Questions
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: A classifier is designed to identify if patients of a clinic need to go through
    the rest of the diagnostic steps after the first round of testing. What classification
    metric would be more or less appropriate? Why?
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个分类器被设计用来确定诊所的患者在第一轮测试后是否需要继续进行诊断步骤。哪种分类度量会更合适或不那么合适？为什么？
- en: A classifier is designed to assess the risk of investment for different investment
    options, for a specific amount of money, and is going to be used to suggest investment
    opportunities to your clients. What classification metric would be more or less
    appropriate? Why?
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个分类器被设计来评估不同投资选项的投资风险，针对特定金额，并将被用来向您的客户提供投资机会。哪种分类度量会更合适或不那么合适？为什么？
- en: If the calculated ROC-AUCs of two binary classification models on the same validation
    set are the same, does it mean that the models are the same?
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果两个二元分类模型在相同的验证集上的计算ROC-AUC值相同，这意味着模型是相同的吗？
- en: If model A has a lower log-loss compared to model B on the same test set, does
    it always mean that the MCC of model A is also higher than model B?
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果模型A在相同的测试集上比模型B具有更低的log-loss，这总是意味着模型A的MCC也高于模型B吗？
- en: If model A has a higher R 2 on the same number of data points compared to model
    B, could we claim that model A is better than model B? How does the number of
    features affect our comparison between the two models?
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果模型A在相同数量的数据点上比模型B具有更高的R²值，我们能否声称模型A比模型B更好？特征数量是如何影响我们对两个模型之间比较的？
- en: If model A has higher performance than model B, does it mean that choosing model
    A is the right one to bring into production?
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果模型A的性能优于模型B，这意味着选择模型A是将其投入生产的正确选择吗？
- en: References
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Rosenberg, Andrew, and Julia Hirschberg. *V-measure: A conditional entropy-based
    external cluster evaluation measure*. Proceedings of the 2007 joint conference
    on empirical methods in natural language processing and computational natural
    language learning (EMNLP-CoNLL). 2007.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rosenberg, Andrew，和Julia Hirschberg. *V-measure: 一种基于条件熵的外部聚类评估度量*. 2007年实证自然语言处理和计算自然语言学习联合会议（EMNLP-CoNLL）论文集。'
- en: 'Vinh, Nguyen Xuan, Julien Epps, and James Bailey. *Information theoretic measures
    for clusterings comparison: is a correction for chance necessary?* Proceedings
    of the 26th annual international conference on machine learning. 2009.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinh, Nguyen Xuan，Julien Epps，和James Bailey. *聚类比较的信息论度量：是否需要校正偶然性？* 第26届国际机器学习年度会议论文集。2009年。
- en: 'Andrew Ng, *Stanford CS229: Machine Learning Course*, Autumn 2018.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrew Ng, *斯坦福CS229：机器学习课程*，2018年秋季。
- en: Van der Maaten, Laurens, and Geoffrey Hinton. *Visualizing data using t-SNE*.
    Journal of machine learning research 9.11 (2008).
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van der Maaten, Laurens，和Geoffrey Hinton. *使用t-SNE可视化数据*. 机器学习研究杂志第9卷第11期（2008年）。
- en: 'McInnes, Leland, John Healy, and James Melville. *Umap: Uniform manifold approximation
    and projection for dimension reduction*. arXiv preprint arXiv:1802.03426 (2018).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McInnes, Leland，John Healy，和James Melville. *Umap：统一流形近似和投影用于降维*. arXiv预印本arXiv:1802.03426（2018年）。
