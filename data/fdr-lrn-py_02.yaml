- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: What Is Federated Learning?
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是联邦学习？
- en: In [*Chapter 1*](B18369_01.xhtml#_idTextAnchor017), *Challenges in Big Data
    and Traditional AI*, we examined how shifting tides in big data and **machine
    learning** (**ML**) have set the stage for a novel approach to practical ML applications.
    This chapter frames **federated learning** (**FL**) as the answer to the desire
    for this new ML approach. In a nutshell, FL is an approach to ML that allows models
    to be trained in parallel across data sources without the transmission of any
    data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第一章*](B18369_01.xhtml#_idTextAnchor017)《大数据和传统人工智能的挑战》中，我们探讨了大数据和**机器学习（ML**）的潮流如何为新的实用ML应用方法奠定了基础。本章将**联邦学习（FL**）视为满足这种新ML方法需求的答案。简而言之，FL是一种ML方法，允许模型在数据源之间并行训练，而不需要传输任何数据。
- en: The goal of this chapter is to build up the case for the FL approach, with explanations
    of the necessary conceptual building blocks in order to ensure that you can achieve
    a similar understanding of the technical aspects and practical usage of FL.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是建立FL方法的论据，解释必要的概念构建块，以确保你可以达到对FL的技术方面和实际应用的类似理解。
- en: After reading the chapter, you should have a high-level understanding of the
    FL process and should be able to visualize where the approach slots into real-world
    problem domains.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章后，你应该对FL过程有一个高级别的理解，并且能够可视化这种方法在现实世界问题领域中的位置。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding the current state of ML
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解当前机器学习（ML）的状态
- en: Distributed learning nature – toward scalable AI
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式学习特性——走向可扩展人工智能
- en: Understanding FL
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解联邦学习（FL）
- en: FL system considerations
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FL系统考虑因素
- en: Understanding the current state of ML
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解当前机器学习（ML）的状态
- en: To understand why the benefits derived from the application of FL can outweigh
    the increased complexity of this approach, it is necessary to understand how ML
    is currently practiced and the associated limitations. The goal of this section
    is to provide you with this context.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解从应用FL中获得的利益为何可能超过这种方法增加的复杂性，有必要了解ML当前的实践及其相关的限制。本节的目标是为你提供这个背景。
- en: What is a model?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是模型？
- en: The term “model” finds usage across numerous different disciplines; however,
    the generalized definition we are interested in can be narrowed down to a working
    representation of the dynamics within some desired system. Simply put, we develop
    a model B of some phenomenon A as a means of better understanding A through the
    increased interactability offered by B. Consider the phenomenon of an object being
    dropped from some point in a vacuum. Using kinematic equations, we can compute
    exactly how long it will take for the object to hit the ground – this is a model
    of the aforementioned phenomenon.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 术语“模型”在众多不同学科中都有应用；然而，我们感兴趣的广义定义可以缩小到某些所需系统内动态的工作表示。简单来说，我们通过B模型更好地理解现象A，A是现象B的某种现象，这是通过B提供的增加的交互性来实现的。考虑一个物体从真空中某个点掉落的现象。使用运动方程，我们可以精确计算出物体落地所需的时间——这就是上述现象的模型。
- en: The power of this approach is the ability to observe results from the created
    model without having to explicitly interact with the phenomenon in question. For
    example, the model of the falling object allows us to determine the difference
    in fall time between a 10 kg object and a 50 kg object at some height without
    having to physically drop real objects from said height in a real vacuum. Evidently,
    the modeling of natural phenomena plays a key role in being able to claim a true
    understanding of said phenomena. Removing the need for the comprehensive observation
    of a phenomenon allows for true generalization in the decision-making process.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的强大之处在于，可以在不与所讨论的现象进行明确交互的情况下观察创建的模型的结果。例如，下落物体的模型使我们能够在某个高度上确定10公斤物体和50公斤物体下落时间的差异，而无需在真实真空中从该高度物理地释放真实物体。显然，对自然现象的建模在能够声称真正理解这些现象方面发挥着关键作用。消除对现象进行全面观察的需要，使得在决策过程中实现真正的泛化成为可能。
- en: The concept of a model is greatly narrowed down within the context of computer
    science. In this context, models are algorithms that allow for some key values
    of a phenomenon to be output given some initial characterization of the phenomenon
    in question. Going back to the falling object example, a computer science model
    could entail the computation of values such as the time to hit the ground and
    the maximum speed given the mass of the object and the height from which it is
    dropped. These computer science models are uniquely powerful due to the superhuman
    ability of computers to compute the output from countless starting phenomenon
    configurations, offering us even greater understanding and generalization.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学领域，模型的概念被大大缩小了。在这种情况下，模型是算法，它允许根据对所讨论现象的某些初始描述输出该现象的一些关键值。回到落体例子，计算机科学模型可能包括根据物体的质量和下落的高度计算值，如击中地面的时间和最大速度。这些计算机科学模型由于计算机在从无数起始现象配置中计算输出的超人类能力而独具特色，为我们提供了更深入的理解和更广泛的概括。
- en: So, how do we create such models? The first and simplest approach is building
    rule-based systems or **white-box** models. A white-box (also known as glass-box
    or clear-box) model is made by writing down the underlying functions of a system
    of interest explicitly. This is only possible when information about the system
    is available *a priori*. Naturally, in this case, the underlying functions are
    relatively simple. One such example is the problem of classifying a randomly selected
    integer as odd or even; we can easily write an algorithm to do this by *checking
    the remainder after dividing the integer by two*. If you want to see how much
    it costs to fill up your gas tank, given how empty the tank is and the price per
    gallon, you can just multiply those values together. Despite their simplicity,
    these examples illustrate that simple models can have a lot of practical applications
    in various fields.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何创建这样的模型呢？第一个也是最简单的方法是构建基于规则的系统或**白盒**模型。白盒模型（也称为玻璃盒或透明盒）是通过明确写出感兴趣系统的底层函数来构建的。这只有在系统信息可预先获得的情况下才可能。自然地，在这种情况下，底层函数相对简单。一个这样的例子是分类随机选择的整数是奇数还是偶数的问题；我们可以很容易地编写一个算法通过*检查整数除以二的余数*来完成这个任务。如果你想看看加满油箱需要花多少钱，给定油箱的空余量和每加仑的价格，你只需将这些值相乘即可。尽管这些例子很简单，但它们说明了简单的模型可以在各个领域有大量的实际应用。
- en: 'Unfortunately, the white-box modeling of underlying functions can quickly become
    too complex to perform directly. In general, systems are often too complex for
    us to be able to construct a white-box model for. For example, let’s say you want
    to predict the future values of your property. You have a lot of metrics about
    the property, such as the area, how old it is, its location, and interest rate
    to name but a few. You believe that there is likely a linear relationship between
    the property value and all of those metrics, such that the weighted sum of all
    of them would give you the property value. Now, if you actually try to build a
    white-box model under that assumption, you will have to directly figure out what
    the parameter (weight) for each metric is, which implies that you must know the
    underlying function of the real estate pricing system. Usually, this is not the
    case. Therefore, we need another approach: **black box** modeling.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，底层函数的白盒建模很快就会变得过于复杂而无法直接执行。一般来说，系统通常过于复杂，我们无法构建白盒模型。例如，假设你想预测你财产的未来价值。你有很多关于财产的指标，比如面积、它的年龄、位置，以及利率等等。你相信财产价值和所有这些指标之间可能存在线性关系，即所有这些指标的加权总和将给出财产价值。现在，如果你实际上基于这个假设尝试构建白盒模型，你必须直接确定每个指标的参数（权重），这意味着你必须知道房地产定价系统的底层函数。通常情况下，这不是事实。因此，我们需要另一种方法：**黑盒**建模。
- en: ML – automating the model creation process
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ML – 自动化模型创建过程
- en: The concept of a black box system was first developed in the field of electric
    circuits during the WWII period. It was the famous cybernetician Norbert Wiener
    who began treating the black box as an abstract concept, and a general theory
    was established by Mario Augusto Bunge in the 1960s. The function for estimating
    future property values, as illustrated earlier, is a good example of a black box.
    As you might expect, the function is complex enough that it is unreasonable for
    us to try to write a white-box model to represent it. This is where ML comes in,
    allowing us to create a model as a black box.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 黑盒系统的概念最早在二战期间的电气电路领域发展起来。正是著名的控制论学家诺伯特·维纳开始将黑盒视为一个抽象概念，并在20世纪60年代由马里奥·奥古斯特·邦格建立了通用理论。如前所述，估计未来房地产价值的函数是一个很好的黑盒例子。正如你所期望的，这个函数足够复杂，我们尝试编写一个白盒模型来表示它是不可行的。这就是机器学习发挥作用的地方，它允许我们创建一个作为黑盒的模型。
- en: Reference
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献
- en: You might be aware that black box modeling has been criticized for its lack
    of interpretability, an important concept outside the scope of this book; we recommend
    Packt’s *Interpretable Machine Learning with Python* by Serg Masís as a reference
    on the subject.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能知道，黑盒建模因其缺乏可解释性而受到批评，这是本书范围之外的一个重要概念；我们推荐Serg Masís所著的Packt出版的《Python可解释机器学习》作为该主题的参考。
- en: 'ML is a type of artificial intelligence that is used to automatically generate
    model parameters for making decisions and predictions. *Figure 2.1* illustrates
    this in a very simple way: those cases where the known values and the unknown
    value have a linear relationship allow a popular algorithm, called **ordinary
    least squares** (**OLS**), to be applied. OLS computes the unknown parameters
    of the linear relationship by finding the set of parameters that produces the
    closest predictions on some set of known examples (pairs of input feature value
    sets and the true output value):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一种人工智能，用于自动生成用于决策和预测的模型参数。*图 2.1* 以非常简单的方式说明了这一点：那些已知值和未知值之间存在线性关系的案例允许应用一个流行的算法，称为**普通最小二乘法**（**OLS**）。OLS通过找到产生最接近预测的一组参数来计算线性关系的未知参数，这些参数是在一些已知示例（输入特征值集和真实输出值对）上进行的：
- en: '![Figure 2.1 – ML determining model parameters'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.1 – 机器学习确定模型参数]'
- en: '](img/B18369_02_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18369_02_01.jpg]'
- en: Figure 2.1 – ML determining model parameters
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.1 – 机器学习确定模型参数]'
- en: The preceding diagram displays a simple two-dimensional linear regression problem
    with one feature/input variable and one output variable. In this toy two-dimensional
    case, it might be relatively straightforward for us to come up with the parameters
    representing the best-fit relationship directly, either through implicit knowledge
    or through testing different values. However, it should be clear that this approach
    quickly becomes intractable as the number of feature variables increases.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表显示了一个简单的二维线性回归问题，包含一个特征/输入变量和一个输出变量。在这个简单的二维案例中，我们可能相对容易直接提出代表最佳拟合关系的参数，无论是通过隐含知识还是通过测试不同的值。然而，很明显，随着特征变量的数量增加，这种方法很快就会变得难以处理。
- en: 'OLS allows us to attack this problem from the reverse direction: instead of
    producing linear relationships and evaluating them on the data, we can use the
    data to compute the parameters of the best-fit relationship directly instead.
    Revisiting the real estate problem, let’s assume that we have collected a large
    number of property valuation data points, consisting of the associated metric
    values and the sale price. We can apply OLS to take these points and find the
    relationship between each metric and the sale price for any property (still under
    the assumption that the true relationship is linear). From this, we can pass in
    the metric values of our property and get the predicted sale price.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最小二乘法（OLS）允许我们从相反的方向解决这个问题：我们不是产生线性关系并在数据上评估它们，而是可以直接使用数据来计算最佳拟合关系的参数。回顾房地产问题，假设我们已经收集了大量房地产估值数据点，包括相关的指标值和销售价格。我们可以应用OLS来取这些点，并找出每个指标与任何房地产销售价格之间的关系（仍然假设真实关系是线性的）。据此，我们可以输入我们房地产的指标值，并得到预测的销售价格。
- en: The power of this approach is the abstraction of this relationship computation
    from any implicit knowledge of the problem. The OLS algorithm doesn’t care what
    the data represents – it just finds the best line for the data it is given. This
    class of approaches is exactly what ML entails, granting the power to create models
    of phenomena without any required knowledge of the internal relationship, given
    a sufficient amount of data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的力量在于将这种关系计算从对问题的任何隐含知识中抽象出来。最小二乘法（OLS）算法并不关心数据代表什么——它只是找到给定数据的最佳直线。这类方法正是机器学习所包含的，它赋予了在没有内部关系知识的情况下，仅凭足够的数据创建现象模型的能力。
- en: In a nutshell, ML lets us program algorithms that can learn to create models
    from data, and our motivation to do so is to approximate complex systems. It is
    important to keep in mind that the underlying functions of a complex system can
    change over time due to outside factors, quickly making models created from old
    data obsolete. For example, the preceding linear regression model might not work
    to estimate property values in a far distant future or a faraway district. Variance
    in such a macroscopic scale is not taken into account in a model containing only
    a few dozen parameters, and we would need different models for separate groups
    of adjacent data points – unless we employ even more sophisticated ML approaches
    such as **deep learning**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，机器学习（ML）让我们能够编写算法，这些算法可以从数据中学习创建模型，而我们这样做的原因是为了近似复杂系统。重要的是要记住，由于外部因素的影响，复杂系统的底层函数可能会随时间而改变，这使得从旧数据中创建的模型很快就会过时。例如，前面的线性回归模型可能无法用于估计遥远未来或遥远地区的财产价值。在只包含几十个参数的模型中，并没有考虑到这种宏观尺度上的变化，我们需要为相邻数据点的不同组别使用不同的模型——除非我们采用更复杂的机器学习方法，如**深度学习**。
- en: Deep learning
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习
- en: So, how did deep learning become synonymous with ML in common usage? Deep learning
    involves the application of a **deep neural network** (**DNN**), which is a type
    of highly-parameterized model inspired by the transmission of signals between
    neurons in the brain. The foundation of deep learning was established in the early
    1960s by Frank Rosenblatt, who is known as the *father of deep learning*. His
    work was further developed in the 1970s and 1980s by computer scientists including
    Geoffrey Hinton, Yann LeCun, and Yoshua Bengio, and the term *deep learning* was
    popularized by the University of California, Irvine’s distinguished Professor
    Rina Dechter. Deep learning can conduct much more complex tasks compared to simpler
    ML algorithms such as linear regression. While the specifics are beyond the scope
    of this book, the key problem that deep learning was able to solve was the modeling
    of complex non-linear relationships, pushing ML as a whole to the forefront of
    numerous fields due to the increased modeling ability it provided.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，深度学习是如何在普通用法中与机器学习（ML）同义的？深度学习涉及应用**深度神经网络**（DNN），这是一种受大脑中神经元之间信号传递启发的超参数化模型。深度学习的基础是在20世纪60年代初由Frank
    Rosenblatt建立的，他被誉为**深度学习之父**。他的工作在20世纪70年代和80年代得到了包括Geoffrey Hinton、Yann LeCun和Yoshua
    Bengio在内的计算机科学家的进一步发展，而“深度学习”这一术语是由加州大学欧文分校的杰出教授Rina Dechter普及的。与简单的机器学习算法（如线性回归）相比，深度学习可以执行更复杂的任务。虽然具体内容超出了本书的范围，但深度学习能够解决的关键问题是复杂非线性关系的建模，由于它提供的建模能力增强，推动了机器学习作为整体在众多领域的最前沿。
- en: This ability has been mathematically proven via specific universal approximation
    theorems for different model size cases. For those of you who are new to deep
    learning or ML in general, the third edition of *Python Machine Learning* by Sebastian
    Raschka and Vahid Mirjalili is a good reference to learn more about the subject.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种能力已经通过针对不同模型大小案例的具体通用逼近定理在数学上得到证明。对于那些对深度学习或机器学习（ML）总体上都是新手的人来说，Sebastian
    Raschka和Vahid Mirjalili合著的《Python机器学习》第三版是一个很好的参考资料，可以了解更多关于这个主题的内容。
- en: Over the past decade, ever-increasingly powerful models have been built by tech
    giants against the backdrop of big data, as discussed in [*Chapter 1*](B18369_01.xhtml#_idTextAnchor017),
    *Challenges in Big Data and Traditional AI*. If we look at the state-of-the-art
    deep learning models today, they could have up to trillions of parameters; expectedly,
    this gives them unparalleled flexibility in modeling complex functions. The reason
    deep learning models can be scaled up to arbitrarily increase performance, unlike
    other ML model types used previously, is due to a phenomenon called **double descent**.
    This refers to the ability for a certain parameterization/training threshold to
    overcome the standard bias-variance trade-off (where increasing complexity leads
    to fine-tuning on training data, reducing bias but increasing variance) and continuing
    to increase performance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，随着大数据的背景，科技巨头们构建了越来越强大的模型，正如在[*第一章*](B18369_01.xhtml#_idTextAnchor017)“大数据与传统人工智能的挑战”中讨论的那样。如果我们看看当今最先进的深度学习模型，它们可能拥有多达万亿个参数；预期地，这赋予了它们在建模复杂函数方面无与伦比的灵活性。深度学习模型之所以能够通过所谓的**双重下降**现象任意扩展以增加性能，而不同于之前使用的其他机器学习模型类型，是因为这种现象。这指的是某个参数化/训练阈值能够克服标准的偏差-方差权衡（其中增加复杂性会导致对训练数据的微调，减少偏差但增加方差），并继续提高性能。
- en: The key takeaway is that the performance of deep learning models can be considered
    to be limited by just the available compute power and data, two factors that have
    surged in growth in the past 10 years due to advances in computing and the ever-increasing
    number of devices and software collecting data, respectively. Deep learning has
    become intertwined with ML, with deep learning playing a significant role within
    the current state of ML and big data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的启示是，深度学习模型的性能可以被认为是仅受限于可用的计算能力和数据，这两个因素在过去十年中由于计算技术的进步以及设备数量和收集数据的软件数量的不断增加而迅速增长。深度学习已经与机器学习交织在一起，深度学习在当前的机器学习和大数据状态下发挥着重要作用。
- en: This section focused on establishing a case for the importance of the modeling
    performed by current ML techniques. In a sense, this can be considered the *what*
    – what exactly FL is trying to do. Next, we will focus on the *where* in terms
    of the desired setting for numerous ML applications.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点在于确立当前机器学习技术所执行建模的重要性。从某种意义上说，这可以被认为是*什么*——FL（联邦学习）究竟试图做什么。接下来，我们将关注*哪里*，即众多机器学习应用所期望的设置。
- en: Distributed learning nature – toward scalable AI
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式学习特性——向可扩展人工智能迈进
- en: In this section, we introduce the distributed computing setting and discuss
    the intersection of this setting with ML approaches to fully establish the support
    for why FL is necessary. The goal of the section is for the user to understand
    both the benefits and limitations imposed by the distributed computing setting,
    in order to understand how FL addresses some of these limitations.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了分布式计算环境，并讨论了该环境与机器学习方法的交汇，以完全确立FL（联邦学习）必要性的支持。本节的目标是让用户理解分布式计算环境带来的优势和局限性，以便理解FL如何解决其中的一些局限性。
- en: Distributed computing
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式计算
- en: The past several years have shown a large but predictable rise in the development
    of new approaches and the conversion of existing server infrastructure within
    the lens of distributed computing. To generalize further, distributed approaches
    themselves have shifted more and more from research implementations to extensive
    use in production settings; one significant example of this phenomenon is the
    usage of cloud computing platforms such as AWS from Amazon, **Google Cloud Platform**
    (**GCP**) from Google, and Azure from Microsoft. It turns out that the flexibility
    of on-demand resources allows for cost-saving and efficiency in numerous applications
    that would, otherwise, be bottlenecked by on-premise servers and computational
    power.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，从分布式计算的角度来看，新方法的开发和对现有服务器基础设施的转换呈现出大幅但可预测的增长。进一步概括，分布式方法本身越来越多地从研究实现转向在生产环境中的广泛应用；这一现象的一个显著例子是亚马逊的AWS、谷歌的**Google
    Cloud Platform**（GCP）和微软的Azure等云计算平台的运用。结果是，按需资源的灵活性使得在许多其他情况下，可能会受到本地服务器和计算能力的瓶颈限制的应用中，实现了成本节约和效率提升。
- en: 'While a parallel cannot exactly be drawn between cloud computing and the concept
    of distributed computing, the key benefits stemming from the distributed nature
    are similar. At a high level, distributed computing involves spreading the work
    necessary for some computational task over a number of computational agents in
    a way that allows each to act near-autonomously. The following figure shows the
    difference between centralized and distributed approaches in the high-level context
    of answering questions:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不能完全将云计算与分布式计算的概念相提并论，但由此产生的关键好处在本质上相似。从高层次来看，分布式计算涉及将某些计算任务的必要工作分散到多个计算代理中，以便每个代理都能近乎自主地行动。以下图显示了在回答问题的高层次上下文中，集中式和分布式方法之间的差异：
- en: '![Figure 2.2 – Centralized versus distributed question answering'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.2 – 集中式与分布式问答'
- en: '](img/B18369_02_02.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18369_02_02.jpg)'
- en: Figure 2.2 – Centralized versus distributed question answering
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – 集中式与分布式问答
- en: In this toy example, the centralized approach involves processing the input
    questions sequentially, whereas the distributed approach is able to process each
    question at the same time. It should be clear that the parallel approach trades
    off computational resource usage for increased answering speed. The natural question,
    then, is whether this trade-off is beneficial for real-world applications.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个玩具示例中，集中式方法涉及按顺序处理输入问题，而分布式方法能够同时处理每个问题。应该清楚的是，并行方法是在计算资源使用和增加回答速度之间进行权衡。那么，这个权衡对于现实世界的应用是否有益，自然就成为了一个问题。
- en: Real-world example – e-commerce
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 现实世界案例 – 电子商务
- en: To understand the practical benefits of distributed computing approaches, let’s
    analyze an example business problem through a traditional and a distributed computing
    lens. Consider an e-commerce business that is trying to host its website using
    on-premise servers. The traditional way to do this would be to perform enough
    analysis on the business side to determine the expected volume of traffic at some
    future time and invest in one or a couple of server machines large enough to handle
    that calculated volume.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解分布式计算方法的实际好处，让我们通过传统计算和分布式计算的角度分析一个业务问题。考虑一个试图使用本地服务器托管其网站的电子商务业务。传统的方法是在业务方面进行足够分析，以确定未来某个时间点的预期流量量，并投资购买一台或几台足够处理该计算出的流量的服务器机器。
- en: Several cases immediately lend themselves to showing the flaws of such an approach.
    Consider a scenario where usage of the websites greatly exceeds the initial projections.
    A fixed number of servers means that all upgrades must be hardware upgrades, resulting
    in old hardware that had to be purchased and is no longer used. Going further,
    there are no guarantees that the now-increased usage will stay fixed. Further
    increases in usage will result in more scaling-up costs, while decreases in usage
    will lead to wasted resources (maintaining large servers when smaller machines
    would be sufficient). A key point is that the integration of additional servers
    is non-trivial due to the single-machine approach used to manage hosting. Additionally,
    we have to consider the hardware limitations of handling large numbers of requests
    in parallel with one or a few machines. The ability to handle requests in parallel
    is limited for each machine – significant volumes of traffic would be almost guaranteed
    to eventually be bottlenecked regardless of the power available to each server.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 几个案例立即揭示了这种方法的缺陷。考虑一种情况，即网站的使用量远远超过了最初的预测。固定数量的服务器意味着所有升级都必须是硬件升级，导致必须购买且不再使用的旧硬件。进一步来说，现在增加的使用量是否保持不变也没有保证。使用量的进一步增加将导致更多的扩展成本，而使用量的减少将导致资源浪费（当小型机器就足够时，仍需维护大型服务器）。一个关键点是，由于使用单机方法来管理托管，额外服务器的集成并不简单。此外，我们必须考虑处理大量并发请求的硬件限制。每个机器处理并发请求的能力有限——大量的流量几乎肯定会最终成为瓶颈，无论每个服务器的可用功率如何。
- en: In comparison, consider the distributed computing-based solution for this problem.
    Based on the initial business projections, a number of smaller server machines
    are purchased and each is set up to handle some fixed volume of traffic. If the
    scenario of incoming traffic exceeding projects arises, no modification to the
    existing machines is necessary; instead, more similarly-sized servers can be purchased
    and configured to handle their designated volume of new traffic. If the incoming
    traffic decreases, the equivalent number of servers can be shut down or shifted
    to handle other tasks. This means that the same hardware can be used for variable
    volumes of traffic.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，考虑基于分布式计算的解决方案。根据初步的商业预测，购买了一些较小的服务器机器，并且每台机器都设置好了以处理一些固定的流量量。如果出现超出预期的流量情况，不需要对现有机器进行修改；相反，可以购买更多类似尺寸的服务器，并配置它们来处理指定的新流量量。如果流量减少，相应数量的服务器可以被关闭或转移到处理其他任务。这意味着相同的硬件可以用于可变流量的处理。
- en: This ability to scale quickly to handle the necessary computational task at
    any moment is precisely due to how distributed computing approaches allow for
    computational agents to seamlessly start and stop working on said task. In addition,
    the use of many smaller machines in parallel, versus using fewer larger machines,
    means that the number of requests that can be handled at the same time is notably
    higher. It is clear that a distributed computing approach, in this case, lends
    itself to cost-saving and flexibility that cannot be matched with more traditional
    methods.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这种能力能够快速扩展以处理任何时刻必要的计算任务，这正是分布式计算方法允许计算代理无缝开始和停止工作在所述任务上的原因。此外，与使用较少的大型机器相比，并行使用许多较小的机器意味着可以同时处理的请求数量显著更高。很明显，在这种情况下，分布式计算方法本身具有节省成本和灵活性的优势，这是更传统的方法无法比拟的。
- en: Benefits of distributed computing
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式计算的好处
- en: In general, distributed computing approaches offer three main benefits for any
    computational task – scalability, throughput, and resilience. In the previous
    case of web hosting, scalability referred to the ability to scale the number of
    servers deployed based on the amount of incoming traffic, whereas throughput refers
    to the ability to reduce request processing latency through the inherent parallelism
    of smaller servers. In this example, resilience could refer to the ability of
    other deployed servers to take on the load from a server that stops working, allowing
    the hosting to continue relatively unfazed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，分布式计算方法为任何计算任务提供了三个主要好处——可扩展性、吞吐量和弹性。在前面的网页托管案例中，可扩展性指的是根据进入的流量量调整部署的服务器数量，而吞吐量指的是通过较小服务器的固有并行性来减少请求处理延迟的能力。在这个例子中，弹性可能指的是其他部署的服务器能够承担停止工作的服务器的负载，从而使得托管能够相对不受影响地继续进行。
- en: Distributed computing often finds uses when working with large stores of data,
    especially when attempting to perform analyses on the data using a single machine
    would be computationally infeasible or otherwise undesirable. In these cases,
    scalability allows for the deployment of a variable number of agents based on
    factors such as the desired runtime and amount of data at any given time, whereas
    the ability of each agent to autonomously work on processing a subset of the data
    in parallel allows for processing throughput that would be impossible for a single
    high-power machine to achieve. It turns out that this lack of reliance on cutting-edge
    hardware leads to further cost savings, as hardware price-to-performance ratios
    are often not linear.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式计算通常在处理大量数据时得到应用，尤其是在使用单一机器进行数据分析在计算上不可行或不太理想的情况下。在这些情况下，可扩展性允许根据所需的运行时间和任何给定时间的数据量等因素部署可变数量的代理，而每个代理能够自主地并行处理数据子集的能力，使得处理吞吐量对于单一高性能机器来说是不可能的。结果发现，这种不依赖于尖端硬件的做法进一步降低了成本，因为硬件价格与性能比通常不是线性的。
- en: While the development of parallelized software to operate in a distributed computing
    setting is non-trivial, hopefully, it is clear that many practical computational
    tasks greatly benefit from the scalability and throughput achieved by such approaches.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然开发用于在分布式计算环境中运行的并行化软件并非易事，但希望这清楚地表明，许多实际计算任务都极大地受益于这种方法实现的可扩展性和吞吐量。
- en: Distributed ML
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式机器学习
- en: When thinking about the types of computational tasks that have proven to be
    valuable in practical applications and that might be directly benefited from increased
    scalability and throughput, it is clear that the rapidly growing field of ML is
    near the top. In fact, we can frame ML tasks as a specific example of the aforementioned
    tasks of analyzing large stores of data, placing emphasis on the data being processed
    and the nature of the analysis being performed. The joint growth of cheap computational
    power (for example, smart devices) and the proven benefits of data analysis and
    modeling have led to companies with both the storage of excessive amounts of data
    and the desire to extract meaningful insights and predictions from said data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑那些在实用应用中已被证明有价值且可能直接受益于增加可扩展性和吞吐量的计算任务类型时，很明显，快速增长的机器学习领域几乎处于顶端。事实上，我们可以将机器学习任务视为上述分析大量数据任务的具体例子，强调正在处理的数据和分析所执行的性质。廉价计算能力（例如，智能设备）的联合增长以及数据分析建模的既定益处，导致了一些公司拥有大量数据存储，并希望从这些数据中提取有意义的见解和预测。
- en: The second part is exactly what ML is geared to solve, and large amounts of
    work have already been completed to do so in various domains. However, like other
    computational tasks, performing ML on large stores of data often leads to a time-computational
    power trade-off in which more powerful machines are needed to perform such tasks
    in reasonable amounts of time. As ML algorithms become more computationally and
    memory-intensive, such as recent state-of-the-art deep learning models with billions
    of parameters, hardware bottlenecks make increasing the computational power infeasible.
    As a result, current ML tasks must apply distributed computing approaches to stay
    cutting-edge while producing results in usable timeframes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分正是机器学习旨在解决的问题，已经在各个领域完成了大量工作。然而，与其他计算任务一样，在大量数据上执行机器学习往往会导致时间-计算能力权衡，需要更强大的机器在合理的时间内完成这些任务。随着机器学习算法在计算和内存方面变得更加密集，例如最近具有数十亿参数的最新深度学习模型，硬件瓶颈使得增加计算能力变得不可行。因此，当前的机器学习任务必须应用分布式计算方法，以保持前沿地位，同时在使用时间内产生结果。
- en: Edge inference
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 边缘推理
- en: Although the prevalence of deep learning described earlier, besides the paradigm
    shift from big data to collective intelligence discussed in [*Chapter 1*](B18369_01.xhtml#_idTextAnchor017),
    *Challenges in Big Data and Traditional AI*, gives enough motivation for distributed
    ML, its physical foundation came from the recent development of **edge computing**.
    The *edge* represents the close proximity around deployed solutions; it follows
    that edge computing refers to processing data at or near the location of the data
    source. Extending the concept of computation to ML leads to the idea of **edge
    AI**, where models are integrated directly into edge devices. A few popular examples
    would be Amazon Alexa, where edge AI takes care of speech recognition, and self-driving
    cars that collect real-world data and incrementally improve with edge AI.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面描述的深度学习的普及，以及在第[*第一章*](B18369_01.xhtml#_idTextAnchor017)中讨论的从大数据到集体智能的范式转变——“大数据与传统人工智能的挑战”，为分布式机器学习提供了足够的动力，但其物理基础却源于最近边缘计算的快速发展。这里的*边缘*指的是部署解决方案的附近区域；因此，边缘计算指的是在数据源附近或其处处理数据。将计算的概念扩展到机器学习导致**边缘人工智能**（Edge
    AI）的想法，其中模型直接集成到边缘设备中。一些流行的例子包括亚马逊Alexa，其中边缘人工智能负责语音识别，以及自动驾驶汽车，它们收集真实世界的数据，并通过边缘人工智能逐步改进。
- en: The most ubiquitous example is the smartphone – some potential uses are the
    recommendation of content to the user, searches with voice assistance and auto-complete,
    auto-sorting of pictures into an album and gallery search, and more. To capitalize
    on this potential, smartphone manufacturers have already begun integrating ML-focused
    processor components into the chips they integrate with their newest phones, such
    as the *Neural Processing Unit* from *Samsung* and the *Tensor Processing Unit*
    on the *Google Tensor chip*. Google has also worked to develop ML-focused APIs
    for Android applications through their *Android ML Kit SDK*. From this, it should
    be clear that ML applications are shifting toward the edge computing paradigm.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最普遍的例子是智能手机——一些潜在用途包括向用户推荐内容、带有语音助手的搜索和自动完成、自动将图片排序到相册和图库搜索等。为了利用这种潜力，智能手机制造商已经开始将专注于机器学习的处理器组件集成到他们与最新手机集成的芯片中，例如三星的*神经处理单元*和谷歌Tensor芯片上的*张量处理单元*。谷歌还通过他们的*Android
    ML Kit SDK*为Android应用程序开发了专注于机器学习的API。从这一点来看，机器学习应用正转向边缘计算范式。
- en: 'Let’s say that smartphones need to use a deep learning model for word recommendation.
    This is so that when you type words on your phone, it gives you suggestions for
    the next word, with the goal being to save you some time. In the scheme of a centralized
    computing process, the central server is the only component that has access to
    this text prediction model and none of the phones have the model stored locally.
    The central server handles all of the requests sent from the phones to return
    word recommendations. As you type, your phone has to send what has been typed
    along with some personal information about you, all the way to the central server.
    The server receives this information, makes a prediction using the deep learning
    model, and then sends the result back to the phone. The following figure reflects
    this scenario:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 假设智能手机需要使用深度学习模型来进行单词推荐。这样，当你用手机输入文字时，它会为你提供下一个单词的建议，目的是节省你的时间。在集中式计算过程的方案中，中央服务器是唯一可以访问这个文本预测模型的组件，而没有任何一部手机本地存储了这个模型。中央服务器处理来自手机的所有请求，以返回单词推荐。当你输入时，你的手机必须将已输入的内容以及一些关于你的个人信息发送到中央服务器。服务器接收到这些信息，使用深度学习模型进行预测，然后将结果发送回手机。以下图反映了这个场景：
- en: '![Figure 2.3 – Centralized inference scenario'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.3 – 集中式推理场景'
- en: '](img/B18369_02_03.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18369_02_03.jpg)'
- en: Figure 2.3 – Centralized inference scenario
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – 集中式推理场景
- en: There are a few problems that become apparent when you look at this scenario.
    First, even a half to one second of latency makes the recommendation slower than
    typing everything yourself, making the system useless. Furthermore, if there is
    no internet connection, the recommendation simply does not work. Another restriction
    of this scheme is the need for the central server to process all of these requests.
    Imagine how many smartphones are being used in the world, and you will realize
    a lack of feasibility due to the extreme scale of this solution.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当你观察这个场景时，会出现一些明显的问题。首先，即使只有半秒到一秒的延迟，推荐的速度也会比你自己输入所有内容慢，使得系统变得无用。此外，如果没有互联网连接，推荐根本无法工作。这个方案的另一个限制是需要中央服务器处理所有这些请求。想象一下世界上有多少部智能手机正在使用，你就会意识到由于这个解决方案的极端规模，其可行性存在不足。
- en: 'Now, let’s look at the same problem from the edge computing perspective. What
    if the smartphones themselves contain the deep learning model? The central server
    is only in charge of managing the latest trained model and communicating this
    model with each phone. Now, whenever you start typing, your phone can use the
    received model locally to make recommendations from what you typed. The following
    figure reflects this scenario:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从边缘计算的角度来看同一个问题。如果智能手机本身包含深度学习模型会怎样？中央服务器只负责管理最新的训练模型，并与每部手机通信这个模型。现在，无论何时你开始输入，你的手机都可以使用接收到的模型在本地进行推荐。以下图反映了这个场景：
- en: '![Figure 2.4 – Edge inference scenario'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.4 – 边缘推理场景'
- en: '](img/B18369_02_04.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18369_02_04.jpg)'
- en: Figure 2.4 – Edge inference scenario
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 边缘推理场景
- en: 'This removes both the latency problem and prevents the need to handle the incoming
    inference requests at a central location. In addition, the phones no longer have
    to maintain a connection with the server to make a recommendation. Each phone
    is in charge of fulfilling requests from its user. This is the core benefit of
    edge computing: *we have moved the computing load from the central server to the
    edge devices/servers*.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这样既消除了延迟问题，又避免了在中央位置处理传入的推理请求的需要。此外，手机不再需要与服务器保持连接以进行推荐。每部手机负责满足其用户的请求。这是边缘计算的核心优势：*我们将计算负载从中央服务器转移到了边缘设备/服务器*。
- en: Edge training
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 边缘训练
- en: The distinction between centralized and decentralized computing can be extended
    to the concept of model training. Let’s stick to the smartphone example but think
    about how we would train the predictive model instead. First, in the centralized
    ML process, all of the data used to train the recommendation model must be collected
    from the users’ devices and stored on the central server. Then, the collected
    data is used to train a model, which is eventually sent to all the phones. This
    means that the central server still has to be able to handle the large volume
    of user data coming in and store it in an efficient way to be able to train the
    model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 集中式与分布式计算之间的区别可以扩展到模型训练的概念。让我们继续以智能手机为例，但思考一下我们如何训练预测模型。首先，在集中式机器学习过程中，用于训练推荐模型的所有数据都必须从用户的设备中收集并存储在中央服务器上。然后，收集到的数据用于训练一个模型，最终发送到所有手机。这意味着中央服务器仍然需要能够处理大量涌入的用户数据，并以高效的方式存储它，以便能够训练模型。
- en: 'This design leads to the problems found in the centralized computing approach:
    as the number of phones connected to the server increases, the server’s ability
    to work with the incoming data needs to scale in order to maintain the training
    process. In addition, since the data needs to be transmitted and stored centrally
    in this approach, there is always the possibility of the interception of transmissions
    or even attacks on the stored data. There are several cases where data confidentiality
    and privacy are required or strongly desired; for example, applications in the
    financial and medical industries. *Centralized model training thus limits use
    cases, and an alternative way to work with data directly on edge devices is required*.
    This exact setting is the motivation for FL.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计导致了集中式计算方法中存在的问题：随着连接到服务器的手机数量的增加，服务器处理传入数据的能力需要扩展，以维持训练过程。此外，由于数据需要在这种方法中集中传输和存储，因此始终存在传输被拦截或存储数据受到攻击的可能性。在许多情况下，需要或强烈希望保护数据机密性和隐私；例如，金融和医疗行业中的应用。*因此，集中式模型训练限制了用例，需要一种直接在边缘设备上处理数据的工作方式*。这正是FL的动机所在。
- en: Understanding FL
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解FL
- en: This section focuses on providing a high-level technical understanding of how
    FL actually slots in as a solution to the problem setting described in the previous
    section. The goal of this section is for you to understand how FL fits as a solution,
    and to provide a conceptual basis that will be filled in by the subsequent chapters.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点在于提供对FL（联邦学习）如何作为解决上一节所述问题设置的一种解决方案的高级技术理解。本节的目标是让您了解FL作为解决方案的适用性，并为后续章节提供概念基础。
- en: Defining FL
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义FL
- en: '*Federated learning is a method to synthesize global models from local models
    trained on the edge*. FL was first developed by Google in 2016 for their Gboard
    application, which incorporates the context of an Android user’s typing history
    to suggest corrections and propose candidates for subsequent words. Indeed, this
    is the exact word recommendation problem discussed in the *Edge inference* and
    *Edge training* sections. The solution that Google produced was a decentralized
    training approach where an iterative process would compute model training updates
    at the edge, aggregating these updates to produce the global update to be applied
    to the model. This core concept of aggregating model updates was key in allowing
    for a single, performant model to be produced from edge training.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**联邦学习是一种从边缘训练的本地模型中综合全局模型的方法**。FL 首次由 Google 在 2016 年为他们的 Gboard 应用开发，该应用结合了
    Android 用户打字历史的上下文来建议更正并提出后续单词的候选词。确实，这正是我们在**边缘推理**和**边缘训练**部分讨论的精确单词推荐问题。Google
    提出的解决方案是一种去中心化的训练方法，其中迭代过程会在边缘计算模型训练更新，并将这些更新聚合起来以生成应用于模型的全球更新。这种聚合模型更新的核心概念对于从边缘训练产生单个、高性能的模型至关重要。'
- en: Let’s break this concept down further. The desired model is distributed across
    the edge and is trained on data collected locally at the edge. Of course, we can
    expect that a model trained on one specific data source is not going to be representative
    of the entire dataset. As a result, we dub such models trained with limited data
    **local models**. One immediate benefit of this approach is the enabling of ML
    on data that would otherwise not be collected in the centralized case, due to
    issues with privacy and efficiency.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步分解这个概念。期望的模型分布在边缘，并在边缘收集的本地数据上训练。当然，我们可以预期，在一个特定数据源上训练的模型不会代表整个数据集。因此，我们将使用有限数据训练的这种模型称为**本地模型**。这种方法的直接好处是，它使得在集中式情况下由于隐私和效率问题而无法收集的数据上的机器学习成为可能。
- en: Aggregation, the key theoretical step of FL, allows for our desired single **global
    model** to be created from the set of local models produced at some iteration.
    The most well-known aggregation algorithm, popular for its simplicity and surprising
    performance, is called **federated averaging** (**FedAvg**). FedAvg is performed
    on a set of local models by computing the parameter-wise arithmetic mean across
    the models, producing an aggregate model. It is important to understand that performing
    aggregation once is not enough to produce a good global aggregate model; instead,
    it is the iterative process of locally training the previous global model and
    aggregating the produced local models into a new global model that allows for
    global training progress to be made.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合，FL 的关键理论步骤，允许我们期望的单一**全局模型**从某些迭代产生的本地模型集合中创建出来。最著名的聚合算法，因其简单性和出人意料的性能而受到欢迎，被称为**联邦平均**（**FedAvg**）。FedAvg
    通过计算模型间的参数算术平均值来在本地模型集上执行，生成一个聚合模型。重要的是要理解，仅进行一次聚合不足以产生一个好的全局聚合模型；相反，是通过在本地训练先前的全局模型并将产生的本地模型聚合到一个新的全局模型中，这个过程允许实现全局训练的进展。
- en: The FL process
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FL 流程
- en: To better understand FL from an iterative process perspective, we break it down
    into the core constituent steps of a single iteration, or *round*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从迭代过程的角度更好地理解 FL，我们将它分解为单个迭代或**轮**的核心组成步骤。
- en: 'The steps for a round can be described as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一轮的步骤可以描述如下：
- en: The aggregate global model parameters are sent to each user’s device.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将聚合全局模型参数发送到每个用户的设备。
- en: The received ML models located on the user devices are trained with local data.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户设备上接收到的 ML 模型使用本地数据进行训练。
- en: After a certain amount of training, the local model parameters are sent to the
    central server.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一定量的训练之后，本地模型参数被发送到中心服务器。
- en: The central server aggregates the local models by applying an aggregation function,
    producing a new aggregate global model.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 中心服务器通过应用聚合函数来聚合本地模型，生成一个新的聚合全局模型。
- en: 'These steps are depicted in *Figure 2.5*:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤在**图 2.5**中有所描述：
- en: '![Figure 2.5 – FL steps'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.5 – FL 步骤'
- en: '](img/B18369_02_05.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18369_02_05.jpg](img/B18369_02_05.jpg)'
- en: Figure 2.5 – FL steps
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 – FL 步骤
- en: The flow from *steps 1 to 4* constitutes a single round of FL. The next round
    begins as the user servers/devices receive the newly created aggregate model and
    start training on the local data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从**步骤 1 到 4**的流程构成了一个 FL 的单轮。下一轮开始于用户服务器/设备接收到新创建的聚合模型并开始在本地数据上训练。
- en: Let’s revisit Google’s word recommendation for Gboard. At some point in time,
    each phone stores a sufficient amount of its user’s typing data. The edge training
    process can create a local model from it, and the parameters will be sent to the
    central server. After receiving parameters from a certain number of phones, the
    server aggregates them to create a global model and sends it to the phones. This
    way, every phone connected to the server receives a model that reflects local
    data in all of the phones without ever transmitting the data from them. In turn,
    each phone retrains the model when another batch of sufficient data is collected,
    sends the model to the server, and receives a new global model. This cycle repeats
    itself over and over according to the configuration of the FL system, resulting
    in the continuous monitoring and updating of the global model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下谷歌为Gboard提供的单词推荐功能。在某个时间点，每部手机都会存储足够多的用户打字数据。边缘训练过程可以从这些数据中创建一个本地模型，并将参数发送到中央服务器。在收到一定数量的手机发送的参数后，服务器将它们汇总以创建一个全局模型，并将其发送到手机。这样，连接到服务器的每部手机都会接收到一个反映所有手机本地数据的模型，而无需从它们那里传输数据。反过来，当收集到另一批足够的数据时，每部手机都会重新训练模型，将模型发送到服务器，并接收一个新的全局模型。这个周期会根据FL系统的配置反复进行，从而实现全局模型的持续监控和更新。
- en: Note that the user data never leaves the edge, only the model parameters; nor
    is there a need to put all the data in a central server to generate a global model,
    allowing for data minimalism. Moreover, model bias can be mitigated with FL methods,
    as discussed in [*Chapter 3*](B18369_03.xhtml#_idTextAnchor058), *Workings of
    the Federated Learning System*. That is why FL can be regarded as a solution to
    the three issues of big data, which were introduced in [*Chapter 1*](B18369_01.xhtml#_idTextAnchor017),
    *Challenges in Big Data and Traditional AI*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，用户数据从未离开边缘，只有模型参数；也不需要将所有数据放入中央服务器以生成全局模型，这允许数据最小化。此外，可以使用FL方法减轻模型偏差，如第[*第3章*](B18369_03.xhtml#_idTextAnchor058)中讨论的，*联邦学习系统的运作*。这就是为什么FL可以被视为解决在[*第1章*](B18369_01.xhtml#_idTextAnchor017)中引入的大数据三个问题的解决方案，即*大数据和传统AI的挑战*。
- en: Transfer learning
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迁移学习
- en: FL is closely related to an ML concept called **transfer learning** (**TL**).
    TL allows us to use large deep learning models that have been trained by researchers
    using plentiful compute power and resources on very generalized datasets. These
    models can be applied to more specific problems.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: FL与机器学习中的一个概念**迁移学习**（**TL**）密切相关。迁移学习允许我们使用研究人员使用大量计算能力和资源在非常通用的数据集上训练的大型深度学习模型。这些模型可以应用于更具体的问题。
- en: For example, we can take an object detection model trained to locate and name
    specific objects in images and retrain it on a limited dataset containing specific
    objects we are interested in, which were not included in the original data. If
    you were to take the original data, add to it the data of those objects of our
    interest, and then train a model from scratch, a lot of computational time and
    power would be required. With TL, you can quicken the process by leveraging a
    key fact about those existing large, generalized models. There is a tendency for
    the intermediate layers of large DNNs to be excellent at extracting features,
    used by the following layers for the specific ML task. We can maintain its learned
    ability to extract features by preserving the parameters in those layers.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以取一个训练用于在图像中定位和命名特定对象的物体检测模型，并在包含我们感兴趣且未包含在原始数据中的特定对象的有限数据集上重新训练它。如果你要从原始数据中取出，添加我们感兴趣的那些对象的资料，然后从头开始训练一个模型，将需要大量的计算时间和能力。有了迁移学习，你可以通过利用现有大型通用模型的一个关键事实来加快这个过程。大型深度神经网络的中层通常非常擅长提取特征，这些特征被后续层用于特定的机器学习任务。我们可以通过保留这些层中的参数来维持其提取特征的学习能力。
- en: In other words, parameters in certain layers of existing pre-trained models
    can be preserved and used to detect new objects – we do not need to reinvent the
    wheel. This technique is called **parameter freezing**. In FL, model training
    often takes place in local devices/servers with limited computational power. One
    example using the Gboard scenario is performing parameter freezing on a pretrained
    word embedding layer to allow training to focus on task-specific information,
    leveraging prior training of the embeddings to greatly reduce the trainable parameter
    count.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，现有预训练模型某些层的参数可以保留并用于检测新对象——我们不需要重新发明轮子。这种技术被称为**参数冻结**。在FL中，模型训练通常在计算能力有限的本地设备/服务器上进行。一个使用Gboard场景的例子是在预训练的词嵌入层上执行参数冻结，以便训练可以专注于特定任务的信息，利用嵌入的先前训练来大大减少可训练参数的数量。
- en: Taking this concept further, the intersection of FL and TL is called **federated
    transfer learning** (**FTL**). FTL allows for the FL approach to be applied in
    cases where the local datasets differ in structure by performing FL on a shared
    subset of the model that can later be extended for specific tasks. For example,
    a sentiment analysis model and a text summarization model could both share a sentence
    encoding component, which can be trained using FL and used for both tasks. TL
    (and, by extension, FTL) are key concepts that allow for training efficiency and
    incremental improvement to be realized in FL.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个概念进一步扩展，联邦学习（FL）和迁移学习（TL）的交集被称为**联邦迁移学习**（**FTL**）。FTL允许在本地数据集结构不同的场景下应用FL方法，通过在模型的一个共享子集上执行FL，该子集可以后来扩展用于特定任务。例如，一个情感分析模型和一个文本摘要模型可以共享一个句子编码组件，该组件可以使用FL进行训练，并用于这两个任务。TL（以及由此扩展的FTL）是允许在FL中实现训练效率和增量改进的关键概念。
- en: Personalization
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 个性化
- en: When edge devices are dealing with data that is not **independent and identically
    distributed** (**IID**), each device can customize the global model. This is an
    idea called **personalization**, which can be considered as fine-tuning the global
    model with local data, or the strategic use of bias in the data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当边缘设备处理的数据不是**独立同分布**（**IID**）时，每个设备都可以定制全局模型。这是一个被称为**个性化**的概念，可以被视为使用本地数据对全局模型进行微调，或者是在数据中战略性地使用偏差。
- en: For example, consider a shopping mall chain that operates in two areas with
    distinct local demographics (that is, the chain deals with non-IID data). If the
    chain seeks tenant recommendations for both locations using FL, each of the locations
    can be better served with personalized models than a single global model, helping
    attract local customers. Since the personalized model is fine-tuned or *biased*
    with local data, we can expect that its performance on general data would not
    be as good as that of the global model. On the other hand, we can also expect
    that the personalized model performs better than the global model on the local
    data for which the model is personalized. There is a trade-off between user-specific
    performance and generalizability, and the power of an FL system comes from its
    flexibility to balance them according to the requirements.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个在两个具有不同本地人口统计数据的地区运营的购物中心连锁店（即，该连锁店处理非IID数据）。如果该连锁店使用FL为两个地点寻求租户推荐，那么每个地点都可以通过个性化的模型比单一的全局模型更好地得到服务，从而有助于吸引当地客户。由于个性化模型是通过本地数据微调或*偏差*的，我们可以预期它在通用数据上的性能不会像全局模型那样好。另一方面，我们也可以预期个性化模型在为模型个性化设计的本地数据上的性能会优于全局模型。在用户特定性能和泛化性之间存在权衡，而FL系统的强大之处在于其灵活性，可以根据需求平衡它们。
- en: Horizontal and vertical FL
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 水平和垂直FL
- en: 'There are two types of FL: **horizontal** or **homogeneous FL** and **vertical**
    or **heterogeneous FL**. Horizontal FL, also called **sample-based FL**, is applicable
    when all local datasets connected with the aggregator server have the same features
    but contain different samples. The Gboard application discussed earlier is a good
    example of horizontal FL in the form of **cross-device FL**, that is, local training
    taking place in edge devices. The datasets in all Android phones have identical
    formats but unique contents that reflect their user’s typing history. On the other
    hand, vertical FL, or **feature-based FL**, is a more advanced technology that
    allows parties holding different features for the same samples to cooperatively
    generate a global model.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: FL 有两种类型：**水平**或**同质**FL 和 **垂直**或**异质**FL。水平 FL，也称为**基于样本的 FL**，适用于所有与聚合服务器连接的本地数据集具有相同的特征但包含不同的样本的情况。前面讨论的
    Gboard 应用就是一个很好的水平 FL 的例子，即**跨设备 FL**，也就是说，本地训练发生在边缘设备上。所有安卓手机的数据库格式相同，但内容独特，反映了用户的打字历史。另一方面，垂直
    FL，或**基于特征的 FL**，是一种更先进的技术，允许持有相同样本的不同特征的各方合作生成一个全局模型。
- en: 'For example, a bank and an e-commerce company might both store the data of
    residents in a city but their features would differ: the former knows the credit
    and expenditure patterns of the citizens, the latter their shopping behavior.
    Both of them can benefit by sharing valuable insights without sharing customer
    data. First, the bank and e-commerce company can identify their common users with
    a technique called **private set intersection** (**PSI**) while preserving data
    privacy using **Rivest-Shamir-Adleman** (**RSA**) encryption. Next, each party
    trains a preliminary model with local data containing unique features. Those models
    are then aggregated to construct a global model. Usually, vertical FL involves
    multiple data silos, and when that is the case, it is also called **cross-silo
    FL**. In China, **federated Ai ecosystem** (**FATE**) is well known for its seminal
    demonstration of vertical FL involving WeBank. If you are interested in further
    conceptual details of FL, there is a very illustrative and well-written report
    by Cloudera Fast Forward Labs, at https://federated.fastforwardlabs.com/.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一家银行和一家电子商务公司可能都存储了一个城市的居民数据，但他们的特征会有所不同：前者了解公民的信用和支出模式，后者了解他们的购物行为。他们都可以通过共享有价值的见解而无需共享客户数据来受益。首先，银行和电子商务公司可以使用**私有集合交集**（**PSI**）技术来识别他们的共同用户，同时使用**Rivest-Shamir-Adleman**（**RSA**）加密来保护数据隐私。接下来，每个当事人使用包含独特特征的本地数据训练一个初步模型。然后，这些模型被聚合起来构建一个全局模型。通常，垂直
    FL 涉及多个数据孤岛，在这种情况下，它也被称为**跨孤岛 FL**。在中国，**联邦人工智能生态系统**（**FATE**）因其涉及 WeBank 的垂直
    FL 的开创性演示而闻名。如果您对 FL 的进一步概念细节感兴趣，Cloudera Fast Forward Labs 有一个非常说明性和文笔优美的报告，网址为
    https://federated.fastforwardlabs.com/。
- en: The information on FL contained in this section should be sufficient to understand
    the following chapters, which examine, in further depth, some of the key concepts
    introduced here. The final section of the chapter aims to cover some of the auxiliary
    concepts focused on the practical application of FL.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中包含的 FL 信息应该足以理解以下章节，这些章节将进一步深入探讨此处介绍的一些关键概念。本章的最后部分旨在涵盖一些关注 FL 实际应用的辅助概念。
- en: FL system considerations
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FL 系统考虑因素
- en: This section mainly focuses on the multi-party computation aspects of FL, including
    theoretical security measures and full decentralization approaches. The goal of
    this section is for you to be aware of some of the more practical considerations
    that should be taken into account for practical FL applications.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 本节主要关注 FL 的多方计算方面，包括理论安全措施和完全去中心化方法。本节的目标是让您意识到一些在实际 FL 应用中应考虑的更实际的考虑因素。
- en: Security for FL systems
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FL 系统的安全性
- en: Despite the nascency of the technology, experimental usage of FL has emerged
    in a few sectors. Specifically, **anti-money laundering** (**AML**) in the financial
    industry and drug discovery and diagnosis in the medical industry have seen promising
    results, as proofs of concepts in those fields have been successfully conducted
    by companies such as Consilient and Owkin. In AML use cases, banks can cooperate
    with one another to identify fraudulent transactions efficiently without sharing
    their account data; and hospitals can keep their patient data to themselves while
    improving ML models for detecting health issues.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这项技术尚处于起步阶段，但FL在一些领域的实验性使用已经出现。具体来说，金融行业的反洗钱（AML）和医疗行业的药物发现与诊断已经看到了有希望的结果，正如Consilient和Owkin等公司在该领域的概念验证已经成功实施。在AML用例中，银行可以相互合作，以高效地识别欺诈交易，而无需共享他们的账户数据；医院可以在保持其患者数据的同时，提高检测健康问题的机器学习模型。
- en: These solutions exploit the power of relatively simple horizontal cross-silo
    FL, as explained in the *Understanding FL* section, and its application is spreading
    to other areas. For example, Edgify is a UK-based company contributing to the
    automation of cashiers at retail stores in collaboration with Intel and Hewlett
    Packard. In Munich, Germany, another UK-based company, Fetch.ai, is developing
    a smart city infrastructure with their FL-based technology. It is clear that the
    practical application of FL is rapidly growing.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这些解决方案利用了相对简单的横向跨领域FL的力量，如*理解FL*部分所述，其应用正在扩展到其他领域。例如，Edgify是一家位于英国的公司，与英特尔和惠普合作，致力于自动化零售店的收银员。在德国慕尼黑，另一家英国公司Fetch.ai正在利用其基于FL的技术开发智能城市基础设施。很明显，FL的实际应用正在迅速增长。
- en: Although FL can circumvent the concern over data privacy thanks to its privacy-by-design
    (model parameters do not expose privacy) and data minimalist (data is not collected
    in the central server) approach, as discussed in [*Chapter 1*](B18369_01.xhtml#_idTextAnchor017),
    *Challenges in Big Data and Traditional AI*, there are potential obstructions
    against its implementation; one such example is *mistrust* among the participants
    of an FL project. Consider a situation where *Bank A* and *Bank B* agree to use
    FL for developing a collaborative AML solution. They decide on the common model
    architecture so that each can train a local model with their own data and aggregate
    the results to create a global model to be used by both.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管FL可以通过其隐私设计（模型参数不暴露隐私）和数据最小化（数据不在中央服务器收集）的方法绕过对数据隐私的担忧，正如在[*第一章*](B18369_01.xhtml#_idTextAnchor017)“大数据和传统AI的挑战”中讨论的那样，但其实施可能存在潜在的障碍；其中一个例子是FL项目参与者之间的*不信任*。考虑一种情况，*银行A*和*银行B*同意使用FL来开发一个协作式反洗钱（AML）解决方案。他们决定采用共同的模型架构，以便每个银行都可以使用自己的数据训练本地模型，并将结果汇总以创建一个全局模型供双方使用。
- en: Naïve implementations of FL might allow for one bank to reconstruct the local
    model from the other bank, using their local model and the aggregate model. From
    this, the bank might be able to extract key information on the data used to train
    the other bank’s model. As a result, there might be a dispute regarding which
    party should host the server to aggregate the local models. A possible solution
    is having a third party host the server and take responsibility for model aggregation.
    Yet, how would *Bank A* know that the third party is not colluding with *Bank
    B*, and vice versa? Going further, the integration of an FL system into a security-focused
    domain leads to new concerns regarding the security and stability of each system
    component. Known security issues tied to different FL system approaches might
    incur an additional potential weakness to adversarial attacks that outweighs the
    benefits of the approach.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: FL的简单实现可能允许一个银行使用其本地模型和聚合模型从另一个银行重建本地模型。据此，该银行可能能够从用于训练另一个银行模型的训练数据中提取关键信息。因此，可能会出现关于哪个方应该托管服务器以聚合本地模型的争议。一个可能的解决方案是由第三方托管服务器并负责模型聚合。然而，*银行A*如何知道第三方没有与*银行B*勾结，反之亦然？进一步来说，将FL系统集成到以安全为重点的领域会导致对每个系统组件的安全性和稳定性的新担忧。与不同FL系统方法相关的已知安全问题可能会给对抗性攻击带来额外的潜在弱点，这种弱点超过了该方法的益处。
- en: There are several security measures to allow FL collaboration without forcing
    the participants to trust one another. With a statistical method called **differential
    privacy** (**DP**), each participant can add random noise to their local model
    parameters to prevent the ability to glean information on the training data distribution
    or specific elements from the transmitted parameters. By sampling the random noise
    from a symmetric distribution with zero mean and relatively low variance (for
    example, Gaussian, Laplace), the random differences added to the local models
    are expected to cancel out when aggregation is performed. As a result, the global
    model is expected to be very similar to what would have been generated without
    DP.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种安全措施可以在不强迫参与者相互信任的情况下允许FL协作。使用一种名为**差分隐私**（**DP**）的统计方法，每个参与者可以向他们的本地模型参数添加随机噪声，以防止从传输的参数中获取训练数据分布或特定元素的信息。通过从具有零均值和相对较低方差（例如高斯、拉普拉斯）的对称分布中采样随机噪声，预期在聚合时添加到本地模型中的随机差异将被抵消。因此，预期全局模型将与没有DP生成的模型非常相似。
- en: However, there is a critical limitation to this approach; for the sum of the
    added random noise to converge to zero, a sufficient number of parties must participate
    in the coalition. This might not be the case for projects involving only a few
    banks or hospitals, and using DP in such cases would harm the global model’s integrity.
    Some additional measures would be necessary, for example, each participant sending
    multiple copies of their local model to increase the number of models so that
    the noise will be offset. Another possibility in certain fully-decentralized FL
    systems is **secure multi-party computation** (**MPC**). MPC-based aggregation
    allows agents to communicate among themselves and compute the aggregate model
    without involving a trusted third-party server, maintaining model parameter privacy.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法存在一个关键的局限性；为了使添加的随机噪声收敛到零，必须足够多的参与者加入联盟。对于仅涉及少数银行或医院的工程项目来说，这种情况可能并不适用，在这种情况下使用差分隐私（DP）会损害全局模型的完整性。可能需要采取一些额外的措施，例如，每个参与者发送他们本地模型的多个副本以增加模型数量，从而使噪声得到抵消。在某些完全去中心化的联邦学习（FL）系统中，另一种可能性是**安全多方计算**（**MPC**）。基于MPC的聚合允许代理之间相互通信并计算聚合模型，而不涉及可信的第三方服务器，从而保持模型参数的隐私性。
- en: 'How could the participants secure the system from outside attacks? **Homomorphic
    encryption** (**HE**), which preserves the effects of addition and multiplication
    on data across encryption, allows the local models to be aggregated into the global
    model in an encrypted form. This precludes the exposure of model parameters to
    outsiders who do not possess the key for decryption. Yet, HE’s effectiveness in
    securing the communication between the participants comes with a prohibitively
    high computational cost: processing the operation on data with the HE algorithm
    can take hundreds of trillions of times longer than otherwise!'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 参与者如何确保系统免受外部攻击？**同态加密**（**HE**）在加密过程中保留了加法和乘法对数据的影响，允许以加密形式将本地模型聚合到全局模型中。这阻止了模型参数暴露给没有解密密钥的外部人员。然而，HE在保护参与者之间通信方面的有效性伴随着极高的计算成本：使用HE算法处理数据的操作可能比其他方式慢数百万亿倍！
- en: A solution to mitigate this challenge is the use of partial HE, which is compatible
    with only one of the additive or multiplicative operations across encryption;
    therefore it is computationally much lighter than the fully homomorphic counterpart.
    Using this scheme, each participant in a coalition can encrypt and send their
    local model to the aggregator, which then sums up all local models and sends the
    aggregated model back to the participants, who, in turn, decrypt the model and
    divide its parameters by the number of participants to receive the global model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这一挑战，可以使用部分同态加密（HE），它仅与加密中的加法或乘法操作之一兼容；因此，它在计算上比完全同态加密轻得多。使用这种方案，联盟中的每个参与者都可以加密并发送他们的本地模型到聚合器，然后聚合器将所有本地模型相加，并将聚合模型发送回参与者，参与者随后解密模型并将参数除以参与者的数量以接收全局模型。
- en: Both HE and DP are essential technology for the practical application of FL.
    Those interested in the implementation of FL in real-world scenarios can learn
    a great deal from *Federated AI for Real-World Business Scenarios* written by
    IBM Research Fellow Dinesh C. Verma.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: HE和DP都是FL在实际应用中的关键技术。对FL在现实场景中实施感兴趣的人可以从IBM研究学者Dinesh C. Verma所著的《*Federated
    AI for Real-World Business Scenarios*》一书中学到很多。
- en: Decentralized FL and blockchain
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去中心化FL和区块链
- en: The architecture of FL discussed so far is based on client-server networks,
    that is, edge devices exchanging models with a central aggregator server. Due
    to the issues surrounding trust between the participants of FL coalitions discussed
    earlier; however, building a system with an aggregator as a separate and central
    entity can be problematic. It can be difficult for the host of an aggregator to
    be impartial and unbiased toward their own data. Also, having a central server
    inevitably leads to a single point of failure in the FL system, which results
    in low resilience. Furthermore, if the aggregator is set up in a cloud server,
    the implementation of such an FL system would require a skilled DevOps engineer,
    who might be difficult to find and expensive to hire.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止讨论的FL架构是基于客户端-服务器网络，即边缘设备与中央聚合服务器交换模型。然而，由于之前讨论的FL联盟参与者之间的信任问题；然而，建立一个以聚合器作为单独和中央实体的系统可能会出现问题。聚合器的主持人可能难以保持公正和无偏见地对待自己的数据。此外，拥有一个中央服务器不可避免地会导致FL系统中的单点故障，这导致系统弹性低。此外，如果聚合器设置在云服务器上，实施这样的FL系统将需要一个熟练的DevOps工程师，这可能很难找到且成本高昂。
- en: 'Given these concerns, Kiyoshi Nakayama (the primary author of this book) co-authored
    an article about the first-ever experimentation of a fully decentralized FL system
    using blockchain technology ([http://www.kiyoshi-nakayama.com/publications/BAFFLE.pdf](http://www.kiyoshi-nakayama.com/publications/BAFFLE.pdf)).
    Leveraging **smart contracts** to coordinate model updates and aggregation, a
    private Ethereum network was constructed to perform FL in a serverless manner.
    The results of the experiment showed that a peer-to-peer, decentralized FL can
    be much more efficient and scalable than an aggregator-based, centralized FL.
    The superiority of decentralized architecture was confirmed in a more recent experiment
    conducted by Hewlett Packard and German research institutes who gave a unique
    name to decentralized FL with blockchain technology: **swarm learning**.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些担忧，本书的主要作者Kiyoshi Nakayama共同撰写了一篇关于首次使用区块链技术进行完全去中心化FL实验的文章([http://www.kiyoshi-nakayama.com/publications/BAFFLE.pdf](http://www.kiyoshi-nakayama.com/publications/BAFFLE.pdf))。利用**智能合约**来协调模型更新和聚合，构建了一个私有以太坊网络以无服务器方式执行FL。实验结果表明，基于对等网络的去中心化FL可以比基于聚合器的集中式FL更加高效和可扩展。在惠普和德国研究机构最近进行的一项实验中，确认了去中心化架构的优越性，他们给使用区块链技术的去中心化FL起了一个独特的名字：**swarm
    learning**。
- en: While research and development in the field of FL are shifting to a decentralized
    model, the rest of this book assumes centralized architecture with an aggregator
    server. There are two reasons for this design. First, blockchain is still a nascent
    technology that AI and ML researchers are not necessarily familiar with. Incorporating
    a peer-to-peer communication scheme can overcomplicate the subject matter. And
    second, the logic of FL itself is independent of the network architecture, and
    there is no problem with the centralized model to illustrate how FL works.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然FL领域的研发正在转向去中心化模式，但本书的其余部分假设使用聚合服务器进行集中式架构。这种设计有两个原因。首先，区块链技术仍然是一个新兴技术，AI和ML研究人员并不一定熟悉。引入对等通信方案可能会使主题过于复杂。其次，FL本身的逻辑独立于网络架构，集中式模型没有问题，可以说明FL的工作原理。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered the two key developments that have resulted from
    the recent growth in accessible computational power at all levels. First, we looked
    at the importance of models and how this has enabled ML to grow considerably in
    practical usage, with increases in computational power allowing stronger models
    that surpass manually created white-box systems to continuously be produced. We
    called this the *what* of FL – ML is what we are trying to perform using FL.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了由于所有级别的可访问计算能力的增长而带来的两个关键发展。首先，我们探讨了模型的重要性以及它是如何使ML在实用应用中显著增长，计算能力的增加允许产生比手动创建的白色盒系统更强的模型，这些模型可以持续产生。我们称之为FL的*what*
    – ML是我们试图使用FL执行的内容。
- en: Then, we took a step back to look at how edge devices are reaching a stage where
    complex computations can be performed within reasonable timeframes for real-world
    applications, such as the text recommendation models on our phones. We called
    this the *where* of FL – the setting where we want to perform ML.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们退后一步，看看边缘设备是如何达到在合理时间内执行复杂计算的阶段，这对于现实世界的应用来说是有意义的，比如我们手机上的文本推荐模型。我们称之为FL的*哪里*——我们想要执行机器学习的环境。
- en: From the what and the where, we get the intersection of these two developments
    – the usage of ML models directly on edge devices. Remember that the standard
    central training approach for ML models greatly suffers from the need to centrally
    collect all of the data in the edge ML case, as this prevents applications requiring
    efficient communication or data privacy from being possible. We showed that *FL*
    directly addresses this problem by performing all training at the edge to produce
    *local models*, at the same location as the requisite data stores. *Aggregation*
    algorithms take these local models and produce a *global model*. By iteratively
    switching between local training and aggregation, FL allows for the creation of
    a model that has effectively been trained across all data stores without ever
    needing to centrally collect the data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 从“什么”和“哪里”，我们得到这两个发展的交集——直接在边缘设备上使用机器学习模型。记住，对于边缘机器学习案例，标准的中心训练方法由于需要集中收集所有数据而严重受限，这阻止了需要高效通信或数据隐私的应用。我们展示了*FL*如何直接解决这个问题，通过在数据存储的同一位置进行所有训练以产生*本地模型*。*聚合*算法将这些本地模型组合成一个*全局模型*。通过在本地训练和聚合之间迭代切换，FL允许创建一个模型，该模型实际上已经在所有数据存储中进行过训练，而无需集中收集数据。
- en: We concluded the chapter by stepping outside the theory behind effective aggregation,
    looking at system and architecture design considerations regarding aspects such
    as model privacy and full decentralization. After reading the chapter, it should
    be clear that the current state of ML, edge computing, and fledgling growth in
    practical FL applications makes it clear that FL is poised for serious growth
    in the near future.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过跳出有效聚合背后的理论，审视系统与架构设计方面的考虑，如模型隐私和完全去中心化等方面，来结束本章。阅读完本章后，应该清楚，当前机器学习（ML）、边缘计算以及实际联邦学习（FL）应用的初步增长表明，FL在不久的将来有望实现显著增长。
- en: In the next chapter, we will examine the implementation of FL from a system-level
    perspective.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将从系统层面考察联邦学习的实现。
- en: Further reading
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To learn more about the topics that were covered in this chapter, please take
    a look at the following references:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多本章涉及的主题，请参阅以下参考文献：
- en: Ruiz, N. W. (2020, December 22). *The Future is Federated*. Medium.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruiz, N. W. (2020, 12月22日). *未来是联邦化的*. Medium.
- en: 'Contessa, G. (2010). *Scientific Models and Fictional Objects*. Synthese, 172(2):
    215–229.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Contessa, G. (2010). *科学模型与虚构对象*. Synthese, 172(2): 215–229.'
- en: Frigg, R. and Hartmann, S. (2020). *Models in Science*. In Zalta, E. N. (ed.).
    The Stanford Encyclopedia of Philosophy.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frigg, R. 和 Hartmann, S. (2020). *科学中的模型*. 在 Zalta, E. N. (编). 斯坦福哲学百科全书.
- en: 'Bunge, M. (1963). *A General Black Box Theory. Philosophy of Science*, 30(4):
    346-358.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bunge, M. (1963). *通用黑盒理论. 科学哲学*, 30(4): 346-358.'
- en: 'Moore, S. K., Schneider, D. and Strickland, E. (2021, September 28). *How Deep
    Learning Works: Inside the neural networks that power today’s AI*. IEEE Spectrum.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moore, S. K.，Schneider, D. 和 Strickland, E. (2021, 9月28日). *深度学习是如何工作的：驱动今天人工智能的神经网络内部*.
    IEEE Spectrum.
- en: 'Raschka, S. and Mirjalili, V. (2019). *Python Machine Learning: Machine Learning
    and Deep Learning with Python, scikit-learn, and TensorFlow 2 (3rd Ed.)*. Birmingham:
    Packt Publishing.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raschka, S. 和 Mirjalili, V. (2019). *Python机器学习：使用Python、scikit-learn和TensorFlow
    2进行机器学习和深度学习（第3版）*. 伯明翰：Packt Publishing.
- en: McMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. (2016).
    *Communication-efficient learning of deep networks from decentralized data*. JMLR
    WandCP, 54.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McMahan, B., Moore, E., Ramage, D., Hampson, S. 和 Arcas, B. A. (2016). *从去中心化数据中高效学习深度网络*.
    JMLR WandCP, 54.
- en: 'Huang, X., Ding, Y., Jiang, Z. L. and Qi, S. (2020). *DP-FL: a novel differentially
    private federated learning framework for the unbalanced data*. World Wide Web,
    23: 2529–254.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '黄，X.，丁，Y.，江，Z. L. 和Qi，S. (2020). *DP-FL：一种用于不平衡数据的创新差分隐私联邦学习框架*. World Wide
    Web, 23: 2529–254.'
- en: 'Yang, Q., Liu, Y., Chen, T., and Tong, Y. (2019). *Federated Machine Learning:
    Concept and Applications. ACM Transactions on Intelligent Systems and Technology
    (TIST)*, 10(2), 1-19.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨，Q.，刘，Y.，陈，T.，和童，Y. (2019). *联邦机器学习：概念与应用*. ACM智能系统与技术交易（TIST），10(2)，1-19.
- en: 'Verma, D. C. (2021). *Federated AI for Real-World Business Scenarios*. Florida:
    CRC Press.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Verma, D. C. (2021). *联邦人工智能：现实世界商业场景应用*. 佛罗里达：CRC出版社.
- en: 'Ramanan, P., and Nakayama, K. (2020). *Baffle: Blockchain Based Aggregator
    Free Federated Learning*. 2020 IEEE International Conference on Blockchain, 72-81.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ramanan, P., and Nakayama, K. (2020). *Baffle: Blockchain Based Aggregator
    Free Federated Learning*. 2020 IEEE International Conference on Blockchain, 72-81.'
- en: Warnat-Herresthal, S., Schultze, H., Shastry, K. L., Manamohan, S., Mukherjee,
    S., Garg, V., and Schultze, J. L. (2021). *Swarm Learning for decentralized and
    confidential clinical machine learning. Nature*, 594(7862), 265-270.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Warnat-Herresthal, S.，Schultze, H.，Shastry, K. L.，Manamohan, S.，Mukherjee, S.，Garg,
    V.，和Schultze, J. L. (2021). *群智学习：用于去中心化和机密临床机器学习*. 自然，594(7862)，265-270.
