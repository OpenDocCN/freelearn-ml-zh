- en: Discovering Hidden Structures with Unsupervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用无监督学习发现隐藏结构
- en: So far, we have focused our attention exclusively on supervised learning problems,
    where every data point in the dataset had a known label or target value. However,
    what do we do when there is no known output or no teacher to supervise the learning
    algorithm?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于监督学习问题，其中数据集中的每个数据点都有一个已知的标签或目标值。然而，当没有已知输出或没有教师来监督学习算法时，我们该怎么办？
- en: This is what **unsupervised learning** is all about. In unsupervised learning,
    the learning process is shown only in the input data and is asked to extract knowledge
    from this data without further instruction. We have already talked about one of
    the many forms that unsupervised learning comes in—**dimensionality reduction**.
    Another popular domain is **cluster analysis**, which aims to partition data into
    distinct groups of similar items.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是**无监督学习**的全部内容。在无监督学习中，学习过程仅体现在输入数据中，并要求从这些数据中提取知识，而不需要进一步的指令。我们已经讨论了无监督学习的一种形式——**降维**。另一个流行的领域是**聚类分析**，其目的是将数据划分为不同的相似项组。
- en: Some of the problems where clustering techniques can be useful are document
    analysis, image retrieval, finding spam emails, identifying fake news, identifying
    criminal activities, and so on.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类技术可能有用的一些问题包括文档分析、图像检索、查找垃圾邮件、识别虚假新闻、识别犯罪活动等等。
- en: In this chapter, we want to understand how different clustering algorithms can
    be used to extract hidden structures in simple, unlabeled datasets. These hidden
    structures have many benefits, whether they are used in feature extraction, image
    processing, or even as a preprocessing step for supervised learning tasks. As
    a concrete example, we will learn how to apply clustering to images to reduce
    their color spaces to 16 bits.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们希望了解如何使用不同的聚类算法从简单的未标记数据集中提取隐藏结构。这些隐藏结构具有许多好处，无论是用于特征提取、图像处理，还是作为监督学习任务的预处理步骤。作为一个具体的例子，我们将学习如何将聚类应用于图像，以将它们的颜色空间减少到16位。
- en: 'More specifically, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们将涵盖以下主题：
- en: '**k-means clustering** and **expectation-maximization** and implementing these
    in OpenCV'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**k-means聚类**和**期望最大化**，以及在OpenCV中实现这些算法'
- en: Arranging clustering algorithms in hierarchical trees and what are the benefits
    that come from that
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将聚类算法排列成层次树，以及由此带来的好处
- en: Using unsupervised learning for preprocessing, image processing, and classification
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用无监督学习进行预处理、图像处理和分类
- en: Let's get started!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Using TF-IDF to improve the result
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TF-IDF来提高结果
- en: It was called the **Term Frequency-Inverse Document Frequency** (**TF****-IDF**),
    and we encountered it in [Chapter 4](142fec63-a847-4cde-9de9-c34805d2bb84.xhtml),
    *Representing Data and Engineering Features*. If you recall, what TF-IDF does
    is basically weigh the word count by a measure of how often the words appear in
    the entire dataset. A useful side effect of this method is the IDF part—the inverse
    frequency of words. This makes sure that frequent words, such as *and*, *the*,
    and *but*, carry only a small weight in the classification.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 它被称为**词频-逆文档频率**（**TF-IDF**），我们在[第4章](142fec63-a847-4cde-9de9-c34805d2bb84.xhtml)中遇到了它，*表示数据和工程特征*。如果你还记得，TF-IDF的基本作用是通过对整个数据集中单词出现频率的度量来权衡单词计数。这种方法的一个有用副作用是IDF部分——单词的逆频率。这确保了像*和*、*the*和*but*这样的常用词在分类中只占很小的权重。
- en: 'We apply TF-IDF to the feature matrix by calling `fit_transform` on our existing
    feature matrix, `X`:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在现有的特征矩阵`X`上调用`fit_transform`来应用TF-IDF：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Don't forget to split the data; also, ...
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记分割数据；另外，...
- en: Summary
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we took our first look at probability theory, learning about
    random variables and conditional probabilities, which allowed us to get a glimpse
    of Bayes' theorem—the underpinning of a Naive Bayes classifier. We talked about
    the difference between discrete and continuous random variables, likelihoods and
    probabilities, priors and evidence, and normal and Naive Bayes classifiers.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首次接触了概率论，了解了随机变量和条件概率，这使我们得以一瞥贝叶斯定理——朴素贝叶斯分类器的基石。我们讨论了离散随机变量和连续随机变量、似然性和概率、先验和证据、以及正态和朴素贝叶斯分类器之间的区别。
- en: Finally, our theoretical knowledge would be of no use if we didn't apply it
    to practical examples. We obtained a dataset of raw email messages, parsed it,
    and trained Bayesian classifiers on it to classify emails as either spam or ham
    (not spam) using a variety of feature extraction approaches.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果我们不将理论知识应用于实际例子，那么我们的理论知识将毫无用处。我们获得了一组原始电子邮件数据集，对其进行了解析，并在其上训练了贝叶斯分类器，以使用各种特征提取方法将电子邮件分类为垃圾邮件或非垃圾邮件（非垃圾邮件）。
- en: In the next chapter, we will switch gears and, for once, discuss what to do
    if we have to deal with unlabeled data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将转换方向，并讨论一次，如果我们必须处理未标记的数据时应该做什么。
- en: Technical requirements
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can refer to the code for this chapter from the following link: [https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter08](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter08).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从以下链接获取本章的代码：[https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter08](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter08)。
- en: 'Here is a summary of the software and hardware requirements:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是软件和硬件要求的总结：
- en: You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要 OpenCV 版本 4.1.x（4.1.0 或 4.1.1 都可以正常工作）。
- en: You will need Python version 3.6 (any Python version 3.x will be fine).
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要 Python 版本 3.6（任何 3.x 版本的 Python 都可以）。
- en: You will need Anaconda Python 3 for installing Python and the required modules.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要 Anaconda Python 3 来安装 Python 和所需的模块。
- en: You can use any operating system—macOS, Windows, and Linux-based OSes—along
    with this book. We recommend you have at least 4 GB RAM in your system.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用任何操作系统——macOS、Windows 和基于 Linux 的操作系统——以及本书。我们建议您的系统至少有 4 GB 的 RAM。
- en: You don't need to have a GPU to run the code provided along with this book.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您不需要 GPU 来运行本书附带提供的代码。
- en: Understanding unsupervised learning
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解无监督学习
- en: Unsupervised learning might come in many shapes and forms, but the goal is always
    to convert original data into a richer, more meaningful representation, whether
    that means making it easier for humans to understand or easier for machine learning
    algorithms to parse.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习可能以多种形式出现，但其目标始终是将原始数据转换为更丰富、更有意义的表示，无论是使人类更容易理解，还是使机器学习算法更容易解析。
- en: 'Some common applications of unsupervised learning include the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习的常见应用包括以下内容：
- en: '**Dimensionality reduction**: This takes a high-dimensional representation
    of data consisting of many features and tries to compress the data so that its
    main characteristics can be explained with a small number of highly informative
    features. For example, when applied to housing prices in the neighborhoods of
    Boston, dimensionality reduction might be able to tell us that the indicators
    we should pay most attention to are the property tax and the neighborhood''s crime
    rate.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：这尝试将包含许多特征的高维数据表示压缩，以便可以用少量高度信息化的特征解释其主要特征。例如，当应用于波士顿地区的房价时，降维可能能够告诉我们，我们应该最关注的指标是房产税和该地区的犯罪率。'
- en: '**Factor analysis**: This tries to find the hidden causes or unobserved components
    that gave rise to the observed data. For example, when applied to all of the episodes
    of the 1970s TV show, *Scooby-Doo, Where Are You!*, factor analysis might be able
    to tell us that (spoiler alert!) every ghost or monster on the show is essentially
    some disgruntled count playing an elaborate hoax on the town.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**因子分析**：这试图找到导致观察数据的隐藏原因或未观察到的成分。例如，当应用于 1970 年代电视节目《Scooby-Doo，Where Are
    You！》的所有剧集时，因子分析可能能够告诉我们（剧透警告！）节目中的每个鬼魂或怪物本质上都是某个不满的伯爵在镇上玩的一个复杂的恶作剧。'
- en: '**Cluster analysis**: This tries to partition the data into distinct groups
    of similar items. This is the type of unsupervised learning we will focus on in
    this chapter. For example, when applied to all of the movies on Netflix, cluster
    analysis might be able to automatically group them into genres.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类分析**：这试图将数据划分为不同的相似项目组。这是我们将在本章中关注的无监督学习类型。例如，当应用于 Netflix 上的所有电影时，聚类分析可能能够自动将它们按类型分组。'
- en: To make things more complicated, these analyses have to be performed on unlabeled
    data, where we do not know beforehand what the right answer should be. Consequently,
    a major challenge in unsupervised learning is to determine whether an algorithm
    did well or learned anything useful. Often, the only way to evaluate the result
    of an unsupervised learning algorithm is to inspect it manually and determine
    by hand whether the result makes sense.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要使事情更复杂，这些分析必须在未标记的数据上执行，我们事先不知道正确的答案应该是什么。因此，无监督学习中的一个主要挑战是确定算法是否表现良好或学到了有用的东西。通常，评估无监督学习算法结果的方法是手动检查并确定结果是否有意义。
- en: That being said, unsupervised learning can be immensely helpful, for example,
    as a preprocessing or feature extraction step. You can think of unsupervised learning
    as a **data transformation**—a way to transform data from its original representation
    into a more informative form. Learning a new representation might give us deeper
    insights into our data, and sometimes, it might even improve the accuracy of supervised
    learning algorithms.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，无监督学习可以非常有帮助，例如，作为预处理或特征提取步骤。你可以把无监督学习看作是一种**数据转换**——一种将数据从其原始表示转换为更信息丰富的形式的方法。学习新的表示可能会让我们对数据有更深的洞察，有时，它甚至可能提高监督学习算法的准确性。
- en: Understanding k-means clustering
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解k-means聚类
- en: The essential clustering algorithm that OpenCV provides is *k*-means clustering, which
    searches for a predestined number of *k-*clusters (or groups) from an unlabeled
    multi-dimensional data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV提供的核心聚类算法是k-means聚类，它从一个未标记的多维数据中搜索预定的k个簇（或组）。
- en: 'It achieves this by using two simple hypotheses about what optimal clustering
    should look like:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过使用关于最佳聚类应该看起来像的两个简单假设来实现这一点：
- en: The center of each cluster is basically the mean of all of the points belonging
    to that cluster, also known as the centroid.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个簇的中心基本上是该簇所有点的平均值，也称为质心。
- en: Each data point in that cluster is closer to its center than to all other cluster
    centers.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该簇中的每个数据点与其中心比与其他所有簇中心更近。
- en: It's easiest to understand the algorithm by looking at a concrete example.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看一个具体的例子，最容易理解该算法。
- en: Implementing our first k-means example
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现我们的第一个k-means示例
- en: 'First, let''s generate a 2D dataset containing four distinct blobs. To emphasize
    that this is an unsupervised approach, we will leave the labels out of the visualization:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们生成一个包含四个不同blob的2D数据集。为了强调这是一个无监督方法，我们将省略可视化中的标签：
- en: 'We will continue using `matplotlib` for all of our visualization purposes:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将继续使用`matplotlib`来完成所有的可视化目的：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Following the same recipe from earlier chapters, we will create a total of
    300 blobs (`n_samples=300`) belonging to four distinct clusters (`centers=4`):'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照前面章节的相同方法，我们将创建总共300个属于四个不同簇的blob（`n_samples=300`）：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will generate the following diagram:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![](img/1479c696-e19b-4992-bbfe-1d4e38a58124.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1479c696-e19b-4992-bbfe-1d4e38a58124.png)'
- en: The preceding diagram shows an example dataset of 300 unlabeled points organized
    into four distinct clusters. Even without assigning target labels to the data,
    it is straightforward to pick out the four clusters by eye. The *k*-means algorithm
    can do this, too, without having any information about target labels or underlying
    data distributions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表显示了300个未标记点组成的示例数据集，这些点组织成四个不同的簇。即使没有将目标标签分配给数据，也可以通过肉眼轻松地挑选出这四个簇。k-means算法也能做到这一点，而无需任何关于目标标签或潜在数据分布的信息。
- en: 'Although *k*-means is, of course, a statistical model, in OpenCV, it does not
    come via the `ml` module and the common `train` and `predict` API calls. Instead,
    it is directly available as `cv2.kmeans`. To use the model, we have to specify
    some arguments, such as the termination criteria and some initialization flags.
    Here, we will tell the algorithm to terminate whenever the error is smaller than
    1.0 (`cv2.TERM_CRITERIA_EPS`) or when ten iterations have been executed (`cv2.TERM_CRITERIA_MAX_ITER`):'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然k-means当然是一个统计模型，但在OpenCV中，它不是通过`ml`模块和常见的`train`和`predict` API调用提供的。相反，它直接作为`cv2.kmeans`可用。要使用该模型，我们必须指定一些参数，例如终止准则和一些初始化标志。在这里，我们将告诉算法，当误差小于1.0（`cv2.TERM_CRITERIA_EPS`）或执行了十个迭代时（`cv2.TERM_CRITERIA_MAX_ITER`）终止：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, we can pass the preceding data matrix (`X`) to `cv2.means`. We also specify
    the number of clusters (`4`) and the number of attempts the algorithm should make
    with different random initial guesses (`10`), as shown in the following snippet:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以将前面的数据矩阵（`X`）传递给`cv2.means`。我们还需要指定簇的数量（`4`）以及算法应该尝试的不同随机初始猜测的数量（`10`），如下面的代码片段所示：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Three different variables are returned.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 返回三个不同的变量。
- en: 'The first one, `compactness`, returns the sum of squared distances from each
    point to their corresponding cluster centers. A high compactness score indicates
    that all points are close to their cluster centers, whereas a low compactness
    score indicates that the different clusters might not be well separated:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个指标，`紧密度`，返回每个点到其对应簇中心的距离平方和。高紧密度分数表明所有点都靠近其簇中心，而低紧密度分数则表明不同的簇可能没有很好地分离：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Of course, this number strongly depends on the actual values in `X`. If the
    distances between points were large, to begin with, we could not expect an arbitrarily
    small compactness score. Hence, it is more informative to plot the data points,
    colored to their assigned cluster labels:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当然，这个数字强烈依赖于`X`中的实际值。如果点之间的距离一开始就很大，我们就不期望有一个任意小的紧密度分数。因此，将数据点着色到它们分配的簇标签上绘制会更具有信息量：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This produces a scatter plot of all of the data points colored according to
    whichever cluster they belong to, with the corresponding cluster centers indicated
    with a blob of a darker shade in the center of each cluster:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会产生一个散点图，所有数据点根据它们所属的簇进行着色，相应的簇中心在每个簇的中心用较深的阴影表示：
- en: '![](img/9305c4e4-b92e-45a9-865b-ffd047e3ca5e.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9305c4e4-b92e-45a9-865b-ffd047e3ca5e.png)'
- en: The preceding diagram shows the result of *k*-means clustering for *k=4*. The
    good news here is that the *k*-means algorithm (at least, in this simple case)
    assigns the points to clusters very similarly to how we might have, had we done
    the job by eye. But how did the algorithm find these different clusters so quickly?
    After all, the number of possible combinations of cluster assignments is exponential
    to the number of data points! By hand, trying all possible combinations would
    have certainly taken forever.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表显示了*k*-means聚类对于*k=4*的结果。好消息是，k-means算法（至少在这个简单的情况下）将点分配到簇中，与我们用肉眼做的工作非常相似。但算法是如何如此快速地找到这些不同簇的呢？毕竟，簇分配的可能组合数量是数据点数量的指数级！手动尝试所有可能的组合肯定需要很长时间。
- en: Fortunately, an exhaustive search is not necessary. Instead, the typical approach
    that *k*-means takes is to use an iterative algorithm, also known as **expectation-maximization**.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，不需要进行穷举搜索。相反，k-means通常采用迭代算法，也称为**期望最大化**。
- en: Understanding expectation-maximization
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解期望最大化
- en: '*k*-means clustering is but one concrete application of a more general algorithm
    known as expectation-maximization. In short, the algorithm works as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means聚类是更一般算法的一个具体应用，该算法称为**期望最大化**。简而言之，算法的工作原理如下：'
- en: Start with some random cluster centers.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一些随机的簇中心开始。
- en: 'Repeat until convergence:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复直到收敛：
- en: '**Expectation step**: Assign all data points to their nearest cluster center.'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**期望步骤**：将所有数据点分配到它们最近的簇中心。'
- en: '**Maximization step**: Update the cluster centers by taking the mean of all
    of the points in the cluster.'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大化步骤**：通过取簇中所有点的平均值来更新簇中心。'
- en: Here, the expectation step is so named because it involves updating our expectation
    of which cluster each point in the dataset belongs to. The maximization step is
    so named because it involves maximizing a fitness function that defines the location
    of the cluster centers. In the case of *k*-means, maximization ...
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，期望步骤之所以称为期望步骤，是因为它涉及到更新我们对数据集中每个点所属簇的期望。最大化步骤之所以称为最大化步骤，是因为它涉及到最大化一个定义簇中心位置的适应度函数。在k-means的情况下，最大化...
- en: Implementing our expectation-maximization solution
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现我们的期望最大化解决方案
- en: 'The expectation-maximization algorithm is simple enough for us to code it ourselves.
    To do so, we will define a function, `find_clusters(X, n_clusters, rseed=5)`,
    that takes as input a data matrix (`X`), the number of clusters we want to discover
    (`n_clusters`), and a random seed (optional, `rseed`). As will become clear in
    a second, scikit-learn''s `pairwise_distances_argmin` function will come in handy:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 期望最大化算法足够简单，以至于我们可以自己编写代码来实现它。为此，我们将定义一个函数，`find_clusters(X, n_clusters, rseed=5)`，它接受一个数据矩阵（`X`）、我们想要发现的聚类数量（`n_clusters`）和一个随机种子（可选，`rseed`）。在接下来的内容中将会变得清晰，scikit-learn
    的 `pairwise_distances_argmin` 函数将会非常有用：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can implement expectation-maximization for *k*-means in five essential steps:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在五个基本步骤中实现 *k*-means 的期望最大化：
- en: '**Initialization**: Randomly choose a number of cluster centers, `n_clusters`.
    We don''t just pick any random number but instead pick actual data points to be
    the cluster centers. We do this by permuting `X` along its first axis and picking
    the first `n_clusters` points in this random permutation:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化**：随机选择一个聚类中心数量，`n_clusters`。我们不仅仅选择任何随机数，而是选择实际的数据点作为聚类中心。我们通过沿其第一个轴对
    `X` 进行排列，并选择这个随机排列中的前 `n_clusters` 个点来实现这一点：'
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**`while` looping forever**: Assign labels based on the closest cluster centers.
    Here, scikit-learn''s `pairwise_distance_argmin` function does exactly what we
    want. It computes, for each data point in `X`, the index of the closest cluster
    center in `centers`:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**`while` 循环无限进行**：根据最近的聚类中心分配标签。在这里，scikit-learn 的 `pairwise_distance_argmin`
    函数正好做了我们想要的事情。它计算 `X` 中每个数据点在 `centers` 中的最近聚类中心的索引：'
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Find the new cluster centers**: In this step, we have to take the arithmetic
    mean of all data points in `X` that belong to a specific cluster (`X[labels ==
    i]`):'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**找到新的聚类中心**：在这个步骤中，我们必须计算属于特定聚类（`X[labels == i]`）的所有数据点的算术平均值：'
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Check for convergence and break the** `while` **loop if necessary**: This
    is the last step to make sure that we stop the execution of the algorithm once
    the job is done. We determine whether the job is done by checking whether all
    of the new cluster centers are equal to the old cluster centers. If this is true,
    we exit the loop; otherwise, we keep looping:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检查收敛性并在必要时中断** `while` **循环**：这是确保算法在任务完成后停止执行的最后一步。我们通过检查所有新的聚类中心是否等于旧的聚类中心来确定任务是否完成。如果是这样，我们退出循环；否则，我们继续循环：'
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Exit the function and return the result:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 退出函数并返回结果：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can apply our function to the preceding data matrix, `X`, we created. Since
    we know what the data looks like, we know that we are looking for four clusters:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将我们的函数应用于之前创建的数据矩阵，`X`。由于我们知道数据的样子，我们知道我们正在寻找四个聚类：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This will generate the following plot. The vital point to observe from the
    following diagram is that, before applying *k*-means clustering, all data points
    were categorized to the same one color; however, after using *k*-means clustering,
    each color is a different cluster (similar data points are clustered or grouped
    in one color) :'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表。从以下图表中观察到的关键点是，在应用 *k*-means 聚类之前，所有数据点都被分类为相同的颜色；然而，在使用 *k*-means
    聚类之后，每种颜色代表一个不同的聚类（相似的数据点被聚集成一个颜色）：
- en: '![](img/9b09ee68-3da7-4eaf-b90b-324b62fa1266.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b09ee68-3da7-4eaf-b90b-324b62fa1266.png)'
- en: The preceding diagram shows the outcome of our home-made *k*-means using expectation-maximization.
    As we can see, our home-made algorithm got the job done! Granted, this particular
    clustering example was fairly easy, and most real-life implementations of *k*-means
    clustering will do a bit more under the hood. But for now, we are happy.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表显示了我们的自制 *k*-means 使用期望最大化算法的结果。正如我们所看到的，我们的自制算法完成了任务！当然，这个特定的聚类示例相当简单，大多数现实生活中的
    *k*-means 聚类实现将在幕后做更多的工作。但就目前而言，我们很满意。
- en: Knowing the limitations of expectation-maximization
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解期望最大化的局限性
- en: 'For all its simplicity, expectation-maximization performs incredibly well in
    a range of scenarios. That being said, there are some potential limitations that
    we need to be aware of:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管期望最大化非常简单，但在一系列场景中表现惊人。然而，我们必须意识到一些潜在的局限性：
- en: Expectation-maximization does not guarantee that we will find the globally best
    solution.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期望最大化不能保证我们找到全局最优解。
- en: We must know the number of desired clusters beforehand.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须事先知道所需聚类的数量。
- en: The decision boundaries of the algorithms are linear.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法的决策边界是线性的。
- en: The algorithm is slow for large datasets.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大数据集，算法运行缓慢。
- en: Let's quickly discuss these potential caveats in a little more detail.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地快速讨论这些潜在的注意事项。
- en: The first caveat – no guarantee of finding the global optimum
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一个注意事项——无法保证找到全局最优解
- en: 'Although mathematicians have proved that the expectation-maximization step
    improves the result in each step, there is still no guarantee that, in the end,
    we will find the global best solution. For example, if we use a different random
    seed in our simple example (such as using seed `10` instead of `5`), we suddenly
    get very poor results:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数学家已经证明期望最大化步骤在每一步都会提高结果，但仍然不能保证最终我们会找到全局最佳解。例如，如果我们在我们简单的例子中使用不同的随机种子（例如使用种子
    `10` 而不是 `5`），我们突然得到非常差的结果：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will generate the following diagram:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图：
- en: '![](img/ec702b4a-9401-4088-b676-9f5b5149ee5d.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec702b4a-9401-4088-b676-9f5b5149ee5d.png)'
- en: The preceding diagram shows an example of *k*-means missing the global optimum.
    What happened?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图显示了 *k*-means 未能找到全局最优解的例子。发生了什么？
- en: The short answer is that the random initialization of cluster centers was unfortunate.
    It led to the center of the yellow cluster migrating in-between the two top blobs,
    essentially combining them into one. As a result, the other clusters got confused
    because they suddenly had to split two visually distinct blobs into three clusters.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 简短的回答是，聚类中心的随机初始化是不幸的。它导致黄色聚类的中心在两个顶部云团之间迁移，实际上将它们合并成一个。结果，其他聚类因为突然需要将两个视觉上明显不同的云团分成三个聚类而感到困惑。
- en: For this reason, it is common for the algorithm to be run for multiple initial
    states. Indeed, OpenCV does this by default (set by the optional `attempts` parameter).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，算法通常会在多个初始状态下运行。实际上，OpenCV 默认就是这样做的（通过可选的 `attempts` 参数设置）。
- en: The second caveat – we must select the number of clusters beforehand
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二个注意事项——我们必须事先选择聚类数量
- en: Another potential limitation is that *k*-means cannot learn the number of clusters
    from the data. Instead, we must tell it how many clusters we expect beforehand.
    You can see how this could be problematic for complicated real-world data that
    you don't fully understand yet.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个潜在的局限性是，*k*-means 无法从数据中学习聚类数量。相反，我们必须事先告诉它我们期望的聚类数量。你可以看到，对于你还不完全理解的真实世界复杂数据，这可能会带来问题。
- en: 'From the viewpoint of *k*-means, there is no wrong or nonsensical number of
    clusters. For example, if we ask the algorithm to identify six clusters in the
    dataset generated in the preceding section, it will happily proceed and find the
    best six clusters:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *k*-means 的角度来看，没有错误或无意义的聚类数量。例如，如果我们要求算法在前一节生成的数据集中识别六个聚类，它将愉快地继续并找到最佳的六个聚类：
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The third caveat – cluster boundaries are linear
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三个注意事项——聚类边界是线性的
- en: The *k*-means algorithm is based on a simple assumption, which is that points
    will be closer to their own cluster center than to others. Consequently, *k*-means
    always assumes linear boundaries between clusters, meaning that it will fail whenever
    the geometry of the clusters is more complicated than that.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means 算法基于一个简单的假设，即点将比其他点更接近其自身的聚类中心。因此，*k*-means 总是假设聚类之间的边界是线性的，这意味着当聚类的几何形状比这更复杂时，它将失败。'
- en: 'We see this limitation for ourselves by generating a slightly more complicated
    dataset. Instead of generating data points from Gaussian blobs, we want to organize
    the data into two overlapping half circles. We can do this using scikit-learn''s
    `make_moons`. Here, we choose 200 data points belonging to two half circles, in
    combination with some Gaussian noise:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过生成一个稍微复杂一些的数据集，我们可以看到这个限制。我们不想从高斯云团生成数据点，而是想将数据组织成两个重叠的半圆。我们可以使用 scikit-learn
    的 `make_moons` 来做到这一点。在这里，我们选择属于两个半圆的 200 个数据点，并结合一些高斯噪声：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This time, we tell *k*-means to look for two clusters:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们告诉 *k*-means 寻找两个聚类：
- en: '[PRE17]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The resulting scatter plot looks like this diagram:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的散点图看起来像这个图：
- en: '![](img/305edf9a-f7c3-491e-8a1e-10c6427d1917.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/305edf9a-f7c3-491e-8a1e-10c6427d1917.png)'
- en: The preceding diagram shows an example of *k*-means finding linear boundaries
    in nonlinear data. As is evident from the plot, *k*-means failed to identify the
    two half circles and instead split the data with what looks like a diagonal straight
    line (from bottom-left to top-right).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图显示了 *k*-means 在非线性数据中找到线性边界的例子。从图中可以看出，*k*-means 未能识别出两个半圆，而是用看起来像对角线的直线（从左下角到右上角）分割了数据。
- en: This scenario should ring a bell. We had the same problem when we talked about
    linear SVMs in [Chapter 6](419719a8-3340-483a-86be-1d9b94f4a682.xhtml), *Detecting
    Pedestrians with Support Vector Machines.* The idea there was to use the kernel
    trick to transform the data into a higher-dimensional feature space. Can we do
    the same here?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这种场景应该让你想起什么。当我们谈到第6章中的线性SVM时，我们遇到了相同的问题，[第6章](419719a8-3340-483a-86be-1d9b94f4a682.xhtml)，*使用支持向量机检测行人.*那里的想法是使用核技巧将数据转换到更高维的特征空间。我们在这里能做同样的事情吗？
- en: 'We most certainly can. There is a form of kernelized *k*-means that works akin
    to the kernel trick for SVMs, called **spectral clustering**. Unfortunately, OpenCV
    does not provide an implementation of spectral clustering. Fortunately, scikit-learn
    does:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当然可以。有一种核化的*k*-means形式，类似于SVM的核技巧，称为**谱聚类**。不幸的是，OpenCV没有提供谱聚类的实现。幸运的是，scikit-learn提供了：
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The algorithm uses the same API as all other statistical models: we set optional
    arguments in the constructor and then call `fit_predict` on the data. Here, we
    want to use the graph of the nearest neighbors to compute a higher-dimensional
    representation of the data and then assign labels using *k*-means:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法使用与其他所有统计模型相同的API：我们在构造函数中设置可选参数，然后在数据上调用`fit_predict`。在这里，我们想使用最近邻图来计算数据的高维表示，然后使用*k*-means分配标签：
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output of spectral clustering looks like this:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 谱聚类的输出看起来像这样：
- en: '![](img/3c51c11b-66f9-408d-b404-ae809e59bb5b.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3c51c11b-66f9-408d-b404-ae809e59bb5b.png)'
- en: We see that spectral clustering gets the job done. Alternatively, we could have
    transformed the data into a more suitable representation ourselves and then applied
    OpenCV's linear *k*-means to it. The lesson of all of this is that, perhaps, again,
    feature engineering saved the day.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到谱聚类完成了工作。或者，我们也可以自己将数据转换成更合适的表示，然后应用OpenCV的线性*k*-means。所有这一切的教训是，也许，再次，特征工程挽救了这一天。
- en: The fourth caveat – k-means is slow for a large number of samples
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四个注意事项——对于大量样本，k-means很慢
- en: 'The final limitation of *k*-means is that it is relatively slow for large datasets.
    You can imagine that quite a lot of algorithms might suffer from this problem.
    However, *k*-means is affected especially badly: each iteration of *k*-means must
    access every single data point in the dataset and compare it to all of the cluster
    centers.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means的最后一个限制是它对于大数据集来说相对较慢。你可以想象很多算法可能会遇到这个问题。然而，*k*-means受到的影响尤其严重：*k*-means的每次迭代都必须访问数据集中的每个数据点，并将其与所有聚类中心进行比较。'
- en: You might wonder whether the requirement to access all data points during each
    iteration is really necessary. For example, you might just use a subset of the
    data to update the cluster centers at each step. Indeed, this is the exact idea
    that underlies a variation of the algorithm called **batch-based *k*-means**.
    Unfortunately, this algorithm is not implemented ...
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道在每次迭代中访问所有数据点是否真的必要。例如，你可能只需使用数据的一个子集来在每一步更新聚类中心。确实，这正是被称为**基于批次的*k*-means**的算法背后的确切想法。不幸的是，这个算法尚未实现...
- en: Compressing color spaces using k-means
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用k-means压缩颜色空间
- en: One interesting use case of *k*-means is the compression of image color spaces.
    For example, a standard **color image** comes with 24-bit color depth, providing
    for a total of 16,777,216 color varieties. However, in most images, a large number
    of colors will be unused, and many of the pixels in the image will have similar
    values. The compressed image can then be sent over the internet at a faster speed,
    and at the receiver end, it can be decompressed to get back the original image.
    Hence, reducing the storage and transmission cost. However, the image color space
    compression will be **lossy** and you might not notice fine details in the image
    after compression.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means的一个有趣用例是图像颜色空间的压缩。例如，一个标准的**彩色图像**带有24位颜色深度，提供了总共16,777,216种颜色变化。然而，在大多数图像中，大量的颜色将不会被使用，并且图像中的许多像素将具有相似值。压缩后的图像可以以更快的速度通过互联网发送，在接收端，它可以被解压缩以恢复原始图像。因此，减少了存储和传输成本。然而，图像颜色空间压缩将是**有损的**，你可能在压缩后不会注意到图像中的细微细节。'
- en: Alternatively, we can use *k*-means to reduce the color palette too. The idea
    here is to think of the cluster centers as the decreased color palette. Then, *k*-means
    will organize the millions of colors in the original image into the appropriate
    number of colors.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以使用**k**-means来减少调色板。这里的想法是将聚类中心视为减少后的调色板。然后，**k**-means将原始图像中的数百万种颜色组织成适当数量的颜色。
- en: Visualizing the true-color palette
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化真彩色调色板
- en: 'By performing the following steps, you will be able to visualize the true-color
    palette of a color image:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行以下步骤，您将能够可视化彩色图像的真实彩色调色板：
- en: 'Let''s have a look at a particular image:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看一张特定的图像：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'By now, we know how to start Matplotlib in our sleep:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到现在为止，我们知道如何在睡梦中启动Matplotlib：
- en: '[PRE21]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'However, this time, we want to disable the grid lines that the `ggplot` option
    typically displays over images:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，这次，我们希望禁用`ggplot`选项通常在图像上显示的网格线：
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, we can visualize Lena with the following command (don''t forget to switch
    the BGR ordering of the color channels to RGB):'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用以下命令可视化Lena（别忘了将颜色通道的BGR顺序切换为RGB）：
- en: '[PRE23]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Reducing the color palette using k-means
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用k-means减少调色板
- en: 'By referring to the following steps, you will be able to project a color image
    into a reduced color palette using *k*-means clustering:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 通过参考以下步骤，您将能够使用**k**-means聚类将彩色图像投影到简化后的调色板：
- en: 'Now, let''s reduce the 16 million colors to a mere 16 by indicating *k*-means
    to cluster all of the 16 million color variations into 16 distinct clusters. We
    will use the previously mentioned procedure, but now define 16 as the number of
    clusters:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将1600万种颜色减少到仅仅16种，通过指示**k**-means将所有1600万种颜色变化聚类到16个不同的聚类中。我们将使用之前提到的程序，但现在将16定义为聚类数：
- en: '[PRE24]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The 16 different colors of your reduced color palette correspond to the resultant
    clusters. The output from the `centers` array reveals that all colors have three
    entries—`B`, `G`, and `R`—with values between 0 and 1:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您简化后的调色板中的16种不同颜色对应于结果聚类。`centers`数组的输出显示所有颜色都有三个条目—`B`、`G`和`R`—其值介于0和1之间：
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `labels` vector contains the 16 colors corresponding to the 16 cluster `labels`.
    So, all of the data points with the label 0 will be colored according to row 0
    in the `centers` array; similarly, all data points with the label 1 will be colored
    according to row 1 in the `centers` array and so on. Hence, we want to use `labels`
    as an index in the `centers` array—these are our new colors:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`labels`向量包含与16个聚类`labels`对应的16种颜色。因此，所有标签为0的数据点将根据`centers`数组中的第0行着色；同样，所有标签为1的数据点将根据`centers`数组中的第1行着色，依此类推。因此，我们希望将`labels`用作`centers`数组中的索引—这些就是我们的新颜色：'
- en: '[PRE26]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can plot the data again, but this time, we will use `new_colors` to color
    the data points accordingly:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以再次绘制数据，但这次，我们将使用`new_colors`相应地着色数据点：
- en: '[PRE27]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The result is the recoloring of the original pixels, where each pixel is assigned
    the color of its closest cluster center:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是原始像素的重新着色，其中每个像素都被分配了其最近聚类中心的颜色：
- en: '![](img/f3b6e0d8-5b3b-4c4f-98fc-c75abe2aefe1.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f3b6e0d8-5b3b-4c4f-98fc-c75abe2aefe1.png)'
- en: 'To observe the effect of recoloring, we have to plot `new_colors` as an image.
    We flattened the earlier image to get from the image to the data matrix. To get
    back to the image now, we need to do the inverse, which is reshape `new_colors`
    according to the shape of the Lena image:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了观察重新着色的影响，我们必须将`new_colors`作为一个图像来绘制。我们之前将图像展平以从图像到数据矩阵转换。现在要回到图像，我们需要进行逆操作，即根据Lena图像的形状重塑`new_colors`：
- en: '[PRE28]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then, we can visualize the recolored Lena image like any other image:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以像其他任何图像一样可视化重新着色的Lena图像：
- en: '[PRE29]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The result looks like this:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来像这样：
- en: '![](img/db7c60af-4276-454d-9f22-a32cda3bdb5e.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db7c60af-4276-454d-9f22-a32cda3bdb5e.png)'
- en: It's pretty awesome, right?
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 真的很棒，对吧？
- en: Overall, the preceding screenshot is quite clearly recognizable although some
    details are arguably lost. Given that you compressed the image by a factor of
    around 1 million, this is pretty remarkable.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，前面的截图非常清晰可辨，尽管一些细节可能有所丢失。鉴于您将图像压缩了大约一百万倍，这相当了不起。
- en: You can repeat this procedure for any desired number of colors.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为任何期望的颜色数重复此过程。
- en: Another way to reduce the color palette of images involves the use of **bilateral
    filters**. The resulting images often look like cartoon versions of the original
    image. You can find an example of this in the book, *OpenCV with Python Blueprints*,
    by M. Beyeler, Packt Publishing.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种减少图像调色板的方法是使用**双边滤波器**。结果图像通常看起来像原始图像的卡通版本。你可以在M. Beyeler的《OpenCV with Python
    Blueprints》，Packt Publishing出版的书中找到一个例子。
- en: 'Another potential application of *k*-means is something you might not expect:
    using it for image classification.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means的另一个潜在应用可能不是你预期的：将其用于图像分类。'
- en: Classifying handwritten digits using k-means
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用k-means对手写数字进行分类
- en: Although the last application was a pretty creative use of *k*-means, we can
    do better still. We have previously discussed *k*-means in the context of unsupervised
    learning, where we tried to discover some hidden structure in the data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然最后一个应用是*k*-means的一个相当有创意的使用，但我们还可以做得更好。我们之前在无监督学习的上下文中讨论了*k*-means，我们试图在数据中找到一些隐藏的结构。
- en: However, doesn't the same concept apply to most classification tasks? Let's
    say our task was to classify handwritten digits. Don't most zeros look similar,
    if not the same? And don't all zeros look categorically different from all possible
    ones? Isn't this exactly the kind of *hidden structure* we set out to discover
    with unsupervised learning? Doesn't this mean we could use clustering for classification
    as well?
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个概念难道不适用于大多数分类任务吗？比如说，我们的任务是分类手写数字。难道大多数零看起来不相似，甚至完全相同吗？而且，所有的零难道不是在类别上与所有可能的“一”都不同吗？这不正是我们使用无监督学习试图发现的*隐藏结构*吗？这不意味着我们也可以使用聚类进行分类吗？
- en: Let's find out together. In this section, we will attempt ...
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一起来看看。在本节中，我们将尝试...
- en: Loading the dataset
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'From the earlier chapters, you might recall that scikit-learn provides a whole
    range of handwritten digits via its `load_digits` utility function. The dataset
    consists of 1,797 samples with 64 features each, where each of the features has
    the brightness of one pixel in an *8 x 8* image:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的章节中，你可能还记得，scikit-learn通过其`load_digits`实用函数提供了一系列手写数字。该数据集包含1,797个样本，每个样本有64个特征，其中每个特征代表一个*8
    x 8*图像中一个像素的亮度：
- en: '[PRE30]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Running k-means
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行k-means
- en: 'Setting up *k*-means works exactly the same as in the previous examples. We
    tell the algorithm to perform at most 10 iterations and stop the process if our
    prediction of the cluster centers does not improve within a distance of `1.0`:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 设置*k*-means与之前的例子完全相同。我们告诉算法最多执行10次迭代，如果我们的聚类中心预测在距离`1.0`内没有改进，则停止过程：
- en: '[PRE31]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, we apply *k*-means to the data as we did before. Since there are 10 different
    digits (0-9), we tell the algorithm to look for 10 distinct clusters:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们像之前一样对数据进行k-means处理。由于有10个不同的数字（0-9），我们告诉算法寻找10个不同的聚类：
- en: '[PRE32]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: And we're done!
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们就完成了！
- en: Similar to the *N x 3* matrices ...
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 与*N x 3*矩阵类似...
- en: Organizing clusters as a hierarchical tree
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将聚类组织成层次树
- en: An alternative to *k*-means is **hierarchical clustering**. One advantage of
    hierarchical clustering is that it allows us to organize the different clusters
    in a hierarchy (also known as a **dendrogram**), which can make it easier to interpret
    the results. Another useful advantage is that we do not need to specify the number
    of clusters upfront.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means的一个替代方法是**层次聚类**。层次聚类的一个优点是它允许我们将不同的聚类组织成层次结构（也称为**树状图**），这可以使结果更容易解释。另一个有用的优点是我们不需要事先指定聚类的数量。'
- en: Understanding hierarchical clustering
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解层次聚类
- en: 'There are two approaches to hierarchical clustering:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类有两种方法：
- en: In **agglomerative hierarchical clustering**, we start with each data point
    potentially being its own cluster, and we subsequently merge the closest pair
    of clusters until only one cluster remains.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**聚合层次聚类**中，我们开始时假设每个数据点可能是一个单独的聚类，然后我们随后合并最近的两个聚类，直到只剩下一个聚类。
- en: In **divisive hierarchical clustering**, it's the other way around; we start
    by assigning all of the data points to the same cluster, and we subsequently split
    the cluster into smaller clusters until each cluster only contains one sample.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**分裂层次聚类**中，情况正好相反；我们首先将所有数据点分配到同一个聚类，然后我们随后将聚类分割成更小的聚类，直到每个聚类只包含一个样本。
- en: 'Of course, we can specify the number of desired clusters if we wish to. In
    the following screenshot, we asked the algorithm to find a total of three clusters:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果我们希望的话，我们可以指定所需的聚类数量。在下面的屏幕截图中，我们要求算法找到总共三个聚类：
- en: The preceding screenshot shows a step-by-step example of agglomerative ...
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 上一张截图显示了层次聚类的逐步示例：
- en: Implementing agglomerative hierarchical clustering
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现层次聚类
- en: 'Although OpenCV does not provide an implementation of agglomerative hierarchical
    clustering, it is a popular algorithm that should, by all means, belong to our
    machine learning skill set:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管OpenCV没有提供层次聚类算法的实现，但它是一种应该无论如何都属于我们机器学习技能集的流行算法：
- en: 'We start by generating 10 random data points, just like in the previous screenshot:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先生成10个随机数据点，就像上一张截图所示：
- en: '[PRE33]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Using the familiar statistical modeling API, we import the `AgglomerativeClustering`
    algorithm and specify the desired number of clusters:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用熟悉的统计建模API，我们导入`AgglomerativeClustering`算法并指定所需的聚类数量：
- en: '[PRE34]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Fitting the model to the data works, as usual, via the `fit_predict` method:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型拟合到数据，通常通过`fit_predict`方法进行：
- en: '[PRE35]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can generate a scatter plot where every data point is colored according
    to the predicted label:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以生成一个散点图，其中每个数据点都根据预测的标签着色：
- en: '[PRE36]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The resulting clustering is equivalent to the following diagram:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的聚类结果等同于以下图示：
- en: '![](img/1c9d3311-d0f0-4f90-89aa-f93c45af9b60.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c9d3311-d0f0-4f90-89aa-f93c45af9b60.png)'
- en: Finally, before we end this chapter, let's look at how to compare clustering
    algorithms and choose the correct clustering algorithm for the data you have!
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在我们结束这一章之前，让我们看看如何比较聚类算法并选择适合你数据的正确聚类算法！
- en: Comparing clustering algorithms
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较聚类算法
- en: 'There are around thirteen different clustering algorithms in the `sklearn`
    library. Having thirteen different sets of choices, the question is: what clustering
    algorithms should you use? The answer is your data. What type of data you have
    and which clustering you would like to apply on it is how you will select the
    algorithm. Having said that, there can be many possible algorithms that could
    be useful for the kind of problem and data you have. Each of the thirteen classes
    in `sklearn` is specialized for specific tasks (such as co-clustering and bi-clustering
    or clustering features instead of data points). An algorithm specializing in text
    clustering would be the right choice for clustering text data. Hence, if ...'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn`库中大约有十三个不同的聚类算法。拥有十三种不同的选择集，问题是：你应该使用哪些聚类算法？答案是你的数据。你有什么类型的数据以及你希望对其应用哪种聚类，这就是你选择算法的方式。话虽如此，对于你拥有的问题和数据，可能有许多可能的算法可能是有用的。`sklearn`中的每个十三类都是针对特定任务专门化的（例如，协同聚类和双聚类或聚类特征而不是数据点）。专注于文本聚类的算法将是聚类文本数据的正确选择。因此，如果
    ...'
- en: Summary
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we talked about some unsupervised learning algorithms, including
    *k*-means, spherical clustering, and agglomerative hierarchical clustering. We
    saw that *k*-means is just a specific application of the more general expectation-maximization
    algorithm, and we discussed its potential limitations. Furthermore, we applied
    *k*-means to two specific applications, which were reducing the color palette
    of images and classifying handwritten digits.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了一些无监督学习算法，包括*k*-means、球形聚类和层次聚类。我们了解到*k*-means只是更一般化的期望最大化算法的一个特定应用，并讨论了其潜在的局限性。此外，我们将*k*-means应用于两个具体的应用，这些应用是减少图像调色板和对手写数字进行分类。
- en: 'In the next chapter, we will move back into the world of supervised learning
    and talk about some of the most powerful current machine learning algorithms:
    neural networks and deep learning.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将回到监督学习的世界，并讨论一些目前最强大的机器学习算法：神经网络和深度学习。
