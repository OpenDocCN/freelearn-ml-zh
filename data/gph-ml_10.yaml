- en: 'Chapter 7: Text Analytics and Natural Language Processing Using Graphs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章：使用图进行文本分析和自然语言处理
- en: Nowadays, a vast amount of information is available in the form of text in terms
    of natural written language. The very same book you are reading right now is one
    such example. The news you read every morning, the tweets or the Facebook posts
    you sent/read earlier, the reports you write for a school assignment, the emails
    we write continuously – these are all examples of information we exchange via
    written documents and text. It is undoubtedly the most common way of indirect
    interaction, as opposed to direct interaction such as talking or gesticulating.
    It is, therefore, crucial to be able to leverage such kinds of information and
    extract insights from documents and texts.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，大量的信息以自然书面语言的形式以文本的形式存在。你现在正在阅读的这本书就是一个例子。你每天早上阅读的新闻，你之前发送/阅读的推文或Facebook帖子，你为学校作业撰写的报告，我们持续撰写的电子邮件——这些都是我们通过书面文档和文本交换信息的例子。这无疑是间接互动中最常见的方式，与直接互动如谈话或手势相反。因此，能够利用这类信息并从文档和文本中提取见解至关重要。
- en: The vast amount of information present nowadays in this form has determined
    the great development and recent advances in the field of **natural language processing**
    (**NLP**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现今以这种形式存在的海量信息决定了自然语言处理（**NLP**）领域的大发展和近期进步。
- en: In this chapter, we will show you how to process natural language texts and
    review some basic models that allow us to structure text information. Using the
    information that's been extracted from a corpus of documents, we will show you
    how to create networks that can be analyzed using some of the techniques we have
    seen in previous chapters. In particular, using a tagged corpus we will show you
    how to develop both supervised (classification models to classify documents in
    pre-determined topics) and unsupervised (community detection to discover new topics)
    algorithms.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向您展示如何处理自然语言文本，并回顾一些基本模型，这些模型使我们能够结构化文本信息。使用从文档语料库中提取的信息，我们将向您展示如何创建可以使用我们在前几章中看到的一些技术进行分析的网络。特别是，使用标记语料库，我们将向您展示如何开发监督（用于将文档分类到预定的主题的分类模型）和无监督（社区检测以发现新主题）算法。
- en: 'The chapter covers the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Providing a quick overview of a dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速概述数据集
- en: Understanding the main concepts and tools used in NLP
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解NLP中使用的主要概念和工具
- en: Creating graphs from a corpus of documents
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文档语料库创建图
- en: Building a document topic classifier
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建文档主题分类器
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using *Python 3.8* for all our exercises. The following is a list
    of Python libraries that you must install for this chapter using pip. To do this,
    run, for example, `pip install networkx==2.4` on the command line and so on:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在所有练习中使用*Python 3.8*。以下是在本章中必须使用pip安装的Python库列表。例如，在命令行上运行`pip install networkx==2.4`等。
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: All the code files relevant to this chapter are available at [https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter07](https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter07).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与本章相关的所有代码文件均可在[https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter07](https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter07)找到。
- en: Providing a quick overview of a dataset
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速概述数据集
- en: 'To show you how to process a corpus of documents with the aim of extracting
    relevant information, we will be using a dataset derived from a well-known benchmark
    in the field of NLP: the so-called **Reuters-21578**. The original dataset includes
    a set of 21,578 news articles that were published in the financial Reuters newswire
    in 1987, which were assembled and indexed in categories. The original dataset
    has a very skewed distribution, with some categories appearing only in the training
    set or in the test set. For this reason, we will use a modified version, known
    as **ApteMod**, also referred to as *Reuters-21578 Distribution 1.0*, that has
    a smaller skew distribution and consistent labels between the training and test
    datasets.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了向您展示如何处理文档语料库以提取相关信息，我们将使用一个来自NLP领域知名基准的数据集：所谓的**路透社-21578**。原始数据集包括一组在1987年发布的21,578篇金融路透社新闻稿，这些新闻稿被汇编并按类别索引。原始数据集具有非常倾斜的分布，一些类别仅在训练集或测试集中出现。因此，我们将使用一个修改后的版本，称为**ApteMod**，也称为*路透社-21578分布1.0*，它具有较小的倾斜分布，并且在训练集和测试数据集之间具有一致的标签。
- en: Even though these articles are a bit outdated, the dataset has been used in
    a plethora of papers on NLP and still represents a dataset that's often used for
    benchmarking algorithms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些文章有些过时，但该数据集已被用于大量关于NLP的论文中，并且仍然代表了一个常用于算法基准的数据集。
- en: Indeed, Reuters-21578 contains enough documents for interesting post-processing
    and insights. A corpus with a larger number of documents can easily be found nowadays
    (see, for instance, [https://github.com/niderhoff/nlp-datasets](https://github.com/niderhoff/nlp-datasets)
    for an overview of the most common ones), but they may require larger storage
    and computational power so that they can be processed. In [*Chapter 9*](B16069_09_Final_JM_ePub.xhtml#_idTextAnchor141)*,
    Building a Data-Driven, Graph-Powered Application*, we will show you some of the
    tools and libraries that can be used to scale out your application and analysis.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，Reuters-21578包含足够多的文档，可以进行有趣的后期处理和洞察。如今可以轻易找到包含更多文档的语料库（例如，查看[https://github.com/niderhoff/nlp-datasets](https://github.com/niderhoff/nlp-datasets)以了解最常见的语料库概述），但它们可能需要更大的存储和计算能力，以便进行处理。在[*第9章*](B16069_09_Final_JM_ePub.xhtml#_idTextAnchor141)*，构建数据驱动、图增强应用程序*中，我们将向您展示一些可以用来扩展您的应用程序和分析的工具和库。
- en: 'Each document of the Reuters-21578 dataset is provided with a set of labels
    that represent its content. This makes it a perfect benchmark for testing both
    supervised and unsupervised algorithms. The Reuters-21578 dataset can easily be
    downloaded using the `nltk` library (which is a very useful library for post-processing
    documents):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Reuters-21578数据集中的每个文档都提供了一组标签，代表其内容。这使得它成为测试监督和无监督算法的完美基准。可以使用`nltk`库轻松下载Reuters-21578数据集（这是一个非常有用的库，用于文档的后期处理）：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As you will see from inspecting the `corpus` DataFrame, the IDs are in the
    form `training/{ID}` and `test/{ID}`, which makes it clear which documents should
    be used for training and for testing. To start, let''s list all the topics and
    see how many documents there are per topic using the following code:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从检查`corpus`数据框中可以看到的，ID的格式为`training/{ID}`和`test/{ID}`，这使得很清楚哪些文档应该用于训练和测试。首先，让我们列出所有主题，并使用以下代码查看每个主题有多少文档：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The Reuters-21578 dataset includes 90 different topics with a significant degree
    of unbalance between classes, with almost 37% of the documents in the `most common`
    category and only 0.01% in each of the five least common categories. As you can
    see from inspecting the text, some of the documents have some newline characters
    embedded, which can easily be removed in the first text cleaning stage:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Reuters-21578数据集包含90个不同的主题，类别之间存在着显著的不平衡，其中`最常见`类别中有近37%的文档，而在五个`最少见`类别中每个类别只有0.01%。正如您从检查文本中可以看到的，一些文档中嵌入了一些换行符，这些换行符在第一次文本清理阶段可以很容易地被移除：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have loaded the data in memory, we can start analyzing it. In the
    next subsection, we will show you some of the main tools that can be used for
    dealing with unstructured text data. They will help you extract structured information
    so that it can be used with ease.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据加载到内存中，我们可以开始分析它了。在下一小节中，我们将向您展示一些可以用来处理非结构化文本数据的主要工具。它们将帮助您提取结构化信息，以便可以轻松使用。
- en: Understanding the main concepts and tools used in NLP
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解NLP中使用的主要概念和工具
- en: When processing documents, the first analytical step is certainly to infer the
    document language. Most analytical engines that are used in NLP tasks are, in
    fact, trained on documents in a specific language and should only be used for
    such a language. Some attempts to build cross-language models (see, for instance,
    multi-lingual embeddings such as [https://fasttext.cc/docs/en/aligned-vectors.html](https://fasttext.cc/docs/en/aligned-vectors.html)
    and [https://github.com/google-research/bert/blob/master/multilingual.md](https://github.com/google-research/bert/blob/master/multilingual.md))
    have recently gained increasing popularity, although they still represent a small
    portion of NLP models. Therefore, it is very common to first infer the language
    so that you can use the correct downstream analytical NLP pipeline.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理文档时，第一个分析步骤无疑是推断文档语言。实际上，用于自然语言处理任务的大多数分析引擎都是在特定语言的文档上训练的，并且应该仅用于这种语言。最近，一些构建跨语言模型的尝试（例如，多语言嵌入[https://fasttext.cc/docs/en/aligned-vectors.html](https://fasttext.cc/docs/en/aligned-vectors.html)和[https://github.com/google-research/bert/blob/master/multilingual.md](https://github.com/google-research/bert/blob/master/multilingual.md)）越来越受欢迎，尽管它们在自然语言处理模型中仍然只占一小部分。因此，首先推断语言以便使用正确的下游分析自然语言处理管道是非常常见的。
- en: You can use different methods to infer the language. One very simple yet effective
    approach relies on looking for the most common words of a language (the so-called
    `stopwords`, such as `the`, `and`, `be`, `to`, `of`, and so on) and building a
    score based on their frequencies. Its precision, however, tends to be limited
    to short text and does not make use of the word's positioning and context. On
    the other hand, Python has many libraries that use more elaborated logic, allowing
    us to infer the language in a more precise manner. Some such libraries are `fasttext`,
    `polyglot`, and `langdetect`, to name just a few.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用不同的方法来推断语言。一种非常简单但有效的方法是查找语言中最常见的单词（所谓的`停用词`，如`the`、`and`、`be`、`to`、`of`等）并根据它们的频率构建一个分数。然而，它的精确度往往局限于短文本，并且没有利用单词的位置和上下文。另一方面，Python有许多使用更复杂逻辑的库，允许我们以更精确的方式推断语言。这些库中的一些是`fasttext`、`polyglot`和`langdetect`，仅举几个例子。
- en: 'As an example, we will use `fasttext` in the following code, which can be integrated
    with very few lines and provides support for more than 150 languages. The language
    can be inferred for all documents using the following snippet:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在下面的代码中，我们将使用`fasttext`，它可以通过很少的代码行进行集成，并支持超过150种语言。可以使用以下代码片段推断所有文档的语言：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you will see in the output, there seem to be documents in languages other
    than English. Indeed, these documents are often either very short or have a strange
    structure, which means they're not actual news articles. When documents represent
    text that a human would read and label as news, the model is generally rather
    precise and accurate.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将在输出中看到的那样，似乎有一些非英语语言的文档。事实上，这些文档通常要么非常短，要么结构奇特，这意味着它们不是真正的新闻文章。当文档代表人类阅读并标记为新闻的文本时，模型通常非常精确和准确。
- en: 'Now that we have inferred the language, we can continue with the language-dependent
    steps of the analytical pipeline. For the following tasks, we will be using `spaCy`,
    which is an extremely powerful library that allows us to embed state-of-the-art
    NLP models with very few lines of code. After installing the library with `pip
    install spaCy`, language-specific models can be integrated by simply installing
    them using the `spaCy` download utility. For instance, the following command can
    be used to download and install the English model:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经推断出了语言，我们可以继续分析管道的语言相关步骤。对于以下任务，我们将使用`spaCy`，这是一个极其强大的库，它允许我们用很少的代码行嵌入最先进的自然语言处理模型。在用`pip
    install spaCy`安装库之后，可以通过使用`spaCy`下载工具简单地安装它们来集成特定语言模型。例如，以下命令可以用来下载和安装英语模型：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we should have the language models for English ready to use. Let''s see
    which information it can provide. Using spaCy is extremely simple and, using just
    one line of code, can embed the computation as a very rich set of information.
    Let''s start by applying the model to one of the documents in the Reuters corpus:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们应该已经准备好了可以使用的英语语言模型。让我们看看它能够提供哪些信息。使用spaCy非常简单，只需一行代码就可以将计算嵌入为一个非常丰富的信息集。让我们首先将模型应用于路透社语料库中的一个文档：
- en: SUBROTO SAYS INDONESIA SUPPORTS TIN PACT EXTENSION
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: SUBROTO 表示印度尼西亚支持锡协议延期
- en: Mines and Energy Minister Subroto confirmed Indonesian support for an extension
    of the sixth **International Tin Agreement** (**ITA**), but said a new pact was
    not necessary. Asked by Reuters to clarify his statement on Monday in which he
    said the pact should be allowed to lapse, Subroto said Indonesia was ready to
    back extension of the ITA. "We can support extension of the sixth agreement,"
    he said. "But a seventh accord we believe to be unnecessary." The sixth ITA will
    expire at the end of June unless a two-thirds majority of members vote for an
    extension.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 能源和矿业部长 Subroto 确认印度尼西亚支持第六次**国际锡协定**（**ITA**）的延长，但表示新的协议并非必要。应路透社要求，Subroto
    在周一澄清了他的声明，称该协议应被允许失效，他表示印度尼西亚准备支持 ITA 的延长。“我们可以支持第六次协议的延长，”他说。“但我们认为第七次协议是不必要的。”除非三分之二的大多数成员投票支持延长，否则第六次
    ITA 将于六月底到期。
- en: '`spacy` can easily be applied just by loading the model and applying it to
    the text:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 只需加载模型并将其应用于文本，`spacy` 就可以轻松应用。
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `parsed` object, which is returned by `spacy`, has several fields due to
    many models being combined into a single pipeline. These provide a different level
    of text structuring. Let''s examine them one by one:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多模型被组合到一个单独的管道中，`parsed` 对象返回时具有几个字段。这些提供了不同级别的文本结构化。让我们逐一检查它们：
- en: '`spacy` generally works fairly well. However, please note that, depending on
    the context, a bit of model tuning or rule modification might be necessary. For
    instance, when you''re dealing with short texts that contain slang, emoticons,
    links, and hashtags, a better choice for text segmentation and tokenization may
    be `TweetTokenizer`, which is included in the `nltk` library. Depending on the
    context, we encourage you to explore other possible segmentations.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spacy` 通常工作得相当好。然而，请注意，根据上下文，可能需要进行一些模型调整或规则修改。例如，当您处理包含俚语、表情符号、链接和标签的短文本时，对于文本分割和标记化，更好的选择可能是
    `TweetTokenizer`，它包含在 `nltk` 库中。根据上下文，我们鼓励您探索其他可能的分割方式。'
- en: 'In the document returned by `spacy`, the sentence segmentation can be found
    in the `sents` attribute of the `parsed` object. Each sentence can be iterated
    over its token by simply using the following code:'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 `spacy` 返回的文档中，句子分割可以在 `parsed` 对象的 `sents` 属性中找到。可以通过以下代码简单地遍历每个句子的标记：
- en: '[PRE7]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Each token is a spaCy `Span` object that has attributes that specify the type
    of token and further characterization that's introduced by the other models.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个标记都是一个 spaCy `Span` 对象，它具有指定标记类型和由其他模型引入的进一步特征化的属性。
- en: '`DET`) is usually followed by a noun, and so on. When using spaCy, the information
    about PoS tagging is usually stored in the `label_` attribute of the `Span` object.
    The types of tags that are available can be found at [https://spacy.io/models/en](https://spacy.io/models/en).
    Conversely, you can get a human-readable value for a given type using the `spacy.explain`
    function.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (`DET`) 通常后面跟着一个名词，等等。当使用 spaCy 时，关于词性标注的信息通常存储在 `Span` 对象的 `label_` 属性中。可用的标签类型可以在
    [https://spacy.io/models/en](https://spacy.io/models/en) 找到。相反，您可以使用 `spacy.explain`
    函数为给定类型获取一个可读的值。
- en: '`ents` attribute of the `parsed` object. spaCy also provides some utilities
    to nicely visualize the entities in a text using the `displacy` module:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parsed` 对象的 `ents` 属性。spaCy 还提供了一些通过 `displacy` 模块可视化文本中实体的实用工具：'
- en: '[PRE8]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This results in the following output:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致了以下输出：
- en: '![Figure 7.1 – Example of the spaCy output for the NER engine'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.1 – spaCy NER 引擎输出的示例'
- en: '](img/B16069_07_01.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16069_07_01.jpg)'
- en: Figure 7.1 – Example of the spaCy output for the NER engine
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – spaCy NER 引擎输出的示例
- en: '`spacy` can be used to build a syntactic tree that can be navigated to identify
    relationships between the tokens. As we will see shortly, this information can
    be crucial when building knowledge graphs:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spacy` 可以用来构建一个可以导航以识别标记之间关系的句法树。正如我们很快就会看到的，当构建知识图谱时，这些信息可能是至关重要的：'
- en: '![Figure 7.2 – Example of a syntactic dependency tree provided by spaCy'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.2 – spaCy 提供的句法依存树示例'
- en: '](img/B16069_07_02.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16069_07_02.jpg)'
- en: Figure 7.2 – Example of a syntactic dependency tree provided by spaCy
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – spaCy 提供的句法依存树示例
- en: '`Span` object via the `lemma_` attribute.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 `lemma_` 属性访问 `Span` 对象。
- en: 'As shown in the preceding diagram, spaCy pipelines can be easily integrated
    to process the entire corpus and store the results in our `corpus` DataFrame:'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前图所示，spaCy 管道可以轻松集成以处理整个语料库并将结果存储在我们的 `corpus` DataFrame 中：
- en: '[PRE9]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This DataFrame represents the structured information of the documents. This
    will be the base of all our subsequent analysis. In the next section, we will
    show you how to build graphs while using such information.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据框代表了文档的结构化信息。这将是所有后续分析的基础。在下一节中，我们将展示如何使用此类信息构建图。
- en: Creating graphs from a corpus of documents
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文档语料库中创建图
- en: 'In this section, we will use the information we extracted in the previous section
    using the different text engines to build networks that relate the different information.
    In particular, we will focus on two kinds of graphs:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用上一节中通过不同的文本引擎提取的信息来构建关联不同信息的网络。特别是，我们将关注两种类型的图：
- en: '**Knowledge-based graphs**, where we will use the semantic meaning of sentences
    to infer relationships between the different entities.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于知识的图**，我们将使用句子的语义意义来推断不同实体之间的关系。'
- en: '**Bipartite graphs**, where we will be connecting the documents to the entities
    that appear in the text. We will then project the bipartite graph into a homogeneous
    graph, which will be made up of either document or entity nodes only.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二分图**，我们将连接文档和文本中出现的实体。然后，我们将二分图投影到一个同质图中，该图只由文档或实体节点组成。'
- en: Knowledge graphs
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识图谱
- en: 'Knowledge graphs are very interesting as they not only relate entities but
    also provide a direction and a meaning to the relationship. For instance, let''s
    take a look at the following relationship:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱非常有趣，因为它们不仅关联实体，还提供了关系的方向和意义。例如，让我们看看以下关系：
- en: I (->) buy (->) a book
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我 (->) 买 (->) 一本书
- en: 'This is substantially different from the following relationship:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这与以下关系有显著不同：
- en: I (->) sell (->) a book
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我 (->) 卖 (->) 一本书
- en: Besides the kind of relationship (buying or selling), it is also important to
    have a direction, where the subject and object are not treated symmetrically,
    but where there is a difference between who is performing the action and who is
    the target of such an action.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 除了关系类型（购买或销售）之外，还有一个方向也很重要，即主语和宾语不是对称处理的，而是存在执行动作的人和动作的目标之间的差异。
- en: So, to create a knowledge graph, we need a function that can identify the **Subject-Verb-Object**
    (**SVO**) triplet for each sentence. This function can then be applied to all
    the sentences in the corpus; then, all the triplets can be aggregated to generate
    the corresponding graph.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要创建一个知识图谱，我们需要一个能够识别每个句子的**主语-谓语-宾语**（**SVO**）三元组的函数。这个函数可以应用于语料库中的所有句子；然后，所有三元组可以汇总生成相应的图。
- en: 'The SVO extractor can be implemented on top of the enrichment provided by spaCy
    models. Indeed, the tagging provided by the dependency tree parser can be very
    helpful for separating main sentences and their subordinates, as well as identifying
    the SOV triplets. The business logic may need to consider a few special cases
    (such as conjunctions, negations, and preposition handling), but this can be encoded
    with a set of rules. Moreover, these rules may also change, depending on the specific
    use case, with slight variations to be tuned by the user. A base implementation
    of such rules can be found at [https://github.com/NSchrading/intro-spacy-nlp/blob/master/subject_object_extraction.py](https://github.com/NSchrading/intro-spacy-nlp/blob/master/subject_object_extraction.py).
    These have been slightly adopted for our scope and are included in the GitHub
    repository provided with this book. Using this helper function, we can compute
    all the triplets in the corpus and store them in our `corpus` DataFrame:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: SVO提取器可以建立在spaCy模型提供的丰富之上。确实，依赖树解析器提供的标记对于区分主句及其从句以及识别SOV三元组非常有帮助。业务逻辑可能需要考虑一些特殊情况（如并列连词、否定和介词处理），但这可以通过一组规则来编码。此外，这些规则也可能根据具体用例而变化，用户可以根据需要进行微调。此类规则的基础实现可以在[https://github.com/NSchrading/intro-spacy-nlp/blob/master/subject_object_extraction.py](https://github.com/NSchrading/intro-spacy-nlp/blob/master/subject_object_extraction.py)找到。这些规则已经稍作修改，并包含在这本书提供的GitHub仓库中。使用这个辅助函数，我们可以计算语料库中的所有三元组并将它们存储在我们的`corpus`数据框中：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The type of the connection (determined by the sentence''s main predicate) is
    stored in the `edge` column. The first 10 most common relationships can be shown
    using the following command:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 连接的类型（由句子的主要谓语决定）存储在`edge`列中。可以使用以下命令显示前10个最常见的关联关系：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The most common edge types correspond to very basic predicates. Indeed, together
    with very general verbs (such as be, have, tell, and give), we can also find predicates
    that are more related to a financial context (such as buy, sell, and make). Using
    all these edges, we can now create our knowledge-based graph using the `networkx`
    utility function:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的边类型对应于非常基本的谓词。确实，除了非常一般的动词（如be、have、tell和give）之外，我们还可以找到更多与金融背景相关的谓词（如buy、sell和make）。使用所有这些边，我们现在可以使用`networkx`实用函数创建我们的基于知识的图：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'By filtering the edge DataFrame and creating a subnetwork using this information,
    we can analyze specific relationship types, such as the `lend` edge:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过过滤边数据框并使用此信息创建子网络，我们可以分析特定关系类型，例如`lend`边：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following diagram shows the subgraph based on the *lend* relationships.
    As we can see, it already provides interesting economical insights, such as the
    economic relationships between countries, such as Venezuela-Ecuador and US-Sudan:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图显示了基于*贷款*关系的子图。正如我们所见，它已经提供了有趣的经济洞察，例如国家之间的经济关系，如委内瑞拉-厄瓜多尔和美苏关系：
- en: '![Figure 7.3 – Example of a portion of the knowledge graph for the edges related
    the lending relationships'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.3 – 与贷款关系相关的知识图的一部分示例]'
- en: '](img/B16069_07_03.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16069_07_03.jpg]'
- en: Figure 7.3 – Example of a portion of the knowledge graph for the edges related
    the lending relationships
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 与贷款关系相关的知识图的一部分示例
- en: You can play around with the preceding code by filtering the graph based on
    other relationships. We definitely encourage you to do so, in order to unveil
    further interesting insights from the knowledge graphs we just created. In the
    next section, we will show you another method that allows us to encode the information
    that's been extracted from the text into a graph structure. In doing so, we will
    also make use of a particular type of graph that we introduced in [*Chapter 1*](B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014)*,*
    *Bipartite Graphs*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过根据其他关系过滤图来玩弄前面的代码。我们确实鼓励您这样做，以便从我们刚刚创建的知识图中揭示更多有趣的见解。在下一节中，我们将向您展示另一种方法，允许我们将从文本中提取的信息编码到图结构中。这样做时，我们还将利用我们在[*第一章*](B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014)*,*
    *二分图*中介绍的一种特定类型的图。
- en: Bipartite document/entity graphs
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文档/实体二分图
- en: Knowledge graphs can unveil and query aggregated information over entities.
    However, other graph representations are also possible and can be useful in other
    situations. For example, when you want to cluster documents semantically, the
    knowledge graph may not be the best data structure to use and analyze. Knowledge
    graphs are also not very effective at finding indirect relationships, such as
    identifying competitors, similar products, and so on, that do not often occur
    in the same sentence, but that often occur in the same document.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图可以揭示和查询实体上的聚合信息。然而，其他图表示也是可能的，并且在其他情况下可能很有用。例如，当您想要在语义上对文档进行聚类时，知识图可能不是最佳的数据结构来使用和分析。知识图在寻找间接关系方面也不是非常有效，例如识别竞争对手、类似产品等，这些关系通常不会出现在同一句话中，但经常出现在同一文档中。
- en: 'To address these limitations, we will encode the information present in the
    document in the form of a **bipartite graph**. For each document, we will extract
    the entities that are most relevant and connect a node, representing the document,
    with all the nodes representing the relevant entities in such a document. Each
    node may have multiple relationships: by definition, each document connects multiple
    entities. By contract, an entity can be referenced in multiple documents. As we
    will see, cross-referencing can be used to create a measure of similarity between
    entities and documents. This similarity can also be used for projecting the bipartite
    graph into one particular set of nodes – either the document nodes or the entity
    nodes.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些限制，我们将以**二分图**的形式对文档中存在的信息进行编码。对于每个文档，我们将提取最相关的实体，并将代表文档的节点与该文档中代表相关实体的所有节点连接起来。每个节点可能有多个关系：根据定义，每个文档连接多个实体。相反，一个实体可以在多个文档中被引用。正如我们将看到的，交叉引用可以用来创建实体和文档之间的相似度度量。这种相似度也可以用来将二分图投影到特定的节点集——要么是文档节点，要么是实体节点。
- en: 'To this aim, to build our bipartite graph, we need to extract the relevant
    entities of a document. The term *relevant entity* is clearly fuzzy and broad.
    In the current context, we will consider a relevant entity to be either a named
    entity (such as an organization, person, or location recognized by the NER engine)
    or a keyword; that is, a word (or a composition of words) that identifies and
    generally describes the document and its content. For instance, the suitable keywords
    for this book may be "graph," "network," "machine learning," "supervised model,"
    and "unsupervised model." Many algorithms exist that extract keywords from a document.
    One very simple way to do this is based on the so-called TF-IDF score, which is
    based on building a score for each token (or group of tokens, often referred to
    as *grams*) that is proportional to the word count in the document (the **Term
    Frequency**, or **TF**) and to the inverse of the frequency of that word in a
    given corpus (the **Inverse Document Frequency**, or **IDF**):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这个目的，构建我们的二分图，我们需要提取文档的相关实体。术语“相关实体”显然是模糊和广泛的。在当前上下文中，我们将相关实体视为命名实体（如由 NER
    引擎识别的组织、人或地点）或关键词；即一个词（或词的组合），它可以识别并通常描述文档及其内容。例如，这本书的合适关键词可能是“图”、“网络”、“机器学习”、“监督模型”和“无监督模型”。存在许多从文档中提取关键词的算法。一种非常简单的方法是基于所谓的
    TF-IDF 分数，它为每个标记（或标记组，通常称为“词组”）构建一个分数，该分数与文档中的词频（**词频**，或 **TF**）成正比，并与该词在给定语料库中的频率的倒数（**逆文档频率**，或
    **IDF**）成正比：
- en: '![](img/B16067_07_001.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16067_07_001.jpg)'
- en: Here, ![](img/B16067_07_002.png) represents the count of word ![](img/B16067_07_003.png)
    in document ![](img/B16067_07_004.png), ![](img/B16067_07_005.png) represents
    the number of documents in the corpus, and ![](img/B16067_07_006.png) is the document
    where the word ![](img/B16067_07_007.png) appears. Therefore, the TF-IDF score
    promotes words that are repeated many times in the document, penalizing words
    that are common and therefore might not be very representative for a document.
    There are also more sophisticated algorithms.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B16067_07_002.png) 代表文档 ![](img/B16067_07_004.png) 中词 ![](img/B16067_07_003.png)
    的计数，![](img/B16067_07_005.png) 代表语料库中的文档数量，而 ![](img/B16067_07_006.png) 是出现词 ![](img/B16067_07_007.png)
    的文档。因此，TF-IDF 分数促进了在文档中多次重复的词，惩罚了常见且可能不是文档非常有代表性的词。也存在更复杂的算法。
- en: 'One method that is quite powerful and worth mentioning in the context of this
    book is indeed `gensim`, which can be used in a straightforward manner:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的上下文中，确实有一种非常强大且值得提及的方法是 `gensim`，它可以以直接的方式使用：
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This produces the following output:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here, the score represents the centrality, which represents the importance
    of a given token. As you can see, some composite tokens may also occur, such as
    `japanese electronics`. Keyword extraction can be implemented to compute the keywords
    for the entire corpus, thus storing the information in our `corpus` DataFrame:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，分数代表中心性，它代表一个给定标记的重要性。正如你所看到的，一些复合标记也可能出现，例如“日本电子产品”。可以通过实现关键词提取来计算整个语料库的关键词，从而将信息存储在我们的
    `corpus` DataFrame 中：
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Besides the keywords, to build the bipartite graph, we also need to parse the
    named entities that were extracted by the NER engine, and then encode the information
    in a similar data format as the one that was used for the keywords. This can be
    done using a few utility functions:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 除了关键词之外，为了构建二分图，我们还需要解析由 NER 引擎提取的命名实体，然后将信息编码成与关键词使用类似的数据格式。这可以通过使用几个实用函数来完成：
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'With these functions, parsing the `spacy` tags can be done with the following
    code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些函数，可以通过以下代码解析 `spacy` 标签：
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `entities` DataFrame can easily be merged with the `corpus` DataFrame using
    the `pd.concat` function, thus placing all the information in a single data structure:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `pd.concat` 函数轻松地将 `entities` DataFrame 与 `corpus` DataFrame 合并，从而将所有信息放置在单一的数据结构中：
- en: '[PRE19]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now that we have all the ingredients for our bipartite graph, we can create
    the edge list by looping over all the documents-entity or document-keyword pairs:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了构建二分图的所有成分，我们可以通过遍历所有文档-实体或文档-关键词对来创建边列表：
- en: '[PRE20]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Once the edge list has been created, we can produce the bipartite graph using
    `networkx` APIs:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了边列表，我们就可以使用 `networkx` API 生成二分图：
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we can look at an overview of our graph by using `nx.info`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 `nx.info` 来查看我们图的概述：
- en: '[PRE22]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the next subsection, we will project the bipartite graph in either of the
    two sets of nodes: entities or documents. This will allow us to explore the difference
    between the two graphs and cluster both the terms and documents using the unsupervised
    techniques described in [*Chapter 4*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064)*,
    Supervised Graph Learning*. Then, we will return to the bipartite graph to show
    an example of supervised classification, which we''ll do by leveraging the network
    information of the bipartite graphs.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我们将对两个节点集之一（实体或文档）中的二分图进行投影。这将使我们能够探索两个图之间的差异，并使用在第[*第4章*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064)*，监督图学习*中描述的无监督技术对术语和文档进行聚类。然后，我们将回到二分图，展示监督分类的一个例子，我们将通过利用二分图的网络信息来完成这项工作。
- en: Entity-entity graph
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实体-实体图
- en: 'We will start by projecting our graph into the set of entity nodes. `networkx`
    provides a special submodule for dealing with bipartite graphs, `networkx.algorithms.bipartite`,
    where a number of algorithms have already been implemented. In particular, the
    `networkx.algorithms.bipartite.projection` submodule provides a number of utility
    functions to project bipartite graphs on a subset of nodes. Before performing
    projection, we must extract the nodes relative to a particular set (either documents
    or entities) using the "bipartite" property we created when we generated the graph:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先将我们的图投影到实体节点集。`networkx`提供了一个专门用于处理二分图的子模块，`networkx.algorithms.bipartite`，其中已经实现了许多算法。特别是，`networkx.algorithms.bipartite.projection`子模块提供了一些实用函数，用于将二分图投影到节点子集。在执行投影之前，我们必须使用我们在生成图时创建的“二分”属性提取与特定集合（文档或实体）相关的节点：
- en: '[PRE23]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The graph projection basically creates a new graph with the set of selected
    nodes. Edges are places between the nodes based on whether two nodes have neighbors
    in common. The basic `projected_graph` function creates such a network with unweighted
    edges. However, it is usually more informative to have edges weighted based on
    the number of common neighbors. The `projection` module provides different functions
    based on how the weights are computed. In the next section, we will use `overlap_weighted_projected_graph`,
    where the edge weight is computed using the Jaccard similarity based on common
    neighbors. However, we encourage you to also explore the other options that, depending
    on your use case and context, may best suit your aims.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图投影基本上是通过选择节点集创建一个新的图。边是节点之间的位置，基于两个节点是否有共同的邻居。基本的`projected_graph`函数创建了一个无权边的网络。然而，基于共同邻居数量的边权重通常更有信息量。`projection`模块提供了基于权重计算方式的不同函数。在下一节中，我们将使用`overlap_weighted_projected_graph`，其中边权重是通过基于共同邻居的Jaccard相似度来计算的。然而，我们也鼓励您探索其他选项，这些选项根据您的用例和上下文，可能最适合您的目标。
- en: Be aware of dimensions – filtering the graph
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意维度 - 过滤图
- en: 'There is another point of caution you should be aware of when dealing with
    projections: the dimension of the projected graph. In certain cases, like the
    one we are considering here, projection may create an extremely large number of
    edges, which makes the graph hard to analyze. In our use case, following the logic
    we used to create our network, a document node is connected to at least 10 keywords,
    plus a few entities. In the resulting entity-entity graph, all these entities
    will be connected to each other as they share at least one common neighbor (the
    document that contains them). Therefore, we will only be generating around ![](img/B16067_07_008.png)
    edges for one document. If we multiply this number for the number of documents,
    ![](img/B16067_07_009.png), we will end up with several edges that, despite the
    small use case, already become almost intractable, since there''s a few million
    edges. Although this surely represents a conservative upper bound (as some of
    the co-occurrence between entities will be common in many documents and therefore
    not repeated), it provides an order of magnitude of the complexity that you might
    expect. Therefore, we encourage you to proceed with caution before projecting
    your bipartite graph, depending on the topology of the underlying network and
    the size of your graph. One trick to reduce this complexity and make the projection
    feasible is to only consider entity nodes that have a certain degree. Most of
    the complexity arises from the presence of entities that appear only once or a
    few times, but still generate *cliques* within the graph. Such entities are not
    very informative for capturing patterns and providing insights. Besides, they
    are possibly strongly affected by statistical variability. On the other hand,
    we should focus on strong correlations that are supported by larger occurrences
    and provide more reliable statistical results.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理投影时，你应该注意的另一个问题是投影图的维度。在某些情况下，例如我们在这里考虑的情况，投影可能会创建极其大量的边，这使得图难以分析。在我们的用例中，根据我们创建网络的逻辑，一个文档节点至少连接到10个关键词，以及一些实体。在生成的实体-实体图中，所有这些实体都会因为至少有一个共同邻居（包含它们的文档）而相互连接。因此，对于一篇文档，我们只会生成大约![img/B16067_07_008.png]条边。如果我们把这个数字乘以文档的数量，![img/B16067_07_009.png]，最终会得到大量边，尽管用例规模较小，但这些边已经几乎无法处理，因为有几百万条边。尽管这无疑是一个保守的上限（因为实体之间的共现可能在许多文档中是共同的，因此不会重复），但它提供了一个可能预期的复杂性的量级。因此，我们鼓励你在投影你的二分图之前谨慎行事，这取决于底层网络的拓扑结构和你的图的大小。减少这种复杂性和使投影可行的一个技巧是只考虑具有一定度的实体节点。大部分复杂性来自于只出现一次或几次的实体，但它们在图中仍然生成*完全图*。这样的实体对于捕捉模式和提供洞察力并不很有信息量。此外，它们可能受到统计变异性的强烈影响。另一方面，我们应该关注由较大出现频率支持的强相关性，并提供更可靠的统计结果。
- en: 'Therefore, we will only consider entity nodes with a certain degree. To this
    aim, we will generate the filtered bipartite subgraph, which excludes nodes with
    low degree values, namely smaller than 5:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们只会考虑具有一定度的实体节点。为此，我们将生成过滤后的二分子图，排除度值较低的节点，即小于5的节点：
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This subgraph can now be projected without generating a graph with an excessive
    number of edges:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以投影这个子图，而不会生成一个包含过多边的图：
- en: '[PRE25]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can check the dimension of the graph with the `networkx` function of `nx.info`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`networkx`的`nx.info`函数来检查图的维度：
- en: '[PRE26]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Despite the filters we''ve applied, the number of edges and the average node
    degree are still quite large. The following graph shows the distribution of the
    degree and of the edge weights, where we can observe one peak in the degree distribution
    at fairly low values, with a fat tail toward large degree values. Also, the edge
    weight shows a similar behavior, with a peak at rather low values and fat right
    tails. These distributions suggest the presence of several small communities,
    namely cliques, which are connected to each other via some central nodes:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已应用了过滤器，边的数量和平均节点度数仍然相当大。以下图表显示了度分布和边权重的分布，我们可以观察到在较低的度值分布中有一个峰值，并向大度值方向有较宽的尾部。同样，边权重也表现出类似的行为，在较低值处有一个峰值，并且右侧尾部较宽。这些分布表明存在几个小型社区，即完全图，它们通过一些中心节点相互连接：
- en: '![Figure 7.4 – Degree and weight distribution for the entity-entity network'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.4 – 实体-实体网络的度分布和权重分布]'
- en: '](img/B16069_07_04(Merged).jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16069_07_04(Merged).jpg]'
- en: Figure 7.4 – Degree and weight distribution for the entity-entity network
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 实体-实体网络的度和权重分布
- en: 'The distribution of the edge weights also suggests that a second filter could
    be applied. The filter on the entity degree that we applied previously on the
    bipartite graph allowed us to filter out rare entities that only appeared in a
    few documents. However, the resulting graph could also be affected by the opposite
    problem: popular entities may be connected just because they tend to appear often
    in documents, even if there is not an interesting causal connection between them.
    Consider the US and Microsoft. They are almost surely connected, as it is extremely
    likely that there will be at least one or a few documents where they both appear.
    However, if there is not a strong and causal connection between them, it is very
    unlikely that the Jaccard similarity will be large. Considering only the edges
    with the largest weights allows you to focus on the most relevant and possibly
    stable relationships. The edge weight distribution shown in the preceding graph
    suggests that a suitable threshold could be `0.05`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 边权重的分布还表明可以应用第二个过滤器。我们在二分图上之前应用的实体度过滤器允许我们过滤掉仅出现在少数文档中的罕见实体。然而，得到的图也可能受到相反问题的困扰：流行的实体可能只是因为它们倾向于经常出现在文档中而连接在一起，即使它们之间没有有趣的因果联系。考虑美国和微软。它们几乎肯定连接在一起，因为它们同时出现在至少一份或几份文档中的可能性极高。然而，如果它们之间没有强大且直接的因果联系，Jaccard相似度很大是不太可能的。仅考虑具有最大权重的边可以使你专注于最相关且可能稳定的联系。前面图表中显示的边权重分布表明，合适的阈值可能是`0.05`：
- en: '[PRE27]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Such a threshold reduces the number of edges significantly, making it feasible
    to analyze the network:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的阈值可以显著减少边的数量，使得分析网络成为可能：
- en: '[PRE28]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![Figure 7.5 – Degree Distribution (Left) and Edge Weight Distribution (Right)
    for the resulting graph, after filtering based on the edge weight](img/B16069_07_05.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 经过基于边权重的过滤后得到的图的度分布（左）和边权重分布（右）](img/B16069_07_05.jpg)'
- en: Figure 7.5 – Degree Distribution (Left) and Edge Weight Distribution (Right)
    for the resulting graph, after filtering based on the edge weight
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 经过基于边权重的过滤后得到的图的度分布（左）和边权重分布（右）
- en: The preceding diagram shows the distribution of the node degree and edge weights
    for the filtered graph. The distribution for the edge weights corresponds to the
    right tail of the distribution shown in *Figure 7.4*. The relationship that the
    degree distribution has with *Figure 7.4* is less obvious, and it shows the peak
    for the nodes that have a degree around 10, as opposed to the peak shown in *Figure
    7.4*, which was observed in the low range, at around 100.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了过滤图的节点度和边权重的分布。边权重的分布对应于*图7.4*中显示的分布的右尾。度分布与*图7.4*的关系不太明显，它显示了度数约为10的节点的峰值，而*图7.4*中观察到的峰值在低范围内，约为100。
- en: Analyzing the graph
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分析图
- en: Using Gephi we can provide an overview of the overall network, which is shown
    in *Figure 7.6*.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Gephi，我们可以提供整体网络的概述，如图*图7.6*所示。
- en: 'The graph is as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图如下：
- en: '![Figure 7.6 – Entity-entity network highlighting the presence of multiple
    small subcommunities](img/B16069_07_06.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – 突出显示存在多个小型子社区的实体-实体网络](img/B16069_07_06.jpg)'
- en: Figure 7.6 – Entity-entity network highlighting the presence of multiple small
    subcommunities
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 突出显示存在多个小型子社区的实体-实体网络
- en: 'To obtain some further insights on the topology of the network, we will also
    compute some global measures, such as the average shortest path, clustering coefficient,
    and global efficiency. Although the graph has five different connected components,
    the largest one almost entirely accounts for the whole graph, including 2,254
    out of 2,265 nodes:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对网络的拓扑结构有更深入的了解，我们还将计算一些全局度量，例如平均最短路径、聚类系数和全局效率。尽管该图有五个不同的连通组件，但最大的一个几乎完全占用了整个图，包括2,254个节点中的2,265个：
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The global properties of the largest components can be found with the following
    code:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码可以找到最大组件的全局属性：
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The shortest path and global efficiency may require a few minutes of computation.
    This results in the following output:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最短路径和全局效率可能需要几分钟的计算时间。这导致以下输出：
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Based on the magnitude of these metrics (with a shortest path of about 5 and
    a clustering coefficient around 0.2), together with the degree distribution shown
    previously, we can see that the network has multiple communities of a limited
    size. Other interesting local properties, such as degree, page rank, and betweenness
    centralities distributions, are shown in the following graph, which shows how
    all these measures tend to correlate and connect to each other:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些指标的大小（最短路径约为5，聚类系数约为0.2），结合之前显示的度分布，我们可以看出该网络具有多个有限大小的社区。以下图表显示了其他有趣的局部属性，如度、页面排名和介数中心性分布，展示了所有这些指标如何相互关联和连接：
- en: '![Figure 7.7 – Relationships and distribution between the degree, page rank,
    and betweenness centrality measures](img/B16069_07_07.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7 – 度、页面排名和介数中心性度量之间的关系和分布](img/B16069_07_07.jpg)'
- en: Figure 7.7 – Relationships and distribution between the degree, page rank, and
    betweenness centrality measures
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 度、页面排名和介数中心性度量之间的关系和分布
- en: After providing a description in terms of loca;/global measures, as well as
    a general visualization of the network, we will apply some of the techniques we
    have seen in the previous chapters to identify some insights and information within
    the network. We will do this using the unsupervised techniques described in [*Chapter
    4*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064)*, Supervised Graph Learning*.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在提供局部/全局度量描述以及网络的一般可视化之后，我们将应用之前章节中看到的一些技术来识别网络中的见解和信息。我们将使用第[*4章*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064)*，监督图学习*中描述的无监督技术来完成这项工作。
- en: 'We will start by using the Louvain community detection algorithms, which, by
    optimizing their modularity, aim to identify the best partitions of the nodes
    in disjoint communities:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用Louvain社区检测算法，通过优化其模块度，旨在识别节点在不相交社区中的最佳分区：
- en: '[PRE32]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note that the results might vary between runs because of random seeds. However,
    a similar partition, with a distribution of cluster memberships similar to the
    one shown in the following graph, should emerge. We generally observe about 30
    communities, with the larger ones containing around 130-150 documents.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于随机种子，结果可能在不同运行之间有所不同。然而，应该会出现一个类似的分区，其聚类成员的分布类似于以下图表中所示。我们通常观察到大约30个社区，其中较大的社区包含大约130-150个文档。
- en: '![Figure 7.8 – Distribution of the size of the detected communities](img/B16069_07_08.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 – 检测到的社区大小的分布](img/B16069_07_08.jpg)'
- en: Figure 7.8 – Distribution of the size of the detected communities
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 检测到的社区大小的分布
- en: '*Figure 7.9* shows a close-up of one of the communities, where we can identify
    a particular topic/argument. On the left, beside the entity nodes, we can also
    see the document nodes, thus uncovering the structure of the related bipartite
    graph:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.9*显示了其中一个社区的一个局部放大图，我们可以识别出一个特定的主题/论点。在左侧，实体节点旁边，我们还可以看到文档节点，从而揭示相关二分图的结构：'
- en: '![Figure 7.9 – Close-up for one of the communities we''ve identified](img/B16069_07_09(Merged).jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图7.9 – 我们已识别的一个社区的一个局部放大图](img/B16069_07_09(Merged).jpg)'
- en: Figure 7.9 – Close-up for one of the communities we've identified
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 我们已识别的一个社区的一个局部放大图
- en: 'As shown in [*Chapter 4*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064),
    *Supervised Graph Learning*, we can extract insightful information about the topology
    and similarity between entities by using node embeddings. In particular, we can
    use Node2Vec, which, by feeding a randomly generated walk to a skip-gram model,
    can project the nodes into a vector space, where close-by nodes are mapped to
    nearby points:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第4章*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064)中所示，*监督图学习*，我们可以通过使用节点嵌入来提取有关实体拓扑和相似性的有见解的信息。特别是，我们可以使用Node2Vec，通过向skip-gram模型提供随机生成的随机游走，可以将节点投影到向量空间中，其中相邻节点被映射到附近的点：
- en: '[PRE33]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In the vector space of embeddings, we can apply traditional clustering algorithms,
    such as *GaussianMixture*, *K-means*, and *DB-scan*. As we did in the previous
    chapters, we can also project the embeddings into a 2D plane using t-SNE to visualize
    clusters and communities. Besides giving us another option to identify clusters/communities
    within the graph, Node2Vec can also be used to provide similarity between words,
    as traditionally done by `turkey`," which provides semantically similar words:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入空间的向量空间中，我们可以应用传统的聚类算法，如*高斯混合*、*K均值*和*DB-scan*。正如我们在前面的章节中所做的那样，我们还可以使用t-SNE将嵌入投影到二维平面上，以可视化集群和社区。除了给我们提供另一种在图中识别集群/社区的方法之外，Node2Vec还可以用来提供词语之间的相似性，就像传统上由`turkey`所做的那样，它提供语义上相似的词语：
- en: '[PRE34]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Although these two approaches, Node2Vec and Word2Vec, share some methodological
    similarities, the two embedding schemes come from different types of information:
    Word2Vec is built directly from the text and encloses relationships at the sentence
    level, while Node2Vec encodes a description that acts more at the document level,
    since it comes from the bipartite entity-document graph.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这两种方法，Node2Vec和Word2Vec，在方法论上有些相似之处，但两种嵌入方案来自不同类型的信息：Word2Vec直接从文本中构建，并在句子级别包含关系，而Node2Vec编码的描述在文档级别上起作用，因为它来自双边实体-文档图。
- en: Document-document graph
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文档-文档图
- en: 'Now, let''s project the bipartite graph into the set of document nodes to create
    a document-document network we can analyze. In a similar way to when we created
    an entity-entity network, we will use the `overlap_weighted_projected_graph` function
    to obtain a weighted graph that can be filtered to reduce the number of significant
    edges. Indeed, the topology of the network and the business logic used to build
    the bipartite graph do not favor clique creation, as we saw for the entity-entity
    graph: two nodes will only be connected when they share at least one keyword,
    organization, location, or person. This is certainly possible, but not extremely
    likely, within groups of 10-15 nodes, as observed for the entities.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将双边图投影到文档节点集合中，以创建一个我们可以分析的文档-文档网络。以我们创建实体-实体网络的方式，我们将使用`overlap_weighted_projected_graph`函数来获取一个加权图，可以过滤以减少显著边的数量。实际上，网络的拓扑结构和构建双边图所使用的业务逻辑并不利于团的形成，正如我们在实体-实体图中看到的那样：只有当两个节点至少共享一个关键词、组织、地点或人物时，它们才会连接起来。这当然是有可能的，但在10-15个节点的组内，这种情况并不极端可能，正如我们观察到的实体那样。
- en: 'As we did previously, we can easily build our network with the following lines:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所做的那样，我们可以轻松地使用以下几行代码构建我们的网络：
- en: '[PRE35]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following graph shows the distribution of the degree and the edge weight.
    This can help us decide on the value of the threshold to be used to filter out
    the edges. Interestingly, the node degree distribution shows a clear peak toward
    large values compared to the degree distribution observed for the entity-entity
    graph. This suggests the presence of a number of *supernodes* (that is, nodes
    with rather large degrees) that are highly connected. Also, the edge weight distribution
    shows the Jaccard index''s tendency to attain values close to 1, which are much
    larger than the ones we observed in the entity-entity graph. These two observations
    highlight a profound difference between the two networks: whereas the entity-entity
    graph is characterized by many tightly connected communities (namely cliques),
    the document-document graph is characterized by a rather tight connection among
    nodes with a large degree (which constitutes the core) versus a periphery of weakly
    connected or disconnected nodes:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了度数和边权重的分布。这可以帮助我们决定用于过滤边的阈值值。有趣的是，与观察到的实体-实体图中的度数分布相比，节点度数分布显示出向大值方向的明显峰值。这表明存在一些*超级节点*（即具有相当大度数的节点）高度连接。此外，边权重分布显示了Jaccard指数趋向于接近1的值，这些值远大于我们在实体-实体图中观察到的值。这两个观察结果突出了两个网络之间的深刻差异：而实体-实体图以许多紧密连接的社区（即团）为特征，文档-文档图则以具有大度数的节点（构成核心）之间相对紧密的连接为特征，而外围则是弱连接或未连接的节点：
- en: '![Figure 7.10 – Degree Distribution and Edge Weight Distribution for the projection
    of the bipartite graph into the document-document network](img/B16069_07_10.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图7.10 – 双边图投影到文档-文档网络中的度数分布和边权重分布](img/B16069_07_10.jpg)'
- en: Figure 7.10 – Degree Distribution and Edge Weight Distribution for the projection
    of the bipartite graph into the document-document network
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 双边图投影到文档-文档网络中的度分布和边权重分布
- en: 'It can be convenient to store all the edges in a DataFrame so that we can plot
    them and then use them to filter and, thus, create a subgraph:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有边存储在DataFrame中可能很方便，这样我们就可以绘制它们，然后使用它们进行过滤，从而创建子图：
- en: '[PRE36]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'By looking at the preceding diagram, it seems reasonable to set a threshold
    value of `0.6` for the edge weight, thus allowing us to generate a more tractable
    network using the `edge_subgraph` function of `networkx`:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看前面的图表，设置边权重阈值为`0.6`似乎是合理的，这样我们可以使用`networkx`的`edge_subgraph`函数生成一个更易于处理的网络：
- en: '[PRE37]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The following graph shows the resulting distribution for the degree and for
    the edge weight for the reduced graph:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图显示了缩减图的度分布和边权重的分布：
- en: '![Figure 7.11 – Degree Distribution and Edge Weight Distribution for the document-document
    filtered network](img/B16069_07_11.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图7.11 – 文档-文档过滤网络的度分布和边权重分布](img/B16069_07_11.jpg)'
- en: Figure 7.11 – Degree Distribution and Edge Weight Distribution for the document-document
    filtered network
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 – 文档-文档过滤网络的度分布和边权重分布
- en: 'The substantial difference in topology of the document-document graph with
    respect to the entity-entity graph can also be clearly seen in the following diagram,
    which shows a full network visualization. As anticipated by the distributions,
    the document-document network is characterized by a core network and several weekly
    connected satellites. These satellites represent all the documents that share
    none or a few keywords or entity common occurrences. The number of disconnected
    documents is quite large and accounts for almost 50% of the total:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 文档-文档图与实体-实体图在拓扑结构上的显著差异也可以在以下图表中清楚地看到，该图表显示了完整的网络可视化。正如分布所预期的，文档-文档网络以核心网络和几个不经常连接的卫星为特征。这些卫星代表所有没有或只有少数关键词或实体共同出现的文档。不连通文档的数量相当大，占总数的近50%：
- en: '![Figure 7.12 – (Left) Representation of the document-document filtered network,
    highlighting the presence of a core and a periphery. (Right) Close-up of the core,
    with some subcommunities embedded. The node size is proportional to the node degree'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.12 – (左) 文档-文档过滤网络的表示，突出显示核心和外围的存在。(右) 核心的特写，其中嵌入了一些子社区。节点大小与节点度成正比](img/B16069_07_13.jpg)'
- en: '](img/B16069_07_12(Merged).jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16069_07_12(Merged).jpg](img/B16069_07_12(Merged).jpg)'
- en: Figure 7.12 – (Left) Representation of the document-document filtered network,
    highlighting the presence of a core and a periphery. (Right) Close-up of the core,
    with some subcommunities embedded. The node size is proportional to the node degree
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 – (左) 文档-文档过滤网络的表示，突出显示核心和外围的存在。(右) 核心的特写，其中嵌入了一些子社区。节点大小与节点度成正比
- en: 'It may be worthwhile extracting the connected components for this network using
    the following commands:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令提取此网络的连通组件可能是值得的：
- en: '[PRE38]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'In the following graph, we can see the distribution for the connected component
    sizes. Here, we can clearly see the presence of a few very large clusters (the
    cores), together with a large number of disconnected or very small components
    (the periphery or satellites). This structure is strikingly different from the
    one we observed for the entity-entity graph, where all the nodes were generated
    by a very large, connected cluster:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，我们可以看到连通组件大小的分布。在这里，我们可以清楚地看到存在一些非常大的簇（核心），以及大量不连通或非常小的组件（外围或卫星）。这种结构与我们所观察到的实体-实体图的结构截然不同，在实体-实体图中，所有节点都是由一个非常大的、连通的簇生成的：
- en: '![Figure 7.13 – Distribution of the connected component sizes, highlighting
    the presence of many small-sized communities (representing the periphery) and
    a few large communities (representing the core)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 – 连通组件大小的分布，突出显示了许多小型社区（代表外围）和少数大型社区（代表核心）
- en: '](img/B16069_07_13.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16069_07_12(Merged).jpg](img/B16069_07_12(Merged).jpg)'
- en: Figure 7.13 – Distribution of the connected component sizes, highlighting the
    presence of many small-sized communities (representing the periphery) and a few
    large communities (representing the core)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 – 连通组件大小的分布，突出显示了许多小型社区（代表外围）和少数大型社区（代表核心）
- en: 'It can be interesting to investigate the structure of the core components further.
    We can extract the subgraph composed of the largest components of the network
    from the full graph with the following code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步研究核心组件的结构可能很有趣。我们可以使用以下代码从完整图中提取由网络的最大组件组成的子图：
- en: '[PRE39]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We can inspect the properties of the core network using `nx.info`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `nx.info` 检查核心网络的属性：
- en: '[PRE40]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The left panel in *Figure 7.12* shows a Gephi visualization of the core. As
    we can see, the core is composed of a few communities, along with nodes with fairly
    large degrees strongly connected to each other.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.12* 的左侧面板显示了核心的 Gephi 可视化。正如我们所见，核心由几个社区组成，以及一些相互之间有相当大度数且强连接的节点。'
- en: As we did for the entity-entity network, we can process the network to identify
    communities embedded in the graph. However, different from what we did previously,
    the document-document graph now provides a mean for judging the clustering using
    the document labels. Indeed, we expect documents belonging to the same topic to
    be close and connected to each other. Moreover, as we will see shortly, this will
    also allow us to identify similarities among topics.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在实体-实体网络中所做的那样，我们可以处理网络以识别图中嵌入的社区。然而，与之前所做不同的是，文档-文档图现在提供了一个使用文档标签来判断聚类的手段。确实，我们期望属于同一主题的文档彼此靠近并相互连接。此外，正如我们很快将看到的，这还将使我们能够识别主题之间的相似性。
- en: 'First, let''s start by extracting the candidate communities:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们先提取候选社区：
- en: '[PRE41]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Then, we will extract the topic mixture within each community to see whether
    there is a homogeneity (all the documents belonging to the same class) or some
    correlation between topics:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将提取每个社区内的主题混合，以查看是否存在同质性（所有文档都属于同一类别）或主题之间的一些相关性：
- en: '[PRE42]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '`normalizedCommunityTopics` is a DataFrame that, for each community (row in
    the DataFrame), provides the topic mixture (in percentage) of the different topics
    (along the column axis). To quantify the heterogeneity of the topic mixture within
    the clusters/communities, we must compute the Shannon entropy of each community:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`normalizedCommunityTopics` 是一个 DataFrame，对于 DataFrame 中的每个社区（行），它提供了不同主题（沿列轴）的主题混合（百分比）。为了量化集群/社区内主题混合的异质性，我们必须计算每个社区的
    Shannon 散度：'
- en: '![](img/B16067_07_010.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B16067_07_010.jpg)'
- en: 'Here, ![](img/B16067_07_011.png) represents the entropy of the cluster, ![](img/B16067_07_012.png),
    and ![](img/B16067_07_013.png) corresponds to the percentage of topic ![](img/B16067_07_014.png)
    in community ![](img/B16067_07_015.png). We must compute the empirical Shannon
    entropy for all communities:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![图片](img/B16067_07_011.png) 代表集群的熵，![图片](img/B16067_07_012.png)，而 ![图片](img/B16067_07_013.png)
    对应于主题 ![图片](img/B16067_07_014.png) 在社区 ![图片](img/B16067_07_015.png) 中的百分比。我们必须计算所有社区的实证
    Shannon 散度：
- en: '[PRE43]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The following graph shows the entropy distribution across all communities.
    Most communities have zero or very low entropy, thus suggesting that the documents
    that belong to the same class (label) tend to cluster together:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图显示了所有社区中的熵分布。大多数社区具有零或非常低的熵，这表明属于同一类别（标签）的文档往往聚集在一起：
- en: '![Figure 7.14 – Entropy distribution of the topic mixture in each community'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.14 – 每个社区主题混合的熵分布'
- en: '](img/B16069_07_14.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B16069_07_14.jpg)'
- en: Figure 7.14 – Entropy distribution of the topic mixture in each community
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 – 每个社区主题混合的熵分布
- en: 'Even if most of the communities show zero or low variability around topics,
    it is interesting to investigate whether there is a relationship between topics,
    when communities show some heterogeneity. Namely, we compute the correlation between
    topic distributions:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 即使大多数社区在主题周围表现出零或低变异性，研究社区表现出一些异质性时主题之间的关系也是有趣的。具体来说，我们计算主题分布之间的相关性：
- en: '[PRE44]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'These can then be represented and visualized using a topic-topic network:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用主题-主题网络来表示和可视化这些属性：
- en: '[PRE45]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The left-hand side of the following diagram shows the full graph representation
    for the topics network. As observed for the document-document network, the topic-topic
    graph shows a structure organized in a periphery of disconnected nodes and a strongly
    connected core. The right-hand side of the following diagram shows a close-up
    of the core network. This indicates a correlation that is supported by a semantic
    meaning, with the topics related to commodities tightly connected to each other:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图的左侧显示了主题网络的完整图表示。正如在文档-文档网络中观察到的，主题-主题图显示了一个由不连接的节点外围和一个强连接的核心组织起来的结构。以下图的右侧显示了核心网络的特写。这表明了一个由语义意义支持的关联，与商品相关的主题紧密相连：
- en: '![Figure 7.15 – (Left) Topic-topic correlation graph, organized with a periphery-core
    structure. (Right) Close-up of the core of the network](img/B16069_07_15(Merged).jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图7.15 – (左) 主题-主题相关性图，采用外围-核心结构组织。(右) 网络核心的特写](img/B16069_07_15(Merged).jpg)'
- en: Figure 7.15 – (Left) Topic-topic correlation graph, organized with a periphery-core
    structure. (Right) Close-up of the core of the network
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 – (左) 主题-主题相关性图，采用外围-核心结构组织。(右) 网络核心的特写
- en: In this section, we analyzed the different types of networks that arise when
    analyzing documents and, more generally, text sources. To do so, we used global
    and local properties to statistically describe the networks, as well as some unsupervised
    algorithms, which allowed us to unveil some structure within the graph. In the
    next section, we will show you how to leverage these graph structures when building
    a machine learning model.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析了在分析文档以及更一般性的文本来源时出现的不同类型的网络。为此，我们使用了全局和局部属性来统计描述网络，以及一些无监督算法，这些算法使我们能够揭示图中的某些结构。在下一节中，我们将向您展示如何利用这些图结构来构建机器学习模型。
- en: Building a document topic classifier
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建文档主题分类器
- en: 'To show you how to leverage a graph structure, we will focus on using the topological
    information and the connections between the entities provided by the bipartite
    entity-document graph to train multi-label classifiers. This will help us predict
    the document topics. To do this, we will analyze two different approaches:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了向您展示如何利用图结构，我们将专注于使用二分实体-文档图提供的拓扑信息和实体之间的连接来训练多标签分类器。这将帮助我们预测文档主题。为此，我们将分析两种不同的方法：
- en: '**A shallow machine-learning approach**, where we will use the embeddings we
    extracted from the bipartite network to train *traditional* classifiers, such
    as a RandomForest classifier.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一种浅层机器学习方法**，我们将使用从二分网络中提取的嵌入来训练*传统*分类器，例如RandomForest分类器。'
- en: '**A more integrated and differentiable approach** based on using a graphical
    neural network that''s been applied to heterogeneous graphs (such as the bipartite
    graph).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一种更集成和可微分的基于使用图形神经网络的方法**，该方法已应用于异构图（如二分图）。'
- en: 'Let''s consider the first 10 topics, which we have enough documentation on
    to train and evaluate our models:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑前10个主题，我们对这些主题有足够的文档来训练和评估我们的模型：
- en: '[PRE46]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The preceding code block produces the following output. This shows the names
    of the topics, all of which we will focus on in the following analysis:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块生成了以下输出。这显示了主题的名称，在以下分析中，我们都会关注这些主题：
- en: '[PRE47]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'When training topic classifiers, we must restrict our focus to only those documents
    that belong to such labels. The filtered corpus can easily be obtained by using
    the following code block:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练主题分类器时，我们必须将我们的重点限制在仅属于此类标签的文档上。可以通过以下代码块轻松获得过滤后的语料库：
- en: '[PRE48]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Now that we have extracted and structured the dataset, we are ready to start
    training our topic models and evaluating their performance. In the next section,
    we will start by creating a simple model using shallow learning methods so that
    we can increase the complexity of the model by using graph neural networks.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经提取并结构化了数据集，我们准备开始训练我们的主题模型并评估其性能。在下一节中，我们将首先创建一个简单的模型，使用浅层学习方法，这样我们就可以通过使用图神经网络来增加模型的复杂性。
- en: Shallow learning methods
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 浅层学习方法
- en: 'We will start by implementing a shallow approach for the topic classification
    tasks by leveraging the network''s information. We will show you how to do this
    so that you can customize even further, depending on your use case:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先通过利用网络信息来实现一个浅层方法，用于主题分类任务。我们将向您展示如何做到这一点，以便您可以根据您的用例进一步定制：
- en: 'First, we will compute the embeddings by using `Node2Vec` on the bipartite
    graph. Filtered document-document networks are characterized by a periphery with
    many nodes that are disconnected, so they would not benefit from topological information.
    On the other hand, the unfiltered document-document network will have many edges,
    which makes the scalability of the approach an issue. Therefore, using the bipartite
    graph is crucial in order to efficiently leverage the topological information
    and the connection between entities and documents:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将使用`Node2Vec`在二分图上计算嵌入。过滤后的文档-文档网络具有许多未连接的节点，因此它们不会从拓扑信息中受益。另一方面，未过滤的文档-文档网络将具有许多边，这使得方法的可扩展性成为一个问题。因此，使用二分图对于有效地利用拓扑信息和实体与文档之间的连接至关重要：
- en: '[PRE49]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Here, the `dimension` embedding, as well as our `window`, which is used for
    generating the walks, are hyperparameters that must be optimized via cross-validation.
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，`dimension`嵌入以及用于生成游走的`window`都是必须通过交叉验证进行优化的超参数。
- en: 'To make this computationally efficient, a set of embeddings can be computed
    beforehand, saved to disk, and then be used in the optimization process. This
    would work based on the assumption that we are in a *semi-supervised* setting
    or in a *transductive* task, where we have connection information about the entire
    dataset, apart from their labels, at training time. Later in this chapter, we
    will outline another approach, based on graph neural networks, that provides an
    inductive framework for integrating topology when training classifiers. Let''s
    store the embeddings in a file:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了提高计算效率，可以事先计算一组嵌入，将其保存到磁盘，然后在优化过程中使用。这基于我们处于一个*半监督*设置或一个*归纳*任务，在训练时间我们有关于整个数据集的连接信息，除了它们的标签。在本章的后面部分，我们将概述另一种基于图神经网络的方法，它为在训练分类器时集成拓扑提供了一个归纳框架。让我们将嵌入存储在文件中：
- en: '[PRE50]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Here, we can choose and loop different values for `dimension` and `window`.
    Some possible choices are 10, 20, and 30 for both variables.
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们可以选择并循环不同的`dimension`和`window`值。对于这两个变量，一些可能的选择是10、20和30。
- en: 'These embeddings can be integrated into a scikit-learn `transformer` so that
    they can be used in a grid search cross-validation process:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些嵌入可以集成到scikit-learn的`transformer`中，以便在网格搜索交叉验证过程中使用：
- en: '[PRE51]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'To build a modeling training pipeline, we will split our corpus into training
    and test sets:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了构建建模训练管道，我们将我们的语料库分为训练集和测试集：
- en: '[PRE52]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We will also build functions to conveniently extract features and labels:'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还将构建函数以方便地提取特征和标签：
- en: '[PRE53]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now, we can instantiate the modeling pipeline:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以实例化建模管道：
- en: '[PRE54]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let''s define the parameter space, as well as the configuration, for the cross-validated
    grid search:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义交叉验证网格搜索的参数空间以及配置：
- en: '[PRE55]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Finally, let''s train our topic model by using the `fit` method of the sklearn
    API:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们使用sklearn API的`fit`方法来训练我们的主题模型：
- en: '[PRE56]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Great! You have just created your topic model, which leverages the graph''s
    information. Once the best model has been identified, we can use this model on
    the test dataset to evaluate its performance. To do so, we must define the following
    helper function, which allows us to obtain a set of predictions:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！你刚刚创建了一个利用图信息的话题模型。一旦确定了最佳模型，我们就可以使用这个模型在测试数据集上评估其性能。为此，我们必须定义以下辅助函数，它允许我们获得一组预测：
- en: '[PRE57]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Using `sklearn` functionalities, we can promptly look at the performance of
    the trained classifier:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sklearn`功能，我们可以迅速查看训练分类器的性能：
- en: '[PRE58]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This provides the following output, which shows the overall performance measure
    that''s received by the F1-score. This is around 0.6 – 0.8, depending on how unbalanced
    classes are accounted for:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了以下输出，显示了通过F1分数获得的总体性能指标。这大约在0.6 – 0.8之间，具体取决于如何处理不平衡的类别：
- en: '[PRE59]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: You can play around with the types and hyperparameters of the analytical pipeline,
    vary the models, and experiment with different values when you're encoding the
    embeddings. As we mentioned previously, the preceding approach is clearly transductive
    since it uses an embedding that's been trained on the entire dataset. This is
    a common situation in semi-supervised tasks, where the labeled information is
    only present in a small subset of points, and the task is to infer the labels
    for all the unknown samples. In the next subsection, we will outline how to build
    an inductive classifier using graph neural networks. These can be used when the
    test samples are not known at training time.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以尝试不同的分析管道类型和超参数，改变模型，并在编码嵌入时尝试不同的值。正如我们之前提到的，前面的方法显然是归纳的，因为它使用了一个在整个数据集上训练的嵌入。这在半监督任务中是一个常见的情况，其中标记信息仅存在于一小部分点上，任务是从所有未知样本中推断标签。在下一个小节中，我们将概述如何使用图神经网络构建一个归纳分类器。这些可以在测试样本在训练时未知的情况下使用。
- en: Graph neural networks
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图神经网络
- en: Now, let's describe a neural network-based approach that natively integrates
    and makes use of the graph structure. Graph neural networks were introduced in
    [*Chapter 3*](B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046), *Unsupervised Graph
    Learning*, and [*Chapter 4*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064),
    *Supervised Graph Learning*. However, here, we will show you how to apply this
    framework to heterogeneous graphs; that is, graphs where there is more than one
    type of node. Each node type might have a different set of features and the training
    might target only one specific node type over the other.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们描述一种基于神经网络的、原生集成并利用图结构的方法。图神经网络在[第3章](B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046)中介绍，*无监督图学习*，以及[第4章](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064)中介绍，*监督图学习*。然而，在这里，我们将向您展示如何将此框架应用于异构图；即，存在多种节点类型的图。每种节点类型可能有一组不同的特征，训练可能只针对一种特定的节点类型而不是其他。
- en: 'The approach we will show here will make use of `stellargraph` and the `GraphSAGE`
    algorithms, which we described previously. These methods also support the use
    of features for each node, instead of just relying on the topology of the graph.
    If you do not have any node features, the one-hot node representation can be used
    in its place, as shown in [*Chapter 6*](B16069_06_Final_JM_ePub.xhtml#_idTextAnchor100),
    *Social Network Graphs*. However, here, to make things more general, we will produce
    a set of node features based on the TF-IDF score (which we saw earlier) for each
    entity and keyword. Here, we will show you a step-by-step guide that will help
    you train and evaluate a model, based on graph neural networks, for predicting
    document topic classification:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里将要展示的方法将利用`stellargraph`和`GraphSAGE`算法，这些算法我们在之前已经描述过。这些方法也支持为每个节点使用特征，而不仅仅是依赖于图的拓扑结构。如果你没有任何节点特征，可以使用一热节点表示法来代替，如[第6章](B16069_06_Final_JM_ePub.xhtml#_idTextAnchor100)中所示，*社交网络图*。然而，在这里，为了使事情更加通用，我们将基于每个实体和关键词的TF-IDF分数（我们之前已经看到过）生成一组节点特征。在这里，我们将向您展示一个逐步指南，这将帮助您基于图神经网络训练和评估一个模型，用于预测文档主题分类：
- en: 'Let''s start by computing the TF-IDF score for each document. `sklearn` already
    provides some functionalities that allow us to easily compute the TF-IDF scores
    from a corpus of documents. The `TfidfVectorizer` `sklearn` class already comes
    with a `tokenizer` embedded. However, since we already have a tokenized and lemmatized
    version that we extracted with `spacy`, we can also provide an implementation
    of a custom tokenizer that leverages on spaCy processing:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先计算每个文档的TF-IDF分数。`sklearn`已经提供了一些功能，允许我们轻松地从文档语料库中计算TF-IDF分数。`TfidfVectorizer`
    `sklearn`类已经内置了一个`tokenizer`。然而，由于我们已经有了一个使用`spacy`提取的标记化和词元化的版本，我们也可以提供一个自定义的`tokenizer`实现，该实现利用spaCy处理：
- en: '[PRE60]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'This can be used in `TfidfVectorizer`:'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这可以在`TfidfVectorizer`中使用：
- en: '[PRE61]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'To make the approach truly inductive, we will only train the TF-IDF for the
    training set. This will only be applied to the test set:'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了使这种方法真正具有归纳性，我们只对训练集进行TF-IDF训练。这仅适用于测试集：
- en: '[PRE62]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'For our convenience, the two TF-IDF representations (for the training and test
    sets) can now be stacked together into a single data structure representing the
    features for the document nodes for the whole graph:'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了方便起见，现在可以将两个TF-IDF表示（训练集和测试集）堆叠成一个单一的数据结构，表示整个图中文档节点的特征：
- en: '[PRE63]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Beside the feature information for document nodes, we will also build a simple
    feature vector for entities, based on the one-hot encoding representation of the
    entity type:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了文档节点的特征信息外，我们还将为实体构建一个简单的特征向量，基于实体类型的one-hot编码表示：
- en: '[PRE64]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We now have all the information we need to create an instance of a `StellarGraph`.
    We will do this by merging the information of the node features, both for documents
    and for entities, with the connections provided by the `edges` DataFrame. We should
    only filter out some of the edges/nodes so that we only include the documents
    that belong to the targeted topics:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在拥有了创建`StellarGraph`实例所需的所有信息。我们将通过合并节点特征的信息，包括文档和实体的信息，以及由`edges`数据框提供的连接来完成此操作。我们应该仅过滤掉一些边/节点，以便只包括属于目标主题的文档：
- en: '[PRE65]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'With that, we have created our `StellarGraph`. We can inspect the network,
    similar to what we did for `networkx`, with the following command:'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这样，我们就创建了我们的`StellarGraph`。我们可以使用以下命令检查网络，类似于我们对`networkx`所做的那样：
- en: '[PRE66]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'This produces the following overview:'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这产生了以下概述：
- en: '[PRE67]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The `StellarGraph` description is actually very informative. Besides, `StellarGraph`
    also natively handles different types of nodes and edges and provides out-of-the-box
    segmented statistics for each node/edge type.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`StellarGraph`的描述实际上非常详尽。此外，`StellarGraph`还原生支持不同类型的节点和边，并为每种节点/边类型提供即插即用的分段统计数据。'
- en: 'You may have noted that the graph we just created includes both training and
    test data. To truly test the performance of an inductive approach and avoid information
    from being linked between the train and test sets, we need to create a subgraph
    that only contains the data available at training time:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可能已经注意到我们刚刚创建的图既包含训练数据又包含测试数据。为了真正测试归纳方法的性能并避免训练集和测试集之间的信息链接，我们需要创建一个仅包含训练时可用数据的子图：
- en: '[PRE68]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The considered subgraph contains 16,927 nodes and 62,454 edges, compared to
    the 23,998 nodes and 86,849 edges in the entire graph.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑的子图包含16,927个节点和62,454条边，与整个图中的23,998个节点和86,849条边相比。
- en: 'Now that we only have the data and the network available at training time,
    we can build our machine learning model on top of it. To do so, we will split
    the data into train, validation, and test data. For training, we will only use
    10% of the data, which resembles a semi-supervised task:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们只有训练时可用数据和网络，我们可以在其上构建我们的机器学习模型。为此，我们将数据分为训练、验证和测试数据。对于训练，我们只使用10%的数据，这类似于半监督任务：
- en: '[PRE69]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now, we can start to build our graph neural network model using `stellargraph`
    and the `keras` API. First, we will create a generator able to produce the samples
    that will feed the neural network. Note that, since we are dealing with a heterogeneous
    graph, we need a generator that will sample examples from nodes that only belong
    to specific class. Here, we will be using the `HinSAGENodeGenerator` class, which
    generalizes the node generator we used for the homogeneous graph into heterogeneous
    graphs, allowing us to specify the node type we want to target:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以开始使用`stellargraph`和`keras` API构建我们的图神经网络模型。首先，我们将创建一个生成器，能够生成将输入神经网络的样本。请注意，由于我们处理的是一个异构图，我们需要一个生成器，它将只从属于特定类的节点中采样示例。在这里，我们将使用`HinSAGENodeGenerator`类，它将用于同构图中的节点生成器推广到异构图，允许我们指定我们想要的目标节点类型：
- en: '[PRE70]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Using this object, we can create a generator for the train and validation datasets:'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用此对象，我们可以为训练和验证数据集创建生成器：
- en: '[PRE71]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Now, we can create our GraphSAGE model. As we did for the generator, we need
    to use a model that can handle heterogenous graphs. Here, `HinSAGE` will be used
    in place of `GraphSAGE`:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以创建我们的GraphSAGE模型。像生成器一样，我们需要使用一个可以处理异构图的模式。在这里，我们将使用`HinSAGE`代替`GraphSAGE`：
- en: '[PRE72]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Note that in the final dense layer, we use a *sigmoid* activation function
    instead of a *softmax* activation function, since the problem at hand is a multi-class,
    multi-label task. Thus, a document may belong to more than one class, and the
    sigmoid activation function seems a more sensible choice in this context. As usual,
    we will compile our Keras model:'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，在最终的密集层中，我们使用的是*sigmoid*激活函数而不是*softmax*激活函数，因为当前的问题是一个多类、多标签任务。因此，一个文档可能属于多个类别，在这种情况下，sigmoid激活函数似乎是一个更合理的选择。像往常一样，我们将编译我们的Keras模型：
- en: '[PRE73]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Finally, we will train the neural network model:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将训练神经网络模型：
- en: '[PRE74]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'This results in the following output:'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure.7.16 – (Top) Train and validation accuracy versus the number of epochs.
    (Bottom) Binary cross-entropy loss for the training and validation dataset versus
    the number of epochs](img/B16069_07_016.jpg)'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图.7.16 – （顶部）训练和验证准确率与训练轮数的关系。（底部）训练和验证数据集的二进制交叉熵损失与训练轮数的关系](img/B16069_07_016.jpg)'
- en: Figure.7.16 – (Top) Train and validation accuracy versus the number of epochs.
    (Bottom) Binary cross-entropy loss for the training and validation dataset versus
    the number of epochs
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图.7.16 – （顶部）训练和验证准确率与训练轮数的关系。（底部）训练和验证数据集的二进制交叉熵损失与训练轮数的关系
- en: The preceding graph shows the plots of the evolution of the train and validation
    losses and accuracy versus the number of epochs. As we can see, the train and
    validation accuracy increase consistently, up to around 30 epochs. Here, the accuracy
    of the validation set settle to a *plateau*, whereas the training accuracy continues
    to increase, indicating a tendency for overfitting. Thus, stopping training at
    around 50 seems a rather legitimate choice.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上一张图显示了训练和验证损失以及准确率随训练轮数变化的曲线。如图所示，训练和验证准确率持续上升，直到大约30轮。在这里，验证集的准确率达到了一个*平台期*，而训练准确率仍在上升，表明有过度拟合的趋势。因此，在约50轮时停止训练似乎是一个相当合理的选择。
- en: 'Once the model has been trained, we can test its performance on the test set:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们就可以在测试集上测试其性能：
- en: '[PRE75]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'This should provide the following values:'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这应该提供以下值：
- en: '[PRE76]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Note that because of the unbalanced label distribution, accuracy may not be
    the best choice for assessing performances. Besides, a value of 0.5 is generally
    used for thresholding, so providing label assignment may also be sub-optimal in
    unbalanced settings.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，由于标签分布不平衡，准确率可能不是评估性能的最佳选择。此外，通常使用0.5作为阈值，因此在不平衡设置中提供标签分配也可能不是最优的。
- en: 'To identify the best threshold to be used to classify the documents, we will
    compute the prediction over all the test samples:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了确定用于分类文档的最佳阈值，我们将对所有测试样本进行预测：
- en: '[PRE77]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Then, we will compute the F1-score with a macro average (where the F1-score
    for the single classes are averaged) for different threshold choices:'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们将计算不同阈值选择的宏平均F1分数：
- en: '[PRE78]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'As shown in the following graph, a threshold value of 0.2 seems to be the best
    choice as it achieves the best performance:'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如以下图表所示，0.2的阈值似乎是最好的选择，因为它实现了最佳的性能：
- en: '![Figure 7.17 – Macro-averaged F1-score versus the threshold used for labeling'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.17 – 使用标签的阈值与宏平均F1分数的关系'
- en: '](img/B16069_07_17.jpg)'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16069_07_17.jpg)'
- en: Figure 7.17 – Macro-averaged F1-score versus the threshold used for labeling
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.17 – 使用标签的阈值与宏平均F1分数的关系
- en: 'Using a threshold value of 0.2, we can extract the classification report for
    the test set:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用0.2的阈值，我们可以提取测试集的分类报告：
- en: '[PRE79]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'This gives us the following output:'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '[PRE80]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'At this point, we have trained a graph neural network model and assessed its
    performance. Now, let''s apply this model to a set of unobserved data – the data
    that we left out at the very beginning – and represent the true test data in an
    inductive setting. To do this, we need to instantiate a new generator:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经训练了一个图神经网络模型并评估了其性能。现在，让我们将此模型应用于一组未观察到的数据——我们在一开始就留下的数据——并在归纳设置中表示真实的测试数据。为此，我们需要实例化一个新的生成器：
- en: '[PRE81]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Note that the graph we''ve taken as an input from `HinSAGENodeGenerator` is
    now the entire graph (in place of the filtered one we used previously), which
    contains both training and test documents. Using this class, we can create a generator
    that only samples from the test nodes, filtering out the ones that do not belong
    to one of our main selected topics:'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，我们从`HinSAGENodeGenerator`获取的图形现在是一个完整的图形（代替我们之前使用的过滤图形），它包含训练和测试文档。使用这个类，我们可以创建一个生成器，它只从测试节点中采样，过滤掉不属于我们主要选择主题之一的节点：
- en: '[PRE82]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The model can then be evaluated over these samples, and the labels are predicted
    using the threshold we identified earlier; that is, 0.2:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型可以在这些样本上评估，并使用我们之前确定的阈值来预测标签；即，0.2：
- en: '[PRE83]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Finally, we can extract the performance of the inductive test dataset:'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们可以提取归纳测试数据集的性能：
- en: '[PRE84]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'This produces the following table:'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会产生以下表格：
- en: '[PRE85]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Compared to the shallow learning method, we can see that we have achieved a
    substantial improvement in performance that's between 5-10%.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 与浅层学习方法相比，我们可以看到我们在性能上取得了显著的提升，介于5-10%之间。
- en: Summary
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, you learned how to process unstructured information and how
    to represent such information by using graphs. Starting from a well-known benchmark
    dataset, the Reuters-21578 dataset, we applied standard NLP engines to tag and
    structure textual information. Then, we used these high-level features to create
    different types of networks: knowledge-based networks, bipartite networks, and
    projections for a subset of nodes, as well as a network relating the dataset topics.
    These different graphs have also allowed us to use the tools we presented in previous
    chapters to extract insights from the network representation.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了如何处理非结构化信息，以及如何使用图来表示此类信息。从众所周知的基准数据集，即路透社-21578数据集开始，我们应用了标准的NLP引擎来标记和结构化文本信息。然后，我们使用这些高级特征来创建不同类型的网络：基于知识的网络、二分网络、以及节点子集的投影，以及与数据集主题相关的网络。这些不同的图也使我们能够使用我们在前几章中介绍的工具从网络表示中提取见解。
- en: We used local and global properties to show you how these quantities can represent
    and describe structurally different types of networks. We then used unsupervised
    techniques to identify semantic communities and cluster documents that belong
    to similar subjects/topics. Finally, we used the labeled information provided
    in a dataset to train supervised multi-class multi-label classifiers, which also
    leveraged the topology of the network.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用局部和全局属性向您展示了这些数量如何表示和描述结构上不同的网络类型。然后，我们使用无监督技术来识别语义社区并对属于相似主题/主题的文档进行聚类。最后，我们使用数据集中提供的标记信息来训练监督多类多标签分类器，这些分类器也利用了网络的拓扑结构。
- en: 'Then, we applied supervised techniques to a heterogeneous graph, where two
    different node types are present: documents and entities. In this setting, we
    showed you how to implement both transductive and inductive approaches by using
    shallow learning and graph neural networks, respectively.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将监督技术应用于一个异构图，其中存在两种不同的节点类型：文档和实体。在这个设置中，我们向您展示了如何通过使用浅层学习和图神经网络，分别实现归纳和演绎方法。
- en: 'In the next chapter, we will look at another domain where graph analytics can
    be efficiently used to extract insights and/or create machine learning models
    that leverage network topology: transactional data. The next use case will also
    allow you to generalize the bipartite graph concepts that were introduced in this
    chapter to another level: tripartite graphs.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨另一个领域，其中图分析可以有效地用于提取见解和/或创建利用网络拓扑的机器学习模型：交易数据。下一个用例也将允许您将本章中引入的二分图概念推广到另一个层次：三分图。
