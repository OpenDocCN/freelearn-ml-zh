- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: On Cybersecurity and Machine Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于网络安全与机器学习
- en: With the dawn of the Information Age, cybersecurity has become a pressing issue
    in today’s society and a skill that is much sought after in industry. Businesses,
    governments, and individual users are all at risk of security attacks and breaches.
    The fundamental goal of cybersecurity is to keep users and their data safe. Cybersecurity
    is a multi-faceted problem, ranging from highly technical domains (cryptography
    and network attacks) to user-facing domains (detecting hate speech or fraudulent
    credit card transactions). It helps to prevent sensitive information from being
    corrupted, avoid financial fraud and losses, and safeguard users and their devices
    from harmful actors.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着信息时代的到来，网络安全已成为当今社会的一个紧迫问题，并且在工业界非常受欢迎的技能。企业、政府和个人用户都面临着安全攻击和泄露的风险。网络安全的根本目标是保护用户及其数据安全。网络安全是一个多方面的难题，从高度技术性的领域（密码学和网络攻击）到面向用户的领域（检测仇恨言论或欺诈信用卡交易）。它有助于防止敏感信息被篡改，避免金融欺诈和损失，并保护用户及其设备免受有害行为者的侵害。
- en: 'A large part of cybersecurity analytics, investigations, and detections are
    now driven by **machine learning** (**ML**)and “smart” systems. Applying data
    science and ML to the security space presents a unique set of challenges: the
    lack of sufficiently labeled data, limitations on powerful models due to the need
    for explainability, and the need for nearly perfect precision due to high-stakes
    scenarios. As we progress through the book, you will learn how to handle these
    critical tasks and apply your ML and data science skills to cybersecurity problems.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 网络安全分析、调查和检测的大部分工作现在都是由**机器学习**（ML）和“智能”系统驱动的。将数据科学和机器学习应用于安全领域带来了一系列独特的挑战：缺乏足够标记的数据，由于需要可解释性而对强大模型的限制，以及由于高风险场景而需要几乎完美的精确度。随着我们继续阅读本书，您将学习如何处理这些关键任务，并将您的机器学习和数据科学技能应用于网络安全问题。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: The basics of cybersecurity
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络安全基础知识
- en: An overview of machine learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习概述
- en: Machine learning – cybersecurity versus other domains
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习——网络安全与其他领域
- en: By the end of this chapter, you will have built the foundation for further projects.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将为后续项目打下基础。
- en: The basics of cybersecurity
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络安全基础知识
- en: 'This book aims to marry two important fields of research: cybersecurity and
    ML. We will present a brief overview of cybersecurity, how it is defined, what
    the end goals are, and what problems arise.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本书旨在将两个重要的研究领域结合起来：网络安全和机器学习（ML）。我们将简要介绍网络安全，它的定义，最终目标是什么，以及会出现什么问题。
- en: Traditional principles of cybersecurity
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络安全传统原则
- en: 'The fundamental aim of cybersecurity is to keep users and data safe. Traditionally,
    the goals of cybersecurity were three-fold: **confidentiality, integrity, and
    availability**, the **CIA** triad.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 网络安全的基本目标是保护用户和数据安全。传统上，网络安全的目的是三方面的：**保密性、完整性和可用性**，即**CIA**三要素。
- en: Let us now examine each of these in depth.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在深入探讨这些问题。
- en: Confidentiality
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保密性
- en: The confidentiality goal aims to keep data secret from unauthorized parties.
    Only authorized entities should have access to data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 保密性目标旨在使数据对未经授权的各方保密。只有授权实体才能访问数据。
- en: Confidentiality can be achieved by encrypting data. Encryption is a process
    where plain-text data is coded into a ciphertext using an encryption key. The
    ciphertext is not human-readable; a corresponding decryption key is needed to
    decode the data. Encryption of information being sent over networks prevents attackers
    from reading the contents, even if they intercept the communication. Encrypting
    data at rest ensures that adversaries will not be able to read your data, even
    if the physical data storage is compromised (for example, if an attacker breaks
    into an office and steals hard drives).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过加密数据可以实现保密性。加密是一个过程，其中使用加密密钥将明文数据编码成密文。密文是不可读的；需要相应的解密密钥来解码数据。通过网络发送的信息加密可以防止攻击者读取内容，即使他们截获了通信。静态数据的加密确保即使物理数据存储被破坏（例如，如果攻击者闯入办公室并窃取硬盘），对手也无法读取您的数据。
- en: Another approach to ensuring confidentiality is access control. Controlling
    access to information is the first step in preventing unauthorized exposure or
    sharing of data (whether intentional or not). Access to data should be granted
    by following the principle of least privilege; an individual or application must
    have access to the minimum data that it requires to perform its function. For
    example, only the finance division in a company should have access to revenue
    and transaction information. Only system administrators should be able to view
    network and access logs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 确保机密性的另一种方法是访问控制。控制对信息的访问是防止未经授权泄露或共享数据（无论是有意还是无意）的第一步。数据访问应遵循最小权限原则；个人或应用程序必须有权访问执行其功能所需的最小数据。例如，公司中的财务部门应该有权访问收入和交易信息。只有系统管理员应该能够查看网络和访问日志。
- en: ML can help in detecting abnormal access, suspicious behavior patterns, or traffic
    from malicious sources, thus ensuring that confidentiality is maintained. For
    example, suppose an administrator suddenly starts accessing confidential/privileged
    files outside of the regular pattern for themselves or administrators in general.
    An ML model may flag it as anomalous and set off alarms so that the administrator
    can be investigated.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以帮助检测异常访问、可疑的行为模式或恶意来源的流量，从而确保机密性得到维护。例如，假设管理员突然开始在他们自己或一般管理员常规模式之外访问机密/特权文件。机器学习模型可能会将其标记为异常并触发警报，以便对管理员进行调查。
- en: Integrity
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完整性
- en: The integrity goal ensures that data is trustworthy and tamper-free. If the
    data can be tampered with, there is no guarantee of its authenticity and accuracy;
    such data cannot be trusted.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 完整性目标确保数据是可信的且不受篡改。如果数据可以被篡改，那么其真实性和准确性就无法保证；这样的数据是不可信的。
- en: Integrity can be ensured using hashing and checksums. A **checksum** is a numerical
    value computed by applying a hash function to the data. Even if a single bit of
    the data were to change, the checksum would change. Digital signatures and certificates
    also facilitate integrity; once an email or code library has been digitally signed
    by a user, it cannot be changed; any change requires a new digital signature.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用哈希和校验和来确保完整性。校验和是通过应用哈希函数到数据上计算出的数值。即使数据的一个比特发生变化，校验和也会改变。数字签名和证书也有助于确保完整性；一旦用户对电子邮件或代码库进行了数字签名，它就不能更改；任何更改都需要新的数字签名。
- en: Attackers may wish to compromise the integrity of a system or service for their
    gain. For example, an attacker may intercept incoming requests to a banking server
    and change the destination account for any money transfer. Malicious browser extensions
    may redirect users to a site for gaming traffic and advertising statistics; the
    original destination URL entered by the user was tampered with. By analyzing the
    patterns in data coupled with other signals, ML models can detect integrity breaches.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者可能为了自己的利益而希望破坏系统或服务的完整性。例如，攻击者可能会拦截对银行服务器的入站请求，并更改任何转账的目的账户。恶意浏览器扩展可能会将用户重定向到用于游戏流量和广告统计的网站；用户输入的原始目标URL已被篡改。通过分析数据模式结合其他信号，机器学习模型可以检测到完整性破坏。
- en: Availability
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可用性
- en: The first two goals ensure that data is kept secret, tamper-free, and safe from
    attackers. However, these guarantees are meaningless if authorized users cannot
    access the data as needed. The availability goal ensures that information is always
    available to legitimate users of a system.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个目标确保数据保持秘密、不受篡改，并免受攻击者的侵害。然而，如果授权用户无法按需访问数据，这些保证就毫无意义。可用性目标确保信息始终对系统的合法用户可用。
- en: Attackers may try to compromise availability by executing a **denial-of-service**
    (**DoS**) attack, where the target service is bombarded with incoming requests
    from unauthorized or dummy nodes. For example, an attacker may send millions of
    dummy queries to a database server. While the attacker may not be able to actually
    extract any useful information, the server is so overwhelmed that legitimate queries
    by authorized users are never executed. Alternatively, an attacker may also degrade
    availability by physically destroying data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者可能会通过执行**拒绝服务**（**DoS**）攻击来尝试破坏可用性，在这种攻击中，目标服务会被来自未经授权或虚假节点的入站请求所淹没。例如，攻击者可能会向数据库服务器发送数百万个虚假查询。虽然攻击者可能无法实际提取任何有用的信息，但服务器如此繁忙，以至于合法用户的合法查询永远不会被执行。或者，攻击者也可能通过物理破坏数据来降低可用性。
- en: Availability can be guaranteed by implementing redundancy in data and services.
    Regular backup of data ensures that it is still available even if one copy is
    destroyed or tampered with. If multiple API endpoints to achieve the same functionality
    are present, legitimate users can switch to another endpoint, and availability
    will be preserved. Similar to confidentiality and integrity, pattern analysis
    and classification algorithms can help detect DoS attacks. An emerging paradigm,
    graph neural networks, can help detect coordinated bot attacks known as **distributed
    denial of** **service** (**DDoS**).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在数据和服务中实施冗余可以保证可用性。定期备份数据确保即使一个副本被破坏或篡改，数据仍然可用。如果存在多个API端点以实现相同的功能，合法用户可以切换到另一个端点，从而保持可用性。类似于机密性和完整性，模式分析和分类算法可以帮助检测拒绝服务攻击。一种新兴的范式，图神经网络，可以帮助检测被称为**分布式拒绝服务**（DDoS）的协调机器人攻击。
- en: Modern cybersecurity – a multi-faceted issue
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现代网络安全 – 一个多方面的问题
- en: The CIA triad focuses solely on cybersecurity for data. However, cybersecurity
    today extends far beyond just data and servers. Data stored in servers becomes
    information used by organizations, which is transformed into the applications
    that are ultimately used by humans. Cybersecurity encompasses all these aspects,
    and there are different problem areas in each aspect.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 中央情报局的三要素主要关注数据的网络安全。然而，今天的网络安全已经远远超出了仅仅数据和服务器。存储在服务器中的数据变成了组织使用的信息，这些信息最终被人类使用的应用程序所转换。网络安全涵盖了所有这些方面，并且每个方面都有不同的问题区域。
- en: '*Figure 1**.1* shows how varied and multi-faceted the issue of cybersecurity
    is and the various elements it encompasses:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1.1* 展示了网络安全问题的多样性和多面性，以及它所包含的各种元素：'
- en: '![Figure 1.1 – Various problem areas in cybersecurity](img/B19327_01_01.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1 – 网络安全中的各种问题区域](img/B19327_01_01.jpg)'
- en: Figure 1.1 – Various problem areas in cybersecurity
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – 网络安全中的各种问题区域
- en: 'In the following sections, we will discuss some common security-related problems
    and research areas in four broad categories: data security, information security,
    application security, and user security.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将讨论四个广泛类别中一些常见的与安全相关的问题和研究领域：数据安全、信息安全、应用安全和用户安全。
- en: Data security
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据安全
- en: Data in its raw format is stored on hard drives in offices or in the cloud,
    which is eventually stored in physical machines in data centers. At the data level,
    the role of cybersecurity is to keep the data safe. Thus, the focus is on maintaining
    confidentiality, integrity, and availability (the three goals of the CIA triad)
    of the data. Cybersecurity problems at this level focus on novel cryptographic
    schemes, lightweight encryption algorithms, fault tolerance systems, and complying
    with regulations for data retention.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据以原始格式存储在办公室的硬盘驱动器上或在云端，最终存储在数据中心物理机器中。在数据层面，网络安全的作用是确保数据安全。因此，重点是保持数据的机密性、完整性和可用性（CIA三要素的目标）。这一层面的网络安全问题主要集中在新型加密方案、轻量级加密算法、容错系统和遵守数据保留法规。
- en: Information security
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信息安全
- en: Data from data centers and the cloud is transformed into information, which
    is used by companies to build various services. At the information level, the
    role of cybersecurity is to ensure that the information is being accessed and
    handled correctly by employees. Problems at this level focus on network security
    administration, detecting policy violations and insider threats, and ensuring
    that there is no inadvertent leakage of private data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中心和企业云中的数据被转换成信息，这些信息被公司用来构建各种服务。在信息层面，网络安全的作用是确保信息被员工正确访问和处理。这一层面的问题主要集中在网络安全管理、检测政策违规和内部威胁，以及确保没有无意中泄露的私人数据。
- en: Application security
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用安全
- en: Information is transformed into a form suitable for consumers using a variety
    of services. An example of this is information about Facebook users being transformed
    into a list of top friend recommendations. At the application level, the role
    of cybersecurity is to ensure that the application cannot be compromised. Problems
    at this level focus on malware detection, supply chain attack detection, anomaly
    detection, detecting bot and automated accounts, and flagging phishing emails
    and malicious URLs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 信息通过各种服务被转换成适合消费者使用的形式。这方面的一个例子是将Facebook用户的信息转换成顶级朋友推荐列表。在应用层面，网络安全的角色是确保应用程序不会被破坏。这一层次的问题主要集中在恶意软件检测、供应链攻击检测、异常检测、检测机器人账户和自动化账户，以及标记钓鱼邮件和恶意URL。
- en: User security
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用户安全
- en: Finally, applications are used by human end users, and the role of cybersecurity
    here is to ensure the safety of these users. Problems at this level include detecting
    hate speech, content moderation, flagging fraudulent transactions, characterizing
    abusive behavior, and protecting users from digital crimes (identity theft, scams,
    and extortion). Note that this aspect goes beyond technical elements of cybersecurity
    and enters into the realm of humanities, law, and the social sciences.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，应用程序由人类最终用户使用，网络安全在这里的角色是确保这些用户的安全。这一层次的问题包括检测仇恨言论、内容审查、标记欺诈交易、描述侮辱性行为，以及保护用户免受数字犯罪（身份盗窃、诈骗和勒索）的侵害。请注意，这一方面超出了网络安全的技术要素，进入了人文、法律和社会科学的领域。
- en: We will now present a case study to explain more clearly how security problems
    arise at each level (data, information, application, and user).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过一个案例研究来更清楚地解释在每一层（数据、信息、应用和用户）如何产生安全问题。
- en: A study on Twitter
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Twitter研究
- en: '*Figure 1**.1* shows the threat model in the four broad levels we described.
    Let us understand this concretely using the example of Twitter. Twitter is a social
    media networking platform where users can post short opinions, photos, and videos
    and interact with the posts of others.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1**.1* 展示了我们描述的四个广泛层次上的威胁模型。让我们通过Twitter的例子具体理解这一点。Twitter是一个社交媒体平台，用户可以发布简短的意见、照片和视频，并与他人的帖子互动。'
- en: At the data level, all of the data (posts, login credentials, and so on) are
    stored in the raw form. An adversary may try to physically break into data centers
    or try to gain access to this data using malicious injection queries. At the information
    level, Twitter itself is using the data to run analytics and train its predictive
    models. An adversary may try to harvest employee credentials via a phishing email
    or poison models with corrupt data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据层面，所有数据（帖子、登录凭证等）都以原始形式存储。对手可能会试图物理闯入数据中心或尝试使用恶意注入查询获取对数据的访问。在信息层面，Twitter本身正在使用这些数据来进行分析和训练其预测模型。对手可能会通过钓鱼邮件窃取员工凭证或用恶意数据毒害模型。
- en: 'At the application level, the analytics are being transformed into actionable
    insights consumable by end users: recommendation lists, news feeds, and top-ranking
    posts. An adversary may create bot accounts that spread misinformation or malicious
    extensions that redirect users outside of Twitter. Finally, at the user level,
    end users actually use the app to tweet. Here, an adversary may try to attack
    users with hate speech or abusive content.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用层面，分析正在被转换成可由最终用户消费的行动洞察：推荐列表、新闻源和排名靠前的帖子。对手可能会创建机器人账户来传播虚假信息或恶意扩展，将用户重定向到Twitter之外。最后，在用户层面，最终用户实际上使用该应用程序来发推文。在这里，对手可能会试图通过仇恨言论或侮辱性内容攻击用户。
- en: 'So far, we have discussed cybersecurity and looked at various cybersecurity
    problems that occur. We will now turn to a related but slightly different topic:
    privacy.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了网络安全，并探讨了各种网络安全问题。现在，我们将转向一个相关但略有不同的话题：隐私。
- en: Privacy
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐私
- en: 'The terms security and privacy are often confused and used interchangeably.
    However, the goals of security and privacy are very different. While security
    aims to secure data and systems, privacy refers to individuals having full control
    over their data. When it comes to privacy, every individual should know the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 安全和隐私这两个术语经常被混淆并互换使用。然而，安全和隐私的目标非常不同。虽然安全旨在保护数据和系统，但隐私指的是个人对其数据的完全控制。当谈到隐私时，每个人都应该了解以下内容：
- en: What data is being collected (location, app usage, web tracking, and health
    metrics)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正在收集哪些数据（位置、应用使用、网络跟踪和健康指标）
- en: How long it will be retained for (deleted immediately or in a week/month/year)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将被保留多长时间（立即删除或在一周/月/年内删除）
- en: Who can access it (advertisers, research organizations, and governments)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁可以访问它（广告商、研究机构和政府）
- en: How it can be deleted (how to make a request to the app)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何删除它（如何向应用提出请求）
- en: Security and privacy are interrelated. If an attacker hacks into a hospital
    or medical database (a breach of security), then they may have access to sensitive
    patient data (a breach of privacy). There are numerous laws around the world,
    such as the **General Data Protection Regulation** (**GDPR**) in Europe, that
    mandate strict security controls in order to ensure user privacy.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性和隐私性是相互关联的。如果一个攻击者黑入医院或医疗数据库（安全漏洞），那么他们可能有权访问敏感的患者数据（隐私漏洞）。世界各地有许多法律，例如欧洲的**通用数据保护条例**（**GDPR**），要求实施严格的安全控制，以确保用户隐私。
- en: Because ML relies on a lot of collected data, there has been a push for privacy-preserving
    ML. We will be discussing some techniques for this in later chapters.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ML依赖于大量的收集数据，因此推动了隐私保护ML的发展。我们将在后面的章节中讨论一些相关技术。
- en: This completes our discussion of cybersecurity. We started by describing the
    traditional concepts of security, followed by various cybersecurity problems at
    multiple levels. We also presented a case study on Twitter that helps put these
    problems in context. Finally, we looked at privacy, a closely related topic.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们对网络安全的讨论。我们首先描述了传统的安全概念，然后介绍了多个层面的各种网络安全问题。我们还提供了一个Twitter案例研究，帮助将这些问题置于背景中。最后，我们探讨了与网络安全紧密相关的隐私问题。
- en: 'Next, we will turn to the second element in the book: ML.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将转向本书的第二个要素：机器学习。
- en: An overview of machine learning
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习概述
- en: 'In this section, we will present a brief overview of ML principles and techniques.
    The traditional computing paradigm defines an algorithm as having three elements:
    the input, an output, and a process that specifies how to derive the output from
    the input. For example, in a credit card detection system, a module to flag suspicious
    transactions may have transaction metadata (location, amount, type) as input and
    the flag (suspicious or not) as output. The process will define the rule to set
    the flag based on the input, as shown in *Figure 1**.2*:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要介绍机器学习（ML）的原则和技术。传统的计算范式定义算法有三个要素：输入、输出以及一个指定如何从输入推导输出的过程。例如，在一个信用卡检测系统中，一个标记可疑交易的模块可能将交易元数据（位置、金额、类型）作为输入，将标记（可疑或不）作为输出。过程将定义基于输入设置标记的规则，如图*1**.2*所示：
- en: '![Figure 1.2 – Traditional input-process-output model for fraud detection](img/B19327_01_02.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图1.2 – 欺诈检测的传统输入-处理-输出模型](img/B19327_01_02.jpg)'
- en: Figure 1.2 – Traditional input-process-output model for fraud detection
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 – 欺诈检测的传统输入-处理-输出模型
- en: ML is a drastic change to the input-process-output philosophy. The traditional
    approach defined computing as deriving the output by applying the process to the
    input. In ML, we are given the input and output, and the task is to derive the
    process that connects the two.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）是对输入-处理-输出哲学的巨大变革。传统的做法将计算定义为通过将过程应用于输入来推导输出。在ML中，我们被给出输入和输出，任务是推导连接两者的过程。
- en: Continuing our analogy of the credit card fraud detection system, we will now
    be provided with a dataset that has the input features and output flags. Our task
    is to learn how the flag can be computed based on the input. Once we learn the
    process, we can generalize it to new data that comes our way in the traditional
    input-process-output way. Most of the security-related problems we will deal with
    in this book are *classification* problems. Classification is a task in which
    data points are assigned discrete, categorical labels (fraud/non-fraud or low-risk/medium-risk/high-risk).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们关于信用卡欺诈检测系统的类比，现在我们将提供一个包含输入特征和输出标记的数据集。我们的任务是学习如何根据输入计算标记。一旦我们学会了过程，我们就可以将其推广到以传统输入-处理-输出方式到来的新数据。本书中我们将处理的绝大多数与安全相关的问题都是*分类*问题。分类是一个将数据点分配为离散的、分类的标签（欺诈/非欺诈或低风险/中风险/高风险）的任务。
- en: ML is not a one-step process. There are multiple steps involved, such as cleaning
    and preparing the data in the right form, training and tuning models, deploying
    them, and monitoring them. In the next section, we will look at the major steps
    involved in the workflow.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习不是一个一步到位的过程。它涉及多个步骤，例如以正确的形式清理和准备数据，训练和调整模型，部署它们，并监控它们。在下一节中，我们将查看工作流程中涉及的主要步骤。
- en: Machine learning workflow
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习工作流程
- en: Now, we will see some of the basic steps that go into end-to-end ML. It begins
    with preprocessing the data to make it fit for use by ML models and ends with
    monitoring and tuning the models.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看到一些端到端机器学习的基本步骤。它从预处理数据开始，使其适合机器学习模型使用，并以监控和调整模型结束。
- en: Data preprocessing
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据预处理
- en: The first step in any ML experiment is to process the data into a format suitable
    for ML. Real-world data is often not suitable to be used directly by a model due
    to two main reasons.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习实验的第一步是将数据处理成适合机器学习的格式。由于两个主要原因，现实世界的数据通常不适合直接由模型使用。
- en: The first reason is variability in format. Data around us is in multiple formats,
    such as numbers, images, text, audio, and video. All of these need to be converted
    into numerical representations for a model to consume them. Preprocessing would
    convert images into matrices, text into word embeddings, and audio streams into
    time series. Some features of data are discrete; they represent categorical variables.
    For example, in a dataset about users, the `Country` field would take on string
    values such as `India`, `Canada`, `China`, and so on. Preprocessing converts such
    categorical variables into a numerical vector form that can be consumed by a model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个原因是数据格式的多样性。我们周围的数据以多种格式存在，如数字、图像、文本、音频和视频。所有这些都需要转换为数值表示，以便模型能够消费它们。预处理会将图像转换为矩阵，文本转换为词嵌入，音频流转换为时间序列。数据的一些特征是离散的；它们代表分类变量。例如，在一个关于用户的数据集中，`Country`字段会采用如`India`、`Canada`、`China`等字符串值。预处理会将这样的分类变量转换为模型可以消费的数值向量形式。
- en: The second reason is noise in real-world data. Measurement inaccuracies, processing
    errors, and human errors can cause corrupt values to be recorded. For example,
    a data entry operator might enter your age as 233 instead of 23 by mistakenly
    pressing the *3* key twice. A web scraper collecting data may face network issues
    and fail to make a request; some fields in the data will then be missing.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是现实世界数据中的噪声。测量不准确、处理错误和人为错误可能导致记录了错误值。例如，一个数据录入员可能错误地按了两次*3*键，将你的年龄输入为233而不是23。一个收集数据的网络爬虫可能遇到网络问题，无法发出请求；数据中的一些字段将缺失。
- en: In a nutshell, preprocessing removes noise, handles missing data, and transforms
    data into a format suitable for consumption by an ML model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，预处理去除噪声，处理缺失数据，并将数据转换成适合机器学习模型消费的格式。
- en: The training phase
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练阶段
- en: After the data has been suitably preprocessed, we train a model to learn from
    the data. A model expresses the relationship between the preprocessed features
    and some target variables. For example, a model to detect phishing emails will
    produce as output a probability that an email is malicious based on the input
    features we define. A model that classifies malware as 1 of 10 malware families
    will output a 10-dimensional vector of probability distribution over the 10 classes.
    In the training phase, the model will learn the parameters of this relationship
    so that the error over the training data is minimized (that is, it is able to
    predict the labels for the training data as closely as possible). During the training
    phase, a subset of the data is reserved as validation data to examine the error
    of the trained model over unseen data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据经过适当的预处理之后，我们训练一个模型从数据中学习。模型表达了预处理特征与某些目标变量之间的关系。例如，用于检测钓鱼邮件的模型将根据我们定义的输入特征输出一个电子邮件是否恶意的概率。将恶意软件分类为10个恶意软件家族之一的模型将输出一个10维的概率分布向量，表示10个类别。在训练阶段，模型将学习这种关系参数，以使训练数据上的误差最小化（即，它能够尽可能准确地预测训练数据的标签）。在训练阶段，数据的一个子集被保留作为验证数据，以检查训练模型在未见数据上的误差。
- en: One of the problems that a model can face is overfitting; if a model learns
    parameters too specific to the training data, it cannot generalize well to newer
    data. This can be diagnosed by comparing the performance of the model on training
    and validation data; a model that overfits will show decreasing loss or error
    over the training data but a non-decreasing loss on validation data. The opposite
    problem to this also exists – the model can suffer from underfitting where it
    is simply not able to learn from the training data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可能面临的一个问题是过拟合；如果模型学习的参数过于特定于训练数据，它就不能很好地推广到新数据。这可以通过比较模型在训练数据和验证数据上的性能来诊断；过拟合的模型将在训练数据上显示损失或错误的减少，但在验证数据上损失不会减少。这个问题也存在相反的情况——模型可能遭受欠拟合，这意味着它根本无法从训练数据中学习。
- en: The inferencing phase
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理阶段
- en: Once a model has been trained, we want to use it to make predictions for new,
    unseen data. There is no parameter learning in this phase; we simply plug in the
    features from the data and inspect the prediction made by the model. Inferencing
    often happens in real time. For example, every time you use your credit card,
    the parameters of your transaction (amount, location, and category) are used to
    run inferencing on a fraud detection model. If the model flags the transaction
    as suspicious, the transaction is declined.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被训练，我们希望用它来对新数据、未见过的数据进行预测。在这个阶段没有参数学习；我们只是将数据中的特征插入，并检查模型做出的预测。推理通常在实时发生。例如，每次你使用信用卡时，你的交易参数（金额、位置和类别）都会用于在欺诈检测模型上运行推理。如果模型将交易标记为可疑，则该交易将被拒绝。
- en: The maintenance phase
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 维护阶段
- en: ML models need continuous monitoring and tuning so that their performance does
    not degrade. A model may become more error-prone in its predictions as time passes
    because it has been trained on older data. For example, a model to detect misinformation
    trained in 2019 would have never been exposed to fake news posts about the COVID-19
    pandemic. As a result, it never learned the characteristics of COVID-related fake
    news and may fail to recognize such articles as misinformation. To avoid this,
    a model has to be retrained on newer data at a regular cadence so that it learns
    from it.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型需要持续的监控和调整，以确保其性能不会下降。随着时间的推移，模型在预测上可能变得更加容易出错，因为它是在旧数据上训练的。例如，2019年训练的用于检测虚假信息的模型从未接触过关于COVID-19大流行的虚假新闻帖子。因此，它从未学习到与COVID相关的虚假新闻的特征，并且可能无法识别此类文章为虚假信息。为了避免这种情况，模型必须定期在新的数据上重新训练，以便从中学习。
- en: Additionally, new features may become available that could help improve the
    performance of the model. We would then need to train the model with the new feature
    set and check the performance gains. There may be different slices of data in
    which different classification thresholds show higher accuracy. The model then
    has to be tuned to use a different threshold in each slice. Monitoring models
    is an ongoing process, and there are automated tools (called MLOps tools) that
    offer functionality for continuous monitoring, training, tuning, and alerting
    of models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，可能还有新的特征可用，这有助于提高模型的性能。然后我们需要用新的特征集训练模型，并检查性能提升。在不同的数据切片中，不同的分类阈值可能显示出更高的准确率。然后模型必须调整以在每个切片中使用不同的阈值。监控模型是一个持续的过程，并且有自动化的工具（称为MLOps工具）提供持续监控、训练、调整和模型警报的功能。
- en: 'Now, we will look at the fundamental ML paradigms: supervised and unsupervised
    learning.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨基本的机器学习范式：监督学习和无监督学习。
- en: Supervised learning
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'ML has two major flavors: supervised and unsupervised. In **supervised learning**,
    we have access to labeled data. From the labeled data, we can learn the relation
    between the data and the labels. The most fundamental example of a supervised
    learning algorithm is **linear regression**.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习有两种主要类型：监督学习和无监督学习。在**监督学习**中，我们有标签数据的访问权限。从标签数据中，我们可以学习数据与标签之间的关系。监督学习算法的最基本例子是**线性回归**。
- en: Linear regression
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性回归
- en: 'Linear regression assumes that the target variable can be expressed as a linear
    function of the features. We initially start with a linear equation with arbitrary
    coefficients, and we tune these coefficients as we learn from the data. At a high
    level and in the simplest form, linear regression works as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归假设目标变量可以表示为特征的一个线性函数。我们最初从一个具有任意系数的线性方程开始，并在从数据中学习时调整这些系数。从高层次和最简单的形式来看，线性回归的工作原理如下：
- en: Let *y* be the target variable and *x*1, *x*2, and *x*3 be the predictor features.
    Assuming a linear relationship, our model is *y = a*0 + a1x1 + a2x2 + a3x3\. Here,
    *a*0, *a*1, *a*2, and *a*3 are parameters initially set to random.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设 *y* 为目标变量，*x*1、*x*2 和 *x*3 为预测特征。假设存在线性关系，我们的模型是 *y = a0 + a1x1 + a2x2 + a3x3*。在这里，*a*0、*a*1、*a*2
    和 *a*3 是最初设置为随机的参数。
- en: Consider the first data point from the training set. It will have its own set
    of predictors (*x*1, *x*2, and *x*3 ) and the target as ground truth (*y*). Calculate
    a predicted value of the target using the equation defined previously; call this
    predicted value *y’*.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑训练集中的第一个数据点。它将有自己的预测器集（*x*1、*x*2 和 *x*3）和作为真实值的目标（*y*）。使用之前定义的方程计算目标变量的预测值；称这个预测值为
    *y′*。
- en: 'Calculate a *loss* that indicates the error of the prediction. Typically, we
    use the **ordinary least squares** (**OLS**) error as the loss function. It is
    simply the square of the difference between the actual and predicted value of
    the target: L = (y − y ′ ) 2.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算一个表示预测误差的*损失*。通常，我们使用**普通最小二乘法**（**OLS**）误差作为损失函数。它简单地是实际值和预测值之间差异的平方：L =
    (y − y′)²。
- en: The loss, *L*, tells us how far off our prediction is from the actual value.
    We use the loss to modify the parameters of our model. This part is the one where
    we *learn* from the data.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失 *L* 告诉我们我们的预测与实际值有多远。我们使用损失来修改我们模型中的参数。这部分是我们从数据中*学习*的部分。
- en: Repeat *steps 2*, *3*, and *4* over each data point from the training set, and
    update the parameters as you go. This completes one *epoch* of training.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练集中的每个数据点重复*步骤 2*、*步骤 3* 和 *步骤 4*，并在过程中更新参数。这完成了一次*时期*的训练。
- en: Repeat the preceding steps for multiple epochs.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对多个时期重复前面的步骤。
- en: In the end, your parameters will have been tuned to capture the linear relationship
    between the features and the target.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你的参数将被调整以捕捉特征和目标之间的线性关系。
- en: We will now look at gradient descent, the algorithm that is the heart and soul
    of linear regression (and many other algorithms).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探讨梯度下降，这是线性回归（以及许多其他算法）的核心和灵魂。
- en: Gradient descent
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'The crucial step in the preceding instructions is *step 4*; this is the step
    where we update the parameters based on the loss. This is typically done using
    an algorithm called **gradient descent**. Let *θ* be the parameters of the model;
    we want to choose the optimal values for *θ* such that the loss is as small as
    possible. We calculate the gradient, which is the derivative of the loss with
    respect to the parameters. We update the parameter *θ* to its new value *θ’* based
    on the gradient as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 上述指令中的关键步骤是*步骤 4*；这是基于损失更新参数的步骤。这通常是通过称为**梯度下降**的算法来完成的。设 *θ* 为模型的参数；我们希望选择
    *θ* 的最佳值，使得损失尽可能小。我们计算梯度，即损失相对于参数的导数。我们根据梯度将参数 *θ* 更新为其新值 *θ′*，如下所示：
- en: θ′= θ − η  δL _ δθ
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: θ′= θ − η δL/δθ
- en: 'The δL _ δθ  gradient represents the slope of the tangent to the loss curve
    at that particular value of *θ*. The sign of the gradient will tell us the direction
    in which we need to change θ in order to reach minima on the loss. We always move
    in the direction of descending gradient to minimize the loss. For a clearer understanding,
    carefully observe the loss curve plotted against the parameter in the following
    chart:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: δL/δθ 梯度表示在特定值 *θ* 处损失曲线的切线斜率。梯度的符号将告诉我们需要改变 θ 的方向，以便在损失上达到最小值。我们总是朝着梯度下降的方向移动以最小化损失。为了更清楚地理解，请仔细观察以下图表中针对参数绘制的损失曲线：
- en: '![Figure 1.3 – Traversing the loss curve using gradient descent](img/B19327_01_03.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.3 – 使用梯度下降遍历损失曲线](img/B19327_01_03.jpg)'
- en: Figure 1.3 – Traversing the loss curve using gradient descent
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – 使用梯度下降遍历损失曲线
- en: Suppose because of our random selection, we select the *θ*1 parameter. When
    we calculate the gradient of the curve (the slope of the tangent to the curve
    at point A), it will be negative. Applying the previous equation of gradient descent,
    we will have to calculate the gradient and add it to *θ*1 to get the new value
    (say *θ*2).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 假设由于我们的随机选择，我们选择了 *θ*1 参数。当我们计算曲线的梯度（曲线在点 A 处的切线斜率）时，它将是负的。应用前面的梯度下降方程，我们将必须计算梯度并将其添加到
    *θ*1 以获得新值（例如 *θ*2）。
- en: We continue this process until we reach point E, where the gradient is very
    small (nearly 0); this is the minimum loss we can reach, and even if we were to
    continue the gradient descent process, the updates would be negligible (because
    of the very small and near-zero value of the gradient). Had we started at point
    H instead of A, the gradient we calculated would have been positive. In that case,
    according to the gradient descent equation, we would have had to decrease *θ*
    to reach the minimum loss. Gradient descent ensures that we always move down the
    curve to reach the minima.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续这个过程，直到达到点E，此时梯度非常小（几乎为0）；这是我们能够达到的最小损失，即使我们继续梯度下降过程，更新也会微不足道（因为梯度的值非常小且接近于零）。如果我们从点H而不是点A开始，我们计算出的梯度将是正的。在这种情况下，根据梯度下降方程，我们必须减小*θ*以达到最小损失。梯度下降确保我们始终沿着曲线向下移动以到达最小值。
- en: An important element in the equation that we have not discussed so far is *η*.
    This parameter is called the **learning rate**. The gradient gives us the direction
    in which we want to change *θ*. The learning rate tells us by how much we want
    to change *θ*. A smaller value of *η* means that we are making very small changes
    and thus taking small steps to reach the minima; it may take a long time to find
    the optimal values. On the other hand, if we choose a very large value for the
    learning rate, we may miss the minima.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前没有讨论的方程中的一个重要元素是*η*。这个参数被称为**学习率**。梯度告诉我们我们想要改变*θ*的方向。学习率告诉我们我们想要改变*θ*的程度。*η*的值越小，我们做出的改变就越小，因此我们采取的小步骤就越少，以到达最小值；可能需要很长时间才能找到最优值。另一方面，如果我们选择一个非常大的学习率，我们可能会错过最小值。
- en: For example, because of a large learning rate, we might jump directly from point
    D to F without ever getting to E. At point F, the direction of the gradient changes,
    and we may jump in the opposite direction back to D. We will keep oscillating
    between these two points without reaching the minima.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，由于学习率很大，我们可能会直接从点D跳到点F，而从未到达点E。在点F，梯度的方向发生变化，我们可能会跳回相反的方向回到点D。我们将在这两个点之间不断振荡，而无法达到最小值。
- en: In the linear regression algorithm discussed previously, we performed the gradient
    descent process for every data point in the training data. This is known as **stochastic
    gradient descent**. In another, more efficient version of the algorithm, we consider
    batches of data, aggregate the loss values, and apply gradient descent over the
    aggregated loss. This is known as **batch** **gradient descent**.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前讨论的线性回归算法中，我们对训练数据中的每个数据点都执行了梯度下降过程。这被称为**随机梯度下降**。在算法的另一个更有效的版本中，我们考虑数据批次，汇总损失值，并在汇总的损失上应用梯度下降。这被称为**批量****梯度下降**。
- en: Gradient descent is at the core of most modern ML algorithms. While we have
    described it in the context of linear regression, it is simply an optimization
    algorithm and is widely used in other models such as deep neural networks as well.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是大多数现代机器学习算法的核心。虽然我们是在线性回归的背景下描述它的，但它仅仅是一个优化算法，并且在其他模型如深度神经网络中也被广泛使用。
- en: Unsupervised learning
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In supervised learning, we had ground truth data to learn the relationship between
    the features and labels. The goal of unsupervised learning is to discover patterns,
    trends, and relationships within data. Unsupervised learning allows us to make
    predictions and detect anomalies without having access to labels during training.
    Let us look at clustering, a popular unsupervised learning problem.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，我们有真实数据来学习特征和标签之间的关系。无监督学习的目标是发现数据中的模式、趋势和关系。无监督学习允许我们在训练期间没有访问标签的情况下进行预测和检测异常。让我们看看聚类，这是一个流行的无监督学习问题。
- en: Clustering
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类
- en: As the name suggests, clustering is the process of grouping data into clusters.
    It allows us to examine how the data points group together, what the common characteristics
    in those groups are, and what the hidden trends in the data are. There is no *learning*
    involved in clustering. An ideal clustering is when the intra-cluster similarity
    is high and inter-cluster similarity is low. This means that points within a cluster
    are very similar to one another and very different from points in other clusters.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，聚类是将数据分组到簇中的过程。它允许我们检查数据点是如何组合在一起的，这些组中的共同特征是什么，以及数据中的隐藏趋势是什么。聚类过程中不涉及*学习*。理想的聚类是簇内相似度高，簇间相似度低。这意味着簇内的点彼此非常相似，与其他簇中的点非常不同。
- en: 'The most fundamental clustering algorithm is called **K-means clustering**.
    It partitions the data points into *k* clusters, where *k* is a parameter that
    has to be preset. The general process of the K-means algorithm is as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的聚类算法称为**K-means聚类**。它将数据点划分为*k*个簇，其中*k*是一个必须预先设置的参数。K-means算法的一般过程如下：
- en: Select *K* points randomly from the training data. These points will be the
    centroids of our clusters.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练数据中随机选择*K*个点。这些点将成为我们簇的质心。
- en: For each point, calculate the distance to each of the centroids. The distance
    metric generally used is **Euclidean distance**.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个点，计算到每个质心的距离。通常使用的距离度量是**欧几里得距离**。
- en: Assign each point to one of the *K* clusters based on the distance. A point
    will be assigned to the centroid that is closest to it (minimum distance).
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据距离将每个点分配到*K*个簇中的一个。一个点将被分配到离它最近的质心（最小距离）。
- en: Each of the *K* centroids will now have a set of points associated with it;
    this forms a cluster. For each cluster, calculate the updated cluster centroids
    as a mean of all the points assigned to that cluster.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，每个*K*个质心都将与一组点相关联；这形成了一个簇。对于每个簇，计算分配给该簇的所有点的平均值，作为更新后的簇质心。
- en: 'Repeat *steps 2–4* until one of the following occurs:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤2-4*，直到以下情况之一发生：
- en: The cluster assignment in *step 3* does not change
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第*3*步中的簇分配不会改变
- en: A fixed number of repetitions of the steps have passed
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤的重复次数达到固定数量
- en: The core concept behind this algorithm is to optimize the cluster assignment
    so that the distance of each point to the centroid of the cluster it belongs to
    is small; that is, clusters should be as tightly knit as possible.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法背后的核心概念是优化簇分配，使得每个点到其所属簇质心的距离尽可能小；也就是说，簇应该尽可能紧密。
- en: We have looked at supervised and unsupervised learning. A third variant, semi-supervised
    learning, is the middle ground between the two.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经研究了监督学习和无监督学习。第三种变体，半监督学习，位于两者之间。
- en: Semi-supervised learning
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 半监督学习
- en: As the name suggests, semi-supervised learning is the middle ground between
    supervised and unsupervised learning. Often (and especially in the case of critical
    security applications), labels are not available or are very few in number. Manual
    labeling is expensive as it requires both time and expert knowledge. Therefore,
    we have to train a model from only these limited labels.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，半监督学习介于监督学习和无监督学习之间。通常（尤其是在关键安全应用的情况下），标签不可用或数量非常少。手动标记既耗时又需要专业知识，因此我们只能从这些有限的标签中训练一个模型。
- en: Semi-supervised learning techniques are generally based on a self-training approach.
    First, we train a supervised model based on the small subset of labeled data.
    We then run inferencing on this model to obtain predicted labels on the unlabeled
    data. We use our original labels, and high-confidence predicted labels together
    to train the final model. This process can be repeated for a fixed number of iterations
    or until we reach a point where the model performance does not change significantly.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习技术通常基于自训练方法。首先，我们基于小部分标记数据训练一个监督模型。然后，我们对这个模型进行推理，以获得未标记数据的预测标签。我们使用原始标签和高置信度的预测标签一起训练最终的模型。这个过程可以重复固定次数的迭代，或者直到我们达到一个点，即模型性能没有显著变化。
- en: Another approach to semi-supervised learning is called **co-training**, where
    we jointly train two models with different views (feature sets) of the data. We
    begin by independently training two models based on different feature sets and
    available labels. We then apply the models to make predictions for the unlabeled
    data and obtain pseudo labels. We add the high-confidence labels from the first
    model to the training set of the second and vice versa. We repeat the process
    of training models and obtain pseudo labels for a fixed number of iterations or
    until we reach a point where the performance of both models does not change significantly.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习的另一种方法是称为**协同训练**，其中我们使用数据的不同视图（特征集）共同训练两个模型。我们首先基于不同的特征集和可用的标签独立地训练两个模型。然后，我们将这些模型应用于对未标记数据进行预测，并获取伪标签。我们将第一个模型中的高置信度标签添加到第二个模型的训练集中，反之亦然。我们重复训练模型的过程，并在固定数量的迭代或直到我们达到一个点，即两个模型的表现没有显著变化。
- en: Now that we have covered the major paradigms in ML, we can turn to evaluate
    the performance of models.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了机器学习中的主要范式，我们可以转向评估模型的性能。
- en: Evaluation metrics
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估指标
- en: So far, we have mentioned model performance in passing. In this section, we
    will define formal metrics for evaluating the performance of a model. As most
    of the problems we deal with will be classification-based, we will discuss metrics
    specific to classification only.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只是提到了模型性能。在本节中，我们将定义用于评估模型性能的正式指标。由于我们处理的大部分问题将是基于分类的，我们将讨论仅针对分类的指标。
- en: A confusion matrix
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: Classification involves predicting the label assigned to a particular data point.
    Consider the case of a fraud detection model. As can be seen in *Figure 1**.4*,
    if the actual and predicted label of a data point are both **Fraud**, then we
    call that example a **True Positive (TP)**. If both are **Non-Fraud**, we call
    it a **True Negative (TN)**. If the predicted label is **Fraud**, but the actual
    label (expected from ground truth) is **Non-Fraud**, then we call it a **False
    Positive (FP)**. Finally, if the predicted label is **Non-Fraud**, but the actual
    label (expected from ground truth) is **Fraud**, we call it a **False** **Negative
    (FN)**.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 分类涉及预测特定数据点的标签。以欺诈检测模型为例。如图*图1.4*所示，如果一个数据点的实际标签和预测标签都是**欺诈**，那么我们称该例子为**真正例（TP）**。如果两者都是**非欺诈**，我们称它为**真正例（TN）**。如果预测标签是**欺诈**，但实际标签（来自真实情况的预期）是**非欺诈**，那么我们称它为**假正例（FP）**。最后，如果预测标签是**非欺诈**，但实际标签（来自真实情况的预期）是**欺诈**，我们称它为**假负例（FN）**。
- en: 'Based on the predicted and the actual label of data, we can construct what
    is called a **confusion matrix**:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 基于预测标签和实际标签的数据，我们可以构建所谓的**混淆矩阵**：
- en: '![Figure 1.4 – Confusion matrix](img/B19327_01_04.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图1.4 – 混淆矩阵](img/B19327_01_04.jpg)'
- en: Figure 1.4 – Confusion matrix
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 – 混淆矩阵
- en: 'The confusion matrix provides a quick way to visually inspect the performance
    of the model. A good model will show the highest true positives and negatives,
    while the other two will have smaller values. The confusion matrix allows us to
    conveniently calculate metrics relevant to classification, namely the accuracy,
    precision, recall, and F-1 score. Let us learn more about these metrics as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵提供了一种快速查看模型性能的方法。一个好的模型将显示出最高的真正例和真正例，而其他两个将具有较小的值。混淆矩阵使我们能够方便地计算与分类相关的指标，即准确率、精确率、召回率和F-1分数。让我们按以下方式了解更多关于这些指标的信息：
- en: '**Accuracy**: This is a measure of how accurate the model predictions are across
    both classes. Simply put, it is the number of examples for which the model is
    able to predict the labels correctly, that is, the proportion of true positives
    and negatives in the data. It can be calculated as follows:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**：这是衡量模型在两个类别中预测准确性的指标。简单来说，它是模型能够正确预测标签的例子数量，即数据中真正例和真正例的比例。它可以按以下方式计算：'
- en: Accuracy =  TP + TN ______________  TP + TN + FP + FN
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率 = TP + TN __________________ TP + TN + FP + FN
- en: '**Precision**: This is a measure of the confidence of the model in the positive
    predictions. In simple terms, it is the proportion of the true positives in all
    the examples predicted as positive. Precision answers the question: Of all the
    examples predicted as fraud, how many of them are actually fraud? Precision can
    be calculated as the following formula:'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确率**：这是衡量模型在正预测中的置信度的指标。简单来说，它是所有预测为正的例子中真正正例的比例。精确率回答的问题是：在所有预测为欺诈的例子中，有多少实际上是欺诈的？精确率可以按以下公式计算：'
- en: Precision =  TP _ TP + FP
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**精确率** = TP _ TP + FP'
- en: '**Recall**: This is a measure of the completeness of the model in the positive
    class. Recall answers the question: Of all the examples that were fraud, how many
    could the model correctly detect as fraud? Recall can be calculated as follows:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**：这是衡量模型在正类中完整性的一个指标。召回率回答的问题是：在所有被认为是欺诈的例子中，模型正确检测出欺诈的有多少？召回率可以按以下方式计算：'
- en: Recall =  TP _ TP + FN
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率 = TP _ TP + FN
- en: When a model has high precision, it means that most of the predicted fraud is
    actually fraud. When a model has a high recall, it means that out of all the fraud
    in the data, most of it is predicted by the model as fraud. Consider a model that
    predicts *everything* as fraud. This model has a high recall; as everything is
    marked as fraud, naturally, all the actual fraud is also being captured, but at
    the cost of a large number of false positives (low precision). Alternatively,
    consider a model that predicts *nothing* as fraud. This model has a high precision;
    as it marks nothing as fraud, there are no false positives, but at the cost of
    a large number of false negatives (low recall). This trade-off between precision
    and recall is a classic problem in ML.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个模型具有高精度时，这意味着大多数预测为欺诈的实际上是欺诈。当一个模型具有高召回率时，这意味着在数据中的所有欺诈中，大部分都被模型预测为欺诈。考虑一个预测**所有**内容为欺诈的模型。这个模型具有高召回率；由于所有内容都被标记为欺诈，自然地，所有的实际欺诈也被捕捉到了，但代价是大量误报（低精度）。另一方面，考虑一个预测**没有**内容为欺诈的模型。这个模型具有高精度；由于它没有标记任何内容为欺诈，因此没有误报，但代价是大量漏报（低召回率）。这种精度和召回率之间的权衡是机器学习中的一个经典问题。
- en: '**F-1 Score**: This measure captures the degrees of both precision and recall.
    High precision comes at the cost of low recall and vice versa. The F-1 score is
    used to identify the model that has the highest precision and recall together.
    Mathematically, it is defined as the harmonic mean of the precision and recall,
    calculated as follows:'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F-1分数**：这个度量捕捉了精度和召回率的程度。高精度是以低召回率为代价的，反之亦然。F-1分数用于识别具有最高精度和召回率的模型。从数学上讲，它被定义为精度和召回率的调和平均值，计算如下：'
- en: F1 Score =  2 ·Precision ·Recall   ______________  Precision + Recall
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: F1分数 = 2 × 精度 × 召回率 / (精度 + 召回率)
- en: '**Receiver Operating Characteristic (ROC) curve**: We have seen that classification
    models assign a label to a data point. In reality, however, the models compute
    a probability that the data point belongs to a particular class. The probability
    is compared with a threshold to determine whether the example belongs to that
    class or not. Typically, a threshold of 0.5 is used. So, in our example of the
    fraud detection model, if the model outputs a value greater than 0.5, we will
    classify the data point as **Fraud**. A smaller value will lead to us classifying
    it as **Non-Fraud**.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**接收者操作特征（ROC）曲线**：我们已经看到分类模型会给数据点分配一个标签。然而，实际上，模型计算的是数据点属于特定类别的概率。这个概率与阈值进行比较，以确定该例子是否属于该类别。通常，使用0.5的阈值。因此，在我们的欺诈检测模型示例中，如果模型输出的值大于0.5，我们将把数据点分类为**欺诈**。较小的值将导致我们将其分类为**非欺诈**。'
- en: The threshold probability is a parameter, and we can tune this parameter to
    achieve high precision or recall. If you set a very low threshold, the bar to
    meet for an example to be fraudulent is very low; for example, at a threshold
    of 0.1, nearly all examples will be classified as fraud. This means that we will
    be catching all the fraud out there; we have a high recall.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值概率是一个参数，我们可以调整这个参数以实现高精度或高召回率。如果你设置一个非常低的阈值，那么一个例子被判定为欺诈的标准就非常低；例如，在0.1的阈值下，几乎所有例子都会被分类为欺诈。这意味着我们将捕捉到所有的欺诈；我们具有高召回率。
- en: 'On the other hand, if you set a very high threshold, the bar to meet will be
    very high. Not all fraud will be caught, but you can be sure that whatever is
    marked as fraud is definitely fraud. In other words, you have high precision.
    This trade-off between precision and recall is captured in the ROC curve, as shown
    in the following diagram:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你设置一个非常高的阈值，达到标准的要求将会非常高。并不是所有的欺诈都会被捕捉到，但你可以确信，被标记为欺诈的任何内容确实都是欺诈。换句话说，你具有高精度。这种精度和召回率之间的权衡在ROC曲线上得到体现，如下面的图表所示：
- en: '![Figure 1.5 – ROC curve](img/B19327_01_05.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图1.5 – ROC曲线](img/B19327_01_05.jpg)'
- en: Figure 1.5 – ROC curve
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 – ROC曲线
- en: The ROC curve plots the **True Positive Rate** (**TPR**) against the **False
    Positive Rate** (**FPR**) for every value of the threshold from 0 to 1\. It allows
    us to observe the precision we have to tolerate to achieve a certain recall and
    vice versa. In the ROC plotted in the preceding diagram, points **A** and **B**
    indicate the true positive and negative rates that we have to tolerate at two
    different thresholds. The **Area under the ROC Curve** (**AUC**), represented
    by the shaded area, provides an overall measure of the performance of the model
    across all threshold values. The AUC can be interpreted as the probability that
    a model scores a random positive example (fraud) higher than a random negative
    one (non-fraud). When comparing multiple models, we choose the one with the highest
    value of the AUC.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线绘制了每个阈值从0到1的**真正率**（**TPR**）与**假正率**（**FPR**）的对比图。它使我们能够观察为了达到一定的召回率而必须容忍的精确度，反之亦然。在前面的ROC图中，点**A**和**B**表示在两个不同阈值下我们必须容忍的真正率和假正率。**ROC曲线下的面积**（**AUC**），由阴影区域表示，提供了模型在所有阈值下的整体性能指标。AUC可以解释为模型将随机正例（欺诈）评分高于随机负例（非欺诈）的概率。在比较多个模型时，我们选择AUC值最高的模型。
- en: We have looked at the fundamental concepts behind ML, the underlying workflow,
    and the evaluation approaches. We will now examine why the application of ML in
    cybersecurity is different from other fields and the novel challenges it poses.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了机器学习背后的基本概念、底层工作流程和评估方法。现在，我们将探讨为什么机器学习在网络安全领域的应用与其他领域不同，以及它带来的新颖挑战。
- en: Machine learning – cybersecurity versus other domains
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习——网络安全与其他领域
- en: 'ML today is applied to a wide variety of domains, some of which are detailed
    in the following list:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的机器学习被应用于广泛的领域，其中一些将在以下列表中详细说明：
- en: In sales and marketing, to identify the segment of customers likely to buy a
    particular product
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在销售和市场营销中，为了确定可能购买特定产品的客户群体
- en: In online advertising, for click prediction and to display ads accordingly
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在在线广告中，用于点击预测和相应地展示广告
- en: In climate and weather forecasting, to predict trends based on centuries of
    data
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在气候和天气预报中，基于数百年的数据预测趋势
- en: In recommendation systems, to find the best items (movies, songs, posts, and
    people) relevant to a user
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推荐系统中，找到与用户最相关的最佳项目（电影、歌曲、帖子和人）
- en: While every sector imaginable applies ML today, the nuances of it being applied
    to cybersecurity are different from other fields. In the following subsections,
    we will see some of the reasons why it is much more challenging to apply ML to
    the cybersecurity domain than to other domains such as sales or advertising.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然今天每个可想象的行业都应用了机器学习，但将其应用于网络安全与其他领域的细微差别是不同的。在以下小节中，我们将看到一些原因，为什么将机器学习应用于网络安全领域比应用于其他领域（如销售或广告）更具挑战性。
- en: High stakes
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高风险
- en: Security problems often involve making crucial decisions that can impact money,
    resources, and even life. A fraud detection model that performs well has the potential
    to save millions of dollars in fraud. A botnet or malware detection model can
    save critical systems (such as in the military) or sensitive data (such as in
    healthcare) from being compromised. A model to flag abusive users on social media
    can potentially save someone’s life. Because the stakes are so high, the precision-recall
    trade-off becomes even more crucial.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 安全问题通常涉及做出可能影响金钱、资源和甚至生命的重大决策。表现良好的欺诈检测模型有可能节省数百万美元的欺诈损失。网络或恶意软件检测模型可以保护关键系统（如军事系统）或敏感数据（如医疗保健数据）免受损害。一个用于标记社交媒体上的滥用用户的模型有可能拯救某人的生命。由于风险如此之高，精确度-召回率权衡变得尤为重要。
- en: In click fraud detection, we have to operate at high precision (or else we would
    end up marking genuine clicks as fraud), and this comes at the cost of poor recall.
    Similarly, in abuse detection, we must operate at high recall (we want all abusers
    to be caught), which comes at the cost of high false positives (low precision).
    In use cases such as click prediction or targeting, the worst thing that might
    happen because of a poor model is a few non-converting clicks or mistargeted advertisements.
    On the other hand, models for security must be thoroughly tuned and diligently
    monitored as the stakes are much higher than in other domains.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在点击欺诈检测中，我们必须保持高精度（否则我们可能会将真实点击标记为欺诈），这以牺牲召回率为代价。同样，在滥用检测中，我们必须保持高召回率（我们希望捕捉到所有滥用者），这以高误报率（低精度）为代价。在使用案例，如点击预测或定位中，由于模型表现不佳，最糟糕的情况可能是几个未转化的点击或定位错误的广告。另一方面，由于安全领域的风险远高于其他领域，因此安全模型必须经过彻底调整和勤勉监控。
- en: Lack of ground truth
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺乏真实情况
- en: Most well-studied ML methods are supervised in nature. This means that they
    depend on ground-truth labels to learn the model. In security problems, the ground
    truth is not always available, unlike in other domains. For example, in a task
    such as customer targeting or weather forecasting, historical information can
    tell us whether a customer actually purchased a product or whether it actually
    rained; this can be used as ground truth.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数研究广泛的机器学习方法本质上是监督性的。这意味着它们依赖于真实标签来学习模型。在安全问题上，真实情况并不总是可用，与其他领域不同。例如，在客户定位或天气预报等任务中，历史信息可以告诉我们客户是否真的购买了产品，或者是否真的下雨了；这可以用作真实情况。
- en: However, such obvious ground truth is not available in security problems such
    as fraud or botnet detection. We depend on expert annotation and manual investigations
    to label data, which is a resource-heavy task. In addition, the labels are based
    on human knowledge and heuristics (i.e., does this seem fraudulent?) and not on
    absolute truth such as the ones we described for customer targeting or weather
    prediction. Due to the lack of high-confidence labels, we often have to rely on
    unsupervised or semi-supervised techniques for security applications.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在诸如欺诈或僵尸网络检测等安全问题上，并不总是存在如此明显的真实情况。我们依赖于专家标注和人工调查来标记数据，这是一项资源密集型任务。此外，标签是基于人类知识和启发式方法（例如，这看起来像是欺诈吗？）而不是基于绝对的真实性，例如我们描述的客户定位或天气预报中的那些。由于缺乏高置信度的标签，我们通常不得不依赖于无监督或半监督技术在安全应用中。
- en: The need for user privacy
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用户隐私的需求
- en: In recent years, there has been a push to maintain user privacy. As we have
    discussed previously, with increased privacy comes reduced utility. This proves
    to be particularly challenging in security tasks. The inability to track users
    across websites hampers our ability to detect click fraud. A lack of permissions
    to collect location information will definitely reduce the signals we have access
    to detect credit card fraud. All of these measures preserve user privacy by avoiding
    undue tracking and profiling; however, they also degrade the performance of security
    models, which are actually for the benefit of the user. The fewer signals available
    to us, the lower the fidelity in the prediction of our models.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，人们一直在努力维护用户隐私。正如我们之前讨论的，随着隐私的增加，实用性会减少。这在安全任务中尤其具有挑战性。无法跨网站跟踪用户阻碍了我们检测点击欺诈的能力。缺乏收集位置信息的权限肯定会减少我们用于检测信用卡欺诈的信号。所有这些措施都通过避免过度跟踪和画像来保护用户隐私；然而，它们也降低了安全模型的表现，而安全模型实际上是为了用户的利益。我们可用的信号越少，我们模型预测的保真度就越低。
- en: The need for interpretability
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释性的需求
- en: Most powerful ML models (neural networks, transformers, and graph-based learning
    models) operate as a black box. The predictions made by the model lack interpretability.
    Because these models learn higher-order features, it is not possible for a human
    to understand and explain why a particular example is classified the way it is.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数强大的机器学习模型（神经网络、转换器和基于图的学习模型）作为黑盒运行。模型做出的预测缺乏可解释性。因为这些模型学习高阶特征，所以人类无法理解和解释为什么某个特定示例被分类成这种方式。
- en: In security applications, however, explainability is important. We need to justify
    each prediction, at least for the positive class. If a transaction is flagged
    as fraudulent, the bank or credit card company needs to understand what went into
    the prediction in order to ascertain the truth. If users are to be blocked or
    banned, the platform may need adequate justification, more convincing than a model
    prediction. Classical ML algorithms such as decision trees and logistic regression
    provide some interpretation based on tree structure and coefficients, respectively.
    This need for interpretability in models is an obstacle to using state-of-the-art
    deep learning methods in security.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在安全应用中，可解释性很重要。我们需要为每个预测提供理由，至少对于正类来说是这样。如果一笔交易被标记为欺诈，银行或信用卡公司需要了解预测中涉及的内容，以便确定真相。如果用户被阻止或禁止，平台可能需要充分的理由，比模型预测更有说服力。像决策树和逻辑回归这样的经典机器学习算法分别基于树结构和系数提供一些解释。模型中这种可解释性的需求是使用最先进的深度学习方法在安全领域的一个障碍。
- en: Data drift
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据漂移
- en: The cybersecurity landscape is an ever-changing one, with new attack strategies
    coming up every day. The nature of attackers and attack vectors is constantly
    evolving. As a result, there is also a change in the features that the model expects
    if data at inference time is significantly different in nature from that which
    the model was trained on. For example, an entirely new variant of malware may
    not be detected by a model as no examples of this variant were in the training
    data. A fake news detection model trained in 2019 may not be able to recognize
    COVID-19-related misinformation as it was never trained on that data. This data
    drift makes it challenging to build models that have sustained performance in
    the wild.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 网络安全领域是一个不断变化的领域，每天都有新的攻击策略出现。攻击者和攻击向量本质上是不断演变的。因此，如果推理时的数据在本质上与模型训练时的数据显著不同，模型期望的特征也会发生变化。例如，一个全新的恶意软件变种可能不会被模型检测到，因为在训练数据中没有这个变种的示例。一个在2019年训练的假新闻检测模型可能无法识别与COVID-19相关的错误信息，因为它从未在那些数据上训练过。这种数据漂移使得构建在野外具有持续性能的模型具有挑战性。
- en: These problems (lack of labels, data drift, and privacy issues) also arise in
    other applications of ML. However, in the case of systems for cybersecurity, the
    stakes are high, and the consequences of an incorrect prediction can be devastating.
    These issues, therefore, are more challenging to handle in cybersecurity.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题（缺乏标签、数据漂移和隐私问题）也出现在机器学习的其他应用中。然而，在网络安全系统的案例中，风险很高，错误预测的后果可能是灾难性的。因此，这些问题在网络安全中处理起来更具挑战性。
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This introductory chapter provided a brief overview of cybersecurity and ML.
    We studied the fundamental goals of traditional cybersecurity and how those goals
    have now evolved to capture other tasks such as fake news, deep fakes, click spam,
    and fraud. User privacy, a topic of growing importance in the world, was also
    introduced. On the ML side, we covered the basics from the ground up: beginning
    with how ML differs from traditional computing and moving on to the methods, approaches,
    and common terms used in ML. Finally, we also highlighted the key differences
    in ML for cybersecurity that make it so much more challenging than other fields.
    The coming chapters will focus on applying these concepts to designing and implementing
    ML models for security issues. In the next chapter, we will discuss how to detect
    anomalies and network attacks using ML.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这本入门章节简要概述了网络安全和机器学习。我们研究了传统网络安全的根本目标，以及这些目标是如何演变以涵盖其他任务，如假新闻、深度伪造、点击垃圾邮件和欺诈。用户隐私，作为一个在世界上日益重要的话题，也被介绍。在机器学习方面，我们从基础开始讲解：从机器学习与传统计算的区别开始，然后介绍机器学习中使用的各种方法、方法和常用术语。最后，我们还强调了网络安全中机器学习的关键差异，这使得它比其他领域更具挑战性。接下来的章节将专注于将这些概念应用于设计和实现用于安全问题的机器学习模型。在下一章中，我们将讨论如何使用机器学习检测异常和网络攻击。
