- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Exploring Generative Adversarial Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索生成对抗网络
- en: In this chapter, we will introduce **Generative Adversarial Networks** (**GANs**)
    and discuss the evolution of this data generation method. You will learn about
    the typical architecture of a GAN. After this, we will explain its training process
    and discuss the main challenges. Then, we will highlight various applications
    of GANs, including generating images and text-to-image translation. Additionally,
    we will study a practical coding example demonstrating how to use GANs to generate
    photorealistic images. Finally, we will also discuss variations of GANs, such
    as conditional GANs, CycleGANs, CTGANs, WGANs, WGAN-GPs, and f-GANs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍 **生成对抗网络**（**GANs**）并讨论这种数据生成方法的演变。你将了解 GAN 的典型架构。之后，我们将解释其训练过程并讨论主要挑战。然后，我们将突出
    GANs 的各种应用，包括图像生成和文本到图像的翻译。此外，我们还将研究一个实际编码示例，展示如何使用 GANs 生成逼真的图像。最后，我们还将讨论 GANs
    的变体，例如条件 GANs、CycleGANs、CTGANs、WGANs、WGAN-GPs 和 f-GANs。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: What is a GAN?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 GAN？
- en: Training a GAN
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练 GAN
- en: Utilizing GANs to generate synthetic data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 GANs 生成合成数据
- en: Hands-on GANs in practice
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实践中的动手 GANs
- en: Variations of GANs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs 的变体
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The code used in this chapter will be available in the corresponding chapter
    folder in the book’s GitHub repository: [https://github.com/PacktPublishing/Synthetic-Data-for-Machine-Learning](https://github.com/PacktPublishing/Synthetic-Data-for-Machine-Learning).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的代码将在本书 GitHub 存储库的相应章节文件夹中提供：[https://github.com/PacktPublishing/Synthetic-Data-for-Machine-Learning](https://github.com/PacktPublishing/Synthetic-Data-for-Machine-Learning)。
- en: What is a GAN?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 GAN？
- en: In this section, we will introduce GANs and briefly discuss the evolution and
    progression of this particular data generation method. Then, we will explain the
    standard architecture of a typical GAN and how they work.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍 GANs 并简要讨论这种特定数据生成方法的演变和进展。然后，我们将解释典型 GAN 的标准架构以及它们是如何工作的。
- en: The concept of GANs was introduced in the 2014 paper *Generative Adversarial
    Networks* ([https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)),
    by Ian J. Goodfellow and his research team. In the same year, **conditional GANs**
    were introduced, allowing us to generate more customizable synthetic data. Then,
    **Deep Convolutional GANs** (**DCGANs**) were suggested in 2015, which facilitated
    the generation of high-resolution images. After that, **CycleGANs** were proposed
    in 2017 for unsupervised image-to-image translation tasks. This opened the door
    for enormous applications such as domain adaptation. **StyleGAN** was introduced
    in 2019, bringing GANs to new fields such as art and fashion.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 的概念是在 2014 年的论文 *Generative Adversarial Networks* 中提出的（[https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)），由
    Ian J. Goodfellow 和他的研究团队完成。同年，**条件 GANs** 被引入，使我们能够生成更多可定制的合成数据。然后，在 2015 年，**深度卷积
    GANs**（**DCGANs**）被提出，这促进了高分辨率图像的生成。之后，在 2017 年，**CycleGANs** 被提出用于无监督的图像到图像的翻译任务。这为领域自适应等巨大应用打开了大门。**StyleGAN**
    在 2019 年被引入，将 GANs 带到了新的领域，如艺术和时尚。
- en: 'GANs have also been showing impressive progress in the field of video synthesis.
    In fact, the recent work by NVIDIA is a testament to their tremendous potential
    (please check this paper for more details: *One-Shot Free-View Neural Talking-Head
    Synthesis for Video Conferencing* at [https://arxiv.org/pdf/2011.15126.pdf](https://arxiv.org/pdf/2011.15126.pdf)).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 在视频合成领域也显示出令人印象深刻的进展。实际上，NVIDIA 最近的工作证明了它们巨大的潜力（请参阅此论文以获取更多详细信息：*One-Shot
    Free-View Neural Talking-Head Synthesis for Video Conferencing*，链接：[https://arxiv.org/pdf/2011.15126.pdf](https://arxiv.org/pdf/2011.15126.pdf))。
- en: 'This work shows that GANs can now recreate a talking-head video using only
    a single source image. For the code, dataset, and online demo, refer to the project’s
    page: [https://nvlabs.github.io/face-vid2vid](https://nvlabs.github.io/face-vid2vid).
    Next, we delve into the architecture of GANs.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作表明，GANs 现在可以使用单个源图像重新创建一个说话头视频。对于代码、数据集和在线演示，请参阅项目的页面：[https://nvlabs.github.io/face-vid2vid](https://nvlabs.github.io/face-vid2vid)。接下来，我们将深入研究
    GANs 的架构。
- en: Most **deep learning** (**DL**) methods and architectures are designed to predict
    something. It could be weather conditions, stock prices, object classes, or something
    else. However, GANs were proposed to *generate* something. It could be images,
    videos, texts, music, or point clouds.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数**深度学习**（**DL**）方法和架构都是设计来预测某物的。这可能包括天气条件、股票价格、物体类别或其他。然而，GANs被提出**生成**某物。这可能包括图像、视频、文本、音乐或点云。
- en: At the heart of this capability lies the essential problem of learning how to
    generate training samples from a given domain or dataset. GANs are DL methods
    that can learn complex data distributions and can be leveraged to generate an
    unlimited number of samples that belong to a specific distribution. These generated
    synthetic samples have many applications for data augmentation, style transfer,
    and data privacy.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种能力的核心是学习如何从给定的域或数据集中生成训练样本的基本问题。GANs是深度学习方法，可以学习复杂的数据分布，并可以用来生成属于特定分布的无限数量的样本。这些生成的合成样本在数据增强、风格迁移和数据隐私等方面有许多应用。
- en: '![Figure 7.1 – A typical architecture and training process of GANs](img/Figure_07_01_B18494.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – GAN的典型架构和训练过程](img/Figure_07_01_B18494.jpg)'
- en: Figure 7.1 – A typical architecture and training process of GANs
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – GAN的典型架构和训练过程
- en: Moving forward, we will learn how to train GANs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何训练GANs。
- en: Training a GAN
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个生成对抗网络（GAN）
- en: In this section, we will learn how to train a typical GAN. Then, we will discuss
    the main challenges and difficulties.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何训练一个典型的GAN。然后，我们将讨论主要挑战和困难。
- en: 'A GAN is trained using **unsupervised learning** techniques where both submodels
    are trained simultaneously using a process called **adversarial training**. A
    typical GAN consists of two neural networks (usually convolutional neural networks):
    the **generator** and the **discriminator**. The generator takes in a random noise
    vector as input and generates a synthetic (fake) sample. The goal of the generator
    is to produce synthetic data that is realistic and indistinguishable from real
    data. The discriminator, on the other hand, is trained to distinguish between
    real and fake samples. It receives a sample and predicts its data source domain:
    real or fake. If the discriminator correctly identifies a real data sample, no
    error is backpropagated. However, if the discriminator fails to identify a synthetic
    sample, it is penalized, and the generator is rewarded. The generator is penalized
    if the discriminator is able to correctly identify generated, synthetic data.
    In this way, both the generator and discriminator are constantly trying to improve
    their performance, resulting in the generation of increasingly realistic synthetic
    data. Refer to *Figure 7**.1* for a visualization of the training process. Let’s
    explore more and learn about the training process in more detail.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）是通过**无监督学习**技术进行训练的，其中两个子模型通过称为**对抗训练**的过程同时训练。一个典型的GAN由两个神经网络（通常是卷积神经网络）组成：**生成器**和**判别器**。生成器接收一个随机噪声向量作为输入并生成一个合成（虚假）样本。生成器的目标是生成逼真且与真实数据不可区分的合成数据。另一方面，判别器被训练来区分真实和虚假样本。它接收一个样本并预测其数据源域：真实或虚假。如果判别器正确地识别了一个真实数据样本，则不会回传错误。然而，如果判别器未能识别合成样本，它将受到惩罚，而生成器将获得奖励。如果判别器能够正确地识别生成的合成数据，生成器将受到惩罚。通过这种方式，生成器和判别器都在不断努力提高其性能，从而生成越来越逼真的合成数据。请参阅*图7.1*以可视化训练过程。让我们进一步探索并更详细地了解训练过程。
- en: Disclaimer on hands-on training of GANs
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 关于GAN实战训练的免责声明
- en: Please note that we do not provide hands-on elements on how to train GANs because
    the chapter is committed to the theoretical, conceptual, and design aspects of
    GANs for synthetic data generation. Thus, hands-on examples are out of the scope
    of this chapter. However, if you are keen to train your GAN, please refer to the
    *Deep Convolutional Generative Adversarial Network* *Tutorial* ([https://www.tensorflow.org/tutorials/generative/dcgan](https://www.tensorflow.org/tutorials/generative/dcgan)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们不会提供如何训练GANs的实战元素，因为本章致力于GANs在合成数据生成方面的理论、概念和设计方面。因此，实战示例不在本章的范围内。但是，如果您热衷于训练您的GAN，请参阅*深度卷积生成对抗网络*教程([https://www.tensorflow.org/tutorials/generative/dcgan](https://www.tensorflow.org/tutorials/generative/dcgan))。
- en: GAN training algorithm
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GAN训练算法
- en: 'The training algorithm is a crucial aspect of enabling GANs to generate useful
    synthetic data. The following is a step-by-step procedure that can be utilized
    to train GANs:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 训练算法是使GAN能够生成有用合成数据的关键方面。以下是一个可以用于训练GAN的逐步程序：
- en: Create z by sampling a random noise following a suitable noise distribution
    such as uniform, Gaussian, Binomial, Poisson, Exponential, Gamma, and Weibull
    distributions.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过采样适合的噪声分布（如均匀分布、高斯分布、二项分布、泊松分布、指数分布、伽马分布和韦伯分布）来创建 z。
- en: Feed z to the generator to produce a synthetic or fake sample, x fake.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 z 输入到生成器中，以产生一个合成或虚假样本，x fake。
- en: Pass both x fake and x real to a `switch` block, which randomly selects one
    of its inputs and passes it to the discriminator.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 x fake 和 x real 传递给一个 `switch` 模块，该模块随机选择其输入之一并将其传递给判别器。
- en: The discriminator classifies the given sample as real or fake.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判别器将给定的样本分类为真实或虚假。
- en: Calculate the error.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算误差。
- en: Backpropagate the error to both the generator and discriminator.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将误差反向传播到生成器和判别器。
- en: Update the weights of the generator and discriminator
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新生成器和判别器的权重
- en: Next, we’ll discuss the loss.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论损失。
- en: Training loss
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练损失
- en: 'The loss shown next is one of many losses that can be used to train a GAN.
    This particular loss is derived from the **cross-entropy loss**:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个显示的损失是用于训练GAN的许多损失之一。这个特定的损失是从**交叉熵损失**派生出来的：
- en: L = E x[log(D(x))] + E z[log(1 − D(G(z)))]
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: L = E x[log(D(x))] + E z[log(1 − D(G(z)))]
- en: 'Let’s break this formula down:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这个公式：
- en: D(x) is the discriminator’s estimate that x is drawn from the real dataset
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D(x) 是判别器对 x 来自真实数据集的估计
- en: E x and E z are the expected values over real and generated synthetic (fake)
    samples, respectively
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: E x 和 E z 分别是真实样本和生成合成（虚假）样本的期望值
- en: G(z) is the output of the generator for a noise vector, z
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: G(z) 是生成器对噪声向量 z 的输出
- en: D(G(z)) is the discriminator’s estimate that a synthetic sample is real
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D(G(z)) 是判别器对合成样本是真实的估计
- en: As expected, the training process of GANs is complex, but it is a powerful technique
    for generating realistic data, which motivated researchers to examine new ways
    to enhance and speed its training and convergence. Next, let us discuss some of
    these challenges.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，GAN的训练过程是复杂的，但它是一种生成逼真数据的有力技术，这促使研究人员探索新的方法来增强和加速其训练和收敛。接下来，让我们讨论一些这些挑战。
- en: GANs in action
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: GANs in action
- en: For an interactive demonstration of how a GAN is trained, please refer to *Play
    with Generative Adversarial Networks (GANs)* in your browser ([https://poloclub.github.io/ganlab](https://poloclub.github.io/ganlab)).
    For more details, check out the corresponding paper *GAN lab:* *Understanding
    Complex Deep Generative Models using Interactive Visual* *Experimentation* ([https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf](https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 想要交互式演示GAN的训练过程，请参考浏览器中的*玩转生成对抗网络（GANs）* ([https://poloclub.github.io/ganlab](https://poloclub.github.io/ganlab))。更多详情，请查看相应的论文
    *GAN lab:* *通过交互式视觉实验理解复杂的深度生成模型* ([https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf](https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf))。
- en: Challenges
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 挑战
- en: 'Now we will cover some common issues and challenges encountered when training
    a GAN. Let’s explore insights and explanations about the cause of such issues:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将讨论在训练生成对抗网络（GAN）时遇到的一些常见问题和挑战。让我们探讨这些问题的原因：
- en: '**Mode collapse**: In this scenario, the generator overfits to a limited number
    of samples and patterns producing the same or similar synthetic samples for different
    z values. For example, a GAN being trained to generate cat images may keep generating
    the same cat image again and again with just minor modifications. This is something
    that we do not want to happen. The point of using GANs is to generate diverse
    synthetic examples. This problem occurs when the generator learns to produce one
    or a few excellent synthetic samples that fool the discriminator. Thus, the generator
    avoids generating other samples and prefers to repeat these excellent synthetic
    samples. There are various solutions to this problem, such as *unrolled GANs*
    ([https://arxiv.org/pdf/1611.02163.pdf](https://arxiv.org/pdf/1611.02163.pdf))
    and *Wasserstein* *loss* ([https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式坍塌**：在这种情况下，生成器过度拟合到有限数量的样本和模式，为不同的z值生成相同或相似的合成样本。例如，一个被训练生成猫图像的GAN可能会不断地生成几乎相同的猫图像，只是进行了一些细微的修改。这是我们不想看到的情况。使用GAN的目的就是要生成多样化的合成示例。当生成器学会产生一个或几个能够欺骗判别器的优秀合成样本时，就会发生这个问题。因此，生成器会避免生成其他样本，而更倾向于重复这些优秀的合成样本。对此问题有各种解决方案，例如*展开GANs*
    ([https://arxiv.org/pdf/1611.02163.pdf](https://arxiv.org/pdf/1611.02163.pdf))
    和 *Wasserstein* *损失* ([https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875))。'
- en: '**Discriminator saturations (diminished gradients)**: As we discussed earlier,
    the training of the generator and discriminator is done in an adversarial manner.
    When the discriminator becomes too successful at classifying real from synthetic
    samples, the error becomes minimal. Thus, the generator can no longer learn useful
    things.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**判别器饱和（梯度减弱）**：正如我们之前讨论的，生成器和判别器的训练是以对抗的方式进行。当判别器在区分真实样本和合成样本方面变得过于成功时，错误变得最小。因此，生成器就不再能学习到有用的东西。'
- en: '**Hyperparameter sensitivity and tuning**: Similar to other DL architectures,
    GANs have many hyperparameters, such as learning rate, batch size, number of layers,
    activation functions, and others. Finding the optimal hyperparameters is problem-
    and task-dependent and usually a train-error process. Thus, it is challenging
    to find the right architecture and hyperparameters to successfully train your
    GAN.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数敏感性和调整**：与其他深度学习架构类似，GAN有许多超参数，例如学习率、批量大小、层数、激活函数等。找到最佳超参数是问题和任务相关的，通常是一个训练错误的过程。因此，找到正确的架构和超参数以成功训练你的GAN是一个挑战。'
- en: '**Instability and non-convergence**: It is not easy to stabilize the training
    process of the generator and discriminator. In fact, it is common to observe that
    one submodel is learning better than another, which causes the GAN to oscillate,
    giving us unpredictable behavior, and the models may never converge. For more
    details, please refer to *On Convergence and Stability of* *GANs* ([https://arxiv.org/pdf/1705.07215.pdf](https://arxiv.org/pdf/1705.07215.pdf)).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不稳定性和非收敛性**：稳定生成器和判别器的训练过程并不容易。事实上，通常观察到某个子模型的学习效果比另一个好，这会导致GAN振荡，产生不可预测的行为，并且模型可能永远不会收敛。更多细节请参阅
    *On Convergence and Stability of GANs* ([https://arxiv.org/pdf/1705.07215.pdf](https://arxiv.org/pdf/1705.07215.pdf))。'
- en: '**Computation complexity**: GANs have a complex structure, being composed of
    two DL models. This makes the training process computationally expensive and time-consuming.
    However, there are some techniques proposed to speed up the training process,
    such as *Small-GAN: Speeding up GAN Training using Core-Sets* ([http://proceedings.mlr.press/v119/sinha20b/sinha20b.pdf](http://proceedings.mlr.press/v119/sinha20b/sinha20b.pdf))
    and *Projected GANs Converge* *Faster* ([https://proceedings.neurips.cc/paper/2021/file/9219adc5c42107c4911e249155320648-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/9219adc5c42107c4911e249155320648-Paper.pdf)).'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算复杂度**：GAN具有复杂的结构，由两个深度学习模型组成。这使得训练过程在计算上既昂贵又耗时。然而，有一些技术被提出以加快训练过程，例如 *Small-GAN:
    使用核心集加速GAN训练* ([http://proceedings.mlr.press/v119/sinha20b/sinha20b.pdf](http://proceedings.mlr.press/v119/sinha20b/sinha20b.pdf))
    和 *投影GANs收敛更快* ([https://proceedings.neurips.cc/paper/2021/file/9219adc5c42107c4911e249155320648-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/9219adc5c42107c4911e249155320648-Paper.pdf))。'
- en: In the next section, we delve into deploying GANs to generate synthetic data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将深入探讨将GAN用于生成合成数据。
- en: Utilizing GANs to generate synthetic data
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用GAN生成合成数据
- en: In this section, we will highlight some interesting applications of GANs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将突出一些GAN的有趣应用。
- en: GANs have enormous applications because they can be used for data augmentation,
    style transfer, privacy protection, and generating photo-realistic images.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: GANs（生成对抗网络）有巨大的应用范围，因为它们可以用于数据增强、风格迁移、隐私保护和生成逼真的照片。
- en: 'Let’s discuss some of these applications:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一些这些应用：
- en: '**Generating images**: GANs can be utilized to generate photorealistic images.
    For instance, GANs were utilized to generate handwritten digits, human faces,
    animals, objects, and scenes. Please check this paper for more details: *Progressive
    Growing of GANs for Improved Quality, Stability, and* *Variation* ([https://arxiv.org/pdf/1710.10196.pdf](https://arxiv.org/pdf/1710.10196.pdf)).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成图像**：GANs可以用来生成逼真的图像。例如，GANs被用来生成手写数字、人脸、动物、物体和场景。请查看这篇论文以获取更多详细信息：*渐进式增长GANs以提高质量、稳定性和*
    *多样性* ([https://arxiv.org/pdf/1710.10196.pdf](https://arxiv.org/pdf/1710.10196.pdf))。'
- en: '**Generating cartoon and anime characters**: GANs can be trained to generate
    appealing and diverse characters. This can be utilized to assess artists, game
    developers, and designers with anime characters. For more details, please check
    the paper *Towards the Automatic Anime Characters Creation with Generative Adversarial
    Networks* ([https://arxiv.org/pdf/1708.05509.pdf](https://arxiv.org/pdf/1708.05509.pdf))
    and the website ([https://make.girls.moe](https://make.girls.moe)).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成卡通和动漫角色**：GANs可以被训练来生成吸引人和多样化的角色。这可以用来评估艺术家、游戏开发者和动漫设计师。更多详细信息，请查看论文*使用生成对抗网络自动创建动漫角色*
    ([https://arxiv.org/pdf/1708.05509.pdf](https://arxiv.org/pdf/1708.05509.pdf))和网站([https://make.girls.moe](https://make.girls.moe))。'
- en: '**Image-to-image translation**: GANs can be utilized to transform images from
    one domain to another domain. For example, **machine learning** (**ML**)-based
    colorizers usually utilize GANs for turning grayscale images into colored ones.
    *Image-to-Image Translation with Conditional Adversarial Networks* ([https://arxiv.org/abs/1611.07004](https://arxiv.org/abs/1611.07004))
    and *StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image
    Translation* ([https://arxiv.org/pdf/1711.09020.pdf](https://arxiv.org/pdf/1711.09020.pdf))
    are well-known examples of image-to-image GAN-based translators.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像到图像翻译**：GANs可以用来将图像从一个域转换到另一个域。例如，基于机器学习的**色彩化**通常使用GANs将灰度图像转换为彩色图像。*基于条件对抗网络的图像到图像翻译*
    ([https://arxiv.org/abs/1611.07004](https://arxiv.org/abs/1611.07004))和*StarGAN：用于多域图像到图像翻译的统一生成对抗网络*
    ([https://arxiv.org/pdf/1711.09020.pdf](https://arxiv.org/pdf/1711.09020.pdf))是图像到图像基于GAN的翻译器的知名例子。'
- en: '**Text-to-image translation**: Another interesting application of GANs is to
    generate appealing images from a given short textual description of scenes and
    objects. As examples, check *StackGAN: Text to Photo-realistic Image Synthesis
    with Stacked Generative Adversarial Networks* ([https://arxiv.org/abs/1612.03242](https://arxiv.org/abs/1612.03242))
    and DALL-E ([https://openai.com/research/dall-e](https://openai.com/research/dall-e)).'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本到图像翻译**：GANs的另一个有趣应用是从给定的场景和对象的简短文本描述中生成吸引人的图像。例如，查看*StackGAN：使用堆叠生成对抗网络进行文本到逼真图像合成*
    ([https://arxiv.org/abs/1612.03242](https://arxiv.org/abs/1612.03242))和DALL-E
    ([https://openai.com/research/dall-e](https://openai.com/research/dall-e))。'
- en: 'In addition to the applications we have discussed, GANs can be used for the
    following non-exhaustive list of interesting tasks and applications:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们讨论的应用之外，GANs还可以用于以下非详尽列表中的有趣任务和应用：
- en: Semantic image-to-photo translation
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义图像到照片翻译
- en: Generate photographs of human faces
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成人脸照片
- en: Face aging
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人脸老化
- en: Pose guided person image generation
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姿势引导的人像生成
- en: Photos to emojis
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 照片转表情符号
- en: Photograph editing
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 照片编辑
- en: Image blending
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像融合
- en: Image inpainting
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像修复
- en: Super-resolution
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超分辨率
- en: Video prediction
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频预测
- en: 3D object generation
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3D物体生成
- en: Texture synthesis
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纹理合成
- en: Anomaly detection
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测
- en: Next, we will delve into a hands-on example that demonstrates the practical
    application of GANs for generating photorealistic synthetic images.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨一个实际应用的实例，展示GANs在生成逼真合成图像方面的实际应用。
- en: Hands-on GANs in practice
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践中的动手GANs
- en: Let’s examine how we can utilize a GAN to generate some synthetic images in
    practice. We will examine *Closed-Form Factorization of Latent Semantics in GANs*
    ([https://arxiv.org/abs/2007.06600](https://arxiv.org/abs/2007.06600)) to learn
    how we can simply generate synthetic images for our ML problem. The code for this
    example was adapted from the paper’s original GitHub ([https://github.com/genforce/sefa](https://github.com/genforce/sefa)).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨如何在实践中利用GAN生成一些合成图像。我们将研究*GANs中潜在语义的闭式分解*([https://arxiv.org/abs/2007.06600](https://arxiv.org/abs/2007.06600))，以了解我们如何简单地为我们的人工智能问题生成合成图像。此示例的代码是从论文原始GitHub([https://github.com/genforce/sefa](https://github.com/genforce/sefa))改编的。
- en: 'We begin by importing the essential libraries as shown:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入必要的库，如下所示：
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we select the parameters of the generation process such as the number
    of images to generate, and the noise seed. Please note that the `seed` parameter
    will help us to get diverse images in this example:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们选择生成过程的参数，例如要生成的图像数量和噪声种子。请注意，`seed`参数将帮助我们在这个例子中获得多样化的图像：
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we have the latent semantics parameters of the GAN as proposed by **SeFa**.
    Simply, we can change some semantics of the synthesized image by changing these
    parameters. For example, we can change the painting style, gender, posture, and
    other semantics of the generated image. For more details about **SeFa**, please
    refer to *Closed-Form Factorization of Latent Semantics in* *GANs* ([https://arxiv.org/abs/2007.06600](https://arxiv.org/abs/2007.06600)):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有了由**SeFa**提出的GAN的潜在语义参数。简单来说，我们可以通过改变这些参数来改变合成图像的一些语义。例如，我们可以改变绘画风格、性别、姿势等生成的图像的语义。有关**SeFa**的更多详细信息，请参阅*GANs中潜在语义的闭式分解*([https://arxiv.org/abs/2007.06600](https://arxiv.org/abs/2007.06600))：
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we have the following models:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有以下模型：
- en: '`stylegan_animeface512`: This can be used to generate anime faces with diverse
    expressions. For more details, please refer to *A Style-Based Generator* *Architecture
    for Generative Adversarial* *Networks* ([https://arxiv.org/abs/1812.04948](https://arxiv.org/abs/1812.04948)).'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stylegan_animeface512`：这可以用来生成具有多种表情的动漫面孔。更多详情请参阅*用于生成对抗网络的基于风格的生成器架构*([https://arxiv.org/abs/1812.04948](https://arxiv.org/abs/1812.04948))。'
- en: '`stylegan_car512`: This can be utilized to generate interesting car models.
    We will use this model in our example.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stylegan_car512`：这可以用来生成有趣的汽车模型。我们将在我们的例子中使用这个模型。'
- en: '`stylegan_cat256`: We can leverage this model to generate photorealistic cat
    images.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stylegan_cat256`：我们可以利用这个模型来生成逼真的猫图像。'
- en: '`pggan_celebahq1024`: This is a **Progressive Growing GAN** (**PGGAN**) that
    was trained to generate photorealistic celebrity images. For more details, please
    refer to *Progressive Growing of GANs for Improved Quality, Stability, and* *Variation*
    ([https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)).'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pggan_celebahq1024`：这是一个**渐进式增长GAN**（**PGGAN**），经过训练以生成逼真的名人图像。更多详情请参阅*GANs的渐进式增长以提高质量、稳定性和多样性*([https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196))。'
- en: '`stylegan_bedroom256`: This can be deployed to generate bedroom layout images.
    For more details, please refer to *Analyzing and Improving the Image Quality of*
    *StyleGAN* ([https://arxiv.org/abs/1912.04958](https://arxiv.org/abs/1912.04958)).'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stylegan_bedroom256`：这可以部署以生成卧室布局图像。更多详情请参阅*分析并改进StyleGAN的图像质量*([https://arxiv.org/abs/1912.04958](https://arxiv.org/abs/1912.04958))。'
- en: 'We select the model name that we want to test:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择我们想要测试的模型名称：
- en: '[PRE3]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we need to load the generator of GAN. Please remember that we do not
    need the discriminator to generate the images. It is only used to help the generator
    to train on generating the images that we want:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要加载GAN的生成器。请记住，我们不需要判别器来生成图像。它仅用于帮助生成器训练生成我们想要的图像：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we send the code to the generator to sample from the latent space. The
    code is simply random noise. It is the random noise vector, z, which we saw in
    *Figure 7**.1*:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将代码发送到生成器以从潜在空间中采样。代码仅仅是随机噪声。正是我们之前在*图7.1*中看到的随机噪声向量z：
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we synthetize the image by sending the noise vector (code):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过发送噪声向量（代码）来合成图像：
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let us visualize the output of the GAN in *Figure 7**.2*:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在*图7.2*中可视化GAN的输出：
- en: '![Figure 7.2 – Images generated using StyleGAN](img/Figure_07_02_B18494.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – 使用StyleGAN生成的图像](img/Figure_07_02_B18494.jpg)'
- en: Figure 7.2 – Images generated using StyleGAN
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 使用StyleGAN生成的图像
- en: 'After changing the latent semantic parameters as described by **SeFa**, we
    get the outputs shown in *Figure 7**.3*:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 根据SeFa所描述的，改变潜在语义参数后，我们得到*图7.3*所示的输出：
- en: '![Figure 7.3 – SeFa approach for controlling generation process by changing
    latent semantic parameters](img/Figure_07_03_B18494.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – 通过改变潜在语义参数来控制生成过程的SeFa方法](img/Figure_07_03_B18494.jpg)'
- en: Figure 7.3 – SeFa approach for controlling generation process by changing latent
    semantic parameters
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 通过改变潜在语义参数来控制生成过程的SeFa方法
- en: 'In the same way, we can generate images of anime facial expressions, celebrities’
    faces, and bedroom layouts using the aforementioned models, as shown in *Figure
    7**.4*:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以使用上述模型生成动漫面部表情、名人面孔和卧室布局的图像，如图*图7.4*所示：
- en: '![Figure 7.4 – A sample of images generated using different GAN models](img/Figure_07_04_B18494.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 使用不同GAN模型生成的图像样本](img/Figure_07_04_B18494.jpg)'
- en: Figure 7.4 – A sample of images generated using different GAN models
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 使用不同GAN模型生成的图像样本
- en: As we have seen in this example, we can effortlessly utilize GANs to generate
    diverse and photorealistic data for training and testing our own ML models. Next,
    we will explore the variations of GANs that facilitate many amazing applications.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本例中看到的，我们可以轻松地利用生成对抗网络（GANs）来生成多样化的、逼真的数据，用于训练和测试我们自己的机器学习模型。接下来，我们将探讨GANs的各种变体，这些变体促进了许多令人惊叹的应用。
- en: Variations of GANs
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GANs的变体
- en: 'In this section, we will explore the main variation of GANs. For an interesting
    practical application of GANs, please refer to [*Chapter 12*](B18494_12.xhtml#_idTextAnchor203)
    and *Case Study 3 – Predictive Analytics* to see how Amazon utilized GANs for
    fraud transaction prediction. For more applications, please refer to *Generative
    Adversarial Networks in the built environment: A comprehensive review of the application
    of GANs across data types and* *scales* ([https://www.sciencedirect.com/science/article/abs/pii/S0360132322007089](https://www.sciencedirect.com/science/article/abs/pii/S0360132322007089)).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨GANs的主要变体。对于GANs的一个有趣的实际应用，请参阅[*第12章*](B18494_12.xhtml#_idTextAnchor203)和*案例研究3
    – 预测分析*，了解亚马逊如何利用GANs进行欺诈交易预测。更多应用，请参阅*建筑环境中生成对抗网络：GANs在数据类型和* *尺度* *上的综合回顾* ([https://www.sciencedirect.com/science/article/abs/pii/S0360132322007089](https://www.sciencedirect.com/science/article/abs/pii/S0360132322007089))。
- en: Conditional GAN (cGAN)
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 条件GAN (cGAN)
- en: A typical GAN generates images given a random noise vector. However, in many
    scenarios, we really want to control the attributes and properties of the generated
    synthetic samples. For example, suppose you are deploying a GAN to generate human
    faces. The standard GAN architecture has no way to let you specify some attributes
    of the generated faces such as gender, age, eye color, and hair length. Using
    cGAN, we can condition the GAN on these attributes in the training process. Thus,
    we are able to generate synthetic samples with certain attributes. For more details,
    refer to *Conditional Generative Adversarial Nets* at [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的GAN在给定一个随机噪声向量的情况下生成图像。然而，在许多场景中，我们确实希望控制生成的合成样本的属性和特性。例如，假设你正在部署一个GAN来生成人脸。标准的GAN架构没有让你指定生成人脸的一些属性的方法，例如性别、年龄、眼睛颜色和发长。使用cGAN，我们可以在训练过程中对这些属性进行条件化。因此，我们能够生成具有特定属性的合成样本。更多详情，请参阅[https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784)上的*条件生成对抗网络*。
- en: CycleGAN
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CycleGAN
- en: In an image-to-image translation task that aims to transform an image from one
    domain to another, DL models usually require matching pairs or pairwise correspondences
    between images from the two domains. This is extremely difficult to achieve. For
    instance, imagine preparing such a dataset for mapping images from one season
    (winter) to another (summer). An elegant solution to the problem is using CycleGANs,
    which can be trained to perform unpaired image-to-image translation between domains
    given only two sets of images from both domains without the need for any matching
    pairs. Thus, you only need to provide images taken in winter and summer and there
    is no need to capture the same scenes in winter and summer to provide matching
    pairs. For more details, please check *Unpaired Image-to-Image Translation using
    Cycle-Consistent Adversarial* *Networks* ([https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593)).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个旨在将图像从一个域转换到另一个域的图像到图像翻译任务中，深度学习模型通常需要两个域中图像的匹配对或成对对应关系。这非常难以实现。例如，想象一下为将一个季节（冬季）的图像映射到另一个季节（夏季）的图像准备这样的数据集。CycleGANs是解决这个问题的优雅解决方案，它可以在仅给定两个域的图像集的情况下，无需任何匹配对，训练成在域之间执行未配对图像到图像翻译。因此，你只需要提供冬季和夏季拍摄的图像，无需在冬季和夏季捕捉相同的场景来提供匹配对。更多详情，请参阅
    *使用循环一致对抗网络进行未配对图像到图像翻译* ([https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593))。
- en: Conditional Tabular GAN (CTGAN)
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 条件表格GAN (CTGAN)
- en: CTGANs are a specific variant of GANs that can generate tabular synthetic data.
    It is very challenging for other GANs to capture the dependencies between columns
    or attributes of a given tabular dataset. A CTGAN is a cGAN that can be utilized
    to model these joint probability distributions between these columns. CTGANs have
    enormous applications in data augmentation, imputation, and anomaly detection.
    For more details, please refer to *Modeling Tabular Data using Conditional* *GAN*
    ([https://arxiv.org/abs/1907.00503](https://arxiv.org/abs/1907.00503)).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: CTGANs是GANs的一个特定变体，可以生成表格合成数据。其他GANs捕捉给定表格数据集中列或属性之间的依赖关系非常具有挑战性。CTGAN是一种cGAN，可以用来建模这些列之间的联合概率分布。CTGANs在数据增强、插补和异常检测中有着巨大的应用。更多详情，请参阅
    *使用条件GAN建模表格数据* ([https://arxiv.org/abs/1907.00503](https://arxiv.org/abs/1907.00503))。
- en: Wasserstein GAN (WGAN) and Wasserstein GAN with Gradient Penalty (WGAN-GP)
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 水晶GAN (WGAN) 和带梯度惩罚的水晶GAN (WGAN-GP)
- en: WGAN and WGAN-GP are variants of the original GANs. Unlike GANs, which use a
    binary cross-entropy loss to classify real and fake samples, this variation utilizes
    Wasserstein distance to measure the distance between the real and fake data probability
    distributions. Furthermore, WGAN-GP implements a gradient penalty term to enforce
    the Lipschitz constraint on the discriminator. These two variants were shown to
    produce better results and to be more stable. For more details, check *Wasserstein
    GAN* ([https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)) and
    *Improved Training of Wasserstein* *GANs* ([https://arxiv.org/abs/1704.00028](https://arxiv.org/abs/1704.00028)).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: WGAN和WGAN-GP是原始GANs的变体。与使用二元交叉熵损失来分类真实和假样本的GANs不同，这种变体利用Wasserstein距离来衡量真实和假数据概率分布之间的距离。此外，WGAN-GP实现了一个梯度惩罚项，以强制对判别器施加Lipschitz约束。这两个变体已被证明能产生更好的结果，并且更稳定。更多详情，请参阅*Wasserstein
    GAN* ([https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)) 和
    *改进Wasserstein GANs的训练* ([https://arxiv.org/abs/1704.00028](https://arxiv.org/abs/1704.00028))。
- en: f-GAN
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: f-GAN
- en: 'f-GANs are another family of GANs that utilize *f*-divergences to measure and
    minimize the divergence between real and fake samples’ probability distributions.
    This variant of GANs has been widely utilized in image and text generation. For
    more details, please check *f-GAN: Training Generative Neural Samplers using Variational
    Divergence* *Minimization* ([https://arxiv.org/abs/1606.00709](https://arxiv.org/abs/1606.00709)).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: f-GANs是另一类利用*f*-散度来衡量和最小化真实和假样本概率分布之间差异的GANs。这种GANs的变体已在图像和文本生成中得到广泛应用。更多详情，请参阅
    *f-GAN：使用变分散度最小化训练生成性神经网络采样器* ([https://arxiv.org/abs/1606.00709](https://arxiv.org/abs/1606.00709))。
- en: DragGAN
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DragGAN
- en: 'DragGANs are another recent promising variation of GANs that open the door
    for many amazing applications, such as point-based image editing. DragGANs allow
    users to generate photorealistic synthetic images in an interactive and intuitive
    manner. DragGANs stand out due to their distinctive approach to optimizing the
    latent space and their unique method of point tracking. For more information,
    please refer to *Drag Your GAN: Interactive Point-based Manipulation on the Generative
    Image* *Manifold* ([https://arxiv.org/abs/2305.10973](https://arxiv.org/abs/2305.10973)).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: DragGANs是GANs最近的一个有希望的变体，为许多令人惊叹的应用打开了大门，例如基于点的图像编辑。DragGANs允许用户以交互式和直观的方式生成逼真的合成图像。DragGANs因其独特的优化潜在空间的方法和独特的点跟踪方法而脱颖而出。更多信息，请参阅*拖动你的GAN：在生成图像流形上的交互式基于点的操作*
    ([https://arxiv.org/abs/2305.10973](https://arxiv.org/abs/2305.10973))。
- en: Let’s wrap things up before we move on.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们总结一下。
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have discussed what GANs are, their architecture, and the
    training process. At the same time, we explored how GANs were utilized for various
    applications such as image-to-image translation. Additionally, we covered a coding
    example demonstrating how to use GANs to generate photorealistic images. In this
    chapter, we also learned about the main variations of GANs. In the next chapter,
    we will continue our learning journey by exploring another exciting approach for
    generating synthetic data by utilizing video games.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了GANs是什么，它们的架构以及训练过程。同时，我们探讨了GANs是如何被用于各种应用的，例如图像到图像的翻译。此外，我们还提供了一个编码示例，展示了如何使用GANs生成逼真的图像。在本章中，我们还了解了GANs的主要变体。在下一章中，我们将通过探索利用视频游戏生成合成数据的另一种令人兴奋的方法来继续我们的学习之旅。
