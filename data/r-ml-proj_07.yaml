- en: Credit Card Fraud Detection Using Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自编码器进行信用卡欺诈检测
- en: Fraud management has been known to be a very painful problem for banking and
    finance firms. Card-related frauds have proven to be especially difficult for
    firms to combat. Technologies such as chip and PIN are available and are already
    used by most credit card system vendors, such as Visa and MasterCard. However,
    the available technology is unable to curtail 100% of credit card fraud. Unfortunately,
    scammers come up with newer ways of phishing to obtain passwords from credit card
    users. Also, devices such as skimmers make stealing credit card data a cake walk!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈管理一直被银行和金融公司视为一个非常痛苦的问题。与卡片相关的欺诈已被证明对公司的对抗特别困难。芯片和PIN等技术已经可用，并被大多数信用卡系统供应商，如Visa和MasterCard所使用。然而，现有技术无法遏制100%的信用卡欺诈。不幸的是，骗子想出了新的钓鱼方式来从信用卡用户那里获取密码。此外，像读卡器这样的设备使窃取信用卡数据变得轻而易举！
- en: Despite the availability of some technical abilities to combat credit card fraud,
    *The Nilson Report*, a leading publication covering payment systems worldwide,
    estimated that credit card fraud is going to soar to $32 billion in 2020 ([https://nilsonreport.com/upload/content_promo/The_Nilson_Report_10-17-2017.pdf](https://nilsonreport.com/upload/content_promo/The_Nilson_Report_10-17-2017.pdf)).
    To get a perspective on the estimated loss, it is more than the recent profits
    posted by companies such as Coca-Cola ($2 billion), Warren Buffet’s Berkshire
    Hathaway ($24 billion), and JP Morgan Chase ($23.5 billion)!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有一些技术能力可以用来对抗信用卡欺诈，但*《尼尔森报告》*，这是一份覆盖全球支付系统的领先出版物，估计信用卡欺诈将在2020年激增至320亿美元([https://nilsonreport.com/upload/content_promo/The_Nilson_Report_10-17-2017.pdf](https://nilsonreport.com/upload/content_promo/The_Nilson_Report_10-17-2017.pdf))。为了了解估计的损失，这超过了可口可乐（20亿美元）、沃伦·巴菲特的大都会公司（240亿美元）和摩根大通（235亿美元）最近公布的利润！
- en: 'While credit card chip technology-providing companies have been investing hugely
    to advance the technology to counter credit card fraud, in this chapter, we are
    going to examine whether and how far machine learning can help deal with the credit
    card fraud problem. We will cover the following topics as we progress through
    this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然提供信用卡芯片技术的公司一直在大量投资以推进技术以应对信用卡欺诈，但在本章中，我们将探讨机器学习是否以及如何帮助解决信用卡欺诈问题。随着我们进入本章，我们将涵盖以下主题：
- en: Machine learning in credit card fraud detection
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信用卡欺诈检测中的机器学习
- en: Autoencoders and the various types
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器和各种类型
- en: The credit card fraud dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信用卡欺诈数据集
- en: Building AEs with the H2O library in R
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在R中使用H2O库构建AE
- en: Implementation of auto encoder for credit card fraud detection
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自编码器进行信用卡欺诈检测的实施
- en: Machine learning in credit card fraud detection
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信用卡欺诈检测中的机器学习
- en: 'The task of fraud detection often boils down to outlier detection, in which
    a dataset is verified to find potential anomalies in the data. Traditionally,
    this task was deemed a manual task, where risk experts checked all transactions
    manually. Even though there is a technical layer, it is purely based on a rules
    base that scans through each transaction, and then those shortlisted as suspicious
    are sent through for a manual review to make a final decision on the transaction.
    However, there are some major drawbacks to this system:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈检测的任务通常归结为异常检测，其中数据集被验证以寻找数据中的潜在异常。传统上，这项任务被认为是一项手动任务，风险专家会手动检查所有交易。尽管存在技术层，但它纯粹基于基于规则的系统，该系统扫描每一笔交易，然后将列为可疑的交易提交人工审查以对交易做出最终决定。然而，这个系统存在一些主要的缺点：
- en: Organizations need substantial fraud management budgets for manual review staff.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织需要为人工审查人员设立大量的欺诈管理预算。
- en: Extensive training is required to train the employees working as manual review
    staff.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对作为人工审查人员工作的员工进行广泛的培训是必要的。
- en: Training the personnel to manually review transactions is time consuming and
    expensive.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 培训人员手动审查交易既耗时又昂贵。
- en: Even the most highly trained manual review staff carry certain biases, therefore
    making the whole review system inaccurate.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使是最受培训的人工审查人员也携带某些偏见，因此使整个审查系统不准确。
- en: Manual reviews increase the time required to fulfill a transaction. The customers
    might get frustrated with the long wait times required to pass a credit card transaction.
    This may impact the loyalty of customers.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工审查增加了完成交易所需的时间。顾客可能会因为通过信用卡交易所需的长等待时间而感到沮丧。这可能会影响顾客的忠诚度。
- en: Manual reviews may yield false positives. A false positive not only affects
    the sale in the process but also lifetime value generated from the customer.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工审查可能会导致假阳性。假阳性不仅会影响过程中的销售，还会影响客户产生的终身价值。
- en: Fortunately, with the rise of **machine learning** (**ML**), **artificial intelligence**
    (**AI**), and deep learning, it became feasible to automate the manual credit
    card transaction review process to a large extent. This not only saves an intensive
    amount of labor but also yields better detection of credit card fraud, which otherwise
    is impacted due to biases that human reviewers carry.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，随着**机器学习**（**ML**）、**人工智能**（**AI**）和深度学习的兴起，在很大程度上自动化了人工信用卡交易审查过程成为可能。这不仅节省了大量劳动力，而且还能更好地检测信用卡欺诈，否则由于人工审查者携带的偏见，欺诈检测可能会受到影响。
- en: ML-based fraud detection strategies generally can be accomplished using both
    supervised ML and unsupervised ML techniques.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于机器学习的欺诈检测策略通常可以使用监督机器学习和无监督机器学习技术来完成。
- en: Supervised ML models are generally used when large amounts of transaction data
    tagged as **genuine **or **fraud **are available. A model is trained on the labeled
    dataset and the resultant model is then used for classifying any new credit card
    transactions into one of the two possible classes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当有大量标记为**真实**或**欺诈**的交易数据可用时，通常使用监督机器学习模型。模型在标记的数据集上训练，然后使用得到的模型将任何新的信用卡交易分类到两个可能的类别之一。
- en: With most organizations, the problem is that labeled data is unavailable, or
    very little labeled data is available. This makes supervised learning models less
    feasible. This is where unsupervised models come into play. They are designed
    to spot anomalous behavior in transactions and they do not need explicit pre-labeled
    data to identify the anomalous behavior. The general idea in unsupervised fraud
    detection is to detect behavior anomalies by identifying transactions that do
    not conform to the majority.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数组织中，问题在于标记的数据不可用，或者可用的标记数据非常少。这使得监督学习模型不太可行。这就是无监督模型发挥作用的地方。它们被设计用来在交易中识别异常行为，并且它们不需要显式的预标记数据来识别异常行为。在无监督欺诈检测中的基本思想是通过识别不符合大多数交易的交易来检测行为异常。
- en: Another thing to keep in mind is that fraud events are rare, and are not as
    common as genuine transactions. Due to the rarity of fraud, severe class imbalance
    problem may be seen in datasets related to credit card fraud. In other words,
    one would observe that 95% or more of the data in the dataset is of genuine transactions,
    and less than 5% of the data belongs to fraudulent transactions. Also, even if
    you learn about a fraudulent transaction today, the model is likely to face an
    anomaly tomorrow with different features. So, the problem space of genuine transactions
    is well known and it is pretty much stagnant; however, the problem space for fraudulent
    transactions is not well known and it is not constant. Due to these reasons, it
    make sense to deal with the fraud detection problem with unsupervised learning
    rather than supervised learning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点需要记住的是，欺诈事件是罕见的，并不像真实交易那样普遍。由于欺诈的罕见性，数据集中可能会出现严重的类别不平衡问题。换句话说，人们会观察到数据集中95%或更多的数据是真实交易，而少于5%的数据属于欺诈交易。此外，即使你今天了解到了一个欺诈交易，模型明天可能也会因为不同的特征而面临异常。因此，真实交易的问题空间是众所周知的，并且几乎是停滞不前的；然而，欺诈交易的问题空间并不为人所知，并且不是恒定的。由于这些原因，用无监督学习而不是监督学习来处理欺诈检测问题是有意义的。
- en: 'Anomaly detection is an unsupervised learning algorithm that is also termed
    a **one-class classification** algorithm. It distinguishes between **normal** and
    **anomalous** observations. The key principle on which the algorithm is built
    is that anomalous observations do not conform to the expected pattern of other
    common observations in a dataset. It is called a one-class classification as it
    learns the pattern of genuine transactions, and anything that shows non-conformance
    to this pattern is termed as an **anomaly**, and therefore as a fraudulent **transaction**.
    The following figure is an illustration showing anomaly detection in a two-dimensional
    space:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测是一种无监督学习算法，也被称为**单类分类**算法。它区分**正常**和**异常**观察。算法建立的关键原则是异常观察不符合数据集中其他常见观察的预期模式。因为它学习真实交易的模式，任何不符合这一模式的都被称为**异常**，因此也被视为**欺诈交易**。以下图示展示了在二维空间中的异常检测：
- en: '![](img/a74dd4da-4def-42ae-aa44-3d2cf811cb62.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a74dd4da-4def-42ae-aa44-3d2cf811cb62.png)'
- en: Anomaly detection illustrated in 2D space
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在2D空间中展示的异常检测
- en: 'A simple example of an anomaly is the identification of data points that are
    too far from the mean (standard deviation) in a time series. The following figure
    is an illustration displaying the data points that are identified as anomalies
    in a time series:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 异常的一个简单例子是识别时间序列中离均值（标准差）太远的点。以下图展示了在时间序列中被识别为异常的数据点：
- en: '![](img/9a73590b-e9ae-403e-86b8-b13ce39d2a66.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9a73590b-e9ae-403e-86b8-b13ce39d2a66.png)'
- en: Anomaly in time series—identified through standard deviation
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通过标准差识别的时间序列中的异常
- en: In this chapter, we will focus our efforts on a type of unsupervised deep learning
    application known as **AEs**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将集中精力研究一种称为**AEs**的无监督深度学习应用类型。
- en: Autoencoders explained
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动编码器解释
- en: '**Autoencoders** (**AEs**) are neural networks that are of a feedforward and
    non-recurrent type. They aim to copy the given inputs to the outputs. An AE works
    by compressing the input into a lower dimensional summary. This summary is often
    referred as latent space representation. An AE attempts to reconstruct the output
    from the latent space representation. An **Encoder**, a **Latent Space Representation**,
    and a **Decoder** are the three parts that make up the AEs. The following figure
    is an illustration showing the application of an AE on a sample picked from the MNIST
    dataset:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**自动编码器**（**AEs**）是前馈和非循环类型的神经网络。它们的目标是将给定的输入复制到输出。自动编码器通过将输入压缩到低维摘要来工作。这个摘要通常被称为潜在空间表示。自动编码器试图从潜在空间表示中重建输出。**编码器**、**潜在空间表示**和**解码器**是构成自动编码器的三个部分。以下图展示了在从MNIST数据集样本中选取的样本上应用自动编码器的示例：'
- en: '![](img/1ec6a118-78cf-4d2c-9747-e56c5f0a31fb.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1ec6a118-78cf-4d2c-9747-e56c5f0a31fb.png)'
- en: Application of AE on MNIST dataset sample
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在MNIST数据集样本上的自动编码器应用
- en: 'The encoder and decoder components of AEs are fully-connected feedforward networks.
    The number of neurons in a latent space representation is a hyperparameter that
    needs to be passed as part of building the AE. The number of neurons or nodes
    that is decided in the latent semantic space dictates the amount of compression
    that is attained while compressing the actual input image into a latent space
    representation. The general architecture of an AE is shown in the following figure:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器（AE）的编码器和解码器组件是完全连接的前馈网络。潜在空间表示中的神经元数量是一个需要作为构建AE的一部分传递的超参数。在潜在语义空间中决定的神经元或节点数量决定了在将实际输入图像压缩到潜在空间表示时获得的压缩量。自动编码器的一般架构如下所示：
- en: '![](img/bfd126a4-2ff5-4855-860e-3fbf58cf59e0.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bfd126a4-2ff5-4855-860e-3fbf58cf59e0.png)'
- en: General architecture of a AE
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器的一般架构
- en: The given input first passes through an **Encoder**, which is a fully-connected
    **artificial neural network** (**ANN**). The **Encoder** acts upon the **Input**
    and reduces its dimensions, as specified in the hyperparameter. The **Decoder**
    is another fully-connected ANN that picks up this reduced **Input** (latent space
    representation) and then reconstructs the **Output**. The goal is to get the **Output**
    identical to that of the **Input**. In general, the architectures of the **Encoder**
    and the **Decoder** are mirror images. Although there is no such requirement that
    mandates that the **Encoder** and **Decoder** architectures should be the same,
    it is generally practiced that way. In fact, the only requirement of the AE is
    to obtain identical output from that of the given input. Anything in between can
    be customized to the whims and fancies of the individual building the AE.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 给定的输入首先通过一个**编码器**，这是一个全连接的**人工神经网络**（**ANN**）。**编码器**作用于**输入**并减少其维度，如超参数所指定。**解码器**是另一个全连接的ANN，它拾取这个减少的**输入**（潜在空间表示）然后重建**输出**。目标是使**输出**与**输入**相同。一般来说，**编码器**和**解码器**的架构是镜像的。尽管没有这样的要求强制**编码器**和**解码器**的架构必须相同，但通常是这样实践的。实际上，自动编码器（AE）的唯一要求是从给定的输入中获得相同的输出。任何介于两者之间的都可以根据构建AE的个人喜好和想法进行定制。
- en: 'Mathematically, the encoder can be represented as:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，编码器可以表示为：
- en: '[![](img/f0867ffa-839a-4d32-b873-572ef7961eb2.png)]'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/f0867ffa-839a-4d32-b873-572ef7961eb2.png)'
- en: 'where *x* is the input and *h* is the function that acts on the input to represent
    it in a concise summary format. A decoder, on the other hand, can be represented
    as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x* 是输入，*h* 是作用于输入以将其表示为简洁摘要格式的函数。另一方面，解码器可以表示为：
- en: '[![](img/f8c2a45f-0bf3-4960-bc5b-5d66df7ad3a2.png).]'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/f8c2a45f-0bf3-4960-bc5b-5d66df7ad3a2.png)。'
- en: While the expectation is to obtain ![](img/7eed3d3e-b794-4999-a444-e13c99a809a8.png),
    this is not always the case as the reconstruction is done from a compact summary
    representation; therefore, there is occurrence of certain error. The error *e*
    is computed from the original input *x* and reconstructed output *r*, ![](img/71b0ef36-ab8e-44a8-9243-cfb52997e5e1.png)
    .
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然期望得到![图片](img/7eed3d3e-b794-4999-a444-e13c99a809a8.png)，但这并不总是如此，因为重建是从紧凑的摘要表示中完成的；因此，会出现某些错误。错误
    *e* 是从原始输入 *x* 和重建输出 *r* 计算得出的，![图片](img/71b0ef36-ab8e-44a8-9243-cfb52997e5e1.png)。
- en: The AE network then learns by reducing the **Mean Squared Error** (**MSE**),
    and the error is propagated back to the hidden layers for adjustment. The weights
    of the decoder and encoder are transposes of each other, which makes it faster
    to learn training parameters. The mirrored architectures of the encoder and decoder
    make it possible to learn the training parameters faster. In different architectures,
    the weights cannot be simply transposed; therefore, the computation time will
    increase. This is the reason for keeping the mirrored architectures for the encoder
    and decoder.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，自动编码器网络通过减少**均方误差**（**MSE**）来学习，错误被传播回隐藏层进行调整。解码器和编码器的权重是彼此的转置，这使得学习训练参数更快。编码器和解码器的镜像架构使得学习训练参数更快成为可能。在不同的架构中，权重不能简单地转置；因此，计算时间会增加。这就是为什么保持编码器和解码器的镜像架构的原因。
- en: Types of AEs based on hidden layers
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于隐藏层的自动编码器类型
- en: 'Based on the size of the hidden layer, AEs can be classified into two types, **undercomplete
    AEs** and **overcomplete AEs**:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 根据隐藏层的大小，自动编码器可以分为两种类型，**欠完备的自动编码器**和**过度完备的自动编码器**：
- en: '**Undercomplete AE**: If the AE simply learns to copy the input to the output,
    then it is not useful. The idea is to produce a concise representation as the
    output of the encoder, and this concise representation should consist of the most
    useful features of the input. The amount of conciseness achieved by the input
    layer is governed by the number of neurons or nodes that we use in the latent
    space representation. This can be set as a parameter while building the AE. If
    the number of neurons is set to fewer dimensions than that of the input features,
    then the AE is forced to learn most of the key features of the input data. The
    architecture where the number of neurons in latent space is less than that of
    input dimensions is called an undercomplete AE.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欠完备的自动编码器**：如果自动编码器只是学习将输入复制到输出，那么它就没有用了。想法是产生一个简洁的表示作为编码器的输出，这个简洁的表示应该包含输入的最有用特征。输入层达到的简洁程度由我们在潜在空间表示中使用的神经元或节点数量控制。这可以在构建自动编码器时作为一个参数设置。如果神经元的数量设置为比输入特征更少的维度，那么自动编码器被迫学习输入数据的大部分关键特征。潜在空间中神经元数量少于输入维度的架构称为欠完备的自动编码器。'
- en: '**Overcomplete AE**: It is possible to represent the number of neurons in latent
    space as equal to or more than that of the input dimensions. This kind of architecture
    is termed an overcomplete AE.  In this case, the AE does not learn anything and
    simply copies the input to the latent space, which in turn is propagated through
    to the decoder.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过度完备的自动编码器**：在潜在空间中，神经元的数量可以等于或超过输入维度。这种架构被称为过度完备的自动编码器。在这种情况下，自动编码器不学习任何东西，只是将输入复制到潜在空间，然后通过解码器传播。'
- en: 'Apart from the number of neurons in the latent space, the following are some
    of the other parameters that can be used in an AE architecture:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 除了潜在空间中神经元的数量外，以下是一些可以在自动编码器（AE）架构中使用的其他参数：
- en: '**Number of layers in the encoder and decoder**: The depth of the encoder and
    decoder can be set to any number. Generally, in a mirrored architecture of encoder
    and decoder, the number of layers is set as the same number. The last figure is
    an illustration showing the AE with two layers, excluding the input and output,
    in both the encoder and decoder.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器和解码器中的层数**：编码器和解码器的深度可以设置为任何数字。通常，在编码器和解码器的镜像架构中，层数设置为相同的数字。最后一张图展示了编码器和解码器中，除了输入和输出之外，都有两层自动编码器的示意图。'
- en: '**Number of neurons per layer in encoder and decoder**: The number of neurons
    decreases with each layer in an encoder and it increases with each layer in a
    decoder. The neurons in layers of encoders and decoders are symmetric.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器和解码器每层的神经元数量**：在编码器中，每层的神经元数量随着层数的减少而减少，在解码器中，每层的神经元数量随着层数的增加而增加。编码器和解码器层的神经元是对称的。'
- en: '**Loss function**: Loss functions such as MSE or cross-entropy are used by
    AEs to learn the weights during backpropagation. If the input is in the range
    of (0,1), then cross-entropy is used as metric, otherwise MSE is used.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数**：自动编码器在反向传播过程中使用如均方误差（MSE）或交叉熵等损失函数来学习权重。如果输入范围在（0,1）之间，则使用交叉熵作为度量标准，否则使用均方误差。'
- en: Types of AEs based on restrictions
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于约束的自动编码器类型
- en: 'Based on the restrictions imposed on the loss, AEs can be grouped into the
    following types:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 根据对损失的约束，自动编码器可以分为以下类型：
- en: '**Plain Vanilla AEs**: This is the simplest AE architecture possible, with
    a fully-connected neural layer as the encoder and decoder.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单自动编码器**：这是可能的最简单的自动编码器架构，其中编码器和解码器都是全连接神经网络层。'
- en: '**Sparse AEs**: Sparse AEs are an alternative method for introducing an information
    bottleneck, without requiring a reduction in the number of nodes in our hidden
    layers. Rather than preferring an undercomplete AE, the loss function is constructed
    in a way that it penalizes the activations within a layer. For any given observation,
    the network is encouraged to learn encoding and decoding, which only relies on
    activating a small number of neurons.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏自动编码器**：稀疏自动编码器是引入信息瓶颈的替代方法，无需减少我们隐藏层中的节点数量。而不是偏好欠完备的自动编码器，损失函数被构建成惩罚层内激活的方式。对于任何给定的观察值，网络被鼓励学习编码和解码，这仅依赖于激活少量神经元。'
- en: '**Denoising AEs**: This is a type of overcomplete AE that experiences the risk
    of learning the **identity function** or **null function**. Essentially, the AE
    learns the output that is equal to the input, therefore making the AE useless. Denoising
    AEs avoid this problem of learning the identity function by randomly initializing
    some of the inputs to 0\. During the computation of the loss function, the noise-induced
    input is not considered; therefore, the network still learns the correct weights
    without the risk of learning the identity function. At the same time, the AE is
    trained to learn to reconstruct the output, even from the corrupted input.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去噪自动编码器**：这是一种过完备的自动编码器，存在学习**恒等函数**或**零函数**的风险。本质上，自动编码器学习到的输出等于输入，因此使自动编码器变得无用。去噪自动编码器通过随机初始化一些输入为0来避免学习恒等函数的问题。在损失函数的计算过程中，不考虑噪声引起的输入；因此，网络仍然学习正确的权重，而不存在学习恒等函数的风险。同时，自动编码器被训练学习从损坏的输入中重建输出。'
- en: 'The following figure is a example of denoising AEs on sample images from the MNIST
    dataset:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示是MNIST数据集样本图像上的去噪自动编码器的示例：
- en: '![](img/d7a07b41-c689-4e55-a083-f06fe8654b83.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d7a07b41-c689-4e55-a083-f06fe8654b83.png)'
- en: Application of denoising AEs on MNIST samples
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在MNIST样本上应用去噪自动编码器
- en: '**Convolutional AEs**: When dealing with images as inputs, one can use convolutional
    layers as part of the encoder and decoder networks. Such kinds of AEs that use
    convolutional layers are termed **convolutional AEs**. The following figure is
    an illustration showing the use of convolutions in AEs:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积自动编码器**：当处理图像作为输入时，可以使用卷积层作为编码器和解码器网络的一部分。这类使用卷积层的自动编码器被称为**卷积自动编码器**。以下图示展示了在自动编码器中使用卷积的示例：'
- en: '![](img/cad4e496-083e-4442-9e06-9dfba7e17d0c.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cad4e496-083e-4442-9e06-9dfba7e17d0c.png)'
- en: Convolutional AEs
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积自动编码器
- en: '**Stacked AEs**: Stacked AEs are ones that have multiple layers in the encoder
    as well as the decoder. You can refer to the general architecture of an AE as
    an example illustration of a stacked AE architecture, with the encoder and decoder
    having two layers (excluding the input and output layers).'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**堆叠自动编码器**：堆叠自动编码器是在编码器和解码器中都有多层的一种自动编码器。你可以参考自动编码器的一般架构作为堆叠自动编码器架构的示例说明，其中编码器和解码器都有两层（不包括输入和输出层）。'
- en: '**Variational AEs**: A **variational AE** (**VAE**), rather than building an
    encoder that outputs a singl'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变分自动编码器**：**变分自动编码器**（**VAE**），而不是构建一个输出单个'
- en: 'e value to describe each latent state attribute, describes a probability distribution
    for each latent attribute. This makes it possible to design complex generative
    models of data and also generate fictional celebrity images and digital artwork.
    The following figure is an illustration depicting the representation of data in
    VAEs:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: e值来描述每个潜在状态属性，为每个潜在属性描述一个概率分布。这使得设计复杂的数据生成模型和生成虚构名人图像以及数字艺术品成为可能。以下图示展示了VAE中数据的表示：
- en: '![](img/f0cf2cf0-7185-44d1-a669-4a7522e025b3.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f0cf2cf0-7185-44d1-a669-4a7522e025b3.png)'
- en: In a VAE, the encoder model is sometimes referred to as the recognition model,
    whereas the decoder model is sometimes referred to as the generative model. The
    encoder outputs a range of statistical distributions for the latent features.
    These features are randomly sampled and used by the decoder to reconstruct the
    input. For any sampling of the latent distributions, the decoder is expected to
    be able to accurately reconstruct the input. Thus, values that are nearby to one
    another in latent space should correspond with very similar reconstructions.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在变分自动编码器（VAE）中，编码器模型有时被称为识别模型，而解码器模型有时被称为生成模型。编码器输出一系列关于潜在特征的统计分布。这些特征是随机采样的，并由解码器用于重建输入。对于任何潜在分布的采样，解码器都应能够准确地重建输入。因此，在潜在空间中彼此靠近的值应该对应于非常相似的重建。
- en: Applications of AEs
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动编码器的应用
- en: 'The following are some of the practical applications where AEs may be used:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些自动编码器可能被使用的实际应用：
- en: '**Image coloring**: Given a grayscale image as input, AEs can auto color the
    image and return the colored image as output.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像着色**：给定一个灰度图像作为输入，自动编码器可以自动着色图像，并以彩色图像作为输出。'
- en: '**Noise removal**: Denoising AEs are able to remove noise from images and reconstruct
    images without noise. Tasks such as watermark removal from videos and images can
    be accomplished.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声去除**：去噪自动编码器能够从图像中去除噪声，并在无噪声的情况下重建图像。例如，可以从视频和图像中去除水印的任务也可以完成。'
- en: '**Dimensionality reduction**: AEs represent the input data in a compressed
    form, but with a focus on key features alone. Therefore, things like images can
    be represented with reduced pixels, without much loss of information during image
    reconstruction.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：自动编码器以压缩形式表示输入数据，但只关注关键特征。因此，像图像这样的东西可以用减少的像素表示，在图像重建过程中信息损失不大。'
- en: '**Image search**: This is used to identify similar images based on a given
    input.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像搜索**：这是根据给定的输入识别相似图像。'
- en: '**Information retrieval**: When retrieving information from a corpus, AEs may
    be used to group together all the documents that belong to a given input.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息检索**：在从语料库检索信息时，自动编码器可以用来将属于给定输入的所有文档分组在一起。'
- en: '**Topic modeling**: Variational AEs are used to approximate the posterior distribution,
    and it has become a promising alternative for inferring latent topic distributions
    of text documents.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题建模**：变分自动编码器用于近似后验分布，并且已经成为推断文本文档潜在主题分布的有希望的替代方案。'
- en: We have covered the fundamentals that are needed for us to understand AEs and
    their applications. Let us understand, at a high level, the solution we are going
    to employ using AEs on the credit card fraud detection problem.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了理解自动编码器及其应用所需的基本知识。让我们从高层次上了解我们将要使用自动编码器在信用卡欺诈检测问题上的解决方案。
- en: The credit card fraud dataset
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信用卡欺诈数据集
- en: Generally in a fraud dataset, we have sufficient data for the negative class
    (non-fraud/genuine transactions) and very few or no data for the positive class
    (fraudulent transactions). This is termed a **class imbalance problem** in the ML
    world. We train an AE on the non-fraud data and learn features using the encoder.
    The decoder is then used to compute the reconstruction error on the training set
    to find a threshold. This threshold will be used on the unseen data (test dataset
    or otherwise). We use the threshold to identify those test instances whose values
    are greater than the threshold as fraud instances.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在一个欺诈数据集中，我们对于负类（非欺诈/真实交易）有足够的数据，而对于正类（欺诈交易）则非常少或没有数据。这在机器学习领域被称为**类别不平衡问题**。我们在非欺诈数据上训练一个自动编码器（AE），并使用编码器学习特征。然后使用解码器在训练集上计算重建误差以找到阈值。这个阈值将用于未见过的数据（测试数据集或其他）。我们使用这个阈值来识别那些测试实例中值大于阈值的欺诈实例。
- en: For the project in this chapter, we will be using a dataset that is sourced
    from this URL: [https://essentials.togaware.com/data/](https://essentials.togaware.com/data/).
    This is a public dataset of credit card transactions. This dataset is originally
    made available through the research paper *Calibrating Probability with Undersampling
    for Unbalanced Classification*, A. Dal Pozzolo, O. Caelen, R. A Johnson and G.
    Bontempi, IEEE **Symposium Series on Computational Intelligence** (**SSCI**),
    Cape Town, South Africa, 2015\. The dataset is also available at this URL: [http://www.ulb.ac.be/di/map/adalpozz/data/creditcard.Rdata](http://www.ulb.ac.be/di/map/adalpozz/data/creditcard.Rdata).
    The dataset was collected and analyzed during a research collaboration of Worldline
    and the Machine Learning Group ([http://mlg.ulb.ac.be](http://mlg.ulb.ac.be/))
    of ULB (Université Libre de Bruxelles) on big data mining and fraud detection.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的项目中，我们将使用以下 URL 源的数据集：[https://essentials.togaware.com/data/](https://essentials.togaware.com/data/)。这是一个公开的信用卡交易数据集。该数据集最初通过研究论文
    *Calibrating Probability with Undersampling for Unbalanced Classification*，A.
    Dal Pozzolo, O. Caelen, R. A Johnson 和 G. Bontempi，IEEE **Symposium Series on
    Computational Intelligence** (**SSCI**)，南非开普敦，2015 年提供。数据集也可在此 URL 上找到：[http://www.ulb.ac.be/di/map/adalpozz/data/creditcard.Rdata](http://www.ulb.ac.be/di/map/adalpozz/data/creditcard.Rdata)。该数据集是在
    Worldline 和 ULB（Université Libre de Bruxelles）机器学习组（[http://mlg.ulb.ac.be](http://mlg.ulb.ac.be/)）在大数据挖掘和欺诈检测研究合作期间收集和分析的。
- en: 'The following are the characteristics of the dataset:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是数据集的特征：
- en: The paper made the dataset available as an Rdata file. There is a CSV converted
    version of this dataset available on Kaggle as well as other sites.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文将数据集以 Rdata 文件的形式提供。该数据集的 CSV 转换版本也可在 Kaggle 以及其他网站上找到。
- en: It contains transactions made by credit cards in September 2013 by European
    cardholders.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它包含 2013 年 9 月欧洲持卡人用信用卡进行的交易。
- en: The transactions occurred on two days are recorded and is presented as the dataset.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录并呈现为数据集的是两天内发生的交易。
- en: There are a total of 284,807 transactions in the dataset.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集中总共有 284,807 笔交易。
- en: The dataset suffers from a severe class imbalance problem. Only 0.172% of all
    transactions are fraudulent transactions (492 fraudulent transactions).
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该数据集存在严重的类别不平衡问题。所有交易中只有 0.172% 是欺诈交易（492 笔欺诈交易）。
- en: There are a total thirty features in the dataset, namely `V1`, `V2`, ...,`V28`,
    `Time`, and `Amount`.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集中总共有三十个特征，即 `V1`, `V2`, ..., `V28`, `Time`, 和 `Amount`。
- en: The variables `V1`, `V2`, ...,`V28` are the principal components obtained with
    PCA from the original set of variables.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量 `V1`, `V2`, ..., `V28` 是从原始变量集中通过主成分分析（PCA）获得的。
- en: Due to confidentiality, the original set of variables that yielded the principal
    components are not revealed.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于保密性，产生主成分的原始变量集没有公开。
- en: The `Time` feature contains the seconds elapsed between each transaction and
    the first transaction in the dataset.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Time` 特征包含每个交易与数据集中第一个交易之间的秒数。'
- en: The `Amount` feature is the transaction amount.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Amount` 特征是交易金额。'
- en: The dependent variable is named `Class`. The fraudulent transactions are represented
    as 1 in the class and genuine transactions are represented as 0.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因变量名为 `Class`。欺诈交易在类别中表示为 1，真实交易表示为 0。
- en: We will now jump into using AEs for the credit card fraud detection.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将进入使用自动编码器（AE）进行信用卡欺诈检测。
- en: Building AEs with the H2O library in R
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 R 中使用 H2O 库构建自动编码器（AE）。
- en: We will be using the AE implementation available in H2O for our project. H2O
    is a fully open source, distributed, in-memory ML platform with linear scalability.
    It offers parallelized implementations of some of the most widely used ML algorithms.
    It supports an easy to use, unsupervised, and non-linear AE as part of its deep
    learning model. The DL AE of H2O is based on the multilayer neural net architecture,
    where the entire network is trained together, instead of being stacked layer by
    layer.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 H2O 中可用的自动编码器实现来执行我们的项目。H2O 是一个完全开源、分布式、内存中的机器学习平台，具有线性可扩展性。它提供了一些最广泛使用的机器学习算法的并行化实现。它支持易于使用的、无监督的、非线性的自动编码器，作为其深度学习模型的一部分。H2O
    的深度学习自动编码器基于多层神经网络架构，整个网络一起训练，而不是逐层堆叠。
- en: 'The `h2o` package can be installed in R with the following command:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令在 R 中安装 `h2o` 包：
- en: '[PRE0]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Additional details on the installation and dependencies of H2O in R are available
    at this URL: [https://cran.r-project.org/web/packages/h2o/index.html](https://cran.r-project.org/web/packages/h2o/index.html).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 关于H2O在R中的安装和依赖的详细信息，可在以下URL找到：[https://cran.r-project.org/web/packages/h2o/index.html](https://cran.r-project.org/web/packages/h2o/index.html)。
- en: 'Once the package is installed successfully, the functions offered by the `h2o`
    package, including the AE, can simply be used by including the following line
    in R code:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦成功安装了包，`h2o`包提供的函数，包括AE，可以通过在R代码中包含以下行简单使用：
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is all we need to do prior to coding our credit card fraud detection system
    with the AE. Without waiting any longer, let's start building our code to explore
    and prepare our dataset, as well as to implement the AE that captures fraudulent
    credit card transactions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用自动编码器（AE）编码我们的信用卡欺诈检测系统之前，我们需要做的一切。不再等待，让我们开始构建我们的代码来探索和准备我们的数据集，以及实现捕获欺诈性信用卡交易的AE。
- en: Autoencoder code implementation for credit card fraud detection
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信用卡欺诈检测的自动编码器代码实现
- en: 'As usual, like all other projects, let''s first load the data into an R dataframe
    and then perform EDA to understand the dataset better. Please note the inclusion
    of `h2o` as well as the `doParallel` library in the code. These inclusions enable
    us to use the AE that is part of the `h2o` library, as well as to utilize the
    multiple CPU cores that are present in the laptop/desktop as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如同所有其他项目一样，让我们首先将数据加载到R数据框中，然后执行EDA以更好地了解数据集。请注意，在代码中包含`h2o`以及`doParallel`库。这些包含使我们能够使用`h2o`库中的AE，以及利用笔记本电脑/台式机上的多个CPU核心，如下所示：
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Initializing the H2O cluster in localhost under the port `54321`. The `nthreads`
    defines the number of thread pools to be used, this is close to the number of
    cpus to be used. In our case, we are saying use all CPUs, we are also specifying
    the maximum memory to use by H2O cluster as `8G`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地主机上的端口`54321`初始化H2O集群。`nthreads`定义要使用的线程池数量，这接近要使用的CPU数量。在我们的情况下，我们说的是使用所有CPU，我们还指定H2O集群使用的最大内存为`8G`：
- en: '[PRE3]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You will get a similar output to that shown in the following code block:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到与以下代码块中所示类似的输出：
- en: '[PRE4]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, to set the working directory of the data file location, load Rdata and
    read it into the dataframe, and view the dataframe using the following code:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了设置数据文件位置的当前工作目录，加载Rdata并将其读入数据框，并使用以下代码查看数据框：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The will give the following output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/3e302731-40c7-43f1-9350-8c3cb93e9608.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3e302731-40c7-43f1-9350-8c3cb93e9608.png)'
- en: 'Let''s now print the dataframe structure using the following code:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下代码打印数据框结构：
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will give the following output:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, to view the class distribution, use the following code:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要查看类别分布，请使用以下代码：
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You will get the following output:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到以下输出：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To view the relationship between the `V1` and `Class` variables, use the following
    code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看`V1`和`Class`变量之间的关系，请使用以下代码：
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will give the following output:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/75981835-0fdf-4ffd-807e-6cceab13ec32.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/75981835-0fdf-4ffd-807e-6cceab13ec32.png)'
- en: 'To visualize the distribution of transaction amounts with respect to class,
    use the following code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化交易金额相对于类别的分布，请使用以下代码：
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will give the following output:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/95ac410b-3790-435f-a980-d0f89f27af11.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/95ac410b-3790-435f-a980-d0f89f27af11.png)'
- en: 'To visualize the distribution of transaction times with respect to class, use
    the following code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化交易时间相对于类别的分布，请使用以下代码：
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will give the following output:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/cc9a994e-1e30-440c-9f0e-80c79e7ae4ba.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cc9a994e-1e30-440c-9f0e-80c79e7ae4ba.png)'
- en: 'Use the following code to visualize the `V2` variable with respect to `Class`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码可视化`V2`变量相对于`Class`：
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You will get the following as the output:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到以下输出：
- en: '![](img/1c891040-a5e6-4568-a669-bfa9307f150e.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1c891040-a5e6-4568-a669-bfa9307f150e.png)'
- en: 'Use the following code to visualize `V3` with respect to `Class`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码来可视化`V3`相对于`Class`：
- en: '[PRE14]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following graph is the resultant output:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表是结果输出：
- en: '![](img/084bee5f-c39d-4cd7-a4ef-f3654ca44992.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/084bee5f-c39d-4cd7-a4ef-f3654ca44992.png)'
- en: 'To visualize the `V3` variable with respect to `Class`, use the following code:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化`V3`变量相对于`Class`，请使用以下代码：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following graph is the resultant output:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表是结果输出：
- en: '![](img/312d60a0-e11b-4ec9-ac1a-5b5a2fb77cc3.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/312d60a0-e11b-4ec9-ac1a-5b5a2fb77cc3.png)'
- en: 'Use the following code to visualize the `V6` variable with respect to `Class`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码来可视化`V6`变量相对于`类别`：
- en: '[PRE16]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following graph is the resultant output:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表是结果输出：
- en: '![](img/0ec1dc19-0116-4bae-b1f4-72f520de15f8.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0ec1dc19-0116-4bae-b1f4-72f520de15f8.png)'
- en: 'Use the following code to visualize the `V7` variable with respect to `Class`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码来可视化`V7`变量相对于`类别`：
- en: '[PRE17]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following graph is the resultant output:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表是结果输出：
- en: '![](img/9b97b5a5-1b06-4151-adde-1b85b632f657.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9b97b5a5-1b06-4151-adde-1b85b632f657.png)'
- en: 'Use the following code to visualize the `V8` variable with respect to `Class`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码来可视化`V8`变量相对于`类别`：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following graph is the resultant output:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表是结果输出：
- en: '![](img/7c471bfc-a6ff-4867-96cc-30da7a810d47.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7c471bfc-a6ff-4867-96cc-30da7a810d47.png)'
- en: 'To visualize the `V9` variable with respect to `Class`, use the following code:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化`V9`变量相对于`类别`，请使用以下代码：
- en: '[PRE19]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following graph is the resultant output:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表是结果输出：
- en: '![](img/b82a52f6-7fe5-4983-b56f-9c9d92dfeed7.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b82a52f6-7fe5-4983-b56f-9c9d92dfeed7.png)'
- en: 'To visualize the `V10` variable with respect to `Class`, use the following
    code:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化`V10`变量相对于`类别`，请使用以下代码：
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following graph is the resultant output:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表是结果输出：
- en: '![](img/aeee5f93-f562-4bd6-944f-e2cfb1936b0c.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/aeee5f93-f562-4bd6-944f-e2cfb1936b0c.png)'
- en: 'From all the visualizations related to variables with respect to class, we
    can infer that most of the principal components are centered on `0`. Now, to plot
    the distribution of classes in the data, use the following code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 从与类别相关的变量的所有可视化中，我们可以推断出大多数主成分都集中在`0`上。现在，为了绘制数据中类别的分布，请使用以下代码：
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following bar graph is the resultant output:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下条形图是结果输出：
- en: '![](img/1ab03c79-8eea-4c65-94a7-ba90deedae22.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1ab03c79-8eea-4c65-94a7-ba90deedae22.png)'
- en: 'We observe that the distribution of classes is very imbalanced. The representation
    of the major class (non-fraudulent transactions, represented by `0`) in the dataset
    is too heavy when compared to the minority class (fraudulent transactions: `1`).
    In the traditional supervised ML way of dealing with this kind of problem, we
    would have treated the class imbalance problem with techniques such as **Synthetic
    Minority Over-Sampling Technique** (**SMORT**). However, with AEs, we do not treat
    the class imbalance during data preprocessing; rather, we feed the data as is
    to the AE for learning. In fact, the AE is learning the thresholds and the characteristics
    of the data from the majority class; this is the reason we call it a one-class
    classification problem.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到类别的分布非常不平衡。与少数类（欺诈交易：`1`）相比，数据集中代表主要类（非欺诈交易，表示为`0`）的表示过于密集。在传统的监督机器学习处理这类问题时，我们会用如**合成少数类过采样技术**（**SMORT**）等技术来处理类别不平衡问题。然而，在使用自编码器（AE）时，我们不在数据预处理阶段处理类别不平衡；相反，我们将数据原样输入到AE中进行学习。实际上，AE正在从多数类学习数据的阈值和特征；这就是我们称之为单类分类问题的原因。
- en: 'We will need to do some feature engineering prior to training our AE. Let''s
    first focus on the `Time` variable in the data. Currently, it is in the seconds
    format, but we may better represent it as days. Run the following code to see
    the current form of time in the dataset:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练我们的自编码器（AE）之前，我们需要做一些特征工程。让我们首先关注数据中的`时间`变量。目前，它是以秒为单位的格式，但我们可以更好地将其表示为天数。运行以下代码以查看数据集中时间的当前形式：
- en: '[PRE22]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You will get the following output:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到以下输出：
- en: '[PRE23]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We know that there are 86,400 seconds in a given day (60 seconds per minute
    * 60 minutes per hour * 24 hours per day). We will convert the `Time` variable
    into `Day` by considering the value in `Time` and representing it as `day1` if
    the number of seconds is less than or equal to 86,400, and anything over 86,400
    becomes `day2.` There are only two days possible, as we can see from the summary
    that the maximum value represented by the time variable is `172792` seconds:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，给定的一天中有86,400秒（每分钟60秒，每小时60分钟，每天24小时）。我们将`时间`变量转换为`天数`，考虑到`时间`中的值，如果秒数小于或等于86,400，则表示为`day1`，超过86,400的任何值变为`day2`。由于从摘要中我们可以看到，时间变量的最大值表示为`172792`秒，因此只有两种可能的天数：
- en: '[PRE24]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following is the resultant output of the first six rows after the conversion:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是转换后的前六行的结果输出：
- en: '![](img/a810fdad-2a9b-4045-a5ba-92635cd881ac.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a810fdad-2a9b-4045-a5ba-92635cd881ac.png)'
- en: 'Now, use the following code to view the last six rows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用以下代码来查看最后六行：
- en: '[PRE25]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following is the resultant output of the last six rows after the conversion:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的最后六行的结果是以下内容：
- en: '![](img/5506a84a-a119-421a-be3c-1284e371c32a.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5506a84a-a119-421a-be3c-1284e371c32a.png)'
- en: 'Now, let''s print the distribution of transactions by the day in which the
    transaction falls, using the following code:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下代码打印交易按交易发生的日期的分布：
- en: '[PRE26]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You will get the following as the output:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到以下输出：
- en: '[PRE27]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s create a new variable, `Time_day`, based on the seconds represented
    in the `Time` variable, and summarize the `Time_day` variable with respect to
    `Day` using the following code:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们根据`Time`变量中的秒数创建一个新的变量`Time_day`，并使用以下代码根据`Day`对`Time_day`变量进行汇总：
- en: '[PRE28]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We get the following as the resultant output:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下结果输出：
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Use the following code the convert all character variables in the dataset to
    factors:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码将数据集中的所有字符变量转换为因子：
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can further fine-tune the `Time_day` variable by converting the variable
    into a factor. The factors represents the time of day at which the transaction
    happened, for example, `morning`, `afternoon`, `evening`, and `night`. We can
    create a new variable called `Time_Group`, based on the various buckets of the
    day, using the following code:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将变量转换为因子来进一步微调`Time_day`变量。因子代表交易发生的时间，例如，“上午”、“下午”、“傍晚”和“夜间”。我们可以使用以下代码创建一个名为`Time_Group`的新变量，基于一天中的不同时间段：
- en: '[PRE31]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following is the resultant output of the first six rows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的前六行的结果是以下内容：
- en: '![](img/26eb4e24-39aa-4069-8b8b-b62b16d185af.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/26eb4e24-39aa-4069-8b8b-b62b16d185af.png)'
- en: 'Use the following code to view and confirm the last six rows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码查看并确认最后六行：
- en: '[PRE32]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This will give the following output, and we see that we have successfully converted
    the data which represent the various time of the day:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出，我们看到我们已经成功地将表示一天中各种时间的交易数据转换过来：
- en: '![](img/4bf537f3-165f-47c8-a6b2-b04cd963ba4e.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4bf537f3-165f-47c8-a6b2-b04cd963ba4e.png)'
- en: 'Take a look at the following code:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 看看以下代码：
- en: '[PRE33]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding code will generate the following output:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将生成以下输出：
- en: '![](img/8c44ad84-e7fe-4871-a9b6-b0a84d54b8d1.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8c44ad84-e7fe-4871-a9b6-b0a84d54b8d1.png)'
- en: We can infer from the visualization that there is no difference in the count
    of transactions that happened on day 1 and day 2\. Both remain close to 150,000
    transactions.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 从可视化中我们可以推断，第一天和第二天发生的交易数量没有差异。两者都接近15万笔交易。
- en: 'Now we will convert the `Class` variable as a factor and then visualize the
    data by `Time_Group` variable using the following code:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将`Class`变量转换为因子，然后使用以下代码通过`Time_Group`变量可视化数据：
- en: '[PRE34]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This will generate the following output:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![](img/e186689d-e77c-4cde-b52d-5ed9c1d5707c.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e186689d-e77c-4cde-b52d-5ed9c1d5707c.png)'
- en: The inference obtained from this visualization is that the number of non-fraudulent
    transactions remains almost the same across all time periods of the day, whereas
    we see a huge rise in the number of fraudulent transactions during the morning
    `Time` group.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个可视化中得到的推断是，非欺诈交易的数量在一天中的所有时间段几乎保持不变，而我们在上午`Time`组中看到欺诈交易数量的巨大增长。
- en: 'Let''s do a last bit of exploration of the transaction amount with respect
    to class:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对交易金额与类别进行最后的探索：
- en: '[PRE35]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The preceding code will generate the following output:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将生成以下输出：
- en: '[PRE36]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: One interesting insight from the summary is that the mean amount in fraudulent
    transactions is higher compared to genuine transactions. However, the maximum
    transaction amount that we see in fraudulent transactions is much lower than the
    genuine transactions. It can also be seen that genuine transactions have a higher
    median amount.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 从摘要中得出的一个有趣见解是，欺诈交易的平均金额比真实交易要高。然而，我们在欺诈交易中看到的最大交易金额远低于真实交易。还可以看出，真实交易的中位数金额更高。
- en: 'Now, let''s convert our R dataframe to an H2O dataframe to apply the AE to
    it. This is a requirement in order to use the functions from the `h2o` library:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将我们的R数据框转换为H2O数据框，以便对其应用AE。这是使用`h2o`库中的函数的一个要求：
- en: '[PRE37]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The `tanh` activation function is a rescaled and shifted logistic function.
    Other functions, such as ReLu and Maxout, are also provided by the `h2o` library
    and they can also be used.  In the first AE model, let's use the `tanh` activation
    function. This choice is arbitrary and other activation functions may also be
    tried as desired.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`tanh`激活函数是一个缩放和移动的对数函数。`h2o`库还提供了其他函数，如ReLu和Maxout，也可以使用。在第一个AE模型中，让我们使用`tanh`激活函数。这个选择是任意的，也可以根据需要尝试其他激活函数。'
- en: 'The `h2o.deeplearning` function has a parameter AE and this should be set to
    `TRUE` to train a AE model. Let''s build our AE model now:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`h2o.deeplearning`函数有一个参数AE，应该将其设置为`TRUE`以训练一个AE模型。现在让我们构建我们的AE模型：'
- en: '[PRE38]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding code generates the following output:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成以下输出：
- en: '[PRE39]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We will save the model so we do not have to retrain t again and again. Then
    load the model that is persisted on the disk and print the model to verify the
    AE learning using the following code:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保存模型，这样我们就不必一次又一次地重新训练。然后加载磁盘上持久化的模型，并使用以下代码打印模型以验证AE学习：
- en: '[PRE40]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This will generate the following output:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE41]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We will now make predictions on test dataset using the AE model that is built,
    using the following code:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用构建的AE模型在测试数据集上进行预测，使用以下代码：
- en: '[PRE42]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This will generate the following output:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE43]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'It is possible to visualize the encoder representing the data in a conscious
    manner in the inner layers through the `h2o.deepfeatures` function. Let''s try
    visualizing the reduced data in a second layer:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`h2o.deepfeatures`函数，我们可以在内部层以有意识的方式可视化表示数据的编码器。让我们尝试可视化第二层中的降维数据：
- en: '[PRE44]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The preceding code will generate the following output:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将生成以下输出：
- en: '[PRE45]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let us now plot the data of `DF.L2.C1` with respect to `DF.L2.C2` to verify
    if the encoder has detected the fraudulent transactions, using the following code:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用以下代码绘制`DF.L2.C1`相对于`DF.L2.C2`的数据，以验证编码器是否检测到了欺诈交易：
- en: '[PRE46]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This will generate the following output:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![](img/1d57f16e-1bf6-47eb-94e4-83557dbb69eb.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d57f16e-1bf6-47eb-94e4-83557dbb69eb.png)'
- en: 'Again we plot the data of `DF.L2.C3` with respect to `DF.L2.C4` to verify the
    if the encoder have detected any fraud transaction, using the following code:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们绘制`DF.L2.C3`相对于`DF.L2.C4`的数据，以验证编码器是否检测到了任何欺诈交易，使用以下代码：
- en: '[PRE47]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The preceding code will generate the following output:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将生成以下输出：
- en: '![](img/3bcd6f63-b386-415a-a650-87cf67b96e22.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3bcd6f63-b386-415a-a650-87cf67b96e22.png)'
- en: 'We see from the two visualizations that the fraudulent transactions are indeed
    detected by the dimensionality reduction approach with our AE model. Those few
    scattered dots (represented by `1`) depicts the fraud transactions that are detected.
    We can also train a new model with the other hidden layers, using our first model.
    This results in 10 columns, since the third layer has 10 nodes. We are just attempting
    to slice out one layer where some level of reduction was done and use that to
    build a new model:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 从两个可视化图中我们可以看出，通过我们的AE模型进行降维处理确实检测到了欺诈交易。那些散布的点（用`1`表示）描绘了被检测到的欺诈交易。我们也可以使用我们的第一个模型来训练一个新的模型，使用其他隐藏层。这会产生10列，因为第三层有10个节点。我们只是在尝试切出一层，在这一层中已经进行了一定程度的降维，并使用它来构建一个新的模型：
- en: '[PRE48]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The preceding code will generate the following output:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将生成以下输出：
- en: '[PRE49]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'As we can see, the training models and data are successfully created. We will
    now go ahead and train the new model, save it and the print it. First, we will
    get the feature names from the sliced encoder layer:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，训练模型和数据已成功创建。我们现在将继续训练新的模型，保存它并打印它。首先，我们将从切片编码器层获取特征名称：
- en: '[PRE50]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Then we will training a new model:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将训练一个新的模型：
- en: '[PRE51]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We will then save the model to avoid retraining again, then retrieve the model
    and print it using the following code:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将保存模型以避免再次重新训练，然后使用以下代码检索模型并打印它：
- en: '[PRE52]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This will generate the following output:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE53]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'For measuring model performance on test data, we need to convert the test data
    to the same reduced dimensions as the training data:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测量模型在测试数据上的性能，我们需要将测试数据转换为与训练数据相同的降维维度：
- en: '[PRE54]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The preceding code will generate the following output:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将生成以下输出：
- en: '[PRE55]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We see, the data has been converted successfully. Now, to make predictions
    on the test dataset with `model_two`, we will use the following code:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，数据已经成功转换。现在，为了使用`model_two`在测试数据集上进行预测，我们将使用以下代码：
- en: '[PRE56]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'This will generate the following output:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE57]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'As we can see, from the output, predictions has been successfully completed
    and now let us visualize the predictions using the following code:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，从输出中，预测已成功完成，现在让我们使用以下代码可视化预测结果：
- en: '[PRE58]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This will generate the following output:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE59]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We see that our AE is able to correctly predict non-fraudulent transactions
    with 98% accuracy, which is good. However, it is yielding only 58% accuracy when
    predicting fraudulent transactions. This is definitely something to focus on.
    Our model needs some improvement, and this can be accomplished through the following
    options:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们的AE能够以98%的准确率正确预测非欺诈交易，这是好的。然而，在预测欺诈交易时，它只产生了58%的准确率。这确实是一个需要关注的问题。我们的模型需要一些改进，这可以通过以下选项实现：
- en: Using other layers' latent space representations as input to build `model_two`
    (recollect that we currently use the layer 3 representation)
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用其他层的潜在空间表示作为输入来构建`model_two`（记住我们目前使用的是第3层的表示）
- en: Using ReLu or Maxout activation functions instead of `Tanh`
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ReLu或Maxout激活函数而不是`Tanh`
- en: Checking the misclassified instances through the `h2o.anomaly` function and
    increasing or decreasing the cutoff threshold MSE values, which separates the
    fraudulent transactions from non-fraudulent transactions
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`h2o.anomaly`函数检查被错误分类的实例，并增加或减少截止阈值MSE值，这些值将欺诈交易与非欺诈交易区分开来
- en: Trying out a more complex architecture in the encoder and decoder
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在编码器和解码器中尝试更复杂的架构
- en: We are not going to be attempting these options in this chapter as they are
    experimental in nature. However, interested readers may try and improve the accuracy
    of the model by trying these options.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中不会尝试这些选项，因为它们具有实验性质。然而，感兴趣的读者可以尝试这些选项来提高模型的准确性。
- en: 'Finally, one best practice is to explicitly shut down the `h2o` cluster. This
    can be accomplished with the following command:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个最佳实践是明确关闭`h2o`集群。这可以通过以下命令完成：
- en: '[PRE60]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Summary
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about an unsupervised deep learning technique called
    AEs. We covered the definition, working principle, types, and applications of
    AEs. H2O, an open source library that enables us to create deep learning models,
    including AEs, was explored. We then discussed a credit card fraud open dataset
    and implemented a project with an AE to detect fraudulent credit card transactions.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了一种无监督的深度学习技术，称为AEs。我们涵盖了AEs的定义、工作原理、类型和应用。我们探讨了H2O，这是一个开源库，使我们能够创建深度学习模型，包括AEs。然后我们讨论了一个信用卡欺诈公开数据集，并使用AE实现了一个项目来检测欺诈信用卡交易。
- en: Can deep neural networks help with creative tasks such as prose generation,
    story writing, caption generation for images, and poem writing? Not sure?! Let's
    explore RNNs, in the next chapter, a special type of deep neural network that
    enables us to accomplish creative tasks. Turn the page to explore the world of
    RNNs for prose generation.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络能否帮助完成创意任务，如散文生成、故事写作、图像标题生成和诗歌创作？不确定吗？！让我们在下一章探索RNNs，这是一种特殊的深度神经网络，使我们能够完成创意任务。翻到下一页，探索用于散文生成的RNNs世界。
