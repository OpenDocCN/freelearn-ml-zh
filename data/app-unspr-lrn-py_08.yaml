- en: '*Chapter 8*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章*'
- en: Market Basket Analysis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 市场篮子分析
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Work with transaction-level data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交易级别的数据
- en: Use market basket analysis in the appropriate context
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在合适的情境中使用市场篮子分析
- en: Run the Apriori algorithm and build association rules
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行Apriori算法并构建关联规则
- en: Perform basic visualizations on association rules
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对关联规则进行基本的可视化
- en: Interpret the key metrics of market basket analysis
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解读市场篮子分析的关键指标
- en: In this chapter, we will explore a foundational and reliable algorithm for analyzing
    transaction data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨一种基础且可靠的算法，用于分析交易数据。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: In this chapter, we are going to change direction entirely. The previous chapter,
    which explored topic models, focused on natural language processing, text data,
    and applying relatively recently developed algorithms. Most data science practitioners
    would agree that natural language processing, including topic models, is toward
    the cutting edge of data science and is an active research area. We now understand
    that topic models can, and should, be leveraged wherever text data could potentially
    drive insights or growth, including in social media analysis, recommendation engines,
    and news filtering.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将完全改变方向。前一章探讨了主题模型，重点是自然语言处理、文本数据和应用相对较新开发的算法。大多数数据科学从业者会同意，自然语言处理（包括主题模型）处于数据科学的前沿，是一个活跃的研究领域。我们现在已经明白，主题模型可以并且应该在任何文本数据可能带来洞察或增长的地方应用，包括社交媒体分析、推荐引擎和新闻过滤。
- en: This chapter takes us into the retail space to explore a foundational and reliable
    algorithm for analyzing transaction data. While this algorithm might not be on
    the cutting edge or in the catalog of the most popular machine learning algorithms,
    it is ubiquitous and undeniably impactful in the retail space. The insights it
    drives are easily interpretable, immediately actionable, and instructive for determining
    analytical next steps. If you work in the retail space or with transaction data,
    you would be well-served to dive deep into market basket analysis.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将带我们进入零售领域，探索一种基础且可靠的算法，用于分析交易数据。虽然这个算法可能不是最前沿的，也不是最流行的机器学习算法之一，但它在零售领域无处不在，并且其影响力不可否认。它带来的洞察易于解读，立刻可以采取行动，并且对确定分析的下一步非常有帮助。如果你从事零售或交易数据分析工作，那么深入了解市场篮子分析将对你非常有益。
- en: Market Basket Analysis
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 市场篮子分析
- en: 'Imagine you work for a retailer that sells dozens of products and your boss
    comes to you and asks the following questions:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你为一家零售商工作，卖着几十种产品，你的老板走过来，问你以下问题：
- en: What products are purchased together most frequently?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些产品最常一起购买？
- en: How should the products be organized and positioned in the store?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品应该如何在商店中组织和定位？
- en: How do we identify the best products to discount via coupons?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何识别最适合通过优惠券打折的产品？
- en: You might reasonably respond with complete bewilderment, as those questions
    are very diverse and do not immediately seem answerable using a single algorithm
    and dataset. However, the answer to all those questions and many more is **market
    basket analysis**. The general idea behind market basket analysis is to identify
    and quantify which items, or groups of items, are purchased together frequently
    enough to drive insight into customer behavior and product relationships.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会感到完全困惑，因为这些问题非常广泛，而且似乎无法通过单一的算法和数据集直接回答。然而，所有这些问题以及更多问题的答案就是**市场篮子分析**。市场篮子分析背后的基本理念是识别并量化哪些商品或商品组经常一起购买，从而为顾客行为和产品关系提供洞察。
- en: Before we dive into the analytics, it is worth defining the term market basket.
    A market basket is a permanent set of products in an economic system. In this
    case, permanent does not necessarily mean permanent in the traditional sense.
    It means that until such time as the product is taken out of the catalog, it will
    consistently be available for purchase. The product referenced in the preceding
    definition is any good, service, or element of a group, including a bicycle, having
    your house painted, or a website. Lastly, an economic system could be a company,
    a collection of activities, or a country. The easiest example of a market basket
    is a grocery store, which is a system made up of a collection of food and drink
    items.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入分析之前，值得定义一下“市场篮子”这个术语。市场篮子是一个经济系统中永久存在的商品集合。在这里，永久并不一定意味着传统意义上的永久。它意味着，直到该商品被从目录中移除之前，它将始终可以购买。前述定义中的商品指的是任何商品、服务或一个群体的组成部分，包括自行车、给房子刷漆或一个网站。最后，经济系统可以是一个公司、一系列活动或一个国家。市场篮子的最简单例子是杂货店，它是由一系列食品和饮料商品组成的系统。
- en: '![Figure 8.1: An example market basket where the economic system is the butcher
    shop and the permanent set of items is all the meat products offered by the butcher'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.1：一个示例市场篮子，其中经济系统是肉店，永久集合的商品是肉店提供的所有肉类产品'
- en: '](img/C12626_08_01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_08_01.jpg)'
- en: 'Figure 8.1: An example market basket where the economic system is the butcher
    shop and the permanent set of items is all the meat products offered by the butcher'
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.1：一个示例市场篮子，其中经济系统是肉店，永久集合的商品是肉店提供的所有肉类产品
- en: Even without using any models or analyses, certain product relationships are
    obvious. Let's take the relationship between meat and vegetables. Typically, market
    basket analysis models return relationships more specific than meat and vegetables,
    but, for argument's sake, we will generalize to meat and vegetables. Okay, there
    is a relationship between meat and vegetables. So what? Well, we know these are
    staple items that are frequently purchased together. We can leverage this information
    by putting the vegetables and meats on opposite sides of the store, which you
    will notice is often the positioning of those two items, forcing customers to
    walk the full distance of the store, and thereby increasing the likelihood that
    they will buy additional items that they might not have bought if they did not
    have to traverse the whole store.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 即使不使用任何模型或分析，某些产品之间的关系也是显而易见的。让我们以肉类和蔬菜的关系为例。通常，市场篮子分析模型会返回比“肉类和蔬菜”更具体的关系，但为了讨论的方便，我们将其概括为“肉类和蔬菜”。好吧，肉类和蔬菜之间确实存在关系。那么呢？我们知道这些是常见的食材，通常会一起购买。我们可以利用这一信息，将蔬菜和肉类分别摆放在商店的两端，你会发现这两样商品常常被摆放在商店的对立面，这迫使顾客走完整个商店，从而增加他们购买额外商品的可能性，这些商品如果顾客不需要走遍整个商店，可能就不会买。
- en: 'One of the things retail companies struggle with is how to discount items effectively.
    Let''s consider another obvious relationship: peanut butter and jelly. In the
    United States, peanut butter and jelly sandwiches are incredibly popular, especially
    among children. When peanut butter is in a shopping basket, the chance jelly is
    also there can be assumed to be quite high. Since we know peanut butter and jelly
    are purchased together, it does not make sense to discount them both. If we want
    customers to buy both items, we can just discount one of the items, knowing that
    if we can get the customers to buy the discounted item, they will probably buy
    the other item too, even if it is full price.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 零售公司面临的一个难题是如何有效地打折商品。让我们考虑另一个显而易见的关系：花生酱和果冻。在美国，花生酱和果冻三明治非常受欢迎，尤其是在孩子们中间。当购物篮里有花生酱时，可以假设果冻也很可能在其中。既然我们知道花生酱和果冻是一起购买的，那么同时对两者打折就没有意义。如果我们希望顾客购买这两件商品，我们只需打折其中一件商品，知道如果我们能让顾客购买打折商品，他们很可能也会购买另一件商品，即使它是原价。
- en: '![Figure 8.2: A visualization of market basket analysis'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.2：市场篮子分析的可视化'
- en: '](img/C12626_08_02.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_08_02.jpg)'
- en: 'Figure 8.2: A visualization of market basket analysis'
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.2：市场篮子分析的可视化
- en: Just like the topic models in the previous chapter, market basket analysis is
    all about identifying frequently occurring groups. Here, we are looking for frequently
    occurring groups of products, whereas in topic models, we were looking for frequently
    occurring groups of words. Thus, as it was to topic models, the word clustering
    could be applied to market basket analysis. The major differences are that the
    clusters in market basket analysis are micro, only a few products per cluster,
    and the order of the items in the cluster matters when it comes to computing probabilistic
    metrics. We will dive much deeper into these metrics and how they are calculated
    later in this chapter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 就像前一章中的主题模型一样，市场篮子分析的关键是识别频繁出现的组。在这里，我们寻找的是频繁出现的产品组，而在主题模型中，我们寻找的是频繁出现的词组。因此，正如它可以应用于主题模型一样，词汇聚类也可以应用于市场篮子分析。主要的不同之处在于，市场篮子分析中的聚类是微观的，每个聚类只有少数几个产品，并且在计算概率指标时，聚类中项目的顺序是至关重要的。我们将在本章后面深入探讨这些指标以及它们是如何计算的。
- en: What has clearly been implied by the previous two examples is that, in market
    basket analysis, retailers can discover the relationships – obvious and surprising
    – between the products that customers buy. Once uncovered, the relationships can
    be used to inform and improve the decision-making process. A great aspect of market
    basket analysis is that while this analysis was developed in relation to, discussed
    in terms of, and mostly applied to the retail world, it can be applied to many
    diverse types of businesses.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面两个例子中可以明显看出，在市场篮子分析中，零售商能够发现顾客购买的产品之间的关系——这些关系有时显而易见，有时又出乎意料。一旦这些关系被揭示出来，就可以用来指导和改善决策过程。市场篮子分析的一个重要特点是，尽管这种分析最初是在零售领域开发、讨论并应用的，但它同样可以应用于许多不同类型的企业。
- en: The only requirement for performing this type of analysis is that the data is
    a list of collections of items. In the retail case, this would be a list of transactions
    where each transaction is a group of purchased products. One example of an alternative
    application is analyzing website traffic. With website traffic, we consider the
    products to be websites, so each element of the list is the collection of websites
    visited by an individual over a specified time period. Needless to say, the applications
    of market basket analysis extend well beyond the principal retail application.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这种分析的唯一要求是数据必须是一个项集合的列表。在零售案例中，这通常是一个包含多次交易的列表，每次交易中包含一组已购买的产品。另一个替代应用的例子是分析网站流量。对于网站流量，我们把网站视作产品，所以列表中的每个元素就是某个个体在特定时间段内访问的所有网站集合。不用说，市场篮子分析的应用远远超出了零售领域的主应用。
- en: Use Cases
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用例
- en: 'There are three principal use cases in the traditional retail application:
    pricing enhancement, coupon and discount recommendation, and store layout. As
    was briefly mentioned previously, by using the product associations uncovered
    by the model, retailers can strategically place products in their stores to get
    customers to buy more items and thus spend more money. If any relationship between
    two or more products is sufficiently strong, meaning the product grouping occurs
    often in the dataset and the individual products in the grouping appear separate
    from the group infrequently, then the products could be placed far away from one
    another in the store without significantly jeopardizing the odds of the customer
    purchasing both products. By forcing the customer to traverse the whole store
    to get both products, the retailer increases the chances that the customer will
    notice and purchase additional products. Likewise, retailers can increase the
    chances of customers purchasing two weakly related or non-staple products by placing
    the two items next to each other. Obviously, there are a lot of factors that drive
    store layout, but market basket analysis is definitely one of those factors:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统零售应用中，有三个主要的用例：定价优化、优惠券和折扣推荐以及商店布局。如前所述，零售商可以利用模型揭示的产品关联，策略性地在商店内摆放商品，从而促使顾客购买更多商品，并因此花费更多的钱。如果两个或更多产品之间的关系足够强大——即该产品组合在数据集中出现的频率很高，并且组合中的单个产品在其他时候很少单独出现——那么这些产品就可以放在商店的远离彼此的地方，而不会显著影响顾客购买这两种产品的几率。通过迫使顾客走遍整个商店去购买这两种产品，零售商增加了顾客注意到并购买其他商品的机会。同样，零售商也可以通过将两种相关性较弱或非基础性产品放在一起，提高顾客购买这两种商品的几率。显然，商店布局受许多因素的影响，但市场篮子分析无疑是其中一个重要因素：
- en: '![Figure 8.3: How product associations can help inform efficient and lucrative
    store layouts'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.3：产品关联如何帮助设计高效且有利可图的商店布局'
- en: '](img/C12626_08_03.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_08_03.jpg)'
- en: 'Figure 8.3: How product associations can help inform efficient and lucrative
    store layouts'
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.3：产品关联如何帮助设计高效且有利可图的商店布局
- en: Pricing enhancement and coupon and discount recommendation are two sides of
    the same coin. They can simply be interpreted as where to raise and where to lower
    prices. Consider the case of two strongly related items. These two items are most
    likely going to be purchased in the same transaction, so one way to increase the
    profitability of that transaction would be to increase the price of one of the
    items. If the association between the two items is sufficiently strong, the price
    increase can be made with little to no risk of the customer not purchasing both
    items. In a similar way, retailers can encourage customers to purchase an item
    weakly associated with another through discounting or couponing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 定价提升和优惠券及折扣推荐是同一个问题的两个方面。可以简单地理解为在哪里提高价格，在哪里降低价格。考虑两个强相关商品的情况。这两件商品很可能会在同一笔交易中被购买，因此提高其中一件商品的价格是增加交易利润的一种方式。如果这两件商品之间的关联足够强，价格的提高几乎不会影响客户购买这两件商品的可能性。以类似的方式，零售商可以通过折扣或优惠券促使客户购买与其他商品有弱关联的商品。
- en: For example, retailers could compare the purchase history of individual customers
    with the results of market basket analysis done on all transactions and find where
    some of the items certain customers are purchasing are weakly associated to items
    those customers are not currently purchasing. Using this comparison, retailers
    could offer discounts to the customers for the as-yet-unpurchased items the model
    suggested were related to the items previously purchased by those customers. If
    you have ever had coupons print out with your receipt at the end of a transaction,
    the chances are high that those items were found to be related to the items involved
    in your just-completed transaction.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，零售商可以将单个客户的购买历史与所有交易的市场篮分析结果进行比较，找出某些客户购买的商品与他们未购买的商品之间的弱关联。通过这个比较，零售商可以为这些客户提供折扣，推荐模型认为与他们之前购买的商品相关的尚未购买的商品。如果你曾在交易结束时收到打印出来的优惠券，极有可能这些商品与刚刚完成的交易中的商品是相关的。
- en: A non-traditional, but viable, use of market basket analysis would be to enhance
    online advertising and search engine optimization. Imagine we had access to lists
    of websites visited by individuals. Using market basket analysis, we could find
    relationships between websites and use those relationships to both strategically
    order and group the websites resulting from a search engine query. In many ways,
    this is similar to the store layout use case.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 市场篮分析的一个非传统但可行的应用是增强在线广告和搜索引擎优化。假设我们可以访问个人访问的网站列表。利用市场篮分析，我们可以找到网站之间的关系，并利用这些关系来策略性地排序和分组搜索引擎查询结果中的网站。在很多方面，这与商店布局的应用场景类似。
- en: With a general sense of what market basket analysis is all about and a clear
    understanding of its use cases, let's dig into the data used in these models.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对市场篮分析的总体了解和对其应用场景的清晰认识，我们现在可以深入研究这些模型中使用的数据。
- en: Important Probabilistic Metrics
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重要的概率度量
- en: Market basket analysis is built upon the computation of several probabilistic
    metrics. The five major metrics covered here are support, confidence, lift, leverage,
    and conviction. Before digging into transaction data and the specific market basket
    analysis models, including the **Apriori algorithm** and **association rules**,
    we should spend some time defining and exploring these metrics using a small,
    made-up set of transactions. We start by making up some data to use.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 市场篮分析是建立在几个概率度量的计算基础上的。这里讨论的五个主要度量是支持度、置信度、提升度、杠杆度和确信度。在深入研究交易数据和具体的市场篮分析模型（包括**Apriori算法**和**关联规则**）之前，我们应该花些时间定义并探讨这些度量，使用一个小的、虚构的交易数据集来说明。我们从编造一些数据开始。
- en: 'Exercise 39: Creating Sample Transaction Data'
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 39：创建样本交易数据
- en: Since this is the first exercise of the chapter, let's set the environment.
    This chapter will use the same environment requirements that were used in *Chapter
    7*, *Topic Modeling*. If any of the packages do not load, as happened in the previous
    chapter, use `pip` to install them via the command line. One of the libraries
    we will use is `mlxtend`, which may be unfamiliar to you. It is a machine learning
    extensions library that contains useful supplemental tools, including ensembling,
    stacking, and, of book, market basket analysis models. This exercise does not
    have any real output. We will simply create a sample transaction dataset for use
    in subsequent exercises.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是本章的第一个练习，让我们设置环境。本章将使用与*第7章*、*主题建模*中相同的环境要求。如果任何一个包没有加载，就像前一章那样，使用`pip`通过命令行安装它们。我们将使用的一个库是`mlxtend`，它可能对你来说比较陌生。它是一个机器学习扩展库，包含了许多有用的辅助工具，包括集成、堆叠和市场篮分析模型。本次练习没有实际输出，我们将简单地创建一个示例交易数据集，用于后续的练习。
- en: Open a Jupyter notebook with Python 3.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个使用Python 3的Jupyter笔记本。
- en: 'Install the following libraries: `matplotlib.pyplot`, which is used to plot
    the results of the models, `mlxtend.frequent_patterns`, which is used to run the
    models, `mlxtend.preprocessing`, which is used to encode and prep the data for
    the models, `numpy`, which is used to work with arrays, and `pandas`, which is
    used to work with DataFrames:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装以下库：`matplotlib.pyplot`，用于绘制模型的结果，`mlxtend.frequent_patterns`，用于运行模型，`mlxtend.preprocessing`，用于对数据进行编码和准备，以适应模型，`numpy`，用于处理数组，`pandas`，用于处理数据框：
- en: Note
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE0]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create 10 fake transactions featuring grocery store items. The data will take
    the form of a list of lists, a data structure that will be relevant later when
    discussing formatting transaction data for the models:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建10个虚拟交易，内容为杂货店项目。这些数据将以列表的形式出现，这种数据结构在后面讨论格式化交易数据以适应模型时会非常有用：
- en: '[PRE1]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This simple dataset will make explaining and interpreting the probabilistic
    metrics much easier.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的数据集将使得解释和理解概率度量变得更加容易。
- en: Support
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持度
- en: '**Support** is simply the probability that the item set appears in the data,
    which can be calculated by counting the number of transactions in which the item
    set appears and dividing that count by the total number of transactions. Note
    that an item set can be a single item or a group of items. Support is an important
    metric, despite being very simple, as it is one of the primary metrics used to
    determine the believability and strength of association between groups of items.
    For example, it is possible to have two items that only occur with each other,
    suggesting that their association is very strong, but in a dataset containing
    100 transactions, only appearing twice is not very impressive. Because the item
    set appears in only 2% of the transactions, and 2% is small in terms of the raw
    number of appearances, the association cannot be considered significant and, thus,
    is probably unusable in decision making.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持度**简单来说就是项目集在数据中出现的概率，可以通过计算项目集出现的交易次数并将该次数除以总交易数来得到。需要注意的是，项目集可以是一个单独的项目，也可以是一组项目。尽管支持度非常简单，但它是一个重要的度量指标，因为它是用于确定项目集之间关联的可信度和强度的主要指标之一。例如，可能有两个项目只在彼此之间出现，表明它们的关联非常强，但在一个包含100个交易的数据集中，只有两次出现并不令人印象深刻。因为该项目集只在2%的交易中出现，而2%在原始出现次数中算是很小的，因此该关联不能被视为显著，因此在决策中可能无法使用。'
- en: Note that since support is a probability, it will fall in the range [0,1]. The
    formula takes the following form if the item set is two items, X and Y, and N
    is the total number of transactions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于支持度是一个概率值，它的范围将在[0,1]之间。如果项目集包含两个项目，X和Y，且N为总交易数，则公式如下所示。
- en: '![Figure 8.4: Formula for support'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.4：支持度公式'
- en: '](img/C12626_08_04.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_08_04.jpg)'
- en: 'Figure 8.4: Formula for support'
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.4：支持度公式
- en: Let's return momentarily to the made-up data from *Exercise 39*, *Creating Sample
    Transaction Data* and define an item set as being milk and bread. We can easily
    look through the 10 transactions and count the number of transactions in which
    this milk and bread item set occurs – that would be 4 times. Given that there
    are 10 transactions, the support of milk and bread is 4 divided by 10, or 0.4\.
    Whether this is large enough support depends on the dataset itself, which we will
    get into in a later section.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时回到*练习39*中制作的数据，*创建样本交易数据*，并将商品集定义为牛奶和面包。我们可以轻松地查看这10个交易，并统计牛奶和面包商品集出现的次数——这是4次。鉴于总共有10个交易，牛奶和面包的支持度是4除以10，即0.4。是否足够大，这取决于数据集本身，我们将在后续部分进行讨论。
- en: Confidence
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 置信度
- en: 'The **confidence** metric can be thought of in terms of conditional probability,
    as it is basically the probability that product B is purchased given the purchase
    of product A. Confidence is typically notated as A ![](img/C12626_Formula_08_01.png)
    B, and expressed as the proportion of transactions containing A that also contain
    B. Hence, confidence is found by filtering the full set of transactions down to
    those containing A, and then computing the proportion of those transactions that
    contain B. Like support, confidence is a probability, so its range is [0,1]. Using
    the same variable definitions from the support section, the following is the formula
    for confidence:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**置信度**度量可以通过条件概率来理解，它基本上是指在购买了产品A的前提下，购买产品B的概率。置信度通常表示为A ![](img/C12626_Formula_08_01.png)
    B，并表达为包含A的交易中同时包含B的比例。因此，置信度是通过将交易全集筛选为包含A的交易，然后计算这些交易中包含B的比例来得出的。与支持度类似，置信度是一个概率值，因此其范围是[0,1]。使用与支持度部分相同的变量定义，以下是置信度的公式：'
- en: '![Figure 8.5: Formula for confidence'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.5：置信度公式'
- en: '](img/C12626_08_05.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_08_05.jpg)'
- en: 'Figure 8.5: Formula for confidence'
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.5：置信度公式
- en: 'To demonstrate confidence, we will use the items beer and wine. Specifically,
    let''s compute the confidence of Beer ![](img/C12626_Formula_08_02.png) Wine.
    To start, we need to identify the transactions that contain beer. There are 3
    of them, and they are transactions 2, 6, and 7\. Now, of those transactions, how
    many contain wine? The answer is all of them. Thus, the confidence of Beer ![](img/C12626_Formula_08_03.png)
    Wine is 1\. Every time a customer bought beer, they also bought wine. It might
    be obvious, but for identifying actionable associations, higher confidence values
    are better:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示置信度，我们将使用啤酒和葡萄酒这两个商品。具体来说，我们来计算啤酒 ![](img/C12626_Formula_08_02.png) 葡萄酒的置信度。首先，我们需要找出包含啤酒的交易。有3个这样的交易，它们是交易2、6和7。现在，在这些交易中，有多少包含葡萄酒？答案是所有的交易都包含葡萄酒。因此，啤酒
    ![](img/C12626_Formula_08_03.png) 葡萄酒的置信度是1。每次顾客购买了啤酒，他们也购买了葡萄酒。这可能很明显，但为了识别可操作的关联，更高的置信度值是更好的：
- en: Lift and Leverage
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升度和杠杆度
- en: 'We will discuss the next two metrics, lift and leverage, simultaneously, since
    despite being calculated differently, both seek to answer the same question. Like
    confidence, **lift** and **leverage** are notated as A ![](img/C12626_Formula_08_04.png)
    B. The question to which we seek an answer is, can one item, say A, be used to
    determine anything about another item, say B? Stated another way, if product A
    is bought by an individual, can we say anything about whether they will or will
    not purchase product B with some level of confidence? These questions are answered
    by comparing the support of A and B under the standard case when A and B are not
    assumed to be independent with the case where the two products are assumed to
    be independent. Lift calculates the ratio of these two cases, so its range is
    [0, Infinity]. When lift equals one, the two products are independent and, hence,
    no conclusions can be made about product B when product A is purchased:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将同时讨论接下来的两个度量，提升度和杠杆度，因为尽管它们的计算方式不同，但都试图回答相同的问题。与置信度一样，**提升度**和**杠杆度**也表示为A
    ![](img/C12626_Formula_08_04.png) B。我们要回答的问题是，是否可以通过一个物品，比如A，来推断另一个物品，比如B？换句话说，如果一个人购买了产品A，我们能否在一定程度上确定他们是否会购买产品B？这些问题通过将A和B的支持度在假设A和B不独立的标准情况下与假设两者独立的情况进行比较来回答。提升度计算这两种情况的比率，因此其范围是[0,
    无限]。当提升度等于1时，两个产品是独立的，因此在购买产品A时，无法得出关于产品B的任何结论：
- en: '![Figure 8.6: Formula for lift'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.6：提升度公式'
- en: '](img/C12626_08_06.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_08_06.jpg)'
- en: 'Figure 8.6: Formula for lift'
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.6：提升度公式
- en: 'Leverage calculates the difference between the two cases, so its range is [-1,
    1]. Leverage equaling zero can be interpreted the same way as lift equaling one:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Leverage 计算两种情况之间的差异，因此其范围是[-1, 1]。Leverage 等于零可以解释为与 lift 等于一相同的含义：
- en: '![Figure 8.7: Formula for leverage'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.7：杠杆公式'
- en: '](img/C12626_08_07.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_08_07.jpg)'
- en: 'Figure 8.7: Formula for leverage'
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.7：杠杆公式
- en: The values of the metrics measure the strength and direction of the relationship
    between the items. If the lift value is 0.1, we could say the relationship between
    the two items is strong in the negative direction. That is, it could be said that
    when one product is purchased, the chance the second product is purchased is diminished.
    The positive and negative associations are separated by the points of independence,
    which, as stated earlier, are 1 for lift and 0 for leverage, and the further away
    the value gets from these points, the stronger the association.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标的值衡量项目之间关系的强度和方向。如果 lift 值为 0.1，我们可以说两个项目之间的关系在负方向上很强。也就是说，可以认为当购买一个产品时，购买第二个产品的机会会减少。正相关和负相关被独立性点所分隔，正如前面所说，lift
    的独立性点为 1，leverage 的独立性点为 0，而值越远离这些点，关联越强。
- en: Conviction
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信念
- en: 'The last metric to be discussed is conviction, which is a bit less intuitive
    than the other metrics. Conviction is the ratio of the expected frequency that
    X occurs without Y, given that X and Y are independent to the frequency of incorrect
    predictions. The frequency of incorrect predictions is defined as 1 minus the
    confidence of X ![](img/C12626_Formula_08_05.png) Y. Remember that confidence
    can be defined as ![](img/C12626_Formula_08_06.png), which means .![](img/C12626_Formula_08_07.png).
    The numerator could also be thought of as ![](img/C12626_Formula_08_08.png). The
    only difference between the two is that the numerator has the assumption of independence
    between X and Y, while the denominator does not. A value greater than 1 is ideal
    because that means the association between products or item sets X and Y is incorrect
    more often if the association between X and Y is random chance (in other words,
    X and Y are independent). To reiterate, this stipulates that the association between
    X and Y is meaningful. A value of 1 applies independence, and a value of less
    than 1 signifies that the random chance X and Y relationship is correct more often
    than the X and Y relationship that has been defined as X ![](img/C12626_Formula_08_09.png)
    Y. Under this situation, the relationship might go the other way (in other words,
    Y ![](img/C12626_Formula_08_10.png) X). Conviction has the range [0, Inf] and
    the following form:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要讨论的指标是信念，它比其他指标稍微不直观。信念是指在 X 和 Y 独立的情况下，X 发生但 Y 不发生的预期频率与错误预测频率的比值。错误预测频率定义为
    1 减去 X 的置信度 ![](img/C12626_Formula_08_05.png) Y。记住，置信度可以定义为 ![](img/C12626_Formula_08_06.png)，这意味着
    .![](img/C12626_Formula_08_07.png)。分子也可以视为 ![](img/C12626_Formula_08_08.png)。两者的唯一区别是分子假设
    X 和 Y 之间是独立的，而分母则没有。理想情况下，值大于 1，因为这意味着当 X 和 Y 之间的关联是随机偶然（换句话说，X 和 Y 是独立的）时，产品或项目集合
    X 和 Y 之间的关联更常是错误的。再强调一遍，这表明 X 和 Y 之间的关联是有意义的。值为 1 表示独立性，而小于 1 的值则意味着 X 和 Y 之间的随机关系比定义为
    X ![](img/C12626_Formula_08_09.png) Y 的关系更常见。在这种情况下，关系可能是反向的（换句话说，Y ![](img/C12626_Formula_08_10.png)
    X）。信念的范围是[0, ∞]，其形式如下：
- en: '![Figure 8.8: Formula for conviction'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.8：信念公式'
- en: '](img/C12626_08_08.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_08_08.jpg)'
- en: 'Figure 8.8: Formula for conviction'
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.8：信念公式
- en: Let's again return to the products beer and wine, but for this explanation,
    we will consider the opposite association of Wine ![](img/C12626_Formula_08_11.png)
    Beer. Support(Y) or, in this case, Support(Beer) is 3/10 and Confidence X ![](img/C12626_Formula_08_12.png)
    Y, or, in this case, Confidence(Wine ![](img/C12626_Formula_08_13.png) Beer),
    is 3/4\. Thus, the Conviction(Wine ![](img/C12626_Formula_08_14.png) Beer) is
    (1-3/10) / (1-3/4) = (7/10) * (4/1). We can conclude by saying that Wine ![](img/C12626_Formula_08_15.png)
    Beer would be incorrect 2.8 times as often if wine and beer were independent.
    Thus, the previously articulated association between wine and beer is legitimate.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次回到啤酒和葡萄酒这两个产品，但为了说明本次情况，我们将考虑葡萄酒 ![](img/C12626_Formula_08_11.png) 啤酒的相反关联。Support(Y)，或在本例中为Support(啤酒)，是3/10，而Confidence
    X ![](img/C12626_Formula_08_12.png) Y，或在本例中为Confidence(葡萄酒 ![](img/C12626_Formula_08_13.png)
    啤酒)，是3/4。因此，Conviction(葡萄酒 ![](img/C12626_Formula_08_14.png) 啤酒)为(1-3/10) / (1-3/4)
    = (7/10) * (4/1)。我们可以得出结论，如果葡萄酒和啤酒是独立的，那么葡萄酒 ![](img/C12626_Formula_08_15.png)
    啤酒的关联会错误出现2.8次。因此，之前提到的葡萄酒和啤酒的关联是有效的。
- en: 'Exercise 40: Computing Metrics'
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 40：计算指标
- en: In this exercise, we use the fake data in *Exercise 39*, *Creating Sample Transaction
    Data* to compute the five previously described metrics, which we will use again
    in the covering of the Apriori algorithm and association rules. The association
    on which these metrics will be evaluated is Milk ![](img/C12626_Formula_08_16.png)
    Bread.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们使用*练习 39*中的虚拟数据，*创建样本事务数据*，来计算之前描述的五个指标，我们将在讲解Apriori算法和关联规则时再次使用这些指标。我们将评估的关联是牛奶
    ![](img/C12626_Formula_08_16.png) 面包。
- en: Note
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: All exercises in this chapter need to be performed in the same Jupyter notebook.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有练习都需要在同一个Jupyter笔记本中完成。
- en: 'Define and print the frequencies that are the basis of all five metrics, which
    would be Frequency(Milk), Frequency(Bread), and Frequency(Milk, Bread). Also,
    define N as the total number of transactions in the dataset:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并打印构成所有五个指标基础的频率，即Frequency(牛奶)、Frequency(面包)和Frequency(牛奶, 面包)。还需定义N为数据集中交易的总数：
- en: '[PRE2]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output is as follows:'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.9: Screenshot of the frequencies'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.9：频率截图'
- en: '](img/C12626_08_09.jpg)'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_08_09.jpg)'
- en: 'Figure 8.9: Screenshot of the frequencies'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.9：频率截图
- en: 'Calculate and print Support(Milk ![](img/C12626_Formula_08_17.png) Bread):'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并打印Support(牛奶 ![](img/C12626_Formula_08_17.png) 面包)：
- en: '[PRE3]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The support of `x` to `y` is `0.4`. From experience, if we were working with
    a full transaction dataset, this support value would be considered very large
    in many cases.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`x` 到 `y` 的支持度为 `0.4`。根据经验，如果我们使用的是完整的交易数据集，那么这个支持值在许多情况下会被认为是非常大的。'
- en: 'Calculate and print Confidence(Milk ![](img/C12626_Formula_08_17.png) Bread):'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并打印Confidence(牛奶 ![](img/C12626_Formula_08_17.png) 面包)：
- en: '[PRE4]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The confidence of `x` to `y` is `0.5714`. This means that the probability of
    Y being purchased given that `x` was purchased is just slightly higher than 50%.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`x` 到 `y` 的置信度为 `0.5714`。这意味着，给定已经购买了`x`，Y被购买的概率略高于50%。'
- en: 'Calculate and print Lift(Milk ![](img/C12626_Formula_08_19.png) Bread):'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并打印Lift(牛奶 ![](img/C12626_Formula_08_19.png) 面包)：
- en: '[PRE5]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The lift of `x` to `y` is `1.1429`.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`x` 到 `y` 的提升度为 `1.1429`。'
- en: 'Calculate and print Leverage(Milk ![](img/C12626_Formula_08_19.png) Bread):'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并打印Leverage(牛奶 ![](img/C12626_Formula_08_19.png) 面包)：
- en: '[PRE6]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The leverage of `x` to `y` is `0.05`. Both lift and leverage can be used to
    say that the association `x` to `y` is positive (in other words, `x` implies `y`),
    but weak. That is, the values are close to 1 and 0, respectively.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`x` 到 `y` 的杠杆度为 `0.05`。提升度和杠杆度都可以用来说明关联`x`到`y`是正向的（换句话说，`x`意味着`y`），但弱。也就是说，值分别接近1和0。'
- en: 'Calculate and print Conviction(Milk ![](img/C12626_Formula_08_19.png) Bread):'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并打印Conviction(牛奶 ![](img/C12626_Formula_08_19.png) 面包)：
- en: '[PRE7]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The conviction value of `1.1667` can be interpreted by saying the Milk ![](img/C12626_Formula_08_19.png)
    Bread association would be incorrect `1.1667` times as often if milk and bread
    were independent.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`1.1667`的置信度值可以解释为，如果牛奶和面包是独立的，那么牛奶 ![](img/C12626_Formula_08_19.png) 面包的关联会错误出现`1.1667`次。'
- en: Before diving into the Apriori algorithm and association rule learning on actual
    data, we will explore transaction data and get some retail data loaded and prepped
    for modeling.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解Apriori算法和实际数据上的关联规则学习之前，我们将先探索事务数据，并加载和准备一些零售数据进行建模。
- en: Characteristics of Transaction Data
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事务数据的特点
- en: 'The data used in market basket analysis is transaction data or any type of
    data that resembles transaction data. In its most basic form, transaction data
    has some sort of transaction identifier, such as an invoice or transaction number,
    and a list of products associated with said identifier. It just so happens that
    these two base elements are all that is needed to perform market basket analysis.
    However, transaction data rarely – it is probably even safe to say never – comes
    in this basic form. Transaction data typically includes pricing information, dates
    and times, and customer identifiers, among many other things:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在市场篮分析中使用的数据是交易数据，或者任何类似交易数据的数据。最基本的交易数据包含某种交易标识符，如发票号或交易号，以及与该标识符相关的产品列表。恰好这两项基本要素就是进行市场篮分析所需的全部内容。然而，交易数据很少——甚至可以说从未——以这种基本形式存在。交易数据通常还包括定价信息、日期和时间、客户标识符等许多其他信息：
- en: '![Figure 8.10: Each available product is going to map back to multiple invoice
    numbers'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.10：每个可用的产品将映射到多个发票号码'
- en: '](img/C12626_08_10.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_08_10.jpg)'
- en: 'Figure 8.10: Each available product is going to map back to multiple invoice
    numbers'
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.10：每个可用的产品将映射到多个发票号码
- en: Due to the complexity of transaction data, data cleaning is crucial. The goal
    of data cleaning in the context of market basket analysis is to filter out all
    the unnecessary information, which includes removing variables in the data that
    are not relevant, and filtering out problematic transactions. The techniques used
    to complete these two cleaning steps vary, depending on the particular transaction
    data file. In an attempt to not get bogged down in data cleaning, the exercises
    from here on out will use a subset of an online retail dataset from the UCI Machine
    Learning Repository, and the activities will use the whole dataset. This both
    limits the data cleaning discussion, but also gives us an opportunity to discuss
    how the results change when the size of the dataset changes. This is important
    because if you work for a retailer and run market basket analysis, it will be
    important to understand and be able to clearly articulate the fact that, as more
    data is received, product relationships can, and most likely will, shift. Before
    discussing the specific cleaning process required for this dataset, let's load
    the online retail dataset.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 由于交易数据的复杂性，数据清洗至关重要。在市场篮分析的背景下，数据清洗的目标是过滤掉所有不必要的信息，包括移除与分析无关的变量，并清理掉有问题的交易。完成这两步清洗所使用的技术有所不同，具体取决于交易数据文件的情况。为了避免在数据清洗中陷入困境，接下来的练习将使用来自UCI机器学习库的在线零售数据集的一个子集，活动将使用完整的数据集。这既限制了数据清洗的讨论，又为我们提供了一个机会，讨论当数据集大小变化时，结果如何变化。这一点很重要，因为如果你为零售商工作并进行市场篮分析，你需要理解并清楚地说明，随着数据量的增加，产品之间的关系可能会发生变化，而且很可能会发生变化。在讨论此数据集所需的具体清洗过程之前，让我们先加载在线零售数据集。
- en: 'Exercise 41: Loading Data'
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 41：加载数据
- en: In this exercise, we will load and view an example online retail dataset. This
    dataset is originally from the UCI Machine Learning Repository and can be found
    at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Exercise39-Exercise45](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Exercise39-Exercise45).
    Once you have downloaded the dataset, save it and note the path. Now, let's proceed
    with the exercise. The output of this exercise is the transaction data that will
    be used in future modeling exercises and some exploratory figures to help us better
    understand the data with which we are working.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将加载并查看一个示例的在线零售数据集。该数据集最初来自UCI机器学习库，可以在[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Exercise39-Exercise45](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Exercise39-Exercise45)找到。下载数据集后，请保存并记下路径。现在，我们开始练习。本练习的输出结果是交易数据，未来建模练习中将使用这些数据，并通过一些探索性图形帮助我们更好地理解我们正在处理的数据。
- en: Note
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is taken from [http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail).
    It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Exercise39-Exercise45](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Exercise39-Exercise45).
    Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry:
    A case study of RFM model-based customer segmentation using data mining, Journal
    of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208,
    2012.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集来自[http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail)。可以从[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Exercise39-Exercise45](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Exercise39-Exercise45)下载。陈大庆、梁赛、郭坤，《面向在线零售行业的数据挖掘：基于RFM模型的客户细分案例研究》，《数据库营销与客户战略管理杂志》，第19卷，第3期，197-208页，2012年。
- en: 'UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA:
    University of California, School of Information and Computer Science.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: UCI机器学习库 [http://archive.ics.uci.edu/ml]。加利福尼亚州欧文市：加利福尼亚大学信息与计算机科学学院。
- en: 'Using the `read_excel` function from `pandas`, load the data. Note that the
    first row of the Excel file contains the column names:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`中的`read_excel`函数加载数据。请注意，Excel文件的第一行包含列名：
- en: '[PRE8]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The path to `Online Retail.xlsx` should be changed as per the location of the
    file on your system.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Online Retail.xlsx`的路径应根据文件在系统中的位置进行修改。'
- en: 'Print out the first 10 rows of the DataFrame. Notice that the data contains
    some columns that will not be relevant to market basket analysis:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出数据框的前10行。请注意，数据中包含一些与市场篮子分析无关的列：
- en: '[PRE9]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.11: The raw online retail data'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.11：原始在线零售数据'
- en: '](img/C12626_08_11.jpg)'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_08_11.jpg)'
- en: 'Figure 8.11: The raw online retail data'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.11：原始在线零售数据
- en: 'Print out the data type for each column in the DataFrame. This information
    will come in handy when trying to perform specific cleaning tasks:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出数据框中每一列的类型。此信息在执行特定的清理任务时将非常有用：
- en: '[PRE10]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.12: Data type for each column in the dataset'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.12：数据集中每列的数据类型'
- en: '](img/C12626_08_12.jpg)'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_08_12.jpg)'
- en: 'Figure 8.12: Data type for each column in the dataset'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.12：数据集中每列的数据类型
- en: 'Get the dimensions of the DataFrame, as well as the number of unique invoice
    numbers and customer identifications:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数据框的维度，以及唯一发票号和客户标识的数量：
- en: '[PRE11]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE12]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this exercise, we have loaded the data and performed some exploratory work.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们已经加载了数据并进行了初步探索性工作。
- en: Data Cleaning and Formatting
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据清理与格式化
- en: With the dataset now loaded, let's delve into the specific data cleaning processes
    to be performed. Since we are going to filter the data down to just the invoice
    numbers and items, we focus the data cleaning on these two columns of the dataset.
    Remember that market basket analysis looks to identify associations between the
    items purchased by all customers over time. As such, the main focus of the data
    cleaning involves removing transactions with non-positive numbers of items. This
    could happen when the transaction involves voiding another transaction, when items
    are returned, or when the transaction is some administrative task. These types
    of transactions will be filtered out in two ways. The first is that canceled transactions
    have invoice numbers that are prefaced with "C," so we will identify those specific
    invoice numbers and remove them from the data. The other approach is to remove
    all transactions with either zero or negative numbers of items. After performing
    these two steps, the data will be subset down to just the invoice number and item
    description columns, and any row of the now two-column dataset with at least one
    missing value is removed.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据集已经加载，让我们深入探讨具体的数据清理过程。由于我们将数据筛选为仅包含发票号和商品项，我们将数据清理工作集中在数据集的这两列上。请记住，市场篮分析旨在识别所有客户在一段时间内购买商品之间的关联。因此，数据清理的主要工作是删除包含非正数商品的交易。这种情况可能发生在交易作废、商品退货或行政操作等场景中。这些类型的交易将通过两种方式进行筛选。首先，已取消的交易其发票号前面带有“C”，所以我们将识别这些特定的发票号并将其从数据中删除。另一种方法是删除所有商品数量为零或负数的交易。执行这两步后，数据将仅保留发票号和商品描述这两列，并删除任何包含至少一个缺失值的行。
- en: The next stage of the data cleaning exercise involves putting the data in the
    appropriate format for modeling. In this and subsequent exercises, we will use
    a subset of the full data. The subset will be done by taking the first 5,000 unique
    invoice numbers. Once we have cut the data down to the first 5,000 unique invoice
    numbers, we change the data structure to that needed to run the models. Note that
    the data is currently in long format, where each item is on its own row. The desired
    format is a list of lists, like the made-up data from earlier in the chapter.
    Each subset list represents a unique invoice number, so in this case, the outer
    list should contain 5,000 sub-lists. The elements of the sub-lists are all the
    items belonging to the invoice number that that sub-list represents. With the
    cleaning process described, let's proceed to the exercise.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理的下一阶段涉及将数据转换为适合建模的格式。在此及后续的练习中，我们将使用完整数据的一个子集。这个子集通过获取前5000个唯一的发票号来创建。将数据缩减到前5000个唯一发票号后，我们将数据结构更改为运行模型所需的格式。请注意，目前数据是长格式，每个商品占一行。期望的格式是一个列表的列表，类似于本章早些时候所提到的虚构数据。每个子集列表表示一个唯一的发票号，因此，在本例中，外部列表应包含5000个子列表。子列表的元素是所有属于该发票号的商品。按照描述的清理过程，我们接下来开始练习。
- en: 'Exercise 42: Data Cleaning and Formatting'
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 42：数据清理与格式化
- en: In this exercise, we will perform the cleaning steps described previously. As
    we work through the process, the evolution of the data will be monitored by printing
    out the current state of the data and computing some basic summary metrics. Be
    sure to perform data cleaning in the same notebook in which the data is loaded.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将执行之前描述的数据清理步骤。在处理过程中，我们将通过打印出数据的当前状态并计算一些基本的汇总指标来监控数据的变化。确保在加载数据的同一个笔记本中执行数据清理。
- en: 'Create an indicator column stipulating whether the invoice number begins with
    "`C`":'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个指示列，标明发票号是否以 "`C`" 开头：
- en: '[PRE13]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Filter out all transactions having either zero or a negative number of items,
    remove all invoice numbers starting with "C" using the column created in step
    one, subset the DataFrame down to `InvoiceNo` and `Description`, and lastly, drop
    all rows with at least one missing value. Rename the DataFrame `online1`:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 筛选出所有商品数量为零或负数的交易，使用第一步创建的列删除所有以“C”开头的发票号，将 DataFrame 子集化为 `InvoiceNo` 和 `Description`，最后删除所有包含至少一个缺失值的行。将
    DataFrame 重命名为 `online1`：
- en: '[PRE14]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Print out the first 10 rows of the filtered DataFrame, `online1`:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出过滤后的 DataFrame `online1` 的前10行：
- en: '[PRE15]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Figure 8.13: The cleaned online retail dataset'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.13：清理后的在线零售数据集'
- en: '](img/C12626_08_13.jpg)'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_08_13.jpg)'
- en: 'Figure 8.13: The cleaned online retail dataset'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.13：清理后的在线零售数据集
- en: 'Print out the dimensions of the cleaned DataFrame and the number of unique
    invoice numbers:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出清理后的DataFrame的维度以及唯一发票号的数量：
- en: '[PRE16]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE17]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Notice that we have already removed approximately 10,000 rows and 5,800 invoice
    numbers.
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们已经删除了大约10,000行和5,800个发票号。
- en: 'Extract the invoice numbers from the DataFrame as a list. Remove duplicate
    elements to create a list of unique invoice numbers. Confirm that the process
    was successful by printing the length of the list of unique invoice numbers. Compare
    with the output of *Step 4*:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将发票号从DataFrame中提取为列表。删除重复元素，生成唯一发票号的列表。通过打印唯一发票号列表的长度来确认处理是否成功。与*步骤4*的输出进行比较：
- en: '[PRE18]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE19]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Take the list from step five and cut it to only include the first 5,000 elements.
    Print out the length of the new list to confirm that it is, in fact, the expected
    length of 5,000:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第五步的列表中取出，仅保留前5,000个元素。打印新列表的长度以确认它确实是预期的5,000长度：
- en: '[PRE20]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is as follows:'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE21]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Filter the `online1` DataFrame down by only keeping the invoice numbers in
    the list from the previous step:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅保留前一步列表中的发票号，过滤`online1` DataFrame：
- en: '[PRE22]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Print out the first 10 rows of `online1`:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出`online1`的前10行：
- en: '[PRE23]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.14: The cleaned dataset with only 5,000 unique invoice numbers'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.14：仅包含5,000个唯一发票号的清理后的数据集'
- en: '](img/C12626_08_14.jpg)'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_08_14.jpg)'
- en: 'Figure 8.14: The cleaned dataset with only 5,000 unique invoice numbers'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.14：仅包含5,000个唯一发票号的清理后的数据集
- en: 'Print out the dimensions of the DataFrame and the number of unique invoice
    numbers to confirm that the filtering and cleaning process was successful:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出DataFrame的维度和唯一发票号的数量，以确认过滤和清理过程是否成功：
- en: '[PRE24]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is as follows:'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE25]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Transform the data in `online1` into the aforementioned list of lists called
    `invoice_item_list`. The process for doing this is to iterate over the unique
    invoice numbers and, at each iteration, extract the item descriptions as a list
    and append that list to the larger `invoice_item_list` list. Print out elements
    one through four of the list:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`online1`中的数据转换为上述所述的列表形式，称为`invoice_item_list`。实现这一过程的方法是遍历唯一的发票号，在每次迭代时提取项目描述作为一个列表，并将该列表附加到更大的`invoice_item_list`列表中。打印列表中的前四个元素：
- en: '[PRE26]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is as follows:'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.15: Four elements of the list of lists, where each sub-list contains
    all the items belonging to an individual invoice'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.15：列表中包含四个元素的子列表，每个子列表包含属于单个发票的所有项目'
- en: '](img/C12626_08_15.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_08_15.jpg)'
- en: 'Figure 8.15: Four elements of the list of lists, where each sub-list contains
    all the items belonging to an individual invoice'
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.15：列表中包含四个元素的子列表，每个子列表包含属于单个发票的所有项目
- en: Note
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This step can take some minutes to complete.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤可能需要几分钟才能完成。
- en: Data Encoding
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据编码
- en: While cleaning the data is crucial, the most important part of the data preparation
    process is molding the data into the correct form. Before running the models,
    the data, currently in the list of lists form, needs to be encoded and recast
    as a DataFrame. To do this, we will leverage `TransactionEncoder` from the `preprocessing`
    module of `mlxtend`. Outputted from the encoder is a multidimensional array, where
    each row is the length of the total number of unique items in the transaction
    dataset and the elements are Boolean variables, indicating whether that particular
    item is linked to the invoice number that row represents. With the data encoded,
    we can recast it as a DataFrame where the rows are the invoice numbers and the
    columns are the unique items in the transaction dataset.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然清理数据至关重要，但数据准备过程最重要的部分是将数据调整为正确的形式。在运行模型之前，当前以列表的形式存在的数据需要被编码并重新转换为DataFrame。为此，我们将利用`mlxtend`的`preprocessing`模块中的`TransactionEncoder`。编码器输出的是一个多维数组，每一行的长度等于事务数据集中唯一项目的总数，元素为布尔变量，指示该特定项目是否与该行所表示的发票号相关联。数据编码后，我们可以将其重新转换为DataFrame，其中行是发票号，列是事务数据集中的唯一项目。
- en: In the following exercise, the data encoding will be done using `mlxtend`, but
    it is very easy to encode the data without using any package. The first step is
    to unlist the list of lists and return one list with every value from the original
    list of lists. Next, the duplicate products are filtered out and, if preferred,
    the data is sorted in alphabetical order. Before doing the actual encoding, we
    initialize the final DataFrame by having all elements equal to false, a number
    of rows equal to the number of invoice numbers in the dataset, and column names
    equal to the non-duplicated list of product names.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下练习中，数据编码将使用 `mlxtend` 完成，但也可以不使用任何包进行编码，方法非常简单。第一步是将嵌套列表展平，返回一个包含原始嵌套列表中每个值的单一列表。接下来，去除重复的产品，并且如果需要，可以按字母顺序对数据进行排序。在进行实际编码之前，我们通过将所有元素初始化为
    `false` 来初始化最终的 DataFrame，行数等于数据集中发票号码的数量，列名为非重复的产品名称列表。
- en: 'In this case, we have 5,000 transactions and over 3,100 unique products. Thus,
    the DataFrame has over 15,000,000 elements. The actual encoding is done by looping
    over each transaction and each item in each transaction. Change the row *i* and
    column *j* cell values in the initialized dataset from false to true if the ![](img/C12626_Formula_08_23.png)
    transaction contains the ![](img/C12626_Formula_08_24.png) product. This double
    loop is not fast as we need to iterate over 15,000,000 cells. There are ways to
    improve performance, including some that have been implemented in `mlxtend`, but
    to better understand the process, it is helpful to work through the double loop
    methodology. The following is an example function to do the encoding from scratch
    without the assistance of any package other than `pandas`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有 5,000 笔交易和超过 3,100 个唯一产品。因此，DataFrame 中包含超过 15,000,000 个元素。实际的编码是通过遍历每笔交易和每笔交易中的每个商品来完成的。如果交易包含某个产品，便将初始化数据集中第
    *i* 行和第 *j* 列的值从 false 改为 true。由于我们需要遍历 15,000,000 个单元格，这个双重循环并不高效。虽然有一些方法可以提高性能，包括在
    `mlxtend` 中实现的一些方法，但为了更好地理解这个过程，通过双重循环的方法是很有帮助的。以下是一个示例函数，用于在不借助任何包（除了 `pandas`）的情况下从头开始进行编码：
- en: '[PRE27]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Exercise 43: Data Encoding'
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 43：数据编码
- en: In this exercise, we continue the data preparation process by taking the list
    of lists generated in the previous exercise and encoding the data in the specific
    way required to run the models.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们通过对上一练习中生成的嵌套列表进行编码，继续数据准备过程，以便以特定方式运行模型。
- en: 'Initialize and fit the transaction encoder. Print out an example of the resulting
    data:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化并拟合事务编码器。打印出结果数据的示例：
- en: '[PRE28]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is as follows:'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.16: The multi-dimensional array containing the Boolean variables
    indicating product presence in each transaction'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.16：包含表示每笔交易中产品存在的布尔变量的多维数组](img/C12626_08_16.jpg)'
- en: '](img/C12626_08_16.jpg)'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_08_16.jpg)'
- en: 'Figure 8.16: The multi-dimensional array containing the Boolean variables indicating
    product presence in each transaction'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.16：包含表示每笔交易中产品存在的布尔变量的多维数组
- en: 'Recast the encoded array as a DataFrame named `online_encoder_df`. Print out
    a predefined subset of the DataFrame that features both true and false values:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将编码后的数组重构为名为 `online_encoder_df` 的 DataFrame。打印出一个预定义的 DataFrame 子集，包含 true
    和 false 值：
- en: '[PRE29]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output will be similar to the following:'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果将类似于以下内容：
- en: '![Figure 8.17: A small section of the encoded data recast as a DataFrame'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.17：将编码数据的小部分重构为 DataFrame](img/C12626_08_17.jpg)'
- en: '](img/C12626_08_17.jpg)'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_08_17.jpg)'
- en: 'Figure 8.17: A small section of the encoded data recast as a DataFrame'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.17：将编码数据的小部分重构为 DataFrame
- en: 'Print out the dimensions of the encoded DataFrame. It should have 5,000 rows
    because the data used to generate it was previously filtered down to 5,000 unique
    invoice numbers:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出编码后 DataFrame 的维度。它应该有 5,000 行，因为生成它的数据已经过滤为 5,000 个唯一的发票号码：
- en: '[PRE30]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output will be similar to the following:'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果将类似于以下内容：
- en: '[PRE31]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The data is now prepped for modeling. In the next section, we will explore the
    Apriori algorithm.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 数据现已准备好用于建模。在接下来的部分，我们将探索 Apriori 算法。
- en: 'Activity 18: Loading and Preparing Full Online Retail Data'
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 18：加载并准备完整的在线零售数据
- en: In this activity, we are charged with loading and preparing a large transaction
    dataset for modeling. The final output will be an appropriately encoded dataset
    that has one row for each unique transaction in the dataset, and one column for
    each unique item in the dataset. If an item appears in an individual transaction,
    that element of the DataFrame will be marked true.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们的任务是加载并准备一个大型事务数据集进行建模。最终输出将是一个适当编码的数据集，每个独特的事务占一行，每个独特的商品占一列。如果某个商品出现在某个事务中，那么该数据框中的该元素将标记为真。
- en: This activity will largely repeat the last few exercises, but will use the complete
    online retail dataset file. No new downloads need to be executed, but you will
    need the path to the file downloaded previously. Perform this activity in a separate
    Jupyter notebook.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动将大致重复前几次练习，但将使用完整的在线零售数据集文件。无需执行新的下载，但你需要先前下载文件的路径。请在单独的 Jupyter 笔记本中执行此活动。
- en: 'The following steps will help you to complete the activity:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成此活动：
- en: 'Load the online retail dataset file:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载在线零售数据集文件：
- en: Note
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: 'This dataset is taken from [http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail).
    It can be downloaded from https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Activity18-Activity20\.
    Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry:
    A case study of RFM model-based customer segmentation using data mining, Journal
    of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208,
    2012.'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此数据集来自 [http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail)。它可以从
    https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Activity18-Activity20
    下载。Daqing Chen、Sai Liang Sain 和 Kun Guo, 数据挖掘在在线零售行业中的应用：基于 RFM 模型的客户细分案例研究，发表于《数据库营销与客户战略管理杂志》，第19卷，第3期，197-208页，2012年。
- en: 'UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA:
    University of California, School of Information and Computer Science.'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: UCI 机器学习库 [http://archive.ics.uci.edu/ml]。加利福尼亚州尔湾：加利福尼亚大学信息与计算机科学学院。
- en: Clean and prep the data for modeling, including turning the cleaned data into
    a list of lists.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清理并准备建模数据，包括将清理后的数据转换为列表的列表。
- en: 'Encode the data and recast it as a DataFrame:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据进行编码并将其重塑为数据框：
- en: Note
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: The solution for this activity can be found on page 366.
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第366页找到。
- en: 'The output will be similar to the following:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '![Figure 8.18: A subset of the cleaned, encoded, and recast DataFrame built
    from the complete online retail dataset'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.18：从完整的在线零售数据集中清理、编码和重塑后的数据框的一个子集](img/C12626_08_18.jpg)'
- en: '](img/C12626_08_18.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_08_18.jpg)'
- en: 'Figure 8.18: A subset of the cleaned, encoded, and recast DataFrame built from
    the complete online retail dataset'
  id: totrans-224
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.18：从完整的在线零售数据集中清理、编码和重塑后的数据框的一个子集
- en: Apriori Algorithm
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apriori 算法
- en: The **Apriori** algorithm is a data mining methodology for identifying and quantifying
    frequent item sets in transaction data, and is the foundational component of association
    rule learning. Extending the results of the Apriori algorithm to association rule
    learning will be discussed in the next section. The minimum value to qualify as
    frequent in the Apriori algorithm is an input into the model and, as such, is
    adjustable. Frequency is quantified here as support, so the value inputted into
    the model is the minimum support acceptable for the analysis being done. The model
    then identifies all item sets whose support is greater than, or equal to, the
    minimum support provided to the model. Note that the minimum support parameter
    is not a parameter that can be optimized via a grid search because there is no
    evaluation metric for the Apriori algorithm. Instead, the minimum support parameter
    is set based on the data, the use case, and domain expertise.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apriori** 算法是一种数据挖掘方法，用于识别和量化事务数据中的频繁项集，是关联规则学习的基础组件。在下一节中，将讨论如何将 Apriori
    算法的结果扩展到关联规则学习。Apriori 算法中作为频繁项集的最小值是模型的输入，因此是可调节的。频率在此通过支持度来量化，因此输入模型的值是分析中接受的最小支持度。模型随后识别所有支持度大于或等于输入给定的最小支持度的项集。请注意，最小支持度参数不是通过网格搜索可以优化的参数，因为
    Apriori 算法没有评估指标。相反，最小支持度参数是根据数据、使用案例和领域专业知识来设置的。'
- en: 'The main idea behind the Apriori algorithm is the Apriori principle: any subset
    of a frequent item set must itself be frequent.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Apriori 算法背后的主要思想是 Apriori 原则：任何频繁项集的子集必须本身也是频繁的。
- en: 'Another aspect worth mentioning is the corollary: no superset of an infrequent
    item set can be frequent.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得提及的方面是推论：不频繁项集的超集不可能是频繁的。
- en: Let's take some examples. If the item set {hammer, saw, and nail} is frequent,
    then, according to the Apriori principle and what is hopefully obvious, any less
    complex item set, say {hammer, saw}, is also frequent. On the contrary, if that
    same item set, {hammer, saw, nail}, is infrequent, then adding complexity, such
    as incorporating wood in the item set {hammer, saw, nail, wood}, is not going
    to result in the item set becoming frequent.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一些例子。如果项集 {锤子、锯子和钉子} 是频繁的，那么根据 Apriori 原则以及显而易见的道理，任何更简单的项集，例如 {锤子、锯子}，也一定是频繁的。相反，如果同样的项集
    {锤子、锯子、钉子} 是不频繁的，那么增加复杂性，比如在项集 {锤子、锯子、钉子} 中加入木材 {锤子、锯子、钉子、木材}，也不会使该项集变得频繁。
- en: 'It might seem straightforward to calculate the support value for every item
    set in a transactional database and only return those item sets whose support
    is greater than or equal to the prespecified minimum support threshold, but it
    is not because of the number of computations that need to happen. For example,
    take an item set with 10 unique items. This would result in 1,023 individual item
    sets for which support would need to be calculated. Now, try to extrapolate out
    to our working dataset that has 3,135 unique items. That is going to be an enormous
    number of item sets for which we need to compute a support value. Computational
    efficiency is a major issue:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 计算事务数据库中每个项集的支持度值，并仅返回支持度大于或等于预设的最小支持度阈值的项集，可能看起来很简单，但实际上并非如此，因为需要进行大量计算。例如，考虑一个包含
    10 个独特项的项集。这将导致 1,023 个单独的项集，需要计算它们的支持度值。现在，试着推算一下我们的工作数据集，它包含 3,135 个独特项。我们需要为这些项集计算支持度值的数量将是巨大的。计算效率是一个重大问题。
- en: '![Figure 8.19: A mapping of how item sets are built and how the Apriori principle
    can greatly decrease the computational requirements (all the grayed-out nodes
    are infrequent)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.19：项集如何构建以及 Apriori 原则如何大幅减少计算需求的映射（所有灰色节点为不频繁项集）'
- en: '](img/C12626_08_19.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_08_19.jpg)'
- en: 'Figure 8.19: A mapping of how item sets are built and how the Apriori principle
    can greatly decrease the computational requirements (all the grayed-out nodes
    are infrequent)'
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.19：项集如何构建以及 Apriori 原则如何大幅减少计算需求的映射（所有灰色节点为不频繁项集）。
- en: 'In order to address the computational demands, the Apriori algorithm is defined
    as a bottom-up model that has two steps. These steps involve generating candidate
    item sets by adding items to already existing frequent item sets and testing these
    candidate item sets against the dataset to determine whether these candidate datasets
    are also frequent. No support value is computed for item sets that contain infrequent
    item sets. This process repeats until no further candidate item sets exist:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决计算需求，Apriori 算法被定义为一个自下而上的模型，包含两个步骤。这些步骤包括通过向已存在的频繁项集中添加项目来生成候选项集，并将这些候选项集与数据集进行测试，以确定这些候选项集是否也是频繁的。对于包含不频繁项集的项集，不会计算支持度值。这个过程会一直重复，直到不再有候选项集存在：
- en: '![Figure 8.20: Assuming a minimum support threshold of 0.4, the diagram shows
    the general Apriori algorithm structure'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.20：假设最小支持度阈值为 0.4，图示显示了一般的 Apriori 算法结构'
- en: '](img/C12626_08_20.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_08_20.jpg)'
- en: 'Figure 8.20: Assuming a minimum support threshold of 0.4, the diagram shows
    the general Apriori algorithm structure'
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.20：假设最小支持度阈值为 0.4，图示显示了一般的 Apriori 算法结构。
- en: The preceding structure includes establishing an item set, computing support
    values, filtering out infrequent item sets, creating new item sets, and repeating
    the process.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 前述结构包括建立项集、计算支持度值、过滤掉不频繁项集、创建新项集并重复此过程。
- en: There is a clear tree-like structure that serves as the path for identifying
    candidate item sets. The specific search technique used, which was built for traversing
    tree-like data structures, is called a breadth-first search, which means that
    each step of the search process focuses on completely searching one level of the
    tree before moving on instead of searching branch by branch.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一个清晰的树状结构，作为识别候选项集的路径。所使用的具体搜索技术，是为遍历树状数据结构而设计的宽度优先搜索，这意味着搜索过程的每一步都专注于完全搜索树的一层，然后再移动到下一层，而不是逐分支进行搜索。
- en: 'The high-level steps of the algorithm are to:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的高层步骤如下：
- en: Define the set of frequent items. To start, this is typically the set of individual
    items.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义频繁项集。首先，这通常是单个项目的集合。
- en: Derive candidate item sets by combining frequent item sets together. Move up
    in size one item at a time. That is, go from item sets with one item to two, two
    to three, and so on.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将频繁项集组合在一起，推导候选项集。每次增加一个项集的大小。也就是说，从一个项集的集合开始，逐渐增加到两个项集、三个项集，依此类推。
- en: Compute the support value for each candidate item set.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个候选项集的支持度值。
- en: Create a new frequent item set made up of the candidate item sets whose support
    value exceeded the specified threshold.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的频繁项集，由支持度值超过指定阈值的候选项集组成。
- en: Repeat *Steps 1* to *4* until there are no more frequent item sets; that is,
    until we have worked through all the combinations.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 重复*步骤 1*到*步骤 4*，直到没有更多的频繁项集；也就是说，直到我们遍历了所有的组合。
- en: 'The pseudo code for the Apriori algorithm is as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Apriori算法的伪代码如下：
- en: '[PRE32]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Despite the Apriori principle, this algorithm can still face significant computational
    challenges depending on the size of the transaction dataset. There are several
    strategies currently accepted to further reduce the computational demands.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管遵循Apriori原则，这个算法仍然可能面临显著的计算挑战，具体取决于事务数据集的大小。目前有几种策略被接受，以进一步减少计算需求。
- en: Computational Fixes
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算修正
- en: Transaction reduction is an easy way to reduce the computational load. Note
    that after each candidate set of item sets is generated, the entirety of the transaction
    data needs to be scanned in order to count the number of appearances of each candidate
    item set. If we could shrink the size of the transaction dataset, the size of
    the dataset scans would decrease dramatically. The shrinking of the transaction
    dataset is done by realizing that any transaction containing no frequent item
    sets in the *ith* iteration is not going to contain any frequent item sets in
    subsequent iterations. Therefore, once each transaction contains no frequent item
    sets, it can be removed from the transaction dataset used for future scans.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 事务减少是一种减少计算负担的简单方法。请注意，在生成每个候选项集之后，必须扫描整个事务数据集，以统计每个候选项集的出现次数。如果我们能缩小事务数据集的大小，数据集扫描的大小将大幅减少。事务数据集的缩小通过意识到任何在*第i*次迭代中不包含频繁项集的事务，在后续迭代中也不会包含频繁项集。因此，一旦每个事务不包含频繁项集，它就可以从未来扫描中使用的事务数据集中移除。
- en: Sampling the transaction dataset and testing each candidate item set against
    it is another approach to reducing the computational requirements associated with
    scanning the transaction dataset to calculate the support of each item set. When
    this approach is implemented it is important to lower the minimum support requirement
    to guarantee that no item sets that should be present in the final data are left
    out. Given that the sampled transaction dataset will naturally cause the support
    values to be smaller, leaving the minimum support at its original value will incorrectly
    remove what should be frequent item sets from the output of the model.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 对事务数据集进行抽样并测试每个候选项集，是减少扫描事务数据集计算每个项集支持度所需的计算量的另一种方法。在实施这一方法时，重要的是要降低最小支持度要求，以确保最终数据中没有遗漏应包含的项集。由于抽样后的事务数据集会自然导致支持度值较小，因此如果将最小支持度保持在原值，将错误地从模型输出中移除那些应该是频繁项集的项集。
- en: A similar approach is partitioning. In this case, the dataset is partitioned
    into several individual datasets on which the evaluation of each candidate item
    set is executed. Item sets are deemed frequent in the full transaction dataset
    if frequent in one of the partitions. Each partition is scanned consecutively
    until frequency for an item set is established.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 一种类似的方法是分区。在这种情况下，数据集被分割成若干个独立的数据集，在每个数据集上评估每个候选项集。如果某个项集在其中一个分区中频繁出现，那么它在完整的交易数据集中也被认为是频繁的。每个分区会被连续扫描，直到确定某个项集的频率。
- en: Regardless of whether or not one of these techniques is employed, the computational
    requirements are always going to be fairly substantial when it comes to the Apriori
    algorithm. As should now be clear, the essence of the algorithm, the computation
    of support, is not as complex as other models discussed in this text.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是否使用这些技术，Apriori算法的计算需求通常都会相当庞大。正如现在应该清楚的，算法的核心，支持度的计算，并不像本文讨论的其他模型那样复杂。
- en: 'Exercise 44: Executing the Apriori algorithm'
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 44：执行Apriori算法
- en: The execution of the Apriori algorithm is made easy with `mlxtend`. As a result,
    this exercise will focus on how to manipulate the outputted dataset and to interpret
    the results. You will recall that the cleaned and encoded transaction data was
    defined as `online_encoder_df`. Perform this exercise in the same notebook that
    all previous exercises were run as we will continue using the environment, data,
    and results already established in that notebook. (So, you should be using the
    notebook that contains the reduced dataset of 5,000 entries, not the full dataset
    as used in the activity.)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlxtend`使得执行Apriori算法变得简单。因此，本次练习将重点讲解如何操作输出的数据集以及如何解读结果。你将回忆起清洗和编码后的交易数据被定义为`online_encoder_df`。请在之前所有练习运行过的相同笔记本中执行本练习，因为我们将继续使用该笔记本中已经建立的环境、数据和结果。（因此，你应该使用包含5000条记录的缩减数据集的笔记本，而不是活动中使用的完整数据集。）'
- en: 'Run the Apriori algorithm using `mlxtend` without changing any of the default
    parameter values:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`mlxtend`运行Apriori算法，不改变任何默认参数值：
- en: '[PRE33]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The output is an empty DataFrame. The default minimum support value is set to
    0.5, so since an empty DataFrame was returned, we know that all item sets have
    a support of less than 0.5\. Depending on the number of transactions and the diversity
    of available items, having no item set with a plus 0.5 support is not unusual.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出是一个空的DataFrame。默认的最小支持度值为0.5，因此，由于返回了一个空的DataFrame，我们知道所有项集的支持度都低于0.5。根据交易的数量和可用项的多样性，没有项集支持度超过0.5并不罕见。
- en: 'Rerun the Apriori algorithm, but with the minimum support set to 0.01\. This
    minimum support value is the same as saying that when analyzing 5,000 transactions,
    we need an item set to appear 50 times to be considered frequent. As mentioned
    previously, the minimum support can be set to any value in the range [0,1]. There
    is no best minimum support value; the setting of this value is entirely subjective.
    Many businesses have their own specific thresholds for significance, but there
    is no industry standard or method for optimizing this value:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新运行Apriori算法，但将最小支持度设置为0.01。这个最小支持度值的含义是，在分析5000笔交易时，项集需要出现50次才被认为是频繁的。如前所述，最小支持度可以设置为[0,1]范围内的任何值。没有最优的最小支持度值；该值的设置完全是主观的。许多企业有自己的特定显著性阈值，但没有行业标准或优化此值的方法：
- en: '[PRE34]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output will be similar to the following:'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '![Figure 8.21: Basic output of the Apriori algorithm run using mlxtend'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.21：使用mlxtend运行的Apriori算法的基本输出'
- en: '](img/C12626_08_21.jpg)'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_08_21.jpg)'
- en: 'Figure 8.21: Basic output of the Apriori algorithm run using mlxtend'
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.21：使用mlxtend运行的Apriori算法的基本输出
- en: Notice that the item sets are designated numerically in the output, which makes
    the results hard to interpret.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，输出中项集的标识是数字形式的，这使得结果难以解读。
- en: 'Rerun the Apriori algorithm with the same minimum support as in *Step 2*, but
    this time set `use_colnames` to True. This will replace the numerical designations
    with the actual item names:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新运行Apriori算法，使用与*步骤 2*中相同的最小支持度，但这次将`use_colnames`设置为True。这样将用实际的项名称替代数字标识：
- en: '[PRE35]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output will be similar to the following:'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '![Figure 8.22: The output of the Apriori algorithm with the actual item names
    instead of numerical designations'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.22：使用实际项名称而非数字标识的Apriori算法输出'
- en: '](img/C12626_08_22.jpg)'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_08_22.jpg)'
- en: 'Figure 8.22: The output of the Apriori algorithm with the actual item names
    instead of numerical designations'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.22：Apriori算法输出，使用实际商品名称代替数字表示
- en: This DataFrame contains every item set whose support value is greater than the
    specified minimum support value. That is, these item sets occur with sufficient
    frequency to potentially be meaningful and therefore actionable.
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个DataFrame包含了每个支持度值大于指定最小支持度值的商品集。也就是说，这些商品集出现的频率足够高，可能具有一定的意义，因而可以采取行动。
- en: 'Add an additional column to the output of *Step 3* that contains the size of
    the item set, which will help with filtering and further analysis:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*步骤3*的输出中添加一列，包含商品集的大小，这有助于过滤和进一步分析：
- en: '[PRE36]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output will be similar to the following:'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果将类似于以下内容：
- en: '![](img/C12626_08_23.jpg)'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12626_08_23.jpg)'
- en: 'Figure 8.23: The Apriori algorithm output plus an additional column containing
    the lengths of the item sets'
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.23：Apriori算法输出，外加一个包含商品集长度的额外列
- en: 'Find the support of the item set containing ''`10 COLOUR SPACEBOY PEN`'':'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找包含'`10 COLOUR SPACEBOY PEN`'的商品集的支持度：
- en: '[PRE37]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.24: The output DataFrame filtered down to a single item set'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.24：输出的DataFrame被过滤为单一商品集'
- en: '](img/C12626_08_24.jpg)'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_08_24.jpg)'
- en: 'Figure 8.24: The output DataFrame filtered down to a single item set'
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.24：输出的DataFrame被过滤为单一商品集
- en: This single row DataFrame gives us the support value for this specific item
    set that contains one item. The support value says that this specific item set
    appears in 1.5% of the transactions.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个单行DataFrame给出了这个特定商品集的支持度值，该商品集包含一个商品。支持度值表示该商品集出现在1.5%的交易中。
- en: Return all item sets of length 2 whose support is in the range [0.02, 0.021]
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回所有长度为2、支持度在[0.02, 0.021]范围内的商品集
- en: '[PRE38]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output will be similar to the following:'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果将类似于以下内容：
- en: '![Figure 8.25: The Apriori algorithm output DataFrame filtered by length and
    support'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.25：通过长度和支持度过滤后的Apriori算法输出DataFrame'
- en: '](img/C12626_08_25.jpg)'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_08_25.jpg)'
- en: 'Figure 8.25: The Apriori algorithm output DataFrame filtered by length and
    support'
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.25：通过长度和支持度过滤后的Apriori算法输出DataFrame
- en: This DataFrame contains all the item sets (pairs of items bought together) whose
    support value is in the range specified at the start of the step. Each of these
    item sets appears in between 2.0% and 2.1% of transactions.
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个DataFrame包含了所有支持度值在步骤开始时指定范围内的商品集（成对购买的商品）。这些商品集出现在约2.0%到2.1%的交易中。
- en: Note that when filtering on `support`, it is wise to specify a range instead
    of a specific value since it is quite possible to pick a value for which there
    are no item sets. The preceding output has 18 item sets. Keep note of that and
    the particular items in the item sets because we will be running this same filter
    when we scale up to the full data and we will want to execute a comparison.
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，当进行`support`过滤时，最好指定一个范围而非具体的值，因为很有可能选择的值没有商品集。前面的输出有18个商品集。请记住这一点，并记住商品集中的具体商品，因为当我们扩展到完整数据时，可能会运行相同的过滤，并且我们希望执行对比。
- en: 'Plot the support values. Note that this plot will have no support values less
    than 0.01 because that was the value used as the minimum support:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制支持度值。请注意，这个图表中不会有小于0.01的支持度值，因为0.01是最小支持度值：
- en: '[PRE39]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output will be similar to the following plot:'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果将类似于以下图表：
- en: '![Figure 8.26: Distribution of the support values returned by the Apriori algorithm'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.26：Apriori算法返回的支持度值分布'
- en: '](img/C12626_08_26.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_08_26.jpg)'
- en: 'Figure 8.26: Distribution of the support values returned by the Apriori algorithm'
  id: totrans-298
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8.26：Apriori算法返回的支持度值分布
- en: The maximum support value is approximately 0.14, which is approximately 700
    transactions. What might appear to be a small value may not be given the number
    of products available. Larger numbers of products tend to result in lower support
    values because the variability of item combinations increases.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 最大支持度值大约为0.14，约为700笔交易。看似较小的值，考虑到可用商品的数量，实际上可能并不小。商品数量较多时，通常会导致较低的支持度值，因为商品组合的变化性增加。
- en: Hopefully, you can think of more ways in which this data could be used and with
    a view to supporting retail businesses. We will generate even more useful information
    in the next section by using the Apriori algorithm results to generate association
    rules.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你能想到更多的方式来利用这些数据，并且从支持零售业务的角度来考虑。我们将在下一节通过使用Apriori算法结果来生成关联规则，从而生成更多有用的信息。
- en: 'Activity 19: Apriori on the Complete Online Retail Dataset'
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动19：在完整的在线零售数据集上运行Apriori算法
- en: Imagine you work for an online retailer. You are given all the transaction data
    from the last month and told to find all the item sets appearing in at least 1%
    of the transactions. Once the qualifying item sets are identified, you are subsequently
    told to identify the distribution of the support values. The distribution of support
    values will tell all interested parties whether groups of items exist that are
    purchased together with high probability as well as the mean of the support values.
    Let's collect all the information for the company leadership and strategists.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你在一家在线零售商工作。你被提供了上个月的所有交易数据，并被要求找出在至少1%的交易中出现的所有项集。确定符合条件的项集后，你接着被要求识别支持度值的分布。支持度值的分布将告诉所有相关方是否存在高概率一起购买的商品组，以及支持度值的平均值。让我们为公司领导和战略家收集所有信息。
- en: In this activity, you will run the Apriori algorithm on the full online retail
    dataset.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，你将对完整的在线零售数据集运行Apriori算法。
- en: Note
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is taken from [http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail).
    It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Activity18-Activity20](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Activity18-Activity20).
    Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry:
    A case study of RFM model-based customer segmentation using data mining, Journal
    of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208,
    2012.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来自[http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail)。你可以从[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Activity18-Activity20](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Activity18-Activity20)下载。Daqing
    Chen, Sai Liang Sain, 和 Kun Guo, 《在线零售行业的数据挖掘：基于RFM模型的数据挖掘客户细分的案例研究》，《数据库营销与客户战略管理期刊》，第19卷，第3期，页197-208，2012年。
- en: 'UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA:
    University of California, School of Information and Computer Science.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: UCI机器学习库[http://archive.ics.uci.edu/ml]。加利福尼亚州欧文市：加利福尼亚大学信息与计算机科学学院。
- en: Ensure that you complete this activity in the same notebook as the previous
    activity (in other words, the notebook that uses the full dataset, not the notebook
    that uses the subset of 5,000 items that you're using for the exercises).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你在与之前活动相同的笔记本中完成此活动（换句话说，使用完整数据集的笔记本，而不是使用你为练习所用的5000个项目的子集的笔记本）。
- en: This will also provide you with an opportunity to compare the results with those
    generated using only 5,000 transactions. This is an interesting activity, as it
    provides some insight into the ways in which the data may change as more data
    is collected, as well as some insight into how support values change when the
    partitioning technique is employed. Note that what was done in the exercises is
    not a perfect representation of the partitioning technique because 5,000 was an
    arbitrary number of transactions to sample.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这也将为你提供一个机会，将结果与仅使用5000个交易生成的结果进行比较。这是一个有趣的活动，因为它提供了一些关于随着更多数据的收集，数据如何变化的见解，同时也为支持度值在使用分区技术时如何变化提供了一些见解。请注意，练习中的做法并不是分区技术的完美代表，因为5000个交易数是一个任意的抽样数。
- en: Note
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: All the activities in this chapter need to be performed in the same notebook.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有活动都需要在同一笔记本中完成。
- en: 'The following steps will help you to complete the activity:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成该活动：
- en: Run the Apriori algorithm on the full data with reasonable parameter settings.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在完整数据上使用合理的参数设置运行Apriori算法。
- en: Filter the results down to the item set containing `10 COLOUR SPACEBOY PEN`.
    Compare the support value to that of *Exercise 44*, *Executing the Apriori algorithm*.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果筛选至包含`10 COLOUR SPACEBOY PEN`的项集。将其支持度值与*练习44*，*执行Apriori算法*的结果进行比较。
- en: Add another column containing the item set length. Then, filter down to those
    item sets whose length is two and whose support is in the range [0.02, 0.021].
    Compare this to the result from *Exercise 44*, *Executing the Apriori algorithm*.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加另一列，包含项集长度。然后，筛选出那些长度为2且支持度在[0.02, 0.021]范围内的项集。将其与*练习44*，*执行Apriori算法*的结果进行比较。
- en: Plot the `support` values.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制`support`值。
- en: Note
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 367.
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本次活动的解答可以在第367页找到。
- en: 'The output of this activity will be similar to the following:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 本次活动的输出将类似于以下内容：
- en: '![Figure 8.27: Distribution of support values'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.27：支持度值的分布'
- en: '](img/C12626_08_27.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_08_27.jpg)'
- en: 'Figure 8.27: Distribution of support values'
  id: totrans-321
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.27：支持度值的分布
- en: Association Rules
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关联规则
- en: Association rule learning is a machine learning model that seeks to unearth
    the hidden patterns (in other words, relationships) in transaction data that describe
    the shopping habits of the customers of any retailer. The definition of an association
    rule was hinted at when the common probabilistic metrics were defined and explained
    previously.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 关联规则学习是一种机器学习模型，旨在发掘交易数据中隐藏的模式（换句话说，关系），这些数据描述了任何零售商的客户的购物习惯。关联规则的定义在之前定义和解释常见的概率度量时已经有所暗示。
- en: 'Consider the imaginary frequent item set {Milk, Bread}. Two association rules
    can be formed from that item set: Milk ![](img/C12626_Formula_08_19.png) Bread
    and Bread ![](img/C12626_Formula_08_19.png) Milk. For simplicity, the first item
    set in the association rule is referred to as the antecedent, while the second
    item set in the association rule is referred to as the consequent. Once the association
    rules have been identified, all the previously discussed metrics can be computed
    to evaluate the validity of the association rules determining whether or not the
    rules can be leveraged in the decision-making process.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑虚拟的频繁项集 {牛奶, 面包}。可以从这个项集形成两个关联规则：牛奶 ![](img/C12626_Formula_08_19.png) 面包 和
    面包 ![](img/C12626_Formula_08_19.png) 牛奶。为了简便，关联规则中的第一个项集称为前件，而第二个项集称为后件。一旦关联规则被识别出来，就可以计算之前讨论的所有度量，来评估这些关联规则的有效性，确定这些规则是否可以在决策过程中发挥作用。
- en: The establishment of an association rule is based on support and confidence.
    Support, as we discussed in the last section, identifies which item sets are frequent,
    while confidence measures the frequency of truthfulness for a particular rule.
    Confidence is typically referred to as the measure of interestingness, as it is
    the metric that determines whether an association should be formed. Thus, the
    establishment of an association rule is a two-step process. Identify frequent
    datasets and then evaluate the confidence of a candidate association rule and,
    if that confidence value exceeds some arbitrary threshold, the result is an association
    rule.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 关联规则的建立基于支持度和置信度。支持度，正如我们在上一节中讨论的，识别哪些项目集是频繁出现的，而置信度则衡量某个特定规则的真实性频率。置信度通常被称为有趣度的度量，因为它是决定是否应该形成关联的指标。因此，建立关联规则是一个两步过程。首先识别频繁的数据集，然后评估候选关联规则的置信度，如果该置信度值超过某个任意的阈值，则该规则就成为一个关联规则。
- en: A major issue of association rule learning is the discovery of spurious associations,
    which are highly likely given the huge numbers of potential rules. Spurious associations
    are defined as associations that occur with surprising regularity in the data
    given that the association occurs entirely by chance. To clearly articulate the
    idea, assume we are in a situation where we have 100 candidate rules. If we run
    a statistical test for independence at the 0.05 significance level, we are still
    faced with a 5% chance that an association is found when no association exists.
    Let's further assume that all 100 candidate rules are not valid associations.
    Given the 5% chance, we should still expect to find 5 valid association rules.
    Now scale the imaginary candidate rule list up to millions or billions, so that
    that 5% amounts to an enormous number of associations. This problem is not unlike
    the issue of statistical significance and error faced by virtually every model.
    It is worth calling out that some techniques exist to combat the spurious association
    issue, but they are neither consistently incorporated in the frequently used association
    rule libraries nor in the scope of this chapter.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 关联规则学习的一个主要问题是发现虚假的关联，这在潜在规则数量庞大的情况下是非常可能发生的。虚假关联被定义为那些在数据中出现的规律性令人惊讶的关联，尽管这些关联完全是偶然发生的。为了清楚地表达这个观点，假设我们处于一个拥有100条候选规则的情境中。如果我们在0.05的显著性水平上进行独立性统计检验，我们仍然会面临5%的概率，即使没有关联，仍然会发现关联。进一步假设所有的100条候选规则都不是有效的关联。由于5%的概率，我们仍然会期望发现5条有效的关联规则。现在，将这些假设的候选规则列表规模扩大到百万或十亿级别，那么这5%的概率就会产生一个巨大的关联数量。这个问题与几乎所有模型面临的统计显著性和错误问题类似。值得指出的是，确实存在一些技术可以用来应对虚假关联问题，但这些技术既没有在常用的关联规则库中得到一致的应用，也不在本章的讨论范围内。
- en: Let's now apply our working knowledge of association rule learning to the online
    retail dataset.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将已掌握的关联规则学习知识应用到在线零售数据集上。
- en: 'Exercise 45: Deriving Association Rules'
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 45：推导关联规则
- en: In this exercise, we will derive association rules for the online retail dataset
    and explore the associated metrics. Ensure that you complete this exercise in
    the same notebook as the previous exercises (in other words, the notebook that
    uses the 5,000-item subset, not the full dataset from the activities).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将为在线零售数据集推导关联规则并探索相关度量。确保在与之前练习相同的笔记本中完成此练习（换句话说，使用5,000项子集的笔记本，而不是活动中的完整数据集）。
- en: 'Use the `mlxtend` library to derive association rules for the online retail
    dataset. Use confidence as the measure of interestingness, set the minimum threshold
    to 0.6, and return all the metrics, not just support. Count the number of returned
    association rules:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`mlxtend`库为在线零售数据集推导关联规则。使用置信度作为有趣性度量，将最小阈值设置为0.6，并返回所有的度量，而不仅仅是支持度。计算返回的关联规则数量：
- en: '[PRE40]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output is similar to the following:'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果类似于以下内容：
- en: '![Figure 8.28: The first 7 rows of the association rules generated using only
    5,000 transactions'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.28: 仅使用5,000笔交易生成的关联规则的前7行'
- en: '](img/C12626_08_28.jpg)'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_08_28.jpg)'
- en: 'Figure 8.28: The first 7 rows of the association rules generated using only
    5,000 transactions'
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 8.28: 仅使用5,000笔交易生成的关联规则的前7行'
- en: 'Print the number of associations as follows:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式打印关联规则的数量：
- en: '[PRE41]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 5,070 association rules were found.
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 找到的关联规则数量为5,070条。
- en: Note
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注：
- en: The number of association rules may differ.
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关联规则的数量可能不同。
- en: 'Try running another version of the model. Choose any minimum threshold and
    any measure of interestingness. Count and explore the returned rules:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试运行模型的另一个版本。选择任何最小阈值和有趣性度量。计算并探索返回的规则：
- en: '[PRE42]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output is as follows:'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.29: The first 7 rows of the association rules'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.29: 关联规则的前7行'
- en: '](img/C12626_08_29.jpg)'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_08_29.jpg)'
- en: 'Figure 8.29: The first 7 rows of the association rules'
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 8.29: 关联规则的前7行'
- en: 'Print the number of associations as follows:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式打印关联规则的数量：
- en: '[PRE43]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The number of association rules found using the metric lift and the minimum
    threshold value of 50 is 26, which is significantly lower than in *Step 2*. We
    will see in the following that 50 is quite a high threshold value, so it is not
    surprising that we returned fewer association rules.
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用lift度量和最小阈值为50时找到的关联规则数量为26，比*步骤2*中的数量明显少。我们将看到，50是一个相当高的阈值，因此返回的关联规则较少并不令人惊讶。
- en: 'Plot confidence against support and identify specific trends in the data:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将置信度与支持度作图并识别数据中的特定趋势：
- en: '[PRE44]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is as follows:'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.30: A plot of confidence against support'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.30：置信度与支持度的关系图'
- en: '](img/C12626_08_30.jpg)'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_08_30.jpg)'
- en: 'Figure 8.30: A plot of confidence against support'
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.30：置信度与支持度的关系图
- en: Notice that there are not any association rules with both extremely high confidence
    and extremely high support. This should hopefully make sense. If an item set has
    high support, the items are likely to appear with many other items, making the
    chances of high confidence very low.
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，没有任何关联规则同时具有极高的置信度和极高的支持度。这应该是可以理解的。如果一个项集有很高的支持度，那么这些项很可能会与许多其他项一起出现，这就使得置信度很高的可能性非常低。
- en: 'Look at the distribution of confidence:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看置信度的分布：
- en: '[PRE45]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output is as follows:'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.31: The distribution of confidence values'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.31：置信度值的分布'
- en: '](img/C12626_08_31.jpg)'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_08_31.jpg)'
- en: 'Figure 8.31: The distribution of confidence values'
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.31：置信度值的分布
- en: 'Now, look at the distribution of lift:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，查看提升值的分布：
- en: '[PRE46]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output is as follows:'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.32: The distribution of lift values'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.32：提升值的分布'
- en: '](img/C12626_08_32.jpg)'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_08_32.jpg)'
- en: 'Figure 8.32: The distribution of lift values'
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.32：提升值的分布
- en: As mentioned previously, this plot shows that 50 is a high threshold value in
    that there are not many points above that value.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前所述，此图显示50是一个较高的阈值，因为在该值之上的点并不多。
- en: 'Now, look at the distribution of leverage:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，查看杠杆值的分布：
- en: '[PRE47]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output is as follows:'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.33: The distribution of leverage values'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.33：杠杆值的分布'
- en: '](img/C12626_08_33.jpg)'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_08_33.jpg)'
- en: 'Figure 8.33: The distribution of leverage values'
  id: totrans-375
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.33：杠杆值的分布
- en: 'Now, look at the distribution of conviction:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，查看定罪度的分布：
- en: '[PRE48]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output is as follows:'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.34: The distribution of conviction values'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.34：定罪度值的分布'
- en: '](img/C12626_08_34.jpg)'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_08_34.jpg)'
- en: 'Figure 8.34: The distribution of conviction values'
  id: totrans-381
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8.34：定罪度值的分布
- en: What is interesting about the four distributions is that spikes of varying sizes
    appear at the upper ends of the plots, implying that there are a few very strong
    association rules. The distribution of confidence tails off as the confidence
    values get larger, but, at the very end, around the highest values, the distribution
    jumps up a little. The lift distribution has the most obvious spike. The conviction
    distribution plot shows a small spike, perhaps more accurately described as a
    bump, around 50\. Lastly, the leverage distribution does not really show any spike
    in the higher values, but it does feature a long tail with some very high leverage
    values.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 四个分布中有趣的是，在图表的上端出现了不同大小的尖峰，这意味着有一些非常强的关联规则。随着置信度值增大，置信度的分布逐渐下降，但在最高值附近，分布略微上升。提升分布有最明显的尖峰。定罪度分布图在50左右有一个小的尖峰，也许更准确地说是一个小的突起。最后，杠杆分布在较高的值处并没有显著的尖峰，但它确实显示了一个长尾，包含一些非常高的杠杆值。
- en: Take some time to explore the association rules found by the model. Do the product
    pairings make sense to you? What happened to the number of association rules when
    you changed the model parameter values? Do you appreciate the impact that these
    rules would have when attempting to improve any retail business?
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 花一些时间探索模型发现的关联规则。产品配对对你有意义吗？当你改变模型参数值时，关联规则的数量发生了什么变化？你是否理解这些规则在尝试改善任何零售业务时可能产生的影响？
- en: 'Activity 20: Finding the Association Rules on the Complete Online Retail Dataset'
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 20：在完整的在线零售数据集上查找关联规则
- en: Let's pick up the scenario set out in *Activity 19* *Apriori on the Complete
    Online Retail Dataset*. The company leadership comes back to you and says it is
    great that we know how frequently each item set occurs in the dataset, but which
    item sets can we act upon? Which item sets can we use to change the store layout
    or adjust pricing? To find these answers, we derive the full association rules.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续在*活动 19* *完整在线零售数据集上的 Apriori* 中设定的场景。公司领导回来说，知道每个项集在数据集中出现的频率很好，但我们可以采取哪些项集的行动？哪些项集可以用来改变商店布局或调整定价？为了找到这些答案，我们推导出了完整的关联规则。
- en: In this activity, let's derive association rules from the complete online retail
    transaction dataset. Ensure that you complete this activity in the notebook that
    uses the full dataset (in other words, the notebook with the complete retail dataset,
    not the notebook from the exercises that uses the 5,000-item subset).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，我们将从完整的在线零售交易数据集推导关联规则。确保你在使用完整数据集的笔记本中完成此活动（换句话说，就是使用完整零售数据集的笔记本，而不是练习中使用的包含5,000个商品子集的笔记本）。
- en: 'These steps will help us to perform the activity:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助我们完成此活动：
- en: Fit the association rule model on the full dataset. Use metric confidence and
    a minimum threshold of 0.6.
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在完整数据集上拟合关联规则模型。使用指标置信度，并设定最小阈值为0.6。
- en: Count the number of association rules. Is the number different to that found
    in *step 1* of Exercise 45, *Deriving Association Rules*?
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算关联规则的数量。这个数量与*练习45的步骤1*中找到的数量是否不同？*推导关联规则*？
- en: Plot confidence against support.
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制置信度与支持度的关系图。
- en: Look at the distributions of confidence, lift, leverage, and conviction.
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看置信度、提升度、杠杆度和确信度的分布情况。
- en: Note
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 370.
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本次活动的解决方案可以在第370页找到。
- en: By the end of this activity, you will have a plot of lift, leverage, and conviction.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 到本次活动结束时，你将获得关于提升度、杠杆度和确信度的图表。
- en: Summary
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Market basket analysis is used to analyze and extract insights from transaction
    or transaction-like data that can be used to help drive growth in many industries,
    most famously the retail industry. These decisions can include how to layout the
    retail space, what products to discount, and how to price products. One of the
    central pillars of market basket analysis is the establishment of association
    rules. Association rule learning is a machine learning approach to uncovering
    the associations between the products individuals purchase that are strong enough
    to be leveraged in business decisions. Association rule learning relies on the
    Apriori algorithm to find frequent item sets in a computationally efficient way.
    These models are atypical of machine learning models because no prediction is
    being done, the results cannot really be evaluated using any one metric, and the
    parameter values are selected not by grid search, but by domain requirements specific
    to the question of interest. That being said, the goal of pattern extraction that
    is at the heart of all machine learning models is most definitely present here.
    At the conclusion of this chapter, you should feel comfortable evaluating and
    interpreting the probabilistic metrics, be able to run and adjust the Apriori
    algorithm and association rule learning model using `mlxtend`, and know how these
    models are applied in business. Know that there is a decent chance the positioning
    and pricing of items in your neighborhood grocery store were chosen based on the
    past actions made by you and many other customers in that store!
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 市场篮子分析用于分析和提取来自交易或类似交易的数据的见解，这些见解可以帮助推动多个行业的增长，最著名的就是零售行业。这些决策可能包括如何布置零售空间、打折哪些产品以及如何定价。市场篮子分析的核心支柱之一是建立关联规则。关联规则学习是一种机器学习方法，用于发现消费者购买商品之间足够强的关联，这些关联可以在商业决策中加以利用。关联规则学习依赖于Apriori算法，以计算高效的方式找到频繁项集。这些模型与传统的机器学习模型不同，因为它们不进行预测，结果不能通过单一指标来评估，且参数值不是通过网格搜索选择的，而是由特定问题的领域需求来决定。尽管如此，所有机器学习模型的核心目标——模式提取，在这里依然存在。在本章结束时，你应该能舒适地评估和解读概率指标，能够运行并调整使用`mlxtend`的Apriori算法和关联规则学习模型，并了解这些模型在商业中的应用。你应该知道，附近超市中的商品陈列和定价很可能是根据你和其他顾客过去的行为做出的决策！
- en: In the next chapter, we explore hotspot analysis using kernel density estimation,
    arguably one of the most frequently used algorithms in all of statistics and machine
    learning.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨使用核密度估计的热点分析，毫无疑问，这是所有统计学和机器学习中最常用的算法之一。
