- en: Chapter 12. Big Data Analysis (R and Hadoop)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章 大数据分析（R和Hadoop）
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Preparing the RHadoop environment
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备RHadoop环境
- en: Installing rmr2
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装rmr2
- en: Installing rhdfs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装rhdfs
- en: Operating HDFS with rhdfs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用rhdfs操作HDFS
- en: Implementing a word count problem with RHadoop
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RHadoop实现词频统计问题
- en: Comparing the performance between an R MapReduce program and a standard R program
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较R MapReduce程序和标准R程序的性能
- en: Testing and debugging the rmr2 program
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试和调试rmr2程序
- en: Installing plyrmr
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装plyrmr
- en: Manipulating data with plyrmr
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用plyrmr操作数据
- en: Conducting machine learning with RHadoop
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RHadoop进行机器学习
- en: Configuring RHadoop clusters on Amazon EMR
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Amazon EMR上配置RHadoop集群
- en: Introduction
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: RHadoop is a collection of R packages that enables users to process and analyze
    big data with Hadoop. Before understanding how to set up RHadoop and put it in
    to practice, we have to know why we need to use machine learning to big-data scale.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: RHadoop是一组R包，使用户能够使用Hadoop处理和分析大数据。在了解如何设置RHadoop并将其付诸实践之前，我们必须知道为什么我们需要使用机器学习来处理大数据规模。
- en: In the previous chapters, we have mentioned how useful R is when performing
    data analysis and machine learning. In traditional statistical analysis, the focus
    is to perform analysis on historical samples (small data), which may ignore rarely
    occurring but valuable events and results to uncertain conclusions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们提到了R在进行数据分析和机器学习时的有用性。在传统的统计分析中，重点是分析历史样本（小数据），这可能会忽略很少发生但很有价值的事件和结果，导致不确定的结论。
- en: The emergence of Cloud technology has made real-time interaction between customers
    and businesses much more frequent; therefore, the focus of machine learning has
    now shifted to the development of accurate predictions for various customers.
    For example, businesses can provide real-time personal recommendations or online
    advertisements based on personal behavior via the use of a real-time prediction
    model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 云技术的出现使得客户与业务之间的实时互动变得更加频繁；因此，机器学习的重点现在已转向为各种客户开发准确的预测。例如，企业可以通过使用实时预测模型，根据个人行为提供实时个性化推荐或在线广告。
- en: 'However, if the data (for example, behaviors of all online users) is too large
    to fit in the memory of a single machine, you have no choice but to use a supercomputer
    or some other scalable solution. The most popular scalable big-data solution is
    Hadoop, which is an open source framework able to store and perform parallel computations
    across clusters. As a result, you can use RHadoop, which allows R to leverage
    the scalability of Hadoop, helping to process and analyze big data. In RHadoop,
    there are five main packages, which are:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果数据（例如，所有在线用户的行为）太大，无法适应单台机器的内存，你就不得不使用超级计算机或其他可扩展的解决方案。最流行的可扩展大数据解决方案是Hadoop，它是一个开源框架，能够在集群之间存储和执行并行计算。因此，你可以使用RHadoop，它允许R利用Hadoop的可扩展性，帮助处理和分析大数据。在RHadoop中，有五个主要包，它们是：
- en: '`rmr`: This is an interface between R and Hadoop MapReduce, which calls the
    Hadoop streaming MapReduce API to perform MapReduce jobs across Hadoop clusters.
    To develop an R MapReduce program, you only need to focus on the design of the
    map and reduce functions, and the remaining scalability issues will be taken care
    of by Hadoop itself.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rmr`：这是R和Hadoop MapReduce之间的接口，通过调用Hadoop流式MapReduce API在Hadoop集群中执行MapReduce作业。要开发R
    MapReduce程序，你只需要关注map和reduce函数的设计，其余的可扩展性问题将由Hadoop本身处理。'
- en: '`rhdfs`: This is an interface between R and HDFS, which calls the HDFS API
    to access the data stored in HDFS. The use of `rhdfs` is very similar to the use
    of the Hadoop shell, which allows users to manipulate HDFS easily from the R console.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhdfs`：这是R和HDFS之间的接口，通过调用HDFS API来访问存储在HDFS中的数据。使用`rhdfs`的方式与使用Hadoop shell非常相似，它允许用户从R控制台轻松地操作HDFS。'
- en: '`rhbase`: This is an interface between R and HBase, which accesses Hbase and
    is distributed in clusters through a Thrift server. You can use `rhbase` to read/write
    data and manipulate tables stored within HBase.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rhbase`：这是R和HBase之间的接口，通过Thrift服务器在集群中访问HBase。你可以使用`rhbase`来读写数据并操作存储在HBase中的表。'
- en: '`plyrmr`: This is a higher-level abstraction of MapReduce, which allows users
    to perform common data manipulation in a plyr-like syntax. This package greatly
    lowers the learning curve of big-data manipulation.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plyrmr`：这是MapReduce的高级抽象，允许用户以plyr-like语法执行常见的数据操作。这个包大大降低了大数据操作的学习曲线。'
- en: '`ravro`: This allows users to read `avro` files in R, or write `avro` files.
    It allows R to exchange data with HDFS.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ravro`：这允许用户在R中读取`avro`文件或写入`avro`文件。它允许R与HDFS交换数据。'
- en: 'In this chapter, we will start by preparing the Hadoop environment, so that
    you can install RHadoop. We then cover the installation of three main packages:
    `rmr`, `rhdfs`, and `plyrmr`. Next, we will introduce how to use `rmr` to perform
    MapReduce from R, operate an HDFS file through `rhdfs`, and perform a common data
    operation using `plyrmr`. Further, we will explore how to perform machine learning
    using RHadoop. Lastly, we will introduce how to deploy multiple RHadoop clusters
    on Amazon EC2.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先准备Hadoop环境，以便你可以安装RHadoop。然后，我们将介绍三个主要包的安装：`rmr`、`rhdfs`和`plyrmr`。接下来，我们将介绍如何使用`rmr`从R执行MapReduce，通过`rhdfs`操作HDFS文件，并使用`plyrmr`执行常见的数据操作。进一步，我们将探讨如何使用RHadoop进行机器学习。最后，我们将介绍如何在Amazon
    EC2上部署多个RHadoop集群。
- en: Preparing the RHadoop environment
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备RHadoop环境
- en: As RHadoop requires an R and Hadoop integrated environment, we must first prepare
    an environment with both R and Hadoop installed. Instead of building a new Hadoop
    system, we can use the **Cloudera QuickStart VM** (the VM is free), which contains
    a single node Apache Hadoop Cluster and R. In this recipe, we will demonstrate
    how to download the Cloudera QuickStart VM.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于RHadoop需要一个R和Hadoop集成环境，我们必须首先准备一个安装了R和Hadoop的环境。我们不必构建一个新的Hadoop系统，可以使用**Cloudera
    QuickStart VM**（该VM免费），它包含一个节点Apache Hadoop集群和R。在这个菜谱中，我们将演示如何下载Cloudera QuickStart
    VM。
- en: Getting ready
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To use the Cloudera QuickStart VM, it is suggested that you should prepare a
    64-bit guest OS with either VMWare or VirtualBox, or the KVM installed.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Cloudera QuickStart VM，建议你准备一个64位虚拟机操作系统，安装VMWare或VirtualBox，或者安装KVM。
- en: 'If you choose to use VMWare, you should prepare a player compatible with WorkStation
    8.x or higher: Player 4.x or higher, ESXi 5.x or higher, or Fusion 4.x or higher.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择使用VMWare，你应该准备一个与WorkStation 8.x或更高版本兼容的播放器：4.x或更高版本，ESXi 5.x或更高版本，或者Fusion
    4.x或更高版本。
- en: Note, 4 GB of RAM is required to start VM, with an available disk space of at
    least 3 GB.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，启动VM需要4 GB的RAM，至少有3 GB的可用磁盘空间。
- en: How to do it...
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to set up a Hadoop environment using the Cloudera
    QuickStart VM:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Cloudera QuickStart VM设置Hadoop环境的以下步骤：
- en: Visit the Cloudera QuickStart VM download site (you may need to update the link
    as Cloudera upgrades its VMs , the current version of CDH is 5.3) at [http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-3-x.html](http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-3-x.html).![How
    to do it...](img/00267.jpeg)
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问Cloudera QuickStart VM下载站点（你可能需要更新链接，因为Cloudera升级了其VMs，当前CDH版本为5.3），请参阅[http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-3-x.html](http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-3-x.html)。![如何操作...](img/00267.jpeg)
- en: A screenshot of the Cloudera QuickStart VM download site
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Cloudera QuickStart VM下载站点的截图
- en: 'Depending on the virtual machine platform installed on your OS, choose the
    appropriate link (you may need to update the link as Cloudera upgrades its VMs)
    to download the VM file:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据你操作系统上安装的虚拟机平台，选择适当的链接（你可能需要更新链接，因为Cloudera升级了其VMs）以下载VM文件：
- en: '**To download VMWare**: You can visit [https://downloads.cloudera.com/demo_vm/vmware/cloudera-quickstart-vm-5.2.0-0-vmware.7z](https://downloads.cloudera.com/demo_vm/vmware/cloudera-quickstart-vm-5.2.0-0-vmware.7z)'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下载VMWare**：你可以访问[https://downloads.cloudera.com/demo_vm/vmware/cloudera-quickstart-vm-5.2.0-0-vmware.7z](https://downloads.cloudera.com/demo_vm/vmware/cloudera-quickstart-vm-5.2.0-0-vmware.7z)'
- en: '**To download KVM**: You can visit [https://downloads.cloudera.com/demo_vm/kvm/cloudera-quickstart-vm-5.2.0-0-kvm.7z](https://downloads.cloudera.com/demo_vm/kvm/cloudera-quickstart-vm-5.2.0-0-kvm.7z)'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下载KVM**：你可以访问[https://downloads.cloudera.com/demo_vm/kvm/cloudera-quickstart-vm-5.2.0-0-kvm.7z](https://downloads.cloudera.com/demo_vm/kvm/cloudera-quickstart-vm-5.2.0-0-kvm.7z)'
- en: '**To download VirtualBox**: You can visit [https://downloads.cloudera.com/demo_vm/virtualbox/cloudera-quickstart-vm-5.2.0-0-virtualbox.7z](https://downloads.cloudera.com/demo_vm/virtualbox/cloudera-quickstart-vm-5.2.0-0-virtualbox.7z)'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下载VirtualBox**：你可以访问[https://downloads.cloudera.com/demo_vm/virtualbox/cloudera-quickstart-vm-5.2.0-0-virtualbox.7z](https://downloads.cloudera.com/demo_vm/virtualbox/cloudera-quickstart-vm-5.2.0-0-virtualbox.7z)'
- en: Next, you can start the QuickStart VM using the virtual machine platform installed
    on your OS. You should see the desktop of Centos 6.2 in a few minutes.![How to
    do it...](img/00268.jpeg)
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您可以使用安装在您的操作系统上的虚拟机平台启动 QuickStart 虚拟机。您应该在几分钟内看到 Centos 6.2 的桌面。[如何操作...](img/00268.jpeg)
- en: The screenshot of Cloudera QuickStart VM.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Cloudera QuickStart 虚拟机的截图。
- en: You can then open a terminal and type `hadoop`, which will display a list of
    functions that can operate a Hadoop cluster.![How to do it...](img/00269.jpeg)
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以打开一个终端并输入 `hadoop`，这将显示可以操作 Hadoop 集群的一组功能。[如何操作...](img/00269.jpeg)
- en: The terminal screenshot after typing `hadoop`
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入 `hadoop` 后的终端截图
- en: 'Open a terminal and type `R`. Access an R session and check whether version
    3.1.1 is already installed in the Cloudera QuickStart VM. If you cannot find R
    installed in the VM, please use the following command to install R:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个终端并输入 `R`。访问 R 会话并检查版本 3.1.1 是否已在 Cloudera QuickStart 虚拟机中安装。如果您在虚拟机中找不到已安装的
    R，请使用以下命令安装 R：
- en: '[PRE0]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How it works...
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Instead of building a Hadoop system on your own, you can use the Hadoop VM application
    provided by Cloudera (the VM is free). The QuickStart VM runs on CentOS 6.2 with
    a single node Apache Hadoop cluster, Hadoop Ecosystem module, and R installed.
    This helps you to save time, instead of requiring you to learn how to install
    and use Hadoop.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 您不必自己构建 Hadoop 系统，可以使用 Cloudera 提供的 Hadoop VM 应用程序（虚拟机是免费的）。QuickStart 虚拟机在
    CentOS 6.2 上运行，包含单个节点 Apache Hadoop 集群、Hadoop 生态系统模块和已安装的 R。这可以帮助您节省时间，而不是需要您学习如何安装和使用
    Hadoop。
- en: The QuickStart VM requires you to have a computer with a 64-bit guest OS, at
    least 4 GB of RAM, 3 GB of disk space, and either VMWare, VirtualBox, or KVM installed.
    As a result, you may not be able to use this version of VM on some computers.
    As an alternative, you could consider using Amazon's Elastic MapReduce instead.
    We will illustrate how to prepare a RHadoop environment in EMR in the last recipe
    of this chapter.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: QuickStart 虚拟机要求您拥有一个具有 64 位客户操作系统的计算机，至少 4 GB 的 RAM、3 GB 的磁盘空间，并且已安装 VMWare、VirtualBox
    或 KVM。因此，您可能无法在某些计算机上使用此版本的虚拟机。作为替代方案，您可以考虑使用 Amazon 的 Elastic MapReduce。我们将在本章的最后一个小节中说明如何在
    EMR 中准备 RHadoop 环境。
- en: Setting up the Cloudera QuickStart VM is simple. Download the VM from the download
    site and then open the built image with either VMWare, VirtualBox, or KVM. Once
    you can see the desktop of CentOS, you can then access the terminal and type `hadoop`
    to see whether Hadoop is working; then, type `R` to see whether R works in the
    QuickStart VM.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 Cloudera QuickStart 虚拟机很简单。从下载站点下载虚拟机，然后使用 VMWare、VirtualBox 或 KVM 打开构建的镜像。一旦您可以看到
    CentOS 的桌面，您就可以访问终端并输入 `hadoop` 来查看 Hadoop 是否正在运行；然后，输入 `R` 来查看 R 是否在 QuickStart
    虚拟机中运行。
- en: See also
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: Besides using the Cloudera QuickStart VM, you may consider using a Sandbox VM
    provided by Hontonworks or MapR. You can find Hontonworks Sandbox at [http://hortonworks.com/products/hortonworks-sandbox/#install](http://hortonworks.com/products/hortonworks-sandbox/#install)
    and mapR Sandbox at [https://www.mapr.com/products/mapr-sandbox-hadoop/download](https://www.mapr.com/products/mapr-sandbox-hadoop/download).
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了使用 Cloudera QuickStart 虚拟机外，您还可以考虑使用由 Hontonworks 或 MapR 提供的沙盒虚拟机。您可以在 [http://hortonworks.com/products/hortonworks-sandbox/#install](http://hortonworks.com/products/hortonworks-sandbox/#install)
    找到 Hontonworks 沙盒，在 [https://www.mapr.com/products/mapr-sandbox-hadoop/download](https://www.mapr.com/products/mapr-sandbox-hadoop/download)
    找到 mapR 沙盒。
- en: Installing rmr2
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 rmr2
- en: The `rmr2` package allows you to perform big data processing and analysis via
    MapReduce on a Hadoop cluster. To perform MapReduce on a Hadoop cluster, you have
    to install R and `rmr2` on every task node. In this recipe, we will illustrate
    how to install `rmr2` on a single node of a Hadoop cluster.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`rmr2` 包允许您通过 Hadoop 集群上的 MapReduce 执行大数据处理和分析。要在 Hadoop 集群上执行 MapReduce，您必须在每个任务节点上安装
    R 和 `rmr2`。在本菜谱中，我们将说明如何在 Hadoop 集群的单个节点上安装 `rmr2`。'
- en: Getting ready
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Ensure that you have completed the previous recipe by starting the Cloudera
    QuickStart VM and connecting the VM to the Internet, so that you can proceed with
    downloading and installing the `rmr2` package.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您已完成了前面的菜谱，通过启动 Cloudera QuickStart 虚拟机并将其连接到互联网，以便您可以继续下载和安装 `rmr2` 包。
- en: How to do it...
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to install `rmr2` on the QuickStart VM:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤在 QuickStart 虚拟机上安装 `rmr2`：
- en: First, open the terminal within the Cloudera QuickStart VM.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在 Cloudera QuickStart 虚拟机内部打开终端。
- en: 'Use the permission of the root to enter an R session:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 root 权限进入 R 会话：
- en: '[PRE1]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can then install dependent packages before installing `rmr2`:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以在安装 `rmr2` 之前安装依赖包：
- en: '[PRE2]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Quit the R session:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 退出 R 会话：
- en: '[PRE3]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, you can download `rmr-3.3.0` to the QuickStart VM. You may need to update
    the link if Revolution Analytics upgrades the version of `rmr2`:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您可以将 `rmr-3.3.0` 下载到 QuickStart VM 上。如果 Revolution Analytics 升级了 `rmr2`
    的版本，您可能需要更新链接：
- en: '[PRE4]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can then install `rmr-3.3.0` to the QuickStart VM:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以将 `rmr-3.3.0` 安装到 QuickStart VM 上：
- en: '[PRE5]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Lastly, you can enter an R session and use the `library` function to test whether
    the library has been successfully installed:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您可以进入 R 会话并使用 `library` 函数来测试库是否已成功安装：
- en: '[PRE6]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: How it works...
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In order to perform MapReduce on a Hadoop cluster, you have to install R and
    RHadoop on every task node. Here, we illustrate how to install `rmr2` on a single
    node of a Hadoop cluster. First, open the terminal of the Cloudera QuickStart
    VM. Before installing `rmr2`, we first access an R session with root privileges
    and install dependent R packages.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 Hadoop 集群上执行 MapReduce，您必须在每个任务节点上安装 R 和 RHadoop。在此，我们将说明如何在 Hadoop 集群的单个节点上安装
    `rmr2`。首先，打开 Cloudera QuickStart VM 的终端。在安装 `rmr2` 之前，我们首先以 root 权限访问 R 会话并安装依赖的
    R 包。
- en: Next, after all the dependent packages are installed, quit the R session and
    use the `wget` command in the Linux shell to download `rmr-3.3.0` from GitHub
    to the local filesystem. You can then begin the installation of `rmr2`. Lastly,
    you can access an R session and use the library function to validate whether the
    package has been installed.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在所有依赖包安装完成后，退出 R 会话，并在 Linux shell 中使用 `wget` 命令从 GitHub 下载 `rmr-3.3.0`
    到本地文件系统。然后，您可以开始安装 `rmr2`。最后，您可以通过 R 会话使用库函数来验证包是否已安装。
- en: See also
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考信息
- en: 'To see more information and read updates about RHadoop, you can refer to the
    RHadoop wiki page hosted on GitHub: [https://github.com/RevolutionAnalytics/RHadoop/wiki](https://github.com/RevolutionAnalytics/RHadoop/wiki)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要获取更多信息和阅读关于 RHadoop 的更新，您可以参考 GitHub 上托管的 RHadoop 维基页面：[https://github.com/RevolutionAnalytics/RHadoop/wiki](https://github.com/RevolutionAnalytics/RHadoop/wiki)
- en: Installing rhdfs
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 rhdfs
- en: The `rhdfs` package is the interface between R and HDFS, which allows users
    to access HDFS from an R console. Similar to `rmr2`, one should install `rhdfs`
    on every task node, so that one can access HDFS resources through R. In this recipe,
    we will introduce how to install `rhdfs` on the Cloudera QuickStart VM.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`rhdfs` 包是 R 和 HDFS 之间的接口，它允许用户从 R 控制台访问 HDFS。类似于 `rmr2`，应该在每个任务节点上安装 `rhdfs`，以便可以通过
    R 控制台访问 HDFS 资源。在本菜谱中，我们将介绍如何在 Cloudera QuickStart VM 上安装 `rhdfs`。'
- en: Getting ready
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Ensure that you have completed the previous recipe by starting the Cloudera
    QuickStart VM and connecting the VM to the Internet, so that you can proceed with
    downloading and installing the `rhdfs` package.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您已通过启动 Cloudera QuickStart VM 并将其连接到互联网来完成前面的菜谱，这样您就可以继续下载和安装 `rhdfs` 包。
- en: How to do it...
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to install `rhdfs`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来安装 `rhdfs`：
- en: 'First, you can download `rhdfs 1.0.8` from GitHub. You may need to update the
    link if Revolution Analytics upgrades the version of `rhdfs`:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您可以从 GitHub 下载 `rhdfs 1.0.8`。如果 Revolution Analytics 升级了 `rhdfs` 的版本，您可能需要更新链接：
- en: '[PRE7]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, you can install `rhdfs` under the command-line mode:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您可以在命令行模式下安装 `rhdfs`：
- en: '[PRE8]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can then set up `JAVA_HOME`. The configuration of `JAVA_HOME` depends on
    the installed Java version within the VM:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以设置 `JAVA_HOME`。`JAVA_HOME` 的配置取决于 VM 内安装的 Java 版本：
- en: '[PRE9]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Last, you can set up the system environment and initialize `rhdfs`. You may
    need to update the environment setup if you use a different version of QuickStart
    VM:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您可以设置系统环境和初始化 `rhdfs`。如果您使用的是不同版本的 QuickStart VM，可能需要更新环境设置：
- en: '[PRE10]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: How it works...
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The package, `rhdfs`, provides functions so that users can manage HDFS using
    R. Similar to `rmr2`, you should install `rhdfs` on every task node, so that one
    can access HDFS through the R console.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 包 `rhdfs` 提供了函数，使用户可以使用 R 管理HDFS。类似于 `rmr2`，您应该在每个任务节点上安装 `rhdfs`，以便可以通过 R 控制台访问
    HDFS。
- en: To install `rhdfs`, you should first download `rhdfs` from GitHub. You can then
    install `rhdfs` in R by specifying where the `HADOOP_CMD` is located. You must
    configure R with Java support through the command, `javareconf`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 `rhdfs`，您首先应从 GitHub 下载 `rhdfs`。然后，您可以通过指定 `HADOOP_CMD` 的位置来在 R 中安装 `rhdfs`。您必须通过命令
    `javareconf` 配置 R 以支持 Java。
- en: Next, you can access R and configure where `HADOOP_CMD` and `HADOOP_STREAMING`
    are located. Lastly, you can initialize `rhdfs` via the `rhdfs.init` function,
    which allows you to begin operating HDFS through `rhdfs`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您可以访问 R 并配置 `HADOOP_CMD` 和 `HADOOP_STREAMING` 的位置。最后，您可以通过 `rhdfs.init`
    函数初始化 `rhdfs`，这允许您通过 `rhdfs` 开始操作 HDFS。
- en: See also
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: To find where `HADOOP_CMD` is located, you can use the `which hadoop` command
    in the Linux shell. In most Hadoop systems, `HADOOP_CMD` is located at `/usr/bin/hadoop`.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要查找 `HADOOP_CMD` 的位置，您可以在 Linux shell 中使用 `which hadoop` 命令。在大多数 Hadoop 系统中，`HADOOP_CMD`
    位于 `/usr/bin/hadoop`。
- en: 'As for the location of `HADOOP_STREAMING`, the streaming JAR file is often
    located in `/usr/lib/hadoop-mapreduce/`. However, if you cannot find the directory,
    `/usr/lib/Hadoop-mapreduce`, in your Linux system, you can search the streaming
    JAR by using the `locate` command. For example:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 `HADOOP_STREAMING` 的位置，流 JAR 文件通常位于 `/usr/lib/hadoop-mapreduce/`。但是，如果您在
    Linux 系统中找不到 `/usr/lib/Hadoop-mapreduce` 目录，您可以使用 `locate` 命令搜索流 JAR。例如：
- en: '[PRE11]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Operating HDFS with rhdfs
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 rhdfs 操作 HDFS
- en: The `rhdfs` package is an interface between Hadoop and R, which can call an
    HDFS API in the backend to operate HDFS. As a result, you can easily operate HDFS
    from the R console through the use of the `rhdfs` package. In the following recipe,
    we will demonstrate how to use the `rhdfs` function to manipulate HDFS.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`rhdfs` 包是 Hadoop 和 R 之间的接口，可以在后端调用 HDFS API 来操作 HDFS。因此，您可以通过使用 `rhdfs` 包轻松地从
    R 控制台操作 HDFS。在下面的菜谱中，我们将演示如何使用 `rhdfs` 函数操作 HDFS。'
- en: Getting ready
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To proceed with this recipe, you need to have completed the previous recipe
    by installing `rhdfs` into R, and validate that you can initial HDFS via the `hdfs.init`
    function.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 要继续本菜谱，您需要完成之前的菜谱，将 `rhdfs` 安装到 R 中，并验证您可以通过 `hdfs.init` 函数初始化 HDFS。
- en: How to do it...
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps to operate files stored on HDFS:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以操作存储在 HDFS 上的文件：
- en: 'Initialize the `rhdfs` package:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 `rhdfs` 包：
- en: '[PRE12]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can then manipulate files stored on HDFS, as follows:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以按照以下方式操作存储在 HDFS 上的文件：
- en: '`hdfs.put`: Copy a file from the local filesystem to HDFS:'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.put`: 从本地文件系统复制文件到 HDFS：'
- en: '[PRE13]'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`hdfs.ls`: Read the list of directory from HDFS:'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.ls`: 读取 HDFS 中的目录列表：'
- en: '[PRE14]'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`hdfs.copy`: Copy a file from one HDFS directory to another:'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.copy`: 将文件从一个 HDFS 目录复制到另一个目录：'
- en: '[PRE15]'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`hdfs.move` : Move a file from one HDFS directory to another:'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.move`：将文件从一个 HDFS 目录移动到另一个目录：'
- en: '[PRE16]'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`hdfs.delete`: Delete an HDFS directory from R:'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.delete`: 从 R 删除 HDFS 目录：'
- en: '[PRE17]'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`hdfs.rm`: Delete an HDFS directory from R:'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.rm`: 从 R 删除 HDFS 目录：'
- en: '[PRE18]'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`hdfs.get`: Download a file from HDFS to a local filesystem:'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.get`: 从 HDFS 下载文件到本地文件系统：'
- en: '[PRE19]'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`hdfs.rename`: Rename a file stored on HDFS:'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.rename`: 重命名 HDFS 上存储的文件：'
- en: '[PRE20]'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`hdfs.chmod`: Change the permissions of a file or directory:'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.chmod`: 更改文件或目录的权限：'
- en: '[PRE21]'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`hdfs.file.info`: Read the meta information of the HDFS file:'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs.file.info`: 读取 HDFS 文件的元信息：'
- en: '[PRE22]'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Also, you can write stream to the HDFS file:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，您还可以将流写入 HDFS 文件：
- en: '[PRE23]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Lastly, you can read stream from the HDFS file:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您还可以从 HDFS 文件中读取流：
- en: '[PRE24]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: How it works...
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we demonstrate how to manipulate HDFS using the `rhdfs` package.
    Normally, you can use the Hadoop shell to manipulate HDFS, but if you would like
    to access HDFS from R, you can use the `rhdfs` package.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，我们演示如何使用 `rhdfs` 包操作 HDFS。通常，您可以使用 Hadoop shell 操作 HDFS，但如果您想从 R 访问 HDFS，则可以使用
    `rhdfs` 包。
- en: Before you start using `rhdfs`, you have to initialize `rhdfs` with `hdfs.init()`.
    After initialization, you can operate HDFS through the functions provided in the
    `rhdfs` package.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用 `rhdfs` 之前，您必须使用 `hdfs.init()` 初始化 `rhdfs`。初始化后，您可以通过 `rhdfs` 包中提供的函数操作
    HDFS。
- en: Besides manipulating HDFS files, you can exchange streams to HDFS through `hdfs.read`
    and `hdfs.write`. We, therefore, demonstrate how to write a data frame in R to
    an HDFS file, `iris.txt`, using `hdfs.write`. Lastly, you can recover the written
    file back to the data frame using the `hdfs.read` function and the `unserialize`
    function.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 除了操作 HDFS 文件外，您还可以通过 `hdfs.read` 和 `hdfs.write` 将流交换到 HDFS。因此，我们将演示如何使用 `hdfs.write`
    将 R 中的数据框写入 HDFS 文件，`iris.txt`。最后，您可以使用 `hdfs.read` 函数和 `unserialize` 函数将写入的文件恢复到数据框。
- en: See also
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: To initialize `rhdfs`, you have to set `HADOOP_CMD` and `HADOOP_STREAMING` in
    the system environment. Instead of setting the configuration each time you're
    using `rhdfs`, you can put the configurations in the `.rprofile` file. Therefore,
    every time you start an R session, the configuration will be automatically loaded.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要初始化`rhdfs`，您必须在系统环境中设置`HADOOP_CMD`和`HADOOP_STREAMING`。您不必每次使用`rhdfs`时都设置配置，可以将配置放在`.rprofile`文件中。因此，每次您启动一个R会话时，配置将自动加载。
- en: Implementing a word count problem with RHadoop
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RHadoop实现单词计数问题
- en: To demonstrate how MapReduce works, we illustrate the example of a word count,
    which counts the number of occurrences of each word in a given input set. In this
    recipe, we will demonstrate how to use `rmr2` to implement a word count problem.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示MapReduce是如何工作的，我们以单词计数为例，它计算给定输入集中每个单词的出现次数。在这个菜谱中，我们将演示如何使用`rmr2`实现单词计数问题。
- en: Getting ready
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will need an input file as our word count program input.
    You can download the example input from [https://github.com/ywchiu/ml_R_cookbook/tree/master/CH12](https://github.com/ywchiu/ml_R_cookbook/tree/master/CH12).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们需要一个输入文件作为我们的单词计数程序输入。您可以从[https://github.com/ywchiu/ml_R_cookbook/tree/master/CH12](https://github.com/ywchiu/ml_R_cookbook/tree/master/CH12)下载示例输入文件。
- en: How to do it...
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps to implement the word count program:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以实现单词计数程序：
- en: 'First, you need to configure the system environment, and then load `rmr2` and
    `rhdfs` into an R session. You may need to update the use of the JAR file if you
    use a different version of QuickStart VM:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您需要配置系统环境，然后将`rmr2`和`rhdfs`加载到一个R会话中。如果您使用的是不同版本的QuickStart VM，可能需要更新JAR文件的使用：
- en: '[PRE25]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You can then create a directory on HDFS and put the input file into the newly
    created directory:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以在HDFS上创建一个目录并将输入文件放入新创建的目录中：
- en: '[PRE26]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, you can create a `map` function:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您可以创建一个`map`函数：
- en: '[PRE27]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Create a `reduce` function:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`reduce`函数：
- en: '[PRE28]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Call the `MapReduce` program to count the words within a document:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用MapReduce程序来计数文档中的单词：
- en: '[PRE29]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Lastly, you can retrieve the top 10 occurring words within the document:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您可以检索文档中出现的最频繁的10个单词：
- en: '[PRE30]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: How it works...
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we demonstrate how to implement a word count using the `rmr2`
    package. First, we need to configure the system environment and load `rhdfs` and
    `rmr2` into R. Then, we specify the input of our word count program from the local
    filesystem into the HDFS directory, `/user/cloudera/wordcount/data`, via the `hdfs.put`
    function.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们演示了如何使用`rmr2`包实现单词计数。首先，我们需要配置系统环境并将`rhdfs`和`rmr2`加载到R中。然后，我们通过`hdfs.put`函数将我们的单词计数程序的输入从本地文件系统指定到HDFS目录`/user/cloudera/wordcount/data`。
- en: Next, we begin implementing the MapReduce program. Normally, we can divide the
    MapReduce program into the `map` and `reduce` functions. In the `map` function,
    we first use the `strsplit` function to split each line into words. Then, as the
    `strsplit` function returns a list of words, we can use the `unlist` function
    to character vectors. Lastly, we can return key-value pairs with each word as
    a key and the value as one. As the `reduce` function receives the key-value pair
    generated from the `map` function, the `reduce` function sums the count and returns
    the number of occurrences of each word (or key).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们开始实现MapReduce程序。通常，我们可以将MapReduce程序分为`map`和`reduce`函数。在`map`函数中，我们首先使用`strsplit`函数将每一行拆分成单词。然后，由于`strsplit`函数返回一个单词列表，我们可以使用`unlist`函数将字符向量转换为列表。最后，我们可以返回键值对，其中每个单词作为键，值为一。由于`reduce`函数接收来自`map`函数生成的键值对，`reduce`函数将计数求和并返回每个单词（或键）的出现次数。
- en: After we have implemented the `map` and `reduce` functions, we can submit our
    job via the `mapreduce` function. Normally, the `mapreduce` function requires
    four inputs, which are the HDFS input path, the HDFS output path, the map function,
    and the reduce function. In this case, we specify the input as `wordcount/data`,
    output as `wordcount/out`, mapfunction as `map`, reduce function as `reduce`,
    and wrap the `mapreduce` call in function, `wordcount`. Lastly, we call the function,
    `wordcount` and store the output path in the variable, `out`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现了`map`和`reduce`函数之后，我们可以通过`mapreduce`函数提交我们的作业。通常，`mapreduce`函数需要四个输入，分别是HDFS输入路径、HDFS输出路径、map函数和reduce函数。在这种情况下，我们指定输入为`wordcount/data`，输出为`wordcount/out`，mapfunction为`map`，reduce
    function为`reduce`，并将`mapreduce`调用封装在`wordcount`函数中。最后，我们调用函数`wordcount`并将输出路径存储在变量`out`中。
- en: We can use the `from.dfs` function to load the HDFS data into the `results`
    variable, which contains the mapping of words and number of occurrences. We can
    then generate the top 10 occurring words from the `results` variable.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`from.dfs`函数将HDFS数据加载到`results`变量中，该变量包含单词和出现次数的映射。然后我们可以从`results`变量中生成出现次数最多的前10个单词。
- en: See also
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In this recipe, we demonstrate how to write an R MapReduce program to solve
    a word count problem. However, if you are interested in how to write a native
    Java MapReduce program, you can refer to [http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html](http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html).
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们展示了如何编写一个R MapReduce程序来解决单词计数问题。然而，如果你对如何编写本地的Java MapReduce程序感兴趣，你可以参考[http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html](http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)。
- en: Comparing the performance between an R MapReduce program and a standard R program
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较R MapReduce程序和标准R程序的性能
- en: Those not familiar with how Hadoop works may often see Hadoop as a remedy for
    big data processing. Some might believe that Hadoop can return the processed results
    for any size of data within a few milliseconds. In this recipe, we will compare
    the performance between an R MapReduce program and a standard R program to demonstrate
    that Hadoop does not perform as quickly as some may believe.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不熟悉Hadoop工作原理的人来说，他们可能会经常将Hadoop视为大数据处理的一种补救措施。有些人可能认为Hadoop可以在几毫秒内为任何大小的数据返回处理结果。在这个菜谱中，我们将比较R
    MapReduce程序和标准R程序的性能，以证明Hadoop的运行速度并不像一些人认为的那样快。
- en: Getting ready
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, you should have completed the previous recipe by installing
    `rmr2` into the R environment.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，你应该通过将`rmr2`安装到R环境中来完成之前的菜谱。
- en: How to do it...
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps to compare the performance of a standard R program
    and an R MapReduce program:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来比较标准R程序和R MapReduce程序的性能：
- en: 'First, you can implement a standard R program to have all numbers squared:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你可以实现一个标准R程序来将所有数字平方：
- en: '[PRE31]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To compare the performance, you can implement an R MapReduce program to have
    all numbers squared:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了比较性能，你可以实现一个R MapReduce程序来将所有数字平方：
- en: '[PRE32]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: How it works...
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we implement two programs to square all the numbers. In the
    first program, we use a standard R function, `sapply`, to square the sequence
    from 1 to 100,000\. To record the program execution time, we first record the
    processing time before the execution in `a.time`, and then subtract `a.time` from
    the current processing time after the execution. Normally, the execution takes
    no more than 10 seconds. In the second program, we use the `rmr2` package to implement
    a program in the R MapReduce version. In this program, we also record the execution
    time. Normally, this program takes a few minutes to complete a task.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们实现了两个程序来平方所有数字。在第一个程序中，我们使用标准的R函数``sapply`来平方从1到100,000的序列。为了记录程序执行时间，我们在执行前首先记录处理时间到`a.time`，然后从执行后的当前处理时间中减去`a.time`。通常，执行时间不超过10秒。在第二个程序中，我们使用`rmr2`包在R
    MapReduce版本中实现程序。在这个程序中，我们也记录了执行时间。通常，这个程序需要几分钟来完成一个任务。
- en: The performance comparison shows that a standard R program outperforms the MapReduce
    program when processing small amounts of data. This is because a Hadoop system
    often requires time to spawn daemons, job coordination between daemons, and fetching
    data from data nodes. Therefore, a MapReduce program often takes a few minutes
    to a couple of hours to finish the execution. As a result, if you can fit your
    data in the memory, you should write a standard R program to solve the problem.
    Otherwise, if the data is too large to fit in the memory, you can implement a
    MapReduce solution.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 性能比较显示，当处理少量数据时，标准R程序的性能优于MapReduce程序。这是因为Hadoop系统通常需要时间来启动守护进程，守护进程之间的作业协调，以及从数据节点获取数据。因此，MapReduce程序通常需要几分钟到几个小时来完成执行。因此，如果你可以将你的数据放入内存中，你应该编写一个标准R程序来解决问题。否则，如果数据太大而无法放入内存，你可以实现一个MapReduce解决方案。
- en: See also
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'In order to check whether a job will run smoothly and efficiently in Hadoop,
    you can run a MapReduce benchmark, MRBench, to evaluate the performance of the
    job:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了检查一个作业在Hadoop中是否能够顺利高效地运行，你可以运行一个MapReduce基准测试，MRBench，来评估作业的性能：
- en: '[PRE33]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Testing and debugging the rmr2 program
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试和调试rmr2程序
- en: Since running a MapReduce program will require a considerable amount of time,
    varying from a few minutes to several hours, testing and debugging become very
    important. In this recipe, we will illustrate some techniques you can use to troubleshoot
    an R MapReduce program.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于运行 MapReduce 程序将需要相当多的时间，从几分钟到几小时不等，因此测试和调试变得非常重要。在这个菜谱中，我们将展示您可以用来调试 R MapReduce
    程序的一些技术。
- en: Getting ready
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, you should have completed the previous recipe by installing
    `rmr2` into an R environment.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，您应该通过将 `rmr2` 安装到 R 环境中来完成前面的菜谱。
- en: How to do it...
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to test and debug an R MapReduce program:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以测试和调试 R MapReduce 程序：
- en: 'First, you can configure the backend as local in `rmr.options`:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您可以在 `rmr.options` 中将后端配置为本地：
- en: '[PRE34]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Again, you can execute the number squared MapReduce program mentioned in the
    previous recipe:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，您可以执行前面菜谱中提到的数字平方 MapReduce 程序：
- en: '[PRE35]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In addition to this, if you want to print the structure information of any
    variable in the MapReduce program, you can use the `rmr.str` function:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，如果您想在 MapReduce 程序中打印任何变量的结构信息，您可以使用 `rmr.str` 函数：
- en: '[PRE36]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: How it works...
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we introduced some debugging and testing techniques you can
    use while implementing the MapReduce program. First, we introduced the technique
    to test a MapReduce program in a local mode. If you would like to run the MapReduce
    program in a pseudo distributed or fully distributed mode, it would take you a
    few minutes to several hours to complete the task, which would involve a lot of
    wastage of time while troubleshooting your MapReduce program. Therefore, you can
    set the backend to the local mode in `rmr.options` so that the program will be
    executed in the local mode, which takes lesser time to execute.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们介绍了一些在实现 MapReduce 程序时可以使用的调试和测试技术。首先，我们介绍了在本地模式下测试 MapReduce 程序的技术。如果您想以伪分布式或完全分布式模式运行
    MapReduce 程序，这将花费您几分钟到几小时的时间来完成，这将在调试您的 MapReduce 程序时造成大量时间的浪费。因此，您可以在 `rmr.options`
    中将后端设置为本地模式，这样程序将以本地模式执行，执行时间更短。
- en: Another debugging technique is to list the content of the variable within the
    `map` or `reduce` function. In an R program, you can use the `str` function to
    display the compact structure of a single variable. In `rmr2`, the package also
    provides a function named `rmr.str`, which allows you to print out the content
    of a single variable onto the console. In this example, we use `rmr.str` to print
    the content of variables within a MapReduce program.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种调试技术是在 `map` 或 `reduce` 函数中列出变量的内容。在 R 程序中，您可以使用 `str` 函数显示单个变量的紧凑结构。在 `rmr2`
    中，该包还提供了一个名为 `rmr.str` 的函数，允许您将单个变量的内容打印到控制台。在这个例子中，我们使用 `rmr.str` 来打印 MapReduce
    程序中变量的内容。
- en: See also
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: 'For those who are interested in the `option` settings for the `rmr2` package,
    you can refer to the help document of `rmr.options`:'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于那些对 `rmr2` 包的 `option` 设置感兴趣的人，您可以参考 `rmr.options` 的帮助文档：
- en: '[PRE37]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Installing plyrmr
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 plyrmr
- en: The `plyrmr` package provides common operations (as found in `plyr` or `reshape2`)
    for users to easily perform data manipulation through the MapReduce framework.
    In this recipe, we will introduce how to install `plyrmr` on the Hadoop system.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`plyrmr` 包为用户提供了一些常见操作（如在 `plyr` 或 `reshape2` 中找到的操作），以便用户能够通过 MapReduce 框架轻松进行数据处理。在这个菜谱中，我们将介绍如何在
    Hadoop 系统上安装 `plyrmr`。'
- en: Getting ready
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Ensure that you have completed the previous recipe by starting the Cloudera
    QuickStart VM and connecting the VM to the Internet. Also, you need to have the
    `rmr2` package installed beforehand.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 通过启动 Cloudera QuickStart 虚拟机并将其连接到互联网，确保您已经完成了前面的菜谱。此外，您需要事先安装 `rmr2` 包。
- en: How to do it...
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to install `plyrmr` on the Hadoop system:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤在 Hadoop 系统上安装 `plyrmr`：
- en: 'First, you should install `libxml2-devel` and `curl-devel` in the Linux shell:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您应该在 Linux 命令行中安装 `libxml2-devel` 和 `curl-devel`：
- en: '[PRE38]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You can then access R and install the dependent packages:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以访问 R 并安装依赖包：
- en: '[PRE39]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Next, you can download `plyrmr 0.5.0` and install it on Hadoop VM. You may
    need to update the link if Revolution Analytics upgrades the version of `plyrmr`:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您可以在 Hadoop 虚拟机上下载 `plyrmr 0.5.0` 并安装它。如果 Revolution Analytics 升级了 `plyrmr`
    的版本，您可能需要更新链接：
- en: '[PRE40]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Lastly, validate the installation:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，验证安装：
- en: '[PRE41]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: How it works...
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Besides writing an R MapReduce program using the `rmr2` package, you can use
    the `plyrmr` to manipulate data. The `plyrmr` package is similar to hive and pig
    in the Hadoop ecosystem, which is the abstraction of the MapReduce program. Therefore,
    we can implement an R MapReduce program in `plyr` style instead of implementing
    the `map` f and `reduce` functions.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用`rmr2`包编写R MapReduce程序外，您还可以使用`plyrmr`来操作数据。`plyrmr`包类似于Hadoop生态系统中的hive和pig，它是MapReduce程序的高级抽象。因此，我们可以以`plyr`风格实现R
    MapReduce程序，而不是实现`map`和`reduce`函数。
- en: To install `plyrmr`, first install the package of `libxml2-devel` and `curl-devel`,
    using the `yum install` command. Then, access R and install the dependent packages.
    Lastly, download the file from GitHub and install `plyrmr` in R.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装`plyrmr`，首先使用`yum install`命令安装`libxml2-devel`和`curl-devel`包。然后，进入R并安装依赖包。最后，从GitHub下载文件并在R中安装`plyrmr`。
- en: See also
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: 'To read more information about `plyrmr`, you can use the `help` function to
    refer to the following document:'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要获取有关`plyrmr`的更多信息，您可以使用`help`函数参考以下文档：
- en: '[PRE42]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Manipulating data with plyrmr
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用plyrmr操作数据
- en: While writing a MapReduce program with `rmr2` is much easier than writing a
    native Java version, it is still hard for nondevelopers to write a MapReduce program.
    Therefore, you can use `plyrmr`, a high-level abstraction of the MapReduce program,
    so that you can use plyr-like operations to manipulate big data. In this recipe,
    we will introduce some operations you can use to manipulate data.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用`rmr2`编写MapReduce程序比编写原生Java版本要容易得多，但对于非开发者来说，编写MapReduce程序仍然很困难。因此，您可以使用`plyrmr`，它是MapReduce程序的高级抽象，这样您就可以使用类似于plyr的操作来操作大数据。在本菜谱中，我们将介绍一些您可以用来操作数据的操作。
- en: Getting ready
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, you should have completed the previous recipes by installing
    `plyrmr` and `rmr2` in R.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，您应该已经通过在R中安装`plyrmr`和`rmr2`来完成之前的菜谱。
- en: How to do it...
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to manipulate data with `plyrmr`:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤使用`plyrmr`操作数据：
- en: 'First, you need to load both `plyrmr` and `rmr2` into R:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您需要将`plyrmr`和`rmr2`都加载到R中：
- en: '[PRE43]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You can then set the execution mode to the local mode:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以将执行模式设置为本地模式：
- en: '[PRE44]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Next, load the Titanic dataset into R:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，将泰坦尼克号数据集加载到R中：
- en: '[PRE45]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Begin the operation by filtering the data:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过过滤数据开始操作：
- en: '[PRE46]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'You can also use a pipe operator to filter the data:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以使用管道操作符过滤数据：
- en: '[PRE47]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Put the Titanic data into HDFS and load the path of the data to the variable,
    `tidata`:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将泰坦尼克号数据放入HDFS，并将数据路径加载到变量`tidata`中：
- en: '[PRE48]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Next, you can generate a summation of the frequency from the Titanic data:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您可以从泰坦尼克号数据中生成频率的汇总：
- en: '[PRE49]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'You can also group the frequency by sex:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以按性别分组频率：
- en: '[PRE50]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'You can then sample 10 records out of the population:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以从总体中抽取10条记录作为样本：
- en: '[PRE51]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In addition to this, you can use plyrmr to join two datasets:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，您还可以使用plyrmr将两个数据集连接起来：
- en: '[PRE52]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: How it works...
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we introduce how to use `plyrmr` to manipulate data. First,
    we need to load the `plyrmr` package into R. Then, similar to `rmr2`, you have
    to set the backend option of `plyrmr` as the local mode. Otherwise, you will have
    to wait anywhere between a few minutes to several hours if `plyrmr` is running
    on Hadoop mode (the default setting).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，我们介绍如何使用`plyrmr`操作数据。首先，我们需要将`plyrmr`包加载到R中。然后，类似于`rmr2`，您必须将`plyrmr`的后端选项设置为本地模式。否则，如果`plyrmr`在Hadoop模式下运行（默认设置），您可能需要等待几分钟到几个小时。
- en: Next, we can begin the data manipulation with data filtering. You can choose
    to call the function nested inside the other function call in step 4\. On the
    other hand, you can use the pipe operator, `%|%`, to chain multiple operations.
    Therefore, we can filter data similar to step 4, using pipe operators in step
    5.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以开始使用数据过滤进行数据操作。您可以选择在步骤4中调用嵌套在其他函数调用中的函数。另一方面，您可以使用管道操作符`%|%`来链式执行多个操作。因此，我们可以在步骤5中使用管道操作符执行类似于步骤4的数据过滤。
- en: Next, you can input the dataset into either the HDFS or local filesystem, using
    `to.dfs` in accordance with the current running mode. The function will generate
    the path of the dataset and save it in the variable, `tidata`. By knowing the
    path, you can access the data using the `input` function. Next, we illustrate
    how to generate a summation of the frequency from the Titanic dataset with the
    `transmute` and `sum` functions. Also, `plyrmr` allows users to sum up the frequency
    by gender.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以使用`to.dfs`根据当前运行模式将数据集输入到HDFS或本地文件系统。该函数将生成数据集的路径并将其保存在变量`tidata`中。通过知道路径，你可以使用`input`函数访问数据。接下来，我们将展示如何使用`transmute`和`sum`函数从泰坦尼克号数据集中生成频率的总和。此外，`plyrmr`允许用户按性别汇总频率。
- en: Additionally, in order to sample data from a population, you can also use the
    `sample` function to select 10 records out of the Titanic dataset. Lastly, we
    demonstrate how to join two datasets using the `merge` function from `plyrmr`.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了从总体中采样数据，你也可以使用`sample`函数从泰坦尼克号数据集中选择10条记录。最后，我们演示了如何使用`plyrmr`的`merge`函数合并两个数据集。
- en: See also
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'Here we list some functions that can be used to manipulate data with `plyrmr`.
    You may refer to the `help` function for further details on their usage and functionalities:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们列出了一些可以使用`plyrmr`来操作数据的函数。你可以参考`help`函数以获取它们的使用和功能的更多详细信息：
- en: 'Data manipulation:'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据操作：
- en: '`bind.cols`: This adds new columns'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bind.cols`: 这用于添加新列'
- en: '`select`: This is used to select columns'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`select`: 这用于选择列'
- en: '`where`: This is used to select rows'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`where`: 这用于选择行'
- en: '`transmute`: This uses all of the above plus their summaries'
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transmute`: 这使用上述所有内容及其汇总'
- en: 'From `reshape2`:'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从`reshape2`：
- en: '`melt` and `dcast`: It converts long and wide data frames'
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`melt`和`dcast`：它将长宽数据框进行转换'
- en: 'Summary:'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要：
- en: '`count`'
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count`'
- en: '`quantile`'
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`quantile`'
- en: '`sample`'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample`'
- en: 'Extract:'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取：
- en: '`top.k`'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top.k`'
- en: '`bottom.k`'
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bottom.k`'
- en: Conducting machine learning with RHadoop
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RHadoop进行机器学习
- en: In the previous chapters, we have demonstrated how powerful R is when used to
    solve machine learning problems. Also, we have shown that the use of Hadoop allows
    R to process big data in parallel. At this point, some may believe that the use
    of RHadoop can easily solve machine learning problems of big data via numerous
    existing machine learning packages. However, you cannot use most of these to solve
    machine learning problems as they cannot be executed in the MapReduce mode. In
    the following recipe, we will demonstrate how to implement a MapReduce version
    of linear regression and compare this version with the one using the `lm` function.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经展示了当使用R解决机器学习问题时，R是多么强大。我们还展示了使用Hadoop可以使R并行处理大数据。此时，有些人可能认为使用RHadoop可以通过现有的许多机器学习包轻松解决大数据的机器学习问题。然而，你不能使用其中大多数来解决机器学习问题，因为它们不能在MapReduce模式下执行。在下面的菜谱中，我们将展示如何实现线性回归的MapReduce版本，并将其与使用`lm`函数的版本进行比较。
- en: Getting ready
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, you should have completed the previous recipe by installing
    `rmr2` into the R environment.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，你应该通过将`rmr2`安装到R环境中来完成之前的菜谱。
- en: How to do it...
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to implement a linear regression in MapReduce:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以在MapReduce中实现线性回归：
- en: 'First, load the `cats` dataset from the `MASS` package:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从`MASS`包中加载`cats`数据集：
- en: '[PRE53]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'You can then generate a linear regression model by calling the `lm` function:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以通过调用`lm`函数来生成线性回归模型：
- en: '[PRE54]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'You can now make a regression plot with the given data points and model:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你现在可以使用给定的数据点和模型制作回归图：
- en: '[PRE55]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![How to do it...](img/00270.jpeg)'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00270.jpeg)'
- en: Linear regression plot of cats dataset
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: cats数据集的线性回归图
- en: 'Load `rmr2` into R:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`rmr2`加载到R中：
- en: '[PRE56]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'You can then set up `X` and `y` values:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以设置`X`和`y`的值：
- en: '[PRE57]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Make a `Sum` function to sum up the values:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`Sum`函数来汇总值：
- en: '[PRE58]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Compute `Xtx` in MapReduce, `Job1`:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在MapReduce的`Job1`中计算`Xtx`：
- en: '[PRE59]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'You can then compute `Xty` in MapReduce, `Job2`:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以在MapReduce的`Job2`中计算`Xty`：
- en: '[PRE60]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Lastly, you can derive the coefficient from `XtX` and `Xty`:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以从`XtX`和`Xty`中推导出系数：
- en: '[PRE61]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: How it works...
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we demonstrate how to implement linear logistic regression in
    a MapReduce fashion in R. Before we start the implementation, we review how traditional
    linear models work. We first retrieve the `cats` dataset from the `MASS` package.
    We then load `X` as the body weight (`Bwt`) and `y` as the heart weight (`Hwt`).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们展示了如何在R中用MapReduce风格实现线性逻辑回归。在我们开始实现之前，我们回顾了传统线性模型是如何工作的。我们首先从`MASS`包中检索`cats`数据集。然后我们将`X`作为体重（`Bwt`）和`y`作为心脏重量（`Hwt`）加载。
- en: Next, we begin to fit the data into a linear regression model using the `lm`
    function. We can then compute the fitted model and obtain the summary of the model.
    The summary shows that the coefficient is 4.0341 and the intercept is -0.3567\.
    Furthermore, we draw a scatter plot in accordance with the given data points and
    then draw a regression line on the plot.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们开始使用`lm`函数将数据拟合到线性回归模型中。然后我们可以计算拟合模型并获得模型的摘要。摘要显示系数为4.0341，截距为-0.3567。此外，我们根据给定的数据点绘制散点图，然后在图上绘制回归线。
- en: 'As we cannot perform linear regression using the `lm` function in the MapReduce
    form, we have to rewrite the regression model in a MapReduce fashion. Here, we
    would like to implement a MapReduce version of linear regression in three steps,
    which are: calculate the `Xtx` value with the MapReduce, job1, calculate the `Xty`
    value with MapReduce, `job2`, and then derive the coefficient value:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不能在MapReduce形式中使用`lm`函数进行线性回归，我们必须以MapReduce风格重写回归模型。在这里，我们希望分三步实现线性回归的MapReduce版本，这些步骤是：使用MapReduce计算`Xtx`值，任务1，使用MapReduce计算`Xty`值，任务2，然后推导出系数值：
- en: In the first step, we pass the matrix, `X`, as the input to the `map` function.
    The `map` function then calculates the cross product of the transposed matrix,
    `X`, and, `X`. The `reduce` function then performs the sum operation defined in
    the previous section.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一步中，我们将矩阵`X`作为输入传递给`map`函数。然后`map`函数计算转置矩阵`X`和`X`的叉积。然后`reduce`函数执行之前定义的求和操作。
- en: In the second step, the procedure of calculating `Xty` is similar to calculating
    `XtX`. The procedure calculates the cross product of the transposed matrix, `X,`
    and, `y`. The `reduce` function then performs the sum operation.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二步中，计算`Xty`的过程与计算`XtX`的过程类似。这个过程计算了转置矩阵`X`和`y`的叉积。然后`reduce`函数执行之前定义的求和操作。
- en: Lastly, we use the `solve` function to derive the coefficient, which is 3.907113.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们使用`solve`函数推导出系数，其值为3.907113。
- en: As the results show, the coefficients computed by `lm` and MapReduce differ
    slightly. Generally speaking, the coefficient computed by the `lm` model is more
    accurate than the one calculated by MapReduce. However, if your data is too large
    to fit in the memory, you have no choice but to implement linear regression in
    the MapReduce version.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如结果所示，`lm`和MapReduce计算出的系数略有差异。一般来说，`lm`模型计算出的系数比MapReduce计算出的更准确。然而，如果你的数据太大而无法适应内存，你不得不选择在MapReduce版本中实现线性回归。
- en: See also
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考信息
- en: 'You can access more information on machine learning algorithms at: [https://github.com/RevolutionAnalytics/rmr2/tree/master/pkg/tests](https://github.com/RevolutionAnalytics/rmr2/tree/master/pkg/tests)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在以下位置获取有关机器学习算法的更多信息：[https://github.com/RevolutionAnalytics/rmr2/tree/master/pkg/tests](https://github.com/RevolutionAnalytics/rmr2/tree/master/pkg/tests)
- en: Configuring RHadoop clusters on Amazon EMR
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Amazon EMR上配置RHadoop集群
- en: Until now, we have only demonstrated how to run a RHadoop program in a single
    Hadoop node. In order to test our RHadoop program on a multi-node cluster, the
    only thing you need to do is to install RHadoop on all the task nodes (nodes with
    either task tracker for mapreduce version 1 or node manager for map reduce version
    2) of Hadoop clusters. However, the deployment and installation is time consuming.
    On the other hand, you can choose to deploy your RHadoop program on Amazon EMR,
    so that you can deploy multi-node clusters and RHadoop on every task node in only
    a few minutes. In the following recipe, we will demonstrate how to configure RHadoop
    cluster on an Amazon EMR service.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只演示了如何在单个Hadoop节点上运行RHadoop程序。为了在多节点集群上测试我们的RHadoop程序，你需要做的唯一一件事是在Hadoop集群的所有任务节点（无论是mapreduce版本1的任务跟踪器还是map
    reduce版本2的节点管理器）上安装RHadoop。然而，部署和安装是耗时的。另一方面，你可以选择在Amazon EMR上部署你的RHadoop程序，这样你就可以在几分钟内部署多节点集群和RHadoop到每个任务节点。在下面的菜谱中，我们将演示如何在Amazon
    EMR服务上配置RHadoop集群。
- en: Getting ready
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, you must register and create an account on AWS, and you also
    must know how to generate a EC2 key-pair before using Amazon EMR.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，你必须注册并创建 AWS 账户，并且在使用 Amazon EMR 之前，你必须知道如何生成 EC2 密钥对。
- en: For those who seek more information on how to start using AWS, please refer
    to the tutorial provided by Amazon at [http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些寻求如何开始使用 AWS 的更多信息的人，请参阅亚马逊提供的教程[http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html)。
- en: How to do it...
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to configure RHadoop on Amazon EMR:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Amazon EMR 上配置 RHadoop 的以下步骤
- en: First, you can access the console of the Amazon Web Service (refer to [https://us-west-2.console.aws.amazon.com/console/](https://us-west-2.console.aws.amazon.com/console/))
    and find EMR in the analytics section. Then, click on **EMR**.![How to do it...](img/00271.jpeg)
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你可以访问亚马逊网络服务的控制台（参考[https://us-west-2.console.aws.amazon.com/console/](https://us-west-2.console.aws.amazon.com/console/))，在分析部分找到
    EMR。然后，点击**EMR**。[如何操作...](img/00271.jpeg)
- en: Access EMR service from AWS console.
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从 AWS 控制台访问 EMR 服务。
- en: You should find yourself in the cluster list of the EMR dashboard (refer to
    [https://us-west-2.console.aws.amazon.com/elasticmapreduce/home?region=us-west-2#cluster-list::](https://us-west-2.console.aws.amazon.com/elasticmapreduce/home?region=us-west-2#cluster-list::));
    click on **Create cluster**.![How to do it...](img/00272.jpeg)
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该发现自己处于 EMR 仪表板的集群列表中（参考[https://us-west-2.console.aws.amazon.com/elasticmapreduce/home?region=us-west-2#cluster-list::](https://us-west-2.console.aws.amazon.com/elasticmapreduce/home?region=us-west-2#cluster-list::))；点击**创建集群**。[如何操作...](img/00272.jpeg)
- en: Cluster list of EMR
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: EMR 集群列表
- en: Then, you should find yourself on the **Create Cluster** page (refer to [https://us-west-2.console.aws.amazon.com/elasticmapreduce/home?region=us-west-2#create-cluster:](https://us-west-2.console.aws.amazon.com/elasticmapreduce/home?region=us-west-2#create-cluster:)).
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你应该发现自己处于**创建集群**页面（参考[https://us-west-2.console.aws.amazon.com/elasticmapreduce/home?region=us-west-2#create-cluster:](https://us-west-2.console.aws.amazon.com/elasticmapreduce/home?region=us-west-2#create-cluster:)）。
- en: Next, you should specify **Cluster name** and **Log folder S3 location** in
    the cluster configuration.![How to do it...](img/00273.jpeg)
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你应在集群配置中指定**集群名称**和**日志文件夹 S3 位置**。[如何操作...](img/00273.jpeg)
- en: Cluster configuration in the create cluster page
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建集群页面中的集群配置
- en: You can then configure the Hadoop distribution on **Software Configuration**.![How
    to do it...](img/00274.jpeg)
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以在**软件配置**中配置 Hadoop 发行版。[如何操作...](img/00274.jpeg)
- en: Configure the software and applications
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 配置软件和应用程序
- en: Next, you can configure the number of nodes within the Hadoop cluster.![How
    to do it...](img/00275.jpeg)
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你可以在 Hadoop 集群内配置节点数量。[如何操作...](img/00275.jpeg)
- en: Configure the hardware within Hadoop cluster
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 配置 Hadoop 集群内的硬件
- en: You can then specify the EC2 key-pair for the master node login.![How to do
    it...](img/00276.jpeg)
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以指定用于主节点登录的 EC2 密钥对。[如何操作...](img/00276.jpeg)
- en: Security and access to the master node of the EMR cluster
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: EMR 集群主节点的安全和访问
- en: 'To set up RHadoop, one has to perform bootstrap actions to install RHadoop
    on every task node. Please write a file named `bootstrapRHadoop.sh`, and insert
    the following lines within the file:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要设置 RHadoop，必须在每个任务节点上执行引导操作来安装 RHadoop。请创建一个名为 `bootstrapRHadoop.sh` 的文件，并在文件中插入以下行：
- en: '[PRE62]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: You should upload `bootstrapRHadoop.sh` to `S3`.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该将 `bootstrapRHadoop.sh` 上传到 `S3`。
- en: You now need to add the bootstrap action with `Custom action`, and add `s3://<location>/bootstrapRHadoop.sh`
    within the S3 location.![How to do it...](img/00277.jpeg)
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你需要使用**自定义操作**添加引导操作，并在 S3 位置添加 `s3://<location>/bootstrapRHadoop.sh`。[如何操作...](img/00277.jpeg)
- en: Set up the bootstrap action
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置引导操作
- en: Next, you can click on **Create cluster** to launch the Hadoop cluster.![How
    to do it...](img/00278.jpeg)
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你可以点击**创建集群**来启动 Hadoop 集群。[如何操作...](img/00278.jpeg)
- en: Create the cluster
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建集群
- en: Lastly, you should see the master public DNS when the cluster is ready. You
    can now access the terminal of the master node with your EC2-key pair:![How to
    do it...](img/00279.jpeg)
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，当集群准备就绪时，你应该看到主节点的公共 DNS。现在，你可以使用你的 EC2 密钥对访问主节点的终端：[如何操作...](img/00279.jpeg)
- en: A screenshot of the created cluster
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建集群的截图
- en: How it works...
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we demonstrate how to set up RHadoop on Amazon EMR. The benefit
    of this is that you can quickly create a scalable, on demand Hadoop with just
    a few clicks within a few minutes. This helps save you time from building and
    deploying a Hadoop application. However, you have to pay for the number of running
    hours for each instance. Before using Amazon EMR, you should create an AWS account
    and know how to set up the EC2 key-pair and the S3\. You can then start installing
    RHadoop on Amazon EMR.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们展示了如何在 Amazon EMR 上设置 RHadoop。这样做的好处是，您只需点击几下，在几分钟内就可以快速创建一个可扩展的、按需的
    Hadoop。这有助于节省您构建和部署 Hadoop 应用程序的时间。然而，您必须为每个实例的运行时间付费。在使用 Amazon EMR 之前，您应该创建一个
    AWS 账户，并了解如何设置 EC2 密钥对和 S3。然后，您可以在 Amazon EMR 上开始安装 RHadoop。
- en: In the first step, access the EMR cluster list and click on **Create cluster**.
    You can see a list of configurations on the **Create cluster** page. You should
    then set up the cluster name and log folder in the S3 location in the cluster
    configuration.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，访问 EMR 集群列表并点击 **创建集群**。您可以在 **创建集群** 页面上看到配置列表。然后，您应该在集群配置中的 S3 位置设置集群名称和日志文件夹。
- en: Next, you can set up the software configuration and choose the Hadoop distribution
    you would like to install. Amazon provides both its own distribution and the MapR
    distribution. Normally, you would skip this section unless you have concerns about
    the default Hadoop distribution.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您可以设置软件配置并选择您想要安装的 Hadoop 发行版。亚马逊提供自己的发行版和 MapR 发行版。通常情况下，除非您对默认的 Hadoop
    发行版有顾虑，否则您会跳过这一部分。
- en: You can then configure the hardware by specifying the master, core, and task
    node. By default, there is only one master node, and two core nodes. You can add
    more core and task nodes if you like. You should then set up the key-pair to login
    to the master node.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过指定主节点、核心节点和任务节点来配置硬件。默认情况下，只有一个主节点和两个核心节点。如果您愿意，可以添加更多核心节点和任务节点。然后，您应该设置密钥对以登录主节点。
- en: You should next make a file containing all the start scripts named `bootstrapRHadoop.sh`.
    After the file is created, you should save the file in the S3 storage. You can
    then specify `custom action` in **Bootstrap Action** with `bootstrapRHadoop.sh`
    as the Bootstrap script. Lastly, you can click on `Create cluster` and wait until
    the cluster is ready. Once the cluster is ready, one can see the master public
    DNS and can use the EC2 key-pair to access the terminal of the master node.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建一个包含所有启动脚本名为 `bootstrapRHadoop.sh` 的文件。文件创建后，您应该将其保存在 S3 存储中。然后，您可以在 **引导操作**
    中指定 `custom action`，并将 `bootstrapRHadoop.sh` 作为引导脚本。最后，您可以点击 **创建集群** 并等待集群就绪。一旦集群就绪，您可以看到主节点的公共
    DNS，并可以使用 EC2 密钥对访问主节点的终端。
- en: Beware! Terminate the running instance if you do not want to continue using
    the EMR service. Otherwise, you will be charged per instance for every hour you
    use.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 警告！如果您不想继续使用 EMR 服务，请终止正在运行的实例。否则，您将按实例每小时收费。
- en: See also
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: Google also provides its own cloud solution, the Google compute engine. For
    those who would like to know more, please refer to [https://cloud.google.com/compute/](https://cloud.google.com/compute/).
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google 也提供了自己的云解决方案，即 Google 计算引擎。如果您想了解更多信息，请参阅 [https://cloud.google.com/compute/](https://cloud.google.com/compute/)。
- en: Appendix A. Resources for R and Machine Learning
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 A. R 和机器学习资源
- en: 'The following table lists all the resources for R and machine learning:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格列出了 R 和机器学习的所有资源：
- en: '| R introduction |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| R 简介 |'
- en: '| --- |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| **Title** | **Link** | **Author** |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| **标题** | **链接** | **作者** |'
- en: '| R in Action | [http://www.amazon.com/R-Action-Robert-Kabacoff/dp/1935182390](http://www.amazon.com/R-Action-Robert-Kabacoff/dp/1935182390)
    | Robert Kabacoff |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 《R 动作》 | [http://www.amazon.com/R-Action-Robert-Kabacoff/dp/1935182390](http://www.amazon.com/R-Action-Robert-Kabacoff/dp/1935182390)
    | Robert Kabacoff |'
- en: '| The Art of R Programming: A Tour of Statistical Software Design | [http://www.amazon.com/The-Art-Programming-Statistical-Software/dp/1593273843](http://www.amazon.com/The-Art-Programming-Statistical-Software/dp/1593273843)
    | Norman Matloff |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 《R 编程艺术：统计软件设计之旅》 | [http://www.amazon.com/The-Art-Programming-Statistical-Software/dp/1593273843](http://www.amazon.com/The-Art-Programming-Statistical-Software/dp/1593273843)
    | Norman Matloff |'
- en: '| An Introduction to R | [http://cran.r-project.org/doc/manuals/R-intro.pdf](http://cran.r-project.org/doc/manuals/R-intro.pdf)
    | W. N. Venables, D. M. Smith, and the R Core Team |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 《R 简介》 | [http://cran.r-project.org/doc/manuals/R-intro.pdf](http://cran.r-project.org/doc/manuals/R-intro.pdf)
    | W. N. Venables, D. M. Smith, 和 R 核心团队 |'
- en: '| Quick-R | [http://www.statmethods.net/](http://www.statmethods.net/) | Robert
    I. Kabacoff, PhD |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| Quick-R | [http://www.statmethods.net/](http://www.statmethods.net/) | 罗伯特·I·卡巴科夫，博士
    |'
- en: '| **Online courses** |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| **在线课程** |'
- en: '| **Title** | **Link** | **Instructor** |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| **标题** | **链接** | **讲师** |'
- en: '| Computing for Data Analysis (with R) | [https://www.coursera.org/course/compdata](https://www.coursera.org/course/compdata)
    | Roger D. Peng, Johns Hopkins University |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 使用R进行数据分析（含R） | [https://www.coursera.org/course/compdata](https://www.coursera.org/course/compdata)
    | 罗杰·D·彭，约翰霍普金斯大学 |'
- en: '| Data Analysis | [https://www.coursera.org/course/dataanalysis](https://www.coursera.org/course/dataanalysis)
    | Jeff Leek, Johns Hopkins University |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 数据分析 | [https://www.coursera.org/course/dataanalysis](https://www.coursera.org/course/dataanalysis)
    | 杰夫·利克，约翰霍普金斯大学 |'
- en: '| Data Analysis and Statistical Inference | [https://www.coursera.org/course/statistics](https://www.coursera.org/course/statistics)
    | Mine Çetinkaya-Rundel, Duke University |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 数据分析与统计推断 | [https://www.coursera.org/course/statistics](https://www.coursera.org/course/statistics)
    | 矿·切廷卡亚-朗德尔，杜克大学 |'
- en: '| **Machine learning** |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| **机器学习** |'
- en: '| **Title** | **Link** | **Author** |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| **标题** | **链接** | **作者** |'
- en: '| Machine Learning for Hackers | [http://www.amazon.com/dp/1449303714?tag=inspiredalgor-20](http://www.amazon.com/dp/1449303714?tag=inspiredalgor-20)
    | Drew Conway and John Myles White |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 为黑客的机器学习 | [http://www.amazon.com/dp/1449303714?tag=inspiredalgor-20](http://www.amazon.com/dp/1449303714?tag=inspiredalgor-20)
    | 德鲁·康威和约翰·迈尔斯·怀特 |'
- en: '| Machine Learning with R | [http://www.packtpub.com/machine-learning-with-r/book](http://www.packtpub.com/machine-learning-with-r/book)
    | Brett Lantz |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 使用R进行机器学习 | [http://www.packtpub.com/machine-learning-with-r/book](http://www.packtpub.com/machine-learning-with-r/book)
    | 布雷特·兰茨 |'
- en: '| **Online blog** |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| **在线博客** |'
- en: '| **Title** | **Link** |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| **标题** | **链接** |'
- en: '| R-bloggers | [http://www.r-bloggers.com/](http://www.r-bloggers.com/) |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| R博客 | [http://www.r-bloggers.com/](http://www.r-bloggers.com/) |'
- en: '| The R Journal | [http://journal.r-project.org/](http://journal.r-project.org/)
    |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| R杂志 | [http://journal.r-project.org/](http://journal.r-project.org/) |'
- en: '| **CRAN task view** |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| **CRAN任务视图** |'
- en: '| **Title** | **Link** |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| **标题** | **链接** |'
- en: '| CRAN Task View: Machine Learning and Statistical Learning | [http://cran.r-project.org/web/views/MachineLearning.html](http://cran.r-project.org/web/views/MachineLearning.html)
    |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| CRAN任务视图：机器学习和统计学习 | [http://cran.r-project.org/web/views/MachineLearning.html](http://cran.r-project.org/web/views/MachineLearning.html)
    |'
- en: Appendix B. Dataset – Survival of Passengers on the Titanic
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 B. 数据集 – 泰坦尼克号乘客的生存情况
- en: Before the exploration process, we would like to introduce the example adopted
    here. It is the demographic information on passengers aboard the RMS Titanic,
    provided by Kaggle ([https://www.kaggle.com/](https://www.kaggle.com/), a platform
    for data prediction competitions). The result we are examining is whether passengers
    on board would survive the shipwreck or not.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索过程之前，我们想介绍这里采用的例子。它是关于RMS泰坦尼克号上乘客的人口统计信息，由Kaggle提供（[https://www.kaggle.com/](https://www.kaggle.com/)，一个数据预测竞赛的平台）。我们正在考察的结果是乘客是否能在船难中幸存。
- en: 'There are two reasons to apply this dataset:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这个数据集有两个原因：
- en: RMS Titanic is considered as the most infamous shipwreck in history, with a
    death toll of up to 1,502 out of 2,224 passengers and crew. However, after the
    ship sank, the passengers' chance of survival was not by chance only; actually,
    the cabin class, sex, age, and other factors might also have affected their chance
    of survival.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMS泰坦尼克号被认为是历史上最臭名昭著的船难，死亡人数高达1,502人，占2,224名乘客和船员的近三分之二。然而，在船沉没后，乘客的生存机会并不仅仅取决于运气；实际上，舱位等级、性别、年龄和其他因素也可能影响了他们的生存机会。
- en: The dataset is relatively simple; you do not need to spend most of your time
    on data munging (except when dealing with some missing values), but you can focus
    on the application of exploratory analysis.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该数据集相对简单；你不需要花大部分时间在数据处理上（除非处理一些缺失值），但你可以专注于探索性分析的应用。
- en: 'The following chart is the variables'' descriptions of the target dataset:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表是目标数据集的变量描述：
- en: '![Dataset – Survival of Passengers on the Titanic](img/00280.jpeg)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![数据集 – 泰坦尼克号乘客的生存情况](img/00280.jpeg)'
- en: Judging from the description of the variables, one might have some questions
    in mind, such as, "Are there any missing values in this dataset?", "What was the
    average age of the passengers on the Titanic?", "What proportion of the passengers
    survived the disaster?", "What social class did most passengers on board belong
    to?". All these questions presented here will be answered in [Chapter 2](part0024_split_000.html#page
    "Chapter 2. Data Exploration with RMS Titanic"), *Data Exploration with RMS Titanic*.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 从变量的描述来看，人们可能会有一些疑问，例如，“这个数据集中是否有缺失值？”、“泰坦尼克号乘客的平均年龄是多少？”、“有多少比例的乘客在灾难中幸存？”、“船上大多数乘客属于哪个社会阶层？”在这里提出的所有这些问题都将在
    [第二章](part0024_split_000.html#page "第二章. 使用 RMS 泰坦尼克号进行数据探索") *《使用 RMS 泰坦尼克号进行数据探索》*
    中得到解答。
- en: Beyond questions relating to descriptive statistics, the eventual object of
    [Chapter 2](part0024_split_000.html#page "Chapter 2. Data Exploration with RMS
    Titanic"), *Data Exploration with RMS Titanic*, is to generate a model to predict
    the chance of survival given by the input parameters. In addition to this, we
    will assess the performance of the generated model to determine whether the model
    is suited for the problem.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 除了与描述性统计相关的问题之外，[第二章](part0024_split_000.html#page "第二章. 使用 RMS 泰坦尼克号进行数据探索")
    *《使用 RMS 泰坦尼克号进行数据探索》* 的最终目标是生成一个模型，用以预测给定输入参数下的生存概率。除此之外，我们还将评估所生成模型的性能，以确定该模型是否适合该问题。
