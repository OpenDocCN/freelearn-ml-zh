- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Conformal Prediction for Natural Language Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理的一致性预测
- en: '**Natural language processing** (**NLP**) grapples with the complexities of
    human language, where uncertainty is an inherent challenge. As NLP models become
    integral to risk-sensitive and critical applications, ensuring their reliability
    is paramount. Conformal prediction emerges as a promising technique, offering
    a way to quantify the trustworthiness of these models’ predictions, particularly
    when faced with miscalibrated outputs from deep learning models.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）处理人类语言的复杂性，其中不确定性是一个固有的挑战。随着NLP模型成为风险敏感和关键应用的核心，确保其可靠性至关重要。一致性预测作为一种有前景的技术，提供了一种量化这些模型预测可信度的方法，尤其是在面对深度学习模型产生的误校准输出时。'
- en: In this chapter, we will navigate the NLP conformal prediction world, understand
    its significance, and learn how to harness its power for more reliable and confident
    predictions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索NLP一致性预测的世界，了解其重要性，并学习如何利用其力量进行更可靠和自信的预测。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Uncertainty quantification for NLP
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP的不确定性量化
- en: Why deep learning produces miscalibrated predictions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么深度学习会产生误校准的预测
- en: Various approaches to quantify uncertainty in NLP problems
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化NLP问题的各种方法
- en: Conformal prediction for NLP
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP的一致性预测
- en: Building NLP classifiers using conformal prediction
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一致性预测构建NLP分类器
- en: Open source tools for conformal prediction in NLP
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP中用于一致性预测的开源工具
- en: Uncertainty quantification for NLP
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP的不确定性量化
- en: Uncertainty quantification in NLP is an essential yet often overlooked aspect
    of model development and deployment. As NLP models become increasingly integrated
    into critical applications—from healthcare diagnostics to financial predictions—the
    need to understand and convey the confidence level of their outputs becomes paramount.
    Uncertainty quantification provides a framework for assessing the reliability
    of predictions, allowing users and developers to gauge the model’s decisiveness
    and the potential risks of relying on its results. This section delves into the
    importance, methodologies, and practical considerations of uncertainty quantification
    in NLP, highlighting its pivotal role in building robust and trustworthy language
    models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: NLP中的不确定性量化是模型开发和部署中一个基本但常常被忽视的方面。随着NLP模型越来越多地集成到关键应用中——从医疗诊断到金融预测——理解和传达其输出置信水平的需求变得至关重要。不确定性量化提供了一个评估预测可靠性的框架，使用户和开发者能够评估模型的决断力和依赖其结果可能存在的潜在风险。本节深入探讨了NLP中不确定性量化的重要性、方法和实际考虑，强调了它在构建稳健和可信的语言模型中的关键作用。
- en: We will now explore uncertainty in NLP and the benefits and challenges of quantifying
    uncertainty in NLP applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探讨自然语言处理（NLP）中的不确定性以及量化NLP应用中不确定性的益处和挑战。
- en: What is uncertainty in NLP?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLP中的不确定性是什么？
- en: NLP, at its core, is about making sense of human language—a medium known for
    its richness, ambiguity, and diversity. The inherent variability in language usage,
    context-driven meanings, and the ever-evolving nature of linguistic constructs
    make NLP tasks inherently uncertain. For instance, the word “bank” could refer
    to a financial institution or the side of a river depending on the context.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）的核心是理解人类语言——一种以其丰富性、模糊性和多样性而闻名的媒介。语言使用中的固有可变性、由上下文驱动的含义以及语言结构不断演变的本质使得NLP任务本质上具有不确定性。例如，“银行”一词可能指金融机构或河流的一侧，这取决于上下文。
- en: Benefits of quantifying uncertainty in NLP
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化NLP中不确定性的益处
- en: 'Quantifying uncertainty in NLP is not just a theoretical exercise; it has the
    following tangible benefits:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP中量化不确定性不仅仅是一个理论练习；它具有以下实际益处：
- en: '**Trustworthiness**: Quantifying uncertainty either bolsters confidence in
    specific predictions or highlights areas of caution.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可信度**：量化不确定性要么增强对特定预测的信心，要么突出需要谨慎的领域。'
- en: '**Performance evaluation**: This assesses the efficacy of various models by
    examining the uncertainty in their metrics.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能评估**：这通过检查其指标中的不确定性来评估各种模型的有效性。'
- en: '**Enhancement opportunities**: It can recognize areas where a model can be
    refined, especially in contexts such as active learning.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强机会**：它能够识别模型可以改进的领域，尤其是在主动学习等场景中。'
- en: '**Risk management**: By understanding the degree of uncertainty in predictions,
    stakeholders can make more informed decisions. For instance, an NLP model predicting
    sentiment might be 80% certain that a review is positive. Knowing this, a business
    might prioritize addressing reviews where the model’s certainty is lower.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**风险管理**：通过理解预测的不确定性程度，利益相关者可以做出更明智的决策。例如，一个预测情感的自然语言处理模型可能对一条评论是积极的预测有80%的确定性。了解这一点后，企业可能会优先处理模型确定性较低的评价。'
- en: '**Model transparency**: A model that can express its uncertainty is perceived
    as more transparent and trustworthy. Users of the model can better understand
    when to trust the model’s output and when to approach it with caution.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型透明度**：能够表达其不确定性的模型被认为更加透明和可信。模型的使用者可以更好地理解何时信任模型的输出，何时需要谨慎对待。'
- en: '**Model training**: During the training phase, understanding areas of high
    uncertainty can guide data collection efforts. If a model is uncertain about a
    particular data type, gathering more of that data can lead to more robust training.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型训练**：在训练阶段，理解高不确定性区域可以指导数据收集工作。如果一个模型对特定数据类型不确定，收集更多此类数据可以导致更稳健的训练。'
- en: The challenges of uncertainty in NLP
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理中的不确定性挑战
- en: 'Despite its importance, managing uncertainty in NLP is challenging. Here are
    some reasons why:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其重要性不言而喻，但在自然语言处理中管理不确定性具有挑战性。以下是原因之一：
- en: '**Data sparsity**: Many NLP tasks lack representative data for all possible
    linguistic variations, leading to models that are uncertain about less common
    data points'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据稀疏性**：许多自然语言处理任务缺乏所有可能的语言变体的代表性数据，导致模型对较少见的数据点不确定。'
- en: '**Ambiguity in language**: As mentioned, words can have multiple meanings based
    on context, leading to inherent uncertainty'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言歧义**：如前所述，根据上下文，单词可以有多种含义，这导致固有的不确定性。'
- en: '**Model complexity**: Advanced models such as deep learning networks can sometimes
    act as black boxes, making it challenging to discern areas of uncertainty'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型复杂性**：深度学习等高级模型有时会像黑盒一样运作，这使得识别不确定性区域变得具有挑战性。'
- en: In building robust NLP systems, understanding and quantifying uncertainty becomes
    paramount. As we dive deeper into the chapter, we’ll explore techniques, particularly
    **conformal prediction**, that offer a structured approach to tackle these challenges
    head-on.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建稳健的自然语言处理（NLP）系统中，理解和量化不确定性变得至关重要。随着我们深入本章，我们将探讨一些技术，特别是**一致性预测**，这些技术提供了一种结构化的方法来直面这些挑战。
- en: Understanding why deep learning produces miscalibrated predictions
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解为什么深度学习会产生失准的预测
- en: In the rapidly evolving field of NLP, deep learning played a pivotal role in
    enabling machines to process and generate language in ways that were once the
    exclusive domain of humans. The next section introduces the key concepts and milestones
    in deep learning that has significantly influenced NLP.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在快速发展的自然语言处理领域，深度学习在使机器以人类曾经独占的方式处理和生成语言方面发挥了关键作用。下一节将介绍深度学习的关键概念和里程碑，这些概念和里程碑对自然语言处理产生了重大影响。
- en: Introduction to deep learning in NLP
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理中的深度学习简介
- en: '**Deep learning**, a subset of machine learning, relies on neural networks
    with many layers (hence “deep”) to analyze various data factors. In the context
    of NLP, deep learning has been a game-changer, enabling machines to understand
    and generate human language with unprecedented accuracy:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习**是机器学习的一个子集，它依赖于具有许多层的神经网络（因此称为“深度”）来分析各种数据因素。在自然语言处理的背景下，深度学习已经是一个颠覆性的变革，使机器以前所未有的准确性理解和生成人类语言：'
- en: '**Evolution of architectures**: The journey began with simpler architectures
    such as feedforward neural networks and **recurrent neural networks** (**RNNs**).
    With its ability to remember past information, the latter was particularly influential
    in sequence-based tasks such as language translation. Later, more advanced architectures
    such as **long short-term memory** (**LSTM**) and the **Transformer model** further
    elevated performance standards.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构演变**：这一旅程始于简单的架构，如前馈神经网络和**循环神经网络**（**RNNs**）。后者能够记住过去的信息，在基于序列的任务（如语言翻译）中特别有影响力。后来，更先进的架构，如**长短期记忆**（**LSTM**）和**Transformer模型**，进一步提升了性能标准。'
- en: '**BERT and transformers**: The introduction of **Bidirectional Encoder Representations
    from Transformers** (**BERT**) marked a significant milestone. BERT achieved state-of-the-art
    results in numerous NLP tasks by analyzing words concerning their entire context
    (both left and right of a word). The Transformer architecture, which BERT is based
    on, introduced attention mechanisms that allow models to focus on specific parts
    of the input text, much like how humans pay attention to particular words when
    comprehending language.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BERT和Transformer**：**双向Transformer编码器表示**（**BERT**）的引入是一个重要的里程碑。BERT通过分析单词的整个上下文（包括单词的左右两侧）在众多NLP任务中实现了最先进的成果。BERT所基于的Transformer架构引入了注意力机制，允许模型关注输入文本的特定部分，就像人类在理解语言时关注特定单词一样。'
- en: '**Language models and LLMs**: **Large language models** (**LLMs**) such as
    **generative pre-trained transformer** (**GPT**) and its iterations, such as ChatGPT,
    have set new standards in NLP. With billions of parameters, these models can generate
    human-like text, answer questions, and even assist in creative writing. ChatGPT,
    in particular, has been influential in creating conversational agents capable
    of more natural and coherent interactions.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言模型和大型语言模型**：**大型语言模型**（**LLMs**）如**生成预训练Transformer**（**GPT**）及其迭代版本，如ChatGPT，在NLP中设定了新的标准。这些模型拥有数十亿个参数，可以生成类似人类的文本，回答问题，甚至帮助进行创意写作。特别是ChatGPT，在创建能够进行更自然和连贯交互的对话代理方面产生了重大影响。'
- en: '**Transfer learning and fine-tuning**: One of the revolutionary aspects of
    these developments is the idea of transfer learning. Models such as BERT and GPT
    are pre-trained on vast corpora and can be fine-tuned on specific tasks with smaller
    datasets. This approach has democratized deep learning in NLP, allowing teams
    with limited resources to achieve competitive results.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迁移学习和微调**：这些发展的革命性方面之一是迁移学习的概念。例如BERT和GPT等模型在庞大的语料库上进行了预训练，并且可以在较小的数据集上针对特定任务进行微调。这种方法使得NLP中的深度学习民主化，允许资源有限的团队实现具有竞争力的成果。'
- en: With these advancements, deep learning models have become the backbone of many
    modern NLP applications, from chatbots to search engines. However, as we’ll explore
    in the subsequent sections, their complexity and sheer scale introduce challenges,
    especially in calibration.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这些进步，深度学习模型已成为许多现代NLP应用的基础，从聊天机器人到搜索引擎。然而，正如我们将在后续章节中探讨的，它们的复杂性和规模引入了挑战，尤其是在校准方面。
- en: Challenges with deep learning predictions in NLP
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLP中深度学习预测的挑战
- en: 'Deep learning has undeniably advanced the capabilities of NLP, but it also
    brings forth several challenges and pitfalls. As we navigate the landscape of
    deep learning in NLP, we must be aware of these issues. Some of the notable challenges
    include the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习无疑提高了NLP的能力，但也带来了几个挑战和陷阱。在我们探索NLP中的深度学习领域时，我们必须意识到这些问题。一些显著挑战包括以下内容：
- en: '**Model overconfidence**: Deep learning models, given their capacity to fit
    complex patterns, often become overconfident in their predictions. For instance,
    in sentiment analysis, a model might predict a text as positive with 60% confidence
    when, in reality, the actual confidence should be much lower due to ambiguous
    phrasing.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型过度自信**：深度学习模型，鉴于其拟合复杂模式的能力，往往对其预测过度自信。例如，在情感分析中，一个模型可能会以60%的置信度预测一段文本为正面，而实际上，由于措辞含糊，实际的置信度应该低得多。'
- en: '**Data distribution shift**: NLP models are often trained on specific datasets
    and may not be exposed to the full linguistic diversity of real-world inputs.
    When faced with out-of-distribution data, these models can produce miscalibrated
    predictions.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分布偏移**：NLP模型通常在特定的数据集上训练，可能没有接触到真实世界输入的全部语言多样性。当面对分布外的数据时，这些模型可能会产生校准不当的预测。'
- en: '**Lack of explicit uncertainty modeling**: Traditional deep learning approaches
    don’t inherently model uncertainty. They optimize for accuracy, often at the cost
    of a reliable uncertainty estimate.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏显式的不确定性建模**：传统的深度学习方法本身不建模不确定性。它们优化准确性，通常以可靠的不确定性估计为代价。'
- en: '**Complexity and non-linearity**: The intricate architectures of deep learning
    models, especially with multiple layers and non-linear activations, can sometimes
    lead to unpredictable behavior, especially when handling edge cases or rare linguistic
    constructs.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性和非线性**：深度学习模型的复杂架构，特别是多层和非线性激活，有时会导致不可预测的行为，尤其是在处理边缘情况或罕见的语言结构时。'
- en: Let’s go through the implications of miscalibration next.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探讨校准偏差的影响。
- en: The implications of miscalibration
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 校准偏差的影响
- en: 'Miscalibration in NLP models is more than just a mere academic concern. In
    real-world applications, it can lead to misinformed decisions, misplaced trust,
    and even potentially harmful outcomes, especially in sensitive areas such as healthcare,
    finance, and legal systems:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: NLP模型中的校准偏差不仅仅是一个纯粹学术上的问题。在现实世界的应用中，它可能导致基于错误信息的决策、不恰当的信任，甚至可能产生有害的结果，尤其是在医疗保健、金融和法律系统等敏感领域：
- en: '**Decision-making risks**: Overconfident models can lead stakeholders to make
    decisions based on misguided confidence, potentially causing miscommunications
    or flawed strategies'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策风险**：过于自信的模型可能导致利益相关者基于误导性的自信做出决策，可能造成误解或策略上的缺陷。'
- en: '**Loss of trust**: Users might lose faith in an NLP system if it frequently
    expresses high confidence in incorrect predictions'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信任丧失**：如果NLP系统频繁表达对错误预测的高度自信，用户可能会对它失去信心。'
- en: '**Resource misallocation**: In automated systems, a miscalibrated model might
    prioritize tasks inefficiently, wasting computational resources on tasks where
    human intervention would have been more appropriate'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源错配**：在自动化系统中，校准不当的模型可能会低效地优先处理任务，在人类干预更合适的情况下浪费计算资源。'
- en: Recognizing these challenges is the first step. As we progress, we’ll delve
    into conformal prediction—a technique that presents a viable solution to the miscalibration
    issues plaguing deep learning models in NLP.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到这些挑战是第一步。随着我们前进，我们将深入研究一致性预测——这是一种针对困扰自然语言处理（NLP）深度学习模型的校准问题的可行解决方案。
- en: Various approaches to quantify uncertainty in NLP problems
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化NLP问题不确定性的各种方法
- en: Multiple methods to quantify uncertainty in NLP problems have been explored
    to address the challenges of miscalibration and language’s inherent unpredictability.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决校准偏差和语言固有的不可预测性的挑战，已经探索了多种量化NLP问题不确定性的方法。
- en: We will now look at Bayesian approaches to UQ.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨贝叶斯方法在不确定性量化中的应用。
- en: Bayesian approaches to uncertainty quantification
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯不确定性量化方法
- en: Bayesian methods provide a framework for modeling uncertainty. By treating model
    parameters as distributions rather than fixed values, Bayesian neural networks
    offer a measure of uncertainty associated with predictions. This probabilistic
    approach ensures that the model not only gives an estimate but also conveys the
    confidence or spread of that estimate.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯方法为建模不确定性提供了一个框架。通过将模型参数视为分布而不是固定值，贝叶斯神经网络提供了与预测相关的不确定性度量。这种概率方法确保模型不仅给出估计，还传达了该估计的置信度或分布。
- en: These are some of the examples of Bayesian approaches to UQ.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是贝叶斯方法在不确定性量化中的一些例子。
- en: '**Variational inference** is a technique to approximate the posterior distribution
    of the model parameters, enabling the network to output distributions for predictions.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变分推断**是一种近似模型参数后验分布的技术，使网络能够输出用于预测的分布。'
- en: '**Bayesian neural networks** (**BNNs**) are neural networks with weights assigned
    to probability distributions. By sampling from these distributions, BNNs can produce
    a range of outputs, reflecting the uncertainty in predictions.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贝叶斯神经网络**（**BNNs**）是具有概率分布权重的神经网络。通过从这些分布中进行采样，BNNs可以产生一系列输出，反映预测的不确定性。'
- en: '**Monte Carlo dropout** is a technique wherein dropout is applied during inference.
    We can gain insight into the model’s uncertainty by running the model multiple
    times and observing the variance in outputs.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蒙特卡洛dropout**是一种在推理过程中应用dropout的技术。通过多次运行模型并观察输出方差，我们可以了解模型的不确定性。'
- en: Bootstrap methods and ensemble techniques
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自助方法和集成技术
- en: Bootstrapping involves creating multiple datasets from the original training
    data through resampling. By training separate models on these datasets, we can
    capture model uncertainty. This variance across different resamples allows for
    a more robust evaluation of how changes in the input data can impact predictions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 自助涉及通过重采样从原始训练数据创建多个数据集。通过在这些数据集上训练不同的模型，我们可以捕捉模型的不确定性。不同重采样之间的方差允许我们更稳健地评估输入数据的变化如何影响预测。
- en: We will now look at some of the examples of bootstrap methods and model ensembles.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨一些自助方法和模型集成的一些例子。
- en: '**Bagging**: Short for bootstrap aggregating, this involves training multiple
    models on different bootstrap samples. The variance in predictions across models
    provides an estimate of uncertainty.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bagging**：简称为自助聚合，涉及在不同的自助样本上训练多个模型。模型之间预测的方差提供了一个不确定性的估计。'
- en: '**Model ensembles**: Combining predictions from multiple models can also capture
    uncertainty. If models trained on the same data but with different architectures
    disagree on a prediction, it indicates higher uncertainty.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型集成**：结合多个模型的预测也可以捕捉到不确定性。如果基于相同数据但具有不同架构的模型在预测上意见不一致，这表明更高的不确定性。'
- en: Out-of-distribution (OOD) detection
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布外（OOD）检测
- en: 'Identifying inputs that are significantly different from the training data
    can also help in uncertainty estimation:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 识别与训练数据显著不同的输入也有助于不确定性估计：
- en: '**Likelihood-based methods**: These methods compare the likelihood of new data
    points to the training data. Lower likelihood indicates higher uncertainty.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于似然的方法**：这些方法比较新数据点的似然性与训练数据。较低的似然性表示更高的不确定性。'
- en: '**Adversarial training**: By training models to recognize adversarial examples,
    we can enhance their ability to identify uncertain inputs.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对抗训练**：通过训练模型识别对抗性示例，我们可以增强它们识别不确定输入的能力。'
- en: Understanding and appropriately employing these techniques is crucial in NLP,
    given human language’s inherent ambiguities and nuances. Each approach has its
    strengths and suitable scenarios, so practitioners must choose wisely based on
    the specifics of their NLP task.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到人类语言的固有歧义和细微差别，理解和恰当地运用这些技术在NLP中至关重要。每种方法都有其优势和适用场景，因此从业者必须根据他们NLP任务的具体情况明智地选择。
- en: Conformal prediction for NLP
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP中的正规预测
- en: Conformal prediction is a flexible and statistically robust approach to uncertainty
    quantification. It is a distribution-free framework that can estimate uncertainty
    for machine learning models without requiring model retraining or access to limited
    APIs. The central idea behind conformal prediction is to output a set of predictions
    containing the correct output with a user-specified probability. Conformal prediction
    can help quantify the uncertainty associated with the model’s predictions in language
    models.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正规预测是一种灵活且统计上稳健的不确定性量化方法。它是一个无分布框架，可以在不要求模型重新训练或访问有限的API的情况下估计机器学习模型的不确定性。正规预测背后的核心思想是输出一个包含正确输出的预测集，其概率由用户指定。正规预测可以帮助量化语言模型预测中的不确定性。
- en: Conformal prediction is a framework that delivers valid confidence intervals
    for predictions, irrespective of the underlying machine learning model. In the
    NLP landscape, with its inherent challenges of ambiguity, context sensitivity,
    and linguistic diversity, conformal prediction offers a structured way to quantify
    uncertainty.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 正规预测是一个提供预测有效置信区间的框架，无论底层机器学习模型如何。在NLP领域，由于其固有的歧义、上下文敏感性和语言多样性等挑战，正规预测提供了一种结构化的方法来量化不确定性。
- en: '**Validity and efficiency** are the two fundamental principles of conformal
    prediction. Validity ensures that the prediction regions (or sets) are correct
    with a predefined probability, while efficiency ensures these regions are as tight
    as possible.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**有效性和效率**是正规预测的两个基本原则。有效性确保预测区域（或集合）以预定义的概率是正确的，而效率确保这些区域尽可能紧凑。'
- en: How conformal prediction works in NLP
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正规预测在NLP中的工作原理
- en: 'The mechanics of conformal prediction are rooted in ordering predictions based
    on their “strangeness” or non-conformity scores. The idea is to understand how
    different a new observation is compared to previous ones:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正规预测的机制基于根据其“奇特性”或非一致性得分对预测进行排序。其想法是理解新观察与先前观察相比有多大的不同：
- en: '**Non-conformity score**: This score measures how different a new prediction
    is from previous predictions for any NLP task. For instance, the non-conformity
    might be based on the distance from the decision boundary in text classification.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非一致性得分**：这个得分衡量了新预测与任何NLP任务中先前预测的不同程度。例如，非一致性可能基于文本分类中决策边界的距离。'
- en: '**P-values**: P-values are calculated based on the non-conformity scores, representing
    the confidence level of predictions.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**P值**：P值是基于非一致性得分计算的，表示预测的置信水平。'
- en: Practical applications of conformal prediction in NLP
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正规预测在NLP中的实际应用
- en: 'Conformal prediction isn’t just a theoretical construct; its practical applications
    in NLP are wide-ranging:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性预测不仅仅是一个理论结构；它在自然语言处理中的实际应用范围广泛：
- en: '**Sentiment analysis**: When determining the sentiment of a text snippet, conformal
    prediction can provide a range or set of possible sentiments, each with its confidence
    level'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：在确定文本片段的情感时，一致性预测可以提供一个或一组可能的情感，每个情感都有其置信度水平'
- en: '**Named entity recognition**: Conformal prediction can give a confidence score
    on each tagged entity instead of just tagging entities, helping in tasks where
    precision is critical'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名实体识别**：一致性预测可以对每个标记的实体给出置信度分数，而不仅仅是标记实体，这有助于精度至关重要的任务'
- en: '**Machine translation**: Beyond translating text, conformal prediction can
    offer confidence intervals for different translation choices, aiding in tasks
    where mistranslations can have significant consequences'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译**：除了翻译文本之外，一致性预测可以为不同的翻译选择提供置信区间，有助于翻译错误可能产生重大后果的任务'
- en: Advantages of using conformal prediction in NLP
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用一致性预测在自然语言处理中的优势
- en: 'Conformal prediction, a relatively recent development in uncertainty quantification,
    brings a fresh perspective and many benefits to NLP. As we venture into an era
    where the demand for reliable and trustworthy models is ever-increasing, methods
    such as conformal prediction stand out, promising to address some innate challenges
    in NLP. Let’s delve into the distinct advantages of integrating conformal prediction
    in NLP tasks:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性预测，作为不确定性量化领域的一个相对较新的发展，为自然语言处理带来了新的视角和许多好处。随着我们进入一个对可靠和值得信赖的模型需求不断增长的时代，像一致性预测这样的方法脱颖而出，有望解决自然语言处理中的一些固有问题。让我们深入了解将一致性预测整合到自然语言处理任务中的独特优势：
- en: '**Model agnostic**: One of the strengths of conformal prediction is its compatibility
    with any machine learning model. Conformal prediction can be applied to any statistical,
    machine, or deep learning model.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型无关性**：一致性预测的一个优势是它与任何机器学习模型兼容。一致性预测可以应用于任何统计、机器或深度学习模型。'
- en: '**Transparent and interpretable**: Conformal prediction doesn’t operate as
    a black box. The non-conformity scores and resulting p-values offer interpretable
    metrics of uncertainty.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明和可解释性**：一致性预测不是作为一个黑盒运行。非一致性分数和结果p值提供了可解释的不确定性度量。'
- en: '**Adaptive**: Conformal prediction is adaptive to the data it’s applied to.
    It doesn’t make strong distributional assumptions, making it robust despite diverse
    linguistic data.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适应性**：一致性预测适用于其应用的数据。它不做强烈的分布假设，因此在处理多样化的语言数据时仍然稳健。'
- en: The introduction of conformal prediction into the NLP toolkit offers a promising
    avenue for practitioners to handle the inherent uncertainties of human language.
    Providing valid and reliable confidence measures helps build more robust and trustworthy
    NLP systems.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 将一致性预测引入自然语言处理工具箱为从业者提供了一个有前景的途径来处理人类语言固有的不确定性。提供有效的和可靠的置信度度量有助于构建更稳健和值得信赖的自然语言处理系统。
- en: 'An example of applied conformal prediction on an NLP task, such as IMDB Movie
    reviews that have been pre-labeled with “positive” and “negative” sentiment class
    labels based on the review content, has been discussed here: [https://github.com/M-Soundouss/density_based_conformal_prediction/tree/master/imdb](https://github.com/M-Soundouss/density_based_conformal_prediction/tree/master/imdb).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里讨论了应用于自然语言处理任务的一致性预测的一个例子，例如基于评论内容的“正面”和“负面”情感类别标签预标记的IMDB电影评论：[https://github.com/M-Soundouss/density_based_conformal_prediction/tree/master/imdb](https://github.com/M-Soundouss/density_based_conformal_prediction/tree/master/imdb)。
- en: Conformal prediction for NLP and LLMs is an emerging and crucial area of research.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理和大型语言模型的一致性预测是一个新兴且至关重要的研究领域。
- en: A notable contribution to this field is a paper by Kumar et.al., titled *Conformal
    Prediction with Large Language Models for Multi-Choice Question* *Answering* ([https://arxiv.org/abs/2305.18404](https://arxiv.org/abs/2305.18404)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域的一个显著贡献是Kumar等人发表的一篇论文，题为《基于大型语言模型的多选题*回答*的一致性预测》([https://arxiv.org/abs/2305.18404](https://arxiv.org/abs/2305.18404))。
- en: This paper dives deep into how conformal prediction can be instrumental in quantifying
    uncertainty in language models, thereby paving the way for a more trustworthy
    and reliable deployment of large language models, especially in scenarios where
    safety is paramount.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本文深入探讨了如何使用一致性预测量化语言模型中的不确定性，从而为大型语言模型的更可靠和可靠的部署铺平道路，特别是在安全至关重要的场景中。
- en: The paper’s primary focus is on multiple-choice question-answering tasks. Through
    a series of experiments, it showcases the efficacy of conformal prediction in
    deriving uncertainty estimates that are in strong correlation with prediction
    accuracy.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的主要焦点是多项选择题回答任务。通过一系列实验，它展示了一致性预测在推导与预测准确度强相关的不确定性估计方面的有效性。
- en: Diving into the experimental setup, the authors employed the LLaMA-13B model.
    This model, boasting 13 billion parameters and trained on a staggering 1 trillion
    tokens, generated predictions for MCQA questions sourced from the `MMLU benchmark`
    dataset ([https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu)).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 深入实验设置，作者使用了LLaMA-13B模型。这个模型拥有130亿个参数，在令人惊叹的1000亿个标记上进行了训练，为来自`MMLU基准`数据集（[https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu)）的MCQA问题生成了预测。
- en: The experiments were structured around a calibration set that trained the conformal
    prediction model and an evaluation set that tested the model’s prowess. A cross-validation
    approach was adopted to ensure the experiment’s integrity, ensuring the calibration
    and evaluation sets were sampled from a consistent distribution.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 实验围绕一个用于训练一致性预测模型的校准集和一个用于测试模型能力的评估集来构建。采用交叉验证方法以确保实验的完整性，确保校准集和评估集是从一致分布中抽取的。
- en: The performance metrics were multifaceted, encompassing accuracy, coverage,
    and efficiency. A pivotal observation was that the softmax outputs from the LLaMA-13B
    model, while reasonably calibrated on average, exhibited tendencies of underconfidence
    and overconfidence, particularly at the extremities of the probability distribution.
    This observation was particularly pronounced in subjects such as formal logic
    and college chemistry, which inherently possess more ambiguity and complexity,
    making them challenging for LLMs to navigate accurately.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 性能指标是多方面的，包括准确度、覆盖率和效率。一个关键的观察结果是，LLaMA-13B模型的softmax输出，尽管平均而言进行了合理的校准，但表现出信心不足和过度自信的倾向，尤其是在概率分布的极端部分。这一观察在诸如形式逻辑和大学化学等主题中尤为明显，这些主题本质上具有更多的模糊性和复杂性，使得它们对LLM准确导航变得具有挑战性。
- en: One of the standout findings was the strong correlation between the uncertainty
    estimates provided by conformal prediction and prediction accuracy. Such a correlation
    implies that when the model exhibits higher uncertainty about its predictions,
    it’s more prone to errors. This insight is invaluable for downstream applications
    such as selective classification. By leveraging these uncertainty estimates, it’s
    feasible to filter out lower-quality predictions, thereby enhancing the overall
    user experience.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个引人注目的发现是，一致性预测提供的不确定性估计与预测准确度之间存在着强烈的关联。这种关联意味着当模型对其预测表现出更高的不确定性时，它更容易出错。这一见解对于下游应用，如选择性分类，极为宝贵。通过利用这些不确定性估计，可以过滤掉低质量的预测，从而提升整体用户体验。
- en: The paper underscores the potential of conformal prediction as a beacon for
    uncertainty quantification in LLMs. By integrating this approach, LLMs can be
    more reliable, especially in high-stakes environments, reinforcing their trustworthiness
    and broadening their applicability.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 论文强调了一致性预测作为LLM中不确定性量化灯塔的潜力。通过整合这种方法，LLM可以更加可靠，尤其是在高风险环境中，增强其可信度和扩大其适用范围。
- en: 'The second critical paper is *Robots That Ask For Help: Uncertainty Alignment
    for Large Language Model Planners* ([https://robot-help.github.io](https://robot-help.github.io)),
    published by a team of researchers from Princeton University and DeepMind.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 第二篇关键论文是《寻求帮助的机器人：大型语言模型规划者的不确定性对齐》（[https://robot-help.github.io](https://robot-help.github.io)），由普林斯顿大学和DeepMind的研究团队发表。
- en: In robotics and artificial intelligence, the aspiration to equip robots with
    the capability to discern when uncertain is a pivotal challenge. The paper addresses
    this challenge, particularly regarding robots instructed via language. With its
    inherent flexibility, language offers a natural interface for humans to convey
    tasks, contextual information, and intentions. It also facilitates humans in providing
    clarifications to robots when they encounter uncertainties.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器人和人工智能领域，赋予机器人辨别不确定性的能力是一个关键挑战。本文针对这一挑战进行了探讨，特别是针对通过语言指令控制的机器人。语言固有的灵活性为人类提供了自然接口，用于传达任务、上下文信息和意图。它还便于人类在机器人遇到不确定性时提供澄清。
- en: Recent advancements have showcased the potential of LLMs in planning. These
    models can interpret and respond to unstructured language instructions, generating
    temporally extended plans. The strength of these LLMs lies in their ability to
    harness the vast knowledge and rich context they have been pre-trained with, leading
    to enhanced abstract reasoning capabilities. However, a significant impediment
    with current LLMs is their propensity to “hallucinate.” In other words, they tend
    to generate outputs with high confidence that, while plausible, might be incorrect
    and not anchored in reality.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 近期进展展示了LLMs在规划方面的潜力。这些模型可以解释和响应非结构化语言指令，生成时间扩展的计划。这些LLMs的优势在于它们能够利用预训练过程中所获得的广泛知识和丰富上下文，从而提高抽象推理能力。然而，当前LLMs的一个显著障碍是它们倾向于“幻觉”。换句话说，它们倾向于生成具有高度自信的输出，虽然看似合理，但可能是不正确的，并且没有基于现实。
- en: Such unwarranted confidence in outputs can be detrimental, especially in LLM-based
    robotic planning. This is further exacerbated when natural language instructions,
    often riddled with inherent or unintentional ambiguities, are provided in real-world
    settings. Misinterpreting such instructions can lead to undesirable or, in extreme
    cases, unsafe actions.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对输出的过度自信可能是有害的，尤其是在基于LLM的机器人规划中。当在现实世界设置中提供通常充满固有或无意模糊性的自然语言指令时，这种情况会进一步加剧。误解此类指令可能导致不希望看到的行为，在极端情况下，甚至可能是不安全的行为。
- en: To illustrate, the paper presents an example where a robot tasked with heating
    food is instructed to place a bowl in the microwave. In scenarios where multiple
    bowls are present, such an instruction becomes ambiguous. Moreover, if one of
    the bowls is metallic, placing it in the microwave would be hazardous. Instead
    of acting on such vague instructions, the ideal robot should recognize its uncertainty
    and seek clarification. While previous works in language-based planning have either
    overlooked the need for such clarifications or relied heavily on extensive prompting,
    this paper introduces **KNOWNO**.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，本文提供了一个例子，其中一台被分配加热食物的机器人被指令将碗放入微波炉中。在存在多个碗的情况下，这样的指令变得模糊。此外，如果其中一个碗是金属的，将其放入微波炉中将是危险的。理想情况下，机器人应该识别其不确定性并寻求澄清。虽然基于语言规划的前期工作要么忽视了这种澄清的需要，要么过度依赖广泛的提示，但本文介绍了**KNOWNO**。
- en: KNOWNO is a framework designed to measure and align the uncertainty of LLM-based
    planners. It ensures that these planners know their limitations and seek assistance
    when required. The foundation of KNOWNO is built on the theory of conformal prediction,
    which offers statistical guarantees on task completion while minimizing the need
    for human intervention in intricate multi-step planning settings. Experiments
    across various simulated and real robot setups demonstrate the framework’s efficacy.
    These experiments encompass tasks with diverse modes of ambiguity, ranging from
    spatial uncertainties to numeric ones and from human preferences to Winograd schemas.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: KNOWNO是一个旨在衡量和调整基于LLM规划者不确定性的框架。它确保这些规划者了解自己的局限性，并在需要时寻求帮助。KNOWNO的基础建立在符合预测理论之上，该理论在完成任务时提供统计保证，同时在复杂的多步骤规划设置中最大限度地减少对人类干预的需求。在各种模拟和真实机器人设置中的实验证明了该框架的有效性。这些实验涵盖了具有不同模糊模式的任务，从空间不确定性到数值不确定性，从人类偏好到Winograd模式。
- en: The paper posits KNOWNO as a promising lightweight approach to model uncertainty.
    It can seamlessly complement and scale with the burgeoning capabilities of foundational
    models. LLMs can be more reliable by leveraging conformal prediction, especially
    when precision and safety are paramount.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出KNOWNO作为一种有潜力的轻量级方法来建模不确定性。它能够无缝地补充并扩展基础模型日益增长的能力。通过利用符合预测，LLMs可以变得更加可靠，尤其是在精度和安全至关重要的场合。
- en: Summary
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In the chapter, we have explored the inherent uncertainty challenges in the
    NLP domain. Recognizing the pivotal role of NLP models in today’s critical systems,
    the chapter emphasizes the importance of ensuring these models’ predictions are
    trustworthy and reliable. The chapter introduces conformal prediction as a solution
    to address the miscalibration seen in deep learning models’ outputs, offering
    a means to quantify the confidence of predictions robustly. Throughout this chapter,
    you gained insights into the intricacies of uncertainty quantification specific
    to NLP, the reasons why deep learning models often produce miscalibrated predictions,
    and various methods of quantifying uncertainty in NLP. Finally, we deeply studied
    the conformal prediction technique tailored for NLP tasks.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了NLP领域中固有的不确定性挑战。认识到NLP模型在当今关键系统中的关键作用，本章强调了确保这些模型的预测是可信和可靠的的重要性。本章介绍了一致性预测作为解决深度学习模型输出中出现的误校准问题的解决方案，提供了一种稳健地量化预测置信度的方法。在本章中，您获得了对NLP中不确定性量化复杂性的洞察，了解了为什么深度学习模型经常产生误校准预测的原因，以及量化NLP中不确定性的各种方法。最后，我们深入研究了针对NLP任务定制的一致性预测技术。
- en: At the end of this chapter, you should have a holistic understanding of the
    challenges of uncertainty in NLP, the merits and mechanics of conformal prediction,
    and practical knowledge to apply this technique to NLP problems effectively.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您应该对NLP中不确定性的挑战、一致性预测的优点和机制，以及如何有效地将此技术应用于NLP问题的实用知识有一个全面的理解。
- en: In the next chapter, we will dive deep into the intriguing world of imbalanced
    data and show how conformal prediction can address existing challenges in handling
    such scenarios.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨不平衡数据的迷人世界，展示一致性预测如何解决处理此类场景中存在的挑战。
- en: 'Part 4: Advanced Topics'
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四部分：高级主题
- en: This part will provide illustrations on how conformal prediction can be used
    to solve imbalanced data problems, introducing you to various conformal prediction
    methods that can be used for multi-class classification problems.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分将提供如何使用一致性预测来解决不平衡数据问题的示例，向您介绍可用于多类分类问题的各种一致性预测方法。
- en: 'This section has the following chapters:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含以下章节：
- en: '[*Chapter 11*](B19925_11.xhtml#_idTextAnchor149), *Handling Imbalanced Data*'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B19925_11.xhtml#_idTextAnchor149), *处理不平衡数据*'
- en: '[*Chapter 12*](B19925_12.xhtml#_idTextAnchor159), *Multi-Class Conformal Prediction*'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第12章*](B19925_12.xhtml#_idTextAnchor159), *多类一致性预测*'
