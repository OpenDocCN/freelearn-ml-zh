- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Applying Machine Learning to Sentiment Analysis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将机器学习应用于情感分析
- en: 'In the modern internet and social media age, people''s opinions, reviews, and
    recommendations have become a valuable resource for political science and businesses.
    Thanks to modern technologies, we are now able to collect and analyze such data
    most efficiently. In this chapter, we will delve into a subfield of **natural
    language processing** (**NLP**) called **sentiment analysis** and learn how to
    use machine learning algorithms to classify documents based on their polarity:
    the attitude of the writer. In particular, we are going to work with a dataset
    of 50,000 movie reviews from the **Internet Movie Database** (**IMDb**) and build
    a predictor that can distinguish between positive and negative reviews.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代互联网和社交媒体时代，人们的意见、评论和推荐已经成为政治学和商业领域的宝贵资源。得益于现代技术，我们现在能够最有效地收集和分析这些数据。在本章中，我们将深入探讨自然语言处理（**NLP**）的一个子领域——**情感分析**，并学习如何使用机器学习算法根据文档的极性（作者的态度）来分类文档。特别地，我们将使用来自**互联网电影数据库**（**IMDb**）的50,000条电影评论数据集，构建一个预测模型，能够区分正面评论和负面评论。
- en: 'The topics that we will cover in the following sections include the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中讨论以下主题：
- en: Cleaning and preparing text data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理和准备文本数据
- en: Building feature vectors from text documents
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本文档中构建特征向量
- en: Training a machine learning model to classify positive and negative movie reviews
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练机器学习模型以分类正面和负面电影评论
- en: Working with large text datasets using out-of-core learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用外部学习处理大型文本数据集
- en: Inferring topics from document collections for categorization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文档集合中推断主题进行分类
- en: Preparing the IMDb movie review data for text processing
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备IMDb电影评论数据以进行文本处理
- en: As mentioned, sentiment analysis, sometimes also called **opinion mining**,
    is a popular subdiscipline of the broader field of NLP; it is concerned with analyzing
    the polarity of documents. A popular task in sentiment analysis is the classification
    of documents based on the expressed opinions or emotions of the authors with regard
    to a particular topic.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，情感分析，有时也叫做**意见挖掘**，是自然语言处理（NLP）这一广泛领域中的一个热门子学科；它关注的是分析文档的极性。情感分析中的一个常见任务是基于作者对某一特定话题表达的意见或情感对文档进行分类。
- en: 'In this chapter, we will be working with a large dataset of movie reviews from
    the Internet Movie Database (IMDb) that has been collected by Andrew Maas and
    others (*Learning Word Vectors for Sentiment Analysis*, *A. L. Maas*, *R. E. Daly*,
    *P. T. Pham*, *D. Huang*, *A. Y. Ng*, and *C. Potts*, *Proceedings of the 49th
    Annual Meeting of the Association for Computational Linguistics: Human Language
    Technologies*, pages 142–150, Portland, Oregon, USA, Association for Computational
    Linguistics, *June 2011*). The movie review dataset consists of 50,000 polar movie
    reviews that are labeled as either positive or negative; here, positive means
    that a movie was rated with more than six stars on IMDb, and negative means that
    a movie was rated with fewer than five stars on IMDb. In the following sections,
    we will download the dataset, preprocess it into a useable format for machine
    learning tools, and extract meaningful information from a subset of these movie
    reviews to build a machine learning model that can predict whether a certain reviewer
    liked or disliked a movie.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '在本章中，我们将使用来自互联网电影数据库（IMDb）的一个大型电影评论数据集，该数据集由Andrew Maas等人收集（*Learning Word
    Vectors for Sentiment Analysis*, *A. L. Maas*, *R. E. Daly*, *P. T. Pham*, *D.
    Huang*, *A. Y. Ng*, and *C. Potts*, *Proceedings of the 49th Annual Meeting of
    the Association for Computational Linguistics: Human Language Technologies*, pages
    142–150, Portland, Oregon, USA, Association for Computational Linguistics, *June
    2011*）。该电影评论数据集包含50,000条极性电影评论，每条评论被标记为正面或负面；其中，正面表示电影在IMDb上的评分超过六星，负面表示电影在IMDb上的评分低于五星。在接下来的章节中，我们将下载数据集，进行预处理，将其转换为适用于机器学习工具的格式，并从这些电影评论的子集提取有意义的信息，构建一个机器学习模型，预测某个评论者是否喜欢或不喜欢一部电影。'
- en: Obtaining the movie review dataset
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取电影评论数据集
- en: 'A compressed archive of the movie review dataset (84.1 MB) can be downloaded
    from [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)
    as a gzip-compressed tarball archive:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从[http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)下载电影评论数据集的压缩档案（84.1
    MB），该档案为gzip压缩的tarball格式：
- en: If you are working with Linux or macOS, you can open a new terminal window,
    `cd` into the download directory and execute `tar -zxf aclImdb_v1.tar.gz` to decompress
    the dataset.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用的是 Linux 或 macOS，可以打开一个新的终端窗口，进入下载目录并执行 `tar -zxf aclImdb_v1.tar.gz` 来解压数据集。
- en: If you are working with Windows, you can download a free archiver, such as 7-Zip
    ([http://www.7-zip.org](http://www.7-zip.org)), to extract the files from the
    download archive.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用的是 Windows，可以下载一个免费的压缩工具，如 7-Zip ([http://www.7-zip.org](http://www.7-zip.org))，用来从下载档案中提取文件。
- en: 'Alternatively, you can directly unpack the gzip-compressed tarball archive
    directly in Python as follows:'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，你也可以直接在 Python 中解压 gzip 压缩的 tarball 文件，方法如下：
- en: '[PRE0]'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Preprocessing the movie dataset into a more convenient format
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将电影数据集预处理为更方便的格式
- en: Having successfully extracted the dataset, we will now assemble the individual
    text documents from the decompressed download archive into a single CSV file.
    In the following code section, we will be reading the movie reviews into a pandas
    `DataFrame` object, which can take up to 10 minutes on a standard desktop computer.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功解压数据集后，我们将把解压后的下载档案中的各个文本文件合并成一个单一的 CSV 文件。在接下来的代码段中，我们将把电影评论读取到一个 pandas
    `DataFrame` 对象中，这个过程在标准桌面计算机上可能需要最多 10 分钟。
- en: 'To visualize the progress and estimated time until completion, we will use
    the **Python Progress Indicator** (**PyPrind**, [https://pypi.python.org/pypi/PyPrind/](https://pypi.python.org/pypi/PyPrind/))
    package, which was developed several years ago for such purposes. PyPrind can
    be installed by executing the `pip install pyprind` command:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化进度和预计完成时间，我们将使用 **Python 进度指示器** (**PyPrind**, [https://pypi.python.org/pypi/PyPrind/](https://pypi.python.org/pypi/PyPrind/))
    包，该包是几年前为此类目的开发的。你可以通过执行 `pip install pyprind` 命令来安装 PyPrind：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding code, we first initialized a new progress bar object, `pbar`,
    with 50,000 iterations, which was the number of documents we were going to read
    in. Using the nested `for` loops, we iterated over the `train` and `test` subdirectories
    in the main `aclImdb` directory and read the individual text files from the `pos`
    and `neg` subdirectories that we eventually appended to the `df` pandas `DataFrame`,
    together with an integer class label (`1` = positive and `0` = negative).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们首先初始化了一个新的进度条对象 `pbar`，设置为 50,000 次迭代，这是我们要读取的文档数量。通过嵌套的 `for` 循环，我们遍历了主
    `aclImdb` 目录中的 `train` 和 `test` 子目录，并从 `pos` 和 `neg` 子目录中读取了各个文本文件，最终将它们与整数分类标签（`1`
    = 正面，`0` = 负面）一起添加到 `df` pandas `DataFrame` 中。
- en: Since the class labels in the assembled dataset are sorted, we will now shuffle
    `DataFrame` using the `permutation` function from the `np.random` submodule—this
    will be useful to split the dataset into training and test datasets in later sections,
    when we will stream the data from our local drive directly.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于合并后的数据集中的类标签是排序的，我们现在将使用 `np.random` 子模块中的 `permutation` 函数打乱 `DataFrame`——这将在后续部分中非常有用，当我们直接从本地驱动器流式传输数据时，用于将数据集分为训练集和测试集。
- en: 'For our own convenience, we will also store the assembled and shuffled movie
    review dataset as a CSV file:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们还将把合并并打乱的电影评论数据集存储为 CSV 文件：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Since we are going to use this dataset later in this chapter, let''s quickly
    confirm that we have successfully saved the data in the right format by reading
    in the CSV and printing an excerpt of the first three examples:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将在本章后续部分使用这个数据集，让我们快速确认一下我们是否成功地将数据保存为正确的格式，通过读取 CSV 文件并打印前三个示例的部分内容：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you are running the code examples in a Jupyter Notebook, you should now
    see the first three examples of the dataset, as shown in the following table:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在 Jupyter Notebook 中运行代码示例，你现在应该能看到数据集的前三个示例，如下表所示：
- en: '![](img/B13208_08_01.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_08_01.png)'
- en: 'As a sanity check, before we proceed to the next section, let''s make sure
    that the `DataFrame` contains all 50,000 rows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个合理性检查，在我们继续下一节之前，让我们确认一下 `DataFrame` 中包含了所有 50,000 行数据：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Introducing the bag-of-words model
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍词袋模型
- en: 'You may remember from *Chapter 4*, *Building Good Training Datasets – Data
    Preprocessing*, that we have to convert categorical data, such as text or words,
    into a numerical form before we can pass it on to a machine learning algorithm.
    In this section, we will introduce the **bag-of-words** model, which allows us
    to represent text as numerical feature vectors. The idea behind bag-of-words is
    quite simple and can be summarized as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得在*第4章*，*构建良好的训练数据集—数据预处理*中，我们需要将分类数据（如文本或单词）转换为数值形式，才能将其传递给机器学习算法。在本节中，我们将介绍**词袋模型**，它允许我们将文本表示为数值特征向量。词袋模型的核心思想非常简单，可以总结如下：
- en: We create a vocabulary of unique tokens—for example, words—from the entire set
    of documents.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从整个文档集合中创建唯一标记的词汇表——例如，单词。
- en: We construct a feature vector from each document that contains the counts of
    how often each word occurs in the particular document.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从每个文档中构建特征向量，该向量包含每个单词在特定文档中出现的次数。
- en: Since the unique words in each document represent only a small subset of all
    the words in the bag-of-words vocabulary, the feature vectors will mostly consist
    of zeros, which is why we call them **sparse**. Do not worry if this sounds too
    abstract; in the following subsections, we will walk through the process of creating
    a simple bag-of-words model step by step.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个文档中的唯一单词仅代表词袋词汇表中所有单词的一个小子集，因此特征向量大部分将由零组成，这就是为什么我们称它们为**稀疏**的原因。如果这听起来太抽象，不用担心；在接下来的小节中，我们将一步步演示如何创建一个简单的词袋模型。
- en: Transforming words into feature vectors
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将单词转换为特征向量
- en: 'To construct a bag-of-words model based on the word counts in the respective
    documents, we can use the `CountVectorizer` class implemented in scikit-learn.
    As you will see in the following code section, `CountVectorizer` takes an array
    of text data, which can be documents or sentences, and constructs the bag-of-words
    model for us:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了基于各个文档中的单词计数构建一个词袋模型，我们可以使用scikit-learn中实现的`CountVectorizer`类。如你将在接下来的代码部分看到的那样，`CountVectorizer`接受一个文本数据数组，这些数据可以是文档或句子，并为我们构建词袋模型：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'By calling the `fit_transform` method on `CountVectorizer`, we constructed
    the vocabulary of the bag-of-words model and transformed the following three sentences
    into sparse feature vectors:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在`CountVectorizer`上调用`fit_transform`方法，我们构建了词袋模型的词汇表，并将以下三个句子转换为稀疏特征向量：
- en: '`''The sun is shining''`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''太阳在照耀''`'
- en: '`''The weather is sweet''`'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''天气很甜美''`'
- en: '`''The sun is shining, the weather is sweet, and one and one is two''`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''太阳在照耀，天气很甜美，一加一等于二''`'
- en: 'Now, let''s print the contents of the vocabulary to get a better understanding
    of the underlying concepts:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打印出词汇表的内容，以更好地理解其中的基本概念：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As you can see from executing the preceding command, the vocabulary is stored
    in a Python dictionary that maps the unique words to integer indices. Next, let''s
    print the feature vectors that we just created:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如你从执行前面的命令中看到的那样，词汇表存储在一个Python字典中，该字典将唯一的单词映射到整数索引。接下来，让我们打印出我们刚刚创建的特征向量：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Each index position in the feature vectors shown here corresponds to the integer
    values that are stored as dictionary items in the `CountVectorizer` vocabulary.
    For example, the first feature at index position `0` resembles the count of the
    word `''and''`, which only occurs in the last document, and the word `''is''`,
    at index position `1` (the second feature in the document vectors), occurs in
    all three sentences. These values in the feature vectors are also called the **raw
    term frequencies**: *tf*(*t*, *d*)—the number of times a term, *t*, occurs in
    a document, *d*. It should be noted that, in the bag-of-words model, the word
    or term order in a sentence or document does not matter. The order in which the
    term frequencies appear in the feature vector is derived from the vocabulary indices,
    which are usually assigned alphabetically.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此处显示的特征向量中的每个索引位置对应于存储为字典项的`CountVectorizer`词汇表中的整数值。例如，索引位置`0`的第一个特征表示单词'and'的出现次数，这个单词只出现在最后一个文档中，而索引位置`1`的单词'is'（文档向量中的第二个特征）在所有三个句子中都出现。这些特征向量中的值也称为**原始词频**：*tf*(*t*,
    *d*)——词汇`t`在文档*d*中出现的次数。需要注意的是，在词袋模型中，句子或文档中单词或词语的顺序并不重要。词频在特征向量中出现的顺序是根据词汇表中的索引派生的，这些索引通常是按字母顺序分配的。
- en: '**N-gram models**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**N-gram模型**'
- en: 'The sequence of items in the bag-of-words model that we just created is also
    called the **1-gram** or **unigram** model—each item or token in the vocabulary
    represents a single word. More generally, the contiguous sequences of items in
    NLP—words, letters, or symbols—are also called **n-grams**. The choice of the
    number, *n*, in the n-gram model depends on the particular application; for example,
    a study by Ioannis Kanaris and others revealed that n-grams of size 3 and 4 yield
    good performances in the anti-spam filtering of email messages (*Words versus
    character n-grams for anti-spam filtering*, *Ioannis Kanaris*, *Konstantinos Kanaris*,
    *Ioannis Houvardas*, and *Efstathios Stamatatos*, *International Journal on Artificial
    Intelligence Tools*, *World Scientific Publishing Company*, 16(06): 1047-1067,
    *2007*).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '我们刚刚创建的词袋模型中的项目序列也被称为 **1-gram** 或 **unigram** 模型——词汇表中的每个项目或标记代表一个单词。更一般地说，NLP
    中的连续项序列——词、字母或符号——也被称为 **n-grams**。n-gram 模型中的数字 *n* 选择取决于具体应用；例如，Ioannis Kanaris
    等人进行的研究表明，大小为 3 和 4 的 n-gram 在反垃圾邮件邮件过滤中表现良好（*Words versus character n-grams for
    anti-spam filtering*, *Ioannis Kanaris*, *Konstantinos Kanaris*, *Ioannis Houvardas*,
    和 *Efstathios Stamatatos*, *International Journal on Artificial Intelligence Tools*,
    *World Scientific Publishing Company*, 16(06): 1047-1067, *2007*）。'
- en: 'To summarize the concept of the n-gram representation, the 1-gram and 2-gram
    representations of our first document "the sun is shining" would be constructed
    as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结 n-gram 表示法的概念，我们将构建我们第一个文档 "the sun is shining" 的 1-gram 和 2-gram 表示法如下：
- en: '1-gram: "the", "sun", "is", "shining"'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1-gram: "the", "sun", "is", "shining"'
- en: '2-gram: "the sun", "sun is", "is shining"'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2-gram: "the sun", "sun is", "is shining"'
- en: The `CountVectorizer` class in scikit-learn allows us to use different n-gram
    models via its `ngram_range` parameter. While a 1-gram representation is used
    by default, we could switch to a 2-gram representation by initializing a new `CountVectorizer`
    instance with `ngram_range=(2,2)`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 中的 `CountVectorizer` 类允许我们通过其 `ngram_range` 参数使用不同的 n-gram 模型。默认情况下使用的是
    1-gram 表示法，我们也可以通过用 `ngram_range=(2,2)` 初始化新的 `CountVectorizer` 实例来切换到 2-gram
    表示法。
- en: Assessing word relevancy via term frequency-inverse document frequency
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过词频-逆文档频率评估词语相关性
- en: 'When we are analyzing text data, we often encounter words that occur across
    multiple documents from both classes. These frequently occurring words typically
    don''t contain useful or discriminatory information. In this subsection, you will
    learn about a useful technique called the **term frequency-inverse document frequency**
    (**tf-idf**), which can be used to downweight these frequently occurring words
    in the feature vectors. The tf-idf can be defined as the product of the term frequency
    and the inverse document frequency:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析文本数据时，我们常常遇到一些在多个文档中都出现的词语。这些高频词通常不包含有用的或区分性的的信息。在本小节中，您将学习一种名为 **词频-逆文档频率**（**tf-idf**）的有用技术，它可以用于降低这些高频词在特征向量中的权重。tf-idf
    可以定义为词频和逆文档频率的乘积：
- en: '![](img/B13208_08_001.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_08_001.png)'
- en: 'Here, *tf*(*t*, *d*) is the term frequency that we introduced in the previous
    section, and *idf*(*t*, *d*) is the inverse document frequency, which can be calculated
    as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*tf*(*t*, *d*) 是我们在上一节中介绍的词频，*idf*(*t*, *d*) 是逆文档频率，可以通过以下方式计算：
- en: '![](img/B13208_08_002.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_08_002.png)'
- en: Here, ![](img/B13208_08_003.png) is the total number of documents, and *df*(*d*,
    *t*) is the number of documents, *d*, that contain the term *t*. Note that adding
    the constant 1 to the denominator is optional and serves the purpose of assigning
    a non-zero value to terms that occur in none of the training examples; the *log*
    is used to ensure that low document frequencies are not given too much weight.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_08_003.png) 是文档总数，*df*(*d*, *t*) 是包含词项 *t* 的文档数 *d*。注意，给分母加上常数
    1 是可选的，它的目的是为那些在任何训练样本中都没有出现的词项分配一个非零值；使用 *log* 是为了确保低文档频率不会被赋予过多的权重。
- en: 'The scikit-learn library implements yet another transformer, the `TfidfTransformer`
    class, which takes the raw term frequencies from the `CountVectorizer` class as
    input and transforms them into tf-idfs:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 库还实现了另一个变换器——`TfidfTransformer` 类，它将 `CountVectorizer` 类的原始词频作为输入，并将其转换为
    tf-idf：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you saw in the previous subsection, the word `'is'` had the largest term
    frequency in the third document, being the most frequently occurring word. However,
    after transforming the same feature vector into tf-idfs, the word `'is'` is now
    associated with a relatively small tf-idf (`0.45`) in the third document, since
    it is also present in the first and second document and thus is unlikely to contain
    any useful discriminatory information.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前一个小节中看到的，单词`'is'`在第三个文档中的词频最高，是最频繁出现的单词。然而，在将相同的特征向量转化为tf-idf后，单词`'is'`现在在第三个文档中的tf-idf值相对较小（`0.45`），因为它也出现在第一和第二个文档中，因此不太可能包含任何有用的区分信息。
- en: 'However, if we''d manually calculated the tf-idfs of the individual terms in
    our feature vectors, we would have noticed that `TfidfTransformer` calculates
    the tf-idfs slightly differently compared to the standard textbook equations that
    we defined previously. The equation for the inverse document frequency implemented
    in scikit-learn is computed as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们手动计算了特征向量中各个词的tf-idf值，我们会注意到`TfidfTransformer`计算tf-idf的方式与我们之前定义的标准教科书公式略有不同。scikit-learn中实现的逆文档频率公式计算如下：
- en: '![](img/B13208_08_004.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_08_004.png)'
- en: 'Similarly, the tf-idf computed in scikit-learn deviates slightly from the default
    equation we defined earlier:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，scikit-learn中计算的tf-idf与我们之前定义的标准公式略有不同：
- en: '![](img/B13208_08_005.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_08_005.png)'
- en: Note that the "+1" in the previous equations is due to setting `smooth_idf=True`
    in the previous code example, which is helpful for assigning zero-weight (that
    is, *idf*(*t*, *d*) = log(1) = 0) to terms that occur in all documents.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面公式中的“+1”是由于在之前的代码示例中设置了`smooth_idf=True`，这有助于对出现在所有文档中的词汇赋予零权重（即，*idf*（*t*，*d*）=
    log(1) = 0）。
- en: 'While it is also more typical to normalize the raw term frequencies before
    calculating the tf-idfs, the `TfidfTransformer` class normalizes the tf-idfs directly.
    By default (`norm=''l2''`), scikit-learn''s `TfidfTransformer` applies the L2-normalization,
    which returns a vector of length 1 by dividing an unnormalized feature vector,
    *v*, by its L2-norm:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在计算tf-idf之前对原始词频进行归一化更为常见，但`TfidfTransformer`类直接对tf-idf进行归一化。默认情况下（`norm='l2'`），scikit-learn的`TfidfTransformer`应用L2归一化，这通过将未归一化的特征向量*v*除以其L2范数，返回一个长度为1的向量：
- en: '![](img/B13208_08_006.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_08_006.png)'
- en: 'To make sure that we understand how `TfidfTransformer` works, let''s walk through
    an example and calculate the tf-idf of the word `''is''` in the third document.
    The word `''is''` has a term frequency of 3 (*tf* = 3) in the third document,
    and the document frequency of this term is 3 since the term `''is''` occurs in
    all three documents (*df* = 3). Thus, we can calculate the inverse document frequency
    as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们理解`TfidfTransformer`的工作原理，让我们通过一个示例来计算单词`'is'`在第三个文档中的tf-idf值。单词`'is'`在第三个文档中的词频是3（*tf*
    = 3），并且该词的文档频率为3，因为单词`'is'`出现在所有三个文档中（*df* = 3）。因此，我们可以按如下方式计算逆文档频率：
- en: '![](img/B13208_08_007.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_08_007.png)'
- en: 'Now, in order to calculate the tf-idf, we simply need to add 1 to the inverse
    document frequency and multiply it by the term frequency:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了计算tf-idf，我们只需要将1加到逆文档频率上，然后将其与词频相乘：
- en: '![](img/B13208_08_008.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_08_008.png)'
- en: 'If we repeated this calculation for all terms in the third document, we''d
    obtain the following tf-idf vectors: `[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0,
    1.69, 1.29]`. However, notice that the values in this feature vector are different
    from the values that we obtained from `TfidfTransformer` that we used previously.
    The final step that we are missing in this tf-idf calculation is the L2-normalization,
    which can be applied as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对第三个文档中的所有词语重复此计算，我们将得到以下tf-idf向量：`[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0,
    1.69, 1.29]`。然而，注意到这个特征向量中的数值与我们之前使用的`TfidfTransformer`得到的值不同。我们在这个tf-idf计算中缺少的最后一步是L2归一化，可以按如下方式应用：
- en: '![](img/B13208_08_009.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_08_009.png)'
- en: As you can see, the results now match the results returned by scikit-learn's
    `TfidfTransformer`, and since you now understand how tf-idfs are calculated, let's
    proceed to the next section and apply those concepts to the movie review dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，结果现在与scikit-learn的`TfidfTransformer`返回的结果一致，既然你现在理解了tf-idf是如何计算的，让我们继续进入下一部分，并将这些概念应用到电影评论数据集上。
- en: Cleaning text data
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清理文本数据
- en: In the previous subsections, we learned about the bag-of-words model, term frequencies,
    and tf-idfs. However, the first important step—before we build our bag-of-words
    model—is to clean the text data by stripping it of all unwanted characters.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的子章节中，我们了解了词袋模型、词频和TF-IDF。然而，首先重要的一步——在构建我们的词袋模型之前——是通过去除所有不需要的字符来清理文本数据。
- en: 'To illustrate why this is important, let''s display the last 50 characters
    from the first document in the reshuffled movie review dataset:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点的重要性，让我们展示重排后的电影评论数据集中第一篇文档的最后50个字符：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As you can see here, the text contains HTML markup as well as punctuation and
    other non-letter characters. While HTML markup does not contain many useful semantics,
    punctuation marks can represent useful, additional information in certain NLP
    contexts. However, for simplicity, we will now remove all punctuation marks except
    for emoticon characters, such as :), since those are certainly useful for sentiment
    analysis. To accomplish this task, we will use Python''s **regular expression**
    (**regex**) library, `re`, as shown here:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在这里看到的，文本中包含了HTML标记、标点符号和其他非字母字符。虽然HTML标记并不包含太多有用的语义，但在某些自然语言处理（NLP）上下文中，标点符号可以表示有用的附加信息。不过，为了简化，我们现在将移除所有标点符号，除了表情符号字符，如：:)
    ，因为这些显然对于情感分析是有用的。为了完成这个任务，我们将使用Python的**正则表达式**（**regex**）库`re`，如下面所示：
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Via the first regex, `<[^>]*>`, in the preceding code section, we tried to remove
    all of the HTML markup from the movie reviews. Although many programmers generally
    advise against the use of regex to parse HTML, this regex should be sufficient
    to *clean* this particular dataset. Since we are only interested in removing HTML
    markup and do not plan to use the HTML markup further, using regex to do the job
    should be acceptable. However, if you prefer using sophisticated tools for removing
    HTML markup from text, you can take a look at Python's HTML parser module, which
    is described at [https://docs.python.org/3/library/html.parser.html](https://docs.python.org/3/library/html.parser.html).
    After we removed the HTML markup, we used a slightly more complex regex to find
    emoticons, which we temporarily stored as emoticons. Next, we removed all non-word
    characters from the text via the regex `[\W]+` and converted the text into lowercase
    characters.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过前面代码段中的第一个正则表达式`<[^>]*>`，我们尝试从电影评论中移除所有的HTML标记。虽然许多程序员一般不建议使用正则表达式来解析HTML，但这个正则表达式应该足以*清理*这个特定的数据集。由于我们只关心移除HTML标记，并且不打算进一步使用HTML标记，因此使用正则表达式来完成这项工作是可以接受的。不过，如果你更倾向于使用更复杂的工具来从文本中移除HTML标记，可以查看Python的HTML解析器模块，相关内容描述在[https://docs.python.org/3/library/html.parser.html](https://docs.python.org/3/library/html.parser.html)中。在我们移除HTML标记之后，我们使用了一个稍微复杂一些的正则表达式来查找表情符号，并将其临时存储为表情符号。接着，我们通过正则表达式`[\W]+`移除文本中的所有非单词字符，并将文本转换为小写字母。
- en: '**Dealing with word capitalization**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**处理单词大小写**'
- en: In the context of this analysis, we assume that the capitalization of a word—for
    example, whether it appears at the beginning of a sentence—does not contain semantically
    relevant information. However, note that there are exceptions; for instance, we
    remove the notation of proper names. But again, in the context of this analysis,
    it is a simplifying assumption that the letter case does not contain information
    that is relevant for sentiment analysis.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次分析的背景下，我们假设一个单词的大小写——例如，单词是否出现在句子的开头——不包含有意义的语义信息。然而，需要注意的是，也有例外；例如，我们会去除专有名词的标记。但在本次分析的背景下，我们简化假设字母的大小写并不包含对于情感分析相关的信息。
- en: Eventually, we added the temporarily stored emoticons to the end of the processed
    document string. Additionally, we removed the *nose* character (- in :-)) from
    the emoticons for consistency.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们将临时存储的表情符号添加到处理后文档字符串的末尾。此外，我们还为了保持一致性，去除了表情符号中的*鼻子*字符（:-)中的-）。
- en: '**Regular expressions**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则表达式**'
- en: Although regular expressions offer an efficient and convenient approach to searching
    for characters in a string, they also come with a steep learning curve. Unfortunately,
    an in-depth discussion of regular expressions is beyond the scope of this book.
    However, you can find a great tutorial on the Google Developers portal at [https://developers.google.com/edu/python/regular-expressions](https://developers.google.com/edu/python/regular-expressions)
    or you can check out the official documentation of Python's `re` module at [https://docs.python.org/3.7/library/re.html](https://docs.python.org/3.7/library/re.html).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然正则表达式提供了一种高效且方便的方式来搜索字符串中的字符，但它们也有陡峭的学习曲线。不幸的是，关于正则表达式的深入讨论超出了本书的范围。然而，你可以在Google开发者门户网站找到一个很好的教程，地址是[https://developers.google.com/edu/python/regular-expressions](https://developers.google.com/edu/python/regular-expressions)，或者你也可以查看Python的`re`模块的官方文档，网址是[https://docs.python.org/3.7/library/re.html](https://docs.python.org/3.7/library/re.html)。
- en: 'Although the addition of the emoticon characters to the end of the cleaned
    document strings may not look like the most elegant approach, we must note that
    the order of the words doesn''t matter in our bag-of-words model if our vocabulary
    consists of only one-word tokens. But before we talk more about the splitting
    of documents into individual terms, words, or tokens, let''s confirm that our
    `preprocessor` function works correctly:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管将表情符号字符添加到清理后的文档字符串末尾可能看起来不是最优雅的方法，但我们必须注意，如果我们的词汇表仅包含单词级别的标记，那么在我们的词袋模型中，单词的顺序并不重要。但在我们进一步讨论如何将文档拆分为单独的术语、单词或标记之前，让我们先确认我们的`preprocessor`函数是否正常工作：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Lastly, since we will make use of the *cleaned* text data over and over again
    during the next sections, let''s now apply our `preprocessor` function to all
    the movie reviews in our `DataFrame`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于我们将在接下来的章节中反复使用*清理后的*文本数据，让我们现在将我们的`preprocessor`函数应用到`DataFrame`中的所有电影评论：
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Processing documents into tokens
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将文档处理成标记
- en: 'After successfully preparing the movie review dataset, we now need to think
    about how to split the text corpora into individual elements. One way to *tokenize*
    documents is to split them into individual words by splitting the cleaned documents
    at their whitespace characters:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 成功准备好电影评论数据集后，我们现在需要考虑如何将文本语料库拆分成单独的元素。*分词*文档的一种方法是通过在清理后的文档中按照空白字符拆分它们，将文档分割成单个单词：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the context of tokenization, another useful technique is **word stemming**,
    which is the process of transforming a word into its root form. It allows us to
    map related words to the same stem. The original stemming algorithm was developed
    by Martin F. Porter in 1979 and is hence known as the **Porter stemmer** algorithm
    (*An algorithm for suffix stripping*, *Martin F. Porter*, *Program: Electronic
    Library and Information Systems*, 14(3): 130–137, *1980*). The **Natural Language
    Toolkit** (**NLTK**, [http://www.nltk.org](http://www.nltk.org)) for Python implements
    the Porter stemming algorithm, which we will use in the following code section.
    In order to install the NLTK, you can simply execute `conda install nltk` or `pip
    install nltk`.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在分词的背景下，另一个有用的技术是**词干提取**，即将一个词转化为其词根形式的过程。它允许我们将相关的词映射到同一个词干。最初的词干提取算法是由Martin
    F. Porter于1979年开发的，因此被称为**Porter词干提取算法**（*一个后缀剥离算法*，*Martin F. Porter*，*程序：电子图书与信息系统*，14(3)：130–137，*1980*）。Python的**自然语言工具包**（**NLTK**，[http://www.nltk.org](http://www.nltk.org)）实现了Porter词干提取算法，我们将在以下代码部分中使用它。为了安装NLTK，你可以简单地执行`conda
    install nltk`或`pip install nltk`。
- en: '**NLTK online book**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**NLTK在线书籍**'
- en: Although the NLTK is not the focus of this chapter, I highly recommend that
    you visit the NLTK website as well as read the official NLTK book, which is freely
    available at [http://www.nltk.org/book/](http://www.nltk.org/book/), if you are
    interested in more advanced applications in NLP.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管NLTK不是本章的重点，但如果你对NLP中的更高级应用感兴趣，我强烈推荐你访问NLTK网站，并阅读官方的NLTK书籍，该书籍可以在[http://www.nltk.org/book/](http://www.nltk.org/book/)免费获得。
- en: 'The following code shows how to use the Porter stemming algorithm:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用Porter词干提取算法：
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Using the `PorterStemmer` from the `nltk` package, we modified our `tokenizer`
    function to reduce words to their root form, which was illustrated by the simple
    preceding example where the word `'running'` was *stemmed* to its root form `'run'`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`nltk`包中的`PorterStemmer`，我们修改了我们的`tokenizer`函数，将单词还原为其词根形式，这在前面的简单示例中有所展示，其中单词`'running'`被*词干提取*为词根形式`'run'`。
- en: '**Stemming algorithms**'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**词干提取算法**'
- en: The Porter stemming algorithm is probably the oldest and simplest stemming algorithm.
    Other popular stemming algorithms include the newer **Snowball stemmer** (Porter2
    or English stemmer) and the **Lancaster stemmer** (Paice/Husk stemmer). While
    both the Snowball and Lancaster stemmers are faster than the original Porter stemmer,
    the Lancaster stemmer is also notorious for being more aggressive than the Porter
    stemmer. These alternative stemming algorithms are also available through the
    NLTK package ([http://www.nltk.org/api/nltk.stem.html](http://www.nltk.org/api/nltk.stem.html)).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Porter词干提取算法可能是最古老且最简单的词干提取算法。其他流行的词干提取算法包括更新版的**Snowball词干提取器**（Porter2或英语词干提取器）和**Lancaster词干提取器**（Paice/Husk词干提取器）。尽管Snowball和Lancaster词干提取器比原始的Porter词干提取器更快，但Lancaster词干提取器因比Porter词干提取器更具侵略性而臭名昭著。这些替代的词干提取算法也可以通过NLTK包使用（[http://www.nltk.org/api/nltk.stem.html](http://www.nltk.org/api/nltk.stem.html)）。
- en: While stemming can create non-real words, such as `'thu'` (from `'thus'`), as
    shown in the previous example, a technique called **lemmatization** aims to obtain
    the canonical (grammatically correct) forms of individual words—the so-called
    **lemmas**. However, lemmatization is computationally more difficult and expensive
    compared to stemming and, in practice, it has been observed that stemming and
    lemmatization have little impact on the performance of text classification (*Influence
    of Word Normalization on Text Classification*, *Michal Toman*, *Roman Tesar*,
    and *Karel Jezek*, *Proceedings of InSciT*, pages 354–358, *2006*).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然词干提取可能会产生非真实单词，例如从`thus`提取的`'thu'`，正如前面的例子所示，但一种叫做**词形还原**的技术旨在获得单个单词的标准（语法正确）形式——即所谓的**词根**。然而，与词干提取相比，词形还原在计算上更为复杂且成本更高，而且在实践中，已观察到词干提取和词形还原对文本分类的性能几乎没有影响（*单词规范化对文本分类的影响*，*Michal
    Toman*，*Roman Tesar*，和*Karel Jezek*，*InSciT会议录*，第354-358页，*2006*）。
- en: Before we jump into the next section, where we will train a machine learning
    model using the bag-of-words model, let's briefly talk about another useful topic
    called **stop-word removal**. Stop-words are simply those words that are extremely
    common in all sorts of texts and probably bear no (or only a little) useful information
    that can be used to distinguish between different classes of documents. Examples
    of stop-words are *is*, *and*, *has*, and *like*. Removing stop-words can be useful
    if we are working with raw or normalized term frequencies rather than tf-idfs,
    which are already downweighting frequently occurring words.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一节之前，我们将训练一个机器学习模型，使用词袋模型，先简要谈谈另一个有用的主题——**停用词移除**。停用词是指在各种文本中非常常见的单词，这些单词可能没有（或仅有少量）可用来区分不同文档类别的有用信息。停用词的例子有*is*，*and*，*has*和*like*。如果我们正在处理原始或规范化的词频，而不是已经对高频词进行降权的tf-idf，移除停用词可能会有所帮助。
- en: 'In order to remove stop-words from the movie reviews, we will use the set of
    127 English stop-words that is available from the NLTK library, which can be obtained
    by calling the `nltk.download` function:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从电影评论中移除停用词，我们将使用来自NLTK库的127个英语停用词集合，您可以通过调用`nltk.download`函数来获取该集合：
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Training a logistic regression model for document classification
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个逻辑回归模型进行文档分类
- en: 'In this section, we will train a logistic regression model to classify the
    movie reviews into *positive* and *negative* reviews based on the bag-of-words
    model. First, we will divide the `DataFrame` of cleaned text documents into 25,000
    documents for training and 25,000 documents for testing:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将训练一个逻辑回归模型，根据词袋模型将电影评论分类为*正面*和*负面*评论。首先，我们将把清理后的文本文档的`DataFrame`分成25,000篇训练文档和25,000篇测试文档：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we will use a `GridSearchCV` object to find the optimal set of parameters
    for our logistic regression model using 5-fold stratified cross-validation:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`GridSearchCV`对象，通过5折分层交叉验证，找到逻辑回归模型的最佳参数组合：
- en: '[PRE17]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Multiprocessing via the n_jobs parameter**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过n_jobs参数进行多进程处理**'
- en: Please note that it is highly recommended to set `n_jobs=-1` (instead of `n_jobs=1`)
    in the previous code example to utilize all available cores on your machine and
    speed up the grid search. However, some Windows users reported issues when running
    the previous code with the `n_jobs=-1` setting related to pickling the `tokenizer`
    and `tokenizer_porter` functions for multiprocessing on Windows. Another workaround
    would be to replace those two functions, `[tokenizer, tokenizer_porter]`, with
    `[str.split]`. However, note that replacement by the simple `str.split` would
    not support stemming.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，强烈建议在前面的代码示例中将`n_jobs=-1`（而不是`n_jobs=1`）设置为利用机器上的所有可用核心，从而加速网格搜索。然而，一些Windows用户在使用`n_jobs=-1`设置时遇到了与Windows上的`tokenizer`和`tokenizer_porter`函数的多进程序列化相关的问题。另一种解决方法是将这两个函数`[tokenizer,
    tokenizer_porter]`替换为`[str.split]`。但是，请注意，使用简单的`str.split`替换后，将不支持词干化。
- en: When we initialized the `GridSearchCV` object and its parameter grid using the
    preceding code, we restricted ourselves to a limited number of parameter combinations,
    since the number of feature vectors, as well as the large vocabulary, can make
    the grid search computationally quite expensive. Using a standard desktop computer,
    our grid search may take up to 40 minutes to complete.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用前面的代码初始化`GridSearchCV`对象及其参数网格时，我们将参数组合的数量限制在一定范围内，因为特征向量的数量以及大量的词汇量可能会使网格搜索的计算开销非常大。在标准台式计算机上，网格搜索可能需要最多40分钟才能完成。
- en: In the previous code example, we replaced `CountVectorizer` and `TfidfTransformer`
    from the previous subsection with `TfidfVectorizer`, which combines `CountVectorizer`
    with the `TfidfTransformer`. Our `param_grid` consisted of two parameter dictionaries.
    In the first dictionary, we used `TfidfVectorizer` with its default settings (`use_idf=True`,
    `smooth_idf=True`, and `norm='l2'`) to calculate the tf-idfs; in the second dictionary,
    we set those parameters to `use_idf=False`, `smooth_idf=False`, and `norm=None`
    in order to train a model based on raw term frequencies. Furthermore, for the
    logistic regression classifier itself, we trained models using L2 and L1 regularization
    via the penalty parameter and compared different regularization strengths by defining
    a range of values for the inverse-regularization parameter `C`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们将上一节中的`CountVectorizer`和`TfidfTransformer`替换为`TfidfVectorizer`，它将`CountVectorizer`和`TfidfTransformer`结合起来。我们的`param_grid`包含了两个参数字典。在第一个字典中，我们使用`TfidfVectorizer`的默认设置（`use_idf=True`、`smooth_idf=True`和`norm='l2'`）来计算tf-idfs；在第二个字典中，我们将这些参数设置为`use_idf=False`、`smooth_idf=False`和`norm=None`，以便基于原始词频训练模型。此外，对于逻辑回归分类器，我们通过惩罚参数训练了使用L2和L1正则化的模型，并通过定义反向正则化参数`C`的值范围来比较不同的正则化强度。
- en: 'After the grid search has finished, we can print the best parameter set:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索完成后，我们可以打印出最佳的参数集：
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see in the preceding output, we obtained the best grid search results
    using the regular `tokenizer` without Porter stemming, no stop-word library, and
    tf-idfs in combination with a logistic regression classifier that uses L2-regularization
    with the regularization strength `C` of `10.0`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的输出所示，我们使用常规的`tokenizer`（不使用Porter词干化、没有停用词库，并且结合tf-idfs）与逻辑回归分类器（使用L2正则化，并设置正则化强度`C`为`10.0`）获得了最佳的网格搜索结果。
- en: 'Using the best model from this grid search, let''s print the average 5-fold
    cross-validation accuracy scores on the training dataset and the classification
    accuracy on the test dataset:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此网格搜索得到的最佳模型，我们可以打印出训练数据集的平均5折交叉验证准确率和测试数据集的分类准确率：
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The results reveal that our machine learning model can predict whether a movie
    review is positive or negative with 90 percent accuracy.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，我们的机器学习模型能够以90%的准确率预测电影评论是正面还是负面。
- en: '**The naïve Bayes classifier**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯分类器**'
- en: A still very popular classifier for text classification is the naïve Bayes classifier,
    which gained popularity in applications of email spam filtering. Naïve Bayes classifiers
    are easy to implement, computationally efficient, and tend to perform particularly
    well on relatively small datasets compared to other algorithms. Although we don't
    discuss naïve Bayes classifiers in this book, the interested reader can find an
    article about naïve Bayes text classification that is freely available on arXiv
    (*Naive Bayes and Text Classification I – Introduction and Theory*, *S. Raschka*,
    *Computing Research Repository* (*CoRR*), abs/1410.5329, *2014*, [http://arxiv.org/pdf/1410.5329v3.pdf](http://arxiv.org/pdf/1410.5329v3.pdf)).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一种仍然非常流行的文本分类器是朴素贝叶斯分类器，它在电子邮件垃圾邮件过滤应用中获得了广泛使用。朴素贝叶斯分类器实现简单、计算高效，且在相对较小的数据集上通常表现特别好，优于其他算法。尽管本书中没有讨论朴素贝叶斯分类器，但有兴趣的读者可以在arXiv上找到一篇关于朴素贝叶斯文本分类的文章（*Naive
    Bayes and Text Classification I – Introduction and Theory*，*S. Raschka*，*Computing
    Research Repository* (*CoRR*)，abs/1410.5329，*2014*， [http://arxiv.org/pdf/1410.5329v3.pdf](http://arxiv.org/pdf/1410.5329v3.pdf)）。
- en: Working with bigger data – online algorithms and out-of-core learning
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理更大的数据 – 在线算法与外部核心学习
- en: If you executed the code examples in the previous section, you may have noticed
    that it could be computationally quite expensive to construct the feature vectors
    for the 50,000-movie review dataset during grid search. In many real-world applications,
    it is not uncommon to work with even larger datasets that can exceed our computer's
    memory. Since not everyone has access to supercomputer facilities, we will now
    apply a technique called **out-of-core learning**, which allows us to work with
    such large datasets by fitting the classifier incrementally on smaller batches
    of a dataset.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你执行了上一节的代码示例，你可能已经注意到，在网格搜索过程中，为50,000条电影评论数据集构建特征向量可能非常耗费计算资源。在许多现实世界的应用中，处理更大的数据集也并不罕见，这些数据集可能超过我们计算机的内存容量。由于并非每个人都有超级计算机设施的使用权限，我们将应用一种叫做**外部核心学习（out-of-core
    learning）**的技术，它可以通过在数据集的小批次上逐步训练分类器，从而使我们能够处理这些大规模数据集。
- en: '**Text classification with recurrent neural networks**'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用递归神经网络进行文本分类**'
- en: In *Chapter 16*, *Modeling Sequential Data Using Recurrent Neural Networks*,
    we will revisit this dataset and train a deep learning-based classifier (a recurrent
    neural network) to classify the reviews in the IMDb movie review dataset. This
    neural network-based classifier follows the same out-of-core principle using the
    stochastic gradient descent optimization algorithm but does not require the construction
    of a bag-of-words model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第16章*，*使用递归神经网络建模序列数据*中，我们将重新访问这个数据集，并训练一个基于深度学习的分类器（递归神经网络），以对IMDb电影评论数据集中的评论进行分类。这个基于神经网络的分类器采用与外部核心学习相同的原则，使用随机梯度下降优化算法，但不需要构建词袋模型。
- en: Back in *Chapter 2*, *Training Simple Machine Learning Algorithms for Classification*,
    the concept of **stochastic gradient descent** was introduced; it is an optimization
    algorithm that updates the model's weights using one example at a time. In this
    section, we will make use of the `partial_fit` function of `SGDClassifier` in
    scikit-learn to stream the documents directly from our local drive and train a
    logistic regression model using small mini-batches of documents.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第2章*，*训练简单的机器学习算法进行分类*中，介绍了**随机梯度下降**的概念；它是一种优化算法，通过每次使用一个示例来更新模型的权重。在本节中，我们将利用scikit-learn中`SGDClassifier`的`partial_fit`函数，直接从本地驱动器流式读取文档，并使用小批量文档训练一个逻辑回归模型。
- en: 'First, we will define a `tokenizer` function that cleans the unprocessed text
    data from the `movie_data.csv` file that we constructed at the beginning of this
    chapter and separate it into word tokens while removing stop-words:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义一个`tokenizer`函数，用于清理我们在本章开头构建的`movie_data.csv`文件中的未处理文本数据，并将其拆分为词语标记，同时去除停用词：
- en: '[PRE20]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we will define a generator function, `stream_docs`, that reads in and
    returns one document at a time:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个生成器函数`stream_docs`，该函数一次读取并返回一份文档：
- en: '[PRE21]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To verify that our `stream_docs` function works correctly, let''s read in the
    first document from the `movie_data.csv` file, which should return a tuple consisting
    of the review text as well as the corresponding class label:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的`stream_docs`函数是否正常工作，让我们从`movie_data.csv`文件中读取第一份文档，该函数应返回一个元组，其中包括评论文本及其对应的类标签：
- en: '[PRE22]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We will now define a function, `get_minibatch`, that will take a document stream
    from the `stream_docs` function and return a particular number of documents specified
    by the `size` parameter:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义一个函数`get_minibatch`，该函数将从`stream_docs`函数中获取文档流，并返回由`size`参数指定的特定数量的文档：
- en: '[PRE23]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Unfortunately, we can''t use `CountVectorizer` for out-of-core learning since
    it requires holding the complete vocabulary in memory. Also, `TfidfVectorizer`
    needs to keep all the feature vectors of the training dataset in memory to calculate
    the inverse document frequencies. However, another useful vectorizer for text
    processing implemented in scikit-learn is `HashingVectorizer`. `HashingVectorizer`
    is data-independent and makes use of the hashing trick via the 32-bit `MurmurHash3`
    function by Austin Appleby ([https://sites.google.com/site/murmurhash/](https://sites.google.com/site/murmurhash/)):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们无法使用`CountVectorizer`进行外部核心学习，因为它要求将完整的词汇表保存在内存中。另外，`TfidfVectorizer`需要将训练数据集的所有特征向量保存在内存中以计算逆文档频率。然而，scikit-learn中另一个有用的文本处理向量化工具是`HashingVectorizer`。`HashingVectorizer`与数据无关，并通过Austin
    Appleby的32位`MurmurHash3`哈希函数使用哈希技巧（[https://sites.google.com/site/murmurhash/](https://sites.google.com/site/murmurhash/)）：
- en: '[PRE24]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Using the preceding code, we initialized `HashingVectorizer` with our `tokenizer`
    function and set the number of features to `2**21`. Furthermore, we reinitialized
    a logistic regression classifier by setting the `loss` parameter of `SGDClassifier`
    to `'log'`. Note that by choosing a large number of features in `HashingVectorizer`,
    we reduce the chance of causing hash collisions, but we also increase the number
    of coefficients in our logistic regression model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述代码，我们通过`tokenizer`函数初始化了`HashingVectorizer`，并将特征数量设置为`2**21`。此外，我们通过将`SGDClassifier`的`loss`参数设置为`'log'`重新初始化了逻辑回归分类器。请注意，通过选择`HashingVectorizer`中的大量特征，我们减少了哈希碰撞的可能性，但也增加了逻辑回归模型中的系数数量。
- en: 'Now comes the really interesting part – having set up all the complementary
    functions, we can start the out-of-core learning using the following code:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在到了真正有趣的部分——在设置好所有的互补功能后，我们可以使用以下代码开始进行外部核心学习：
- en: '[PRE25]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Again, we made use of the PyPrind package in order to estimate the progress
    of our learning algorithm. We initialized the progress bar object with 45 iterations
    and, in the following `for` loop, we iterated over 45 mini-batches of documents
    where each mini-batch consists of 1,000 documents. Having completed the incremental
    learning process, we will use the last 5,000 documents to evaluate the performance
    of our model:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们利用了PyPrind包来估计学习算法的进度。我们将进度条对象初始化为45次迭代，在接下来的`for`循环中，我们遍历了45个小批次的文档，每个小批次由1,000个文档组成。在完成增量学习过程后，我们将使用最后5,000个文档来评估模型的性能：
- en: '[PRE26]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As you can see, the accuracy of the model is approximately 87 percent, slightly
    below the accuracy that we achieved in the previous section using the grid search
    for hyperparameter tuning. However, out-of-core learning is very memory efficient
    and it took less than a minute to complete. Finally, we can use the last 5,000
    documents to update our model:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，该模型的准确率约为87%，略低于我们在上一节中通过网格搜索超参数调优所得到的准确率。然而，外部核心学习非常节省内存，完成的时间不到一分钟。最后，我们可以使用最后5,000个文档来更新我们的模型：
- en: '[PRE27]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**The word2vec model**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**word2vec模型**'
- en: A more modern alternative to the bag-of-words model is **word2vec**, an algorithm
    that Google released in 2013 (*Efficient Estimation of Word Representations in
    Vector Space*, *T. Mikolov*, *K. Chen*, *G. Corrado*, and *J. Dean*, arXiv preprint
    arXiv:1301.3781, *2013*).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一个比词袋模型更现代的替代方案是**word2vec**，这是Google在2013年发布的一种算法（*高效估计词向量表示*，*T. Mikolov*，*K.
    Chen*，*G. Corrado*，*J. Dean*，arXiv预印本arXiv:1301.3781，*2013*）。
- en: The word2vec algorithm is an unsupervised learning algorithm based on neural
    networks that attempts to automatically learn the relationship between words.
    The idea behind word2vec is to put words that have similar meanings into similar
    clusters, and via clever vector-spacing, the model can reproduce certain words
    using simple vector math, for example, *king* – *man* + *woman* = *queen*.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec算法是一种基于神经网络的无监督学习算法，旨在自动学习单词之间的关系。word2vec的思想是将意义相似的单词聚集在相似的簇中，通过巧妙的向量间距，模型可以使用简单的向量数学重新生成某些单词，例如，*king*
    – *man* + *woman* = *queen*。
- en: The original C-implementation with useful links to the relevant papers and alternative
    implementations can be found at [https://code.google.com/p/word2vec/](https://code.google.com/p/word2vec/).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的C语言实现，以及相关论文和其他实现的有用链接，可以在[https://code.google.com/p/word2vec/](https://code.google.com/p/word2vec/)找到。
- en: Topic modeling with Latent Dirichlet Allocation
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用潜在狄利克雷分配进行主题建模
- en: '**Topic modeling** describes the broad task of assigning topics to unlabeled
    text documents. For example, a typical application would be the categorization
    of documents in a large text corpus of newspaper articles. In applications of
    topic modeling, we then aim to assign category labels to those articles, for example,
    sports, finance, world news, politics, local news, and so forth. Thus, in the
    context of the broad categories of machine learning that we discussed in *Chapter
    1*, *Giving Computers the Ability to Learn from Data*, we can consider topic modeling
    as a clustering task, a subcategory of unsupervised learning.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**主题建模**描述了将主题分配给无标签文本文档的广泛任务。例如，一个典型的应用场景是将大量报纸文章文本集合中的文档进行分类。在主题建模的应用中，我们的目标是将这些文章分配到不同的类别标签，如体育、财经、世界新闻、政治、地方新闻等。因此，在我们在*第1章*《赋予计算机从数据中学习的能力》中讨论的机器学习广泛类别的背景下，我们可以将主题建模视为一种聚类任务，它是无监督学习的一个子类别。'
- en: In this section, we will discuss a popular technique for topic modeling called
    **Latent Dirichlet Allocation** (**LDA**). However, note that while Latent Dirichlet
    Allocation is often abbreviated as LDA, it is not to be confused with *linear
    discriminant analysis*, a supervised dimensionality reduction technique that was
    introduced in *Chapter 5*, *Compressing Data via Dimensionality Reduction*.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一种流行的主题建模技术，称为**潜在狄利克雷分配**（**LDA**）。然而，请注意，尽管潜在狄利克雷分配通常缩写为LDA，但它不能与*线性判别分析*混淆，后者是一种监督式的降维技术，在*第5章*《通过降维压缩数据》中进行了介绍。
- en: '**Embedding the movie review classifier into a web application**'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**将电影评论分类器嵌入到Web应用程序中**'
- en: LDA is different from the supervised learning approach that we took in this
    chapter to classify movie reviews as positive and negative. Thus, if you are interested
    in embedding scikit-learn models into a web application via the Flask framework
    using the movie reviewer as an example, please feel free to jump to the next chapter
    and revisit this standalone section on topic modeling later on.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: LDA不同于我们在本章中采用的监督学习方法，通过该方法我们将电影评论分类为正面和负面。因此，如果你有兴趣通过Flask框架将scikit-learn模型嵌入到Web应用程序中，并以电影评论为例，请随时跳到下一章，并在稍后的时间回到这一独立的主题建模部分。
- en: Decomposing text documents with LDA
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LDA分解文本文档
- en: 'Since the mathematics behind LDA is quite involved and requires knowledge about
    Bayesian inference, we will approach this topic from a practitioner''s perspective
    and interpret LDA using layman''s terms. However, the interested reader can read
    more about LDA in the following research paper: *Latent Dirichlet Allocation*,
    *David M. Blei*, *Andrew Y. Ng*, and *Michael I. Jordan*, *Journal of Machine
    Learning Research 3*, pages: 993-1022, *Jan 2003*.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LDA背后的数学内容较为复杂，并且需要一定的贝叶斯推断知识，我们将从实践者的角度来探讨这个主题，并用通俗的语言解释LDA。然而，感兴趣的读者可以通过以下研究论文了解更多关于LDA的内容：《潜在狄利克雷分配》（*Latent
    Dirichlet Allocation*），*David M. Blei*，*Andrew Y. Ng*，和*Michael I. Jordan*，*机器学习研究期刊
    3*，第993-1022页，*2003年1月*。
- en: 'LDA is a generative probabilistic model that tries to find groups of words
    that appear frequently together across different documents. These frequently appearing
    words represent our topics, assuming that each document is a mixture of different
    words. The input to an LDA is the bag-of-words model that we discussed earlier
    in this chapter. Given a bag-of-words matrix as input, LDA decomposes it into
    two new matrices:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: LDA是一种生成式概率模型，它试图找到在不同文档中频繁同时出现的词组。这些频繁出现的词代表我们的主题，假设每个文档都是不同词的混合体。LDA的输入是我们在本章早些时候讨论的词袋模型。给定一个词袋矩阵作为输入，LDA将其分解为两个新的矩阵：
- en: A document-to-topic matrix
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档到主题矩阵
- en: A word-to-topic matrix
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词到主题矩阵
- en: LDA decomposes the bag-of-words matrix in such a way that if we multiply those
    two matrices together, we will be able to reproduce the input, the bag-of-words
    matrix, with the lowest possible error. In practice, we are interested in those
    topics that LDA found in the bag-of-words matrix. The only downside may be that
    we must define the number of topics beforehand—the number of topics is a hyperparameter
    of LDA that has to be specified manually.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 将词袋矩阵分解成两部分，若我们将这两个矩阵相乘，就能够以最低的误差重建输入，即词袋矩阵。实际上，我们关注的是 LDA 在词袋矩阵中找到的那些主题。唯一的缺点可能是我们必须预先定义主题的数量——主题的数量是
    LDA 的一个超参数，必须手动指定。
- en: LDA with scikit-learn
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 进行 LDA
- en: In this subsection, we will use the `LatentDirichletAllocation` class implemented
    in scikit-learn to decompose the movie review dataset and categorize it into different
    topics. In the following example, we will restrict the analysis to 10 different
    topics, but readers are encouraged to experiment with the hyperparameters of the
    algorithm to further explore the topics that can be found in this dataset.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将使用 scikit-learn 实现的 `LatentDirichletAllocation` 类，来分解电影评论数据集，并将其分类为不同的主题。在以下示例中，我们将分析限制为
    10 个不同的主题，但鼓励读者尝试调整算法的超参数，以进一步探索该数据集中可以找到的主题。
- en: 'First, we are going to load the dataset into a pandas `DataFrame` using the
    local `movie_data.csv` file of the movie reviews that we created at the beginning
    of this chapter:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用本章开头创建的本地 `movie_data.csv` 文件，将数据集加载到 pandas `DataFrame` 中：
- en: '[PRE28]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Next, we are going to use the already familiar `CountVectorizer` to create the
    bag-of-words matrix as input to the LDA.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用已经熟悉的 `CountVectorizer` 来创建词袋矩阵，作为 LDA 的输入。
- en: 'For convenience, we will use scikit-learn''s built-in English stop-word library
    via `stop_words=''english''`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，我们将通过 `stop_words='english'` 使用 scikit-learn 内置的英语停用词库：
- en: '[PRE29]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Notice that we set the maximum document frequency of words to be considered
    to 10 percent (`max_df=.1`) to exclude words that occur too frequently across
    documents. The rationale behind the removal of frequently occurring words is that
    these might be common words appearing across all documents that are, therefore,
    less likely to be associated with a specific topic category of a given document.
    Also, we limited the number of words to be considered to the most frequently occurring
    5,000 words (`max_features=5000`), to limit the dimensionality of this dataset
    to improve the inference performed by LDA. However, both `max_df=.1` and `max_features=5000`
    are hyperparameter values chosen arbitrarily, and readers are encouraged to tune
    them while comparing the results.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将考虑的词的最大文档频率设置为 10% (`max_df=.1`)，以排除在文档中出现频率过高的词。移除频繁出现的词的理由是，这些词可能是所有文档中常见的词，因此不太可能与给定文档的特定主题类别相关联。同时，我们将考虑的词的数量限制为最常出现的
    5000 个词 (`max_features=5000`)，以限制数据集的维度，从而提高 LDA 推理的效果。然而，`max_df=.1` 和 `max_features=5000`
    都是任意选择的超参数值，鼓励读者在比较结果时进行调优。
- en: 'The following code example demonstrates how to fit a `LatentDirichletAllocation`
    estimator to the bag-of-words matrix and infer the 10 different topics from the
    documents (note that the model fitting can take up to 5 minutes or more on a laptop
    or standard desktop computer):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例演示了如何将 `LatentDirichletAllocation` 估算器拟合到词袋矩阵上，并从文档中推断出 10 个不同的主题（请注意，模型拟合可能需要
    5 分钟或更长时间，具体取决于笔记本电脑或标准桌面电脑的性能）：
- en: '[PRE30]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: By setting `learning_method='batch'`, we let the `lda` estimator do its estimation
    based on all available training data (the bag-of-words matrix) in one iteration,
    which is slower than the alternative `'online'` learning method but can lead to
    more accurate results (setting `learning_method='online'` is analogous to online
    or mini-batch learning, which we discussed in *Chapter 2*, *Training Simple Machine
    Learning Algorithms for Classification*, and in this chapter).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置 `learning_method='batch'`，我们让 `lda` 估算器基于所有可用的训练数据（词袋矩阵）进行一次性估算，这比替代的 `'online'`
    学习方法要慢，但可能会导致更准确的结果（设置 `learning_method='online'` 类似于在线或小批量学习，我们在 *第 2 章* *训练简单的机器学习分类算法*
    和本章中有讨论）。
- en: '**Expectation-Maximization**'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**期望最大化**'
- en: 'The scikit-learn library''s implementation of LDA uses the **expectation-maximization**
    (**EM**) algorithm to update its parameter estimates iteratively. We haven''t
    discussed the EM algorithm in this chapter, but if you are curious to learn more,
    please see the excellent overview on Wikipedia ([https://en.wikipedia.org/wiki/Expectation–maximization_algorithm](https://en.wikipedia.org/wiki/Expectation-maximization_algorithm))
    and the detailed tutorial on how it is used in LDA in Colorado Reed''s tutorial,
    *Latent Dirichlet Allocation: Towards a Deeper Understanding*, which is freely
    available at [http://obphio.us/pdfs/lda_tutorial.pdf](http://obphio.us/pdfs/lda_tutorial.pdf).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 'scikit-learn 库中 LDA 的实现使用了 **期望最大化**（**EM**）算法，通过迭代更新其参数估计。我们在本章中没有讨论 EM 算法，但如果你有兴趣了解更多内容，请参考
    Wikipedia 上的精彩概述（[https://en.wikipedia.org/wiki/Expectation–maximization_algorithm](https://en.wikipedia.org/wiki/Expectation-maximization_algorithm)）以及
    Colorado Reed 的教程《Latent Dirichlet Allocation: Towards a Deeper Understanding》，该教程详细介绍了如何在
    LDA 中使用 EM 算法，且可在 [http://obphio.us/pdfs/lda_tutorial.pdf](http://obphio.us/pdfs/lda_tutorial.pdf)
    免费阅读。'
- en: 'After fitting the LDA, we now have access to the `components_` attribute of
    the `lda` instance, which stores a matrix containing the word importance (here,
    `5000`) for each of the 10 topics in increasing order:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合 LDA 模型后，我们现在可以访问 `lda` 实例的 `components_` 属性，该属性存储了一个矩阵，包含了每个主题的单词重要性（此处为
    `5000`），并按递增顺序排列：
- en: '[PRE31]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To analyze the results, let''s print the five most important words for each
    of the 10 topics. Note that the word importance values are ranked in increasing
    order. Thus, to print the top five words, we need to sort the topic array in reverse
    order:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析结果，我们首先打印每个主题的五个最重要的单词。请注意，单词的重要性值是按递增顺序排名的。因此，为了打印出前五个单词，我们需要将主题数组按逆序排序：
- en: '[PRE32]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Based on reading the five most important words for each topic, you may guess
    that the LDA identified the following topics:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 基于每个主题的五个最重要单词，你可能会猜测 LDA 识别出了以下几个主题：
- en: Generally bad movies (not really a topic category)
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一般来说，糟糕的电影（这并不是真正的主题类别）
- en: Movies about families
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于家庭的电影
- en: War movies
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 战争电影
- en: Art movies
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 艺术电影
- en: Crime movies
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 犯罪电影
- en: Horror movies
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 恐怖电影
- en: Comedy movies reviews
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 喜剧电影评论
- en: Movies somehow related to TV shows
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与电视节目某种程度相关的电影
- en: Movies based on books
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于书籍改编的电影
- en: Action movies
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动作电影
- en: 'To confirm that the categories make sense based on the reviews, let''s plot
    three movies from the horror movie category (horror movies belong to category
    6 at index position `5`):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认这些类别是否合理，我们绘制了三个来自恐怖电影类别的电影（恐怖电影属于类别 6，索引位置为 `5`）：
- en: '[PRE33]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Using the preceding code example, we printed the first 300 characters from
    the top three horror movies. The reviews—even though we don''t know which exact
    movie they belong to—sound like reviews of horror movies (however, one might argue
    that `Horror movie #2` could also be a good fit for topic category 1: *Generally
    bad movies*).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '使用前面的代码示例，我们打印了前三部恐怖电影的前 300 个字符。评论——尽管我们不知道它们确切属于哪部电影——听起来像是恐怖电影的评论（然而，有人可能会认为
    `Horror movie #2` 也可能适合分类为主题 1: *一般来说，糟糕的电影*）。'
- en: Summary
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned how to use machine learning algorithms to classify
    text documents based on their polarity, which is a basic task in sentiment analysis
    in the field of NLP. Not only did you learn how to encode a document as a feature
    vector using the bag-of-words model, but you also learned how to weight the term
    frequency by relevance using tf-idf.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何使用机器学习算法根据文本的极性对文档进行分类，这在自然语言处理领域的情感分析中是一个基础任务。你不仅学会了如何使用词袋模型将文档编码为特征向量，还学习了如何使用
    tf-idf 根据相关性对词频进行加权。
- en: Working with text data can be computationally quite expensive due to the large
    feature vectors that are created during this process; in the last section, we
    covered how to utilize out-of-core or incremental learning to train a machine
    learning algorithm without loading the whole dataset into a computer's memory.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 处理文本数据的计算开销可能非常大，因为在这个过程中会创建大量的特征向量；在上一节中，我们介绍了如何利用外存或增量学习来训练机器学习算法，而不需要将整个数据集加载到计算机的内存中。
- en: Lastly, you were introduced to the concept of topic modeling using LDA to categorize
    the movie reviews into different categories in an unsupervised fashion.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你了解了使用 LDA 进行主题建模的概念，通过无监督的方式将电影评论分类到不同的类别中。
- en: In the next chapter, we will use our document classifier and learn how to embed
    it into a web application.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用我们的文档分类器，并学习如何将其嵌入到 Web 应用程序中。
