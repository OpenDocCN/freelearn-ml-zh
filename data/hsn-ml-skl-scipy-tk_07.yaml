- en: Classifying Text Using Naive Bayes
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯分类器进行文本分类
- en: '"Language is a process of free creation; its laws and principles are fixed,
    but the manner in which the principles of generation are used is free and infinitely
    varied. Even the interpretation and use of words involves a process of free creation."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “语言是一个自由创造的过程；它的规律和原则是固定的，但这些生成原则的运用方式是自由且无限变化的。甚至单词的解释和使用也涉及自由创造的过程。”
- en: – Noam Chomsky
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: – 诺姆·乔姆斯基
- en: 'Not all information exists in tables. From Wikipedia to social media, there
    are billions of written words that we would like our computers to process and
    extract bits of information from. The sub-field of machine learning that deals
    with textual data goes by names such as **Text Mining** and **Natural Language
    Processing** (**NLP**). These different names reflect the fact that the field
    inherits from multiple disciplines. On the one hand, we have computer science
    and statistics, and on the other hand, we have linguistics. I''d argue that the
    influence of linguistics was stronger when the field was at its infancy, but in
    later stages, practitioners came to favor mathematical and statistical tools,
    as they require less human intervention and can get away without humans manually
    codifying linguistic rules into the algorithms:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有信息都存在于表格中。从维基百科到社交媒体，成千上万的文字信息需要我们的计算机进行处理和提取。处理文本数据的机器学习子领域有着如**文本挖掘**和**自然语言处理**（**NLP**）等不同的名称。这些名称反映了该领域从多个学科继承而来。一方面，我们有计算机科学和统计学，另一方面，我们有语言学。我认为，在该领域初期，语言学的影响较大，但随着发展，实践者们更倾向于使用数学和统计工具，因为它们需要较少的人工干预，并且不需要人工将语言规则编入算法中：
- en: '"Every time I fire a linguist, the performance of our speech recognition system
    goes up."'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: “每次我解雇一个语言学家，我们的语音识别系统性能都会提升。”
- en: – Fred Jelinek
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: – 弗雷德·杰里内克
- en: Having said that, it is essential to have a basic understanding of how things
    have progressed over time and not jump to the bleeding-edge solutions right away.
    This enables us to pick our tools wisely while being aware of the tradeoffs we
    are making. Thus, we will start this chapter by processing textual data and presenting
    it to our algorithms in formats they understand. This preprocessing stage has
    an important effect on the performance of the downstream algorithms. Therefore,
    I will make sure to shed light on the pros and cons of each method explained here.
    Once the data is ready, we will use a **Naive Bayes** classifier to detect the
    sentiment of different Twitter users based on the messages they send to multiple
    airway services.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，了解事物随着时间的进展是如何发展的，避免直接跳入前沿解决方案，这一点至关重要。这使我们能够在意识到权衡取舍的基础上明智地选择工具。因此，我们将从处理文本数据开始，并以算法能够理解的格式呈现数据。这个预处理阶段对下游算法的性能有着重要影响。因此，我会确保阐明每种方法的优缺点。一旦数据准备好，我们将使用**朴素贝叶斯**分类器根据用户发送给多个航空公司服务的消息，检测不同Twitter用户的情感。
- en: 'In this chapter, the following topics will be covered:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下主题：
- en: Splitting sentences into tokens
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将句子拆分成词元
- en: Token normalization
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词元归一化
- en: Using bag of words to represent tokens
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用词袋模型表示词元
- en: Using n-grams to represent tokens
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用n-gram模型表示词元
- en: Using Word2Vec to represent tokens
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Word2Vec表示词元
- en: Text classification with a Naive Bayes classifier
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯分类器进行文本分类
- en: Splitting sentences into tokens
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将句子拆分成词元
- en: '"A word after a word after a word is power."'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: “一个字接一个字，形成了力量。”
- en: – Margaret Atwood
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: – 玛格丽特·阿特伍德
- en: So far, the data we have dealt with has either been table data with columns
    as features or image data with pixels as features. In the case of text, things
    are less obvious. Shall we use sentences, words, or characters as our features?
    Sentences are very specific. For example, it is very unlikely to have the exact
    same sentence appearing in two or more Wikipedia articles. Therefore, if we use
    sentences as features, we will end up with tons of features that do not generalize
    well.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们处理的数据要么是带有列作为特征的表格数据，要么是带有像素作为特征的图像数据。而在文本的情况下，问题变得不那么明确。我们应该使用句子、单词，还是字符作为特征？句子非常具体。例如，两篇维基百科文章中出现完全相同的句子的可能性非常小。因此，如果我们将句子作为特征，最终会得到大量的特征，这些特征的泛化能力较差。
- en: Characters, on the other hand, are limited. For example, there are only 26 letters
    in the English language. This small variety is likely to limit the ability of
    the separate characters to carry enough information for the downstream algorithms
    to extract. As a result, words are typically used as features for most tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，字符是有限的。例如，英语中只有 26 个字母。这种小的变化可能限制了单个字符携带足够信息的能力，无法让下游算法提取出有效的特征。因此，单词通常作为大多数任务的特征。
- en: Later in this chapter, we will see that fairly specific tokens are still possible,
    but let's stick to words as features for now. Finally, we do not want to limit
    ourselves to dictionary words; Twitter hashtags, numbers, and URLs can also be
    extracted from text and treated as features. That's why we prefer to use the term
    *token* instead *word*, since it is more generic. The process where a stream of
    text is split into tokens is called tokenization, and we are going to learn about
    that in the next section.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章稍后我们会看到，尽管可以得到相当具体的标记，但现在让我们暂时仅把单词作为特征。最后，我们并不想局限于字典中的单词；Twitter 标签、数字和 URL
    也可以从文本中提取并作为特征。因此，我们更倾向于使用 *token* 而不是 *word* 这个术语，因为 *token* 更为通用。将文本流分割成标记的过程称为分词，我们将在下一节中学习这个过程。
- en: Tokenizing with string split
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用字符串分割进行分词
- en: Different tokenization methods lead to different results. To demonstrate these
    differences, let's take the following three lines of text and see how can we tokenize
    them.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的分词方法会导致不同的结果。为了演示这些差异，让我们以以下三行文本为例，看看如何对它们进行分词。
- en: 'Here I write the lines of text as strings and put them into a list:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我将文本行作为字符串写入并放入一个列表中：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'One obvious way to do this is to use Python''s built-in `split()` method as
    follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一种明显的方法是使用 Python 内置的 `split()` 方法，如下所示：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When no parameters are given, `split()` uses white spaces to split strings
    based on. Thus, we get the following output:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当没有提供参数时，`split()` 会根据空格来分割字符串。因此，我们得到以下输出：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You may notice that the punctuation was kept as part of the tokens. The question
    mark was left at the end of `tokenize`, and the period remained attached to `boss`.
    The hashtag is made of two words, but since there are no spaces between them,
    it was kept as a single token along with its leading hash sign.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，标点符号被保留为标记的一部分。问号被保留在 `tokenize` 的末尾，句号也附着在 `boss` 后面。井号标签由两个单词组成，但由于它们之间没有空格，它被作为一个整体标记保留，并带有前导的井号符号。
- en: Tokenizing using regular expressions
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用正则表达式进行分词
- en: 'We may also use regular expressions to treat sequences of letters and numbers
    as tokens, and split our sentences accordingly. The pattern used here, `"\w+"`,
    refers to any sequence of one or more alphanumeric characters or underscores.
    Compiling our patterns gives us a regular expression object that we can use for
    matching. Finally, we loop over each line and use the regular expression object
    to split it into tokens:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用正则表达式将字母和数字序列视为标记，并相应地分割我们的句子。这里使用的模式 `"\w+"` 表示任何一个或多个字母数字字符或下划线的序列。编译我们的模式会得到一个正则表达式对象，我们可以用它来进行匹配。最后，我们遍历每一行并使用正则表达式对象将其拆分为标记：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This gives us the following output:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, the punctuation has been removed, but the URL has been split into four
    tokens.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，标点符号已被去除，但 URL 被分割成了四个标记。
- en: Scikit-learn uses regular expressions for tokenization by default. However,
    the following pattern, `r"(?u)\b\w\w+\b"`, is used instead of `r"\w+"`. This pattern
    ignores all punctuation and words shorter than two letters. So, the "a" token
    would be omitted. You can still overwrite the default pattern by providing your
    custom one.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 默认使用正则表达式进行分词。然而，`r"(?u)\b\w\w+\b"` 这个模式被用来代替 `r"\w+"`。这个模式会忽略所有标点符号和短于两个字母的单词。因此，"a"
    这个词会被省略。你仍然可以通过提供自定义模式来覆盖默认模式。
- en: Using placeholders before tokenizing
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用占位符进行分词前的处理
- en: 'To deal with the previous problem, we may decide to replace the numbers, URLs,
    and hashtags with placeholders before tokenizing our sentences. This is useful
    if we don''t really care to differentiate between their content. A URL may be
    just a URL to me, regardless of where it leads to. The following function converts
    its input into lower case, then replaces any URL it finds with a `_url_` placeholder.
    Similarly, it converts the hashtags and numbers into their corresponding placeholders.
    Finally, the input is split based on white spaces, and the resulting tokens are
    returned:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决前面的问题，我们可以决定在对句子进行分词之前，将数字、URL和标签（hashtags）替换为占位符。如果我们不在意区分它们的内容，这样做是有用的。对我来说，URL可能只是一个URL，无论它指向哪里。以下函数将输入转换为小写字母，然后将找到的任何URL替换为`_url_`占位符。类似地，它将标签和数字转换为相应的占位符。最后，输入根据空白字符进行分割，并返回结果的词元：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This gives us the following output:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了以下输出：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, the new placeholder tells us that a URL existed in the second
    sentence, but it doesn't really care where the URL links to. If we have another
    sentence with a different URL, it will just get the same placeholder as well.
    The same goes for the numbers and hashtags.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，新的占位符告诉我们第二个句子中存在一个URL，但它并不关心该URL指向哪里。如果我们有另一个包含不同URL的句子，它也会得到相同的占位符。数字和标签也是一样的。
- en: Depending on your use case, this may not be ideal if your hashtags carry information
    that you would not like to lose. Again, this is a tradeoff you have to make based
    on your use case. Usually, you can intuitively tell which technique is more suitable
    for the problem at hand, but sometimes evaluating a model after multiple tokenization
    techniques can be the only way to tell which one is more suitable. Finally, in
    practice, you may use libraries such as **NLTK** and **spaCy** to tokenize your
    text. They already have the necessary regular expressions under the hood. We will
    be using spaCy later on in this chapter.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的使用情况，如果你的标签包含你不想丢失的信息，这种方法可能并不理想。同样，这是一个你必须根据具体应用做出的权衡。通常，你可以直观地判断哪种技术更适合当前问题，但有时，评估经过多次分词技术后的模型，可能是唯一判断哪种方法更合适的方式。最后，在实际应用中，你可能会使用**NLTK**和**spaCy**等库来对文本进行分词。它们已经在后台实现了必要的正则表达式。在本章稍后的部分，我们将使用spaCy。
- en: Note how I converted the sentence into lower case before processing it. This
    is called normalization. Without normalization, a capitalized word and a lowercase
    version of it will be seen as two different tokens. This is not ideal, since *Boy*
    and *boy* are conceptually the same, hence normalization is usually required.
    Scikit-learn converts input text to lower case by default.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我在处理句子之前将其转换为小写字母。这被称为归一化。如果没有归一化，首字母大写的单词和它的小写版本会被视为两个不同的词元。这不是理想的，因为*Boy*和*boy*在概念上是相同的，因此通常需要进行归一化。Scikit-learn默认会将输入文本转换为小写字母。
- en: Vectorizing text into matrices
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将文本向量化为矩阵
- en: In text mining, a dataset is usually called a **corpus**. Each data sample in
    it is usually called a **document**. Documents are made of **tokens**, and a set
    of distinct tokens is called a **vocabulary**. Putting this information into a
    matrix is called **vectorization**. In the following sections, we are going to
    see the different kinds of vectorizations that we can get.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本挖掘中，一个数据集通常被称为**语料库**。其中的每个数据样本通常被称为**文档**。文档由**词元**组成，一组不同的词元被称为**词汇表**。将这些信息放入矩阵中称为**向量化**。在接下来的章节中，我们将看到我们可以获得的不同类型的向量化方法。
- en: Vector space model
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量空间模型
- en: We still miss our beloved feature matrices, where we expect each token to have
    its own column and each document to be represented by a separate row. This kind
    of representation for textual data is known as the **vecto****r****space mo****del**.
    From a linear-algebraic point of view, the documents in this representation are
    seen as vectors (rows), and the different terms are the dimensions of this space
    (columns), hence the name vector space model. In the next section, we will learn
    how to vectorize our documents.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然缺少我们心爱的特征矩阵，在这些矩阵中，我们期望每个词元（token）有自己的列，每个文档由单独的一行表示。这种文本数据的表示方式被称为**向量空间模型**。从线性代数的角度来看，这种表示中的文档被视为向量（行），而不同的词项是该空间的维度（列），因此称为向量空间模型。在下一节中，我们将学习如何将文档向量化。
- en: Bag of words
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词袋模型
- en: 'We need to convert the documents into tokens and put them into the vector space
    model. `CountVectorizer` can be used here to tokenize the documents and put them
    into the desired matrix. Here, we are going to use it with the help of the tokenizer
    we created in the previous section. As usual, we import and initialize `CountVectorizer`,
    and then we use its `fit_transform` method to convert our documents. We also specified
    that we want to use the tokenizer we built in the previous section:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将文档转换为标记，并将它们放入向量空间模型中。此处可以使用`CountVectorizer`对文档进行标记化并将其放入所需的矩阵中。在这里，我们将使用我们在上一节创建的分词器。像往常一样，我们导入并初始化`CountVectorizer`，然后使用其`fit_transform`方法来转换我们的文档。我们还指定希望使用我们在上一节构建的分词器：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Most of the cells in the returned matrix are zeros. To save space, it is saved
    as a sparse matrix; however, we can turn it into a dense matrix using its `todense()`
    method. The vectorizer holds the set of encountered vocabulary, which can be retrieved
    using `get_feature_names()`. Using this information, we can convert `x` into a
    DataFrame as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的矩阵中大部分单元格都是零。为了节省空间，它被保存为稀疏矩阵；然而，我们可以使用其`todense()`方法将其转换为稠密矩阵。向量化器保存了遇到的词汇表，可以使用`get_feature_names()`来检索。通过这些信息，我们可以将`x`转换为DataFrame，如下所示：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This gives us the following matrix:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们以下矩阵：
- en: '![](img/d6c4ca9d-16c6-4fef-9180-13492778d67f.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6c4ca9d-16c6-4fef-9180-13492778d67f.png)'
- en: Each cell contains the number of times each token appears in each document.
    However, the vocabulary does not follow any order; therefore, it is not possible
    to tell the order of the tokens in each document from this matrix.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单元格包含每个标记在每个文档中出现的次数。然而，词汇表没有遵循任何顺序；因此，从这个矩阵中无法判断每个文档中标记的顺序。
- en: Different sentences, same representation
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不同的句子，相同的表示
- en: 'Take these two sentences with opposite meanings:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 取这两句话，它们有相反的意思：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If we use the count vectorizer to represent them, we will end up with the following
    matrix:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用计数向量化器来表示它们，我们将得到以下矩阵：
- en: '![](img/2f1e5f0c-b2aa-4795-b849-0fe56cb9ca8a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f1e5f0c-b2aa-4795-b849-0fe56cb9ca8a.png)'
- en: As you can see, the order of the tokens in the sentences is lost. That is why
    this method is known as **bag of words** – the result is like a bag that words
    are just put into without any order. Obviously, this makes it impossible to tell
    which of the two people is happy and which is not. To fix this problem, we may
    need to use **n-grams**, as we will do in the following section.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，句子中标记的顺序丢失了。这就是为什么这种方法被称为**词袋模型（bag of words）**——结果就像一个袋子，单词只是被放入其中，没有任何顺序。显然，这使得无法分辨哪一个人是开心的，哪一个不是。为了解决这个问题，我们可能需要使用**n-grams**，正如我们将在下一节中所做的那样。
- en: N-grams
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: N-grams
- en: 'Rather than treating each term as a token, we can treat the combinations of
    each two consecutive terms as a single token. All we have to do is to set `ngram_range`
    in `CountVectorizer` to `(2,2)`, as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与其将每个术语视为一个标记，我们可以将每两个连续术语的组合视为一个单独的标记。我们要做的就是将`CountVectorizer`中的`ngram_range`设置为`(2,2)`，如下所示：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Using similar code to that used in the previous section, we can put the resulting
    `x` into a DataFrame and get the following matrix:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与上一节相似的代码，我们可以将结果的`x`放入DataFrame中并得到以下矩阵：
- en: '![](img/2c2c898e-bdec-4f61-ab94-ef3bbe0bdc6b.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c2c898e-bdec-4f61-ab94-ef3bbe0bdc6b.png)'
- en: Now we can tell who is happy and who is not. When using word pairs, this is
    known as **bigrams**. We can also do 3-grams (with three consecutive words), 4-grams,
    or any other number of grams. Setting `ngram_range` to (1,1) takes us back to
    the original representation where each separate word is a token, which is **unigrams**.
    We can also mix unigrams with bigrams by setting `ngram_range`**to (1,2). In brief,
    this range tells the tokenizer the minimum and maximum values for*n* to use in
    our n-grams.**
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以知道谁是开心的，谁不是。当使用词对时，这被称为**大ram（bigrams）**。我们还可以使用3-gram（由三个连续单词组成），4-gram或任何其他数量的gram。将`ngram_range`设置为(1,1)将使我们回到原始表示形式，其中每个单独的单词是一个标记，这就是**单gram（unigrams）**。我们还可以通过将`ngram_range`设置为(1,2)来混合单gram和大gram。简而言之，这个范围告诉分词器用于我们n-gram的最小值和最大值*n*。
- en: '**If you set *n* to a high value – say, 8 – this means that sequences of eight
    words are treated as tokens. Now, how likely do you think it is that a sequence
    of eight words will appear more than once in your dataset? Most likely, you will
    see it once in your training set and never again into the test set. That''s why
    *n* is usually set to something between 2 and 3, with some unigrams also being
    used to capture rare words.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你将*n*设置为一个较高的值——比如 8——这意味着八个单词的序列将被当作标记。那么，你认为一个包含八个单词的序列在你的数据集中出现的概率有多大？大概率是它只会在训练集中出现一次，而在测试集中从未出现过。这就是为什么*n*通常设置为
    2 到 3 之间的数值，并且有时会使用一些 unigram 来捕捉稀有词汇。**'
- en: Using characters instead of words
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用字符代替单词
- en: Up until now, words have been the atoms of our textual universe. However, some
    situations may require us to tokenize our documents based on characters instead.
    In situations where word boundaries are not clear, such as in hashtags and URLs,
    the use of characters as tokens may help. Natural languages tend to have different
    frequencies for their characters. The letter **e** is the most commonly used character
    in the English language, and character combinations such as **th**, **er**, and
    **on** are also very common. Other languages, such as French and Dutch, have different
    character frequencies. If our aim is to classify documents based on their languages,
    the use of characters instead of words can come in handy.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，单词一直是我们文本宇宙中的原子。然而，有些情况可能需要我们基于字符来进行文档的标记化。在单词边界不明确的情况下，比如标签和 URL，使用字符作为标记可能会有所帮助。自然语言的字符频率通常不同。字母**e**是英语中使用最频繁的字符，字符组合如**th**、**er**和**on**也非常常见。其他语言，如法语和荷兰语，也有不同的字符频率。如果我们的目标是基于语言来分类文档，使用字符而不是单词可能会派上用场。
- en: 'The very same `CountVectorizer` can help us tokenize our documents into characters.
    We can also combine this with the `n-grams` setting to get subsequences within
    words, as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的`CountVectorizer`可以帮助我们将文档标记化为字符。我们还可以将其与`n-grams`设置结合，以获取单词中的子序列，如下所示：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can put the resulting `x` into a DataFrame, as we did earlier, to get the
    following matrix:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像之前一样将结果`x`放入 DataFrame 中，从而得到如下矩阵：
- en: '![](img/ea3c937d-0287-467f-8625-0844a79f8a9a.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea3c937d-0287-467f-8625-0844a79f8a9a.png)'
- en: All our tokens are made of four characters now. Whitespaces are also treated
    as characters, as you can see. With characters, it is more common to go for higher
    values of *n*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们所有的标记都由四个字符组成。如你所见，空格也被视作字符。使用字符时，通常会选择更高的*n*值。
- en: Capturing important words with TF-IDF
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 TF-IDF 捕捉重要词汇
- en: Another discipline that we borrow lots of ideas from here is the **information
    retrieval** field. It's the field responsible for the algorithms that run search
    engines such as Google, Bing, and DuckDuckGo.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里借鉴的另一个学科是**信息检索**领域。它是负责运行搜索引擎算法的领域，比如 Google、Bing 和 DuckDuckGo。
- en: 'Now, take the following quotation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，看看下面这段引文：
- en: '"From a linguistic point of view, you can''t really take much objection to
    the notion that a show is a show is a show."'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: “从语言学的角度来看，你真的不能对‘一个节目就是一个节目’这一概念提出太多反对意见。”
- en: – Walter Becker
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: – 沃尔特·贝克尔
- en: The word **linguistic** and the word **that** both appeared exactly once in
    the previous quotation. Nevertheless, we would only worry about the word **linguistic**,
    not the word **that**, if we were searching for this quotation on the internet.
    We know that it is more significant, although it appeared only once, just as many
    times as **that**. The word **show** appeared three times. From a count vectorizer's
    point of view, it should carry three times more information than the word **linguistic**.
    I assume you also disagree with the vectorizer about that. Those issues are fundamentally
    the raison d'être of **Term Frequency**-**Inverse Document Frequency***(**TF-IDF**).
    The IDF part not only involves weighting the value of the words based on how frequently
    they appear in a certain document, but also discounting weights from them if they
    happen to be very common in other documents. The word **that** is so common across
    other documents that it shouldn't be given as much value as **linguistic**. Furthermore,
    IDF uses a logarithmic scale to better represent the information a word carries
    based on its frequency in a document.*
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**linguistic**和**that**这两个词在前述引用中都出现过一次。然而，如果我们在互联网上搜索这段引文，我们只会关注**linguistic**这个词，而不是**that**这个词。我们知道，尽管它只出现了一次，和**that**出现的次数一样多，但它更为重要。**show**这个词出现了三次。从计数向量化器的角度来看，它应该比**linguistic**包含更多的信息。我猜你也不同意向量化器的看法。这些问题从根本上来说是**词频-逆文档频率**（**TF-IDF**）的存在原因。IDF部分不仅涉及根据单词在某个文档中出现的频率来加权单词的值，还会在这些单词在其他文档中非常常见时对它们的权重进行折扣。**that**这个词在其他文档中如此常见，以至于它不应该像**linguistic**一样被赋予那么高的权重。此外，IDF使用对数尺度来更好地表示一个词根据它在文档中的频率所携带的信息。*'
- en: '*Let''s use the following three documents to demonstrate how TF-IDF works:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们使用以下三个文档来演示TF-IDF是如何工作的：'
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`TfidfVectorizer` has an almost identical interface to that of`CountVectorizer`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`TfidfVectorizer`的接口与`CountVectorizer`几乎完全相同：'
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is a comparison for the outputs of the two vectorizers side by side:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这是两种向量化器输出的并排比较：
- en: '![](img/224edc67-6a5f-4823-bfe1-b259b922069c.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/224edc67-6a5f-4823-bfe1-b259b922069c.png)'
- en: As you can see, unlike in `CountVectorizer`, not all words were treated equally
    by `TfidfVectorizer`. More emphasis was given to the fruit names compared to the
    other, less informative words that happened to appear in all three sentences.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，与`CountVectorizer`不同，`TfidfVectorizer`并没有对所有单词进行平等对待。相比于其他出现在所有三句话中的不太有信息量的词，更多的强调被放在了水果名称上。
- en: Both `CountVectorizer` and**`TfidfVectorizer`**have a parameter called `stop_words`.
    It can be used to specify tokens to be ignored. You can provide your own list
    of less informative words, such as **a**, **an**, and **the**. You can also provide
    the `english`*keyword to specify the common stop words in the English language.
    Having said that, it is important to note that some words can be informative for
    one task but not for another. Furthermore, IDF usually does what you need it to
    do automatically and gives low weights to non-informative words. That is why I
    usually prefer not to manually remove stop words, instead trying things such as
    `TfidfVectorizer`, feature selection, and regularization**first.*******
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer`和**`TfidfVectorizer`**都有一个名为`stop_words`的参数。它可以用来指定需要忽略的词元。你可以提供自己的不太有信息量的词列表，例如**a**、**an**和**the**。你也可以提供`english`*关键字来指定英语中常见的停用词。话虽如此，需要注意的是，一些词对于某个任务来说可能有信息量，但对另一个任务则可能没有。此外，IDF通常会自动完成你需要它做的工作，并且给非信息性词语赋予较低的权重。这就是为什么我通常不手动去除停用词，而是尝试使用`TfidfVectorizer`、特征选择和正则化**优先**的方法。*******'
- en: '*******Besides its original use case,`TfidfVectorizer` is commonly used as
    a preprocessing step for text classification. Nevertheless, it usually gives good
    results when longer documents are to be classified. For short documents, it may
    produce noisy transformation, and it is advised to give`CountVectorizer` a try
    in such cases.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*******除了它的原始用途，`TfidfVectorizer`通常作为文本分类的预处理步骤。然而，当需要对较长的文档进行分类时，它通常能给出不错的结果。对于较短的文档，它可能会产生嘈杂的转化，建议在这种情况下尝试使用`CountVectorizer`。'
- en: In a basic search engine, when someone types a query, it gets converted into
    the same vector space where all the documents to be searched exist, using TF-IDF.
    Once the search query and the documents exist as vectors in the same space, a
    simple distance measure such as cosine distance can be used to find the closest
    documents to the query. Modern search engines vary from this basic idea, but it
    is a good base to build your understanding of information retrieval on.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个基础的搜索引擎中，当有人输入查询时，它会通过 TF-IDF 转换为与所有待搜索文档存在于同一向量空间中的形式。一旦查询和文档作为向量存在于同一空间中，就可以使用简单的距离度量方法，如余弦距离，来查找与查询最接近的文档。现代搜索引擎在这个基础概念上有所变化，但这是构建信息检索理解的良好基础。
- en: Representing meanings with word embedding
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用词嵌入表示意义
- en: 'As documents are collections of tokens, their vector representations are basically
    the sum of the vectors of the tokens they contain. As we have seen earlier, the
    **I like apples** document was represented by `CountVectorizer` using the vector
    [1,1,1,0,0]:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文档是由词元组成的，它们的向量表示基本上是包含的词元向量之和。正如我们之前看到的，**I like apples**文档通过`CountVectorizer`被表示为向量[1,1,1,0,0]：
- en: '![](img/7838de63-46e3-4cae-84cc-810e05362a52.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7838de63-46e3-4cae-84cc-810e05362a52.png)'
- en: From this representation, we can also deduce that the terms **I**, **like**,
    **apples**, and **oranges** are represented by the following four five-dimensional
    vectors, [0,1,0,0,0], [0,0,1,0,0], [1,0,0,0,0], and [0,0,0,1,0]. We have a five-dimensional
    space, given our vocabulary of five terms. Each term has a magnitude of 1 in one
    dimension and 0 in the other four dimensions. From a linear algebraic point of
    view, all five terms are orthogonal (perpendicular) to each other. Nevertheless,
    **apples**, **pears**, and **oranges** are all fruits, and conceptually they have
    some similarity that was not captured by this model. Therefore, we would ideally
    like to represent them with vectors that are closer to each other, unlike these
    orthogonal vectors. The same issue here applied to `TfidfVectorizer`, by the way***.***
    This was the driver for researchers to come up with better representations, and
    word embedding is the coolest kid on the natural language processing block nowadays,
    as it tries to capture meaning better than traditional vectorizers. In the next
    section, we will get to know one popular embedding technique, Word2Vec.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从这种表示方式出发，我们还可以推断出**I**、**like**、**apples**和**oranges**分别由以下四个五维向量表示：[0,1,0,0,0]，[0,0,1,0,0]，[1,0,0,0,0]和[0,0,0,1,0]。我们有一个五维空间，基于我们五个词的词汇表。每个词在一个维度上的值为1，其他四个维度上的值为0。从线性代数的角度来看，所有五个词是正交的（垂直的）。然而，**apples**、**pears**和**oranges**都是水果，在概念上它们有一定的相似性，但这种相似性并没有被这个模型捕捉到。因此，我们理想的做法是使用相互接近的向量来表示它们，而不是这些正交的向量。顺便提一下，`TfidfVectorizer`也存在类似问题***。***
    这促使研究人员提出了更好的表示方法，而词嵌入如今成为自然语言处理领域的热门技术，因为它比传统的向量化方法更好地捕捉了意义。在下一节中，我们将了解一种流行的嵌入技术——Word2Vec。
- en: Word2Vec
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Word2Vec
- en: Without getting into the details too much, Word2Vec uses neural networks to
    predict words from their context, that is, from their surrounding words. By doing
    so, it learns better representations for the different words, and these representations
    incorporate the meanings of the words they represent. Unlike the previously mentioned
    vectorizers, the dimensionality of the word representation is not directly linked
    to the size of our vocabulary. We get to choose the length of our embedding vectors.
    Once each word is represented by a vector, the document's representation is usually
    the summation of all the vectors of its words. Averaging is also an option instead
    of summation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 不深入细节，Word2Vec 使用神经网络从上下文中预测单词，也就是说，从单词的周围词汇中进行预测。通过这种方式，它学习了更好的单词表示，并且这些表示包含了它们所代表的单词的意义。与前面提到的向量化方法不同，单词表示的维度与我们词汇表的大小没有直接关系。我们可以选择嵌入向量的长度。一旦每个单词被表示为一个向量，文档的表示通常是所有单词向量的和。平均值也是一个替代选择，而不是求和。
- en: Since the size of our vectors is independent of the size of the vocabulary of
    the documents we are dealing with, researchers can reuse a pre-trained Word2Vec
    model that wasn't made specifically for their particular problem. This ability
    to re-use pre-trained models is known as transfer learning. Some researchers can
    train an embedding on a huge amount of documents using expensive machines and
    release the resulting vectors for the entire world to use. Then, the next time
    we deal with a specific natural language processing task, all we need to do is
    to get these vectors and use them to represent our new documents. spaCy ([https://spacy.io/](https://spacy.io/))
    is an open source software library that comes with word vectors for different
    languages.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们向量的大小与我们处理的文档的词汇量无关，研究人员可以重新使用未专门为他们特定问题训练的预训练Word2Vec模型。这种重新使用预训练模型的能力被称为迁移学习。一些研究人员可以使用昂贵的机器在大量文档上训练嵌入，并发布得到的向量供全世界使用。然后，下次我们处理特定的自然语言处理任务时，我们所需要做的就是获取这些向量并用它们来表示我们新的文档。spaCy
    ([https://spacy.io/](https://spacy.io/))是一个开源软件库，提供了不同语言的词向量。
- en: 'In the following few lines of code, we will install spaCy, download its language
    model data, and use it to convert words into vectors:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几行代码中，我们将安装spaCy，下载它的语言模型数据，并使用它将单词转换为向量：
- en: 'To use spaCy, we can install the library and download its pre-trained models
    for the English language by running the following commands in our terminal:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用spaCy，我们可以安装这个库并通过运行以下命令在终端中下载其英语预训练模型：
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, we can assign the downloaded vectors to our five words as follows:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以将下载的向量分配给我们的五个单词，如下所示：
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here is the representation for **apples**:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是**苹果**的表示：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: I promised you that the representations for **apples**, **oranges**, and **pears**
    would not be orthogonal as in the case with `CountVectorizer`. However, with 300
    dimensions, it is hard for me to visually prove that. Luckily, we have already
    learned how to calculate the cosine of the angle between two vectors. Orthogonal
    vectors should have 90^o angles between them, whose cosines are equal to 0\. The
    cosine for the zero angle between two vectors going in the exact same direction
    is 1.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾承诺你，**苹果**、**橙子**和**梨**的表示不会像`CountVectorizer`那样正交。然而，使用300个维度时，我很难直观地证明这一点。幸运的是，我们已经学会了如何计算两个向量之间的余弦角度。正交向量之间的角度应该是90°，其余弦值为0。而两个方向完全相同的向量之间的零角度的余弦值为1。
- en: 'Here, we calculate the cosine between all the five vectors we got from spaCy.
    I used some pandas and seaborn styling to make the numbers clearer:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们计算了来自spaCy的五个向量之间的余弦相似度。我使用了一些pandas和seaborn的样式，使数字更清晰：
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, I showed the results in the following DataFrame:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我在下面的DataFrame中展示了结果：
- en: '![](img/54477c03-db2a-4444-8191-78cc4fc479e5.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54477c03-db2a-4444-8191-78cc4fc479e5.png)'
- en: Clearly, the new representation understands that fruit names are more similar
    to each other than they are to words such as **I** and **like**. It also considered
    **apples** and **pears** to be very similar to each other, as opposed to **oranges**.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，新的表示方法理解到水果名称之间的相似度远高于它们与像**I**和**like**这样的词的相似度。它还认为**苹果**和**梨**非常相似，而**橙子**则不然。
- en: You may have noticed that Word2Vec suffers from the same problem as unigrams;
    words are encoded without much attention being paid to their context. The representation
    for the word "book" in "I will read a book" is the same as its representation
    in "I will book a flight." That's why newer techniques, such as **Embeddings from
    Language Models** (**ELMo**), **Bidirectional Encoder Representations from Transformers**
    (**BERT**) and OpenAI's recent **GPT-3** are gaining more popularity nowadays
    as they respect the words' context. I expect them to be included in more libraries
    soon for anyone to easily use them.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，Word2Vec存在与一元词相同的问题；词语的编码并没有太多关注它们的上下文。在句子“I will read a book”和“I will
    book a flight”中，单词“book”的表示是一样的。这就是为什么像**语言模型嵌入**（**ELMo**）、**双向编码器表示从变换器**（**BERT**）以及OpenAI最近的**GPT-3**等新技术现在越来越受欢迎的原因，因为它们尊重词语的上下文。我预计它们很快会被更多的库所采用，供大家轻松使用。
- en: The embedding concept is recycled and reused by machine learning practitioners
    everywhere nowadays. Apart from its use in natural language processing, it is
    used for feature reduction and in recommendation systems. For instance, every
    time a customer adds an item to their online shopping cart, if we treat the cart
    as a sentence and the items as words, we end up with item embeddings (**Item2Vec**).
    These new representations for the items can easily be plugged into a downstream
    classifier or a recommender system.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入概念现在被各地的机器学习从业者回收并重新利用。除了在自然语言处理中的应用外，它还用于特征降维和推荐系统。例如，每当顾客将商品添加到在线购物车时，如果我们将购物车视为一个句子，将商品视为单词，那么我们就得到了商品的嵌入
    (**Item2Vec**) 。这些商品的新表示可以轻松地插入到下游分类器或推荐系统中。
- en: Before moving to text classification, we need to stop and spend some time first
    to learn about the classifier we are going to use – the **Naive Bayes classifier**.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入文本分类之前，我们需要先停下来花一些时间了解我们将要使用的分类器——**朴素贝叶斯分类器**。
- en: Understanding Naive Bayes
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解朴素贝叶斯
- en: The Naive Bayes classifier is commonly used in classifying textual data. In
    the following sections, we are going to see its different flavors and learn how
    to configure their parameters. But first, to understand the Naive Bayes classifier,
    we need to first go through Thomas Bayes' theorem, which he published in the 18^(th)
    century.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器通常用于文本数据的分类。在接下来的部分中，我们将看到其不同的变种，并学习如何配置它们的参数。但首先，为了理解朴素贝叶斯分类器，我们需要先了解托马斯·贝叶斯在18世纪发表的贝叶斯定理。
- en: The Bayes rule
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯规则
- en: 'When talking about classifiers, we can describe the probability of a certain
    sample belonging to a certain class using conditional probability, *P(y|x)*. This
    is the probability of a sample belonging to class *y* given its features, *x*.
    The pipe sign (|) is what we use to refer to conditional probability, that is,
    *y* given *x*. The Bayes rule is capable of expressing this conditional probability
    in terms of *P(x|y)*, *P(x)*, and *P(y)*, using the following formula:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论分类器时，我们可以使用条件概率*P(y|x)*来描述某个样本属于某个类别的概率。这是给定其特征*x*的情况下，样本属于类别*y*的概率。管道符号（|）是我们用来表示条件概率的符号，即给定*x*的情况下的*y*。贝叶斯规则可以用以下公式将这种条件概率表达为*P(x|y)*、*P(x)*和*P(y)*：
- en: '![](img/5af08cd0-2cfe-4726-95b2-787a4098441d.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5af08cd0-2cfe-4726-95b2-787a4098441d.png)'
- en: 'Usually, we ignore the denominator part of the equation and convert it into
    a proportion as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们忽略方程中的分母部分，将其转换为如下比例：
- en: '![](img/82a8bcff-9301-4949-8174-c274048cad51.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82a8bcff-9301-4949-8174-c274048cad51.png)'
- en: The probability of a class, *P(y)*, is known as the prior probability. It's
    basically the number of samples that belong to a certain class out of all training
    samples. The conditional probability, *P(x|y)*, is known as the likelihood. It's
    what we calculate from the training samples. Once the two probabilities are known
    at training time, we can use them to predict the chance of a new sample belonging
    to a certain class at prediction time, *P(y|x)*, also known as the posterior probability.
    Calculating the likelihood part of the equation is not as simple as we expect.
    So, in the next section, we are going to discuss the assumption we can make to
    ease this calculation.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一个类别的概率，*P(y)*，称为先验概率。它基本上是所有训练样本中属于某一类别的样本数。条件概率，*P(x|y)*，称为似然度。它是我们从训练样本中计算出来的。一旦这两个概率在训练时已知，我们就可以利用它们来预测新样本属于某一类别的概率，即预测时的*P(y|x)*，也称为后验概率。计算方程中的似然度部分并不像我们预期的那么简单。因此，在接下来的部分中，我们将讨论为了简化这一计算，我们可以做出哪些假设。
- en: Calculating the likelihood naively
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素地计算似然度
- en: A data sample is made of multiple features, which means that in reality, the
    *x* part of *P(x|y)* is made of *x[1]*, *x[2]*, *x[3]*, .... *x[k]*, where *k*
    is the number of features. Thus, the conditional probability can be expressed
    as *P(x[1], x[2], x[3], .... x[k]|y)*. In practice, this means that we need to
    calculate this conditional probability for all possible combinations of *x*. The
    main drawback of this is the lack of generalization of our models.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一个数据样本由多个特征构成，这意味着在实际应用中，*P(x|y)*中的*x*部分由*x[1]*、*x[2]*、*x[3]*、.... *x[k]*构成，其中*k*是特征的数量。因此，条件概率可以表示为*P(x[1],
    x[2], x[3], .... x[k]|y)*。实际上，这意味着我们需要为*x*的所有可能组合计算该条件概率。这么做的主要缺点是我们模型的泛化能力不足。
- en: 'Let''s use the following toy example to make things clearer:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下的玩具示例来澄清这一点：
- en: '| **Text** | **Does the text suggest that the writer likes fruit?** |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| **文本** | **文本是否表明作者喜欢水果？** |'
- en: '| I like apples | Yes |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 我喜欢苹果 | 是 |'
- en: '| I like oranges | Yes |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 我喜欢橙子 | 是 |'
- en: '| I hate pears | No |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 我讨厌梨 | 否 |'
- en: If the previous table is our training data, the likelihood probability, *P(x|y)*,
    for the first sample is the probability of seeing the three words **I**, **like**,
    and **apples** together, given the target, **Yes**. Similarly, for the second
    sample, it is the probability of seeing the three words **I**, **like**, and **oranges**
    together, given the target, **Yes**. The same goes for the third sample, where
    the target is **No** instead of **Yes**. Now, say we are given a new sample, **I
    hate apples**. The problem is that we have never seen these three words together
    before. You might say, "But we've seen each individual word of the sentence before,
    just separately!" That's correct, but our formula only cares about combinations
    of words. It cannot learn anything from each separate feature on its own.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的表格是我们的训练数据，第一个样本的似然概率，*P(x|y)*，就是给定目标**是**时，看到三个词**我**、**喜欢**和**苹果**一起出现的概率。同理，第二个样本的概率是给定目标**是**时，看到三个词**我**、**喜欢**和**橙子**一起出现的概率。第三个样本也是如此，只不过目标是**否**而不是**是**。现在，假设我们给定一个新样本，**我讨厌苹果**。问题是，我们之前从未见过这三个词一起出现。你可能会说：“但是我们以前见过每个单独的词，只是分开出现！”这是正确的，但我们的公式只关心词的组合。它无法从每个单独的特征中学到任何东西。
- en: You may recall from [Chapter 4](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=27&action=edit),
    *Preparing Your Data*, that *P(x[1], x[2], x[3], .... x[k]|y)* can only be expressed
    as *P(x[1]|y)* P(x[2]|y)x[3]* .. * P(x[k]|y)* if *x[1], x[2], x[3], .... x[k]*
    are independent. Their independence is not something we can be sure of, yet we
    still make this naive assumption in order to make the model more generalizable.
    As a result of this assumption and dealing with separate words, we can now learn
    something about the phrase **I hate apples**, despite not seeing it before. This
    naive yet useful assumption of independence is what gave the classifier's name
    its "naive" prefix.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能记得在[第4章](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=27&action=edit)中，*准备你的数据*，*P(x[1],
    x[2], x[3], .... x[k]|y)* 只有在 *x[1], x[2], x[3], .... x[k]* 互相独立时，才能表示为 *P(x[1]|y)*
    P(x[2]|y)x[3]* .. * P(x[k]|y)*。它们的独立性并非我们可以确定的，但我们仍然做出了这个朴素的假设，以使模型更具普适性。由于这一假设，并且由于我们处理的是独立的单词，现在我们可以了解关于短语**我讨厌苹果**的一些信息，尽管我们以前从未见过它。这种虽然朴素但有用的独立性假设，正是给分类器命名时加上“朴素”前缀的原因。
- en: Naive Bayes implementations
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯实现
- en: In scikit-learn, there are various Naive Bayes implementations.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，有多种朴素贝叶斯实现方式。
- en: The **multinomial Naive Bayes** classifier is the most commonly used implementation
    for text classification. Its implementation is most similar to what we saw in
    the previous section.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多项式朴素贝叶斯**分类器是文本分类中最常用的实现。它的实现方式与我们在前一节看到的最为相似。'
- en: The **Bernoulli Naive Bayes****classifier assumes the features to be binary.
    Rather than counting how many times a term appears in each document, in the Bernoulli
    version, we only care whether a term exists or not. The way the likelihood is
    calculated explicitly penalizes the non-occurrence of the terms in the documents,
    and it might perform better on some datasets, especially those with shorter documents.**
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伯努利朴素贝叶斯**分类器假设特征是二元的。在伯努利版本中，我们关注的不是每个文档中某个词出现的次数，而是该词是否存在。计算似然的方式明确惩罚文档中未出现的词汇，在一些数据集上，尤其是短文档的数据集上，它可能表现得更好。'
- en: '***   **Gaussian Naive Bayes** is used with continuous features. It assumes
    the features to be normally distributed and calculates the likelihood probabilities
    using maximum likelihood estimation. This implementation is useful for other cases
    aside from text analysis.**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '***高斯朴素贝叶斯**用于连续特征。它假设特征呈正态分布，并使用最大似然估计计算似然概率。该实现适用于文本分析之外的其他情况。**'
- en: '**Furthermore, you can also read about two other implementations, **complement
    Naive Bayes** and **categorical Naive Bayes**, in the scikit-learn user guide
    ([https://scikit-learn.org/stable/modules/naive_bayes.html](https://scikit-learn.org/stable/modules/naive_bayes.html)).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**此外，你还可以在scikit-learn用户指南中阅读关于另外两个实现——**互补朴素贝叶斯**和**类别朴素贝叶斯**的内容，链接为([https://scikit-learn.org/stable/modules/naive_bayes.html](https://scikit-learn.org/stable/modules/naive_bayes.html))。'
- en: Additive smoothing
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加性平滑
- en: When a term not seen during training appears during prediction, we set its probability
    to 0\. This sounds logical, yet it is a problematic decision to make given our
    naive assumption. Since *P(x[1], x[2], x[3], .... x[k]|y)* is equal to *P(x[1]|y)*
    P(x[2]|y)*P(x[3]|y) * .. * P(x[k]|y),* setting the conditional probability for
    any term to zero will set the entire *P(x[1], x[2], x[3], .... x[k]|y)* to zero
    as a result. To avoid this problem, we pretend that a new document that contains
    the whole vocabulary was added to each class. Conceptually, this new hypothetical
    document takes a portion of the probability mass assigned to the terms we have
    seen and reassigns it to the unseen terms. The `alpha` parameter controls how
    much of the probability mass we want to reassign to the unseen terms. Setting
    `alpha` to 1 is called **Laplace smoothing**, while setting it to values between
    0 and 1 is called **Lidstone****smoothing**.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当在预测过程中出现训练时未见过的词汇时，我们将其概率设置为0。这听起来很合乎逻辑，但鉴于我们天真的假设，这其实是一个有问题的决定。由于*P(x[1],
    x[2], x[3], .... x[k]|y)* 等于 *P(x[1]|y)* P(x[2]|y)*P(x[3]|y)* .. * P(x[k]|y)，*将任何词汇的条件概率设为零，将导致整个
    *P(x[1], x[2], x[3], .... x[k]|y)* 被设置为零。为了避免这个问题，我们假设每个类别中都加入了一份包含所有词汇的文档。从概念上讲，这个新的假设性文档将从我们已见过的词汇中分配一部分概率质量，并将其重新分配给未见过的词汇。`alpha`
    参数控制我们希望重新分配给未见过的词汇的概率质量。将 `alpha` 设置为1被称为**拉普拉斯平滑**，而将其设置为介于0和1之间的值则称为**利德斯通平滑**。
- en: 'I find myself using Laplace smoothing a lot when calculating ratios. In addition
    to preventing us from dividing by zero, it also helps to deal with uncertainties.
    Let me explain further using the following two examples:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现在计算比率时，经常使用拉普拉斯平滑。除了防止我们出现除以零的情况外，它还帮助处理不确定性。让我通过以下两个例子进一步解释：
- en: '**Example 1**: 10,000 people saw a link, and 9,000 of them clicked on it. We
    can obviously estimate the click-through rate to be 90%.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**例子 1**：10,000人看到了一个链接，其中9,000人点击了它。显然，我们可以估算点击率为90%。'
- en: '**Example 2**: If our data has only one person, and that person saw the link
    and clicked on it, would we be confident enough to say that the click-through
    rate was 100%?'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**例子 2**：如果我们的数据中只有一个人，而且这个人看到了链接并点击了它，我们能有足够的信心说点击率是100%吗？'
- en: In the previous examples, if we pretended that there were two additional users,
    where only one of them clicked on the link, the click-through rate in the first
    example would become 9,001 out of 10,002, which is still almost 90%. In the second
    example, though, we would be dividing 2 by 3, which would leave 60%, instead of
    the 100% calculated earlier. Laplace smoothing and Lidstone smoothing can be linked
    to the Bayesian way of thinking. Those two users, where 50% of them clicked on
    the link, are our prior belief. Initially, we do not know much, so we assume a
    50% click-through rate. Now, in the first example, we have enough data to overrule
    this prior belief, while in the second case, the fewer data points were only able
    to move the prior so much.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，如果我们假设有两个额外的用户，其中只有一个点击了链接，那么第一个例子的点击率将变为9,001/10,002，仍然接近90%。然而，在第二个例子中，我们将用2除以3，这将得到60%，而不是之前计算出的100%。拉普拉斯平滑和利德斯通平滑可以与贝叶斯的思维方式联系起来。这两个用户，其中50%的人点击了链接，代表了我们的先验信念。最初，我们了解的信息很少，所以我们假设点击率为50%。现在，在第一个例子中，我们有足够的数据来推翻这个先验信念，而在第二个例子中，较少的数据点只能稍微调整先验。
- en: That's enough theory for now – let's use everything we have learned so far to
    tell whether some reviewers are happy about their movie-watching experience or
    not.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在先不谈理论 – 让我们用到目前为止学到的所有内容，来判断一些评论者是否对他们的观影体验感到满意。
- en: Classifying text using a Naive Bayes classifier
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯分类器进行文本分类
- en: In this section, we are going to get a list of sentences and classify them based
    on the user's sentiment. We want to tell whether the sentence carries a positive
    or a negative sentiment. *Dimitrios Kotzias et al* created this dataset for their
    research paper, *From Group to Individual Labels using Deep Features*. They collected
    a list of random sentences from three different websites, where each sentence
    is labeled with either 1 (positive sentiment) or 0 (negative sentiment).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将获取一组句子，并根据用户的情感对其进行分类。我们要判断该句子是带有积极情感还是消极情感。*Dimitrios Kotzias 等人* 为他们的研究论文《*从群体到个体标签，利用深度特征*》创建了这个数据集。他们从三个不同的网站收集了一组随机句子，并将每个句子标记为1（积极情感）或0（消极情感）。
- en: In total, there are 2,745 sentences in the data set. In the following sections,
    we are going to download the dataset, preprocess it, and classify the sentences
    in it.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中总共有2,745个句子。在接下来的部分中，我们将下载数据集、预处理数据，并对其中的句子进行分类。
- en: Downloading the data
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载数据
- en: You can just open the browser, download the CSV files into a local folder, and
    use pandas to load the files into DataFrames. However, I prefer to use Python
    to download the files, rather than the browser. I don't do this out of geekiness,
    but to ensure the reproducibility of my entire process by putting it into code.
    Anyone can just run my Python code and get the same results, without having to
    read a lousy documentation file, find a link to the compressed file, and follow
    the instructions to get the data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以直接打开浏览器，将CSV文件下载到本地文件夹，并使用pandas将文件加载到数据框中。然而，我更喜欢使用Python来下载文件，而不是使用浏览器。我这么做不是因为我是极客，而是为了确保我的整个过程的可重现性，将其编写成代码。任何人都可以运行我的Python代码并得到相同的结果，而不需要阅读糟糕的文档文件，找到压缩文件的链接，并按照指示获取数据。
- en: 'Here are the steps to download the data we need:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是下载所需数据的步骤：
- en: 'First, let''s create a folder to store the downloaded data into it. The following
    code checks whether the required folder exists or not. If it is not there, it
    creates it into the current working directory:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个文件夹来存储下载的数据。以下代码检查所需文件夹是否存在。如果不存在，它会在当前工作目录中创建该文件夹：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then we need to install the `requests` library using `pip`, as we will use
    it to download the data:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们需要使用`pip`安装`requests`库，因为我们将使用它来下载数据：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, we download the compressed data as follows:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们按照以下方式下载压缩数据：
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we can uncompress the data and store it into the data folder we have just
    created. We will be using the `zipfile` module to uncompress our data. The `ZipFile`
    method expects to read a file object. Thus, we use `BytesIO` to convert the content
    of the response into a file-like object. Then we extract the content of the zip
    file into our folder as follows:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以解压数据并将其存储到刚创建的数据文件夹中。我们将使用`zipfile`模块来解压数据。`ZipFile`方法期望读取一个文件对象。因此，我们使用`BytesIO`将响应内容转换为类似文件的对象。然后，我们将zip文件的内容提取到我们的文件夹中，如下所示：
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now that our data is written into 3 separate files in our data folder, we can
    load each one of the 3 files into a separate data frame. Then, we can combine
    the 3 data frames into a single data frame as follows:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们的数据已经写入到数据文件夹中的3个独立文件中，我们可以将这3个文件分别加载到3个数据框中。然后，我们可以将这3个数据框合并成一个单一的数据框，如下所示：
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can display the distribution of the sentiment labels using the following
    code:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码显示情感标签的分布：
- en: '[PRE23]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As we can see, the two classes are more or less equal. It is a good practice
    to check the distribution of your classes before running any classification task:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，两个类别大致相等。在进行任何分类任务之前，检查类别的分布是一种好习惯：
- en: '![](img/2a79736b-17ac-4eb2-bed3-ed7cbe53ea12.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a79736b-17ac-4eb2-bed3-ed7cbe53ea12.png)'
- en: 'We can also display a few sample sentences using the following code, after
    tweaking pandas'' settings to display more characters per cell:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以使用以下代码显示一些示例句子，调整pandas的设置以显示更多字符：
- en: '[PRE24]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'I set the `random_state` to an arbitrary value to make sure we both get the
    same samples as below:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我将`random_state`设置为一个任意值，以确保我们得到相同的样本，如下所示：
- en: '![](img/62567e34-644b-408c-bb6c-8ef36434604f.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62567e34-644b-408c-bb6c-8ef36434604f.png)'
- en: Preparing the data
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'Now we need to prepare the data for our classifier to use it:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要为分类器准备数据以供使用：
- en: 'As we usually do, we start by splitting the DataFrame into training and testing
    sets. I kept 40% of the data set for testing, and also set `random_state` to an
    arbitrary value to make sure we both get the same random split:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像我们通常做的那样，我们首先将数据框分割为训练集和测试集。我将40%的数据集用于测试，并将`random_state`设置为一个任意值，以确保我们都获得相同的随机分割：
- en: '[PRE25]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then we get our labels from the sentiment column as follows:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们从情感列中获取标签，如下所示：
- en: '[PRE26]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As for the textual features, let''s convert them using `CountVectorizer`. We
    will include unigrams as well as bigrams and trigrams. We can also ignore rare
    words by setting `min_df` to `3` to exclude words appearing in fewer than three
    documents. This is a useful practice for removing spelling mistakes and noisy
    tokens. Finally, we can strip accents from letters and convert them to `ASCII`:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于文本特征，让我们使用`CountVectorizer`来转换它们。我们将包括一元组、二元组和三元组。我们还可以通过将`min_df`设置为`3`来忽略稀有词，这样就可以排除出现在少于三个文档中的单词。这是去除拼写错误和噪声标记的一个有效做法。最后，我们可以去掉字母的重音并将其转换为`ASCII`：
- en: '[PRE27]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the end, we can use the Naive Bayes classifier to classify our data. We
    set `fit_prior=True` for the model to use the distribution of the class labels
    in the training data as its prior:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用朴素贝叶斯分类器来分类我们的数据。我们为模型设置`fit_prior=True`，使其使用训练数据中类别标签的分布作为先验：
- en: '[PRE28]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This time, our old good accuracy score may not be informative enough. We want
    to know how accurate we are per class. Furthermore, depending on our use case,
    we may need to tell whether the model was able to identify all the negative tweets,
    even if it did that at the expense of misclassifying some positive tweets. To
    be able to get this information, we need to use the `precision` and `recall` scores.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们的传统准确度得分可能不足以提供足够的信息。我们希望知道每个类别的准确性。此外，根据我们的使用案例，我们可能需要判断模型是否能够识别所有的负面推文，即使这样做的代价是错误地分类了一些正面推文。为了获得这些信息，我们需要使用`精度`和`召回率`得分。
- en: Precision, recall, and F1 score
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精度、召回率和F1得分
- en: 'Out of the samples that were assigned to the positive class, the percentage
    of them that were actually positive is the**precision** of this class. For the
    positive tweets, the percentage of them that the classifier correctly predicted
    to be positive is the **recall** for this class. As you can see, the precision
    and recall are calculated per class. Here is how we formally express the **precision
    score** in terms of true positives and false positives:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在被分配到正类的样本中，实际上为正类的百分比就是该类别的**精度**。对于正面推文，分类器正确预测为正面的推文百分比就是该类别的**召回率**。如你所见，精度和召回率是按类别计算的。以下是我们如何用真实正例和假正例正式表达**精度得分**：
- en: '![](img/eeb004df-1cbb-438e-9d77-7389cd083b56.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eeb004df-1cbb-438e-9d77-7389cd083b56.png)'
- en: The **recall score** is expressed in terms of true positives and false negatives*:*
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**召回率得分**是通过真实正例和假负例来表示的*：*'
- en: '![](img/c8d8af97-42d7-4c94-a66d-33da59706fa2.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8d8af97-42d7-4c94-a66d-33da59706fa2.png)'
- en: 'To summarize the two previous scores into one number, the *F[1] score* can
    be used. It combines the precision and recall scores using the following formula:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将前两个得分汇总为一个数字，可以使用*F[1]得分*。它通过以下公式将精度和召回率得分结合起来：
- en: '![](img/bb6db0cd-d232-4b46-b24b-27bfe55625de.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb6db0cd-d232-4b46-b24b-27bfe55625de.png)'
- en: 'Here we calculate the three aforementioned metrics for our classifier:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们计算了我们的分类器的三项上述度量：
- en: '[PRE29]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To make it clear, I put the resulting metrics into the following table. Keep
    in mind that the support is just the number of samples in each class:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地说明，我将结果度量放入以下表格中。请记住，支持度仅仅是每个类别中的样本数量：
- en: '![](img/a748541e-1612-4be1-863d-635d2eee912e.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a748541e-1612-4be1-863d-635d2eee912e.png)'
- en: We have equivalent scores given that the sizes of the two classes are almost
    equal. In cases where the classes are imbalanced, it is more common to see one
    class achieving a higher precision or a higher recall compared to the other.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 由于两个类别的大小几乎相等，因此得分是相等的。如果类别不平衡，通常会看到某个类别的精度或召回率高于另一个类别。
- en: Since these metrics are calculated per class label, we can also get their macro
    averages. For this example here, the macro average precision score will be the
    average of **0.81**, and **0.77**, which is **0.79**. A micro average, on the
    other hand, calculates these scores globally based on the overall number of true
    positive, false positive, and false negative samples.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些度量是按类别标签计算的，我们还可以获得它们的宏观平均值。在此示例中，宏观平均精度得分将是**0.81**和**0.77**的平均值，即**0.79**。另一方面，微观平均是基于总体的真实正例、假正例和假负例样本数量来计算这些得分的。
- en: Pipelines
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流水线
- en: In the previous chapters, we used a grid search to find the optimal hyperparameters
    for our estimators. Now, we have multiple things to optimize at once. One the
    one hand, we want to optimize the Naive Bayes hyperparameters, but on the other
    hand, we also want to optimize the parameters of the vectorizer used at the preprocessing
    step. Since a grid search expects one object only, scikit-learn provides a `pipeline`
    wrapper where we can combine multiple transformers and estimators into one.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们使用网格搜索来找到估计器的最佳超参数。现在，我们有多个东西要同时优化。一方面，我们想优化朴素贝叶斯的超参数，另一方面，我们还想优化预处理步骤中使用的向量化器的参数。由于网格搜索只期望一个对象，scikit-learn
    提供了一个`pipeline`封装器，我们可以在其中将多个转换器和估计器组合成一个。
- en: 'As the name suggests, the pipeline is made of a set of sequential steps. Here
    we start with `CountVectorizer` and have `MultinomialNB` as the second and final
    step:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，管道是由一系列顺序步骤组成的。在这里，我们从`CountVectorizer`开始，`MultinomialNB`作为第二步也是最后一步：
- en: '[PRE30]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: All objects but the one in the last step are expected to be `transformers`;
    that is, they should have the `fit`, `transform`, and `fit_transform` methods.
    The object in the last step is expected to be `estimator`, meaning it should have
    the `fit` and `predict` methods. You can also build your custom transformers and
    estimators and use them in the pipeline as long as they have the expected methods.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 除了最后一步的对象外，其他所有对象都应该是`transformers`，即它们应具有`fit`、`transform`和`fit_transform`方法。最后一步的对象应为`estimator`，意味着它应该具有`fit`和`predict`方法。你还可以构建自定义的转换器和估计器，并在管道中使用，只要它们具有预期的方法。
- en: Now that we have our pipeline ready, we can plug it into `GridSearchCV` to find
    the optimal hyperparameters.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的管道已经准备好，我们可以将其插入`GridSearchCV`中，以寻找最佳超参数。
- en: Optimizing for different scores
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 针对不同得分的优化
- en: '"What gets measured gets managed."'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: “你所衡量的，便是你所管理的。”
- en: – Peter Drucker
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ——彼得·德鲁克
- en: When we used `GridSearchCV` before, we did not specify which metric we want
    to optimize our hyperparameters for. The classifier's accuracy was used by default.
    Alternatively, you can also choose to optimize your hyperparameters for the precision
    score or the recall score. We will set our grid search here to optimize for the
    macro precision score.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们之前使用`GridSearchCV`时，我们没有指定要优化的超参数的度量标准。默认情况下使用分类器的准确度。或者，你也可以选择优化超参数的精确度得分或召回率得分。在这里，我们将设置网格搜索以优化宏精确度得分。
- en: 'We start by setting the different hyperparameters that we want to search within.
    Since we are using a pipeline here, we prefix each hyperparameter with the name
    of the step it is designated for, in order for the pipeline to assign the parameter
    to the correct step:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从设置要搜索的不同超参数开始。由于我们在这里使用管道，因此我们需要为每个超参数添加步骤名称的前缀，以便管道将参数分配给正确的步骤：
- en: '[PRE31]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: By default, the priors, `P(y)`, in the Bayes rule are set based on the number
    of samples in each class. However, we can set them to be constant for all classes
    by setting `fit_prior=False`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，贝叶斯规则中的先验`P(y)`是根据每个类别中的样本数设置的。然而，我们可以通过设置`fit_prior=False`将其设置为所有类别的常量。
- en: 'Here, we run `GridSearchCV` while letting it know that we care about precision
    the most:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们运行`GridSearchCV`，并告诉它我们最关心的是精确度：
- en: '[PRE32]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This gives us the following hyperparameters:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了以下超参数：
- en: '`ngram_range`: (1, 3)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram_range`: (1, 3)'
- en: '`alpha`: 1'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`: 1'
- en: '`fit_prior`: False'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fit_prior`: False'
- en: We get a macro precision of 80.5% and macro recall of 80.5%.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了80.5%的宏精确度和80.5%的宏召回率。
- en: Due to the balanced class distributions, it was expected for the prior not to
    add much value. We also get similar precision and recall scores. Thus, it doesn't
    make sense now to re-run the grid search again for an optimized recall. We will
    most likely get identical results anyway. Nevertheless, things will likely be
    different when you deal with highly imbalanced classes, and you want to maximize
    the recall of one class at the expense of the others.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 由于类别分布平衡，预计先验不会增加太多价值。我们还获得了相似的精确度和召回率得分。因此，现在重新运行网格搜索以优化召回率没有意义，我们无论如何都会得到相同的结果。然而，当处理高度不平衡的类别时，事情可能会有所不同，这时你可能希望通过牺牲其他类别的效果来最大化某一类别的召回率。
- en: In the next section, we are going to use word embeddings to represent our tokens.
    Let's see if this form of transfer learning will help our classifier perform better.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用词嵌入来表示我们的标记。让我们看看这种迁移学习方法是否能帮助我们的分类器表现得更好。
- en: Creating a custom transformer
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建自定义转换器
- en: Before ending this chapter, we can also create a custom transformer based on
    the `Word2Vec` embedding and use it in our classification pipeline instead of
    `CountVectorizer`. In order to be able to use our custom transformer in the pipeline,
    we need to make sure it has `fit`, `transform`, and `fit_transform` methods.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本章之前，我们还可以基于`Word2Vec`嵌入创建一个自定义变换器，并在我们的分类管道中使用它，而不是使用`CountVectorizer`。为了能够在管道中使用我们的自定义变换器，我们需要确保它具备`fit`、`transform`和`fit_transform`方法。
- en: 'Here is our new transformer, whichwe will call `WordEmbeddingVectorizer`:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们新的变换器，我们将其命名为`WordEmbeddingVectorizer`：
- en: '[PRE33]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The `fit` method here is impotent—it does not do anything since we are using
    a pre-trained model from spaCy. We can use the newly created transformer as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的`fit`方法是无效的——它什么也不做，因为我们使用的是spaCy的预训练模型。我们可以按照以下方式使用新创建的变换器：
- en: '[PRE34]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Instead of the Naive Bayes classifier, we can also use this transformer with
    other classifiers, such as `LogisticRegression` or `Multi-layer Perceptron`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个变换器与其他分类器一起使用，而不仅仅是朴素贝叶斯分类器，例如`LogisticRegression`或`Multi-layer Perceptron`。
- en: 'The `apply` function in pandas can be slow, especially when dealing with high
    volumes of data. I like to use a library called `tqdm`, which allows me to replace
    the `apply()` method with `progress_apply()`, which then displays a progress bar
    while running. All you have to do after importing the library is run `tqdm.pandas()`;
    this adds the `progress_apply()` method to the pandas Series and DataFrame objects.
    Fun fact: the word `tqdm`means *progress* in Arabic.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: pandas中的`apply`函数可能会很慢，尤其是在处理大量数据时。我喜欢使用一个叫做`tqdm`的库，它可以让我将`apply()`方法替换为`progress_apply()`，这样在运行时就会显示进度条。导入库后，你只需运行`tqdm.pandas()`；这会将`progress_apply()`方法添加到pandas的Series和DataFrame对象中。顺便说一句，`tqdm`这个词在阿拉伯语中意味着*进度*。
- en: Summary
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Personally, I find the field of natural language processing very exciting. The
    vast majority of our knowledge as humans is contained in books, documents, and
    web pages. Knowing how to automatically extract this information and organize
    it with the help of machine learning is essential to our scientific progress and
    endeavors in automation. This is why multiple scientific fields, such as information
    retrieval, statistics, and linguistics, borrow ideas from each other and try to
    solve the same problem from different angles. In this chapter, we also borrowed
    ideas from all these fields and learned how to represent textual data in formats
    suitable to machine learning algorithms. We also learned about the utilities that
    scikit-learn provides to aid in building and optimizing end-to-end solutions.
    We also encountered concepts such as transfer learning, and we were able to seamlessly
    incorporate spaCy's language models into scikit-learn.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 就我个人而言，我发现自然语言处理领域非常令人兴奋。我们人类的绝大多数知识都包含在书籍、文档和网页中。了解如何借助机器学习自动提取这些信息并组织它们，对我们的科学进步和自动化事业至关重要。这就是为什么多个科学领域，如信息检索、统计学和语言学，相互借鉴思想并试图从不同角度解决同一个问题。在本章中，我们也借鉴了这些领域的思想，并学习了如何将文本数据表示为适合机器学习算法的格式。我们还了解了scikit-learn提供的工具，以帮助构建和优化端到端解决方案。我们还遇到了转移学习等概念，并能够无缝地将spaCy的语言模型集成到scikit-learn中。
- en: From the next chapter, we are going to deal with slightly advanced topics. In
    the next chapter, we will learn about artificial neural networks (multi-layer
    perceptron). This is a very hot topic nowadays, and understanding its main concepts
    helps anyone who wants to get deeper into deep learning. Since neural networks
    are commonly used in image processing, we will seize the opportunity to build
    on what we learned in [Chapter 5](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=28&action=edit),
    Image Processing with Nearest Neighbors and expand our image processing knowledge
    even further.************
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 从下一章开始，我们将处理一些稍微高级的话题。在下一章中，我们将学习人工神经网络（多层感知机）。这是当今非常热门的话题，理解其主要概念对于任何想深入学习深度学习的人来说都很有帮助。由于神经网络通常用于图像处理，我们将借此机会，在[第5章](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=28&action=edit)《最近邻图像处理》中继续扩展我们的图像处理知识。
