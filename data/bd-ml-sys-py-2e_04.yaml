- en: Chapter 4. Topic Modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章 主题建模
- en: In the previous chapter, we grouped text documents using clustering. This is
    a very useful tool, but it is not always the best. Clustering results in each
    text belonging to exactly one cluster. This book is about machine learning and
    Python. Should it be grouped with other Python-related works or with machine-related
    works? In a physical bookstore, we will need a single place to stock the book.
    In an Internet store, however, the answer is *this book is about both machine
    learning and Python* and the book should be listed in both the sections in an
    online bookstore. This does not mean that the book will be listed in all the sections,
    of course. We will not list this book with other baking books.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用聚类方法对文本文档进行了分组。这是一个非常有用的工具，但并不总是最好的。聚类将每个文本分配到一个唯一的簇中。这本书是关于机器学习和
    Python 的。它应该与其他 Python 相关书籍分在一起，还是与机器学习相关书籍分在一起？在实体书店中，我们需要为这本书提供一个固定的存放位置。然而，在互联网书店中，答案是*这本书既关于机器学习，也关于
    Python*，所以这本书应该列在在线书店的两个类别中。当然，这并不意味着这本书会出现在所有类别中。我们不会把这本书列在烘焙类书籍中。
- en: In this chapter, we will learn methods that do not cluster documents into completely
    separate groups but allow each document to refer to several **topics**. These
    topics will be identified automatically from a collection of text documents. These
    documents may be whole books or shorter pieces of text such as a blogpost, a news
    story, or an e-mail.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将学习一些方法，这些方法不会将文档完全分为独立的组，而是允许每个文档涉及多个**主题**。这些主题将自动从一组文本文档中识别出来。这些文档可以是整本书，或者是较短的文本片段，如博客文章、新闻故事或电子邮件。
- en: We would also like to be able to infer the fact that these documents may have
    topics that are central to them, while referring to other topics only in passing.
    This book mentions plotting every so often, but it is not a central topic as machine
    learning is. This means that documents have topics that are central to them and
    others that are more peripheral. The subfield of machine learning that deals with
    these problems is called **topic modeling** and is the subject of this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望能够推断出这些文档可能有些主题是其核心内容，而仅仅是提及其他主题。例如，这本书偶尔提到绘图，但它不是像机器学习那样的核心主题。这意味着文档有些主题是其核心内容，其他则是较为外围的内容。处理这些问题的机器学习子领域被称为**主题建模**，也是本章的主题。
- en: Latent Dirichlet allocation
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配
- en: '**LDA and LDA—**unfortunately, there are two methods in machine learning with
    the initials LDA: latent Dirichlet allocation, which is a topic modeling method
    and linear discriminant analysis, which is a classification method. They are completely
    unrelated, except for the fact that the initials LDA can refer to either. In certain
    situations, this can be confusing. The scikit-learn tool has a submodule, `sklearn.lda`,
    which implements linear discriminant analysis. At the moment, scikit-learn does
    not implement latent Dirichlet allocation.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**LDA 和 LDA——**不幸的是，在机器学习中有两个以 LDA 为首字母缩写的方法：潜在狄利克雷分配（Latent Dirichlet Allocation，LDA），一种主题建模方法，以及线性判别分析（Linear
    Discriminant Analysis，LDA），一种分类方法。它们完全不相关，除了 LDA 的首字母可以指代这两者之外。在某些情况下，这可能会造成混淆。scikit-learn
    工具包有一个子模块 `sklearn.lda`，用于实现线性判别分析。目前，scikit-learn 并没有实现潜在狄利克雷分配。'
- en: The topic model we will look at is **latent Dirichlet allocation** (**LDA**).
    The mathematical ideas behind LDA are fairly complex, and we will not go into
    the details here.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要看的主题模型是**潜在狄利克雷分配**（**LDA**）。LDA 背后的数学原理相当复杂，我们在这里不会详细讨论。
- en: 'For those who are interested, and adventurous enough, Wikipedia will provide
    all the equations behind these algorithms: [http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些有兴趣并且足够冒险的人，维基百科将提供所有这些算法背后的方程式：[http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)。
- en: However, we can understand the ideas behind LDA intuitively at a high-level.
    LDA belongs to a class of models that are called generative models as they have
    a sort of fable, which explains how the data was generated. This generative story
    is a simplification of reality, of course, to make machine learning easier. In
    the LDA fable, we first create topics by assigning probability weights to words.
    Each topic will assign different weights to different words. For example, a Python
    topic will assign high probability to the word "variable" and a low probability
    to the word "inebriated". When we wish to generate a new document, we first choose
    the topics it will use and then mix words from these topics.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以在高层次上直观地理解 LDA 背后的思想。LDA 属于一种叫做生成模型的模型类别，因为它们有一个类似寓言的故事，解释了数据是如何生成的。当然，这个生成的故事是对现实的简化，以便让机器学习更容易。在
    LDA 的寓言中，我们首先通过为单词分配概率权重来创建主题。每个主题会为不同的单词分配不同的权重。例如，Python 主题会为单词“variable”分配较高的概率，为单词“inebriated”分配较低的概率。当我们希望生成一个新文档时，首先选择它将使用的主题，然后从这些主题中混合单词。
- en: 'For example, let''s say we have only three topics that books discuss:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们只有三种书籍讨论的主题：
- en: Machine learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习
- en: Python
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python
- en: Baking
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 烘焙
- en: For each topic, we have a list of words associated with it. This book will be
    a mixture of the first two topics, perhaps 50 percent each. The mixture does not
    need to be equal, it can also be a 70/30 split. When we are generating the actual
    text, we generate word by word; first we decide which topic this word will come
    from. This is a random decision based on the topic weights. Once a topic is chosen,
    we generate a word from that topic's list of words. To be precise, we choose a
    word in English with the probability given by the topic.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个主题，我们都有一个与之相关的单词列表。这本书将是前两个主题的混合，可能每个占 50%。混合比例不一定要相等，它也可以是 70/30 的分配。在生成实际文本时，我们是一个一个单词生成的；首先决定这个单词将来自哪个主题。这是基于主题权重的随机决定。一旦选择了一个主题，我们就从该主题的单词列表中生成一个单词。准确来说，我们会根据该主题给定的概率选择一个英文单词。
- en: In this model, the order of words does not matter. This is a *bag of words*
    model as we have already seen in the previous chapter. It is a crude simplification
    of language, but it often works well enough, because just knowing which words
    were used in a document and their frequencies are enough to make machine learning
    decisions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，单词的顺序不重要。这是一个 *词袋模型*，正如我们在上一章中看到的那样。它是对语言的粗略简化，但通常足够有效，因为仅仅知道文档中使用了哪些单词及其频率，就足以做出机器学习决策。
- en: In the real world, we do not know what the topics are. Our task is to take a
    collection of text and to reverse engineer this fable in order to discover what
    topics are out there and simultaneously figure out which topics each document
    uses.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，我们不知道主题是什么。我们的任务是获取一组文本，并反向工程这个寓言，以发现有哪些主题，并同时弄清楚每个文档使用了哪些主题。
- en: Building a topic model
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建主题模型
- en: 'Unfortunately, scikit-learn does not support latent Dirichlet allocation. Therefore,
    we are going to use the gensim package in Python. Gensim is developed by Radim
    Řehůřek who is a machine learning researcher and consultant in the United Kingdom.
    We must start by installing it. We can achieve this by running the following command:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，scikit-learn 不支持潜在狄利克雷分配（LDA）。因此，我们将使用 Python 中的 gensim 包。Gensim 由 Radim
    Řehůřek 开发，他是英国的机器学习研究员和顾问。我们必须首先安装它。可以通过运行以下命令来实现：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As input data, we are going to use a collection of news reports from the **Associated
    Press** (**AP**). This is a standard dataset for text modeling research, which
    was used in some of the initial works on topic models. After downloading the data,
    we can load it by running the following code:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作为输入数据，我们将使用来自 **美联社** (**AP**) 的一组新闻报道。这是文本建模研究的标准数据集，在一些最初的主题模型研究中被使用。下载数据后，我们可以通过运行以下代码加载它：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `corpus` variable holds all of the text documents and has loaded them in
    a format that makes for easy processing. We can now build a topic model using
    this object as input:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`corpus` 变量保存所有文本文档，并已将它们加载为易于处理的格式。我们现在可以使用这个对象作为输入构建主题模型：'
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This single constructor call will statistically infer which topics are present
    in the corpus. We can explore the resulting model in many ways. We can see the
    list of topics a document refers to using the `model[doc]` syntax, as shown in
    the following example:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个单一的构造函数调用将统计推断出语料库中存在的主题。我们可以通过多种方式来探索生成的模型。我们可以使用`model[doc]`语法查看文档涉及的主题列表，如下所示的示例：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The result will almost surely look different on our computer! The learning algorithm
    uses some random numbers and every time you learn a new topic model on the same
    input data, the result is different. Some of the qualitative properties of the
    model will be stable across different runs if your data is well behaved. For example,
    if you are using the topics to compare documents, as we do here, then the similarities
    should be robust and change only slightly. On the other hand, the order of the
    different topics will be completely different.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 结果在我们的计算机上几乎肯定会有所不同！学习算法使用了一些随机数，每次你在相同的输入数据上学习新的主题模型时，结果都会不同。如果数据表现得比较规范，模型的一些定性属性会在不同的运行中保持稳定。例如，如果你使用主题来比较文档，就像我们在这里做的那样，那么相似性应该是稳健的，只会稍微变化。另一方面，不同主题的顺序将完全不同。
- en: 'The format of the result is a list of pairs: `(topic_index, topic_weight)`.
    We can see that only a few topics are used for each document (in the preceding
    example, there is no weight for topics 0, 1, and 2; the weight for those topics
    is 0). The topic model is a sparse model, as although there are many possible
    topics; for each document, only a few of them are used. This is not strictly true
    as all the topics have a nonzero probability in the LDA model, but some of them
    have such a small probability that we can round it to zero as a good approximation.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的格式是一个由对组成的列表：`(topic_index, topic_weight)`。我们可以看到，每个文档仅使用了少数几个主题（在前面的示例中，主题0、1和2的权重为零；这些主题的权重为0）。主题模型是一个稀疏模型，虽然有很多可能的主题，但对于每个文档，只使用其中的少数几个。这并不完全准确，因为所有主题在LDA模型中都有非零概率，但其中一些概率非常小，我们可以将其四舍五入为零，作为一个较好的近似值。
- en: 'We can explore this further by plotting a histogram of the number of topics
    that each document refers to:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过绘制每个文档涉及的主题数量的直方图来进一步探索这一点：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You will get the following plot:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到以下图表：
- en: '![Building a topic model](img/2772OS_04_01.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![构建主题模型](img/2772OS_04_01.jpg)'
- en: Tip
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Sparsity** means that while you may have large matrices and vectors, in principle,
    most of the values are zero (or so small that we can round them to zero as a good
    approximation). Therefore, only a few things are relevant at any given time.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**稀疏性**意味着尽管你可能有大的矩阵和向量，但原则上，大多数值是零（或非常小，以至于我们可以将它们四舍五入为零，作为一个较好的近似）。因此，任何给定时刻，只有少数几件事是相关的。'
- en: Often problems that seem too big to solve are actually feasible because the
    data is sparse. For example, even though any web page can link to any other web
    page, the graph of links is actually very sparse as each web page will link to
    a very tiny fraction of all other web pages.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 经常看起来无法解决的问题实际上是可行的，因为数据是稀疏的。例如，尽管任何网页都可以链接到其他任何网页，但链接图实际上是非常稀疏的，因为每个网页只会链接到所有其他网页的极小一部分。
- en: In the preceding graph, we can see that about 150 documents have 5 topics, while
    the majority deals with around 10 to 12 of them. No document talks about more
    than 20 different topics.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，我们可以看到大约150篇文档涉及5个主题，而大多数文档涉及大约10到12个主题。没有任何文档讨论超过20个不同的主题。
- en: To a large extent, this is due to the value of the parameters that were used,
    namely, the `alpha` parameter. The exact meaning of alpha is a bit abstract, but
    bigger values for alpha will result in more topics per document.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在很大程度上，这是由于所使用参数的值，特别是`alpha`参数。`alpha`的确切含义有些抽象，但较大的`alpha`值将导致每个文档涉及更多的主题。
- en: 'Alpha needs to be a value greater than zero, but is typically set to a lesser
    value, usually, less than one. The smaller the value of `alpha`, the fewer topics
    each document will be expected to discuss. By default, gensim will set `alpha`
    to `1/num_topics`, but you can set it explicitly by passing it as an argument
    in the `LdaModel` constructor as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Alpha需要大于零，但通常设定为较小的值，通常小于1。`alpha`的值越小，每个文档预期讨论的主题就越少。默认情况下，gensim会将`alpha`设置为`1/num_topics`，但你可以通过在`LdaModel`构造函数中显式传递它作为参数来设置它，如下所示：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In this case, this is a larger alpha than the default, which should lead to
    more topics per document. As we can see in the combined histogram given next,
    gensim behaves as we expected and assigns more topics to each document:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，这是一个比默认值更大的 alpha 值，这应该会导致每个文档包含更多的主题。正如我们在接下来的合并直方图中看到的那样，gensim 按照我们的预期表现，给每个文档分配了更多的主题：
- en: '![Building a topic model](img/2772OS_04_02.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![构建主题模型](img/2772OS_04_02.jpg)'
- en: Now, we can see in the preceding histogram that many documents touch upon 20
    to 25 different topics. If you set the value lower, you will observe the opposite
    (downloading the code from the online repository will allow you to play around
    with these values).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在前面的直方图中看到，许多文档涉及20到25个不同的主题。如果你设置一个较低的值，你将看到相反的情况（从在线仓库下载代码将允许你调整这些值）。
- en: What are these topics? Technically, as we discussed earlier, they are multinomial
    distributions over words, which means that they assign a probability to each word
    in the vocabulary. Words with high probability are more associated with that topic
    than words with lower probability.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是什么主题？从技术上讲，正如我们之前讨论过的，它们是关于单词的多项式分布，这意味着它们为词汇表中的每个单词分配一个概率。高概率的单词与该主题的关联性大于低概率的单词。
- en: Our brains are not very good at reasoning with probability distributions, but
    we can readily make sense of a list of words. Therefore, it is typical to summarize
    topics by the list of the most highly weighted words.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑并不擅长处理概率分布的推理，但我们能够轻松理解一系列单词。因此，通常通过列出最重要的单词来总结主题。
- en: 'In the following table, we display the first ten topics:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，我们展示了前十个主题：
- en: '| Topic no. | Topic |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: 主题编号 | 主题 |
- en: '| --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 | dress military soviet president new state capt carlucci states leader
    stance government |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 穿着军装的苏联总统新国家领袖立场政府 |'
- en: '| 2 | koch zambia lusaka oneparty orange kochs party i government mayor new
    political |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 科赫赞比亚卢萨卡一党橙色科赫党我政府市长新政治 |'
- en: '| 3 | human turkey rights abuses royal thompson threats new state wrote garden
    president |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 人权土耳其虐待皇家汤普森威胁新国家写的花园总统 |'
- en: '| 4 | bill employees experiments levin taxation federal measure legislation
    senate president whistleblowers sponsor |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 法案雇员实验莱文税收联邦措施立法参议院总统举报人赞助 |'
- en: '| 5 | ohio july drought jesus disaster percent hartford mississippi crops northern
    valley virginia |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 俄亥俄州七月干旱耶稣灾难百分比哈特福德密西西比作物北部山谷弗吉尼亚 |'
- en: '| 6 | united percent billion year president world years states people i bush
    news |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 联合百分比十亿年总统世界年美国人民我布什新闻 |'
- en: '| 7 | b hughes affidavit states united ounces squarefoot care delaying charged
    unrealistic bush |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 7 | b hughes 宣誓书声明联合盎司平方英尺护理延迟被指控不现实布什 |'
- en: '| 8 | yeutter dukakis bush convention farm subsidies uruguay percent secretary
    general i told |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 约特杜卡基斯布什大会农场补贴乌拉圭百分比秘书长我告诉 |'
- en: '| 9 | kashmir government people srinagar india dumps city two jammukashmir
    group moslem pakistan |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 克什米尔政府人民斯里那加印度倾倒城市两座查谟克什米尔集团穆斯林巴基斯坦 |'
- en: '| 10 | workers vietnamese irish wage immigrants percent bargaining last island
    police hutton I |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 工人越南爱尔兰工资移民百分比谈判最后岛屿警察赫顿 |'
- en: 'Although daunting at first glance, when reading through the list of words,
    we can clearly see that the topics are not just random words, but instead these
    are logical groups. We can also see that these topics refer to older news items,
    from when the Soviet Union still existed and Gorbachev was its Secretary General.
    We can also represent the topics as word clouds, making more likely words larger.
    For example, this is the visualization of a topic which deals with the Middle
    East and politics:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管乍一看令人望而生畏，但当我们浏览这些单词列表时，我们可以清楚地看到这些主题并非随机的单词，而是逻辑分组。我们还可以看到，这些主题与苏联仍然存在且戈尔巴乔夫是其总书记时的旧新闻相关。我们还可以将这些主题表示为词云，使得高频词更加突出。例如，这是一个涉及中东和政治的主题的可视化：
- en: '![Building a topic model](img/2772OS_04_03.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![构建主题模型](img/2772OS_04_03.jpg)'
- en: We can also see that some of the words should perhaps be removed (for example,
    the word "I") as they are not so informative, they are stop words. When building
    topic modeling, it can be useful to filter out stop words, as otherwise, you might
    end up with a topic consisting entirely of stop words. We may also wish to preprocess
    the text to stems in order to normalize plurals and verb forms. This process was
    covered in the previous chapter and you can refer to it for details. If you are
    interested, you can download the code from the companion website of the book and
    try all these variations to draw different pictures.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以发现，某些词可能应该被去除（例如，“I”），因为它们并不提供太多信息，它们是停用词。在构建主题模型时，过滤停用词是非常有用的，否则，你可能会得到一个完全由停用词组成的主题。我们也可能希望对文本进行预处理，提取词干，以标准化复数形式和动词形式。这个过程在上一章已经讲过，您可以参考那一章获取详细信息。如果你有兴趣，可以从本书的配套网站下载代码，尝试这些不同的变体来绘制不同的图像。
- en: Note
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Building a word cloud like the previous one can be done with several different
    pieces of software. For the graphics in this chapter, we used a Python-based tool
    called pytagcloud. This package requires a few dependencies to install and is
    not central to machine learning, so we won't consider it in the main text; however,
    we have all of the code available in the online code repository to generate the
    figures in this chapter.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 构建像前面那样的词云可以通过几种不同的软件完成。对于本章中的图形，我们使用了一个基于Python的工具叫做pytagcloud。这个包需要安装一些依赖项，并且与机器学习没有直接关系，因此我们在正文中不会讨论它；然而，我们将所有的代码都放在了在线代码库中，供大家生成本章中的图形。
- en: Comparing documents by topics
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按主题比较文档
- en: Topics can be useful on their own to build the sort of small vignettes with
    words that are shown in the previous screenshot. These visualizations can be used
    to navigate a large collection of documents. For example, a website can display
    the different topics as different word clouds, allowing a user to click through
    to the documents. In fact, they have been used in just this way to analyze large
    collections of documents.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 主题本身就可以用来构建像前面截图中所展示的那种小型文字片段。这些可视化可以用来浏览大量文档。例如，一个网站可以展示不同的主题作为不同的词云，让用户点击以进入相关文档。事实上，词云就是以这种方式被用来分析大量文档的。
- en: However, topics are often just an intermediate tool to another end. Now that
    we have an estimate for each document of how much of that document comes from
    each topic, we can compare the documents in topic space. This simply means that
    instead of comparing word to word, we say that two documents are similar if they
    talk about the same topics.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，主题通常只是通向另一个目标的中间工具。现在我们已经估算了每个文档中每个主题的占比，我们可以在主题空间中比较文档。这意味着，和逐字比较不同，我们认为两篇文档如果讨论的是相同的主题，那么它们就相似。
- en: This can be very powerful as two text documents that share few words may actually
    refer to the same topic! They may just refer to it using different constructions
    (for example, one document may read "the President of the United States" while
    the other will use the name "Barack Obama").
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常强大，因为两篇共享很少相同单词的文档，实际上可能指的是相同的主题！它们可能只是用不同的表达方式提到相同的主题（例如，一篇文档可能写的是“美国总统”，而另一篇则用“巴拉克·奥巴马”）。
- en: Note
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Topic models are good on their own to build visualizations and explore data.
    They are also very useful as an intermediate step in many other tasks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型本身就可以用来构建可视化并探索数据。它们也作为许多其他任务中的中间步骤非常有用。
- en: At this point, we can redo the exercise we performed in the last chapter and
    look for the most similar post to an input query, by using the topics to define
    similarity. Whereas, earlier we compared two documents by comparing their word
    vectors directly, we can now compare two documents by comparing their topic vectors.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们可以重新进行上一章中的练习，通过使用主题来定义相似性，查找与输入查询最相似的帖子。此前，我们是通过直接比较文档的词向量来进行比较，现在我们可以通过比较它们的主题向量来进行比较。
- en: For this, we are going to project the documents to the topic space. That is,
    we want to have a vector of topics that summarize the document. How to perform
    these types of **dimensionality reduction** in general is an important task in
    itself and we have a chapter entirely devoted to this task. For the moment, we
    just show how topic models can be used for exactly this purpose; once topics have
    been computed for each document, we can perform operations on its topic vector
    and forget about the original words. If the topics are meaningful, they will be
    potentially more informative than the raw words. Additionally, this may bring
    computational advantages, as it is much faster to compare 100 vectors of topic
    weights than vectors of the size of the vocabulary (which will contain thousands
    of terms).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将把文档投影到主题空间。也就是说，我们希望得到一个主题向量，用来总结文档。如何执行这些类型的**降维**通常是一个重要任务，我们有一个专门的章节来讨论这个任务。暂时，我们只展示如何使用主题模型来完成这一任务；一旦为每个文档计算出主题，我们可以在其主题向量上执行操作，而不再考虑原始单词。如果主题有意义，它们可能比原始单词更具信息性。此外，这样做还可能带来计算上的优势，因为比较100个主题权重向量要比比较包含数千个术语的词汇向量更快。
- en: 'Using gensim, we have seen earlier how to compute the topics corresponding
    to all the documents in the corpus. We will now compute these for all the documents
    and store it in a NumPy arrays and compute all pairwise distances:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用gensim，我们已经看到如何计算语料库中所有文档对应的主题。现在，我们将为所有文档计算这些主题，并将其存储在NumPy数组中，然后计算所有成对距离：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, `topics` is a matrix of topics. We can use the `pdist` function in SciPy
    to compute all pairwise distances. That is, with a single function call, we compute
    all the values of `sum((topics[ti] – topics[tj])**2)`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`topics`是一个主题矩阵。我们可以使用SciPy中的`pdist`函数来计算所有的成对距离。也就是说，通过一次函数调用，我们可以计算出所有`sum((topics[ti]
    – topics[tj])**2)`的值：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we will employ one last little trick; we will set the diagonal elements
    of the `distance` matrix to a high value (it just needs to be larger than the
    other values in the matrix):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用最后一个小技巧；我们将`distance`矩阵的对角元素设置为一个较大的值（它只需要大于矩阵中其他值即可）：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And we are done! For each document, we can look up the closest element easily
    (this is a type of nearest neighbor classifier):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了！对于每个文档，我们可以轻松查找最相似的元素（这是一种邻近分类器）：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note that this will not work if we had not set the diagonal elements to a large
    value: the function will always return the same element as it is the one most
    similar to itself (except in the weird case where two elements had exactly the
    same topic distribution, which is very rare unless they are exactly the same).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果我们没有将对角元素设置为较大值，这将不起作用：该函数将始终返回相同的元素，因为它与自己最为相似（除非出现非常罕见的情况，即两个元素的主题分布完全相同，通常只有在它们完全相同的情况下才会发生）。
- en: 'For example, here is one possible query document (it is the second document
    in our collection):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，下面是一个可能的查询文档（它是我们集合中的第二个文档）：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If we ask for the most similar document to `closest_to(1)`, we receive the
    following document as a result:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们请求与`closest_to(1)`最相似的文档，我们会得到以下文档作为结果：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The system returns a post by the same author discussing medications.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 系统返回了同一作者讨论药物的帖子。
- en: Modeling the whole of Wikipedia
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建模整个维基百科
- en: While the initial LDA implementations can be slow, which limited their use to
    small document collections, modern algorithms work well with very large collections
    of data. Following the documentation of gensim, we are going to build a topic
    model for the whole of the English-language Wikipedia. This takes hours, but can
    be done even with just a laptop! With a cluster of machines, we can make it go
    much faster, but we will look at that sort of processing environment in a later
    chapter.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然最初的LDA实现可能比较慢，限制了它们在小型文档集合中的使用，但现代算法在处理非常大的数据集时表现良好。根据gensim的文档，我们将为整个英文维基百科构建一个主题模型。这需要几个小时，但即使是笔记本电脑也能完成！如果使用集群计算机，我们可以大大加快速度，不过这类处理环境将在后续章节中讨论。
- en: 'First, we download the whole Wikipedia dump from [http://dumps.wikimedia.org](http://dumps.wikimedia.org).
    This is a large file (currently over 10 GB), so it may take a while, unless your
    Internet connection is very fast. Then, we will index it with a gensim tool:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从[http://dumps.wikimedia.org](http://dumps.wikimedia.org)下载整个维基百科的数据库文件。这个文件很大（目前超过10
    GB），因此可能需要一些时间，除非你的互联网连接非常快。然后，我们将使用gensim工具对其进行索引：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Run the previous line on the command shell, not on the Python shell. After
    a few hours, the index will be saved in the same directory. At this point, we
    can build the final topic model. This process looks exactly like what we did for
    the small AP dataset. We first import a few packages:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请在命令行中运行上一行，而不是在 Python shell 中运行。几个小时后，索引将保存在同一目录中。此时，我们可以构建最终的话题模型。这个过程与我们之前在小型
    AP 数据集上的操作完全相同。我们首先导入一些包：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we set up logging, using the standard Python logging module (which gensim
    uses to print out status messages). This step is not strictly necessary, but it
    is nice to have a little more output to know what is happening:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们设置日志记录，使用标准的 Python 日志模块（gensim 用于打印状态消息）。这一步并非严格必要，但有更多的输出可以帮助我们了解发生了什么：
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now we load the preprocessed data:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们加载预处理后的数据：
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we build the LDA model as we did earlier:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们像之前一样构建 LDA 模型：
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This will again take a couple of hours. You will see the progress on your console,
    which can give you an indication of how long you still have to wait.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这将再次花费几个小时。你将在控制台上看到进度，这可以给你一个大致的等待时间。
- en: 'Once it is done, we can save the topic model to a file, so we don''t have to
    redo it:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，我们可以将话题模型保存到文件中，这样就不必重新执行它：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If you exit your session and come back later, you can load the model again
    using the following command (after the appropriate imports, naturally):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你退出会话并稍后再回来，你可以使用以下命令重新加载模型（自然地，首先要进行适当的导入）：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `model` object can be used to explore the collection of documents, and build
    the `topics` matrix as we did earlier.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`model` 对象可用于探索文档集合，并像我们之前一样构建 `topics` 矩阵。'
- en: 'We can see that this is still a sparse model even if we have many more documents
    than we had earlier (over 4 million as we are writing this):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，即使我们拥有比之前更多的文档（目前超过 400 万），这仍然是一个稀疏模型：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: So, the average document mentions 6.4 topics and 94 percent of them mention
    10 or fewer topics.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，平均每个文档提到了 6.4 个话题，其中 94% 的文档提到 10 个或更少的话题。
- en: 'We can ask what the most talked about topic in Wikipedia is. We will first
    compute the total weight for each topic (by summing up the weights from all the
    documents) and then retrieve the words corresponding to the most highly weighted
    topic. This is performed using the following code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以询问维基百科中最常被提及的话题是什么。我们将首先计算每个话题的总权重（通过将所有文档中的权重加总），然后检索与最具权重话题相关的词语。此操作使用以下代码执行：
- en: '[PRE20]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Using the same tools as we did earlier to build up a visualization, we can
    see that the most talked about topic is related to music and is a very coherent
    topic. A full 18 percent of Wikipedia pages are partially related to this topic
    (5.5 percent of all the words in Wikipedia are assigned to this topic). Take a
    look at the following screenshot:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与之前相同的工具构建可视化，我们可以看到最常提及的话题与音乐相关，并且是一个非常连贯的话题。18% 的维基百科页面与这个话题部分相关（维基百科中 5.5%
    的词汇分配给了这个话题）。看看下面的截图：
- en: '![Modeling the whole of Wikipedia](img/2772OS_04_04.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![建模整个维基百科](img/2772OS_04_04.jpg)'
- en: Note
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: These plots and numbers were obtained when the book was being written. As Wikipedia
    keeps changing, your results will be different. We expect that the trends will
    be similar, but the details may vary.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图表和数据是在书籍编写时获得的。由于维基百科在不断变化，你的结果可能会有所不同。我们预计趋势会相似，但细节可能会有所不同。
- en: 'Alternatively, we can look at the least talked about topic:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以查看最少被提及的话题：
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Modeling the whole of Wikipedia](img/2772OS_04_05.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![建模整个维基百科](img/2772OS_04_05.jpg)'
- en: The least talked about topic is harder to interpret, but many of its top words
    refer to airports in eastern countries. Just 1.6 percent of documents touch upon
    it, and it represents just 0.1 percent of the words.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最少被提及的话题较难解释，但它的许多高频词与东部国家的机场有关。只有 1.6% 的文档涉及到它，它仅占 0.1% 的词汇。
- en: Choosing the number of topics
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择话题的数量
- en: So far in the chapter, we have used a fixed number of topics for our analyses,
    namely 100\. This was a purely arbitrary number, we could have just as well used
    either 20 or 200 topics. Fortunately, for many uses, this number does not really
    matter. If you are going to only use the topics as an intermediate step, as we
    did previously when finding similar posts, the final behavior of the system is
    rarely very sensitive to the exact number of topics used in the model. This means
    that as long as you use enough topics, whether you use 100 topics or 200, the
    recommendations that result from the process will not be very different; 100 is
    often a good enough number (while 20 is too few for a general collection of text
    documents). The same is true of setting the `alpha` value. While playing around
    with it can change the topics, the final results are again robust against this
    change.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们使用了固定数量的主题进行分析，即100个。这是一个完全任意的数字，我们也可以选择使用20个或200个主题。幸运的是，对于许多应用场景来说，这个数字其实并不重要。如果你只是将这些主题作为一个中间步骤，正如我们之前在查找相似帖子时所做的那样，模型中使用的具体主题数量通常对系统的最终行为影响不大。这意味着，只要使用足够数量的主题，无论是100个主题还是200个主题，从该过程中得出的推荐结果不会有太大差异；100个主题通常已经足够（而20个主题对于一般的文本文档集合来说太少）。设置`alpha`值也是如此。虽然调整它会改变主题，但最终结果对这种变化具有较强的鲁棒性。
- en: Tip
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Topic modeling is often an end towards a goal. In that case, it is not always
    very important exactly which parameter values are used. A different number of
    topics or values for parameters such as `alpha` will result in systems whose end
    results are almost identical in their final results.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模通常是为达成某个目标。这样一来，具体使用哪些参数值并不总是特别重要。不同的主题数量或`alpha`等参数值将导致系统的最终结果几乎完全相同。
- en: On the other hand, if you are going to explore the topics directly, or build
    a visualization tool that exposes them, you should probably try a few values and
    see which gives you the most useful or most appealing results.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你打算直接探索主题，或者构建一个能够展示主题的可视化工具，那么你应该尝试不同的值，看看哪个值能为你提供最有用或最吸引人的结果。
- en: Alternatively, there are a few methods that will automatically determine the
    number of topics for you, depending on the dataset. One popular model is called
    the **hierarchical Dirichlet process**. Again, the full mathematical model behind
    it is complex and beyond the scope of this book. However, the fable we can tell
    is that instead of having the topics fixed first as in the LDA generative story,
    the topics themselves were generated along with the data, one at a time. Whenever
    the writer starts a new document, they have the option of using the topics that
    already exist or to create a completely new one. When more topics have already
    been created, the probability of creating a new one, instead of reusing what exists
    goes down, but the possibility always exists.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，有一些方法可以根据数据集自动确定主题的数量。一个流行的模型叫做**层次狄利克雷过程**。同样，这背后的完整数学模型很复杂，超出了本书的讨论范围。不过，我们可以讲一个简化的故事：与LDA生成模型中预先固定主题的做法不同，层次狄利克雷过程中的主题是随着数据逐一生成的。每当作者开始写一篇新文档时，他们可以选择使用已有的主题，或者创建一个全新的主题。当已经创建了更多主题时，创建新主题的概率会下降，因为更倾向于复用已有的主题，但这种可能性始终存在。
- en: 'This means *that the more documents we have, the more topics we will end up
    with*. This is one of those statements that is unintuitive at first but makes
    perfect sense upon reflection. We are grouping documents and the more examples
    we have, the more we can break them up. If we only have a few examples of news
    articles, then "Sports" will be a topic. However, as we have more, we start to
    break it up into the individual modalities: "Hockey", "Soccer", and so on. As
    we have even more data, we can start to tell nuances apart, articles about individual
    teams and even individual players. The same is true for people. In a group of
    many different backgrounds, with a few "computer people", you might put them together;
    in a slightly larger group, you will have separate gatherings for programmers
    and systems administrators; and in the real-world, we even have different gatherings
    for Python and Ruby programmers.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着*我们拥有的文档越多，最终得到的主题也会越多*。这是一个起初难以理解的陈述，但经过反思后，它完全合理。我们是在将文档分组，文档越多，我们能够划分得越细。如果我们只有少数新闻文章的例子，那么“体育”可能就是一个主题。然而，当我们有更多的文章时，我们开始将其拆分成不同的子类别：“冰球”、“足球”，等等。随着数据量的增多，我们甚至能区分细微的差别，像是关于特定球队或球员的文章。对人群也是如此。在一个背景差异较大的群体中，如果有几个“计算机人士”，你可能会将他们放在一起；如果是稍微大一点的群体，你会把程序员和系统管理员分开；而在现实世界中，我们甚至有不同的聚会，专门为Python和Ruby程序员提供。
- en: 'The **hierarchical Dirichlet process** (**HDP**) is available in gensim. Using
    it is trivial. To adapt the code we wrote for LDA, we just need to replace the
    call to `gensim.models.ldamodel.LdaModel` with a call to the `HdpModel` constructor
    as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**层次狄利克雷过程**（**HDP**）在gensim中可用。使用它非常简单。为了适应我们为LDA编写的代码，我们只需将对`gensim.models.ldamodel.LdaModel`的调用替换为对`HdpModel`构造函数的调用，代码如下：'
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: That's it (except that it takes a bit longer to compute—there are no free lunches).
    Now, we can use this model in much the same way as we used the LDA model, except
    that we did not need to specify the number of topics.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样（不过它需要更长的计算时间——没有免费的午餐）。现在，我们可以像使用LDA模型一样使用这个模型，区别在于我们不需要指定主题的数量。
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed topic modeling. Topic modeling is more flexible
    than clustering as these methods allow each document to be partially present in
    more than one group. To explore these methods, we used a new package, gensim.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了主题建模。主题建模比聚类更灵活，因为这些方法允许每个文档部分地存在于多个组中。为了探索这些方法，我们使用了一个新包——gensim。
- en: Topic modeling was first developed and is easier to understand in the case of
    text, but in the computer vision chapter we will see how some of these techniques
    may be applied to images as well. Topic models are very important in modern computer
    vision research. In fact, unlike the previous chapters, this chapter was very
    close to the cutting edge of research in machine learning algorithms. The original
    LDA algorithm was published in a scientific journal in 2003, but the method that
    gensim uses to be able to handle Wikipedia was only developed in 2010 and the
    HDP algorithm is from 2011\. The research continues and you can find many variations
    and models with wonderful names such as *the Indian buffet process* (not to be
    confused with the *Chinese restaurant process*, which is a different model), or
    *Pachinko allocation* (Pachinko being a type of Japanese game, a cross between
    a slot-machine and pinball).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模最初是在文本情况下开发的，并且更容易理解，但在计算机视觉一章中，我们将看到这些技术如何也能应用于图像。主题模型在现代计算机视觉研究中非常重要。事实上，与前几章不同，本章非常接近机器学习算法研究的前沿。原始的LDA算法发表于2003年的科学期刊，但gensim处理维基百科的能力是在2010年才开发出来的，而HDP算法则来自2011年。研究仍在继续，你可以找到许多变种和模型，它们有着一些很有趣的名字，比如*印度自助餐过程*（不要与*中国餐馆过程*混淆，后者是一个不同的模型），或者*八金球分配*（八金球是一种日本游戏，介于老虎机和弹球之间）。
- en: 'We have now gone through some of the major machine learning modes: classification,
    clustering, and topic modeling.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经走过了一些主要的机器学习模式：分类、聚类和主题建模。
- en: In the next chapter, we go back to classification, but this time, we will be
    exploring advanced algorithms and approaches.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将回到分类问题，但这次我们将探索高级算法和方法。
