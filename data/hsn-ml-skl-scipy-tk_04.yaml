- en: Making Decisions with Linear Equations
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 使用线性方程做决策
- en: The method of least squares regression analysis dates back to the time of Carl
    Friedrich Gauss in the 18^(th) century. For over two centuries, many algorithms
    have been built on top of it or have been inspired by it in some form. These linear
    models are possibly the most commonly used algorithms today for both regression
    and classification. We will start this chapter by looking at the basic least squares
    algorithm, then we will move on to more advanced algorithms as the chapter progresses.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 最小二乘回归分析方法可以追溯到18世纪卡尔·弗里德里希·高斯的时代。两个多世纪以来，许多算法基于它或在某种形式上受到它的启发。这些线性模型可能是今天回归和分类中最常用的算法。我们将从本章开始，首先看一下基本的最小二乘算法，然后随着章节的深入，我们将介绍更高级的算法。
- en: 'Here is a list of the topics covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章涵盖的主题列表：
- en: Understanding linear models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解线性模型
- en: Predicting house prices in Boston
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测波士顿的房价
- en: Regularizing the regressor
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对回归器进行正则化
- en: Finding regression intervals
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找回归区间
- en: Additional linear regressors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 额外的线性回归器
- en: Using logistic regression for classification
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用逻辑回归进行分类
- en: Additional linear classifiers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 额外的线性分类器
- en: Understanding linear models
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解线性模型
- en: To be able to explain linear models well, I would like to start with an example
    where the solution can be found using a system of linear equations—a technique
    we all learned in school when we were around 12 years old. We will then see why
    this technique doesn't always work with real-life problems, and so a linear regression
    model is needed. Then, we will apply the regression model to a real-life regression
    problem and learn how to improve our solution along the way.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够很好地解释线性模型，我想从一个例子开始，在这个例子中，解决方案可以通过线性方程组来求解——这是我们在12岁左右上学时学到的一项技术。然后，我们将看到为什么这种技术并不总是适用于现实生活中的问题，因此需要线性回归模型。接着，我们将把回归模型应用于一个现实中的回归问题，并在此过程中学习如何改进我们的解决方案。
- en: Linear equations
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性方程
- en: '"Mathematics is the most beautiful and most powerful creation of the human
    spirit."'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '"数学是人类精神最美丽和最强大的创造。"'
- en: – Stefan Banach
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: – 斯特凡·巴纳赫
- en: 'In this example, we have five passengers who have taken a taxi trip. Here,
    we have a record of the distance each taxi covered in kilometers and the fair
    displayed on its meter at the end of each trip:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们有五个乘客，他们乘坐了出租车旅行。这里记录了每辆出租车行驶的距离（以公里为单位）以及每次旅行结束时计价器上显示的费用：
- en: '![](img/c515257d-2dbc-4faa-b78a-7ec497bd5bb9.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c515257d-2dbc-4faa-b78a-7ec497bd5bb9.png)'
- en: 'We know that taxi meters usually start with a certain amount and then they
    add a fixed charge for each kilometer traveled. We can model the meter using the
    following equation:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，出租车计价器通常会从一定的起始费用开始，然后根据每公里的行驶距离收取固定费用。我们可以用以下方程来建模计价器：
- en: '![](img/c2a865a5-da87-469e-84c3-a3412c7f67fd.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2a865a5-da87-469e-84c3-a3412c7f67fd.png)'
- en: 'Here, *A* is the meter''s starting value and *B* is the charge added per kilometer.
    We also know that with two unknowns—*A* and *B*—we just need two data samples
    to figure out that *A* is `5` and *B* is `2.5`. We can also plot the formula with
    the values for *A* and *B*, as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*A*是计价器的起始值，*B*是每公里增加的费用。我们还知道，对于两个未知数——*A*和*B*——我们只需要两个数据样本就可以确定*A*是`5`，*B*是`2.5`。我们还可以用*A*和*B*的值绘制公式，如下所示：
- en: '![](img/284965ec-b7d8-4136-9c86-c73ed11ee969.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/284965ec-b7d8-4136-9c86-c73ed11ee969.png)'
- en: We also know that the blue line will meet the *y*-axis at the value of *A* (`5`).
    So, we call *A* the **intercept**. We also know that the slope of the line equals
    *B* (`2.5`).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还知道，蓝线会在*y*轴上与*A*（`5`）相交。因此，我们将*A*称为**截距**。我们还知道，直线的斜率等于*B*（`2.5`）。
- en: 'The passengers didn''t always have change, so they sometimes rounded up the
    amount shown on the meter to add a tip for the driver. Here is the data for the
    amount each passenger ended up paying:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 乘客们并不总是带有零钱，所以他们有时会将计价器上显示的金额四舍五入，加上小费给司机。这是每位乘客最终支付的金额数据：
- en: '![](img/67a8b31e-d936-4d2a-8237-a3561d53129b.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67a8b31e-d936-4d2a-8237-a3561d53129b.png)'
- en: 'After we add the tips, it''s clear that the relationship between the distance
    traveled and the amount paid is no longer linear. The plot on the right-hand side
    shows that a straight line cannot be drawn to capture this relationship:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们加入小费后，很明显，行驶距离与支付金额之间的关系不再是线性的。右侧的图表显示，无法通过一条直线来捕捉这种关系：
- en: '![](img/95527f92-ecc4-40f7-ab82-cdb9aae56698.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95527f92-ecc4-40f7-ab82-cdb9aae56698.png)'
- en: We now know that our usual method of solving equationswill not work this time.
    Nevertheless, we can tell that there is still a line that can somewhat approximate
    this relationship. In the next section, we will use a linear regression algorithm
    to find this approximation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道，之前的解方程方法在此时不再适用。然而，我们可以看出，仍然存在一条线，能够在某种程度上近似这个关系。在接下来的部分，我们将使用线性回归算法来找到这个近似值。
- en: Linear regression
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归
- en: 'Algorithms are all about objectives. Our objective earlier was to find a single
    line that goes through all the points in the graph. We have seen that this objective
    is not feasible if a linear relationship does not exist between the points. Therefore,
    we will use the linear regression algorithm since it has a different objective.
    The linear regression algorithm tries to find a line where the mean of the squared
    errors between the estimated points on the line and the actual points is minimal.
    Visually speaking, in the following graph, we want a dotted line that makes the
    average squared lengths of the vertical lines minimal:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的核心是目标。我们之前的目标是找到一条通过图中所有点的直线。我们已经看到，如果这些点之间不存在线性关系，那么这个目标是无法实现的。因此，我们将使用线性回归算法，因为它有不同的目标。线性回归算法试图找到一条线，使得估计点与实际点之间的平方误差的均值最小。从视觉上看，在下面的图中，我们希望找到一条虚线，使得所有垂直线的平方长度的平均值最小：
- en: '![](img/7cc116de-9a1e-436d-b53d-8a1da2aafe7f.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7cc116de-9a1e-436d-b53d-8a1da2aafe7f.png)'
- en: The method used here to find a line that minimizes the **Mean Squared Error**
    (**MSE**) is known as ordinary least squares. Often, linear regression just means
    ordinary least squares. Nevertheless, throughout this chapter, I will be using
    the term `LinearRegression` (as a single word) to refer to scikit-learn's implementation
    of ordinary least squares, and I will reserve the term *linear regression* (as
    two separate words) for referring to the general concept of linear regression,
    whether the ordinary least squares method is used or a different method is being
    employed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里用来找到一条最小化**均方误差**（**MSE**）的线性回归方法被称为普通最小二乘法。通常，线性回归就意味着普通最小二乘法。然而，在本章中，我将使用`LinearRegression`（作为一个词）来指代scikit-learn实现的普通最小二乘法，而将*线性回归*（作为两个词）保留用来指代线性回归的通用概念，无论是使用普通最小二乘法方法还是其他方法。
- en: The method of ordinary least squares is about two centuries old and it uses
    simple mathematics to estimate the parameters. That's why some may argue that
    this algorithm is not actually a machine learning one. Personally, I follow a
    more liberal approach when categorizing what is machine learning and what is not.
    As long as the algorithm automatically learns from data and we use that data to
    evaluate it, then for me, it falls within the machine learning paradigm.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 普通最小二乘法方法已有两个世纪的历史，它使用简单的数学来估算参数。这也是为什么一些人认为这个算法实际上不是机器学习算法的原因。就个人而言，我在分类什么是机器学习、什么不是时，采取了更加宽松的方式。只要算法能从数据中自动学习，并且我们用这些数据来评估它，那么在我看来，它就属于机器学习范畴。
- en: Estimating the amount paid to the taxi driver
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 估算支付给出租车司机的金额
- en: Now that we know how linear regression works, let's take a look at how to estimate
    the amount paid to the taxi driver.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了线性回归的工作原理，接下来让我们看看如何估算支付给出租车司机的金额。
- en: 'Let''s use scikit-learn to build a regression model to estimate the amount
    paid to the taxi driver:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用scikit-learn构建一个回归模型来估算支付给出租车司机的金额：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Clearly, scikit-learn has a consistent interface. We have used the same `fit()`
    and `predict()` methods as in the previous chapter, but this time with the `LinearRegression`
    object.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，scikit-learn具有一致的接口。我们使用了与前一章节相同的`fit()`和`predict()`方法，只不过这次使用的是`LinearRegression`对象。
- en: We only have one feature this time, `Kilometres`; nevertheless, the `fit()`
    and `predict()` methods expect a two-dimensional `ax`, which is why we enclosed
    `Kilometers` in an extra set of square brackets—`df_taxi[['Kilometres']]`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们只有一个特征`Kilometres`，然而`fit()`和`predict()`方法期望的是一个二维的`ax`，这就是为什么我们将`Kilometres`放入了一个额外的方括号中——`df_taxi[['Kilometres']]`。
- en: 'We put our predictions in the same DataFrame under `Paid (Predicted)`. We can
    then plot the actual values versus the estimated ones using the following code:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将预测结果放在同一个数据框架中的`Paid (Predicted)`列下。然后，我们可以使用以下代码绘制实际值与估算值的对比图：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'I cut out the formatting parts of the code to keep it short and to the point.
    Here is the final result:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我删去了代码中的格式部分，以保持简洁和直接。以下是最终结果：
- en: '![](img/42d4da52-87fe-41c7-98ba-f1b78e138679.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/42d4da52-87fe-41c7-98ba-f1b78e138679.png)'
- en: 'Once a linear model is trained, you can get its intercept and coefficients
    using the `intercept_` and `coef_` parameters. So, we can use the following code
    snippet to create the linear equations of the estimated line:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦线性模型训练完成，您可以使用`intercept_`和`coef_`参数来获取其截距和系数。因此，我们可以使用以下代码片段来创建估计直线的线性方程：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following equation is then printed:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后打印出以下方程：
- en: '![](img/98c62d28-a9c5-4f5f-b481-c4fcdb8cc961.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98c62d28-a9c5-4f5f-b481-c4fcdb8cc961.png)'
- en: Getting the parameters for the linear equation can be handy in cases where you
    want to build a model in scikit-learn and then use it in another language or even
    in your favorite spreadsheet software. Knowing the coefficient also helps us understand
    why the model made certain decisions. More on this later in this chapter.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 获取线性方程的参数在某些情况下非常有用，尤其是当您想要在scikit-learn中构建一个模型，然后在其他语言中使用它，甚至是在您最喜欢的电子表格软件中使用它时。了解系数还有助于我们理解模型为什么做出某些决策。更多内容将在本章后面详细讨论。
- en: In software, the input to functions and methods is referred to as parameters.
    In machine learning, the weights learned for a model are also referred to as parameters.
    When setting a model, we pass its configuration to its `__init__` method. Thus,
    to prevent any confusion, the model's configurations are called hyperparameters.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件中，函数和方法的输入被称为参数。在机器学习中，模型学习到的权重也被称为参数。在设置模型时，我们将其配置传递给`__init__`方法。因此，为了避免任何混淆，模型的配置被称为超参数。
- en: Predicting house prices in Boston
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测波士顿的房价
- en: Now that we understand how linear regression works, let's move on to looking
    at a real dataset where we can demonstrate a more practical use case.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了线性回归的工作原理，接下来我们将研究一个真实的数据集，展示一个更实际的用例。
- en: 'The Boston dataset is a small set representing the house prices in the city
    of Boston. It contains 506 samples and 13 features. Let''s load the data into
    a DataFrame, as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 波士顿数据集是一个小型数据集，表示波士顿市的房价。它包含506个样本和13个特征。我们可以将数据加载到一个DataFrame中，如下所示：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Data exploration
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据探索
- en: 'It''s important to make sure you do not have any null values in your data;
    otherwise, scikit-learn will complain about it. Here, I will count the sum of
    the null values in each column, then take the sum of it. If I get `0`, then I
    am a happy man:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 确保数据中没有任何空值非常重要；否则，scikit-learn会报错。在这里，我将统计每一列中的空值总和，然后对其求和。如果得到的是`0`，那么我就会很高兴：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For a regression problem, the most important thing to do is to understand the
    distribution of your target. If a target ranges between `1` and `10`, and after
    training our model we get a mean absolute error of `5`, we can tell that the error
    is large in this context.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，最重要的是理解目标变量的分布。如果目标变量的范围在`1`到`10`之间，而我们训练模型后得到的平均绝对误差为`5`，那么在这个情况下，我们可以判断误差较大。
- en: 'However, the same error for a target that ranges between `500,000` and `1,000,000`
    is negligible. Histograms are your friend when you want to visualize distributions.
    In addition to the target''s distribution, let''s also plot the mean values for
    each feature:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于一个目标值在`500,000`到`1,000,000`之间的情况，相同的误差是可以忽略不计的。当您想要可视化分布时，直方图是您的好帮手。除了目标的分布，我们还可以绘制每个特征的均值：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This gives us the following graphs:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了以下图表：
- en: '![](img/0268778e-62bf-4ccf-abb4-b737d7f6f4f3.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0268778e-62bf-4ccf-abb4-b737d7f6f4f3.png)'
- en: 'In the preceding graph, it is observed that:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，我们观察到：
- en: The prices range between `5` and `50`. Obviously, these are not real prices,
    probably normalized values, but this doesn't matter for now.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 价格范围在`5`到`50`之间。显然，这些并非真实价格，可能是归一化后的值，但现在这并不重要。
- en: 'Furthermore, we can tell from the histogram that most of the prices are below
    `35`. We can use the following code snippet to see that 90% of the prices are
    below `34.8`:'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，从直方图中我们可以看出，大多数价格都低于`35`。我们可以使用以下代码片段，看到90%的价格都低于`34.8`：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can always go deeper with your data exploration, but we will stop here on
    this occasion.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以始终深入进行数据探索，但这次我们就到此为止。
- en: Splitting the data
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据划分
- en: 'When it comes to small datasets, it''s advised that you allocate enough data
    for testing. So, we will split our data into 60% for training and 40% for testing
    using the `train_test_split` function:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于小型数据集，建议为测试预留足够的数据。因此，我们将数据划分为60%的训练数据和40%的测试数据，使用`train_test_split`函数：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Once you have the training and test sets, split them further into *x* sets and
    *y* sets. Then, we are ready to move to the next step.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拥有了训练集和测试集，就将它们进一步拆分为*x*集和*y*集。然后，我们就可以进入下一步。
- en: Calculating a baseline
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算基准
- en: The distribution of the target gave us an idea of what level of error we can
    tolerate. Nevertheless, it is always useful to compare our final model to something.
    If we were in the real estate business and human agents were used to estimate
    house prices, then we would most likely be expected to build a model that can
    do better than the human agents. Nevertheless, since we do not know any real estimations
    to compare our model to, we can come up with our own baseline instead. The mean
    house price is `22.5`. If we build a dummy model that returns the mean price regardless
    of the data given to it, then it would make a reasonable baseline.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 目标值的分布让我们了解了我们能容忍的误差水平。然而，比较我们的最终模型与某些基准总是有用的。如果我们从事房地产行业，并且由人类代理估算房价，那么我们很可能会被期望建立一个比人类代理更准确的模型。然而，由于我们无法获得实际估算值来与我们的模型进行比较，因此我们可以自己提出一个基准。房屋的均价是`22.5`。如果我们建立一个虚拟模型，无论输入什么数据都返回均价，那么它就会成为一个合理的基准。
- en: Keep in mind that the value of `22.5` is calculated for the entire dataset,
    but since we are pretending to only have access to the training data, then it
    makes sense to calculate the mean price for the training set only. To save us
    all this effort, scikit-learn has dummy regressors available that do all this
    work for us.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`22.5`的值是针对整个数据集计算的，但因为我们假装只能访问训练数据，所以只计算训练集的均值是有意义的。为了节省我们的精力，scikit-learn
    提供了虚拟回归器，可以为我们完成所有这些工作。
- en: 'Here, we will create a dummy regressor and use it to calculate baseline predictions
    for the test set:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将创建一个虚拟回归器，并用它来计算测试集的基准预测值：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There are other strategies that we can use, such as finding the median (the
    50^(th) quantile) or any other *N*^(th) quantile. Keep in mind that for the same
    data, using the mean as an estimation gives a lower MSE compared to when the median
    is used. Conversely, the median gives a lower **Mean Absolute Error** (**MAE**).
    We want our model to beat the baseline for both the MAE and MSE.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用其他策略，比如找到中位数（第50^(th) 分位数）或任何其他*N*^(th) 分位数。请记住，对于相同的数据，使用均值作为估算值相比于使用中位数时，会得到更低的均方误差（MSE）。相反，中位数会得到更低的**平均绝对误差**（**MAE**）。我们希望我们的模型在MAE和MSE两方面都能超越基准。
- en: Training the linear regressor
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练线性回归器
- en: 'Isn''t the code for the baseline model almost identical to the one for the
    actual models? That''s the beauty of scikit-learn''s API. It means that when we
    decide to try a different algorithm—say, the decision tree algorithm from the
    previous chapter—we only need to change a few lines of code. Anyway, here is the
    code for the linear regressor:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 基准模型的代码和实际模型几乎一模一样，不是吗？这就是scikit-learn API的优点。意味着当我们决定尝试不同的算法，比如上一章的决策树算法时，我们只需要更改几行代码。无论如何，下面是线性回归器的代码：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We are going to stick to the default configuration for now.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们暂时会坚持默认配置。
- en: Evaluating our model's accuracy
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型的准确性
- en: 'There are three commonly used metrics for regression: *R²*, *MAE*, and *MSE*.
    Let''s first write the code that calculates the three metrics and prints the results:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归中，有三种常用的指标：*R²*、*MAE*和*MSE*。首先让我们编写计算这三个指标并打印结果的代码：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here are the results we get:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们得到的结果：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'By now, you should already know how *MAE* and *MSE* are calculated. Just keep
    in mind that *MSE* is more sensitive to outliers than *MAE*. That''s why the mean
    estimations for the baseline scored badly there. As for the *R**²*, let''s look
    at its formula:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该已经知道如何计算*MAE*和*MSE*了。只需要记住，*MSE*比*MAE*对异常值更敏感。这就是为什么基准的均值估算得分较差的原因。至于*R²*，让我们看一下它的公式：
- en: '![](img/f27ffaad-70dd-4596-85cd-172325776462.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f27ffaad-70dd-4596-85cd-172325776462.png)'
- en: 'Here''s an explanation of the preceding formula:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是前面公式的解释：
- en: The numerator probably reminds you of *MSE*. We basically calculate the squared
    differences between all the predicted values and their corresponding actual values.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分子可能让你想起了*MSE*。我们基本上计算所有预测值与对应实际值之间的平方差。
- en: As for the denominator, we use the mean of the actual values as pseudo estimations.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至于分母，我们使用实际值的均值作为伪估算值。
- en: Basically, this metric tells us how much better our predictions are compared
    to using the target's mean as an estimation.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本上，这个指标告诉我们，和使用目标均值作为估算值相比，我们的预测有多么准确。
- en: An R² score of `1` is the best we could get, and a score of `0` means that we
    offered no additional value in comparison to using a biased model that just relies
    on the mean as an estimation.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`的R²分数是我们能得到的最佳结果，`0`的分数意味着我们与一个仅依赖均值作为估计的有偏模型相比没有提供任何附加价值。'
- en: A negative score means that we should throw our model in the trash and use the
    target's mean instead.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个负分数意味着我们应该把模型扔进垃圾桶，改用目标的均值作为预测。
- en: Obviously, in the baseline model, we already used the target's mean as the prediction.
    That's why its R² score is `0`.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显然，在基线模型中，我们已经使用目标的均值作为预测。因此，它的R²分数是`0`。
- en: For*MAE*and*MSE*, the smaller their values, the better the model is. Conversely,
    for *R**²*, the higher its values, the better the model is. In scikit-learn, the
    names of metric functions, where higher values correlate with better results,
    end with `_score`, while for functions ending with `_error` or `_loss`, the lower
    the value, the better.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*MAE*和*MSE*，它们的值越小，模型就越好。相反，对于*R²*，它的值越高，模型就越好。在scikit-learn中，那些值越高表示结果越好的度量函数名称以`_score`结尾，而以`_error`或`_loss`结尾的函数则是值越小，越好。
- en: Now, if we compare the scores, it is clear that our model scored better than
    the baseline in all of the three scores used. Congratulations!
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们比较得分，就会发现我们的模型在所有三项得分中都优于基线得分。恭喜！
- en: Showing feature coefficients
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 显示特征系数
- en: 'We know that a linear model multiplies each of the features by a certain coefficient,
    and then gets the sum of these products as its final prediction. We can use the
    regressor''s `coef_` method after the model is trained to print these coefficients:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道线性模型会将每个特征乘以一个特定的系数，然后将这些乘积的和作为最终预测结果。我们可以在模型训练后使用回归器的`coef_`方法打印这些系数：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As we can see in these results, some coefficients are positive and others are
    negative. A positive coefficient means that the feature correlates positively
    with the target and vice versa. I also added another column for the absolute values
    of the coefficients:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在这些结果中看到的，某些系数是正的，其他的是负的。正系数意味着特征与目标正相关，反之亦然。我还添加了系数绝对值的另一列：
- en: '![](img/13b79c89-465b-4c68-b59e-cc398779f2a1.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13b79c89-465b-4c68-b59e-cc398779f2a1.png)'
- en: 'In the preceding screenshot, the following is observed :'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，观察到如下情况：
- en: Ideally, the value for each coefficient should tell us how important each feature
    is. A higher absolute value, regardless of its sign, reflects high importance.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理想情况下，每个系数的值应该告诉我们每个特征的重要性。绝对值越高，不管符号如何，都表示特征越重要。
- en: However, I made a mistake here. If you check the data, you will notice that
    the maximum value for `NOX` is `0.87`, while `TAX` goes up to `711`. This means
    that if `NOX` has just marginal importance, its coefficient will still be high
    to balance its small value, while for `TAX` , its coefficient will always be small
    compared to the high values of the feature itself.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，我在这里犯了一个错误。如果你查看数据，你会注意到`NOX`的最大值是`0.87`，而`TAX`的最大值是`711`。这意味着如果`NOX`只有微不足道的重要性，它的系数仍然会很高，以平衡它的较小值；而对于`TAX`，它的系数会始终相对较小，因为特征本身的值较高。
- en: So, we want to scale the features to keep them all in the comparable ranges.
    In the next section, we are going to see how to scale our features.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所以，我们需要对特征进行缩放，以保持它们在可比较的范围内。在接下来的章节中，我们将看到如何对特征进行缩放。
- en: Scaling for more meaningful coefficients
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为了更有意义的系数进行缩放
- en: 'scikit-learn has a number of scalers. We are going to use `MinMaxScaler` for
    now. Using it with its default configuration will squeeze out all the values for
    all the features between `0` and `1`. The scaler needs to be fitted first to learn
    the features'' ranges. Fitting should be done on the training *x* set only. Then,
    we use the scaler''s `transform` function to scale both the training and test
    *x* sets:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn有多种缩放器。我们现在将使用`MinMaxScaler`。使用其默认配置时，它会将所有特征的值压缩到`0`和`1`之间。该缩放器需要先进行拟合，以了解特征的范围。拟合应该仅在训练*X*数据集上进行。然后，我们使用缩放器的`transform`函数对训练集和测试集的*X*数据进行缩放：
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'There is a shorthand version of this code for fitting one dataset and then
    transforming it. In other words, the following uncommented line takes the place
    of the two commented ones:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一行简化代码，它用于拟合一个数据集并进行转换。换句话说，以下未注释的行代替了两行注释的代码：
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We will be using the `fit_transform()` function a lot from now on where needed.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我们将经常使用`fit_transform()`函数，视需要而定。
- en: It's important to scale your features if you want meaningful coefficients. Furthermore,
    scaling helps gradient-based solvers converge quicker (more on this later). In
    addition to scaling, you should also make sure you don't have highly correlated
    features for more meaningful coefficients and a stable linear regression model.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要有意义的系数，缩放特征非常重要。更进一步，缩放有助于基于梯度的求解器更快地收敛（稍后会详细说明）。除了缩放，你还应该确保没有高度相关的特征，这样可以获得更有意义的系数，并使线性回归模型更稳定。
- en: 'Now that we have scaled our features and retrained the model, we can print
    the features and their coefficients again:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对特征进行了缩放并重新训练了模型，我们可以再次打印特征及其系数：
- en: '![](img/605484b4-76a8-455e-86ff-bfcdb35e22e7.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/605484b4-76a8-455e-86ff-bfcdb35e22e7.png)'
- en: Notice how `NOX` is less important now than before.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`NOX` 现在比之前更不重要了。
- en: Adding polynomial features
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加多项式特征
- en: 'Now that we know what the most important features are, we can plot the target
    against them to see how they correlate with them:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了最重要的特征，我们可以将目标与这些特征进行绘图，看看它们与目标之间的相关性：
- en: '![](img/abc909a7-3fe5-4cb0-ab64-0806c175e522.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/abc909a7-3fe5-4cb0-ab64-0806c175e522.png)'
- en: 'In the preceding screenshot, the following is observed:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，观察到以下情况：
- en: These plots don't seem to be very linear to me, and a linear model will not
    be able to capture this non-linearity.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些图看起来似乎并不完全是线性的，线性模型无法捕捉到这种非线性。
- en: Although we cannot turn a linear model into a non-linear one, we can still transform
    the data instead.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然我们不能将线性模型转变为非线性模型，但我们可以通过数据转换来实现。
- en: 'Think of it this way: if *y* is a function of *x²*, we can either use a non-linear
    model—one that is capable of capturing the quadratic relation between *x* and
    *y*—or we can just calculate *x²* and give it to a linear model instead of *x*.
    Furthermore, linear regression algorithms do not capture feature interactions.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这样想：如果 *y* 是 *x²* 的函数，我们可以使用一个非线性模型——一个能够捕捉 *x* 和 *y* 之间二次关系的模型——或者我们可以直接计算
    *x²* 并将其提供给线性模型，而不是 *x*。此外，线性回归算法无法捕捉特征交互。
- en: The current model cannot capture interactions between multiple features.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前模型无法捕捉多个特征之间的交互。
- en: 'A polynomial transformation can solve both the non-linearity and feature interaction
    issues for us. Given the original data, scikit-learn''s polynomial transformer
    will transform the features into higher dimensions (for example, it will add the
    quadratic and cubic values for each feature). Additionally, it will also add the
    products to each feature-pair (or triplets). `PolynomialFeatures` works in a similar
    fashion to the scaler we used earlier in this chapter. We are going to use its
    `fit_transform` variable and a `transform()` method, as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式变换可以解决非线性和特征交互问题。给定原始数据，scikit-learn 的多项式变换器将把特征转化为更高维度（例如，它会为每个特征添加平方值和立方值）。此外，它还会将每对特征（或三元组）之间的乘积添加进去。`PolynomialFeatures`
    的工作方式类似于我们在本章前面使用的缩放器。我们将使用其 `fit_transform` 变量和 `transform()` 方法，如下所示：
- en: '[PRE15]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: To get both the quadratic and cubic feature transformation, we set the `degree`
    parameter to `3`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得二次和三次特征转换，我们将 `degree` 参数设置为 `3`。
- en: 'One annoying thing about `PolynomialFeatures` is that it doesn''t keep track
    of the DataFrame''s column names. It replaces the feature names with `x0`, `x1`,
    `x2`, and so on. However, with our Python skills at hand, we can reclaim our column
    names. Let''s do exactly that using the following block of code:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`PolynomialFeatures` 有一个令人烦恼的地方，它没有保留 DataFrame 的列名。它将特征名替换为 `x0`、`x1`、`x2`
    等。然而，凭借我们的 Python 技能，我们可以恢复列名。我们就用以下代码块来实现这一点：'
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can now use the newly derived polynomial features instead of the original
    ones.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用新派生的多项式特征，而不是原始特征。
- en: Fitting the linear regressor with the derived features
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用派生特征拟合线性回归模型
- en: '*"When I was six, my sister was half my age. Now I am 60 years old, how old
    is my sister?"*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*"当我六岁时，我妹妹只有我一半大。现在我60岁，我妹妹多大了？"*'
- en: '*This is a puzzle found on the internet. If your answer is 30, then you forgot
    to fit an intercept into your linear regression model.*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是在互联网上找到的一个谜题。如果你的答案是30，那么你忘记为线性回归模型拟合截距了。*'
- en: 'Now, we are ready to use our linear regressor with the newly transformed features.
    One thing to keep in mind is that the `PolynomialFeatures`transformer adds one
    additional column where all the values are `1`. The coefficient this column gets
    after training is equivalent to the intercept. So, we will not fit an intercept
    by setting `fit_intercept=False`when training our regressor this time:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备使用带有新转换特征的线性回归器。需要记住的一点是，`PolynomialFeatures`转换器会添加一个额外的列，所有值都是`1`。训练后，这一列得到的系数相当于截距。因此，我们这次训练回归器时，将通过设置`fit_intercept=False`来避免拟合截距：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, as we print the *R²*, *MAE*, and *MSE* results, we face the following
    unpleasant surprise:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当我们打印*R²*、*MAE*和*MSE*结果时，迎来了一些不太愉快的惊讶：
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The regressor is way worse than before and even worse than the baseline. What
    did the polynomial features do to our model?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 回归器的表现比之前差得多，甚至比基准模型还要差。多项式特征究竟对我们的模型做了什么？
- en: One major problem with the ordinary least squares regression algorithm is that
    it doesn't work well with highly correlated features (multicollinearity).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 普通最小二乘回归算法的一个主要问题是它在面对高度相关的特征（多重共线性）时效果不好。
- en: The polynomial feature transformation's kitchen-sink approach—where we add features,
    their squared and cubic values, and the product of the features' pairs and triples—will
    very likely give us multiple correlated features. This multi-collinearity harms
    the model's performance. Furthermore, if you print the shape of `x_train_poly`,
    you will see that it has 303 samples and 560 features. This is another problem
    known as the curse of dimensionality.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式特征转换的“厨房水槽”方法——我们添加特征、它们的平方值和立方值，以及特征对和三重对的乘积——很可能会给我们带来多个相关的特征。多重共线性会损害模型的表现。此外，如果你打印`x_train_poly`的形状，你会看到它有303个样本和560个特征。这是另一个问题，称为“维度灾难”。
- en: The **curse of dimensionality** is when you have too many features compared
    to your samples. If you imagine your DataFrame as a rectangle with the features
    as its base and the samples as its height, you always want your rectangle to have
    a much bigger height than its base. Imagine having two binary columns—`x1` and
    `x2`. They can take four possible value combinations—`(0, 0)`, `(0, 1)`, `(1,
    0)`, and `(1, 1)`. Similarly, for *n* columns, they can take *2^n* combinations.
    As you can see, the number of possibilities increases exponentially with the number
    of features. For a supervised learning algorithm to work well, it needs enough
    samples to cover a reasonable number of all these possibilities. This problem
    is even more drastic when we have non-binary features, as is our case here.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**维度灾难**是指当你的特征数远超过样本数时的问题。如果你把数据框想象成一个矩形，特征是矩形的底边，样本是矩形的高度，你总是希望矩形的高度远大于底边。假设有两列二进制特征——`x1`和`x2`。它们可以有四种可能的值组合——`(0,
    0)`、`(0, 1)`、`(1, 0)`和`(1, 1)`。同样，对于*n*列，它们可以有*2^n*种组合。正如你所看到的，随着特征数的增加，可能性数量呈指数增长。为了使监督学习算法有效工作，它需要足够的样本来覆盖所有这些可能性中的合理数量。当我们有非二进制特征时（如本例所示），这个问题更为严重。'
- en: Thankfully, two centuries is long enough for people to find solutions to these
    two problems. Regularization is the solution we are going to have fun with in
    the next section.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，两个世纪的时间足够让人们找到这两个问题的解决方案。正则化就是我们在下一部分将要深入探讨的解决方案。
- en: Regularizing the regressor
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化回归器
- en: '"It is vain to do with more what can be done with fewer."'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: “用更多做本可以用更少做的事是徒劳的。”
- en: – William of Occam
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ——奥卡姆的威廉
- en: Originally, our objective was to minimize the MSE value of the regressor. Later
    on, we discovered that too many features are an issue. That's why we need a new
    objective. We still need to minimize the MSE value of the regressor, but we also
    need to incentivize the model to ignore the useless features. This second part
    of our objective is what regularization does in a nutshell.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们的目标是最小化回归器的MSE值。后来我们发现，特征过多是一个问题。这就是为什么我们需要一个新的目标。我们仍然需要最小化回归器的MSE值，但同时我们还需要激励模型忽略无用的特征。这个目标的第二部分，就是正则化的作用。
- en: Two algorithms are commonly used for regularized linear regression—**lasso**
    and **ridge**. Lasso pushes the model to have fewer coefficients—that is, it sets
    as many coefficients as possible to `0`—while ridge pushes the model to have as
    small values as possible for its coefficients. Lasso uses a form of regularization
    called L1, which penalizes the absolute values of the coefficients, while ridge
    uses L2, which penalizes the squared values of the coefficients. These two algorithms
    have a hyperparameter (alpha), which controls how strongly the coefficients will
    be regularized. Setting alpha to `0` means no regularization at all, which brings
    us back to an ordinary least squares regressor. While larger values for alpha
    specify stronger regularization, we will start with the default value for alpha,
    and then see how to set it correctly later on.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 常用于正则化线性回归的两种算法是**Lasso**和**Ridge**。Lasso使得模型的系数更少——也就是说，它将尽可能多的系数设为`0`——而Ridge则推动模型的系数尽可能小。Lasso使用一种叫做L1的正则化形式，它惩罚系数的绝对值，而Ridge使用L2，它惩罚系数的平方值。这两种算法都有一个超参数（alpha），用来控制系数的正则化程度。将alpha设为`0`意味着没有任何正则化，这就回到了普通最小二乘回归。较大的alpha值指定更强的正则化，而我们将从alpha的默认值开始，稍后再看看如何正确设置它。
- en: The standard approach used in the ordinary least squares algorithm does not
    work here. We now have an objective function that aims to minimize the size of
    the coefficients, in addition to minimizing the predictor's MSE values. So, a
    solver is used to find the optimum coefficients to minimize the new objective
    functions. We will look further at solvers later in this chapter.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 普通最小二乘法算法中使用的标准方法在这里不起作用。现在，我们有了一个目标函数，旨在最小化系数的大小，同时最小化预测器的MSE值。因此，使用求解器来找到能够最小化新目标函数的最佳系数。我们将在本章稍后进一步讨论求解器。
- en: Training the lasso regressor
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练Lasso回归器
- en: 'Training lasso is no different to training any other model. Similar to what
    we did in the previous section, we will set `fit_intercept` to `False` here:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 训练Lasso与训练其他模型没有区别。与我们在前一节中所做的类似，我们将在这里将`fit_intercept`设置为`False`：
- en: '[PRE19]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Once done, we can print the R², MAE, and MSE:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，我们可以打印R²、MAE和MSE：
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Not only did we fix the problems introduced by the polynomial features, but
    we also have better performance than the original linear regressor. *MAE* is `2.4`
    here, compared to `3.6` before, *MSE* is `16.2`, compared to `25.8` before, and
    *R²* is `0.79`, compared to `0.73` before.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅修复了多项式特征引入的问题，而且还比原始线性回归器有了更好的表现。*MAE*值为`2.4`，相比之前的`3.6`，*MSE*为`16.2`，相比之前的`25.8`，*R²*为`0.79`，相比之前的`0.73`。
- en: Now that we have seen promising results after applying regularization, it's
    time to see how to set an optimum value for the regularization parameter.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了应用正则化后的 promising results，接下来是时候看看如何为正则化参数设置一个最佳值。
- en: Finding the optimum regularization parameter
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找最佳正则化参数
- en: 'Ideally, after splitting the data into training and test sets, we would further
    split the training set into *N* folds. Then, we would make a list of all the values
    of alpha that we would like to test and loop over them one after the other. With
    each iteration, we would apply *N*-fold cross-validation to find the value for
    alpha that gives the minimal error. Thankfully, scikit-learn has a module called
    `LassoCV` (`CV` stands for cross-validation). Here, we are going to use this module
    to find the best value for alpha using five-fold cross-validation:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，在将数据拆分为训练集和测试集之后，我们会将训练集进一步拆分为*N*个折叠。然后，我们会列出我们想要测试的所有alpha值，并逐一循环进行测试。每次迭代时，我们将应用*N*-fold交叉验证，找出能够产生最小误差的alpha值。幸运的是，scikit-learn有一个叫做`LassoCV`的模块（`CV`代表交叉验证）。在这里，我们将使用这个模块，利用五折交叉验证来找到最佳的alpha值：
- en: '[PRE21]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Once done, we can use the model for predictions. You may want to predict for
    both the training and test sets and see whether the model overfits on the training
    set. We can also print the chosen alpha, as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，我们可以使用模型进行预测。你可能想预测训练集和测试集，并查看模型是否在训练集上出现过拟合。我们还可以打印选择的alpha值，如下所示：
- en: '[PRE22]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: I got an `alpha` value of `1151.4`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我得到了`1151.4`的`alpha`值。
- en: Furthermore, we can also see, for each value of alpha, what the *MSE* value
    for each of the five folds was. We can access this information via `mse_path_`.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以看到，对于每个alpha值，五个折叠中的*MSE*值是多少。我们可以通过`mse_path_`访问这些信息。
- en: Since we have five values for *MSE* for each value of alpha, we can plot the
    mean of these five values, as well as the confidence interval around the mean.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个alpha值对应五个*MSE*值，我们可以绘制这五个值的平均值，并绘制围绕平均值的置信区间。
- en: The confidence interval is used to show the expected range that observed data
    may take. A 95% confidence interval means that we expect 95% of our values to
    fall within this range. Having a wide confidence interval means that the data
    may take a wide range of values, while a narrower confidence interval means that
    we can almost pinpoint exactly what value the data will take.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 置信区间用于展示观察数据可能取值的预期范围。95%的置信区间意味着我们期望95%的值落在这个范围内。较宽的置信区间意味着数据可能取值的范围较大，而较窄的置信区间则意味着我们几乎可以准确地预测数据会取什么值。
- en: 'A 95% confidence interval is calculated as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 95%的置信区间计算如下：
- en: '![](img/01b5aeff-8657-4768-97f9-0ba308fb09c9.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01b5aeff-8657-4768-97f9-0ba308fb09c9.png)'
- en: Here, the standard error is equal to the standard deviation divided by the square
    root of the number of samples ([![](img/be193ff5-f516-4cdf-a157-9287578ae2b2.png)],
    since we have five folds here).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，标准误差等于标准差除以样本数量的平方根（[![](img/be193ff5-f516-4cdf-a157-9287578ae2b2.png)]，因为我们这里有五个折数）。
- en: The equation for the confidence interval here is not 100% accurate. Statistically
    speaking, when dealing with small samples, and their underlying variance is not
    known, a t-distribution should be used instead of a z-distribution. Thus, given
    the small number of folds here, the 1.96 coefficient should be replaced by a more
    accurate value from the t-distribution table, where its degrees of freedom are
    inferred from the number of folds.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的置信区间公式并不是100%准确。从统计学角度来看，当处理小样本且其基本方差未知时，应该使用t分布而非z分布。因此，鉴于这里的折数较小，1.96的系数应当用t分布表中更准确的值来替代，其中自由度由折数推断得出。
- en: 'The following code snippets calculate and plot the confidence intervals for
    MSE versus alpha:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段计算并绘制了MSE与alpha的置信区间：
- en: 'We start by calculating the descriptive statistics of the *MSE* values returned:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先计算返回的*MSE*值的描述性统计数据：
- en: '[PRE23]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then, we put our calculations into a data frame and plot them using the default
    line chart:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将计算结果放入数据框中，并使用默认的折线图进行绘制：
- en: '[PRE24]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here is the output of the previous code:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '![](img/7c750341-03f6-4570-8edb-b0b98a6920da.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c750341-03f6-4570-8edb-b0b98a6920da.png)'
- en: The MSE value is lowest at the chosen alpha value. The confidence interval is
    also narrower there, which reflects more confidence in the expected *MSE* result.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择的alpha值下，MSE值最小。此时，置信区间也更窄，这反映了对预期的*MSE*结果更高的信心。
- en: 'Finally, setting the model''s alpha value to the onesuggested and using it
    to make predictions for the test data gives us the following results:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将模型的alpha值设置为建议值，并使用它对测试数据进行预测，得出了以下结果：
- en: '|  | **Baseline** | **Linear Regression** | **Lasso at Alpha = 1151.4** |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | **基准** | **线性回归** | **Lasso（Alpha = 1151.4）** |'
- en: '| **R²** | `0.00` | `0.73` | `0.83` |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| **R²** | `0.00` | `0.73` | `0.83` |'
- en: '| **MAE** | `7.20` | `3.56` | `2.76` |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| **MAE** | `7.20` | `3.56` | `2.76` |'
- en: '| **MSE** | `96.62` | `25.76` | `16.31` |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| **MSE** | `96.62` | `25.76` | `16.31` |'
- en: Clearly, regularization fixed the issues caused by the curse of dimensionality
    earlier. Furthermore, we were able to use cross-validation to find the optimum
    regularization parameter. We plotted the confidence intervals of errors to visualize
    the effect of alpha on the regressor. The fact that I have been talking about
    the confidence intervals in this section inspired me to dedicate the next section
    to regression intervals.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，正则化解决了由维度灾难引起的问题。此外，我们通过交叉验证找到了最佳的正则化参数。我们绘制了误差的置信区间，以可视化alpha对回归器的影响。我在本节讨论置信区间的内容，激发了我将下一节专门用于回归区间的写作。
- en: Finding regression intervals
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找回归区间
- en: '"Exploring the unknown requires tolerating uncertainty."'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: “探索未知需要容忍不确定性。”
- en: – Brian Greene
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: – 布莱恩·格林
- en: It's not always guaranteed that we have accurate models. Sometimes, our data
    is inherently noisy and we cannot model it using a regressor. In these cases,
    it is important to be able to quantify how certain we arein our estimations. Usually,
    regressors make point predictions. These are the expected values (typically the
    mean) of the target (*y*) at each value of *x*. A Bayesian ridge regressor is
    capable of returning the expected values as usual, yet it also returns the standard
    deviation of the target (*y*) at each value of *x*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法总是保证得到准确的模型。有时，我们的数据本身就很嘈杂，无法使用回归模型进行建模。在这些情况下，能够量化我们估计结果的可信度非常重要。通常，回归模型会做出点预测。这些是目标值（通常是均值）在每个
    *x* 值下的预期值 (*y*)。贝叶斯岭回归模型通常也会返回预期值，但它还会返回每个 *x* 值下目标值 (*y*) 的标准差。
- en: 'To demonstrate how this works, let''s create a noisy dataset, where ![](img/e6f30f23-e2ed-4ea5-827e-21067cb1c1d8.png):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示这一点，我们来创建一个带噪声的数据集，其中 ![](img/e6f30f23-e2ed-4ea5-827e-21067cb1c1d8.png)：
- en: '[PRE25]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, we can plot it in the form of a scatter plot:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将其绘制成散点图：
- en: '[PRE26]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Plotting the resulting data frame will give us the following plot:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制结果数据框将给我们以下图形：
- en: '![](img/253614ca-7f47-4c93-be97-f64ca09cc0d9.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/253614ca-7f47-4c93-be97-f64ca09cc0d9.png)'
- en: 'Now, let''s train two regressors on the same data—`LinearRegression` and `BayesianRidge`.
    I will stick to the default values for the Bayesian ridge hyperparameters here:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在相同的数据上训练两个回归模型——`LinearRegression` 和 `BayesianRidge`。这里我将坚持使用默认的贝叶斯岭回归超参数值：
- en: '[PRE27]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Notice how the Bayesian ridge regressor returns two values when predicting.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，贝叶斯岭回归模型在预测时会返回两个值。
- en: The Bayesian approach to linear regression differs from the aforementioned algorithms
    in the way that it sees its coefficients. For all the algorithms we have seen
    so far, each coefficient takes a single value after training, but for a Bayesian
    model, a coefficient is rather a distribution with an estimated mean and standard
    deviation. A coefficient is initialized using a prior distribution, which gets
    updated by the training data to reach a posterior distribution via Bayes' theorem.
    The Bayesian ridge regressor is a regularized Bayesian regressor.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯线性回归与前面提到的算法在看待其系数的方式上有所不同。对于我们到目前为止看到的所有算法，每个系数在训练后都取一个单一的值，但对于贝叶斯模型，系数实际上是一个分布，具有估计的均值和标准差。系数是通过一个先验分布进行初始化的，然后通过训练数据更新，最终通过贝叶斯定理达到后验分布。贝叶斯岭回归模型是一个正则化的贝叶斯回归模型。
- en: 'The predictions made by the two models are very similar. Nevertheless, we can
    use the standard deviation returned to calculate a range around the values that
    we expect most of the future data to fall into.The following code snippet creates
    plots for the two models and their predictions:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型的预测结果非常相似。然而，我们可以使用返回的标准差来计算我们预期大多数未来数据落入的范围。以下代码片段绘制了这两个模型及其预测的图形：
- en: '[PRE28]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Running the preceding code gives us the following graphs. In the `BayesianRidge`
    case, the shaded area shows where we expect 95% of our targets to fall:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码会给我们以下图形。在`BayesianRidge`的案例中，阴影区域显示了我们预期95%的目标值会落在其中：
- en: '![](img/c7f47af5-d666-4262-ad74-0716d20f0daa.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7f47af5-d666-4262-ad74-0716d20f0daa.png)'
- en: Regression intervals are handy when we want to quantify our uncertainties. In
    [Chapter 8](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=30&action=edit),
    *Ensembles – When One Model Is Not Enough*, we will revisit regression intervals
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 回归区间在我们想量化不确定性时非常有用。在[第8章](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=30&action=edit)，*集成方法——当一个模型不够用时*，我们将重新讨论回归区间。
- en: Getting to know additional linear regressors
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解更多的线性回归模型
- en: 'Before moving on to linear classifiers, it makes sense to also add the following
    additional linear regression algorithms to your toolset:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续学习线性分类器之前，理应将以下几种额外的线性回归算法加入到你的工具箱中：
- en: '**Elastic-net** uses a mixture of L1 and L2 regularization techniques, where
    `l1_ratio` controls the mix of the two. This is useful in cases when you want
    to learn a sparse model where few of the weights are non-zero (as in **lasso**)
    while keeping the benefits of **ridge** regularization.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Elastic-net** 使用 L1 和 L2 正则化技术的混合，其中 `l1_ratio` 控制两者的混合比例。这在你希望学习一个稀疏模型，其中只有少数权重为非零（如
    **lasso**）的情况下非常有用，同时又能保持 **ridge** 正则化的优点。'
- en: '**Random Sample Consensus**(**RANSAC**) is useful when your data has outliers.
    It tries to separate the outliers from the inlier samples. Then, it fits the model
    on the inliers only.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机样本一致性**（**RANSAC**）在数据存在离群点时非常有用。它试图将离群点与内点样本分开。然后，它仅对内点样本拟合模型。'
- en: '**Least-Angle Regression** (**LARS**) is useful when dealing with high-dimensional
    data—that is, when there is a significant number of features compared to the number
    of samples. You may want to try it with the polynomial features example we saw
    earlier and see how it performs there.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小角回归**（**LARS**）在处理高维数据时非常有用——也就是当特征数量与样本数量相比显著较多时。你可以尝试将其应用到我们之前看到的多项式特征示例中，看看它的表现如何。'
- en: Let's move on to the next section of the book where you will learn to use logistic
    regression to classify data.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进入书中的下一个章节，你将学习如何使用逻辑回归来分类数据。
- en: Using logistic regression for classification
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用逻辑回归进行分类
- en: '"You can tell whether a man is clever by his answers. You can tell whether
    a man is wise by his questions."'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: “你可以通过一个人的答案看出他是否聪明。你可以通过一个人的问题看出他是否智慧。”
- en: – Naguib Mahfouz
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: – 纳吉布·马赫福兹
- en: 'One day, when applying for a job, an interviewer asks: *So tell me, is logistic
    regression a classification or a regression algorithm?* The short answer to this
    is that it is a classification algorithm, but a longer and more interesting answer
    requires a good understanding of the logistic function. Then, the question may
    end up having a different meaning altogether.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 有一天，在面试时，面试官问：“*那么告诉我，逻辑回归是分类算法还是回归算法？*” 对此的简短回答是它是分类算法，但更长且更有趣的回答需要对逻辑函数有很好的理解。然后，问题可能会完全改变其意义。
- en: Understanding the logistic function
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解逻辑函数
- en: 'The logistic function is a member of the sigmoid (*s*-shaped) functions, and
    it is represented by the following formula:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑函数是S型（*s*形）函数的一种，它的表达式如下：
- en: '![](img/d263ba93-0475-4daa-91a5-1c6645de549f.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d263ba93-0475-4daa-91a5-1c6645de549f.png)'
- en: 'Don''t let this equation scare you. What actually matters is how this function
    looks visually. Luckily, we can use our computer to generate a bunch of values
    for theta—for example, between `-10` and `10`. Then, we can plug these values
    into the formula and plot the resulting `y` values versus the theta values, as
    we have done in the following code:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 别让这个方程吓到你。真正重要的是这个函数的视觉效果。幸运的是，我们可以用计算机生成一系列θ的值——比如在`-10`到`10`之间。然后，我们可以将这些值代入公式，并绘制出对应的`y`值与θ值的关系图，正如我们在以下代码中所做的：
- en: '[PRE29]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Running this code gives us the following graph:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码将生成以下图表：
- en: '![](img/313d6584-8e70-4dc0-a531-b5c989b2975a.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/313d6584-8e70-4dc0-a531-b5c989b2975a.png)'
- en: 'Two key characteristics to notice in the logistic function are as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑函数中需要注意的两个关键特征如下：
- en: '*y* only goes between `0` and `1`. It approaches `1` as theta approaches infinity,
    and approaches `0` as theta approaches negative infinity.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* 仅在`0`和`1`之间变化。当θ趋近于正无穷时，*y*趋近于`1`；当θ趋近于负无穷时，*y*趋近于`0`。'
- en: '`y` takes the value of `0.5` when theta is `0`.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当θ为`0`时，`y`的值为`0.5`。
- en: Plugging the logistic function into a linear model
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将逻辑函数代入线性模型
- en: '"Probability is not a mere computation of odds on the dice or more complicated
    variants; it is the acceptance of the lack of certainty in our knowledge and the
    development of methods for dealing with our ignorance."'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: “概率不仅仅是对骰子上的赔率或更复杂的变种进行计算；它是接受我们知识中不确定性的存在，并发展出应对我们无知的方法。”
- en: – Nassim Nicholas Taleb
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: – 纳西姆·尼古拉斯·塔勒布
- en: 'For a line model with a couple of features, *x[1]* and *x[2]*, we can have
    an intercept and two coefficients. Let''s call them ![](img/7b829d0f-4b60-4173-90b1-ca98ebe6e69d.png),
    ![](img/16a7a93c-f405-4f3e-abff-a2a80688aaf3.png) , and ![](img/a914785a-b7f0-4197-8e3d-62e47bac1a5c.png).
    Then, the linear regression equation will be as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个包含两个特征的线性模型，*x[1]* 和 *x[2]*，我们可以有一个截距和两个系数。我们将它们称为![](img/7b829d0f-4b60-4173-90b1-ca98ebe6e69d.png)、![](img/16a7a93c-f405-4f3e-abff-a2a80688aaf3.png)
    和 ![](img/a914785a-b7f0-4197-8e3d-62e47bac1a5c.png)。那么，线性回归方程将如下所示：
- en: '![](img/4cd8809f-b19d-4c1e-87e5-0a52e8419f49.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cd8809f-b19d-4c1e-87e5-0a52e8419f49.png)'
- en: 'Separately, we can also plug the right-hand side of the preceding equation
    into the logistic function in place of ![](img/e4173069-bef8-4e29-8c88-10cfc96da625.png).
    This will give the following equation for *y*:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们也可以将前面方程右侧的部分代入逻辑函数，替代![](img/e4173069-bef8-4e29-8c88-10cfc96da625.png)。这将得到以下的*y*方程：
- en: '![](img/d78652ca-db05-492b-a081-edde13e8d20f.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d78652ca-db05-492b-a081-edde13e8d20f.png)'
- en: In this case, the variation in the values of *x* will move *y* between `0` and
    `1`. Higher values for the products of *x* and its coefficients will move *y*
    closer to `1`, and lower values will move it toward `0`. We also know that probabilities
    take values between `0` and `1`. So, it makes sense to interpret *y* as the probability
    of *y* belonging to a certain class, given the value of *x*. If we don't want
    to deal with probabilities, we can just specify ![](img/d4188933-f93c-4834-9b32-7a83a1377fcd.png);
    then, our sample belongs to class 1, and it belongs to class 0 otherwise.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，*x*值的变化将使得*y*在`0`和`1`之间波动。*x*与其系数的乘积的较高值会使得*y*接近`1`，较低值会使其接近`0`。我们也知道，概率的值介于`0`和`1`之间。因此，将*y*解释为给定*x*的情况下，*y*属于某一类的概率是有意义的。如果我们不想处理概率，我们可以直接指定
    ![](img/d4188933-f93c-4834-9b32-7a83a1377fcd.png)；那么，我们的样本就属于类别1，否则属于类别0。
- en: This was a brief look at how logistic regression works. It is a classifier,
    yet it is called *regression* since it's basically a regressor returning a value
    between `0` and `1`, which we interpret as probabilities.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对逻辑回归工作原理的简要介绍。它是一个分类器，但被称为*回归*，因为它基本上是一个回归器，返回一个介于`0`和`1`之间的值，我们将其解释为概率。
- en: To train the logistic regression model, we need an objective function, as well
    as a solver that tries to find the optimal coefficients to minimize this function.
    In the following sections, we will go through all of these in more detail.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练逻辑回归模型，我们需要一个目标函数，以及一个求解器，用来寻找最优的系数以最小化这个函数。在接下来的章节中，我们将更详细地讲解这些内容。
- en: Objective function
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标函数
- en: 'During the training phase, the algorithm loops through the data trying to find
    the coefficients that minimize a predefined objective (loss) function. The loss
    function we try to minimize in the case of logistic regression is called log loss.
    It measures how far the predicted probabilities (*p*) are from the actual class
    labels (*y*) using the following formula:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，算法会遍历数据，尝试找到能够最小化预定义目标（损失）函数的系数。在逻辑回归的情况下，我们尝试最小化的损失函数被称为对数损失。它通过以下公式来衡量预测概率(*p*)与实际类别标签(*y*)之间的差距：
- en: '*-log(p) if y == 1 else -log(1 - p)*'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '*-log(p) if y == 1 else -log(1 - p)*'
- en: Mathematicians use a rather ugly way to express this formula due to their lack
    of `if-else` conditions. So, I chose to display the Python form here for its clarity.
    Jokes aside, the mathematical formula will turn out to be beautiful once you know
    its informational theory roots, but that's not something we'll look at now.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 数学家使用一种相当难看的方式来表达这个公式，因为他们缺少`if-else`条件。所以，我选择在这里显示Python形式，便于理解。开个玩笑，数学公式在你了解其信息论根源后会变得非常优美，但这不是我们现在要讨论的内容。
- en: Regularization
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化
- en: Furthermore, scikit-learn's implementation of logistic regression algorithms
    uses regularization by default. Out of the box, it uses L2 regularization (as
    in the ridge regressor), but it can also use L1 (as in lasso) or a mixture of
    L1 and L2 (as in elastic-net).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，scikit-learn实现的逻辑回归算法默认使用正则化。开箱即用时，它使用L2正则化（如岭回归器），但它也可以使用L1（如Lasso）或L1和L2的混合（如Elastic-Net）。
- en: Solvers
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 求解器
- en: Finally, how do we find the optimal coefficients to minimize our loss function?
    A naive approach would be to try all the possible combinations of the coefficients
    until the minimal loss is found. Nevertheless, since an exhaustive search is not
    feasible given the infinite combinations, solvers are there to efficiently search
    for the best coefficients. scikit-learn implements about half a dozen solvers.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们如何找到最优的系数来最小化我们的损失函数呢？一个天真的方法是尝试所有可能的系数组合，直到找到最小损失。然而，由于考虑到无限的组合，全面搜索是不可行的，因此求解器的作用就是高效地搜索最优系数。scikit-learn实现了大约六种求解器。
- en: The choice of solver, along with the regularization method used, are the two
    main decisions to take when configuring the logistic regression algorithm. In
    the next section, we are going to see how and when to pick each one.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 求解器的选择以及所使用的正则化方法是配置逻辑回归算法时需要做出的两个主要决策。在接下来的章节中，我们将讨论如何以及何时选择每一个。
- en: Configuring the logistic regression classifier
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置逻辑回归分类器
- en: 'Before talking about solvers, let''s go through some of the common hyperparameters
    used:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在谈论求解器之前，让我们先了解一些常用的超参数：
- en: '**`fit_intercept`**: Usually, in addition to the coefficient for each feature,
    there is a constant intercept in your equation. Nevertheless, there are cases
    where you might not need an intercept—for example, if you know for sure that the
    value of *y* is supposed to be `0.5` when all the values of *x* are `0`. One other
    case is when your data already has an additional constant column with all values
    set to `1`. This usually occurs if your data has been processed in an earlier
    stage, as in the case of the polynomial processor. The coefficient for the `constant`
    column will be interpreted as the intercept in this case. The same configuration
    exits for the linear regression algorithms explained earlier.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`fit_intercept`**：通常，除了每个特征的系数外，方程中还有一个常数截距。然而，有些情况下你可能不需要截距，例如，当你确定当所有 *x*
    的值为 `0` 时，*y* 的值应该是 `0.5`。另一个情况是当你的数据已经有一个常数列，所有值都设为 `1`。这种情况通常发生在数据的早期处理阶段，比如在多项式处理器的情况下。此时，`constant`
    列的系数将被解释为截距。线性回归算法中也有类似的配置。'
- en: '**`max_iter`**: For the solver to find the optimum coefficients, it loops over
    the training data more than once. These iterations are also called epochs. You
    usually set a limit on the number of iterations to prevent overfitting. The same
    hyperparameter is used by the lasso and ridge regressors explained earlier.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`max_iter`**：为了让求解器找到最佳系数，它会多次遍历训练数据。这些迭代也称为周期（epochs）。通常会设置迭代次数的上限，以防止过拟合。与之前解释的
    lasso 和 ridge 回归器使用的超参数相同。'
- en: '**`tol`**: This is another way to stop the solver from iterating too much.
    If you set this to a high value, it means that only high improvements between
    one iteration and the next are tolerated; otherwise, the solver will stop. Conversely,
    a lower value will keep the solver going for more iterations until it reaches
    **`max_iter`.**'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`tol`**：这是另一种停止求解器过多迭代的方法。如果将其设置为较高的值，意味着只接受相邻两次迭代之间的较大改进；否则，求解器将停止。相反，较低的值将使求解器继续迭代更多次，直到达到
    **`max_iter`**。'
- en: '**`penalty`**: This picks the regularization techniques to be used. This can
    be either L1, L2, elastic-net, or none for no regularization. Regularization helps
    to prevent overfitting, so it is important to use it when you have a lot of features.
    It also mitigates the overfitting effect when `max_iter` and**`tol`** are set
    to high values.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`penalty`**：选择要使用的正则化技术。可以是 L1、L2、弹性网（elastic-net）或无正则化（none）。正则化有助于防止过拟合，因此当特征较多时，使用正则化非常重要。当
    `max_iter` 和 **`tol`** 设置为较高值时，它还可以减轻过拟合的效果。'
- en: '`C` or `alpha`: These are parameters for setting how strong you want the regularization
    to be. Since we are going to use two different implementations of the logistic
    regression algorithm here, it is important to know that each of these two implementations
    uses a different parameter (`C` versus `alpha`). `alpha` is basically the inverse
    of `C`—(![](img/c4f16ce4-15f1-4eed-b539-3895626e3ad4.png)). This means that smaller
    values for `C` specify stronger regularization, while for `alpha`, larger values
    are needed for stronger regularization.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`C` 或 `alpha`：这些是用于设置正则化强度的参数。由于我们在此使用两种不同的逻辑回归算法实现，因此需要了解这两种实现使用了不同的参数（`C`
    与 `alpha`）。`alpha` 基本上是 `C` 的倒数—(![](img/c4f16ce4-15f1-4eed-b539-3895626e3ad4.png))。这意味着，较小的
    `C` 值表示较强的正则化，而对于 `alpha`，则需要较大的值来表示较强的正则化。'
- en: '`l1_ratio`: When using a mixture of L1 and L2, as in elastic-net, this fraction
    specifies how much weight to give to L1 versus L2\.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l1_ratio`：当使用 L1 和 L2 的混合时，例如弹性网（elastic-net），此值指定 L1 与 L2 的权重比例。'
- en: 'The following are some of the solvers we can use:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们可以使用的一些求解器：
- en: '`liblinear`:**This solver is implemented in `LogisticRegression` and is recommended
    for smaller datasets. It supports L1 and L2 regularization, but you cannot use
    it if you want to use elastic-net, nor if you do not want to use regularization
    at all.**'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`liblinear`：**该求解器在 `LogisticRegression` 中实现，推荐用于较小的数据集。它支持 L1 和 L2 正则化，但如果想使用弹性网，或者如果不想使用正则化，则无法使用此求解器。**'
- en: '***   `sag` or `saga`: These solvers are implemented in `LogisticRegression`
    and `RidgeClassifier`. They are faster for larger datasets. However, you need
    to scale your features for them to converge. We used `MinMaxScaler` earlier in
    this chapter to scale our features. Now, it is not only needed for more meaningful
    coefficients, but also for the solver to find a solution earlier. `saga` supports
    all four penalty options.*   `lbfgs`:**This solver is implemented in `LogisticRegression`.
    It supports the L2 penalty or no regularization at all.****'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '*** `sag`或`saga`：这些求解器在`LogisticRegression`和`RidgeClassifier`中实现，对于较大的数据集，它们运行更快。然而，你需要对特征进行缩放，才能使其收敛。我们在本章早些时候使用了`MinMaxScaler`来缩放特征。现在，这不仅仅是为了更有意义的系数，也是为了让求解器更早地找到解决方案。`saga`支持四种惩罚选项。*
    `lbfgs`：**此求解器在`LogisticRegression`中实现。它支持L2惩罚或根本不使用正则化。****'
- en: '*****   **Stochastic Gradient Descent** (**SGD**): There are dedicated implementations
    for SGD—`SGDClassifier` and `SGDRegressor`. This is different to `LogisticRegression`,
    where the focus is on performing logistic regression by optimizing the one-loss
    function—log loss. The focus of `SGDClassifier` is on the SGD solver itself, which
    means that the same classifier allows different loss functions to be used. If
    `loss` is set to `log`, then it is a logistic regression model. However, setting
    `loss` to `hinge` or `perceptron` turns it into a **Support Vector Machine** (**SVM**)
    or perceptron, respectively. These are two other linear classifiers.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '***** **随机梯度下降**（**SGD**）：SGD有专门的实现——`SGDClassifier`和`SGDRegressor`。这与`LogisticRegression`不同，后者的重点是通过优化单一的损失函数——对数损失来进行逻辑回归。`SGDClassifier`的重点是SGD求解器本身，这意味着相同的分类器可以使用不同的损失函数。如果将`loss`设置为`log`，那么它就是一个逻辑回归模型。然而，将`loss`设置为`hinge`或`perceptron`，则分别变成**支持向量机**（**SVM**）或感知机。这是另外两种线性分类器。'
- en: '**Gradie****nt****descent** is an optimization algorithm that aims to find
    a local minimum in a function by iteratively moving in the direction of steepest
    descent. The direction of the steepest descent is found using calculus, hence
    the term *gradient*. If you imagine the objective (loss) function as a curve,
    the gradient descent algorithm blindly lands on a random point on this curve and
    uses the gradient at the point it is on as a guiding stick to move to a local
    minimum step by step. Usually, the loss function is chosen to be a convex one
    so that its local minima is also its global one. In**the **stochastic** version
    of **gradient descent**, rather than calculating the gradient for the entire training
    data, the estimator''s weights are updated with each training sample. Gradient
    descent is covered in more detail in [Chapter 7](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=33&action=edit),
    *Neural Networks – Here Comes the Deep Learning*.** **## Classifying the Iris
    dataset using logistic regression'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度下降**是一种优化算法，旨在通过迭代地沿着最陡下降的方向移动来找到函数的局部最小值。最陡下降的方向通过微积分求得，因此称之为*梯度*。如果你将目标（损失）函数想象成一条曲线，梯度下降算法会盲目地选择曲线上的一个随机点，并利用该点的梯度作为指导，逐步向局部最小值移动。通常，损失函数选择为凸函数，这样它的局部最小值也就是全局最小值。在**随机**梯度下降的版本中，估算器的权重在每个训练样本上都会更新，而不是对整个训练数据计算梯度。梯度下降的更多细节内容可以参考[第7章](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=33&action=edit)，《神经网络——深度学习来临》。**
    **## 使用逻辑回归对鸢尾花数据集进行分类'
- en: 'We will load the Iris dataset into a data frame. The following is a similar
    block of code to the one used in [Chapter 2](http://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=25&action=edit),
    *Making Decisions with Trees*, to load the dataset:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把鸢尾花数据集加载到数据框中。以下代码块与在[第2章](http://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=25&action=edit)《使用树做决策》中使用的代码类似，用于加载数据集：
- en: '[PRE30]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, we will use `cross_validate` to evaluate the accuracy of the `LogisticRegression`
    algorithm using six-fold cross-validation, as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用`cross_validate`通过六折交叉验证来评估`LogisticRegression`算法的准确性，具体如下：
- en: '[PRE31]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Running the preceding code will give us a set of accuracy scores with a 95%
    confidence interval that ranges between `0.95` and `1.00`. Running the same code
    for the decision tree classifier gives us a confidence interval that ranges between
    `0.93` and `0.99`.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码将给我们一组准确率得分，并且其95%的置信区间在`0.95`和`1.00`之间。运行相同的代码进行决策树分类器训练时，得到的置信区间在`0.93`和`0.99`之间。
- en: Since we have three classes here, the coefficients calculated for each class
    boundary are separate from the others. After we train the logistic regression
    algorithm once more without the `cross_validate` wrapper, we can access the coefficients
    via `coef_`. We can also access the intercepts via `intercept_`.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们这里有三个类别，因此为每个类别边界计算的系数与其他类别是分开的。在我们再次训练逻辑回归算法且不使用`cross_validate`包装器之后，我们可以通过`coef_`访问系数。我们也可以通过`intercept_`访问截距。
- en: 'In the next code snippet, I will be using a dictionary comprehension. In Python,
    one way to create the `[0, 1, 2, 3]` list is by using the `[i for i in range(4)]`
    list comprehension. This basically executes the loop to populate the list. Similarly,
    the `[''x'' for i in range(4)]` list comprehension will create the `[''x'', ''x'',
    ''x, ''x'']` list. Dictionary comprehension works in the same fashion. For example,
    the `{str(i): i for i in range(4)}` line of code will create the`{''0'': 0, ''1'':
    1, ''2'': 2, ''3'': 3}` dictionary.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '在下一段代码中，我将使用字典推导式。在 Python 中，创建`[0, 1, 2, 3]`列表的一种方法是使用`[i for i in range(4)]`列表推导式。这基本上执行循环来填充列表。同样，`[''x''
    for i in range(4)]`列表推导式将创建`[''x'', ''x'', ''x'', ''x'']`列表。字典推导式以相同的方式工作。例如，`{str(i):
    i for i in range(4)}`这一行代码将创建`{''0'': 0, ''1'': 1, ''2'': 2, ''3'': 3}`字典。'
- en: 'The following code puts the coefficients into a data frame. It basically creates
    a dictionary whose keys are the class IDs and maps each ID to a list of its corresponding
    coefficients. Once the dictionary is created, we convert it into a data frame
    and add the intercepts to the data frame before displaying it:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将系数放入数据框中。它基本上创建了一个字典，字典的键是类别 ID，并将每个 ID 映射到其相应系数的列表。一旦字典创建完成，我们将其转换为数据框，并在显示之前将截距添加到数据框中：
- en: '[PRE32]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Don''t forget to scale your features before training. Then, you should get
    a coefficient data frame that looks like this:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，别忘了对特征进行缩放。然后，你应该得到一个看起来像这样的系数数据框：
- en: '![](img/a1f7d610-7309-4052-9edd-39bd765144c6.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1f7d610-7309-4052-9edd-39bd765144c6.png)'
- en: 'The table in the preceding screenshot shows the following:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 上面截图中的表格显示了以下内容：
- en: From the first row, we can tell that the increase in sepal length is correlated
    with classes 1 and 2 more than the remaining class, based on the positive sign
    of classes 1 and class 2's coefficients.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从第一行可以看出，花萼长度的增加与类别 1 和类别 2 的相关性高于其他类别，这是基于类别 1 和类别 2 系数的正号。
- en: Having a linear model here means that the class boundaries will not be limited
    to horizontal and vertical lines, as in the case of decision trees, but they will
    take linear forms.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里使用线性模型意味着类别边界不会像决策树那样局限于水平和垂直线，而是会呈现线性形态。
- en: To better understand this, in the next section, we will draw the logistic regression
    classifier's decision boundaries and compare them to those of decision trees.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这一点，在下一部分中，我们将绘制逻辑回归分类器的决策边界，并将其与决策树的边界进行比较。
- en: Understanding the classifier's decision boundaries
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解分类器的决策边界
- en: 'By seeing the decision boundaries visually, we can understand why the model
    makes certain decisions. Here are the steps for plotting those boundaries:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 通过可视化决策边界，我们可以理解模型为什么做出某些决策。以下是绘制这些边界的步骤：
- en: 'We start by creating a function that takes the classifier''s object and data
    samples and then plots the decision boundaries for that particular classifier
    and data:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先创建一个函数，该函数接受分类器的对象和数据样本，然后为特定的分类器和数据绘制决策边界：
- en: '[PRE33]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then, we split our data into training and test sets:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将数据分为训练集和测试集：
- en: '[PRE34]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To be able to visualize things easily, we are going to use two features. In
    the following code, we will train a logistic regression model and a decision tree
    model, and then compare their decision boundaries when trained on the same data:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了方便可视化，我们将使用两个特征。在下面的代码中，我们将训练一个逻辑回归模型和一个决策树模型，然后在相同数据上训练后比较它们的决策边界：
- en: '[PRE35]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Running this code will give us the following graphs:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码将给我们以下图表：
- en: '![](img/1b01de5a-6f69-4161-b6e9-8690fbfeb07a.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b01de5a-6f69-4161-b6e9-8690fbfeb07a.png)'
- en: 'In the preceding graph, the following is observed:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图表中，观察到以下内容：
- en: The logistic regression model did not perform well this time when only two features
    were used. Nevertheless, what we care about here is the shape of the boundaries.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这次，当只使用两个特征时，逻辑回归模型的表现并不好。然而，我们关心的是边界的形状。
- en: It's clear that the boundaries on the left are not horizontal and vertical lines
    as on the right. While the ones on the right can be composed of multiple line
    fragments, the ones on the left can only be made of continuous lines.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 很明显，左侧的边界不像右侧那样是水平和垂直的线。虽然右侧的边界可以由多个线段组成，但左侧的边界只能由连续的线组成。
- en: Getting to know additional linear classifiers
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解额外的线性分类器
- en: 'Before ending this chapter, it is useful to highlight some additional linear
    classification algorithms:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本章之前，有必要强调一些额外的线性分类算法：
- en: '**SGD** is a versatile solver. As mentioned earlier, it can perform a logistic
    regression classification in addition to SVM and perceptron classification, depending
    on the loss function used. It also allows regularized penalties.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SGD**是一种多功能求解器。如前所述，它可以执行逻辑回归分类，以及SVM和感知机分类，这取决于使用的损失函数。它还允许进行正则化惩罚。'
- en: '**The r****ide****classifier** converts class labels into `1` and `-1` and
    treats the problem as a regression task. It also deals well with non-binary classification
    tasks. Due to its design, it uses a different set of solvers, so it''s worth trying
    as it may be quicker to learn when dealing with a large number of classes.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**r****ide****分类器**将类别标签转换为`1`和`-1`，并将问题视为回归任务。它还能够很好地处理非二分类任务。由于其设计，它使用不同的求解器，因此在处理大量类别时，它可能会更快地学习，值得尝试。'
- en: '**Line****ar****Support****Vector****Classification** (**LinearSVC**) is another
    linear model. Rather than log loss, it uses the `hinge` function, which aims to
    find class boundaries where the samples of each class are as far as possible from
    the boundaries. This is not to be confused with SVMs. Contrary to their linear
    cousins, SVMs are non-linear algorithms, due to what is known as the kernel trick.
    SVMs are not as commonly used as they used to be a couple of decades ago, and
    they are beyond the scope of this book.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性支持向量分类**（**LinearSVC**）是另一个线性模型。与对数损失不同，它使用`hinge`函数，旨在找到类别边界，使得每个类别的样本尽可能远离边界。这与支持向量机（SVM）不同。与线性模型相反，SVM是一种非线性算法，因为它采用了所谓的核技巧。SVM不再像几十年前那样广泛使用，且超出了本书的范围。'
- en: Summary
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Linear models are found everywhere. Their simplicity, as well as the capabilities
    they offer—such as regularization—makes them popular among practitioners. They
    also share many concepts with neural networks, which means that understanding
    them will help you in later chapters.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型无处不在。它们的简单性以及提供的功能——例如正则化——使得它们在实践者中非常受欢迎。它们还与神经网络共享许多概念，这意味着理解它们将有助于你在后续章节的学习。
- en: Being linear isn't usually a limiting factor as long as we can get creative
    with our feature transformation. Furthermore, in higher dimensions, the linearity
    assumption may hold more often than we think. That's why it is advised to always
    start with a linear model and then decide whether you need to go for a more advanced
    model.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 线性通常不是限制因素，只要我们能通过特征转换发挥创意。此外，在更高维度下，线性假设可能比我们想象的更常见。这就是为什么建议总是从线性模型开始，然后再决定是否需要选择更高级的模型。
- en: Having that said, it can sometimesbe tricky to figure out the best configurations
    for your linear model or decide on which solver to use. In this chapter, we learned
    about using cross-validation to fine-tune a model's hyperparameters. We have also
    seen the different hyperparameters and solvers available, with tips for when to
    use each one.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，有时确实很难确定线性模型的最佳配置或决定使用哪种求解器。在本章中，我们学习了如何使用交叉验证来微调模型的超参数。我们还了解了不同的超参数和求解器，并获得了何时使用每一个的提示。
- en: So far, for all the datasets that we have dealt with in the first two chapters,
    we were lucky to have the data in the correct format. We have dealt only with
    numerical data with no missing values. Nevertheless, in real-world scenarios,
    this is rarely the case.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在我们处理的前两章中的所有数据集上，我们很幸运数据格式是正确的。我们只处理了没有缺失值的数值数据。然而，在实际场景中，这种情况很少见。
- en: In the next chapter, we are going to learn more about data preprocessing so
    that we can seamlessly continue working with more datasets and more advanced algorithms
    later on.******
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习更多关于数据预处理的内容，以便我们能够无缝地继续处理更多的数据集和更高级的算法。
