- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Deploying Models on a Mobile Platform
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在移动平台上部署模型
- en: In this chapter, we’ll discuss deploying **machine learning** (**ML**) models
    on mobile devices running on the Android operating system. ML can be used to improve
    the user experience on mobile devices, especially since we can create more autonomous
    features that allow our devices to learn and adapt to user behavior. For example,
    ML can be used for image recognition, allowing devices to identify objects in
    photos and videos. This feature can be useful for applications such as augmented
    reality or photo editing tools. Additionally, ML-powered speech recognition can
    enable voice assistants to better understand and respond to natural language commands.
    Another important benefit of the autonomous features development is that they
    can work without an internet connection. This is particularly useful in situations
    where connectivity is limited or unreliable, such as when traveling in remote
    areas or during natural disasters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论在运行Android操作系统的移动设备上部署**机器学习**（**ML**）模型。机器学习可以用于改善移动设备上的用户体验，尤其是我们可以创建更多自主功能，使我们的设备能够学习和适应用户行为。例如，机器学习可用于图像识别，使设备能够识别照片和视频中的对象。此功能对于增强现实或照片编辑工具等应用程序可能很有用。此外，由机器学习驱动的语音识别可以使语音助手更好地理解和响应自然语言命令。自主功能开发的另一个重要好处是它们可以在没有互联网连接的情况下工作。这在连接有限或不稳定的情况下尤其有用，例如在偏远地区旅行或自然灾害期间。
- en: Using C++ on mobile devices allows us to make programs faster and more compact.
    We can utilize as many computational resources as possible because modern compilers
    can optimize the program concerning the target CPU architecture. C++ doesn’t use
    an additional garbage collector for memory management, which can have a significant
    impact on program performance. Program size can be reduced because C++ doesn’t
    use an additional **Virtual Machine** (**VM**) and is compiled directly into machine
    code. Also, the use of C++ can help optimize battery life by more precise resource
    usage and adjusting accordingly. These facts make C++ the right choice for mobile
    devices with a limited amount of resources and can be used to solve heavy computational
    tasks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在移动设备上使用C++可以使我们编写更快速、更紧凑的程序。由于现代编译器可以针对目标CPU架构优化程序，我们可以利用尽可能多的计算资源。C++不使用额外的垃圾回收器进行内存管理，这可能会对程序性能产生重大影响。程序大小可以减小，因为C++不使用额外的**虚拟机**（**VM**）并且直接编译成机器码。此外，使用C++可以通过更精确的资源使用和相应调整来帮助优化电池寿命。这些事实使C++成为资源有限的移动设备的正确选择，并且可以用于解决重计算任务。
- en: By the end of the chapter, you will learn how to implement real-time object
    detection using a camera on an Android mobile platform using PyTorch and YOLOv5\.
    But this chapter is not a comprehensive introduction to Android development; rather,
    it can be used as a starting point for experiments with ML and computer vision
    on an Android platform. It provides a complete minimal example of the project
    that you will be able to extend for your task.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将学习如何使用PyTorch和YOLOv5在Android移动平台上通过相机实现实时对象检测。但本章并非Android开发的全面介绍；相反，它可以作为在Android平台上进行机器学习和计算机视觉实验的起点。它提供了一个完整的、所需的最小项目示例，你可以根据你的任务对其进行扩展。
- en: 'This chapter covers the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: Creating the minimal required project for Android C++ development
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建Android C++开发所需的最小项目
- en: Implementing the minimal required Kotlin functionality for object detection
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现对象检测所需的最小Kotlin功能
- en: Initializing the image-capturing session in the C++ part of the project
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在项目的C++部分初始化图像捕获会话
- en: Using OpenCV to process native camera images and draw results
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用OpenCV处理原生相机图像并绘制结果
- en: Using PyTorch script to launch the YOLOv5 model on the Android platform
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch脚本在Android平台上启动YOLOv5模型
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following are the technical requirements for this chapter:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为本章节的技术要求：
- en: Android Studio, **Android Software Development Kit** (**SDK**), and Android
    **Native Development** **Kit** (**NDK**)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Android Studio，**Android软件开发工具包**（**SDK**），以及Android **本地开发** **工具包**（**NDK**）
- en: The PyTorch library
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch库
- en: A modern C++ compiler with C++20 support
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持C++20的现代C++编译器
- en: CMake build system version >= 3.22
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CMake构建系统版本 >= 3.22
- en: 'The code files for this chapter can be found at the following GitHub repository:
    [https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-Second-edition/tree/main/Chapter14](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-Second-edition/tree/main/Chapter14).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在以下GitHub仓库中找到：[https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-Second-edition/tree/main/Chapter14](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-Second-edition/tree/main/Chapter14)。
- en: Developing object detection on Android
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Android上开发目标检测
- en: There are many approaches regarding how to deploy an ML model to a mobile device
    with Android. We can use PyTorch, ExecuTorch, TensorFlow Lite, NCNN, ONNX Runtime,
    or others. We’ll use the PyTorch framework in this chapter since we have discussed
    it in the previous chapters, and because it allows us to use almost any PyTorch
    model with minimal functional restrictions. Unfortunately, we will be able to
    use only the target device CPU for inference. Other frameworks, such as ExecuTorch,
    TensorFlow Lite, NCNN, and ONNX Runtime, allow you to use other inference backends,
    such as onboard GPU or **Neural Processing Unit** (**NPU**). However, this option
    also comes with a notable restriction, which is the lack of certain operators
    or functions, which can limit the types of models that can be deployed on mobile
    devices. Dynamic shape support is usually limited, making it difficult to handle
    data with varying dimensions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何将机器学习模型部署到Android移动设备上，有许多方法。我们可以使用PyTorch、ExecuTorch、TensorFlow Lite、NCNN、ONNX
    Runtime或其他工具。在本章中，我们将使用PyTorch框架，因为我们已经在之前的章节中讨论过它，并且因为它允许我们使用几乎任何PyTorch模型，同时功能限制最小。不幸的是，我们只能使用目标设备的CPU进行推理。其他框架，如ExecuTorch、TensorFlow
    Lite、NCNN和ONNX Runtime，允许您使用其他推理后端，例如板载GPU或**神经处理单元**（**NPU**）。然而，这个选项也带来一个显著的限制，即缺少某些操作符或函数，这可能会限制可以在移动设备上部署的模型类型。动态形状支持通常有限，这使得处理不同维度的数据变得困难。
- en: Another challenge is restricted control flow, which limits the ability to use
    models with dynamic computational graphs and implement advanced algorithms. These
    restrictions can make it more challenging to deploy ML models on mobile platforms
    using the frameworks described earlier. So, there is a trade-off between the model’s
    functionality and the required performance when you deploy ML models on mobile
    devices. To balance functionality and performance, developers must carefully evaluate
    their requirements and choose a framework that meets their specific needs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战是受限的控制流，这限制了使用具有动态计算图和实现高级算法的模型的能力。这些限制可能会使得使用前面描述的框架在移动平台上部署机器学习模型变得更加困难。因此，当在移动设备上部署机器学习模型时，模型的功能和所需性能之间存在权衡。为了平衡功能和性能，开发者必须仔细评估他们的需求，并选择满足他们特定需求的框架。
- en: The mobile version of the PyTorch framework
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch框架的移动版本
- en: There is an available binary distribution of PyTorch for mobile devices available
    in the Maven repository named `org.pytorch:pytorch_android_lite`. However, this
    distribution is outdated. So, to use the most recent version, we need to build
    it from source code. We can do this in the same way as we compile its regular
    version but with additional CMake parameters to enable mobile mode. You also have
    to install the Android NDK, which includes an appropriate version of the C/C++
    compiler and the Android native libraries that are required to build the application.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Maven仓库中有一个名为`org.pytorch:pytorch_android_lite`的PyTorch移动设备二进制分发版。然而，这个分发版已经过时。因此，要使用最新版本，我们需要从源代码构建它。我们可以像编译其常规版本一样做，但需要额外的CMake参数来启用移动模式。您还必须安装Android
    NDK，它包括适当的C/C++编译器和构建应用程序所需的Android原生库。
- en: 'The simplest way to install Android development tools is to download the Android
    Studio IDE and use the SDK Manager tool from that. You can find the SDK Manager
    under the `cmdline-tools` package. However, you need to have Java in your system;
    for Ubuntu, you can install Java as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Android开发工具最简单的方法是下载Android Studio IDE，并使用该IDE中的SDK Manager工具。您可以在`cmdline-tools`包下找到SDK
    Manager。然而，您需要在系统中安装Java；对于Ubuntu，您可以按照以下方式安装Java：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following command line script shows you how to install all required packages
    for CLI development:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令行脚本展示了如何安装CLI开发所需的所有包：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, we used the `sdkmanager` manager utility to install all required components
    with appropriate versions. Using this script, the path to NDK will be as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`sdkmanager`管理工具来安装所有所需的组件及其适当的版本。使用此脚本，NDK的路径将如下所示：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Having installed build tools and NDK, we can move on to the PyTorch mobile version
    compilation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了构建工具和 NDK 后，我们可以继续编译 PyTorch 移动版本。
- en: 'The following code snippet shows you how to use the command line environment
    to check out PyTorch and build it:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何使用命令行环境检出 PyTorch 并构建它：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we assumed that `/home/[USER]` is the user’s home directory. The main
    requirement when it comes to building the mobile version of PyTorch is to declare
    the `ANDROID_NDK` environmental variable, which should point to the Android NDK
    installation directory. The `ANDROID_ABI` environment variable can be used to
    specify the `arm64-v8a` architecture.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们假设 `/home/[USER]` 是用户的家目录。构建 PyTorch 移动版本时的主要要求是声明 `ANDROID_NDK` 环境变量，它应该指向
    Android NDK 安装目录。可以使用 `ANDROID_ABI` 环境变量来指定 `arm64-v8a` 架构。
- en: We used the `build_android.sh` script from the PyTorch source code distribution
    to build mobile PyTorch binaries. This script uses the CMake command internally,
    which is why it takes CMake parameter definitions as arguments. Notice that we
    passed the `BUILD_CAFFE2_MOBILE=OFF` parameter to disable building the mobile
    version of `Caffe2`, which is hard to use in the current version because the library
    is deprecated. The second important parameter we used was `BUILD_SHARED_LIBS=ON`,
    which enabled us to build shared libraries. Also, we disabled the Vulkan API support
    by using `DUSE_VULKAN=OFF` because it’s still experimental and has some compilation
    problems. The other parameters that were configured were the Python installation
    paths for intermediate build code generation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 PyTorch 源代码分发中的 `build_android.sh` 脚本来构建移动 PyTorch 二进制文件。这个脚本内部使用 CMake
    命令，这就是为什么它需要将 CMake 参数定义作为参数。请注意，我们传递了 `BUILD_CAFFE2_MOBILE=OFF` 参数来禁用构建 `Caffe2`
    的移动版本，因为当前版本中该库已被弃用，难以使用。我们使用的第二个重要参数是 `BUILD_SHARED_LIBS=ON`，这使我们能够构建共享库。我们还通过使用
    `DUSE_VULKAN=OFF` 禁用了 Vulkan API 支持，因为它仍然是实验性的，并且存在一些编译问题。其他配置的参数是用于中间构建代码生成的
    Python 安装路径。
- en: Now that we have the mobile PyTorch libraries, that is, `libc10.so` and `libtorch.so`,
    we can start developing the application. We are going to build an object detection
    application based on the YOLOv5 neural network architecture.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了移动 PyTorch 库，即 `libc10.so` 和 `libtorch.so`，我们可以开始开发应用程序了。我们将基于 YOLOv5
    神经网络架构构建一个目标检测应用程序。
- en: YOLOv5 is an object detection model based on the **You Only Look Once** (**YOLO**)
    architecture. It’s a state-of-the-art deep learning model that can detect objects
    in images and videos with high accuracy and speed. The model is relatively small
    and lightweight, making it easy to deploy on resource-constrained devices. Also,
    it’s fast enough, which is important for real-time applications that analyze a
    real-time video stream. It’s open source software, which means that developers
    can freely access the code and modify it to suit their needs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv5 是一种基于 **You Only Look Once** (**YOLO**) 架构的目标检测模型。它是一个最先进的深度学习模型，能够以高精度和速度在图像和视频中检测物体。该模型相对较小且轻量，便于在资源受限的设备上部署。此外，它足够快速，这对于需要分析实时视频流的实时应用来说非常重要。它是一个开源软件，这意味着开发者可以自由访问代码并根据他们的需求进行修改。
- en: Using TorchScript for a model snapshot
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TorchScript 进行模型快照
- en: In this section, we will discuss how to get the YOLOv5 model TorchScript file
    so that we can use it in our mobile application. In the previous chapters, we
    discussed how to save and load model parameters and how to use the ONNX format
    to share models between frameworks. When we use the PyTorch framework, there is
    another method we can use to share models between the Python API and C++ API called
    **TorchScript**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何获取 YOLOv5 模型的 TorchScript 文件，以便我们可以在我们的移动应用程序中使用它。在前面的章节中，我们讨论了如何保存和加载模型参数，以及如何使用
    ONNX 格式在框架之间共享模型。当我们使用 PyTorch 框架时，我们还可以使用另一种方法在 Python API 和 C++ API 之间共享模型，称为
    **TorchScript**。
- en: This method uses real-time model tracing to get a special type of model definition
    that can be executed by the PyTorch engine, regardless of API. In PyTorch, only
    the Python API can create such definitions, but we can use the C++ API to load
    the model and execute it. Also, the mobile version of the PyTorch framework doesn’t
    allow us to program neural networks with a full-featured C++ API. However, as
    was said earlier, TorchScript allows us to export and run models with complex
    control flow and dynamic shapes, which is not fully possible now for ONNX and
    other formats used in other mobile frameworks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法使用实时模型跟踪来获取一种特殊类型的模型定义，该定义可以由PyTorch引擎执行，不受API限制。在PyTorch中，只有Python API可以创建此类定义，但我们可以使用C++
    API来加载模型并执行它。此外，PyTorch框架的移动版本不允许我们使用功能齐全的C++ API来编程神经网络。然而，正如之前所说，TorchScript允许我们导出和运行具有复杂控制流和动态形状的模型，这在目前对于ONNX和其他在其他移动框架中使用的格式来说是不完全可能的。
- en: For now, the YOLOv5 PyTorch model can be directly exported only into TorchScript
    for inference on mobile CPUs. For example, there are YOLOv5 models adapted for
    TensorFlow Lite and NCNN frameworks, but we will not discuss these cases because
    we are using PyTorch mostly. I have to say that using NCNN will allow you to use
    a mobile GPU though the Vulkan API and using TensorFlow Lite or ONNX Runtime for
    Android will allow you to use a mobile NPU for some devices. However, you will
    need to adapt the model into another format by reducing some functionality or
    developing it with TensorFlow.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，YOLOv5 PyTorch模型可以直接导出为TorchScript，以便在移动CPU上进行推理。例如，有针对TensorFlow Lite和NCNN框架优化的YOLOv5模型，但我们将不讨论这些情况，因为我们主要使用PyTorch。我必须说，使用NCNN将允许你通过Vulkan
    API使用移动GPU，而使用TensorFlow Lite或ONNX Runtime for Android将允许你使用某些设备的移动NPU。然而，你需要通过减少一些功能或使用TensorFlow进行开发来将模型转换为另一种格式。
- en: 'So, in this example, we are going to use the TorchScript model to perform object
    detection. To get the YOLOv5 model, we have to perform the following steps:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这个例子中，我们将使用TorchScript模型来进行目标检测。为了获取YOLOv5模型，我们必须执行以下步骤：
- en: 'Clone the model repository from GitHub and install dependencies; run these
    commands in the terminal:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从GitHub克隆模型仓库并安装依赖项；在终端中运行以下命令：
- en: '[PRE4]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: python export.py --weights yolov5s.torchscript --include torchscript --optimize
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: python export.py --weights yolov5s.torchscript --include torchscript --optimize
- en: '[PRE5]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The script from the second step automatically traces the model and saves the
    TorchScript file for us. After we made these steps, there will be the `yolo5s.torchscript`
    file, which we will be able to load and use in C++.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步的脚本会自动跟踪模型并为我们保存TorchScript文件。在我们完成这些步骤后，将会有`yolo5s.torchscript`文件，我们将能够加载并在C++中使用它。
- en: Now, we have all the prerequisites to move on and make an Android Studio project
    for our application.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经具备了所有先决条件，可以继续创建我们的Android Studio项目。
- en: The Android Studio project
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Android Studio项目
- en: 'In this section, we will use the Android Studio IDE to create our mobile application.
    We can use a default `objectdetection` and select **Kotlin** as the programming
    language, then Android Studio will create a particular project structure; the
    following sample shows the most valuable parts of it:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Android Studio IDE来创建我们的移动应用程序。我们可以使用默认的`objectdetection`并选择**Kotlin**作为编程语言，然后Android
    Studio将创建一个特定的项目结构；以下示例显示了其最有价值的部分：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `cpp` folder contains the C++ part of the whole project. In this project,
    the Android Studio IDE created the C++ part as a native shared library project
    that had been configured with the CMake build generation system. The `java` folder
    contains the Kotlin part of the project. In our case, it is a single file that
    defines the main activity—the object that’s used as a connection between the UI
    elements and event handlers. The `res` folder contains project resources, such
    as UI elements and string definitions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`cpp`文件夹包含整个项目的C++部分。在这个项目中，Android Studio IDE将C++部分创建为一个配置了CMake构建生成系统的本地共享库项目。`java`文件夹包含项目的Kotlin部分。在我们的案例中，它是一个定义主活动的单个文件——该活动用于将UI元素和事件处理器连接起来。`res`文件夹包含项目资源，例如UI元素和字符串定义。'
- en: 'We also need to create the `jniLibs` folder, under the `main` folder, with
    the following structure:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要在`main`文件夹下创建一个名为`jniLibs`的文件夹，其结构如下：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Android Studio requires us to place additional native libraries in such folders
    to correctly package them into the final application. It also allows the `arm64-v8a`
    folder because they have only been compiled for this CPU architecture. If you
    have libraries for other architectures, you have to create folders with corresponding
    names.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Android Studio要求我们将额外的本地库放置在这样的文件夹中，以便正确地将它们打包到最终应用程序中。它还允许`arm64-v8a`文件夹，因为它们只编译了这种CPU架构。如果您有其他架构的库，您必须创建具有相应名称的文件夹。
- en: 'Also, in the previous subsection, we learned how to get the YOLOv5 torch script
    model. The model file and corresponding file, along with the class IDs, should
    be placed in the `assets` folder. This folder should be created beside the `JniLibs`
    folder on the same folder level, as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在上一小节中，我们学习了如何获取YOLOv5 torch脚本模型。模型文件及其对应文件，以及类ID，应放置在`assets`文件夹中。此文件夹应创建在`JniLibs`文件夹旁边，处于同一文件夹级别，如下所示：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The file that maps string class names to numerical IDs, which the model returns,
    can be downloaded from [https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml](https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 将字符串类名称映射到模型返回的数值ID的文件可以从[https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml](https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml)下载。
- en: In our example, we simply convert the YAML file into the text one to make its
    parsing simpler.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们简单地将YAML文件转换为文本文件，以简化其解析。
- en: The IDE uses the Gradle build system for project configuration, so there are
    two files named `build.gradle.kts`, one for the application module and another
    one for the project properties. Look at the `build.gradle` file for the application
    module in our example. There are two variables that define paths to the PyTorch
    source code folder and to the OpenCV Android SDK folder. You need to update their
    values if you change these paths. The prebuilt OpenCV Android SDK can be downloaded
    from the official GitHub repository ([https://github.com/opencv/opencv/releases](https://github.com/opencv/opencv/releases))
    and simply unpacked.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该IDE使用Gradle构建系统进行项目配置，因此有两个名为`build.gradle.kts`的文件，一个用于应用程序模块，另一个用于项目属性。查看我们示例中的应用程序模块的`build.gradle`文件。有两个变量定义了PyTorch源代码文件夹和OpenCV
    Android SDK文件夹的路径。如果您更改这些路径，则需要更新它们的值。预构建的OpenCV Android SDK可以从官方GitHub仓库([https://github.com/opencv/opencv/releases](https://github.com/opencv/opencv/releases))下载，并直接解压。
- en: The Kotlin part of the project
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目的Kotlin部分
- en: In this project, we are going to use the native C++ part to draw the captured
    picture with bounding boxes and class labels for detected objects. So, there will
    be no UI code and declarations in the Kotlin part. However, the Kotlin part will
    used to request and check required camera access permissions. Also, it will start
    a camera capture session if permissions are granted. All Kotlin code will be in
    the `MainActivity.kt` file.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将使用本地的C++部分来绘制捕获的图片，并带有检测到的对象的边界框和类标签。因此，Kotlin部分将没有UI代码和声明。然而，Kotlin部分将用于请求和检查所需的相机访问权限。如果权限被授予，它还将启动相机捕获会话。所有Kotlin代码都将位于`MainActivity.kt`文件中。
- en: Preserving camera orientation
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保留相机方向
- en: 'In our project, we skip the implementation of device rotation handling to make
    code simpler and show just the most interesting parts of working with the object
    detection model. So, to make our code stable, we have to disable the landscape
    mode, which can be done in the `AndroidManifest.xml` file, as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的项目中，我们跳过了设备旋转处理的实现，以使代码更简单，并仅展示与目标检测模型一起工作的最有趣的部分。因此，为了使我们的代码稳定，我们必须禁用横屏模式，这可以在`AndroidManifest.xml`文件中完成，如下所示：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We added the screen orientation instruction to the activity entity. This is
    not a good solution because there are devices that work only in landscape mode
    and our application will not work with them. In a real production-ready application,
    you should handle different orientation modes; for example, for most smartphones,
    this dirty solution should work.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将屏幕方向指令添加到活动实体中。这不是一个好的解决方案，因为有些设备只能在横屏模式下工作，我们的应用程序将无法与它们一起工作。在实际的生产型应用程序中，您应该处理不同的方向模式；例如，对于大多数智能手机，这种脏解决方案应该可以工作。
- en: Handling camera permission requests
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理相机权限请求
- en: There are no C++ APIs in Android NDK to request permissions. We can request
    the required permission only from the Java/Kotlin side or with JNI from C++. It’s
    simpler to write the Kotlin code to request the camera permission than to write
    JNI calls.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在Android NDK中没有C++ API来请求权限。我们只能从Java/Kotlin侧或通过JNI从C++请求所需的权限。编写Kotlin代码请求相机权限比编写JNI调用要简单。
- en: 'The first step is modifying the declaration of the `MainActivity` class to
    be able to process permission request results. It’s done as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是修改`MainActivity`类的声明，以便能够处理权限请求结果。如下所示完成：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here, we inherited the `MainActivity` class from the `OnRequestPermissionsResultCallback`
    interface. It gives us the possibility to override the `onRequestPermissionsResult`
    method where we will be able to check a result. However, to get a result, we have
    to make a request first, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们继承了`MainActivity`类自`OnRequestPermissionsResultCallback`接口。这给了我们重写`onRequestPermissionsResult`方法的可能性，在那里我们将能够检查结果。然而，要获取结果，我们必须首先发出请求，如下所示：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We overrode the `onResume` method of the `Activity` class. This method is called
    every time when our application starts to work or is resumed from the background.
    We initialized the `cameraPermission` variable with the required camera permission
    constant value. Then, we checked whether we already granted this permission using
    the `checkSelfPermission` method. If we don’t have the camera permission, we ask
    for it with the `requestPermissions` method.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重写了`Activity`类的`onResume`方法。此方法在每次我们的应用程序开始工作或从后台恢复时都会被调用。我们使用所需的相机权限常量值初始化了`cameraPermission`变量。然后，我们使用`checkSelfPermission`方法检查我们是否已经授予了此权限。如果没有相机权限，我们使用`requestPermissions`方法请求它。
- en: 'Notice that we used the `CAM_PERMISSION_CODE` code to identify our request
    in the callback method. If we were granted access to a camera, we tried to get
    the back-facing camera ID and initialize the object detection pipeline for this
    camera. If we can’t get access to a camera, we finish the Android activity with
    the `finish` method and the corresponding message. In the `onRequestPermissionsResult`
    method, we check if the required permission was granted, as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在回调方法中使用了`CAM_PERMISSION_CODE`代码来识别我们的请求。如果我们被授予访问相机的权限，我们尝试获取背面相机的ID并为该相机初始化对象检测管道。如果我们无法访问相机，我们使用`finish`方法和相应的消息结束Android活动。在`onRequestPermissionsResult`方法中，我们检查是否已授予所需的权限，如下所示：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: At first, we called the parent method to preserve the standard application behavior.
    Then, we checked the permission identification code, `CAM_PERMISSION_CODE`, and
    whether or not the permission was granted. In the failure case, we just show the
    error message and finish the Android activity.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们调用了父方法以保留标准应用程序行为。然后，我们检查了权限识别代码`CAM_PERMISSION_CODE`以及权限是否被授予。在失败情况下，我们只显示错误消息并结束Android活动。
- en: 'As we said before, in the success case, we looked for the back-facing camera
    ID, which is done as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所说，在成功的情况下，我们寻找背面相机的ID，如下所示：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We got the instance of the `CameraManager` object and used this object to iterate
    over every camera on a device. For each camera object, we asked for its characteristics,
    supported hardware level, and where this camera faces. If a camera is a regular
    legacy device and faces back, we return its ID. If we didn’t find a suitable device,
    we returned an empty string.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获取了`CameraManager`对象的实例，并使用此对象遍历设备上的每个相机。对于每个相机对象，我们询问其特征、支持的硬件级别以及该相机面向的方向。如果一个相机是常规的遗留设备并且面向背面，我们返回其ID。如果我们没有找到合适的设备，我们返回一个空字符串。
- en: 'Having granted the camera access permission and the camera ID, we called the
    `initObjectDetection` function to start image capturing and object detection.
    This and the `stopObjectDetection` function are functions provided through the
    JNI from the C++ part to the Kotlin part. The `stopObjectDetection` function is
    used to stop the camera capturing session, as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在授予相机访问权限和相机ID之后，我们调用了`initObjectDetection`函数来开始图像捕获和对象检测。这个函数和`stopObjectDetection`函数是通过JNI从C++部分提供给Kotlin部分的函数。`stopObjectDetection`函数用于停止相机捕获会话，如下所示：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the overridden `onPause` activity method, we just stopped the camera capturing
    session. This method is called every time the Android application is closed or
    goes into the background.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在重写的`onPause`活动方法中，我们只是停止了相机捕获会话。此方法在每次Android应用程序关闭或进入后台时都会被调用。
- en: Native library loading
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原生库加载
- en: There are two methods, `initObjectDetection` and `stopObjectDetection`, which
    are JNI calls to the native library functions that are implemented with C++. To
    connect the native library with the Java or Kotlin code, we use JNI. This is a
    standard mechanism that’s used for calling C/C++ functions from Kotlin or Java.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个方法，`initObjectDetection`和`stopObjectDetection`，它们是JNI调用原生库中用C++实现的函数。为了将原生库与Java或Kotlin代码连接起来，我们使用JNI。这是一个标准的机制，用于从Kotlin或Java调用C/C++函数。
- en: 'First, we have to load the native library with the `System.LoadLibrary` call
    and place it in the companion object for our activity. Then, we have to define
    the methods that are implemented in the native library by declaring them as `external`.
    The following snippet shows how to define these methods in Kotlin:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须使用`System.LoadLibrary`调用加载原生库，并将其放置在我们活动的伴随对象中。然后，我们必须通过将它们声明为`external`来定义原生库中实现的方法。以下代码片段展示了如何在Kotlin中定义这些方法：
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Such declarations allow Kotlin to find the corresponding native library binary,
    load it, and access the functions. JNI works by providing a set of APIs that allow
    Java code to call into native code and vice versa. The JNI API consists of a number
    of functions that can be called from Java or native code. These functions allow
    you to perform tasks, such as creating and accessing Java objects from native
    code, calling Java methods from native code, and accessing native data structures
    from Java.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的声明允许Kotlin找到相应的原生库二进制文件，加载它，并访问函数。JNI通过提供一组API来实现，允许Java代码调用原生代码，反之亦然。JNI
    API由一组可以从Java或原生代码调用的函数组成。这些函数允许你执行诸如从原生代码创建和访问Java对象、从原生代码调用Java方法以及从Java访问原生数据结构等任务。
- en: Internally, JNI works by mapping Java objects and types to their corresponding
    native counterparts. This mapping is done using the `JNIEnv` interface, which
    provides access to the `JNIEnv` is used to find the corresponding native method
    and pass it the necessary arguments. Similarly, when a native method returns a
    value, `JNIEnv` is used to convert the native value to a Java object. The JVM
    manages memory for both Java and native objects. However, native code must explicitly
    allocate and free its own memory. JNI provides functions for allocating and freeing
    memory, as well as for copying data between Java and native memory. JNI code must
    be thread-safe. This means that any data accessed by JNI must be properly synchronized
    to avoid race conditions. Using JNI can have performance implications. Native
    code is typically faster than Java code, but there is overhead associated with
    calling into native code through JNI.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 内部，JNI通过将Java对象和类型映射到它们的对应原生版本来工作。这种映射是通过`JNIEnv`接口完成的，它提供了访问`JNIEnv`的方法，用于查找相应的原生方法并传递必要的参数。同样，当原生方法返回一个值时，使用`JNIEnv`将原生值转换为Java对象。JVM管理Java和原生对象内存。然而，原生代码必须显式地分配和释放自己的内存。JNI提供了分配和释放内存的函数，以及用于在Java和原生内存之间复制数据的函数。JNI代码必须是线程安全的。这意味着任何由JNI访问的数据都必须正确同步，以避免竞争条件。使用JNI可能会有性能影响。原生代码通常比Java代码快，但通过JNI调用原生代码会有开销。
- en: In the next section, we will discuss the C++ part of the project.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论项目的C++部分。
- en: The native C++ part of the project
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目的原生C++部分
- en: The main functionality of this example project is implemented in the native
    C++ part. It’s designed to use the OpenCV library to deal with camera images and
    the PyTorch framework for object detection model inference. Such an approach allows
    you to port this solution to another platform if needed and allows you to use
    standard desktop instruments, such as OpenCV and PyTorch, to develop and debug
    algorithms that will be used on mobile platforms.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例项目的主要功能是在原生C++部分实现的。它旨在使用OpenCV库处理摄像头图像，并使用PyTorch框架进行目标检测模型推理。这种做法允许你在需要时将此解决方案移植到其他平台，并允许你使用标准桌面工具，如OpenCV和PyTorch，来开发和调试将在移动平台上使用的算法。
- en: There are two main C++ classes in this project. The `Detector` class is the
    application facade that implements a connection with the Android activity image-capturing
    pipeline, and delegates object detection to the second class, `YOLO`. The `YOLO`
    class implements the object detection model loading and its inference.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目中主要有两个C++类。`Detector`类是应用程序外观，它实现了与Android活动图像捕获管道的连接，并将对象检测委托给第二个类`YOLO`。`YOLO`类实现了对象检测模型的加载及其推理。
- en: The following subsections will describe the implementation details of these
    classes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的子节将描述这些类的实现细节。
- en: Initialization of object detection with JNI
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 JNI 初始化对象检测
- en: 'We finished our discussion of the Kotlin part by talking about the JNI function
    declarations. The corresponding C++ implementation for `initObjectDetection` and
    `stopObjectDetection` are located in the `native-lib.cpp` file. This file is automatically
    created by the Android Studio IDE for the native activity projects. The following
    code snippet shows the `initObjectDetection` function definition:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过讨论 JNI 函数声明结束了 Kotlin 部分的讨论。`initObjectDetection` 和 `stopObjectDetection`
    的相应 C++ 实现在 `native-lib.cpp` 文件中。此文件由 Android Studio IDE 自动创建，用于原生活动项目。以下代码片段展示了
    `initObjectDetection` 函数的定义：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We followed JNI rules to make the function declaration correct and visible from
    the Java/Kotlin part. The name of the function includes the full Java package
    name, including namespaces, and our first two required parameters are the `JNIEnv*`
    and `jobject` types. The third parameter is the string and corresponds to the
    camera ID; this is the parameter that exists in the Kotlin declaration of the
    function.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循 JNI 规则，使函数声明正确且在 Java/Kotlin 部分可见。函数名称包括完整的 Java 包名，包括命名空间，我们前两个必需的参数是
    `JNIEnv*` 和 `jobject` 类型。第三个参数是字符串，对应于相机 ID；这是在函数的 Kotlin 声明中存在的参数。
- en: In the function implementation, we checked whether the `ObjectDetector` object
    was already instantiated and, in this case, we called the `allow_camera_session`
    method with the camera ID and then called the `configure_resources` method. These
    calls make the `ObjectDetector` object remember what camera to use and initialize,
    configure the output window, and initialize the image-capturing pipeline.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数实现中，我们检查 `ObjectDetector` 对象是否已经实例化，如果是这样，我们使用相机 ID 调用 `allow_camera_session`
    方法，然后调用 `configure_resources` 方法。这些调用使 `ObjectDetector` 对象记住要使用哪个相机以及初始化，配置输出窗口，并初始化图像捕获管道。
- en: 'The second function we used in the Kotlin part is the `stopObjectDetection`,
    and its implementation is done as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kotlin 部分我们使用的第二个函数是 `stopObjectDetection`，其实现如下：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, we just released resources used for the image-capturing pipeline because
    when the application is suspended, access to the camera device is blocked. When
    the application is activated again, the `initObjectDetection` function will be
    called and the image-capturing pipeline will be initialized again.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只是释放了用于图像捕获管道的资源，因为当应用程序挂起时，对相机设备的访问被阻止。当应用程序再次激活时，`initObjectDetection`
    函数将被调用，图像捕获管道将重新初始化。
- en: 'You can see that we used the `LOGI` and the `LOGE` functions, which are defined
    as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到我们使用了 `LOGI` 和 `LOGE` 函数，其定义如下：
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We defined these functions to log messages into the Android `logcat` subsystem
    more easily. This series of functions uses the same tag for logging and has fewer
    arguments than the original `__android_log_xxx` functions. Also, the log level
    was encoded in the function name.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了这些函数，以便更容易地将消息记录到 Android 的 `logcat` 子系统中。这一系列函数使用相同的标签进行记录，并且比原始的 `__android_log_xxx`
    函数具有更少的参数。此外，日志级别被编码在函数名称中。
- en: Main application loop
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主应用程序循环
- en: This project will use the Native App Glue library. This is a library for Android
    developers that helps to create native applications. It provides an abstraction
    layer between the Java code and the native code, making it easier to develop applications
    using both languages.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 此项目将使用 Native App Glue 库。这是一个帮助 Android 开发者创建原生应用的库。它提供了一个抽象层，在 Java 代码和原生代码之间，使得使用这两种语言开发应用程序变得更加容易。
- en: 'The use of this library allows us to have the standard `main` function with
    a loop that runs continuously, updating the UI, processing user input, and responding
    to system events. The following code snippet shows how we implemented the main
    function in the `native-lib.cpp` file:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个库，我们可以拥有一个带有循环的标准 `main` 函数，该循环持续运行，更新 UI，处理用户输入，并响应系统事件。以下代码片段展示了我们如何在
    `native-lib.cpp` 文件中实现 `main` 函数：
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This `android_main` function takes the instance of the `android_app` type, instead
    of regular `argc` and `argv` parameters. The `android_app` is a C++ class that
    provides access to the Android framework and allows you to interact with system
    services. Also, you can use it to access the device hardware, such as sensors
    and cameras.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`android_main` 函数接收 `android_app` 类型的实例，而不是常规的 `argc` 和 `argv` 参数。`android_app`
    是一个 C++ 类，它提供了对 Android 框架的访问权限，并允许你与系统服务进行交互。此外，你可以使用它来访问设备硬件，例如传感器和摄像头。'
- en: The `android_main` main function is the starting point for our native module.
    So, we initialized the global `object_detector_` object here, and it became available
    for the `initObjectDetection` and `stopObjectDetection` functions. For initialization,
    the `ObjectDetector` instance takes the pointer to the `android_app` object.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`android_main` 主函数是我们本地模块的起点。因此，我们在这里初始化了全局的 `object_detector_` 对象，使其对 `initObjectDetection`
    和 `stopObjectDetection` 函数可用。对于初始化，`ObjectDetector` 实例接收 `android_app` 对象的指针。'
- en: Then, we attached the command processing function to the Android application
    object. Finally, we started the main loop, and it worked until the application
    was destroyed (closed). In this loop, we used the `ALooper_pollOnce` Android NDK
    function to get a pointer to the commands (events) poller object.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将命令处理函数附加到 Android 应用程序对象上。最后，我们启动了主循环，它一直工作到应用程序被销毁（关闭）。在这个循环中，我们使用 `ALooper_pollOnce`
    Android NDK 函数获取到命令（事件）轮询对象的指针。
- en: We called the `process` method of this object to dispatch the current command
    to our `ProcessAndroidCmd` function through the `app` object. At the end of the
    loop, we used our object detector object to grab the current camera picture and
    process it in the `draw_frame` method.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用了该对象的 `process` 方法，通过 `app` 对象将当前命令派发到我们的 `ProcessAndroidCmd` 函数。在循环结束时，我们使用我们的目标检测器对象抓取当前摄像头图像，并在
    `draw_frame` 方法中对其进行处理。
- en: 'The `ProcessAndroidCmd` function is implemented as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`ProcessAndroidCmd` 函数的实现如下：'
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, we processed only two commands that correspond to the application window
    initialization and termination. We used them to initialize and clear the image-capturing
    pipeline in the object detector. When the window is created, we configure its
    dimensions according to the capturing resolution. The window termination command
    allows us to clear capturing resources to prevent access to the already blocked
    camera device.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只处理了两个与应用程序窗口初始化和终止相对应的命令。我们使用它们在目标检测器中初始化和清除图像捕获管道。当窗口创建时，我们根据捕获分辨率配置其尺寸。窗口终止命令允许我们清除捕获资源，以防止访问已阻塞的摄像头设备。
- en: That is all the information about the `native-lib.cpp` file. The next subsections
    will look at the `ObjectDetector` class implementation details.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是关于 `native-lib.cpp` 文件的所有信息。接下来的小节将探讨 `ObjectDetector` 类的实现细节。
- en: The ObjectDetector class overview
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`ObjectDetector` 类概述'
- en: 'This is the main facade of the whole object detection pipeline of our application.
    The following list shows the functionality items it implements:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是应用程序整个目标检测管道的主界面。以下列表显示了它所实现的功能项：
- en: Camera device access management
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄像头设备访问管理
- en: Application window dimensions configuration
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序窗口尺寸配置
- en: Image-capturing pipeline management
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像捕获管道管理
- en: Camera image converting into OpenCV matrix objects
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将摄像头图像转换为 OpenCV 矩阵对象
- en: Drawing an object detection result into the application window
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将对象检测结果绘制到应用程序窗口中
- en: Delegating the object detection to the YOLO inference object
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将对象检测委托给 YOLO 推理对象
- en: 'Before we start looking at these item details, let’s see how the constructor,
    the destructor, and some helper methods are implemented. The constructor implementation
    is as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始查看这些项目细节之前，让我们看看构造函数、析构函数和一些辅助方法的实现。构造函数的实现如下：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We just saved the pointer to the `android_app` object and created the `YOLO`
    class inference object. Also, we used the `android_app` object to get a pointer
    to the `AssetManager` object, which is used to load files packaged into the **Android
    Application Package** (**APK**). The destructor is implemented as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是保存了 `android_app` 对象的指针，并创建了 `YOLO` 类推理对象。此外，我们使用 `android_app` 对象获取到 `AssetManager`
    对象的指针，该对象用于加载打包到 **Android 应用程序包**（**APK**）中的文件。析构函数的实现如下：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We called the `release_resources` method, which is where we close the opened
    camera device and clear capturing pipeline objects. The following code snippet
    shows the methods that are used from the Kotlin part through the `initObjectDetection`
    function:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用了`release_resources`方法，这是关闭已打开的相机设备和清除捕获管道对象的地方。以下代码片段显示了通过`initObjectDetection`函数从
    Kotlin 部分使用的方法：
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In `allow_camera_session`, we saved the camera ID string; the device with this
    ID will be opened in the `configure_resources` method. As we already know, the
    camera ID will be passed to `ObjectDetector` only if the required permission is
    granted and there is a back-facing camera on the Android device. So, we defined
    `is_session_allowed` as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在`allow_camera_session`中，我们保存了相机 ID 字符串；具有此 ID 的设备将在`configure_resources`方法中打开。正如我们所知，只有当所需的权限被授予且
    Android 设备上有后置摄像头时，相机 ID 才会传递给`ObjectDetector`。因此，我们定义了`is_session_allowed`如下：
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here, we just checked if a camera ID is not empty.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们只是检查了相机 ID 是否不为空。
- en: The following subsections will show the main functionality items in detail.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节将详细展示主要功能项。
- en: Camera device and application window configuration
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相机设备和应用程序窗口配置
- en: 'There is the `create_camera` method in the `ObjectDetection` class that implements
    the creation of a camera manager object and a camera device opening as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在`ObjectDetection`类中有一个名为`create_camera`的方法，它实现了创建相机管理器对象和打开相机设备，如下所示：
- en: '[PRE25]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`camera_mgr_` is the `ObjectDetector` member variable and after initialization,
    it is used to open a camera device. The pointer to the opened camera device will
    be stored in the `camera_device_` member variable. Also, notice that we used the
    camera ID string to open the particular device. The `camera_device_callbacks`
    variable is defined as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`camera_mgr_`是`ObjectDetector`成员变量，初始化后用于打开相机设备。打开的相机设备的指针将存储在`camera_device_`成员变量中。注意，我们使用相机
    ID 字符串打开特定设备。`camera_device_callbacks`变量定义如下：'
- en: '[PRE26]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We defined the `ACameraDevice_stateCallbacks` structure object with references
    to functions that simply report if the camera is opened or closed. These handlers
    can do some more useful work in other applications, but we can’t initialize them
    with nulls due to the API requirements.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了带有指向函数引用的`ACameraDevice_stateCallbacks`结构对象，这些函数简单地报告相机是否已打开或关闭。在其他应用程序中，这些处理程序可以执行一些更有用的操作，但由于
    API 要求，我们无法用空值初始化它们。
- en: 'The `create_camera` method is called in the `configure_resources` method of
    the `ObjectDetection` class. This method is called every time the application
    is activated and it has the following implementation:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_camera`方法在`ObjectDetection`类的`configure_resources`方法中被调用。每次应用程序激活时都会调用此方法，其实现如下：'
- en: '[PRE27]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the beginning, we checked that there are all required resources: the camera
    ID, the `android_app` object, and that this object has a pointer to the application
    window. Then, we created a camera manager object and opened a camera device. Using
    the camera manager, we got the camera sensor orientation to configure the appropriate
    width and height for the application window. Also, using values for image capture
    width and height, we configured the window dimensions, as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始时，我们检查了所有必需的资源：相机 ID、`android_app`对象，以及该对象是否指向应用程序窗口。然后，我们创建了一个相机管理器对象并打开了一个相机设备。使用相机管理器，我们获取了相机传感器的方向以配置应用程序窗口的适当宽度和高度。此外，使用图像捕获宽度和高度值，我们配置了窗口尺寸，如下所示：
- en: '[PRE28]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Here, we used the `ACameraManager_getCameraCharacteristics` function to get
    the camera metadata characteristics object. Then, we read the `ACAMERA_SENSOR_ORIENTATION`
    property with the `ACameraMetadata_getConstEntry` function. After, we chose the
    appropriate width and height order based on the orientation used with the `ANativeWindow_setBuffersGeometry`
    function to set application output window dimensions and rendering buffer format.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们使用了`ACameraManager_getCameraCharacteristics`函数来获取相机元数据特性对象。然后，我们使用`ACameraMetadata_getConstEntry`函数读取`ACAMERA_SENSOR_ORIENTATION`属性。之后，我们根据使用的方向，基于`ANativeWindow_setBuffersGeometry`函数选择适当的宽度和高度顺序，以设置应用程序输出窗口尺寸和渲染缓冲区格式。
- en: The format we set is `32`-bit `800` for height and `600` for width in portrait
    mode. This orientation handling is very simple and is needed only to work with
    output window buffers correctly. Previously, we disabled the landscape mode for
    our application so we will ignore the camera sensor orientation in the camera
    image decoding.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置的格式是竖屏模式下的`32`位`800`像素高度和`600`像素宽度。这种方向处理非常简单，只需正确处理输出窗口缓冲区即可。之前，我们已禁用了应用中的横屏模式，因此我们将忽略相机传感器的方向在相机图像解码中的使用。
- en: At the end of the `configure_resources` method, we created the camera reader
    object and initialized the capturing pipeline.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在`configure_resources`方法结束时，我们创建了相机读取器对象并初始化了捕获管道。
- en: Image-capturing pipeline construction
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像捕获管道构建
- en: 'Previously, we saw that before the capturing pipeline initialization, we created
    the image reader object. It’s done in the `create_image_reader` method, as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们看到在捕获管道初始化之前，我们创建了图像读取器对象。这是在`create_image_reader`方法中完成的，如下所示：
- en: '[PRE29]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We used `AImageReader_new` to create the `AImageReader` object with a particular
    width and height, the YUV format, and four image buffers. The width and height
    values we used were the same that were used for the output window dimensions configuration.
    The YUV format was used because it’s the native image format for most camera devices.
    Four image buffers were used to make image capturing sightly independent from
    their processing. It means that the image reader will fill one image buffer with
    camera data while we are reading another buffer and processing it.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`AImageReader_new`创建了一个具有特定宽度和高度、YUV格式和四个图像缓冲区的`AImageReader`对象。我们使用的宽度和高度值与输出窗口尺寸配置中使用的相同。我们使用YUV格式，因为它是最多相机设备的原生图像格式。使用四个图像缓冲区是为了使图像捕获稍微独立于它们的处理。这意味着当我们在读取另一个缓冲区并处理它时，图像读取器将用相机数据填充一个图像缓冲区。
- en: 'The capture session initialization is a complex process that requires several
    objects’ instantiation and their connection with each other. The `create_session`
    method implements it as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 捕获会话初始化是一个复杂的过程，需要创建几个对象并将它们相互连接。`create_session`方法如下实现：
- en: '[PRE30]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We started with getting a native window from the image reader object and acquiring
    it. The window acquisition means that we took the reference to the window and
    the system should not delete it. This image reader window will be used as output
    for the capturing pipeline, so camera images will be drawn into it.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从图像读取器对象中获取并获取了一个原生窗口。窗口获取意味着我们获取了窗口的引用，系统不应删除它。这个图像读取器窗口将被用作捕获管道的输出，因此相机图像将被绘制到其中。
- en: Then, we created the session output object and the container for the session
    output. The capturing session can have several outputs and they should be placed
    into a container. Every session output is a connection object for a concrete surface
    or a window output; in our case, it’s the image reader window.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建了会话输出对象和会话输出的容器。会话可以有几个输出，它们应该放入一个容器中。每个会话输出都是一个具体表面或窗口输出的连接对象；在我们的情况下，它是图像读取器窗口。
- en: Having configured session outputs, we created the capture request object and
    made sure that its output target was the image reader window. We configured the
    capture request for our opened camera device and the preview mode. After that,
    we instantiated the capturing session object and pointed it to the opened camera
    device, which had the container with the outputs we created earlier.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置了会话输出后，我们创建了捕获请求对象，并确保其输出目标是图像读取窗口。我们为打开的相机设备配置了捕获请求，并设置了预览模式。之后，我们实例化了捕获会话对象，并将其指向打开的相机设备，该设备包含我们之前创建的输出容器。
- en: 'Finally, we started the capturing by setting the repeated request for the session.
    The connection between the session and capture request is the follows: we created
    the capturing session that was configured with a list of possible outputs, and
    the capture request specifies what surfaces will actually be used. There can be
    several capture requests and several outputs. In our case, we have a single capture
    request with a single output that will be continuously repeated. So, in general,
    we will capture real-time pictures for the camera like a video stream. The following
    picture shows the logical scheme of an image data flow in the capturing session:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过设置会话的重复请求来开始捕获。会话与捕获请求之间的关系如下：我们创建了一个配置了可能输出列表的捕获会话，捕获请求指定了实际将使用的表面。可能有多个捕获请求和多个输出。在我们的情况下，我们有一个单一的捕获请求和一个单一的输出，它将连续重复。因此，总的来说，我们将像视频流一样捕获摄像头的实时图片。以下图片显示了捕获会话中图像数据流的逻辑方案：
- en: '![Figure 14.1 – The logical data flow in a capturing session](img/B19849_14_1.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图14.1 – 捕获会话中的逻辑数据流](img/B19849_14_1.jpg)'
- en: Figure 14.1 – The logical data flow in a capturing session
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1 – 捕获会话中的逻辑数据流
- en: This is not the actual data flow scheme but the logical one that shows how the
    capture session objects are connected. The dotted line shows the request path
    and the solid line shows the logical image data path.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是实际的数据流方案，而是逻辑方案，显示了捕获会话对象是如何连接的。虚线表示请求路径，实线表示逻辑图像数据路径。
- en: The capture image and the output window buffer management
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 捕获图像和输出窗口缓冲区管理
- en: 'When we discussed the main application loop, we mentioned the `draw_frame`
    method, which is called in this loop after command processing. This method is
    used to take a captured image from the image reader object, then detect objects
    on it and draw the detection results in the application window. The following
    code snippet shows the `draw_fame` method implementation:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论主应用程序循环时，我们提到了`draw_frame`方法，该方法在处理命令后在此循环中被调用。此方法用于从图像读取对象中获取捕获的图像，然后检测其上的对象并在应用程序窗口中绘制检测结果。以下代码片段显示了`draw_frame`方法实现：
- en: '[PRE31]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We acquired the next image received by the image reader object. Remember that
    we initialized it to have four image buffers. So, we acquire images from these
    buffers one by one in the main loop, and while we process one image, the capturing
    session fills another one that has already been processed. It’s done in a circular
    manner. Having the image from a camera, we acquired and locked the application
    window, but if the lock fails, we delete the current image reference, stop processing,
    and go to the next iteration of the main loop. Otherwise, if we successfully lock
    the application window, we process the current image, detect objects on it, and
    draw detection results into an application window—this is done in the `process_image`
    method. This method takes the `AImage` and the `ANativeWindow_Buffer` objects.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获取了图像读取对象接收到的下一张图像。记住我们初始化了它以拥有四个图像缓冲区。因此，我们在主循环中逐个从这些缓冲区获取图像，同时我们处理一张图像时，捕获会话会填充另一个已经处理过的图像。这是以循环方式完成的。拥有来自摄像头的图像，我们获取并锁定应用程序窗口，但如果锁定失败，我们删除当前图像引用，停止处理，并进入主循环的下一个迭代。否则，如果我们成功锁定应用程序窗口，我们处理当前图像，检测其上的对象，并将检测结果绘制到应用程序窗口中——这是在`process_image`方法中完成的。此方法接受`AImage`和`ANativeWindow_Buffer`对象。
- en: When we lock the application window, we get the pointer to the internal buffer
    that will be used for drawing. After we process the image and draw results, we
    unlock the application window to make its buffer available for the system, release
    the reference to the window, and delete the reference to the image object. So,
    this method is mostly about resource management, and the real image processing
    is done in the `process_image` method, which we will discuss in the following
    subsection.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们锁定应用程序窗口时，我们获得用于绘制的内部缓冲区的指针。在处理图像并绘制结果后，我们解锁应用程序窗口以使其缓冲区可供系统使用，释放对窗口的引用，并删除对图像对象的引用。因此，这种方法主要关于资源管理，而真正的图像处理是在`process_image`方法中完成的，我们将在下一小节中讨论。
- en: The captured image processing
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 捕获图像处理
- en: 'The `process_image` method implements the following tasks:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`process_image`方法实现了以下任务：'
- en: Convert Android YUV image data into the OpenCV matrix.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Android YUV图像数据转换为OpenCV矩阵。
- en: Dispatch the image matrix to the YOLO object detector.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像矩阵调度到YOLO对象检测器。
- en: Draw detection results into the OpenCV matrix.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将检测结果绘制到OpenCV矩阵中。
- en: Copy the OpenCV results matrix into the RGB (red, blue, green) window buffer.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 OpenCV 结果矩阵复制到 RGB（红色、蓝色、绿色）窗口缓冲区。
- en: 'Let’s see implementations for these tasks one by one. The `process_image` method
    signature looks as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一查看这些任务的实现。`process_image` 方法的签名如下所示：
- en: '[PRE32]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This method takes the application window buffer object for results drawing
    and the image object for actual processing. To be able to process an image, we
    have to convert it into some appropriate data-structure format; in our case, this
    is the OpenCV matrix. We start with image format properties checking as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法接受用于结果绘制的应用程序窗口缓冲区对象和用于实际处理的图像对象。为了能够处理图像，我们必须将其转换为某种适当的数据结构格式；在我们的情况下，这就是
    OpenCV 矩阵。我们首先检查图像格式属性，如下所示：
- en: '[PRE33]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We checked that the image format is YUV (Luminance (Y), blue luminance (U),
    and red luminance (V)) and the image has three planes, so we can proceed with
    its conversion. Then, we got image dimensions, which will be used later. After
    that, we verified the input data we extracted from the YUV plane data as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查了图像格式为 YUV（亮度（Y）、蓝色亮度（U）和红色亮度（V）），并且图像有三个平面，因此我们可以继续其转换。然后，我们获得了图像尺寸，这些尺寸将在以后使用。之后，我们验证了从
    YUV 平面数据中提取的输入数据，如下所示：
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We got strides, data sizes, and pointers to the actual YUV plane data. In this
    format, the image data is split into three components: luma (`y`), representing
    brightness, and two chroma components (`u` and `v`), which represent color information.
    The `y` component is usually stored at full resolution, while the `u` and `v`
    components may be subsampled. This allows for more efficient storage and transmission
    of video data. The Android YUV image uses the half-sized resolution for `u` and
    `v`. The strides will allow us to correctly access the row data in the plane buffers;
    these strides depend on the image resolution and a data memory layout.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得了步长、数据大小以及实际 YUV 平面数据的指针。在此格式中，图像数据分为三个组件：亮度（`y`），表示亮度，以及两个色度组件（`u` 和 `v`），它们表示颜色信息。`y`
    成分通常以全分辨率存储，而 `u` 和 `v` 成分可能被子采样。这允许更有效地存储和传输视频数据。Android YUV 图像使用 `u` 和 `v` 的半分辨率。步长将使我们能够正确访问平面缓冲区中的行数据；这些步长取决于图像分辨率和数据内存布局。
- en: 'Having the YUV plane data and its strides and lengths, we convert them into
    OpenCV matrix objects, as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有 YUV 平面数据及其步长和长度后，我们将它们转换为 OpenCV 矩阵对象，如下所示：
- en: '[PRE35]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We created the two `cv::Size` objects to store the original image size for
    the Y plane and the half size for the `u` and `v` planes. Then, we used these
    sizes, pointers to data, and strides to create an OpenCV matrix for every plane.
    We didn’t copy actual data into the OpenCV matrix objects; they will use data
    pointers that were passed for initialization. Such a view-creation approach saves
    memory and computational resources. The `y`-plane matrix has the 8-bit single-channel
    type but the `u` and `v` matrices have the 8-bit 2-channel type. We can use these
    matrices with the OpenCV `cvtColorTwoPlane` function to convert them into the
    RGBA format as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了两个 `cv::Size` 对象来存储 Y 平面的原始图像大小以及 `u` 和 `v` 平面的半大小。然后，我们使用这些大小、数据指针和步长为每个平面创建一个
    OpenCV 矩阵。我们没有将实际数据复制到 OpenCV 矩阵对象中；它们将使用传递给初始化的数据指针。这种视图创建方法节省了内存和计算资源。`y`-平面矩阵具有
    8 位单通道类型，而 `u` 和 `v` 矩阵具有 8 位双通道类型。我们可以使用这些矩阵与 OpenCV 的 `cvtColorTwoPlane` 函数将它们转换为
    RGBA 格式，如下所示：
- en: '[PRE36]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We used the address difference to determine the ordering of the u and v planes:
    a positive difference indicates the NV12 format, while a negative difference indicates
    the NV21 format. `NV12` and `NV21` are types of the YUV format that differ in
    the order of the `u` and `v` components in the chroma plane. In `NV12`, the `u`
    component precedes the `v` component, while in `NV21`, it’s the opposite. Such
    plane ordering plays a role in memory consumption and image processing performance,
    so the choice of which to use depends on the actual task and project. Also, the
    format can depend on the actual camera device, which is why we added this detection.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用地址差异来确定 `u` 和 `v` 平面的顺序：正差异表示 NV12 格式，而负差异表示 NV21 格式。`NV12` 和 `NV21` 是 YUV
    格式的类型，它们在色度平面中 `u` 和 `v` 成分的顺序上有所不同。在 `NV12` 中，`u` 成分先于 `v` 成分，而在 `NV21` 中则相反。这种平面顺序在内存消耗和图像处理性能中发挥作用，因此选择使用哪种格式取决于实际任务和项目。此外，格式可能取决于实际的相机设备，这就是我们添加此检测的原因。
- en: The `cvtColorTwoPlane` function takes the `y`-plane and `uv`-plane matrices
    as input arguments and outputs the RGBA image matrix into the `rgba_img_` variable.
    The last argument is the flag that tells the function what actual conversion it
    should perform. Now, this function can convert only YUV formats into RGB or RGBA
    formats.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`cvtColorTwoPlane`函数接受`y`平面和`uv`平面矩阵作为输入参数，并将RGBA图像矩阵输出到`rgba_img_`变量中。最后一个参数是告诉函数它应该执行什么实际转换的标志。现在，这个函数只能将YUV格式转换为RGB或RGBA格式。'
- en: 'As we said before, our application works only in portrait mode, but to make
    the image look normal, we need to rotate it as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说的，我们的应用程序仅在纵向模式下工作，但为了使图像看起来正常，我们需要将其旋转如下：
- en: '[PRE37]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Android camera sensors return camera images rotated even if we fixed our orientation,
    so we used the `cv::rotate` function to make it look vertical.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们的方向已经固定，Android相机传感器返回的图像仍然是旋转的，所以我们使用了`cv::rotate`函数使其看起来是垂直的。
- en: 'Having prepared the RGBA image, we pass it to the `YOLO` object detector and
    get the detection results. For every result item, we draw rectangles and labels
    on the image matrix we have already used for detection. These steps are implemented
    as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好RGBA图像后，我们将其传递给`YOLO`对象检测器，并获取检测结果。对于每个结果项，我们在已经用于检测的图像矩阵上绘制矩形和标签。这些步骤的实现方式如下：
- en: '[PRE38]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We called the `detect` method of the `YOLO` object and got the `results` container.
    This method will be discussed later. Then, for each item in the container, we
    draw a bounding box and a text label for the detected object. We used the OpenCV
    `rectangle` function with the `rgba_img_` destination image argument. Also, the
    text was rendered into the `rgba_img_` object. The detection result is the structure
    defined in the `yolo.h` header file as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用了`YOLO`对象的`detect`方法，并获取了`results`容器。这个方法将在稍后讨论。然后，对于容器中的每个项，我们为检测到的对象绘制一个边界框和文本标签。我们使用了带有`rgba_img_`目标图像参数的OpenCV
    `rectangle`函数。此外，文本也被渲染到`rgba_img_`对象中。检测结果是`yolo.h`头文件中定义的结构，如下所示：
- en: '[PRE39]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: So, a detection result has the class index and name properties, the model confidence
    score, and the bounding box in the image coordinates. For our results visualization,
    we used only rectangle and class name properties.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个检测结果具有类别索引和名称属性、模型置信度分数以及图像坐标中的边界框。对于我们的结果可视化，我们只使用了矩形和类别名称属性。
- en: 'The last task that the `process_image` method does is to render the resulting
    image into the application window buffer. It’s implemented as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`process_image`方法最后执行的任务是将生成的图像渲染到应用程序窗口缓冲区中。其实现方式如下：'
- en: '[PRE40]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We created the OpenCV `buffer_mat` matrix to wrap the given window buffer. Then,
    we simply used the OpenCV `copyTo` method to put the RGBA image with rendered
    rectangles and class labels into the `buffer_mat` object. `buffer_mat` is the
    OpenCV view for the Android window buffer. We created it to follow the window
    buffer format we configured in the `configure_resources` method, the `WINDOW_FORMAT_RGBA_8888`
    format. So, we created the OpenCV matrix with the 8-bit 4-channel type and used
    the buffer stride information to satisfy memory layout access. Such a view allows
    us to write less code and use OpenCV routines for memory management.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了OpenCV的`buffer_mat`矩阵来包装给定的窗口缓冲区。然后，我们简单地使用OpenCV的`copyTo`方法将带有渲染矩形和类别标签的RGBA图像放入`buffer_mat`对象中。`buffer_mat`是OpenCV对Android窗口缓冲区的视图。我们创建它是为了遵循在`configure_resources`方法中配置的窗口缓冲区格式，即`WINDOW_FORMAT_RGBA_8888`格式。因此，我们创建了一个8位4通道类型的OpenCV矩阵，并使用缓冲区步进信息来满足内存布局访问。这样的视图使我们能够编写更少的代码，并使用OpenCV例程进行内存管理。
- en: We discussed the main facade of our object detection application and in the
    following subsections, we will discuss details of how the YOLO model inference
    is implemented and how its results are parsed into the `YOLOResult` structures.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了我们的对象检测应用程序的主要外观，在接下来的小节中，我们将讨论YOLO模型推理的实现细节以及其结果如何解析到`YOLOResult`结构中。
- en: The YOLO wrapper initialization
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: YOLO包装器初始化
- en: 'There is only the constructor and the `detect` method in the `YOLO` class public
    API. We already saw that the `YOLO` object is initialized in the `ObjectDetector`
    class constructor, and the `detect` method is used in the `process_image` method.
    The `YOLO` class constructor takes only the asset manager object as a single argument
    and is implemented as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`YOLO`类的公共API中只有构造函数和`detect`方法。我们已经看到`YOLO`对象是在`ObjectDetector`类的构造函数中初始化的，`detect`方法是在`process_image`方法中使用的。`YOLO`类的构造函数只接受资产管理器对象作为单个参数，其实现方式如下：'
- en: '[PRE41]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Remember that we added the `yolov5s.torchscript` and the `classes.txt` files
    to the `assets` folder of our project. These files can be accessed in the application
    with the `AAssetManager` class object; this object was taken from the `android_app`
    object in the `android_main` function. So, in the constructor, we loaded the model
    binary and classes list file with a call to the `read_asset` function. Then, the
    model binary data was used to load and initialize the PyTorch script module with
    the `torch::jit::_load_for_mobile` function.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们已经将`yolov5s.torchscript`和`classes.txt`文件添加到我们项目的`assets`文件夹中。这些文件可以通过`AAssetManager`类对象在应用程序中访问；此对象是从`android_main`函数中的`android_app`对象中获取的。因此，在构造函数中，我们通过调用`read_asset`函数加载模型二进制文件和类列表文件。然后，使用`torch::jit::_load_for_mobile`函数使用模型二进制数据加载和初始化PyTorch脚本模块。
- en: 'Notice that the scripted model should be saved with optimization for mobile
    and loaded with the corresponding function. When PyTorch for mobile was compiled,
    the regular `torch::jit::load` functionality was automatically disabled. Let’s
    look at the `read_asset` function that reads assets from the application bundle
    as `std::vector<char>` objects. The following code shows its implementation:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，脚本模型应该以针对移动设备优化的方式保存，并使用相应的函数加载。当编译PyTorch for mobile时，常规的`torch::jit::load`功能会自动禁用。让我们看看`read_asset`函数，该函数以`std::vector<char>`对象的形式从应用程序包中读取资源。以下代码展示了其实现：
- en: '[PRE42]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: There are four Android framework functions that we used to read an asset from
    the application bundle. The `AAssetManager_open` function opened the asset and
    returned the not null pointer to the `AAsset` object. This function assumes that
    the path to the asset is in the file path format and that the root of this path
    is the `assets` folder. After we opened the asset, we used the `AAsset_getLength`
    function to get the file size and allocated the memory for `std::vector<char>`
    with the `std::vector::resize` method. Then, we used the `AAsset_read()` function
    to read the whole file to the `buf` object.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了四个Android框架函数来从应用程序包中读取资产。`AAssetManager_open`函数打开了资产，并返回指向`AAsset`对象的非空指针。此函数假设资产的路径是文件路径格式，并且此路径的根是`assets`文件夹。在我们打开资产后，我们使用`AAsset_getLength`函数获取文件大小，并使用`std::vector::resize`方法为`std::vector<char>`分配内存。然后，我们使用`AAsset_read()`函数将整个文件读取到`buf`对象中。
- en: 'This function does the following:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数执行以下操作：
- en: It takes the pointer to the asset object to read from
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它获取要读取的资产对象的指针
- en: It takes the `void*` pointer to the memory buffer to read in
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要使用内存缓冲区的`void*`指针来读取
- en: It measures the size of the bytes to read
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它测量要读取的字节大小
- en: So, as you can see, the assets API is pretty much the same as the standard C
    library API for file operations. When we’d finished working with the asset object,
    we used the `AAsset_close` function to notify the system that we didn’t need access
    to this asset anymore. If your assets are in the `.zip` archive format, you should
    check the number of bytes returned by the `AAsset_read` function because the Android
    framework reads archives chunk by chunk.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正如你所看到的，资源API基本上与标准C库API的文件操作相同。当我们完成与资源对象的协作后，我们使用`AAsset_close`函数通知系统我们不再需要访问此资源。如果你的资源以`.zip`存档格式存储，你应该检查`AAsset_read`函数返回的字节数，因为Android框架是分块读取存档的。
- en: 'You might see that we didn’t pass the vector of chars directly to the `torch::jit::_load_for_mobile`
    function. This function doesn’t work with standard C++ streams and types; instead,
    it accepts a pointer to an object of the `caffe2::serialize::ReadAdapterInterface`
    class. The following code shows how you to make the concrete implementation of
    the `caffe2::serialize::ReadAdapterInterface` class, which wraps the `std::vector<char>`
    object:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到我们没有直接将字符向量传递给`torch::jit::_load_for_mobile`函数。此函数不与标准C++流和类型一起工作；相反，它接受一个指向`caffe2::serialize::ReadAdapterInterface`类对象的指针。以下代码展示了如何具体实现`caffe2::serialize::ReadAdapterInterface`类，该类封装了`std::vector<char>`对象：
- en: '[PRE43]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The `ReaderAdapter` class overrides two methods, `size` and `read`, from the
    `caffe2::serialize::ReadAdapterInterface` base class. Their implementations are
    pretty obvious: the `size` method returns the size of the underlying vector object,
    while the `read` method copies the `n` bytes (chars) from the vector to the destination
    buffer with the standard algorithm function, that is, `std::copy_n`.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReaderAdapter` 类重写了 `caffe2::serialize::ReadAdapterInterface` 基类中的两个方法，`size`
    和 `read`。它们的实现相当明显：`size` 方法返回底层向量对象的大小，而 `read` 方法使用标准算法函数 `std::copy_n` 将 `n`
    字节（字符）从向量复制到目标缓冲区。'
- en: 'To load class information, we used the `VectorStreamBuf` adapter class to convert
    `std::vector<char>` into the `std::istream` type object. It was done because the
    `YOLO::load_classes` method takes an object of the `std::istream` type. The `VectorStreamBuf`
    implementation is as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加载类信息，我们使用了 `VectorStreamBuf` 适配器类将 `std::vector<char>` 转换为 `std::istream`
    类型对象。这样做是因为 `YOLO::load_classes` 方法需要一个 `std::istream` 类型的对象。`VectorStreamBuf`
    的实现如下：
- en: '[PRE44]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We inherited from the `std::basic_streambuf` class and in the constructor,
    we initialized the `streambuf` internal data with char values from the input vector.
    Then, we used an object of this adapter class as regular C++ input stream. You
    can see it in the `load_classes` method implementation, which is shown in the
    following snippet:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 `std::basic_streambuf` 类继承，并在构造函数中，使用输入向量的字符值初始化 `streambuf` 内部数据。然后，我们使用此适配器类的对象作为常规
    C++ 输入流。您可以在以下代码片段中看到它，这是 `load_classes` 方法实现的一部分：
- en: '[PRE45]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The lines in `classes.txt` are in the following format:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`classes.txt` 中的行格式如下：'
- en: '[PRE46]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: So, we read this file line by line and split each line at the position of the
    first space character. The first part of each line is the class identifier, while
    the second one is the class name. To match the model’s evaluation result with
    the correct class name, we created the dictionary (map) object, where the key
    is the `id` value and the value is `label` (e.g., the class name)s.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们逐行读取此文件，并在第一个空格字符的位置拆分每一行。每一行的第一部分是类标识符，而第二部分是类名。为了将模型的评估结果与正确的类名匹配，我们创建了一个字典（映射）对象，其键是
    `id` 值，值是 `label`（例如，类名）。
- en: The YOLO detection inference
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: YOLO 检测推理
- en: 'The `detect` method of the `YOLO` class is the place where we do the actual
    object detection. This method takes the OpenCV matrix object that represents the
    RGB image as an argument and its implementation is as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`YOLO` 类的 `detect` 方法是我们进行实际对象检测的地方。此方法以表示 RGB 图像的 OpenCV 矩阵对象为参数，其实现如下：'
- en: '[PRE47]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We defined constants that represent the width and height of the model input;
    it’s `640` x `640` because the YOLO model was trained on images of this size.
    Using these constants, we resized the input image. Also, we removed the alpha
    channel and made the RGB image. We calculated scale factors for image dimensions,
    as they will be used to re-scale detected object boundaries to the original image
    size. Having scaled the image, we converted the OpenCV matrix into a PyTorch Tensor
    object using the `mat2tensor` function, whose implementation we will discuss later.
    The object type cast of the PyTorch Tensor object we added to the container of
    the `torch::jit::IValue` values was done automatically. There is a single element
    in this `inputs` container since the YOLO model takes a single RGB image input.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了代表模型输入宽度和高度的常量；它是 `640` x `640`，因为 YOLO 模型是在这种大小的图像上训练的。使用这些常量，我们调整了输入图像的大小。此外，我们移除了
    alpha 通道并制作了 RGB 图像。我们计算了图像尺寸的缩放因子，因为它们将被用来将检测到的对象边界重新缩放到原始图像大小。在调整了图像大小后，我们使用
    `mat2tensor` 函数将 OpenCV 矩阵转换为 PyTorch Tensor 对象，我们将在稍后讨论其实现。我们将 PyTorch Tensor
    对象添加到 `torch::jit::IValue` 值的容器中时，对象类型转换是自动完成的。在这个 `inputs` 容器中只有一个元素，因为 YOLO
    模型需要一个 RGB 图像输入。
- en: Then, we used the `forward` function of the YOLO `model_` object to perform
    inference. The PyTorch API script modules return the `torch::jit::Tuple` type.
    So, we explicitly cast the returned `torch::jit::IValue` object to the tuple and
    took the first element. This element was cast to the PyTorch `Tensor` object and
    the batch dimension was removed from it with the `squeeze` method. So, we got
    the `torch::Tensor` type `output` object of size `25200` x `85`. Here, `25200`
    is the number of detected objects and we will apply the non-max suppression algorithm
    to get the final reduced output. The `85` means `80` class scores, `4` bounding
    box locations (x, y, width, height), and `1` confidence score. The resulting tensor
    was parsed into the `YOLOResult` structures in the `output2results` method. As
    we said, we used the `non_max_suppression` method to select the best detection
    results.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用 YOLO `model_` 对象的 `forward` 函数进行推理。PyTorch API 脚本模块返回 `torch::jit::Tuple`
    类型。因此，我们显式地将返回的 `torch::jit::IValue` 对象转换为元组，并取第一个元素。该元素被转换为 PyTorch `Tensor`
    对象，并使用 `squeeze` 方法从其中移除批维度。因此，我们得到了大小为 `25200` x `85` 的 `torch::Tensor` 类型 `output`
    对象。其中，`25200` 是检测到的对象数量，我们将应用非极大值抑制算法以获得最终减少的输出。`85` 表示 `80` 个类别分数、`4` 个边界框位置（x,
    y, 宽度，高度）和 `1` 个置信度分数。结果张量在 `output2results` 方法中解析为 `YOLOResult` 结构。正如我们所说的，我们使用了
    `non_max_suppression` 方法来选择最佳检测结果。
- en: Let’s see the details of all the intermediate functions we used for inference.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们用于推理的所有中间函数的详细信息。
- en: Converting OpenCV matrix into torch::Tensor
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 OpenCV 矩阵转换为 torch::Tensor
- en: 'The `mat2tensor` function convents an OpenCV `mat` object into a `torch::Tensor`
    object and is implemented as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`mat2tensor` 函数将 OpenCV `mat` 对象转换为 `torch::Tensor` 对象，其实现如下：'
- en: '[PRE48]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We used the `torch::from_blob` function to create the torch `Tensor` object
    from the raw data. The data pointer is what we took from the OpenCV object with
    the `data` property. The shape we used, `[HIGHT, WIDTH, CHANNELS]`, follows the
    OpenCV memory layout where the last dimension is the channel number dimension.
    Then, we made tensor float and normalized it to the `[0,1]` interval. PyTorch
    and the YOLO model use different shape layouts to `[CHANNELS, HEIGHT, WIDTH]`.
    So, we transpose the tensor channels appropriately.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `torch::from_blob` 函数从原始数据创建 torch `Tensor` 对象。数据指针是我们从 OpenCV 对象的 `data`
    属性中获取的。我们使用的形状 `[HEIGHT, WIDTH, CHANNELS]` 遵循 OpenCV 内存布局，其中最后一个维度是通道号维度。然后，我们将张量转换为浮点数并归一化到
    `[0,1]` 区间。PyTorch 和 YOLO 模型使用不同的形状布局 `[CHANNELS, HEIGHT, WIDTH]`。因此，我们适当地转置张量通道。
- en: Processing model output tensor
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理模型输出张量
- en: 'The next function we used is `output2results`, which converts the output Tensor
    object into the vector of the `YOLOResult` structures. It has the following implementation:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的下一个函数是 `output2results`，它将输出 Tensor 对象转换为 `YOLOResult` 结构的向量。其实现如下：
- en: '[PRE49]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'In the beginning, we used the `accessor<float, 2>` method of the torch Tensor
    object to get a very useful accessor for the tensor. This accessor allowed us
    to use the square brackets operator to access the elements in a multidimensional
    tensor. The number `2` means that the tensor is 2D. Then, we made a loop over
    tensor rows because every row corresponds to a single detection result. Inside
    the loop, we did the following steps:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始时，我们使用 torch Tensor 对象的 `accessor<float, 2>` 方法来获取一个对张量非常有用的 accessor。这个
    accessor 允许我们使用方括号运算符访问多维张量中的元素。数字 `2` 表示张量是 2D 的。然后，我们对张量行进行循环，因为每一行对应一个单独的检测结果。在循环内部，我们执行以下步骤：
- en: We read the confidence score from the element with row index `4`.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从行索引 `4` 的元素中读取置信度分数。
- en: We continued result row processing if the confidence score was greater than
    the threshold.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果置信度分数大于阈值，我们继续处理结果行。
- en: We read the `0, 1, 2, 3` elements, which are the [x, y, width, height] coordinates
    of the bounding rectangle.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们读取 `0, 1, 2, 3` 元素，这些是边界矩形的 `[x, y, 宽度, 高度]` 坐标。
- en: Using the previously calculated scale factors, we converted these coordinates
    into the [left, top, width, height] format.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用先前计算的比例因子，我们将这些坐标转换为 `[左, 上, 宽度, 高度]` 格式。
- en: We read the elements 5-84, which are class probabilities, and selected the class
    with the maximum value.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们读取元素 5-84，这些是类别概率，并选择最大值的类别。
- en: We created the `YOLOResult` structure with calculated values and inserted it
    into the `results_` container.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用计算出的值创建 `YOLOResult` 结构并将其插入到 `results_` 容器中。
- en: 'The bounding box calculation was done as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 边界框计算如下：
- en: '[PRE50]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The YOLO model returns X and Y coordinates for the center of a rectangle so
    we converted them into the image(screen) coordinate system: to the top-left point.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO 模型返回矩形的中心 X 和 Y 坐标，因此我们将它们转换为图像（屏幕）坐标系：到左上角点。
- en: 'The class ID selection was implemented as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 ID 选择实现如下：
- en: '[PRE51]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We used the loop over the last elements that represent 79 class probabilities
    to select the index of the maximum value. This index was used as a class ID.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用遍历代表 79 个类别概率的最后元素来选择最大值的索引。这个索引被用作类别 ID。
- en: NMS and IoU
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NMS 和 IoU
- en: '**Non-Maximum Suppression** (**NMS**) and **Intersection over Union** (**IoU**)
    are two key algorithms used in YOLO for refining and filtering the output predictions
    to get the best results.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '**非极大值抑制**（**NMS**）和 **交并比**（**IoU**）是 YOLO 中用于精炼和过滤输出预测以获得最佳结果的两个关键算法。'
- en: 'NMS is used to suppress or eliminate duplicate detections that overlap with
    each other. It works by comparing the predicted bounding boxes from the network
    and removing those that have high overlaps with others. For example, if there
    are two bounding boxes predicted for the same object, NMS will keep only the one
    with the highest confidence score and discard the rest. The following picture
    shows how NMS works:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 非极大值抑制（NMS）用于抑制或消除相互重叠的重复检测。它通过比较网络预测的边界框并移除与其他边界框重叠度高的那些来实现。例如，如果有两个边界框被预测为同一对象，NMS
    将只保留置信度评分最高的那个，其余的将被丢弃。以下图片展示了 NMS 的工作原理：
- en: '![Figure 14.2 – NMS](img/B19849_14_2.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.2 – NMS](img/B19849_14_2.jpg)'
- en: Figure 14.2 – NMS
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2 – NMS
- en: 'IoU is another algorithm used in conjunction with NMS to measure the overlap
    between bounding boxes. IoU calculates the ratio of an intersection area to a
    union area between two boxes. The `0` to `1`, where `0` means no overlap, and
    `1` indicates perfect overlap. The following picture shows how IoU works:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: IoU 是另一种与 NMS 结合使用的算法，用于测量边界框之间的重叠。IoU 计算两个框之间交集面积与并集面积的比率。范围在 `0` 到 `1` 之间，其中
    `0` 表示没有重叠，`1` 表示完全重叠。以下图片展示了 IoU 的工作原理：
- en: '![Figure 14.3 – IoU](img/B19849_14_3.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.3 – IoU](img/B19849_14_3.jpg)'
- en: Figure 14.3 – IoU
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.3 – IoU
- en: 'We implemented NMS in the `non_max_suppression` method as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `non_max_suppression` 方法中实现了 NMS，如下所示：
- en: '[PRE52]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: At first, we sorted all detection results by confidence score in descending
    order. We marked all results as active. If a detection result is active, then
    we can compare another result with it, otherwise, the result is already suppressed.
    Then, every active detection result was sequentially compared with the following
    active results; remember that the container is sorted. The comparison was done
    by calculating the IoU value for bounding boxes and comparing the IoU value with
    a threshold. If the IoU value is greater than the threshold, we marked the result
    with a lower confidence value as non-active; we suppressed it. So, we defined
    the nested comparison loop. In the outer loop, we ignored suppressed results too.
    Also, this nested loop has the check for the maximum allowed number of results;
    refer to the use of the `nms_limit` value.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们按置信度分数降序对所有检测结果进行排序。我们将所有结果标记为活动状态。如果一个检测结果是活动的，那么我们可以将其与另一个结果进行比较，否则，该结果已被抑制。然后，每个活动检测结果依次与后续的活动结果进行比较；记住容器是排序的。比较是通过计算边界框的
    IoU 值并与阈值进行比较来完成的。如果 IoU 值大于阈值，我们将置信度值较低的结果标记为非活动状态；我们抑制了它。因此，我们定义了嵌套比较循环。在外层循环中，我们也忽略了被抑制的结果。此外，这个嵌套循环还有检查允许的最大结果数量的检查；请参阅
    `nms_limit` 值的使用。
- en: 'The IoU algorithm for two bounding boxes is implemented in the `IOU` function
    as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 两个边界框的 IoU 算法在 `IOU` 函数中实现如下：
- en: '[PRE53]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: At first, we checked bounding boxes for emptiness; if one of the boxes is empty,
    the IoU value is zero. Then, we calculated the intersection area. This was done
    by finding the minimum and maximum values for X and Y, considering both bounding
    boxes, and then taking the product of the difference between these values. The
    union area was calculated by summing the areas of two bounding boxes minus the
    intersection area. You can see this calculation in the return statement where
    we calculated the area’s ratio.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们检查边界框是否为空；如果一个框为空，则 IoU 值为零。然后，我们计算交集面积。这是通过找到 X 和 Y 的最小和最大值，考虑两个边界框，然后取这些值之间的差异的乘积来完成的。并集面积是通过将两个边界框的面积相加减去交集面积来计算的。您可以在返回语句中看到这个计算，我们在其中计算了面积比率。
- en: Together, NMS and IoU help improve the accuracy and precision of YOLO by discarding
    false positives and ensuring that only relevant detections are included in the
    final output.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: NMS和IoU共同帮助通过丢弃假阳性并确保只有相关的检测被包含在最终输出中，从而提高了YOLO的准确性和精确度。
- en: 'In this section, we looked at the implementation of object detection applications
    for the Android system. We learned how to export a pre-trained model from a Python
    program as a PyTorch script file. Then, we delved into developing a mobile application
    with Android Studio IDE and the mobile version of the PyTorch C++ library. In
    the following figure, you can see an example of the application output window:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了Android系统对象检测应用的实现。我们学习了如何将预训练模型从Python程序导出为PyTorch脚本文件。然后，我们深入开发了一个使用Android
    Studio IDE和PyTorch C++库移动版本的移动应用程序。在下面的图中，你可以看到一个应用程序输出窗口的示例：
- en: '![Figure 14.4 – Object detection application output](img/B19849_14_4.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图14.4 – 对象检测应用输出](img/B19849_14_4.jpg)'
- en: Figure 14.4 – Object detection application output
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4 – 对象检测应用输出
- en: In this figure, you can see that our application successfully detected a laptop
    and a computer mouse in front of the smartphone camera. Every detection result
    was marked with the bounding box and corresponding label.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图中，你可以看到我们的应用成功地在智能手机摄像头前检测到了一台笔记本电脑和一只鼠标。每一个检测结果都用边界框和相应的标签进行了标记。
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how to deploy ML models, especially neural networks,
    to mobile platforms. We examined that, on these platforms, we usually need a customized
    build of the ML framework that we used in our project. Mobile platforms use different
    CPUs, and sometimes, they have specialized neural network accelerator devices,
    so you need to compile your application and ML framework in regard to these architectures.
    These architectures differ from development environments, and you often use them
    for two different purposes. The first case is to use powerful machine configuration
    with GPUs to accelerate the ML training process, so you need to build your application
    while taking the use of one or multiple GPUs into account. The other case is using
    a device for inference only. In this case, you typically don’t need a GPU at all
    because a modern CPU can, in many cases, satisfy your performance requirements.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何部署机器学习模型，特别是神经网络，到移动平台。我们探讨了在这些平台上，我们通常需要为我们项目使用的机器学习框架的定制构建。移动平台使用不同的CPU，有时，它们有专门的神经网络加速器设备，因此你需要根据这些架构编译你的应用程序和机器学习框架。这些架构与开发环境不同，你通常用于两个不同的目的。第一种情况是使用配备GPU的强大机器配置来加速机器学习训练过程，因此你需要构建你的应用程序时考虑到一个或多个GPU的使用。另一种情况是仅使用设备进行推理。在这种情况下，你通常根本不需要GPU，因为现代CPU在很多情况下可以满足你的性能要求。
- en: In this chapter, we developed an object detection application for the Android
    platform. We learned how to connect the Kotlin module with the native C++ library
    through JNI. Then, we examined how to build the PyTorch C++ library for Android
    using the NDK and saw what limitations there are to using the mobile version.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们为Android平台开发了一个对象检测应用。我们学习了如何通过JNI将Kotlin模块与原生C++库连接起来。然后，我们探讨了如何使用NDK构建Android平台的PyTorch
    C++库，并看到了使用移动版本的限制。
- en: This was the last chapter of the book; I hope you have enjoyed this book and
    found it helpful in your journey to mastering the use of C++ for ML. I hope that
    by now, you have gained a solid understanding of how to leverage the power of
    C++ to build robust and efficient ML models. Throughout the book, I have aimed
    to provide clear explanations of complex concepts, practical examples, and step-by-step
    guides to help you get started with C++ for ML. I have also included tips and
    best practices to help you avoid common pitfalls and optimize your models for
    performance. I want to remind you that the possibilities are endless when it comes
    to using C++ for ML.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的最后一章；我希望你喜欢这本书，并觉得它在你掌握使用C++进行机器学习的过程中有所帮助。我希望到现在为止，你已经对如何利用C++的力量来构建稳健和高效的机器学习模型有了坚实的理解。在整个书中，我旨在提供复杂概念的清晰解释、实用示例和逐步指南，帮助你开始使用C++进行机器学习。我还包括了一些提示和最佳实践，帮助你避免常见的陷阱并优化你的模型以获得更好的性能。我想提醒你，在使用C++进行机器学习时，可能性是无限的。
- en: Whether you are a beginner or an experienced developer, there is always something
    new to learn and explore. With that in mind, I encourage you to continue to push
    your boundaries and experiment with different approaches and techniques. The world
    of ML is constantly evolving, and by staying up to date with the latest trends
    and developments, you can stay ahead of the curve and build cutting-edge models
    that can solve complex problems.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 不论你是初学者还是有经验的开发者，总有新的东西可以学习和探索。考虑到这一点，我鼓励你继续挑战自己的极限，尝试不同的方法和技巧。机器学习的世界不断在发展，通过跟上最新的趋势和发展，你可以保持领先，构建能够解决复杂问题的尖端模型。
- en: Thank you again for choosing my book and for taking the time to learn about
    using C++ for ML. I hope that you find it to be a valuable resource and that it
    helps you on your journey toward becoming a skilled and successful ML developer.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 再次感谢您选择我的书籍，并抽出时间学习如何使用 C++ 进行机器学习。我希望您能觉得这是一份有价值的资源，并帮助您在成为熟练且成功的机器学习开发者之路上取得进步。
- en: Further reading
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'PyTorch C++ API: [https://pytorch.org/cppdocs/](https://pytorch.org/cppdocs/)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch C++ API：[https://pytorch.org/cppdocs/](https://pytorch.org/cppdocs/)
- en: 'Documentation for app developers: [https://developer.android.com/develop](https://developer.android.com/develop)'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用开发者文档：[https://developer.android.com/develop](https://developer.android.com/develop)
- en: 'Android NDK: [https://developer.android.com/ndk](https://developer.android.com/ndk)'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Android NDK：[https://developer.android.com/ndk](https://developer.android.com/ndk)
- en: 'PyTorch guides for mobile development: [https://pytorch.org/mobile/android/](https://pytorch.org/mobile/android/)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 移动开发指南：[https://pytorch.org/mobile/android/](https://pytorch.org/mobile/android/)
- en: 'PyTorch guide for optimized mobile script exporting: [https://pytorch.org/tutorials/recipes/script_optimized.html](https://pytorch.org/tutorials/recipes/script_optimized.html)'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 优化移动脚本导出指南：[https://pytorch.org/tutorials/recipes/script_optimized.html](https://pytorch.org/tutorials/recipes/script_optimized.html)
- en: 'OpenCV Android SDK tutorial: [https://docs.opencv.org/4.x/d5/df8/tutorial_dev_with_OCV_on_Android.html](https://docs.opencv.org/4.x/d5/df8/tutorial_dev_with_OCV_on_Android.html)'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenCV Android SDK 教程：[https://docs.opencv.org/4.x/d5/df8/tutorial_dev_with_OCV_on_Android.html](https://docs.opencv.org/4.x/d5/df8/tutorial_dev_with_OCV_on_Android.html)
- en: 'ExcuTorch – a new framework for running PyTorch on embedded devices: [https://pytorch.org/executorch/stable/index.html](https://pytorch.org/executorch/stable/index.html)'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ExcuTorch – 在嵌入式设备上运行 PyTorch 的新框架：[https://pytorch.org/executorch/stable/index.html](https://pytorch.org/executorch/stable/index.html)
