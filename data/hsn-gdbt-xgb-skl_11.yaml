- en: '*Chapter 8*: XGBoost Alternative Base Learners'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章*: XGBoost替代基础学习器'
- en: In this chapter, you will analyze and apply different `gbtree`, additional options
    for base learners include `gblinear` and `dart`. Furthermore, XGBoost has its
    own implementations of random forests as base learners and as tree ensemble algorithms
    that you will experiment with in this chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将分析并应用不同的`gbtree`，基础学习器的附加选项包括`gblinear`和`dart`。此外，XGBoost还有其自己的随机森林实现，作为基础学习器以及作为树集成算法，您将在本章中进行实验。
- en: By learning how to apply alternative base learners, you will greatly extend
    your range with XGBoost. You will have the capacity to build many more models
    and you will learn new approaches to developing linear, tree-based, and random
    forest machine learning algorithms. The goal of the chapter is to give you proficiency
    in building XGBoost models with alternative base learners so that you can leverage
    advanced XGBoost options to find the best possible model for a range of situations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通过学习如何应用替代的基础学习器，您将大大扩展XGBoost的使用范围。您将能够构建更多的模型，并学习开发线性、基于树的以及随机森林机器学习算法的新方法。本章的目标是让您熟练掌握使用替代基础学习器构建XGBoost模型，从而能够利用XGBoost的高级选项，在各种情况下找到最优的模型。
- en: 'In this chapter, we cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Exploring alternative base learners
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索替代的基础学习器
- en: Applying `gblinear`
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用`gblinear`
- en: Comparing `dart`
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较`dart`
- en: Finding XGBoost random forests
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找XGBoost随机森林
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code and datasets for this chapter may be found at [https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码和数据集可以在[https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08)找到。
- en: Exploring alternative base learners
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索替代的基础学习器
- en: The base learner is the machine learning model that XGBoost uses to build the
    first model in its ensemble. The word *base* is used because it's the model that
    comes first, and the word *learner* is used because the model iterates upon itself
    after learning from the errors.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基础学习器是XGBoost用来构建其集成中第一个模型的机器学习模型。使用*base*这个词是因为它是第一个出现的模型，而使用*learner*是因为该模型会在从错误中学习后自我迭代。
- en: Decision trees have emerged as the preferred base learners for XGBoost on account
    of the excellent scores that boosted trees consistently produce. The popularity
    of decision trees extends beyond XGBoost to other ensemble algorithms such as
    random forests and `ExtraTreesClassifier` and `ExtraTreesRegressor` ([https://scikit-learn.org/stable/modules/ensemble.html](https://scikit-learn.org/stable/modules/ensemble.html)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树已成为XGBoost首选的基础学习器，因为提升树模型能够 consistently 产生优秀的分数。决策树的受欢迎程度不仅仅局限于XGBoost，还扩展到其他集成算法，如随机森林以及`ExtraTreesClassifier`和`ExtraTreesRegressor`（[https://scikit-learn.org/stable/modules/ensemble.html](https://scikit-learn.org/stable/modules/ensemble.html)）。
- en: In XGBoost, the default base learner, known as `gbtree`, is one of several base
    learners. There is also `gblinear`, a gradient boosted linear model, and `dart`,
    a variation of decision trees that includes a dropout technique based on neural
    networks. Furthermore, there are XGBoost random forests. In the next section,
    we will explore the differences between these base learners before applying them
    in subsequent sections.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在XGBoost中，默认的基础学习器是`gbtree`，它是多种基础学习器之一。还有`gblinear`，一种梯度提升线性模型，以及`dart`，一种包括基于神经网络的dropout技术的决策树变种。此外，XGBoost还有随机森林的实现。在接下来的部分中，我们将探讨这些基础学习器之间的差异，然后在后续章节中应用它们。
- en: gblinear
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: gblinear
- en: Decision trees are optimal for **non-linear data** as they can easily access
    points by splitting the data as many times as needed. Decision trees are often
    preferable as base learners because real data is usually non-linear.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树对于**非线性数据**非常适用，因为它们可以通过多次划分数据来轻松访问数据点。由于实际数据通常是非线性的，决策树通常更适合作为基础学习器。
- en: There may be cases, however, where a `gblinear` as an option for a **linear
    base learner**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，可能会有某些情况，`gblinear`作为**线性基础学习器**的选项。
- en: The general idea behind boosted linear models is the same as boosted tree models.
    A base model is built, and each subsequent model is trained upon the residuals.
    At the end, the individual models are summed for the final result. The primary
    distinction with linear base learners is that each model in the ensemble is linear.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 提升线性模型的基本思想与提升树模型相同。首先构建一个基模型，然后在每个后续模型中训练残差。最后，将所有模型的结果求和得到最终结果。与线性基学习器的主要区别在于，集成中的每个模型都是线性的。
- en: Like `gblinear` also adds regularization terms to linear regression. Tianqi
    Chin, the founder and developer of XGBoost commented on GitHub that multiple rounds
    of boosting `gblinear` may be used *to get back a single lasso regression* ([https://github.com/dmlc/xgboost/issues/332](https://github.com/dmlc/xgboost/issues/332)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与`gblinear`一样，`dart`也在线性回归中添加了正则化项。XGBoost的创始人兼开发者Tianqi Chin在GitHub上评论称，经过多轮提升后，`gblinear`可以用来*返回一个单一的lasso回归*（[https://github.com/dmlc/xgboost/issues/332](https://github.com/dmlc/xgboost/issues/332)）。
- en: '`gblinear` may also be used for classification problems via **logistic regression**.
    This works because logistic regression is also built by finding optimal coefficients
    (weighted inputs), as in **linear regression**, and summed via the **sigmoid equation**
    (see [*Chapter 1*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022), *Machine Learning
    Landscape*).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`gblinear`也可以通过**逻辑回归**应用于分类问题。这是因为逻辑回归的构建方式与**线性回归**相同，都是通过找到最优系数（加权输入）并通过**sigmoid方程**求和（见[*第1章*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022)，*机器学习概览*）。'
- en: We will explore the details and applications of `gblinear` in the *Applying
    gblinear* section in this chapter. For now, let's learn about `dart`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的*应用gblinear*部分深入探讨`gblinear`的细节和应用。现在，先了解一下`dart`。
- en: DART
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DART
- en: '**Dropouts meet Multiple Additive Regression Trees**, simply known as **DART**,
    was introduced in 2015 by K. V. Rashmi from UC Berkeley and Ran Gilad-Bachrach
    from Microsoft in the following paper: [http://proceedings.mlr.press/v38/korlakaivinayak15.pdf](http://proceedings.mlr.press/v38/korlakaivinayak15.pdf).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**丢弃法与多重加法回归树**，简称**DART**，由UC Berkeley的K. V. Rashmi和微软的Ran Gilad-Bachrach于2015年在以下论文中提出：[http://proceedings.mlr.press/v38/korlakaivinayak15.pdf](http://proceedings.mlr.press/v38/korlakaivinayak15.pdf)。'
- en: Rashmi and Gilad-Bachrach highlight **Multiple Additive Regression Trees** (**MART**)
    as a successful model that suffers from too much dependency on earlier trees.
    Instead of focusing on **shrinkage**, a standard penalization term, they use the
    **dropout** technique from **neural networks**. Simply put, the dropout technique
    eliminates nodes (mathematical points) from each layer of learning in a neural
    network, thereby reducing overfitting. In other words, the dropout technique slows
    down the learning process by eliminating information from each round.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Rashmi和Gilad-Bachrach指出，**多重加法回归树**（**MART**）是一种成功的模型，但其问题在于过度依赖早期的树。为了改进这一点，他们没有侧重于**收缩**这一标准的惩罚项，而是使用了来自**神经网络**的**丢弃法**技术。简单来说，丢弃法技术通过从神经网络的每一层学习中删除节点（数学点），从而减少过拟合。换句话说，丢弃法通过在每一轮中删除信息，减缓了学习过程。
- en: In DART, in each new round of boosting, instead of summing the residuals from
    all previous trees to build a new model, DART selects a random sample of previous
    trees and normalizes the leaves by a scaling factor ![](img/Formula_08_001.png)
    where ![](img/Formula_08_002.png) is the number of trees dropped.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在DART中，每一轮新的提升中，DART并不是通过对所有前一个树的残差求和来建立新模型，而是随机选择部分前一个树的样本，并通过一个缩放因子来归一化叶子节点！[](img/Formula_08_001.png)，其中![](img/Formula_08_002.png)是被丢弃的树的数量。
- en: DART is a variation of decision trees. The XGBoost implementation of DART is
    similar to `gbtree` with additional hyperparameters to accommodate dropouts.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: DART是决策树的一种变体。XGBoost中的DART实现类似于`gbtree`，但是增加了一些超参数来适应丢弃法。
- en: For the mathematical details of DART, reference the original paper highlighted
    in the first paragraph of this section.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有关DART的数学细节，请参考本节第一段中提到的原始论文。
- en: You will practice building machine learning models with `DART` base learners
    in the *Comparing dart* section later in this chapter.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在本章稍后的*比较dart*部分，练习使用`DART`基学习器来构建机器学习模型。
- en: XGBoost random forests
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost随机森林
- en: The final option that we'll explore in this section is XGBoost random forests.
    Random forests may be implemented as base learners by setting `num_parallel_trees`
    equal to an integer greater than `1`, and as class options within XGBoost defined
    as `XGBRFRegressor` and `XGBRFClassifier`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们将探索的最后一个选项是 XGBoost 随机森林。通过将 `num_parallel_trees` 设置为大于 `1` 的整数，可以将随机森林实现为基础学习器，并作为
    XGBoost 中的类选项，定义为 `XGBRFRegressor` 和 `XGBRFClassifier`。
- en: Keep in mind that gradient boosting was designed to improve upon the errors
    of relatively weak base learners, not strong base learners like random forests.
    Nevertheless, there may be fringe cases where random forest base learners can
    be advantageous so it's a nice option to have.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，梯度提升法是为改进相对较弱的基础学习器的错误而设计的，而不是像随机森林这样强大的基础学习器。然而，可能会有一些边缘情况，随机森林基础学习器也可能具有优势，因此它是一个不错的选择。
- en: As an additional bonus, XGBoost provides `XGBRFRegressor` and `XGBRFClassifier`
    as random forest machine learning algorithms that are not base learners, but algorithms
    in their own right. These algorithms work in a similar manner as scikit-learn's
    random forests (see [*Chapter 3*](B15551_03_Final_NM_ePUB.xhtml#_idTextAnchor070),
    *Bagging with Random Forests*). The primary difference is that XGBoost includes
    default hyperparameters to counteract overfitting and their own methods for building
    individual trees. XGBoost random forests have been in the experimental stage,
    but they are starting to outperform scikit-learn's random forests as of late 2020
    as you willwill see in this chapter.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 作为额外奖励，XGBoost 提供了 `XGBRFRegressor` 和 `XGBRFClassifier` 作为随机森林机器学习算法，它们不是基础学习器，而是独立的算法。这些算法的工作方式与
    scikit-learn 的随机森林类似（参见 [*第3章*](B15551_03_Final_NM_ePUB.xhtml#_idTextAnchor070)，*使用随机森林的袋装法*）。主要的区别在于，XGBoost
    包括默认的超参数来对抗过拟合，并有自己构建单棵树的方法。XGBoost 随机森林虽然还处于实验阶段，但自 2020 年末以来，它们已经开始超越 scikit-learn
    的随机森林，如你将在本章中看到的。
- en: In the final section of this chapter, we will experiment with XGBoost's random
    forests, both as base learners and as models in their own right.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，我们将分别尝试将 XGBoost 的随机森林作为基础学习器和独立模型进行实验。
- en: Now that you have an overview of XGBoost base learners, let's apply them one
    at a time.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对 XGBoost 基础学习器有了一个概览，接下来我们将逐一应用它们。
- en: Applying gblinear
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用 gblinear
- en: It's challenging to find real-world datasets that work best with linear models.
    It's often the case that real data is messy and more complex models like tree
    ensembles produce better scores. In other cases, linear models may generalize
    better.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 很难找到与线性模型最为契合的真实世界数据集。实际数据往往比较杂乱，复杂的模型（如树集成）通常会产生更好的得分。而在其他情况下，线性模型可能会有更好的泛化能力。
- en: The success of machine learning algorithms depends on how they perform with
    real-world data. In the next section, we will apply `gblinear` to the Diabetes
    dataset first and then to a linear dataset by construction.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法的成功依赖于它们在真实世界数据上的表现。在接下来的部分，我们将首先将 `gblinear` 应用于糖尿病数据集，然后再应用于一个通过构造生成的线性数据集。
- en: Applying gblinear to the Diabetes dataset
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 gblinear 应用于糖尿病数据集
- en: 'The Diabetes dataset is a regression dataset of 442 diabetes patients provided
    by scikit-learn. The prediction columns include age, sex, **BMI** (**body mass
    index**), **BP** (**blood pressure**), and five serum measurements. The target
    column is the progression of the disease after 1 year. You can read about the
    dataset in the original paper here: [http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf](http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 糖尿病数据集是由 scikit-learn 提供的一个回归数据集，包含442名糖尿病患者。预测列包括年龄、性别、**BMI**（**体重指数**）、**BP**（**血压**）和五项血清测量值。目标列是疾病在1年后的进展。你可以在原始论文中阅读有关数据集的详细信息，链接在这里：[http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf](http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)。
- en: Scikit-learn's datasets are already split into predictor and target columns
    for you. They are preprocessed for machine learning with `X`, the predictor columns,
    and `y`, the target column, loaded separately.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 的数据集已经为你将预测列和目标列拆分好了。它们已预处理，机器学习时 `X` 是预测列，`y` 是目标列，分别加载。
- en: 'Here is the full list of imports that you will need to work with this dataset
    and the rest of this chapter:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是你需要用来处理这个数据集以及本章其余部分的完整导入列表：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s begin! To use the Diabetes dataset, do the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！要使用糖尿病数据集，请执行以下操作：
- en: 'You first need to define `X` and `y` using `load_diabetes` with the `return_X_y`
    parameter set equal to `True`:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你首先需要使用 `load_diabetes` 并将 `return_X_y` 参数设置为 `True`，以定义 `X` 和 `y`：
- en: '[PRE1]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The plan is to use `cross_val_score` and `GridSearchCV`, so let's create folds
    in advance to obtain consistent scores. In [*Chapter 6*](B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136),
    *XGBoost Hyperparameters*, we used `StratifiedKFold`, which stratifies the target
    column, ensuring that each test set includes the same number of classes.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计划是使用 `cross_val_score` 和 `GridSearchCV`，所以我们先创建折叠以获得一致的得分。在 [*第6章*](B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136)，*XGBoost
    超参数* 中，我们使用了 `StratifiedKFold`，它对目标列进行分层，确保每个测试集包含相同数量的类别。
- en: This approach works for classification, but not for regression, where the target
    column takes on continuous values and classes are not involved. `KFold` achieves
    a similar goal without stratification by creating consistent splits in the data.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种方法适用于分类问题，但不适用于回归问题，其中目标列是连续值，不涉及类别。`KFold` 通过在数据中创建一致的分割来实现类似的目标，而不进行分层。
- en: 'Now, shuffle the data and use `5` splits with `KFold` using the following parameters:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，打乱数据并使用 `KFold` 对数据进行 `5` 次分割，使用以下参数：
- en: '[PRE2]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Build a function with `cross_val_score` that takes a machine learning model
    as input and returns the mean score of `5` folds as the output, making sure to
    set `cv=kfold`:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个使用 `cross_val_score` 的函数，输入一个机器学习模型，并返回 `5` 次折叠的平均得分，确保设置 `cv=kfold`：
- en: '[PRE3]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To use `gblinear` as the base model, just set `booster=''gblinear''` for `XGBRegressor`
    inside the regression function:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用 `gblinear` 作为基本模型，只需在回归函数中的 `XGBRegressor` 中设置 `booster='gblinear'`：
- en: '[PRE4]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The score is as follows:'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE5]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s check this score against other linear models including `LinearRegression`,
    `Lasso`, which uses `Ridge`, which uses `LinearRegression` is as follows:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查这个得分与其他线性模型的对比，包括 `LinearRegression`，`Lasso`，使用 `Ridge`，使用 `LinearRegression`
    如下：
- en: '[PRE6]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The score is as follows:'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE7]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'b) `Lasso` is as follows:'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `Lasso` 如下：
- en: '[PRE8]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The score is as follows:'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE9]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'c) `Ridge` is as follows:'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) `Ridge` 如下：
- en: '[PRE10]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The score is as follows:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE11]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you can see, `XGBRegressor` with `gblinear` as the base learner performs
    the best, along with `LinearRegression`.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如您所见，`XGBRegressor` 在使用 `gblinear` 作为基本学习器时表现最佳，与 `LinearRegression` 一同表现突出。
- en: 'Now place `booster=''gbtree''` inside `XGBRegressor`, which is the default
    base learner:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在将 `booster='gbtree'` 放入 `XGBRegressor` 中，这是默认的基本学习器：
- en: '[PRE12]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The score is as follows:'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE13]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see, the `gbtree` base learner does not perform nearly as well as
    the `gblinear` base learner in this case indicating that a linear model is ideal.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在这种情况下，`gbtree` 基本学习器的表现远不如 `gblinear` 基本学习器，这表明线性模型更为理想。
- en: Let's see if we can modify hyperparameters to make some gains with `gblinear`
    as the base learner.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看是否能通过调整超参数使 `gblinear` 作为基本学习器获得一些提升。
- en: gblinear hyperparameters
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: gblinear 超参数
- en: It's important to understand the differences between `gblinear` and `gbtree`
    when adjusting hyperparameters. Many of the XGBoost hyperparameters presented
    in [*Chapter 6*](B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136)*, XGBoost Hyperparameters*,
    are tree hyperparameters and do not apply to `gblinear`. For instance, `max_depth`
    and `min_child_weight` are hyperparameters specifically designed for trees.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整超参数时，理解 `gblinear` 和 `gbtree` 之间的区别非常重要。在 [*第6章*](B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136)*，XGBoost
    超参数* 中介绍的许多 XGBoost 超参数是树形超参数，不适用于 `gblinear`。例如，`max_depth` 和 `min_child_weight`
    是专门为树形设计的超参数。
- en: The following list is a summary of XGBoost `gblinear` hyperparameters that are
    designed for linear models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是针对线性模型设计的 XGBoost `gblinear` 超参数总结。
- en: reg_lambda
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: reg_lambda
- en: 'Scikit-learn uses `reg_lambda` instead of `lambda`, which is a reserved keyword
    for lambda functions in Python. This is the standard L2 regularization used by
    `Ridge`. Values close to `0` tend to work best:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 使用 `reg_lambda` 代替 `lambda`，因为 `lambda` 是 Python 中保留的关键字，用于定义 Lambda
    函数。这是 `Ridge` 使用的标准 L2 正则化。接近 `0` 的值通常效果最好：
- en: '*Default: 0*'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*默认值：0*'
- en: '*Range: [0, inf)*'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范围：[0, inf)*'
- en: '*Increasing prevents overfitting*'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*增大可防止过拟合*'
- en: '*Alias: lambda*'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*别名：lambda*'
- en: reg_alpha
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: reg_alpha
- en: 'Scikit-learn accepts both `reg_alpha` and `alpha`. This is the standard L1
    regularization used by `Lasso`. Values close to `0` tend to work best:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 接受 `reg_alpha` 和 `alpha` 两种方式。这是 `Lasso` 使用的标准 L1 正则化。接近 `0` 的值通常效果最好：
- en: '*Default: 0*'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*默认值：0*'
- en: '*Range: [0, inf)*'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范围：[0, inf)*'
- en: '*Increasing prevents overfitting*'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*增大可防止过拟合*'
- en: '*Alias: alpha*'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*别名：alpha*'
- en: updater
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更新器
- en: 'This is the algorithm that XGBoost uses to build the linear model during each
    round of boosting. `shotgun` uses `hogwild` parallelism with coordinate descent
    to produce a non-deterministic solution. By contrast, `coord_descent` is ordinary
    coordinate descent with a deterministic solution:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是XGBoost在每轮提升过程中用于构建线性模型的算法。`shotgun`使用`hogwild`并行化与坐标下降法来生成非确定性解。相比之下，`coord_descent`是普通的坐标下降法，产生确定性解：
- en: '*Default: shotgun*'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*默认值：shotgun*'
- en: '*Range: shotgun, coord_descent*'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范围：shotgun, coord_descent*'
- en: Note
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: '*Coordinate descent* is a machine learning term defined as minimizing the error
    by finding the gradient one coordinate at a time.'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*坐标下降法*是机器学习术语，定义为通过逐一寻找每个坐标的梯度来最小化误差。'
- en: feature_selector
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: feature_selector
- en: '`feature_selector` determines how the weights are selected with the following
    options:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`feature_selector`决定了如何选择权重，具体选项如下：'
- en: a) `cyclic` – cycles through features iteratively
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: a) `cyclic` – 迭代循环通过特征
- en: b) `shuffle` – cyclic with random feature-shuffling in each round
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: b) `shuffle` – 每轮随机特征重排的循环方式
- en: c) `random` – the coordinate selector during coordinate descent is random
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: c) `random` – 坐标下降法中的坐标选择是随机的
- en: d) `greedy` – time-consuming; selects the coordinate with the greatest gradient
    magnitude
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: d) `greedy` – 耗时；选择具有最大梯度幅度的坐标
- en: e) `thrifty` – approximately greedy, reorders features according to weight changes
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: e) `thrifty` – 大致是贪婪的，根据权重变化重新排序特征
- en: '*Default: cyclic*'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*默认值：cyclic*'
- en: '*Range must be used in conjunction with updater as follows:*'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范围必须与`updater`一起使用，具体如下：*'
- en: 'a) `shotgun`: `cyclic`, `shuffle`'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'a) `shotgun`: `cyclic`, `shuffle`'
- en: 'b) `coord_descent`: `random`, `greedy`, `thrifty`'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'b) `coord_descent`: `random`, `greedy`, `thrifty`'
- en: Note
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: '`greedy` will be computationally expensive for large datasets, but the number
    of features that `greedy` considers may be reduced by changing the parameter `top_k`
    (see the following).'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于大数据集来说，`greedy`计算开销较大，但通过更改参数`top_k`（见下文），可以减少`greedy`考虑的特征数量。
- en: top_k
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: top_k
- en: '`top_k` is the number of features that `greedy` and `thrifty` select from during
    coordinate descent:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`top_k`是`greedy`和`thrifty`在坐标下降法中选择特征的数量：'
- en: '*Default: 0 (all features)*'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*默认值：0（所有特征）*'
- en: '*Range: [0, max number of features]*'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范围：[0，最大特征数]*'
- en: Note
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on XGBoost `gblinear` hyperparameters consult the official
    XGBoost documentation page at [https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-linear-booster-booster-gblinear](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-linear-booster-booster-gblinear).
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 欲了解更多关于XGBoost `gblinear`超参数的信息，请查阅XGBoost官方文档页面：[https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-linear-booster-booster-gblinear](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-linear-booster-booster-gblinear)。
- en: gblinear grid search
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: gblinear 网格搜索
- en: 'Now that you are familiar with the range of hyperparameters that `gblinear`
    may use, let''s use `GridSearchCV` in a customized `grid_search` function to find
    the best ones:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经熟悉了`gblinear`可能使用的超参数范围，接下来让我们使用`GridSearchCV`在自定义的`grid_search`函数中找到最佳参数：
- en: 'Here is a version of our `grid_search` function from [*Chapter 6*](B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136),
    *XGBoost Hyperparameters*:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是我们从[*第6章*](B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136)中得到的`grid_search`函数版本，*XGBoost超参数*：
- en: '[PRE14]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s start by modifying `alpha` with a standard range:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从使用标准范围修改`alpha`开始：
- en: '[PRE15]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE16]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The score is about the same, with a very slight improvement.
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分大致相同，但略有提升。
- en: 'Next, let''s modify `reg_lambda` with the same range:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们使用相同的范围修改`reg_lambda`：
- en: '[PRE17]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE18]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This score here is very similar but slightly worse.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的得分非常相似，但略逊一筹。
- en: Now let's use `feature_selector` in tandem with `updater`. By default, `updater=shotgun`
    and `feature_selector=cyclic`. When `updater=shotgun`, the only other option for
    `feature_selector` is `shuffle`.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们将`feature_selector`与`updater`一起使用。默认情况下，`updater=shotgun`，`feature_selector=cyclic`。当`updater=shotgun`时，`feature_selector`唯一的另一个选择是`shuffle`。
- en: 'Let''s see if `shuffle` can perform better than `cyclic`:'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们看看`shuffle`是否比`cyclic`表现更好：
- en: '[PRE19]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE20]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In this case, `shuffle` does not perform better.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，`shuffle`表现不佳。
- en: 'Now let''s change `updater` to `coord_descent`. As a result, `feature_selector`
    may take on `random`, `greedy`, or `thrifty`. Try all `feature_selector` alternatives
    in `grid_search` by entering the following code:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们将`updater`更改为`coord_descent`。因此，`feature_selector`可以选择`random`、`greedy`或`thrifty`。通过输入以下代码，尝试在`grid_search`中测试所有`feature_selector`选项：
- en: '[PRE21]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output is as follows:'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE22]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The final hyperparameter to check is `top_k`, which defines the number of features
    that `greedy` and `thrifty` check during coordinate descent. A range from `2`
    to `9` is acceptable since there are 10 features in total.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后一个需要检查的超参数是 `top_k`，它定义了在坐标下降过程中，`greedy` 和 `thrifty` 检查的特征数量。由于总共有 10 个特征，`top_k`
    的范围从 `2` 到 `9` 都是可以接受的。
- en: 'Enter a range for `top_k` inside `grid_search` for `greedy` and `thrifty` to
    find the best option:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `grid_search` 中为 `greedy` 和 `thrifty` 输入 `top_k` 的范围，以找到最佳选项：
- en: '[PRE23]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE24]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This is the best score yet.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是迄今为止最好的得分。
- en: Before moving on, note that additional hyperparameters that are not limited
    to trees, such as `n_estimators` and `learning_rate`, may be used as well.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请注意，除了树以外，还可以使用其他超参数，比如 `n_estimators` 和 `learning_rate`。
- en: Now let's see how `gblinear` works on a dataset that is linear by construction.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看 `gblinear` 在一个构建时就是线性的数据集上的表现。
- en: Linear datasets
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性数据集
- en: One way to ensure that a dataset is linear is by construction. We can choose
    a range of `X` values, say `1` to `99`, and multiply them by a scaling factor
    with some randomness involved.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 确保数据集是线性的一个方法是通过构建。我们可以选择一系列 `X` 值，比如从 `1` 到 `99`，然后乘以一个缩放因子，并加入一些随机性。
- en: 'Here are the steps to construct a linear dataset:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这是构建线性数据集的步骤：
- en: 'Set the range of `X` values from `1` to `100`:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 `X` 值的范围从 `1` 到 `100`：
- en: '[PRE25]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Declare a random seed using NumPy to ensure the consistency of the results:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 NumPy 声明一个随机种子，以确保结果的一致性：
- en: '[PRE26]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create an empty list defined as `y`:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个空列表，定义为 `y`：
- en: '[PRE27]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Loop through `X`, multiplying each entry by a random number from `-0.2` to
    `0.2`:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历 `X`，将每个条目乘以一个从 `-0.2` 到 `0.2` 的随机数：
- en: '[PRE28]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Transform `y` to a `numpy` array for machine learning:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `y` 转换为 `numpy` 数组以用于机器学习：
- en: '[PRE29]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Reshape `X` and `y` so that they contain as many rows as members in the array
    and one column since columns are expected as machine learning inputs with scikit-learn:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重塑 `X` 和 `y`，使它们包含与数组成员相同数量的行和一列，因为列是 scikit-learn 期望的机器学习输入：
- en: '[PRE30]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We now have a linear dataset that includes randomness in terms of `X` and `y`.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们现在有一个线性数据集，其中 `X` 和 `y` 含有随机性。
- en: 'Let''s run the `regression_model` function again with `gblinear` as the base
    learner:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次运行 `regression_model` 函数，这次使用 `gblinear` 作为基础学习器：
- en: '[PRE31]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The score is as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE32]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now run the `regression_model` function with `gbtree` as the base learner:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用 `gbtree` 作为基础学习器运行 `regression_model` 函数：
- en: '[PRE33]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The score is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE34]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As you can see, `gblinear` performs much better in our constructed linear dataset.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`gblinear` 在我们构建的线性数据集上表现更好。
- en: 'For good measure, let''s try `LinearRegression` on the same dataset:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更精确，让我们在同一数据集上尝试 `LinearRegression`：
- en: '[PRE35]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The score is as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE36]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In this case, `gblinear` performs slightly better, perhaps negligibly, scoring
    `0.00002` points lower than `LinearRegression`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`gblinear` 的表现稍好，或许差距微乎其微，得分比 `LinearRegression` 低了 `0.00002` 分。
- en: Analyzing gblinear
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析 gblinear
- en: '`gblinear` is a compelling option, but it should only be used when you have
    reason to believe that a linear model may perform better than a tree-based model.
    `gblinear` did outperform `LinearRegression` in the real and constructed datasets
    by a very slight margin. Within XGBoost, `gblinear` is a strong option for a base
    learner when datasets are large and linear. `gblinear` is an option for classification
    datasets as well, an option that you will apply in the next section.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`gblinear` 是一个有吸引力的选择，但只有在你有理由相信线性模型可能比基于树的模型表现更好时，才应使用它。在真实数据集和构建的数据集中，`gblinear`
    比 `LinearRegression` 的表现高出非常微弱的优势。在 XGBoost 中，`gblinear` 在数据集庞大且线性时，是一个很强的基础学习器选择。`gblinear`
    也可以用于分类数据集，下一节中你将应用这一方法。'
- en: Comparing dart
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较 dart
- en: The base learner `dart` is similar to `gbtree` in the sense that both are gradient
    boosted trees. The primary difference is that `dart` removes trees (called dropout)
    during each round of boosting.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 基础学习器 `dart` 类似于 `gbtree`，因为它们都是梯度提升树。主要的区别是，`dart` 在每一轮提升中移除一些树（称为 dropout）。
- en: In this section, we will apply and compare the base learner `dart` to other
    base learners in regression and classification problems.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将应用并比较基础学习器 `dart` 与其他基础学习器在回归和分类问题中的表现。
- en: DART with XGBRegressor
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 XGBRegressor 的 DART 方法
- en: 'Let''s see how `dart` performs on the Diabetes dataset:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 `dart` 在糖尿病数据集上的表现：
- en: 'First, redefine `X` and `y` using `load_diabetes` as before:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，像之前一样使用 `load_diabetes` 重新定义 `X` 和 `y`：
- en: '[PRE37]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'To use `dart` as the XGBoost base learner, set the `XGBRegressor` parameter
    `booster=''dart''` inside the `regression_model` function:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要将`dart`作为 XGBoost 的基础学习器使用，需要在`regression_model`函数内将`XGBRegressor`参数`booster='dart'`设置：
- en: '[PRE38]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The score is as follows:'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分数如下：
- en: '[PRE39]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The `dart` base learner gives the same result as the `gbtree` base learner down
    to two decimal places. The similarity of results is on account of the small dataset
    and the success of the `gbtree` default hyperparameters to prevent overfitting
    without requiring the dropout technique.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`dart`基础学习器与`gbtree`基础学习器的结果相同，精确到小数点后两位。结果相似是因为数据集较小，且`gbtree`默认的超参数能够有效防止过拟合，无需采用丢弃树技术。'
- en: Let's see how `dart` performs compared to `gbtree` on a larger dataset with
    classification.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`dart`与`gbtree`在更大数据集上的分类表现如何。
- en: dart with XGBClassifier
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: dart 与 XGBClassifier
- en: 'You have used the Census dataset in multiple chapters throughout this book.
    A clean version of the dataset that we modified in [*Chapter 1*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022),
    *Machine Learning Landscape*, has been pre-loaded for you along with the code
    for [*Chapter 8*](B15551_08_Final_NM_ePUB.xhtml#_idTextAnchor189), *XGBoost Alternative
    Base Learners*, at [https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08).
    Let''s now begin to test how `dart` performs on a larger dataset:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你在本书的多个章节中使用了 Census 数据集。我们在[*第1章*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022)《机器学习概况》中修改过的清洁版数据集，已经为你预加载，另外还包括了[*第8章*](B15551_08_Final_NM_ePUB.xhtml#_idTextAnchor189)《XGBoost
    替代基础学习器》中的代码，数据集可以从[https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08)获取。现在让我们开始测试`dart`在更大数据集上的表现：
- en: 'Load the Census dataset into a DataFrame and split the predictor and target
    columns into `X` and `y` using the last index (`-1`) as the target column:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Census 数据集加载到 DataFrame 中，并使用最后一列索引（`-1`）作为目标列，将预测列和目标列拆分为`X`和`y`：
- en: '[PRE40]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Define a new classification function that uses `cross_val_score` with the machine
    learning model as input and the mean score as output similar to the regression
    function defined earlier in this chapter:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个新的分类函数，使用`cross_val_score`，输入为机器学习模型，输出为均值分数，类似于本章前面定义的回归函数：
- en: '[PRE41]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now call the function twice using `XGBClassifier` with `booster=''gbtree''`
    and `booster=''dart''` to compare results. Note that the run time will be longer
    since the dataset is larger:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在使用`XGBClassifier`分别设置`booster='gbtree'`和`booster='dart'`调用函数两次进行结果比较。请注意，由于数据集更大，运行时间会更长：
- en: 'a) Let''s first call `XGBClassifier` with `booster=''gbtree''`:'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 首先，让我们调用`XGBClassifier`并设置`booster='gbtree'`：
- en: '[PRE42]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The score is as follows:'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分数如下：
- en: '[PRE43]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'b) Now, let''s call `XGBClassifier` with `booster=''dart''`:'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 现在，让我们调用`XGBClassifier`并设置`booster='dart'`：
- en: '[PRE44]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The score is as follows:'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分数如下：
- en: '[PRE45]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This is surprising. `dart` gives the exact same result as `gbtree` for all 16
    decimal places! It's unclear whether trees have been dropped or the dropping of
    trees has had zero effect.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这令人吃惊。`dart`与`gbtree`的结果完全相同，精确到所有16位小数！目前尚不清楚是否真的丢弃了树，或者丢弃树对结果没有任何影响。
- en: 'We can adjust hyperparameters to ensure that trees are dropped, but first,
    let''s see how `dart` compares to `gblinear`. Recall that `gblinear` also works
    for classification by using the sigmoid function to scale weights as with logistic
    regression:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调整超参数以确保丢弃树，但首先，让我们看看`dart`与`gblinear`的比较。回想一下，`gblinear`通过使用 sigmoid 函数来对权重进行缩放，类似于逻辑回归，从而也能用于分类：
- en: 'Call the `classification_model` function with `XGBClassifier` and set `booster=''gblinear''`:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`XGBClassifier`调用`classification_model`函数，并设置`booster='gblinear'`：
- en: '[PRE46]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The score is as follows:'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分数如下：
- en: '[PRE47]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: This linear base learner does not perform as well as the tree base learners.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种线性基础学习器的表现不如树型基础学习器。
- en: 'Let''s see how `gblinear` compares with logistic regression. Since the dataset
    is large, it''s best to adjust logistic regression''s `max_iter` hyperparameter
    from `100` to `1000` to allow more time for convergence and to silence warnings.
    Note that increasing `max_iter` increases the accuracy in this case:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看`gblinear`与逻辑回归的比较。由于数据集较大，最好将逻辑回归的`max_iter`超参数从`100`调整为`1000`，以便有更多时间进行收敛并消除警告。请注意，在这种情况下，增加`max_iter`能提高准确率：
- en: '[PRE48]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The score is as follows:'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分数如下：
- en: '[PRE49]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '`gblinear` maintains a clear edge over logistic regression in this case. It''s
    worth underscoring that XGBoost''s `gblinear` option in classification provides
    a viable alternative to logistic regression.'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，`gblinear`比逻辑回归具有明显的优势。值得强调的是，XGBoost 的`gblinear`选项在分类中提供了一个可行的替代逻辑回归的方案。
- en: Now that you have seen how `dart` compares with `gbtree` and `gblinear` as a
    base learner, let's modify `dart`'s hyperparameters.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了`dart`与`gbtree`和`gblinear`作为基本学习器的比较，接下来我们来修改`dart`的超参数。
- en: DART hyperparameters
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DART 超参数
- en: '`dart` includes all `gbtree` hyperparameters along with its own set of additional
    hyperparameters designed to adjust the percentage, frequency, and probability
    of dropout trees. See the XGBoost documentation at [https://xgboost.readthedocs.io/en/latest/parameter.html#additional-parameters-for-dart-booster-booster-dart](https://xgboost.readthedocs.io/en/latest/parameter.html#additional-parameters-for-dart-booster-booster-dart)
    for detailed information.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`dart`包含所有`gbtree`的超参数，并且还包括一组额外的超参数，用于调整丢弃树的百分比、频率和概率。有关详细信息，请参见 XGBoost 文档：[https://xgboost.readthedocs.io/en/latest/parameter.html#additional-parameters-for-dart-booster-booster-dart](https://xgboost.readthedocs.io/en/latest/parameter.html#additional-parameters-for-dart-booster-booster-dart)。'
- en: The following sections are a summary of XGBoost hyperparameters that are unique
    to `dart`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节是 XGBoost 中专门针对`dart`的超参数总结。
- en: sample_type
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: sample_type
- en: 'The options for `sample_type` include `uniform`, which drops trees uniformly,
    and `weighted`, which drops trees in proportion to their weights:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample_type`的选项包括`uniform`，表示树是均匀丢弃的，和`weighted`，表示树按其权重比例丢弃：'
- en: '*Default: "uniform"*'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*默认值："uniform"*'
- en: '*Range: ["uniform", "weighted"]*'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范围：["uniform", "weighted"]*'
- en: '*Determines how dropped trees are selected*'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*决定丢弃树的选择方式*'
- en: normalize_type
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: normalize_type
- en: 'The options for `normalize_type` include `tree`, where new trees have the same
    weight as dropped trees, and `forest`, where new trees have the same weight as
    the sum of dropped trees:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`normalize_type`的选项包括`tree`，即新树的权重与丢弃的树相同，和`forest`，即新树的权重与丢弃的树的总和相同：'
- en: '*Default: "tree"*'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*默认值："tree"*'
- en: '*Range: ["tree", "forest"]*'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范围：["tree", "forest"]*'
- en: '*Calculates weights of trees in terms of dropped trees*'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计算树的权重，以丢弃的树为单位*'
- en: rate_drop
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: rate_drop
- en: '`rate_drop` allows the user to set exactly how many trees are dropped percentage-wise:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`rate_drop`允许用户精确设置丢弃树的百分比：'
- en: '*Default: 0.0*'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*默认值：0.0*'
- en: '*Range: [0.0, 1.0]*'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范围：[0.0, 1.0]*'
- en: '*Percentage of trees that are dropped*'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*丢弃树的百分比*'
- en: one_drop
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: one_drop
- en: 'When set to `1`, `one_drop` ensures that at least one tree is always dropped
    during the boosting round:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 当设置为`1`时，`one_drop`确保每次提升轮次中总有一棵树被丢弃：
- en: '*Default: 0*'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*默认值：0*'
- en: '*Range: [0, 1]*'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范围：[0, 1]*'
- en: '*Used to ensure drops*'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用于确保丢弃*'
- en: skip_drop
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: skip_drop
- en: '`skip_drop` gives the probability of skipping the dropout entirely. In the
    official documentation, XGBoost says that `skip_drop` has a higher priority than
    `rate_drop` or `one_drop`. By default, each tree is dropped with the same probability
    so there is a probability that no trees are dropped for a given boosting round.
    `skip_drop` allows this probability to be updated to control the number of dropout
    rounds:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`skip_drop`给出了完全跳过丢弃的概率。在官方文档中，XGBoost 说 `skip_drop` 的优先级高于`rate_drop`或`one_drop`。默认情况下，每棵树被丢弃的概率相同，因此对于某次提升轮次可能没有树被丢弃。`skip_drop`允许更新此概率，以控制丢弃轮次的数量：'
- en: '*Default: 0.0*'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*默认值：0.0*'
- en: '*Range: [0.0, 1.0]*'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范围：[0.0, 1.0]*'
- en: '*Probability of skipping the dropout*'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*跳过丢弃的概率*'
- en: Now let's modify `dart` hyperparameters to differentiate scores.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们修改`dart`的超参数，以区分不同的得分。
- en: Modifying dart hyperparameters
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修改 dart 超参数
- en: 'To ensure that at least one tree in each boosting round is dropped, we can
    set `one_drop=1`. Do this with the Census dataset via the `classification_model`
    function now:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保每次提升轮次中至少有一棵树被丢弃，我们可以设置`one_drop=1`。现在通过`classification_model`函数使用 Census
    数据集来实现：
- en: '[PRE50]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The result is as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE51]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This is an improvement by a tenth of a percentage point, indicating that dropping
    at least one tree per boosting round can be advantageous.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个提高了百分之一点的改进，表明每次提升轮次丢弃至少一棵树可能是有利的。
- en: 'Now that we are dropping trees to change scores, let''s return to the smaller
    and faster Diabetes dataset to modify the remaining hyperparameters:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们正在丢弃树以更改得分，让我们回到更小且更快的糖尿病数据集，修改剩余的超参数：
- en: 'Using the `regression_model` function, change `sample_type` from `uniform`
    to `weighted`:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`regression_model`函数，将`sample_type`从`uniform`更改为`weighted`：
- en: '[PRE52]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The score is as follows:'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE53]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: This is 0.002 points better than the `gbtree` model scored earlier.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个得分比之前 `gbtree` 模型得分高出 0.002 分。
- en: 'Change `normalize_type` to `forest` to include the sum of trees when updating
    weights:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `normalize_type` 更改为 `forest`，以便在更新权重时包括树的总和：
- en: '[PRE54]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The score is as follows:'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE55]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: There is no change in the score, which may happen with a shallow dataset.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分数没有变化，这可能发生在数据集较浅的情况下。
- en: 'Change `one_drop` to `1` guaranteeing that at least one tree is dropped each
    boosting round:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `one_drop` 更改为 `1`，确保每次提升回合至少丢弃一棵树：
- en: '[PRE56]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The score is as follows:'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE57]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: This is a clear improvement, gaining four full points.
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个明显的改进，得分提高了四个完整点。
- en: 'When it comes to `rate_drop`, the percentage of trees that will be dropped,
    a range of percentages may be used with the `grid_search` function as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `rate_drop`，即将被丢弃的树的百分比，可以使用以下 `grid_search` 函数来设置百分比范围：
- en: '[PRE58]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The results are as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE59]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: This is the best result yet.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这是迄今为止的最佳结果。
- en: 'We can implement a similar range with `skip_drop`, which gives the probability
    that a given tree is *not* dropped:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用类似的范围来实现 `skip_drop`，它给出了某棵树*不*被丢弃的概率：
- en: '[PRE60]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The results are as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE61]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: This is a good score, but `skip_drop` has resulted in no substantial gains.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个不错的得分，但 `skip_drop` 没有带来实质性的提升。
- en: Now that you see how `dart` works in action, let's analyze the results.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你看到 `dart` 的实际应用，让我们分析一下结果。
- en: Analyzing dart
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析 dart
- en: '`dart` provides a compelling option within the XGBoost framework. Since `dart`
    accepts all `gbtree` hyperparameters, it''s easy to change the base learner from
    `gbtree` to `dart` when modifying hyperparameters. In effect, the advantage is
    that you can experiment with new hyperparameters including `one_drop`, `rate_drop`,
    `normalize`, and others to see if you can make additional gains. `dart` is definitely
    worth trying as a base learner in your research and model-building with XGBoost.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '`dart` 在 XGBoost 框架中提供了一个很有吸引力的选项。由于 `dart` 接受所有 `gbtree` 超参数，因此在修改超参数时，可以轻松地将基础学习器从
    `gbtree` 改为 `dart`。实际上，优点是你可以尝试包括 `one_drop`、`rate_drop`、`normalize` 等新的超参数，以查看是否能获得额外的收益。在你的研究和
    XGBoost 模型构建中，`dart` 绝对值得尝试作为基础学习器。'
- en: Now that you have a good understanding of `dart`, it's time to move on to random
    forests.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对 `dart` 有了很好的理解，是时候转向随机森林了。
- en: Finding XGBoost random forests
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找 XGBoost 随机森林
- en: There are two strategies to implement random forests within XGBoost. The first
    is to use random forests as the base learner, the second is to use XGBoost's original
    random forests, `XGBRFRegressor` and `XGBRFClassifier`. We start with our original
    theme, random forests as alternative base learners.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在 XGBoost 中实现随机森林有两种策略。第一种是将随机森林作为基础学习器，第二种是使用 XGBoost 的原始随机森林，即 `XGBRFRegressor`
    和 `XGBRFClassifier`。我们从原始主题开始，即将随机森林作为替代基础学习器。
- en: Random forests as base learners
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林作为基础学习器
- en: There is not an option to set the booster hyperparameter to a random forest.
    Instead, the hyperparameter `num_parallel_tree` may be increased from its default
    value of `1` to transform `gbtree` (or `dart`) into a boosted random forest. The
    idea here is that each boosting round will no longer consist of one tree, but
    a number of parallel trees, which in turn make up a forest.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 没有选项可以将提升器的超参数设置为随机森林。相反，可以将超参数 `num_parallel_tree` 从其默认值 `1` 增加，以将 `gbtree`（或
    `dart`）转变为一个提升的随机森林。这里的思路是，每个提升回合将不再是单棵树，而是多个并行的树，这些树共同构成一片森林。
- en: The following is a quick summary of the XGBoost hyperparameter `num_parallel_tree`.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 XGBoost 超参数 `num_parallel_tree` 的简要总结。
- en: num_parallel_tree
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: num_parallel_tree
- en: '`num_parallel_tree` gives the number of trees, potentially more than 1, that
    are built during each boosting round:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`num_parallel_tree` 指定了在每次提升回合中构建的树的数量，可能大于 1：'
- en: '*Default: 1*'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*默认值：1*'
- en: '*Range: [1, inf)*'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范围：[1, inf)*'
- en: '*Gives number of trees boosted in parallel*'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*给出并行提升的树的数量*'
- en: '*Value greater than 1 turns booster into random forest*'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*大于 1 的值会将提升器转变为随机森林*'
- en: By including multiple trees per round, the base learner is no longer a tree,
    but a forest. Since XGBoost includes the same hyperparameters as random forests,
    the base learner is appropriately classified as a random forest when `num_parallel_tree`
    exceeds 1.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在每回合中包含多棵树，基础学习器不再是单棵树，而是森林。由于 XGBoost 包含与随机森林相同的超参数，因此当 `num_parallel_tree`
    超过 1 时，基础学习器被适当分类为随机森林。
- en: 'Let''s see how XGBoost random forest base learners work in practice:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 XGBoost 随机森林基础学习器在实际中的表现：
- en: 'Call `regression_model` with `XGBRegressor` and set `booster=''gbtree''`. Additionally,
    set `num_parallel_tree=25` meaning that each boosted round consists of a forest
    of `25` trees:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `XGBRegressor` 调用 `regression_model` 并设置 `booster='gbtree'`。此外，设置 `num_parallel_tree=25`，意味着每次提升回合由
    `25` 棵树组成：
- en: '[PRE62]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The score is as follows:'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评分如下：
- en: '[PRE63]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: The score is respectable, and in this case, it's nearly the same as boosting
    a single `gbtree`. The reason is that gradient boosting is designed to learn from
    the mistakes of the previous trees. By starting with a robust random forest, there
    is little to be learned and the gains are minimal at best.
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评分是相当不错的，在这种情况下，几乎和提升一个单独的 `gbtree` 相同。原因在于梯度提升的设计是通过从前一个树的错误中学习来优化性能。通过从一个强大的随机森林开始，学习的空间有限，因此收益最多是微小的。
- en: Understanding the fundamental point that gradient boosting's strength as an
    algorithm comes from the learning process is essential. It makes sense, therefore,
    to try a much smaller value for `num_parallel_tree`, such as `5`.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 理解梯度提升算法的核心优势来自于学习过程至关重要。因此，尝试将 `num_parallel_tree` 设置为一个更小的值（例如 `5`）是有意义的。
- en: 'Set `num_parallel_tree=5` inside the same regression model:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一个回归模型中设置 `num_parallel_tree=5`：
- en: '[PRE64]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The score is as follows:'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评分如下：
- en: '[PRE65]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Technically, this score is 0.002 points better than the score produced by a
    forest of 25 trees. Although the improvement is not much, generally speaking,
    when building XGBoost random forests, low values of `num_parallel_tree` are better.
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从技术上讲，这个得分比 25 棵树的森林产生的得分高 0.002 分。虽然这个改善不大，但一般来说，在构建 XGBoost 随机森林时，较低的 `num_parallel_tree`
    值会更好。
- en: Now that you have seen how random forests may be implemented as base learners
    within XGBoost, it's time to build random forests as original XGBoost models.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到如何将随机森林作为 XGBoost 中的基础学习器实现，是时候将随机森林构建为原始的 XGBoost 模型了。
- en: Random forests as XGBoost models
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作为 XGBoost 模型的随机森林
- en: In addition to `XGBRegressor` and `XGBClassifier`, `XGBoost` also comes with
    `XGBRFRegressor` and `XGBRFClassifier` to build random forests.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `XGBRegressor` 和 `XGBClassifier`，`XGBoost` 还提供了 `XGBRFRegressor` 和 `XGBRFClassifier`
    来构建随机森林。
- en: According to the official XGBoost documentation at [https://xgboost.readthedocs.io/en/latest/tutorials/rf.html](https://xgboost.readthedocs.io/en/latest/tutorials/rf.html),
    the random forest scikit-learn wrapper is still in the experimentation stage and
    the defaults may be changed at any time. At the time of writing, in 2020, the
    following `XGBRFRegressor` and `XGBRFClassifier` defaults are included.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 根据官方 XGBoost 文档 [https://xgboost.readthedocs.io/en/latest/tutorials/rf.html](https://xgboost.readthedocs.io/en/latest/tutorials/rf.html)，scikit-learn
    的随机森林包装器仍处于实验阶段，默认值可能会随时更改。在编写本文时（2020年），以下是 `XGBRFRegressor` 和 `XGBRFClassifier`
    的默认值。
- en: n_estimators
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: n_estimators
- en: 'Use `n_estimators` and not `num_parallel_tree` when using `XGBRFRegressor`
    or `XGBRFClassifier` to build a random forest. Keep in mind that when using `XGBRFRegressor`
    and `XGBRFClassifier`, you are not gradient boosting but bagging trees in one
    round only as is the case with a traditional random forest:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `n_estimators` 而不是 `num_parallel_tree` 来构建随机森林时，请使用 `XGBRFRegressor` 或 `XGBRFClassifier`。请记住，在使用
    `XGBRFRegressor` 和 `XGBRFClassifier` 时，你并不是在做梯度提升，而是在一次回合中对树进行集成，就像传统的随机森林一样：
- en: '*Default: 100*'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*默认值： 100*'
- en: '*Range: [1, inf)*'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范围： [1, inf)*'
- en: '*Automatically converted to num_parallel_tree for random forests*'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动转换为 num_parallel_tree 用于随机森林*'
- en: learning_rate
  id: totrans-317
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: learning_rate
- en: '`learning_rate` is generally designed for models that learn, including boosters,
    not `XGBRFRegressor` or `XGBRFClassifier` since they consist of one round of trees.
    Nevertheless, changing `learning_rate` from 1 will change the scores, so modifying
    this hyperparameter is generally not advised:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '`learning_rate` 通常是为学习型模型设计的，包括增强器，而不是 `XGBRFRegressor` 或 `XGBRFClassifier`，因为它们仅由一轮树组成。然而，将
    `learning_rate` 从 1 改变会影响得分，因此通常不建议修改这个超参数：'
- en: '*Default: 1*'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*默认值： 1*'
- en: '*Range: [0, 1]*'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范围： [0, 1]*'
- en: subsample, colsample_by_node
  id: totrans-321
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: subsample, colsample_by_node
- en: 'Scikit-learn''s random forest keeps these defaults at `1`, making the default
    `XGBRFRegressor` and `XGBRFClassifier` less prone to overfitting. This is the
    primary difference between the XGBoost and scikit-learn random forest default
    implementations:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 的随机森林将这些默认值保持为 `1`，使得默认的 `XGBRFRegressor` 和 `XGBRFClassifier` 更不容易过拟合。这是
    XGBoost 和 scikit-learn 随机森林默认实现之间的主要区别：
- en: '*Defaults: 0.8*'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*默认值： 0.8*'
- en: '*Range: [0, 1]*'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*范围： [0, 1]*'
- en: '*Decreasing helps prevent overfitting*'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*减少有助于防止过拟合*'
- en: 'Now, let''s see how XGBoost''s random forests work in practice:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 XGBoost 的随机森林在实践中的工作原理：
- en: 'First, place `XGBRFRegressor` inside of the `regression_model` function:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，将`XGBRFRegressor`放入`regression_model`函数中：
- en: '[PRE66]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The score is as follows:'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE67]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: This score is a little better than the `gbtree` model presented earlier, and
    a little worse than the best linear models presented in this chapter.
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个得分稍微比之前展示的`gbtree`模型好一些，但比本章中展示的最佳线性模型稍差。
- en: 'As a comparison, let''s see how `RandomForestRegressor` performs by placing
    it inside the same function:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了对比，看看将`RandomForestRegressor`放入相同函数后的表现：
- en: '[PRE68]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The score is as follows:'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE69]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: This score is slightly worse than `XGBRFRegressor`.
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个得分稍微比`XGBRFRegressor`差一些。
- en: 'Now let''s compare the XGBoost random forest with scikit-learn''s standard
    random forest using the larger Census dataset for classification:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用更大的Census数据集进行分类，将XGBoost随机森林与scikit-learn的标准随机森林进行比较：
- en: 'Place `XGBRFClassifier` inside of the `classification_model` function to see
    how well it predicts user income:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`XGBRFClassifier`放入`classification_model`函数中，看看它在预测用户收入时的表现如何：
- en: '[PRE70]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The score is as follows:'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE71]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: This is a good score, a little off the mark from `gbtree`, which previously
    gave 87%.
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个得分很好，比`gbtree`稍低，`gbtree`之前的得分是87%。
- en: 'Now place `RandomForestClassifier` inside the same function to compare results:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在将`RandomForestClassifier`放入相同的函数中，比较结果：
- en: '[PRE72]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The score is as follows:'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE73]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: This is slightly worse than XGBoost's implementation.
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个得分稍微比XGBoost的实现差一些。
- en: Since XGBoost's random forests are still in the developmental stage, we'll stop
    here and analyze the results.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 由于XGBoost的随机森林仍处于开发阶段，我们将在此结束并分析结果。
- en: Analyzing XGBoost random forests
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析XGBoost随机森林
- en: You can try a random forest as your XGBoost base learner anytime by increasing
    `num_parallel_tree` to a value greater than `1`. Although, as you have seen in
    this section, boosting is designed to learn from weak models, not strong models,
    so values for `num_parallel_tree` should remain close to `1`. Trying random forests
    as base learners should be used sparingly. If boosting single trees fails to produce
    optimal scores, random forest base learners are an option.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过将`num_parallel_tree`增加到大于`1`的值，随时尝试将随机森林作为XGBoost的基础学习器。尽管正如你在本节中所见，提升法（boosting）是为了从弱模型中学习，而不是从强模型中学习，因此`num_parallel_tree`的值应该保持接近`1`。将随机森林作为基础学习器应该谨慎使用。如果单棵树的提升法未能产生最佳分数，随机森林基础学习器是一个可选方案。
- en: Alternatively, the XGBoost random forest's `XGBRFRegressor` and `XGBRFClassifier`
    may be implemented as alternatives to scikit-learn's random forests. XGBoost's
    new `XGBRFRegressor` and `XGBRFClassifier` outperformed scikit-learn's `RandomForestRegressor`
    and `RandomForestClassifier`, although the comparison was very close. Given the
    overall success of XGBoost in the machine learning community, it's definitely
    worth using `XGBRFRegressor` and `XGBRFClassifier` as viable options going forward.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，XGBoost的随机森林的`XGBRFRegressor`和`XGBRFClassifier`可以作为scikit-learn随机森林的替代方法来实现。XGBoost新的`XGBRFRegressor`和`XGBRFClassifier`表现超过了scikit-learn的`RandomForestRegressor`和`RandomForestClassifier`，尽管这次比较非常接近。鉴于XGBoost在机器学习社区中的总体成功，未来使用`XGBRFRegressor`和`XGBRFClassifier`作为可行的选择绝对值得尝试。
- en: Summary
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you greatly extended your range of XGBoost by applying all
    XGBoost base learners, including `gbtree`, `dart`, `gblinear`, and random forests,
    to regression and classification datasets. You previewed, applied, and tuned hyperparameters
    unique to base learners to improve scores. Furthermore, you experimented with
    `gblinear` using a linearly constructed dataset and with `XGBRFRegressor` and
    `XGBRFClassifier` to build XGBoost random forests without any boosting whatsoever.
    Now that you have worked with all base learners, your comprehension of the range
    of XGBoost is at an advanced level.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你通过将所有XGBoost基础学习器（包括`gbtree`、`dart`、`gblinear`和随机森林）应用于回归和分类数据集，极大地扩展了XGBoost的使用范围。你预览、应用并调优了独特的基础学习器超参数以提高得分。此外，你还尝试了使用线性构建数据集的`gblinear`，以及使用`XGBRFRegressor`和`XGBRFClassifier`构建没有任何提升法的XGBoost随机森林。现在，你已经熟悉了所有基础学习器，你对XGBoost的理解已经达到了高级水平。
- en: In the next chapter, you will analyze tips and tricks from Kaggle masters to
    advance your XGBoost skills even further!
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将分析Kaggle高手的技巧，进一步提升你的XGBoost技能！
