- en: Getting Started with Data Mining Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门数据挖掘技术
- en: 'In 2003, Linden, Smith, and York of Amazon.com published a paper entitled Item-to-Item
    Collaborative Filtering, which explained how product recommendations at Amazon
    work. Since then, this class of algorithmg has gone on to dominate the industry
    standard for recommendations. Every website or app with a sizeable user base,
    be it Netflix, Amazon, or Facebook, makes use of some form of collaborative filters
    to suggest items (which may be movies, products, or friends):'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 2003年，亚马逊的Linden、Smith和York发布了一篇名为《项目与项目协同过滤》的论文，解释了亚马逊如何进行产品推荐。从那时起，这类算法已经主导了推荐行业的标准。每一个拥有大量用户的的网站或应用，无论是Netflix、Amazon还是Facebook，都使用某种形式的协同过滤来推荐项目（这些项目可能是电影、商品或朋友）：
- en: '![](img/a10e3bca-ab33-450c-a0f4-ddf773ad5652.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a10e3bca-ab33-450c-a0f4-ddf773ad5652.jpg)'
- en: As described in the first chapter, collaborative filters try to leverage the
    power of the community to give reliable, relevant, and sometime, even surprising
    recommendations. If Alice and Bob largely like the same movies (say The Lion King,
    Aladdin, and Toy Story) and Alice also likes Finding Nemo, it is extremely likely
    that Bob, who hasn't watched Finding Nemo, will like it too.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正如第一章所描述的，协同过滤试图利用社区的力量来提供可靠、相关的，甚至有时是令人惊讶的推荐。如果Alice和Bob大多数情况下喜欢相同的电影（比如《狮子王》、《阿拉丁》和《玩具总动员》），而且Alice还喜欢《海底总动员》，那么Bob很可能也会喜欢这部电影，尽管他还没有看过。
- en: We will be building powerful collaborative filters in the next chapter. However,
    before we do that, it is important that we have a good grasp of the underlying
    techniques, principles, and algorithms that go into building collaborative filters.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章构建强大的协同过滤器。然而，在此之前，了解构建协同过滤器所涉及的基础技术、原理和算法是非常重要的。
- en: 'Therefore, in this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将涵盖以下主题：
- en: '**Similarity measures**: Given two items, how do we mathematically quantify
    how different or similar they are to each other? Similarity measures help us in
    answering this question.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相似度度量**：给定两个项目，我们如何数学地量化它们之间的差异或相似性？相似度度量帮助我们解答这个问题。'
- en: We have already made use of a similarity measure (the cosine score) while building
    our content recommendation engine. In this chapter, we will be looking at a few
    other popular similarity scores.
  id: totrans-7
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在构建内容推荐引擎时，我们已经使用了相似度度量（余弦相似度）。在本章中，我们将介绍一些其他流行的相似度度量。
- en: '**Dimensionality reduction**: When building collaborative filters, we are usually
    dealing with millions of users rating millions of items. In such cases, our user
    and item vectors are going to be of a dimension in the order of millions. To improve
    performance, speed up calculations, and avoid the curse of dimensionality, it
    is often a good idea to reduce the number of dimensions considerably, while retaining
    most of the information. This section of the chapter will describe techniques
    that do just that.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：在构建协同过滤时，我们通常会处理数百万用户对数百万项目的评分。在这种情况下，我们的用户和项目向量的维度往往达到数百万。为了提高性能，加速计算并避免维度灾难，通常建议大幅减少维度的数量，同时保留大部分信息。本章这一部分将描述一些能够实现这一目标的技术。'
- en: '**Supervised learning**: Supervised learning is a class of machine learning
    algorithm that makes use of label data to infer a mapping function that can then
    be used to predict the label (or class) of unlabeled data. We will be looking
    at some of the most popular supervised learning algorithms, such as support vector
    machines, logistic regression, decision trees, and ensembling.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：监督学习是一类利用标注数据推断映射函数的机器学习算法，这个映射函数可以用来预测未标注数据的标签（或类别）。我们将学习一些最流行的监督学习算法，如支持向量机、逻辑回归、决策树和集成方法。'
- en: '**Clustering**: Clustering is a type of unsupervised learning where the algorithm
    tries to divide all the data points into a certain number of clusters. Therefore,
    without the use of a label dataset, the clustering algorithm is able to assign
    classes to all the unlabel points. In this section, we will be looking at k-means
    clustering, a simple but powerful algorithm popularly used in collaborative filters.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：聚类是一种无监督学习方法，算法尝试将所有数据点划分为若干个簇。因此，在没有标签数据集的情况下，聚类算法能够为所有未标注的数据点分配类别。在本节中，我们将学习k-means聚类算法，这是一种简单但功能强大的算法，广泛应用于协同过滤中。'
- en: '**Evaluation methods and metrics**: We will take a look at a few evaluation
    metrics that are used to gauge the performance of these algorithms. The metrics
    include accuracy, precision, and recall.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估方法和指标**：我们将介绍几种用于评估这些算法性能的评估指标，包括准确率、精确度和召回率。'
- en: The topics covered in this chapter merit an entire textbook. Since this is a
    hands-on recommendation engine tutorial, we will not be delving too deeply into
    the functioning of most of the algorithms. Nor will we code them up from scratch.
    What we will do is gain an understanding of how and when they work, their advantages
    and disadvantages, and their easy-to-use implementations using the scikit-learn
    library.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所涉及的主题足以成为一本完整的教材。由于这是一个动手的推荐引擎教程，我们不会过多深入探讨大多数算法的工作原理，也不会从零开始编写它们。我们要做的是理解它们的工作方式及适用时机，它们的优缺点，以及如何利用
    scikit-learn 库轻松实现它们。
- en: Problem statement
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题陈述
- en: 'Collaborative filtering algorithms try to solve the prediction problem (as
    described in the [Chapter 1](c4bff0e9-57b3-44ec-90cb-9e5950696b27.xhtml), *Getting
    Started with Recommender Systems*). In other words, we are given a matrix of i
    users and j items. The value in the ith row and the jth column (denoted by rij)
    denotes the rating given by user i to item j:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 协同过滤算法试图解决预测问题（如[第一章](c4bff0e9-57b3-44ec-90cb-9e5950696b27.xhtml)《推荐系统入门》中所述）。换句话说，我们给定了一个
    i 个用户和 j 个项目的矩阵。矩阵中第 i 行第 j 列的值（记作 rij）表示用户 i 对项目 j 给出的评分：
- en: '![](img/d6fd11cf-ee20-4af0-bcd8-bd9a3c3acc5b.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6fd11cf-ee20-4af0-bcd8-bd9a3c3acc5b.png)'
- en: Matrix of i users and j items
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: i 个用户和 j 个项目的矩阵
- en: Our job is to complete this matrix. In other words, we need to predict all the
    cells in the matrix that we have no data for. For example, in the preceding diagram,
    we are asked to predict whether user E will like the music player item. To accomplish
    this task, some ratings are available (such as User A liking the music player
    and video games) whereas others are not (for instance, we do not know whether
    Users C and D like video games).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是完成这个矩阵。换句话说，我们需要预测矩阵中没有数据的所有单元格。例如，在前面的图示中，我们需要预测用户 E 是否会喜欢音乐播放器项。为完成这一任务，一些评分是已知的（例如用户
    A 喜欢音乐播放器和视频游戏），而另一些则未知（例如我们不知道用户 C 和 D 是否喜欢视频游戏）。
- en: Similarity measures
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相似度度量
- en: From the rating matrix in the previous section, we see that every user can be
    represented as a j-dimensional vector where the kth dimension denotes the rating
    given by that user to the kth item. For instance, let 1 denote a like, -1 denote
    a dislike, and 0 denote no rating. Therefore, user B can be represented as (0,
    1, -1, -1). Similarly, every item can also be represented as an i-dimensional
    vector where the kth dimension denotes the rating given to that item by the kth user.
    The video games item is therefore represented as (1, -1, 0, 0, -1).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一节中的评分矩阵来看，每个用户可以用一个 j 维的向量表示，其中第 k 维表示该用户对第 k 项的评分。例如，设 1 表示喜欢，-1 表示不喜欢，0
    表示没有评分。因此，用户 B 可以表示为 (0, 1, -1, -1)。类似地，每个项目也可以用一个 i 维的向量表示，其中第 k 维表示第 k 用户对该项目的评分。因此，视频游戏项目可以表示为
    (1, -1, 0, 0, -1)。
- en: We have already computed a similarity score for like-dimensional vectors when
    we built our content-based recommendation engine. In this section, we will take
    a look at the other similarity measures and also revisit the cosine similarity
    score in the context of the other scores.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在构建基于内容的推荐引擎时，已经计算了相似度得分，针对相同维度的向量。在本节中，我们将讨论其他相似度度量，并重新审视在其他得分背景下的余弦相似度得分。
- en: Euclidean distance
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: 'The Euclidean distance can be defined as the length of the line segment joining
    the two data points plotted on an *n*-dimensional Cartesian plane. For example,
    consider two points plotted in a 2D plane:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离可以定义为连接在 *n* 维笛卡尔平面上绘制的两个数据点的线段长度。例如，考虑在二维平面上绘制的两个点：
- en: '![](img/1c808a35-3c9d-4bbe-a6ae-e858a3961159.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c808a35-3c9d-4bbe-a6ae-e858a3961159.png)'
- en: Euclidean distance
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: The distance, d, between the two points gives us the Euclidean distance and
    its formula in the 2D space is given in the preceding graph.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 两个点之间的距离 d 给出了欧几里得距离，其在二维空间中的公式如前图所示。
- en: 'More generally, consider two *n*-dimensional points (or vectors):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，考虑两个 *n* 维的点（或向量）：
- en: '**v1**: (q1, q2,...., qn)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**v1**: (q1, q2,...., qn)'
- en: '**v2**: (r1, r2,....., rn)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**v2**: (r1, r2,....., rn)'
- en: 'Then, the Euclidean score is mathematically defined as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，欧几里得得分在数学上定义为：
- en: '![](img/5327766a-fae3-4b13-8664-dfec476932a1.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5327766a-fae3-4b13-8664-dfec476932a1.png)'
- en: 'Euclidean scores can take any value between 0 and infinity. The lower the Euclidean
    score (or distance), the more similar the two vectors are to each other. Let''s
    now define a simple function using NumPy, which allows us to compute the Euclidean
    distance between two *n*-dimensional vectors using the aforementioned formula:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得分数可以取从 0 到无穷大的任何值。欧几里得分数（或距离）越低，两个向量就越相似。现在，我们来定义一个简单的函数，使用 NumPy 来计算两个
    *n* 维向量之间的欧几里得距离，公式如下：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, let''s define three users who have rated five different movies:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义三个对五部不同电影进行了评分的用户：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'From the ratings, we can see that users 1 and 2 have extremely different tastes,
    whereas the tastes of users 1 and 3 are largely similar. Let''s see whether the
    Euclidean distance metric is able to capture this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从评分来看，我们可以看到用户 1 和 2 的口味极为不同，而用户 1 和 3 的口味大致相似。我们来看看欧几里得距离是否能够捕捉到这一点：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The Euclidean distance between users 1 and 2 comes out to be approximately
    7.48:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 用户 1 和 2 之间的欧几里得距离约为 7.48：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Users 1 and 3 have a much smaller Euclidean score between them than users 1
    and 2\. Therefore, in this case, the Euclidean distance was able to satisfactorily
    capture the relationships between our users.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 用户 1 和 3 之间的欧几里得距离明显小于用户 1 和 2 之间的距离。因此，在这种情况下，欧几里得距离能够令人满意地捕捉到我们用户之间的关系。
- en: Pearson correlation
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 皮尔逊相关系数
- en: 'Consider two users, Alice and Bob, who have rated the same five movies. Alice
    is extremely stingy with her ratings and never gives more than a 4 to any movie.
    On the other hand, Bob is more liberal and never gives anything below a 2 when
    rating movies. Let''s define the matrices representing Alice and Bob and compute
    their Euclidean distance:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两位用户 Alice 和 Bob，他们对同五部电影进行了评分。Alice 对评分非常苛刻，任何电影的评分都不超过 4。另一方面，Bob 比较宽松，评分从不低于
    2。我们来定义代表 Alice 和 Bob 的矩阵，并计算他们的欧几里得距离：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We get a Euclidean distance of about 2.23\. However, on closer inspection, we
    see that Bob always gives a rating that is one higher than Alice. Therefore, we
    can say that Alice and Bob's ratings are extremely correlated. In other words,
    if we know Alice's rating for a movie, we can compute Bob's rating for the same
    movie with high accuracy (in this case, by just adding 1).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了约 2.23 的欧几里得距离。然而，经过更仔细的检查，我们发现 Bob 总是给出比 Alice 高 1 分的评分。因此，我们可以说 Alice
    和 Bob 的评分是极其相关的。换句话说，如果我们知道 Alice 对一部电影的评分，我们可以通过简单地加 1 来高精度地计算 Bob 对同一部电影的评分。
- en: 'Consider another user, Eve, who has the polar opposite tastes to Alice:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑另一个用户 Eve，她的口味与 Alice 完全相反：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We get a very high score of 6.32, which indicates that the two people are very
    dissimilar. If we used Euclidean distances, we would not be able to do much beyond
    this. However, on inspection, we see that the sum of Alice's and Eve's ratings
    for a movie always add up to 6\. Therefore, although very different people, one's
    rating can be used to accurately predict the corresponding rating of the other.
    Mathematically speaking, we say Alice's and Eve's ratings are strongly negatively
    correlated.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了一个非常高的得分 6.32，这表明这两个人的差异很大。如果我们使用欧几里得距离，我们无法做出更多的分析。然而，经过检查，我们发现 Alice
    和 Eve 对一部电影的评分总是加起来等于 6。因此，尽管两个人非常不同，但一个人的评分可以用来准确预测另一个人的对应评分。从数学角度来说，我们说 Alice
    和 Eve 的评分是强烈的负相关。
- en: Euclidean distances place emphasis on magnitude, and in the process, are not
    able to gauge the degree of similarity or dissimilarity well. This is where the
    Pearson correlation comes into the picture. The Pearson correlation is a score
    between -1 and 1, where -1 indicates total negative correlation (as in the case
    with Alice and Eve) and 1 indicates total positive correlation (as in the case
    with Alice and Bob), whereas 0 indicates that the two entities are in no way correlated
    with each other (or are independent of each other).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离强调的是大小，而在这个过程中，它无法很好地衡量相似性或差异性。这就是皮尔逊相关系数发挥作用的地方。皮尔逊相关系数是一个介于 -1 和 1 之间的分数，其中
    -1 表示完全负相关（如 Alice 和 Eve 的情况），1 表示完全正相关（如 Alice 和 Bob 的情况），而 0 表示两者之间没有任何相关性（或彼此独立）。
- en: 'Mathematically, the Pearson correlation is defined as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，皮尔逊相关系数的定义如下：
- en: '![](img/d8d9ac65-0fe2-452f-a394-ecf52c6be691.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8d9ac65-0fe2-452f-a394-ecf52c6be691.png)'
- en: Here, ![](img/cf6be388-6b92-4140-9b16-06c11f0d25d7.png) denotes the mean of
    all the elements in vector *i*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/cf6be388-6b92-4140-9b16-06c11f0d25d7.png) 表示向量 *i* 中所有元素的平均值。
- en: 'The SciPy package gives us access to a function that computes the Pearson Similarity
    Scores:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: SciPy包提供了一个计算皮尔逊相似度分数的函数：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first element of our list output is the Pearson score. We see that Alice
    and Bob have the highest possible similarity score, whereas Alice and Eve have
    the lowest possible score.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列表输出的第一个元素是皮尔逊分数。我们看到Alice和Bob有最高可能的相似度分数，而Alice和Eve有最低可能的分数。
- en: Can you guess the similarity score for Bob and Eve?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你能猜出Bob和Eve的相似度分数吗？
- en: Cosine similarity
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 余弦相似度
- en: 'In the previous chapter, we mathematically defined the cosine similarity score
    and used it extensively while building our content-based recommenders:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们数学上定义了余弦相似度分数，并在构建基于内容的推荐系统时广泛使用它：
- en: '![](img/6d9a8b2c-2455-46e6-af8d-24e85bb2a810.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d9a8b2c-2455-46e6-af8d-24e85bb2a810.png)'
- en: 'Mathematically, the Cosine similarity is defined as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，余弦相似度定义如下：
- en: '![](img/c2d6d4ef-6faa-45c4-a75d-db6d6faa9690.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2d6d4ef-6faa-45c4-a75d-db6d6faa9690.png)'
- en: The cosine similarity score computes the cosine of the angle between two vectors
    in an *n*-dimensional space. When the cosine score is 1 (or angle is 0), the vectors
    are exactly similar. On the other hand, a cosine score of -1 (or angle 180 degrees)
    denotes that the two vectors are exactly dissimilar to each other.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度分数计算在 *n* 维空间中两个向量之间的夹角余弦值。当余弦分数为1（或角度为0）时，向量完全相似。另一方面，余弦分数为-1（或角度为180度）表示两个向量完全不相似。
- en: Now, consider two vectors, x and y, both with zero mean. We see that when this
    is the case, the Pearson correlation score is exactly the same as the cosine similarity
    Score. In other words, for centered vectors with zero mean, the Pearson correlation
    is the cosine similarity score.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑两个均值为零的向量 x 和 y。我们看到在这种情况下，皮尔逊相关分数与余弦相似度分数完全相同。换句话说，对于均值为零的中心化向量，皮尔逊相关性即是余弦相似度分数。
- en: Different similarity scores are appropriate in different scenarios. For cases
    where the magnitude is important, the Euclidean distance is an appropriate metric
    to use. However, as we saw in the case described in the Pearson correlation subsection,
    magnitude is not as important to us as correlation. Therefore, we will be using
    the Pearson and the cosine similarity scores when building our filters.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的场景中，适合使用不同的相似度分数。对于重视幅度的情况，欧几里得距离是一个适当的度量标准。然而，正如我们在皮尔逊相关子节中所看到的那样，对我们而言幅度并不像相关性重要。因此，在构建过滤器时，我们将使用皮尔逊和余弦相似度分数。
- en: Clustering
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: One of the main ideas behind collaborative filtering is that if user A has the
    same opinion of a product as user B, then A is also more likely to have the same
    opinion as B on another product than that of a randomly chosen user.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 协同过滤背后的主要思想之一是，如果用户A对某个产品的意见与用户B相同，则A在另一个产品上与B的意见相同的可能性也比随机选择的用户更高。
- en: 'Clustering is one of the most popular techniques used in collaborative-filtering
    algorithms. It is a type of unsupervised learning that groups data points into
    different classes in such a way that data points belonging to a particular class
    are more similar to each other than data points belonging to different classes:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是协同过滤算法中最流行的技术之一。它是一种无监督学习，根据数据点之间的相似性将数据点分组到不同的类中：
- en: '![](img/f30db156-43ab-479f-aa83-cdb67d2f265f.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f30db156-43ab-479f-aa83-cdb67d2f265f.png)'
- en: For example, imagine that all our users were plotted on a two-dimensional Cartesian
    plane, as shown in the preceding graph. The job of a clustering algorithm is to
    assign classes to every point on this plane. Just like the similarity measures,
    there is no one clustering algorithm to rule them all. Each algorithm has its
    specific use case and is suitable only in certain problems. In this section, we
    will be looking only at the k-means clustering algorithm, which will perform a
    satisfactory job is assigning classes to the collection of preceding points. We
    will also see a case where k-means will not prove to be suitable.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们所有的用户都被绘制在一个二维笛卡尔平面上，如前图所示。聚类算法的任务是为平面上的每个点分配类别。就像相似性度量一样，没有一种聚类算法可以解决所有问题。每种算法都有其特定的使用场景，只适用于某些问题。在本节中，我们只会讨论
    k-means 聚类算法，它能够很好地为前述的点集分配类别。我们还将看到一个 k-means 不适用的案例。
- en: k-means clustering
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means 聚类
- en: The k-means algorithm is one of the simplest yet most popular machine learning
    algorithms. It takes in the data points and the number of clusters (k) as input.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 算法是最简单也是最流行的机器学习算法之一。它将数据点和聚类的数量（k）作为输入。
- en: 'Next, it randomly plots k different points on the plane (called centroids).
    After the k centroids are randomly plotted, the following two steps are repeatedly
    performed until there is no further change in the set of k centroids:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，它会随机绘制 k 个不同的点在平面上（称为质心）。在随机绘制了 k 个质心后，以下两个步骤会反复执行，直到 k 个质心的集合不再发生变化：
- en: Assignment of points to the centroids: Every data point is assigned to the centroid
    that is the closest to it. The collection of data points assigned to a particular
    centroid is called a cluster. Therefore, the assignment of points to k centroids
    results in the formation of k clusters.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点到质心的分配：每个数据点会被分配给离它最近的质心。被分配给某个特定质心的数据点集合称为一个聚类。因此，将点分配给 k 个质心会形成 k 个聚类。
- en: 'Reassignment of centroids: In the next step, the centroid of every cluster
    is recomputed to be the center of the cluster (or the average of all the points
    in the cluster). All the data points are then reassigned to the new centroids:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质心的重新分配：在下一步中，每个聚类的质心会重新计算，作为该聚类的中心（或者是该聚类中所有点的平均值）。然后，所有数据点将重新分配到新的质心：
- en: '![](img/df507754-0da0-43a7-8918-9ec46d18fe33.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df507754-0da0-43a7-8918-9ec46d18fe33.png)'
- en: The preceding screenshot shows a visualization of the steps involved in a k-means
    clustering algorithm, with the number of assigned clusters as two.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的截图展示了 k-means 聚类算法步骤的可视化，已分配的聚类数量为两个。
- en: Some sections in this chapter make use of the Matplotlib and Seaborn libraries
    for visualizations. You don't need to understand the plotting code written as
    part of this book, but if you're still interested, you can find the official matplotlib
    tutorial at [https://matplotlib.org/users/pyplot_tutorial.html](https://matplotlib.org/users/pyplot_tutorial.html)
    and the official seaborn tutorial at [https://seaborn.pydata.org/tutorial.html](https://seaborn.pydata.org/tutorial.html).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的某些部分使用了 Matplotlib 和 Seaborn 库进行可视化。你不需要理解书中编写的绘图代码，但如果你仍然感兴趣，可以在[https://matplotlib.org/users/pyplot_tutorial.html](https://matplotlib.org/users/pyplot_tutorial.html)找到官方的
    Matplotlib 教程，在[https://seaborn.pydata.org/tutorial.html](https://seaborn.pydata.org/tutorial.html)找到官方的
    Seaborn 教程。
- en: 'We will not be implementing the k-means algorithm from scratch. Instead, we
    will use its implementation provided by scikit-learn. As a first step, let''s
    access the data points as plotted in the beginning of this section:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会从头实现 k-means 算法，而是使用 scikit-learn 提供的实现。作为第一步，让我们访问本节开头绘制的数据点：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'One of the most important steps while using the k-means algorithm is determining
    the number of clusters. In this case, it can be clearly seen from the plot (and
    the code) that we''ve plotted the points in such a way that they form three clearly
    separable clusters. Let''s now apply the k-means algorithm via scikit-learn and
    assess its performance:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 k-means 算法时，最重要的步骤之一是确定聚类的数量。在这种情况下，从图中（以及代码中）可以清楚地看出，我们已经将点绘制成三个明显可分的聚类。现在，让我们通过
    scikit-learn 应用 k-means 算法并评估其性能：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We see that the algorithm proves to be extremely successful in identifying
    the three clusters. The three final centroids are also marked with an X on the
    plot:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到该算法在识别三个聚类方面表现非常成功。三个最终的质心也在图中标记为 X：
- en: '![](img/60b43706-0325-4a32-a270-ec3193c351f3.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60b43706-0325-4a32-a270-ec3193c351f3.png)'
- en: Choosing k
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择k
- en: As stated in the previous subsection, choosing a good value of k is vital to
    the success of the k-means clustering algorithm. The number of clusters can be
    anywhere between 1 and the total number of data points (where each point is assigned
    to its own cluster).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一小节所述，选择合适的k值对k-means聚类算法的成功至关重要。聚类数可以在1到数据点总数之间的任何数值（每个点被分配到自己的聚类中）。
- en: Data in the real world is seldom of the type explored previously, where the
    points formed well defined, visually separable clusters on a two-dimensional plane.
    There are several methods available to determine a good value of K. In this section,
    we will explore the Elbow method of determining k.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界中的数据很少像之前探讨的那样，其中数据点在二维平面上形成了明确定义、视觉上可分的聚类。为了确定一个合适的K值，有几种方法可供选择。在本节中，我们将探讨通过肘部法则来确定k值的方法。
- en: The Elbow method computes the sum of squares for each value of  k and chooses
    the elbow point of the sum-of-squares v/s  K plot as the best value for k. The
    elbow point is defined as the value of k at which the sum-of-squares value for
    every subsequent k starts decreasing much more slowly.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 肘部法则计算每个k值的平方和，并选择平方和与K的关系图中的肘部点作为k的最佳值。肘部点定义为k值，在该点之后，所有后续k的平方和值开始显著减缓下降。
- en: 'The sum of squares value is defined as the sum of the distances of each data
    point to the centroid of the cluster to which it was assigned. Mathematically,
    it is expressed as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 平方和值被定义为每个数据点到其所属聚类质心的距离的平方和。数学表达式如下：
- en: '![](img/209c345a-a658-4081-9f33-8d91c766aa37.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/209c345a-a658-4081-9f33-8d91c766aa37.png)'
- en: Here, Ck is the kth cluster and uk is the corresponding centroid of Ck.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，Ck是第k个聚类，uk是Ck对应的质心。
- en: 'Fortunately for us, scikit-learn''s implementation of k-means automatically
    computes the value of sum-of-squares when it is computing the clusters. Let''s
    now visualize the Elbow plot for our data and determine the best value of K:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，scikit-learn的k-means实现会在计算聚类时自动计算平方和。现在我们来可视化数据的肘部图，并确定最佳的K值：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/6b410c80-031b-44d6-9e5a-8bb5f1450210.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b410c80-031b-44d6-9e5a-8bb5f1450210.png)'
- en: From the plot, it is clear that the Elbow is at K=3\. From what we visualized
    earlier, we know that this is indeed the optimum number of clusters for this data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看出，肘部出现在K=3处。根据我们之前的可视化结果，我们知道这确实是该数据集的最佳聚类数。
- en: Other clustering algorithms
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他聚类算法
- en: The k-means algorithm, although very powerful, is not ideal for every use case.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管k-means算法非常强大，但并不适用于每一种情况。
- en: 'To illustrate, let''s construct a plot with two half moons. Like the preceding
    blobs, scikit-learn gives us a convenient function to plot half-moon clusters:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我们构造了一个包含两个半月形状的图。与之前的簇形图相似，scikit-learn为我们提供了一个便捷的函数来绘制半月形聚类：
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](img/c23431ea-c235-4672-8fbc-12a742546d04.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c23431ea-c235-4672-8fbc-12a742546d04.png)'
- en: 'Will the k-means algorithm be able to figure out the two half moons correctly?
    Let''s find out:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: k-means算法能否正确识别两个半月形状的聚类？我们来看看：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s now visualize what k-means thinks the two clusters that exist for this
    set of data points are:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来可视化k-means认为这组数据点存在的两个聚类：
- en: '![](img/f399828f-41b5-48ce-b1c3-e37c170b798a.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f399828f-41b5-48ce-b1c3-e37c170b798a.png)'
- en: We see that the k-means algorithm doesn't do a very good job of identifying
    the correct clusters. For clusters such as these half moons, another algorithm,
    called spectral clustering, with nearest-neighbor, affinity performs much better.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，k-means算法在识别正确的聚类上做得不好。对于像这些半月形状的聚类，另一种名为谱聚类的算法，结合最近邻和相似度，表现得要好得多。
- en: 'We will not go into the workings of spectral clustering. Instead, we will use
    its scikit-learn implementation and assess its performance directly:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入探讨谱聚类的原理。相反，我们将使用其在scikit-learn中的实现，并直接评估其性能：
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/43fa2b76-f6b8-46fb-8c2d-4535e14d548e.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43fa2b76-f6b8-46fb-8c2d-4535e14d548e.png)'
- en: We see that spectral clustering does a very good job of identifying the half-moon
    clusters.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，谱聚类在识别半月形聚类方面表现得非常好。
- en: 'We have seen that different clustering algorithms are appropriate in different
    cases. The same applies to cases of collaborative filters. For instance, the surprise package,
    which we will visit in the next chapter, has an implementation of a collaborative
    filter that makes use of yet another clustering algorithm, called co-clustering.
    We will wrap up our discussion of clustering and move on to another important
    data mining technique: dimensionality reduction.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，不同的聚类算法适用于不同的情况。协同过滤的情况也一样。例如，我们将在下一章讨论的 surprise 包，它实现了一种协同过滤方法，使用了另一种聚类算法，叫做共聚类。我们将总结聚类的讨论，并转向另一种重要的数据挖掘技术：降维。
- en: Dimensionality reduction
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: 'Most machine learning algorithms tend to perform poorly as the number of dimensions
    in the data increases. This phenomenon is often known as the curse of dimensionality.
    Therefore, it is a good idea to reduce the number of features available in the
    data, while retaining the maximum amount of information possible. There are two
    ways to achieve this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习算法随着数据维度数量的增加而表现不佳。这种现象通常被称为维度灾难。因此，减少数据中可用特征的数量，同时尽可能保留最多的信息，是一个好主意。实现这一目标有两种方式：
- en: '**Feature selection**: This method involves identifying the features that have
    the least predictive power and dropping them altogether. Therefore, feature selection
    involves identifying a subset of features that is most important for that particular
    use case. An important distinction of feature selection is that it maintains the
    original meaning of every retained feature. For example, let''s say we have a
    housing dataset with price, area, and number of roomsas features. Now, if we were
    to drop the areafeature, the remaining price and number of roomsfeatures will
    still mean what they did originally.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择**：这种方法涉及识别出最少预测能力的特征，并将其完全删除。因此，特征选择涉及识别出对特定用例最重要的特征子集。特征选择的一个重要特点是，它保持每个保留特征的原始意义。例如，假设我们有一个包含价格、面积和房间数量的住房数据集。如果我们删除了面积特征，那么剩下的价格和房间数量特征仍然会保留其原本的意义。'
- en: '**Feature extraction**: Feature extraction takes in *m*-dimensional data and
    transforms it into an *n*-dimensional output space (usually where *m* >> *n*),
    while retaining most of the information. However, in doing so, it creates new
    features that have no inherent meaning. For example, if we took the same housing
    dataset and used feature extraction to output it into a 2D space, the new features
    won''t mean price, area, or number of rooms.They will be devoid of any meaning.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：特征提取将 *m* 维数据转化为 *n* 维输出空间（通常 *m* >> *n*），同时尽可能保留大部分信息。然而，在此过程中，它创建了没有内在意义的新特征。例如，如果我们使用特征提取将同一个住房数据集输出为二维空间，新的特征将不再表示价格、面积或房间数量。它们将不再具有任何意义。'
- en: In this section, we will take a look at an important feature-extraction method: **Principal
    component analysis** (or **PCA**).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一个重要的特征提取方法：**主成分分析**（或**PCA**）。
- en: Principal component analysis
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: '**Principal component analysis** is an unsupervised feature extraction algorithm
    that takes in *m*-dimensional input to create a set of *n* (*m* >> *n*) linearly
    uncorrelated variables (called principal components) in such a way that the *n *dimensions
    lose as little variance (or information) as possible due to the loss of the (*m*-*n*)
    dimensions.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**是一种无监督的特征提取算法，它将 *m* 维输入转化为一组 *n* （*m* >> *n*）线性不相关的变量（称为主成分），以尽可能减少由于丢失 (*m*
    - *n*) 维度而导致的方差（或信息）损失。'
- en: The linear transformation in PCA is done in such a way that the first principal
    component holds the maximum variance (or information). It does so by considering
    those variables that are highly correlated to each other. Every principal component
    has more variance than every succeeding component and is orthogonal to the preceding
    component.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: PCA中的线性变换是以这样的方式进行的：第一个主成分具有最大方差（或信息）。它通过考虑那些彼此高度相关的变量来实现这一点。每个主成分的方差都大于后续成分，且与前一个成分正交。
- en: 'Consider a three-dimensional space where two features are highly correlated
    to each other and relatively uncorrelated to the third:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个三维空间，其中两个特征高度相关，而与第三个特征的相关性较低：
- en: '![](img/9f8c85c1-cbc2-44e8-8b2a-9c4a34fcd4e0.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f8c85c1-cbc2-44e8-8b2a-9c4a34fcd4e0.png)'
- en: Let's say that we want to convert this into a two-dimensional space. To do this,
    PCA tries to identify the first principal component, which will hold the maximum
    possible variance. It does so by defining a new dimension using the two highly
    correlated variables. Now, it tries to define the next dimension in such a way
    that it holds the maximum variance, is orthogonal to the first principal component
    constructed, and also is uncorrelated to it. The two new dimensions (or principal
    components), PC 1 and PC 2, are shown in the preceding figure.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们希望将其转换为二维空间。为此，PCA试图识别第一个主成分，该主成分将包含最大可能的方差。它通过定义一个新维度，使用这两个高度相关的变量来实现。接下来，它试图定义下一个维度，使其具有最大方差，与第一个主成分正交，并且与其不相关。前面的图展示了这两个新维度（或主成分），PC
    1和PC 2。
- en: Understanding the PCA algorithm requires linear algebraic concepts that are
    beyond the scope of this book. Instead, we will use the black box implementation
    of PCA that `scikit-learn` gives us and consider a use case with the well-known
    Iris dataset.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解PCA算法，需要一些超出本书范围的线性代数概念。相反，我们将使用`scikit-learn`提供的PCA黑盒实现，并以著名的Iris数据集为例来考虑一个实际案例。
- en: 'The first step is to load the Iris dataset from the UCI machine learning repository
    into a pandas DataFrame:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将Iris数据集从UCI机器学习库加载到pandas DataFrame中：
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/6a8745dd-6316-4c05-9afb-81f42946e2d7.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a8745dd-6316-4c05-9afb-81f42946e2d7.png)'
- en: 'The PCA algorithm is extremely sensitive to scale. Therefore, we are going
    to scale all the features in such a way that they have a mean of 0 and a variance
    of 1:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: PCA算法对数据的尺度非常敏感。因此，我们将对所有特征进行缩放，使其具有均值为0、方差为1的分布：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/0365714a-25dc-45f9-a203-f1eddb655995.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0365714a-25dc-45f9-a203-f1eddb655995.png)'
- en: 'We''re now in a good place to apply the PCA algorithm. Let''s transform our
    data into the two-dimensional space:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好应用PCA算法了。让我们将数据转换到二维空间中：
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/d5b1f7a0-f605-4645-a932-062be42e66e6.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5b1f7a0-f605-4645-a932-062be42e66e6.png)'
- en: 'The `scikit-Learn`''s PCA implementation also gives us information about the
    ratio of variance contained by each principal component:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`的PCA实现还会为我们提供每个主成分所包含的方差比率信息：'
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We see that the first principal component holds about 72.8% of the information,
    whereas the second principal component holds about 23.3%. In total, 95.8% of the
    information is retained, whereas 4.2% of the information is lost in removing two
    dimensions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，第一个主成分包含大约72.8%的信息，而第二个主成分包含大约23.3%的信息。总的来说，保留了95.8%的信息，而去除两个维度则损失了4.2%的信息。
- en: 'Finally, let''s visualize our data points by class in the new 2D plane:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们在新的二维平面中按类别可视化我们的数据点：
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/d48fdcaf-da25-44dc-826b-40695edab808.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d48fdcaf-da25-44dc-826b-40695edab808.png)'
- en: Other dimensionality reduction techniques
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他降维技术
- en: Linear-discriminant analysis
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性判别分析
- en: Like PCA, linear-discriminant analysis is a linear transformation method that
    aims to transform *m*-dimensional data into an *n*-dimensional output space.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 与PCA类似，线性判别分析是一种线性变换方法，旨在将*m*维数据转换为*n*维输出空间。
- en: However, unlike PCA, which tries to retain the maximum information, LDA aims
    to identify a set of *n* features that result in the maximum separation (or discrimination)
    of classes. Since LDA requires labeled data in order to determine its components,
    it is a type of supervised learning algorithm.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与PCA不同，PCA旨在保留最大的信息量，而LDA则旨在识别一组*n*个特征，从而使类之间的分离度（或判别度）最大化。由于LDA需要标注的数据来确定其成分，因此它是一种监督学习算法。
- en: 'Let''s now apply the LDA algorithm to the Iris dataset:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们对Iris数据集应用LDA算法：
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/902577ac-de3d-4433-89f0-5f0c0ee1f823.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/902577ac-de3d-4433-89f0-5f0c0ee1f823.png)'
- en: We see that the classes are much more separable than in PCA.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，类之间的可分性比PCA方法下要好得多。
- en: Singular value decomposition
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奇异值分解
- en: Singular value decomposition, or SVD, is a type of matrix analysis technique
    that allows us to represent a high-dimensional matrix in a lower dimension. SVD
    achieves this by identifying and removing the less important parts of the matrix
    and producing an approximation in the desired number of dimensions.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 奇异值分解（SVD）是一种矩阵分析技术，它允许我们将一个高维矩阵表示为低维矩阵。SVD通过识别并去除矩阵中不重要的部分来实现这一点，从而在所需的维度数中生成一个近似值。
- en: The SVD approach to collaborative filtering was first proposed by Simon Funk
    and proved to be extremely popular and effective during the Netflix prize competition.
    Unfortunately, understanding SVD requires a grasp of linear algebraic topics that
    are beyond the scope of this book. However, we will use a black box implementation
    of the SVD collaborative filter as provided by the `surprise`package in the next
    chapter.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 协同过滤的 SVD 方法最早由 Simon Funk 提出，并在 Netflix 奖竞赛中证明了其极高的受欢迎度和有效性。不幸的是，理解 SVD 需要掌握超出本书范围的线性代数知识。不过，在下一章中，我们将使用
    `surprise` 包提供的黑箱实现来进行 SVD 协同过滤。
- en: Supervised learning
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有监督学习
- en: Supervised learning is a class of machine learning algorithm that takes in a
    series of vectors and their corresponding output (a continuous value or a class)
    as input, and produces an inferred function that can be used to map new examples.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督学习是一类机器学习算法，它接收一系列向量及其对应的输出（一个连续值或一个类别）作为输入，生成一个推断函数，该函数可用于映射新的例子。
- en: An important precondition for using supervised learning is the availability
    of labeled data. In other words, it is necessary that we have access to input
    for which we already know the correct output.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用有监督学习的一个重要前提是需要有标注数据。换句话说，我们必须能够访问已经知道正确输出的输入数据。
- en: 'Supervised learning can be classified into two types: classification and regression.
    A classification problem has a discrete set of values as the target variable (for
    instance, a likeand a dislike), whereas a regression problem has a continuous
    value as its target (for instance, an average rating between one and five).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督学习可以分为两种类型：分类和回归。分类问题的目标变量是一个离散值集（例如，喜欢和不喜欢），而回归问题的目标是一个连续值（例如，介于一到五之间的平均评分）。
- en: Consider the rating matrix defined earlier. It is possible to treat (*m-1*)
    columns as the input and the m^(th) column as the target variable. In this way,
    it should be possible to predict an unavailable value in the m^(th) column by
    passing in the corresponding (m-1) dimensional vector.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑前面定义的评分矩阵。可以将（*m-1*）列作为输入，而第 m^(th) 列作为目标变量。通过这种方式，我们应该能够通过传入相应的（m-1）维向量来预测
    m^(th) 列中不可用的值。
- en: Supervised learning is one of the most mature subfields of machine learning
    and, as a result, there are plenty of potent algorithms available for performing
    accurate predictions. In this section, we will look at some of the most popular
    algorithms used successfully in a variety of applications (including collaborative
    filters).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督学习是机器学习中最成熟的子领域之一，因此，现有许多强大的算法可以用于进行准确的预测。在这一部分，我们将探讨一些在多种应用中成功使用的最流行算法（包括协同过滤器）。
- en: k-nearest neighbors
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-最近邻
- en: '**k-nearest neighbors** (**k-NN**) is perhaps the simplest machine learning
    algorithm. In the case of classification, it assigns a class to a particular data
    point by a majority vote of its *k* nearest neighbors. In other words, the data
    point is assigned the class that is the most common among its k-nearest neighbors.
    In the case of regression, it computes the average value for the target variable
    based on its k-nearest neighbors.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**k-最近邻** (**k-NN**) 可能是最简单的机器学习算法。在分类问题中，它通过其 *k* 个最近邻的多数投票为特定数据点分配一个类别。换句话说，数据点被分配给在其
    k 个最近邻中最常见的类别。在回归问题中，它基于 k 个最近邻计算目标变量的平均值。'
- en: Unlike most machine learning algorithms, k-NN is non-parametric and lazy in
    nature. The former means that k-NN does not make any underlying assumptions about
    the distribution of the data. In other words, the model structure is determined
    by the data. The latter means that k-NN undergoes virtually no training. It only
    computes the k-nearest neighbors of a particular point in the prediction phase.
    This also means that the k-NN model needs to have access to the training data
    at all times and cannot discard it during prediction like its sister algorithms.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数机器学习算法不同，k-NN 是非参数且懒惰的。前者意味着 k-NN 不对数据的分布做任何假设，换句话说，模型结构是由数据决定的。后者意味着 k-NN
    几乎不进行任何训练，它只在预测阶段计算特定点的 k 个最近邻。这也意味着 k-NN 模型在预测过程中需要始终访问训练数据，不能像其他算法那样在预测时丢弃训练数据。
- en: Classification
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: '![](img/d37d07d3-2ce0-4165-ba28-5157422bb1f5.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d37d07d3-2ce0-4165-ba28-5157422bb1f5.png)'
- en: k-NN classification is best explained with the help of an example. Consider
    a dataset that has binary classes (represented as the blue squares and the red
    triangles). k-NN now plots this into *n*-dimensional space (in this case, two
    dimensions).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN分类最好通过一个示例来解释。考虑一个具有二进制类的数据集（表示为蓝色方块和红色三角形）。k-NN现在将其绘制到*n*维空间中（在这种情况下是二维）。
- en: Let's say we want to predict the class of the green circle. Before the k-NN
    algorithm can make predictions, it needs to know the number of nearest neighbors
    that have to be taken into consideration (the value of *k*). *k* is usually odd
    (to avoid ties in the case of binary classification).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想预测绿色圆圈的类。在k-NN算法做出预测之前，它需要知道需要考虑的最近邻的数量（即*k*的值）。*k*通常是奇数（以避免在二进制分类的情况下出现平局）。
- en: Consider the case where *k=3*.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 假设*k=3*。
- en: k-NN computes the distance metric (usually the Euclidean distance) from the
    green circle to every other point in the training dataset and selects the three
    data points that are closest to it. In this case, these are the points contained
    in the solid inner circle.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN计算绿色圆圈与训练数据集中每个其他点的距离度量（通常是欧几里得距离），并选择与其最接近的三个数据点。在这种情况下，这些点位于实心内圈内。
- en: The next step is to determine the majority class among the three points. There
    are two red triangles and one blue square. Therefore, the green circle is assigned
    the class of red triangle.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是确定三点中的多数类。这里有两个红色三角形和一个蓝色方块。因此，绿色圆圈被分配为红色三角形类。
- en: Now, consider the case where *k=5*.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设*k=5*。
- en: In this case, the nearest neighbors are all the points contained within the
    dotted outer circle. This time around, we have two red triangles and three blue
    squares. Therefore, the green circle is assigned the class of blue square.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最近邻是所有包含在虚线外圈内的点。这次，我们有两个红色三角形和三个蓝色方块。因此，绿色圆圈被分配为蓝色方块类。
- en: From the preceding case, it is clear that the value of *k* is extremely significant
    in determining the final class assigned to a data point. It is often a good practice
    to test different values of *k* and assess its performance with your cross-validation
    and test datasets.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的例子可以看出，*k*的值在确定数据点最终分配的类时至关重要。通常最好测试不同的*k*值，并使用交叉验证和测试数据集评估其性能。
- en: Regression
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归
- en: k-NN regression works in almost the same way. Instead of classes, we compute
    the property values of the k-NN.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN回归几乎以相同的方式工作。不同之处在于，我们计算的是k-NN的属性值，而不是类。
- en: Imagine that we have a housing dataset and we're trying to predict the price
    of a house. The price of a particular house will therefore be determined by the
    average of the prices of the houses of its *k* nearest neighbors. As with classification,
    the final target value may differ depending on the value of *k*.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个住房数据集，并且我们正在尝试预测一栋房子的价格。因此，特定房子的价格将由其*k*个最近邻房子的平均价格来决定。与分类一样，最终的目标值可能会根据*k*的值有所不同。
- en: For the rest of the algorithms in this section, we will go through only the
    classification process. However, just like k-NN, most algorithms require only
    very slight modifications to be suitable for use in a regression problem.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节中的其余算法，我们将只介绍分类过程。然而，像k-NN一样，大多数算法只需要做出非常小的修改，就可以适用于回归问题。
- en: Support vector machines
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: The support vector machine is one of the most popular classification algorithms
    used in the industry. It takes in an *n*-dimensional dataset as input and constructs
    an (*n-1*) dimensional hyperplane in such a way that there is maximum separation
    of  classes.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机是行业中最流行的分类算法之一。它将一个*n*维数据集作为输入，并构建一个(*n-1*)维超平面，使得类之间的分离最大化。
- en: 'Consider the visualization of a binary dataset in the following screenshot:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看下面截图中二进制数据集的可视化：
- en: '![](img/4cb564ba-4cc1-4bb3-aa45-4849836ccaf5.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cb564ba-4cc1-4bb3-aa45-4849836ccaf5.png)'
- en: The preceding graph shows three possible hyperplanes (the straight lines) that
    separate the two classes. However, the solid line is the one with the maximum
    margin. In other words, it is the hyperplane that maximally separates the two
    classes. Also, it divides the entire plane into two regions. Any point below the
    hyperplane will be classified as a red square, and any point above will be classified
    as a blue circle.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图展示了三个可能的超平面（直线），它们将两个类别分开。然而，实线是具有最大间隔的超平面。换句话说，它是最大程度上将两个类别分开的超平面。同时，它将整个平面分为两个区域。任何位于超平面下方的点将被分类为红色方块，而任何位于上方的点将被分类为蓝色圆圈。
- en: 'The SVM model is only dependent on `support vectors`*; *these are the points
    that determine the maximum margin possible between the two classes. In the preceding
    graph, the filled squares and circles are the support vectors. The rest of the
    points do not have an effect on the workings of the SVM:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: SVM模型仅依赖于`support vectors`*；*这些是确定两个类别之间最大间隔的点。在上面的图中，填充的方块和圆圈是支持向量。其余的点对SVM的运作没有影响：
- en: '![](img/122d0c7b-0c42-4c81-8483-c8a36025727e.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/122d0c7b-0c42-4c81-8483-c8a36025727e.png)'
- en: SVMs are also capable of separating classes that are not linearly separable
    (such as in the preceding figure). It does so with special tools, called radial
    kernel functions, that plot the points in a higher dimension and attempt to construct
    a maximum margin hyperplane there.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: SVM也能够分离那些非线性可分的类别（如前面的图所示）。它通过特殊的工具——径向核函数，来完成这一点，这些函数将数据点绘制到更高的维度，并尝试在那里构建最大间隔超平面。
- en: Decision trees
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision trees are extremely fast and simple tree-based algorithms that branch
    out on features that result in the largest information gain*. *Decision trees,
    although not very accurate, are extremely interpretable.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是非常快速且简单的基于树的算法，它根据信息增益最大的特征进行分支。*决策树虽然不够准确，但具有极高的可解释性。*
- en: 'We will not delve into the inner workings of the decision tree, but we will
    see it in action via a visualization:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入探讨决策树的内部工作原理，但我们会通过可视化展示它的实际操作：
- en: '![](img/dc4b45e3-bf2a-43ab-84ad-b2fda443aa18.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc4b45e3-bf2a-43ab-84ad-b2fda443aa18.png)'
- en: Let's say we want to classify the Iris dataset using a decision tree. A decision
    tree performing the classification is shown in the preceding diagram. We start
    at the top and go deeper into the tree until we reach a leaf node.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想使用决策树对鸢尾花数据集进行分类。前面的图中展示了执行分类的决策树。我们从顶部开始，一直到树的深处，直到到达叶节点。
- en: For example, if the petal width of a flower is less than 0.8 cm, we reach a
    leaf node and it gets classified as setosa*. *If not, it goes into the other branch
    and the process continues until a leaf node is reached.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果花的花瓣宽度小于0.8厘米，我们就会到达一个叶节点，并将其分类为setosa*。*如果不满足条件，它会进入另一个分支，直到到达一个叶节点。
- en: Decision trees have an element of randomness in their workings and come up with
    different conditions in different iterations. As stated before, they are also
    not very accurate in their predictions. However, their randomness and fast execution
    make them extremely popular in ensemble models, which will be explained in the
    next section.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在其运作中具有随机性，并且在不同的迭代中会得出不同的条件。正如前面所说，它们的预测精度也不高。然而，它们的随机性和快速执行使得它们在集成模型中非常流行，接下来的部分将对此进行解释。
- en: Ensembling
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成
- en: The main idea behind ensembling is that the predictive power of multiple algorithms
    is much greater than a single algorithm. Decision trees are the most common base
    algorithm used when building ensembling models.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 集成的主要思想是多个算法的预测能力远大于单一算法。在构建集成模型时，决策树是最常用的基础算法。
- en: Bagging and random forests
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 装袋法与随机森林
- en: Bagging is short for bootstrap aggregating. Like most other ensemble methods,
    it averages over a large number of base classification models and averages their
    results to deliver its final prediction.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 装袋法是自助法聚合的简称。像大多数其他集成方法一样，它会对大量的基础分类模型进行平均，并将它们的结果平均化以提供最终预测。
- en: 'These are the steps involved in building a bagging model:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 构建装袋模型的步骤如下：
- en: A certain percentage of the data points are sampled (say 10%). The Sampling
    is done with replacement. In other words, a particular data point can appear in
    multiple iterations.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一定比例的数据点会被抽样（假设为10%）。抽样是有放回的。换句话说，一个特定的数据点可以在多个迭代中出现。
- en: A baseline classification model (typically a decision tree) is trained on this
    sampled data.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基线分类模型（通常是决策树）在这些抽样数据上进行训练。
- en: This process is repeated until *n* number of models are trained. The final prediction
    delivered by the bagging model is the average of all the predictions of all the
    base models.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个过程会一直重复，直到训练出*n*个模型为止。Bagging模型的最终预测是所有基模型预测的平均值。
- en: 'An improvement on the bagging model is the random forest model. In addition
    to sampling data points, the random forest ensemble method also forces each baseline
    model to randomly select a subset of the features (usually a number equal to the
    square root of the total number of features):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对Bagging模型的改进是随机森林模型。除了对数据点进行采样外，随机森林集成方法还强制每个基准模型随机选择一部分特征（通常是等于特征总数平方根的数目）：
- en: '![](img/ef09911f-dd49-461f-9d27-baa0ad54e2a6.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef09911f-dd49-461f-9d27-baa0ad54e2a6.png)'
- en: Selecting a subset of samples, as well as features, to build the baseline decision
    trees greatly enhances the randomness of each individual tree. This, in turn,
    increases the robustness of the random forest and allows it to perform extremely
    well with noisy data.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一部分样本和特征来构建基准决策树，大大增强了每棵树的随机性。这反过来提高了随机森林的鲁棒性，使其能够在噪声数据上表现得非常好。
- en: Additionally, building baseline models from a subset of features and analyzing
    their contribution to the final prediction also allows the random forest to determine
    the importance of each feature. It is therefore possible to perform feature-selectionusing
    random forests (recall that feature-selection is a type of dimensionality reduction).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，从特征子集构建基准模型并分析其对最终预测的贡献，还使得随机森林能够确定每个特征的重要性。因此，通过随机森林进行特征选择是可能的（回想一下，特征选择是一种降维方法）。
- en: Boosting
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升算法
- en: The bagging and the random forest models train baseline models that are completely
    independent of each other. Therefore, they do not learn from the mistakes that
    each learner has made. This is where boosting comes into play.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging和随机森林模型训练出的基准模型是完全相互独立的。因此，它们不会从每个学习器所犯的错误中学习。这就是提升算法发挥作用的地方。
- en: Like random forests, boosting models build a baseline model using a subset of
    samples and features. However, while building the next learners, the boosting
    model tries to rectify the mistakes that the previous learners made. Different
    boosting algorithms do this in different ways.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 和随机森林一样，提升模型通过使用样本和特征的子集来构建基准模型。然而，在构建下一个学习器时，提升模型试图纠正前一个学习器所犯的错误。不同的提升算法以不同的方式实现这一点。
- en: For example, the original boosting algorithm simply added 50% of the misclassified
    samples to the second learner, and all the samples that the first two learners
    disagree upon to build the third and final learner. This ensemble of three learners
    was then used to make predictions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，原始的提升算法只是将50%的错误分类样本加入第二个学习器，并将前两个学习器不同意见的所有样本加入，构建第三个最终学习器。然后，使用这三种学习器的集成进行预测。
- en: Boosting algorithms are extremely robust and routinely provide high performance.
    This makes them extremely popular in data science competitions and, as far as
    we are concerned, in building powerful collaborative filters.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 提升算法极其鲁棒，通常能够提供高性能。这使得它们在数据科学竞赛中非常流行，而就我们而言，它们在构建强大的协同过滤器时也非常有用。
- en: 'The `scikit-learn` gives us access to implementations of all the algorithms
    described in this section. The usage of every algorithm is almost the same. As
    an illustration, let''s apply gradient boosting to classify the Iris dataset:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`为我们提供了本节所描述的所有算法的实现。每个算法的使用方式几乎相同。作为示例，让我们应用梯度提升算法对鸢尾花数据集进行分类：'
- en: '[PRE19]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We see that the classifier achieves a 97.3% accuracy on the unseen test data.
    Like random forests, gradient boosting machines are able to gauge the predictive
    power of each feature. Let''s plot the feature importances of the Iris dataset:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，分类器在未见过的测试数据上达到了97.3%的准确率。像随机森林一样，梯度提升机能够衡量每个特征的预测能力。让我们绘制一下鸢尾花数据集的特征重要性：
- en: '[PRE20]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/66f0e4eb-ffbc-4c0d-9549-39053234113c.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/66f0e4eb-ffbc-4c0d-9549-39053234113c.png)'
- en: Evaluation metrics
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估指标
- en: In this section, we will take a look at a few metrics that will allow us to
    mathematically quantify the performance of our classifiers, regressors, and filters.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看一些指标，这些指标可以让我们从数学角度量化分类器、回归器和过滤器的性能。
- en: Accuracy
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确率
- en: 'Accuracy is the most widely used metric to gauge the performance of a classification
    model. It is the ratio of the number of correct predictions to the total number
    of predictions made by the model:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是最常用的分类模型性能衡量指标。它是正确预测的案例数与模型预测的总案例数之间的比值：
- en: '![](img/c4e870eb-dca2-45be-900d-2439cd5b75f4.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4e870eb-dca2-45be-900d-2439cd5b75f4.png)'
- en: Root mean square error
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均方根误差
- en: 'The **Root Mean Square Error** (or **RMSE**) is a metric widely used to gauge
    the performance of regressors. Mathematically, it is represented as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方根误差** (**RMSE**) 是一种广泛用于衡量回归模型表现的指标。从数学上讲，它可以表示为：'
- en: '![](img/619cb11f-b47a-428e-92d9-05bf08b0a841.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/619cb11f-b47a-428e-92d9-05bf08b0a841.png)'
- en: Here, ![](img/7ca4e738-dbfe-4c82-9f9c-d6ac7ff621a0.png) is the i^(th) real target
    value and ![](img/7ae9ea8f-9150-4c9a-8028-df08658eff7e.png) is the i^(th) predicted
    target value.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/7ca4e738-dbfe-4c82-9f9c-d6ac7ff621a0.png) 是第i^(个)实际目标值，![](img/7ae9ea8f-9150-4c9a-8028-df08658eff7e.png)
    是第i^(个)预测目标值。
- en: Binary classification metrics
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二分类指标
- en: Sometimes, accuracy does not give us a good estimate of the performance of a
    model.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，准确率无法很好地估计模型的表现。
- en: For instance, consider a binary class dataset where 99% of the data belongs
    to one class and only 1% of the data belongs to the other class. Now, if a classifier
    were to always predict the majority class for every data point, it would have
    99% accuracy. But that wouldn't mean that the classifier is performing well.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，考虑一个二分类数据集，其中99%的数据属于一个类别，只有1%的数据属于另一个类别。如果一个分类器总是预测多数类，那么它的准确率会是99%。但这并不意味着分类器表现得好。
- en: 'For such cases, we make use of other metrics. To understand them, we first
    need to define a few terms:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种情况，我们需要使用其他指标。为了理解这些指标，我们首先需要定义一些术语：
- en: '**True positive** (**TP**): True positive refers to all cases where the actual
    and the predicted classes are both positive'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正例** (**TP**)：真正例指的是实际类别和预测类别都为正类的所有情况。'
- en: '**True negative** (**TN**): True negative refers to all cases where the actual
    and the predicted classes are both negative'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真负例** (**TN**)：真负例指的是实际类别和预测类别都为负类的所有情况。'
- en: '**False positive** (**FP**):These are all the cases where the actual class
    is negative but the predicted class is positive'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假正例** (**FP**)：假正例是指实际类别为负类，但预测类别为正类的所有情况。'
- en: '**False negative** (**FN**):These are all the cases where the actual class
    is positive but the predicted class is negative'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假负例** (**FN**)：假负例是指实际类别为正类，但预测类别为负类的所有情况。'
- en: To illustrate, consider a test that tries to determine whether a person has
    cancer. If the test predicts that a person does have cancer when in fact they
    don't, it is a false positive. On the other hand, if the test fails to detect
    cancer in a person actually suffering from it, it is a false negative.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，假设有一项测试试图确定某人是否患有癌症。如果测试预测某人患有癌症，而实际并未患病，那么就是假正例。相反，如果测试未能检测到某人实际上患有癌症，那么就是假负例。
- en: Precision
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确率
- en: 'The precision is the ratio of the number of positive cases that were correct
    to all the cases that were identified as positive. Mathematically, it looks like
    this:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 精确率是指正确识别出的正类案例数与所有被识别为正类的案例数的比值。从数学上看，它如下所示：
- en: '![](img/b5aa14fe-1440-4bad-bc9a-f602f717df1a.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5aa14fe-1440-4bad-bc9a-f602f717df1a.png)'
- en: Recall
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 召回率
- en: 'The recall is the ratio of the number of positive cases that were identified
    to the all positive cases present in the dataset:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率是指识别出的正类案例数与数据集中所有正类案例数的比值：
- en: '![](img/44d25d38-9af6-489d-8231-4f4aaa40ae3a.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/44d25d38-9af6-489d-8231-4f4aaa40ae3a.png)'
- en: F1 score
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: F1分数
- en: 'The F1 score is a metric that conveys the balance between precision and recall.
    It is the harmonic mean of the precision and recall. An F1 score of 1 implies
    perfect precision and recall, whereas a score of 0 implies precision and recall
    are not possible:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: F1分数是一个衡量精确率和召回率之间平衡的指标。它是精确率和召回率的调和平均数。F1分数为1表示精确率和召回率完美，F1分数为0则表示精确率和召回率都无法达到：
- en: '![](img/dcd94ad1-96f6-4e27-84c9-d6f42e1efee2.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dcd94ad1-96f6-4e27-84c9-d6f42e1efee2.png)'
- en: Summary
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have covered a lot of topics that will help us to build
    powerful collaborative filters. We took a look at clustering, a form of unsupervised
    learning algorithm that could help us to segregate users into well defined clusters.
    Next, we went through a few dimensionality reduction techniques to overcome the
    curse of dimensionality and improve the performance of our learning algorithms.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了许多主题，这些主题将帮助我们构建强大的协同过滤器。我们看到了聚类，它是一种无监督学习算法，能够帮助我们将用户划分为明确的群体。接下来，我们通过了一些降维技术，以克服维度灾难并提升学习算法的性能。
- en: The subsequent section dealt with supervised learning algorithms, and finally
    we ended the chapter with a brief overview of various evaluation metrics.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的章节讨论了监督学习算法，最后我们以各种评估指标的简要概述结束了本章。
- en: The topics covered in this chapter merit an entire book and we did not analyze
    the techniques in the depth usually required of machine learning engineers. However,
    what we have learned in this chapter should be sufficient to help us build and
    understand collaborative filters, which is one of the main objectives of this
    book. In case you're interested, a more detailed treatment of the topics presented
    in this chapter is available in an excellent book entitled *Python Machine Learning *by
    Sebastian Thrun.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主题足以成为一本书的内容，我们并没有像通常的机器学习工程师那样深入分析这些技术。然而，我们在本章中学到的内容足以帮助我们构建和理解协同过滤器，这也是本书的主要目标之一。如果你有兴趣，关于本章所呈现主题的更详细处理，可以参考Sebastian
    Thrun的优秀书籍《*Python机器学习*》。
