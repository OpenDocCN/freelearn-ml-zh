- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Visualizing Convolutional Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化卷积神经网络
- en: Up to this point, we have only dealt with tabular data and, briefly, text data,
    in *Chapter 5*, *Local Model-Agnostic Interpretation Methods*. This chapter will
    exclusively explore interpretation methods that work with images and, in particular,
    with the **Convolutional Neural Network** (**CNN**) models that train image classifiers.
    Typically, deep learning models are regarded as the epitome of black box models.
    However, one of the benefits of a CNN is how easily it lends itself to visualization,
    so we can not only visualize outcomes but also every step of the learning process
    with **activations**. The possibility of interpreting these steps is rare among
    so-called black box models. Once we have grasped how CNNs learn, we will study
    how to use state-of-the-art gradient-based attribution methods, such as *saliency
    maps* and *Grad-CAM* to debug class attribution. Lastly, we will extend our attribution
    debugging know-how with perturbation-based attribution methods such as *occlusion
    sensitivity* and `KernelSHAP`.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只处理了表格数据，以及在*第五章*，*局部模型无关解释方法*中简要提到的文本数据。本章将专门探讨适用于图像的解释方法，特别是训练图像分类器的**卷积神经网络**（**CNN**）模型。通常，深度学习模型被视为黑盒模型的典范。然而，CNN的一个优点是它很容易进行可视化，因此我们不仅可以可视化结果，还可以通过**激活**来可视化学习过程中的每一步。在所谓的黑盒模型中，解释这些步骤的可能性是罕见的。一旦我们掌握了CNN的学习方式，我们将研究如何使用最先进的基于梯度的属性方法，如*显著性图*和*Grad-CAM*来调试类别属性。最后，我们将通过基于扰动的属性方法，如*遮挡敏感性*和`KernelSHAP`来扩展我们的属性调试知识。
- en: 'These are the main topics we are going to cover:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们将要讨论的主要主题：
- en: Assessing the CNN classifier with traditional interpretation methods
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用传统解释方法评估CNN分类器
- en: Visualizing the learning process with an activation-based method
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于激活的方法可视化学习过程
- en: Evaluating misclassifications with gradient-based attribution methods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于梯度的属性方法评估误分类
- en: Understanding classifications with perturbation-based attribution methods
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于扰动的属性方法理解分类
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter’s example uses the `mldatasets`, `pandas`, `numpy`, `sklearn`,
    `tqdm`, `torch`, `torchvision`, `pytorch-lightning`, `efficientnet-pytorch`, `torchinfo`,
    `matplotlib`, `seaborn`, and `captum` libraries. Instructions on how to install
    all of these libraries are in the *Preface*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例使用了`mldatasets`、`pandas`、`numpy`、`sklearn`、`tqdm`、`torch`、`torchvision`、`pytorch-lightning`、`efficientnet-pytorch`、`torchinfo`、`matplotlib`、`seaborn`和`captum`库。如何安装所有这些库的说明在*前言*中。
- en: 'The code for this chapter is located here: [https://packt.link/qzUvD](https://packt.link/qzUvD).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于此处：[https://packt.link/qzUvD](https://packt.link/qzUvD)。
- en: The mission
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务
- en: Over two billion tons of waste is produced annually globally, and it’s expected
    to grow to over 3.5 billion tons by 2050\. The alarming rise in global waste production
    and the need for effective waste management systems have become increasingly critical
    in recent years. Over half of all household trash in high-income countries is
    recyclable, with 20% in lower-income countries and rising. Currently, most waste
    ends up in landfills or incinerated, contributing to environmental pollution and
    climate change. This is avoidable, considering that, globally, a significant portion
    of all recyclable materials is not recycled.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 全球每年产生超过20亿吨垃圾，预计到2050年将增长到超过35亿吨。近年来，全球垃圾产量急剧上升和有效废物管理系统需求日益迫切。在高收入国家，超过一半的家庭垃圾是可回收的，在低收入国家为20%，并且还在上升。目前，大多数垃圾最终都堆放在垃圾填埋场或焚烧，导致环境污染和气候变化。考虑到全球范围内，很大一部分可回收材料没有得到回收，这是可以避免的。
- en: 'Assuming recyclable waste is collected, it can still be hard and costly to
    sort it. Previously, waste classification technologies included:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 假设可回收垃圾被收集，但仍可能很难且成本高昂地进行分类。以前，废物分类技术包括：
- en: Separating materials by size with rotating cylindrical screens with holes (“trommel
    screens”)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过旋转圆柱形筛网（“摇筛”）按尺寸分离材料
- en: Separating ferrous and non-ferrous metals with magnetic forces and magnetic
    fields (“eddy current separators”)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过磁力和磁场分离铁和非铁金属（“涡流分离器”）
- en: Separating by weight with air
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过空气按重量分离
- en: Separating by density with water (“sink-float separation”)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过水按密度分离（“沉浮分离”）
- en: Manual sorting performed by humans
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由人工执行的手动分类
- en: Implementing all of these techniques effectively can be challenging, even for
    a large, wealthy, urban municipality. To tackle this challenge, **smart recycling
    systems** have emerged, leveraging computer vision and AI to classify waste efficiently
    and accurately.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对于大型、富裕的城市市政府，有效地实施所有这些技术也可能具有挑战性。为了应对这一挑战，**智能回收系统**应运而生，利用计算机视觉和人工智能高效、准确地分类废物。
- en: The development of smart recycling systems can be traced back to the early 2010s
    when researchers and innovators started exploring the potential of computer vision
    and AI to improve waste management processes. They first developed basic image
    recognition algorithms, utilizing features such as color, shape, and texture to
    identify waste materials. These systems were primarily used in research settings
    with limited commercial applications. As machine learning and AI became more advanced,
    smart recycling systems underwent significant improvements. CNNs and other deep
    learning techniques enabled these systems to learn from vast amounts of data and
    improve their waste classification accuracy. Additionally, the integration of
    AI-driven robotics allowed for automated sorting and handling of waste materials,
    increasing efficiency in recycling plants.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 智能回收系统的发展可以追溯到2010年代初，当时研究人员和革新者开始探索计算机视觉和人工智能改善废物管理流程的潜力。他们首先开发了基本的图像识别算法，利用颜色、形状和纹理等特征来识别废物材料。这些系统主要用于研究环境，商业应用有限。随着机器学习和人工智能的进步，智能回收系统经历了显著的改进。卷积神经网络（CNN）和其他深度学习技术使这些系统能够从大量数据中学习并提高其废物分类的准确性。此外，人工智能驱动的机器人集成使得废物材料的自动化分拣和处理成为可能，从而提高了回收工厂的效率。
- en: Costs are significantly lower than a decade ago for cameras, robots, and even
    chips that run deep learning models in low-latency, high-volume scenarios, making
    state-of-the-art smart recycling systems accessible to even smaller and poorer
    municipal waste management departments. One of these municipalities in Brazil
    is looking to revamp their 20-year-old recycling plant made up of a patchwork
    of machines with a collective sorting accuracy of only 70%. Human sorting can
    only partially compensate for the difference, leading to inevitable pollution
    and contamination issues. The Brazilian municipality want to replace the current
    system with a single conveyor belt that sorts waste efficiently from 12 different
    categories into bins with a series of robots.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 摄像头、机器人和用于低延迟、高容量场景运行深度学习模型的芯片等成本与十年前相比显著降低，这使得最先进的智能回收系统对甚至更小、更贫穷的城市废物管理部门也变得可负担。巴西的一个城市正在考虑翻新他们20年前建成的一个由各种机器拼凑而成的回收厂，这些机器的集体分拣准确率仅为70%。人工分拣只能部分弥补这一差距，导致不可避免的污染和污染问题。该巴西市政府希望用一条单条传送带替换当前系统，这条传送带由一系列机器人高效地将12种不同类别的废物分拣到垃圾桶中。
- en: They purchased the conveyor belt, industrial robots, and cameras. Then, they
    paid an AI consultancy company to develop a model to classify the recyclables.
    Still, they wanted models of different sizes because they weren’t sure how quickly
    these would run on the hardware they had.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 他们购买了传送带、工业机器人和摄像头。然后，他们支付了一家人工智能咨询公司开发一个用于分类可回收物的模型。然而，他们想要不同大小的模型，因为他们不确定这些模型在他们的硬件上运行的速度有多快。
- en: 'As requested, the consultancy returned with models of various sizes between
    4 and 64 million parameters. The largest model (b7) is over six times slower than
    the smallest one (b0). Still, the largest model has a significantly higher validation
    F1 score at 96% (F1 val), as opposed to approximately 90% for the smallest one:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如请求，咨询公司带回了4到6400万参数之间各种大小的模型。最大的模型（b7）比最小的模型（b0）慢六倍以上。然而，最大的模型在验证F1分数上显著更高，达到96%（F1
    val），而最小的模型大约为90%：
- en: '![Chart, line chart  Description automatically generated](img/B18406_07_01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成描述](img/B18406_07_01.png)'
- en: 'Figure 7.1: F1 scores for models delivered by the AI consultancy company'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：由人工智能咨询公司提供的模型F1分数
- en: The municipal leadership was delighted with the results but also surprised because
    the consultants asked for no domain knowledge or data to train the models, which
    made them very skeptical. They asked their recycling plant workers to test the
    models with a batch of recyclables. They got a 25% misclassification rate with
    that one batch.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 市政领导对结果感到非常满意，但也感到惊讶，因为顾问们要求不要提供任何领域知识或用于训练模型的数据，这使得他们非常怀疑。他们要求回收厂的工人用一批可回收物测试这些模型。他们用这一批次的模型得到了25%的错误分类率。
- en: To seek a second opinion and an honest evaluation of the model, the municipality
    has approached another AI consultancy firm – yours!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了寻求第二意见和模型的诚实评估，市政厅联系了另一家AI咨询公司——你的公司！
- en: The first order of business was to assemble a test dataset that was more realistic
    of the edge cases that the recycling plant workers found among the misclassifications.
    Your colleague obtained F1 scores with the test dataset between 62% and 66% (F1
    test). Next, they have asked you to understand what’s causing those misclassifications.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项任务是组装一个更符合回收工厂工人在误分类中发现的边缘情况的测试数据集。你的同事使用测试数据集获得了62%到66%的F1分数（F1测试）。接下来，他们要求你理解导致这些误分类的原因。
- en: The approach
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: 'No single interpretation method is perfect, and even the best scenario can
    only tell you one part of the story. Therefore, you have decided to, first, assess
    the model’s predictive performance using traditional interpretation methods, including
    the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 没有一种解释方法完美无缺，即使是最好的情况也只能告诉你故事的一部分。因此，你决定首先使用传统的解释方法来评估模型的预测性能，包括以下方法：
- en: ROC curves and ROC-AUC
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROC曲线和ROC-AUC
- en: Confusion matrices and some metrics derived from them, such as accuracy, precision,
    recall, and F1
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆矩阵以及由此派生的一些指标，如准确率、精确率、召回率和F1
- en: 'Then, you’ll examine the model using an activation-based method:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将使用基于激活的方法检查模型：
- en: Intermediate activation
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中间激活
- en: 'This is followed by evaluating decisions with three gradient-based methods:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这之后是使用三种基于梯度的方法评估决策：
- en: Saliency maps
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显著性图
- en: Grad-CAM
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grad-CAM
- en: Integrated gradients
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成梯度
- en: 'And a backpropagation-based method:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以及一个基于反向传播的方法：
- en: DeepLIFT
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepLIFT
- en: 'This is followed by three perturbation-based methods:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这之后是三种基于扰动的算法：
- en: Occlusion sensitivity
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遮蔽敏感性
- en: Feature ablation
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征消除
- en: Shapley value sampling
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shapley值采样
- en: I hope that you understand why the model is not performing as it should and
    how to fix it by the end of this process. You can also leverage the many plots
    and visualizations you will produce to communicate this story to the municipality’s
    executives.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你在这一过程中理解为什么模型的表现不符合预期，以及如何修复它。你还可以利用你将生成的许多图表和可视化来向市政厅的行政人员传达这个故事。
- en: Preparations
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'You will find most of the code for this example here: [https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/07/GarbageClassifier.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/07/GarbageClassifier.ipynb)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现这个示例的大部分代码都在这里：[https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/07/GarbageClassifier.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/07/GarbageClassifier.ipynb)
- en: Loading the libraries
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载库
- en: 'To run this example, you need to install the following libraries:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行这个示例，你需要安装以下库：
- en: '`torchvision` to load the dataset'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`torchvision`加载数据集
- en: '`mldatasets`, `pandas`, `numpy`, and `sklearn` (scikit-learn) to manipulate
    the dataset'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`mldatasets`、`pandas`、`numpy`和`sklearn`（scikit-learn）来操作数据集
- en: '`torch`, `pytorch-lightning`, `efficientnet-pytorch`, and `torchinfo` to predict
    with the models and show info about the models'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`torch`、`pytorch-lightning`、`efficientnet-pytorch`和`torchinfo`模型进行预测并显示模型信息
- en: '`matplotlib`, `seaborn`, `cv2`, `tqdm`, and `captum` to make and visualize
    the interpretations'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`matplotlib`、`seaborn`、`cv2`、`tqdm`和`captum`来制作和可视化解释
- en: 'You should load all of them first:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先应该加载所有这些库：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we will load and prepare the data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将加载和准备数据。
- en: Understanding and preparing the data
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解和准备数据
- en: The data used to train the model is publicly available at Kaggle ([https://www.kaggle.com/datasets/mostafaabla/garbage-classification](https://www.kaggle.com/datasets/mostafaabla/garbage-classification)).
    It’s called “Garbage Classification” and is a compilation of several different
    online sources, including web scraping. It has already been split into training
    and test datasets and also comes with an additional smaller test dataset taken
    from Wikimedia Commons that your colleague used to test the models. These test
    images come in a slightly higher resolution too.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型所使用的数据在Kaggle上公开可用（[https://www.kaggle.com/datasets/mostafaabla/garbage-classification](https://www.kaggle.com/datasets/mostafaabla/garbage-classification)）。它被称为“垃圾分类”，是几个不同在线资源的汇编，包括网络爬取。它已经被分割成训练集和测试集，还附带了一个额外的较小的测试数据集，这是你的同事用来测试模型的。这些测试图像的分辨率也略高。
- en: 'We download the data from a ZIP file like this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像这样从ZIP文件中下载数据：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It will also extract the ZIP file into four folders corresponding to the three
    datasets and the larger resolution test dataset. Please note that `garbage_dataset_sample`
    has only a fraction of the training and validation datasets. If you want to download
    the full datasets, then use `dataset_file = "garbage_dataset"`. It won’t impact
    the size of the test dataset either way. Next, we can initialize the transformation
    and loading of the datasets like this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 它还会将ZIP文件提取到四个文件夹中，分别对应三个数据集和更高分辨率的测试数据集。请注意，`garbage_dataset_sample`只包含训练和验证数据集的一小部分。如果你想下载完整的数据集，请使用`dataset_file
    = "garbage_dataset"`。无论哪种方式，都不会影响测试数据集的大小。接下来，我们可以这样初始化数据集的转换和加载：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'What the above code does is compose a series of standard transforms such as
    normalization and converting images to tensors. Then, it instantiates PyTorch
    datasets corresponding to each folder – that is, one for the training, validation,
    and test datasets, as well as the larger resolution test dataset (`test_400_data`).
    These datasets also include transforms. That way, each time an image is loaded
    from one of the datasets, it is automatically transformed. We can verify that
    the shapes of the datasets match our expectations with the following code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码所做的就是组合一系列标准转换，如归一化和将图像转换为张量。然后，它实例化与每个文件夹对应的PyTorch数据集——即一个用于训练、验证和测试数据集，以及更高分辨率的测试数据集（`test_400_data`）。这些数据集也包括转换。这样，每次从数据集中加载图像时，它都会自动进行转换。我们可以使用以下代码来验证数据集的形状是否符合我们的预期：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code outputs the number of images in each dataset and the dimensions
    of the images in the datasets. You can tell that there are over 3,700 training
    images, 900 validation images, and 120 test images of 3 x 224 x 224 dimensions.
    The first number corresponds to the channels (red, green, and blue) and the following
    two to the width and height in pixels, which is what the model uses for inference.
    The Test 400 dataset is the same as the Test dataset, the except images have a
    larger height and width. We won’t need the Test 400 dataset for inference, so
    it’s Okay that it doesn’t meet the model’s dimension requirements:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出了每个数据集中的图像数量和图像的维度。你可以看出，有超过3,700张训练图像，900张验证图像和120张测试图像，它们的维度为3 x 224
    x 224。第一个数字对应于通道（红色、绿色和蓝色），接下来的两个数字对应于像素的宽度和高度，这是模型用于推理的。Test 400数据集与Test数据集相同，只是图像的高度和宽度更大。我们不需要Test
    400数据集进行推理，所以它不符合模型的维度要求也是可以的：
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Data preparation
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'If you print`(test_data[0])`, you’ll notice that it will first output a tensor
    with the image and then a single integer, which we call a scalar. This integer
    is a number between 0 and 11, which corresponds to the labels used. For quick
    reference, these are the 12 labels:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打印`(test_data[0])`，你会注意到它首先会输出一个包含图像的张量，然后是一个单独的整数，我们称之为标量。这个整数是一个介于0到11之间的数字，对应于使用的标签。为了快速参考，以下是12个标签：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Interpreting often involves taking single samples and extracting them from
    the dataset to later perform inference with the model. To that end, it’s important
    to get familiar with extracting any image from the dataset, say the very first
    sample from the test dataset:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 解释通常涉及从数据集中提取单个样本，以便稍后使用模型进行推理。为此，熟悉从数据集中提取任何图像，比如测试数据集的第一个样本是很重要的：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![A picture containing diagram  Description automatically generated](img/B18406_07_02.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![包含图表的图片 自动生成描述](img/B18406_07_02.png)'
- en: 'Figure 7.2: A test sample for a recyclable alkaline battery'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：一个可回收碱性电池的测试样本
- en: 'Another preprocessing step we will need to perform is the **One-Hot Encoding**
    (**OHE**) of the `y` labels because we will need the OHE form to evaluate the
    model’s predictive performance. Once we initialize the `OneHotEncoder`, we will
    need to `fit` it to the test labels (`y_test`) in array format. But first, we
    will need to put the test labels into a list (`y_test`). We can do the same with
    the validation labels because these will also be useful for easy evaluation:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要执行的一个预处理步骤是对`y`标签进行**独热编码**（**OHE**），因为我们需要OHE形式来评估模型的预测性能。一旦我们初始化了`OneHotEncoder`，我们需要将其`fit`到测试标签（`y_test`）的数组格式中。但首先，我们需要将测试标签放入一个列表（`y_test`）。我们也可以用同样的方法处理验证标签，因为这些标签也便于评估：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Also, for the sake of reproducibility, always initialize your random seeds
    like this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了确保可重复性，始终这样初始化你的随机种子：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It is acknowledged that determinism is very difficult with deep learning and
    is often session-, platform-, and architecture-dependent. If you are using an
    **NVIDIA GPU**, you can attempt to use PyTorch to avoid nondeterministic algorithms
    with the command `torch.use_deterministic_algorithms(True)`. It’s not a guarantee,
    but it will throw an error when the operation that you are attempting can’t be
    accomplished deterministically. If it succeeds, it will be much slower. It’s only
    worth it if you need to make model outcomes identical – for instance, for scientific
    research or regulatory compliance. For further details about reproducibility and
    PyTorch, look here: [https://pytorch.org/docs/stable/notes/randomness.html](https://pytorch.org/docs/stable/notes/randomness.html).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中确定性的实现非常困难，并且通常依赖于会话、平台和架构。如果你使用**NVIDIA GPU**，你可以尝试使用PyTorch通过命令`torch.use_deterministic_algorithms(True)`来避免非确定性算法。这并不保证，但如果尝试的操作无法以确定性完成，它将引发错误。如果成功，它将运行得慢得多。只有在你需要使模型结果一致时才值得这样做——例如，用于科学研究或合规性。有关可重复性和PyTorch的更多详细信息，请查看此处：[https://pytorch.org/docs/stable/notes/randomness.html](https://pytorch.org/docs/stable/notes/randomness.html)。
- en: Inspect data
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查数据
- en: 'Now, let’s take a peek at what images are in our datasets. We know that the
    training and validation datasets are very similar, so we will start with the validation
    dataset. We can iterate every class in `labels_l` and randomly select a single
    one from the validation dataset with `np.random.choice`. We place each image on
    a 4 × 3 grid with the class label above it:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的数据集中有哪些图像。我们知道训练和验证数据集非常相似，所以我们从验证数据集开始。我们可以迭代`labels_l`中的每个类别，并使用`np.random.choice`从验证数据集中随机选择一个。我们将每个图像放置在一个4×3的网格中，类别标签位于其上方：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding code generates *Figure 7.3*. You can tell that there is significant
    pixelation around the edges of the items; some items appear much darker than others,
    and some of the pictures are from odd angles:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码生成了*图7.3*。你可以看出，物品的边缘存在明显的像素化；有些物品比其他物品暗得多，而且有些图片是从奇怪的角度拍摄的：
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B18406_07_03.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 描述自动生成，置信度中等](img/B18406_07_03.png)'
- en: 'Figure 7.3: A random sample of the validation dataset'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：验证数据集的随机样本
- en: 'Let’s now do the same for the test dataset to compare it to the validation/training
    datasets. We can use the same code as before, except we replace `y_val` with `y_test`,
    and `val_data` with `test_data.` The resulting code generates *Figure 7.4*. You
    can tell that the test set has less pixelated and more consistently lit items,
    mostly from the top- and side-facing angles:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对测试数据集做同样的处理，以便与验证/训练数据集进行比较。我们可以使用之前的相同代码，只需将`y_val`替换为`y_test`，将`val_data`替换为`test_data`。生成的代码生成了*图7.4*。你可以看出，测试集的像素化较少，物品的照明更一致，主要是从正面和侧面角度拍摄的：
- en: '![A picture containing text, different, various  Description automatically
    generated](img/B18406_07_04.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本的图片，不同，各种 描述自动生成](img/B18406_07_04.png)'
- en: 'Figure 7.4: A random sample of the test dataset'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：测试数据集的随机样本
- en: We won’t need to train a CNN in this chapter. Thankfully, it has been provided
    to us by the client.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们不需要训练CNN。幸运的是，客户已经为我们提供了它。
- en: The CNN models
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CNN模型
- en: The models trained by the other consultancy company are fine-tuned EfficientNet
    models. In other words, the AI consultancy company took a previously trained model
    with the EfficientNet architecture and trained it further with the garbage classification
    dataset. This technique is called **transfer learning** because it allows a model
    to utilize previously learned knowledge from a large dataset (in this case, a
    million images from the ImageNet database) and apply it to new tasks with smaller
    datasets. The advantage is it significantly reduces training time and computational
    resources while maintaining high performance because it has already learned to
    extract useful features from images, which can be a valuable starting point for
    a new task and only needs to adapt to the specific task at hand.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其他咨询公司训练的模型是微调后的EfficientNet模型。换句话说，AI咨询公司使用EfficientNet架构的先前训练模型，并使用垃圾分类数据集进一步训练它。这种技术被称为**迁移学习**，因为它允许模型利用从大型数据集（在这种情况下，来自ImageNet数据库的百万张图片）中学习到的先前知识，并将其应用于具有较小数据集的新任务。其优势是显著减少了训练时间和计算资源，同时保持高性能，因为它已经学会了从图像中提取有用的特征，这可以成为新任务的宝贵起点，并且只需要适应手头的特定任务。
- en: It makes sense that they chose EfficientNet. After all, EfficientNet is a family
    of CNNs introduced by Google AI researchers in 2019\. The key innovation of EfficientNet
    is its compound scaling method, which enables the model to achieve higher accuracy
    and efficiency than other CNNs. In addition, it is based on the observation that
    different dimensions of the model, such as width, depth, and resolution, contribute
    to the overall performance in a balanced way. The EfficientNet architecture is
    built upon a baseline model called EfficientNet-B0\. A compound scaling method
    is employed to create larger and more powerful versions of the baseline model,
    which simultaneously scales up the width, depth, and resolution of the network.
    This results in a series of models, EfficientNet-B1 to EfficientNet-B7, with increasing
    capacity and performance. The largest model, EfficientNet-B7, has achieved state-of-the-art
    performance on several benchmarks, such as ImageNet.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 EfficientNet 是有道理的。毕竟，EfficientNet 是由 Google AI 研究人员在 2019 年引入的一组 CNN。EfficientNet
    的关键创新是其复合缩放方法，这使得模型能够比其他 CNN 实现更高的准确性和效率。此外，它基于这样的观察：模型的各个维度，如宽度、深度和分辨率，以平衡的方式对整体性能做出贡献。EfficientNet
    架构建立在称为 EfficientNet-B0 的基线模型之上。采用复合缩放方法创建基线模型更大、更强大的版本，同时提高网络的宽度、深度和分辨率。这产生了一系列模型，从
    EfficientNet-B1 到 EfficientNet-B7，容量和性能逐渐提高。最大的模型 EfficientNet-B7 在多个基准测试中实现了最先进的性能，例如
    ImageNet。
- en: Load the CNN model
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载 CNN 模型
- en: 'Before we can load the model, we must define the class for EfficientLite –
    a class that inherits from PyTorch Lightning’s `pl.LightningModule`. This class
    is designed to create a custom model based on the EfficientNet architecture, train
    it, and perform inference. We only need it for the latter, which is why we have
    also adapted it to include a `predict()` function – much like scikit-learn models
    do for the convenience of being able to use similar evaluation functions to would
    with these models:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够加载模型之前，我们必须定义 EfficientLite 类的类——一个继承自 PyTorch Lightning 的 `pl.LightningModule`
    的类。这个类旨在创建基于 EfficientNet 架构的定制模型，对其进行训练并执行推理。我们只需要它来执行后者，这就是为什么我们还将其修改为包含一个 `predict()`
    函数——类似于 scikit-learn 模型，以便能够使用类似的评估函数：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You will notice that the class has three functions:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到这个类有三个函数：
- en: '`__init__`: This is the constructor for the `EfficientLite` class. It initializes
    the model by loading a pretrained EfficientNet model using the `efficientnet_pytorch.EfficientNet.from_pretrained()`
    method. It then replaces the last fully connected layer (`_fc`) with a new `torch.nn.Linear`
    layer that has the same number of input features but a different number of output
    features equal to the number of classes (`num_class`).'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__init__`: 这是 `EfficientLite` 类的构造函数。它通过使用 `efficientnet_pytorch.EfficientNet.from_pretrained()`
    方法加载预训练的 EfficientNet 模型来初始化模型。然后，它将最后一个全连接层 (`_fc`) 替换为一个新创建的 `torch.nn.Linear`
    层，该层具有相同数量的输入特征，但输出特征的数量不同，等于类别的数量 (`num_class`)。'
- en: '`forward`: This method defines the forward pass of the model. It takes an input
    tensor `x` and passes it through the model, returning the output.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward`: 此方法定义了模型的正向传播。它接收一个输入张量 `x` 并将其通过模型传递，返回输出。'
- en: '`predict`: This method takes a dataset and performs inference using the trained
    model. It first sets the model to evaluation mode (`self.model.eval()`). The input
    dataset is converted into a DataLoader object with a batch size of 32\. The method
    iterates over the DataLoader, processing each batch of data and computing probabilities
    using the softmax function. The `clear_gpu_cache()` function is called after each
    iteration to release unused GPU memory. Finally, the method returns the computed
    probabilities as a `numpy` array.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict`: 此方法接收一个数据集并使用训练好的模型进行推理。它首先将模型设置为评估模式 (`self.model.eval()`)。输入数据集被转换为具有
    32 个批次的 DataLoader 对象。该方法遍历 DataLoader，处理每个数据批次，并使用 softmax 函数计算概率。在每个迭代之后调用 `clear_gpu_cache()`
    函数以释放未使用的 GPU 内存。最后，该方法返回计算出的概率作为 `numpy` 数组。'
- en: 'If you are using a CUDA-enabled GPU, there’s a utility function called `clear_gpu_cache()`,
    which is run every time there’s a GPU-intensive operation. Depending on how powerful
    your GPU is, you may need to run it more often. Feel free to use another convenience
    function, `print_gpu_mem_used()`, to check how much GPU memory is utilized at
    any given moment or to print the entire summary with `print(torch.cuda.memory_summary())`.
    The next code downloads the pre-trained EfficientNet model, loads the model weights
    to EfficientLite, and prepares the model for inference. Lastly, it prints a summary:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用支持CUDA的GPU，有一个名为`clear_gpu_cache()`的实用函数，每次进行GPU密集型操作时都会运行。根据你的GPU性能如何，你可能需要更频繁地运行它。你可以自由地使用另一个便利函数`print_gpu_mem_used()`来检查在任何给定时刻GPU内存的使用情况，或者使用`print(torch.cuda.memory_summary())`来打印整个摘要。接下来的代码下载预训练的EfficientNet模型，将模型权重加载到EfficientLite中，并准备模型进行推理。最后，它打印了一个摘要：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The code is pretty straightforward but what’s important to note is that we
    are choosing the b4 model for this chapter, which is in between b0 and b7 in terms
    of size, speed, and accuracy. You can change the last digit according to your
    hardware’s abilities, but it might change some of the outcomes of this chapter’s
    code. The preceding snippet outputs the following summary:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 代码相当直接，但重要的是要注意，我们在这个章节选择了b4模型，它在大小、速度和准确性方面介于b0和b7之间。你可以根据你的硬件能力更改最后一位数字，但这可能会改变本章代码的一些结果。前面的代码片段输出了以下摘要：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: It has pretty much everything we need to know about the model. It has two custom
    convolutional layers (`Conv2dStaticSamePadding`), each followed by a batch normalization
    layer (`BatchNorm2d`) and 32 `MBConvBlock` modules.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 它几乎包含了我们需要的关于模型的所有信息。它有两个自定义卷积层（`Conv2dStaticSamePadding`），每个卷积层后面跟着一个批归一化层（`BatchNorm2d`）和32个`MBConvBlock`模块。
- en: The network also has a memory-efficient implementation of the Swish activation
    function (`MemoryEfficientSwish`), which, like all activation functions, introduces
    non-linearity into the model. It’s smooth and non-monotonic, which helps it converge
    more quickly while learning more complex and nuanced patterns. It also has a global
    average pooling operation (`AdaptiveAvgPool2d`), which reduces the spatial dimensions
    of the feature maps. It then has a first `Dropout` layer for regularization, followed
    by a fully connected layer (`Linear`) that takes it from 1792 nodes to 12\. Dropout
    prevents overfitting by making a fraction of the neurons inactive in each update
    cycle. If you want to see more details of how the output shape of each layer gets
    reduced between one layer and another, enter the `input_size` into the summary
    – like `summary(garbage_mdl, input_size=(64, 3, 224, 224))` – because the network
    was designed with a batch size of 64 in mind. Don’t worry if none of these terms
    sound familiar to you. We will revisit them later.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 网络还有一个Swish激活函数的内存高效实现（`MemoryEfficientSwish`），就像所有激活函数一样，它将非线性引入模型。它是平滑且非单调的，有助于它更快地收敛，同时学习更复杂和细微的模式。它还有一个全局平均池化操作（`AdaptiveAvgPool2d`），它减少了特征图的空间维度。然后有一个用于正则化的第一个`Dropout`层，后面跟着一个将节点数从1792减少到12的完全连接层（`Linear`）。Dropout通过在每个更新周期中使一部分神经元不活跃来防止过拟合。如果你想知道每个层之间的输出形状是如何减少的，可以将`input_size`输入到摘要中——例如`summary(garbage_mdl,
    input_size=(64, 3, 224, 224))`——因为网络是针对64个批次的尺寸设计的。如果你对这些术语不熟悉，不要担心。我们稍后会重新讨论它们。
- en: Assessing the CNN classifier with traditional interpretation methods
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用传统解释方法评估CNN分类器
- en: 'We will first evaluate the model using the validation dataset with the `evaluate_multiclass_mdl`
    function. The arguments include the model (`garbage_mdl`), our validation data
    (`val`_`data`), as well as the class names (`labels_l`) and the encoder (`ohe`).
    Lastly, we won’t plot the ROC curves (`plot_roc=False`). This function returns
    the predicted labels and probabilities, which we can store in variables for later
    use:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用`evaluate_multiclass_mdl`函数和验证数据集来评估模型。参数包括模型（`garbage_mdl`）、我们的验证数据（`val_data`）、类别名称（`labels_l`）以及编码器（`ohe`）。最后，我们不会绘制ROC曲线（`plot_roc=False`）。此函数返回预测标签和概率，我们可以将它们存储在变量中以供以后使用：
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code generates both *Figure 7.5* with a confusion matrix and
    *Figure 7.6* with performance metrics for each class:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了带有混淆矩阵的*图7.5*和每个类别的性能指标的*图7.6*：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18406_07_05.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面、文本、应用程序、电子邮件  自动生成的描述](img/B18406_07_05.png)'
- en: 'Figure 7.5: The confusion matrix for the validation dataset'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：验证数据集的混淆矩阵
- en: 'Even though the confusion matrix in *Figure 7.5* seems to suggest a perfect
    classification, once you see the precision and recall breakdown in *Figure 7.6*,
    you can tell that the model had issues with metal, plastic, and white glass:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管*图7.5*中的混淆矩阵似乎表明分类完美，但一旦你看到*图7.6*中的精确率和召回率分解，你就可以知道模型在金属、塑料和白色玻璃方面存在问题：
- en: '![A picture containing text, receipt  Description automatically generated](img/B18406_07_06.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本的图片，收据  自动生成的描述](img/B18406_07_06.png)'
- en: 'Figure 7.6: The classification report for the validation dataset'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：验证数据集的分类报告
- en: You can expect a model to always reach `100%` training accuracy if you train
    it for enough epochs using optimal hyperparameters. A near-perfect validation
    accuracy is harder to achieve, depending on how different these two are. We know
    that the validation dataset is simply a sample of images from the same collection,
    so it’s not particularly surprising that 94.7% was achieved.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用最优的超参数对模型进行足够的轮次训练，你可以期望模型总是达到`100%`的训练准确率。接近完美的验证准确率更难实现，这取决于这两个值之间的差异。我们知道验证数据集只是来自同一集合的图像样本，所以达到94.7%并不特别令人惊讶。
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Chart, line chart  Description automatically generated](img/B18406_07_07.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成的描述](img/B18406_07_07.png)'
- en: 'Figure 7.7: The ROC curve for the test dataset'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7：测试数据集的ROC曲线
- en: The test ROC plot (*Figure 7.7*) shows the macro-average and micro-average ROC
    curves. The difference in both of these is in how they are calculated. Macro metrics
    are computed for each class independently and then averaged, treating each differently,
    whereas micro-averages factor in the contribution or representation of each class;
    generally, micro-averages are more reliable.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 测试ROC图(*图7.7*)显示了宏平均和微平均的ROC曲线。这两者的区别在于它们的计算方式。宏度量是独立地对每个类别进行计算然后平均，对待每个类别不同，而微平均则考虑了每个类别的贡献或代表性；一般来说，微平均更可靠。
- en: '![Chart, scatter chart  Description automatically generated](img/B18406_07_08.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成的描述](img/B18406_07_08.png)'
- en: 'Figure 7.8: The confusion matrix for the test dataset'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：测试数据集的混淆矩阵
- en: If we take a look at the confusion matrix in *Figure 7.8*, we can tell that
    only biological, green glass, and shoes are getting 10-out-of-10 classifications.
    However, a lot of items are being misclassified as biologicals and shoes. On the
    other hand, many items are more often than not misclassified, such as metal, paper,
    and plastic. Many of them are similar in shape or color, so you could understand
    how that would happen, but how does a piece of metal get confused with white glass,
    or paper with a battery?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看*图7.8*中的混淆矩阵，我们可以看出只有生物、绿色玻璃和鞋子得到了10/10的分类。然而，很多物品被错误地分类为生物和鞋子。另一方面，很多物品经常被错误分类，比如金属、纸张和塑料。许多物品在形状或颜色上相似，所以你可以理解为什么会这样，但金属怎么会和白色玻璃混淆，或者纸张会和电池混淆呢？
- en: '![Table  Description automatically generated](img/B18406_07_09.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![表格  自动生成的描述](img/B18406_07_09.png)'
- en: 'Figure 7.9: The predictive performance metrics for the test dataset'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9：测试数据集的预测性能指标
- en: 'When classification models are discussed in a business setting, stakeholders
    are often only interested in one number: accuracy. It’s easy to let this drive
    the discussion, but there’s much more nuance to it. For instance, the disappointing
    test accuracy (68.3%) could mean many things. It could mean that six classes are
    getting perfect classification, and all others are not, or that 12 classes are
    getting only half misclassified. There are many possibilities of what could be
    going on.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在商业环境中讨论分类模型时，利益相关者通常只对一个数字感兴趣：准确率。很容易让这个数字驱动讨论，但其中有很多细微差别。例如，令人失望的测试准确率（68.3%）可能意味着很多事情。这可能意味着六个类别得到了完美的分类，而其他所有类别都没有，或者12个类别只有一半被错误分类。可能发生的事情有很多。
- en: In any case, when dealing with a multiclass classification problem, even an
    accuracy below 50% might not be as bad as it seems. Consider that the **no information
    rate** represents the accuracy that can be achieved by a naive model that always
    predicts the most frequent class in the dataset. It serves as a benchmark to ensure
    that the developed model is providing insights beyond this simplistic approach.
    And with 12 evenly split, the **no information rate** is likely to be around 8.33%
    (100%/12 classes), so 68% is still orders of magnitude higher than that. In fact,
    there is less of a leap to 100%! To a machine learning practitioner, this means
    that if we judge solely based on test accuracy results, the model is still learning
    something of value that can be improved upon.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，处理多类分类问题时，即使准确率低于50%也可能不像看起来那么糟糕。考虑到**无信息率**代表了在数据集中总是预测最频繁类别的朴素模型所能达到的准确率。它作为一个基准，确保开发出的模型提供了超越这种简单方法的见解。并且，如果数据集被平均分成12类，那么**无信息率**可能大约是8.33%（100%/12类），所以68%仍然比这高得多。实际上，距离100%的差距还要小！对于一个机器学习从业者来说，这意味着如果我们仅仅根据测试准确率结果来判断，模型仍在学习一些有价值的东西，这些是可以进一步改进的。
- en: In any case, the predictive performance metrics in *Figure 7.9* for the test
    dataset are consistent with what we saw in the confusion matrix. Biological gets
    high recall but low precision and metal, paper, plastic, and trash are low for
    both.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，测试数据集在*图7.9*中的预测性能指标与我们在混淆矩阵中看到的一致。生物类别召回率高但精确率低，而金属、纸张、塑料和垃圾的召回率都很低。
- en: Determining what misclassifications to focus on
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确定要关注的错误分类
- en: 'We have already noticed some exciting misclassifications we can focus on:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经注意到一些有趣的错误分类，我们可以集中关注：
- en: '**Metal false positives**: 16 out of the 120 samples in the test dataset were
    misclassified as metal. That’s 42% of all misclassifications! What is it about
    metal that renders it so easily confused with other garbage according to the model?'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金属的假阳性**：测试数据集中有120个样本中的16个被错误分类为金属。这是所有错误分类的42%！模型为什么如此容易将金属与其他垃圾混淆，这是怎么回事？'
- en: '**Plastic false negatives**: 70% of all true plastic samples were misclassified.
    Thus, plastics had the lowest recall of any material besides trash. It’s easy
    to tell why trash would be so difficult to classify because it’s exceedingly diverse
    but not plastic.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**塑料的假阴性**：70%的所有真实塑料样本都被错误分类。因此，塑料在所有材料中除了垃圾之外，召回率最低。很容易理解为什么垃圾分类如此困难，因为它极其多样，但不是塑料。'
- en: We should also examine some true positives to contrast these misclassifications.
    Namely, batteries because they get many false positives as metals and plastic,
    and white glass because it gets false negatives as metals 30% of the time. Since
    there are so many metal false positives, we should narrow them down to just those
    that are for batteries.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该检查一些真实阳性，以对比这些错误分类。特别是电池，因为它们作为金属和塑料有很多假阳性，以及白色玻璃，因为它30%的时间作为金属有假阴性。由于金属的假阳性很多，我们应该将它们缩小到仅仅是电池的。
- en: 'To visualize the tasks ahead, we can create a DataFrame (`preds_df`) with the
    true labels (`y_true`) in one column and predicted labels in another (`y_pred`).
    And to understand how certain the models are of these predictions, we can create
    another DataFrame with the probabilities (`probs_df`). We can generate column
    totals for these probabilities to sort the columns according to which category
    the model is most certain about across all samples. Then, we can concatenate our
    predictions DataFrame with the first 12 columns from our probabilities DataFrame:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化前面的任务，我们可以创建一个DataFrame（`preds_df`），其中包含一个列的真实标签（`y_true`）和另一个列的预测标签。为了了解模型对这些预测的确定性，我们可以创建另一个包含概率的DataFrame（`probs_df`）。我们可以为这些概率生成列总计，以便根据模型在所有样本中最确定哪个类别来排序列。然后，我们可以将我们的预测DataFrame与概率DataFrame的前12列连接起来：
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s now output the DataFrame with color coding for the prediction instances
    we are interested in assessing. On one hand, we have the metal false positives
    and, on the other, the plastic false negatives. But we also have the true positives
    for battery and white glass. Lastly, we have bolded all probabilities over 50%
    and hidden all probabilities of 0% so that it’s easier to spot any predictions
    with high probabilities:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们输出感兴趣的预测实例的DataFrame，并对其进行颜色编码。一方面，我们有金属的假阳性，另一方面，我们有塑料的假阴性。但我们还有电池和白色玻璃的真实阳性。最后，我们将所有超过50%的概率加粗，并将所有0%的概率隐藏起来，这样更容易发现任何高概率的预测：
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Table  Description automatically generated](img/B18406_07_10.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18406_07_10.png)'
- en: 'Figure 7.10: Table with all 38 misclassifications in the test dataset, selected
    true positives, and their true and predicted labels, as well as their predicted
    probabilities'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10：测试数据集中所有38个错误分类、选定的真实正例及其真实和预测标签，以及它们的预测概率的表格
- en: 'We can easily store the indexes for these instances in lists with the following
    code. That way, for future reference, we can iterate through these lists to assess
    individual predictions or subset arrays with them to perform interpretation tasks
    for the entire group. As you can tell, we have lists for all four groups:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码轻松地将这些实例的索引存储在列表中。这样，为了未来的参考，我们可以遍历这些列表来评估单个预测，或者用它们来对整个组执行解释任务。正如你所看到的，我们有所有四个组的列表：
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now that we have all our data preprocessed, the model is fully loaded and lists
    the groups of predictions to debug. Now we can move forward. Let the interpretation
    begin!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经预处理了所有数据，模型已完全加载并列出要调试的预测组。现在我们可以继续前进。让我们开始解释！
- en: Visualizing the learning process with activation-based methods
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于激活的方法可视化学习过程
- en: Before we get into discussing activations, layers, filters, neurons, gradients,
    convolutions, kernels, and all the fantastic elements that make up a CNN, let’s
    first briefly revisit the mechanics of a CNN and one in particular.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始讨论激活、层、过滤器、神经元、梯度、卷积、核以及构成卷积神经网络（CNN）的所有神奇元素之前，让我们首先简要回顾一下CNN的机制，特别是其中一个机制。
- en: The convolution layer is the essential building block of a CNN, which is a sequential
    neural network. It convolves the input with **learnable filters**, which are relatively
    small but are applied across the entire width, height, and depth at specific distances
    or **strides**. Each filter produces a two-dimensional **activation map** (also
    known as a **feature map**). It’s called an activation map because it denotes
    positions of activations in the images – in other words, where specific “features”
    are located. In this context, a feature is an abstract spatial representation
    that, downstream in the process, is reflected in the learned weights of fully
    connected (**linear**) layers. For instance, in the garbage CNN case, the first
    convolutional layer has 48 filters with a 3 × 3 kernel, a 2 × 2 stride, and static
    padding, which ensure that the output maps maintain the same size as the inputs.
    Filters are template matching because they end up activating areas of the activation
    map when certain patterns are found in the input image.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层是CNN的基本构建块，它是一个顺序神经网络。它通过**可学习的过滤器**对输入进行卷积，这些过滤器相对较小，但会在特定的距离或**步长**上应用于整个宽度、高度和深度。每个过滤器产生一个二维的**激活图**（也称为**特征图**）。之所以称为激活图，是因为它表示图像中激活的位置——换句话说，特定“特征”所在的位置。在这个上下文中，特征是一个抽象的空间表示，在处理过程的下游，它反映在完全连接（**线性**）层的所学权重中。例如，在垃圾CNN案例中，第一个卷积层有48个过滤器，3
    × 3的核，2 × 2的步长和静态填充，这确保输出图保持与输入相同的大小。过滤器是模板匹配的，因为当在输入图像中找到某些模式时，它们最终会在激活图中激活区域。
- en: But before we get to our fully connected layers, we have to reduce the dimensions
    of our filters until they have a workable size. For instance, if we flattened
    the output of our first convolution (48 × 112 × 112), we would have over 602,000
    features. I think we can all agree that that would be too much to feed into a
    fully connected layer. Even if we used enough neurons to handle this workload,
    we probably wouldn’t have captured enough spatial representations for the neural
    network to make sense of the images. For this reason, convolutional layers are
    often paired with pooling layers, which downsample the input – in other words,
    they reduce the dimensionality of the data. In this case, there’s an adaptive
    average pooling layer (`AdaptiveAvgPool2d`) that performs an average across all
    the channels as well as many pooling layers within the **Mobile Inverted Bottleneck
    Convolution Blocks** (`MBConvBlock`).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我们到达完全连接层之前，我们必须减小过滤器的尺寸，直到它们达到可工作的尺寸。例如，如果我们展平第一个卷积的输出（48 × 112 × 112），我们就会有超过602,000个特征。我想我们都可以同意，这会太多以至于无法输入到完全连接层中。即使我们使用了足够的神经元来处理这项工作负载，我们可能也没有捕捉到足够的空间表示，以便神经网络能够理解图像。因此，卷积层通常与池化层配对，池化层对输入进行下采样——换句话说，它们减少了数据的维度。在这种情况下，有一个自适应平均池化层（`AdaptiveAvgPool2d`），它在所有通道上执行平均，以及许多在**Mobile
    Inverted Bottleneck Convolution Blocks**（`MBConvBlock`）内的池化层。
- en: 'Incidentally, `MBConvBlock`, `Conv2dStaticSamePadding`, and `BatchNorm2d` are
    the building blocks of the EfficientNet architecture. These components work together
    to create a highly efficient and accurate convolutional neural network:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，`MBConvBlock`、`Conv2dStaticSamePadding`和`BatchNorm2d`是EfficientNet架构的构建块。这些组件共同工作，创建了一个高度高效且准确的卷积神经网络：
- en: '`MBConvBlock`: Mobile inverted bottleneck convolution blocks that form the
    core of the EfficientNet architecture. In traditional convolutional layers, filters
    are applied across all input channels simultaneously, resulting in a high number
    of computations, but `MBConvBlocks` divide the process into two steps: first,
    they apply depthwise convolutions that handle each input channel separately, and
    then use pointwise (1 x 1) convolutions to combine the information from different
    channels. For this reason, inside the `MBConvBlock` modules for B0, there are
    three convolutional layers: a depthwise convolution, a pointwise (1 x 1) convolution
    (called project convolution), and another pointwise (1 x 1) convolution (called
    expand convolution) in some blocks. However, the first block only contains two
    convolutional layers (depthwise and project convolutions) because it doesn’t have
    an expand convolution. For B4, the architecture is similar except more convolutions
    are stacked in each block and there are twice as many `MBConvBlocks`. Naturally,
    B7 has many more blocks and convolutional layers. For B4, there are a total of
    158 convolutional operations between the 32 `MBConvBlocks`.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MBConvBlock`: 形成EfficientNet架构核心的移动倒置瓶颈卷积块。在传统的卷积层中，过滤器同时应用于所有输入通道，导致计算量很大，但`MBConvBlocks`将这个过程分为两个步骤：首先，它们应用深度卷积，分别处理每个输入通道，然后使用点卷积（1
    x 1）来结合来自不同通道的信息。因此，在B0的`MBConvBlock`模块中，有三个卷积层：一个深度卷积，一个点卷积（称为项目卷积），以及在某些块中的另一个点卷积（称为扩展卷积）。然而，第一个块只包含两个卷积层（深度卷积和项目卷积），因为它没有扩展卷积。对于B4，架构类似，但每个块中堆叠的卷积更多，`MBConvBlocks`的数量也翻倍。自然地，B7有更多的块和卷积层。对于B4，总共有158次卷积操作分布在32个`MBConvBlocks`之间。'
- en: '`Conv2dStaticSamePadding`: Unlike traditional convolutional layers (such as
    `Conv2d`), these don’t reduce the dimensions. It ensures the input and output
    feature maps have the same spatial dimensions.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Conv2dStaticSamePadding`: 与传统的卷积层（如`Conv2d`）不同，这些层不会减少维度。它确保输入和输出特征图具有相同的空间维度。'
- en: '`BatchNorm2d`: Batch normalization layers that help stabilize and accelerate
    training by normalizing the input features, which helps keep the distribution
    of the input features consistent during training.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BatchNorm2d`: 批标准化层，通过归一化输入特征来帮助稳定和加速训练，这有助于在训练过程中保持输入特征的分布一致性。'
- en: 'Once the over 230 convolutional and pooling operations are performed, we are
    left with a flattened output of a more workable size: 1,792 features, which the
    fully connected layer converts into 12, which, leveraging **softmax** activation,
    outputs probabilities between 0 and 1 for each of the classes. In the garbage
    CNN, there is a **dropout** layer involved to help regularize the training. We
    can ignore this entirely because, for inference, they are ignored.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦执行了超过230次的卷积和池化操作，我们得到一个更易于处理的扁平化输出：1,792个特征，全连接层将这些特征转换为12个，利用**softmax**激活函数，为每个类别输出介于0和1之间的概率。在垃圾CNN中，有一个**dropout**层用于帮助正则化训练。我们可以完全忽略这一点，因为在推理过程中，它们是被忽略的。
- en: If this wasn’t entirely clear, don’t fret! The sections that follow will demonstrate
    visually through activations, gradients, and perturbations how the network probably
    learned or did not learn image representations.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这还不够清晰，不要担心！接下来的部分将通过激活、梯度和扰动直观地展示网络可能学习或未学习的图像表示方式。
- en: Intermediate activations
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 中间激活
- en: For inference, the image goes through the network’s input and the prediction
    comes out through the output traversing every single layer. However, one of the
    advantages of having a sequential and layered architecture is that we can extract
    any layer’s output and not just the final layer. The **intermediate activations**
    are simply the outputs of any of the convolution or pooling layers. They are activation
    maps because, after an activation function has been applied, the brighter spots
    map to the image’s features. In this case, the model used ReLU on all convolutional
    layers, so that is what activates the spots. We are only interested in the convolutional
    layers’ intermediate activations because the pooling layers are simply downsampled
    versions of these ones. Why not see the higher-resolution version instead?
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推理，图像通过网络的输入，预测通过输出穿过每个层。然而，具有顺序和分层架构的一个优点是我们可以提取任何层的输出，而不仅仅是最终层。**中间激活**是任何卷积或池化层的输出。它们是激活图，因为激活函数应用后，亮度较高的点映射到图像的特征。在这种情况下，模型在所有卷积层上使用了ReLU，这就是激活点的原理。我们只对卷积层的中间激活感兴趣，因为池化层只是这些层的下采样版本。为什么不去看更高分辨率的版本呢？
- en: As the filters become smaller in width and height, the learned representations
    will be larger. In other words, the first convolutional layer may be about details
    such as texture, the following one about edges, and the last one about shapes.
    We must then flatten the convolutional layers’ output to feed it to the multilayer
    perceptron that takes over from then on.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 随着滤波器宽度和高度的减小，学习到的表示将会更大。换句话说，第一个卷积层可能关于细节，如纹理，下一个关于边缘，最后一个关于形状。然后我们必须将卷积层的输出展平，以便将其输入到从那时起接管的多层感知器。
- en: 'What we will do now is extract activations for some of the convolutional layers.
    In B4, there are 158, so we can’t do all of them! To this end, we will obtain
    the first level of layers with `model.children()`, and iterate across them. We
    will append the two `Conv2dStaticSamePadding` layers from this top level into
    a `conv_layers` list. But we will also go deeper, appending the first convolutional
    layer for the first six `MBConvBlock` layers in the `ModuleList` layers. In the
    end, we should have eight convolutional layers – the six in the middle belonging
    to Mobile Inverted Bottleneck Convolution blocks:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在要做的是提取一些卷积层的激活。在B4中，有158个，所以我们不能全部做！为此，我们将使用`model.children()`获取第一层的层，并遍历它们。我们将从这个顶层将两个`Conv2dStaticSamePadding`层添加到`conv_layers`列表中。但我们会更深入，将`ModuleList`层中的前六个`MBConvBlock`层的第一个卷积层也添加进去。最后，我们应该有八个卷积层——中间的六个属于Mobile
    Inverted Bottleneck Convolution块：
- en: '[PRE19]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Before we iterate across all of them producing activation maps for each convolutional
    layer, let’s do it for a single filter and layer:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们遍历所有它们，为每个卷积层生成激活图之前，让我们为单个滤波器和层做一下：
- en: '[PRE20]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let’s visualize the first filter, but before we do that, we must decide what
    colormap to use. A colormap will determine what colors to assign to different
    numbers as a gradient. For instance, the following colormap has white for `0`
    (`#ffffff` in hexadecimal), a medium gray for `0.25`, and black (`#000000` in
    hexadecimal) for `1` with a gradient between these colors:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化第一个滤波器，但在我们这样做之前，我们必须决定使用什么颜色图。颜色图将决定将不同数字分配给哪些颜色作为渐变。例如，以下颜色图将白色分配给`0`（十六进制中的`#ffffff`），中等灰色分配给`0.25`，黑色（十六进制中的`#000000`）分配给`1`，这些颜色之间有一个渐变：
- en: '[PRE21]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You can also use any of the named colormaps from [https://matplotlib.org/stable/tutorials/colors/colormaps.html](https://matplotlib.org/stable/tutorials/colors/colormaps.html),
    rather than using your own. Next, let’s plot the attribution for the first filter
    like this:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用[https://matplotlib.org/stable/tutorials/colors/colormaps.html](https://matplotlib.org/stable/tutorials/colors/colormaps.html)上的任何命名颜色图，而不是使用你自己的。接下来，让我们像这样绘制第一个滤波器的属性图：
- en: '[PRE22]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![Diagram  Description automatically generated with low confidence](img/B18406_07_11.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成，置信度低](img/B18406_07_11.png)'
- en: 'Figure 7.11: Intermediate activation map for the first filter for the first
    convolutional layer for the first true positive battery sample'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11：第一个真实正样本电池样本的第一个卷积层的第一个滤波器的中间激活图
- en: As you can tell in *Figure 7.11*, it seems like the intermediate activations
    for the first filter are finding the edges of the battery and the most prominent
    text.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在*图7.11*中可以看到，第一个滤波器的中间激活似乎在寻找电池的边缘和最突出的文本。
- en: 'Next, we will iterate across all computational layers and every battery and
    visualize attributions for each one. Now, some of these attribution operations
    can be computationally expensive, so it’s important to clear the GPU cache (`clear_gpu_cache()`)
    in between them:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将遍历所有计算层和每个电池，并可视化每个的归因。现在，一些这些归因操作可能计算成本很高，因此在这些操作之间清除GPU缓存（`clear_gpu_cache()`）是很重要的：
- en: '[PRE23]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As you can tell from *Figure 7.12*, the first convolutional layer seems to
    be picking up on the battery’s letters as well as its contours:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从*图7.12*中可以看出，第一层卷积似乎在捕捉电池的字母以及其轮廓：
- en: '![Graphical user interface  Description automatically generated with low confidence](img/B18406_07_12.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面  自动生成的描述，置信度低](img/B18406_07_12.png)'
- en: 'Figure 7.12: Intermediate activations for the first convolutional layer for
    battery #4'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12：电池#4的第一卷积层的中间激活
- en: 'However, *Figure 7.13* shows how, by the fourth convolutional layer, the network
    understands a battery’s contours better:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，*图7.13*显示了网络如何通过第四层卷积更好地理解电池的轮廓：
- en: '![Graphical user interface  Description automatically generated](img/B18406_07_13.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面  自动生成的描述，置信度低](img/B18406_07_13.png)'
- en: 'Figure 7.13: Intermediate activations for the fourth convolutional layer for
    battery #4'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13：电池#4的第四卷积层的中间激活
- en: 'The last convolutional layer in *Figure 7.14* is impossible to interpret because
    there are 1,792 filters that are 7 pixels wide and high, but rest assured, there
    are some very high-level features encoded in those tiny maps:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.14*中，最后一层卷积层难以解释，因为这里有1,792个7像素宽和高的过滤器，但请放心，那些微小的图中编码了一些非常高级的特征：
- en: '![A picture containing text  Description automatically generated](img/B18406_07_14.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本的图片  自动生成的描述](img/B18406_07_14.png)'
- en: 'Figure 7.14: Intermediate activations for the last convolutional layer for
    battery #4'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14：电池#4的最后一层卷积层的中间激活
- en: 'Extracting intermediate activations can provide you with some insight on a
    sample-by-sample basis. In other words, it’s a **local model interpretation method**.
    It’s by no means the only layerwise-attribution method. Captum has more than ten
    layer attribution methods: [https://github.com/pytorch/captum#about-captum](https://github.com/pytorch/captum#about-captum).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 提取中间激活可以为你提供基于样本的某些洞察。换句话说，它是一种**局部模型解释方法**。这绝对不是唯一的逐层归因方法。Captum 有超过十种层归因方法：[https://github.com/pytorch/captum#about-captum](https://github.com/pytorch/captum#about-captum)。
- en: Evaluating misclassifications with gradient-based attribution methods
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于梯度的归因方法评估误分类
- en: '**Gradient-based methods** calculate **attribution maps** for each classification
    with both forward and background passes through the CNN. As the name suggests,
    these methods leverage the gradients in the backward pass to compute the attribution
    maps. All of these methods are local interpretation methods because they only
    derive a single interpretation per sample. Incidentally, attributions in this
    context mean that we are attributing the predicted labels to areas of an image.
    They are often called **sensitivity maps** in academic literature, too.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于梯度的方法**通过CNN的前向和反向传递计算每个分类的**归因图**。正如其名所示，这些方法利用反向传递中的梯度来计算归因图。所有这些方法都是局部解释方法，因为它们只为每个样本推导出一个解释。顺便提一下，在这个上下文中，归因意味着我们将预测标签归因于图像的某些区域。在学术文献中，它们也常被称为**敏感性图**。'
- en: 'To get started, we will first need to create an array with all of our misclassification
    samples (`X_misclass`) from the test dataset (`test_data`) using the combined
    indexes for all of our misclassifications of interest (`misclass_idxs`). Since
    there aren’t that many misclassifications, we are loading a single batch of them
    (`next`):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，我们首先需要创建一个数组，包含测试数据集（`test_data`）中所有我们的误分类样本（`X_misclass`），使用所有我们感兴趣的误分类的合并索引（`misclass_idxs`）。由于误分类并不多，我们正在加载它们的一个批次（`next`）：
- en: '[PRE24]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The next step is to create a utility function we can reuse to obtain the attribution
    maps for any method. Optionally, we can smooth the map with a method called `NoiseTunnel`
    ([https://github.com/pytorch/captum#getting-started](https://github.com/pytorch/captum#getting-started)).
    We will cover this method in more detail later:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一个我们可以重用的实用函数来获取任何方法的归因图。可选地，我们可以使用名为`NoiseTunnel`的方法（[https://github.com/pytorch/captum#getting-started](https://github.com/pytorch/captum#getting-started)）来平滑地图。我们将在稍后更详细地介绍这种方法：
- en: '[PRE25]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The preceding code can create attribution maps for any Captum method for a given
    model and device. To that end, it takes tensors for the images, `X`, and their
    corresponding labels, `y`. The labels are optional and only needed if the attribution
    method is targeted – most methods are. Most attribution methods (`attr_method`)
    are initialized with only the model, but some require some additional arguments
    (`init_args`). Where they tend to have the most arguments is when the attribution
    is generated with the `attribute` function, which is why we have the `**kwargs`
    collect additional arguments in the `get_attribution_maps` function and place
    them in this call.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码可以为给定模型和设备的任何Captum方法创建归因图。为此，它需要图像的张量`X`及其相应的标签`y`。标签是可选的，只有在归因方法是针对特定目标时才需要
    - 大多数方法都是。大多数归因方法（`attr_method`）仅使用模型初始化，但一些需要一些额外的参数（`init_args`）。它们通常在用`attribute`函数生成归因时具有最多的参数，这就是为什么我们在`get_attribution_maps`函数中收集额外的参数（`**kwargs`），并将它们放在这个调用中。
- en: One important thing to note is that, in this function, we iterate across all
    the samples in the `X` tensor and create the attribute maps for each one independently.
    This is often unnecessary because the attribute methods are all equipped to process
    a batch at once. However, there’s a risk the hardware can’t handle an entire batch,
    and at the time of this writing, very few methods come with an `internal_batch_size`
    argument, which can limit how many samples are processed at a time. What we are
    doing here is essentially equivalent to setting this number to `1` every single
    time in an effort to ensure that we don’t run into memory issues. However, if
    you have powerful hardware, you can rewrite the function to process the `X` and
    `y` tensors directly.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一个重要事项是，在这个函数中，我们遍历`X`张量中的所有样本，并为每个样本独立创建属性图。这通常是不必要的，因为属性方法都配备了同时处理一批数据的能力。然而，存在硬件无法处理整个批次的风险，在撰写本文时，非常少的方法带有`internal_batch_size`参数，这可能会限制一次可以处理的样本数量。我们在这里所做的是本质上等同于每次都将这个数字设置为`1`，以努力确保我们不会遇到内存问题。然而，如果你有强大的硬件，你可以重写函数以直接处理`X`和`y`张量。
- en: Next, we will perform our first gradient-based attribution method.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将执行我们的第一个基于梯度的归因方法。
- en: Saliency maps
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 显著性图
- en: '**Saliency maps** rely on the absolute value of gradients. The intuition is
    that it will find the pixels in the image that can be perturbed the least so that
    the output changes the most with these values. It doesn’t perform perturbations,
    so it doesn’t validate the hypothesis, and the use of absolute values prevents
    it from finding other evidence to the contrary.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**显著性图**依赖于梯度的绝对值。直觉上，它会找到图像中可以扰动最少且输出变化最大的像素。它不执行扰动，因此不验证假设，而绝对值的使用阻止它找到相反的证据。'
- en: This first saliency map method was groundbreaking at the time and has inspired
    a bunch of different methods. It’s typically nicknamed “vanilla” to distinguish
    it from other saliency maps.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这种首次提出的显著性图方法在当时具有开创性，并激发了许多不同的方法。它通常被昵称为“vanilla”，以区别于其他显著性图。
- en: 'Generating saliency maps for all of our misclassified samples is relatively
    simple with our `get_attribution_maps` function. All you need is the Captum attribution
    method (`attr.Saliency`), model (`garbage_mdl`), device, and the tensors for the
    misclassified samples (`X_misclass` and `y_misclass`):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的`get_attribution_maps`函数为所有误分类的样本生成显著性图相对简单。你所需要的是Captum归因方法（`attr.Saliency`）、模型（`garbage_mdl`）、设备以及误分类样本的张量（`X_misclass`和`y_misclass`）：
- en: '[PRE26]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can plot the output of one of these saliency maps, the fifth one, side by
    side with the sample image to provide context. Matplotlib can do this easily with
    a `subplots` grid. We will make a 1 × 3 grid and place the sample image in the
    first spot, its saliency heatmap in the second, and one overlayed over the other
    in the third. As we have done with previous attribution maps, we can use `tensor_to_img`
    to convert the images to `numpy` arrays while also applying a colormap to the
    attribution. It uses the jet colormap (`cmap=''jet''`) by default to make the
    salient areas appear more striking:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以绘制其中一个显著性图的输出，第五个，与样本图像并排显示以提供上下文。Matplotlib可以通过`subplots`网格轻松完成此操作。我们将创建一个1
    × 3的网格，并将样本图像放在第一个位置，其显著性热图放在第二个位置，第三个位置是叠加在一起的。就像我们之前对归因图所做的那样，我们可以使用`tensor_to_img`将图像转换为`numpy`数组，同时应用归因的调色板。它默认使用jet调色板（`cmap='jet'`）使显著的区域看起来更加突出：
- en: '[PRE27]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The preceding code generates the plot in *Figure 7.15*:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了*图7.15*中的图表：
- en: '![Chart  Description automatically generated](img/B18406_07_15.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B18406_07_15.png)'
- en: 'Figure 7.15: Saliency maps for plastic misclassified as biological waste'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15：将塑料误分类为生物废物的显著性图
- en: The sample image in *Figure 7.15* appears to be shredded plastic, but the prediction
    is for biological waste. The vanilla saliency map attributes that prediction mostly
    to the smoother duller areas of the plastic. It appears that the lack of specular
    highlights has thrown the model off, but typically, older broken pieces of plastic
    lose their shine.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.15*中的样本图像看起来像是被撕碎的塑料，但预测结果是生物废物。标准的显著性图将这个预测主要归因于塑料上较平滑、较暗的区域。看起来是缺乏镜面高光让模型产生了偏差，但通常，较旧的破损塑料会失去光泽。'
- en: Specular highlights are bright spots of light that appear on the surface of
    an object when it reflects light. They are often the direct reflections of a light
    source and are more pronounced on shiny or glossy surfaces, such as metal, glass,
    or water.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 镜面高光是在物体表面反射光线时出现的明亮光点。它们通常是光源的直接反射，并且在光滑或光亮的表面上更为明显，例如金属、玻璃或水。
- en: Guided Grad-CAM
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引导Grad-CAM
- en: 'To discuss **guided Grad-CAM**, we first ought to discuss **CAM**, which stands
    for **Class Activation Map**. The way CAM works is that it removes all but the
    last fully connected layers, and it replaces the last **MaxPooling** layer with
    a **Global Average Pooling** (**GAP**) layer. A GAP layer calculates the average
    value of each feature map, reducing it to a single value per map, while a MaxPooling
    layer downsizes feature maps by selecting the maximum value from a set of values
    in a local region of the map. For instance, in this case:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 要讨论**引导Grad-CAM**，我们首先应该讨论**CAM**，它代表**类别激活图**。CAM的工作方式是移除除了最后一层全连接层之外的所有层，并用**全局平均池化**（GAP）层替换最后一个**最大池化**层。GAP层计算每个特征图的平均值，将其减少到每个图的单个值，而最大池化层通过从图的一个局部区域中的值集中选择最大值来减小特征图的大小。例如，在这个案例中：
- en: The last convolutional layer outputs a tensor that is `1792` × `7` × `7`.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一个卷积层输出一个`1792` × `7` × `7`的张量。
- en: GAP reduces dimensions by merely averaging the last two dimensions of this tensor,
    producing a `1792` × `1` × `1` tensor.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GAP通过仅平均这个张量的最后两个维度来减少维度，产生一个`1792` × `1` × `1`的张量。
- en: It then feeds this to a fully connected layer with 12 neurons corresponding
    to each class.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它将这个结果输入到一个有12个神经元的全连接层中，每个神经元对应一个类别。
- en: Once you retrain a CAM model and pass a sample image through the CAM model,
    it takes the weights from the last layer (a `1792` × `12` tensor) and extracts
    the values corresponding to the predicted class (a `1792` × `1` tensor).
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦重新训练了一个CAM模型并通过样本图像通过CAM模型，它将从最后一层（一个`1792` × `12`的张量）中提取与预测类别相对应的值（一个`1792`
    × `1`的张量）。
- en: Then, you calculate the dot product of the last convolutional layer’s output
    (`1792` × `7` × `7`) with the weight tensor (`1792` x `1`).
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你计算最后一个卷积层输出（`1792` × `7` × `7`）与权重张量（`1792` x `1`）的点积。
- en: This weighted sum will end with a `1` × `7` × `7` tensor.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个加权的总和将结束于一个`1` × `7` × `7`的张量。
- en: With bilinear interpolation to stretch it out to `1` × `224` × `224`, this becomes
    an upsampled class activation map. When you upsample data, you increase its dimensions.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过双线性插值将其拉伸到`1` × `224` × `224`，这变成了一个上采样后的激活图。当你上采样数据时，你增加了其维度。
- en: The intuition behind CAM is that CNNs inherently retain spatial details in convolutional
    layers but they are, sadly, lost in fully connected layers. In fact, each filter
    in the last convolutional layer represents visual patterns at different spatial
    locations. Once weighted, they represent the most salient regions in the entire
    image. However, to apply CAM, you must radically modify a model and retrain it,
    and some models don’t lend themselves easily to this.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: CAM背后的直觉是，CNN在卷积层中本质上保留了空间细节，但遗憾的是，这些细节在全连接层中丢失了。实际上，最后一个卷积层中的每个滤波器代表不同空间位置上的视觉模式。一旦加权，它们就代表了整个图像中最显著的区域。然而，要应用CAM，你必须彻底修改模型并重新训练它，而且有些模型并不容易适应这种修改。
- en: As the name suggests, Grad-CAM is a similar concept but lacks the modifying
    and retraining hassle, and uses gradients instead – specifically, those of the
    class score (prior to softmax) concerning the convolutional layer’s activation
    maps. GAP is performed on these gradients to obtain **neuron importance weights**.
    Then, we compute a weighted linear combination of activation maps with these weights,
    followed by a ReLU. The ReLU is very important because it ensures locating features
    that only positively influence the outcome. Like CAM, it is upsampled with bilinear
    interpolation to match the dimensions of the image.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称所示，Grad-CAM是一个类似的概念，但避免了修改和重新训练的麻烦，并使用梯度代替——具体来说，是关于卷积层激活图的类别分数（在softmax之前）的梯度。对这些梯度执行GAP操作以获得**神经元重要性权重**。然后，我们使用这些权重计算激活图的加权线性组合，随后是ReLU。ReLU非常重要，因为它确保定位只对结果产生正面影响的特征。像CAM一样，它通过双线性插值上采样以匹配图像的尺寸。
- en: Grad-CAM does have some shortcomings too, such as failing to identify multiple
    occurrences or the entirety of the object represented by the predicted class.
    Like CAM, the resolution of the activation maps may be limited by the final convolutional
    layer’s dimensions, hence the upsampling.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Grad-CAM也有一些缺点，例如无法识别多个发生或由预测类别表示的物体的全部。像CAM一样，激活图的分辨率可能受到最终卷积层维度的限制，因此需要上采样。
- en: For these reasons, we are using **guided Grad-CAM** instead. Guided Grad-CAM
    is a combination of Grad-CAM and guided backpropagation. Guided backpropagation
    is another visualization method that computes the gradients of the target class
    with respect to the input image, but it modifies the backpropagation process to
    only propagate positive gradients for positive activations. This results in a
    higher-resolution, more detailed visualization. This is achieved by performing
    an element-wise multiplication of the Grad-CAM heatmap (upsampled to the input
    image resolution) with the guided backpropagation result. The output is a visualization
    that emphasizes the most relevant features in the image for the given class, with
    higher spatial detail than Grad-CAM alone.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用**引导Grad-CAM**。引导Grad-CAM是Grad-CAM和引导反向传播的结合。引导反向传播是另一种可视化方法，它计算目标类别相对于输入图像的梯度，但它修改了反向传播过程，只传播正激活的正梯度。这导致了一个更高分辨率、更详细的可视化。这是通过将Grad-CAM热图（上采样到输入图像分辨率）与引导反向传播结果进行逐元素乘法来实现的。输出是一个可视化，强调给定类别在图像中最相关的特征，比单独的Grad-CAM具有更高的空间细节。
- en: 'Generating Grad-CAM attribution maps for all of our misclassified samples can
    be done with our `get_attribution_maps` function. All you need is the Captum attribution
    method (`attr.GuidedGradCam`), model (`garbage_mdl`), device, and the tensors
    for the misclassified samples (`X_misclass` and `y_misclass`), and, within the
    method initialization arguments, a layer for which Grad-CAM attributions are computed:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的`get_attribution_maps`函数为所有误分类样本生成Grad-CAM归因图。你需要的是Captum归因方法（`attr.GuidedGradCam`）、模型（`garbage_mdl`）、设备以及误分类样本的张量（`X_misclass`和`y_misclass`），并在方法初始化参数中，一个用于计算Grad-CAM归因的层：
- en: '[PRE28]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Notice that we aren’t using the last layer (which can be indexed with `7` or
    `-1`) but the fourth one (`3`). This is just to keep things interesting, but we
    can change it. Next, let’s plot the attributions just as we have before. Nearly
    the same code is used except `saliency_maps` is replaced by `gradcam_maps`. The
    output is depicted in *Figure 7.16*.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们并没有使用最后一层（可以用`7`或`-1`索引）而是第四层（`3`）。这样做只是为了保持事情有趣，但我们也可以更改它。接下来，让我们像之前一样绘制归因图。代码几乎相同，只是将`saliency_maps`替换为`gradcam_maps`。输出结果如图*7.16*所示。
- en: '![Chart  Description automatically generated](img/B18406_07_16.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B18406_07_16.png)'
- en: 'Figure 7.16: Guided Grad-CAM heatmaps for plastic misclassified as biological
    waste'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16：将塑料误分类为生物废物的引导Grad-CAM热图
- en: As you can observe in *Figure 7.16*, similar smooth matte areas are highlighted
    as with the saliency attribution maps, that except guided Grad-CAM yields a few
    bright areas and edges.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在*图7.16*中观察到的，与显著性归因图一样，类似的平滑哑光区域被突出显示，除了引导Grad-CAM产生一些亮区和边缘。
- en: Take all of this with a grain of salt. There is still a lot of ongoing debate
    in the CNN interpretation domain. And researchers are still coming up with new
    and better methods, and even techniques that are nearly perfect for most use cases
    still have flaws. Regarding CAM-like methods, there are many newer ones, such
    as **Score-CAM**, **Ablation-CAM**, and **Eigen-CAM**, which provide similar functionality
    but don’t rely on gradients, which can be unstable and, therefore, occasionally
    unreliable. We won’t discuss them here because, of course, they aren’t gradient-based!
    But it’s essential to note that it doesn’t hurt to try different methods to see
    what works for your use case.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有这些内容都要持保留态度。在CNN解释领域，仍然存在许多持续的争论。研究人员仍在提出新的和更好的方法，甚至对于大多数用例几乎完美的技术仍然存在缺陷。关于类似CAM的方法，有许多新的方法，例如**Score-CAM**、**Ablation-CAM**和**Eigen-CAM**，它们提供了类似的功能，但不需要依赖梯度，而梯度可能是不稳定的，因此有时是不可靠的。我们在这里不会讨论它们，因为当然，它们不是基于梯度的！但是，尝试不同的方法以查看哪些适用于您的用例是有益的。
- en: Integrated gradients
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成梯度
- en: '**Integrated gradients** (**IG**), also known as **path-integrated gradients**,
    is a technique that is not exclusive to CNNs. You can apply it to any neural network
    architecture because it computes the gradients of the output with respect to the
    inputs averaged all along a path between a **baseline** and the actual input.
    It is agnostic to the presence of convolutional layers. However, it requires the
    definition of a baseline, which is supposed to convey a lack of signal, like a
    uniformly colored image. In practice, for CNNs in particular, this is what a zero
    baseline represents, which, for every pixel, would usually mean a completely black
    image. Also, although the name suggests the use of **path integrals**, integrals
    aren’t computed but approximated, with summation in sufficiently small intervals
    for a certain number of steps. For a CNN, this means it makes variations of the
    input image progressively darker or lighter until it becomes the baseline corresponding
    to the predefined number of steps. It then feeds these variations to the CNN,
    computes the gradients for each one, and averages them. The IG is the dot product
    of the image times the gradient averages.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成梯度**（**IG**），也称为**路径积分梯度**，是一种不限于CNN的技术。您可以将它应用于任何神经网络架构，因为它计算了输出相对于输入的梯度，这些梯度是在从**基线**到实际输入之间的路径上平均计算的。它对卷积层的存在不敏感。然而，它需要定义一个基线，这个基线应该传达信号缺失的概念，比如一个均匀着色的图像。在实践中，特别是对于CNN来说，这表示零基线，对于每个像素来说，通常意味着一个完全黑色的图像。尽管名称暗示了使用**路径积分**，但积分并不是计算的，而是用足够小的区间内的求和来近似，对于一定数量的步骤。对于CNN来说，这意味着它使输入图像的变体逐渐变暗或变亮，直到它成为对应于预定义步骤数的基线。然后它将这些变体输入CNN，为每个变体计算梯度，并取平均值。IG是图像与梯度平均值之间的点积。'
- en: Like Shapley values, IG is grounded in solid mathematical theory. In this case,
    it’s the **fundamental theorem of calculus for line integrals**. The mathematical
    proof of the IG method ensures that the attributions of all the features add up
    to the difference between the model’s prediction on the input data and its prediction
    on the baseline input. In addition to this property, which they call **completeness**,
    there is linearity preservation, symmetry preservation, and sensitivity. We won’t
    describe each of these properties here. However, it’s important to note that some
    interpretation methods satisfy notable mathematical properties, while others demonstrate
    their effectiveness in practical terms.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 与Shapley值一样，IG建立在坚实的数学理论基础上。在这种情况下，它是**线积分的基本定理**。IG方法的数学证明确保了所有特征的归因之和等于模型在输入数据上的预测与在基线输入上的预测之间的差异。除了他们称之为**完备性**的这种属性之外，还有线性保持、对称保持和敏感性。我们在这里不会描述这些属性中的每一个。然而，重要的是要注意，一些解释方法满足显著的数学属性，而其他方法则从实际应用中证明了它们的有效性。
- en: In addition to IG, we will also leverage `NoiseTunnel` to perform small random
    perturbations on the sample image – in other words, to add noise. It creates different
    noisy versions of the same sample image multiple times and then computes the attribution
    method for each. It then averages these attributions, potentially making the attribution
    maps much smoother, which is why this method is called **SmoothGrad**.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 除了IG之外，我们还将利用`NoiseTunnel`对样本图像进行小的随机扰动——换句话说，就是添加噪声。它多次创建相同样本图像的不同噪声版本，然后计算每个版本的归因方法。然后它对这些归因进行平均，这可能是使归因图更加平滑的原因，这就是为什么这种方法被称为**SmoothGrad**。
- en: 'But wait, you may ask: Shouldn’t it be a perturbation-based method then?! We’ve
    already dealt with several perturbation-based methods before in this book, from
    SHAP to anchors, and something they have in common is that they perturb the input
    to measure the effect on the output. SmoothGrad doesn’t measure the impact on
    the outputs. It only helps yield a more robust attribution map because the mean
    attribution of perturbed inputs should make for more trustworthy attribution maps.
    We perform cross-validation to evaluate machine learning models for the same reason:
    the average metrics performed on different test datasets with slightly different
    distributions make for better metrics.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 但等等，你可能要问：那它不应该是一种基于扰动的算法吗？！在这本书中，我们之前已经处理了几种基于扰动的算法，从SHAP到锚点，它们共有的特点是它们扰动输入以测量对输出的影响。SmoothGrad并不测量对输出的影响。它只帮助生成一个更鲁棒的归因图，因为扰动输入的平均归因应该会生成更可靠的归因图。我们进行交叉验证来评估机器学习模型也是出于同样的原因：在不同分布的测试数据集上执行的平均指标会生成更好的指标。
- en: 'For IG, we will use very similar code as we did for Saliency, except we will
    add several arguments related to `NoiseTunnel`, such as the type of noise tunnel
    (`nt_type=''smoothgrad''`), the sample variations to produce (`nt_samples=20`),
    and an amount of random noise to add to each one in standard deviations (`stdevs=0.2`).
    We will find that the more permuted samples to generate, the better, up to a point,
    and then it doesn’t have much effect. However, there is such a thing as too much
    noise, and if you use too little, there won’t be any effect:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 对于IG，我们将使用与Saliency相同的非常相似的代码，除了我们将添加几个与`NoiseTunnel`相关的参数，例如噪声隧道的类型（`nt_type='smoothgrad'`）、用于生成的样本变化（`nt_samples=20`）以及添加到每个样本中的随机噪声的量（以标准差计`stdevs=0.2`）。我们会发现，生成的置换样本越多，效果越好，但达到一定程度后，效果就不会有太大变化。然而，噪声过多也是一种情况，如果你使用得太少，就不会有任何效果：
- en: '[PRE29]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can also optionally define the number of steps for the IG (`n_steps`). It’s
    set to `50` by default, and we can also modify the baselines, which is a tensor
    of zeros by default. As we’ve done with Grad-CAM, we can plot our first sample
    image side by side with the IG map, but this time, we will modify the code to
    plot the SmoothGrad integrated gradients (`smooth_ig_maps`) in the third position,
    like this:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以选择性地定义IG的步数（`n_steps`）。默认设置为`50`，我们还可以修改基线，默认情况下是一个全零的张量。正如我们使用Grad-CAM所做的那样，我们可以将第一个样本图像与IG图并排显示，但这次，我们将修改代码以在第三个位置绘制SmoothGrad集成梯度（`smooth_ig_maps`），如下所示：
- en: '[PRE30]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![Chart  Description automatically generated](img/B18406_07_17.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B18406_07_17.png)'
- en: 'Figure 7.17: Integrated gradient heatmaps for plastic misclassified as biological
    waste'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17：将塑料误分类为生物垃圾的集成梯度热图
- en: The areas in the IG heatmap in *Figure 7.17* coincide with many of the regions
    spotted by the saliency and guided Grad-CAM maps. However, there are more clusters
    of strong attributions in the bright yellow areas as well as in brownish shadowed
    areas, which is consistent with how some foods look when they are disposed of
    (like banana peels and rotten leafy greens). On the other hand, the bright orange
    and green areas aren’t.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在图7.17的IG热图中的区域与显著性图和引导Grad-CAM图检测到的许多区域相吻合。然而，在明亮的黄色区域以及棕色的阴影区域中，有更多的强归因簇，这与某些食物被丢弃时的外观（如香蕉皮和腐烂的叶状蔬菜）一致。另一方面，明亮的橙色和绿色区域则不是这样。
- en: As for the SmoothGrad IG heatmap, it is striking how different this map is compared
    to the non-smooth IG heatmap. This is not always the case; often, it’s just a
    smoother version. What likely happened was that the `0.2` noise distorted the
    attributions a bit too much, or that 20 perturbed samples weren’t enough. However,
    it’s tough to tell because it’s also possible that SmoothGrad more accurately
    depicts the real story.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 至于SmoothGrad IG热图，与不平滑的IG热图相比，这张图非常不同。这并不总是如此；通常，它只是更平滑的版本。可能发生的情况是`0.2`噪声对归因的影响过大，或者20个扰动样本不够。然而，很难说，因为也有可能SmoothGrad更准确地描绘了真实的故事。
- en: We won’t do this now, but you can visually “tune” the `stdevs` and `nt_samples`
    parameters. You can try it with less noise and more samples, using a series of
    combinations, such as `0.1` and `80`, and `0.15` and `40`, trying to figure out
    whether you see a commonality between them. The one you go with is the one that
    most clearly depicts this consistent story. One of the shortcomings of SmoothGrad
    is having to define optimal parameters. Incidentally, IG also has the same issue
    with defining the baselines and number of steps (`n_steps`). The default baseline
    won’t work in cases where the input image is too large or small, so it must be
    changed, and the authors of the IG paper suggest that 20-300 steps will approximate
    the integral within 5%.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在不会做这件事，但你可以直观地“调整”`stdevs`和`nt_samples`参数。你可以尝试使用更少的噪声和更多的样本，使用一系列组合，例如`0.1`和`80`，以及`0.15`和`40`，试图找出它们之间是否存在共性。你所选择的那个最能清楚地描绘出这个一致的故事。SmoothGrad的一个缺点是必须定义最优参数。顺便提一下，IG在定义基线和步数（`n_steps`）方面也存在相同的问题。默认的基线在输入图像太大或太小时将不起作用，因此必须更改，IG论文的作者建议20-300步将使积分在5%以内。
- en: 'Bonus method: DeepLIFT'
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励方法：DeepLIFT
- en: IG has its detractors, who have made similar methods that avoid using gradients,
    such as **DeepLIFT**. IG can be sensitive to zero-valued gradients and discontinuities
    with gradients, which can lead to misleading attributions. But these point to
    general disadvantages shared by all gradient-based methods. For this reason, we
    are introducing the **Deep Learning Important FeaTures** algorithm (**DeepLIFT**).
    It’s neither a gradient-based nor a perturbation-based method. It’s a backpropagation-based
    approach!
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: IG有一些批评者，他们已经创建了避免使用梯度的类似方法，例如**DeepLIFT**。IG对零值梯度和梯度的不连续性可能很敏感，这可能导致误导性的归因。但这些指向的是所有基于梯度的方法共有的缺点。因此，我们引入了**深度学习重要特征**算法（**DeepLIFT**）。它既不是基于梯度的，也不是基于扰动的。它是一种基于反向传播的方法！
- en: In this section, we will contrast it with IG. Like IG and Shapley values, DeepLIFT
    was designed for **completeness**, and as such, complies with remarkable mathematical
    properties. In addition to that, like IG, DeepLIFT can also be applied to various
    deep learning architectures, including CNNs and **recurrent neural networks**
    (**RNNs**), making it versatile for different use cases.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将将其与IG进行对比。像IG和Shapley值一样，DeepLIFT是为了**完整性**而设计的，因此符合显著的数学性质。除此之外，像IG一样，DeepLIFT也可以应用于各种深度学习架构，包括CNN和**循环神经网络**（**RNN**），使其适用于不同的用例。
- en: DeepLIFT works by decomposing the output prediction of the model into contributions
    from each input feature, using the concept of “difference-from-reference.” It
    backpropagates these contributions through the network layers to assign an importance
    score to each input feature.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: DeepLIFT通过使用“参考差异”的概念，将模型的输出预测分解为每个输入特征的贡献。它通过网络层反向传播这些贡献，为每个输入特征分配一个重要性分数。
- en: More specifically, like IG, it uses a baseline that represents no specific information
    about any class. However, it then calculates the difference in the activations
    of each neuron between the input and the baseline, and it backpropagates these
    differences through the network, calculating each neuron’s contribution to the
    output prediction. Then we sum the contributions for each input feature to obtain
    its importance score (attribution).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，像IG一样，它使用一个基线，该基线代表关于任何类别的信息。然而，它随后计算输入和基线之间每个神经元的激活差异，并通过网络反向传播这些差异，计算每个神经元对输出预测的贡献。然后我们为每个输入特征求和其贡献，以获得其重要性分数（归因）。
- en: 'It’s advantages over IG are as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 它相对于IG的优势如下：
- en: '**Reference-based**: Unlike gradient-based methods such as IG, DeepLIFT explicitly
    compares the input to a reference input, making the attributions more interpretable
    and meaningful.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于参考的**：与IG等基于梯度的方法不同，DeepLIFT明确地将输入与参考输入进行比较，这使得归因更加可解释和有意义。'
- en: '**Non-linear interactions**: DeepLIFT considers the non-linear interactions
    between neurons when computing attributions. It captures these interactions by
    considering the multipliers (the change in output due to the change in input)
    in each layer of the neural network.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非线性交互**：DeepLIFT在计算归因时考虑了神经元之间的非线性交互。它通过考虑神经网络每一层的乘数（由于输入的变化而导致的输出的变化）来捕捉这些交互。'
- en: '**Stability**: DeepLIFT is more stable than gradient-based methods, as it is
    less sensitive to small changes in the input, providing more consistent attributions.
    So, using a SmoothGrad is unnecessary on DeepLIFT attributions although highly
    recommended for gradient-based methods.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稳定性**：DeepLIFT比基于梯度的方法更稳定，因为它对输入的小变化不太敏感，提供了更一致的归因。因此，在DeepLIFT归因上使用SmoothGrad是不必要的，尽管对于基于梯度的方法来说强烈推荐。'
- en: Overall, DeepLIFT provides a more interpretable, stable, and comprehensive approach
    to attributions, making it a valuable tool for understanding and explaining deep
    learning models.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，DeepLIFT提供了一种更可解释、更稳定和更全面的归因方法，使其成为理解和解释深度学习模型的有价值工具。
- en: 'Next, we will create DeepLIFT attribution maps in a similar fashion as we have
    done the others:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将以类似的方式创建DeepLIFT归因图：
- en: '[PRE31]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: To plot an attribution map, nearly the same code as with Grad-CAM is used, except
    `gradcam_maps` is replaced by `deeplift_maps`. The output is depicted in *Figure
    7.18*.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 要绘制一个归因图，使用的代码几乎与Grad-CAM相同，只是将`gradcam_maps`替换为`deeplift_maps`。输出在*图7.18*中展示。
- en: '![Chart  Description automatically generated](img/B18406_07_18.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B18406_07_18.png)'
- en: 'Figure 7.18: DeepLIFT heatmaps for plastic misclassified as biological waste'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18：将塑料误分类为生物废物的DeepLIFT热图
- en: The attributions of *Figure 7.18* are not as noisy as in IG. But they also seem
    to cluster around some dull yellows and dark areas in the shadows; it also points
    toward dull greens near the top-right corner.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.18*的归因不如IG那样嘈杂。但它们似乎也聚集在阴影中的一些单调的黄色和深色区域；它还指向右上角附近的一些单调的绿色。'
- en: Tying it all together
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有这些结合起来
- en: 'Now, we will take everything that we have learned about gradient-based attribution
    methods and use it to understand the reasons for all the chosen misclassifications
    (the plastic false negatives and metal false positives). As we did with intermediate
    activation maps, we can leverage the `compare_img_pred_viz` function to place
    the higher-resolution sample image side by side with four attribution maps: saliency,
    Grad-CAM, SmoothGrad IG, and DeepLift. To this end, we first have to iterate all
    the misclassifications’ positions and indexes and extract all the maps. Note that
    we are using `overlay_bg` in the `tensor_to_img` function to produce a new image
    overlaying the original image with the heatmap for each. Lastly, we concatenate
    the four attribution outputs into a single image (`viz_img`). Just as we have
    done before, we extract the actual label (`y_true`), predicted label (`y_pred`),
    and `pandas` series with the probabilities (`probs_s`) to add some context to
    the plot we will produce. The `for` loop will produce six plots but, for brevity’s
    sake, we are only going to discuss three of them:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将运用我们所学到的关于基于梯度的归因方法的一切知识，来理解所有选择的错误分类（塑料的假阴性金属的假阳性）的原因。正如我们处理中间激活图一样，我们可以利用`compare_img_pred_viz`函数将高分辨率的样本图像与四个归因图并排显示：显著性、Grad-CAM、SmoothGrad
    IG和DeepLift。为此，我们首先必须迭代所有错误分类的位置和索引，并提取所有图。请注意，我们正在使用`tensor_to_img`函数中的`overlay_bg`来生成一个新的图像，每个图像都叠加了原始图像和热图。最后，我们将四个归因输出连接成一个单独的图像（`viz_img`）。正如我们之前所做的那样，我们提取实际的标签（`y_true`）、预测标签（`y_pred`）和带有概率的`pandas`系列（`probs_s`），以便为我们将生成的图表添加一些上下文。`for`循环将生成六个图表，但为了简洁起见，我们只将讨论其中的三个：
- en: '[PRE32]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The preceding code generates *Figures 7.19* to *7.21*. It’s important to note
    that in all generated plots, we can observe saliency attributions at the top left,
    SmoothGrad IG at the top right, guided Grad-CAM at the bottom left, and DeepLIFT
    at the bottom right:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码生成了*图7.19*到*图7.21*。重要的是要注意，在所有生成的图表中，我们都可以观察到左上角的显著性归因、右上角的SmoothGrad IG、左下角的引导Grad-CAM和右下角的DeepLIFT：
- en: '![Graphical user interface  Description automatically generated](img/B18406_07_19.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面  自动生成描述](img/B18406_07_19.png)'
- en: 'Figure 7.19: Gradient-based attributions for battery as metal misclassification
    #8'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '图7.19：金属误分类为电池的基于梯度的归因 #8'
- en: In *Figure 7.19*, there’s a lack of consistency between all four attribution
    methods. The saliency attribution maps show that all the center parts of the batteries
    are seen as metal surfaces, in addition to the white parts of the cardboard container.
    On the other hand, SmoothGrad IG zeros in on the white cardboard and Grad-CAM
    on the blue cardboard almost exclusively. Lastly, DeepLIFT is much more sparse,
    only pointing to some parts of the white cardboard.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.19*中，所有四种归因方法之间缺乏一致性。显著性归因图显示，所有电池的中心部分都被视为金属表面，除了纸箱的白色部分。另一方面，SmoothGrad
    IG主要聚焦于白色纸箱，而Grad-CAM几乎完全聚焦于蓝色纸箱。最后，DeepLIFT的归因非常稀疏，仅指向白色纸箱的一些部分。
- en: 'In *Figure 7.20*, the attributions are much more consistent than in *Figure
    7.19*. Matte white areas are clearly confusing the model. This makes sense considering
    that the plastic in the training data was mostly single pieces of empty plastic
    containers – including white milk jugs. However, people do recycle toys, plastic
    tools like spatulas, and other plastic objects. Interestingly enough, although
    all attribution methods were salient around white and light-yellow surfaces, SmoothGrad
    IG also highlights some edges, like one of the ducks’ hats and another one’s collar:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.20*中，归因比*图7.19*中的一致性要好得多。哑光白色区域明显让模型感到困惑。考虑到训练数据中的塑料主要是空塑料容器的单个部件——包括白色牛奶壶——这是有道理的。然而，人们确实回收玩具、塑料工具如勺子和其他塑料物品。有趣的是，尽管所有归因方法都在白色和浅黄色表面上都很显著，SmoothGrad
    IG还突出了某些边缘，如一只鸭子的帽子和另一只的领子：
- en: '![Graphical user interface, application  Description automatically generated](img/B18406_07_20.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用  自动生成描述](img/B18406_07_20.png)'
- en: 'Figure 7.20: Gradient-based attributions for plastic misclassification #86'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '图7.20：塑料误分类的基于梯度的归因 #86'
- en: 'To continue with the recycling toys theme, how do LEGO bricks get misclassified
    as batteries? See *Figure 7.21* for an interpretation:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 继续探讨回收玩具的主题，乐高积木是如何被错误分类为电池的？参见*图7.21*以获取解释：
- en: '![Diagram  Description automatically generated](img/B18406_07_21.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图表  自动生成描述](img/B18406_07_21.png)'
- en: 'Figure 7.21: Gradient-based attributions for plastic misclassification #89'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '图7.21：塑料误分类的基于梯度的归因 #89'
- en: '*Figure 7.21* shows how, among all the attribution methods, it’s mostly the
    yellow and green bricks (and to a lesser degree, light blue) that are to blame
    for the misclassification because these are popular colors among battery manufacturers,
    as attested by the training data. Also, the flat surface in between the studs
    got the most attributions since these resemble the contacts in batteries and,
    more specifically, 9-volt square batteries. As with the other examples, saliency
    was the most noisy method. However, this time, guided Grad-CAM was the least noisy.
    It was also more salient on the edges than on surfaces, unlike the others.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.21*展示了在所有归因方法中，主要是黄色和绿色的积木（以及较少的浅蓝色）是误分类的罪魁祸首，因为这些颜色在电池制造商中很受欢迎，正如训练数据所证明的那样。此外，
    studs 之间的平面表面获得了最多的归因，因为这些表面与电池的接触相似，尤其是9伏方形电池。与其他示例一样，显著性是最嘈杂的方法。然而，这次，引导Grad-CAM是最不嘈杂的。它也比其他方法在边缘上的显著性更强，而不是在表面上。'
- en: We will next try to discover what the model learned about batteries (in addition
    to white glass) through perturbation-based attribution methods performed on true
    positives.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来将尝试通过在真实正例上执行的基于扰动的归因方法，来发现模型关于电池（除了白色玻璃之外）学到了什么。
- en: Understanding classifications with perturbation-based attribution methods
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过扰动归因方法理解分类
- en: Perturbation-based methods have already been covered to a great extent in this
    book so far. So many of the methods we have covered, including SHAP, LIME, anchors,
    and even permutation feature importance, employ perturbation-based strategies.
    The intuition behind them is that if you remove, alter, or mask features in your
    input data and then make predictions with them, you’ll be able to attribute the
    difference between the new predictions and the original predictions to the changes
    you made in the input. These strategies can be leveraged in both global and local
    interpretation methods.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这本书已经对基于扰动的方 法进行了大量的介绍。因此，我们介绍的大多数方法，包括SHAP、LIME、锚点，甚至排列特征重要性，都采用了基于扰动的策略。这些策略背后的直觉是，如果你从你的输入数据中删除、更改或屏蔽特征，然后使用它们进行预测，你将能够将新预测与原始预测之间的差异归因于你在输入中做出的更改。这些策略可以在全局和局部解释方法中加以利用。
- en: 'We will now do the same as we did with the misclassification samples, but to
    the chosen true positives, and gather four of each class in a single tensor (`X_correctcls`):'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将像对错误分类样本所做的那样做，但针对选定的真阳性，并在单个张量（`X_correctcls`）中收集每个类别的四个样本：
- en: '[PRE33]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'One of the more complicated aspects of performing permutation methods on images
    is that there are not just a few dozen features but many thousands to permute.
    Picture this: 224 x 224 equals 50,176 pixels, and if we want to measure how a
    change in each pixel independently affects the outcome, we’ll need to make at
    least 20 permuted samples for each pixel. So, over a million! For this reason,
    several permutation methods accept masks to determine which blocks of pixels to
    permute at once. If we group them in blocks of 32 x 32 pixels, this means we’ll
    have only 49 blocks in total to permute. However, although it will speed up the
    attribution methods, we’ll miss out on the effects on smaller sets of pixels the
    larger the block.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像上执行排列方法的一个更复杂方面是，不仅有几十个特征，而是有成千上万个特征需要排列。想象一下：224 x 224等于50,176像素，如果我们想测量每个像素独立变化对结果的影响，我们至少需要为每个像素制作20个排列样本。所以，超过一百万！因此，几个排列方法接受掩码来确定一次要排列哪些像素块。如果我们将它们分成32
    x 32像素的块，这意味着我们总共只有49个块需要排列。然而，尽管这会加快归因方法的速度，但如果我们块越大，就会错过对较小像素集的影响。
- en: 'We can use many methods to create masks, such as using a segmentation algorithm
    to break up the images into intuitive blocks based on surfaces and edges. Segmentation
    is done per image, so the number and placement of segments will vary on an image-to-image
    basis. There are many methods with scikit-learn’s image segmentation library (`skimage.segmentation`):
    [https://scikit-image.org/docs/stable/api/skimage.segmentation.html](https://scikit-image.org/docs/stable/api/skimage.segmentation.html).
    However, we are going to keep things simple and create one mask for all 224 x
    224 images with the following code:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用许多方法来创建掩码，例如使用分割算法根据表面和边缘将图像分割成直观的块。分割是按图像进行的，因此段的数量和位置将在图像之间变化。scikit-learn的图像分割库（`skimage.segmentation`）有许多方法：[https://scikit-image.org/docs/stable/api/skimage.segmentation.html](https://scikit-image.org/docs/stable/api/skimage.segmentation.html)。然而，我们将保持简单，并使用以下代码为所有224
    x 224图像创建一个掩码：
- en: '[PRE34]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: What the preceding code does is initialize a tensor of zeros the size of the
    model’s input. It’s easier to conceptualize this tensor as an empty image. Then
    it moves across strides that are 16 pixels wide and high, from the top-left corner
    of the image to the bottom right. As it moves across, it sets the values with
    consecutive numbers with the `counter`. What you end up with is a tensor with
    all values filled with numbers between 0 and 195, and, if you visualized it as
    an image, it would be a diagonal gradient from black at the top left to light
    gray at the bottom right. What’s important to note is that each block with the
    same value is treated as if it were the same pixel by the attribution method.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码所做的初始化一个与模型输入大小相同的零张量。将这个张量概念化为一个空图像会更简单。然后它沿着16像素宽和高的步长移动，从图像的左上角到右下角。在移动过程中，它使用`counter`设置连续数字的值。最终你得到一个所有值都填充了0到195之间数字的张量，如果你将其可视化为一幅图像，它将是一个从左上角的黑色到右下角浅灰色的对角渐变。重要的是要注意，具有相同值的每个块都被归因方法视为相同的像素。
- en: Before we move forward, let’s discuss baselines. In Captum attribution methods,
    as in other libraries for that matter, the default baseline is a tensor of zeros,
    which is usually equivalent to a black image when images are made up of floating-point
    numbers between 0 and 1\. However, in our case, we are standardizing our input
    tensors so the model doesn’t see tensors with a minimum of 0 but a mean of 0!
    Therefore, for our garbage model, a tensor of zeros corresponds to a medium gray
    image, not a black image. For gradient-based methods, there’s nothing inherently
    wrong with a gray image baseline because there are likely a number of steps between
    it and the input image. However, perturbation-based methods can be particularly
    sensitive to having baselines that are too close to the input image because if
    you replace parts of the input image with the baseline, the model won’t tell the
    difference!
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续前进之前，让我们讨论一下基线。在Captum归因方法中，正如其他库的情况一样，默认基线是一个全零张量，当图像由介于0和1之间的浮点数组成时，这通常等同于一个黑色图像。然而，在我们的情况下，我们正在标准化我们的输入张量，这样模型就不会看到最小值为0但平均值为0的张量！因此，对于我们的垃圾模型，全零张量对应于中等灰色图像，而不是黑色图像。对于基于梯度的方法，灰色图像基线本身并没有固有的错误，因为很可能存在许多步骤介于它和输入图像之间。然而，基于扰动的方
    法可能对基线过于接近输入图像特别敏感，因为如果你用基线替换输入图像的部分，模型将无法区分出来！
- en: 'For our garbage model’s case, a black image is made up of tensors of `-2.1179`
    because one of the transformations performed to standardize the input tensors
    was `(x-0.485)/0.229`, which happens to equal approximately `-2.1179`, when `x=0`.
    You can also calculate the tensors when `x=1`; it converts to `2.64` for a white
    image. That being said, there’s no harm in assuming that somewhere in our true
    positive samples, there’s at least one pixel that has the lowest value and another
    with the highest, so we will just use `max()` and `min()` to create both light
    and dark baselines:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的垃圾模型的情况，一个黑色图像由张量`-2.1179`组成，因为我们对输入张量执行标准化操作之一是`(x-0.485)/0.229`，当`x=0`时，这恰好等于大约`-2.1179`。你还可以计算当`x=1`时的张量；它转换为白色图像的`2.64`。话虽如此，假设在我们的真实阳性样本中，至少有一个像素具有最低值，另一个具有最高值，是没有害处的，因此我们将只使用`max()`和`min()`来创建亮暗基线：
- en: '[PRE35]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We will use only one baseline for all but one perturbation method but feel free
    to switch them around. Now, on to creating attribution maps for each method!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只对除了一个扰动方法之外的所有方法使用一个基线，但请随意切换它们。现在，让我们继续为每种方法创建归因图！
- en: Feature ablation
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征消除
- en: '**Feature ablation** is a relatively simple method. What it does is occlude
    portions of the sample input image by replacing it with the baseline, which is,
    by default, zero. The goal is to understand the importance of each input feature
    (or feature group) in making a prediction by observing the effect of altering
    it.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征消除**是一种相对简单的方法。它所做的是通过用基线替换它来遮挡样本输入图像的一部分，默认情况下，基线为零。目标是通过对改变它的效果进行观察，了解每个输入特征（或特征组）在做出预测中的重要性。'
- en: 'Here’s how feature ablation works:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是特征消除是如何工作的：
- en: '**Obtain the original prediction**: First, the model’s prediction for the original
    input is obtained. This serves as a baseline for comparing the effect of perturbing
    the input features.'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**获取原始预测**：首先，获取模型对原始输入的预测。这作为比较扰动输入特征效果的基准。'
- en: '**Perturb the input feature**: Next, for each input feature (or feature group
    as set by the feature mask), it is replaced with the baselines value. This creates
    an “ablated” version of the input.'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**扰动输入特征**：接下来，对于每个输入特征（或由特征掩码设置的特征组），它被替换为基线值。这创建了一个“消除”版本的输入。'
- en: '**Obtain the prediction for the perturbed input**: The model’s prediction is
    calculated for the ablated input.'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**获取扰动输入的预测**：计算消除输入的模型预测。'
- en: '**Compute the attribution**: The difference in the model’s predictions between
    the original input and the ablated input is calculated. This difference is attributed
    to the altered feature, indicating its importance in the prediction.'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算归因**：计算原始输入和消除输入之间模型预测的差异。这个差异归因于改变的特征，表明它在预测中的重要性。'
- en: Feature ablation is a simple and intuitive approach to understanding the importance
    of input features in a model’s prediction. However, it has some limitations. It
    assumes that features are independent and may not accurately capture the effects
    of interactions between features. Additionally, it can be computationally expensive
    for models with a large number of input features or complex input structures.
    Despite these limitations, feature ablation is a valuable tool for understanding
    and interpreting model behavior.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 特征消除是一种简单直观的方法，用于理解模型预测中输入特征的重要性。然而，它也有一些局限性。它假设特征是独立的，可能无法准确捕捉特征之间交互的影响。此外，对于具有大量输入特征或复杂输入结构的模型，它可能计算成本高昂。尽管存在这些局限性，特征消除仍然是理解和解释模型行为的一个有价值的工具。
- en: 'To generate the attribution maps, we will use the `get_attribution_maps` function
    as we have before, and enter the additional arguments for the `feature_mask` and
    `baselines`:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成归因图，我们将使用之前使用的`get_attribution_maps`函数，并输入额外的`feature_mask`和`baselines`参数：
- en: '[PRE36]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To plot an example of the attribution map, you can copy the same code that
    we used for saliency, except `saliency_maps` is replaced by `ablation_maps`, and
    we are using the second image in the `occlusion_maps` array, like this:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 要绘制归因图的示例，你可以复制我们用于显著性的相同代码，只是将`saliency_maps`替换为`ablation_maps`，并且我们使用`occlusion_maps`数组中的第二个图像，如下所示：
- en: '[PRE37]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B18406_07_22.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 描述自动生成，置信度中等](img/B18406_07_22.png)'
- en: 'Figure 7.22: Feature ablation maps for a white glass true positive from the
    test dataset'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.22：测试数据集中白色玻璃真阳性的特征消除图
- en: In *Figure 7.22*, the feature groups in the bottom of the bowl of the wine glass
    appear to be most important because their absence makes the biggest difference
    in the outcome, but other portions of the glass are also salient to a lesser degree,
    except the stem of the wine glass. It makes sense because a wine glass without
    a stem is still a glass-like container.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.22*中，酒杯底部的特征组似乎是最重要的，因为它们的缺失对结果的影响最大，但酒杯的其他部分也有一定程度的显著性，除了酒杯的茎。这是有道理的，因为没有茎的酒杯仍然是一个类似玻璃的容器。
- en: Next, we will discuss a similar method that will be able to show us attributions
    with greater detail.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论一种类似的方法，它将能够以更详细的方式展示归因。
- en: Occlusion sensitivity
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 遮挡敏感性
- en: '**Occlusion sensitivity** is very similar to feature ablation because it also
    replaces portions of the image with a baseline. However, unlike feature ablation,
    it doesn’t use a feature mask to group pixels together. Instead, it groups contiguous
    features automatically with a sliding window and strides. In this process, it
    creates many overlapping regions. When this happens, it averages the output differences
    to compute the attribution for each pixel.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '**遮挡敏感性**与特征消除非常相似，因为它也用基线替换了图像的部分。然而，与特征消除不同，它不使用特征掩码来分组像素。相反，它使用滑动窗口和步长自动将连续特征分组，在这个过程中，它创建了多个重叠区域。当这种情况发生时，它会对输出差异进行平均，以计算每个像素的归因。'
- en: In this scenario, besides overlapping regions and their corresponding averages,
    occlusion sensitivity and feature ablation are identical. In fact, if we used
    both sliding windows and strides of 3 x 16 x 16, there wouldn’t be any overlapping
    areas and the feature grouping would be identical to those defined by our `feature_mask`
    made up of 16 x 16 blocks.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个场景中，除了重叠区域及其对应平均值之外，遮挡敏感性和特征消除是相同的。事实上，如果我们使用滑动窗口和3 x 16 x 16的步长，就不会有任何重叠区域，特征分组将与由16
    x 16块组成的`feature_mask`定义的特征分组相同。
- en: So, you may wonder, what’s the point of being familiar with both methods? The
    point is occlusion sensitivity is only suitable for use when a fixed grouping
    of contiguous features matter, like with images and perhaps other spatial data.
    And because of its use of strides, it can capture local dependencies and spatial
    relationships between features. However, although we used contiguous blocks of
    features, feature ablation doesn’t have to because the `feature_mask` can be arranged
    in whichever way it makes most sense for your inputs to be segmented. This small
    detail makes it very versatile to other data types. Therefore, feature ablation
    is a more general approach that can handle various input types and model architectures,
    while occlusion sensitivity is specifically tailored to image data and convolutional
    neural networks, with a focus on spatial relationships between features.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你可能想知道，熟悉这两种方法有什么意义？意义在于遮挡敏感性仅在固定分组连续特征很重要时才适用，比如图像和可能的其他空间数据。由于其使用步长，它可以捕捉特征之间的局部依赖性和空间关系。然而，尽管我们使用了连续的特征块，特征消融不必如此，因为`feature_mask`可以以任何对输入进行分段最有意义的方式排列。这个细节使其非常适用于其他数据类型。因此，特征消融是一种更通用的方法，可以处理各种输入类型和模型架构，而遮挡敏感性则是专门针对图像数据和卷积神经网络定制的，重点关注特征之间的空间关系。
- en: 'To generate the attribution maps for occlusion, we will do as before, and enter
    the additional arguments for the `baselines`, `sliding_window_shapes`, and `strides`:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成遮挡的归因图，我们将像以前一样操作，并输入额外的参数`baselines`、`sliding_window_shapes`和`strides`：
- en: '[PRE38]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Please note that we are creating ample overlapping regions by setting the strides
    to be only 8 pixels while the sliding windows are 16 pixels. To plot an attribution,
    you can copy the same code that we used for feature ablation, except `ablation_maps`
    is replaced by `occlusion_maps`. The output is depicted in *Figure 7.23*:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们通过将步长设置为仅8像素，而滑动窗口为16像素，创建了充足的重叠区域。要绘制归因图，你可以复制我们用于特征消融的相同代码，只是将`ablation_maps`替换为`occlusion_maps`。输出如图**图7.23**所示：
- en: '![Graphical user interface  Description automatically generated](img/B18406_07_23.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 描述自动生成](img/B18406_07_23.png)'
- en: 'Figure 7.23: Occlusion sensitivity maps for a white glass true positive from
    the test dataset'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.23：测试数据集中白色玻璃真阳性的遮挡敏感性图
- en: With *Figure 7.23*, we can tell that occlusion’s attributions are eerily similar
    to the ablation’s attribution, except with more resolution. This resemblance shouldn’t
    be surprising considering how the feature mask of the former aligns with the sliding
    window of the latter.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**图7.23**，我们可以看出遮挡的归因与消融的归因惊人地相似，只是分辨率更高。考虑到前者的特征掩码与后者的滑动窗口对齐，这种相似性并不令人惊讶。
- en: Whether we use blocks of non-overlapping 16 x 16 pixels or overlapping 8 x 8,
    the impact of their absence is measured independently to create the attributions.
    Therefore, both ablation and occlusion methods aren’t equipped to measure interactions
    between non-contiguous feature groups. This can prove to be a problem when the
    absence of two non-contiguous feature groups is what causes a classification to
    change. For instance, can a wine glass without a stem or a base still be considered
    a wine glass? It can certainly be considered glass, one would hope, but perhaps
    the model has learned the wrong relationships.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们使用16 x 16像素的非重叠块还是8 x 8像素的重叠块，它们的缺失影响都是独立测量的，以创建归因。因此，消融和遮挡方法都没有装备来测量非连续特征组之间的交互。当两个非连续特征组的缺失导致分类发生变化时，这可能会成为一个问题。例如，没有把手或底座的酒杯还能被认为是酒杯吗？当然可以被认为是玻璃，人们希望如此，但也许模型学到了错误的关系。
- en: 'Speaking of relationships, next, we will revisit an old friend: Shapley!'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 说到关系，接下来，我们将回顾一个老朋友：Shapley！
- en: Shapley value sampling
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Shapley值采样
- en: If you recall from *Chapter 4*, *Global Model-Agnostic Interpretation Methods*,
    Shapley has provided a method that is very good at measuring and attributing the
    impact of coalitions of features to the outcome. Shapley does this by permuting
    entire coalitions of features at a time rather than permuting one feature at a
    time, like the two previous methods. That way, it can tease out how more than
    one feature or feature group interacts with one another.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得*第4章*，*全局模型无关解释方法*，Shapley提供了一种非常擅长衡量和归因特征联盟对结果影响的方法。Shapley通过一次对整个特征联盟进行排列，而不是像前两种方法那样一次排列一个特征，来实现这一点。这样，它可以揭示多个特征或特征组如何相互作用。
- en: 'The code to create the attribution maps should be very familiar by now. This
    method uses the `feature_mask` and `baselines` but also the number of feature
    permutations tested (`n_samples`). This last attribute has a huge impact on the
    fidelity of the method. However, it can make it notoriously computationally expensive,
    so we are not going to run it with the default 25 samples per permutation. Instead,
    we will use 5 samples to make things more manageable. However, feel free to tweak
    it should your hardware be able to handle it:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 创建归因图的代码现在应该非常熟悉了。这种方法使用`feature_mask`和`baselines`，但也测试了特征排列的数量（`n_samples`）。这个最后的属性对方法的保真度有巨大影响。然而，它可能会使计算成本变得非常昂贵，所以我们不会使用默认的每个排列25个样本来运行它。相反，我们将使用5个样本来使事情更易于管理。然而，如果你的硬件能够处理，请随意调整它：
- en: '[PRE39]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](img/B18406_07_24.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18406_07_24.png)'
- en: 'Figure 7.24: Shapley value sampling maps for a white glass true positive from
    the test dataset'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.24：测试数据集中白色玻璃真阳性的Shapley值采样图
- en: '*Figure 7.24* shows some consistent attributions, such as the most salient
    area being in the bottom-left corner of the wine glass bowl. Also, the base seems
    to be more important than the occlusion and ablation methods suggested.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.24*显示了一些一致的归因，例如最显著的区域位于酒杯碗的左下角。此外，底部似乎比遮挡和消融方法更重要。'
- en: However, the attributions are a lot more noisy than the previous ones. This
    is partially because we didn’t use a sufficient number of samples to cover all
    the combinations of features and interactions, and partially because of the messy
    nature of interactions. It makes sense that attributions for a single independent
    feature are concentrated in a few areas, such as the bowl of a wine glass. However,
    interactions can rely on several parts of the image, such as the base and the
    rim of the wine glass. They may become important only when they appear together.
    More interesting is the effect of a background. For instance, if you remove portions
    of the background, does the wine glass no longer look like a wine glass? Perhaps
    the background is more important than you think, especially when dealing with
    a translucent material.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些归因比之前的要嘈杂得多。这部分的理由是因为我们没有使用足够数量的样本来覆盖所有特征和交互的组合，部分原因是因为交互的混乱性质。对于单个独立特征的归因集中在几个区域是有意义的，例如酒杯的碗。然而，交互可能依赖于图像的几个部分，例如酒杯的底部和边缘。它们可能只有在它们一起出现时才变得重要。更有趣的是背景的影响。例如，如果你移除背景的一部分，酒杯是否不再像酒杯？也许背景比你想象的更重要，尤其是在处理半透明材料时。
- en: KernelSHAP
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KernelSHAP
- en: Since we are on the topic of Shapley values, let’s try `KernelSHAP` from *Chapter
    4*, *Global Model-Agnostic Interpretation Methods*. It leverages LIME to compute
    Shapley values more efficiently. The Captum implementation is similar to the SHAP
    one except it uses linear regression and not Lasso, and it computes the kernel
    differently. Also, for the LIME image explainer, it is best to use meaningful
    feature groups (called superpixels) rather than the contiguous blocks we have
    used in the feature mask. The same advice persists for `KernelSHAP`. However,
    we will keep this simple for this exercise, and also consistent for comparing
    with the other three permutation-based methods.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们谈论到了Shapley值，那么让我们尝试一下来自*第4章*，*全局模型无关解释方法*中的`KernelSHAP`。它利用LIME来更高效地计算Shapley值。Captum的实现与SHAP类似，但它使用的是线性回归而不是Lasso，并且计算核的方式也不同。此外，对于LIME图像解释器，最好使用有意义的特征组（称为超像素）而不是我们在特征掩码中使用的连续块。同样的建议也适用于`KernelSHAP`。然而，为了这个练习的简单性，我们也将保持一致性，并与其他三种基于排列的方法进行比较。
- en: 'We will now create the attribution maps but, this time, we will do one with
    light baselines and another with dark ones. Because `KernelSHAP` is an approximation
    to Shapley sampling values and not as computationally expensive, we can set `n_samples=300`.
    However, this won’t necessarily guarantee high fidelity because it takes a high
    amount of samples in `KernelSHAP` to approximate what a relatively low amount
    of samples can do exhaustively with Shapley:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将创建归因图，但这次我们将使用浅色基线和深色基线各做一个。因为`KernelSHAP`是对Shapley采样值的近似，并且计算成本不是很高，所以我们可以将`n_samples`设置为300。然而，这并不一定能保证高保真度，因为`KernelSHAP`需要大量的样本来近似相对较少的样本可以用Shapley彻底做到的事情：
- en: '[PRE40]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B18406_07_25.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![包含图形用户界面的图片，自动生成描述](img/B18406_07_25.png)'
- en: 'Figure 7.25: KernelSHAP maps for a white glass true positive from the test
    dataset'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.25：测试数据集中白色玻璃真正阳性的KernelSHAP图
- en: The two attribution maps in *Figure 7.25* are mostly not consistent with each
    other, but more importantly, not with the previous attributions. Sometimes, some
    methods have a harder time than others, or it takes some tweaking to get them
    to work how we expect them to.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.25*中的两个归因图在大多数情况下并不一致，更重要的是，与之前的归因不一致。有时，某些方法比其他方法更难，或者需要一些调整才能按预期工作。'
- en: Tying it all together
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有这些结合起来
- en: 'Now, we will take everything that we have learned about perturbation-based
    attribution methods and use it to understand the reasons for all the chosen true
    positive classifications (for both white glass and batteries). As we did before,
    we can leverage the `compare_img_pred_viz` function to place the higher-resolution
    sample image side by side with the four attribution maps: feature ablation, occlusion
    sensitivity, Shapley, and `KernelSHAP`. First, we have to iterate all the classifications’
    positions and indexes and extract all the maps. Note that we are using `overlay_bg`
    to produce a new image overlaying the original image with the heatmap for every
    attribution, as we did for the gradient-based section. Lastly, we concatenate
    the four attribution outputs into a single image (`viz_img`). Just as we have
    done before, we extract the actual label (`y_true`), predicted label (`y_pred`),
    and `pandas` series with the probabilities (`probs_s`) to add some context to
    the plot we will produce. The `for` loop will produce six plots, but we will only
    discuss two of them:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将利用关于基于扰动归因方法的所有知识，来理解所有选择的真正阳性分类（无论是白色玻璃还是电池）的原因。正如我们之前所做的那样，我们可以利用`compare_img_pred_viz`函数将高分辨率样本图像与四个归因图并排放置：特征消融、遮挡敏感性、Shapley和`KernelSHAP`。首先，我们必须迭代所有分类的位置和索引，并提取所有图。请注意，我们正在使用`overlay_bg`来生成一个新的图像，该图像将每个归因的热图叠加到原始图像上，就像我们在基于梯度的部分所做的那样。最后，我们将四个归因输出连接成一个单独的图像（`viz_img`）。正如我们之前所做的那样，我们提取实际的标签（`y_true`）、预测标签（`y_pred`）和包含概率的`pandas`系列（`probs_s`），以便为我们将生成的图表添加一些上下文。`for`循环将生成六个图表，但我们只会讨论其中的两个：
- en: '[PRE42]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Overall, you can tell that ablation and occlusion are very consistent, while
    much less so with Shapley and `KernelSHAP`. However, what Shapley and `KernelSHAP`
    have in common is that the attributions are more spread out.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，你可以看出消融和遮挡非常一致，而Shapley和`KernelSHAP`则不太一致。然而，Shapley和`KernelSHAP`的共同之处在于归因更加分散。
- en: 'In *Figure 7.26*, all attribution methods have text highlighted, as well as,
    at least, the left contact of the battery. This is similar to *Figure 7.28*, where
    the text is abundantly highlighted as well as the top contact. This suggests that,
    for batteries, the model has learned that text and a contact matter. As for white
    glass, it is less clear. All the attribution methods in *Figure 7.27* point to
    some of the edges of the broken vase, but not always the same edges (except for
    ablation and occlusion, which are consistent):'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.26*中，所有归因方法都突出了文本，以及至少电池的左侧接触。这与*图7.28*相似，那里的文本也被大量突出显示，以及顶部接触。这表明，对于电池，模型已经学会了文本和接触都很重要。至于白色玻璃，则不太清楚。*图7.27*中的所有归因方法都指向破碎花瓶的一些边缘，但并不总是相同的边缘（除了消融和遮挡，它们是一致的）：
- en: '![Graphical user interface, application  Description automatically generated](img/B18406_07_26.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，自动生成描述](img/B18406_07_26.png)'
- en: 'Figure 7.26: Perturbation-based attributions for battery classification #1'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.26：电池分类的第1个基于扰动的归因
- en: 'White glass is the hardest glass to classify of the three, and it’s not hard
    to tell why:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 白色玻璃是三种玻璃中最难分类的，原因也不难理解：
- en: '![Graphical user interface  Description automatically generated](img/B18406_07_27.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面  自动生成的描述](img/B18406_07_27.png)'
- en: 'Figure 7.27: Perturbation-based attributions for white glass classification
    #113'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7.27**：基于扰动的白色玻璃分类#113的归因'
- en: As noted in *Figure 7.27*, and others in the test example, it’s hard for the
    model to distinguish white glass from the light backgrounds. It manages to classify
    it correctly with these examples. However, this doesn’t mean it will generalize
    well in other examples where glass is in shards and not as well-lit. As long as
    the attributions show significant influence from the background, it’s hard to
    trust that it can recognize glass for its specular highlights, texture, and edges
    alone.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 如**图7.27**和其他测试示例中所述，模型很难区分白色玻璃和浅色背景。它设法用这些例子正确分类。然而，这并不意味着它在其他例子中也能很好地泛化，例如玻璃碎片和照明不足的情况。只要归因显示背景有显著的影响，就很难相信它仅凭镜面高光、纹理和边缘就能识别玻璃。
- en: '![Graphical user interface  Description automatically generated](img/B18406_07_28.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面  自动生成的描述](img/B18406_07_28.png)'
- en: 'Figure 7.28: Perturbation-based attributions for battery classification #2'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7.28**：基于扰动的电池分类#2的归因'
- en: For *Figure 7.28*, the background is also highlighted significantly in all attribution
    maps. But perhaps this is because the baseline was dark and so is the object in
    its entirety. If you replace an area just outside the edge of the battery with
    a black square, it makes sense that the model would be confused. For this reason,
    with permutation-based methods, it’s important to choose an appropriate baseline.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**图7.28**，在所有归因图中背景也被显著突出。但也许这是因为基线是暗的，整个物体也是如此。如果你用黑色方块替换电池边缘外的区域，模型会感到困惑是有道理的。因此，在使用基于排列的方法时，选择一个合适的基线非常重要。
- en: Mission accomplished
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务完成
- en: The mission was to provide an objective evaluation of the garbage classification
    model for the municipal recycling plant. The predictive performance on out-of-sample
    validation images was dismal! You could have stopped there, but then you would
    not have known how to make a better model.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是提供一个对市政回收厂垃圾分类模型的客观评估。在样本外验证图像上的预测性能非常糟糕！你本可以就此停止，但那样你就不知道如何制作一个更好的模型。
- en: 'However, the predictive performance evaluation was instrumental in deriving
    specific misclassifications, as well as correct classifications, to assess using
    other interpretation methods. To this end, you ran a comprehensive suite of interpretation
    methods, including activation, gradient, perturbation, and backpropagation-based
    methods. The consensus between all the methods was that the model was having the
    following issues:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，预测性能评估对于推导出特定的误分类以及正确的分类，以评估使用其他解释方法至关重要。为此，你运行了一系列的解释方法，包括激活、梯度、扰动和基于反向传播的方法。所有方法的一致意见是模型存在以下问题：
- en: Differentiating between the background and the objects
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分背景和物体
- en: Understanding that different objects share similar color hues
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解不同物体共享相似的颜色色调
- en: Confounding lighting conditions, such as specular highlights as specific material
    characteristics, like with the wine glasses
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混乱的照明条件，例如像酒杯那样的特定材料特性产生的镜面高光
- en: An inability to separate unique features of each object, such as plastic studs
    in LEGO bricks from battery contacts
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法区分每个物体的独特特征，例如乐高砖块中的塑料凸起与电池接触
- en: Being confused by objects with multiple materials, such as batteries contained
    in plastic and even cardboard packaging
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 被由多种材料组成的物体所困惑，例如塑料包装和纸盒包装中的电池
- en: To address these problems, the model needed to be trained with a more varied
    dataset – hopefully, one that reflects the real-world conditions of the recycling
    plant; for instance, the expected background (on a conveyor belt), different lighting
    conditions, and even objects partially occluded by hands, gloves, bags, and so
    on. Also, they ought to add a category for miscellaneous objects that are made
    up of multiple materials.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，模型需要用更多样化的数据集进行训练——希望是一个反映回收厂真实世界条件的数据集；例如，预期的背景（在输送带上）、不同的照明条件，甚至被手、手套、袋子等部分遮挡的物体。此外，他们应该为由多种材料组成的杂项物体添加一个类别。
- en: 'Once this dataset has been compiled, it is essential to leverage data augmentation
    to make the model even more robust to all sorts of variations: angle, brightness,
    contrast, saturation, and hue variants. And they won’t have to retrain the model
    from scratch! They can even fine-tune EfficientNet!'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这个数据集被编译，利用数据增强使模型对各种变化（角度、亮度、对比度、饱和度和色调变化）更加鲁棒是至关重要的。他们甚至不需要从头开始重新训练模型！他们甚至可以微调
    EfficientNet！
- en: Summary
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: After reading this chapter, you should understand how to leverage traditional
    interpretation methods to more thoroughly assess predictive performance on a CNN
    classifier and visualize the learning process of CNNs with activation-based methods.
    You should also understand how to compare and contrast misclassifications and
    true positives with gradient-based and perturbation-based attribution methods.
    In the next chapter, we will study interpretation methods for NLP transformers.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章后，你应该了解如何利用传统的解释方法来更全面地评估 CNN 分类器的预测性能，并使用基于激活的方法可视化 CNN 的学习过程。你还应该了解如何使用基于梯度和扰动的方法比较和对比误分类和真实正例。在下一章中，我们将研究
    NLP 变换器的解释方法。
- en: Further reading
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Smilkov, D., Thorat, N., Kim, B., Viégas, F., and Wattenberg, M., 2017, *SmoothGrad:
    Removing noise by adding noise*. ArXiv, abs/1706.03825: [https://arxiv.org/abs/1706.03825](https://arxiv.org/abs/1706.03825)'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Smilkov, D., Thorat, N., Kim, B., Viégas, F., and Wattenberg, M., 2017, *SmoothGrad:
    通过添加噪声去除噪声*. ArXiv, abs/1706.03825: [https://arxiv.org/abs/1706.03825](https://arxiv.org/abs/1706.03825)'
- en: 'Sundararajan, M., Taly, A., and Yan, Q., 2017, *Axiomatic Attribution for Deep
    Networks*. Proceedings of Machine Learning Research, pp. 3319–3328, International
    Convention Centre, Sydney, Australia: [https://arxiv.org/abs/1703.01365](https://arxiv.org/abs/1703.01365)'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sundararajan, M., Taly, A., and Yan, Q., 2017, *深度网络的公理化归因*. 机器学习研究会议论文集，第
    3319–3328 页，国际会议中心，悉尼，澳大利亚: [https://arxiv.org/abs/1703.01365](https://arxiv.org/abs/1703.01365)'
- en: 'Zeiler, M.D., and Fergus, R., 2014, *Visualizing and Understanding Convolutional
    Networks*. In European conference on computer vision, pp. 818–833: [https://arxiv.org/abs/1311.2901](https://arxiv.org/abs/1311.2901)'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeiler, M.D., and Fergus, R., 2014, *视觉化和理解卷积网络*. 在欧洲计算机视觉会议，第 818–833 页: [https://arxiv.org/abs/1311.2901](https://arxiv.org/abs/1311.2901)'
- en: 'Shrikumar, A., Greenside, P., and Kundaje, A., 2017, *Learning Important Features
    Through Propagating Activation Differences*: [https://arxiv.org/abs/1704.02685](https://arxiv.org/abs/1704.02685)'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shrikumar, A., Greenside, P., and Kundaje, A., 2017, *通过传播激活差异学习重要特征*: [https://arxiv.org/abs/1704.02685](https://arxiv.org/abs/1704.02685)'
- en: Learn more on Discord
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Discord 上了解更多
- en: 'To join the Discord community for this book – where you can share feedback,
    ask the author questions, and learn about new releases – follow the QR code below:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入本书的 Discord 社区——在那里你可以分享反馈、向作者提问，并了解新版本——请扫描下面的二维码：
- en: '[https://packt.link/inml](Chapter_7.xhtml)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/inml](Chapter_7.xhtml)'
- en: '![](img/QR_Code107161072033138125.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code107161072033138125.png)'
