- en: '*Chapter 9*: Hyperparameter Tuning via Optuna'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*: 通过Optuna进行超参数调优'
- en: '`scikit-optimize`.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-optimize`。'
- en: In this chapter, you’ll be introduced to the `Optuna` package, starting with
    its numerous features, how to utilize it to perform hyperparameter tuning, and
    all of the other important things you need to know about `Optuna`. We’ll not only
    learn how to utilize `Optuna` to perform hyperparameter tuning with their default
    configurations but also discuss the available configurations along with their
    usage. Moreover, we’ll also discuss how the implementation of the hyperparameter
    tuning methods is related to the theory that we have learned in previous chapters,
    since there may be some minor differences or adjustments made in the implementation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解`Optuna`包，从其众多功能开始，学习如何利用它进行超参数调优，以及您需要了解的关于`Optuna`的所有其他重要事项。我们不仅将学习如何利用`Optuna`及其默认配置进行超参数调优，还将讨论可用的配置及其用法。此外，我们还将讨论超参数调优方法的实现与我们在前几章中学到的理论之间的关系，因为实现中可能会有一些细微的差异或调整。
- en: By the end of this chapter, you will be able to understand all of the important
    things you need to know about `Optuna` and implement various hyperparameter tuning
    methods available in this package. You’ll also be able to understand each of the
    important parameters of the classes and how they are related to the theory that
    we have learned in previous chapters. Finally, equipped with the knowledge from
    previous chapters, you will also be able to understand what’s happening if there
    are errors or unexpected results and understand how to set up the method configuration
    to match your specific problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将能够了解关于`Optuna`的所有重要事项，并实现该包中提供的各种超参数调优方法。您还将能够理解每个类的重要参数以及它们与我们之前章节中学到的理论之间的关系。最后，凭借前几章的知识，您还将能够理解如果出现错误或意外结果时会发生什么，并了解如何设置方法配置以匹配您特定的难题。
- en: 'The following are the main topics that will be discussed in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论以下主要主题：
- en: Introducing Optuna
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Optuna
- en: Implementing TPE
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现TPE
- en: Implementing Random Search
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现随机搜索
- en: Implementing Grid Search
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现网格搜索
- en: Implementing Simulated Annealing
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现模拟退火
- en: Implementing Successive Halving
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现逐次减半
- en: Implementing Hyperband
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现Hyperband
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will learn how to implement various hyperparameter tuning methods with `Optuna`.
    To ensure that you are able to reproduce the code examples in this chapter, you
    will require the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习如何使用`Optuna`实现各种超参数调优方法。为确保您能够重现本章中的代码示例，您需要以下条件：
- en: Python 3 (version 3.7 or above)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3（版本3.7或更高）
- en: Installed `pandas` package (version 1.3.4 or above)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装`pandas`包（版本1.3.4或更高）
- en: Installed `NumPy` package (version 1.21.2 or above)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装`NumPy`包（版本1.21.2或更高）
- en: Installed `Matplotlib` package (version 3.5.0 or above)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装`Matplotlib`包（版本3.5.0或更高）
- en: Installed `scikit-learn` package (version 1.0.1 or above)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装`scikit-learn`包（版本1.0.1或更高）
- en: Installed `Tensorflow` package (version 2.4.1 or above)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装`Tensorflow`包（版本2.4.1或更高）
- en: Installed `Optuna` package (version 2.10.0 or above)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装`Optuna`包（版本2.10.0或更高）
- en: All of the code examples for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python](https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码示例都可以在GitHub上找到：[https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python](https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python)。
- en: Introducing Optuna
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Optuna
- en: '`Optuna` is a hyperparameter tuning package in Python that provides several
    hyperparameter tuning methods implementation, such as Grid Search, Random Search,
    Tree-structured Parzen Estimators (TPE), and many more. Unlike `Hyperopt`, which
    assumes we are always working with a minimization problem (see [*Chapter 8*](B18753_08_ePub.xhtml#_idTextAnchor074)*,
    Hyperparameter Tuning via Hyperopt*), we can tell `Optuna` the type of optimization
    problem we are working on: minimization or maximization.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`Optuna`是一个Python超参数调优包，提供了多种超参数调优方法的实现，例如网格搜索、随机搜索、树结构帕累托估计器（TPE）等。与假设我们始终在处理最小化问题（参见[*第8章*](B18753_08_ePub.xhtml#_idTextAnchor074)*，通过Hyperopt进行超参数调优*）的`Hyperopt`不同，我们可以告诉`Optuna`我们正在处理哪种优化问题：最小化或最大化。'
- en: '`Optuna` has two main classes, namely **samplers** and **pruners**. Samplers
    are responsible for performing the hyperparameter tuning optimization, whereas
    pruners are responsible for judging whether we should prune the trials based on
    the reported values. In other words, pruners act like *early stopping methods*
    where we will stop a hyperparameter tuning iteration whenever it seems that there’s
    no additional benefit to continuing the process.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`Optuna`有两个主要类，即**采样器**和**剪枝器**。采样器负责执行超参数调整优化，而剪枝器负责根据报告的值判断是否应该剪枝试验。换句话说，剪枝器就像*早期停止方法*，当我们认为继续过程没有额外好处时，我们将停止超参数调整迭代。'
- en: The built-in implementation for samplers includes several hyperparameter tuning
    methods that we have learned in *Chapters 3 - 4*, namely Grid Search, Random Search,
    and TPE, and also other methods that are outside of the scope of this book, such
    as CMA-ES, NSGA-II, and many more. We can also define our own custom samplers,
    such as the Simulated Annealing (SA), which will be discussed in the upcoming
    section. Furthermore, `Optuna` also allows us to integrate samplers from another
    package, such as from the `scikit-optimize` (`skopt`) package where we can utilize
    many Bayesian optimization-based methods from there.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 内置的采样器实现包括我们在第3章到第4章中学到的几种超参数调整方法，即网格搜索、随机搜索和TPE，以及本书范围之外的其他方法，例如CMA-ES、NSGA-II等。我们还可以定义自己的自定义采样器，例如模拟退火（SA），这将在下一节中讨论。此外，`Optuna`还允许我们集成来自另一个包的采样器，例如来自`scikit-optimize`（`skopt`）包，在那里我们可以利用许多基于贝叶斯优化的方法。
- en: Integrations in Optuna
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna的集成
- en: Besides `skopt`, there are also many other integrations provided by `Optuna,`
    including but not limited, to `scikit-learn`, `Keras`, `PyTorch`, `XGBoost`, `LightGBM`,
    `FastAI`, `MLflow`, and many more. For more information about the available integrations,
    please see the official documentation ([https://optuna.readthedocs.io/en/v2.10.0/reference/integration.html](https://optuna.readthedocs.io/en/v2.10.0/reference/integration.html)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`skopt`之外，`Optuna`还提供了许多其他集成，包括但不限于`scikit-learn`、`Keras`、`PyTorch`、`XGBoost`、`LightGBM`、`FastAI`、`MLflow`等。有关可用集成的更多信息，请参阅官方文档([https://optuna.readthedocs.io/en/v2.10.0/reference/integration.html](https://optuna.readthedocs.io/en/v2.10.0/reference/integration.html))。
- en: As for pruners, `Optuna` provides both statistics-based and multi-fidelity optimization
    (MFO)-based methods. There are `MedianPruner`, `PercentilePruner`, and `ThresholdPruner`
    for the statistics-based group. `MedianPruner` will prune the trials whenever
    the current trial’s best intermediate result is worse compared to the median of
    the result of the previous trial. `PercentilePruner` will perform pruning when
    the current best intermediate value is part of the bottom percentile from previous
    trials. `ThresholdPruner` will simply perform pruning whenever the predefined
    threshold is met. The MFO-based pruners implemented in `Optuna` are `SuccessiveHalvingPruner`
    and `HyperbandPruner`. Both of them *define the resource as the number of training
    steps or epochs*, not as the number of samples such as in the implementations
    of `scikit-learn`. We will learn how to utilize these MFO-based pruners in the
    upcoming sections.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于剪枝器，`Optuna`提供了基于统计和基于多保真优化（MFO）的方法。对于基于统计的组，有`MedianPruner`、`PercentilePruner`和`ThresholdPruner`。`MedianPruner`将在当前试验的最佳中间结果比前一个试验的结果中位数更差时剪枝。`PercentilePruner`将在当前最佳中间值是前一个试验的底部百分位数之一时进行剪枝。`ThresholdPruner`将简单地在任何预定义的阈值满足时进行剪枝。`Optuna`中实现的基于MFO的剪枝器是`SuccessiveHalvingPruner`和`HyperbandPruner`。两者都将资源定义为训练步骤或epoch的数量，而不是样本数量，如`scikit-learn`的实现。我们将在下一节中学习如何利用这些基于MFO的剪枝器。
- en: 'To perform hyperparameter tuning with `Optuna`, we can simply perform the following
    simple steps (more detailed steps, including the code implementation, will be
    given through various examples in the upcoming sections):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`Optuna`执行超参数调整，我们可以简单地执行以下简单步骤（更详细的步骤，包括代码实现，将在下一节中的各种示例中给出）：
- en: Define the `objective` function along with the hyperparameter space.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义目标函数以及超参数空间。
- en: Initiate a `study` object via the `create_study()` function.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`create_study()`函数初始化`study`对象。
- en: Perform hyperparameter tuning by calling the `optimize()` method on the `study`
    object.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在`study`对象上调用`optimize()`方法执行超参数调整。
- en: Train the model on full training data using the best set of hyperparameters
    found.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用找到的最佳超参数集在全部训练数据上训练模型。
- en: Test the final trained model on the test data.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测试最终训练好的模型。
- en: In `Optuna`, we can directly define the hyperparameter space within the `objective`
    function itself. There’s no need to define another dedicated separate object just
    to store the hyperparameter space. This means that implementing conditional hyperparameters
    in `Optuna` becomes very easy since we just need to put them within the corresponding
    `if-else` blocks in the `objective` function. `Optuna` also provides very handy
    hyperparameter sampling distribution methods including `suggest_categorical`,
    `suggest_discrete_uniform`, `suggest_int`, and `suggest_float`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Optuna`中，我们可以在`objective`函数本身内直接定义超参数空间。无需定义另一个专门的独立对象来存储超参数空间。这意味着在`Optuna`中实现条件超参数变得非常容易，因为我们只需将它们放在`objective`函数中的相应`if-else`块内。`Optuna`还提供了非常实用的超参数采样分布方法，包括`suggest_categorical`、`suggest_discrete_uniform`、`suggest_int`和`suggest_float`。
- en: The `suggest_categorical` method will suggest value from a categorical type
    of hyperparameters, which works similarly with the `random.choice()` method. The
    `suggest_discrete_uniform` can be utilized for a discrete type of hyperparameters,
    which works very similar to the `hp.quniform` in Hyperopt (see [*Chapter 8*](B18753_08_ePub.xhtml#_idTextAnchor074)*,
    Hyperparameter Tuning via Hyperopt*) by sampling uniformly from the range of `[low,
    high]` with a `q` step of discretization. The `suggest_int` method works similarly
    to the `random.randint()` method. Finally, the `suggest_float` method. This method
    works for a floating type of hyperparameters and is actually a wrapper of two
    other sampling distribution methods, namely the `suggest_uniform` and `suggest_loguniform`.
    To utilize `suggest_loguniform`, simply set the `log` parameter in `suggest_float`
    as `True`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`suggest_categorical`方法将建议从分类类型的超参数中获取值，这与`random.choice()`方法的工作方式类似。`suggest_discrete_uniform`可用于离散类型的超参数，其工作方式与Hyperopt中的`hp.quniform`非常相似（参见[*第8章*](B18753_08_ePub.xhtml#_idTextAnchor074)中通过Hyperopt进行超参数调整），通过从`[low,
    high]`范围内以`q`步长进行离散化均匀采样。`suggest_int`方法与`random.randint()`方法类似。最后是`suggest_float`方法。此方法适用于浮点类型的超参数，实际上是两个其他采样分布方法的包装，即`suggest_uniform`和`suggest_loguniform`。要使用`suggest_loguniform`，只需将`suggest_float`中的`log`参数设置为`True`。'
- en: 'To have a better understanding of how we can define the hyperparameter space
    within the `objective` function, the following code shows an example of how to
    define an `objective` function using `objective` function, to ensure readability
    and to enable us to write the code in a modular fashion. However, you can also
    put all of the code within one single `objective` function directly. The data
    and preprocessing steps used in this example are the same as in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062)*,
    Hyperparameter Tuning via Scikit*. However, in this example, we are using a **neural
    network** model instead of a random forest as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解我们如何在`objective`函数内定义超参数空间，以下代码展示了如何使用`objective`函数定义一个`objective`函数的示例，以确保可读性并使我们能够以模块化方式编写代码。然而，您也可以直接将所有代码放在一个单独的`objective`函数中。本例中使用的数据和预处理步骤与[*第7章*](B18753_07_ePub.xhtml#_idTextAnchor062)中相同，即通过Scikit进行超参数调整。然而，在本例中，我们使用的是**神经网络**模型而不是随机森林，如下所示：
- en: 'Create a function to define the model architecture. Here, we create a binary
    classifier model where the number of hidden layers, number of units, dropout rate,
    and the `activation` function for each layer are part of the hyperparameter space,
    as follows:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数来定义模型架构。在这里，我们创建了一个二元分类器模型，其中隐藏层的数量、单元数量、dropout率和每层的`activation`函数都是超参数空间的一部分，如下所示：
- en: '[PRE0]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create a function to define the model’s optimizer. Notice that we define conditional
    hyperparameters in this function where we have a different set of hyperparameters
    for a different chosen optimizer as follows:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数来定义模型的优化器。请注意，我们在该函数中定义了条件超参数，其中针对不同选择的优化器有不同的超参数集，如下所示：
- en: '[PRE1]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create the `train` and `validation` functions. Note that the preprocessing
    code is not shown here, but you can see it in the GitHub repo mentioned in the
    *Technical requirements* section for the full code. As the case with the examples
    in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062), we are also using F1-score
    as the evaluation metric of the model as follows:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`train`和`validation`函数。请注意，预处理代码在此处未显示，但您可以在*技术要求*部分提到的GitHub仓库中看到完整的代码。与[*第7章*](B18753_07_ePub.xhtml#_idTextAnchor062)中的示例一样，我们也将F1分数作为模型的评估指标，如下所示：
- en: '[PRE2]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Create the `objective` function. Here, we split the original training data into
    training data for hyperparameter tuning, `df_train_hp`, and validation data, `df_val`.
    We won’t follow the k-fold cross-validation evaluation method since it will take
    too much time for the neural network model to go through several folds of evaluation
    within each tuning trial (see [*Chapter 1*](B18753_01_ePub.xhtml#_idTextAnchor014)*,Evaluating
    Machine Learning Models*).
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`objective`函数。在这里，我们将原始训练数据分为用于超参数调整的训练数据`df_train_hp`和验证数据`df_val`。我们不会遵循k折交叉验证评估方法，因为这会在每个调整试验中花费太多时间让神经网络模型通过几个评估折（参见[*第1章*](B18753_01_ePub.xhtml#_idTextAnchor014)*，评估机器学习模型*）。
- en: '[PRE3]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To perform hyperparameter tuning in `Optuna`, we need to initiate a `study`
    object via the `create_study()` function. The `study` object provides interfaces
    to run a new `Trial` object and access the trials’ history. The `Trial` object
    is simply an object that involves the process of evaluating an `objective` function.
    This object will be passed to the `objective` function and it is responsible for
    managing the trial’s state, providing interfaces upon receiving the parameter
    suggestion just as we saw earlier in the `objective` function. The following code
    shows how to utilize the `create_study()` function to initiate a `study` object:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要在`Optuna`中执行超参数调整，我们需要通过`create_study()`函数初始化一个`study`对象。`study`对象提供了运行新的`Trial`对象和访问试验历史的接口。`Trial`对象简单地说是一个涉及评估`objective`函数过程的对象。此对象将被传递给`objective`函数，并负责管理试验的状态，在接收到参数建议时提供接口，就像我们在`objective`函数中之前看到的那样。以下代码展示了如何利用`create_study()`函数来初始化一个`study`对象：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: There are several important parameters in the `create_study()` function. The
    `direction` parameter allows us to tell `Optuna` what kind of optimization problem
    we are working on. There are two valid values for this parameter, namely *‘maximize’*
    and *‘minimize’*. By setting the `direction` parameter equal to *‘maximize’*,
    it means that we tell `Optuna` that we are currently working on a maximization
    problem. `Optuna` sets this parameter to *‘minimize’* by default. The `sampler`
    parameter refers to the hyperparameter tuning algorithm that we want to use. By
    default, `Optuna` will use TPE as the sampler. The `pruner` parameter refers to
    the pruning algorithm that we want to use, where `MedianPruner()` is used by default.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在`create_study()`函数中，有几个重要的参数。`direction`参数允许我们告诉`Optuna`我们正在处理哪种优化问题。此参数有两个有效值，即*‘maximize’*和*‘minimize’*。通过将`direction`参数设置为*‘maximize’*，这意味着我们告诉`Optuna`我们目前正在处理一个最大化问题。`Optuna`默认将此参数设置为*‘minimize’*。`sampler`参数指的是我们想要使用的超参数调整算法。默认情况下，`Optuna`将使用TPE作为采样器。`pruner`参数指的是我们想要使用的修剪算法，其中默认使用`MedianPruner()`。
- en: Pruning in Optuna
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna中的修剪
- en: 'Although `MedianPruner()` is chosen by default, the pruning process will not
    be performed unless we explicitly tell `Optuna` to do so within the `objective`
    function. This example shows how to perform a simple pruning procedure with the
    default pruner in `Optuna` at the following link: [https://github.com/optuna/optuna-examples/blob/main/simple_pruning.py](https://github.com/optuna/optuna-examples/blob/main/simple_pruning.py).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`MedianPruner()`默认被选中，但除非我们明确在`objective`函数中告诉`Optuna`这样做，否则修剪过程将不会执行。以下链接展示了如何使用`Optuna`的默认修剪器执行简单的修剪过程：[https://github.com/optuna/optuna-examples/blob/main/simple_pruning.py](https://github.com/optuna/optuna-examples/blob/main/simple_pruning.py)。
- en: Besides the three preceding parameters, there are also other parameters in the
    `create_study()` function, namely `storage`, `study_name`, and `load_if_exists`.
    The `storage` parameter expects a database URL input, which will be handled with
    `Optuna`. If we do not pass a database URL, `Optuna` will use the in-memory storage
    instead. The `study_name` parameter is simply the name that we want to give to
    the current `study` object. If we do not pass a name, `Optuna` will automatically
    generate a random name for us. Last but not least, the `load_if_exists` parameter
    is a Boolean parameter that handles cases when there might be conflicting study
    names. If the study name is already generated in the storage, and we set `load_if_exists=False`,
    then `Optuna` will raise an error. On the other hand, if the study name is already
    generated in the storage, but we set `load_if_exists=True`, `Optuna` will just
    load the existing `study` object instead of creating a new one.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面提到的三个参数之外，`create_study()`函数中还有其他参数，即`storage`、`study_name`和`load_if_exists`。`storage`参数期望一个数据库URL输入，它将由`Optuna`处理。如果我们没有传递数据库URL，`Optuna`将使用内存存储。`study_name`参数是我们想要赋予当前`study`对象的名称。如果我们没有传递名称，`Optuna`将自动为我们生成一个随机名称。最后但同样重要的是，`load_if_exists`参数是一个布尔参数，用于处理可能存在冲突的实验名称的情况。如果存储中已经生成了实验名称，并且我们将`load_if_exists`设置为`False`，那么`Optuna`将引发错误。另一方面，如果存储中已经生成了实验名称，但我们设置了`load_if_exists=True`，`Optuna`将只加载现有的`study`对象而不是创建一个新的对象。
- en: 'Once the `study` object is initiated along with the appropriate parameters,
    we can start performing the hyperparameter tuning by calling the `optimize()`
    method. The following code shows you how to do that:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦初始化了`study`对象并设置了适当的参数，我们就可以通过调用`optimize()`方法开始执行超参数调优。以下代码展示了如何进行操作：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: There are several important parameters in the `optimize()` method. The first
    and most important method is the `func` parameter. This parameter expects a callable
    that implements the `objective` function. Here, we don’t directly pass the `objective`
    function to the `func` parameter since our `objective` function expects two inputs,
    while by default, `Optuna` can only handle an `objective` function with one input,
    which is the `Trial` object itself. That’s why we need the help of Python’s built-in
    `lambda` function to pass the second input to our `objective` function. You can
    also utilize the same `lambda` function if your `objective` function has more
    than two inputs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在`optimize()`方法中存在几个重要的参数。第一个也是最重要的参数是`func`参数。这个参数期望一个可调用的对象，该对象实现了`objective`函数。在这里，我们并没有直接将`objective`函数传递给`func`参数，因为我们的`objective`函数需要两个输入，而默认情况下，`Optuna`只能处理一个输入的`objective`函数，即`Trial`对象本身。这就是为什么我们需要Python内置的`lambda`函数来将第二个输入传递给我们的`objective`函数。如果你的`objective`函数有超过两个输入，你也可以使用相同的`lambda`函数。
- en: The second most important parameter is `n_trials`, which refers to the number
    of trials or iterations for the hyperparameter tuning process. Another implemented
    parameter that can be used as the stopping criteria is the `timeout` parameter.
    This parameter expects the stopping criteria in the unit of seconds. By default,
    `Optuna` sets the `n_trials` and `timeout` parameters to `None`. If we leave it
    be, then `Optuna` will run the hyperparameter tuning process until it receives
    a termination signal, such as `Ctrl+C` or `SIGTERM`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个最重要的参数是`n_trials`，它指的是超参数调优过程中的试验次数或迭代次数。另一个可以作为停止标准的实现参数是`timeout`参数。这个参数期望以秒为单位的停止标准。默认情况下，`Optuna`将`n_trials`和`timeout`参数设置为`None`。如果我们让它保持原样，那么`Optuna`将运行超参数调优过程，直到接收到终止信号，例如`Ctrl+C`或`SIGTERM`。
- en: Last but not least, `Optuna` also allows us to utilize the parallel resources
    through a parameter called `n_jobs`. By default, Optuna will set `n_jobs=1`, meaning
    that it will only utilize one job. Here, we set `n_jobs=-1`, meaning that we will
    use all of the CPU counts in our computer to perform parallel computation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，`Optuna`还允许我们通过一个名为`n_jobs`的参数来利用并行资源。默认情况下，`Optuna`将`n_jobs`设置为`1`，这意味着它将只利用一个工作。在这里，我们将`n_jobs`设置为`-1`，这意味着我们将使用计算机上的所有CPU核心来执行并行计算。
- en: Hyperparameter’s Importance in Optuna
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna中超参数的重要性
- en: '`Optuna` provides a very nice module to measure the importance of each hyperparameter
    in the search space. As per version 2.10.0, there are two methods implemented,
    namely the **fANOVA** and **Mean Decrease Impurity** methods. Please see the official
    documentation on how to utilize this module and the theory behind the implemented
    methods, available at the following link: [https://optuna.readthedocs.io/en/v2.10.0/reference/importance.html](https://optuna.readthedocs.io/en/v2.10.0/reference/importance.html).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`Optuna`提供了一个非常棒的模块来衡量搜索空间中每个超参数的重要性。根据2.10.0版本，实现了两种方法，即**fANOVA**和**Mean
    Decrease Impurity**方法。请参阅官方文档了解如何利用此模块以及实现方法的背后理论，文档链接如下：[https://optuna.readthedocs.io/en/v2.10.0/reference/importance.html](https://optuna.readthedocs.io/en/v2.10.0/reference/importance.html)。'
- en: In this section, we learned what `Optuna` is in general, the available features
    that we can utilize, and the general steps as to how to perform hyperparameter
    tuning with this package. `Optuna` also has various visualization modules that
    can help us track our hyperparameter tuning experiments, which will be discussed
    in [*Chapter 13*](B18753_13_ePub.xhtml#_idTextAnchor125), *Tracking Hyperparameter
    Tuning Experiments*. In the upcoming sections, we will learn how to perform various
    hyperparameter tuning methods with `Optuna` through examples.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解了`Optuna`的一般概念，我们可以利用的功能，以及如何使用此包进行超参数调整的一般步骤。`Optuna`还提供了各种可视化模块，可以帮助我们跟踪我们的超参数调整实验，这将在[*第13章*](B18753_13_ePub.xhtml#_idTextAnchor125)中讨论，*跟踪超参数调整实验*。在接下来的章节中，我们将通过示例学习如何使用`Optuna`执行各种超参数调整方法。
- en: Implementing TPE
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现TPE
- en: 'TPE is one of the variants of the Bayesian optimization hyperparameter tuning
    group (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)), which is the
    default sampler in `Optuna`. To perform hyperparameter tuning with TPE in `Optuna`,
    we can just simply pass the `optuna.samplers.TPESampler()` class to the sampler
    parameter of the `create_study()` function. The following example shows how to
    implement TPE in `Optuna`. We’ll use the same data as in the examples in [*Chapter
    7*](B18753_07_ePub.xhtml#_idTextAnchor062) and follow the steps introduced in
    the preceding section as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: TPE是贝叶斯优化超参数调整组（见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)）的一种变体，是`Optuna`中的默认采样器。要在`Optuna`中使用TPE进行超参数调整，我们只需将`optuna.samplers.TPESampler()`类传递给`create_study()`函数的采样器参数。以下示例展示了如何在`Optuna`中实现TPE。我们将使用[*第7章*](B18753_07_ePub.xhtml#_idTextAnchor062)中示例中的相同数据，并按照前节中介绍的步骤进行如下操作：
- en: Define the `objective` function along with the hyperparameter space. Here, we’ll
    use the same function that we defined in the *Introducing Optuna* section. Remember
    that we use the train-validation split instead of the k-fold cross-validation
    method within the `objective` function.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`objective`函数以及超参数空间。在这里，我们将使用与*介绍Optuna*部分中定义的相同的函数。请记住，我们在`objective`函数中使用的是训练-验证分割，而不是k折交叉验证方法。
- en: 'Initiate a `study` object via the `create_study()` function as follows:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`create_study()`函数初始化`study`对象，如下所示：
- en: '[PRE7]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Perform hyperparameter tuning by calling the `optimize()` method on the `study`
    object as follows:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在`study`对象上调用`optimize()`方法来执行超参数调整，如下所示：
- en: '[PRE8]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Based on the preceding code, we get around `0.563` of F1-score evaluated in
    the validation data. We also get a dictionary consisting of the best set of hyperparameters
    as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，我们在验证数据上得到了大约`0.563`的F1分数。我们还得到了一个包含最佳超参数集的字典，如下所示：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Train the model on full training data using the best set of hyperparameters
    found. Here, we define another function called `train_and_evaluate_final()` that
    has the purpose of training the model in the full training data based on the best
    set of hyperparameters found in the preceding step, as well as evaluating it on
    the test data. You can see the implemented function in the GitHub repo mentioned
    in the *Technical requirements* section. Define the function as follows:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用找到的最佳超参数集在全部训练数据上训练模型。在这里，我们定义了一个名为`train_and_evaluate_final()`的另一个函数，其目的是基于前一步找到的最佳超参数集在全部训练数据上训练模型，并在测试数据上对其进行评估。您可以在*技术要求*部分提到的GitHub仓库中看到实现的函数。定义函数如下：
- en: '[PRE10]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Test the final trained model on the test data. Based on the results from the
    preceding step, we get around `0.604` in F1-score when testing our final trained
    neural network model with the best set of hyperparameters on the test set.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测试最终训练好的模型。根据前一步的结果，当使用最佳超参数集在测试集上测试我们最终训练的神经网络模型时，F1 分数大约为 `0.604`。
- en: 'There are several important parameters for the `TPESampler` class. First, there
    is the `gamma` parameter, which refers to the threshold used in TPE to divide
    good and bad samples (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)).The
    `n_startup_trials` parameter is responsible for controlling how many trials will
    utilize Random Search before starting to perform the TPE algorithm. The `n_ei_candidates`
    parameter is responsible for controlling how many candidate samples are used to
    calculate the `expected improvement acquisition` function. Last but not least,
    the `seed` parameter, which controls the random seed of the experiment. There
    are many other parameters available for the `TPESampler` class, so please see
    the original documentation for more information, available at the following link:
    [https://optuna.readthedocs.io/en/v2.10.0/reference/generated/optuna.samplers.TPESampler.html](https://optuna.readthedocs.io/en/v2.10.0/reference/generated/optuna.samplers.TPESampler.html).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`TPESampler`类有几个重要的参数。首先，是`gamma`参数，它指的是TPE中用于区分好样本和坏样本的阈值（参见[*第 4 章*](B18753_04_ePub.xhtml#_idTextAnchor036)）。`n_startup_trials`参数负责控制在进行TPE算法之前，将有多少次试验使用随机搜索。`n_ei_candidates`参数负责控制用于计算`预期改进获取函数`的候选样本数量。最后但同样重要的是，`seed`参数，它控制实验的随机种子。`TPESampler`类还有许多其他参数，请参阅以下链接的原版文档获取更多信息：[https://optuna.readthedocs.io/en/v2.10.0/reference/generated/optuna.samplers.TPESampler.html](https://optuna.readthedocs.io/en/v2.10.0/reference/generated/optuna.samplers.TPESampler.html)。'
- en: In this section, we have learned how to perform hyperparameter tuning with TPE
    in `Optuna` using the same data as in the example in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062).
    As mentioned in [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*, Exploring
    Bayesian Optimization* `Optuna` also implements the multivariate TPE, which is
    able to capture the interdependencies among hyperparameters. To enable the multivariate
    TPE, we can just simply set the `multivariate` parameter in `optuna.samplers.TPESampler()`
    as `True`. In the next section, we will learn how to perform Random Search with
    `Optuna`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何在`Optuna`中使用与[*第 7 章*](B18753_07_ePub.xhtml#_idTextAnchor062)示例中相同的数据执行超参数调优。如[*第
    4 章*](B18753_04_ePub.xhtml#_idTextAnchor036)中所述，探索贝叶斯优化，`Optuna`也实现了多变量TPE，能够捕捉超参数之间的相互依赖关系。要启用多变量TPE，我们只需将`optuna.samplers.TPESampler()`中的`multivariate`参数设置为`True`。在下一节中，我们将学习如何使用`Optuna`进行随机搜索。
- en: Implementing Random Search
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现随机搜索
- en: 'Implementing Random Search in `Optuna` is very similar to implementing TPE
    in `Optuna`. We can just follow a similar procedure to the preceding section and
    change the `sampler` parameter in the `optimize()` method in *step 2*. The following
    code shows you how to do that:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Optuna`中实现随机搜索与实现TPE（Tree-based Parzen Estimator）在`Optuna`中非常相似。我们只需遵循前一个章节的类似步骤，并在*步骤
    2*中更改`optimize()`方法中的`sampler`参数。以下代码展示了如何进行操作：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Using the exact same data, preprocessing steps, hyperparameter space, and `objective`
    function, we get around `0.548` in the F1-score evaluated in the validation data.
    We also get a dictionary consisting of the best set of hyperparameters as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用完全相同的数据、预处理步骤、超参数空间和`objective`函数，我们在验证数据中评估的F1分数大约为 `0.548`。我们还得到了一个包含最佳超参数集的字典，如下所示：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: After the model is trained with full data using the best set of hyperparameters,
    we get around `0.596` in F1-score when we test the final neural network model
    trained on the test data. Notice that although we have defined many hyperparameters
    earlier, (see the `objective` function in the preceding section), here, we do
    not get all of them in the results. This is because most of the hyperparameters
    are conditional hyperparameters. For example, since the chosen value for the *’num_layers’*
    hyperparameter is zero, there will be no *’n_units_layer_{layer_i}’*, *’dropout_rate_layer_{layer_i}’*,
    or *‘actv_func _layer_{layer_i}’* since those hyperparameters will only exist
    when the *’num_layers’* hyperparameter is greater than zero.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最佳超参数集在完整数据上训练模型后，我们在测试数据上训练的最终神经网络模型测试时，F1分数大约为`0.596`。请注意，尽管我们之前定义了许多超参数（参见前一小节中的`objective`函数），但在这里，我们并没有在结果中得到所有这些超参数。这是因为大多数超参数都是条件超参数。例如，由于为`*’num_layers’*`超参数选择的值是零，因此将不存在`*’n_units_layer_{layer_i}’*`、`*’dropout_rate_layer_{layer_i}’*`或`*‘actv_func
    _layer_{layer_i}’*`，因为这些超参数只有在`*’num_layers’*超参数大于零时才会存在。
- en: In this section, we have seen how to perform hyperparameter tuning using the
    Random Search method with `Optuna`. In the next section, we will learn how to
    implement Grid Search with the `Optuna` package.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了如何使用`Optuna`的随机搜索方法进行超参数调整。在下一节中，我们将学习如何使用`Optuna`包实现网格搜索。
- en: Implementing Grid Search
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现网格搜索
- en: Implementing Grid Search in `Optuna` is a bit different from implementing TPE
    and Random Search. Here, we need to also define the search space object and pass
    it to `optuna.samplers.GridSampler()`. The search space object is just a Python
    dictionary data structure consisting of hyperparameters’ names as the keys and
    the possible values of the corresponding hyperparameter as the dictionary’s values.
    `GridSampler` will stop the hyperparameter tuning process if all of the combinations
    in the search space have already been evaluated, even though the number of trials,
    `n_trials`, passed to the `optimize()` method has not been reached yet. Furthermore,
    `GridSampler` will only get the value stated in the search space no matter the
    range we pass to the sampling distribution methods, such as `suggest_categorical`,
    `suggest_discrete_uniform`, `suggest_int`, and `suggest_float`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Optuna`中实现网格搜索与实现TPE和随机搜索略有不同。在这里，我们还需要定义搜索空间对象并将其传递给`optuna.samplers.GridSampler()`。搜索空间对象只是一个Python字典数据结构，其键是超参数的名称，而字典的值是对应超参数的可能值。如果搜索空间中的所有组合都已评估，即使传递给`optimize()`方法的`n_trials`数量尚未达到，`GridSampler`也会停止超参数调整过程。此外，无论我们传递给采样分布方法（如`suggest_categorical`、`suggest_discrete_uniform`、`suggest_int`和`suggest_float`）的范围如何，`GridSampler`都只会获取搜索空间中声明的值。
- en: 'The following code shows how to perform Grid Search in `Optuna`. The overall
    procedure to implement Grid Search in `Optuna` is similar to the procedure stated
    in the *Implementing Tree-structured Parzen Estimators* section. The only difference
    is that we have to define the search space and change the `sampler` parameter
    to `optuna.samplers.GridSampler()` in the `optimize()` method in *step 2* as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何在`Optuna`中执行网格搜索。在`Optuna`中实现网格搜索的总体步骤与*实现树结构帕累托估计器*一节中所述的步骤相似。唯一的区别是我们必须定义搜索空间对象，并在*步骤2*中的`optimize()`方法中将`sampler`参数更改为`optuna.samplers.GridSampler()`，如下所示：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Based on the preceding code, we get around `0.574` of the F1-score evaluated
    in the validation data. We also get a dictionary consisting of the best set of
    hyperparameters as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，我们在验证数据上评估的F1分数大约为`0.574`。我们还得到了一个包含最佳超参数集的字典，如下所示：
- en: '[PRE26]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: After the model is trained on full data using the best set of hyperparameters,
    we get around `0.610` in F1-score when we test the final neural network model
    trained on the test data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最佳超参数集在完整数据上训练模型后，我们在测试数据上训练的最终神经网络模型测试时，F1分数大约为`0.610`。
- en: It is worth noting that `GridSampler` will rely on the search space to perform
    the hyperparameter sampling. For example, in the search space, we only define
    the valid values for `num_layers` as `[0,1]`. So, although within the `objective`
    function we set `trial.suggest_int(‘num_layers’,low=0,high=3)` (see the *Introducing
    Optuna* section), only `0` and `1` will be tested during the tuning process. Remember
    that, in `Optuna`, we can specify the stopping criterion through the `n_trials`
    or `timeout` parameters. If we specify either one of those criteria, `GridSampler`
    will not test all of the possible combinations in the search space; the tuning
    process will stop once the stopping criterion is met. In this example, we set
    `n_trials=50`, just like the example in the preceding section.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，`GridSampler`将依赖于搜索空间来执行超参数采样。例如，在搜索空间中，我们只定义了`num_layers`的有效值为`[0,1]`。因此，尽管在`objective`函数中我们设置了`trial.suggest_int(‘num_layers’,low=0,high=3)`（参见*介绍Optuna*部分），但在调整过程中只会测试`0`和`1`。记住，在`Optuna`中，我们可以通过`n_trials`或`timeout`参数指定停止标准。如果我们指定了这些标准之一，`GridSampler`将不会测试搜索空间中的所有可能组合；一旦满足停止标准，调整过程将停止。在这个例子中，我们设置了`n_trials=50`，就像前一个示例部分中那样。
- en: In this section, we have learned how to perform hyperparameter tuning using
    the Grid Search method with `Optuna`. In the next section, we will learn how to
    implement SA with the `Optuna` package.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用`Optuna`的网格搜索方法进行超参数调整。在下一节中，我们将学习如何使用`Optuna`包实现模拟退火（SA）。
- en: Implementing Simulated Annealing
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现模拟退火
- en: SA is not part of the built-in implementation of the hyperparameter tuning method
    in `Optuna`. However, as mentioned in the first section of this chapter, we can
    define our own custom sampler in `Optuna`. When creating a custom sampler, we
    need to create a class that inherits from the `BaseSampler` class. The most important
    method that we need to define within our custom class is the `sample_relative()`
    method. This method is responsible for sampling the corresponding hyperparameters
    from the search space based on the hyperparameter tuning algorithm we chose.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: SA不是`Optuna`内置的超参数调整方法的一部分。然而，正如本章第一部分所述，我们可以在`Optuna`中定义自己的自定义采样器。在创建自定义采样器时，我们需要创建一个继承自`BaseSampler`类的类。在我们自定义类中需要定义的最重要方法是`sample_relative()`方法。此方法负责根据我们选择的超参数调整算法从搜索空间中采样相应的超参数。
- en: 'The complete custom `SimulatedAnnealingSampler()` class with geometric cooling
    annealing schedule (see [*Chapter 5*](B18753_05_ePub.xhtml#_idTextAnchor047))
    has been defined and can be seen in the GitHub repo mentioned in the *Technical
    requirements* section. The following code shows only the implementation of the
    `sample_relative()` method within the class:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的自定义`SimulatedAnnealingSampler()`类，包括几何退火调度计划（参见[*第5章*](B18753_05_ePub.xhtml#_idTextAnchor047)），已在*技术要求*部分中提到的GitHub仓库中定义，并可以查看。以下代码仅展示了类中`sample_relative()`方法的实现：
- en: '[PRE27]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The following code shows how to perform hyperparameter tuning with SA in `Optuna`.
    The overall procedure to implement SA in `Optuna` is similar to the procedure
    stated in the *Implementing Tree-structured Parzen Estimators* section. The only
    difference is that we have to change the `sampler` parameter to `SimulatedAnnealingSampler()`
    in the `optimize()` method in *step 2* as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何在`Optuna`中使用SA进行超参数调整。在`Optuna`中实现SA的整体过程与*实现树结构帕累托估计器*部分中所述的过程类似。唯一的区别是我们必须在*步骤2*的`optimize()`方法中将`sampler`参数更改为`SimulatedAnnealingSampler()`，如下所示：
- en: '[PRE41]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Using the exact same data, preprocessing steps, hyperparameter space, and `objective`
    function, we get around `0.556` of the F1-score evaluated in the validation data.
    We also get a dictionary consisting of the best set of hyperparameters as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用完全相同的数据、预处理步骤、超参数空间和`objective`函数，我们在验证数据中得到的F1分数大约为`0.556`。我们还得到了一个包含最佳超参数集的字典，如下所示：
- en: '[PRE44]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: After the model is trained on full data using the best set of hyperparameters,
    we get around `0.559` in F1-score when we test the final neural network model
    trained on the test data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用最佳超参数集在全部数据上训练模型后，当我们测试在测试数据上训练的最终神经网络模型时，F1分数大约为`0.559`。
- en: In this section, we have learned how to perform hyperparameter tuning using
    the SA algorithm with `Optuna`. In the next section, we will learn how to utilize
    Successive Halving as a pruning method in `Optuna`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用`Optuna`的SA算法进行超参数调整。在下一节中，我们将学习如何在`Optuna`中利用逐次减半作为剪枝方法。
- en: Implementing Successive Halving
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Successive Halving
- en: '`Optuna`, meaning that it is responsible for stopping hyperparameter tuning
    iterations whenever it seems that there’s no additional benefit to continuing
    the process. Since it is implemented as a pruner, the resource definition of SH
    (see [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054)) in `Optuna` refers
    to the number of training steps or epochs of the model, instead of the number
    of samples, as it does in `scikit-learn`’s implementation.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`Optuna`意味着它负责在似乎没有继续进行过程的好处时停止超参数调整迭代。由于它被实现为剪枝器，`Optuna`中SH（Successive Halving）的资源定义（见[*第6章*](B18753_06_ePub.xhtml#_idTextAnchor054)）指的是模型的训练步数或epoch数，而不是样本数，正如`scikit-learn`实现中那样。'
- en: 'We can utilize SH as a pruner along with any sampler that we use. This example
    shows you how to perform hyperparameter tuning with the Random Search algorithm
    as the sampler and SH as the pruner. The overall procedure is similar to the procedure
    stated in the *Implementing TPE* section. Since we are utilizing SH as a pruner,
    we have to edit our `objective` function so that it will utilize the pruner during
    the optimization process. In this example, we can use the callback integration
    with TFKeras provided by `Optuna` via `optuna.integration.TFKerasPruningCallback`.
    We simply need to pass this class to the `callbacks` parameter when fitting the
    model within the `train` function as shown in the following code:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用SH（Successive Halving）作为剪枝器，同时使用我们使用的任何采样器。本例展示了如何使用随机搜索算法作为采样器，SH作为剪枝器来执行超参数调整。整体流程与*实现TPE*部分中所述的流程类似。由于我们使用SH作为剪枝器，我们必须编辑我们的`objective`函数，以便在优化过程中使用剪枝器。在本例中，我们可以使用`Optuna`提供的`TFKeras`的回调集成，通过`optuna.integration.TFKerasPruningCallback`。我们只需在`train`函数中拟合模型时将此类传递给`callbacks`参数，如下面的代码所示：
- en: '[PRE45]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Once we have told `Optuna` to utilize the pruner, we also need to set the `pruner`
    parameter in the `optimize()` method to `optuna.pruners.SuccessiveHalvingPruner()`
    in *step 2* of the *Implementing Tree-structured Parzen Estimators* sectionas
    follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们告诉`Optuna`使用剪枝器，我们还需要在*实现树结构Parzen估计器*部分的*步骤2*中将`optimize()`方法中的`pruner`参数设置为`optuna.pruners.SuccessiveHalvingPruner()`，如下所示：
- en: '[PRE54]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'In this example, we also increased the number of trials from `50` to `100`
    since most of the trials will be pruned anyway as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们也增加了试验次数从`50`到`100`，因为大多数试验无论如何都会被剪枝，如下所示：
- en: '[PRE58]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Using the exact same data, preprocessing steps, and hyperparameter space, we
    get around `0.582` of the F1-score evaluated in the validation data. Out of `100`
    trials performed, there are `87` trials pruned by SH, which implies only `13`
    completed trials. We also get a dictionary consisting of the best set of hyperparameters
    as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用完全相同的数据、预处理步骤和超参数空间，我们在验证数据中得到的F1分数大约是`0.582`。在`100`次试验中，有`87`次试验被SH剪枝，这意味着只有`13`次试验完成。我们还得到了一个包含最佳超参数集的字典，如下所示：
- en: '[PRE61]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: After the model is trained on full data using the best set of hyperparameters,
    we get around `0.597` in F1-score when we test the final neural network model
    trained on the test data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用最佳超参数集在全部数据上训练模型之后，我们在测试数据上训练的最终神经网络模型的F1分数大约是`0.597`。
- en: It is worth noting that there are several parameters for `SuccessiveHalvingPruner`
    that we can customize based on our needs. The `reduction_factor` parameter refers
    to the multiplier factor of SH (see [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054)).
    The `min_resource` parameter refers to the minimum number of resources to be used
    at the first trial. This parameter is set to *‘auto’*, by default, where a heuristic
    is utilized to calculate the most appropriate value based on the number of required
    steps for the first trial to be completed. In other words, `Optuna` will only
    be able to start the tuning process after the `min_resource` training steps or
    epochs have been performed.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，`SuccessiveHalvingPruner`有几个参数我们可以根据我们的需求进行自定义。`reduction_factor`参数指的是SH（Successive
    Halving）的乘数因子（见[*第6章*](B18753_06_ePub.xhtml#_idTextAnchor054)）。`min_resource`参数指的是第一次试验中要使用的最小资源数量。默认情况下，此参数设置为`‘auto’`，其中使用启发式算法根据第一次试验完成所需的步数来计算最合适的值。换句话说，`Optuna`只有在执行了`min_resource`训练步数或epoch数之后才能开始调整过程。
- en: '`Optuna` also provides the `min_early_stopping_rate` parameter, which has the
    exact same meaning as we defined in [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054).
    Last but not least, the `bootstrap_count` parameter. This parameter is not part
    of the original SH algorithm. The purpose of this parameter is to control the
    minimum number of trials that need to be completed before the actual SH iterations
    start.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`Optuna`还提供了`min_early_stopping_rate`参数，其意义与我们定义在[*第6章*](B18753_06_ePub.xhtml#_idTextAnchor054)中的完全相同。最后但同样重要的是，`bootstrap_count`参数。此参数不是原始SH算法的一部分。此参数的目的是控制实际SH迭代开始之前需要完成的试验的最小数量。'
- en: You may wonder, what about the parameter that controls the value of maximum
    resources and the number of candidates in SH? Here, in `Optuna`, the maximum resources
    definition will be automatically derived based on the total number of training
    steps or epochs within the defined `objective` function. As for the parameter
    that controls the number of candidates, `Optuna` delegates this responsibility
    to the `n_trials` parameter in the `study.optimize()` method.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，关于控制最大资源和SH中候选人数的参数是什么？在这里，在`Optuna`中，最大资源的定义将根据定义的`objective`函数中的总训练步骤或epoch数自动推导。至于控制候选人数的参数，`Optuna`将此责任委托给`study.optimize()`方法中的`n_trials`参数。
- en: In this section, we have learned how to utilize SH as a pruner during the hyperparameter
    tuning process. In the next section, we will learn how to utilize Hyperband, the
    extended algorithm of SH, as a pruning method in `Optuna`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何在参数调整过程中利用SH作为剪枝器。在下一节中，我们将学习如何利用SH的扩展算法Hyperband作为`Optuna`中的剪枝方法。
- en: Implementing Hyperband
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Hyperband
- en: 'Implementing `Optuna` is very similar to implementing Successive Halving as
    a pruner. The only difference is that we have to set the `pruner` parameter in
    the `optimize()` method to `optuna.pruners.HyperbandPruner()` in *step 2* in the
    preceding section. The following code shows you how to perform hyperparameter
    tuning with the Random Search algorithm as the sampler and HB as the pruner:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 实现`Optuna`与实现Successive Halving作为剪枝器非常相似。唯一的区别是我们必须在上一节中的*步骤2*中将`optimize()`方法中的`pruner`参数设置为`optuna.pruners.HyperbandPruner()`。以下代码展示了如何使用随机搜索算法作为采样器，HB作为剪枝器进行超参数调整：
- en: '[PRE62]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: All of the parameters of `HyperbandPruner` are the same as `SuccessiveHalvingPruner`’s,
    except that, here, there is no `min_early_stopping_rate` parameter and there is
    a `max_resource` parameter. The `min_early_stopping_rate` parameter is removed
    since it is set automatically based on the ID of each bracket. The `max_resource`
    parameter is responsible for setting the maximum resource allocated to a trial.
    By default, this parameter is set to *‘auto’*, which means that the value will
    be set as the largest step in the first completed trial.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`HyperbandPruner`的所有参数都与`SuccessiveHalvingPruner`相同，除了这里没有`min_early_stopping_rate`参数，而有一个`max_resource`参数。`min_early_stopping_rate`参数被移除，因为它根据每个括号的ID自动设置。`max_resource`参数负责设置分配给试验的最大资源。默认情况下，此参数设置为`‘auto’`，这意味着其值将设置为第一个完成的试验中的最大步长。'
- en: 'Using the exact same data, preprocessing steps, and hyperparameter space, we
    get around `0.580` of the F1-score evaluated in the validation data. Out of `100`
    trials performed, there are `79` trials pruned by SH, which implies only `21`
    completed trials. We also get a dictionary consisting of the best set of hyperparameters
    as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 使用完全相同的数据、预处理步骤和超参数空间，我们在验证数据中得到的F1分数大约是`0.580`。在进行的`100`次试验中，有`79`次试验被SH剪枝，这意味着只有`21`次试验完成。我们还得到了一个包含最佳超参数集的字典，如下所示：
- en: '[PRE66]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: After the model is trained on full data using the best set of hyperparameters,
    we get around `0.609` in F1-score when we test the final neural network model
    trained on the test data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用最佳超参数集在全部数据上训练模型后，当我们测试在测试数据上训练的最终神经网络模型时，F1分数大约是`0.609`。
- en: In this section, we have learned how to utilize HB as a pruner during the hyperparameter
    tuning process with `Optuna`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何在`Optuna`的参数调整过程中利用HB作为剪枝器。
- en: Summary
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have learned all of the important aspects of the `Optuna`
    package. We have also learned how to implement various hyperparameter tuning methods
    using the help of this package, in addition to understanding each of the important
    parameters of the classes and how are they related to the theory that we have
    learned in previous chapters. From now on, you should be able to utilize the packages
    we have discussed in the last few chapters to implement your chosen hyperparameter
    tuning method, and ultimately, boost the performance of your ML model. Equipped
    with the knowledge from *Chapters 3 - 6*, you will also be able to debug your
    code if there are errors or unexpected results, and you will be able to craft
    your own experiment configuration to match your specific problem.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了`Optuna`包的所有重要方面。我们还学会了如何利用这个包实现各种超参数调优方法，并且理解了每个类的重要参数以及它们与我们之前章节中学到的理论之间的关系。从现在开始，你应该能够利用我们在上一章中讨论的包来实现你选择的超参数调优方法，并最终提升你的机器学习模型的性能。掌握了第3章至第6章的知识，你还将能够调试代码，如果出现错误或意外结果，你还将能够制定自己的实验配置以匹配你的特定问题。
- en: In the next chapter, we will learn about the DEAP and Microsoft NNI packages
    and how to utilize them to perform various hyperparameter tuning methods. The
    goal of the next chapter is similar to this chapter, which is to be able to utilize
    the package for hyperparameter tuning purposes and understand each of the parameters
    of the implemented classes.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习DEAP和Microsoft NNI包以及如何利用它们来执行各种超参数调优方法。下一章的目标与本章类似，即能够利用包进行超参数调优，并理解实现类中的每个参数。
