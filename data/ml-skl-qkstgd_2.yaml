- en: Predicting Categories with K-Nearest Neighbors
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 K-近邻预测类别
- en: 'The **k-Nearest Neighbors** (**k-NN**) algorithm is a form of supervised machine
    learning that is used to predict categories. In this chapter, you will learn about
    the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**k-近邻（k-Nearest Neighbors）**（**k-NN**）算法是一种监督学习算法，用于预测类别。在本章中，你将学习以下内容：'
- en: Preparing a dataset for machine learning with scikit-learn
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为使用 scikit-learn 进行机器学习准备数据集
- en: How the k-NN algorithm works *under the hood*
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k-NN* 算法的工作原理 *在幕后*'
- en: Implementing your first k-NN algorithm to predict a fraudulent transaction
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现你的第一个 k-NN 算法来预测欺诈交易
- en: Fine-tuning the parameters of the k-NN algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调 k-NN 算法的参数
- en: Scaling your data for optimized performance
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为优化性能而对数据进行缩放
- en: The k-NN algorithm has a wide range of applications in the field of classification
    and supervised machine learning. Some of the real-world applications for this
    algorithm include predicting loan defaults and credit-based fraud in the financial
    industry and predicting whether a patient has cancer in the healthcare industry.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN 算法在分类和监督学习领域有广泛的应用。该算法的一些实际应用包括预测金融行业的贷款违约和信用欺诈，以及预测医疗行业中患者是否患有癌症。
- en: This book's design facilitates the implementation of a robust machine learning
    pipeline for each and every algorithm mentioned in the book, and a Jupyter Notebook
    will be required.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的设计旨在实现一个稳健的机器学习管道，适用于书中提到的每个算法，并且需要使用 Jupyter Notebook。
- en: 'The Jupyter Notebook can be installed on your local machine by following the
    instructions provided at: [https://jupyter.org/install.html](https://jupyter.org/install.html).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter Notebook 可以通过以下链接提供的安装说明安装到你的本地机器上：[https://jupyter.org/install.html](https://jupyter.org/install.html)。
- en: 'Alternatively, you can also work with the Jupyter Notebook in the browser by
    using: [https://jupyter.org/try](https://jupyter.org/try).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你也可以通过使用以下链接在浏览器中使用 Jupyter Notebook：[https://jupyter.org/try](https://jupyter.org/try)。
- en: Each chapter in this book comes with a pipeline that is implemented in a Jupyter
    Notebook on the official GitHub repository of this book, and as such, it is highly
    recommended that you install Jupyter Notebook on your local machine.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的每一章都配有一个在 Jupyter Notebook 中实现的管道，存放在本书的官方 GitHub 仓库中，因此，强烈推荐你在本地机器上安装 Jupyter
    Notebook。
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will be required to have Python 3.6 or greater, Pandas ≥ 0.23.4, Scikit-learn
    ≥ 0.20.0, NumPy ≥ 1.15.1, and Matplotlib ≥ 3.0.0 installed on your system.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要在系统中安装 Python 3.6 或更高版本、Pandas ≥ 0.23.4、Scikit-learn ≥ 0.20.0、NumPy ≥ 1.15.1
    和 Matplotlib ≥ 3.0.0。
- en: 'The code files of this chapter can be found on GitHub:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在 GitHub 上找到：
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_02.ipynb](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_02.ipynb)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_02.ipynb](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_02.ipynb)'
- en: 'Check out the following video to see the code in action:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，看看代码的实际应用：
- en: '[http://bit.ly/2Q2DGop](http://bit.ly/2Q2DGop)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2Q2DGop](http://bit.ly/2Q2DGop)'
- en: Preparing a dataset for machine learning with scikit-learn
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为使用 scikit-learn 进行机器学习准备数据集
- en: The first step to implementing any machine learning algorithm with scikit-learn
    is data preparation. Scikit-learn comes with a set of constraints to implementation
    that will be discussed later in this section. The dataset that we will be using
    is based on mobile payments and is found on the world's most popular competitive
    machine learning website – Kaggle.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 实现任何机器学习算法的第一步是数据准备。scikit-learn 提供了一些实现的限制，这些限制将在本节稍后讨论。我们将使用的数据集基于移动支付，来自全球最受欢迎的竞争性机器学习网站——Kaggle。
- en: 'You can download the dataset from: [https://www.kaggle.com/ntnu-testimon/paysim1](https://www.kaggle.com/ntnu-testimon/paysim1).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从以下网址下载数据集：[https://www.kaggle.com/ntnu-testimon/paysim1](https://www.kaggle.com/ntnu-testimon/paysim1)。
- en: 'Once downloaded, open a new Jupyter Notebook by using the following code in
    Terminal (macOS/Linux) or Anaconda Prompt/PowerShell (Windows):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下载后，通过在终端（macOS/Linux）或 Anaconda Prompt/PowerShell（Windows）中使用以下代码来打开一个新的 Jupyter
    Notebook：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The fundamental goal of this dataset is to predict whether a mobile transaction
    is fraudulent. In order to do this, we need to first have a brief understanding
    of the contents of our data. In order to explore the dataset, we will use the
    `pandas`package in Python. You can install pandas by using the following code
    in Terminal (macOS/Linux) or PowerShell (Windows):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集的基本目标是预测一个移动交易是否为欺诈交易。为了实现这一目标，我们首先需要对数据的内容有一个简要了解。为了探索数据集，我们将使用Python中的`pandas`包。你可以通过在终端（macOS/Linux）或PowerShell（Windows）中使用以下代码来安装pandas：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Pandas can be installed on Windows machines in an Anaconda Prompt by using
    the following code:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在Anaconda命令提示符下使用以下代码，在Windows机器上安装Pandas：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can now read in the dataset into our Jupyter Notebook by using the following
    code:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过使用以下代码将数据集读取到Jupyter Notebook中：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This produces an output as illustrated in the following screenshot:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成如下截图所示的输出：
- en: '![](img/29cad5dd-77a7-4546-bbdd-5cf4e997053b.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29cad5dd-77a7-4546-bbdd-5cf4e997053b.png)'
- en: Dropping features that are redundant
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 删除冗余特征
- en: 'From the dataset seen previously, there are a few columns that are redundant
    to the machine learning process:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前看到的数据集中，有一些列对于机器学习过程来说是冗余的：
- en: '`nameOrig`:This column is a unique identifier that belongs to each customer.
    Since each identifier is unique with every row of the dataset, the machine learning
    algorithm will not be able to discern any patterns from this feature.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nameOrig`：这一列是属于每个客户的唯一标识符。由于每个标识符在数据集的每一行都是唯一的，机器学习算法将无法从该特征中辨别出任何模式。'
- en: '`nameDest`:This column is also a unique identifier that belongs to each customer
    and as such provides no value to the machine learning algorithm.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nameDest`：这一列也是每个客户的唯一标识符，因此对机器学习算法没有价值。'
- en: '`isFlaggedFraud`: This column flags a transaction as fraudulent if a person
    tries to transfer more than 200,000 in a single transaction. Since we already
    have a feature called `isFraud`that flags a transaction as fraud, this feature
    becomes redundant.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`isFlaggedFraud`：如果一个人尝试在单笔交易中转账超过200,000，该列会标记该交易为欺诈交易。由于我们已经有一个名为`isFraud`的功能，它会将交易标记为欺诈，因此这个功能变得多余。'
- en: 'We can drop these features from the dataset by using the following code:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码从数据集中删除这些特征：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Reducing the size of the data
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少数据的大小
- en: 'The dataset that we are working with contains over 6 million rows of data.
    Most machine learning algorithms will take a large amount of time to work with
    a dataset of this size. In order to make our execution time quicker, we will reduce
    the size of the dataset to 20,000 rows. We can do this by using the following
    code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在处理的数据集包含超过600万行数据。大多数机器学习算法在处理这么大规模的数据集时会需要很长时间。为了加快执行速度，我们将数据集的大小缩减至20,000行。我们可以使用以下代码来做到这一点：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding code, the fraudulent rows are stored in one dataframe. This
    dataframe contains a little over 8,000 rows. The 12,000 non-fraudulent rows are
    stored in another dataframe, and the two dataframes are joined together using
    the `concat`method from pandas.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，欺诈交易的行被存储在一个数据框中。这个数据框包含稍微超过8,000行数据。12,000行非欺诈交易存储在另一个数据框中，两个数据框通过pandas的`concat`方法连接在一起。
- en: This results in a dataframe with a little over 20,000 rows, over which we can
    now execute our algorithms relatively quickly.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生一个稍微超过20,000行的数据框，现在我们可以相对快速地在其上执行算法。
- en: Encoding the categorical variables
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码类别变量
- en: 'One of the main constraints of scikit-learn is that you cannot implement the
    machine learning algorithms on columns that are categorical in nature. For example,
    the `type`column in our dataset has five categories:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的一个主要限制是，你不能在具有类别性质的列上实施机器学习算法。例如，我们数据集中的`type`列有五个类别：
- en: '`CASH-IN`'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CASH-IN`'
- en: '`CASH-OUT`'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CASH-OUT`'
- en: '`DEBIT`'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DEBIT`'
- en: '`PAYMENT`'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PAYMENT`'
- en: '`TRANSFER`'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TRANSFER`'
- en: These categories will have to be encoded into numbers that scikit-learn can
    make sense of. In order to do this, we have to implement a two-step process.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类别将需要被编码成scikit-learn可以理解的数字。为了做到这一点，我们必须实施一个两步的过程。
- en: 'The first step is to convert each category into a number: `CASH-IN = 0`, `CASH-OUT
    = 1`, `DEBIT = 2`, `PAYMENT = 3`, `TRANSFER = 4`. We can do this by using the
    following code:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将每个类别转换为数字：`CASH-IN = 0`，`CASH-OUT = 1`，`DEBIT = 2`，`PAYMENT = 3`，`TRANSFER
    = 4`。我们可以使用以下代码来完成此操作：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The code first coverts the `type` column to a categorical feature. We then use
    `LabelEncoder()` in order to initialize an integer encoder object that is called
    `type_encode`*.* Finally, we apply the `fit_transform`method on the `type`column
    in order to convert each category into a number.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先将`type`列转换为分类特征。然后，我们使用`LabelEncoder()`来初始化一个称为`type_encode`的整数编码器对象。最后，我们在`type`列上应用`fit_transform`方法，以将每个类别转换为一个数字。
- en: 'Broadly speaking, there are two types of categorical variables:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 大体上讲，有两种类型的分类变量：
- en: Nominal
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 名义
- en: Ordinal
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有序
- en: Nominal categorical variables have no inherent order to them. An example of
    the nominal type of categorical variable is the `type`column.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 名义分类变量没有固有的顺序。名义分类变量的一个例子是`type`列。
- en: Ordinal categorical variables have an inherent order to them. An example of
    the ordinal type of categorical variable is Education Level, in which people with
    a Master's degree will have a higher order/value compared to people with a Undergraduate
    degree only.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有序分类变量具有固有的顺序。有序分类变量的一个例子是教育水平，其中拥有硕士学位的人将比仅有本科学位的人具有更高的顺序/值。
- en: 'In the case of ordinal categorical variables, integer encoding, as illustrated
    previously, is sufficient and we do not need to one-hot encode them. Since the
    `type`column is a nominal categorical variable, we have to one-hot encode it after
    integer encoding it. This is done by using the following code:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于有序分类变量，仅需整数编码即足够，无需进行独热编码，如前所示。由于`type`列是名义分类变量，因此我们在整数编码后必须进行独热编码。通过以下代码实现：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the code, we first create a one-hot encoding object called `type_one_hot`*.*
    We then transform the `type`column into one-hot encoded columns by using the
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们首先创建一个名为`type_one_hot`的独热编码对象。然后，我们通过使用`fit_transform`方法将`type`列转换为独热编码列。
- en: '`fit_transform`method.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit_transform`方法。'
- en: 'We have five categories that are represented by integers 0 to 4\. Each of these
    five categories will now get its own column. Therefore, we create five columns
    called `type_0`, `type_1`, `type_2`, `type_3`, and `type_4`. Each of these five
    columns is represented by two values: `1`, which indicates the presence of that
    category, and `0`, which indicates the absence of that category.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有五个由整数0到4表示的类别。现在，每个这五个类别都将有自己的列。因此，我们创建了五列，分别称为`type_0`、`type_1`、`type_2`、`type_3`和`type_4`。这五列中的每一列都由两个值表示：`1`表示该类别存在，`0`表示该类别不存在。
- en: This information is stored in the `ohe_variable`*.* Since this variable holds
    the five columns, we will join this to the original dataframe by using the `concat`method
    from `pandas`*.*
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此信息存储在`ohe_variable`中。由于此变量包含五列，我们将使用`pandas`的`concat`方法将其与原始数据框连接起来。
- en: 'The ordinal `type` column is then dropped from the dataframe as this column
    is now redundant post one hot encoding. The final dataframe now looks like this:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行独热编码后，顺序`type`列会从数据框中删除，因为此列在独热编码后已经变得多余。最终的数据框现在如下所示：
- en: '![](img/d1f08c7a-918a-4852-9ec1-485138e2c010.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1f08c7a-918a-4852-9ec1-485138e2c010.png)'
- en: Missing values
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺失值
- en: 'Another constraint with scikit-learn is that it cannot handle data with missing
    values. Therefore, we must check whether our dataset has any missing values in
    any of the columns to begin with. We can do this by using the following code:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个与scikit-learn的限制是它不能处理带有缺失值的数据。因此，我们必须首先检查数据集中是否有任何列中的缺失值。我们可以通过使用以下代码来实现这一点：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This produces this output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/539429b2-889d-474b-9403-e591e58f082b.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/539429b2-889d-474b-9403-e591e58f082b.png)'
- en: Here we note that every column has some amount of missing values.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在此我们注意到每列都有一些缺失值。
- en: 'Missing values can be handled in a variety of ways, such as the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过多种方式处理缺失值，例如以下方式：
- en: Median imputation
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中位数插补
- en: Mean imputation
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值插补
- en: Filling them with the majority value
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用大多数值填充它们
- en: The amount of techniques is quite large and varies depending on the nature of
    your dataset. This process of handling features with missing values is called
    **feature engineering**.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 技术的数量非常多，具体取决于数据集的性质。处理具有缺失值特征的过程称为**特征工程**。
- en: Feature engineering can be done for both categorical and numerical columns and
    would require an entire book to explain the various methodologies that comprise
    the topic.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程可以用于分类和数值列，需要一本完整的书来解释组成该主题的各种方法。
- en: Since this book provides you with a deep focus on the art of applying the various
    machine learning algorithms that scikit-learn offers, feature engineering will
    not be dealt with.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书将重点介绍如何应用scikit-learn提供的各种机器学习算法，因此不会涉及特征工程。
- en: So, for the purpose of aligning with the goals that this book intends to achieve,
    we will impute all the missing values with a zero.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了与本书的目标保持一致，我们将用零来填补所有缺失的值。
- en: 'We can do this by using the following code:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码来实现：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We now have a dataset that is ready for machine learning with scikit-learn.
    We will use this dataset for all the other chapters that we will go through in
    the future. To make it easy for us, then, we will export this dataset as a `.csv`
    file and store it in the same directory that you are working in with the Jupyter
    Notebook.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个适用于scikit-learn机器学习的数据集。我们将使用这个数据集来进行未来章节的学习。为了方便起见，我们将把这个数据集导出为`.csv`文件，并存储在你正在使用Jupyter笔记本的同一目录中。
- en: 'We can do this by using the following code:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码来实现：
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This will create a `.csv` file of this dataset in the directory that you are
    working in, which you can load into the notebook again using pandas.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个`.csv`文件，存储在你工作的目录中，你可以使用pandas将其再次加载到笔记本中。
- en: The k-NN algorithm
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-NN算法
- en: 'Mathematically speaking, the k-NN algorithm is one of the most simple machine
    learning algorithms out there. See the following diagram for a visual overview
    of how it works:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，k-NN算法是最简单的机器学习算法之一。请参考以下图表，了解它如何工作的概览：
- en: '![](img/ce91b1e6-4ccd-4b65-a34a-19b913cb1651.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce91b1e6-4ccd-4b65-a34a-19b913cb1651.png)'
- en: How k-NN works under the hood
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN算法的工作原理
- en: The stars in the preceding diagram represent new data points. If we built a
    k-NN algorithm with three neighbors, then the stars would search for the three
    data points that are closest to it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 前面图中的星号代表新的数据点。如果我们构建一个带有三个邻居的k-NN算法，那么这些星号将寻找与其最接近的三个数据点。
- en: In the lower-left case, the star sees two triangles and one circle. Therefore,
    the algorithm would classify the star as a triangle since the number of triangles
    was greater than the number of circles.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在左下方的情况下，星号看到了两个三角形和一个圆形。因此，算法会将星号分类为三角形，因为三角形的数量大于圆形的数量。
- en: In the upper-right case, the star sees two circles and one circle. Therefore,
    the algorithm will classify the star as a circle since the number of circles was
    greater than the number of triangles.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在右上方的情况下，星号看到了两个圆形和一个三角形。因此，算法将把星号分类为圆形，因为圆形的数量大于三角形的数量。
- en: The real algorithm does this in a very probabilistic manner and picks the category/shape
    with the highest probability.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 真实的算法以一种非常概率的方式进行，并选择具有最高概率的类别/形状。
- en: Implementing the k-NN algorithm using scikit-learn
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn实现k-NN算法
- en: In the following section, we will implement the first version of the k-NN algorithm
    and assess its initial accuracy. When implementing machine learning algorithms
    using scikit-learn, it is always a good practice to implement algorithms without
    fine-tuning or optimizing any of the associated parameters first in order to evaluate
    how well it performs.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将实现k-NN算法的第一个版本并评估其初始准确性。在使用scikit-learn实现机器学习算法时，通常的做法是首先实现算法而不对任何相关参数进行微调或优化，以便评估其性能。
- en: 'In the following section, you will learn how to do the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将学习如何完成以下任务：
- en: Split your data into training and test sets
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据分割为训练集和测试集
- en: Implement the first version of the algorithm on the data
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据上实现算法的第一个版本
- en: Evaluate the accuracy of your model using a k-NN score
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用k-NN分数评估模型的准确性
- en: Splitting the data into training and test sets
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据分割为训练集和测试集
- en: The idea of training and test sets is fundamental to every machine learning
    problem. When thinking about this concept, it is easy to understand why the concept
    was introduced. Think of machine learning as the direct equivalent to the process
    of human learning; when learning a concept in mathematics, we first learn how
    to solve a set of problems with solutions attached to them so that we can understand
    the exact methodology involved in solving these problems. We then take a test
    at school or university and attempt to solve problems that we have not encountered
    or seen before in order to test our understanding.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和测试集的概念对于每个机器学习问题来说都是基础。当我们思考这个概念时，很容易理解为什么引入了这一概念。将机器学习看作是人类学习过程的直接对应；当我们学习数学概念时，我们首先学习如何解决一组附带解答的题目，以便理解解决这些问题的具体方法。然后，我们会参加学校或大学的考试，尝试解决一些我们之前未曾接触或见过的问题，以测试我们对概念的理解。
- en: The training set is a part of the dataset that a machine learning algorithm
    uses to learn from. The test set is a part of the dataset that the machine learning
    algorithm has not seen before and is used to assess the performance of the machine
    learning algorithm.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集是机器学习算法用来学习的数据集的一部分。测试集是机器学习算法未见过的数据集的一部分，用来评估算法的性能。
- en: The first step to this process is to save all our features into one variable
    and the target variable, which contains the labels into another variable.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的第一步是将所有特征保存到一个变量中，将包含标签的目标变量保存到另一个变量中。
- en: 'In our dataset, the target variable is called `isFraud`and contains two labels:
    0 if the transaction is not a fraud and 1 if the transaction is a fraud. The features
    are the remaining variables. We can store these into two separate variables by
    using the following code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集中，目标变量被称为`isFraud`，包含两个标签：0表示交易不是欺诈，1表示交易是欺诈。特征是剩余的变量。我们可以通过以下代码将其存储到两个单独的变量中：
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding code, .*values* is used to convert the values in the features
    and target variables into NumPy arrays.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`.*values*`用于将特征和目标变量中的值转换为NumPy数组。
- en: 'Next, we will split the features and target into training and test sets by
    using the following code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用以下代码将特征和目标拆分为训练集和测试集：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We use the `train_test_split` from `sklearn.model_selection`in order to perform
    this task. In the preceding code, we have four variables. `X_train` and `X_test`
    correspond to the training and test sets for the features, while `y_train` and
    `y_test` correspond to training and test sets for the target variable.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`sklearn.model_selection`中的`train_test_split`来执行此任务。在前面的代码中，我们有四个变量。`X_train`和`X_test`对应特征的训练集和测试集，而`y_train`和`y_test`对应目标变量的训练集和测试集。
- en: The `train_test_split()`function takes in four arguments. The first argument
    is the array containing the features, the second argument is the array containing
    the target variable. The `test_size` argumentis used to specify the amount of
    data that will be split and stored into the test set. Since we specified `0.3`,
    30% of the original data will be stored in the test set, while 70% of the original
    data will be used for training.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_test_split()`函数接受四个参数。第一个参数是包含特征的数组，第二个参数是包含目标变量的数组。`test_size`参数用于指定将拆分并存储到测试集的数据量。由于我们指定了`0.3`，原始数据的30%将被存储到测试集中，而70%的原始数据将用于训练。'
- en: 'There are two primary ways in which the `train_test_split()` function shuffles
    data into training and test sets for the target variable:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_test_split()`函数有两种主要方式将数据打乱并划分为训练集和测试集：'
- en: '**Random sampling**: Randomly puts target labels into training and test sets
    (`y_train` and `y_test` in the preceding case).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机抽样**：将目标标签随机分配到训练集和测试集中（在前面的例子中为`y_train`和`y_test`）。'
- en: '**Stratified sampling**:Ensures that the target labels are represented adequately
    in the training and test sets. In the preceding code, the *stratify* argument
    has been set to the target labels to ensure that this happens.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分层抽样**：确保目标标签在训练集和测试集中得到充分表示。在前面的代码中，*stratify*参数已设置为目标标签，以确保这一点。'
- en: Implementation and evaluation of your model
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型的实现和评估
- en: 'Now that we have the training and test splits, we can implement the k-NN algorithm
    on the training sets and evaluate its score on the test sets. We can do this by
    using the following code:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得了训练集和测试集的划分，我们可以在训练集上实施k-NN算法，并在测试集上评估其得分。我们可以通过以下代码实现这一点：
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the preceding code, we first initialize a k-NN classifier with three neighbors.
    The number of neighbors is chosen arbitrarily, and three is a good starting number.
    Next, we use the `.fit()`method to fit this classifier onto our training data.
    Finally, by using the `.score()`method on the test data, we obtain a value between
    0 and 1 that indicates how accurate the classifier is.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们首先用三个邻居初始化一个k-NN分类器。邻居数量是任意选择的，三个是一个不错的起始数。接下来，我们使用`.fit()`方法将该分类器拟合到我们的训练数据上。最后，通过在测试数据上使用`.score()`方法，我们获得一个介于0和1之间的值，表示分类器的准确性。
- en: In our case, we obtained an accuracy score of `0.98`, which is very good!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们获得了`0.98`的准确率，这非常好！
- en: There are many ways of assessing and evaluating the performance of the classifier,
    and the accuracy score should not be the only way you evaluate the performance
    of your classifier. Further methods of evaluation will be discussed at a later
    stage in the chapter.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多评估和评价分类器性能的方法，准确率不应该是唯一评估分类器性能的标准。更多的评估方法将在本章后续部分讨论。
- en: Fine-tuning the parameters of the k-NN algorithm
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调k-NN算法的参数
- en: In the previous section, we arbitrarily set the number of neighbors to three
    while initializing the k-NN classifier. However, is this the optimal value? Well,
    it could be, since we obtained a relatively high accuracy score in the test set.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们在初始化k-NN分类器时任意设置了邻居数为三。然而，这真的是最优值吗？嗯，这可能是，因为我们在测试集上得到了相对较高的准确率。
- en: Our goal is to create a machine learning model that does not overfit or underfit
    the data. Overfitting the data means that the model has been trained very specifically
    to the training examples provided and will not generalize well to cases/examples
    of data that it has not encountered before. For instance, we might have fit the
    model very specifically to the training data, with the test cases being also very
    similar to the training data. Thus, the model would have been able to perform
    very well and produce a very high value of accuracy.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是创建一个既不对数据过拟合也不欠拟合的机器学习模型。过拟合数据意味着模型被非常具体地训练在提供的训练样本上，并且在遇到之前没有见过的数据时，无法很好地泛化。例如，我们可能已经非常具体地将模型拟合到训练数据上，而测试案例也与训练数据非常相似。因此，模型能够表现得非常好，并且产生非常高的准确率。
- en: Underfitting is another extreme case, in which the model fits the data in a
    very generic way and does not perform well in predicting the correct class labels
    in the test set. This is the exact opposite of overfitting.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 欠拟合是另一种极端情况，在这种情况下，模型以非常通用的方式拟合数据，在测试集中预测正确类别标签的表现不佳。这与过拟合恰好相反。
- en: Both these cases can be avoided by visualizing how well the model performs in
    the training and test sets by using a different number of neighbors. To do this,
    we first find the optimal number of neighbors by using the `GridSearchCV` algorithm.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通过可视化模型在训练集和测试集中的表现，使用不同数量的邻居，可以避免这两种情况。为此，我们首先使用`GridSearchCV`算法来找到最优的邻居数量。
- en: '`GridSearchCV` creates an empty grid and fills it with possible values of the
    number of neighbors or any other machine learning parameter that we want to optimize.
    It then uses each value in the grid and tests its performance and determines the
    optimal value of the parameter. We can implement the `GridSearchCV` algorithm
    to find the optimal number of neighbors by using the following code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV`创建一个空的网格，并将其填充为我们希望优化的邻居数量或任何其他机器学习参数的可能值。然后，它使用网格中的每个值测试其性能，并确定该参数的最优值。我们可以通过以下代码实现`GridSearchCV`算法来找到最优的邻居数量：'
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this code, we first initialize a number array with values between 1 and 24\.
    This range was chosen arbitrarily and you can increase or decrease the range.
    However, increasing the range will mean that it will take more computational time
    to compute and find the optimal number of neighbors, especially when your dataset
    is large.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们首先初始化一个包含1到24之间值的数字数组。这个范围是任意选择的，你可以增加或减少范围。然而，增加范围意味着计算时需要更多的时间，特别是当你的数据集较大时，找到最优邻居数的过程将更加耗时。
- en: Next, we initialize a k-NN classifier and use the `GridSearchCV()`function on
    the classifier along with the grid. We set the `cv`argument to 10, indicating
    that we want to use 10-fold cross validation while doing this. Cross validation
    is a technique in which the classifier first divides the data into 10 parts. The
    first nine parts are used as the training set while the 10^(th) part is used as
    the test set. In the second iteration, we use the first eight parts and the 10^(th)
    part as the training set, while the ninth part is used as the test set. This process
    is repeated until every part of the data is used for testing. This creates a very
    robust classifier, since we have used the entire dataset for training and testing
    and have not left out any part of the data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们初始化一个k-NN分类器，并使用`GridSearchCV()`函数与网格一起对分类器进行操作。我们将`cv`参数设置为10，表示在执行时我们希望使用10折交叉验证。交叉验证是一种技术，其中分类器首先将数据划分为10个部分。前九个部分作为训练集，第10部分作为测试集。在第二次迭代中，我们使用前八个部分和第10部分作为训练集，第九部分作为测试集。这个过程会重复，直到每一部分数据都被用于测试。这种方法可以创建一个非常强大的分类器，因为我们使用了整个数据集进行训练和测试，确保没有遗漏任何数据。
- en: 'Cross-validation is illustrated for you in the following diagram:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证在下面的图示中进行了说明：
- en: '![](img/21251247-757b-4866-bc0d-8cbbb882ec41.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21251247-757b-4866-bc0d-8cbbb882ec41.png)'
- en: Cross-validation in action
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证实战
- en: In the preceding diagram, the black boxes illustrate the training data while
    the white box illustrates the test data.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，黑色框表示训练数据，而白色框表示测试数据。
- en: Finally, we use the `.best_params_`to extract the optimal number of neighbors.
    In our case, the optimal number of neighbors was 1, which resulted in an accuracy
    score of `0.985`. This is an improvement of 0.002 from the original classifier
    that we built, which had a score of `0.983` with three neighbors.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`.best_params_`来提取最佳的邻居数量。在我们的例子中，最佳邻居数量是1，导致了`0.985`的准确率分数。这比我们最初构建的分类器有所改进，后者使用三个邻居的准确率为`0.983`。
- en: Using cross-validation ensures that we do not overfit or underfit the data as
    we have used the entire dataset for training and testing.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用交叉验证可以确保我们不会对数据进行过拟合或欠拟合，因为我们已使用整个数据集进行训练和测试。
- en: Scaling for optimized performance
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为了优化性能进行缩放
- en: The k-NN algorithm is an algorithm that works based on distance. When a new
    data point is thrown into the dataset and the algorithm is given the task of classifying
    this new data point, it uses distance to check the points that are closest to
    it.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN算法是一种基于距离的算法。当一个新的数据点被引入数据集，并且算法需要对该新数据点进行分类时，它会使用距离来检查与之最接近的点。
- en: If we have features that have different ranges of values – for example, feature
    one has a range between 0 to 800 while feature two has a range between one to
    five – this distance metric does not make sense anymore. We want all the features
    to have the same range of values so that the distance metric is on level terms
    across all features.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有不同范围值的特征——例如，特征一的范围是0到800，而特征二的范围是1到5——那么这个距离度量就不再有意义了。我们希望所有特征的值范围相同，以便距离度量在所有特征上都具有相同的标准。
- en: 'One way to do this is to subtract each value of each feature by the mean of
    that feature and divide by the variance of that feature. This is called **standardization**:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种方法是将每个特征的每个值减去该特征的均值，并除以该特征的方差。这称为**标准化**：
- en: '![](img/4c23e62d-4280-4704-8ecb-1c1acc45a6ba.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c23e62d-4280-4704-8ecb-1c1acc45a6ba.png)'
- en: 'We can do this for our dataset by using the following code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码对数据集进行操作：
- en: '[PRE15]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this code, we specify the order in which the pipeline has to be executed.
    We store this order in a variable called `pipeline_order`by specifying that we
    want to scale our data first by using the `StandardScaler()`function and then
    build a k-NN classifier with one neighbor.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们指定了管道执行的顺序。我们将这个顺序存储在一个名为`pipeline_order`的变量中，指定首先使用`StandardScaler()`函数对数据进行缩放，然后构建一个具有一个邻居的k-NN分类器。
- en: Next, we use the `Pipeline()`function and pass in the order of the pipeline
    as the only argument. We then fit this pipeline to the training set and extract
    the accuracy scores from the test set.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`Pipeline()`函数并将管道的顺序作为唯一参数传入。然后，我们将该管道拟合到训练集，并从测试集中提取准确率分数。
- en: The `Pipeline` function, as the name implies, is used to fit multiple functions
    into a pipeline and execute them in a specified order that we think is apt for
    the process. This function helps us streamline and automate common machine learning
    tasks.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pipeline` 函数，顾名思义，用于将多个函数组合成一个管道，并按我们认为适合的顺序执行它们。这个函数帮助我们简化和自动化常见的机器学习任务。'
- en: This resulted in an accuracy score of `0.997`, which is a substantial improvement
    from the score of `0.985`. Thus, we see how scaling the data results in improved
    performance.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了 `0.997` 的准确率，这比 `0.985` 的得分有了显著的提高。因此，我们看到数据缩放有助于提升性能。
- en: Summary
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter was fundamental in helping you prepare a dataset for machine learning
    with scikit-learn. You have learned about the constraints that are imposed when
    you do machine learning with scikit-learn and how to create a dataset that is
    perfect for scikit-learn.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本章帮助你为使用 scikit-learn 进行机器学习准备了一个数据集。你了解了在使用 scikit-learn 进行机器学习时所施加的约束，以及如何创建一个适合
    scikit-learn 的完美数据集。
- en: You have also learned how the k-NN algorithm works behind the scenes and have
    implemented a version of it using scikit-learn to predict whether a transaction
    was fraudulent. You then learned how to optimize the parameters of the algorithm
    using the popular `GridSearchCV` algorithm. Finally, you have learnt how to standardize
    and scale your data in order to optimize the performance of your model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学习了 k-NN 算法的工作原理，并使用 scikit-learn 实现了该算法的一个版本，用于预测交易是否欺诈。然后，你学习了如何使用流行的 `GridSearchCV`
    算法优化算法的参数。最后，你了解了如何标准化和缩放数据，以优化模型的性能。
- en: In the next chapter, you will learn how to classify fraudulent transactions
    yet again with a new algorithm – the logistic regression algorithm!
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将再次学习如何使用一种新算法——逻辑回归算法，来分类欺诈交易！
