- en: '18'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '18'
- en: Reinforcement Learning for Decision Making in Complex Environments
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习在复杂环境中的决策制定
- en: In the previous chapters, we focused on supervised and unsupervised machine
    learning. We also learned how to leverage artificial neural networks and deep
    learning to tackle problems encountered with these types of machine learning.
    As you'll recall, supervised learning focuses on predicting a category label or
    continuous value from a given input feature vector. Unsupervised learning focuses
    on extracting patterns from data, making it useful for data compression (*Chapter
    5*, *Compressing Data via Dimensionality Reduction*), clustering (*Chapter 11*,
    *Working with Unlabeled Data – Clustering Analysis*), or approximating the training
    set distribution for generating new data (*Chapter 17*, *Generative Adversarial
    Networks for Synthesizing New Data*).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们重点讨论了监督学习和无监督学习。我们还学习了如何利用人工神经网络和深度学习来解决这些类型机器学习中遇到的问题。如你所记得，监督学习专注于根据给定的输入特征向量预测类别标签或连续值。无监督学习则专注于从数据中提取模式，因此它在数据压缩（*第5章*，*通过降维压缩数据*）、聚类（*第11章*，*处理无标签数据——聚类分析*）或通过近似训练集分布来生成新数据（*第17章*，*生成对抗网络用于合成新数据*）等方面非常有用。
- en: 'In this chapter, we turn our attention to a separate category of machine learning,
    **reinforcement learning** (**RL**), which is different from the previous categories
    as it is focused on learning *a series of actions* for optimizing an overall reward—for
    example, winning at a game of chess. In summary, this chapter will cover the following
    topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论机器学习的另一类，**强化学习**（**RL**），它不同于前面讨论的类别，因为它专注于学习*一系列动作*以优化整体奖励——例如，在国际象棋游戏中获胜。总之，本章将涵盖以下内容：
- en: Learning the basics of RL, getting familiar with agent/environment interactions,
    and understanding how the reward process works, in order to help make decisions
    in complex environments
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习RL的基础知识，熟悉智能体/环境交互，并理解奖励过程的工作原理，从而帮助在复杂环境中做出决策
- en: Introducing different categories of RL problems, model-based and model-free
    learning tasks, Monte Carlo, and temporal difference learning algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍不同类别的RL问题、基于模型和无模型的学习任务、蒙特卡洛算法和时序差分学习算法
- en: Implementing a Q-learning algorithm in a tabular format
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在表格格式中实现Q学习算法
- en: Understanding function approximation for solving RL problems, and combining
    RL with deep learning by implementing a *deep* Q-learning algorithm
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解函数逼近在解决RL问题中的应用，并通过实现*深度*Q学习算法将RL与深度学习相结合
- en: RL is a complex and vast area of research, and this chapter focuses on the fundamentals.
    As this chapter serves as an introduction, and in order to keep our attention
    on the important methods and algorithms, we will mainly work with basic examples
    that illustrate the main concepts. However, toward the end of this chapter, we
    will go over a more challenging example and utilize deep learning architectures
    for a particular RL approach, which is known as deep Q-learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一个复杂且广泛的研究领域，本章重点介绍基础内容。由于本章是入门章节，为了让我们集中精力关注重要的方法和算法，我们将主要使用基本示例来说明主要概念。然而，在本章的最后，我们将讨论一个更具挑战性的例子，并使用深度学习架构来实现一种特定的RL方法，称为深度Q学习。
- en: Introduction – learning from experience
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言——从经验中学习
- en: In this section, we will first introduce the concept of RL as a branch of machine
    learning and see its major differences compared with other tasks of machine learning.
    After that, we will cover the fundamental components of an RL system. Then, we
    will see the RL mathematical formulation based on the Markov decision process.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先介绍强化学习（RL）作为机器学习的一个分支，并与其他机器学习任务进行比较，了解其主要差异。接着，我们将讨论RL系统的基本组成部分。然后，我们将介绍基于马尔可夫决策过程的RL数学模型。
- en: Understanding reinforcement learning
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解强化学习
- en: Until this point, this book has primarily focused on *supervised* and *unsupervised*
    learning. Recall that in *supervised* learning, we rely on labeled training examples,
    which are provided by a supervisor or a human expert, and the goal is to train
    a model that can generalize well to unseen, unlabeled test examples. This means
    that the supervised learning model should learn to assign the same labels or values
    to a given input example as the supervisor human expert. On the other hand, in
    *unsupervised* learning, the goal is to learn or capture the underlying structure
    of a dataset, such as in clustering and dimensionality reduction methods; or learning
    how to generate new, synthetic training examples with a similar underlying distribution.
    RL is substantially different from supervised and unsupervised learning, and so
    RL is often regarded as the "third category of machine learning."
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书主要关注了*监督*学习和*无监督*学习。回顾一下，在*监督*学习中，我们依赖于标注的训练样本，这些样本由监督者或人类专家提供，目标是训练一个能够很好地对未见过的、未标注的测试样本进行泛化的模型。这意味着监督学习模型应该学习如何为给定的输入样本分配与监督者或人类专家相同的标签或值。另一方面，在*无监督*学习中，目标是学习或捕捉数据集的潜在结构，例如在聚类和降维方法中；或者学习如何生成具有相似潜在分布的新的合成训练样本。强化学习（RL）与监督学习和无监督学习有本质的不同，因此强化学习通常被视为“机器学习的第三类”。
- en: The key element that distinguishes RL from other subtasks of machine learning,
    such as supervised and unsupervised learning, is that RL is centered around the
    concept of *learning by interaction*. This means that in RL, the model learns
    from interactions with an environment to maximize a *reward function*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 区分强化学习与机器学习其他子任务（如监督学习和无监督学习）的关键因素是，强化学习围绕*通过交互学习*的概念展开。这意味着在强化学习中，模型通过与环境的交互来学习，以最大化*奖励函数*。
- en: While maximizing a reward function is related to the concept of minimizing the
    cost function in supervised learning, the *correct* labels for learning a series
    of actions are not known or defined upfront in RL—instead, they need to be learned
    through interactions with the environment, in order to achieve a certain desired
    outcome—such as winning at a game. With RL, the model (also called an **agent**)
    interacts with its environment, and by doing so generates a sequence of interactions
    that are together called an *episode*. Through these interactions, the agent collects
    a series of rewards determined by the environment. These rewards can be positive
    or negative, and sometimes they are not disclosed to the agent until the end of
    an episode.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然最大化奖励函数与监督学习中最小化成本函数的概念相关，但在强化学习（RL）中，学习一系列动作的*正确*标签是未知的或没有事先定义的——相反，它们需要通过与环境的交互来学习，以实现某个期望的结果——例如赢得一场游戏。在强化学习中，模型（也称为**智能体**）与环境进行交互，通过这些交互生成一系列的交互过程，统称为*一个回合*。通过这些交互，智能体收集由环境确定的一系列奖励。这些奖励可以是正向的，也可以是负向的，有时直到回合结束才会向智能体披露奖励。
- en: For example, imagine that we want to teach a computer to play the game of chess
    and win against human players. The labels (rewards) for each individual chess
    move made by the computer are not known until the end of the game, because during
    the game itself, we don't know whether a particular move will result in winning
    or losing that game. Only right at the end of the game is the feedback determined.
    That feedback would likely be a positive reward given if the computer won the
    game, because the agent had achieved the overall desired outcome; and vice versa,
    a negative reward would likely be given if the computer had lost the game.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设我们想教一个计算机下棋，并且赢得人类玩家的比赛。每一个由计算机做出的棋步的标签（奖励）在游戏结束之前是无法知道的，因为在游戏过程中，我们并不清楚某一步棋是否会导致胜利或失败。只有在游戏结束时，反馈才会被确定。如果计算机赢得了比赛，那么反馈很可能是一个正向奖励，因为智能体实现了预期的最终目标；相反，如果计算机输了比赛，那么很可能会给一个负向奖励。
- en: Furthermore, considering the example of playing chess, the input is the current
    configuration, for instance, the arrangement of the individual chess pieces on
    the board. Given the large number of possible inputs (the states of the system),
    it is impossible to label each configuration or state as positive or negative.
    Therefore, to define a learning process, we provide rewards (or penalties) at
    the end of each game, when we know whether we reached the desired outcome—whether
    we won the game or not.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，考虑到下棋的例子，输入是当前的配置，例如棋盘上各个棋子的排列。由于可能的输入（系统的状态）种类繁多，我们无法将每种配置或状态标记为正面或负面。因此，为了定义学习过程，我们在每场比赛结束时提供奖励（或惩罚），此时我们已经知道是否达到了期望的结果——无论我们是否赢得了比赛。
- en: This is the essence of RL. In RL, we cannot or do not teach an agent, computer
    or robot, *how* to do things; we can only specify *what* we want the agent to
    achieve. Then, based on the outcome of a particular trial, we can determine rewards
    depending on the agent's success or failure. This makes RL very attractive for
    decision making in complex environments—especially when the problem-solving task
    requires a series of steps, which are unknown, or are hard to explain, or hard
    to define.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是强化学习的本质。在强化学习中，我们不能也不需要教智能体、计算机或机器人，*如何*做事情；我们只能指定*我们*希望智能体达成的目标。然后，根据特定尝试的结果，我们可以根据智能体的成功或失败来确定奖励。这使得强化学习在复杂环境中的决策制定中非常有吸引力——特别是当问题解决任务需要一系列未知的步骤，或者这些步骤很难解释或定义时。
- en: Besides applications in games and robotics, examples of RL can also be found
    in nature. For example, training a dog involves RL—we hand out rewards (treats)
    to the dog when it performs certain desirable actions. Or consider a medical dog
    that is trained to warn its partner of an oncoming seizure. In this case, we do
    not know the exact mechanism by which the dog is able to detect an oncoming seizure,
    and we certainly wouldn't be able to define a series of steps to learn seizure
    detection, even if we had precise knowledge of this mechanism. However, we can
    reward the dog with a treat if it successfully detects a seizure to *reinforce*
    this behavior!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在游戏和机器人学中的应用外，强化学习的例子也可以在自然界中找到。例如，训练一只狗就涉及强化学习——当狗做出某些期望的动作时，我们会给予它奖励（零食）。再比如，考虑一只训练有素的医疗犬，它能够警告其主人即将发生癫痫发作。在这种情况下，我们不知道狗如何准确地检测到即将发生的癫痫发作，甚至如果我们知道这一机制，也无法定义一系列步骤来学习癫痫检测。然而，如果狗成功地检测到癫痫发作，我们可以通过奖励它零食来*强化*这一行为！
- en: While RL provides a powerful framework for learning an arbitrary series of actions,
    in order to achieve a certain goal, please do keep in mind that RL is still a
    relatively young and active area of research with many unresolved challenges.
    One aspect that makes training RL models particularly challenging is that the
    consequent model inputs depend on actions taken previously. This can lead to all
    sorts of problems, and usually results in unstable learning behavior. Also, this
    sequence-dependence in RL creates a so-called *delayed effect*, which means that
    the action taken at a time step *t* may result in a future reward appearing some
    arbitrary number of steps later.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然强化学习提供了一个强大的框架，用于学习实现某个目标所需的任意一系列动作，但请记住，强化学习仍然是一个相对年轻且活跃的研究领域，面临许多未解决的挑战。使得训练强化学习模型特别具有挑战性的一方面是，随之而来的模型输入依赖于先前采取的行动。这可能会导致各种问题，通常会导致不稳定的学习行为。此外，强化学习中的这一序列依赖性会产生所谓的*延迟效应*，这意味着在时间步*
    t *采取的动作可能导致未来某个任意步数后出现奖励。
- en: Defining the agent-environment interface of a reinforcement learning system
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义强化学习系统中的智能体-环境接口
- en: 'In all examples of RL, we can find two distinct entities: an agent and an environment.
    Formally, an **agent** is defined as an entity that learns how to make decisions
    and interacts with its surrounding environment by taking an action. In return,
    as a consequence of taking an action, the agent receives observations and a reward
    signal as governed by the environment. The **environment** is anything that falls
    outside the agent. The environment communicates with the agent and determines
    the reward signal for the agent''s action as well as its observations.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有强化学习的例子中，我们可以找到两个截然不同的实体：一个是智能体，一个是环境。正式来说，**智能体**被定义为一种通过采取行动来学习如何做出决策并与周围环境互动的实体。作为回报，智能体采取行动后，环境根据规定返回观察值和奖励信号。**环境**是指智能体之外的任何事物。环境与智能体进行交互，并确定智能体行为的奖励信号及其观察结果。
- en: 'The **reward signal** is the feedback that the agent receives from interacting
    with the environment, which is usually provided in the form of a scalar value
    and can be either positive or negative. The purpose of the reward is to tell the
    agent how well it has performed. The frequency at which the agent receives the
    reward depends on the given task or problem. For example, in the game of chess,
    the reward would be determined after a full game based on the outcome of all the
    moves: a win or a loss. On the other hand, we could define a maze such that the
    reward is determined after each time step. In such a maze, the agent then tries
    to maximize its accumulated rewards over its lifetime—where lifetime describes
    the duration of an episode.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**奖励信号**是智能体通过与环境互动所收到的反馈，通常以标量值的形式提供，可能是正值也可能是负值。奖励的目的是告诉智能体它的表现如何。智能体接收到奖励的频率取决于特定的任务或问题。例如，在国际象棋游戏中，奖励会在整场比赛结束后，根据所有棋步的结果来决定：获胜或失败。另一方面，我们也可以定义一个迷宫，在每个时间步之后决定奖励。在这样的迷宫中，智能体会尝试最大化它在生命周期中的累计奖励——其中生命周期指的是一个回合的持续时间。'
- en: 'The following diagram illustrates the interactions and communication between
    the agent and the environment:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了智能体与环境之间的交互和通信：
- en: '![](img/B13208_18_01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_01.png)'
- en: The state of the agent, as illustrated in the previous figure, is the set of
    all of its variables (1). For example, in the case of a robot drone, these variables
    could include the drone's current position (longitude, latitude, and altitude),
    the drone's remaining battery life, the speed of each fan, and so forth. At each
    time step, the agent interacts with the environment through a set of available
    actions ![](img/B13208_18_001.png) (2). Based on the action taken by the agent
    denoted by ![](img/B13208_18_002.png), while it is at state ![](img/B13208_18_003.png),
    the agent will receive a reward signal ![](img/B13208_18_004.png) (3), and its
    state will become ![](img/B13208_18_005.png) (4).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，智能体的状态是它所有变量的集合（1）。例如，在机器人无人机的情况下，这些变量可能包括无人机的当前位置（经度、纬度和高度）、剩余电池电量、每个风扇的速度等。每个时间步，智能体通过一组可用动作与环境进行交互
    ![](img/B13208_18_001.png)（2）。根据智能体所采取的动作！[](img/B13208_18_002.png)，当它处于状态 ![](img/B13208_18_003.png)
    时，智能体将接收到一个奖励信号 ![](img/B13208_18_004.png)（3），并且它的状态将变为 ![](img/B13208_18_005.png)（4）。
- en: During the learning process, the agent must try different actions (**exploration**),
    so that it can progressively learn which actions to prefer and perform more often
    (**exploitation**) in order to maximize the total, cumulative reward. To understand
    this concept, let's consider a very simple example where a new computer science
    graduate with a focus on software engineering is wondering whether to start working
    at a company (exploitation) or to pursue a Master's or Ph.D. degree to learn more
    about data science and machine learning (exploration). In general, exploitation
    will result in choosing actions with a greater short-term reward, whereas exploration
    can potentially result in greater total rewards in the long run. The tradeoff
    between exploration and exploitation has been studied extensively, and yet, there
    is no universal answer to this decision-making dilemma.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习过程中，智能体必须尝试不同的动作（**探索**），这样它就能逐步学会哪些动作更优，并更加频繁地执行这些动作（**利用**），以最大化总的累计奖励。为了理解这个概念，我们可以考虑一个非常简单的例子：一个新毕业的计算机科学硕士，专注于软件工程，他在考虑是加入一家公司工作（利用）还是继续攻读硕士或博士学位，深入学习数据科学和机器学习（探索）。一般来说，利用会导致选择短期回报更大的动作，而探索则有可能在长期内带来更大的总回报。探索与利用之间的权衡已被广泛研究，但对于这一决策困境，目前并没有普遍的答案。
- en: The theoretical foundations of RL
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的理论基础
- en: Before we jump into some practical examples and start training an RL model,
    which we will be doing later in this chapter, let's first understand some of the
    theoretical foundations of RL. The following sections will begin by first examining
    the mathematical formulation of **Markov decision processes**, episodic versus
    continuing tasks, some key RL terminology, and dynamic programming using the **Bellman
    equation**. Let's start with Markov decision processes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们跳入一些实际的例子并开始训练一个强化学习（RL）模型之前，接下来我们将在本章进行相关操作，首先让我们了解一下强化学习的一些理论基础。接下来的章节将首先通过考察**马尔可夫决策过程**的数学形式、阶段性任务与连续任务的区别、一些强化学习的关键术语，以及使用**贝尔曼方程**的动态规划，来开始介绍。让我们从马尔可夫决策过程开始。
- en: Markov decision processes
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: In general, the type of problems that RL deals with are typically formulated
    as **Markov decision processes** (**MDPs**). The standard approach for solving
    MDP problems is by using dynamic programming, but RL offers some key advantages
    over dynamic programming.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，强化学习所处理的问题类型通常被表述为**马尔可夫决策过程**（**MDPs**）。解决 MDP 问题的标准方法是使用动态规划，但强化学习相比动态规划具有一些关键优势。
- en: '**Dynamic programming**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态规划**'
- en: Dynamic programming refers to a set of computer algorithms and programming methods
    that was developed by Richard Bellman in the 1950s. In a sense, dynamic programming
    is about recursive problem solving—solving relatively complicated problems by
    breaking them down into smaller subproblems.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划是理查德·贝尔曼（Richard Bellman）在1950年代开发的一组计算机算法和编程方法。从某种意义上说，动态规划是一种递归问题求解方法——通过将相对复杂的问题分解为更小的子问题来解决。
- en: The key difference between recursion and dynamic programming is that dynamic
    programming stores the results of subproblems (usually as a dictionary or other
    form of lookup table) so that they can be accessed in constant time (instead of
    recalculating them) if they are encountered again in future.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 递归和动态规划之间的主要区别在于，动态规划会存储子问题的结果（通常以字典或其他查找表的形式），以便在将来再次遇到时可以在常数时间内访问（而不是重新计算）。
- en: Examples of some famous problems in computer science that are solved by dynamic
    programming include sequence alignment and computing the shortest path from point
    A to point B.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划解决的计算机科学中的一些著名问题包括序列对齐和计算从 A 点到 B 点的最短路径。
- en: Dynamic programming is not a feasible approach, however, when the size of states
    (that is, the number of possible configurations) is relatively large. In such
    cases, RL is considered a much more efficient and practical alternative approach
    for solving MDPs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当状态的规模（即可能的配置数量）相对较大时，动态规划并不是一种可行的方法。在这种情况下，强化学习被认为是一种更加高效且实用的替代方法，用于解决 MDP
    问题。
- en: The mathematical formulation of Markov decision processes
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程的数学表述
- en: The types of problems that require learning an interactive and sequential decision-making
    process, where the decision at time step *t* affects the subsequent situations,
    are mathematically formalized as Markov decision processes (MDPs).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 需要学习互动和顺序决策过程的问题类型，其中时间步 *t* 的决策会影响后续的情况，数学上被形式化为马尔可夫决策过程（MDPs）。
- en: 'In case of the agent/environment interactions in RL, if we denote the agent''s
    starting state as ![](img/B13208_18_006.png), the interactions between the agent
    and the environment result in a sequence as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，若我们将智能体的初始状态表示为 ![](img/B13208_18_006.png)，那么智能体和环境之间的交互会产生如下的序列：
- en: '![](img/B13208_18_007.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_007.png)'
- en: 'Note that the braces serve only as a visual aid. Here, ![](img/B13208_18_008.png)
    and ![](img/B13208_18_009.png) stand for the state and the action taken at time
    step *t*. ![](img/B13208_18_010.png) denotes the reward received from the environment
    after performing action ![](img/B13208_18_011.png). Note that ![](img/B13208_18_012.png),
    ![](img/B13208_18_013.png), and ![](img/B13208_18_014.png) are time-dependent
    random variables that take values from predefined finite sets denoted by ![](img/B13208_18_015.png),
    ![](img/B13208_18_016.png), and ![](img/B13208_18_017.png), respectively. In an
    MDP, these time-dependent random variables, ![](img/B13208_18_018.png) and ![](img/B13208_18_019.png),
    have probability distributions that only depend on their values at the preceding
    time step, *t* – 1\. The probability distribution for ![](img/B13208_18_020.png)
    and ![](img/B13208_18_021.png) can be written as a conditional probability over
    the preceding state (![](img/B13208_18_022.png)) and taken action (![](img/B13208_18_023.png))
    as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，大括号仅作为视觉辅助。这里，![](img/B13208_18_008.png) 和 ![](img/B13208_18_009.png) 表示时间步
    *t* 时的状态和所采取的动作。![](img/B13208_18_010.png) 表示执行动作 ![](img/B13208_18_011.png) 后从环境中获得的奖励。请注意，![](img/B13208_18_012.png)、![](img/B13208_18_013.png)
    和 ![](img/B13208_18_014.png) 是时间相关的随机变量，它们的取值来自预定义的有限集合，分别由 ![](img/B13208_18_015.png)、![](img/B13208_18_016.png)
    和 ![](img/B13208_18_017.png) 表示。在马尔可夫决策过程中，这些时间相关的随机变量 ![](img/B13208_18_018.png)
    和 ![](img/B13208_18_019.png) 的概率分布只依赖于它们在前一个时间步 *t* - 1 的值。![](img/B13208_18_020.png)
    和 ![](img/B13208_18_021.png) 的概率分布可以表示为对前一个状态 (![](img/B13208_18_022.png)) 和所采取动作
    (![](img/B13208_18_023.png)) 的条件概率，如下所示：
- en: '![](img/B13208_18_024.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_024.png)'
- en: This probability distribution completely defines the **dynamics of the environment**
    (or model of the environment) because, based on this distribution, all transition
    probabilities of the environment can be computed. Therefore, the environment dynamics
    are a central criterion for categorizing different RL methods. The types of RL
    methods that require a model of the environment or try to learn a model of the
    environment (that is, the environment dynamics) are called *model-based* methods,
    as opposed to *model-free* methods.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概率分布完全定义了**环境的动态**（或环境模型），因为基于这个分布，可以计算环境的所有转移概率。因此，环境动态是对不同强化学习方法进行分类的核心标准。需要环境模型或试图学习环境模型（即环境动态）的强化学习方法被称为*基于模型*方法，区别于*无模型*方法。
- en: '**Model-free and model-based RL**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**无模型和基于模型的强化学习**'
- en: When the probability ![](img/B13208_18_025.png) is known, then the learning
    task can be solved with dynamic programming. But when the dynamics of the environment
    are not known, as it is the case in many real-world problems, then you would need
    to acquire a large number of samples through interacting with the environment
    to compensate for the unknown environment dynamics.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当概率 ![](img/B13208_18_025.png) 已知时，可以通过动态规划来解决学习任务。但当环境的动态未知时（这在许多现实世界问题中是常见的），则需要通过与环境交互获得大量样本，以弥补环境动态的未知。
- en: 'Two main approaches for dealing with this problem are the model-free Monte
    Carlo (MC) and temporal difference (TD) methods. The following chart displays
    the two main categories and the branches of each method:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的两种主要方法是无模型蒙特卡洛（MC）方法和时间差分（TD）方法。下图展示了这两种主要类别及其各自的分支：
- en: '![](img/B13208_18_02.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_02.png)'
- en: We will cover these different approaches and their branches from theory to practical
    algorithms in this chapter.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将从理论到实际算法介绍这些不同的方法及其分支。
- en: The environment dynamics can be considered deterministic if particular actions
    for given states are always or never taken, that is, ![](img/B13208_18_026.png).
    Otherwise, in the more general case, the environment would have stochastic behavior.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在给定状态下的特定动作总是或从不被执行，则可以认为环境动态是确定性的，即 ![](img/B13208_18_026.png)。否则，在更一般的情况下，环境将表现出随机行为。
- en: To make sense of this stochastic behavior, let's consider the probability of
    observing the future state ![](img/B13208_18_027.png) conditioned on the current
    state ![](img/B13208_18_028.png) and the performed action ![](img/B13208_18_029.png).
    This is denoted by ![](img/B13208_18_030.png).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这种随机行为，让我们考虑在当前状态 ![](img/B13208_18_028.png) 和执行的动作 ![](img/B13208_18_029.png)
    条件下观察未来状态 ![](img/B13208_18_027.png) 的概率。这用 ![](img/B13208_18_030.png) 表示。
- en: 'It can be computed as a marginal probability by taking the sum over all possible
    rewards:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以通过对所有可能的奖励求和来计算为边际概率：
- en: '![](img/B13208_18_031.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_031.png)'
- en: This probability is called **state-transition probability**. Based on the state-transition
    probability, if the environment dynamics are deterministic, then it means that
    when the agent takes action ![](img/B13208_18_032.png) at state ![](img/B13208_18_033.png),
    the transition to the next state, ![](img/B13208_18_034.png), will be 100 percent
    certain, that is, ![](img/B13208_18_035.png).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概率称为**状态转移概率**。基于状态转移概率，如果环境动态是确定性的，那么这意味着当智能体在状态 ![](img/B13208_18_033.png)
    执行动作 ![](img/B13208_18_032.png) 时，转移到下一个状态 ![](img/B13208_18_034.png) 将是 100%
    确定的，即 ![](img/B13208_18_035.png)。
- en: Visualization of a Markov process
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 马尔可夫过程的可视化
- en: A Markov process can be represented as a directed cyclic graph in which the
    nodes in the graph represent the different states of the environment. The edges
    of the graph (that is, the connections between the nodes) represent the transition
    probabilities between the states.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫过程可以表示为一个有向环图，其中图中的节点表示环境的不同状态。图的边（即节点之间的连接）表示状态之间的转移概率。
- en: 'For example, let''s consider a student deciding between three different situations:
    (A) studying for an exam at home, (B) playing video games at home, or (C) studying
    at the library. Furthermore, there is a terminal state (T) for going to sleep.
    The decisions are made every hour, and after making a decision, the student will
    remain in a chosen situation for that particular hour. Then, assume that when
    staying at home (state A), there is a 50 percent likelihood that the student switches
    the activity to playing video games. On the other hand, when the student is at
    state B (playing video games), there is a relatively high chance (80 percent)
    that the student will continue playing the video game in the subsequent hours.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们考虑一个学生在三种不同情境间做出选择：(A) 在家备考，(B) 在家玩电子游戏，或者 (C) 在图书馆学习。此外，还存在一个终端状态 (T)，即去睡觉。每小时做出一个决策，做出决策后，学生将在该小时内保持在选择的情境中。假设在家（状态
    A）时，学生有50%的概率会切换到玩电子游戏（状态 B）。而当学生处于状态 B（玩电子游戏）时，学生有相对较高的概率（80%）在接下来的小时继续玩电子游戏。
- en: 'The dynamics of the student''s behavior is shown as a Markov process in the
    following figure, which includes a cyclic graph and a transition table:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 学生行为的动态在下图中作为马尔可夫过程展示，其中包含一个循环图和转移表：
- en: '![](img/B13208_18_03.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_03.png)'
- en: The values on the edges of the graph represent the transition probabilities
    of the student's behavior, and their values are also shown in the table to the
    right. When considering the rows in the table, please note that the transition
    probabilities coming out of each state (node) always sum to 1.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图中边缘的数值表示学生行为的转移概率，这些数值也在右侧的表格中显示。查看表格中的行时，请注意，从每个状态（节点）出来的转移概率总和始终为1。
- en: Episodic versus continuing tasks
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回合任务与持续任务
- en: As the agent interacts with the environment, the sequence of observations or
    states forms a trajectory. There are two types of trajectories. If an agent's
    trajectory can be divided into subparts such that each starts at time *t* = 0
    and ends in a terminal state ![](img/B13208_18_036.png) (at *t* = *T*), the task
    is called an *episodic task*. On the other hand, if the trajectory is infinitely
    continuous without a terminal state, the task is called a *continuing task*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 随着代理与环境的互动，观察或状态的序列形成一条轨迹。轨迹有两种类型。如果一个代理的轨迹可以被分割成若干子部分，使得每部分都从时间 *t* = 0 开始，并以终端状态
    ![](img/B13208_18_036.png) 结束（在 *t* = *T* 时），则该任务被称为 *回合任务*。另一方面，如果轨迹是无限连续的，并且没有终端状态，则该任务被称为
    *持续任务*。
- en: The task related to a learning agent for the game of chess is an episodic task,
    whereas a cleaning robot that is keeping a house tidy is typically performing
    a continuing task. In this chapter, we only consider episodic tasks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与学习代理相关的任务，例如国际象棋游戏，是一个回合任务；而保持房屋整洁的清洁机器人通常执行的是一个持续任务。本章中，我们仅考虑回合任务。
- en: 'In episodic tasks, an **episode** is a sequence or trajectory that an agent
    takes from a starting state, ![](img/B13208_18_006.png), to a terminal state,
    ![](img/B13208_18_036.png):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在回合任务中，**回合**是一个代理从起始状态 ![](img/B13208_18_006.png) 到终端状态 ![](img/B13208_18_036.png)
    所经历的序列或轨迹：
- en: '![](img/B13208_18_039.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_039.png)'
- en: 'For the Markov process shown in the previous figure, which depicts the task
    of a student studying for an exam, we may encounter episodes like the following
    three examples:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前述图中展示的马尔可夫过程，该过程描述了学生为考试学习的任务，我们可能会遇到如下三个示例的回合：
- en: '![](img/B13208_18_040.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_040.png)'
- en: 'RL terminology: return, policy, and value function'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习术语：回报、策略和值函数
- en: Next, let's define some additional RL-specific terminology that we will need
    for the remainder of this chapter.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一些剩余章节中将用到的强化学习（RL）专有术语。
- en: The return
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回报
- en: The so-called return at time *t* is the cumulated reward obtained from the entire
    duration of an episode. Recall that ![](img/B13208_18_041.png) is the *immediate
    reward* obtained after performing an action, ![](img/B13208_18_042.png), at time
    *t*; the *subsequent* rewards are ![](img/B13208_18_043.png), ![](img/B13208_18_044.png),
    and so forth.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 所谓的时间 *t* 时的回报是从整个回合期间获得的累计奖励。回想一下，![](img/B13208_18_041.png) 是执行动作 ![](img/B13208_18_042.png)
    后，在时间 *t* 获得的 *即时奖励*；*后续*奖励则是 ![](img/B13208_18_043.png)，![](img/B13208_18_044.png)，依此类推。
- en: 'The return at time *t* can then be calculated from the immediate reward as
    well as the subsequent ones, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 时间 *t* 时的回报可以通过即时奖励以及后续奖励计算得出，公式如下：
- en: '![](img/B13208_18_045.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_045.png)'
- en: Here, ![](img/B13208_18_046.png) is the *discount factor* in range [0, 1]. The
    parameter ![](img/B13208_18_047.png) indicates how much the future rewards are
    "worth" at the current moment (time *t*). Note that by setting ![](img/B13208_18_048.png),
    we would imply that we do not care about future rewards. In this case, the return
    will be equal to the immediate reward, ignoring the subsequent rewards after *t*
    + 1, and the agent will be short-sighted. On the other hand, if ![](img/B13208_18_049.png),
    the return will be the unweighted sum of all subsequent rewards.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_18_046.png)是[0, 1]范围内的*折现因子*。参数 ![](img/B13208_18_047.png)表示当前时刻（时间*t*）未来奖励的“价值”。请注意，通过设置
    ![](img/B13208_18_048.png)，我们实际上是在表明不关心未来的奖励。在这种情况下，回报将等于即时奖励，忽略*t*+1之后的奖励，智能体将变得目光短浅。另一方面，如果
    ![](img/B13208_18_049.png)，则回报将是所有后续奖励的无权重总和。
- en: 'Moreover, note that the equation for the return can be expressed in a simpler
    way using a *recursion* as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，回报的方程可以通过*递归*的方式更简洁地表达，如下所示：
- en: '![](img/B13208_18_050.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_050.png)'
- en: This means that the return at time *t* is equal to the immediate reward *r*
    plus the discounted future-return at time *t* + 1\. This is a very important property,
    which facilitates the computations of the return.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在时间*t*时的回报等于即时奖励*r*加上时间*t*+1时折现后的未来回报。这是一个非常重要的属性，便于回报的计算。
- en: '**Intuition behind discount factor**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**折现因子的直觉**'
- en: 'To get an understanding of the discount factor, consider the following figure
    showing the value of earning a $100 bill today compared to earning it in a year
    from now. Under certain economic situations, like inflation, earning this $100
    bill right now could be worth more than earning it in future:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解折现因子的含义，可以参考以下图示，展示今天赚取100美元与一年后赚取100美元的价值。在某些经济情况下，比如通货膨胀，今天赚到100美元的价值可能比一年后赚到更多：
- en: '![](img/B13208_18_04.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_04.png)'
- en: Therefore, we say that if this bill is worth $100 right now, then it would be
    worth $90 in a year with a discount factor ![](img/B13208_18_051.png).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以说，如果这张票现在值100美元，那么在一年后，考虑到折现因子，它的价值将是90美元！[](img/B13208_18_051.png)。
- en: Let's compute the return at different time steps for the episodes in our previous
    student example. Assume ![](img/B13208_18_052.png), and that the only reward given
    is based on the result of the exam (+1 for passing the exam, and –1 for failing
    it). The rewards for intermediate time steps are 0.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算在前面学生示例中的不同时间步的回报。假设 ![](img/B13208_18_052.png)，并且唯一的奖励是基于考试结果（通过考试得+1，未通过得–1）。中间时间步的奖励为0。
- en: '![](img/B13208_18_053.png):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B13208_18_053.png)：'
- en: '![](img/B13208_18_054.png)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B13208_18_054.png)'
- en: '![](img/B13208_18_055.png)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B13208_18_055.png)'
- en: '![](img/B13208_18_056.png)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B13208_18_056.png)'
- en: '...'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '...'
- en: '![](img/B13208_18_057.png)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B13208_18_057.png)'
- en: '![](img/B13208_18_058.png)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B13208_18_058.png)'
- en: '![](img/B13208_18_059.png):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B13208_18_059.png)：'
- en: '![](img/B13208_18_060.png)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B13208_18_060.png)'
- en: '![](img/B13208_18_061.png)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B13208_18_061.png)'
- en: '...'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '...'
- en: '![](img/B13208_18_062.png)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B13208_18_062.png)'
- en: '![](img/B13208_18_063.png)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B13208_18_063.png)'
- en: We leave the computation of the returns for the third episode as an exercise
    for the reader.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将回报的计算留给第三集作为读者的练习。
- en: Policy
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略
- en: 'A *policy* typically denoted by ![](img/B13208_18_064.png) is a function that
    determines the next action to take, which can be either deterministic, or stochastic
    (that is, the probability for taking the next action). A stochastic policy then
    has a probability distribution over actions that an agent can take at a given
    state:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通常用 ![](img/B13208_18_064.png)表示的*策略*是一个函数，用来决定采取什么动作，动作可以是确定性的，也可以是随机的（即，采取下一步动作的概率）。随机策略则有一个动作的概率分布，表示在给定状态下智能体可以采取的动作：
- en: '![](img/B13208_18_065.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_065.png)'
- en: During the learning process, the policy may change as the agent gains more experience.
    For example, the agent may start from a random policy, where the probability of
    all actions is uniform; meanwhile, the agent will hopefully learn to optimize
    its policy toward reaching the optimal policy. The *optimal policy* ![](img/B13208_18_066.png)
    is the policy that yields the highest return.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习过程中，随着智能体积累更多经验，策略可能会发生变化。例如，智能体可能从一个随机策略开始，在这种策略下，所有动作的概率是均匀的；与此同时，智能体希望能够学习并优化其策略，朝着最优策略靠近。*最优策略*
    ![](img/B13208_18_066.png)是能带来最高回报的策略。
- en: Value function
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 价值函数
- en: The *value function*, also referred to as the *state-value function*, measures
    the *goodness* of each state—in other words, how good or bad it is to be in a
    particular state. Note that the criterion for goodness is based on the return.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*价值函数*，也称为*状态价值函数*，衡量每个状态的*优劣*——换句话说，就是处于某个特定状态时，这个状态好坏的衡量标准。请注意，优劣的标准是基于回报的。'
- en: 'Now, based on the return ![](img/B13208_18_067.png), we define the value function
    of state *s* as the expected return (the average return over all possible episodes)
    after *following policy* ![](img/B13208_18_068.png):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，基于回报 ![](img/B13208_18_067.png)，我们将状态 *s* 的价值函数定义为在*遵循策略* ![](img/B13208_18_068.png)
    后的期望回报（所有可能情境的平均回报）：
- en: '![](img/B13208_18_069.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_069.png)'
- en: In an actual implementation, we usually estimate the value function using lookup
    tables, so we do not have to recompute it multiple times. (This is the dynamic
    programming aspect.) For example, in practice, when we estimate the value function
    using such tabular methods, we store all the state values in a table denoted by
    *V*(*s*). In a Python implementation, this could be a list or a NumPy array whose
    indexes refer to different states; or, it could be a Python dictionary, where
    the dictionary keys map the states to the respective values.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际实现中，我们通常使用查找表来估算价值函数，这样就不必多次重新计算它。（这就是动态规划的方面。）例如，在实际应用中，当我们使用这种表格方法来估算价值函数时，我们将所有状态值存储在一个由
    *V*(*s*) 表示的表格中。在 Python 实现中，这可能是一个列表或一个 NumPy 数组，其索引指向不同的状态；或者，它也可以是一个 Python
    字典，其中字典的键将状态映射到相应的值。
- en: 'Moreover, we can also define a value for each state-action pair, which is called
    the *action-value function* and is denoted by ![](img/B13208_18_070.png). The
    action-value function refers to the expected return ![](img/B13208_18_071.png)
    when the agent is at state ![](img/B13208_18_072.png) and takes action ![](img/B13208_18_073.png).
    Extending the definition of state-value function to state-action pairs, we get
    the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以为每个状态-动作对定义一个值，这个值称为*动作价值函数*，表示为 ![](img/B13208_18_070.png)。动作价值函数指的是在代理处于状态
    ![](img/B13208_18_072.png) 并执行动作 ![](img/B13208_18_073.png) 时的期望回报 ![](img/B13208_18_071.png)。将状态价值函数的定义扩展到状态-动作对，我们得到以下公式：
- en: '![](img/B13208_18_074.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_074.png)'
- en: Similar to referring to the optimal policy as ![](img/B13208_18_075.png), ![](img/B13208_18_076.png)
    and ![](img/B13208_18_077.png) also denote the optimal state-value and action-value
    functions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 与最优策略的表示类似，![](img/B13208_18_075.png)、![](img/B13208_18_076.png) 和 ![](img/B13208_18_077.png)
    同样表示最优的状态价值函数和动作价值函数。
- en: Estimating the value function is an essential component of RL methods. We will
    cover different ways of calculating and estimating the state-value function and
    action-value function later in this chapter.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 估算价值函数是强化学习（RL）方法中的一个重要组成部分。我们将在本章稍后讨论计算和估算状态价值函数和动作价值函数的不同方法。
- en: '**The difference between the reward, return, and value function**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**奖励、回报和价值函数之间的区别**'
- en: The **reward** is a consequence of the agent taking an action given the current
    state of the environment. In other words, the reward is a signal that the agent
    receives when performing an action to transition from one state to the next. However,
    remember that not every action yields a positive or negative reward—think back
    to our chess example where a positive reward is only received upon winning the
    game, and the reward for all intermediate actions is zero.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**奖励**是代理在给定环境当前状态下执行某个动作后获得的结果。换句话说，奖励是代理在执行动作以从一个状态转移到下一个状态时收到的信号。然而，请记住，并不是每个动作都会产生正或负的奖励——回想一下我们的象棋示例，只有在赢得比赛时才会获得正奖励，所有中间动作的奖励都是零。'
- en: A state itself has a certain value, which we assign to it to measure how good
    or bad this state is – this is where the value function comes into play. Typically,
    the states with a "high" or "good" value are those states that have a high expected
    **return** and will likely yield a high reward given a particular policy.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一个状态本身有一个特定的价值，我们为它赋值以衡量这个状态的优劣——这就是价值函数的作用所在。通常，具有“高”或“好”价值的状态是那些具有高期望**回报**的状态，并且在给定特定策略时，很可能会产生高奖励。
- en: For example, let's consider a chess-playing computer once more. A positive reward
    may only be given at the end of the game if the computer wins the game. There
    is no (positive) reward if the computer loses the game. Now, imagine the computer
    performs a particular chess move that captures the opponent's queen without any
    negative consequences for the computer. Since the computer only receives a reward
    for winning the game, it does not get an immediate reward by making this move
    that captures the opponent's queen. However, the new state (the state of the board
    after capturing the queen) may have a **high value**, which may yield a reward
    (if the game is won afterward). Intuitively, we can say that the high value associated
    with capturing the opponent's queen is associated with the fact that capturing
    the queen often results in winning the game—and thus the high expected return,
    or value. However, note that capturing the opponent's queen does not always lead
    to winning the game; hence, the agent is likely to receive a positive reward,
    but it is not guaranteed.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们再次考虑一个下棋的计算机。如果计算机赢得比赛，可能只有在比赛结束时才会给予正向奖励。如果计算机输掉比赛，则不会有（正向）奖励。现在，假设计算机进行了一步棋，捕获了对手的皇后，并且这对计算机没有负面影响。由于计算机只有在赢得比赛时才会获得奖励，因此通过这一捕获对手皇后的动作，计算机并不会立即获得奖励。然而，新的状态（捕获皇后后的棋盘状态）可能具有**较高的价值**，这可能会在比赛最终获胜时带来奖励。从直觉上讲，我们可以认为，捕获对手皇后的高价值与捕获皇后通常会导致赢得比赛这一事实相关联——因此有较高的预期回报或价值。然而，请注意，捕获对手的皇后并不总是意味着赢得比赛；因此，智能体很可能会获得正向奖励，但这并不保证。
- en: In short, the **return** is the weighted sum of **rewards** for an entire episode,
    which would be equal to the discounted final reward in our chess example (since
    there is only one reward). The **value function** is the expectation over all
    possible episodes, which basically computes how "valuable" it is on average to
    make a certain move.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，**回报**是整个回合**奖励**的加权和，在我们的国际象棋示例中，它将等于折扣后的最终奖励（因为只有一个奖励）。**值函数**是所有可能回合的期望，它基本上计算的是做出某个特定动作的“价值”平均值。
- en: Before we move directly ahead into some RL algorithms, let's briefly go over
    the derivation for the Bellman equation, which we can use to implement the policy
    evaluation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们直接进入一些强化学习（RL）算法之前，先简要回顾一下贝尔曼方程的推导过程，我们可以利用它来实现策略评估。
- en: Dynamic programming using the Bellman equation
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用贝尔曼方程的动态规划
- en: The Bellman equation is one of the central elements of many RL algorithms. The
    Bellman equation simplifies the computation of the value function, such that rather
    than summing over multiple time steps, it uses a recursion that is similar to
    the recursion for computing the return.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程是许多强化学习算法的核心元素之一。贝尔曼方程简化了值函数的计算，避免了对多个时间步的求和，而是使用类似于计算回报的递归。
- en: 'Based on the recursive equation for the total return ![](img/B13208_18_078.png),
    we can rewrite the value function as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 基于总回报的递归方程！[](img/B13208_18_078.png)，我们可以将值函数重写如下：
- en: '![](img/B13208_18_079.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_079.png)'
- en: Notice that the immediate reward *r* is taken out of the expectation since it
    is a constant and known quantity at time *t*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，即时奖励*r*被从期望值中移除，因为它是一个常量，并且在时刻*t*时是已知的。
- en: 'Similarly, for the action-value function, we could write:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于动作值函数，我们可以写成：
- en: '![](img/B13208_18_080.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_080.png)'
- en: 'We can use the environment dynamics to compute the expectation via summing
    over all probabilities of the next state ![](img/B13208_18_081.png) and the corresponding
    rewards *r*:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用环境动态来通过对所有可能的下一状态！[](img/B13208_18_081.png)及相应的奖励*r*的概率求和，来计算期望：
- en: '![](img/B13208_18_082.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_082.png)'
- en: 'Now, we can see that expectation of the return, ![](img/B13208_18_083.png),
    is essentially the state-value function ![](img/B13208_18_084.png). So, we can
    write ![](img/B13208_18_085.png) as a function of ![](img/B13208_18_086.png):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到回报的期望！[](img/B13208_18_083.png)，本质上是状态值函数！[](img/B13208_18_084.png)。所以，我们可以将！[](img/B13208_18_085.png)写成！[](img/B13208_18_086.png)的函数：
- en: '![](img/B13208_18_087.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_087.png)'
- en: This is called the **Bellman equation**, which relates the value function for
    a state, *s,* to the value function of its subsequent state, ![](img/B13208_18_088.png).
    This greatly simplifies the computation of the value function because it eliminates
    the iterative loop along the time axis.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为**贝尔曼方程**，它将一个状态*s*的价值函数与其后继状态的价值函数 ![](img/B13208_18_088.png) 关联起来。这大大简化了价值函数的计算，因为它消除了沿时间轴的迭代循环。
- en: Reinforcement learning algorithms
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习算法
- en: 'In this section, we will cover a series of learning algorithms. We will start
    with dynamic programming, which assumes that the transition dynamics (or the environment
    dynamics, that is, ![](img/B13208_18_089.png), are known. However, in most RL
    problems, this is not the case. To work around the unknown environment dynamics,
    RL techniques were developed that learn through interacting with the environment.
    These techniques include MC, TD learning, and the increasingly popular Q-learning
    and deep Q-learning approaches. The following figure describes the course of advancing
    RL algorithms, from dynamic programming to Q-learning:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一系列学习算法。我们将从动态规划开始，假设转移动态（或者环境动态，即 ![](img/B13208_18_089.png)）是已知的。然而，在大多数强化学习（RL）问题中，情况并非如此。为了应对未知的环境动态，开发了通过与环境互动来学习的RL技术。这些技术包括MC、TD学习，以及日益流行的Q学习和深度Q学习方法。下图描述了RL算法的进展过程，从动态规划到Q学习：
- en: '![](img/B13208_18_05.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_05.png)'
- en: In the following sections of this chapter, we will step through each of these
    RL algorithms. We will start with dynamic programming, before moving on to MC,
    and finally on to TD and its branches of on-policy **SARSA** (**state–action–reward–state–action**)
    and off-policy Q-learning. We will also move into deep Q-learning while we build
    some practical models.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后续部分，我们将逐步讲解这些RL算法。我们将从动态规划开始，然后是MC，最后是TD及其分支：基于策略的**SARSA**（**状态–动作–奖励–状态–动作**）和基于离策略的Q学习。我们还将进入深度Q学习，同时构建一些实际模型。
- en: Dynamic programming
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态规划
- en: 'In this section, we will focus on solving RL problems under the following assumptions:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点解决在以下假设下的RL问题：
- en: We have full knowledge about the environment dynamics; that is, all transition
    probabilities ![](img/B13208_18_090.png) are known.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对环境动态有完整的知识；也就是说，所有的转移概率 ![](img/B13208_18_090.png) 都是已知的。
- en: The agent's state has the Markov property, which means that the next action
    and reward only depend on the current state and the choice of action we make at
    this moment or current time step.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体的状态具有马尔可夫性质，这意味着下一个动作和奖励仅依赖于当前状态以及我们在此时刻或当前时间步选择的动作。
- en: The mathematical formulation for RL problems using a Markov decision process
    (MDP) was introduced earlier in this chapter. If you need a refresher, please
    refer to the section called *The mathematical formulation of Markov decision processes*,
    which introduced the formal definition of the value function ![](img/B13208_18_091.png)
    following the policy ![](img/B13208_18_092.png), and the Bellman equation, which
    was derived using the environment dynamics.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用马尔可夫决策过程（MDP）来描述RL问题的数学公式已在本章前面介绍。如果需要复习，请参考名为*马尔可夫决策过程的数学公式*的章节，其中介绍了价值函数的正式定义
    ![](img/B13208_18_091.png)，跟随策略 ![](img/B13208_18_092.png)，以及通过环境动态推导出的贝尔曼方程。
- en: We should emphasize that dynamic programming is not a practical approach for
    solving RL problems. The problem with using dynamic programming is that it assumes
    full knowledge of the environment dynamics, which is usually unreasonable or impractical
    for most real-world applications. However, from an educational standpoint, dynamic
    programming helps with introducing RL in a simple fashion and motivates the use
    of more advanced and complicated RL algorithms.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该强调，动态规划并不是解决RL问题的实际方法。使用动态规划的一个问题是它假设已知环境动态，但在大多数现实应用中，这通常是不可行或不现实的。然而，从教育的角度来看，动态规划有助于以简单的方式引入RL，并激发使用更先进和复杂的RL算法的兴趣。
- en: 'There are two main objectives via the tasks described in the following subsections:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节描述的任务有两个主要目标：
- en: Obtain the true state-value function, ![](img/B13208_18_093.png); this task
    is also known as the prediction task and is accomplished with *policy evaluation*.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取真实的状态值函数，![](img/B13208_18_093.png)；此任务也称为预测任务，通过*策略评估*来完成。
- en: Find the optimal value function, ![](img/B13208_18_094.png), which is accomplished
    via *generalized policy iteration*.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到最优值函数，![](img/B13208_18_094.png)，这是通过*广义策略迭代*来完成的。
- en: Policy evaluation – predicting the value function with dynamic programming
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略评估——使用动态规划预测值函数
- en: 'Based on the Bellman equation, we can compute the value function for an arbitrary
    policy ![](img/B13208_18_095.png) with dynamic programming when the environment
    dynamics are known. For computing this value function, we can adapt an iterative
    solution, where we start from ![](img/B13208_18_096.png), which is initialized
    to zero values for each state. Then, at each iteration *i* + 1, we update the
    values for each state based on the Bellman equation, which is, in turn, based
    on the values of states from a previous iteration *i*, as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 基于贝尔曼方程，当已知环境动态时，我们可以通过动态规划计算任意策略 ![](img/B13208_18_095.png) 的值函数。为了计算这个值函数，我们可以采用迭代解法，从
    ![](img/B13208_18_096.png) 开始，该值对于每个状态初始化为零。然后，在每次迭代 *i* + 1 时，我们根据贝尔曼方程更新每个状态的值，而贝尔曼方程又基于上一迭代
    *i* 的状态值，具体如下：
- en: '![](img/B13208_18_097.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_097.png)'
- en: It can be shown that as the iterations increase to infinity, ![](img/B13208_18_098.png)
    converges to the true state-value function ![](img/B13208_18_093.png).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明，当迭代次数趋于无穷大时，![](img/B13208_18_098.png) 会收敛到真实的状态值函数 ![](img/B13208_18_093.png)。
- en: Also, notice here that we do not need to interact with the environment. The
    reason for this is that we already know the environment dynamics accurately. As
    a result, we can leverage this information and estimate the value function easily.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请注意，我们在这里不需要与环境互动。原因是我们已经准确了解了环境动态。因此，我们可以利用这些信息，轻松估算值函数。
- en: After computing the value function, an obvious question is how that value function
    can be useful for us if our policy is still a random policy. The answer is that
    we can actually use this computed ![](img/B13208_18_085.png) to improve our policy,
    as we will see next.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 计算完值函数后，一个显而易见的问题是，如果我们的策略仍然是随机策略，那么该值函数如何对我们有用。答案是，我们实际上可以利用这个计算出的 ![](img/B13208_18_085.png)
    来改进我们的策略，正如我们接下来将看到的那样。
- en: Improving the policy using the estimated value function
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用估计的值函数改进策略
- en: 'Now that we have computed the value function ![](img/B13208_18_101.png) by
    following the existing policy, ![](img/B13208_18_102.png), we want to use ![](img/B13208_18_103.png)
    and improve the existing policy, ![](img/B13208_18_104.png). This means that we
    want to find a new policy, ![](img/B13208_18_105.png), that for each state, *s*,
    following ![](img/B13208_18_106.png) would yield higher or at least equal value
    than using the current policy, ![](img/B13208_18_107.png). In mathematical terms,
    we can express this objective for the improved policy, ![](img/B13208_18_108.png),
    as:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经通过遵循现有策略 ![](img/B13208_18_102.png) 计算出了值函数 ![](img/B13208_18_101.png)，我们希望利用
    ![](img/B13208_18_103.png) 来改进现有的策略 ![](img/B13208_18_104.png)。这意味着我们希望找到一个新的策略，![](img/B13208_18_105.png)，对于每个状态
    *s*，遵循 ![](img/B13208_18_106.png) 会比使用当前策略 ![](img/B13208_18_107.png) 产生更高的值或至少相等的值。用数学语言表达，我们可以将改进策略
    ![](img/B13208_18_108.png) 的目标表示为：
- en: '![](img/B13208_18_109.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_109.png)'
- en: First, recall that a policy, ![](img/B13208_18_068.png), determines the probability
    of choosing each action, *a,* while the agent is at state *s*. Now, in order to
    find ![](img/B13208_18_111.png) that always has a better or equal value for each
    state, we first compute the action-value function, ![](img/B13208_18_112.png),
    for each state, *s,* and action, *a*, based on the computed state value using
    the value function ![](img/B13208_18_113.png). We iterate through all the states,
    and for each state, *s*, we compare the value of the next state ![](img/B13208_18_114.png),
    that would occur if action *a* was selected.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，回顾一下，策略 ![](img/B13208_18_068.png) 决定了在代理处于状态 *s* 时选择每个动作 *a* 的概率。现在，为了找到
    ![](img/B13208_18_111.png)，确保每个状态都具有更好或至少相等的值，我们首先基于计算出的状态值（使用值函数 ![](img/B13208_18_113.png)）为每个状态
    *s* 和动作 *a* 计算动作值函数 ![](img/B13208_18_112.png)。我们遍历所有状态，对于每个状态 *s*，我们比较如果选择动作
    *a* 时，下一状态 ![](img/B13208_18_114.png) 的值。
- en: After we have obtained the highest state value by evaluating all state-action
    pairs via ![](img/B13208_18_115.png), we can compare the corresponding action
    with the action selected by the current policy. If the action suggested by the
    current policy (that is, ![](img/B13208_18_116.png)) is different than the action
    suggested by the action-value function (that is, ![](img/B13208_18_117.png)),
    then we can update the policy by reassigning the probabilities of actions to match
    the action that gives the highest action value, ![](img/B13208_18_118.png). This
    is called the *policy improvement* algorithm.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过 ![](img/B13208_18_115.png) 评估所有状态-动作对后获得最高状态值后，我们可以将对应的动作与当前策略选择的动作进行比较。如果当前策略建议的动作（即
    ![](img/B13208_18_116.png)）与动作值函数建议的动作（即 ![](img/B13208_18_117.png)）不同，那么我们可以通过重新分配动作的概率来更新策略，以匹配给出最高动作值的动作，即
    ![](img/B13208_18_118.png)。这被称为 *策略改进* 算法。
- en: Policy iteration
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略迭代
- en: Using the policy improvement algorithm described in the previous subsection,
    it can be shown that the policy improvement will strictly yield a better policy,
    unless the current policy is already optimal (which means ![](img/B13208_18_119.png)
    for each ![](img/B13208_18_120.png)). Therefore, if we iteratively perform policy
    evaluation followed by policy improvement, we are guaranteed to find the optimal
    policy.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上一小节中描述的策略改进算法，可以证明，策略改进将严格地产生一个更好的策略，除非当前策略已经是最优的（这意味着对于每个 ![](img/B13208_18_120.png)，都有
    ![](img/B13208_18_119.png)）。因此，如果我们反复执行策略评估然后进行策略改进，我们就能确保找到最优策略。
- en: Note that this technique is referred to as **generalized policy iteration**
    (**GPI**), which is common among many RL methods. We will use the GPI in later
    sections of this chapter for the MC and TD learning methods.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种技术被称为 **广义策略迭代**（**GPI**），它在许多强化学习方法中很常见。我们将在本章的后续部分中使用 GPI 来实现蒙特卡洛和时序差分学习方法。
- en: Value iteration
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 值迭代
- en: 'We saw that by repeating the policy evaluation (compute ![](img/B13208_18_121.png)
    and ![](img/B13208_18_122.png)) and policy improvement (finding ![](img/B13208_18_123.png)
    such that ![](img/B13208_18_124.png)), we can reach the optimal policy. However,
    it can be more efficient if we combine the two tasks of policy evaluation and
    policy improvement into a single step. The following equation updates the value
    function for iteration *i* + 1 (denoted by ![](img/B13208_18_125.png)) based on
    the action that maximizes the weighted sum of the next state value and its immediate
    reward (![](img/B13208_18_126.png)):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，通过反复进行策略评估（计算 ![](img/B13208_18_121.png) 和 ![](img/B13208_18_122.png)）和策略改进（找到
    ![](img/B13208_18_123.png)，使得 ![](img/B13208_18_124.png)），我们可以达到最优策略。然而，如果我们将策略评估和策略改进的两个任务合并为一个步骤，那么可以提高效率。以下方程式更新迭代
    *i* + 1 的值函数（用 ![](img/B13208_18_125.png) 表示），它基于选择最大化下一个状态值和即时奖励的加权和的动作（![](img/B13208_18_126.png)）。
- en: '![](img/B13208_18_127.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_127.png)'
- en: In this case, the updated value for ![](img/B13208_18_128.png) is maximized
    by choosing the best action out of all possible actions, whereas in policy evaluation,
    the updated value was using the weighted sum over all actions.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，更新后的值是通过从所有可能的动作中选择最佳动作来最大化的，而在策略评估中，更新的值是通过对所有动作的加权和来计算的。
- en: '**Notation for tabular estimates of the state value and action value functions**'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**状态值函数和动作值函数的表格估计符号**'
- en: In most RL literature and textbooks, the lowercase ![](img/B13208_18_129.png)
    and ![](img/B13208_18_130.png) are used to refer to the true state-value and true
    action-value functions, respectively, as mathematical functions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数强化学习文献和教科书中，使用小写字母 ![](img/B13208_18_129.png) 和 ![](img/B13208_18_130.png)
    来表示真实的状态值函数和真实的动作值函数，分别作为数学函数。
- en: Meanwhile, for practical implementations, these value functions are defined
    as lookup tables. The tabular estimates of these value functions are denoted by
    ![](img/B13208_18_131.png) and ![](img/B13208_18_132.png). We will also use this
    notation in this chapter.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，对于实际的实现，这些值函数被定义为查找表。这些值函数的表格估计用 ![](img/B13208_18_131.png) 和 ![](img/B13208_18_132.png)
    表示。本章中我们也将使用这种符号。
- en: Reinforcement learning with Monte Carlo
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒙特卡洛强化学习
- en: As we saw in the previous section on dynamic programming, it relies on a simplistic
    assumption that the environment's dynamics are fully known. Moving away from the
    dynamic programming approach, we now assume that we do not have any knowledge
    about the environment dynamics.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节动态规划中看到的，它依赖于一个简单的假设，即环境的动态是完全已知的。脱离动态规划方法，我们现在假设我们对环境的动态一无所知。
- en: That is, we do not know the state-transition probabilities of the environment,
    and instead, we want the agent to learn through *interacting* with the environment.
    Using MC methods, the learning process is based on the so-called *simulated experience*.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们不知道环境的状态转移概率，而是希望代理通过*与环境交互*来学习。使用MC方法，学习过程基于所谓的*模拟经验*。
- en: For MC-based RL, we define an agent class that follows a probabilistic policy,
    ![](img/B13208_18_133.png), and based on this policy, our agent takes an action
    at each step. This results in a simulated episode.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于MC的强化学习，我们定义一个遵循概率策略的代理类，![](img/B13208_18_133.png)，并根据这个策略，代理在每一步采取一个动作。这会生成一个模拟情节。
- en: Earlier, we defined the state-value function, such that the value of a state
    indicates the expected return from that state. In dynamic programming, this computation
    relied on the knowledge of the environment dynamics, that is, ![](img/B13208_18_134.png).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，我们定义了状态值函数，其中状态的值表示从该状态开始的预期回报。在动态规划中，这一计算依赖于对环境动态的了解，即！[](img/B13208_18_134.png)。
- en: However, from now on, we will develop algorithms that do not require the environment
    dynamics. MC-based methods solve this problem by generating simulated episodes
    where an agent interacts with the environment. From these simulated episodes,
    we will be able to compute the average return for each state visited in that simulated
    episode.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从现在开始，我们将开发不需要环境动态的算法。基于MC的方法通过生成模拟情节来解决这个问题，代理与环境进行交互。从这些模拟情节中，我们将能够计算在该模拟情节中访问的每个状态的平均回报。
- en: State-value function estimation using MC
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用MC进行状态值函数估计
- en: After generating a set of episodes, for each state, *s*, the set of episodes
    that all pass through state *s* is considered for calculating the value of state
    *s*. Let's assume that a lookup table is used for obtaining the value corresponding
    to the value function, ![](img/B13208_18_135.png). MC updates for estimating the
    value function are based on the total return obtained in that episode starting
    from the first time that state *s* is visited. This algorithm is called *first-visit
    Monte Carlo* value prediction.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成一组情节后，对于每个状态*s*，考虑所有通过状态*s*的情节集合来计算状态*s*的值。假设使用查找表来获取与值函数对应的值，![](img/B13208_18_135.png)。MC更新用于估计值函数，基于从第一次访问状态*s*开始的情节中获得的总回报。这个算法被称为*首次访问蒙特卡罗*值预测。
- en: Action-value function estimation using MC
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用MC进行动作值函数估计
- en: When the environment dynamics are known, we can easily infer the action-value
    function from a state-value function by looking one step ahead to find the action
    that gives the maximum value, as was shown in the *Dynamic programming* section.
    However, this is not feasible if the environment dynamics are unknown.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当环境的动态已知时，我们可以通过向前看一步来推断出动作值函数，找到给出最大值的动作，正如*动态规划*部分所示。然而，如果环境动态未知，这是不可行的。
- en: To solve this issue, we can extend the algorithm for estimating the first-visit
    MC state-value prediction. For instance, we can compute the *estimated* return
    for each state-action pair using the action-value function. To obtain this estimated
    return, we consider visits to each state-action pair (*s*, *a*), which refers
    to visiting state *s* and taking action *a*.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以扩展算法，用于估计首次访问MC状态值预测。例如，我们可以使用动作值函数来计算每个状态-动作对的*估计*回报。为了获得这个估计回报，我们考虑访问每个状态-动作对（*s*，*a*），这指的是访问状态*s*并采取动作*a*。
- en: However, a problem arises since some actions may never be selected, resulting
    in insufficient exploration. There are a few ways to resolve this. The simplest
    approach is called *exploratory start*, which assumes that every state-action
    pair has a non zero probability at the beginning of the episode.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，出现了一个问题，因为有些动作可能永远不会被选择，从而导致探索不足。有几种方法可以解决这个问题。最简单的方法叫做*探索性起始*，它假设每个状态-动作对在情节开始时有一个非零的概率。
- en: Another approach to deal with this lack-of-exploration issue is called ![](img/B13208_18_136.png)-*greedy
    policy*, which will be discussed in the next section on policy improvement.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 解决探索不足问题的另一种方法叫做 ![](img/B13208_18_136.png)-*贪婪策略*，将在下一节关于策略改进的内容中讨论。
- en: Finding an optimal policy using MC control
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 MC 控制找到最优策略
- en: '*MC control* refers to the optimization procedure for improving a policy. Similar
    to the policy iteration approach in previous section (*Dynamic programming*),
    we can repeatedly alternate between policy evaluation and policy improvement until
    we reach the optimal policy. So, starting from a random policy, ![](img/B13208_18_137.png),
    the process of alternating between policy evaluation and policy improvement can
    be illustrated as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '*MC 控制*是指优化过程以改进策略。类似于前一节中的策略迭代方法（*动态规划*），我们可以反复在策略评估和策略改进之间交替，直到得到最优策略。因此，从一个随机策略开始，![](img/B13208_18_137.png)，策略评估和策略改进交替进行的过程可以表示如下：'
- en: '![](img/B13208_18_138.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_138.png)'
- en: Policy improvement – computing the greedy policy from the action-value function
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略改进——从动作-价值函数计算贪婪策略
- en: 'Given an action-value function, *q*(*s*, *a*), we can generate a greedy (deterministic)
    policy as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个动作-价值函数，*q*(*s*, *a*)，我们可以生成如下的贪婪（确定性）策略：
- en: '![](img/B13208_18_139.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_139.png)'
- en: In order to avoid the lack-of-exploration problem, and to consider the non-visited
    state-action pairs as discussed earlier, we can let the non-optimal actions have
    a small chance (![](img/B13208_18_140.png)) to be chosen. This is called the ![](img/B13208_18_141.png)-greedy
    policy, according to which, all non-optimal actions at state *s* have a minimal
    ![](img/B13208_18_142.png) probability of being selected (instead of 0), and the
    optimal action has a probability of ![](img/B13208_18_143.png) (instead of 1).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免探索不足问题，并考虑到之前讨论的未访问的状态-动作对，我们可以让非最优动作有一个小的概率（![](img/B13208_18_140.png)）被选择。这被称为
    ![](img/B13208_18_141.png)-贪婪策略，根据该策略，所有在状态 *s* 下的非最优动作都有一个最小的 ![](img/B13208_18_142.png)
    概率被选择（而不是 0），而最优动作的概率是 ![](img/B13208_18_143.png)（而不是 1）。
- en: Temporal difference learning
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时序差分学习
- en: So far, we have seen two fundamental RL techniques, dynamic programming and
    MC-based learning. Recall that dynamic programming relies on the complete and
    accurate knowledge of the environment dynamics. The MC-based method, on the other
    hand, learns by simulated experience. In this section, we will now introduce a
    third RL method called TD learning, which can be considered as an improvement
    or extension of the MC-based RL approach.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了两种基本的强化学习技术，动态规划和基于 MC 的学习。回想一下，动态规划依赖于对环境动态的完全准确了解，而基于 MC 的方法则通过模拟经验进行学习。在本节中，我们将介绍第三种强化学习方法——TD
    学习，它可以被视为对基于 MC 的强化学习方法的改进或扩展。
- en: Similar to the MC technique, TD learning is also based on learning by experience
    and, therefore, does not require any knowledge of environment dynamics and transition
    probabilities. The main difference between the TD and MC techniques is that in
    MC, we have to wait until the end of the episode to be able to calculate the total
    return.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 MC 技术，TD 学习也是基于经验学习，因此不需要了解环境动态和转移概率。TD 和 MC 技术的主要区别在于，在 MC 中，我们必须等到一集结束才能计算总回报。
- en: However, in TD learning, we can leverage some of the learned properties to update
    the estimated values before reaching the end of the episode. This is called *bootstrapping*
    (in the context of RL, the term bootstrapping is not to be confused with the bootstrap
    estimates we used in *Chapter 7*, *Combining Different Models for Ensemble Learning*).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在 TD 学习中，我们可以利用一些已经学习到的属性，在一集结束之前就更新估计值。这被称为 *引导*（在强化学习的上下文中，术语引导不应与我们在*第7章*
    *结合不同模型进行集成学习* 中使用的自举估计混淆）。
- en: 'Similar to the dynamic programming approach and MC-based learning, we will
    consider two tasks: estimating the value function (which is also called value
    prediction) and improving the policy (which is also called the control task).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于动态规划方法和基于 MC 的学习，我们将考虑两个任务：估计价值函数（也叫做价值预测）和改进策略（也叫做控制任务）。
- en: TD prediction
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TD 预测
- en: 'Let''s first revisit the value prediction by MC. At the end of each episode,
    we are able to estimate the return ![](img/B13208_18_144.png) for each time step
    *t*. Therefore, we can update our estimates for the visited states as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先回顾一下MC的值预测。在每个回合结束时，我们能够为每个时间步*t*估计回报！[](img/B13208_18_144.png)。因此，我们可以按如下方式更新已访问状态的估计值：
- en: '![](img/B13208_18_145.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_145.png)'
- en: Here, ![](img/B13208_18_146.png) is used as the *target return* to update the
    estimated values, and ![](img/B13208_18_147.png) is a *correction* term added
    to our current estimate of the value ![](img/B13208_18_148.png). The value ![](img/B13208_18_149.png)
    is a hyperparameter denoting the learning rate, which is kept constant during
    learning.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B13208_18_146.png)用作*目标回报*来更新估计值，而![](img/B13208_18_147.png)是添加到我们当前估计值![](img/B13208_18_148.png)中的*修正*项。值![](img/B13208_18_149.png)是表示学习率的超参数，在学习过程中保持不变。
- en: Notice that in MC, the correction term uses the *actual* return, ![](img/B13208_18_150.png),
    which is not known until the end of the episode. To clarify this, we can rename
    the actual return, ![](img/B13208_18_151.png), to ![](img/B13208_18_152.png),
    where the subscript ![](img/B13208_18_153.png) indicates that this is the return
    obtained at time step *t* while considering all the events occurred from time
    step *t* until the final time step, *T*.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在MC中，修正项使用的是*实际*回报！[](img/B13208_18_150.png)，这在回合结束之前是未知的。为了澄清这一点，我们可以将实际回报！[](img/B13208_18_151.png)重命名为！[](img/B13208_18_152.png)，其中下标![](img/B13208_18_153.png)表示这是在时间步*t*时获得的回报，并且考虑了从时间步*t*到最终时间步*T*期间发生的所有事件。
- en: 'In TD learning, we replace the actual return, ![](img/B13208_18_154.png), with
    a new target return, ![](img/B13208_18_155.png), which significantly simplifies
    the updates for the value function, ![](img/B13208_18_156.png). The update-formula
    based on TD learning is as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在TD学习中，我们将实际回报！[](img/B13208_18_154.png)替换为新的目标回报！[](img/B13208_18_155.png)，这大大简化了值函数![](img/B13208_18_156.png)的更新。基于TD学习的更新公式如下：
- en: '![](img/B13208_18_157.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_157.png)'
- en: Here, the target return, ![](img/B13208_18_158.png), is using the observed reward,
    ![](img/B13208_18_159.png), and estimated value of the next immediate step. Notice
    the difference between MC and TD. In MC, ![](img/B13208_18_160.png) is not available
    until the end of the episode, so we should execute as many steps as needed to
    get there. On the contrary, in TD, we only need to go one step ahead to get the
    target return. This is also known as TD(0).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，目标回报！[](img/B13208_18_158.png)使用的是观察到的奖励！[](img/B13208_18_159.png)和下一步的估计值。注意MC与TD的区别。在MC中，![](img/B13208_18_160.png)直到本回合结束后才能获得，因此我们需要执行尽可能多的步骤才能到达那里。相反，在TD中，我们只需前进一步即可获得目标回报。这也称为TD(0)。
- en: 'Furthermore, the TD(0) algorithm can be generalized to the so-called *n-step
    TD* algorithm, which incorporates more future steps – more precisely, the weighted
    sum of *n* future steps. If we define *n* = 1, then the n-step TD procedure is
    identical to TD(0), which was described in the previous paragraph. If ![](img/B13208_18_161.png),
    however, the n-step TD algorithm will be the same as the MC algorithm. The update-rule
    for n-step TD is as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，TD(0)算法可以推广到所谓的*n步TD*算法，它包括更多的未来步骤——更精确地说，是*n*个未来步骤的加权和。如果我们定义*n* = 1，那么n步TD过程与上一段中描述的TD(0)相同。但是，如果![](img/B13208_18_161.png)，那么n步TD算法将与MC算法相同。n步TD的更新规则如下：
- en: '![](img/B13208_18_162.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_162.png)'
- en: 'And ![](img/B13208_18_163.png) is defined as:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 而![](img/B13208_18_163.png)定义为：
- en: '![](img/B13208_18_164.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_164.png)'
- en: '**MC versus TD: which method converges faster?**'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**MC与TD：哪种方法收敛更快？**'
- en: 'While the precise answer to this question is still unknown, in practice, it
    is empirically shown that TD can converge faster than MC. If you are interested,
    you can find more details on the convergences of MC and TD in the book titled
    *Reinforcement Learning: An Introduction*, by Richard S. Sutton and Andrew G.
    Barto.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个问题的精确答案仍然未知，但在实践中，实验证明TD收敛的速度通常比MC快。如果你感兴趣的话，可以在《强化学习：导论》一书中，Richard S.
    Sutton和Andrew G. Barto为你提供更多关于MC和TD收敛性的信息。
- en: 'Now that we have covered the prediction task using the TD algorithm, we can
    move on to the control task. We will cover two algorithms for TD control: an *on-policy*
    control and an *off-policy* control. In both cases, we use the GPI that was used
    in both the dynamic programming and MC algorithms. In on-policy TD control, the
    value function is updated based on the actions from the same policy that the agent
    is following, while in an off-policy algorithm, the value function is updated
    based on actions outside the current policy.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讲解了使用TD算法的预测任务，我们可以继续讨论控制任务。我们将讨论两种TD控制算法：*基于策略的*控制和*离策略的*控制。在这两种情况下，我们都使用了在动态规划和MC算法中使用的GPI。在基于策略的TD控制中，价值函数是根据智能体遵循的相同策略中的动作来更新的，而在离策略算法中，价值函数是根据当前策略以外的动作来更新的。
- en: On-policy TD control (SARSA)
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于策略的TD控制（SARSA）
- en: 'For simplicity, we only consider the one-step TD algorithm, or TD(0). However,
    the on-policy TD control algorithm can be readily generalized to *n*-step TD.
    We will start by extending the prediction formula for defining the state-value
    function to describe the action-value function. To do this, we use a lookup table,
    that is, a tabular 2D-array, ![](img/B13208_18_165.png), which represents the
    action-value function for each state-action pair. In this case, we will have the
    following:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们只考虑一步TD算法，或者说TD(0)。然而，基于策略的TD控制算法可以很容易地推广到 *n* 步TD。我们将从扩展预测公式开始，定义状态值函数，并用它来描述动作值函数。为此，我们使用查找表，即一个二维数组
    ![](img/B13208_18_165.png)，它表示每个状态-动作对的动作值函数。在这种情况下，我们将有以下内容：
- en: '![](img/B13208_18_166.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_166.png)'
- en: This algorithm is often called SARSA, referring to the quintuple ![](img/B13208_18_167.png)
    that is used in the update formula.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法通常称为SARSA，指的是更新公式中使用的五元组 ![](img/B13208_18_167.png)。
- en: As we saw in the previous sections describing the dynamic programming and MC
    algorithms, we can use the GPI framework, and starting from the random policy,
    we can repeatedly estimate the action-value function for the current policy and
    then optimize the policy using the ![](img/B13208_18_168.png)-greedy policy based
    on the current action-value function.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的部分描述动态规划和MC算法时所看到的，我们可以使用GPI框架，并从随机策略开始，我们可以反复估计当前策略的动作值函数，然后使用基于当前动作值函数的
    ![](img/B13208_18_168.png)-贪婪策略来优化策略。
- en: Off-policy TD control (Q-learning)
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 离策略TD控制（Q学习）
- en: We saw when using the previous on-policy TD control algorithm that how we estimate
    the action-value function is based on the policy that is used in the simulated
    episode. After updating the action-value function, a separate step for policy
    improvement is performed by taking the action that has the higher value.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，在使用前面提到的基于策略的TD控制算法时，如何估计动作值函数是基于在模拟回合中使用的策略。在更新了动作值函数后，我们通过采取具有更高值的动作来执行单独的策略改进步骤。
- en: An alternative (and better) approach is to combine these two steps. In other
    words, imagine the agent is following policy ![](img/B13208_18_102.png), generating
    an episode with the current transition quintuple ![](img/B13208_18_170.png). Instead
    of updating the action-value function using the action value of ![](img/B13208_18_171.png)
    that is taken by the agent, we can find the best action even if it is not actually
    chosen by the agent following the current policy. (That's why this is considered
    an *off-policy* algorithm.)
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代（且更好的）方法是将这两步结合起来。换句话说，假设智能体遵循策略 ![](img/B13208_18_102.png)，生成一个当前过渡五元组
    ![](img/B13208_18_170.png)的回合。与其使用智能体采取的 ![](img/B13208_18_171.png) 动作值来更新动作值函数，我们可以即使该动作没有被智能体根据当前策略实际选择，也能找到最佳动作。（这就是为什么它被认为是
    *off-policy* 算法的原因。）
- en: 'To do this, we can modify the update rule to consider the maximum Q-value by
    varying different actions in the next immediate state. The modified equation for
    updating the Q-values is as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们可以修改更新规则，通过变化下一状态中不同动作的最大Q值来进行考虑。更新Q值的修改公式如下：
- en: '![](img/B13208_18_172.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_172.png)'
- en: We encourage you to compare the update rule here with that of the SARSA algorithm.
    As you can see, we find the best action in the next state, ![](img/B13208_18_173.png),
    and use that in the correction term to update our estimate of ![](img/B13208_18_174.png).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励你将此处的更新规则与SARSA算法的更新规则进行比较。正如你所看到的，我们在下一个状态中找到最佳动作 ![](img/B13208_18_173.png)，并将其用于修正项，以更新我们对
    ![](img/B13208_18_174.png) 的估计。
- en: To put these materials into perspective, in the next section, we will see how
    to implement the Q-learning algorithm for solving the *grid world problem*.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这些材料，在接下来的部分，我们将展示如何实现用于解决*网格世界问题*的Q学习算法。
- en: Implementing our first RL algorithm
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现我们的第一个强化学习算法
- en: In this section, we will cover the implementation of the Q-learning algorithm
    to solve the *grid world problem*. To do this, we use the OpenAI Gym toolkit.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将介绍如何实现Q学习算法来解决*网格世界问题*。为此，我们使用OpenAI Gym工具包。
- en: Introducing the OpenAI Gym toolkit
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍OpenAI Gym工具包
- en: 'OpenAI Gym is a specialized toolkit for facilitating the development of RL
    models. OpenAI Gym comes with several predefined environments. Some basic examples
    are CartPole and MountainCar, where the tasks are to balance a pole and to move
    a car up a hill, respectively, as the names suggest. There are also many advanced
    robotics environments for training a robot to fetch, push, and reach for items
    on a bench or training a robotic hand to orient blocks, balls, or pens. Moreover,
    OpenAI Gym provides a convenient, unified framework for developing new environments.
    More information can be found on its official website: [https://gym.openai.com/](https://gym.openai.com/).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym是一个专门的工具包，用于促进强化学习模型的开发。OpenAI Gym提供了多个预定义的环境。一些基础示例包括CartPole和MountainCar，任务分别是保持杆子平衡和让小车爬坡，正如其名称所示。还有许多高级机器人环境，用于训练机器人去取、推、或获取桌子上的物品，或者训练机器人手去定位块、球或笔。此外，OpenAI
    Gym还提供了一个便捷的统一框架，供开发新环境使用。更多信息可以在其官网找到：[https://gym.openai.com/](https://gym.openai.com/)。
- en: 'To follow the OpenAI Gym code examples in the next sections, you need to install
    the `gym` library, which can be easily done using `pip`:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随接下来的OpenAI Gym代码示例，你需要安装`gym`库，可以通过`pip`轻松安装：
- en: '[PRE0]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you need additional help with the installation, please see the official installation
    guide at [https://gym.openai.com/docs/#installation](https://gym.openai.com/docs/#installation).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要额外的安装帮助，请参考官方安装指南：[https://gym.openai.com/docs/#installation](https://gym.openai.com/docs/#installation)。
- en: Working with the existing environments in OpenAI Gym
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用OpenAI Gym中的现有环境
- en: 'For practice with the Gym environments, let''s create an environment from `CartPole-v1`,
    which already exists in OpenAI Gym. In this example environment, there is a pole
    attached to a cart that can move horizontally, as shown in the next figure:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了练习使用Gym环境，让我们从OpenAI Gym中创建一个现有的`CartPole-v1`环境。在这个示例环境中，有一个杆子附着在一个可以水平移动的小车上，如下图所示：
- en: '![](img/B13208_18_06.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_06.png)'
- en: The movements of the pole are governed by the laws of physics, and the goal
    for RL agents is to learn how to move the cart to stabilize the pole and prevent
    it from tipping over to either side.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 杆子的运动受物理定律的控制，强化学习代理的目标是学习如何移动小车以稳定杆子，并防止其倾斜到任一侧。
- en: 'Now, let''s look at some properties of the CartPole environment in the context
    of RL, such as its state (or observation) space, action space, and how to execute
    an action:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从强化学习的角度来看一下CartPole环境的一些特性，比如其状态（或观测）空间、动作空间，以及如何执行一个动作：
- en: '[PRE1]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the preceding code, we created an environment for the `CartPole` problem.
    The observation space for this environment is `Box(4,)`, which represents a four-dimensional
    space corresponding to four real-valued numbers: the position of the cart, the
    cart''s velocity, the angle of the pole, and the velocity of the tip of the pole.
    The action space is a discrete space, `Discrete(2)`, with two choices: pushing
    the cart either to the left or to the right.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们为`CartPole`问题创建了一个环境。该环境的观测空间是`Box(4,)`，表示一个四维空间，对应四个实数值：小车的位置、小车的速度、杆子的角度和杆子尖端的速度。动作空间是一个离散空间，`Discrete(2)`，有两个选择：将小车推向左边或右边。
- en: 'The environment object, `env`, that we previously created by calling `gym.make(''CartPole-v1'')`
    has a `reset()` method that we can use to reinitialize an environment prior to
    each episode. Calling the `reset()` method will basically set the pole''s starting
    state (![](img/B13208_18_175.png)):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前通过调用`gym.make('CartPole-v1')`创建的环境对象`env`，有一个`reset()`方法，我们可以在每次开始一个新回合之前使用它重新初始化环境。调用`reset()`方法将基本上设置杆子的初始状态（![](img/B13208_18_175.png)）：
- en: '[PRE2]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The values in the array returned by the `env.reset()` method call mean that
    the initial position of the cart is –0.039 with velocity –0.008, and the angle
    of the pole is 0.033 radian while the angular velocity of its tip is –0.021\.
    Upon calling the `reset()` method, these values are initialized with random values
    with uniform distribution in the range [–0.05, 0.05].
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`env.reset()`方法调用返回的数组中的值表示小车的初始位置为–0.039，速度为–0.008，杆子的角度为0.033弧度，而其尖端的角速度为–0.021。调用`reset()`方法时，这些值会初始化为在[–0.05,
    0.05]范围内的均匀分布的随机值。'
- en: 'After resetting the environment, we can interact with the environment by choosing
    an action and executing it by passing the action to the `step()` method:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 重置环境后，我们可以通过选择一个动作并将该动作传递给`step()`方法来与环境进行交互：
- en: '[PRE3]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Via the previous two commands, `env.step(action=0)` and `env.step(action=1)`,
    we pushed the cart to the left (`action=0`) and then to the right (`action=1`),
    respectively. Based on the selected action, the cart and its pole can move as
    governed by the laws of physics. Every time we call `env.step()`, it returns a
    tuple consisting of four elements:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 通过之前的两个命令，`env.step(action=0)`和`env.step(action=1)`，我们分别将小车推向左侧（`action=0`）和右侧（`action=1`）。根据选择的动作，小车及其杆子会按照物理定律运动。每次调用`env.step()`时，它会返回一个包含四个元素的元组：
- en: An array for the new state (or observations)
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新状态（或观测）的数组
- en: A reward (a scalar value of type `float`)
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励（类型为`float`的标量值）
- en: A termination flag (`True` or `False`)
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 终止标志（`True`或`False`）
- en: A Python dictionary containing auxiliary information
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含辅助信息的Python字典
- en: The `env` object also has a `render()` method, which we can execute after each
    step (or a series of steps) to visualize the environment and the movements of
    the pole and cart, through time.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`env`对象也有一个`render()`方法，我们可以在每个步骤（或一系列步骤）之后执行它，以便通过时间可视化环境以及杆子和小车的运动。'
- en: The episode terminates when the angle of the pole becomes larger than 12 degrees
    (from either side) with respect to an imaginary vertical axis, or when the position
    of the cart is more than 2.4 units from the center position. The reward defined
    in this example is to maximize the time the cart and pole are stabilized within
    the valid regions—in other words, the total reward (that is, return) can be maximized
    by maximizing the length of the episode.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 当杆子的角度相对于虚拟垂直轴超过12度（无论哪一侧）时，或者当小车的位置距离中心位置超过2.4个单位时，剧集终止。在这个示例中定义的奖励是最大化小车和杆子在有效区域内稳定的时间——换句话说，通过最大化剧集的长度可以最大化总奖励（即回报）。
- en: A grid world example
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网格世界示例
- en: 'After introducing the CartPole environment as a warm-up exercise for working
    with the OpenAI Gym toolkit, we will now switch to a different environment. We
    will work with a grid world example, which is a simplistic environment with *m*
    rows and *n* columns. Considering *m* = 4 and *n* = 6, we can summarize this environment
    as shown in the following figure:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在将CartPole环境作为使用OpenAI Gym工具包的热身练习之后，我们将切换到另一个环境。我们将使用一个网格世界示例，这是一个简化的环境，具有*m*行和*n*列。假设*m*
    = 4且*n* = 6，我们可以总结这个环境，如下图所示：
- en: '![](img/B13208_18_07.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_07.png)'
- en: 'In this environment, there are 30 different possible states. Four of these
    states are terminal states: a pot of gold at state 16 and three traps at states
    10, 15, and 22\. Landing in any of these four terminal states will end the episode,
    but with a difference between the gold and trap states. Landing on the gold state
    yields a positive reward, +1, whereas moving the agent onto one of the trap states
    yields a negative reward, –1\. All other states have a reward of 0\. The agent
    always starts from state 0\. Therefore, every time we reset the environment, the
    agent will go back to state 0\. The action space consists of four directions:
    move up, down, left, and right.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个环境中，有30个不同的可能状态。其中四个状态是终止状态：在状态16处有一个金锅，状态10、15和22处分别有三个陷阱。落入这四个终止状态中的任何一个都会结束剧集，但金锅和陷阱状态有所不同。落入金锅状态会获得正奖励+1，而进入陷阱状态则会获得负奖励–1。所有其他状态的奖励为0。代理始终从状态0开始。因此，每次重置环境时，代理都会回到状态0。动作空间由四个方向组成：向上、向下、向左和向右。
- en: When the agent is at the outer boundary of the grid, selecting an action that
    would result in leaving the grid will not change the state.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理位于网格的外边界时，选择一个会导致离开网格的动作不会改变状态。
- en: Next, we will see how to implement this environment in Python, using the OpenAI
    Gym package.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到如何使用OpenAI Gym包在Python中实现这个环境。
- en: Implementing the grid world environment in OpenAI Gym
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在OpenAI Gym中实现网格世界环境
- en: For experimenting with the grid world environment via OpenAI Gym, using a script
    editor or IDE rather than executing the code interactively is highly recommended.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过OpenAI Gym实验网格世界环境时，强烈建议使用脚本编辑器或IDE，而不是交互式执行代码。
- en: First, we create a new Python script named `gridworld_env.py` and then proceed
    by importing the necessary packages and two helper functions that we define for
    building the visualization of the environment.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个新的Python脚本，命名为`gridworld_env.py`，然后导入必要的包和我们定义的两个辅助函数，这些函数用于构建环境的可视化。
- en: In order to render the environments for visualization purposes, OpenAI Gym library
    uses the Pyglet library and provides wrapper classes and functions for our convenience.
    We will use these wrapper classes for visualizing the grid world environment in
    the following code example. More details about these wrapper classes can be found
    at
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将环境渲染以便进行可视化，OpenAI Gym库使用了Pyglet库，并为我们的方便提供了封装类和函数。在以下代码示例中，我们将使用这些封装类来可视化网格世界环境。关于这些封装类的更多信息，可以参考：
- en: '[https://github.com/openai/gym/blob/master/gym/envs/classic_control/rendering.py](https://github.com/openai/gym/blob/master/gym/envs/classic_control/rendering.py)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/openai/gym/blob/master/gym/envs/classic_control/rendering.py](https://github.com/openai/gym/blob/master/gym/envs/classic_control/rendering.py)'
- en: 'The following code example uses those wrapper classes:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例使用了这些封装类：
- en: '[PRE4]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first helper function, `get_coords()`, returns the coordinates of the geometric
    shapes that we will use to annotate the grid world environment, such as a triangle
    to display the gold or circles to display the traps. The list of coordinates is
    passed to `draw_object()`, which decides to draw a circle, a triangle, or a polygon
    based on the length of the input list of coordinates.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个辅助函数`get_coords()`返回我们将用于标注网格世界环境的几何形状的坐标，例如用三角形表示金币，或者用圆形表示陷阱。坐标列表传递给`draw_object()`，该函数根据输入坐标列表的长度决定绘制圆形、三角形或多边形。
- en: 'Now, we can define the grid world environment. In the same file (`gridworld_env.py`),
    we define a class named `GridWorldEnv`, which inherits from OpenAI Gym''s `DiscreteEnv`
    class. The most important function of this class is the constructor method, `__init__()`,
    where we define the action space, specify the role of each action, and determine
    the terminal states (gold as well as traps) as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义网格世界环境。在同一个文件（`gridworld_env.py`）中，我们定义了一个名为`GridWorldEnv`的类，该类继承自OpenAI
    Gym的`DiscreteEnv`类。这个类最重要的功能是构造方法`__init__()`，在该方法中我们定义了动作空间，指定了每个动作的作用，并确定了终止状态（金币和陷阱），具体如下：
- en: '[PRE5]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This code implements the grid world environment, from which we can create instances
    of this environment. We can then interact with it in a manner similar to that
    in the CartPole example. The implemented class, `GridWorldEnv`, inherits methods
    such as `reset()` for resetting the state and `step()` for executing an action.
    The details of the implementation are as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码实现了网格世界环境，我们可以基于此创建该环境的实例。然后，我们可以像在CartPole示例中一样与之互动。实现的类`GridWorldEnv`继承了`reset()`等方法用于重置状态，以及`step()`方法用于执行动作。实现的具体细节如下：
- en: 'We defined the four different actions using lambda functions: `move_up()`,
    `move_down()`, `move_left()`, and `move_right()`.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用lambda函数定义了四个不同的动作：`move_up()`、`move_down()`、`move_left()`和`move_right()`。
- en: The NumPy array `isd` holds the probabilities of the starting states so that
    a random state will be selected based on this distribution when the `reset()`
    method (from the parent class) is called. Since we always start from state 0 (the
    lower-left corner of the grid world), we set the probability of state 0 to 1.0
    and the probabilities of all other 29 states to 0.0.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy数组`isd`保存了起始状态的概率，因此在调用`reset()`方法（来自父类）时，系统会根据该分布随机选择一个状态。由于我们总是从状态0（网格世界的左下角）开始，因此我们将状态0的概率设置为1.0，其他29个状态的概率设置为0.0。
- en: The transition probabilities, defined in the Python dictionary `P`, determine
    the probabilities of moving from one state to another state when an action is
    selected. This allows us to have a probabilistic environment where taking an action
    could have different outcomes based on the stochasticity of the environment. For
    simplicity, we just use a single outcome, which is to change the state in the
    direction of the selected action. Finally, these transition probabilities will
    be used by the `env.step()` function to determine the next state.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python字典`P`中定义的转移概率，决定了在选择一个动作时，从一个状态到另一个状态的概率。这使得我们可以拥有一个概率环境，其中执行一个动作可能会有不同的结果，取决于环境的随机性。为了简单起见，我们只使用一个结果，那就是根据选择的动作改变状态。最终，这些转移概率将由`env.step()`函数用于确定下一个状态。
- en: Furthermore, the function `_build_display()` will set up the initial visualization
    of the environment, and the `render()` function will show the movements of the
    agent.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，函数`_build_display()`将设置环境的初始可视化，而`render()`函数将展示代理的运动过程。
- en: Note that during the learning process, we do not know about the transition probabilities,
    and the goal is to learn through interacting with the environment. Therefore,
    we do not have access to `P` outside the class definition.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在学习过程中，我们并不知道转移概率，目标是通过与环境的互动来学习。因此，我们在类定义之外无法访问`P`。
- en: 'Now, we can test this implementation by creating a new environment and visualize
    a random episode by taking random actions at each state. Include the following
    code at the end of the same Python script (`gridworld_env.py`) and then execute
    the script:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过创建一个新的环境并通过在每个状态下采取随机动作来可视化一个随机的回合，来测试这个实现。在同一个Python脚本（`gridworld_env.py`）的末尾加入以下代码，然后执行脚本：
- en: '[PRE6]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After executing the script, you should see a visualization of the grid world
    environment as depicted in the following figure:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 执行脚本后，你应该会看到一个网格世界环境的可视化，如下图所示：
- en: '![Une image contenant capture d’écran  Description générée automatiquement](img/B13208_18_08.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![Une image contenant capture d’écran  Description générée automatiquement](img/B13208_18_08.png)'
- en: Solving the grid world problem with Q-learning
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Q-learning解决网格世界问题
- en: After focusing on the theory and the development process of RL algorithms, as
    well as setting up the environment via the OpenAI Gym toolkit, we will now implement
    the currently most popular RL algorithm, Q-learning. For this, we will use the
    grid world example that we already implemented in the script `gridworld_env.py`.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在关注RL算法的理论与开发过程，并通过OpenAI Gym工具包设置环境之后，我们将实现当前最流行的RL算法——Q-learning。为此，我们将使用之前在脚本`gridworld_env.py`中实现的网格世界示例。
- en: Implementing the Q-learning algorithm
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现Q-learning算法
- en: 'Now, we create a new script and name it `agent.py`. In this `agent.py` script,
    we define an agent for interacting with the environment as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们创建一个新的脚本并命名为`agent.py`。在这个`agent.py`脚本中，我们定义了一个用于与环境交互的代理，具体如下：
- en: '[PRE7]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `__init__()` constructor sets up various hyperparameters such as the learning
    rate, discount factor (![](img/B13208_18_046.png)), and the parameters for the
    ![](img/B13208_18_177.png)-greedy policy. Initially, we start with a high value
    of ![](img/B13208_18_178.png), but the method `_adjust_epsilon()` reduces it until
    it reaches the minimum value, ![](img/B13208_18_179.png). The method `choose_action()`
    chooses an action based on the ![](img/B13208_18_180.png)-greedy policy as follows.
    A random uniform number is selected to determine whether the action should be
    selected randomly or otherwise, based on the action-value function. The method
    `_learn()` implements the update rule for the Q-learning algorithm. It receives
    a tuple for each transition, which consists of the current state (*s*), selected
    action (*a*), observed reward (*r*), next state (*s'*), as well as a flag to determine
    whether the end of the episode has been reached or not. The target value is equal
    to the observed reward (*r*) if this is flagged as end-of-episode; otherwise,
    the target is ![](img/B13208_18_181.png).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__()`构造函数设置了各种超参数，如学习率、折扣因子(![](img/B13208_18_046.png))，以及![](img/B13208_18_177.png)-贪婪策略的参数。最初，我们从较高的![](img/B13208_18_178.png)值开始，但方法`_adjust_epsilon()`会逐步将其减少，直到达到最小值![](img/B13208_18_179.png)。方法`choose_action()`根据![](img/B13208_18_180.png)-贪婪策略选择一个动作。通过选择一个随机均匀数，来决定动作是应该随机选择还是根据动作价值函数进行选择。方法`_learn()`实现了Q-learning算法的更新规则。它接收每个过渡的元组，其中包括当前状态(*s*)、选择的动作(*a*)、观察到的奖励(*r*)、下一个状态(*s''*)，以及一个标志来判断是否已达到回合结束。若该标志表示回合结束，则目标值等于观察到的奖励(*r*)；否则，目标值为![](img/B13208_18_181.png)。'
- en: Finally, for our next step, we create a new script, `qlearning.py`, to put everything
    together and train the agent using the Q-learning algorithm.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了进行下一步操作，我们创建了一个新的脚本`qlearning.py`，将所有内容整合在一起，并使用Q-learning算法训练代理。
- en: In the following code, we define a function, `run_qlearning()`, that implements
    the Q-learning algorithm, simulating an episode by calling the `_choose_action()`
    method of the agent and executing the environment. Then, the transition tuple
    is passed to the `_learn()` method of the agent to update the action-value function.
    In addition, for monitoring the learning process, we also store the final reward
    of each episode (which could be –1 or +1), as well as the length of episodes (the
    number of moves taken by the agent from the start of the episode until the end).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们定义了一个函数`run_qlearning()`，它实现了Q-learning算法，通过调用代理的`_choose_action()`方法并执行环境来模拟一个回合。然后，过渡元组被传递给代理的`_learn()`方法，用于更新动作价值函数。此外，为了监控学习过程，我们还存储了每个回合的最终奖励（可能是-1或+1），以及回合的长度（代理从回合开始到结束所采取的动作数）。
- en: 'The list of rewards and the number of moves is then plotted using the function
    `plot_learning_history()`:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用函数`plot_learning_history()`绘制奖励和动作次数的列表：
- en: '[PRE8]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Executing this script will run the Q-learning program for 50 episodes. The
    behavior of the agent will be visualized, and you can see that at the beginning
    of the learning process, the agent mostly ends up in the trap states. But through
    time, it learns from its failures and eventually finds the gold state (for instance,
    the first time in episode 7). The following figure shows the agent''s number of
    moves and rewards:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此脚本将运行Q-learning程序50个回合。代理的行为将被可视化，您会看到在学习过程的开始阶段，代理大多会进入陷阱状态。但随着时间的推移，它从失败中学习，并最终找到金状态（例如，第7回合第一次找到）。下图展示了代理的动作次数和奖励：
- en: '![](img/B13208_18_09.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_09.png)'
- en: The plotted learning history shown in the previous figure indicates that the
    agent, after 30 episodes, learns a short path to get to the gold state. As a result,
    the lengths of the episodes after the 30th episode are more or less the same,
    with minor deviations due to the ![](img/B13208_18_182.png)-greedy policy.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 前面图中绘制的学习历史表明，经过30个回合后，代理学会了找到一条通往金状态的短路径。因此，30回合之后的每个回合长度基本相同，只有由于![](img/B13208_18_182.png)-贪婪策略产生的微小偏差。
- en: A glance at deep Q-learning
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度Q学习概述
- en: In the previous code, we saw an implementation of the popular Q-learning algorithm
    for the grid world example. This example consisted of a discrete state space of
    size 30, where it was sufficient to store the Q-values in a Python dictionary.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们展示了一个针对网格世界示例的流行Q-learning算法的实现。该示例包含了一个大小为30的离散状态空间，在这里，将Q值存储在Python字典中就足够了。
- en: However, we should note that sometimes the number of states can get very large,
    possibly almost infinitely large. Also, we may be dealing with a continuous state
    space instead of working with discrete states. Moreover, some states may not be
    visited at all during training, which can be problematic when generalizing the
    agent to deal with such unseen states later.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们应该注意到，有时状态的数量可能非常大，甚至几乎是无限大的。此外，我们可能会处理连续的状态空间，而不是离散的状态。并且，某些状态在训练期间可能根本不会被访问，这在将智能体泛化到处理这些未见过的状态时可能会带来问题。
- en: To address these problems, instead of representing the value function in a tabular
    format like ![](img/B13208_18_183.png), or ![](img/B13208_18_184.png), for the
    action-value function, we use a *function approximation* approach. Here, we define
    a parametric function, ![](img/B13208_18_185.png), that can learn to approximate
    the true value function, that is, ![](img/B13208_18_186.png), where ![](img/B13208_18_187.png)
    is a set of input features (or "featurized" states).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们不再像图像 ![](img/B13208_18_183.png) 或 ![](img/B13208_18_184.png) 中那样以表格形式表示值函数，或者对于动作值函数，我们使用
    *函数逼近* 方法。在这里，我们定义了一个参数化函数 ![](img/B13208_18_185.png)，它可以学习逼近真实的值函数，即 ![](img/B13208_18_186.png)，其中
    ![](img/B13208_18_187.png) 是一组输入特征（或“特征化”状态）。
- en: 'When the approximator function, ![](img/B13208_18_188.png), is a deep neural
    network (DNN), the resulting model is called a **deep Q-network** (**DQN**). For
    training a DQN model, the weights are updated according to the Q-learning algorithm.
    An example of a DQN model is shown in the following figure, where the states are
    represented as features passed to the first layer:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 当逼近器函数 ![](img/B13208_18_188.png) 是深度神经网络（DNN）时，得到的模型称为 **深度 Q 网络**（**DQN**）。在训练
    DQN 模型时，权重会根据 Q-learning 算法进行更新。下图展示了一个 DQN 模型的示例，其中状态表示为传递到第一层的特征：
- en: '![](img/B13208_18_10.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_10.png)'
- en: Now, let's see how we can train a DQN using the *deep Q-learning* algorithm.
    Overall, the main approach is very similar to the tabular Q-learning method. The
    main difference is that we now have a multilayer NN that computes the action values.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用 *深度 Q-learning* 算法训练 DQN。总体来说，主要方法与表格 Q-learning 方法非常相似。主要区别在于，我们现在有一个多层神经网络来计算动作值。
- en: Training a DQN model according to the Q-learning algorithm
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 根据 Q-learning 算法训练 DQN 模型
- en: In this section, we describe the procedure for training a DQN model using the
    Q-learning algorithm. The deep Q-learning approach requires us to make some modifications
    to our previously implemented standard Q-learning approach.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了使用 Q-learning 算法训练 DQN 模型的过程。深度 Q-learning 方法要求我们对之前实现的标准 Q-learning
    方法进行一些修改。
- en: One such modification is in the agent's `choose_action()` method, which in the
    code of the previous section for Q-learning was simply accessing the action values
    stored in a dictionary. Now this function should be changed to perform a forward
    pass of the NN model for computing the action values.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个修改是在智能体的 `choose_action()` 方法中，在上一节 Q-learning 的代码中，这个方法仅仅是访问存储在字典中的动作值。现在，这个函数应该被修改为执行神经网络模型的前向传播，以计算动作值。
- en: The other modifications needed for the deep Q-learning algorithm are described
    in the following two subsections.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 Q-learning 算法所需的其他修改在以下两个小节中描述。
- en: Replay memory
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 回放记忆
- en: Using the previous tabular method for Q-learning, we could update the values
    for specific state-action pairs without affecting the values of others. However,
    now that we approximate *q*(*s*, *a*) with an NN model, updating the weights for
    a state-action pair will likely affect the output of other states as well. When
    training NNs using stochastic gradient descent for a supervised task (for example,
    a classification task), we use multiple epochs to iterate through the training
    data multiple times until it converges.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前的表格方法进行 Q-learning 时，我们可以更新特定状态-动作对的值，而不会影响其他值。然而，现在我们使用神经网络模型来逼近 *q*(*s*,
    *a*)，更新一个状态-动作对的权重很可能会影响其他状态的输出。在使用随机梯度下降法训练神经网络进行监督任务（例如分类任务）时，我们会使用多个训练周期多次迭代训练数据，直到收敛。
- en: This is not feasible in Q-learning, since the episodes will change during the
    training and as a result, some states that were visited in the early stages of
    training will become less likely to be visited later.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这在 Q-learning 中不可行，因为在训练过程中，回合会发生变化，结果是，一些在训练早期访问的状态在后期变得不太可能被再次访问。
- en: Furthermore, another problem is that when we train an NN, we assume that the
    training examples are **IID** (**independent and identically distributed**). However,
    the samples taken from an episode of the agent are not IID, as they obviously
    form a sequence of transitions.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，另一个问题是，当我们训练神经网络时，通常假设训练示例是 **IID**（**独立同分布**）。然而，从智能体的一个回合中采样的样本并不是 IID，因为它们显然形成了一个过渡序列。
- en: To solve these issues, as the agent interacts with the environment and generates
    a transition quintuple ![](img/B13208_18_189.png), we store a large (but finite)
    number of such transitions in a memory buffer, often called *replay memory*. After
    each new interaction (that is, the agent selects an action and executes it in
    the environment), the resulting new transition quintuple is appended to the memory.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，当智能体与环境交互并生成一个过渡五元组 ![](img/B13208_18_189.png) 时，我们将大量（但有限）这样的过渡存储在一个记忆缓冲区中，这通常称为
    *回放记忆*。每次新的交互（即智能体选择一个动作并在环境中执行）之后，产生的新过渡五元组会被添加到记忆中。
- en: 'To keep the size of the memory bounded, the oldest transition will be removed
    from the memory (for example, if it is a Python list, we can use the `pop(0)`
    method to remove the first element of the list). Then, a mini-batch of examples
    is randomly selected from the memory buffer, which will be used for computing
    the loss and updating the network parameters. The following figure illustrates
    the process:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持记忆的大小有限，最旧的过渡将从记忆中移除（例如，如果它是一个 Python 列表，我们可以使用 `pop(0)` 方法移除列表中的第一个元素）。然后，从记忆缓冲区中随机选择一个小批量的示例，用于计算损失并更新网络参数。下图说明了这个过程：
- en: '![](img/B13208_18_11.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_11.png)'
- en: '**Implementing the replay memory**'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现回放记忆**'
- en: The replay memory can be implemented using a Python list, where every time we
    add a new element to the list, we need to check the size of the list and call
    `pop(0)` if needed.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 回放记忆可以使用 Python 列表实现，每次向列表中添加新元素时，我们需要检查列表的大小，并在必要时调用 `pop(0)`。
- en: Alternatively, we can use the `deque` data structure from the Python `collections`
    library, which allows us to specify an optional argument, `max_len`. By specifying
    the `max_len` argument, we will have a bounded deque. Therefore, when the object
    is full, appending a new element results in automatically removing an element
    from it.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用 Python `collections` 库中的 `deque` 数据结构，它允许我们指定一个可选参数 `max_len`。通过指定
    `max_len` 参数，我们将获得一个有界的双端队列。因此，当队列已满时，添加新元素会自动移除队列中的一个元素。
- en: Note that this is more efficient than using a Python list, since removing the
    first element of a list using `pop(0)` has O(n) complexity, while the deque's
    runtime complexity is O(1). You can learn more about the deque implementation
    from the official documentation that is available at
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这比使用 Python 列表更高效，因为使用 `pop(0)` 从列表中移除第一个元素的时间复杂度是 O(n)，而双端队列的运行时复杂度是 O(1)。你可以通过官方文档了解更多关于双端队列的实现。
- en: '[https://docs.python.org/3.7/library/collections.html#collections.deque](https://docs.python.org/3.7/library/collections.html#collections.deque).'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.python.org/3.7/library/collections.html#collections.deque](https://docs.python.org/3.7/library/collections.html#collections.deque)。'
- en: Determining the target values for computing the loss
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 确定计算损失的目标值
- en: Another required change from the tabular Q-learning method is how to adapt the
    update rule for training the DQN model parameters. Recall that a transition quintuple,
    *T*, stored in the batch of examples, contains ![](img/B13208_18_191.png).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 从表格 Q-learning 方法中需要做的另一个变更是如何调整更新规则来训练 DQN 模型参数。回想一下，批量示例中存储的过渡五元组 *T* 包含 ![](img/B13208_18_191.png)。
- en: As shown in the following figure, we perform two forward passes of the DQN model.
    The first forward pass uses the features of the current state (![](img/B13208_18_192.png)).
    Then, the second forward pass uses the features of the next state (![](img/B13208_18_193.png)).
    As a result, we will obtain the estimated action values, ![](img/B13208_18_194.png)
    and ![](img/B13208_18_195.png), from the first and second forward pass, respectively.
    (Here, this ![](img/B13208_18_196.png) notation means a vector of Q-values for
    all actions in ![](img/B13208_18_197.png).) From the transition quintuple, we
    know that action *a* is selected by the agent.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，我们执行了DQN模型的两次前向传播。第一次前向传播使用当前状态的特征（！[](img/B13208_18_192.png)）。然后，第二次前向传播使用下一个状态的特征（！[](img/B13208_18_193.png)）。因此，我们将分别从第一次和第二次前向传播中获得估计的动作值，！[](img/B13208_18_194.png)和！[](img/B13208_18_195.png)。
    （在这里，这个！[](img/B13208_18_196.png)符号表示的是在！[](img/B13208_18_197.png)中所有动作的Q值向量。）从过渡五元组中，我们知道智能体选择了动作*a*。
- en: 'Therefore, according to the Q-learning algorithm, we need to update the action
    value corresponding to the state-action pair ![](img/B13208_18_198.png) with the
    scalar target value ![](img/B13208_18_199.png). Instead of forming a scalar target
    value, we will create a target action-value vector that retains the action values
    for other actions, ![](img/B13208_18_200.png), as shown in the following figure:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据Q学习算法，我们需要更新与状态-动作对对应的动作值！[](img/B13208_18_198.png)，其标量目标值为！[](img/B13208_18_199.png)。我们将不再形成标量目标值，而是创建一个目标动作值向量，该向量保留了其他动作的动作值，！[](img/B13208_18_200.png)，如下面的图所示：
- en: '![](img/B13208_18_12.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_12.png)'
- en: 'We treat this as a regression problem, using the following three quantities:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其视为一个回归问题，使用以下三个量：
- en: The currently predicted values, ![](img/B13208_18_201.png)
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前预测值，！[](img/B13208_18_201.png)
- en: The target value vector as described
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如上所述的目标值向量
- en: The standard mean squared error (MSE) cost function
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准均方误差（MSE）损失函数
- en: As a result, the losses will be zero for every action except for *a*. Finally,
    the computed loss will be backpropagated to update the network parameters.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，除了*a*以外，所有动作的损失都为零。最终，计算出的损失将通过反向传播更新网络参数。
- en: Implementing a deep Q-learning algorithm
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现深度Q学习算法
- en: Finally, we will use all these techniques for implementing a deep Q-learning
    algorithm. This time, we use the CartPole environment from the OpenAI gym environment
    that we introduced earlier. Recall that the CartPole environment has a continuous
    state space of size 4\. In the following code, we define a class, `DQNAgent`,
    that builds the model and specifies various hyperparameters.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用所有这些技术来实现深度Q学习算法。这次，我们使用的是前面介绍的OpenAI Gym环境中的CartPole环境。回想一下，CartPole环境的状态空间是连续的，大小为4。在接下来的代码中，我们定义了一个类`DQNAgent`，该类构建了模型并指定了各种超参数。
- en: 'This class has two additional methods compared to the previous agent that was
    based on tabular Q-learning. The method `remember()` will append a new transition
    quintuple to the memory buffer, and the method `replay()` will create a mini-batch
    of example transitions and pass that to the `_learn()` method for updating the
    network''s weight parameters:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前基于表格Q学习的智能体相比，此类多了两个方法。方法`remember()`将会把新的过渡五元组添加到记忆缓冲区，而方法`replay()`会创建一个小批量的示例过渡，并将其传递给`_learn()`方法以更新网络的权重参数：
- en: '[PRE9]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, with the following code, we train the model for 200 episodes, and
    at the end visualize the learning history using the function `plot_learning_history()`:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用以下代码，我们训练模型200个回合，并在结束时使用`plot_learning_history()`函数可视化学习历史：
- en: '[PRE10]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After training the agent for 200 episodes, we see that it indeed learned to
    increase the total rewards over time, as shown in the following plot:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练智能体200个回合后，我们发现它确实学会了随着时间的推移增加总奖励，如下图所示：
- en: '![](img/B13208_18_13.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_18_13.png)'
- en: Note that the total rewards obtained in an episode is equal to the amount of
    time that the agent is able to balance the pole. The learning history plotted
    in this figure shows that after about 30 episodes, the agent learns how to balance
    the pole and hold it for more than 200 time steps.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，某一回合中获得的总奖励等于智能体能够保持杆平衡的时间。图中的学习历史显示，在大约30个回合后，智能体学会了如何平衡杆并将其保持超过200个时间步。
- en: Chapter and book summary
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 章节和书籍总结
- en: In this chapter, we covered the essential concepts in RL, starting from the
    very foundations, and how RL can support decision making in complex environments.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了强化学习（RL）中的基本概念，从最基础的内容开始，探讨了强化学习如何支持在复杂环境中做出决策。
- en: 'We learned about agent-environment interactions and Markov decision processes
    (MDP), and we considered three main approaches for solving RL problems: dynamic
    programming, MC learning, and TD learning. We discussed that the dynamic programming
    algorithm assumes that the full knowledge of environment dynamics is available,
    an assumption that is not typically true for most real-world problems.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了智能体与环境的互动和马尔科夫决策过程（MDP），并考虑了解决强化学习问题的三种主要方法：动态规划、MC学习和TD学习。我们讨论了动态规划算法假设已知完整的环境动态，而这一假设通常不适用于大多数实际问题。
- en: Then, we saw how the MC- and TD-based algorithms learn by allowing an agent
    to interact with the environment and generate a simulated experience. After discussing
    the underlying theory, we implemented the Q-learning algorithm as an off-policy
    subcategory of the TD algorithm for solving the grid world example. Finally, we
    covered the concept of function approximation and deep Q-learning in particular,
    which can be used for problems with large or continuous state spaces.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们看到基于MC和TD的算法通过允许智能体与环境互动并生成模拟经验来学习。在讨论了相关理论之后，我们实现了Q学习算法，作为TD算法的一个离策略子类别，用于解决网格世界示例。最后，我们介绍了函数逼近的概念，特别是深度Q学习，它可以用于处理具有大规模或连续状态空间的问题。
- en: We hope you enjoyed this last chapter of *Python Machine Learning* and our exciting
    tour of machine learning and deep learning. Through the journey of this book,
    we've covered the essential topics that this field has to offer, and you should
    now be well equipped to put those techniques into action to solve real-world problems.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望你喜欢*《Python机器学习》*的最后一章，以及我们对机器学习和深度学习的精彩之旅。在本书的旅程中，我们涵盖了该领域提供的基本主题，现在你应该已经能够将这些技术应用于解决实际问题。
- en: 'We started our journey with a brief overview of the different types of learning
    tasks: supervised learning, reinforcement learning, and unsupervised learning.
    We then discussed several different learning algorithms that you can use for classification,
    starting with simple single-layer NNs in *Chapter 2*, *Training Simple Machine
    Learning Algorithms for Classification*.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从简要概述不同类型的学习任务开始：监督学习、强化学习和无监督学习。接着，我们讨论了几种用于分类的学习算法，从*第二章*中的简单单层神经网络开始，*训练简单的机器学习算法进行分类*。
- en: We continued to discuss advanced classification algorithms in *Chapter 3*, *A
    Tour of Machine Learning Classifiers Using scikit-learn*, and we learned about
    the most important aspects of a machine learning pipeline in *Chapter 4*, *Building
    Good Training Datasets – Data Preprocessing*, and *Chapter 5*, *Compressing Data
    via Dimensionality Reduction*.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续在*第三章*中讨论了高级分类算法，*使用scikit-learn探索机器学习分类器*，并且在*第四章*中学习了机器学习管道中最重要的几个方面，*构建良好的训练数据集——数据预处理*，以及*第五章*，*通过降维压缩数据*。
- en: Remember that even the most advanced algorithm is limited by the information
    in the training data that it gets to learn from. So, in *Chapter 6*, *Learning
    Best Practices for Model Evaluation and Hyperparameter Tuning*, we learned about
    the best practices to build and evaluate predictive models, which is another important
    aspect in machine learning applications.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，即使是最先进的算法，也受到它所学习的训练数据中信息的限制。因此，在*第六章*中，*构建模型评估和超参数调优的最佳实践*，我们学习了构建和评估预测模型的最佳实践，这是机器学习应用中另一个重要的方面。
- en: If one single learning algorithm does not achieve the performance we desire,
    it can be sometimes helpful to create an ensemble of experts to make a prediction.
    We explored this in *Chapter 7*, *Combining Different Models for Ensemble Learning*.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如果单一的学习算法未能达到我们期望的性能，有时通过创建一个专家集群来进行预测是有帮助的。我们在*第七章*中探讨了这一点，*结合不同模型进行集成学习*。
- en: Then in *Chapter 8*, *Applying Machine Learning to Sentiment Analysis*, we applied
    machine learning to analyze one of the most popular and interesting forms of data
    in the modern age that's dominated by social media platforms on the Internet—text
    documents.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在*第八章*中，*将机器学习应用于情感分析*，我们应用机器学习来分析现代社交媒体平台主导的互联网时代中最受欢迎和最有趣的数据形式之一——文本文件。
- en: Next, we reminded ourselves that machine learning techniques are not limited
    to offline data analysis, and in *Chapter 9*, *Embedding a Machine Learning Model
    into a Web Application*, we saw how to embed a machine learning model into a web
    application to share it with the outside world.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们提醒自己，机器学习技术不仅限于离线数据分析，在*第9章*，*将机器学习模型嵌入到Web应用中*中，我们展示了如何将机器学习模型嵌入到Web应用中，与外界分享。
- en: For the most part, our focus was on algorithms for classification, which is
    probably the most popular application of machine learning. However, this is not
    where our journey ended! In *Chapter 10*, *Predicting Continuous Target Variables
    with Regression Analysis*, we explored several algorithms for regression analysis
    to predict continuous continuous target variables.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分时间里，我们的重点是分类算法，这是机器学习中最常见的应用。然而，这并不是我们旅程的终点！在*第10章*，*使用回归分析预测连续目标变量*中，我们探讨了几种回归分析算法，用于预测连续的目标变量。
- en: Another exciting subfield of machine learning is clustering analysis, which
    can help us find hidden structures in the data, even if our training data does
    not come with the right answers to learn from. We worked with this in *Chapter
    11*, *Working with Unlabeled Data – Clustering Analysis*.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的另一个令人激动的子领域是聚类分析，它可以帮助我们发现数据中隐藏的结构，即使我们的训练数据没有正确的答案可以学习。在*第11章*，*处理无标签数据——聚类分析*中，我们探讨了这一领域。
- en: We then shifted our attention to one of one of the most exciting algorithms
    in the whole machine learning field—artificial neural networks. We started by
    implementing a multilayer perceptron from scratch with NumPy in *Chapter 12*,
    *Implementing a Multilayer Artificial Neural Network from Scratch*.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们将注意力转向了整个机器学习领域中最令人兴奋的算法之一——人工神经网络。我们从*第12章*，*从头开始实现多层感知器*开始，使用NumPy实现了一个多层感知器。
- en: The utility of TensorFlow 2 for deep learning became obvious in *Chapter 13*,
    *Parallelizing Neural Network Training with TensorFlow*, where we used TensorFlow
    to facilitate the process of building NN models and worked with TensorFlow `Dataset`
    objects, and learned how to apply preprocessing steps to a dataset.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '*第13章*，*使用TensorFlow并行化神经网络训练*中，TensorFlow 2在深度学习中的应用变得显而易见，我们使用TensorFlow简化了神经网络模型的构建过程，处理了TensorFlow
    `Dataset`对象，并学习了如何对数据集应用预处理步骤。'
- en: We delved deeper into the mechanics of TensorFlow in *Chapter 14*, *Going Deeper
    – The Mechanics of TensorFlow*, and discussed the different aspects and mechanics
    of TensorFlow, including variables, TensorFlow function decoration, computing
    gradients of a computation, as well as TensorFlow estimators.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第14章*，*深入探讨——TensorFlow的机制*中，我们更深入地了解了TensorFlow的机制，讨论了TensorFlow的各个方面，包括变量、TensorFlow函数装饰器、计算梯度以及TensorFlow估算器。
- en: In *Chapter 15*, *Classifying Images with Deep Convolutional Neural Networks*,
    we dived into convolutional neural networks, which are widely used in computer
    vision at the moment, due to their great performance in image classification tasks.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第15章*，*使用深度卷积神经网络分类图像*中，我们深入研究了卷积神经网络，目前这些网络在计算机视觉中广泛应用，特别是在图像分类任务中的出色表现。
- en: In *Chapter 16*, *Modeling Sequential Data Using Recurrent Neural Networks*,
    we learned about sequence modeling using RNNs, and covered the Transformer model,
    one of the most recent deep learning algorithms for seq2seq modeling.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第16章*，*使用循环神经网络建模序列数据*中，我们学习了使用RNN进行序列建模，并介绍了Transformer模型，这是最近在seq2seq建模中使用的深度学习算法之一。
- en: In *Chapter 17*, *Generative Adversarial Networks for Synthesizing New Data*,
    we saw how to generate new images using GANs, and along the way, we also learned
    about autoencoders, batch normalization, transposed convolution, and Wasserstein
    GANs.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第17章*，*利用生成对抗网络合成新数据*中，我们学习了如何使用GAN生成新图像，同时还了解了自动编码器、批量归一化、转置卷积以及Wasserstein
    GAN。
- en: Finally, in this chapter, we covered a completely separate category of machine
    learning tasks and saw how to develop algorithms that learn by interacting with
    their environment through a reward process.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在本章中，我们讨论了机器学习任务的一个完全独立的类别，展示了如何开发通过与环境交互并通过奖励过程进行学习的算法。
- en: While a comprehensive study of deep learning is well beyond the scope of this
    book, we hope that we've kindled your interest enough to follow the most recent
    advancements in this field of deep learning.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深入研究深度学习远超本书的范围，但我们希望已经激发了你足够的兴趣，能够跟进这一领域中最新的深度学习进展。
- en: 'If you''re considering a career in machine learning, or you just want to keep
    up to date with the current advancements in this field, we can recommend to you
    the works of the following leading experts in the machine learning field:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在考虑从事机器学习职业，或者只是想跟上这一领域的最新进展，我们可以推荐以下机器学习领域的顶尖专家的作品：
- en: Geoffrey Hinton ([http://www.cs.toronto.edu/~hinton/](http://www.cs.toronto.edu/~hinton/))
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geoffrey Hinton ([http://www.cs.toronto.edu/~hinton/](http://www.cs.toronto.edu/~hinton/))
- en: Andrew Ng ([http://www.andrewng.org/](http://www.andrewng.org/))
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrew Ng ([http://www.andrewng.org/](http://www.andrewng.org/))
- en: Yann LeCun ([http://yann.lecun.com](http://yann.lecun.com))
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yann LeCun ([http://yann.lecun.com](http://yann.lecun.com))
- en: Juergen Schmidhuber ([http://people.idsia.ch/~juergen/](http://people.idsia.ch/~juergen/))
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Juergen Schmidhuber ([http://people.idsia.ch/~juergen/](http://people.idsia.ch/~juergen/))
- en: Yoshua Bengio ([http://www.iro.umontreal.ca/~bengioy/yoshua_en/](http://www.iro.umontreal.ca/~bengioy/yoshua_en/))
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yoshua Bengio ([http://www.iro.umontreal.ca/~bengioy/yoshua_en/](http://www.iro.umontreal.ca/~bengioy/yoshua_en/))
- en: Just to name a few!
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅列举几位！
- en: 'Lastly, you can find out what we, the authors, are up to at these sites:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以通过以下网站了解我们这些作者的最新动态：
- en: '[https://sebastianraschka.com](https://sebastianraschka.com)'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://sebastianraschka.com](https://sebastianraschka.com)'
- en: '[http://vahidmirjalili.com](http://vahidmirjalili.com).'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://vahidmirjalili.com](http://vahidmirjalili.com)。'
- en: You're always welcome to contact us if you have any questions about this book
    or if you need some general tips about machine learning.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对本书有任何问题，或需要一些关于机器学习的常规建议，欢迎随时联系我们。
