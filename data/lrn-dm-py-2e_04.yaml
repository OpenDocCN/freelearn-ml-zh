- en: Recommending Movies Using Affinity Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用亲和力分析推荐电影
- en: In this chapter, we will look at **affinity analysis** which determines when
    objects occur frequently together. This is also colloquially called market basket
    analysis, after one of the common use cases - determining when items are purchased
    together frequently in a store.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨**亲和力分析**，该分析用于确定对象何时频繁地一起出现。这通常也被称为市场篮子分析，因为这是一种常见的用例——确定在商店中频繁一起购买的商品。
- en: In [Chapter 3](e3e085cc-17ce-429b-8a71-b4f09a12b672.xhtml)*, Predicting Sports
    Winners with Decision Trees*, we looked at an object as a focus and used features
    to describe that object. In this chapter, the data has a different form. We have
    transactions where the objects of interest (movies, in this chapter) are used
    within those transactions in some way. The aim is to discover when objects occur
    simultaneously. In a case where we wish to work out when two movies are recommended
    by the same reviewers, we can use affinity analysis.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](e3e085cc-17ce-429b-8a71-b4f09a12b672.xhtml)*，使用决策树预测体育比赛胜者*中，我们将对象作为焦点，并使用特征来描述该对象。在本章中，数据具有不同的形式。我们有交易，其中感兴趣的物体（在本章中为电影）以某种方式被用于这些交易中。目标是发现对象何时同时出现。如果我们想找出两部电影是否被同一评论家推荐，我们可以使用亲和力分析。
- en: 'The key concepts of this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的关键概念如下：
- en: Affinity analysis for product recommendations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亲和力分析用于产品推荐
- en: Feature association mining using the Apriori algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Apriori算法进行特征关联挖掘
- en: Recommendation Systems and the inherent challenges
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐系统和固有的挑战
- en: Sparse data formats and how to use them
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏数据格式及其使用方法
- en: Affinity analysis
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亲和力分析
- en: Affinity analysis is the task of determining when objects are used in similar
    ways. In the previous chapter, we focused on whether the objects themselves are
    similar - in our case whether the games were similar in nature. The data for affinity
    analysis is often described in the form of a transaction. Intuitively, this comes
    from a transaction at a store—determining when objects are purchased together
    as a way to recommend products to users that they might purchase.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 亲和力分析是确定对象以相似方式使用的任务。在前一章中，我们关注的是对象本身是否相似——在我们的案例中是游戏在本质上是否相似。亲和力分析的数据通常以交易的形式描述。直观地说，这来自商店的交易——通过确定对象何时一起购买，作为向用户推荐他们可能购买的产品的方式。
- en: 'However, affinity analysis can be applied to many processes that do not use
    transactions in this sense:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，亲和力分析可以应用于许多不使用这种意义上的交易的流程：
- en: Fraud detection
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欺诈检测
- en: Customer segmentation
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户细分
- en: Software optimization
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件优化
- en: Product recommendations
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品推荐
- en: Affinity analysis is usually much more exploratory than classification. At the
    very least, we often simply rank the results and choose the top five recommendations
    (or some other number), rather than expect the algorithm to give us a specific
    answer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 亲和力分析通常比分类更具探索性。至少，我们通常只是简单地排名结果并选择前五项推荐（或某个其他数字），而不是期望算法给出一个特定的答案。
- en: Furthermore, we often don't have the complete dataset we expect for many classification
    tasks. For instance, in movie recommendation, we have reviews from different people
    on different movies. However, it is highly unlikely we have each reviewer review
    all of the movies in our dataset. This leaves an important and difficult question
    in affinity analysis. If a reviewer hasn't reviewed a movie, is that an indication
    that they aren't interested in the movie (and therefore wouldn't recommend it)
    or simply that they haven't reviewed it yet?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们通常没有我们期望的完整数据集来完成许多分类任务。例如，在电影推荐中，我们有不同人对不同电影的评论。然而，我们几乎不可能让每个评论家都评论我们数据集中的所有电影。这给亲和力分析留下了一个重要且困难的问题。如果一个评论家没有评论一部电影，这是否表明他们不感兴趣（因此不会推荐）或者只是他们还没有评论？
- en: Thinking about gaps in your datasets can lead to questions like this. In turn,
    that can lead to answers that may help improve the efficacy of your approach.
    As a budding data miner, knowing where your models and methodologies need improvement
    is key to creating great results.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 思考数据集中的差距可以导致这样的问题。反过来，这可能导致有助于提高你方法有效性的答案。作为一个初露头角的数据挖掘者，知道你的模型和方法需要改进的地方是创造出色结果的关键。
- en: Algorithms for affinity analysis
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亲和力分析算法
- en: We introduced a basic method for affinity analysis in Chapter 1*, Getting Started
    with Data Mining*, which tested all of the possible rule combinations. We computed
    the confidence and support for each rule, which in turn allowed us to rank them
    to find the best rules.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章*《数据挖掘入门》*中，我们介绍了一种基本的关联分析方法，它测试了所有可能的规则组合。我们计算了每个规则的置信度和支持度，这反过来又允许我们根据规则进行排序，以找到最佳规则。
- en: However, this approach is not efficient. Our dataset in Chapter 1*, Getting
    Started with Data Mining*, had just five items for sale. We could expect even
    a small store to have hundreds of items for sale, while many online stores would
    have thousands (or millions!). With a naive rule creation, such as our previous
    algorithm from Chapter 1*, Getting Started with Data Mining*, the growth in the time
    needed to compute these rules increases exponentially. As we add more items, the
    time it takes to compute all rules increases significantly faster. Specifically,
    the total possible number of rules is *2n - 1*. For our five-item dataset, there
    are 31 possible rules. For 10 items, it is 1023\. For just 100 items, the number
    has 30 digits. Even the drastic increase in computing power couldn't possibly
    keep up with the increases in the number of items stored online. Therefore, we
    need algorithms that work smarter, as opposed to computers that work harder.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法并不高效。我们在第一章*《数据挖掘入门》*中的数据集只有五个销售项目。我们可以预期即使是小型商店也会有数百个销售项目，而许多在线商店会有数千（甚至数百万！）项目。使用我们之前在第一章*《数据挖掘入门》*中提到的简单规则创建方法，这些规则计算所需的时间会呈指数增长。随着我们添加更多项目，计算所有规则所需的时间增长得更快。具体来说，可能的总规则数是*2n
    - 1*。对于五个项目的数据集，有31个可能的规则。对于十个项目，这个数字是1023。对于仅仅100个项目，这个数字有30位。即使计算能力的急剧增加也无法跟上在线存储项目数量的增长。因此，我们需要更智能的算法，而不是更努力工作的计算机。
- en: The classic algorithm for affinity analysis is called the **Apriori algorithm**.
    It addresses the exponential problem of creating sets of items that occur frequently
    within a database, called **frequent itemsets**. Once these frequent itemsets
    are discovered, creating association rules is straightforward, which we will see
    later in the chapter.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 关联分析的经典算法被称为**Apriori算法**。它解决了在数据库中创建频繁项集（称为**频繁项集**）的指数级问题。一旦发现这些频繁项集，创建关联规则就变得简单，我们将在本章后面看到这一点。
- en: The intuition behind Apriori is both simple and clever. First, we ensure that
    a rule has sufficient support within the dataset. Defining a minimum support level
    is the key parameter for Apriori. To build a frequent itemset we combine smaller
    frequent itemsets. For itemset (A, B) to have a support of at least 30, both A
    and B must occur at least 30 times in the database. This property extends to larger
    sets as well. For an itemset (A, B, C, D) to be considered frequent, the set (A,
    B, C) must also be frequent (as must D).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Apriori背后的直觉既简单又巧妙。首先，我们确保规则在数据集中有足够的支持度。定义最小支持度是Apriori的关键参数。为了构建频繁项集，我们结合较小的频繁项集。对于项集（A，B）要有至少30%的支持度，A和B必须在数据库中至少出现30次。这一属性也适用于更大的集合。对于一个项集（A，B，C，D）要被认为是频繁的，集合（A，B，C）也必须是频繁的（同样，D也必须是频繁的）。
- en: These frequent itemsets can be built and possible itemsets that are not frequent
    (of which there are many) will never be tested. This saves significant time in
    testing new rules, as the number of frequent itemsets is expected to be significantly
    fewer than the total number of possible itemsets.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些频繁项集可以构建，而不频繁的可能项集（其中有很多）将永远不会被测试。这在新规则测试中节省了大量的时间，因为频繁项集的数量预计将远少于可能项集的总数。
- en: Other example algorithms for affinity analysis build on this, or similar concepts,
    including the **Eclat** and **FP-growth** algorithms. There are many improvements
    to these algorithms in the data mining literature that further improve the efficiency
    of the method. In this chapter, we will focus on the basic Apriori algorithm.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其他关联分析的示例算法基于这个或类似的概念，包括**Eclat**和**FP-growth**算法。数据挖掘文献中有许多对这些算法的改进，进一步提高了方法的效率。在本章中，我们将重点关注基本的Apriori算法。
- en: Overall methodology
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总体方法
- en: To perform association rule mining for affinity analysis, we first use the Apriori
    algorithm to generate frequent itemsets. Next, we create association rules (for
    example, *if a person recommended movie X, they would also recommend movie Y*)
    by testing combinations of premises and conclusions within those frequent itemsets.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行关联规则挖掘以进行亲和力分析，我们首先使用Apriori算法生成频繁项集。接下来，我们通过测试那些频繁项集中前提和结论的组合来创建关联规则（例如，*如果一个人推荐了电影X，他们也会推荐电影Y*）。
- en: For the first stage, the Apriori algorithm needs a value for the minimum support
    that an itemset needs to be considered frequent. Any itemsets with less support
    will not be considered.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一阶段，Apriori算法需要一个值来表示项集需要达到的最小支持度，才能被认为是频繁的。任何支持度低于这个值的项集都不会被考虑。
- en: Setting this minimum support too low will cause Apriori to test a larger number
    of itemsets, slowing the algorithm down. Setting it too high will result in fewer
    itemsets being considered frequent.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个最小支持度设置得太低会导致Apriori测试更多的项集，从而减慢算法的速度。设置得太高会导致考虑的频繁项集更少。
- en: In the second stage, after the frequent itemsets have been discovered, association
    rules are tested based on their confidence. We could choose a minimum confidence
    level, a number of rules to return, or simply return all of them and let the user
    decide what to do with them.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第二阶段，在频繁项集被发现之后，基于它们的置信度来测试关联规则。我们可以选择一个最小的置信度水平，返回的规则数量，或者简单地返回所有规则并让用户决定如何处理它们。
- en: In this chapter, we will return only rules above a given confidence level. Therefore,
    we need to set our minimum confidence level. Setting this too low will result
    in rules that have a high support, but are not very accurate. Setting this higher
    will result in only more accurate rules being returned, but with fewer rules being
    discovered overall.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们只返回高于给定置信度水平的规则。因此，我们需要设置我们的最小置信度水平。设置得太低会导致具有高支持度但不太准确的规则。设置得更高将导致只返回更准确的规则，但总体上发现的规则更少。
- en: Dealing with the movie recommendation problem
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理电影推荐问题
- en: Product recommendation is a big business. Online stores use it to up-sell to
    customers by recommending other products that they could buy. Making better recommendations
    leads to better sales. When online shopping is selling to millions of customers
    every year, there is a lot of potential money to be made by selling more items
    to these customers.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 产品推荐是一个庞大的产业。在线商店通过推荐其他可能购买的产品来向上销售给客户。做出更好的推荐可以带来更好的销售业绩。当在线购物每年向数百万客户销售时，通过向这些客户销售更多商品，就有大量的潜在利润可赚。
- en: Product recommendations, including movie and books, have been researched for
    many years; however, the field gained a significant boost when Netflix ran their
    Netflix Prize between 2007 and 2009\. This competition aimed to determine if anyone
    can predict a user's rating of a film better than Netflix was currently doing.
    The prize went to a team that was just over 10 percent better than the current
    solution. While this may not seem like a large improvement, such an improvement
    would net millions to Netflix in revenue from better movie recommendations over
    the following years.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 产品推荐，包括电影和书籍，已经研究了许多年；然而，当Netflix在2007年至2009年期间举办Netflix Prize时，该领域得到了显著的发展。这次比赛旨在确定是否有人能比Netflix目前所做的更好预测用户的电影评分。奖项授予了一个团队，他们的表现比当前解决方案高出10%以上。虽然这种改进可能看起来并不大，但这样的改进将为Netflix在接下来的几年中带来数百万美元的收益，因为更好的电影推荐。
- en: Obtaining the dataset
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据集
- en: Since the inception of the Netflix Prize, Grouplens, a research group at the
    University of Minnesota, has released several datasets that are often used for
    testing algorithms in this area. They have released several versions of a movie
    rating dataset, which have different sizes. There is a version with 100,000 reviews,
    one with 1 million reviews and one with 10 million reviews.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 自从Netflix Prize启动以来，明尼苏达大学的Grouplens研究小组已经发布了几个常用于测试该领域算法的数据集。他们发布了多个电影评分数据集的不同版本，大小不同。有一个版本有10万条评论，一个版本有100万条评论，还有一个版本有1000万条评论。
- en: 'The datasets are available from [http://grouplens.org/datasets/movielens/](http://grouplens.org/datasets/movielens/)
    and the dataset we are going to use in this chapter is the *MovieLens 100K dataset*
    (with 100,000 reviews). Download this dataset and unzip it in your data folder.
    Start a new Jupyter Notebook and type the following code:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以从[http://grouplens.org/datasets/movielens/](http://grouplens.org/datasets/movielens/)获取，本章我们将使用的是*MovieLens
    100K数据集*（包含10万条评论）。下载此数据集并将其解压到您的数据文件夹中。启动一个新的Jupyter Notebook，并输入以下代码：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Ensure that `ratings_filename` points to the u.data file in the unzipped folder.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 确保变量`ratings_filename`指向解压文件夹中的u.data文件。
- en: Loading with pandas
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用pandas加载
- en: The `MovieLens` dataset is in a good shape; however, there are some changes
    from the default options in `pandas.read_csv` that we need to make. To start with,
    the data is separated by tabs, not commas. Next, there is no heading line. This
    means the first line in the file is actually data and we need to manually set
    the column names.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`MovieLens`数据集状况良好；然而，与`pandas.read_csv`的默认选项相比，我们需要做一些更改。首先，数据是以制表符分隔的，而不是逗号。其次，没有标题行。这意味着文件中的第一行实际上是数据，我们需要手动设置列名。'
- en: 'When loading the file, we set the delimiter parameter to the tab character,
    tell pandas not to read the first row as the header (with `header=None`) and to
    set the column names with given values. Let''s look at the following code:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载文件时，我们将分隔符参数设置为制表符，告诉pandas不要将第一行作为标题读取（使用`header=None`），并使用给定的值设置列名。让我们看看以下代码：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: While we won't use it in this chapter, you can properly parse the date timestamp
    using the following line. Dates for reviews can be an important feature in recommendation
    prediction, as movies that are rated together often have more similar rankings
    than movies ranked separately. Accounting for this can improve models significantly.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们本章不会使用它，但您可以使用以下行正确解析日期时间戳。评论的日期对于推荐预测可能是一个重要特征，因为一起评分的电影通常比单独评分的电影有更相似的排名。考虑到这一点可以显著提高模型的效果。
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can view the first few records by running the following in a new cell:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在新的单元格中运行以下代码来查看前几条记录：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The result will come out looking something like this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将类似于以下内容：
- en: '|  | UserID | MovieID | Rating | Datetime |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | UserID | MovieID | Rating | Datetime |'
- en: '| 0 | 196 | 242 | 3 | 1997-12-04 15:55:49 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 196 | 242 | 3 | 1997-12-04 15:55:49 |'
- en: '| 1 | 186 | 302 | 3 | 1998-04-04 19:22:22 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 186 | 302 | 3 | 1998-04-04 19:22:22 |'
- en: '| 2 | 22 | 377 | 1 | 1997-11-07 07:18:36 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 22 | 377 | 1 | 1997-11-07 07:18:36 |'
- en: '| 3 | 244 | 51 | 2 | 1997-11-27 05:02:03 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 244 | 51 | 2 | 1997-11-27 05:02:03 |'
- en: '| 4 | 166 | 346 | 1 | 1998-02-02 05:33:16 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 166 | 346 | 1 | 1998-02-02 05:33:16 |'
- en: Sparse data formats
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏数据格式
- en: This dataset is in a sparse format. Each row can be thought of as a cell in
    a large feature matrix of the type used in previous chapters, where rows are users
    and columns are individual movies. The first column would be each user's review of
    the first movie, the second column would be each user's review of the second movie,
    and so on.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集是稀疏格式。每一行可以被视为之前章节中使用的大型特征矩阵中的一个单元格，其中行是用户，列是单独的电影。第一列将是每个用户对第一部电影的评论，第二列将是每个用户对第二部电影的评论，依此类推。
- en: There are around 1,000 users and 1,700 movies in this dataset, which means that
    the full matrix would be quite large (nearly 2 million entries). We may run into
    issues storing the whole matrix in memory and computing on it would be troublesome.
    However, this matrix has the property that most cells are empty, that is, there
    is no review for most movies for most users. There is no review of movie number 675
    for user number 213 though, and not for most other combinations of user and movie.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集中大约有1,000个用户和1,700部电影，这意味着完整的矩阵会相当大（近200万条记录）。我们可能会遇到在内存中存储整个矩阵的问题，并且对其进行计算会相当麻烦。然而，这个矩阵具有大多数单元格为空的性质，也就是说，大多数用户对大多数电影没有评论。尽管如此，用户编号213对电影编号675的评论不存在，以及其他大多数用户和电影的组合也是如此。
- en: The format given here represents the full matrix, but in a more compact way.
    The first row indicates that user  number 196 reviewed movie number 242, giving
    it a ranking of 3 (out of five) on December 4, 1997.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这里给出的格式代表完整的矩阵，但以更紧凑的方式呈现。第一行表示用户编号196在1997年12月4日对电影编号242进行了评分，评分为3（满分五分）。
- en: Any combination of user and movie that isn't in this database is assumed to
    not exist. This saves significant space, as opposed to storing a bunch of zeroes
    in memory. This type of format is called a sparse matrix format. As a rule of
    thumb, if you expect about 60 percent or more of your dataset to be empty or zero,
    a sparse format will take less space to store.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 任何不在数据库中的用户和电影的组合都被假定为不存在。这节省了大量的空间，与在内存中存储一串零相比。这种格式称为稀疏矩阵格式。一般来说，如果你预计你的数据集中有60%或更多的数据为空或为零，稀疏格式将占用更少的空间来存储。
- en: When computing on sparse matrices, the focus isn't usually on the data we don't
    have—comparing all of the zeroes. We usually focus on the data we have and compare
    those.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在稀疏矩阵上进行计算时，我们通常不会关注我们没有的数据——比较所有的零。我们通常关注我们有的数据，并比较这些数据。
- en: Understanding the Apriori algorithm and its implementation
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Apriori算法及其实现
- en: 'The goal of this chapter is to produce rules of the following form: *if a person
    recommends this set of movies, they will also recommend this movie*. We will also
    discuss extensions where a person who recommends a set of movies, is likely to
    recommend another particular movie.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是产生以下形式的规则：*如果一个人推荐了这组电影，他们也会推荐这部电影*。我们还将讨论扩展，其中推荐一组电影的人可能会推荐另一部特定的电影。
- en: 'To do this, we first need to determine if a person recommends a movie. We can
    do this by creating a new feature Favorable, which is True if the person gave
    a favorable review to a movie:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们首先需要确定一个人是否推荐了一部电影。我们可以通过创建一个新的特征“赞同”，如果该人对电影给出了好评，则为True：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can see the new feature by viewing the dataset:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看数据集来查看新功能：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '|  | UserID | MovieID | Rating | Datetime | Favorable |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | 用户ID | 电影ID | 评分 | 日期时间 | 赞同 |'
- en: '| 10 | 62 | 257 | 2 | 1997-11-12 22:07:14 | False |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 62 | 257 | 2 | 1997-11-12 22:07:14 | False |'
- en: '| 11 | 286 | 1014 | 5 | 1997-11-17 15:38:45 | True |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 286 | 1014 | 5 | 1997-11-17 15:38:45 | True |'
- en: '| 12 | 200 | 222 | 5 | 1997-10-05 09:05:40 | True |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 200 | 222 | 5 | 1997-10-05 09:05:40 | True |'
- en: '| 13 | 210 | 40 | 3 | 1998-03-27 21:59:54 | False |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 210 | 40 | 3 | 1998-03-27 21:59:54 | False |'
- en: '| 14 | 224 | 29 | 3 | 1998-02-21 23:40:57 | False |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 224 | 29 | 3 | 1998-02-21 23:40:57 | False |'
- en: 'We will sample our dataset to form training data. This also helps reduce the
    size of the dataset that will be searched, making the Apriori algorithm run faster.
    We obtain all reviews from the first 200 users:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采样我们的数据集以形成训练数据。这也帮助减少了要搜索的数据集的大小，使Apriori算法运行得更快。我们获取了前200个用户的所有评论：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we can create a dataset of only the favorable reviews in our sample:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以创建一个只包含样本中好评的评论文本的数据集：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will be searching the user''s favorable reviews for our itemsets. So, the
    next thing we need is the movies which each user has given a favorable rating.
    We can compute this by grouping the dataset by the `UserID` and iterating over
    the movies in each group:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在用户的好评中搜索我们的项集。因此，我们接下来需要的是每个用户给出的好评电影。我们可以通过按`UserID`对数据集进行分组并遍历每个组中的电影来计算这一点：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding code, we stored the values as a `frozenset`, allowing us to
    quickly check if a movie has been rated by a user.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将值存储为`frozenset`，这样我们可以快速检查用户是否对电影进行了评分。
- en: Sets are much faster than lists for this type of operation, and we will use
    them in later code.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种类型的操作，集合比列表快得多，我们将在后面的代码中使用它们。
- en: 'Finally, we can create a `DataFrame` that tells us how frequently each movie
    has been given a favorable review:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以创建一个`DataFrame`，告诉我们每部电影被好评的频率：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can see the top five movies by running the following code:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下代码，我们可以看到前五部电影：
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let's see the top five movies list. We only have IDs now, and will get their
    titles later in the chapter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看前五部电影列表。我们现在只有ID，将在本章后面获取它们的标题。
- en: '| Movie ID | Favorable |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 电影ID | 赞同 |'
- en: '| 50 | 100 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 100 |'
- en: '| 100 | 89 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 89 |'
- en: '| 258 | 83 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 258 | 83 |'
- en: '| 181 | 79 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 181 | 79 |'
- en: '| 174 | 74 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 174 | 74 |'
- en: Looking into the basics of the Apriori algorithm
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索Apriori算法的基本原理
- en: 'The Apriori algorithm is part of our affinity analysis methodology and deals
    specifically with finding frequent itemsets within the data. The basic procedure
    of Apriori builds up new candidate itemsets from previously discovered frequent
    itemsets. These candidates are tested to see if they are frequent, and then the
    algorithm iterates as explained here:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Apriori算法是我们亲和力分析方法的一部分，专门处理在数据中寻找频繁项集的问题。Apriori的基本程序是从先前发现的频繁项集中构建新的候选项集。这些候选集被测试以查看它们是否频繁，然后算法按以下方式迭代：
- en: Create initial frequent itemsets by placing each item in its own itemset. Only
    items with at least the minimum support are used in this step.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将每个项目放置在其自己的项目集中来创建初始频繁项目集。在此步骤中仅使用至少具有最小支持度的项目。
- en: New candidate itemsets are created from the most recently discovered frequent
    itemsets by finding supersets of the existing frequent itemsets.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从最近发现的频繁项目集中创建新的候选项目集，通过找到现有频繁项目集的超集。
- en: All candidate itemsets are tested to see if they are frequent. If a candidate
    is not frequent then it is discarded. If there are no new frequent itemsets from
    this step, go to the last step.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有候选项目集都会被测试以确定它们是否频繁。如果一个候选项目集不是频繁的，则将其丢弃。如果没有从这个步骤中产生新的频繁项目集，则转到最后一步。
- en: Store the newly discovered frequent itemsets and go to the second step.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存储新发现的频繁项目集并转到第二步。
- en: Return all of the discovered frequent itemsets.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回所有发现的频繁项目集。
- en: 'This process is outlined in the following workflow:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程在以下工作流程中概述：
- en: '![](img/B06162_04_01.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06162_04_01.jpg)'
- en: Implementing the Apriori algorithm
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Apriori算法
- en: On the first iteration of Apriori, the newly discovered itemsets will have a
    length of 2, as they will be supersets of the initial itemsets created in the
    first step. On the second iteration (after applying the fourth step and going
    back to step 2), the newly discovered itemsets will have a length of 3\. This
    allows us to quickly identify the newly discovered itemsets, as needed in the second
    step.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在Apriori的第一轮迭代中，新发现的项集长度将为2，因为它们将是第一步中创建的初始项集的超集。在第二轮迭代（应用第四步并返回到第二步之后），新发现的项集长度将为3。这使我们能够快速识别新发现的项集，正如在第二步中所需的那样。
- en: 'We can store our discovered frequent itemsets in a dictionary, where the key
    is the length of the itemsets. This allows us to quickly access the itemsets of
    a given length, and therefore the most recently discovered frequent itemsets,
    with the help of the following code:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在字典中存储发现频繁项目集，其中键是项目集的长度。这允许我们快速访问给定长度的项目集，以及通过以下代码帮助快速访问最近发现的频繁项目集：
- en: '[PRE11]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We also need to define the minimum support needed for an itemset to be considered
    frequent. This value is chosen based on the dataset but try different values to
    see how that affects the result. I recommend only changing it by 10 percent at
    a time though, as the time the algorithm takes to run will be significantly different!
    Let''s set a minimum support value:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要定义一个项目集被认为是频繁所需的最小支持度。此值基于数据集选择，但尝试不同的值以查看它如何影响结果。尽管如此，我建议每次只改变10%，因为算法运行所需的时间将显著不同！让我们设置一个最小支持度值：
- en: '[PRE12]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To implement the first step of the Apriori algorithm, we create an itemset with
    each movie individually and test if the itemset is frequent. We use `frozenset`**,**
    as they allow us to perform faster set-based operations later on, and they can
    also be used as keys in our counting dictionary (normal sets cannot).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现Apriori算法的第一步，我们为每部电影单独创建一个项目集，并测试该项目集是否频繁。我们使用`frozenset`**，**因为它们允许我们在稍后执行更快的基于集合的操作，并且它们还可以用作计数字典中的键（普通集合不能）。
- en: 'Let''s look at the following example of `frozenset` code:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看以下`frozenset`代码的示例：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We implement the second and third steps together for efficiency by creating
    a function that takes the newly discovered frequent itemsets, creates the supersets,
    and then tests if they are frequent. First, we set up the function to perform
    these steps:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高效率，我们将第二步和第三步一起实现，通过创建一个函数来执行这些步骤，该函数接受新发现的频繁项目集，创建超集，然后测试它们是否频繁。首先，我们设置函数以执行这些步骤：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In keeping with our rule of thumb of reading through the data as little as possible,
    we iterate over the dataset once per call to this function. While this doesn't
    matter too much in this implementation (our dataset is relatively small compared
    to the average computer), **single-pass** is a good practice to get into for larger
    applications.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了遵循我们尽可能少读取数据的经验法则，我们每次调用此函数时只遍历数据集一次。虽然在这个实现中这不是很重要（与平均计算机相比，我们的数据集相对较小），**单次遍历**是对于更大应用的良好实践。
- en: Let's have a look at the core of this function in detail. We iterate through
    each user, and each of the previously discovered itemsets, and then check if it
    is a subset of the current set of reviews, which are stored in `k_1_itemsets`
    (note that here, k_1 means *k-1*). If it is, this means that the user has reviewed
    each movie in the itemset. This is done by the `itemset.issubset(reviews)` line.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看这个函数的核心。我们遍历每个用户，以及之前发现的每个项集，然后检查它是否是当前存储在`k_1_itemsets`中的评论集的子集（注意，这里的k_1意味着*k-1*）。如果是，这意味着用户已经评论了项集中的每部电影。这是通过`itemset.issubset(reviews)`这一行完成的。
- en: We can then go through each individual movie that the user has reviewed (that
    is not already in the itemset), create a superset by combining the itemset with
    the new movie and record that we saw this superset in our counting dictionary.
    These are the candidate frequent itemsets for this value of *k*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以遍历用户评论的每部单独的电影（那些尚未在项集中），通过将项集与新电影结合来创建超集，并在我们的计数字典中记录我们看到了这个超集。这些都是这个*k*值的候选频繁项集。
- en: We end our function by testing which of the candidate itemsets have enough support
    to be considered frequent and return only those that have a support more than
    our `min_support` value.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过测试候选项集是否有足够的支持被认为是频繁的来结束我们的函数，并只返回那些支持超过我们的`min_support`值的项集。
- en: 'This function forms the heart of our Apriori implementation and we now create
    a loop that iterates over the steps of the larger algorithm, storing the new itemsets
    as we increase *k* from 1 to a maximum value. In this loop, k represents the length
    of the soon-to-be discovered frequent itemsets, allowing us to access the previously
    most discovered ones by looking in our frequent_itemsets dictionary using the
    key *k - 1*. We create the frequent itemsets and store them in our dictionary
    by their length. Let''s look at the code:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数构成了我们Apriori实现的核心，我们现在创建一个循环，遍历更大算法的步骤，随着*k*从1增加到最大值，存储新的项集。在这个循环中，k代表即将发现的频繁项集的长度，允许我们通过在我们的频繁项集字典中使用键*k-1*来访问之前发现的最频繁的项集。我们通过它们的长度创建频繁项集并将它们存储在我们的字典中。让我们看看代码：
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If we do find frequent itemsets, we print out a message to let us know the loop
    will be running again. If we don't, we stop iterating, as there cannot be frequent
    itemsets for *k+1* if there are no frequent itemsets for the current value of
    *k*, therefore we finish the algorithm.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们找到了频繁项集，我们打印一条消息来让我们知道循环将再次运行。如果没有，我们停止迭代，因为没有频繁项集对于*k+1*，如果当前*k*值没有频繁项集，因此我们完成算法。
- en: We use `sys.stdout.flush()` to ensure that the printouts happen while the code
    is still running. Sometimes, in large loops in particular cells, the printouts
    will not happen until the code has completed. Flushing the output in this way
    ensures that the printout happens when we want, rather than when the interface
    decides it can allocate the time to print. Don't flush too frequently though—the
    flush operation carries a computational cost (as does normal printing) and this
    will slow down the program.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`sys.stdout.flush()`来确保打印输出在代码仍在运行时发生。有时，特别是在某些单元格的大循环中，打印输出可能直到代码完成才发生。以这种方式刷新输出确保打印输出在我们想要的时候发生，而不是当界面决定可以分配时间打印的时候。但是，不要过于频繁地刷新——刷新操作（以及正常的打印）都会带来计算成本，这会减慢程序的速度。
- en: You can now run the above code.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以运行上述代码。
- en: The preceding code returns about 2000 frequent itemsets of varying lengths.
    You'll notice that the number of itemsets grows as the length increases before
    it shrinks. It grows because of the increasing number of possible rules. After
    a while, the large number of combinations no longer has the support necessary
    to be considered frequent. This results in the number shrinking. This shrinking
    is the benefit of the Apriori algorithm. If we search all possible itemsets (not
    just the supersets of frequent ones), we would be searching thousands of times
    more itemsets to see if they are frequent.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码返回了大约2000个不同长度的频繁项集。你会注意到，随着长度的增加，项集的数量先增加后减少。这是因为可能规则的数目在增加。过了一段时间，大量组合不再有必要的支持被认为是频繁的。这导致数量减少。这种减少是Apriori算法的优点。如果我们搜索所有可能的项集（而不仅仅是频繁项集的超集），我们将需要搜索成千上万的项集来查看它们是否频繁。
- en: Even if this shrinking didn't occur, the algorithm meets an absolute end when
    rules for a combination of all movies together is discovered. Therefore the Apriori
    algorithm will always terminate.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这种缩小没有发生，当发现所有电影的组合规则时，算法将达到绝对结束。因此，Apriori 算法将始终终止。
- en: It may take a few minutes for this code to run, more if you have older hardware.
    If you find you are having trouble running any of the code samples, take a look
    at using an online cloud provider for additional speed. Details about using the
    cloud to do the work are given in Appendix, Next Steps.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码可能需要几分钟，如果你有较旧的硬件，可能需要更长的时间。如果你发现运行任何代码示例有困难，可以考虑使用在线云服务提供商以获得额外的速度。有关使用云进行工作的详细信息，请参阅附录，下一步。
- en: Extracting association rules
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取关联规则
- en: After the Apriori algorithm has completed, we have a list of frequent itemsets.
    These aren't exactly association rules, but they can easily be converted into
    these rules. A frequent itemset is a set of items with a minimum support, while
    an association rule has a premise and a conclusion. The data is the same for the
    two.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Apriori 算法完成后，我们将有一个频繁项集的列表。这些不是精确的关联规则，但它们可以很容易地转换为这些规则。频繁项集是一组具有最小支持度的项，而关联规则有一个前提和结论。这两个的数据是相同的。
- en: 'We can make an *association rule from a frequent itemset* by taking one of
    the movies in the itemset and denoting it as the conclusion. The other movies
    in the itemset will be the premise. This will form rules of the following form:
    *if a reviewer recommends all of the movies in the premise, they will also recommend
    the conclusion movie*.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将项集中的一部电影作为结论，并将其他电影作为前提来从频繁项集中创建一个关联规则。这将形成以下形式的规则：*如果一个评论家推荐了前提中的所有电影，他们也会推荐结论电影*。
- en: For each itemset, we can generate a number of association rules by setting each
    movie to be the conclusion and the remaining movies as the premise.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个项集，我们可以通过将每部电影设置为结论，将剩余的电影作为前提来生成多个关联规则。
- en: In code, we first generate a list of all of the rules from each of the frequent
    itemsets, by iterating over each of the discovered frequent itemsets of each length.
    We then iterate over every movie in the itemset, as the conclusion.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们首先通过遍历每个长度的发现频繁项集，从每个频繁项集中生成所有规则的列表。然后，我们遍历项集中的每一部电影作为结论。
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This returns a very large number of candidate rules. We can see some by printing
    out the first few rules in the list:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个非常大的候选规则数。我们可以通过打印列表中的前几条规则来查看一些：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The resulting output shows the rules that were obtained:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出显示了获得的规则：
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In these rules, the first part (the `frozenset`) is the list of movies in the
    premise, while the number after it is the conclusion. In the first case, if a
    reviewer recommends movie 79, they are also likely to recommend movie 258.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些规则中，第一部分（`frozenset`）是前提中的电影列表，而它后面的数字是结论。在第一种情况下，如果一个评论家推荐了电影79，他们也很可能推荐电影258。
- en: Next, we compute the confidence of each of these rules. This is performed much
    like in Chapter 1*, Getting Started with Data Mining*, with the only changes being
    those necessary for computing using the new data format.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算这些规则中每个规则的置信度。这与第1章*《数据挖掘入门》*中的操作非常相似，唯一的区别是那些必要的更改，以便使用新的数据格式进行计算。
- en: The process of computing confidence starts by creating dictionaries to store
    how many times we see the premise leading to the conclusion (a correct example
    of the rule) and how many times it doesn't (an incorrect example). We then iterate
    over all reviews and rules, working out whether the premise of the rule applies
    and, if it does, whether the conclusion is accurate.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 计算置信度的过程首先是通过创建字典来存储我们看到前提导致结论（规则的正确示例）和它没有发生（规则的错误示例）的次数。然后，我们遍历所有评论和规则，确定规则的前提是否适用，如果适用，结论是否准确。
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We then compute the confidence for each rule by dividing the correct count
    by the total number of times the rule was seen:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过将正确计数除以规则被看到的总次数来计算每个规则的置信度：
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we can print the top five rules by sorting this confidence dictionary and
    printing the results:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过排序这个置信度字典并打印结果来打印前五条规则：
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The resulting printout shows only the movie IDs, which isn't very helpful without
    the names of the movies also. The dataset came with a file called u.items, which
    stores the movie names and their corresponding MovieID (as well as other information,
    such as the genre).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的打印输出只显示电影ID，没有电影名称的帮助并不太有用。数据集附带一个名为u.items的文件，该文件存储电影名称及其对应的MovieID（以及其他信息，如类型）。
- en: We can load the titles from this file using pandas. Additional information about
    the file and categories is available in the README file that came with the dataset.
    The data in the files is in CSV format, but with data separated by the | symbol;
    it has no header
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用pandas从这个文件中加载标题。有关文件和类别的更多信息可在随数据集提供的README文件中找到。文件中的数据是CSV格式，但数据由|符号分隔；它没有标题
- en: and the encoding is important to set. The column names were found in the README
    file.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 并且编码设置很重要。列名在README文件中找到。
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Getting the movie title is an important and frequently used step, therefore
    it makes sense to turn it into a function. We will create a function that will
    return a movie''s title from its MovieID, saving us the trouble of looking it
    up each time. Let''s look at the code:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 获取电影标题是一个重要且常用的步骤，因此将其转换为函数是有意义的。我们将创建一个函数，该函数将从其MovieID返回电影标题，从而避免每次都查找的麻烦。让我们看看代码：
- en: '[PRE23]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In a new Jupyter Notebook cell, we adjust our previous code for printing out
    the top rules to also include the titles:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个新的Jupyter Notebook单元中，我们调整了之前用于打印最佳规则的代码，以包括标题：
- en: '[PRE24]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The result is much more readable (there are still some issues, but we can ignore
    them for now):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 结果更易于阅读（仍然有一些问题，但现在我们可以忽略它们）：
- en: '[PRE25]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Evaluating the association rules
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估关联规则
- en: In a broad sense, we can evaluate the association rules using the same concept
    as for classification. We use a test set of data that was not used for training,
    and evaluate our discovered rules based on their performance in this test set.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在广义上，我们可以使用与分类相同的概念来评估关联规则。我们使用未用于训练的数据测试集，并根据它们在这个测试集中的性能来评估我们发现的规则。
- en: To do this, we will compute the test set confidence, that is, the confidence
    of each rule on the testing set. We won't apply a formal evaluation metric in
    this case; we simply examine the rules and look for good examples.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们将计算测试集置信度，即每个规则在测试集中的置信度。在这种情况下，我们不会应用正式的评估指标；我们只是检查规则并寻找好的例子。
- en: 'Formal evaluation could include a classification accuracy by determining the
    accuracy of predicting whether a user rates a given movie as favorable. In this
    case, as described below, we will informally look at the rules to find those that
    are more reliable:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正式评估可能包括通过确定用户是否对给定电影给予好评的预测准确性来进行分类准确率。在这种情况下，如下所述，我们将非正式地查看规则以找到那些更可靠的规则：
- en: 'First, we extract the test dataset, which is all of the records that we didn''t
    use in the training set. We used the first 200 users (by ID value) for the training
    set, and we will use all of the rest for the testing dataset. As with the training
    set, we will also get the favorable reviews for each of the users in this dataset
    as well. Let''s look at the code:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们提取测试数据集，这是我们未在训练集中使用的所有记录。我们使用了前200个用户（按ID值）作为训练集，我们将使用其余所有用户作为测试数据集。与训练集一样，我们还将获取该数据集中每个用户的正面评论。让我们看看代码：
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We then count the correct instances where the premise leads to the conclusion,
    in the same way that we did before. The only change here is the use of the test
    data instead of the training data. Let''s look at the code:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们计算前提导致结论的正确实例数，就像我们之前做的那样。这里唯一的变化是使用测试数据而不是训练数据。让我们看看代码：
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we compute the confidence of each rule from the correct counts and sort
    them. Let''s look at the code:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们计算每个规则的置信度，并按此排序。让我们看看代码：
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, we print out the best association rules with the titles instead of
    the movie IDs:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们以标题而不是电影ID的形式打印出最佳关联规则：
- en: '[PRE29]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can now see which rules are most applicable in new unseen data:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到哪些规则在新的未见数据中最为适用：
- en: '[PRE30]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The second rule, for instance, has a perfect confidence in the training data,
    but it is only accurate in 60 percent of cases for the test data. Many of the
    other rules in the top 10 have high confidences in test data, making them good
    rules for making recommendations.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，第二个规则在训练数据中具有完美的置信度，但在测试数据中只有60%的案例是准确的。前10条规则中的许多其他规则在测试数据中具有高置信度，这使得它们成为制定推荐的好规则。
- en: You may also notice that these movies tend to be very popular and good films.
    This gives us a baseline algorithm that we could compare against, i.e. instead
    of trying to do personalized recommendations, just recommend the most liked movies
    overall. Have a shot at implementing this algorithm - does the Apriori algorithm
    outperform it and by how much? Another baseline could be to simply recommend movies
    at random from the same genre.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还会注意到，这些电影往往非常受欢迎且是优秀的电影。这为我们提供了一个基准算法，我们可以将其与之比较，即不是尝试进行个性化推荐，而是推荐最受欢迎的电影。尝试实现这个算法——Apriori算法是否优于它，以及优于多少？另一个基准可能是简单地从同一类型中随机推荐电影。
- en: If you are looking through the rest of the rules, some will have a test confidence
    of -1\. Confidence values are always between 0 and 1\. This value indicates that
    the particular rule wasn't found in the test dataset at all.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在查看其余的规则，其中一些将具有-1的测试置信度。置信值总是在0和1之间。这个值表示特定的规则根本未在测试数据集中找到。
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter we performed affinity analysis in order to recommend movies
    based on a large set of reviewers. We did this in two stages. First, we found
    frequent itemsets in the data using the Apriori algorithm. Then, we created association
    rules from those itemsets.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们进行了亲和力分析，以便根据大量评论者推荐电影。我们分两个阶段进行。首先，我们使用Apriori算法在数据中找到频繁项集。然后，我们从这些项集中创建关联规则。
- en: 'The use of the Apriori algorithm was necessary due to the size of the dataset.
    In [Chapter 1](3dc86298-cd8c-4d02-a373-6cd303d5c558.xhtml)*, Getting Started With
    Data Mining*, we used a brute-force approach, which has exponential growth in
    the time needed to compute those rules required for a smarter approach. This is
    a common pattern for data mining: we can solve many problems in a brute force
    manner for small datasets, but smarter algorithms are required to apply the concepts
    to larger datasets.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集的大小，使用Apriori算法是必要的。在[第1章](3dc86298-cd8c-4d02-a373-6cd303d5c558.xhtml)*，数据挖掘入门*中，我们使用了暴力方法，这种方法在计算那些用于更智能方法的规则所需的时间上呈指数增长。这是数据挖掘中的一种常见模式：对于小数据集，我们可以以暴力方式解决许多问题，但对于大数据集，则需要更智能的算法来应用这些概念。
- en: We performed training on a subset of our data in order to find the association
    rules, and then tested those rules on the rest of the data—a testing set. From
    what we discussed in the previous chapters, we could extend this concept to use
    cross-fold validation to better evaluate the rules. This would lead to a more
    robust evaluation of the quality of each rule.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在我们的数据的一个子集上进行了训练，以找到关联规则，然后在这些规则的其余数据——测试集上进行了测试。根据我们之前章节的讨论，我们可以将这个概念扩展到使用交叉验证来更好地评估规则。这将导致对每个规则质量的更稳健的评估。
- en: To take the concepts in this chapter further, investigate which movies obtain
    high overall scores (i.e. lots of recommendations), but do not have adequate rules
    to recommend them to new users. How would you alter the algorithm to recommend
    these movies?
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步探讨本章的概念，研究哪些电影获得了很高的总体评分（即有很多推荐），但没有足够的规则来向新用户推荐它们。你将如何修改算法来推荐这些电影？
- en: So far, all of our datasets have been described in terms of features. However,
    not all datasets are *pre-defined* in this way. In the next chapter, we will look
    at scikit-learn's transformers (they were introduced in *Chapter 3, Predicting
    Sports Winners with Decision Trees*) as a way to extract features from data. We
    will discuss how to implement our own transformers, extend existing ones, and
    concepts we can implement using them.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所有的数据集都是用特征来描述的。然而，并非所有数据集都是以这种方式*预先定义*的。在下一章中，我们将探讨scikit-learn的转换器（它们在*第3章，使用决策树预测体育比赛赢家*中介绍过）作为从数据中提取特征的方法。我们将讨论如何实现我们自己的转换器，扩展现有的转换器，以及我们可以使用它们实现的概念。
