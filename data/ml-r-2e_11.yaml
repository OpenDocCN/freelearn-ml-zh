- en: Chapter 11. Improving Model Performance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章：改进模型性能
- en: When a sports team falls short of meeting its goal—whether the goal is to obtain
    an Olympic gold medal, a league championship, or a world record time—it must search
    for possible improvements. Imagine that you're the team's coach. How would you
    spend your practice sessions? Perhaps you'd direct the athletes to train harder
    or train differently in order to maximize every bit of their potential. Or, you
    might emphasize better teamwork, utilizing the athletes' strengths and weaknesses
    more smartly.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当一支运动队未能达到其目标时——无论目标是获得奥运金牌、联赛冠军还是世界纪录——它必须寻找可能的改进方向。假设你是该队的教练，你会如何安排训练？或许你会指示运动员更加努力训练或改变训练方式，以最大化他们的潜力。或者，你可能会强调更好的团队合作，更聪明地利用运动员的长处和短处。
- en: Now imagine that you're training a world champion machine learning algorithm.
    Perhaps you hope to compete in data mining competitions such as those posted on
    Kaggle ([http://www.kaggle.com/competitions](http://www.kaggle.com/competitions)).
    Maybe you simply need to improve business results. Where do you begin? Although
    the context differs, the strategies one uses to improve sports team performance
    can also be used to improve the performance of statistical learners.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你正在训练一款世界级的机器学习算法。也许你希望参加数据挖掘竞赛，比如Kaggle上发布的竞赛（[http://www.kaggle.com/competitions](http://www.kaggle.com/competitions)）。或者你仅仅是希望改善商业成果。你该从哪里开始？尽管背景不同，但提升运动队表现的策略同样可以用来提升统计学习器的表现。
- en: 'As the coach, it is your job to find the combination of training techniques
    and teamwork skills that allow you to meet your performance goals. This chapter
    builds upon the material covered throughout this book to introduce a set of techniques
    for improving the predictive performance of machine learners. You will learn:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作为教练，你的任务是找到训练技巧和团队协作技能的组合，以帮助你实现性能目标。本章在本书中所覆盖的内容的基础上，介绍了一组提高机器学习器预测性能的技巧。你将学习：
- en: How to automate model performance tuning by systematically searching for the
    optimal set of training conditions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过系统地寻找最佳训练条件的组合来自动化模型性能调优
- en: The methods for combining models into groups that use teamwork to tackle tough
    learning tasks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型组合成利用团队合作解决困难学习任务的方法
- en: How to apply a variant of decision trees, which has quickly become popular due
    to its impressive performance
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何应用一种变体的决策树，这种决策树因其出色的表现而迅速流行
- en: None of these methods will be successful for every problem. Yet, looking at
    the winning entries to machine learning competitions, you'll likely find that
    at least one of them has been employed. To be competitive, you too will need to
    add these skills to your repertoire.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法并非对每个问题都有效。然而，从机器学习竞赛的获奖作品来看，你可能会发现其中至少使用了某种方法。为了具有竞争力，你也需要将这些技能纳入你的技能库。
- en: Tuning stock models for better performance
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调优标准模型以提高性能
- en: Some learning problems are well-suited to the stock models presented in the
    previous chapters. In such cases, it may not be necessary to spend much time iterating
    and refining the model; it may perform well enough as it is. On the other hand,
    some problems are inherently more difficult. The underlying concepts to be learned
    may be extremely complex, requiring an understanding of many subtle relationships,
    or it may be affected by random variation, making it difficult to define the signal
    within the noise.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一些学习问题非常适合前几章中介绍的标准模型。在这种情况下，可能不需要花费太多时间反复调整和优化模型；它可能已经足够好。然而，另一方面，有些问题本质上更为复杂。需要学习的基本概念可能极为复杂，涉及许多微妙的关系，或者它可能受到随机变化的影响，使得在噪音中定义信号变得困难。
- en: Developing models that perform extremely well on difficult problems is every
    bit an art as it is a science. Sometimes a bit of intuition is helpful when trying
    to identify areas where performance can be improved. In other cases, finding improvements
    will require a brute-force, trial and error approach. Of course, the process of
    searching numerous possible improvements can be aided by the use of automated
    programs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 开发在困难问题上表现极佳的模型既是一门艺术，也是一门科学。在尝试找出性能提升的方向时，直觉有时是有帮助的。在其他情况下，找到提升的方法可能需要一种蛮力式的反复试验方法。当然，借助自动化程序，搜索可能的改进方法的过程可以得到帮助。
- en: 'In [Chapter 5](ch05.html "Chapter 5. Divide and Conquer – Classification Using
    Decision Trees and Rules"), *Divide and Conquer – Classification Using Decision
    Trees and Rules*, we attempted a difficult problem: identifying loans that were
    likely to enter into default. Although we were able to use performance tuning
    methods to obtain a respectable classification accuracy of about 82 percent, upon
    a more careful examination in [Chapter 10](ch10.html "Chapter 10. Evaluating Model
    Performance"), *Evaluating Model Performance*, we realized that the high accuracy
    was a bit misleading. In spite of the reasonable accuracy, the kappa statistic
    was only about 0.28, which suggested that the model was actually performing somewhat
    poorly. In this section, we''ll revisit the credit scoring model to see whether
    we can improve the results.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html "第5章：分治法——使用决策树和规则进行分类")，*分治法——使用决策树和规则进行分类*中，我们尝试了解决一个难题：识别可能进入违约的贷款。尽管我们通过性能调优方法获得了大约82%的可接受分类准确率，但在[第10章](ch10.html
    "第10章：模型性能评估")，*模型性能评估*中仔细检查后，我们发现高准确率有些误导。尽管准确率合理，但 Kappa 统计量只有大约 0.28，表明模型的实际表现并不理想。在这一节中，我们将重新审视信用评分模型，看看是否可以改善结果。
- en: Tip
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: To follow along with the examples, download the `credit.csv` file from the Packt
    Publishing website and save it to your R working directory. Load the file into
    R using the command `credit <- read.csv("credit.csv")`.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随示例，下载 Packt Publishing 网站上的 `credit.csv` 文件，并将其保存到你的 R 工作目录中。使用命令 `credit
    <- read.csv("credit.csv")` 将文件加载到 R 中。
- en: You will recall that we first used a stock C5.0 decision tree to build the classifier
    for the credit data. We then attempted to improve its performance by adjusting
    the `trials` parameter to increase the number of boosting iterations. By increasing
    the number of iterations from the default of 1 up to the value of 10, we were
    able to increase the model's accuracy. This process of adjusting the model options
    to identify the best fit is called **parameter** **tuning**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，我们首先使用了一个标准的 C5.0 决策树来构建信用数据的分类器。然后，我们尝试通过调整 `trials` 参数来增加提升迭代次数，从而提高模型性能。通过将迭代次数从默认值
    1 增加到 10，我们成功提高了模型的准确度。这个调整模型选项以找出最佳拟合的过程称为**参数** **调优**。
- en: Parameter tuning is not limited to decision trees. For instance, we tuned k-NN
    models when we searched for the best value of *k*. We also tuned neural networks
    and support vector machines as we adjusted the number of nodes or hidden layers,
    or chose different kernel functions. Most machine learning algorithms allow the
    adjustment of at least one parameter, and the most sophisticated models offer
    a large number of ways to tweak the model fit. Although this allows the model
    to be tailored closely to the learning task, the complexity of all the possible
    options can be daunting. A more systematic approach is warranted.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 参数调优不限于决策树。例如，当我们搜索最佳 *k* 值时，我们对 k-NN 模型进行了调优。当我们调整神经网络和支持向量机的节点数或隐藏层数，或者选择不同的核函数时，我们也进行了调优。大多数机器学习算法允许调整至少一个参数，而最复杂的模型提供了大量调节模型拟合的方法。尽管这使得模型能够更好地适应学习任务，但所有可能选项的复杂性可能会让人感到压倒。此时，更系统的方法是必要的。
- en: Using caret for automated parameter tuning
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 caret 进行自动化参数调优
- en: Rather than choosing arbitrary values for each of the model's parameters—a task
    that is not only tedious, but also somewhat unscientific—it is better to conduct
    a search through many possible parameter values to find the best combination.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与其为每个模型的参数选择任意的值——这不仅是繁琐的，而且有些不科学——不如通过搜索多个可能的参数值来找到最佳组合。
- en: The `caret` package, which we used extensively in [Chapter 10](ch10.html "Chapter 10. Evaluating
    Model Performance"), *Evaluating Model Performance*, provides tools to assist
    with automated parameter tuning. The core functionality is provided by a `train()`
    function that serves as a standardized interface for over 175 different machine
    learning models for both classification and regression tasks. By using this function,
    it is possible to automate the search for optimal models using a choice of evaluation
    methods and metrics.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`caret` 包，我们在[第10章](ch10.html "第10章：模型性能评估")，*模型性能评估*中广泛使用，提供了帮助自动化参数调优的工具。核心功能由
    `train()` 函数提供，该函数作为一个标准化接口，支持超过175种不同的机器学习模型，用于分类和回归任务。通过使用这个函数，可以通过选择不同的评估方法和指标，自动化地搜索最优模型。'
- en: Tip
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Do not feel overwhelmed by the large number of models—we've already covered
    many of them in the earlier chapters. Others are simple variants or extensions
    of the base concepts. Given what you've learned so far, you should be confident
    that you have the ability to understand all of the available methods.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 不要被大量模型吓到——我们在前面的章节中已经介绍了很多模型。其他的模型只是基础概念的简单变体或扩展。考虑到你目前所学的内容，你应该有信心能理解所有可用的方法。
- en: 'Automated parameter tuning requires you to consider three questions:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 自动调优参数需要你考虑三个问题：
- en: What type of machine learning model (and specific implementation) should be
    trained on the data?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该在数据上训练什么类型的机器学习模型（以及具体的实现）？
- en: Which model parameters can be adjusted, and how extensively should they be tuned
    to find the optimal settings?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些模型参数可以调整，应该如何调节这些参数以找到最佳设置？
- en: What criteria should be used to evaluate the models to find the best candidate?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该使用什么标准来评估模型，以找到最佳候选模型？
- en: Answering the first question involves finding a well-suited match between the
    machine learning task and one of the 175 models. Obviously, this requires an understanding
    of the breadth and depth of machine learning models. It can also help to work
    through a process of elimination. Nearly half of the models can be eliminated
    depending on whether the task is classification or numeric prediction; others
    can be excluded based on the format of the data or the need to avoid black box
    models, and so on. In any case, there's also no reason you can't try several approaches
    and compare the best results of each.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 回答第一个问题需要在机器学习任务和175个模型之间找到一个合适的匹配。显然，这需要对机器学习模型的广度和深度有一定了解。进行排除法也会有所帮助。根据任务是分类还是数值预测，几乎一半的模型可以被排除；其他的可以根据数据的格式或是否需要避免使用黑箱模型来排除。无论如何，也没有理由不能尝试多种方法并比较每种方法的最佳结果。
- en: Addressing the second question is a matter largely dictated by the choice of
    model, since each algorithm utilizes a unique set of parameters. The available
    tuning parameters for the predictive models covered in this book are listed in
    the following table. Keep in mind that although some models have additional options
    not shown, only those listed in the table are supported by `caret` for automatic
    tuning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 解决第二个问题在很大程度上取决于模型的选择，因为每个算法使用一组独特的参数。本书中涵盖的预测模型的可用调优参数列在下表中。请记住，虽然一些模型可能有未显示的额外选项，但`caret`仅支持表中列出的选项进行自动调优。
- en: '| Model | Learning Task | Method name | Parameters |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 学习任务 | 方法名称 | 参数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| k-Nearest Neighbors | Classification | `knn` | `k` |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| k近邻算法 | 分类 | `knn` | `k` |'
- en: '| Naive Bayes | Classification | `nb` | `fL`, `usekernel` |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 分类 | `nb` | `fL`, `usekernel` |'
- en: '| Decision Trees | Classification | `C5.0` | `model`, `trials`, `winnow` |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 分类 | `C5.0` | `model`, `trials`, `winnow` |'
- en: '| OneR Rule Learner | Classification | `OneR` | None |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| OneR规则学习器 | 分类 | `OneR` | 无 |'
- en: '| RIPPER Rule Learner | Classification | `JRip` | `NumOpt` |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| RIPPER规则学习器 | 分类 | `JRip` | `NumOpt` |'
- en: '| Linear Regression | Regression | `lm` | None |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 线性回归 | 回归 | `lm` | 无 |'
- en: '| Regression Trees | Regression | `rpart` | `cp` |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 回归树 | 回归 | `rpart` | `cp` |'
- en: '| Model Trees | Regression | `M5` | `pruned`, `smoothed`, `rules` |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 模型树 | 回归 | `M5` | `pruned`, `smoothed`, `rules` |'
- en: '| Neural Networks | Dual use | `nnet` | `size`, `decay` |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 神经网络 | 双重用途 | `nnet` | `size`, `decay` |'
- en: '| Support Vector Machines (Linear Kernel) | Dual use | `svmLinear` | `C` |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 支持向量机（线性核） | 双重用途 | `svmLinear` | `C` |'
- en: '| Support Vector Machines (Radial Basis Kernel) | Dual use | `svmRadial` |
    `C, sigma` |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 支持向量机（径向基核） | 双重用途 | `svmRadial` | `C, sigma` |'
- en: '| Random Forests | Dual use | `rf` | `mtry` |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | 双重用途 | `rf` | `mtry` |'
- en: Tip
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: For a complete list of the models and corresponding tuning parameters covered
    by `caret`, refer to the table provided by package author Max Kuhn at [http://topepo.github.io/caret/modelList.html](http://topepo.github.io/caret/modelList.html).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看`caret`所涵盖的模型及其调优参数的完整列表，请参考包作者Max Kuhn提供的表格：[http://topepo.github.io/caret/modelList.html](http://topepo.github.io/caret/modelList.html)。
- en: 'If you ever forget the tuning parameters for a particular model, the `modelLookup()`
    function can be used to find them. Simply supply the method name, as illustrated
    here for the C5.0 model:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你忘记了某个模型的调优参数，可以使用`modelLookup()`函数来查找它们。只需提供方法名称，以下是C5.0模型的示例：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The goal of automatic tuning is to search a set of candidate models comprising
    a matrix, or **grid**, of parameter combinations. Because it is impractical to
    search every conceivable combination, only a subset of possibilities is used to
    construct the grid. By default, `caret` searches at most three values for each
    of the *p* parameters. This means that at most *3^p* candidate models will be
    tested. For example, by default, the automatic tuning of k-Nearest Neighbors will
    compare *3^1 = 3* candidate models with `k=5`, `k=7`, and `k=9`. Similarly, tuning
    a decision tree will result in a comparison of up to 27 different candidate models,
    comprising the grid of *3^3 = 27* combinations of `model`, `trials`, and `winnow`
    settings. In practice, however, only 12 models are actually tested. This is because
    the `model` and `winnow` parameters can only take two values (`tree` versus `rules`
    and `TRUE` versus `FALSE`, respectively), which makes the grid size *3 * 2 * 2
    = 12*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 自动调整的目标是搜索一个候选模型集合，这些模型由一组参数组合的矩阵或**网格**构成。由于不现实去搜索所有可能的组合，因此只使用部分可能性来构建网格。默认情况下，`caret`
    每个 *p* 参数最多搜索三个值。这意味着最多会测试 *3^p* 个候选模型。例如，默认情况下，k-最近邻的自动调整会比较 *3^1 = 3* 个候选模型，分别对应
    `k=5`、`k=7` 和 `k=9`。类似地，调整决策树将比较最多 27 个不同的候选模型，这些模型由 *3^3 = 27* 个 `model`、`trials`
    和 `winnow` 设置的组合构成。然而，实际上，只会测试 12 个模型。这是因为 `model` 和 `winnow` 参数只能取两个值（分别是 `tree`
    与 `rules` 和 `TRUE` 与 `FALSE`），所以网格大小为 *3 * 2 * 2 = 12*。
- en: Tip
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Since the default search grid may not be ideal for your learning problem, `caret`
    allows you to provide a custom search grid defined by a simple command, which
    we will cover later.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于默认的搜索网格可能并不适合你的学习问题，`caret` 允许你通过一个简单的命令提供自定义的搜索网格，我们将在后面介绍。
- en: The third and final step in automatic model tuning involves identifying the
    best model among the candidates. This uses the methods discussed in [Chapter 10](ch10.html
    "Chapter 10. Evaluating Model Performance"), *Evaluating Model Performance*, such
    as the choice of resampling strategy for creating training and test datasets and
    the use of model performance statistics to measure the predictive accuracy.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 自动调整模型的第三步也是最后一步，涉及从候选模型中识别最佳模型。这个过程使用了在[第10章](ch10.html "第10章. 评估模型性能")中讨论的方法，即通过选择重采样策略来创建训练集和测试集，并使用模型性能统计量来衡量预测准确性。
- en: All of the resampling strategies and many of the performance statistics we've
    learned are supported by `caret`. These include statistics such as accuracy and
    kappa (for classifiers) and R-squared or RMSE (for numeric models). Cost-sensitive
    measures such as sensitivity, specificity, and area under the ROC curve (AUC)
    can also be used, if desired.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所有我们学到的重采样策略和许多性能统计量都得到了 `caret` 的支持。这些包括精度、Kappa（对于分类器）以及 R-squared 或 RMSE（对于数值模型）等统计量。如果需要，还可以使用如灵敏度、特异性和
    ROC 曲线下的面积（AUC）等成本敏感度度量。
- en: By default, `caret` will select the candidate model with the largest value of
    the desired performance measure. As this practice sometimes results in the selection
    of models that achieve marginal performance improvements via large increases in
    model complexity, alternative model selection functions are provided.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`caret` 会选择具有最大性能度量值的候选模型。由于这种做法有时会选择那些通过大幅增加模型复杂度来实现边际性能提升的模型，因此提供了替代的模型选择函数。
- en: Given the wide variety of options, it is helpful that many of the defaults are
    reasonable. For instance, `caret` will use prediction accuracy on a bootstrap
    sample to choose the best performer for classification models. Beginning with
    these default values, we can then tweak the `train()` function to design a wide
    variety of experiments.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于有各种各样的选项，许多默认设置是合理的，这一点非常有帮助。例如，`caret` 将使用在自助法样本上的预测准确度来选择最佳的分类模型表现者。通过这些默认值开始，我们可以调整
    `train()` 函数来设计各种各样的实验。
- en: Creating a simple tuned model
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个简单的调整模型
- en: To illustrate the process of tuning a model, let's begin by observing what happens
    when we attempt to tune the credit scoring model using the `caret` package's default
    settings. From there, we will adjust the options to our liking.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明调整模型的过程，我们首先观察一下当我们尝试使用 `caret` 包的默认设置来调整信用评分模型时会发生什么。接下来，我们将根据需要调整选项。
- en: 'The simplest way to tune a learner requires you to only specify a model type
    via the `method` parameter. Since we used C5.0 decision trees previously with
    the credit model, we''ll continue our work by optimizing this learner. The basic
    `train()` command for tuning a C5.0 decision tree using the default settings is
    as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 调整学习器的最简单方法只需通过`method`参数指定模型类型即可。由于我们之前在`credit`模型中使用了C5.0决策树，因此我们将继续通过优化此学习器来进行后续工作。使用默认设置调整C5.0决策树的基本`train()`命令如下：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: First, the `set.seed()` function is used to initialize R's random number generator
    to a set starting position. You may recall that we used this function in several
    prior chapters. By setting the `seed` parameter (in this case to the arbitrary
    number 300), the random numbers will follow a predefined sequence. This allows
    simulations that use random sampling to be repeated with identical results—a very
    helpful feature if you are sharing code or attempting to replicate a prior result.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用`set.seed()`函数初始化R的随机数生成器，以一个设定的起始位置。你可能还记得我们在之前的章节中使用过这个函数。通过设置`seed`参数（在这里设为任意的数字300），随机数将按照预定的序列生成。这使得使用随机抽样的模拟可以重复进行并得到相同的结果——这对于共享代码或重复先前的结果非常有帮助。
- en: Next, we define a tree as `default ~ .` using the R formula interface. This
    models loan default status (`yes` or `no`) using all of the other features in
    the `credit` data frame. The parameter `method = "C5.0"` tells `caret` to use
    the C5.0 decision tree algorithm.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用R公式接口定义一个树模型，表示为`default ~ .`。这个模型使用`credit`数据框中的所有其他特征来建模贷款违约状态（`yes`或`no`）。参数`method
    = "C5.0"`告诉`caret`使用C5.0决策树算法。
- en: After you've entered the preceding command, there may be a significant delay
    (dependent upon your computer's capabilities) as the tuning process occurs. Even
    though this is a fairly small dataset, a substantial amount of calculation must
    occur. R must repeatedly generate random samples of data, build decision trees,
    compute performance statistics, and evaluate the result.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入前述命令后，可能会有一个较长的延迟（具体取决于你计算机的性能），因为调整过程正在进行。尽管这是一个相对较小的数据集，但仍需要进行大量的计算。R必须反复生成随机数据样本，构建决策树，计算性能统计量并评估结果。
- en: 'The result of the experiment is saved in an object named `m`. If you would
    like to examine the object''s contents, the `str(m)` command will list all the
    associated data, but this can be quite overwhelming. Instead, simply type the
    name of the object for a condensed summary of the results. For instance, typing
    `m` yields the following output (note that labels have been added for clarity):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 实验的结果保存在一个名为`m`的对象中。如果你想检查该对象的内容，`str(m)`命令会列出所有相关数据，但这可能会显得有些繁杂。相反，只需输入对象的名称，即可获得一个简洁的结果概览。例如，输入`m`会产生以下输出（注意，为了清晰起见，已添加标签）：
- en: '![Creating a simple tuned model](img/B03905_11_01.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![创建一个简单的调整模型](img/B03905_11_01.jpg)'
- en: 'The labels highlight four main components in the output:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 标签突出显示了输出中的四个主要组成部分：
- en: '**A brief description of the input dataset**: If you are familiar with your
    data and have applied the `train()` function correctly, this information should
    not be surprising.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入数据集的简要描述**：如果你对自己的数据比较熟悉，并且正确应用了`train()`函数，这些信息应该不会让你感到惊讶。'
- en: '**A report of the preprocessing and resampling methods applied**: Here, we
    see that 25 bootstrap samples, each including 1,000 examples, were used to train
    the models.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**应用的预处理和重采样方法报告**：在这里，我们看到使用了25个bootstrap样本，每个样本包括1,000个示例，用于训练模型。'
- en: '**A list of the candidate models evaluated**: In this section, we can confirm
    that 12 different models were tested, based on the combinations of three C5.0
    tuning parameters—`model`, `trials`, and `winnow`. The average and standard deviation
    of the accuracy and kappa statistics for each candidate model are also shown.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估的候选模型列表**：在这一部分，我们可以确认测试了12个不同的模型，这些模型是基于三个C5.0调整参数的组合——`model`、`trials`和`winnow`。每个候选模型的准确性和卡帕统计量的平均值和标准差也在这里显示。'
- en: '**The choice of the best model**: As the footnote describes, the model with
    the largest accuracy was selected. This was the model that used a decision tree
    with 20 trials and the setting `winnow = FALSE`.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最佳模型的选择**：正如脚注所述，选择了准确性最高的模型。这个模型使用了一个有20次试验的决策树，并设置`winnow = FALSE`。'
- en: 'After identifying the best model, the `train()` function uses its tuning parameters
    to build a model on the full input dataset, which is stored in the `m` list object
    as `m$finalModel`. In most cases, you will not need to work directly with the
    `finalModel` sub-object. Instead, simply use the `predict()` function with the
    `m` object as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定最佳模型后，`train()` 函数使用其调优参数在完整的输入数据集上构建模型，并将其存储在 `m` 列表对象中的 `m$finalModel`。在大多数情况下，你不需要直接使用
    `finalModel` 子对象。相反，只需按如下方式使用 `predict()` 函数和 `m` 对象：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The resulting vector of predictions works as expected, allowing us to create
    a confusion matrix that compares the predicted and actual values:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的预测向量按预期工作，使我们能够创建一个比较预测值和实际值的混淆矩阵：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Of the 1,000 examples used for training the final model, only two were misclassified.
    However, it is very important to note that since the model was built on both the
    training and test data, this accuracy is optimistic and thus, should not be viewed
    as indicative of performance on unseen data. The bootstrap estimate of 73 percent
    (shown in the summary output) is a more realistic estimate of future performance.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在用于训练最终模型的 1,000 个示例中，只有两个被错误分类。然而，非常重要的是要注意，由于模型是基于训练数据和测试数据构建的，因此该准确性是乐观的，不能作为未见数据的性能指标。总结输出中的
    73% 自助法估计值是对未来表现的更现实估计。
- en: Using the `train()` and `predict()` functions also offers a couple of benefits
    in addition to the automatic parameter tuning.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `train()` 和 `predict()` 函数除了自动参数调优外，还提供了几个额外的好处。
- en: First, any data preparation steps applied by the `train()` function will be
    similarly applied to the data used for generating predictions. This includes transformations
    such as centering and scaling as well as imputation of missing values. Allowing
    `caret` to handle the data preparation will ensure that the steps that contributed
    to the best model's performance will remain in place when the model is deployed.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，`train()` 函数应用的任何数据准备步骤将同样应用于用于生成预测的数据。这包括像中心化、缩放以及缺失值填充等变换。让 `caret` 处理数据准备可以确保那些对最佳模型性能有贡献的步骤在模型部署时得以保留。
- en: 'Second, the `predict()` function provides a standardized interface for obtaining
    predicted class values and class probabilities, even for model types that ordinarily
    would require additional steps to obtain this information. The predicted classes
    are provided by default:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，`predict()` 函数提供了一个标准化接口，用于获取预测的类别值和类别概率，即使是那些通常需要额外步骤才能获取这些信息的模型类型。预测类别默认会被提供：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To obtain the estimated probabilities for each class, use the `type = "prob"`
    parameter:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得每个类别的估计概率，请使用 `type = "prob"` 参数：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Even in cases where the underlying model refers to the prediction probabilities
    using a different string (for example, `"raw"` for a `naiveBayes` model), the
    `predict()` function will translate `type = "prob"` to the appropriate string
    behind the scenes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在底层模型使用不同字符串表示预测概率的情况下（例如，`naiveBayes` 模型使用 `"raw"`），`predict()` 函数也会在幕后将
    `type = "prob"` 转换为适当的字符串。
- en: Customizing the tuning process
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义调优过程
- en: The decision tree we created previously demonstrates the `caret` package's ability
    to produce an optimized model with minimal intervention. The default settings
    allow optimized models to be created easily. However, it is also possible to change
    the default settings to something more specific to a learning task, which may
    assist with unlocking the upper echelon of performance.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前创建的决策树展示了 `caret` 包在最小干预下生成优化模型的能力。默认设置使得优化模型的创建变得简单。然而，也可以更改默认设置，使其更符合学习任务的具体需求，这有助于解锁更高水平的性能。
- en: Each step in the model selection process can be customized. To illustrate this
    flexibility, let's modify our work on the credit decision tree to mirror the process
    we had used in [Chapter 10](ch10.html "Chapter 10. Evaluating Model Performance"),
    *Evaluating Model Performance*. If you remember, we had estimated the kappa statistic
    using 10-fold cross-validation. We'll do the same here, using kappa to optimize
    the boosting parameter of the decision tree. Note that decision tree boosting
    was previously covered in [Chapter 5](ch05.html "Chapter 5. Divide and Conquer
    – Classification Using Decision Trees and Rules"), *Divide and Conquer – Classification
    Using Decision Trees and Rules*, and will also be covered in greater detail later
    this chapter.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择过程中的每个步骤都可以定制。为了说明这一灵活性，让我们修改在信用决策树中的工作，使其与我们在[第10章](ch10.html "第10章：评估模型性能")中使用的过程相似，*评估模型性能*。如果你还记得，我们使用10折交叉验证估算了kappa统计量。我们将在这里做同样的事情，使用kappa来优化决策树的提升参数。请注意，决策树提升在[第5章](ch05.html
    "第5章：分而治之——使用决策树和规则进行分类")中已有介绍，并将在本章后面更详细地讨论。
- en: 'The `trainControl()` function is used to create a set of configuration options
    known as a **control object**, which guides the `train()` function. These options
    allow for the management of model evaluation criteria such as the resampling strategy
    and the measure used for choosing the best model. Although this function can be
    used to modify nearly every aspect of a tuning experiment, we''ll focus on the
    two important parameters: `method` and `selectionFunction`.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`trainControl()`函数用于创建一组配置选项，称为**控制对象**，它指导`train()`函数的执行。这些选项允许管理模型评估标准，例如重采样策略和用于选择最佳模型的度量标准。尽管此函数可以用来修改调优实验的几乎每个方面，我们将重点关注两个重要的参数：`method`和`selectionFunction`。'
- en: Tip
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If you're eager for more details, you can use the `?trainControl` command for
    a list of all the parameters.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你渴望获取更多细节，可以使用`?trainControl`命令查看所有参数的列表。
- en: For the `trainControl()` function, the `method` parameter is used to set the
    resampling method, such as holdout sampling or k-fold cross-validation. The following
    table lists the possible method types as well as any additional parameters for
    adjusting the sample size and number of iterations. Although the default options
    for these resampling methods follow popular convention, you may choose to adjust
    these depending upon the size of your dataset and the complexity of your model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`trainControl()`函数，`method`参数用于设置重采样方法，如留出法或k折交叉验证。下表列出了所有可能的重采样方法类型以及调整样本大小和迭代次数的其他参数。尽管这些重采样方法的默认选项遵循了常见的惯例，但你可以根据数据集的大小和模型的复杂性调整这些选项。
- en: '| Resampling method | Method name | Additional options and default values |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 重采样方法 | 方法名称 | 额外选项及默认值 |'
- en: '| --- | --- | --- |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Holdout sampling | `LGOCV` | `p = 0.75` (training data proportion) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 留出法采样 | `LGOCV` | `p = 0.75`（训练数据比例） |'
- en: '| k-fold cross-validation | `cv` | `number = 10` (number of folds) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| k折交叉验证 | `cv` | `number = 10`（折数） |'
- en: '| Repeated k-fold cross-validation | `repeatedcv` | `number = 10` (number of
    folds)`repeats = 10` (number of iterations) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 重复k折交叉验证 | `repeatedcv` | `number = 10`（折数）`repeats = 10`（迭代次数） |'
- en: '| Bootstrap sampling | `boot` | `number = 25` (resampling iterations) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 自助法采样 | `boot` | `number = 25`（重采样迭代次数） |'
- en: '| 0.632 bootstrap | `boot632` | `number = 25` (resampling iterations) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 0.632自助法 | `boot632` | `number = 25`（重采样迭代次数） |'
- en: '| Leave-one-out cross-validation | `LOOCV` | None |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 留一交叉验证 | `LOOCV` | 无 |'
- en: The `selectionFunction` parameter is used to specify the function that will
    choose the optimal model among the various candidates. Three such functions are
    included. The `best` function simply chooses the candidate with the best value
    on the specified performance measure. This is used by default. The other two functions
    are used to choose the most parsimonious, or simplest, model that is within a
    certain threshold of the best model's performance. The `oneSE` function chooses
    the simplest candidate within one standard error of the best performance, and
    `tolerance` uses the simplest candidate within a user-specified percentage.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`selectionFunction` 参数用于指定将在各种候选模型中选择最佳模型的函数。包括三种此类函数。`best` 函数简单地选择在指定性能度量上表现最好的候选模型，这是默认使用的函数。其他两个函数用于选择在性能上接近最佳模型的最简模型。`oneSE`
    函数选择在最佳表现的一个标准误差范围内最简单的候选模型，而 `tolerance` 函数则在用户指定的百分比范围内选择最简单的候选模型。'
- en: Tip
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Some subjectivity is involved with the `caret` package's ranking of models by
    simplicity. For information on how models are ranked, see the help page for the
    selection functions by typing `?best` at the R command prompt.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`caret` 包根据简化性对模型进行排名时涉及一些主观性。如需了解如何对模型进行排名，请在 R 命令提示符下输入 `?best`，查看选择函数的帮助页面。'
- en: 'To create a control object named `ctrl` that uses 10-fold cross-validation
    and the `oneSE` selection function, use the following command (note that `number
    = 10` is included only for clarity; since this is the default value for `method
    = "cv"`, it could have been omitted):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 若要创建一个名为 `ctrl` 的控制对象，该对象使用 10 折交叉验证和 `oneSE` 选择函数，请使用以下命令（请注意，`number = 10`
    仅为清晰起见包含，因为这是 `method = "cv"` 的默认值，可以省略）：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We'll use the result of this function shortly.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快将使用这个函数的结果。
- en: In the meantime, the next step in defining our experiment is to create the grid
    of parameters to optimize. The grid must include a column named for each parameter
    in the desired model, prefixed by a period. It must also include a row for each
    desired combination of parameter values. Since we are using a C5.0 decision tree,
    this means we'll need columns named `.model`, `.trials`, and `.winnow`. For other
    machine learning models, refer to the table presented earlier in this chapter
    or use the `modelLookup()` function to lookup the parameters as described previously.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，定义实验的下一步是创建要优化的参数网格。该网格必须包含每个期望模型参数命名的列，并以句点作为前缀。它还必须包含每个期望参数值组合的行。由于我们使用的是
    C5.0 决策树，这意味着我们需要名为 `.model`、`.trials` 和 `.winnow` 的列。对于其他机器学习模型，请参考本章前面提供的表格，或者使用
    `modelLookup()` 函数根据之前的描述查找参数。
- en: 'Rather than filling this data frame cell by cell—a tedious task if there are
    many possible combinations of parameter values—we can use the `expand.grid()`
    function, which creates data frames from the combinations of all the values supplied.
    For example, suppose we would like to hold constant `model = "tree"` and `winnow
    = "FALSE"` while searching eight different values of trials. This can be created
    as:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与其一一填写数据框的每个单元格——如果有许多参数值组合，这将是一项繁琐的任务——我们可以使用 `expand.grid()` 函数，它可以根据提供的所有值的组合创建数据框。例如，假设我们希望保持
    `model = "tree"` 和 `winnow = "FALSE"` 不变，同时搜索八种不同的 `trials` 值。这可以这样创建：
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The resulting grid data frame contains *1 * 8 * 1 = 8* rows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的网格数据框包含 *1 * 8 * 1 = 8* 行：
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `train()` function will build a candidate model for evaluation using each
    row's combination of model parameters.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`train()` 函数将使用每行的模型参数组合构建一个候选模型进行评估。'
- en: 'Given this search grid and the control list created previously, we are ready
    to run a thoroughly customized `train()` experiment. As we did earlier, we''ll
    set the random seed to the arbitrary number `300` in order to ensure repeatable
    results. But this time, we''ll pass our control object and tuning grid while adding
    a parameter `metric = "Kappa"`, indicating the statistic to be used by the model
    evaluation function—in this case, `"oneSE"`. The full command is as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这个搜索网格和先前创建的控制列表，我们准备运行一个彻底定制的 `train()` 实验。像之前一样，我们将随机种子设置为任意数字 `300`，以确保结果可重复。但这次，我们将在传递控制对象和调参网格的同时，添加一个参数
    `metric = "Kappa"`，指示模型评估函数要使用的统计量——在这种情况下为 `"oneSE"`。完整命令如下：
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This results in an object that we can view by typing its name:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个对象，我们可以通过输入其名称查看：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Customizing the tuning process](img/B03905_11_02.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![自定义调优过程](img/B03905_11_02.jpg)'
- en: Although much of the output is similar to the automatically tuned model, there
    are a few differences of note. As 10-fold cross-validation was used, the sample
    size to build each candidate model was reduced to 900 rather than the 1,000 used
    in the bootstrap. As we requested, eight candidate models were tested. Additionally,
    because `model` and `winnow` were held constant, their values are no longer shown
    in the results; instead, they are listed as a footnote.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大部分输出与自动调优后的模型相似，但仍有一些值得注意的差异。由于使用了10折交叉验证，构建每个候选模型的样本大小被减少到了900，而不是自助法中的1,000。根据我们的要求，测试了八个候选模型。此外，由于`model`和`winnow`保持不变，它们的值不再显示在结果中，而是作为脚注列出。
- en: The best model here differs quite significantly from the prior trial. Before,
    the best model used `trials = 20`, whereas here, it used `trials = 1`. This seemingly
    odd finding is due to the fact that we used the `oneSE` rule rather the `best`
    rule to select the optimal model. Even though the 35-trial model offers the best
    raw performance according to kappa, the 1-trial model offers nearly the same performance
    with a much simpler form. Not only are simple models more computationally efficient,
    but they also reduce the chance of overfitting the training data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的最佳模型与之前的实验差异明显。之前，最佳模型使用了`trials = 20`，而这里使用的是`trials = 1`。这个看似奇怪的发现是因为我们使用了`oneSE`规则，而不是`best`规则来选择最佳模型。尽管35次实验模型根据kappa值提供了最好的原始性能，但1次实验模型在性能上几乎相同，而且形式更加简单。简单模型不仅计算效率更高，而且还能减少过拟合训练数据的可能性。
- en: Improving model performance with meta-learning
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过元学习提高模型性能
- en: As an alternative to increasing the performance of a single model, it is possible
    to combine several models to form a powerful team. Just as the best sports teams
    have players with complementary rather than overlapping skillsets, some of the
    best machine learning algorithms utilize teams of complementary models. Since
    a model brings a unique bias to a learning task, it may readily learn one subset
    of examples, but have trouble with another. Therefore, by intelligently using
    the talents of several diverse team members, it is possible to create a strong
    team of multiple weak learners.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提高单一模型性能的替代方法，可以将多个模型组合成一个强大的团队。就像最优秀的运动队拥有互补而非重叠技能的球员一样，一些最好的机器学习算法利用互补模型的团队。由于每个模型都会为学习任务带来独特的偏差，它可能很容易学习某个子集的样本，但对另一个子集则可能表现不佳。因此，通过智能地利用多个不同团队成员的优势，可以创建一个由多个弱学习者组成的强大团队。
- en: This technique of combining and managing the predictions of multiple models
    falls into a wider set of **meta-learning** methods defining techniques that involve
    learning how to learn. This includes anything from simple algorithms that gradually
    improve performance by iterating over design decisions—for instance, the automated
    parameter tuning used earlier in this chapter—to highly complex algorithms that
    use concepts borrowed from evolutionary biology and genetics for self-modifying
    and adapting to learning tasks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 结合和管理多个模型预测的技术属于**元学习**方法的一部分，定义了涉及学习如何学习的技术。这包括从简单的算法（通过反复设计决策逐步改进性能——例如，本章早些时候提到的自动参数调优）到使用借鉴自进化生物学和遗传学的概念进行自我修改和适应学习任务的高度复杂算法。
- en: For the remainder of this chapter, we'll focus on meta-learning only as it pertains
    to modeling a relationship between the predictions of several models and the desired
    outcome. The teamwork-based techniques covered here are quite powerful, and are
    used quite often to build more effective classifiers.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章剩余部分，我们将专注于**元学习**，仅限于建模多个模型预测与期望结果之间关系的内容。本节中介绍的基于团队合作的技术非常强大，并且在构建更有效的分类器时被广泛使用。
- en: Understanding ensembles
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解集成模型
- en: Suppose you were a contestant on a television trivia show that allowed you to
    choose a panel of five friends to assist you with answering the final question
    for the million-dollar prize. Most people would try to stack the panel with a
    diverse set of subject matter experts. A panel containing professors of literature,
    science, history, and art, along with a current pop-culture expert would be a
    safely well-rounded group. Given their breadth of knowledge, it would be unlikely
    to find a question that stumps the group.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是电视答题节目的一名参赛者，可以选择五个朋友组成团队来帮助你回答最终的百万美元大奖问题。大多数人会试图选取一组多样化的学科专家。一个包含文学、科学、历史、艺术教授以及当代流行文化专家的团队，将是一个均衡的团队。考虑到他们的知识广度，几乎不可能有一个问题能让这个团队感到难倒。
- en: 'The meta-learning approach that utilizes a similar principle of creating a
    varied team of experts is known as an **ensemble**. All the ensemble methods are
    based on the idea that by combining multiple weaker learners, a stronger learner
    is created. The various ensemble methods can be distinguished, in large part,
    by the answers to these two questions:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 利用类似于创建一个多样化专家团队的原理的元学习方法被称为**集成方法**。所有的集成方法都基于这样一个理念：通过将多个较弱的学习器组合起来，创造出一个更强的学习器。各种集成方法的区别，主要可以通过以下两个问题的答案来区分：
- en: How are the weak learning models chosen and/or constructed?
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何选择和/或构建弱学习模型？
- en: How are the weak learners' predictions combined to make a single final prediction?
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将弱学习器的预测结果组合成一个最终的预测？
- en: 'When answering these questions, it can be helpful to imagine the ensemble in
    terms of the following process diagram; nearly all ensemble approaches follow
    this pattern:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在回答这些问题时，想象集成方法的过程图可能会很有帮助；几乎所有的集成方法都遵循这个模式：
- en: '![Understanding ensembles](img/B03905_11_03.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成方法](img/B03905_11_03.jpg)'
- en: First, input training data is used to build a number of models. The **allocation
    function** dictates how much of the training data each model receives. Do they
    each receive the full training dataset or merely a sample? Do they each receive
    every feature or a subset?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用输入的训练数据来构建多个模型。**分配函数**决定了每个模型接收多少训练数据。它们是否每个都接收到完整的训练数据集，还是仅仅接收到一个样本？它们是否每个都接收到所有特征，还是仅接收到一部分特征？
- en: Although the ideal ensemble includes a diverse set of models, the allocation
    function can increase diversity by artificially varying the input data to bias
    the resulting learners, even if they are the same type. For instance, it might
    use bootstrap sampling to construct unique training datasets or pass on a different
    subset of features or examples to each model. On the other hand, if the ensemble
    already includes a diverse set of algorithms—such as a neural network, a decision
    tree, and a k-NN classifier—the allocation function might pass the data on to
    each algorithm relatively unchanged.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然理想的集成方法包括多样化的模型集，但分配函数可以通过人为改变输入数据来增加多样性，从而使得生成的学习器产生偏差，即使它们是同一类型的。例如，它可能使用自助抽样（bootstrap
    sampling）来构建独特的训练数据集，或者将不同的特征或样本子集传递给每个模型。另一方面，如果集成方法已经包含了多种算法——如神经网络、决策树和k-NN分类器——那么分配函数可能会将数据传递给每个算法，而数据保持相对不变。
- en: After the models are constructed, they can be used to generate a set of predictions,
    which must be managed in some way. The **combination function** governs how disagreements
    among the predictions are reconciled. For example, the ensemble might use a majority
    vote to determine the final prediction, or it could use a more complex strategy
    such as weighting each model's votes based on its prior performance.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型构建完成后，它们可以用于生成一组预测结果，这些预测结果必须以某种方式进行管理。**组合函数**决定了如何解决预测之间的分歧。例如，集成方法可能会使用多数投票来确定最终的预测结果，或者使用更复杂的策略，比如根据每个模型的历史表现来加权每个模型的投票。
- en: Some ensembles even utilize another model to learn a combination function from
    various combinations of predictions. For example, suppose that when *M1* and *M2*
    both vote yes, the actual class value is usually no. In this case, the ensemble
    could learn to ignore the vote of *M1* and *M2* when they agree. This process
    of using the predictions of several models to train a final arbiter model is known
    as **stacking**.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一些集成方法甚至使用另一个模型来学习从各种预测组合中得到一个组合函数。例如，假设当*M1*和*M2*都投票“是”时，实际的类别值通常是“否”。在这种情况下，集成方法可以学习忽略*M1*和*M2*的投票，当它们一致时。这种使用多个模型的预测结果来训练最终裁定模型的过程被称为**堆叠**。
- en: '![Understanding ensembles](img/B03905_11_04.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![理解集成方法](img/B03905_11_04.jpg)'
- en: 'One of the benefits of using ensembles is that they may allow you to spend
    less time in pursuit of a single best model. Instead, you can train a number of
    reasonably strong candidates and combine them. Yet, convenience isn''t the only
    reason why ensemble-based methods continue to rack up wins in machine learning
    competitions; ensembles also offer a number of performance advantages over single
    models:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用集成方法的一个好处是，它们可能让你在追求单一最佳模型时花费更少的时间。你可以训练多个合理强大的候选模型并将其结合起来。然而，方便性并不是集成方法在机器学习竞赛中持续获胜的唯一原因；集成方法在多个方面也提供了相对于单一模型的性能优势：
- en: '**Better generalizability to future problems**: As the opinions of several
    learners are incorporated into a single final prediction, no single bias is able
    to dominate. This reduces the chance of overfitting to a learning task.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更好的泛化能力**：由于多个学习者的意见被整合到最终的预测中，因此没有任何单一的偏差能够主导预测结果。这减少了过拟合学习任务的风险。'
- en: '**Improved performance on massive or miniscule datasets**: Many models run
    into memory or complexity limits when an extremely large set of features or examples
    are used, making it more efficient to train several small models than a single
    full model. Conversely, ensembles also do well on the smallest datasets because
    resampling methods such as bootstrapping are inherently a part of many ensemble
    designs. Perhaps most importantly, it is often possible to train an ensemble in
    parallel using distributed computing methods.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在大规模或极小数据集上提升性能**：许多模型在使用非常大规模的特征或样本集时会遇到内存或复杂度的限制，这时训练多个小模型比训练一个完整的模型更高效。相反，集成方法在最小的数据集上也表现良好，因为许多集成设计本身就包含了如自助抽样（bootstrapping）等重采样方法。或许最重要的是，集成方法通常可以通过分布式计算方法并行训练。'
- en: '**The ability to synthesize data from distinct domains**: Since there is no
    one-size-fits-all learning algorithm, the ensemble''s ability to incorporate evidence
    from multiple types of learners is increasingly important as complex phenomena
    rely on data drawn from diverse domains.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合成来自不同领域的数据的能力**：由于没有一种适用于所有情况的学习算法，集成方法能够结合来自多种学习者的证据，这在复杂现象依赖于来自不同领域的数据时变得越来越重要。'
- en: '**A more nuanced understanding of difficult learning tasks**: Real-world phenomena
    are often extremely complex with many interacting intricacies. Models that divide
    the task into smaller portions are likely to more accurately capture subtle patterns
    that a single global model might miss.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对困难学习任务的更细致理解**：现实世界中的现象往往非常复杂，包含许多相互作用的细节。将任务划分为更小部分的模型，往往能更准确地捕捉到单一全局模型可能忽略的微妙模式。'
- en: None of these benefits would be very helpful if you weren't able to easily apply
    ensemble methods in R, and there are many packages available to do just that.
    Let's take a look at several of the most popular ensemble methods and how they
    can be used to improve the performance of the credit model we've been working
    on.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你无法轻松地在R中应用集成方法，那么这些好处将大打折扣，幸运的是，已经有许多包可以用来实现这一点。我们来看一下几种最流行的集成方法，以及它们如何帮助提升我们正在研究的信用模型的性能。
- en: Bagging
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自助法（Bagging）
- en: One of the first ensemble methods to gain widespread acceptance used a technique
    called **bootstrap aggregating** or **bagging** for short. As described by Leo
    Breiman in 1994, bagging generates a number of training datasets by bootstrap
    sampling the original training data. These datasets are then used to generate
    a set of models using a single learning algorithm. The models' predictions are
    combined using voting (for classification) or averaging (for numeric prediction).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个获得广泛认可的集成方法使用了一种叫做**自助聚合**（bootstrap aggregating），简称**Bagging**的方法。正如Leo
    Breiman在1994年所描述的那样，自助法通过对原始训练数据进行自助抽样生成多个训练数据集。这些数据集随后被用来生成一组模型，使用同一个学习算法。这些模型的预测结果通过投票（分类）或平均（数值预测）进行结合。
- en: Note
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意事项
- en: For additional information on bagging, refer to Breiman L. *Bagging predictors*.
    Machine Learning. 1996; 24:123-140.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 关于自助法的更多信息，请参阅 Breiman L. *Bagging predictors*. Machine Learning. 1996; 24:123-140.
- en: Although bagging is a relatively simple ensemble, it can perform quite well
    as long as it is used with relatively **unstable** learners, that is, those generating
    models that tend to change substantially when the input data changes only slightly.
    Unstable models are essential in order to ensure the ensemble's diversity in spite
    of only minor variations between the bootstrap training datasets. For this reason,
    bagging is often used with decision trees, which have the tendency to vary dramatically
    given minor changes in the input data.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然袋装是一种相对简单的集成方法，但只要与相对**不稳定**的学习器一起使用，它可以表现得相当好。所谓不稳定学习器，就是那些生成的模型在输入数据发生轻微变化时会发生显著变化的模型。为了确保集成的多样性，即使是从bootstrap训练数据集之间仅有微小的变化，不稳定的模型是至关重要的。正因如此，袋装方法常常与决策树一起使用，因为决策树在输入数据发生微小变化时往往会发生剧烈变化。
- en: The `ipred` package offers a classic implementation of bagged decision trees.
    To train the model, the `bagging()` function works similar to many of the models
    used previously. The `nbagg` parameter is used to control the number of decision
    trees voting in the ensemble (with a default value of `25`). Depending on the
    difficulty of the learning task and the amount of training data, increasing this
    number may improve the model's performance up to a limit. The downside is that
    this comes at the expense of additional computational expense because a large
    number of trees may take some time to train.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`ipred`包提供了经典的袋装决策树实现。为了训练模型，`bagging()`函数的工作方式与之前使用的许多模型类似。`nbagg`参数用于控制在集成中投票的决策树数量（默认值为`25`）。根据学习任务的难度和训练数据的数量，增加这个数量可能会提高模型的性能，但也有一个限制。缺点是，这会带来额外的计算开销，因为训练大量的树可能需要一些时间。'
- en: 'After installing the `ipred` package, we can create the ensemble as follows.
    We''ll stick to the default value of 25 decision trees:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`ipred`包后，我们可以按如下方式创建集成。我们将保持默认的25棵决策树：
- en: '[PRE11]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The resulting model works as expected with the `predict()` function:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的模型按预期工作，可以使用`predict()`函数：
- en: '[PRE12]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Given the preceding results, the model seems to have fit the training data
    extremely well. To see how this translates into future performance, we can use
    the bagged trees with 10-fold CV using the `train()` function in the `caret` package.
    Note that the method name for the `ipred` bagged trees function is `treebag`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前的结果，该模型似乎非常适合训练数据。为了查看这一点如何转化为未来的表现，我们可以使用带有10倍交叉验证的袋装树，并使用`caret`包中的`train()`函数。请注意，`ipred`袋装树函数的方法名是`treebag`：
- en: '[PRE13]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The kappa statistic of 0.33 for this model suggests that the bagged tree model
    performs at least as well as the best C5.0 decision tree we tuned earlier in this
    chapter. This illustrates the power of ensemble methods; a set of simple learners
    working together can outperform very sophisticated models.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的kappa统计量为0.33，表明集成树模型的表现至少与我们在本章早些时候调优的最佳C5.0决策树相当。这说明了集成方法的强大；一组简单的学习器协同工作可以超越非常复杂的模型。
- en: 'To get beyond bags of decision trees, the `caret` package also provides a more
    general `bag()` function. It includes native support for a handful of models,
    though it can be adapted to other types with a bit of additional effort. The `bag()`
    function uses a control object to configure the bagging process. It requires the
    specification of three functions: one for fitting the model, one for making predictions,
    and one for aggregating the votes.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了超越决策树的袋装，`caret`包还提供了一个更通用的`bag()`函数。它原生支持一些模型，尽管通过一些额外的努力，它可以适配到其他类型的模型。`bag()`函数使用控制对象来配置袋装过程。它需要指定三个函数：一个用于拟合模型，一个用于做出预测，一个用于聚合投票。
- en: For example, suppose we wanted to create a bagged support vector machine model,
    using the `ksvm()` function in the `kernlab` package we used in [Chapter 7](ch07.html
    "Chapter 7. Black Box Methods – Neural Networks and Support Vector Machines"),
    *Black Box Methods – Neural Networks and Support Vector Machines*. The `bag()`
    function requires us to provide functionality for training the SVMs, making predictions,
    and counting votes.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想要创建一个袋装支持向量机模型，可以使用我们在[第7章](ch07.html "第7章：黑盒方法——神经网络和支持向量机")中使用的`kernlab`包中的`ksvm()`函数。`bag()`函数要求我们提供训练SVM、做出预测和统计投票的功能。
- en: 'Rather than writing these ourselves, the `caret` package''s built-in `svmBag`
    list object supplies three functions we can use for this purpose:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无需自己编写这些功能，`caret`包内置的`svmBag`列表对象提供了三个我们可以使用的函数：
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'By looking at the `svmBag$fit` function, we see that it simply calls the `ksvm()`
    function from the `kernlab` package and returns the result:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看`svmBag$fit`函数，我们可以看到它只是调用了`kernlab`包中的`ksvm()`函数并返回结果：
- en: '[PRE15]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `pred` and `aggregate` functions for `svmBag` are also similarly straightforward.
    By studying these functions and creating your own in the same format, it is possible
    to use bagging with any machine learning algorithm you would like.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`svmBag`的`pred`和`aggregate`函数也同样简单。通过研究这些函数并以相同的格式创建自己的函数，可以使用袋装法与任何你想要的机器学习算法。'
- en: Tip
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The `caret` package also includes example objects for bags of naive Bayes models
    (`nbBag`), decision trees (`ctreeBag`), and neural networks (`nnetBag`).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`caret`包还包括了朴素贝叶斯模型袋（`nbBag`）、决策树（`ctreeBag`）和神经网络（`nnetBag`）的示例对象。'
- en: 'Applying the three functions in the `svmBag` list, we can create a bagging
    control object:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用`svmBag`列表中的三个函数，我们可以创建一个袋装控制对象：
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'By using this with the `train()` function and the training control object (`ctrl`),
    defined earlier, we can evaluate the bagged SVM model as follows (note that the
    `kernlab` package is required for this to work; you will need to install it if
    you have not done so previously):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将此与`train()`函数和之前定义的训练控制对象（`ctrl`）一起使用，我们可以如下评估袋装SVM模型（请注意，`kernlab`包是必需的，如果尚未安装，需要先安装它）：
- en: '[PRE17]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Given that the kappa statistic is below 0.30, it seems that the bagged SVM model
    performs worse than the bagged decision tree model. It's worth pointing out that
    the standard deviation of the kappa statistic is fairly large compared to the
    bagged decision tree model. This suggests that the performance varies substantially
    among the folds in the cross-validation. Such variation may imply that the performance
    might be improved further by upping the number of models in the ensemble.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 由于kappa统计量低于0.30，似乎袋装SVM模型的表现不如袋装决策树模型。值得指出的是，与袋装决策树模型相比，kappa统计量的标准差相当大。这表明，在交叉验证的各个折叠中，性能变化很大。这种变化可能意味着通过增加集成中的模型数量，性能可能会进一步提高。
- en: Boosting
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升法
- en: Another common ensemble-based method is called **boosting** because it boosts
    the performance of weak learners to attain the performance of stronger learners.
    This method is based largely on the work of Robert Schapire and Yoav Freund, who
    have published extensively on the topic.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的基于集成的方法称为**提升法**，因为它通过提升弱学习器的表现来达到强学习器的表现。这种方法主要基于Robert Schapire和Yoav
    Freund的工作，他们在这个主题上发表了大量的研究。
- en: Note
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For additional information on boosting, refer to Schapire RE, Freund Y. *Boosting:
    Foundations and Algorithms*. Cambridge, MA, The MIT Press; 2012.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '有关提升法的更多信息，请参考Schapire RE, Freund Y. *Boosting: Foundations and Algorithms*。剑桥，马萨诸塞州，麻省理工学院出版社；2012年。'
- en: Similar to bagging, boosting uses ensembles of models trained on resampled data
    and a vote to determine the final prediction. There are two key distinctions.
    First, the resampled datasets in boosting are constructed specifically to generate
    complementary learners. Second, rather than giving each learner an equal vote,
    boosting gives each learner's vote a weight based on its past performance. Models
    that perform better have greater influence over the ensemble's final prediction.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于袋装法，提升法（boosting）使用在重采样数据上训练的模型集和投票来确定最终预测。这里有两个关键的区别。首先，提升法中的重采样数据集是特别构建的，目的是生成互补的学习器。其次，提升法不是给每个学习器平等的投票，而是根据其过去的表现为每个学习器的投票加权。表现较好的模型在集成中的最终预测中有更大的影响。
- en: Boosting will result in performance that is often quite better and certainly
    no worse than the best of the models in the ensemble. Since the models in the
    ensemble are built to be complementary, it is possible to increase ensemble performance
    to an arbitrary threshold simply by adding additional classifiers to the group,
    assuming that each classifier performs better than random chance. Given the obvious
    utility of this finding, boosting is thought to be one of the most significant
    discoveries in machine learning.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 提升法通常会产生比集成中最好的模型更优的表现，且绝不会差于最强的模型。由于集成中的模型是为了互补性而构建的，假设每个分类器的表现优于随机机会，理论上可以通过增加额外的分类器来随意提高集成的性能。考虑到这一发现的明显实用性，提升法被认为是机器学习领域最重要的发现之一。
- en: Tip
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Although boosting can create a model that meets an arbitrarily low error rate,
    this may not always be reasonable in practice. For one, the performance gains
    are incrementally smaller as additional learners are added, making some thresholds
    practically infeasible. Additionally, the pursuit of pure accuracy may result
    in the model being overfitted to the training data and not generalizable to unseen
    data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管提升方法可以创建一个满足任意低误差率的模型，但在实践中这可能并不总是合理的。首先，随着更多学习器的加入，性能提升会越来越小，这使得某些阈值在实际中不可行。此外，追求纯粹的准确度可能会导致模型过拟合训练数据，而无法推广到未见过的数据。
- en: A boosting algorithm called **AdaBoost** or **adaptive boosting** was proposed
    by Freund and Schapire in 1997\. The algorithm is based on the idea of generating
    weak learners that iteratively learn a larger portion of the difficult-to-classify
    examples by paying more attention (that is, giving more weight) to frequently
    misclassified examples.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一种名为**AdaBoost**或**自适应增强**的提升算法由Freund和Schapire于1997年提出。该算法基于生成弱学习器的思想，通过对频繁被误分类的样本给予更多关注（即，赋予更大的权重），迭代地学习更多难以分类的样本。
- en: Beginning from an unweighted dataset, the first classifier attempts to model
    the outcome. Examples that the classifier predicted correctly will be less likely
    to appear in the training dataset for the following classifier, and conversely,
    the difficult-to-classify examples will appear more frequently. As additional
    rounds of weak learners are added, they are trained on data with successively
    more difficult examples. The process continues until the desired overall error
    rate is reached or performance no longer improves. At that point, each classifier's
    vote is weighted according to its accuracy on the training data on which it was
    built.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个未加权的数据集开始，第一个分类器尝试对结果进行建模。分类器正确预测的样本将不太可能出现在下一个分类器的训练数据集中，反之，难以分类的样本将更频繁地出现。随着更多轮弱学习器的加入，它们在越来越难分类的样本上进行训练。该过程持续进行，直到达到预期的整体误差率或性能不再提高为止。此时，每个分类器的投票将根据其在训练数据上的准确性进行加权。
- en: Though boosting principles can be applied to nearly any type of model, the principles
    are most commonly used with decision trees. We already used boosting in this way
    in [Chapter 5](ch05.html "Chapter 5. Divide and Conquer – Classification Using
    Decision Trees and Rules"), *Divide and Conquer – Classification Using Decision
    Trees and Rules*, as a method to improve the performance of a C5.0 decision tree.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然提升原理几乎可以应用于任何类型的模型，但这些原理最常见的应用是与决策树一起使用。我们在[第5章](ch05.html "第5章. 分治法 – 使用决策树和规则进行分类")中，*分治法
    – 使用决策树和规则进行分类*，已经使用提升方法来提高C5.0决策树的性能。
- en: The **AdaBoost.M1** algorithm provides another tree-based implementation of
    AdaBoost for classification. The AdaBoost.M1 algorithm can be found in the `adabag`
    package.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**AdaBoost.M1**算法提供了另一个基于树的AdaBoost实现，用于分类。AdaBoost.M1算法可以在`adabag`包中找到。'
- en: Note
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more information about the `adabag` package, refer to Alfaro E, Gamez M,
    Garcia N. *adabag – an R package for classification with boosting and bagging*.
    Journal of Statistical Software. 2013; 54:1-35.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解更多关于`adabag`包的信息，请参考Alfaro E, Gamez M, Garcia N的文章。*adabag – an R package
    for classification with boosting and bagging*。统计软件杂志。2013；54：1-35。
- en: 'Let''s create an `AdaBoost.M1` classifier for the credit data. The general
    syntax for this algorithm is similar to other modeling techniques:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为信用数据创建一个`AdaBoost.M1`分类器。该算法的一般语法与其他建模技术类似：
- en: '[PRE18]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As usual, the `predict()` function is applied to the resulting object to make
    predictions:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，`predict()`函数应用于结果对象以进行预测：
- en: '[PRE19]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Departing from convention, rather than returning a vector of predictions, this
    returns an object with information about the model. The predictions are stored
    in a sub-object called `class`:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统方法不同，该方法并不是返回预测结果的向量，而是返回一个包含模型信息的对象。预测结果存储在名为`class`的子对象中：
- en: '[PRE20]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'A confusion matrix can be found in the `confusion` sub-object:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵可以在`confusion`子对象中找到：
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Did you notice that the AdaBoost model made no mistakes? Before you get your
    hopes up, remember that the preceding confusion matrix is based on the model's
    performance on the training data. Since boosting allows the error rate to be reduced
    to an arbitrarily low level, the learner simply continued until it made no more
    errors. This likely resulted in overfitting on the training dataset.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到AdaBoost模型没有犯错吗？在你兴奋之前，记住之前的混淆矩阵是基于模型在训练数据上的表现。由于提升方法允许将错误率降低到任意低的水平，学习器会继续训练直到不再犯错。这很可能导致了在训练数据集上的过拟合。
- en: 'For a more accurate assessment of performance on unseen data, we need to use
    another evaluation method. The `adabag` package provides a simple function to
    use 10-fold CV:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更准确地评估在未见数据上的表现，我们需要使用另一种评估方法。`adabag`包提供了一个简单的函数来使用10折交叉验证（10-fold CV）：
- en: '[PRE22]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Depending on your computer''s capabilities, this may take some time to run,
    during which it will log each iteration to screen. After it completes, we can
    view a more reasonable confusion matrix:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你计算机的性能，这可能需要一些时间，期间它会在屏幕上记录每次迭代。完成后，我们可以查看一个更合理的混淆矩阵：
- en: '[PRE23]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We can find the kappa statistic using the `vcd` package as described in [Chapter
    10](ch10.html "Chapter 10. Evaluating Model Performance"), *Evaluating Model Performance*.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`vcd`包找到kappa统计量，具体方法参见[第10章](ch10.html "第10章：评估模型性能")，*评估模型性能*。
- en: '[PRE24]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: With a kappa of about 0.36, this is our best-performing credit scoring model
    yet. Let's see how it compares to one last ensemble method.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的kappa值约为0.36，这是我们迄今为止表现最好的信用评分模型。让我们看看它与最后一种集成方法的比较。
- en: Tip
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The AdaBoost.M1 algorithm can be tuned in `caret` by specifying `method = "AdaBoost.M1"`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在`caret`中指定`method = "AdaBoost.M1"`来调整AdaBoost.M1算法。
- en: Random forests
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林
- en: Another ensemble-based method called **random forests** (or **decision** **tree
    forests**) focuses only on ensembles of decision trees. This method was championed
    by Leo Breiman and Adele Cutler, and combines the base principles of bagging with
    random feature selection to add additional diversity to the decision tree models.
    After the ensemble of trees (the forest) is generated, the model uses a vote to
    combine the trees' predictions.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种基于集成的方法，称为**随机森林**（或**决策树森林**），仅专注于决策树的集成。这一方法由Leo Breiman和Adele Cutler提倡，结合了袋装（bagging）和随机特征选择的基本原则，以增加决策树模型的多样性。在生成了决策树集成（森林）之后，模型通过投票来合并这些树的预测结果。
- en: Note
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more detail on how random forests are constructed, refer to Breiman L. *Random
    Forests*. Machine Learning. 2001; 45:5-32.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 关于随机森林构建的更多细节，请参阅Breiman L. *Random Forests*。机器学习，2001；45：5-32。
- en: Random forests combine versatility and power into a single machine learning
    approach. As the ensemble uses only a small, random portion of the full feature
    set, random forests can handle extremely large datasets, where the so-called "curse
    of dimensionality" might cause other models to fail. At the same time, its error
    rates for most learning tasks are on par with nearly any other method.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林将多样性和强大功能融合为单一的机器学习方法。由于集成只使用了完整特征集中的一小部分随机特征，随机森林可以处理非常大的数据集，在这些数据集中，所谓的“维度灾难”可能会导致其他模型失败。与此同时，它在大多数学习任务上的错误率与几乎所有其他方法相当。
- en: Tip
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Although the term "Random Forests" is trademarked by Breiman and Cutler, the
    term is sometimes used colloquially to refer to any type of decision tree ensemble.
    A pedant would use the more general term "decision tree forests" except when referring
    to the specific implementation by Breiman and Cutler.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然“随机森林”（Random Forests）这个术语是由Breiman和Cutler注册的商标，但有时人们也用它来泛指任何类型的决策树集成。一个严格的学者会使用更通用的术语“决策树森林”，除非是在指Breiman和Cutler的具体实现。
- en: 'It''s worth noting that relative to other ensemble-based methods, random forests
    are quite competitive and offer key advantages relative to the competition. For
    instance, random forests tend to be easier to use and less prone to overfitting.
    The following table lists the general strengths and weaknesses of random forest
    models:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，相对于其他基于集成的方法，随机森林非常具有竞争力，并且相较于其他方法具有关键优势。例如，随机森林往往更容易使用，且不容易发生过拟合。下表列出了随机森林模型的一般优缺点：
- en: '| Strengths | Weaknesses |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 优点 | 缺点 |'
- en: '| --- | --- |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: An all-purpose model that performs well on most problems
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种在大多数问题上表现良好的通用模型
- en: Can handle noisy or missing data as well as categorical or continuous features
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能处理噪声或缺失数据以及类别型或连续型特征
- en: Selects only the most important features
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅选择最重要的特征
- en: Can be used on data with an extremely large number of features or examples
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以用于具有极大量特征或样本的数据
- en: '|'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Unlike a decision tree, the model is not easily interpretable
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与决策树不同，该模型不容易解释
- en: May require some work to tune the model to the data
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能需要一些工作来调整模型以适应数据
- en: '|'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Due to their power, versatility, and ease of use, random forests are quickly
    becoming one of the most popular machine learning methods. Later on in this chapter,
    we'll compare a random forest model head-to-head against the boosted C5.0 tree.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机森林具有强大的功能、广泛的适应性和易用性，它们正在迅速成为最受欢迎的机器学习方法之一。在本章后面，我们将把随机森林模型与增强版的C5.0决策树进行正面比较。
- en: Training random forests
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练随机森林
- en: 'Though there are several packages to create random forests in R, the `randomForest`
    package is perhaps the implementation that is most faithful to the specification
    by Breiman and Cutler, and is also supported by `caret` for automated tuning.
    The syntax for training this model is as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在R中有多个包可以创建随机森林，但`randomForest`包可能是最忠实于Breiman和Cutler规范的实现，并且也受到`caret`包的支持，可以进行自动化调参。训练该模型的语法如下：
- en: '![Training random forests](img/B03905_11_05.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![训练随机森林](img/B03905_11_05.jpg)'
- en: By default, the `randomForest()` function creates an ensemble of 500 trees that
    consider `sqrt(p)` random features at each split, where `p` is the number of features
    in the training dataset and `sqrt()` refers to R's square root function. Whether
    or not these default parameters are appropriate depends on the nature of the learning
    task and training data. Generally, more complex learning problems and larger datasets
    (either more features or more examples) work better with a larger number of trees,
    though this needs to be balanced with the computational expense of training more
    trees.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`randomForest()`函数会创建一个包含500棵树的集成模型，每棵树在每次分裂时都会考虑`sqrt(p)`个随机特征，其中`p`是训练数据集中的特征数量，`sqrt()`是R的平方根函数。是否使用这些默认参数取决于学习任务和训练数据的性质。一般来说，更复杂的学习问题和更大的数据集（无论是更多的特征还是更多的样本）都能通过更多的树来取得更好的效果，但这需要与训练更多树的计算开销进行平衡。
- en: The goal of using a large number of trees is to train enough so that each feature
    has a chance to appear in several models. This is the basis of the `sqrt(p)` default
    value for the `mtry` parameter; using this value limits the features sufficiently
    so that substantial random variation occurs from tree-to-tree. For example, since
    the credit data has 16 features, each tree would be limited to splitting on four
    features at any time.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大量决策树的目的是训练足够多的树，以便每个特征都有机会出现在多个模型中。这也是`mtry`参数的默认值`sqrt(p)`的基础；使用该值限制特征的数量，确保树与树之间有足够的随机变化。例如，由于信用数据有16个特征，每棵树在任何时候只能在四个特征上进行分裂。
- en: 'Let''s see how the default `randomForest()` parameters work with the credit
    data. We''ll train the model just as we did with other learners. Again, the `set.seed()`
    function ensures that the result can be replicated:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下默认的`randomForest()`参数如何与信用数据一起使用。我们将像之前训练其他学习器一样训练模型。再次强调，`set.seed()`函数确保结果可以复现：
- en: '[PRE25]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To look at a summary of the model''s performance, we can simply type the resulting
    object''s name:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看模型表现的总结，我们只需输入结果对象的名称：
- en: '[PRE26]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The output notes that the random forest included 500 trees and tried four variables
    at each split, just as we expected. At first glance, you might be alarmed at the
    seemingly poor performance according to the confusion matrix—the error rate of
    23.8 percent is far worse than the resubstitution error of any of the other ensemble
    methods so far. However, this confusion matrix does not show resubstitution error.
    Instead, it reflects the **out-of-bag error rate** (listed in the output as `OOB
    estimate of error rate`), which unlike resubstitution error, is an unbiased estimate
    of the test set error. This means that it should be a fairly reasonable estimate
    of future performance.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果显示随机森林包括了500棵树，并且在每次分裂时尝试了四个变量，正如我们所期望的那样。乍一看，你可能会对困惑矩阵中看似不理想的表现感到惊讶——23.8%的错误率远高于到目前为止其他集成方法的重置误差。然而，这个困惑矩阵并没有显示重置误差。相反，它反映的是**袋外错误率**（在输出中列为`OOB
    estimate of error rate`），与重置误差不同，袋外错误率是测试集错误的无偏估计。这意味着它应该是对未来表现的相当合理的估计。
- en: The out-of-bag estimate is computed during the construction of the random forest.
    Essentially, any example not selected for a single tree's bootstrap sample can
    be used to test the model's performance on unseen data. At the end of the forest
    construction, the predictions for each example each time it was held out are tallied,
    and a vote is taken to determine the final prediction for the example. The total
    error rate of such predictions becomes the out-of-bag error rate.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 袋外估计在构建随机森林时计算。实际上，任何没有被选为单棵树的自助法样本的例子，都可以用来测试模型在未见过数据上的表现。在森林构建结束时，每个例子在每次被保留时的预测都会被统计，并通过投票来确定该例子的最终预测结果。这样的预测的总误差率即为袋外误差率。
- en: Evaluating random forest performance
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估随机森林性能
- en: As mentioned previously, the `randomForest()` function is supported by `caret`,
    which allows us to optimize the model while, at the same time, calculating performance
    measures beyond the out-of-bag error rate. To make things interesting, let's compare
    an auto-tuned random forest to the best auto-tuned boosted C5.0 model we've developed.
    We'll treat this experiment as if we were hoping to identify a candidate model
    for submission to a machine learning competition.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`randomForest()`函数得到了`caret`的支持，这使我们能够优化模型，同时计算袋外误差率以外的性能度量。为了增加趣味性，我们将比较一个自动调优的随机森林与我们开发的最佳自动调优提升C5.0模型。我们将这次实验视为希望找到一个候选模型，提交到机器学习竞赛中。
- en: We must first load `caret` and set our training control options. For the most
    accurate comparison of model performance, we'll use repeated 10-fold cross-validation,
    or 10-fold CV repeated 10 times. This means that the models will take a much longer
    time to build and will be more computationally intensive to evaluate, but since
    this is our final comparison we should be *very* sure that we're making the right
    choice; the winner of this showdown will be our only entry into the machine learning
    competition.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要加载`caret`并设置训练控制选项。为了对模型性能进行最准确的比较，我们将使用重复的10折交叉验证，或者说是重复10次的10折交叉验证。这意味着模型的构建时间将大大增加，并且评估计算量会更大，但由于这是我们的最终比较，我们必须*非常*确保我们做出了正确的选择；这场对决的胜者将是我们唯一进入机器学习竞赛的模型。
- en: '[PRE27]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we''ll set up the tuning grid for the random forest. The only tuning
    parameter for this model is `mtry`, which defines how many features are randomly
    selected at each split. By default, we know that the random forest will use `sqrt(16)`,
    or four features per tree. To be thorough, we''ll also test values half of that,
    twice that, as well as the full set of 16 features. Thus, we need to create a
    grid with values of `2`, `4`, `8`, and `16` as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将为随机森林设置调优网格。该模型的唯一调优参数是`mtry`，它定义了每次分裂时随机选择多少特征。默认情况下，我们知道随机森林将使用`sqrt(16)`，即每棵树使用四个特征。为了全面起见，我们还将测试该值的一半、两倍以及完整的16个特征。因此，我们需要创建一个包含`2`、`4`、`8`和`16`的网格，如下所示：
- en: '[PRE28]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Tip
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: A random forest that considers the full set of features at each split is essentially
    the same as a bagged decision tree model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一个在每次分裂时考虑完整特征集的随机森林，本质上与一个集成决策树模型相同。
- en: 'We can supply the resulting grid to the `train()` function with the `ctrl`
    object as follows. We''ll use the kappa metric to select the best model:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将生成的网格传递给`train()`函数，并使用`ctrl`对象，如下所示。我们将使用kappa指标来选择最佳模型：
- en: '[PRE29]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The preceding command may take some time to complete as it has quite a bit
    of work to do! When it finishes, we''ll compare that to a boosted tree using `10`,
    `20`, `30`, and `40` iterations:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令可能需要一些时间才能完成，因为它有很多工作要做！完成后，我们将与使用`10`、`20`、`30`和`40`次迭代的提升树进行比较：
- en: '[PRE30]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'When the C5.0 decision tree finally completes, we can compare the two approaches
    side-by-side. For the random forest model, the results are:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 当C5.0决策树最终完成时，我们可以将这两种方法进行并排比较。对于随机森林模型，结果是：
- en: '[PRE31]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'For the boosted C5.0 model, the results are:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 对于提升C5.0模型，结果是：
- en: '[PRE32]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: With a kappa of about 0.361, the random forest model with `mtry = 16` was the
    winner among these eight models. It was higher than the best C5.0 decision tree,
    which had a kappa of about 0.334, and slightly higher than the `AdaBoost.M1` model
    with a kappa of about 0.360\. Based on these results, we would submit the random
    forest as our final model. Without actually evaluating the model on the competition
    data, we have no way of knowing for sure whether it will end up winning, but given
    our performance estimates, it's the safer bet. With a bit of luck, perhaps we'll
    come away with the prize.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在`mtry = 16`时，随机森林模型的kappa约为0.361，是这八个模型中表现最好的。它高于最佳的C5.0决策树，其kappa约为0.334，并且略高于kappa约为0.360的`AdaBoost.M1`模型。根据这些结果，我们将提交随机森林模型作为最终模型。在没有实际在竞赛数据上评估模型的情况下，我们无法确定它是否最终获胜，但根据我们的性能评估，它是更安全的选择。幸运的话，也许我们能赢得奖项。
- en: Summary
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: After reading this chapter, you should now know the base techniques that are
    used to win data mining and machine learning competitions. Automated tuning methods
    can assist with squeezing every bit of performance out of a single model. On the
    other hand, performance gains are also possible by creating groups of machine
    learning models that work together.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章后，你现在应该知道了在数据挖掘和机器学习竞赛中获胜所需的基本技术。自动调优方法可以帮助从单一模型中挤出每一分性能。另一方面，通过创建多个协同工作的机器学习模型，也可以实现性能的提升。
- en: Although this chapter was designed to help you prepare competition-ready models,
    note that your fellow competitors have access to the same techniques. You won't
    be able to get away with stagnancy; therefore, continue to add proprietary methods
    to your bag of tricks. Perhaps you can bring unique subject-matter expertise to
    the table, or perhaps your strengths include an eye for detail in data preparation.
    In any case, practice makes perfect, so take advantage of open competitions to
    test, evaluate, and improve your own machine learning skillset.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章旨在帮助你准备竞赛级别的模型，但请注意，你的竞争对手也可以使用相同的技术。你不能停滞不前；因此，继续将独特的方法加入你的工具箱。也许你能带来独特的专业知识，或者也许你的优势在于数据准备时对细节的关注。无论如何，实践出真知，因此利用开放竞赛来测试、评估和提升你自己的机器学习技能。
- en: In the next chapter—the last in this book—we'll take a bird's eye look at ways
    to apply machine learning to some highly specialized and difficult domains using
    R. You'll gain the knowledge needed to apply machine learning to tasks at the
    cutting edge of the field.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章——本书的最后一章——我们将从更高的角度审视如何利用R语言将机器学习应用于一些高度专业化且复杂的领域。你将获得将机器学习应用于前沿任务所需的知识。
