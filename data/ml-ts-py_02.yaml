- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Preprocessing Time-Series
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列的预处理
- en: Preprocessing is a crucial step in machine learning that is nonetheless often
    neglected. Many books don't cover preprocessing in any depth or skip preprocessing
    entirely. When presenting to outsiders about a machine learning project, curiosity
    is naturally attracted to the algorithm rather than the dataset or the preprocessing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理是机器学习中的关键步骤，但却常常被忽视。许多书籍对预处理没有深入讨论，甚至完全跳过了预处理。在向外部人员展示机器学习项目时，大家的好奇心自然会集中在算法上，而不是数据集或预处理上。
- en: One reason for the relative silence on preprocessing could be that it's less
    glamorous than machine learning itself. It is, however, often the step that takes
    the most time, sometimes estimated at around 98% of the whole machine learning
    process. And it is often in preprocessing that relatively easy work can have a
    great impact on the eventual performance of the machine learning model. The quality
    of the data goes a long way toward determining the outcome – low-quality input,
    in the worst case, can invalidate the machine learning work altogether (this is
    summarized in the adage "garbage in, garbage out").
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理相对较少被关注的原因之一可能是它比机器学习本身不那么吸引人。然而，它通常是最耗时的步骤，有时被估计占整个机器学习过程的 98%。而且，预处理往往是那种相对简单的工作对最终机器学习模型性能产生巨大影响的步骤。数据的质量在很大程度上决定了结果——低质量的输入在最糟糕的情况下甚至可能完全使机器学习工作无效（这也就是俗话所说的“垃圾进，垃圾出”）。
- en: Preprocessing includes curating and screening the data, something that overlaps
    with the analysis process covered in the previous chapter, *Chapter 2*, *Time-Series
    Analysis with Python*. The expected output of the preprocessing is a dataset on
    which it is easier to conduct machine learning. This can mean that it is more
    reliable and less noisy than the original dataset.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理包括对数据的整理和筛选，这与上一章中提到的分析过程有所重叠，*第 2 章*，*使用 Python 进行时间序列分析*。预处理的预期输出是一个更适合进行机器学习的数据集。这意味着它可能比原始数据集更可靠、噪声更少。
- en: You can find the code for this chapter as a Jupyter notebook in the book's GitHub
    repository.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的 GitHub 仓库中找到本章的代码，形式为 Jupyter notebook。
- en: 'We''re going to cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: What Is Preprocessing?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是预处理？
- en: Feature Transforms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征变换
- en: Feature Engineering
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程
- en: Python Practice
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 实践
- en: We'll start off by discussing the basics of preprocessing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从讨论预处理的基础知识开始。
- en: What Is Preprocessing?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是预处理？
- en: Anyone who's ever worked in a company on a machine learning project knows that
    real-world data is messy. It's often aggregated from multiple sources or using
    multiple platforms or recording devices, and it's incomplete and inconsistent.
    In preprocessing, we want to improve the data quality to successfully apply a
    machine learning model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 任何曾在机器学习项目中工作过的人都知道，现实世界中的数据是杂乱无章的。数据通常是从多个来源或多个平台或记录设备中汇总的，而且它是不完整且不一致的。在预处理阶段，我们希望提高数据质量，以便能够成功应用机器学习模型。
- en: 'Data preprocessing includes the following set of techniques:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理包括以下一组技术：
- en: Feature transforms
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征变换
- en: Scaling
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放
- en: Power/log transforms
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 幂/对数变换
- en: Imputation
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失值填补
- en: Feature engineering
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程
- en: 'These techniques fall largely into two classes: either they tailor to the assumptions
    of the machine learning algorithm (feature transforms) or they are concerned with
    constructing more complex features from multiple underlying features (feature
    engineering). We''ll only deal with univariate feature transforms, transforms
    that apply to one feature at a time. We won''t discuss multivariate feature transforms
    (data reduction) such as variable selection or dimensionality reduction since
    they are not particular to time-series datasets.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术大致分为两类：一种是针对机器学习算法的假设进行调整（特征变换），另一种则是关注从多个基础特征中构造更复杂的特征（特征工程）。我们将仅讨论单变量特征变换，也就是一次只对一个特征进行变换。我们不会讨论多变量特征变换（数据降维），例如变量选择或降维，因为它们不特定于时间序列数据集。
- en: Missing values are a common problem in machine learning, so we will discuss
    replacing missing values (imputation) in this chapter as well.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值是机器学习中的常见问题，因此我们将在本章讨论缺失值的填补（插补）。
- en: We'll be talking about features as the elementary units of preprocessing. We
    want to create input features for our machine learning process that make the model
    easier to train, easier to evaluate, or to improve the quality of model predictions.
    Our goal is to have features that are predictive of the target, and decorrelated
    (not redundant between themselves). Decorrelation is a requirement for linear
    models, but less important for more modern, for example tree-based, algorithms.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论特征作为预处理的基本单元。我们希望为机器学习过程创建输入特征，这些特征能够使模型更容易训练、更容易评估，或提高模型预测的质量。我们的目标是拥有能够预测目标且不相关（相互之间不冗余）的特征。去相关性是线性模型的要求，但对更现代的算法（例如基于树的算法）来说不那么重要。
- en: Although we'll mostly deal with feature engineering, we'll also mention target
    transformations. We could refer to target transformations more specifically as
    target engineering; however, since methods applied on targets are the same methods
    as those applied to features, I've included them under the same headings about
    feature engineering or feature transformations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们主要处理特征工程，但我们也会提到目标转换。我们可以更具体地称目标转换为目标工程；然而，由于应用于目标的方法与应用于特征的方法相同，所以我将它们包括在特征工程或特征转换的同一章节下。
- en: Please note that we define the main goal of our preprocessing as increasing
    the predictiveness of our features, or, in other words, we want to elevate the
    quality of our machine learning model predictions. We could have alternatively
    defined data quality in terms of accuracy, completeness, and consistency, which
    would have cast a much wider net including data aggregation and cleaning techniques,
    and methods of data quality assessment.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将预处理的主要目标定义为提高特征的预测能力，换句话说，我们希望提高机器学习模型预测的质量。我们本可以将数据质量定义为准确性、完整性和一致性，这样定义会包括数据聚合和清洗技术，以及数据质量评估方法，范围更广。
- en: In this chapter, we are pragmatically reducing the scope of the treatment here
    to usefulness in machine learning. If our model is not fit for purpose, we might
    want to repeat or improve data collection, do more feature engineering, or build
    a better model. This again emphasizes the point that data analysis, preprocessing,
    and machine learning are an iterative process.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们务实地将讨论范围缩小到在机器学习中的实用性。如果我们的模型不符合预期，我们可能需要重新收集数据、进行更多特征工程，或构建更好的模型。这再次强调了数据分析、预处理和机器学习是一个迭代过程。
- en: Binning or discretization can be a part of preprocessing as well, but can also
    be used for grouping data points by their similarity. We'll be discussing discretization
    together with other clustering techniques in *Chapter 6*, *Unsupervised Methods
    for Time-Series*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**分箱**或**离散化**也可以是预处理的一部分，但也可以用于根据数据点的相似性对其进行分组。我们将在*第六章*中讨论离散化与其他聚类技术，*无监督时间序列方法*。'
- en: Before we continue, let's go through some of the basics of preprocessing time-series
    datasets with Python. This will cover the theory behind operations with time-series
    data as an introduction.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们先了解一些使用Python预处理时间序列数据集的基础知识。这将作为时间序列数据操作的理论介绍。
- en: Feature Transforms
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征变换
- en: Many models or training processes depend on the assumption that the data is
    distributed according to the normal distribution. Even the most widely used descriptors,
    the arithmetic mean and standard deviation, are largely useless if your dataset
    has a skew or several peaks (multi-modal). Unfortunately, observed data often
    doesn't fall within the normal distribution, so that traditional algorithms can
    yield invalid results.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 许多模型或训练过程依赖于数据符合正态分布的假设。即使是最常用的描述符——算术平均值和标准差——如果数据集存在偏态或多个峰值（多模态），也几乎没有用处。不幸的是，观察到的数据通常不符合正态分布，这样传统算法可能会产生无效结果。
- en: When data is non-normal, transformations of data are applied to make the data
    as normal-like as possible and, thus, increase the validity of the associated
    statistical analyses.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据不符合正态分布时，会对数据进行变换，使其尽可能接近正态分布，从而提高相关统计分析的有效性。
- en: Often it can be easier to eschew traditional machine learning algorithms of
    dealing with time-series data and, instead, use newer, so-called non-linear methods
    that are not dependent on the distribution of the data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，处理时间序列数据时，避免使用传统的机器学习算法，而转而使用更新的、所谓的非线性方法，这些方法不依赖于数据的分布。
- en: As a final remark, while all the following transformations and scaling methods
    can be applied to features directly, an interesting spin with time-series datasets
    is that they change over time, and we might not have full knowledge of the time-series.
    Many of these transformations have online variants, where all statistics are estimated
    and adjusted on the fly. You can take a look at *Chapter 8*, *Online Learning
    for Time-Series*, for more details on this topic.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后需要提到的是，尽管所有以下的变换和缩放方法可以直接应用于特征，但在时间序列数据集中的一个有趣之处是，时间序列随着时间的推移而变化，并且我们可能无法完全了解时间序列。许多变换方法都有在线变种，可以动态估算并调整所有统计数据。你可以查看*第8章*，*时间序列的在线学习*，以获取有关此主题的更多细节。
- en: In the next section, we'll look at scaling, which is a general issue in regression.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论缩放问题，这在回归分析中是一个普遍问题。
- en: Scaling
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩放
- en: Some features have natural bounds such as the age of a person or the year of
    production of a product. If these ranges differ between features, some model types
    (again, mostly linear models) struggle with this, preferring similar ranges, and
    similar means.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有些特征有自然的界限，比如一个人的年龄或产品的生产年份。如果这些范围在不同特征之间有所不同，一些模型类型（尤其是线性模型）会遇到困难，更倾向于类似的范围和均值。
- en: Two very common scaling methods are min-max scaling and z-score normalization.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 两种非常常见的缩放方法是最小-最大缩放和Z-score标准化。
- en: '**Min-max scaling** involves restricting the range of the feature within two
    constants, *a* and *b*. This is defined as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**最小-最大缩放**涉及将特征的范围限制在两个常数 *a* 和 *b* 之间。其定义如下：'
- en: '![](img/B17577_03_001.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_03_001.png)'
- en: In the special case that *a* is 0 and *b* is 1, this restricts the range of
    the feature within 0 and 1.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在特殊情况下，当 *a* 为0且 *b* 为1时，这会将特征的范围限制在0和1之间。
- en: '**Z-score normalization** is setting the mean of the feature to 0 and the variance
    to 1 (unit variance) like this:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**Z-score标准化**是将特征的均值设置为0，方差设置为1（单位方差），如下所示：'
- en: '![](img/B17577_03_002.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_03_002.png)'
- en: In the case that *x* comes from a Gaussian distribution, ![](img/B17577_03_003.png)
    is a standard normal distribution.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *x* 来自高斯分布， ![](img/B17577_03_003.png) 则是标准正态分布。
- en: In the next section, we'll look at log and power transformations. These transformations
    are quite important, especially for the traditional time-series models that we'll
    come across in *Chapter 5*, *Time-Series Forecasting with Moving Averages and
    Autoregressive Models*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将研究对数和幂变换。这些变换非常重要，特别是对于传统的时间序列模型，我们将在*第5章*，*使用移动平均和自回归模型进行时间序列预测*中遇到这些模型。
- en: Log and Power Transformations
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对数和幂变换
- en: Both log and power transformations can compress values that spread over large
    magnitudes into a narrow range of output values. A **log transformation** is a
    feature transformation in which each value *x* gets replaced by *log(x)*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对数和幂变换都可以将覆盖大范围的值压缩到一个较窄的输出范围内。**对数变换**是一种特征变换，其中每个值 *x* 被替换为 *log(x)*。
- en: The log function is the inverse of the exponential function, and it's important
    to remember that the range between 0 and 1 gets mapped to negative numbers (![](img/B17577_03_004.png)
    while numbers *x>=1* get compressed in the positive range. The choice of the logarithm
    is usually between the natural and base 10 but can be anything that can help so
    that your feature becomes closer to the symmetric bell-shaped distribution, which
    is the normal distribution.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对数函数是指数函数的反函数，重要的是要记住，范围在0和1之间的值会被映射为负数（![](img/B17577_03_004.png)），而 *x>=1*
    的值会被压缩到正数范围内。对数的选择通常是在自然对数和以10为底的对数之间，但也可以选择任何能帮助你的特征更接近对称的钟形分布（即正态分布）的对数。
- en: Log transformation is, arguably, one of the most popular among the different
    types of transformations applied to take the distribution of the data closer to
    a Gaussian distribution. Log transformation can be used to reduce the skew of
    a distribution. In the best scenario, if the feature follows a log-normal distribution,
    then the log-transformed data follows a normal distribution. Unfortunately, your
    feature might not be distributed according to a log-normal distribution, so applying
    this transformation doesn't help.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对数变换可以说是所有变换方法中最受欢迎的之一，特别是在将数据分布逼近高斯分布时。对数变换可以用来减少分布的偏斜度。在最佳情况下，如果特征遵循对数正态分布，那么对数变换后的数据将遵循正态分布。不幸的是，你的特征可能并非按照对数正态分布分布，因此应用这种变换并没有效果。
- en: Generally, I'd recommend exercising caution with data transformations. You should
    always inspect your data before and after the transformation. You want the variance
    of your feature to capture that of the target, so you should make sure you are
    not losing resolution. Further, you might want to check your data conforms – as
    should be the goal – more closely to the normal distribution. Many statistical
    methods have been developed to test the normality assumption of observed data,
    but even a simple histogram can give a great idea of the distribution.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我建议在进行数据变换时要小心。在变换前后，你应始终检查数据。你希望特征的方差能够反映目标变量的方差，因此需要确保不会丢失分辨率。此外，你可能还需要检查数据是否符合正态分布——这是目标之一。许多统计方法已经开发出来用以检验观测数据的正态性假设，但即使是简单的直方图也能给出分布的大致情况。
- en: Power transformations are often applied to transform the data from its original
    distribution to something that's more like a normal distribution. As discussed
    in the introduction, this can make a huge difference to the machine learning algorithm's
    ability to find a solution.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 幂变换通常用于将数据从原始分布转化为更接近正态分布的形式。如在介绍中所讨论的，这对机器学习算法找到解决方案的能力有巨大影响。
- en: '**Power transforms** are transformations preserving the original order (this
    property is called monotonicity) using power functions. A **power function** is
    a function of this form:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**幂变换**是保持原始顺序（该特性称为单调性）的变换，使用幂函数。**幂函数**是此形式的函数：'
- en: '![](img/B17577_03_005.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_03_005.png)'
- en: where ![](img/B17577_03_006.png).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B17577_03_006.png)。
- en: When *n* is an integer and bigger than 1, we can make two major distinctions
    depending on whether *n* is odd or even. If it is even, the function ![](img/B17577_03_007.png)
    will tend toward positive infinity with large *x*, either positive or negative.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *n* 是一个大于 1 的整数时，根据 *n* 是奇数还是偶数，我们可以做出两种主要的区分。如果是偶数，函数 ![](img/B17577_03_007.png)
    将随着大 *x*（无论是正数还是负数）的增加而趋向正无穷。
- en: If it is odd, *f(x)* will tend toward positive infinity with increasing *x*,
    but toward negative infinity with *x*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是奇数，*f(x)* 随着 *x* 的增加将趋向正无穷，但随着 *x* 的减少将趋向负无穷。
- en: 'A power transformation is generally defined like this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 幂变换通常定义如下：
- en: '![](img/B17577_03_008.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_03_008.png)'
- en: 'where *GM(x)* is the geometric mean of *x*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *GM(x)* 是 *x* 的几何平均值：
- en: '![](img/B17577_03_009.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_03_009.png)'
- en: 'This reduces the transformation to the optimal choice of the parameter ![](img/B17577_03_010.png).
    For practical purposes, two power transformations are most commonly used:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这将变换简化为参数 ![](img/B17577_03_010.png) 的最优选择。实际应用中，最常用的两种幂变换是：
- en: Box-Cox transformation
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Box-Cox 变换
- en: Yeo–Johnson
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yeo–Johnson
- en: 'For **Box-Cox transformation**, there are two variants, the one-parameter variant
    and the two-parameter variant. One-parameter Box–Cox transformations are defined like
    this:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 **Box-Cox 变换**，有两种变体：单参数变体和双参数变体。单参数 Box-Cox 变换定义如下：
- en: '![](img/B17577_03_011.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_03_011.png)'
- en: The value of the parameter ![](img/B17577_03_012.png) can be via different optimization
    methods such as the maximum likelihood that the transformed feature is Gaussian.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 ![](img/B17577_03_012.png) 的值可以通过不同的优化方法来确定，例如最大似然估计，即变换后的特征是高斯分布。
- en: So the value of lambda corresponds to the exponent of the power operation, for
    example, ![](img/B17577_03_013.png) with ![](img/B17577_03_014.png) or ![](img/B17577_03_015.png)with
    ![](img/B17577_03_016.png).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，lambda 的值对应于幂运算的指数，例如，![](img/B17577_03_013.png) 与 ![](img/B17577_03_014.png)
    或 ![](img/B17577_03_015.png) 与 ![](img/B17577_03_016.png)。
- en: 'Fun fact: Box-Cox transformation is named after statisticians George E.P Box
    and David Cox, who decided they had to work together because Box-Cox would sound
    good.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 趣事：Box-Cox 变换以统计学家 George E.P Box 和 David Cox 的名字命名，他们决定合作，因为 Box-Cox 听起来很不错。
- en: '**Yeo–Johnson transformation** is an extension of Box-Cox transformation that
    allows zero and negative values of *x*. ![](img/B17577_03_017.png) can be any
    real number, where ![](img/B17577_03_018.png) =1 produces the identity transformation.
    The transformation is defined as:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**Yeo–Johnson 变换**是 Box-Cox 变换的扩展，允许 *x* 的值为零或负数。![](img/B17577_03_017.png)
    可以是任何实数，其中 ![](img/B17577_03_018.png) = 1 会产生恒等变换。变换定义为：'
- en: '![](img/B17577_03_019.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_03_019.png)'
- en: Finally, **quantile transformation** can map a feature to the uniform distribution
    based on an estimate of the cumulative distribution function. Optionally, this
    can then be mapped in a second step to normal distribution. The advantage of this
    transform, similar to other transformations that we've talked about, is that it
    makes features more convenient to process and plot, and easier to compare, even
    if they were measured at different scales.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，**分位数转换**可以根据累计分布函数的估计将特征映射到均匀分布。可选地，这可以在第二步中映射到正态分布。这种转换的优势，类似于我们之前谈到的其他转换，是使特征更方便处理和绘图，并且即使它们是在不同的尺度上测量的，也更容易进行比较。
- en: In the next section, we'll look at imputation, which literally means the assignment
    of values by inference, but in machine learning, often is more narrowly meant
    to refer to replacing missing values.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论填充，这字面意思是通过推理分配值，但在机器学习中，通常更狭义地指代替换缺失值。
- en: Imputation
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填充
- en: 'Imputation is the replacement of missing values. This is important for any
    machine learning algorithm that can''t handle missing values. Generally, we can
    distinguish the following types of imputation techniques:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 填充是对缺失值的替换。这对任何不能处理缺失值的机器学习算法来说都很重要。通常，我们可以区分以下几种填充技术：
- en: Unit imputation – where missing values are replaced by a constant such as the
    mean or 0
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单位填充——将缺失值替换为常数，例如均值或 0
- en: Model-based imputation – where missing values are replaced with predictions
    from a machine learning model
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于模型的填充——将缺失值替换为机器学习模型的预测值
- en: Unit imputation is by far the most popular imputation technique, partly because
    it's very easy to do, and because it's less heavy on computational resources than
    model-based imputation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 单位填充是迄今为止最流行的填充技术，部分原因是它非常简单，而且比基于模型的填充对计算资源的需求较低。
- en: We'll do imputation in the practice section of this chapter. In the next section,
    we'll talk about feature engineering.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的实践部分进行填充。在下一节中，我们将讨论特征工程。
- en: Feature Engineering
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程
- en: Machine learning algorithms can use different representations of the input features.
    As we've mentioned in the introduction, the goal of feature engineering is to
    produce new features that can help us in the machine learning process. Some representations
    or augmentations of features can boost performance.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法可以使用不同的输入特征表示。正如我们在介绍中提到的，特征工程的目标是产生可以帮助我们进行机器学习过程的新特征。一些特征的表示或增强可以提升性能。
- en: We can distinguish between hand-crafted and automated feature extraction, where
    hand-crafted means that we look through the data and try to come up with representations
    that could be useful, or we can use a set of features that have been established
    from the work of researchers and practitioners before. An example of a set of
    established features is **Catch22**, which includes 22 features and simple summary
    statistics extracted from phase-dependant intervals. The Catch22 set is a subset
    of the **Highly Comparative Time-Series Analysis** (**HCTSA**) toolbox, another
    set of features.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以区分手工特征提取和自动化特征提取，其中手工特征提取意味着我们查看数据并尝试提出可能有用的表示，或者我们可以使用以前由研究人员和实践者提出的一组已建立的特征。一个已建立特征集的例子是**Catch22**，它包括22个特征和从相位依赖的区间提取的简单摘要统计量。Catch22特征集是**高度比较时间序列分析**（**HCTSA**）工具箱的一个子集，另一个特征集。
- en: Another distinction is between interpretable and non-interpretable features.
    Interpretable features could be summary features such as the mean, max, min, and
    others. These could be pooled within time periods, windows, to give us more features.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个区分是可解释特征和不可解释特征。可解释特征可以是摘要特征，例如均值、最大值、最小值等。这些特征可以在时间段内进行汇总，或者在窗口内汇聚，以提供更多特征。
- en: In features for time-series, a few preprocessing methods come with their recommended
    machine learning model. For example, a ROCKET model is a linear model on top of
    the ROCKET features.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列特征中，一些预处理方法与其推荐的机器学习模型一起使用。例如，ROCKET模型是在ROCKET特征基础上的线性模型。
- en: Taken to the extreme, this can be a form of **model stacking**, where the outcomes
    of models serve as the inputs to other models. This can be an effective way of
    decomposing the learning problem by training less complex (fewer features, fewer
    parameters) models in a supervised setting and using their outputs for training
    other models.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 极端情况下，这可以是一种**模型堆叠**的形式，其中模型的输出作为其他模型的输入。这可以通过在监督环境中训练较简单（特征较少、参数较少）的模型，并利用它们的输出训练其他模型，从而有效地分解学习问题。
- en: Please note it is important that any new feature depends only on past and present
    inputs. In signal processing, this kind of operation is called a **causal filter**.
    The word causal indicates that the filter output for a value at time *t* only
    uses information available at time *t* and doesn't peek into the future. Conversely,
    a filter whose output also depends on future inputs is non-causal. We'll discuss
    Temporal Convolutional Networks, basically causal convolutions, in Chapter 10,
    Deep Learning for Time-Series.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，重要的是任何新特征只能依赖于过去和现在的输入。在信号处理领域，这种操作称为**因果滤波器**。因果一词表示，时间*t*的值的滤波器输出仅使用时间*t*时可用的信息，而不窥视未来。相反，输出也依赖于未来输入的滤波器是非因果的。我们将在第10章《时间序列的深度学习》中讨论时序卷积网络，基本上是因果卷积。
- en: We should take great care in training and testing that any statistics extracted
    and applied in preprocessing are carefully considered – at best, the model performance
    will be overly optimistic if it relies on data that shouldn't be available during
    prediction. We'll discuss leakage in the next chapter, *Chapter 4*, *Machine Learning
    for Time-Series*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该在训练和测试中非常小心，确保在预处理过程中提取和应用的任何统计数据都经过仔细考虑——如果模型依赖于在预测过程中不应当可用的数据，最好的情况下模型性能会过于乐观。我们将在下一章讨论数据泄漏问题，*第4章*，*时间序列的机器学习*。
- en: If we have many features, we might want to simplify our model building process
    by pruning the available features and using only a subset (feature selection),
    or instead, using a new set of features that describe the essential quality of
    the features (dimensionality reduction).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有许多特征，可能想通过修剪可用特征并仅使用子集（特征选择）来简化模型构建过程，或者使用描述特征本质的新特征集合（降维）。
- en: 'We can distinguish the following types of features with time-series:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以区分以下几种时间序列特征：
- en: Date- and time-related features
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期和时间相关特征
- en: Calendar features (date-related)
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日历特征（日期相关）
- en: Time-related features
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间相关特征
- en: Window-based features
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于窗口的特征
- en: Calendar- and time-related features are very similar, so we'll discuss them
    in the same section.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 日历和时间相关特征非常相似，因此我们将在同一节中讨论它们。
- en: Window-based features are features that integrate features within a (rolling)
    window, that is, within a time period. Examples of these are averages over 15-minute
    windows or sales within 7 days. Since we dealt with rolling windows in *Chapter
    2*, *Time-Series Analysis with Python*, in this chapter, we'll deal with more
    complex features such as convolutions and shapelets.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 基于窗口的特征是将特征集成到一个（滚动）窗口中的特征，也就是说，集成到一个时间段内。例如，15分钟窗口内的平均值或7天内的销售额。由于我们在*第2章*，*使用Python进行时间序列分析*中讨论了滚动窗口，本章将讨论更复杂的特征，如卷积和形状特征。
- en: Many preprocessing algorithms are implemented in `sktime`. Another handy library
    is tsfresh, which calculates an enormous number of interpretable features for
    time-series. In the code in this chapter, we've accessed tsfresh features through
    feature tools.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 许多预处理算法都已在`sktime`中实现。另一个方便的库是tsfresh，它计算时间序列的海量可解释特征。在本章的代码中，我们通过特征工具访问了tsfresh特征。
- en: Let's do some more time-series preprocessing in Python! We'll discuss date-
    and time-related features next.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在Python中做更多时间序列预处理！接下来我们将讨论日期和时间相关特征。
- en: Date- and Time-Related Features
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日期和时间相关特征
- en: Date and time variables contain information about dates, time, or a combination
    (datetime). We saw several examples in the previous chapter, *Chapter 2*, *Time-Series
    Analysis with Python* – one of them was the year corresponding to pollution. Other
    examples could be the birth year of a person or the date of a loan being taken
    out.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 日期和时间变量包含关于日期、时间或其组合（日期时间）的信息。我们在上一章中看到了几个例子，*第2章*，*使用Python进行时间序列分析*——其中之一是与污染相关的年份。其他的例子可能包括一个人的出生年份或贷款申请的日期。
- en: If we want to feed these fields into a machine learning model, we need to derive
    relevant information. We could feed the year as an integer, for example, but there
    are many more examples of extracted features from datetime variables, which we'll
    deal with in this section. We can significantly improve the performance of our
    machine learning model by enriching our dataset with these extracted features.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想将这些字段输入到机器学习模型中，我们需要提取相关信息。例如，我们可以将年份作为整数输入，但还有更多从日期时间变量中提取的特征示例，我们将在本节中讨论。通过使用这些提取的特征来丰富我们的数据集，我们可以显著提升机器学习模型的性能。
- en: Workalendar is a Python module that provides classes able to handle calendars,
    including lists of bank and religious holidays, and it offers working-day-related
    functions. Python-holidays is a similar library, but here we'll go with workalendar.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Workalendar是一个Python模块，提供能够处理日历的类，包括银行和宗教假期列表，并提供与工作日相关的函数。Python-holidays是一个类似的库，但这里我们将使用workalendar。
- en: In the next section, we'll discuss ROCKET features.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论ROCKET特征。
- en: ROCKET
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 火箭
- en: 'The research paper *ROCKET: Exceptionally fast and accurate time-series classification
    using random convolutional kernels* (Angus Dempster, François Petitjean, Geoffrey
    I. Webb; 2019) presents a novel methodology for convolving time-series data with
    random kernels that can result in higher accuracy and faster training times for
    machine learning models. What makes this paper unique is banking on the recent
    successes of convolutional neural networks and transferring them to preprocessing
    for time-series datasets.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '研究论文*ROCKET: 使用随机卷积核进行异常快速和准确的时间序列分类*（作者：Angus Dempster, François Petitjean,
    Geoffrey I. Webb；2019）提出了一种新的方法，通过随机核对时间序列数据进行卷积，从而可以提高机器学习模型的准确性并加速训练时间。使这篇论文独特的地方在于，它利用了卷积神经网络的最新成功，并将其转移到时间序列数据集的预处理上。'
- en: We will go into more details of this paper. **ROCKET**, short for **RandOm Convolutional
    KErnel Transform**, is based on convolutions, so let's start with convolutions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将深入探讨这篇论文的细节。**ROCKET**，即**随机卷积核变换**的缩写，基于卷积，因此让我们从卷积开始。
- en: Convolutions are a very important transformation, especially in image processing,
    and are one of the most important building blocks of deep neural networks in image
    recognition. Convolutions consist of feedforward connections, called **filters**
    or **kernels**, that are applied to rectangular patches of the image (the previous
    layer). Each resulting image is then the sliding window of the kernel over the
    whole image. Simply put, in the case of images, a kernel is a matrix used to modify
    the images.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是一种非常重要的变换，尤其在图像处理领域，它是深度神经网络在图像识别中的重要构建模块之一。卷积由前馈连接组成，这些连接被称为**滤波器**或**核**，它们应用于图像的矩形区域（上一层）。每个生成的图像就是核在整个图像上滑动窗口的结果。简单来说，在图像的情况下，核是用来修改图像的矩阵。
- en: 'A sharpening kernel can look like this:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 锐化核可以是这样的：
- en: '![](img/B17577_03_020.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_03_020.png)'
- en: 'If we multiply this kernel to all local neighborhoods in turn, we get a sharper
    image as illustrated below (original on the left, sharpened on the right):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将此核依次乘以所有局部邻域，我们将得到一张更锐利的图像，如下所示（左侧为原图，右侧为锐化后的图像）：
- en: '![merged.png](img/B17577_03_01.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![merged.png](img/B17577_03_01.png)'
- en: 'Figure 3.1: Sharpening filter'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：锐化滤波器
- en: This picture is a gray version of "A woman divided into two, representing life
    and death" (owned by the Wellcome Collection, a museum and exhibition center in
    London; licensed under CC BY 4.0).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片是“一个女人被分成两部分，代表生与死”的灰色版本（该作品归伦敦的Wellcome Collection博物馆和展览中心所有，采用CC BY 4.0许可）。
- en: The sharpening kernel emphasizes differences in adjacent pixel values. You can
    see that the picture on the right is much grainier or vivid – a result of the
    convolution.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 锐化核强调相邻像素值之间的差异。你可以看到右侧的图片明显更加颗粒感或生动——这是卷积的结果。
- en: We can apply kernels not only to images but also to vectors or matrices, and
    this brings us back to ROCKET. ROCKET computes two aggregate features from each
    kernel and feature convolution. The two features are created using the well-known
    methodology global/average max pooling and a novel methodology that we'll come
    to in a second.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅可以将核应用于图像，还可以将其应用于向量或矩阵，这将我们带回到ROCKET。ROCKET从每个核和特征卷积中计算出两个汇总特征。这两个特征是通过广为人知的全局/平均最大池化方法以及我们稍后会介绍的全新方法创建的。
- en: '**Global max pooling** outputs the maximum value from the result of convolution
    and max pooling takes the maximum value within a pool size. For example, if the
    result of the convolution is 0,1,2,2,5,1,2, global max pooling returns 5, whereas
    **max pooling** with pool size 3 outputs the maxima within windows of 3, so we''ll
    get 2,2,5,5,5\.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**全局最大池化（Global max pooling）**输出卷积结果中的最大值，而最大池化（max pooling）则是在池化大小内选择最大值。例如，如果卷积结果为
    0,1,2,2,5,1,2，则全局最大池化返回 5，而池化大小为 3 的最大池化会在 3 个窗口内选择最大值，所以结果为 2,2,5,5,5。'
- en: '**Positive Proportion Value** (**PPV**), the methodology from the paper, is
    the proportion (percentage) of values from the convolution that are positive (or
    above a bias threshold).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**正比例值**（**PPV**）是论文中的方法学，它是卷积结果中正值（或高于偏置阈值）的比例（百分比）。'
- en: We can improve machine learning accuracy from time-series by applying transformations
    with convolutional kernels. Each feature gets transformed by random kernels, the
    number of which is a parameter to the algorithm. This is set to 10,000 by default.
    The transformed features can now be fed as input into any machine learning algorithm.
    The authors propose to use linear algorithms like ridge regression classifier
    or logistic regression.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过应用带有卷积核的变换来提高时间序列的机器学习准确度。每个特征都通过随机核进行变换，核的数量是算法的一个参数。默认设置为 10,000。变换后的特征现在可以作为输入，输入到任何机器学习算法中。作者建议使用线性算法，如岭回归分类器或逻辑回归。
- en: 'The idea of ROCKET is very similar to Convolutional Neural Networks (CNNs),
    which we''ll discuss in chapter 10, Deep Learning for Time-Series, however, two
    big differences are:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ROCKET 的思想与卷积神经网络（CNN）非常相似，我们将在第 10 章《时间序列深度学习》中讨论这一点。然而，有两个显著的区别：
- en: ROCKET doesn't use any hidden layers or non-linearities
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ROCKET 不使用任何隐藏层或非线性函数。
- en: The convolutions are applied independently for each feature
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积是针对每个特征独立应用的。
- en: In the next section, we'll be discussing shapelets.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将讨论形状特征。
- en: Shapelets
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 形状特征
- en: 'Shapelets for time-series were presented in the research paper *Time-Series
    Shapelets: A New Primitive for Data Mining* (Lexiang Ye and Eamonn Keogh, 2009).
    The basic idea of shapelets is decomposing time-series into discriminative subsections
    (shapelets).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列的形状特征在研究论文《时间序列形状特征：数据挖掘的新原语》（Lexiang Ye 和 Eamonn Keogh，2009）中提出。形状特征的基本思想是将时间序列分解为具有判别性的子部分（形状特征）。
- en: In the first step, the shapelets are learned. The algorithm calculates the information
    gain of possible candidates and picks the best candidates to create a shapelet
    dictionary of discriminating subsections. This can be quite expensive. Then, based
    on the shapelet decomposition of the features, a decision tree or other machine
    learning algorithms can be applied.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，形状特征被学习。算法计算可能候选项的信息增益，并选择最佳候选项来创建一个包含判别子部分的形状特征字典。这可能是非常耗费计算的。然后，根据特征的形状特征分解，可以应用决策树或其他机器学习算法。
- en: 'Shapelets have several advantages over other methods:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 形状特征相比其他方法有几个优势：
- en: They can provide interpretable results
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以提供可解释的结果。
- en: The application of shapelets can be very fast – only depending on the matching
    of features against the dictionary of shapelets
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 形状特征的应用可以非常快速——只需要根据特征与形状特征字典的匹配来进行。
- en: The performance of machine learning algorithms on top of shapelets is usually
    very competitive
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于形状特征的机器学习算法的表现通常非常具有竞争力。
- en: It's time that we go through Python exercises with actual datasets.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候通过实际数据集来进行 Python 练习了。
- en: Python Practice
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 实践
- en: NumPy and SciPy offer most of the functionality that we need, but we might need
    a few more libraries.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 和 SciPy 提供了我们所需的大部分功能，但我们可能需要一些额外的库。
- en: 'In this section, we''ll use several libraries, which we can quickly install
    from the terminal, **the Jupyter Notebook**, or similarly **from Anaconda Navigator**:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将使用几个库，这些库可以通过终端、**Jupyter Notebook** 或类似的 **Anaconda Navigator** 快速安装：
- en: '[PRE0]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: All of these libraries are quite powerful and each of them deserves more than
    the space we can give to it in this chapter.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些库都非常强大，每一个都值得我们在本章中所能给予的更多空间。
- en: Let's start with log and power transformations.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从对数和幂变换开始。
- en: Log and Power Transformations in Practice
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对数变换和幂变换的实践应用
- en: Let's create a distribution that's not normal, and let's log-transform it. We'll
    plot the original and transformed distribution for comparison, and we'll apply
    a statistical test for normality.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个不是正态的分布，然后进行对数变换。我们将原始分布和变换后的分布进行对比，并应用正态性检验。
- en: 'Let''s first create the distribution:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们创建该分布：
- en: '[PRE1]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Values are sampled from a lognormal distribution. I've added a call to the random
    number generator seed function to make sure the result is reproducible for readers.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 值是从对数正态分布中采样的。我添加了对随机数生成器种子函数的调用，以确保结果对读者是可重复的。
- en: 'We can visualize our array as a histogram:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将我们的数组可视化为一个直方图：
- en: '![lognormal_hist.png](img/B17577_03_02.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![lognormal_hist.png](img/B17577_03_02.png)'
- en: 'Figure 3.2: An array sampled from a lognormal distribution'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：从对数正态分布中采样的数组
- en: I've used a log scale on the y-axis. We can see that the values spread over
    a number of orders of magnitude.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 y 轴上使用了对数尺度。我们可以看到，值分布在多个数量级上。
- en: 'We can apply the standard normalization to z-scores. We can also apply a statistical
    normality test on one of the transformed distributions:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以应用标准化 z 分数变换。我们还可以对其中一个变换后的分布进行正态性检验：
- en: '[PRE2]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The null hypothesis of this statistical test is that the sample comes from a
    normal distribution. Therefore significance values (p-values) lower than a threshold,
    typically set to 0.05 or 0.01, would let us reject the null hypothesis.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 该统计检验的零假设是样本来自正态分布。因此，低于阈值（通常设置为 0.05 或 0.01）的显著性值（p 值）将使我们拒绝零假设。
- en: 'We are getting this output: `significance: 0.00`.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '我们得到的输出是：`significance: 0.00`。'
- en: We can conclude from the test that we are not getting a null distribution from
    our transformation by standard scaling.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从测试中得出结论：通过标准缩放变换得到的分布不是零假设分布。
- en: 'This should be obvious, but let''s get it out of the way: we are getting the
    same significance for the minmax transformed values:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该很明显，但还是说清楚：我们对 minmax 变换后的值得到了相同的显著性值：
- en: '[PRE3]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We therefore reach the same conclusion: the minmax transformation hasn''t helped
    us get a normal-like distribution.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们得出相同的结论：minmax 变换没有帮助我们得到接近正态的分布。
- en: We can plot the original and the standard scaled distribution against each other.
    Unsurprisingly, visually, the two distributions look the same except for the scale.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将原始分布和标准缩放后的分布相互对比。毫不意外地，从视觉上看，两个分布除了缩放外看起来相同。
- en: '![standard_scaled.png](img/B17577_03_03.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![standard_scaled.png](img/B17577_03_03.png)'
- en: 'Figure 3.3: The linear transformation against the original values'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3：线性变换与原始值的对比
- en: We can see everything lies on the diagonal.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到所有数据点都位于对角线上。
- en: 'Let''s use a log transformation:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来进行对数变换：
- en: '[PRE4]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We are getting a significance of `0.31`. This lets us conclude that we can't
    reject the null hypothesis. Our distribution is similar to normal. In fact, we
    get a standard deviation close to 1.0 and a mean close to 0.0 as we would expect
    with a normal distribution.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了 `0.31` 的显著性值。这使我们得出结论，我们无法拒绝原假设。我们的分布类似于正态分布。实际上，我们得到的标准差接近 1.0，均值接近 0.0，符合正态分布的预期。
- en: We can see that the log-normal distribution is a continuous probability distribution
    whose logarithm is normally distributed, so it's not entirely surprising that
    we get this result.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对数正态分布是一个连续的概率分布，其对数是正态分布的，因此我们得到这个结果也不完全令人惊讶。
- en: 'We can plot the histogram of the log-transformed distribution:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以绘制对数变换后分布的直方图：
- en: '![log_transformed_hist.png](img/B17577_03_04.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![log_transformed_hist.png](img/B17577_03_04.png)'
- en: 'Figure 3.4: Log-transformed lognormal array'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4：对数变换后的对数正态数组
- en: The log transform looks much more normal-like as we can appreciate.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，对数变换看起来更接近正态分布。
- en: 'We can also apply Box-Cox transformation:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以应用 Box-Cox 变换：
- en: '[PRE5]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We are getting a significance of 0.46\. Again, we can conclude that our Box-Cox
    transform is normal-like. We can also see this in a plot of the Box-Cox transformed
    distribution:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了 0.46 的显著性值。再次，我们可以得出结论，我们的 Box-Cox 变换接近正态分布。我们也可以通过绘制 Box-Cox 变换后的分布来看到这一点：
- en: '![bc_transformed_hist.png](img/B17577_03_041.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![bc_transformed_hist.png](img/B17577_03_041.png)'
- en: 'Figure 3.5: Box-Cox transformed lognormal array'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5：Box-Cox 变换后的对数正态数组
- en: Again, this looks very much like a normal distribution. This plot looks pretty
    much the same as the previous one of the log transformation, which shouldn't be
    surprising given that the log operation corresponds to a lambda parameter of 0
    in the Box-Cox transformation.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来非常像正态分布。这个图与之前的对数变换图几乎一样，考虑到对数操作对应于Box-Cox变换中的lambda参数为0，这一点并不令人惊讶。
- en: This is a small selection of transformations that can help us reconcile our
    data with the common normality assumption in classical forecasting methods.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一些转换的小选择，可以帮助我们将数据与经典预测方法中的常见正态性假设对齐。
- en: Let's look at imputation in practice.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看填补的实际操作。
- en: Imputation
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填补
- en: It is rather uncommon for machine learning algorithms to be able to deal with
    missing values directly. Rather, we'll either have to replace missing values with
    constants or infer probable values given the other features.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法直接处理缺失值并不常见。相反，我们通常需要用常数替代缺失值，或者根据其他特征推断可能的值。
- en: 'The scikit-learn documentation lists a simple example of unit imputation:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn文档中列出了一个简单的单位填补示例：
- en: '[PRE6]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We are again using (similar to the standard scaler before) the scikit-learn
    transformers, which come with `fit()` and `transform()` methods.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用（类似之前的标准化器）scikit-learn的转换器，它们带有`fit()`和`transform()`方法。
- en: 'We get the following imputed values:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下填补后的值：
- en: '[PRE7]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The missing values are replaced with the mean of the columns.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值将被列的均值替代。
- en: Let's look at annotating holidays as derived date features.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看将假期注释为派生日期特征。
- en: Holiday Features
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 假期特征
- en: 'If we want to get the holidays for the United Kingdom, we can do this:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想获取英国的假期，可以这样做：
- en: '[PRE8]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We get the following holidays:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下假期：
- en: '[PRE9]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can generate holidays by year and then look up holidays by date.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按年份生成假期，然后按日期查找假期。
- en: 'Similarly, we can get holidays for other places, for example, California, USA.
    We can also extract lists of holidays, and add custom holidays:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以获取其他地方的假期，例如美国加利福尼亚州。我们还可以提取假期列表，并添加自定义假期：
- en: '[PRE10]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This gives us our custom holidays for the year 2021:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了2021年的自定义假期：
- en: '[PRE11]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Please note that we are using type hints in the code segment above. We are
    declaring the signature of our function like this:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在上面的代码片段中使用了类型提示。我们像这样声明函数的签名：
- en: '[PRE12]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This means we are expecting an integer called `year` and we are expecting a
    `List` as an output. Annotations are optional and they are not getting checked
    (if you don't invoke mypy), but they can make code in Python much clearer.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们期望一个名为`year`的整数，并期望一个`List`作为输出。注释是可选的，且不会被检查（如果你没有调用mypy），但它们可以使Python代码更加清晰。
- en: 'Now we can implement a simple lookup like this:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以实现一个简单的查找操作，如下所示：
- en: '[PRE13]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: I am getting a `False` even though I wish it were a holiday.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我希望它是一个假期，我得到的也是`False`。
- en: This can be a very useful feature for a machine learning model. For example,
    we could imagine a different profile of users who apply for loans on bank holidays
    or on a weekday.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这对机器学习模型来说是一个非常有用的特征。例如，我们可以设想在银行假期或工作日申请贷款的用户群体有所不同。
- en: Date Annotation
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日期注释
- en: The calendar module offers lots of methods, for example, `monthrange() - calendar.monthrange`
    returns the first weekday of the month and the number of days in a month for a
    given year and month. The day of the week is given as an integer, where Monday
    is 0 and Sunday is 6.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: calendar模块提供了许多方法，例如，`monthrange() - calendar.monthrange`返回给定年份和月份的第一天是星期几，以及该月的天数。星期几以整数表示，星期一为0，星期日为6。
- en: '[PRE14]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We should be getting (`4, 31`). This means the first weekday of 2021 was a Friday.
    January 2021 had 31 days.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该得到（`4, 31`）。这意味着2021年的第一天是星期五。2021年1月有31天。
- en: 'We can also extract features relevant to the day with respect to the year.
    The following function provides the number of days since the end of the previous
    year and to the end of the current year:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以提取与年份相关的日期特征。以下函数提供了从前一年结束到当前年结束的天数：
- en: '[PRE15]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This feature could provide a general idea of how far into the year we are. This
    can be useful both for estimating a trend and for capturing cyclic variations.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特征可以提供我们在一年中的大致位置。这对于估计趋势和捕捉周期性变化都非常有用。
- en: 'Similarly, we can extract the number of days from the first of the month and
    to the end of the month:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以提取从每月的第一天到月底的天数：
- en: '[PRE16]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: A feature like this could also provide some useful information. I am getting
    `(10, 8)`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的特征也可以提供一些有用的信息。我得到的是`(10, 8)`。
- en: In retail, it is very important to predict the spending behavior of customers.
    Therefore, in the next section, we'll write annotation for pay days.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在零售行业，预测顾客的消费行为非常重要。因此，在接下来的章节中，我们将为发薪日编写注解。
- en: Paydays
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发薪日
- en: We could imagine that some people get paid in the middle or at the end of the
    month, and would then access our website to buy our products.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以想象一些人会在月中或月底领取工资，然后访问我们的网站购买我们的产品。
- en: 'Most people would get paid on the last Friday of the month, so let''s write
    a function for this:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人会在月末的最后一个星期五领到工资，因此我们来编写一个函数来处理这个问题：
- en: '[PRE17]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: I am getting 30 as the last Friday.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我得到的30是最后一个星期五。
- en: Seasons can also be predictive.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 季节也可以具有预测性。
- en: Seasons
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 季节
- en: 'We can get the season for a specific date:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以获取特定日期的季节：
- en: '[PRE18]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We should be getting `spring` here, but the reader is encouraged to try this
    with different values.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该在这里获得`spring`，但鼓励读者尝试不同的值。
- en: The Sun and Moon
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 太阳与月亮
- en: 'The Astral module offers information about sunrise, moon phases, and more.
    Let''s get the hours of sunlight for a given day in London:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Astral模块提供了关于日出、月相等信息。让我们获取伦敦某一天的日照时间：
- en: '[PRE19]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: I am getting `9.788055555555555` hours of daylight.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我得到的是`9.788055555555555`小时的日照时间。
- en: It can often be observed that the more hours of daylight, the more business
    activity. We could speculate that this feature could be helpful in predicting
    the volume of our sales.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们常常可以观察到，日照时间越长，商业活动越多。我们可以推测，这一特征可能有助于预测我们的销售量。
- en: Business Days
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作日
- en: 'Similarly, if a month has more business days, we could expect more sales for
    our retail store. On the other hand, if we are selling windsurfing lessons, we
    might want to know the number of holidays in a given month. The following function
    extracts the number of business days and weekends/holidays in a month:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，如果一个月有更多的工作日，我们可能会预计零售店的销售量会更多。另一方面，如果我们正在出售风帆冲浪课程，我们可能想知道某个月的假期数量。以下函数提取了一个月中的工作日和周末/假期数量：
- en: '[PRE20]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We should be getting `(22, 9)` – 22 business days and 9 weekend days and holidays.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该得到`(22, 9)`——22个工作日和9个周末及假期。
- en: Automated Feature Extraction
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动特征提取
- en: 'We can also use automated feature extraction tools from modules like featuretools.
    Featuretools calculates many datetime-related functions. Here''s a quick example:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用像featuretools这样的模块中的自动特征提取工具。Featuretools可以计算许多与日期时间相关的函数。这里是一个快速示例：
- en: '[PRE21]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Our features are `Minute`, `Hour`, `Day`, `Month`, `Year`, and `Weekday`. Here''s
    our DataFrame, `fm`:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的特征有`Minute`、`Hour`、`Day`、`Month`、`Year`和`Weekday`。这是我们的DataFrame，`fm`：
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/(A Document
    Being Saved By screencaptureui 3)/Screenshot 2021-04-11 at 21.58.52.png](img/B17577_03_05.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/(A Document
    Being Saved By screencaptureui 3)/Screenshot 2021-04-11 at 21.58.52.png](img/B17577_03_05.png)'
- en: 'Figure 3.6: Featuretools output'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6：Featuretools 输出
- en: We could extract many more features. Please see the featuretools documentation
    for more details.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提取更多的特征。请参阅featuretools文档以获取更多详细信息。
- en: 'The tsfresh module also provides automated functionality for feature extraction:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: tsfresh模块还提供了自动化的特征提取功能：
- en: '[PRE22]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Please note that tsfresh optimizes features using statsmodels' autoregression,
    and (last I checked) still hasn't been updated to use `statsmodels.tsa.AutoReg`
    instead of `statsmodels.tsa.AR`, which has been deprecated.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，tsfresh通过使用statsmodels的自回归优化特征，（截至我上次检查）仍未更新为使用`statsmodels.tsa.AutoReg`，而`statsmodels.tsa.AR`已经被弃用。
- en: We get 1,574 features that describe our `time` object. These features could
    help us in machine learning models.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得了1,574个特征，用于描述我们的`time`对象。这些特征可能在机器学习模型中帮助我们。
- en: Let's demonstrate how to extract ROCKET features from a time-series.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们演示如何从时间序列中提取ROCKET特征。
- en: ROCKET
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ROCKET
- en: We'll be using the implementation of ROCKET in the `sktime` library.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`sktime`库中的ROCKET实现。
- en: The `sktime` library represents data in a nested DataFrame. Each column stands
    for a feature, as expected, however, what may be surprising is that each row is
    an instance of a time-series. Each cell contains an array of all values for a
    given feature over time. In other words, each cell has a nested object structure,
    where instance-feature combinations are stored.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`sktime` 库以嵌套 DataFrame 的形式表示数据。每一列代表一个特征，正如预期的那样，但令人惊讶的是，每一行都是一个时间序列的实例。每个单元格包含一个数组，表示某个特征在时间维度上的所有值。换句话说，每个单元格都有一个嵌套的对象结构，其中存储了实例-特征组合。'
- en: This structure makes sense, because it allows us to store multiple instances
    of time-series in the same DataFrame, however, it's not intuitive at first. Fortunately,
    SkTime provides utility functions to unnest the SkTime datasets, as we will see.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构是合理的，因为它允许我们在同一个 DataFrame 中存储多个时间序列实例，然而，刚开始时这并不直观。幸运的是，SkTime 提供了实用函数来解嵌套
    SkTime 数据集，正如我们接下来会看到的那样。
- en: 'If we want to load an example time-series in SkTime, we can do this:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在 SkTime 中加载一个示例时间序列，我们可以这样做：
- en: '[PRE23]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We get an unnested DataFrame like this:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到一个未嵌套的 DataFrame，如下所示：
- en: '![../../../../../../Desktop/Screenshot%202021-04-12%20at%2](img/B17577_03_06.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![../../../../../../Desktop/Screenshot%202021-04-12%20at%2](img/B17577_03_06.png)'
- en: 'Figure 3.7: ROCKET features'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7：ROCKET 特征
- en: Again, each row is a time-series. There's only one feature, called `dim_0`.
    The time-series has 251 measurements.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，每一行是一个时间序列。这里只有一个特征，叫做 `dim_0`。这个时间序列有 251 个测量值。
- en: We can import ROCKET, and then create the ROCKET features. We'll first have
    to learn the features and then apply them. This is a typical pattern for machine
    learning. In scitkit-learn, we'd use the `fit()` and `predict()` methods for models,
    where `fit()` is applied on the training data and `predict()` gives the predictions
    on a test set.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以导入 ROCKET，然后创建 ROCKET 特征。我们首先需要学习这些特征，然后应用它们。这是机器学习中的一个典型模式。在 scikit-learn
    中，我们会使用 `fit()` 和 `predict()` 方法来训练模型，其中 `fit()` 应用在训练数据上，`predict()` 用于在测试集上给出预测。
- en: 'The learning step should only ever be applied to the training set. One of the
    parameters in ROCKET is the number of kernels. We''ll set it to 1,000 here, but
    we can set it to a higher number as well. 10,000 kernels is the default:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 学习步骤应该只应用于训练集。ROCKET 的一个参数是内核数量。我们在这里将其设置为 1,000，但也可以设置为更高的数字。默认值是 10,000 个内核：
- en: '[PRE24]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The returned dataset is not nested, and it contains 2,000 columns. Each column
    describes the whole time-series but is the result from a different kernel.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的数据集没有嵌套，包含 2,000 列。每一列描述整个时间序列，但它是来自不同内核的结果。
- en: In the next section, we'll do a shapelets exercise.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将进行一个形状体练习。
- en: Shapelets in Practice
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 形状体在实践中的应用
- en: 'Let''s create shapelets for the dataset we used before when we looked at ROCKET.
    We''ll again use `SkTime`:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为之前在查看 ROCKET 时使用的数据集创建形状体。我们将再次使用 `SkTime`：
- en: '[PRE25]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The training could take a few minutes. We'll get some output about the candidates
    that are being examined.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 训练可能需要几分钟。我们将得到一些关于正在检查的候选项的输出。
- en: 'We can again transform our time-series using our shapelet transformer. This
    works as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次使用我们的形状体转换器来变换时间序列。其工作原理如下：
- en: '[PRE26]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This gives us a transformed dataset that we can use in machine learning models.
    We encourage the reader to play around with these feature sets.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一个转化后的数据集，我们可以将其用于机器学习模型。我们鼓励读者在这些特征集上进行实验。
- en: This concludes our Python practice.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的 Python 实践的结束。
- en: Summary
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Preprocessing is a crucial step in machine learning that is often neglected.
    Many books don't cover preprocessing as a topic or skip preprocessing entirely.
    However, it is often in preprocessing that relatively easy wins can be achieved.
    The quality of the data determines the outcome.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理是机器学习中一个至关重要的步骤，但通常被忽视。许多书籍没有将预处理作为话题，或者完全跳过了预处理。然而，正是在预处理阶段，通常可以获得相对简单的胜利。数据的质量决定了结果。
- en: Preprocessing includes curating and screening the data. The expected output
    of the preprocessing is a dataset on which it is easier to conduct machine learning.
    This can mean that it is more reliable and less noisy than the original dataset.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理包括策划和筛选数据。预处理的预期输出是一个更易于进行机器学习的数据集。这意味着该数据集比原始数据集更可靠且噪声更少。
- en: We've talked about feature transforms and feature engineering approaches to
    time-series data, and we've talked about automated approaches as well.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论过时间序列数据的特征变换和特征工程方法，并且还讨论过自动化方法。
- en: In the next chapters, we'll explore how we can use these extracted features
    in a machine learning model. We'll discuss combinations of features and modeling
    algorithms in the next chapter, *Chapter 4*, *Introduction to Machine Learning
    for Time-Series*. In *Chapter 5*, *Time-Series Forecasting with Moving Averages
    and Autoregressive Models*, we'll be using machine learning pipelines, where we
    can connect feature extraction and machine learning models.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨如何在机器学习模型中使用这些提取出的特征。在下一章节，*第四章*，*时间序列的机器学习简介*中，我们将讨论特征组合和建模算法。在*第五章*，*使用移动平均和自回归模型进行时间序列预测*中，我们将使用机器学习管道，将特征提取和机器学习模型连接起来。
