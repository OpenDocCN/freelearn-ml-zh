- en: Chapter 9. From the Perceptron to Support Vector Machines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章. 从感知机到支持向量机
- en: In the previous chapter we discussed the perceptron. As a binary classifier,
    the perceptron cannot be used to effectively classify linearly inseparable feature
    representations. We encountered a similar problem to this in our discussion of
    multiple linear regression in [Chapter 2](ch02.html "Chapter 2. Linear Regression"),
    *Linear Regression*; we examined a dataset in which the response variable was
    not linearly related to the explanatory variables. To improve the accuracy of
    the model, we introduced a special case of multiple linear regression called polynomial
    regression. We created synthetic combinations of features, and were able to model
    a linear relationship between the response variable and the features in the higher-dimensional
    feature space.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了感知机。作为一种二分类器，感知机无法有效地对线性不可分的特征表示进行分类。在我们讨论[第2章](ch02.html "第2章. 线性回归")中*线性回归*时，也遇到了类似的问题；我们检查了一个响应变量与解释变量之间不是线性关系的数据集。为了提高模型的准确性，我们引入了一种多重线性回归的特例，称为多项式回归。我们创建了特征的合成组合，并能够在更高维的特征空间中建模响应变量与特征之间的线性关系。
- en: While this method of increasing the dimensions of the feature space may seem
    like a promising technique to use when approximating nonlinear functions with
    linear models, it suffers from two related problems. The first is a computational
    problem; computing the mapped features and working with larger vectors requires
    more computing power. The second problem pertains to generalization; increasing
    the dimensions of the feature representation introduces the curse of dimensionality.
    Learning from high-dimensional feature representations requires exponentially
    more training data to avoid overfitting.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种增加特征空间维度的方法看起来是用线性模型逼近非线性函数的有希望的技术，但它有两个相关的问题。第一个是计算问题；计算映射后的特征并处理更大的向量需要更多的计算能力。第二个问题涉及到泛化；增加特征表示的维度引入了维度灾难。学习高维特征表示需要指数级更多的训练数据，以避免过拟合。
- en: In this chapter, we will discuss a powerful model for classification and regression
    called the **support vector** **machine** (**SVM**). First, we will revisit mapping
    features to higher-dimensional spaces. Then, we will discuss how support vector
    machines mitigate the computation and generalization problems encountered when
    learning from the data mapped to higher-dimensional spaces. Entire books are devoted
    to describing support vector machines, and describing the optimization algorithms
    used to train SVMs requires more advanced math than we have used in previous chapters.
    Instead of working through toy examples in detail as we have done in previous
    chapters, we will try to develop an intuition for how support vector machines
    work in order to apply them effectively with scikit-learn.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将讨论一种强大的分类和回归模型，称为**支持向量****机**（**SVM**）。首先，我们将重新探讨将特征映射到更高维空间。然后，我们将讨论支持向量机如何解决在学习映射到更高维空间的数据时遇到的计算和泛化问题。整个书籍都致力于描述支持向量机，描述训练SVM所用的优化算法需要比我们在之前的章节中使用的更为高级的数学。我们不会像之前那样通过玩具示例详细推导，而是尽量通过建立直觉来理解支持向量机的工作原理，以便有效地在scikit-learn中应用它们。
- en: Kernels and the kernel trick
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核与核技巧
- en: 'Recall that the perceptron separates the instances of the positive class from
    the instances of the negative class using a hyperplane as a decision boundary.
    The decision boundary is given by the following equation:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，感知机通过使用超平面作为决策边界，将正类的实例与负类的实例分开。决策边界由以下方程给出：
- en: '![Kernels and the kernel trick](img/8365OS_09_01.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![核与核技巧](img/8365OS_09_01.jpg)'
- en: 'Predictions are made using the following function:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 预测是通过以下函数进行的：
- en: '![Kernels and the kernel trick](img/8365OS_09_02.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![核与核技巧](img/8365OS_09_02.jpg)'
- en: Note that previously we expressed the inner product ![Kernels and the kernel
    trick](img/8365OS_09_40.jpg) as ![Kernels and the kernel trick](img/8365OS_09_41.jpg).
    To be consistent with the notational conventions used for support vector machines,
    we will adopt the former notation in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，之前我们将内积表示为![核与核技巧](img/8365OS_09_40.jpg)，即![核与核技巧](img/8365OS_09_41.jpg)。为了与支持向量机中使用的符号约定保持一致，我们将在本章采用前一种表示法。
- en: 'While the proof is beyond the scope of this chapter, we can write the model
    differently. The following expression of the model is called the **dual** form.
    The expression we used previously is the **primal** form:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然证明超出了本章的范围，但我们可以用不同的方式来写这个模型。下面的模型表达式被称为**对偶**形式。我们之前使用的表达式是**原始**形式：
- en: '![Kernels and the kernel trick](img/8365OS_09_05.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![核函数与核技巧](img/8365OS_09_05.jpg)'
- en: The most important difference between the primal and dual forms is that the
    primal form computes the inner product of the *model parameters* and the test
    instance's feature vector, while the dual form computes the inner product of the
    *training instances* and the test instance's feature vector. Shortly, we will
    exploit this property of the dual form to work with linearly inseparable classes.
    First, we must formalize our definition of mapping features to higher-dimensional
    spaces.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 原始形式与对偶形式之间最重要的区别在于，原始形式计算的是*模型参数*和测试实例特征向量的内积，而对偶形式计算的是*训练实例*和测试实例特征向量的内积。稍后，我们将利用对偶形式的这一性质来处理线性不可分的类别。首先，我们需要正式化将特征映射到更高维空间的定义。
- en: 'In the section on polynomial regression in [Chapter 2](ch02.html "Chapter 2. Linear
    Regression"), *Linear Regression*, we mapped features to a higher-dimensional
    space in which they were linearly related to the response variable. The mapping
    increased the number of features by creating quadratic terms from combinations
    of the original features. These synthetic features allowed us to express a nonlinear
    function with a linear model. In general, a mapping is given by the following
    expression:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html "第2章：线性回归")的多项式回归部分中，*线性回归*，我们将特征映射到一个更高维的空间，在这个空间中它们与响应变量是线性相关的。这个映射通过从原始特征的组合中创建二次项来增加特征的数量。这些合成特征使我们能够用线性模型表示一个非线性函数。一般来说，映射由以下表达式给出：
- en: '![Kernels and the kernel trick](img/8365OS_09_07.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![核函数与核技巧](img/8365OS_09_07.jpg)'
- en: 'The plot on the left in the following figure shows the original feature space
    of a linearly inseparable data set. The plot on the right shows that the data
    is linearly separable after mapping to a higher-dimensional space:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下图左侧的图示展示了一个线性不可分数据集的原始特征空间。右侧的图示显示了数据在映射到更高维空间后变得线性可分：
- en: '![Kernels and the kernel trick](img/8365OS_09_08.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![核函数与核技巧](img/8365OS_09_08.jpg)'
- en: 'Let''s return to the dual form of our decision boundary, and the observation
    that the feature vectors appear only inside of a dot product. We could map the
    data to a higher-dimensional space by applying the mapping to the feature vectors
    as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到决策边界的对偶形式，并观察到特征向量仅出现在点积中。我们可以通过如下方式将数据映射到更高维空间，通过对特征向量应用映射：
- en: '![Kernels and the kernel trick](img/8365OS_09_09.jpg)![Kernels and the kernel
    trick](img/8365OS_09_10.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![核函数与核技巧](img/8365OS_09_09.jpg)![核函数与核技巧](img/8365OS_09_10.jpg)'
- en: As noted, this mapping allows us to express more complex models, but it introduces
    computation and generalization problems. Mapping the feature vectors and computing
    their dot products can require a prohibitively large amount of processing power.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这个映射使我们能够表示更复杂的模型，但它也带来了计算和泛化问题。映射特征向量并计算它们的点积可能需要大量的计算资源。
- en: Observe in the second equation that while we have mapped the feature vectors
    to a higher-dimensional space, the feature vectors still only appear as a dot
    product. The dot product is scalar; we do not require the mapped feature vectors
    once this scalar has been computed. If we can use a different method to produce
    the same scalar as the dot product of the mapped vectors, we can avoid the costly
    work of explicitly computing the dot product and mapping the feature vectors.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意第二个方程式，尽管我们已将特征向量映射到更高维空间，但特征向量仍然只以点积的形式出现。点积是标量；一旦计算出这个标量，我们就不再需要映射后的特征向量。如果我们能够使用其他方法来产生与映射向量点积相同的标量，就可以避免显式计算点积和映射特征向量的高昂工作。
- en: 'Fortunately, there is such a method called the **kernel trick**. A **kernel**
    is a function that, given the original feature vectors, returns the same value
    as the dot product of its corresponding mapped feature vectors. Kernels do not
    explicitly map the feature vectors to a higher-dimensional space, or calculate
    the dot product of the mapped vectors. Kernels produce the same value through
    a different series of operations that can often be computed more efficiently.
    Kernels are defined more formally in the following equation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一种方法叫做 **核技巧**。**核函数**是一种函数，给定原始特征向量后，它返回与其对应的映射特征向量的点积相同的值。核函数并不显式地将特征向量映射到更高维的空间，或者计算映射向量的点积。核函数通过一系列不同的操作产生相同的值，这些操作通常可以更高效地计算。核函数在以下方程中定义得更为正式：
- en: '![Kernels and the kernel trick](img/8365OS_09_11.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![核函数与核技巧](img/8365OS_09_11.jpg)'
- en: 'Let''s demonstrate how kernels work. Suppose that we have two feature vectors,
    *x* and *z*:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们演示一下核函数是如何工作的。假设我们有两个特征向量，*x* 和 *z*：
- en: '![Kernels and the kernel trick](img/8365OS_09_12.jpg)![Kernels and the kernel
    trick](img/8365OS_09_13.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![核函数与核技巧](img/8365OS_09_12.jpg)![核函数与核技巧](img/8365OS_09_13.jpg)'
- en: 'In our model, we wish to map the feature vectors to a higher-dimensional space
    using the following transformation:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型中，我们希望通过以下转换将特征向量映射到更高维的空间：
- en: '![Kernels and the kernel trick](img/8365OS_09_14.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![核函数与核技巧](img/8365OS_09_14.jpg)'
- en: 'The dot product of the mapped, normalized feature vectors is equivalent to:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 映射后的归一化特征向量的点积相当于：
- en: '![Kernels and the kernel trick](img/8365OS_09_15.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![核函数与核技巧](img/8365OS_09_15.jpg)'
- en: 'The kernel given by the following equation produces the same value as the dot
    product of the mapped feature vectors:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由以下方程给出的核函数产生的值与映射特征向量的点积相同：
- en: '![Kernels and the kernel trick](img/8365OS_09_16.jpg)![Kernels and the kernel
    trick](img/8365OS_09_11.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![核函数与核技巧](img/8365OS_09_16.jpg)![核函数与核技巧](img/8365OS_09_11.jpg)'
- en: 'Let''s plug in values for the feature vectors to make this example more concrete:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为特征向量代入值，使这个例子更加具体：
- en: '![Kernels and the kernel trick](img/8365OS_09_17.jpg)![Kernels and the kernel
    trick](img/8365OS_09_18.jpg)![Kernels and the kernel trick](img/8365OS_09_19.jpg)![Kernels
    and the kernel trick](img/8365OS_09_20.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![核函数与核技巧](img/8365OS_09_17.jpg)![核函数与核技巧](img/8365OS_09_18.jpg)![核函数与核技巧](img/8365OS_09_19.jpg)![核函数与核技巧](img/8365OS_09_20.jpg)'
- en: 'The kernel ![Kernels and the kernel trick](img/8365OS_09_43.jpg) produced the
    same value as the dot product ![Kernels and the kernel trick](img/8365OS_09_44.jpg)
    of the mapped feature vectors, but never explicitly mapped the feature vectors
    to the higher-dimensional space and required fewer arithmetic operations. This
    example used only two dimensional feature vectors. Data sets with even a modest
    number of features can result in mapped feature spaces with massive dimensions.
    scikit-learn provides several commonly used kernels, including the polynomial,
    sigmoid, Gaussian, and linear kernels. Polynomial kernels are given by the following
    equation:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由以下方程给出的核函数 ![核函数与核技巧](img/8365OS_09_43.jpg) 产生了与映射特征向量的点积 ![核函数与核技巧](img/8365OS_09_44.jpg)
    相同的值，但从未显式地将特征向量映射到更高维空间，并且需要更少的算术运算。这个例子使用的是二维特征向量。即使是具有适度特征数量的数据集，也可能导致映射特征空间的维度巨大。scikit-learn
    提供了几种常用的核函数，包括多项式核函数、Sigmoid 核函数、高斯核函数和线性核函数。多项式核函数由以下方程给出：
- en: '![Kernels and the kernel trick](img/8365OS_09_21.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![核函数与核技巧](img/8365OS_09_21.jpg)'
- en: Quadratic kernels, or polynomial kernels where *k* is equal to 2, are commonly
    used in natural language processing.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 二次核函数，或多项式核函数，其中 *k* 等于 2，通常用于自然语言处理。
- en: 'The sigmoid kernel is given by the following equation. ![Kernels and the kernel
    trick](img/8365OS_09_45.jpg) and ![Kernels and the kernel trick](img/8365OS_09_46.jpg)
    are hyperparameters that can be tuned through cross-validation:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 核函数由以下方程给出。![核函数与核技巧](img/8365OS_09_45.jpg) 和 ![核函数与核技巧](img/8365OS_09_46.jpg)
    是可以通过交叉验证调整的超参数：
- en: '![Kernels and the kernel trick](img/8365OS_09_22.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![核函数与核技巧](img/8365OS_09_22.jpg)'
- en: 'The Gaussian kernel is a good first choice for problems requiring nonlinear
    models. The Gaussian kernel is a **radial basis function**. A decision boundary
    that is a hyperplane in the mapped feature space is similar to a decision boundary
    that is a hypersphere in the original space. The feature space produced by the
    Gaussian kernel can have an infinite number of dimensions, a feat that would be
    impossible otherwise. The Gaussian kernel is given by the following equation:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯核是处理需要非线性模型问题的一个不错的选择。高斯核是一个**径向基函数**。在映射后的特征空间中，作为超平面的决策边界类似于原始空间中的超球面决策边界。高斯核产生的特征空间可以有无限多个维度，这是其他方法无法做到的。高斯核的表达式如下：
- en: '![Kernels and the kernel trick](img/8365OS_09_23.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![核函数与核技巧](img/8365OS_09_23.jpg)'
- en: '![Kernels and the kernel trick](img/8365OS_09_45.jpg) is a hyperparameter.
    It is always important to scale the features when using support vector machines,
    but feature scaling is especially important when using the Gaussian kernel.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![核函数与核技巧](img/8365OS_09_45.jpg)是一个超参数。在使用支持向量机时，特征缩放始终很重要，但在使用高斯核时，特征缩放尤其重要。'
- en: 'Choosing a kernel can be challenging. Ideally, a kernel will measure the similarity
    between instances in a way that is useful to the task. While kernels are commonly
    used with support vector machines, they can also be used with any model that can
    be expressed in terms of the dot product of two feature vectors, including logistic
    regression, perceptrons, and principal component analysis. In the next section,
    we will address the second problem caused by mapping to high-dimensional feature
    spaces: generalization.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个核函数可能具有挑战性。理想情况下，核函数将以一种对任务有用的方式来衡量实例之间的相似性。虽然核函数通常与支持向量机一起使用，但它们也可以与任何可以通过两个特征向量的点积表示的模型一起使用，包括逻辑回归、感知机和主成分分析。在下一节中，我们将讨论映射到高维特征空间所导致的第二个问题：泛化。
- en: Maximum margin classification and support vectors
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大间隔分类与支持向量
- en: The following figure depicts instances from two linearly separable classes and
    three possible decision boundaries. All of the decision boundaries separate the
    training instances of the positive class from the training instances of the negative
    class, and a perceptron could learn any of them. Which of these decision boundaries
    is most likely to perform best on test data?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了来自两个线性可分类的实例以及三个可能的决策边界。所有这些决策边界都将正类的训练实例与负类的训练实例分开，且感知机可以学习它们中的任何一个。哪个决策边界最有可能在测试数据上表现最佳？
- en: '![Maximum margin classification and support vectors](img/8365OS_09_24.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![最大间隔分类与支持向量](img/8365OS_09_24.jpg)'
- en: 'From this visualization, it is intuitive that the dotted decision boundary
    is the best. The solid decision boundary is near many of the positive instances.
    The test set could contain a positive instance that has a slightly smaller value
    for the first explanatory variable, ![Maximum margin classification and support
    vectors](img/8365OS_09_47.jpg); this instance would be classified incorrectly.
    The dashed decision boundary is farther away from most of the training instances;
    however, it is near one of the positive instances and one of the negative instances.
    The following figure provides a different perspective on evaluating decision boundaries:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个可视化中可以直观地看出，虚线决策边界是最优的。实线决策边界靠近许多正实例。测试集可能包含一个正实例，其第一个解释变量的值稍小，![最大间隔分类与支持向量](img/8365OS_09_47.jpg)；该实例将被错误分类。虚线决策边界远离大多数训练实例，但它靠近一个正实例和一个负实例。下图提供了评估决策边界的不同视角：
- en: '![Maximum margin classification and support vectors](img/8365OS_09_25.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![最大间隔分类与支持向量](img/8365OS_09_25.jpg)'
- en: 'Assume that the line plotted is the decision boundary for a logistic regression
    classifier. The instance labeled **A** is far from the decision boundary; it would
    be predicted to belong to the positive class with a high probability. The instance
    labeled **B** would still be predicted to belong to the positive class, but the
    probability would be lower as the instance is closer to the decision boundary.
    Finally, the instance labeled **C** would be predicted to belong to the positive
    class with a low probability; even a small change to the training data could change
    the class that is predicted. The most confident predictions are for the instances
    that are farthest from the decision boundary. We can estimate the confidence of
    the prediction using its **functional margin**. The functional margin of the training
    set is given by the following equations:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 假设所绘制的线是逻辑回归分类器的决策边界。标记为**A**的实例远离决策边界；它会被预测为属于正类，并且概率较高。标记为**B**的实例仍会被预测为属于正类，但由于该实例接近决策边界，概率会较低。最后，标记为**C**的实例会被预测为属于正类，但概率较低；即使训练数据有细微变化，也可能改变预测的类别。最有信心的预测是针对那些远离决策边界的实例。我们可以通过其**函数间隔**来估计预测的信心。训练集的函数间隔由以下方程给出：
- en: '![Maximum margin classification and support vectors](img/8365OS_09_26.jpg)![Maximum
    margin classification and support vectors](img/8365OS_09_01.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![最大间隔分类与支持向量](img/8365OS_09_26.jpg)![最大间隔分类与支持向量](img/8365OS_09_01.jpg)'
- en: 'In the preceding formulae ![Maximum margin classification and support vectors](img/8365OS_09_48.jpg)
    is the true class of the instance. The functional margin is large for instance
    **A** and small for instance **C**. If **C** were misclassified, the functional
    margin would be negative. The instances for which the functional margin is equal
    to one are called **support vectors**. These instances alone are sufficient to
    define the decision boundary; the other instances are not required to predict
    the class of a test instance. Related to the functional margin is the **geometric
    margin**, or the maximum width of the band that separates the support vectors.
    The geometric margin is equal to the normalized functional margin. It is necessary
    to normalize the functional margins as they can be scaled by using ![Maximum margin
    classification and support vectors](img/8365OS_09_49.jpg), which is problematic
    for training. When ![Maximum margin classification and support vectors](img/8365OS_09_49.jpg)
    is a unit vector, the geometric margin is equal to the functional vector. We can
    now formalize our definition of the best decision boundary as having the greatest
    geometric margin. The model parameters that maximize the geometric margin can
    be solved through the following constrained optimization problem:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述公式中，![最大间隔分类与支持向量](img/8365OS_09_48.jpg)是实例的真实类别。对于实例**A**，函数间隔较大，而对于实例**C**，函数间隔较小。如果**C**被错误分类，则函数间隔为负数。函数间隔等于1的实例被称为**支持向量**。这些实例足以定义决策边界；其他实例则不需要用于预测测试实例的类别。与函数间隔相关的是**几何间隔**，即分隔支持向量的带状区域的最大宽度。几何间隔等于标准化后的函数间隔。必须对函数间隔进行标准化，因为它们可以通过使用![最大间隔分类与支持向量](img/8365OS_09_49.jpg)来缩放，这对训练来说是有问题的。当![最大间隔分类与支持向量](img/8365OS_09_49.jpg)是单位向量时，几何间隔等于函数向量。我们现在可以将最佳决策边界的定义形式化为具有最大几何间隔。通过以下约束优化问题，可以求解最大化几何间隔的模型参数：
- en: '![Maximum margin classification and support vectors](img/8365OS_09_27.jpg)![Maximum
    margin classification and support vectors](img/8365OS_09_28.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![最大间隔分类与支持向量](img/8365OS_09_27.jpg)![最大间隔分类与支持向量](img/8365OS_09_28.jpg)'
- en: 'A useful property of support vector machines is that this optimization problem
    is convex; it has a single local minimum that is also the global minimum. While
    the proof is beyond the scope of this chapter, the previous optimization problem
    can be written using the dual form of the model to accommodate kernels as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机的一个有用特性是，该优化问题是凸的；它有一个单一的局部最小值，这也是全局最小值。虽然证明超出了本章的范围，但之前的优化问题可以通过模型的对偶形式来表示，以适应核方法，如下所示：
- en: '![Maximum margin classification and support vectors](img/8365OS_09_29.jpg)![Maximum
    margin classification and support vectors](img/8365OS_09_34.jpg)![Maximum margin
    classification and support vectors](img/8365OS_09_30.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![最大间隔分类和支持向量](img/8365OS_09_29.jpg)![最大间隔分类和支持向量](img/8365OS_09_34.jpg)![最大间隔分类和支持向量](img/8365OS_09_30.jpg)'
- en: Finding the parameters that maximize the geometric margin subject to the constraints
    that all of the positive instances have functional margins of at least 1 and all
    of the negative instances have functional margins of at most -1 is a quadratic
    programming problem. This problem is commonly solved using an algorithm called
    **Sequential Minimal Optimization** (**SMO**). The SMO algorithm breaks the optimization
    problem down into a series of the smallest possible subproblems, which are then
    solved analytically.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 找到最大化几何间隔的参数，前提是所有正实例的函数间隔至少为 1，所有负实例的函数间隔最多为 -1，这是一个二次规划问题。这个问题通常使用一种叫做 **序列最小优化**（**SMO**）的算法来解决。SMO
    算法将优化问题分解为一系列最小的子问题，然后通过解析方法解决这些子问题。
- en: Classifying characters in scikit-learn
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中分类字符
- en: 'Let''s apply support vector machines to a classification problem. In recent
    years, support vector machines have been used successfully in the task of character
    recognition. Given an image, the classifier must predict the character that is
    depicted. Character recognition is a component of many optical character-recognition
    systems. Even small images require high-dimensional representations when raw pixel
    intensities are used as features. If the classes are linearly inseparable and
    must be mapped to a higher-dimensional feature space, the dimensions of the feature
    space can become even larger. Fortunately, SVMs are suited to working with such
    data efficiently. First, we will use scikit-learn to train a support vector machine
    to recognize handwritten digits. Then, we will work on a more challenging problem:
    recognizing alphanumeric characters in photographs.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用支持向量机解决一个分类问题。近年来，支持向量机在字符识别任务中得到了成功应用。给定一张图像，分类器必须预测出图像中的字符。字符识别是许多光学字符识别系统中的一个组件。即便是较小的图像，在使用原始像素强度作为特征时，也需要高维表示。如果类别是线性不可分的，并且必须映射到更高维度的特征空间，特征空间的维度可能会变得更大。幸运的是，SVM
    适合高效地处理这种数据。首先，我们将使用 scikit-learn 来训练一个支持向量机以识别手写数字。接着，我们将处理一个更具挑战性的问题：在照片中识别字母数字字符。
- en: Classifying handwritten digits
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类手写数字
- en: 'The Mixed National Institute of Standards and Technology database is a collection
    of 70,000 images of handwritten digits. The digits were sampled from documents
    written by employees of the US Census Bureau and American high school students.
    The images are grayscale and 28 x 28 pixels in dimension. Let''s inspect some
    of the images using the following script:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 混合国家标准与技术研究所（MNIST）数据库包含 70,000 张手写数字图像。这些数字是从美国人口普查局的员工和美国高中生的手写文档中采样得到的。图像为灰度图，尺寸为
    28 x 28 像素。我们可以使用以下脚本查看其中的一些图像：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'First, we load the data. scikit-learn provides the `fetch_mldata` convenience
    function to download the data set if it is not found on disk, and read it into
    an object. Then, we create a subplot for five instances for the digits zero, one,
    and two. The script produces the following figure:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载数据。如果数据集未在磁盘上找到，scikit-learn 提供了 `fetch_mldata` 方便的函数来下载数据集并将其读入一个对象。然后，我们为数字零、数字一和数字二创建一个子图，显示五个实例。脚本将生成以下图形：
- en: '![Classifying handwritten digits](img/8365OS_09_32.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![分类手写数字](img/8365OS_09_32.jpg)'
- en: The MNIST data set is partitioned into a training set of 60,000 images and test
    set of 10,000 images. The dataset is commonly used to evaluate a variety of machine
    learning models; it is popular because little preprocessing is required. Let's
    use scikit-learn to build a classifier that can predict the digit depicted in
    an image.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集被划分为 60,000 张图像的训练集和 10,000 张图像的测试集。该数据集通常用于评估各种机器学习模型，之所以流行，是因为几乎不需要预处理。我们将使用
    scikit-learn 构建一个分类器，能够预测图像中展示的数字。
- en: 'First, we import the necessary classes:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的类：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The script will fork additional processes during grid search, which requires
    execution from a `__main_`_ block.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在网格搜索过程中，脚本会派生额外的进程，这要求从 `__main__` 块执行。
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we load the data using the `fetch_mldata` convenience function. We scale
    the features and center each feature around the origin. We then split the preprocessed
    data into training and test sets using the following line of code:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`fetch_mldata`便捷函数加载数据。我们对特征进行缩放，并将每个特征围绕原点居中。然后，我们使用以下代码行将预处理后的数据分割为训练集和测试集：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, we instantiate an `SVC`, or support vector classifier, object. This object
    exposes an API like that of scikit-learn's other estimators; the classifier is
    trained using the `fit` method, and predictions are made using the `predict` method.
    If you consult the documentation for `SVC`, you will find that the estimator requires
    more hyperparameters than most of the other estimators we discussed. It is common
    for more powerful estimators to require more hyperparameters. The most interesting
    hyperparameters for `SVC` are set by the `kernel`, `gamma`, and `C` keyword arguments.
    The `kernel` keyword argument specifies the kernel to be used. scikit-learn provides
    implementations of the linear, polynomial, sigmoid, and radial basis function
    kernels. The `degree` keyword argument should also be set when the polynomial
    kernel is used. `C` controls regularization; it is similar to the lambda hyperparameter
    we used for logistic regression. The keyword argument `gamma` is the kernel coefficient
    for the sigmoid, polynomial, and RBF kernels. Setting these hyperparameters can
    be challenging, so we tune them by grid searching with the following code.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实例化一个`SVC`，即支持向量分类器对象。该对象提供了类似于scikit-learn其他估算器的API；分类器使用`fit`方法进行训练，并通过`predict`方法进行预测。如果查看`SVC`的文档，你会发现该估算器需要的超参数比我们讨论的大多数其他估算器要多。通常，更强大的估算器需要更多的超参数。对于`SVC`来说，最有趣的超参数是通过`kernel`、`gamma`和`C`关键字参数设置的。`kernel`关键字参数指定要使用的核。scikit-learn提供了线性、多项式、sigmoid和径向基函数核的实现。当使用多项式核时，还应该设置`degree`关键字参数。`C`控制正则化，它类似于我们在逻辑回归中使用的lambda超参数。`gamma`关键字参数是sigmoid、多项式和RBF核的核系数。设置这些超参数可能会很具挑战性，因此我们通过网格搜索来调整它们，代码如下所示。
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following is the output of the preceding script:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述脚本的输出：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The best model has an average F1 score of 0.97; this score can be increased
    further by training on more than the first ten thousand instances.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的模型具有0.97的平均F1分数；通过在超过一万实例上进行训练，可以进一步提高此分数。
- en: Classifying characters in natural images
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然图像中的字符分类
- en: Now let's try a more challenging problem. We will classify alphanumeric characters
    in natural images. The Chars74K dataset, collected by T. E. de Campos, B. R. Babu,
    and M. Varma for *Character Recognition in Natural Images*, contains more than
    74,000 images of the digits zero through to nine and the characters for both cases
    of the English alphabet. The following are three examples of images of the lowercase
    letter `z`. Chars74K can be downloaded from [http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/](http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试一个更具挑战性的问题。我们将在自然图像中分类字母数字字符。Chars74K数据集由T. E. de Campos、B. R. Babu和M.
    Varma为*自然图像中的字符识别*收集，包含了超过74,000张数字0到9以及大写和小写字母的图像。以下是三张小写字母`z`的图像示例。Chars74K可以从[http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/](http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/)下载。
- en: '![Classifying characters in natural images](img/8365OS_09_33.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![自然图像中的字符分类](img/8365OS_09_33.jpg)'
- en: Several types of images comprise the collection. We will use 7,705 images of
    characters that were extracted from photographs of street scenes taken in Bangalore,
    India. In contrast to MNIST, the images in this portion of Chars74K depict the
    characters in a variety of fonts, colors, and perturbations. After expanding the
    archive, we will use the files in the `English/Img/GoodImg/Bmp/` directory. First
    we will import the necessary classes.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 收集的图像类型各异。我们将使用7,705张从印度班加罗尔街景照片中提取的字符图像。与MNIST相比，这部分Chars74K中的图像展示了各种字体、颜色和扰动。解压档案后，我们将使用`English/Img/GoodImg/Bmp/`目录中的文件。首先，我们将导入所需的类。
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next we will define a function that resizes images using the Python Image Library:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个使用Python图像库调整图像大小的函数：
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then we load will the images for each of the 62 classes and convert them to
    grayscale. Unlike MNIST, the images of Chars74K do not have consistent dimensions,
    so we will resize them to 30 pixels on a side using the resize_and_crop function
    we defined. Finally, we will convert the processed images to a NumPy array:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将加载每个 62 个类别的图像，并将它们转换为灰度图像。与 MNIST 不同，Chars74K 的图像尺寸不一致，因此我们将使用我们定义的 `resize_and_crop`
    函数将其调整为边长为 30 像素的大小。最后，我们将处理后的图像转换为 NumPy 数组：
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding script produces the following output:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上述脚本生成以下输出：
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It is apparent that this is a more challenging task than classifying digits
    in MNIST. The appearances of the characters vary more widely, the characters are
    perturbed more since the images were sampled from photographs rather than scanned
    documents. Furthermore, there are far fewer training instances for each class
    in Chars74K than there are in MNIST. The performance of the classifier could be
    improved by adding training data, preprocessing the images differently, or using
    more sophisticated feature representations.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这比在 MNIST 中分类数字要更加具有挑战性。字符的外观变化更为广泛，由于这些图像是从照片中采样的，而非扫描文档，字符也更容易受到扰动。此外，Chars74K
    中每个类别的训练实例远少于 MNIST 中的数量。通过增加训练数据、采用不同的图像预处理方法，或使用更复杂的特征表示，分类器的性能可以得到提升。
- en: Summary
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the support vector machine—a powerful model that
    can mitigate some of the limitations of perceptrons. The perceptron can be used
    effectively for linearly separable classification problems, but it cannot express
    more complex decision boundaries without expanding the feature space to higher
    dimensions. Unfortunately, this expansion is prone to computation and generalization
    problems. Support vector machines redress the first problem using kernels, which
    avoid explicitly computing the feature mapping. They redress the second problem
    by maximizing the margin between the decision boundary and the nearest instances.
    In the next chapter, we will discuss models called artificial neural networks,
    which, like support vector machines, extend the perceptron to overcome its limitations.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了支持向量机——一种强大的模型，可以缓解感知机的一些局限性。感知机可以有效地用于线性可分的分类问题，但它无法表示更复杂的决策边界，除非将特征空间扩展到更高的维度。不幸的是，这种扩展容易引发计算和泛化问题。支持向量机通过使用核函数来解决第一个问题，核函数避免了显式计算特征映射。它们通过最大化决策边界与最近实例之间的边距来解决第二个问题。在下一章中，我们将讨论人工神经网络模型，类似于支持向量机，这些模型通过扩展感知机来克服其局限性。
