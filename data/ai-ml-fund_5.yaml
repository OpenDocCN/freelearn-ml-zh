- en: '5'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '5'
- en: Using Trees for Predictive Analysis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用树进行预测分析
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Understand the metrics used for evaluating the utility of a data model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解用于评估数据模型效用度的指标
- en: Classify datapoints based on decision trees
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据决策树对数据点进行分类
- en: Classify datapoints based on the random forest algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据随机森林算法对数据点进行分类
- en: In this chapter, we will learn about two types of supervised learning algorithm
    in detail. The first algorithm will help us to classify data points using decision
    trees, while the other algorithm will help us classify using random forests.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细介绍两种监督学习算法。第一个算法将帮助我们使用决策树对数据点进行分类，而另一个算法将帮助我们使用随机森林进行分类。
- en: Introduction to Decision Trees
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树简介
- en: In decision trees, we have input and corresponding output in the training data.
    A decision tree, like any tree, has leaves, branches, and nodes. Leaves are the
    end nodes like a yes or no. Nodes are where a decision is taken. A decision tree
    consists of rules that we use to formulate a decision on the prediction of a data
    point.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树中，我们在训练数据中有输入和相应的输出。决策树，就像任何树一样，有叶子、分支和节点。叶子是像是是或否的终端节点。节点是做出决策的地方。决策树由我们用来对数据点的预测做出决策的规则组成。
- en: Every node of the decision tree represents a feature and every edge coming out
    of an internal node represents a possible value or a possible interval of values
    of the tree. Each leaf of the tree represents a label value of the tree.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的每个节点代表一个特征，每个从内部节点出来的边代表树的可能值或可能值的区间。树的每个叶子代表树的标签值。
- en: As we learned in the previous chapters, data points have features and labels.
    A task of a decision tree is to predict the label value based on fixed rules.
    The rules come from observing patterns on the training data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在前几章所学，数据点有特征和标签。决策树的任务是根据固定规则预测标签值。规则来自观察训练数据上的模式。
- en: Let's consider an example of determining the label values
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个确定标签值的例子
- en: 'Suppose the following training dataset is given. Formulate rules that help
    you determine the label value:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 假设以下训练数据集给出。制定规则以帮助你确定标签值：
- en: '![](img/Image00049.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/Image00049.jpg)'
- en: 'Figure 5.1: Dataset to formulate the rules'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.1：制定规则的数据库
- en: 'In this example, we predict the label value based on four features. To set
    up a decision tree, we have to make observations on the available data. Based
    on the data that''s available to us, we can conclude the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们根据四个特征预测标签值。为了设置决策树，我们必须对可用数据进行观察。基于我们可用的数据，我们可以得出以下结论：
- en: All house loans are determined as credit-worthy.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有房屋贷款都被认定为有信用资格。
- en: Studies loans are credit-worthy as long as the debtor is employed. If the debtor
    is not employed, he/she is not creditworthy.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只要债务人就业，助学贷款就有信用资格。如果债务人没有就业，他就/她没有信用资格。
- en: Loans are credit-worthy above 75,000/year income.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年收入超过75,000元的贷款有信用资格。
- en: At or below 75,000/year, car loans are credit-worthy whenever the debtor is
    not employed.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在75,000元/年或以下，如果债务人没有就业，汽车贷款就有信用资格。
- en: 'Depending on the order of how we take these rules into consideration, we can
    build a tree and describe one possible way of credit scoring. For instance, the
    following tree maps the preceding four rules:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们考虑这些规则的顺序，我们可以构建一棵树并描述一种可能的信用评分方式。例如，以下树映射了前面的四个规则：
- en: '![](img/Image00050.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/Image00050.jpg)'
- en: 'Figure 5.2: Decision Tree for the loan type'
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.2：贷款类型的决策树
- en: We first determine the loan type. House loans are automatically credit-worthy
    according to the first rule. Studies loans are described by the second rule, resulting
    in a subtree containing another decision on employment. As we have covered both
    house and studies loans, there are only car loans left. The third rule describes
    an income decision, while the fourth rule describes a decision on employment.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先确定贷款类型。根据第一条规则，房屋贷款自动具有信用资格。助学贷款由第二条规则描述，导致包含另一个关于就业决策的子树。因为我们已经涵盖了房屋和助学贷款，只剩下汽车贷款。第三条规则描述了收入决策，而第四条规则描述了就业决策。
- en: Whenever we have to score a new debtor to determine if he/she is credit-worthy,
    we have to go through the decision tree from top to bottom and observe the true
    or false value at the bottom.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们需要评分一个新债务人以确定他/她是否有信用资格时，我们必须从决策树顶部到底部进行遍历，并观察底部的真或假值。
- en: Obviously, a model based on seven data points is highly inaccurate, because
    we can generalize rules that simply do not match reality. Therefore, rules are
    often determined based on large amounts of data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，基于七个数据点的模型非常不准确，因为我们可以概括出一些根本不符合现实的规则。因此，规则通常是基于大量数据确定的。
- en: This is not the only way that we can create a decision tree. We can build decision
    trees based on other sequences of rules, too. Let's extract some other rules from
    the Dataset in Figure 5.1.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是创建决策树的唯一方法。我们也可以根据其他规则的序列构建决策树。让我们从图5.1中的数据集中提取一些其他规则。
- en: 'Observation 1: Notice that individual salaries that are strictly greater than
    75,000 are all credit-worthy. This means that we can classify four out of seven
    data points with one decision.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**观察1**：注意，严格大于75,000的个体工资都是有信用资格的。这意味着我们可以用一个决策将七个数据点中的四个进行分类。'
- en: '**Rule 1:** Income > 75,000 => CreditWorthy is true.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**规则1**：收入`> 75,000` => 有信用资格`is true`。'
- en: '**Rule 1** classifies four out of seven data points; we need more rules for
    the remaining three data points.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**规则1**将七个数据点中的四个进行了分类；我们需要为剩下的三个数据点制定更多规则。'
- en: '**Observation 2:** Out of the remaining three data points, two are not employed.
    One is employed and is not credit worthy. With a vague generalization, we can
    claim the following rule:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**观察2**：在剩下的三个数据点中，有两个不是有工作的。有一个是有工作但无信用资格的。通过一个模糊的概括，我们可以提出以下规则：'
- en: '**Rule 2:** Assuming Income <= 75,000, the following holds: Employed == true
    => CreditWorthy is false.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**规则2**：假设收入`<= 75,000`，以下成立：有工作`== true` => 有信用资格`is false`。'
- en: 'The first two rules classify five data points. Only two data points are left.
    We know that their income is less than or equal to 75,000 and that none of them
    are employed. There are some differences between them, though:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 前两条规则对五个数据点进行了分类。只剩下两个数据点。我们知道他们的收入小于或等于75,000，并且他们都没有工作。尽管如此，他们之间还是有一些差异：
- en: The credit-worthy person makes 75,000, while the non-credit-worthy person makes
    25,000.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有信用资格的人收入为75,000，而没有信用资格的人收入为25,000。
- en: The credit-worthy person took a car loan, while the non-credit-worthy person
    took a studies loan.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有信用资格的人借了汽车贷款，而没有信用资格的人借了学习贷款。
- en: The credit-worthy person took a loan of 30,000, while the non-credit-worthy
    person took a loan of 15,000
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有信用资格的人借了30,000元，而没有信用资格的人借了15,000元。
- en: Any of these differences can be extracted into a rule. For discrete ranges,
    such as car, studies, and house, the rule is a simple membership check. In the
    case of continuous ranges such as salary and loan amount, we need to determine
    a range to branch off.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这些差异中的任何一个都可以提取成规则。对于离散的范围，如汽车、学习和房屋，规则是一个简单的成员资格检查。在连续范围的情况下，如工资和贷款金额，我们需要确定一个范围来分支。
- en: Let's suppose that we chose the loan amount as a basis for our third rule.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们选择了贷款金额作为我们的第三条规则的依据。
- en: '**Rule 3:**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**规则3**：'
- en: Assuming `Income <= 75,000` and `Employed == false` ,
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 假设`Income <= 75,000`和`Employed == false`，
- en: If `LoanAmount <= AMOUNT`
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`LoanAmount <= AMOUNT`
- en: Then `CreditWorthy` is `false`
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后`CreditWorthy`为`false`。
- en: Else `CreditWorthy` is `true`
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 否则`CreditWorthy`为`true`。
- en: The first line describes the path that leads to this decision. The second line
    formulates the condition, and the last two lines describe the result.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行描述了导致这个决策的路径。第二行制定了条件，最后两行描述了结果。
- en: Notice that there is a constant AMOUNT in the rule. What should AMOUNT be equal
    to?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，规则中有一个恒定的数量。这个数量应该是多少？
- en: 'The answer is, any number is fine in the range 15,000 <= AMOUNT < 30,000\.
    We are free to select any number. In this example, we chose the bottom end of
    the range:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是，在15,000 <= AMOUNT < 30,000的范围内，任何数字都是可以的。我们可以自由选择任何数字。在这个例子中，我们选择了范围的低端：
- en: '![](img/Image00051.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Image00051.jpg)'
- en: 'Figure 5.3: Decision Tree for Income'
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.3：收入决策树
- en: The second decision tree is less complex. At the same time, we cannot overlook
    the fact that the model says, "higher loan amounts are more likely to be repaid
    than lower loan amounts." It is also hard to overlook the fact that employed people
    with a lower income never pay back their loans. Unfortunately, there is not enough
    training data available, which makes it likely that we end up with false conclusions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个决策树更简单。同时，我们不能忽视模型所说的，“较高的贷款金额比较低的贷款金额更有可能被偿还。”同样，我们也无法忽视这样一个事实，即收入较低的有工作的人从未偿还过他们的贷款。不幸的是，可用的训练数据不足，这使得我们最终得出错误结论的可能性很大。
- en: Overfitting is a frequent problem in decision trees when making a decision based
    on a few data points. This decision is rarely representative.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树中，基于少量数据点做出决策时，过拟合是一个常见问题。这个决策很少具有代表性。
- en: Since we can build decision trees in any possible order, it makes sense to define
    the desired way of algorithmically constructing a decision tree. Therefore, we
    will now explore a good measure for optimally ordering the features in the decision
    process.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以在任何可能的顺序中构建决策树，因此定义算法构建决策树所需的方式是有意义的。因此，我们现在将探索一个用于在决策过程中最优排序特征的良好度量。
- en: Entropy
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 熵
- en: In information theory, entropy measures how randomly distributed the possible
    values of an attribute are. The higher the degree of randomness, the higher the
    entropy of the attribute.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息论中，熵衡量一个属性的可能的值分布的随机性。随机性程度越高，属性的熵就越高。
- en: Entropy is the highest possibility of an event. If we know beforehand what the
    outcome will be then the event has no randomness. So, entropy is zero.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 熵是事件最高可能性。如果我们事先知道结果将会是什么，那么事件就没有随机性。因此，熵为零。
- en: When measuring the entropy of a system to be classified, we measure the entropy
    of the label.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在测量要分类的系统的熵时，我们测量标签的熵。
- en: 'Entropy is defined as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 熵被定义为如下：
- en: '`[v1, v2, ..., vn]` are the possible values of an attribute'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[v1, v2, ..., vn]` 是一个属性的可能的值'
- en: '`[p1, p2, ..., pn]` is the probability of these values occurring for that attribute
    assuming the values are equally distributed'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[p1, p2, ..., pn]` 是这些值对于该属性的假设概率，假设这些值是均匀分布的'
- en: '`p1 + p2 + ... + pn = 1`![](img/Image00052.jpg)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p1 + p2 + ... + pn = 1` ![图片 Image00052.jpg](img/Image00052.jpg)'
- en: 'Figure 5.4: Entropy Formula'
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.4：熵公式
- en: The symbol of entropy is H in information theory. Not because entropy has anything
    to do with the h sound, but because H is the symbol for the upper-case Greek letter
    eta. Eta is, the symbol of entropy.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息论中，熵的符号是 H。不是因为熵与 h 声音有什么关系，而是因为 H 是大写希腊字母 eta 的符号。Eta 是熵的符号。
- en: Note
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: We use entropy to order the nodes in the decision tree because the lower the
    entropy, the less randomly its values are distributed. The less randomness there
    is in the distribution, the more likely it is that the value of the label can
    be determined.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用熵对决策树中的节点进行排序，因为熵越低，其值的分布就越不随机。分布中的随机性越少，确定标签值的可能性就越大。
- en: 'To calculate the entropy of a distribution in Python, we have to use the NumPy
    library:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Python 中计算分布的熵，我们必须使用 NumPy 库：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The distribution is given as a NumPy array or a regular list. On line 2, you
    have to insert your own distribution in place of `[p1, p2, …, pn]` .
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 分布以 NumPy 数组或常规列表的形式给出。在第 2 行，你必须将你自己的分布 `[p1, p2, …, pn]` 插入其中。
- en: We need to create a vector of the negated values of the distribution in line
    3.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建一个包含第 3 行分布的负值的向量。
- en: On line 4, we must take the base 2 logarithm of each value in the distribution
    list
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 行，我们必须对分布列表中的每个值取以 2 为底的对数
- en: Finally, we calculate the sum with the scalar product, also known as the dot
    product of the two vectors.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过标量积（也称为两个向量的点积）来计算总和。
- en: 'Let''s define the preceding calculation in the form of a function:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将前面的计算以函数的形式定义：
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You first learned about the dot product in *Chapter 3* , *Regression* . The
    dot product of two vectors is calculated by multiplying the ith coordinate of
    the first vector by the ith coordinate of the second vector, for each i. Once
    we have all of the products, we sum the values:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先在 *第 3 章* ，*回归* 中了解了点积。两个向量的点积是通过将第一个向量的第 i 个坐标乘以第二个向量的第 i 个坐标来计算的，对于每个 i。一旦我们有了所有的乘积，我们求和这些值：
- en: '`np.dot([1, 2, 3], [4, 5, 6]) # 1*4 + 2*5 + 3*6 = 32`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`np.dot([1, 2, 3], [4, 5, 6]) # 1*4 + 2*5 + 3*6 = 32`'
- en: 'Exercise 15: Calculating the Entropy'
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 15：计算熵
- en: Calculate the entropy of the features in the dataset in *Figure 5.1* .
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 计算数据集 *图 5.1* 中特征的熵。
- en: We will calculate entropy for all features.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为所有特征计算熵。
- en: 'We have four features: **Employed** , **Income** , **LoanType** , and **LoanAmount**
    . For simplicity, we will treat the values in **Income** and **LoanAmount** as
    discrete values for now.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有四个特征：**Employed**，**Income**，**LoanType**，和 **LoanAmount**。为了简单起见，我们现在将 **Income**
    和 **LoanAmount** 中的值视为离散值。
- en: 'The following is the distribution of values for **Employed** :'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下表是 **Employed** 的值分布：
- en: '`true 4/7 times`'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`true 4/7 times`'
- en: '`false 3/7 times`'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`false 3/7 times`'
- en: 'Let''s use the entropy function to calculate the entropy of the Employed column:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用熵函数来计算 Employed 列的熵：
- en: '[PRE2]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The output is `0.9852` .
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果是 `0.9852`。
- en: 'The following is the distribution of values for **Income** :'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是对 **Income** 的值分布：
- en: '`25,000 1/7 times`'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`25,000 1/7 次`'
- en: '`75,000 2/7 times`'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`75,000 2/7 次`'
- en: '`80,000 1/7 times`'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`80,000 1/7 次`'
- en: '`100,000 2/7 times`'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`100,000 2/7 次`'
- en: '`125,000 1/7 times`'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`125,000 1/7 次`'
- en: 'Let''s use the entropy function to calculate the entropy of the Income column:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用熵函数来计算 Income 列的熵：
- en: '[PRE3]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output is `2.2359` .
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果是 `2.2359`。
- en: 'The following is the distribution of values for **LoanType** :'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是对 **LoanType** 的值分布：
- en: '`car 3/7 times`'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`car 3/7 次`'
- en: '`studies 2/7 times`'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`studies 2/7 次`'
- en: '`house 2/7 times`'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`house 2/7 次`'
- en: 'Let''s use the entropy function to calculate the entropy of the LoanType column:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用熵函数来计算 LoanType 列的熵：
- en: '[PRE4]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The output is `1.5567` .
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果是 `1.5567`。
- en: 'The following is the distribution of values for **LoanAmount** :'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是对 **LoanAmount** 的值分布：
- en: '`15,000 1/7 times`'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`15,000 1/7 次`'
- en: '`25,000 1/7 times`'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`25,000 1/7 次`'
- en: '`30,000 3/7 times`'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`30,000 3/7 次`'
- en: '`125,000 1/7 times`'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`125,000 1/7 次`'
- en: '`150,000 1/7 times`'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`150,000 1/7 次`'
- en: 'Let''s use the entropy function to calculate the entropy of the LoanAmount
    column:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用熵函数来计算 LoanAmount 列的熵：
- en: '[PRE5]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output is `2.1281` .
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果是 `2.1281`。
- en: As you can see, the closer the distribution is to the uniform distribution,
    the higher the entropy.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如您所见，分布越接近均匀分布，熵就越高。
- en: In this exercise, we were cheating a bit, because these are not the entropies
    that we will be using to construct the tree. In both trees, we had conditions
    like "greater than 75,000". We will therefore calculate the entropies belonging
    to the decision points we used in our original trees as well.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个练习中，我们有点作弊，因为这些不是我们将用于构建树的熵。在两个树中，我们都有像“大于 75,000”这样的条件。因此，我们将计算我们在原始树中使用的决策点的熵。
- en: The following is the distribution of values for **Income>75000**
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是对 **Income>75000** 的值分布：
- en: '`true 4/7 times`'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`true 4/7 次`'
- en: '`false 3/7 times`'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`false 3/7 次`'
- en: 'Let''s use the entropy function to calculate the entropy of the Income>75,000
    column:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用熵函数来计算 Income>75,000 列的熵：
- en: '[PRE6]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The output is `0.9852` .
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果是 `0.9852`。
- en: The following is the distribution of values for **LoanAmount>15000**
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是对 **LoanAmount>15000** 的值分布：
- en: '`true 6/7 times`'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`true 6/7 次`'
- en: '`false 1/7 times`'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`false 1/7 次`'
- en: Let's use the entropy function to calculate the entropy of the **LoanAmount**
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用熵函数来计算 **LoanAmount** 的熵：
- en: '>15,000 column:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '>15,000 列：'
- en: '[PRE7]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output is `0.5917` .
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果是 `0.5917`。
- en: Intuitively, the distribution [1] is the most deterministic distribution. This
    is because we know for a fact that there is 100% chance that the value of a feature
    stays fixed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，分布 [1] 是最确定的分布。这是因为我们知道一个事实，即特征值保持固定的概率是 100%。
- en: '`H([1]) = 1 * np.log2( 1 ) # 1*0 =0`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`H([1]) = 1 * np.log2( 1 ) # 1*0 =0`'
- en: We can conclude that the entropy of a distribution is strictly non-negative.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得出结论，分布的熵是严格非负的。
- en: Information Gain
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信息增益
- en: When we partition the data points in a dataset according to the values of an
    attribute, we reduce the entropy of the system.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们根据属性的值划分数据集中的数据点时，我们降低了系统的熵。
- en: 'To describe information gain, we can calculate the distribution of the labels.
    Initially, we have five credit-worthy and two not credit-worthy individuals in
    our dataset. The entropy belonging to the initial distribution is as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要描述信息增益，我们可以计算标签的分布。最初，在我们的数据集中有五个有信用和两个无信用的人。初始分布的熵如下：
- en: '[PRE8]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let's see what happens if we partition the dataset based on whether the loan
    amount is greater than 15,000 or not.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如果我们根据贷款金额是否大于 15,000 来划分数据集会发生什么。
- en: In group 1, we get one data point belonging to the 15,000 loan amount. This
    data point is not credit-worthy.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第 1 组中，我们得到一个属于 15,000 贷款金额的数据点。这个数据点无信用。
- en: In group 2, we have 5 credit-worthy and 1 non-credit-worthy individuals.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第 2 组中，我们有 5 个有信用和 1 个无信用的人。
- en: 'The entropy of the labels in each group is as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 每个组中标签的熵如下：
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To calculate the information gain, let''s calculate the weighted average of
    the group entropies:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算信息增益，让我们计算组熵的加权平均值：
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: When creating the decision tree, on each node, our job is to partition the dataset
    using a rule that maximizes the information gain.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建决策树时，在每个节点上，我们的任务是使用最大化信息增益的规则来划分数据集。
- en: We could also use Gini Impurity instead of entropy-based information gain to
    construct the best rules for splitting decision trees.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用 Gini 不纯度代替基于熵的信息增益来构建决策树的最佳分割规则。
- en: Gini Impurity
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Gini 不纯度
- en: 'Instead of entropy, there is another widely used metric that can be used to
    measure the randomness of a distribution: Gini Impurity.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 除了熵之外，还有一个广泛使用的指标可以用来衡量分布的随机性：Gini 不纯度。
- en: 'Gini Impurity is defined as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Gini 不纯度定义为如下：
- en: '![](img/Image00053.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/Image00053.jpg)'
- en: 'Fig 5.5: Gini Impurity'
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.5：Gini 不纯度
- en: 'For two variables, the Gini Impurity is:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两个变量，Gini 不纯度为：
- en: '![](img/Image00054.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/Image00054.jpg)'
- en: 'Fig 5.6: Gini Impurity for two variables'
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.6：两个变量的 Gini 不纯度
- en: Entropy may be a bit slower to calculate because of the usage of the logarithm.
    Gini Impurity, on the other hand, is less precise when it comes to measuring randomness.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用了对数，熵的计算可能稍微慢一些。另一方面，在衡量随机性方面，Gini 不纯度不够精确。
- en: Note
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Is information gain with entropy or Gini Impurity better for creating a decision
    tree?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益与熵或 Gini 不纯度哪一个更适合创建决策树？
- en: 'Some people prefer Gini Impurity, because you don''t have to calculate with
    logarithms. Computation-wise, none of the solutions are particularly complex,
    and so both of them can be used. When it comes to performance, the following study
    concluded that there are often just minimal differences between the two metrics:
    [https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf](https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf)
    .'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人更喜欢 Gini 不纯度，因为不需要计算对数。从计算的角度来看，这些解决方案都不特别复杂，因此两者都可以使用。在性能方面，以下研究得出结论，这两个指标之间通常只有微小的差异：[https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf](https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf)。
- en: We have learned that we can optimize a decision tree based on information gain
    or Gini Impurity. Unfortunately, these metrics are only available for discrete
    values. What if the label is defined on a continuous interval such as a price
    range or salary range?
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到我们可以根据信息增益或 Gini 不纯度来优化决策树。不幸的是，这些指标仅适用于离散值。如果标签定义在连续区间，比如价格范围或薪资范围，怎么办呢？
- en: We have to use other metrics. You can technically understand the idea behind
    creating a decision tree based on a continuous label, which was about regression.
    The metric we can reuse from this chapter is the mean squared error. Instead of
    Gini Impurity or information gain, we have to minimize the mean squared error
    to optimize the decision tree. As this is a beginner's book, we will omit this
    metric.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须使用其他指标。从技术上讲，你可以理解基于连续标签创建决策树的想法，这涉及到回归。我们可以从本章重用的指标是均方误差。我们不仅要最小化 Gini
    不纯度或信息增益，还要最小化均方误差来优化决策树。由于这是一本入门书籍，我们将省略此指标。
- en: Exit Condition
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 退出条件
- en: We can continuously split the data points according to rule values until each
    leaf of the decision tree has an entropy of zero. The question is whether this
    end state is desirable.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据规则值连续分割数据点，直到决策树的每个叶子节点熵为零。问题是这种最终状态是否可取。
- en: Often, this state is not desirable, because we risk overfitting the model. When
    our rules for the model are too specific and too nitpicky, and the sample size
    on which the decision was made is too small, we risk making a false conclusion,
    thus recognizing a pattern in the dataset that simply does not exist in real life.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这种状态并不理想，因为我们可能会过度拟合模型。当我们的模型规则过于具体和过于挑剔，并且决策所依据的样本量太小，我们可能会得出错误的结论，从而在数据集中识别出在现实生活中并不存在的模式。
- en: For instance, if we spin a roulette wheel three times and we get 12, 25, 12,
    concluding that every odd spin result in the value 12 is not a sensible strategy.
    By assuming that every odd spin equals 12, we find a rule that is exclusively
    due to random noise.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们旋转轮盘三次，得到 12、25、12，得出每次奇数次旋转的结果都是 12 的结论，这不是一个明智的策略。通过假设每次奇数次旋转等于 12，我们发现了一个仅由随机噪声产生的规则。
- en: Therefore, posing a restriction on the minimum size of the dataset that we can
    still split is an exit condition that works well in practice. For instance, if
    you stop splitting as soon as you have a dataset that's lower than 50, 100, 200,
    or 500 in size, you avoid drawing conclusions on random noise, and so you minimize
    the risk of overfitting the model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对我们可以分割的数据集的最小大小的限制是一个在实践中效果很好的退出条件。例如，如果你在数据集大小低于50、100、200或500时停止分割，你就可以避免在随机噪声上得出结论，从而最小化模型过拟合的风险。
- en: Another popular exit condition is a maximum restriction on the depth of the
    tree. Once we reach a fixed tree depth, we classify the data points in the leaves.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行的退出条件是对树深度的最大限制。一旦我们达到固定的树深度，我们就在叶子节点上对数据点进行分类。
- en: Building Decision Tree Classifiers using scikit-learn
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用scikit-learn构建决策树分类器
- en: We have already learned how to load data from a `.csv` file, how to apply preprocessing
    on the data, and how to split data into a training and testing dataset. If you
    need to refresh yourself on this knowledge, go back to previous chapters, where
    you go through this process in the context of regression and classification.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何从`.csv`文件加载数据，如何对数据进行预处理，以及如何将数据分割成训练集和测试集。如果你需要复习这方面的知识，请回到前面的章节，在那里你会在回归和分类的上下文中进行这个过程。
- en: We will now assume that a set of training features, training labels, testing
    features, and testing labels are given as a return value of the scikit-learn train-test-split
    call.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将假设一组训练特征、训练标签、测试特征和测试标签作为scikit-learn train-test-split调用的返回值。
- en: 'Notice that, in older versions of scikit-learn, you have to import cross_validation
    instead of model selection:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在scikit-learn的旧版本中，你必须导入cross_validation而不是model selection：
- en: '[PRE11]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We will not focus on how we got these data points, because the process is exactly
    the same as in the case of regression and classification.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会关注我们如何得到这些数据点，因为过程与回归和分类的情况完全相同。
- en: 'It''s time to import and use the decision tree classifier of scikit-learn:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候导入并使用scikit-learn的决策树分类器了：
- en: '[PRE12]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We set one optional parameter in the `DecisionTreeClassifier` , that is, `max_depth`
    , to limit the depth of the decision tree. You can read the official documentation
    for a full list of parameters: [http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)
    . Some of the more important parameters are as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`DecisionTreeClassifier`中设置了一个可选参数，即`max_depth`，以限制决策树的深度。你可以阅读官方文档以获取参数的完整列表：[http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)。一些更重要的参数如下：
- en: '**criterion** : Gini stands for Gini Impurity, while entropy stands for information
    gain.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**criterion**：Gini代表基尼不纯度，而entropy代表信息增益。'
- en: '**max_depth** : This is the maximum depth of the tree.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_depth**：这是树的最大深度。'
- en: '**min_samples_split** : This is the minimum number of samples needed to split
    an internal node.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**min_samples_split**：这是分割内部节点所需的最小样本数。'
- en: You can also experiment with all of the other parameters enumerated in the documentation.
    We will omit them in this topic.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在文档中列举的所有其他参数上进行实验。我们将在本主题中省略它们。
- en: 'Once the model has been built, we can use the decision tree classifier to predict
    data:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建完成后，我们可以使用决策树分类器来预测数据：
- en: '[PRE13]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You will build a decision tree classifier in the activity at the end of this
    topic.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在本主题末尾的活动构建一个决策树分类器。
- en: Evaluating the Performance of Classifiers
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估分类器的性能
- en: 'After splitting training and testing data, the decision tree model has a score
    method to evaluate how well testing data is classified by the model. We already
    learned how to use the score method in Chapters 3 and 4:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在分割训练和测试数据后，决策树模型有一个`score`方法来评估测试数据被模型分类得有多好。我们已经在第3章和第4章中学习了如何使用`score`方法：
- en: '[PRE14]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The return value of the score method is a number that's less than or equal to
    1\. The closer we get to 1, the better our model is.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`score`方法的返回值是一个小于或等于1的数字。我们越接近1，我们的模型就越好。'
- en: We will now learn another way to evaluate the model. Feel free to use this method
    on the models you constructed in the previous chapter as well.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将学习另一种评估模型的方法。你也可以在上一章中构建的模型上使用这种方法。
- en: 'Suppose we have one test feature and one test label:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个测试特征和一个测试标签：
- en: '[PRE15]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Suppose we are investigating a label value, positiveValue.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在调查一个标签值，positiveValue。
- en: 'We will use the following definitions to define some metrics that help you
    evaluate how good your classifier is:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下定义来定义一些指标，帮助您评估您的分类器有多好：
- en: '**Definition (True Positive)** : `positiveValue == predictedLabel == testLabel`'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义（真正阳性）**：`positiveValue == predictedLabel == testLabel`'
- en: '**Definition (True Negative)** : `positiveValue != predictedLabel == testLabel`'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义（真正阴性）**：`positiveValue != predictedLabel == testLabel`'
- en: '**Definition (False Positive)** : `positiveValue == predictedLabel != testLabel`'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义（假阳性）**：`positiveValue == predictedLabel != testLabel`'
- en: '**Definition (False Negative)** : `positiveValue != predictedLabel != testLabel`'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义（假阴性）**：`positiveValue != predictedLabel != testLabel`'
- en: A false positive is a prediction that is equal to the positive value, but the
    actual label in the test data is not equal to this positive value. For instance,
    in a tech interview, a false positive is an incompetent software developer who
    got hired because he acted in a convincing manner, hiding his complete lack of
    competence.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 假阳性是指预测结果等于正值，但测试数据中的实际标签并不等于这个正值。例如，在技术面试中，假阳性是指一个能力不足的软件开发者因为表现得很令人信服而被录用，隐藏了他完全缺乏的能力。
- en: Don't confuse a false positive with a false negative. Using the tech interview
    example, a false negative is a software developer who was competent enough to
    do the job, but he did not get hired.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 不要将假阳性与假阴性混淆。使用技术面试的例子，假阴性是指一个有能力完成工作的软件开发者，但他没有得到录用。
- en: 'Using the preceding four definitions, we can define three metrics that describe
    how well our model predicts reality. The symbol #( X ) denotes the number of values
    in X. Using technical terms, #( X ) denotes the cardinality of X:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '使用前面的四个定义，我们可以定义三个指标来描述我们的模型如何预测现实。符号 #( X ) 表示 X 中值的数量。使用技术术语，#( X ) 表示 X
    的基数：'
- en: '**Definition (Precision):**'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义（精度）**：'
- en: '`#( True Positives ) / (#( True Positives ) + #( False Positives ))`'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`#( 真正值 ) / (#( 真正值 ) + #( 假阳性值 ))`'
- en: '**Definition (Recall):**'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义（召回率）：**'
- en: '`#( True Positives ) / (#( True Positives ) + #( False Negatives ))`'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`#( 真正值 ) / (#( 真正值 ) + #( 假阴性值 ))`'
- en: Precision centers around values that our classifier found to be positive. Some
    of these results are true positive, while others are false positive. A high precision
    means that the number of false positive results are very low compared to true
    positive results. This means that a precise classifier rarely makes a mistake
    when finding a positive result.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 精度关注的是我们的分类器找到的正值。其中一些结果是真正的正值，而其他结果是假阳性。高精度意味着与真正的正值相比，假阳性结果的数量非常低。这意味着一个精确的分类器在寻找正值时很少出错。
- en: Recall that centers around values are positive among the test data. Some of
    these results are found by the classifier. These are the true positive values.
    Those positive values that are not found by the classifier are false negatives.
    A classifier with a high recall value finds most of the positive values.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率关注的是测试数据中值为正的部分。其中一些结果是由分类器找到的。这些是真正的正值。那些未被分类器找到的正值是假阴性。具有高召回率的分类器能找到大多数正值。
- en: 'Exercise 16: Precision and Recall'
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 16：精度和召回率
- en: 'Find the precision and the recall value of the following two classifiers:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 找出以下两个分类器的精度和召回率值：
- en: '[PRE16]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'According to the formula let''s calculate the number of true positives, false
    positives, and false negatives for classifier 1:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据公式，让我们计算分类器1中真正的正值、假阳性和假阴性的数量：
- en: '[PRE17]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The first classifier has excellent precision, but bad recall. Let's calculate
    the same for the second classifier.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个分类器具有出色的精度，但召回率不佳。让我们为第二个分类器计算相同的结果。
- en: '[PRE18]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The second classifier has excellent recall, but its precision is not perfect.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个分类器具有出色的召回率，但其精度并不完美。
- en: The F1 score is the harmonic mean of precision and recall. Its value ranges
    between 0 and 1\. The advantage of the F1 score is that it considers both false
    positives and false negatives.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: F1分数是精度和召回率的调和平均值。其值介于0和1之间。F1分数的优势在于它考虑了假阳性和假阴性。
- en: 'Exercise 17: Calculating the F1 Score'
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 17：计算F1分数
- en: 'Calculate the F1 Score of the two classifiers from the previous exercise:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 计算前一个练习中两个分类器的F1分数：
- en: 'The formula for calculating the F1 Score is as follows:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算F1分数的公式如下：
- en: '[PRE19]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The first classifier has the following F1 Score:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个分类器的F1分数如下：
- en: '[PRE20]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The second classifier has the following F1 Score:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个分类器的F1分数如下：
- en: '[PRE21]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now that we know what precision, recall, and F1 score mean, let''s use a scikit-learn
    utility to calculate and print these values:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了精确率、召回率和F1分数的含义，让我们使用scikit-learn实用工具来计算并打印这些值：
- en: '[PRE22]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output will be as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果将如下所示：
- en: '[PRE23]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In this example, there are four possible label values, denoted by 0, 1, 2, and
    3\. In each row, you get a precision, recall, and F1 score value belonging to
    each possible label value. You can also see in the support column how many of
    these label values exist in the dataset. The last row contains an aggregated precision,
    recall, and f1-score.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，有四个可能的标签值，分别用0、1、2和3表示。在每一行中，你得到属于每个可能标签值的精确率、召回率和F1分数值。你还可以在支持列中看到这些标签值在数据集中存在的数量。最后一行包含汇总的精确率、召回率和f1分数。
- en: 'If you used label encoding to encode string labels to numbers, you may want
    to perform an inverse transformation to find out which row belongs to which label.
    In the following example, Class is the name of the label, and `labelEncoders[''Class'']`
    is the label encoder belonging to the Class label:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用标签编码将字符串标签编码为数字，你可能想要执行逆变换以找出哪些行属于哪个标签。在以下示例中，Class 是标签的名称，而 `labelEncoders['Class']`
    是属于 Class 标签的标签编码器：
- en: '[PRE24]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If you prefer calculating the precision, recall, and F1 Score on its own, you
    can use individual calls. Note that in the next example, we will call each score
    function twice: once with `average=None` , and once with `average=''weighted''`
    .'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更喜欢单独计算精确率、召回率和F1分数，你可以使用单独的调用。请注意，在下一个示例中，我们将对每个分数函数调用两次：一次使用 `average=None`
    ，另一次使用 `average='weighted'` 。
- en: When the average is specified as None, we get the score value belonging to each
    possible label value. You can see the same values rounded in the table if you
    compare the results to the first four values of the corresponding column.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当平均指定为None时，我们得到属于每个可能标签值的分数值。如果你将结果与对应列的前四个值进行比较，你可以看到相同的值四舍五入在表中。
- en: 'When the average is specified as weighted, you get the cell value belonging
    to the column of the score name and the avg/total row:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 当平均指定为加权时，你得到属于分数名称列的单元格值和 avg/total 行：
- en: '[PRE25]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Calculating the precision score with no average can be done like so:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 使用无平均计算精确率分数可以这样进行：
- en: '[PRE26]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE27]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Calculating the precision score with a weighted average can be done like so:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 使用加权平均计算精确率分数可以这样进行：
- en: '[PRE28]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The output is `0.989402697495183` .
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为 `0.989402697495183` 。
- en: 'Calculating the recall score with no average can be done like so:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 使用无平均计算召回率分数可以这样进行：
- en: '[PRE29]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output is as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE30]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Calculating the recall score with a weighted average can be done like so:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 使用加权平均计算召回率分数可以这样进行：
- en: '[PRE31]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The output is `0.9884393063583815` .
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为 `0.9884393063583815` 。
- en: 'Calculating the f1_score with no average can be done like so:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 使用无平均计算f1_score可以这样进行：
- en: '[PRE32]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output is as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE33]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Calculating the f1_score with a weighted average can be done like so:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 使用加权平均计算f1_score可以这样进行：
- en: '[PRE34]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The output is `0.988690625785373` .
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为 `0.988690625785373` 。
- en: 'There is one more score worth investigating: the accuracy score. Suppose #(
    Dataset ) denotes the length of the total dataset, or in other words, the sum
    of true positives, true negatives, false positives, and false negatives.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '值得进一步研究的一个分数是准确率分数。假设 #( Dataset ) 表示总数据集的长度，换句话说，是真正例、真负例、假正例和假负例的总和。'
- en: 'Accuracy is defined as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率的定义如下：
- en: '**Definition (Accuracy)** : #( True Positives ) + #( True Negatives ) / #(
    Dataset )'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义（准确率）**： #( 真正例 ) + #( 真负例 ) / #( 数据集 )'
- en: 'Accuracy is a metric that''s used for determining how many times the classifier
    gives us the correct answer. This is the first metric we used to evaluate the
    score of a classifier. Whenever we called the score method of a classifier model,
    we calculated its accuracy:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是一个用于确定分类器给出正确答案次数的指标。这是我们用来评估分类器分数的第一个指标。每次我们调用分类器模型的分数方法时，我们都计算其准确率：
- en: '[PRE35]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The output is `0.9884393063583815` .
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为 `0.9884393063583815` 。
- en: 'Calculating the decision tree score can be done like so:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 计算决策树分数可以这样进行：
- en: '[PRE36]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The output is `0.9884393063583815` .
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为 `0.9884393063583815` 。
- en: Confusion Matrix
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: 'We will conclude this topic with one data structure that helps you evaluate
    the performance of a classification model: the confusion matrix.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用一个有助于评估分类模型性能的数据结构来结束这个主题：混淆矩阵。
- en: A confusion matrix is a square matrix, where the number of rows and columns
    equals the number of distinct label values. In the columns of the matrix, we place
    each test label value. In the rows of the matrix, we place each predicted label
    value.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是一个方阵，其中行数和列数等于不同标签值的数量。在矩阵的列中，我们放置每个测试标签值。在矩阵的行中，我们放置每个预测标签值。
- en: For each data point, we add one to the corresponding cells of the confusion
    matrix based on the predicted and actual label value.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个数据点，我们根据预测的实际标签值，将其加到混淆矩阵的相应单元格中。
- en: 'Exercise 18: Confusion Matrix'
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习18：混淆矩阵
- en: 'Construct the confusion matrix of the following two distributions:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 构建以下两个分布的混淆矩阵：
- en: '[PRE37]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We will start with the first classifier. The columns determine the place of
    the test labels, while the rows determine the place of the predicted labels. The
    first entry is `TestLabels1[0]` and `PredictedLabels1[0]` . The values are true
    and true, and so we add 1 to the top-left column.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从第一个分类器开始。列确定测试标签的位置，而行确定预测标签的位置。第一个条目是`TestLabels1[0]`和`PredictedLabels1[0]`。这些值都是真实的，所以我们将其加到左上角的列中。
- en: The second values are `TestLabels1[1] = True` and `PredictedLabels1[1] = False.`
    These values determine the bottom-left cell of the 2x2 matrix.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个值是`TestLabels1[1] = True`和`PredictedLabels1[1] = False`。这些值决定了2x2矩阵的左下角单元格。
- en: 'After finishing the placement of all five label pairs, we get the following
    confusion matrix:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在完成所有五个标签对的放置后，我们得到以下混淆矩阵：
- en: '[PRE38]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'After finishing the placement of all five label pairs, we get the following
    confusion matrix:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在完成所有五个标签对的放置后，我们得到以下混淆矩阵：
- en: '[PRE39]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In a 2x2 matrix, we have the following distribution:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个2x2矩阵中，我们有以下分布：
- en: '[PRE40]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The confusion matrix can be used to calculate precision, recall, accuracy, and
    f1_score metrics. The calculation is straightforward and is implied by the definitions
    of the metrics.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混淆矩阵可以用来计算精确率、召回率、准确率和f1_score指标。计算是直接的，并且由指标的定义隐含。
- en: 'The confusion matrix can be calculated by scikit-learn:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混淆矩阵可以通过scikit-learn计算：
- en: '[PRE41]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note that this is not the same example as the one we used in the previous section.
    Therefore, if you use the values inside the confusion matrix, you will get different
    precision, recall, and f1_score values.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，这并不是我们在上一节中使用过的相同示例。因此，如果你使用混淆矩阵中的值，你将得到不同的精确率、召回率和f1_score值。
- en: 'You can also use pandas to create the confusion matrix:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你也可以使用pandas创建混淆矩阵：
- en: '[PRE42]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let''s verify these values by calculating the accuracy score of the model:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过计算模型的准确率来验证这些值：
- en: We have 127 + 25 = 152 data points that were classified correctly.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有127 + 25 = 152个被正确分类的数据点。
- en: The total number of data points is 152 + 11 + 5 + 5 = 173.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据点的总数是152 + 11 + 5 + 5 = 173。
- en: 152/173 is 0.8786127167630058.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 152/173是0.8786127167630058。
- en: 'Let''s calculate the accuracy score by using the scikit-learn utility we used
    before:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用之前使用的scikit-learn工具来计算准确率：
- en: '[PRE43]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output is as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE44]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We got the same value. All of the metrics can be derived from the confusion
    matrix.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了相同的价值。所有指标都可以从混淆矩阵中推导出来。
- en: 'Activity 10: Car Data Classification'
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动10：汽车数据分类
- en: 'In this section, we will discuss how to build a reliable decision tree model
    that''s capable of aiding your company in finding cars that clients are likely
    to buy. We will be assuming that you are employed by a car rental agency who''s
    focusing on building a lasting relationship with its clients. Your task is to
    build a decision tree model that classifies cars into one of four categories:
    unacceptable, acceptable, good, and very good.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何构建一个可靠的决策树模型，该模型能够帮助你的公司在寻找客户可能购买的汽车方面发挥作用。我们将假设你受雇于一家汽车租赁代理机构，该机构专注于与客户建立长期关系。你的任务是构建一个决策树模型，将汽车分类为以下四个类别之一：不可接受、可接受、良好和非常好。
- en: 'The dataset for this activity can be accessed here: [https://archive.ics.uci.edu/ml/datasets/Car+Evaluation](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation)
    . Click the Data Folder link to download the dataset. Then, click the Dataset
    Description link to access the description of the attributes.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的数据集可以在此处访问：[https://archive.ics.uci.edu/ml/datasets/Car+Evaluation](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation)。点击数据文件夹链接下载数据集。然后，点击数据集描述链接以访问属性描述。
- en: 'Let''s evaluate the utility of your decision tree model:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来评估你的决策树模型的效用：
- en: 'Download the car data file from here: [https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data](https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data)
    . Add a header line to the front of the CSV file so that you can reference it
    in Python easily. We simply call the label Class. We named the six features after
    their descriptions in [https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.names](https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.names)
    .'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这里下载汽车数据文件：[https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data](https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data)。在CSV文件的前面添加一个标题行，这样你就可以在Python中轻松引用它。我们简单地将标签称为Class。我们根据[https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.names](https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.names)中的描述命名了六个特征。
- en: Load the dataset into Python and check if it has loaded properly.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集加载到Python中，并检查是否正确加载。
- en: It's time to separate the training and testing data with the cross-validation
    (in newer versions, this is model-selection) feature of scikit-learn. We will
    use 10% test data.
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是时候使用scikit-learn的交叉验证（在新版本中，这是模型选择）功能来分离训练数据和测试数据了。我们将使用10%的测试数据。
- en: Note that the `train_test_split` method will be available in the `model_selection`
    module, and not in the `cross_validation` module, starting from scikit-learn 0.20\.
    In previous versions, `model_selection` already contains the `train_test_split`
    method.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，从scikit-learn 0.20版本开始，`train_test_split`方法将在`model_selection`模块中可用，而不是在`cross_validation`模块中。在之前的版本中，`model_selection`已经包含了`train_test_split`方法。
- en: Build the decision tree classifier.
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 构建决策树分类器。
- en: Check the score of our model based on the test data.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查基于测试数据的模型得分。
- en: Create a deeper evaluation of the model based on the `classification_report`
    feature.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于`classification_report`功能创建对模型的更深入评估。
- en: Note
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity is available at page 282.
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该活动的解决方案可在第282页找到。
- en: Random Forest Classifier
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林分类器
- en: 'If you think about the name Random forest classifier, it makes sense to conclude
    the following:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑随机森林分类器的名字，可以得出以下结论：
- en: A forest consists of multiple trees.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 森林由多棵树组成。
- en: These trees can be used for classification.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些树可以用于分类。
- en: Since the only tree we have used so far for classification is a decision tree,
    it makes sense that the random forest is a forest of decision trees.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们迄今为止用于分类的唯一树是决策树，因此随机森林是决策树的森林是有意义的。
- en: The random nature of the trees means that our decision trees are constructed
    in a randomized manner.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树的随机性意味着我们的决策树是以随机方式构建的。
- en: As a consequence, we will base our decision tree construction on information
    gain or Gini Impurity.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，我们将基于信息增益或基尼不纯度构建决策树。
- en: Once you understand these basic concepts, you essentially know what a Random
    forest classifier is all about. The more trees you have in the forest, the more
    accurate prediction is going to be. When performing prediction, each tree performs
    classification. We collect the results, and the class that gets the most votes
    wins.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你理解了这些基本概念，你本质上就知道了随机森林分类器是什么。森林中的树越多，预测的准确性就越高。在执行预测时，每棵树都会进行分类。我们收集结果，得票最多的类别获胜。
- en: Random forests can be used for regression as well as for classification. When
    using random forests for regression, instead of counting the most votes for a
    class, we take the average of the arithmetic mean (average) of the prediction
    results and return it. Random forests are not as ideal for regression as they
    are for classification, though, because the models used to predict values are
    often out of control, and often return a wide range of values. The average of
    these values is often not too meaningful. Managing the noise in a regression exercise
    is harder than in classification.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林既可以用于回归，也可以用于分类。当使用随机森林进行回归时，我们不是对类别的投票进行计数，而是取预测结果算术平均值的平均值（平均）并返回它。尽管如此，随机森林在回归方面并不像在分类方面那样理想，因为用于预测值的模型往往失控，并且经常返回一个很宽的范围的值。这些值的平均值通常并不太有意义。在回归练习中管理噪声比在分类中更难。
- en: Random forests are often better than one simple decision tree because they provide
    redundancy. They treat outlier values better and have a lower probability of overfitting
    the model. Decision trees seem to behave great as long as you are using them on
    data that you used when creating the model. Once you use them to predict new data,
    random forests lose their edge. Random forests are widely used for classification
    problems, whether it be customer segmentation for banks or e-commerce, classifying
    images, or medicine. If you own an Xbox with Kinect, your Kinect device contains
    a random forest classifier to detect your body parts.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林通常比一个简单的决策树更好，因为它们提供了冗余。它们处理异常值更好，并且有更低的模型过拟合概率。只要你在创建模型时使用的数据上使用决策树，它们似乎表现得很好。一旦你使用它们来预测新数据，随机森林就会失去优势。随机森林被广泛用于分类问题，无论是银行的客户细分、电子商务、图像分类还是医学。如果你拥有一台带有
    Kinect 的 Xbox，你的 Kinect 设备中就包含一个用于检测你身体部位的随机森林分类器。
- en: Random Forest Classification and regression are ensemble algorithms. The idea
    behind ensemble learning is that we take an aggregated view over a decision of
    multiple agents that potentially have different weaknesses. Due to the aggregated
    vote, these weaknesses cancel out, and the majority vote likely represents the
    correct result.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林分类和回归是集成算法。集成学习的理念是我们对多个潜在具有不同弱点的代理人的决策进行聚合视图。由于聚合投票，这些弱点相互抵消，多数投票很可能代表正确的结果。
- en: Constructing a Random Forest
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建随机森林
- en: One way to construct the trees of a random forest is to limit the number of
    features used in the classification task. Suppose you have a feature set, F. The
    length of the feature set is `#(F)` . The number of features in the feature set
    is `dim(F)` , where dim stands for dimension.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 构建随机森林的树的一种方法是在分类任务中限制使用的特征数量。假设你有一个特征集，F。特征集的长度是 `#(F)`。特征集中的特征数量是 `dim(F)`，其中
    dim 表示维度。
- en: 'Suppose we limit the training data to a different subset of size *s < #(F)*
    , and each random forest receives a different training data set of size s. Suppose
    we specify that we will use *k < dim(F)* features out of the possible features
    to construct a tree in the random forest. The selection of k features is chosen
    at random.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '假设我们将训练数据限制为大小为 *s < #(F)* 的不同子集，并且每个随机森林接收一个大小为 s 的不同训练数据集。假设我们指定我们将使用 *k
    < dim(F)* 个可能的特征来构建随机森林中的树。k 个特征的选取是随机的。'
- en: We construct each decision tree completely. Once we get a new data po int to
    classify, we execute each tree in the random forest to perform the prediction.
    Once the prediction results are in, we count the votes, and the most voted class
    is going to be the class of the data point predicted by the random forest.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完全构建每个决策树。一旦我们得到一个新的数据点进行分类，我们就执行随机森林中的每个树来进行预测。一旦预测结果出来，我们统计投票，得票最多的类别将成为随机森林预测的数据点的类别。
- en: 'In random forest terminology, we describe the performance benefits of random
    forests with one word: bagging. Bagging is a technique that consists of bootstrapping
    and using aggregated decision making. Bootstrapping is responsible for creating
    a dataset that contains a subset of the entries of the original dataset. The size
    of the original dataset and the bootstrapped dataset is still the same because
    we are allowed to select the same data points multiple times in the bootstrapped
    dataset.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林术语中，我们用一个词来描述随机森林的性能优势：袋装法。袋装法是一种由自助法和聚合决策组成的技巧。自助法负责创建一个包含原始数据集条目子集的数据集。原始数据集和自助数据集的大小仍然相同，因为我们允许在自助数据集中多次选择相同的数据点。
- en: Out of bag data points are ones that don't end up in some bootstrapped datasets.
    To measure the out of bag error of a random forest classifier, we have to run
    all out of bag data points on trees of the random forest classifier that were
    built without considering the out of bag data points. The margin of error is the
    ratio between correctly classified out of bag data points and all out of bag data
    points.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 出袋数据点是那些最终没有出现在某些自助数据集中的数据点。为了衡量随机森林分类器的出袋误差，我们必须在未考虑出袋数据点的随机森林分类器的树上运行所有出袋数据点。误差范围是正确分类的出袋数据点与所有出袋数据点之间的比率。
- en: Random Forest Classification Using scikit-learn
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 进行随机森林分类
- en: 'Our starting point is the result of the train-test splitting:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的起点是训练-测试分割的结果：
- en: '[PRE45]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The random forest classifier can be implemented as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林分类器可以如下实现：
- en: '[PRE46]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The interface of scikit-learn makes it easy to handle the random forest classifier.
    Throughout the last three chapters, we have already gotten used to this way of
    calling a classifier or a regression model for prediction.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的接口使得处理随机森林分类器变得容易。在整个前三章中，我们已经习惯了这种调用分类器或回归模型进行预测的方式。
- en: Parameterization of the random forest classifier
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林分类器的参数化
- en: 'As usual, consult the documentation for the full list of parameters. You can
    find the documentation here: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，请查阅完整参数列表的文档。您可以在以下位置找到文档：http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier。
- en: 'We will only consider a subset of the possible parameters, based on what you
    already know, which is based on the description of constructing random forests:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只考虑可能参数的一个子集，这是基于您已经了解的内容，即基于构建随机森林的描述：
- en: '**n_estimators** : The number of trees in the random forest. The default value
    is 10.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**n_estimators**：随机森林中的树的数量。默认值为10。'
- en: '**criterion** : Use Gini or entropy to determine whether you use Gini Impurity
    or information gain using entropy in each tree.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**criterion**：使用Gini或熵来确定是否在每个树中使用Gini不纯度或使用熵的信息增益。'
- en: '**max_features** : The maximum number of features considered in any tree of
    the forest. Possible values include an integer. You can also add some strings
    such as "`sqrt` " for the square root of the number of features.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_features**：森林中任何树中考虑的最大特征数。可能的值包括一个整数。您还可以添加一些字符串，如"`sqrt`"，表示特征数的平方根。'
- en: '**max_depth** : The maximum depth of each tree.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_depth**：每棵树的最大深度。'
- en: '**min_samples_split** : The minimum number of samples in the dataset in a given
    node to perform a split. This may also reduce the tree''s size.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**min_samples_split**：在给定节点中，数据集中样本的最小数量，以执行分割。这也可能减少树的大小。'
- en: '**bootstrap** : A Boolean indicating whether to use bootstrapping on data points
    when constructing trees.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**bootstrap**：一个布尔值，表示在构建树时是否对数据点使用自助法。'
- en: Feature Importance
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征重要性
- en: A random forest classifier gives you information on how important each feature
    in data classification is. Remember, we use a lot of randomly constructed decision
    trees to classify data points. We can measure how accurately these data points
    behave, and we can also see which features are vital in decision making.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林分类器会告诉你数据分类中每个特征的重要性。记住，我们使用大量的随机构造的决策树来分类数据点。我们可以测量这些数据点的行为准确性，我们还可以看到哪些特征在决策中至关重要。
- en: 'We can retrieve the array of feature importance scores with the following query:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下查询检索特征重要性分数数组：
- en: '[PRE47]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output is as follows:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE48]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In this six-feature classifier, the fourth and the sixth features are clearly
    a lot more important than any other features. The third feature has a very low
    importance score.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个六特征分类器中，第四和第六个特征显然比其他任何特征都重要得多。第三个特征的重要性得分非常低。
- en: Feature importance scores come in handy when we have a lot of features and we
    want to reduce the feature size to avoid the classifier getting lost in the details.
    When we have a lot of features, we risk overfitting the model. Therefore, reducing
    the number of features by dropping the least significant ones is often helpful.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有很多特征并且想要减少特征大小以避免分类器陷入细节时，特征重要性分数很有用。当我们有很多特征时，我们可能会过度拟合模型。因此，通过删除最不显著的特征来减少特征数量通常是有帮助的。
- en: Extremely Randomized Trees
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 极端随机树
- en: Extremely randomized trees increase randomization inside random forests by randomizing
    the splitting rules on top of the already randomized factors in random forests.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 极端随机树通过在随机森林中随机化分割规则来增加随机化，这些规则是在随机森林中已经随机化的因素之上。
- en: 'The parameterization is similar to the random forest classifier. You can see
    the full list of parameters here: [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)
    .'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 参数化与随机森林分类器相似。您可以在以下位置查看完整参数列表：[http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)。
- en: 'The Python implementation is as follows:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: Python实现如下：
- en: '[PRE49]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Activity 11: Random Forest Classification for Your Car Rental Company'
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动11：为您的租车公司进行随机森林分类
- en: 'In this section, we will optimize your classifier so that you satisfy your
    clients more when selecting future cars for your car fleet. We will be performing
    random forest and Extreme random forest classification on the car dealership dataset
    that you worked on in previous activity of this chapter. Suggest further improvements
    for the model to improve the performance of the classifier:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将优化你的分类器，以便在选择未来的车队车辆时更能满足客户的需求。我们将对你在本章前一个活动中工作的汽车经销商数据集执行随机森林和超随机树分类。为提高分类器的性能，提出对模型的进一步改进建议：
- en: Follow steps 1 to 5 of previous activity.
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照前一个活动的步骤1到5进行。
- en: If you are using `IPython` , your variables may already be accessible in your
    console.
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你使用的是`IPython`，你的变量可能已经在你的控制台中可访问。
- en: Create a random forest and an extremely randomized trees classifier and train
    the models.
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个随机森林和一个超随机树分类器，并训练模型。
- en: Estimate how well the two models perform on the test data. We can also calculate
    the accuracy scores.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 估计两个模型在测试数据上的表现如何。我们还可以计算准确率评分。
- en: As a first optimization technique, let's see which features more important and
    which features are less important. Due to randomization, removing the least important
    features may reduce the random noise in the model.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为一种首次优化技术，让我们看看哪些特征更重要，哪些特征不太重要。由于随机化，移除最不重要的特征可能会减少模型中的随机噪声。
- en: Remove the third feature from the model and retrain the classifier. Compare
    how well the new models fare compared to the original ones.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从模型中移除第三个特征并重新训练分类器。比较新模型与原始模型的表现如何。
- en: Tweak the parameterization of the classifiers a bit more.
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 稍微调整一下分类器的参数化。
- en: Note that we reduced the amount of nondeterminism by allowing the maximum number
    of features to go up to this could eventually lead to some degree of overfitting.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们通过允许最大特征数增加到这个程度来减少了非确定性，这最终可能导致一定程度上的过拟合。
- en: Note
  id: totrans-355
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity is available at page 285.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可在第285页找到。
- en: Summary
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have learned how to use decision trees for prediction. Using
    ensemble learning techniques, we created complex reinforcement learning models
    to predict the class of an arbitrary data point.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用决策树进行预测。通过集成学习技术，我们创建了复杂的强化学习模型来预测任意数据点的类别。
- en: Decision trees on their own proved to be very accurate on the surface, but they
    were prone to overfitting the model. Random Forests and Extremely Randomized Trees
    combat overfitting by introducing some random elements and a voting algorithm,
    where the majority wins.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 独立的决策树在表面上证明非常准确，但它们容易过拟合模型。随机森林和超随机树通过引入一些随机元素和投票算法（多数胜出）来对抗过拟合。
- en: Beyond decision trees, random forests, and Extremely Randomized Trees, we also
    learned about new methods for evaluating the utility of a model. After using the
    well-known accuracy score, we started using the precision, recall, and F1 score
    metrics to evaluate how well our classifier works. All of these values were derived
    from the confusion matrix.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 除了决策树、随机森林和超随机树之外，我们还学习了评估模型效用的新方法。在采用众所周知的准确率评分之后，我们开始使用精确率、召回率和F1评分指标来评估我们的分类器工作得如何。所有这些值都是从混淆矩阵中推导出来的。
- en: In the next chapter, we will describe the clustering problem and compare and
    contrast two clustering algorithms.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将描述聚类问题，并比较和对比两种聚类算法。
