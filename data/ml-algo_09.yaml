- en: Clustering Fundamentals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类基础
- en: In this chapter, we're going to introduce the basic concepts of clustering and
    the structure of k-means, a quite common algorithm that can solve many problems
    efficiently. However, its assumptions are very strong, in particular those concerning
    the convexity of the clusters, and this can lead to some limitations in its adoption.
    We're going to discuss its mathematical foundation and how it can be optimized.
    Moreover, we're going to analyze two alternatives that can be employed when k-means
    fails to cluster a dataset. These alternatives are DBSCAN, (which works by considering
    the differences of sample density), and spectral clustering, a very powerful approach
    based on the affinity among points.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍聚类的基概念和 k-means 的结构，这是一个相当常见的算法，可以有效地解决许多问题。然而，它的假设非常强烈，特别是关于簇的凸性，这可能导致其在应用中存在一些局限性。我们将讨论其数学基础及其优化方法。此外，我们将分析两种在
    k-means 无法对数据集进行聚类时可以采用的替代方案。这些替代方案是 DBSCAN（通过考虑样本密度的差异来工作）和基于点之间亲和力的非常强大的方法——谱聚类。
- en: Clustering basics
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类基础
- en: 'Let''s consider a dataset of points:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个点集数据集：
- en: '![](img/166885de-3d63-48d5-a7a0-d08c5ec35c22.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/166885de-3d63-48d5-a7a0-d08c5ec35c22.png)'
- en: 'We assume that it''s possible to find a criterion (not unique) so that each
    sample can be associated with a specific group:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设可以找到一个标准（不是唯一的）以便每个样本都能与一个特定的组相关联：
- en: '![](img/c8142ca3-03ec-4ebb-ac1a-42b0e66b3145.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c8142ca3-03ec-4ebb-ac1a-42b0e66b3145.png)'
- en: 'Conventionally, each group is called a **cluster** and the process of finding
    the function *G* is called **clustering**. Right now, we are not imposing any
    restriction on the clusters; however, as our approach is unsupervised, there should
    be a similarity criterion to join some elements and separate other ones. Different
    clustering algorithms are based on alternative strategies to solve this problem,
    and can yield very different results. In the following figure, there''s an example
    of clustering based on four sets of bidimensional samples; the decision to assign
    a point to a cluster depends only on its features and sometimes on the position
    of a set of other points (neighborhood):'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，每个组被称为**簇**，寻找函数 *G* 的过程称为**聚类**。目前，我们没有对簇施加任何限制；然而，由于我们的方法是未监督的，应该有一个相似性标准来连接某些元素并分离其他元素。不同的聚类算法基于不同的策略来解决这个问题，并可能产生非常不同的结果。在下图中，有一个基于四组二维样本的聚类示例；将一个点分配给簇的决定仅取决于其特征，有时还取决于一组其他点的位置（邻域）：
- en: '![](img/ae488a1f-8aac-4104-94c1-81059dc3a61f.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ae488a1f-8aac-4104-94c1-81059dc3a61f.png)'
- en: 'In this book, we''re going to discuss **hard clustering** techniques, where
    each element must belong to a single cluster. The alternative approach, called
    **soft clustering** (or **fuzzy clustering**), is based on a membership score
    that defines how much the elements are "compatible" with each cluster. The generic
    clustering function becomes:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们将讨论**硬聚类**技术，其中每个元素必须属于单个簇。另一种方法称为**软聚类**（或**模糊聚类**），它基于一个成员分数，该分数定义了元素与每个簇“兼容”的程度。通用的聚类函数变为：
- en: '![](img/8b957908-ca3d-4ae6-be64-a65745af825d.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8b957908-ca3d-4ae6-be64-a65745af825d.png)'
- en: A vector *m[i]* represents the relative membership of *x[i]*, and it's often
    normalized as a probability distribution.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 *m[i]* 代表 *x[i]* 的相对成员资格，通常将其归一化为概率分布。
- en: K-means
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means
- en: 'The k-means algorithm is based on the (strong) initial condition to decide
    the number of clusters through the assignment of k initial **centroids** or **means**:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 算法基于（强烈的）初始条件，通过分配 k 个初始**质心**或**均值**来决定簇的数量：
- en: '![](img/57f9c0a1-3a73-47ff-a35e-ee9e4fe2b787.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/57f9c0a1-3a73-47ff-a35e-ee9e4fe2b787.png)'
- en: 'Then the distance between each sample and each centroid is computed and the
    sample is assigned to the cluster where the distance is minimum. This approach
    is often called **minimizing the inertia** of the clusters, which is defined as
    follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算每个样本与每个质心之间的距离，并将样本分配到距离最小的簇。这种方法通常被称为**最小化簇的惯性**，其定义如下：
- en: '![](img/ffa4f61c-5be4-48ea-87f1-52e27213501b.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ffa4f61c-5be4-48ea-87f1-52e27213501b.png)'
- en: The process is iterative—once all the samples have been processed, a new set
    of centroids *K^((1))* is computed (now considering the actual elements belonging
    to the cluster), and all the distances are recomputed. The algorithm stops when
    the desired tolerance is reached, or in other words, when the centroids become
    stable and, therefore, the inertia is minimized.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程是迭代的——一旦所有样本都已被处理，就会计算一个新的质心集 *K^((1))*（现在考虑属于聚类的实际元素），并且重新计算所有距离。算法在达到所需的容差时停止，换句话说，当质心变得稳定，因此惯性最小化时停止。
- en: Of course, this approach is quite sensitive to the initial conditions, and some
    methods have been studied to improve the convergence speed. One of them is called
    **k-means++** (Karteeka Pavan K., Allam Appa Rao, Dattatreya Rao A. V., and Sridhar
    G.R., *Robust Seed Selection Algorithm for K-Means Type Algorithms*, International
    Journal of Computer Science and Information Technology 3, no. 5, October 30, 2011),
    which selects the initial centroids so that they are statistically close to the
    final ones. The mathematical explanation is quite difficult; however, this method
    is the default choice for scikit-learn, and it's normally the best choice for
    any clustering problem solvable with this algorithm.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这种方法对初始条件非常敏感，已经研究了某些方法来提高收敛速度。其中之一被称为**k-means++**（Karteeka Pavan K.，Allam
    Appa Rao，Dattatreya Rao A. V.，和Sridhar G.R.，《K-Means类型算法的鲁棒种子选择算法》，国际计算机科学和信息技术杂志3，第5期，2011年10月30日），该方法选择初始质心，使其在统计上接近最终质心。数学解释相当困难；然而，这种方法是scikit-learn的默认选择，并且通常对于任何可以用此算法解决的聚类问题来说都是最佳选择。
- en: 'Let''s consider a simple example with a dummy dataset:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的示例，使用一个虚拟数据集：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We expect to have three clusters with bidimensional features and a partial overlap
    due to the standard deviation of each blob. In our example, we won't use the *Y*
    variable (which contains the expected cluster) because we want to generate only
    a set of locally coherent points to try our algorithms.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们期望有三个具有二维特征的聚类，由于每个团块的方差，它们之间存在部分重叠。在我们的例子中，我们不会使用*Y*变量（它包含预期的聚类），因为我们只想生成一组局部一致的点来尝试我们的算法。
- en: 'The resultant plot is shown in the following figure:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图示如下所示：
- en: '![](img/ae60d470-3a18-4476-9072-f17138b8f586.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ae60d470-3a18-4476-9072-f17138b8f586.png)'
- en: 'In this case, the problem is quite simple to solve, so we expect k-means to
    separate the three groups with minimum error in the region of *X* bounded between
    [-5, 0]. Keeping the default values, we get:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，问题非常简单，所以我们期望k-means在*X*的[-5, 0]区间内以最小误差将三个组分开。保持默认值，我们得到：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Replotting the data using three different markers, it''s possible to verify
    how k-means successfully separated the data:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用三种不同的标记重新绘制数据，可以验证k-means如何成功地将数据分离：
- en: '![](img/86c72a22-3ffd-4b06-ac81-6c4ff4ded9d3.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/86c72a22-3ffd-4b06-ac81-6c4ff4ded9d3.png)'
- en: In this case, the separation was very easy because k-means is based on Euclidean
    distance, which is radial, and therefore the clusters are expected to be convex.
    When this doesn't happen, the problem cannot be solved using this algorithm. Most
    of the time, even if the convexity is not fully guaranteed, k-means can produce
    good results, but there are several situations when the expected clustering is
    impossible and letting k-means find out the centroid can lead to completely wrong
    solutions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，分离非常容易，因为k-means基于欧几里得距离，它是径向的，因此预期聚类将是凸集。当这种情况不发生时，无法使用此算法解决问题。大多数时候，即使凸性没有得到完全保证，k-means也能产生良好的结果，但有一些情况下预期的聚类是不可能的，让k-means找到质心可能会导致完全错误的结果。
- en: 'Let''s consider the case of concentric circles. scikit-learn provides a built-in
    function to generate such datasets:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑同心圆的情况。scikit-learn提供了一个内置函数来生成这样的数据集：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The plot of this dataset is shown in the following figure:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集的图示如下所示：
- en: '![](img/dba7d538-1ae1-491d-87c7-40988971e54c.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dba7d538-1ae1-491d-87c7-40988971e54c.png)'
- en: 'We would like to have an internal cluster (corresponding to the samples depicted
    with triangular markers) and an external one (depicted by dots). However, such
    sets are not convex, and it''s impossible for k-means to separate them correctly
    (the means should be the same!). In fact, suppose we try to apply the algorithm
    to two clusters:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望有一个内部聚类（对应于用三角形标记表示的样本）和一个外部聚类（用点表示）。然而，这样的集合不是凸集，k-means无法正确地将它们分离（均值应该是相同的！）。实际上，假设我们尝试将算法应用于两个聚类：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We get the separation shown in the following figure:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下图所示的分离：
- en: '![](img/3ce4374e-9ffc-40ae-86d2-250a23da572f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3ce4374e-9ffc-40ae-86d2-250a23da572f.png)'
- en: As expected, k-means converged on the two centroids in the middle of the two
    half-circles, and the resulting clustering is quite different from what we expected.
    Moreover, if the samples must be considered different according to the distance
    from the common center, this result will lead to completely wrong predictions.
    It's obvious that another method must be employed.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，k-means收敛到两个半圆中间的两个质心，并且得到的聚类结果与我们预期的完全不同。此外，如果必须根据与公共中心的距离来考虑样本的不同，这个结果将导致完全错误的预测。显然，必须采用另一种方法。
- en: Finding the optimal number of clusters
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找最佳聚类数量
- en: One of the most common disadvantages of k-means is related to the choice of
    the optimal number of clusters. An excessively small value will determine large
    groupings that contain heterogeneous elements, while a large number leads to a
    scenario where it can be difficult to identify the differences among clusters.
    Therefore, we're going to discuss some methods that can be employed to determine
    the appropriate number of splits and to evaluate the corresponding performance.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: k-means最常见的一个缺点与选择最佳聚类数量有关。过小的值将确定包含异质元素的大分组，而较大的值可能导致难以识别聚类之间差异的场景。因此，我们将讨论一些可以用来确定适当分割数量和评估相应性能的方法。
- en: Optimizing the inertia
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化惯性
- en: The first method is based on the assumption that an appropriate number of clusters
    must produce a small inertia. However, this value reaches its minimum (0.0) when
    the number of clusters is equal to the number of samples; therefore, we can't
    look for the minimum, but for a value which is a trade-off between the inertia
    and the number of clusters.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法基于这样的假设：适当的聚类数量必须产生小的惯性。然而，当聚类数量等于样本数量时，这个值达到最小（0.0）；因此，我们不能寻找最小值，而是寻找一个在惯性和聚类数量之间权衡的值。
- en: 'Let''s suppose we have a dataset of 1,000 elements. We can compute and collect
    the inertias (scikit-learn stores these values in the instance variable `inertia_`)
    for a different number of clusters:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含1,000个元素的数据库。我们可以计算并收集不同数量聚类下的惯性（scikit-learn将这些值存储在实例变量`inertia_`中）：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Plotting the values, we get the result shown in the following figure:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制值，我们得到以下图所示的结果：
- en: '![](img/09130468-38ab-4b44-a9ce-c7d700bb673d.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/09130468-38ab-4b44-a9ce-c7d700bb673d.png)'
- en: As you can see, there's a dramatic reduction between 2 and 3 and then the slope
    starts flattening. We want to find a value that, if reduced, leads to a great
    inertial increase and, if increased, produces a very small inertial reduction.
    Therefore, a good choice could be 4 or 5, while greater values are likely to produce
    unwanted intracluster splits (till the extreme situation where each point becomes
    a single cluster). This method is very simple, and can be employed as the first
    approach to determine a potential range. The next strategies are more complex,
    and can be used to find the final number of clusters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，2和3之间有一个戏剧性的减少，然后斜率开始变平。我们希望找到一个值，如果减少，会导致惯性大幅增加，如果增加，会产生非常小的惯性减少。因此，一个好的选择可能是4或5，而更大的值可能会产生不希望的聚类内分割（直到极端情况，每个点成为一个单独的聚类）。这种方法非常简单，可以用作确定潜在范围的第一个方法。接下来的策略更复杂，可以用来找到最终的聚类数量。
- en: Silhouette score
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 形状系数
- en: The silhouette score is based on the principle of "maximum internal cohesion
    and maximum cluster separation". In other words, we would like to find the number
    of clusters that produce a subdivision of the dataset into dense blocks that are
    well separated from each other. In this way, every cluster will contain very similar
    elements and, selecting two elements belonging to different clusters, their distance
    should be greater than the maximum intracluster one.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 形状系数基于“最大内部凝聚力和最大聚类分离”的原则。换句话说，我们希望找到产生数据集细分，形成彼此分离的密集块的数量。这样，每个聚类将包含非常相似的元素，并且选择属于不同聚类的两个元素，它们的距离应该大于最大聚类内距离。
- en: 'After defining a distance metric (Euclidean is normally a good choice), we
    can compute the average intracluster distance for each element:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义距离度量（欧几里得通常是不错的选择）之后，我们可以计算每个元素的聚类内平均距离：
- en: '![](img/52a32904-d907-4c36-9912-5d96f96b5af3.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/52a32904-d907-4c36-9912-5d96f96b5af3.png)'
- en: 'We can also define the average nearest-cluster distance (which corresponds
    to the lowest intercluster distance):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以定义平均最近簇距离（这对应于最低的簇间距离）：
- en: '![](img/79c11243-250f-4c9c-b722-62686b2f9665.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/79c11243-250f-4c9c-b722-62686b2f9665.png)'
- en: 'The silhouette score for an element *x[i]* is defined as:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 元素*x[i]*的轮廓得分定义为：
- en: '![](img/33afd3c2-4b5b-44fb-8592-f83970dce3ae.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/33afd3c2-4b5b-44fb-8592-f83970dce3ae.png)'
- en: 'This value is bounded between -1 and 1, with the following interpretation:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个值介于-1和1之间，其解释如下：
- en: A value close to 1 is good (1 is the best condition) because it means that *a(x[i])
    << b(x[i])*
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个接近1的值是好的（1是最佳条件），因为这表示*a(x[i]) << b(x[i])*。
- en: A value close to 0 means that the difference between intra and inter cluster
    measures is almost null and therefore there's a cluster overlap
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接近0的值表示簇内和簇间测量的差异几乎为零，因此存在簇重叠。
- en: A value close to -1 means that the sample has been assigned to a wrong cluster
    because *a(x[i]) >> b(x[i])*
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接近-1的值表示样本被分配到了错误的聚类，因为*a(x[i]) >> b(x[i])*。
- en: 'scikit-learn allows computing the average silhouette score to have an immediate
    overview for different numbers of clusters:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 允许计算平均轮廓得分，以便对不同数量的聚类有一个立即的概览：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The corresponding plot is shown in the following figure:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的图表如下所示：
- en: '![](img/4a1a75ed-58d7-457f-98f6-82a890aedc23.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4a1a75ed-58d7-457f-98f6-82a890aedc23.png)'
- en: 'The best value is 3 (which is very close to 1.0), however, bearing in mind
    the previous method, 4 clusters provide a smaller inertia, together with a reasonable
    silhouette score. Therefore, a good choice could be 4 instead of 3\. However,
    the decision between 3 and 4 is not immediate and should be evaluated by also
    considering the nature of the dataset. The silhouette score indicates that there
    are 3 dense agglomerates, but the inertia diagram suggests that one of them (at
    least) can probably be split into two clusters. To have a better understanding
    of how the clustering is working, it''s also possible to graph the silhouette
    plots, showing the sorted score for each sample in all clusters. In the following
    snippet we create the plots for a number of clusters equal to 2, 3, 4, and 8:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳值是3（非常接近1.0），然而，考虑到前面的方法，4个聚类提供了更小的惯性，同时轮廓得分也合理。因此，选择4而不是3可能是一个更好的选择。然而，3和4之间的决定并不立即，应该通过考虑数据集的性质来评估。轮廓得分表明存在3个密集的聚簇，但惯性图表明其中至少有一个可以可能分成两个簇。为了更好地理解聚类是如何工作的，还可以绘制轮廓图，显示所有簇中每个样本的排序得分。在以下代码片段中，我们为2、3、4和8个簇创建图表：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The silhouette coefficients for each sample are computed using the function
    `silhouette_values` (which are always bounded between -1 and 1). In this case,
    we are limiting the graph between -0.15 and 1 because there are no smaller values.
    However, it's important to check the whole range before restricting it.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 每个样本的轮廓系数是通过函数 `silhouette_values`（这些值始终介于-1和1之间）计算的。在这种情况下，我们将图表限制在-0.15和1之间，因为没有更小的值。然而，在限制之前检查整个范围是很重要的。
- en: 'The resulting graph is shown in the following figure:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表如下所示：
- en: '![](img/94cbeff8-0256-4d3d-a26d-8945814fba7d.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/94cbeff8-0256-4d3d-a26d-8945814fba7d.png)'
- en: The width of each silhouette is proportional to the number of samples belonging
    to a specific cluster, and its shape is determined by the scores of each sample.
    An ideal plot should contain homogeneous and long silhouettes without peaks (they
    must be similar to trapezoids rather than triangles) because we expect to have
    a very low score variance among samples in the same cluster. For 2 clusters, the
    shapes are acceptable, but one cluster has an average score of 0.5, while the
    other has a value greater than 0.75; therefore, the first cluster has a low internal
    coherence. A completely different situation is shown in the plot corresponding
    to 8 clusters. All the silhouettes are triangular and their maximum score is slightly
    greater than 0.5\. It means that all the clusters are internally coherent, but
    the separation is unacceptable. With 3 clusters, the plot is almost perfect, except
    for the width of the second silhouette. Without further metrics, we could consider
    this number as the best choice (confirmed also by the average score), but the
    inertia is lower for a higher numbers of clusters. With 4 clusters, the plot is
    slightly worse, with two silhouettes having a maximum score of about 0.5\. This
    means that two clusters are perfectly coherent and separated, while the remaining
    two are rather coherent, but they aren't probably well separated. Right now, our
    choice should be made between 3 and 4\. The next methods will help us in banishing
    all doubts.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 每个轮廓的宽度与属于特定聚类的样本数量成正比，其形状由每个样本的得分决定。理想的图表应包含均匀且长的轮廓，没有峰值（它们必须类似于梯形而不是三角形），因为我们期望同一聚类中的样本得分方差非常低。对于两个聚类，形状是可以接受的，但一个聚类的平均得分为0.5，而另一个的值大于0.75；因此，第一个聚类的内部一致性较低。在对应于8个聚类的图表中，展示了完全不同的情况。所有轮廓都是三角形的，其最大得分略大于0.5。这意味着所有聚类在内部是一致的，但分离度不可接受。对于三个聚类，图表几乎是完美的，除了第二个轮廓的宽度。如果没有其他指标，我们可以考虑这个数字是最好的选择（也由平均得分证实），但聚类的数量越多，惯性越低。对于四个聚类，图表略差，有两个轮廓的最大得分约为0.5。这意味着两个聚类完美一致且分离，而剩下的两个则相对一致，但它们可能没有很好地分离。目前，我们应在3和4之间做出选择。接下来，我们将介绍其他方法，以消除所有疑虑。
- en: Calinski-Harabasz index
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卡尔金斯-哈拉巴斯指数
- en: 'Another method that is based on the concept of dense and well-separated clusters
    is the Calinski-Harabasz index. To build it, we need first to define the inter
    cluster dispersion. If we have k clusters with their relative centroids and the
    global centroid, the inter-cluster dispersion (BCD) is defined as:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种基于密集和分离良好聚类概念的方法是卡尔金斯-哈拉巴斯指数。要构建它，我们首先需要定义簇间分散度。如果我们有k个聚类及其相对质心和全局质心，簇间分散度（BCD）定义为：
- en: '![](img/61698f0e-0e3d-430c-81e7-ca1ff5689493.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/61698f0e-0e3d-430c-81e7-ca1ff5689493.png)'
- en: 'In the above expression, *n[k]* is the number of elements belonging to the
    cluster k, *mu* (the Greek letter in the formula) is the global centroid, and
    *mu*[i] is the centroid of cluster *i*. The intracluster dispersion (WCD) is defined
    as:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述表达式中，*n[k]* 是属于聚类k的元素数量，*mu*（公式中的希腊字母）是全局质心，而 *mu*[i] 是聚类 *i* 的质心。簇内分散度（WCD）定义为：
- en: '![](img/318e76f2-f257-43fa-b84c-873789494b11.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/318e76f2-f257-43fa-b84c-873789494b11.png)'
- en: 'The Calinski-Harabasz index is defined as the ratio between *BCD(k)* and *WCD(k)*:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 卡尔金斯-哈拉巴斯指数定义为 *BCD(k)* 和 *WCD(k)* 之间的比率：
- en: '![](img/9e3c3dde-0942-4d7d-a775-941a6683a9b7.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9e3c3dde-0942-4d7d-a775-941a6683a9b7.png)'
- en: 'As we look for a low intracluster dispersion (dense agglomerates) and a high
    intercluster dispersion (well-separated agglomerates), we need to find the number
    of clusters that maximizes this index. We can obtain a graph in a way similar
    to what we have already done for the silhouette score:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在寻找低簇内分散度（密集的聚团）和高簇间分散度（分离良好的聚团），需要找到最大化此指数的聚类数量。我们可以以类似于我们之前为轮廓得分所做的方式获得一个图表：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The resulting plot is shown in the following figure:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表如下所示：
- en: '![](img/bedfcc53-43b8-4b6c-be5c-4c35cf2ea966.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bedfcc53-43b8-4b6c-be5c-4c35cf2ea966.png)'
- en: As expected, the highest value (5,500) is obtained with 3 clusters, while 4
    clusters yield a value slightly below 5,000\. Considering only this method, there's
    no doubt that the best choice is 3, even if 4 is still a reasonable value. Let's
    consider the last method, which evaluates the overall stability.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，最高值（5,500）是在三个聚类时获得的，而四个聚类得到的值略低于5,000。仅考虑这种方法，没有疑问，最佳选择是3，即使4也是一个合理的值。让我们考虑最后一种方法，它评估整体稳定性。
- en: Cluster instability
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类不稳定性
- en: 'Another approach is based on the concept of cluster instability defined in Von
    Luxburg U., *Cluster stability: an overview*, arXiv 1007:1075v1, 7 July 2010*. *Intuitively,
    we can say that a clustering approach is stable if perturbed versions of the same
    dataset produce very similar results. More formally, if we have a dataset *X*,
    we can define a set of *m* perturbed (or noisy) versions:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种方法基于在 Von Luxburg U. 的文章《Cluster stability: an overview》中定义的簇不稳定性概念，arXiv
    1007:1075v1，2010 年 7 月 7 日。直观地说，我们可以认为，如果一个聚类方法在扰动相同数据集的版本中产生非常相似的结果，那么这个聚类方法是稳定的。更正式地说，如果我们有一个数据集
    *X*，我们可以定义一组 *m* 扰动（或噪声）版本：'
- en: '![](img/69ae3442-57db-4512-b20d-e6a63f7a1ff5.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/69ae3442-57db-4512-b20d-e6a63f7a1ff5.png)'
- en: 'Considering a distance metric *d(C(X[1]), C(X[2]))* between two clusterings
    with the same number (k) of clusters, the instability is defined as the average
    distance between couples of clusterings of noisy versions:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两个具有相同簇数（k）的聚类之间的距离度量 *d(C(X[1]), C(X[2]))*，不稳定性定义为噪声版本聚类对之间的平均距离：
- en: '![](img/8fb57b45-0c9a-4d8d-9e6e-d10689759aee.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8fb57b45-0c9a-4d8d-9e6e-d10689759aee.png)'
- en: 'For our purposes, we need to find the value of k that minimizes *I(C)* (and
    therefore maximizes the stability). First of all, we need to produce some noisy
    versions of the dataset. Let''s suppose that *X* contains 1,000 bidimensional
    samples with a standard deviation of 10.0\. We can perturb *X* by adding a uniform
    random value (in the range [-2.0, 2.0]) with a probability of 0.25:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们需要找到使 *I(C)* 最小化的 k 值（因此最大化稳定性）。首先，我们需要生成一些数据集的噪声版本。假设 *X* 包含 1,000
    个二维样本，标准差为 10.0。我们可以通过添加一个均匀随机值（范围在 [-2.0, 2.0] 内）以 0.25 的概率扰动 *X*：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here we are assuming to have four perturbed versions. As a metric, we adopt
    the Hamming distance, which is proportional (if normalized) to the number of output
    elements that disagree. At this point, we can compute the instabilities for various
    numbers of clusters:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们假设有四个扰动版本。作为一个度量标准，我们采用汉明距离，该距离与不同意的输出元素数量成比例（如果归一化）。在这个阶段，我们可以计算不同簇数量下的不稳定性：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As the distances are symmetrical, we compute them only for the upper triangular
    part of the matrix. The result is shown in the following figure:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于距离是对称的，我们只计算矩阵的上三角部分。结果如下所示：
- en: '![](img/7f2485c9-b154-444a-8823-1a3adde2a9be.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7f2485c9-b154-444a-8823-1a3adde2a9be.png)'
- en: 'Excluding the configuration with 2 clusters, where the inertia is very high,
    we have a minimum for 3 clusters, a value that has already been confirmed by the
    three previous methods. Therefore, we can finally decide to set `n_clusters=3`, excluding
    the options of 4 or more clusters.  This method is very powerful, but it''s important
    to evaluate the stability with a reasonable number of noisy datasets, taking care
    not to excessively alter the original geometry. A good choice is to use Gaussian
    noise with a variance set to a fraction (for example 1/10) of the dataset variance.
    Alternative approaches are presented in Von Luxburg U., *Cluster stability: an
    overview*, arXiv 1007:1075v1, 7 July 2010.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '排除具有 2 个簇的配置，其中惯性非常高，我们有 3 个簇的最小值，这个值已经被前三种方法所确认。因此，我们最终可以决定将 `n_clusters`
    设置为 3，排除 4 个或更多簇的选项。这种方法非常强大，但重要的是要用合理的噪声数据集数量来评估稳定性，注意不要过度改变原始几何形状。一个好的选择是使用高斯噪声，方差设置为数据集方差的分数（例如
    1/10）。其他方法在 Von Luxburg U. 的文章《Cluster stability: an overview》中有所介绍，arXiv 1007:1075v1，2010
    年 7 月 7 日。'
- en: Even if we have presented these methods with k-means, they can be applied to
    any clustering algorithm to evaluate the performance and compare them.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们已经用 k-means 展示了这些方法，它们也可以应用于任何聚类算法来评估性能并比较它们。
- en: DBSCAN
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DBSCAN
- en: 'DBSCAN or **Density-Based Spatial Clustering of Applications with Noise** is
    a powerful algorithm that can easily solve non-convex problems where k-means fails.
    The idea is simple: A cluster is a high-density area (there are no restrictions
    on its shape) surrounded by a low-density one. This statement is generally true,
    and doesn''t need an initial declaration about the number of expected clusters.
    The procedure starts by analyzing a small area (formally, a point surrounded by
    a minimum number of other samples). If the density is enough, it is considered
    part of a cluster. At this point, the neighbors are taken into account. If they
    also have a high density, they are merged with the first area; otherwise, they
    determine a topological separation. When all the areas have been scanned, the
    clusters have also been determined because they are islands surrounded by empty
    space.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 或 **基于密度的空间聚类应用噪声** 是一种强大的算法，可以轻松解决 k-means 无法解决的非凸问题。其思想很简单：簇是一个高密度区域（对其形状没有限制），周围被低密度区域包围。这个陈述通常是正确的，并且不需要对预期簇的数量进行初始声明。该过程从分析一个小区域（形式上，一个由最小数量的其他样本包围的点）开始。如果密度足够，它被认为是簇的一部分。此时，考虑邻居。如果它们也有高密度，它们将与第一个区域合并；否则，它们将确定拓扑分离。当扫描完所有区域后，簇也已经确定，因为它们是被空空间包围的岛屿。
- en: 'scikit-learn allows us to control this procedure with two parameters:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 允许我们通过两个参数来控制此过程：
- en: '`eps`: Responsible for defining the maximum distance between two neighbors.
    Higher values will aggregate more points, while smaller ones will create more
    clusters.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps`: 负责定义两个邻居之间的最大距离。值越高，聚合的点越多，而值越小，创建的簇越多。'
- en: '`min_samples`: This determines how many surrounding points are necessary to
    define an area (also known as the core-point).'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples`: 这决定了定义一个区域（也称为核心点）所需的周围点的数量。'
- en: 'Let''s try DBSCAN with a very hard clustering problem, called half-moons. The
    dataset can be created using a built-in function:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个非常困难的聚类问题，称为半月形。可以使用内置函数创建数据集：
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'A plot of the dataset is shown in the following figure:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的图示如下所示：
- en: '![](img/252726e9-66ee-44b4-88cb-594b961906ab.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/252726e9-66ee-44b4-88cb-594b961906ab.png)'
- en: 'Just to understand, k-means will cluster by finding the optimal convexity,
    and the result is shown in the following figure:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解，k-means 将通过寻找最优凸性来进行聚类，结果如下所示：
- en: '![](img/49b7dea9-457b-407f-bb37-a803af3e5a4b.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/49b7dea9-457b-407f-bb37-a803af3e5a4b.png)'
- en: 'Of course, this separation is unacceptable, and there''s no way to improve
    the accuracy. Let''s try it with DBSCAN (with `eps` set to 0.1 and the default
    value of 5 for `min_samples`):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这种分离是不可接受的，而且没有方法可以提高准确性。让我们尝试使用 DBSCAN（将 `eps` 设置为 0.1，`min_samples` 的默认值为
    5）：
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In a different manner than other implementations, DBSCAN predicts the label
    during the training process, so we already have an array `Y` containing the cluster
    assigned to each sample. In the following figure, there''s a representation with
    two different markers:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他实现方式不同，DBSCAN 在训练过程中预测标签，因此我们已经有了一个包含每个样本分配的簇的数组 `Y`。在下图中，有两种不同的标记表示：
- en: '![](img/281c2a7b-b947-49a7-b3c6-2fe0e88a43d0.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/281c2a7b-b947-49a7-b3c6-2fe0e88a43d0.png)'
- en: 'As you can see, the accuracy is very high and only three isolated points are
    misclassified (in this case, we know their class, so we can use this term even
    if it''s a clustering process). However, by performing a grid search, it''s easy
    to find the best values that optimize the clustering process. It''s important
    to tune up those parameters to avoid two common problems: few big clusters and
    many small ones. This problem can be easily avoided using the following method.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，准确度非常高，只有三个孤立点被错误分类（在这种情况下，我们知道它们的类别，因此我们可以使用这个术语，即使它是一个聚类过程）。然而，通过执行网格搜索，很容易找到优化聚类过程的最佳值。调整这些参数非常重要，以避免两个常见问题：少数大簇和许多小簇。这个问题可以通过以下方法轻松避免。
- en: Spectral clustering
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谱聚类
- en: 'Spectral clustering is a more sophisticated approach based on a symmetric affinity
    matrix:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 谱聚类是一种基于对称亲和矩阵的更复杂的方法：
- en: '![](img/78106f0d-bdd9-47d8-9516-a352132c5fea.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/78106f0d-bdd9-47d8-9516-a352132c5fea.png)'
- en: Here, each element *a[ij]* represents a measure of affinity between two samples.
    The most diffused measures (also supported by scikit-learn) are radial basis function
    and nearest neighbors. However, any kernel can be used if it produces measures
    that have the same features of a distance (non-negative, symmetric, and increasing).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个元素*a[ij]*代表两个样本之间的亲和度度量。最常用的度量（也由scikit-learn支持）是径向基函数和最近邻。然而，如果核产生的度量具有距离的特征（非负、对称和递增），则可以使用任何核。
- en: The Laplacian matrix is computed and a standard clustering algorithm is applied
    to a subset of eigenvectors (this element is strictly related to each single strategy).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 计算拉普拉斯矩阵并应用标准聚类算法到特征向量的子集（这个元素严格与每个单独的策略相关）。
- en: scikit-learn implements the Shi-Malik algorithm (*Shi J., Malik J., Normalized
    Cuts and Image Segmentation, IEEE Transactions on Pattern Analysis and Machine
    Intelligence, Vol. 22, 08/2000*), also known as normalized-cuts, which partitions
    the samples into two sets (*G[1]* and *G[2]*, which are formally graphs where
    each point is a vertex and the edges are derived from the normalized Laplacian
    matrix) so that the weights corresponding to the points inside a cluster are quite
    higher than the one belonging to the cut. A complete mathematical explanation
    is beyond the scope of this book; however, in *Von Luxburg U., A Tutorial on Spectral
    Clustering, 2007*, you can read a full explanation of many alternative spectral
    approaches.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn实现了Shi-Malik算法（*Shi J., Malik J., Normalized Cuts and Image Segmentation,
    IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 22, 08/2000*），也称为normalized-cuts，该算法将样本划分为两个集合（*G[1]*和*G[2]*，这些集合形式上是图，其中每个点是一个顶点，边由归一化拉普拉斯矩阵导出），使得属于簇内点的权重远高于属于分割的权重。完整的数学解释超出了本书的范围；然而，在*Von
    Luxburg U., A Tutorial on Spectral Clustering, 2007*中，你可以阅读关于许多替代谱方法的完整解释。
- en: 'Let''s consider the previous half-moon example. In this case, the affinity
    (just like for DBSCAN) should be based on the nearest neighbors function; however,
    it''s useful to compare different kernels. In the first experiment, we use an
    RBF kernel with different values for the `gamma` parameter:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑之前的半月形示例。在这种情况下，亲和度（就像DBSCAN一样）应该基于最近邻函数；然而，比较不同的核很有用。在第一个实验中，我们使用具有不同`gamma`参数值的RBF核：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this algorithm, we need to specify how many clusters we want, so we set
    the value to 2\. The resulting plots are shown in the following figure:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个算法中，我们需要指定我们想要多少个簇，因此我们将值设置为2。结果图如下所示：
- en: '![](img/6d730f90-a7b3-40fa-8363-db26464084b2.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6d730f90-a7b3-40fa-8363-db26464084b2.png)'
- en: 'As you can see, when the scaling factor gamma is increased the separation becomes
    more accurate; however, considering the dataset, using the nearest neighbors kernel
    is not necessary in any search:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，当缩放因子gamma增加时，分离变得更加准确；然而，考虑到数据集，在任何搜索中都不需要使用最近邻核。
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The resulting plot is shown in the following figure:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图如下所示：
- en: '![](img/54637c5e-b476-439e-b043-9aefef953a45.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/54637c5e-b476-439e-b043-9aefef953a45.png)'
- en: As for many other kernel-based methods, spectral clustering needs a previous
    analysis to detect which kernel can provide the best values for the affinity matrix.
    scikit-learn also allows us to define custom kernels for those tasks that cannot
    easily be solved using the standard ones.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多基于核的方法，谱聚类需要先前的分析来检测哪个核可以提供亲和度矩阵的最佳值。scikit-learn也允许我们为那些难以使用标准核解决的问题定义自定义核。
- en: Evaluation methods based on the ground truth
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于真实情况的评估方法
- en: In this section, we present some evaluation methods that require the knowledge
    of the ground truth. This condition is not always easy to obtain because clustering
    is normally applied as an unsupervised method; however, in some cases, the training
    set has been manually (or automatically) labeled, and it's useful to evaluate
    a model before predicting the clusters of new samples.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了一些需要了解真实情况的评估方法。由于聚类通常作为无监督方法应用，因此这种条件并不总是容易获得；然而，在某些情况下，训练集已经被手动（或自动）标记，在预测新样本的簇之前评估模型是有用的。
- en: Homogeneity
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 同质性
- en: 'An important requirement for a clustering algorithm (given the ground truth)
    is that each cluster should contain only samples belonging to a single class.
    In [Chapter 2](c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml), *Important Elements
    in Machine Learning*, we have defined the concepts of entropy *H(X)* and conditional
    entropy *H(X|Y)*, which measures the uncertainty of *X* given the knowledge of
    *Y*. Therefore, if the class set is denoted as *C* and the cluster set as *K*,
    *H(C|K)* is a measure of the uncertainty in determining the right class after
    having clustered the dataset. To have a homogeneity score, it''s necessary to
    normalize this value considering the initial entropy of the class set *H(C)*:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个聚类算法（给定真实情况）的一个重要要求是，每个簇应只包含属于单个类别的样本。在[第2章](c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml)《机器学习中的重要元素》中，我们定义了熵
    *H(X)* 和条件熵 *H(X|Y)* 的概念，这些概念衡量了在知道 *Y* 的情况下 *X* 的不确定性。因此，如果类集表示为 *C*，聚类集表示为 *K*，则
    *H(C|K)* 是在聚类数据集后确定正确类别的不确定性的度量。为了得到同质性分数，有必要考虑类集的初始熵 *H(C)* 来归一化这个值：
- en: '![](img/741df5c8-8901-4a85-b75e-d9c9f3c50316.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/741df5c8-8901-4a85-b75e-d9c9f3c50316.png)'
- en: 'In scikit-learn, there''s the built-in function `homogeneity_score()` that
    can be used to compute this value. For this and the next few examples, we assume
    that we have a labeled dataset *X* (with true labels *Y*):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，有一个内置函数 `homogeneity_score()` 可以用来计算这个值。对于这个和接下来的几个例子，我们假设我们有一个标记的数据集
    *X*（带有真实标签 *Y*）：
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: A value of 0.8 means that there's a residual uncertainty of about 20% because
    one or more clusters contain some points belonging to a secondary class. As with
    the other methods shown in the previous section, it's possible to use the homogeneity
    score to determine the optimal number of clusters.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 0.8的值意味着大约有20%的残余不确定性，因为一个或多个簇包含一些属于次要类别的点。与其他在上一节中展示的方法一样，可以使用同质性分数来确定最佳簇数量。
- en: Completeness
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完整性
- en: 'A complementary requirement is that each sample belonging to a class is assigned
    to the same cluster. This measure can be determined using the conditional entropy
    *H(K|C)*, which is the uncertainty in determining the right cluster given the
    knowledge of the class. Like for the homogeneity score, we need to normalize this
    using the entropy *H(K)*:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个互补的要求是，属于一个类别的每个样本都被分配到同一个簇中。这个度量可以通过条件熵 *H(K|C)* 来确定，这是在知道类别的情况下确定正确簇的不确定性。像同质性分数一样，我们需要使用熵
    *H(K)* 来归一化这个值：
- en: '![](img/4cb96f4f-63b6-4ca5-9f76-491a9c46d069.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4cb96f4f-63b6-4ca5-9f76-491a9c46d069.png)'
- en: 'We can compute this score (on the same dataset) using the function `completeness_score()`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用函数 `completeness_score()`（在相同的数据集上）来计算这个分数：
- en: '[PRE15]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Also, in this case, the value is rather high, meaning that the majority of samples
    belonging to a class have been assigned to the same cluster. This value can be
    improved using a different number of clusters or changing the algorithm.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在这种情况下，这个值相当高，这意味着大多数属于一个类别的样本已经被分配到同一个簇中。这个值可以通过不同的簇数量或改变算法来提高。
- en: Adjusted rand index
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整后的rand指数
- en: 'The adjusted rand index measures the similarity between the original class
    partitioning (*Y*) and the clustering. Bearing in mind the same notation adopted
    in the previous scores, we can define:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的rand指数衡量原始类划分（*Y*）和聚类之间的相似性。考虑到与前面评分中采用相同的符号，我们可以定义：
- en: '**a**: The number of pairs of elements belonging to the same partition in the
    class set *C* and to the same partition in the clustering set *K*'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**a**：属于类集 *C* 和聚类集 *K* 中相同划分的元素对的数量'
- en: '**b**: The number of pairs of elements belonging to different partitions in
    the class set *C* and to different partitions in the clustering set **K**'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**b**：属于类集 *C* 和聚类集 **K** 中不同划分的元素对的数量'
- en: 'If we total number of samples in the dataset is *n*, the rand index is defined
    as:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集中的样本总数为 *n*，则rand指数定义为：
- en: '![](img/bc2c20a7-dd7c-4db5-b25a-621ecb046789.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bc2c20a7-dd7c-4db5-b25a-621ecb046789.png)'
- en: 'The *Corrected for Chance* version is the adjusted rand index, defined as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*校正后的随机性*版本是调整后的rand指数，其定义如下：'
- en: '![](img/492e610a-b5ee-4914-94d8-8a4f3a96fc36.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/492e610a-b5ee-4914-94d8-8a4f3a96fc36.png)'
- en: 'We can compute the adjusted rand score using the function `adjusted_rand_score()`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用函数 `adjusted_rand_score()` 来计算调整后的rand分数：
- en: '[PRE16]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As the adjusted rand score is bounded between -1.0 and 1.0, with negative values
    representing a bad situation (the assignments are strongly uncorrelated), a score
    of 0.83 means that the clustering is quite similar to the ground truth. Also,
    in this case, it's possible to optimize this value by trying different numbers
    of clusters or clustering strategies.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 由于调整后的兰德指数介于-1.0和1.0之间，负值表示不良情况（分配高度不相关），0.83的分数意味着聚类与真实情况非常相似。此外，在这种情况下，可以通过尝试不同的簇数量或聚类策略来优化这个值。
- en: References
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Karteeka Pavan K., Allam Appa Rao, Dattatreya Rao A. V., and Sridhar G.R., *Robust
    seed selection algorithm for k-means type algorithms*, International Journal of
    Computer Science and Information Technology 3, no. 5 (October 30, 2011)
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karteeka Pavan K., Allam Appa Rao, Dattatreya Rao A. V. 和 Sridhar G.R.，*针对k-means类型算法的鲁棒种子选择算法*，《International
    Journal of Computer Science and Information Technology》第3卷第5期（2011年10月30日）
- en: Shi J., Malik J., *Normalized Cuts and Image Segmentation*, *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, Vol. 22 (08/2000)
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi J., Malik J., *归一化切割与图像分割*，《IEEE Transactions on Pattern Analysis and Machine
    Intelligence》，第22卷（2000年8月）
- en: Von Luxburg U., *A Tutorial on Spectral Clustering*, 2007
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Von Luxburg U.，*谱聚类教程*，2007
- en: 'Von Luxburg U., *Cluster stability: an overview*, arXiv 1007:1075v1, 7 July
    2010'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Von Luxburg U.，*簇稳定性：概述*，arXiv 1007:1075v1，2010年7月7日
- en: Summary
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced the k-means algorithm, which is based on the
    idea of defining (randomly or according to some criteria) k centroids that represent
    the clusters and optimize their position so that the sum of squared distances
    for every point in each cluster and the centroid is minimum. As the distance is
    a radial function, k-means assumes clusters to be convex and cannot solve problems
    where the shapes have deep concavities (like the half-moon problem).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了基于定义（随机或根据某些标准）k个质心代表簇并优化它们的位置，使得每个簇中每个点到质心的平方距离之和最小的k-means算法。由于距离是一个径向函数，k-means假设簇是凸形的，不能解决形状有深凹处的（如半月形问题）问题。
- en: In order to solve such situations, we presented two alternatives. The first
    one is called DBSCAN and is a simple algorithm that analyzes the difference between
    points surrounded by other samples and boundary samples. In this way, it can easily
    determine high-density areas (which become clusters) and low-density spaces among
    them. There are no assumptions about the shape or the number of clusters, so it's
    necessary to tune up the other parameters so as to generate the right number of
    clusters.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这类情况，我们提出了两种替代方案。第一个被称为DBSCAN，它是一个简单的算法，分析被其他样本包围的点与边界样本之间的差异。这样，它可以很容易地确定高密度区域（成为簇）以及它们之间的低密度空间。对于簇的形状或数量没有假设，因此需要调整其他参数，以便生成正确的簇数量。
- en: Spectral clustering is a family of algorithms based on a measure of affinity
    among samples. They use a classical method (such as k-means) on subspaces generated
    by the Laplacian of the affinity matrix. In this way, it's possible to exploit
    the power of many kernel functions to determine the affinity between points, which
    a simple distance cannot classify correctly. This kind of clustering is particularly
    efficient for image segmentation, but it can also be a good choice whenever the
    other methods fail to separate a dataset correctly.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 谱聚类是一类基于样本之间亲和度度量的算法。它们在由亲和度矩阵的拉普拉斯算子生成的子空间上使用经典方法（如k-means）。这样，就可以利用许多核函数的力量来确定点之间的亲和度，而简单的距离无法正确分类。这种聚类对于图像分割特别有效，但也可以在其他方法无法正确分离数据集时成为一个好的选择。
- en: In the next chapter, we're going to discuss another approach called hierarchical
    clustering. It allows us to segment data by splitting and merging clusters until
    a final configuration is reached.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论另一种称为层次聚类的另一种方法。它允许我们通过分割和合并簇直到达到最终配置来分割数据。
