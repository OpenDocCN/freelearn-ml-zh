- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Applying Machine Learning Algorithms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用机器学习算法
- en: In the previous chapter, you learned about understanding data and visualization.
    It is now time to move on to the modeling phase and study machine learning algorithms!
    In the earlier chapters, you learned that building machine learning models requires
    a lot of knowledge about AWS services, data engineering, data exploration, data
    architecture, and much more. This time, you will delve deeper into the algorithms
    that have been introduced and more.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了关于数据理解和可视化的知识。现在是时候进入建模阶段，研究机器学习算法了！在早期章节中，你了解到构建机器学习模型需要大量关于AWS服务、数据工程、数据探索、数据架构等方面的知识。这次，你将更深入地研究已经介绍过的算法以及更多内容。
- en: Having a good sense of the different types of algorithms and machine learning
    approaches will put you in a very good position to make decisions during your
    projects. Of course, this type of knowledge is also crucial to the AWS Certified
    Machine Learning Specialty exam.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对不同类型算法和机器学习方法的良好理解将使你在项目决策中处于非常有利的地位。当然，这种知识对于AWS认证机器学习专业考试也是至关重要的。
- en: Bear in mind that there are thousands of algorithms out there. You can even
    propose your own algorithm for a particular problem. In this chapter, you will
    learn about the most relevant ones and, hopefully, the ones that you will probably
    face in the exam.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，有数千种算法存在。你甚至可以为特定问题提出自己的算法。在本章中，你将了解最相关的算法，并希望这些算法你可能在考试中遇到。
- en: 'The main topics of this chapter are as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要内容包括以下几方面：
- en: Storing the training data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储训练数据
- en: A word about ensemble models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于集成模型的说明
- en: 'Supervised learning:'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习：
- en: Regression models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归模型
- en: Classification models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类模型
- en: Forecasting models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测模型
- en: Object2Vec
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Object2Vec
- en: 'Unsupervised learning:'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习：
- en: Clustering
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: Anomaly detection
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测
- en: Dimensionality reduction
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度约简
- en: IP Insights
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IP洞察
- en: Textual analysis (natural language processing)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分析（自然语言处理）
- en: Image processing
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像处理
- en: Reinforcement learning
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: Alright, grab a coffee and rock it!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，拿杯咖啡，享受这个过程吧！
- en: Introducing this chapter
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍本章
- en: During this chapter, you will read about several algorithms, modeling concepts,
    and learning strategies. All these topics are beneficial for you to know for the
    exam and throughout your career as a data scientist.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解几种算法、建模概念和学习策略。所有这些话题对于你在考试和作为数据科学家职业生涯中的知识都是有益的。
- en: This chapter has been structured in such a way that it not only covers the necessary
    topics of the exam but also gives you a good sense of the most important learning
    strategies out there. For example, the exam will check your knowledge regarding
    the basic concepts of K-Means. However, this chapter will cover it on a much deeper
    level, since this is an important topic for your career as a data scientist.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的结构不仅涵盖了考试所需的必要主题，还让你对最重要的学习策略有一个很好的了解。例如，考试将检查你对K-Means基本概念的了解。然而，本章将更深入地探讨这一点，因为这对于你作为数据科学家的职业生涯来说是一个重要的话题。
- en: 'The chapter will follow this approach of looking deeper into the algorithms’
    logic for some types of models that every data scientist should master. Furthermore,
    keep this in mind: sometimes you may go deeper than what is expected of you in
    the exam, but that will be extremely important for you in your career.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将采用深入探讨某些类型模型算法逻辑的方法，这些模型是每位数据科学家都应该掌握的。此外，请记住：有时你可能需要比考试预期更深入地学习，但这对你职业生涯来说将极其重要。
- en: Many times during this chapter, you will see the term **built-in algorithms**.
    This term will be used to refer to the list of algorithms implemented by AWS on
    their SageMaker SDK.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将多次看到术语**内置算法**。这个术语将用来指代AWS在他们的SageMaker SDK中实现的算法列表。
- en: 'Here is a concrete example: you can use scikit-learn’s **K-nearest neighbors**
    algorithm, or KNN for short (if you don’t remember what scikit-learn is, refresh
    your memory by going back to [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*,
    Machine Learning Fundamentals*) to create a classification model and deploy it
    to SageMaker. However, AWS also offers its own implementation of the KNN algorithm
    on its SDK, which is optimized to run in the AWS environment. Here, KNN is an
    example of a built-in algorithm.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个具体的例子：你可以使用scikit-learn的**K最近邻**算法，或简称KNN（如果你不记得scikit-learn是什么，可以通过回到[*第1章*](B21197_01.xhtml#_idTextAnchor018)*“机器学习基础”来刷新你的记忆）来创建一个分类模型并将其部署到SageMaker。然而，AWS也在其SDK中提供了KNN算法的自己的实现，该实现针对AWS环境进行了优化。在这里，KNN是一个内置算法的例子。
- en: 'The possibilities on AWS are endless because you can either take advantage
    of built-in algorithms or bring in your own algorithm to create models on SageMaker.
    Finally, just to make this very clear, here is an example of how to import a built-in
    algorithm from the AWS SDK:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS上的可能性是无限的，因为你可以利用内置算法，或者引入你自己的算法来在SageMaker上创建模型。最后，为了使这一点非常清楚，这里有一个从AWS
    SDK导入内置算法的例子：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You will learn how to create models on SageMaker in [*Chapter 9*](B21197_09.xhtml#_idTextAnchor1224)*,
    Amazon SageMaker Modeling*. For now, just understand that AWS has its own set
    of libraries where those built-in algorithms are implemented.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在[*第9章*](B21197_09.xhtml#_idTextAnchor1224)*“Amazon SageMaker建模”中学习如何在SageMaker上创建模型。目前，你只需了解AWS有一套自己的库，其中实现了这些内置算法。
- en: To train and evaluate a model, you need training and testing data. After instantiating
    your estimator, you should then feed it with those datasets. Not to spoil [*Chapter
    9*](B21197_09.xhtml#_idTextAnchor1224)*, Amazon SageMaker Modeling*, but you should
    know about the concept of **data channels** in advance.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练和评估一个模型，你需要训练数据和测试数据。在实例化你的估计器之后，你应该用这些数据集来喂养它。不要破坏[*第9章*](B21197_09.xhtml#_idTextAnchor1224)*“Amazon
    SageMaker建模”，但你应该提前了解**数据通道**的概念。
- en: Data channels are configurations related to input data that you can pass to
    SageMaker when you are creating a training job. You should set these configurations
    just to inform SageMaker of how your input data is formatted.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据通道是与输入数据相关的配置，你可以在创建训练作业时将其传递给SageMaker。你应该设置这些配置，只是为了通知SageMaker你的输入数据是如何格式化的。
- en: In [*Chapter 9*](B21197_09.xhtml#_idTextAnchor1224)*, Amazon SageMaker Modeling*,
    you will learn how to create training jobs and how to set data channels. As of
    now, you should know that while configuring data channels, you can set a `ContentType`)
    and an `TrainingInputMode`). You will now take a closer look at how and where
    the training data should be stored so that it can be integrated properly with
    AWS’s built-in algorithms.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第9章*](B21197_09.xhtml#_idTextAnchor1224)*“Amazon SageMaker建模”中，你将学习如何创建训练作业以及如何设置数据通道。到目前为止，你应该知道在配置数据通道时，你可以设置一个`ContentType`和一个`TrainingInputMode`。现在，你将更详细地了解训练数据应该如何存储，以便能够与AWS的内置算法正确集成。
- en: Storing the training data
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储训练数据
- en: First of all, you can use multiple AWS services to prepare data for machine
    learning, such as **Elastic MapReduce (EMR),** Redshift, Glue, and so on. After
    preprocessing the training data, you should store it in S3, in a format expected
    by the algorithm you are using. *Table 6.1* shows the list of acceptable data
    formats per algorithm.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你可以使用多个AWS服务来准备机器学习的数据，例如**弹性映射减少（EMR）**，Redshift，Glue等。在预处理训练数据后，你应该将其存储在S3中，以算法期望的格式存储。*表6.1*显示了每个算法可接受的数据格式列表。
- en: '| **Data format** | **Algorithm** |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| **数据格式** | **算法** |'
- en: '| `Application/x-image` | Object detection algorithm, semantic segmentation
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| `Application/x-image` | 目标检测算法，语义分割 |'
- en: '| `Application/x-recordio` | Object detection algorithm |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| `Application/x-recordio` | 目标检测算法 |'
- en: '| `Application/x-recordio-protobuf` | Factorization machines, K-Means, KNN,
    latent Dirichlet allocation, linear learner, NTM, PCA, RCF, sequence-to-sequence
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| `Application/x-recordio-protobuf` | 因子分解机，K-Means，KNN，潜在狄利克雷分配，线性学习器，NTM，PCA，RCF，序列到序列
    |'
- en: '| `Application/jsonlines` | BlazingText, DeepAR |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| `Application/jsonlines` | BlazingText, DeepAR |'
- en: '| `Image/.jpeg` | Object detection algorithm, semantic segmentation |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| `Image/.jpeg` | 目标检测算法，语义分割 |'
- en: '| `Image/.png` | Object detection algorithm, semantic segmentation |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| `Image/.png` | 目标检测算法，语义分割 |'
- en: '| `Text/.csv` | IP Insights, K-Means, KNN, latent Dirichlet allocation, linear
    learner, NTM, PCA, RCF, XGBoost |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| `Text/.csv` | IP Insights, K-Means, KNN, 潜在狄利克雷分配，线性学习器，NTM，PCA，RCF，XGBoost
    |'
- en: '| `Text/.libsvm` | XGBoost |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| `Text/.libsvm` | XGBoost |'
- en: Table 6.1 – Data formats that are acceptable per AWS algorithm
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 – AWS算法可接受的每种数据格式
- en: 'As you can see, many algorithms accept `Text/.csv` format. You should follow
    these rules if you want to use that format:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，许多算法接受`Text/.csv`格式。如果你想使用该格式，你应该遵循以下规则：
- en: Your CSV file *cannot* have a header record.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的CSV文件*不能*有标题记录。
- en: For supervised learning, the target variable must be in the first column.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于监督学习，目标变量必须在第一列。
- en: While configuring the training pipeline, set the input data channel as `content_type`
    equal to `text/csv`.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在配置训练管道时，将输入数据通道设置为`content_type`等于`text/csv`。
- en: 'For unsupervised learning, set `label_size` within `content_type`, as follows:
    `''content_type=text/csv;label_size=0''`.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于无监督学习，在`content_type`中设置`label_size`，如下所示：`'content_type=text/csv;label_size=0'`。
- en: Although `text/.csv` format is fine for many use cases, most of the time, AWS’s
    built-in algorithms work better with `recordIO-protobuf`. This is an optimized
    data format that is used to train AWS’s built-in algorithms, where SageMaker converts
    each observation in the dataset into a binary representation that is a set of
    4-byte floats.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对于许多用例来说，`text/.csv`格式是可行的，但大多数情况下，AWS的内置算法与`recordIO-protobuf`配合得更好。这是一种用于训练AWS内置算法的优化数据格式，其中SageMaker将数据集中的每个观测值转换为二进制表示，即一组4字节的浮点数。
- en: 'RecordIO-protobuf accepts two types of input modes: pipe mode and file mode.
    In pipe mode, the data will be streamed directly from S3, which helps optimize
    storage. In file mode, the data is copied from S3 to the training instance’s store
    volume.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: RecordIO-protobuf接受两种输入模式：管道模式和文件模式。在管道模式下，数据将直接从S3流式传输，这有助于优化存储。在文件模式下，数据将从S3复制到训练实例的存储卷中。
- en: You are almost ready! Now you can take a quick look at some modeling definitions
    that will help you understand some more advanced algorithms.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你几乎准备好了！现在你可以快速查看一些建模定义，这将帮助你理解一些更高级的算法。
- en: A word about ensemble models
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于集成模型的一些话
- en: Before you start diving into the algorithms, there is an important modeling
    concept that you should be aware of – **ensemble**. The term ensemble is used
    to describe methods that use multiple algorithms to create a model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开始深入研究算法之前，有一个重要的建模概念你应该了解——**集成**。集成这个术语用来描述使用多个算法来创建模型的方法。
- en: A regular algorithm that *does not* implement ensemble methods will rely on
    a single model to train and predict the target variable. That is what happens
    when you create a decision tree or regression model. On the other hand, algorithms
    that *do* implement ensemble methods will rely on multiple models to predict the
    target variable. In that case, since each of these models might come up with a
    different prediction for the target variable, ensemble algorithms implement either
    a voting (for classification models) or averaging (for regression models) system
    to output the final results. *Table 6.2* illustrates a very simple voting system
    for an ensemble algorithm composed of three models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个不实现集成方法的常规算法将依赖于单个模型来训练和预测目标变量。这就是当你创建决策树或回归模型时发生的情况。另一方面，实现集成方法的算法将依赖于多个模型来预测目标变量。在这种情况下，由于每个模型可能会对目标变量提出不同的预测，集成算法实现了一个投票系统（用于分类模型）或平均系统（用于回归模型）来输出最终结果。*表6.2*展示了由三个模型组成的集成算法的一个非常简单的投票系统。
- en: '| **Transaction** | **Model A** | **Model B** | **Model C** | **Prediction**
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| **交易** | **模型A** | **模型B** | **模型C** | **预测** |'
- en: '| 1 | Fraud | Fraud | Not Fraud | Fraud |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 欺诈 | 欺诈 | 非欺诈 | 欺诈 |'
- en: '| 2 | Not Fraud | Not Fraud | Not Fraud | Not Fraud |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 非欺诈 | 非欺诈 | 非欺诈 | 非欺诈 |'
- en: '| 3 | Fraud | Fraud | Fraud | Fraud |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 欺诈 | 欺诈 | 欺诈 | 欺诈 |'
- en: '| 4 | Not Fraud | Not Fraud | Fraud | Not Fraud |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 非欺诈 | 非欺诈 | 欺诈 | 非欺诈 |'
- en: Table 6.2 – An example of a voting system on ensemble methods
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2 – 集成方法中投票系统的一个示例
- en: As described before, the same approach works for regression problems, where
    instead of voting, it could average the results of each model and use that as
    the outcome.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，同样的方法也适用于回归问题，其中不是投票，而是可以平均每个模型的预测结果，并使用该结果作为输出。
- en: Voting and averaging are just two examples of ensemble approaches. Other powerful
    techniques include blending and stacking, where you can create multiple models
    and use the outcome of each model as a feature for a main model. Looking back
    at *Table 6.2*, columns *Model A*, *Model B*, and *Model C* could be used as features
    to predict the final outcome.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 投票和平均只是集成方法的两个例子。其他强大的技术包括混合和堆叠，其中你可以创建多个模型，并将每个模型的输出作为主模型的特征。回顾一下 *表 6.2*，*模型
    A*、*模型 B* 和 *模型 C* 的列可以用作预测最终结果的特性。
- en: 'It turns out that many machine learning algorithms use ensemble methods while
    training, in an embedded way. These algorithms can be classified into two main
    categories:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，许多机器学习算法在训练过程中使用集成方法，以嵌入式的方式。这些算法可以分为两大类：
- en: '**Bootstrapping aggregation** or **bagging**: With this approach, several models
    are trained on top of different samples of data. Predictions are then made through
    the voting or averaging system. The most popular algorithm from this category
    is known as **Random Forest**.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自助聚合** 或 **袋装法**：这种方法中，在数据的不同样本上训练多个模型。然后，通过投票或平均系统进行预测。这个类别中最受欢迎的算法被称为 **随机森林**。'
- en: '**Boosting**: With this approach, several models are trained on top of different
    samples of the data. One model then tries to correct the error of the next model
    by penalizing incorrect predictions. The most popular algorithms from this category
    are **stochastic gradient boosting** and **AdaBoost**.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升法**：这种方法中，在数据的不同样本上训练多个模型。然后，一个模型试图通过惩罚错误预测来纠正下一个模型的错误。这个类别中最受欢迎的算法是 **随机梯度提升**
    和 **AdaBoost**。'
- en: Now that you know what ensemble models are, you can look at some machine learning
    algorithms that are likely to be present in your exam. Not all of them use ensemble
    approaches.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了集成模型，你可以看看一些可能出现在你的考试中的机器学习算法。并非所有这些算法都使用集成方法。
- en: 'The next few sections are split based on AWS algorithm categories, as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下几节将根据 AWS 算法类别进行划分，如下所示：
- en: Supervised learning
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised learning
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Textual analysis
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分析
- en: Image processing
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像处理
- en: Finally, you will have an overview of reinforcement learning on AWS.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将了解 AWS 上的强化学习概述。
- en: Supervised learning
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'AWS provides supervised learning algorithms for general purposes (regression
    and classification tasks) and more specific purposes (forecasting and vectorization).
    The list of built-in algorithms that can be found in these sub-categories is as
    follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 为通用目的（回归和分类任务）和更具体的目的（预测和向量化）提供了监督学习算法。以下是在这些子类别中可以找到的内置算法列表：
- en: Linear learner algorithm
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性学习算法
- en: Factorization machines algorithm
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因子分解机算法
- en: XGBoost algorithm
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost 算法
- en: KNN algorithm
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KNN 算法
- en: Object2Vec algorithm
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Object2Vec 算法
- en: DeepAR forecasting algorithm
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepAR 预测算法
- en: You will start by learning about regression models and the linear learner algorithm.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你将首先学习关于回归模型和线性学习算法。
- en: Working with regression models
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与回归模型一起工作
- en: Looking at **linear regression** models is a nice way to understand what is
    going on inside **regression models** in general (linear and non-linear regression
    models). This is mandatory knowledge for every data scientist and can help you
    solve real challenges as well. You will now take a closer look at this in the
    following subsections.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 **线性回归** 模型是理解一般回归模型（线性回归和非线性回归模型）内部情况的好方法。这是每位数据科学家必备的知识，也能帮助你解决实际问题。你将在接下来的小节中对此进行更深入的了解。
- en: Introducing regression algorithms
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍回归算法
- en: Linear regression models aim to predict a numeric value (*y*) according to one
    or more variables (*x*). Mathematically, such a relationship can be defined as
    *y = f(x)*, where *y* is known as the **dependent variable** and *x* is known
    as the **independent variable**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型旨在根据一个或多个变量（*x*）预测一个数值（*y*）。从数学上讲，这种关系可以定义为 *y = f(x)*，其中 *y* 被称为 **因变量**，*x*
    被称为 **自变量**。
- en: With regression models, the component that you want to predict (*y*) is always
    a continuous number – for example, the price of houses or the number of transactions.
    You saw this in [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*, Machine* *Learning
    Fundamentals*, in *Figure 1**.2*, when you were learning about the right type
    of supervised learning algorithm, given the target variable. Please feel free
    to go back and review it.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归模型中，你想要预测的组成部分（*y*）始终是一个连续的数字——例如，房屋价格或交易数量。你在[*第一章*](B21197_01.xhtml#_idTextAnchor018)*，机器学习基础*，*图1**.2中看到了这一点，当你学习关于给定目标变量的正确监督学习算法时。请随时回去复习。
- en: When you use *just one variable to predict y*, this problem is referred to as
    **simple linear regression**. On the other hand, when you use *more than one variable
    to predict y*, you have a **multiple linear** **regression** problem.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当你只用一个变量来预测*y*时，这个问题被称为**简单线性回归**。另一方面，当你使用多个变量来预测*y*时，你面临的是一个**多元线性****回归**问题。
- en: There is also another class of regression models, known as **non-linear regression**.
    However, let us put that aside for a moment and understand what simple linear
    regression means.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一种被称为**非线性回归**的回归模型类别。然而，让我们暂时将其放在一边，先来理解一下简单线性回归的含义。
- en: Regression models belong to the supervised side of machine learning (the other
    side is non-supervised) because algorithms try to predict values according to
    existing correlations between independent and dependent variables.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型属于机器学习的监督方面（另一方面是非监督），因为算法试图根据独立变量和依赖变量之间的现有相关性来预测值。
- en: 'But what does *f* mean in *y=f(x)*? Here, *f* is the regression function responsible
    for predicting *y* based on *x*. In other words, this is the function that you
    want to find. When talking about simple linear regression, pay attention to the
    next three questions and answers:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 但在*y=f(x)*中，*f*代表什么？在这里，*f*是负责根据*x*预测*y*的回归函数。换句话说，这就是你想要找到的函数。在谈论简单线性回归时，请注意以下三个问题和答案：
- en: What is the shape of *f* in linear regression?
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归中*f*的形状是什么？
- en: Linear, of course!
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当然是线性的！
- en: How can you represent a linear relationship?
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何表示线性关系？
- en: Using a *straight* line (you will understand why in a few minutes).
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用一条*直线*（你将在几分钟内理解原因）。
- en: So what is the function that defines a line?
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那么，定义一条线的函数是什么？
- en: '*ax + b* (just check any *mathematics* book).'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*ax + b*（只需查看任何一本*数学*书）。'
- en: That is it! Linear regression models are given by *y = ax + b*. When you are
    trying to predict *y* given *x*, you just need to find out the values of *a* and
    *b*. You can adopt the same logic to figure out what is going on inside other
    kinds of regression.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了！线性回归模型由*y = ax + b*给出。当你试图根据*x*预测*y*时，你只需要找出*a*和*b*的值。你可以采用相同的逻辑来了解其他类型回归内部的情况。
- en: Finding out the values of *a* and *b* is the only thing you are going to do.
    It is nice to know that *a* is also known as the **alpha coefficient**, or **slope**,
    and represents the line’s inclination, while *b* is also known as the **beta coefficient**,
    or **y intercept**, and represents the place where the line crosses the *y* axis
    (into a two-dimensional plane consisting of *x* and *y*). You will learn about
    these two terms in a later subsection.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 找出*a*和*b*的值是你唯一要做的事情。值得知道的是，*a*也被称为**alpha系数**，或**斜率**，表示线的倾斜度，而*b*也被称为**beta系数**，或**y截距**，表示线与*y*轴交叉的位置（进入由*x*和*y*组成的二维平面）。你将在下一小节中了解这两个术语。
- en: It is also nice to know that there is a bias (*e*) associated with every predictor
    that you do not have control over. That being said, the formal definition of simple
    linear regression is given by *y = ax + b +* *e*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个值得知道的事实，每个你无法控制的预测因子都存在一个偏差（*e*）。换句话说，简单线性回归的正式定义是*y = ax + b +* *e*。
- en: In the next subsection, you will learn how to find alpha and beta to solve a
    simple linear regression problem.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，你将学习如何找到alpha和beta来解决简单线性回归问题。
- en: Least squares method
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小二乘法
- en: 'There are different ways to find the slope and *y* intercept of a line, but
    the most used method is known as the **least squares method**. The principle behind
    this method is simple: you have to find the *best line that reduces the sum of*
    *squared error*.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方法来找到直线的斜率和*y*截距，但最常用的方法是**最小二乘法**。这个方法背后的原理很简单：你必须找到一条*最佳线*，以减少*平方误差*的总和。
- en: 'In *Figure 6**.1*, you can see a Cartesian plane with multiple points and lines
    in it. *Line a* represents the best fit for this data – in other words, that would
    be the best linear regression function for those points. But how can you know
    that? It is simple: if you compute the error associated with each point, you will
    realize that *Line a* contains the least sum of squared errors.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6*.1中，你可以看到一个带有多个点和线的笛卡尔平面。*线a*代表这些数据点的最佳拟合线——换句话说，那将是这些点的最佳线性回归函数。但你怎么知道呢？很简单：如果你计算每个点的误差，你就会发现*线a*包含了最小的平方误差总和。
- en: '![Figure 6.1 – Visualizing the principle of the least squares method](img/B21197_06_01.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 最小二乘法原理的可视化](img/B21197_06_01.jpg)'
- en: Figure 6.1 – Visualizing the principle of the least squares method
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 最小二乘法原理的可视化
- en: It is worth understanding linear regression from scratch not only for the certification
    exam but also for your career as a data scientist. To provide you with a complete
    example, a spreadsheet containing all the calculations that you are going to see
    has been developed! You are encouraged to jump on this support material and perform
    some simulations. In any case, you will see these calculations in action in the
    next subsection.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始理解线性回归不仅对认证考试很重要，而且对你的数据科学家职业生涯也至关重要。为了提供一个完整的示例，已经开发了一个包含你将要看到的全部计算的电子表格！鼓励你利用这份辅助材料进行一些模拟。无论如何，你将在下一小节中看到这些计算的实际应用。
- en: Creating a linear regression model from scratch
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从零开始创建线性回归模型
- en: 'You are going to use a very simple dataset, with only two variables:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用一个非常简单的数据集，其中只有两个变量：
- en: '*x*: Represents the person’s number of years of work experience'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*：代表一个人的工作年限'
- en: '*y*: Represents the person’s average salary'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*：代表一个人的平均工资'
- en: You want to understand the relationship between *x* and *y* and, if possible,
    predict the salary (*y*) based on years of experience (*x*). Real problems very
    often have far more independent variables and are not necessarily linear. However,
    this example will give you the baseline knowledge to master more complex algorithms.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要了解*x*和*y*之间的关系，如果可能的话，根据工作经验*x*预测工资(*y*)。现实问题通常有更多的独立变量，并且不一定呈线性。然而，这个例子将为你掌握更复杂的算法提供基础知识。
- en: To find out what the alpha and beta coefficients are (or slope and *y* intercept
    if you prefer), you need to find some statistics related to the dataset. In *Table
    6.3*, you have the data and these auxiliary statistics.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出alpha和beta系数（或者如果你更喜欢，斜率和*y*截距），你需要找到与数据集相关的某些统计信息。在*表6.3*中，你有数据以及这些辅助统计信息。
- en: '| **X (****INDEPENDENT)** | **Y (****DEPENDENT)** | **X MEAN** | **Y MEAN**
    | **COVARIANCE  (****X,Y)** | **X VARIANCE** | **Y VARIANCE** |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **X (****独立变量)** | **Y (****依赖变量)** | **X 平均值** | **Y 平均值** | **协方差  (****X,Y)**
    | **X 方差** | **Y 方差** |'
- en: '| *1* | *1.000* |  |  | 21.015 | 20 | 21.808.900 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| *1* | *1.000* |  |  | 21.015 | 20 | 21.808.900 |'
- en: '| *2* | *1.500* |  |  | 14.595 | 12 | 17.388.900 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| *2* | *1.500* |  |  | 14.595 | 12 | 17.388.900 |'
- en: '| *3* | *3.700* |  |  | 4.925 | 6 | 3.880.900 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| *3* | *3.700* |  |  | 4.925 | 6 | 3.880.900 |'
- en: '| *4* | *5.000* |  |  | 1.005 | 2 | 448.900 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| *4* | *5.000* |  |  | 1.005 | 2 | 448.900 |'
- en: '| *5* | *4.000* |  |  | 835 | 0 | 2.788.900 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| *5* | *4.000* |  |  | 835 | 0 | 2.788.900 |'
- en: '| *6* | *6.500* |  |  | 415 | 0 | 688.900 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| *6* | *6.500* |  |  | 415 | 0 | 688.900 |'
- en: '| *7* | *7.000* |  |  | 1.995 | 2 | 1.768.900 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| *7* | *7.000* |  |  | 1.995 | 2 | 1.768.900 |'
- en: '| *8* | *9.000* |  |  | 8.325 | 6 | 11.088.900 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| *8* | *9.000* |  |  | 8.325 | 6 | 11.088.900 |'
- en: '| *9* | *9.000* |  |  | 11.655 | 12 | 11.088.900 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| *9* | *9.000* |  |  | 11.655 | 12 | 11.088.900 |'
- en: '| *10* | *10.000* |  |  | 19.485 | 20 | 18.748.900 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| *10* | *10.000* |  |  | 19.485 | 20 | 18.748.900 |'
- en: '| COUNT | 10 | **5,50** | **5.670,00** | **8.425,00** | **8,25** | **8.970.100,00**
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| COUNT | 10 | **5,50** | **5.670,00** | **8.425,00** | **8,25** | **8.970.100,00**
    |'
- en: Table 6.3 – Dataset to predict average salary based on the amount of work experience
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.3 – 基于工作经验预测平均工资的数据集
- en: 'As you can see, there is an almost perfect linear relationship between *x*
    and *y*. As the amount of work experience increases, so does the salary. In addition
    to *x* and *y*, you need to compute the following statistics: the number of records,
    the mean of *x*, the mean of *y*, the covariance of *x* and *y*, the variance
    of *x*, and the variance of *y*. *Figure 6**.2 * depicts formulas that provide
    a mathematical representation of variance and covariance (respectively), where
    *x bar*, *y bar*, and *n* represent the mean of *x*, the mean of *y*, and the
    number of records, respectively:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，*x* 和 *y* 之间存在几乎完美的线性关系。随着工作经验的增加，工资也随之增加。除了 *x* 和 *y*，你还需要计算以下统计量：记录数、*x*
    的平均值、*y* 的平均值、*x* 和 *y* 的协方差、*x* 的方差和 *y* 的方差。*图 6.2* 描述了提供方差和协方差（分别）的数学表示的公式，其中
    *x bar*、*y bar* 和 *n* 分别代表 *x* 的平均值、*y* 的平均值和记录数：
- en: '![Figure 6.2 – Mathematical representation of variance and covariance respectively](img/B21197_06_02.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – 方差和协方差的数学表示](img/B21197_06_02.jpg)'
- en: Figure 6.2 – Mathematical representation of variance and covariance respectively
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 方差和协方差的数学表示
- en: If you want to check the calculation details of the formulas for each of those
    auxiliary statistics in *Table 6.2*, please refer to the support material provided
    along with this book. There, you will find these formulas already implemented
    for you.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想检查 *表 6.2* 中那些辅助统计量的公式的计算细节，请参阅本书附带的支持材料。在那里，你会发现这些公式已经为你实现了。
- en: These statistics are important because they will be used to compute the alpha
    and beta coefficients. *Figure 6**.3* explains how you can compute both coefficients,
    along with the correlation coefficients R and R squared. These last two metrics
    will give you an idea about the quality of the model, where the closer they are
    to 1, the better the model is.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些统计量很重要，因为它们将被用来计算 alpha 和 beta 系数。*图 6.3* 解释了如何计算这两个系数，以及相关系数 R 和 R 平方。这两个指标将给你一个关于模型质量的概念，它们越接近
    1，模型就越好。
- en: '![Figure 6.3 – Equations to calculate coefficients for simple linear regression](img/B21197_06_03.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – 计算简单线性回归系数的方程](img/B21197_06_03.jpg)'
- en: Figure 6.3 – Equations to calculate coefficients for simple linear regression
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – 计算简单线性回归系数的方程
- en: 'After applying these formulas, you will come up with the results shown in *Table
    6.4*. It already contains all the information that you need to make predictions,
    on top of the new data. If you replace the coefficients in the original equation,
    *y = ax + b + e*, you will find the regression formula to be as follows: *y =
    1021.212 * x +* *53.3*.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这些公式后，你将得到 *表 6.4* 中所示的结果。它已经包含了你所需的所有信息，以便在新的数据上做出预测。如果你将系数替换到原始方程 *y = ax
    + b + e* 中，你会发现回归公式如下：*y = 1021.212 * x + 53.3*。
- en: '| **Coefficient** | **Description** | **Value** |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| **系数** | **描述** | **值** |'
- en: '| Alpha | Line inclination | 1,021,212,121 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Alpha | 线性斜率 | 1,021,212,121 |'
- en: '| Beta | Interceptor | 53 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Beta | 截距 | 53 |'
- en: '| R | Correlation | 0,979,364,354 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| R | 相关系数 | 0,979,364,354 |'
- en: '| R^2 | Determination | 0,959,154,538 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| R^2 | 决定系数 | 0,959,154,538 |'
- en: Table 6.4 – Finding regression coefficients
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.4 – 求回归系数
- en: From this point on, to make predictions, all you have to do is replace *x* with
    the number of years of experience. As a result, you will find *y*, which is the
    projected salary. You can see the model fit in *Figure 6**.4* and some model predictions
    in *Table 6.5*.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，为了进行预测，你只需要将 *x* 替换为工作经验的年数。结果，你会发现 *y*，即预测的工资。你可以在 *图 6.4* 中看到模型拟合，以及在
    *表 6.5* 中的一些模型预测。
- en: '![Figure 6.4 – Fitting data in the regression equation](img/B21197_06_04.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 回归方程中的拟合数据](img/B21197_06_04.jpg)'
- en: Figure 6.4 – Fitting data in the regression equation
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 回归方程中的拟合数据
- en: '| **INPUT** | **PREDICTION** | **ERROR** |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| **输入** | **预测** | **误差** |'
- en: '| --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 1.075 | 75 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.075 | 75 |'
- en: '| 2 | 2.096 | 596 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2.096 | 596 |'
- en: '| 3 | 3.117 | - 583 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 3.117 | - 583 |'
- en: '| 4 | 4.138 | - 862 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 4.138 | - 862 |'
- en: '| 5 | 5.159 | 1.159 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 5.159 | 1.159 |'
- en: '| 6 | 6.181 | - 319 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 6.181 | - 319 |'
- en: '| 7 | 7.202 | 202 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 7.202 | 202 |'
- en: '| 8 | 8.223 | - 777 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 8.223 | - 777 |'
- en: '| 9 | 9.244 | 244 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 9.244 | 244 |'
- en: '| 10 | 10.265 | 265 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 10.265 | 265 |'
- en: '| 11 | 11.287 |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 11.287 |  |'
- en: '| 12 | 12.308 |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 12.308 |  |'
- en: '| 13 | 13.329 |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 13.329 |  |'
- en: '| 14 | 14.350 |  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 14.350 |  |'
- en: '| 15 | 15.372 |  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 15.372 |  |'
- en: '| 16 | 16.393 |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 16.393 |  |'
- en: '| 17 | 17.414 |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 17 | 17.414 |  |'
- en: '| 18 | 18.435 |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 18 | 18.435 |  |'
- en: '| 19 | 19.456 |  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 19 | 19.456 |  |'
- en: '| 20 | 20.478 |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 20.478 |  |'
- en: Table 6.5 – Model predictions
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.5 – 模型预测
- en: While you are analyzing regression models, you should be able to know whether
    your model is of good quality or not. You read about many modeling issues (such
    as overfitting) in [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*, Machine Learning
    Fundamentals*, and you already know that you always have to check model performance.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当你分析回归模型时，你应该能够知道你的模型是否质量良好。你在[*第一章*](B21197_01.xhtml#_idTextAnchor018)*机器学习基础*中了解到许多建模问题（例如过拟合），并且你已经知道你总是要检查模型性能。
- en: A good approach to regression models is performing what is called residual analysis.
    This is where you plot the errors of the model in a scatter plot and check whether
    they are randomly distributed (as expected) or not. If the errors are *not* randomly
    distributed, this means that your model was unable to generalize the data. *Figure
    6**.5* shows a residual analysis based on the data from *Table 6.5*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型的一个良好方法是进行所谓的残差分析。这就是你在散点图上绘制模型的误差，并检查它们是否随机分布（如预期的那样）或不随机分布。如果误差不是随机分布的，这意味着你的模型无法泛化数据。"图6.5**.5*"显示了基于"表6.5"数据的残差分析。
- en: '![Figure 6.5 – Residual analysis](img/B21197_06_05.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 残差分析](img/B21197_06_05.jpg)'
- en: Figure 6.5 – Residual analysis
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 残差分析
- en: The takeaway here is that the errors are randomly distributed. Such evidence,
    along with a high R squared rating, can be used as arguments to support the use
    of this model.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的要点是误差是随机分布的。这样的证据，加上高R平方评分，可以用作支持使用此模型的论据。
- en: Important note
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In [*Chapter 7*](B21197_07.xhtml#_idTextAnchor970)*, Evaluating and Optimizing
    Models*, you will learn about evaluation metrics. For instance, you will learn
    that each type of model may have its own set of evaluation metrics. Regression
    models are commonly evaluated with **Mean Squared Error (MSE)** and **Root Mean
    Squared Error (RMSE)**. In other words, apart from R, R squared, and residual
    analysis, ideally, you will execute your model on test sets to extract other performance
    metrics. You can even use a cross-validation system to check model performance,
    as you learned in [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*, Machine* *Learning
    Fundamentals*.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第七章*](B21197_07.xhtml#_idTextAnchor970)*评估和优化模型*中，你将了解评估指标。例如，你将了解到每种类型的模型可能都有自己的评估指标集。回归模型通常使用**均方误差（MSE）**和**均方根误差（RMSE）**进行评估。换句话说，除了R、R平方和残差分析之外，理想情况下，你将在测试集上执行你的模型以提取其他性能指标。你甚至可以使用交叉验证系统来检查模型性能，正如你在[*第一章*](B21197_01.xhtml#_idTextAnchor018)*机器学习基础*中学到的。
- en: Very often, when the model residuals *do* present a pattern and are *not* randomly
    distributed, it is because the existing relationship in the data is not linear,
    but non-linear, so another modeling technique must be applied. In the next subsection,
    you will learn how you can interpret regression models.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 非常常见的是，当模型残差确实呈现模式并且不是随机分布时，这是因为数据中现有的关系不是线性的，而是非线性的，因此必须应用另一种建模技术。在下一小节中，你将学习如何解释回归模型。
- en: Interpreting regression models
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释回归模型
- en: It is also good to know how to interpret a linear regression model. Sometimes,
    you use linear regression not necessarily to create a predictive model but to
    do a regression analysis. You can then use regression analysis to understand the
    relationship between the independent and dependent variables.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 了解如何解释线性回归模型也是很好的。有时，你使用线性回归不一定是为了创建预测模型，而是为了进行回归分析。然后你可以使用回归分析来理解自变量和因变量之间的关系。
- en: 'Looking back at the regression equation (*y = 1021.212 * x + 53.30*), you can
    see the two terms: alpha or slope (*1021.20*) and beta or *y* intercept (*53.3*).
    You can interpret this model as follows: *for each additional year of working
    experience, you will increase your salary by $1,021.20*. Also, note that when
    “years of experience” is equal to 0, the expected salary is going to be $53.30
    (this is the point where the straight line crosses the *y* axis).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾回归方程（*y = 1021.212 * x + 53.30*），你可以看到两个术语：alpha或斜率（*1021.20*）和beta或*y*截距（*53.3*）。你可以这样解释这个模型：*对于每增加一年工作经验，你的薪水将增加1,021.20美元*。此外，请注意，当“工作经验年数”等于0时，预期的薪水将是53.30美元（这是直线与*y*轴相交的点）。
- en: 'From a broad perspective, your regression analysis should answer the following
    question: for each extra unit that is added to the independent variable (slope),
    what is the average change in the dependent variable?'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个更广泛的角度来看，你的回归分析应该回答以下问题：对于每个添加到自变量（斜率）的额外单位，因变量的平均变化是多少？
- en: Checking adjusted R squared
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查调整后的R平方
- en: At this point, you have a much better idea of regression models! There is just
    one other very important topic that you should be aware of, regardless of whether
    it will come up in the exam or not, which is the parsimony aspect of your model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，你对回归模型有了更深入的了解！还有一个非常重要的主题你应该知道，无论它是否会在考试中出现，那就是你模型的经济性方面。
- en: You have already heard about parsimony in [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*,
    Machine Learning Fundamentals*. This is the ability to prioritize simple models
    over complex ones. Looking into regression models, you might have to use more
    than one feature to predict your outcome. This is also known as a multiple regression
    model.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经在[*第1章*](B21197_01.xhtml#_idTextAnchor018)*，机器学习基础*中听说过经济性。这是优先考虑简单模型而不是复杂模型的能力。在研究回归模型时，你可能需要使用多个特征来预测你的结果。这也被称为多元回归模型。
- en: When that is the case, the R and R squared coefficients tend to reward more
    complex models with more features. In other words, if you keep adding new features
    to a multiple regression model, you will come up with higher R and R squared coefficients.
    That is why you *cannot* anchor your decisions *only* based on those two metrics.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，R和R平方系数往往会奖励具有更多特征的更复杂模型。换句话说，如果你继续向多元回归模型添加新特征，你会得到更高的R和R平方系数。这就是为什么你不能仅仅基于这两个指标来做决定。
- en: Another additional metric that you could use (apart from R, R squared, MSE,
    and RMSE) is known as **adjusted R squared**. This metric is penalized when you
    add extra features to the model that do not bring any real value. In *Table 6.6*,
    you can see when a model is starting to lose parsimony.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用的一个额外指标（除了R、R平方、MSE和RMSE之外）被称为**调整后的R平方**。当你向模型添加不带来任何实际价值的额外特征时，这个指标会受到惩罚。在*表6.6*中，你可以看到模型开始失去经济性。
- en: '| **Number** **of features** | **R squared** | **Adjusted** **R squared** |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| **特征** **数量** | **R平方** | **调整后的R平方** |'
- en: '| 1 | 81 | 79 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 81 | 79 |'
- en: '| 2 | 83 | 82 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 83 | 82 |'
- en: '| 3 | 88 | 87 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 88 | 87 |'
- en: '| 4 | 90 | 86 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 90 | 86 |'
- en: '| 5 | 92 | 85 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 92 | 85 |'
- en: Table 6.6 – Comparing R squared and adjusted R squared
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.6 – 比较R平方和调整后的R平方
- en: Here, you can conclude that maintaining three variables in the model is better
    than maintaining four or five. Adding four or five variables to the model will
    increase the R squared (as expected), but decrease the adjusted R squared.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以得出结论，保持模型中的三个变量比保持四个或五个更好。向模型中添加四个或五个变量会增加R平方（正如预期的那样），但会降低调整后的R平方。
- en: At this point, you should have a very good understanding of regression models.
    Now, let us check what AWS offers in terms of built-in algorithms for this class
    of models.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该对回归模型有了很好的理解。现在，让我们看看AWS为这类模型提供的内置算法有哪些。
- en: Regression modeling on AWS
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS上的回归建模
- en: AWS has a built-in algorithm known as **linear learner**, where you can implement
    linear regression models. The built-in linear learner uses **Stochastic Gradient
    Descent (SGD)** to train the model.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: AWS有一个内置算法称为**线性学习器**，你可以用它来实现线性回归模型。内置的线性学习器使用**随机梯度下降（SGD）**来训练模型。
- en: Important note
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You will learn more about SGD when neural networks are discussed. For now, you
    can look at SGD as an alternative to the popular least squares error method that
    was just discussed.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 当讨论神经网络时，你将了解更多关于SGD的内容。现在，你可以将SGD视为之前讨论的流行的最小二乘误差方法的替代方案。
- en: The linear learner built-in algorithm provides a hyperparameter that can apply
    normalization to the data, prior to the training process. The name of this hyperparameter
    is `normalize_data`. This is very helpful since linear models are sensitive to
    the scale of the data and usually take advantage of data normalization.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 内置的线性学习器算法提供了一个超参数，可以在训练过程之前对数据进行归一化。这个超参数的名称是`normalize_data`。这非常有帮助，因为线性模型对数据的规模很敏感，通常利用数据归一化。
- en: Important note
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Data normalization was discussed in [*Chapter 4*](B21197_04.xhtml#_idTextAnchor451)*,
    Data Preparation and Transformation*. Please review that chapter if you need to.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B21197_04.xhtml#_idTextAnchor451)*，数据准备和转换*中讨论了数据归一化。如果你需要，请回顾该章节。
- en: Some other important hyperparameters of the linear learner algorithm are **L1**
    and **wd**, which play the roles of **L1 regularization** and **L2** **regularization**,
    respectively.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 线性学习器算法的一些其他重要超参数是**L1**和**wd**，分别扮演**L1正则化**和**L2正则化**的角色。
- en: L1 and L2 regularization help the linear learner (or any other regression algorithm
    implementation) to avoid overfitting. Conventionally, regression models that implement
    L1 regularization are called **lasso regression** models, while regression models
    with L2 regularization are called **ridge** **regression** models.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: L1和L2正则化有助于线性学习器（或任何其他回归算法实现）避免过度拟合。传统上，实现L1正则化的回归模型被称为**lasso回归**模型，而具有L2正则化的回归模型被称为**岭回归**模型。
- en: 'Although it might sound complex, it is not! The regression model equation is
    still the same: *y = ax + b + e*. The change is in the loss function, which is
    used to find the coefficients that best minimize the error. If you look back at
    *Figure 6**.1*, you will see that the error function is defined as *e = (ŷ -
    y)^2*, where *ŷ* is the regression function value and *y* is the real value.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能听起来很复杂，但实际上并不复杂！回归模型方程仍然是相同的：*y = ax + b + e*。变化在于损失函数，它用于找到最佳最小化误差的系数。如果你回顾*图6.1*，你会看到误差函数被定义为*e
    = (ŷ - y)^2*，其中*ŷ*是回归函数值，*y*是真实值。
- en: 'L1 and L2 regularization add a penalty term to the loss function, as shown
    in the formulas in *Figure 6**.6* (note that you are replacing *ŷ* with *ax +*
    *b*):'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: L1和L2正则化将惩罚项添加到损失函数中，如*图6.6*中的公式所示（注意你正在用*ax + b*替换*ŷ*）：
- en: '![Figure 6.6 – L1 and L2 regularization](img/B21197_06_06.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – L1和L2正则化](img/B21197_06_06.jpg)'
- en: Figure 6.6 – L1 and L2 regularization
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – L1和L2正则化
- en: The λ (lambda) parameter must be greater than 0 and manually tuned. A very high
    lambda value may result in an underfitting issue, while a very low lambda may
    not result in expressive changes in the end results (if your model is overfitted,
    it will stay overfitted).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: λ（lambda）参数必须大于0，并且需要手动调整。一个非常高的λ值可能会导致欠拟合问题，而一个非常低的λ值可能不会导致最终结果有显著的变化（如果你的模型已经过度拟合，它将保持过度拟合）。
- en: In practical terms, the main difference between L1 and L2 regularization is
    that L1 will shrink the less important coefficients to 0, which will force the
    feature to be dropped (acting as a feature selector). In other words, if your
    model is overfitting because of the high number of features, L1 regularization
    should help you solve this problem.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，L1正则化和L2正则化之间的主要区别在于，L1会将不那么重要的系数缩小到0，这将迫使特征被删除（充当特征选择器）。换句话说，如果你的模型因为特征数量过多而过度拟合，L1正则化应该能帮助你解决这个问题。
- en: Important note
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: During your exam, remember the basis of L1 and L2 regularization, especially
    the key difference between them, where L1 works well as a feature selector.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在考试期间，记住L1和L2正则化的基础，特别是它们之间的关键区别，其中L1作为一个特征选择器效果很好。
- en: Finally, many built-in algorithms can serve multiple modeling purposes. The
    linear learner algorithm can be used for regression, binary classification, and
    multi-classification. Make sure you remember this during your exam (it is *not
    just* about regression models).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，许多内置算法可以服务于多种建模目的。线性学习器算法可以用于回归、二分类和多分类。确保你在考试中记住这一点（这不仅仅是关于回归模型）。
- en: AWS has other built-in algorithms that work for regression and classification
    problems –that is**, factorization machines, KNN,** and the **XGBoost** algorithm.
    Since these algorithms can also be used for classification purposes, these will
    be covered in the section about classification algorithms.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: AWS有其他内置算法适用于回归和分类问题——那就是**分解机、KNN**和**XGBoost**算法。由于这些算法也可以用于分类目的，这些内容将在关于分类算法的章节中介绍。
- en: Important note
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'You’ve just been given a very important tip to remember during the exam: linear
    learner, factorization machines, KNN, and XGBoost are suitable for both regression
    and classification problems. These algorithms are often known as algorithms for
    general purposes.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 你在考试期间需要记住一个非常重要的提示：线性学习器、分解机、KNN和XGBoost算法都适用于回归和分类问题。这些算法通常被称为通用算法。
- en: 'With that, you have reached the end of this section about regression models.
    Remember to check out the supporting material before you take the exam. You can
    also use the reference material when you are working on your daily activities!
    Now, let us move on to another classical example of a machine learning problem:
    classification models.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，您已经到达了关于回归模型的这一节结束。记得在考试前查看相关材料。您也可以在日常工作时使用参考材料！现在，让我们继续探讨另一个经典的机器学习问题示例：分类模型。
- en: Working with classification models
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与分类模型一起工作
- en: You have been learning what classification models are throughout this book.
    However, now, you are going to look at some algorithms that are suitable for classification
    problems. Keep in mind that there are hundreds of classification algorithms out
    there, but since you are preparing for the AWS Certified Machine Learning Specialty
    exam, the ones that have been pre-built by AWS will be covered.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 您在本章中一直在学习什么是分类模型。然而，现在，您将查看一些适合分类问题的算法。请记住，有数百种分类算法，但由于您正在为AWS认证机器学习专业考试做准备，所以将涵盖AWS预先构建的算法。
- en: You will start with **factorization machines**. Factorization machines is considered
    an extension of the linear learner algorithm, optimized to find the relationship
    between features within high-dimensional sparse datasets.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 您将从**因子机**开始。因子机被认为是线性学习算法的扩展，优化以在具有高维稀疏数据集的特征之间找到关系。
- en: Important note
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: A very traditional use case for factorization machines is *recommendation systems*,
    where you usually have a high level of sparsity in the data. During the exam,
    if you are faced with a general-purpose problem (either a regression or binary
    classification task) where the underlying datasets are sparse, then factorization
    machines is probably the best answer from an algorithm perspective.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 因子机的一个非常传统的用例是*推荐系统*，其中数据通常具有很高的稀疏度。在考试中，如果您面临一个通用问题（无论是回归还是二分类任务），其中基础数据集是稀疏的，那么从算法角度来看，因子机可能是最好的答案。
- en: When you use factorization machines in a regression model, the RMSE will be
    used to evaluate the model. On the other hand, in the binary classification mode,
    the algorithm will use log loss, accuracy, and F1 score to evaluate results. A
    deeper discussion about evaluation metrics will be provided in [*Chapter 7*](B21197_07.xhtml#_idTextAnchor970)*,
    Evaluating and* *Optimizing Models*.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在回归模型中使用因子机时，将使用RMSE来评估模型。另一方面，在二分类模式下，算法将使用对数损失、准确率和F1分数来评估结果。关于评估指标的更深入讨论将在[*第7章*](B21197_07.xhtml#_idTextAnchor970)*，评估和优化模型*中提供。
- en: You should be aware that factorization machines only accepts input data in the
    `text/.csv` format.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该知道，因子机只接受`text/.csv`格式的输入数据。
- en: 'The next built-in algorithm suitable for classification problems is known as
    K-nearest neighbors, or KNN for short. As the name suggests, this algorithm will
    try to find the *K* closest points to the input data and return either of the
    following predictions:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于分类问题的下一个内置算法被称为K最近邻，简称KNN。正如其名所示，此算法将尝试找到输入数据最近的*K*个点，并返回以下预测之一：
- en: The most repeated class of the *K* closest points, if it is a classification
    task
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是一个分类任务，则*K*个最近点的最频繁出现的类别
- en: The average value of the label of the *K* closest points, if it is a regression
    task
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是一个回归任务，则标签为*K*个最近点的平均值
- en: KNN is an **index-based algorithm** because it computes distances between points,
    assigns indexes for these points, and then stores the sorted distances and their
    indexes. With that type of data structure, KNN can easily select the top *K* closest
    points to make the final prediction. Note that *K* is a hyperparameter of KNN
    and should be optimized during the modeling process.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: KNN是一个**基于索引的算法**，因为它计算点之间的距离，为这些点分配索引，然后存储排序后的距离及其索引。有了这种类型的数据结构，KNN可以轻松选择最接近的*K*个点来做出最终预测。请注意，*K*是KNN的超参数，应在建模过程中进行优化。
- en: The other AWS built-in algorithm available for general purposes, including classification,
    is known as **eXtreme Gradient Boosting**, or **XGBoost** for short. This is an
    ensemble, decision tree-based model.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个适用于通用目的的AWS内置算法，包括分类，被称为**极端梯度提升**，简称**XGBoost**。这是一个基于集成和决策树的模型。
- en: XGBoost uses a set of **weaker** models (decision trees) to predict the target
    variable, which can be a regression task, binary class, or multi-class. This is
    a very popular algorithm and has been used in machine learning competitions by
    the top performers.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost使用一组**较弱的**模型（决策树）来预测目标变量，这可以是回归任务、二分类或多分类。这是一个非常流行的算法，并且已经被顶尖选手在机器学习竞赛中使用。
- en: XGBoost uses a boosting learning strategy, in which one model tries to correct
    the error of the prior model. It carries the name “gradient” because it uses the
    gradient descent algorithm to minimize the loss when adding new trees.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost使用提升学习策略，其中一个模型试图纠正先前模型的错误。它被称为“梯度”是因为它使用梯度下降算法在添加新树时最小化损失。
- en: Important note
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The term *weaker* is used in this context to describe very simple decision trees.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，术语*较弱*用来描述非常简单的决策树。
- en: Although XGBoost is much more robust than a single decision tree, it is important
    to go into the exam with a clear understanding of what decision trees are and
    their main configurations. By the way, they are the base model of many ensemble
    algorithms, such as AdaBoost, Random Forest, gradient boost, and XGBoost.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然XGBoost比单个决策树更稳健，但在考试中，对决策树及其主要配置有清晰的理解是非常重要的。顺便说一下，它们是许多集成算法（如AdaBoost、随机森林、梯度提升和XGBoost）的基础模型。
- en: Decision trees are rule-based algorithms that organize decisions in the form
    of a tree, as shown in *Figure 6**.7*.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是基于规则的算法，以树的形式组织决策，如图*图6.7*所示。
- en: '![Figure 6.7 – Example of what a decision tree model looks like](img/B21197_06_07.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – 决策树模型示例](img/B21197_06_07.jpg)'
- en: Figure 6.7 – Example of what a decision tree model looks like
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 决策树模型示例
- en: They are formed by a root node (at the very top of the tree), intermediary or
    decision nodes (in the middle of the tree), and leaf nodes (bottom nodes with
    no splits). The depth of the tree is given by the difference between the root
    node and the very last leaf node. For example, in *Figure 6**.7*, the depth of
    the tree is 3.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 它们由根节点（位于树的顶部）、中间或决策节点（位于树的中间）和叶节点（底部节点，没有分割）组成。树的深度由根节点和最后一个叶节点之间的差异给出。例如，在*图6.7*中，树的深度是3。
- en: The depth of the tree is one of the most important hyperparameters of this type
    of model and it is often known as the **max depth**. In other words, the max depth
    controls the maximum depth that a decision tree can reach.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 树的深度是这类模型最重要的超参数之一，通常被称为**最大深度**。换句话说，最大深度控制了决策树可以达到的最大深度。
- en: Another very important hyperparameter of decision tree models is the minimum
    number of samples/observations in the leaf nodes. It is also used to control the
    growth of the tree.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树模型的另一个非常重要的超参数是叶节点中的最小样本/观察数。它也用于控制树的生长。
- en: Decision trees have many other types of hyperparameters, but these two are especially
    important for controlling how the model overfits. Decision trees with a high depth
    or a very small number of observations in the leaf nodes are likely to face issues
    during extrapolation/prediction.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树有许多其他类型的超参数，但这两个对于控制模型过拟合尤为重要。深度较高或叶节点中观察数非常少的决策树在预测/外推过程中可能会遇到问题。
- en: 'The reason for this is simple: decision trees use data from the leaf nodes
    to make predictions, based on the proportion (for classification tasks) or average
    value (for regression tasks) of each observation/target variable that belongs
    to that node. Thus, the node should have enough data to make good predictions
    outside the training set.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为决策树使用叶节点中的数据来做出预测，基于属于该节点的每个观察/目标变量的比例（对于分类任务）或平均值（对于回归任务）。因此，节点应该有足够的数据来在训练集之外做出良好的预测。
- en: If you encounter the term **CART** during the exam, you should know that it
    stands for **Classification and Regression Trees**, since decision trees can be
    used for classification and regression tasks.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在考试中遇到**CART**这个术语，你应该知道它代表**分类和回归树**，因为决策树可以用于分类和回归任务。
- en: To select the best variables to split the data in the tree, the model will choose
    the ones that maximize the separation of the target variables across the nodes.
    This task can be performed by different methods, such as **Gini** and **information
    gain**.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择在树中分割数据的最佳变量，模型将选择那些最大化节点间目标变量分离的变量。这项任务可以通过不同的方法完成，例如**基尼系数**和**信息增益**。
- en: Forecasting models
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测模型
- en: Time series refers to data points that are collected on a regular basis with
    a sequence dependency. Time series have a measure, a fact, and a time unit, as
    shown in *Figure 6**.8*.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列是指按一定规律收集的具有序列依赖性的数据点。时间序列具有度量、事实和时间单位，如图*图6*。8*所示。
- en: '![Figure 6.8 – Time series statement](img/B21197_06_08.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – 时间序列陈述](img/B21197_06_08.jpg)'
- en: Figure 6.8 – Time series statement
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 时间序列陈述
- en: Additionally, time series can be classified as **univariate** or **multivariate**.
    A univariate time series contains just one variable connected across a period
    of time, while a multivariate time series contains two or more variables connected
    across a period. *Figure 6**.9* shows the univariate time series.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，时间序列还可以被分类为**单变量**或**多变量**。单变量时间序列在一段时间内只包含一个变量，而多变量时间序列在一段时间内包含两个或更多变量。*图6*。9*显示了单变量时间序列。
- en: '![Figure 6.9 – Time series example](img/B21197_06_09.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图6.9 – 时间序列示例](img/B21197_06_09.jpg)'
- en: Figure 6.9 – Time series example
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – 时间序列示例
- en: 'Time series can be decomposed as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列可以按以下方式分解：
- en: '**Observed** or **level**: The average values of the series'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察值**或**水平**：序列的平均值'
- en: '**Trend**: Increasing, decreasing pattern (sometimes, there is no trend)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**趋势**：增加、减少的模式（有时没有趋势）'
- en: '**Seasonality**: Regular peaks at specific periods of time (sometimes, there
    is no seasonality)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**季节性**：在特定时间段内的规律性峰值（有时没有季节性）'
- en: '**Noise**: Something that cannot be explained'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声**：无法解释的东西'
- en: Sometimes, you can also find isolated peaks in the series that cannot be captured
    in a forecasting model. In such cases, you might want to consider those peaks
    as outliers. *Figure 6**.10* is a decomposition of the time series shown in *Figure
    6**.9*.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，你还可以在序列中找到无法在预测模型中捕捉到的孤立峰值。在这种情况下，你可能希望将这些峰值视为异常值。*图6*。10*是*图6*。9*所示时间序列的分解。
- en: '![Figure 6.10 – Time series decomposition](img/B21197_06_10.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图6.10 – 时间序列分解](img/B21197_06_10.jpg)'
- en: Figure 6.10 – Time series decomposition
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – 时间序列分解
- en: It is also worth highlighting that you can use **additive** or **multiplicative**
    approaches to decompose time series. Additive models suggest that your time series
    *adds* each component to explain the target variable – that is, *y(t) = level
    + trend + seasonality +* *noise*.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 值得强调的是，你可以使用**加法**或**乘法**方法来分解时间序列。加法模型建议你的时间序列将每个组成部分相加以解释目标变量——即，*y(t) = 水平
    + 趋势 + 季节性 +* *噪声*。
- en: Multiplicative models, on the other hand, suggest that your time series *multiplies*
    each component to explain the target variable – that is, *y(t) = level * trend
    * seasonality ** *noise*.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，乘法模型建议你的时间序列将每个组成部分相乘以解释目标变量——即，*y(t) = 水平 * 趋势 * 季节性 ** *噪声*。
- en: In the next section, you will take a closer look at time series components.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将更深入地了解时间序列的组成部分。
- en: Checking the stationarity of time series
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查时间序列的平稳性
- en: Decomposing time series and understanding how their components interact with
    additive and multiplicative models is a great achievement! However, the more you
    learn, the more you want to go deeper into the problem. Maybe you have realized
    that time series without trend and seasonality are easier to predict than the
    ones with all those components!
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 将时间序列分解并理解其组成部分如何通过加法和乘法模型相互作用是一项伟大的成就！然而，你学得越多，你就越想深入探究问题。也许你已经意识到，没有趋势和季节性的时间序列比具有所有这些成分的时间序列更容易预测！
- en: That is naturally right. If you do not have to understand trend and seasonality,
    and if you do not have control over the noise, all you have to do is explore the
    observed values and find their regression relationship.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这自然是正确的。如果你不需要理解趋势和季节性，并且如果你无法控制噪声，你所要做的就是探索观察到的值并找到它们的回归关系。
- en: A time series with constant mean and variance across a time period is known
    as **stationary**. In general, time series *with* trend and seasonality are *not*
    stationary. It is possible to apply data transformations to the series to transform
    it into a stationary time series so that the modeling task tends to be easier.
    This type of transformation is known as **differentiation**.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个时间段内具有恒定均值和方差的时间序列被称为**平稳的**。一般来说，具有趋势和季节性的时间序列**不是**平稳的。可以对序列应用数据转换，将其转换为平稳时间序列，从而使建模任务变得更容易。这种转换被称为**微分**。
- en: While you are exploring a time series, you can check stationarity by applying
    hypothesis tests, such as **Dickey-Fuller**, **KPSS**, and **Phillips-Perron**,
    just to mention a few. If you find it non-stationary, then you can apply differentiation
    to make it a stationary time series. Some algorithms already have that capability
    embedded.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在探索时间序列时，你可以通过应用假设检验，如**迪基-富勒**、**KPSS**和**菲利普斯-佩荣**，来检查平稳性，仅举几个例子。如果你发现它是非平稳的，那么你可以应用微分使其成为平稳时间序列。一些算法已经内置了这种能力。
- en: Exploring, exploring, and exploring
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索，探索，再探索
- en: At this point, it is important to remember that exploration tasks happen all
    the time in data science. Nothing is different here. While you are building time
    series models, you might want to take a look at the data and check whether it
    is suitable for this type of modeling.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，重要的是要记住，在数据科学中，探索任务一直在进行。这里没有什么不同。当你构建时间序列模型时，你可能想查看数据并检查它是否适合这种类型的建模。
- en: '**Autocorrelation plots** are one of the tools that you can use for time series
    analysis. Autocorrelation plots allow you to check the correlations between lags
    in the time series. *Figure 6**.11* shows an example of this type of visualization.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '**自相关图**是你可以用于时间序列分析的工具之一。自相关图允许你检查时间序列中滞后之间的相关性。*图6.11*展示了这种可视化类型的一个示例。'
- en: '![Figure 6.11 – Autocorrelation plot](img/B21197_06_11.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![图6.11 – 自相关图](img/B21197_06_11.jpg)'
- en: Figure 6.11 – Autocorrelation plot
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – 自相关图
- en: Remember, if you are playing with univariate time series, your time series just
    contains one variable. Therefore, finding autocorrelation across the lags of your
    unique variable is crucial to understanding whether you can build a good model
    or not.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，如果你在处理单变量时间序列，你的时间序列只包含一个变量。因此，找到你独特变量的滞后之间的自相关对于理解你是否能构建一个好的模型至关重要。
- en: And yes, it turns out that, sometimes, it might happen that you do not have
    a time series in front of you. Furthermore, no matter your efforts, you will not
    be able to model this data as a time series. This type of data is often known
    as **white** **noise**.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，有时候，可能会发生这样的情况，你面前没有时间序列。而且，无论你多么努力，你都无法将这类数据建模为时间序列。这类数据通常被称为**白噪声**。
- en: Another type of series that you cannot predict is known as a **random** **walk**.
    Random walks are random by nature, but they have a dependency on the previous
    time step. For example, the next point of a random walk could be a random number
    between 0 and 1, and also the last point of the series.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种你无法预测的序列被称为**随机游走**。随机游走本质上是随机的，但它们依赖于前一时间步。例如，随机游走的下一个点可能是在0和1之间的随机数，也可能是序列的最后一个点。
- en: Important note
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Be careful if you come across those terms in the exam and remember to relate
    them to randomness in time series.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在考试中遇到这些术语，请务必将它们与时间序列中的随机性联系起来。
- en: With that, you have covered the main theory behind time series modeling. You
    should also be aware that the most popular algorithms out there for working with
    time series are known as **Auto-Regressive Integrated Moving Average (ARIMA)**
    and **Exponential Smoothing (ETS)**. This book will not go into the details of
    these two models. Instead, you will see what AWS can offer in terms of time series
    modeling.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，你已经了解了时间序列建模背后的主要理论。你也应该意识到，目前最流行的用于处理时间序列的算法被称为**自回归积分移动平均（ARIMA）**和**指数平滑（ETS）**。本书不会深入探讨这两个模型。相反，你将看到AWS在时间序列建模方面能提供什么。
- en: Understanding DeepAR
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解DeepAR
- en: The **DeepAR** forecasting algorithm is a built-in SageMaker algorithm that
    is used to forecast a one-dimensional time series using a **Recurrent Neural**
    **Network (RNN)**.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '**DeepAR**预测算法是一个内置的SageMaker算法，用于使用**循环神经网络（RNN）**预测一维时间序列。'
- en: Traditional time series algorithms, such as ARIMA and ETS, are designed to fit
    one model per time series. For example, if you want to forecast sales per region,
    you might have to create one model per region, since each region may have its
    own sales behaviors. DeepAR, on the other hand, allows you to operate more than
    one time series in a single model, which seems to be a huge advantage for more
    complex use cases.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的时序算法，如ARIMA和ETS，旨在为每个时间序列拟合一个模型。例如，如果你想预测每个地区的销售额，你可能需要为每个地区创建一个模型，因为每个地区可能有自己独特的销售行为。另一方面，DeepAR允许你在单个模型中操作多个时间序列，这在更复杂的应用场景中似乎是一个巨大的优势。
- en: 'The input data for DeepAR, as expected, is *one or more* time series. Each
    of these time series can be associated with the following:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: DeepAR的输入数据，正如预期的那样，是*一个或多个*时间序列。这些时间序列中的每一个都可以与以下内容相关联：
- en: A vector of static (time-independent) categorical features, controlled by the
    `cat` field
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个静态（时间独立）的类别特征向量，由`cat`字段控制
- en: A vector of dynamic (time-dependent) time series, controlled by `dynamic_feat`
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个动态（时间依赖）时间序列的向量，由`dynamic_feat`控制
- en: Important note
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Note that the ability to train and make predictions on top of multiple time
    series is strictly related to the vector of static categorical features. While
    defining the time series that DeepAR will train on, you can set categorical variables
    to specify which group each time series belongs to.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在多个时间序列上训练和进行预测的能力与静态类别特征向量密切相关。在定义DeepAR将要训练的时间序列时，你可以设置类别变量来指定每个时间序列属于哪个组。
- en: Two of the main hyperparameters of DeepAR are `context_length`, which is used
    to control how far in the past the model can see during the training process,
    and `prediction_length`, which is used to control how far in the future the model
    will output predictions.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: DeepAR的两个主要超参数是`context_length`，它用于控制在训练过程中模型可以看到多远的历史，以及`prediction_length`，它用于控制模型将输出预测的多远未来。
- en: DeepAR can also handle missing values, which, in this case, refers to existing
    gaps in the time series. A very interesting functionality of DeepAR is its ability
    to create derived features from time series. These derived features, which are
    created from basic time frequencies, help the algorithm learn time-dependent patterns.
    *Table 6.7* shows all the derived features created by DeepAR, according to each
    type of time series that it is trained on.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: DeepAR还可以处理缺失值，在这种情况下，指的是时间序列中存在的间隙。DeepAR的一个非常有趣的功能是从时间序列中创建派生特征。这些派生特征，由基本时间频率创建，有助于算法学习时间依赖的模式。*表6.7*显示了DeepAR根据其训练的每种类型的时间序列创建的所有派生特征。
- en: '| **Frequency of the** **time series** | **Derived feature** |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| **时间序列的** **频率** | **派生特征** |'
- en: '| Minute | Minute of hour, hour of day, day of week, day of month, day of year
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 分钟 | 小时中的分钟数，天的小时数，周中的天数，月份中的天数，年份中的天数 |'
- en: '| Hour | Hour of day, day of week, day of month, day of year |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 小时 | 天的小时数，周中的天数，月份中的天数，年份中的天数 |'
- en: '| Day | Day of week, day of month, day of year |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 天 | 周中的天数，月份中的天数，年份中的天数 |'
- en: '| Week | Day of month, week of year |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 周 | 月份中的天数，年份中的周数 |'
- en: '| Month | Month of year |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 月份 | 年份中的月份 |'
- en: Table 6.7 – DeepAR derived features per frequency of time series
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.7 – 按时间序列频率派生的DeepAR特征
- en: You have now completed this section about forecasting models. Next, you will
    take a look at the last algorithm regarding supervised learning – that is, the
    **Object2Vec** algorithm.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经完成了关于预测模型的这一部分。接下来，你将查看关于监督学习的最后一个算法——即**Object2Vec**算法。
- en: Object2Vec
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Object2Vec
- en: Object2Vec is a built-in SageMaker algorithm that generalizes the well-known
    **Word2Vec** algorithm. Object2Vec is used to create **embedding spaces** for
    high dimensional objects. These embedding spaces are, per definition, compressed
    representations of the original object and can be used for multiple purposes,
    such as feature engineering or object comparison.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: Object2Vec是SageMaker内置算法，它推广了众所周知的**Word2Vec**算法。Object2Vec用于为高维对象创建**嵌入空间**。根据定义，这些嵌入空间是原始对象的压缩表示，可用于多个目的，例如特征工程或对象比较。
- en: '![Figure 6.12 – A visual example of an embedding space](img/B21197_06_12.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![图6.12 – 嵌入空间的视觉示例](img/B21197_06_12.jpg)'
- en: Figure 6.12 – A visual example of an embedding space
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – 嵌入空间的视觉示例
- en: '*Figure 6**.12* illustrates what is meant by an embedding space. The first
    and last layers of the neural network model just map the input data with itself
    (represented by the same vector size).'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6**.12*说明了嵌入空间的意思。神经网络模型的第一层和最后一层只是将输入数据映射到自身（由相同大小的向量表示）。'
- en: As you move on to the internal layers of the model, the data is compressed more
    and more until it hits the layer in the middle of this architecture, known as
    the embedding layer. On that particular layer, you have a smaller vector, which
    aims to be an accurate and compressed representation of the high-dimensional original
    vector from the first layer.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 当你深入到模型的内部层时，数据被越来越压缩，直到它达到这个架构中间的层，即嵌入层。在这一特定层上，你有一个更小的向量，其目的是对来自第一层的高维原始向量进行准确和压缩的表示。
- en: With this, you just completed the first section about machine learning algorithms
    in AWS. Coming up next, you will take a look at some unsupervised algorithms.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，你已经完成了关于AWS中机器学习算法的第一部分。接下来，你将了解一些无监督算法。
- en: Unsupervised learning
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: 'AWS provides several unsupervised learning algorithms for the following tasks:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: AWS为以下任务提供了几个无监督学习算法：
- en: 'Clustering: K-Means algorithm'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类：K-Means算法
- en: 'Dimension reduction: **Principal Component** **Analysis (PCA)**'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度降低：**主成分分析（Principal Component Analysis，简称PCA**）
- en: 'Pattern recognition: IP Insights'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式识别：IP洞察
- en: 'Anomaly detection: The **Random Cut Forest (****RCF)** algorithm'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测：**随机切割森林（Random Cut Forest，简称RCF）**算法
- en: 'Let us start by talking about clustering and how the most popular clustering
    algorithm works: K-Means.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从聚类以及最流行的聚类算法K-Means的工作原理开始谈。
- en: Clustering
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类
- en: Clustering algorithms are very popular in data science. Basically, they aim
    to identify similar groups in a given dataset, also known as *clusters*. Clustering
    algorithms belong to the field of non-supervised learning, which means that they
    do not need a label or response variable to be trained.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法在数据科学中非常流行。基本上，它们的目的是在给定的数据集中识别相似的组，也称为**聚类**。聚类算法属于非监督学习领域，这意味着它们不需要标签或响应变量来训练。
- en: This is just fantastic since labeled data is very scarce! However, it comes
    with some limitations. The main one is that clustering algorithms provide clusters
    for you, but not the meaning of each cluster. Thus, someone, as a subject matter
    expert, has to analyze the properties of each cluster to define their meanings.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是太棒了，因为标记数据非常稀缺！然而，它也有一些限制。主要的一个是，聚类算法为你提供聚类，但不提供每个聚类的含义。因此，必须有人，作为领域专家，分析每个聚类的属性来定义它们的含义。
- en: There are many types of clustering approaches, such as hierarchical clustering
    and partitional clustering. Inside each approach, you will find several algorithms.
    However, K-Means is probably the most popular clustering algorithm, and you are
    likely to come across it in your exam.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多种聚类方法，例如层次聚类和划分聚类。在每种方法内部，你都会发现几个算法。然而，K-Means可能是最流行的聚类算法，你很可能在考试中会遇到它。
- en: When you are playing with K-Means, somehow, you have to specify the number of
    clusters that you want to create. Then, you have to allocate the data points across
    each cluster, so that each data point will belong to a single cluster. This is
    exactly what you should expect as a result at the end of the clustering process!
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在玩K-Means时，你必须要指定你想要创建的聚类数量。然后，你必须将数据点分配到每个聚类中，以便每个数据点只属于一个聚类。这正是聚类过程结束时你应该期待的结果！
- en: You need to specify the number of clusters that you want to create and pass
    this number to the K-Means algorithm. Then, the algorithm will randomly initiate
    the central point of each cluster (this is known as **centroid initialization**).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要指定你想要创建的聚类数量，并将这个数字传递给K-Means算法。然后，算法将随机初始化每个聚类的中心点（这被称为**中心点初始化**）。
- en: Once you have the centroids of each cluster, all you need to do is assign a
    cluster to each data point. To do that, you have to use a proximity or distance
    metric! This book will use the term *distance metric*.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了每个聚类的中心点，你所需要做的就是为每个数据点分配一个聚类。要做到这一点，你必须使用一个邻近度或距离度量！这本书将使用术语*距离度量*。
- en: The **distance metric** is responsible for calculating the distance between
    data points and centroids. The data point will belong to the closer cluster centroid,
    according to the distance metric.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '**距离度量**负责计算数据点和中心点之间的距离。根据距离度量，数据点将属于最近的聚类中心。'
- en: 'The most popular distance metric is called **Euclidean distance** and the math
    behind it is simple; imagine that the points of your dataset are composed of two
    dimensions, *x* and *y*. So, you could consider points *a* and *b* as follows:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的距离度量称为**欧几里得距离**，其背后的数学很简单；想象一下，你的数据集的点由两个维度组成，*x*和*y*。因此，你可以将点*a*和*b*考虑如下：
- en: '*a (**x=1, y=1)*'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a（x=1, y=1）*'
- en: '*b (**x=2, y=5)*'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b（x=2, y=5）*'
- en: 'The Euclidean distance between points *a* and *b* is given by the following
    formula, where *x*1 and *y*1 refer to the values of point *a*, and *x*2 and *y*2
    refer to the values of point *b*: ![](img/B21197_06_12a.png). The same function
    can be generalized by the following equation: ![](img/B21197_06_12b.png). Once
    you have completed this process and assigned a cluster for each data point, you
    ![](img/B21197_06_12c.png) methods, such as **single link, average link**, and
    **complete link**.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 点 *a* 和 *b* 之间的欧几里得距离由以下公式给出，其中 *x*1 和 *y*1 是点 *a* 的值，而 *x*2 和 *y*2 是点 *b* 的值：![](img/B21197_06_12a.png)。该函数可以通过以下方程推广：![](img/B21197_06_12b.png)。一旦您完成这个过程，并为每个数据点分配了一个簇，您可以使用
    ![](img/B21197_06_12c.png) 方法，如 **单链接、平均链接** 和 **完全链接**。
- en: Due to this centroid refreshment, you will have to keep checking the closest
    cluster for each data point and keep refreshing the centroids, iteratively, until
    the cluster centroids converge and no cluster reassignment is needed, or the maximum
    number of allowed iterations is reached.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种中心点刷新，您将不得不持续检查每个数据点的最近簇，并迭代地刷新中心点，直到簇中心收敛且不需要重新分配簇，或者达到允许的最大迭代次数。
- en: 'Alright, the following is a summarization of the components and steps that
    compose the K-Means method:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，以下是对构成 K-Means 方法的组件和步骤的总结：
- en: Centroid initialization, cluster assignment, centroid refreshment, and then
    redo the last two steps until it converges
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中心点初始化、簇分配、中心点刷新，然后重复最后两个步骤，直到收敛
- en: A distance metric to assign data points to each cluster (in this case, Euclidian
    distance)
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种距离度量，用于将数据点分配到每个簇（在这种情况下，欧几里得距离）
- en: A linkage method to recalculate the cluster centroids (for the sake of our demonstration,
    you will learn about the average linkage)
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种重新计算簇中心点的链接方法（为了我们的演示，您将了解平均链接）
- en: With these definitions, you are now ready to walk through the following real
    example, step by step (some support material is also available for your reference).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些定义，您现在可以逐步了解以下真实示例，一步一步来（一些参考资料也供您参考）。
- en: Computing K-Means step by step
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逐步计算 K-Means
- en: In this example, you will simulate K-Means in a very small dataset, with only
    two columns (*x* and *y*) and six data points (*A*, *B*, *C*, *D*, *E*, and *F*),
    as defined in *Table 6.8*.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，您将在一个非常小的数据集上模拟 K-Means，该数据集只有两列（*x* 和 *y*）和六个数据点（*A*、*B*、*C*、*D*、*E*
    和 *F*），如 *表 6.8* 所定义。
- en: '| **Point** | **x** | **y** |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| **点** | **x** | **y** |'
- en: '| A | 1 | 1 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| A | 1 | 1 |'
- en: '| B | 2 | 2 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| B | 2 | 2 |'
- en: '| C | 5 | 5 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| C | 5 | 5 |'
- en: '| D | 5 | 6 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| D | 5 | 6 |'
- en: '| E | 1 | 5 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| E | 1 | 5 |'
- en: '| F | 2 | 6 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| F | 2 | 6 |'
- en: '| **Cluster 1** | **1** | **1** |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| **簇 1** | **1** | **1** |'
- en: '| **Cluster 2** | **2** | **2** |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| **簇 2** | **2** | **2** |'
- en: '| **Cluster 3** | **5** | **5** |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| **簇 3** | **5** | **5** |'
- en: Table 6.8 – Iteration input data for K-Means
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.8 – K-Means 迭代输入数据
- en: '*Table 6.8* contains three clusters with the following centroids: *(1,1), (2,2),
    (5,5).* The number of clusters (3) was defined *a priori* and the centroid for
    each cluster was randomly defined. *Figure 6**.13* shows the stage of the algorithm
    that you are at right now.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 6.8* 包含三个簇，其中心点如下：*(1,1), (2,2), (5,5)*。簇的数量（3）是事先定义的，每个簇的中心点是随机定义的。*图 6**.13*
    显示了您现在所处的算法阶段。'
- en: '![Figure 6.13 – Plotting the K-Means results before completing the first iteration](img/B21197_06_13.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.13 – 完成第一次迭代前的 K-Means 结果](img/B21197_06_13.jpg)'
- en: Figure 6.13 – Plotting the K-Means results before completing the first iteration
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 – 完成第一次迭代前的 K-Means 结果
- en: Here, you can’t see points *A*, *B*, and *C* since they overlap with cluster
    centroids, but don’t worry – they will appear soon. Next, you have to compute
    the distance of each data point to each cluster centroid, and then, you need to
    choose the cluster that is the closest to each point.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，由于它们与簇中心重叠，您看不到点 *A*、*B* 和 *C*，但请放心——它们很快就会出现。接下来，您必须计算每个数据点到每个簇中心的距离，然后，您需要选择离每个点最近的簇。
- en: '| **xc1** | **yc1** | **xc2** | **yc2** | **xc3** | **yc3** | **distance-c1**
    | **distance-c2** | **distance-c3** | **Cluster** |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| **xc1** | **yc1** | **xc2** | **yc2** | **xc3** | **yc3** | **distance-c1**
    | **distance-c2** | **distance-c3** | **簇** |'
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 0,0 | 1,4 | 5,7 | Cluster 1 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 2 | 2 | 5 | 5 | 0,0 | 1,4 | 5,7 | 簇 1 |'
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 1,4 | 0,0 | 4,2 | Cluster 2 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 2 | 2 | 5 | 5 | 1,4 | 0,0 | 4,2 | 簇 2 |'
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 5,7 | 4,2 | 0,0 | Cluster 3 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 2 | 2 | 5 | 5 | 5,7 | 4,2 | 0,0 | 簇 3 |'
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 6,4 | 5,0 | 1,0 | Cluster 3 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 2 | 2 | 5 | 5 | 6,4 | 5,0 | 1,0 | 簇 3 |'
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 4,0 | 3,2 | 4,0 | Cluster 2 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 2 | 2 | 5 | 5 | 4,0 | 3,2 | 4,0 | 聚类 2 |'
- en: '| 1 | 1 | 2 | 2 | 5 | 5 | 5,1 | 4,0 | 3,2 | Cluster 3 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 2 | 2 | 5 | 5 | 5,1 | 4,0 | 3,2 | 聚类 3 |'
- en: '| Legendxc1 = x value of cluster 1yc1 = y value of cluster 1 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 图例xc1 = 聚类 1 的 x 值yc1 = 聚类 1 的 y 值 |'
- en: Table 6.9 – Processing iteration 1
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.9 – 处理迭代 1
- en: '*Table 6.9* contains the following elements:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 6.9* 包含以下元素：'
- en: Each row represents a data point.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一行代表一个数据点。
- en: The first six columns represent the centroid axis (*x* and *y*) of each cluster.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前六列代表每个聚类的质心轴（*x* 和 *y*）。
- en: The next three columns represent the distance of each data point to each cluster
    centroid.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来的三列表示每个数据点到每个聚类质心的距离。
- en: The last column represents the clusters that are the closest to each data point.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一列代表每个数据点最近的聚类。
- en: Looking at data point *A* (first row), you can see that it was assigned to cluster
    1 because the distance from data point *A* to cluster 1 is 0 (do you remember
    that they were overlapping?). The same calculation happens to all other data points
    to define a cluster for each data point.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 观察数据点 *A*（第一行），你可以看到它被分配到聚类 1，因为数据点 *A* 到聚类 1 的距离是 0（你还记得它们是重叠的吗？）。同样的计算会应用于所有其他数据点，以定义每个数据点的聚类。
- en: Before you move on, you might want to see how those Euclidian distances between
    the clusters and the data points were computed. For demonstration purposes, the
    following simulation will consider the distance from data point *A* to cluster
    3 (the first row in *Table 6.9*, column `distance-c3`, value *5,7*).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在你继续之前，你可能想看看聚类和数据点之间的欧几里得距离是如何计算的。为了演示目的，以下模拟将考虑数据点 *A* 到聚类 3 的距离（*表 6.9* 中的第一行，`distance-c3`
    列，值为 *5,7*）。
- en: 'First of all, the following formula was used to calculate the Euclidian distance:
    ![](img/B21197_06_13a.png)'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，以下公式被用来计算欧几里得距离：![](img/B21197_06_13a.png)
- en: 'Here, you have the following:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，你有以下内容：
- en: '*x*1 = *x* of data point *A* = 1'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*1 = 数据点 *A* 的 *x* 值 = 1'
- en: '*y*1 = *y* of data point *A* = 1'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*1 = 数据点 *A* 的 *y* 值 = 1'
- en: '*x*2 = *x* of cluster 3 = 5'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*2 = 聚类 3 的 *x* 值 = 5'
- en: '*y*2 = *y* of cluster 3 = 5'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*2 = 聚类 3 的 *y* 值 = 5'
- en: '*Figure 6**.14* applies the formula, step by step.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6**.14* 逐步应用公式。'
- en: '![Figure 6.14 – Computing the Euclidian distance step by step](img/B21197_06_14.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.14 – 逐步计算欧几里得距离](img/B21197_06_14.jpg)'
- en: Figure 6.14 – Computing the Euclidian distance step by step
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14 – 逐步计算欧几里得距离
- en: 'That is just fantastic, isn’t it? You have almost completed the first iteration
    of K-Means. In the very last step of iteration 1, you have to refresh the cluster
    centroids. Remember: initially, they were randomly defined, but now, you have
    just assigned some data points to each cluster, which means you should be able
    to identify where the central point of the cluster is.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是太棒了，不是吗？你几乎完成了 K-Means 的第一次迭代。在第一次迭代的最后一步，你必须刷新聚类质心。记住：最初它们是随机定义的，但现在，你已经将一些数据点分配给每个聚类，这意味着你应该能够识别出聚类的中心点在哪里。
- en: In this example, the **linkage** method will be used to refresh the cluster
    centroids. This is a very simple step, and the results are presented in *Table
    6.10*.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，将使用 **链接** 方法来刷新聚类质心。这是一个非常简单的步骤，结果在 *表 6.10* 中展示。
- en: '| **Point** | **x** | **y** |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| **点** | **x** | **y** |'
- en: '| A | 1 | 1 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| A | 1 | 1 |'
- en: '| B | 2 | 2 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| B | 2 | 2 |'
- en: '| C | 5 | 5 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| C | 5 | 5 |'
- en: '| D | 5 | 6 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| D | 5 | 6 |'
- en: '| E | 1 | 5 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| E | 1 | 5 |'
- en: '| F | 2 | 6 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| F | 2 | 6 |'
- en: '| **Cluster 1** | **1** | **1** |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| **聚类 1** | **1** | **1** |'
- en: '| **Cluster 2** | **1,5** | **3,5** |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| **聚类 2** | **1,5** | **3,5** |'
- en: '| **Cluster 3** | **4** | **5,7** |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| **聚类 3** | **4** | **5,7** |'
- en: Table 6.10 – K-Means results after iteration 1
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.10 – 第 1 次迭代后的 K-Means 结果
- en: '*Table 6.10* shows the same data points (*A* to *F*) that you are dealing with
    (by the way, they will never change), and the centroids of clusters 1, 2, and
    3\. Those centroids are quite different from what they were initially, as shown
    in *Table 6.8*. This is because they were refreshed using average linkage! The
    method got the average value of all the *x* and *y* values of the data points
    of each cluster. In the next simulation, have a look at how *(1.5, 3.5)* were
    obtained as centroids of cluster 2.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 6.10* 显示了你正在处理的数据点（顺便说一句，它们永远不会改变），以及聚类 1、2 和 3 的质心。这些质心与 *表 6.8* 中最初显示的质心相当不同。这是因为它们使用了平均链接来刷新！该方法得到了每个聚类中所有
    *x* 和 *y* 值的平均值。在下一个模拟中，看看如何将 *(1.5, 3.5)* 获得为聚类 2 的质心。'
- en: 'If you look at *Table 6.9*, you will see that cluster 2 only has two data points
    assigned to it: *B* and *E*. These are the second and fifth rows in that figure.
    If you take the average values of the *x* axis of each point, then you will have
    *(2 + 1) / 2 = 1.5* and *(2 + 5) / 2 =* *3.5*.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看 *表 6.9*，你会看到簇 2 只有两个数据点分配给它：*B* 和 *E*。这些是图中第二行和第五行。如果你取每个点的 *x* 轴的平均值，那么你将得到
    *(2 + 1) / 2 = 1.5* 和 *(2 + 5) / 2 = 3.5*。
- en: With that, you are done with iteration 1 of K-Means and you can view the results
    in *Figure 6**.15*.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，你就完成了 K-Means 的第一次迭代，你可以在 *图 6.15* 中查看结果。
- en: '![Figure 6.15 – Plotting the K-Means results after the first iteration](img/B21197_06_15.jpg)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.15 – 第一次迭代后的 K-Means 结果](img/B21197_06_15.jpg)'
- en: Figure 6.15 – Plotting the K-Means results after the first iteration
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 – 第一次迭代后的 K-Means 结果
- en: 'Now, you can see almost all the data points, except for data point A because
    it is still overlapping with the centroid of cluster 1\. Moving on, you have to
    redo the following steps:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以看到几乎所有的数据点，除了数据点 A，因为它仍然与簇 1 的中心点重叠。继续，你必须重新执行以下步骤：
- en: Recalculate the distance between each data point and each cluster centroid and
    reassign clusters, if needed.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新计算每个数据点与每个簇中心点之间的距离，并在必要时重新分配簇。
- en: Recalculate the cluster centroids.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新计算簇中心点。
- en: You do those two tasks many times until the cluster centroids converge and they
    don’t change anymore, *or* you reach the maximum number of allowed iterations,
    which can be set as a hyperparameter of K-Means. For demonstration purposes, after
    four iterations, your clusters will look like *Figure 6**.16*.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 你会多次执行这两个任务，直到簇中心点收敛并且它们不再改变，*或者*你达到了允许的最大迭代次数，这可以作为 K-Means 的超参数设置。为了演示目的，经过四次迭代，你的簇将看起来像
    *图 6.16*。
- en: '![Figure 6.16 – Plotting the K-Means results after the fourth iteration](img/B21197_06_16.jpg)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.16 – 第四次迭代后的 K-Means 结果](img/B21197_06_16.jpg)'
- en: Figure 6.16 – Plotting the K-Means results after the fourth iteration
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16 – 第四次迭代后的 K-Means 结果
- en: On the fourth iteration, all the cluster centroids look pretty consistent, and
    you can clearly see that all data points could be grouped according to their proximity.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 在第四次迭代中，所有簇的中心点看起来相当一致，你可以清楚地看到所有数据点都可以根据它们的邻近性进行分组。
- en: Important note
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In this example, you have only set two dimensions for each data point (dimensions
    *x* and *y*). In real use cases, you can see far more dimensions, and that is
    why clustering algorithms play a very important role in identifying groups in
    the data in a more automated fashion.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，你只为每个数据点设置了两个维度（维度 *x* 和 *y*）。在实际应用中，你可以看到更多的维度，这就是为什么聚类算法在以更自动化的方式识别数据中的组时扮演着非常重要的角色。
- en: Hopefully, you have enjoyed how to compute K-Means from scratch! This knowledge
    will be beneficial for the exam and for your career as a data scientist. By the
    way, as advised many times, data scientists must be skeptical and curious, so
    you might be wondering why three clusters were defined in this example and not
    two or four. You may also be wondering how you measure the quality of the clusters.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你能享受从零开始计算 K-Means 的过程！这些知识将对考试和你的数据科学家职业生涯有益。顺便说一句，正如多次建议的那样，数据科学家必须持怀疑态度和好奇心，所以你可能想知道为什么在这个例子中定义了三个簇而不是两个或四个。你也可能想知道如何衡量簇的质量。
- en: You didn’t think this explanation wouldn’t be provided, did you?
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 你难道认为这个解释不会提供，对吧？
- en: Defining the number of clusters and measuring cluster quality
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义簇数量和测量簇质量
- en: Although K-Means is a great algorithm for finding patterns in your data, it
    will not provide the meaning of each cluster, nor the number of clusters you have
    to create to maximize cluster quality.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然K-Means 是一种在数据中寻找模式的好算法，但它不会提供每个簇的含义，也不会提供你必须创建以最大化簇质量的簇数量。
- en: In clustering, cluster quality means that you want to create groups with a high
    homogeneity among the elements of the same cluster, and a high heterogeneity among
    the elements of different clusters. In other words, the elements of the same clusters
    should be close/similar, whereas the elements of different clusters should be
    well separated.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类中，簇质量意味着你希望创建具有高同质性的簇组，以及不同簇元素之间的高异质性。换句话说，同一簇的元素应该接近/相似，而不同簇的元素应该很好地分离。
- en: 'One way to compute the cluster’s homogeneity is by using a metric known as
    the **sum of square errors**, or **SSE** for short. This metric will compute the
    sum of squared differences between each data point and its cluster centroid. For
    example, when all the data points are located at the same point where the cluster
    centroid is, then the SSE will be 0\. In other words, you want to minimize the
    SSE. The following equation formally defines the SSE: ![](img/B21197_06_16a.png)'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 计算簇的同质性的一个方法是通过使用一个称为**平方误差和**的度量，简称**SSE**。这个度量将计算每个数据点与其簇质心的平方差的和。例如，当所有数据点都位于簇质心所在的同一点时，SSE将为0。换句话说，你希望最小化SSE。以下方程正式定义了SSE：![](img/B21197_06_16a.png)
- en: Now that you know how to check the cluster quality, it is easier to understand
    how to define the number of appropriate clusters for a given dataset. All you
    have to do is find the optimal number of clusters to minimize the SSE. A very
    popular method that works around that logic is known as the **elbow method**.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了如何检查簇的质量，理解如何为给定的数据集定义合适的簇数量就更容易了。你所要做的就是找到最小化SSE的最优簇数量。一个围绕该逻辑工作的非常流行的方法被称为**肘部方法**。
- en: The elbow method proposes executing the clustering algorithm many times. In
    each execution, you will test a different number of clusters, *k*. After each
    execution, you compute the SSE related to that *k* number of clusters. Finally,
    you can plot these results and select the number of *k* where the SSE stops to
    drastically decrease.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 肘部方法建议多次执行聚类算法。在每次执行中，你将测试不同数量的簇，*k*。在每次执行后，你将计算与该*k*数量簇相关的SSE。最后，你可以绘制这些结果，并选择SSE停止急剧下降的*k*数量。
- en: Important note
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Adding more clusters will naturally decrease the SSE. In the elbow method, you
    want to find the point where that change becomes smoother, which means that the
    addition of new clusters will not bring too much value.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 添加更多的簇将自然会降低SSE。在肘部方法中，你希望找到这种变化变得平滑的点，这意味着新簇的添加不会带来太多的价值。
- en: In the previous example, three clusters were created. *Figure 6**.17* shows
    the elbow analysis that supports this decision.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，创建了三个簇。*图6**.17显示了支持这一决策的肘部分析。
- en: '![Figure 6.17 – The elbow method](img/B21197_06_17.jpg)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![图6.17 – 肘部方法](img/B21197_06_17.jpg)'
- en: Figure 6.17 – The elbow method
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 – 肘部方法
- en: You can conclude that adding more than three or four clusters will add unnecessary
    complexity to the clustering process.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以得出结论，添加超过三个或四个簇将给聚类过程增加不必要的复杂性。
- en: Of course, you should always consider the business background while defining
    the number of clusters. For example, if you are creating a customer segmentation
    model and your company has prepared the commercial team and business processes
    to support four segments of customers, there is no harm in setting up four clusters
    instead of three.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在定义簇的数量时，你应该始终考虑业务背景。例如，如果你正在创建客户细分模型，而你的公司已经为四个客户细分准备了商业团队和业务流程，那么设置四个簇而不是三个是没有害处的。
- en: Finally, you should know that AWS has implemented K-Means as part of its list
    of built-in algorithms. In other words, you don’t have to use external libraries
    or bring your own algorithm to play with K-Means on AWS.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你应该知道AWS已经将K-Means算法作为其内置算法列表的一部分实现了。换句话说，你不需要使用外部库或自己带来算法来在AWS上使用K-Means。
- en: Conclusion
  id: totrans-419
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: 'That was a really good accomplishment: you just mastered the basics of clustering
    algorithms and you should now be able to drive your own projects and research
    about this topic! For the exam, remember that clustering belongs to the unsupervised
    field of machine learning, so there is no need to have labeled data.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是一个了不起的成就：你刚刚掌握了聚类算法的基础，你现在应该能够推动你自己的项目和关于这个主题的研究！对于考试，记住聚类属于机器学习的无监督领域，因此不需要有标记的数据。
- en: Also, make sure that you know how the most popular algorithm of this field works
    – that is, K-Means. Although clustering algorithms do not provide the meaning
    of each group, they are very powerful for finding patterns in the data, either
    to model a particular problem or just to explore the data.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，确保你知道这个领域最流行的算法是如何工作的——那就是K-Means。尽管聚类算法不提供每个组的含义，但它们在寻找数据中的模式方面非常强大，无论是为了建模特定问题还是仅仅为了探索数据。
- en: Coming up next, you will keep studying unsupervised algorithms and see how AWS
    has built one of the most powerful algorithms out there for anomaly detection,
    known as **RCF**.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将继续学习无监督算法，并了解AWS如何构建了目前最强大的异常检测算法之一，该算法被称为**RCF**。
- en: Anomaly detection
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常检测
- en: Finding anomalies in data is a very common task in modeling and data exploratory
    analysis. Sometimes, you might want to find anomalies in the data just to remove
    them before fitting a regression model, while other times, you might want to create
    a model that identifies anomalies as an end goal – for example, in fraud detection
    systems.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模和数据探索性分析中寻找数据中的异常是一个非常常见的任务。有时，你可能只想在拟合回归模型之前删除数据中的异常，而有时，你可能想创建一个以识别异常为最终目标的模型——例如，在欺诈检测系统中。
- en: 'Again, you can use many different methods to find anomalies in the data. With
    some creativity, the possibilities are endless. However, there is a particular
    algorithm that works around this problem that you should definitely be aware of
    for your exam: RCF.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，你可以使用许多不同的方法来寻找数据中的异常。只要有些创意，可能性是无限的。然而，有一个特定的算法可以解决这个问题，你应该在考试中务必了解：RCF。
- en: RCF is an unsupervised decision tree-based algorithm that creates multiple decision
    trees (forests) using random subsamples of the training data. Technically, it
    randomizes the data and then creates samples according to the number of trees.
    Finally, these samples are distributed across each tree.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: RCF是一种基于无监督决策树的算法，它使用训练数据的随机子样本创建了多个决策树（森林）。技术上，它随机化数据，然后根据树的数量创建样本。最后，这些样本被分配到每一棵树上。
- en: These sets of trees are used to assign an anomaly score to the data points.
    To calculate the anomaly score for a particular data point, it is passed down
    each tree in the forest. As the data point moves through the tree, the path length
    from the root node to the leaf node is recorded for that specific tree. The anomaly
    score for that data point is then determined by considering the distribution of
    path lengths across all the trees in the forest.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 这些树集被用来为数据点分配异常分数。为了计算特定数据点的异常分数，它被传递到森林中的每一棵树。当数据点穿过树时，记录从根节点到叶节点的路径长度，这是针对特定树的。然后，通过考虑森林中所有树的路径长度分布，确定该数据点的异常分数。
- en: If a data point follows a short path in most trees (i.e., it is close to the
    root node), it is considered a common point and will have a lower anomaly score.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个数据点在大多数树中遵循一条短路径（即它靠近根节点），则被视为一个常见点，并将具有较低的异常分数。
- en: On the other hand, if a data point follows a long path in many trees (i.e.,
    it is far from the root node), it is considered an uncommon point and will have
    a higher anomaly score.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果一个数据点在许多树中遵循一条长路径（即它远离根节点），则被视为一个不常见点，并将具有较高的异常分数。
- en: The most important hyperparameters of RCF are `num_trees` and `num_samples_per_tree`,
    which are the number of trees in the forest and the number of samples per tree,
    respectively.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: RCF最重要的超参数是`num_trees`和`num_samples_per_tree`，分别代表森林中的树的数量和每棵树中的样本数量。
- en: Dimensionality reduction
  id: totrans-431
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维度降低
- en: Another unsupervised algorithm that was implemented by AWS in its list of built-in
    algorithms is known as principal component analysis, or PCA for short. PCA is
    a technique that’s used to reduce the number of variables/dimensions in a dataset.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: AWS在其内置算法列表中实现的一种无监督算法被称为主成分分析，简称PCA。PCA是一种用于减少数据集中变量/维度的数量的技术。
- en: The main idea behind PCA is plotting the data points to another set of coordinates,
    known as **Principal Components (PCs)**, which aims to explain the most variance
    in the data. By definition, the first component will capture more variance than
    the second component, then the second component will capture more variance than
    the third one, and so on.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）背后的主要思想是将数据点绘制到另一组坐标上，这组坐标被称为**主成分（PCs）**，其目的是解释数据中的最大方差。根据定义，第一个成分将比第二个成分捕获更多的方差，然后第二个成分将比第三个成分捕获更多的方差，依此类推。
- en: 'You can set up as many PCs as you need, as long as it does not surpass the
    number of variables in your dataset. *Figure 6**.18* shows how these PCs are drawn:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以设置你需要的PCs数量，只要它不超过你的数据集中的变量数量。*图6.18*显示了这些PCs是如何绘制的：
- en: '![Figure 6.18 – Finding PCs in PCA](img/B21197_06_18.jpg)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
  zh: '![图6.18 – PCA中的PCs寻找](img/B21197_06_18.jpg)'
- en: Figure 6.18 – Finding PCs in PCA
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 – PCA中的PCs寻找
- en: As mentioned previously, the first PC will be drawn in such a way that it will
    capture most of the variance in the data. That is why it passes near the majority
    of the data points in *Figure 6**.18*.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，第一个PC将以这种方式绘制，以便它能够捕获数据中的大部分变差。这就是为什么它在*图6**.18*中靠近大多数数据点的原因。
- en: Then, the second PC will be perpendicular to the first one, so that it will
    be the second component that explains the variance in the data. If you want to
    create more components (consequentially, capturing more variance), you just have
    to follow the same rule of adding perpendicular components. **Eigenvectors** and
    **eigenvalues** are the linear algebra concepts associated with PCA that compute
    the PCs.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，第二台PC将垂直于第一台，因此它将成为解释数据中变差的第二个组成部分。如果您想创建更多组件（从而捕获更多变差），您只需遵循添加垂直组件的相同规则。**特征向量**和**特征值**是与PCA相关的线性代数概念，用于计算主成分。
- en: So, what is the story with dimension reduction here? In case it is not clear
    yet, these PCs can be used to replace your original variables. For example, consider
    you have 10 variables in your dataset, and you want to reduce this dataset to
    three variables that best represent the others. A potential solution for that
    would be applying PCA and extracting the first three PCs!
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，关于这里的降维有什么故事？如果还不清楚，这些PC可以用来替换您的原始变量。例如，假设您的数据集中有10个变量，您想将此数据集减少到三个最能代表其他变量的变量。解决这个问题的潜在方法就是应用PCA并提取前三个PC！
- en: Do these three components explain 100% of your dataset? Probably not, but ideally,
    they will explain most of the variance. Adding more PCs will explain more variance
    but at the cost of adding extra dimensions.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个组件解释了您数据集的100%吗？可能不是，但理想情况下，它们将解释大部分变差。添加更多PC将解释更多变差，但代价是增加额外的维度。
- en: Using AWS’s built-in algorithm for PCA
  id: totrans-441
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用AWS内置的PCA算法
- en: 'In AWS, PCA works in two different modes:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS中，PCA以两种不同的模式工作：
- en: '**Regular**: For datasets with a moderate number of observations and features'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**常规**：适用于具有适度观察和特征的集合'
- en: '**Randomized**: For datasets with a large number of observations and features'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机化**：适用于具有大量观察和特征的集合'
- en: The difference is that, in randomized mode, it is used as an approximation algorithm.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 不同之处在于，在随机模式下，它被用作近似算法。
- en: Of course, the main hyperparameter of PCA is the number of components that you
    want to extract, known as `num_components`.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，PCA的主要超参数是您想要提取的组件数量，称为`num_components`。
- en: IP Insights
  id: totrans-447
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IP洞察
- en: IP Insights is an unsupervised algorithm that is used for pattern recognition.
    Essentially, it learns the usage pattern of IPv4 addresses.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: IP洞察是一种无监督算法，用于模式识别。本质上，它学习IPv4地址的使用模式。
- en: 'The *modus operandi* of this algorithm is very intuitive: it is trained on
    top of pairs of events in the format of entity and IPv4 address so that it can
    understand the pattern of each entity that it was trained on.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的**操作方法**非常直观：它在实体和IPv4地址格式的成对事件上训练，以便它可以理解它所训练的每个实体的模式。
- en: Important note
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For instance, you can understand “entity” as user IDs or account numbers.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以理解“实体”为用户ID或账户号码。
- en: Then, to make predictions, it receives a pair of events with the same data structure
    (entity, IPv4 address) and returns an anomaly score for that particular IP address,
    according to the input entity.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了进行预测，它接收具有相同数据结构（实体，IPv4地址）的成对事件，并返回针对该特定IP地址的异常分数，根据输入实体。
- en: Important note
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: This anomaly score that is returned by IP Insights infers how anomalous the
    pattern of the event is.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: IP洞察返回的这种异常分数推断事件模式的异常程度。
- en: You might come across many applications with IP Insights. For example, you can
    create an IP Insights model that was trained on top of your application login
    events (this is your entity). You should be able to expose this model through
    an API endpoint to make predictions in real time.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会遇到许多带有IP洞察的应用。例如，您可以在您的应用程序登录事件（这是您的实体）上训练一个IP洞察模型。您应该能够通过API端点公开此模型，以便实时进行预测。
- en: Then, during the authentication process of your application, you could call
    your endpoint and pass the IP address that is trying to log in. If you got a high
    score (meaning this pattern of logging in looks anomalous), you can request extra
    information before authorizing access (even if the password was right).
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在您应用程序的认证过程中，您可以调用您的端点并传递尝试登录的IP地址。如果您得到了高分（这意味着这种登录模式看起来异常），在授权访问之前（即使密码是正确的），您可以请求更多信息。
- en: This is just one of the many applications of IP Insights you could think about.
    Next, you will learn about textual analysis.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是您可以考虑的IP Insights的许多应用之一。接下来，您将了解文本分析。
- en: Textual analysis
  id: totrans-458
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分析
- en: Modern applications use **Natural Language Processing (NLP)** for several purposes,
    such as text translation, document classifications, web search, **Named Entity
    Recognition (NER)**, and many others.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 现代应用使用**自然语言处理（NLP）**进行多种目的，例如文本翻译、文档分类、网络搜索、**命名实体识别（NER）**等。
- en: AWS offers a suite of algorithms for most NLP use cases. In the next few subsections,
    you will have a look at these built-in algorithms for textual analysis.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: AWS为大多数NLP用例提供了一套算法。在接下来的几个小节中，您将了解这些内置的文本分析算法。
- en: BlazingText algorithm
  id: totrans-461
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BlazingText算法
- en: 'BlazingText does two different types of tasks: text classification, which is
    a supervised learning approach that extends the **fastText** text classifier,
    and Word2Vec, which is an unsupervised learning algorithm.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: BlazingText执行两种不同的任务：文本分类，这是一种监督学习方法，扩展了**fastText**文本分类器，以及Word2Vec，这是一种无监督学习算法。
- en: BlazingText’s implementations of these two algorithms are optimized to run on
    large datasets. For example, you can train a model on top of billions of words
    in a few minutes.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: BlazingText对这些两种算法的实现进行了优化，以便在大数据集上运行。例如，您可以在几分钟内训练一个在数十亿单词之上的模型。
- en: 'This scalability aspect of BlazingText is possible due to the following:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: BlazingText的这一可扩展性方面是由于以下原因实现的：
- en: Its ability to use multi-core CPUs and a single GPU to accelerate text classification
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它能够使用多核CPU和单个GPU来加速文本分类。
- en: Its ability to use multi-core CPUs or GPUs, with custom CUDA kernels for GPU
    acceleration, when playing with the Word2Vec algorithm
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在玩Word2Vec算法时能够使用多核CPU或GPU，以及定制的CUDA内核进行GPU加速。
- en: The Word2Vec option supports **batch_skipgram** mode, which allows BlazingText
    to do distributed training across multiple CPUs.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec选项支持**batch_skipgram**模式，这使得BlazingText能够在多个CPU上执行分布式训练。
- en: Important note
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The distributed training that’s performed by BlazingText uses a mini-batching
    approach to convert **level-1 BLAS (Basic Linear Algebra Subprograms)** operations
    into **level-3 BLAS** operations. If you see these terms during your exam, you
    should know that they are related to BlazingText (Word2Vec mode).
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: BlazingText执行的分布式训练使用小批量方法将**一级BLAS（基本线性代数子程序）**操作转换为**三级BLAS**操作。如果在考试中遇到这些术语，应知道它们与BlazingText（Word2Vec模式）相关。
- en: Still in Word2Vec mode, BlazingText supports both the **skip-gram** and **Continuous
    Bag of Words (****CBOW)** architectures.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然在Word2Vec模式下，BlazingText支持**skip-gram**和**连续词袋（CBOW）**架构。
- en: 'Finally, note the following configurations of BlazingText, since they are likely
    to be present in your exam:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意以下BlazingText的配置，因为它们很可能出现在您的考试中：
- en: In Word2Vec mode, only the train channel is available.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Word2Vec模式下，只有训练通道可用。
- en: BlazingText expects a single text file with space-separated tokens. Each line
    of the file must contain a single sentence. This means you usually have to preprocess
    your corpus of data before using BlazingText.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BlazingText期望一个单独的文本文件，其中包含空格分隔的标记。文件的每一行必须包含一个句子。这意味着在使用BlazingText之前，通常需要预处理您的数据集。
- en: Sequence-to-sequence algorithm
  id: totrans-474
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列到序列算法
- en: This is a supervised algorithm that transforms an input sequence into an output
    sequence. This sequence can be a text sentence or even an audio recording.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个将输入序列转换为输出序列的监督算法。这个序列可以是文本句子，甚至是音频记录。
- en: The most common use cases for sequence-to-sequence are machine translation,
    text summarization, and speech-to-text. Anything that you think is a sequence-to-sequence
    problem can be approached by this algorithm.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列最常见的用例是机器翻译、文本摘要和语音到文本。您认为任何是序列到序列问题的事情都可以通过这个算法来处理。
- en: 'Technically, AWS SageMaker’s Seq2Seq uses two types of neural networks to create
    models: an **RNN** and a **Convolutional Neural Network (CNN)** with an attention
    mechanism.'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，AWS SageMaker的Seq2Seq使用两种类型的神经网络来创建模型：一个带有注意力机制的**RNN**和一个**卷积神经网络（CNN**）。
- en: '**Latent Dirichlet allocation**, or **LDA** for short, is used for topic modeling.
    Topic modeling is a textual analysis technique where you can extract a set of
    topics from a corpus of text data. LDA learns these topics based on the probability
    distribution of the words in the corpus of text.'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**，简称**LDA**，用于主题建模。主题建模是一种文本分析技术，可以从文本数据语料库中提取一组主题。LDA基于文本语料库中单词的概率分布来学习这些主题。'
- en: Since this is an unsupervised algorithm, there is no need to set a target variable.
    Also, the number of topics must be specified up-front, and you will have to analyze
    each topic to find its domain meaning.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个无监督算法，因此不需要设置目标变量。此外，必须事先指定主题的数量，并且你必须分析每个主题以找到其领域含义。
- en: Neural Topic Model algorithm
  id: totrans-480
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经主题模型算法
- en: Just like the LDA algorithm, the **Neural Topic Model (NTM)** also aims to extract
    topics from a corpus of data. However, the difference between LDA and NTM is their
    learning logic. While LDA learns from probability distributions of the words in
    the documents, NTM is built on top of neural networks.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 就像LDA算法一样，**神经主题模型（NTM**）也旨在从数据语料库中提取主题。然而，LDA和NTM之间的区别在于它们的学习逻辑。LDA通过文档中单词的概率分布来学习这些主题，而NTM建立在神经网络之上。
- en: The NTM network architecture has a bottleneck layer, which creates an embedding
    representation of the documents. This bottleneck layer contains all the necessary
    information to predict document composition, and its coefficients can be considered
    topics.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: NTM网络架构有一个瓶颈层，它创建文档的嵌入表示。这个瓶颈层包含预测文档组成所需的所有必要信息，其系数可以被认为是主题。
- en: With that, you have completed this section on textual analysis. In the next
    section, you will learn about image processing algorithms.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，你已经完成了关于文本分析的这一部分。在下一部分，你将学习关于图像处理算法的内容。
- en: Image processing
  id: totrans-484
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像处理
- en: 'Image processing is a very popular topic in machine learning. The idea is pretty
    self-explanatory: creating models that can analyze images and make inferences
    on top of them. By inference, you can understand this as detecting objects in
    an image, classifying images, and so on.'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 图像处理是机器学习中的一个非常热门的话题。其想法相当直观：创建可以分析图像并在其上做出推断的模型。通过推断，你可以理解为检测图像中的对象、对图像进行分类等等。
- en: AWS offers a set of built-in algorithms you can use to train image processing
    models. In the next few sections, you will have a look at those algorithms.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供了一套内置算法，你可以使用这些算法来训练图像处理模型。在接下来的几节中，你将了解这些算法。
- en: Image classification algorithm
  id: totrans-487
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像分类算法
- en: As the name suggests, the image classification algorithm is used to classify
    images using supervised learning. In other words, it needs a label within each
    image. It supports multi-label classification.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，图像分类算法用于使用监督学习对图像进行分类。换句话说，它需要在每个图像中有一个标签。它支持多标签分类。
- en: 'The way it operates is simple: during training, it receives an image and its
    associated labels. During inference, it receives an image and returns all the
    predicted labels. The image classification algorithm uses a CNN (**ResNet**) for
    training. It can either train the model from scratch or take advantage of transfer
    learning to pre-load the first few layers of the neural network.'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 它的工作方式很简单：在训练期间，它接收一个图像及其相关的标签。在推理期间，它接收一个图像并返回所有预测的标签。图像分类算法使用CNN（**ResNet**）进行训练。它可以从头开始训练模型，或者利用迁移学习预先加载神经网络的前几层。
- en: According to AWS’s documentation, the `.jpg` and `.png` file formats are supported,
    but the recommended format is **MXNet’s RecordIO**.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 根据AWS的文档，支持`.jpg`和`.png`文件格式，但推荐格式是**MXNet的RecordIO**。
- en: Semantic segmentation algorithm
  id: totrans-491
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义分割算法
- en: The semantic segmentation algorithm provides a pixel-level capability for creating
    computer vision applications. It tags each pixel of the image with a class, which
    is an important feature for complex applications such as self-driving and medical
    image diagnostics.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割算法提供了创建计算机视觉应用的像素级能力。它将图像的每个像素标记为类别，这对于自动驾驶和医学图像诊断等复杂应用是一个重要特性。
- en: 'In terms of its implementation, the semantic segmentation algorithm uses the
    **MXNet Gluon framework** and the **Gluon CV toolkit**. You can choose any of
    the following algorithms to train a model:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 在其实现方面，语义分割算法使用**MXNet Gluon框架**和**Gluon CV工具包**。你可以选择以下任何算法来训练模型：
- en: '**Fully Convolutional** **Network (FCN)**'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全卷积网络（FCN**）'
- en: '**Pyramid Scene** **Parsing (PSP)**'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金字塔场景解析（PSP**）'
- en: DeepLabV3
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepLabV3
- en: All these options work as an **encoder-decoder** neural network architecture.
    The output of the network is known as a **segmentation mask**.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些选项都作为**编码器-解码器**神经网络架构工作。网络的输出被称为**分割掩码**。
- en: Object detection algorithm
  id: totrans-498
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标检测算法
- en: 'Just like the image classification algorithm, the main goal of the object detection
    algorithm is also self-explanatory: it detects and classifies objects in images.
    It uses a supervised approach to train a deep neural network.'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 就像图像分类算法一样，目标检测算法的主要目标也是不言而喻的：它在图像中检测和分类对象。它使用监督方法来训练深度神经网络。
- en: 'During the inference process, this algorithm returns the identified objects
    and a score of confidence regarding the prediction. The object detection algorithm
    uses **Single Shot MultiBox Detector (SSD)** and supports two types of network
    architecture: **Visual Geometry Group (VGG)** and **Residual** **Network (ResNet).**'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，此算法返回识别出的对象以及关于预测的置信度分数。目标检测算法使用**单次多框检测器（SSD**）并支持两种类型的网络架构：**视觉几何组（VGG**）和**残差网络（ResNet**）。
- en: Summary
  id: totrans-501
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'That was such a journey! Take a moment to recap what you have just learned.
    This chapter had four main topics: supervised learning, unsupervised learning,
    textual analysis, and image processing. Everything that you have learned fits
    into those subfields of machine learning.'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是一次难忘的旅程！花点时间回顾一下你刚刚学到的内容。本章有四个主要主题：监督学习、无监督学习、文本分析和图像处理。你所学的所有内容都适合于机器学习的这些子领域。
- en: 'The list of supervised learning algorithms that you have studied includes the
    following:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 你所研究的监督学习算法列表包括以下内容：
- en: Linear learner
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性学习器
- en: Factorization machines
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分解机
- en: XGBoost
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost
- en: KNN
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KNN
- en: Object2Vec
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Object2Vec
- en: DeepAR forecasting
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepAR预测
- en: Remember that you can use linear learner, factorization machines, XGBoost, and
    KNN for multiple purposes, including solving regression and classification problems.
    Linear learner is probably the simplest algorithm out of these four; factorization
    machines extends linear earner and is good for sparse datasets, XGBoost uses an
    ensemble method based on decision trees, and KNN is an index-based algorithm.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，你可以使用线性学习器、分解机、XGBoost和KNN来完成多种目的，包括解决回归和分类问题。线性学习器可能是这四种算法中最简单的；分解机扩展了线性学习器，适用于稀疏数据集，XGBoost使用基于决策树的集成方法，而KNN是一种基于索引的算法。
- en: The other two algorithms, Object2Vec and DeepAR, are used for specific purposes.
    Object2Vec is used to create vector representations of the data, while DeepAR
    is used to create forecast models.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 其他两种算法，Object2Vec和DeepAR，用于特定的目的。Object2Vec用于创建数据的向量表示，而DeepAR用于创建预测模型。
- en: 'The list of unsupervised learning algorithms that you have studied includes
    the following:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 你所研究的无监督学习算法列表包括以下内容：
- en: K-Means
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-Means
- en: PCA
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA
- en: IP Insights
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IP Insights
- en: RCF
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RCF
- en: K-Means is a very popular algorithm that is used for clustering. PCA is used
    for dimensionality reduction, IP Insights is used for pattern recognition, and
    RCF is used for anomaly detection.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means是一个非常流行的算法，用于聚类。PCA用于降维，IP Insights用于模式识别，RCF用于异常检测。
- en: You then looked at regression models and K-Means in more detail. You did this
    because, as a data scientist, you should at least master these two very popular
    algorithms so that you can go deeper into other algorithms by yourself.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你更详细地研究了回归模型和K-Means。你这样做是因为，作为一名数据科学家，你应该至少掌握这两个非常流行的算法，这样你就可以自己深入研究其他算法。
- en: 'Then, you moved on to the second half of this chapter, where you learned about
    textual analysis and the following algorithms:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你继续学习本章的第二部分，其中你学习了文本分析和以下算法：
- en: BlazingText
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BlazingText
- en: Sequence-to-sequence
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列到序列
- en: LDA
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDA
- en: NTM
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NTM
- en: 'Finally, you learned about image processing and looked at the following:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你学习了图像处理，并查看以下内容：
- en: Image classification algorithm
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分类算法
- en: Semantic segmentation algorithm
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义分割算法
- en: Object detection algorithm
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测算法
- en: Since the topics covered in this chapter are very important with regard to the
    AWS Certified Machine Learning Specialty exam, you are highly encouraged to jump
    into the AWS website and search for machine learning algorithms. There, you will
    find the most recent information about the algorithms that you have just learned
    about. Please make sure you do it before taking the exam.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本章涉及的主题对于AWS认证机器学习专业考试非常重要，我们强烈建议您访问AWS网站并搜索机器学习算法。在那里，您将找到您刚刚学到的算法的最新信息。请确保在考试前完成此操作。
- en: That brings you to the end of this quick refresher and the end of this chapter.
    In the next chapter, you will learn about the existing mechanisms provided by
    AWS that you can use to optimize and evaluate these algorithms.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了这个快速复习和本章的内容。在下一章中，您将了解AWS提供的现有机制，您可以使用这些机制来优化和评估这些算法。
- en: Exam Readiness Drill – Chapter Review Questions
  id: totrans-530
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 考试准备练习 – 章节复习问题
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对关键概念有扎实的理解外，能够在时间压力下快速思考是一项有助于您通过认证考试的技能。这就是为什么在您的学习旅程早期就培养这些技能是关键。
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 章节复习问题旨在随着您学习并复习每个章节的内容，逐步提高您的应试技巧，同时检查您对章节中关键概念的理解。您将在每个章节的末尾找到这些内容。
- en: How To Access These Resources
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 如何访问这些资源
- en: To learn how to access these resources, head over to the chapter titled [*Chapter
    11*](B21197_11.xhtml#_idTextAnchor1477), *Accessing the Online* *Practice Resources*.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何访问这些资源，请转到名为[*第11章*](B21197_11.xhtml#_idTextAnchor1477)的章节，*访问在线练习资源*。
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 要打开本章的章节复习问题，请执行以下步骤：
- en: Click the link – [https://packt.link/MLSC01E2_CH06](https://packt.link/MLSC01E2_CH06).
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击链接 – [https://packt.link/MLSC01E2_CH06](https://packt.link/MLSC01E2_CH06)。
- en: 'Alternatively, you can scan the following **QR code** (*Figure 6**.19*):'
  id: totrans-537
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，您可以扫描以下**二维码**（*图6.19*）：
- en: '![Figure 6.19 – QR code that opens Chapter Review Questions for logged-in users](img/B21197_06_19.jpg)'
  id: totrans-538
  prefs: []
  type: TYPE_IMG
  zh: '![图6.19 – 为登录用户打开章节复习问题的二维码](img/B21197_06_19.jpg)'
- en: Figure 6.19 – QR code that opens Chapter Review Questions for logged-in users
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19 – 为登录用户打开章节复习问题的二维码
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 6**.20*:'
  id: totrans-540
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦登录，您将看到一个类似于*图6.20*所示的页面：
- en: '![Figure 6.20 – Chapter Review Questions for Chapter 6](img/B21197_06_20.jpg)'
  id: totrans-541
  prefs: []
  type: TYPE_IMG
  zh: '![图6.20 – 第6章的章节复习问题](img/B21197_06_20.jpg)'
- en: Figure 6.20 – Chapter Review Questions for Chapter 6
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20 – 第6章的章节复习问题
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  id: totrans-543
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备就绪后，开始以下练习，多次重新尝试测验。
- en: Exam Readiness Drill
  id: totrans-544
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 考试准备练习
- en: For the first three attempts, don’t worry about the time limit.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前三次尝试，不必担心时间限制。
- en: ATTEMPT 1
  id: totrans-546
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试1
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次，目标至少达到**40%**。查看您答错的答案，并再次阅读章节中的相关部分以修复学习差距。
- en: ATTEMPT 2
  id: totrans-548
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试2
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次，目标至少达到**60%**。查看您答错的答案，并再次阅读章节中的相关部分以修复任何剩余的学习差距。
- en: ATTEMPT 3
  id: totrans-550
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试3
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 第三次，目标至少达到**75%**。一旦得分达到75%或更高，您就开始练习计时。
- en: Tip
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要超过**三次**尝试才能达到75%。没关系。只需复习章节中的相关部分，直到达到目标。
- en: Working On Timing
  id: totrans-554
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正在练习计时
- en: 'Target: Your aim is to keep the score the same while trying to answer these
    questions as quickly as possible. Here’s an example of how your next attempts
    should look like:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 目标：您的目标是保持得分不变，同时尽可能快速地回答这些问题。以下是如何进行下一次尝试的示例：
- en: '| **Attempt** | **Score** | **Time Taken** |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| **尝试** | **得分** | **用时** |'
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| 尝试 5 | 77% | 21 分 30 秒 |'
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| 尝试 6 | 78% | 18 分 34 秒 |'
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| 尝试 7 | 76% | 14 分 44 秒 |'
- en: Table 6.11 – Sample timing practice drills on the online platform
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.11 – 在线平台上的样本时间练习练习
- en: Note
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 上表中显示的时间限制只是示例。根据网站上的测验时间限制，每次尝试时自行设定你的时间限制。
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 每次新的尝试，你的分数应保持在 **75%** 以上，同时完成所需的时间“应减少”。重复尝试，直到你觉得自己能够自信地应对时间压力。
