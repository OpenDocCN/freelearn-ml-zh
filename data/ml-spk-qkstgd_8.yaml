- en: Real-Time Machine Learning Using Apache Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Spark进行实时机器学习
- en: 'In this chapter, we will extend our deployment of machine learning models beyond
    batch processing in order to learn from data, make predictions, and identify trends
    in real time! We will develop and deploy a real-time stream processing and machine
    learning application comprised of the following high-level technologies:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将扩展我们的机器学习模型部署，使其超越批量处理，以便从数据中学习、做出预测和实时识别趋势！我们将开发并部署一个由以下高级技术组成的实时流处理和机器学习应用程序：
- en: Apache Kafka producer application
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Kafka生产者应用程序
- en: Apache Kafka consumer application
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Kafka消费者应用程序
- en: Apache Spark's Structured Streaming engine
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark的Structured Streaming引擎
- en: Apache Spark's machine learning library, `MLlib`
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark的机器学习库，`MLlib`
- en: Distributed streaming platform
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式流平台
- en: 'So far in this book, we have been performing batch processing—that is, we have
    been provided with bounded raw data files and processed that data as a group.
    As we saw in [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml), *The Big
    Data Ecosystem*, stream processing differs from batch processing in the fact that
    data is processed as and when individual units, or streams, of data arrive. We
    also saw in [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml), *The Big
    Data Ecosystem*, how **Apache Kafka**, as a distributed *streaming platform*,
    allows us to move real-time data between systems and applications in a fault-tolerant
    and reliable manner via a logical streaming architecture comprising of the following
    components:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这本书中，我们一直在执行批量处理——也就是说，我们被提供了有界原始数据文件，并将这些数据作为一个组进行处理。正如我们在[第1章](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml)，“大数据生态系统”中看到的，流处理与批量处理的不同之处在于数据是按需或单个数据单元（或流）到达时处理的。我们还在[第1章](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml)，“大数据生态系统”中看到，**Apache
    Kafka**作为一个分布式*流平台，通过以下组件的逻辑流架构，以容错和可靠的方式在系统和应用程序之间移动实时数据：
- en: '**Producers**: Applications that generate and send messages'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生产者**：生成并发送消息的应用程序'
- en: '**Consumers**: Applications that subscribe to and consume messages'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费者**：订阅并消费消息的应用程序'
- en: '**Topics**: Streams of records belonging to a particular category and stored
    as a sequence of ordered and immutable records partitioned and replicated across
    a distributed cluster'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题**：属于特定类别并存储为有序且不可变记录序列的记录流，这些记录在分布式集群中分区和复制'
- en: '**Stream processors**: Applications that process messages in a certain manner,
    such as data transformations and machine learning models'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流处理器**：以特定方式处理消息的应用程序，例如数据转换和机器学习模型'
- en: 'A simplified illustration of this logical streaming architecture is shown in *Figure
    8.1*:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 该逻辑流架构的简化示意图如图*8.1*所示：
- en: '![](img/92b5f057-8b17-4e0f-a931-9f5cb7851f82.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92b5f057-8b17-4e0f-a931-9f5cb7851f82.png)'
- en: 'Figure 8.1: Apache Kafka logical streaming architecture'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：Apache Kafka逻辑流架构
- en: Distributed stream processing engines
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式流处理引擎
- en: 'Apache Kafka allows us to *move* real-time data reliably between systems and
    applications. But we still need some sort of processing engine to process and
    transform that real-time data in order ultimately to derive value from it based
    on the use case in question. Fortunately, there are a number of *stream processing
    engines* available to allow us to do this, including—but not limited—to the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka使我们能够在系统和应用程序之间可靠地移动实时数据。但是，我们仍然需要一个某种处理引擎来处理和转换这些实时数据，以便根据特定用例从中提取价值。幸运的是，有多个*流处理引擎*可供我们使用，包括但不限于以下：
- en: '**Apache Spark:** [https://spark.apache.org/](https://spark.apache.org/)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Spark**：[https://spark.apache.org/](https://spark.apache.org/)'
- en: '**Apache Storm:** [http://storm.apache.org/](http://storm.apache.org/)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Storm**：[http://storm.apache.org/](http://storm.apache.org/)'
- en: '**Apache Flink: **[https://flink.apache.org/](https://flink.apache.org/)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Flink**：[https://flink.apache.org/](https://flink.apache.org/)'
- en: '**Apache Samza:** [http://samza.apache.org/](http://samza.apache.org/)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Samza**：[http://samza.apache.org/](http://samza.apache.org/)'
- en: '**Apache Kafka (via its Streams API):** [https://kafka.apache.org/documentation/](https://kafka.apache.org/documentation/)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Kafka（通过其Streams API）**：[https://kafka.apache.org/documentation/](https://kafka.apache.org/documentation/)'
- en: '**KSQL:** [https://www.confluent.io/product/ksql/](https://www.confluent.io/product/ksql/)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KSQL**：[https://www.confluent.io/product/ksql/](https://www.confluent.io/product/ksql/)'
- en: Though a detailed comparison of the available stream processing engines is beyond
    the scope of this book, you are encouraged to explore the preceding links and
    study the differing architectures available. For the purposes of this chapter,
    we will be using Apache Spark's Structured Streaming engine as our stream processing
    engine of choice.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对可用的流处理引擎进行详细比较超出了本书的范围，但鼓励读者探索前面的链接并研究可用的不同架构。为了本章的目的，我们将使用 Apache Spark
    的 Structured Streaming 引擎作为我们选择的流处理引擎。
- en: Streaming using Apache Spark
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Apache Spark 进行流式传输
- en: 'At the time of writing, there are two stream processing APIs available in Spark:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Spark 中提供了两个流处理 API：
- en: '**Spark Streaming (DStreams): **[https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Streaming (DStreams):** [https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)'
- en: '**Structured Streaming:** [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Structured Streaming:** [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)'
- en: Spark Streaming (DStreams)
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Streaming (DStreams)
- en: '*Spark Streaming (DStreams)* extends the core Spark API and works by dividing
    real-time data streams into *input batches* that are then processed by Spark''s
    core API, resulting in a final stream of *processed batches*, as illustrated in
    *Figure 8.2*. A sequence of RDDs form what is known as a *discretized stream*
    (or DStream), which represents the continuous stream of data:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*Spark Streaming (DStreams)* 扩展了核心 Spark API，通过将实时数据流划分为 *输入批次*，然后由 Spark 的核心
    API 处理，从而生成最终的 *处理批次* 流，如图 8.2 所示。一系列 RDD 构成了所谓的 *离散流*（或 DStream），它代表了数据的连续流：'
- en: '![](img/bfd910ee-aea3-4890-92e1-10daa23bb5c5.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bfd910ee-aea3-4890-92e1-10daa23bb5c5.png)'
- en: 'Figure 8.2: Spark Streaming (DStreams)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2：Spark Streaming (DStreams)
- en: Structured Streaming
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Structured Streaming
- en: '*Structured Streaming*, on the other hand, is a newer and highly optimized
    stream processing engine built on the Spark SQL engine in which streaming data
    can be stored and processed using Spark''s Dataset/DataFrame API (see [Chapter
    1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml), *The Big Data Ecosystem*). As
    of Spark 2.3, Structured Streaming offers the ability to process data streams
    using both micro-batch processing, with latencies as low as 100 milliseconds,
    and *continuous processing*, with latencies as low as 1 millisecond (thereby providing
    *true* real-time processing). Structured Streaming works by modelling data streams
    as an unbounded table that is being continuously appended. When a transformation
    or other type of query is processed on this unbounded table, a results table will
    be generated that is representative of that moment in time.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，*Structured Streaming* 是一个基于 Spark SQL 引擎构建的较新的且高度优化的流处理引擎，其中可以使用 Spark
    的 Dataset/DataFrame API 存储和处理流数据（参见第 1 章，*大数据生态系统*）。截至 Spark 2.3，Structured Streaming
    提供了使用微批处理和连续处理两种方式处理数据流的能力，微批处理的延迟低至 100 毫秒，连续处理的延迟低至 1 毫秒（从而提供真正的实时处理）。Structured
    Streaming 通过将数据流建模为一个不断追加的无界表来工作。当对这个无界表执行转换或其他类型的查询时，将生成一个结果表，该表代表了那个时刻的数据。
- en: 'After a configurable trigger interval, new data in the data stream is modeled
    as new rows appended to this unbounded table and the results table is subsequently
    updated, as illustrated in *Figure* *8.3*:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在可配置的触发间隔之后，数据流中的新数据被建模为追加到这个无界表的新行，随后结果表被更新，如图 *8.3* 所示：
- en: '![](img/9b60d2e4-726c-40ca-ab4f-a886e3e85b3f.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b60d2e4-726c-40ca-ab4f-a886e3e85b3f.png)'
- en: 'Figure 8.3: Spark Structured Streaming logical model'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：Spark Structured Streaming 逻辑模型
- en: As streaming data is exposed via the Dataset/DataFrame API, both SQL-like operations
    (including aggregations and joins) and RDD operations (including map and filtering)
    can easily be executed on real-time streams of data. Furthermore, Structured Streaming
    offers features that are designed to cater for data that arrives late, the management
    and monitoring of streaming queries, and the ability to recover from failures.
    As such, Structured Streaming is an extremely versatile, efficient, and reliable
    way to process streaming data with extremely low latencies, and is the stream
    processing engine that we will use for the remainder of this chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于流数据通过Dataset/DataFrame API暴露，因此可以轻松地在实时数据流上执行SQL-like操作（包括聚合和连接）和RDD操作（包括map和过滤）。此外，结构化流提供了针对迟到数据的处理、流查询的管理和监控以及从故障中恢复的能力。因此，结构化流是一种极其灵活、高效且可靠的流数据处理方式，具有极低的延迟，是我们将在本章剩余部分使用的流处理引擎。
- en: In general, it is advised that developers use this newer and highly optimized
    engine over Spark Streaming (DStreams). However, since it is a newer API, there
    may be certain features that are not yet available as of Spark 2.3.2, which will
    mean the continued occasional usage of the DStreams RDD-based approach while the
    newer API is being developed.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常建议开发者使用这个较新且高度优化的引擎而不是Spark Streaming（DStreams）。然而，由于这是一个较新的API，截至Spark 2.3.2，可能某些功能尚未提供，这意味着在开发新API的同时，DStreams
    RDD-based方法仍会偶尔使用。
- en: Stream processing pipeline
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流处理管道
- en: 'In this section, we will develop an end-to-end stream processing pipeline that
    is capable of streaming data from a source system that generates continuous data,
    and thereafter able to publish those streams to an Apache Kafka distributed cluster.
    Our stream processing pipeline will then use Apache Spark to both consume data
    from Apache Kafka, using its Structured Streaming engine, and apply trained machine
    learning models to these streams in order to derive insights in real time using
    `MLlib`. The end-to-end stream processing pipeline that we will develop is illustrated
    in *Figure 8.4*:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开发一个端到端流处理管道，它能够从生成连续数据的数据源系统中流式传输数据，然后能够将这些流发布到Apache Kafka分布式集群。我们的流处理管道将使用Apache
    Spark从Apache Kafka中消费数据，使用其结构化流引擎，并将训练好的机器学习模型应用于这些流，以使用`MLlib`实时提取洞察。我们将开发的端到端流处理管道如图8.4所示：
- en: '![](img/79b87bac-111a-4861-8808-9654b33c059c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/79b87bac-111a-4861-8808-9654b33c059c.png)'
- en: 'Figure 8.4: Our end-to-end stream processing pipeline'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4：我们的端到端流处理管道
- en: Case study – real-time sentiment analysis
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究 – 实时情感分析
- en: In the case study for this chapter, we will extend the sentiment analysis model
    that we developed in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural
    Language Processing Using Apache Spark*, to operate in real time. In [Chapter
    6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural Language Processing Using
    Apache Spark*, we trained a decision tree classifier to predict and classify the
    underlying sentiment of tweets based on a training dataset of historic tweets
    about airlines. In this chapter, we will apply this trained decision tree classifier
    to real-time tweets in order to predict their sentiment and identify negative
    tweets so that airlines may act on them as soon as possible.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的案例研究中，我们将扩展我们在[第6章](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml)，“使用Apache
    Spark进行自然语言处理”中开发的情感分析模型，使其能够在实时环境中运行。在第6章“使用Apache Spark进行自然语言处理”中，我们训练了一个决策树分类器，根据关于航空公司的历史推文训练数据集来预测和分类推文的潜在情感。在本章中，我们将应用这个训练好的决策树分类器来处理实时推文，以便预测它们的情感并识别负面推文，以便航空公司能够尽快采取行动。
- en: 'Our end-to-end stream processing pipeline can therefore be extended, as illustrated
    in *Figure 8.5*:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的端到端流处理管道可以扩展，如图8.5所示：
- en: '![](img/07bd36e1-f7f5-4696-9710-517f5f63d164.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/07bd36e1-f7f5-4696-9710-517f5f63d164.png)'
- en: 'Figure 8.5: Our end-to-end stream processing pipeline for real-time sentiment
    analysis'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5：我们的端到端流处理管道，用于实时情感分析
- en: 'The core stages of our stream processing pipeline for real-time sentiment analysis
    are as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于实时情感分析的流处理管道的核心阶段如下：
- en: '**Kafka producer:** We will develop a Python application, using the `pykafka`
    (an Apache Kafka client for Python) and `tweepy` (a Python library for accessing
    the Twitter API) libraries that we installed in [Chapter 2](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml), *Setting
    Up a Local Development Environment*, to capture tweets about airlines that are
    being tweeted in real time and to then publish those tweets to an Apache Kafka
    topic called `twitter`.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Kafka生产者：** 我们将开发一个Python应用程序，使用我们在[第2章](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml)，“设置本地开发环境”中安装的`pykafka`（一个Python的Apache
    Kafka客户端）和`tweepy`（一个用于访问Twitter API的Python库）库，以捕获实时发布的关于航空公司的推文，并将这些推文发布到名为`twitter`的Apache
    Kafka主题。'
- en: '**Kafka consumer:** We will then develop a Spark application, using its Structured
    Streaming API, to subscribe to and then consume tweets from the `twitter` topic
    into a Spark dataframe.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Kafka消费者：** 然后，我们将开发一个Spark应用程序，使用其Structured Streaming API，订阅并从`twitter`主题消费推文到Spark数据框。'
- en: '**Stream processor and** `MLlib`**:** We will then preprocess the raw textual
    content of the tweets stored in this Spark dataframe using the same pipeline of
    feature transformers and feature extractors that we studied and developed in [Chapter
    6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural Language Processing Using
    Apache Spark*, namely tokenization, removing stop words, stemming, and normalization—before
    applying the HashingTF transformer to generate feature vectors in real time.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**流处理器和** `MLlib`**：** 然后，我们将使用我们在[第6章](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml)，“使用Apache
    Spark进行自然语言处理”中研究和开发的相同管道中的特征转换器和特征提取器，对存储在此Spark数据框中的推文的原始文本内容进行预处理，这些特征转换器和特征提取器包括分词、去除停用词、词干提取和归一化——在应用HashingTF转换器生成实时特征向量之前。'
- en: '**Trained decision tree classifier:** Next, we will load the decision tree
    classifier that we trained in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural
    Language Processing Using Apache Spark,* and persisted to the local filesystem
    of our single development node. Once loaded, we will apply this trained decision
    tree classifier to the Spark dataframe containing our preprocessed feature vectors
    derived from real time tweets in order to predict and classify their underlying
    sentiment.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练好的决策树分类器：** 接下来，我们将加载我们在[第6章](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml)，“使用Apache
    Spark进行自然语言处理”中训练的决策树分类器，并将其持久化到我们的单个开发节点的本地文件系统。一旦加载，我们将应用这个训练好的决策树分类器到包含我们从实时推文中提取的预处理特征向量的Spark数据框，以预测和分类其潜在的情感。'
- en: '**Output sink:** Finally, we will output the results of our sentiment analysis
    model applied to real-time tweets to a target destination, called an output *sink*.
    In our case, the output sink will be the *console* sink, one of the built-in output
    sinks provided natively by the Structured Streaming API. By using this sink, the
    output is printed to the console/**standard output** (**stdout***)* every time
    there is a trigger. From this console, we will be able to read both the raw textual
    content of the original tweets and the predicted sentiment classification from
    our model, namely negative or non-negative. To learn more about the various output
    sinks available, please visit [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks).'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输出目标：** 最后，我们将把对实时推文应用的情感分析模型的结果输出到目标目的地，称为输出*目标*。在我们的案例中，输出目标将是*控制台*目标，这是Structured
    Streaming API原生提供的内置输出目标之一。通过使用此目标，每次触发时输出都会打印到控制台/**标准输出**（**stdout**）。从此控制台，我们将能够读取原始推文的原始文本内容和来自我们模型的预测情感分类，即负面或非负面。要了解更多关于可用的各种输出目标，请访问[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks)。'
- en: The following subsections describe the technical steps that we will follow to
    develop, deploy, and run our end-to-end stream processing pipeline for real-time
    sentiment analysis.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节将描述我们将遵循的技术步骤来开发、部署和运行我们的端到端流处理管道，以进行实时情感分析。
- en: Note that for the purposes of this case study, we will not be using Jupyter
    notebooks for development. This is because separate code files are required for
    the separate components, as described previously. This case study therefore provides
    another glimpse into how a production-grade pipeline should be developed and executed.
    Rather than instantiating a `SparkContext` explicitly within a notebook, we will
    instead submit our Python code files and all dependencies to `spark-submit` via
    the Linux command line.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于本案例研究的目的，我们不会使用Jupyter笔记本进行开发。这是因为需要为单独的组件编写单独的代码文件，如前所述。因此，本案例研究提供了另一个了解如何开发和执行生产级管道的视角。我们不会在笔记本中显式实例化`SparkContext`，而是将通过Linux命令行将我们的Python代码文件及其所有依赖项提交给`spark-submit`。
- en: Start Zookeeper and Kafka Servers
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动Zookeeper和Kafka服务器
- en: 'The first step is to ensure that our single-node Kafka cluster is up and running.
    As described in [Chapter 2](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml), *Setting
    Up a Local Development Environment*, please execute the following commands to
    start Apache Kafka:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是确保我们的单节点Kafka集群正在运行。如[第2章](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml)中所述，“设置本地开发环境”，请执行以下命令以启动Apache
    Kafka：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Kafka topic
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka主题
- en: 'Next, we need to create a Kafka topic to which our Python Kafka producer application
    (which we will develop later on) will publish real-time tweets about airlines.
    In our case, we will call the topic `twitter`. As demonstrated in [Chapter 2](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml),* Setting
    Up a Local Development Environment*, this can be achieved as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建一个Kafka主题，我们的Python Kafka生产者应用程序（我们将在稍后开发）将发布有关航空公司的实时推文。在我们的案例中，我们将该主题称为`twitter`。如[第2章](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml)中所示，“设置本地开发环境”，可以通过以下方式实现：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Twitter developer account
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Twitter开发者账户
- en: In order for our Python Kafka producer application to capture tweets in real
    time, we require access to the Twitter API. As of July 2018, a Twitter *developer
    account*, in addition to a normal Twitter account, must be created and approved
    in order to access its API. In order to apply for a developer account, please
    go to [https://apps.twitter.com/](https://apps.twitter.com/), click on the Apply
    for a Developer Account button, and fill in the required details.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的Python Kafka生产者应用程序实时捕获推文，我们需要访问Twitter API。截至2018年7月，除了普通Twitter账户外，还必须创建并批准一个Twitter
    *开发者账户*，才能访问其API。为了申请开发者账户，请访问[https://apps.twitter.com/](https://apps.twitter.com/)，点击“申请开发者账户”按钮，并填写所需详细信息。
- en: Twitter apps and the Twitter API
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Twitter应用程序和Twitter API
- en: 'Once you have created your Twitter developer account, in order to use the Twitter
    API, a Twitter *app* must be created. A Twitter app provides authenticated and
    authorized access to the Twitter API based on the specific purpose of the app
    that you intend to create. In order to create a Twitter app for the purposes of
    our real-time sentiment analysis model, please go through the following instructions
    (valid at the time of writing):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您创建了Twitter开发者账户，为了使用Twitter API，必须创建一个Twitter *应用程序*。Twitter应用程序根据您打算创建的应用程序的具体目的，提供认证和授权访问Twitter
    API。为了创建用于我们实时情感分析模型的Twitter应用程序，请按照以下说明（截至撰写时有效）进行操作：
- en: Navigate to [https://developer.twitter.com/en/apps](https://developer.twitter.com/en/apps).
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到[https://developer.twitter.com/en/apps](https://developer.twitter.com/en/apps)。
- en: Select the Create an App button.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“创建应用程序”按钮。
- en: 'Provide the following mandatory app details:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供以下必填的应用程序详细信息：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Click the Create button to create your Twitter app.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“创建”按钮创建您的Twitter应用程序。
- en: Once your Twitter app has been created, navigate to the Keys and Tokens tab.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦您的Twitter应用程序创建完成，导航到“密钥和令牌”选项卡。
- en: Make a note of your Consumer API Key and Consumer API Secret Key strings respectively.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分别记下您的消费者API密钥和消费者API密钥字符串。
- en: Then click the Create button under Access Token & Access Token Secret to generate
    access tokens for your Twitter app. Set the access level to Read-only as this
    Twitter app will only read tweets, and will not generate any of its own.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后点击“访问令牌 & 访问令牌密钥”下的“创建”按钮，为您的Twitter应用程序生成访问令牌。将访问级别设置为只读，因为此Twitter应用程序将只读取推文，不会生成任何自己的内容。
- en: Make a note of the resulting **Access Token** and Access Token Secret strings
    respectively.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记下生成的 **访问令牌**和访问令牌密钥字符串。
- en: The consumer API keys and access tokens will be used to provision our Python-based
    Kafka producer application read-only access to the stream of real-time tweets
    via the Twitter API, so it is important that you make a note of them.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者API密钥和访问令牌将被用于授权我们的基于Python的Kafka生产者应用程序以只读方式访问通过Twitter API获取的实时推文流，因此您需要将它们记录下来。
- en: Application configuration
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用程序配置
- en: 'We are now ready to start developing our end-to-end stream processing pipeline!
    First, let''s create a configuration file in Python that will store all environmental
    and application-level options pertinent to our pipeline and local development
    node, as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好开始开发我们的端到端流处理管道！首先，让我们创建一个Python配置文件，该文件将存储与我们的管道和本地开发节点相关的所有环境和应用程序级别的选项，如下所示：
- en: The following Python configuration file, called `config.py`, can be found in
    the GitHub repository accompanying this book.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python配置文件，称为`config.py`，可以在伴随本书的GitHub存储库中找到。
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This Python configuration file defines the following pertinent options:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此Python配置文件定义了以下相关选项：
- en: '`bootstrap_servers`: A comma-delimited list of the hostname/IP address and
    port number pairings for the Kafka brokers. In our case, this is just the hostname/IP
    address of our single-node development environment at port `9092` by default.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bootstrap_servers`：Kafka代理的主机名/IP地址和端口号配对的逗号分隔列表。在我们的案例中，这是默认情况下位于端口`9092`的单节点开发环境的主机名/IP地址。'
- en: '`consumer_api_key`: Enter the consumer API key associated with your Twitter
    app here.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`consumer_api_key`：在此处输入与您的Twitter应用程序关联的消费者API密钥。'
- en: '`consumer_api_secret`: Enter the consumer API secret key associated with your
    Twitter app here.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`consumer_api_secret`：在此处输入与您的Twitter应用程序关联的消费者API密钥。'
- en: '`access_token`: Enter the access token associated with your Twitter app here.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`access_token`：在此处输入与您的Twitter应用程序关联的访问令牌。'
- en: '`access_token_secret`: Enter the access token secret associated with your Twitter
    app here.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`access_token_secret`：在此处输入与您的Twitter应用程序关联的访问令牌密钥。'
- en: '`twitter_kafka_topic_name`: The name of the Kafka topic to which our Kafka
    producer will publish tweets and from which our Structured Streaming Spark application
    will consume tweets.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`twitter_kafka_topic_name`：我们的Kafka生产者将发布的Kafka主题名称，以及我们的结构化流Spark应用程序将从中消费推文的主题。'
- en: '`twitter_stream_filter`: A keyword, Twitter handle, or hashtag to use in order
    to filter the stream of real-time tweets being captured from the Twitter API.
    In our case, we are filtering for real-time tweets directed at `@British_Airways`.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`twitter_stream_filter`：一个关键字、Twitter用户名或标签，用于过滤从Twitter API捕获的实时推文流。在我们的案例中，我们正在过滤针对`@British_Airways`的实时推文。'
- en: '`trained_classification_model_path`: The absolute path where we saved our trained
    decision tree classifier in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml),
    *Natural Language Processing Using Apache Spark*.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trained_classification_model_path`：我们保存我们的训练决策树分类器（在第6章中介绍，*使用Apache Spark进行自然语言处理*）的绝对路径。'
- en: Kafka Twitter producer application
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Twitter生产者应用程序
- en: 'We are now ready to develop our Python-based Kafka producer application that
    will capture tweets about airlines that are being tweeted in real-time and then
    publish those tweets to the Apache Kafka `twitter` topic that we created previously.
    We will be using the following two Python libraries in order to develop our Kafka
    producer:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好开发我们的基于Python的Kafka生产者应用程序，该程序将捕获有关航空公司实时推文的推文，并将这些推文发布到我们之前创建的Apache
    Kafka `twitter` 主题。在开发我们的Kafka生产者时，我们将使用以下两个Python库：
- en: '`tweepy`: This library allows us to access the Twitter API programmatically using
    Python and the consumer API keys and access tokens that we generated earlier'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tweepy`：这个库允许我们使用Python和之前生成的消费者API密钥和访问令牌以编程方式访问Twitter API。'
- en: '`pykafka`: This library allow us to instantiate a Python-based Apache Kafka
    client through which we can communicate and transact with our single-node Kafka
    cluster.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pykafka`：这个库允许我们实例化一个基于Python的Apache Kafka客户端，通过它可以与我们的单节点Kafka集群进行通信和交易。'
- en: The following Python code file, called `kafka_twitter_producer.py`, can be found
    in the GitHub repository accompanying this book.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python代码文件，称为`kafka_twitter_producer.py`，可以在伴随本书的GitHub存储库中找到。
- en: 'In regards to our Python-based Kafka producer application, we perform the following
    steps (numbered to correspond to the numbered comments in our Python code file):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 关于我们的基于Python的Kafka生产者应用程序，我们执行以下步骤（编号与Python代码文件中的编号注释相对应）：
- en: 'First, we import the required modules from the `tweepy` and `pykafka` libraries
    respectively, as shown in the following code. We also import the configuration
    from our `config.py` file, which we created earlier:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们分别从 `tweepy` 和 `pykafka` 库中导入所需的模块，如下面的代码所示。我们还导入了我们之前创建的 `config.py` 文件中的配置：
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we instantiate a `tweepy` wrapper for the Twitter API using the consumer
    API keys and access tokens defined in `config.py` to provide us authenticated
    and authorized programmatic access to the Twitter API, as follows:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `config.py` 中定义的消费者 API 密钥和访问令牌实例化一个 `tweepy` Twitter API 包装器，如下所示，以提供我们认证和授权的程序访问
    Twitter API：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We then define a class in Python called `KafkaTwitterProducer`, which once instantiated,
    provides us with a `pykafka` client to our single-node Apache Kafka cluster, as
    shown in the following code. When this class is instantiated, it initially executes
    the code defined in the `__init__` function, which creates a `pykafka` client
    using the bootstrap servers, the locations of which may be found in `config.py`. It
    then creates a Kafka producer that associates the producer to the `twitter_kafka_topic_name`
    Kafka topic also defined in `config.py`. When data is captured by our `pykafka`
    producer, the `on_data` function is invoked, which physically publishes the data
    to the Kafka topic.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们在 Python 中定义了一个名为 `KafkaTwitterProducer` 的类，一旦实例化，它就为我们提供了一个 `pykafka`
    客户端到我们的单节点 Apache Kafka 集群，如下面的代码所示。当这个类被实例化时，它最初执行 `__init__` 函数中定义的代码，使用引导服务器创建一个
    `pykafka` 客户端，这些服务器的位置可以在 `config.py` 中找到。然后它创建了一个与 `config.py` 中定义的 `twitter_kafka_topic_name`
    Kafka 主题关联的生产者。当我们的 `pykafka` 生产者捕获数据时，会调用 `on_data` 函数，该函数将数据物理发布到 Kafka 主题。
- en: 'If our `pykafka` producer encounters an error, then the `on_error` function
    is invoked, which, in our case, simply prints the error to the console and goes
    on to process the next message:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的 `pykafka` 生产者遇到错误，则调用 `on_error` 函数，在我们的情况下，它只是将错误打印到控制台并继续处理下一个消息：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we instantiate a Twitter stream using the `Stream` module of the `tweepy`
    library. To achieve this, we simply pass our Twitter app authentication details
    and an instance of our `KafkaTwitterProducer` class to the Stream module:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `tweepy` 库的 `Stream` 模块实例化一个 Twitter 流。为此，我们只需将我们的 Twitter 应用程序认证详情和
    `KafkaTwitterProducer` 类的实例传递给 `Stream` 模块：
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now that we have instantiated a Twitter stream, the final step is to filter
    the stream to deliver tweets of interest, based on the `twitter_stream_filter`
    option found in `config.py`, as shown in the following code:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经实例化了一个 Twitter 流，最后一步是根据 `config.py` 中的 `twitter_stream_filter` 选项过滤流，以传递感兴趣的推文，如下面的代码所示：
- en: '[PRE8]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We are now ready to run our Kafka producer application! Since it is a Python
    application, the easiest way to run it is simply to use the Linux command line,
    navigate to the directory containing `kafka_twitter_producer.py`, and execute
    it as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以运行我们的 Kafka 生产者应用程序了！由于它是一个 Python 应用程序，运行它的最简单方法就是使用 Linux 命令行，导航到包含
    `kafka_twitter_producer.py` 的目录，并按以下方式执行：
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To check that it is actually capturing and publishing real-time tweets to Kafka,
    as described in [Chapter 2](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml), *Setting
    Up a Local Development Environment*, you can start a command-line consumer application
    to consume messages from the Twitter topic and print them to the console, as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证它实际上正在捕获并将实时推文发布到 Kafka，如第 2 章所述，*设置本地开发环境*，你可以启动一个命令行消费者应用程序来从 Twitter
    主题中消费消息并将它们打印到控制台，如下所示：
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Hopefully, you will see tweets printed to the console in real time. In our case,
    these tweets are all directed to `"@British_Airways"`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你能看到实时打印到控制台上的推文。在我们的例子中，这些推文都是指向 `"@British_Airways"` 的。
- en: The tweets themselves are captured via the Twitter API in JSON format, and contain
    not only the raw textual content of the tweet, but also associated metadata, such
    as the tweet ID, the username of the tweeter, the timestamp, and so on. For a
    full description of the JSON schema, please visit [https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 推文本身是通过 Twitter API 以 JSON 格式捕获的，不仅包含推文的原始文本内容，还包含相关的元数据，如推文 ID、推文者的用户名、时间戳等。有关
    JSON 模式的完整描述，请访问 [https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html)。
- en: Preprocessing and feature vectorization pipelines
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理和特征向量化流程
- en: As described earlier, in order to be able to apply our trained decision tree
    classifier to these real-time tweets, we first need to preprocess and vectorize
    them exactly as we did with our training and test datasets in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml),
    *Natural Language Processing Using Apache Spark*. However, rather than duplicating
    the preprocessing and vectorization pipeline logic within our Kafka consumer application
    itself, we will define our pipeline logic in a separate Python module and within
    Python *functions*. This way, any time we need to preprocess text as we did in
    [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural Language Processing
    Using Apache Spark*, we simply call the relevant Python function, thereby avoiding
    the need to duplicate the same code across different Python code files.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，为了能够将我们训练好的决策树分类器应用于这些实时推文，我们首先需要像在[第6章](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml)《使用Apache
    Spark的自然语言处理》中处理我们的训练和测试数据集那样对它们进行预处理和向量化。然而，我们不会在Kafka消费者应用程序本身中重复预处理和向量化流程的逻辑，而是将在一个独立的Python模块和Python
    *函数*中定义我们的流程逻辑。这样，每次我们需要像在[第6章](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml)《使用Apache
    Spark的自然语言处理》中那样预处理文本时，我们只需调用相关的Python函数，从而避免在不同Python代码文件中重复相同的代码。
- en: The following Python code file, called `model_pipelines.py`, can be found in
    the GitHub repository accompanying this book.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下名为`model_pipelines.py`的Python代码文件，可以在本书配套的GitHub仓库中找到。
- en: 'In the following Python module, we define two functions. The first function
    applies the exact same pipeline of `MLlib` and `spark-nlp` feature transformers
    that we studied in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural
    Language Processing Using Apache Spark*, in order to preprocess the raw textual
    content of the tweets. The second function then takes a preprocessed Spark dataframe
    and applies the HashingTF transformer to it in order to generate feature vectors
    based on term frequencies, exactly as we studied in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml),
    *Natural Language Processing Using Apache Spark*. The result is a Spark dataframe
    containing the original raw text of the tweet in a column called `text` and term
    frequency feature vectors in a column called `features`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下Python模块中，我们定义了两个函数。第一个函数应用了我们在[第6章](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml)《使用Apache
    Spark的自然语言处理》中学习的`MLlib`和`spark-nlp`特征转换器的相同流程，以预处理推文的原始文本内容。第二个函数随后对预处理后的Spark数据框应用`HashingTF`转换器，以根据词频生成特征向量，正如我们在[第6章](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml)《使用Apache
    Spark的自然语言处理》中所学习的那样。结果是包含原始推文文本的Spark数据框，该文本位于名为`text`的列中，以及位于名为`features`的列中的词频特征向量：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Kafka Twitter consumer application
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Twitter消费者应用程序
- en: We are finally ready to develop our Kafka consumer application using the Spark
    Structured Streaming engine in order to apply our trained decision tree classifier
    to the stream of real-time tweets in order to deliver real-time sentiment analysis!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终准备好使用Spark Structured Streaming引擎开发我们的Kafka消费者应用程序，以便将我们的训练好的决策树分类器应用于实时推文流，以提供实时情感分析！
- en: The following Python code file, called `kafka_twitter_consumer.py`, can be found
    in the GitHub repository accompanying this book.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以下名为`kafka_twitter_consumer.py`的Python代码文件，可以在本书配套的GitHub仓库中找到。
- en: 'In regards to our Spark Structured-Streaming-based Kafka consumer application,
    we perform the following steps (numbered to correspond to the numbered comments
    in our Python code file):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 关于我们的基于Spark Structured-Streaming的Kafka消费者应用程序，我们执行以下步骤（编号与Python代码文件中的注释编号相对应）：
- en: 'First, we import the configuration from our `config.py` file. We also import
    the Python functions containing the logic for our preprocessing and vectorization
    pipelines that we created earlier, as follows:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从`config.py`文件中导入配置。我们还导入了我们之前创建的包含预处理和向量化流程逻辑的Python函数，如下所示：
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Unlike our Jupyter notebook case studies, there is no need explicitly to instantiate
    a `SparkContext` as this will be done for us when we execute our Kafka consumer
    application via `spark-submit` in the command line. In this case study, we create
    a `SparkSession`, as shown in the following code that acts as an entry point into
    the Spark execution environment—even if it is already running—and which subsumes
    `SQLContext`. We can therefore use `SparkSession` to undertake the same SQL-like
    operations over data that we have seen previously, while still using the Spark
    Dataset/DataFrame API:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与我们的 Jupyter 笔记本案例研究不同，没有必要显式实例化一个 `SparkContext`，因为这将在我们通过命令行中的 `spark-submit`
    执行 Kafka 消费者应用程序时为我们完成。在本案例研究中，我们创建了一个 `SparkSession`，如下面的代码所示，它作为 Spark 执行环境的入口点——即使它已经在运行——并且它包含了
    `SQLContext`。因此，我们可以使用 `SparkSession` 来执行与之前看到的相同类型的 SQL 操作，同时仍然使用 Spark Dataset/DataFrame
    API：
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In this step, we load the decision tree classifier that we trained in [Chapter
    6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural Language Processing Using
    Apache Spark*, (which used the *HashingTF* feature extractor) from the local filesystem
    into a `DecisionTreeClassificationModel` object so that we can apply it later
    on, as shown in the following code. Note that the absolute path to the trained
    decision tree classifier has been defined in `config.py`:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将我们在第 6 章[自然语言处理使用 Apache Spark](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml)中训练的决策树分类器（使用了
    *HashingTF* 特征提取器）从本地文件系统加载到一个 `DecisionTreeClassificationModel` 对象中，以便我们可以在以后应用它，如下面的代码所示。注意，训练好的决策树分类器的绝对路径已在
    `config.py` 中定义：
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We are almost ready to start consuming messages from our single-node Kafka
    cluster. However, before doing so, we must note that Spark does not yet support
    the automatic inference and parsing of JSON key values into Spark dataframe columns.
    We must therefore explicitly define the JSON schema, or the subset of the JSON
    schema that we wish to retain, as follows:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们几乎准备好开始从我们的单节点 Kafka 集群中消费消息了。然而，在这样做之前，我们必须注意 Spark 还不支持自动推断和解析 JSON 键值到
    Spark 数据框列。因此，我们必须明确定义 JSON 架构，或者我们希望保留的 JSON 架构的子集，如下所示：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now that we have defined our JSON schema, we are ready to start consuming messages.
    To do this, we invoke the `readStream` method on our `SparkSession` instance to
    consume streaming data. We specify that the source of our stream will be a Kafka
    cluster using the `format` method, after which we define the Kafka bootstrap servers
    and the name of the Kafka topic to which we want to subscribe, both of which have
    been defined in `config.py`. Finally, we invoke the `load` method to stream the
    latest messages consumed from the `twitter` topic to an unbounded Spark dataframe
    called `tweets_df`, as shown in the following code:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的 JSON 架构，我们准备开始消费消息。为此，我们在我们的 `SparkSession` 实例上调用 `readStream`
    方法来消费流数据。我们指定流数据的来源将是一个 Kafka 集群，使用 `format` 方法，之后我们定义 Kafka 启动服务器和我们要订阅的 Kafka
    主题的名称，这两个都在 `config.py` 中定义过。最后，我们调用 `load` 方法将 `twitter` 主题中消费的最新消息流式传输到一个无界的
    Spark 数据框 `tweets_df`，如下面的代码所示：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Records stored in a Kafka topic are persisted in binary format. In order to
    process the JSON representing our tweets, which are stored in a Kafka record under
    a field called `value`, we must first `CAST` the contents of `value` as a string.
    We then apply our defined schema to this JSON string and extract the fields of
    interest, as shown in the following code. In our case, we are only interested
    in the tweet ID, stored in the JSON key called `id`, and its raw textual content,
    stored in the JSON key called `text`. The resulting Spark dataframe will therefore
    have two string columns, `id` and `text`, containing these respective fields of
    interest:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存储在 Kafka 主题中的记录以二进制格式持久化。为了处理代表我们的推文的 JSON，这些推文存储在名为 `value` 的 Kafka 记录字段下，我们必须首先将
    `value` 的内容 `CAST` 为字符串。然后我们将定义的架构应用于这个 JSON 字符串并提取感兴趣的字段，如下面的代码所示。在我们的例子中，我们只对存储在名为
    `id` 的 JSON 键中的推文 ID 和存储在名为 `text` 的 JSON 键中的原始文本内容感兴趣。因此，生成的 Spark 数据框将有两个字符串列，`id`
    和 `text`，包含这些感兴趣的相应字段：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now that we have consumed raw tweets from our Kafka topic and parsed them into
    a Spark dataframe, we can apply our preprocessing pipeline as we did in [Chapter
    6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml),* Natural Language Processing Using
    Apache Spark*. However, rather than duplicating the same code from [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural
    Language Processing Using Apache Spark*, into our Kafka consumer application,
    we simply call the relevant function that we defined in `model_pipelines.py`,
    namely `preprocessing_pipeline()`, as shown in the following code. This preprocessing
    pipeline tokenizes the raw text, removes stop words, applies a stemming algorithm,
    and normalizes the resulting tokens:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经从我们的Kafka主题中消费了原始推文并将它们解析为Spark数据框，我们可以像在[第6章](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml)中做的那样应用我们的预处理管道，*使用Apache
    Spark进行自然语言处理*。然而，我们不是将[第6章](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml)中*使用Apache
    Spark进行自然语言处理*的相同代码复制到我们的Kafka消费者应用程序中，而是简单地调用我们在`model_pipelines.py`中定义的相关函数，即`preprocessing_pipeline()`，如下面的代码所示。这个预处理管道将原始文本分词，去除停用词，应用词干提取算法，并规范化生成的标记：
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we generate feature vectors from these tokens, as we did in [Chapter
    6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural Language Processing Using
    Apache Spark*. We call the `vectorizer_pipeline()` function from `model_pipelines.py`
    to generate feature vectors based on term frequencies, as shown in the following
    code. The resulting Spark dataframe, called `features_df`, contains three pertinent
    columns, namely `id` (raw tweet ID), `text` (raw tweet text), and `features` (term-frequency
    feature vectors):'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们生成这些标记的特征向量，就像在[第6章](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml)中*使用Apache
    Spark进行自然语言处理*所做的那样。我们调用`model_pipelines.py`中的`vectorizer_pipeline()`函数来根据词频生成特征向量，如下面的代码所示。生成的Spark数据框，称为`features_df`，包含三个相关列，即`id`（原始推文ID）、`text`（原始推文文本）和`features`（词频特征向量）：
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now that we have generated feature vectors from our stream of tweets, we can
    apply our trained decision tree classifier to this stream in order to predict
    and classify their underlying sentiment. We do this as normal, by invoking the
    `transform` method on the `features_df` dataframe, resulting in a new Spark dataframe
    called `predictions_df`, containing the columns `id` and `text` as before, and
    a new column called `prediction` that contains our predicted classification, as
    shown in the following code. As described in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml), *Natural
    Language Processing Using Apache Spark*, a prediction of `1` implies non-negative
    sentiment, and a prediction of `0` implies negative sentiment:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经从我们的推文流中生成了特征向量，我们可以将我们的训练好的决策树分类器应用到这个流中，以便预测和分类其潜在的 sentiment。我们像平常一样这样做，通过在`features_df`数据框上调用`transform`方法，结果生成一个新的Spark数据框，称为`predictions_df`，包含与之前相同的`id`和`text`列，以及一个新的名为`prediction`的列，其中包含我们的预测分类，如下面的代码所示。正如在[第6章](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml)中*使用Apache
    Spark进行自然语言处理*所描述的，预测值为`1`表示非负sentiment，预测值为`0`表示负sentiment：
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, we write our predicted results dataframe to an output sink. In our
    case, we define the output sink as simply the console that is used to execute
    the Kafka consumer PySpark application—that is, the console from which we execute
    `spark-submit`. We achieve this by invoking the `writeStream` method on the relevant
    Spark dataframe and stating `console` as the `format` of choice. We start our
    output stream by invoking the `start` method, and invoke the `awaitTermination`
    method, which tells Spark to continue processing our streaming pipeline indefinitely
    until it is explicitly interrupted and stopped, as follows:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将我们的预测结果数据框写入输出接收器。在我们的例子中，我们将输出接收器定义为简单地是用于执行Kafka消费者PySpark应用程序的控制台——即执行`spark-submit`的控制台。我们通过在相关的Spark数据框上调用`writeStream`方法并指定`console`作为选择的`format`来实现这一点。我们通过调用`start`方法启动输出流，并调用`awaitTermination`方法，这告诉Spark无限期地继续处理我们的流处理管道，直到它被明确中断并停止，如下所示：
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Note that the `outputMode` method defines what gets written to the output sink,
    and can take one of the following options:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`outputMode`方法定义了写入输出接收器的内容，可以取以下选项之一：
- en: '`complete`: The entire (updated) results table is written to the output sink'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`complete`：将整个（更新后的）结果表写入输出接收器'
- en: '`append`: Only the new rows appended to the results table since the last trigger
    is written to the output sink'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`append`：仅写入自上次触发以来添加到结果表的新行到输出接收器'
- en: '`update`: Only the rows that were updated in the results table since the last
    trigger is written to the output sink'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update`: 只有自上次触发以来在结果表中更新的行会被写入输出接收器'
- en: 'We are now ready to run our Kafka consumer application! Since it is a Spark
    application, we can execute it via *spark-submit* on the Linux command line. To
    do this, navigate to the directory where we installed Apache Spark (see [Chapter
    2](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml), *Setting Up a Local Development
    Environment*). Thereafter we can execute the `spark-submit` program by passing
    to it the following command-line arguments:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以运行我们的 Kafka 消费者应用程序了！由于它是一个 Spark 应用程序，我们可以在 Linux 命令行上通过 *spark-submit*
    来执行它。为此，导航到我们安装 Apache Spark 的目录（见第 2 章，*设置本地开发环境*）。然后我们可以通过传递以下命令行参数来执行 `spark-submit`
    程序：
- en: '`--master`: The Spark master URL.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--master`: Spark 主机 URL。'
- en: '`--packages`: The third-party libraries and dependencies required for the given
    Spark application to work. In our case, our Kafka consumer application is dependent
    on the availability of two third-party libraries, namely `spark-sql-kafka` (Spark
    Kafka integration) and `spark-nlp` (NLP algorithms, as studied in [Chapter 6](2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml),
    *Natural Language Processing Using Apache Spark*).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--packages`: 给定 Spark 应用程序运行所需的第三方库和依赖项。在我们的例子中，我们的 Kafka 消费者应用程序依赖于两个第三方库的可用性，即
    `spark-sql-kafka`（Spark Kafka 集成）和 `spark-nlp`（自然语言处理算法，如第 6 章所述，*使用 Apache Spark
    进行自然语言处理*）。'
- en: '`--py-files`: Since our Kafka consumer is a PySpark application, we can use
    this argument to pass a comma-delimited list of filesystem paths to any Python
    code files that our application is dependent on. In our case, our Kafka consumer
    application is dependent on `config.py` and `model_pipelines.py` respectively.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--py-files`: 由于我们的 Kafka 消费者是一个 PySpark 应用程序，我们可以使用这个参数来传递一个以逗号分隔的文件系统路径列表，这些路径包含我们的应用程序所依赖的任何
    Python 代码文件。在我们的例子中，我们的 Kafka 消费者应用程序分别依赖于 `config.py` 和 `model_pipelines.py`。'
- en: The final argument is the path to the Python code file containing our Spark
    Structured Streaming driver program, in our case, `kafka_twitter_consumer.py`
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个参数是我们 Spark Structured Streaming 驱动程序的 Python 代码文件的路径，在我们的例子中，是 `kafka_twitter_consumer.py`
- en: 'The final commands to execute therefore look as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最终的命令如下所示：
- en: '[PRE22]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Assuming that the Python-based Kafka producer application is running as well,
    the results table should periodically be written to the console and contain the
    real-time prediction and classification of the underlying sentiment behind the
    stream of airline tweets being consumed by the Twitter API from across the world
    and written by real Twitter users! A selection of real-world classified tweets
    that was processed when filtering using `"@British_Airways"` is shown in the following
    table:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 假设基于 Python 的 Kafka 生产者应用程序也在运行，结果表应该定期写入控制台，并包含来自全球 Twitter API 消费并由真实 Twitter
    用户编写的航班推文流背后的实时预测和分类！以下表格显示了在过滤使用 `"@British_Airways"` 时处理的真实世界分类推文选择：
- en: '| **Tweet Raw Contents** | **Predicted Sentiment** |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| **推文原始内容** | **预测情感** |'
- en: '| @British_Airways @HeathrowAirport I mean I''m used to cold up in Shetland
    but this is a whole different kind of cold!! | Non-negative |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| @British_Airways @HeathrowAirport 我习惯了在设得兰的寒冷，但这是完全不同的一种寒冷！！ | 非负面 |'
- en: '| @British_Airways have just used the app to check-in for our flight bari to
    lgw but the app shows no hold luggage | Negative |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| @British_Airways 刚刚使用该应用程序为我们的航班 bari 到 lgw 办理登机手续，但应用程序显示没有行李托运 | 负面 |'
- en: '| She looks more Beautiful at Night &#124; A380 takeoff London Heathrow @HeathrowAviYT
    @HeathrowAirport @British_Airways | Non-negative |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 她在夜晚看起来更美 &#124; A380 在伦敦希思罗机场起飞 @HeathrowAviYT @HeathrowAirport @British_Airways
    | 非负面 |'
- en: '| The @British_Airways #B747 landing into @HeathrowAirport | Non-negative |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| The @British_Airways #B747 landing into @HeathrowAirport | 非负面 |'
- en: '| @British_Airways trying to check in online for my flight tomorrow and receiving
    a ''sorry we are unable to offer you online check-in for this flight'' message.
    Any idea why?? | Negative |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| @British_Airways 正在尝试为明天的航班在线登机，但收到一条消息“很抱歉，我们无法为此次航班提供在线登机服务”。有什么想法吗？？ |
    负面 |'
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we developed Apache Kafka producer and consumer applications
    and utilized Spark's Structured Streaming engine to process streaming data consumed
    from a Kafka topic. In our real-world case study, we designed, developed, and
    deployed an end-to-end stream processing pipeline that was capable of consuming
    real tweets being authored across the world and then classified their underlying
    sentiment using machine learning, all of which was done in real time.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们开发了Apache Kafka的生产者和消费者应用程序，并利用Spark的Structured Streaming引擎处理从Kafka主题中消费的流数据。在我们的实际案例研究中，我们设计、开发和部署了一个端到端流处理管道，该管道能够消费全球各地发布的真实推文，并使用机器学习对这些推文背后的情感进行分类，所有这些都是在实时完成的。
- en: In this book, we went on both a theoretical and a hands-on journey through some
    of the most important and exciting technologies and frameworks that underpin the
    data-intelligence-driven revolution being seen across industry today. We started
    out by describing a new breed of distributed and scalable technologies that allow
    us to store, process, and analyze huge volumes of structured, semi-structured,
    and unstructured data. Using these technologies as a base, we established the
    context for artificial intelligence and how it relates to machine learning and
    deep learning. We then went on to explore some of the key concepts in, and applications
    of, machine learning, including supervised learning, unsupervised learning, natural
    language processing, and deep learning. We illustrated these key concepts through
    a wide variety of relevant and exciting use cases that were implemented using
    our big data processing engine of choice—Apache Spark. Finally, because timely
    decisions are critical to many businesses and organizations in the modern world,
    we extended our deployment of machine learning models beyond batch processing
    to real-time streaming applications!
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们经历了一次理论与实践相结合的旅程，探索了支撑当今行业数据智能革命的一些最重要和最激动人心的技术和框架。我们首先描述了一种新型分布式和可扩展的技术，这些技术使我们能够存储、处理和分析大量结构化、半结构化和非结构化数据。以这些技术为基础，我们建立了人工智能的背景及其与机器学习和深度学习的关系。然后，我们继续探讨了机器学习的关键概念及其应用，包括监督学习、无监督学习、自然语言处理和深度学习。我们通过大量相关且令人兴奋的用例来阐述这些关键概念，这些用例都是使用我们选择的Apache
    Spark大数据处理引擎实现的。最后，鉴于及时决策对许多现代企业和组织至关重要，我们将机器学习模型的部署从批量处理扩展到了实时流应用！
