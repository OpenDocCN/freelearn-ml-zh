- en: Chapter 8. The Perceptron
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 感知器
- en: In previous chapters we discussed generalized linear models that relate a linear
    combination of explanatory variables and model parameters to a response variable
    using a link function. In this chapter, we will discuss another linear model called
    the perceptron. The perceptron is a binary classifier that can learn from individual
    training instances, which can be useful for training from large datasets. More
    importantly, the perceptron and its limitations inspire the models that we will
    discuss in the final chapters.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了广义线性模型，该模型通过连接函数将解释变量和模型参数的线性组合与响应变量关联。在本章中，我们将讨论另一种线性模型，称为感知器。感知器是一种二分类器，可以从单个训练实例中学习，这对于从大数据集中进行训练非常有用。更重要的是，感知器及其局限性激发了我们将在最后几章讨论的模型。
- en: Invented by Frank Rosenblatt at the Cornell Aeronautical Laboratory in the late
    1950's, the development of the perceptron was originally motivated by efforts
    to simulate the human brain. A brain is composed of cells called **neurons** that
    process information and connections between neurons called **synapses** through
    which information is transmitted. It is estimated that human brain is composed
    of as many as 100 billion neurons and 100 trillion synapses. As shown in the following
    image, the main components of a neuron are dendrites, a body, and an axon. The
    dendrites receive electrical signals from other neurons. The signals are processed
    in the neuron's body, which then sends a signal through the axon to another neuron.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器由弗兰克·罗森布拉特（Frank Rosenblatt）于1950年代末在康奈尔航空实验室发明，感知器的开发最初是受模拟人脑的努力驱动的。大脑由称为**神经元**的细胞组成，这些神经元处理信息，并通过称为**突触**的神经元之间的连接传递信息。据估计，人脑由多达1000亿个神经元和100万亿个突触组成。如下面的图所示，神经元的主要组成部分包括树突、细胞体和轴突。树突从其他神经元接收电信号。信号在神经元的细胞体中被处理，然后通过轴突传递到另一个神经元。
- en: '![The Perceptron](img/8365OS_08_01.jpg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![感知器](img/8365OS_08_01.jpg)'
- en: 'An individual neuron can be thought of as a computational unit that processes
    one or more inputs to produce an output. A perceptron functions analogously to
    a neuron; it accepts one or more inputs, processes them, and returns an output.
    It may seem that a model of just one of the hundreds of billions of neurons in
    the human brain will be of limited use. To an extent that is true; the perceptron
    cannot approximate some basic functions. However, we will still discuss perceptrons
    for two reasons. First, perceptrons are capable of online, error-driven learning;
    the learning algorithm can update the model''s parameters using a single training
    instance rather than the entire batch of training instances. Online learning is
    useful for learning from training sets that are too large to be represented in
    memory. Second, understanding how the perceptron works is necessary to understand
    some of the more powerful models that we will discuss in subsequent chapters,
    including support vector machines and artificial neural networks. Perceptrons
    are commonly visualized using a diagram like the following one:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 单个神经元可以被看作是一个计算单元，处理一个或多个输入并生成输出。感知器的功能与神经元类似；它接受一个或多个输入，处理它们并返回一个输出。看起来仅仅模拟人脑中数百亿个神经元中的一个，似乎用途有限。一定程度上这是正确的；感知器无法逼近一些基本的函数。然而，我们仍然会讨论感知器，原因有二。首先，感知器能够进行在线的错误驱动学习；学习算法可以通过单个训练实例而不是整个训练实例批次来更新模型的参数。在线学习对于从太大以至于无法全部存储在内存中的训练集进行学习非常有用。第二，理解感知器的工作原理对于理解我们将在后续章节中讨论的一些更强大的模型是必要的，包括支持向量机和人工神经网络。感知器通常使用如下图所示的图表来表示：
- en: '![The Perceptron](img/8365OS_08_02.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![感知器](img/8365OS_08_02.jpg)'
- en: The circles labeled ![The Perceptron](img/8365OS_08_18.jpg), ![The Perceptron](img/8365OS_08_19.jpg),
    and ![The Perceptron](img/8365OS_08_20.jpg) are inputs units. Each input unit
    represents one feature. Perceptrons frequently use an additional input unit that
    represents a constant bias term, but this input unit is usually omitted from diagrams.
    The circle in the center is a computational unit or the neuron's body. The edges
    connecting the input units to the computational unit are analogous to dendrites.
    Each edge is **weighted**, or associated with a parameter. The parameters can
    be interpreted easily; an explanatory variable that is correlated with the positive
    class will have a positive weight, and an explanatory variable that is correlated
    with the negative class will have a negative weight. The edge directed away from
    the computational unit returns the output and can be thought of as the axon.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 标记为 ![感知机](img/8365OS_08_18.jpg)、![感知机](img/8365OS_08_19.jpg) 和 ![感知机](img/8365OS_08_20.jpg)
    的圆圈是输入单元。每个输入单元代表一个特征。感知机通常会使用一个额外的输入单元来表示常数偏置项，但该输入单元通常在图示中省略。中间的圆圈是计算单元或神经元的主体。连接输入单元和计算单元的边类似于树突。每条边是**加权**的，或与一个参数相关联。参数可以容易地解释；与正类相关的解释变量将具有正权重，而与负类相关的解释变量将具有负权重。指向计算单元外部的边返回输出，可以将其视为轴突。
- en: Activation functions
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: The perceptron classifies instances by processing a linear combination of the
    explanatory variables and the model parameters using an **activation function**
    as shown in the following equation. The linear combination of the parameters and
    inputs is sometimes called the perceptron's **preactivation**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机通过使用**激活函数**处理解释变量和模型参数的线性组合来对实例进行分类，如下方公式所示。参数和输入的线性组合有时被称为感知机的**预激活**。
- en: '![Activation functions](img/8365OS_08_03.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![激活函数](img/8365OS_08_03.jpg)'
- en: 'Here, ![Activation functions](img/8365OS_08_21.jpg) are the model''s parameters,
    ![Activation functions](img/8365OS_08_22.jpg) is a constant bias term, and ![Activation
    functions](img/8365OS_08_23.jpg) is the activation function. Several different
    activation functions are commonly used. Rosenblatt''s original perceptron used
    the **Heaviside step** function. Also called the unit step function, the Heaviside
    step function is shown in the following equation, where ![Activation functions](img/8365OS_08_24.jpg)
    is the weighted combination of the features:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![激活函数](img/8365OS_08_21.jpg) 是模型的参数，![激活函数](img/8365OS_08_22.jpg) 是常数偏置项，![激活函数](img/8365OS_08_23.jpg)
    是激活函数。常用几种不同的激活函数。Rosenblatt 原始的感知机使用了**Heaviside阶跃**函数。Heaviside阶跃函数也叫做单位阶跃函数，表示如下公式，其中
    ![激活函数](img/8365OS_08_24.jpg) 是特征的加权组合：
- en: '![Activation functions](img/8365OS_08_04.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![激活函数](img/8365OS_08_04.jpg)'
- en: 'If the weighted sum of the explanatory variables and the bias term is greater
    than zero, the activation function returns one and the perceptron predicts that
    the instance is the positive class. Otherwise, the function returns zero and the
    perceptron predicts that the instance is the negative class. The Heaviside step
    activation function is plotted in the following figure:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果解释变量和偏置项的加权和大于零，激活函数返回一，感知机预测该实例属于正类。否则，函数返回零，感知机预测该实例属于负类。Heaviside阶跃激活函数在下图中绘制：
- en: '![Activation functions](img/8365OS_08_05.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![激活函数](img/8365OS_08_05.jpg)'
- en: 'Another common activation function is the **logistic sigmoid** activation function.
    The gradients for this activation function can be calculated efficiently, which
    will be important in later chapters when we construct artificial neural networks.
    The logistic sigmoid activation function is given by the following equation, where
    ![Activation functions](img/8365OS_08_24.jpg) is the sum of the weighted inputs:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的激活函数是**逻辑 sigmoid** 激活函数。这个激活函数的梯度可以高效地计算，这在后续章节构建人工神经网络时非常重要。逻辑 sigmoid
    激活函数由以下公式给出，其中 ![激活函数](img/8365OS_08_24.jpg) 是加权输入的总和：
- en: '![Activation functions](img/8365OS_08_06.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![激活函数](img/8365OS_08_06.jpg)'
- en: This model should seem familiar; it is a linear combination of the values of
    the explanatory variables and the model parameters processed through the logistic
    function. That is, this is identical to the model for logistic regression. While
    a perceptron with a logistic sigmoid activation function has the same model as
    logistic regression, it learns its parameters differently.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型应该很熟悉；它是解释变量值和模型参数的线性组合，通过逻辑函数进行处理。也就是说，它与逻辑回归的模型相同。虽然使用逻辑 sigmoid 激活函数的感知机与逻辑回归有相同的模型，但它学习参数的方式不同。
- en: The perceptron learning algorithm
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知机学习算法
- en: 'The perceptron learning algorithm begins by setting the weights to zero or
    to small random values. It then predicts the class for a training instance. The
    perceptron is an **error-driven** learning algorithm; if the prediction is correct,
    the algorithm continues to the next instance. If the prediction is incorrect,
    the algorithm updates the weights. More formally, the update rule is given by
    the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机学习算法首先将权重设置为零或小的随机值。然后它预测一个训练实例的类别。感知机是一种**基于错误的**学习算法；如果预测正确，算法继续处理下一个实例。如果预测错误，算法会更新权重。更正式地，更新规则如下所示：
- en: '![The perceptron learning algorithm](img/8365OS_08_07.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![感知机学习算法](img/8365OS_08_07.jpg)'
- en: For each training instance, the value of the parameter for each explanatory
    variable is incremented by ![The perceptron learning algorithm](img/8365OS_08_25.jpg),
    where ![The perceptron learning algorithm](img/8365OS_08_26.jpg) is the true class
    for instance ![The perceptron learning algorithm](img/8365OS_08_27.jpg), ![The
    perceptron learning algorithm](img/8365OS_08_28.jpg) is the predicted class for
    instance ![The perceptron learning algorithm](img/8365OS_08_27.jpg), ![The perceptron
    learning algorithm](img/8365OS_08_29.jpg) is the value of the ![The perceptron
    learning algorithm](img/8365OS_08_30.jpg) explanatory variable for instance ![The
    perceptron learning algorithm](img/8365OS_08_27.jpg), and ![The perceptron learning
    algorithm](img/8365OS_08_31.jpg) is a hyperparameter that controls the learning
    rate. If the prediction is correct, ![The perceptron learning algorithm](img/8365OS_08_32.jpg)
    equals zero, and the ![The perceptron learning algorithm](img/8365OS_08_25.jpg)
    term equals zero. So, if the prediction is correct, the weight is not updated.
    If the prediction is incorrect, the weight is incremented by the product of the
    learning rate, ![The perceptron learning algorithm](img/8365OS_08_32.jpg), and
    the value of the feature.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个训练实例，每个解释变量的参数值增加！[感知机学习算法](img/8365OS_08_25.jpg)，其中！[感知机学习算法](img/8365OS_08_26.jpg)是实例！[感知机学习算法](img/8365OS_08_27.jpg)的真实类别，！[感知机学习算法](img/8365OS_08_28.jpg)是实例！[感知机学习算法](img/8365OS_08_27.jpg)的预测类别，！[感知机学习算法](img/8365OS_08_29.jpg)是实例！[感知机学习算法](img/8365OS_08_27.jpg)的解释变量！[感知机学习算法](img/8365OS_08_30.jpg)的值，而！[感知机学习算法](img/8365OS_08_31.jpg)是控制学习率的超参数。如果预测正确，！[感知机学习算法](img/8365OS_08_32.jpg)等于零，且！[感知机学习算法](img/8365OS_08_25.jpg)项等于零。因此，如果预测正确，权重不会更新。如果预测错误，权重将增加学习率、！[感知机学习算法](img/8365OS_08_32.jpg)和特征值的乘积。
- en: This update rule is similar to the update rule for gradient descent in that
    the weights are adjusted towards classifying the instance correctly and the size
    of the update is controlled by a learning rate. Each pass through the training
    instances is called an **epoch.** The learning algorithm has converged when it
    completes an epoch without misclassifying any of the instances. The learning algorithm
    is not guaranteed to converge; later in this chapter, we will discuss linearly
    inseparable datasets for which convergence is impossible. For this reason, the
    learning algorithm also requires a hyperparameter that specifies the maximum number
    of epochs that can be completed before the algorithm terminates.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个更新规则类似于梯度下降的更新规则，权重的调整是为了正确分类实例，并且更新的大小由学习率控制。每次遍历训练实例称为**一个周期（epoch）**。当学习算法在完成一个周期时没有错误分类任何实例，就认为学习算法已收敛。学习算法并不保证一定会收敛；在本章后面，我们将讨论无法线性分离的数据集，对于这些数据集，收敛是无法实现的。因此，学习算法还需要一个超参数，指定在算法终止之前可以完成的最大周期数。
- en: Binary classification with the perceptron
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知机的二分类
- en: 'Let''s work through a toy classification problem. Suppose that you wish to
    separate adult cats from kittens. Only two explanatory variables are available
    in your dataset: the proportion of the day that the animal was asleep and the
    proportion of the day that the animal was grumpy. Our training data consists of
    the following four instances:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解决一个玩具分类问题。假设您希望将成年猫与小猫分开。您的数据集中只有两个解释变量：动物白天睡觉的时间比例和动物白天易怒的时间比例。我们的训练数据包括以下四个实例：
- en: '| Instance | Proportion of the day spent sleeping | Proportion of the day spent
    being grumpy | Kitten or Adult? |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 实例 | 白天睡觉的时间比例 | 易怒的时间比例 | 小猫还是成年猫？ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | 0.2 | 0.1 | Kitten |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.2 | 0.1 | 小猫 |'
- en: '| 2 | 0.4 | 0.6 | Kitten |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.4 | 0.6 | 小猫 |'
- en: '| 3 | 0.5 | 0.2 | Kitten |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.5 | 0.2 | 小猫 |'
- en: '| 4 | 0.7 | 0.9 | Adult |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.7 | 0.9 | 成年猫 |'
- en: 'The following scatter plot of the instances confirms that they are linearly
    separable:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的散点图显示这些实例是线性可分的：
- en: '![Binary classification with the perceptron](img/8365OS_08_10.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![感知器进行二元分类](img/8365OS_08_10.jpg)'
- en: Our goal is to train a perceptron that can classify animals using the two real-valued
    explanatory variables. We will represent kittens with the positive class and adult
    cats with the negative class. The preceding network diagram describes the perceptron
    that we will train.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是训练一个能够使用两个实数解释变量分类动物的感知器。我们将小猫表示为正类，成年猫表示为负类。前面的网络图描述了我们将要训练的感知器。
- en: 'Our perceptron has three input units. ![Binary classification with the perceptron](img/8365OS_08_34.jpg)
    is the input unit for the bias term. ![Binary classification with the perceptron](img/8365OS_08_35.jpg)
    and ![Binary classification with the perceptron](img/8365OS_08_36.jpg) are input
    units for the two features. Our perceptron''s computational unit uses a Heaviside
    activation function. In this example, we will set the maximum number of training
    epochs to ten; if the algorithm does not converge within 10 epochs, it will stop
    and return the current values of the weights. For simplicity, we will set the
    learning rate to one. Initially, we will set all of the weights to zero. Let''s
    examine the first training epoch, which is shown in the following table:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的感知器有三个输入单元。![感知器进行二元分类](img/8365OS_08_34.jpg) 是偏置项的输入单元。![感知器进行二元分类](img/8365OS_08_35.jpg)
    和 ![感知器进行二元分类](img/8365OS_08_36.jpg) 是两个特征的输入单元。我们感知器的计算单元使用了海维赛德激活函数。在这个例子中，我们将最大训练轮数设为十；如果算法在10轮内没有收敛，它将停止并返回当前权重值。为了简单起见，我们将学习率设为一。最初，我们将所有权重设置为零。让我们看一下第一个训练轮次，如下表所示：
- en: '| Epoch 1 |   |   |   |   |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| Epoch 1 |   |   |   |   |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Instance** | **Initial Weights****x****Activation** | **Prediction, Target**
    | **Correct** | **Updated weights** |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **实例** | **初始权重****x****激活** | **预测，目标** | **正确** | **更新后的权重** |'
- en: '| 0 | 0, 0, 0;1.0, 0.2, 0.1;1.0*0 + 0.2*0 + 0.1*0 = 0.0; | 0, 1 | False | 1.0,
    0.2, 0.1 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0, 0, 0;1.0, 0.2, 0.1;1.0*0 + 0.2*0 + 0.1*0 = 0.0; | 0, 1 | False | 1.0,
    0.2, 0.1 |'
- en: '| 1 | 1.0, 0.2, 0.1;1.0, 0.4, 0.6;1.0*1.0 + 0.4*0.2 + 0.6*0.1 = 1.14; | 1,
    1 | True | 1.0, 0.2, 0.1 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.0, 0.2, 0.1;1.0, 0.4, 0.6;1.0*1.0 + 0.4*0.2 + 0.6*0.1 = 1.14; | 1,
    1 | True | 1.0, 0.2, 0.1 |'
- en: '| 2 | 1.0, 0.2, 0.1;1.0, 0.5, 0.2;1.0*1.0 + 0.5*0.2 + 0.2*0.1 = 1.12; | 1,
    1 | True | 1.0, 0.2, 0.1 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.0, 0.2, 0.1;1.0, 0.5, 0.2;1.0*1.0 + 0.5*0.2 + 0.2*0.1 = 1.12; | 1,
    1 | True | 1.0, 0.2, 0.1 |'
- en: '| 3 | 1.0, 0.2, 0.1;1.0, 0.7, 0.9;1.0*1.0 + 0.7*0.2 + 0.9*0.1 = 1.23; | 1,
    0 | False | 0, -0.5, -0.8 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1.0, 0.2, 0.1;1.0, 0.7, 0.9;1.0*1.0 + 0.7*0.2 + 0.9*0.1 = 1.23; | 1,
    0 | False | 0, -0.5, -0.8 |'
- en: Initially, all of the weights are equal to zero. The weighted sum of the explanatory
    variables for the first instance is zero, the activation function outputs zero,
    and the perceptron incorrectly predicts that the kitten is an adult cat. As the
    prediction was incorrect, we update the weights according to the update rule.
    We increment each of the weights by the product of the learning rate, the difference
    between the true and predicted labels and the value of the corresponding feature.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，所有权重都等于零。第一个实例的解释变量的加权和为零，激活函数输出为零，感知器错误地预测小猫为成年猫。由于预测错误，我们根据更新规则更新权重。我们将每个权重增加学习率、真实标签与预测标签之间的差异以及相应特征的值的乘积。
- en: 'We then continue to the second training instance and calculate the weighted
    sum of its features using the updated weights. This sum equals 1.14, so the activation
    function outputs one. This prediction is correct, so we continue to the third
    training instance without updating the weights. The prediction for the third instance
    is also correct, so we continue to the fourth training instance. The weighted
    sum of the features for the fourth instance is 1.23\. The activation function
    outputs one, incorrectly predicting that this adult cat is a kitten. Since this
    prediction is incorrect, we increment each weight by the product of the learning
    rate, the difference between the true and predicted labels, and its corresponding
    feature. We completed the first epoch by classifying all of the instances in the
    training set. The perceptron did not converge; it classified half of the training
    instances incorrectly. The following figure depicts the decision boundary after
    the first epoch:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们继续进行第二次训练实例，并使用更新后的权重计算特征的加权和。这个加权和等于1.14，因此激活函数输出1。这个预测是正确的，所以我们继续进行第三个训练实例，并且不更新权重。第三个实例的预测也是正确的，因此我们继续进行第四个训练实例。第四个实例的特征加权和为1.23。激活函数输出1，错误地预测这个成年猫是小猫。由于这个预测是错误的，我们将每个权重增加学习率、真实标签与预测标签之间的差异以及相应特征的乘积。我们通过对训练集中的所有实例进行分类完成了第一次训练周期。感知机并没有收敛；它错误地分类了训练集中的一半实例。下图展示了第一次训练周期后的决策边界：
- en: '![Binary classification with the perceptron](img/8365OS_08_11.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![使用感知机的二元分类](img/8365OS_08_11.jpg)'
- en: 'Note that the decision boundary moved throughout the epoch; the decision boundary
    formed by the weights at the end of the epoch would not necessarily have produced
    the same predictions seen earlier in the epoch. Since we have not exceeded the
    maximum number of training epochs, we will iterate through the instances again.
    The second training epoch is shown in the following table:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，决策边界在整个周期中都有移动；周期结束时由权重形成的决策边界可能并不会产生周期初期看到的相同预测。由于我们没有超过最大训练周期数，我们将再次遍历这些实例。第二次训练周期如下表所示：
- en: '| Epoch 2 |   |   |   |   |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 第2周期 |   |   |   |   |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Instance** | **Initial Weights****x****Activation** | **Prediction, Target**
    | **Correct** | **Updated weights** |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| **实例** | **初始权重** **x** **激活** | **预测, 目标** | **正确** | **更新后的权重** |'
- en: '| 0 | 0, -0.5, -0.81.0, 0.2, 0.11.0*0 + 0.2*-0.5 + 0.1*-0.8 = -0.18 | 0, 1
    | False | 1, -0.3, -0.7 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0, -0.5, -0.81.0, 0.2, 0.11.0*0 + 0.2*-0.5 + 0.1*-0.8 = -0.18 | 0, 1
    | 错误 | 1, -0.3, -0.7 |'
- en: '| 1 | 1, -0.3, -0.71.0, 0.4, 0.61.0*1.0 + 0.4*-0.3 + 0.6*-0.7 = 0.46 | 1, 1
    | True | 1, -0.3, -0.7 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1, -0.3, -0.71.0, 0.4, 0.61.0*1.0 + 0.4*-0.3 + 0.6*-0.7 = 0.46 | 1, 1
    | 正确 | 1, -0.3, -0.7 |'
- en: '| 2 | 1, -0.3, -0.71.0, 0.5, 0.21.0*1.0 + 0.5*-0.3 + 0.2*-0.7 = 0.71 | 1, 1
    | True | 1, -0.3, -0.7 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1, -0.3, -0.71.0, 0.5, 0.21.0*1.0 + 0.5*-0.3 + 0.2*-0.7 = 0.71 | 1, 1
    | 正确 | 1, -0.3, -0.7 |'
- en: '| 3 | 1, -0.3, -0.71.0, 0.7, 0.91.0*1.0 + 0.7*-0.3 + 0.9*-0.7 = 0.16 | 1, 0
    | False | 0, -1, -1.6 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1, -0.3, -0.71.0, 0.7, 0.91.0*1.0 + 0.7*-0.3 + 0.9*-0.7 = 0.16 | 1, 0
    | 错误 | 0, -1, -1.6 |'
- en: The second epoch begins using the values of the weights from the first epoch.
    Two training instances are classified incorrectly during this epoch. The weights
    are updated twice, but the decision boundary at the end of the second epoch is
    similar the decision boundary at the end of the first epoch.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个训练周期开始时，使用了第一个训练周期的权重值。在这个周期中，有两个训练实例被错误分类。权重更新了两次，但第二个周期结束时的决策边界与第一个周期结束时的决策边界相似。
- en: '![Binary classification with the perceptron](img/8365OS_08_12.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![使用感知机的二元分类](img/8365OS_08_12.jpg)'
- en: 'The algorithm failed to converge during this epoch, so we will continue training.
    The following table describes the third training epoch:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在这个周期未能收敛，因此我们将继续训练。下表描述了第三个训练周期：
- en: '| Epoch 3 |   |   |   |   |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 第3周期 |   |   |   |   |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Instance** | **Initial Weights****x****Activation** | **Prediction, Target**
    | **Correct** | **Updated Weights** |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| **实例** | **初始权重** **x** **激活** | **预测, 目标** | **正确** | **更新后的权重** |'
- en: '| 0 | 0, -1, -1.61.0, 0.2, 0.11.0*0 + 0.2*-1.0 + 0.1*-1.6 = -0.36 | 0, 1 |
    `False` | 1,-0.8, -1.5 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0, -1, -1.61.0, 0.2, 0.11.0*0 + 0.2*-1.0 + 0.1*-1.6 = -0.36 | 0, 1 |
    `错误` | 1,-0.8, -1.5 |'
- en: '| 1 | 1,-0.8, -1.51.0, 0.4, 0.61.0*1.0 + 0.4*-0.8 + 0.6*-1.5 = -0.22 | 0, 1
    | False | 2, -0.4, -0.9 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1,-0.8, -1.51.0, 0.4, 0.61.0*1.0 + 0.4*-0.8 + 0.6*-1.5 = -0.22 | 0, 1
    | 错误 | 2, -0.4, -0.9 |'
- en: '| 2 | 2, -0.4, -0.91.0, 0.5, 0.21.0*2.0 + 0.5*-0.4 + 0.2*-0.9 = 1.62 | 1, 1
    | True | 2, -0.4, -0.9 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2, -0.4, -0.91.0, 0.5, 0.21.0*2.0 + 0.5*-0.4 + 0.2*-0.9 = 1.62 | 1, 1
    | 正确 | 2, -0.4, -0.9 |'
- en: '| 3 | 2, -0.4, -0.91.0, 0.7, 0.91.0*2.0 + 0.7*-0.4 + 0.9*-0.9 = 0.91 | 1, 0
    | False | 1, -1.1, -1.8 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2, -0.4, -0.91.0, 0.7, 0.91.0*2.0 + 0.7*-0.4 + 0.9*-0.9 = 0.91 | 1, 0
    | 错误 | 1, -1.1, -1.8 |'
- en: 'The perceptron classified more instances incorrectly during this epoch than
    during previous epochs. The following figure depicts the decision boundary at
    the end of the third epoch:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机在这一周期中比之前的周期分类更多的实例错误。下图描绘了第三个周期结束时的决策边界：
- en: '![Binary classification with the perceptron](img/8365OS_08_13.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![感知机的二分类](img/8365OS_08_13.jpg)'
- en: 'The perceptron continues to update its weights throughout the fourth and fifth
    training epochs, and it continues to classify training instances incorrectly.
    During the sixth epoch the perceptron classified all of the instances correctly;
    it converged on a set of weights that separates the two classes. The following
    table describes the sixth training epoch:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机在第四和第五训练周期中继续更新其权重，并且继续对训练实例进行错误分类。在第六个周期中，感知机正确分类了所有实例；它收敛到了一个权重集，可以将两个类别分开。下表描述了第六个训练周期：
- en: '| Epoch 6 |   |   |   |   |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 第6周期 |   |   |   |   |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Instance** | **Initial Weights****x****Activation** | **Prediction, Target**
    | **Correct** | **Updated weights** |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| **实例** | **初始权重****x****激活值** | **预测值，目标值** | **正确** | **更新后的权重** |'
- en: '| 0 | 2, -1, -1.51.0, 0.2, 0.11.0*2 + 0.2*-1 + 0.1*-1.5 = 1.65 | 1, 1 | True
    | 2, -1, -1.5 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 2, -1, -1.51.0, 0.2, 0.11.0*2 + 0.2*-1 + 0.1*-1.5 = 1.65 | 1, 1 | 正确
    | 2, -1, -1.5 |'
- en: '| 1 | 2, -1, -1.51.0, 0.4, 0.61.0*2 + 0.4*-1 + 0.6*-1.5 = 0.70 | 1, 1 | True
    | 2, -1, -1.5 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2, -1, -1.51.0, 0.4, 0.61.0*2 + 0.4*-1 + 0.6*-1.5 = 0.70 | 1, 1 | 正确
    | 2, -1, -1.5 |'
- en: '| 2 | 2, -1, -1.51.0, 0.5, 0.21.0*2 + 0.5*-1 + 0.2*-1.5 = 1.2 | 1, 1 | True
    | 2, -1, -1.5 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2, -1, -1.51.0, 0.5, 0.21.0*2 + 0.5*-1 + 0.2*-1.5 = 1.2 | 1, 1 | 正确 |
    2, -1, -1.5 |'
- en: '| 3 | 2, -1, -1.51.0, 0.7, 0.91.0*2 + 0.7*-1 + 0.9*-1.5 = -0.05 | 0, 0 | True
    | 2, -1, -1.5 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2, -1, -1.51.0, 0.7, 0.91.0*2 + 0.7*-1 + 0.9*-1.5 = -0.05 | 0, 0 | 正确
    | 2, -1, -1.5 |'
- en: 'The decision boundary at the end of the sixth training epoch is shown in the
    following figure:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 第六次训练周期结束时的决策边界如下图所示：
- en: '![Binary classification with the perceptron](img/8365OS_08_16.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![感知机的二分类](img/8365OS_08_16.jpg)'
- en: The following figure shows the decision boundary throughout all the training
    epochs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了所有训练周期中的决策边界。
- en: '![Binary classification with the perceptron](img/8365OS_08_17.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![感知机的二分类](img/8365OS_08_17.jpg)'
- en: Document classification with the perceptron
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知机的文档分类
- en: scikit-learn provides an implementation of the perceptron. As with the other
    implementations that we used, the constructor for the `Perceptron` class accepts
    keyword arguments that set the algorithm's hyperparameters. `Perceptron` similarly
    exposes the `fit_transform()` and `predict()` methods. `Perceptron` also provides
    a `partial_fit()` method, which allows the classifier to train and make predictions
    for streaming data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供了感知机的实现。与我们使用的其他实现一样，`Perceptron` 类的构造函数接受设置算法超参数的关键字参数。`Perceptron`
    同样暴露了 `fit_transform()` 和 `predict()` 方法。`Perceptron` 还提供了 `partial_fit()` 方法，允许分类器训练并对流数据进行预测。
- en: 'In this example, we train a perceptron to classify documents from the 20 newsgroups
    dataset. The dataset consists of approximately 20,000 documents sampled from 20
    Usenet newsgroups. The dataset is commonly used in document classification and
    clustering experiments; scikit-learn provides a convenience function to download
    and read the dataset. We will train a perceptron to classify documents from three
    newsgroups: `rec.sports.hockey`, `rec.sports.baseball`, and `rec.auto`. scikit-learn''s
    `Perceptron` natively supports multiclass classification; it will use the one
    versus all strategy to train a classifier for each of the classes in the training
    data. We will represent the documents as TF-IDF-weighted bags of words. The `partial_fit()`
    method could be used in conjunction with `HashingVectorizer` to train from large
    or streaming data in a memory-constrained setting:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们训练一个感知器来分类20个新闻组数据集中的文档。该数据集由约20,000个文档组成，采样自20个Usenet新闻组。该数据集通常用于文档分类和聚类实验；scikit-learn提供了一个便利的函数来下载和读取数据集。我们将训练一个感知器来分类来自三个新闻组的文档：`rec.sports.hockey`、`rec.sports.baseball`和`rec.auto`。scikit-learn的`Perceptron`原生支持多类分类；它将使用“一对多”策略为训练数据中的每个类别训练一个分类器。我们将文档表示为TF-IDF加权的词袋。`partial_fit()`方法可以与`HashingVectorizer`结合使用，在内存受限的环境中对大量或流数据进行训练：
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following is the output of the script:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是脚本的输出：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: First, we download and read the dataset using the `fetch_20newsgroups()` function.
    Consistent with other built-in datasets, the function returns an object with `data`,
    `target`, and `target_names` fields. We also specify that the documents' headers,
    footers, and quotes should be removed. Each of the newsgroups used different conventions
    in the headers and footers; retaining these explanatory variables makes classifying
    the documents artificially easy. We produce TF-IDF vectors using `TfifdVectorizer`,
    train the perceptron, and evaluate it on the test set. Without hyperparameter
    optimization, the perceptron's average precision, recall, and F1 score are 0.85.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`fetch_20newsgroups()`函数下载并读取数据集。与其他内置数据集一致，函数返回一个包含`data`、`target`和`target_names`字段的对象。我们还指定删除文档的标题、页脚和引用。每个新闻组在标题和页脚中使用不同的约定；保留这些解释性变量会使得文档分类变得过于简单。我们使用`TfifdVectorizer`生成TF-IDF向量，训练感知器，并在测试集上进行评估。未经超参数优化，感知器的平均精度、召回率和F1分数为0.85。
- en: Limitations of the perceptron
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知器的局限性
- en: While the perceptron classified the instances in our example well, the model
    has limitations. Linear models like the perceptron with a Heaviside activation
    function are not **universal function approximators**; they cannot represent some
    functions. Specifically, linear models can only learn to approximate the functions
    for **linearly separable** datasets. The linear classifiers that we have examined
    find a hyperplane that separates the positive classes from the negative classes;
    if no hyperplane exists that can separate the classes, the problem is not linearly
    separable.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管感知器在我们的例子中很好地分类了实例，但该模型存在局限性。像感知器这种使用Heaviside激活函数的线性模型并不是**通用函数逼近器**；它们无法表示某些函数。具体来说，线性模型只能学习逼近**线性可分**数据集的函数。我们所检查的线性分类器找到一个超平面，将正类与负类分开；如果没有一个超平面能够分开这些类别，那么问题就不是线性可分的。
- en: 'A simple example of a function that is linearly inseparable is the logical
    operation **XOR**, or exclusive disjunction. The output of XOR is one when one
    of its inputs is equal to one and the other is equal to zero. The inputs and outputs
    of XOR are plotted in two dimensions in the following graph. When XOR outputs
    **1**, the instance is marked with a circle; when XOR outputs **0**, the instance
    is marked with a diamond, as shown in the following figure:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的线性不可分的函数示例是逻辑运算**XOR**，即排他或运算。XOR的输出为1，当其输入之一为1而另一个为0时。XOR的输入和输出在以下图中以二维形式绘制。当XOR输出**1**时，实例用圆圈标记；当XOR输出**0**时，实例用菱形标记，如下图所示：
- en: '![Limitations of the perceptron](img/8365OS_08_09.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![感知器的局限性](img/8365OS_08_09.jpg)'
- en: It is impossible to separate the circles from the diamonds using a single straight
    line. Suppose that the instances are pegs on a board. If you were to stretch a
    rubber band around both of the positive instances, and stretch a second rubber
    band around both of the negative instances, the bands would intersect in the middle
    of the board. The rubber bands represent **convex** **hulls**, or the envelope
    that contains all of the points within the set and all of the points along any
    line connecting a pair points within the set. Feature representations are more
    likely to be linearly separable in higher dimensional spaces than lower dimensional
    spaces. For instance, text classification problems tend to be linearly separable
    when high-dimensional representations like the bag-of-words are used.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单一的直线无法将圆形与菱形分开。假设这些实例是放置在板上的钉子。如果你用一根橡皮筋围绕正实例，并且用第二根橡皮筋围绕负实例，那么橡皮筋会在板中间交叉。橡皮筋代表**凸**
    **外壳**，即包含集合内所有点以及连接集合中一对点的任何直线上的所有点的外包络。特征表示在更高维空间中比在低维空间中更有可能是线性可分的。例如，当使用高维表示如词袋模型时，文本分类问题往往是线性可分的。
- en: In the next two chapters, we will discuss techniques that can be used to model
    linearly inseparable data. The first technique, called **kernelization**, projects
    linearly inseparable data to a higher dimensional space in which it is linearly
    separable. Kernelization can be used in many models, including perceptrons, but
    it is particularly associated with support vector machines, which we will discuss
    in the next chapter. Support vector machines also support techniques that can
    find the hyperplane that separates linearly inseparable classes with the fewest
    errors. The second technique creates a directed graph of perceptrons. The resulting
    model, called an **artificial neural network**, is a universal function approximator;
    we will discuss artificial neural networks in [Chapter 10](ch10.html "Chapter 10. From
    the Perceptron to Artificial Neural Networks"), *From the Perceptron to Artificial
    Neural Networks*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两章中，我们将讨论可以用于建模线性不可分数据的技术。第一种技术叫做**核化**，它将线性不可分的数据投影到一个更高维的空间，在这个空间中数据是线性可分的。核化可以应用于许多模型，包括感知机，但它与支持向量机特别相关，支持向量机将在下一章中讨论。支持向量机还支持可以找到将线性不可分的类别以最少错误分开的超平面的技术。第二种技术创建了一个感知机的有向图。由此生成的模型，称为**人工神经网络**，是一个通用的函数逼近器；我们将在[第10章](ch10.html
    "第10章. 从感知机到人工神经网络")中讨论人工神经网络，*从感知机到人工神经网络*。
- en: Summary
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we discussed the perceptron. Inspired by neurons, the perceptron
    is a linear model for binary classification. The perceptron classifies instances
    by processing a linear combination of the explanatory variables and weights with
    an activation function. While a perceptron with a logistic sigmoid activation
    function is the same model as logistic regression, the perceptron learns its weights
    using an online, error-driven algorithm. The perceptron can be used effectively
    in some problems. Like the other linear classifiers that we have discussed, the
    perceptron is not a universal function approximator; it can only separate the
    instances of one class from the instances of the other using a hyperplane. Some
    datasets are not linearly separable; that is, no possible hyperplane can classify
    all of the instances correctly. In the following chapters, we will discuss two
    models that can be used with linearly inseparable data: the artificial neural
    network, which creates a universal function approximator from a graph of perceptrons
    and the support vector machine, which projects the data onto a higher dimensional
    space in which it is linearly separable.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们讨论了感知机。感知机受到神经元的启发，是一种用于二分类的线性模型。感知机通过处理解释变量和权重的线性组合以及激活函数来对实例进行分类。虽然带有逻辑
    sigmoid 激活函数的感知机与逻辑回归是相同的模型，但感知机通过在线的、基于误差的算法来学习其权重。感知机在某些问题中可以有效使用。像我们讨论的其他线性分类器一样，感知机并不是一个通用的函数逼近器；它只能通过超平面将一种类别的实例与另一种类别的实例分开。一些数据集是线性不可分的；也就是说，没有可能的超平面能够正确地分类所有实例。在接下来的章节中，我们将讨论两种可以处理线性不可分数据的模型：人工神经网络，它通过感知机图构建一个通用的函数逼近器；以及支持向量机，它将数据投影到一个更高维的空间，在这个空间中数据是线性可分的。
