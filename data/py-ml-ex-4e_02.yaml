- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Building a Movie Recommendation Engine with Naïve Bayes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯构建电影推荐引擎
- en: As promised, in this chapter, we will kick off our supervised learning journey
    with machine learning classification, and specifically, binary classification.
    The goal of the chapter is to build a movie recommendation system, which is a
    good starting point for learning classification from a real-life example—movie
    streaming service providers are already doing this, and we can do the same.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如承诺的那样，在本章中，我们将以机器学习分类，特别是二元分类，开始我们的监督学习之旅。本章的目标是构建一个电影推荐系统，这是从现实生活中的一个例子学习分类的一个很好的起点——电影流媒体服务提供商已经在做这件事，我们也可以做到。
- en: In this chapter, you will learn the fundamental concepts of classification,
    including what it does and its various types and applications, with a focus on
    solving a binary classification problem using a simple, yet powerful, algorithm,
    Naïve Bayes. Finally, the chapter will demonstrate how to fine-tune a model, which
    is an important skill that every data science or machine learning practitioner
    should learn.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习分类的基本概念，包括分类的作用、不同类型及应用，重点解决一个二元分类问题，使用一种简单而强大的算法——朴素贝叶斯。最后，本章将演示如何微调模型，这是一项每个数据科学或机器学习从业者都应该掌握的重要技能。
- en: 'We will go into detail on the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细讨论以下主题：
- en: Getting started with classification
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始分类
- en: Exploring Naïve Bayes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索朴素贝叶斯
- en: Implementing Naïve Bayes
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现朴素贝叶斯
- en: Building a movie recommender with Naïve Bayes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯构建电影推荐系统
- en: Evaluating classification performance
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估分类性能
- en: Tuning models with cross-validation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交叉验证调整模型
- en: Getting started with classification
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始分类
- en: Movie recommendation can be framed as a machine learning classification problem.
    If it is predicted that you’ll like a movie because you’ve liked or watched similar
    movies, for example, then it will be on your recommended list; otherwise, it won’t.
    Let’s get started by learning the important concepts of machine learning classification.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 电影推荐可以被看作一个机器学习分类问题。例如，如果预测您会喜欢某部电影，因为您曾喜欢或观看过类似的电影，那么它会出现在您的推荐列表中；否则，它不会。让我们从学习机器学习分类的基本概念开始。
- en: '**Classification** is one of the main instances of supervised learning. Given
    a training set of data containing observations and their associated categorical
    outputs, the goal of classification is to learn a general rule that correctly
    maps the **observations** (also called **features** or **predictive variables**)
    to the target **categories** (also called **labels** or **classes**). Putting
    it another way, a trained classification model will be generated after the model
    learns from the features and targets of training samples, as shown in the first
    half of *Figure 2.1*. When new or unseen data comes in, the trained model will
    be able to determine their desired class memberships. Class information will be
    predicted based on the known input features using the trained classification model,
    as displayed in the second half of *Figure 2.1*:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类**是监督学习的主要实例之一。给定一个包含观测值及其相关类别输出的训练数据集，分类的目标是学习一个通用规则，将**观测值**（也称为**特征**或**预测变量**）正确映射到目标**类别**（也称为**标签**或**类**）。换句话说，一个训练好的分类模型会在模型从训练样本的特征和目标中学习后生成，如*图2.1*的前半部分所示。当新的或未见过的数据输入时，训练好的模型将能够确定它们的目标类别。类信息将根据已知的输入特征，使用训练好的分类模型进行预测，如*图2.1*的后半部分所示：'
- en: '![](img/B21047_02_01.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_01.png)'
- en: 'Figure 2.1: The training and prediction stages in classification'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：分类中的训练和预测阶段
- en: In general, there are three types of classification based on the possibility
    of class output—**binary**, **multiclass**, and **multi-label classification**.
    We will cover them one by one in this section.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，分类根据类别输出的可能性有三种类型——**二元**、**多类**和**多标签分类**。我们将在本节中逐一介绍它们。
- en: Binary classification
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 二元分类
- en: Binary classification classifies observations into one of two possible classes.
    Spam email filtering we encounter every day is a typical use case of binary classification,
    which identifies email messages (input observations) as spam or not spam (output
    classes). Customer churn prediction is another frequently mentioned example, where
    a prediction system takes in customer segment data and activity data from **customer
    relationship management** (**CRM**) systems and identifies which customers are
    likely to churn.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类将观测值分类为两种可能类别之一。我们每天遇到的垃圾邮件过滤就是二分类的典型应用，它识别电子邮件（输入观测值）是垃圾邮件还是非垃圾邮件（输出类别）。客户流失预测是另一个常见的例子，预测系统从**客户关系管理**（**CRM**）系统中获取客户细分数据和活动数据，并识别哪些客户可能会流失。
- en: Another application in the marketing and advertising industry is click-through
    prediction for online ads—that is, whether or not an ad will be clicked, given
    users’ interest information and browsing history. Last but not least, binary classification
    is also being employed in biomedical science, for example, in early cancer diagnosis,
    classifying patients into high- or low-risk groups based on MRI images.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 营销和广告行业的另一个应用是在线广告的点击预测——即根据用户的兴趣信息和浏览历史，预测广告是否会被点击。最后但同样重要的是，二分类也在生物医学科学中得到了应用，例如，在癌症早期诊断中，根据
    MRI 图像将患者分类为高风险或低风险组。
- en: 'As demonstrated in *Figure 2.2*, binary classification tries to find a way
    to separate data into two classes (denoted by dots and crosses):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 2.2*所示，二分类尝试找到一种方法，将数据分为两个类别（分别用点和叉表示）：
- en: '![](img/B21047_02_02.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_02.png)'
- en: 'Figure 2.2: Binary classification example'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：二分类示例
- en: Don’t forget that predicting whether a person likes a movie is also a binary
    classification problem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 别忘了，预测一个人是否喜欢某部电影也是一个二分类问题。
- en: Multiclass classification
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多类分类
- en: This type of classification is also referred to as **multinomial classification**.
    It allows more than two possible classes, as opposed to only two in binary cases.
    Handwritten digit recognition is a common instance of classification and has a
    long history of research and development since the early 1900s. A classification
    system, for example, can learn to read and understand handwritten ZIP codes (digits
    from 0 to 9 in most countries) by which envelopes are automatically sorted.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分类方法也称为**多项式分类**。与二分类只有两种可能类别不同，它允许多于两个的类别。手写数字识别是一个常见的分类实例，自 20 世纪初以来，已进行大量研究和开发。例如，一个分类系统可以学习读取并理解手写的邮政编码（大多数国家的数字从
    0 到 9），从而自动对信封进行分类。
- en: 'Handwritten digit recognition has become a *“Hello, World!”* in the journey
    of studying machine learning, and the scanned document dataset constructed by
    the **National Institute of Standards and Technology** (**NIST**), called **Modified
    National Institute of Standards and Technology** (**MNIST**), is a benchmark dataset
    frequently used to test and evaluate multiclass classification models. *Figure
    2.3* shows four samples taken from the MNIST dataset, representing the digits
    “`9`,” “`2`,” “`1`,” and “`3`,” respectively:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 手写数字识别已成为学习机器学习过程中的*“Hello, World!”*，由**国家标准与技术研究院**（**NIST**）构建的扫描文档数据集，称为**修改版国家标准与技术研究院**（**MNIST**），是一个常用的基准数据集，用于测试和评估多类分类模型。*图
    2.3*展示了从 MNIST 数据集中提取的四个样本，分别代表数字“`9`”、“`2`”、“`1`”和“`3`”：
- en: '![](img/B21047_02_03.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_03.png)'
- en: 'Figure 2.3: Samples from the MNIST dataset'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：来自 MNIST 数据集的样本
- en: 'As another example, in *Figure 2.4*, the multiclass classification model tries
    to find segregation boundaries to separate data into the following three different
    classes (denoted by dots, crosses, and triangles):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子，在*图 2.4*中，多类分类模型尝试找到分隔边界，将数据分为以下三种不同的类别（分别用点、叉和三角形表示）：
- en: '![A picture containing screenshot, diagram  Description automatically generated](img/B21047_02_04.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含屏幕截图、图表的图片  描述自动生成](img/B21047_02_04.png)'
- en: 'Figure 2.4: Multiclass classification example'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：多类分类示例
- en: Multi-label classification
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多标签分类
- en: In the first two types of classification, target classes are mutually exclusive
    and a sample is assigned *one, and only one*, label. It is the opposite in multi-label
    classification. Increasing research attention has been drawn to multi-label classification
    by the nature of the combination of categories in modern applications. For example,
    a picture that captures a sea and a sunset can simultaneously belong to both conceptual
    scenes, whereas it can only be an image of either a cat or dog in a binary case,
    or one type of fruit among oranges, apples, and bananas in a multiclass case.
    Similarly, adventure films are often combined with other genres, such as fantasy,
    science fiction, horror, and drama.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两种类型的分类中，目标类别是互斥的，每个样本只能分配*一个，且仅有一个*标签。而在多标签分类中则正好相反。由于现代应用中类别组合的特性，越来越多的研究关注多标签分类。例如，一张同时捕捉到大海和日落的图片可以同时属于这两种概念场景，而在二分类情况下，它只能是猫或狗的图像，或者在多类别情况下只能是橙子、苹果或香蕉中的一种水果。同样，冒险电影常常与其他类型的电影结合，如奇幻、科幻、恐怖和剧情片。
- en: Another typical application is protein function classification, as a protein
    may have more than one function—storage, antibody, support, transport, and so
    on.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个典型的应用是蛋白质功能分类，因为一个蛋白质可能具有多种功能——存储、抗体、支撑、运输等等。
- en: A typical approach to solving an *n*-label classification problem is to transform
    it into a set of *n* binary classification problems, where each binary classification
    problem is handled by an individual binary classifier.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 解决*n*标签分类问题的典型方法是将其转化为一组*n*二分类问题，每个二分类问题由一个独立的二分类器处理。
- en: 'Refer to *Figure 2.5* to see the restructuring of a multi-label classification
    problem into a multiple-binary classification problem:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考*图2.5*，查看如何将多标签分类问题重构为多个二分类问题：
- en: '![A diagram of a multi-label classifier  Description automatically generated
    with medium confidence](img/B21047_02_05.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![A diagram of a multi-label classifier  Description automatically generated
    with medium confidence](img/B21047_02_05.png)'
- en: 'Figure 2.5: Transforming three-label classification into three independent
    binary classifications'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5：将三标签分类转化为三个独立的二分类问题
- en: 'Using the protein function classification example once more, we can transform
    it into several binary classifications, such as: Is it for storage? Is it for
    antibodies? Is it for support?'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用蛋白质功能分类的例子，我们可以将其转化为几个二分类问题，例如：它是用来存储的吗？它是用来抗体的吗？它是用来支撑的吗？
- en: To solve problems like these, researchers have developed many powerful classification
    algorithms, among which Naïve Bayes, **Support Vector Machines** (**SVMs**), decision
    trees, logistic regression, and neural networks are often used.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，研究人员开发了许多强大的分类算法，其中朴素贝叶斯、**支持向量机**（**SVMs**）、决策树、逻辑回归和神经网络常常被使用。
- en: In the following sections, we will cover the mechanics of Naïve Bayes and its
    in-depth implementation, along with other important concepts, including classifier
    tuning and classification performance evaluation. Stay tuned for upcoming chapters
    that cover the other classification algorithms.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍朴素贝叶斯的机制及其深入实现，以及其他重要概念，包括分类器调优和分类性能评估。请关注接下来的章节，了解其他分类算法。
- en: Exploring Naïve Bayes
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索朴素贝叶斯
- en: 'The **Naïve Bayes** classifier belongs to the family of probabilistic classifiers.
    It computes the probabilities of each predictive **feature** (also referred to
    as an **attribute** or **signal**) of the data belonging to each class in order
    to make a prediction of the probability distribution over all classes. Of course,
    from the resulting probability distribution, we can conclude the most likely class
    that the data sample is associated with. What Naïve Bayes does specifically, as
    its name indicates, is as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯**分类器属于概率分类器的范畴。它计算每个预测**特征**（也称为**属性**或**信号**）属于每个类别的概率，从而对所有类别的概率分布做出预测。当然，从得到的概率分布中，我们可以得出数据样本最有可能关联的类别。朴素贝叶斯具体做的事情，如其名所示，包含以下内容：'
- en: '**Bayes**: As in, it maps the probability of observed input features given
    a possible class to the probability of the class given observed pieces of evidence
    based on Bayes’ theorem.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贝叶斯**：也就是说，它根据贝叶斯定理，将观察到的输入特征在给定可能类别下的概率映射到在观察到的证据基础上给定类别的概率。'
- en: '**Naïve**: As in, it simplifies probability computation by assuming that predictive
    features are mutually independent.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**朴素**：即假设预测特征是相互独立的，从而简化概率计算。'
- en: I will explain Bayes’ theorem with examples in the next section.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在下一节通过实例来解释贝叶斯定理。
- en: Bayes’ theorem by example
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯定理通过实例
- en: 'It is important to understand Bayes’ theorem before diving into the classifier.
    Let *A* and *B* denote any two events. Events could be that *it will rain tomorrow,
    two kings are drawn from a deck of cards, or a person has cancer*. In Bayes’ theorem,
    *P*(*A* | *B*) is the probability that *A* occurs given that *B* is true. It can
    be computed as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解分类器之前，理解贝叶斯定理非常重要。让*A*和*B*表示任何两个事件。事件可能是“明天会下雨”，“从一副扑克牌中抽出两张国王”或者“一个人患有癌症”。在贝叶斯定理中，*P*(*A*
    | *B*)表示在*B*为真的情况下，*A*发生的概率。它可以通过以下公式计算：
- en: '![](img/B21047_02_001.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_001.png)'
- en: 'Here, *P*(*B* | *A*) is the probability of observing *B* given that *A* occurs,
    while *P*(*A*) and *P*(*B*) are the probability that *A* and *B* occur, respectively.
    Is that too abstract? Let’s consider the following concrete examples:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*P*(*B* | *A*)是给定*B*发生时*B*的概率，而*P*(*A*)和*P*(*B*)分别是*B*和*A*发生的概率。是不是太抽象了？让我们通过以下具体的例子来考虑：
- en: '**Example 1**: Given two coins, one is unfair, with 90% of flips getting a
    head and 10% getting a tail, while the other one is fair. Randomly pick one coin
    and flip it. What is the probability that this coin is the unfair one, if we get
    a head?'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**例子 1**：给定两个硬币，其中一个是不公平的，90%的翻转结果为正面，10%的结果为反面，另一个是公平的。随机选择一个硬币并进行翻转。如果我们得到正面，那个硬币是那个不公平的硬币的概率是多少？'
- en: 'We can solve this by first denoting *U* for the event of picking the unfair
    coin, *F* for the fair coin, and *H* for the event of getting a head. So, the
    probability that the unfair coin has been picked when we get a head, *P(U |H)*,
    can be calculated with the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过首先将*U*表示为选择不公平硬币的事件，*F*表示公平硬币，*H*表示出现正面的事件来解决这个问题。因此，在出现正面时，*P(U | H)*是选择不公平硬币的概率，可以通过以下公式计算：
- en: '![](img/B21047_02_002.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_002.png)'
- en: 'As we know, *P*(*H* | *U*) is `0.9`. *P*(*U*) is `0.5` because we randomly
    pick a coin out of two. However, deriving the probability of getting a head, *P*(*H*),
    is not that straightforward, as two events can lead to the following, where *U*
    is when the unfair coin is picked, and *F* is when the fair coin is picked:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，*P*(*H* | *U*)为`0.9`。*P*(*U*)为`0.5`，因为我们从两个硬币中随机选择一个。然而，推导出出现正面的概率*P*(*H*)并不那么简单，因为两种事件可能导致以下情况，其中*U*是选择不公平硬币，*F*是选择公平硬币：
- en: '![](img/B21047_02_003.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_003.png)'
- en: 'Now, *P*(*U*|*H*) becomes the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，*P*(*U* | *H*)变为以下形式：
- en: '![](img/B21047_02_004.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_004.png)'
- en: So, under Bayes’ theorem, the probability that the unfair coin has been picked
    when we get a head is `0.64`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在贝叶斯定理下，得到正面时选择不公平硬币的概率是`0.64`。
- en: '**Example 2**: Suppose a physician reported the following cancer screening
    test scenario among 10,000 people:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**例子 2**：假设一位医生报告了以下癌症筛查测试结果，涉及10,000人：'
- en: '|  | **Cancer** | **No Cancer** | **Total** |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | **癌症** | **无癌症** | **总计** |'
- en: '| **Test Positive** | 80 | 900 | 980 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| **筛查阳性** | 80 | 900 | 980 |'
- en: '| **Test Negative** | 20 | 9000 | 9020 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| **筛查阴性** | 20 | 9000 | 9020 |'
- en: '| **Total** | 100 | 9900 | 10000 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| **总计** | 100 | 9900 | 10000 |'
- en: 'Table 2.1: Example of a cancer screening result'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.1：癌症筛查结果示例
- en: This indicates that 80 out of 100 cancer patients are correctly diagnosed, while
    the other 20 are not; cancer is falsely detected in 900 out of 9,900 healthy people.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明100名癌症患者中有80人被正确诊断，另外20人未被诊断出；癌症在9,900名健康人中有900人被误诊。
- en: If the result of this screening test on a person is positive, what is the probability
    that they actually have cancer? Let’s assign the event of having cancer and positive
    testing results as *C* and *Pos*, respectively. So we have *P*(*Pos* |*C*) = `80/100`
    = `0.8`, *P*(*C*) = `100/10000` = `0.01`, and *P*(*Pos*) = `980/10000` = `0.098`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个人的筛查结果为阳性，那么他实际上患有癌症的概率是多少？我们将患癌症和筛查结果为阳性分别表示为*C*和*Pos*。因此，*P*(*Pos* |*C*)
    = `80/100` = `0.8`，*P*(*C*) = `100/10000` = `0.01`，*P*(*Pos*) = `980/10000` =
    `0.098`。
- en: 'We can apply Bayes’ theorem to calculate *P*(*C*|*Pos*):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以应用贝叶斯定理来计算*P*(*C* | *Pos*)：
- en: '![](img/B21047_02_005.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_005.png)'
- en: Given a positive screening result, the chance that the subject has cancer is
    8.16%, which is significantly higher than the one under the general assumption
    (100/10000=1%) without the subject undergoing the screening.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个阳性筛查结果，受试者患癌症的几率是 8.16%，这比在没有进行筛查的情况下（假设 100/10000=1%）高出很多。
- en: '**Example 3**: Three machines, *A*, *B*, and *C*, in a factory account for
    35%, 20%, and 45% of bulb production. The fraction of defective bulbs produced
    by each machine is 1.5%, 1%, and 2%, respectively. A bulb produced by this factory
    was identified as defective, which is denoted as event *D*. What are the probabilities
    that this bulb was manufactured by machine *A*, *B*, or *C*, respectively?'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**例子 3**：在一个工厂中，三台机器 *A*、*B* 和 *C* 分别占据了 35%、20% 和 45% 的灯泡生产份额。每台机器生产的次品灯泡比例分别为
    1.5%、1% 和 2%。一只由该工厂生产的灯泡被鉴定为次品，表示为事件 *D*。分别求出这只灯泡是由机器 *A*、*B* 或 *C* 制造的概率。'
- en: 'Again, we can simply follow Bayes’ theorem:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以简单地遵循贝叶斯定理：
- en: '![](img/B21047_02_006.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_006.png)'
- en: '![](img/B21047_02_007.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_007.png)'
- en: '![](img/B21047_02_008.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_008.png)'
- en: '![](img/B21047_02_009.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_009.png)'
- en: '![](img/B21047_02_010.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_010.png)'
- en: '![](img/B21047_02_011.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_011.png)'
- en: So, under Bayes’ theorem, the probabilities that this bulb was manufactured
    by machine *A*, *B*, or *C*, are `0.323`, `0.123`, and `0.554` respectively.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，根据贝叶斯定理，这个灯泡是由 *A*、*B* 或 *C* 机器制造的概率分别是 `0.323`、`0.123` 和 `0.554`。
- en: 'Also, either way, we do not even need to calculate *P*(*D*) since we know that
    the following is the case:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，无论哪种方式，我们甚至不需要计算 *P*(*D*)，因为我们知道以下情况：
- en: '![](img/B21047_02_012.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_012.png)'
- en: 'We also know the following concept:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还知道以下概念：
- en: '![](img/B21047_02_013.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_013.png)'
- en: 'So, we have the following formula:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们有以下公式：
- en: '![](img/B21047_02_014.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_014.png)'
- en: '![](img/B21047_02_015.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_015.png)'
- en: This shortcut approach gave us the same results as the original method, but
    faster. Now that you understand Bayes’ theorem as the backbone of Naïve Bayes,
    we can easily move forward with the classifier itself.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简便的方法得出了与原始方法相同的结果，但速度更快。现在你理解了贝叶斯定理是朴素贝叶斯的核心，我们可以轻松地继续进行分类器本身的内容。
- en: The mechanics of Naïve Bayes
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯的原理
- en: Let’s start by discussing the magic behind the algorithm—how Naïve Bayes works.
    Given a data sample, *x*, with *n* features, *x*[1], *x*[2],..., *x*[n] (*x* represents
    a feature vector and *x* = (*x*[1], *x*[2],..., *x*[n])), the goal of Naïve Bayes
    is to determine the probabilities that this sample belongs to each of *K* possible
    classes *y*[1], *y*[2],..., *y*[K], which is *P(y*[K] *|x)* or *P*(*x*[1], *x*[2],...,
    *x*[n]), where *k* = 1, 2, …, *K*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从讨论算法背后的魔力开始——朴素贝叶斯是如何工作的。给定一个数据样本，*x*，它有*n*个特征，*x*[1]、*x*[2]、...、*x*[n]（*x*
    代表一个特征向量，并且 *x* = (*x*[1]、*x*[2]、...、*x*[n]）），朴素贝叶斯的目标是确定这个样本属于 *K* 个可能类别 *y*[1]、*y*[2]、...、*y*[K]
    中每一个的概率，即 *P(y*[K] *|x)* 或 *P*(*x*[1]、*x*[2]、...、*x*[n])，其中 *k* = 1, 2, …, *K*。
- en: 'This looks no different from what we have just dealt with: *x* or *x*[1], *x*[2],...,
    *x*[n]. This is a joint event where a sample that has observed feature values
    *x*[1], *x*[2],..., *x*[n]. *y*[K] is the event that the sample belongs to class
    *k*. We can apply Bayes’ theorem right away:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来和我们刚刚处理的没什么不同：*x* 或 *x*[1]、*x*[2]、...、*x*[n]。这是一个联合事件，表示一个样本观察到特征值 *x*[1]、*x*[2]、...、*x*[n]。*y*[K]
    是样本属于类别 *k* 的事件。我们可以直接应用贝叶斯定理：
- en: '![](img/B21047_02_016.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_016.png)'
- en: 'Let’s look at each component in detail:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看一下每个组成部分：
- en: '*P*(*y*[k]) portrays how classes are distributed, with no further knowledge
    of observation features. Thus, it is also called **prior** in Bayesian probability
    terminology. Prior can be either predetermined (usually in a uniform manner where
    each class has an equal chance of occurrence) or learned from a set of training
    samples.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*y*[k]) 描述了类别的分布情况，并没有进一步观察特征的知识。因此，它也被称为贝叶斯概率术语中的**先验概率**。先验概率可以是预定的（通常是均匀的，即每个类别的发生机会相同），也可以通过一组训练样本进行学习。'
- en: '*P*(*y*[k]|*x*), in contrast to prior *P*(*y*[k]), is the **posterior**, with
    extra knowledge of observation.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*y*[k]|*x*)，与先前的 *P*(*y*[k]) 相比，是**后验概率**，它包含了额外的观察信息。'
- en: '*P*(*x* |*y*[K]), or *P*(*x*[1]*, x*[2]*,..., x*[n]|*y*[k]), is the joint distribution
    of *n* features, given that the sample belongs to class *y*[k]. This is how likely
    the features with such values co-occur. This is named **likelihood** in Bayesian
    terminology. Obviously, the likelihood will be difficult to compute as the number
    of features increases. In Naïve Bayes, this is solved thanks to the feature independence
    assumption. The joint conditional distribution of *n* features can be expressed
    as the joint product of individual feature conditional distributions:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*x* |*y*[K])，或 *P*(*x*[1]*, x*[2]*,..., x*[n]|*y*[k])，是给定样本属于类别*y*[k]时，*n*个特征的联合分布。这表示具有这些值的特征共同发生的可能性。在贝叶斯术语中，这被称为**似然**。显然，随着特征数量的增加，计算似然将变得困难。在朴素贝叶斯中，这是通过特征独立性假设来解决的。*n*个特征的联合条件分布可以表示为单个特征条件分布的联合乘积：'
- en: '![](img/B21047_02_017.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_017.png)'
- en: Each conditional distribution can be efficiently learned from a set of training
    samples.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 每个条件分布可以通过一组训练样本高效地学习得到。
- en: '*P*(*x*), also called **evidence**, solely depends on the overall distribution
    of features, which is not specific to certain classes and is therefore a normalization
    constant. As a result, posterior is proportional to prior and likelihood:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*x*)，也称为**证据**，仅依赖于特征的整体分布，而与特定类别无关，因此它是一个归一化常数。因此，后验概率与先验概率和似然成正比：'
- en: '![](img/B21047_02_018.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_018.png)'
- en: '*Figure 2.6* summarizes how a Naïve Bayes classification model is trained and
    applied to new data:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.6* 总结了如何训练朴素贝叶斯分类模型并将其应用于新数据：'
- en: '![](img/B21047_02_06.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_06.png)'
- en: 'Figure 2.6: Training and prediction stages in Naïve Bayes classification'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6：朴素贝叶斯分类的训练和预测阶段
- en: A Naïve Bayes classification model is trained using labeled data, where each
    instance is associated with a class label. During training, the model learns the
    probability distribution of the features given each class. This involves calculating
    the likelihood of observing each feature value given each class. Once trained,
    the model can be applied to new, unlabeled data. To classify a new instance, the
    model calculates the probability of each class given the observed features using
    Bayes’ theorem.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类模型是通过使用标记数据进行训练的，每个实例都与一个类别标签相关联。在训练过程中，模型学习给定每个类别时特征的概率分布。这涉及到计算给定每个类别时观察到每个特征值的似然。一旦训练完成，模型就可以应用于新的、未标记的数据。为了对一个新实例进行分类，模型使用贝叶斯定理计算给定观察到的特征下每个类别的概率。
- en: 'Let’s see a Naïve Bayes classifier in action through a simplified example of
    movie recommendation before we jump to the implementations of Naïve Bayes. Given
    four (pseudo) users, whether they like each of three movies, *m*[1]*, m*[2]*,*
    and *m*[3] (indicated as 1 or 0), and whether they like a target movie (denoted
    as event *Y*) or not (denoted as event *N*), as shown in the following table,
    we are asked to predict how likely it is that another user will like that movie:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨朴素贝叶斯的实现之前，让我们通过一个简化的电影推荐示例来看一下朴素贝叶斯分类器的应用。给定四个（伪）用户，他们是否喜欢三部电影，*m*[1]*，m*[2]*，*和*m*[3]（用1或0表示），以及他们是否喜欢目标电影（表示为事件*Y*）或不喜欢（表示为事件*N*），如以下表格所示，我们需要预测另一个用户喜欢该电影的可能性：
- en: '|  | **ID** | **m1** | **m2** | **m3** | **Whether the user likes the target
    movie** |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | **ID** | **m1** | **m2** | **m3** | **用户是否喜欢目标电影** |'
- en: '| **Training data** | 1 | 0 | 1 | 1 | **Y** |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| **训练数据** | 1 | 0 | 1 | 1 | **Y** |'
- en: '| 2 | 0 | 0 | 1 | **N** |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 0 | 1 | **N** |'
- en: '| 3 | 0 | 0 | 0 | **Y** |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | 0 | 0 | **Y** |'
- en: '| 4 | 1 | 1 | 0 | **Y** |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1 | 1 | 0 | **Y** |'
- en: '| **Testing case** | 5 | 1 | 1 | 0 | **?** |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| **测试案例** | 5 | 1 | 1 | 0 | **?** |'
- en: 'Table 2.2: Toy data example for a movie recommendation'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.2：电影推荐的玩具数据示例
- en: Whether users like three movies, *m*[1]*, m*[2]*,* and *m*[3], are features
    (signals) that we can utilize to predict the target class. The training data we
    have are the four samples with both ratings and target information.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 用户是否喜欢三部电影，*m*[1]*，m*[2]*，*和*m*[3]，是我们可以用来预测目标类别的特征（信号）。我们拥有的训练数据是包含评分和目标信息的四个样本。
- en: 'Now, let’s first compute the prior, *P*(*Y*) and *P*(*N*). From the training
    set, we can easily get the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们首先计算先验，*P*(*Y*) 和 *P*(*N*)。通过训练集，我们可以轻松获得以下数据：
- en: '![](img/B21047_02_019.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_019.png)'
- en: Alternatively, we can also impose an assumption of a uniform prior that *P*(*Y*)
    = 50%, for example.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们也可以假设一个均匀先验，例如*P*(*Y*) = 50%。
- en: 'For simplicity, we will denote the event that a user likes three movies or
    not as *f*[1]*, f*[2]*,* and *f*[3], respectively. To calculate posterior *P*(*Y|
    x*)*,* where *x* = (1, 1, 0), the first step is to compute the likelihoods, *P*(*f*[1]
    *= 1*| *Y*), *P*(*f*[2] *= 1 Y*), and *P*(*f*[3] *= 0*| *Y*), and similarly, *P*(*f*[1]
    *= 1*| *N*), *P*(*f*[2] *= 1*| *N*), and *P*(*f*[3] *= 0*| *N*), based on the
    training set. However, you may notice that since *f*[1] *= 1* was not seen in
    the *N* class, we will get *P*(*f*[1] *= 1*|*N*) *= 0*. Consequently, we will
    have the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们将用户喜欢三部电影与否的事件分别表示为*f*[1]*, f*[2]*,* 和 *f*[3]。为了计算后验概率 *P*(*Y| x*)*，其中
    *x* = (1, 1, 0)，第一步是计算可能性，*P*(*f*[1] *= 1*| *Y*), *P*(*f*[2] *= 1*| *Y*), 和 *P*(*f*[3]
    *= 0*| *Y*)，同样地，*P*(*f*[1] *= 1*| *N*), *P*(*f*[2] *= 1*| *N*), 和 *P*(*f*[3] *=
    0*| *N*)，基于训练集计算。然而，你可能注意到，由于在*N*类中没有看到 *f*[1] *= 1*，我们会得到 *P*(*f*[1] *= 1*| *N*)
    *= 0*。因此，我们将得到如下结果：
- en: '![](img/B21047_02_020.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_020.png)'
- en: This means we will recklessly predict class = *Y* by any means.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们将不加思索地通过任何方式预测类别 = *Y*。
- en: 'To eliminate the zero-multiplication factor, the unknown likelihood, we usually
    assign an initial value of 1 to each feature, that is, we start counting each
    possible value of a feature from one. This technique is also known as **Laplace
    smoothing**. With this amendment, we now have the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了消除零乘法因子（未知的可能性），我们通常会为每个特征分配一个初始值为1的值，即我们从1开始计算每个特征的可能值。这种技术也被称为**拉普拉斯平滑**。通过这种修改，我们现在得到如下结果：
- en: '![](img/B21047_02_021.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_021.png)'
- en: '![](img/B21047_02_022.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_022.png)'
- en: Here, given class *N*, 0 + 1 means there are zero likes of *m*[1] plus + 1 smoothing;
    1 + 2 means there is one data point (ID = 2) plus 2 (2 possible values) + 1 smoothing.
    Given class *Y*, 1 + 1 means there is one like of *m*[1] (ID = 4) plus + 1 smoothing;
    3 + 2 means there are 3 data points (ID = 1, 3, 4) plus 2 (2 possible values)
    + 1 smoothing.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，给定类别*N*，0 + 1表示有零个*m*[1]的点赞，加上+1平滑；1 + 2表示有一个数据点（ID = 2），加上2（2个可能的值）+ 1平滑。给定类别*Y*，1
    + 1表示有一个*m*[1]的点赞（ID = 4），加上+1平滑；3 + 2表示有3个数据点（ID = 1, 3, 4），加上2（2个可能的值）+ 1平滑。
- en: 'Similarly, we can compute the following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以计算以下内容：
- en: '![](img/B21047_02_023.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_023.png)'
- en: '![](img/B21047_02_024.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_024.png)'
- en: '![](img/B21047_02_025.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_025.png)'
- en: '![](img/B21047_02_026.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_026.png)'
- en: 'Now, we can compute the ratio between two posteriors as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以按如下方式计算两个后验概率之间的比率：
- en: '![](img/B21047_02_027.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_027.png)'
- en: 'Also, remember this:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请记住这一点：
- en: '![](img/B21047_02_028.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_028.png)'
- en: 'So, finally, we have the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，最后我们得到了以下结果：
- en: '![](img/B21047_02_029.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_029.png)'
- en: There is a `92.1%` chance that the new user will like the target movie.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 新用户喜欢目标电影的概率为`92.1%`。
- en: I hope that you now have a solid understanding of Naïve Bayes after going through
    the theory and a toy example. Let’s get ready for its implementation in the next
    section.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望在经历了理论和一个玩具示例后，你现在对朴素贝叶斯有了扎实的理解。让我们准备在下一部分实现它。
- en: Implementing Naïve Bayes
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现朴素贝叶斯
- en: After calculating the movie preference example by hand, as promised, we are
    going to implement Naïve Bayes from scratch. After that, we will implement it
    using the `scikit-learn` package.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在手动计算完电影偏好示例之后，正如我们承诺的那样，我们将从头开始实现朴素贝叶斯。然后，我们将使用`scikit-learn`包来实现它。
- en: Implementing Naïve Bayes from scratch
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从头实现朴素贝叶斯
- en: 'Before we develop the model, let’s define the toy dataset we just worked with:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开发模型之前，先定义一下我们刚才使用的玩具数据集：
- en: '[PRE0]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For the model, starting with the prior, we first group the data by label and
    record their indices by classes:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型，从先验开始，我们首先按标签对数据进行分组，并按类别记录它们的索引：
- en: '[PRE1]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Take a look at what we get:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 看看我们得到的结果：
- en: '[PRE2]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With `label_indices`, we calculate the prior:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`label_indices`，我们计算先验概率：
- en: '[PRE3]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Take a look at the computed prior:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 看看计算得到的先验：
- en: '[PRE4]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With `prior` calculated, we continue with `likelihood`, which is the conditional
    probability, `P(feature|class)`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 计算完`prior`后，我们继续计算`likelihood`，即条件概率`P(feature|class)`：
- en: '[PRE5]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We set the `smoothing` value to 1 here, which can also be 0 for no smoothing,
    or any other positive value, as long as a higher classification performance is
    achieved:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将`smoothing`值设置为1，也可以设置为0表示没有平滑，或者设置为其他任何正值，只要能够获得更好的分类性能：
- en: '[PRE6]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If you ever find any of this confusing, feel free to check *Figure 2.7* to
    refresh your memory:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得这些内容有点困惑，可以随时查看*图 2.7*来刷新一下记忆：
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B21047_02_07.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B21047_02_07.png)'
- en: 'Figure 2.7: A simple example of computing prior and likelihood'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7：计算先验和似然的简单示例
- en: 'With prior and likelihood ready, we can now compute posterior for the testing/new
    samples:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备好先验和似然后，我们可以计算测试/新样本的后验概率：
- en: '[PRE7]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, let’s predict the class of our one sample test set using this prediction
    function:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用这个预测函数预测我们一个样本测试集的类别：
- en: '[PRE8]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This is exactly what we got previously. We have successfully developed Naïve
    Bayes from scratch and we can now move on to the implementation using `scikit-learn`.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们之前得到的结果。我们已经成功从零开始开发了朴素贝叶斯，现在可以开始使用`scikit-learn`实现了。
- en: Implementing Naïve Bayes with scikit-learn
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用scikit-learn实现朴素贝叶斯
- en: 'Coding from scratch and implementing your own solutions is the best way to
    learn about machine learning models. Of course, you can take a shortcut by directly
    using the `BernoulliNB` module ([https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html))
    from the scikit-learn API:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从头编写代码并实现你自己的解决方案是学习机器学习模型的最佳方式。当然，你也可以通过直接使用scikit-learn API中的`BernoulliNB`模块（[https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)）来走捷径：
- en: '[PRE9]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s initialize a model with a smoothing factor (specified as `alpha` in `scikit-learn`)
    of `1.0`, and `prior` learned from the training set (specified as `fit_prior=True`
    in `scikit-learn`):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们初始化一个具有平滑因子（在`scikit-learn`中指定为`alpha`）为`1.0`，并且从训练集学习的`prior`（在`scikit-learn`中指定为`fit_prior=True`）的模型：
- en: '[PRE10]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To train the Naïve Bayes classifier with the `fit` method, we use the following
    line of code:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`fit`方法训练朴素贝叶斯分类器，我们使用以下代码行：
- en: '[PRE11]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To obtain the predicted probability results with the `predict_proba` method,
    we use the following lines of code:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`predict_proba`方法获取预测概率结果，我们使用以下代码行：
- en: '[PRE12]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we do the following to directly acquire the predicted class with the
    `predict` method (0.5 is the default threshold, and if the predicted probability
    of class `Y` is greater than 0.5, class `Y` is assigned; otherwise, `N` is used):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们做如下操作，使用`predict`方法直接获取预测的类别（0.5是默认的阈值，如果类别`Y`的预测概率大于0.5，则分配类别`Y`；否则，使用类别`N`）：
- en: '[PRE13]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The prediction results using scikit-learn are consistent with what we got using
    our own solution. Now that we’ve implemented the algorithm both from scratch and
    using `scikit-learn`, why don’t we use it to solve the movie recommendation problem?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn得到的预测结果与我们使用自己解决方案得到的结果一致。既然我们已经从零开始和使用`scikit-learn`实现了算法，为什么不直接用它来解决电影推荐问题呢？
- en: Building a movie recommender with Naïve Bayes
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯构建电影推荐系统
- en: After the toy example, it is now time to build a movie recommender (or, more
    specifically, movie preference classifier) using a real dataset. We herein use
    a movie rating dataset ([https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/)).
    The movie rating data was collected by the GroupLens Research group from the MovieLens
    website ([http://movielens.org](http://movielens.org)).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成了玩具示例之后，现在是时候使用一个真实数据集构建一个电影推荐系统（或更具体地说，构建一个电影偏好分类器）了。我们这里使用一个电影评分数据集（[https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/)）。该数据集由GroupLens研究小组从MovieLens网站（[http://movielens.org](http://movielens.org)）收集。
- en: 'For demonstration purposes, we will use the stable small dataset, MovieLens
    1M Dataset (which can be downloaded from [https://files.grouplens.org/datasets/movielens/ml-1m.zip](https://files.grouplens.org/datasets/movielens/ml-1m.zip)
    or [https://grouplens.org/datasets/movielens/1m/](https://grouplens.org/datasets/movielens/1m/))
    for `ml-1m.zip` (size: 1 MB) file). It has around 1 million ratings, ranging from
    1 to 5 with half-star increments, given by 6,040 users on 3,706 movies (last updated
    September 2018).'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，我们将使用稳定的小型数据集——MovieLens 1M 数据集（可以从[https://files.grouplens.org/datasets/movielens/ml-1m.zip](https://files.grouplens.org/datasets/movielens/ml-1m.zip)或[https://grouplens.org/datasets/movielens/1m/](https://grouplens.org/datasets/movielens/1m/)下载），它包含约100万条评分，评分范围从1到5，分半星递增，由6,040个用户对3,706部电影进行评分（最后更新于2018年9月）。
- en: 'Unzip the `ml-1m.zip` file and you will see the following four files:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 解压`ml-1m.zip`文件，你将看到以下四个文件：
- en: '`movies.dat`: It contains the movie information in the format of `MovieID::Title::Genres`.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`movies.dat`：它以`MovieID::Title::Genres`格式包含电影信息。'
- en: '`ratings.dat`: It contains user movie ratings in the format of `UserID::MovieID::Rating::Timestamp`.
    We will only be using data from this file in this chapter.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ratings.dat`：它包含用户电影评分，格式为`UserID::MovieID::Rating::Timestamp`。在本章中，我们将只使用此文件中的数据。'
- en: '`users.dat`: It contains user information in the format of `UserID::Gender::Age::Occupation::Zip-code`.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`users.dat`：它包含用户信息，格式为`UserID::Gender::Age::Occupation::Zip-code`。'
- en: '`README`'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`README`'
- en: Let’s attempt to predict whether a user likes a particular movie based on how
    they rate other movies (again, ratings are from 1 to 5).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试根据用户对其他电影的评分（再次，评分范围从1到5）预测用户是否喜欢某部电影。
- en: Preparing the data
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'First, we import all the necessary modules and read the `ratings.dat` into
    a `pandas` DataFrame object:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入所有必要的模块，并将`ratings.dat`读取到`pandas` DataFrame对象中：
- en: '[PRE14]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, let’s see how many unique users and movies are in this million-row dataset:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这个百万行数据集中有多少独特的用户和电影：
- en: '[PRE15]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we will construct a 6,040 (the number of users) by 3,706 (the number
    of movies) matrix where each row contains movie ratings from a user, and each
    column represents a movie, using the following function:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构建一个6,040（用户数量）行和3,706（电影数量）列的矩阵，其中每一行包含用户的电影评分，每一列代表一部电影，使用以下函数：
- en: '[PRE16]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Besides the rating matrix `data`, we also record the `movie ID` to column index
    mapping. The column index is from 0 to 3,705 as we have 3,706 movies.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 除了评分矩阵`data`，我们还记录了`电影ID`与列索引的映射。列索引从0到3,705，因为我们有3,706部电影。
- en: 'It is always recommended to analyze the data distribution in order to identify
    if there is a class imbalance issue in the dataset. We do the following:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 始终建议分析数据分布，以识别数据集中是否存在类别不平衡问题。我们进行如下操作：
- en: '[PRE17]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you can see, most ratings are unknown; for the known ones, 35% are of rating
    4, followed by 26% of rating 3, 23% of rating 5, and then 11% and 6% of ratings
    2 and 1, respectively.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，大多数评分是未知的；对于已知评分，35%的评分为4分，其次是26%的评分为3分，23%的评分为5分，然后是11%和6%的评分为2分和1分。
- en: 'Since most ratings are unknown, we take the movie with the most known ratings
    as our target movie for easier prediction validation. We look for rating counts
    for each movie as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大多数评分是未知的，我们选择评分已知最多的电影作为目标电影，以便更容易进行预测验证。我们按以下方式查找每部电影的评分数量：
- en: '[PRE18]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'So, the target movie is ID, and we will treat ratings of other movies as features.
    We only use rows with ratings available for the target movie so we can validate
    how good the prediction is. We construct the dataset accordingly as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，目标电影是ID，我们将把其他电影的评分作为特征。我们只使用目标电影有评分的行，以便验证预测的准确性。我们将数据集按如下方式构建：
- en: '[PRE19]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can consider movies with ratings greater than 3 as being liked (being recommended):'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将评分大于3的电影视为喜欢的（被推荐的）电影：
- en: '[PRE20]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As a rule of thumb in solving classification problems, we need to always analyze
    the label distribution and see how balanced (or imbalanced) the dataset is.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 作为解决分类问题的一条经验法则，我们始终需要分析标签分布，并查看数据集是否平衡（或不平衡）。
- en: '**Best practice**'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'Dealing with imbalanced datasets in classification problems requires careful
    consideration and appropriate techniques to ensure that the model effectively
    learns from the data and produces reliable predictions. Here are several strategies
    to address class imbalance:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 处理分类问题中的不平衡数据集需要仔细考虑并采用适当的技术，以确保模型能够有效地从数据中学习并生成可靠的预测。以下是几种应对类别不平衡的策略：
- en: '**Oversampling**: We can increase the number of instances in the minority class
    by generating synthetic samples or duplicating existing ones.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过采样**：我们可以通过生成合成样本或复制现有样本来增加少数类实例的数量。'
- en: '**Undersampling**: We can decrease the number of instances in the majority
    class by randomly removing samples. Note that we can even combine oversampling
    and undersampling for a more balanced dataset.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欠采样**：我们可以通过随机删除样本来减少多数类实例的数量。请注意，我们甚至可以结合过采样和欠采样来获得更平衡的数据集。'
- en: '**Class weighting**: We can also assign higher weights to minority class samples
    during model training. In this way, we penalize misclassifications of the minority
    class more heavily.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别加权**：我们还可以在模型训练期间为少数类样本分配更高的权重。通过这种方式，我们可以对少数类的误分类进行更严重的惩罚。'
- en: Next, to comprehensively evaluate our classifier’s performance, we can randomly
    split the dataset into two sets, the training and testing sets, which simulate
    learning data and prediction data, respectively. Generally, the proportion of
    the original dataset to include in the testing split can be 20%, 25%, 30%, or
    33.3%.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了全面评估分类器的表现，我们可以将数据集随机划分为两个集合：训练集和测试集，分别模拟学习数据和预测数据。通常，原始数据集中包含在测试集中的比例可以是20%、25%、30%或33.3%。
- en: '**Best practice**'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'Here are some guidelines for choosing the testing split:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是选择测试集划分的一些指南：
- en: '**Small datasets**: If you have a small dataset (e.g., less than a few thousand
    samples), a larger testing split (e.g., 25% to 30%) may be appropriate to ensure
    that you have enough data for training and testing.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小规模数据集**：如果你的数据集较小（例如，少于几千个样本），较大的测试集划分（例如，25%到30%）可能更为合适，以确保有足够的数据进行训练和测试。'
- en: '**Medium to large datasets**: For medium to large datasets (e.g., tens of thousands
    to millions of samples), a smaller testing split (e.g., 20%) may still provide
    enough data for evaluation while allowing more data to be used for training. A
    20% testing split is a common choice in such cases.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中到大规模数据集**：对于中到大规模的数据集（例如，数万到数百万的样本），较小的测试集划分（例如，20%）可能仍然为评估提供足够的数据，同时允许更多数据用于训练。20%的测试集划分在这种情况下是一个常见的选择。'
- en: '**Simple models**: Less complex models are generally less prone to overfitting,
    so using a smaller test set split may work.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单模型**：较少复杂的模型通常不容易发生过拟合，因此使用较小的测试集划分可能是可行的。'
- en: '**Complex models**: Complex models like deep learning models can be more prone
    to overfitting. Hence, a larger test set split (e.g., 30%) is recommended.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂模型**：像深度学习模型这样的复杂模型更容易发生过拟合。因此，建议使用更大的测试集划分（例如，30%）。'
- en: 'We use the `train_test_split` function from `scikit-learn` to do the random
    splitting and to preserve the percentage of samples for each class:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`scikit-learn`中的`train_test_split`函数进行随机划分，并保留每个类别的样本比例：
- en: '[PRE21]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: It is a good practice to assign a fixed `random_state` (for example, `42`) during
    experiments and exploration in order to guarantee that the same training and testing
    sets are generated every time the program runs. This allows us to make sure that
    the classifier functions and performs well on a fixed dataset before we incorporate
    randomness and proceed further.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验和探索过程中，指定一个固定的`random_state`（例如，`42`）是一个好的做法，以确保每次程序运行时生成相同的训练集和测试集。这可以确保我们在引入随机性并进一步进行时，分类器能够在固定的数据集上正常运行并表现良好。
- en: 'We check the training and testing sizes as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查训练集和测试集的大小如下：
- en: '[PRE22]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Another good thing about the `train_test_split` function is that the resulting
    training and testing sets will have the same class ratio.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_test_split`函数的另一个优点是，结果中的训练集和测试集将具有相同的类别比例。'
- en: Training a Naïve Bayes model
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练朴素贝叶斯模型
- en: 'Next, we train a Naïve Bayes model on the training set. You may notice that
    the values of the input features are from 0 to 5, as opposed to 0 or 1 in our
    toy example. Hence, we use the `MultinomialNB` module ([https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html))
    from scikit-learn instead of the `BernoulliNB` module, as `MultinomialNB` can
    work with integer features as well as fractional counts. We import the module,
    initialize a model with a smoothing factor of `1.0` and `prior` learned from the
    training set, and train this model against the training set as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在训练集上训练一个朴素贝叶斯模型。你可能会注意到输入特征的值从0到5，而不是我们玩具示例中的0或1。因此，我们使用来自scikit-learn的`MultinomialNB`模块（[https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)），而不是`BernoulliNB`模块，因为`MultinomialNB`可以处理整数特征以及分数计数。我们导入该模块，初始化一个平滑因子为`1.0`、`prior`从训练集学习得到的模型，并根据训练集训练该模型，如下所示：
- en: '[PRE23]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then, we use the trained model to make predictions on the testing set. We get
    the predicted probabilities as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用训练好的模型对测试集进行预测。我们得到预测概率如下：
- en: '[PRE24]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: For each testing sample, we output the probability of class 0, followed by the
    probability of class 1.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个测试样本，我们输出类别0的概率，接着是类别1的概率。
- en: 'We get the predicted class for the test set as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到测试集的预测类别如下：
- en: '[PRE25]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, we evaluate the model’s performance with classification accuracy,
    which is the proportion of correct predictions:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过分类准确度来评估模型的性能，准确度是正确预测的比例：
- en: '[PRE26]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The classification accuracy is around 72%, which means that the Naïve Bayes
    classifier we’ve constructed accurately suggests movies to users about three quarters
    of the time. Ideally, we could also utilize movie genre information from the `movies.dat`
    file, and user demographics (gender, age, occupation, and ZIP code) information
    from the `users.dat` file. Obviously, movies in similar genres tend to attract
    similar users, and users of similar demographics likely have similar movie preferences.
    We will leave it as an exercise for you to explore further.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 分类准确率约为72%，这意味着我们构建的朴素贝叶斯分类器能大约三分之二的时间正确地为用户推荐电影。理想情况下，我们还可以利用`movies.dat`文件中的电影类型信息，以及`users.dat`文件中的用户人口统计信息（性别、年龄、职业和邮政编码）。显然，相似类型的电影通常会吸引相似的用户，而相似人口统计特征的用户可能有相似的电影偏好。我们将把这部分留给你作为练习，进一步探索。
- en: So far, we have covered in depth the first machine learning classifier and evaluated
    its performance by prediction accuracy. Are there any other classification metrics?
    Let’s see in the next section.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经深入介绍了第一个机器学习分类器，并通过预测准确度评估了其性能。还有其他分类指标吗？让我们在下一节看看。
- en: Evaluating classification performance
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估分类性能
- en: 'Beyond accuracy, there are several metrics we can use to gain more insight
    and avoid class imbalance effects. These are as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 除了准确率外，还有几个评估指标可以帮助我们深入了解分类性能，避免类别不平衡的影响。具体如下：
- en: Confusion matrix
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: Precision
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确度
- en: Recall
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 召回率
- en: F1 score
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F1得分
- en: The area under the curve
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线下面积
- en: 'A **confusion matrix** summarizes testing instances by their predicted values
    and true values, presented as a contingency table:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**混淆矩阵**通过预测值和真实值总结测试实例，并以列联表的形式呈现：'
- en: '![A picture containing text, screenshot, font, number  Description automatically
    generated](img/B21047_02_08.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text, screenshot, font, number  Description automatically
    generated](img/B21047_02_08.png)'
- en: 'Figure 2.8: Contingency table for a confusion matrix'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8：混淆矩阵的列联表
- en: 'To illustrate this, we can compute the confusion matrix of our Naïve Bayes
    classifier. We use the `confusion_matrix` function from `scikit-learn` to compute
    it, but it is very easy to code it ourselves:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我们可以计算我们朴素贝叶斯分类器的混淆矩阵。我们使用`scikit-learn`中的`confusion_matrix`函数来计算，但也可以很容易自己编写代码：
- en: '[PRE27]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As you can see from the resulting confusion matrix, there are 47 false positive
    cases (where the model misinterprets a dislike as a like for a movie), and 148
    false negative cases (where it fails to detect a like for a movie). Hence, classification
    accuracy is just the proportion of all true cases:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果的混淆矩阵可以看出，有47个假阳性案例（模型错误地将不喜欢的电影判定为喜欢），和148个假阴性案例（模型未能检测到喜欢的电影）。因此，分类准确度仅是所有真实案例的比例：
- en: '![](img/B21047_02_030.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_030.png)'
- en: '**Precision** measures the fraction of positive calls that are correct, which
    are the following, in our case:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**精确度**衡量的是正确的正类预测所占的比例，在我们的案例中如下：'
- en: '![](img/B21047_02_031.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_031.png)'
- en: '**Recall**, on the other hand, measures the fraction of true positives that
    are correctly identified, which are the following in our case:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '**召回率**，另一方面，衡量的是被正确识别的真实正例的比例，在我们的案例中如下：'
- en: '![](img/B21047_02_032.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_032.png)'
- en: Recall is also called the **true positive rate**.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率也称为**真正率**。
- en: 'The **f1 score** comprehensively includes both the precision and the recall
    and equates to their **harmonic mean**:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**F1得分**全面包含了精确度和召回率，并等同于它们的**调和平均数**：'
- en: '![](img/B21047_02_033.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_033.png)'
- en: We tend to value the **f1** score above precision or recall alone.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常更看重**F1得分**，而非单独的精确度或召回率。
- en: 'Let’s compute these three measurements using corresponding functions from `scikit-learn`,
    as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`scikit-learn`中的相应函数计算这三个指标，具体如下：
- en: '[PRE28]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'On the other hand, the negative (dislike) class can also be viewed as positive,
    depending on the context. For example, assign the `0` class as `pos_label` and
    we have the following:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，负类（不喜欢）也可以根据上下文视为正类。例如，将`0`类分配为`pos_label`，我们将得到以下结果：
- en: '[PRE29]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To obtain the precision, recall, and f1 score for each class, instead of exhausting
    all class labels in the three function calls as shown earlier, a quicker way is
    to call the `classification_report` function:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取每个类别的精确率、召回率和f1得分，我们可以不需要像之前那样耗费精力调用三个函数处理所有类别标签，快速的方法是调用`classification_report`函数：
- en: '[PRE30]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here, `weighted avg` is the weighted average according to the proportions of
    the class.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`weighted avg`是根据各类别比例计算的加权平均值。
- en: The classification report provides a comprehensive view of how the classifier
    performs on each class. It is, as a result, useful in imbalanced classification,
    where we can easily obtain high accuracy by simply classifying every sample as
    the dominant class, while the precision, recall, and f1 score measurements for
    the minority class, however, will be significantly low.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 分类报告提供了分类器在每个类别上的表现的全面视图。因此，它在类别不平衡的分类任务中非常有用，因为在这种情况下，我们可以通过将每个样本分类为占主导地位的类别来轻松获得高准确率，但少数类别的精确率、召回率和f1得分将显著较低。
- en: Precision, recall, and the f1 score are also applicable to **multiclass** classification,
    where we can simply treat a class we are interested in as a positive case, and
    any other classes as negative cases.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 精确率、召回率和f1得分同样适用于**多类**分类，在这种情况下，我们可以简单地将我们感兴趣的类别视为正类，其他类别视为负类。
- en: During the process of tweaking a binary classifier (that is, trying out different
    combinations of hyperparameters, for example, the smoothing factor in our Naïve
    Bayes classifier), it would be perfect if there was a set of parameters in which
    the highest averaged and class individual f1 scores are achieved at the same time.
    It is, however, usually not the case. Sometimes, a model has a higher average
    f1 score than another model, but a significantly low f1 score for a particular
    class; sometimes, two models have the same average f1 scores, but one has a higher
    f1 score for one class and a lower score for another class. In situations such
    as these, how can we judge which model works better? The **Area Under the Curve**
    (**AUC**) of the **Receiver Operating Characteristic** (**ROC**) is a consolidated
    measurement frequently used in binary classification.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整二分类器的过程中（即尝试不同的超参数组合，例如我们朴素贝叶斯分类器中的平滑因子），如果有一组参数能够在同时达到最高的平均f1得分和各类别的个别f1得分，那将是最理想的情况。然而，通常情况并非如此。有时，某个模型的平均f1得分高于另一个模型，但某个特定类别的f1得分却显著较低；有时，两个模型的平均f1得分相同，但一个模型在某个类别上的f1得分较高，而在另一个类别上的得分较低。在这种情况下，我们如何判断哪个模型效果更好呢？**曲线下面积**（**AUC**）是**接收器操作特性**（**ROC**）的一个综合指标，经常用于二分类任务。
- en: 'The ROC curve is a plot of the true positive rate versus the false positive
    rate at various probability thresholds, ranging from 0 to 1\. For a testing sample,
    if the probability of a positive class is greater than the threshold, then a positive
    class is assigned; otherwise, we use a negative class. To recap, the true positive
    rate is equivalent to recall, and the false positive rate is the fraction of negatives
    that are incorrectly identified as positive. Let’s code and exhibit the ROC curve
    (under thresholds of `0.0`, `0.1`, `0.2`, …, `1.0`) of our model:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线是将不同概率阈值下的真实正例率与假正例率进行比较的图形，阈值范围从0到1。对于一个测试样本，如果其正类的概率大于阈值，则分配为正类；否则，使用负类。总结一下，真实正例率等同于召回率，而假正例率是错误地将负类标识为正类的比例。让我们编写代码并展示我们模型在`0.0`、`0.1`、`0.2`、……、`1.0`等阈值下的ROC曲线：
- en: '[PRE31]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, let’s calculate the true and false positive rates for all threshold settings
    (remember, there are `516.0` positive testing samples and `1191` negative ones):'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们计算所有阈值设置下的真实正例率和假正例率（记住，正测试样本有`516.0`个，负测试样本有`1191`个）：
- en: '[PRE32]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, we can plot the ROC curve with `matplotlib`:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`matplotlib`绘制ROC曲线：
- en: '[PRE33]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Refer to *Figure 2.9* for the resulting ROC curve:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 参见*图2.9*中的ROC曲线结果：
- en: '![A picture containing text, line, plot, screenshot  Description automatically
    generated](img/B21047_02_09.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text, line, plot, screenshot  Description automatically
    generated](img/B21047_02_09.png)'
- en: 'Figure 2.9: ROC curve'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9：ROC曲线
- en: 'In the graph, the dashed line is the baseline representing random guessing,
    where the true positive rate increases linearly with the false positive rate;
    its AUC is 0.5\. The solid line is the ROC plot of our model, and its AUC is somewhat
    less than 1\. In a perfect case, the true positive samples have a probability
    of 1, so that the ROC starts at the point with 100% true positive and 0% false
    positive. The AUC of such a perfect curve is 1\. To compute the exact AUC of our
    model, we can resort to the `roc_auc_score` function of `scikit-learn`:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表中，虚线是基准线，表示随机猜测，其中真正例率与假正例率线性增加；其 AUC 为 0.5。实线是我们模型的 ROC 曲线，其 AUC 略低于 1。在理想情况下，真正例样本的概率为
    1，因此 ROC 曲线从 100% 的真正例和 0% 的假正例点开始。这样的完美曲线的 AUC 为 1。为了计算我们模型的准确 AUC，可以借助 `scikit-learn`
    的 `roc_auc_score` 函数：
- en: '[PRE34]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'What AUC value leads to the conclusion that a classifier is good? Unfortunately,
    there is no such “magic” number. We use the following rule of thumb as general
    guidelines: classification models achieving an AUC of `0.7` to `0.8` are considered
    acceptable, `0.8` to `0.9` are great, and anything above `0.9` are superb. Again,
    in our case, we are only using the very sparse movie rating data. Hence, an AUC
    of `0.69` is actually acceptable.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 什么 AUC 值表明分类器表现良好？不幸的是，并没有一个“魔法”数字。我们使用以下经验法则作为一般指导：AUC 在 `0.7` 到 `0.8` 之间的分类模型被认为是可接受的，`0.8`
    到 `0.9` 之间的是优秀的，任何高于 `0.9` 的则是卓越的。再次强调，在我们的案例中，我们只使用了非常稀疏的电影评分数据。因此，`0.69` 的 AUC
    实际上是可以接受的。
- en: You have learned several classification metrics, and we will explore how to
    measure them properly and how to fine-tune our models in the next section.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学习了几种分类指标，我们将在下一节探讨如何正确衡量它们以及如何微调我们的模型。
- en: Tuning models with cross-validation
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用交叉验证来调整模型
- en: Limiting the evaluation to a single fixed set may be misleading since it’s highly
    dependent on the specific data points chosen for that set. We can simply avoid
    adopting the classification results from one fixed testing set, which we did in
    experiments previously. Instead, we usually apply the **k-fold cross-validation**
    technique to assess how a model will generally perform in practice.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 将评估限制在一个固定的数据集上可能会产生误导，因为它很大程度上依赖于为该数据集选择的具体数据点。我们可以简单地避免采用来自一个固定测试集的分类结果，这也是我们之前实验中所做的。相反，我们通常采用
    **k** 折交叉验证技术来评估模型在实际中的整体表现。
- en: 'In the *k*-fold cross-validation setting, the original data is first randomly
    divided into *k* equal-sized subsets, in which class proportion is often preserved.
    Each of these *k* subsets is then successively retained as the testing set for
    evaluating the model. During each trial, the rest of the *k* -1 subsets (excluding
    the one-fold holdout) form the training set for driving the model. Finally, the
    average performance across all *k* trials is calculated to generate an overall
    result:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *k* 折交叉验证设置中，原始数据首先随机划分为 *k* 个大小相等的子集，并且通常保持类别比例。然后，这些 *k* 个子集依次作为测试集，用于评估模型。在每次试验中，其余的
    *k*-1 个子集（不包括保留的一折）构成训练集，用于训练模型。最后，计算所有 *k* 次试验的平均性能，以生成整体结果：
- en: '![](img/B21047_02_10.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_02_10.png)'
- en: 'Figure 2.10: Diagram of 3-fold cross-validation'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10：三折交叉验证示意图
- en: Statistically, the average performance of *k*-fold cross-validation is a better
    estimate of how a model performs in general. Given different sets of parameters
    pertaining to a machine learning model and/or data preprocessing algorithms, or
    even two or more different models, the goal of model tuning and/or model selection
    is to pick a set of parameters of a classifier so that the best average performance
    is achieved. With these concepts in mind, we can now start to tweak our Naïve
    Bayes classifier, incorporating cross-validation and the AUC of ROC measurements.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 从统计学角度看，*k* 折交叉验证的平均性能是评估模型整体表现的更好估计。给定与机器学习模型和/或数据预处理算法相关的不同参数集，甚至是两个或多个不同的模型，模型调优和/或模型选择的目标是选择一个分类器的参数集，以便获得最佳的平均性能。牢记这些概念后，我们可以开始调整我们的朴素贝叶斯分类器，结合交叉验证和
    ROC 曲线的 AUC 测量。
- en: In *k*-fold cross-validation, *k* is usually set at 3, 5, or 10\. If the training
    size is small, a large *k* (5 or 10) is recommended to ensure sufficient training
    samples in each fold. If the training size is large, a small value (such as 3
    or 4) works fine since a higher *k* will lead to an even higher computational
    cost of training on a large dataset.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在*k*折交叉验证中，通常将*k*设置为3、5或10。如果训练集较小，推荐使用较大的*k*（5或10），以确保每个折叠中有足够的训练样本。如果训练集较大，较小的值（如3或4）就足够了，因为较高的*k*会导致在大数据集上训练时计算成本过高。
- en: 'We will use the `split()` method from the `StratifiedKFold` class of `scikit-learn`
    to divide the data into chunks with preserved class distribution:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`scikit-learn`的`StratifiedKFold`类中的`split()`方法将数据分割成保持类分布的块：
- en: '[PRE35]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'After initializing a 5-fold generator, we choose to explore the following values
    for the following parameters:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化5折生成器后，我们选择探索以下参数的不同值：
- en: '`alpha`: This represents the smoothing factor, the initial value for each feature'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`：表示平滑因子，每个特征的初始值。'
- en: '`fit_prior`: This represents whether to use prior tailored to the training
    data'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fit_prior`：表示是否使用专门为训练数据调整的先验。'
- en: 'We start with the following options:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从以下选项开始：
- en: '[PRE36]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, for each fold generated by the `split()` method of the `k_fold` object,
    we repeat the process of classifier initialization, training, and prediction with
    one of the aforementioned combinations of parameters, and record the resulting
    AUCs:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于`k_fold`对象的`split()`方法生成的每个折叠，我们重复分类器初始化、训练和预测的过程，并使用上述参数组合之一，记录结果AUC：
- en: '[PRE37]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Finally, we present the results as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们展示如下结果：
- en: '[PRE38]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The (`2`, `False`) set enables the best averaged AUC, at `0.65823`.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: (`2`, `False`)的设置能够提供最佳的平均AUC，值为`0.65823`。
- en: 'Finally, we retrain the model with the best set of hyperparameters (`2`, `False`)
    and compute the AUC:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用最佳超参数组合（`2`, `False`）重新训练模型并计算AUC：
- en: '[PRE39]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: An AUC of `0.686` is achieved with the fine-tuned model. In general, tweaking
    model hyperparameters using cross-validation is one of the most effective ways
    to boost learning performance and reduce overfitting.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 经过微调的模型达到了`0.686`的AUC值。通常，使用交叉验证调整模型超参数是提高学习性能和减少过拟合的最有效方法之一。
- en: Summary
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about the fundamental concepts of machine learning
    classification, including types of classification, classification performance
    evaluation, cross-validation, and model tuning. You also learned about the simple,
    yet powerful, classifier, Naïve Bayes. We went in depth through the mechanics
    and implementations of Naïve Bayes with a couple of examples, the most important
    one being the movie recommendation project.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了机器学习分类的基本概念，包括分类类型、分类性能评估、交叉验证和模型调优。你还了解了简单却强大的分类器——朴素贝叶斯。我们深入探讨了朴素贝叶斯的机制和实现，并通过几个示例进行说明，其中最重要的一个是电影推荐项目。
- en: 'Binary classification using Naïve Bayes was the main talking point of this
    chapter. In the next chapter, we will solve ad click-through prediction using
    another binary classification algorithm: a **decision tree**.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯进行二分类是本章的主要讨论内容。在下一章中，我们将使用另一种二分类算法——**决策树**来解决广告点击率预测问题。
- en: Exercises
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: As mentioned earlier, we extracted user-movie relationships only from the movie
    rating data where most ratings are unknown. Can you also utilize data from the
    `movies.dat` and `users.dat` files?
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，我们仅从电影评分数据中提取了用户与电影的关系，其中大多数评分是未知的。你能否还利用`movies.dat`和`users.dat`文件中的数据？
- en: Practice makes perfect—another great project to deepen your understanding could
    be heart disease classification. The dataset can be downloaded directly from [https://archive.ics.uci.edu/ml/datasets/Heart+Disease](https://archive.ics.uci.edu/ml/datasets/Heart+Disease).
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 熟能生巧——另一个可以加深理解的好项目是心脏病分类。数据集可以直接从[https://archive.ics.uci.edu/ml/datasets/Heart+Disease](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)下载。
- en: Don’t forget to fine-tune the model you obtained from Exercise 2 using the techniques
    you learned in this chapter. What is the best AUC it achieves?
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 别忘了使用你在本章中学到的技术，微调从练习2中得到的模型。它能达到的最佳AUC是多少？
- en: References
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'To acknowledge the use of the MovieLens dataset in this chapter, I would like
    to cite the following paper:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认在本章中使用了MovieLens数据集，我想引用以下论文：
- en: 'F. Maxwell Harper and Joseph A. Konstan. 2015\. *The MovieLens Datasets: History
    and Context*. ACM **Transactions on Interactive Intelligent Systems** (**TiiS**)
    5, 4, Article 19 (December 2015), 19 pages. DOI: [http://dx.doi.org/10.1145/2827872](http://dx.doi.org/10.1145/2827872).'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: F. Maxwell Harper 和 Joseph A. Konstan. 2015\. *电影Lens数据集：历史与背景*. ACM **互动智能系统学报**
    (**TiiS**) 5, 4, 文章 19 (2015年12月)，19页。DOI：[http://dx.doi.org/10.1145/2827872](http://dx.doi.org/10.1145/2827872)。
- en: Join our book’s Discord space
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord讨论区
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord讨论区，与作者及其他读者进行交流：
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/yuxi](https://packt.link/yuxi)'
- en: '![](img/QR_Code187846872178698968.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code187846872178698968.png)'
