- en: 3\. Neighborhood Approaches and DBSCAN
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 邻域方法与DBSCAN
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we will see how neighborhood approaches to clustering work
    from start to end and implement the **Density-Based Spatial Clustering of Applications
    with Noise** (**DBSCAN**) algorithm from scratch by using packages. We will also
    identify the most suitable algorithm to solve your problem from k-means, hierarchical
    clustering, and DBSCAN. By the end of this chapter, we will see how the DBSCAN
    clustering approach will serve us best in the sphere of highly complex data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将展示基于邻域的聚类方法如何从头到尾工作，并通过使用软件包从零开始实现**基于密度的空间聚类算法（带噪声）**（**DBSCAN**）算法。我们还将从k-means、层次聚类和DBSCAN中识别出最适合解决您问题的算法。到本章结束时，我们将看到DBSCAN聚类方法如何在处理高度复杂数据时为我们提供最佳服务。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In previous chapters, we evaluated a number of different approaches to data
    clustering, including k-means and hierarchical clustering. While k-means is the
    simplest form of clustering, it is still extremely powerful in the right scenarios.
    In situations where k-means can't capture the complexity of the dataset, hierarchical
    clustering proves to be a strong alternative.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们评估了多种不同的数据聚类方法，包括k-means和层次聚类。虽然k-means是最简单的聚类形式，但在适当的场景中，它仍然非常强大。在k-means无法捕捉数据集复杂性的情况下，层次聚类证明是一个强有力的替代方法。
- en: One of the key challenges in unsupervised learning is that you will be presented
    with a collection of feature data but no complementary labels telling you what
    a target state will be. While you may not get a discrete view of what the target
    labels are, you can get some semblance of structure out of the data by clustering
    similar groups together and seeing what is similar within groups. The first approach
    we covered to achieve this goal of clustering similar data points is k-means.
    K-means clustering works best for simple data challenges where speed is paramount.
    Simply looking at the closest data point (cluster centroid) does not require a
    lot of computational overhead; however, there is also a greater challenge posed
    when it comes to higher-dimensional datasets. K-means clustering is also not ideal
    if you are unaware of the potential number of clusters you are looking for. An
    example we worked with in *Chapter 2*, *Hierarchical Clustering*, entailed looking
    at chemical profiles to determine which wines belonged together in a disorganized
    shipment. This exercise only worked well because we knew that three wine types
    were ordered; however, k-means would have been less successful if you had no idea
    regarding what the original order constituted.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习的关键挑战之一是，您会获得一组特征数据，但没有额外的标签告诉您目标状态是什么。虽然您可能无法明确知道目标标签是什么，但通过将相似的群体聚集在一起并观察群体内的相似性，您可以从数据中提取出一些结构。我们首先介绍的聚类相似数据点的方法是k-means。K-means聚类最适合处理那些速度至关重要的简单数据问题。只需查看最接近的数据点（聚类中心），就不需要太多计算开销；然而，当面对高维数据集时，也会面临更大的挑战。如果您不知道需要查找的聚类数目，k-means也不是理想选择。我们在*第2章*《*层次聚类*》中探讨的一个例子是，通过分析化学特征来确定哪些葡萄酒在一次杂乱的运输中属于同一类。之所以能够顺利进行，是因为我们知道有三种葡萄酒类型已被订购；然而，如果您不知道原始顺序是什么，k-means可能就不会那么成功。
- en: The second clustering approach we explored was hierarchical clustering. This
    method can work in two ways – either agglomerative or divisive. Agglomerative
    clustering works with a bottom-up approach, treating each data point as its own
    cluster and recursively grouping them together with linkage criteria. Divisive
    clustering works in the opposite way by treating all data points as one large
    class and recursively breaking them down into smaller clusters. This approach
    has the benefit of fully understanding the entire data distribution, as it calculates
    splitting potential; however, it is typically not implemented in practice due
    to its greater complexity. Hierarchical clustering is a strong contender for your
    clustering needs when it comes to not knowing anything about the data. Using a
    dendrogram, you can visualize all the splits in your data and consider what number
    of clusters makes sense after the fact. This can be really helpful in your specific
    use case; however, it also comes at a higher computational cost than is associated
    with k-means.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索的第二种聚类方法是层次聚类。该方法有两种工作方式——聚合性（agglomerative）或分裂性（divisive）。聚合性聚类采用自下而上的方法，将每个数据点视为自己的簇，并根据链接标准递归地将它们组合在一起。分裂性聚类则采用相反的方式，将所有数据点视为一个大类，并递归地将它们分解成更小的簇。该方法的优点在于能够全面理解数据的分布，因为它计算了分割潜力；然而，由于其复杂性较高，通常在实践中不被采用。层次聚类对于当你对数据一无所知时是一个很有竞争力的选择。通过使用树状图（dendrogram），你可以可视化数据中的所有分割，并考虑事后哪个聚类数最合理。这在特定的使用场景中非常有用，但它的计算成本高于k-means。
- en: 'In this chapter, we will cover a clustering approach that will serve us best
    in the sphere of highly complex data: **Density-Based Spatial Clustering of Applications
    with Noise** (**DBSCAN**). Canonically, this method has always been seen as a
    high performer in datasets that have a lot of densely interspersed data. Let''s
    walk through why it does so well in these use cases.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一种在处理高度复杂数据时最适合的聚类方法：**基于密度的空间聚类应用与噪声（DBSCAN）**。传统上，这种方法在具有大量密集交错数据的数据库中一直被认为是一种高效的聚类方法。让我们一起看看它为什么在这些应用场景中表现得如此出色。
- en: Clusters as Neighborhoods
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作为邻域的簇
- en: 'Until now, we have explored the concept of likeness being described as a function
    of Euclidean distance – data points that are closer to any one point can be seen
    as similar, while those that are further away in Euclidean space can be seen as
    dissimilar. This notion is seen once again in the DBSCAN algorithm. As alluded
    to by the lengthy name, the DBSCAN approach expands upon basic distance metric
    evaluation by also incorporating the notion of density. If there are clumps of
    data points that all exist in the same area as one another, they can be seen as
    members of the same cluster:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们探讨的相似性概念是通过欧几里得距离来描述的——距离某一点较近的数据点可以看作是相似的，而在欧几里得空间中距离较远的数据点则被视为不相似。这个概念在DBSCAN算法中再次出现。如同其冗长的名字所暗示的那样，DBSCAN方法不仅基于距离度量评估，还引入了密度的概念。如果有一些数据点聚集在同一区域内，它们可以视为同一簇的成员：
- en: '![Figure 3.1: Neighbors have a direct connection to clusters](img/B15923_03_01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1：邻居与簇之间有直接联系](img/B15923_03_01.jpg)'
- en: 'Figure 3.1: Neighbors have a direct connection to clusters'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：邻居与簇之间有直接联系
- en: In the preceding figure, we can see four neighborhoods. The density-based approach
    has a number of benefits when compared to the past approaches we've covered that
    focus exclusively on distance. If you were just focusing on distance as a clustering
    threshold, then you may find your clustering makes little sense if faced with
    a sparse feature space with outliers. Both k-means and hierarchical clustering
    will automatically group together all data points in the space until no points
    are left.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到四个邻域。与我们之前讨论的仅仅基于距离的方法相比，基于密度的方法有很多优势。如果你只关注距离作为聚类的阈值，那么在面对稀疏特征空间和离群点时，你可能会发现聚类结果毫无意义。无论是k-means还是层次聚类，都会自动将空间中的所有数据点分组，直到没有剩余的点。
- en: 'While hierarchical clustering does provide a path around this issue somewhat,
    since you can dictate where clusters are formed using a dendrogram post-clustering
    run, k-means is the most susceptible to failure as it is the simplest approach
    to clustering. These pitfalls are less evident when we begin evaluating neighborhood
    approaches to clustering. In the following dendrogram, you can see an example
    of the pitfall where all data points are grouped together. Clearly, as you travel
    down the dendrogram, there is a lot of potential variation that gets grouped together
    since every point needs to be a member of a cluster. This is less of an issue
    with neighborhood-based clustering:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管层次聚类在一定程度上提供了解决此问题的途径，因为你可以在聚类运行后使用树状图来确定聚类的形成位置，但k-means作为聚类的最简单方法，最容易失败。当我们开始评估基于邻域的聚类方法时，这些陷阱就不那么明显了。在下面的树状图中，你可以看到一个典型的陷阱，其中所有数据点都被归为一类。显然，当你沿着树状图向下走时，许多具有潜在差异的数据点被聚在一起，因为每个点都必须属于一个聚类。使用基于邻域的聚类时，这个问题要少得多：
- en: '![Figure 3.2: Example dendrogram](img/B15923_03_02.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2：示例树状图](img/B15923_03_02.jpg)'
- en: 'Figure 3.2: Example dendrogram'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：示例树状图
- en: By incorporating the notion of neighbor density in DBSCAN, we can leave outliers
    out of clusters if we choose to, based on the hyperparameters we choose at runtime.
    Only the data points that have close neighbors will be seen as members within
    the same cluster, and those that are farther away can be left as unclustered outliers.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在DBSCAN中引入邻居密度的概念，我们可以选择性地将异常值排除在聚类之外，具体取决于我们在运行时选择的超参数。只有那些具有相近邻居的数据点才会被视为同一聚类中的成员，而那些距离较远的点则会被视为未聚类的异常值。
- en: Introduction to DBSCAN
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DBSCAN简介
- en: In DBSCAN, density is evaluated as a combination of neighborhood radius and
    minimum points found in a neighborhood deemed a cluster. This concept can be driven
    home if we reconsider the scenario where you are tasked with organizing an unlabeled
    shipment of wine for your store. In the previous example, it was made clear that
    we can find similar wines based on their features, such as chemical traits. Knowing
    this information, we can more easily group together similar wines and efficiently
    have our products organized for sale in no time. In the real world, however, the
    products that you order to stock your store will reflect real-world purchase patterns.
    To promote variety in your inventory, but still have sufficient stock of the most
    popular wines, there is a highly uneven distribution of product types that you
    have available. Most people love the classic wines, such as white and red; however,
    you may still carry more exotic wines for your customers who love expensive varieties.
    This makes clustering more difficult, since there are uneven class distributions
    (you don't order 10 bottles of every wine available, for example).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在DBSCAN中，密度是通过邻域半径和在被认为是一个聚类的邻域内找到的最小点数的组合来评估的。如果我们重新考虑一下你负责为商店整理一批未标记的葡萄酒货物的场景，这个概念就会变得更清晰。在前面的例子中，已经明确了我们可以根据葡萄酒的特征，如化学特性，来找到相似的葡萄酒。知道这些信息后，我们可以更容易地将相似的葡萄酒归为一类，迅速地将我们的产品整理好，准备好销售。然而，在现实世界中，你为商店订购的产品将反映出真实的购买模式。为了在库存中促使多样性，同时保证最受欢迎的葡萄酒有足够的存货，你的产品种类会呈现出极为不均衡的分布。大多数人喜欢经典的葡萄酒，如白葡萄酒和红葡萄酒；然而，你可能还会为喜欢昂贵酒款的客户提供一些更为独特的葡萄酒。这使得聚类变得更加困难，因为不同类别的分布不均（例如，你不会为每种葡萄酒都订购10瓶）。
- en: DBSCAN differs from k-means and hierarchical clustering because you can build
    this intuition into how we evaluate the clusters of customers we are interested
    in forming. It can cut through the noise in an easier fashion and only point out
    customers who have the highest potential for remarketing in a campaign.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN与k-means和层次聚类的不同之处在于，你可以将这种直觉融入到我们评估所关注的客户聚类中。它能够更轻松地去除噪声，仅指向那些在营销活动中具有最高再营销潜力的客户。
- en: By clustering through the concept of a neighborhood, we can separate out the
    one-off customers who can be seen as random noise, relative to the more valuable
    customers who come back to our store time and time again. This approach calls
    into question how we establish the best numbers when it comes to neighborhood
    radius and minimum points per neighborhood.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用邻域的概念进行聚类，我们可以将那些一次性光顾的客户（可以看作是随机噪声）与那些反复光顾我们商店的更有价值的客户区分开来。这种方法让我们重新思考在确定邻域半径和每个邻域的最小点数时，如何建立最佳数值。
- en: As a high-level heuristic, we want our neighborhood radius to be small, but
    not too small. At one end of the extreme, you can have the neighborhood radius
    quite high – this can max out at treating all points in the feature space as one
    massive cluster. At the opposite end of the extreme, you can have a very small
    neighborhood radius. Overly small neighborhood radii can result in no points being
    clustered together and having a large collection of single-member clusters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种高级启发式方法，我们希望邻域半径较小，但又不能过小。在一个极端情况下，邻域半径可以非常大——这可能会将特征空间中的所有点都视为一个庞大的簇。在另一个极端情况下，邻域半径可以非常小。过小的邻域半径可能导致没有点被聚类在一起，从而产生大量单一成员的簇。
- en: Similar logic applies when it comes to the minimum number of points that can
    make up a cluster. Minimum points can be seen as a secondary threshold that tunes
    the neighborhood radius a bit, depending on what data you have available in your
    space. If all of the data in your feature space is extremely sparse, minimum points
    become extremely valuable, in tandem with the neighborhood radius, to make sure
    you don't just have a large number of uncorrelated data points. When you have
    very dense data, the minimum points threshold becomes less of a driving factor
    than neighborhood radius.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的逻辑适用于构成簇的最小点数。最小点数可以看作是一个次要阈值，它根据你的数据空间中的数据来调整邻域半径。如果特征空间中的所有数据都非常稀疏，最小点数就变得非常重要，与邻域半径一起工作，确保你不会只得到大量不相关的数据点。当你拥有非常密集的数据时，最小点数阈值就不像邻域半径那样成为一个主导因素。
- en: As you can see from these two hyperparameter rules, the best options are, as
    usual, dependent on what your dataset looks like. Oftentimes, you will want to
    find the perfect "goldilocks" zone of not being too small in your hyperparameters,
    but also not too large.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从这两个超参数规则中看到的，最佳选项通常依赖于数据集的具体情况。通常，你会希望找到一个完美的“黄金中介”区域，既不过小，也不过大。
- en: DBSCAN in Detail
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DBSCAN 详细介绍
- en: 'To see how DBSCAN works, we can trace the path of a simple toy program as it
    merges together to form a variety of clusters and noise-labeled data points:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解 DBSCAN 如何工作，我们可以通过一个简单的示例程序的路径，查看它如何将数据点合并成不同的簇和噪声标签的数据点：
- en: Out of *n* unvisited sample data points, we'll first move through each point
    in a loop and mark each one as visited.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *n* 个未访问的样本数据点中，我们首先通过每个点并标记它为已访问。
- en: From each point, we'll look at the distance to every other point in the dataset.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从每个点出发，我们将查看到数据集中每个其他点的距离。
- en: All points that fall within the neighborhood radius hyperparameter should be
    considered as neighbors.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有位于邻域半径超参数范围内的点都应视为邻居。
- en: The number of neighbors should be at least as many as the minimum points required.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 邻居的数量应至少与所需的最小点数相等。
- en: If the minimum point threshold is reached, the points should be grouped together
    as a cluster, or else marked as noise.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果达到了最小点阈值，点应被归为一个簇，否则标记为噪声。
- en: This process should be repeated until all data points are categorized in clusters
    or as noise.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个过程应该重复进行，直到所有数据点都被分类为簇或噪声。
- en: DBSCAN is fairly straightforward in some senses – while there are the new concepts
    of density through neighborhood radius and minimum points, at its core, it is
    still just evaluating using a distance metric.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些方面，DBSCAN 相当简单——尽管有邻域半径和最小点的新概念，但本质上，它仍然只是使用距离度量来进行评估。
- en: Walkthrough of the DBSCAN Algorithm
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DBSCAN 算法演示
- en: 'The following steps will walk you through this path in slightly more detail:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将更详细地引导你完成此路径：
- en: 'Given six sample data points, view each point as its own cluster [ (1,3) ],
    [ (-8,6) ], [ (-6,4) ] , [ (4,-2) ], ] (2,5) ], [ (-2,0) ]:![Figure 3.3: Plot
    of sample data points](img/B15923_03_03.jpg)'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '给定六个样本数据点，将每个点视为其自己的簇：[ (1,3) ]， [ (-8,6) ]， [ (-6,4) ]， [ (4,-2) ]， [ (2,5)
    ]， [ (-2,0) ]: ![图 3.3：样本数据点的绘图](img/B15923_03_03.jpg)'
- en: 'Figure 3.3: Plot of sample data points'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.3：样本数据点的绘图
- en: 'Calculate the pairwise Euclidean distance between each of the points:![Figure
    3.4: Point distances](img/B15923_03_04.jpg)'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每两个点之间的欧几里得距离：![图 3.4：点之间的距离](img/B15923_03_04.jpg)
- en: 'Figure 3.4: Point distances'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.4：点之间的距离
- en: From each point, expand a neighborhood size outward and form clusters. For the
    purpose of this example, imagine you pass through a neighborhood radius of five.
    This means that any two points will be neighbors if the distance between them
    is less than five units. For example, point (1,3) has points (2,5) and (-2,0)
    as neighbors.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从每个点出发，向外扩展邻域大小并形成簇。为了方便理解，假设你通过一个半径为五的邻域。这意味着，如果两点之间的距离小于五个单位，它们就会成为邻居。例如，点
    (1,3) 有点 (2,5) 和 (-2,0) 作为邻居。
- en: 'Depending on the number of points in the neighborhood of a given point, the
    point can be classified into the following three categories:'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据给定点邻域中点的数量，该点可以被分类为以下三类：
- en: '**Core Point**: If the point under observation has data points greater than
    the minimum number of points in its neighborhood that make up a cluster, then
    that point is called a core point of the cluster. All core points within the neighborhood
    of other core points are part of the same cluster. However, all the core points
    that are not in same neighborhood are part of another cluster.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**核心点**：如果观察点在其邻域中拥有比最低点数要求更多的数据点，这些点构成了一个簇，则该点被称为该簇的核心点。所有核心点所在的邻域中的其他核心点都属于同一簇。然而，所有不在同一邻域内的核心点属于另一个簇。'
- en: '**Boundary Point**: If the point under observation does not have sufficient
    neighbors (data points) of its own, but it has at least one core point (in its
    neighborhood), then that point represents the boundary point of the cluster. Boundary
    points belong to the same cluster of their nearest core point.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**边界点**：如果观察点周围没有足够的邻居（数据点），但至少有一个核心点（在其邻域内），则该点表示该簇的边界点。边界点属于其最近核心点的同一簇。'
- en: '**Noise Point**: A data point is treated as a noise point if it does not have
    the required minimum number of data points in its neighborhood and is not associated
    with a core point. This point is treated as pure noise and is excluded from clustering.'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**噪声点**：如果数据点在其邻域内没有满足要求的最小数据点数量，并且与任何核心点无关，则该点被视为噪声点，并从聚类中排除。'
- en: Points that have neighbors are then evaluated to see whether they pass the minimum
    points threshold. In this example, if we had passed through a minimum points threshold
    of two, then points (1,3), (2,5), and (-2,0) could formally be grouped together
    as a cluster. If we had a minimum points threshold of four, then these three data
    points would be considered superfluous noise.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有邻居的点将进行评估，以查看它们是否通过最小点数的阈值。在这个例子中，如果我们通过了最小点数阈值为二，那么点 (1,3)，(2,5)，和 (-2,0)
    可以正式归为同一簇。如果我们通过最小点数阈值为四，那么这三个数据点将被视为多余的噪声。
- en: Points that have fewer neighbors than the minimum number of neighboring points
    required and whose neighborhood does not contain a core point are marked as noise
    and remain unclustered. Thus, points (-6,4), (4,-2), and (-8,6) fall under this
    category. However, points such as (2,5) and (2,0), though don't satisfy the criteria
    of the minimum number of points in neighborhood, do contain a core point as their
    neighbor, and are therefore marked as boundary points.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有比邻域内所需最小邻居数量更少邻居，并且其邻域不包含核心点的点被标记为噪声点，且不参与聚类。因此，点 (-6,4)，(4,-2)，和 (-8,6) 属于这一类别。然而，像
    (2,5) 和 (2,0) 这样的点，尽管它们不满足邻域内最小点数的标准，但它们确实包含一个核心点作为邻居，因此被标记为边界点。
- en: 'The following table summarizes the neighbors of a particular point and classifies
    them as core, boundary, and noise data points (mentioned in the preceding step)
    for a neighborhood radius of 5 and a minimum-neighbor criterion of 2.![Figure
    3.5: Table showing details of neighbors for given points](img/B15923_03_05.jpg)'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下表格总结了某个特定点的邻居，并将其分类为核心点、边界点和噪声数据点（如前述步骤所提到），邻域半径为 5，最小邻居标准为 2。![图 3.5：表格展示了给定点的邻居详细信息](img/B15923_03_05.jpg)
- en: 'Figure 3.5: Table showing details of neighbors for given points'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.5：表格展示了给定点的邻居详细信息
- en: Repeat this process on any remaining unvisited data points.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对任何剩余的未访问数据点重复此过程。
- en: At the end of this process, you will have sorted your entire dataset into either
    clusters or unrelated noise. DBSCAN performance is highly dependent on the threshold
    hyperparameters you choose. This means that you may have to run DBSCAN a couple
    of times with different hyperparameter options to get an understanding of how
    they influence overall performance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在此过程结束时，你将把整个数据集划分为聚类或无关的噪声。DBSCAN 的性能在很大程度上取决于你选择的阈值超参数。这意味着你可能需要多次运行 DBSCAN，并使用不同的超参数选项，以了解它们如何影响整体性能。
- en: Note that DBSCAN does not require the centroids that we saw in both k-means
    and centroid-focused implementation of hierarchical clustering. This feature allows
    DBSCAN to work better for complex datasets, since most data is not shaped like
    clean blobs. DBSCAN is also more effective against outliers and noise than k-means
    or hierarchical clustering.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，DBSCAN 不需要我们在 k-means 和以质心为中心的层次聚类实现中看到的质心。这个特点使得 DBSCAN 对于复杂数据集更有效，因为大多数数据并不是像干净的块状物一样的形状。与
    k-means 或层次聚类相比，DBSCAN 在应对离群点和噪声方面也更有效。
- en: Let's now see how the performance of DBSCAN changes with varying neighborhood
    radius sizes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看随着邻域半径大小的变化，DBSCAN 的性能如何变化。
- en: 'Exercise 3.01: Evaluating the Impact of Neighborhood Radius Size'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '练习 3.01: 评估邻域半径大小的影响'
- en: For this exercise, we will work in reverse of what we have typically seen in
    previous examples by first seeing the packaged implementation of DBSCAN in scikit-learn,
    and then implementing it on our own. This is done on purpose to fully explore
    how different neighborhood radius sizes drastically impact DBSCAN performance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将颠倒我们通常在前面示例中看到的顺序，首先查看 scikit-learn 中的 DBSCAN 打包实现，然后再自己实现它。这是故意的，目的是充分探索不同邻域半径大小如何显著影响
    DBSCAN 的性能。
- en: 'By completing this exercise, you will become familiar with how tuning neighborhood
    radius size can change how well DBSCAN performs. It is important to understand
    these facets of DBSCAN, as they can save you time in the future by troubleshooting
    your clustering algorithms efficiently:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这个练习后，你将熟悉调整邻域半径大小如何改变 DBSCAN 的性能。了解 DBSCAN 的这些方面非常重要，因为它们可以通过高效排查聚类算法问题，为你节省未来的时间：
- en: 'Import the packages from scikit-learn and matplotlib that are necessary for
    this exercise:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 scikit-learn 和 matplotlib 中本次练习所需的包：
- en: '[PRE0]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Generate a random cluster dataset to experiment on; X = coordinate points,
    and y = cluster labels (not needed):'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个随机聚类数据集来进行实验；X = 坐标点，y = 聚类标签（不需要）：
- en: '[PRE1]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output is as follows:'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 3.6: Visualized toy data example](img/B15923_03_06.jpg)'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.6: 可视化的玩具数据示例](img/B15923_03_06.jpg)'
- en: 'Figure 3.6: Visualized toy data example'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 3.6: 可视化的玩具数据示例'
- en: 'After plotting the dummy data for this toy problem, you will see that the dataset
    has two features and approximately seven to eight clusters. To implement DBSCAN
    using scikit-learn, you will need to instantiate a new scikit-learn class:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在为这个玩具问题绘制虚拟数据之后，你将看到数据集有两个特征，大约七到八个聚类。要使用 scikit-learn 实现 DBSCAN，你需要实例化一个新的
    scikit-learn 类：
- en: '[PRE2]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Our example DBSCAN instance is stored in the `db` variable, and our hyperparameters
    are passed through on creation. For the sake of this example, you can see that
    the neighborhood radius (`eps`) is set to `0.5`, while the minimum number of points
    is set to `10`. To keep in line with our past chapters, we will once again be
    using Euclidean distance as our distance metric.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的示例 DBSCAN 实例存储在 `db` 变量中，并且我们的超参数在创建时传入。为了本示例，你可以看到邻域半径（`eps`）被设置为 `0.5`，而最小点数设置为
    `10`。为了与我们之前的章节保持一致，我们将再次使用欧几里得距离作为我们的距离度量。
- en: Note
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: '`eps` stands for epsilon and is the radius of the neighborhood that your algorithm
    will look within when searching for neighbors.'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`eps` 代表 epsilon，是你的算法在查找邻居时会搜索的邻域半径。'
- en: 'Let''s set up a loop that allows us to explore potential neighborhood radius
    size options interactively:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们设置一个循环，允许我们交互式地探索潜在的邻域半径大小选项：
- en: '[PRE3]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code results in the following plots:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生了以下图表：
- en: '![Figure 3.7: Resulting plots](img/B15923_03_07.jpg)'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.7: 结果图](img/B15923_03_07.jpg)'
- en: 'Figure 3.7: Resulting plots'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.7: 结果图'
- en: As you can see from the plots, setting our neighborhood size too small will
    cause everything to be seen as random noise (purple points). Bumping our neighborhood
    size up a little bit allows us to form clusters that make more sense. A larger
    epsilon value would again convert the entire dataset into a single cluster (purple
    data points). Try recreating the preceding plots and experiment with varying `eps`
    sizes.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看出，将邻域大小设置得过小会导致所有内容都被视为随机噪声（紫色点）。稍微增加邻域大小后，我们能够形成更有意义的簇。再大的 epsilon 值会将整个数据集合并为一个簇（紫色数据点）。尝试重新创建前面的图并实验不同的
    `eps` 大小。
- en: Note
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3gEijGC](https://packt.live/3gEijGC).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问这一部分的源代码，请参考 [https://packt.live/3gEijGC](https://packt.live/3gEijGC)。
- en: You can also run this example online at [https://packt.live/2ZPBfeJ](https://packt.live/2ZPBfeJ).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 [https://packt.live/2ZPBfeJ](https://packt.live/2ZPBfeJ) 在线运行这个例子。
- en: DBSCAN Attributes – Neighborhood Radius
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DBSCAN 属性 – 邻域半径
- en: In the preceding exercise, you saw how impactful setting the proper neighborhood
    radius is on the performance of your DBSCAN implementation. If your neighborhood
    is too small, then you will run into issues where all the data will be treated
    as noise and is left unclustered. If you set your neighborhood too large, then
    all of the data will similarly be grouped together into one cluster and not provide
    any value. If you explored the preceding exercise further with your own `eps`
    sizes, you may have noticed that it is very difficult to perform effective clustering
    using only the neighborhood size. This is where a minimum points threshold comes
    in handy. We will visit that topic later.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的练习中，你看到设置合适的邻域半径对 DBSCAN 实现性能的影响。如果你的邻域设置得太小，那么所有数据都会被当作噪声处理，无法聚类。如果将邻域设置得过大，所有数据也会被归为一个簇，无法提供任何价值。如果你用自己的
    `eps` 值进一步探索了前面的练习，你可能会注意到，单纯依靠邻域大小进行有效聚类是非常困难的。这时，最小点数阈值就派上了用场。我们稍后会讲到这个话题。
- en: 'To go deeper into the neighborhood concept of DBSCAN, let''s take a deeper
    look at the `eps` hyperparameter you pass at instantiation time. This epsilon
    value is converted to a radius that sweeps around any given data point in a circular
    manner to serve as a neighborhood:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地理解 DBSCAN 的邻域概念，我们来看一下在实例化时传入的 `eps` 超参数。这个 epsilon 值会被转换为一个半径，围绕任意给定的数据点以圆形方式进行扫描，作为邻域的定义：
- en: '![Figure 3.8: Visualization of the neighborhood radius; the red circle is the
    neighborhood](img/B15923_03_08.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.8：邻域半径的可视化；红色圆圈表示邻域](img/B15923_03_08.jpg)'
- en: 'Figure 3.8: Visualization of the neighborhood radius; the red circle is the
    neighborhood'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8：邻域半径的可视化；红色圆圈表示邻域
- en: In this instance, there will be four neighbors of the center point, as can be
    seen in the preceding plot.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，中心点会有四个邻居，正如前面的图中所示。
- en: 'One key aspect to observe here is that the shape formed by your neighborhood
    search is a circle in two dimensions, and a sphere in three dimensions. This may
    impact the performance of your model simply based on how the data is structured.
    Once again, blobs may seem like an intuitive structure to find – this may not
    always be the case. Fortunately, DBSCAN is well equipped to handle this dilemma
    of clusters that you may be interested in, but that do not fit the explicit blob
    structure:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要观察的一个关键点是，你的邻域搜索形成的形状在二维空间中是圆形的，在三维空间中是球形的。这可能会根据数据的结构影响你模型的性能。再一次，簇可能看起来像是直观的结构，但这不一定总是如此。幸运的是，DBSCAN
    很好地解决了这类簇的问题，虽然这些簇可能是你感兴趣的，但并不符合显式的簇结构：
- en: '![Figure 3.9: Impact of varying neighborhood radius size](img/B15923_03_09.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.9：邻域半径大小变化的影响](img/B15923_03_09.jpg)'
- en: 'Figure 3.9: Impact of varying neighborhood radius size'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9：邻域半径大小变化的影响
- en: On the left, the data point will be classified as random noise. On the right,
    the data point has multiple neighbors and could be its own cluster.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的数据点将被分类为随机噪声。右侧的数据点有多个邻居，可能会形成一个自己的簇。
- en: 'Activity 3.01: Implementing DBSCAN from Scratch'
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 3.01：从零开始实现 DBSCAN
- en: During an interview, you are asked to create the DBSCAN algorithm from scratch
    using a generated two-dimensional dataset. To do this, you will need to convert
    the theory behind neighborhood searching into production code, with a recursive
    call that adds neighbors. As explained in the previous section, you will use a
    distance scan in space surrounding a specified point to add these neighbors.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在面试中，你被要求使用生成的二维数据集从零开始创建 DBSCAN 算法。为此，你需要将邻域搜索的理论转化为生产代码，并通过递归调用来添加邻居。正如前一节所解释的，你将使用距离扫描，在指定点周围的空间内添加这些邻居。
- en: Given what you've learned about DBSCAN and distance metrics from prior chapters,
    build an implementation of DBSCAN from scratch in Python. You are free to use
    NumPy and SciPy to evaluate distances here.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你在前几章中学到的关于 DBSCAN 和距离度量的知识，从零开始用 Python 实现 DBSCAN。你可以自由使用 NumPy 和 SciPy 来评估距离。
- en: 'These steps will help you to complete the activity:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成此活动：
- en: Generate a random cluster dataset.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个随机聚类数据集。
- en: Visualize the data.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化数据。
- en: Create functions from scratch that allow you to call DBSCAN on a dataset.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从零开始创建函数，允许你在数据集上调用 DBSCAN。
- en: Use your created DBSCAN implementation to find clusters in the generated dataset.
    Feel free to use hyperparameters as you see fit, tuning them based on their performance.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你创建的 DBSCAN 实现来找到生成数据集中的聚类。根据需要调整超参数，依据其性能来调优。
- en: Visualize the clustering performance of your DBSCAN implementation from scratch.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化你从零开始实现的 DBSCAN 聚类性能。
- en: 'The desired outcome of this exercise is for you to implement how DBSCAN works
    from the ground up before you use the fully packaged implementation in scikit-learn.
    Taking this approach to any machine learning algorithm from scratch is important,
    as it helps you "earn" the ability to use easier implementations, while still
    being able to discuss DBSCAN in depth in the future:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习的目标是让你在使用 scikit-learn 中的完整实现之前，从零开始实现 DBSCAN 的工作原理。通过这种方式从零开始处理任何机器学习算法是非常重要的，它帮助你“赚取”使用更简便实现的能力，同时仍然能够在未来深入讨论
    DBSCAN：
- en: '![Figure 3.10: Expected outcome](img/B15923_03_10.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.10：预期结果](img/B15923_03_10.jpg)'
- en: 'Figure 3.10: Expected outcome'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10：预期结果
- en: Note
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 428.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解决方案可以在第 428 页找到。
- en: DBSCAN Attributes – Minimum Points
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DBSCAN 属性 – 最小点数
- en: The other core component to a successful implementation of DBSCAN beyond the
    neighborhood radius is the minimum number of points required to justify membership
    within a cluster. As mentioned earlier, it is more obvious that this lower bound
    benefits your algorithm when it comes to sparser datasets. That's not to say that
    it is a useless parameter when you have very dense data; however, while having
    single data points randomly interspersed through your feature space can be easily
    bucketed as noise, it becomes more of a gray area when we have random patches
    of two to three, for example. Should these data points be their own cluster, or
    should they also be categorized as noise? Minimum points thresholding helps to
    solve this problem.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 除了邻域半径外，成功实现 DBSCAN 的另一个核心组成部分是聚类内所需的最小数据点数，以证明数据点属于该聚类。如前所述，较低的阈值在稀疏数据集上对算法的优化非常明显。这并不意味着它在密集数据中没有用处；然而，虽然将单个随机分布的数据点轻松归类为噪声，但当我们有两个到三个点随机分布时，就变得模糊不清。例如，这些数据点应该是自己的聚类，还是也应该被分类为噪声？最小点数阈值有助于解决这个问题。
- en: 'In the scikit-learn implementation of DBSCAN, this hyperparameter is seen in
    the `min_samples` field passed on DBSCAN instance creation. This field is very
    valuable in tandem with the neighborhood radius size hyperparameter to fully round
    out your density-based clustering approach:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 实现的 DBSCAN 中，这个超参数出现在创建 DBSCAN 实例时传递的 `min_samples` 字段。这个字段与邻域半径大小超参数一起非常有价值，能够完善你的基于密度的聚类方法：
- en: '![Figure 3.11: Minimum points threshold deciding whether a group of data points
    is noise or a cluster](img/B15923_03_11.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.11：决定数据点是否为噪声或聚类的最小点数阈值](img/B15923_03_11.jpg)'
- en: 'Figure 3.11: Minimum points threshold deciding whether a group of data points
    is noise or a cluster'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11：决定数据点是否为噪声或聚类的最小点数阈值
- en: On the right, if the minimum points threshold is 10 points, it will classify
    data in this neighborhood as noise.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果最小点数阈值为 10 个点，则它将在该邻域内将数据归类为噪声。
- en: In real-world scenarios, you can see minimum points being highly impactful when
    you have truly large amounts of data. Going back to the wine-clustering example,
    if your store was actually a large wine warehouse, you could have thousands of
    individual wines with only one or two bottles that could easily be viewed as their
    own cluster. This may be helpful depending on your use case; however, it is important
    to keep in mind the subjective magnitudes that come with your data. If you have
    millions of data points, then random noise can easily be seen as hundreds or even
    thousands of random one-off sales. However, if your data is on the scale of hundreds
    or thousands, single data points can be seen as random noise.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，当你有大量数据时，最小点数会产生很大的影响。回到葡萄酒聚类的例子，如果你的商店实际上是一个大型葡萄酒仓库，你可能有成千上万瓶酒，其中只有一两瓶酒可以被视为一个单独的聚类。根据你的用例，这可能是有用的；然而，需要牢记的是，数据中的主观量级。如果你有数百万个数据点，随机噪声很容易被视为数百甚至数千个单独的销售记录。然而，如果你的数据规模是几百或几千个数据点，单个数据点也可能被视为随机噪声。
- en: 'Exercise 3.02: Evaluating the Impact of the Minimum Points Threshold'
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习3.02：评估最小点数阈值的影响
- en: Similar to *Exercise 3.01*, *Evaluating the Impact of Neighborhood Radius Size*,
    where we explored the value of setting a proper neighborhood radius size, we will
    repeat the exercise, but instead will change the minimum points threshold on a
    variety of datasets.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于*练习3.01*，*评估邻域半径大小的影响*，我们探讨了设置合适邻域半径大小的值，本次我们将重复该练习，但改为在各种数据集上改变最小点数阈值。
- en: Using our current implementation of DBSCAN, we can easily tune the minimum points
    threshold. Tune this hyperparameter and see how it performs on generated data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们当前的DBSCAN实现，我们可以轻松调整最小点数阈值。调整此超参数，并观察它在生成数据上的表现。
- en: By tuning the minimum points threshold for DBSCAN, you will understand how it
    can affect the quality of your clustering predictions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整DBSCAN的最小点数阈值，你将了解它如何影响你的聚类预测质量。
- en: 'Once again, let''s start with randomly generated data:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们从随机生成的数据开始：
- en: 'Generate a random cluster dataset, as follows:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个随机聚类数据集，如下所示：
- en: '[PRE4]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Visualize the data as follows:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，可视化数据：
- en: '[PRE5]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 3.12: Plot of the data generated'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.12：生成数据的图表'
- en: '](img/B15923_03_12.jpg)'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_03_12.jpg)'
- en: 'Figure 3.12: Plot of the data generated'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.12：生成数据的图表
- en: 'With the same plotted data as before, let''s grab one of the better-performing
    neighborhood radius sizes from *Exercise 3.01*, *Evaluating the Impact of Neighborhood
    Radius Size* – `eps = 0.7`:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用与之前相同的绘图数据，让我们选取*练习3.01*，*评估邻域半径大小的影响*中的一个表现更好的邻域半径大小——`eps = 0.7`：
- en: '[PRE6]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: '`eps` is a tunable hyperparameter. Earlier in *Step 3* of the previous exercise,
    we used a value of `0.5`. In this step, we are using `eps = 0.7` based on our
    experimentation with this parameter.'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`eps`是一个可调的超参数。在前一个练习的*步骤3*中，我们使用了`0.5`的值。在这一步，我们根据对该参数的实验使用了`eps = 0.7`。'
- en: 'After instantiating the DBSCAN clustering algorithm, let''s treat the `min_samples`
    hyperparameters as the variable we wish to tune. We can cycle through a loop to
    find which minimum number of points works best for our use case:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化DBSCAN聚类算法后，我们将`min_samples`超参数视为我们希望调整的变量。我们可以循环遍历，找到适合我们用例的最小点数：
- en: '[PRE7]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Looking at the first plot generated, we can see where we ended if you followed
    *Exercise 3.01*, *Evaluating the Impact of Neighborhood Radius Size* exactly,
    using 10 minimum points to mark the threshold for cluster membership:'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查看生成的第一个图表，我们可以看到如果你严格按照*练习3.01*，*评估邻域半径大小的影响*的要求，用10个最小点作为聚类成员资格的阈值，你会得到的结果：
- en: '![Figure 3.13: Plot of the toy problem with a minimum of 10 points](img/B15923_03_13.jpg)'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图3.13：具有最小10个点的玩具问题图](img/B15923_03_13.jpg)'
- en: 'Figure 3.13: Plot of the toy problem with a minimum of 10 points'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13：具有最小10个点的玩具问题图
- en: 'The remaining two hyperparameter options can be seen to greatly impact the
    performance of your DBSCAN clustering algorithm, and show how a shift in one number
    can greatly influence performance:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的两个超参数选项可以明显影响你的DBSCAN聚类算法的性能，并展示了数字的微小变化如何极大地影响性能：
- en: '![Figure 3.14: Plots of the toy problem](img/B15923_03_14.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图3.14：玩具问题的图表](img/B15923_03_14.jpg)'
- en: 'Figure 3.14: Plots of the toy problem'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14：玩具问题的图表
- en: As you can see, simply changing the number of minimum points from 19 to 20 adds
    an additional (incorrect!) cluster to our feature space. Given what you've learned
    about minimum points through this exercise, you can now tweak both epsilon and
    minimum points thresholding in your scikit-learn implementation to achieve the
    optimal number of clusters.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，仅仅将最小点数从19改为20就会在我们的特征空间中增加一个额外（不正确的！）的聚类。通过本练习学到的关于最小点数的知识，您现在可以调整
    scikit-learn 实现中的 epsilon 和最小点数阈值，以达到最佳聚类数。
- en: Note
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In our original generation of the data, we created eight clusters. These indicate
    that small changes in minimum points can add entire new clusters that we know
    shouldn't be there.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们原始生成的数据中，我们创建了八个聚类。这表明最小点数的微小变化可能会添加完全不应存在的新聚类。
- en: To access the source code for this specific section, please refer to [https://packt.live/3fa4L5F](https://packt.live/3fa4L5F).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅[https://packt.live/3fa4L5F](https://packt.live/3fa4L5F)。
- en: You can also run this example online at [https://packt.live/31XUeqi](https://packt.live/31XUeqi).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在[https://packt.live/31XUeqi](https://packt.live/31XUeqi)上在线运行此示例。
- en: 'Activity 3.02: Comparing DBSCAN with k-means and Hierarchical Clustering'
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 3.02：比较 DBSCAN 与 k-means 和层次聚类
- en: In the preceding chapter, we attempted to group different wines together using
    hierarchical clustering. Let's attempt this approach again with DBSCAN and see
    whether a neighborhood search fares any better. As a reminder, you are managing
    store inventory and have received a large shipment of wine, but the brand labels
    fell off the bottles during transit. Fortunately, your supplier provided you with
    the chemical readings for each bottle along with their respective serial numbers.
    Unfortunately, you aren't able to open each bottle of wine and taste test the
    difference – you must find a way to group the unlabeled bottles back together
    according to their chemical readings! You know from the order list that you ordered
    three different types of wine and are given only two wine attributes to group
    the wine types back together.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们尝试使用层次聚类将不同的葡萄酒分组在一起。让我们再次尝试使用 DBSCAN 这种方法，看看邻域搜索是否能取得更好的效果。作为提醒，您正在管理商店的库存，并收到了大量葡萄酒的运输过程中标签掉落的情况。幸运的是，您的供应商提供了每瓶酒的化学读数及其对应的序列号。不幸的是，您无法打开每瓶酒品尝其差异
    - 您必须找到一种方法将未贴标签的酒重新按照其化学读数分组！您从订单列表中知道，您订购了三种不同类型的葡萄酒，并且只给出了两种葡萄酒属性来将酒类重新分组。
- en: In the previous sections, we were able to see how k-means and hierarchical clustering
    performed on the wine dataset. In our best-case scenario, we were able to achieve
    a silhouette score of 0.59\. Using scikit-learn's implementation of DBSCAN, let's
    see whether we can get even better clustering.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们能够看到 k-means 和层次聚类在葡萄酒数据集上的表现。在我们的最佳情况下，我们能够实现0.59的轮廓分数。使用 scikit-learn
    的 DBSCAN 实现，让我们看看是否可以获得更好的聚类效果。
- en: 'These steps will help you to complete the activity:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤将帮助您完成活动：
- en: Import the necessary packages.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包。
- en: Load the wine dataset and check what the data looks like.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载葡萄酒数据集并检查数据的外观。
- en: Visualize the data.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化数据。
- en: Generate clusters using k-means, agglomerative clustering, and DSBSCAN.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 k-means、凝聚聚类和 DBSCAN 生成聚类。
- en: Evaluate a few different options for DSBSCAN hyperparameters and their effect
    on the silhouette score.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估几种不同的 DSBSCAN 超参数选项及其对轮廓分数的影响。
- en: Generate the final clusters based on the highest silhouette score.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于最高轮廓分数生成最终的聚类。
- en: Visualize clusters generated using each of the three methods.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化使用三种方法生成的聚类。
- en: Note
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'We have sourced this dataset from [https://archive.ics.uci.edu/ml/datasets/wine](https://archive.ics.uci.edu/ml/datasets/wine).
    [Citation: Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science].
    You can also access it at [https://packt.live/3bW8NME](https://packt.live/3bW8NME).'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '我们从[https://archive.ics.uci.edu/ml/datasets/wine](https://archive.ics.uci.edu/ml/datasets/wine)获取了此数据集。[引用：Dua,
    D. and Graff, C. (2019). UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。Irvine,
    CA: 加利福尼亚大学，信息与计算机科学学院]。您还可以在[https://packt.live/3bW8NME](https://packt.live/3bW8NME)上访问它。'
- en: By completing this activity, you will be recreating a full workflow of a clustering
    problem. You have already made yourself familiar with the data in *Chapter 2*,
    *Hierarchical Clustering*, and, by the end of this activity, you will have performed
    model selection to find the best model and hyperparameters for your dataset. You
    will have silhouette scores of the wine dataset for each type of clustering.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此活动后，你将重新构建一个完整的聚类问题工作流程。你已经在*第2章*、*层次聚类*中熟悉了数据，并且在本活动结束时，你将进行模型选择，找到适合你数据集的最佳模型和超参数。你将为每种聚类类型获得酒类数据集的轮廓系数。
- en: Note
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 431.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第431页找到。
- en: DBSCAN versus k-means and Hierarchical Clustering
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DBSCAN与k均值和层次聚类
- en: Now that you've reached an understanding of how DBSCAN is implemented and how
    many different hyperparameters you can tweak to drive performance, let's survey
    how it compares to the clustering methods we have covered previously – k-means
    clustering and hierarchical clustering.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了DBSCAN的实现方式，以及可以调整的多个超参数来驱动性能，让我们来看看它与我们之前介绍的聚类方法——k均值聚类和层次聚类——有何不同。
- en: You may have noticed in *Activity 3.02*, *Comparing DBSCAN with k-means and
    Hierarchical Clustering,* that DBSCAN can be a bit finicky when it comes to finding
    the optimal clusters via a silhouette score. This is a downside of the neighborhood
    approach – k-means and hierarchical clustering really excel when you have some
    idea regarding the number of clusters in your data. In most cases, this number
    is low enough that you can iteratively try a few different numbers and see how
    it performs. DBSCAN, instead, takes a more bottom-up approach by working with
    your hyperparameters and finding the clusters it views as important. In practice,
    it is helpful to consider DBSCAN when the first two options fail, simply because
    of the amount of tweaking needed to get it to work properly. That said, when your
    DBSCAN implementation is working correctly, it will often immensely outperform
    k-means and hierarchical clustering (in practice, this often happens with highly
    intertwined, yet still discrete, data, such as a feature space containing two
    half-moons).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在*活动3.02*、*比较DBSCAN与k均值和层次聚类*中注意到，DBSCAN在通过轮廓系数找到最优簇时可能有些挑剔。这是邻域方法的一个缺点——当你对数据中簇的数量有一些了解时，k均值和层次聚类的表现通常非常好。在大多数情况下，这个簇的数量较少，你可以通过多次尝试不同的数量来观察其表现。相比之下，DBSCAN采取了一种自下而上的方法，通过调整超参数并找到它认为重要的簇。在实际操作中，当前两种方法失败时，考虑使用DBSCAN会很有帮助，因为它需要较多的调整才能正常工作。尽管如此，当你的DBSCAN实现正常工作时，它通常会远远优于k均值和层次聚类（在实际中，这通常发生在高度交织但仍然离散的数据中，例如包含两个半月形的特征空间）。
- en: Compared to k-means and hierarchical clustering, DBSCAN can be seen as being
    potentially more efficient, since it only has to look at each data point once.
    Instead of multiple iterations of finding new centroids and evaluating where their
    nearest neighbors are, once a point has been assigned to a cluster in DBSCAN,
    it does not change cluster membership. The other key feature that DBSCAN and hierarchical
    clustering both share, in comparison with k-means, is not needing to explicitly
    pass a number of clusters expected at the time of creation. This can be extremely
    helpful when you have no external guidance on how to break your dataset down.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 与k均值和层次聚类相比，DBSCAN可能更高效，因为它只需要查看每个数据点一次。与多次迭代寻找新质心并评估其最近邻的过程不同，在DBSCAN中，一旦一个点被分配到某个簇，它的簇成员关系就不会再发生变化。DBSCAN与层次聚类相比，另一大关键特点是，它不需要在创建时显式地传递期望的簇数量，而k均值却需要。这一点在你没有外部指导如何拆分数据集时非常有帮助。
- en: Summary
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed hierarchical clustering and DBSCAN, and in what
    type of situations they are best employed. While hierarchical clustering can,
    in some respects, be seen as an extension of the nearest-neighbor approach seen
    in k-means, DBSCAN approaches the problem of finding neighbors by applying a notion
    of density. This can prove extremely beneficial when it comes to highly complex
    data that is intertwined in a complex fashion. While DBSCAN is very powerful,
    it is not infallible and can even be overkill, depending on what your original
    data looks like.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了层次聚类和DBSCAN，以及它们最适合应用的情况。虽然层次聚类在某些方面可以被看作是k-means中最近邻方法的扩展，但DBSCAN通过应用密度的概念来解决寻找邻居的问题。当数据非常复杂且交织在一起时，这种方法可能非常有用。虽然DBSCAN非常强大，但它并不是万无一失的，具体效果也取决于原始数据的表现，有时甚至可能显得过于复杂。
- en: Combined with k-means and hierarchical clustering, however, DBSCAN completes
    a strong toolbox when it comes to the unsupervised learning task of clustering
    your data. When faced with any problem in this space, it is worthwhile comparing
    the performance of each method and seeing which performs best.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，结合了k-means和层次聚类，DBSCAN在聚类任务中为无监督学习提供了一个强大的工具箱。在面对该领域的任何问题时，比较每种方法的表现并观察哪种方法效果最佳是非常值得的。
- en: 'With clustering explored, we will now move onto another key piece of rounding
    out your skills in unsupervised learning: dimensionality reduction. Through the
    smart reduction of dimensions, we can make clustering easier to understand and
    communicate to stakeholders. Dimensionality reduction is also key to creating
    all types of machine learning models in the most efficient manner possible. In
    the next chapter, we will dive deeper into topic models and see how the aspects
    of clustering learned in these chapters apply to NLP-type problems.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索了聚类后，我们将进入无监督学习中另一个关键技能：降维。通过智能地减少维度，我们可以使聚类更加易于理解，并能够与利益相关者进行沟通。降维对于以最有效的方式创建各种机器学习模型也至关重要。在下一章，我们将深入研究主题模型，并查看在这些章节中学习的聚类方面如何应用于自然语言处理（NLP）类型的问题。
