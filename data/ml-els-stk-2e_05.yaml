- en: '*Chapter 3*: Anomaly Detection'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第3章*: 异常检测'
- en: '**Anomaly detection** was the original capability of Elastic ML and is the
    most mature, stretching its roots back to the Prelert days (before the acquisition
    by Elastic in 2016). This technology is robust, easy to use, powerful, and broadly
    applicable to all kinds of use cases for time series data.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**异常检测**是Elastic ML的原始功能，也是最成熟的，其根源可以追溯到Prelert时代（在2016年被Elastic收购之前）。这项技术稳健、易于使用、功能强大，并且广泛适用于所有时间序列数据的使用案例。'
- en: This jam-packed chapter will focus on using Elastic ML to detect anomalies in
    the occurrence rates of documents/events, rare occurrences of things, and numerical
    values outside of expected normal operation. We will run through some simple but
    effective examples that will highlight both the efficacy of Elastic ML and its
    ease of use.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这本内容丰富的章节将专注于使用Elastic ML检测文档/事件发生率的异常、罕见事件以及超出预期正常操作的数值。我们将通过一些简单但有效的示例来展示Elastic
    ML的功效及其易用性。
- en: 'Specifically, we will cover the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将涵盖以下内容：
- en: Elastic ML job types
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elastic ML作业类型
- en: Dissecting the detector
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拆解检测器
- en: Detecting changes in event rates
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测事件率的变化
- en: Detecting changes in metric values
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测指标值的变化
- en: Understanding the advanced detector functions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解高级检测器功能
- en: Splitting analysis along categorical features
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沿着分类特征进行拆分分析
- en: Understanding temporal versus population analysis
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解时间序列分析与人口分析的区别
- en: Categorization analysis of unstructured messages
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非结构化消息的分类分析
- en: Managing Elastic ML via the API
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过API管理Elastic ML
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The information in this chapter is based on the Elastic Stack as it exists
    in v7.10\. As with all of the chapters, all the example code can be found on GitHub:
    [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition.](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition
    )'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的信息基于v7.10版本的Elastic Stack。与所有章节一样，所有示例代码都可以在GitHub上找到：[https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition.](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition
    )
- en: Elastic ML job types
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elastic ML作业类型
- en: 'When we start using the Elastic ML UI to configure anomaly detection jobs,
    we will see that there are five different job wizards that are shown:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始使用Elastic ML UI来配置异常检测作业时，我们会看到有五个不同的作业向导被展示：
- en: '![Figure 3.1 – The Create job UI showing different configuration wizards'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 3.1 – 创建作业UI显示不同的配置向导'
- en: '](img/B17040_03_001.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_03_001.jpg](img/B17040_03_001.jpg)'
- en: Figure 3.1 – The Create job UI showing different configuration wizards
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – 创建作业UI显示不同的配置向导
- en: The existence of these different configuration wizards implies that there are
    different "types" of jobs. In actuality, there is really only one job type—it
    is just that the anomaly detection job has many options, and many of these wizards
    make certain aspects of that configuration easier. Everything that you may desire
    to configure can be done via the **Advanced** wizard (or the API). In fact, when
    Elastic ML was first released as beta in v5.4, that was all that existed. Since
    then, the other wizards have been added for simplicity and usability in specific
    use cases.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同配置向导的存在意味着存在不同的“类型”的作业。实际上，实际上只有一个作业类型——只是异常检测作业有许多选项，而且许多向导使配置的某些方面更容易。你可以通过**高级**向导（或API）完成你可能希望配置的所有内容。实际上，当Elastic
    ML首次在v5.4版本中以beta版发布时，这就是所有存在的。从那时起，其他向导已经添加，以简化特定用例的可用性。
- en: An anomaly detection job has many configuration settings, but the two most important
    ones are the **analysis configuration** and the **datafeed**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测作业有许多配置设置，但其中最重要的两个是**分析配置**和**数据源**。
- en: The analysis configuration is the recipe for what anomalies the job will detect.
    It contains a detection configuration (called the **detector**) as well as a few
    other settings, such as the bucket span. The datafeed is the configuration of
    the query that will be executed by Elasticsearch to retrieve the data that is
    to be analyzed by the detector.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 分析配置是作业将检测的异常的配方。它包含一个检测配置（称为**检测器**）以及一些其他设置，例如桶跨度。数据源是Elasticsearch将要执行的查询配置，用于检索检测器将要分析的数据。
- en: 'With respect to the different job wizards, the following are true:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 关于不同的作业向导，以下说法是正确的：
- en: '*Jobs created by the Single metric wizard have only one detector*. Their datafeeds
    contain a query and aggregations, thus only sending summarized data to the ML
    algorithms. The aggregations are automatically created for you based upon your
    configuration parameters in the wizard. The job also makes use of a flag called
    `summary_count_field_name` (set with the value of `doc_count`) to signal that
    aggregated data (and not raw data from the source index) is to be expected.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单一指标向导创建的作业只有一个检测器。它们的数据源包含一个查询和聚合，因此只向机器学习算法发送汇总数据。聚合是自动为您根据向导中的配置参数创建的。作业还使用一个名为`summary_count_field_name`的标志（设置为`doc_count`的值）来表示预期的将是聚合数据（而不是来自源索引的原始数据）。
- en: '*Jobs created with the Multi-metric wizard can have one or more detectors*.
    The analysis can also be split along categorical fields by setting `partition_field_name`
    (described later in the chapter). Their datafeeds do not contain aggregations
    (because the ML code needs to see all documents for every possible instance of
    a field value and will aggregate it on its own), thus full Elasticsearch documents
    are passed to the ML algorithms.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多指标向导创建的作业可以有一个或多个检测器。分析还可以通过设置`partition_field_name`（在章节后面描述）来按分类字段拆分。它们的数据源不包含聚合（因为机器学习代码需要看到每个字段值的每个可能实例的所有文档，并将自行进行聚合），因此将完整的Elasticsearch文档传递给机器学习算法。
- en: '*Jobs created with the Population wizard can have one or more detectors*. The
    wizard also sets `over_field_name` (described later in the chapter), which signals
    that population analysis is to be used. The analysis can also be split along categorical
    fields by setting `by_field_name` (described later in the chapter). Their datafeeds
    do not contain aggregations, thus full Elasticsearch documents are passed to the
    ML algorithms.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用人口向导创建的作业可以有一个或多个检测器。向导还设置了`over_field_name`（在章节后面描述），表示将使用人口分析。分析还可以通过设置`by_field_name`（在章节后面描述）来按分类字段拆分。它们的数据源不包含聚合，因此将完整的Elasticsearch文档传递给机器学习算法。
- en: '*Jobs created with the Categorization wizard have only one detector*. The wizard
    also sets `categorization_field_name` (described later in the chapter), which
    signals that categorization analysis is to be used. Categorization analysis also
    sets `by_field_name` (described later in the chapter) to a value of `mlcategory`.
    The analysis can also be split along categorical fields by setting `partition_field_name`
    (described later in the chapter). Their datafeeds do not contain aggregations,
    thus full Elasticsearch documents are passed to the ML algorithms.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分类向导创建的作业只有一个检测器。向导还设置了`categorization_field_name`（在章节后面描述），表示将使用分类分析。分类分析还将`by_field_name`（在章节后面描述）设置为`mlcategory`的值。分析还可以通过设置`partition_field_name`（在章节后面描述）来按分类字段拆分。它们的数据源不包含聚合，因此将完整的Elasticsearch文档传递给机器学习算法。
- en: '*Jobs created with the Advanced wizard can leverage every option available*.
    The onus is on the user to know what they are doing and configure the job correctly.
    The UI does prevent the user from making most mistakes, however. An experienced
    user can exclusively use the Advanced wizard to create any anomaly detection job.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用高级向导创建的作业可以利用所有可用的选项。用户需要知道自己在做什么，并正确配置作业。然而，UI可以防止用户犯大多数错误。有经验的用户可以专门使用高级向导创建任何异常检测作业。
- en: The options around job creation might seem daunting given what was just described.
    But do not fret—once we have gotten familiar with the terminology and have walked
    through some examples, you will find that the job configurations are very sensible;
    as more experience is gained, the configuration of jobs will become second nature.
    Let's take the next step and break down the components of the detector.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 根据刚才描述的，作业创建的选项可能看起来令人畏惧。但不要担心——一旦我们熟悉了术语并走过了几个示例，你会发现作业配置是非常合理的；随着经验的积累，作业的配置将变得自然而然。让我们继续下一步，分解检测器的组件。
- en: Dissecting the detector
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解构检测器
- en: 'At the heart of the anomaly detection job are the analysis configuration and
    the detector. The detector has several key components to it:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测作业的核心是分析配置和检测器。检测器有几个关键组件：
- en: The **function**
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**函数**'
- en: The **field**
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字段**'
- en: The **partition field**
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区字段**'
- en: The **by field**
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**按字段**'
- en: The **over field**
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**覆盖字段**'
- en: We will go through each in turn to fully understand them all. Note that in the
    next few sections, however, we will often refer to the actual names of settings
    within the job configuration as if we were using the advanced job editor or the
    API. Although it is good to fully understand the nomenclature, as you progress
    through this chapter you will also notice that many of the details of the job
    configuration are abstracted away from the user or are given more "UI-friendly"
    labels than the real setting names.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐一介绍它们，以便完全理解它们。请注意，然而，在接下来的几节中，我们经常会引用作业配置中的实际设置名称，就像我们正在使用高级作业编辑器或API一样。尽管完全理解命名法是好的，但随着你通过本章，你也会注意到许多作业配置的细节都被抽象化了，或者比实际的设置名称有更多的“UI友好”标签。
- en: The function
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 函数
- en: 'The detector **function** describes how the data will be aggregated or measured
    within the analysis interval (bucket span). There are many functions, but they
    can be classified into the following categories:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 检测器**函数**描述了数据将在分析间隔（桶跨度）内如何聚合或测量。有许多函数，但它们可以被归类为以下几类：
- en: '![Figure 3.2 – Table of detector functions'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.2 – 检测器函数表'
- en: '](img/B17040_03_002.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_03_002.jpg)'
- en: Figure 3.2 – Table of detector functions
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – 检测器函数表
- en: Items marked with an asterisk (`*`) also have high/low one-sided variants (such
    as `low_distinct_count`) that allow the detection of anomalies in only one direction.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 带有星号（`*`）的项目也有高/低单侧变体（如`low_distinct_count`），这允许仅在一个方向上检测异常。
- en: The field
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字段
- en: 'Some functions in the detector require a field within the data to operate on.
    Take the following examples:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 检测器中的一些函数需要在数据中操作一个字段。以下是一些例子：
- en: '`max(bytes)`'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max(bytes)`'
- en: '`mean(products.price)`'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean(products.price)`'
- en: '`high_distinct_count(destination.port)`'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`high_distinct_count(destination.port)`'
- en: Therefore, the name of the field the function directly operates on is simply
    called `field_name`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该函数直接操作的字段名称简单地称为`field_name`。
- en: The partition field
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分配字段
- en: There are often cases in which the detection analysis needs to be split along
    a categorical field so the analysis can be done separately for all unique instances
    of that field. In this case, the `partition` field (the setting is called `partition_field_name`)
    defines the field to split on. For example, in e-commerce, you might want to see
    the average revenue per category (men's clothing, women's accessories, and so
    on). In this case, the `category` field would be the `partition` field. We will
    explore the splitting of the analysis later in this chapter.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 经常会有这样的情况，检测分析需要沿着一个分类字段进行拆分，以便对该字段的所有唯一实例分别进行分析。在这种情况下，`partition`字段（设置称为`partition_field_name`）定义了要拆分的字段。例如，在电子商务中，你可能想查看每个类别的平均收入（男士服装、女士配饰等）。在这种情况下，`category`字段将是`partition`字段。我们将在本章后面探讨分析拆分。
- en: The by field
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分区字段
- en: Similar to the `partition` field, the `by` field (the setting is called `by_field_name`)
    is another mechanism to split the analysis, but it behaves differently with respect
    to how the results are modeled and scored. Additionally, the `by` field is mandatory
    if `rare` or `freq_rare` is used. More details on the differences in using the
    `by` field for splitting versus using the `partition` field will be discussed
    later in the chapter.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与`partition`字段类似，`by`字段（设置称为`by_field_name`）是另一种拆分分析的手段，但在如何建模和评分结果方面表现不同。此外，如果使用`rare`或`freq_rare`，则`by`字段是强制性的。关于使用`by`字段进行拆分与使用`partition`字段进行拆分的差异的更多细节将在本章后面讨论。
- en: The over field
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超字段
- en: The `over_field_name`) signals to the anomaly detection algorithms that **population
    analysis** is desired, where entities are compared to their peers (instead of
    against their own past behavior). Population analysis is discussed in depth later
    in this chapter.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`over_field_name`)向异常检测算法发出信号，希望进行**总体分析**，其中实体与其同伴进行比较（而不是与自己的过去行为进行比较）。总体分析将在本章后面进行深入讨论。'
- en: The "formula"
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “公式”
- en: 'If we were to document all of the possible configuration options for a detector
    and then create a flow chart-like map, it would look like the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要记录一个检测器的所有可能的配置选项，然后创建一个类似于流程图的地图，它将看起来如下：
- en: '![Figure 3.3 – The "formula" for building a detector from scratch'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.3 – 从零开始构建检测器的“公式”'
- en: '](img/B17040_03_003.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_03_003.jpg)'
- en: Figure 3.3 – The "formula" for building a detector from scratch
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – 从零开始构建检测器的“公式”
- en: 'The following are things to note about the diagram shown in *Figure 3.3*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些关于*图3.3*中所示图表的注意事项：
- en: Capitalized text is the explanation and italics text is the detector configuration
    settings (`by_field_name`, `partition_field_name`, and `over_field_name` are shortened
    to be simply `by`, `partition`, and `over`).
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大写文本是解释，斜体文本是检测器配置设置（`by_field_name`、`partition_field_name`和`over_field_name`简化为`by`、`partition`和`over`）。
- en: Items in square brackets are optional (high, low, non-zero, non-null).
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方括号中的项是可选的（高、低、非零、非空）。
- en: Choose only one exit branch (notice only one exit branch out of `rare`/`freq_rare`
    because `by` is mandatory).
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只选择一个退出分支（注意`rare`/`freq_rare`中只有一个退出分支，因为`by`是强制性的）。
- en: Comparison of something versus its own history is accomplished simply by *not*
    choosing an `over` field.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过不选择`over`字段，简单地比较某事物与其自身历史记录。
- en: With a comprehensive understanding of the construction of a detector, we will
    now move into practical examples of using detectors for different use cases. First,
    we will explore the count functions that allow us to detect changes in event rates
    over time.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在对检测器结构有全面理解之后，我们现在将进入使用检测器针对不同用例的实用示例。首先，我们将探索允许我们检测事件率随时间变化的计数函数。
- en: Detecting changes in event rates
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 检测事件率的变化
- en: 'There are many important use cases that revolve around the idea of event change
    detection. These include the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多重要的用例围绕着事件变化检测的概念。以下是一些例子：
- en: Discovering a flood of error messages suddenly cropping up in a log file
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在日志文件中突然出现大量错误消息
- en: Detecting a sudden drop in the number of orders processed by an online system
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测在线系统处理的订单数量的突然下降
- en: Determining a sudden excessive number of attempts at accessing something (for
    example, a sudden increase in the number of login attempts on a particular user
    ID)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定对某事物进行访问尝试的突然过多（例如，特定用户 ID 上登录尝试数量的突然增加）
- en: In order for us to find the abnormal, we must first have a mechanism to understand
    the normal rate of occurrence. But relying on our fallible human observation and
    intuition is not always the easiest (or most reliable) approach.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了我们能够找到异常，我们首先必须有一个机制来理解正常发生率。但是，依赖我们易出错的人类观察和直觉并不总是最容易（或最可靠）的方法。
- en: Exploring the count functions
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索计数函数
- en: 'As mentioned in [*Chapter 2*](B17040_02_Epub_AM.xhtml#_idTextAnchor033), *Enabling
    and Operationalization*, Elastic ML jobs have an anomaly detection "recipe" known
    as the **detector**. The detector is key to defining what anomalies the user wants
    to detect. Within the detector is the **function**, which selects the "feature"
    of what is to be detected. In the case of the count functions, the feature is
    the occurrence rate of something over time. There are three main count functions
    that we will see:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第 2 章*](B17040_02_Epub_AM.xhtml#_idTextAnchor033)中所述，*启用和实施*，Elastic ML 作业有一个称为**检测器**的异常检测“配方”。检测器是定义用户想要检测的异常的关键。在检测器中是**函数**，它选择要检测的“特征”。在计数函数的情况下，特征是某事物随时间发生的频率。我们将看到三个主要的计数函数：
- en: '`count`: Counts the number of documents in the bucket resulting from a query
    of the raw data index'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count`：计算从原始数据索引查询中得到的桶中文档的数量'
- en: '`high_count`: The same as `count`, but will only flag an anomaly if the count
    is higher than expected'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`high_count`：与`count`相同，但只有当计数高于预期时才会标记异常'
- en: '`low_count`: The same as `count`, but will only flag an anomaly if the count
    is lower than expected'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_count`：与`count`相同，但只有当计数低于预期时才会标记异常'
- en: We will see that there are a variety of one-sided functions in Elastic ML (to
    only detect anomalies in a certain direction). Additionally, it is important to
    know that the count functions are not counting a field or even the existence of
    fields within a document; they are merely counting the documents in the index
    over time.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到 Elastic ML 中有许多单侧函数（仅用于检测某一方向上的异常）。此外，重要的是要知道计数函数并不是在计数字段或文档中字段的存不存在；它们只是在索引中随时间计数文档。
- en: 'To get a more intuitive feeling for what the count functions do, let''s jump
    into a simple example using the sample data within Kibana:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更直观地了解计数函数的作用，让我们通过 Kibana 内的示例数据来举一个简单的例子：
- en: To enable the sample data, from the Kibana home screen, click on the **Add data**
    button (in either location) as shown in *Figure 3.4*:![Figure 3.4 – The Kibana
    home screen with Add data options
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要启用示例数据，从 Kibana 主屏幕点击**添加数据**按钮（任一位置），如图 3.4 所示：![图 3.4 – 带有添加数据选项的 Kibana
    主屏幕
- en: '](img/B17040_03_004.jpg)'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B17040_03_004.jpg]'
- en: Figure 3.4 – The Kibana home screen with Add data options
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.4 – 带有添加数据选项的Kibana主屏幕
- en: After clicking on **Add data**, select **Sample data** to reveal three sets
    of data:![Figure 3.5 – Adding sample data
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**添加数据**后，选择**样本数据**以显示三组数据：![图3.5 – 添加样本数据
- en: '](img/B17040_03_005.jpg)'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_03_005.jpg)'
- en: Figure 3.5 – Adding sample data
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.5 – 添加样本数据
- en: Click on each of the three **Add data** buttons in each section to load that
    sample dataset into your Elastic Stack. Once the loading is complete, we will
    jump directly to ML by selecting the three-horizontal-lines menu icon (![Text
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击每个部分的三个**添加数据**按钮，将样本数据集加载到你的Elastic Stack中。一旦加载完成，我们将通过选择三个横线菜单图标（![文本
- en: Description automatically generated](img/B17040_03_032.png)) at the top left
    of Kibana to reveal the list of apps, and then select **Machine Learning**:![Figure
    3.6 – Selecting Machine Learning from the Kibana apps menu
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17040_03_032.png)) 在Kibana的左上角以显示应用程序列表，然后选择**机器学习**：![图3.6 – 从Kibana应用程序菜单中选择机器学习
- en: '](img/B17040_03_006.jpg)'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_03_006.jpg)'
- en: Figure 3.6 – Selecting Machine Learning from the Kibana apps menu
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.6 – 从Kibana应用程序菜单中选择机器学习
- en: Once this has been clicked, we will be on the ML overview page, where we can
    immediately see where we can create our first anomaly detection job. Click on
    the **Create job** button, as shown in *Figure 3.7*:![Figure 3.7 – Elastic Cloud
    welcome screen
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦点击，我们将进入ML概览页面，在那里我们可以立即看到我们可以创建我们的第一个异常检测作业。点击**创建作业**按钮，如图3.7所示：![图3.7 –
    Elastic Cloud欢迎屏幕
- en: '](img/B17040_03_007.jpg)'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_03_007.jpg)'
- en: Figure 3.7 – Elastic Cloud welcome screen
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.7 – Elastic Cloud欢迎屏幕
- en: Our next task is to select the index pattern (marked with an index with shards
    icon) or a saved search (marked with a magnifying glass icon) that contains the
    data that we'd like to analyze. If a saved search is chosen, then a filtered query
    that was previously created and saved within Kibana's `kibana_sample_data_logs`
    index, as we want to pass every document in that index through Elastic ML:![Figure
    3.8 – Selecting the kibana_sample_data_logs index for analysis
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们接下来的任务是选择索引模式（带有索引分片图标标记）或一个已保存的搜索（带有放大镜图标标记），其中包含我们想要分析的数据。如果选择了一个已保存的搜索，那么一个之前在Kibana的`kibana_sample_data_logs`索引中创建并保存的过滤查询，因为我们希望将那个索引中的每个文档都通过Elastic
    ML：![图3.8 – 选择kibana_sample_data_logs索引进行分析
- en: '](img/B17040_03_008.jpg)'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_03_008.jpg)'
- en: Figure 3.8 – Selecting the kibana_sample_data_logs index for analysis
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.8 – 选择kibana_sample_data_logs索引进行分析
- en: 'On the next screen, we will select the **Single metric** job wizard because,
    at this point, we''re interested in analyzing only one aspect of the data: its
    count over time:![Figure 3.9 – Choosing a Single metric job'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一屏幕上，我们将选择**单个度量**作业向导，因为此时我们只对分析数据的单一方面感兴趣：其随时间的变化计数：![图3.9 – 选择单个度量作业
- en: '](img/B17040_03_009.jpg)'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_03_009.jpg)'
- en: Figure 3.9 – Choosing a Single metric job
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.9 – 选择单个度量作业
- en: On the next screen, in order to follow along with this example, *you must select
    the* **Use full kibana_sample_logs_data** *button in order to include the sample
    anomaly in this dataset*:![Figure 3.10 – Selecting to use all the data within
    the index
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一屏幕上，为了跟随这个示例，*你必须选择* **使用完整的kibana_sample_logs_data** *按钮，以便将样本异常包含在这个数据集中*：![图3.10
    – 选择使用索引中的所有数据
- en: '](img/B17040_03_010.jpg)'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_03_010.jpg)'
- en: Figure 3.10 – Selecting to use all the data within the index
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.10 – 选择使用索引中的所有数据
- en: Note
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: This demo data, when installed, actually puts about half of the data in the
    past and half in the future (by dynamically modifying the timestamps on ingest).
    This is done to provide a mechanism for the static data to look "real-time" when
    dashboards are viewed on data in the "last hour," for example. As a result of
    this, we're really going to ask Elastic ML to analyze data from the past and the
    future, where normally it would be impossible to have data from the future. Suspend
    belief for now for the sake of the example as the anomaly we'd like to demonstrate
    is in the second half of the dataset.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当此演示数据安装时，实际上将大约一半的数据放在过去，另一半放在未来（通过动态修改摄取时间戳）。这样做是为了提供一个机制，使得静态数据在查看“过去一小时”的数据仪表板时看起来像是“实时”的。因此，我们实际上会要求Elastic
    ML分析过去和未来的数据，而通常情况下，未来的数据是无法获得的。为了这个示例，现在暂时放下怀疑，因为我们想展示的异常在数据集的第二部分。
- en: Now, click the **Next** button to advance to the next step in the configuration
    wizard.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，点击**下一步**按钮以进入配置向导的下一步。
- en: After clicking the **Next** button, we will need to select what we want to analyze
    from the **Pick fields** drop-down box. We will select **Count(Event rate)** to
    focus on our original goal here, which is to detect changes in the event rate
    in this index over time:![Figure 3.11 – Selecting the count of events over time
    as our detection
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**下一步**按钮后，我们需要从**选择字段**下拉框中选择我们想要分析的内容。我们将选择**事件计数（事件率）**，以关注我们的原始目标，即检测在此索引中事件率随时间的变化：![图3.11
    – 选择事件计数随时间变化作为我们的检测
- en: '](img/B17040_03_011.jpg)'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_03_011.jpg)'
- en: Figure 3.11 – Selecting the count of events over time as our detection
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.11 – 选择事件计数随时间变化作为我们的检测
- en: Notice that looking through this drop-down box shows that other analyses could
    be done, depending on the data type of the field in the data. We will explore
    some of these other options later on in subsequent examples.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，查看这个下拉框会显示，根据数据中字段的数据类型，可以进行其他分析。我们将在后续示例中探索这些其他选项。
- en: Click the **Next** button to proceed, leaving the other options as their defaults
    for now.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 点击**下一步**按钮继续，现在保留其他选项为默认值。
- en: Now, we need to name our anomaly detection job. In the `web_logs_rate` was used:![Figure
    3.12 – Naming the anomaly detection job
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要为我们的异常检测工作命名。在`web_logs_rate`中使用：![图3.12 – 为异常检测工作命名
- en: '](img/B17040_03_012.jpg)'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_03_012.jpg)'
- en: Figure 3.12 – Naming the anomaly detection job
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.12 – 为异常检测工作命名
- en: Again, leave the other options as their defaults and click the **Next** button.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再次，将其他选项保留为默认值，并点击**下一步**按钮。
- en: A validation step takes place to ensure that everything is reasonable for the
    analysis to work:![Figure 3.13 – Job validation step
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行验证步骤以确保分析合理：![图3.13 – 工作验证步骤
- en: '](img/B17040_03_013.jpg)'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_03_013.jpg)'
- en: Figure 3.13 – Job validation step
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.13 – 工作验证步骤
- en: Click the **Next** button to proceed.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 点击**下一步**按钮继续。
- en: At this point, the job is ready to be created (and notice in *Figure 3.14* that
    some sensible default options, such as **Model memory limit** and **Enable model
    plot**, were chosen for you):![Figure 3.14 – Anomaly detection job ready to be
    created](img/B17040_03_014.jpg)
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，工作已经准备好创建（注意在*图3.14*中，一些合理的默认选项，例如**模型内存限制**和**启用模型绘图**，已经为您选择了）：![图3.14
    – 准备创建的异常检测工作](img/B17040_03_014.jpg)
- en: Figure 3.14 – Anomaly detection job ready to be created
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.14 – 准备创建的异常检测工作
- en: After the **Create job** button is clicked, you will see an animated preview
    of the results superimposed on top of the data, as follows:![Figure 3.15 – Results
    preview of the job execution displayed
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建工作**按钮后，您将看到结果动画预览叠加在数据上方，如下所示：![图3.15 – 显示工作执行结果的预览
- en: '](img/B17040_03_015.jpg)'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_03_015.jpg)'
- en: Figure 3.15 – Results preview of the job execution displayed
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.15 – 显示工作执行结果的预览
- en: Let's now click the **View results** button to investigate in detail what the
    anomaly detection job has found in the data.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们点击**查看结果**按钮，以详细了解异常检测工作在数据中发现了什么。
- en: 'Using the scrubber below the main graph, adjust the location and width of the
    viewing area to zoom in on the big spike:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用主图下方的刮擦器，调整查看区域的定位和宽度，以便放大查看大峰值：
- en: '![Figure 3.16 – Results shown for a critical anomaly'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.16 – 显示关键异常的结果'
- en: '](img/B17040_03_016.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_03_016.jpg)'
- en: Figure 3.16 – Results shown for a critical anomaly
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 – 显示关键异常的结果
- en: Note
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: As you zoom in, out, and around, just be aware of the chart aggregation interval
    as compared to the job's bucket span (as circled in *Figure 3.16*). If you are
    zoomed out to a wider view, the chart aggregation interval can be larger than
    the job's bucket span, making the position of the drawn anomaly on the chart less
    exact.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当您放大、缩小和移动时，请注意图表聚合间隔与工作桶跨度（如图3.16中圈出所示）的比较。如果您放大到一个更宽的视图，图表聚合间隔可以大于工作桶跨度，使得图表上绘制的异常位置不够精确。
- en: 'Here, in *Figure 3.16*, we can see that the very large spike in events was
    flagged as two distinct anomalies because the actual number of web requests seen
    in the logs was around 11 times higher than expected (given the learned model
    of the data up until that point in time). You may notice that the chart shows
    two anomalies next to each other because clearly, the spike in events spanned
    more than one 15-minute bucket interval. You may also notice that by default,
    there is only one anomaly shown in the table below the chart. This is because
    **Interval** defaults to **Auto** and time-adjacent anomalies are summarized together,
    with only the highest score shown. If **Interval** is changed to **Show all**,
    then both anomaly records are listed in the table:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，在*图3.16*中，我们可以看到事件的大幅上升被标记为两个不同的异常，因为日志中实际看到的Web请求数量比预期的高出大约11倍（考虑到直到那个时间点之前学习到的数据模型）。你可能注意到图表显示了两个相邻的异常，因为显然事件的上升跨越了多个15分钟的桶间隔。你也可能注意到，默认情况下，图表下面的表格中只显示了一个异常。这是因为**间隔**默认设置为**自动**，时间相邻的异常被汇总在一起，只显示最高分。如果**间隔**改为**显示所有**，那么两个异常记录都会列在表格中：
- en: '![Figure 3.17 – Interval set to Show all anomalies'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.17 – 设置为显示所有异常'
- en: '](img/B17040_03_017.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_03_017.jpg)'
- en: Figure 3.17 – Interval set to Show all anomalies
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 – 设置为显示所有异常
- en: 'There''s one final thing to notice in this example, which is the other, lesser-scored
    anomalies earlier in the dataset:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，还有一点需要注意，那就是数据集中较早出现的其他低分异常：
- en: '![Figure 3.18 – Multi-bucket anomalies'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.18 – 多桶异常'
- en: '](img/B17040_03_018.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_03_018.jpg)'
- en: Figure 3.18 – Multi-bucket anomalies
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18 – 多桶异常
- en: 'There are a few key things to recognize about these less-than-obvious anomalies:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些不太明显的异常，有几个关键点需要注意：
- en: They have lower scores than the massive spike we just investigated because relatively
    speaking, these are not as anomalous, but are interestingly significant.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们的分数低于我们刚刚调查的大幅波动，因为相对而言，这些并不那么异常，但有趣的是，它们具有显著性。
- en: The anomaly here is the "lack" of expected values. In other words, the `count`
    function interprets *no data* as 0 and that can be anomalous if normally there's
    an expectation that events should be occurring.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里的异常是“预期值”的“缺乏”。换句话说，`count`函数将“无数据”解释为0，如果通常情况下预期事件应该发生，那么这可能是异常的。
- en: These anomalies are not single-bucket anomalies, but rather **multi-bucket anomalies**.
    Multi-bucket anomalies are designated with a different symbol in the UI (a cross
    instead of a dot). They denote cases in which the actual singular value may not
    necessarily be anomalous, but there is a trend that is occurring in a sliding
    window of 12 consecutive buckets. Here, you can see that there is a noticeable
    slump spanning several adjacent buckets.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些异常不是单个桶的异常，而是**多桶异常**。在UI中，多桶异常用不同的符号表示（一个十字而不是一个点）。它们表示实际的单个值可能并不一定是异常的，但在12个连续桶的滑动窗口中存在一个趋势。在这里，你可以看到有几个相邻桶出现了一个明显的下滑。
- en: Note
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on interpreting multi-bucket anomalies, see the detailed
    blog post at [elastic.co/blog/interpreting-multi-bucket-impact-anomalies-using-elastic-machine-learning-features](http://elastic.co/blog/interpreting-multi-bucket-impact-anomalies-using-elastic-machine-learning-features).
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于解释多桶异常的更多信息，请参阅[elastic.co/blog/interpreting-multi-bucket-impact-anomalies-using-elastic-machine-learning-features](http://elastic.co/blog/interpreting-multi-bucket-impact-anomalies-using-elastic-machine-learning-features)上的详细博客文章。
- en: We have seen, through this example, how the count function allows us to easily
    detect an obvious (and not-so-obvious) set of anomalies relating to the overall
    rate of occurrence of events (documents) in an index over time. Let's continue
    our journey by looking at other count and occurrence-based functions.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个例子，我们已经看到了`count`函数如何使我们能够轻松地检测到一组明显（和不那么明显）的异常，这些异常与索引中事件（文档）随时间发生的总体发生率有关。让我们继续我们的旅程，通过查看其他基于计数和发生率的函数。
- en: Other counting functions
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他计数函数
- en: In addition to the functions that we've described so far, there are several
    other counting functions that enable a broader set of use cases.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们之前描述的函数之外，还有几个其他计数函数，可以支持更广泛的使用场景。
- en: Non-zero count
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非零计数
- en: 'The non-zero count functions (`non_zero_count`, `low_non_zero_count`, and `high_non_zero_count`)
    allow the handling of count-based analysis, as well as allowing accurate modeling
    in cases where the data may be sparse and you would not want the non-existence
    of data to be explicitly treated as zero, but rather as null—in other words, a
    dataset in time, which looks like the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 非零计数函数（`non_zero_count`、`low_non_zero_count`和`high_non_zero_count`）允许处理基于计数的分析，同时允许在数据可能稀疏且您不希望将数据不存在明确地视为零，而是将其视为空值的情况下进行准确建模——换句话说，一个看起来像以下的时间序列数据集：
- en: '[PRE0]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Data with the `non_zero_count` functions will be interpreted as the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`non_zero_count`函数的数据将被解释为以下内容：
- en: '[PRE1]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The act of treating zeros as null can be useful in cases where the non-existence
    of measurements at regular intervals is expected. Some practical examples of this
    are as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 将零视为空值在预期到在常规间隔内不存在测量值的情况下是有用的。以下是一些实际例子：
- en: The number of airline tickets purchased per month by an individual
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每月个人购买的航空机票数量
- en: The number of times a server reboots in a day
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每天服务器重启的次数
- en: The number of login attempts on a system per hour
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每小时系统登录尝试的次数
- en: 'To select the non-zero count version of the count functions in the job wizards,
    just toggle the **Sparse data** option during the setup:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 要在作业向导中选择计数函数的非零计数版本，只需在设置期间切换**稀疏数据**选项：
- en: '![Figure 3.19 – Adding the Sparse data option to select the non-zero count'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.19 – 添加稀疏数据选项以选择非零计数'
- en: '](img/B17040_03_019.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_03_019.jpg)'
- en: Figure 3.19 – Adding the Sparse data option to select the non-zero count
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.19 – 添加稀疏数据选项以选择非零计数
- en: We will see later in the chapter, when we are configuring the jobs in the advanced
    job wizard or via the API, that we will be explicitly using function names (such
    as `high_non_zero_count`) instead of toggling options with more conceptual descriptions.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面看到，当我们通过高级作业向导或API配置作业时，我们将明确使用函数名（如`high_non_zero_count`）而不是使用更具概念描述的选项切换。
- en: Distinct count
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独特计数
- en: 'The distinct count functions (`distinct_count`, `low_distinct_count`, and `high_distinct_count`)
    measure the uniqueness (`distinct_count(url.keyword)` as the detector configuration
    in the last example on the `kibana_sample_data_logs` index, we would have caught
    the same anomalous timeframe, but for a different reason—not only was the overall
    volume of requests high, as we saw back in *Figure 3.16*, but here in *Figure
    3.20*, we see that there was a high diversity of URLs being requested:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 独特计数函数（`distinct_count`、`low_distinct_count`和`high_distinct_count`）测量唯一性（在`kibana_sample_data_logs`索引的最后一个示例中，`distinct_count(url.keyword)`作为检测器配置，我们会捕捉到相同的时间段异常，但原因不同——不仅请求的整体量很高，正如我们在*图3.16*中看到的，而且在*图3.20*中，我们看到请求的URL多样性很高）：
- en: '![Figure 3.20 – Distinct count detector example'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.20 – 独特计数检测器示例'
- en: '](img/B17040_03_020.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_03_020.jpg)'
- en: Figure 3.20 – Distinct count detector example
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.20 – 独特计数检测器示例
- en: With an appreciation of count-based functions, let's now turn to metric-based
    functions, which allow us to analyze numerical fields in the data.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解基于计数的功能之后，我们现在转向基于度量的功能，这些功能使我们能够分析数据中的数值字段。
- en: Detecting changes in metric values
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测度量值的变化
- en: Obviously, not all data being emitted from systems will be text or categorical
    in nature—a vast amount of it is numerical. Detecting changes in metric values
    over time is perfectly suited for anomaly detection because, as mentioned in [*Chapter
    1*](B17040_01_Epub_AM.xhtml#_idTextAnchor016), *Machine Learning for IT*, the
    historical paradigm of alerting on exceptions in numerical values via static thresholds
    has been troublesome for decades. Let's explore all that Elastic ML has to offer
    with respect to the functions that help you detect changes in numerical fields
    in your data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，并非所有从系统中发出的数据都将是有文本或分类性质的——其中大量的是数值数据。检测度量值随时间的变化非常适合用于异常检测，因为正如在[*第一章*](B17040_01_Epub_AM.xhtml#_idTextAnchor016)《IT机器学习》中提到的，通过静态阈值在数值值上的异常警报的历史范式已经困扰了数十年。让我们一起来探索Elastic
    ML在帮助您检测数据中数值字段变化的功能方面所能提供的一切。
- en: Metric functions
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 度量函数
- en: Metric functions operate on numerical fields and return numerical values. They
    are perhaps the easiest of the detector functions to understand.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 度量函数在数值字段上操作并返回数值。它们可能是最容易理解的检测器函数。
- en: min, max, mean, median, and metric
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小值，最大值，平均值，中位数和度量
- en: 'These functions do exactly as you would expect: they return the minimum, maximum,
    average/mean, and median of all of the numerical observations for the field of
    interest in the bucket span.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数确实如您预期的那样工作：它们返回桶跨度内感兴趣字段所有数值观察值的最低值、最高值、平均值/均值和中位数。
- en: The `metric` function is a little unique in that it is really just a shorthand
    way of specifying that `min`, `max`, and `mean` are to be used together.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`metric` 函数有点独特，因为它实际上只是指定 `min`、`max` 和 `mean` 一起使用的一种简写方式。'
- en: It should be noted that if the frequency of the data (for example, data that
    comes from a sampling source such as Metricbeat) exactly matches the bucket span,
    then there is only one sample per bucket span. This means that the minimum, maximum,
    average/mean, and median of the field of interest are all the same value (the
    value of the single observation itself). Therefore, if possible, it is usually
    better to have multiple numerical samples per bucket span if you want to have
    discrimination using these functions.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，如果数据的频率（例如，来自采样源（如Metricbeat）的数据）与桶跨度完全匹配，那么每个桶跨度只有一个样本。这意味着感兴趣字段的最低值、最高值、平均值/均值和中位数都是相同的值（即单个观察值本身的值）。因此，如果你想要使用这些函数进行区分，通常最好在每个桶跨度内有多于一个数值样本。
- en: Another fact to note is that these metric functions treat the lack of data as
    *null*. In other words, if your data is sparse and there are bucket spans in which
    no observations are seen, the lack of data will not "drag down" the statistics
    for the field of interest. This is why these metric-based functions have no "non-zero"
    or "non-null" counterpart.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的事实是，这些指标函数将数据缺失视为 *null*。换句话说，如果你的数据稀疏，并且在某些桶跨度中没有观察到观察值，数据缺失不会“拖累”感兴趣字段的统计数据。这就是为什么基于指标的函数没有“非零”或“非空”对应物。
- en: varp
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: varp
- en: The `varp` function measures the overall variance of a metric over time—its
    volatility. Using this function might be applicable to finding cases where the
    numerical value of a field should normally be rather consistent, but you would
    like to detect whether there was a change.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`varp` 函数测量一个指标随时间变化的总体方差——其波动性。使用此函数可能适用于寻找字段数值通常应该相当一致的情况，但你希望检测是否有变化。'
- en: Sum and non-null sum
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 求和与非空求和
- en: The `sum` function will return the sum of all of the numerical observations
    for the field of interest in the bucket span. Use the "non-null" version if you
    have sparse data and do not want the lack of data being treated as *zero*, which
    will inevitably "drag down" the value of the sum.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`sum` 函数将返回桶跨度内感兴趣字段所有数值观察值的总和。如果你有稀疏数据且不希望将数据缺失视为 *零*，这将不可避免地“拖累”总和的值，请使用“非空”版本。'
- en: 'If we had selected `sum(bytes)` as the detector configuration in the last example
    on the `kibana_sample_data_logs` index, we would have caught the same anomalous
    timeframe, but for a different reason—we see that the requests made also resulted
    in a higher quantity of bytes being transferred from the web server:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在上一个例子中在 `kibana_sample_data_logs` 索引中将 `sum(bytes)` 作为检测器配置，我们会捕捉到相同的时间段异常，但原因不同——我们注意到请求也导致了从网络服务器传输的字节数量增加：
- en: '![Figure 3.21 – Sum detector example'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.21 – 求和检测器示例'
- en: '](img/B17040_03_021.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.21 – 求和检测器示例](img/B17040_03_021.jpg)'
- en: Figure 3.21 – Sum detector example
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.21 – 求和检测器示例
- en: This is totally sensible, given that an increased number of requests to a web
    server will correlate with an increase in the number of bytes being transferred.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这完全是有道理的，因为对网络服务器的请求增加将与传输的字节数量增加相关。
- en: Now that we have an appreciation for the simpler detector functions, let's move
    on to the more complex, advanced functions.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经欣赏了简单的检测器函数，让我们继续探讨更复杂、更高级的函数。
- en: Understanding the advanced detector functions
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解高级检测器函数
- en: In addition to the detector functions mentioned so far, there are also a few
    other, more advanced functions that allow some very unique capabilities. Some
    of these functions are only available if the ML job is configured via the advanced
    job wizard or via the API.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前提到的检测器函数外，还有一些其他更高级的函数，允许一些非常独特的功能。其中一些函数只有在通过高级作业向导或通过API配置ML作业时才可用。
- en: rare
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: rare
- en: In the context of a stream of temporal information (such as a log file), the
    notion of something being statistically rare (occurring at a low frequency) is
    paradoxically both intuitive and hard to understand. If I were asked, for example,
    to trawl through a log file and find a rare message, I might be tempted to label
    the first novel message that I saw as a rare one. But what if practically every
    message was novel? Are they all rare? Or is nothing rare?
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间信息流（如日志文件）的上下文中，某事物在统计上稀有（以低频率发生）的概念既直观又难以理解。例如，如果我被要求浏览日志文件并找到一条稀有信息，我可能会倾向于将我看到的第一条新颖信息标记为稀有。但如果我们几乎每条信息都是新颖的怎么办？它们都是稀有的吗？或者什么都不是稀有的？
- en: In order to define rarity to be useful in the context of a stream of events
    in time, we need to agree that the declaration of something as being rare must
    take into account the context in which it exists. If there are lots of other routine
    things and a small number of unique things, then we can deem the unique things
    rare. If there are many unique things, then we will deem that nothing is rare.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在时间事件流的环境中定义稀有性以使其有用，我们需要同意，将某事物声明为稀有必须考虑其存在的上下文。如果有许多其他常规事物和少量独特事物，那么我们可以认为独特事物是稀有的。如果有许多独特事物，那么我们将认为没有什么事物是稀有的。
- en: 'When applying the `rare` function in an ML job, there is a requirement to declare
    which field the `rare` function is focusing on. This field is then defined as
    `by_field_name`. Configuration of the `rare` function does not have its own wizard
    in the Elastic ML UI, so you will need to define it using the advanced job wizard.
    For example, to find log entries that reference a rare country name, structure
    your detector similar to this:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在ML作业中应用`rare`函数时，需要声明`rare`函数关注的字段。该字段随后被定义为`by_field_name`。`rare`函数的配置在Elastic
    ML UI中没有自己的向导，因此您需要使用高级作业向导来定义它。例如，为了找到引用稀有国家名称的日志条目，将探测器结构设计如下：
- en: '![Figure 3.22 – Rare detector example'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.22 – 稀有探测器示例'
- en: '](img/B17040_03_022.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_03_022.jpg]'
- en: Figure 3.22 – Rare detector example
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.22 – 稀有探测器示例
- en: This could be handy for finding unexpected geographical access (as in "Our admins
    usually log in from the New York and London offices almost daily, but never from
    Moscow!").
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于查找意外的地理访问（例如，“我们的管理员几乎每天都从纽约和伦敦办公室登录，但从未从莫斯科登录！”）可能很有用。
- en: Frequency rare
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 频率稀有
- en: The `freq_rare` function is a specialized version of `rare`, in that it looks
    for members of a population that cause rare values of `by_field_name` to occur
    frequently. For example, you could locate a particular IP address that is attempting
    to access many rare URLs that are not generally seen across the entire population
    of all client IP addresses. This IP address could be attempting to access otherwise
    hidden sections of a website in a nefarious way, or may be attempting attacks
    such as SQL injection.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`freq_rare`函数是`rare`的一个专用版本，它寻找导致`by_field_name`出现稀有值的群体成员。例如，您可以定位一个试图访问许多罕见URL的特定IP地址，这些URL在整个客户端IP地址群体中通常看不到。这个IP地址可能正在以恶意的方式尝试访问网站的隐藏部分，或者可能正在尝试SQL注入等攻击。'
- en: Information content
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信息含量
- en: The `info_content` function is perhaps the most specialized detector function
    in Elastic ML's arsenal. It was originally written as a means to measure the amount
    of **entropy** in text strings (how many and how diverse the characters are).
    This is because there are well-known techniques in malware that encrypt instructions
    and/or payload data for transmission for **command and control** (**C2**) and
    data exfiltration activity. Detecting this activity along this feature of the
    data is more reliable than looking at other features (such as the number of bytes
    sent or counting distinct entities).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`info_content`函数可能是Elastic ML工具箱中最专业的探测器函数。它最初被编写为测量文本字符串中**熵**的量（字符的数量和多样性）。这是因为已知在恶意软件中有加密指令和/或有效载荷数据以进行**命令和控制**（**C2**）和数据泄露活动的技术。通过数据的这一特征检测此活动比查看其他特征（如发送的字节数或计数不同的实体）更可靠。'
- en: 'The algorithm used will essentially do the following steps:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的算法将基本上执行以下步骤：
- en: Sort the unique strings into alphabetical order.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将独特的字符串按字母顺序排序。
- en: Concatenate those unique strings into one long string.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将那些独特的字符串连接成一个长字符串。
- en: Perform the `gzip` algorithm on that long string to compress it.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对那个长字符串执行`gzip`算法以压缩它。
- en: The information content is the length of the compressed data.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息含量是压缩数据的长度。
- en: Some of the ML jobs in the Elastic SIEM utilize the `info_content` function—stay
    tuned for [*Chapter 8*](B17040_08_Epub_AM.xhtml#_idTextAnchor146), *Anomaly Detection
    in Other Elastic Stack Apps*, for more details.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Elastic SIEM中的一些ML作业使用了`info_content`函数——敬请期待[第8章](B17040_08_Epub_AM.xhtml#_idTextAnchor146)《其他Elastic
    Stack应用中的异常检测》以获取更多详细信息。
- en: Geographic
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 地理
- en: If you find a geographic location that is unusual to a learned location area
    on Earth, then the `lat_long` function will be helpful, taking a `field_name`
    argument that is a comma-separated pair of numbers in the range of -180 to 180
    (for example, `40.75, -73.99`, the coordinates of Times Square in New York City).
    The `lat_long` function can also operate on a `geo_point` field, a `geo_shape`
    field that contains point values, or a `geo_centroid` aggregation. An example
    use case would be to flag a location that isn't normal (and potentially fraudulent
    or malicious) for a specific user, transaction, and so on.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您发现一个地理位置与地球上已学习到的地理位置区域不寻常，那么`lat_long`函数将很有帮助，它接受一个`field_name`参数，该参数是一个范围在-180到180（例如，`40.75,
    -73.99`，纽约市时代广场的坐标）的逗号分隔的数字对。`lat_long`函数还可以在`geo_point`字段、包含点值的`geo_shape`字段或`geo_centroid`聚合上操作。一个示例用例可能是标记一个对于特定用户、交易等来说不正常（可能是有欺诈或恶意）的位置。
- en: Time
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间
- en: Not all things occur randomly in time, especially with things involving human
    behavior. We may eat, commute, or log into certain systems at predictable times
    of the day or week. Using the `time_of_day` and `time_of_week` functions, you
    can detect changes of behavior from a learned temporal routine. If a behavior
    is predictable on a 24-hour timeframe, then `time_of_day` is more appropriate.
    If the routine is day-of-the-week-dependent, then `time_of_week` should be a more
    logical choice.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有事物在时间上都是随机发生的，尤其是涉及人类行为的事物。我们可能在一天或一周的某个可预测的时间吃饭、通勤或登录某些系统。使用`time_of_day`和`time_of_week`函数，您可以检测从学习到的时间常规中的行为变化。如果行为在24小时时间框架内是可预测的，那么`time_of_day`更合适。如果常规依赖于一周中的某一天，那么`time_of_week`应该是一个更合理的选项。
- en: Note
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Do not confuse the usage of these time functions with the natural temporal learning
    of all detectors in the anomaly detection jobs. As explained in [*Chapter 1*](B17040_01_Epub_AM.xhtml#_idTextAnchor016),
    *Machine Learning for IT*, the de-trending capability of the modeling will take
    into account the time at which something occurs. These functions simply model
    the event's timestamp within the day or week. For example, if something routinely
    happens at 2:00 A.M. every day, the function will learn that the normal time for
    this to happen is at the 7,200th second into the day.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 不要混淆这些时间函数的使用与异常检测作业中所有检测器的自然时间学习。如[第1章](B17040_01_Epub_AM.xhtml#_idTextAnchor016)《IT机器学习》中所述，建模的去趋势能力将考虑某事发生的时间。这些函数只是将事件的时间戳建模在一天或一周之内。例如，如果某事每天晚上2:00
    A.M.都会例行发生，该函数将学习到这种事情发生的正常时间是每天的第7,200秒。
- en: Now that we've been through the entire catalog of detector functions, let's
    look ahead and see how we can expand the breadth of our analysis by splitting
    the modeling across entities that are represented by categorical fields.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了所有检测函数的整个目录，让我们展望一下，看看我们如何通过拆分由分类字段表示的实体之间的建模来扩展分析的范围。
- en: Splitting analysis along categorical features
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 沿分类特征拆分分析
- en: We have seen the power of anomaly detection jobs in uncovering interesting anomalies
    in a single time series dataset. However, there are a few mechanisms by which
    the analysis can be split along a categorical field to invoke a parallel analysis
    across tens, hundreds, and even multiple thousands of unique entities.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了异常检测作业在揭示单个时间序列数据集中有趣异常方面的力量。然而，分析可以通过分类字段进行拆分，从而在成千上万的独特实体上调用并行分析。
- en: Setting the split field
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置拆分字段
- en: 'When using some of the job wizards (such as the Multi-metric and Population
    wizards), you will see an option to split the analysis:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用一些工作向导（例如多指标和人口向导）时，您将看到一个选项来拆分分析：
- en: '![Figure 3.23 – Splitting on a categorical field'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.23 – 在分类字段上拆分'
- en: '](img/B17040_03_023.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_03_023.jpg)'
- en: Figure 3.23 – Splitting on a categorical field
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.23 – 在分类字段上拆分
- en: 'Here, in *Figure 3.23*, which uses the Multi-metric wizard to build a job against
    the `kibana_sample_data_ecommerce` index, we see that the high sum function on
    the `taxful_total_price` field is being split per instance on the field called
    `category.keyword` (plus turning the **Sparse data** option on). In other words,
    the analysis will be done for every category of items in this e-commerce store
    (men''s clothing, women''s accessories, and so on). If the analysis is run and
    the results are inspected using the Anomaly Explorer UI, the result might look
    like the following:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*图3.23*中，使用多度量向导构建针对`kibana_sample_data_ecommerce`索引的作业，我们看到`taxful_total_price`字段上的高总和函数正在按实例分割在名为`category.keyword`的字段上（同时开启**稀疏数据**选项）。换句话说，分析将针对这个电子商务店中的每个商品类别（男装、女装配饰等）进行。如果运行分析并使用异常探索器UI检查结果，结果可能看起来像以下这样：
- en: '![Figure 3.24 – Results of split analysis'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.24 – 分割分析的结果'
- en: '](img/B17040_03_024.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_03_024.jpg)'
- en: Figure 3.24 – Results of split analysis
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.24 – 分割分析的结果
- en: Notice in *Figure 3.24*, that the Anomaly Explorer view is different from what
    we've seen so far in the Single Metric Viewer. The Anomaly Explorer shows the
    top 10 most anomalous categories (the field we split on) over time. Notice that
    not every category is shown, only the ones with anomalies—and clearly, the **Men's
    Clothing** category was the most unusual with a revenue of $2,250 on November
    9th (in this version of the dataset). We will be learning more about understanding
    the results of multi-metric jobs and will use the Anomaly Explorer extensively
    in [*Chapter 5*](B17040_05_Epub_AM.xhtml#_idTextAnchor090), *Interpreting Results*.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在*图3.24*中，异常探索视图与我们迄今为止在单度量查看器中看到的不同。异常探索显示了随时间变化的10个最异常的类别（我们分割的字段）。注意，并不是每个类别都显示出来，只有具有异常的类别——显然，**男装**类别在11月9日的收入为$2,250，是最不寻常的（在这个数据集版本中）。我们将学习更多关于理解多度量作业结果的知识，并在[*第五章*](B17040_05_Epub_AM.xhtml#_idTextAnchor090)
    *解释结果*中广泛使用异常探索器。
- en: The difference between splitting using partition and by_field
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用分区和by_field分割的区别
- en: As a reminder, when using the Multi-metric wizard and a split is invoked, the
    `partition_field_name` setting is set with the value of the field chosen in the
    UI.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，当使用多度量向导并调用分割时，`partition_field_name`设置被设置为UI中选择的字段值。
- en: When splitting is chosen in the Population wizard, however, `by_field_name`
    is chosen to split the analysis. If the Advanced wizard is used, then `partition_field_name`
    and/or `by_field_name` can be defined (if both, then it's effectively a double-split).
    Therefore, it would be helpful to know how these two settings, which effectively
    split the analysis, are different from each other.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当在人口向导中选择分割时，`by_field_name`被选中以分割分析。如果使用高级向导，则可以定义`partition_field_name`和/或`by_field_name`（如果两者都定义，则实际上是一个双重分割）。因此，了解这两个实际上分割分析的设置如何彼此不同将是有帮助的。
- en: 'If you want to "hard split" the analysis, use `partition_field_name`:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想“硬分割”分析，使用`partition_field_name`：
- en: The field chosen should, in general, have <10,000 distinct values per job, as
    more memory is required to partition.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，所选字段在每个工作项中应具有<10,000个不同的值，因为需要更多的内存来分区。
- en: Each instance of the field is like an independent variable.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字段的每个实例就像一个独立变量。
- en: The scoring of anomalies in one partition is more independent from other partitions.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分区中异常的评分与其他分区更独立。
- en: 'If you want a "soft split," use `by_field_name`:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想进行“软分割”，使用`by_field_name`：
- en: The field chosen should, in general, have <100,000 distinct values per job.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，所选字段在每个工作项中应具有<100,000个不同的值。
- en: More appropriate for attributes of an entity (dependent variables).
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更适合实体的属性（因变量）。
- en: Scoring considers the history of other `by` fields.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评分考虑其他`by`字段的历史。
- en: Let's dive deep into that last listed item—relating to the "history" of the
    other `by` fields. What exactly does that mean?
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨最后列出的项目——与“历史”相关的其他`by`字段。这究竟意味着什么？
- en: 'In general, there is a concept in anomaly detection job analysis relating to
    when an entity first happens, which we''ll call the `host:X` or `error_code:Y`),
    there may be one of two situations:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在异常检测工作分析中有一个概念，与实体首次发生的时间相关，我们可以称之为`host:X`或`error_code:Y`，可能存在两种情况：
- en: That new entity is seen as "novel" and that, in itself, is notable and potentially
    worthy of being flagged as anomalous. To do that, you need to have your "dawn
    of time" be when the job starts.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新实体被视为“新颖”的，这本身就很引人注目，并且可能值得将其标记为异常。为此，您需要将“时间的黎明”设定为作业开始的时候。
- en: That new entity is just part of the normal "expansion" of the data—perhaps a
    new server was added to the mix or a new `product_id` was added to the catalog.
    In this case, just start modeling that new entity and don't make a fuss about
    it showing up. To do that, you need to have the "dawn of time" be when that entity
    first shows up.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新实体只是数据正常“扩展”的一部分——也许新服务器被添加到混合中，或者新的`product_id`被添加到目录中。在这种情况下，只需开始对该新实体进行建模，不要对它出现而大惊小怪。为此，您需要将“时间的黎明”设定为该实体首次出现的时候。
- en: When analyzing splits using `by_field_name`, the dawn of time is when the ML
    job was started and when split using `partition_field_name`, the dawn of time
    is when that partition first showed up in the data. As such, you will get different
    results if you split one way versus the other for a situation in which something
    "new" comes along.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`by_field_name`分析拆分时，“时间的黎明”是ML作业开始的时候，而当使用`partition_field_name`进行拆分时，“时间的黎明”是那个分区首次出现在数据中的时候。因此，如果您在出现“新事物”的情况下以不同方式拆分，您将得到不同的结果。
- en: Is double-splitting the limit?
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 双重拆分是极限吗？
- en: As mentioned, by using both `partition_field_name` and `by_field_name` when
    in the advanced job wizard, you can effectively get a double-split. But, if you
    need to split more, you'll have to rely on some other methods. Namely, you'll
    have to create a **scripted field** that is a concatenation of two (or more) fields.
    Using scripted fields is something that is covered in one of the examples in the
    [*Appendix*](B17040_14_Epub_AM.xhtml#_idTextAnchor248).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在高级作业向导中使用`partition_field_name`和`by_field_name`，您可以有效地实现双重拆分。但是，如果您需要拆分更多，您将不得不依赖其他方法。具体来说，您需要创建一个**脚本字段**，它是两个（或更多）字段的连接。使用脚本字段是[*附录*](B17040_14_Epub_AM.xhtml#_idTextAnchor248)中某个示例中涵盖的内容。
- en: Now that we have learned about the concept of splitting the analysis, let's
    focus on the differences between temporal and population analysis in anomaly detection.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了分析拆分的概念，让我们关注异常检测中时间序列分析和人口分析之间的差异。
- en: Understanding temporal versus population analysis
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解时间序列分析 versus 人口分析
- en: 'We learned back in [*Chapter 1*](B17040_01_Epub_AM.xhtml#_idTextAnchor016),
    *Machine Learning for IT*, that there are effectively two ways to consider something
    as anomalous:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第一章*](B17040_01_Epub_AM.xhtml#_idTextAnchor016)中学习了《IT机器学习》，了解到将某事物视为异常实际上有两种有效的方法：
- en: Whether or not something changes drastically with respect to its own behavior
    over time
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否随着时间的推移，某事物的行为发生了剧烈变化
- en: Whether or not something is drastically different when compared to its peers
    in an otherwise homogeneous population
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他同质群体中的同类相比，某事物是否发生了剧烈的不同
- en: By default, the former (which we'll simply call temporal analysis) is the mode
    used *unless* the `over_field_name` setting is specified in the detector config.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，前者（我们将其简单地称为时间序列分析）是检测器配置中未指定`over_field_name`设置时使用的模式。
- en: 'Population analysis can be very useful in finding outliers in a variety of
    important use cases. For example, perhaps we want to find machines that are logging
    more (or less) than similarly configured machines in the following scenarios:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 人口分析在寻找各种重要用例中的异常值时非常有用。例如，我们可能希望在下述场景中找到记录更多（或更少）的机器：
- en: Incorrect configuration changes that have caused more errors to suddenly occur
    in the log file for the system or application.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不正确的配置更改导致系统或应用程序的日志文件中突然出现更多错误。
- en: A system that might be compromised by malware may actually be instructed to
    suppress logging in certain situations, thus drastically decreasing the log volume.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能被恶意软件破坏的系统可能实际上被指示在某些情况下抑制日志记录，从而大幅减少日志量。
- en: A system that has lost connectivity or has operationally failed, thus having
    its log volume diminished.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个失去连接或操作失败的系统，因此其日志量减少。
- en: An otherwise harmless change to a logging-level setting (debug instead of normal),
    now annoyingly making your logs take up more disk space.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对日志级别设置（从正常改为调试）的更改，原本是无害的，但现在却令人烦恼地使日志占用更多的磁盘空间。
- en: 'Another way population analysis is often used is with respect to **User/Entity
    Behavioral Analysis** (**EUBA**), where a comparison of an entity''s or human''s
    actions compared against their peers might reveal the following:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种人口分析经常使用的方式是与**用户/实体行为分析**（**EUBA**）相关，其中将实体或人类的行为与他们的同龄人进行比较，可能会揭示以下情况：
- en: '**Automated users**: Instead of the typical human behavior or usage pattern,
    an automated script may exhibit behavioral patterns that look quite different
    in terms of the speed, duration, and diversity of events they create. Whether
    it is finding a crawler trying to harvest the products and prices of an online
    catalog or detecting a bot that might be engaged in the spread of misinformation
    on social media, the automatic identification of automated users can be helpful.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化用户**：与典型的人类行为或使用模式不同，自动化脚本可能表现出在事件的速度、持续时间和多样性方面看起来相当不同的行为模式。无论是寻找试图收集在线目录中的产品和价格的网络爬虫，还是检测可能在社会媒体上传播虚假信息的机器人，自动识别自动化用户可能是有帮助的。'
- en: '`distinct_count` function can help find a snooper.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distinct_count` 函数可以帮助找到间谍。'
- en: '**Malicious/abusive users**: After the reconnaissance phase, a malicious user
    or malware will move on to actively wreaking havoc and will engage in active measures
    such as denial of service, brute-forcing, or stealing valuable information. Again,
    compared with typical users, malicious and abusive users have stark contrasts
    in their behavior regarding the volume, diversity, and intensity of activity per
    unit of time.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**恶意/滥用用户**：在侦察阶段之后，恶意用户或恶意软件将转向积极造成破坏，并采取主动措施，如拒绝服务、暴力破解或窃取有价值的信息。再次强调，与典型用户相比，恶意和滥用用户在每单位时间的行为量、多样性和强度方面有明显的差异。'
- en: 'A practical example might be to find a customer that drastically spends a lot
    more than their peers. Whether or not you do this in the context of proactively
    investigating potential fraud, or whether you are interested in increasing the
    marketing to your most affluent customers, you still need to find those outliers.
    If we were to use the `kibana_sample_data_ecommerce` index that we added earlier
    in the chapter, we could create a population job by selecting the `customer_full_name.keyword`
    field for `taxful_total_price` field, which is the total revenue for each order
    placed by individuals:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 找到一个客户比他们的同龄人花费多得多的实际例子。无论你是否在主动调查潜在欺诈的情况下做这件事，或者你是否对增加对最富裕客户的营销感兴趣，你仍然需要找到这些异常值。如果我们使用本章早期添加的`kibana_sample_data_ecommerce`索引，我们可以通过选择`customer_full_name.keyword`字段为`taxful_total_price`字段创建一个群体任务，这是每个个人订单的总收入：
- en: '![Figure 3.25 – Population analysis of revenue over users'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.25 – 用户收入群体分析](img/B17040_03_025.jpg)'
- en: '](img/B17040_03_025.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_03_025.jpg)'
- en: Figure 3.25 – Population analysis of revenue over users
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.25 – 用户收入群体分析
- en: 'After this job is executed, you should see the following results:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此任务后，你应该看到以下结果：
- en: '![Figure 3.26 – Population analysis results of the biggest spenders'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.26 – 最大支出者的群体分析结果](img/B17040_03_026.jpg)'
- en: '](img/B17040_03_026.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_03_026.jpg)'
- en: Figure 3.26 – Population analysis results of the biggest spenders
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.26 – 最大支出者的群体分析结果
- en: Here, in *Figure 3.26*, we see that the list of the most unusual users (in this
    case, the biggest spenders per unit of time) is dominated by a user named **Wagdi
    Shaw**, who apparently placed an order for $2,250 worth of goods. The astute among
    you will recognize this anomaly from an earlier example—except this time, we are
    orienting our analysis around the user, not around the inventory category.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，在*图3.26*中，我们看到最不寻常的用户列表（在这种情况下，每单位时间的最大支出者）主要由名为**Wagdi Shaw**的用户主导，他显然订购了价值2250美元的商品。你们中敏锐的人会从早期示例中识别出这个异常——但这次，我们分析的重点是用户，而不是库存类别。
- en: As you can see, population analysis can be very powerful and is heavily used
    in use cases in which individual entities are targeted. Thus, it is very useful
    in security analytics use cases. Let's now pivot to focus on one additional, but
    powerful, capability of Elastic ML's anomaly detection—the ability to effectively
    analyze unstructured log messages via a process called categorization.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，人口分析可以非常强大，并且在针对单个实体的用例中得到了广泛的应用。因此，它在安全分析用例中非常有用。现在，让我们转向关注Elastic ML的异常检测的一个额外但强大的功能——通过称为分类的过程有效地分析非结构化日志消息的能力。
- en: Categorization analysis of unstructured messages
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非结构化消息的分类分析
- en: 'Imagine that you are troubleshooting a problem by looking at a particular log
    file. You see a line in the log that looks like the following:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在通过查看特定的日志文件来排查问题。你看到日志中的一行，看起来如下：
- en: '[PRE2]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Unless you have some intimate knowledge about the inner workings of the application
    that created this log, you may not know whether the message is important. Having
    the database be `Not Updated` possibly sounds like a negative situation. However,
    if you knew that the application routinely writes this message, day in and day
    out, several hundred times per hour, then you would naturally realize that this
    message is benign and should possibly be ignored, because clearly the application
    works fine every day despite this message being written to the log file.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你对创建此日志的应用程序的内部工作原理有深入了解，否则你可能不知道这条消息是否重要。数据库被标记为“未更新”可能听起来像是一种负面情况。然而，如果你知道该应用程序例行地每小时写这条消息几百次，那么你自然会意识到这条消息是无害的，可能应该被忽略，因为显然应用程序每天都能正常工作，尽管这条消息被写入日志文件。
- en: The problem, obviously, is one of human interpretation. Inspection of the text
    of the message and the reading of a negative phrase (`Not Updated`) potentially
    biases a person toward thinking that the message is noteworthy because of a possible
    problem. However, the frequency of the message (it happens routinely) should inform
    the person that the message must not be that important because the application
    is working (that is, there are no reported outages) despite these messages being
    written to the log.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，问题是人类解释。检查消息文本和阅读一个负面短语（“未更新”）可能会使一个人倾向于认为这条消息值得关注，因为可能存在问题。然而，消息的频率（它例行发生）应该让这个人知道这条消息可能并不重要，因为应用程序正在运行（也就是说，没有报告故障），尽管这些消息被写入日志。
- en: It can be hard for a human to process that information (assess the message content/relevance
    and also the frequency over time) for just a few types of messages in a log file.
    Imagine if there were thousands of unique message types occurring at a total rate
    of millions of log lines per day. Even the most seasoned expert in both the application
    content and search/visualizations will find this impractical, if not impossible,
    to wrangle.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 对于日志文件中仅几种类型的消息，人类处理这些信息（评估消息内容/相关性以及随时间的变化频率）可能很困难。想象一下，如果每天有数百万条独特的消息类型以每天数百万条日志行的总速率发生。即使是最有经验的在应用程序内容和搜索/可视化方面的专家，也会发现这既不实际，甚至可能不可能处理。
- en: Elastic ML comes to the rescue with capabilities that allow the empirical assessment
    of both the uniqueness of the content of the messages and the relative frequency
    of occurrence.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Elastic ML通过允许对消息内容的唯一性和相对发生频率进行经验评估的能力来提供帮助。
- en: Types of messages that are good candidates for categorization
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适合分类的消息类型
- en: We need to be a little rigorous in our definition of the kinds of message-based
    log lines that are good for this kind of analysis. What we are *not* considering
    are log lines/events/documents that are completely freeform and likely the result
    of human creation (emails, tweets, comments, and so on). These kinds of messages
    are too arbitrary and variable in their construction and content.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对我们定义的消息日志行类型进行一定的严谨性，以适用于这种分析。我们**不考虑**的是完全自由形式的日志行/事件/文档，这些可能是人类创作的结果（电子邮件、推文、评论等）。这类消息在构建和内容上过于随意和多变。
- en: 'Instead, we will focus on machine-generated messages that are obviously emitted
    when an application encounters different situations or exceptions, thus constraining
    their construction and content to a relatively discrete set of possibilities (understanding
    that there may indeed be some variable aspects of the message). For example, let''s
    look at the following few lines of an application log:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们将专注于机器生成的消息，这些消息在应用程序遇到不同情况或异常时显然会被发出，从而将它们的构建和内容限制在相对离散的可能性的集合中（理解到消息确实可能有一些可变方面）。例如，让我们看看以下几行应用程序日志：
- en: '[PRE3]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we can see that there is a variety of messages with different text in
    each, but there is some structure here. After the date/time stamp and the server
    name from which the message originates (here, `ACME6`), there is the actual meat
    of the message, where the application is informing the outside world what is happening
    at that moment—whether something is being tried or errors are occurring.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到有各种消息，每个消息中的文本都不同，但这里有一些结构。在日期/时间和消息来源的服务器名称（这里为`ACME6`）之后，是消息的实际内容，其中应用程序正在向外界告知当时正在发生的事情——是正在尝试某事还是发生了错误。
- en: The process used by categorization
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类过程使用的算法
- en: 'In order to bring some order to the otherwise disorderly flow of the messages
    in the log file, Elastic ML will employ a technique of grouping similar messages
    together by using a string-similarity clustering algorithm. The heuristics behind
    this algorithm are roughly as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使日志文件中消息的无序流动变得有序，Elastic ML将采用一种通过使用字符串相似度聚类算法将相似消息分组在一起的技术。该算法背后的启发式方法大致如下：
- en: Focus on the (English) dictionary words more than `network` and `address` are
    dictionary words, but `dbmssocn` is likely a mutable/variable string).
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重点关注（英语）词典中的单词，因为`network`和`address`是词典中的单词，但`dbmssocn`可能是一个可变/变量的字符串）。
- en: Pass the immutable dictionary words through a string-similarity algorithm (similar
    to the **Levenshtein distance**) to determine how similar the log line is to past
    log lines.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将不可变的词典单词通过字符串相似度算法（类似于**Levenshtein距离**）传递，以确定日志行与过去日志行的相似程度。
- en: If the difference between the current log line and an existing category is small,
    then group the existing log line into that category. Otherwise, create a new category
    for the current log line.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果当前日志行与现有类别之间的差异很小，那么将现有日志行分组到该类别中。否则，为当前日志行创建一个新类别。
- en: 'As a simple example, consider these three messages:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 作为简单的例子，考虑以下三条消息：
- en: '[PRE4]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The algorithm would cluster the first two messages together in the same category,
    as they would be deemed as `Error writing file on` types of messages, whereas
    the third message would be given its own (new) category.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法会将前两条消息聚类到同一个类别中，因为它们会被视为`Error writing file on`类型的消息，而第三条消息将获得其自己的（新）类别。
- en: 'The naming of these categories is simple: ML will just call them `mlcategory
    N`, where `N` is an incrementing integer. Therefore, in this example, the first
    two lines will be associated with `mlcategory 1`, and the third line will be associated
    with `mlcategory 2`. In a realistic machine log, there may be thousands (or even
    tens of thousands) of categories that are generated due to the diversity of the
    log messages, but the set of possible categories should be finite. However, if
    the number of categories starts to get into the hundreds of thousands, it may
    become obvious that the log messages are not a constrained set of possible message
    types and will not be a good candidate for this type of analysis.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类别的命名很简单：ML将简单地称它们为`mlcategory N`，其中`N`是一个递增的整数。因此，在这个例子中，前两行将与`mlcategory
    1`相关联，而第三行将与`mlcategory 2`相关联。在现实机器日志中，可能由于日志消息的多样性，会有成千上万（甚至数十万）个类别被生成，但可能的类别集合应该是有限的。然而，如果类别的数量开始达到数十万，可能就会明显看出日志消息不是一个受约束的可能消息类型的集合，并且不会是这种分析的好候选。
- en: Analyzing the categories
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析类别
- en: 'Now that the messages are going to be categorized by the algorithm described
    previously, the next part of the process is to do the analysis (using either `count`
    or `rare`). In this case, we''re not going to be counting the log lines (and thus
    the documents of an Elasticsearch index) themselves; instead, we''re going to
    be counting the occurrence rate of the different categories that are the output
    of the algorithm. So, for example, given the example log lines in the previous
    section, if they occurred within the same bucket span, we would have the following
    output of the categorization algorithm:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在消息将要被前面描述的算法进行分类，接下来过程的下一步是对其进行分析（使用`count`或`rare`）。在这种情况下，我们不会计算日志行（以及Elasticsearch索引中的文档）本身；相反，我们将计算算法输出的不同类别的出现频率。例如，给定前一个部分中的示例日志行，如果它们发生在同一个桶跨度内，分类算法的输出如下：
- en: '[PRE5]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In other words, there were two occurrences of the `Error writing file on` types
    of messages and one occurrence of the `Opening database on host` type in the last
    bucket span interval. It is this information that will ultimately be modeled by
    the ML job in order to determine whether it is unusual.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，在最后一个桶跨度间隔中，出现了两次`Error writing file on`类型的消息和一次`Opening database on host`类型的消息。这些信息最终将由ML作业建模，以确定其是否异常。
- en: Categorization job example
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类作业示例
- en: 'With the categorization job wizard in the UI, the process of configuring this
    type of job is extremely easy. Let''s first assume we have an unstructured log
    file ingested (perhaps such as the `secure.log` file in the `example_data` folder
    on GitHub):'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在UI中的分类作业向导中，配置此类作业的过程非常简单。让我们首先假设我们有一个未结构化的日志文件被摄取（例如，GitHub上`example_data`文件夹中的`secure.log`文件）：
- en: Note
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on how to ingest data using the File Visualizer, see the
    detailed blog post at [elastic.co/blog/importing-csv-and-log-data-into-elasticsearch-with-file-data-visualizer](http://elastic.co/blog/importing-csv-and-log-data-into-elasticsearch-with-file-data-visualizer).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何使用文件可视化器摄取数据的更多信息，请参阅[elastic.co](http://elastic.co/blog/importing-csv-and-log-data-into-elasticsearch-with-file-data-visualizer)上的详细博客文章。
- en: After picking the index of interest and choosing the Categorization wizard,
    and then selecting the appropriate time range for the analysis, we see that the
    wizard will ask us which `@timestamp` and `message`). Therefore, the `message`
    field is the field we would like Elastic ML to categorize. We will also pick the
    **Count** detector in this example:![Figure 3.27 – Categorization job configuration
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在选择感兴趣的索引和选择分类向导后，然后选择分析的时间范围，我们看到向导将询问我们哪个`@timestamp`和`message`)。因此，`message`字段是我们希望Elastic
    ML进行分类的字段。在这个例子中，我们还将选择**计数**检测器：![图3.27 – 分类作业配置
- en: '](img/B17040_03_027.jpg)'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片 B17040_03_027.jpg]'
- en: Figure 3.27 – Categorization job configuration
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.27 – 分类作业配置
- en: Notice in *Figure 3.27* that there is a check on the selected category field
    to make sure it will yield sensible results. Also notice that in the **Examples**
    section, you get visual confirmation of Elastic ML focusing on the non-mutable
    text of the log messages.
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意在*图3.27*中，对所选类别字段进行了检查，以确保它将产生合理的结果。同时注意，在**示例**部分，你可以通过视觉确认Elastic ML专注于日志消息的非可变文本。
- en: Once the configuration is confirmed and the job is started in the wizard, you
    will see a preview of the results as they are being discovered and analyzed:![Figure
    3.28 – Categorization job execution preview
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦确认配置并在向导中启动作业，你将看到结果的预览，这些结果正在被发现和分析：![图3.28 – 分类作业执行预览
- en: '](img/B17040_03_028.jpg)'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片 B17040_03_028.jpg]'
- en: Figure 3.28 – Categorization job execution preview
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.28 – 分类作业执行预览
- en: Notice that in this simple example, a total of 23 categories were discovered
    in the data. When the results are viewed in the Anomaly Explorer, we see that
    the top anomaly here is `mlcategory` number 7\.
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，在这个简单的例子中，数据中共发现了23个类别。当在异常探索器中查看结果时，我们看到这里的顶级异常是`mlcategory`编号7。
- en: When you click on the `Received disconnect` messages.
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你点击`Received disconnect`消息时。
- en: By clicking on the gear icon, as shown in *Figure 3.29*, we can select **View
    examples** to transport us over to the Kibana Discover UI, but filtered to this
    appropriate message and zoomed into the relevant timeframe:![Figure 3.30 – Inspecting
    raw log lines from the categorization job results
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过单击图3.29中显示的齿轮图标，我们可以选择**查看示例**，这会将我们带到Kibana Discover UI，但仅过滤到适当的消息并缩放到相关的时间段：![图3.30
    – 检查分类作业结果中的原始日志行
- en: '](img/B17040_03_030.jpg)'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片 B17040_03_030.jpg]'
- en: Figure 3.30 – Inspecting raw log lines from the categorization job results
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.30 – 检查分类作业结果中的原始日志行
- en: Notice that the Discover query bar has been automatically filled in with an
    appropriate KQL query to limit our view to the kind of messages that were anomalous.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，发现查询栏已自动填充了适当的KQL查询，以限制我们的视图仅限于异常的消息类型。
- en: 'If we were to remove that query filter, we would see all of the messages in
    the log file at the time of this anomaly, and we would see the bigger story, which
    is that someone or something was attempting a lot of authentications:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们删除那个查询过滤器，我们将看到这个异常发生时的日志文件中的所有消息，并且我们会看到更大的故事，即有人或某物正在尝试进行大量身份验证：
- en: '![Figure 3.31 – Inspecting all log lines during the time of the anomaly'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.31 – 检查异常发生时的所有日志行'
- en: '](img/B17040_03_031.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_03_031.jpg)'
- en: Figure 3.31 – Inspecting all log lines during the time of the anomaly
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.31 – 检查异常发生时的所有日志行
- en: As we can see in *Figure 3.31*, there seems to be a flurry of authentication
    attempts using well-known usernames (`user`, `test`, and so on). Looks like we
    found a brute-force authentication attempt merely by using categorization!
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*图3.31*中看到的，似乎有大量的使用知名用户名（如`user`、`test`等）的认证尝试。看起来我们只是通过分类就找到了暴力认证尝试！
- en: When to avoid using categorization
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当避免使用分类时
- en: 'Despite categorization being quite useful, it''s not without its limitations.
    Specifically, here are some cases where attempting to use categorization will
    likely return poor results:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管分类非常有用，但它并非没有局限性。具体来说，以下是一些尝试使用分类可能返回较差结果的情况：
- en: With fields of text that are freeform, likely created by humans. Examples include
    tweets, comments, emails, and notes.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自由形式的文本字段，这些文本很可能是由人类创建的。例如，包括推文、评论、电子邮件和笔记等。
- en: With log lines that should otherwise really be parsed into proper name/value
    pairs, such as a web access log.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于应该被解析成正确的名称/值对的日志行，例如Web访问日志。
- en: With documents that contain a lot of multi-line text. This would include stack
    traces, XML, and so on.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于包含大量多行文本的文档。这包括堆栈跟踪、XML等。
- en: With that said, we can see that categorization can still be extremely useful
    in cases where analyzing unstructured text would otherwise be an increased burden
    on a human analyst.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们可以看到，在分析非结构化文本可能会给人类分析师带来额外负担的情况下，分类仍然可以非常有用。
- en: Managing Elastic ML via the API
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过API管理Elastic ML
- en: As with just about everything in the Elastic Stack, ML can also be completely
    automated via API calls—including job configuration, execution, and result gathering.
    Actually, all interactions you have in the Kibana UI leverage the ML API behind
    the scenes. You could, for example, completely write your own UI if there were
    specific workflows or visualizations that you wanted.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 就像Elastic Stack中的几乎所有事情一样，ML也可以通过API调用完全自动化——包括作业配置、执行和结果收集。实际上，您在Kibana UI中的所有交互都利用了背后的ML
    API。例如，如果您想编写自己的UI，并且有特定的工作流程或可视化需求，您完全可以这样做。
- en: Note
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For more in-depth information about the anomaly detection APIs, please refer
    to [elastic.co/guide/en/machine-learning/current/ml-api-quickref.html](http://elastic.co/guide/en/machine-learning/current/ml-api-quickref.html).
    The data frame analytics part of Elastic ML has a completely separate API, which
    will be discussed in *Chapters 9* to *13*.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 有关异常检测API的更多信息，请参阅[elastic.co/guide/en/machine-learning/current/ml-api-quickref.html](http://elastic.co/guide/en/machine-learning/current/ml-api-quickref.html)。Elastic
    ML的数据帧分析部分有一个完全独立的API，将在第9章到第13章中进行讨论。
- en: We won't go into each API call, but we would like to highlight some parts that
    are worth a detour.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入到每个API调用，但我们想强调一些值得注意的部分。
- en: 'The obvious first API to mention is the job creation API, which allows the
    creation of the ML job configuration. For example, if you wanted to recreate the
    population analysis job shown in *Figure 3.25*, the following call would create
    that job, which we will call `revenue_over_users_api`:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要提到的明显API是作业创建API，它允许创建ML作业配置。例如，如果您想重新创建*图3.25*中显示的种群分析作业，以下调用将创建该作业，我们将称之为`revenue_over_users_api`：
- en: '[PRE6]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that the `job_id` field needs to be unique when creating the job.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，创建作业时`job_id`字段必须是唯一的。
- en: 'In order to create the companion datafeed configuration for this job, we would
    issue this separate API call:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为这个作业创建配套的数据源配置，我们会发出这个单独的API调用：
- en: '[PRE7]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Notice that the default query to the index is `match_all`, which means that
    no filtering will take place. We could, of course, insert any valid Elasticsearch
    DSL in the query block to perform custom filters or aggregations. This concept
    will be covered later in the book.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，默认的索引查询是`match_all`，这意味着不会进行任何过滤。当然，我们可以在查询块中插入任何有效的Elasticsearch DSL来执行自定义过滤或聚合。这个概念将在本书的后续部分进行介绍。
- en: There are other APIs that can be used to extract results or modify other operational
    aspects of the ML job. Consult the online documentation for more information.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 有其他API可以用来提取结果或修改ML作业的其他操作方面。有关更多信息，请参阅在线文档。
- en: Summary
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We've seen that Elastic ML can highlight variations in volume, diversity, and
    uniqueness in metrics and log messages, including those that need some categorization
    first. Also, we've shown that population analysis can be an extremely interesting
    alternative to temporal anomaly detection when the focus is more on finding the
    most unusual entities. These techniques help solve the challenges we described
    before, where a human might struggle to recognize what is truly unusual and worthy
    of attention and investigation.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，Elastic ML可以突出指标和日志消息中的数量、多样性和独特性的变化，包括那些需要先进行一些分类的。此外，我们还展示了当重点更多地放在寻找最不寻常的实体时，人口分析可以成为时间异常检测的一个极其有趣的替代方案。这些技术有助于解决我们之前描述的挑战，在这些挑战中，人类可能难以识别真正不寻常且值得注意和调查的事物。
- en: The skills learned in this chapter will be helpful in subsequent chapters, where
    we will see how ML assists in the process of getting to the root cause of complex
    IT problems, identifying application performance slowdowns, or when ML can assist
    in the identification of malware and/or malicious activity.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 本章学到的技能将在后续章节中有所帮助，其中我们将看到机器学习如何协助找到复杂IT问题的根本原因，识别应用程序性能的放缓，或者当机器学习可以协助识别恶意软件和/或恶意活动时。
- en: In the next chapter, we'll see how the expressive time series models built by
    anomaly detection jobs can be leveraged to forecast trends of your data into the
    future.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到如何利用异常检测作业构建的表达式时间序列模型来预测数据未来的趋势。
