- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Combining Different Models for Ensemble Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组合不同的模型进行集成学习
- en: 'In the previous chapter, we focused on the best practices for tuning and evaluating
    different models for classification. In this chapter, we will build upon those
    techniques and explore different methods for constructing a set of classifiers
    that can often have a better predictive performance than any of its individual
    members. We will learn how to do the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们专注于调整和评估不同分类模型的最佳实践。在本章中，我们将进一步探讨这些技术，并探索构建一组分类器的不同方法，这些方法通常比其各个成员单独具有更好的预测性能。我们将学习如何执行以下操作：
- en: Make predictions based on majority voting
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于多数投票进行预测
- en: Use bagging to reduce overfitting by drawing random combinations of the training
    dataset with repetition
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用装袋减少过拟合，通过重复绘制训练数据集的随机组合
- en: Apply boosting to build powerful models from weak learners that learn from their
    mistakes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将提升应用于从其错误中学习的弱学习器构建强大的模型
- en: Learning with ensembles
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用集成学习
- en: The goal of **ensemble methods** is to combine different classifiers into a
    meta-classifier that has better generalization performance than each individual
    classifier alone. For example, assuming that we collected predictions from 10
    experts, ensemble methods would allow us to strategically combine those predictions
    by the 10 experts to come up with a prediction that was more accurate and robust
    than the predictions by each individual expert. As you will see later in this
    chapter, there are several different approaches for creating an ensemble of classifiers.
    This section will introduce a basic explanation of how ensembles work and why
    they are typically recognized for yielding a good generalization performance.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成方法**的目标是将不同的分类器组合成一个元分类器，其泛化性能比单个分类器更好。例如，假设我们从10个专家的预测中收集了预测结果，集成方法允许我们通过这10个专家的预测策略性地组合这些预测结果，以得出比每个单独专家预测更准确和更稳健的预测结果。正如本章后面将看到的那样，有几种不同的方法可以创建一组分类器的集成。本节将介绍集成如何工作的基本解释，以及为什么它们通常因产生良好的泛化性能而受到认可。'
- en: 'In this chapter, we will focus on the most popular ensemble methods that use
    the **majority voting** principle. Majority voting simply means that we select
    the class label that has been predicted by the majority of classifiers, that is,
    received more than 50 percent of the votes. Strictly speaking, the term "majority
    vote" refers to binary class settings only. However, it is easy to generalize
    the majority voting principle to multiclass settings, which is called **plurality
    voting**. Here, we select the class label that received the most votes (the mode).
    The following diagram illustrates the concept of majority and plurality voting
    for an ensemble of 10 classifiers, where each unique symbol (triangle, square,
    and circle) represents a unique class label:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点介绍使用**多数投票**原则的最流行的集成方法。多数投票简单地意味着我们选择由大多数分类器预测的类标签，即获得超过50%选票的类标签。严格来说，“多数投票”一词仅适用于二元分类设置。然而，很容易将多数投票原则推广到多类设置，这称为**多数投票**。在这里，我们选择获得最多票数（众数）的类标签。下图说明了对10个分类器的集成进行多数投票和多数投票的概念，其中每个唯一符号（三角形、正方形和圆形）表示唯一的类标签：
- en: '![](img/B13208_07_01.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_01.png)'
- en: 'Using the training dataset, we start by training *m* different classifiers
    (![](img/B13208_07_001.png)). Depending on the technique, the ensemble can be
    built from different classification algorithms, for example, decision trees, support
    vector machines, logistic regression classifiers, and so on. Alternatively, we
    can also use the same base classification algorithm, fitting different subsets
    of the training dataset. One prominent example of this approach is the random
    forest algorithm, which combines different decision tree classifiers. The following
    figure illustrates the concept of a general ensemble approach using majority voting:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练数据集，我们首先通过训练*m*不同的分类器（![](img/B13208_07_001.png)）来开始。根据技术的不同，集成可以由不同的分类算法构建，例如决策树、支持向量机、逻辑回归分类器等。或者，我们也可以使用相同的基础分类算法，适合训练数据集的不同子集。这种方法的一个显著例子是随机森林算法，它结合了不同的决策树分类器。下图说明了使用多数投票的一般集成方法的概念：
- en: '![](img/B13208_07_02.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_02.png)'
- en: 'To predict a class label via simple majority or plurality voting, we can combine
    the predicted class labels of each individual classifier, ![](img/B13208_07_002.png),
    and select the class label, ![](img/B13208_07_003.png), that received the most
    votes:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过简单的多数或选举投票来预测类别标签，我们可以将每个单独分类器预测的类别标签结合起来，![](img/B13208_07_002.png)，并选择获得最多票数的类别标签，![](img/B13208_07_003.png)：
- en: '![](img/B13208_07_004.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_004.png)'
- en: (In statistics, the mode is the most frequent event or result in a set. For
    example, mode{1, 2, 1, 1, 2, 4, 5, 4} = 1.)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: （在统计学中，众数是一个集合中最频繁出现的事件或结果。例如，mode{1, 2, 1, 1, 2, 4, 5, 4} = 1。）
- en: 'For example, in a binary classification task where class1 = –1 and class2 =
    +1, we can write the majority vote prediction as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个二分类任务中，其中class1 = –1，class2 = +1，我们可以将多数投票预测写作：
- en: '![](img/B13208_07_005.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_005.png)'
- en: 'To illustrate why ensemble methods can work better than individual classifiers
    alone, let''s apply the simple concepts of combinatorics. For the following example,
    we will make the assumption that all *n*-base classifiers for a binary classification
    task have an equal error rate, ![](img/B13208_07_006.png). Furthermore, we will
    assume that the classifiers are independent and the error rates are not correlated.
    Under those assumptions, we can simply express the error probability of an ensemble
    of base classifiers as a probability mass function of a binomial distribution:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明为什么集成方法比单一分类器更有效，我们可以应用组合学的简单概念。在以下示例中，我们假设所有用于二分类任务的*n*基分类器具有相同的错误率，![](img/B13208_07_006.png)。此外，我们假设分类器是独立的，且错误率之间没有相关性。在这些假设下，我们可以简单地将基分类器集成的错误概率表示为二项分布的概率质量函数：
- en: '![](img/B13208_07_007.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_007.png)'
- en: 'Here, ![](img/B13208_07_008.png) is the binomial coefficient *n choose k*.
    In other words, we compute the probability that the prediction of the ensemble
    is wrong. Now, let''s take a look at a more concrete example of 11 base classifiers
    (*n* = 11), where each classifier has an error rate of 0.25 (![](img/B13208_07_009.png)):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_07_008.png) 是二项系数 *n 选择 k*。换句话说，我们计算集成的预测错误的概率。现在，让我们看看一个更具体的例子，假设有11个基分类器（*n*
    = 11），每个分类器的错误率为0.25（![](img/B13208_07_009.png)）：
- en: '![](img/B13208_07_010.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_010.png)'
- en: '**The binomial coefficient**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**二项系数**'
- en: 'The binomial coefficient refers to the number of ways we can choose subsets
    of *k*-unordered elements from a set of size *n*; thus, it is often called "n
    choose k." Since the order does not matter here, the binomial coefficient is also
    sometimes referred to as *combination* or *combinatorial number*, and in its unabbreviated
    form, it is written as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 二项系数指的是从大小为 *n* 的集合中选择 *k* 个无序元素的方式数量；因此，它通常被称为“n 选择 k”。由于这里顺序无关，二项系数有时也被称为*组合*或*组合数*，在其完全形式下，它写作：
- en: '![](img/B13208_07_011.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_011.png)'
- en: Here, the symbol (!) stands for factorial—for example, ![](img/B13208_07_012.png).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，符号 (!) 代表阶乘，例如 ![](img/B13208_07_012.png)。
- en: 'As you can see, the error rate of the ensemble (0.034) is much lower than the
    error rate of each individual classifier (0.25) if all the assumptions are met.
    Note that, in this simplified illustration, a 50-50 split by an even number of
    classifiers, *n*, is treated as an error, whereas this is only true half of the
    time. To compare such an idealistic ensemble classifier to a base classifier over
    a range of different base error rates, let''s implement the probability mass function
    in Python:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，如果所有假设都成立，集成的错误率（0.034）明显低于每个单独分类器的错误率（0.25）。请注意，在这个简化的示例中，基分类器通过偶数个分类器进行50-50的划分时被视为错误，而实际上这种情况只在一半的时间内成立。为了将这样一个理想化的集成分类器与基分类器进行比较，我们将在不同的基错误率范围内实现概率质量函数，使用Python进行如下操作：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After we have implemented the `ensemble_error` function, we can compute the
    ensemble error rates for a range of different base errors from 0.0 to 1.0 to visualize
    the relationship between ensemble and base errors in a line graph:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现了 `ensemble_error` 函数之后，我们可以计算出一系列不同基分类器错误率从0.0到1.0的集成错误率，并使用折线图可视化集成错误与基分类器错误之间的关系：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As you can see in the resulting plot, the error probability of an ensemble
    is always better than the error of an individual base classifier, as long as the
    base classifiers perform better than random guessing (![](img/B13208_07_013.png)).
    Note that the *y* axis depicts the base error (dotted line) as well as the ensemble
    error (continuous line):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如结果图所示，只要基分类器的表现优于随机猜测（![](img/B13208_07_013.png)），集成的错误概率总是比单个基分类器的错误率更好。注意，*y*
    轴表示基误差（虚线）和集成误差（实线）：
- en: '![](img/B13208_07_03.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_03.png)'
- en: Combining classifiers via majority vote
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过多数投票组合分类器
- en: After the short introduction to ensemble learning in the previous section, let's
    start with a warm-up exercise and implement a simple ensemble classifier for majority
    voting in Python.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节简要介绍了集成学习后，让我们通过一个热身练习开始，使用 Python 实现一个简单的集成分类器进行多数投票。
- en: '**Plurality voting**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**多数投票**'
- en: Although the majority voting algorithm that we will discuss in this section
    also generalizes to multiclass settings via plurality voting, the term "majority
    voting" will be used for simplicity, as is often the case in the literature.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们将在本节讨论的多数投票算法也可以通过多数投票推广到多类问题，但为了简化起见，文献中通常使用“多数投票”这个术语。
- en: Implementing a simple majority vote classifier
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现一个简单的多数投票分类器
- en: 'The algorithm that we are going to implement in this section will allow us
    to combine different classification algorithms associated with individual weights
    for confidence. Our goal is to build a stronger meta-classifier that balances
    out the individual classifiers'' weaknesses on a particular dataset. In more precise
    mathematical terms, we can write the weighted majority vote as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节实现的算法将允许我们结合不同的分类算法，并为每个算法分配与置信度相关的权重。我们的目标是构建一个更强大的元分类器，平衡单个分类器在特定数据集上的弱点。用更精确的数学术语来说，我们可以将加权多数投票表示为：
- en: '![](img/B13208_07_014.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_014.png)'
- en: 'Here, ![](img/B13208_07_015.png) is a weight associated with a base classifier,
    ![](img/B13208_07_016.png); ![](img/B13208_07_017.png) is the predicted class
    label of the ensemble; A is the set of unique class labels; ![](img/B13208_07_018.png)
    (Greek chi) is the characteristic function or indicator function, which returns
    1 if the predicted class of the *j*th classifier matches *i* (![](img/B13208_07_019.png)).
    For equal weights, we can simplify this equation and write it as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_07_015.png) 是与基分类器 ![](img/B13208_07_016.png) 相关的权重；![](img/B13208_07_017.png)
    是集成预测的类别标签；A 是唯一类别标签的集合；![](img/B13208_07_018.png)（希腊字母 chi）是特征函数或指示函数，如果第 *j*
    个分类器的预测类别与 *i*（![](img/B13208_07_019.png)）匹配，则返回 1。对于相等的权重，我们可以简化这个方程并表示为：
- en: '![](img/B13208_07_020.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_020.png)'
- en: 'To better understand the concept of *weighting*, we will now take a look at
    a more concrete example. Let''s assume that we have an ensemble of three base
    classifiers, ![](img/B13208_07_021.png), and we want to predict the class label,
    ![](img/B13208_07_022.png), of a given example, *x*. Two out of three base classifiers
    predict the class label 0, and one, ![](img/B13208_07_023.png), predicts that
    the example belongs to class 1\. If we weight the predictions of each base classifier
    equally, the majority vote predicts that the example belongs to class 0:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解 *加权* 的概念，我们现在来看一个更具体的例子。假设我们有一个由三个基分类器组成的集成！[](img/B13208_07_021.png)，并且我们想要预测给定样本
    *x* 的类别标签！[](img/B13208_07_022.png)。三个基分类器中有两个预测类别为 0，一个预测类别为 1。如果我们对每个基分类器的预测给予相同的权重，那么多数投票将预测该样本属于类别
    0：
- en: '![](img/B13208_07_024.png)![](img/B13208_07_025.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_024.png)![](img/B13208_07_025.png)'
- en: 'Now, let''s assign a weight of 0.6 to ![](img/B13208_07_026.png), and let''s
    weight ![](img/B13208_07_027.png) and ![](img/B13208_07_028.png) by a coefficient
    of 0.2:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们给 ![](img/B13208_07_026.png) 赋予 0.6 的权重，并将 ![](img/B13208_07_027.png)
    和 ![](img/B13208_07_028.png) 的权重设置为 0.2：
- en: '![](img/B13208_07_029.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_029.png)'
- en: 'More simply, since ![](img/B13208_07_030.png), we can say that the prediction
    made by ![](img/B13208_07_031.png) has three times more weight than the predictions
    by ![](img/B13208_07_032.png) or ![](img/B13208_07_033.png), which we can write
    as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单地说，由于 ![](img/B13208_07_030.png)，我们可以说，由 ![](img/B13208_07_031.png) 做出的预测比
    ![](img/B13208_07_032.png) 或 ![](img/B13208_07_033.png) 的预测重三倍，表示为：
- en: '![](img/B13208_07_034.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_034.png)'
- en: 'To translate the concept of the weighted majority vote into Python code, we
    can use NumPy''s convenient `argmax` and `bincount` functions:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要将加权多数投票的概念转化为 Python 代码，我们可以使用 NumPy 中方便的 `argmax` 和 `bincount` 函数：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As you will remember from the discussion on logistic regression in *Chapter
    3*, *A Tour of Machine Learning Classifiers Using scikit-learn*, certain classifiers
    in scikit-learn can also return the probability of a predicted class label via
    the `predict_proba` method. Using the predicted class probabilities instead of
    the class labels for majority voting can be useful if the classifiers in our ensemble
    are well calibrated. The modified version of the majority vote for predicting
    class labels from probabilities can be written as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从 *第 3 章* 中的逻辑回归讨论中记得的那样，*《使用 scikit-learn 的机器学习分类器之旅》*，scikit-learn 中某些分类器也可以通过
    `predict_proba` 方法返回预测的类标签的概率。使用预测的类概率而不是类标签进行多数投票，如果我们的集成中的分类器经过良好校准，这种做法是非常有用的。根据概率预测类标签的修改版多数投票可以写成如下：
- en: '![](img/B13208_07_035.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_035.png)'
- en: Here, ![](img/B13208_07_036.png) is the predicted probability of the *j*th classifier
    for class label *i*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里， ![](img/B13208_07_036.png) 是 *j*th 分类器对于类标签 *i* 的预测概率。
- en: 'To continue with our previous example, let''s assume that we have a binary
    classification problem with class labels ![](img/B13208_07_037.png) and an ensemble
    of three classifiers, ![](img/B13208_07_038.png). Let''s assume that the classifiers
    ![](img/B13208_07_039.png) return the following class membership probabilities
    for a particular example, *x*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们之前的例子，假设我们有一个二分类问题，类标签是 ![](img/B13208_07_037.png)，并且有三个分类器的集成 ![](img/B13208_07_038.png)。假设分类器
    ![](img/B13208_07_039.png) 为某个特定示例 *x* 返回以下的类成员概率：
- en: '![](img/B13208_07_040.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_040.png)'
- en: 'Using the same weights as previously (0.2, 0.2, and 0.6), we can then calculate
    the individual class probabilities as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前相同的权重（0.2、0.2 和 0.6），然后可以计算各个类的概率，如下所示：
- en: '![](img/B13208_07_041.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_041.png)'
- en: 'To implement the weighted majority vote based on class probabilities, we can
    again make use of NumPy, using `np.average` and `np.argmax`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现基于类概率的加权多数投票，我们可以再次使用 NumPy，利用 `np.average` 和 `np.argmax`：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Putting everything together, let''s now implement `MajorityVoteClassifier`
    in Python:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 把所有的内容组合起来，现在我们来实现 Python 中的 `MajorityVoteClassifier`：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: I've added a lot of comments to the code to explain the individual parts. However,
    before we implement the remaining methods, let's take a quick break and discuss
    some of the code that may look confusing at first. We used the `BaseEstimator`
    and `ClassifierMixin` parent classes to get some base functionality *for free*,
    including the `get_params` and `set_params` methods to set and return the classifier's
    parameters, as well as the `score` method to calculate the prediction accuracy.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我在代码中添加了大量注释，来解释各个部分。然而，在我们实现剩下的方法之前，先休息一下，讨论一些可能一开始看起来有点困惑的代码。我们使用了 `BaseEstimator`
    和 `ClassifierMixin` 父类来获取一些基本功能，*无需额外编写代码*，包括 `get_params` 和 `set_params` 方法来设置和返回分类器的参数，以及
    `score` 方法来计算预测准确性。
- en: 'Next, we will add the `predict` method to predict the class label via a majority
    vote based on the class labels if we initialize a new `MajorityVoteClassifier`
    object with `vote=''classlabel''`. Alternatively, we will be able to initialize
    the ensemble classifier with `vote=''probability''` to predict the class label
    based on the class membership probabilities. Furthermore, we will also add a `predict_proba`
    method to return the averaged probabilities, which is useful when computing the
    receiver operating characteristic area under the curve (ROC AUC):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将添加 `predict` 方法，通过基于类标签的多数投票来预测类标签，如果我们用 `vote='classlabel'` 初始化新的 `MajorityVoteClassifier`
    对象。或者，我们也可以用 `vote='probability'` 来初始化集成分类器，通过类成员概率来预测类标签。此外，我们还会添加一个 `predict_proba`
    方法，用来返回平均后的概率，这在计算接收者操作特征曲线下面积（ROC AUC）时非常有用：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Also, note that we defined our own modified version of the `get_params` method
    to use the `_name_estimators` function to access the parameters of individual
    classifiers in the ensemble; this may look a little bit complicated at first,
    but it will make perfect sense when we use grid search for hyperparameter tuning
    in later sections.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请注意，我们定义了自己修改版的 `get_params` 方法，通过使用 `_name_estimators` 函数来访问集成中各个分类器的参数；这可能一开始看起来有点复杂，但当我们在后续章节使用网格搜索进行超参数调优时，这一做法就会变得非常有意义。
- en: '**VotingClassifier in scikit-learn**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**scikit-learn 中的 VotingClassifier**'
- en: Although the `MajorityVoteClassifier` implementation is very useful for demonstration
    purposes, we implemented a more sophisticated version of this majority vote classifier
    in scikit-learn based on the implementation in the first edition of this book.
    The ensemble classifier is available as `sklearn.ensemble.VotingClassifier` in
    scikit-learn version 0.17 and newer.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`MajorityVoteClassifier`的实现非常适用于演示目的，但我们基于本书第一版中的实现在scikit-learn中实现了一个更复杂的多数投票分类器版本。该集成分类器在scikit-learn版本0.17及更高版本中作为`sklearn.ensemble.VotingClassifier`可用。
- en: Using the majority voting principle to make predictions
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用多数投票原则进行预测
- en: 'Now it is time to put the `MajorityVoteClassifier` that we implemented in the
    previous section into action. But first, let''s prepare a dataset that we can
    test it on. Since we are already familiar with techniques to load datasets from
    CSV files, we will take a shortcut and load the Iris dataset from scikit-learn''s
    `datasets` module. Furthermore, we will only select two features, *sepal width*
    and *petal length*, to make the classification task more challenging for illustration
    purposes. Although our `MajorityVoteClassifier` generalizes to multiclass problems,
    we will only classify flower examples from the `Iris-versicolor` and `Iris-virginica`
    classes, with which we will compute the ROC AUC later. The code is as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候让我们在上一节中实现的`MajorityVoteClassifier`发挥作用了。但首先，让我们准备一个可以测试的数据集。由于我们已经熟悉从CSV文件加载数据集的技巧，我们将采取捷径并从scikit-learn的`datasets`模块加载鸢尾花数据集。此外，我们将仅选择两个特征，即*萼片宽度*和*花瓣长度*，以便更具挑战性地进行分类任务以进行说明。尽管我们的`MajorityVoteClassifier`适用于多类问题，但我们只会对来自`Iris-versicolor`和`Iris-virginica`类别的花卉示例进行分类，稍后我们将计算ROC
    AUC。代码如下：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Class membership probabilities from decision trees**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树中的类成员概率**'
- en: Note that scikit-learn uses the `predict_proba` method (if applicable) to compute
    the ROC AUC score. In *Chapter 3*, *A Tour of Machine Learning Classifiers Using
    scikit-learn*, we saw how the class probabilities are computed in logistic regression
    models. In decision trees, the probabilities are calculated from a frequency vector
    that is created for each node at training time. The vector collects the frequency
    values of each class label computed from the class label distribution at that
    node. Then, the frequencies are normalized so that they sum up to 1\. Similarly,
    the class labels of the k-nearest neighbors are aggregated to return the normalized
    class label frequencies in the k-nearest neighbors algorithm. Although the normalized
    probabilities returned by both the decision tree and k-nearest neighbors classifier
    may look similar to the probabilities obtained from a logistic regression model,
    we have to be aware that these are actually not derived from probability mass
    functions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，scikit-learn使用`predict_proba`方法（如果适用）来计算ROC AUC分数。在*第3章*，*使用scikit-learn进行机器学习分类器之旅*中，我们看到了如何计算逻辑回归模型中的类概率。在决策树中，概率是从在训练时为每个节点创建的频率向量计算出来的。该向量收集从该节点的类标签分布中计算出的每个类标签的频率值。然后，将频率归一化，使它们总和为1。类似地，k最近邻算法中的k个最近邻居的类标签被聚合以返回归一化的类标签频率。尽管从决策树和k最近邻分类器返回的归一化概率看起来类似于从逻辑回归模型获得的概率，但我们必须意识到，这些实际上并非来自概率质量函数。
- en: 'Next, we will split the Iris examples into 50 percent training and 50 percent
    test data:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把鸢尾花示例分成50%的训练数据和50%的测试数据：
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Using the training dataset, we now will train three different classifiers:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练数据集，我们现在将训练三种不同的分类器：
- en: Logistic regression classifier
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归分类器
- en: Decision tree classifier
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树分类器
- en: k-nearest neighbors classifier
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k最近邻分类器
- en: 'We will then evaluate the model performance of each classifier via 10-fold
    cross-validation on the training dataset before we combine them into an ensemble
    classifier:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将通过训练数据集上的10折交叉验证评估每个分类器的模型性能，然后将它们组合成一个集成分类器：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output that we receive, as shown in the following snippet, shows that the
    predictive performances of the individual classifiers are almost equal:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如下片段所示，我们收到的输出显示各个分类器的预测性能几乎相等：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You may be wondering why we trained the logistic regression and k-nearest neighbors
    classifier as part of a pipeline. The reason behind it is that, as discussed in
    *Chapter 3*, *A Tour of Machine Learning Classifiers Using scikit-learn*, both
    the logistic regression and k-nearest neighbors algorithms (using the Euclidean
    distance metric) are not scale-invariant, in contrast to decision trees. Although
    the Iris features are all measured on the same scale (cm), it is a good habit
    to work with standardized features.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么我们在一个管道中训练了逻辑回归和k近邻分类器。其背后的原因是，正如在*第3章*中讨论的，*《使用 scikit-learn 进行机器学习分类器巡礼》*，逻辑回归和k近邻算法（使用欧几里得距离度量）并不具备尺度不变性，这与决策树不同。尽管鸢尾花的特征都是以相同的尺度（厘米）测量的，但使用标准化特征是一个良好的习惯。
- en: 'Now, let''s move on to the more exciting part and combine the individual classifiers
    for majority rule voting in our `MajorityVoteClassifier`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入更有趣的部分，将各个分类器组合在一起，以便在我们的`MajorityVoteClassifier`中进行多数投票：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see, the performance of `MajorityVotingClassifier` has improved over
    the individual classifiers in the 10-fold cross-validation evaluation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`MajorityVotingClassifier`在 10 次交叉验证评估中相较于单一分类器的表现有所提高。
- en: Evaluating and tuning the ensemble classifier
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估和调整集成分类器
- en: 'In this section, we are going to compute the ROC curves from the test dataset
    to check that `MajorityVoteClassifier` generalizes well with unseen data. We must
    remember that the test dataset is not to be used for model selection; its purpose
    is merely to report an unbiased estimate of the generalization performance of
    a classifier system:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从测试数据集中计算 ROC 曲线，以检查`MajorityVoteClassifier`在处理未见数据时是否能够很好地泛化。我们必须记住，测试数据集不应被用于模型选择；它的目的仅仅是提供分类器系统泛化性能的无偏估计：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As you can see in the resulting ROC, the ensemble classifier also performs
    well on the test dataset (ROC AUC = 0.95). However, you can see that the logistic
    regression classifier performs similarly well on the same dataset, which is probably
    due to the high variance (in this case, sensitivity of how we split the dataset)
    given the small size of the dataset:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在结果 ROC 曲线中看到的，集成分类器在测试数据集上表现也很出色（ROC AUC = 0.95）。然而，你可以看到逻辑回归分类器在相同数据集上的表现也很相似，这可能是由于数据集较小，导致高方差（在此情况下，是我们划分数据集时的敏感性）所致：
- en: '![](img/B13208_07_04.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_04.png)'
- en: Since we only selected two features for the classification examples, it would
    be interesting to see what the decision region of the ensemble classifier actually
    looks like.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只选择了两个特征进行分类示例，因此查看集成分类器的决策区域究竟是什么样子会很有趣。
- en: 'Although it is not necessary to standardize the training features prior to
    model fitting, because our logistic regression and k-nearest neighbors pipelines
    will automatically take care of it, we will standardize the training dataset so
    that the decision regions of the decision tree will be on the same scale for visual
    purposes. The code is as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在模型拟合之前标准化训练特征并非必要，因为我们的逻辑回归和k近邻管道会自动处理此操作，但为了视觉目的，我们将标准化训练数据集，以便决策树的决策区域能够在相同的尺度上呈现。代码如下：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Interestingly, but also as expected, the decision regions of the ensemble classifier
    seem to be a hybrid of the decision regions from the individual classifiers. At
    first glance, the majority vote decision boundary looks a lot like the decision
    of the decision tree stump, which is orthogonal to the *y* axis for *sepal width*
    ≥ 1\. However, you can also notice the non-linearity from the k-nearest neighbor
    classifier mixed in:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，尽管也是预期中的，集成分类器的决策区域似乎是单个分类器决策区域的混合体。乍一看，多数投票决策边界看起来与决策树树桩的决策非常相似，对于*萼片宽度*≥1的情况，它与*y*轴正交。然而，你也可以注意到，k近邻分类器的非线性特征也被混合在其中：
- en: '![](img/B13208_07_05.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_05.png)'
- en: 'Before we tune the individual classifier''s parameters for ensemble classification,
    let''s call the `get_params` method to get a basic idea of how we can access the
    individual parameters inside a `GridSearchCV` object:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整单个分类器的参数以进行集成分类之前，让我们调用`get_params`方法，初步了解如何在`GridSearchCV`对象中访问单个参数：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Based on the values returned by the `get_params` method, we now know how to
    access the individual classifier''s attributes. Let''s now tune the inverse regularization
    parameter, `C`, of the logistic regression classifier and the decision tree depth
    via a grid search for demonstration purposes:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 `get_params` 方法返回的值，我们现在知道如何访问单个分类器的属性。接下来，为了演示，我们将通过网格搜索调整逻辑回归分类器的逆正则化参数
    `C` 和决策树的深度：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After the grid search has completed, we can print the different hyperparameter
    value combinations and the average ROC AUC scores computed via 10-fold cross-validation
    as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在网格搜索完成后，我们可以打印不同的超参数值组合以及通过 10 折交叉验证计算出的平均 ROC AUC 分数，具体如下：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As you can see, we get the best cross-validation results when we choose a lower
    regularization strength (`C=0.001`), whereas the tree depth does not seem to affect
    the performance at all, suggesting that a decision stump is sufficient to separate
    the data. To remind ourselves that it is a bad practice to use the test dataset
    more than once for model evaluation, we are not going to estimate the generalization
    performance of the tuned hyperparameters in this section. We will move on swiftly
    to an alternative approach for ensemble learning: **bagging**.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，当我们选择较低的正则化强度（`C=0.001`）时，我们得到最佳的交叉验证结果，而树的深度似乎对性能没有任何影响，这表明决策树桩足以分离数据。为了提醒自己，使用测试数据集多次进行模型评估是一个不好的做法，我们在本节中不会估计调整超参数的泛化性能。我们将迅速转向集成学习的另一种方法：**自助聚合（bagging）**。
- en: '**Building ensembles using stacking**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用堆叠构建集成**'
- en: The majority vote approach we implemented in this section is not to be confused
    with **stacking**. The stacking algorithm can be understood as a two-level ensemble,
    where the first level consists of individual classifiers that feed their predictions
    to the second level, where another classifier (typically logistic regression)
    is fit to the level-one classifier predictions to make the final predictions.
    The stacking algorithm has been described in more detail by David H. Wolpert in
    *Stacked generalization*, *Neural Networks*, 5(2):241–259, *1992*. Unfortunately,
    an implementation of this algorithm has not been implemented in scikit-learn at
    the time of writing; however, this feature is underway. In the meantime, you can
    find scikit-learn-compatible implementations of stacking at [http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/)
    and [http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中实现的多数投票方法不应与**堆叠（stacking）**混淆。堆叠算法可以理解为一个两层集成方法，其中第一层由独立的分类器组成，这些分类器将它们的预测结果传递给第二层，在第二层中，另一个分类器（通常是逻辑回归）对第一层分类器的预测进行拟合，从而做出最终预测。David
    H. Wolpert在《堆叠泛化》（*Stacked generalization*）一文中对此算法进行了更详细的描述，该文章发表在*Neural Networks*期刊，第5卷第2期，241-259页，*1992*年。不幸的是，在撰写本文时，scikit-learn
    尚未实现该算法的实现；然而，这个功能正在开发中。与此同时，你可以在[http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/)
    和 [http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/)
    找到与 scikit-learn 兼容的堆叠实现。
- en: Bagging – building an ensemble of classifiers from bootstrap samples
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自助聚合 — 从自助抽样中构建分类器集成
- en: Bagging is an ensemble learning technique that is closely related to the `MajorityVoteClassifier`
    that we implemented in the previous section. However, instead of using the same
    training dataset to fit the individual classifiers in the ensemble, we draw bootstrap
    samples (random samples with replacement) from the initial training dataset, which
    is why bagging is also known as *bootstrap aggregating*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 自助聚合是一种集成学习技术，与我们在上一节中实现的 `MajorityVoteClassifier` 密切相关。然而，和使用相同的训练数据集来拟合集成中的单个分类器不同，我们从初始训练数据集中抽取自助样本（带放回的随机样本），这也是为什么自助聚合又被称为
    *自助抽样聚合*（bootstrap aggregating）。
- en: 'The concept of bagging is summarized in the following diagram:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 自助聚合的概念在下图中总结：
- en: '![](img/B13208_07_06.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_06.png)'
- en: In the following subsections, we will work through a simple example of bagging
    by hand and use scikit-learn for classifying wine examples.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的子节中，我们将通过手动方式进行一个简单的自助聚合示例，并使用 scikit-learn 对葡萄酒样本进行分类。
- en: Bagging in a nutshell
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自助聚合概述
- en: 'To provide a more concrete example of how the bootstrap aggregating of a bagging
    classifier works, let''s consider the example shown in the following figure. Here,
    we have seven different training instances (denoted as indices 1-7) that are sampled
    randomly with replacement in each round of bagging. Each bootstrap sample is then
    used to fit a classifier, ![](img/B13208_07_042.png), which is most typically
    an unpruned decision tree:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更具体地展示bagging分类器的自助聚合是如何工作的，我们考虑以下图示的例子。在这里，我们有七个不同的训练实例（表示为索引1-7），在每一轮bagging中都被随机地抽取且允许重复。每个自助样本随后用来拟合一个分类器，![](img/B13208_07_042.png)，它通常是一个未剪枝的决策树：
- en: '![](img/B13208_07_07.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_07.png)'
- en: As you can see from the previous illustration, each classifier receives a random
    subset of examples from the training dataset. We denote these random samples obtained
    via bagging as *Bagging round 1*, *Bagging round 2*, and so on. Each subset contains
    a certain portion of duplicates and some of the original examples don't appear
    in a resampled dataset at all due to sampling with replacement. Once the individual
    classifiers are fit to the bootstrap samples, the predictions are combined using
    majority voting.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的插图中可以看出，每个分类器都接收来自训练数据集的随机子集。我们将通过bagging获得的这些随机样本称为*Bagging轮次1*、*Bagging轮次2*，依此类推。每个子集包含一定比例的重复样本，且由于有放回抽样，一些原始样本在重新采样的数据集中根本不会出现。一旦个别分类器拟合了这些自助样本，预测结果便通过多数投票结合起来。
- en: Note that bagging is also related to the random forest classifier that we introduced
    in *Chapter 3*, *A Tour of Machine Learning Classifiers Using scikit-learn*. In
    fact, random forests are a special case of bagging where we also use random feature
    subsets when fitting the individual decision trees.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，bagging（自助聚合法）与我们在*第3章*《使用scikit-learn的机器学习分类器概览》中介绍的随机森林分类器也有关。事实上，随机森林是bagging的一个特例，在拟合单个决策树时，我们还会使用随机的特征子集。
- en: '**Model ensembles using bagging**'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用bagging的模型集成**'
- en: Bagging was first proposed by Leo Breiman in a technical report in 1994; he
    also showed that bagging can improve the accuracy of unstable models and decrease
    the degree of overfitting. I highly recommend that you read about his research
    in *Bagging predictors*, *L. Breiman*, *Machine Learning*, 24(2):123–140, *1996*,
    which is freely available online, to learn more details about bagging.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging首次由Leo Breiman在1994年的一份技术报告中提出；他还展示了bagging可以提高不稳定模型的准确性，并减少过拟合的程度。我强烈推荐你阅读他在*Bagging
    predictors*, *L. Breiman*, *Machine Learning*, 24(2):123–140, *1996*中的研究，文献可以在网上免费获取，以了解更多关于bagging的细节。
- en: Applying bagging to classify examples in the Wine dataset
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用bagging来对葡萄酒数据集中的样本进行分类
- en: 'To see bagging in action, let''s create a more complex classification problem
    using the Wine dataset that was introduced in *Chapter 4*, *Building Good Training
    Datasets – Data Preprocessing*. Here, we will only consider the Wine classes 2
    and 3, and we will select two features – `Alcohol` and `OD280/OD315 of diluted
    wines`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示bagging的实际应用，我们将使用*第4章*《构建优质训练数据集——数据预处理》中介绍的葡萄酒数据集，创建一个更复杂的分类问题。在这里，我们将只考虑葡萄酒类别2和3，并选择两个特征——`酒精`和`稀释酒的OD280/OD315`：
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we will encode the class labels into binary format and split the dataset
    into 80 percent training and 20 percent test datasets, respectively:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把类别标签编码成二进制格式，并将数据集分别分割为80%的训练集和20%的测试集：
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Obtaining the Wine dataset**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**获取葡萄酒数据集**'
- en: 'You can find a copy of the Wine dataset (and all other datasets used in this
    book) in the code bundle of this book, which you can use if you are working offline
    or the UCI server at [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)
    is temporarily unavailable. For instance, to load the Wine dataset from a local
    directory, take the following lines:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的代码包中找到葡萄酒数据集的副本（以及本书中使用的所有其他数据集），如果你离线工作或UCI服务器[https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)暂时不可用，你可以使用它。例如，要从本地目录加载葡萄酒数据集，可以执行以下代码：
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'and replace them with these:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 并将其替换为以下内容：
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'A `BaggingClassifier` algorithm is already implemented in scikit-learn, which
    we can import from the `ensemble` submodule. Here, we will use an unpruned decision
    tree as the base classifier and create an ensemble of 500 decision trees fit on
    different bootstrap samples of the training dataset:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`BaggingClassifier`算法已经在scikit-learn中实现，我们可以从`ensemble`子模块中导入。这里，我们将使用未修剪的决策树作为基础分类器，并创建一个由500棵决策树组成的集成，这些决策树在训练数据集的不同自助抽样样本上进行拟合：'
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we will calculate the accuracy score of the prediction on the training
    and test datasets to compare the performance of the bagging classifier to the
    performance of a single unpruned decision tree:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算训练数据集和测试数据集上的预测准确度，以便将袋装分类器的表现与单个未修剪决策树的表现进行比较：
- en: '[PRE21]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Based on the accuracy values that we printed here, the unpruned decision tree
    predicts all the class labels of the training examples correctly; however, the
    substantially lower test accuracy indicates high variance (overfitting) of the
    model:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们在这里打印的准确度值，未修剪的决策树正确地预测了所有训练示例的类别标签；然而，明显较低的测试准确度表明模型的方差较高（过拟合）：
- en: '[PRE22]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Although the training accuracies of the decision tree and bagging classifier
    are similar on the training dataset (both 100 percent), we can see that the bagging
    classifier has a slightly better generalization performance, as estimated on the
    test dataset. Next, let''s compare the decision regions between the decision tree
    and the bagging classifier:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管决策树和袋装分类器在训练数据集上的训练准确度相似（都是100%），但我们可以看到，袋装分类器在估计的测试数据集上的泛化性能稍好。接下来，让我们比较决策树和袋装分类器的决策区域：
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As we can see in the resulting plot, the piece-wise linear decision boundary
    of the three-node deep decision tree looks smoother in the bagging ensemble:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果图中可以看到，三节点深度决策树的分段线性决策边界在袋装集成中看起来更平滑：
- en: '![](img/B13208_07_08.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_08.png)'
- en: We only looked at a very simple bagging example in this section. In practice,
    more complex classification tasks and a dataset's high dimensionality can easily
    lead to overfitting in single decision trees, and this is where the bagging algorithm
    can really play to its strengths. Finally, we must note that the bagging algorithm
    can be an effective approach to reducing the variance of a model. However, bagging
    is ineffective in reducing model bias, that is, models that are too simple to
    capture the trend in the data well. This is why we want to perform bagging on
    an ensemble of classifiers with low bias, for example, unpruned decision trees.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一节中只看了一个非常简单的袋装示例。在实际应用中，更多复杂的分类任务和数据集的高维度很容易导致单个决策树出现过拟合，而此时袋装算法正好能够发挥其优势。最后，我们必须注意，袋装算法在降低模型方差方面是一种有效的方法。然而，袋装并不能有效降低模型的偏差，也就是说，对于那些过于简单，无法很好地捕捉数据趋势的模型，袋装并不起作用。这就是为什么我们希望对低偏差的分类器集成进行袋装，比如未修剪的决策树。
- en: Leveraging weak learners via adaptive boosting
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过自适应提升（AdaBoost）利用弱学习器
- en: 'In this last section about ensemble methods, we will discuss **boosting**,
    with a special focus on its most common implementation: **Adaptive Boosting**
    (**AdaBoost**).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节关于集成方法的最后，我们将讨论**提升**，特别关注其最常见的实现：**自适应提升**（**AdaBoost**）。
- en: '**AdaBoost recognition**'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**AdaBoost识别**'
- en: 'The original idea behind AdaBoost was formulated by Robert E. Schapire in 1990\.
    *The Strength of Weak Learnability*, *R. E. Schapire*, *Machine Learning*, 5(2):
    197-227, *1990*. After Robert Schapire and Yoav Freund presented the AdaBoost
    algorithm in the *Proceedings of the Thirteenth International Conference* (ICML
    1996), AdaBoost became one of the most widely used ensemble methods in the years
    that followed (*Experiments with a New Boosting Algorithm by Y. Freund*, *R. E.
    Schapire*, and others, *ICML*, volume 96, 148-156, *1996*). In 2003, Freund and
    Schapire received the Goedel Prize for their groundbreaking work, which is a prestigious
    prize for the most outstanding publications in the field of computer science.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost的最初理念是由Robert E. Schapire于1990年提出的。*弱学习能力的强度*，*R. E. Schapire*，*机器学习*，5(2)：197-227，*1990*。在Robert
    Schapire和Yoav Freund于*第十三届国际会议论文集*（ICML 1996）上提出AdaBoost算法后，AdaBoost成为随后几年最广泛使用的集成方法之一（*Y.
    Freund*、*R. E. Schapire*等人的*新提升算法实验*，*ICML*，第96卷，148-156，*1996*）。2003年，Freund和Schapire因其开创性的工作获得了Gödel奖，这是计算机科学领域最具声望的奖项，授予最杰出的出版物。
- en: In boosting, the ensemble consists of very simple base classifiers, also often
    referred to as **weak learners**, which often only have a slight performance advantage
    over random guessing—a typical example of a weak learner is a decision tree stump.
    The key concept behind boosting is to focus on training examples that are hard
    to classify, that is, to let the weak learners subsequently learn from misclassified
    training examples to improve the performance of the ensemble.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在提升算法中，集成模型由非常简单的基本分类器组成，这些分类器通常被称为**弱学习器**，它们在性能上通常仅稍微优于随机猜测——一个典型的弱学习器例子是决策树桩。提升的关键概念是聚焦于那些难以分类的训练示例，即让弱学习器从错误分类的训练示例中学习，以提升集成模型的性能。
- en: The following subsections will introduce the algorithmic procedure behind the
    general concept of boosting and AdaBoost. Lastly, we will use scikit-learn for
    a practical classification example.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节将介绍提升和AdaBoost背后的算法过程。最后，我们将使用scikit-learn进行一个实际的分类示例。
- en: How boosting works
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升算法如何工作
- en: 'In contrast to bagging, the initial formulation of the boosting algorithm uses
    random subsets of training examples drawn from the training dataset without replacement;
    the original boosting procedure can be summarized in the following four key steps:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 与袋装法（bagging）不同，提升算法的初始形式使用的是从训练数据集中随机抽取的训练示例子集，且不进行替换；原始的提升过程可以总结为以下四个关键步骤：
- en: Draw a random subset (sample) of training examples, ![](img/B13208_07_043.png),
    without replacement from the training dataset, *D*, to train a weak learner, ![](img/B13208_07_044.png).
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练数据集中抽取一个随机子集（样本），![](img/B13208_07_043.png)，并且不进行替换，从数据集*D*中训练一个弱学习器，![](img/B13208_07_044.png)。
- en: Draw a second random training subset, ![](img/B13208_07_045.png), without replacement
    from the training dataset and add 50 percent of the examples that were previously
    misclassified to train a weak learner, ![](img/B13208_07_046.png).
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练数据集中抽取第二个随机训练子集，![](img/B13208_07_045.png)，并且不进行替换，同时将之前被错误分类的50%的示例加入其中，来训练一个弱学习器，![](img/B13208_07_046.png)。
- en: Find the training examples, ![](img/B13208_07_047.png), in the training dataset,
    *D*, which ![](img/B13208_07_048.png) and ![](img/B13208_07_033.png) disagree
    upon, to train a third weak learner, ![](img/B13208_07_050.png).
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据集*D*中找到那些与![](img/B13208_07_048.png)和![](img/B13208_07_033.png)存在分歧的训练示例，![](img/B13208_07_047.png)，来训练第三个弱学习器，![](img/B13208_07_050.png)。
- en: Combine the weak learners ![](img/B13208_07_051.png), ![](img/B13208_07_052.png),
    and ![](img/B13208_07_053.png) via majority voting.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过多数投票将弱学习器![](img/B13208_07_051.png)，![](img/B13208_07_052.png)和![](img/B13208_07_053.png)结合起来。
- en: As discussed by Leo Breiman (*Bias, variance, and arcing classifiers*, *L. Breiman*,
    *1996*), boosting can lead to a decrease in bias as well as variance compared
    to bagging models. In practice, however, boosting algorithms such as AdaBoost
    are also known for their high variance, that is, the tendency to overfit the training
    data (*An improvement of AdaBoost to avoid overfitting*, *G. Raetsch*, *T. Onoda*,
    and *K. R. Mueller*. *Proceedings of the International Conference on Neural Information
    Processing*, *CiteSeer*, *1998*).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 正如Leo Breiman（*偏差、方差与弧形分类器*，*L. Breiman*，*1996*）所讨论的，提升方法相比袋装法可以减少偏差和方差。然而，实际上，像AdaBoost这样的提升算法也因其较高的方差而著名，即它们有过拟合训练数据的倾向（*AdaBoost的改进以避免过拟合*，*G.
    Raetsch*，*T. Onoda*和*K. R. Mueller*，*神经信息处理国际会议论文集*，*CiteSeer*，*1998*）。
- en: In contrast to the original boosting procedure described here, AdaBoost uses
    the complete training dataset to train the weak learners, where the training examples
    are reweighted in each iteration to build a strong classifier that learns from
    the mistakes of the previous weak learners in the ensemble.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 与此处描述的原始提升过程不同，AdaBoost使用完整的训练数据集来训练弱学习器，其中在每次迭代中重新加权训练示例，以构建一个强分类器，使其能够从之前弱学习器的错误中学习。
- en: 'Before we dive deeper into the specific details of the AdaBoost algorithm,
    let''s take a look at the following figure to get a better grasp of the basic
    concept behind AdaBoost:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨AdaBoost算法的具体细节之前，我们先看一下下面的图，以便更好地理解AdaBoost背后的基本概念：
- en: '![](img/B13208_07_09.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_09.png)'
- en: To walk through the AdaBoost illustration step by step, we will start with subfigure
    **1**, which represents a training dataset for binary classification where all
    training examples are assigned equal weights. Based on this training dataset,
    we train a decision stump (shown as a dashed line) that tries to classify the
    examples of the two classes (triangles and circles), as well as possibly minimizing
    the cost function (or the impurity score in the special case of decision tree
    ensembles).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了逐步演示 AdaBoost，我们从子图 **1** 开始，该子图表示一个用于二分类的训练数据集，所有训练示例都被分配了相等的权重。基于该训练数据集，我们训练了一个决策桩（以虚线表示），它试图对两类（三角形和圆形）的示例进行分类，同时尽可能地最小化代价函数（或者在决策树集成的特殊情况下最小化杂质分数）。
- en: For the next round (subfigure **2**), we assign a larger weight to the two previously
    misclassified examples (circles). Furthermore, we lower the weight of the correctly
    classified examples. The next decision stump will now be more focused on the training
    examples that have the largest weights—the training examples that are supposedly
    hard to classify. The weak learner shown in subfigure **2** misclassifies three
    different examples from the circle class, which are then assigned a larger weight,
    as shown in subfigure **3**.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下一轮（子图 **2**），我们为之前被误分类的两个示例（圆形）分配更大的权重。同时，我们降低了正确分类的示例的权重。下一次决策桩将更加关注权重较大的训练示例——这些训练示例通常是难以分类的。在子图
    **2** 中显示的弱学习器将三个来自圆形类的不同示例误分类，这些示例随后被赋予更大的权重，如子图 **3** 所示。
- en: Assuming that our AdaBoost ensemble only consists of three rounds of boosting,
    we then combine the three weak learners trained on different reweighted training
    subsets by a weighted majority vote, as shown in subfigure **4**.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的 AdaBoost 集成仅由三轮提升组成，我们通过加权多数投票将训练在不同重加权训练子集上的三个弱学习器组合起来，如子图 **4** 所示。
- en: 'Now that we have a better understanding of the basic concept of AdaBoost, let''s
    take a more detailed look at the algorithm using pseudo code. For clarity, we
    will denote element-wise multiplication by the cross symbol (![](img/B13208_07_054.png))
    and the dot-product between two vectors by a dot symbol (![](img/B13208_07_055.png)):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 AdaBoost 的基本概念有了更清楚的了解，接下来我们将使用伪代码更加详细地了解该算法。为了清晰起见，我们将用叉号（![](img/B13208_07_054.png)）表示逐元素乘法，用点号（![](img/B13208_07_055.png)）表示两个向量的点积：
- en: Set the weight vector, *w*, to uniform weights, where ![](img/B13208_07_056.png).
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将权重向量 *w* 设置为均匀权重，其中 ![](img/B13208_07_056.png)。
- en: 'For *j* in *m* boosting rounds, do the following:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *m* 次提升轮中的 *j*，执行以下操作：
- en: 'Train a weighted weak learner: ![](img/B13208_07_057.png).'
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个加权的弱学习器：![](img/B13208_07_057.png)。
- en: 'Predict class labels: ![](img/B13208_07_058.png).'
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测类别标签：![](img/B13208_07_058.png)。
- en: 'Compute weighted error rate: ![](img/B13208_07_059.png).'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算加权错误率：![](img/B13208_07_059.png)。
- en: 'Compute coefficient: ![](img/B13208_07_060.png)'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算系数：![](img/B13208_07_060.png)
- en: 'Update weights: ![](img/B13208_07_061.png).'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重：![](img/B13208_07_061.png)。
- en: 'Normalize weights to sum to 1: ![](img/B13208_07_062.png).'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将权重归一化使其总和为 1：![](img/B13208_07_062.png)。
- en: 'Compute the final prediction: ![](img/B13208_07_063.png).'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算最终预测：![](img/B13208_07_063.png)。
- en: Note that the expression ![](img/B13208_07_064.png) in step 2c refers to a binary
    vector consisting of 1s and 0s, where a 1 is assigned if the prediction is incorrect
    and 0 is assigned otherwise.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，步骤 2c 中的表达式 ![](img/B13208_07_064.png) 表示一个由 1 和 0 组成的二进制向量，其中如果预测错误则赋值为
    1，否则赋值为 0。
- en: 'Although the AdaBoost algorithm seems to be pretty straightforward, let''s
    walk through a more concrete example using a training dataset consisting of 10
    training examples, as illustrated in the following table:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 AdaBoost 算法看起来非常简单，但我们可以通过一个包含 10 个训练示例的训练数据集来详细演示，具体如以下表格所示：
- en: '![](img/B13208_07_10.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_10.png)'
- en: The first column of the table depicts the indices of training examples 1 to
    10\. In the second column, you can see the feature values of the individual samples,
    assuming this is a one-dimensional dataset. The third column shows the true class
    label, ![](img/B13208_07_065.png), for each training sample, ![](img/B13208_07_066.png),
    where ![](img/B13208_07_067.png). The initial weights are shown in the fourth
    column; we initialize the weights uniformly (assigning the same constant value)
    and normalize them to sum to 1\. In the case of the 10-sample training dataset,
    we therefore assign 0.1 to each weight, ![](img/B13208_07_068.png), in the weight
    vector, *w*. The predicted class labels, ![](img/B13208_07_069.png), are shown
    in the fifth column, assuming that our splitting criterion is ![](img/B13208_07_070.png).
    The last column of the table then shows the updated weights based on the update
    rules that we defined in the pseudo code.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表的第一列显示了训练样本1到10的索引。在第二列中，您可以看到单个样本的特征值，假设这是一个一维数据集。第三列显示了每个训练样本![](img/B13208_07_065.png)
    的真实类标签，其中![](img/B13208_07_066.png)，其中![](img/B13208_07_067.png)。初始权重显示在第四列中；我们均匀初始化权重（分配相同的常量值），并将它们归一化为总和为1。对于10个样本的训练数据集，我们因此将0.1分配给权重向量*w*中的每个权重![](img/B13208_07_068.png)。假设我们的分割标准是![](img/B13208_07_070.png)，则第五列显示了预测的类标签![](img/B13208_07_069.png)。然后，表的最后一列根据我们在伪代码中定义的更新规则显示了更新后的权重。
- en: 'Since the computation of the weight updates may look a little bit complicated
    at first, we will now follow the calculation step by step. We will start by computing
    the weighted error rate, ![](img/B13208_07_071.png), as described in step 2c:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 由于权重更新的计算可能一开始看起来有点复杂，我们现在将逐步跟随计算。我们将从计算加权错误率![](img/B13208_07_071.png) 开始，如第2c步所述：
- en: '![](img/B13208_07_072.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_072.png)'
- en: 'Next, we will compute the coefficient, ![](img/B13208_07_073.png)—shown in
    step 2d—which will later be used in step 2e to update the weights, as well as
    for the weights in the majority vote prediction (step 3):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算系数，![](img/B13208_07_073.png)—如第2d步所示—这将在第2e步中用于更新权重，以及在多数投票预测（第3步）中使用权重：
- en: '![](img/B13208_07_074.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_074.png)'
- en: 'After we have computed the coefficient, ![](img/B13208_07_075.png), we can
    now update the weight vector using the following equation:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们计算系数![](img/B13208_07_075.png) 后，现在可以使用以下方程更新权重向量：
- en: '![](img/B13208_07_076.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_076.png)'
- en: 'Here, ![](img/B13208_07_077.png) is an element-wise multiplication between
    the vectors of the predicted and true class labels, respectively. Thus, if a prediction,
    ![](img/B13208_07_078.png), is correct, ![](img/B13208_07_079.png) will have a
    positive sign so that we decrease the *i*th weight, since ![](img/B13208_07_080.png)
    is a positive number as well:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_07_077.png) 是预测和真实类标签向量的逐元素乘积。因此，如果预测![](img/B13208_07_078.png)
    正确，![](img/B13208_07_079.png) 将具有正号，因此我们减少第*i*个权重，因为![](img/B13208_07_080.png)
    也是一个正数：
- en: '![](img/B13208_07_081.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_081.png)'
- en: 'Similarly, we will increase the *i*th weight if ![](img/B13208_07_082.png)
    predicted the label incorrectly, like this:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果![](img/B13208_07_082.png) 预测的标签是错误的，我们将增加第*i*个权重：
- en: '![](img/B13208_07_083.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_083.png)'
- en: 'Alternatively, it''s like this:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 或者说，它就像这样：
- en: '![](img/B13208_07_084.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_084.png)'
- en: 'After we have updated each weight in the weight vector, we normalize the weights
    so that they sum up to 1 (step 2f):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们更新权重向量中的每个权重之后，我们将归一化权重，使它们总和为1（第2f步）：
- en: '![](img/B13208_07_085.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_085.png)'
- en: Here, ![](img/B13208_07_086.png).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_07_086.png)。
- en: Thus, each weight that corresponds to a correctly classified example will be
    reduced from the initial value of 0.1 to ![](img/B13208_07_087.png) for the next
    round of boosting. Similarly, the weights of the incorrectly classified examples
    will increase from 0.1 to ![](img/B13208_07_088.png).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于每个正确分类的样本对应的权重，其将从初始值0.1减少到![](img/B13208_07_087.png)，用于下一轮增强。类似地，错误分类样本的权重将从0.1增加到![](img/B13208_07_088.png)。
- en: Applying AdaBoost using scikit-learn
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用scikit-learn应用AdaBoost
- en: 'The previous subsection introduced AdaBoost in a nutshell. Skipping to the
    more practical part, let''s now train an AdaBoost ensemble classifier via scikit-learn.
    We will use the same Wine subset that we used in the previous section to train
    the bagging meta-classifier. Via the `base_estimator` attribute, we will train
    the `AdaBoostClassifier` on 500 decision tree stumps:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 前一小节简要介绍了AdaBoost。跳到更实际的部分，我们现在通过scikit-learn来训练一个AdaBoost集成分类器。我们将使用前一节中用于训练bagging元分类器的相同Wine子集。通过`base_estimator`属性，我们将在500个决策树桩上训练`AdaBoostClassifier`：
- en: '[PRE24]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As you can see, the decision tree stump seems to underfit the training data
    in contrast to the unpruned decision tree that we saw in the previous section:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，决策树桩似乎相较于我们在前一节看到的未经剪枝的决策树，对训练数据进行了欠拟合：
- en: '[PRE25]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here, you can see that the AdaBoost model predicts all class labels of the training
    dataset correctly and also shows a slightly improved test dataset performance
    compared to the decision tree stump. However, you can also see that we introduced
    additional variance by our attempt to reduce the model bias—a greater gap between
    training and test performance.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到AdaBoost模型正确预测了训练数据集的所有类别标签，并且与决策树桩相比，测试数据集的性能也有所提高。然而，你也可以看到，由于我们试图减少模型偏差，导致了额外的方差——即训练和测试性能之间的差距更大。
- en: Although we used another simple example for demonstration purposes, we can see
    that the performance of the AdaBoost classifier is slightly improved compared
    to the decision stump and achieved very similar accuracy scores as the bagging
    classifier that we trained in the previous section. However, we must note that
    it is considered bad practice to select a model based on the repeated usage of
    the test dataset. The estimate of the generalization performance may be over-optimistic,
    which we discussed in more detail in *Chapter 6*, *Learning Best Practices for
    Model Evaluation and Hyperparameter Tuning*.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们使用了另一个简单的示例来演示，但我们可以看到，AdaBoost分类器的性能相比决策树桩有所改善，并且与我们在前一节中训练的bagging分类器达到了非常相似的准确率。然而，我们必须注意，基于反复使用测试数据集来选择模型被认为是不良做法。对泛化性能的估计可能过于乐观，关于这一点我们在*第六章*中有更详细的讨论，*模型评估与超参数调优的最佳实践学习*。
- en: 'Lastly, let''s check what the decision regions look like:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们检查一下决策区域的情况：
- en: '[PRE26]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'By looking at the decision regions, you can see that the decision boundary
    of the AdaBoost model is substantially more complex than the decision boundary
    of the decision stump. In addition, note that the AdaBoost model separates the
    feature space very similarly to the bagging classifier that we trained in the
    previous section:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看决策区域，可以看到AdaBoost模型的决策边界比决策树桩的决策边界复杂得多。此外，注意到AdaBoost模型的特征空间划分与我们在前一节中训练的bagging分类器非常相似：
- en: '![](img/B13208_07_11.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_07_11.png)'
- en: As concluding remarks about ensemble techniques, it is worth noting that ensemble
    learning increases the computational complexity compared to individual classifiers.
    In practice, we need to think carefully about whether we want to pay the price
    of increased computational costs for an often relatively modest improvement in
    predictive performance.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对集成技术的总结，值得注意的是，集成学习相比单个分类器增加了计算复杂度。在实践中，我们需要仔细考虑是否愿意为相对适度的预测性能提升支付增加的计算成本。
- en: 'An often-cited example of this tradeoff is the famous $1 million *Netflix Prize*,
    which was won using ensemble techniques. The details about the algorithm were
    published in *The BigChaos Solution to the Netflix Grand Prize* by *A. Toescher*,
    *M. Jahrer*, and *R. M. Bell*, *Netflix Prize documentation*, *2009*, which is
    available at [http://www.stat.osu.edu/~dmsl/GrandPrize2009_BPC_BigChaos.pdf](http://www.stat.osu.edu/~dmsl/GrandPrize2009_BPC_BigChaos.pdf).
    The winning team received the $1 million grand prize money; however, Netflix never
    implemented their model due to its complexity, which made it infeasible for a
    real-world application:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常被引用的关于这种权衡的例子是著名的100万美元*Netflix大奖*，该大奖通过集成技术获得。关于该算法的细节已发布在*A. Toescher*、*M.
    Jahrer*和*R. M. Bell*的《Netflix大奖的BigChaos解决方案》中，*Netflix Prize文档*，*2009年*，可以在[http://www.stat.osu.edu/~dmsl/GrandPrize2009_BPC_BigChaos.pdf](http://www.stat.osu.edu/~dmsl/GrandPrize2009_BPC_BigChaos.pdf)查看。获胜团队获得了100万美元的大奖奖金；然而，由于模型的复杂性，Netflix从未实现他们的模型，因为它对于实际应用来说不可行：
- en: '"We evaluated some of the new methods offline but the additional accuracy gains
    that we measured did not seem to justify the engineering effort needed to bring
    them into a production environment."'
  id: totrans-198
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们离线评估了一些新的方法，但我们测量的额外准确性提升似乎并不足以证明将它们引入生产环境所需的工程工作量是值得的。”
- en: ''
  id: totrans-199
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html](http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html)'
  id: totrans-200
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html](http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html)'
- en: '**Gradient boosting**'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度提升**'
- en: 'Another popular variant of boosting is **gradient boosting**. AdaBoost and
    gradient boosting share the main overall concept: boosting weak learners (such
    as decision tree stumps) to strong learners. The two approaches, adaptive and
    gradient boosting, differ mainly with regard to how the weights are updated and
    how the (weak) classifiers are combined. If you are familiar with gradient-based
    optimization and interested in gradient boosting, I recommend reading Jerome Friedman''s
    work (*Greedy function approximation: a gradient boosting machine*. *Jerome Friedman*.
    *Annals of Statistics 2001*, pp.1189-1232) and the more recent paper on XGBoost,
    which is essentially a computationally efficient implementation of the original
    gradient boost algorithm (*XGBoost: A scalable tree boosting system*. *Tianqi
    Chen* and *Carlos Guestrin*. *Proceeding of the 22nd ACM SIGKDD International
    Conference on Knowledge Discovery and Data Mining*. *ACM 2016*, pp. 785-794).
    Note that next to the `GradientBoostingClassifier` implementation in scikit-learn,
    scikit-learn now also includes a substantially faster version of gradient boosting
    in version 0.21, `HistGradientBoostingClassifier`, which is even faster than XGBoost.
    For more information about the `GradientBoostingClassifier` and `HistGradientBoostingClassifier`
    in scikit-learn, you can read the documentation at [https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting).
    Also, a short, general explanation of gradient boosting can be found in the lecture
    notes at [https://sebastianraschka.com/pdf/lecture-notes/stat479fs19/07-ensembles__notes.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat479fs19/07-ensembles__notes.pdf)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的提升方法是**梯度提升**。AdaBoost和梯度提升共享一个主要的整体概念：将弱学习器（例如决策树桩）提升为强学习器。这两种方法——自适应提升和梯度提升——主要的区别在于权重的更新方式以及如何结合（弱）分类器。如果你熟悉基于梯度的优化并对梯度提升感兴趣，我推荐阅读Jerome
    Friedman的工作（*贪婪函数近似：一种梯度提升机*。*Jerome Friedman*。*Annals of Statistics 2001*，第1189-1232页）以及关于XGBoost的最新论文，XGBoost本质上是原始梯度提升算法的一种计算高效实现（*XGBoost：一种可扩展的树提升系统*。*Tianqi
    Chen* 和 *Carlos Guestrin*。*Proceeding of the 22nd ACM SIGKDD International Conference
    on Knowledge Discovery and Data Mining*。*ACM 2016*，第785-794页）。请注意，除了scikit-learn中的`GradientBoostingClassifier`实现，scikit-learn现在还在0.21版本中包含了一种显著更快的梯度提升版本——`HistGradientBoostingClassifier`，它比XGBoost还要快。有关scikit-learn中`GradientBoostingClassifier`和`HistGradientBoostingClassifier`的更多信息，你可以阅读文档：[https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting)。此外，梯度提升的简短通用解释可以在以下讲义中找到：[https://sebastianraschka.com/pdf/lecture-notes/stat479fs19/07-ensembles__notes.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat479fs19/07-ensembles__notes.pdf)
- en: Summary
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we looked at some of the most popular and widely used techniques
    for ensemble learning. Ensemble methods combine different classification models
    to cancel out their individual weaknesses, which often results in stable and well-performing
    models that are very attractive for industrial applications as well as machine
    learning competitions.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了一些最受欢迎和广泛使用的集成学习技术。集成方法将不同的分类模型结合起来，消除它们各自的弱点，这通常会导致稳定且表现优异的模型，这些模型对工业应用以及机器学习竞赛都非常具有吸引力。
- en: At the beginning of this chapter, we implemented `MajorityVoteClassifier` in
    Python, which allows us to combine different algorithms for classification. We
    then looked at bagging, a useful technique for reducing the variance of a model
    by drawing random bootstrap samples from the training dataset and combining the
    individually trained classifiers via majority vote. Lastly, we learned about AdaBoost,
    which is an algorithm that is based on weak learners that subsequently learn from
    mistakes.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开始时，我们在 Python 中实现了 `MajorityVoteClassifier`，它允许我们将不同的分类算法结合起来。接着我们了解了袋装法（bagging），这是一种通过从训练数据集中随机抽取自助样本，并通过多数投票结合单独训练的分类器来降低模型方差的有用技术。最后，我们学习了
    AdaBoost，它是一种基于弱学习者的算法，弱学习者通过从错误中学习来改进。
- en: Throughout the previous chapters, we learned a lot about different learning
    algorithms, tuning, and evaluation techniques. In the next chapter, we will look
    at a particular application of machine learning, sentiment analysis, which has
    become an interesting topic in the internet and social media era.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学到了许多关于不同学习算法、调优和评估技术的内容。在下一章中，我们将探讨机器学习的一个特定应用——情感分析，它已成为互联网和社交媒体时代的一个有趣话题。
