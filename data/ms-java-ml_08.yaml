- en: Chapter 8. Text Mining and Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 文本挖掘与自然语言处理
- en: '**Natural language processing** (**NLP**) is ubiquitous today in various applications
    such as mobile apps, ecommerce websites, emails, news websites, and more. Detecting
    spam in e-mails, characterizing e-mails, speech synthesis, categorizing news,
    searching and recommending products, performing sentiment analysis on social media
    brands—these are all different aspects of NLP and mining text for information.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理（NLP**）如今在各种应用中无处不在，如移动应用、电子商务网站、电子邮件、新闻网站等。检测电子邮件中的垃圾邮件、描述电子邮件、语音合成、分类新闻、搜索和推荐产品、对社交媒体品牌进行情感分析——这些都是NLP和挖掘文本信息的不同方面。'
- en: There has been an exponential increase in digital information that is textual
    in content—in the form of web pages, e-books, SMS messages, documents of various
    formats, e-mails, social media messages such as tweets and Facebook posts, now
    ranges in exabytes (an exabyte is 1,018 bytes). Historically, the earliest foundational
    work relying on automata and probabilistic modeling began in the 1950s. The 1970s
    saw changes such as stochastic modeling, Markov modeling, and syntactic parsing,
    but their progress was limited during the 'AI Winter' years. The 1990s saw the
    emergence of text mining and a statistical revolution that included ideas of corpus
    statistics, supervised Machine Learning, and human annotation of text data. From
    the year 2000 onwards, with great progress in computing and Big Data, as well
    as the introduction of sophisticated Machine Learning algorithms in supervised
    and unsupervised learning, the area has received rekindled interest and is now
    among the hottest topics in research, both in academia and the R&D departments
    of commercial enterprises. In this chapter, we will discuss some aspects of NLP
    and text mining that are essential in Machine Learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数字文本信息呈指数级增长——以网页、电子书、短信、各种格式的文档、电子邮件、社交媒体消息（如推文和Facebook帖子）等形式，现在达到泽字节（1泽字节等于1,018字节）。历史上，最早依赖自动机和概率建模的基础性工作始于20世纪50年代。20世纪70年代见证了诸如随机建模、马尔可夫建模和句法解析等变化，但在“人工智能寒冬”年份，它们的进展有限。20世纪90年代见证了文本挖掘和统计革命的兴起，包括语料库统计、监督机器学习和文本数据的标注等思想。从2000年开始，随着计算和大数据的巨大进步，以及监督学习和无监督学习中复杂机器学习算法的引入，该领域重新引起了人们的兴趣，现在成为学术界和商业企业研发部门研究的热点话题之一。在本章中，我们将讨论NLP和文本挖掘在机器学习中的关键方面。
- en: The chapter begins with an introduction to the key areas within NLP, and it
    then explains the important processing and transformation steps that make the
    documents more suitable for Machine Learning, whether supervised or unsupervised.
    The concept of topic modeling, clustering, and named entity recognition follow,
    with brief descriptions of two Java toolkits that offer powerful text processing
    capabilities. The case study for this chapter uses another widely-known dataset
    to demonstrate several techniques described here through experiments using the
    tools KNIME and Mallet.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先介绍了自然语言处理（NLP）中的关键领域，然后解释了使文档更适合机器学习（无论是监督学习还是无监督学习）的重要处理和转换步骤。接下来是主题建模、聚类和命名实体识别的概念，以及简要介绍了两个提供强大文本处理能力的Java工具包。本章的案例研究使用另一个广为人知的数据集，通过使用KNIME和Mallet工具进行实验，展示了这里描述的几种技术。
- en: 'The chapter is organized as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章组织如下：
- en: 'NLP, subfields, and tasks:'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP，子领域和任务：
- en: Text categorization
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类
- en: POS tagging
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性标注
- en: Text clustering
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本聚类
- en: Information extraction and named entity recognition
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息提取和命名实体识别
- en: Sentiment analysis
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析
- en: Coreference resolution
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共指消解
- en: Word-sense disambiguation
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词义消歧
- en: Machine translation
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译
- en: Semantic reasoning and inferencing
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义推理和推断
- en: Summarization
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要
- en: Questions and answers
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答
- en: Issues with mining and unstructured data
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据挖掘和无结构数据的问题
- en: 'Text processing components and transformations:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本处理组件和转换：
- en: Document collection and standardization
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档收集和标准化
- en: Tokenization
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词
- en: Stop words removal
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停用词去除
- en: Stemming/Lemmatization
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词干提取/词元还原
- en: Local/Global dictionary
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地/全局字典
- en: Feature extraction/generation
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取/生成
- en: Feature representation and similarity
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征表示和相似性
- en: Feature selection and dimensionality reduction
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征选择和降维
- en: 'Topics in text mining:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本挖掘主题：
- en: Topic modeling
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题建模
- en: Text clustering
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本聚类
- en: Named entity recognition
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: Deep learning and NLP
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习和NLP
- en: 'Tools and usage:'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具和用法：
- en: Mallet
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mallet
- en: KNIME
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: KNIME
- en: Case study
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 案例研究
- en: NLP, subfields, and tasks
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP，子领域和任务
- en: Information about the real world exists in the form of structured data, typically
    generated by automated processes, or unstructured data, which, in the case of
    text, is created by direct human agency in the form of the written or spoken word.
    The process of observing real-world situations and using either automated processes
    or having humans perceive and convert that information into understandable data
    is very similar in both structured and unstructured data. The transformation of
    the observed world into unstructured data involves complexities such as the language
    of the text, the format in which it exists, variances among different observers
    in interpreting the same data, and so on. Furthermore, the ambiguity caused by
    the syntax and semantics of the chosen language, subtlety in expression, the context
    in the data, and so on, make the task of mining text data very difficult.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 实际世界的信息以结构化数据的形式存在，通常由自动化过程生成，或者以非结构化数据的形式存在，在文本的情况下，它是由直接的人类活动以书面或口头语言的形式创造的。观察现实世界情况并使用自动化过程或让人类感知并将该信息转换为可理解数据的过程，在结构化和非结构化数据中都非常相似。将观察到的世界转化为非结构化数据涉及诸如文本的语言、存在的格式、不同观察者对同一数据的解释差异等复杂性。此外，所选语言的语法和语义的歧义、表达的微妙之处、数据中的上下文等，使得挖掘文本数据变得非常困难。
- en: Next, we will discuss some high-level subfields and tasks that involve NLP and
    text mining. The subject of NLP is quite vast, and the following topics is in
    no way comprehensive.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论一些涉及NLP和文本挖掘的高级子领域和任务。NLP的主题非常广泛，以下话题远非全面。
- en: Text categorization
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本分类
- en: This field is one of the most well-established, and in its basic form classifies
    documents with unstructured text data into predefined categories. This can be
    viewed as a direct extension of supervised Machine Learning in the unstructured
    text world, learning from historic documents to predict categories of unseen documents
    in the future. Basic methods in spam detection in e-mails or news categorization
    are among some of the most prominent applications of this task.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个领域是最为成熟的之一，其基本形式是将包含非结构化文本数据的文档分类到预定义的类别中。这可以被视为在非结构化文本世界中监督机器学习的直接扩展，通过学习历史文档来预测未来未见文档的类别。在电子邮件垃圾邮件检测或新闻分类中的一些基本方法，是这个任务最突出的应用之一。
- en: '![Text categorization](img/B05137_08_001.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![文本分类](img/B05137_08_001.jpg)'
- en: 'Figure 1: Text Categorization showing classification into different categories'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：文本分类显示分类到不同的类别
- en: Part-of-speech tagging (POS tagging)
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词性标注（POS标注）
- en: Another subtask in NLP that has seen a lot of success is associating parts-of-speech
    of the language—such as nouns, adjectives, verbs—to words in a text, based on
    context and relationship to adjacent words. Today, instead of manual POS tagging,
    automated and sophisticated POS taggers perform the job.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，另一个取得巨大成功的子任务是，根据上下文和与相邻词语的关系，将语言的词性（如名词、形容词、动词）与文本中的词语关联起来。如今，我们不再进行手动词性标注，而是由自动化且复杂的词性标注器来完成这项工作。
- en: '![Part-of-speech tagging (POS tagging)](img/B05137_08_002.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![词性标注（POS标注）](img/B05137_08_002.jpg)'
- en: 'Figure 2: POS Tags associated with segment of text'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：与文本片段关联的词性标注
- en: Text clustering
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本聚类
- en: Clustering unstructured data for organization, retrieval, and groupings based
    on similarity is the subfield of text clustering. This field is also well-developed
    with advancements in different clustering and text representations suited for
    learning.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 文本聚类是将非结构化数据聚类以组织、检索和基于相似性的分组，这是文本聚类的子领域。这个领域也得到了很好的发展，不同聚类和适合学习的文本表示方法都有所进步。
- en: '![Text clustering](img/B05137_08_003.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![文本聚类](img/B05137_08_003.jpg)'
- en: 'Figure 3: Clustering of OS news documents to various OS specific clusters'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：将操作系统新闻文档聚类到各种操作系统特定集群
- en: Information extraction and named entity recognition
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信息提取和命名实体识别
- en: The task of extracting specific elements, such as time, location, organization,
    entities, and so on, comes under the topic of information extraction. Named entity
    recognition is a sub-field that has wide applications in different domains, from
    reviews of historical documents to bioinformatics with gene and drug information.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 提取特定元素的任务，如时间、地点、组织、实体等，属于信息提取的范畴。命名实体识别是一个在多个领域有广泛应用子领域，从历史文件的评论到生物信息学中的基因和药物信息。
- en: '![Information extraction and named entity recognition](img/B05137_08_004.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![信息提取和命名实体识别](img/B05137_08_004.jpg)'
- en: 'Figure 4: Named Entity Recognition in a sentence'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：句子中的命名实体识别
- en: Sentiment analysis and opinion mining
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 情感分析和意见挖掘
- en: Another sub-field in the area of NLP involves inferring the sentiments of observers
    in order to categorize them with an understandable metric or to give insights
    into their opinions. This area is not as advanced as some of the ones mentioned
    previously, but much research is being done in this direction.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）领域的另一个子领域涉及推断观察者的情感，以便用可理解的指标对他们进行分类，或者提供他们对意见的见解。这个领域不如前面提到的某些领域先进，但在这个方向上正在进行大量研究。
- en: '![Sentiment analysis and opinion mining](img/B05137_08_005.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![情感分析和意见挖掘](img/B05137_08_005.jpg)'
- en: 'Figure 5: Sentiment Analysis showing positive and negative sentiments for sentences'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：情感分析显示句子的正面和负面情感
- en: Coreference resolution
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指代消解
- en: Understanding references to multiple entities existing in the text and disambiguating
    that reference is another popular area of NLP. This is considered as a stepping
    stone in doing more complex tasks such as question answering and summarization,
    which will be discussed later.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 理解文本中存在的多个实体的引用并消除歧义是自然语言处理（NLP）中另一个流行的领域。这被认为是进行更复杂任务（如问答和摘要）的垫脚石，这些内容将在后面讨论。
- en: '![Coreference resolution](img/B05137_08_006.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![指代消解](img/B05137_08_006.jpg)'
- en: 'Figure 6: Coreference resolution showing how pronouns get disambiguated'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：指代消解显示代词如何被消歧
- en: Word sense disambiguation
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词义消歧
- en: In a language such as English, since the same word can have multiple meanings
    based on the context, deciphering this automatically is an important part of NLP,
    and the focus of **word sense disambiguation** (**WSD**).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在像英语这样的语言中，由于同一个词可以根据上下文有多种含义，自动解析这一点是自然语言处理（NLP）的一个重要部分，也是**词义消歧（WSD**）的焦点。
- en: '![Word sense disambiguation](img/B05137_08_007.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![词义消歧](img/B05137_08_007.jpg)'
- en: 'Figure 7: Showing how word "mouse" is associated with right word using the
    context'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：展示如何使用上下文将单词“mouse”与正确的单词关联起来
- en: Machine translation
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器翻译
- en: Translating text from one language to another or from speech to text in different
    languages is broadly covered in the area of **machine translation** (**MT**).
    This field has made significant progress in the last few years, with the usage
    of Machine Learning algorithms in supervised, unsupervised, and semi-supervised
    learning. Deep learning with techniques such as LSTM has been proved to be the
    most effective technique in this area, and is widely used by Google for its translation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本从一种语言翻译成另一种语言，或者在不同语言中将语音转换为文本，这在**机器翻译（MT**）领域得到了广泛的涵盖。这个领域在过去的几年里取得了显著的进步，机器学习算法在监督学习、无监督学习和半监督学习中的应用。使用LSTM等技术的深度学习已被证明是这个领域最有效的技术，并被Google广泛用于其翻译服务。
- en: '![Machine translation](img/B05137_08_008.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![机器翻译](img/B05137_08_008.jpg)'
- en: 'Figure 8: Machine Translation showing English to Chinese conversion'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：机器翻译显示英语到中文的转换
- en: Semantic reasoning and inferencing
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义推理和推断
- en: Reasoning, deriving logic, and inferencing from unstructured text is the next
    level of advancement in NLP.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从非结构化文本中推理、推导逻辑和进行推断是自然语言处理（NLP）进步的下一个层次。
- en: '![Semantic reasoning and inferencing](img/B05137_08_009.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![语义推理和推断](img/B05137_08_009.jpg)'
- en: 'Figure 9: Semantic Inferencing answering complex questions'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：语义推断回答复杂问题
- en: Text summarization
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本摘要
- en: A subfield that is growing in popularity in NLP is the automated summarization
    of large documents or passages of text to a small representative text that can
    be easily understood. This is one of the budding research areas in NLP. Search
    engines' usage of summaries, multi-document summarizations for experts, and so
    on, are some of the applications that are benefiting from this field.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，一个越来越受欢迎的子领域是将大型文档或文本段落自动总结成易于理解的小型代表性文本。这是NLP中一个新兴的研究领域。搜索引擎使用摘要、为专家提供的多文档摘要等，都是从这个领域受益的应用之一。
- en: Automating question and answers
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化问答
- en: Answering questions posed by humans in natural language, ranging from questions
    specific to a certain domain to generic, open-ended questions is another emerging
    field in the area of NLP.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 用自然语言回答人类提出的问题，这些问题从特定领域的具体问题到通用、开放式问题不等，是自然语言处理（NLP）领域另一个新兴的领域。
- en: Issues with mining unstructured data
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矿掘非结构化数据的问题
- en: 'Humans can read, parse, and understand unstructured text/documents more easily
    than computer-based programs. Some of the reasons why text mining is more complicated
    than general supervised or unsupervised learning are given here:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于计算机的程序相比，人类更容易阅读、解析和理解非结构化文本/文档。以下是为什么文本挖掘比一般的监督学习或无监督学习更复杂的一些原因：
- en: Ambiguity in terms and phrases. The word *bank* has multiple meanings, which
    a human reader can correctly associate based on context, yet this requires preprocessing
    steps such as POS tagging and word sense disambiguation, as we have seen. According
    to the Oxford English Dictionary, the word *run* has no fewer than 645 different
    uses in the verb form alone and we can see that such words can indeed present
    problems in resolving the meaning intended (between them, the words run, put,
    set, and take have more than a thousand meanings).
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 术语和短语的不确定性。单词“bank”有多个含义，人类读者可以根据上下文正确地将其关联起来，但这需要预处理步骤，如词性标注和词义消歧，正如我们所看到的。根据牛津高阶英汉双解大词典，单词“run”在动词形式下就有不少于645种不同的用法，我们可以看到这样的单词确实会在解决意图含义（例如，run、put、set和take这些词之间有超过一千种含义）时带来问题。
- en: Context and background knowledge associated with the text. Consider a sentence
    that uses a neologism with the suffix *gate* to signify a political scandal, as
    in, *With cries for impeachment and popularity ratings in a nosedive, Russiagate
    finally dealt a deathblow to his presidency*. A human reader can surmise what
    is being referred to by the coinage *Russiagate* as something that recalls the
    sense of high-profile intrigue, by association via an affix, of another momentous
    scandal in US political history, *Watergate*. This is particularly difficult for
    a machine to make sense of.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与文本相关的上下文和背景知识。考虑一个使用后缀“gate”来表示政治丑闻的新词，例如，“随着弹劾的呼声和民意调查的急剧下降，Russiagate最终给他的总统生涯带来了致命一击”。人类读者可以通过联想，通过前缀与另一个美国政治历史上的重大丑闻“Watergate”的关联，推断出“Russiagate”所指的是什么，即回忆起高调阴谋的感觉。这对机器来说尤其难以理解。
- en: Reasoning, that is, inferencing from documents is very difficult as mapping
    unstructured information to knowledge bases is itself a big hurdle.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理，即从文档中进行推断，是非常困难的，因为将非结构化信息映射到知识库本身就是一大障碍。
- en: Ability to perform supervised learning needs labeled training documents and
    based on the domain, performing labeling on the documents can be time consuming
    and costly.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行监督学习的能力需要标记的训练文档，并且根据领域，对文档进行标记可能既耗时又昂贵。
- en: Text processing components and transformations
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本处理组件和转换
- en: In this section, we will discuss some common preprocessing and transformation
    steps that are done in most text mining processes. The general concept is to convert
    the documents into structured datasets with features or attributes that most Machine
    Learning algorithms can use to perform different kinds of learning.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论在大多数文本挖掘过程中执行的一些常见的预处理和转换步骤。一般概念是将文档转换为具有特征或属性的结构化数据集，这些特征或属性是大多数机器学习算法可以用来执行不同类型学习的基础。
- en: 'We will briefly describe some of the most used techniques in the next section.
    Different applications of text mining might use different pieces or variations
    of the components shown in the following figure:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节简要描述一些最常用的技术。文本挖掘的不同应用可能使用以下图中显示的组件的不同部分或变体：
- en: '![Text processing components and transformations](img/B05137_08_010.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![文本处理组件和转换](img/B05137_08_010.jpg)'
- en: 'Figure 10: Text Processing components and the flow'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：文本处理组件和流程
- en: Document collection and standardization
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文档收集和标准化
- en: One of the first steps in most text mining applications is the collection of
    data in the form of a body of documents—often referred to as a *corpus* in the
    text mining world. These documents can have predefined categorization associated
    with them or it can simply be an unlabeled corpus. The documents can be of heterogeneous
    formats or standardized into one format for the next process of tokenization.
    Having multiple formats such as text, HTML, DOCs, PDGs, and so on, can lead to
    many complexities and hence one format, such as XML or **JavaScript Object Notation**
    (**JSON**) is normally preferred in most applications.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数文本挖掘应用中的第一步通常是收集以文档形式存在的数据——在文本挖掘领域通常被称为*语料库*。这些文档可以与预定义的分类相关联，或者它可能只是一个未标记的语料库。这些文档可以是异构格式，或者为了下一阶段的分词过程而标准化为一种格式。拥有多种格式，如文本、HTML、DOCs、PDGs等，可能会导致许多复杂性，因此大多数应用中通常首选一种格式，如XML或**JavaScript对象表示法**（**JSON**）。
- en: Inputs and outputs
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Inputs are vast collections of homogeneous or heterogeneous sources and outputs
    are a collection of documents standardized into one format, such as XML.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是大量同质或异质来源的集合，输出是一组标准化为一种格式（如XML）的文档集合。
- en: How does it work?
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: 'Standardization involves ensuring tools and formats are agreed upon based on
    the needs of the application:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化涉及到确保工具和格式基于应用需求达成一致：
- en: Agree to a standard format, such as XML, with predefined tags that provide information
    on meta-attributes of documents `(<author>`, `<title>`, `<date>`, and so on) and
    actual content, such as `<document>`.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就标准格式达成一致，如XML，其中预定义的标签提供了关于文档元属性（如`<author>`、`<title>`、`<date>`等）和实际内容（如`<document>`）的信息。
- en: Most document processors can either be transformed into XML or transformation
    code can be written to perform this.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大多数文档处理器都可以转换为XML，或者可以编写转换代码来执行此操作。
- en: Tokenization
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词
- en: The task of tokenization is to extract words or meaningful characters from the
    text containing a stream of these words. For example, the text *The boy stood
    up. He then ran after the dog* can be tokenized into tokens such as *{the, boy,
    stood, up, he, ran, after, the, dog}*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 分词的任务是从包含这些单词的文本流中提取单词或有意义的字符。例如，文本*The boy stood up. He then ran after the
    dog*可以被分词成标记，如*{the, boy, stood, up, he, ran, after, the, dog}*。
- en: Inputs and outputs
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入和输出
- en: An input is a collection of documents in a well-known format as described in
    the last section and an output is a document with tokens of words or characters
    as needed in the application.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是一组如上一节所述的已知格式的文档，输出是包含所需单词或字符标记的文档。
- en: How does it work?
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: 'Any automated system for tokenization must address the particular challenges
    presented by the language(s) it is expected to handle:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 任何自动化的分词系统都必须解决它预期处理的语言（们）所提出的特定挑战：
- en: In languages such as English, tokenization is relatively simple due to the presence
    of white space, tabs, and newline for separating the words.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在像英语这样的语言中，由于存在空白、制表符和换行符来分隔单词，分词相对简单。
- en: There are different challenges in each language—even in English, abbreviations
    such as *Dr.*, alphanumeric characters (*B12*), different naming schemes (*O'Reilly*),
    and so on, must be tokenized appropriately.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每种语言都有不同的挑战——即使在英语中，像*Dr.*这样的缩写、代数字符（*B12*）、不同的命名方案（*O'Reilly*）等，也必须适当地分词。
- en: Language-specific rules in the form of if-then instructions are written to extract
    tokens from the documents.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写特定于语言的规则，以if-then指令的形式从文档中提取标记。
- en: Stop words removal
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 停用词移除
- en: This involves removing high frequency words that have no discriminatory or predictive
    value. If every word can be viewed as a feature, this process reduces the dimension
    of the feature vector by a significant number. Prepositions, articles, and pronouns
    are some of the examples that form the stop words that are removed without affecting
    the performance of text mining in many applications.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及到移除没有区分性或预测价值的常用词。如果每个词都可以被视为一个特征，这个过程就会通过显著减少特征向量的维度来降低特征数量。介词、冠词和代词是形成停用词的一些例子，这些停用词被移除，而不会影响许多应用中文本挖掘的性能。
- en: Inputs and outputs
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入和输出
- en: An input is a collection of documents with the tokens extracted and an output
    is a collection of documents with tokens reduced by removing stop words.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是一组已提取标记的文档，输出是一组通过移除停用词来减少标记的文档。
- en: How does it work?
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: There are various techniques that have evolved in the last few years ranging
    from manually precompiled lists to statistical elimination using either term-based
    or mutual information.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，已经发展出各种技术，从手动预编译的列表到基于术语或互信息的统计消除。
- en: The most commonly used technique for many languages is a manually precompiled
    list of stop words, including prepositions (in, for, on), articles (a, an, the),
    pronouns (his, her, they, their), and so on.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于许多语言来说，最常用的技术是手动预编译的停用词列表，包括介词（in、for、on）、冠词（a、an、the）、代词（his、her、they、their）等等。
- en: Many tools use Zipf's law (*References* [3]), where high frequency words, singletons,
    and unique terms are removed. Luhn's early work (*References* [4]), as represented
    in the following figure 11, shows thresholds of the upper bound and lower bound
    of word frequency, which give us the significant words that can be used for modeling:![How
    does it work?](img/B05137_08_012.jpg)
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多工具使用Zipf定律（*参考文献* [3]），其中移除了高频词、单音节词和独特术语。Luhn的早期工作（*参考文献* [4]），如图11所示，显示了单词频率的上限和下限阈值，这些阈值为我们提供了可用于建模的显著单词：![如何工作？](img/B05137_08_012.jpg)
- en: 'Figure 11: Word Frequency distribution, showing how frequently used, significant
    and rare words exist in corpus'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11：单词频率分布，显示了在语料库中频繁使用、显著和罕见单词的存在
- en: Stemming or lemmatization
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词干提取或词形还原
- en: The idea of normalizing tokens of similar words into one is known as stemming
    or lemmatization. Thus, reducing all occurrences of "talking", "talks", "talked",
    and so on in a document to one root word "talk" in the document is an example
    of stemming.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 将相似单词的标记归一化为一个的想法被称为词干提取或词形还原。因此，将文档中所有"talking"、"talks"、"talked"等出现归一化到一个根词"talk"的例子就是词干提取。
- en: Inputs and outputs
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入和输出
- en: An input is documents with tokens and an output is documents with reduced tokens
    normalized to their stem or root words.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是带有标记的文档，输出是带有减少标记并归一化到其词干或根词的文档。
- en: How does it work?
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: 'There are basically two types of stemming: inflectional stemming and stemming
    to the root.'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基本上有两种词干提取类型：变化词干提取和根词提取。
- en: Inflectional stemming generally involves removing affixes, normalizing the verb
    tenses and removing plurals. Thus, "ships" to "ship", "is", "are" and "am" to
    "be" in English.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变化词干提取通常涉及去除词缀，使动词时态正常化并去除复数形式。因此，英语中的"ships"变为"ship"，"is"、"are"和"am"变为"be"。
- en: Stemming to the root is generally a more aggressive form than inflectional stemming,
    where words are normalized to their roots. An example of this is "applications",
    "applied", "reapply", and so on, all reduced to the root word "apply".
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将词干提取到根词通常比变化词干提取更为激进，后者将单词归一化到其根词。这种例子是"applications"、"applied"、"reapply"等等，都减少到根词"apply"。
- en: Lovin's stemmer was one of the first stemming algorithms (*References* [1]).
    Porter's stemming, which evolved in the 1980s with around 60 rules in 6 steps,
    is still the most widely used form of stemming (*References* [2]).
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lovin的词干提取算法是第一批词干提取算法之一（*参考文献* [1]）。Porter的词干提取，在20世纪80年代发展起来，有60条规则分6步进行，仍然是应用最广泛的词干提取形式（*参考文献*
    [2]）。
- en: Present-day applications drive a wide range of statistical techniques based
    on stemming, including those using n-grams (a contiguous sequence of n items,
    either letters or words from a given sequence of text), **hidden Markov models**
    (**HMM**), and context-sensitive stemming.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当前的应用推动了基于词干提取的广泛统计技术，包括使用n-gram（给定文本序列中n个连续项的连续序列，可以是字母或单词）、**隐马尔可夫模型**（**HMM**）和上下文敏感词干提取。
- en: Local/global dictionary or vocabulary?
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局部/全局字典或词汇表？
- en: Once the preprocessing task of converting documents into tokens is performed,
    the next step is the creation of a corpus or vocabulary as a single dictionary,
    using all the tokens from all documents. Alternatively, several dictionaries are
    created based on category, using specific tokens from fewer documents.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦执行了将文档转换为标记的预处理任务，下一步就是创建一个语料库或词汇表，作为一个单一的字典，使用所有文档中的所有标记。或者，根据类别创建几个字典，使用较少文档中的特定标记。
- en: Many applications in topic modeling and text categorization perform well when
    dictionaries are created per topic/category, which is known as a local dictionary.
    On the other hand, many applications in document clustering and information extraction
    perform well when one single global dictionary is created from all the document
    tokens. The choice of creating one or many specific dictionaries depends on the
    core NLP task, as well as on computational and storage requirements.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在主题建模和文本分类的许多应用中，当按主题/类别创建字典时，即所谓的本地字典，表现良好。另一方面，在文档聚类和信息提取的许多应用中，当从所有文档标记中创建一个单一的全局字典时，表现良好。创建一个或多个特定字典的选择取决于核心NLP任务，以及计算和存储需求。
- en: Feature extraction/generation
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征提取/生成
- en: A key step in converting the document(s) with unstructured text is to transform
    them into datasets with structured features, similar to what we have seen so far
    in Machine Learning datasets. Extracting features from text so that it can be
    used in Machine Learning tasks such as supervised, unsupervised, and semi-supervised
    learning depends on many factors, such as the goals of the applications, domain-specific
    requirements, and feasibility. There are a wide variety of features, such as words,
    phrases, sentences, POS-tagged words, typographical elements, and so on, that
    can be extracted from any document. We will give a broad range of features that
    are commonly used in different Machine Learning applications.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 将包含非结构化文本的文档（s）转换为具有结构化特征的集合的关键步骤是将它们转换为类似于我们在迄今为止的机器学习数据集中所看到的结构化特征集合。从文本中提取特征以便在机器学习任务中使用，如监督学习、无监督学习和半监督学习，这取决于许多因素，例如应用目标、特定领域的需求以及可行性。可以从任何文档中提取各种特征，例如单词、短语、句子、词性标注的单词、排版元素等。我们将给出在不同的机器学习应用中常用的一系列特征。
- en: Lexical features
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词汇特征
- en: Lexical features are the most frequently used features in text mining applications.
    Lexical features form the basis for the next level of features. They are the simple
    character- or word- level features constructed without trying to capture information
    about intent or the various meanings associated with the text. Lexical features
    can be further broken down into character-based features, word-based features,
    part-of-speech features, and taxonomies, for example. In the next section, we
    will describe some of them in greater detail.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇特征是文本挖掘应用中最常用的特征。词汇特征是下一级特征的基础。它们是构建在未尝试捕获关于意图或与文本相关的各种含义的信息的简单字符或单词级别的特征。词汇特征可以进一步细分为基于字符的特征、基于单词的特征、词性特征和分类法，例如。在下一节中，我们将更详细地描述其中的一些。
- en: Character-based features
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于字符的特征
- en: Individual characters (unigram) or a sequence of characters (n-gram) are the
    simplest forms of features that can be constructed from the text document. The
    bag of characters or unigram characters have no positional information, while
    higher order n-grams capture some amount of context and positional information.
    These features can be encoded or given numeric values in different ways, such
    as binary 0/1 values, or counts, for example, as discussed later in the next section.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 单个字符（单语元）或字符序列（n-gram）是从文本文档中可以构建的最简单形式的特征。字符包或单语元字符没有位置信息，而高阶n-gram捕获了一定程度的内容和位置信息。这些特征可以用不同的方式编码或赋予数值，例如二进制0/1值或计数，如下一节中所述。
- en: Let us consider the memorable Dr. Seuss rhyme as the text content—"the Cat in
    the Hat steps onto the mat". While the bag-of-characters (1-gram or unigram features)
    will generate unique characters {"t","h", "e", "c","a","i","n","s","p","o","n","m"}
    as features, the 3-gram features are { "\sCa" ,"\sHa", "\sin" , "\sma", "\son",
    "\sst", "\sth", "Cat", "Hat", "at\s", "e\sC", "e\sH", "e\sm", "eps", "he\s", "in\s
    ", "mat", "n\st", "nto", "o\st", "ont", "ps\s", "s\so" , "ste"," t\si"," t\ss"
    , "tep", "the", "to\s "}. As can be seen, as "n" increases, the number of features
    increases exponentially and soon becomes unwieldy. The advantage of n-grams is
    that at the cost of increasing the total number of features, the assembled features
    often seem to capture combinations of characters that are more interesting than
    the individual characters themselves.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以难忘的苏斯博士的韵文作为文本内容——“the Cat in the Hat steps onto the mat”。虽然字符袋（1-gram或单词特征）将生成唯一的字符{"t","h",
    "e", "c","a","i","n","s","p","o","n","m"}作为特征，但3-gram特征为{"\sCa" ,"\sHa", "\sin"
    , "\sma", "\son", "\sst", "\sth", "Cat", "Hat", "at\s", "e\sC", "e\sH", "e\sm",
    "eps", "he\s", "in\s ", "mat", "n\st", "nto", "o\st", "ont", "ps\s", "s\so" ,
    "ste"," t\si"," t\ss" , "tep", "the", "to\s "}。正如所见，随着“n”的增加，特征的数量呈指数增长，很快变得难以控制。n-gram的优势在于，在增加总特征数量的同时，组装的特征往往似乎捕捉到了比单个字符本身更有趣的字符组合。
- en: Word-based features
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于单词的特征
- en: Instead of generating features from characters, features can similarly be constructed
    from words in a unigram and n-gram manner. These are the most popular feature
    generation techniques. The unigram or 1-word token is also known as the bag of
    words model. So the example of "the Cat in the Hat steps onto the mat" when considered
    as unigram features is {"the", "Cat", "in", "Hat", "steps", "onto", "mat"}. Similarly,
    bigram features on the same text would result in {"the Cat", "Cat in", "in the",
    "the Hat", "Hat step", "steps onto", "onto the", "the mat"}. As in the case of
    character-based features, by going to higher "n" in the n-grams, the number of
    features increases, but so does the ability to capture word sense via context.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 与从字符生成特征不同，特征也可以以单词和n-gram的方式从单词中构建。这些是最流行的特征生成技术。单词或1-gram标记也被称为词袋模型。因此，当“the
    Cat in the Hat steps onto the mat”作为单词特征考虑时，其例子为{"the", "Cat", "in", "Hat", "steps",
    "onto", "mat"}。同样，同一文本上的二元特征将产生{"the Cat", "Cat in", "in the", "the Hat", "Hat
    step", "steps onto", "onto the", "the mat"}。与基于字符的特征类似，通过在n-gram中增加到更高的“n”，特征的数量增加，但捕捉单词意义的能力也随之增强。
- en: Part-of-speech tagging features
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 词性标注特征
- en: The input is text with words and the output is text where every word is associated
    with the grammatical tag. In many applications, part-of-speech gives a context
    and is useful in identification of named entities, phrases, entity disambiguation,
    and so on. In the example "the Cat in the Hat steps onto the mat" , the output
    is {"the\Det", "Cat\Noun", "in\Prep", "the\Det", "Hat\Noun", "steps\Verb", "onto\Prep",
    "the\Det", "mat\Noun"}. Language specific rule-based taggers or Markov chain–based
    probabilistic taggers are often used in this process.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是包含单词的文本，输出是每个单词都与语法标签关联的文本。在许多应用中，词性提供了上下文，并在识别命名实体、短语、实体消歧等方面很有用。在“the Cat
    in the Hat steps onto the mat”的例子中，输出为{"the\Det", "Cat\Noun", "in\Prep", "the\Det",
    "Hat\Noun", "steps\Verb", "onto\Prep", "the\Det", "mat\Noun"}。在此过程中，通常使用基于语言规则的标记器或基于马尔可夫链的概率标记器。
- en: Taxonomy features
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分类学特征
- en: Creating taxonomies from the text data and using it to understand relationships
    between words is also useful in different contexts. Various taxonomical features
    such as hypernyms, hyponyms, is-member, member-of, is-part, part-of, antonyms,
    synonyms, acronyms, and so on, give lexical context that proves useful in searches,
    retrieval, and matching in many text mining scenarios.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本数据创建分类法并使用它来理解单词之间的关系在不同情境中也很有用。各种分类学特征，如上位词、下位词、成员、成员-of、部分、部分-of、反义词、同义词、首字母缩略词等，为搜索、检索和许多文本挖掘场景中的匹配提供了有益的词汇上下文。
- en: Syntactic features
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语法特征
- en: The next level of features that are higher than just characters or words in
    text documents are the syntax-based features. Syntactical representation of sentences
    in text is generally in the form of syntax trees. Syntax trees capture nodes as
    terms used in sentences, and relationships between the nodes are captured as links.
    Syntactic features can also capture more complex features about sentences and
    usage—such as aggregates—that can be used for Machine Learning. It can also capture
    statistics about syntax trees—such as sentences being left-heavy, right-heavy,
    or balanced—that can be used to understand signatures of different content or
    writers.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 低于文本文档中的字符或单词的下一级特征是基于句法的特征。文本中句子的句法表示通常以句法树的形式出现。句法树捕获句子中使用的术语作为节点，节点之间的关系以链接的形式捕获。句法特征还可以捕获关于句子和使用的更复杂特征——例如聚合——这些特征可用于机器学习。它还可以捕获关于句法树的统计数据——例如句子是左重、右重或平衡——这些数据可用于理解不同内容或作者的签名。
- en: 'Two sentences can have the same characters and words in the lexical analysis,
    but their syntax trees or intent could be completely different. Breaking the sentences
    in the text into different phrases—**Noun Phrase** (**NP**), **Prepositional Phrase**
    (**PP**), **Verbal** (or Gerund) **Phrase** (**VP**), and so on—and capturing
    phrase structure trees for the sentences are part of this processing task. The
    following is the syntactic parse tree for our example sentence:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 两个句子在词汇分析中可能有相同的字符和单词，但它们的句法树或意图可能完全不同。将文本中的句子分解成不同的短语——**名词短语**（**NP**）、**介词短语**（**PP**）、**动词短语**（或动名词短语）**VP**）等——并捕获句子的短语结构树是这一处理任务的一部分。以下是我们示例句子的句法分析树：
- en: '[PRE0]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Syntactic Language Models** (**SLM**) are about determining the probability
    of a sequence of terms. Language model features are used in machine translation,
    spelling correction, speech translation, summarization, and so on, to name a few
    of the applications. Language models can additionally use parse trees and syntax
    trees in their computation as well.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**句法语言模型**（**SLM**）是确定术语序列的概率。语言模型特征用于机器翻译、拼写校正、语音翻译、摘要等应用，仅举几例。语言模型还可以在其计算中使用分析树和句法树。'
- en: 'The chain rule is applied to compute the joint probability of terms in the
    sentence:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则用于计算句子中术语的联合概率：
- en: '![Syntactic features](img/B05137_08_013.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![句法特征](img/B05137_08_013.jpg)'
- en: 'In the example "the cat in the hat steps onto the mat":'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例 "the cat in the hat steps onto the mat" 中：
- en: '![Syntactic features](img/B05137_08_014.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![句法特征](img/B05137_08_014.jpg)'
- en: Generally, the estimation of the probability of long sentences based on counts
    using any corpus is difficult due to the need for many examples of such sentences.
    Most language models use the Markov assumption of independence and n-grams (2-5
    words) in practical implementations (*References* [8]).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，由于需要大量此类句子的示例，基于任何语料库使用计数来估计长句的概率是困难的。大多数语言模型在实际实现中采用马尔可夫独立假设和 n-gram（2-5
    个单词）(*参考文献* [8])。
- en: Semantic features
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语义特征
- en: Semantic features attempt to capture the "meaning" of the text, which is then
    used for different applications of text mining. One of the simplest forms of semantic
    features is the process of adding annotations to the documents. These annotations,
    or metadata, can have additional information that describes or captures the intent
    of the text or documents. Adding tags using collaborative tagging to capture tags
    as keywords describing the text is a common semantic feature generation process.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 语义特征试图捕捉文本的“意义”，然后用于文本挖掘的不同应用。语义特征的最简单形式之一是对文档添加注释的过程。这些注释或元数据可以包含描述或捕捉文本或文档意图的附加信息。使用协作标记添加标签以捕获描述文本的关键字是常见的语义特征生成过程。
- en: Another form of semantic feature generation is the process of ontological representation
    of the text. Generic and domain specific ontologies that capture different relationships
    between objects are available in knowledge bases and have well-known specifications,
    such as Semantic Web 2.0\. These ontological features help in deriving complex
    inferencing, summarization, classification, and clustering tasks in text mining.
    The terms in the text or documents can be mapped to "concepts" in ontologies and
    stored in knowledge bases. These concepts in ontologies have semantic properties,
    and are related to other concepts in a number of ways, such as generalization/specialization,
    member-of/isAMember, association, and so on, to name a few. These attributes or
    properties of concepts and relationships can be further used in search, retrieval,
    and even in predictive modeling. Many semantic features use the lexical and syntactic
    processes as pre-cursors to the semantic process and use the outputs, such as
    nouns, to map to concepts in ontologies, for example. Adding the concepts to an
    existing ontology or annotating it with more concepts makes the structure more
    suitable for learning. For example, in the "the cat in the .." sentence, "cat"
    has properties such as {age, eats, ...} and has different relationships, such
    as { "isA Mammal", "hasChild", "hasParent", and so on}.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种语义特征生成形式是文本的本体表示过程。在知识库中可用的通用和领域特定本体捕获了对象之间不同的关系，并且具有众所周知的规范，例如语义网2.0。这些本体特征有助于在文本挖掘中推导复杂的推理、摘要、分类和聚类任务。文本或文档中的术语可以映射到本体中的“概念”并存储在知识库中。这些本体中的概念具有语义属性，并以多种方式与其他概念相关联，例如泛化/特殊化、成员-of/是成员、关联等，仅举几例。这些概念或关系的属性或属性可以进一步用于搜索、检索甚至在预测建模中。许多语义特征使用词汇和句法过程作为语义过程的先导，并使用输出，如名词，将其映射到本体中的概念，例如。将概念添加到现有本体或用更多概念对其进行注释，使结构更适合学习。例如，在“the
    cat in the ..”这个句子中，“cat”具有诸如{age, eats, ...}等属性，并且有不同的关系，如{ "isA Mammal", "hasChild",
    "hasParent"，等等}。
- en: Feature representation and similarity
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征表示和相似性
- en: Lexical, syntactic, and semantic features, described in the last section, often
    have representations that are completely different from each other. Representations
    of the same feature type, that is, lexical, syntactic, or semantic, can differ
    based on the computation or mining task for which they are employed. In this section,
    we will describe the most common lexical feature-based representation known as
    vector space models.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中描述的词汇、句法和语义特征，通常具有完全不同的表示。同一特征类型的表示，即词汇、句法或语义，可以根据它们被用于的计算或挖掘任务而有所不同。在本节中，我们将描述最常用的基于词汇特征表示，即向量空间模型。
- en: Vector space model
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量空间模型
- en: The **vector space model** (**VSM**) is a transformation of the unstructured
    document to a numeric vector representation where terms in the corpus form the
    dimensions of the vector and we use some numeric way of associating value with
    these dimensions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量空间模型**（**VSM**）是将非结构化文档转换为数值向量表示的一种转换，其中语料库中的术语形成向量的维度，我们使用某种数值方式将这些维度与值关联。'
- en: As discussed in the section on dictionaries, a corpus is formed out of unique
    words and phrases from the entire collection of documents in a domain or within
    local sub-categories of one. Each of the elements of such a dictionary are the
    dimensions of the vector. The terms—which can be single words or phrases, as in
    n-grams—form the dimensions and can have different values associated with them
    in a given text/document. The goal is to capture the values in the dimensions
    in a way that reflects the relevancy of the term(s) in the entire corpus (*References*
    [11]). Thus, each document or file is represented as a high-dimensional numeric
    vector. Due to the sparsity of terms, the numeric vector representation has a
    sparse representation in numeric space. Next, we will give some well-known ways
    of associating values to these terms.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如在字典章节所述，语料库是由来自一个领域或一个局部子类别中所有文档的独特单词和短语组成的。这样一个字典的每个元素都是向量的维度。这些术语——可以是单个单词或短语，如n-gram——形成维度，并且在一个给定的文本/文档中可以与它们相关联不同的值。目标是按反映整个语料库中术语（*参考文献*
    [11]）的相关性的方式捕获维度中的值。因此，每个文档或文件都表示为一个高维数值向量。由于术语的稀疏性，数值向量表示在数值空间中具有稀疏表示。接下来，我们将介绍一些将这些术语与值关联的知名方法。
- en: Binary
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 二进制
- en: 'This is the simplest form of associating value to the terms, or dimensions.
    In binary form, each term in the corpus is given a 0 or 1 value based on the presence
    or absence of the term in the document. For example, consider the following three
    documents:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单的将值关联到术语或维度的形式。在二进制形式中，语料库中的每个术语根据术语在文档中的出现与否被赋予0或1的值。例如，考虑以下三个文档：
- en: 'Document 1: "The Cat in the Hat steps onto the mat"'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '文档 1: "The Cat in the Hat steps onto the mat"'
- en: 'Document 2: "The Cat sat on the Hat"'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '文档 2: "The Cat sat on the Hat"'
- en: 'Document 3: "The Cat loves to step on the mat"'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '文档 3: "The Cat loves to step on the mat"'
- en: 'After preprocessing by removing stop words {on, the, in, onto} and stemming
    {love/loves, steps/step} using a unigram or bag of words, {cat, hat, step, mat,
    sat, love} are the features of the corpus. Each document is now represented in
    a binary vector space model as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用单语元或词袋模型，通过去除停用词{on, the, in, onto}和词干提取{love/loves, steps/step}进行预处理后，{cat,
    hat, step, mat, sat, love}是语料库的特征。现在每个文档都表示为一个二元向量空间模型，如下所示：
- en: '| Terms | cat | hat | step | mat | sat | love |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 术语 | cat | hat | step | mat | sat | love |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Document 1 | 1 | 1 | 1 | 1 | 0 | 0 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 文档 1 | 1 | 1 | 1 | 1 | 0 | 0 |'
- en: '| Document 2 | 1 | 1 | 0 | 0 | 1 | 0 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 文档 2 | 1 | 1 | 0 | 0 | 1 | 0 |'
- en: '| Document 3 | 1 | 0 | 1 | 1 | 0 | 1 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 文档 3 | 1 | 0 | 1 | 1 | 0 | 1 |'
- en: Term frequency (TF)
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 术语频率（TF）
- en: 'In **term frequency** (**TF**), as the name suggests, the frequency of terms
    in the entire document forms the numeric value of the feature. The basic assumption
    is that the higher the frequency of the term, the greater the relevance of that
    term for the document. Counts of terms or normalized counts of terms are used
    as values in each column of terms:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在**词频**（**TF**）中，正如其名所示，整个文档中术语的频率构成了特征的数值。基本假设是，术语的频率越高，该术语与文档的相关性就越大。术语的计数或归一化计数被用作每个术语列的值：
- en: '*tf(t) = count(D, t)*'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*tf(t) = count(D, t)*'
- en: 'The following table gives term frequencies for the three documents in our example:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 下表给出了我们示例中三个文档的术语频率：
- en: '| TF / Terms | cat | hat | step | mat | sat | love |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| TF / 术语 | cat | hat | step | mat | sat | love |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Document 1 | 1 | 1 | 1 | 1 | 0 | 0 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 文档 1 | 1 | 1 | 1 | 1 | 0 | 0 |'
- en: '| Document 2 | 1 | 1 | 0 | 0 | 1 | 0 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 文档 2 | 1 | 1 | 0 | 0 | 1 | 0 |'
- en: '| Document 3 | 1 | 0 | 1 | 1 | 0 | 1 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 文档 3 | 1 | 0 | 1 | 1 | 0 | 1 |'
- en: Inverse document frequency (IDF)
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 逆文档频率（IDF）
- en: '**Inverse document frequency** (**IDF**) has various flavors, but the most
    common way of computing it is using the following:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**逆文档频率**（**IDF**）有多种形式，但计算它的最常见方法是使用以下方法：'
- en: '![Inverse document frequency (IDF)](img/B05137_08_016.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![逆文档频率 (IDF)](img/B05137_08_016.jpg)'
- en: Here, ![Inverse document frequency (IDF)](img/B05137_08_017.jpg) ![Inverse document
    frequency (IDF)](img/B05137_08_018.jpg) IDF favors mostly those terms that occur
    relatively infrequently in the documents. Some empirically motivated improvements
    to IDF have also been proposed in the research (*References* [7]).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![逆文档频率 (IDF)](img/B05137_08_017.jpg) ![逆文档频率 (IDF)](img/B05137_08_018.jpg)
    IDF主要青睐那些在文档中相对不常出现的术语。研究（*参考文献* [7]）中也提出了对IDF的一些基于经验动机的改进。
- en: 'TF for our example corpus:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: TF对于我们示例语料库：
- en: '| Terms | cat | hat | step | mat | sat | love |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 术语 | cat | hat | step | mat | sat | love |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| N/nj | 3/3 | 3/2 | 3/2 | 3/2 | 3/1 | 3/1 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| N/nj | 3/3 | 3/2 | 3/2 | 3/2 | 3/1 | 3/1 |'
- en: '| IDF | 0.0 | 0.40 | 0.40 | 0.40 | 1.10 | 1.10 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| IDF | 0.0 | 0.40 | 0.40 | 0.40 | 1.10 | 1.10 |'
- en: Term frequency-inverse document frequency (TF-IDF)
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 术语频率-逆文档频率（TF-IDF）
- en: 'Combining both term frequencies and inverse document frequencies in one metric,
    we get the term frequency-inverse document frequency values. The idea is to value
    those terms that are relatively uncommon in the corpus (high IDF), but are reasonably
    relevant for the document (high TF). TF-IDF is the most common form of value association
    in many text mining processes:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 将术语频率和逆文档频率结合在一个指标中，我们得到术语频率-逆文档频率值。其思想是重视那些在语料库中相对不常见（高IDF），但对于文档来说相对相关（高TF）的术语。TF-IDF是许多文本挖掘过程中最常见的值关联形式：
- en: '![Term frequency-inverse document frequency (TF-IDF)](img/B05137_08_019.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![术语频率-逆文档频率 (TF-IDF)](img/B05137_08_019.jpg)'
- en: 'This gives us the TF-IDF for all the terms in each of the documents:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了每个文档中所有术语的TF-IDF值：
- en: '| TF-IDF/Terms | cat | hat | step | mat | sat | love |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| TF-IDF/术语 | cat | hat | step | mat | sat | love |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Document 1 | 0.0 | 0.40 | 0.40 | 0.40 | 1.10 | 1.10 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 文档1 | 0.0 | 0.40 | 0.40 | 0.40 | 1.10 | 1.10 |'
- en: '| Document 2 | 0.0 | 0.40 | 0.0 | 0.0 | 1.10 | 0.0 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 文档2 | 0.0 | 0.40 | 0.0 | 0.0 | 1.10 | 0.0 |'
- en: '| Document 3 | 0.0 | 0.0 | 0.40 | 0.40 | 0.0 | 1.10 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 文档3 | 0.0 | 0.0 | 0.40 | 0.40 | 0.0 | 1.10 |'
- en: Similarity measures
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相似度度量
- en: Many techniques in supervised, unsupervised, and semi-supervised learning use
    "similarity" measures in their underlying algorithms to find similar patterns
    or to separate different patterns. Similarity measures are tied closely to the
    representation of the data. In the VSM representation of documents, the vectors
    are very high dimensional and sparse. This poses a serious issue in most traditional
    similarity measures for classification, clustering, or information retrieval.
    Angle-based similarity measures, such as cosine distances or Jaccard coefficients,
    are more often used in practice. Consider two vectors represented by **t**[1]
    and **t**[2] corresponding to two text documents.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习、无监督学习和半监督学习中的许多技术在其底层算法中使用“相似度”度量来寻找相似模式或分离不同模式。相似度度量与数据的表示紧密相关。在文档的VSM表示中，向量是高维且稀疏的。这在分类、聚类或信息检索的大多数传统相似度度量中提出了一个严重问题。基于角度的相似度度量，如余弦距离或Jaccard系数，在实践中更常被使用。考虑由
    **t**[1] 和 **t**[2] 表示的两个向量，它们对应于两个文本文档。
- en: Euclidean distance
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: 'This is the L2 norm in the feature space of the documents:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这是文档特征空间中的L2范数：
- en: '![Euclidean distance](img/B05137_08_023.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![欧几里得距离](img/B05137_08_023.jpg)'
- en: Cosine distance
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 余弦距离
- en: 'This angle-based similarity measure considers orientation between vectors only
    and not their lengths. It is equal to the cosine of the angle between the vectors.
    Since the vector space model is a positive space, cosine distance varies from
    0 (orthogonal, no common terms) to 1 (all terms are common to both, but not necessarily
    with the same term frequency):'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于角度的相似度度量仅考虑向量之间的方向，而不考虑它们的长度。它等于向量之间角度的余弦值。由于向量空间模型是一个正空间，余弦距离从0（正交，没有共同项）变化到1（所有项都是两个文档共有的，但不一定是相同的词频）：
- en: '![Cosine distance](img/B05137_08_024.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![余弦距离](img/B05137_08_024.jpg)'
- en: Pairwise-adaptive similarity
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 成对自适应相似度
- en: 'This measure the distance in a reduced feature space by only considering the
    features that are most important in the two documents:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这种度量通过仅考虑两个文档中最重要的特征来在降维特征空间中测量距离：
- en: '![Pairwise-adaptive similarity](img/B05137_08_025.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![成对自适应相似度](img/B05137_08_025.jpg)'
- en: Here, **t**[i,k] is a vector formed from a subset of the features of **t**[i]
    (*i* = 1, 2) containing the union of the *K* largest features appearing in **t**[1]
    and **t**[2].
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**t**[i,k] 是由 **t**[i] (*i* = 1, 2) 的特征子集形成的向量，包含 **t**[1] 和 **t**[2] 中出现的
    *K* 个最大特征的并集。
- en: Extended Jaccard coefficient
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 扩展Jaccard系数
- en: 'This measure is computed as a ratio of the shared terms to the union of the
    terms between the documents:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这种度量是文档之间共享项与项的并集之比：
- en: '![Extended Jaccard coefficient](img/B05137_08_031.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![扩展Jaccard系数](img/B05137_08_031.jpg)'
- en: Dice coefficient
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Dice系数
- en: 'The Dice coefficient is given by the following:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Dice系数由以下公式给出：
- en: '![Dice coefficient](img/B05137_08_032.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![Dice系数](img/B05137_08_032.jpg)'
- en: Feature selection and dimensionality reduction
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征选择和降维
- en: The goal is the same as in [Chapter 2](ch02.html "Chapter 2. Practical Approach
    to Real-World Supervised Learning"), *Practical Approach to Real-World Supervised
    Learning* and [Chapter 3](ch03.html "Chapter 3. Unsupervised Machine Learning
    Techniques"), *Unsupervised Machine Learning Techniques*. The problem of the curse
    of dimensionality becomes even more pronounced with text mining and high dimensional
    features.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 目标与[第2章](ch02.html "第2章. 实际应用中的监督学习") *实际应用中的监督学习* 和[第3章](ch03.html "第3章. 无监督机器学习技术")
    *无监督机器学习技术* 中的目标相同。随着文本挖掘和高维特征的出现，维度诅咒问题变得更加明显。
- en: Feature selection
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征选择
- en: Most feature selection techniques are supervised techniques that depend on the
    labels or the outcomes for scoring the features. In the majority of cases, we
    perform filter-based rather than wrapper-based feature selection, due to the lower
    performance cost. Even among filter-based methods, some, such as those involving
    multivariate techniques such as **Correlation based Feature selection** (**CFS**),
    as described in [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World
    Supervised Learning"), *Practical Approach to Real-World Supervised Learning*,
    can be quite costly or result in suboptimal performance due to high dimensionality
    (*References* [9]).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数特征选择技术是监督技术，依赖于标签或结果来评分特征。在大多数情况下，我们执行基于过滤器的特征选择而不是基于包装器的特征选择，因为性能成本较低。即使在基于过滤器的方
    法中，一些方法，如[第2章](ch02.html "第2章. 实际应用中的监督学习方法")中描述的涉及多元技术的方法，如**基于相关性的特征选择**（CFS），也可能因为高维性而成本高昂或导致次优性能（*参考文献*
    [9]）。
- en: Information theoretic techniques
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 信息论技术
- en: As shown in [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World
    Supervised Learning"), *Practical Approach to Real-World Supervised Learning*,
    filter-based univariate feature selection methods, such as **Information gain**
    (**IG**) and **Gain Ratio** (**GR**), are most commonly used once preprocessing
    and feature extraction is done.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第2章](ch02.html "第2章. 实际应用中的监督学习方法")所示，*实际应用中的监督学习方法*，在预处理和特征提取完成后，基于过滤器的单变量特征选择方法，如**信息增益**（IG）和**增益比率**（GR），是最常用的。
- en: In their research, Yang and Pederson (*References* [10]) clearly showed the
    benefits of feature selection and reduction using IG to remove close to 98% of
    terms and yet improve the predictive capability of the classifiers.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的研究中，Yang和Pederson（*参考文献* [10]）清楚地展示了使用IG进行特征选择和降低，可以移除近98%的术语，同时提高分类器的预测能力。
- en: Many of the information theoretic or entropy-based methods have a stronger influence
    resulting from the marginal probabilities of the tokens. This can be an issue
    when the terms have equal conditional probability P(t|class), the rarer terms
    may have better scores than the common terms.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 许多基于信息论或熵的方法具有更强的边际概率影响。当术语具有相等的条件概率P(t|class)时，这可能会成为一个问题，即较不常见的术语可能比常见术语有更好的得分。
- en: Statistical-based techniques
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于统计的技术
- en: ?² feature selection is one of the most common statistical-based techniques
    employed to perform feature selection in text mining. ?² statistics, as shown
    in [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World Supervised
    Learning"), *Practical Approach to Real-World Supervised Learning*, give the independence
    relationship between the tokens in the text and the classes.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 卡方特征选择是文本挖掘中用于特征选择的最常见的基于统计的技术之一。卡方统计量，如[第2章](ch02.html "第2章. 实际应用中的监督学习方法")所示，*实际应用中的监督学习方法*，给出了文本中的标记与类别之间的独立性关系。
- en: It has been shown that ?² statistics for feature selection may not be effective
    when there are low-frequency terms (*References* [19]).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 已有研究表明，当存在低频项时，特征选择的卡方统计量可能并不有效（*参考文献* [19]）。
- en: Frequency-based techniques
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于频率的技术
- en: Using the term frequency or the document frequency described in the section
    on feature representation, a threshold can be manually set, and only terms above
    or below a certain threshold can be allowed used for modeling in either classification
    or clustering tasks. Note that **term frequency** (**TF**) and **document frequency**
    (**DF**) methods are biased towards common words while some of the information
    theoretic or statistical-based methods are biased towards less frequent words.
    The choice of selection of features depends on the domain, the particular application
    of predictive learning, and more importantly, on how models using these features
    are evaluated, especially on the unseen dataset.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特征表示部分中描述的词频或文档频率，可以手动设置一个阈值，并且只允许高于或低于一定阈值的术语用于分类或聚类任务中的建模。请注意，**词频**（TF）和**文档频率**（DF）方法倾向于常见词汇，而一些基于信息论或统计的方法则倾向于不常见词汇。特征选择的选择取决于领域、特定预测学习的应用，以及更重要的是，使用这些特征的模型如何评估，尤其是在未见数据集上的评估。
- en: Dimensionality reduction
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 维度降低
- en: Another approach that we saw in [Chapter 3](ch03.html "Chapter 3. Unsupervised
    Machine Learning Techniques"), *Unsupervised Machine Learning Techniques*, was
    to use unsupervised techniques to reduce the features using some form of transformation
    to decide their usefulness.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第三章](ch03.html "第三章。无监督机器学习技术")中，我们看到了另一种方法，即使用无监督技术通过某种形式的转换来减少特征，以决定其有用性。
- en: '**Principal component analysis** (**PCA**) computes a covariance or correlation
    matrix from the document-term matrix. It transforms the data into linear combinations
    of terms in the inputs in such a way that the transformed combination of features
    or terms has higher discriminating power than the input terms. PCA with cut-off
    or thresholding on the transformed features, as shown in [Chapter 3](ch03.html
    "Chapter 3. Unsupervised Machine Learning Techniques"), *Unsupervised Machine
    Learning Techniques*, can bring down the dimensionality substantially and even
    improve or give comparable performance to the high dimensional input space. The
    only issue with using PCA is that the transformed features are not interpretable,
    and for domains where understanding which terms or combinations yield better predictive
    models, this technique has some limitations.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）从文档-词矩阵计算协方差或相关矩阵。它将数据转换成输入中术语的线性组合，使得转换后的特征或术语组合具有比输入术语更高的区分能力。PCA在转换特征上使用截止值或阈值，如第三章中所示（ch03.html
    "第三章。无监督机器学习技术"），可以显著降低维度，甚至提高或与高维输入空间具有可比的性能。使用PCA的唯一问题是转换后的特征不可解释，对于理解哪些术语或组合产生更好的预测模型，这种技术在某些领域有限制。'
- en: '**Latent semantic analysis** (**LSA**) is another way of using the input matrix
    constructed from terms and documents and transforming it into lower dimensions
    with latent concepts discovered through combinations of terms used in documents
    (*References* [5]). The following figure captures the process using the **singular
    value decomposition** (**SVD**) method for factorizing the input document-term
    matrix:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在语义分析**（**LSA**）是另一种使用从词和文档构建的输入矩阵并将其转换为具有通过文档中使用的术语组合发现的潜在概念的低维度的方法（*参考文献*
    [5]）。以下图展示了使用**奇异值分解**（**SVD**）方法对输入文档-词矩阵进行因式分解的过程：'
- en: '![Dimensionality reduction](img/B05137_08_035.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![维度缩减](img/B05137_08_035.jpg)'
- en: 'Figure 12: SVD factorization of input document-terms into LSA document vectors
    and LSA term vectors'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：输入文档-词向量的SVD分解和LSA文档向量以及LSA词向量
- en: LSA has been shown to be a very effective way of reducing the dimensions and
    also of improving the predictive performance in models. The disadvantage of LSA
    is that storage of both vectors U and V is needed for performing retrievals or
    queries. Determining the lower dimension k is hard and needs some heuristics similar
    to k-means discussed in [Chapter 3](ch03.html "Chapter 3. Unsupervised Machine
    Learning Techniques"), *Unsupervised Machine Learning Techniques*.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: LSA已被证明是一种非常有效的降低维度并提高模型预测性能的方法。LSA的缺点是需要存储向量U和V以执行检索或查询。确定较低维度k是困难的，需要一些类似于第三章中讨论的k-means的启发式方法。
- en: Topics in text mining
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本挖掘中的主题
- en: As we saw in the first section, the area of text mining and performing Machine
    Learning on text spans a wide range of topics. Each topic discussed has some customizations
    to the mainstream algorithms, or there are specific algorithms that have been
    developed to perform the task called for in that area. We have chosen four broad
    topics, namely, text categorization, topic modeling, text clustering, and named
    entity recognition, and will discuss each in some detail.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第一部分所看到的，文本挖掘和文本上的机器学习涵盖了广泛的主题。每个讨论的主题都有对主流算法的一些定制，或者在该领域已经开发出特定的算法来执行所需的任务。我们选择了四个广泛的主题，即文本分类、主题建模、文本聚类和命名实体识别，并将对每个主题进行一些详细的讨论。
- en: Text categorization/classification
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本分类/分类
- en: The text classification problem manifests itself in different applications,
    such as document filtering and organization, information retrieval, opinion and
    sentiment mining, e-mail spam filtering, and so on. Similar to the classification
    problem discussed in [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World
    Supervised Learning"), *Practical Approach to Real-World Supervised Learning*,
    the general idea is to train on the training data with labels and to predict the
    labels of unseen documents.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类问题在不同的应用中表现出来，例如文档过滤和组织、信息检索、意见和情感挖掘、电子邮件垃圾邮件过滤等。与第2章中讨论的分类问题类似，即《面向现实世界监督学习的实用方法》，其基本思想是在带有标签的训练数据上训练，并预测未见文档的标签。
- en: As discussed in the previous section, the preprocessing steps help to transform
    the unstructured document collection into well-known numeric or categorical/binary
    structured data arranged in terms of a document-term matrix. The choice of performing
    some preprocessing steps, such as stemming or customizing stop words, depends
    on the data and applications. The feature choice is generally basic lexical features,
    n-grams of words as terms, and only in certain cases do we use the entire text
    as a string without breaking it into terms or tokens. It is common to use binary
    feature representation or frequency-based representation for document-term structured
    data. Once this transformation is complete, we do feature selection using univariate
    analysis, such as information gain or chi-square, to choose discriminating features
    above certain thresholds of scores. One may also perform feature transformation
    and dimensionality reduction such as PCA or LSA in many applications.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 如前文所述，预处理步骤有助于将非结构化的文档集合转换成以文档-词矩阵形式排列的已知数值或分类/二进制结构化数据。是否执行某些预处理步骤，例如词干提取或自定义停用词，取决于数据和应用程序。特征选择通常是基本的词汇特征，单词的n-gram作为术语，只有在某些情况下，我们才使用整个文本作为字符串，而不将其分解成术语或标记。对于文档-词结构化数据，通常使用二进制特征表示或基于频率的表示。一旦这种转换完成，我们就使用单变量分析，如信息增益或卡方检验，来选择得分高于一定阈值的判别特征。在许多应用中，也可以执行特征转换和降维，如PCA或LSA。
- en: There is a wide range in the choice of classifiers once we get structured data
    from the preceding process. In the research as well as in commercial applications,
    we see the use of most of the common modeling techniques, including linear (linear
    regression, logistic regression, and so on), non-linear (SVM, neural networks,
    KNN), generative (naïve bayes, bayesian networks), interpretable (decision trees,
    rules), and ensemble-based (bagging, boosting, random forest) classifiers. Many
    algorithms use similarity or distance metrics, of which cosine distance is the
    most popular choice. In certain classifiers, such as SVM, the string representation
    of the document can be used as is, with the right choice of string kernels and
    similarity-based metrics on strings to compute the dot products.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们从前面的过程中得到结构化数据，选择分类器的范围就非常广泛。在研究和商业应用中，我们看到大多数常见的建模技术都被使用，包括线性（线性回归、逻辑回归等）、非线性（SVM、神经网络、KNN）、生成性（朴素贝叶斯、贝叶斯网络）、可解释性（决策树、规则）和基于集成（bagging、boosting、随机森林）的分类器。许多算法使用相似度或距离度量，其中余弦距离是最受欢迎的选择。在某些分类器中，如SVM，文档的字符串表示可以直接使用，通过选择合适的字符串核和基于字符串的相似度度量来计算点积。
- en: Validation and evaluation methods are similar to supervised classification methodologies—splitting
    the data into train/validation/test, training on training data, tuning parameters
    of algorithm(s) on validation data, and estimating the performance of the models
    on hold-out or test data.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 验证和评估方法与监督分类方法类似——将数据分为训练/验证/测试集，在训练数据上训练，在验证数据上调整算法的参数，并在保留或测试数据上估计模型的性能。
- en: 'Since most of text classification involves a large number of documents, and
    the target classes are rare, the metrics used for evaluation, tuning, or choosing
    algorithms are in most cases precision, recall, and F-score measure, as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大多数文本分类涉及大量文档，且目标类别很少见，因此用于评估、调整或选择算法的指标通常是精确度、召回率和F分数，如下所示：
- en: '![Text categorization/classification](img/B05137_08_039.jpg)![Text categorization/classification](img/B05137_08_040.jpg)![Text
    categorization/classification](img/B05137_08_041.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![文本分类/分类](img/B05137_08_039.jpg)![文本分类/分类](img/B05137_08_040.jpg)![文本分类/分类](img/B05137_08_041.jpg)'
- en: Topic modeling
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主题建模
- en: 'A topic is a distribution over a fixed vocabulary. Topic modeling can be defined
    as an ability to capture different core ideas or themes in various documents.
    This has a wide range of applications, such as the summarization of documents,
    understanding reasons for sentiments, trends, the news, and many others. The following
    figure shows how topic modeling can discern a user-specified number *k* of topics
    from a corpus and then, for every document, assign proportions representing how
    much of each topic is found in the document:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 主题是一个固定词汇表上的分布。主题建模可以定义为捕捉各种文档中不同核心思想或主题的能力。这有广泛的应用范围，例如文档摘要、理解情感、趋势、新闻等。以下图示展示了主题建模如何从语料库中识别用户指定的主题数量*k*，然后为每个文档分配比例，表示文档中每个主题出现的程度：
- en: '![Topic modeling](img/B05137_08_043.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![主题建模](img/B05137_08_043.jpg)'
- en: 'Figure 13: Probabilistic topic weights assignment for documents'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：文档的概率主题权重分配
- en: There are quite a few techniques for performing topic modeling using supervised
    and unsupervised learning in the literature (*References* [13]). We will discuss
    the most common technique known as **probabilistic latent semantic index** (**PLSI**).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中关于使用监督和无监督学习进行主题建模的技术有很多（*参考文献* [13]）。我们将讨论最常见的技术，即**概率潜在语义索引**（**PLSI**）。
- en: Probabilistic latent semantic analysis (PLSA)
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概率潜在语义分析（PLSA）
- en: The idea of PLSA, as in the LSA for feature reduction, is to find latent concepts
    hidden in the corpus by discovering the association between co-occurring terms
    and treating the documents as mixtures of these concepts. This is an unsupervised
    technique, similar to dimensionality reduction, but the idea is to use it to model
    the mixture of topics or latent concepts in the document (*References* [12]).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: PLSA的想法，类似于用于特征降维的LSA，是通过发现共现术语之间的关联，并将文档视为这些概念的混合物来寻找语料库中隐藏的潜在概念。这是一种无监督技术，类似于降维，但其目的是用它来模拟文档中的主题或潜在概念的混合（*参考文献*
    [12]）。
- en: 'As shown in the following figure, the model may associate terms occurring together
    often in the corpus with a latent concept, and each document can then be said
    to exhibit that topic to a smaller or larger extent:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下图所示，模型可能会将语料库中经常一起出现的术语与一个潜在概念关联起来，然后每个文档可以被认为在较小或较大的程度上表现出该主题：
- en: '![Probabilistic latent semantic analysis (PLSA)](img/B05137_08_044.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![概率潜在语义分析（PLSA）](img/B05137_08_044.jpg)'
- en: 'Figure 14: Latent concept of Baseball capturing the association between documents
    and related terms'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：棒球潜在概念图，捕捉文档与相关术语之间的关联
- en: Input and output
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: 'The inputs are:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 输入如下：
- en: A collection of documents following a certain format and structure. We will
    give the notation:![Input and output](img/B05137_08_045.jpg)
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照一定格式和结构组织的文档集合。我们将给出以下符号：![输入和输出](img/B05137_08_045.jpg)
- en: The number of topics that need to be modeled or discovered as *k*.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要建模或发现的主题数量*k*。
- en: 'The output is:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '*k* topics identified T = {T[1], T[2],…T[k]}.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别*k*个主题，T = {T[1], T[2],…T[k]}。
- en: For each document, coverage of the topic given in the document *d*[i] can be
    written as = {*p*[i1], *p*[i2], …*p*[ik]}, where *p*[ij]is the probability of
    the document *d*i covering the topic T[j].
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个文档，给定文档*d*[i]的主题覆盖范围可以表示为 = {*p*[i1], *p*[i2], …*p*[ik]}，其中*p*[ij]是文档*d*i覆盖主题T[j]的概率。
- en: How does it work?
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: 'Implementations of PLSA generally follow the steps described here:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: PLSA的实现通常遵循以下步骤：
- en: Basic preprocessing steps as discussed previously, such as tokenization, stop
    words removal, stemming, dictionary of words formation, feature extraction (unigrams
    or n-grams, and so on), and feature selection (unsupervised techniques) are carried
    out, if necessary.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基本预处理步骤，如之前所述，包括分词、去除停用词、词干提取、构建词汇表、特征提取（单词或n-gram等）以及特征选择（无监督技术），如有必要，则执行这些步骤。
- en: The problem can be reduced to estimating the distribution of terms in a document,
    and, given the distribution, choosing the topic based on the maximum terms corresponding
    to the topic.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该问题可以简化为估计文档中术语的分布，并给定分布，根据与主题对应的最大术语选择主题。
- en: Introducing a "latent variable" *z* helps us to select whether the term belongs
    to a topic. Note that *z* is not "observed", but we assume that it is related
    to picking the term from the topic. Thus, the probability of the term *t* given
    the document *t* can be expressed in terms of this latent variable as:![How does
    it work?](img/B05137_08_053.jpg)
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 引入一个“潜在变量”*z*帮助我们选择一个术语是否属于一个主题。请注意，*z*不是“观察到的”，但我们假设它与从主题中选择术语有关。因此，给定文档*t*的术语*t*的概率可以用这个潜在变量表示：![如何工作？](img/B05137_08_053.jpg)
- en: By using two sets of variables (?, p) the equation can be written as:![How does
    it work?](img/B05137_08_055.jpg)
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用两组变量(?, p)，方程可以写成：![如何工作？](img/B05137_08_055.jpg)
- en: Here, p(t|z; ?) is the probability of latent concepts in terms and p(z|d; p)
    is the probability of latent concepts in document-specific mixtures.
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，p(t|z; ?)是术语中的潜在概念的概率，p(z|d; p)是文档特定混合中潜在概念的概率。
- en: Using log-likelihood to estimate the parameters to maximize:![How does it work?](img/B05137_08_058.jpg)
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用对数似然估计参数以最大化：![如何工作？](img/B05137_08_058.jpg)
- en: 'Since this equation involves nonconvex optimization, the EM algorithm is often
    used to find the parameters iteratively until convergence is reached or the total
    number of iterations are completed (*References* [6]):'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这个方程涉及非凸优化，EM算法通常用于迭代寻找参数，直到收敛或完成总迭代次数（*参考文献* [6]）：
- en: The E-step of the EM algorithm is used to determine the posterior probability
    of the latent concepts. The probability of term t occurring in the document d,
    can be explained by the latent concept z as:![How does it work?](img/B05137_08_062.jpg)
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: EM算法的E步骤用于确定潜在概念的后验概率。文档d中术语t出现的概率可以用潜在概念z来解释：![如何工作？](img/B05137_08_062.jpg)
- en: The M-step of the EM algorithm uses the values obtained from the E-step, that
    is, p(z|d, t) and does parameter estimation as:![How does it work?](img/B05137_08_064.jpg)
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: EM算法的M步骤使用从E步骤获得的价值，即p(z|d, t)，并进行参数估计：![如何工作？](img/B05137_08_064.jpg)
- en: '![How does it work?](img/B05137_08_065.jpg) = how often the term *t* is associated
    with concept *z*:![How does it work?](img/B05137_08_067.jpg)'
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_08_065.jpg) = 术语*t*与概念*z*关联的频率：![如何工作？](img/B05137_08_067.jpg)'
- en: '![How does it work?](img/B05137_08_068.jpg)= how often document *d* is associated
    with concept *z*.'
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![如何工作？](img/B05137_08_068.jpg)= 文档*d*与概念*z*关联的频率。'
- en: Advantages and limitations
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: 'The advantages and limitations are as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 优点和局限性如下：
- en: Though widely used, PLSA has some drawbacks that have been overcome by more
    recent techniques.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管广泛使用，PLSA也有一些缺点，这些缺点已被更近期的技术所克服。
- en: The unsupervised nature of the algorithm and its general applicability allows
    it to be used in a wide variety of similar text mining applications, such as clustering
    documents, associating topics related to authors/time, and so on.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法的无监督性质及其通用适用性使其能够应用于各种类似的文本挖掘应用，例如聚类文档、关联作者/时间相关的主题等。
- en: PLSA with EM algorithms, as discussed in previous chapters, face the problem
    of getting "stuck in local optima", unlike other global algorithms, such as evolutionary
    algorithms.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用EM算法的PLSA，如前几章所述，面临“陷入局部最优”的问题，这与其他全局算法，如进化算法不同。
- en: PLSA algorithms can only do topic identification in known documents, but cannot
    do any predictive modeling. PLSA has been generalized and is known as **latent
    dirichlet allocation** (**LDA**) to overcome this (*References* [14]).
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PLSA算法只能在已知文档中执行主题识别，但不能进行任何预测建模。PLSA已被推广，并被称为**潜在狄利克雷分配**（**LDA**）以克服这一点（*参考文献*
    [14]）。
- en: Text clustering
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本聚类
- en: The goal of clustering, as seen in [Chapter 3](ch03.html "Chapter 3. Unsupervised
    Machine Learning Techniques"), *Unsupervised Machine Learning Techniques*, is
    to find groups of data, text, or documents that are similar to one another within
    the group. The granularity of unstructured data can vary from small phrases or
    sentences, paragraphs, and passages of text to a collection of documents. Text
    clustering finds its application in many domains, such as information retrieval,
    summarization, topic modeling, and document classification in unsupervised situations,
    to name a few. Traditional techniques in clustering can be employed once the unstructured
    text data is transformed into structured data via preprocessing. The difficulty
    with traditional clustering techniques is the high-dimensional and sparse nature
    of the dataset obtained using the transformed document-term matrix representation.
    Many traditional clustering algorithms work only on numeric values of features.
    Because of this constraint, categorical or binary representation of terms cannot
    be used and often TF or TF-IDF are used for representation of the document-term
    matrix.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的目标，如[第3章](ch03.html "第3章。无监督机器学习技术")中所述的“无监督机器学习技术”，是找到组内彼此相似的数据、文本或文档。非结构化数据的粒度可以从短语或句子、段落和文本段落到文档集合不等。文本聚类在许多领域都有应用，例如信息检索、摘要、主题建模和无监督情况下的文档分类，仅举几例。一旦通过预处理将非结构化文本数据转换为结构化数据，就可以使用传统的聚类技术。传统聚类技术的困难在于使用转换后的文档-术语矩阵表示获得的具有高维和稀疏性的数据集。许多传统的聚类算法仅适用于特征的数值。由于这种限制，不能使用术语的分类或二进制表示，通常使用TF或TF-IDF来表示文档-术语矩阵。
- en: In this section, we will discuss some of the basic processes and techniques
    in clustering. We will start with pre-processing and transformations and then
    discuss some techniques that are widely used and the modifications made to them.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论聚类中的一些基本过程和技术。我们将从预处理和转换开始，然后讨论一些广泛使用的技术及其修改。
- en: Feature transformation, selection, and reduction
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征转换、选择和减少
- en: Most of the pre-processing steps discussed in this section are normally used
    to get either unigram or n-gram representation of terms in documents. Dimensionality
    reduction techniques, such as LSA, are often employed to transform the features
    into smaller latent space.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中讨论的大多数预处理步骤通常用于获取文档中术语的单词或n-gram表示。降维技术，如LSA，通常用于将特征转换为更小的潜在空间。
- en: Clustering techniques
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类技术
- en: The techniques for clustering in text include probabilistic models, as well
    as those that use distance-based methods, which are familiar to us from when we
    learned about structured data. We will also discuss **Non-negative Matrix Factorization**
    (**NNMF**) as an effective technique with good performance and interpretability.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 文本聚类的技术包括概率模型以及使用基于距离的方法，这些方法在我们学习结构化数据时是熟悉的。我们还将讨论**非负矩阵分解**（**NNMF**）作为一种具有良好性能和可解释性的有效技术。
- en: Generative probabilistic models
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成概率模型
- en: 'There is commonality between topic modeling and text clustering in generative
    methods. As shown in the following figure, clustering associates a document with
    a single cluster (generally), compared to topic modeling where each document can
    have a probability of coverage in multiple topics. Every word in topic modeling
    can be generated by multiple topics in an independent manner, whereas in clustering
    all the words are generated from the same cluster:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成方法中，主题建模和文本聚类之间存在共性。如图所示，聚类将一个文档与单个聚类（通常）关联起来，而主题建模中每个文档可以在多个主题中具有覆盖概率。主题建模中的每个词可以由多个主题独立生成，而在聚类中，所有词都来自同一个聚类：
- en: '![Generative probabilistic models](img/B05137_08_069.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![生成概率模型](img/B05137_08_069.jpg)'
- en: 'Figure 15: Exclusive mapping of documents to K-Clusters'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：文档到K-聚类的独占映射
- en: Mathematically, this can be explained using two topics T = {T[1], T[2]} and
    two clusters c = {c[1], c[2]}.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，这可以用两个主题T = {T[1]，T[2]}和两个聚类c = {c[1]，c[2]}来解释。
- en: 'In clustering, the likelihood of the document can be given as:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类中，文档的可能性可以表示为：
- en: '![Generative probabilistic models](img/B05137_08_072.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![生成概率模型](img/B05137_08_072.jpg)'
- en: 'If the document has, say, L terms, this can be further expanded as:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文档有L个术语，这可以进一步展开为：
- en: '![Generative probabilistic models](img/B05137_08_074.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![生成概率模型](img/B05137_08_074.jpg)'
- en: Thus, once you "assume" a cluster, all the words come from that cluster. The
    product of all terms is computed, followed by the summation across all clusters.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一旦“假设”一个簇，所有单词都来自该簇。计算所有术语的乘积，然后对所有簇进行求和。
- en: 'In topic modeling, the likelihood of the document can be given as:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在主题建模中，文档的似然可以表示为：
- en: '![Generative probabilistic models](img/B05137_08_075.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![生成概率模型](img/B05137_08_075.jpg)'
- en: Thus, each term ti can be picked independently from topics and hence summation
    is done inside and the product is done outside.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个术语ti可以独立地从主题中选择，因此求和是在内部进行的，而乘积是在外部进行的。
- en: Input and output
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入和输出
- en: 'The inputs are:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是：
- en: 'A collection of documents following a certain format and structure expressed
    with the following notation:'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以以下符号表示的遵循一定格式和结构的文档集合：
- en: D = {*d*1, *d*2, … *d*n}
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D = {*d*1, *d*2, … *d*n}
- en: The number of clusters that need to be modeled or discovered as *k*.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要建模或发现的簇数*k*。
- en: 'The output is:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '*k* clusters identified c = {c[1], c[2], … c[k]}.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k*个识别出的簇c = {c[1], c[2], … c[k]}。'
- en: For each document, *p(d*[i]*)* is mapped to one of the clusters *k*.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个文档，*p(d*[i]*)*被映射到簇*k*中的一个。
- en: How does it work?
  id: totrans-309
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 如何工作？
- en: 'Here are the steps:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是步骤：
- en: Basic preprocessing steps as discussed previously, such as tokenization, stop
    words removal, stemming, dictionary of words formation, feature extraction (unigrams
    or n-grams, and so on) of terms, feature transformations (LSA), and even feature
    selection. Let *t* be the terms in the final feature set; they correspond to the
    dictionary or vocabulary ![How does it work?](img/B05137_08_154.jpg).
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基本预处理步骤，如之前所述，包括分词、去除停用词、词干提取、形成词汇表、术语的特征提取（单词或n-gram等），以及特征转换（LSA）甚至特征选择。令*t*为最终特征集中的术语；它们对应于词汇表或词汇![如何工作？](img/B05137_08_154.jpg)。
- en: Similar to PLSA, we introduce a "latent variable", *z*, which helps us to select
    whether the document belonging to the cluster falls in the range of *z* ={1, 2,
    … *k*} corresponding to the clusters. Let the *?* parameter be the parameter we
    estimate for each latent variable such that *p(?*[i]*)* corresponds to the probability
    of cluster *z = i*.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与PLSA类似，我们引入一个“潜在变量”，*z*，帮助我们选择文档所属的簇是否落在*z*={1, 2, … *k*}的范围内，对应于簇。令*?*参数为我们为每个潜在变量估计的参数，使得*p(?*[i]*)*对应于簇*z
    = i*的概率。
- en: The probability of a document belonging to a cluster is given by *p(?*[i]*)*,
    and every term in the document generated from that cluster is given by *p(t|?*[i]*)*.
    The likelihood equation can be written as:![How does it work?](img/B05137_08_087.jpg)
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文档属于簇的概率由*p(?*[i]*)*给出，并且从该簇生成的文档中的每个术语由*p(t|?*[i]*)*给出。似然方程可以写成：![如何工作？](img/B05137_08_087.jpg)
- en: Note that instead of going through the documents, it is rewritten with terms
    *t* in the vocabulary ![How does it work?](img/B05137_08_154.jpg) raised to the
    number of times that term appears in the document.
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，我们不是通过文档进行，而是用词汇表![如何工作？](img/B05137_08_154.jpg)中术语*t*出现的次数作为指数重新编写。
- en: 'Perform the EM algorithm in a similar way to the method we used previously
    to estimate the parameters as follows:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以与我们之前使用的方法类似的方式执行EM算法来估计参数，如下所示：
- en: The E-step of the EM algorithm is used to infer the cluster from which the document
    was generated:![How does it work?](img/B05137_08_090.jpg)
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: EM算法的E步用于推断文档生成的簇：![如何工作？](img/B05137_08_090.jpg)
- en: 'The M-step of the EM algorithm is used to re-estimate the parameters using
    the result of the E-step, as shown here:'
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: EM算法的M步用于使用E步的结果重新估计参数，如下所示：
- en: '![How does it work?](img/B05137_08_091.jpg)![How does it work?](img/B05137_08_093.jpg)'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_08_091.jpg)![如何工作？](img/B05137_08_093.jpg)'
- en: 'The final probability estimate for each document can be done using either the
    maximum likelihood or by using a Bayesian algorithm with prior probabilities,
    as shown here: ![How does it work?](img/B05137_08_094.jpg) or ![How does it work?](img/B05137_08_095.jpg)'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个文档的最终概率估计可以使用最大似然估计或使用具有先验概率的贝叶斯算法，如下所示：![如何工作？](img/B05137_08_094.jpg)或![如何工作？](img/B05137_08_095.jpg)
- en: Advantages and limitations
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: Generative-based models have similar advantages to LSA and PLSA, where we get
    a probabilistic score for documents in clusters. By applying domain knowledge
    or priors using cluster size, the assignments can be further fine-tuned.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于生成的模型与LSA和PLSA有类似的优点，我们为簇中的文档得到一个概率分数。通过应用领域知识或先验知识，使用簇的大小，可以对分配进行进一步的微调。
- en: The disadvantages of the EM algorithm having to do with getting stuck in local
    optima and being sensitive to the starting point are still true here.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与EM算法相关的缺点，即陷入局部最优和敏感于起始点，在这里仍然成立。
- en: Distance-based text clustering
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于距离的文本聚类
- en: Most distance-based clustering algorithms rely on the similarity or the distance
    measure used to determine how far apart instances are from each other in feature
    space. Normally in datasets with numeric values, Euclidean distance or its variations
    work very well. In text mining, even after converting unstructured text to structured
    features of terms with numeric values, it has been found that the cosine and Jaccard
    similarity functions perform better.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数基于距离的聚类算法依赖于相似度或距离度量，用于确定实例在特征空间中的距离。通常在具有数值数据的数据集中，欧几里得距离或其变体工作得非常好。在文本挖掘中，即使在将非结构化文本转换为具有数值值的结构化特征项之后，也发现余弦和Jaccard相似度函数表现更佳。
- en: Often, Agglomerative or Hierarchical clustering, discussed in [Chapter 3](ch03.html
    "Chapter 3. Unsupervised Machine Learning Techniques"), *Unsupervised Machine
    Learning Techniques*, are used, which can merge documents based on similarity,
    as discussed previously. Merging the documents or groups is often done using single
    linkage, group average linkage, and complete linkage techniques. Agglomerative
    clustering also results in a structure that can be used for information retrieval
    and the searching of documents.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如第3章中讨论的聚合或层次聚类，*无监督机器学习技术*，用于根据相似性合并文档，如前所述。合并文档或组通常使用单链接、组平均链接和完全链接技术。聚合聚类还产生一个可用于信息检索和文档搜索的结构。
- en: The partition-based clustering techniques k-means and k-medoids accompanied
    by *h* a suitable similarity or distance method are also employed. The issue with
    k-means, as indicated in the discussion on clustering techniques, is the sensitivity
    to starting conditions along with computation space and time. k-medoids are sensitive
    to the sparse data structure and also have computation space and time constraints.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 基于划分的聚类技术k-means和k-medoids，伴随着一个合适的相似度或距离方法也被采用。k-means的问题，如聚类技术讨论中所示，是对起始条件的敏感性，以及计算空间和时间。k-medoids对稀疏数据结构敏感，并且也有计算空间和时间限制。
- en: Non-negative matrix factorization (NMF)
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非负矩阵分解（NMF）
- en: Non-negative matrix factorization is another technique used to factorize a large
    data-feature matrix into two non-negative matrices, which not only perform the
    dimensionality reduction, but are also easier to inspect. NMF has gained popularity
    for document clustering, and many variants of NMF with different optimization
    functions have now been shown to be very effective in clustering text (*References*
    [15]).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 非负矩阵分解是另一种将大型数据-特征矩阵分解为两个非负矩阵的技术，这不仅执行降维，而且更容易检查。NMF在文档聚类中获得了流行，现在已证明许多具有不同优化函数的NMF变体在聚类文本方面非常有效（*参考文献*
    [15]）。
- en: Input and output
  id: totrans-329
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入和输出
- en: 'The inputs are:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 输入为：
- en: 'A collection of documents following a certain format and structure given by
    the notation:'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照给定符号的格式和结构组织的一组文档：
- en: D = {d[1], d[2], … d[n]}
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D = {d[1], d[2], … d[n]}
- en: Number of clusters that need to be modeled or discovered as *k*.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要建模或发现的簇的数量为*k*。
- en: 'The output is:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为：
- en: k clusters identified c = {c[1], c[2], … c[k]} with documents assigned to the
    clusters.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别k个簇，c = {c[1]，c[2]，… c[k]}，文档分配到簇中。
- en: How does it work?
  id: totrans-336
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: 'The mathematical details and interpretation of NMF are given in the following:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: NMF的数学细节和解释如下：
- en: The basic idea behind NMF is to factorize the input matrix using low-rank approximation,
    as follows:![How does it work?](img/B05137_08_098.jpg)
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NMF背后的基本思想是使用低秩近似分解输入矩阵，如下所示：![如何工作？](img/B05137_08_098.jpg)
- en: A non-linear optimization function is used as:![How does it work?](img/B05137_08_099.jpg)
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用如下非线性优化函数：![如何工作？](img/B05137_08_099.jpg)
- en: This is convex in W or H, but not in both, resulting in no guarantees of a global
    minimum. Various algorithms that use constrained least squares, such as mean-square
    error and gradient descent, are used to solve the optimization function.
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 W 或 H 中是凸的，但不是两者都是，因此无法保证全局最小值。各种使用约束最小二乘法（如均方误差和梯度下降）的算法被用来解决优化函数。
- en: The interpretation of NMF, especially in understanding the latent topics based
    on terms, makes it very useful. The input A[m x n] of terms and documents, can
    be represented in low rank approximation as W[m x k] H[k x n] matrices, where
    W[m x k] is the term-topic representation whose columns are NMF basis vectors.
    The non zero elements of column 1 of W, given by W[1], correspond to particular
    terms. Thus, the w[ij] can be interpreted as a basis vector W[i] about the terms
    j. The H[i1] can be interpreted as how much the document given by doc 1 has affinity
    towards the direction of the topic vector W[i].
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NMF 的解释，特别是在基于术语理解潜在主题方面，使其非常有用。术语和文档的输入 A[m x n]，可以用低秩近似表示为 W[m x k] H[k x
    n] 矩阵，其中 W[m x k] 是术语-主题表示，其列是 NMF 基向量。W 的第 1 列的非零元素，由 W[1] 给出，对应于特定的术语。因此，w[ij]
    可以解释为关于术语 j 的基向量 W[i]。H[i1] 可以解释为文档 1 给定的文档对主题向量 W[i] 方向的亲和力。
- en: From the paper (*References* [18]) it was clearly shown how the basis vectors
    obtained for the medical abstracts, known as the Medlars dataset, creates highly
    interpretable basis vectors. The highest weighted terms in these basis vectors
    directly correspond to the concept, for example, W[1] corresponds to the topic
    related to "heart" and W[5] is related to "developmental disability".![How does
    it work?](img/B05137_08_112.jpg)
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从论文（*参考文献* [18]）中清楚地展示了为医学摘要（称为 Medlars 数据集）获得的基向量，这些基向量具有高度的解释性。这些基向量中最权重的术语直接对应于概念，例如，W[1]
    对应于与“心脏”相关的主题，而 W[5] 与“发育障碍”相关。![如何工作？](img/B05137_08_112.jpg)
- en: 'Figure 16: From Langville et al (2006) showing some basis vectors for medical
    datasets for interpretability.'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 16：来自 Langville 等人（2006）的展示，用于解释医疗数据集的一些基向量。
- en: Advantages and limitations
  id: totrans-344
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: 'NMF has been shown to be almost equal in performance with top algorithms, such
    as LSI, for information retrieval and queries:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: NMF 已被证明在信息检索和查询方面与顶级算法（如 LSI）几乎性能相当：
- en: Scalability, computation, and storage is better in NMF than in LSA or LSI using
    SVD.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与使用 SVD 的 LSA 或 LSI 相比，NMF 在可扩展性、计算和存储方面表现更好。
- en: NMF has a problem with optimization not being global and getting stuck in local
    minima.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NMF 存在一个问题，即优化不是全局的，并且会陷入局部最小值。
- en: NMF generation of factors depends on the algorithms for optimization and the
    parameters chosen, and is not unique.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NMF 因素的产生取决于优化算法和选择的参数，并且不是唯一的。
- en: Evaluation of text clustering
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本聚类的评估
- en: In the case of labeled datasets, all the external measures discussed in [Chapter
    3](ch03.html "Chapter 3. Unsupervised Machine Learning Techniques"), *Unsupervised
    Machine Learning Techniques,* such as F-measure and Rand Index are useful in evaluating
    the clustering techniques. When the dataset doesn't have labels, some of the techniques
    described as the internal measures, such as the Davies–Bouldin Index, R-Squared,
    and Silhouette Index, can be used.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记数据集的情况下，[第 3 章](ch03.html "第 3 章。无监督机器学习技术") 中讨论的所有外部度量，如 F 度量和 Rand 指数，对于评估聚类技术是有用的。当数据集没有标签时，可以采用一些描述为内部度量的技术，如
    Davies–Bouldin 指数、R-Squared 和 Silhouette 指数。
- en: The general good practice is to adapt and make sure similarity between the documents,
    as discussed in this section, is used for measuring closeness, remoteness, and
    spread of the cluster when applied to text mining data. Similarly usage depends
    on the algorithm and some relevance to the problem too. In distance-based partition
    algorithms, the similarity of the document can be computed with the mean vector
    or the centroid. In hierarchical algorithms, similarity can be computed with most
    similar or dissimilar documents in the group.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 一般的良好实践是适应并确保在应用于文本挖掘数据时，如本节所述，文档之间的相似性用于测量簇的接近度、距离和分布。类似的使用取决于算法，并且与问题也有一定的相关性。在基于距离的划分算法中，可以使用均值向量或质心来计算文档的相似性。在层次算法中，可以使用组中最相似或最不相似的文档来计算相似性。
- en: Named entity recognition
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: '**Named entity recognition** (**NER**) is one of the most important topics
    in information retrieval for text mining. Many complex mining tasks, such as the
    identification of relations, annotations of events, and correlation between entities,
    use NER as the initial component or basic preprocessing step.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '**命名实体识别**（**NER**）是文本挖掘中信息检索最重要的主题之一。许多复杂的挖掘任务，如关系的识别、事件的标注以及实体之间的相关性，都将NER作为初始组件或基本预处理步骤。'
- en: Historically, manual rules-based and regular expression-based techniques were
    used for entity recognition. These manual rules relied on basic pre processing,
    using POS tags as features, along with hand-engineered features, such as the presence
    of capital words, usage of punctuations prior to words, and so on.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，手动基于规则和正则表达式的技术被用于实体识别。这些手动规则依赖于基本的预处理，使用词性标注作为特征，以及手工设计的特征，如大写单词的存在、单词前的标点符号使用等。
- en: Statistical learning-based techniques are now used more for NER and its variants.
    NER can be mapped to sequence labeling and prediction problems in Machine Learning.
    BIO notation, where each entity type T has two labels B-T and I-T corresponding
    to beginning and intermediate, respectively, is labeled, and learning involves
    finding the pattern and predicting it in unseen data. The O represents an outside
    or unrelated entity in the sequence of text. The entity type T is further classified
    into Person, Organization, Data, and location in the most basic form.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 基于统计学习的技巧现在更多地用于NER及其变体。NER可以被映射到机器学习中的序列标注和预测问题。BIO表示法，其中每个实体类型T有两个标签B-T和I-T，分别对应于开始和中间，被标注，学习涉及在未见数据中找到模式和预测它。O代表文本序列中的外部或不相关实体。实体类型T在基本形式上进一步被分类为人物、组织、数据和地点。
- en: 'In this section, we will discuss the two most common algorithms used: generative-based
    hidden Markov models and discriminative-based maximum entropy models.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论两种最常用的算法：基于生成模型的隐马尔可夫模型和基于判别模型的最大熵模型。
- en: Though we are discussing these algorithms in the context of Named Entity Recognition,
    the same algorithms and processes can be used for other NLP tasks such as POS
    Tagging, where tags are associated with a sequence rather than associating the
    NER classes.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们是在讨论命名实体识别的上下文中这些算法，但相同的算法和过程也可以用于其他NLP任务，如词性标注（POS Tagging），其中标签与一个序列相关联，而不是与NER类别相关联。
- en: Hidden Markov models for NER
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 命名实体识别的隐马尔可夫模型
- en: Hidden Markov models, as explained in [Chapter 6](ch06.html "Chapter 6. Probabilistic
    Graph Modeling"), *Probabilistic Graph Modeling*, are the sequence-based generative
    models that assume an underlying distribution that generates the sequences. The
    training data obtained by labeling sequences with the right NER classes can be
    used to learn the distribution and parameters, so that for unseen future sequences,
    effective predictions can be performed.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型，如[第6章](ch06.html "第6章。概率图建模")《概率图建模》中所述，是基于序列的生成模型，它假设存在一个生成序列的潜在分布。通过用正确的NER类别标注序列获得训练数据可以用来学习分布和参数，以便对于未见未来的序列，可以执行有效的预测。
- en: Input and output
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: The training data consists of text sequences x ={x[1], x[2], ... x[n]} where
    each xi is a word in the text sequence and labels for each word are available
    as y ={y[1], y[2], ... y[n]}. The algorithm generates a model so that on testing
    on unseen data, the labels for new sequences can be generated.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据由文本序列x = {x[1], x[2], ... x[n]}组成，其中每个xi是文本序列中的一个词，每个词的标签作为y = {y[1], y[2],
    ... y[n]}提供。算法生成一个模型，以便在测试未见数据时，可以生成新序列的标签。
- en: How does it work?
  id: totrans-362
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: In the simplest form, a Markov assumption is made, which is that the hidden
    states and labels of the sequences are only dependent on the previous state. An
    adaptation to the sequence of words with labels is shown in the following figure:![How
    does it work?](img/B05137_08_116.jpg)
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最简单的情况下，假设马尔可夫假设，即序列的隐藏状态和标签仅依赖于前一个状态。对带标签的单词序列的适应在以下图中展示：![如何工作？](img/B05137_08_116.jpg)
- en: 'Figure 17: Text sequence and labels corresponding to NER in Hidden Markov Chain'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图17：隐马尔可夫链中NER对应的文本序列和标签
- en: The HMM formulation of the sequence classification helps in estimating the joint
    probability maximized on training data:![How does it work?](img/B05137_08_117.jpg)
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 序列分类的HMM公式有助于在训练数据上估计最大化的联合概率：![如何工作？](img/B05137_08_117.jpg)
- en: Each *y*[i] is assumed to be generated based on *y*[i–1] and *x*i. The first
    word in the entity is generated conditioned on current and previous labels, that
    is, *y*[i] and *y*[i–1]. If the instance is already a Named entity, then conditioning
    is only on previous instances, that is, *x*[i–1]. Outside words such as "visited"
    and "in" are considered "not a name class".
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个y[i]被假定为基于y[i–1]和x[i]生成的。实体的第一个单词是在当前和前一个标签的条件下生成的，即y[i]和y[i–1]。如果实例已经是命名实体，则条件仅基于前一个实例，即x[i–1]。像"visited"和"in"这样的外部单词被认为是"非名称类别"。
- en: The HMM formulation with the forward-backward algorithm can be used to determine
    the likelihood of a sequence of observations with parameters learned from the
    training data.
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前向-后向算法的HMM公式可以用来确定从训练数据中学习到的参数的观察序列的概率。
- en: Advantages and limitations
  id: totrans-368
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: 'The advantages and limitations are as follows:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 优点和局限性如下：
- en: HMMs are good for short sequences, as shown, with one word or term and independence
    assumption. For sequences with entities that have a longer span, the results will
    violate these assumptions.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HMM对于短序列很好，如图所示，只有一个单词或术语，并且假设独立性。对于具有较长实体范围的序列，结果将违反这些假设。
- en: The HMM needs a large set of training data to estimate the parameters.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HMM需要大量的训练数据来估计参数。
- en: Maximum entropy Markov models for NER
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 命名实体识别的最大熵马尔可夫模型
- en: '**Maximum entropy Markov model** (**MEMM**) is a popular NER technique that
    uses the concept of Markov chains and maximum entropy models to learn and predict
    the named entities (*References* [16] and [17]).'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大熵马尔可夫模型**（**MEMM**）是一种流行的命名实体识别（NER）技术，它利用马尔可夫链和最大熵模型的概念来学习和预测命名实体（参考[16]和[17]）。'
- en: Input and output
  id: totrans-374
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: The training data consists of text sequences x ={x1, x2, ... xn} where each
    xi is a word in the text sequence and labels for each word are available as y
    ={y1, y2, ... yn}. The algorithm generates models so that, on testing on unseen
    data, the labels for new sequences can be generated.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据由文本序列x={x1，x2，... xn}组成，其中每个xi是文本序列中的一个单词，每个单词的标签作为y={y1，y2，... yn}可用。算法生成模型，以便在测试未见数据时，可以生成新序列的标签。
- en: How does it work?
  id: totrans-376
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: The following illustrates how the MEMM method is used for learning named entities.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 以下说明了MEMM方法是如何用于学习命名实体的。
- en: The features in MEMM can be word features or other types of features, such as
    "isWordCapitalized", and so on, which gives it a bit more context and improves
    performance compared to HMM, where it is only word-based.
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MEMM中的特征可以是单词特征或其他类型的特征，如"isWordCapitalized"等，这给它提供了一点更多的上下文，并且与仅基于单词的HMM相比，提高了性能。
- en: Next, let us look at a maximum entropy model known as a MaxEnt model, which
    is an exponential probabilistic model, but which can also be seen as a multinomial
    logistic regression model. In basic MaxEnt models, given the features {f[1], f[2]
    … f[N]} and classes c[1], c[2] … c[C], weights for these features are learned
    {w[c1], w[c2] … w[cN]} per class using optimization methods from the training
    data, and the probability of a particular class can be estimated as:![How does
    it work?](img/B05137_08_126.jpg)
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们看看一种称为最大熵模型（MaxEnt模型）的最大熵模型，它是一个指数概率模型，但也可以被视为多项式逻辑回归模型。在基本的MaxEnt模型中，给定特征{f[1]，f[2]…f[N]}和类别c[1]，c[2]…c[C]，使用从训练数据中得到的优化方法为每个类别学习这些特征的权重{w[c1]，w[c2]…w[cN]}，可以估计特定类别的概率如下：![如何工作？](img/B05137_08_126.jpg)
- en: The feature fi is formally written as f[i](c, x), which means the feature f[i]
    for class c and observation x. The f[i](c, x) is generally binary with values
    1/0 in most NER models. Thus, it can be written as:![How does it work?](img/B05137_08_131.jpg)
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征fi正式表示为f[i](c, x)，这意味着对于类别c和观察x的特征f[i]。在大多数NER模型中，f[i](c, x)通常是二元的，值为1/0。因此，它可以写成：![如何工作？](img/B05137_08_131.jpg)
- en: 'Maximum likelihood based on the probability of prediction across class can
    be used to select a single class:'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于预测概率跨类最大似然可以用来选择单个类别：
- en: '![How does it work?](img/B05137_08_132.jpg)'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_08_132.jpg)'
- en: For every word, we use the current word, the features from "nearby" words, and
    the predictions on the nearby words to create a joint probability model. This
    is also called local learning as the chunks of test and distribution are learned
    around local features corresponding to the word.
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个单词，我们使用当前单词、"附近"单词的特征以及附近单词的预测来创建一个联合概率模型。这也被称为局部学习，因为测试和分布的块是在与单词对应的局部特征周围学习的。
- en: 'Mathematically, we see how a discriminative model is created from current word
    and last prediction as:'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从当前单词和最后预测中，我们可以从数学上看到如何创建一个判别模型：
- en: '![How does it work?](img/B05137_08_133.jpg)![How does it work?](img/B05137_08_134.jpg)'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![它是如何工作的？](img/B05137_08_133.jpg)![它是如何工作的？](img/B05137_08_134.jpg)'
- en: 'Generalizing for the k features:'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于k个特征进行泛化：
- en: '![How does it work?](img/B05137_08_135.jpg)'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![它是如何工作的？](img/B05137_08_135.jpg)'
- en: Thus, in MEMM we compute the probability of the state, which is the class in
    NER, and even though we condition on prediction of nearby words given by y[i–1],
    in general we can use more features, and that is the advantage over the HMM model
    discussed previously:![How does it work?](img/B05137_08_137.jpg)![How does it
    work?](img/B05137_08_138.jpg)
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，在MEMM中，我们计算状态的概率，即命名实体识别（NER）中的类别，尽管我们基于y[i–1]给出的邻近单词的预测进行条件化，但通常我们可以使用更多特征，这就是相对于之前讨论的HMM模型的优势：![它是如何工作的？](img/B05137_08_137.jpg)![它是如何工作的？](img/B05137_08_138.jpg)
- en: 'Figure 18 : Text Sequences and observation probabilities with labels'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图18：带标签的文本序列和观察概率
- en: The Viterbi algorithm is used to perform the estimation of class for the word
    or decoding/inferencing in HMM, that is, to get estimates for p(y[i]|y[i–1], X[i])
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 维特比算法用于在HMM中对单词或解码/推理进行类别估计，即得到p(y[i]|y[i–1], X[i])的估计
- en: Finally, the MaxEnt model is used to estimate the weights as before using the
    optimization methods for state changes in general:![How does it work?](img/B05137_08_140.jpg)
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用状态变化的优化方法，MaxEnt模型被用来估计权重，就像之前一样：![它是如何工作的？](img/B05137_08_140.jpg)
- en: Advantages and limitations
  id: totrans-392
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: MEMM has more flexibility in using features that are not just word-based or
    even human-engineered, giving it more richness and enabling its models to be more
    predictive.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MEMM在使用特征方面具有更大的灵活性，这些特征不仅基于单词，甚至不是由人类设计的，这使得它更加丰富，并使其模型具有更强的预测能力。
- en: MEMM can have a range of more than just close words, giving it an advantage
    of detection over larger spans compared to HMM.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MEMM可以检测比HMM更大的跨度，具有比HMM更大的检测优势。
- en: Deep learning and NLP
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习和NLP
- en: In the last few years, Deep Learning and its application to various areas of
    NLP has shown huge success and is considered the cutting edge of technology these
    days. The main advantage of using Deep Learning lies in a small subset of tools
    and methods, which are useful in a wide variety of NLP problems. It solves the
    basic issue of feature engineering and carefully created manual representations
    by automatically learning them, and thus solves the issue of having a large number
    of language experts dealing with a wide range of problems, such as text classification,
    sentiment analysis, POS tagging, and machine translation, to name a few. In this
    section, we will try to cover important concepts and research in the area of Deep
    Learning and NLP.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，深度学习和其在自然语言处理（NLP）各个领域的应用取得了巨大成功，被认为是当今技术的尖端。使用深度学习的主要优势在于一小部分工具和方法，这些工具和方法在广泛的NLP问题中都很有用。它通过自动学习来解决特征工程和精心创建的手动表示的基本问题，从而解决了需要大量语言专家处理广泛问题的问题，例如文本分类、情感分析、词性标注和机器翻译等。在本节中，我们将尝试涵盖深度学习和NLP领域的重要概念和研究。
- en: In his seminal paper, Bengio introduced one of the most important building blocks
    for deep learning known as word embedding or word vector (*References* [20]).
    Word embedding can be defined as a parameterized function that maps words to a
    high dimensional vector (usually 25 to 500 dimensions based on the application).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的开创性论文中，Bengio介绍了深度学习最重要的构建块之一，称为词嵌入或词向量（*参考文献* [20]）。词嵌入可以被定义为将单词映射到高维向量（通常基于应用，25到500维）的参数化函数。
- en: Formally, this can be written as ![Deep learning and NLP](img/B05137_08_141.jpg).
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，这可以写成![深度学习和NLP](img/B05137_08_141.jpg)。
- en: For example, ![Deep learning and NLP](img/B05137_08_142.jpg) and ![Deep learning
    and NLP](img/B05137_08_143.jpg), and so on.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，![深度学习和NLP](img/B05137_08_142.jpg)和![深度学习和NLP](img/B05137_08_143.jpg)，等等。
- en: 'A neural network (*R*) whose inputs are the words from sentences or n-grams
    of sentences with binary classification, such as whether the sequence of words
    in n-grams are valid or not, is used to train and learn the *W* and *R*:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经网络（*R*），其输入是句子中的单词或句子的n-gram，用于二分类，例如n-gram中单词序列是否有效，用于训练和学习*W*和*R*：
- en: '![Deep learning and NLP](img/B05137_08_146.jpg)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习和NLP](img/B05137_08_146.jpg)'
- en: 'Figure 19: A modular neural network learning 5-gram words for valid–invalid
    classification'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：一个模块化神经网络学习5-gram单词进行有效-无效分类
- en: 'For example:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '*R(W(cat),W(sat ),W(on),W(the),W(mat)) = 1(valid)*'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R(W(cat),W(sat ),W(on),W(the),W(mat)) = 1(有效)*'
- en: '*R(W(cat),W(sat),W(on),W(the),W(mat)) = 0 (Invalid)*'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R(W(cat),W(sat),W(on),W(the),W(mat)) = 0 (无效)*'
- en: 'The idea of training these sentences or n-grams is not only to learn the correct
    structure of the phrases, but also the right parameters for *W* and *R*. The word
    embeddings can also be projected on to a lower dimensional space, such as a 2D
    space, using various linear and non linear feature reduction/visualization techniques
    introduced in [Chapter 3](ch03.html "Chapter 3. Unsupervised Machine Learning
    Techniques"), *Unsupervised Machine Learning Techniques*, which humans can easily
    visualize. This visualization of word embeddings in two dimensions using techniques
    such as t-SNE discovers important information about the closeness of words based
    on semantic meaning and even clustering of words in the area, as shown in the
    following figure:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这些句子或n-gram的想法不仅是为了学习短语的正确结构，也是为了学习*W*和*R*的正确参数。单词嵌入也可以使用第3章中介绍的线性和非线性特征降维/可视化技术投影到低维空间，例如二维空间，这些技术人类可以轻松可视化。使用t-SNE等技术将单词嵌入在二维空间中的可视化揭示了基于语义意义单词的接近性以及该区域单词的聚类信息，如下所示图所示：
- en: '![Deep learning and NLP](img/B05137_08_149.jpg)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习和NLP](img/B05137_08_149.jpg)'
- en: 'Figure 20: t-SNE representation of a small section of the entire word mappings.
    Numbers in Roman numerals and words are shown clustered together on the left,
    while semantically close words are clustered together on the right.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：整个单词映射的小部分t-SNE表示。罗马数字和单词在左侧聚集在一起显示，而语义相近的单词在右侧聚集在一起。
- en: Further extending the concepts, both Collobert and Mikolov showed that the side
    effects of learning the word embeddings can be very useful in a variety of NLP
    tasks, such as similar phrase learning (for example, *W("the color is red"))*
    *?* *W("the color is yellow"))*, finding synonyms (for example, *W("nailed"))*
    *?* *W("smashed"))*, analogy mapping (for example, *W("man")?W("woman") then W("king")?W("queen"))*,
    and even complex relationship mapping (for example, *W("Paris")?W("France") then
    W("Tokyo")?W("Japan"))* (*References* [21 and 22]).
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步扩展这些概念，Collobert和Mikolov都展示了学习单词嵌入的副作用在多种NLP任务中非常有用，例如相似短语学习（例如，*W("the color
    is red")* *?* *W("the color is yellow"))*，寻找同义词（例如，*W("nailed")* *?* *W("smashed"))*，类比映射（例如，*W("man")?W("woman")然后W("king")?W("queen"))*，甚至复杂的关联映射（例如，*W("Paris")?W("France")然后W("Tokyo")?W("Japan"))*
    (*参考文献* [21和22])。
- en: The extension of word embedding concepts towards a generalized representation
    that helps us reuse the representation with various NLP problems (with minor extensions)
    has been the main reason for many recent successes of Deep Learning in NLP. Socher,
    in his research, extended the word embeddings concept to produce bilingual word
    embeddings, that is, embed words from two different languages, such as Chinese
    (Mandarin) and English into a shared space (*References* [23]). By learning two
    language word embeddings independently of each other and then projecting them
    in a same space, his work gives us interesting insights into word similarities
    across languages that can be extended for Machine Translation. Socher also did
    interesting work on projecting the images learned from CNNs with the word embedding
    in to the same space for associating words with images as a basic classification
    problem (*References* [24]). Google, around the same time, has also been working
    on similar concepts, but at a larger scale for word-image matching and learning
    (*References* [26]).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 将单词嵌入概念扩展到一种通用表示，帮助我们以（稍作扩展）的方式重用该表示来解决各种NLP问题，这是许多最近深度学习在NLP中取得成功的主要原因。Socher在他的研究中将单词嵌入概念扩展到产生双语单词嵌入，即把来自两种不同语言的单词，如中文（普通话）和英语嵌入到一个共享空间中
    (*参考文献* [23])。通过独立学习两种语言的单词嵌入，然后将它们投影到同一个空间，他的工作为我们提供了关于跨语言单词相似性的有趣见解，这些见解可以扩展到机器翻译。Socher还就使用CNN学习到的图像与单词嵌入投影到同一个空间进行了有趣的工作，以将单词与图像关联起来作为一个基本的分类问题
    (*参考文献* [24])。大约在同一时间，谷歌也在研究类似的概念，但规模更大，用于单词-图像匹配和学习 (*参考文献* [26])。
- en: 'Extending the word embedding concept to have combiners or association modules
    that can help combine the words, words-phrases, phrases-phrases in all combinations
    to learn complex sentences has been the idea of Recursive Neural Networks. The
    following figure shows how complex association *((the cat)(sat(on (the mat))))*
    can be learned using Recursive Neural Networks. It also removes the constraint
    of a "fixed" number of inputs in neural networks because of the ability to recursively
    combine:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 将词嵌入概念扩展到具有组合器或关联模块，这些模块可以帮助组合所有组合的词语、词语-短语、短语-短语，以学习复杂句子，这是递归神经网络（Recursive
    Neural Networks）的理念。以下图示展示了如何使用递归神经网络学习复杂的关联 *((the cat)(sat(on (the mat))))*。它还由于递归组合的能力而消除了神经网络中“固定”输入数量的限制：
- en: '![Deep learning and NLP](img/B05137_08_155.jpg)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习和NLP](img/B05137_08_155.jpg)'
- en: 'Figure 21: Recursive Neural Network showing how complex phrases can be learned.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 图21：递归神经网络展示了如何学习复杂短语。
- en: 'Recursive Neural Networks have been showing great promise in NLP tasks, such
    as sentiment analysis, where association of one negative word at the start of
    many positive words has an overall negative impact on sentences, as shown in the
    following figure:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络在NLP任务中显示出巨大的潜力，例如情感分析，其中许多正面词语开头的单个负面词语会对句子产生整体负面的影响，如下图所示：
- en: '![Deep learning and NLP](img/B05137_08_156.jpg)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习和NLP](img/B05137_08_156.jpg)'
- en: 'Figure 22: A complex sentence showing words with negative (as red circle),
    positive (green circle), and neutral (empty with 0) connected through RNN with
    overall negative sentiment.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 图22：一个复杂的句子，展示了通过RNN连接的带有负面（红色圆圈）、正面（绿色圆圈）和中立（空，0）的词语，整体呈现负面情感。
- en: The concept of recursive neural networks is now extended using the building
    blocks of encoders and decoders to learn reversible sentence representation—that
    is, reconstructing the original sentence with roughly the same meaning from the
    input sentence (*References* [27]). This has become the central core theme behind
    Neural Machine Translation. Modeling conversations using the encoder-decoder framework
    of RNNs has also made huge breakthroughs (*References* [28]).
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 现在通过编码器和解码器的构建块扩展递归神经网络的概念，以学习可逆的句子表示——即从输入句子中重建具有大致相同意义的原始句子（*参考文献* [27]）。这已成为神经机器翻译背后的核心主题。使用RNN的编码器-解码器框架建模对话也取得了巨大的突破（*参考文献*
    [28]）。
- en: Tools and usage
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具和用法
- en: We will now discuss some of the most well-known tools and libraries in Java
    that are used in various NLP and text mining applications.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将讨论一些在Java中用于各种NLP和文本挖掘应用的最知名的工具和库。
- en: Mallet
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Mallet
- en: Mallet is a Machine Learning toolkit for text written in Java, which comes with
    several natural language processing libraries, including those some for document
    classification, sequence tagging, and topic modeling, as well as various Machine
    Learning algorithms. It is open source, released under CPL. Mallet exposes an
    extensive API (see the following screenshots) to create and configure sequences
    of "pipes" for pre-processing, vectorizing, feature selection, and so on, as well
    as to extend implementations of classification and clustering algorithms, plus
    a host of other text analytics and Machine Learning capabilities.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: Mallet是一个用Java编写的机器学习工具包，用于文本处理，附带几个自然语言处理库，包括一些用于文档分类、序列标记和主题建模的库，以及各种机器学习算法。它是开源的，在CPL下发布。Mallet提供了一个广泛的API（见以下截图），用于创建和配置“管道”序列以进行预处理、向量化、特征选择等，以及扩展分类和聚类算法的实现，以及一系列其他文本分析和机器学习功能。
- en: '![Mallet](img/B05137_08_157.jpg)![Mallet](img/B05137_08_158.jpg)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![Mallet](img/B05137_08_157.jpg)![Mallet](img/B05137_08_158.jpg)'
- en: KNIME
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KNIME
- en: KNIME is an open platform for analytics with Open GL licensing with a number
    of powerful tools for conducting all aspects of data science. The Text Processing
    module is available for separate download from KNIME Labs. KNIME has an intuitive
    drag and drop UI with downloadable examples available from their workflow server.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: KNIME是一个具有OpenGL许可的开源分析平台，提供了一系列强大的工具，用于执行数据科学的各个方面。文本处理模块可以从KNIME Labs单独下载。KNIME具有直观的拖放UI，并提供从其工作流服务器下载的示例。
- en: Note
  id: totrans-425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'KNIME: [https://www.knime.org/](https://www.knime.org/)'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 'KNIME: [https://www.knime.org/](https://www.knime.org/)'
- en: 'KNIME Labs: [https://tech.knime.org/knime-text-processing](https://tech.knime.org/knime-text-processing)'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 'KNIME Labs: [https://tech.knime.org/knime-text-processing](https://tech.knime.org/knime-text-processing)'
- en: The platform includes a Node repository that contains all the necessary tools
    to compose your workflow with a convenient nesting of nodes that can easily be
    reused by copying and pasting. The execution of the workflows is simple. Debugging
    errors can take some getting used to, so our recommendation is to take the text
    mining example, use a different dataset as input, and make the workflow execute
    without errors. This is the quickest way to get familiar with the platform.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 该平台包括一个节点仓库，其中包含所有必要的工具，可以方便地通过节点的嵌套组合您的流程，这些节点可以很容易地通过复制粘贴来重复使用。工作流程的执行很简单。调试错误可能需要一些习惯，因此我们的建议是使用文本挖掘示例，使用不同的数据集作为输入，并确保工作流程无错误执行。这是熟悉平台的最快方式。
- en: Topic modeling with mallet
  id: totrans-429
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Mallet进行主题建模
- en: 'We will now illustrate the usage of API and Java code to implement Topic Modeling
    to give the user an illustration on how to build a text learning pipeline for
    a problem in Java:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将展示如何使用API和Java代码实现主题建模，以向用户展示如何在Java中构建一个文本学习流程：
- en: '[PRE1]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'ParallelTopicModel in Mallet has an API with parameters such as the number
    of topics, alpha, and beta that control the underlying parameter for tuning the
    LDA using Dirichlet distribution. Parallelization is very well supported, as seen
    by the increased number of threads available in the system:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: Mallet中的ParallelTopicModel具有一个API，包含如主题数量、alpha和beta等参数，这些参数控制着使用Dirichlet分布调整LDA的底层参数。并行化得到了很好的支持，正如系统中可用的线程数量增加所显示的那样：
- en: '[PRE2]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Topic and term association is shown in the following screenshot as the result
    of running the ParallelTopicModel in Mallet. Clearly, the top terms and association
    of the topics are very well discovered in many cases, such as the classes of exec,
    acq, wheat, crude, corn, and earning:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 主题和术语关联如下截图所示，是运行Mallet中的ParallelTopicModel的结果。很明显，在许多情况下，如exec、acq、wheat、crude、corn和earning等类别，顶级术语和主题的关联都得到了很好的发现：
- en: '![Topic modeling with mallet](img/B05137_08_159.jpg)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
  zh: '![使用Mallet进行主题建模](img/B05137_08_159.jpg)'
- en: Business problem
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 商业问题
- en: The Reuters corpus labels each document with one of 10 categories. The aim of
    the experiments in this case study is to employ the techniques of text processing
    learned in this chapter to give structure to these documents using vector space
    modeling. This is done in three different ways, and four classification algorithms
    are used to train and make predictions using the transformed dataset in each of
    the three cases. The open source Java analytics platform KNIME was used for text
    processing and learning.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: Reuters语料库将每个文档标记为10个类别之一。本案例研究的实验目的是利用本章学到的文本处理技术，通过向量空间模型为这些文档提供结构。这是以三种不同的方式完成的，并且使用了四种分类算法来训练和预测在每个案例中转换后的数据集。开源Java分析平台KNIME用于文本处理和学习。
- en: Machine Learning mapping
  id: totrans-438
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习映射
- en: Among the learning techniques for unstructured data, such as text or images,
    classification of the data into different categories given a training set with
    labels is a supervised learning problem. However, since the data is unstructured,
    some statistical or information theoretic means are necessary to extract learnable
    features from the data. In the design of this study, we performed feature representation
    and selection on the documents before using linear, non-linear, and ensemble methods
    for classification.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在无结构数据的学习技术中，例如文本或图像，给定带有标签的训练集将数据分类到不同的类别是一个监督学习问题。然而，由于数据是无结构的，一些统计或信息论方法对于从数据中提取可学习特征是必要的。在本研究的构思中，我们在使用线性、非线性以及集成方法进行分类之前，对文档进行了特征表示和选择。
- en: Data collection
  id: totrans-440
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据收集
- en: 'The dataset used in the experiments is a version of the Reuters-21578 Distribution
    1.0 Text Categorization Dataset available from the UCI Machine Learning Repository:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 实验中使用的数据集是来自UCI机器学习仓库的Reuters-21578 Distribution 1.0文本分类数据集的一个版本：
- en: Note
  id: totrans-442
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Reuters-21578 dataset: [https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection](https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection)'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: Reuters-21578数据集：[https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection](https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection)
- en: This dataset is a Modified-Apte split containing 9,981 documents, each with
    a class label indicating the category of the document. There are 10 distinct categories
    in the dataset.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集是一个包含9,981个文档的Modified-Apte分割，每个文档都有一个类别标签，指示文档的类别。数据集中有10个不同的类别。
- en: Data sampling and transformation
  id: totrans-445
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据采样和转换
- en: 'After importing the data file, we performed a series of pre-processing steps
    in order to enrich and transform the data before training any models on the documents.
    These steps can be seen in the screenshot of the workflow created in KNIME. They
    include:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入数据文件后，我们进行了一系列预处理步骤，以便在训练任何模型之前丰富和转换数据。这些步骤可以在KNIME创建的工作流程快照中看到。它们包括：
- en: Punctuation erasure
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标点符号擦除
- en: N char filtering (removes tokens less than four characters in length)
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N字符过滤（移除长度小于四个字符的标记）
- en: Number filtering
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字过滤
- en: Case conversion – convert all to lower case
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 案例转换 - 将所有内容转换为小写
- en: Stop word filtering
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停用词过滤
- en: Stemming
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词干提取
- en: Prior to the learning step, we sampled the data randomly into a 70-30 split
    for training and testing, respectively. We used five-fold cross-validation in
    each experiment.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习步骤之前，我们将数据随机分为70-30的比例进行训练和测试。每个实验中使用了五折交叉验证。
- en: '![Data sampling and transformation](img/B05137_08_160.jpg)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
  zh: '![数据采样和转换](img/B05137_08_160.jpg)'
- en: The preceding screenshot shows the workflow for the first experiment set, which
    uses a binary vector of features. Data import is followed by a series of pre processing
    nodes, after which the dataset is transformed into a document vector. After adding
    back the target vector, the workflow branches out into four classification tasks,
    each using a five-fold cross-validation setup. Results are gathered in the Scorer
    node.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 上述截图显示了第一个实验集的工作流程，它使用特征的二进制向量。数据导入后，一系列预处理节点随后，数据集被转换为文档向量。在添加回目标向量后，工作流程分支为四个分类任务，每个任务使用五折交叉验证设置。结果在评分节点中汇总。
- en: Feature analysis and dimensionality reduction
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征分析和降维
- en: 'We conducted three sets of experiments in total. In the first set, after pre
    processing, we used binary vectorization of the terms, which adds a representation
    indicating whether or not a term appeared in the document:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总共进行了三组实验。在第一组中，预处理后，我们使用了术语的二进制向量化，这添加了一个表示术语是否出现在文档中的表示：
- en: '![Feature analysis and dimensionality reduction](img/B05137_1.jpg)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![特征分析和降维](img/B05137_1.jpg)'
- en: In the second experiment, we used the values for relative **Term Frequency**
    (**TF**) for each term, resulting in a value between 0 and 1\.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二次实验中，我们使用了每个术语的相对**词频**（**TF**）值，结果在0到1之间。
- en: '![Feature analysis and dimensionality reduction](img/B05137_08_163.jpg)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![特征分析和降维](img/B05137_08_163.jpg)'
- en: In the third, we performed feature selection by filtering out terms that had
    a relative TF score of less than 0.01.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三组中，我们通过过滤掉相对TF分数小于0.01的术语来进行特征选择。
- en: Models, results, and evaluation
  id: totrans-462
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型、结果和评估
- en: For each of the three sets of experiments, we used two linear classifiers (naïve
    Bayes and SVM using linear kernel) and two non-linear classifiers (Decision Tree
    and AdaBoost with Naïve Bayes as base learner). In text mining classification,
    precision/recall metrics are generally chosen as the evaluation metric over accuracy,
    which is more common in traditional, balanced classification problems.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这三组实验中的每一组，我们使用了两种线性分类器（朴素贝叶斯和线性核的SVM）以及两种非线性分类器（决策树和以朴素贝叶斯为基础学习器的AdaBoost）。在文本挖掘分类中，通常选择精确率/召回率指标作为评估指标，而不是准确率，后者在传统的、平衡的分类问题中更为常见。
- en: 'The results from the three sets of experiments are given in the tables. Scores
    are averages over all the classes:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 三组实验的结果在表中给出。分数是所有类别的平均值：
- en: 'Binary Term Vector:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制术语向量：
- en: '| Classifier | Recall | Precision | Sensitivity | Specificity | F-measure |
    Accuracy | Cohen''s kappa |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| 分类器 | 召回率 | 精确率 | 灵敏度 | 特异性 | F度量 | 准确率 | 科亨κappa系数 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Naïve Bayes | 0.5079 | 0.5281 | 0.5079 | 0.9634 | 0.5087 | 0.7063 | 0.6122
    |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| 基于朴素贝叶斯 | 0.5079 | 0.5281 | 0.5079 | 0.9634 | 0.5087 | 0.7063 | 0.6122 |'
- en: '| Decision Tree | 0.4989 | 0.5042 | 0.4989 | 0.9518 | 0.5013 | 0.7427(2) |
    0.6637(2) |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.4989 | 0.5042 | 0.4989 | 0.9518 | 0.5013 | 0.7427(2) | 0.6637(2)
    |'
- en: '| AdaBoost(NB) | 0.5118(2) | 0.5444(2) | 0.5118(2) | 0.9665(2) | 0.5219(2)
    | 0.7285 | 0.6425 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| AdaBoost(NB) | 0.5118(2) | 0.5444(2) | 0.5118(2) | 0.9665(2) | 0.5219(2)
    | 0.7285 | 0.6425 |'
- en: '| LibSVM | 0.6032(1) | 0.5633(1) | 0.6032(1) | 0.9808(1) | 0.5768(1) | 0.8290(1)
    | 0.7766(1) |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| LibSVM | 0.6032(1) | 0.5633(1) | 0.6032(1) | 0.9808(1) | 0.5768(1) | 0.8290(1)
    | 0.7766(1) |'
- en: 'Relative TF vector:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 相对TF向量：
- en: '| Classifier | Recall | Precision | Sensitivity | Specificity | F-measure |
    Accuracy | Cohen''s kappa |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| 分类器 | 召回率 | 精确率 | 灵敏度 | 特异性 | F度量 | 准确率 | 科亨κappa系数 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Naïve Bayes | 0.4853 | 0.5480(2) | 0.4853 | 0.9641 | 0.5113(2) | 0.7248 |
    0.6292 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 0.4853 | 0.5480(2) | 0.4853 | 0.9641 | 0.5113(2) | 0.7248 | 0.6292
    |'
- en: '| Decision Tree | 0.4947(2) | 0.4954 | 0.4947(2) | 0.9703(2) | 0.4950 | 0.7403(2)
    | 0.6612(2) |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.4947(2) | 0.4954 | 0.4947(2) | 0.9703(2) | 0.4950 | 0.7403(2) | 0.6612(2)
    |'
- en: '| AdaBoost(NB) | 0.4668 | 0.5326 | 0.4668 | 0.9669 | 0.4842 | 0.6963 | 0.6125
    |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| AdaBoost(NB) | 0.4668 | 0.5326 | 0.4668 | 0.9669 | 0.4842 | 0.6963 | 0.6125
    |'
- en: '| LibSVM | 0.6559(1) | 0.6651(1) | 0.6559(1) | 0.9824(1) | 0.6224(1) | 0.8433(1)
    | 0.7962(1) |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| LibSVM | 0.6559(1) | 0.6651(1) | 0.6559(1) | 0.9824(1) | 0.6224(1) | 0.8433(1)
    | 0.7962(1) |'
- en: 'Relative TF vector with threshold filtering (rel TF > 0.01):'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 相对TF向量与阈值过滤（rel TF > 0.01）：
- en: '| Classifier | Recall | Precision | Sensitivity | Specificity | F-measure |
    Accuracy | Cohen''s kappa |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| 分类器 | 召回率 | 精确率 | 敏感性 | 特异性 | F度量 | 准确率 | 科亨κ系数 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Naïve Bayes | 0.4689 | 0.5456(2) | 0.4689 | 0.9622 | 0.4988 | 0.7133 | 0.6117
    |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 0.4689 | 0.5456(2) | 0.4689 | 0.9622 | 0.4988 | 0.7133 | 0.6117 |'
- en: '| Decision Tree | 0.5008(2) | 0.5042 | 0.5008(2) | 0.9706(2) | 0.5022(2) |
    0.7439(2) | 0.6657(2) |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.5008(2) | 0.5042 | 0.5008(2) | 0.9706(2) | 0.5022(2) | 0.7439(2)
    | 0.6657(2) |'
- en: '| AdaBoost(NB) | 0.4435 | 0.4992 | 0.4435 | 0.9617 | 0.4598 | 0.6870 | 0.5874
    |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| AdaBoost(NB) | 0.4435 | 0.4992 | 0.4435 | 0.9617 | 0.4598 | 0.6870 | 0.5874
    |'
- en: '| LibSVM | 0.6438(1) | 0.6326(1) | 0.6438(1) | 0.9810(1) | 0.6118(1) | 0.8313(1)
    | 0.7806(1) |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| LibSVM | 0.6438(1) | 0.6326(1) | 0.6438(1) | 0.9810(1) | 0.6118(1) | 0.8313(1)
    | 0.7806(1) |'
- en: Analysis of text processing results
  id: totrans-486
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本处理结果分析
- en: 'The analysis of results obtained from our experiments on the Reuters dataset
    is presented here with some key observations:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里展示了从我们的Reuters数据集实验中获得的结果分析，并附带一些关键观察：
- en: As seen in the first table, with the binary representation of terms, Naïve Bayes
    scores around 0.7, which indicates that the features generated have good discriminating
    power. AdaBoost on the same configuration of Naïve Bayes further improves all
    the metrics, such as precision, recall, F1-measure, and accuracy, by about 2%,
    indicating the advantage of boosting and meta-learning.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如第一张表所示，使用项的二进制表示，朴素贝叶斯得分为约0.7，这表明生成的特征具有良好的区分能力。在相同配置的朴素贝叶斯上进一步使用AdaBoost可以提升所有指标，如精确率、召回率、F1度量以及准确率，大约提高2%，这表明了提升和元学习的优势。
- en: As seen in the first table, non-linear classifiers, such as Decision Tree, do
    only marginally better than linear Naïve Bayes in most metrics. SVM with a linear
    classifier increases accuracy by 17% over linear Naïve Bayes and has better metrics
    similarly in almost all measures. SVM and kernels, which have no issues with higher
    dimensional data, the curse of text classification, are thus one of the better
    algorithms for modeling, and the results confirm this.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如第一张表所示，非线性分类器，如决策树，在大多数指标上仅略优于线性朴素贝叶斯。使用线性分类器的SVM比线性朴素贝叶斯提高了17%的准确率，并且在几乎所有指标上都有更好的表现。SVM和核函数，在处理高维数据时没有问题，解决了文本分类的诅咒，因此是建模的较好算法之一，结果也证实了这一点。
- en: Changing the representation from binary to TF improves many measures, such as
    accuracy, for linear Naïve Bayes (from 0.70 to 0.72) and SVM (0.82 to 0.84). This
    indeed confirms that TF-based representation in many numeric-based algorithms,
    such as SVM. AdaBoost performance with Naïve Bayes drops in most metrics when
    the underlying classifier Bayes gets stronger in performance, as shown in many
    theoretical and empirical results.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将表示从二进制转换为TF（词频）改进了许多指标，如准确率，对于线性朴素贝叶斯（从0.70提高到0.72）和SVM（从0.82提高到0.84）。这确实证实了基于TF的表示在许多基于数字的算法中，如SVM。当底层分类器贝叶斯在性能上变得更强大时，如许多理论和实证结果所示，AdaBoost与朴素贝叶斯在大多数指标上的性能会下降。
- en: Finally, by reducing features using threshold TF > 0.01, as used here, we get
    almost similar or somewhat reduced performance in most classifiers, indicating
    that although certain terms seem rare, they have discriminating power, and reducing
    them has a negative impact.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，通过使用阈值TF > 0.01减少特征，正如这里所使用的，我们在大多数分类器中得到了几乎相似或略有降低的性能，这表明尽管某些术语似乎很少见，但它们具有区分能力，减少它们会产生负面影响。
- en: Summary
  id: totrans-492
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: A large proportion of information in the digital world is textual. Text mining
    and NLP are areas concerned with extracting information from this unstructured
    form of data. Several important sub areas in the field are active topics of research
    today and an understanding of these areas is essential for data scientists.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 数字世界中的大量信息是文本形式。文本挖掘和自然语言处理是关注从这种非结构化数据形式中提取信息的领域。该领域中的几个重要子领域现在是活跃的研究主题，对这些领域的理解对于数据科学家来说是必不可少的。
- en: Text categorization is concerned with classifying documents into pre-determined
    categories. Text may be enriched by annotating words, as with POS tagging, in
    order to give it more structure for subsequent processing tasks to act on. Unsupervised
    techniques such as clustering can be applied to documents as well. Information
    extraction and named entity recognition help identify information-rich specifics
    such as location, person or organization name, and so on. Summarization is another
    important application for producing concise abstracts of larger documents or sets
    of documents. Various ambiguities of language and semantics such as context, word
    sense, and reasoning make the tasks of NLP challenging.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类关注将文档分类到预先确定的类别中。文本可以通过标注单词（如词性标注）来丰富，以便为后续处理任务提供更多结构。聚类等无监督技术也可以应用于文档。信息提取和命名实体识别有助于识别信息丰富的具体内容，如地点、人名或组织名称等。摘要也是另一个重要应用，用于生成较大文档或文档集的简洁摘要。语言和语义的多种歧义，如上下文、词义和推理，使得自然语言处理（NLP）的任务具有挑战性。
- en: Transformations of the contents of text include tokenization, stop words removal,
    and word stemming, all of which prepare the corpus by standardizing the content
    so Machine Learning techniques can be applied productively. Next, lexical, semantic,
    and syntactic features are extracted so numerical values can represent the document
    structure more conventionally with a vector space model. Similarity and distance
    measures can then be applied to effectively compare documents for sameness. Dimensionality
    reduction is key due to the large number of features that are typically present.
    The details of the techniques for topic modeling, PLSA and text clustering, and
    named entity recognition are described in this chapter. Finally, the recent techniques
    employing deep learning in various fields of NLP are introduced to the readers.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 文本内容的转换包括分词、停用词去除和词干提取，所有这些通过标准化内容来准备语料库，以便机器学习技术能够有效地应用。接下来，提取词汇、语义和句法特征，以便使用向量空间模型以更传统的方式用数值表示文档结构。然后可以应用相似性和距离度量来有效地比较文档的相似性。由于通常存在大量特征，降维是关键。本章描述了主题建模技术、PLSA和文本聚类以及命名实体识别的技术细节。最后，介绍了在自然语言处理的各个领域中应用深度学习的最新技术。
- en: Mallet and KNIME are two open source Java-based tools that provide powerful
    NLP and Machine Learning capabilities. The case study examines performance of
    different classifiers on the Reuters corpus using KNIME.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: Mallet和KNIME是两种开源Java工具，它们提供了强大的自然语言处理和机器学习功能。案例研究考察了使用KNIME在Reuters语料库上不同分类器的性能。
- en: References
  id: totrans-497
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: J. B. Lovins (1968). *Development of a stemming algorithm*, Mechanical Translation
    and Computer Linguistic, vol.11, no.1/2, pp. 22-31\.
  id: totrans-498
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: J. B. Lovins (1968). *词干算法的开发*, 机械翻译与计算机语言学, 第11卷，第1/2期，第22-31页。
- en: Porter M.F, (1980). *An algorithm for suffix stripping*, Program; 14, 130-137.
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Porter M.F (1980). *后缀剥离算法*, 程序; 14, 130-137。
- en: ZIPF, H.P., (1949). *Human Behaviour and the Principle of Least Effort*, Addison-Wesley,
    Cambridge, Massachusetts.
  id: totrans-500
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ZIPF, H.P. (1949). *人类行为与最小努力原则*, 哈德逊出版社，马萨诸塞州剑桥。
- en: LUHN, H.P., (1958). *The automatic creation of literature abstracts*', IBM Journal
    of Research and Development, 2, 159-165.
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LUHN, H.P. (1958). *自动生成文献摘要*, IBM研究与发展杂志, 2, 159-165。
- en: Deerwester, S., Dumais, S., Furnas, G., & Landauer, T. (1990), *Indexing by
    latent semantic analysis*, Journal of the American Society for Information Sciences,
    41, 391–407\.
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Deerwester, S., Dumais, S., Furnas, G., & Landauer, T. (1990), *基于潜在语义分析的索引*,
    美国信息科学学会杂志, 41, 391–407。
- en: Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977), *Maximum likelihood from
    incomplete data via the EM algorithm*. Journal of the Royal Statistic Society,
    Series B, 39(1), 1–38.
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977), *通过EM算法从不完全数据中估计最大似然*.
    英国皇家统计学会，系列B，39(1)，1–38。
- en: Greiff, W. R. (1998). *A theory of term weighting based on exploratory data
    analysis*. In 21st Annual International ACM SIGIR Conference on Research and Development
    in Information Retrieval, New York, NY. ACM.
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Greiff, W. R. (1998). *基于探索性数据分析的词权重理论*. 在第21届国际ACM SIGIR信息检索研究与发展会议，纽约，纽约。ACM。
- en: P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della Pietra, and J/ C. Lai
    (1992), *Class-based n-gram models of natural language*, Computational Linguistics,
    18, 4, 467-479.
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della Pietra, 和 J/ C. Lai (1992),
    *基于类别的自然语言n-gram模型*, 计算语言学, 18, 4, 467-479.
- en: T. Liu, S. Lin, Z. Chen, W.-Y. Ma (2003), *An Evaluation on Feature Selection
    for Text Clustering*, ICML Conference.
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T. Liu, S. Lin, Z. Chen, W.-Y. Ma (2003)，*文本聚类特征选择的评估*，ICML 会议。
- en: Y. Yang, J. O. Pederson (1995). *A comparative study on feature selection in
    text categorization*, ACM SIGIR Conference.
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Y. Yang, J. O. Pederson (1995). *文本分类中特征选择的比较研究*，ACM SIGIR 会议。
- en: Salton, G. & Buckley, C. (1998). *Term weighting approaches in automatic text
    retrieval*. Information Processing & Management, 24(5), 513–523\.
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Salton, G. & Buckley, C. (1998). *自动文本检索中的词频加权方法*，《信息处理与管理》，24(5)，513–523。
- en: Hofmann, T. (2001). *Unsupervised learning by probabilistic latent semantic
    analysis*. Machine Learning Journal, 41(1), 177–196\.
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hofmann, T. (2001). *无监督学习通过概率潜在语义分析*. 机器学习杂志，41(1)，177–196。
- en: D. Blei, J. Lafferty (2006). *Dynamic topic models*. ICML Conference.
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D. Blei, J. Lafferty (2006). *动态主题模型*，ICML 会议。
- en: 'D. Blei, A. Ng, M. Jordan (2003). *Latent Dirichlet allocation*, Journal of
    Machine Learning Research, 3: pp. 993–1022.'
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'D. Blei, A. Ng, M. Jordan (2003). *潜在狄利克雷分配*，机器学习研究杂志，3: pp. 993–1022。'
- en: W. Xu, X. Liu, and Y. Gong (2003). *Document-Clustering based on Non-negative
    Matrix Factorization*. Proceedings of SIGIR'03, Toronto, CA, pp. 267-273,
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: W. Xu, X. Liu, 和 Y. Gong (2003). *基于非负矩阵分解的文档聚类*，SIGIR'03 会议论文，多伦多，加拿大，pp. 267-273。
- en: Dud´ik M. and Schapire (2006). R. E. *Maximum entropy distribution estimation
    with generalized regularization*. In Lugosi, G. and Simon, H. (Eds.), COLT, Berlin,
    pp. 123– 138, Springer-Verlag,.
  id: totrans-513
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dud´ik M. 和 Schapire (2006). R. E. *广义正则化下的最大熵分布估计*，Lugosi, G. 和 Simon, H. (编)，COLT，柏林，pp.
    123–138，Springer-Verlag。
- en: McCallum, A., Freitag, D., and Pereira, F. C. N. (2000). *Maximum Entropy Markov
    Models for Information Extraction and Segmentation*. In ICML, pp. 591–598..
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: McCallum, A., Freitag, D., 和 Pereira, F. C. N. (2000). *信息提取和分割的最大熵马尔可夫模型*，ICML，pp.
    591–598。
- en: Langville, A. N, Meyer, C. D., Albright, R. (2006). *Initializations for the
    Nonnegative Factorization*. KDD, Philadelphia, USA
  id: totrans-515
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Langville, A. N, Meyer, C. D., Albright, R. (2006). *非负因子分解的初始化*，KDD，费城，美国。
- en: Dunning, T. (1993). *Accurate Methods for the Statistics of Surprise and Coincidence.
    Computational Linguistics*, 19, 1, pp. 61-74.
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dunning, T. (1993). *准确计算惊喜和巧合的统计方法*，《计算语言学》，19，1，pp. 61-74。
- en: Y. Bengio, R. Ducharme, P. Vincent and C. Jauvin (2003). *A Neural Probabilistic
    Language Model*. Journal of Machine Learning Research.
  id: totrans-517
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Y. Bengio, R. Ducharme, P. Vincent 和 C. Jauvin (2003). *神经概率语言模型*，机器学习研究杂志。
- en: R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa.
    (2011). *Natural language processing (almost) from scratch*. Journal of Machine
    Learning Research, 12:2493–2537.
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, 和 P. Kuksa. (2011).
    *从零开始的自然语言处理*，机器学习研究杂志，12:2493–2537。
- en: T. Mikolov, K. Chen, G. Corrado and J. Dean (2013). *Efficient Estimation of
    Word Representations in Vector Space*. arXiv:1301.3781v1.
  id: totrans-519
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T. Mikolov, K. Chen, G. Corrado 和 J. Dean (2013). *在向量空间中高效估计词表示*，arXiv:1301.3781v1。
- en: R. Socher, Christopher Manning, and Andrew Ng. (2010). *Learning continuous
    phrase representations and syntactic parsing with recursive neural networks*.
    In NIPS 2010 Workshop on Deep Learning and Unsupervised Feature Learning.
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: R. Socher, Christopher Manning, 和 Andrew Ng. (2010). *使用递归神经网络学习连续短语表示和句法解析*，NIPS
    2010 深度学习和无监督特征学习研讨会。
- en: R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning. (2011).
    *Semi-supervised recursive autoencoders for predicting sentiment distributions*.
    In EMNLP.
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, 和 C. D. Manning. (2011). *半监督递归自动编码器用于预测情感分布*，EMNLP。
- en: M. Luong, R. Socher and C. Manning (2013). *Better word representations with
    recursive neural networks for morphology*. CONLL.
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: M. Luong, R. Socher 和 C. Manning (2013). *使用递归神经网络为形态学获得更好的词表示*，CONLL。
- en: 'A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, T. Mikolov, et al (2013).
    *Devise: A deep visual-semantic embedding model*. In NIPS Proceedings.'
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, T. Mikolov, 等人 (2013).
    *Devise：一种深度视觉-语义嵌入模型*，NIPS 论文。
- en: Léon Bottou (2011). From Machine Learning to Machine Reasoning. [https://arxiv.org/pdf/1102.1808v3.pdf](https://arxiv.org/pdf/1102.1808v3.pdf).
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Léon Bottou (2011). 从机器学习到机器推理。[https://arxiv.org/pdf/1102.1808v3.pdf](https://arxiv.org/pdf/1102.1808v3.pdf)。
- en: Cho, Kyunghyun, et al (2014). *Learning phrase representations using rnn encoder-decoder
    for statistical machine translation*. arXiv preprint arXiv:1406.1078.
  id: totrans-525
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cho, Kyunghyun, 等人 (2014). *使用 RNN 编码器-解码器学习短语表示以进行统计机器翻译*，arXiv 预印本 arXiv:1406.1078。
