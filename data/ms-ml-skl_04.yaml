- en: Chapter 4. From Linear Regression to Logistic Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章 从线性回归到逻辑回归
- en: In [Chapter 2](ch02.html "Chapter 2. Linear Regression"), *Linear Regression*,
    we discussed simple linear regression, multiple linear regression, and polynomial
    regression. These models are special cases of the generalized linear model, a
    flexible framework that requires fewer assumptions than ordinary linear regression.
    In this chapter, we will discuss some of these assumptions as they relate to another
    special case of the generalized linear model called **logistic regression**.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html "第2章. 线性回归")，*线性回归*中，我们讨论了简单线性回归、多元线性回归和多项式回归。这些模型是广义线性模型的特例，广义线性模型是一种灵活的框架，相比普通线性回归，它要求的假设更少。在本章中，我们将讨论这些假设，尤其是它们与广义线性模型的另一个特例——**逻辑回归**的关系。
- en: Unlike the models we discussed previously, logistic regression is used for classification
    tasks. Recall that the goal in classification tasks is to find a function that
    maps an observation to its associated class or label. A learning algorithm must
    use pairs of feature vectors and their corresponding labels to induce the values
    of the mapping function's parameters that produce the best classifier, as measured
    by a particular performance metric. In binary classification, the classifier must
    assign instances to one of the two classes. Examples of binary classification
    include predicting whether or not a patient has a particular disease, whether
    or not an audio sample contains human speech, or whether or not the Duke men's
    basketball team will lose in the first round of the NCAA tournament. In multiclass
    classification, the classifier must assign one of several labels to each instance.
    In multilabel classification, the classifier must assign a subset of the labels
    to each instance. In this chapter, we will work through several classification
    problems using logistic regression, discuss performance measures for the classification
    task, and apply some of the feature extraction techniques you learned in the previous
    chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前讨论的模型不同，逻辑回归用于分类任务。回想一下，在分类任务中，目标是找到一个函数，将一个观测值映射到其关联的类别或标签。学习算法必须使用特征向量对及其对应的标签来推导映射函数参数的值，从而生成最佳分类器，这个分类器的性能通过特定的性能度量来衡量。在二分类问题中，分类器必须将实例分配到两个类别中的一个。二分类的例子包括预测病人是否患有某种疾病，音频样本是否包含人类语音，或者杜克大学男篮队是否会在NCAA锦标赛第一轮输掉比赛。在多分类任务中，分类器必须为每个实例分配多个标签中的一个。在多标签分类中，分类器必须为每个实例分配多个标签中的一个子集。在本章中，我们将通过几个使用逻辑回归的分类问题，讨论分类任务的性能度量，并应用你在上一章中学到的一些特征提取技术。
- en: Binary classification with logistic regression
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用逻辑回归进行二分类
- en: Ordinary linear regression assumes that the response variable is normally distributed.
    The **normal** **distribution**, also known as the **Gaussian distribution** or
    **bell curve**, is a function that describes the probability that an observation
    will have a value between any two real numbers. Normally distributed data is symmetrical.
    That is, half of the values are greater than the mean and the other half of the
    values are less than the mean. The mean, median, and mode of normally distributed
    data are also equal. Many natural phenomena approximately follow normal distributions.
    For instance, the height of people is normally distributed; most people are of
    average height, a few are tall, and a few are short.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 普通线性回归假设响应变量服从正态分布。**正态** **分布**，也称为**高斯分布**或**钟形曲线**，是一个描述观测值在任意两个实数之间的概率的函数。正态分布的数据是对称的。也就是说，一半的值大于均值，另一半的值小于均值。正态分布数据的均值、中位数和众数也相等。许多自然现象大致遵循正态分布。例如，人的身高是正态分布的；大多数人身高适中，少数人高，少数人矮。
- en: 'In some problems the response variable is not normally distributed. For instance,
    a coin toss can result in two outcomes: heads or tails. The **Bernoulli distribution**
    describes the probability distribution of a random variable that can take the
    positive case with probability *P* or the negative case with probability *1-P*.
    If the response variable represents a probability, it must be constrained to the
    range {0,1}. Linear regression assumes that a constant change in the value of
    an explanatory variable results in a constant change in the value of the response
    variable, an assumption that does not hold if the value of the response variable
    represents a probability. Generalized linear models remove this assumption by
    relating a linear combination of the explanatory variables to the response variable
    using a link function. In fact, we already used a link function in [Chapter 2](ch02.html
    "Chapter 2. Linear Regression"), *Linear Regression*; ordinary linear regression
    is a special case of the generalized linear model that relates a linear combination
    of the explanatory variables to a normally distributed response variable using
    the **identity link function**. We can use a different link function to relate
    a linear combination of the explanatory variables to the response variable that
    is not normally distributed.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些问题中，响应变量并非服从正态分布。例如，抛硬币可以有两个结果：正面或反面。**伯努利分布**描述了一个随机变量的概率分布，该变量以概率*P*取正类，以概率*1-P*取负类。如果响应变量代表一个概率，它必须限制在{0,1}的范围内。线性回归假设解释变量值的恒定变化会导致响应变量值的恒定变化，但如果响应变量的值代表一个概率，这一假设就不成立。广义线性模型通过使用链接函数将解释变量的线性组合与响应变量联系起来，从而消除了这一假设。事实上，我们已经在[第2章](ch02.html
    "第2章. 线性回归")中使用了一个链接函数，*线性回归*；普通线性回归是广义线性模型的一个特例，它使用**恒等链接函数**将解释变量的线性组合与正态分布的响应变量联系起来。我们可以使用不同的链接函数将解释变量的线性组合与非正态分布的响应变量联系起来。
- en: 'In logistic regression, the response variable describes the probability that
    the outcome is the positive case. If the response variable is equal to or exceeds
    a discrimination threshold, the positive class is predicted; otherwise, the negative
    class is predicted. The response variable is modeled as a function of a linear
    combination of the explanatory variables using the **logistic** **function**.
    Given by the following equation, the logistic function always returns a value
    between zero and one:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中，响应变量描述了结果为正类的概率。如果响应变量等于或超过判别阈值，则预测为正类；否则，预测为负类。响应变量被建模为解释变量线性组合的**逻辑**
    **函数**的函数。逻辑函数由以下方程给出，它始终返回一个介于零和一之间的值：
- en: '![Binary classification with logistic regression](img/8365OS_04_01.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![使用逻辑回归的二分类](img/8365OS_04_01.jpg)'
- en: 'The following is a plot of the value of the logistic function for the range
    {-6,6}:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是逻辑函数在区间{-6,6}范围内的值的图示：
- en: '![Binary classification with logistic regression](img/8365OS_04_02.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![使用逻辑回归的二分类](img/8365OS_04_02.jpg)'
- en: 'For logistic regression, ![Binary classification with logistic regression](img/8365OS_04_16.jpg)
    is equal to a linear combination of explanatory variables, as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于逻辑回归，![使用逻辑回归的二分类](img/8365OS_04_16.jpg)等同于解释变量的线性组合，如下所示：
- en: '![Binary classification with logistic regression](img/8365OS_04_03.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![使用逻辑回归的二分类](img/8365OS_04_03.jpg)'
- en: 'The **logit function** is the inverse of the logistic function. It links ![Binary
    classification with logistic regression](img/8365OS_04_17.jpg) back to a linear
    combination of the explanatory variables:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**Logit 函数**是逻辑函数的逆函数。它将![使用逻辑回归的二分类](img/8365OS_04_17.jpg)与解释变量的线性组合联系起来：'
- en: '![Binary classification with logistic regression](img/8365OS_04_04.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![使用逻辑回归的二分类](img/8365OS_04_04.jpg)'
- en: Now that we have defined the model for logistic regression, let's apply it to
    a binary classification task.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了逻辑回归模型，接下来让我们将其应用于一个二分类任务。
- en: Spam filtering
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 垃圾邮件过滤
- en: 'Our first problem is a modern version of the canonical binary classification
    problem: spam classification. In our version, however, we will classify spam and
    ham SMS messages rather than e-mail. We will extract TF-IDF features from the
    messages using techniques you learned in [Chapter 3](ch03.html "Chapter 3. Feature
    Extraction and Preprocessing"), *Feature Extraction and Preprocessing*, and classify
    the messages using logistic regression.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个问题是经典的二分类问题的现代版本：垃圾邮件分类。然而，在我们的版本中，我们将对垃圾短信和正常短信进行分类，而不是电子邮件。我们将使用在[第3章](ch03.html
    "第3章. 特征提取与预处理")，*特征提取与预处理*中学到的技术从短信中提取TF-IDF特征，并使用逻辑回归对短信进行分类。
- en: 'We will use the SMS Spam Classification Data Set from the UCI Machine Learning
    Repository. The dataset can be downloaded from [http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection](http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection).
    First, let''s explore the data set and calculate some basic summary statistics
    using pandas:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自UCI机器学习库的SMS垃圾邮件分类数据集。该数据集可以从[http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection](http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)下载。首先，让我们探索数据集，并使用pandas计算一些基本的统计信息：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'A binary label and a text message comprise each row. The data set contains
    5,574 instances; 4,827 messages are ham and the remaining 747 messages are spam.
    The ham messages are labeled with zero, and the spam messages are labeled with
    one. While the noteworthy, or case, outcome is often assigned the label one and
    the non-case outcome is often assigned zero, these assignments are arbitrary.
    Inspecting the data may reveal other attributes that should be captured in the
    model. The following selection of messages characterizes both of the classes:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每行数据包含一个二进制标签和一条短信。数据集包含5,574个实例；4,827条消息是正常短信，剩余747条是垃圾短信。正常短信标记为零，垃圾短信标记为一。虽然值得注意的或案例结果通常被分配标签一，而非案例结果通常分配标签零，但这些分配是任意的。检查数据可能会揭示应当在模型中捕获的其他属性。以下是一些典型消息，代表了两类短信：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s make some predictions using scikit-learn''s `LogisticRegression` class:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用scikit-learn的`LogisticRegression`类进行一些预测：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'First, we load the `.csv` file using pandas and split the data set into training
    and test sets. By default, `train_test_split()` assigns 75 percent of the samples
    to the training set and allocates the remaining 25 percent of the samples to the
    test set:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用pandas加载`.csv`文件，并将数据集划分为训练集和测试集。默认情况下，`train_test_split()`将75%的样本分配到训练集中，并将剩余的25%的样本分配到测试集中：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we create a `TfidfVectorizer`. Recall from [Chapter 3](ch03.html "Chapter 3. Feature
    Extraction and Preprocessing"), *Feature Extraction and Preprocessing*, that `TfidfVectorizer`
    combines `CountVectorizer` and `TfidfTransformer`. We fit it with the training
    messages, and transform both the training and test messages:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个`TfidfVectorizer`。回顾[第3章](ch03.html "第3章. 特征提取与预处理")，*特征提取与预处理*，`TfidfVectorizer`结合了`CountVectorizer`和`TfidfTransformer`。我们用训练短信对其进行拟合，并对训练和测试短信进行转换：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we create an instance of `LogisticRegression` and train our model.
    Like `LinearRegression`, `LogisticRegression` implements the `fit()` and `predict()`
    methods. As a sanity check, we printed a few predictions for manual inspection:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建一个`LogisticRegression`实例并训练我们的模型。像`LinearRegression`一样，`LogisticRegression`实现了`fit()`和`predict()`方法。作为一个合理性检查，我们打印了一些预测结果供人工检查：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following is the output of the script:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是脚本的输出：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: How well does our classifier perform? The performance metrics we used for linear
    regression are inappropriate for this task. We are only interested in whether
    the predicted class was correct, not how far it was from the decision boundary.
    In the next section, we will discuss some performance metrics that can be used
    to evaluate binary classifiers.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分类器表现如何？我们用于线性回归的性能指标对于这个任务不合适。我们关心的只是预测类别是否正确，而不是距离决策边界有多远。在接下来的部分，我们将讨论一些可以用来评估二分类器的性能指标。
- en: Binary classification performance metrics
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二分类性能指标
- en: A variety of metrics exist to evaluate the performance of binary classifiers
    against trusted labels. The most common metrics are **accuracy**, **precision**,
    **recall**, **F1 measure**, and **ROC AUC score**. All of these measures depend
    on the concepts of **true positives**, **true** **negatives**, **false positives**,
    and **false negatives**. *Positive* and *negative* refer to the classes. *True*
    and *false* denote whether the predicted class is the same as the true class.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多种指标来评估二元分类器对可信标签的性能。最常见的指标包括**准确性**、**精确率**、**召回率**、**F1度量**和**ROC AUC分数**。所有这些度量都依赖于**真正预测**、**真负预测**、**假正预测**和**假负预测**的概念。*正*和*负*是指类别。*真*和*假*表示预测类别是否与真实类别相同。
- en: 'For our SMS spam classifier, a true positive prediction is when the classifier
    correctly predicts that a message is spam. A true negative prediction is when
    the classifier correctly predicts that a message is ham. A prediction that a ham
    message is spam is a false positive prediction, and a spam message incorrectly
    classified as ham is a false negative prediction. A **confusion matrix**, or **contingency
    table**, can be used to visualize true and false positives and negatives. The
    rows of the matrix are the true classes of the instances, and the columns are
    the predicted classes of the instances:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的短信垃圾分类器，真正预测是指分类器正确预测消息是垃圾。真负预测是指分类器正确预测消息是非垃圾。将非垃圾消息预测为垃圾是假正预测，将垃圾消息错误分类为非垃圾是假负预测。**混淆矩阵**或**列联表**可以用来可视化真正和假正、假负预测。矩阵的行是实例的真实类别，列是实例的预测类别：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The confusion matrix indicates that there were four true negative predictions,
    three true positive predictions, two false negative predictions, and one false
    positive prediction. Confusion matrices become more useful in multi-class problems,
    in which it can be difficult to determine the most frequent types of errors.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵显示有四个真负预测，三个真正预测，两个假负预测和一个假正预测。在多类问题中，混淆矩阵变得更加有用，因为在这些问题中确定最常见的错误类型可能很困难。
- en: '![Binary classification performance metrics](img/8365OS_04_05.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![二分类性能指标](img/8365OS_04_05.jpg)'
- en: Accuracy
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准确性
- en: 'Accuracy measures a fraction of the classifier''s predictions that are correct.
    scikit-learn provides a function to calculate the accuracy of a set of predictions
    given the correct labels:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性衡量分类器预测正确的分数比例。scikit-learn提供一个函数来计算给定正确标签的一组预测的准确性：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`LogisticRegression.score()` predicts and scores labels for a test set using
    accuracy. Let''s evaluate our classifier''s accuracy:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`LogisticRegression.score()` 使用准确率预测和评分测试集标签。让我们评估我们分类器的准确性：'
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that your accuracy may differ as the training and test sets are assigned
    randomly. While accuracy measures the overall correctness of the classifier, it
    does not distinguish between false positive errors and false negative errors.
    Some applications may be more sensitive to false negatives than false positives,
    or vice versa. Furthermore, accuracy is not an informative metric if the proportions
    of the classes are skewed in the population. For example, a classifier that predicts
    whether or not credit card transactions are fraudulent may be more sensitive to
    false negatives than to false positives. To promote customer satisfaction, the
    credit card company may prefer to risk verifying legitimate transactions than
    risk ignoring a fraudulent transaction. Because most transactions are legitimate,
    accuracy is not an appropriate metric for this problem. A classifier that always
    predicts that transactions are legitimate could have a high accuracy score, but
    would not be useful. For these reasons, classifiers are often evaluated using
    two additional measures called precision and recall.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于训练集和测试集是随机分配的，您的准确性可能会有所不同。虽然准确性衡量了分类器的整体正确性，但它不区分假正误差和假负误差。有些应用程序对假负误差比假正误差更敏感，反之亦然。此外，如果人群中类别的比例不均衡，准确性不是一个信息丰富的度量。例如，一个预测信用卡交易是否欺诈的分类器可能更容易对假负误差敏感，而不是对假正误差敏感。为了促进客户满意度，信用卡公司可能更愿意冒险验证合法交易，而不是冒险忽略欺诈交易。由于大多数交易是合法的，准确性不是这个问题的合适度量。一个总是预测交易合法的分类器可能有很高的准确性分数，但是并不实用。因此，分类器通常使用称为精确率和召回率的两个额外指标进行评估。
- en: Precision and recall
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精确率和召回率
- en: 'Recall from [Chapter 1](ch01.html "Chapter 1. The Fundamentals of Machine Learning"),
    *The Fundamentals of Machine Learning*, that precision is the fraction of positive
    predictions that are correct. For instance, in our SMS spam classifier, precision
    is the fraction of messages classified as spam that are actually spam. Precision
    is given by the following ratio:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾[第 1 章](ch01.html "第 1 章. 机器学习基础")，*机器学习基础*，精确度是正确预测为正类的比例。例如，在我们的 SMS 垃圾邮件分类器中，精确度是被分类为垃圾邮件的消息中实际是垃圾邮件的比例。精确度通过以下比率给出：
- en: '![Precision and recall](img/8365OS_04_06.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![精确度和召回率](img/8365OS_04_06.jpg)'
- en: 'Sometimes called sensitivity in medical domains, recall is the fraction of
    the truly positive instances that the classifier recognizes. A recall score of
    one indicates that the classifier did not make any false negative predictions.
    For our SMS spam classifier, recall is the fraction of spam messages that were
    truly classified as spam. Recall is calculated with the following ratio:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在医学领域，有时称为敏感度，召回率是分类器识别的真正正实例的比例。召回率为 1 表示分类器没有做出任何假阴性预测。对于我们的 SMS 垃圾邮件分类器，召回率是正确分类为垃圾邮件的垃圾邮件消息的比例。召回率通过以下比率计算：
- en: '![Precision and recall](img/8365OS_04_07.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![精确度和召回率](img/8365OS_04_07.jpg)'
- en: 'Individually, precision and recall are seldom informative; they are both incomplete
    views of a classifier''s performance. Both precision and recall can fail to distinguish
    classifiers that perform well from certain types of classifiers that perform poorly.
    A trivial classifier could easily achieve a perfect recall score by predicting
    positive for every instance. For example, assume that a test set contains ten
    positive examples and ten negative examples. A classifier that predicts positive
    for every example will achieve a recall of one, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 单独来看，精确度和召回率通常并不能提供足够的信息；它们都是分类器性能的片面视图。精确度和召回率都可能无法区分表现良好的分类器和某些表现不佳的分类器。一种简单的分类器通过对每个实例都预测为正类，可以轻松获得完美的召回率。例如，假设测试集包含十个正类实例和十个负类实例。一个对每个实例都预测为正类的分类器将获得
    1 的召回率，如下所示：
- en: '![Precision and recall](img/8365OS_04_08.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![精确度和召回率](img/8365OS_04_08.jpg)'
- en: A classifier that predicts negative for every example, or that makes only false
    positive and true negative predictions, will achieve a recall score of zero. Similarly,
    a classifier that predicts that only a single instance is positive and happens
    to be correct will achieve perfect precision.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一个对每个示例都预测为负类的分类器，或者只做出假阳性和真阴性预测的分类器，将会获得零的召回率。同样，一个预测只有单个实例为正类且恰好正确的分类器，将获得完美的精确度。
- en: 'scikit-learn provides a function to calculate the precision and recall for
    a classifier from a set of predictions and the corresponding set of trusted labels.
    Let''s calculate our SMS classifier''s precision and recall:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供了一个函数，可以根据一组预测和相应的可信标签集来计算分类器的精确度和召回率。让我们计算一下我们 SMS 分类器的精确度和召回率：
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Our classifier's precision is 0.992; almost all of the messages that it predicted
    as spam were actually spam. Its recall is lower, indicating that it incorrectly
    classified approximately 22 percent of the spam messages as ham. Your precision
    and recall may vary since the training and test data are randomly partitioned.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分类器的精确度是 0.992；几乎所有它预测为垃圾邮件的消息实际上都是垃圾邮件。它的召回率较低，这表明它错误地将大约 22% 的垃圾邮件消息误分类为非垃圾邮件。由于训练和测试数据是随机划分的，您的精确度和召回率可能有所不同。
- en: Calculating the F1 measure
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算 F1 得分
- en: 'The F1 measure is the harmonic mean, or weighted average, of the precision
    and recall scores. Also called the f-measure or the f-score, the F1 score is calculated
    using the following formula:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: F1 得分是精确度和召回率的调和平均数或加权平均数。也称为 f 值或 f 得分，F1 得分通过以下公式计算：
- en: '![Calculating the F1 measure](img/8365OS_04_09.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![计算 F1 得分](img/8365OS_04_09.jpg)'
- en: 'The F1 measure penalizes classifiers with imbalanced precision and recall scores,
    like the trivial classifier that always predicts the positive class. A model with
    perfect precision and recall scores will achieve an F1 score of one. A model with
    a perfect precision score and a recall score of zero will achieve an F1 score
    of zero. As for precision and recall, scikit-learn provides a function to calculate
    the F1 score for a set of predictions. Let''s compute our classifier''s F1 score.
    The following snippet continues the previous code sample:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: F1度量值惩罚那些精度和召回率不平衡的分类器，例如总是预测正类的简单分类器。一个具有完美精度和召回率的模型将获得F1分数为1。一个具有完美精度但召回率为零的模型将获得F1分数为零。至于精度和召回率，scikit-learn提供了一个函数来计算一组预测的F1分数。让我们计算我们分类器的F1分数。以下代码片段继续前面的示例：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The arithmetic mean of our classifier's precision and recall scores is 0.803\.
    As the difference between the classifier's precision and recall is small, the
    F1 measure's penalty is small. Models are sometimes evaluated using the F0.5 and
    F2 scores, which favor precision over recall and recall over precision, respectively.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分类器的精度和召回率的算术平均值为0.803。由于分类器的精度和召回率差异较小，因此F1度量的惩罚较小。有时模型会使用F0.5和F2分数进行评估，分别偏向精度和召回率。
- en: ROC AUC
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ROC AUC
- en: 'A **Receiver Operating Characteristic**, or **ROC curve**, visualizes a classifier''s
    performance. Unlike accuracy, the ROC curve is insensitive to data sets with unbalanced
    class proportions; unlike precision and recall, the ROC curve illustrates the
    classifier''s performance for all values of the discrimination threshold. ROC
    curves plot the classifier''s recall against its **fall-out**. Fall-out, or the
    false positive rate, is the number of false positives divided by the total number
    of negatives. It is calculated using the following formula:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收者操作特征**（Receiver Operating Characteristic），或称**ROC曲线**，可视化分类器的性能。与准确率不同，ROC曲线对类别不平衡的数据集不敏感；与精度和召回率不同，ROC曲线展示了分类器在所有判别阈值下的表现。ROC曲线绘制了分类器的召回率与其**误报率**之间的关系。误报率，或称假阳性率，是假阳性数量除以总负样本数。它通过以下公式计算：'
- en: '![ROC AUC](img/8365OS_04_10.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![ROC AUC](img/8365OS_04_10.jpg)'
- en: '**AUC** is the area under the ROC curve; it reduces the ROC curve to a single
    value, which represents the expected performance of the classifier. The dashed
    line in the following figure is for a classifier that predicts classes randomly;
    it has an AUC of 0.5\. The solid curve is for a classifier that outperforms random
    guessing:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**AUC**是ROC曲线下的面积，它将ROC曲线简化为一个值，该值代表分类器的预期性能。下图中的虚线表示一个随机预测类别的分类器，它的AUC为0.5。实线则表示一个优于随机猜测的分类器：'
- en: '![ROC AUC](img/8365OS_04_11.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![ROC AUC](img/8365OS_04_11.jpg)'
- en: 'Let''s plot the ROC curve for our SMS spam classifier:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制我们SMS垃圾短信分类器的ROC曲线：
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'From the ROC AUC plot, it is apparent that our classifier outperforms random
    guessing; most of the plot area lies under its curve:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从ROC AUC图中可以明显看出，我们的分类器优于随机猜测；大部分图形区域位于其曲线下方：
- en: '![ROC AUC](img/8365OS_04_12.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![ROC AUC](img/8365OS_04_12.jpg)'
- en: Tuning models with grid search
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用网格搜索调优模型
- en: 'Hyperparameters are parameters of the model that are not learned. For example,
    hyperparameters of our logistic regression SMS classifier include the value of
    the regularization term and thresholds used to remove words that appear too frequently
    or infrequently. In scikit-learn, hyperparameters are set through the model''s
    constructor. In the previous examples, we did not set any arguments for `LogisticRegression()`;
    we used the default values for all of the hyperparameters. These default values
    are often a good start, but they may not produce the optimal model. **Grid search**
    is a common method to select the hyperparameter values that produce the best model.
    Grid search takes a set of possible values for each hyperparameter that should
    be tuned, and evaluates a model trained on each element of the Cartesian product
    of the sets. That is, grid search is an exhaustive search that trains and evaluates
    a model for each possible combination of the hyperparameter values supplied by
    the developer. A disadvantage of grid search is that it is computationally costly
    for even small sets of hyperparameter values. Fortunately, it is an **embarrassingly
    parallel** problem; many models can easily be trained and evaluated concurrently
    since no synchronization is required between the processes. Let''s use scikit-learn''s
    `GridSearchCV()` function to find better hyperparameter values:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数是模型的参数，它们不是通过学习获得的。例如，我们的逻辑回归 SMS 分类器的超参数包括正则化项的值和用于去除频繁或不频繁出现的词汇的阈值。在 scikit-learn
    中，超参数通过模型的构造函数进行设置。在之前的示例中，我们没有为 `LogisticRegression()` 设置任何参数；我们使用了所有超参数的默认值。这些默认值通常是一个好的起点，但它们可能无法产生最优模型。**网格搜索**是一种常见的方法，用于选择产生最佳模型的超参数值。网格搜索为每个需要调整的超参数提供一组可能的值，并评估在这些值的笛卡尔积元素上训练的模型。也就是说，网格搜索是一种穷举搜索，它为开发者提供的每种超参数值的所有可能组合训练和评估模型。网格搜索的一个缺点是，即使对于小型的超参数值集合，它的计算成本也很高。幸运的是，这是一个**尴尬的并行**问题；因为进程之间不需要同步，许多模型可以轻松并行训练和评估。让我们使用
    scikit-learn 的 `GridSearchCV()` 函数来寻找更好的超参数值：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`GridSearchCV()` takes an estimator, a parameter space, and performance measure.
    The argument `n_jobs` specifies the maximum number of concurrent jobs; set `n_jobs`
    to `-1` to use all CPU cores. Note that `fit()` must be called in a Python `main`
    block in order to fork additional processes; this example must be executed as
    a script, and not in an interactive interpreter:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV()` 接受一个估计器、一个参数空间和一个性能衡量标准。参数 `n_jobs` 指定并行作业的最大数量；将 `n_jobs`
    设置为 `-1` 以使用所有 CPU 核心。请注意，`fit()` 必须在 Python 的 `main` 块中调用，以便分叉出额外的进程；此示例必须作为脚本执行，而不是在交互式解释器中运行：'
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following is the output of the script:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是脚本的输出：
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Optimizing the values of the hyperparameters has improved our model's recall
    score on the `test` set.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 优化超参数的值提高了我们模型在`test`集上的召回率。
- en: Multi-class classification
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多类分类
- en: In the previous sections you learned to use logistic regression for binary classification.
    In many classification problems, however, there are more than two classes that
    are of interest. We might wish to predict the genres of songs from samples of
    audio, or classify images of galaxies by their types. The goal of **multi-class
    classification** is to assign an instance to one of the set of classes. scikit-learn
    uses a strategy called **one-vs.-all**, or **one-vs.-the-rest**, to support multi-class
    classification. One-vs.-allclassification uses one binary classifier for each
    of the possible classes. The class that is predicted with the greatest confidence
    is assigned to the instance. `LogisticRegression` supports multi-class classification
    using the one-versus-all strategy out of the box. Let's use `LogisticRegression`
    for a multi-class classification problem.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你学会了使用逻辑回归进行二分类。然而，在许多分类问题中，通常会涉及多个类别。我们可能希望根据音频样本预测歌曲的类型，或根据星系的类型对图像进行分类。**多类分类**的目标是将实例分配给一组类别中的一个。scikit-learn
    使用一种称为**一对多**（one-vs.-all）或**一对其余**（one-vs.-the-rest）的方法来支持多类分类。一对多分类为每个可能的类别使用一个二分类器。预测时，具有最大置信度的类别会被分配给该实例。`LogisticRegression`
    天生支持使用一对多策略进行多类分类。让我们使用 `LogisticRegression` 来解决一个多类分类问题。
- en: Assume that you would like to watch a movie, but you have a strong aversion
    to watching bad movies. To inform your decision, you could read reviews of the
    movies you are considering, but unfortunately you also have a strong aversion
    to reading movie reviews. Let's use scikit-learn to find the movies with good
    reviews.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想看一部电影，但你对观看糟糕的电影有强烈的排斥感。为了帮助你做出决策，你可以阅读一些关于你考虑的电影的评论，但不幸的是，你也对阅读电影评论有强烈的排斥感。让我们使用
    scikit-learn 来找出评价较好的电影。
- en: 'In this example, we will classify the sentiments of phrases taken from movie
    reviews in the Rotten Tomatoes data set. Each phrase can be classified as one
    of the following sentiments: negative, somewhat negative, neutral, somewhat positive,
    or positive. While the classes appear to be ordered, the explanatory variables
    that we will use do not always corroborate this order due to sarcasm, negation,
    and other linguistic phenomena. Instead, we will approach this problem as a multi-class
    classification task.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将对来自 Rotten Tomatoes 数据集中的电影评论短语进行情感分类。每个短语可以被分类为以下情感之一：负面、稍微负面、中立、稍微正面或正面。虽然这些类别看似有序，但我们将使用的解释变量并不总是能支持这种顺序，因为讽刺、否定和其他语言现象的存在。相反，我们将把这个问题作为一个多类分类任务来处理。
- en: 'The data can be downloaded from [http://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data](http://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data).
    First, let''s explore the data set using pandas. Note that the import and data-loading
    statements in the following snippet are required for the subsequent snippets:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以从[http://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data](http://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data)下载。首先，让我们使用
    pandas 来探索数据集。请注意，以下代码片段中的导入和数据加载语句是后续代码片段所必需的：
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The columns of the data set are tab delimited. The data set contains 1,56,060
    instances.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的列是以制表符分隔的。数据集包含 156,060 个实例。
- en: '[PRE17]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `Sentiment` column contains the response variables. The `0` label corresponds
    to the sentiment `negative`, `1` corresponds to `somewhat negative`, and so on.
    The `Phrase` column contains the raw text. Each sentence from the movie reviews
    has been parsed into smaller phrases. We will not require the `PhraseId` and `SentenceId`
    columns in this example. Let''s print some of the phrases and examine them:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`情感`列包含响应变量。`0` 标签对应情感`负面`，`1` 对应`稍微负面`，依此类推。`短语`列包含原始文本。每个电影评论的句子已被解析成更小的短语。在这个例子中，我们不需要`短语ID`和`句子ID`列。让我们打印一些短语并查看它们：'
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now let''s examine the target classes:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来查看目标类别：
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The most common class, `Neutral`, includes more than 50 percent of the instances.
    Accuracy will not be an informative performance measure for this problem, as a
    degenerate classifier that predicts only `Neutral` can obtain an accuracy near
    0.5\. Approximately one quarter of the reviews are positive or somewhat positive,
    and approximately one fifth of the reviews are negative or somewhat negative.
    Let''s train a classifier with scikit-learn:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的类别是`中立`，它包含超过 50% 的实例。准确度对于这个问题来说并不是一个有意义的性能衡量标准，因为一个退化的分类器只预测`中立`就能获得接近
    0.5 的准确度。大约四分之一的评论是正面或稍微正面的，约五分之一的评论是负面或稍微负面的。让我们用 scikit-learn 来训练一个分类器：
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following is the output of the script:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是脚本的输出：
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Multi-class classification performance metrics
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多类分类性能指标
- en: 'As with binary classification, confusion matrices are useful for visualizing
    the types of errors made by the classifier. Precision, recall, and F1 score can
    be computed for each of the classes, and accuracy for all of the predictions can
    also be calculated. Let''s evaluate our classifier''s predictions. The following
    snippet continues the previous example:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与二分类一样，混淆矩阵有助于可视化分类器所犯的错误类型。可以为每个类别计算精确度、召回率和 F1 分数，同时也可以计算所有预测的准确性。让我们评估分类器的预测结果。以下代码片段继续上一个示例：
- en: '[PRE22]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following will be appended to the output:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容将被追加到输出中：
- en: '[PRE23]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: First, we make predictions using the best parameter set found by using grid
    searching. While our classifier is an improvement over the baseline classifier,
    it frequently mistakes `Somewhat Positive` and `Somewhat Negative` for `Neutral`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用通过网格搜索找到的最佳参数集进行预测。尽管我们的分类器比基线分类器有所改进，但它仍然常常将`稍微正面`和`稍微负面`误分类为`中立`。
- en: Multi-label classification and problem transformation
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多标签分类和问题转化
- en: In the previous sections, we discussed binary classification, in which each
    instance must be assigned to one of the two classes, and multi-class classification,
    in which each instance must be assigned to one of the set of classes. The final
    type of classification problem that we will discuss is multi-label classification,
    in which each instance can be assigned a subset of the set of classes. Examples
    of multi-label classification include assigning tags to messages posted on a forum,
    and classifying the objects present in an image. There are two groups of approaches
    for multi-label classification.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们讨论了二分类问题，其中每个实例必须被分配到两个类别中的一个；还有多分类问题，其中每个实例必须被分配到类别集合中的一个。接下来我们要讨论的分类问题类型是多标签分类问题，其中每个实例可以被分配到类别集合的一个子集。多标签分类的例子包括为论坛上的帖子分配标签，以及对图像中的物体进行分类。多标签分类的方法有两种类型。
- en: '**Problem transformation** methods are techniques that cast the original multi-label
    problem as a set of single-label classification problems. The first problem transformation
    method that we will review converts each set of labels encountered in the training
    data to a single label. For example, consider a multi-label classification problem
    in which news articles must be assigned to one or more categories from a set.
    The following training data contains seven articles that can pertain to one or
    more of the five categories.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题转化**方法是将原始的多标签问题转化为一组单标签分类问题的技术。我们将要回顾的第一个问题转化方法是将训练数据中遇到的每个标签集转化为单一标签。例如，考虑一个多标签分类问题，在该问题中，新闻文章必须从一组类别中分配到一个或多个类别。以下训练数据包含七篇文章，这些文章可以涉及五个类别中的一个或多个。'
- en: '|   | Categories |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|   | 类别 |'
- en: '| --- | --- |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **Instance** | **Local** | **US** | **Business** | **Science and Technology**
    | **Sports** |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| **实例** | **本地** | **美国** | **商业** | **科技** | **体育** |'
- en: '| 1 | ✔ | ✔ |   |   |   |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 1 | ✔ | ✔ |   |   |   |'
- en: '| 2 | ✔ |   | ✔ |   |   |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 2 | ✔ |   | ✔ |   |   |'
- en: '| 3 |   |   | ✔ | ✔ |   |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 3 |   |   | ✔ | ✔ |   |'
- en: '| 4 |   |   |   |   | ✔ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 4 |   |   |   |   | ✔ |'
- en: '| 5 | ✔ |   |   |   |   |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 5 | ✔ |   |   |   |   |'
- en: '| 6 |   |   | ✔ |   |   |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 6 |   |   | ✔ |   |   |'
- en: '| 7 |   | ✔ |   | ✔ |   |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 7 |   | ✔ |   | ✔ |   |'
- en: Transforming the problem into a single-label classification task using the power
    set of labels seen in the training data results in the following training data.
    Previously, the first instance was classified as `Local` and `US`. Now it has
    a single label, `Local` ![Multi-label classification and problem transformation](img/8365OS_04_18.jpg)
    ` US`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 将问题转化为一个单标签分类任务，使用在训练数据中看到的标签的幂集，最终得到以下训练数据。之前，第一个实例被分类为`本地`和`美国`。现在它只有一个标签，`本地`
    ![多标签分类和问题转化](img/8365OS_04_18.jpg) ` 美国`。
- en: '|   | Category |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|   | 类别 |'
- en: '| --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **Instance** | **Local** | **Local** ![Multi-label classification and problem
    transformation](img/8365OS_04_18.jpg) ** US** | **Business** | **Local** ![Multi-label
    classification and problem transformation](img/8365OS_04_18.jpg) ** Business**
    | **US** ![Multi-label classification and problem transformation](img/8365OS_04_18.jpg)
    ** Science and Technology** | **Business** ![Multi-label classification and problem
    transformation](img/8365OS_04_18.jpg) ** Science and Technology** | **Sports**
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| **实例** | **本地** | **本地** ![多标签分类和问题转化](img/8365OS_04_18.jpg) ** 美国** | **商业**
    | **本地** ![多标签分类和问题转化](img/8365OS_04_18.jpg) ** 商业** | **美国** ![多标签分类和问题转化](img/8365OS_04_18.jpg)
    ** 科技** | **商业** ![多标签分类和问题转化](img/8365OS_04_18.jpg) ** 科技** | **体育** |'
- en: '| 1 |   | ✔ |   |   |   |   |   |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 1 |   | ✔ |   |   |   |   |   |'
- en: '| 2 |   |   |   | ✔ |   |   |   |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 2 |   |   |   | ✔ |   |   |   |'
- en: '| 3 |   |   |   |   |   | ✔ |   |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 3 |   |   |   |   |   | ✔ |   |'
- en: '| 4 |   |   |   |   |   |   | ✔ |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 4 |   |   |   |   |   |   | ✔ |'
- en: '| 5 | ✔ |   |   |   |   |   |   |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 5 | ✔ |   |   |   |   |   |   |'
- en: '| 6 |   |   | ✔ |   |   |   |   |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 6 |   |   | ✔ |   |   |   |   |'
- en: '| 7 |   |   |   |   | ✔ |   |   |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 7 |   |   |   |   | ✔ |   |   |'
- en: The multi-label classification problem that had five classes is now a multi-class
    classification problem with seven classes. While the power set problem transformation
    is intuitive, increasing the number of classes is frequently impractical; this
    transformation can produce many new labels that correspond to only a few training
    instances. Furthermore, the classifier can only predict combinations of labels
    that were seen in the training data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 具有五个类别的多标签分类问题现在变成了一个具有七个类别的多类分类问题。虽然幂集问题转换直观易懂，但增加类别的数量通常是不可行的；此转换可能会生成许多新标签，而这些标签仅对应少数训练实例。此外，分类器只能预测在训练数据中出现过的标签组合。
- en: '|   | Category |   | Category |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|   | 类别 |   | 类别 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Instance** | **Local** | **¬Local** | **Instance** | **Business** | **¬Business**
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| **实例** | **本地** | **非本地** | **实例** | **商业** | **非商业** |'
- en: '| 1 | ✔ |   | 1 |   | ✔ |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 1 | ✔ |   | 1 |   | ✔ |'
- en: '| 2 | ✔ |   | 2 | ✔ |   |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 2 | ✔ |   | 2 | ✔ |   |'
- en: '| 3 |   | ✔ | 3 | ✔ |   |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 3 |   | ✔ | 3 | ✔ |   |'
- en: '| 4 |   | ✔ | 4 |   | ✔ |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 4 |   | ✔ | 4 |   | ✔ |'
- en: '| 5 | ✔ |   | 5 |   | ✔ |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 5 | ✔ |   | 5 |   | ✔ |'
- en: '| 6 |   | ✔ | 6 | ✔ |   |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 6 |   | ✔ | 6 | ✔ |   |'
- en: '| 7 |   | ✔ | 7 |   | ✔ |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 7 |   | ✔ | 7 |   | ✔ |'
- en: '|   |   |   |   |   |   |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|   |   |   |   |   |   |'
- en: '|   | **Category** |   | **Category** |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|   | **类别** |   | **类别** |'
- en: '| Instance | US | ¬US | Instance | US | ¬US |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 实例 | 美国 | 非美国 | 实例 | 美国 | 非美国 |'
- en: '| 1 | ✔ |   | 1 | ✔ |   |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 1 | ✔ |   | 1 | ✔ |   |'
- en: '| 2 | ✔ |   | 2 | ✔ |   |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 2 | ✔ |   | 2 | ✔ |   |'
- en: '| 3 |   | ✔ | 3 |   | ✔ |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 3 |   | ✔ | 3 |   | ✔ |'
- en: '| 4 |   | ✔ | 4 |   | ✔ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 4 |   | ✔ | 4 |   | ✔ |'
- en: '| 5 |   | ✔ | 5 |   | ✔ |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 5 |   | ✔ | 5 |   | ✔ |'
- en: '| 6 |   | ✔ | 6 |   | ✔ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 6 |   | ✔ | 6 |   | ✔ |'
- en: '| 7 | ✔ |   | 7 | ✔ |   |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 7 | ✔ |   | 7 | ✔ |   |'
- en: '|   |   |   |   |   |   |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|   |   |   |   |   |   |'
- en: '|   | **Category** |   | **Category** |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|   | **类别** |   | **类别** |'
- en: '| Instance | Sci. and Tech. | ¬Sci. and Tech. | Instance | Sports | ¬Sports
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 实例 | 科学与技术 | 非科学与技术 | 实例 | 体育 | 非体育 |'
- en: '| 1 |   | ✔ | 1 |   | ✔ |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 1 |   | ✔ | 1 |   | ✔ |'
- en: '| 2 |   | ✔ | 2 |   | ✔ |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 2 |   | ✔ | 2 |   | ✔ |'
- en: '| 3 | ✔ |   | 3 |   | ✔ |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 3 | ✔ |   | 3 |   | ✔ |'
- en: '| 4 |   | ✔ | 4 | ✔ |   |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 4 |   | ✔ | 4 | ✔ |   |'
- en: '| 5 |   | ✔ | 5 |   | ✔ |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 5 |   | ✔ | 5 |   | ✔ |'
- en: '| 6 |   | ✔ | 6 |   | ✔ |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 6 |   | ✔ | 6 |   | ✔ |'
- en: A second problem transformation is to train one binary classifier for each of
    the labels in the training set. Each classifier predicts whether or not the instance
    belongs to one label. Our example would require five binary classifiers; the first
    classifier would predict whether or not an instance should be classified as `Local`,
    the second classifier would predict whether or not an instance should be classified
    as `US`, and so on. The final prediction is the union of the predictions from
    all of the binary classifiers. The transformed training data is shown in the previous
    figure. This problem transformation ensures that the single-label problems will
    have the same number of training examples as the multilabel problem, but ignores
    relationships between the labels.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种问题转换是为训练集中的每个标签训练一个二分类器。每个分类器预测实例是否属于某个标签。我们的示例需要五个二分类器；第一个分类器预测实例是否应该被分类为`Local`，第二个分类器预测实例是否应该被分类为`US`，依此类推。最终预测是所有二分类器预测的联合结果。转换后的训练数据如上图所示。此问题转换确保单标签问题将拥有与多标签问题相同数量的训练样本，但忽略了标签之间的关系。
- en: Multi-label classification performance metrics
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多标签分类性能评估指标
- en: 'Multi-label classification problems must be assessed using different performance
    measures than single-label classification problems. Two of the most common performance
    metrics are **Hamming loss** and **Jaccard similarity**. Hamming loss is the average
    fraction of incorrect labels. Note that Hamming loss is a loss function, and that
    the perfect score is zero. Jaccard similarity, or the Jaccard index, is the size
    of the intersection of the predicted labels and the true labels divided by the
    size of the union of the predicted and true labels. It ranges from zero to one,
    and one is the perfect score. Jaccard similarity is calculated by the following
    equation:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类问题必须使用与单标签分类问题不同的性能评估指标进行评估。最常见的两种性能指标是**汉明损失**和**贾卡尔相似度**。汉明损失是错误标签的平均比例。需要注意的是，汉明损失是一个损失函数，完美的得分是零。贾卡尔相似度，或称贾卡尔指数，是预测标签与真实标签交集的大小除以预测标签与真实标签并集的大小。它的取值范围从零到一，得分为一时表示完美。贾卡尔相似度的计算公式如下：
- en: '![Multi-label classification performance metrics](img/8365OS_04_13.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![多标签分类性能评估指标](img/8365OS_04_13.jpg)'
- en: '[PRE24]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we discussed generalized linear models, which extend ordinary
    linear regression to support response variables with non-normal distributions.
    Generalized linear models use a link function to relate a linear combination of
    the explanatory variables to the response variable; unlike ordinary linear regression,
    the relationship does not need to be linear. In particular, we examined the logistic
    link function, a sigmoid function that returns a value between zero and one for
    any real number.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们讨论了广义线性模型，它将普通线性回归扩展，以支持具有非正态分布的响应变量。广义线性模型使用连接函数将解释变量的线性组合与响应变量关联起来；与普通线性回归不同，关系不必是线性的。特别地，我们考察了逻辑连接函数，这是一种sigmoid函数，它对任何实数值返回一个介于零和一之间的值。
- en: We discussed logistic regression, a generalized linear model that uses the logistic
    link function to relate explanatory variables to a Bernoulli-distributed response
    variable. Logistic regression can be used for binary classification, a task in
    which an instance must be assigned to one of the two classes; we used logistic
    regression to classify spam and ham SMS messages. We then discussed multi-class
    classification, a task in which each instance must be assigned one label from
    a set of labels. We used the one-vs.-all strategy to classify the sentiments of
    movie reviews. Finally, we discussed multi-label classification, in which instances
    must be assigned a subset of a set of labels. Having completed our discussion
    of regression and classification with generalized linear models, we will introduce
    a non-linear model for regression and classification called the decision tree
    in the next chapter.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了逻辑回归，它是一种广义线性模型，使用逻辑连接函数将解释变量与伯努利分布的响应变量关联起来。逻辑回归可用于二分类任务，在该任务中，实例必须被分配到两个类别中的一个；我们使用逻辑回归来分类垃圾短信和正常短信。接着我们讨论了多类分类任务，在此任务中，每个实例必须从标签集合中分配一个标签。我们使用一对多策略来分类电影评论的情感。最后，我们讨论了多标签分类任务，其中实例必须被分配到标签集合的子集。完成了广义线性模型的回归与分类讨论后，我们将在下一章介绍一种非线性模型——决策树，用于回归和分类。
