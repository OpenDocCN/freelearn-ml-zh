- en: Feature Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征学习
- en: In our final chapter, where we will be exploring feature engineering techniques,
    we will be taking a look at what is likely the most powerful feature engineering
    tool at our disposal. Feature learning algorithms are able to take in cleaned
    data (yes, you still need to do some work) and create brand-new features by exploiting
    latent structures within data. If all of this sounds familiar, that is because
    this is the description that we used in the previous chapter for feature transformations.
    The differences between these two families of algorithms are in the *parametric*
    assumptions that they make when attempting to create new features.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的最后一章中，我们将探讨特征工程技术，我们将查看我们可能拥有的最强大的特征工程工具。特征学习算法能够接受清洗后的数据（是的，你仍然需要做一些工作）并利用数据中的潜在结构创建全新的特征。如果这一切听起来很熟悉，那是因为我们在上一章中用于特征转换的描述。这两个算法家族之间的区别在于它们在尝试创建新特征时所做的**参数**假设。
- en: 'We will be covering the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Parametric assumptions of data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的参数假设
- en: Restricted Boltzmann Machines
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制性玻尔兹曼机
- en: The BernoulliRBM
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伯努利RBM
- en: Extracting RBM components from MNIST
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从MNIST中提取RBM组件
- en: Using RBMs in a machine learning pipeline
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习管道中使用RBMs
- en: Learning text features—word vectorization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习文本特征——词向量化
- en: Parametric assumptions of data
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据的参数假设
- en: When we say **parametric assumptions**, we are referring to base assumptions
    that algorithms make about the *shape* of the data. In the previous chapter, while
    exploring **principal component analysis** (**PCA**), we discovered that the end
    result of the algorithm produced components that we could use to transform data
    through a single matrix multiplication. The assumption that we were making was
    that the original data took on a shape that could be decomposed and represented
    by a single linear transformation (the matrix operation). But what if that is
    not true? What if PCA is unable to extract *useful* features from the original
    dataset? Algorithms such as PCA and **linear discriminate analysis** (**LDA**)
    will always be able to find features, but they may not be useful at all. Moreover,
    these algorithms rely on a predetermined equation and will always output the same
    features each and every time they are run. This is why we consider both LDA and
    PCA as being *linear transformations.*
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提到**参数假设**时，我们指的是算法对数据**形状**的基本假设。在上一章中，当我们探索**主成分分析**（PCA）时，我们发现算法的最终结果产生了我们可以用来通过单次矩阵乘法转换数据的组件。我们所做的假设是原始数据具有可以被分解并由单个线性变换（矩阵运算）表示的形状。但如果这不是真的呢？如果PCA无法从原始数据集中提取**有用**的特征呢？像PCA和**线性判别分析**（LDA）这样的算法总是能够找到特征，但它们可能根本无用。此外，这些算法依赖于预定的方程，并且每次运行时都会输出相同的特征。这就是为什么我们将LDA和PCA都视为**线性变换**。
- en: Feature learning algorithms attempt to solve this issue by removing that parametric
    assumption. They do not make any assumptions about the shape of the incoming data
    and rely on *stochastic learning*. This means that, instead of throwing the same
    equation at the matrix of data every time, they will attempt to figure out the
    best features to extract by looking at the data points over and over again (in
    epochs) and converge onto a solution (potentially different ones at runtime).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 特征学习算法试图通过去除那个参数假设来解决这个问题。它们不对输入数据的形状做出任何假设，并依赖于**随机学习**。这意味着，它们不会每次都将相同的方程应用于数据矩阵，而是会通过反复查看数据点（在时代中）来尝试找出最佳的特征提取方法，并收敛到一个解决方案（在运行时可能是不同的）。
- en: 'For more information on how stochastic learning (and stochastic gradient descent)
    works, please refer to the *Principles of Data Science*, by Sinan Ozdemir at:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 关于随机学习（以及随机梯度下降）的工作原理的更多信息，请参阅Sinan Ozdemir所著的《数据科学的原理》：
- en: '[https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science](https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[数据科学的原理](https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science)'
- en: This allows feature learning algorithms to bypass the parametric assumption
    made by algorithms such as PCA and LDA and opens us up to solve much more difficult
    questions than we could previously in this text. Such a complex idea (bypassing
    parametric assumptions) requires the use of complex algorithms. *Deep learning*
    algorithms are the choice of many data scientists and machine learning to learn
    new features from raw data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得特征学习算法可以绕过PCA和LDA等算法所做的参数假设，使我们能够解决比之前在文本中能解决的更难的问题。这样一个复杂的思想（绕过参数假设）需要使用复杂的算法。*深度学习*算法是许多数据科学家和机器学习专家从原始数据中学习新特征的选择。
- en: 'We will assume that the reader has a basic familiarity with the neural network
    architecture in order to focus on applications of these architectures for feature
    learning. The following table summarizes the basic differences between feature
    learning and transformation:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将假设读者对神经网络架构有基本的了解，以便专注于这些架构在特征学习方面的应用。以下表格总结了特征学习和转换之间的基本差异：
- en: '|  | **Parametric?** | **Simple to use?** | **Creates new feature set?** |
    **Deep learning?** |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|  | **参数化？** | **易于使用？** | **创建新的特征集？** | **深度学习？** |'
- en: '| Feature transformation algorithms | Yes | Yes | Yes | No |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 特征转换算法 | 是 | 是 | 是 | 否 |'
- en: '| Feature learning algorithms | No | No (usually) | Yes | Yes (usually) |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 特征学习算法 | 否 | 否（通常） | 是 | 是（通常） |'
- en: 'The fact that both feature learning and feature transformation algorithms create
    new feature sets means that we regard both of them as being under the umbrella
    of *feature extraction. *The following figure shows this relationship:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 事实是，特征学习和特征转换算法都创建了新的特征集，这意味着我们将它们都视为属于*特征提取*的范畴。以下图显示了这种关系：
- en: '![](img/4587fa19-778b-42ac-b14d-65c784ba9a66.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4587fa19-778b-42ac-b14d-65c784ba9a66.png)'
- en: Feature extraction as a superset of feature learning and feature transformation.
    Both families of algorithms work to exploit latent structure in order to transform
    raw data into a new feature set
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取作为特征学习和特征转换的超集。这两个算法家族都致力于利用潜在结构，将原始数据转换成新的特征集
- en: Both **feature learning** and **feature transformation** fall under the category
    of feature extraction as they are both trying to create a new feature set from
    the latent structure of raw data. The methods in which they are allowed to work,
    though, are the main differentiators.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于**特征学习**和**特征转换**都属于特征提取的范畴，因为它们都试图从原始数据的潜在结构中创建新的特征集。尽管如此，它们允许工作的方法却是主要的区别点。
- en: Non-parametric fallacy
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非参数谬误
- en: It is important to mention that a model being non-parametric doesn't mean that
    there are no assumptions at all made by the model during training.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 需要强调的是，一个模型是非参数的并不意味着在训练过程中模型没有任何假设。
- en: While the algorithms that we will be introducing in this chapter forgo the assumption
    on the shape of the data, they still may make assumptions on other aspects of
    the data, for example, the values of the cells.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们将在本章中介绍的一些算法放弃了关于数据形状的假设，但它们仍然可能在数据的其他方面做出假设，例如，单元格的值。
- en: The algorithms of this chapter
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本章的算法
- en: 'In this chapter, we will focus on two feature learning areas:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注两个特征学习领域：
- en: '**Restricted Boltzmann Machines** (**RBM**): A simple deep learning architecture
    that is set up to learn a set number of new dimensions based on a probabilistic
    model that data follows. These machines are in fact a family of algorithms with
    only one implemented in scikit-learn. The **BernoulliRBM **may be a non-parametric
    feature learner; however, as the name suggests, some expectations are set as to
    the values of the cells of the dataset.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**受限玻尔兹曼机**（**RBM**）：一个简单的深度学习架构，它基于数据遵循的概率模型来学习一定数量的新维度。这些机器实际上是一系列算法，其中只有一个是scikit-learn中实现的。**伯努利RBM**可能是一个非参数特征学习器；然而，正如其名称所暗示的，对数据集单元格的值有一些期望。'
- en: '**Word embeddings**: Likely one of the biggest contributors to the recent deep
    learning-fueled advancements of natural language processing/understanding/generation
    is the ability to project strings (words and phrases) into an n-dimensional feature
    set in order to grasp context and minute detail in wording. We will use the `gensim` Python
    package to prepare our own word embeddings and then use pre-trained word embeddings
    to see some examples of how these word embeddings can be used to enhance the way
    we interact with text.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词嵌入**：自然语言处理/理解/生成领域近年来由深度学习推动的进步中，最大的贡献者之一可能是将字符串（单词和短语）投影到n维特征集中，以便把握语境和措辞的细微差别。我们将使用`gensim`Python包来准备我们自己的词嵌入，然后使用预训练的词嵌入来查看这些词嵌入如何被用来增强我们与文本的交互方式。'
- en: All of these examples have something in common. They all involve learning brand
    new features from raw data. They then use these new features to enhance the way
    that they interact with data. For the latter two examples, we will have to move
    away from scikit-learn as these more advanced techniques are not (yet) implemented
    in the latest versions. Instead, we will see examples of deep learning neural
    architectures implemented in TensorFlow and Keras.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些例子都有共同之处。它们都涉及从原始数据中学习全新的特征。然后，它们使用这些新特征来增强与数据的交互方式。对于后两个例子，我们将不得不离开scikit-learn，因为这些更高级的技术（尚未）在最新版本中实现。相反，我们将看到TensorFlow和Keras中实现的深度学习神经网络架构的例子。
- en: For all of these techniques, we will be focusing less on the very low-level
    inner workings of the models, and more on how they work to interpret data. We
    will go in order and start with the only algorithm that has a scikit-learn implementation,
    the restricted Boltzmann machine family of algorithms.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有这些技术，我们将更多地关注模型如何解释数据，而不是非常低级的内部工作原理。我们将按顺序进行，并从唯一一个有scikit-learn实现的算法——限制性玻尔兹曼机算法系列开始。
- en: Restricted Boltzmann Machines
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制性玻尔兹曼机
- en: RBMs are a family of unsupervised feature learning algorithms that use probabilistic
    models to learn new features. Like PCA and LDA, we can use RBMs to extract a new
    feature set from raw data and use them to enhance machine learning pipelines.
    The features that are extracted by RBMs tend to work best when followed by linear
    models such as linear regression, logistic regression, perceptron's, and so on.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: RBMs是一族无监督特征学习算法，使用概率模型来学习新特征。像PCA和LDA一样，我们可以使用RBMs从原始数据中提取新的特征集，并使用它们来增强机器学习流程。RBMs提取的特征通常在跟随线性模型（如线性回归、逻辑回归、感知器等）时效果最佳。
- en: The unsupervised nature of RBMs is important as they are more similar to PCA
    algorithms than they are to LDA. They do not require a ground-truth label for
    data points to extract new features. This makes them useful in a wider variety
    of machine learning problems.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: RBMs的无监督特性非常重要，因为它们与PCA算法更相似，而不是与LDA相似。它们不需要为数据点提取新特征而提供真实标签。这使得它们在更广泛的机器学习问题中非常有用。
- en: 'Conceptually, RBMs are shallow (two-layer) neural networks. They are thought
    to be the building blocks of a class of algorithms called **Deep Belief Networks**
    (**DBN**). Keeping with standard terminology, there is a visible layer (the first
    layer), followed by a hidden layer (the second layer). These are the only two
    layers of the network:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，RBMs是浅层（两层）神经网络。它们被认为是称为**深度信念网络**（**DBN**）的一类算法的构建块。遵循标准术语，有一个可见层（第一层），然后是一个隐藏层（第二层）。这些是网络中唯一的两个层：
- en: '![](img/31045c70-ef67-4429-9f70-4dc418d6373c.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/31045c70-ef67-4429-9f70-4dc418d6373c.png)'
- en: The setup for a restricted Boltzmann Machine. The circles represent nodes in
    the graph
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 限制性玻尔兹曼机的设置。圆圈代表图中的节点
- en: Like any neural network, we have nodes in our two layers. The first visible
    layer of the network has as many layers as the input feature dimension. In our
    upcoming example, we will be working with 28 x 28 images necessitating 784 (28
    x 28) nodes in our input layer. The number of nodes in the hidden layer is a human-chosen
    number and represents the number of features that we wish to learn.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何神经网络一样，我们的两个层中都有节点。网络的第一层可见层与输入特征维度一样多。在我们即将到来的例子中，我们将处理28 x 28的图像，这需要输入层中有784（28
    x 28）个节点。隐藏层中的节点数是一个人为选择的数字，代表我们希望学习的特征数量。
- en: Not necessarily dimension reduction
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不一定是降维
- en: In PCA and LDA, we had severe limits to the number of components we were allowed
    to extract. For PCA, we were capped by the number of original features (we could
    only use less than or equal to the number of original columns), while LDA enforced
    the much stricter imposition that caps the number of extracted features to the
    number of categories in the ground truth minus one.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在主成分分析（PCA）和线性判别分析（LDA）中，我们提取的组件数量受到了严格的限制。对于PCA，我们受限于原始特征的数量（我们只能使用原始列数或更少的数量），而LDA则实施了更为严格的限制，即提取的特征数量不得超过真实数据集中类别数减一。
- en: The only restriction on the number of features RBMs are allowed to learn is
    that they are limited by the computation power of the computer running the network
    and human interpretation. RBMs can learn fewer or *more* features than we originally
    began with. The exact number of features to learn is up to the problem and can
    be gridsearched.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: RBM允许学习的特征数量的唯一限制是，它们受限于运行网络的计算机的计算能力和人类的解释。RBM可以学习比我们最初开始时更多的或更少的特征。要学习的确切特征数量取决于问题，并且可以通过网格搜索来确定。
- en: The graph of a Restricted Boltzmann Machine
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制性玻尔兹曼机（RBM）的图
- en: 'So far, we have seen the visible and hidden layers of RBMs, but we have not
    yet seen how they learn features. Each of the visible layer''s nodes take in a
    single feature from the dataset to be learned from. This data is then passed from
    the visible layer to the hidden layer through weights and biases:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了RBM的可见层和隐藏层，但我们还没有看到它们是如何学习特征的。可见层中的每个节点都从要学习的数据集中接收一个特征。然后，通过权重和偏置，这些数据从可见层传递到隐藏层：
- en: '![](img/bfe9aba3-88cd-4492-972b-e9df0618da42.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bfe9aba3-88cd-4492-972b-e9df0618da42.png)'
- en: This visualization of an RBM shows the movement of a single data point through
    the graph through a single hidden node
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个RBM的可视化显示了单个数据点通过单个隐藏节点在图中的移动
- en: The preceding visualization of an RBM shows the movement of a single data point
    through the graph and through a single hidden node. The visible layer has four
    nodes, representing the four columns of the original data. Each arrow represents
    a single feature of the data point moving through the four visible nodes in the
    first layer of the RBM. Each of the feature values is multiplied by a weight associated
    to that feature and are added up together. This calculation can also be summed
    up by a dot product between an input vector of data and a weight vector. The resulting
    weighted sum of the data is added to a bias variable and sent through an activation
    function (sigmoidal is popular). The result is stored in a variable called `a`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的RBM可视化显示了单个数据点通过图和单个隐藏节点的移动。可见层有四个节点，代表原始数据的四列。每条箭头代表数据点的单个特征通过RBM第一层的四个可见节点移动。每个特征值都乘以与该特征关联的权重，并加在一起。这个计算也可以通过输入数据向量和权重向量之间的点积来总结。数据的结果加权总和加到一个偏置变量上，并通过一个激活函数（如Sigmoid函数）传递。结果存储在一个名为`a`的变量中。
- en: 'As an example in Python, this code shows how a single data point (`inputs`)
    is multiplied by our `weights` vector and combined with the `bias` variable to
    create the activated variable, `a`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以Python为例，以下代码展示了单个数据点（`inputs`）是如何乘以我们的`weights`向量，并与`bias`变量结合以创建激活变量`a`：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In a real RBM, each of the visible nodes is connected to each of the hidden
    nodes, and it looks something like this:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在真实的RBM中，每个可见节点都与每个隐藏节点相连，其结构看起来像这样：
- en: '![](img/dd0adfcc-12be-4d6c-91f0-8834abe52bc6.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd0adfcc-12be-4d6c-91f0-8834abe52bc6.png)'
- en: Because inputs from each visible node are passed to every single hidden node,
    an RBM can be defined as a **symmetrical bipartite graph**. The symmetrical part
    comes from the fact that the visible nodes are all connected with each hidden
    node. Bipartite means it has two parts (layers).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个可见节点的输入都传递给每个隐藏节点，因此RBM可以被定义为一种**对称的二分图**。对称性源于可见节点与每个隐藏节点都相连。二分图意味着它由两部分（层）组成。
- en: The restriction of a Boltzmann Machine
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玻尔兹曼机的限制
- en: With our two layers of visible and hidden nodes, we have seen the connection
    between the layers (inter-layer connections), but we haven't seen any connections
    between nodes in the same layer (intra-layer connections). That is because there
    aren't any. The restriction in the RBM is that we do not allow for any intra-layer
    communication. This lets nodes independently create weights and biases that end
    up being (hopefully) independent features for our data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了两个可见和隐藏节点层的层间连接（层间连接），但我们还没有看到同一层中节点之间的连接（层内连接）。这是因为没有这样的连接。RBM的限制是我们不允许任何层内通信。这允许节点独立创建权重和偏置，最终成为（希望是）我们数据独立的特征。
- en: Reconstructing the data
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据重建
- en: In this forward pass of the network, we can see how data goes forward through
    the network (from the visible layer to the hidden layer), but that doesn't explain
    how the RBM is able to learn new features from our data without ground truths.
    This is done through multiple forward and backward passes through the network
    between our visible and hidden layer.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个网络的前向传递中，我们可以看到数据是如何通过网络（从可见层到隐藏层）前向传递的，但这并不能解释RBM是如何从我们的数据中学习新特征的，而没有地面实况。这是通过在可见层和隐藏层之间通过网络进行多次正向和反向传递来实现的。
- en: 'In the reconstruction phase, we switch the network around and let the hidden
    layer become the input layer and let it feed our activation variables (`a`) backwards
    into the visible layer using the same weights, but a new set of biases. The activated
    variables that we calculated during the forward pass are then used to reconstruct
    the original input vectors. The following visualization shows us how activations
    are fed backwards through our graph using the same weights and different biases:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在重建阶段，我们改变网络的结构，让隐藏层成为输入层，并让它使用相同的权重但新的偏置将激活变量（`a`）反向传递到可见层。在正向传递过程中计算出的激活变量随后被用来重建原始输入向量。以下可视化展示了如何使用相同的权重和不同的偏置将激活信号反向传递通过我们的图：
- en: '![](img/20e88959-47cd-4c59-a7ea-f20841310bdf.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/20e88959-47cd-4c59-a7ea-f20841310bdf.png)'
- en: This becomes the network's way of evaluating itself. By passing the activations
    backwards through the network and obtaining an approximation of the original input,
    the network can adjust the weights in order to make the approximations closer
    to the original input. Towards the beginning of training, because the weights
    are randomly initialized (this is standard practice), the approximations will
    likely be very far off. Backpropagation through the network, which occurs in the
    same direction as our forward pass (confusing, we know), then adjusts the weights
    to minimize the distance between original input and approximations. This process
    is then repeated until the approximations are as close to the original input as
    possible. The number of times this back and forth process occurs is called the
    number of **iterations**.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这成为了网络自我评估的方式。通过将激活信号反向传递通过网络并获得原始输入的近似值，网络可以调整权重以使近似值更接近原始输入。在训练的初期，由于权重是随机初始化的（这是标准做法），近似值可能会非常不准确。反向传播通过网络，与我们的正向传递方向相同（我们知道这很令人困惑），然后调整权重以最小化原始输入和近似值之间的距离。这个过程然后重复进行，直到近似值尽可能接近原始输入。这种来回传递过程发生的次数被称为迭代次数。
- en: The end result of this process is a network that has an *alter-ego* for each
    data point. To transform data, we simply pass it through the network and retrieve
    the activation variables and call those the new features. This process is a type
    of *generative learning* that attempts to learn a probability distribution that
    generated the original data and exploit knowledge to give us a new feature set
    of our raw data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的最终结果是，网络为每个数据点都有一个“替身”。要转换数据，我们只需将其通过网络，检索激活变量，并将这些称为新特征。这个过程是一种类型的生成学习，它试图学习生成原始数据的概率分布，并利用知识来为我们提供原始数据的新的特征集。
- en: For example, if we were given an image of a digit and asked to classify which
    digit (0-9) the image was of, the forward pass of the network asks the question,
    given these pixels, what digit should I expect? On the backwards pass, the network
    is asking given a digit, what pixels should I expect? This is called a **joint
    probability** and it is the simultaneous probability of *y *given *x *and *x *given *y,*
    and it is expressed as the shared weights between the two layers of our network.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们被给了一个数字的图片并要求我们分类这个图片是哪个数字（0-9），网络的正向传播会问这样的问题：给定这些像素，我应该期望哪个数字？在反向传播中，网络会问给定一个数字，我应该期望哪些像素？这被称为**联合概率**，它是给定*x*时*y*的概率和给定*y*时*x*的概率的联合，它通过我们网络中两层之间的共享权重来表示。
- en: Let's introduce our new dataset and let it elucidate the usefulness of RBMs
    in feature learning.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们引入我们的新数据集，并让它阐明RBM在特征学习中的有用性。
- en: MNIST dataset
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST数据集
- en: The `MNIST` dataset consists of 6,000 images of handwritten digits between zero
    and nine and a ground-truth label to learn from. It is not unlike most of the
    other datasets that we have been working with in that we are attempting to fit
    a machine learning model to classify a response variable given a set of data points.
    The main difference here is that we are working with very low-level features as
    opposed to more interpretable features. Each data point will consist of 784 features
    (pixel values in a grey-scale image).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`MNIST`数据集包含6000张0到9的手写数字图片以及一个用于学习的真实标签。它与其他我们一直在使用的其他数据集类似，我们试图将机器学习模型拟合到响应变量，给定一组数据点。这里的主要区别在于我们处理的是非常低级的特征，而不是更可解释的特征。每个数据点将包含784个特征（灰度图像中的像素值）。'
- en: 'Let''s begin with our imports:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从导入开始：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The new import is the BernoulliRBM, which is the only RBM implementation in
    scikit-learn as of now. As the name suggests, we will have to do a small amount
    of preprocessing to ensure that our data complies with the assumptions required.
    Let''s import our dataset directly into a NumPy array:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 新的导入是BernoulliRBM，这是目前scikit-learn中唯一的RBM实现。正如其名所示，我们将需要进行一些预处理以确保我们的数据符合所需的假设。让我们直接将我们的数据集导入到NumPy数组中：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can verify the number of rows and columns that we are working with:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以验证我们正在处理的行数和列数：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The 785 is comprised of 784 pixels and a single response column in the beginning
    (first column). Every column besides the response column holds a value between
    zero and 255 representing pixel intensity, where zero means a white background
    and 255 means a fully black pixel. We can extract the `X` and `y` variables from
    the data by separating the first column from the rest of the data:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 785由784个像素和一个起始处的单个响应列组成。除了响应列之外，每一列都持有介于0到255之间的值，代表像素强度，其中0表示白色背景，255表示全黑像素。我们可以通过将第一列与其他数据分开来从数据中提取`X`和`y`变量：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If we take a look at the first image, we will see what we are working with:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们看一下第一张图片，我们会看到我们正在处理的内容：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The plot is as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图表如下：
- en: '![](img/396ff5d9-160d-431d-88cd-0ea40f3db828.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/396ff5d9-160d-431d-88cd-0ea40f3db828.png)'
- en: Looking good. Because the scikit-learn implementation of Restricted Boltzmann
    Machines will not allow for values outside of the range of 0-1, we will have to
    do a bit of preprocessing work.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错。因为scikit-learn对受限玻尔兹曼机的实现不允许值超出0-1的范围，我们不得不做一些预处理工作。
- en: The BernoulliRBM
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BernoulliRBM
- en: 'The only scikit-learn implemented version of a Restricted Boltzmann Machine is
    called **BernoulliRBM** because it imposes a constraint on the type of probability
    distribution it can learn. The Bernoulli distribution allows for data values to
    be between zero and one. The scikit-learn documentation states that the model
    *assumes the inputs are either binary values or values between zero and one*.
    This is done to represent the fact that the node values represent a probability
    that the node is activated or not. It allows for quicker learning of feature sets.
    To account for this, we will alter our dataset to account for only hardcoded white/black
    pixel intensities. By doing so, every cell value will either be zero or one (white
    or black) to make learning more robust. We will accomplish this in two steps:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 限制玻尔兹曼机（RBM）的唯一scikit-learn实现版本被称为**BernoulliRBM**，因为它对其可以学习的概率分布类型施加了约束。伯努利分布允许数据值在零到一之间。scikit-learn文档指出，该模型*假设输入是二进制值或零到一之间的值*。这是为了表示节点值代表节点被激活或未激活的概率。这允许更快地学习特征集。为了解决这个问题，我们将调整我们的数据集，只考虑硬编码的白色/黑色像素强度。通过这样做，每个单元格的值要么是零要么是一（白色或黑色），以使学习更加稳健。我们将通过以下两个步骤来完成这项工作：
- en: We will scale the values of the pixels to be between zero and one
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将把像素值的范围缩放到零到一之间
- en: We will change the pixel values in place to be true if the value is over `0.5`,
    and false otherwise
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将就地更改像素值，如果值超过`0.5`则为真，否则为假
- en: 'Let''s start by scaling the pixel values to be between 0 and 1:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先对像素值进行缩放，使其介于0和1之间：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s take a look at the same number five digit, as we did previously, with
    our newly altered pixels:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看之前看到的相同的数字5，用我们新更改的像素：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The plot is as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图如下：
- en: '![](img/c8c8f0b8-57a4-43e1-87ed-bb6374c77d48.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8c8f0b8-57a4-43e1-87ed-bb6374c77d48.png)'
- en: We can see that the fuzziness of the image has disappeared and we are left with
    a very crisp digit to classify with. Let's try now to extract features from our
    dataset of digits.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到图像的模糊性已经消失，我们剩下的是一个非常清晰的数字来进行分类。现在让我们尝试从我们的数字数据集中提取特征。
- en: Extracting PCA components from MNIST
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从MNIST中提取PCA组件
- en: Before we move to our RBM, let's take a look at what happens when we apply a
    PCA to our dataset. Like we did in the last chapter, we will take our features
    (784 pixels that are either on or off) and apply an eigenvalue decomposition to
    the matrix to extract *eigendigits* from the dataset.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们转向我们的RBM之前，让我们看看当我们对数据集应用PCA时会发生什么。就像我们在上一章做的那样，我们将取我们的特征（784个像素，要么开启要么关闭）并对矩阵进行特征值分解，从数据集中提取*特征数字*。
- en: 'Let''s take 100 components from the possible 784 and plot the components to
    see what the extracted features look like. We will do this by importing our PCA
    module, fitting it to our data with 100 components, and creating a matplotlib
    gallery to display the top 100 components available to us:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从可能的784个组件中取出100个，并绘制这些组件以查看提取的特征是什么样的。我们将通过导入我们的PCA模块，用100个组件拟合我们的数据，并创建一个matplotlib图库来显示我们可用的前100个组件：
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following is the plot of the preceding code block:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图是前面代码块的图：
- en: '![](img/4e10bb9a-dc1c-492f-90ba-2c523541c708.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e10bb9a-dc1c-492f-90ba-2c523541c708.png)'
- en: This gallery of images is showing us what the eigenvalues of the covariance
    matrix look like when reshaped to the same dimensions as the original images.
    This is an example of what extracted components look like when we focus our algorithms
    on a dataset of images. It is quite interesting to take a sneak peek into how
    the PCA components are attempting to grab linear transformations from the dataset.
    Each component is attempting to understand a certain "aspect" of the images that
    will translate into interpretable knowledge. For example, the first (and most
    important) eigen-image is likely capturing an images 0-quality that is, how like
    a 0 the digit looks.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图像图库向我们展示了协方差矩阵的特征值在重塑为与原始图像相同维度时的样子。这是当我们将算法集中在图像数据集上时，提取的组件的一个例子。窥视PCA组件如何尝试从数据集中获取线性变换是非常有趣的。每个组件都试图理解图像的某个“方面”，这将转化为可解释的知识。例如，第一个（也是最重要的）特征图像很可能是捕捉到图像的0质量，即数字看起来像0的程度。
- en: It also is evident that the first ten components seem to retain some of the
    shape of the digits and after that, they appear to start devolving into what looks
    like nonsensical images. By the end of the gallery, we appear to be looking at
    random assortments of black and white pixels swirling around. This is probably
    because PCA (and also LDA) are parametric transformations and they are limited
    in the amount of information they can extract from complex datasets like images.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，很明显，前十个组件似乎保留了一些数字的形状，之后，它们似乎开始退化成看似无意义的图像。到画廊结束时，我们似乎在观察黑白像素的随机组合旋转。这可能是由于PCA（以及LDA）是参数化变换，它们在从复杂的数据集（如图像）中提取信息的能力上有限。
- en: 'If we take a look to see how much variance the first 30 components are explaining,
    we would see that they are able to capture the majority of the information:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看前30个组件解释了多少方差，我们会发现它们能够捕捉到大部分信息：
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This tells us that the first few dozen components are doing a good job at capturing
    the essence of the data, but after that, the components are likely not adding
    too much.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，前几十个组件在捕捉数据的本质方面做得很好，但之后，组件可能没有增加太多。
- en: 'This can be further seen in a scree plot showing us the cumulative explained
    variance for our PCA components:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过一个显示我们PCA组件累积解释方差的scree图进一步看出：
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following is the plot of the scree plot where the number of PCA components
    are on the *x* axis and the amount of cumulative variance explained lives on the
    *y* axis:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是特征值累积图，其中PCA组件的数量位于*x*轴上，而解释的累积方差则位于*y*轴上：
- en: '![](img/01e3257b-e9f3-430a-ad2a-39f16bcf8401.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/01e3257b-e9f3-430a-ad2a-39f16bcf8401.png)'
- en: 'As we saw in the previous chapter, the transformations made by PCA are done
    through a single linear matrix operation by multiplying the components attribute
    of the PCA module with the data. We will show this again by taking the scikit-learn
    PCA object that we fit to 100 features and using it to transform a single MNIST
    image. We will take that transformed image and compare it the result of multiplying
    the original image with the `components_` attribute of the PCA module:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中看到的，PCA所做的变换是通过乘以PCA模块的组件属性与数据来通过单个线性矩阵操作完成的。我们将通过使用拟合到100个特征的scikit-learn
    PCA对象并使用它来转换单个MNIST图像来再次展示这一点。我们将取那个转换后的图像，并将其与原始图像乘以PCA模块的`components_`属性的结果进行比较：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Extracting RBM components from MNIST
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从MNIST中提取RBM组件
- en: Let's now create our first RBM in scikit-learn. We will start by instantiating
    a module to extract 100 components from our `MNIST` dataset.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将在scikit-learn中创建我们的第一个RBM。我们将首先实例化一个模块，从我们的`MNIST`数据集中提取100个组件。
- en: 'We will also set the verbose parameter to `True` to allow us visibility into
    the training process as well as the `random_state` parameter to `0`. The `random_state`
    parameter is an integer that allows for reproducibility in code. It fixes the
    random number generator and sets the weights and biases *randomly* at the same
    time, every time. We finally let `n_iter` be `20`. This is the number of iterations
    we wish to do, or back and forth passes of the network:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将设置`verbose`参数为`True`，以便我们可以看到训练过程，以及将`random_state`参数设置为`0`。`random_state`参数是一个整数，它允许代码的可重复性。它固定了随机数生成器，并在每次同时随机设置权重和偏差。我们最后将`n_iter`设置为`20`。这是我们希望进行的迭代次数，即网络的来回传递次数：
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Once training is complete; we can explore the end result of the process. RBM
    also has a `components` module, like PCA does:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们可以探索过程的最终结果。RBM也有一个`components`模块，就像PCA一样：
- en: '[PRE13]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can also plot the RBM components that were learned by the module to see
    how they differ from our eigendigits:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制模块学习到的RBM组件，以查看它们与我们自己的特征数字有何不同：
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following is the result of the preceding code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码是上述代码的结果：
- en: '![](img/b0691218-0d6d-463d-94a3-05cdd4ef3c01.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b0691218-0d6d-463d-94a3-05cdd4ef3c01.png)'
- en: These features look very interesting. Where the PCA components became visual
    distortions after a while, the RBM components seem to be extracting various shapes
    and pen strokes with each component. At first glance, it looks like we have repeat
    features (for example, feature 15, 63, 64, and 70). We can do a quick NumPy check
    to see if any of the components are actually repeating, or if they are just very
    similar.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征看起来非常有趣。PCA组件在一段时间后变成了视觉扭曲，而RBM组件似乎每个组件都在提取不同的形状和笔触。乍一看，我们似乎有重复的特征（例如，特征15、63、64和70）。我们可以快速使用NumPy检查是否有任何组件实际上是重复的，或者它们只是非常相似。
- en: 'This code will check to see how many unique elements exist in `rbm.components_`.
    If the resulting shape has 100 elements in it, that means that every component
    of the RBM is in fact different:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将检查`rbm.components_`中存在多少唯一元素。如果结果形状中有100个元素，这意味着RBM的每个组件实际上都是不同的：
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This validates that our components are all unique from one another. We can
    use the RBM to transform data like we can with PCA by utilizing the `transform`
    method within the module:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这验证了我们的组件彼此之间都是独特的。我们可以使用RBM来转换数据，就像我们可以使用PCA一样，通过在模块中使用`transform`方法：
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'And we can also see that these components are **not** used in the same way
    as PCAs are, meaning that a simple matrix multiplication will not yield the same
    transformation as invoking the `transform` method embedded within the module:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到，这些组件并不是像PCA那样被使用的，这意味着简单的矩阵乘法不会产生与调用模块内嵌入的`transform`方法相同的转换：
- en: '[PRE17]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now that we know that we have 100 new features to work with and we've seen them,
    let's see how they interact with our data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了我们有100个新特征可以工作，并且我们已经看到了它们，让我们看看它们如何与我们的数据互动。
- en: 'Let''s start by grabbing the `20` most represented features for the first image
    in our dataset, the digit 5:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从抓取数据集中第一张图像，即数字5的图像中，最常出现的20个特征开始：
- en: '[PRE18]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In this case, we actually have seven features in which the RBM has a full 100%.
    In our graph, this means that passing in these 784 pixels into our visible layers
    lights up nodes 56, 63, 62, 14, 69, 83, and 82 at full capacity. Let''s isolate
    these features:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们实际上有七个特征，其中RBM有100%的覆盖率。在我们的图表中，这意味着将这些784个像素输入到可见层时，节点56、63、62、14、69、83和82会完全点亮。让我们隔离这些特征：
- en: '[PRE19]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We get the following result for the preceding code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了前面代码的以下结果：
- en: '![](img/ae5253eb-ec7b-4787-9a97-02b40badec81.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ae5253eb-ec7b-4787-9a97-02b40badec81.png)'
- en: 'Taking a look at some of these, they make quite a lot of sense. **Component
    45** seems to isolate the top-left corner of the digit **5**, while **Component
    21** seems to grab the bottom loop of the digit. **Component 82** and **Component
    34** seem to grab almost an entire 5 in one go. Let''s see what the bottom of
    the barrel looks like for the number 5 by isolating the bottom 20 features that
    lit up in the RBM graph when these pixels were passed through:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下这些图表中的一些，它们非常有道理。**组件45**似乎隔离了数字**5**的左上角，而**组件21**似乎抓取了数字的底部环。**组件82**和**组件34**似乎一次性抓取了几乎整个数字5。让我们通过隔离这些像素通过RBM图时亮起的底部20个特征，来看看数字5的底部桶状部分是什么样子：
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We get the following plot for the preceding code:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了前面代码的以下图表：
- en: '![](img/e54d08a5-bbb2-4253-934f-e637c13655ce.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e54d08a5-bbb2-4253-934f-e637c13655ce.png)'
- en: '**Component 13**, **Component 4**, **Component 97**, and others seem to be
    trying to reveal different digits and not a 5, so it makes sense that these components
    are not being lit up by this combination of pixel strengths.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**组件13**、**组件4**、**组件97**以及其他组件似乎试图揭示不同的数字而不是5，因此这些组件没有被这种像素强度的组合点亮是有道理的。'
- en: Using RBMs in a machine learning pipeline
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在机器学习管道中使用RBM
- en: 'Of course, we want to see how the RBM performs in our machine learning pipelines
    to not just visualize the workings of the model, but to see concrete results of
    the feature learning. To do this, we will create and run three pipelines:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们想看看RBM在我们的机器学习管道中的表现，不仅是为了可视化模型的工作原理，还要看到特征学习的具体结果。为此，我们将创建并运行三个管道：
- en: A logistic regression model by itself running on the raw pixel strengths
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅在原始像素强度上运行的逻辑回归模型
- en: A logistic regression running on extracted PCA components
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提取的PCA组件上运行的逻辑回归
- en: A logistic regression running on extracted RBM components
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提取的RBM组件上运行的逻辑回归
- en: Each of these pipelines will be grid-searched across a number of components
    (for PCA and RBM) and the `C` parameter for logistic regression. Let's start with
    our simplest pipeline. We will run the raw pixel values through a logistic regression
    to see if the linear model is enough to separate out the digits.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些管道将跨多个组件（对于PCA和RBM）以及逻辑回归的`C`参数进行网格搜索。让我们从最简单的管道开始。我们将运行原始像素值通过逻辑回归，看看线性模型是否足以分离出数字。
- en: Using a linear model on raw pixel values
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在原始像素值上使用线性模型
- en: To begin, we will run the raw pixel values through a logistic regression model
    in order to obtain something of a baseline model. We want to see if utilizing
    PCA or RBM components will allow the same linear classifier to perform better
    or worse. If we can find that the extracted latent features are performing better
    (in terms of accuracy of our linear model) then we can be sure it is the feature
    engineering that we are employing that is enhancing our pipeline, and nothing
    else.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将运行原始像素值通过逻辑回归模型，以获得某种基准模型。我们想看看利用PCA或RBM组件是否能让相同的线性分类器表现更好或更差。如果我们能发现提取的潜在特征表现更好（就线性模型的准确率而言），那么我们可以确信是我们在管道中使用的特征工程增强了我们的管道，而不是其他因素。
- en: 'First we will create our instantiated modules:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建我们的实例化模块：
- en: '[PRE21]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Once we done this, we can fit our module to our raw image data. This will give
    us a rough idea of how the raw pixel data performs in a machine learning pipeline:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成这个步骤，我们就可以将我们的模块拟合到我们的原始图像数据中。这将给我们一个大致的了解，原始像素数据在机器学习管道中的表现：
- en: '[PRE22]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Logistic regression by itself does a decent job at using the raw pixel values
    to identify digits by giving about an **88.75% cross-validated accuracy**.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 仅凭逻辑回归本身就能相当好地使用原始像素值来识别数字，给出了大约**88.75%的交叉验证准确率**。
- en: Using a linear model on extracted PCA components
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在提取的PCA组件上使用线性模型
- en: 'Let''s see if we can add in a PCA component to the pipeline to enhance this
    accuracy. We will begin again by setting up our variables. This time we will need
    to create a scikit-learn pipeline object to house the PCA module as well as our
    linear model. We will keep the same parameters that we used for the linear classifier
    and add new parameters for our PCA. We will attempt to find the optimal number
    of components between 10, 100, and 200 components. Try to take a moment and hypothesize
    which of three will end up being the best (hint, think back to the scree plot
    and explained variance):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们是否可以在管道中添加一个PCA组件来提高这个准确性。我们将再次开始，设置我们的变量。这次我们需要创建一个scikit-learn管道对象，以容纳PCA模块以及我们的线性模型。我们将保持与线性分类器相同的参数，并为我们的PCA添加新参数。我们将尝试在10、100和200个组件之间找到最佳组件数量。试着花点时间，猜测这三个中哪一个最终会是最好的（提示：回想一下之前的scree图和解释方差）：
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can now fit the gridsearch object to our raw image data. Note that the pipeline
    will take care of automatically extracting features from and transforming our
    raw pixel data:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将gridsearch对象拟合到我们的原始图像数据中。请注意，管道将自动提取并转换我们的原始像素数据：
- en: '[PRE24]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We end up with a (slightly better) **88.95% cross-validated accuracy.** If we
    think about it, we should not be surprised that 100 was the best option out of
    10, 100, and 200\. From our brief analysis with the scree plot in a previous section,
    we found that 64% of the data was explained by a mere 30 components, so 10 components
    would definitely not be enough to explain the images well. The scree plot also
    started to level out at around 100 components, meaning that after the 100th component,
    the explained variance was truly not adding much, so 200 was too many components
    to use and would have started to lead to some overfitting. That leaves us with
    100 as being the optimal number of PCA components to use. It should be noted that
    we could go further and attempt some hyper-parameter tuning to find an even more
    optimal number of components, but for now we will leave our pipeline as is and
    move to using RBM components.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到了（略微更好的）**88.95%交叉验证准确率**。如果我们仔细想想，我们不应该对100是10、100和200中的最佳选择感到惊讶。从我们之前章节中的scree图简短分析中，我们发现64%的数据仅由30个组件解释，所以10个组件肯定不足以很好地解释图像。scree图也大约在100个组件时开始平缓，这意味着在100个组件之后，解释方差实际上并没有增加多少，所以200个组件太多，会导致一些过拟合。这使我们得出结论，100个PCA组件是使用最佳的数量。应该注意的是，我们可以进一步尝试一些超参数调整，以找到更优的组件数量，但就目前而言，我们将保持我们的管道不变，并转向使用RBM组件。
- en: Using a linear model on extracted RBM components
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在提取的RBM组件上使用线性模型
- en: Even the optimal number of PCA components was unable to beat the logistic regression
    alone by much in terms of accuracy. Let's see how our RBM does. To make the following
    pipeline, we will keep the same parameters for the logistic regression model and
    find the optimal number of components between 10, 100, and 200 (like we did for
    the PCA pipeline). Note that we could try to expand the number of features past
    the number of raw pixels (784) but we will not attempt to.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是最佳数量的PCA组件也无法在准确率方面大幅超越单独的逻辑回归。让我们看看我们的RBM表现如何。为了构建以下管道，我们将保持逻辑回归模型的相同参数，并在10、100和200（就像我们在PCA管道中所做的那样）之间找到最佳组件数量。请注意，我们可以尝试将特征数量扩展到原始像素数（784）以上，但我们不会尝试这样做。
- en: 'We begin the same way by setting up our variables:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以同样的方式开始，设置我们的变量：
- en: '[PRE25]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Fitting this grid search to our raw pixels will reveal the optimal number of
    components:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个网格搜索拟合到我们的原始像素上，将揭示最佳组件数量：
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Our RBM module, with a **91.75% cross-validated accuracy**, was able to extract
    200 new features from our digits and give us a boost of three percent in accuracy
    (which is a lot!) by not doing anything other than adding the BernoulliRBM module
    into our pipeline.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的RBM模块，具有**91.75%的交叉验证准确率**，能够从我们的数字中提取200个新特征，并通过在我们的管道中添加BernoulliRBM模块，在不做任何其他事情的情况下，将准确率提高了三个百分点（这是一个很大的提升！）。
- en: The fact that 200 was the optimal number of components suggests that we may
    even obtain a higher performance by trying to extract more than 200 components.
    We will leave this as an exercise to the reader.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 200是最佳组件数量的事实表明，我们甚至可以通过尝试提取超过200个组件来获得更高的性能。我们将把这个留作读者的练习。
- en: This is evidence to the fact that feature learning algorithms work very well
    when dealing with very complex tasks such as image recognition, audio processing,
    and natural language processing. These large and interesting datasets have hidden
    components that are difficult for linear transformations like PCA or LDA to extract
    but non-parametric algorithms like RBM can.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了特征学习算法在处理非常复杂任务（如图像识别、音频处理和自然语言处理）时工作得非常好。这些大型且有趣的数据集具有隐藏的组件，这些组件对于PCA或LDA等线性变换来说很难提取，但对于RBM等非参数算法来说却可以。
- en: Learning text features – word vectorizations
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习文本特征 – 词向量化
- en: Our second example of feature learning will move away from images and towards
    text and natural language processing. When machines learn to read/write, they
    face a very large problem, context. In previous chapters, we have been able to
    vectorize documents by counting the number of words that appeared in each document
    and we fed those vectors into machine learning pipelines. By constructing new
    count-based features, we were able to use text in our supervised machine learning
    pipelines. This is very effective, up until a point. We are limited to only being
    to understand text as if they were only a **Bag of Words** (**BOW**). This means
    that we regard documents as being nothing more than a collection of words out
    of order.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个特征学习示例将远离图像，转向文本和自然语言处理。当机器学习读写时，它们面临一个非常大的问题，即上下文。在前面的章节中，我们已经能够通过计算每个文档中出现的单词数量来向量化文档，并将这些向量输入到机器学习管道中。通过构建新的基于计数的特征，我们能够在我们的监督机器学习管道中使用文本。这非常有效，直到某个点。我们局限于只能将文本理解为一个**词袋模型（BOW**）。这意味着我们将文档视为仅仅是无序单词的集合。
- en: What's more is that each word on its own has no meaning. It is only in a collection
    of other words that a document can have meaning when using modules such as `CountVectorizer`
    and `TfidfVectorizer`. It is for this reason that we will turn our attention away
    from scikit-learn and onto a module called `gensim` for computing word embeddings.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，每个单词本身并没有意义。只有在一个由其他单词组成的集合中，文档在使用`CountVectorizer`和`TfidfVectorizer`等模块时才能具有意义。正因为如此，我们将把注意力从scikit-learn转向一个名为`gensim`的模块，用于计算词嵌入。
- en: Word embeddings
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Up until this point, we have used scikit-learn to embed documents (tweets, reviews,
    URLs, and so on) into a vectorized format by regarding tokens (words, n-grams)
    as features and documents as having a certain amount of these tokens. For example,
    if we had 1,583 documents and we told our `CountVectorizer` to learn the top 1,000
    tokens of `ngram_range` from one to five, we would end up with a matrix of shape
    (1583, 1000) where each row represented a single document and the 1,000 columns
    represented literal n-grams found in the corpus. But how do we achieve an even
    lower level of understanding? How do we start to teach the machine what words *mean*
    in context?
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用scikit-learn将文档（推文、评论、URL等）嵌入到向量格式中，将标记（单词、n-gram）视为特征，并将文档视为具有一定数量的这些标记。例如，如果我们有1,583个文档，并告诉我们的`CountVectorizer`从`ngram_range`的一到五学习前1,000个标记，我们最终会得到一个形状为（1583,
    1000）的矩阵，其中每一行代表一个单独的文档，而1,000列代表在语料库中找到的原始n-gram。但如何达到更低的理解层次？我们如何开始教机器在上下文中理解单词的含义？
- en: 'For example, if we were to ask you the following questions, you may give the
    following answers:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们问你以下问题，你可能会给出以下答案：
- en: '*Q: What would you get if we took a king, removed the man aspect of it, and
    replaced it with a woman?*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*问：如果我们取一个国王，去掉它的男性特征，并用女性来替换，我们会得到什么？*'
- en: '***A: A queen***'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '***答：王后***'
- en: '*Q: London is to England as Paris is to ____.*'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*问：伦敦对英格兰就像巴黎对____*。'
- en: '***A: France***'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '***答：法国***'
- en: You, a human, may find these questions simple, but how would a machine figure
    this out without knowing what the words by themselves mean in context? This is,
    in fact, one of the greatest challenges that we face in **natural language processing**
    (**NLP**) tasks.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 你，作为一个人类，可能会觉得这些问题很简单，但如果没有知道单词在上下文中的含义，机器如何解决这个问题呢？这实际上是我们面临的最大挑战之一，在**自然语言处理**（**NLP**）任务中。
- en: 'Word embeddings are one approach to helping a machine understand context. A **word
    embedding** is a vectorization of a single word in a feature space of n dimensions,
    where *n* represents the number of latent characteristics that a word can have.
    This means that every word in our vocabulary is not longer, just a string, but
    a vector in and of itself. For example, if we extracted n=5 characteristics about
    each word, then each word in our vocabulary would correspond to a 1 x 5 vector.
    For example, we might have the following vectorizations:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是帮助机器理解上下文的一种方法。**词嵌入**是将单个词在n维特征空间中的向量表示，其中*n*代表一个词可能具有的潜在特征数量。这意味着我们词汇表中的每个词不再仅仅是字符串，而本身就是一个向量。例如，如果我们从每个词中提取了n=5个特征，那么我们词汇表中的每个词就会对应一个1
    x 5的向量。例如，我们可能会有以下向量表示：
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'And with these vectorizations, we can tackle the question *What would you get
    if we took a king, removed the man aspect of it, and replaced it with a woman?*
    by performing the following operation:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些向量表示，我们可以通过以下操作来处理问题“如果我们取一个国王，去掉它的男性特征，并用女性来替换，我们会得到什么？”：
- en: '*king - man + woman*'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*king - man + woman*'
- en: 'In code, this would look like:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，这看起来会是这样：
- en: '[PRE28]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This seems simple but it does some with a few caveats:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很简单，但有一些注意事项：
- en: Context (in the form of word embeddings) changes from corpus to corpus as does
    word meanings. This means that static word embeddings by themselves are not always
    the most useful
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文（以词嵌入的形式）和词义一样，从语料库到语料库都在变化。这意味着静态的词嵌入本身并不总是最有用的
- en: Word embeddings are dependent on the corpus that they were learned from
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词嵌入依赖于它们从中学习到的语料库
- en: Word embeddings allow us to perform very precise calculations on single words
    to achieve what we might consider context.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入允许我们对单个词进行非常精确的计算，以达到我们可能认为的上下文。
- en: Two approaches to word embeddings - Word2vec and GloVe
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 两种词嵌入方法 - Word2vec和GloVe
- en: There are two families of algorithms that dominate the space of word embeddings.
    They are called **Word2vec **and **GloVe**. Both methods are responsible for producing
    word embeddings by learning from very large corpus (collection of text documents).
    The main difference between these two algorithms is that the GloVe algorithm,
    out of Stanford, learns word embeddings through a series of matrix statistics
    while Word2vec, out of Google, learns them through a deep learning approach. Both
    algorithms have merits and our text will focus on using the Word2vec algorithm
    to learn word embeddings.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种算法家族主导着词嵌入的空间。它们被称为 **Word2vec** 和 **GloVe**。这两种方法都通过从非常大的语料库（文本文档集合）中学习来生成词嵌入。这两种算法之间的主要区别是，GloVe
    算法（来自斯坦福大学）通过一系列矩阵统计来学习词嵌入，而 Word2vec（来自谷歌）通过深度学习方法来学习。这两种算法都有其优点，我们的文本将专注于使用
    Word2vec 算法来学习词嵌入。
- en: Word2Vec - another shallow neural network
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2Vec - 另一个浅层神经网络
- en: 'In order to learn and extract word embeddings, Word2vec will implement another
    shallow neural network. This time, instead of generically throwing in new data
    into our visible layer, we will deliberately put in the correct data to give us
    the right word embeddings. Without going too into much detail, imagine a neural
    architecture with the following structure:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习和提取词嵌入，Word2vec 将实现另一个浅层神经网络。这次，我们不会将新数据通用地投入可见层，而是会故意放入正确数据以获得正确的词嵌入。不深入细节的话，想象一个具有以下结构的神经网络架构：
- en: '![](img/8f071838-2990-44e4-bbdf-7953d705d259.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8f071838-2990-44e4-bbdf-7953d705d259.png)'
- en: Like the RBM, we have a visible input layer and a hidden layer. In this case,
    our input layer has as many nodes as the length of vocabulary that we wish to
    learn from. This comes in handy if we have a corpus of millions of words but we
    wish to only learn a handful of them. In the preceding graph, we would be learning
    the context of 5,000 words. The hidden layer in this graph represents the number
    of features we wish to learn about each word. In this case, we will be embedding
    words into a 300-dimensional space.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 与 RBM 类似，我们有一个可见输入层和一个隐藏层。在这种情况下，我们的输入层节点数量与我们要学习的词汇长度相同。如果我们有一个包含数百万词汇的语料库，但只想学习其中的一小部分，这会非常有用。在前面的图中，我们将学习
    5,000 个词汇的上下文。这个图中的隐藏层代表我们希望了解的每个词汇的特征数量。在这种情况下，我们将词汇嵌入到 300 维的空间中。
- en: The main difference between this neural network and the one we used for RBM
    is the existence of an output layer. Note that in our graph, the output layer
    has as many nodes as the input layer. This is not a coincidence. Word embedding
    models work by *predicting* nearby words based on the existence of a reference
    word. For example, if we were to predict the word *calculus*, we would want the
    final `math` node to light up the most. This gives the semblance of a supervised
    machine learning algorithm.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们用于 RBM 的神经网络相比，这个神经网络的主要区别在于存在一个输出层。请注意，在我们的图中，输出层的节点数量与输入层相同。这并非巧合。词嵌入模型通过基于参考词的存在来*预测*附近的词汇。例如，如果我们想要预测单词
    *calculus*，我们希望最终的 `math` 节点被点亮得最亮。这给人一种监督机器学习算法的表象。
- en: We then train the graph on this structure and eventually learn the 300-dimension
    word representations by passing in one-hot vectors of words and extracting the
    hidden layer's output vector and using that as our latent structure. In production,
    the previous diagram is extremely inefficient due to the very large number of
    output nodes. In order to make this process extremely more computationally efficient,
    different loss functions are utilized to capitalize on the text's structure.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后在这个结构上训练图，并最终通过输入单词的单热向量并提取隐藏层的输出向量来学习 300 维的词表示。在生产环境中，由于输出节点数量非常大，之前的图示非常低效。为了使这个过程在计算上更加高效，利用文本结构的不同损失函数。
- en: The gensim package for creating Word2vec embeddings
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于创建 Word2vec 嵌入的 gensim 包
- en: 'We will not be implementing a full working neural network that performs the
    word embedding procedure, however we will be using a Python package called `gensim`
    to do this work for us:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会实现一个完整的、执行词嵌入过程的神经网络，然而我们将使用一个名为 `gensim` 的 Python 包来为我们完成这项工作：
- en: '[PRE29]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'A `gensim` can take in a corpora of text and run the preceding neural network
    structure for us and obtain word embeddings with only a few lines of code. To
    see this in action, let''s import a standard corpus to get started. Let''s set
    a logger in our notebook so that we can see the training process in a bit more
    detail:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim`可以接受一个文本语料库，并为我们运行前面的神经网络结构，仅用几行代码就获得单词嵌入。为了看到这个动作，让我们导入一个标准语料库以开始。让我们在我们的笔记本中设置一个记录器，以便我们可以更详细地看到训练过程：'
- en: '[PRE30]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, let''s create our corpus:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建我们的语料库：
- en: '[PRE31]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Notice the term `word2vec`. This is a specific algorithm used to calculate word
    embeddings and the main algorithm used by `gensim`. It is one of the standards
    for word embeddings.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 注意术语“word2vec”。这是一个用于计算单词嵌入的特定算法，也是`gensim`使用的主要算法。它是单词嵌入的标准之一。
- en: 'For gensim to do its job, sentences needs to be any iterable (list, generator,
    tuple, and so on) that holds sentences that are already tokenized. Once we have
    such a variable, we can put `gensim` to work by learning word embeddings:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让`gensim`完成其工作，句子需要是任何可迭代的（列表、生成器、元组等），它包含已经分词的句子。一旦我们有了这样的变量，我们就可以通过学习单词嵌入来使用`gensim`：
- en: '[PRE32]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This line of code will start the learning process. If you are passing in a
    large corpus, it may take a while. Now that the `gensim` module is done fitting,
    we can use it. We can grab individual embeddings by passing strings into the `word2vec`
    object:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码将启动学习过程。如果你传递了一个大的语料库，这可能需要一段时间。现在`gensim`模块已经完成拟合，我们可以使用它。我们可以通过将字符串传递给`word2vec`对象来获取单个嵌入：
- en: '[PRE33]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The `gensim` has built-in methods to get the most out of our word embeddings.
    For example, to answer the question about the `king`, we can use the the `most_similar`
    method:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim`内置了获取我们单词嵌入最大价值的方法。例如，为了回答关于“king”的问题，我们可以使用`most_similar`方法：'
- en: '[PRE34]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Hmm, unfortunately this isn''t giving us the answer we''d expect: `queen`.
    Let''s try the `Paris` word association:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，不幸的是，这并没有给出我们预期的答案：“queen”。让我们尝试“Paris”单词联想：
- en: '[PRE35]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: It appears that the word `Paris` was never even learned as it did not appear
    in our corpus. We can start to see the limitations to this procedure. Our embeddings
    will only be as good as the corpus we are selecting and the machines we use to
    calculate these embeddings. In our data directory, we have provided a pre-trained
    vocabulary of words that spans across 3,000,000 words found on websites indexed
    by Google with 300 dimensions learned for each word.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来单词“Paris”甚至没有被学习过，因为它没有出现在我们的语料库中。我们可以开始看到这个过程的局限性。我们的嵌入将与我们选择的语料库和用于计算这些嵌入的机器一样好。在我们的数据目录中，我们提供了一个包含300维的预训练词汇表，它跨越了在Google索引的网站上找到的3,000,000个单词。
- en: 'Let''s go ahead and import these pre-trained embeddings. We can do this by
    using built-in importer tools in `gensim`:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续导入这些预训练的嵌入。我们可以通过使用`gensim`中的内置导入工具来完成这项工作：
- en: '[PRE36]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'These embeddings have been trained using vastly more powerful machines than
    anything we have at home and for a much longer period of time. Let''s try our
    word problems out now:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入是通过比我们家里任何机器都强大的机器训练的，并且持续了更长的时间。现在让我们尝试我们的单词问题：
- en: '[PRE37]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Excellent! It seems as though these word embeddings were trained enough to
    allow us to answer these complex word puzzles. The `most_similar` method, as used
    previously, will return the token in the vocabulary that is most similar to the
    words provided. Words in the `positive` list are vectors added to one another,
    while words in the `negative` list are subtracted from the resulting vector. The
    following graph provides a visual representation of how we use word vectors to
    extract meaning:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！看起来这些单词嵌入已经训练得足够好，可以让我们回答这些复杂的单词谜题。正如之前使用的那样，`most_similar`方法将返回词汇表中与提供的单词最相似的标记。`positive`列表中的单词是相互添加的向量，而`negative`列表中的单词是从结果向量中减去的。以下图表提供了我们如何使用单词向量提取意义的视觉表示：
- en: '![](img/1c327f18-9c2e-4bd7-9274-64ac71200591.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1c327f18-9c2e-4bd7-9274-64ac71200591.png)'
- en: 'Here, we are starting with the vector representation for **king** and adding
    to it the concept (vector) for **woman**. From there, we subtract the **man**
    vector (by adding the negative of the vector) to obtain the dotted vector. This
    vector is the most similar to the vector representation for **queen**. This is
    how we obtain the formula:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们以“king”的向量表示开始，并添加“woman”的概念（向量）。从那里，我们通过添加向量的负值来减去“man”向量，以获得点向量。这个向量与“queen”的向量表示最相似。这就是我们获得公式的途径：
- en: '*king + woman - man = queen*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*king + woman - man = queen*'
- en: 'The `gensim` has other methods that we may utilize such as `doesnt_match`.
    This method singles out words that do not belong to a list of words. It does so
    by isolating the word that is the most dissimilar on average to the rest of the
    words. For example, if we give the method four words, three of which are animals
    and the other is a plant, we hope it will figure out which of those doesn''t belong:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim`还有其他我们可以利用的方法，例如`doesnt_match`。这种方法会找出不属于单词列表的单词。它是通过隔离平均与其他单词最不相似的单词来做到这一点的。例如，如果我们给这个方法四个单词，其中三个是动物，另一个是植物，我们希望它能找出哪个不属于：'
- en: '[PRE38]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The package also includes methods to calculate a 0-1 score of similarity between
    single words that can be used to compare words on the fly:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 该包还包括计算单个单词之间0-1相似度分数的方法，这些分数可以用来即时比较单词：
- en: '[PRE39]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Here, we can see that `man` is more similar to `woman` than `man` is to `tree`.
    We can use these helpful methods in order to implement some useful applications
    of word embeddings that would not be possible otherwise.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到“man”比“man”与“tree”更相似。我们可以使用这些有用的方法来实现一些其他情况下无法实现的有用应用，例如单词嵌入：
- en: Application of word embeddings - information retrieval
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词嵌入的应用 - 信息检索
- en: There are countless applications for word embeddings; one of these is in the
    field of information retrieval. When humans input keywords and key phrases into
    search engines, search engines are able to recall and surface specific articles/stories
    that match those keywords exactly. For example, if we search for articles about
    dogs, we will get articles that mention the word dog. But what if we search for
    the word canine? We should still expect to see articles about dogs based on the
    fact that canines are dogs. Let's implement a simple information retrieval system
    to showcase the power of word embeddings.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 单词嵌入有无数的应用，其中之一是信息检索领域。当人类在搜索引擎中输入关键词和短语时，搜索引擎能够召回并展示与这些关键词完全匹配的特定文章/故事。例如，如果我们搜索关于狗的文章，我们会得到提到狗这个词的文章。但如果我们搜索“canine”这个词呢？基于canines是狗的事实，我们仍然应该期望看到关于狗的文章。让我们实现一个简单的信息检索系统来展示单词嵌入的力量。
- en: 'Let''s create a function that tries to grab embeddings of individual words
    from our gensim package and returns None if this lookup fails:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个函数，尝试从我们的gensim包中提取单个单词的嵌入，如果这个查找失败则返回None：
- en: '[PRE40]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, let''s create three article titles, one about a `dog`, one about a `cat`,
    and one about absolutely `nothing` at all for a distractor:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建三个文章标题，一个关于`dog`，一个关于`cat`，还有一个关于绝对`nothing`的干扰项：
- en: '[PRE41]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The goal is to input a reference word that is similar to `dog` or `cat` and
    be able to grab the more relevant title. To do this, we will first create a 3
    x 300 matrix of vectorizations for each sentence. We will do this by taking the
    mean of every word in the sentence and using the resulting mean vector as an estimation
    of the vectorization of the entire sentence. Once we have a vectorization of every
    sentence, we can compare that against the embedding of the reference word by taking
    a dot product between them. The closest vector is the one with the largest dot
    product:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是输入一个与`dog`或`cat`相似的参考词，并能够抓取更相关的标题。为此，我们首先将为每个句子创建一个3 x 300的向量矩阵。我们将通过取句子中每个单词的平均值，并使用得到的平均值向量作为整个句子向量化的估计。一旦我们有了每个句子的向量化，我们就可以通过取它们之间的点积来比较这些向量。最接近的向量是点积最大的那个：
- en: '[PRE42]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'One thing to notice here is that we are creating a vectorization of documents
    (collection of words) and not considering the order of the words. How is this
    better than utilizing a `CountVectorizer` or a `TfidfVectorizer` to grab a count-based
    vectorization of text? The gensim method is attempting to project our text onto
    a latent structure learned by the context of individual words, while the scikit-learn
    vectorizers are only able to use the vocab at our disposal to create our vectorizations.
    In these three sentences, there are only seven unique words:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一点是，我们正在创建文档（单词集合）的向量化，而不是考虑单词的顺序。这与使用`CountVectorizer`或`TfidfVectorizer`来获取基于计数的文本向量化相比有什么好处？gensim方法试图将我们的文本投影到由单个单词的上下文学习到的潜在结构上，而scikit-learn向量器只能使用我们可用的词汇来创建我们的向量化。在这三个句子中，只有七个独特的单词：
- en: '`this`, `is`, `about`, `a`, `dog`, `cat`, `nothing`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`this`, `is`, `about`, `a`, `dog`, `cat`, `nothing`'
- en: 'So, the maximum shape our `CountVectorizer` or `TfidfVectorizer` can project
    is (3, 7). Let''s try to grab the most relevant sentence to the word `dog`:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的 `CountVectorizer` 或 `TfidfVectorizer` 可以投射的最大形状是 (3, 7)。让我们尝试找到与单词 `dog`
    最相关的句子：
- en: '[PRE43]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'That one was easy. Given the word `dog`, we should be able to retrieve the
    sentence about a `dog`. This should also hold true if we input the word `cat`:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 那个很简单。给定单词 `dog`，我们应该能够检索到关于 `dog` 的句子。如果我们输入单词 `cat`，这也应该是正确的：
- en: '[PRE44]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, let''s try something harder. Let''s input the words `canine` and `tiger` and
    see if we get the `dog` and `cat` sentences respectively:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试一个更难的例子。让我们输入单词 `canine` 和 `tiger`，看看我们是否能分别得到 `dog` 和 `cat` 的句子：
- en: '[PRE45]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let''s try a slightly more interesting example. The following are chapter titles
    from Sinan''s first book,* Principles of Data Science*:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个稍微有趣一点的例子。以下是从 Sinan 的第一本书，《数据科学原理》中的章节标题：
- en: '[PRE46]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This will give us a list of 12 different chapter titles to retrieve from. The
    goal then will be to use a reference word to sort and serve up the top three most
    relevant chapter titles to read, given the topic. For example, if we asked our
    algorithm to give us chapters relating to *math*, we might expect to be recommended
    the chapters about basic mathematics, statistics, and probability.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们一个包含 12 个不同章节标题的列表，以便检索。那么目标将是使用参考词对章节进行排序并提供最相关的三个章节标题来阅读，给定主题。例如，如果我们要求我们的算法给我们与
    *math* 相关的章节，我们可能会期望推荐关于基础数学、统计学和概率的章节。
- en: 'Let''s try to see which chapters are the best to read, given human input. Before
    we do so, let''s calculate a matrix of vectorized documents like we did with our
    previous three sentences:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试看看哪些章节最适合阅读，给定人类输入。在我们这样做之前，让我们计算一个向量化的文档矩阵，就像我们之前对前三个句子所做的那样：
- en: '[PRE47]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, let''s find the chapters that are most related to `math`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们找到与 `math` 最相关的章节：
- en: '[PRE48]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, let''s say we are giving a talk about data and want to know which chapters
    are going to be the most helpful in that area:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们正在做一个关于数据的演讲，并想知道哪些章节在这个领域最有帮助：
- en: '[PRE49]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'And finally, which chapters are about `AI`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，哪些章节是关于 `AI` 的：
- en: '[PRE50]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We can see how we can use word embeddings to retrieve information in the form
    of text given context learned from the universe of text.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到如何使用词嵌入根据从文本宇宙中学习到的上下文检索文本形式的信息。
- en: Summary
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This chapter focused on two feature learning tools: RBM and word embedding
    processes.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍了两种特征学习工具：RBM 和词嵌入过程。
- en: Both of these processes utilized deep learning architectures in order to learn
    new sets of features based on raw data. Both techniques took advantage of shallow
    networks in order to optimize for training times and used the weights and biases
    learned during the fitting phase to extract the latent structure of the data.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个过程都利用了深度学习架构，基于原始数据学习新的特征集。这两种技术都利用了浅层网络来优化训练时间，并使用在拟合阶段学习到的权重和偏差来提取数据的潜在结构。
- en: Our next chapter will showcase four examples of feature engineering on real
    data taken from the open internet and how the tools that we have learned in this
    book will help us create the optimal machine learning pipelines.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一章将展示四个在公开互联网上收集的实时数据特征工程的例子，以及我们在这本书中学到的工具将如何帮助我们创建最佳的机器学习管道。
