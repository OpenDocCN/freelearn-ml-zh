- en: '*Chapter 3*: Data-Centric Approaches'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第三章*：数据中心化方法'
- en: In the *Defining explanation methods and approaches* section of [*Chapter 1*](B18216_01_ePub.xhtml#_idTextAnchor014),
    *Foundational Concepts of Explainability Techniques*, when we looked at the various
    dimensions of explainability, we discussed how data is one of the important dimensions.
    In fact, all **machine learning** (**ML**) algorithms depend on the underlying
    data being used.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第一章*](B18216_01_ePub.xhtml#_idTextAnchor014)“可解释性技术基础概念”的*定义解释方法和途径*部分，当我们查看可解释性的各个维度时，我们讨论了数据是其中一个重要维度。实际上，所有**机器学习**（**ML**）算法都依赖于所使用的基础数据。
- en: In the previous chapter, we discussed various *model explainability methods*.
    Most of the methods discussed in [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033),
    *Model Explainability Methods*, are model-centric. The concepts and ideas discussed
    were focused on making black-box models interpretable. But recently, the ML and
    AI communities have realized the core importance of data for any analysis or modeling
    purposes. So, more and more AI researchers are exploring new ideas and concepts
    around **data-centric AI**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了各种**模型可解释性方法**。在[*第二章*](B18216_02_ePub.xhtml#_idTextAnchor033)“模型可解释性方法”中讨论的大多数方法都是模型中心的。讨论的概念和思想集中在使黑盒模型可解释。但最近，机器学习和人工智能社区已经意识到数据对于任何分析或建模目的的核心重要性。因此，越来越多的AI研究人员正在探索围绕**数据中心化AI**的新想法和概念。
- en: Since data plays a vital role in the model-building and prediction process,
    it is even more important for us to explain the functioning of any ML and AI algorithm
    with respect to the underlying data. From what I have observed from my experience
    in this field, the failure of any ML systems in production happens neither due
    to the poor choice of ML algorithm nor due to an inefficient model training or
    tuning process, but rather it occurs mostly due to inconsistencies in the underlying
    dataset. So, this chapter focuses on the concepts of **data-centric explainable
    AI** (**XAI**).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据在模型构建和预测过程中发挥着至关重要的作用，因此，我们更有必要从基础数据的角度解释任何机器学习和人工智能算法的工作原理。从我在这领域的经验观察来看，任何机器学习系统在生产中的失败并不是由于选择了糟糕的机器学习算法，也不是由于模型训练或调整过程效率低下，而是大多数情况下是由于基础数据集的不一致性。因此，本章专注于**数据中心化可解释人工智能**（**XAI**）的概念。
- en: 'The goal of this chapter is to introduce you to the concepts of data-centric
    XAI. After reading this chapter, you will get to know about the various methods
    that can be performed to check the quality of the data, which might influence
    the model outcome. For production-level ML systems, the inference data can have
    issues related to its consistency and quality. So, monitoring these drifts is
    extremely important. Additionally, there can be external noise or perturbations
    affecting the data that can impact the model. So, these are some data-centric
    approaches that we will be discussing that are used for explaining ML models.
    In this chapter, the following main topics will be covered:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是向您介绍数据中心化XAI的概念。阅读本章后，您将了解可以执行的各种方法，以检查可能影响模型结果的数据质量。对于生产级别的机器学习系统，推理数据可能存在与其一致性和质量相关的问题。因此，监控这些漂移至关重要。此外，可能存在影响数据的外部噪声或扰动，这可能会影响模型。因此，我们将讨论一些用于解释机器学习模型的数据中心化方法。在本章中，以下主要主题将被涵盖：
- en: Introduction to data-centric XAI
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中心化XAI简介
- en: Thorough data analysis and profiling processes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 详尽的数据分析和配置文件过程
- en: Monitoring and anticipating drifts
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控和预测漂移
- en: Checking adversarial robustness
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查对抗鲁棒性
- en: Measuring data forecastability
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量数据可预测性
- en: Now, let's dive in!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入探讨！
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Similar to [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model Explainability
    Methods*, for this chapter, certain tutorial examples to implement some of the
    techniques to perform data-centric XAI in Python on certain interesting datasets
    have been provided. We will be using Python Jupyter notebooks to run the code
    and visualize the output throughout this book. The code and dataset resources
    for this chapter can be downloaded or cloned from the following GitHub repository:
    [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03).
    Other important Python frameworks required to run the code will be mentioned in
    the notebooks with other relevant details to understand the code implementation
    of these concepts. Other important Python frameworks required to run the code
    will be mentioned in the notebooks along with other relevant details to understand
    the code implementation of these concepts. Please note that this chapter mainly
    focuses on providing a conceptual understanding of the topics covered. The Jupyter
    notebooks will help you gain the supplementary knowledge that is required to implement
    these concepts in practice. I recommend that you first read this chapter and then
    work on executing the Jupyter notebooks.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与[*第二章*](B18216_02_ePub.xhtml#_idTextAnchor033)中的*模型可解释性方法*类似，本章提供了一些教程示例，以实现某些在特定有趣数据集上执行以数据为中心的XAI技术的技术。我们将使用Python
    Jupyter笔记本来运行代码并在整本书中可视化输出。本章的代码和数据集资源可以从以下GitHub仓库下载或克隆：[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03)。其他运行代码所需的Python框架将在笔记本中提及，以及其他相关细节，以了解这些概念代码的实现。其他运行代码所需的Python框架将在笔记本中提及，以及了解这些概念代码实现的相关细节。请注意，本章主要侧重于提供对所涵盖主题的概念性理解。Jupyter笔记本将帮助您获得在实践实现这些概念所需的补充知识。我建议您首先阅读本章，然后尝试执行Jupyter笔记本。
- en: Introduction to data-centric XAI
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据为中心的XAI简介
- en: '*Andrew Ng*, one of the influential thought leaders in the field of AI and
    ML, has often highlighted the importance of using a systematic approach to build
    AI systems with high-quality data. He is one of the pioneers of the idea of **data-centric
    AI**, which focuses on developing systematic processes to develop models using
    clean, consistent, and reliable data, instead of focusing on the code and the
    algorithm. If the data is consistent, unambiguous, balanced, and available in
    sufficient quantity, this leads to faster model building, improved accuracy, and
    faster deployment for any production-level system.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*安德鲁·吴*是AI和ML领域有影响力的思想领袖之一，他经常强调使用系统方法构建具有高质量数据的AI系统的重要性。他是**以数据为中心的AI**理念的先驱之一，该理念侧重于开发系统流程，使用干净、一致和可靠的数据来开发模型，而不是专注于代码和算法。如果数据是一致的、明确的、平衡的，并且有足够的数量，这将导致模型构建更快、准确性更高，以及任何生产级系统的部署更快。'
- en: Unfortunately, all AI and ML systems that exist in production today are not
    in alignment with the principles of data-centric AI. Consequently, there can be
    severe issues with the underlying data that seldom get detected but eventually
    lead to the failure of ML systems. That is why **data-centric XAI** is important
    to inspect and evaluate the quality of the data being used.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，今天所有在生产中存在的AI和ML系统都不符合以数据为中心的AI原则。因此，可能存在严重的数据问题，这些问题很少被发现，但最终会导致ML系统失败。这就是为什么**以数据为中心的XAI**对于检查和评估所使用数据的质量很重要。
- en: When we talk about explaining the functioning of any black-box model with respect
    to the data, we should inspect the *volume of the data*, the *consistency of the
    data* (particularly for supervised ML problems), and the *purity and integrity
    of the data*. Now, let's discuss these important aspects of data-centric XAI and
    understand why they are important.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论根据数据解释任何黑盒模型的运作时，我们应该检查数据的*量*、数据的*一致性*（尤其是对于监督式ML问题），以及数据的*纯净性和完整性*。现在，让我们讨论数据为中心的XAI的重要方面，并了解为什么它们很重要。
- en: Analyzing data volume
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析数据量
- en: One of the classical problems of ML algorithms is the lack of generalization
    due to **overfitting**. Overfitting can be reduced by adding more data or by getting
    datasets of the appropriate volume to solve the underlying problem. So, the very
    first question we should ask about the data to explain the black-box model is
    "*Is the model trained on sufficient data?*" But for any industrial application,
    since data is very expensive, adding more data is not always feasible. So, the
    question should be "*How do we find out if the model was trained on sufficient
    data?*"
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法的经典问题之一是由于**过拟合**导致的泛化不足。可以通过添加更多数据或获取适当体积的数据集来解决潜在问题，从而减少过拟合。因此，我们应该首先问关于数据的问题来解释黑盒模型：“*模型是否在足够的数据上训练过？*”但对于任何工业应用，由于数据非常昂贵，添加更多数据并不总是可行的。因此，问题应该是：“*我们如何找出模型是否在足够的数据上训练过？*”
- en: One way to inspect whether the model was trained on sufficient data is by training
    models with 70%, 80%, and 90% of the training dataset. If the model accuracy shows
    an increasing trend, with an increase in the volume of the data, that means the
    volume of the data can influence the model's performance. If the accuracy of the
    trained model, which has been trained on the entire training dataset, is low,
    then an increasing trend of model accuracy with increasing data volume indicates
    that the model is not trained on sufficient data. Therefore, more data is needed
    to make the model more accurate and generalized.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 检查模型是否在足够的数据上训练过的一种方法是通过使用70%、80%和90%的训练数据集来训练模型。如果模型准确率显示出随着数据量的增加而增加的趋势，这意味着数据量可以影响模型的表现。如果基于整个训练数据集训练的模型的准确率较低，那么随着数据量的增加而增加的模型准确率趋势表明模型没有在足够的数据上训练。因此，需要更多数据来使模型更准确和更具泛化能力。
- en: For production systems, if data is continuously flowing and there is no constraint
    on the availability of data, continuous training and monitoring of the models
    should be done on changing volumes of the data to understand and analyze its impact
    on the overall model performance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生产系统，如果数据持续流动且没有对数据可用性的限制，应该对数据的变化量进行持续的训练和监控，以了解和分析其对整体模型性能的影响。
- en: Analyzing data consistency
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析数据一致性
- en: Data consistency is another important factor to inspect when explaining ML models
    with respect to the data. One of the fundamental steps of analyzing data consistency
    is by understanding the distribution of the data. If the *data is not evenly distributed*,
    if there is a *class imbalance*, or if the *data is skewed toward a particular
    direction*, it is very likely that the model will be *biased* or less efficient
    for a particular segment of the data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据一致性是解释机器学习模型时需要检查的另一个重要因素。分析数据一致性的基本步骤之一是理解数据的分布。如果数据*分布不均匀*，如果存在*类别不平衡*，或者如果*数据偏向特定方向*，那么模型很可能对数据的一个特定部分存在*偏差*或效率较低。
- en: For production systems, it has been often observed that the inference data used
    in the production application might have some variance with the data used during
    training and validation. This phenomenon is known as **data drift**, and it refers
    to an unexpected change in the data structure or the statistical properties of
    the dataset, thus making the data corrupt and hampering the functioning of the
    ML system.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生产系统，通常观察到在生产应用中使用的推理数据可能与训练和验证期间使用的数据进行一些差异。这种现象被称为**数据漂移**，它指的是数据结构或数据集的统计属性出现意外变化，从而使数据变得不完整并妨碍机器学习系统的运行。
- en: Data drift is very common for most real-time predictive models. This is simply
    because, in most situations, the data distribution changes over a period of time.
    This can happen due to a variety of reasons, for instance, if the system through
    which the data is being collected (for example, sensors) starts malfunctioning
    or needs to be recalibrated, then data drift can occur. Other external factors
    such as the surrounding temperature and surrounding noise can also introduce data
    drift. There can be a natural change in the relationship between the features
    that might cause data to drift. Consequently, if the training data is significantly
    different from the inference data, the model will make an error when predicting
    the outcome.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数实时预测模型来说，数据漂移非常常见。这仅仅是因为，在大多数情况下，数据分布会在一段时间内发生变化。这可能是由多种原因造成的，例如，如果收集数据所通过的系统（例如，传感器）开始出现故障或需要重新校准，那么就会发生数据漂移。其他外部因素，如周围温度和周围噪声，也可能引入数据漂移。特征之间的关系可能会发生自然变化，这可能导致数据漂移。因此，如果训练数据与推理数据显著不同，模型在预测结果时将产生错误。
- en: Now, sometimes, there can be a drift in the whole dataset or there can be a
    drift in one or more features. If there is a drift in a single feature, this is
    referred to as **feature drift**. There are multiple ways to detect feature drift
    such as the **Population Stability Index** (**PSI**) ([https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf](https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf)),
    **Kullback–Leibler Divergence** (**KL Divergence**)([https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)),
    and **Wasserstein metric** (**Earth Movers Distance**)([http://infolab.stanford.edu/pub/cstr/reports/cs/tr/99/1620/CS-TR-99-1620.ch4.pdf](http://infolab.stanford.edu/pub/cstr/reports/cs/tr/99/1620/CS-TR-99-1620.ch4.pdf)).
    The application of some of these techniques has been demonstrated at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03).
    These statistical methods are popular ways to measure the distance between two
    data distributions. So, if the distance is significantly large, this is an indication
    of the presence of drift.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有时整个数据集可能会出现漂移，或者一个或多个特征可能会出现漂移。如果单个特征出现漂移，这被称为**特征漂移**。有多种方法可以检测特征漂移，例如**人口稳定性指数**（**PSI**）([https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf](https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf)）、**Kullback–Leibler
    散度**（**KL 散度**）([https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)）和**Wasserstein
    距离**（**地球搬家距离**）([http://infolab.stanford.edu/pub/cstr/reports/cs/tr/99/1620/CS-TR-99-1620.ch4.pdf](http://infolab.stanford.edu/pub/cstr/reports/cs/tr/99/1620/CS-TR-99-1620.ch4.pdf))。其中一些技术的应用已在[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03)中展示。这些统计方法是衡量两个数据分布之间距离的流行方式。因此，如果距离显著较大，这表明存在漂移。
- en: Apart from feature drift, **Label Drift** or **Concept Drift** can occur if
    the statistical properties of the target variable change over a period of time
    due to unknown reasons. However, overall data consistency is an important parameter
    for *root cause analysis inspection* when interpreting black-box models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除了特征漂移，如果由于未知原因，目标变量的统计特性在一段时间内发生变化，则可能会发生**标签漂移**或**概念漂移**。然而，在解释黑盒模型时，整体数据一致性是一个重要的参数，用于**根本原因分析检查**。
- en: Analyzing data purity
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析数据纯净度
- en: The datasets used for practical industrial problems are never clean, even though
    most organizations spend a significant amount of time and investment on data engineering
    and data curation to drive a culture of *data-driven decision-making*. Yet, almost
    all practical datasets are messy and require a systematic approach to curation
    and preparation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 用于实际工业问题的数据集永远不会是干净的，尽管大多数组织在数据工程和数据整理上投入了大量的时间和资金，以推动**数据驱动决策**的文化。然而，几乎所有实际数据集都是杂乱的，需要系统性的整理和准备方法。
- en: When we train a model, usually, we put our efforts into data preprocessing and
    preparation steps such as *finding duplicates or unique values*, *removing noise
    or unwanted values* from the data, *detecting outliers*, *handling missing values*,
    *handling features with mixed data types*, and even *transforming raw features
    or feature engineering* to get better ML models. On a high level, these methods
    are meant for removing impurities from the training data. But what if *a black-box
    ML model is trained on a dataset with less purity and, hence, performs poorly*?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练一个模型时，通常，我们会将精力投入到数据预处理和准备步骤中，例如*查找重复或唯一值*，*从数据中移除噪声或不想要的值*，*检测异常值*，*处理缺失值*，*处理具有混合数据类型的特征*，甚至*转换原始特征或进行特征工程*以获得更好的机器学习模型。从高层次来看，这些方法旨在从训练数据中去除杂质。但是，如果在一个**黑盒机器学习模型**上训练的数据纯净度较低，因此表现不佳，会怎样呢？
- en: 'That is why analyzing data purity is an important step in data-centric XAI.
    Aside from the data preprocessing and preparation methods mentioned earlier, there
    are other data integrity issues that exist as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，分析数据纯净度是数据为中心的XAI的一个重要步骤。除了前面提到的数据预处理和准备方法之外，还存在其他数据完整性问题，如下所示：
- en: '**Label ambiguity**: For supervised ML problems, **label ambiguity** can be
    a very critical problem. If two or more instances of a dataset, which are very
    similar, have multiple labels, then this can lead to label ambiguity. Ambiguous
    labeling of the target variable can increase the difficulty of even domain experts
    classifying the samples correctly. Label ambiguity can be a very common problem,
    as, usually, labeled datasets are prepared by human subject-matter experts who
    are prone to *human error*.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签模糊性**：对于监督式机器学习问题，**标签模糊性**可能是一个非常关键的问题。如果数据集中两个或多个非常相似的实例具有多个标签，那么这可能导致标签模糊性。目标变量的模糊标签可能会增加即使是领域专家正确分类样本的难度。标签模糊性可能是一个非常常见的问题，因为通常，标记数据集是由容易犯**人为错误**的人类领域专家准备的。'
- en: '**Dominating features frequency change** (**DFCC**): Inspecting DFFC in the
    training and inference dataset is another parameter that can cause data integrity
    issues. In [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model Explainability
    Methods*, when we discussed feature importance, we understood that not all features
    within the dataset are equally important, and some of the features have more influential
    power on the model''s decision-making process. These are the dominating features
    in the dataset, and if the variance in the values of the dominating features in
    the training and the inference dataset is high, it is very likely that the model
    will make errors when predicting the outcome.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主导特征频率变化**（**DFCC**）：检查训练和推理数据集中的DFCC是另一个可能导致数据完整性问题的参数。在[*第2章*](B18216_02_ePub.xhtml#_idTextAnchor033)“模型可解释性方法”中，当我们讨论特征重要性时，我们了解到数据集中并非所有特征都同等重要，有些特征对模型的决策过程有更大的影响力。这些是数据集中的主导特征，如果训练和推理数据集中主导特征值的方差很高，那么模型在预测结果时出现错误的概率非常高。'
- en: Other data purity issues, such as the introduction of a new label or new feature
    category, or out of bound values (or anomalies) for a particular feature in the
    inference set, can cause the failure of ML systems in production.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其他数据纯净度问题，例如引入新的标签或新的特征类别，或者在推理集中某个特定特征的越界值（或异常值），可能导致生产中的机器学习系统失败。
- en: 'The following table shows certain important data purity checks that can be
    performed using the **Deepchecks Python framework**:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下表显示了可以使用**Deepchecks Python框架**执行的某些重要数据纯净度检查：
- en: '![Figure 3.1 – Data purity checks using the Deepchecks framework'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.1 – 使用Deepchecks框架进行数据纯净度检查'
- en: '](img/B18216_03_1.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18216_03_1.jpg)'
- en: Figure 3.1 – Data purity checks using the Deepchecks framework
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – 使用Deepchecks框架进行数据纯净度检查
- en: Data-centric XAI also includes other parameters that can be analyzed such as
    *adversarial robustness* ([https://adversarial-ml-tutorial.org/introduction/](https://adversarial-ml-tutorial.org/introduction/)),
    the *trust score comparison* ([https://arxiv.org/abs/1805.11783](https://arxiv.org/abs/1805.11783)),
    *covariate shift* ([https://arxiv.org/abs/2111.08234](https://arxiv.org/abs/2111.08234)),
    *data leakage between the training and validation datasets* ([https://machinelearningmastery.com/data-leakage-machine-learning/](https://machinelearningmastery.com/data-leakage-machine-learning/)),
    and *model performance sensitivity analysis* based on data alterations. All of
    these concepts apply to both tabular data and unstructured data such as images
    and text.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以数据为中心的XAI还包括其他可以分析的参数，例如*对抗鲁棒性*([https://adversarial-ml-tutorial.org/introduction/](https://adversarial-ml-tutorial.org/introduction/))、*信任分数比较*([https://arxiv.org/abs/1805.11783](https://arxiv.org/abs/1805.11783))、*协变量偏移*([https://arxiv.org/abs/2111.08234](https://arxiv.org/abs/2111.08234))、*训练集和验证集之间的数据泄露*([https://machinelearningmastery.com/data-leakage-machine-learning/](https://machinelearningmastery.com/data-leakage-machine-learning/))以及基于数据变更的*模型性能敏感性分析*。所有这些概念都适用于表格数据和非结构化数据，如图像和文本。
- en: To explore practical ways of data purity analysis, you can refer to the Jupyter
    notebook at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter03/Data_Centric_XAI_part_1.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter03/Data_Centric_XAI_part_1.ipynb).
    We will discuss these topics later in the chapter.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要探索数据纯度分析的实际方法，你可以参考[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter03/Data_Centric_XAI_part_1.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter03/Data_Centric_XAI_part_1.ipynb)上的Jupyter笔记本。我们将在本章后面讨论这些主题。
- en: Thorough data analysis and profiling process
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 详尽的数据分析和概要化流程
- en: 'In the previous section, you were introduced to the concept of data-centric
    XAI in which we discussed three important aspects of data-centric XAI: analyzing
    data volume, data consistency, and data purity. You might already be aware of
    some of the methods of data analysis and data profiling that we are going to learn
    in this section. But we are going to assume that we already have a trained ML
    model and, now, we are working toward explaining the model''s decision-making
    process by adopting data-centric approaches.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你被介绍到了以数据为中心的XAI的概念，我们讨论了数据为中心XAI的三个重要方面：分析数据量、数据一致性和数据纯度。你可能已经了解了一些我们将要学习的数据分析和数据概要化方法。但我们将假设我们已经有了一个训练好的机器学习模型，现在我们正在通过采用以数据为中心的方法来解释模型的决策过程。
- en: The need for data analysis and profiling processes
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需要进行数据分析和概要化流程
- en: In [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model Explainability
    Methods*, when we discussed knowledge extraction using **exploratory data analysis**
    (**EDA**), we discovered that this was a pre-hoc analysis process, in which we
    try to understand the data to form relevant hypotheses. As data scientists, these
    initial hypotheses are important as they allow us to take the necessary steps
    to build a better model. But let's suppose that we have a baseline trained ML
    model and the model is not performing as expected because it is not meeting the
    benchmark accuracy scores that were set.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第二章*](B18216_02_ePub.xhtml#_idTextAnchor033) *模型可解释性方法*中，当我们讨论使用**探索性数据分析**（**EDA**）进行知识提取时，我们发现这是一个预先分析过程，其中我们试图理解数据以形成相关假设。作为数据科学家，这些初步假设很重要，因为它们允许我们采取必要的步骤来构建更好的模型。但让我们假设我们有一个基线训练好的机器学习模型，而这个模型的表现并不如预期，因为它没有达到设定的基准精度分数。
- en: Following the principles of model-centric approaches, most data scientists might
    want to spend more time in hyperparameter tuning, training for a greater number
    of epochs, feature engineering, or choosing a more complex algorithm. After a
    certain point, these methods will become limited and provide a very small boost
    to the model's accuracy. That is when data-centric approaches prove to be very
    efficient.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循以模型为中心的方法原则，大多数数据科学家可能会想要花更多时间在超参数调整、训练更多轮次、特征工程或选择更复杂的算法上。然而，在某个点上，这些方法将变得有限，并且对模型精度的提升非常微小。这时，以数据为中心的方法证明是非常有效的。
- en: Data analysis as a precautionary step
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分析作为预防措施
- en: By the principles of data-centric explainability approaches, at first, we try
    to perform a thorough analysis of the underlying dataset. We try to randomly reshuffle
    the data to create different training and validation sets and observe any overfitting
    or underfitting effects. If the model is overfitting or underfitting, clearly
    more data is required to generalize the model. If the available data is not sufficient
    in volume, there are ways to generate synthetic or artificial data. One such popular
    technique used for image classification is **data augmentation** ([https://research.aimultiple.com/data-augmentation/](https://research.aimultiple.com/data-augmentation/)).
    The **synthetic minority oversampling technique** (**SMOTE**) ([https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/))
    is also a powerful method that you can use to increase the size of the dataset.
    Some of these data-centric approaches are usually practiced during conventional
    ML workflows. However, we need to realize the importance of these steps for the
    explainability of black-box models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 根据以数据为中心的可解释性方法的原则，首先，我们尝试对基础数据集进行彻底分析。我们尝试随机重新排列数据以创建不同的训练集和验证集，并观察任何过拟合或欠拟合效应。如果模型过拟合或欠拟合，显然需要更多的数据来泛化模型。如果可用的数据量不足，有方法可以生成合成或人工数据。用于图像分类的一种流行技术是**数据增强**（[https://research.aimultiple.com/data-augmentation/](https://research.aimultiple.com/data-augmentation/)）。**合成少数过采样技术**（**SMOTE**）（[https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/））也是一种强大的方法，您可以使用它来增加数据集的大小。这些以数据为中心的方法通常在传统的机器学习工作流程中实践。然而，我们需要认识到这些步骤对于黑盒模型可解释性的重要性。
- en: Once we have done enough tests to understand whether the volume of the data
    is sufficient, we can try to inspect the consistency and purity of the data at
    a segmented level. If we are working on a classification problem, we can try to
    understand whether the model performance is consistent for all the classes. If
    not, we can isolate the particular class or classes for which the model performance
    is poor. Then, we check for data drifts, feature drifts, concept drifts, label
    ambiguity, data leakage (for example, when unseen test data trickles into the
    training data), and other data integrity checks for that particular class (or
    classes). Usually, if there is any abnormality with the data for a particular
    class (or classes), these checks are sufficient to isolate the problem. A thorough
    data analysis acts as a precautionary step to detect any loopholes in the modeling
    process.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们进行了足够的测试以了解数据量是否充足，我们就可以尝试在分段级别上检查数据的一致性和纯度。如果我们正在处理一个分类问题，我们可以尝试了解模型性能是否对所有类别都是一致的。如果不一致，我们可以隔离模型性能较差的特定类别或类别。然后，我们检查数据漂移、特征漂移、概念漂移、标签模糊、数据泄露（例如，当未见过的测试数据逐渐进入训练数据时），以及针对该特定类别（或类别）的其他数据完整性检查。通常，如果特定类别（或类别）的数据有任何异常，这些检查就足以隔离问题。彻底的数据分析作为预防措施，用于检测建模过程中的任何漏洞。
- en: Building robust data profiles
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建稳健的数据概要
- en: Another approach is to build statistical profiles of the data and then compare
    the profiles between the training data and the inference data. A statistical profile
    of a dataset is a collection of certain statistical measures of its feature values
    segmented by the target variable class (or, in the case of a regression problem,
    the bin of values). The selection of the statistical measures might change from
    use case to use case, but usually, I select statistical measures such as the mean,
    median, average variance, average standard deviation, coefficient of variation
    (standard deviation/mean), and z-scores ((value – mean)/standard deviation) for
    creating data profiles. In the case of time series data, measures such as the
    moving average and the moving median can also be very important.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是建立数据的统计概要，然后比较训练数据和推理数据之间的概要。数据集的统计概要是一组统计度量，这些度量按目标变量类别（或，在回归问题的情况下，按值区间）分段。统计度量的选择可能因用例而异，但通常，我选择如均值、中位数、平均方差、平均标准差、变异系数（标准差/均值）和z分数（（值
    - 均值）/标准差）等统计度量来创建数据概要。在时间序列数据的情况下，移动平均和移动中位数等度量也非常重要。
- en: 'Next, let''s try to understand how this approach is useful. Suppose there is
    an arbitrary dataset that has three classes (namely class 0, 1, and 2) and only
    two features: feature 1 and feature 2\. When we try to prepare the statistical
    profile, we would try to calculate certain statistical measures (such as the mean,
    median, and average variance in this example) for each feature and each class.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，让我们尝试理解这种方法的有用之处。假设有一个任意数据集，它有三个类别（即类别0、1和2）并且只有两个特征：特征1和特征2。当我们尝试准备统计概览时，我们会尝试计算每个特征和每个类别的某些统计量（例如，在这个例子中的均值、中位数和平均方差）。 '
- en: 'So, for class 0, a set of profile values consisting of the mean of feature
    1, the median of feature 1, the average variance of feature 1, the mean of feature
    2, the median of feature 2, and the average variance of feature 2 will be generated.
    Similarly, for class 1 and class 2, a set of profile values will be created for
    each class. The following table represents the statistical profile of the arbitrary
    dataset that we have considered for this example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于类别0，将生成一个包含特征1的均值、特征1的中位数、特征1的平均方差、特征2的均值、特征2的中位数和特征2的平均方差的概览值集合。同样，对于类别1和类别2，将为每个类别创建一个概览值集合。以下表格代表了我们在本例中考虑的任意数据集的统计概览：
- en: '![Figure 3.2 – A table showing a statistical profile segmented by each class
    for an arbitrary dataset'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.2 – 一个表格，展示了针对任意数据集按每个类别分割的统计概览'
- en: '](img/B18216_03_2.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_03_2.jpg)'
- en: Figure 3.2 – A table showing a statistical profile segmented by each class for
    an arbitrary dataset
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – 一个表格，展示了针对任意数据集按每个类别分割的统计概览
- en: These statistical measures of the feature values can be used to compare the
    different classes. If a trained model predicts a particular class, we can compare
    the feature values with the statistical profile values for that particular class
    to get a fair idea about the influential features contributing to the decision-making
    process of the model. But more importantly, we can create separate statistical
    profiles for the validation set, test set, and inference data used in the production
    systems and compare them with the statistical profile of the training set. If
    the absolute percentage change between the values is significantly higher (say,
    > 20%), then this indicates the presence of data drift.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征值的统计量可以用来比较不同的类别。如果一个训练模型预测了特定的类别，我们可以将该类别的特征值与统计概览值进行比较，以获得关于模型决策过程中影响特征的一个公平的概念。但更重要的是，我们可以为验证集、测试集和生产系统中使用的推理数据创建单独的统计概览，并将它们与训练集的统计概览进行比较。如果值之间的绝对百分比变化显著较高（比如说，>
    20%），那么这表明存在数据漂移。
- en: In our example, let's suppose that if the absolute percentage change in the
    average variance score for feature 1 for class 1 is about 25% between the training
    data and the inference data, then we have a feature drift for feature 1, and this
    might lead to poor model performance with the inference data for the production
    systems. Statistical profiles can also be created for unstructured data such as
    images and text, although the choice of statistical measures might be slightly
    complicated.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，假设如果类别1的特征1的平均方差分数在训练数据和推理数据之间绝对百分比变化约为25%，那么我们就有了特征1的特征漂移，这可能会导致生产系统中的推理数据模型性能不佳。统计概览也可以为非结构化数据，如图像和文本，创建，尽管统计量的选择可能稍微复杂一些。
- en: In general, this approach is very easy to implement and it helps us to validate
    whether the data used for training a model and the data used during testing or
    inference are consistent or not, which is an important step for data-centric model
    explainability. In the next section, we will discuss about the importance of monitoring
    and anticipating drifts for explaining ML systems.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这种方法很容易实现，并且它帮助我们验证用于训练模型的数据和用于测试或推理的数据是否一致，这对于以数据为中心的模型可解释性是一个重要的步骤。在下一节中，我们将讨论监控和预测漂移对于解释机器学习系统的重要性。
- en: Monitoring and anticipating drifts
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控和预测漂移
- en: In the previous section, we understood how a thorough data analysis and data
    profiling approach can help us to identify data issues related to volume, consistency,
    and purity. Usually, during the initial data exploration process, most data scientists
    try to inspect issues in the dataset in terms of volume and purity and perform
    necessary preprocessing and feature engineering steps to handle these issues.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们了解了如何通过彻底的数据分析和数据概要分析的方法帮助我们识别与数据量、一致性和纯度相关的数据问题。通常，在初始数据探索过程中，大多数数据科学家会尝试从数据量、纯度等方面检查数据集中的问题，并执行必要的预处理和特征工程步骤来处理这些问题。
- en: 'But the detection of data consistency for real-time systems and production
    systems is a challenging problem for almost all ML systems. Additionally, issues
    relating to data consistency are often overlooked and are quite unpredictable
    as they can happen at any point in time in production systems. Some of the cases
    where data consistency issues can occur are listed as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 但对于几乎所有的机器学习系统来说，检测实时系统和生产系统中的数据一致性是一个具有挑战性的问题。此外，与数据一致性相关的问题通常被忽视，并且相当不可预测，因为它们可能在生产系统的任何时间点发生。以下是一些可能发生数据一致性问题的案例：
- en: They can occur due to natural reasons such as changes in external environmental
    conditions or due to the natural wear and tear of sensors or systems capturing
    the inference data.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可能由于自然原因而发生，例如外部环境条件的变化或捕捉推理数据的传感器或系统的自然磨损和老化。
- en: They can happen due to human-induced reasons such as any physical damage caused
    to the system collecting the data, any bug in the software program running the
    algorithm due to which the input data is being transformed incorrectly, or any
    noise introduced to the system while upgrading an older version of the system.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可能由于人为原因而发生，例如对收集数据的系统造成的任何物理损坏，由于软件程序中的任何错误导致输入数据被错误地转换，或者在升级旧系统版本时向系统中引入的任何噪声。
- en: So, all of these situations can introduce data drifts and concept drifts, which
    eventually lead to the poor performance of ML models. And since drifts are very
    common in reality, issues related to drifts should be pre-anticipated and should
    be considered during the design process of any ML system.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，所有这些情况都可能引入数据漂移和概念漂移，最终导致机器学习模型性能下降。由于漂移在现实中非常普遍，因此与漂移相关的问题应该预先考虑，并且在任何机器学习系统的设计过程中都应该予以考虑。
- en: Detecting drifts
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检测漂移
- en: 'After trained models are deployed for any production ML system, performance
    monitoring and feedback based on model performance is a necessary process. As
    we monitor the model performance, checking for any data or concept drifts is also
    critical in this step. At this point, you might be wondering two things:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何生产机器学习系统中部署训练好的模型后，性能监控和基于模型性能的反馈是一个必要的流程。在我们监控模型性能的同时，检查任何数据或概念漂移也是这一步骤中的关键。此时，你可能会有两个疑问：
- en: '*What is the best way to identify the presence of a drift?*'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何最好地识别漂移的存在？*'
- en: '*What happens when we detect the presence of a drift?*'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*当我们检测到漂移的存在时会发生什么？*'
- en: As discussed in the *Analyzing data consistency* section, there are two types
    of data drifts – *feature drifts* and *concept drifts*. Feature drifts happen
    when the statistical properties of the features or the independent variables change
    due to an unforeseen reason. In comparison, concept drift occurs when the target
    class variable, which the model is trying to predict, changes its initial relationship
    with the input features in a dynamic setting. In both cases, there is a statistical
    change in the underlying data. So, my recommendation for detecting drifts is to
    use the data profiling method discussed in the previous section.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如在*分析数据一致性*一节中讨论的，存在两种类型的数据漂移——*特征漂移*和*概念漂移*。特征漂移发生在由于不可预见的原因导致特征或独立变量的统计属性发生变化时。相比之下，概念漂移发生在模型试图预测的目标类别变量在动态环境中改变其与输入特征的初始关系时。在这两种情况下，底层数据都发生了统计变化。因此，我推荐检测漂移时使用上一节中讨论的数据概要分析方法。
- en: A real-time monitoring dashboard is always helpful for any real-time application
    to monitor any drift. In the dashboard, try to have necessary visualizations for
    each class and each feature, comparing the statistical profile values with the
    actual live values flowing into the trained model.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何实时应用程序来说，实时监控仪表板总是有助于监控任何漂移。在仪表板中，尝试为每个类别和每个特征提供必要的可视化，比较统计概要值与实际流入训练模型的实时值。
- en: Particularly for concept drifts, comparing the correlations of the features
    with the target outcome is extremely helpful. Since drifts can arise after a certain
    period of time or even during a specific point in time due to external reasons,
    it is always advisable to monitor the statistical properties of the inference
    data in a time window period (for instance, for 50 consecutive data points or
    100 consecutive data points) rather than a continuous cumulative basis. For the
    purposes of feedback, necessary alerts and triggers can be set when abnormal data
    points are detected in the inference data, which might indicate the presence of
    data drift.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其对于概念漂移，比较特征与目标结果之间的相关性非常有帮助。由于外部原因，漂移可能在某个时间段后甚至可能在特定时间点出现，因此始终建议在时间窗口期内（例如，50个连续数据点或100个连续数据点）监控推理数据的统计特性，而不是基于连续累积的基础。当在推理数据中检测到异常数据点时，可以设置必要的警报和触发器，这可能表明存在数据漂移。
- en: Selection of statistical measures
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计指标的选择
- en: Sometimes, the selection of statistical measures can be difficult. So, we usually
    go for some popular distribution metrics to detect the presence of data drift
    using a quantitative approach. One such metric is called **trust score distribution**
    ([https://arxiv.org/abs/1805.11783](https://arxiv.org/abs/1805.11783)).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，选择统计指标可能很困难。因此，我们通常选择一些流行的分布指标，通过定量方法检测数据漂移。其中一个指标被称为**信任度分布**([https://arxiv.org/abs/1805.11783](https://arxiv.org/abs/1805.11783))。
- en: 'The following diagram shows the trust score distribution plot obtained using
    the *Deepchecks Python framework*:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图显示了使用**Deepchecks Python 框架**获得的信任度分布图：
- en: '![Figure 3.3 – An example of the trust score distribution between the training
    dataset and the inference dataset'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.3 – 训练数据集和推理数据集之间信任度分布的示例'
- en: '](img/B18216_03_3.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18216_03_3.jpg](img/B18216_03_3.jpg)'
- en: Figure 3.3 – An example of the trust score distribution between the training
    dataset and the inference dataset
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – 训练数据集和推理数据集之间信任度分布的示例
- en: Trust score is a distribution metric used to measure the agreement between the
    ML classifier on the training set and an updated **k-Nearest Neighbor** (**kNN**)
    classifier on the inference dataset. The preceding diagram shows a trust score
    distribution plot between the training dataset and the inference dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 信任度是一种分布指标，用于衡量训练集上的机器学习分类器与推理数据集上的更新**k-最近邻**（**kNN**）分类器之间的协议。前面的图显示了训练数据集和推理数据集之间的信任度分布图。
- en: Ideally, the distributions should be almost the same for both the train and
    test datasets. However, if the trust score distribution for the inference set
    is skewed toward the extreme left, this indicates that the trained model has less
    confidence in the inference data, thereby alluding to the presence of drift. If
    the distribution of the trust score on the inference data is skewed toward the
    extreme right, there might be some problem with the model and there is a high
    probability of data leakage, as ideally, the trained model cannot be more confident
    in the test data in comparison to the training data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，训练和测试数据集的分布应该几乎相同。然而，如果推理数据集的信任度分布偏向极端左侧，这表明训练模型对推理数据的信心较低，从而暗示存在漂移。如果推理数据上信任度分布偏向极端右侧，可能存在模型问题，并且数据泄露的可能性很高，因为理想情况下，训练模型对测试数据的信心不应比训练数据更高。
- en: To detect feature drifts on categorical features, the popular choice of metric
    is the **population stability index** (**PSI**) ([https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf](https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf)).
    This is a statistical method used to measure the shift in a variable over a period
    of time. If the overall drift score is more than 0.2 or 20%, then the drift is
    considered to be significant, establishing the presence of feature drift.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要检测分类特征上的特征漂移，常用的指标是**人口稳定性指数**（**PSI**）([https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf](https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf))。这是一种统计方法，用于测量变量在一定时间内的变化。如果整体漂移分数超过0.2或20%，则认为漂移是显著的，从而确定存在特征漂移。
- en: To detect feature drifts in numeric features, the **Wasserstein metric** ([https://kowshikchilamkurthy.medium.com/wasserstein-distance-contraction-mapping-and-modern-rl-theory-93ef740ae867](https://kowshikchilamkurthy.medium.com/wasserstein-distance-contraction-mapping-and-modern-rl-theory-93ef740ae867))
    is the popular choice. This is a distance function for measuring the distance
    between two probability distributions. Similar to PSI, if the drift score using
    the Wasserstein metric is higher than 20%, this is considered to be significant
    and the numerical feature is considered to have feature drift.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要检测数值特征中的特征漂移，**Wasserstein度量**（[https://kowshikchilamkurthy.medium.com/wasserstein-distance-contraction-mapping-and-modern-rl-theory-93ef740ae867](https://kowshikchilamkurthy.medium.com/wasserstein-distance-contraction-mapping-and-modern-rl-theory-93ef740ae867)）是流行的选择。这是一个用于测量两个概率分布之间距离的函数。类似于PSI，如果使用Wasserstein度量得到的漂移分数高于20%，这被认为是显著的，并且数值特征被认为存在特征漂移。
- en: 'The following diagram illustrates feature drift estimation using the Wasserstein
    (Earth Mover''s) distance and **Predictive Power Score** (**PPS**) with the Deepchecks
    framework:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了使用Wasserstein（地球迁移）距离和**预测能力分数**（**PPS**）在Deepchecks框架中进行的特征漂移估计：
- en: '![Figure 3.4 – Feature drift estimation using Wasserstein (Earth Mover''s)
    distance and PPS of features'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.4 – 使用Wasserstein（地球迁移）距离和特征PPS进行特征漂移估计'
- en: '](img/B18216_03_4.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_03_4.jpg)'
- en: Figure 3.4 – Feature drift estimation using Wasserstein (Earth Mover's) distance
    and PPS of features
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – 使用Wasserstein（地球迁移）距离和特征PPS进行特征漂移估计
- en: Similar concept drifts can also be detected using these metrics. For regression
    problems, the Wasserstein metric is effective, while for classification problems
    PSI is more effective. You can see the application of these methods on a practical
    dataset at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03).
    Additionally, there are other statistical methods that are extremely useful for
    detecting data drifts such as **Kullback-Leibler Divergence** (**KL Divergence**),
    the **Bhattacharyya distance**, **Jensen-Shannon Divergence** (**JS Divergence**),
    and more.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些指标也可以检测到类似的概念漂移。对于回归问题，Wasserstein度量是有效的，而对于分类问题，PSI更有效。您可以在[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03)上的实际数据集上看到这些方法的运用。此外，还有其他非常有用的统计方法，如**Kullback-Leibler散度**（**KL散度**）、**Bhattacharyya距离**、**Jensen-Shannon散度**（**JS散度**）等，可用于检测数据漂移。
- en: In this chapter, our focus is not on learning these metrics, but I strongly
    recommend you to take a look at the *Reference* section to find out more about
    these metrics and their application for finding data drifts. These methods are
    also applicable to images. Instead of structured feature values, the distributions
    of the pixel intensity value of the image datasets are used to detect drifts.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们的重点不是学习这些指标，但我强烈建议您查看**参考**部分，以了解更多关于这些指标及其在检测数据漂移中的应用。这些方法也适用于图像。与结构化特征值不同，图像数据集的像素强度值分布被用来检测漂移。
- en: Now that we are aware of certain effective ways in which to detect drifts, *what
    do we do when we have identified the presence of drifts?* The first step is to
    alert our stakeholders if the ML system is already in production. Incorrect predictions
    due to data drift can impact many end users, which might, ultimately, lead to
    the loss of trust of the end users. The next step is to check whether the drift
    is *temporary*, *seasonal*, or *permanent* in nature. Analysis of the nature of
    the drift can be challenging, but if the changes that are causing the drift can
    be identified and reverted, then that is the best solution.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了一些有效的检测漂移的方法，*当我们确定了漂移的存在时，我们该怎么办？* 第一步是如果机器学习系统已经在生产中，则通知我们的利益相关者。由于数据漂移导致的错误预测可能会影响许多最终用户，这最终可能导致最终用户对失去信任。下一步是检查漂移是否是*暂时的*、*季节性的*或*永久的*。漂移性质的分析可能具有挑战性，但如果可以识别并撤销导致漂移的变化，那么这就是最好的解决方案。
- en: If the drift is temporary, the first step is to identify the temporary change
    that caused the drift and then revert the changes. For seasonal drifts, seasonal
    changes to the data should be accounted for during the training process or as
    an additional preprocessing step to normalize any seasonal effects on the data.
    This is so that the model is aware of the seasonal pattern in the data. However,
    if the drift is permanent, then the only option is to retrain the model on the
    new data and deploy the newly trained model for the production system.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果漂移是暂时的，第一步是确定导致漂移的临时变化，然后撤销这些更改。对于季节性漂移，应在训练过程中或作为额外的预处理步骤考虑数据中的季节性变化，以归一化数据中的任何季节性影响。这样，模型就会意识到数据中的季节性模式。然而，如果漂移是永久的，那么唯一的选择就是在新数据上重新训练模型，并将新训练的模型部署到生产系统中。
- en: In the context of XAI, the detection of drifts can justify the failure of any
    ML model or algorithm and helps to improve the model by identifying the root cause
    of the failure. In the next section, we will discuss another data-centric quality
    inspection step that can be performed to ensure the robustness of ML models.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在XAI的背景下，漂移的检测可以证明任何ML模型或算法的失败，并有助于通过识别失败的根本原因来改进模型。在下一节中，我们将讨论另一个可以执行的数据中心化质量检查步骤，以确保ML模型的鲁棒性。
- en: Checking adversarial robustness
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查对抗鲁棒性
- en: In the previous section, we discussed the importance of anticipating and monitoring
    drifts for any production-level ML system. Usually, this type of monitoring is
    done after the model has been deployed in production. But even before the model
    is deployed in production, it is extremely critical to check for the **adversarial
    robustness** of the model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了预测和监控任何生产级ML系统漂移的重要性。通常，这种监控是在模型部署到生产之后进行的。但在模型部署到生产之前，检查模型的**对抗鲁棒性**是极其关键的。
- en: Most ML models are prone to adversarial attacks or an injection of noise to
    the input data, causing the model to fail by making incorrect predictions. The
    degree of adversarial attacks increases with the model's complexity, as complex
    models are very sensitive to noisy data samples. So, checking for adversarial
    robustness is about evaluating how sensitive the trained model is toward adversarial
    attacks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数ML模型都容易受到对抗攻击或输入数据中噪声注入的影响，导致模型通过做出错误预测而失败。对抗攻击的程度随着模型复杂性的增加而增加，因为复杂模型对噪声数据样本非常敏感。因此，检查对抗鲁棒性就是评估训练好的模型对对抗攻击的敏感程度。
- en: In this section, first, we will try to understand the impact of adversarial
    attacks on the model and why this is important in the context of XAI. Then, we
    will discuss certain techniques that we can use to increase the adversarial robustness
    of ML models. Finally, we will discuss the methods that are used to evaluate the
    adversarial robustness of models, which can be performed as an exercise before
    deploying ML models into production, and how this forms a vital part of explainable
    ML systems.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，首先，我们将尝试理解对抗攻击对模型的影响以及为什么这在XAI的背景下很重要。然后，我们将讨论我们可以使用的某些技术来提高ML模型的对抗鲁棒性。最后，我们将讨论用于评估模型对抗鲁棒性的方法，这可以在将ML模型部署到生产之前作为一个练习来完成，以及这是可解释ML系统的一个关键部分。
- en: Impact of adversarial attacks
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗攻击的影响
- en: Over the past few years, adversarial attacks have been a cause of great concern
    for the AI community. These attacks can inject noise to modify the input data
    in such a way that a human observer can easily identify the correct outcome but
    an ML model can be easily fooled and start predicting completely incorrect outcomes.
    The extent of the attack depends on the attacker's access to the model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，对抗攻击一直是AI社区关注的焦点。这些攻击可以通过向输入数据注入噪声来修改数据，使得人类观察者可以轻易地识别出正确的结果，但ML模型却很容易被欺骗并开始预测完全错误的结果。攻击的强度取决于攻击者对模型的访问权限。
- en: Usually, in production systems, the trained model (especially the model parameters)
    is fixed and cannot be modified. But the inference data flowing into the model
    can be polluted with abrupt noise signals, thus making the model misclassify.
    Human experts are extremely efficient in filtering out the injected noise, but
    ML models fail to isolate the noise from the actual data if the model has not
    been exposed to such noisy samples during the training phase. Sometimes, these
    attacks can be *targeted* attacks, too.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在生产系统中，训练好的模型（尤其是模型参数）是固定的，不能修改。但是，流入模型的可疑数据可能会被突然的噪声信号污染，从而使模型分类错误。人类专家在过滤掉注入的噪声方面非常高效，但如果模型在训练阶段没有接触过这种噪声样本，那么机器学习模型就无法从实际数据中隔离噪声。有时，这些攻击也可能是**有针对性的**攻击。
- en: For example, if a face recognition system allows access to only a specific person,
    adversarial attacks can modify the image of any person to a specific person by
    introducing some noise. In this case, an adversarial algorithm would have to be
    trained using the target sample to construct the noise signal. There are other
    forms of adversarial attacks as well, which can modify the model during the training
    phase itself. However, since we are discussing this in the context of XAI, we
    will concentrate on the impact of adversarial effects on trained ML models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个面部识别系统只允许特定人员访问，那么通过引入一些噪声，对抗攻击可以将任何人的图像修改成特定的人。在这种情况下，需要使用目标样本来训练对抗算法，以构建噪声信号。还有其他形式的对抗攻击，它们可以在训练阶段本身修改模型。然而，由于我们是在XAI的背景下讨论这个问题，我们将集中讨论对抗效果对训练好的机器学习模型的影响。
- en: 'There are different types of adversarial attacks that can impact trained ML
    models:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同类型的对抗攻击可以影响训练好的机器学习模型：
- en: '**Fast Gradient Sign Method** (**FGSM**): FGSM is one such method that uses
    gradients of deep learning models to learn adversarial samples. For image classifiers,
    this can be a common problem, as FGSM creates perturbations on the pixel values
    of an image by adding or subtracting pixel intensity values depending on the *direction
    of the gradient descent* of the model. This can fool the model to misclassify
    and severely affect the performance of the model, but it does not create any problem
    for a human observer. Even if the modification appears to be negligible, the method
    adds an evenly distributed noise that is enough to cause the misclassifications.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**快速梯度符号方法**（**FGSM**）：FGSM是这种方法之一，它使用深度学习模型的梯度来学习对抗样本。对于图像分类器来说，这可能是一个常见问题，因为FGSM通过添加或减去像素强度值来在图像的像素值上创建扰动，这取决于模型的**梯度下降方向**。这可能会欺骗模型进行错误分类，并严重影响模型性能，但这对人类观察者来说不会造成任何问题。即使修改看起来微不足道，该方法添加的均匀分布噪声足以导致错误分类。'
- en: '**The Carlini & Wagner** (**C&W**) **attack**: Another common adversarial attack
    is the C&W attack. This method uses the three norm-based distance metrics (![](img/B18216_03_001.png),
    ![](img/B18216_03_002.png), and ![](img/B18216_03_003.png)) to find adversarial
    examples, such that the distance between the adversarial example and the original
    sample is minimal. Detecting C&W attacks is more difficult than FGSM attacks.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Carlini & Wagner**（**C&W**）**攻击**：另一种常见的对抗攻击是C&W攻击。这种方法使用基于三个范数的距离度量（![](img/B18216_03_001.png)，![](img/B18216_03_002.png)，和![](img/B18216_03_003.png)）来寻找对抗示例，使得对抗示例与原始样本之间的距离最小。检测C&W攻击比检测FGSM攻击更困难。'
- en: '**Targeted adversarial patch attacks**: Sometimes, injecting noise (that is,
    the addition of noisy random pixels) into the entire image is not necessary. The
    addition of a noisy image segment to only a small portion of the image can be
    equally harmful to the model. Targeted adversarial patch attacks can generate
    a small adversarial patch that is then superimposed with the original sample,
    thus occluding the key features of the data and making the model classify incorrectly.
    There are other forms of adversarial attacks too, and many more new methods can
    be discovered in the future. However, the impact will still be the same.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标对抗补丁攻击**：有时，在整个图像中注入噪声（即添加噪声随机像素）并不是必要的。仅向图像的一小部分添加噪声图像片段，对模型同样有害。目标对抗补丁攻击可以生成一个小型的对抗补丁，然后将其叠加到原始样本上，从而遮挡数据的特征，使模型分类错误。还有其他形式的对抗攻击，未来还可以发现更多新方法。然而，影响仍然相同。'
- en: 'The following diagram shows how different adversarial attacks can introduce
    noise in an image, thereby making it difficult for the model to give correct predictions.
    Despite the addition of noise, we, as human beings, can still predict the correct
    outcome, but the trained model is completely fooled by adversarial attacks:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了不同的对抗攻击如何向图像中引入噪声，从而使模型难以给出正确的预测。尽管添加了噪声，但作为人类，我们仍然可以预测正确的结果，但训练好的模型却被对抗攻击完全欺骗了：
- en: '![Figure 3.5 – Adversarial attacks on the inference data leading to incorrect
    model predictions'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.5 – 对推理数据进行的对抗攻击导致模型预测错误'
- en: '](img/B18216_03_5.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_03_5.jpg](img/B18216_03_5.jpg)'
- en: Figure 3.5 – Adversarial attacks on the inference data leading to incorrect
    model predictions
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 对推理数据进行的对抗攻击导致模型预测错误
- en: Adversarial attacks can force an ML model to produce incorrect outcomes that
    can severely affect end users. In the next section, let's try to explore ways
    to increase the adversarial robustness of models.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击可以迫使机器学习模型产生错误的输出，这可能会严重影响最终用户。在下一节中，让我们尝试探索提高模型对抗鲁棒性的方法。
- en: Methods to increase adversarial robustness
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高对抗鲁棒性的方法
- en: 'In production systems, adversarial attacks can mostly inject noise into the
    inference data. So, to reduce the impact of adversarial attacks, we would either
    need to teach the model to filter out the noise or expose the presence of noisy
    samples during the training process or train the models to detect adversarial
    samples:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产系统中，对抗攻击通常会在推理数据中注入噪声。因此，为了减少对抗攻击的影响，我们可能需要教会模型过滤掉噪声，或在训练过程中暴露噪声样本的存在，或者训练模型检测对抗样本：
- en: The easiest option is to filter out the noise as a defense mechanism to increase
    the adversarial robustness of ML models. Any adversarial noise results in an abrupt
    change in the input samples. In order to filter out any abrupt change from any
    signal, we usually try to apply a smoothing filter such as **Spatial smoothing**.
    Spatial smoothing is equivalent to the **blurring operation** in images and is
    used to reduce the impact of the adversarial attack. From experience, I have observed
    that an *adaptive median spatial smoothing* ([https://homepages.inf.ed.ac.uk/rbf/HIPR2/median.htm](https://homepages.inf.ed.ac.uk/rbf/HIPR2/median.htm)),
    which works at a local level through a windowing approach is more effective than
    smoothing at a global level. The statistical measure of the median is always more
    effective in filtering out noise or outliers from the data.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最简单的方法是将噪声作为防御机制过滤掉，以增加机器学习模型的对抗鲁棒性。任何对抗噪声都会导致输入样本发生急剧变化。为了过滤掉任何信号中的急剧变化，我们通常尝试应用一种平滑滤波器，例如**空间平滑**。空间平滑在图像中相当于**模糊操作**，用于减少对抗攻击的影响。根据经验，我发现一种*自适应中值空间平滑*（[https://homepages.inf.ed.ac.uk/rbf/HIPR2/median.htm](https://homepages.inf.ed.ac.uk/rbf/HIPR2/median.htm)），通过窗口方法在局部层面工作，比全局层面的平滑更有效。中值统计量在过滤掉数据中的噪声或异常值方面总是更有效。
- en: Another approach to increase adversarial robustness is by introducing adversarial
    examples during the training process. By using the technique of **data augmentation**,
    we can generate adversarial samples from the original data and include the augmented
    data during the training process. If training the model from scratch using augmented
    adversarial samples is not feasible, then the trained ML model can actually be
    fine-tuned on the adversarial samples using **transfer learning**. Here, the trained
    model weights can be taken to be fine-tuned on the newer samples.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高对抗鲁棒性的另一种方法是，在训练过程中引入对抗示例。通过使用**数据增强**技术，我们可以从原始数据生成对抗样本，并在训练过程中包含增强数据。如果从头开始使用增强对抗样本进行训练不可行，那么训练好的机器学习模型可以使用**迁移学习**对对抗样本进行微调。在这里，可以将训练好的模型权重用于在新样本上进行微调。
- en: The process of training a model with adversarial samples is often referred to
    as **adversarial training**. We can even train a separate model using adversarial
    training, just to detect adversarial samples from original samples, and use it
    along with the main model to trigger alerts if adversarial samples are generated.
    The idea of exposing the model to possible adversarial samples is similar to the
    idea of *stress testing* in cyber security ([https://ieeexplore.ieee.org/document/6459909](https://ieeexplore.ieee.org/document/6459909)).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用对抗样本训练模型的过程通常被称为**对抗训练**。我们甚至可以使用对抗训练来训练一个单独的模型，仅用于从原始样本中检测对抗样本，并将其与主模型一起使用，以在生成对抗样本时触发警报。将模型暴露于可能的对抗样本的想法类似于网络安全中的*压力测试*（[https://ieeexplore.ieee.org/document/6459909](https://ieeexplore.ieee.org/document/6459909)）。
- en: '*Figure 3.6* illustrates how spatial smoothing can be used as a defense mechanism
    to minimize the impact of adversarial attacks:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3.6* 展示了如何使用空间平滑作为防御机制来最小化对抗攻击的影响：'
- en: '![Figure 3.6 – Using spatial smoothing as a defense mechanism to minimize the
    impact of adversarial attacks'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.6 – 使用空间平滑作为防御机制以最小化对抗攻击的影响'
- en: '](img/B18216_03_6.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_03_6.jpg)'
- en: Figure 3.6 – Using spatial smoothing as a defense mechanism to minimize the
    impact of adversarial attacks
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 使用空间平滑作为防御机制以最小化对抗攻击的影响
- en: Using the methods that we have discussed so far, we can increase the adversarial
    robustness of trained ML models to a great extent. In the next section, we will
    try to explore ways to evaluate the adversarial robustness of ML models.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们迄今为止讨论的方法，我们可以极大地提高训练的机器学习模型的对抗鲁棒性。在下一节中，我们将尝试探索评估机器学习模型对抗鲁棒性的方法。
- en: Evaluating adversarial robustness
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估对抗鲁棒性
- en: Now that we have learned certain approaches in which to defend against adversarial
    attacks, the immediate question that might come to mind is *how can we measure
    the adversarial robustness of models?*
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了某些防御对抗攻击的方法，紧接着可能想到的问题是*我们如何衡量模型的对抗鲁棒性？*
- en: 'Unfortunately, I have never come across any dedicated metric to quantitatively
    measure the adversarial robustness of ML models, but it is an important research
    topic for the AI community. The most common approaches by which data scientists
    evaluate the adversarial robustness of ML models are **stress testing** and **segmented
    stress testing**:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我从未遇到过任何专门用于定量衡量机器学习模型对抗鲁棒性的指标，但这对于人工智能社区来说是一个重要的研究课题。数据科学家评估机器学习模型对抗鲁棒性的最常见方法包括**压力测试**和**分段压力测试**：
- en: In **stress testing**, adversarial examples are generated by FGSM or C&W methods.
    Following this, the model's accuracy is measured on the adversarial examples and
    compared to the model accuracy obtained with the original data. The strength of
    the adversarial attack can also be increased or decreased to observe the variation
    of the model performance with the attack strength. Sometimes, a particular class
    or feature can become more vulnerable to adversarial attacks than the entire dataset.
    In those scenarios, segmented stress testing is beneficial.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**压力测试**中，通过FGSM或C&W方法生成对抗示例。随后，在对抗示例上测量模型的准确性，并将其与使用原始数据获得的模型准确性进行比较。对抗攻击的强度也可以增加或减少，以观察模型性能随攻击强度的变化。有时，某个特定类别或特征可能比整个数据集更容易受到对抗攻击。在这些情况下，分段压力测试是有益的。
- en: In **segmented stress testing**, instead of measuring the adversarial robustness
    of the entire model on the entire dataset, segments of the dataset (either for
    specific classes or for specific features) are considered to compare the model
    robustness with the adversarial attack strengths. Adversarial examples can be
    generated with random noise or with Gaussian noise. For certain datasets, quantitative
    metrics such as the **Peak Signal-to-Noise ratio** (**PSNR**) ([https://www.ni.com/nl-be/innovations/white-papers/11/peak-signal-to-noise-ratio-as-an-image-quality-metric.html](https://www.ni.com/nl-be/innovations/white-papers/11/peak-signal-to-noise-ratio-as-an-image-quality-metric.html))
    and **Erreur Relative Globale Adimensionnelle de Synthese** (**ERGAS**) ([https://www.researchgate.net/figure/Erreur-Relative-Globale-Adimensionnelle-de-Synthese-ERGAS-values-of-fused-images_tbl1_248978216](https://www.researchgate.net/figure/Erreur-Relative-Globale-Adimensionnelle-de-Synthese-ERGAS-values-of-fused-images_tbl1_248978216))
    are used to measure the data or signal quality. Otherwise, the adversarial robustness
    of ML models can be quantitatively inspected by the model's prediction of adversarial
    samples.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**分段压力测试**中，不是在整个数据集上测量整个模型的对抗鲁棒性，而是考虑数据集的各个部分（无论是特定类别还是特定特征），以比较模型的鲁棒性与对抗攻击的强度。可以通过随机噪声或高斯噪声生成对抗示例。对于某些数据集，可以使用诸如**峰值信噪比**（**PSNR**）([https://www.ni.com/nl-be/innovations/white-papers/11/peak-signal-to-noise-ratio-as-an-image-quality-metric.html](https://www.ni.com/nl-be/innovations/white-papers/11/peak-signal-to-noise-ratio-as-an-image-quality-metric.html))和**综合相对误差**（**ERGAS**）([https://www.researchgate.net/figure/Erreur-Relative-Globale-Adimensionnelle-de-Synthese-ERGAS-values-of-fused-images_tbl1_248978216](https://www.researchgate.net/figure/Erreur-Relative-Globale-Adimensionnelle-de-Synthese-ERGAS-values-of-fused-images_tbl1_248978216))等量化指标来衡量数据或信号质量。否则，可以通过模型对对抗样本的预测来定量检查机器学习模型的对抗鲁棒性。
- en: More than the method of evaluating adversarial robustness, inspecting adversarial
    robustness and monitoring the detection of adversarial attacks is an essential
    part of explainable ML systems. Next, let's discuss the importance of measuring
    data forecastability as a method to provide data-centric model explainability.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 除了评估对抗鲁棒性的方法之外，检查对抗鲁棒性和监测对抗性攻击的检测是可解释机器学习系统的一个基本部分。接下来，让我们讨论测量数据可预测性的重要性，作为提供以数据为中心的模型可解释性的方法。
- en: Measuring data forecastability
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量数据可预测性
- en: So far, we have learned about the importance of analyzing data by inspecting
    its consistency and purity, looking for monitoring drifts, and checking for any
    adversarial attacks to explain the working of ML models. But some datasets are
    extremely complex and, hence, training accurate models even with complex algorithms
    is not feasible. If the trained model is not accurate, it is prone to make incorrect
    predictions. Now the question is *how do we gain the trust of our end users if
    we know that the trained model is not extremely accurate in making the correct
    predictions?*
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经通过检查数据的一致性和纯度、寻找监测漂移以及检查任何对抗性攻击来了解分析数据的重要性，以解释机器学习模型的工作原理。但是，一些数据集非常复杂，因此即使使用复杂的算法，训练准确的模型也是不可行的。如果训练的模型不准确，它就倾向于做出错误的预测。现在的问题是，如果我们知道训练的模型在做出正确预测方面并不非常准确，我们如何赢得最终用户的信任？
- en: I would say that the best way to gain trust is by being transparent and clearly
    communicating what is feasible. So, measuring **data forecastability** and communicating
    the model's efficiency to end users helps to set the right expectation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我会说，获得信任的最好方式是保持透明，并清楚地传达什么是可行的。因此，测量**数据可预测性**并向最终用户传达模型的效率有助于设定正确的期望。
- en: Data forecastability is an estimation of the model's performance using the underlying
    data. For example, let's suppose we have a model to predict the stock price of
    a particular company. The stock price data that is being modeled by the ML algorithm
    can predict the stock price with a maximum of 60% accuracy. Beyond that point,
    it is not practically possible to generate a more accurate outcome using the given
    dataset.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可预测性是对模型性能使用底层数据进行估计。例如，假设我们有一个预测特定公司股价的模型。被机器学习算法建模的股价数据可以以最多60%的准确率预测股价。超过这个点，使用给定的数据集生成更准确的预测结果在实际上是不可能的。
- en: But let's say that if other external factors are considered to supplement the
    current data, the model's accuracy can be boosted. This proves that it is not
    the ML algorithm that is limiting the performance of the system, but rather the
    dataset that is used for modeling does not have sufficient information to get
    a better model performance. Hence, it is a limitation of the dataset that can
    be estimated by measure of data forecastability.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 但假设如果考虑其他外部因素来补充当前数据，可以提高模型的准确性。这证明限制系统性能的不是机器学习算法，而是用于建模的数据集没有足够的信息来获得更好的模型性能。因此，这是可以通过数据可预测性度量来估计的数据集限制。
- en: 'The following diagram shows a number of model evaluation visualizations that
    can be used to analyze data forecastability using the Deepchecks framework:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了可以使用Deepchecks框架分析数据可预测性的多个模型评估可视化。
- en: '![Figure 3.7 – Data forecastability using the model evaluation report and the
    Deepchecks framework'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 3.7 – 使用模型评估报告和Deepchecks框架估计数据可预测性'
- en: '](img/B18216_03_7.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_03_7.jpg](img/B18216_03_7.jpg)'
- en: Figure 3.7 – Data forecastability using the model evaluation report and the
    Deepchecks framework
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 – 使用模型评估报告和Deepchecks框架估计数据可预测性
- en: Next, let's discuss how to estimate data forecastability.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论如何估计数据可预测性。
- en: Estimating data forecastability
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 估计数据可预测性
- en: 'Data forecastability is estimated using the model evaluation metrics. Data
    forecastability can also be measured by performing model error analysis. The choice
    of the metrics depends on the type of dataset and the type of problem being solved.
    For example, take a look at the following list:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可预测性是通过模型评估指标来估计的。数据可预测性也可以通过执行模型错误分析来衡量。指标的选择取决于数据集的类型和要解决的问题的类型。例如，看看以下列表：
- en: For time series data, data forecastability is obtained by metrics such as the
    **mean absolute percentage error** (**MAPE**), the **symmetric mean absolute percentage
    error** (**SMAPE**), the **coefficient of variation** (**CoV**), and more.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于时间序列数据，数据可预测性通过如**平均绝对百分比误差**（MAPE）、**对称平均绝对百分比误差**（SMAPE）、**变异系数**（CoV）等指标来获得。
- en: For classification problems, I usually go for **ROC-AUC Scores**, the **confusion
    matrix**, **precision**, **recall**, and **F1 scores** along with **accuracy**.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于分类问题，我通常选择**ROC-AUC分数**、**混淆矩阵**、**精确度**、**召回率**和**F1分数**以及**准确度**。
- en: For regression problems, we can look at the **mean square error** (**MSE**),
    the **R2 score**, the **root mean square error** (**RMSE**), the **sum of squared
    errors** (**SSE**), and more.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于回归问题，我们可以查看**均方误差**（MSE）、**R2分数**、**均方根误差**（RMSE）、**平方误差和**（SSE）等。
- en: You might have already used most of these metrics to evaluate trained ML models.
    Data forecastability is not just about evaluating trained models according to
    your choice of metric, but is the measure of predictability of the model using
    the given dataset.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经使用过这些指标中的大多数来评估训练好的机器学习模型。数据可预测性不仅仅是根据你选择的指标来评估训练模型，而是使用给定数据集来衡量模型的预测性。
- en: Let's suppose you are applying three different ML algorithms such as decision
    trees, support vector machine (SVM), and random forests for a classification problem,
    and your choice of metric is recall. This is because your goal is to minimize
    the impact of false positives. After rigorous training and validation on the unseen
    data, you are able to obtain recall scores of 70% with decision tree, 85% with
    SVM, and 90% with random forest. What do you think your data forecastability will
    be? Is it 70%, 90%, or 81.67% (the average of the three scores)?
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在应用三种不同的机器学习算法，如决策树、支持向量机（SVM）和随机森林，来解决一个分类问题，并且你选择的指标是召回率。这是因为你的目标是最小化假阳性的影响。经过对未见数据的严格训练和验证后，你能够获得决策树的70%召回率，SVM的85%，随机森林的90%。你认为你的数据可预测性会是多少？是70%，90%，还是81.67%（三个分数的平均值）？
- en: I would say that the correct answer is between 70% and 90%. It is always better
    to consider forecastability as a ballpark estimate, as providing a range of values
    rather than a single value gives an idea of the best-case and worst-case scenarios.
    Communicating about the data forecastability increases the confidence of the end
    stakeholders in ML systems. If the end users are consciously aware that the algorithm
    is only 70% accurate, they will not blindly trust the model even if the system
    predicts incorrectly. The end users would be more considerate if the model outcome
    does not match the actual outcome when they are aware of the model's limitations.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我会说，正确答案在70%到90%之间。总是最好将可预测性视为一个大致估计，因为提供一个值范围而不是单个值可以给出最佳和最坏情况场景的印象。关于数据可预测性的沟通增加了最终利益相关者对机器学习系统的信心。如果最终用户意识到算法只有70%的准确性，即使系统预测错误，他们也不会盲目信任模型。如果用户意识到模型的局限性，当模型结果与实际结果不符时，他们会更加谨慎。
- en: Most ML systems in production have started using prediction probability or model
    confidence as a measure of data forecastability, which is communicated to the
    end users. For example, nowadays, most weather forecasting applications show that
    there is a certain percentage of chance (or probability) for rainfall or snowfall.
    Therefore, data forecastability increases the explainability of AI algorithms
    by setting up the right expectation for the accuracy of the predicted outcome.
    It is not just the measure of the model performance, but rather a measure of the
    predictability of a model which is trained on a specific dataset.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 生产中的大多数机器学习系统已经开始使用预测概率或模型置信度作为数据可预测性的衡量标准，并将其传达给最终用户。例如，如今，大多数天气预报应用都会显示降雨或降雪有一定概率（或概率）。因此，数据可预测性通过为预测结果的准确性设定正确的期望，增加了AI算法的可解释性。这不仅仅是模型性能的衡量标准，而是特定数据集上训练的模型可预测性的衡量标准。
- en: This brings us to the end of the chapter. Let's summarize what we have discussed
    in the following section.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这就带我们来到了本章的结尾。让我们在下面的部分中总结一下我们讨论的内容。
- en: Summary
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Now, let's try to summarize what you have learned in this chapter. In this chapter,
    we focused on data-centric approaches for XAI. We learned the importance of explaining
    black-box models with respect to the underlying data, as data is the central part
    of any ML model. The concept of data-centric XAI might be new to many of you,
    but it is an important area of research for the entire AI community. Data-centric
    XAI can provide explainability to the black-box model in terms of data volume,
    data consistency, and data purity.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试总结一下你在本章中学到的内容。在本章中，我们专注于XAI的数据中心方法。我们学习了从底层数据解释黑盒模型的重要性，因为数据是任何机器学习模型的中心部分。数据为中心的XAI的概念可能对你们中的许多人来说都是新的，但它是对整个AI社区的一个重要研究领域。数据为中心的XAI可以在数据量、数据一致性和数据纯度方面为黑盒模型提供可解释性。
- en: Data-centric explainability methods are still active research topics, and there
    is no single Python framework that exists that covers all of the various aspects
    of data-centric XAI. Please explore the supplementary Jupyter notebook tutorials
    provided at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03)
    to gain more practical knowledge on this topic.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中心的可解释性方法是活跃的研究课题，目前还没有单一的Python框架涵盖数据为中心的XAI的所有各个方面。请探索在[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03)提供的补充Jupyter笔记本教程，以获得更多关于这个主题的实用知识。
- en: We learned about the idea of thorough data inspection and data profiling to
    estimate the consistency of training data and inference data. Monitoring data
    drifts for production ML systems is also an essential part of the data-centric
    XAI process. Apart from data drifts, estimating the adversarial robustness of
    ML models and the detection of adversarial attacks form an important part of the
    process.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了彻底检查数据和数据概要化的概念，以估计训练数据和推理数据的一致性。监控生产机器学习系统的数据漂移也是数据为中心的XAI过程中的一个重要部分。除了数据漂移之外，估计机器学习模型的对抗鲁棒性和检测对抗攻击也是这个过程的重要组成部分。
- en: Finally, we learned about the importance of data forecastability to set the
    right expectation to end stakeholders about what the model can achieve and how
    this is a necessary practice that can increase the trust of our end users.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学习了数据可预测性的重要性，这对于设定正确的期望，让利益相关者了解模型可以实现什么，以及这是增加我们最终用户信任的必要实践至关重要。
- en: You have been introduced to many statistical concepts in this chapter. Covering
    everything about each statistical method is beyond the scope of this chapter.
    However, I strongly recommend that you go through the reference links shared to
    understand these topics in greater depth.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了许多统计概念。涵盖每种统计方法的全部内容超出了本章的范围。然而，我强烈建议您查看共享的参考链接，以更深入地了解这些主题。
- en: This brings us to the end of part 1 of this book, in which you have been exposed
    to the conceptual understanding of certain key topics of XAI. From the next chapter
    onward, we will start exploring popular Python frameworks for applying the concepts
    of XAI to practical real-world problems. In the next chapter, we will cover an
    important XAI framework called LIME and examine how it can be used in practice.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的第一部分到此结束，您已经接触到了XAI某些关键主题的概念理解。从下一章开始，我们将开始探索将XAI的概念应用于实际现实问题的流行Python框架。在下一章中，我们将介绍一个重要的XAI框架LIME，并探讨其在实践中的应用。
- en: References
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'To gain additional information about the topics in this chapter, please refer
    to the following resources:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 若要获取本章主题的更多信息，请参考以下资源：
- en: '*Andrew Ng Launches A Campaign For Data-Centric AI*: [https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=5333db3a74f5](https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=5333db3a74f5)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*安德鲁·吴发起以数据为中心的AI运动*: [https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=5333db3a74f5](https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=5333db3a74f5)'
- en: '*Landing.AI: Data-Centric AI*: [https://landing.ai/data-centric-ai/](https://landing.ai/data-centric-ai/)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Landing.AI: 以数据为中心的AI*: [https://landing.ai/data-centric-ai/](https://landing.ai/data-centric-ai/)'
- en: '*Jiang et al. "To Trust Or Not To Trust A Classifier" (2018)*: [https://arxiv.org/abs/1805.11783](https://arxiv.org/abs/1805.11783)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*江等人“信任或不信任分类器”（2018年）*: [https://arxiv.org/abs/1805.11783](https://arxiv.org/abs/1805.11783)'
- en: '*Deepchecks*: [https://docs.deepchecks.com/en/stable/](https://docs.deepchecks.com/en/stable/)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Deepchecks*: [https://docs.deepchecks.com/en/stable/](https://docs.deepchecks.com/en/stable/)'
- en: '*Statistical distance*: [https://en.wikipedia.org/wiki/Statistical_distance](https://en.wikipedia.org/wiki/Statistical_distance)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*统计距离*: [https://en.wikipedia.org/wiki/Statistical_distance](https://en.wikipedia.org/wiki/Statistical_distance)'
- en: '*Lin et al. "Examining Distributional Shifts by Using Population Stability
    Index (PSI) for Model Validation and Diagnosis"*: [https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf](https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*林等人“通过使用人口稳定性指数（PSI）检查分布变化，用于模型验证和诊断”*: [https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf](https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf)'
- en: '*Wasserstein metric*: [https://en.wikipedia.org/wiki/Wasserstein_metric](https://en.wikipedia.org/wiki/Wasserstein_metric)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Wasserstein度量*: [https://en.wikipedia.org/wiki/Wasserstein_metric](https://en.wikipedia.org/wiki/Wasserstein_metric)'
- en: '*Bhattacharyya Distance based Concept Drift Detection Method For evolving data
    stream*: [https://www.researchgate.net/publication/352044688_Bhattacharyya_Distance_based_Concept_Drift_Detection_Method_For_evolving_data_stream](https://www.researchgate.net/publication/352044688_Bhattacharyya_Distance_based_Concept_Drift_Detection_Method_For_evolving_data_stream)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于Bhattacharyya距离的演变数据流概念漂移检测方法*: [https://www.researchgate.net/publication/352044688_Bhattacharyya_Distance_based_Concept_Drift_Detection_Method_For_evolving_data_stream](https://www.researchgate.net/publication/352044688_Bhattacharyya_Distance_based_Concept_Drift_Detection_Method_For_evolving_data_stream)'
