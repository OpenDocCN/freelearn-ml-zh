- en: Chapter 12. Specialized Machine Learning Topics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章：专门的机器学习主题
- en: Congratulations on reaching this point in your machine learning journey! If
    you have not already started work on your own projects, you will do so soon. And
    in doing so, you may find that the task of turning data into action is more difficult
    than it first appeared.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你已经到达了机器学习旅程的这一阶段！如果你还没有开始自己的项目，很快就会开始。而在这过程中，你可能会发现将数据转化为行动的任务比最初想象的要更为困难。
- en: As you gathered data, you may have realized that the information was trapped
    in a proprietary format or spread across pages on the Web. Making matters worse,
    after spending hours reformatting the data, maybe your computer slowed to a crawl
    after running out of memory. Perhaps R even crashed or froze your machine. Hopefully,
    you were undeterred, as these issues can be remedied with a bit more effort.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当你收集数据时，你可能已经意识到信息被困在专有格式中，或者分布在互联网上的各个页面上。更糟糕的是，经过几个小时的重新格式化，可能因为内存不足，电脑变得极其缓慢。也许R甚至崩溃或冻结了你的机器。希望你没有气馁，因为这些问题可以通过多一点努力得到解决。
- en: 'This chapter covers techniques that may not apply to every project, but will
    prove useful for working around such specialized issues. You might find the information
    particularly useful if you tend to work with data that is:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了可能并不适用于所有项目的技术，但在处理这些专业性问题时，将会非常有用。如果你经常处理以下类型的数据，你可能会特别觉得这些信息有用：
- en: Stored in unstructured or proprietary formats such as web pages, web APIs, or
    spreadsheets
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储在无结构或专有格式中，例如网页、Web API或电子表格
- en: From a specialized domain such as bioinformatics or social network analysis
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自生物信息学或社交网络分析等专业领域
- en: Too large to fit in memory or analyses take a very long time to complete
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据太大无法加载到内存中，或者分析需要非常长的时间才能完成
- en: You're not alone if you suffer from any of these problems. Although there is
    no panacea—these issues are the bane of the data scientist as well as the reason
    data skills are in high demand—through the dedicated efforts of the R community,
    a number of R packages provide a head start toward solving the problem.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到这些问题，你并不孤单。虽然没有万能的解决方案——这些问题是数据科学家的痛点，也是数据技能需求高涨的原因——通过R社区的努力，许多R包为解决这些问题提供了一个起点。
- en: This chapter provides a cookbook of such solutions. Even if you are an experienced
    R veteran, you may discover a package that simplifies your workflow. Or, perhaps
    one day, you will author a package that makes work easier for everybody else!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了一本这样的解决方案食谱。即使你是经验丰富的R老手，你也可能会发现一个简化工作流程的包。或者，也许有一天，你会编写一个让大家的工作变得更轻松的包！
- en: Working with proprietary files and databases
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理专有文件和数据库
- en: Unlike the examples in this book, real-world data is rarely packaged in a simple
    CSV form that can be downloaded from a website. Instead, significant effort is
    needed to prepare data for analysis. Data must be collected, merged, sorted, filtered,
    or reformatted to meet the requirements of the learning algorithm. This process
    is informally known as **data munging** or **data wrangling**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书中的例子不同，现实世界中的数据很少以简单的CSV格式进行打包，可以从网站下载。相反，准备数据进行分析需要付出相当大的努力。数据必须被收集、合并、排序、过滤或重新格式化，以满足学习算法的要求。这个过程通常被称为**数据清理**或**数据整理**。
- en: Data preparation has become even more important, as the size of typical datasets
    has grown from megabytes to gigabytes, and data is gathered from unrelated and
    messy sources, many of which are stored in massive databases. Several packages
    and resources for retrieving and working with proprietary data formats and databases
    are listed in the following sections.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着典型数据集的大小从兆字节增长到千兆字节，且数据来自无关且凌乱的来源，许多数据被存储在庞大的数据库中，数据准备变得更加重要。以下章节列出了几个用于检索和处理专有数据格式及数据库的包和资源。
- en: Reading from and writing to Microsoft Excel, SAS, SPSS, and Stata files
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Microsoft Excel、SAS、SPSS和Stata文件中读取和写入数据
- en: A frustrating aspect of data analysis is the large amount of work required to
    pull and combine data from various proprietary formats. Vast troves of data exist
    in files and databases that simply need to be unlocked for use in R. Thankfully,
    packages exist for exactly this purpose.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析的一个令人沮丧的方面是需要花费大量工作去从各种专有格式中提取和结合数据。海量的数据存储在文件和数据库中，只需解锁它们，便可用于R中。幸运的是，正是为了这个目的，存在相关的R包。
- en: 'What used to be a tedious and time-consuming process, requiring knowledge of
    specific tricks and tools across multiple R packages, has been made trivial by
    a relatively new R package called `rio` (an acronym for R input and output). This
    package, by Chung-hong Chan, Geoffrey CH Chan, Thomas J. Leeper, and Christopher
    Gandrud, is described as a "Swiss-army knife for data". It is capable of importing
    and exporting a large variety of file formats, including but not limited to: tab-separated
    (`.tsv`), comma-separated (`.csv`), JSON (`.json`), Stata (`.dta`), SPSS (`.sav`
    and `.por`), Microsoft Excel (`.xls` and `.xlsx`), Weka (`.arff`), and SAS (`.sas7bdat`
    and `.xpt`).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，需要掌握多个R包中的特定技巧和工具，才能完成繁琐且费时的过程。而现在，由于一个相对较新的R包`rio`（代表R输入输出），这一过程变得轻而易举。这个包由Chung-hong
    Chan、Geoffrey CH Chan、Thomas J. Leeper和Christopher Gandrud开发，被描述为“数据的瑞士军刀”。它能够导入和导出多种文件格式，包括但不限于：制表符分隔（`.tsv`）、逗号分隔（`.csv`）、JSON（`.json`）、Stata（`.dta`）、SPSS（`.sav`和`.por`）、Microsoft
    Excel（`.xls`和`.xlsx`）、Weka（`.arff`）和SAS（`.sas7bdat`和`.xpt`）等。
- en: Note
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For the complete list of file types `rio` can import and export, as well as
    more detailed usage examples, see [http://cran.r-project.org/web/packages/rio/vignettes/rio.html](http://cran.r-project.org/web/packages/rio/vignettes/rio.html).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有关`rio`可以导入和导出的文件类型的完整列表，以及更详细的使用示例，请参见[http://cran.r-project.org/web/packages/rio/vignettes/rio.html](http://cran.r-project.org/web/packages/rio/vignettes/rio.html)。
- en: 'The `rio` package consists of three functions for working with proprietary
    data formats: `import()`, `export()`, and `convert()`. Each does exactly what
    you''d expect, given their name. Consistent with the package''s philosophy of
    keeping things simple, each function uses the filename extension to guess the
    type of file to import, export, or convert.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`rio`包包含三个用于处理专有数据格式的函数：`import()`、`export()`和`convert()`。根据函数的名称，它们分别完成预期的操作。与该包保持简单的理念一致，每个函数通过文件名扩展名来猜测要导入、导出或转换的文件类型。'
- en: 'For example, to import the credit data from previous chapters, which is stored
    in CSV format, simply type:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要导入前几章中的信用数据，它以CSV格式存储，只需键入：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This creates the `credit` data frame as expected; as a bonus, not only did we
    not have to specify the CSV file type, `rio` automatically set `stringsAsFactors
    = FALSE` as well as other reasonable defaults.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建预期的`credit`数据框；作为额外好处，我们不仅无需指定CSV文件类型，`rio`还自动设置了`stringsAsFactors = FALSE`以及其他合理的默认值。
- en: 'To export the `credit` data frame to Microsoft Excel (`.xlsx`) format, use
    the `export()` function while specifying the desired filename, as follows. For
    other formats, simply change the file extension to the desired output type:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要将`credit`数据框导出为Microsoft Excel（`.xlsx`）格式，请使用`export()`函数并指定所需的文件名，如下所示。对于其他格式，只需将文件扩展名更改为所需的输出类型：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It is also possible to convert the CSV file to another format directly, without
    an import step, using the `convert()` function. For example, this converts the
    `credit.csv` file to Stata (`.dta`) format:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以直接使用`convert()`函数将CSV文件转换为另一种格式，无需导入步骤。例如，这将`credit.csv`文件转换为Stata（`.dta`）格式：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Though the `rio` package covers many common proprietary data formats, it does
    not do everything. The next section covers other ways to get data into R via database
    queries.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`rio`包覆盖了许多常见的专有数据格式，但它并不支持所有操作。下一节将介绍通过数据库查询将数据导入R的其他方法。
- en: Querying data in SQL databases
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询SQL数据库中的数据
- en: Large datasets are often stored in **Database Management Systems** (**DBMSs**)
    such as Oracle, MySQL, PostgreSQL, Microsoft SQL, or SQLite. These systems allow
    the datasets to be accessed using a **Structured Query Language** (**SQL**), a
    programming language designed to pull data from databases. If your DBMS is configured
    to allow **Open Database Connectivity** (**ODBC**), the `RODBC` package by Brian
    Ripley can be used to import this data directly into an R data frame.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 大型数据集通常存储在**数据库管理系统**（**DBMSs**）中，如Oracle、MySQL、PostgreSQL、Microsoft SQL或SQLite。这些系统允许使用**结构化查询语言**（**SQL**）访问数据集，SQL是一种用于从数据库中提取数据的编程语言。如果你的DBMS配置为允许**开放数据库连接**（**ODBC**），则可以使用Brian
    Ripley的`RODBC`包将数据直接导入R数据框。
- en: Tip
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If you have trouble using ODBC to connect to your database, you may try one
    of the DMBS-specific R packages. These include `ROracle`, `RMySQL`, `RPostgresSQL`,
    and `RSQLite`. Though they will function largely similar to the instructions here,
    refer to the package documentation on CRAN for instructions specific to each package.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在使用 ODBC 连接到数据库时遇到困难，可以尝试使用一些特定于 DBMS 的 R 包。这些包包括 `ROracle`、`RMySQL`、`RPostgresSQL`
    和 `RSQLite`。虽然它们的功能与这里的指令大致相似，但请参考 CRAN 上的包文档，获取每个包特定的说明。
- en: ODBC is a standard protocol for connecting to databases regardless of operating
    system or DBMS. If you were previously connected to an ODBC database, you most
    likely would have referred to it via its **Data Source Name** (**DSN**). You will
    need the DSN, plus a username and password (if your database requires it) to use
    `RODBC`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ODBC 是一种标准协议，用于连接数据库，与操作系统或数据库管理系统无关。如果您之前连接过 ODBC 数据库，您很可能会通过其 **数据源名称** (**DSN**)
    来引用它。要使用 `RODBC`，您需要 DSN 以及用户名和密码（如果数据库需要的话）。
- en: Tip
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The instructions to configure an ODBC connection are highly specific to the
    combination of the OS and DBMS. If you are having trouble setting up an ODBC connection,
    check with your database administrator. Another way to obtain help is via the
    `RODBC` package `vignette`, which can be accessed in R with the `vignette("RODBC")`
    command after the `RODBC` package has been installed.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 配置 ODBC 连接的指令非常依赖于操作系统和数据库管理系统（DBMS）的组合。如果您在设置 ODBC 连接时遇到问题，请与您的数据库管理员联系。另一种获取帮助的方式是通过
    `RODBC` 包的 `vignette`，在安装了 `RODBC` 包之后，可以通过 R 中的 `vignette("RODBC")` 命令访问。
- en: 'To open a connection called `my_db` for the database with the `my_dsn` DSN,
    use the `odbcConnect()` function:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要为数据库使用 `my_dsn` DSN 打开一个名为 `my_db` 的连接，请使用 `odbcConnect()` 函数：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Alternatively, if your ODBC connection requires a username and password, they
    should be specified while calling the `odbcConnect()` function:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的 ODBC 连接需要用户名和密码，应该在调用 `odbcConnect()` 函数时指定：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: With an open database connection, we can use the `sqlQuery()` function to create
    an R data frame from the database rows pulled by an SQL query. This function,
    like the many functions that create data frames, allows us to specify `stringsAsFactors
    = FALSE` to prevent R from automatically converting character data into factors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在打开的数据库连接下，我们可以使用 `sqlQuery()` 函数根据 SQL 查询拉取的数据库行创建 R 数据框。此函数与许多创建数据框的函数类似，允许我们指定
    `stringsAsFactors = FALSE`，以防止 R 自动将字符数据转换为因子。
- en: 'The `sqlQuery()` function uses typical SQL queries, as shown in the following
    command:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`sqlQuery()` 函数使用典型的 SQL 查询，如下面的命令所示：'
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The resulting `results_df` object is a data frame containing all of the rows
    selected using the SQL query stored in `sql_query`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 `results_df` 对象是一个数据框，包含了使用存储在 `sql_query` 中的 SQL 查询选择的所有行。
- en: 'Once you are done using the database, the connection can be closed using the
    following command:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成使用数据库，可以使用以下命令关闭连接：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Although R will automatically close ODBC connections at the end of an R session,
    it is a better practice to do so explicitly.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 R 会在 R 会话结束时自动关闭 ODBC 连接，但最好明确地执行此操作。
- en: Working with online data and services
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用在线数据和服务
- en: With the growing amount of data available from web-based sources, it is increasingly
    important for machine learning projects to be able to access and interact with
    online services. R is able to read data from online sources natively, with some
    caveats. Firstly, by default, R cannot access secure websites (those using the
    `https://` rather than the `http://` protocol). Secondly, it is important to note
    that most web pages do not provide data in a form that R can understand. The data
    would need to be **parsed**, or broken apart and rebuilt into a structured form,
    before it can be useful. We'll discuss the workarounds shortly.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 随着来自网络来源的数据量不断增加，机器学习项目能否访问并与在线服务互动变得越来越重要。R 本身能够从在线源读取数据，但有一些注意事项。首先，默认情况下，R
    无法访问安全网站（即使用 `https://` 而非 `http://` 协议的网站）。其次，需要注意的是，大多数网页并没有以 R 可以理解的格式提供数据。在数据可以有用之前，它需要被
    **解析**，即拆解并重建为结构化的形式。我们稍后将讨论一些解决方法。
- en: 'However, if neither of these caveats applies (that is, if data are already
    online on a nonsecure website and in a tabular form, like CSV, that R can understand
    natively), then R''s `read.csv()` and `read.table()` functions will be able to
    access data from the Web just as if it were on your local machine. Simply supply
    the full URL for the dataset as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果这些警告都不适用（也就是说，如果数据已经在一个不安全的网站上并且是表格形式，如CSV格式，R可以原生理解），那么R的`read.csv()`和`read.table()`函数就能像访问本地机器上的数据一样，访问网络上的数据。只需提供数据集的完整URL，如下所示：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'R also provides functionality to download other files from the Web, even if
    R cannot use them directly. For a text file, try the `readLines()` function as
    follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: R还提供了从网络下载其他文件的功能，即使R无法直接使用它们。对于文本文件，可以尝试以下`readLines()`函数：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For other types of files, the `download.file()` function can be used. To download
    a file to R''s current working directory, simply supply the URL and destination
    filename as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他类型的文件，可以使用`download.file()`函数。要将文件下载到R的当前工作目录，只需提供URL和目标文件名，如下所示：
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Beyond this base functionality, there are numerous packages that extend R's
    capabilities to work with online data. The most basic of them will be covered
    in the sections that follow. As the Web is massive and ever-changing, these sections
    are far from a comprehensive set of all the ways R can connect to online data.
    There are literally hundreds of packages for everything from niche projects to
    massive ones.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些基本功能外，还有许多包扩展了R处理在线数据的能力。最基础的部分将在接下来的章节中介绍。由于网络庞大且不断变化，这些章节远不是R连接到在线数据的所有方法的全面集合。实际上，几乎每个领域都有成百上千个包，涵盖从小型项目到大型项目的各种需求。
- en: Note
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For the most complete and up-to-date list of packages, refer to the regularly
    updated CRAN Web Technologies and Services task view at [http://cran.r-project.org/web/views/WebTechnologies.html](http://cran.r-project.org/web/views/WebTechnologies.html).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取最完整且最新的包列表，请参考定期更新的CRAN网页技术与服务任务视图，地址为[http://cran.r-project.org/web/views/WebTechnologies.html](http://cran.r-project.org/web/views/WebTechnologies.html)。
- en: Downloading the complete text of web pages
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载网页的完整文本
- en: The `RCurl` package by Duncan Temple Lang supplies a more robust way of accessing
    web pages by providing an R interface to the **curl** (client for URLs) utility,
    a command-line tool to transfer data over networks. The curl program acts much
    like a programmable web browser; given a set of commands, it can access and download
    the content of nearly anything available on the Web. Unlike R, it can access secure
    websites as well as post data to online forms. It is an incredibly powerful utility.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Duncan Temple Lang的`RCurl`包提供了一种更强大的方式来访问网页，通过为**curl**（URL客户端）工具提供一个R接口，这个工具是一个命令行工具，用于通过网络传输数据。curl程序就像一个可编程的网页浏览器；给定一组命令，它可以访问并下载网络上几乎所有可用的内容。与R不同，它不仅可以访问安全网站，还能向在线表单提交数据。它是一个极其强大的工具。
- en: Note
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Precisely because it is so powerful, a complete curl tutorial is outside the
    scope of this chapter. Instead, refer to the online `RCurl` documentation at [http://www.omegahat.org/RCurl/](http://www.omegahat.org/RCurl/).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正因为curl功能如此强大，完整的curl教程超出了本章的范围。相反，请参考在线`RCurl`文档，地址为[http://www.omegahat.org/RCurl/](http://www.omegahat.org/RCurl/)。
- en: 'After installing the `RCurl` package, downloading a page is as simple as typing:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`RCurl`包后，下载一个页面就像输入以下命令那样简单：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will save the full text of the Packt Publishing homepage (including all
    the web markup) into the R character object named `packt_page`. As shown in the
    following lines, this is not very useful:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把Packt Publishing主页的完整文本（包括所有网页标记）保存到名为`packt_page`的R字符对象中。正如接下来的几行所示，这并不是非常有用：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The reason that the first 200 characters of the page look like nonsense is because
    the websites are written using **Hypertext Markup Language** (**HTML**), which
    combines the page text with special tags that tell web browsers how to display
    the text. The `<title>` and `</title>` tags here surround the page title, telling
    the browser that this is the Packt Publishing homepage. Similar tags are used
    to denote other portions of the page.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 页面前200个字符看起来像是乱码的原因是因为网站使用**超文本标记语言**（**HTML**）编写，它将页面文本与特殊的标签结合，这些标签告诉浏览器如何显示文本。这里的`<title>`和`</title>`标签包围了页面标题，告诉浏览器这是Packt
    Publishing主页。类似的标签用于标示页面的其他部分。
- en: 'Though curl is the cross-platform standard to access online content, if you
    work with web data frequently in R, the `httr` package by Hadley Wickham builds
    upon the foundation of `RCurl` to make it more convenient and R-like. We can see
    some of the differences immediately by attempting to download the Packt Publishing
    homepage using the `httr` package''s `GET()` function:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 curl 是访问在线内容的跨平台标准，但如果你在 R 中频繁处理网页数据，Hadley Wickham 开发的 `httr` 包基于 `RCurl`
    打造，使其更加便捷且符合 R 的使用习惯。通过尝试使用 `httr` 包的 `GET()` 函数下载 Packt Publishing 的主页，我们可以立刻看到一些不同：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Where the `getURL()` function in `RCurl` downloaded only the HTML, the `GET()`
    function returns a list with site properties in addition to the HTML. To access
    the page content itself, we need to use the `content()` function:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `RCurl` 中的 `getURL()` 函数仅下载 HTML 内容，而 `GET()` 函数则返回一个包含网站属性的列表，此外还包括 HTML
    内容。要访问页面内容本身，我们需要使用 `content()` 函数：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In order to use this data in an R program, it is necessary to process the page
    to transform it into a structured format like a list or data frame. Functions
    to do so are discussed in the sections that follow.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 R 程序中使用这些数据，需要处理页面，将其转化为像列表或数据框这样的结构化格式。如何进行这一步骤将在接下来的章节中讨论。
- en: Note
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For detailed `httr` documentation and tutorials, visit the project GitHub page
    at [https://github.com/hadley/httr](https://github.com/hadley/httr). The quickstart
    guide is particularly helpful to learn the base functionality.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更详细的 `httr` 文档和教程，请访问项目的 GitHub 页面：[https://github.com/hadley/httr](https://github.com/hadley/httr)。快速入门指南特别有助于学习基本功能。
- en: Scraping data from web pages
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从网页抓取数据
- en: Because there is a consistent structure of the HTML tags of many web pages,
    it is possible to write programs that look for desired sections of the page and
    extract them in order to compile them into a dataset. This process practice of
    harvesting data from websites and transforming it into a structured form is known
    as **web scraping**.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多网页的 HTML 标签结构是统一的，我们可以编写程序来寻找页面中的目标部分，并将其提取出来，最终将数据编译成一个数据集。这一从网站抓取数据并将其转换为结构化形式的过程，称为
    **网页抓取**。
- en: Tip
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Though it is frequently used, scraping should be considered a last resort to
    get data from the Web. This is because any changes to the underlying HTML structure
    may break your code, requiring efforts to be fixed. Even worse, it may introduce
    unnoticed errors into your data. Additionally, many websites' terms of use agreements
    explicitly forbid automated data extraction, not to mention the fact that your
    program's traffic may overload their servers. Always check the site's terms before
    you begin your project; you may even find that the site offers its data freely
    via a developer agreement.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管网页抓取被广泛使用，但它应该被视为从 Web 获取数据的最后手段。原因在于，任何对基础 HTML 结构的更改都可能使代码失效，修复时可能需要额外的工作。更糟糕的是，这可能会在数据中引入不可察觉的错误。此外，许多网站的使用条款明确禁止自动化数据提取，更不用说你的程序的流量可能会超载他们的服务器。在开始项目前，务必检查网站的条款；你甚至可能发现该网站通过开发者协议免费提供数据。
- en: The `rvest` package (a pun on the term "harvest") by Hadley Wickham makes web
    scraping a largely effortless process, assuming the data you want can be found
    in a consistent place within HTML.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Hadley Wickham 开发的 `rvest` 包（"harvest" 的双关语）使得网页抓取过程基本不费吹灰之力，前提是你想要的数据可以在 HTML
    中的某个固定位置找到。
- en: 'Let''s start with a simple example using the Packt Publishing homepage. We
    begin by downloading the page as before, using the `html()` function in the `rvest`
    package. Note that this function, when supplied with a URL, simply calls the `GET()`
    function in Hadley Wickham''s `httr` package:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简单的例子开始，使用 Packt Publishing 的主页。我们首先使用 `rvest` 包中的 `html()` 函数下载页面。请注意，这个函数在提供
    URL 时，实际上是调用了 Hadley Wickham 的 `httr` 包中的 `GET()` 函数：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Suppose we''d like to scrape the page title. Looking at the previous HTML code,
    we know that there is only one title per page wrapped within `<title>` and `</title>`
    tags. To pull the title, we supply the tag name to the `html_node()` function,
    as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要抓取页面标题。通过查看之前的 HTML 代码，我们知道每个页面只有一个标题，它被包裹在 `<title>` 和 `</title>` 标签之间。为了提取标题，我们将标签名称传递给
    `html_node()` 函数，代码如下：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This keeps the HTML formatting in place, including the `<title>` tags and the
    `&amp;` code, which is the HTML designation for the ampersand symbol. To translate
    this into plain text, we simply run it through the `html_text()` function, as
    shown:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这样可以保持 HTML 格式不变，包括 `<title>` 标签和 `&amp;` 代码，这是 HTML 中代表和符号的标记。要将其转换为纯文本，我们只需通过
    `html_text()` 函数进行处理，如下所示：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Notice the use of the `%>%` operator. This is known as a pipe, because it essentially
    "pipes" data from one function to another. The use of pipes allows the creation
    of powerful chains of functions to process HTML data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 注意使用了`%>%`操作符。这被称为管道，因为它本质上是将数据从一个函数“传输”到另一个函数。管道的使用允许创建强大的函数链来处理HTML数据。
- en: Note
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The pipe operator is a part of the `magrittr` package by Stefan Milton Bache
    and Hadley Wickham, installed by default with the `rvest` package. The name is
    a play on René Magritte's famous painting of a pipe (you may recall seeing it
    in [Chapter 1](ch01.html "Chapter 1. Introducing Machine Learning"), *Introducing
    Machine Learning*). For more information on the project, visit its GitHub page
    at [https://github.com/smbache/magrittr](https://github.com/smbache/magrittr).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 管道操作符是`magrittr`包的一部分，由Stefan Milton Bache和Hadley Wickham开发，并与`rvest`包一起默认安装。这个名字来源于René
    Magritte的著名画作《管子》，你可能记得它出现在[第1章](ch01.html "第1章：介绍机器学习")，*介绍机器学习*中。有关该项目的更多信息，请访问其GitHub页面：[https://github.com/smbache/magrittr](https://github.com/smbache/magrittr)。
- en: 'Let''s try a slightly more interesting example. Suppose we''d like to scrape
    a list of all the packages on the CRAN machine learning task view. We begin as
    in the same way we did it earlier, by downloading the HTML page using the `html()`
    function. Since we don''t know how the page is structured, we''ll also peek into
    HTML by typing `cran_ml`, the name of the R object we created:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个稍微有趣一点的例子。假设我们想抓取CRAN机器学习任务视图中的所有包的列表。我们与之前一样，通过使用`html()`函数下载HTML页面开始。由于我们不知道页面的结构，因此我们还会通过键入`cran_ml`，即我们创建的R对象的名称，来查看HTML内容：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Looking over the output, we find that one section appears to have the data
    we''re interested in. Note that only a subset of the output is shown here:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 查看输出结果，我们发现有一个部分似乎包含了我们感兴趣的数据。请注意，这里只显示了输出的一个子集：
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `<h3>` tags imply a header of size 3, while the `<ul>` and `<li>` tags refer
    to the creation of an unordered list and list items, respectively. The data elements
    we want are surrounded by `<a>` tags, which are hyperlink anchor tags that link
    to the CRAN page for each package.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`<h3>`标签表示一个大小为3的标题，而`<ul>`和`<li>`标签分别用于创建无序列表和列表项。我们想要的数据元素被`<a>`标签包围，这些是超链接锚标签，指向每个包的CRAN页面。'
- en: Tip
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Because the CRAN page is actively maintained and may be changed at any time,
    do not be surprised if your results differ from those shown here.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于CRAN页面是持续维护的，且随时可能发生变化，因此如果你的结果与这里展示的不同，不要感到惊讶。
- en: 'With this knowledge in hand, we can scrape the links much like we did previously.
    The one exception is that, because we expect to find more than one result, we
    need to use the `html_nodes()` function to return a vector of results rather than
    `html_node()`, which returns only a single item:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些知识，我们可以像之前那样抓取链接。唯一的例外是，由于我们预计会找到多个结果，我们需要使用`html_nodes()`函数来返回一个结果向量，而不是`html_node()`，后者只返回单个项：
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s peek at the result using the `head()` function:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`head()`函数查看结果：
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As we can see on line 6, it looks like the links to some other projects slipped
    in. This is because some packages are hyperlinked to additional websites; in this
    case, the `RWeka` package is linked to both CRAN and its homepage. To exclude
    these results, you might chain this output to another function that could look
    for the `/packages` string in the hyperlink.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在第6行所看到的，似乎一些其他项目的链接也出现在了结果中。这是因为有些包被超链接到其他网站；在这种情况下，`RWeka`包同时链接到CRAN和其主页。为了排除这些结果，你可以将这个输出链接到另一个函数，查找超链接中的`/packages`字符串。
- en: Tip
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In general, web scraping is always a process of iterate-and-refine as you identify
    more specific criteria to exclude or include specific cases. The most difficult
    cases may even require a human eye to achieve 100 percent accuracy.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，网页抓取总是一个不断迭代和改进的过程，随着你识别出更具体的标准来排除或包括特定的情况。最困难的情况可能甚至需要人工检查才能达到100%的准确性。
- en: These are simple examples that merely scratch the surface of what is possible
    with the `rvest` package. Using the pipe functionality, it is possible to look
    for tags nested within tags or specific classes of HTML tags. For these types
    of complex examples, refer to the package documentation.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是简单的示例，仅仅触及了`rvest`包的可能性。通过使用管道功能，实际上可以查找嵌套在标签中的标签或特定类别的HTML标签。对于这些复杂的例子，请参考包的文档。
- en: Parsing XML documents
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解析XML文档
- en: XML is a plaintext, human-readable, structured markup language upon which many
    document formats have been based. It employs a tagging structure in some ways
    similar to HTML, but is far stricter about formatting. For this reason, it is
    a popular online format to store structured datasets.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: XML 是一种纯文本、可读性强的结构化标记语言，许多文档格式都基于它。它使用类似 HTML 的标签结构，但在格式上要严格得多。因此，它是一个流行的在线格式，用于存储结构化数据集。
- en: The `XML` package by Duncan Temple Lang provides a suite of R functionality
    based on the popular C-based `libxml2` parser to read and write XML documents.
    It is the grandfather of XML parsing packages in R and is still widely used.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Duncan Temple Lang 的 `XML` 包提供了一套基于流行的 C 语言 `libxml2` 解析器的 R 功能，能够读取和写入 XML
    文档。它是 R 中 XML 解析包的祖先，至今仍广泛使用。
- en: Note
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Information on the `XML` package, including simple examples to get you started
    quickly, can be found on the project's website at [http://www.omegahat.org/RSXML/](http://www.omegahat.org/RSXML/).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 `XML` 包的信息，包括一些简单的示例，可以在该项目的官方网站上找到：[http://www.omegahat.org/RSXML/](http://www.omegahat.org/RSXML/)。
- en: Recently, the `xml2` package by Hadley Wickham has surfaced as an easier and
    more R-like interface to the `libxml2` library. The `rvest` package, which was
    covered earlier in this chapter, utilizes `xml2` behind the scenes to parse HTML.
    Moreover, `rvest` can be used to parse XML as well.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Hadley Wickham 的 `xml2` 包作为一个更简单、更像 R 的接口，浮出水面，成为对 `libxml2` 库的封装。前面章节中提到的
    `rvest` 包在幕后利用 `xml2` 来解析 HTML。此外，`rvest` 也可以用来解析 XML。
- en: Note
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `xml2` GitHub page is found at [https://github.com/hadley/xml2](https://github.com/hadley/xml2).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`xml2` 的 GitHub 页面可以在此找到：[https://github.com/hadley/xml2](https://github.com/hadley/xml2)。'
- en: Because parsing XML is so closely related to parsing HTML, the exact syntax
    is not covered here. Please refer to these packages' documentation for examples.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由于解析 XML 与解析 HTML 密切相关，因此这里不涉及具体的语法。有关示例，请参考这些软件包的文档。
- en: Parsing JSON from web APIs
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从 Web API 解析 JSON
- en: Online applications communicate with one another using web-accessible functions
    known as **Application Programming Interfaces** (**APIs**). These interfaces act
    much like a typical website; they receive a request from a client via a particular
    URL and return a response. The difference is that a normal website returns HTML
    meant for display in a web browser, while an API typically returns data in a structured
    form meant for processing by a machine.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在线应用程序通过称为**应用程序编程接口** (**APIs**) 的 Web 可访问函数进行互相通信。这些接口的工作方式类似于典型的网站；它们通过特定的
    URL 接收客户端请求，并返回响应。不同之处在于，普通网站返回的是用于在 Web 浏览器中显示的 HTML，而 API 通常返回的是结构化的数据，供机器处理。
- en: Though it is not uncommon to find XML-based APIs, perhaps the most common API
    data structure today is **JavaScript Object Notation** (**JSON**). Like XML, it
    is a standard, plaintext format, most often used for data structures and objects
    on the Web. The format has become popular recently due to its roots in browser-based
    JavaScript applications, but despite the pedigree, its utility is not limited
    to the Web. The ease in which JSON data structures can be understood by humans
    and parsed by machines makes it an appealing data structure for many types of
    projects.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于 XML 的 API 并不少见，但今天最常见的 API 数据结构可能是**JavaScript 对象表示法** (**JSON**)。像 XML
    一样，JSON 是一种标准的纯文本格式，通常用于 Web 上的数据结构和对象。由于其与基于浏览器的 JavaScript 应用程序的联系，JSON 最近变得流行，但尽管如此，它的应用并不限于
    Web。JSON 数据结构易于人类理解且易于机器解析，这使得它成为许多项目类型中有吸引力的数据结构。
- en: 'JSON is based on a simple `{key: value}` format. The `{ }` brackets denote
    a JSON object, and the `key` and `value` parameters denote a property of the object
    and the status of the property. An object can have any number of properties and
    the properties themselves may be objects. For example, a JSON object for this
    book might look something like this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 'JSON 基于简单的 `{key: value}` 格式。`{ }` 括号表示一个 JSON 对象，`key` 和 `value` 参数表示对象的属性及该属性的状态。一个对象可以包含任意数量的属性，且属性本身也可以是对象。例如，这本书的
    JSON 对象可能如下所示：'
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This example illustrates the data types available to JSON: numeric, character,
    array (surrounded by `[` and `]` characters), and object. Not shown are the `null`
    and Boolean (`true` or `false`) values. The transmission of these types of objects
    from application to application and application to web browser, is what powers
    many of the most popular websites.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了 JSON 可用的数据类型：数字、字符、数组（由 `[` 和 `]` 括起来）和对象。未显示的是 `null` 和布尔值（`true` 或
    `false`）。这些类型的对象在应用程序与应用程序之间，以及应用程序与 Web 浏览器之间的传输，是许多最受欢迎网站的动力来源。
- en: Note
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For details on the JSON format, go to [http://www.json.org/](http://www.json.org/).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 JSON 格式的详细信息，请访问 [http://www.json.org/](http://www.json.org/)。
- en: Public-facing APIs allow programs like R to systematically query websites to
    retrieve results in the JSON format, using packages like `RCurl` and `httr`. Though
    a full tutorial on using web APIs is worthy of a separate book, the basic process
    relies on only a couple of steps—it's the details that are tricky.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 公共 API 允许像 R 这样的程序系统地查询网站，使用像 `RCurl` 和 `httr` 这样的包以 JSON 格式检索结果。虽然使用 Web API
    的完整教程值得单独成书，但基本过程只依赖于几个步骤——真正复杂的是细节。
- en: 'Suppose we wanted to query the Google Maps API to locate the latitude and longitude
    of the Eiffel Tower in France. We first need to review the Google Maps API documentation
    to determine the URL and parameters needed to make this query. We then supply
    this information to the `httr` package''s `GET()` function, adding a list of query
    parameters in order to apply the search address:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要查询 Google Maps API 以定位法国埃菲尔铁塔的纬度和经度。首先，我们需要查看 Google Maps API 文档，以确定进行此查询所需的
    URL 和参数。然后，我们将这些信息提供给 `httr` 包的 `GET()` 函数，并添加一组查询参数以进行地址搜索：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'By typing the name of the resulting object, we can see some details about the
    request:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通过输入结果对象的名称，我们可以查看关于请求的一些详细信息：
- en: '[PRE23]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To access the resulting JSON, which the `httr` package parsed automatically,
    we use the `content()` function. For brevity, only a handful of lines are shown
    here:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问 `httr` 包自动解析的 JSON 结果，我们使用 `content()` 函数。为简便起见，这里只展示了几行代码：
- en: '[PRE24]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To access these contents individually, simply refer to them using list syntax.
    The names are based on the JSON objects returned by the Google API. For instance,
    the entire set of results is in an object appropriately named `results` and each
    result is numbered. In this case, we will access the formatted address property
    of the first result, as well as the latitude and longitude:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要单独访问这些内容，只需使用列表语法引用它们。名称基于 Google API 返回的 JSON 对象。例如，整个结果集在一个名为 `results` 的对象中，每个结果都有编号。在此案例中，我们将访问第一个结果的格式化地址属性，以及纬度和经度：
- en: '[PRE25]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: These data elements could then be used in an R program as desired.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据元素可以在 R 程序中根据需要使用。
- en: Tip
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Because the Google Maps API may be updated in the future, if you find that your
    results differ from those shown here, please check the Packt Publishing support
    page for updated code.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 Google Maps API 可能在未来进行更新，如果你发现你的结果与此处显示的不同，请检查 Packt Publishing 支持页面以获取更新的代码。
- en: On the other hand, if you would like to do a conversion to and from the JSON
    format outside the `httr` package, there are a number of packages that add this
    functionality.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你想在 `httr` 包之外进行 JSON 格式的转换，可以使用一些包来实现这个功能。
- en: 'The `rjson` package by Alex Couture-Beil was one of the earliest packages to
    allow R data structures to be converted back and forth from the JSON format. The
    syntax is simple. After installing the `rjson` package, to convert from an R object
    to a JSON string, we use the `toJSON()` function. Notice that the quote characters
    have escaped using the `\"` notation:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Alex Couture-Beil 的 `rjson` 包是最早允许 R 数据结构与 JSON 格式互转的包之一。其语法简单。在安装了 `rjson`
    包后，要将 R 对象转换为 JSON 字符串，我们使用 `toJSON()` 函数。请注意，引用字符已使用 `\"` 符号转义：
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'To convert a JSON string into an R object, use the `fromJSON()` function. Quotation
    marks in the string need to be escaped, as shown:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 JSON 字符串转换为 R 对象，使用 `fromJSON()` 函数。字符串中的引号需要转义，如下所示：
- en: '[PRE27]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This results in a list structure in a form much like the original JSON:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生一个与原始 JSON 形式相似的列表结构：
- en: '[PRE28]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Recently, two new JSON packages have arrived on the scene. The first, `RJSONIO`,
    by Duncan Temple Lang was intended to be a faster and more extensible version
    of the `rjson` package, though they are now virtually identical. A second package,
    `jsonlite`, by Jeroen Ooms has quickly gained prominence as it creates data structures
    that are more consistent and R-like, especially while using data from web APIs.
    Which of these packages you use is a matter of preference; all three are virtually
    identical in practice as they each implement a `fromJSON()` and `toJSON()` function.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，出现了两个新的JSON包。第一个是由Duncan Temple Lang开发的`RJSONIO`，旨在成为`rjson`包的更快和更可扩展的版本，尽管现在它们几乎是相同的。第二个包是由Jeroen
    Ooms开发的`jsonlite`，它因能够创建更一致且更符合R语言的数据结构而迅速受到关注，尤其是在使用来自Web API的数据时。你选择使用哪个包是个人偏好问题；实际上，所有三个包在实践中几乎是相同的，因为它们都实现了`fromJSON()`和`toJSON()`函数。
- en: Note
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information on the potential benefits of the `jsonlite` package, see:
    Ooms J. The jsonlite package: a practical and consistent mapping between JSON
    data and R objects. 2014\. Available at: [http://arxiv.org/abs/1403.2805](http://arxiv.org/abs/1403.2805)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '有关`jsonlite`包的更多信息，参见：Ooms J. The jsonlite package: a practical and consistent
    mapping between JSON data and R objects. 2014. 可访问：[http://arxiv.org/abs/1403.2805](http://arxiv.org/abs/1403.2805)'
- en: Working with domain-specific data
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用特定领域的数据
- en: Machine learning has undoubtedly been applied to problems across every discipline.
    Although the basic techniques are similar across all domains, some are so specialized
    that communities are formed to develop solutions to the challenges unique to the
    field. This leads to the discovery of new techniques and new terminology that
    is relevant only to domain specific problems.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习无疑已应用于各个学科的问题。虽然基本技术在所有领域中都很相似，但有些技术非常专业化，以至于形成了特定社区，专门开发解决该领域独特挑战的方法。这促使了新技术和新术语的出现，这些术语仅与特定领域的问题相关。
- en: This section covers a pair of domains that use machine learning techniques extensively,
    but require specialized knowledge to unlock their full potential. Since entire
    books have been written on these topics, it will serve only as the briefest of
    introductions. For more details, seek out the help provided by the resources cited
    in each section.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了一对广泛使用机器学习技术，但需要专业知识才能充分发挥其潜力的领域。由于这些话题已有专门的书籍撰写，本节仅提供最简短的介绍。欲了解更多详情，请参考每节中引用的资源。
- en: Analyzing bioinformatics data
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析生物信息学数据
- en: The field of **bioinformatics** is concerned with the application of computers
    and data analysis to the biological domain, particularly with regard to better
    understanding the genome. As genetic data is unique compared to many other types,
    data analysis in the field of bioinformatics offers a number of unique challenges.
    For example, because living creatures have a tremendous number of genes and genetic
    sequencing is still relatively expensive, typical datasets are much wider than
    they are long; that is, they have more features (genes) than examples (creatures
    that have been sequenced). This creates problems while attempting to apply conventional
    visualizations, statistical tests, and machine learning methods to such data.
    Additionally, the increasing use of proprietary **microarray** "lab-on-a-chip"
    techniques requires highly specialized knowledge simply to load the genetic data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**生物信息学**领域关注计算机和数据分析在生物领域的应用，尤其是在更好地理解基因组方面。由于基因数据与许多其他类型的数据不同，生物信息学领域的数据分析面临着一些独特的挑战。例如，由于生物体拥有大量基因且基因测序仍然相对昂贵，典型的数据集比它们长的维度宽；也就是说，它们的特征（基因）多于样本（已测序的生物体）。这在尝试应用传统可视化、统计测试和机器学习方法时会遇到问题。此外，越来越多的专有**微阵列**“芯片上实验室”技术的使用，仅仅加载基因数据就需要高度专业的知识。'
- en: Note
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A CRAN task view, which lists some of R's specialized packages for statistical
    genetics and bioinformatics, is available at [http://cran.r-project.org/web/views/Genetics.html](http://cran.r-project.org/web/views/Genetics.html).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一个CRAN任务视图列出了R语言在统计遗传学和生物信息学方面的一些专用包，具体内容可参见：[http://cran.r-project.org/web/views/Genetics.html](http://cran.r-project.org/web/views/Genetics.html)。
- en: The **Bioconductor** project of the Fred Hutchinson Cancer Research Center in
    Seattle, Washington, aims to solve some of these problems by providing a standardized
    set of methods for analyzing genomic data. Using R as its foundation, Bioconductor
    adds bioinformatics-specific packages and documentation on top of the base R software.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**Bioconductor** 项目是位于华盛顿州西雅图的弗雷德·哈钦森癌症研究中心发起的，旨在通过提供一套标准化的基因组数据分析方法来解决一些问题。Bioconductor
    以 R 语言为基础，向其基础软件添加了生物信息学特定的包和文档。'
- en: Bioconductor provides workflows to analyze DNA and protein microarray data from
    common microarray platforms such as Affymetrix, Illumina, Nimblegen, and Agilent.
    Additional functionality includes sequence annotation, multiple testing procedures,
    specialized visualizations, tutorials, documentation, and much more.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Bioconductor 提供了用于分析来自常见微阵列平台（如 Affymetrix、Illumina、Nimblegen 和 Agilent）的 DNA
    和蛋白质微阵列数据的工作流程。其他功能包括序列注释、多重测试程序、专业可视化、教程、文档等。
- en: Note
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on the Bioconductor project, visit the project website
    at [http://www.bioconductor.org](http://www.bioconductor.org).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Bioconductor 项目的更多信息，请访问项目网站 [http://www.bioconductor.org](http://www.bioconductor.org)。
- en: Analyzing and visualizing network data
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析和可视化网络数据
- en: Social network data and graph datasets consist of structures that describe connections,
    or **links** (sometimes also called **edges**), between people or objects known
    as **nodes**. With *N* nodes, a *N* x *N = N[2]* matrix of potential links can
    be created. This creates tremendous computational complexity as the number of
    nodes grows.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 社交网络数据和图形数据集由描述连接或 **链接**（有时也叫 **边**）的结构组成，这些连接发生在人或物体之间，这些人或物体称为 **节点**。对于
    *N* 个节点，可以创建一个 *N* x *N = N[2]* 的潜在链接矩阵。随着节点数量的增加，这会带来巨大的计算复杂性。
- en: The field of **network analysis** is concerned with statistical measures and
    visualizations that identify meaningful patterns of connections. For example,
    the following figure shows three clusters of circular nodes, all connected via
    a square node at the center. A network analysis may reveal the importance of the
    square node, among other key metrics.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络分析**领域关注的是识别连接的有意义模式的统计度量和可视化。例如，下面的图展示了三个由圆形节点组成的集群，所有这些节点通过位于中心的方形节点连接。网络分析可能揭示方形节点的重要性，以及其他关键度量。'
- en: '![Analyzing and visualizing network data](img/B03905_12_01.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![分析和可视化网络数据](img/B03905_12_01.jpg)'
- en: The `network` package by Carter T. Butts, David Hunter, and Mark S. Handcock
    offers a specialized data structure to work with networks. This data structure
    is necessary due to the fact that the matrix needed to store *N[2]* potential
    links will quickly exceed available memory; the `network` data structure uses
    a sparse representation to store only existent links, saving a great deal of memory
    if most relationships are nonexistent. A closely related package, `sna` (social
    network analysis), allows the analysis and visualization of the `network` objects.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Carter T. Butts、David Hunter 和 Mark S. Handcock 的 `network` 包提供了一种专门的数据结构用于处理网络。由于需要存储
    *N[2]* 个潜在链接的矩阵很快就会超过可用内存，因此这个数据结构是必要的；`network` 数据结构使用稀疏表示法，只存储存在的链接，如果大多数关系不存在，这可以节省大量内存。一个密切相关的包
    `sna`（社交网络分析）允许对 `network` 对象进行分析和可视化。
- en: Note
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on `network` and `sna`, including very detailed tutorials
    and documentation, refer to the project website hosted by the University of Washington
    at [http://www.statnet.org/](http://www.statnet.org/).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 `network` 和 `sna` 的更多信息，包括非常详细的教程和文档，请参考由华盛顿大学主办的项目网站 [http://www.statnet.org/](http://www.statnet.org/)。
- en: The `igraph` package by Gábor Csárdi provides another set of tools to visualize
    and analyze network data. It is capable of calculating metrics for very large
    networks. An additional benefit of `igraph` is the fact that it has analogous
    packages for the Python and C programming languages, allowing it to be used to
    perform analyses virtually anywhere. As we will demonstrate shortly, it is very
    easy to use.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Gábor Csárdi 的 `igraph` 包提供了一组用于可视化和分析网络数据的工具。它能够计算非常大的网络的度量。`igraph` 的另一个优点是，它拥有适用于
    Python 和 C 编程语言的类似包，使得可以在几乎任何地方进行分析。正如我们稍后将演示的，它非常易于使用。
- en: Note
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on the `igraph` package, including demos and tutorials,
    visit the homepage at [http://igraph.org/r/](http://igraph.org/r/).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 `igraph` 包的更多信息，包括演示和教程，请访问主页 [http://igraph.org/r/](http://igraph.org/r/)。
- en: Using network data in R requires the use of specialized formats, as network
    data are not typically stored in typical tabular data structures like CSV files
    and data frames. As mentioned previously, because there are *N[2]* potential connections
    between *N* network nodes, a tabular structure would quickly grow to be unwieldy
    for all but the smallest *N* values. Instead, graph data are stored in a form
    that lists only the connections that are truly present; absent connections are
    inferred from the absence of data.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中使用网络数据需要使用专门的格式，因为网络数据通常不会以CSV文件或数据框等典型的表格数据结构存储。如前所述，由于在*N*个网络节点之间有*N[2]*个潜在连接，除非是最小的*N*值，否则表格结构很快就会变得不适用。因此，图形数据通常以只列出实际存在的连接的形式存储；缺失的连接通过缺少数据来推断。
- en: 'Perhaps the simplest of such formats is **edgelist**, which is a text file
    with one line per network connection. Each node must be assigned a unique identifier
    and the links between the nodes are defined by placing the connected nodes'' identifiers
    together on a single line separated by a space. For instance, the following edgelist
    defines three connections between node 0 and nodes 1, 2, and 3:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 其中最简单的一种格式是**边列表**，这是一种文本文件，每行表示一个网络连接。每个节点必须分配一个唯一的标识符，节点之间的链接通过将连接节点的标识符放在同一行中并用空格分隔来定义。例如，以下边列表定义了节点0与节点1、2和3之间的三条连接：
- en: '[PRE29]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To load network data into R, the `igraph` package provides a `read.graph()`
    function that can read edgelist files as well as other more sophisticated formats
    like **Graph Modeling Language** (**GML**). To illustrate this functionality,
    we''ll use a dataset describing friendship among the members of a small karate
    club. To follow along, download the `karate.txt` file from the Packt Publishing
    website and save it in your R working directory. After you''ve installed the `igraph`
    package, the karate network can be read into R as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 要将网络数据加载到R中，`igraph`包提供了一个`read.graph()`函数，可以读取边列表文件以及其他更复杂的格式，如**图建模语言**（**GML**）。为了说明这一功能，我们将使用一个描述小型空手道俱乐部成员之间友谊的数据集。要跟随操作，请从Packt
    Publishing网站下载`karate.txt`文件并将其保存在R的工作目录中。安装了`igraph`包后，可以通过以下方式将空手道网络读入R中：
- en: '[PRE30]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This will create a sparse matrix object that can be used for graphing and network
    analysis. Note that the `directed = FALSE` parameter forces the network to use
    undirected or bidirectional links between the nodes. Since the karate dataset
    describes friendship, it means that if person 1 is friends with person 2, then
    person 2 must be friends with person 1\. On the other hand, if the dataset described
    fight outcomes, the fact that person 1 defeated person 2 would certainly not imply
    that person 2 defeated person 1\. In this case, the `directed = TRUE` parameter
    should be set.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个稀疏矩阵对象，可用于绘图和网络分析。注意，`directed = FALSE`参数强制网络使用节点之间的无向或双向链接。由于空手道数据集描述的是友谊关系，这意味着如果人1是人2的朋友，那么人2也必须是人1的朋友。另一方面，如果数据集描述的是战斗结果，那么人1击败人2并不意味着人2击败了人1。在这种情况下，应设置`directed
    = TRUE`参数。
- en: Note
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The karate network dataset used here was compiled by *M.E.J. Newman* of the
    University of Michigan. It was first presented in Zachary WW. *An information
    flow model for conflict and fission in small groups*. Journal of Anthropological
    Research. 1977; 33:452-473.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的空手道网络数据集由密歇根大学的*M.E.J. Newman*编制。该数据集首次出现在Zachary WW的《小组中的信息流模型：冲突与分裂》一文中。人类学研究期刊，1977；33：452-473。
- en: 'To examine the graph, use the `plot()` function:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看图形，可以使用`plot()`函数：
- en: '[PRE31]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This produces the following figure:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生如下图形：
- en: '![Analyzing and visualizing network data](img/B03905_12_02.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![分析和可视化网络数据](img/B03905_12_02.jpg)'
- en: Examining the network visualization, it is apparent that there are a few highly
    connected members of the karate club. Nodes 1, 33, and 34 seem to be more central
    than the others, which remain at the club periphery.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查网络可视化图，可以明显看到空手道俱乐部中有一些连接度较高的成员。节点1、33和34似乎比其他节点更为中心，其他节点则位于俱乐部的边缘。
- en: 'Using `igraph` to calculate graph metrics, it is possible to demonstrate our
    hunch analytically. The **degree** of a node measures how many nodes it is linked
    to. The `degree()` function confirms our hunch that nodes 1, 33, and 34 are more
    connected than the others with `16`, `12`, and `17` connections, respectively:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`igraph`计算图度量时，可以从分析中证明我们的直觉。**度数**是指节点连接的其他节点数量。`degree()`函数确认了我们的直觉，即节点1、33和34比其他节点更为连接，分别有`16`、`12`和`17`个连接：
- en: '[PRE32]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Because some connections are more important than others, a variety of network
    measures have been developed to measure node connectivity with this consideration.
    A network metric called **betweenness centrality** is intended to capture the
    number of shortest paths between nodes that pass through each node. Nodes that
    are truly more central to the entire graph will have a higher betweenness centrality
    value, because they act as a bridge between the other nodes. We obtain a vector
    of the centrality measures using the `betweenness()` function, as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 因为某些连接比其他连接更重要，所以已经开发出多种网络度量方法来衡量节点的连接性。一个名为 **介数中心性** 的网络度量旨在捕捉节点之间通过每个节点的最短路径数量。真正对整个图形更为核心的节点会有更高的介数中心性值，因为它们充当了其他节点之间的桥梁。我们可以使用
    `betweenness()` 函数获得中心性度量的向量，方法如下：
- en: '[PRE33]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As nodes 1 and 34 have much greater betweenness values than the others, they
    are more central to the karate club's friendship network. These two individuals,
    with extensive personal friendship networks, may be the "glue" that holds the
    network together.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于节点 1 和 34 的介数值远高于其他节点，它们在空手道俱乐部的友谊网络中更加核心。这两个拥有广泛个人友谊网络的人，可能是将整个网络联系在一起的“粘合剂”。
- en: Tip
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Betweenness centrality is only one of many metrics intended to capture a node's
    importance, and it isn't even the only measure of centrality. Refer to the `igraph`
    documentation for definitions of other network properties.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 介数中心性只是众多用于捕捉节点重要性的度量之一，它甚至不是唯一的中心性度量。请参考 `igraph` 文档以获取其他网络属性的定义。
- en: The `sna` and `igraph` packages are capable of computing many such graph metrics,
    which may then be used as inputs to machine learning functions. For example, suppose
    we were attempting to build a model predicting who would win an election for the
    club's president. The fact that nodes 1 and 34 are well-connected suggests that
    they may have the social capital needed for such a leadership role. These might
    be the highly valuable predictors of the election's results.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`sna` 和 `igraph` 包能够计算许多此类图形度量，计算结果可以作为机器学习函数的输入。例如，假设我们试图建立一个预测谁会赢得俱乐部会长选举的模型。节点
    1 和 34 之间的良好连接表明他们可能具备担任此类领导角色所需的社交资本。这些可能是选举结果的关键预测因子。'
- en: Tip
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: By combining network analysis with machine learning, services like Facebook,
    Twitter, and LinkedIn provide vast stores of network data to make predictions
    about the users' future behavior. A high-profile example is the 2012 U.S. Presidential
    campaign in which chief data scientist Rayid Ghani utilized Facebook data to identify
    people who might be persuaded to vote for Barack Obama.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将网络分析与机器学习相结合，像 Facebook、Twitter 和 LinkedIn 这样的服务提供了大量的网络数据，用以预测用户未来的行为。一个引人注目的例子是
    2012 年美国总统竞选，在该竞选中，首席数据科学家 Rayid Ghani 利用 Facebook 数据识别出那些可能被说服去投票给巴拉克·奥巴马的人。
- en: Improving the performance of R
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高 R 性能
- en: R has a reputation for being slow and memory-inefficient, a reputation that
    is at least somewhat earned. These faults are largely unnoticed on a modern PC
    for datasets of many thousands of records, but datasets with a million records
    or more can exceed the limits of what is currently possible with consumer-grade
    hardware. The problem worsens if the dataset contains many features or if complex
    learning algorithms are being used.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: R 以运行缓慢和内存效率低而著称，这种声誉至少在某种程度上是当之无愧的。对于包含数千条记录的数据集，这些缺点在现代 PC 上通常不易察觉，但当数据集包含百万条记录或更多时，可能会超出当前消费级硬件所能处理的极限。如果数据集包含许多特征或使用复杂的学习算法，这个问题会更加严重。
- en: Note
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: CRAN has a high-performance computing task view that lists packages pushing
    the boundaries of what is possible in R. It can be viewed at [http://cran.r-project.org/web/views/HighPerformanceComputing.html](http://cran.r-project.org/web/views/HighPerformanceComputing.html).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: CRAN 提供了一个高性能计算任务视图，列出了推动 R 功能极限的相关包。可以通过 [http://cran.r-project.org/web/views/HighPerformanceComputing.html](http://cran.r-project.org/web/views/HighPerformanceComputing.html)
    进行查看。
- en: 'Packages that extend R past the capabilities of the base software are being
    developed rapidly. This work comes primarily on two fronts: some packages add
    the capability to manage extremely large datasets by making data operations faster
    or allowing the size of the data to exceed the amount of available system memory;
    others allow R to work faster, perhaps by spreading the work over additional computers
    or processors, utilizing specialized computer hardware, or providing machine learning
    algorithms optimized for big data problems.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展 R 功能的包正在快速开发。这项工作主要体现在两个方面：一些包通过加快数据操作速度或允许数据大小超过可用系统内存的限制，来管理极大的数据集；其他包则使
    R 工作更高效，可能通过将工作分布到更多计算机或处理器上，利用专用硬件，或提供针对大数据问题优化的机器学习算法。
- en: Managing very large datasets
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理超大数据集
- en: Extremely large datasets can cause R to grind to a halt when the system runs
    out of memory to store data. Even if the entire dataset can fit into the available
    memory, additional memory overhead will be needed for data processing. Furthermore,
    very large datasets can take a long amount of time to analyze for no reason other
    than the sheer volume of records; even a quick operation can cause delays when
    performed many millions of times.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 极大的数据集可能会导致 R 在系统内存不足以存储数据时陷入停顿。即使整个数据集能够装入可用内存，数据处理仍然需要额外的内存开销。此外，极大的数据集可能会因为记录量庞大而花费很长时间进行分析；即使是简单操作，在进行数百万次时也可能会造成延迟。
- en: Years ago, many would perform data preparation outside R in another programming
    language, or use R but perform analyses on a smaller subset of data. However,
    this is no longer necessary, as several packages have been contributed to R to
    address these problems.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 多年前，许多人会在 R 外的其他编程语言中进行数据准备，或使用 R 但只在数据的较小子集上进行分析。然而，现在不再需要这样做，因为已经有多个包被贡献到
    R 中，以解决这些问题。
- en: Generalizing tabular data structures with dplyr
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 dplyr 对表格数据结构进行泛化
- en: The `dplyr` package introduced in 2014 by Hadley Wickham and Romain Francois
    is perhaps the most straightforward way to begin working with large datasets in
    R. Though other packages may exceed its capabilities in terms of raw speed or
    the raw size of the data, `dplyr` is still quite capable. More importantly, it
    is virtually transparent after the initial learning curve has passed.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`dplyr` 包由 Hadley Wickham 和 Romain Francois 于 2014 年推出，它可能是开始在 R 中处理大数据集最直接的方式。尽管其他包在原始速度或数据大小上可能超过它的能力，`dplyr`
    仍然非常强大。更重要的是，在初步学习曲线过后，它几乎是透明的。'
- en: Note
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on `dplyr`, including some very helpful tutorials, refer
    to the project's GitHub page at [https://github.com/hadley/dplyr](https://github.com/hadley/dplyr).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 `dplyr` 的更多信息，包括一些非常有用的教程，请访问该项目的 GitHub 页面：[https://github.com/hadley/dplyr](https://github.com/hadley/dplyr)。
- en: 'Put simply, the package provides an object called `tbl`, which is an abstraction
    of tabular data. It acts much like a data frame, with several important exceptions:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，该包提供了一个名为 `tbl` 的对象，它是表格数据的抽象。它的作用类似于数据框，但有几个重要的例外：
- en: Key functionality has been written in C++, which according to the authors results
    in a 20x to 1000x performance increase for many operations.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键功能已经用 C++ 编写，作者表示，这对于许多操作能带来 20 倍到 1000 倍的性能提升。
- en: R data frames are limited by available memory. The `dplyr` version of a data
    frame can be linked transparently to disk-based databases that can exceed what
    can be stored in memory.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R 数据框受到可用内存的限制。`dplyr` 版本的数据框可以透明地与基于磁盘的数据库链接，这些数据库的存储量超过了内存能容纳的数据。
- en: The `dplyr` package makes reasonable assumptions about data frames that optimize
    your effort as well as memory use. It doesn't automatically change data types.
    And, if possible, it avoids making copies of data by pointing to the original
    value instead.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dplyr` 包对数据框做出合理假设，从而优化了你的工作效率和内存使用。它不会自动改变数据类型。而且，如果可能，它会避免通过指向原始值来创建数据副本。'
- en: New operators are introduced that allow common data transformations to be performed
    with much less code while remaining highly readable.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新的操作符被引入，可以用更少的代码完成常见的数据转换，同时保持高度可读性。
- en: 'Making the transition from data frames to `dplyr` is easy. To convert an existing
    data frame into a `tbl` object, use the `as.tbl()` function:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据框转换到 `dplyr` 非常简单。要将现有的数据框转换为 `tbl` 对象，可以使用 `as.tbl()` 函数：
- en: '[PRE34]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Typing the name of the table provides information about the object. Even here,
    we see a distinction between `dplyr` and typical R behavior; where as a traditional
    data frame would have displayed many rows of data, `dplyr` objects are more considerate
    of real-world needs. For example, typing the name of the object provides output
    summarized in a form that fits a single screen:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 输入表名会提供有关该对象的信息。即使在这里，我们也看到了`dplyr`和典型R行为之间的区别；传统的数据框会显示大量数据行，而`dplyr`对象则更考虑实际需求。例如，输入对象名称时，输出会以适合单屏显示的形式进行总结：
- en: '[PRE35]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![Generalizing tabular data structures with dplyr](img/B03905_12_03.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![使用dplyr推广表格数据结构](img/B03905_12_03.jpg)'
- en: Connecting `dplyr` to an external database is straightforward as well. The `dplyr`
    package provides functions to connect to MySQL, PostgreSQL, and SQLite databases.
    These create a connection object that allows `tbl` objects to be pulled from the
    database.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 将`dplyr`连接到外部数据库也很简单。`dplyr`包提供了连接到MySQL、PostgreSQL和SQLite数据库的函数。这些函数创建一个连接对象，允许从数据库中提取`tbl`对象。
- en: 'Let''s use the `src_sqlite()` function to create a SQLite database to store
    credit data. SQLite is a simple database that doesn''t require a server. It simply
    connects to a database file, which we''ll call `credit.sqlite3`. Since the file
    doesn''t exist yet, we need to set the `create = TRUE` parameter to create the
    file. Note that for this step to work, you may require to install the `RSQLite`
    package if you have not already done so:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`src_sqlite()`函数创建一个SQLite数据库来存储信用数据。SQLite是一个简单的数据库，不需要服务器，它只是连接到一个数据库文件，我们将其命名为`credit.sqlite3`。由于该文件尚未创建，我们需要设置`create
    = TRUE`参数来创建该文件。请注意，要使此步骤正常工作，如果您尚未安装`RSQLite`包，可能需要先安装它：
- en: '[PRE36]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'After creating the connection, we need to load the data into the database using
    the `copy_to()` function. This uses the `credit_tbl` object to create a database
    table within the database specified by `credit_db_conn`. The `temporary = FALSE`
    parameter forces the table to be created immediately. Since `dplyr` tries to avoid
    copying data unless it must, it will only create the table if it is explicitly
    asked to:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 创建连接后，我们需要使用`copy_to()`函数将数据加载到数据库中。此函数使用`credit_tbl`对象在由`credit_db_conn`指定的数据库中创建一个数据库表。`temporary
    = FALSE`参数强制立即创建该表。由于`dplyr`尽量避免复制数据，除非必须，它只有在明确要求时才会创建表：
- en: '[PRE37]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Executing the `copy_to()` function will store the data in the `credit.sqlite3`
    file, which can be transported to other systems as needed. To access this file
    later, simply reopen the database connection and create a `tbl` object, as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 执行`copy_to()`函数将把数据存储到`credit.sqlite3`文件中，该文件可以根据需要传输到其他系统。要稍后访问该文件，只需重新打开数据库连接并创建`tbl`对象，如下所示：
- en: '[PRE38]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In spite of the fact that `dplyr` is routed through a database, the `credit_tbl`
    object here will act exactly like any other `tbl` object and will gain all the
    other benefits of the `dplyr` package.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`dplyr`通过数据库进行路由，但这里的`credit_tbl`对象将像其他任何`tbl`对象一样工作，并且将获得`dplyr`包的所有其他好处。
- en: Making data frames faster with data.table
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用`data.table`加速数据框操作
- en: The `data.table` package by Matt Dowle, Tom Short, Steve Lianoglou, and Arun
    Srinivasan provides an enhanced version of a data frame called a **data table**.
    The `data.table` objects are typically much faster than data frames for subsetting,
    joining, and grouping operations. For the largest datasets—those with many millions
    of rows—these objects may be substantially faster than even `dplyr` objects. Yet,
    because it is essentially an improved data frame, the resulting objects can still
    be used by any R function that accepts a data frame.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`data.table`包由Matt Dowle、Tom Short、Steve Lianoglou和Arun Srinivasan开发，提供了一种称为**数据表**的增强版数据框。与数据框相比，`data.table`对象在子集化、连接和分组操作中通常更快。对于最大的数据集——包含数百万行的数据集——这些对象可能比`dplyr`对象更快。然而，由于它本质上是一个改进版的数据框，生成的对象仍然可以被任何接受数据框的R函数使用。'
- en: Note
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `data.table` project can be found on GitHub at [https://github.com/Rdatatable/data.table/wiki](https://github.com/Rdatatable/data.table/wiki).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`data.table`项目可以在GitHub上找到，网址是[https://github.com/Rdatatable/data.table/wiki](https://github.com/Rdatatable/data.table/wiki)。'
- en: 'After installing the `data.table` package, the `fread()` function will read
    tabular files like CSVs into data table objects. For instance, to load the credit
    data used previously, type:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`data.table`包后，`fread()`函数将读取类似CSV的表格文件并将其转换为数据表对象。例如，要加载之前使用的信用数据，可以输入：
- en: '[PRE39]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The credit data table can then be queried using syntax similar to R''s `[row,
    col]` form, but optimized for speed and some additional useful conveniences. In
    particular, the data table structure allows the `row` portion to select rows using
    an abbreviated subsetting command, and the `col` portion to use a function that
    does something with the selected rows. For example, the following command computes
    the mean requested loan amount for people with a good credit history:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以使用类似于 R 中 `[row, col]` 形式的语法查询信用数据表，但它经过优化以提高速度并提供一些额外的实用功能。特别是，数据表结构允许
    `row` 部分使用简化的子集命令选择行，`col` 部分则可以使用一个对所选行执行某些操作的函数。例如，以下命令计算具有良好信用历史的人的平均请求贷款金额：
- en: '[PRE40]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: By building larger queries with this simple syntax, very complex operations
    can be performed on data tables. Since the data structure is optimized for speed,
    it can be used with large datasets.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这种简单的语法构建更大的查询，可以在数据表上执行非常复杂的操作。由于数据结构经过优化以提高速度，因此可以在大型数据集上使用。
- en: One limitation of the `data.table` structures is that like data frames they
    are limited by the available system memory. The next two sections discuss packages
    that overcome this shortcoming at the expense of breaking compatibility with many
    R functions.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`data.table` 结构的一个限制是，与数据框一样，它们受限于系统可用内存。接下来的两个章节讨论了可以克服这一缺点的包，但代价是破坏与许多 R
    函数的兼容性。'
- en: Tip
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The `dplyr` and `data.table` packages each have unique strengths. For an in-depth
    comparison, check out the following Stack Overflow discussion at [http://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly](http://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly).
    It is also possible to have the best of both worlds, as `data.table` structures
    can be loaded into `dplyr` using the `tbl_dt()` function.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`dplyr` 和 `data.table` 包各自有独特的优点。要进行深入比较，可以查看以下 Stack Overflow 讨论：[http://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly](http://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly)。也可以两者兼得，因为
    `data.table` 结构可以通过 `tbl_dt()` 函数加载到 `dplyr` 中。'
- en: Creating disk-based data frames with ff
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 ff 创建基于磁盘的数据框
- en: The `ff` package by Daniel Adler, Christian Gläser, Oleg Nenadic, Jens Oehlschlägel,
    and Walter Zucchini provides an alternative to a data frame (`ffdf`) that allows
    datasets of over two billion rows to be created, even if this far exceeds the
    available system memory.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Daniel Adler、Christian Gläser、Oleg Nenadic、Jens Oehlschlägel 和 Walter Zucchini
    提供的 `ff` 包提供了一种替代数据框（`ffdf`）的方法，允许创建超过二十亿行的数据集，即使这远超出可用系统内存。
- en: The `ffdf` structure has a physical component that stores the data on a disk
    in a highly efficient form, and a virtual component that acts like a typical R
    data frame, but transparently points to the data stored in the physical component.
    You can imagine the `ffdf` object as a map that points to a location of the data
    on a disk.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`ffdf` 结构具有一个物理组件，用于以高效的形式将数据存储在磁盘上，和一个虚拟组件，类似于典型的 R 数据框，但透明地指向存储在物理组件中的数据。你可以把
    `ffdf` 对象想象成一个指向磁盘上数据位置的映射。'
- en: Note
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `ff` project is on the Web at [http://ff.r-forge.r-project.org/](http://ff.r-forge.r-project.org/).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`ff` 项目可以在网上访问：[http://ff.r-forge.r-project.org/](http://ff.r-forge.r-project.org/)。'
- en: A downside of `ffdf` data structures is that they cannot be used natively by
    most R functions. Instead, the data must be processed in small chunks, and the
    results must be combined later on. The upside of chunking the data is that the
    task can be divided across several processors simultaneously using the parallel
    computing methods presented later in this chapter.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`ffdf` 数据结构的一个缺点是大多数 R 函数不能原生使用它们。相反，数据必须分块处理，结果必须稍后合并。分块处理数据的好处是，可以使用本章后面介绍的并行计算方法同时在多个处理器上进行任务分配。'
- en: 'After installing the `ff` package, to read in a large CSV file, use the `read.csv.ffdf()`
    function, as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 `ff` 包后，要读取一个大型 CSV 文件，可以使用 `read.csv.ffdf()` 函数，示例如下：
- en: '[PRE41]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Unfortunately, we cannot work directly with the `ffdf` object, as attempting
    to treat it like a traditional data frame results in an error message:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们无法直接与 `ffdf` 对象进行操作，因为试图将其当作传统数据框使用会导致错误消息：
- en: '[PRE42]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The `ffbase` package by Edwin de Jonge, Jan Wijffels, and Jan van der Laan
    addresses this issue somewhat by adding capabilities for basic analyses using
    `ff` objects. This makes it possible to use `ff` objects directly for data exploration.
    For instance, after installing the `ffbase` package, the mean function works as
    expected:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Edwin de Jonge、Jan Wijffels和Jan van der Laan所开发的`ffbase`软件包通过为`ff`对象提供基本分析功能，部分解决了这一问题。这样可以直接使用`ff`对象进行数据探索。例如，安装`ffbase`软件包后，均值函数将按预期工作：
- en: '[PRE43]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The package also provides other basic functionality such as mathematical operators,
    query functions, summary statistics, and wrappers to work with optimized machine
    learning algorithms like `biglm` (described later in this chapter). Though these
    do not completely eliminate the challenges of working with extremely large datasets,
    they make the process a bit more seamless.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 该软件包还提供了其他基本功能，如数学运算符、查询函数、摘要统计和与优化过的机器学习算法（如`biglm`，将在本章后面描述）配合使用的包装器。虽然这些功能不能完全消除处理极大数据集时的挑战，但它们使整个过程变得更加顺畅。
- en: Note
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on advanced functionality, visit the `ffbase` project site
    at [http://github.com/edwindj/ffbase](http://github.com/edwindj/ffbase).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于高级功能的信息，请访问`ffbase`项目网站：[http://github.com/edwindj/ffbase](http://github.com/edwindj/ffbase)。
- en: Using massive matrices with bigmemory
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用bigmemory的大型矩阵
- en: The `bigmemory` package by Michael J. Kane, John W. Emerson, and Peter Haverty
    allows the use of extremely large matrices that exceed the amount of available
    system memory. The matrices can be stored on a disk or in shared memory, allowing
    them to be used by other processes on the same computer or across a network. This
    facilitates parallel computing methods, such as the ones covered later in this
    chapter.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Michael J. Kane、John W. Emerson和Peter Haverty开发的`bigmemory`软件包允许使用超出可用系统内存的大型矩阵。这些矩阵可以存储在磁盘或共享内存中，使得它们可以被同一计算机上的其他进程或通过网络的进程使用。这促进了并行计算方法的应用，如本章后面讨论的方法。
- en: Note
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Additional documentation on the `bigmemory` package can be found at [http://www.bigmemory.org/](http://www.bigmemory.org/).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 相关文档可在`bigmemory`软件包的网站找到：[http://www.bigmemory.org/](http://www.bigmemory.org/)。
- en: Because `bigmemory` matrices are intentionally unlike data frames, they cannot
    be used directly with most of the machine learning methods covered in this book.
    They also can only be used with numeric data. That said, since they are similar
    to a typical R matrix, it is easy to create smaller samples or chunks that can
    be converted into standard R data structures.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`bigmemory`矩阵与数据框有意不同，它们不能直接与本书中涵盖的大多数机器学习方法一起使用。它们也只能用于数值数据。尽管如此，鉴于它们与典型的R矩阵相似，创建可以转换为标准R数据结构的较小样本或数据块是很容易的。
- en: The authors also provide the `bigalgebra`, `biganalytics`, and `bigtabulate`
    packages, which allow simple analyses to be performed on the matrices. Of particular
    note is the `bigkmeans()` function in the `biganalytics` package, which performs
    k-means clustering as described in [Chapter 9](ch09.html "Chapter 9. Finding Groups
    of Data – Clustering with k-means"), *Finding Groups of Data – Clustering with
    k-means*. Due to the highly specialized nature of these packages, use cases are
    outside the scope of this chapter.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们还提供了`bigalgebra`、`biganalytics`和`bigtabulate`软件包，允许对这些矩阵执行简单的分析。特别值得注意的是，`biganalytics`包中的`bigkmeans()`函数，它执行如[第9章](ch09.html
    "第9章 数据分组—使用k均值聚类")中所描述的k均值聚类，*数据分组—使用k均值聚类*。由于这些软件包的高度专业化，使用案例超出了本章的范围。
- en: Learning faster with parallel computing
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用并行计算加速学习
- en: In the early days of computing, processors executed instructions in **serial**
    which meant that they were limited to performing a single task at a time. The
    next instruction could not be started until the previous instruction was complete.
    Although it was widely known that many tasks could be completed more efficiently
    by completing the steps simultaneously, the technology simply did not exist yet.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机早期，处理器以**串行**方式执行指令，这意味着它们一次只能执行一个任务。在完成上一条指令之前，下一条指令无法开始。尽管广泛认为通过同时完成多个步骤可以更高效地完成许多任务，但当时的技术尚不支持这一点。
- en: '![Learning faster with parallel computing](img/B03905_12_04.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![使用并行计算加速学习](img/B03905_12_04.jpg)'
- en: This was addressed by the development of **parallel computing** methods, which
    use a set of two or more processors or computers to solve a larger problem. Many
    modern computers are designed for parallel computing. Even in the cases in which
    they have a single processor, they often have two or more **cores** that are capable
    of working in parallel. This allows tasks to be accomplished independently of
    one another.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题通过**并行计算**方法得到了改进，后者使用两台或更多的处理器或计算机来解决更大的问题。许多现代计算机都设计为支持并行计算。即使它们只有一个处理器，它们通常也拥有两个或更多能够并行工作的**核心**。这使得任务可以独立完成。
- en: '![Learning faster with parallel computing](img/B03905_12_05.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![使用并行计算加速学习](img/B03905_12_05.jpg)'
- en: Networks of multiple computers called **clusters** can also be used for parallel
    computing. A large cluster may include a variety of hardware and be separated
    over large distances. In this case, the cluster is known as a **grid**. Taken
    to an extreme, a cluster or grid of hundreds or thousands of computers running
    commodity hardware could be a very powerful system.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 多台计算机组成的网络，称为**集群**，也可以用于并行计算。一个大型集群可能包含多种硬件，并且分布在较远的距离上。在这种情况下，集群被称为**网格**。如果将其推向极限，使用普通硬件的数百或数千台计算机组成的集群或网格可能成为一个非常强大的系统。
- en: The catch, however, is that not every problem can be parallelized. Certain problems
    are more conducive to parallel execution than others. One might expect that adding
    100 processors would result in accomplishing 100 times the work in the same amount
    of time (that is, the overall execution time would be 1/100), but this is typically
    not the case. The reason is that it takes effort to manage the workers. Work must
    be divided into equal, nonoverlapping tasks, and each of the workers' results
    must be combined into one final answer.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，问题是，并不是所有问题都可以并行化。有些问题更适合并行执行。一些人可能期望添加100个处理器会使得在相同的时间内完成100倍的工作（即整体执行时间为1/100），但通常并非如此。原因是管理这些工作者需要付出努力。工作必须被分割成等量的、不重叠的任务，并且每个工作者的结果必须合并成最终答案。
- en: So-called **embarrassingly parallel** problems are ideal. It is easy to reduce
    these tasks into nonoverlapping blocks of work and recombine the results. An example
    of an embarrassingly parallel machine learning task would be 10-fold cross-validation;
    once the 10 samples are divided, each of the 10 blocks of work is independent,
    meaning that they do not affect the others. As you will soon see, this task can
    be sped up quite dramatically using parallel computing.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 所谓的**极度并行**问题是理想的。这类任务容易被拆分为不重叠的工作块，并且可以重新组合结果。一个极度并行的机器学习任务例子是10折交叉验证；一旦将10个样本划分好，每一个工作块都是独立的，意味着它们不会相互影响。正如你很快将看到的，这项任务可以通过并行计算显著加速。
- en: Measuring execution time
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测量执行时间
- en: Efforts to speed up R will be wasted if it is not possible to systematically
    measure how much time is saved. Although a stopwatch is one option, an easier
    solution would be to wrap the code in a `system.time()` function.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果无法系统地衡量节省的时间，那么加速 R 的努力将会白费。虽然使用秒表是一种选择，但一个更简单的解决方案是将代码封装在`system.time()`函数中。
- en: 'For example, on my laptop, the `system.time()` function notes that it takes
    about `0.093` seconds to generate a million random numbers:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我的笔记本电脑上，`system.time()`函数显示生成100万个随机数大约需要`0.093`秒：
- en: '[PRE44]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The same function can be used to evaluate the improvement in performance obtained
    by using the methods that were just described or any R function.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的函数可以用来评估使用刚才描述的方法或任何R函数所获得的性能提升。
- en: Note
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For what it's worth, when the first edition was published, generating a million
    random numbers took 0.13 seconds. Although I'm now using a slightly more powerful
    computer, this reduction of about 30 percent of the processing time just two years
    later illustrates how quickly computer hardware and software are improving.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 说到这里，值得一提的是，当第一版发布时，生成100万个随机数需要0.13秒。尽管我现在使用的计算机稍微更强大一些，但仅仅两年后，处理时间减少了约30%，这展示了计算机硬件和软件是如何快速发展的。
- en: Working in parallel with multicore and snow
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用多核和雪崩并行工作
- en: The `parallel` package, now included with R version 2.14.0 and higher, has lowered
    the entry barrier to deploy parallel algorithms by providing a standard framework
    to set up worker processes that can complete tasks simultaneously. It does this
    by including components of the `multicore` and `snow` packages, each taking a
    different approach towards multitasking.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`parallel`包，现在已包含在R版本2.14.0及更高版本中，降低了部署并行算法的门槛，提供了一个标准框架来设置工作进程，使它们可以同时完成任务。它通过包含`multicore`和`snow`包的组件来实现，每个组件对多任务处理采取不同的方法。'
- en: 'If your computer is reasonably recent, you are likely to be able to use parallel
    processing. To determine the number of cores your machine has, use the `detectCores()`
    function as follows. Note that your output will differ depending on your hardware
    specifications:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的计算机相对较新，你很可能能够使用并行处理。要确定你的机器有多少个核心，可以使用`detectCores()`函数，如下所示。请注意，输出结果会根据你的硬件规格有所不同：
- en: '[PRE45]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The `multicore` package was developed by Simon Urbanek and allows parallel processing
    on a single machine that has multiple processors or processor cores. It utilizes
    the multitasking capabilities of a computer's operating system to **fork** additional
    R sessions that share the same memory. It is perhaps the simplest way to get started
    with parallel processing in R. Unfortunately, because Windows does not support
    forking, this solution will not work everywhere.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`multicore`包由Simon Urbanek开发，允许在具有多个处理器或处理器核心的单台机器上进行并行处理。它利用计算机操作系统的多任务能力，通过**fork**额外的R会话，共享相同的内存。它可能是开始使用R进行并行处理的最简单方法。不幸的是，由于Windows不支持fork，这种解决方案并非适用于所有环境。'
- en: 'An easy way to get started with the `multicore` functionality is to use the
    `mclapply()` function, which is a parallel version of `lapply()`. For instance,
    the following blocks of code illustrate how the task of generating a million random
    numbers can be divided across 1, 2, 4, and 8 cores. The `unlist()` function is
    used to combine the parallel results (a list) into a single vector after each
    core has completed its chunk of work:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`multicore`功能的一个简单方法是使用`mclapply()`函数，它是`lapply()`的并行版本。例如，以下代码块演示了如何将生成一百万个随机数的任务分配到1、2、4和8个核心上。在每个核心完成其任务后，使用`unlist()`函数将并行结果（一个列表）合并成一个单一的向量：
- en: '[PRE46]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Notice how as the number of cores increases, the elapsed time decreases, and
    the benefit tapers off. Though this is a simple example, it can be adapted easily
    to many other tasks.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当核心数增加时，经过的时间减少，但收益逐渐减小。虽然这是一个简单的例子，但它可以轻松地适应许多其他任务。
- en: 'The `snow` package (simple networking of workstations) by Luke Tierney, A.
    J. Rossini, Na Li, and H. Sevcikova allows parallel computing on multicore or
    multiprocessor machines as well as on a network of multiple machines. It is slightly
    more difficult to use, but offers much more power and flexibility. After installing
    `snow`, to set up a cluster on a single machine, use the `makeCluster()` function
    with the number of cores to be used:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`snow`包（工作站的简单网络连接）由Luke Tierney、A. J. Rossini、Na Li和H. Sevcikova开发，允许在多核或多处理器机器以及多个机器的网络上进行并行计算。它使用起来稍微有些复杂，但提供了更多的能力和灵活性。安装`snow`后，要在单台机器上设置集群，可以使用`makeCluster()`函数，并指定要使用的核心数量：'
- en: '[PRE47]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Because `snow` communicates via network traffic, depending on your operating
    system, you may receive a message to approve access through your firewall.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`snow`是通过网络通信的，根据你的操作系统，你可能会收到一条消息，要求批准通过防火墙的访问。
- en: 'To confirm whether the cluster is operational, we can ask each node to report
    back its hostname. The `clusterCall()` function executes a function on each machine
    in the cluster. In this case, we''ll define a function that simply calls the `Sys.info()`
    function and returns the `nodename` parameter:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认集群是否正常运行，我们可以要求每个节点报告其主机名。`clusterCall()`函数会在集群中的每台机器上执行一个函数。在这个例子中，我们将定义一个简单的函数，它调用`Sys.info()`函数并返回`nodename`参数：
- en: '[PRE48]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Unsurprisingly, since all four nodes are running on a single machine, they
    report back the same hostname. To have the four nodes run a different command,
    supply them with a unique parameter via the `clusterApply()` function. Here, we''ll
    supply each node with a different letter. Each node will then perform a simple
    function on its letter in parallel:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，由于所有四个节点都在同一台机器上运行，它们会返回相同的主机名。为了让四个节点运行不同的命令，可以通过`clusterApply()`函数为它们提供一个唯一的参数。在这里，我们将为每个节点提供一个不同的字母。然后，每个节点将并行地在其字母上执行一个简单的函数：
- en: '[PRE49]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Once we''re done with the cluster, it''s important to terminate the processes
    it spawned. This will free up the resources each node is using:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成了集群的操作，重要的是终止它所启动的进程。这将释放每个节点所占用的资源：
- en: '[PRE50]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Using these simple commands, it is possible to speed up many machine learning
    tasks. For larger big data problems, much more complex `snow` configurations are
    possible. For instance, you may attempt to configure a **Beowulf cluster**—a network
    of many consumer-grade machines. In academic and industry research settings with
    dedicated computing clusters, `snow` can use the `Rmpi` package to access these
    high-performance **message-passing interface** (**MPI**) servers. Working with
    such clusters requires the knowledge of network configurations and computing hardware,
    which is outside the scope of this book.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些简单的命令，可以加速许多机器学习任务。对于更大的大数据问题，可以配置更复杂的`snow`配置。例如，你可以尝试配置一个**Beowulf集群**——由许多消费级机器组成的网络。在学术和工业研究环境中，使用专用计算集群时，`snow`可以利用`Rmpi`包访问这些高性能的**消息传递接口**（**MPI**）服务器。与此类集群合作需要了解网络配置和计算硬件，这超出了本书的范围。
- en: Note
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For a much more detailed introduction to `snow`, including some information
    on how to configure parallel computing on several computers over a network, see
    [http://homepage.stat.uiowa.edu/~luke/classes/295-hpc/notes/snow.pdf](http://homepage.stat.uiowa.edu/~luke/classes/295-hpc/notes/snow.pdf).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解关于`snow`的更详细介绍，包括如何在多台计算机之间通过网络配置并行计算的相关信息，请参阅[http://homepage.stat.uiowa.edu/~luke/classes/295-hpc/notes/snow.pdf](http://homepage.stat.uiowa.edu/~luke/classes/295-hpc/notes/snow.pdf)。
- en: Taking advantage of parallel with foreach and doParallel
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 利用并行与foreach和doParallel
- en: The `foreach` package by Steve Weston of Revolution Analytics provides perhaps
    the easiest way to get started with parallel computing, particularly if you are
    running R on Windows, as some of the other packages are platform-specific.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 由Revolution Analytics的Steve Weston开发的`foreach`包提供了可能是最简单的并行计算入门方式，特别是如果你在Windows上运行R，因为其他一些包是特定于平台的。
- en: The core of the package is a new `foreach` looping construct. If you have worked
    with other programming languages, you may be familiar with it. Essentially, it
    allows looping over a number of items in a set without explicitly counting the
    number of items; in other words, *for each* item in the set, *do* something.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 该包的核心是一个新的`foreach`循环结构。如果你曾使用过其他编程语言，可能对它有所了解。它本质上允许在一个集合中的多个项上进行循环，而无需显式地计算项的数量；换句话说，*对于每一个*集合中的项，*执行*某些操作。
- en: Note
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In addition to the `foreach` package, Revolution Analytics (recently acquired
    by Microsoft) has developed high-performance, enterprise-ready R builds. Free
    versions are available for trial and academic use. For more information, see their
    website at [http://www.revolutionanalytics.com/](http://www.revolutionanalytics.com/).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`foreach`包外，Revolution Analytics（最近被微软收购）还开发了高性能、企业级的R构建版本。免费版本可供试用和学术使用。欲了解更多信息，请访问他们的网站：[http://www.revolutionanalytics.com/](http://www.revolutionanalytics.com/)。
- en: 'If you''re thinking that R already provides a set of apply functions to loop
    over the sets of items (for example, `apply()`, `lapply()`, `sapply()`, and so
    on), you are correct. However, the `foreach` loop has an additional benefit: iterations
    of the loop can be completed in parallel using a very simple syntax. Let''s see
    how this works.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为R已经提供了一组用于循环集合项的apply函数（例如，`apply()`、`lapply()`、`sapply()`等等），你是对的。然而，`foreach`循环有一个额外的好处：循环的迭代可以使用非常简单的语法并行完成。让我们看看它是如何工作的。
- en: 'Recall the command we''ve been using to generate a million random numbers:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们一直在使用的生成一百万个随机数的命令：
- en: '[PRE51]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'After the `foreach` package has been installed, it can be expressed by a loop
    that generates four sets of 250,000 random numbers in parallel. The `.combine`
    parameter is an optional setting that tells `foreach` which function it should
    use to combine the final set of results from each loop iteration. In this case,
    since each iteration generates a set of random numbers, we simply use the `c()`
    concatenate function to create a single, combined vector:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`foreach`包后，它可以通过一个循环来表示，该循环并行生成四组各250,000个随机数。`.combine`参数是一个可选设置，用来告诉`foreach`它应该使用哪个函数来组合每次循环迭代的最终结果。在这种情况下，由于每次迭代都会生成一组随机数，我们只是使用`c()`连接函数来创建一个单一的合并向量：
- en: '[PRE52]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'If you noticed that this function didn''t result in a speed improvement, good
    catch! The reason is that by default, the `foreach` package runs each loop iteration
    in serial. The `doParallel` sister package provides a parallel backend for `foreach`
    that utilizes the `parallel` package included with R, which was described earlier
    in this chapter. After installing the `doParallel` package, simply register the
    number of cores and swap the `%do%` command with `%dopar%`, as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你注意到这个函数没有带来速度改进，那就很好！原因是默认情况下 `foreach` 软件包在串行模式下运行每个循环迭代。`doParallel` 的姐妹软件包为
    `foreach` 提供了一个并行后端，利用了本章早些时候描述的 R 包中包含的 `parallel` 软件包。安装 `doParallel` 软件包后，只需注册核心数量，并将
    `%do%` 命令替换为 `%dopar%`，如下所示：
- en: '[PRE53]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: As shown in the output, this code results in the expected performance improvement,
    nearly cutting the execution time in half.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如输出所示，此代码导致预期的性能改进，几乎将执行时间减少了一半。
- en: 'To close the `doParallel` cluster, simply type:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 要关闭 `doParallel` 集群，只需键入：
- en: '[PRE54]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Though the cluster will be closed automatically at the conclusion of the R session,
    it is better form to do so explicitly.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在 R 会话结束时集群会自动关闭，但最好还是显式关闭。
- en: Parallel cloud computing with MapReduce and Hadoop
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 MapReduce 和 Hadoop 进行并行云计算
- en: 'The **MapReduce** programming model was developed at Google as a way to process
    their data on a large cluster of networked computers. MapReduce defined parallel
    programming as a two-step process:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '**MapReduce** 编程模型是在 Google 开发的，用于在大型网络计算机集群上处理它们的数据。MapReduce 将并行编程定义为一个两步过程：'
- en: A **map** step in which a problem is divided into smaller tasks that are distributed
    across the computers in the cluster
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**map** 步骤将问题分解为较小的任务，分布在集群中的计算机上'
- en: A **reduce** step in which the results of the small chunks of work are collected
    and synthesized into a final solution to the original problem
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**reduce** 步骤中，将小块工作的结果收集并合成为解决原始问题的最终解决方案'
- en: A popular open source alternative to the proprietary MapReduce framework is
    **Apache Hadoop**. The Hadoop software comprises of the MapReduce concept, plus
    a distributed filesystem capable of storing large amounts of data across a cluster
    of computers.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流行的开源替代专有 MapReduce 框架的选项是 **Apache Hadoop**。Hadoop 软件包括 MapReduce 概念，以及一个分布式文件系统，能够在计算机集群中存储大量数据。
- en: Note
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Packt Publishing has published a large number of books on Hadoop. To search
    current offerings, visit [https://www.packtpub.com/all/?search=hadoop](https://www.packtpub.com/all/?search=hadoop).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: Packt Publishing 出版了大量关于 Hadoop 的书籍。要搜索当前的产品，请访问 [https://www.packtpub.com/all/?search=hadoop](https://www.packtpub.com/all/?search=hadoop)。
- en: Several R projects that provide an R interface to Hadoop are in development.
    The RHadoop project by Revolution Analytics provides an R interface to Hadoop.
    The project provides a package, `rmr`, intended to be an easy way for R developers
    to write MapReduce programs. Another companion package, `plyrmr`, provides functionality
    similar to the `dplyr` package to process large datasets. Additional RHadoop packages
    provide R functions to access Hadoop's distributed data stores.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 正在开发几个将 R 接口提供给 Hadoop 的 R 项目。Revolution Analytics 的 RHadoop 项目提供了一个 R 接口给 Hadoop。该项目提供了一个名为
    `rmr` 的软件包，旨在为 R 开发人员编写 MapReduce 程序提供一种简单的方式。另一个伴侣软件包 `plyrmr` 提供了类似于 `dplyr`
    软件包的功能，用于处理大型数据集。其他 RHadoop 软件包提供了访问 Hadoop 分布式数据存储的 R 函数。
- en: Note
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on the RHadoop project, see [https://github.com/RevolutionAnalytics/RHadoop/wiki](https://github.com/RevolutionAnalytics/RHadoop/wiki).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 RHadoop 项目的更多信息，请参见 [https://github.com/RevolutionAnalytics/RHadoop/wiki](https://github.com/RevolutionAnalytics/RHadoop/wiki)。
- en: Another similar project is RHIPE by Saptarshi Guha, which attempts to bring
    Hadoop's divide and recombine philosophy into R by managing the communication
    between R and Hadoop.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个类似的项目是由 Saptarshi Guha 开发的 RHIPE，它试图通过管理 R 和 Hadoop 之间的通信，将 Hadoop 的分割和重新组合哲学带入
    R 中。
- en: Note
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `RHIPE` package is not yet available at CRAN, but it can be built from the
    source available on the Web at [http://www.datadr.org](http://www.datadr.org).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`RHIPE` 软件包目前尚未在 CRAN 上提供，但可以从 [http://www.datadr.org](http://www.datadr.org)
    上的源代码构建。'
- en: GPU computing
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU 计算
- en: An alternative to parallel processing uses a computer's **Graphics Processing
    Unit** (**GPU**) to increase the speed of mathematical calculations. A GPU is
    a specialized processor that is optimized to rapidly display images on a computer
    screen. Because a computer often needs to display complex 3D graphics (particularly
    for video games), many GPUs use hardware designed for parallel processing and
    extremely efficient matrix and vector calculations. A side benefit is that they
    can be used to efficiently solve certain types of mathematical problems. Where
    a computer processor may have 16 cores, a GPU may have thousands.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 一种并行处理的替代方法是使用计算机的**图形处理单元**（**GPU**）来加速数学计算。GPU是一种专用处理器，经过优化以快速在计算机屏幕上显示图像。由于计算机通常需要显示复杂的3D图形（特别是用于视频游戏），许多GPU使用为并行处理和极为高效的矩阵与向量计算而设计的硬件。一个额外的好处是，它们可以高效地解决某些类型的数学问题。计算机处理器可能有16个核心，而GPU可能有成千上万个。
- en: '![GPU computing](img/B03905_12_06.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![GPU计算](img/B03905_12_06.jpg)'
- en: The downside of GPU computing is that it requires specific hardware that is
    not included in many computers. In most cases, a GPU from the manufacturer Nvidia
    is required, as they provide a proprietary framework called **Complete Unified
    Device Architecture** (**CUDA**) that makes the GPU programmable using common
    languages such as C++.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: GPU计算的缺点是，它需要特定的硬件，而许多计算机并不包含此类硬件。在大多数情况下，需要使用Nvidia的GPU，因为Nvidia提供了一种名为**完全统一设备架构**（**CUDA**）的专有框架，使得GPU可以使用C++等常见语言进行编程。
- en: Note
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on Nvidia's role in GPU computing, go to [http://www.nvidia.com/object/what-is-gpu-computing.html](http://www.nvidia.com/object/what-is-gpu-computing.html).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多Nvidia在GPU计算中的角色，请访问[http://www.nvidia.com/object/what-is-gpu-computing.html](http://www.nvidia.com/object/what-is-gpu-computing.html)。
- en: The `gputools` package by Josh Buckner, Mark Seligman, and Justin Wilson implements
    several R functions, such as matrix operations, clustering, and regression modeling
    using the Nvidia CUDA toolkit. The package requires a CUDA 1.3 or higher GPU and
    the installation of the Nvidia CUDA toolkit.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`gputools`包由Josh Buckner、Mark Seligman和Justin Wilson开发，包含多个R函数，例如使用Nvidia CUDA工具包进行矩阵运算、聚类和回归建模。该包需要CUDA
    1.3或更高版本的GPU，并且需要安装Nvidia CUDA工具包。'
- en: Deploying optimized learning algorithms
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署优化的学习算法
- en: Some of the machine learning algorithms covered in this book are able to work
    on extremely large datasets with relatively minor modifications. For instance,
    it would be fairly straightforward to implement Naive Bayes or the Apriori algorithm
    using one of the data structures for big datasets described in the previous sections.
    Some types of learners, such as ensembles, lend themselves well to parallelization,
    because the work of each model can be distributed across processors or computers
    in a cluster. On the other hand, some require larger changes to the data or algorithm,
    or need to be rethought altogether, before they can be used with massive datasets.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中讨论的部分机器学习算法能够通过相对较小的修改，在极大的数据集上工作。例如，使用前面章节中描述的大数据集数据结构来实现朴素贝叶斯或Apriori算法将是相当直接的。一些类型的学习器，如集成方法，非常适合并行化，因为每个模型的工作可以分配到集群中的处理器或计算机上。另一方面，有些算法需要对数据或算法进行较大的改动，或者完全重新设计，才能在庞大的数据集上使用。
- en: The following sections examine packages that provide optimized versions of the
    learning algorithms we've worked with so far.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将探讨提供优化版本的学习算法的包，这些算法是我们迄今为止使用过的。
- en: Building bigger regression models with biglm
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用biglm构建更大的回归模型
- en: The `biglm` package by Thomas Lumley provides functions to train regression
    models on datasets that may be too large to fit into memory. It works by using
    an iterative process in which the model is updated little by little using small
    chunks of data. In spite of it being a different approach, the results will be
    nearly identical to what would be obtained by running the conventional `lm()`
    function on the entire dataset.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: Thomas Lumley开发的`biglm`包提供了在可能无法完全载入内存的大数据集上训练回归模型的功能。它通过使用迭代过程，将小块数据逐步更新模型，尽管这是不同的方法，但结果几乎与在整个数据集上运行传统的`lm()`函数所得结果相同。
- en: For convenience while working with the largest datasets, the `biglm()` function
    allows the use of a SQL database in place of a data frame. The model can also
    be trained with chunks obtained from data objects created by the `ff` package
    described previously.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便处理最大的数据库，`biglm()`函数允许使用SQL数据库代替数据框。该模型也可以使用通过之前提到的`ff`包创建的数据对象的块来进行训练。
- en: Growing bigger and faster random forests with bigrf
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用`bigrf`扩展随机森林的规模和速度
- en: The `bigrf` package by Aloysius Lim implements the training of random forests
    for classification and regression on datasets that are too large to fit into memory.
    It uses the `bigmemory` objects as described earlier in this chapter. For speedier
    forest growth, the package can be used with the `foreach` and `doParallel` packages
    described previously to grow trees in parallel.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: Aloysius Lim的`bigrf`包实现了在无法完全加载到内存中的数据集上进行分类和回归的随机森林训练。它使用前面章节中描述的`bigmemory`对象。为了更快地增长森林，可以将该包与前面提到的`foreach`和`doParallel`包一起使用，以并行方式生长树木。
- en: Note
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more information, including examples and Windows installation instructions,
    see the package's wiki, which is hosted on GitHub at [https://github.com/aloysius-lim/bigrf](https://github.com/aloysius-lim/bigrf).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，包括示例和Windows安装说明，请访问此软件包的wiki，地址为[https://github.com/aloysius-lim/bigrf](https://github.com/aloysius-lim/bigrf)。
- en: Training and evaluating models in parallel with caret
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用`caret`并行训练和评估模型
- en: The `caret` package by Max Kuhn (covered extensively in [Chapter 10](ch10.html
    "Chapter 10. Evaluating Model Performance"), *Evaluating Model Performance* and
    [Chapter 11](ch11.html "Chapter 11. Improving Model Performance"), *Improving
    Model Performance*) will transparently utilize a parallel backend if one has been
    registered with R using the `foreach` package described previously.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: Max Kuhn的`caret`包（在[第10章](ch10.html "第10章. 评估模型性能")，*评估模型性能* 和 [第11章](ch11.html
    "第11章. 改进模型性能")，*改进模型性能* 中有详细介绍）将自动使用并行后端，如果通过前面提到的`foreach`包已在R中注册。
- en: 'Let''s take a look at a simple example in which we attempt to train a random
    forest model on the credit dataset. Without parallelization, the model takes about
    109 seconds to be trained:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的例子，在这个例子中，我们尝试在信用数据集上训练一个随机森林模型。如果不使用并行化，模型训练大约需要109秒：
- en: '[PRE55]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'On the other hand, if we use the `doParallel` package to register the four
    cores to be used in parallel, the model takes under 32 seconds to build—less than
    a third of the time—and we didn''t need to change even a single line of the `caret`
    code:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们使用`doParallel`包注册四个核心进行并行，模型的训练时间不到32秒——不到原来的三分之一——而且我们甚至没有需要修改`caret`代码中的一行：
- en: '[PRE56]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Many of the tasks involved in training and evaluating models, such as creating
    random samples and repeatedly testing predictions for 10-fold cross-validation
    are embarrassingly parallel and ripe for performance improvements. With this in
    mind, it is wise to always register multiple cores before beginning a `caret`
    project.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 许多涉及训练和评估模型的任务，如创建随机样本和反复测试10折交叉验证的预测，都是典型的并行任务，十分适合性能提升。因此，在开始`caret`项目之前，建议始终注册多个核心。
- en: Note
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Configuration instructions and a case study of the performance improvements
    needed to enable parallel processing in `caret` are available on the project's
    website at [http://topepo.github.io/caret/parallel.html](http://topepo.github.io/caret/parallel.html).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 配置说明和使`caret`启用并行处理所需的性能改进案例研究，可以在项目网站[http://topepo.github.io/caret/parallel.html](http://topepo.github.io/caret/parallel.html)上找到。
- en: Summary
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: It is certainly an exciting time to be studying machine learning. Ongoing work
    on the relatively uncharted frontiers of parallel and distributed computing offers
    great potential for tapping the knowledge found in the deluge of big data. The
    burgeoning data science community is facilitated by the free and open source R
    programming language, which provides a very low barrier for entry—you simply need
    to be willing to learn.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是学习机器学习的好时光。并行和分布式计算领域的不断发展为挖掘大数据中的知识提供了巨大的潜力。蓬勃发展的数据科学社区得益于免费的开源R编程语言，这为入门提供了非常低的门槛——你只需要愿意学习。
- en: The topics you have learned, both in this chapter and in the previous chapters,
    provide the foundation to understand more advanced machine learning methods. It
    is now your responsibility to keep learning and adding tools to your arsenal.
    Along the way, be sure to keep in mind the *No Free Lunch* theorem—no learning
    algorithm can rule them all, and they all have varying strengths and weaknesses.
    For this reason, there will always be a human element to machine learning, adding
    subject-specific knowledge and the ability to match the appropriate algorithm
    to the task at hand.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 你在本章和之前章节中学到的内容为理解更高级的机器学习方法打下了基础。现在，继续学习并为自己的工具库添加新工具是你的责任。在这个过程中，一定要牢记*无免费午餐*定理——没有任何一种学习算法能做到全能，它们各自有不同的优缺点。正因为如此，机器学习中将始终存在一个人类因素，提供领域特定的知识，并能够将合适的算法与眼前的任务匹配。
- en: In the coming years, it will be interesting to see how the human side changes
    as the line between machine learning and human learning is blurred. Services such
    as Amazon's Mechanical Turk provide crowd-sourced intelligence, offering a cluster
    of human minds ready to perform simple tasks at a moment's notice. Perhaps one
    day, just as we have used computers to perform tasks that human beings cannot
    do easily, computers will employ human beings to do the reverse. What interesting
    food for thought!
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的几年里，随着机器学习与人类学习之间的界限越来越模糊，看看人类一方如何变化将会是非常有趣的。像亚马逊的机械土耳其人（Mechanical Turk）这样的服务提供了众包智能，汇集了一群随时准备执行简单任务的人类大脑。也许有一天，正如我们曾利用计算机执行人类无法轻易完成的任务一样，计算机会雇佣人类来做相反的事情。真是令人深思的有趣话题！
