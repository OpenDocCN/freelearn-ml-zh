- en: Chapter 9. Finding Groups of Data – Clustering with k-means
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 数据分组—使用k-means进行聚类
- en: Have you ever spent time watching a large crowd? If so, you are likely to have
    seen some recurring personalities. Perhaps a certain type of person, identified
    by a freshly pressed suit and a briefcase, comes to typify the "fat cat" business
    executive. A twenty-something wearing skinny jeans, a flannel shirt, and sunglasses
    might be dubbed a "hipster," while a woman unloading children from a minivan may
    be labeled a "soccer mom."
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 您是否曾花时间观察过一大群人？如果有，您可能已经见过一些反复出现的个性。也许某种类型的人，穿着刚熨好的西装，手拿公文包，成为了典型的“肥猫”商界高管。一个穿着紧身牛仔裤、法兰绒衬衫和太阳镜的二十多岁年轻人可能被称为“嬉皮士”，而一位从小面包车里抱出孩子的女性则可能被标记为“足球妈妈”。
- en: Of course, these types of stereotypes are dangerous to apply to individuals,
    as no two people are exactly alike. Yet understood as a way to describe a collective,
    the labels capture some underlying aspect of similarity among the individuals
    within the group.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些刻板印象在应用到个体时是危险的，因为没有两个完全相同的人。然而，作为描述一个集体的方式，这些标签捕捉到了群体内个体之间某些潜在的相似性。
- en: 'As you will soon learn, the act of clustering, or spotting patterns in data,
    is not much different from spotting patterns in groups of people. In this chapter,
    you will learn:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您很快会学到的，聚类或在数据中识别模式的过程，与在人群中识别模式并没有太大不同。在这一章中，您将学到：
- en: The ways clustering tasks differ from the classification tasks we examined previously
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类任务与我们之前研究的分类任务有何不同
- en: How clustering defines a group, and how such groups are identified by k-means,
    a classic and easy-to-understand clustering algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类如何定义一个组，以及如何通过k-means这一经典且易于理解的聚类算法识别这些组
- en: The steps needed to apply clustering to a real-world task of identifying marketing
    segments among teenage social media users
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将聚类应用于现实世界任务的步骤，比如在青少年社交媒体用户中识别营销细分
- en: Before jumping into action, we'll begin by taking an in-depth look at exactly
    what clustering entails.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始实际操作之前，我们将深入探讨聚类的具体内容。
- en: Understanding clustering
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解聚类
- en: Clustering is an unsupervised machine learning task that automatically divides
    the data into **clusters,** or groups of similar items. It does this without having
    been told how the groups should look ahead of time. As we may not even know what
    we're looking for, clustering is used for knowledge discovery rather than prediction.
    It provides an insight into the natural groupings found within data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种无监督的机器学习任务，它会自动将数据分成**聚类**，即一组组相似的项目。它在不知道如何预先划分组的情况下进行。这就像我们可能甚至不知道自己在寻找什么，聚类用于知识发现而非预测。它为我们提供了数据中自然分组的洞察。
- en: Without advance knowledge of what comprises a cluster, how can a computer possibly
    know where one group ends and another begins? The answer is simple. Clustering
    is guided by the principle that items inside a cluster should be very similar
    to each other, but very different from those outside. The definition of similarity
    might vary across applications, but the basic idea is always the same—group the
    data so that the related elements are placed together.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有预先了解一个聚类由什么组成的情况下，计算机如何可能知道一个组何时结束，另一个组何时开始呢？答案很简单。聚类是由一个原则指导的：聚类内部的项应该彼此非常相似，而与聚类外部的项有很大不同。相似性的定义可能会因应用场景的不同而有所变化，但基本思想始终相同——将数据分组，使相关元素聚集在一起。
- en: 'The resulting clusters can then be used for action. For instance, you might
    find clustering methods employed in the following applications:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的聚类可以用于实际操作。例如，您可能会发现聚类方法应用于以下场景：
- en: Segmenting customers into groups with similar demographics or buying patterns
    for targeted marketing campaigns
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将客户按相似的人口统计信息或购买模式进行分组，以进行定向营销活动
- en: Detecting anomalous behavior, such as unauthorized network intrusions, by identifying
    patterns of use falling outside the known clusters
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过识别使用模式，发现超出已知聚类范围的异常行为，例如未经授权的网络入侵
- en: Simplifying extremely large datasets by grouping features with similar values
    into a smaller number of homogeneous categories
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将具有相似值的特征分组为更少的同质类别，从而简化极其庞大的数据集
- en: Overall, clustering is useful whenever diverse and varied data can be exemplified
    by a much smaller number of groups. It results in meaningful and actionable data
    structures that reduce complexity and provide insight into patterns of relationships.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，聚类在数据多样且可以通过更少的组来概括时非常有用。它能产生有意义且可操作的数据结构，减少复杂性并为关系模式提供洞察。
- en: Clustering as a machine learning task
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类作为机器学习任务
- en: Clustering is somewhat different from the classification, numeric prediction,
    and pattern detection tasks we examined so far. In each of these cases, the result
    is a model that relates features to an outcome or features to other features;
    conceptually, the model describes the existing patterns within data. In contrast,
    clustering creates new data. Unlabeled examples are given a cluster label that
    has been inferred entirely from the relationships within the data. For this reason,
    you will, sometimes, see the clustering task referred to as **unsupervised classification**
    because, in a sense, it classifies unlabeled examples.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类与我们之前探讨的分类、数值预测和模式检测任务有所不同。在这些任务中，结果是一个模型，将特征与结果或特征之间的关系进行关联；从概念上讲，模型描述了数据中的现有模式。相比之下，聚类会创造新的数据。未标记的示例会被赋予一个聚类标签，该标签完全是根据数据内部的关系推断出来的。因此，有时你会看到聚类任务被称为**无监督分类**，因为从某种意义上来说，它是对未标记的示例进行分类。
- en: The catch is that the class labels obtained from an unsupervised classifier
    are without intrinsic meaning. Clustering will tell you which groups of examples
    are closely related—for instance, it might return the groups A, B, and C—but it's
    up to you to apply an actionable and meaningful label. To see how this impacts
    the clustering task, let's consider a hypothetical example.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于，来自无监督分类器的类别标签没有内在意义。聚类将告诉你哪些示例组之间有紧密的关系——例如，它可能会返回A、B和C组——但你需要为这些组应用一个可操作且有意义的标签。为了了解这对聚类任务的影响，我们来看一个假设的例子。
- en: 'Suppose you were organizing a conference on the topic of data science. To facilitate
    professional networking and collaboration, you planned to seat people in groups
    according to one of three research specialties: computer and/or database science,
    math and statistics, and machine learning. Unfortunately, after sending out the
    conference invitations, you realize that you had forgotten to include a survey
    asking which discipline the attendee would prefer to be seated with.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在组织一个关于数据科学的会议。为了促进专业的社交和合作，你计划根据三种研究专长之一将与会者分组：计算机和/或数据库科学、数学和统计学、以及机器学习。不幸的是，在发送会议邀请后，你意识到忘记了包含一项调查，询问与会者希望与哪个学科的人员坐在一起。
- en: 'In a stroke of brilliance, you realize that you might be able to infer each
    scholar''s research specialty by examining his or her publication history. To
    this end, you begin collecting data on the number of articles each attendee published
    in computer science-related journals and the number of articles published in math
    or statistics-related journals. Using the data collected for several scholars,
    you create a scatterplot:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一时灵感迸发，你意识到可以通过检查每个学者的出版历史来推断他们的研究专长。为此，你开始收集每个与会者在计算机科学相关期刊上发表的文章数量，以及在数学或统计学相关期刊上发表的文章数量。通过收集几位学者的数据，你绘制了一个散点图：
- en: '![Clustering as a machine learning task](img/B03905_09_01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![聚类作为机器学习任务](img/B03905_09_01.jpg)'
- en: As expected, there seems to be a pattern. We might guess that the upper-left
    corner, which represents people with many computer science publications but few
    articles on math, could be a cluster of computer scientists. Following this logic,
    the lower-right corner might be a group of mathematicians. Similarly, the upper-right
    corner, those with both math and computer science experience, may be machine learning
    experts.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，似乎确实存在一个模式。我们可能会猜测，位于左上角的群体，代表那些在计算机科学方面有许多出版物，但在数学方面文章较少的人，可能是计算机科学家的聚类。按照这一逻辑，右下角可能是数学家的群体。同样，右上角那些既有数学又有计算机科学经验的人，可能是机器学习专家。
- en: 'Our groupings were formed visually; we simply identified clusters as closely
    grouped data points. Yet in spite of the seemingly obvious groupings, we unfortunately
    have no way to know whether they are truly homogeneous without personally asking
    each scholar about his/her academic specialty. The labels we applied required
    us to make qualitative, presumptive judgments about the types of people that would
    fall into the group. For this reason, you might imagine the cluster labels in
    uncertain terms, as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分组是通过视觉方式形成的；我们只是简单地将数据点作为紧密聚集的群体进行识别。然而，尽管这些分组看起来显而易见，但不幸的是，我们没有办法知道这些分组是否真正同质化，因为我们无法逐个询问每位学者的学术专长。我们所应用的标签要求我们对可能属于该组的人进行定性、假设性的判断。因此，你可以把群体标签理解为不确定的术语，如下所示：
- en: '![Clustering as a machine learning task](img/B03905_09_02.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![作为机器学习任务的聚类](img/B03905_09_02.jpg)'
- en: Rather than defining the group boundaries subjectively, it would be nice to
    use machine learning to define them objectively. Given the axis-parallel splits
    in the preceding diagram, our problem seems like an obvious application for the
    decision trees described in [Chapter 5](ch05.html "Chapter 5. Divide and Conquer
    – Classification Using Decision Trees and Rules"), *Divide and Conquer – Classification
    Using Decision Trees and Rules*. This might provide us with a rule in the form
    "if a scholar has few math publications, then he/she is a computer science expert."
    Unfortunately, there's a problem with this plan. As we do not have data on the
    true class value for each point, a supervised learning algorithm would have no
    ability to learn such a pattern, as it would have no way of knowing what splits
    would result in homogenous groups.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与其主观地定义群体边界，不如使用机器学习来客观地定义它们。考虑到前面图中的轴向分割，我们的问题似乎是[第5章](ch05.html "Chapter 5.
    Divide and Conquer – Classification Using Decision Trees and Rules")中所描述的决策树的明显应用，*分治法
    – 使用决策树和规则进行分类*。这可能会为我们提供一个规则，形式为“如果学者的数学出版物较少，那么他/她就是计算机科学专家。”不幸的是，这个计划存在问题。由于我们没有每个点的真实类别数据，监督学习算法无法学习到这样的模式，因为它无法知道哪些分割能形成同质化的群体。
- en: On the other hand, clustering algorithms use a process very similar to what
    we did by visually inspecting the scatterplot. Using a measure of how closely
    the examples are related, homogeneous groups can be identified. In the next section,
    we'll start looking at how clustering algorithms are implemented.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，聚类算法使用的过程与我们通过视觉检查散点图所做的非常相似。通过衡量样本之间的关系程度，可以识别出同质的群体。在接下来的部分中，我们将开始探讨聚类算法是如何实现的。
- en: Tip
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: This example highlights an interesting application of clustering. If you begin
    with unlabeled data, you can use clustering to create class labels. From there,
    you could apply a supervised learner such as decision trees to find the most important
    predictors of these classes. This is called **semi-supervised learning**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子突出了聚类的一个有趣应用。如果你从未标记的数据开始，你可以使用聚类来创建类别标签。从那里，你可以应用监督学习算法，如决策树，来找出这些类别最重要的预测因素。这被称为**半监督学习**。
- en: The k-means clustering algorithm
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-means 聚类算法
- en: The **k-means** **algorithm** is perhaps the most commonly used clustering method.
    Having been studied for several decades, it serves as the foundation for many
    more sophisticated clustering techniques. If you understand the simple principles
    it uses, you will have the knowledge needed to understand nearly any clustering
    algorithm in use today. Many such methods are listed on the following site, the
    **CRAN Task View** for clustering at [http://cran.r-project.org/web/views/Cluster.html](http://cran.r-project.org/web/views/Cluster.html).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**k-means** **算法**可能是最常用的聚类方法。经过数十年的研究，它为许多更复杂的聚类技术奠定了基础。如果你理解了它使用的简单原理，你就具备了理解今天几乎所有聚类算法所需的知识。许多此类方法列在以下网站上，即**CRAN聚类任务视图**：[http://cran.r-project.org/web/views/Cluster.html](http://cran.r-project.org/web/views/Cluster.html)。'
- en: Note
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'As k-means has evolved over time, there are many implementations of the algorithm.
    One popular approach is described in : Hartigan JA, Wong MA. A k-means clustering
    algorithm. Applied *Statistics*. 1979; 28:100-108.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 随着k-means的不断发展，该算法有许多不同的实现。一个常见的方法描述在：Hartigan JA, Wong MA. A k-means clustering
    algorithm. Applied *Statistics*. 1979; 28:100-108。
- en: 'Even though clustering methods have advanced since the inception of k-means,
    this is not to imply that k-means is obsolete. In fact, the method may be more
    popular now than ever. The following table lists some reasons why k-means is still
    used widely:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管聚类方法自k-means诞生以来已有了进展，但这并不意味着k-means已经过时。事实上，这种方法现在可能比以往任何时候都更受欢迎。以下表格列出了一些k-means仍然广泛使用的原因：
- en: '| Strengths | Weaknesses |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 优势 | 弱点 |'
- en: '| --- | --- |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Uses simple principles that can be explained in non-statistical terms
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用简单的原则，可以用非统计学术语进行解释
- en: Highly flexible, and can be adapted with simple adjustments to address nearly
    all of its shortcomings
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度灵活，可以通过简单的调整适应，解决几乎所有的缺点
- en: Performs well enough under many real-world use cases
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在许多实际应用场景中表现良好
- en: '|'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Not as sophisticated as more modern clustering algorithms
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比不上更现代的聚类算法复杂
- en: Because it uses an element of random chance, it is not guaranteed to find the
    optimal set of clusters
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于它使用了随机因素，因此无法保证找到最优的聚类集
- en: Requires a reasonable guess as to how many clusters naturally exist in the data
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要合理猜测数据中自然存在多少个聚类
- en: Not ideal for non-spherical clusters or clusters of widely varying density
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于非球形聚类或密度差异较大的聚类不理想
- en: '|'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: If the name k-means sounds familiar to you, you may be recalling the k-NN algorithm
    discussed in [Chapter 3](ch03.html "Chapter 3. Lazy Learning – Classification
    Using Nearest Neighbors"), *Lazy Learning – Classification Using Nearest Neighbors*.
    As you will soon see, k-means shares more in common with the k-nearest neighbors
    than just the letter k.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果“k-means”这个名字让你觉得熟悉，你可能是在回忆[第3章](ch03.html "第3章 懒学习 – 使用最近邻分类")中讨论的k-NN算法，*懒学习
    – 使用最近邻分类*。正如你将很快看到的，k-means与k最近邻的相似之处远不止字母k。
- en: The k-means algorithm assigns each of the *n* examples to one of the *k* clusters,
    where *k* is a number that has been determined ahead of time. The goal is to minimize
    the differences within each cluster and maximize the differences between the clusters.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: k-means算法将每个*n*个样本分配到*k*个聚类中的一个，*k*是一个事先确定的数字。目标是最小化每个聚类内的差异，并最大化聚类之间的差异。
- en: Unless *k* and *n* are extremely small, it is not feasible to compute the optimal
    clusters across all the possible combinations of examples. Instead, the algorithm
    uses a heuristic process that finds **locally optimal** solutions. Put simply,
    this means that it starts with an initial guess for the cluster assignments, and
    then modifies the assignments slightly to see whether the changes improve the
    homogeneity within the clusters.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 除非*k*和*n*非常小，否则计算所有可能组合的最优聚类是不可行的。相反，算法使用启发式过程来寻找**局部最优**解。简单来说，这意味着它从初始聚类分配开始，然后稍微修改分配，看看这些变化是否能提高聚类内部的一致性。
- en: We will cover the process in depth shortly, but the algorithm essentially involves
    two phases. First, it assigns examples to an initial set of *k* clusters. Then,
    it updates the assignments by adjusting the cluster boundaries according to the
    examples that currently fall into the cluster. The process of updating and assigning
    occurs several times until changes no longer improve the cluster fit. At this
    point, the process stops and the clusters are finalized.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将很快深入讨论这个过程，但该算法基本上分为两个阶段。首先，它将样本分配到初始的*k*个聚类中。然后，它通过根据当前落入聚类中的样本调整聚类边界来更新分配。更新和分配的过程会重复几次，直到变化不再改善聚类拟合为止。此时，过程停止，聚类最终确定。
- en: Tip
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Due to the heuristic nature of k-means, you may end up with somewhat different
    final results by making only slight changes to the starting conditions. If the
    results vary dramatically, this could indicate a problem. For instance, the data
    may not have natural groupings or the value of *k* has been poorly chosen. With
    this in mind, it's a good idea to try a cluster analysis more than once to test
    the robustness of your findings.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于k-means的启发式特性，仅通过轻微改变初始条件，你可能会得到略有不同的最终结果。如果结果差异很大，这可能意味着存在问题。例如，数据可能没有自然分组，或者*k*的值选择不当。考虑到这一点，最好多次尝试聚类分析，以测试结果的稳健性。
- en: To see how the process of assigning and updating works in practice, let's revisit
    the case of the hypothetical data science conference. Though this is a simple
    example, it will illustrate the basics of how k-means operates under the hood.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解分配和更新的过程如何在实践中工作，让我们重新审视一下假设的数据科学会议的案例。虽然这是一个简单的例子，但它将展示 k-means 在背后如何运作的基础。
- en: Using distance to assign and update clusters
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用距离分配和更新聚类
- en: As with k-NN, k-means treats feature values as coordinates in a multidimensional
    feature space. For the conference data, there are only two features, so we can
    represent the feature space as a two-dimensional scatterplot as depicted previously.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 与 k-NN 一样，k-means 将特征值视为多维特征空间中的坐标。对于这次会议数据，只有两个特征，因此我们可以将特征空间表示为如前所示的二维散点图。
- en: 'The k-means algorithm begins by choosing *k* points in the feature space to
    serve as the cluster centers. These centers are the catalyst that spurs the remaining
    examples to fall into place. Often, the points are chosen by selecting *k* random
    examples from the training dataset. As we hope to identify three clusters, according
    to this method, *k = 3* points will be selected at random. These points are indicated
    by the star, triangle, and diamond in the following diagram:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 算法首先通过在特征空间中选择 *k* 个点作为聚类中心。这些中心是推动剩余样本归类的催化剂。通常，这些点通过从训练数据集中随机选择 *k*
    个样本来确定。假设我们希望识别三个聚类，按照这种方法，*k = 3* 个点将被随机选择。这些点在下图中分别用星号、三角形和菱形表示：
- en: '![Using distance to assign and update clusters](img/B03905_09_03.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![使用距离分配和更新聚类](img/B03905_09_03.jpg)'
- en: It's worth noting that although the three cluster centers in the preceding diagram
    happen to be widely spaced apart, this is not always necessarily the case. Since
    they are selected at random, the three centers could have just as easily been
    three adjacent points. As the k-means algorithm is highly sensitive to the starting
    position of the cluster centers, this means that random chance may have a substantial
    impact on the final set of clusters.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管前图中的三个聚类中心恰好相距较远，但这并不一定总是如此。由于它们是随机选择的，这三个中心也有可能是相邻的三个点。由于 k-means
    算法对聚类中心的初始位置高度敏感，这意味着随机因素可能对最终的聚类结果产生重大影响。
- en: To address this problem, k-means can be modified to use different methods for
    choosing the initial centers. For example, one variant chooses random values occurring
    anywhere in the feature space (rather than only selecting among the values observed
    in the data). Another option is to skip this step altogether; by randomly assigning
    each example to a cluster, the algorithm can jump ahead immediately to the update
    phase. Each of these approaches adds a particular bias to the final set of clusters,
    which you may be able to use to improve your results.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，k-means 可以通过不同的方法来选择初始中心。例如，一种变体会在特征空间中的任意位置选择随机值（而不仅仅是从数据中选择已观察到的值）。另一种选择是完全跳过这一步；通过将每个样本随机分配到一个聚类中，算法可以立即跳到更新阶段。每种方法都会对最终的聚类集添加特定的偏差，你可能可以利用这些偏差来改进你的结果。
- en: Note
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'In 2007, an algorithm called **k-means++** was introduced, which proposes an
    alternative method for selecting the initial cluster centers. It purports to be
    an efficient way to get much closer to the optimal clustering solution while reducing
    the impact of random chance. For more information, refer to *Arthur D, Vassilvitskii
    S*. k-means++: The advantages of careful seeding. *Proceedings of the eighteenth
    annual ACM-SIAM symposium on discrete algorithms*. 2007:1027–1035.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '2007年，提出了一种名为 **k-means++** 的算法，它提供了一种选择初始聚类中心的替代方法。该方法声称是一种高效的方式，能够在减少随机因素影响的同时，接近最优的聚类结果。欲了解更多信息，请参阅
    *Arthur D, Vassilvitskii S*。k-means++: 精心初始化的优势。*第十八届ACM-SIAM离散算法年会论文集*，2007：1027–1035。'
- en: After choosing the initial cluster centers, the other examples are assigned
    to the cluster center that is nearest according to the distance function. You
    will remember that we studied distance functions while learning about k-Nearest
    Neighbors. Traditionally, k-means uses Euclidean distance, but Manhattan distance
    or Minkowski distance are also sometimes used.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 选择初始聚类中心后，其他样本会根据距离函数被分配到最近的聚类中心。你应该还记得我们在学习 k-最近邻时研究了距离函数。传统上，k-means 使用欧几里得距离，但也有时使用曼哈顿距离或闵可夫斯基距离。
- en: 'Recall that if *n* indicates the number of features, the formula for Euclidean
    distance between example *x* and example *y* is:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，如果 *n* 表示特征的数量，则示例 *x* 和示例 *y* 之间的欧几里得距离公式为：
- en: '![Using distance to assign and update clusters](img/B03905_09_04.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![使用距离分配和更新聚类](img/B03905_09_04.jpg)'
- en: 'For instance, if we are comparing a guest with five computer science publications
    and one math publication to a guest with zero computer science papers and two
    math papers, we could compute this in R as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们要比较一位有五篇计算机科学论文和一篇数学论文的访客与一位没有计算机科学论文但有两篇数学论文的访客，我们可以在 R 中按如下方式计算：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using this distance function, we find the distance between each example and
    each cluster center. The example is then assigned to the nearest cluster center.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该距离函数，我们计算每个示例与每个聚类中心之间的距离。然后，示例被分配到最近的聚类中心。
- en: Tip
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Keep in mind that as we are using distance calculations, all the features need
    to be numeric, and the values should be normalized to a standard range ahead of
    time. The methods discussed in [Chapter 3](ch03.html "Chapter 3. Lazy Learning
    – Classification Using Nearest Neighbors"), *Lazy Learning – Classification Using
    Nearest Neighbors*, will prove helpful for this task.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，由于我们正在使用距离计算，所有特征需要是数值型的，且值应提前标准化到一个标准范围。第[3章](ch03.html "第3章：惰性学习——使用最近邻分类")中讨论的方法，*惰性学习——使用最近邻分类*，对这个任务会很有帮助。
- en: 'As shown in the following diagram, the three cluster centers partition the
    examples into three segments labeled **Cluster A**, **Cluster B**, and **Cluster
    C**. The dashed lines indicate the boundaries for the **Voronoi diagram** created
    by the cluster centers. The Voronoi diagram indicates the areas that are closer
    to one cluster center than any other; the vertex where all the three boundaries
    meet is the maximal distance from all three cluster centers. Using these boundaries,
    we can easily see the regions claimed by each of the initial k-means seeds:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，三个聚类中心将示例分为三个部分，分别标记为**聚类 A**、**聚类 B**和**聚类 C**。虚线表示由聚类中心创建的**Voronoi
    图**的边界。Voronoi 图显示了离某个聚类中心比其他任何中心都近的区域；三个边界相交的顶点是离所有三个聚类中心最远的点。利用这些边界，我们可以轻松看到每个初始
    k-means 种子所占据的区域：
- en: '![Using distance to assign and update clusters](img/B03905_09_05.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![使用距离分配和更新聚类](img/B03905_09_05.jpg)'
- en: 'Now that the initial assignment phase has been completed, the k-means algorithm
    proceeds to the update phase. The first step of updating the clusters involves
    shifting the initial centers to a new location, known as the **centroid**, which
    is calculated as the average position of the points currently assigned to that
    cluster. The following diagram illustrates how as the cluster centers shift to
    the new centroids, the boundaries in the Voronoi diagram also shift and a point
    that was once in **Cluster B** (indicated by an arrow) is added to **Cluster A**:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在初始分配阶段已经完成，k-means 算法进入更新阶段。更新聚类的第一步是将初始中心移动到新的位置，称为**质心**，其位置是当前分配给该聚类的点的平均位置。下图展示了当聚类中心移动到新的质心时，Voronoi
    图中的边界也会发生变化，而原本在**聚类 B**中的一个点（由箭头表示）被加入到了**聚类 A**中：
- en: '![Using distance to assign and update clusters](img/B03905_09_06.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![使用距离分配和更新聚类](img/B03905_09_06.jpg)'
- en: 'As a result of this reassignment, the k-means algorithm will continue through
    another update phase. After shifting the cluster centroids, updating the cluster
    boundaries, and reassigning points into new clusters (as indicated by arrows),
    the figure looks like this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此次重新分配，k-means 算法将继续进行另一个更新阶段。在移动聚类中心、更新聚类边界并重新分配点到新聚类之后（如箭头所示），图形如下所示：
- en: '![Using distance to assign and update clusters](img/B03905_09_07.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![使用距离分配和更新聚类](img/B03905_09_07.jpg)'
- en: 'Because two more points were reassigned, another update must occur, which moves
    the centroids and updates the cluster boundaries. However, because these changes
    result in no reassignments, the k-means algorithm stops. The cluster assignments
    are now final:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于又有两个点被重新分配，因此必须进行另一次更新，这将移动中心并更新聚类边界。然而，由于这些变化没有导致任何重新分配，k-means 算法停止。聚类分配现在是最终的：
- en: '![Using distance to assign and update clusters](img/B03905_09_08.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![使用距离分配和更新聚类](img/B03905_09_08.jpg)'
- en: The final clusters can be reported in one of the two ways. First, you might
    simply report the cluster assignments such as A, B, or C for each example. Alternatively,
    you could report the coordinates of the cluster centroids after the final update.
    Given either reporting method, you are able to define the cluster boundaries by
    calculating the centroids or assigning each example to its nearest cluster.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的聚类结果可以通过两种方式之一来报告。首先，你可以简单地报告每个样本的聚类分配，如 A、B 或 C。或者，你可以报告最终更新后聚类中心的坐标。无论哪种报告方式，你都可以通过计算中心点或将每个样本分配给其最近的聚类来定义聚类边界。
- en: Choosing the appropriate number of clusters
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择适当数量的聚类
- en: In the introduction to k-means, we learned that the algorithm is sensitive to
    the randomly-chosen cluster centers. Indeed, if we had selected a different combination
    of three starting points in the previous example, we may have found clusters that
    split the data differently from what we had expected. Similarly, k-means is sensitive
    to the number of clusters; the choice requires a delicate balance. Setting *k*
    to be very large will improve the homogeneity of the clusters, and at the same
    time, it risks overfitting the data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在 k-means 的介绍中，我们了解到该算法对随机选择的聚类中心非常敏感。实际上，如果我们在前一个例子中选择了不同的三个起始点，可能会得到与我们预期不同的数据分组。同样，k-means
    对聚类数量也非常敏感；这个选择需要一个微妙的平衡。将 *k* 设置得非常大将提高聚类的同质性，但同时也有过拟合数据的风险。
- en: Ideally, you will have *a priori* knowledge (a prior belief) about the true
    groupings and you can apply this information to choosing the number of clusters.
    For instance, if you were clustering movies, you might begin by setting *k* equal
    to the number of genres considered for the Academy Awards. In the data science
    conference seating problem that we worked through previously, *k* might reflect
    the number of academic fields of study that were invited.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，你应该具有 *先验* 知识（即先前的信念）关于真实的分组情况，并可以利用这些信息来选择聚类数量。例如，如果你在对电影进行聚类，你可能会将 *k*
    设置为奥斯卡奖提名的类型数量。在我们之前讨论的数据科学会议座位问题中，*k* 可能反映了受邀的学术领域数量。
- en: Sometimes the number of clusters is dictated by business requirements or the
    motivation for the analysis. For example, the number of tables in the meeting
    hall could dictate how many groups of people should be created from the data science
    attendee list. Extending this idea to another business case, if the marketing
    department only has resources to create three distinct advertising campaigns,
    it might make sense to set *k = 3* to assign all the potential customers to one
    of the three appeals.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，聚类的数量由业务需求或分析的动机决定。例如，会议厅中的桌子数量可能决定了应该从数据科学参与者名单中创建多少个小组。将这个思路扩展到另一个商业案例，如果市场部门只有限制资源来创建三种不同的广告活动，那么将
    *k = 3* 设置为将所有潜在客户分配给三个吸引点中的一个可能是合理的。
- en: Without any prior knowledge, one rule of thumb suggests setting *k* equal to
    the square root of *(n / 2)*, where *n* is the number of examples in the dataset.
    However, this rule of thumb is likely to result in an unwieldy number of clusters
    for large datasets. Luckily, there are other statistical methods that can assist
    in finding a suitable k-means cluster set.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有任何先验知识的情况下，有一个经验法则建议将 *k* 设置为 *(n / 2)* 的平方根，其中 *n* 是数据集中的样本数量。然而，对于大型数据集来说，这个经验法则可能会导致聚类数量过多而难以处理。幸运的是，还有其他统计方法可以帮助找到合适的
    k-means 聚类集。
- en: A technique known as the **elbow method** attempts to gauge how the homogeneity
    or heterogeneity within the clusters changes for various values of *k*. As illustrated
    in the following diagrams, the homogeneity within clusters is expected to increase
    as additional clusters are added; similarly, heterogeneity will also continue
    to decrease with more clusters. As you could continue to see improvements until
    each example is in its own cluster, the goal is not to maximize homogeneity or
    minimize heterogeneity, but rather to find *k* so that there are diminishing returns
    beyond that point. This value of *k* is known as the **elbow point** because it
    looks like an elbow.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一种名为 **肘部法则** 的技术试图衡量在不同的 *k* 值下，聚类内的同质性或异质性是如何变化的。正如下图所示，随着聚类数量的增加，聚类内部的同质性预期会增加；同样，异质性也会随着聚类数量的增加而继续减少。尽管你可以继续观察到每个样本被分配到其自己的聚类，目标不是最大化同质性或最小化异质性，而是找到一个
    *k*，在这个点之后，收益递减。这种 *k* 值被称为 **肘部点**，因为它看起来像一个肘部。
- en: '![Choosing the appropriate number of clusters](img/B03905_09_09.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![选择适当的聚类数](img/B03905_09_09.jpg)'
- en: There are numerous statistics to measure homogeneity and heterogeneity within
    the clusters that can be used with the elbow method (the following information
    box provides a citation for more detail). Still, in practice, it is not always
    feasible to iteratively test a large number of *k* values. This is in part because
    clustering large datasets can be fairly time consuming; clustering the data repeatedly
    is even worse. Regardless, applications requiring the exact optimal set of clusters
    are fairly rare. In most clustering applications, it suffices to choose a *k*
    value based on convenience rather than strict performance requirements.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多统计方法可以衡量聚类内部的同质性和异质性，这些方法可以与肘部法（以下信息框提供了详细的引用）一起使用。然而，在实际应用中，并不总是可行的反复测试大量*k*值。部分原因是，聚类大数据集本身就可能非常耗时；而反复进行数据聚类则更加浪费时间。无论如何，需要精确的最优聚类集的应用相对较少。在大多数聚类应用中，选择一个方便的*k*值而非严格的性能要求通常就足够了。
- en: Note
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For a very thorough review of the vast assortment of cluster performance measures,
    refer to: *Halkidi M, Batistakis Y, Vazirgiannis M*. On clustering validation
    techniques. *Journal of Intelligent Information Systems*. 2001; 17:107-145.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于关于聚类性能度量的大量综述，请参考：*Halkidi M, Batistakis Y, Vazirgiannis M*。关于聚类验证技术。*智能信息系统杂志*。2001；17:107-145。
- en: The process of setting *k* itself can sometimes lead to interesting insights.
    By observing how the characteristics of the clusters change as *k* is varied,
    one might infer where the data have naturally defined boundaries. Groups that
    are more tightly clustered will change a little, while less homogeneous groups
    will form and disband over time.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 设置*k*的过程本身有时会带来有趣的洞见。通过观察随着*k*变化，聚类的特征如何变化，可能会推测出数据自然的边界所在。聚集得更紧密的组变化较小，而异质性较大的组则会随着时间的推移不断形成和解散。
- en: In general, it may be wise to spend little time worrying about getting *k* exactly
    right. The next example will demonstrate how even a tiny bit of subject-matter
    knowledge borrowed from a Hollywood film can be used to set *k* such that actionable
    and interesting clusters are found. As clustering is unsupervised, the task is
    really about what you make of it; the value is in the insights you take away from
    the algorithm's findings.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，花费很少的时间去担心*k*是否完全准确是明智的。下一个例子将展示，即使是从一部好莱坞电影中借来的一点点专业知识，也能用于设定*k*，以便发现可操作且有趣的聚类。由于聚类是无监督的，任务的本质实际上是你如何理解它；真正的价值在于从算法发现中获得的洞察。
- en: Example – finding teen market segments using k-means clustering
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例——使用k-means聚类寻找青少年市场细分
- en: Interacting with friends on a **social networking service** (**SNS**), such
    as Facebook, Tumblr, and Instagram has become a rite of passage for teenagers
    around the world. Having a relatively large amount of disposable income, these
    adolescents are a coveted demographic for businesses hoping to sell snacks, beverages,
    electronics, and hygiene products.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 与朋友在**社交网络服务**（**SNS**）上互动，例如Facebook、Tumblr和Instagram，已经成为全球青少年的一项通行仪式。这些青少年通常拥有相对较多的可支配收入，因此成为了企业争相吸引的群体，企业希望通过他们销售零食、饮料、电子产品和卫生用品。
- en: The many millions of teenage consumers using such sites have attracted the attention
    of marketers struggling to find an edge in an increasingly competitive market.
    One way to gain this edge is to identify segments of teenagers who share similar
    tastes, so that clients can avoid targeting advertisements to teens with no interest
    in the product being sold. For instance, sporting apparel is likely to be a difficult
    sell to teens with no interest in sports.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些网站的数百万青少年消费者已经吸引了市场营销人员的关注，他们在日益竞争激烈的市场中努力寻找竞争优势。一种获得这种优势的方法是识别出具有相似品味的青少年群体，以便客户避免将广告投放给那些对所售产品没有兴趣的青少年。例如，运动服饰可能很难成功地销售给对体育不感兴趣的青少年。
- en: Given the text of teenagers' SNS pages, we can identify groups that share common
    interests such as sports, religion, or music. Clustering can automate the process
    of discovering the natural segments in this population. However, it will be up
    to us to decide whether or not the clusters are interesting and how we can use
    them for advertising. Let's try this process from start to finish.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 给定青少年SNS页面的文本，我们可以识别出一些具有共同兴趣的群体，例如体育、宗教或音乐。聚类可以自动化发现这一人群中的自然分段过程。然而，是否认为这些聚类有趣，以及如何利用它们进行广告投放，仍然需要我们自己决定。让我们从头到尾尝试这一过程。
- en: Step 1 – collecting data
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步 – 收集数据
- en: For this analysis, we will use a dataset representing a random sample of 30,000
    U.S. high school students who had profiles on a well-known SNS in 2006\. To protect
    the users' anonymity, the SNS will remain unnamed. However, at the time the data
    was collected, the SNS was a popular web destination for US teenagers. Therefore,
    it is reasonable to assume that the profiles represent a fairly wide cross section
    of American adolescents in 2006.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本次分析，我们将使用一个代表2006年在一个著名SNS上有个人资料的30,000名美国高中生的随机样本数据集。为了保护用户的匿名性，SNS的名称将保持不公开。然而，在数据收集时，这个SNS是美国青少年常用的网络平台。因此，可以合理假设这些个人资料代表了2006年美国青少年群体的一个较为广泛的横截面。
- en: Tip
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: This dataset was compiled by Brett Lantz while conducting sociological research
    on the teenage identities at the University of Notre Dame. If you use the data
    for research purposes, please cite this book chapter. The full dataset is available
    at the Packt Publishing website with the filename `snsdata.csv`. To follow along
    interactively, this chapter assumes that you have saved this file to your R working
    directory.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集是Brett Lantz在大学进行青少年身份的社会学研究时编制的。如果你用于研究目的，请引用这本书的章节。完整数据集可在Packt Publishing网站上下载，文件名为`snsdata.csv`。为了进行互动操作，本章假设你已将该文件保存到你的R工作目录中。
- en: The data was sampled evenly across four high school graduation years (2006 through
    2009) representing the senior, junior, sophomore, and freshman classes at the
    time of data collection. Using an automated web crawler, the full text of the
    SNS profiles were downloaded, and each teen's gender, age, and number of SNS friends
    was recorded.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 数据在四个高中毕业年份（2006年至2009年）之间均匀抽样，代表了数据收集时的高年级、低年级、二年级和一年级学生。使用自动化的网络爬虫，下载了SNS个人资料的完整文本，并记录了每个青少年的性别、年龄和SNS好友数量。
- en: 'A text mining tool was used to divide the remaining SNS page content into words.
    From the top 500 words appearing across all the pages, 36 words were chosen to
    represent five categories of interests: namely extracurricular activities, fashion,
    religion, romance, and antisocial behavior. The 36 words include terms such as
    *football*, *sexy*, *kissed*, *bible*, *shopping*, *death*, and *drugs*. The final
    dataset indicates, for each person, how many times each word appeared in the person''s
    SNS profile.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用了一个文本挖掘工具，将其余的SNS页面内容分割成单词。从所有页面中出现的前500个单词中，选择了36个单词来代表五类兴趣：即课外活动、时尚、宗教、浪漫和反社会行为。选中的36个单词包括*足球*、*性感*、*亲吻*、*圣经*、*购物*、*死亡*和*毒品*等。最终的数据集显示了每个人在其SNS个人资料中每个单词出现的次数。
- en: Step 2 – exploring and preparing the data
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步 – 探索和准备数据
- en: 'We can use the default settings of `read.csv()` to load the data into a data
    frame:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`read.csv()`的默认设置将数据加载到数据框中：
- en: '[PRE1]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s also take a quick look at the specifics of the data. The first several
    lines of the `str()` output are as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也快速查看一下数据的具体情况。`str()`输出的前几行如下：
- en: '[PRE2]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we had expected, the data include 30,000 teenagers with four variables indicating
    personal characteristics and 36 words indicating interests.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们预期的那样，数据包括30,000名青少年，四个变量表示个人特征，36个单词表示兴趣。
- en: Do you notice anything strange around the `gender` row? If you were looking
    carefully, you may have noticed the `NA` value, which is out of place compared
    to the `1` and `2` values. The `NA` is R's way of telling us that the record has
    a missing value—we do not know the person's gender. Until now, we haven't dealt
    with missing data, but it can be a significant problem for many types of analyses.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否注意到`gender`行中的异常情况？如果你仔细查看，可能已经注意到`NA`值，它与`1`和`2`的值不太一致。`NA`是R用来告诉我们记录缺少值的方式——我们不知道这个人的性别。到目前为止，我们还没有处理缺失数据，但对于许多类型的分析来说，缺失数据可能是一个重要问题。
- en: 'Let''s see how substantial this problem is. One option is to use the `table()`
    command, as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个问题的严重程度。一个选择是使用`table()`命令，如下所示：
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Although this command tells us how many `F` and `M` values are present, the
    `table()` function excluded the `NA` values rather than treating it as a separate
    category. To include the `NA` values (if there are any), we simply need to add
    an additional parameter:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管此命令告诉我们有多少`F`和`M`值存在，但`table()`函数排除了`NA`值，而不是将其视为一个单独的类别。为了包括`NA`值（如果有的话），我们只需添加一个额外的参数：
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we see that 2,724 records (9 percent) have missing gender data. Interestingly,
    there are over four times as many females as males in the SNS data, suggesting
    that males are not as inclined to use SNS websites as females.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到有2,724条记录（占9%）缺少性别数据。有趣的是，SNS数据中女性的数量是男性的四倍多，这表明男性使用SNS网站的倾向不如女性。
- en: 'If you examine the other variables in the data frame, you will find that besides
    `gender`, only `age` has missing values. For numeric data, the `summary()` command
    tells us the number of missing `NA` values:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你检查数据框中的其他变量，你会发现除了`gender`，只有`age`存在缺失值。对于数值型数据，`summary()`命令会告诉我们缺失的`NA`值的数量：
- en: '[PRE5]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: A total of 5,086 records (17 percent) have missing ages. Also concerning is
    the fact that the minimum and maximum values seem to be unreasonable; it is unlikely
    that a 3 year old or a 106 year old is attending high school. To ensure that these
    extreme values don't cause problems for the analysis, we'll need to clean them
    up before moving on.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有5,086条记录（占17%）缺少年龄数据。令人担忧的是，最小和最大值似乎不合理；一个3岁或106岁的学生不太可能上高中。为了确保这些极端值不会对分析造成问题，我们需要在继续之前清理它们。
- en: 'A more reasonable range of ages for the high school students includes those
    who are at least 13 years old and not yet 20 years old. Any age value falling
    outside this range should be treated the same as missing data—we cannot trust
    the age provided. To recode the age variable, we can use the `ifelse()` function,
    assigning `teen$age` the value of `teen$age` if the age is at least 13 and less
    than 20 years; otherwise, it will receive the value `NA`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 高中生的合理年龄范围应包括至少13岁但不满20岁的学生。任何超出此范围的年龄值应视为缺失数据——我们无法信任提供的年龄。为了重新编码年龄变量，我们可以使用`ifelse()`函数，当年龄至少为13岁且小于20岁时，将`teen$age`的值设为`teen$age`；否则，赋值为`NA`：
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'By rechecking the `summary()` output, we see that the age range now follows
    a distribution that looks much more like an actual high school:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重新检查`summary()`输出，我们发现现在的年龄范围呈现出一个更像实际高中学生的分布：
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Unfortunately, now we've created an even larger missing data problem. We'll
    need to find a way to deal with these values before continuing with our analysis.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，现在我们制造了一个更大的缺失数据问题。在继续分析之前，我们需要找到一种处理这些缺失值的方法。
- en: Data preparation – dummy coding missing values
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备——虚拟编码缺失值
- en: An easy solution for handling the missing values is to exclude any record with
    a missing value. However, if you think through the implications of this practice,
    you might think twice before doing so—just because it is easy does not mean it
    is a good idea! The problem with this approach is that even if the missingness
    is not extensive, you can easily exclude large portions of the data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失值的一个简单方法是排除任何缺失值的记录。然而，如果你考虑这种做法的后果，可能会在做之前三思而后行——仅仅因为这种方法简单，并不意味着它是个好主意！这种方法的问题在于，即使缺失值的数量不多，你也可能会轻易排除大量数据。
- en: For example, suppose that in our data, the people with the `NA` values for gender
    are completely different from those with missing age data. This would imply that
    by excluding those missing either gender or age, you would exclude *9% + 17% =
    26%* of the data, or over 7,500 records. And this is for missing data on only
    two variables! The larger the number of missing values present in a dataset, the
    more likely it is that any given record will be excluded. Fairly soon, you will
    be left with a tiny subset of data, or worse, the remaining records will be systematically
    different or non-representative of the full population.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设在我们的数据中，缺失性别值的人群与缺失年龄数据的人群完全不同。这意味着，如果你排除了缺失性别或年龄的记录，你将排除*9% + 17% = 26%*的数据，或者超过7,500条记录。而这仅仅是针对两个变量的缺失数据！缺失值数量越多，任何给定记录被排除的可能性就越大。很快，你将只剩下一个非常小的子集数据，或者更糟，剩下的记录将系统性地不同或不具代表性，无法代表整体人群。
- en: An alternative solution for categorical variables like gender is to treat a
    missing value as a separate category. For instance, rather than limiting to female
    and male, we can add an additional category for the unknown gender. This allows
    us to utilize dummy coding, which was covered in [Chapter 3](ch03.html "Chapter 3. Lazy
    Learning – Classification Using Nearest Neighbors"), *Lazy Learning – Classification
    Using Nearest Neighbors*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像性别这样的分类变量，另一种选择是将缺失值视为单独的类别。例如，而不是仅限于女性和男性，我们可以为未知性别添加一个额外的类别。这允许我们使用虚拟编码，这在[第
    3 章](ch03.html "第 3. 懒惰学习 – 使用最近邻分类")中有讲解，*懒惰学习 – 使用最近邻分类*。
- en: 'If you recall, dummy coding involves creating a separate binary (1 or 0) valued
    dummy variable for each level of a nominal feature except one, which is held out
    to serve as the reference group. The reason one category can be excluded is because
    its status can be inferred from the other categories. For instance, if someone
    is not female and not unknown gender, they must be male. Therefore, in this case,
    we need to only create dummy variables for female and unknown gender:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，虚拟编码涉及为名义特征的每个级别创建一个单独的二进制（1 或 0）值的虚拟变量，除了一个级别，该级别被保留作为参考组。可以排除一个类别的原因是因为可以从其他类别推断出它的状态。例如，如果某人既不是女性也不是未知性别，他们必须是男性。因此，在这种情况下，我们只需为女性和未知性别创建虚拟变量：
- en: '[PRE8]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As you might expect, the `is.na()` function tests whether gender is equal to
    `NA`. Therefore, the first statement assigns `teens$female` the value `1` if gender
    is equal to `F` and the gender is not equal to `NA`; otherwise, it assigns the
    value `0`. In the second statement, if `is.na()` returns `TRUE`, meaning the gender
    is missing, the `teens$no_gender` variable is assigned `1`; otherwise, it is assigned
    the value `0`. To confirm that we did the work correctly, let''s compare our constructed
    dummy variables to the original `gender` variable:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能预期的那样，`is.na()` 函数测试性别是否等于 `NA`。因此，第一条语句如果性别等于 `F` 且性别不等于 `NA`，则为 `teens$female`
    赋值 `1`；否则，赋值 `0`。在第二条语句中，如果 `is.na()` 返回 `TRUE`，表示性别缺失，则将 `teens$no_gender` 变量赋值为
    `1`；否则，赋值为 `0`。为了确认我们的工作是否正确，让我们将构建的虚拟变量与原始的 `gender` 变量进行比较：
- en: '[PRE9]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The number of `1` values for `teens$female` and `teens$no_gender` matches the
    number of `F` and `NA` values, respectively, so we should be able to trust our
    work.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `teens$female` 和 `teens$no_gender` 中的 `1` 值数量与 `F` 和 `NA` 值的数量匹配，因此我们应该能够信任我们的工作。
- en: Data preparation – imputing the missing values
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备 – 填补缺失值
- en: Next, let's eliminate the 5,523 missing age values. As age is numeric, it doesn't
    make sense to create an additional category for the unknown values—where would
    you rank "unknown" relative to the other ages? Instead, we'll use a different
    strategy known as **imputation**, which involves filling in the missing data with
    a guess as to the true value.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们排除 5,523 个缺失的年龄值。由于年龄是数值型的，为未知值创建一个额外的类别没有意义——相对于其他年龄，你会将"未知"排名在哪里呢？相反，我们将使用一种称为**插补**的不同策略，它涉及用真实值的猜测填补缺失数据。
- en: Can you think of a way we might be able to use the SNS data to make an informed
    guess about a teenager's age? If you are thinking of using the graduation year,
    you've got the right idea. Most people in a graduation cohort were born within
    a single calendar year. If we can identify the typical age for each cohort, we
    would have a fairly reasonable estimate of the age of a student in that graduation
    year.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你能想到我们如何能够利用 SNS 数据来推断青少年的年龄吗？如果你考虑使用毕业年份，那么你有正确的想法。一个毕业队列中的大多数人在一个日历年内出生。如果我们能够确定每个队列的典型年龄，我们将有一个相当合理的估计来描述该毕业年份的学生的年龄。
- en: 'One way to find a typical value is by calculating the average or mean value.
    If we try to apply the `mean()` function, as we did for previous analyses, there''s
    a problem:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 找到典型值的一种方法是计算平均值或均值。如果我们尝试应用 `mean()` 函数，就像我们之前分析过的那样，会有一个问题：
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The issue is that the mean is undefined for a vector containing missing data.
    As our age data contains missing values, `mean(teens$age)` returns a missing value.
    We can correct this by adding an additional parameter to remove the missing values
    before calculating the mean:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于对包含缺失数据的向量计算均值是未定义的。由于我们的年龄数据包含缺失值，`mean(teens$age)` 返回一个缺失值。我们可以通过添加额外的参数在计算均值之前删除缺失值来纠正这一点：
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This reveals that the average student in our data is about 17 years old. This
    only gets us part of the way there; we actually need the average age for each
    graduation year. You might be tempted to calculate the mean four times, but one
    of the benefits of R is that there''s usually a way to avoid repeating oneself.
    In this case, the `aggregate()` function is the tool for the job. It computes
    statistics for subgroups of data. Here, it calculates the mean age by graduation
    year after removing the `NA` values:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们数据中的平均学生年龄大约为17岁。这只帮助我们达成了一部分目标；我们实际上需要每个毕业年份的平均年龄。你可能会想计算四次均值，但 R 的一个优点是通常有方法避免重复。在这种情况下，`aggregate()`
    函数就是合适的工具。它计算数据子组的统计信息。在这里，它计算每个毕业年份的平均年龄，并在去除 `NA` 值后进行：
- en: '[PRE12]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The mean age differs by roughly one year per change in graduation year. This
    is not at all surprising, but a helpful finding for confirming our data is reasonable.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 平均年龄每变化一次毕业年份大约差一年。这一点并不令人惊讶，但对于确认我们的数据合理性是一个有用的发现。
- en: 'The `aggregate()` output is a data frame. This is helpful for some purposes,
    but would require extra work to merge back onto our original data. As an alternative,
    we can use the `ave()` function, which returns a vector with the group means repeated
    so that the result is equal in length to the original vector:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`aggregate()` 输出的是一个数据框。这对某些目的很有帮助，但需要额外的工作将其合并回原始数据。作为替代方案，我们可以使用 `ave()`
    函数，它返回一个包含组均值的向量，这样结果的长度与原始向量相同：'
- en: '[PRE13]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To impute these means onto the missing values, we need one more `ifelse()`
    call to use the `ave_age` value only if the original age value was `NA`:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这些均值填充到缺失值上，我们需要再调用一次 `ifelse()`，只有当原始年龄值为 `NA` 时才使用 `ave_age` 值：
- en: '[PRE14]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `summary()` results show that the missing values have now been eliminated:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`summary()` 结果显示，现在缺失值已被消除：'
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With the data ready for analysis, we are ready to dive into the interesting
    part of this project. Let's see whether our efforts have paid off.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备好进行分析后，我们可以深入到这个项目的有趣部分。让我们看看我们的努力是否得到了回报。
- en: Step 3 – training a model on the data
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步 – 在数据上训练模型
- en: To cluster the teenagers into marketing segments, we will use an implementation
    of k-means in the `stats` package, which should be included in your R installation
    by default. If by chance you do not have this package, you can install it as you
    would any other package and load it using the `library(stats)` command. Although
    there is no shortage of k-means functions available in various R packages, the
    `kmeans()` function in the `stats` package is widely used and provides a vanilla
    implementation of the algorithm.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将青少年分为不同的营销群体，我们将使用 `stats` 包中的 k-means 实现，该包应该默认包含在你的 R 安装中。如果你碰巧没有这个包，可以像安装其他包一样安装它，并使用
    `library(stats)` 命令加载它。尽管各种 R 包中有很多 k-means 函数，但 `stats` 包中的 `kmeans()` 函数被广泛使用，并提供了该算法的标准实现。
- en: '![Step 3 – training a model on the data](img/B03905_09_10.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![第3步 – 在数据上训练模型](img/B03905_09_10.jpg)'
- en: The `kmeans()` function requires a data frame containing only numeric data and
    a parameter specifying the desired number of clusters. If you have these two things
    ready, the actual process of building the model is simple. The trouble is that
    choosing the right combination of data and clusters can be a bit of an art; sometimes
    a great deal of trial and error is involved.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`kmeans()` 函数需要一个仅包含数值数据的数据框以及一个指定所需簇数的参数。如果你准备好了这两样东西，实际构建模型的过程就很简单。问题在于，选择合适的数据和簇的组合有时是一门艺术；这通常需要经过大量的反复试验。'
- en: 'We''ll start our cluster analysis by considering only the 36 features that
    represent the number of times various interests appeared on the teen SNS profiles.
    For convenience, let''s make a data frame containing only these features:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过仅考虑代表青少年 SNS 个人资料中各种兴趣出现次数的36个特征来开始我们的聚类分析。为方便起见，我们创建一个仅包含这些特征的数据框：
- en: '[PRE16]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If you recall from [Chapter 3](ch03.html "Chapter 3. Lazy Learning – Classification
    Using Nearest Neighbors"), *Lazy Learning – Classification Using Nearest Neighbors*,
    a common practice employed prior to any analysis using distance calculations is
    to normalize or z-score standardize the features so that each utilizes the same
    range. By doing so, you can avoid a problem in which some features come to dominate
    solely because they have a larger range of values than the others.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得[第3章](ch03.html "第3章. 懒学习 - 使用最近邻分类")，*懒学习 - 使用最近邻分类*，在进行任何距离计算分析之前的常见做法是对特征进行归一化或z-score标准化，以确保每个特征使用相同的范围。通过这样做，你可以避免某些特征仅因为它们具有比其他特征更大的数值范围而主导结果的问题。
- en: The process of z-score standardization rescales features so that they have a
    mean of zero and a standard deviation of one. This transformation changes the
    interpretation of the data in a way that may be useful here. Specifically, if
    someone mentions football three times on their profile, without additional information,
    we have no idea whether this implies they like football more or less than their
    peers. On the other hand, if the z-score is three, we know that that they mentioned
    football many more times than the average teenager.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: z-score标准化过程会重新缩放特征，使其均值为零，标准差为一。这种转换改变了数据的解释方式，这在此处可能会有用。具体来说，如果某人在其个人资料中提到足球三次，在没有更多信息的情况下，我们无法判断这是否意味着他们比其他人更喜欢足球。另一方面，如果z-score为三，我们就知道他们提到足球的次数远远超过了平均水平的青少年。
- en: 'To apply the z-score standardization to the `interests` data frame, we can
    use the `scale()` function with `lapply()` as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要对`interests`数据框应用z-score标准化，我们可以使用`lapply()`配合`scale()`函数，如下所示：
- en: '[PRE17]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Since `lapply()` returns a matrix, it must be coerced back to data frame form
    using the `as.data.frame()` function.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`lapply()`返回的是矩阵，因此必须使用`as.data.frame()`函数将其转换回数据框形式。
- en: Our last decision involves deciding how many clusters to use for segmenting
    the data. If we use too many clusters, we may find them too specific to be useful;
    conversely, choosing too few may result in heterogeneous groupings. You should
    feel comfortable experimenting with the values of *k*. If you don't like the result,
    you can easily try another value and start over.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后的决定是决定使用多少个簇来对数据进行分段。如果我们使用太多的簇，可能会发现它们过于具体，无法发挥作用；相反，选择太少的簇可能导致分组不均。你应该敢于尝试不同的*k*值。如果你不喜欢结果，可以轻松尝试另一个值并重新开始。
- en: Tip
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Choosing the number of clusters is easier if you are familiar with the analysis
    population. Having a hunch about the true number of natural groupings can save
    you some trial and error.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对分析群体有所了解，选择簇的数量会更加容易。对自然分组的真实数量有直觉可以帮助你节省一些试错的时间。
- en: 'To help us predict the number of clusters in the data, I''ll defer to one of
    my favorite films, The *Breakfast Club*, a coming-of-age comedy released in 1985
    and directed by John Hughes. The teenage characters in this movie are identified
    in terms of five stereotypes: a brain, an athlete, a basket case, a princess,
    and a criminal. Given that these identities prevail throughout popular teen fiction,
    five seems like a reasonable starting point for *k*.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们预测数据中的簇的数量，我想引用我最喜欢的电影之一，《*早餐俱乐部*》，这是一部1985年上映的成长喜剧片，由约翰·休斯执导。电影中的青少年角色按照五种刻板印象进行分类：学霸、运动员、怪胎、公主和罪犯。考虑到这些身份在流行的青少年小说中常常出现，五个似乎是*k*的一个合理起点。
- en: 'To use the k-means algorithm to divide the teenagers'' interest data into five
    clusters, we use the `kmeans()` function on the `interests` data frame. Because
    the k-means algorithm utilizes random starting points, the `set.seed()` function
    is used to ensure that the results match the output in the examples that follow.
    If you recall from the previous chapters, this command initializes R''s random
    number generator to a specific sequence. In the absence of this statement, the
    results will vary each time the k-means algorithm is run:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用k-means算法将青少年的兴趣数据分成五个簇，我们在`interests`数据框上使用`kmeans()`函数。由于k-means算法使用随机起始点，因此使用`set.seed()`函数来确保结果与以下示例中的输出一致。如果你记得前几章的内容，这个命令初始化了R的随机数生成器，设置为特定的序列。如果没有这个命令，每次运行k-means算法时结果都会不同：
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The result of the k-means clustering process is a list named `teen_clusters`
    that stores the properties of each of the five clusters. Let's dig in and see
    how well the algorithm has divided the teens' interest data.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: k-means聚类过程的结果是一个名为`teen_clusters`的列表，存储了五个聚类的各项属性。让我们深入了解一下，看看算法是如何将青少年的兴趣数据划分的。
- en: Tip
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If you find that your results differ from those shown here, ensure that the
    `set.seed(2345)` command is run immediately prior to the `kmeans()` function.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现你的结果与这里展示的不同，请确保在运行`kmeans()`函数之前，立即执行`set.seed(2345)`命令。
- en: Step 4 – evaluating model performance
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 4 – 评估模型性能
- en: Evaluating clustering results can be somewhat subjective. Ultimately, the success
    or failure of the model hinges on whether the clusters are useful for their intended
    purpose. As the goal of this analysis was to identify clusters of teenagers with
    similar interests for marketing purposes, we will largely measure our success
    in qualitative terms. For other clustering applications, more quantitative measures
    of success may be needed.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 评估聚类结果可能是有一定主观性的。最终，模型的成功或失败取决于聚类是否能为其预期的目的提供帮助。由于本次分析的目标是识别具有相似兴趣的青少年群体，以便用于市场营销，我们将主要从定性角度来衡量成功。对于其他聚类应用，可能需要更多定量的成功衡量标准。
- en: 'One of the most basic ways to evaluate the utility of a set of clusters is
    to examine the number of examples falling in each of the groups. If the groups
    are too large or too small, they are not likely to be very useful. To obtain the
    size of the `kmeans()` clusters, use the `teen_clusters$size` component as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 评估一组聚类有效性的最基本方法之一是检查每个组中示例的数量。如果这些组太大或太小，它们可能不会非常有用。要获取`kmeans()`聚类的大小，请使用`teen_clusters$size`组件，如下所示：
- en: '[PRE19]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Here, we see the five clusters we requested. The smallest cluster has 600 teenagers
    (2 percent) while the largest cluster has 21,514 (72 percent). Although the large
    gap between the number of people in the largest and smallest clusters is slightly
    concerning, without examining these groups more carefully, we will not know whether
    or not this indicates a problem. It may be the case that the clusters' size disparity
    indicates something real, such as a big group of teens that share similar interests,
    or it may be a random fluke caused by the initial k-means cluster centers. We'll
    know more as we start to look at each cluster's homogeneity.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到了我们请求的五个聚类。最小的聚类有600个青少年（占2%），而最大的聚类有21,514个（占72%）。虽然最大和最小聚类之间人数差距较大，这有点令人担忧，但在没有更仔细检查这些组的情况下，我们无法知道这是否表示存在问题。也许，聚类大小的差异反映了某些实际情况，例如一大群有相似兴趣的青少年，或者它可能是由初始的k-means聚类中心引起的随机巧合。随着我们开始查看每个聚类的同质性，我们会了解更多。
- en: Tip
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Sometimes, k-means may find extremely small clusters—occasionally, as small
    as a single point. This can happen if one of the initial cluster centers happens
    to fall on an outlier far from the rest of the data. It is not always clear whether
    to treat such small clusters as a true finding that represents a cluster of extreme
    cases, or a problem caused by random chance. If you encounter this issue, it may
    be worth re-running the k-means algorithm with a different random seed to see
    whether the small cluster is robust to different starting points.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，k-means可能会找到极小的聚类——有时甚至只有一个点。如果初始聚类中心恰好落在一个远离其他数据的离群点上，就可能发生这种情况。是否将这样的极小聚类视为一个真实的发现，代表一个极端案例的聚类，或者视为由随机机会引起的问题，并不总是很明确。如果遇到这个问题，可以考虑使用不同的随机种子重新运行k-means算法，看看这个小聚类是否对不同的起始点具有稳健性。
- en: 'For a more in-depth look at the clusters, we can examine the coordinates of
    the cluster centroids using the `teen_clusters$centers` component, which is as
    follows for the first four interests:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要更深入地了解这些聚类，我们可以通过`teen_clusters$centers`组件检查聚类中心的坐标，以下是前四个兴趣的情况：
- en: '[PRE20]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The rows of the output (labeled `1` to `5`) refer to the five clusters, while
    the numbers across each row indicate the cluster's average value for the interest
    listed at the top of the column. As the values are z-score standardized, positive
    values are above the overall mean level for all the teens and negative values
    are below the overall mean. For example, the third row has the highest value in
    the basketball column, which means that cluster `3` has the highest average interest
    in basketball among all the clusters.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的行（标记为`1`到`5`）表示五个聚类，而每行中的数字表示该聚类在每列顶部列出的兴趣项的平均值。由于这些值是经过z分数标准化的，正值表示该兴趣项高于所有青少年总体均值，负值表示低于总体均值。例如，第三行在篮球这一列中的值最高，这意味着聚类`3`在所有聚类中对篮球的平均兴趣最高。
- en: 'By examining whether the clusters fall above or below the mean level for each
    interest category, we can begin to notice patterns that distinguish the clusters
    from each other. In practice, this involves printing the cluster centers and searching
    through them for any patterns or extreme values, much like a word search puzzle
    but with numbers. The following screenshot shows a highlighted pattern for each
    of the five clusters, for 19 of the 36 teen interests:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查聚类在每个兴趣类别上是否高于或低于平均水平，我们可以开始注意到区分聚类的模式。实际上，这涉及打印聚类中心并搜索其中的模式或极端值，类似于一个数字版的单词搜索谜题。以下截图显示了五个聚类在36个青少年兴趣中的19个兴趣的突出模式：
- en: '![Step 4 – evaluating model performance](img/B03905_09_11.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![步骤4 – 评估模型性能](img/B03905_09_11.jpg)'
- en: Given this subset of the interest data, we can already infer some characteristics
    of the clusters. **Cluster 3** is substantially above the mean interest level
    on all the sports. This suggests that this may be a group of **Athletes** per
    *The Breakfast Club* stereotype. **Cluster 1** includes the most mentions of "cheerleading,"
    the word "hot," and is above the average level of football interest. Are these
    the so-called **Princesses**?
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这一子集的兴趣数据，我们已经能够推断出一些聚类的特征。**聚类3**在所有体育项目上的兴趣水平都显著高于平均水平。这表明这可能是*早餐俱乐部*刻板印象中的**运动员**群体。**聚类1**包括最多提到“啦啦队”的内容、词汇“热”和高于平均水平的足球兴趣。这些是所谓的**公主**吗？
- en: By continuing to examine the clusters in this way, it is possible to construct
    a table listing the dominant interests of each of the groups. In the following
    table, each cluster is shown with the features that most distinguish it from the
    other clusters, and The *Breakfast Club* identity that most accurately captures
    the group's characteristics.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 通过继续以这种方式检查聚类，我们可以构建一张表，列出每个群体的主要兴趣。在以下表格中，显示了每个聚类与其他聚类最具区别性的特征，以及最能准确描述该群体特点的*早餐俱乐部*身份。
- en: Interestingly, **Cluster 5** is distinguished by the fact that it is unexceptional;
    its members had lower-than-average levels of interest in every measured activity.
    It is also the single largest group in terms of the number of members. One potential
    explanation is that these users created a profile on the website but never posted
    any interests.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，**聚类5**的特点在于其成员在每项活动中的兴趣水平都低于平均值，且它是成员数量最多的单一群体。一种可能的解释是，这些用户创建了一个网站个人资料，但从未发布任何兴趣。
- en: '![Step 4 – evaluating model performance](img/B03905_09_12.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![步骤4 – 评估模型性能](img/B03905_09_12.jpg)'
- en: Tip
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: When sharing the results of a segmentation analysis, it is often helpful to
    apply informative labels that simplify and capture the essence of the groups such
    as *The Breakfast Club* typology applied here. The risk in adding such labels
    is that they can obscure the groups' nuances by stereotyping the group members.
    As such labels can bias our thinking, important patterns can be missed if labels
    are taken as the whole truth.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在分享分段分析结果时，通常应用具有信息量的标签可以帮助简化并捕捉群体的本质，如此处应用的*早餐俱乐部*类型学。加入此类标签的风险在于，它们可能通过刻板印象掩盖群体的细微差别。由于此类标签可能会偏向我们的思维，如果将标签视为全部真理，可能会错过一些重要的模式。
- en: Given the table, a marketing executive would have a clear depiction of five
    types of teenage visitors to the social networking website. Based on these profiles,
    the executive could sell targeted advertising impressions to businesses with products
    relevant to one or more of the clusters. In the next section, we will see how
    the cluster labels can be applied back to the original population for such uses.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 根据表格，营销主管将能清晰地描绘出五种类型的青少年社交网站访客。基于这些档案，主管可以向相关产品的企业出售有针对性的广告展示。在接下来的部分中，我们将看到如何将群体标签应用回原始人群以用于此类用途。
- en: Step 5 – improving model performance
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第五步 – 改进模型性能
- en: Because clustering creates new information, the performance of a clustering
    algorithm depends at least somewhat on both the quality of the clusters themselves
    as well as what is done with that information. In the preceding section, we already
    demonstrated that the five clusters provided useful and novel insights into the
    interests of teenagers. By that measure, the algorithm appears to be performing
    quite well. Therefore, we can now focus our effort on turning these insights into
    action.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由于聚类创造了新信息，聚类算法的表现至少在某种程度上取决于群体本身的质量以及如何利用这些信息。在前面的部分中，我们已经展示了这五个群体为青少年的兴趣提供了有用且新颖的见解。以此衡量，算法的表现相当不错。因此，我们现在可以将精力集中在将这些见解转化为行动上。
- en: 'We''ll begin by applying the clusters back onto the full dataset. The `teen_clusters`
    object created by the `kmeans()` function includes a component named `cluster`
    that contains the cluster assignments for all 30,000 individuals in the sample.
    We can add this as a column on the `teens` data frame with the following command:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先将群体应用回完整数据集。由`kmeans()`函数创建的`teen_clusters`对象包含一个名为`cluster`的组件，该组件包含样本中30,000个个体的群体分配信息。我们可以通过以下命令将其作为一列添加到`teens`数据框中：
- en: '[PRE21]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Given this new data, we can start to examine how the cluster assignment relates
    to individual characteristics. For example, here''s the personal information for
    the first five teens in the SNS data:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这一新数据，我们可以开始检查群体分配与个人特征之间的关系。例如，以下是SNS数据中前五个青少年的个人信息：
- en: '[PRE22]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Using the `aggregate()` function, we can also look at the demographic characteristics
    of the clusters. The mean age does not vary much by cluster, which is not too
    surprising as these teen identities are often determined before high school. This
    is depicted as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`aggregate()`函数，我们还可以查看各群体的不同人口特征。群体间的平均年龄变化不大，这并不令人惊讶，因为这些青少年的身份通常在上高中之前就已确定。情况如下所示：
- en: '[PRE23]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'On the other hand, there are some substantial differences in the proportion
    of females by cluster. This is a very interesting finding as we didn''t use gender
    data to create the clusters, yet the clusters are still predictive of gender:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，不同群体中女性所占比例存在一些显著差异。这是一个非常有趣的发现，因为我们并未使用性别数据来创建群体，但群体仍然能预测性别：
- en: '[PRE24]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Recall that overall about 74 percent of the SNS users are female. **Cluster
    1**, the so-called **Princesses**, is nearly 84 percent female, while **Cluster
    2** and **Cluster 5** are only about 70 percent female. These disparities imply
    that there are differences in the interests that teen boys and girls discuss on
    their social networking pages.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，SNS用户中大约74%是女性。**群体1**，即所谓的**公主群体**，女性比例接近84%，而**群体2**和**群体5**的女性比例仅约为70%。这些差异表明青少年男孩和女孩在社交网络页面上讨论的兴趣有所不同。
- en: 'Given our success in predicting gender, you might also suspect that the clusters
    are predictive of the number of friends the users have. This hypothesis seems
    to be supported by the data, which is as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们在预测性别上的成功，您可能也会怀疑群体能否预测用户拥有的朋友数量。这个假设似乎得到了数据的支持，具体如下：
- en: '[PRE25]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: On an average, **Princesses** have the most friends (41.4), followed by **Athletes**
    (37.2) and **Brains** (32.6). On the low end are **Criminals** (30.5) and **Basket
    Cases** (27.7). As with gender, the connection between a teen's number of friends
    and their predicted cluster is remarkable, given that we did not use the friendship
    data as an input to the clustering algorithm. Also interesting is the fact that
    the number of friends seems to be related to the stereotype of each clusters'
    high school popularity; the stereotypically popular groups tend to have more friends.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，**公主**拥有最多的朋友（41.4），其次是**运动员**（37.2）和**学霸**（32.6）。最低的是**罪犯**（30.5）和**精神病患者**（27.7）。与性别一样，青少年朋友数量与其预测聚类之间的联系非常显著，尽管我们并未将朋友数据作为聚类算法的输入。另外有趣的是，朋友数量似乎与每个群体在高中受欢迎程度的刻板印象相关；那些在刻板印象中受欢迎的群体往往拥有更多的朋友。
- en: The association among group membership, gender, and number of friends suggests
    that the clusters can be useful predictors of behavior. Validating their predictive
    ability in this way may make the clusters an easier sell when they are pitched
    to the marketing team, ultimately improving the performance of the algorithm.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 群体成员、性别和朋友数量之间的关联表明，聚类可以是行为的有用预测指标。通过这种方式验证它们的预测能力，可以使聚类在向营销团队推销时更具吸引力，从而最终提升算法的表现。
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Our findings support the popular adage that "birds of a feather flock together."
    By using machine learning methods to cluster teenagers with others who have similar
    interests, we were able to develop a typology of teen identities that was predictive
    of personal characteristics, such as gender and the number of friends. These same
    methods can be applied to other contexts with similar results.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的发现支持了那句流行的格言：“物以类聚，人以群分。”通过使用机器学习方法将青少年与有相似兴趣的人进行聚类，我们能够开发出一种青少年身份的分类法，该分类法能够预测个人特征，如性别和朋友数量。这些相同的方法也可以应用于其他情境，并取得类似的结果。
- en: This chapter covered only the fundamentals of clustering. As a very mature machine
    learning method, there are many variants of the k-means algorithm as well as many
    other clustering algorithms that bring unique biases and heuristics to the task.
    Based on the foundation in this chapter, you will be able to understand and apply
    other clustering methods to new problems.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 本章仅介绍了聚类的基本概念。作为一种非常成熟的机器学习方法，k-means算法有许多变体，还有许多其他聚类算法，它们为任务带来了独特的偏差和启发式方法。基于本章的基础，你将能够理解并应用其他聚类方法来解决新的问题。
- en: In the next chapter, we will begin to look at methods for measuring the success
    of a learning algorithm, which are applicable across many machine learning tasks.
    While our process has always devoted some effort to evaluating the success of
    learning, in order to obtain the highest degree of performance, it is crucial
    to be able to define and measure it in the strictest terms.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将开始探讨衡量学习算法成功的方法，这些方法适用于许多机器学习任务。虽然我们的过程一直在评估学习的成功，但为了获得最高的性能水平，能够在最严格的术语下定义和衡量它是至关重要的。
