- en: Chapter 7. Online and Batch Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。在线和批量学习
- en: In this chapter, you will be presented with best practices when it comes to
    training classifiers on big data. The new approach, exposed in the following pages,
    is both scalable and generic, making it perfect for datasets with a huge number
    of observations. Moreover, this approach can allow you to cope with streaming
    datasets—that is, datasets with observations transmitted on-the-fly and not all
    available at the same time. Furthermore, such an approach enhances precision,
    as more data is fed in during the training process.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解在大数据上训练分类器的最佳实践。在接下来的几页中公开的新方法既可扩展又通用，使其非常适合具有大量观测值的数据集。此外，这种方法还可以让您处理流数据集——即观测值在飞行中传输且不是同时可用的数据集。此外，这种方法通过在训练过程中输入更多数据来提高精度。
- en: With respect to the classic approach seen so far in the book, batch learning,
    this new approach is, not surprisingly, called online learning. The core of online
    learning is the *divide et impera* (divide and conquer) principle whereby each
    step of a mini-batch of the data serves as input to train and improve the classifier.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与书中迄今为止看到的经典方法（批量学习）相比，这种新的方法不出所料地被称为在线学习。在线学习的核心是 *divide et impera*（分而治之）原则，即数据的小批量中的每一步都作为训练和改进分类器的输入。
- en: In this chapter, we will first focus on batch learning and its limitations,
    and then introduce online learning. Finally, we will supply an example of big
    data, showing the benefits of combining online learning and hashing tricks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先关注批量学习及其局限性，然后介绍在线学习。最后，我们将提供一个大数据的例子，展示结合在线学习和哈希技巧的好处。
- en: Batch learning
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量学习
- en: When the dataset is fully available at the beginning of a supervised task, and
    doesn't exceed the quantity of RAM on your machine, you can train the classifier
    or the regression using batch learning. As seen in previous chapters, during training
    the learner scans the full dataset. This also happens when **stochastic gradient
    descent** (SGD)-based methods are used (see [Chapter 2](part0018_split_000.html#H5A42-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 2. Approaching Simple Linear Regression"), *Approaching Simple Linear
    Regression* and [Chapter 3](part0023_split_000.html#LTSU2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 3. Multiple Regression in Action"), *Multiple Regression in Action*).
    Let's now compare how much time is needed to train a linear regressor and relate
    its performance with the number of observations in the dataset (that is, the number
    of rows of the feature matrix *X*) and the number of features (that is, the number
    of columns of *X*). In this first experiment, we will use the plain vanilla `LinearRegression()`
    and `SGDRegressor()` classes provided by Scikit-learn, and we will store the actual
    time taken to fit a classifier, without any parallelization.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当监督任务开始时数据集完全可用，并且不超过您机器上的RAM数量时，您可以使用批量学习来训练分类器或回归。如前几章所见，在学习过程中，学习器会扫描整个数据集。这也发生在使用基于
    **随机梯度下降**（SGD）的方法时（见[第2章](part0018_split_000.html#H5A42-a2faae6898414df7b4ff4c9a487a20c6
    "第2章。接近简单线性回归")，*接近简单线性回归*和[第3章](part0023_split_000.html#LTSU2-a2faae6898414df7b4ff4c9a487a20c6
    "第3章。实际中的多重回归")，*实际中的多重回归*)。现在让我们比较训练线性回归器所需的时间，并将其性能与数据集中的观测值数量（即特征矩阵 *X* 的行数）和特征数量（即
    *X* 的列数）联系起来。在这个第一个实验中，我们将使用Scikit-learn提供的普通`LinearRegression()`和`SGDRegressor()`类，并且我们将存储拟合分类器实际花费的时间，而不进行任何并行化。
- en: 'Let''s first create a function to create fake datasets: it takes as parameters
    the number of training points and the number of features (plus, optionally, the
    noise variance), and returns normalized training and testing feature matrixes
    and labels. Note that all the features in the *X* matrix are numerical:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先创建一个函数来创建假数据集：它接受训练点的数量和特征的数目（以及可选的噪声方差）作为参数，并返回归一化的训练和测试特征矩阵以及标签。请注意，*X*
    矩阵中的所有特征都是数值型的：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s now store the time needed to train and test the learner in all the combinations
    of these configurations:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们存储训练和测试学习者在所有这些配置组合中所需的时间：
- en: 'Two classifiers: `LinearRegression()` and `SGDRegressor()`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个分类器：`LinearRegression()` 和 `SGDRegressor()`
- en: 'Number of observations: `1000`, `10000`, and `100000`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观测值数量：`1000`，`10000` 和 `100000`
- en: 'Number of features: `10`, `50`, `100`, `500`, and `1000`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征数量：`10`，`50`，`100`，`500` 和 `1000`
- en: 'To average the results, each training operation is performed five times. The
    testing dataset always comprises `1000` observations:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了平均结果，每个训练操作执行五次。测试数据集始终包含1000个观察值：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s finally plot the results. In the following screenshot, the chart on
    the left shows the training time of the `LogisticRegressor` algorithm against
    the number of features, whereas the chart on the right displays the time against
    the number of observations:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们绘制结果。在下面的屏幕截图中，左边的图表显示了 `LogisticRegressor` 算法的训练时间与特征数量的关系，而右边的图表显示了时间与观察数量的关系：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Batch learning](img/00113.jpeg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![批量学习](img/00113.jpeg)'
- en: 'In the plots, you can see that the classifier is pretty good on small datasets,
    with a small number of features and observations. While dealing with the largest
    *X* matrix, 1,000 features and 100,000 observations (containing 100 million entries),
    the training time is just above `30` seconds: that''s also the limit above which
    the regressor no longer scales.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表中，你可以看到，对于小数据集，分类器表现相当不错，具有少量特征和观察值。而在处理最大的 *X* 矩阵时，1,000个特征和100,000个观察值（包含1亿条记录），训练时间仅为30秒以上：这也是回归器不再可扩展的极限。
- en: 'Let''s now see what happens with the testing time:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看测试时间的情况：
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Batch learning](img/00114.jpeg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![批量学习](img/00114.jpeg)'
- en: The testing time does scale as a linear function of the number of features,
    and it's independent of it. It seems that applying the linear approach is not
    very much of a problem on big data, fortunately.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 测试时间随着特征数量的增加而线性增加，并且与特征数量无关。幸运的是，在大数据中应用线性方法似乎并不是一个大问题。
- en: 'Let''s now see what happens with the SGD implementation of linear regression:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看线性回归的SGD实现会发生什么：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Batch learning](img/00115.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![批量学习](img/00115.jpeg)'
- en: 'The results have drastically changed in comparison with the previous regressor:
    on the biggest matrix, this learner takes around 1.5 seconds. It also seems that
    the time needed to train an SGD regressor is linear with respect to the number
    of features and the number of training points. Let''s now verify how it works
    in testing:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的回归器相比，结果发生了巨大变化：在最大的矩阵上，这个学习者大约需要1.5秒。似乎训练SGD回归器所需的时间与特征数量和训练点的数量呈线性关系。现在让我们验证它在测试中的表现：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Batch learning](img/00116.jpeg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![批量学习](img/00116.jpeg)'
- en: Applying the SGD-based learner on a test dataset takes about the same time as
    the other implementation. Here, again, there is really no problem when scaling
    the solution on big datasets.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据集上应用基于SGD的学习者所需的时间与其他实现方式大致相同。在这里，同样，当在大数据集上扩展解决方案时，实际上并没有真正的问题。
- en: Online mini-batch learning
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线小批量学习
- en: 'From the previous section, we''ve learned an interesting lesson: for big data,
    always use SGD-based learners because they are faster, and they do scale.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一节中，我们学到了一个有趣的教训：对于大数据，始终使用基于SGD的学习者，因为它们更快，并且可以扩展。
- en: 'Now, in this section, let''s consider this regression dataset:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在本节中，让我们考虑这个回归数据集：
- en: 'Massive number of observations: 2M'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察数量巨大：200万
- en: 'Large number of features: 100'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征数量众多：100
- en: Noisy dataset
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 噪声数据集
- en: The `X_train` matrix is composed of 200 million elements, and may not completely
    fit in memory (on a machine with 4 GB RAM); the testing set is composed of 10,000
    observations.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`X_train` 矩阵由2亿个元素组成，可能无法完全适应内存（在4GB RAM的机器上）；测试集由10,000个观察值组成。'
- en: 'Let''s first create the datasets, and print the memory footprint of the biggest
    one:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先创建数据集，并打印出最大的数据集的内存占用：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `X_train` matrix is itself `1.6` GB of data; we can consider it as a starting
    point for big data. Let's now try to classify it using the best model we got from
    the previous section, `SGDRegressor()`. To access its performance, we use MAE,
    the Mean Absolute Error (as for error evaluation, the lower the better).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`X_train` 矩阵本身就有1.6GB的数据；我们可以将其视为大数据的起点。现在让我们尝试使用上一节中得到的最佳模型 `SGDRegressor()`
    来对其进行分类。为了评估其性能，我们使用MAE，即平均绝对误差（对于误差评估，越低越好）。'
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: On our computer (equipped with Mac OS and 4 GB of RAM), this operation takes
    around 6 seconds, and the final MAE is 10^(-1.24).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的电脑上（配备Mac OS和4GB的RAM），这个操作大约需要6秒钟，最终的MAE是10^(-1.24)。
- en: Can we do better? Yes, with mini-batches and online learning. Before we see
    these in action, let's introduce how SGD works with mini-batches.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做得更好吗？是的，通过小批量学习和在线学习。在我们看到这些实际应用之前，让我们介绍一下SGD如何与小批量一起工作。
- en: Split the `X_train` matrix in batches of *N* observations. Since we're using
    SGD, if possible it's better to shuffle the observations, as the method is strongly
    driven by the order of the input vectors. At this point, every mini-batch has
    *N* lines and *M* columns (where *M* is the number of features).
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`X_train`矩阵分成*N*观测的批次。由于我们使用SGD，如果可能的话，最好对观测进行洗牌，因为该方法强烈依赖于输入向量的顺序。此时，每个小批量有*N*行和*M*列（其中*M*是特征的数量）。
- en: We train the learner using a mini-batch. SGD coefficients are initialized randomly,
    as shown previously.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用小批量来训练学习器。SGD系数的初始化是随机的，如前所述。
- en: We train the learner using another mini-batch. SGD coefficients are initialized
    as the output of the previous step (using the `partial_fit` method).
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用另一个小批量来训练学习器。SGD系数的初始化是前一步的输出（使用`partial_fit`方法）。
- en: Repeat step 3 until you've used all the mini-batches. In each step, the coefficients
    of the SGD model are refined and modified according to the input.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤3，直到使用完所有的小批量。在每一步中，根据输入，SGD模型的系数都会被细化并修改。
- en: This is clearly a smart approach, and it doesn't take too long to implement.
    You just need to set the initial values of each coefficient for every new batch
    and train the learner on the mini-batch.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然是一个明智的方法，并且实现起来不会花费太多时间。你只需要为每个新批量设置每个系数的初始值，并在小批量上训练学习器。
- en: Now, in terms of performance, what do we get using online learning?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从性能的角度来看，我们使用在线学习能得到什么？
- en: We have an incremental way to train the model. Since, at every step, we can
    test the model, we can stop at any point we think is good enough.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有一种增量训练模型的方法。由于在每一步我们都可以测试模型，我们可以在我们认为足够好的任何一点停止。
- en: We don't need to keep the whole `X_train` matrix in memory; we just need to
    keep the mini-batch in RAM. That also means that the consumed RAM is constant.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不需要将整个`X_train`矩阵保存在内存中；我们只需要将小批量保存在RAM中。这也意味着消耗的RAM是恒定的。
- en: 'We have a way to control the learning: we can have small mini-batches or big
    ones.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有一种控制学习的方法：我们可以有小的或大的小批量。
- en: 'Let''s see now how it performs, by changing the batch size (that is, the number
    of observations for each observation):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看它如何表现，通过改变批量大小（即每个观测的观测数）：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Online mini-batch learning](img/00117.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![在线小批量学习](img/00117.jpeg)'
- en: In the end, the final MAE is always the same; that is, batch learning and online
    learning eventually provide the same results when both are trained on the whole
    training set.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最终的MAE总是相同的；也就是说，当两者都在整个训练集上训练时，批量学习和在线学习最终会提供相同的结果。
- en: We also see that, by using a small mini-batch (1,000 observations), we have
    a working model after just 1 millisecond. Of course, it's not a perfect solution
    since its MAE is just 10^(-0.94), but still we now have a reasonable working model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到，通过使用小批量（1,000个观测），我们只需1毫秒就能得到一个工作模型。当然，这不是一个完美的解决方案，因为它的MAE仅为10^(-0.94)，但我们现在有一个合理的工作模型。
- en: Now, let's compare timings to fully train the model. Using mini-batches, the
    total time is around 1.2 seconds; using the batch it was greater than 5 seconds.
    The MAEs are more or less equal—why such a difference in timings? Because the
    dataset didn't all fit in RAM and the system kept on swapping data with storage
    memory.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们比较一下完全训练模型所需的时间。使用小批量，总时间大约为1.2秒；使用批量则超过5秒。MAEs 大致相等——为什么时间上有这么大的差异？因为数据集并没有全部适合RAM，系统一直在与存储内存交换数据。
- en: 'Let''s now focus on mini-batch size: is smaller really always better? Actually,
    it will produce an output earlier, but it will take more time in total.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们关注一下小批量大小：小批量是否总是更好？实际上，它将更早地产生输出，但总体上会花费更多时间。
- en: 'Here''s a plot of the training time and the MAE of the learner, when trained
    with different mini-batch sizes:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用不同小批量大小训练学习器时，训练时间和MAE的图表：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Online mini-batch learning](img/00118.jpeg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![在线小批量学习](img/00118.jpeg)'
- en: A real example
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个真实示例
- en: 'Let''s now combine feature hashing (seen in [Chapter 5](part0035_split_000.html#11C3M2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 5. Data Preparation"), *Data Preparation*), batch-learning, and SGD.
    From what we''ve seen so far, this should be the best way to deal with big data
    because:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们结合特征哈希（见[第5章](part0035_split_000.html#11C3M2-a2faae6898414df7b4ff4c9a487a20c6
    "第5章。数据准备")，*数据准备*)、批量学习和SGD。根据我们迄今为止所看到的，这应该是处理大数据的最佳方式，因为：
- en: The number of features is constant (feature hashing).
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征的数量是恒定的（特征哈希）。
- en: The number of observations per batch is constant (batch learning).
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每批次的观测数是恒定的（批量学习）。
- en: It allows streaming datasets.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它允许流式数据集。
- en: The algorithm is stochastic (SGD).
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法是随机的（SGD）。
- en: 'All these points together will ensure a few consequences:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些点加在一起将确保一些后果：
- en: We can very quickly have a model (after the first mini-batch) that is refined
    with time.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以非常快速地得到一个模型（在第一个小批量之后），该模型会随着时间的推移而改进。
- en: RAM consumption is constant (since every mini-batch has exactly the same size).
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RAM消耗是恒定的（因为每个小批量的大小完全相同）。
- en: Ideally, we can deal with as many observation as we want.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理想情况下，我们可以处理我们想要的任意多的观察值。
- en: 'In the real-world example, let''s use a textual input: the Twenty Newsgroups
    dataset. This dataset contains 20,000 messages (textual content) extracted from
    20 different newsgroups, each of them on a different topic. The webpage of the
    project is: [https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups](https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的例子中，让我们使用文本输入：二十个新闻组数据集。这个数据集包含从20个不同的新闻组中提取的20,000条消息（文本内容），每个新闻组都涉及不同的主题。项目的网页是：[https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups](https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups)。
- en: 'The goal is to classify each document in one of the possible labels (it''s
    a classification task). Let''s first load it, and split it into train and test.
    To make it more real, we''re going to remove headers, footers, and quoted e-mail
    from the dataset:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是将每个文档分类到可能的标签之一（这是一个分类任务）。让我们首先加载它，并将其分为训练集和测试集。为了使其更真实，我们将从数据集中删除标题、页脚和引用的电子邮件：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s now create a function that yields mini-batches of the dataset:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个函数，该函数可以生成数据集的小批量：
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, the core task is simply to classify the document. We first apply feature
    hashing via the `HashingVectorizer` class, whose output feeds a `SGDClassifier`
    (another class with the `partial_fit` method). This fact will ensure an additional
    advantage: since the output of the `HashingVectorizer` is very sparse, a sparse
    representation is used, making the mini-batch size even more compact in memory'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，核心任务仅仅是分类文档。我们首先通过`HashingVectorizer`类应用特征哈希，其输出连接到一个`SGDClassifier`（另一个具有`partial_fit`方法的类）。这一事实将确保一个额外的优势：由于`HashingVectorizer`的输出非常稀疏，因此使用稀疏表示，使得内存中的小批量大小更加紧凑。
- en: 'To understand what the best hash size is, we may try a full search with sizes
    of `1000`, `5000`, `10000`, `50000`, and `100000` and then measure the accuracy
    for each learner:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解最佳哈希大小是多少，我们可以尝试使用`1000`、`5000`、`10000`、`50000`和`100000`的大小进行全搜索，然后测量每个学习者的准确度：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we plot our results on a graph representing time and accuracy for
    each size of the hash size. The *X* signs on the graph are the instances (and
    related accuracy) when the classifier outputs a model:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在表示每个哈希大小的时间准确性的图表上绘制我们的结果。图表上的*X*符号是当分类器输出模型时的实例（和相关准确度）：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![A real example](img/00119.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![一个真实示例](img/00119.jpeg)'
- en: It appears from the obtained results that using a hash table bigger than 10,000
    elements can allow us to achieve the best performance. In this exercise, the mini-batch
    size was fixed to 1,000 observations; this means that every mini-batch was a matrix
    of 10 M elements, represented in sparse way. It also means that, for every mini-batch,
    the memory used is up to 80 MB of RAM.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 从获得的结果来看，使用大于10,000个元素的哈希表可以让我们达到最佳性能。在这个练习中，小批量大小被固定为1,000个观察值；这意味着每个小批量都是一个包含10
    M个元素的矩阵，以稀疏方式表示。这也意味着，对于每个小批量，使用的内存高达80 MB的RAM。
- en: Streaming scenario without a test set
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无测试集的流式场景
- en: 'In many real cases, the test dataset is not available. What can we do? The
    best practice would be to:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际情况下，测试数据集是不可用的。我们能做什么？最佳实践将是：
- en: Fetch the data until you reach a specific mini-batch size; let's say 10 observations.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 持续获取数据，直到达到特定的批量大小；比如说10个观察值。
- en: Shuffle the observations, and store eight of them within the train set, and
    two in the test set (for an 80/20 validation).
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打乱观察值，并将其中8个存储在训练集中，2个存储在测试集中（用于80/20的验证）。
- en: Train the classifier on the train set and test on the test set.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集上训练分类器，在测试集上测试。
- en: Go back to step 1\. With each mini-batch, the train set will increase by 10
    observations and the test set by 2.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回步骤1。随着每个小批量的到来，训练集将增加10个观察值，测试集增加2个。
- en: 'We have just described the classic method, used when data is consistent and
    the dataset is not very large. If the features change throughout the streaming,
    and you need to build a learner that has to adapt to rapid changes of feature
    statistics. then simply don''t use a test set and follow this algorithm. In addition,
    this is the preferred way to learn from big data:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚描述了经典方法，该方法在数据一致且数据集不是很大时使用。如果特征在整个流中发生变化，并且你需要构建一个必须适应特征统计快速变化的学习者，那么就简单地不要使用测试集，并遵循此算法。此外，这是从大数据中学习的首选方式：
- en: Fetch the data till you reach a mini-batch size; let's say 10 observations.
    Don't shuffle and train the learner with all the observations.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数据，直到达到小批量大小；比如说 10 个观察结果。不要打乱顺序，并使用所有观察结果来训练学习者。
- en: Wait till you fetch another mini-batch. Test the classifier on those observations.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等到你获取到另一个小批量数据。在这些观察结果上测试分类器。
- en: Update the classifier with the mini-batch you received in the previous step.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上一步中接收到的那个小批量更新分类器。
- en: Go back to step 2.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回到步骤 2。
- en: The good thing about this algorithm is that you don't have to keep anything
    but the model and the current mini-batch in memory; these are used first to test
    the learner, and then to update it.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法的好处是，你只需要在内存中保留模型和当前的小批量数据；这些首先用于测试学习者，然后用于更新它。
- en: Summary
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we've introduced the concepts of batch and online learning,
    which are necessary to be able to process big datasets (big data) in a quick and
    scalable way.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了批量和在线学习概念，这对于能够快速且可扩展地处理大数据集（大数据）是必要的。
- en: In the next chapter, we will explore some advanced techniques of machine learning
    that will produce great results for some classes of well-known problems.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨一些机器学习的先进技术，这些技术将为一些已知问题类别产生很好的结果。
