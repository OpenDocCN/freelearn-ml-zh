- en: Section 4 – Modeling Dichotomous and Multiclass Targets with Supervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4节 - 使用监督学习建模二分类和多类目标
- en: There are a good number of high performing algorithms for predicting categorical
    targets. We will examine the most popular classification algorithms in this part.
    We will also consider why we might choose one algorithm over any of the others
    given the attributes our data and our domain knowledge.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预测分类目标，有许多高性能算法。在本部分，我们将考察最流行的分类算法。我们还将考虑为什么根据我们的数据和领域知识，我们可能会选择一种算法而不是其他算法。
- en: We are as concerned with underfitting and overfitting with classification models
    as we were with regression models in the previous part. When the relationship
    between features and the target is complicated, we need to use an algorithm that
    can capture that complexity. But there is often a non-trivial risk of overfitting.
    We will discuss strategies for modeling complexity without overfitting in the
    chapters in this part. This usually involves some form of regularization for logistic
    regression models, limits on tree depth for decision trees, and adjusting the
    tolerance for margin violations with support vector classification.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对分类模型中的欠拟合和过拟合的关注程度与之前部分中对回归模型的关注程度一样。当特征与目标之间的关系复杂时，我们需要使用能够捕捉这种复杂性的算法。但往往存在非同小可的过拟合风险。在本部分章节中，我们将讨论建模复杂性而不出现过拟合的策略。这通常涉及对逻辑回归模型进行某种形式的正则化、对决策树的树深度进行限制，以及调整支持向量分类中边缘违规的容忍度。
- en: If we are trying to model complexity without overfitting we have to be prepared
    to spend a good chunk of time doing hyperparameter tuning. We will definitely
    spend a fair bit of time on that in these chapters. Related to that, we also get
    really good at cross validation and generating and interpreting evaluation metrics.
    We will discuss accuracy, precision, sensitivity, and specificity in each of the
    next five chapters. We will also get very used to staring at a confusion matrix.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们试图建模复杂性而不出现过拟合，我们必须准备好花大量时间进行超参数调整。在这些章节中，我们肯定会在这方面花费相当多的时间。与此相关，我们也会在交叉验证、生成和解释评估指标方面变得非常熟练。在接下来的五个章节中，我们将讨论准确率、精确度、敏感度和特异性。我们还将非常习惯于查看混淆矩阵。
- en: We will also examine how these algorithms can be extended to multiclass targets.
    This is straightforward with k-nearest neighbors and decision tress, but requires
    extension to the algorithm for logistic regression and support vector regression.
    We go over that in these chapters.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨如何将这些算法扩展到多类目标。对于k-最近邻和决策树来说，这是直截了当的，但对于逻辑回归和支持向量回归算法则需要扩展。这些内容将在本章中介绍。
- en: 'This section comprises the following chapters:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包括以下章节：
- en: '[*Chapter 10*](B17978_10_ePub.xhtml#_idTextAnchor126), *Logistic Regression*'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B17978_10_ePub.xhtml#_idTextAnchor126), *逻辑回归*'
- en: '[*Chapter 11*](B17978_11_ePub.xhtml#_idTextAnchor135), *Decision Trees and
    Random Forest Classification*'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B17978_11_ePub.xhtml#_idTextAnchor135), *决策树和随机森林分类*'
- en: '[*Chapter 12*](B17978_12_ePub.xhtml#_idTextAnchor144), *K-Nearest Neighbors
    for Classification*'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第12章*](B17978_12_ePub.xhtml#_idTextAnchor144), *K-最近邻分类*'
- en: '[*Chapter 13*](B17978_13_ePub.xhtml#_idTextAnchor152), *Support Vector Machine
    Classification*'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第13章*](B17978_13_ePub.xhtml#_idTextAnchor152), *支持向量机分类*'
- en: '[*Chapter 14*](B17978_14_ePub.xhtml#_idTextAnchor162), *Naive Bayes Classification*'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第14章*](B17978_14_ePub.xhtml#_idTextAnchor162), *朴素贝叶斯分类*'
