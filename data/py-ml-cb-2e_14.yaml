- en: Unsupervised Representation Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督表示学习
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下食谱：
- en: Using denoising autoencoders to detect fraudulent transactions
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用降噪自编码器检测欺诈交易
- en: Generating word embeddings using CBOW or skipgram representations
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CBOW或skipgram表示生成词嵌入
- en: Visualizing the MNIST dataset using PCA and t-SNE
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PCA和t-SNE可视化MNIST数据集
- en: Using word vectors for Twitter sentiment analysis
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用词向量进行Twitter情感分析
- en: Implementing LDA with scikit-learn
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-learn实现LDA
- en: Using LDA to classify text documents
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LDA对文本文档进行分类
- en: Preparing data for LDA
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为LDA准备数据
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To address the recipes in this chapter, you need the following files (available
    on GitHub):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理本章中的食谱，你需要以下文件（可在GitHub上找到）：
- en: '`CreditCardFraud.py`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CreditCardFraud.py`'
- en: '`creditcard.csv`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`creditcard.csv`'
- en: '`WordEmbeddings.py`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WordEmbeddings.py`'
- en: '`MnistTSNE.py`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MnistTSNE.py`'
- en: '`TweetEmbeddings.py`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TweetEmbeddings.py`'
- en: '`Tweets.csv`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tweets.csv`'
- en: '`LDA.py`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LDA.py`'
- en: '`TopicModellingLDA.py`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TopicModellingLDA.py`'
- en: '`PrepDataLDA.py`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PrepDataLDA.py`'
- en: Introduction
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: In [Chapter 4](74eaa4cc-b9da-4cf4-9041-3ce42f998499.xhtml), *Clustering with
    Unsupervised Learning*, we have already addressed unsupervised learning. We said
    that unsupervised learning is a paradigm in machine learning where we build models
    without relying on labeled training data. Why return to this topic? In this case,
    we will discuss the problem of learning representations for data such as images,
    video, and the corpus of natural language, in an unsupervised way.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](74eaa4cc-b9da-4cf4-9041-3ce42f998499.xhtml)《无监督学习聚类》中，我们已经讨论了无监督学习。我们说无监督学习是机器学习中的一个范例，我们在其中构建模型而不依赖于标记的训练数据。为什么回到这个话题？在这种情况下，我们将以无监督的方式讨论学习图像、视频和自然语言语料库等数据的表示问题。
- en: Using denoising autoencoders to detect fraudulent transactions
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用降噪自编码器检测欺诈交易
- en: In [Chapter 4](74eaa4cc-b9da-4cf4-9041-3ce42f998499.xhtml), *Clustering with
    Unsupervised Learning*, we dealt with the topic of **autoencoders**. In the *Autoencoders
    to reconstruct handwritten digit images* recipe, there is a neural network whose
    purpose is to code its input into small dimensions, and the result obtained, to
    be able to reconstruct the input itself. The purpose of autoencoders is not simply
    to perform a sort of compression of the input or look for an approximation of
    the identity function; there are also techniques that allow us to direct the model
    (starting from a hidden layer of reduced dimensions) to give greater importance
    to some data properties.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](74eaa4cc-b9da-4cf4-9041-3ce42f998499.xhtml)《无监督学习聚类》中，我们处理了**自编码器**的主题。在*自编码器重建手写数字图像*食谱中，有一个神经网络，其目的是将其输入编码为小维度，并得到的结果能够重建输入本身。自编码器的目的不仅仅是执行一种输入的压缩或寻找身份函数的近似；还有一些技术可以让我们将模型（从减少维度的隐藏层开始）引导到给予某些数据属性更多的重要性。
- en: Getting ready
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this recipe, we will train an autoencoder in unsupervised mode to detect
    anomalies in credit card transaction data. To do this, the credit card fraud detection
    dataset will be used. This is a dataset containing the anonymized credit card
    transactions labeled as fraudulent or genuine. Transactions made by credit cards
    in September 2013 by European cardholders are listed. This dataset presents 492
    transactions labeled as frauds out of 284,807 transactions. The dataset is highly
    unbalanced, as the positive class (frauds) accounts for 0.172% of all transactions.
    The dataset is available on Kabble at the following URL: [https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将以无监督模式训练一个自编码器来检测信用卡交易数据中的异常。为此，将使用信用卡欺诈检测数据集。这是一个包含标记为欺诈或真实的匿名信用卡交易的数据集。列出了2013年9月欧洲持卡人用信用卡进行的交易。该数据集中有492笔交易被标记为欺诈，占284,807笔交易中的0.172%。该数据集高度不平衡，因为正类（欺诈）占所有交易的0.172%。该数据集可在以下URL的Kabble上找到：[https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud)。
- en: How to do it…
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'Let''s see how to use denoising autoencoders to detect fraudulent transactions:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用降噪自编码器来检测欺诈交易：
- en: 'Create a new Python file, and import the following packages (the full code
    is in the `CreditCardFraud.py` file that''s already provided to you):'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件，并导入以下包（完整的代码在已经提供给你的`CreditCardFraud.py`文件中）：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To make the experiment reproducible, in the sense that it provides the same
    results with each reproduction, it is necessary to set the seed:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使实验可重复，即在每次重复时提供相同的结果，有必要设置种子：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As already said, we will use the credit card fraud detection dataset (`creditcard.csv`)
    that is already provided to you:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，我们将使用已经提供给你的信用卡欺诈检测数据集（`creditcard.csv`）：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s count the occurrences of the two classes (`fraud`= `1`; `normal`=`0`):'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们计算两个类别（`fraud`= `1`；`normal`=`0`）的出现次数：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following results are returned:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As anticipated, the dataset is highly unbalanced—the positive class (`frauds`)
    is `492` out of `284315`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，数据集高度不平衡——正类（`frauds`）有`492`个，而总共有`284315`个。
- en: 'Among the available variables, the amount of the transactions (`Amount`) is
    the most interesting one. Let''s calculate some statistics:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在可用的变量中，交易金额（`Amount`）是最有趣的一个。让我们计算一些统计数据：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following results are returned:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As we can see, the values are very different with a high standard deviation.
    It is advisable to perform a scaling of the data. Remember, it is a good practice
    to rescale the data before training a machine learning algorithm. With rescaling,
    data units are eliminated, allowing you to compare data from different locations
    easily. To do this, we will use the `sklearn` `StandardScaler()` function. This
    function removes the mean and scales the values to unit variance:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如我们所见，这些值差异很大，标准差很高。建议对数据进行缩放。记住，在训练机器学习算法之前重新缩放数据是一个好的实践。通过缩放，消除了数据单位，使得你可以轻松地比较来自不同位置的数据。为此，我们将使用`sklearn`的`StandardScaler()`函数。此函数移除了平均值并将值缩放到单位方差：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following results are returned:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We have thus confirmed that now the data has `mean=0` and unit variance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经确认现在数据具有`mean=0`和单位方差。
- en: 'Now, we split the starting data into two sets: the training set (70%) and test
    set (30%). The training set will be used to train a classification model, and
    the test set will be used to test the model''s performance:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将起始数据分为两个集合：训练集（70%）和测试集（30%）。训练集将用于训练分类模型，测试集将用于测试模型性能：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can build the Keras model as follows:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以按照以下方式构建Keras模型：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following shows the model architecture:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了模型架构：
- en: '![](img/655679f4-bb24-407d-b351-19b51b67eae3.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/655679f4-bb24-407d-b351-19b51b67eae3.png)'
- en: 'So, we have to configure the model for training. To do this, we will use the
    `compile()` method, as follows:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们必须为训练配置模型。为此，我们将使用`compile()`方法，如下所示：
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'At this point, we can train the model:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到这一点，我们可以训练模型：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we can plot the loss history to evaluate the model convergence:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以绘制损失历史记录来评估模型收敛性：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'At this point, we use the model to reconstruct the result of the transactions:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到这一点，我们使用模型来重建交易的预测结果：
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To evaluate the quality of the prediction, we used the **mean squared error**
    (**MSE**) loss function. MSE measures the average of the squares of the errors—that
    is, the average squared difference between the estimated values and what is estimated.
    MSE is a measure of the quality of an estimator—it is always non-negative, and
    has values close to zero. So, we calculated some statistics related to the error
    and the real values. The following results were obtained:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估预测的质量，我们使用了**均方误差**（**MSE**）损失函数。MSE衡量误差平方的平均值——即估计值与实际估计值之间的平均平方差。MSE是估计质量的一个度量——它总是非负的，并且值接近零。因此，我们计算了一些与误差和真实值相关的统计数据。以下结果如下：
- en: '![](img/c90abb89-fcb9-4bbb-b4e0-b67aee64c4d2.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c90abb89-fcb9-4bbb-b4e0-b67aee64c4d2.png)'
- en: 'Now, we can compare the results of the classification to the actual values.
    The best way to do this is to use a **confusion matrix**. In a confusion matrix,
    we compare our results to real data. What''s good about a confusion matrix is
    that it identifies the nature of the classification errors, as well as their quantities.
    In this matrix, the diagonal cells show the number of cases that were correctly
    classified; all the other cells show the misclassified cases. To calculate the
    confusion matrix, we can use the `confusion_matrix()` function that''s contained
    in the `sklearn.metrics` package as follows:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以比较分类结果与实际值。最好的方法是使用**混淆矩阵**。在混淆矩阵中，我们比较我们的结果与真实数据。混淆矩阵的好处是它确定了分类错误的性质以及它们的数量。在这个矩阵中，对角线单元格显示了正确分类的案例数量；所有其他单元格显示了错误分类的案例。为了计算混淆矩阵，我们可以使用包含在`sklearn.metrics`包中的`confusion_matrix()`函数，如下所示：
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following confusion matrix is returned:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下混淆矩阵：
- en: '[PRE16]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we will calculate the accuracy of the model:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将计算模型的准确度：
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following accuracy is obtained:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为获得的准确度：
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The result looks great, but unfortunately the input dataset is highly unbalanced.
    If we evaluate the accuracy of fraudulent transactions only, this data is significantly
    reduced.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来很棒，但不幸的是，输入数据集高度不平衡。如果我们只评估欺诈交易的准确度，这些数据将显著减少。
- en: How it works...
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'There are different types of autoencoders available:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的自动编码器类型不同：
- en: '**Vanilla autoencoder**: It is the simplest form, characterized by a three-layer
    network, that is, a neural network with only one hidden layer. Input and output
    are the same.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**香草自动编码器**：这是最简单的形式，其特征是一个三层网络，即只有一个隐藏层的神经网络。输入和输出是相同的。'
- en: '**Multilayer autoencoder**: If only one hidden layer is not enough, we can
    extend the autoencoder along the depth dimension. For example, three hidden layers
    are used, for better generalization, but we will also have to make the symmetric
    network using the intermediate layer.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多层自动编码器**：如果只有一个隐藏层不够，我们可以沿着深度维度扩展自动编码器。例如，使用三个隐藏层以获得更好的泛化能力，但我们也必须使用中间层构建对称网络。'
- en: '**Convolutional autoencoder**: Three-dimensional vectors are used instead of
    one-dimensional vectors. The input image is sampled to obtain a latent representation,
    that is, a dimensional reduction, thus forcing the autoencoder to learn from a
    compressed version of the image.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积自动编码器**：使用三维向量而不是一维向量。输入图像被采样以获得潜在表示，即降维，从而迫使自动编码器从图像的压缩版本中学习。'
- en: '**Regularized autoencoder**: Rather than limiting the model''s capacity by
    maintaining a shallow encoder and decoder architecture, as well as a forced reduction,
    regularized autoencoders use a loss function to encourage the model to assume
    properties that go beyond the simple ability to copy the input to the output.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化自动编码器**：正则化自动编码器不是通过维持浅层的编码器和解码器架构以及强制减少来限制模型的能力，而是使用损失函数来鼓励模型假设一些超出简单复制输入到输出的能力的属性。'
- en: There's more…
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容...
- en: 'In practice, we find two different types:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们发现有两种不同类型：
- en: '**Sparse autoencoder**: This is usually used for classification. By training
    an autoencoder, the hidden units in the middle layer are activated too frequently.
    To avoid this, we need to lower their activation rate by limiting it to a fraction
    of the training data. This constraint is called a **sparsity constraint**, as
    each unit is activated only by a pre-defined type of input.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏自动编码器**：这通常用于分类。通过训练自动编码器，中间层的隐藏单元被激活得太频繁。为了避免这种情况，我们需要通过将其限制为训练数据的一部分来降低它们的激活率。这种约束称为**稀疏性约束**，因为每个单元只被预定义类型的一种输入激活。'
- en: '**Denoising autoencoder**: Rather than adding a penalty to the `loss` function,
    we can make the object change, adding noise to the input image and making the
    autoencoder learn to remove it autonomously. This means that the network will
    extract only the most relevant information, and learn from a robust representation
    of the data.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去噪自动编码器**：我们不是对损失函数添加惩罚，而是让对象发生变化，向输入图像添加噪声，并让自动编码器自主地学习去除它。这意味着网络将只提取最相关的信息，并从数据的一个鲁棒表示中学习。'
- en: See also
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考内容
- en: Refer to the official documentation of the Keras library: [https://keras.io/](https://keras.io/)
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考Keras库的官方文档：[https://keras.io/](https://keras.io/)
- en: 'Refer to *Autoencoders* (from Stanford University): [http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考来自斯坦福大学的*自动编码器*教程：[http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/)
- en: Generating word embeddings using CBOW and skipgram representations
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CBOW和skipgram表示生成词嵌入
- en: In [Chapter 7](fc31e304-3301-4ebf-80e4-404ac6e26606.xhtml), *Analyzing Text
    Data,* we already dealt with this topic. In the *Word2Vec using gensim* recipe,
    we used the `gensim` library to build a word2vec model. Now, we will deepen the
    topic. **Word embedding** allows the computer to memorize both semantic and syntactic
    information of words starting from an unknown corpus and constructs a vector space
    in which the vectors of words are closer if the words occur in the same linguistic
    contexts, that is, if they are recognized as semantically more similar. **Word2vec** is
    a set of templates that are used to produce word embedding.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](fc31e304-3301-4ebf-80e4-404ac6e26606.xhtml)“分析文本数据”中，我们已经处理了这个主题。在*使用gensim的Word2Vec*配方中，我们使用了`gensim`库来构建word2vec模型。现在，我们将深入探讨这个主题。**词嵌入**允许计算机从未知语料库开始记住单词的语义和句法信息，并构建一个向量空间，其中单词的向量如果它们在相同的语言环境中出现，即如果它们被认为是语义上更相似的话，则彼此更接近。**Word2vec**是一组模板，用于生成词嵌入。
- en: Getting ready
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this recipe, we will use the `gensim` library to generate word embeddings.
    We will also analyze two techniques to do this: CBOW and skipgram representations.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将使用`gensim`库生成词嵌入。我们还将分析两种实现此目的的技术：CBOW和skip gram表示。
- en: How to do it…
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s see how to generate word embeddings using CBOW and skipgram representations:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用CBOW和skip gram表示来生成词嵌入：
- en: 'Create a new Python file, and import the following packages (the full code
    is in the `WordEmbeddings.py` file that''s already provided to you):'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件，并导入以下包（完整代码在已提供的`WordEmbeddings.py`文件中）：
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s define the training data:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义训练数据：
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we can train the first model:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以训练第一个模型：
- en: '[PRE21]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Three arguments are used:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用了三个参数：
- en: '`sentences`: The training data'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sentences`: 训练数据'
- en: '`min_count=1`: The minimum count of words to consider when training the model'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_count=1`: 训练模型时考虑的单词的最小计数'
- en: '`sg=0`: The training algorithm, CBOW (0) or skip gram (1)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sg=0`: 训练算法，CBOW（0）或skip gram（1）'
- en: 'Let''s print a summary of the model:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印出模型的摘要：
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following result is returned:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s list and then print a summary of the vocabulary:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们列出并打印词汇表的摘要：
- en: '[PRE24]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following results are returned:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE25]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, we will access the vector for one word (`book`):'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将访问一个单词的向量（`book`）：
- en: '[PRE26]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following results are returned:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '![](img/a70d17e4-d07f-41ec-beac-86b4b5208671.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a70d17e4-d07f-41ec-beac-86b4b5208671.png)'
- en: 'To use the `skipgram` algorithm, we have to perform a similar procedure except
    we set the argument `sg=1`, as follows:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用`skipgram`算法，我们需要执行类似的步骤，但我们将参数`sg`设置为1，如下所示：
- en: '[PRE27]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: How it works...
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Word2vec uses **continuous bag-of-words** (**CBOW**) and skipgram to word embeddings.
    In the CBOW algorithm, the model predicts the current word from a window of surrounding
    context words. Context word order does not influence prediction. In the skipgram
    algorithm, the model uses the current word to predict the surrounding window of
    context words.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec使用**连续词袋**（**CBOW**）和skip gram进行词嵌入。在CBOW算法中，模型从周围上下文词的窗口中预测当前词。上下文词的顺序不影响预测。在skip
    gram算法中，模型使用当前词来预测周围窗口的上下文词。
- en: There's more…
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: According to authors, CBOW is and skip-gram is but does a better job for infrequent
    words.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 根据作者的说法，CBOW和skip-gram都是好的，但对于不常见的单词来说，skip-gram做得更好。
- en: See also
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: Refer to the official documentation of the `gensim` library: [https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考官方的`gensim`库文档：[https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)
- en: 'Refer to *Efficient Estimation of Word Representations in Vector Space* (by
    Tomas Mikolov and others): [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考由Tomas Mikolov等人撰写的《在向量空间中高效估计词表示》([https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781))
- en: Visualizing the MNIST dataset using PCA and t-SNE
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PCA和t-SNE可视化MNIST数据集
- en: In the case of datasets of important dimensions, the data is previously transformed
    into a reduced series of representation functions. This process of transforming
    the input data into a set of functionalities is called **features extraction**.
    This is because the extraction of the characteristics proceeds from an initial
    series of measured data and produces derived values that can keep the information
    contained in the original dataset, but discharged from the redundant data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在重要维度的数据集中，数据先前被转换成一系列表示函数的简化序列。将输入数据转换成一组功能的过程称为**特征提取**。这是因为特征提取是从一系列初始测量数据开始的，并产生导出的值，这些值可以保留原始数据集中的信息，但释放了冗余数据。
- en: In this way, the subsequent learning and generalization phases will be facilitated
    and, in some cases, this will lead to better interpretations. It is a process
    of extracting new features from the original features, thereby reducing the cost
    of feature measurement, which boosts classifier efficiency. If the features are
    carefully chosen, it is assumed that the features set will run the desired task
    with the reduced representation, instead of the full-sized input.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，后续的学习和泛化阶段将得到简化，在某些情况下，这会导致更好的解释。这是一个从原始特征中提取新特征的过程，从而降低了特征测量的成本，提高了分类器的效率。如果特征选择得当，假设特征集将以减少的表示形式运行所需的任务，而不是全尺寸的输入。
- en: Getting ready
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will use **principal component analysis** (**PCA**) and **t-distributed
    Stochastic Neighbor Embedding methods** (**t-SNE**) to perform a feature extraction
    procedure. In this way, we will be able to visualize how the different elements
    of a very large dataset, such as MNIST, are grouped together.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用**主成分分析**（**PCA**）和**t-分布随机邻域嵌入方法**（**t-SNE**）来执行特征提取过程。这样，我们将能够可视化一个非常大的数据集（如MNIST）的不同元素是如何组合在一起的。
- en: How to do it…
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Let''s see how to visualize the MNIST dataset using PCA and t-SNE:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用PCA和t-SNE可视化MNIST数据集：
- en: 'Create a new Python file, and import the following packages (the full code
    is in the `MnistTSNE.py` file that''s already provided to you):'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件，并导入以下包（完整代码在已提供的`MnistTSNE.py`文件中）：
- en: '[PRE28]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'To import the `mnist` dataset, the following code must be used:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要导入`mnist`数据集，必须使用以下代码：
- en: '[PRE29]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following tuples are returned:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下元组：
- en: '`XTrain`, `XTest`: A `uint8` array of grayscale image data with the (`num_samples`,
    28, 28) shape'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`XTrain`，`XTest`：一个形状为（`num_samples`，28，28）的灰度图像数据的`uint8`数组'
- en: '`YTrain`, `YTest`: A `uint8` array of digit labels (integers in the range 0-9)
    with the (`num_samples`) shape'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`YTrain`，`YTest`：一个形状为（`num_samples`）的数字标签（0-9范围内的整数）的`uint8`数组'
- en: 'To reduce the dimensionality, we will flatten the 28 x 28 images into vectors
    of size 784:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了降低维度，我们将28 x 28的图像展平成大小为784的向量：
- en: '[PRE30]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We extract only a part of the data from this large dataset to obtain a better
    visualization (only 1,000 records):'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只从这个大数据集中提取一部分数据以获得更好的可视化（仅1,000条记录）：
- en: '[PRE31]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let''s perform a `pca` analysis:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们进行`pca`分析：
- en: '[PRE32]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We display the data available in the new plan:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们显示新计划中可用的数据：
- en: '[PRE33]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The following results are returned:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '![](img/bf01e21e-ec24-414e-83b7-51ec58641e64.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf01e21e-ec24-414e-83b7-51ec58641e64.png)'
- en: 'At this point, we will repeat the procedure using the t-SNE method:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，我们将使用t-SNE方法重复该过程：
- en: '[PRE34]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We display the data available in the new plan:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们显示新计划中可用的数据：
- en: '[PRE35]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following results are returned:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '![](img/fff99ad2-2180-4e93-bf40-c924903a3664.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fff99ad2-2180-4e93-bf40-c924903a3664.png)'
- en: Comparing the two results obtained, it is clear that the second method allows
    us to identify the groups representing the different digits in more detail.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 比较获得的两个结果，很明显，第二种方法使我们能够更详细地识别代表不同数字的组。
- en: How it works...
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: PCA creates a new set of variables that are principal components. Each main
    component is a linear combination of the original variables. All principal components
    are orthogonal to one another, so there is no redundant information. The principal
    components as a whole constitute an orthogonal basis for the data space. The goal
    of PCA is to explain the maximum amount of variance with the least number of principal
    components. PCA is a form of multidimensional scaling. It transforms variables
    into a lower-dimensional space that retains the maximum details regarding the
    variables. A principal component is therefore a combination of the original variables
    after a linear transformation.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 创建了一组新的变量，即主成分。每个主成分是原始变量的线性组合。所有主成分彼此正交，因此没有冗余信息。整体而言，主成分构成了数据空间的正交基。PCA
    的目标是使用最少的几个主成分来解释最大的方差。PCA 是一种多维缩放技术。它将变量转换到一个低维空间，保留变量最多的细节。因此，主成分是原始变量经过线性变换后的组合。
- en: t-SNE is a dimensionality reduction algorithm developed by Geoffrey Hinton and
    Laurens van der Maaten, widely used as an automatic learning tool in many research
    fields. It is a non-linear dimension reduction technique that lends itself particularly
    to the embedding of high-dimensional datasets in a two- or three-dimensional space,
    in which they can be visualized by means of a dispersion plot. The algorithm models
    the points so that nearby objects in the original space are close together with
    reduced dimensionality, and distant objects are far away, trying to preserve the
    local structure.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE 是由 Geoffrey Hinton 和 Laurens van der Maaten 开发的一种降维算法，在许多研究领域被广泛用作自动学习工具。它是一种非线性降维技术，特别适合将高维数据集嵌入到二维或三维空间中，通过散点图进行可视化。该算法对点进行建模，使得原始空间中邻近的对象在降维后彼此靠近，而远离的对象则相隔较远，试图保留局部结构。
- en: There's more…
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容...
- en: A t-SNE algorithm is divided into two main phases. In the first phase, a probability
    distribution is constructed so that each pair of points in the original high-dimensional
    space associates a high probability value if the two points are similar, and low
    if they are dissimilar. Then, a second analogous probability distribution is defined
    in the small-sized space. The algorithm then minimizes the divergence of Kullback–Leibler
    of the two distributions by descending the gradient, reorganizing the points in
    the small-sized space.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE 算法分为两个主要阶段。在第一阶段，构建一个概率分布，使得原始高维空间中的每对点，如果两点相似，则赋予高概率值，如果不相似，则赋予低概率值。然后，在小型空间中定义第二个类似的概率分布。算法随后通过下降梯度，最小化两个分布的
    Kullback-Leibler 散度，重新组织小型空间中的点。
- en: See also
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: 'Refer to the official documentation of the `sklearn.decomposition.PCA` function:
    [https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考官方文档中的 `sklearn.decomposition.PCA` 函数：[https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)
- en: 'Official documentation of the `sklearn.manifold.TSNE` function: [https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.htm](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn.manifold.TSNE` 函数的官方文档：[https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.htm](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)'
- en: Using word embedding for Twitter sentiment analysis
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用词嵌入进行 Twitter 情感分析
- en: In [Chapter 7](fc31e304-3301-4ebf-80e4-404ac6e26606.xhtml), *Analyzing Text
    Data*, we have already dealt with sentiment analysis. In *Analyzing the sentiment
    of a sentence* recipe, we have analyzed the sentiment of a sentence using a Naive
    Bayes classifier starting from the data contained in the `movie_reviews` corpus.
    On that occasion, we said that sentiment analysis is one of the most popular applications
    of NLP. *Sentiment analysis* refers to the process of determining whether a given
    piece of text is positive or negative. In some variations, we consider "neutral"
    as a third option.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 7 章](fc31e304-3301-4ebf-80e4-404ac6e26606.xhtml)，*分析文本数据* 中，我们已经处理了情感分析。在
    *分析句子情感* 的配方中，我们使用 `movie_reviews` 语料库中的数据，通过朴素贝叶斯分类器分析了句子的情感。当时，我们说情感分析是 NLP
    最受欢迎的应用之一。*情感分析* 指的是确定给定文本片段是正面还是负面的过程。在某些变体中，我们将 "中性" 作为第三个选项。
- en: Getting ready
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will use the word embedding method to analyze the sentiment
    of Twitter posts by customers of some US airlines. The Twitter data was classified
    based on the opinions of some contributors. They were asked to first classify
    positive, negative, and neutral tweets, followed by categorizing negative ones.
    The dataset is available at the following link: [https://www.kaggle.com/crowdflower/twitter-airline-sentiment](https://www.kaggle.com/crowdflower/twitter-airline-sentiment).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用词嵌入方法来分析一些美国航空公司的客户Twitter帖子的情感。Twitter数据是根据一些贡献者的意见进行分类的。他们首先被要求对正面、负面和中性的帖子进行分类，然后对负面帖子进行分类。数据集可在以下链接获取：[https://www.kaggle.com/crowdflower/twitter-airline-sentiment](https://www.kaggle.com/crowdflower/twitter-airline-sentiment)。
- en: How to do it…
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'Let''s see how to use word embedding for Twitter sentiment analysis:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用词嵌入进行Twitter情感分析：
- en: 'Create a new Python file, and import the following packages (the full code
    is in the `TweetEmbeddings.py` file that''s already provided to you):'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件，并导入以下包（完整的代码在您已提供的`TweetEmbeddings.py`文件中）：
- en: '[PRE36]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To import the `Tweets` dataset (the `Tweets.csv` file that''s already provided
    to you), the following code must be used:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要导入`Tweets`数据集（您已提供的`Tweets.csv`文件），必须使用以下代码：
- en: '[PRE37]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Only two columns are extracted:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 只提取了两列：
- en: '`text`: Twitter posts'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text`：Twitter帖子'
- en: '`airline_sentiment`: Positive, neutral, or negative classification'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`airline_sentiment`：正面、中性或负面的分类'
- en: 'Now, we split the starting data into two sets: the training set (70%) and test
    set (30%). The training set will be used to train a classification model, and
    the test set will be used to test the model''s performance:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将起始数据分为两个集合：训练集（70%）和测试集（30%）。训练集将用于训练分类模型，测试集将用于测试模型性能：
- en: '[PRE38]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now, we will convert words to numbers:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将单词转换为数字：
- en: '[PRE39]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: To do this and tokenize the `XTrain` dataset `Tokenizer`, the `fit_on_texts` and
    `texts_to_sequences` methods were used.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点并对`XTrain`数据集进行分词，使用了`fit_on_texts`和`texts_to_sequences`方法。
- en: 'To transform the input data into a format compatible with Keras, `pad_sequences`
    models will be used. This method transforms a list of sequences (lists of scalars)
    into a 2D NumPy array, as follows:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将输入数据转换为与Keras兼容的格式，将使用`pad_sequences`模型。该方法将序列（标量列表）转换为二维NumPy数组，如下所示：
- en: '[PRE40]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'So, we will convert the target classes to numbers:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们将目标类别转换为数字：
- en: '[PRE41]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, we will build the Keras model:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将构建Keras模型：
- en: '[PRE42]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The embedding layer takes as input a 2D tensor with shape (`batch_size`, `sequence_length`),
    where each entry is a sequence of integers. A 3D tensor with the shape (`batch_size`,
    `sequence_length`, and `output_dim`) is returned.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层接受一个形状为（`batch_size`，`sequence_length`）的二维张量作为输入，其中每个条目都是一个整数序列。返回一个形状为（`batch_size`，`sequence_length`，`output_dim`）的三维张量。
- en: 'Now, we will compile and fit the model created:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将编译和拟合创建的模型：
- en: '[PRE43]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'To evaluate the model''s performance, let''s print the accuracy:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了评估模型性能，让我们打印准确率：
- en: '[PRE44]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The following results are returned:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE45]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Finally, we will plot the model history:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将绘制模型历史：
- en: '[PRE46]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The following diagram is returned:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下图表：
- en: '![](img/cd725421-9e17-46be-b82d-68c280a9048e.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd725421-9e17-46be-b82d-68c280a9048e.png)'
- en: Analyzing the progress of the validation loss, we realize that the model is
    overfitting. To handle orverfitting, as we learned in the *Building a ridge regressor*
    recipe in [Chapter 1](f552bbc7-5e56-41b8-8e8d-915cc1bd53ab.xhtml), *The Realm
    of Supervised Learning*, we need to use a regularization method.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 分析验证损失的进展，我们发现模型过拟合了。为了处理过拟合，正如我们在第1章中学习到的*Building a ridge regressor*菜谱中，*The
    Realm of Supervised Learning*，我们需要使用正则化方法。
- en: How it works...
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The term **sentiment analysis** refers to the use of NLP techniques, text analysis,
    and computational linguistics to identify and extract subjective information in
    written or spoken text sources. Sentiment analysis can be tackled through different
    approaches. The most commonly used can be grouped into four macro-categories (*A
    Study and Comparison of* *Sentiment Analysis Methods for Reputation Evaluation*
    by Collomb A, Costea C, Joyeux D, Hasan O, and Brunie L, 2014):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 术语**情感分析**指的是使用自然语言处理技术、文本分析和计算语言学来识别和提取书面或口头文本来源中的主观信息。情感分析可以通过不同的方法来解决。最常用的方法可以分为四个宏观类别（Collomb
    A, Costea C, Joyeux D, Hasan O, 和 Brunie L 的2014年论文《*A Study and Comparison of
    Sentiment Analysis Methods for Reputation Evaluation*》）：
- en: '**Lexicon-based methods**: These detect emotional keywords, and assign arbitrary
    words affinity likely to represent particular emotions.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于词典的方法**：这些方法检测情感关键词，并为可能代表特定情感的任意单词分配亲和力。'
- en: '**Rule-based methods**: These classify texts using emotional categories, based
    on the presence of unambiguous emotional words, such as *happy*, *sad*, and *bored*.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于规则的方法**：这些方法根据情感词（如*快乐*、*悲伤*和*无聊*）的存在使用情感类别对文本进行分类。'
- en: '**Statistical methods**: Here, we try to identify the owner of a sentiment,
    that is, who the subject is, and the objective, or the object to which the sentiment
    is felt. To measure opinion in context and find the characteristic that was judged,
    we check the grammatical relations between the words in the text. This is obtained
    through a thorough scan of the text.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统计方法**：在这里，我们试图识别情感的所有者，即主体是谁，以及目标，即情感所感受到的对象。为了在上下文中衡量观点并找到被评判的特征，我们检查文本中单词之间的语法关系。这是通过对文本进行彻底扫描获得的。'
- en: '**Machine learning methods**: These use different learning algorithms to determine
    sentiment by having a dataset classified (supervised methods). The learning process
    is not immediate; in fact, models have to be built that associate a polarity to
    different types of comments and, if necessary, a topic for analysis purposes.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习方法**：这些方法使用不同的学习算法通过使数据集分类（监督方法）来确定情感。学习过程不是即时的；事实上，必须构建将极性关联到不同类型评论的模型，以及必要时用于分析目的的主题。'
- en: There's more…
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: The `regularization` methods involve modifying the performance function, normally
    selected as the sum of the squares of regression errors on the training set. When
    a large number of variables are available, the least square estimates of a linear
    model often have a low bias but a high variance with respect to models with fewer
    variables. Under these conditions, there is an overfitting problem. To improve
    precision prediction by allowing greater bias but a small variance, we can use
    variable selection methods and dimensionality reduction, but these methods may
    be unattractive for computational burdens in the first case, or provide a difficult
    interpretation in the other case.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化**方法涉及修改性能函数，通常选择为训练集上回归误差平方和。当有大量变量可用时，线性模型的平方估计通常具有较低的偏差，但相对于变量较少的模型具有较高的方差。在这些条件下，存在过拟合问题。为了通过允许更大的偏差但较小的方差来提高预测精度，我们可以使用变量选择方法和降维，但这些方法在第一种情况下可能因计算负担而不吸引人，或者在另一种情况下可能难以解释。'
- en: See also
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关内容
- en: Refer to the official documentation of the Keras library: [https://keras.io/](https://keras.io/)
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考Keras库的官方文档：[https://keras.io/](https://keras.io/)
- en: Refer to* Sentiment Analysis* (from Stanford University): [https://web.stanford.edu/class/cs124/lec/sentiment.pdf](https://web.stanford.edu/class/cs124/lec/sentiment.pdf)
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考斯坦福大学的*情感分析*（[https://web.stanford.edu/class/cs124/lec/sentiment.pdf](https://web.stanford.edu/class/cs124/lec/sentiment.pdf)）
- en: Implementing LDA with scikit-learn
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn实现LDA
- en: '**Latent Dirichlet allocation** (**LDA**) is a generative model, used in the
    study of natural language, which allows you to extract arguments from a set of
    source documents and provide a logical explanation on the similarity of individual
    parts of documents. Each document is considered as a set of words that, when combined,
    form one or more subsets of latent topics. Each topic is characterized by a particular
    distribution of terms.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）是一种生成模型，用于自然语言研究，它允许您从一组源文档中提取论点，并对文档各个部分的相似性提供逻辑解释。每个文档被视为一组单词，当它们结合在一起时，形成一个或多个潜在主题的子集。每个主题由特定术语分布的特征所表征。'
- en: Getting ready
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will use the `sklearn.decomposition.LatentDirichletAllocation`
    function to produce a feature matrix of token counts, similar to what the `CountVectorizer`
    function (just used in the *Building a bag-of-words model* recipe of [Chapter
    7](fc31e304-3301-4ebf-80e4-404ac6e26606.xhtml), *Analyzing Text Data*) would produce
    on the text.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用`sklearn.decomposition.LatentDirichletAllocation`函数生成一个标记计数特征矩阵，类似于`CountVectorizer`函数（在[第7章](fc31e304-3301-4ebf-80e4-404ac6e26606.xhtml)的*构建词袋模型*菜谱中已使用）在文本上产生的结果。
- en: How to do it…
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Let''s see how to implement LDA with scikit-learn:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用scikit-learn实现LDA：
- en: 'Create a new Python file, and import the following packages (the full code
    is in the `LDA.py` file that''s already provided to you):'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件，并导入以下包（完整代码在已提供的`LDA.py`文件中）：
- en: '[PRE47]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'To produce the input data, we will use the `sklearn.datasets.make_multilabel_classification`
    function. This function generates a random multilabel classification problem,
    as follows:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了生成输入数据，我们将使用`sklearn.datasets.make_multilabel_classification`函数。此函数生成一个随机的多标签分类问题，如下所示：
- en: '[PRE48]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The following data is returned:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下数据：
- en: '`X`: The generated samples that is an array of shape [`n_samples`, `n_features`]'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X`：生成的样本，是一个形状为`[n_samples, n_features]`的数组'
- en: '`Y`: The label sets that is an array or sparse CSR matrix of shape [`n_samples`,
    `n_classes`]'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Y`：标签集，是一个形状为`[n_samples, n_classes]`的数组或稀疏CSR矩阵'
- en: In our case, the `Y` variable will not serve us, as we will use an unsupervised
    method that, as we know, does not require prior knowledge of the data label.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，`Y`变量将不会为我们服务，因为我们将使用一种无监督方法，正如我们所知，这种方法不需要对数据标签的先验知识。
- en: 'Now, we can build the `LatentDirichletAllocation()` model (with an online variational
    Bayes algorithm):'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以构建`LatentDirichletAllocation()`模型（使用在线变分贝叶斯算法）：
- en: '[PRE49]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Only two parameters are passed:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 只传递了两个参数：
- en: '`n_components=5`: This is the number of topics, `5`, because we used an input
    dataset built on the basis of five groups.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_components=5`：这是主题的数量，`5`，因为我们使用了一个基于五个组构建的输入数据集。'
- en: '`random_state=1`: This is the seed used by the random number generator.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_state=1`：这是随机数生成器使用的种子。'
- en: 'Now, we will train the model for the data, `X`, with a variational Bayes method:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用变分贝叶斯方法对数据`X`进行模型训练：
- en: '[PRE50]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Finally, we will get topics for the last 10 samples of the `X` dataset:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将获取`X`数据集最后10个样本的主题：
- en: '[PRE51]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The following results are returned:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '![](img/0753c1cd-e12b-4272-a2c0-d2c1d700c891.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0753c1cd-e12b-4272-a2c0-d2c1d700c891.png)'
- en: For each example provided as input, a sequence of five values is returned representing
    the probabilities that the topic belongs to that group. Obviously, the value closest
    to 1 represents the best probability.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 对于提供的每个输入示例，返回一个包含五个值的序列，表示该主题属于该组的概率。显然，最接近1的值代表最佳概率。
- en: How it works...
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The generative process of the LDA algorithm is based on the analysis of the
    data contained in the text. Word combinations are considered random variables.
    The LDA algorithm can be executed in the following ways:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: LDA算法的生成过程基于对文本中包含的数据的分析。将词组合视为随机变量。LDA算法可以以下方式进行执行：
- en: A word distribution is associated with each topic
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个主题都与一个词分布相关联
- en: Each document is found in a topic distribution
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个文档都位于一个主题分布中
- en: For each word in the document, verify its attribution to a document topic and
    to a word distribution of the topic
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于文档中的每个单词，验证其归属到文档主题和主题的词分布
- en: There's more…
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容...
- en: Depending on the type of inference, the LDA algorithm allows us to reach a certain
    level of effectiveness and cost (efficiency) in terms of temporal and spatial
    complexity. The LDA model was presented for the first time in 2003 in a paper
    published by David Blei, Andrew Ng, and Michael Jordan.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 根据推理类型，LDA算法允许我们在时间和空间复杂度方面达到一定水平的效果和成本（效率）。LDA模型首次于2003年由David Blei、Andrew
    Ng和Michael Jordan在发表的一篇论文中提出。
- en: See also
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: 'Refer to the official documentation of the `sklearn.decomposition.LatentDirichletAllocation`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请参考`sklearn.decomposition.LatentDirichletAllocation`函数的官方文档：[https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)
- en: 'Refer to *Latent Dirichlet Allocation* (by David Blei, Andrew Ng, and Michael
    Jordan): [http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请参考*潜在狄利克雷分配*（由David Blei、Andrew Ng和Michael Jordan所著）：[http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)
- en: Refer to [Chapter 7](fc31e304-3301-4ebf-80e4-404ac6e26606.xhtml), *Analyzing
    Text Data*
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请参考[第7章](fc31e304-3301-4ebf-80e4-404ac6e26606.xhtml)，*分析文本数据*
- en: Using LDA to classify text documents
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LDA对文本文档进行分类
- en: LDA is a natural language analysis model that allows to understand the semantic
    meaning of the text by analyzing the similarity between the distribution of the
    terms of the document with that of a specific topic (topic) or of an entity. More
    recently, LDA has gained notoriety even in semantic SEO as a possible ranking
    factor for the Google search engine.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: LDA是一种自然语言分析模型，它通过分析文档中术语分布与特定主题（主题）或实体分布的相似性，来理解文本的语义意义。最近，LDA在语义SEO领域也因其可能是谷歌搜索引擎的排名因素而声名鹊起。
- en: Getting ready
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will use the `sklearn.decomposition.LatentDirichletAllocation` function
    to perform a topic modeling analysis.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将使用`sklearn.decomposition.LatentDirichletAllocation`函数进行主题建模分析。
- en: How to do it…
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Let''s see how to use LDA to classify text documents:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用LDA对文本文档进行分类：
- en: 'Create a new Python file, and import the following packages (the full code
    is in the `TopicModellingLDA.py` file that''s already provided to you):'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件，并导入以下包（完整的代码在您已提供的`TopicModellingLDA.py`文件中）：
- en: '[PRE52]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'To import the data, we will use the `fetch_20newsgroups` dataset from the `sklearn`
    library:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了导入数据，我们将使用`sklearn`库中的`fetch_20newsgroups`数据集：
- en: '[PRE53]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: This is a collection of about 20,000 newsgroup documents, divided into 20 different
    newsgroups. The dataset is particularly useful for dealing with text classification
    problems.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个大约有20,000篇新闻组文档的集合，分为20个不同的新闻组。该数据集特别适用于处理文本分类问题。
- en: 'Now, we will print the name of the newsgroup available:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将打印可用的新闻组名称：
- en: '[PRE54]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The following results are returned:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果返回：
- en: '[PRE55]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'There are 11,314 samples in the data. We will extract only 2,000:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据中有11,314个样本。我们将只提取2,000个：
- en: '[PRE56]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now, we will extract a document term matrix. This is basically a matrix that
    counts the number of occurrences of each word in the document. So, we will define
    the object, and extract the document term matrix:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将提取一个文档词频矩阵。这基本上是一个计数文档中每个单词出现次数的矩阵。因此，我们将定义对象，并提取文档词频矩阵：
- en: '[PRE57]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, we can build the LDA model (with an online variational Bayes algorithm):'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以构建LDA模型（使用在线变分贝叶斯算法）：
- en: '[PRE58]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We will train the model for the data `NGDataVectModel` with a variational Bayes
    method:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用变分贝叶斯方法对`NGDataVectModel`数据进行模型训练：
- en: '[PRE59]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Finally, we will print the topic extracted:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将打印提取的主题：
- en: '[PRE60]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The following results are returned:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果返回：
- en: '![](img/302f85f8-f61e-4d2c-9594-863a60d0c01b.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/302f85f8-f61e-4d2c-9594-863a60d0c01b.png)'
- en: How it works...
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理…
- en: '**Topic modeling** refers to the process of identifying hidden patterns in
    text data. The goal is to uncover some hidden thematic structure in a collection
    of documents. This will help us organize our documents in a better way so that
    we can use them for analysis. This is an active area of research in NLP.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '**主题建模**指的是在文本数据中识别隐藏模式的过程。目标是揭示文档集合中的一些隐藏主题结构。这将帮助我们更好地组织文档，以便我们可以用于分析。这是NLP研究的一个活跃领域。'
- en: The LDA analysis automatically allows to go back to the topic of the phrases
    by the association of co-occurrences with a reference **knowledge base** (**KB**),
    without having to interpret the meaning of the sentences.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: LDA分析自动允许通过与参考**知识库**（**KB**）的共现关联，回溯到短语的议题，而无需解释句子的含义。
- en: There's more…
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容…
- en: Here, **de Finetti's theorem** establishes that any collection of changeable
    random variables is represented as a mixture of distributions, so if you want
    to have an exchangeable representation of words and documents, it is necessary
    to consider mixtures that capture the exchangeability of both. At the base of
    this methodology of thought lie the roots of the LDA model.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**德·芬蒂定理**表明，任何可变随机变量的集合都可以表示为分布的混合，因此如果您想要有可交换的单词和文档表示，就必须考虑同时捕捉两者可交换性的混合。LDA模型思想方法的基础就是这种方法的根源。
- en: See also
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: Refer to the official documentation of the `sklearn.decomposition.LatentDirichletAllocation` function: [https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考官方文档的`sklearn.decomposition.LatentDirichletAllocation`函数：[https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)
- en: Refer to [Chapter 7](fc31e304-3301-4ebf-80e4-404ac6e26606.xhtml), *Analyzing
    Text Data*
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请参阅[第7章](fc31e304-3301-4ebf-80e4-404ac6e26606.xhtml)，*分析文本数据*
- en: 'Refer to *Exchangeability and de Finetti’s Theorem* (from the University of
    Oxford): [http://www.stats.ox.ac.uk/~steffen/teaching/grad/definetti.pdf](http://www.stats.ox.ac.uk/~steffen/teaching/grad/definetti.pdf)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考牛津大学的*交换性和de Finetti定理*（[http://www.stats.ox.ac.uk/~steffen/teaching/grad/definetti.pdf](http://www.stats.ox.ac.uk/~steffen/teaching/grad/definetti.pdf)）
- en: Preparing data for LDA
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备LDA数据
- en: In the previous recipe, *Using LDA to classify text documents*, we have seen
    how to use the LDA algorithm for topic modeling. We have seen that, before constructing
    the algorithm, the dataset must be appropriately processed so as to prepare the
    data in a format compatible with the input provided by the LDA model. In this
    recipe, we will analyze in detail these procedures.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的配方中，“使用LDA对文本文档进行分类”，我们看到了如何使用LDA算法进行主题建模。我们了解到，在构建算法之前，数据集必须经过适当的处理，以便将数据准备成与LDA模型提供的输入格式兼容的形式。在这个配方中，我们将详细分析这些程序。
- en: Getting ready
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will analyze the procedures necessary to transform the data
    contained in a specific dataset. This data will then be used as input for an algorithm
    based on the LDA method.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将分析将特定数据集中包含的数据进行转换所需的程序。然后，这些数据将被用作基于LDA方法的算法的输入。
- en: How to do it...
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s see how to prepare data for LDA:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何为LDA准备数据：
- en: 'Create a new Python file, and import the following packages (the full code
    is in the `PrepDataLDA.py` file that''s already provided to you):'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件，并导入以下包（完整的代码在已经提供给你的`PrepDataLDA.py`文件中）：
- en: '[PRE61]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We define a series of sentences from which we want to extract the topics:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了一系列我们想要从中提取主题的句子：
- en: '[PRE62]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: In the sentences we have just defined, there are topics that are repeated with
    different meanings. It is not easy to derive a link between them.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们刚刚定义的句子中，有一些主题以不同的含义重复出现。很难在这些主题之间建立联系。
- en: 'We insert these sentences into a list:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将这些句子插入到一个列表中：
- en: '[PRE63]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We set the elements that we will use in the transformation procedure:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了在转换过程中将使用的元素：
- en: '[PRE64]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'To perform the transformation on all the phrases, it is necessary to set a
    loop that just goes through the list:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要对所有短语进行转换，需要设置一个循环，只需遍历列表：
- en: '[PRE65]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now, we can start preparing the data. **Tokenization** is the process of dividing
    text into a set of meaningful pieces. These pieces are called **tokens**. For
    example, we can divide a chunk of text into words, or we can divide it into sentences.
    Let''s start with sentence tokenization:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以开始准备数据。**分词**是将文本分割成一组有意义的片段的过程。这些片段被称为**标记**。例如，我们可以将一大块文本分割成单词，或者我们可以将其分割成句子。让我们从句子分词开始：
- en: '[PRE66]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Let''s move on to the removal of meaningless words. There are some words in
    typical English sentences that do not take on significant significance for the
    construction of a topic model. For example, conjunctions and articles do not help
    identify topics. These terms are called **stop words** and must be removed from
    our token list. These terms (stop words) change according to the context in which
    we operate. Let''s remove the stop words:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续到移除无意义词语。在典型的英语句子中，有些词对于主题模型的建设并不具有显著意义。例如，连词和冠词并不能帮助识别主题。这些术语被称为**停用词**，必须从我们的标记列表中移除。这些术语（停用词）根据我们操作的环境而变化。让我们移除停用词：
- en: '[PRE67]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The last phase of data preparation concerns the stemming. The goal of stemming
    is to reduce these different forms into a common base form. This uses a heuristic
    process to cut off the ends of words to extract the base form. Let''s make the
    stemming:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据准备的最后阶段是词干提取。词干提取的目标是将这些不同形式归入一个共同的基形式。这使用一种启发式过程来截断词尾以提取基形式。让我们进行词干提取：
- en: '[PRE68]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We just have to add the element obtained to the text list:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只需将获得的元素添加到文本列表中：
- en: '[PRE69]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'At this point, we have to turn our token list into a dictionary:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，我们必须将我们的标记列表转换成字典：
- en: '[PRE70]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'So, let''s build a document term matrix using tokenized documents:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，让我们使用标记文档构建一个文档-词矩阵：
- en: '[PRE71]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Finally, we build an LDA model and print the topics extracted:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们构建了一个LDA模型并打印出提取的主题：
- en: '[PRE72]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The following results are returned:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE73]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: How it works...
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Data preparation is essential for the creation of a topic model. The preparation
    of the data goes through the following procedures:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备对于创建主题模型至关重要。数据准备要经过以下程序：
- en: '**Tokenization**: The conversion of a document into its atomic elements'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词**：将文档转换为它的原子元素'
- en: '**Stop words**: Removes meaningless words'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停用词**：移除无意义的词'
- en: '**Stemming**: The fusion of equivalent words in meaning'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词干提取**：意义等效词的融合'
- en: There's more…
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Data preparation depends on the type of text we are processing. In some cases,
    it is necessary to carry out further operations before submitting the data to
    the LDA algorithm. For example, punctuation removal can be one of them, as well
    as the removal of special characters.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备取决于我们处理文本的类型。在某些情况下，在将数据提交给LDA算法之前，执行进一步操作是必要的。例如，可以包括去除标点符号，以及去除特殊字符。
- en: See also
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: Refer to *Tokenization* (from the NLP group at Stanford University): [https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考斯坦福大学NLP小组的*分词*：[https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)
- en: 'Refer to *Stemming and lemmatization* (from Stanford University): [https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考斯坦福大学的*词干提取和词形还原*：[https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)
- en: 'Refer to *Dropping common terms: stop words* (from Stanford University): [https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html)'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考斯坦福大学的*去除常见术语：停用词*：[https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html)
