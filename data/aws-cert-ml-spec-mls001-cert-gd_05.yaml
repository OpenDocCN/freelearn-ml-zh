- en: '*Chapter 3*: Data Preparation and Transformation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第三章* 数据准备和转换'
- en: You have probably heard that data scientists spend most of their time working
    on data preparation-related activities. It is now time to explain why that happens
    and which types of activities we are talking about.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能听说过数据科学家大部分时间都在从事与数据准备相关的活动。现在是时候解释为什么会这样，以及我们谈论的是哪些类型的活动了。
- en: In this chapter, you will learn how to deal with categorical and numerical features,
    as well as applying different techniques to transform your data, such as one-hot
    encoding, binary encoders, ordinal encoding, binning, and text transformations.
    You will also learn how to handle missing values and outliers in your data, which
    are two important tasks you can implement to build good machine learning models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何处理分类和数值特征，以及应用不同的技术来转换你的数据，例如独热编码、二进制编码器、顺序编码、分箱和文本转换。你还将学习如何处理数据中的缺失值和异常值，这两个任务对于构建良好的机器学习模型至关重要。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Identifying types of features
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别特征类型
- en: Dealing with categorical features
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理分类特征
- en: Dealing with numerical features
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理数值特征
- en: Understanding data distributions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据分布
- en: Handling missing values
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: Dealing with outliers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理异常值
- en: Dealing with unbalanced datasets
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理不平衡数据集
- en: Dealing with text data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理文本数据
- en: This is a lengthy chapter, so bear with us! Knowing about these topics in detail
    will definitely put you in a good position for the AWS Machine Learning Specialty
    exam.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个篇幅较长的章节，所以请耐心等待！详细了解这些主题将肯定让你在AWS机器学习专业考试中处于有利位置。
- en: Identifying types of features
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别特征类型
- en: 'We *cannot* start modeling without knowing what a **feature** is and which
    type of information it might store. You have already read about different processes
    that deal with features. For example, you know that feature engineering is related
    to the task of building and preparing features to your models; you also know that
    feature selection is related to the task of choosing the best set of features
    to feed a particular algorithm. These two tasks have one behavior in common: they
    may vary according to the types of features they are processing.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在不知道**特征**是什么以及它可能存储哪种类型的信息之前，我们无法开始建模。你已经阅读了关于处理特征的不同的过程。例如，你知道特征工程与构建和准备特征到模型的任务相关；你也知道特征选择与选择最佳特征集以供特定算法使用的任务相关。这两个任务有一个共同的行为：它们可能根据它们处理的特征类型而变化。
- en: It is very important to understand this behavior (feature type versus applicable
    transformations) because it will help you eliminate invalid answers during your
    exam (and, most importantly, you will become a better data scientist).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这种行为（特征类型与适用的转换）非常重要，因为它将帮助你消除考试中的无效答案（最重要的是，你将成为一名更好的数据科学家）。
- en: 'When we refer to types of features, we are talking about the data type that
    a particular feature is supposed to store. The following diagram shows how we
    could potentially describe the different types of features of a model:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提到特征类型时，我们指的是特定特征应该存储的数据类型。以下图表展示了我们如何潜在地描述模型的不同特征类型：
- en: '![Figure 3.1 – Feature types'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.1 – 特征类型'
- en: '](img/B16735_03_001.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16735_03_001.jpg]'
- en: Figure 3.1 – Feature types
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – 特征类型
- en: 'In [*Chapter 1*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014)*, Machine
    Learning Fundamentals*, you were introduced to the feature classification shown
    in the preceding diagram. Now, let''s look at some real examples in order to eliminate
    any remaining questions you may have:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第一章*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014)*，机器学习基础*中，你已了解到前面图表中展示的特征分类。现在，让我们看看一些真实示例，以便消除你可能存在的任何疑问：
- en: '![Figure 3.2 – Real examples of feature values'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.2 – 特征值的真实示例'
- en: '](img/B16735_03_002.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16735_03_002.jpg]'
- en: Figure 3.2 – Real examples of feature values
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – 特征值的真实示例
- en: 'Although looking at the values of the variable may help you find its type,
    you should never rely only on this aspect. The nature of the variable is also
    very important for making such decisions. For example, someone could encode the
    cloud provider variable (shown in the preceding table) as follows: 1 (AWS), 2
    (MS), 3 (Google). In that case, the variable is still a nominal feature, even
    if it is now represented by discrete numbers.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然查看变量的值可能有助于您找到其类型，但您绝不应该只依赖这个方面。变量的性质对于做出这样的决定也非常重要。例如，有人可以将云服务提供商变量（如前表所示）编码如下：1（AWS），2（MS），3（Google）。在这种情况下，变量仍然是一个名义特征，即使它现在由离散数字表示。
- en: If you are building a ML model and you don't tell your algorithm that this variable
    is not a discrete number, but instead a nominal variable, the algorithm will treat
    it as a number and the model won't be interpretable anymore.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在构建机器学习模型，并且没有告诉您的算法这个变量不是一个离散数字，而是一个名义变量，那么算法将把它当作一个数字处理，模型将不再可解释。
- en: Important note
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Before feeding any ML algorithm with data, make sure your feature types have
    been properly identified.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在将任何机器学习算法与数据结合之前，请确保您的特征类型已经被正确识别。
- en: 'In theory, if you are happy with your features and have properly classified
    each of them, you should be ready to go into the modeling phase of the CRISP-DM
    methodology, shouldn''t you? Well, maybe not. There are many reasons you may want
    to spend a little more time on data preparation, even after you have correctly
    classified your features:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，如果您对您的特征满意并且已经正确地对每个特征进行了分类，您应该准备好进入CRISP-DM方法的建模阶段，不是吗？好吧，也许不是。即使您已经正确地分类了特征，您可能还有很多原因想要在数据准备上花费更多的时间：
- en: Some algorithm implementations, such as `scikit-learn`, may not accept string
    values on your categorical features.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些算法实现，如`scikit-learn`，可能不接受您的分类特征上的字符串值。
- en: The data distribution of your variable may not be the most optimal distribution
    for your algorithm.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您变量的数据分布可能不是您算法的最优分布。
- en: Your ML algorithm may be impacted by the scale of your data.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的机器学习算法可能会受到您数据规模的影响。
- en: Some observations (rows) of your variable may be missing information and you
    will have to fix it. These are also known as missing values.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的变量的一些观测值（行）可能缺少信息，您将不得不修复它们。这些也被称为缺失值。
- en: You may find outlier values of your variable that can potentially add bias to
    your model.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能会发现变量的异常值，这些异常值可能会给您的模型带来潜在的偏差。
- en: Your variable may be storing different types of information and you may only
    be interested in a few of them (for example, a date variable can store the day
    of the week or the week of the month).
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的变量可能存储着不同类型的信息，而您可能只对其中的一些感兴趣（例如，日期变量可以存储星期几或月份的哪一周）。
- en: You might want to find a mathematical representation for a text variable.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能希望为文本变量找到一个数学表示。
- en: And believe me, this list will never end.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并且相信我，这个列表永远不会结束。
- en: In the following sections, we will understand how to address all these points,
    starting with categorical features.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将了解如何解决所有这些问题，从分类特征开始。
- en: Dealing with categorical features
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理分类特征
- en: Data transformation methods for categorical features will vary according to
    the sub-type of your variable. In the upcoming sections, we will understand how
    to transform nominal and ordinal features.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类特征的数据转换方法将根据您变量的子类型而有所不同。在接下来的章节中，我们将了解如何转换名义和有序特征。
- en: Transforming nominal features
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换名义特征
- en: You may have to create numerical representations of your categorical features
    before applying ML algorithms to them. Some libraries may have embedded logic
    to handle that transformation for you, but most of them do not.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用机器学习算法之前，您可能需要创建您分类特征的数值表示。一些库可能已经内置了处理这种转换的逻辑，但大多数都没有。
- en: 'The first transformation we will cover is known as **label encoding**. A label
    encoder is suitable for categorical/nominal variables and it will just associate
    a number with each distinct label of your variable. The following table shows
    how a label encoder works:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要介绍的第一种转换方法被称为**标签编码**。标签编码器适用于分类/名义变量，它将为您的变量的每个不同的标签分配一个数字。以下表格显示了标签编码器是如何工作的：
- en: '![Figure 3.3 – Label encoder in action'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.3 – 标签编码器在作用中'
- en: '](img/B16735_03_003.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_003.jpg)'
- en: Figure 3.3 – Label encoder in action
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – 标签编码器在作用中
- en: A label encoder will always ensure that a unique number is associated with each
    distinct label. In the preceding table, although "India" appears twice, the same
    number was assigned to it.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 标签编码器将始终确保每个不同的标签都关联一个唯一的数字。在前面的表格中，尽管“印度”出现了两次，但它被分配了相同的数字。
- en: You now have a numerical representation of each country, but this does not mean
    you can use that numerical representation in your models! In this particular case,
    we are transforming a nominal feature, *which does not have an order*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在有了每个国家的数值表示，但这并不意味着您可以在模型中使用这种数值表示！在这个特定的情况下，我们正在转换一个没有顺序的命名特征。
- en: According to the preceding table, if we pass the encoded version of the *country*
    variable to a model, it will make assumptions such as "Brazil (3) is greater than
    Canada (2)", which does not make any sense.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的表格，如果我们将*国家*变量的编码版本传递给模型，它将做出诸如“巴西（3）大于加拿大（2）”这样的假设，这是没有意义的。
- en: 'A possible solution for that scenario is applying another type of transformation
    on top of "*country"*: **one-hot encoding**. This transformation will represent
    all the categories from the original feature as individual features (also known
    as **dummy variables**), which will store the "presence or absence" of each category.
    The following table is transforming the same information we looked at in the preceding
    table, but this time it''s applying one-hot encoding:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种情况的一个可能解决方案是在"*国家"*上应用另一种类型的转换：**独热编码**。这种转换将表示原始特征中的所有类别作为单独的特征（也称为**虚拟变量**），这将存储每个类别的“存在或不存在”。以下表格正在转换我们在前面表格中查看的相同信息，但这次它正在应用独热编码：
- en: '![Figure 3.4 – One-hot encoding in action'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.4 – 独热编码的实际应用'
- en: '](img/B16735_03_004.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片/B16735_03_004.jpg]'
- en: Figure 3.4 – One-hot encoding in action
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – 独热编码的实际应用
- en: We can now use the one-hot encoded version of the *country* variable as a feature
    of a ML model. However, your work as a skeptical data scientist is never done,
    and your critical thinking ability will be tested in the AWS Machine Learning
    Specialty exam.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用*国家*变量的独热编码版本作为机器学习模型的特征。然而，作为一名怀疑论的数据科学家，您的工作永远不会结束，您的批判性思维能力将在AWS机器学习专业考试中得到考验。
- en: 'Let''s suppose you have 150 distinct countries in your dataset. How many dummy
    variables would you come up with? 150, right? Here, we just found a potential
    issue: apart from adding complexity to your model (which is not a desired characteristic
    of any model at all), dummy variables also add **sparsity** to your data.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的数据集中有150个不同的国家。您会想出多少个虚拟变量？150个，对吧？这里，我们刚刚发现一个潜在问题：除了增加模型的复杂性（这绝不是任何模型所期望的特性）之外，虚拟变量还会给您的数据增加**稀疏性**。
- en: A sparse dataset has a lot of variables filled with zeros. Often, it is hard
    to fit this type of data structure into memory (you can easily run out of memory)
    and it is very time-consuming for ML algorithms to process sparse structures.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏数据集有很多变量填充了零。通常，很难将这种数据结构拟合到内存中（您很容易耗尽内存），并且对于机器学习算法来说，处理稀疏结构非常耗时。
- en: You can work around the sparsity problem by grouping your original data and
    reducing the number of categories, and you can even use custom libraries that
    compress your sparse data and make it easier for manipulation (such as `scipy.sparse.csr_matrix`,
    from Python).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将原始数据分组并减少类别数量来绕过稀疏性问题，甚至可以使用自定义库来压缩稀疏数据，使其更容易操作（例如，Python中的`scipy.sparse.csr_matrix`）。
- en: Therefore, during the exam, remember that one-hot encoding is definitely the
    right way to go when you need to transform categorical/nominal data to feed ML
    models; however, take the number of unique categories of your original feature
    into account and think about if it makes sense to create dummy variables for all
    of them (maybe it does not make sense, if you have a very large number of unique
    categories).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在考试期间，请记住，当您需要将分类/名义数据转换为供机器学习模型使用时，独热编码绝对是正确的做法；然而，考虑到您原始特征中唯一类别的数量，并思考是否为所有这些类别创建虚拟变量是有意义的（如果您有非常多的唯一类别，可能并不合理）。
- en: Applying binary encoding
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用二进制编码
- en: For those types of variables with a higher number of unique categories, a potential
    approach to creating a numerical representation for them is applying **binary
    encoding**. In this approach, the goal is transforming a categorical column into
    multiple binary columns, but minimizing the number of new columns.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有更多唯一类别的变量类型，创建它们的数值表示的一个潜在方法是应用**二进制编码**。在这种方法中，目标是把一个分类列转换成多个二进制列，但最小化新列的数量。
- en: 'This process consists of three basic steps:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程包括三个基本步骤：
- en: The categorical data is converted into numerical data after being passed through
    an ordinal encoder.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类数据在通过序数编码器后转换为数值数据。
- en: The resulting number is then converted into a binary value.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将得到的数字转换为二进制值。
- en: The binary value is split into different columns.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 二进制值被分割成不同的列。
- en: 'Let''s reuse our data from *Figure 3.3* to see how we could use binary encoding
    in this particular case:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重用我们的*图3.3*中的数据来查看我们如何在这个特定情况下使用二进制编码：
- en: '![Figure 3.5 – Binary encoding in action'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.5 – 二进制编码的实际应用'
- en: '](img/B16735_03_005.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_005.jpg)'
- en: Figure 3.5 – Binary encoding in action
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 – 二进制编码的实际应用
- en: As we can see, we now have three columns (Col1, Col2, and Col3) instead of four.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们现在有三个列（Col1、Col2和Col3），而不是四个。
- en: Transforming ordinal features
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换序数特征
- en: 'Ordinal features have a very specific characteristic: *they have an order*.
    Because they have this quality, it does *not* make sense to apply one-hot encoding
    to them; if you do so, you will lose the magnitude of order of your feature.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 序数特征有一个非常具体的特征：*它们有顺序*。因为它们具有这种特性，所以对它们应用独热编码是没有意义的；如果你这样做，你将失去特征的顺序大小。
- en: 'The most common transformation for this type of variable is known as **ordinal
    encoding**. An ordinal encoder will associate a number with each distinct label
    of your variable, just like a label encoder does, but this time, it will respect
    the order of each category. The following table shows how an ordinal encoder works:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型变量的最常见转换方法被称为**序数编码**。序数编码器将每个变量的不同标签与一个数字相关联，就像标签编码器一样，但这次，它将尊重每个类别的顺序。以下表格显示了序数编码器是如何工作的：
- en: '![Figure 3.6 – Ordinal encoding in action'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.6 – 序数编码的实际应用'
- en: '](img/B16735_03_006.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_006.jpg)'
- en: Figure 3.6 – Ordinal encoding in action
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 – 序数编码的实际应用
- en: We can now pass the encoded variable to ML models and they will be able to handle
    this variable properly, with no need to apply one-hot encoding transformations.
    This time, comparisons such as "Sr Data Analyst is greater than Jr. Data Analyst"
    make total sense.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将编码后的变量传递给机器学习模型，它们将能够正确地处理这个变量，无需应用独热编码转换。这次，比较如“高级数据分析师大于初级数据分析师”是完全有意义的。
- en: Avoiding confusion in our train and test datasets
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免在训练和测试数据集中产生混淆
- en: 'Do not forget the following statement: encoders are **fitted** on training
    data and **transformed** on test and production data. This is how your ML pipeline
    should work.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记以下声明：编码器是在训练数据上**拟合**的，在测试和生产数据上**转换**的。这就是你的机器学习管道应该如何工作。
- en: Let's suppose you have created a one-hot encoder that fits the data from *Figure
    3.3* and returns data according to *Figure 3.4*. In this example, we will assume
    this is our training data. Once you have completed your training process, you
    may want to apply the same one-hot encoding transformation to your testing data
    to check the model's results.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经创建了一个适合*图3.3*数据的独热编码器，并返回根据*图3.4*的数据。在这个例子中，我们将假设这是我们的训练数据。一旦你完成了训练过程，你可能想要将相同的独热编码转换应用到你的测试数据上，以检查模型的结果。
- en: In the scenario that we just described (which is a very common situation in
    modeling pipelines), you *cannot* retrain your encoder on top of the testing data!
    You should just reuse the previous encoder object that you have created on top
    of the training data. Technically, we say that you shouldn't use the `fit` method
    again, and use the `transform` method instead.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们刚才描述的场景中（这在建模管道中是一个非常常见的情况），你**不能**在测试数据上重新训练你的编码器！你应该只是重用你在训练数据上创建的之前的编码器对象。技术上，我们说你不应该再次使用`fit`方法，而应该使用`transform`方法。
- en: 'You may already know the reasons why you should follow this rule, but let''s
    recap: the testing data was created to extract the performance metrics of your
    model, so you should not use it to extract any other knowledge. If you do so,
    your performance metrics will be biased by the testing data and you cannot infer
    that the same performance (shown in the test data) is likely to happen in production
    (when new data will come in).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经知道为什么你应该遵循这个规则的原因，但让我们回顾一下：测试数据是为了提取你模型的性能指标而创建的，所以你不应该用它来提取任何其他知识。如果你这样做，你的性能指标将会受到测试数据的影响，你无法推断出相同的性能（在测试数据中显示）在生产环境中（当新数据到来时）很可能发生。
- en: Alright, all good so far. However, what if our testing set has a new category
    that was not present in the train set? How are we supposed to transform this data?
    Let's walk through this particular scenario.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，到目前为止一切顺利。但是，如果我们测试集中有一个训练集中没有的新类别，我们该如何转换这些数据呢？让我们来分析这个特定场景。
- en: 'Going back to our one-hot encoding example we looked at in *Figures 3.3* (input
    data) and *Figure* *3.4* (output data), our encoder knows how to transform the
    following countries: Australia, Brazil, Canada, and India. If we had a different
    country in the testing set, the encoder would not know how to transform it, and
    that''s why we need to define how it will behave in scenarios where there are
    exceptions.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们之前在 *图 3.3*（输入数据）和 *图 3.4*（输出数据）中看到的 one-hot 编码示例，我们的编码器知道如何转换以下国家：澳大利亚、巴西、加拿大和印度。如果我们测试集中有其他国家，编码器将不知道如何转换它，这就是为什么我们需要定义它在有异常情况下的行为。
- en: 'Most of the ML libraries provide specific parameters for these situations.
    In our example, we could program the encoder to either raise an error or set all
    zeros on our dummy variables, as shown in the following table:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习库都为这些情况提供了特定的参数。在我们的例子中，我们可以编程编码器在虚拟变量上引发错误或设置所有为零，如下表所示：
- en: '![Figure 3.7 – Handling unknown values on one-hot encoding transformations'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.7 – 在 one-hot 编码转换中处理未知值'
- en: '](img/B16735_03_007.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_007.jpg)'
- en: Figure 3.7 – Handling unknown values on one-hot encoding transformations
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 在 one-hot 编码转换中处理未知值
- en: As we can see, Portugal was not present in the training set (*Figure 3.3*),
    so during the transformation, we are keeping the same list of known countries
    and saying that Portugal *IS NOT* any of them (all zeros).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，葡萄牙没有出现在训练集中（*图 3.3*），因此在转换过程中，我们保持相同的已知国家列表，并说葡萄牙*不是*其中任何一个（所有为零）。
- en: As the very good, skeptical data scientist we know you are becoming, should
    you be concerned about the fact that you have a particular category that has not
    been used during training? Well, maybe. This type of analysis really depends on
    your problem domain.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们知道的非常优秀、持怀疑态度的数据科学家，你应该担心这样一个事实，即你有一个在训练过程中没有使用过的特定类别吗？好吧，也许吧。这种分析类型真的取决于你的问题领域。
- en: Handling unknown values is very common and something that you should expect
    to do in your ML pipeline. However, you should also ask yourself, due to the fact
    that you did not use that particular category during your training process, if
    your model can be extrapolated and generalized.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 处理未知值是非常常见的事情，也是你在机器学习流程中应该预期要做的事情。然而，你也应该问自己，由于你在训练过程中没有使用那个特定的类别，你的模型是否可以被外推和泛化。
- en: Remember, your testing data must follow the same data distribution as your training
    data, and you are very likely to find all (or at least most) of the categories
    (of a categorical feature) either in the training or test sets. Furthermore, if
    you are facing overfitting issues (doing well in the training, but poorly in the
    test set) and, at the same time, you realize that your categorical encoders are
    transforming a lot of unknown values in the test set, guess what? It's likely
    that your training and testing samples are not following the same distribution,
    invalidating your model entirely.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，你的测试数据必须遵循与你的训练数据相同的数据分布，你很可能在训练或测试集中找到所有（或至少大多数）类别（分类特征的类别）。此外，如果你面临过拟合问题（在训练中表现良好，但在测试集中表现不佳），同时你意识到你的分类编码器在测试集中转换了大量的未知值，猜猜看？很可能你的训练和测试样本没有遵循相同的分布，这完全无效化了你的模型。
- en: As you can see, slowly, we are getting there. We are talking about bias and
    investigation strategies in fine-grained detail! Now, let's move on and look at
    performing transformations on numerical features. Yes, each type of data matters
    and drives your decisions!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，我们正逐渐接近目标。我们正在详细讨论偏差和调查策略！现在，让我们继续前进，看看对数值特征进行转换。是的，每种类型的数据都很重要，并驱动着你的决策！
- en: Dealing with numerical features
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理数值特征
- en: In terms of numerical features (discrete and continuous), we can think of transformations
    that rely on the training data and others that rely purely on the observation
    being transformed.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在数值特征（离散和连续）方面，我们可以考虑依赖于训练数据的转换，以及其他仅依赖于被转换的观察的转换。
- en: Those that rely on the training data will use the train set to learn the necessary
    parameters during `fit`, and then use them to transform any test or new data.
    The logic is pretty much the same as what we just reviewed for categorical features;
    however, this time, the encoder will learn different parameters.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 那些依赖于训练数据的将使用训练集在`fit`期间学习必要的参数，然后使用它们来转换任何测试或新数据。逻辑基本上与我们刚才讨论的分类特征相同；然而，这次，编码器将学习不同的参数。
- en: On the other hand, those that rely purely on observations do not care about
    train or test sets. They will simply perform a mathematical computation on top
    of an individual value. For example, we could apply an exponential transformation
    to a particular variable by squaring its value. There is no dependency on learned
    parameters from anywhere – just get the value and square it.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，那些纯粹依赖于观察的算法不关心训练集或测试集。它们将简单地在一个单独的值上执行数学计算。例如，我们可以通过平方其值来对一个特定变量应用指数转换。这里没有依赖任何地方学习到的参数——只需获取值并平方它即可。
- en: 'At this point, you might be thinking about dozens of available transformations
    for numerical features! Indeed, there are so many options that we can''t describe
    all of them here, and you are not supposed to know all of them for the AWS Machine
    Learning Specialty exam anyway. We will cover the most important ones (for the
    exam) here, but I don''t want to limit your modeling skills: take a moment to
    think about the unlimited options you have by creating custom transformations
    according to your use case.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能已经在想针对数值特征的成百上千种可用的转换了！确实，选项如此之多，我们无法在这里全部描述，而且对于AWS机器学习专业考试，你也不需要知道所有这些。在这里，我们将介绍最重要的（对于考试而言）一些，但我不想限制你的建模技能：花点时间想想，根据你的用例创建自定义转换，你有多少无限的选择。
- en: Data normalization
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据标准化
- en: Applying data **normalization** means changing the scale of the data. For example,
    your feature may store employee salaries that range between 20,000 and 200,000
    dollars/year and you want to put this data in the range of 0 and 1, where 20,000
    (the minimum observed value) will be transformed into 0 and 200,000 (the maximum
    observed value) will be transformed into 1.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 应用数据**标准化**意味着改变数据的规模。例如，你的特征可能存储员工年薪，范围在每年20,000到200,000美元之间，而你希望将这些数据放入0到1的范围内，其中20,000（观察到的最小值）将被转换为0，200,000（观察到的最大值）将被转换为1。
- en: This type of technique is specifically important when you want to fit your training
    data on top of certain types of algorithms that are impacted by the scale/magnitude
    of the underlying data. For instance, we can think about those algorithms that
    use the dot product of the input variables (such as neural networks or linear
    regression) and those algorithms that rely on distance measures (such as **k-nearest
    neighbor** (**KNN**) or **k-means**).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术在你想将训练数据适配到受底层数据规模/幅度影响的特定类型算法上时尤为重要。例如，我们可以考虑那些使用输入变量点积（如神经网络或线性回归）的算法，以及那些依赖于距离度量的算法（如**k-最近邻**（**KNN**）或**k-均值**）。
- en: On the other hand, applying data normalization will not result in performance
    improvements for rule-based algorithms, such as decision trees, since they will
    be able to check the predictive power of the features (either via **entropy**
    or **information gain** analysis), regardless of the scale of the data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，应用数据标准化不会提高基于规则的算法（如决策树）的性能，因为它们将能够检查特征的可预测性（无论是通过**熵**还是**信息增益**分析），而不管数据的规模如何。
- en: Important note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We will learn about these algorithms, along with the appropriate details, in
    the later chapters of this book. For instance, you can look at entropy and information
    gain as two types of metrics used by decision trees to check feature importance.
    Knowing the predictive power of each feature helps the algorithm define the optimal
    root, intermediaries, and leaf nodes of the tree.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的后续章节中学习这些算法，以及相关的细节。例如，你可以查看熵和信息增益作为决策树用来检查特征重要性的两种度量。了解每个特征的预测能力有助于算法定义树的根节点、中间节点和叶节点。
- en: Let's take a moment to understand why data normalization will help those types
    of algorithms. We already know that the goal of a clustering algorithm is to find
    groups or clusters in your data, and one of the most used clustering algorithms
    is known as k-means. We will use k-means to see this problem in action, since
    it is impacted by data scaling.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间来理解为什么数据归一化将有助于那些类型的算法。我们已经知道，聚类算法的目的是在你的数据中找到组或簇，而最常用的聚类算法之一就是k-means。我们将使用k-means来观察这个问题在实际中的应用，因为它受到数据缩放的影响。
- en: 'The following image shows how different scales of the variable could change
    the hyper plan''s projection:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了变量的不同尺度如何改变超平面的投影：
- en: '![Figure 3.8 – Plotting data of different scales in a hyper plan'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.8 – 在超平面中绘制不同尺度的数据]'
- en: '](img/B16735_03_008.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![图B16735_03_008.jpg]'
- en: Figure 3.8 – Plotting data of different scales in a hyper plan
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 – 在超平面中绘制不同尺度的数据
- en: On the left-hand side of the preceding image, we can see a single data point
    plotted in a hyper plan that has three dimensions (x, y, and z). All three dimensions
    (also known as features) were normalized to the scale of 0 and 1\. On the right-hand
    side, we can see the same data point, but this time, the "x" dimension was *not*
    normalized. We can clearly see that the hyper plan has changed.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图像的左侧，我们可以看到一个数据点在三维（x，y，z）的超平面中绘制。所有三个维度（也称为特征）都被归一化到0和1的尺度。在右侧，我们可以看到相同的数据点，但这次“x”维度没有被归一化。我们可以清楚地看到超平面已经改变了。
- en: In a real scenario, we would have far more dimensions and data points. The difference
    in the scale of the data would change the centroids of each clusters and could
    potentially change the assigned clusters of some points. This same problem will
    happen on other algorithms that rely on distances calculation, such as KNN.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际场景中，我们会有更多的维度和数据点。数据尺度的差异会改变每个簇的中心，并可能改变某些点的分配簇。同样的问题也会出现在依赖于距离计算的算法中，如KNN。
- en: Other algorithms, such as neural networks and linear regression, will compute
    weighted sums using your input data. Usually, these types of algorithms will perform
    operations such as *W1*X1 + W2*X2 + Wi*Xi*, where *Xi* and *Wi* refer to a particular
    feature value and its weight, respectively. Again, we will cover details of neural
    networks and linear models later, but can you see the data scaling problem by
    just looking at the calculations that we just described? We can easily come up
    with very large values if X (feature) and W (weight) are large numbers. That will
    make the algorithm's optimizations much more complex.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 其他算法，如神经网络和线性回归，将使用您的输入数据计算加权总和。通常，这些类型的算法将执行如 *W1*X1 + W2*X2 + Wi*Xi* 的操作，其中
    *Xi* 和 *Wi* 分别指代特定的特征值及其权重。再次，我们将在后面的章节中详细讲解神经网络和线性模型，但你能仅通过查看我们刚才描述的计算过程就看出数据缩放问题吗？如果
    X（特征）和 W（权重）是很大的数字，我们很容易得到非常大的值。这将使算法的优化变得更加复杂。
- en: I hope you now have a very good understanding about the reasons you should apply
    data normalization (and when you should not). Data normalization is often implemented
    in ML libraries as **Min Max Scaler**. If you find this term in the exam, then
    remember it is the same as data normalization.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你现在对为什么应该应用数据归一化（以及何时不应应用）有了很好的理解。数据归一化通常在机器学习库中作为 **Min Max Scaler** 实现。如果你在考试中遇到这个术语，那么请记住它与数据归一化是相同的。
- en: 'Additionally, data normalization does not necessarily need to transform your
    feature into a range between 0 and 1\. In reality, we can transform the feature
    into any range we want. The following is how a normalization is formally defined:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据归一化不一定需要将你的特征转换到0和1的范围内。实际上，我们可以将特征转换到我们想要的任何范围内。以下是对归一化的正式定义：
- en: '![Figure 3.9 – Normalization formula'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.9 – 归一化公式]'
- en: '](img/B16735_03_009.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![图B16735_03_009.jpg]'
- en: Figure 3.9 – Normalization formula
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 – 归一化公式
- en: Here, *X*min and *X*max are the lower and upper values of the range; *X* is
    the value of the feature. Apart from data normalization, there is another very
    important technique regarding numerical transformations that you *must* be aware
    of, not only for the exam, but also for your data science career. We'll look at
    this in the next section.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*X*min和*X*max是范围的上下限值；*X*是特征的值。除了数据归一化外，还有另一个非常重要的关于数值转换的技术，您必须了解，这不仅是为了考试，也是为了您的数据科学职业生涯。我们将在下一节中探讨这一点。
- en: Data standardization
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据标准化
- en: 'Data **standardization** is another scaling method that transforms the distribution
    of the data, so that the mean will become zero and the standard deviation will
    become one. The following image formally describes this scaling technique, where
    *X* represents the value to be transformed, *µ* refers to the mean of *X*, and
    *σ* is the standard deviation of *X*:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 数据**标准化**是另一种缩放方法，它转换数据的分布，使得均值变为零，标准差变为一。以下图像正式描述了这种缩放技术，其中*X*代表要转换的值，*µ*指的是*X*的均值，*σ*是*X*的标准差：
- en: '![Figure 3.10 – Standardization formula'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.10 – 标准化公式'
- en: '](img/B16735_03_010.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_010.jpg)'
- en: Figure 3.10 – Standardization formula
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 – 标准化公式
- en: Unlike normalization, data standardization will *not* result in a predefined
    range of values. Instead, it will transform your data into a standard **Gaussian
    distribution**, where your transformed values will represent the number of standard
    deviations of each value to the mean of the distribution.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 与归一化不同，数据标准化不会产生预定义的值范围。相反，它将您的数据转换成标准的**高斯分布**，其中您的转换值将代表每个值与分布均值的标准差数。
- en: Important note
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'Gaussian distribution, also known as normal distribution, is one of the most
    used distribution on statistical models. This is a continuous distribution with
    two main controlled parameters: µ (mean) and σ (standard deviation). Normal distributions
    are symmetric around the mean. In other words, most of the values will be close
    to the mean of the distribution.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯分布，也称为正态分布，是统计模型中最常用的分布之一。这是一个具有两个主要控制参数的连续分布：µ（均值）和σ（标准差）。正态分布围绕均值对称。换句话说，大多数值将接近分布的均值。
- en: 'Data standardization if often referred to as the **zscore** and is widely used
    to identify outliers on your variable, which we will see later in this chapter.
    For the sake of demonstration, the following table simulates the data standardization
    of a small dataset. The input value is present in the "Age" column, while the
    scaled value is present in the "Zscore" column:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标准化通常被称为**z分数**，广泛用于识别变量的异常值，我们将在本章后面看到。为了演示，以下表格模拟了一个小数据集的数据标准化。输入值位于“年龄”列中，而缩放值位于“Z分数”列中：
- en: '![Figure 3.11 – Data standardization in action'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.11 – 数据标准化过程'
- en: '](img/B16735_03_011.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_011.jpg)'
- en: Figure 3.11 – Data standardization in action
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 – 数据标准化过程
- en: Make sure you are confident when applying normalization and standardization
    by hand in the AWS Machine Learning Specialty exam. They might provide a list
    of values, as well as mean and standard deviation, and ask you the scaled value
    of each element of the list.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS机器学习专业考试中，确保你在手动应用归一化和标准化时充满信心。他们可能会提供一个值列表，以及均值和标准差，并询问列表中每个元素的缩放值。
- en: Applying binning and discretization
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用分箱和离散化
- en: '**Binning** is a technique where you can group a set of values into a bucket
    or bin; for example, grouping people between 0 and 14 years old into a bucket
    named "children," another group of people between 15 and 18 years old into a bucket
    named "teenager," and so on.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**分箱**是一种技术，可以将一组值分组到一个桶或箱中；例如，将0到14岁之间的人分组到名为“儿童”的桶中，将15到18岁之间的人分组到名为“青少年”的桶中，依此类推。'
- en: '**Discretization** is the process of transforming a continuous variable into
    discrete or nominal attributes. These continuous values can be discretized by
    multiple strategies, such as **equal-width** and **equal-frequency**.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**离散化**是将连续变量转换为离散或名义属性的过程。这些连续值可以通过多种策略进行离散化，例如**等宽**和**等频**。'
- en: An equal-width strategy will split your data across multiple bins of the same
    width. Equal-frequency will split your data across multiple bins with the same
    number of frequencies.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 等宽策略将数据分布在多个相同宽度的桶中。等频策略将数据分布在具有相同频率的多个桶中。
- en: 'Let''s look at an example. Suppose we have the following list containing 16
    numbers: 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 90\. As we
    can see, this list ranges between 10 and 90\. Assuming we want to create four
    bins using an equal-width strategy, we would come up with the following bins:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。假设我们有一个包含 16 个数字的列表：10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,
    22, 23, 24, 90。正如我们所见，这个列表的范围在 10 到 90 之间。如果我们想使用等宽策略创建四个区间，我们会得到以下区间：
- en: Bin >= 10 <= 30 > 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区间 >= 10 <= 30 > 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24
- en: Bin > 30 <= 50 >
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区间 > 30 <= 50 >
- en: Bin > 50 <= 70 >
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区间 > 50 <= 70 >
- en: Bin > 71 <= 90 > 90
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区间 > 71 <= 90 > 90
- en: 'In this case, the width of each bin is the same (20 units), but the observations
    are not equally distributed. Now, let''s simulate an equal-frequency strategy:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，每个区间的宽度相同（20 个单位），但观察值并不均匀分布。现在，让我们模拟一个等频率策略：
- en: Bin >= 10 <= 13 > 10, 11, 12, 13
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区间 >= 10 <= 13 > 10, 11, 12, 13
- en: Bin > 13 <= 17 > 14, 15, 16, 17
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区间 > 13 <= 17 > 14, 15, 16, 17
- en: Bin > 17 <= 21 > 18, 19, 20, 21
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区间 > 17 <= 21 > 18, 19, 20, 21
- en: Bin > 21 <= 90 > 22, 23, 24, 90
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区间 > 21 <= 90 > 22, 23, 24, 90
- en: In this case, all the bins have the same frequency of observations, although
    they have been built with different bin widths to make that possible.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，所有区间都有相同的观察频率，尽管它们是通过不同的区间宽度构建的，以实现这一点。
- en: 'Once you have computed your bins, you should be wondering what''s next, right?
    Here, you have some options:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你计算出了你的区间，你可能想知道下一步是什么，对吧？在这里，你有一些选择：
- en: You can name your bins and use them as a nominal feature on your model! Of course,
    as a nominal variable, you should think about applying one-hot encoding before
    feeding a ML model with this data.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以为你的区间命名，并将它们用作模型上的名义特征！当然，作为一个名义变量，你应该在将数据输入机器学习模型之前考虑应用独热编码。
- en: You might want to order your bins and use them as an ordinal feature.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能想要对你的区间进行排序并用作有序特征。
- en: Maybe you want to remove some noise from your feature by averaging the minimum
    and maximum values of each bin and using that value as your transformed feature.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能你想通过取每个区间的最小值和最大值的平均值来去除特征中的噪声，并使用这个值作为你的变换特征。
- en: 'Take a look at the following table to understand these approaches using our
    equal-frequency example:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 看看以下表格，了解这些方法使用我们的等频率示例：
- en: '![Figure 3.12 – Different approaches to working with bins and discretization'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.12 – 处理区间和离散化的不同方法'
- en: '](img/B16735_03_012.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_012.jpg)'
- en: Figure 3.12 – Different approaches to working with bins and discretization
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12 – 处理区间和离散化的不同方法
- en: Again, playing with different binning strategies will give you different results
    and you should analyze/test the best approach for your dataset. There is no standard
    answer here – it is all about data exploration!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，尝试不同的区间划分策略会得到不同的结果，你应该分析/测试最适合你数据集的最佳方法。这里没有标准答案——一切都关于数据探索！
- en: Applying other types of numerical transformations
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用其他类型的数值变换
- en: 'Normalization and standardization rely on your training data to fit their parameters:
    minimum and maximum values in the case of normalization, and mean and standard
    deviation in the case of standard scaling. This also means you must fit those
    parameters using *only* your training data and never the testing data.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化和标准化依赖于你的训练数据来调整它们的参数：在归一化情况下是最小值和最大值，在标准缩放情况下是平均值和标准差。这也意味着你必须使用*仅*你的训练数据来调整这些参数，永远不要使用测试数据。
- en: 'However, there are other types of numerical transformations that do not require
    parameters from training data to be applied. These types of transformations rely
    purely on mathematical computations. For example, one of these transformations
    is known as **logarithmic transformation**. This is a very common type of transformation
    in machine learning models and is especially beneficial for **skewed** features.
    In case you don''t know what a skewed distribution is, take a look at the following
    diagram:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有其他类型的数值变换不需要从训练数据中获取参数来应用。这些类型的变换完全依赖于数学计算。例如，其中一种变换被称为**对数变换**。这在机器学习模型中是一种非常常见的变换类型，特别适用于**偏斜**特征。如果你不知道什么是偏斜分布，请看以下图表：
- en: '![Figure 3.13 – Skewed distributions'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.13 – 偏斜分布'
- en: '](img/B16735_03_013.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_013.jpg)'
- en: Figure 3.13 – Skewed distributions
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 – 偏斜分布
- en: In the middle, we have a normal distribution (or Gaussian distribution). On
    the left- and right-hand sides, we have skewed distributions. In terms of skewed
    features, there will be some values far away from the mean in one single direction
    (either left or right). Such behavior will push both the median and mean values
    of this distribution in the same direction of the long tail we can see in the
    preceding diagram.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在中间，我们有一个正态分布（或高斯分布）。在左侧和右侧，我们有偏斜分布。在偏斜特征方面，将有一些值远离均值，且仅在一个方向上（要么是左边，要么是右边）。这种行为将推动这个分布的中位数和均值向前面图表中可以看到的长尾的同一方向移动。
- en: One very clear example of data that used to be skewed is the annual salaries
    of a particular group of professionals in a given region, such as senior data
    scientists working in Florida, US. This type of variable usually has most of its
    values close to the others (because people used to earn an average salary) and
    just has a few very high values (because a small group of people makes much more
    money than others).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一个曾经是偏斜的数据的非常明显的例子是特定地区某一组专业人士的年度工资，例如在美国佛罗里达州工作的资深数据科学家。这类变量通常大部分值都接近其他值（因为人们过去挣的是平均工资），只有少数几个非常高的值（因为一小部分人比其他人挣得多得多）。
- en: Hopefully, you can now easily understand why the mean and median values will
    move to the tail direction, right? The big salaries will push them in that direction.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你现在可以很容易地理解为什么均值和中位数会移动到尾部方向，对吧？高薪将推动它们向那个方向移动。
- en: 'Alright, but why will a logarithmic transformation be beneficial for this type
    of feature? The answer to this question can be explained by the math behind it:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，但为什么对数变换会对此类特征有益呢？这个问题的答案可以通过其背后的数学来解释：
- en: '![Figure 3.14 – Logarithmic properties'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 3.14 – Logarithmic properties'
- en: '](img/B16735_03_014.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16735_03_014.jpg](img/B16735_03_014.jpg)'
- en: Figure 3.14 – Logarithmic properties
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 – 对数特性
- en: 'Computing the log of a number is the inverse of the exponential function. Log
    transformation will then reduce the scale of your number according to a given
    base (such as base 2, base 10, or base e, in the case of a natural logarithm).
    Looking at our salary''s distribution from the previous example, we would bring
    all those numbers down so that the higher the number, the higher the reduction;
    however, we would do this in a log scale and not in a linear fashion. Such behavior
    will remove the outliers of this distribution (making it closer to a normal distribution),
    which is beneficial for many ML algorithms, such as linear regression. The following
    table shows you some of the differences when transforming a number in a linear
    scale versus a log scale:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 计算一个数的对数是指数函数的逆运算。对数变换将根据给定的基数（例如基数2、基数10或自然对数的情况下的基数e）来缩小你的数字的规模。查看上一个例子中我们工资的分布，我们会将这些数字都降下来，使得数字越高，降低的幅度越大；然而，我们会以对数尺度而不是线性方式来做这件事。这种行为将消除这个分布的异常值（使其更接近正态分布），这对许多机器学习算法（如线性回归）是有益的。下表显示了在将数字从线性尺度转换为对数尺度时的一些差异：
- en: '![Figure 3.15 – Differences between linear transformation and log transformation'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 3.15 – Differences between linear transformation and log transformation'
- en: '](img/B16735_03_015.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16735_03_015.jpg](img/B16735_03_015.jpg)'
- en: Figure 3.15 – Differences between linear transformation and log transformation
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15 – 线性变换与对数变换的区别
- en: I hope you can see that the linear transformation kept the original magnitude
    of the data (we can still see outliers, but in another scale), while the log transformation
    removed those differences of magnitude and still kept the order of the values.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你能看到线性变换保持了数据的原始幅度（我们仍然可以看到异常值，但是在另一个尺度上），而对数变换则消除了这些幅度的差异，同时仍然保持了值的顺序。
- en: 'Would you be able to think about another type of mathematical transformation
    that follows the same behavior of *log* (making the distribution closer to Gaussian)?
    OK, I can give you another: square root. Take the square root of those numbers
    shown in the preceding table and see yourself!'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 你能想到另一种遵循相同行为（使分布更接近高斯分布）的数学变换类型吗？好的，我可以给你另一个：平方根。取前面表格中显示的这些数字的平方根，看看你自己吧！
- en: 'Now, pay attention to this: both log and square root belong to a set of transformations
    known as **power transformations**, and there is very popular method, which is
    likely to be mentioned on your AWS exam, that can perform a range of power transformations
    like those we have seen. This method was proposed by George Box and David Cox
    and its name is **Box-Cox**.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，请注意这一点：对数和平方根都属于一组称为**幂变换**的变换，而且有一个非常流行的、可能在你的AWS考试中提到的方法，可以执行一系列我们看到的幂变换。这种方法是由乔治·博克斯和大卫·考克斯提出的，其名称是**Box-Cox**。
- en: Important note
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: During your exam, if you see questions around the Box-Cox transformation, remember
    that it is a method that can perform many power transformations (according to
    a lambda parameter), and its end goal is making the original distribution closer
    to a normal distribution (do not forget that).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的考试中，如果你看到关于Box-Cox转换的问题，请记住，这是一种可以执行许多幂变换（根据lambda参数）的方法，其最终目标是使原始分布更接近正态分布（不要忘记这一点）。
- en: Just to conclude our discussion regarding why mathematical transformations can
    really make a difference to ML models, I will give you an intuitive example about
    **exponential transformations**.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 只为了总结我们关于为什么数学变换真的可以影响机器学习模型效果的讨论，我将给你一个关于**指数变换**的直观例子。
- en: 'Suppose you have a set of data points, such as those on the left-hand side
    of *Figure 3.16*. Your goal is to draw a line that will perfectly split blue and
    red points. Just by looking at the original data (again, on the left-hand side),
    we know that our best guess for performing this linear task would be the one you
    can see in the same image. However, the science (not magic) happens on the right-hand
    side of the image! By squaring those numbers and plotting them in another hyper
    plan, we can perfectly separate each group of points:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一组数据点，例如*图3.16*左侧的数据点。你的目标是画一条线，能够完美地分割蓝色和红色点。仅通过观察原始数据（再次，在左侧），我们知道我们执行这个线性任务的最佳猜测就是你在同一张图片中看到的那个。然而，科学（不是魔法）发生在图片的右侧！通过将这些数字平方并在另一个超平面上绘制，我们可以完美地分离每一组点：
- en: '![Figure 3.16 – Exponential transformation in action'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.16 – 指数变换的实际应用](img/B16735_03_016.jpg)'
- en: '](img/B16735_03_016.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16735_03_016.jpg](img/B16735_03_016.jpg)'
- en: Figure 3.16 – Exponential transformation in action
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 – 指数变换的实际应用
- en: I know you might be thinking about the infinite ways you can deal with your
    data. Although this is true, you should always take the business scenario you
    are working on into account and plan your work accordingly. Remember that model
    improvements or exploration is always possible, but you have to define your goals
    (remember the CRISP-DM methodology) and move on.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道你可能正在思考处理数据的各种无限可能方式。尽管这是真的，但你应该始终考虑你正在处理的业务场景，并据此规划你的工作。记住，模型改进或探索总是可能的，但你必须定义你的目标（记住CRISP-DM方法论）然后继续前进。
- en: By the way, data transformation is important, but just one piece of your work
    as a data scientist. Your modeling journey still has to move to other important
    topics, such as missing values and outliers handling, which we will look at next.
    However, before that, you may have noticed that you were introduced to Gaussian
    distributions during this section, so let's take a moment to discuss them in a
    bit more detail.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，数据转换很重要，但只是作为数据科学家工作的你工作中的一小部分。你的建模之旅仍需转向其他重要主题，例如处理缺失值和异常值，我们将在下一节中探讨。然而，在那之前，你可能已经注意到在这一节中你被介绍了高斯分布，所以让我们花点时间更详细地讨论一下。
- en: Understanding data distributions
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据分布
- en: Although the Gaussian distribution is probably the most common distribution
    for statistical and machine learning models, you should be aware that it is not
    the only one. There are other types of data distributions, such as the **Bernoulli**,
    **Binomial**, and **Poisson** distributions.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管高斯分布可能是统计和机器学习模型中最常见的分布，但你应该知道它不是唯一的。还有其他类型的数据分布，例如**伯努利**、**二项**和**泊松**分布。
- en: 'The Bernoulli distribution is a very simple one, as there are only two types
    of possible events: success or failure. The success event has a probability "p"
    of happening, while the failure one has a probability of "1-p".'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 二项分布是一个非常简单的分布，因为只有两种可能的事件类型：成功或失败。成功事件发生的概率是“p”，而失败事件的概率是“1-p”。
- en: 'Some examples that follow a Bernoulli distribution are rolling a six-sided
    die or flipping a coin. In both cases, you must define the event of success and
    the event of failure. For example, suppose our events for success and failure
    in the die example are as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一些遵循伯努利分布的例子包括掷一个六面骰子或抛硬币。在这两种情况下，你必须定义成功事件和失败事件。例如，假设我们在骰子例子中的成功和失败事件如下：
- en: 'Success: Getting a number 6'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功：得到一个6
- en: 'Failure: Getting any other number'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 失败：得到任何其他数字
- en: We can then say that we have a p probability of success (1/6 = 0.16 = 16%) and
    a 1-p probability of failure (1 - 0.16 = 0.84 = 84%).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以说我们有p的成功概率（1/6 = 0.16 = 16%）和1-p的失败概率（1 - 0.16 = 0.84 = 84%）。
- en: The Binomial distribution generalizes the Bernoulli distribution. The Bernoulli
    distribution has only one repetition of an event, while the Binomial distribution
    allows the event to be repeated many times, and we must count the number of successes.
    Let's continue with our prior example; that is, counting the number of times we
    got a 6 out of our 10 dice rolls. Due to the nature of this example, Binomial
    distribution has two parameters, n and p, where n is the number of repetitions
    and p is the probability of success in every repetition.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 二项分布是伯努利分布的推广。伯努利分布只有一个事件的重复，而二项分布允许事件重复多次，我们必须计算成功的次数。让我们继续使用之前的例子；即，计算我们在10次掷骰子中得到6的次数。由于这个例子本身的性质，二项分布有两个参数，n和p，其中n是重复的次数，p是每次重复成功的概率。
- en: 'Finally, a Poisson distribution allows us to find a number of events in a time
    period, given the number of times an event occurs in an interval. It has three
    parameters: lambda, e, and k, where lambda is the average number of events per
    interval, e is the Euler number, and k is the number of times an event occurs
    in an interval.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，泊松分布允许我们在给定事件在区间内发生的次数的情况下，找到一个时间段内的事件数量。它有三个参数：lambda（λ）、e（欧拉数）和k，其中lambda是每个区间内事件发生的平均次数，e是欧拉数，k是事件在区间内发生的次数。
- en: With all those distributions, including the Gaussian one, it is possible to
    compute the expected mean value and variance based on their parameters. This information
    is usually used in hypothesis tests to check whether some sample data follows
    a given distribution, by comparing the mean and variance **of the sample** against
    the **expected** mean and variance of the distribution.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有这些分布，包括高斯分布，我们可以根据它们的参数计算期望的均值和方差。这些信息通常用于假设检验，以检查某些样本数据是否遵循给定的分布，通过比较样本的均值和方差与分布的**期望**均值和方差。
- en: I hope you are now more familiar with data distributions generally, not only
    Gaussian distributions. We will keep talking about data distributions throughout
    this book. For now, let's move on to missing values and outlier detection.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你现在对数据分布有了更全面的了解，而不仅仅是高斯分布。我们将在整本书中持续讨论数据分布。现在，让我们继续讨论缺失值和异常值检测。
- en: Handling missing values
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: As the name suggests, missing values refer to the absence of data. Such absences
    are usually represented by tokens, which may or may not be implemented in a standard
    way.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，缺失值指的是数据的缺失。这种缺失通常由标记表示，这些标记可能或可能没有以标准方式实现。
- en: Although using tokens is standard, the way those tokens are displayed may vary
    across different platforms. For example, relational databases represent missing
    data with *NULL*, core Python code will use *None*, and some Python libraries
    will represent missing numbers as (**Not a Number** (**NaN**).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用标记是标准的，但这些标记的显示方式可能因不同平台而异。例如，关系型数据库使用*NULL*表示缺失数据，核心Python代码将使用*None*，而一些Python库将缺失数字表示为（**非数字**（**NaN**）。
- en: Important note
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For numerical fields, don't replace those standard missing tokens with *zeros*.
    By default, zero is not a missing value, but another number. I said "by default"
    because, in data science, we may face some data quality issues, which we will
    cover next.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值字段，不要用*零*替换那些标准缺失标记。默认情况下，零不是缺失值，而是另一个数字。我说“默认情况下”，因为在数据科学中，我们可能会遇到一些数据质量问题，我们将在下一章中讨论。
- en: However, in real business scenarios, you may or may not find those standard
    tokens. For example, a software engineering team might have designed the system
    to automatically fill missing data with specific tokens, such as "unknown" for
    strings or "-1" for numbers. In that case, you would have to search by those two
    tokens to find missing data. People can set anything.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实际的商业场景中，你可能找不到那些标准标记。例如，一个软件工程团队可能已经设计了系统，自动用特定的标记填充缺失数据，例如字符串中的“未知”或数字中的“-1”。在这种情况下，你必须通过这两个标记来搜索以找到缺失数据。人们可以设置任何东西。
- en: In the previous example, the software engineering team was still kind enough
    to give us standard tokens. However, there are many cases where legacy systems
    do not add any data quality layer in front of the user, and you may find an address
    field filled with "I don't want to share" or a phone number field filled with
    "Don't call me". This is clearly missing data, but not as standard as the previous
    example.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，软件工程团队仍然足够友好，给了我们标准标记。然而，有许多情况是遗留系统在用户面前没有添加任何数据质量层，你可能会发现地址字段填写了“我不想分享”或电话号码字段填写了“不要给我打电话”。这显然是缺失数据，但不如前面的例子标准化。
- en: 'There are many more nuances that you will learn regarding missing data, all
    of which we will cover in this section, but be advised: before you start making
    decisions about missing values, you should prepare a good data exploration and
    make sure you find those values. You can either compute data frequencies or use
    missing plots, but please do something. Never assume that your missing data is
    represented only by those handy standard tokens.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 关于缺失数据，你将学到更多细微之处，所有这些内容我们将在本节中涵盖，但请记住：在你开始对缺失值做出决策之前，你应该做好数据探索的准备，并确保你找到了这些值。你可以计算数据频率或使用缺失图，但请采取一些措施。永远不要假设你的缺失数据只由那些方便的标准标记表示。
- en: Why should we care about this type of data? Well, first, because most algorithms
    (apart from decision trees implemented on very specific ML libraries) will raise
    errors when they find a missing value. Second (and maybe most importantly), by
    grouping all the missing data in the same bucket, you are assuming that they are
    all the same, but in reality, you don't know that.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要关注这种类型的数据呢？首先，因为大多数算法（除了在非常具体的机器学习库上实现的决策树之外）在发现缺失值时会引发错误。其次（也许是最重要的），通过将所有缺失数据归入同一个类别，你是在假设它们都是相同的，但在现实中，你并不知道这一点。
- en: Such a decision will not only add bias to your model – it will reduce its interpretability,
    as you will be unable to explain the missing data. Once we know why we want to
    treat the missing values, we can take a look at our options.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的决策不仅会给你的模型增加偏差，还会降低其可解释性，因为你将无法解释缺失数据。一旦我们知道我们为什么要处理缺失值，我们就可以看看我们的选项。
- en: 'Theoretically, we can classify missing values into two main groups: **MCAR**
    or **MNAR**. MCAR stands for **Missing Completely at Random** and states that
    there is no pattern associated with the missing data. On the other hand, MNAR
    stands for **Missing Not at Random** and means that the underlying process used
    to generate the data is strictly connected to the missing values.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，我们可以将缺失值分为两大类：**MCAR** 或 **MNAR**。MCAR 代表 **完全随机缺失**，表示缺失数据没有与之相关的模式。另一方面，MNAR
    代表 **非随机缺失**，意味着用于生成数据的底层过程与缺失值严格相关。
- en: Let me give you an example of MNAR missing values. Suppose you are collecting
    user feedback about a particular product in an online survey. Your process of
    asking questions is dynamic and depends on user answers. When a user specifies
    an age lower than 18 years old, you never ask his/her marital status. In this
    case, missing values of marital status are connected to the age of the user (MNAR).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我给你一个 MNAR 缺失值的例子。假设你正在收集用户对特定产品的在线调查反馈。你提问的过程是动态的，取决于用户的回答。当一个用户指定年龄低于 18
    岁时，你永远不会询问他的婚姻状况。在这种情况下，婚姻状况的缺失值与用户的年龄相关联（MNAR）。
- en: Knowing the class of missing values that you are dealing with will help you
    understand if you have any control over the underlying process that generates
    the data. Sometimes, you can come back to the source process and, somehow, complete
    your missing data.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 了解你所处理的缺失值的类别将帮助你理解你是否对生成数据的底层过程有任何控制权。有时，你可以回到源过程，以某种方式完成你的缺失数据。
- en: Important note
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Although, in real scenarios, we usually have to treat missing data via exclusion
    or imputation, never forget that you can always try to look at the source process
    and check if you can retrieve (or, at least, better understand) the missing data.
    You may face this option in the exam.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在现实场景中，我们通常必须通过排除或插补来处理缺失数据，但永远不要忘记，你总是可以尝试查看源过程并检查你是否可以检索（或者至少更好地理解）缺失数据。你可能会在考试中遇到这个选项。
- en: If you don't have an opportunity to recover your missing data from somewhere,
    then you should move on to other approaches, such as **listwise deletion** and
    **imputation**.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有机会从某处恢复你的缺失数据，那么你应该转向其他方法，例如**逐行删除**和**插补**。
- en: Listwise deletion refers to the process of discarding some data, which is the
    downside of this choice. This may happen at the row level or at the column level.
    For example, suppose you have a DataFrame containing four columns and one of them
    has 90% of its data missing. In such cases, what usually makes more sense is dropping
    the entire feature (column), since you don't have that information for the majority
    of your observations (rows).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 逐行删除指的是丢弃一些数据的过程，这是这种选择的一个缺点。这可能在行级别或列级别发生。例如，假设你有一个包含四个列的DataFrame，其中一列有90%的数据缺失。在这种情况下，通常更有意义的是删除整个特征（列），因为你没有大多数观察（行）的这些信息。
- en: From a row perspective, you may have a DataFrame with a small number of observations
    (rows) containing missing data in one of its features (columns). In such scenarios,
    instead of removing the entire feature, what makes more sense is removing only
    those few observations.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 从行的角度来看，你可能有一个包含少量观察（行）的DataFrame，其中其一个特征（列）包含缺失数据。在这种情况下，与其删除整个特征，不如只删除那些少数观察值。
- en: The benefit of using this method is the simplicity of dropping a row or a column.
    Again, the downside is losing information. If you don't want to lose information
    while handling your missing data, then you should go for an imputation strategy.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法的好处是删除行或列的简单性。同样，缺点是丢失信息。如果你在处理缺失数据时不想丢失信息，那么你应该选择插补策略。
- en: 'Imputation is also known as replacement, where you will replace missing values
    by substituting a value. The most common approach of imputation is replacing the
    missing value with the mean of the feature. Please take a note of this approach
    because it is likely to appear in your exam:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 插补也称为替换，其中你将用替代值替换缺失值。插补最常见的方法是用特征的均值替换缺失值。请注意这种方法，因为它很可能出现在你的考试中：
- en: '![Figure 3.17 – Replacing missing values with the mean or median'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.17 – 使用均值或中位数替换缺失值'
- en: '](img/B16735_03_017.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_017.jpg)'
- en: Figure 3.17 – Replacing missing values with the mean or median
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.17 – 使用均值或中位数替换缺失值
- en: 'The preceding table shows a very simple dataset with one single feature and
    five observations, where the third observation has a missing value. If we decide
    to replace that missing data with the mean value of the feature, we will come
    up with 49\. Sometimes, when we have outliers in the data, the median might be
    more appropriate (in this case, the median would be 35):'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的表显示了一个非常简单的数据集，只有一个特征和五个观察值，其中第三个观察值有缺失值。如果我们决定用特征的均值来替换那个缺失数据，我们会得到49。有时，当数据中有异常值时，中位数可能更合适（在这种情况下，中位数将是35）：
- en: '![Figure 3.18 – Replacing missing values with the mean or median of the group'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.18 – 使用组均值或中位数替换缺失值'
- en: '](img/B16735_03_018.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_018.jpg)'
- en: Figure 3.18 – Replacing missing values with the mean or median of the group
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.18 – 使用组均值或中位数替换缺失值
- en: If you want to go deeper, you could find the mean or median value according
    to a given group of features. For example, in the preceding table, we expanded
    our previous dataset by adding the Job status column. Now, we have clues that
    help us suspect that our initial approach of changing the missing value by using
    the overall median (35 years old) is likely to be wrong (since that person is
    retired).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要深入了解，你可以根据给定的特征组找到均值或中值。例如，在前面的表中，我们通过添加工作状态列扩展了我们的先前数据集。现在，我们有线索使我们怀疑我们最初通过使用整体中位数（35岁）来改变缺失值的方法可能是错误的（因为那个人已经退休了）。
- en: What you can do now is replace the missing value with the mean or median of
    the other observations that belong to the same job status. Using this new approach,
    we can change the missing information to 77.5\. Taking into account that the person
    is retired, 77.5 makes more sense than 35 years old.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以替换缺失值为你所属相同工作状态的观测值的平均值或中位数。使用这种方法，我们可以将缺失信息更改为77.5。考虑到这个人已经退休，77.5比35岁更有意义。
- en: Important note
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In the case of categorical variables, you can replace the missing data with
    the value that has the highest occurrence in your dataset. The same logic of grouping
    the dataset according to specific features is still applicable.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类变量的情况下，你可以用你在数据集中出现频率最高的值来替换缺失数据。根据特定特征对数据集进行分组的逻辑仍然适用。
- en: You can also use more sophisticated methods of imputation, including constructing
    a ML model to predict the value of your missing data. The downside of these imputation
    approaches (either by averaging or predicting the value) is that you are making
    inferences on the data, which are not necessarily right and will add bias to the
    dataset.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用更复杂的插补方法，包括构建一个机器学习模型来预测你缺失数据的值。这些插补方法（无论是通过平均还是预测值）的缺点是，你正在对数据做出推断，这些推断不一定正确，并且会向数据集添加偏差。
- en: To sum this up, the trade-off while dealing with missing data is having a balance
    between losing data or adding bias to the dataset. Unfortunately, there is no
    scientific manual that you can follow, whatever your problem. To decide on what
    you are going to do, you must look to your success criteria, explore your data,
    run experiments, and then make your decisions.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，处理缺失数据时的权衡是在丢失数据或向数据集添加偏差之间保持平衡。不幸的是，没有科学手册可以遵循，无论你的问题是什么。为了决定你要做什么，你必须参考你的成功标准，探索你的数据，运行实验，然后做出决定。
- en: We will now move to another headache for many ML algorithms, also known as outliers.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将转向许多机器学习算法的另一个头痛问题，也称为异常值。
- en: Dealing with outliers
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理异常值
- en: We are not on this studying journey just to pass the AWS Machine Learning Specialty
    exam, but also to become better data scientists. There are many different ways
    to look at the outlier problem purely from a mathematical perspective; however,
    the datasets we use are derived from the underlying business process, so we must
    include a business perspective during an outlier analysis.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行这项研究之旅，不仅仅是为了通过AWS机器学习专业考试，也是为了成为更好的数据科学家。从数学角度纯粹地看待异常值问题有许多不同的方法；然而，我们使用的数据集是从底层业务流程中提取的，因此在进行异常值分析时，我们必须包括业务视角。
- en: 'An **outlier** is an atypical data point in a set of data. For example, the
    following chart shows some data points that have been plotted in a two-dimension
    plan; that is, x and y. The red point is an outlier, since it is an atypical value
    on this series of data:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**异常值**是一组数据中的非典型数据点。例如，以下图表显示了一些在二维平面上绘制的数据点；也就是说，x和y。红色点是异常值，因为它在这个数据系列中是一个非典型值：'
- en: '![Figure 3.19 – Identifying an outlier'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 3.19 – Identifying an outlier]'
- en: '](img/B16735_03_019.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16735_03_019.jpg]'
- en: Figure 3.19 – Identifying an outlier
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 3.19 – Identifying an outlier
- en: We want to treat outlier values because some statistical methods are impacted
    by them. Still, in the preceding chart, we can see this behavior in action. On
    the left-hand side, we drew a line that best fits those data points, ignoring
    the red point. On the right-hand side, we also drew the best line to fit the data
    but included the red point.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要处理异常值，因为某些统计方法会受到它们的影响。然而，在先前的图表中，我们可以看到这种行为的实际应用。在左侧，我们绘制了一条最佳拟合线，忽略了红色点。在右侧，我们也绘制了最佳拟合线来拟合数据，但包括了红色点。
- en: We can visually conclude that, by ignoring the outlier point, we will come up
    with a better solution on the plan of the left-hand side of the preceding chart
    since it was able to pass closer to most of the values. We can also prove this
    by computing an associated error for each line (which we will discuss later in
    this book).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 通过忽略异常值点，我们可以得出结论，在先前的图表左侧的平面图上，我们将得到一个更好的解决方案，因为它能够更接近大多数值。我们还可以通过计算每条线相关的误差来证明这一点（我们将在本书的后面讨论）。
- en: 'It is worth reminding you that you have also seen the outlier issue in action
    in another situation in this book: specifically, in *Figure 3.17*, when we had
    to deal with missing values. In that example, we used the median (instead of the
    mean) to work around the problem. Feel free to go back and read it again, but
    what should be very clear at this point is that median values are less impacted
    by outliers than average values.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 值得提醒的是，你也在本书的另一个情境中看到了异常值问题：具体来说，在*图3.17*中，当我们必须处理缺失值时。在那个例子中，我们使用了中位数（而不是平均值）来解决这个问题。你可以随时回去再次阅读，但此时应该非常清楚的是，中位数比平均值受异常值的影响较小。
- en: You now know what outliers are and why you should treat them. You should always
    consider your business perspective while dealing with outliers, but there are
    mathematical methods to find them. Now, let's look at these methods of outlier
    detection.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在知道了什么是异常值以及为什么你应该处理它们。在处理异常值时，你应该始终考虑你的业务视角，但存在数学方法来查找它们。现在，让我们来看看这些异常值检测的方法。
- en: 'You have already learned about the most common method: zscore. In *Figure 3.11*,
    we saw a table containing a set of ages. Refer to it again to refresh your memory.
    In the last column of that table, we are computing the zscore of each age, according
    to the equation shown in *Figure 3.10*.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学习了最常见的方法：z分数。在*图3.11*中，我们看到了一个包含一组年龄的表格。再次查阅它以刷新你的记忆。在那个表格的最后一列，我们根据*图3.10*中显示的公式计算每个年龄的z分数。
- en: 'There is no well-defined range for those zscore values; however, in a normal
    distribution *without* outliers, they will range between -3 and 3\. Remember:
    zscore will give you the number of standard deviations from the mean of the distribution.
    The following diagram shows some of the properties of a normal distribution:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这些z分数值没有明确的范围；然而，在一个没有异常值的正态分布中，它们将在-3和3之间。记住：z分数将给出分布平均值的标准差数。以下图表显示了正态分布的一些特性：
- en: '![Figure 3.20 – Normal distribution properties. Image adapted from https://pt.wikipedia.org/wiki/Ficheiro:The_Normal_Distribution.svg'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.20 – 正态分布特性。图片改编自https://pt.wikipedia.org/wiki/Ficheiro:The_Normal_Distribution.svg'
- en: '](img/B16735_03_020.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_020.jpg)'
- en: Figure 3.20 – Normal distribution properties. Image adapted from https://pt.wikipedia.org/wiki/Ficheiro:The_Normal_Distribution.svg
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.20 – 正态分布特性。图片改编自https://pt.wikipedia.org/wiki/Ficheiro:The_Normal_Distribution.svg
- en: According to the normal distribution properties, 95% of values will belong to
    the range of -2 and 2 standard deviations from the mean, while 99% of the values
    will belong to the range of -3 and 3\. Coming back to the outlier detection context,
    we can set thresholds on top of those zscore values to specify whether a data
    point is an outlier or not!
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 根据正态分布的性质，95%的值将属于平均值的-2和2个标准差范围内，而99%的值将属于平均值的-3和3个标准差范围内。回到异常值检测的上下文中，我们可以在这些z分数值上设置阈值，以指定一个数据点是否为异常值！
- en: 'There is no standard threshold that you can use to classify outliers. Ideally,
    you should look at your data and see what makes more sense for you… usually (this
    is not a rule), you will use some number between 2 and 3 standard deviations from
    the mean to set outliers, since more than 95% of your data will be out of that
    range. You may remember that there are outliers *below* and *above* the mean value
    of the distribution, as shown in the following table, where we have flagged outliers
    with an **absolute** zscore greater than 3 (the value column is hidden for the
    sake of this demonstration):'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 没有一个标准的阈值可以用来分类异常值。理想情况下，你应该查看你的数据，看看什么对你更有意义……通常（这不是规则），你将使用平均值的2到3个标准差之间的某个数字来设置异常值，因为超过95%的数据将超出这个范围。你可能记得，分布的均值*下方*和*上方*都有异常值，如下表所示，我们用**绝对**z分数大于3的异常值进行了标记（值列为了演示而隐藏）：
- en: '![Figure 3.21 – Flagging outliers according to the zscore value'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.21 – 根据z分数值标记异常值'
- en: '](img/B16735_03_021.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_021.jpg)'
- en: Figure 3.21 – Flagging outliers according to the zscore value
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.21 – 根据z分数值标记异常值
- en: 'We found two outliers in the preceding table: row number three and row number
    five. Another way to find outliers in the data is by applying the **box plot**
    logic. You will learn about box plots in more detail in the next chapter of this
    book. For now, let''s focus on using this method to find outliers.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在先前的表格中发现了两个异常值：第三行和第五行。另一种在数据中查找异常值的方法是应用**箱线图**逻辑。你将在本书的下一章中更详细地了解箱线图。现在，让我们专注于使用这种方法来查找异常值。
- en: When we look at a numerical variable, it is possible to extract many descriptive
    statistics from it, not only the mean, median, minimum, and maximum values, as
    we have seen previously. Another property that's present in data distributions
    is known as **quantiles**.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们观察一个数值变量时，我们可以从中提取许多描述性统计量，而不仅仅是之前看到的均值、中位数、最小值和最大值。数据分布中存在的一种属性被称为**分位数**。
- en: 'Quantiles are cut-off points that are established at regular intervals from
    the cumulative distribution function of a random variable. Those regular intervals,
    also known as *q-quantiles*, will be nearly the same size and will receive special
    names in some situations; for example:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 分位数是从随机变量的累积分布函数中按常规间隔建立的截断点。这些常规间隔，也称为*q分位数*，在某种情况下将几乎相同大小，并会得到特殊名称；例如：
- en: The 4-quantiles are called quartiles.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四分位数被称为四分位。
- en: The 10-quantiles are called deciles.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 10分位数被称为十分位数。
- en: The 100-quantiles are called percentiles.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 100分位数被称为百分位数。
- en: 'For example, the 20th percentile (of a 100-quantile regular interval) specifies
    that 20% of the data is below that point. In a box plot, we use regular intervals
    of 4-quantiles (also known as *quartiles*) to expose the distribution of the data
    (Q1 and Q3), as shown in the following diagram:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，20百分位数（100分位数常规间隔）指定了20%的数据低于该点。在箱线图中，我们使用4分位数的常规间隔（也称为*四分位*）来展示数据的分布（Q1和Q3），如下图中所示：
- en: '![Figure 3.22 – Box plot definition'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.22 – 箱线图定义'
- en: '](img/B16735_03_022.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_022.jpg)'
- en: Figure 3.22 – Box plot definition
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.22 – 箱线图定义
- en: Q1 is also known as the lower quartile or 25th quartile, and this means that
    25% of the data is below that point in the distribution. Q3 is also known as the
    upper quartile or 75th quartile, and this means that 75% of the data is below
    that point in the distribution.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Q1也被称为下四分位数或25分位数，这意味着25%的数据在分布中低于该点。Q3也被称为上四分位数或75分位数，这意味着75%的数据在分布中低于该点。
- en: Computing the difference between Q1 and Q3 will give you the **interquartile
    range (IQR)** value, which you can then use to compute the limits of the box plot,
    shown by the "minimum" and "maximum" labels in the preceding diagram.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 计算Q1和Q3之间的差异将给出**四分位距（IQR）**值，然后你可以使用这个值来计算箱线图的界限，如前图中“最小值”和“最大值”标签所示。
- en: After all, we can finally infer that anything below the "minimum" value or above
    the "maximum" value of the box plot will be flagged as an outlier.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以最终推断出，任何低于箱线图“最小值”或高于“最大值”的值都将被标记为异常值。
- en: 'You have now learned about two different ways you can flag outliers on your
    data: zscore and box plot. You can decide whether you are going to remove these
    points from your dataset or create another variable to specify that they are,
    indeed, outliers (as we did in *Figure 3.21*).'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经了解了两种不同的方法可以在你的数据上标记异常值：z分数和箱线图。你可以决定是否要从数据集中删除这些点，或者创建另一个变量来指定它们确实是异常值（如我们在*图3.21*中所做的那样）。
- en: Let's continue our journey of data preparation and look at other types of problems
    we will find in real life. Next, you will learn that several use cases have something
    known as **rare events**, which makes ML algorithms focus on the wrong side of
    the problem and propose bad solutions. Luckily, we will learn how to either tune
    them or prepare the data to make them smarter.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续我们的数据准备之旅，看看我们在现实生活中会遇到的其他类型的问题。接下来，你将了解到一些用例中存在所谓的**罕见事件**，这使得机器学习算法关注问题的错误一边，并提出不良解决方案。幸运的是，我们将学习如何调整它们或准备数据以使它们更智能。
- en: Dealing with unbalanced datasets
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理不平衡数据集
- en: At this point, I hope you have realized why data preparation is probably the
    longest part of our work. We have learned about data transformation, missing data
    values, and outliers, but the list of problems goes on. Don't worry – bear with
    me and let's master this topic together!
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我希望你已经意识到为什么数据准备可能是我们工作中最长的部分。我们已经学习了数据转换、缺失数据值和异常值，但问题列表还在继续。不用担心——请耐心等待，让我们一起掌握这个主题！
- en: Another well-known problem with ML models, specifically with binary classification
    problems, is unbalanced classes. In a binary classification model, we say that
    a dataset is unbalanced when most of its observations belong to the same class
    (target variable).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型中另一个众所周知的问题是，特别是在二元分类问题中，类别不平衡。在二元分类模型中，我们说当大多数观测值属于同一类别（目标变量）时，数据集是不平衡的。
- en: This is very common in fraud identification systems, for example, where most
    of the events belong to a regular operation, while a very small number of events
    belong to a fraudulent operation. In this case, we can also say that fraud is
    a rare event.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这在欺诈识别系统中非常常见，例如，其中大多数事件属于常规操作，而非常少数的事件属于欺诈操作。在这种情况下，我们也可以说欺诈是一个罕见事件。
- en: There is no strong rule for defining whether a dataset is unbalanced or not,
    in the sense of it being necessary to worry about it. Most challenge problems
    will present more than 99% of the observations in the majority class.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义数据集是否不平衡时没有强规则，从需要担心它的意义上讲。大多数挑战问题将呈现超过99%的观察值属于多数类。
- en: 'The problem with unbalanced datasets is very simple: ML algorithms will try
    to find the best fit in the training data to maximize their accuracy. In a dataset
    where 99% of the cases belong to one single class, without any tuning, the algorithm
    is likely to prioritize the assertiveness of the majority class. In the worst-case
    scenario, it will classify all the observations as the majority class and ignore
    the minority one, which is usually our interest when modeling.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡数据集的问题非常简单：机器学习算法将试图在训练数据中找到最佳拟合以最大化其准确性。在一个99%的案例属于一个单一类的数据集中，如果没有调整，算法可能会优先考虑多数类的确定性。在最坏的情况下，它将把所有观察值分类为多数类，并忽略少数类，而这通常是我们建模时的兴趣所在。
- en: 'To deal with unbalanced datasets, we have two major directions we can follow:
    tuning the algorithm to handle the issue or resampling the data to make it more
    balanced.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 要处理不平衡数据集，我们可以遵循两个主要方向：调整算法来处理这个问题或重新采样数据以使其更平衡。
- en: By tuning the algorithm, you have to specify the weight of each class under
    classification. This class weight configuration belongs to the algorithm, not
    to the training data, so it is a hyperparameter setting. It is important to keep
    in mind that not all algorithms will have that type of configuration, and that
    not all ML frameworks will expose it, either. As a quick reference, we can mention
    the `DecisionTreeClassifier` class, from the scikit-learn ML library, as a good
    example that does implement the class weight hyperparameter.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整算法，你必须指定分类中每个类的权重。这种类权重配置属于算法，而不是训练数据，因此它是一个超参数设置。重要的是要记住，并非所有算法都会有那种配置，也不是所有机器学习框架都会公开它。作为一个快速参考，我们可以提到来自scikit-learn机器学习库的`DecisionTreeClassifier`类，作为一个实现了类权重超参数的好例子。
- en: Another way to work around unbalanced problems is changing the training dataset
    by applying **undersampling** or **oversampling**. If you decide to apply undersampling,
    all you have to do is remove some observations from the majority class until you
    get a more balanced dataset. Of course, the downside of this approach is that
    you may lose important information about the majority class that you are removing
    observations from.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 解决不平衡问题的另一种方法是通过对训练数据集进行**下采样**或**上采样**来改变。如果你决定采用下采样，你所要做的就是从多数类中移除一些观察值，直到你得到一个更平衡的数据集。当然，这种方法的一个缺点是，你可能会丢失关于你正在移除观察值的多数类的重要信息。
- en: The most common approach for undersampling is known as random undersampling,
    which is a naïve resampling approach where we randomly remove some observations
    from the training set.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的下采样方法是随机下采样，这是一种天真重采样方法，其中我们随机从训练集中移除一些观察值。
- en: On the other hand, you can decide to go for oversampling, where you will create
    new observations/samples of the minority class. The simplest approach is the naïve
    one, where you randomly select observations from the training set (with replacement)
    for duplication. The downside of this method is the potential issue of overfitting,
    since you will be duplicating/highlighting the observed pattern of the minority
    class.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，你可以选择进行上采样，在这种情况下，你将创建少数类的新的观察值/样本。最简单的方法是天真方法，其中你从训练集中随机选择观察值（带替换）进行复制。这种方法的一个缺点是可能存在过拟合的问题，因为你将复制/突出显示少数类的观察模式。
- en: To either underfit or overfit of your model, you should always test the fitted
    model on your testing set.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 要避免你的模型欠拟合或过拟合，你应该始终在测试集上测试拟合好的模型。
- en: Important note
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'The testing set cannot be under/over sampled: only the training set should
    pass through these resampling techniques.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集不能进行下采样/上采样：只有训练集应该通过这些重采样技术。
- en: 'You can also oversample the training set by applying synthetic sampling techniques.
    Random oversample does not add any new information to the training set: it just
    duplicates the existing ones. By creating synthetic samples, you are deriving
    those new observations from the existing ones (instead of simply copying them).
    This is a type of data augmentation technique known as the **Synthetic Minority
    Oversampling Technique** (**SMOTE**).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过应用合成采样技术来过采样训练集。随机过采样不会向训练集添加任何新信息：它只是复制现有的。通过创建合成样本，你是从现有样本中推导出那些新观察结果（而不是简单地复制它们）。这是一种称为**合成少数过采样技术**（SMOTE）的数据增强技术。
- en: Technically, what SMOTE does is plot a line in the feature space of the minority
    class and extract points that are close to that line.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，SMOTE所做的是在少数类的特征空间中绘制一条线，并提取接近该线的点。
- en: Important note
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'You may find questions in your exam where the term SMOTE has been used. If
    that happens, keep the context where this term is applied in mind: oversampling.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会在考试中发现使用过SMOTE这个术语的问题。如果发生这种情况，请记住这个术语应用时的上下文：过采样。
- en: Now, let's move on to the next topic regarding data preparation, where we will
    learn how to prepare text data for machine learning models.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续下一个关于数据准备的主题，我们将学习如何为机器学习模型准备文本数据。
- en: Dealing with text data
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理文本数据
- en: We have already learned how to transform categorical features into numerical
    representations, either using label encoders, ordinal encoders, or one-hot encoding.
    However, what if we have fields containing long piece of text in our dataset?
    How are we supposed to provide a mathematical representation for them in order
    to properly feed ML algorithms? This is a common issue in **natural language processing**
    (**NLP**), a subfield of AI.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何将分类特征转换为数值表示，无论是使用标签编码器、顺序编码器还是独热编码。然而，如果我们数据集中有包含长文本的字段怎么办？我们该如何为它们提供数学表示，以便正确地输入机器学习算法？这是一个在**自然语言处理**（NLP），人工智能的一个子领域中常见的问题。
- en: NLP models aim to extract knowledge from texts; for example, translating text
    between languages, identifying entities in a corpus of text (also known as **Name
    Entity Recognition** (**NER**)), classifying sentiments from a user review, and
    many other applications.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: NLP模型旨在从文本中提取知识；例如，在语言之间翻译文本、在文本语料库中识别实体（也称为**命名实体识别**（NER））、从用户评论中分类情感，以及许多其他应用。
- en: Important note
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In [*Chapter 2*](B16735_02_Final_VK_ePub.xhtml#_idTextAnchor032)*, AWS Application
    Services for AI/ML*, you learned about some AWS application services that apply
    NLP to their solutions, such as Amazon Translate and Amazon Comprehend. During
    the exam, you might be asked to think about the fastest or easiest way (with the
    least development effort) to build certain types of NLP applications. The fastest
    or easiest way is usually to use those out of the box AWS services, since they
    offer pre-trained models for some use cases (especially machine translation, sentiment
    analysis, topic modeling, document classification, and entity recognition).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第2章*](B16735_02_Final_VK_ePub.xhtml#_idTextAnchor032)*中，AWS人工智能/机器学习应用服务*，你了解了一些将NLP应用于其解决方案的AWS应用服务，例如Amazon
    Translate和Amazon Comprehend。在考试中，你可能会被要求思考构建某些类型NLP应用的最快或最简单的方法（开发工作量最小）。通常，最快或最简单的方法是使用那些现成的AWS服务，因为它们为某些用例提供了预训练模型（尤其是机器翻译、情感分析、主题建模、文档分类和实体识别）。
- en: In a few chapters' time, you will also learn about some built-in AWS algorithms
    for NLP applications, such as BlazingText, **Latent Dirichlet Allocation** (**LDA**),
    **Neural Topic Modeling**, and the Sequence-to-Sequence algorithm. Those algorithms
    also let you create the same NLP solutions that are created by those out of box
    services; however, you have to use them on SageMaker and write your own solution.
    In other words, they offer more flexibility, but demand more development effort.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几章中，你还将了解一些用于NLP应用的内置AWS算法，例如BlazingText、**潜在狄利克雷分配**（LDA）、**神经主题建模**和序列到序列算法。这些算法也让你可以创建与那些现成服务创建的相同NLP解决方案；然而，你必须使用SageMaker并编写自己的解决方案。换句话说，它们提供了更多的灵活性，但需要更多的开发工作量。
- en: Keep that in mind for your exam!
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在考试中请记住这一点！
- en: Although AWS offers many out of the box services and built-in algorithms that
    allow us to create NLP applications, we will not look at those AWS product features
    now (as we did in [*Chapter 2*](B16735_02_Final_VK_ePub.xhtml#_idTextAnchor032)*,
    AWS Application Services for AI/ML*, and will do so again in [*Chapter 7*](B16735_07_Final_VK_ePub.xhtml#_idTextAnchor136)*,
    Applying Machine Learning Algorithms*). We will finish this chapter by looking
    at some data preparation techniques that are extremely important for preparing
    your data for NLP.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然AWS提供了许多开箱即用的服务和内置算法，允许我们创建NLP应用，但现在我们不会查看这些AWS产品特性（如我们在[*第2章*](B16735_02_Final_VK_ePub.xhtml#_idTextAnchor032)*，AWS人工智能/机器学习应用服务*中所述，我们将在[*第7章*](B16735_07_Final_VK_ePub.xhtml#_idTextAnchor136)*，应用机器学习算法*中再次讨论）。我们将通过查看一些数据准备技术来结束本章，这些技术对于准备你的数据以进行NLP非常重要。
- en: Bag of words
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词袋模型
- en: 'The first one we will cover is known as **bag of words** (**BoW**). This is
    a very common and simple technique, applied to text data, that creates matrix
    representations to describe the number of words within the text. BoW consists
    of two main steps: creating a vocabulary and creating a representation of the
    presence of those known words from the vocabulary in the text. These steps can
    be seen in the following diagram:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍的是所谓的**词袋模型**（**BoW**）。这是一个非常常见且简单的技术，应用于文本数据，通过创建矩阵表示来描述文本中的单词数量。BoW包括两个主要步骤：创建词汇表和创建表示文本中已知词汇出现情况。这些步骤可以在以下图中看到：
- en: '![Figure 3.23 – BoW in action'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.23 – 词袋模型的应用'
- en: '](img/B16735_03_023.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B16735_03_023.jpg)'
- en: Figure 3.23 – BoW in action
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.23 – 词袋模型的应用
- en: First things first, we usually can't use raw texts to prepare a bag of words
    representation. There is a data cleansing step where we will lowercase the text;
    split each work into tokens; remove punctuation, non-alphabetical, and stop words;
    and, whenever necessary, apply any other custom cleansing technique you may want.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通常不能使用原始文本来准备词袋模型表示。这里有一个数据清洗步骤，我们将文本转换为小写；将每个单词拆分为标记；删除标点符号、非字母字符和停用词；并在必要时，应用任何其他你可能想要的定制清洗技术。
- en: Once you have cleansed your raw text, you can add each word to a global vocabulary.
    Technically, this is usually a dictionary of tuples, in the form of (word, number
    of occurrences); for example, {(apple, 10), (watermelon, 20)}. As I mentioned
    previously, this is a global dictionary, and you should consider all the texts
    you are analyzing.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你清洗了原始文本，你就可以将每个单词添加到全局词汇中。技术上，这通常是一个元组的字典，形式为（单词，出现次数）；例如，{(apple, 10), (watermelon,
    20)}。如我之前提到的，这是一个全局字典，你应该考虑你正在分析的所有文本。
- en: Now, with the cleansed text and updated vocabulary, we can build our text representation
    in the form of a matrix, where each column represents one word from the global
    vocabulary and each row represents a text you have analyzed. The way you represent
    those texts on each row may vary according to different strategies, such as binary,
    frequency, and count. Let's dive into these strategies a little more.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有了清洗后的文本和更新的词汇表，我们可以以矩阵的形式构建我们的文本表示，其中每一列代表全局词汇中的一个单词，每一行代表你已分析的文本。你如何在每一行中表示这些文本可能因不同的策略而异，例如二进制、频率和计数。让我们更深入地探讨这些策略。
- en: In the preceding diagram, we are processing a single text but trying the three
    different strategies for bag of words. That's why you can see three rows on that
    table, instead of just one (in a real scenario, you have to choose one of them
    for implementation).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们正在处理单个文本，但尝试了三种不同的词袋策略。这就是为什么你可以在表格上看到三行，而不是一行（在实际场景中，你必须从中选择一个进行实施）。
- en: In the first row, we have used a binary strategy, which will assign 1 if the
    word exists in the global vocabulary and 0 if not. Because our vocabulary was
    built on a single text, all the words from that text belong to the vocabulary
    (the reason you can only see 1s in the binary strategy).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行，我们使用了二进制策略，如果单词存在于全局词汇中，则分配 1，否则分配 0。因为我们的词汇是在单个文本上构建的，所以该文本中的所有单词都属于词汇（这就是为什么在二进制策略中你只能看到
    1 的原因）。
- en: In the second row, we have used a frequency strategy, which will check the number
    of occurrences of each word within the text and divide it by the total number
    of words within the text. For example, the word "this" appears just once (1) and
    there are seven other words in the text (7), so 1/7 is equal to 0.14.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二行，我们使用了频率策略，该策略会检查文本中每个单词出现的次数，并将其除以文本中单词的总数。例如，单词 "this" 只出现一次（1），而文本中还有七个其他单词（7），所以
    1/7 等于 0.14。
- en: Finally, in the third row, we have used a count strategy, which is a simple
    count of occurrences of each word within the text.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在第三行，我们使用了计数策略，这是一种简单的对文本中每个单词出现次数的计数。
- en: Important note
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: This note is really important – you are likely to find it in your exam. You
    may have noticed that our BoW matrix contains **unique words** in the *columns*
    and **each text** representation is in the *rows*. If you have 100 long texts
    with only 50 unique words across them, your BoW matrix will have 50 columns and
    100 rows. During your exam, you are likely to receive a list of texts and be asked
    to prepare the BoW matrix.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提示非常重要——你很可能在考试中找到它。你可能已经注意到，我们的BoW矩阵在*列*中包含**独特单词**，而每个文本的表示都在*行*中。如果你有100篇很长的文本，它们之间只有50个独特的单词，你的BoW矩阵将有50列和100行。在考试中，你很可能会收到一个文本列表，并被要求准备BoW矩阵。
- en: There is one more extremely important concept you should know about BoW, which
    is the **n-gram** configuration. The term n-gram is used to describe the way you
    would like to look at your vocabulary, either via single words (uni-gram), groups
    of two words (bi-gram), groups of three words (tri-gram), or even groups of n
    words (n-gram). So far, we have seen BoW representations using a uni-gram approach,
    but more sophisticated representations of BoW may use bi-grams, tri-grams, or
    n-grams.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该了解关于BoW的一个极其重要的概念，那就是**n-gram**配置。n-gram这个术语用来描述你希望如何查看你的词汇，无论是通过单个单词（单语元），两个单词的组合（双语元），三个单词的组合（三元组），甚至是n个单词的组合（n-gram）。到目前为止，我们已经看到了使用单语元方法的BoW表示，但更复杂的BoW表示可能使用双语元、三元组或n-gram。
- en: 'The main logic itself does not change, but you need to know how to represent
    n-grams in BoW. Still using our example from the preceding diagram, a bi-gram
    approach would combine those words in the following way: [this movie, movie really,
    really good, good although, although old, old production]. Make sure you understand
    this before taking the exam.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 主要逻辑本身并没有改变，但你需要知道如何在BoW中表示n-gram。仍然使用我们前面图中的例子，双语元方法会以以下方式组合这些单词：[这部电影，电影真的很，真的很棒，好尽管，尽管老，老制作]。在考试之前，确保你理解这一点。
- en: Important note
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The power and simplicity of BoW comes from the fact that you can easily come
    up with a training set to test your algorithms, or even create a baseline model.
    If you look at *Figure 3.23*, can you see that having more data and just adding
    a classification column to that table, such as good or bad review, would allow
    us to train a binary classification model to predict sentiments?
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: BoW的强大和简单之处在于你可以轻松地构建一个训练集来测试你的算法，甚至创建一个基线模型。如果你看*图3.23*，你能看到拥有更多数据和仅仅在表中添加一个分类列，比如好评或差评，这将使我们能够训练一个二元分类模型来预测情感吗？
- en: Alright – you might have noticed that many of the awesome techniques that I
    have introduced for you come with some downsides. The problem with BoW is the
    challenge of maintaining its vocabulary. We can easily see that, in a huge corpus
    of texts, the vocabulary size tends to become bigger and bigger and the matrices'
    representations tend to be sparse (I know – the sparsity issue again).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧——你可能已经注意到，我为你介绍的大多数令人惊叹的技术都伴随着一些缺点。BoW的问题在于维护其词汇表的挑战。我们很容易看到，在一个巨大的文本语料库中，词汇量往往会越来越大，而矩阵表示往往会变得稀疏（我知道——又是稀疏性问题）。
- en: One possible way to solve the vocabulary size issue is by using word hashing
    (also known in ML as the **hashing trick**). Hash functions map data of arbitrary
    sizes to data of a fixed size. This means you can use the hash trick to represent
    each text with a fixed number of features (regardless of the vocabulary's size).
    Technically, this hashing space allows collisions (different texts represented
    by the same features), so this is something to take into account when you're implementing
    feature hashing.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 解决词汇量问题的可能方法之一是使用词哈希（在机器学习中也称为**哈希技巧**）。哈希函数将任意大小的数据映射到固定大小的数据。这意味着你可以使用哈希技巧用固定数量的特征来表示每个文本（无论词汇的大小）。技术上，这个哈希空间允许碰撞（不同的文本由相同的特征表示），所以在实现特征哈希时需要考虑这一点。
- en: TF-IDF
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Another problem that comes with BoW, especially when we use the frequency strategy
    to build the feature space, is that more frequent words will strongly boost their
    scores due to the high number of occurrences within the document. It turns out
    that, often, those words with high occurrences are not the key words of the document,
    but just other words that *also* appear many times in several other documents.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 与BoW（词袋模型）相关联的另一个问题是，尤其是在我们使用频率策略构建特征空间时，更频繁出现的单词会因为文档中出现的次数多而显著提高它们的分数。结果往往是，这些高频出现的单词并不是文档的关键词，而只是也在其他几份文档中多次出现的其他单词。
- en: '**Term Frequency – Inverse Term Frequency** (**TF-IDF**) helps penalize these
    types of words, by checking how frequent they are in other documents and using
    that information to rescale the frequency of the words within the document.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '**词频-逆词频（TF-IDF**）通过检查它们在其他文档中的频率并使用这些信息来重新调整文档中单词的频率，帮助惩罚这些类型的单词。'
- en: At the end of the process, TF-IDF tends to give more importance to words that
    are unique to the document (document-specific words). Let's look at a concrete
    example so that you can understand it in depth.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理结束时，TF-IDF倾向于给予文档中独特的单词（文档特定单词）更多的重视。让我们看看一个具体的例子，以便你能深入理解。
- en: Consider that we have a text corpus containing 100 words and the word "Amazon"
    appears three times. The **Term Frequency** (**TF**) of this word would be 3/100,
    which is equal to 0.03\. Now, suppose we have other 1,000 documents and that the
    word "Amazon" appears in 50 of these. In this case, the **Inverse Term Frequency**
    (**IDF**) would be given by the log as 1,000/50, which is equal to 1.30\. The
    TF-IDF score of the word "Amazon", in that specific document under analysis, will
    be the product of TF * IDF, which is 0.03 * 1.30 (*0.039*).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们有一个包含100个单词的语料库，其中单词"Amazon"出现了三次。这个单词的**词频（TF**）将是3/100，等于0.03。现在，假设我们还有其他1,000份文档，其中"Amazon"单词出现在这50份文档中。在这种情况下，**逆词频（IDF**）将由对数给出，即1,000/50，等于1.30。在分析的具体文档中，单词"Amazon"的TF-IDF分数将是TF
    * IDF的乘积，即0.03 * 1.30（0.039）。
- en: Let's suppose that instead of 50 documents, the word "Amazon" had also appeared
    on another 750 documents – in other words, much more frequently than in the prior
    scenario. In this case, we will not change the TF part of this equation – it is
    still 0.03\. However, the IDF piece will change a little since this time, it will
    be log 1,000/750, which is equal to *0.0036*. As you can see, this time, the word
    "Amazon" has much less importance than in the previous example.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，除了50份文档外，单词"Amazon"还出现在另外750份文档上——换句话说，比先前的场景出现得更频繁。在这种情况下，我们将不会改变这个方程的TF部分——它仍然是0.03。然而，IDF部分将略有变化，因为这次，它将是log
    1,000/750，等于0.0036。正如你所看到的，这次，单词"Amazon"的重要性比先前的例子要小得多。
- en: Word embedding
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Unlike traditional approaches, such as BoW and TD-IDF, modern methods of text
    representation will take care of the context of the information, as well as the
    presence or the frequency of words. One very popular and powerful approach that
    follows this concept is known as **word embedding**. Word embeddings create a
    dense vector of a fixed length that can store information about the context and
    meaning of the document.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的BoW（词袋模型）和TD-IDF等方法不同，现代文本表示方法将注意信息的上下文，以及单词的存在或频率。遵循这一概念的一个非常流行且强大的方法被称为**词嵌入**。词嵌入创建一个固定长度的密集向量，可以存储关于文档上下文和意义的信息。
- en: Each word is represented by a data point in a multidimensional hyper plan, which
    we call an embedding space. This embedding space will have "n" dimensions, where
    each of these dimensions refers to a particular position of this dense vector.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词在多维超平面上的一个数据点表示，我们称之为嵌入空间。这个嵌入空间将有"n"个维度，其中每个维度都指代这个密集向量的特定位置。
- en: 'Although it may sound confusing, the concept it is actually pretty simple.
    Let''s suppose we have a list of four words, and we want to plot them in an embedding
    space of five dimensions. The words are king, queen, live, and castle. The following
    table shows how to do this:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能听起来有些令人困惑，但这个概念实际上相当简单。假设我们有一个包含四个单词的列表，我们想在五维嵌入空间中绘制它们。这些单词是king、queen、live和castle。以下表格显示了如何做到这一点：
- en: '![Figure 3.24 – An embedding space representation'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.24 – 嵌入空间表示'
- en: '](img/B16735_03_024.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16735_03_024.jpg]'
- en: Figure 3.24 – An embedding space representation
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.24 – 嵌入空间表示
- en: Forget the hypothetical numbers in the preceding table and focus on the data
    structure; you will see that each word is now represented by "n" dimensions in
    the embedding space. This process of transforming words into vectors can be performed
    by many different methods, but the most popular ones are **word2vec** and **GloVe**.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 忘记前面表格中的假设数字，专注于数据结构；你会看到，现在每个单词在嵌入空间中由 "n" 维度表示。将单词转换为向量的这个过程可以通过许多不同的方法来完成，但最流行的是
    **word2vec** 和 **GloVe**。
- en: Once you have each word represented as a vector of a fixed length, you can apply
    many other techniques to do whatever you need. One very common task is plotting
    those "words" (actually, their dimensions) in a hyper plan and, visually, checking
    how close they are to each other!
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦每个单词都表示为固定长度的向量，你就可以应用许多其他技术来完成你需要做的事情。一个非常常见的任务是在超平面中绘制这些 "单词"（实际上，它们的维度），并直观地检查它们彼此之间的接近程度！
- en: Technically, we don't use this to plot them as-is, since human brains cannot
    interpret more than three dimensions. Then, we usually apply a dimensionality
    reduction technique (such as principal component analysis, which you will learn
    about later) to reduce the number of dimensions to two, and finally plot the words
    in a cartesian plan. That's why you might have seen pictures like the one at the
    bottom of the following diagram. Have you ever asked yourself how is it possible
    to plot words on a graph?
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 技术上，我们不会直接用这个来绘制它们，因为人类大脑无法解释超过三个维度。然后，我们通常应用降维技术（例如你稍后将要学习的主成分分析），将维度数减少到两个，最后在笛卡尔平面上绘制单词。这就是为什么你可能看到下面图表底部的图片。你有没有想过如何在图表上绘制单词？
- en: '![Figure 3.25 – Plotting words'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.25 – 绘制单词'
- en: '](img/B16735_03_025.jpg)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_025.jpg)'
- en: Figure 3.25 – Plotting words
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.25 – 绘制单词
- en: In case you are wondering how we came up with the dimensions shown in *Figure
    3.24*, let's dive into it. Again, there are different methods to do this, but
    let's look at one of the most popular, which uses a co-occurrence matrix with
    a fixed context window.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道我们是如何得到 *图 3.24* 中显示的维度的，让我们深入探讨。同样，有不同方法来做这件事，但让我们看看最流行的一种，它使用固定上下文窗口的共现矩阵。
- en: First, we have to come up with some logic to represent each word, keeping in
    mind that we also have to take their context into consideration. To solve the
    context requirement, we will define a **fixed context window**, which is going
    to be responsible for specifying how many words will be grouped together for context
    learning. For instance, let's set this fixed context window to 2.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须想出一些逻辑来表示每个单词，同时考虑到我们还要考虑它们的上下文。为了解决上下文要求，我们将定义一个**固定上下文窗口**，它将负责指定将有多少个单词一起用于上下文学习。例如，让我们将这个固定上下文窗口设置为
    2。
- en: 'Next, we will create a **co-occurrence matrix**, which will count the number
    of occurrences of each pair of words, according to the pre-defined context window.
    Let''s see this in action. Consider the following text: "I will pass this exam,
    you will see. I will pass it".'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个**共现矩阵**，它将根据预定义的上下文窗口计算每对单词的出现次数。让我们看看它是如何工作的。考虑以下文本："I will pass
    this exam, you will see. I will pass it"。
- en: 'The context window of the first word "pass" would be the ones in *bold*: "*I
    will* pass *this exam*, you will see. I will pass it". Considering this logic,
    let''s see how many times each pair of words appears in the context window:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个单词 "pass" 的上下文窗口将是以下这些加粗的："*I will* pass *this exam*, you will see. I will
    pass it"。考虑到这个逻辑，让我们看看每个单词对在上下文窗口中出现的次数：
- en: '![Figure 3.26 – Co-occurrence matrix'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.26 – 共现矩阵'
- en: '](img/B16735_03_026.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_026.jpg)'
- en: Figure 3.26 – Co-occurrence matrix
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.26 – 共现矩阵
- en: 'As you can see, the pair of words "I will" appears three times when we use
    a context window of size 2:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，当我们使用大小为 2 的上下文窗口时，单词对 "I will" 出现了三次：
- en: '*I will* pass this exam, you will see. I will pass it.'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*I will* pass this exam, you will see. I will pass it。'
- en: I will pass this exam, you *will* see. *I* will pass it.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我会通过这次考试，你会看到的。*我会*通过它。
- en: I will pass this exam, you will see. *I will* pass it.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我会通过这次考试，你会看到的。*I will* pass it。
- en: Looking at the preceding table, the same logic should be applied to all other
    pairs of words, replacing "…" with the associated number of occurrences. You now
    have a numerical representation for each word!
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 观察前面的表格，应该将相同的逻辑应用于所有其他单词对，用相关的出现次数替换 "…"。你现在为每个单词都有了数值表示！
- en: Important note
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You should be aware that there many alternatives to co-occurrence matrices with
    a fixed context window, such as using TD-IDF vectorization or even simpler counters
    of words per document. The most important message here is that, somehow, you must
    come up with a numerical representation for each word.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该知道，与固定上下文窗口的共现矩阵相比，有许多替代方案，例如使用TD-IDF向量化或甚至更简单的文档中单词计数。这里最重要的信息是，无论如何，你必须为每个单词提供一个数值表示。
- en: 'The last step is finally finding those dimensions shown in *Figure 3.24*. You
    can do this by creating a multilayer model, usually based on neural networks,
    where the hidden layer will represent your embedding space. The following diagram
    shows a simplified example where we could potentially compress our words shown
    in the preceding table into an embedding space of five dimensions:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是最终找到*图3.24*中显示的那些维度。你可以通过创建一个多层模型来实现，通常基于神经网络，其中隐藏层将代表你的嵌入空间。以下图示展示了我们可以将前面表格中显示的单词压缩到五个维度的简化示例：
- en: '![Figure 3.27 – Building embedding spaces with neural networks'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.27 – 使用神经网络构建嵌入空间'
- en: '](img/B16735_03_027.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_027.jpg)'
- en: Figure 3.27 – Building embedding spaces with neural networks
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.27 – 使用神经网络构建嵌入空间
- en: We will talk about neural networks in more detail later in this book. For now,
    understanding where the embedding vector comes from is already an awesome achievement!
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的后面部分更详细地讨论神经网络。现在，理解嵌入向量来自何处已经是一项了不起的成就！
- en: Another important thing you should keep in mind while modeling natural language
    problems is that you can reuse a pre-trained embedding space in your models. Some
    companies have created modern neural network architectures, trained on billions
    of documents, which has become the state of the art in this field. For your reference,
    take a look at **Bidirectional Encoder Representations from Transformers** (**BERT**),
    which was proposed by Google and has been widely used by the data science community
    and industry.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模自然语言问题时，你还需要记住的另一件重要事情是，你可以在你的模型中重用预训练的嵌入空间。一些公司已经创建了基于数十亿文档的现代神经网络架构，这已经成为该领域的尖端技术。为了参考，请查看由谷歌提出的**双向编码器表示的Transformer**（**BERT**），它已被数据科学社区和行业广泛使用。
- en: We have now reached the end of this long – but very important – chapter about
    data preparation and transformation. Let's take this opportunity to do a quick
    recap of the awesome things we have learned.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经到达了关于数据准备和转换的漫长——但非常重要——章节的结尾。让我们抓住这个机会，快速回顾一下我们学到的精彩内容。
- en: Summary
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: First, you were introduced to the different types of features that you might
    have to work with. Identifying the type of variable you'll be working with is
    very important for defining the types of transformations and techniques that can
    be applied to each case.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你被介绍到了你可能需要与之合作的特征类型。确定你将与之合作的变量的类型对于定义可以应用于每种情况的转换和技术的类型非常重要。
- en: Then, we learned how to deal with categorical features. We saw that, sometimes,
    categorical variables do have an order (such as the ordinal ones), while other
    times, they don't (such as the nominal ones). You learned that one-hot encoding
    (or dummy variables) is probably the most common type of transformation for nominal
    features; however, depending on the number of unique categories, after applying
    one-hot encoding, your data might suffer from sparsity issues. Regarding ordinal
    features, you shouldn't create dummy variables on top of them, since you would
    be losing the information of order that's been incorporated into the variable.
    In those cases, ordinal encoding is the most appropriate transformation.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们学习了如何处理分类特征。我们看到了，有时，分类变量确实有顺序（例如有序变量），而有时则没有（例如名义变量）。你了解到，独热编码（或虚拟变量）可能是名义特征的转换中最常见的一种；然而，根据唯一类别的数量，在应用独热编码后，你的数据可能会出现稀疏性问题。至于有序特征，你不应该在它们之上创建虚拟变量，因为这样你会丢失变量中已经包含的顺序信息。在这些情况下，有序编码是最合适的转换。
- en: We continued our journey by looking at numerical features, where we learned
    how to deal with continuous and discrete data. We walked through the most important
    types of transformations, such as normalization, standardization, binning, and
    discretization. You saw that some types of transformation rely on the underlying
    data to find their parameters, so it is very important to avoid using the testing
    set to learn anything from the data (it must be used strictly for testing).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续我们的旅程，通过查看数值特征，我们学习了如何处理连续和离散数据。我们探讨了最重要的转换类型，如归一化、标准化、分箱和离散化。你看到，某些类型的转换依赖于底层数据来找到它们的参数，因此非常重要，避免使用测试集从数据中学习任何东西（它必须严格用于测试）。
- en: You have also seen that we can even apply pure math to transform our data; for
    example, you learned that power transformations can be used to reduce the skewness
    of your feature and make it more normal.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 你还看到，我们甚至可以将纯数学应用于转换我们的数据；例如，你了解到幂变换可以用来减少特征的偏度，使其更加正常。
- en: Next, we looked at missing data and got a sense for how critical this task is.
    When you are modeling, you *can't* look at the missing values as a simple computational
    problem, where you just have to replace x with y. This is a much bigger problem
    where you need to start solving it by exploring your data, and then checking if
    your missing data was generated at random or not.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们研究了缺失数据，并了解了这项任务的重要性。当你建模时，你不能将缺失值视为一个简单的计算问题，其中你只需要用y替换x。这是一个更大的问题，你需要通过探索你的数据来开始解决这个问题，然后检查你的缺失数据是否是随机生成的。
- en: When you are making the decision to remove or replace missing data, you must
    be aware that you are either losing information or adding bias to the data, respectively.
    Remember to review all the important notes we gave you, since they are likely
    to be, somehow, present in your exam.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 当你决定删除或替换缺失数据时，你必须意识到你可能会丢失信息或向数据添加偏差，分别如此。请记住回顾我们给你提供的所有重要笔记，因为它们很可能以某种方式出现在你的考试中。
- en: Next, you learned about outlier detection. You looked at different ways to find
    outliers, such as the zscore and box plot approaches. Most importantly, you learned
    that you can either flag or smooth them.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你学习了关于异常值检测的内容。你研究了不同的方法来找到异常值，如z分数和箱线图方法。最重要的是，你学习了你可以标记或平滑它们。
- en: At the start, I told you that this chapter would be a long but worthwhile journey
    about data preparation, which is why I needed to give you a good sense of how
    to deal with rare events, since this is one the most challenging problems on ML.
    You learned that, sometimes, your data might be unbalanced, and you must either
    trick your algorithm (by changing the class weight) or resample your data (applying
    undersampling and oversampling).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始时，我告诉你这一章将是一段漫长但值得的旅程，关于数据准备，这就是为什么我需要给你一个很好的感觉，了解如何处理罕见事件，因为这是机器学习中最具挑战性的问题之一。你了解到，有时你的数据可能是不平衡的，你必须要么通过改变类别权重来欺骗你的算法，要么通过应用欠采样和过采样来重新采样你的数据。
- en: Finally, you learned how to deal with text data for NLP. You should now be able
    to manually compute bag of words and TF-IDF matrices! We went even deeper and
    learned how word embedding works. During this subsection, we learned that we can
    either create our own embedding space (using many different methods) or reuse
    a pre-trained one, such as BERT.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你学习了如何处理NLP中的文本数据。你现在应该能够手动计算词袋和TF-IDF矩阵！我们甚至更深入地学习了词嵌入的工作原理。在本小节中，我们了解到我们可以创建自己的嵌入空间（使用许多不同的方法）或重用预训练的一个，例如BERT。
- en: We are done! I am so glad you made it here and I am sure this chapter is crucial
    to your success in the exam. Finally, we have prepared some practice questions
    for you; I hope you enjoy them.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完成了！我很高兴你做到了，我确信这一章对你的考试成功至关重要。最后，我们为你准备了一些练习题；我希望你会喜欢它们。
- en: In the next chapter, we will dive into data visualization techniques.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入研究数据可视化技术。
- en: Questions
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: You are working as a data scientist for a healthcare company and are creating
    a machine learning model to predict fraud, waste, and abuse across the company's
    claims. One of the features of this model is the number of times a particular
    drug has been prescribed, to the same patient of the claim, in a period of 2 years.
    Which type of feature is this?
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你正在为一家医疗保健公司担任数据科学家，并创建一个机器学习模型来预测公司索赔中的欺诈、浪费和滥用。该模型的一个特征是，在2年期间，特定药物被开具给索赔相同患者的次数。这是哪种类型的功能？
- en: a) Discrete
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 离散型
- en: b) Continuous
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 连续型
- en: c) Nominal
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 名义型
- en: d) Ordinal
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 序数
- en: Answer
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: a, The feature is counting the number of times that a particular drug has been
    prescribed. Individual and countable items are classified as discrete data.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a, 该特征是计算特定药物被开具的次数。个体和可数项被归类为离散数据。
- en: 'You are building a ML model for an educational group that owns schools and
    universities across the globe. Your model aims to predict how likely a particular
    student is to leave his/her studies. Many factors may contribute to school dropout,
    but one of your features is the current academic stage of each student: preschool,
    elementary school, middle school, or high school. Which type of feature is this?'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你正在为一家拥有全球学校和大学的培训机构构建一个机器学习模型。你的模型旨在预测特定学生离开其学习的机会。许多因素可能导致辍学，但你的一个特征是每个学生的当前学术阶段：幼儿园、小学、中学或高中。这是什么类型的特征？
- en: a) Discrete
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 离散
- en: b) Continuous
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 连续
- en: c) Nominal
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 名义
- en: d) Ordinal
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 序数
- en: Answer
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: d, The feature has an implicit order and should be considered a categorical/ordinal
    variable.
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d, 该特征具有隐含的顺序，应被视为分类/序数变量。
- en: You are building a machine learning model for a car insurance company. The company
    wants to create a binary classification model that aims to predict how likely
    their insured vehicles are to be stolen. You have considered many features to
    this model, including the type of vehicle (economy, compact, premium, luxury,
    van, sport, and convertible). How would you transform the type of vehicle in order
    to use it in your model?
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你正在为一家汽车保险公司构建一个机器学习模型。该公司希望创建一个旨在预测其承保车辆被盗可能性的二元分类模型。你已经考虑了许多特征来构建此模型，包括车辆类型（经济型、紧凑型、高级、豪华、货车、运动型和敞篷车）。你将如何转换车辆类型以便在模型中使用它？
- en: a) Applying ordinal encoding
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 应用序数编码
- en: b) Applying one-hot encoding
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 应用独热编码
- en: c) No transformation is needed
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 不需要转换
- en: d) Options A and B are valid transformations for this problem
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 选项A和B是此问题的有效转换
- en: Answer
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: b, In this case, we have a categorical/nominal variable (there is no order among
    each category). Additionally, the number of unique categories looks pretty manageable;
    furthermore, one-hot encoding would fit very well for this type of data.
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b, 在这种情况下，我们有一个分类/名义变量（每个类别之间没有顺序）。此外，唯一类别的数量看起来相当可管理；此外，独热编码非常适合此类数据。
- en: You are working as a data scientist for a financial company. The company wants
    to create a model that aims to classify improper payments. You have decided to
    use "type of transaction" as one of your features (local, international, pre-approved,
    and so on). After applying one-hot encoding to this variable, you realize that
    your dataset has many more variables, and your model is taking a lot of time to
    train. How could you potentially solve this problem?
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你正在为一家金融公司担任数据科学家。该公司希望创建一个旨在分类不当付款的模型。你决定将“交易类型”作为你的一个特征（本地、国际、预先批准的等等）。在对此变量应用独热编码后，你意识到你的数据集有更多的变量，并且你的模型训练时间很长。你如何可能解决这个问题？
- en: a) By applying ordinal encoding instead of one-hot encoding. In this case, we
    would be creating just one feature.
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 通过应用序数编码而不是独热编码。在这种情况下，我们只需创建一个特征。
- en: b) By applying label encoding.
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 通过应用标签编码。
- en: c) By analyzing which types of transactions have the most impact on improper/proper
    payments. Only apply one-hot encoding to the reduced types of transactions.
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 通过分析哪些类型的交易对不当/正当付款的影响最大。仅对减少的交易类型应用独热编码。
- en: d) By porting your model to another programming language that can handle sparse
    data in a better way.
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 通过将你的模型迁移到另一种可以更好地处理稀疏数据的编程语言。
- en: Answer
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: c, Your transformation resulted in more features due to the excessive number
    of categories in the original variable. Although the one-hot encoding approach
    looks right, since the variable is a nominal feature, the number of levels (unique
    values) for that feature is probably too high.
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c, 由于原始变量中类别数量过多，你的转换导致产生了更多特征。尽管独热编码方法看起来是正确的，因为该变量是一个名义特征，该特征的级别（唯一值）数量可能太高。
- en: In this case, you could do exploratory data analysis to understand the most
    important types of transactions for your problem. Once you know that information,
    you can then restrict the transformation to just those specific types (reducing
    the sparsity of your data). It is worth adding that you would be missing some
    information during this process because now, your dummy variables would only be
    focusing only on a subset of categories, but it is a valid approach.
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，你可以进行探索性数据分析，了解对你问题最重要的交易类型。一旦你知道了这些信息，你就可以将转换限制在这些特定类型上（减少数据稀疏性）。值得注意的是，在这个过程中，你会丢失一些信息，因为现在你的虚拟变量只会关注子集的类别，但这是一种有效的方法。
- en: You are working as a data scientist for a marketing company. Your company is
    building a clustering model that will be used to segment customers. You decided
    to normalize the variable "annual income", which ranges from between 50,000 and
    300,000.
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你正在一家营销公司担任数据科学家。你的公司正在构建一个聚类模型，用于细分客户。你决定对变量“年收入”进行归一化，其范围在 50,000 到 300,000
    之间。
- en: After applying normalization, what would be the normalized values of a group
    of customers that earn 50,000, 125,000 and 300,000?
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在应用归一化后，收入为 50,000、125,000 和 300,000 的客户群体的归一化值是什么？
- en: a) 1, 2, and 3
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 1, 2 和 3
- en: b) 0, 0.5, and 1
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 0, 0.5 和 1
- en: c) 0, 0.25, and 1
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 0, 0.25 和 1
- en: d) 5, 12, and 30
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 5, 12 和 30
- en: Answer
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: b, Applying the normalization formula and assuming the expected range would
    be 0 and 1, the correct answer is b.
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b, 应用归一化公式并假设期望的范围是 0 和 1，正确答案是 b。
- en: Consider a dataset that stores the salaries of employees in a particular column.
    The mean value of salaries on this column is $2,000, while the standard deviation
    is equal to $300\. What is the standard scaled value of someone that earns $3,000?
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑一个存储员工工资的特定列的数据集。该列工资的均值是 $2,000，而标准差等于 $300\. 那位收入 $3,000 的人的标准尺度值是多少？
- en: a) 3.33
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 3.33
- en: b) 6.66
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 6.66
- en: c) 10
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 10
- en: d) 1
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 1
- en: Answer
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: 'a, Remember the standard scale formula: (X - µ) / σ, which is (3,000 – 2,000)
    / 300.'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a, 记住标准尺度公式：(X - µ) / σ，即 (3,000 – 2,000) / 300。
- en: Which type of data transformations can we apply to convert a continuous variable
    into a binary variable?
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以应用哪种类型的数据转换将连续变量转换为二元变量？
- en: a) Binning and one-hot encoding
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 分箱和独热编码
- en: b) Standardization and binning
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 标准化和分箱
- en: c) Normalization and one-hot encoding
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 归一化和独热编码
- en: d) Standardization and one-hot encoding
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 标准化和独热编码
- en: Answer
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: a, In this case, we could discretize a continuous variable by applying binning
    and then getting the dummy variables.
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a, 在这种情况下，我们可以通过应用分箱然后获取虚拟变量来对连续变量进行离散化。
- en: You are a data scientist for a financial company and you have been assigned
    the task of creating a binary classification model to predict whether a customer
    will leave the company or not (also known as churn). During your exploratory work,
    you realize that there is a particular feature (credit utilization amount) with
    some missing values. This variable is expressed in real numbers; for example,
    $1,000\. What would be the fastest approach to dealing with those missing values,
    assuming you don't want to lose information?
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你是一家金融公司的数据科学家，你被分配了一个创建二元分类模型的任务，以预测客户是否会离开公司（也称为流失）。在你的探索性工作中，你意识到有一个特定的特征（信用使用金额）有一些缺失值。这个变量以实数表示；例如，$1,000\.
    假设你不想丢失信息，处理这些缺失值的最快方法是什么？
- en: a) Dropping any missing values.
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 删除任何缺失值。
- en: b) Creating a classification model to predict the credit utilization amount
    and use it to predict the missing data.
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 创建一个分类模型来预测信用使用金额，并使用它来预测缺失数据。
- en: c) Creating a regression model to predict the credit utilization amount and
    use it to predict the missing data.
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 创建一个回归模型来预测信用使用金额，并使用它来预测缺失数据。
- en: d) Replacing the missing data with the mean or median value of the variable.
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 用变量的均值或中位数替换缺失数据。
- en: Answer
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: d, In this case, almost all the options are valid approaches to deal with missing
    data for this problem, except option b because predicting the missing data would
    require you to create a regression model, not a classification model. Option a
    is a valid approach to deal with missing data, but not to this problem where we
    don't want to lose information. Option c is also a valid approach, but not the
    fastest one. Option d is the most appropriate answer for this question.
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d, 在这个情况下，几乎所有的选项都是处理这个问题的有效方法，除了选项b，因为预测缺失数据需要你创建一个回归模型，而不是分类模型。选项a是处理缺失数据的有效方法，但不是针对我们不想丢失信息的问题。选项c也是一个有效的方法，但不是最快的。选项d是此问题的最合适的答案。
- en: You have to create a machine learning model for a particular client, but you
    realize that most of the features have more than 50% of data missing. What's our
    best option on this critical case?
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你必须为特定的客户创建一个机器学习模型，但你意识到大部分特征有超过50%的数据缺失。在这个关键情况下，我们的最佳选择是什么？
- en: a) Dropping entire columns with more than 50% of missing data
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 删除超过50%缺失数据的整个列
- en: b) Removing all the rows that contains at least one missing information
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 删除包含至少一条缺失信息的所有行
- en: c) Check with the dataset owner if you can retrieve the missing data from somewhere
    else
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 与数据集所有者核实是否可以从其他地方检索缺失数据
- en: d) Replace the missing information by the mean or median of the feature
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 用特征的均值或中位数替换缺失信息
- en: Answer
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: c, This is a very critical case where most of your information is actually missing.
    You should work with the dataset owner to understand why that problem is occurring
    and check the process that generates this data. If you decide to drop missing
    values, you would be losing a lot of information. On the other hand, if you decide
    to replace missing values, you would be adding a lot of bias to your data.
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c, 这是一个非常关键的情况，其中大部分信息实际上都是缺失的。你应该与数据集所有者合作，了解为什么会出现这个问题，并检查生成这些数据的过程。如果你决定删除缺失值，你将丢失大量信息。另一方面，如果你决定替换缺失值，你将在数据中添加大量偏差。
- en: You are working as a senior data scientist from a human resource company. You
    are creating a particular machine learning model that uses an algorithm that does
    not perform well on skewed features, such as the one in the following image:![](img/B16735_03_028.jpg)
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你作为一个人力资源公司的高级数据科学家，正在创建一个特定的机器学习模型，该模型使用一个在倾斜特征上表现不佳的算法，如下面的图像所示：![](img/B16735_03_028.jpg)
- en: Figure 3.28 – Skewed feature
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.28 – 倾斜特征
- en: Which transformations could you apply to this feature to reduce its skewness
    (choose all the correct answers)?
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以应用哪些变换来减少这个特征的倾斜度（选择所有正确答案）？
- en: a) Normalization
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 归一化
- en: b) Log transformation
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 对数变换
- en: c) Exponential transformation
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 指数变换
- en: d) Box-Cox transformation
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) Box-Cox变换
- en: Answer
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: b, d, To reduce skewness, power transformations are the most appropriate. Particularly,
    you could apply the log transformation or Box-Cox transformation to make this
    distribution more similar to a Gaussian one.
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b, d, 为了减少倾斜度，幂变换是最合适的方法。特别是，你可以应用对数变换或Box-Cox变换来使这个分布更接近高斯分布。
- en: You are working on a fraud identification issue where most of your labeled data
    belongs to one single class (not fraud). Only 0.1% of the data refers to fraudulent
    cases. Which modeling techniques would you propose to use on this use case (choose
    all the correct answers)?
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你正在处理一个欺诈识别问题，其中大部分标记数据属于一个单一类别（非欺诈）。只有0.1%的数据指的是欺诈案例。你将提出哪些建模技术来使用在这个用例中（选择所有正确答案）？
- en: a) Applying random oversampling to create copies of the fraudulent cases.
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 应用随机过采样来创建欺诈案例的副本。
- en: b) Applying random undersampling to remove observations from the not fraudulent
    cases.
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 应用随机欠采样来从非欺诈案例中移除观察值。
- en: c) We can't create a classification model on such an unbalanced dataset. The
    best thing to do would be to ask for more fraudulent cases from the dataset owner.
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 在这样一个不平衡的数据集上创建分类模型是不可能的。最好的办法是向数据集所有者请求更多的欺诈案例。
- en: d) Applying synthetic oversampling to create copies of the not fraudulent cases.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 应用合成过采样来创建非欺诈案例的副本。
- en: Answer
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: a, b, Unbalanced datasets are very common with ML, and there are many different
    ways to deal with this problem. Option c is definitely not the right one. Pay
    attention to option d; you should be able to apply synthetic oversampling to your
    data, but to create more observations of the minority class, not from the majority
    class. Options a and b are correct.
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a, b, 在机器学习中，不平衡的数据集非常常见，有许多不同的方法可以处理这个问题。选项c绝对不是正确的。注意选项d；你应该能够对你的数据应用合成过采样，但为了创建更多少数类的观测值，而不是来自多数类。选项a和b是正确的。
- en: 'You are preparing text data for machine learning. This time, you want to create
    a bi-gram BoW matrix on top of the following texts:'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你正在为机器学习准备文本数据。这次，你想要在以下文本之上创建一个二元词袋BoW矩阵：
- en: '"I will master this certification exam"'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"我将掌握这个认证考试"'
- en: '"I will pass this certification exam"'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"我将通过这个认证考试"'
- en: How many rows and columns would you have on your BoW matrix representation?
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你的BoW矩阵表示将有多少行和列？
- en: a) 7 rows and 2 columns
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 7行和2列
- en: b) 2 rows and 7 columns
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 2行和7列
- en: c) 14 rows and 4 columns
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 14行和4列
- en: d) 4 columns and 14 rows
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 4列和14行
- en: Answer
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: b, Let's compute the matrix in the following table together.
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b, 让我们一起计算下表中矩阵。
- en: 'As you can see, the trick is knowing which tokens are common to the two texts.
    We only have two texts; the number of rows will be also two – one for each of
    them:'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如你所见，技巧在于知道哪些标记是两个文本共有的。我们只有两个文本；行数也将是两个——每个文本一个：
- en: '![](img/B16735_03_029.jpg)'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B16735_03_029.jpg)'
- en: Figure 3.29 – Resulting bag of words
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.29 – 结果词袋
- en: You are running a survey to check how many years of experience a group of people
    has on a particular development tool. You came up with the following distribution:![](img/B16735_03_030.jpg)
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你正在进行一项调查，以检查一组人在特定开发工具上的经验年数。你提出了以下分布：![](img/B16735_03_030.jpg)
- en: Figure 3.30 – Skewed data distribution
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.30 – 偏斜数据分布
- en: What can you say about the mode, mean, and median of this distribution (choose
    all the correct answers)?
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你能说些什么关于这个分布的众数、平均值和中位数（选择所有正确答案）？
- en: a) The mean is greater than the median
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 平均值大于中位数
- en: b) The mean is smaller than the median
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 平均值小于中位数
- en: c) The median is greater than the mode
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 中位数大于众数
- en: d) The median is smaller than the mode
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 中位数小于众数
- en: Answer
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: a, c, This is a skewed distribution (to the right-hand side). This means your
    mean value will be pushed to the same side of the tail, followed by the median.
    As result, we will have mode < median < mean.
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a, c, 这是一个偏斜分布（向右）。这意味着你的平均值将被推向尾巴的同一侧，然后是中位数。因此，我们将有众数 < 中位数 < 平均值。
- en: You are working as a data scientist for a retail company. You are building a
    decision tree-based model for classification purposes. During your evaluation
    process, you realize that the model's accuracy is not acceptable. Which of the
    following tasks is *not* a valid approach for increasing the accuracy of your
    decision tree model?
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你作为一家零售公司的数据科学家工作。你正在为分类目的构建一个基于决策树的模型。在你的评估过程中，你意识到模型的准确率是不可接受的。以下哪项任务不是提高你的决策树模型准确率的有效方法？
- en: a) Tuning the hyperparameters of the model.
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 调整模型的超参数。
- en: b) Scaling numerical features.
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 缩放数值特征。
- en: c) Trying different approaches for transforming categorical features, such as
    binary encoding, one-hot encoding, and ordinal encoding (whenever applicable).
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 尝试不同的方法来转换分类特征，例如二进制编码、独热编码和有序编码（当适用时）。
- en: d) If the dataset is not balanced, it is worth trying different types of resampling
    techniques (undersampling and oversampling).
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 如果数据集不平衡，尝试不同的重采样技术（欠采样和过采样）是值得的。
- en: Answer
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: b, Scaling numerical features is an important task with machine learning models
    but is not always needed. Particularly on decision tree-based models, changing
    the scale of the data will not result in better model performance, since this
    type of model is not impacted by the scale of the data.
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b, 缩放数值特征是机器学习模型中的一个重要任务，但并不总是需要的。特别是在基于决策树的模型中，改变数据的比例不会导致更好的模型性能，因为这种类型的模型不受数据比例的影响。
- en: You are working on a data science project where you must create an NLP model.
    You have decided to test Word2vec and GloVe during your model development, as
    an attempt to improve model accuracy. Word2vec and GloVe are two types of what?
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你正在从事一个数据科学项目，其中你必须创建一个NLP模型。你决定在模型开发期间测试Word2vec和GloVe，以尝试提高模型精度。Word2vec和GloVe是两种什么类型的？
- en: a) Pre-trained word embeddings
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 预训练词嵌入
- en: b) Pre-trained TF-IDF vectors
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 预训练的 TF-IDF 矢量
- en: c) One-hot encoding techniques
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) One-hot 编码技术
- en: d) None of above
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 以上皆非
- en: Answer
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案
- en: a, Some NPL architectures may include embedding layers. If you are facing this
    type of project, you can either create your own embedding space by training a
    specific model for that purpose (using your own corpus of data) or you can use
    any pre-trained word embedding model, such as the Word2vec model, which is pre-trained
    by Google.
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a, 一些自然语言处理（NLP）架构可能包括嵌入层。如果你遇到这类项目，你可以通过训练一个特定模型来创建自己的嵌入空间（使用你自己的数据集）或者你可以使用任何预训练的词嵌入模型，例如由谷歌预训练的
    Word2vec 模型。
