- en: 'Chapter 5: Model Training and Inference'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章：模型训练和推理
- en: In the last chapter, we discussed **Feast deployment** in the AWS cloud and
    set up S3 as an offline store and DynamoDB as an online store for the model. We
    also revisited the few stages of the ML life cycle using the **Customer Lifetime
    Value** (**LTV**/**CLTV**) model built in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014),
    *An Overview of the Machine Learning Life Cycle*. During the processing of model
    development, we performed data cleaning and feature engineering and produced the
    feature set for which the feature definitions were created and applied to Feast.
    In the end, we ingested the features into Feast successfully and we were also
    able to query the ingested data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了在AWS云中的**Feast部署**，并将S3设置为离线存储，将DynamoDB设置为在线存储模型。我们还回顾了使用[*第1章*](B18024_01_ePub.xhtml#_idTextAnchor014)，*机器学习生命周期概述*中构建的**客户终身价值**（**LTV**/**CLTV**）模型所经历的几个ML生命周期阶段。在模型开发处理过程中，我们进行了数据清洗和特征工程，并生成了特征集，为这些特征定义创建了并应用于Feast。最后，我们成功地将特征导入Feast，并且也能够查询导入的数据。
- en: In this chapter, we will continue with the rest of the ML life cycle, which
    will involve model training, packaging, batch, and online model inference using
    the feature store. The goal of this chapter is to continue using the feature store
    infrastructure that was created in the previous chapter and go through the rest
    of the ML life cycle. As we go through this process, it will provide an opportunity
    to learn how using the feature store in ML development can improve the time to
    production of the model, decouples different stages of the ML life cycle, and
    helps in collaboration. We will also look back at [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014),
    *An Overview of the Machine Learning Life Cycle*, and compare the different stages
    as we go through these steps. This chapter will help you understand how to use
    the feature store for model training, followed by model inference. We will also
    learn what use case the online store serves and use cases served by the offline
    store.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续ML生命周期的其余部分，这将涉及使用特征存储进行模型训练、打包、批量推理和在线模型推理。本章的目标是继续使用上一章创建的特征存储基础设施，并完成ML生命周期的其余部分。在这个过程中，我们将有机会了解如何在ML开发中使用特征存储来提高模型的上市时间，解耦ML生命周期的不同阶段，并有助于协作。我们还将回顾[*第1章*](B18024_01_ePub.xhtml#_idTextAnchor014)，*机器学习生命周期概述*，并在执行这些步骤时比较不同阶段。本章将帮助您了解如何使用特征存储进行模型训练，然后进行模型推理。我们还将学习在线存储库所服务的用例以及离线存储库所服务的用例。
- en: 'We will discuss the following topics in order:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按以下顺序讨论以下主题：
- en: Model training with the feature store
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特征存储进行模型训练
- en: Model packaging
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型打包
- en: Batch model inference with Feast
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Feast进行批量模型推理
- en: Online model inference with Feast
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Feast进行在线模型推理
- en: Handling changes to the feature set during development
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理开发过程中功能集的变化
- en: Prerequisites
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前提条件
- en: 'To run through the examples and to get a better understanding of this chapter,
    the resources created in [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065),
    *Adding Feature Store to ML Models*, are required. In this chapter, we will use
    the resources created in the previous chapter and also use the feature repository
    created in the chapter. The following GitHub link points to the feature repository
    I created: [https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行示例并更好地理解本章内容，需要使用在[*第4章*](B18024_04_ePub.xhtml#_idTextAnchor065)，*将特征存储添加到机器学习模型*中创建的资源。在本章中，我们将使用上一章创建的资源，并使用本章创建的特征存储库。以下GitHub链接指向我创建的特征存储库：[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation)。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To follow the code examples in the chapter, all you need is familiarity with
    Python and any notebook environment, which could be a local setup such as Jupyter
    or an online notebook environment such as Google Colab, Kaggle, or SageMaker.
    You will also need an AWS account with full access to resources such as Redshift,
    S3, Glue, DynamoDB, the IAM console, and more. You can create a new account and
    use all the services for free during the trial period. In the last part, you will
    need an IDE environment to develop the REST endpoints for the online model. You
    can find the code examples of the book at the following GitHub link: [https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter05](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter05).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随章节中的代码示例，你需要熟悉Python和任何笔记本环境，这可能是一个本地的设置，如Jupyter，或者是一个在线笔记本环境，如Google Colab、Kaggle或SageMaker。你还需要一个AWS账户，可以完全访问Redshift、S3、Glue、DynamoDB、IAM控制台等资源。你可以在试用期间创建一个新账户并免费使用所有服务。在最后一部分，你需要一个IDE环境来开发在线模型的REST端点。你可以在以下GitHub链接中找到本书的代码示例：[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter05](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter05)。
- en: Model training with the feature store
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用特征存储进行模型训练
- en: In [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An Overview of the
    Machine Learning Life Cycle*, after feature engineering, we jumped right into
    model training in the same notebook. Whereas in [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065),
    *Adding Feature Store to ML Models*, the generated features were ingested into
    the feature store. This is one of the standardizations that the feature store
    helps with in the ML life cycle. By ingesting features into the feature store,
    a discoverable, sharable, reusable, and versioned dataset/feature set was created.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第一章*](B18024_01_ePub.xhtml#_idTextAnchor014)，*机器学习生命周期概述*中，在特征工程之后，我们直接在同一笔记本中开始了模型训练。而相比之下，在[*第四章*](B18024_04_ePub.xhtml#_idTextAnchor065)，*将特征存储添加到机器学习模型*中，生成的特征被摄入到特征存储中。这是特征存储在机器学习生命周期中帮助实现的标准化的一个方面。通过将特征摄入到特征存储中，创建了一个可发现、可共享、可重用和版本化的数据集/特征集。
- en: Now let's assume that two data scientists, Ram and Dee, are working on the same
    model. Both can use this feature set without having to do anything extra. Not
    only that, if the background data gets refreshed every day, then all that needs
    to be done is to run the feature engineering notebook once a day when data scientists
    comes in, and the latest features will be available for consumption. An even better
    thing to do is schedule the feature engineering notebook using an orchestration
    framework such as **Airflow**, **AWS Step Functions**, or even **GitHub** workflows.
    Once that is done, the latest features are available for experimentation for both
    Dee and Ram when they come to work.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设有两个数据科学家，拉姆和迪，他们正在同一个模型上工作。他们都可以使用这个特征集，而无需做任何额外的工作。不仅如此，如果背景数据每天都会更新，那么所有需要做的就是在数据科学家到来时每天运行一次特征工程笔记本，最新的特征就会可供使用。更好的做法是使用如**Airflow**、**AWS
    Step Functions**或甚至**GitHub**工作流等编排框架来安排特征工程笔记本。一旦完成，拉姆和迪在来工作时都可以使用最新的特征进行实验。
- en: As we have been discussing, one of the biggest advantages that data engineers
    and scientists get out of feature stores is collaboration. Let's try and see how
    our two data scientists, Dee and Ram, can collaborate/compete in model building.
    Every day when Dee and Ram come into work, assuming that the scheduled feature
    engineering has run successfully, they start with the model training. The other
    important thing to note here is, for model training, the source is the feature
    store. Data scientists don't need to go into raw data sources to generate features
    unless they are not happy with the model produced by the existing features. In
    which case, data scientists would go into data exploration again, generate additional
    feature sets, and ingest them into the feature store. The ingested features are
    again available for everybody to use. This will go on until the team/data scientist
    is happy with the model's performance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们一直在讨论的，数据工程师和科学家从特征存储中获得的最大优势之一是协作。让我们尝试看看我们的两位数据科学家 Dee 和 Ram 在模型构建中如何协作/竞争。每天
    Dee 和 Ram 来上班时，假设计划中的特征工程已经成功运行，他们就开始进行模型训练。这里需要注意的另一件重要事情是，对于模型训练，数据源是特征存储。数据科学家不需要进入原始数据源来生成特征，除非他们对现有特征的模型不满意。在这种情况下，数据科学家将再次进行数据探索，生成额外的特征集，并将它们导入特征存储。导入的特征再次可供每个人使用。这将一直持续到团队/数据科学家对模型的性能满意为止。
- en: 'Before we split the workflow of two data scientists, Dee and Ram, let''s run
    through the common steps of their model training notebook. Let''s open a new Python
    notebook, call it `model-training.ipynb`, and generate training data. The offline
    store will be used for generating training datasets as it stores the historical
    data and versions the data with a timestamp. In Feast, the interface to data stores
    is through an API, as we looked at in [*Chapter 3*](B18024_03_ePub.xhtml#_idTextAnchor050)*,
    Feature Store Fundamentals, Terminology, and Usage.* and in [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065),
    *Adding Feature Store to ML Models*. Hence, to generate the training dataset,
    we will be using `get_historical_features`. One of the inputs of the `get_historical_features`
    API is entity IDs. Usually, in enterprises, entity IDs can be fetched from the
    raw data source. The typical raw sources include databases, data warehouses, object
    stores, and more. The queries to fetch entities could be as simple as `select
    unique {entity_id} from {table};`. Let''s do something similar here. Our raw data
    source is the CSV file. Let''s use that to fetch the entity IDs. Before we go
    further, let''s install the required packages:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将 Dee 和 Ram 两位数据科学家的工作流程分开之前，让我们回顾一下他们模型训练笔记本中的共同步骤。让我们打开一个新的 Python 笔记本，命名为
    `model-training.ipynb`，并生成训练数据。离线存储将用于生成训练数据集，因为它存储历史数据，并使用时间戳对数据进行版本控制。在 Feast
    中，数据存储的接口是通过 API 实现的，正如我们在 [*第 3 章*](B18024_03_ePub.xhtml#_idTextAnchor050) *特征存储基础、术语和用法*
    和 [*第 4 章*](B18024_04_ePub.xhtml#_idTextAnchor065) *将特征存储添加到机器学习模型* 中所看到的。因此，为了生成训练数据集，我们将使用
    `get_historical_features`。`get_historical_features` API 的一个输入是实体 ID。通常，在企业中，实体
    ID 可以从原始数据源中获取。典型的原始数据源包括数据库、数据仓库、对象存储等。获取实体的查询可能像 `select unique {entity_id}
    from {table};` 这样简单。让我们在这里做类似的事情。我们的原始数据源是 CSV 文件。让我们使用它来获取实体 ID。在我们继续之前，让我们安装所需的包：
- en: 'The following code block installs the required packages for the model training:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码块安装了模型训练所需的包：
- en: '[PRE0]'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After installing the required packages, if you haven''t already cloned the
    feature repository, please do so, since we need to connect to the feature store
    to generate the training dataset. The following code clones the repository:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完所需的包后，如果您还没有克隆特征仓库，请克隆它，因为我们需要连接到特征存储来生成训练数据集。以下代码克隆了仓库：
- en: '[PRE1]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now that we have the feature repository, let''s connect to Feast/the feature
    store and make sure that everything works as expected before we move on:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了特征仓库，让我们连接到 Feast/特征存储，确保一切按预期工作，然后再继续：
- en: '[PRE2]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The preceding code block connects to the Feast feature repository The `repo_path="."`
    parameter indicates that `feature_store.yaml` is in the current working directory.
    It also lists the available entities in the `customer_segmentation` feature repository
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块连接到 Feast 功能仓库。`repo_path="."` 参数表示 `feature_store.yaml` 文件位于当前工作目录中。它还列出了
    `customer_segmentation` 功能仓库中可用的实体。
- en: Now that we are able to connect to the feature repository let's create the list
    of entity IDs that are required for training the model. To get the list of entity
    IDs, in this case, `CustomerId`, let's use the raw dataset and filter out the
    entity IDs from it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们能够连接到特征仓库，让我们创建训练模型所需的实体ID列表。为了获取实体ID列表，在这种情况下，`CustomerId`，让我们使用原始数据集并从中过滤出实体ID。
- en: Important Note
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'We are using the same raw dataset that was used in [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065),
    *Adding Feature Store to ML Models*. Here is the URL of the dataset: [https://www.kaggle.com/datasets/vijayuv/onlineretail](https://www.kaggle.com/datasets/vijayuv/onlineretail).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与[*第4章*](B18024_04_ePub.xhtml#_idTextAnchor065)中使用的相同原始数据集，*将特征存储添加到机器学习模型中*。以下是数据集的URL：[https://www.kaggle.com/datasets/vijayuv/onlineretail](https://www.kaggle.com/datasets/vijayuv/onlineretail)。
- en: 'The following code block loads the raw data:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码块加载了原始数据：
- en: '[PRE3]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Important Note
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: You might question why we need raw data here. Feast allows queries on entities.
    Hence, we need the entity IDs for which the features are needed.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可能会质疑为什么在这里需要原始数据。Feast允许对实体进行查询。因此，我们需要需要特征的实体ID。
- en: Let's filter out the customer IDs that are of interest, similar to the filtering
    done during feature creation. The following code block selects the dataset that
    doesn't belong to the United Kingdom and also the customer IDs that exists in
    the three months dataset (the reason for picking the customers in the three months
    dataset is, after generating the RFM feature, we performed a left join on the
    dataset in the feature engineering notebook).
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们过滤掉感兴趣的顾客ID，类似于在特征创建过程中所做的过滤。以下代码块选择不属于英国的数据库集，以及存在于三个月数据集中的顾客ID（选择三个月数据集中的顾客的原因是，在生成RFM特征后，我们在特征工程笔记本中的数据集上执行了左连接）。
- en: 'The following code block performs the described filtering:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块执行了所描述的过滤操作：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: From `uk_data_3m`, we need to fetch the unique `CustomerId`. The additional
    column required in the entity data is the timestamp to perform point-in-time joins.
    For now, I'm going to use the latest timestamp for all the entity IDs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从`uk_data_3m`中，我们需要获取唯一的`CustomerId`。实体数据中还需要额外的列是时间戳，以执行点时间连接。现在，我将使用所有实体ID的最新时间戳。
- en: 'The following code block creates the entity DataFrame required for querying
    the historical store:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码块创建了查询历史商店所需的实体DataFrame：
- en: '[PRE5]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding code block produces the following output:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个代码块生成了以下输出：
- en: '![Figure 5.1 – Entity DataFrame for generating the training dataset'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.1 – 用于生成训练数据集的实体DataFrame'
- en: '](img/B18024_05_001.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_05_001.jpg)'
- en: Figure 5.1 – Entity DataFrame for generating the training dataset
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 用于生成训练数据集的实体DataFrame
- en: 'As you can see in *Figure 5.1*, the entity DataFrame contains two columns:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图5.1*所示，实体DataFrame包含两列：
- en: '**CustomerID**: A list of customers for whom the features need to be fetched.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CustomerID**：需要获取特征的客户列表。'
- en: '`event_timestamp`.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`event_timestamp`。'
- en: Now that the common steps in Dee and Ram's model training notebook are complete,
    let's split their workflow and look at how they can collaborate.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在Dee和Ram的模型训练笔记本中的共同步骤已完成，让我们分割他们的工作流程，看看他们如何协作。
- en: Dee's model training experiments
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dee的模型训练实验
- en: 'Continuing from the last step (feel free to copy the code blocks and run them
    in a different notebook and name it as `dee-model-training.ipynb`), it is time
    to pick the feature set required for training the model:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一步骤继续（您可以随意复制代码块并在不同的笔记本中运行它们，并将其命名为`dee-model-training.ipynb`），现在是时候选择训练模型所需的特征集了：
- en: 'To pick the features, Dee would run the following command to look at the available
    features in the existing feature view:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了选择特征，Dee将运行以下命令来查看现有特征视图中可用的特征：
- en: '[PRE6]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding command outputs the feature view. The following block displays
    a part of the output that includes features and entities that are part of the
    feature view:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令输出了特征视图。以下块显示了输出的一部分，包括特征和实体，它们是特征视图的一部分：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: From the feature set, let's assume that Dee wants to leave out frequency-related
    features and see how the performance of the model is affected. Hence, she picks
    all other features for the query and leaves out *frequency* and *F*, which indicates
    frequency group.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从特征集中，假设Dee想要排除与频率相关的特征，看看这会对模型的性能产生什么影响。因此，她选择了所有其他特征进行查询，并排除了*频率*和*F*，其中*F*表示频率组。
- en: 'The following code block queries the historical/offline store to fetch the
    required features using the entity DataFrame shown in *Figure 5.1*:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码块查询历史/离线存储，使用*图5.1*中显示的实体DataFrame获取所需特征：
- en: '[PRE8]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The previous code block outputs the DataFrame shown as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块输出以下DataFrame：
- en: '![Figure 5.2 – Training dataset for Dee''s model'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.2 – Dee模型的训练数据集'
- en: '](img/B18024_05_002.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_05_002.jpg)'
- en: Figure 5.2 – Training dataset for Dee's model
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – Dee模型的训练数据集
- en: Important Note
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Replace `<aws_key_id>` and `<aws_secret>` in the preceding code block with the
    user credentials created in [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065),
    *Adding Feature Store to ML Models*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，将`<aws_key_id>`和`<aws_secret>`替换为在[*第4章*](B18024_04_ePub.xhtml#_idTextAnchor065)，*将特征存储添加到机器学习模型*中创建的用户凭据。
- en: 'Now that Dee has the training dataset generated, the next step is model training.
    Let''s build the XGBoost model with the same parameters that were used in [*Chapter
    1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An Overview of the Machine Learning
    Life Cycle*. The following code block splits the dataset into training and testing:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，Dee已经生成了训练数据集，下一步是模型训练。让我们使用与[*第1章*](B18024_01_ePub.xhtml#_idTextAnchor014)，*机器学习生命周期概述*中使用的相同参数构建XGBoost模型。以下代码块将数据集分为培训和测试：
- en: '[PRE9]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following code block uses the training and test dataset created in the
    previous example and trains an `XGBClassifier` model:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码块使用前一个示例中创建的培训和测试数据集，训练一个`XGBClassifier`模型：
- en: '[PRE10]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding code block prints the accuracy of the model:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块打印出模型的准确率：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following code block runs the `predict` function on the test dataset and
    prints the classification report:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码块在测试数据集上运行`predict`函数并打印出分类报告：
- en: '[PRE12]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding code block produces the following output:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块产生以下输出：
- en: '![Figure 5.3 – Classification report of Dee''s model'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.3 – Dee模型的分类报告'
- en: '](img/B18024_05_003.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_05_003.jpg)'
- en: Figure 5.3 – Classification report of Dee's model
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – Dee模型的分类报告
- en: Not only this but Dee can also try out different feature sets and algorithms.
    For now, let's assume Dee is happy with her model. Let's move on and look at what
    Ram does.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅于此，Dee还可以尝试不同的特征集和算法。目前，我们假设Dee对她自己的模型感到满意。让我们继续看看Ram会做什么。
- en: Ram's model training experiments
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ram的模型训练实验
- en: 'Again, we''ll continue in the notebook from the step after *Figure 5.1* (feel
    free to copy the code blocks, run them in a different notebook, and name it as
    `ram-model-training.ipynb`). It''s time to pick the feature set required for training
    the model. To pick the features, Ram would follow similar steps as Dee did. Let''s
    assume that Ram thinks differently – instead of dropping out one specific category,
    he drops the features with actual values and just uses the R, F, and M categorical
    features and segments categorical features. According to Ram, these categorical
    variables are some transformations of the actual values:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们将从*图5.1*之后的步骤继续在笔记本中操作（您可以自由复制代码块，在另一个笔记本中运行它们，并将其命名为`ram-model-training.ipynb`）。现在是选择训练模型所需特征集的时候了。为了选择特征，Ram将遵循与Dee类似的步骤。让我们假设Ram有不同的想法——他不是删除一个特定的类别，而是删除具有实际值的特征，只使用R、F和M分类特征以及分类特征段。根据Ram的说法，这些分类变量是实际值的一些转换：
- en: 'The following code block produces the features set required by Ram to train
    the model:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码块产生Ram训练模型所需的特征集：
- en: '[PRE13]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Important Note
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: Replace `<aws_key_id>` and `<aws_secret>` in the preceding code block with the
    user credentials created in [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065),
    *Adding Feature Store to ML Models*.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码块中，将`<aws_key_id>`和`<aws_secret>`替换为在[*第4章*](B18024_04_ePub.xhtml#_idTextAnchor065)，*将特征存储添加到机器学习模型*中创建的用户凭据。
- en: 'The preceding code block produces the following output:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块产生以下输出：
- en: '![Figure 5.4 – Training dataset for Ram''s model'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.4 – Ram模型的训练数据集'
- en: '](img/B18024_05_004.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_05_004.jpg)'
- en: Figure 5.4 – Training dataset for Ram's model
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – Ram模型的训练数据集
- en: The next step is similar to what Dee performed, which is to train the model
    and look at its classification report. Let's do that.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步与Dee执行的操作类似，即训练模型并查看其分类报告。让我们来做这件事。
- en: 'The following code block trains the model on the feature set in *Figure 5.4*:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块在*图5.4*中的特征集上训练模型：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding code block prints the model accuracy after training and scoring
    the model on the test set. The code is similar to what Dee was using, but instead
    of `XGBClassifier` uses `LogisticRegression`. The code block produces the following
    output:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码块在训练后打印了模型在测试集上的准确率。代码与 Dee 所使用的类似，但使用的是 `LogisticRegression` 而不是 `XGBClassifier`。代码块生成了以下输出：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s print the classification report on the test dataset so that we can compare
    Ram and Dee''s models. The following code block produces the classification report
    for the model:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印测试数据集上的分类报告，以便我们可以比较 Ram 和 Dee 的模型。以下代码块生成了该模型的分类报告：
- en: '[PRE16]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The preceding code block produces the following output:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码块生成了以下输出：
- en: '![Figure 5.5 – Classification report of Ram''s model'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.5 – Ram 模型的分类报告]'
- en: '](img/B18024_05_005.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18024_05_005.jpg]'
- en: Figure 5.5 – Classification report of Ram's model
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – Ram 模型的分类报告
- en: Ram and Dee can now compare each other's work by looking at the experiments
    each of them has run. Not only these two experiments but they can run multiple
    experiments and come up with the best model after all the comparisons. Not only
    that, but they can also automate the experimentation by writing code to try out
    all combinations of feature sets and look at and explore more data or work on
    some other aspect while these experiments are run.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Ram 和 Dee 可以通过查看他们各自运行的实验来比较彼此的工作。不仅这些两个实验，他们还可以运行多个实验，在所有比较之后得出最佳模型。不仅如此，他们还可以通过编写代码尝试所有特征集的组合来自动化实验，在运行这些实验的同时查看和探索更多数据或专注于其他方面的工作。
- en: 'One other thing I suggest here is to use one of the experiment tracking tools/software.
    There are many out there on the market. Some of them come with the notebook infrastructure
    that you use. For example, **Databricks** offers **MLflow**, **SageMaker** has
    its own, and there are also third-party experiment tracking tools such as **Neptune**,
    **ClearML**, and others. More tools for experiment tracking and comparison can
    be found in the following blog: [https://neptune.ai/blog/best-ml-experiment-tracking-tools](https://neptune.ai/blog/best-ml-experiment-tracking-tools).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里建议的另一件事是使用实验跟踪工具/软件之一。市场上有很多这样的工具。其中一些提供了您使用的笔记本基础设施。例如，**Databricks** 提供
    **MLflow**，**SageMaker** 有自己的，还有第三方实验跟踪工具，如 **Neptune**、**ClearML** 等。更多实验跟踪和比较的工具可以在以下博客中找到：[https://neptune.ai/blog/best-ml-experiment-tracking-tools](https://neptune.ai/blog/best-ml-experiment-tracking-tools)。
- en: Let's assume that Dee and Ram, after all the experimentation, conclude that
    `XGBClassifier` performed better and decide to use that model. Let's look at model
    packaging in the next section.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 Dee 和 Ram 在所有实验之后得出结论，认为 `XGBClassifier` 表现得更好，并决定使用该模型。接下来，让我们看看下一节中的模型打包。
- en: Model packaging
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型打包
- en: 'In the previous section, we built two versions of the model. In this section,
    let''s package one of the models and save it for model scoring and deployment.
    As mentioned in the previous section, let''s package the `XGBClassifier` model.
    Again, for packaging, there are different solutions and tools available. To avoid
    setting up another tool, I will be using the `joblib` library to package the model:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们构建了两个模型版本。在本节中，我们将打包其中一个模型并保存它以供模型评分和部署。正如上一节所述，我们将打包 `XGBClassifier`
    模型。再次强调，对于打包，有不同解决方案和工具可用。为了避免设置另一个工具，我将使用 `joblib` 库来打包模型：
- en: 'Continuing in the same notebook that produced the `XGBClassifier` model, the
    following code block installs the `joblib` library:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生成 `XGBClassifier` 模型的同一笔记本中，以下代码块安装了 `joblib` 库：
- en: '[PRE17]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After installing the `joblib` library, the next step is to package the model
    object using it. The following code block packages the model and writes the model
    to a specific location on the filesystem:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 `joblib` 库后，下一步是使用它来打包模型对象。以下代码块打包了模型并将其写入文件系统上的特定位置：
- en: '[PRE18]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The preceding code block creates a file in the `/content` folder. To verify
    that, run an `ls` command and check whether the file exists. Let's also verify
    whether the model can be loaded and if we can run the `predict` function on it.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码块在 `/content` 文件夹中创建了一个文件。为了验证这一点，运行一个 `ls` 命令并检查文件是否存在。让我们也验证模型是否可以被加载，并且我们是否可以在其上运行
    `predict` 函数。
- en: 'The following code block loads the model from the location `/content/customer_segment-v0.0`
    and runs predictions on a sample dataset:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码块从 `/content/customer_segment-v0.0` 位置加载模型并在样本数据集上运行预测：
- en: '[PRE19]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The preceding code block should run without any errors and print the following
    prediction output:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块应该没有错误地运行，并打印以下预测输出：
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now that we have the packaged model, the next step is to register it in the
    model repository. Again, there are a bunch of tools available to use to manage
    the model, such as MLflow, SageMaker, and others. I would highly recommend using
    one of them as they handle a lot of use cases for sharing, deployment, standard
    versioning, and more. For simplicity, I will use an S3 bucket as the model registry
    here and upload the trained model to it.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了打包好的模型，下一步是将它注册到模型仓库中。同样，有许多工具可供使用来管理模型，例如 MLflow、SageMaker 以及其他工具。我强烈建议使用其中之一，因为它们可以处理许多用于共享、部署、标准版本控制等方面的用例。为了简化，我将在这里使用
    S3 存储桶作为模型注册处，并将训练好的模型上传到那里。
- en: 'The following code uploads the packaged model into the S3 bucket:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码将打包好的模型上传到 S3 存储桶：
- en: '[PRE21]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The preceding code block uploads the file S3 bucket, `feast-demo-mar-2022`,
    into the following prefix: `model-repo/customer_segment-v0.0`. Please verify this
    by visiting the AWS console to make sure the model is uploaded to the specified
    location.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块将文件 `S3 bucket, feast-demo-mar-2022` 上传到以下前缀：`model-repo/customer_segment-v0.0`。请通过访问
    AWS 控制台来验证这一点，以确保模型已上传到指定的位置。
- en: So far, we are done with model training and experimentation and have registered
    a candidate model in the model registry (S3 bucket). Let's create the model prediction
    notebook for a batch model use case in the next section.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经完成了模型训练和实验，并在模型仓库（S3 存储桶）中注册了一个候选模型。让我们在下一节创建一个用于批量模型用例的模型预测笔记本。
- en: Batch model inference with Feast
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Feast 进行批量模型推理
- en: 'In this section, let''s look at how to run prediction for batch models. To
    perform prediction for a batch model, we need two things: one is a model and the
    other is a list of customers and their feature set for prediction. In the previous
    section, we created and registered a model in the model registry (which is S3).
    Also, the required features are available in the feature store. All we need is
    the list of customers for whom we need to run the predictions. The list of customers
    can be generated from the raw dataset as we did before, during model training.
    However, for the purpose of this exercise, we will take a small subset of customers
    and run predictions on them.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，让我们看看如何运行批量模型的预测。为了对批量模型进行预测，我们需要两样东西：一个是模型，另一个是用于预测的客户及其特征集列表。在上一节中，我们在模型注册处（即
    S3）创建并注册了一个模型。同时，所需的特征在特征存储中也是可用的。我们需要的只是需要运行预测的客户列表。客户列表可以从我们之前在模型训练期间使用的原始数据集中生成。然而，为了这个练习的目的，我们将取一小部分客户并对其运行预测。
- en: 'Let''s create a model prediction notebook and load the model that is registered
    in the model registry:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个模型预测笔记本并加载在模型仓库中注册的模型：
- en: 'The following code block installs the required dependencies for the prediction
    notebook:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面的代码块安装了预测笔记本所需的依赖项：
- en: '[PRE22]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: After installing the dependencies, the other required step is to fetch the feature
    repository if you haven't already. This is one of the common requirements in all
    the notebooks that use Feast. However, the process may not be the same in other
    feature stores. One of the reasons for this being Feast is SDK/CLI oriented. Other
    feature stores, such as SageMaker and Databricks, might just need the credentials
    to access it. We will look at an example in a later chapter.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在安装了依赖项之后，其他所需的步骤是如果尚未完成，则获取特征仓库。这是所有使用 Feast 的笔记本中常见的需求之一。然而，在其他特征存储中，这个过程可能并不相同。其中一个原因是
    Feast 是以 SDK/CLI 为导向的。其他特征存储，如 SageMaker 和 Databricks，可能只需要凭证来访问它。我们将在下一章中查看一个示例。
- en: Assuming that you have cloned the Feast repository that was created in the previous
    chapter (which was also used during the model creation), the next step is to fetch
    the model from the model registry S3\.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你已经克隆了上一章中创建的 Feast 仓库（该仓库也用于模型创建），下一步是从模型注册处的 S3 获取模型。
- en: 'The following code block downloads the model from the S3 location (the same
    location to which the model was uploaded):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块从 S3 位置（即模型上传到的位置）下载模型：
- en: '[PRE23]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: After executing the preceding code block, you should see a file named `customer_segment-v0.0`
    in the current work directory. You can verify it using the `ls` command or through
    the folder explorer.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行前面的代码块之后，你应该在当前工作目录中看到一个名为 `customer_segment-v0.0` 的文件。你可以使用 `ls` 命令或通过文件夹浏览器来验证它。
- en: Important Note
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Replace `<aws_key_id>` and `<aws_secret>` in the preceding code block with the
    user credentials created in [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065),
    *Adding Feature Store to ML Models*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 将前一个代码块中的 `<aws_key_id>` 和 `<aws_secret>` 替换为在 [*第4章*](B18024_04_ePub.xhtml#_idTextAnchor065)
    中创建的用户凭据，*添加特征存储到机器学习模型*。
- en: The next step is to get the list of customers who need to be scored. As mentioned
    before, this can be fetched from the raw data source, but for the purpose of the
    exercise, I will be hardcoding a sample list of customers. To mimic fetching the
    customers from the raw data source, I will be invoking a function that returns
    the list of customers.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是获取需要评分的客户列表。如前所述，这可以从原始数据源中获取，但为了练习的目的，我将硬编码一个客户样本列表。为了模拟从原始数据源获取客户，我将调用一个返回客户列表的函数。
- en: 'The following code block displays the mock function to fetch customers from
    the raw data source:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块显示了从原始数据源获取客户的模拟函数：
- en: '[PRE24]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now that we have the list of customers to be scored, the next step is to fetch
    the features for these customers. There are different ways to do it. One way is
    to use the online store and the other is to use the offline store. For batch models,
    since latency is not a requirement, the most cost-efficient way is to use the
    offline store; it's just that offline stores need to be queried for the latest
    features. This can be done by using the `event_timestamp` column. Let's use the
    offline store and query the required features for the given customer list. To
    do that, we need the entity DataFrame. Let's create that next.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了要评分的客户列表，下一步是获取这些客户的特征。有几种不同的方法可以做到这一点。一种方法是使用在线存储，另一种方法是使用离线存储。对于批处理模型，由于延迟不是必需的，最经济的方法是使用离线存储；只是离线存储需要查询最新的特征。这可以通过使用
    `event_timestamp` 列来完成。让我们使用离线存储并查询给定客户列表所需的特征。为此，我们需要实体DataFrame。让我们接下来创建它。
- en: 'The following code block creates the required entity DataFrame to fetch the
    latest features:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面的代码块创建所需的实体DataFrame以获取最新的特征：
- en: '[PRE25]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding code block outputs the following entity DataFrame:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块输出以下实体DataFrame：
- en: '![Figure 5.6 – Entity DataFrame for prediction'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 5.6 – 预测用实体DataFrame'
- en: '](img/B18024_05_006.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_05_006.jpg]'
- en: Figure 5.6 – Entity DataFrame for prediction
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – 预测用实体DataFrame
- en: To fetch the latest features for any customer, you need to set `event_timestamp`
    to `datetime.now()`. Let's use the entity DataFrame in *Figure 5.4* to query the
    offline store.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取任何客户的最新特征，您需要将 `event_timestamp` 设置为 `datetime.now()`。让我们使用 *图5.4* 中的实体DataFrame来查询离线存储。
- en: 'The following code block fetches the features for the given entity DataFrame:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面的代码块获取给定实体DataFrame的特征：
- en: '[PRE26]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The preceding code block produces the following output:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块产生以下输出：
- en: '![Figure 5.7 – Features for prediction'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 5.7 – 预测用特征'
- en: '](img/B18024_05_007.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_05_007.jpg]'
- en: Figure 5.7 – Features for prediction
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 – 预测用特征
- en: 'Now that we have the features for prediction, the next step is to load the
    downloaded model and run the prediction for the customers using the features in
    *Figure 5.5*. The following code block does just that:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了用于预测的特征，下一步是加载下载的模型，并使用 *图5.5* 中的特征为客户运行预测。下面的代码块正是这样做的：
- en: '[PRE27]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The last step after running the prediction is to store the prediction results
    in a database or object store for later consumption. In this exercise, I will
    be writing the prediction results to an S3 bucket. Feel free to sink the results
    into other data stores.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行预测的最后一步是将预测结果存储在数据库或对象存储中，以便以后使用。在这个练习中，我将把预测结果写入S3桶。您可以将结果沉入其他数据存储。
- en: 'The following code block saves the prediction results along with the features
    in an S3 location:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面的代码块将预测结果以及特征保存到S3位置：
- en: '[PRE28]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: With that last code block, we are done with the implementation of a batch model.
    The question in your mind will be *how has the introduction of the feature store
    changed the ML life cycle so far?*. The early adoption of it decoupled the steps
    of feature engineering, model training, and model scoring. Any of them can be
    run independently without having to disturb the other parts of the pipeline. That's
    a huge benefit. The other part is deployment. The notebook we created in step
    one is concrete and does a specific job such as feature engineering, model training,
    and model scoring.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最后一个代码块，我们完成了批量模型的实现。你心中的疑问可能是 *特征存储的引入是如何改变到目前为止的机器学习生命周期的？*。它的早期采用解耦了特征工程、模型训练和模型评分的步骤。它们中的任何一个都可以独立运行，而无需干扰管道的其他部分。这是一个巨大的好处。另一部分是部署。我们在第一步中创建的笔记本是具体的，执行特定的任务，如特征工程、模型训练和模型评分。
- en: Now, to productionize the model, all we need to do is schedule the feature engineering
    notebook and model scoring notebook using an orchestration framework and the model
    will be running at full scale. We will look at the productionization of the model
    in the next chapter.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了将模型投入生产，我们只需要使用编排框架安排特征工程笔记本和模型评分笔记本，模型就会以全规模运行。我们将在下一章中探讨模型的投入生产。
- en: In the next section, let's look at what needs to be done for the online model
    use case.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看看在线模型使用案例需要做些什么。
- en: Online model inference with Feast
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Feast 进行在线模型推理
- en: In the last section, we discussed how to use Feast in batch model inference.
    Now, it's time to look at the online model use case. One of the requirements of
    online model inference is that it should return results in low latency and also
    be invoked from anywhere. One of the common paradigms is to expose the model as
    a REST API endpoint. In the *Model packaging* section, we logged the model using
    the `joblib` library. That model needs to be wrapped with the RESTful framework
    to be deployable as a REST endpoint. Not only that but the features also need
    to be fetched in real time when the inference endpoint is invoked. Unlike in [*Chapter
    1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An Overview of the Machine Learning
    Life Cycle*, where we didn't have the infrastructure for serving features in real
    time, here, we already have that in place thanks to Feast. However, we need to
    run the command to sync offline features to the online store using the Feast library.
    Let's do that first. Later, we will look into packaging.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了如何在批量模型推理中使用 Feast。现在，是时候看看在线模型的使用案例了。在线模型推理的一个要求是它应该以低延迟返回结果，并且可以从任何地方调用。其中一种常见的范式是将模型作为
    REST API 端点公开。在 *模型打包* 部分，我们使用 `joblib` 库记录了模型。该模型需要用 RESTful 框架包装，以便作为 REST 端点部署。不仅如此，当推理端点被调用时，特征也需要实时获取。与
    [*第 1 章*](B18024_01_ePub.xhtml#_idTextAnchor014) 中讨论的，在 *机器学习生命周期概述* 中，我们没有实时服务特征的架构不同，这里，我们已经有了
    Feast 的支持。然而，我们需要运行命令，使用 Feast 库将离线特征同步到在线商店。让我们先做这个。稍后，我们将探讨打包。
- en: Syncing the latest features from the offline to the online store
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 同步最新特征从离线到在线商店
- en: 'To load features from the offline to the online store, we need the Feast library:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要将特征从离线存储加载到在线商店，我们需要 Feast 库：
- en: 'Let''s open a notebook and install the required dependencies:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打开一个笔记本并安装所需的依赖项：
- en: '[PRE29]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After installing the required dependencies, clone the feature store repository
    As mentioned before, this is a requirement for all notebooks. Assuming you have
    cloned the repository in the current working directory, the following command
    will load the latest features from the offline to the online store:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在安装所需的依赖项后，克隆特征存储仓库。如前所述，这是所有笔记本的要求。假设您已经将仓库克隆到当前工作目录中，以下命令将从离线存储加载最新特征到在线商店：
- en: '[PRE30]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The preceding command outputs the progress as shown in the following screenshot:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令会输出如下截图所示的进度：
- en: '![Figure 5.8 – Sync the offline to the online store'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.8 – 同步离线数据到在线商店'
- en: '](img/B18024_05_008.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_05_008.jpg)'
- en: Figure 5.8 – Sync the offline to the online store
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – 同步离线数据到在线商店
- en: 'After loading the offline data into the online store, let''s run a query on
    the online store and make sure that it works as expected. To query the online
    store, initialize the feature store object and invoke the `get_online_features`
    API, as shown in the following code block:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在将离线数据加载到在线商店后，让我们在在线商店上运行一个查询，并确保它按预期工作。要查询在线商店，初始化特征商店对象并调用 `get_online_features`
    API，如下面的代码块所示：
- en: '[PRE31]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The preceding code block fetches the data from the online store (**DynamoDB**)
    at low latency. When you run the preceding block, you will notice how quickly
    it responds compared to the historical store queries. The output of the code block
    is as shown in the following screenshot:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码块以低延迟从在线商店（**DynamoDB**）获取数据。当你运行上述代码块时，你会注意到它响应的速度有多快，与历史存储查询相比。代码块的输出如下所示：
- en: '![Figure 5.9 – Query the online store'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.9 – 查询在线商店'
- en: '](img/B18024_05_009.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_05_009.jpg)'
- en: Figure 5.9 – Query the online store
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 查询在线商店
- en: In *Figure 5.7*, the last row contains `NaN` values. That is an example of how
    Feast would respond if any of the given entity IDs don't exist in the online store.
    In this example, the customer with the ID `abcdef` doesn't exist in the feature
    store, hence it returns `NaN` values for the corresponding row.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 5.7* 的最后一行包含 `NaN` 值。这是 Feast 如果给定的任何实体 ID 都不存在于在线商店中时的响应示例。在这个例子中，具有 ID
    `abcdef` 的客户不存在于特征商店中，因此它为相应的行返回 `NaN` 值。
- en: Now that the online store is ready with the latest features, let's look into
    packaging the model as a RESTful API next.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在线商店已经准备好最新的特征，让我们看看如何将模型打包成 RESTful API。
- en: Packaging the online model as a REST endpoint with Feast code
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Feast 代码将在线模型打包成 REST 端点
- en: 'This part is more about software engineering than data engineering or data
    science skills. There are many REST API frameworks for Python that are available
    out there, namely `POST` method endpoint, which will take a list of customer IDs
    as input and return the prediction list:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分更多地关于软件工程，而不是数据工程或数据科学技能。Python 有许多 REST API 框架可供选择，例如 `POST` 方法端点，它将接受客户
    ID 列表作为输入并返回预测列表：
- en: 'The following code block shows the API contract that will be implemented:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面的代码块显示了将要实现的 API 协议：
- en: '[PRE32]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now that we have the API contract, the next step is to choose the REST framework
    that we are going to use. There are different trade-offs in choosing one over
    the other among the existing REST frameworks. Since that is out of scope for this
    book, I will use `fastapi` ([https://fastapi.tiangolo.com/](https://fastapi.tiangolo.com/))
    as it is an async framework. If you are familiar with other frameworks such as
    `flask` or `django`, feel free to use them. The prediction result will be the
    same irrespective of the framework you use. Whatever framework you choose, just
    remember that we will be dockerizing the REST API before deployment.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了 API 协议，下一步是选择我们将要使用的 REST 框架。在现有的 REST 框架中选择一个框架与其他框架相比有不同的权衡。由于这超出了本书的范围，我将使用
    `fastapi` ([https://fastapi.tiangolo.com/](https://fastapi.tiangolo.com/))，因为它是一个异步框架。如果你熟悉其他框架，如
    `flask` 或 `django`，请随意使用。无论你使用哪个框架，预测结果都将相同。无论你选择哪个框架，请记住，在部署之前，我们将对 REST API
    进行 Docker 化。
- en: To build the API, I will be using the PyCharm IDE. If you have another favorite
    IDE, feel free to use that. Also, for the development of the API and for running
    the API, we need the following libraries:  `feast[aws]`, `uvicorn[standard]`,
    `fastapi`, `joblib`, and `xgboost`. You can install the libraries using the `pip
    install` command. I will leave it up to you since the steps to install differ
    based on the IDE and the platform you are using and also personal preferences.
    However, I will be using `virtualenv` to manage my Python environment.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建 API，我将使用 PyCharm IDE。如果你有其他喜欢的 IDE，请随意使用。此外，为了开发 API 和运行 API，我们需要以下库：`feast[aws]`、`uvicorn[standard]`、`fastapi`、`joblib`
    和 `xgboost`。你可以使用 `pip install` 命令安装这些库。我将由你来决定，因为安装步骤取决于你使用的 IDE、平台以及个人偏好。然而，我将使用
    `virtualenv` 来管理我的 Python 环境。
- en: 'The folder structure of my project looks as shown in the following figure.
    If you haven''t noticed already, the feature repository is also copied into the
    same folder as we need to initialize the feature store object and also the online
    store for features:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我的项目文件夹结构如下所示。如果你还没有注意到，特征仓库也被复制到了同一个文件夹中，因为我们需要初始化特征商店对象以及特征在线商店：
- en: '![Figure 5.10 – Online model folder structure in the IDE'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.10 – 在 IDE 中的在线模型文件夹结构'
- en: '](img/B18024_05_010.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_05_010.jpg](img/B18024_05_010.jpg)'
- en: Figure 5.10 – Online model folder structure in the IDE
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – IDE 中的在线模型文件夹结构
- en: 'In the `main.py` file, let''s define the APIs that we will be implementing.
    Copy the following code and paste it into the `main.py` file:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `main.py` 文件中，让我们定义我们将要实现的 API。复制以下代码并将其粘贴到 `main.py` 文件中：
- en: '[PRE33]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As you can see in the preceding code block, there are two APIs: `ping` and
    `inference`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个代码块所示，有两个 API：`ping` 和 `inference`：
- en: '`ping`: The `ping` API is a health check endpoint that will be required when
    deploying the application. The ping URL will be used by the infrastructure, such
    as ECS or Kubernetes, to check whether the application is healthy.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ping`：`ping` API 是一个健康检查端点，在部署应用程序时将需要它。ping URL 将由基础设施，如 ECS 或 Kubernetes，用于检查应用程序是否健康。'
- en: '`inference`: The `inference` API, on the other hand, will contain the logic
    for fetching the features for the given customers from the feature store, scoring
    against the model, and returning the results.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inference`：另一方面，`inference` API 将包含从特征存储中获取给定客户的特征、对模型进行评分并返回结果的逻辑。'
- en: 'Once you''ve copied the preceding code and pasted it into `main.py` and saved,
    go to the terminal and run the following command:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你复制了前面的代码并将其粘贴到 `main.py` 文件中并保存，请转到终端并运行以下命令：
- en: '[PRE34]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The preceding commands will run the FastAPI server in a local server and print
    output similar to the following code block:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的命令将在本地服务器上运行 FastAPI 服务器并打印类似于以下代码块的输出：
- en: '[PRE35]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Important Note
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: Make sure that you have activated the virtual environment in the terminal before
    running the command.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确保在运行命令之前已在终端中激活了虚拟环境。
- en: 'Once the application is run, visit the URL [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs).
    You should see a Swagger UI, as shown in the following screenshot:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用程序运行后，访问 URL [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)。你应该会看到一个
    Swagger UI，如下面的截图所示：
- en: '![Figure 5.11 – Swagger UI for the API'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 5.11 – API 的 Swagger UI](img/B18024_05_011.jpg)'
- en: '](img/B18024_05_011.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_05_011.jpg](img/B18024_05_011.jpg)'
- en: Figure 5.11 – Swagger UI for the API
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – API 的 Swagger UI
- en: We will be using the Swagger UI in *Figure 5.9* to invoke the APIs later. For
    now, feel free to play around, explore what is available, and invoke the APIs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 *图 5.9* 中使用 Swagger UI 来稍后调用 API。现在，请随意玩耍，探索可用的功能，并调用 API。
- en: Now that we have the API structure set up, let's implement the `inference` API
    next. As mentioned, the `inference` API will read the features from the feature
    store and run predictions.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经设置了 API 结构，接下来让我们实现 `inference` API。如前所述，`inference` API 将从特征存储中读取特征并运行预测。
- en: 'We also need to load the model from the model repository. In our case, the
    repository is S3\. Hence, we need code to download the model from the S3 location
    and load the model into the memory. The following code block downloads the model
    from S3 and loads it into the memory. Please note that this is a one-time activity
    during the initial application load. Hence, let''s add the following code outside
    the functions in the `main.py` file:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要从模型仓库中加载模型。在我们的案例中，仓库是 S3。因此，我们需要代码从 S3 位置下载模型并将其加载到内存中。以下代码块从 S3 下载模型并将其加载到内存中。请注意，这是在应用程序初始加载期间的一次性活动。因此，让我们在
    `main.py` 文件中的函数外部添加以下代码：
- en: '[PRE36]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now that the model is loaded into the memory, the next step is to initialize
    the feature store object. The initialization can also be outside the method since
    it is a one-time activity:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在模型已加载到内存中，下一步是初始化特征存储对象。初始化也可以在方法外部进行，因为它是一次性活动：
- en: '[PRE37]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'As the `customer_segmentation` feature repository is at the same level as that
    of `main.py` file, as shown in *Figure 5.8*, I have set `repo_path` appropriately.
    The remaining logic to fetch features from the online store, run prediction, and
    return results goes into the `inference` method definition. The following code
    block contains the same. Copy the method and replace it in the `main.py` file:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于 `customer_segmentation` 功能仓库与 `main.py` 文件处于同一级别，如图 5.8 所示，我已经适当地设置了 `repo_path`。从在线商店获取特征、运行预测和返回结果的剩余逻辑将放入
    `inference` 方法定义中。以下代码块包含相同的内容。复制该方法并将其替换到 `main.py` 文件中：
- en: '[PRE38]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now that the prediction logic is complete, let''s run the application and try
    running the prediction. To run the application, the command is the same as the
    one used before:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在预测逻辑已完成，让我们运行应用程序并尝试运行预测。要运行应用程序，命令与之前使用的一样：
- en: '[PRE39]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Once the application loads successfully, visit the Swagger UI URL ([http://localhost:8000/docs](http://localhost:8000/docs)).
    In the Swagger UI, expand the `invocations` API and click on **Try out**. You
    should see a screen similar to the one in *Figure 5.12*.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦应用程序成功加载，请访问 Swagger UI 网址 ([http://localhost:8000/docs](http://localhost:8000/docs))。在
    Swagger UI 中，展开 `invocations` API 并点击 **尝试**。你应该会看到一个类似于 *图 5.12* 的屏幕。
- en: '![Figure 5.12 – Swagger UI invocation API'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.12 – Swagger UI 调用 API'
- en: '](img/B18024_05_012.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_05_012.jpg)'
- en: Figure 5.12 – Swagger UI invocation API
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – Swagger UI 调用 API
- en: 'In the request body, provide the input as shown in *Figure 5.12* (the one in
    the following code block):'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在请求体中，提供如图 *图 5.12* 所示的输入（以下代码块中的那个）：
- en: '[PRE40]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'With this input, submit a request by clicking on **Execute**. The API should
    respond within milliseconds and the output will be visible when you scroll down
    on the screen. The following figure shows an example output:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个输入，通过点击 **执行** 提交请求。API 应该在毫秒内响应，当你在屏幕上向下滚动时，输出将可见。以下图显示了示例输出：
- en: '![Figure: 5.13 – Online model response'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '![图：5.13 – 在线模型响应'
- en: '](img/B18024_05_013.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_05_013.jpg)'
- en: 'Figure: 5.13 – Online model response'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图：5.13 – 在线模型响应
- en: That completes the steps of building a REST API for an online model with code
    to fetch features from Feast. Now that we have both the online and the batch model,
    in the next chapter, we will look at how to productionize these and how the transition
    from development to production is simple as we adopted the feature store and MLOps
    early.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这样就完成了为在线模型构建 REST API 的步骤，并附有从 Feast 获取特征的代码。现在我们既有在线模型也有批量模型，在下一章中，我们将探讨如何将这些模型投入生产，以及如何通过早期采用特征存储和
    MLOps，将开发到生产的过渡变得简单。
- en: One thing that we are yet to look into is how to change/update or add additional
    features. Let's look at this briefly before we move on.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有探讨的是如何更改/更新或添加额外的特征。在我们继续之前，让我们简要地看看这个问题。
- en: Handling changes to the feature set during development
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在开发过程中处理特征集的变化
- en: Model development is an evolving process. So are models – they evolve over time.
    Today, we may be using a few features for a specific model, but as and when we
    discover and try out new features, if the performance is better than the current
    model, we might end up including the new features in the model training and scoring.
    Hence, the feature set may change over time. What that means with the feature
    store is some of the steps we performed in [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065),
    *Adding Feature Store to ML Models*, might need to be revisited. Let's look at
    what those steps are.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 模型开发是一个不断演变的过程。模型也是如此——它们会随着时间的推移而演变。今天，我们可能只使用几个特征来构建特定的模型，但随着我们不断发现和尝试新的特征，如果新特征的性能优于当前模型，我们可能会在模型训练和评分中包含这些新特征。因此，特征集可能会随时间而变化。这意味着在特征存储中，我们在
    [*第 4 章*](B18024_04_ePub.xhtml#_idTextAnchor065) 中执行的一些步骤，即 *将特征存储添加到机器学习模型中*，可能需要重新审视。让我们看看这些步骤是什么。
- en: Important Note
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The assumption here is feature definitions change during model development,
    not after production. We will look at how to handle changes to the feature set
    after the model goes into production in later chapters.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的假设是在模型开发过程中特征定义发生变化，而不是在生产之后。我们将在后面的章节中探讨模型投入生产后如何处理特征集的变化。
- en: Step 1 – Change feature definitions
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 1 步 – 更改特征定义
- en: If the features or entities change during the model development, the first step
    is to update the feature definitions in the feature repository If you recall correctly,
    when the features were finalized, the first thing that we did was to create feature
    definitions. In the feature repository, the file `rfm_features.py` contains the
    definitions. After making the changes, run the `feast apply` command to update
    the feature definition in the resource. If you create or delete new entities or
    views, the corresponding online store resources (DynamoDB tables) will be created
    or deleted. You can verify that in the console. If there are minor changes such
    as changing the data type or feature name, the changes will be saved in the feature
    repository registry.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在模型开发过程中特征或实体发生变化，第一步是更新特征存储库中的特征定义。如果你记得正确，当特征最终确定时，我们首先做的事情是创建特征定义。在特征存储库中，文件
    `rfm_features.py` 包含了定义。在做出更改后，运行 `feast apply` 命令以更新资源中的特征定义。如果你创建或删除了新的实体或视图，相应的在线存储资源（DynamoDB
    表）将被创建或删除。你可以在控制台中验证这一点。如果只有一些小的更改，例如更改数据类型或特征名称，这些更改将被保存在特征存储库注册表中。
- en: Step 2 – Add/update schema in the Glue/Lake Formation console
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步 – 在Glue/Lake Formation控制台中添加/更新模式
- en: The second step is to define new tables in the Glue/Lake Formation database
    that we created. If the old tables are not required, you can delete them to avoid
    any confusion later. In case of the schema changes (if the feature name or data
    type changes), you need to update the existing schema to reflect the changes.
    If the schema is not updated with the changes, there will be errors when you query
    the historical store or try to load the latest feature from an offline to an online
    store. One other thing to note here is, when defining the schema, we set an S3
    location for the feature views. Now that this location contains the old data,
    which works only with the old schema, you need to define a new path to which the
    data that adheres to the new schema will be written.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是定义我们在Glue/Lake Formation数据库中创建的新表。如果旧表不再需要，您可以删除它们以避免以后产生混淆。在模式变更的情况下（如果特征名称或数据类型发生变化），您需要更新现有模式以反映这些更改。如果模式没有随着更改而更新，那么在查询历史存储或尝试从离线存储加载最新特征到在线存储时将出现错误。在此处还需要注意的另一件事是，在定义模式时，我们为特征视图设置了S3位置。现在这个位置包含旧数据，它仅适用于旧模式，因此您需要定义一个新的路径，以便符合新模式的数据将被写入。
- en: An alternate approach would be to define a brand new table with the new schema
    definitions and new S3 path for data and also update the Redshift source definitions
    in the feature repository with the new table name. If you do that, you can query
    the data in both old and new definitions. However, keep in mind that you may be
    managing two versions of the feature set, one with the older schema and one with
    the new schema. Also, there will be two DynamoDB tables.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是定义一个新的表，包含新的模式定义和新的S3路径，以及更新特征存储库中的Redshift源定义以新的表名。如果您这样做，您可以查询旧定义和新定义中的数据。然而，请记住，您可能需要管理两个版本的特征集，一个具有旧模式，另一个具有新模式。此外，将有两个DynamoDB表。
- en: Step 3 – Update notebooks with the changes
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步 – 更新笔记本中的更改
- en: The last step is simple, which is to go update all the affected notebooks. In
    the feature engineering notebook, the update would be to write data into the new
    location, whereas in the model training and scoring notebook, it would be to update
    the feature name or fetch additional features during training and scoring respectively.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步很简单，就是更新所有受影响的笔记本。在特征工程笔记本中，更新将写入新位置的数据，而在模型训练和评分笔记本中，则分别是在训练和评分期间更新特征名称或获取额外的特征。
- en: These are the three steps you need to perform every time there are updates to
    the feature definitions. With that, let's summarize what we learned in this chapter,
    and in the next chapter, we will look at how to productionize the online and batch
    models that we built in the chapter and what the challenges are beyond production.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是每次特征定义更新时都需要执行的三个步骤。有了这些，让我们总结一下本章学到的内容，在下一章中，我们将探讨如何将本章构建的在线和批量模型投入生产，以及生产之外面临的挑战。
- en: Summary
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, our aim was to look at how model training and scoring change
    with the feature store. To go through the training and scoring stages of the ML
    life cycle, we used the resources that were created in the last chapter. In the
    model training phase, we looked at how data engineers and data scientists can
    collaborate and work towards building a better model. In model prediction, we
    discussed batch model scoring and how using an offline store is a cost-effective
    way of running a batch model. We also built a REST wrapper for the online model
    and added Feast code to fetch the features for prediction during runtime. At the
    end of the chapter, we looked at the required changes if there are updates to
    features during development.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们的目标是探讨模型训练和评分如何随着特征存储的变化而变化。为了通过ML生命周期的训练和评分阶段，我们使用了上一章中创建的资源。在模型训练阶段，我们探讨了数据工程师和数据科学家如何协作并共同努力构建更好的模型。在模型预测中，我们讨论了批量模型评分，以及使用离线存储作为运行批量模型的成本效益方式。我们还为在线模型构建了一个REST包装器，并添加了Feast代码，以便在运行时获取预测所需的特征。在本章结束时，我们探讨了在开发过程中如果特征有更新时所需的更改。
- en: In the next chapter, we will continue using the batch model and the online model
    that we built in this chapter, productionize them and look at what the challenges
    are once the models are in production.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续使用本章构建的批量模型和在线模型，将它们投入生产，并探讨模型投入生产后面临的挑战。
- en: Further reading
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can find more information on Feast in the following references:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下参考资料中找到更多关于 Feast 的信息：
- en: 'Feast: [https://docs.feast.dev/](https://docs.feast.dev/)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feast: [https://docs.feast.dev/](https://docs.feast.dev/)'
- en: 'Feast AWS credit scoring tutorial: [https://github.com/feast-dev/feast-aws-credit-scoring-tutorial](https://github.com/feast-dev/feast-aws-credit-scoring-tutorial)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feast AWS 信用评分教程: [https://github.com/feast-dev/feast-aws-credit-scoring-tutorial](https://github.com/feast-dev/feast-aws-credit-scoring-tutorial)'
