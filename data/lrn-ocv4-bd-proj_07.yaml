- en: Detecting Face Parts and Overlaying Masks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测面部部位并叠加口罩
- en: In [Chapter 6](83822325-00be-4874-813c-b90097030d85.xhtml), *Learning Object
    Classification*, we learned about object classification and how machine learning
    can be used to achieve it. In this chapter, we are going to learn how to detect
    and track different face parts. We will start the discussion by understanding
    the face detection pipeline and how it's built. We will then use this framework
    to detect face parts, such as the eyes, ears, mouth, and nose. Finally, we will
    learn how to overlay funny masks on these face parts in a live video.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 6 章](83822325-00be-4874-813c-b90097030d85.xhtml)“学习对象分类”中，我们学习了对象分类以及如何使用机器学习来实现它。在本章中，我们将学习如何检测和跟踪不同的面部部位。我们将从理解面部检测流程及其构建方式开始讨论。然后，我们将使用这个框架来检测面部部位，如眼睛、耳朵、嘴巴和鼻子。最后，我们将学习如何在实时视频中将这些面部部位叠加有趣的口罩。
- en: 'By the end of this chapter, we should be familiar with the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们应该熟悉以下主题：
- en: Understanding Haar cascades
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Haar 级联
- en: Integral images and why we need them
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整数图像及其必要性
- en: Building a generic face detection pipeline
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建通用的面部检测流程
- en: Detecting and tracking faces, eyes, ears, noses, and mouths in a live video
    stream from the webcam
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实时视频流中检测和跟踪面部、眼睛、耳朵、鼻子和嘴巴
- en: Automatically overlaying a face mask, sunglasses, and a funny nose on a person's
    face in a video
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在视频中自动叠加人脸面具、太阳镜和有趣的鼻子
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter requires basic familiarity with the C++ programming language. All
    the code used in this chapter can be downloaded from the following GitHub link: [https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_07](https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_07). The
    code can be executed on any operating system, though it is only tested on Ubuntu.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要您对 C++ 编程语言有基本的熟悉度。本章中使用的所有代码都可以从以下 GitHub 链接下载：[https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_07](https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_07)。代码可以在任何操作系统上执行，尽管它仅在
    Ubuntu 上进行了测试。
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频以查看代码的实际应用：
- en: '[http://bit.ly/2SlpTK6](http://bit.ly/2SlpTK6)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2SlpTK6](http://bit.ly/2SlpTK6)'
- en: Understanding Haar cascades
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Haar 级联
- en: 'Haar cascades are cascade classifiers that are based on Haar features. What
    is a cascade classifier? It is simply a concatenation of a set of weak classifiers
    that can be used to create a strong classifier. What do we mean by **weak** and
    **strong** classifiers? Weak classifiers are classifiers whose performance is
    limited. They don''t have the ability to classify everything correctly. If you
    keep the problem really simple, they might perform at an acceptable level. Strong
    classifiers, on the other hand, are really good at classifying our data correctly.
    We will see how it all comes together in the next couple of paragraphs. Another
    important part of Haar cascades is **Haar features**. These features are simple
    summations of rectangles and differences of those areas across the image. Let''s
    consider the following diagram:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Haar 级联是基于 Haar 特征的级联分类器。什么是级联分类器？它简单地说是一系列弱分类器的串联，这些弱分类器可以用来创建一个强分类器。我们所说的**弱**和**强**分类器是什么意思？弱分类器是性能有限的分类器。它们没有正确分类所有事物的能力。如果你把问题简化到极致，它们可能达到可接受的水平。另一方面，强分类器在正确分类我们的数据方面非常出色。我们将在接下来的几段中看到这一切是如何结合在一起的。Haar
    级联的另一个重要部分是**Haar 特征**。这些特征是矩形和这些区域之间差异的简单求和。让我们考虑以下图表：
- en: '![](img/50bdb239-0bd8-49d2-af12-631b7735f346.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50bdb239-0bd8-49d2-af12-631b7735f346.png)'
- en: If we want to compute the Haar features for region ABCD, we just need to compute
    the difference between the white pixels and the blue pixels in that region. As
    we can see from the four diagrams, we use different patterns to build Haar features.
    There are a lot of other patterns that are used as well. We do this at multiple
    scales to make the system scale-invariant. When we say multiple scales, we just
    scale the image down to compute the same features again. This way, we can make
    it robust against size variations of a given object.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要计算区域ABCD的Haar特征，我们只需要计算该区域中白色像素和蓝色像素之间的差异。正如我们从四个图中可以看到的，我们使用不同的模式来构建Haar特征。还有许多其他模式也被用作此目的。我们在多个尺度上这样做，以使系统具有尺度不变性。当我们说多个尺度时，我们只是将图像缩小以再次计算相同的特征。这样，我们可以使系统对给定对象的尺寸变化具有鲁棒性。
- en: As it turns out, this concatenation system is a very good method for detecting
    objects in an image. In 2001, Paul Viola and Michael Jones published a seminal
    paper where they described a fast and effective method for object detection. If
    you are interested in learning more about it, you can check out their paper at [http://www.cs.ubc.ca/~lowe/425/slides/13-ViolaJones.pdf](http://www.cs.ubc.ca/~lowe/425/slides/13-ViolaJones.pdf).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，这个拼接系统是检测图像中对象的一个非常好的方法。在2001年，保罗·维奥拉和迈克尔·琼斯发表了一篇开创性的论文，其中他们描述了一种快速有效的对象检测方法。如果你对了解更多信息感兴趣，你可以查看他们的论文，链接为[http://www.cs.ubc.ca/~lowe/425/slides/13-ViolaJones.pdf](http://www.cs.ubc.ca/~lowe/425/slides/13-ViolaJones.pdf)。
- en: Let's dive deeper into it to understand what they actually did. They basically
    described an algorithm that uses a boosted cascade of simple classifiers. This
    system is used to build a strong classifier that can perform really well. Why
    did they use these simple classifiers instead of complex classifiers, which can
    be more accurate? Well, using this technique they were able to avoid the problem
    of having to build a single classifier that can perform with high precision. These
    single-step classifiers tend to be complex and computationally intensive. The
    reason their technique works so well is because the simple classifiers can be
    weak learners, which means they don't need to be complex. Consider the problem
    of building a table detector. We want to build a system that will automatically
    learn what a table looks like. Based on that knowledge, it should be able to identify
    whether there is a table in any given image. To build this system, the first step
    is to collect images that can be used to train our system. There are a lot of
    techniques available in the machine learning world that can be used to train a
    system such as this. Bear in mind that we need to collect a lot of table and non-table
    images if we want our system to perform well. In machine learning lingo, table
    images are called **positive** samples and the non-table images are called **negative**
    samples. Our system will ingest this data and then learn to differentiate between
    these two classes. In order to build a real-time system, we need to keep our classifier
    nice and simple. The only concern is that simple classifiers are not very accurate.
    If we try to make them more accurate, then the process will end up being computationally
    intensive, and hence slow. This trade-off between accuracy and speed is very common
    in machine learning. So, we overcome this problem by concatenating a bunch of
    weak classifiers to create a strong and unified classifier. We don't need the
    weak classifiers to be very accurate. To ensure the quality of the overall classifier,
    Viola and Jones have described a nifty technique in the cascading step. You can
    go through the paper to understand the full system.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨，了解他们实际上做了什么。他们基本上描述了一个使用简单分类器级联提升的算法。这个系统被用来构建一个能够真正表现良好的强大分类器。他们为什么使用这些简单的分类器而不是更复杂的分类器，后者可能更准确呢？嗯，使用这种技术，他们能够避免构建一个需要具有高精度性能的单个分类器的问题。这些单步分类器往往很复杂且计算密集。他们的技术之所以效果如此之好，是因为简单的分类器可以是弱学习器，这意味着它们不需要很复杂。考虑构建一个表格检测器的问题。我们希望构建一个能够自动学习表格外观的系统。基于这个知识，它应该能够识别任何给定图像中是否存在表格。为了构建这个系统，第一步是收集可以用来训练我们系统的图像。在机器学习领域有许多技术可以用来训练这样的系统。记住，如果我们想让我们的系统表现良好，我们需要收集大量的表格和非表格图像。在机器学习的术语中，表格图像被称为**正样本**，而非表格图像被称为**负样本**。我们的系统将摄取这些数据，并学会区分这两类。为了构建一个实时系统，我们需要保持我们的分类器既简单又好。唯一的问题是简单分类器不太准确。如果我们试图使它们更准确，那么这个过程最终会变得计算密集，从而变慢。在机器学习中，准确性和速度之间的这种权衡非常常见。因此，我们通过串联多个弱分类器来创建一个强大且统一的分类器来克服这个问题。我们不需要弱分类器非常准确。为了确保整体分类器的质量，Viola和Jones在级联步骤中描述了一种巧妙的技术。你可以阅读论文来了解整个系统。
- en: Now that we understand the general pipeline, let's see how to build a system
    that can detect faces in a live video. The first step is to extract features from
    all the images. In this case, the algorithms need these features to learn and
    understand what faces look like. They used Haar features in their paper to build
    the feature vectors. Once we extract these features, we pass them through a cascade
    of classifiers. We just check all the different rectangular sub-regions and keep
    discarding the ones that don't have faces in them. This way, we arrive at the
    final answer quickly to see whether a given rectangle contains a face or not.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了整个流程，让我们看看如何构建一个能够在实时视频中检测人脸的系统。第一步是从所有图像中提取特征。在这种情况下，算法需要这些特征来学习和理解人脸的外观。他们在论文中使用了Haar特征来构建特征向量。一旦我们提取了这些特征，我们就将它们通过一个分类器的级联。我们只是检查所有不同的矩形子区域，并丢弃其中没有人脸的子区域。这样，我们就能快速得出结论，看一个给定的矩形是否包含人脸。
- en: What are integral images?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是积分图像？
- en: 'In order to extract these Haar features, we will have to calculate the sum
    of the pixel values enclosed in many rectangular regions of the image. To make
    it scale-invariant, we are required to compute these areas at multiple scales
    (for various rectangle sizes). Implemented naively, this would be a very computationally-intensive
    process; we would have to iterate over all the pixels of each rectangle, including
    reading the same pixels multiple times if they are contained in different overlapping
    rectangles. If you want to build a system that can run in real-time, you cannot
    spend so much time in computation. We need to find a way to avoid this huge redundancy
    during the area computation because we iterate over the same pixels multiple times.
    To avoid it, we can use something called integral images. These images can be
    initialized at a linear time (by iterating only twice over the image) and then
    provide the sum of the pixels inside any rectangle of any size by reading only
    four values. To understand it better, let''s look at the following diagram:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取这些Haar特征，我们必须计算图像中许多矩形区域中像素值的总和。为了使其尺度不变，我们需要在多个尺度（各种矩形大小）上计算这些面积。如果天真地实现，这将是一个非常计算密集的过程；我们不得不迭代每个矩形的所有像素，包括如果它们包含在不同的重叠矩形中，则多次读取相同的像素。如果你想要构建一个可以实时运行的系统，你不能在计算上花费这么多时间。我们需要找到一种方法来避免在面积计算中的这种巨大冗余，因为我们多次迭代相同的像素。为了避免它，我们可以使用一种称为积分图像的东西。这些图像可以在线性时间内初始化（通过仅迭代图像两次）并且然后通过读取仅四个值来提供任何大小矩形的像素总和。为了更好地理解它，让我们看一下以下图示：
- en: '![](img/17189123-7d03-4a17-a954-222e9ca17c79.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17189123-7d03-4a17-a954-222e9ca17c79.png)'
- en: 'If we want to calculate the area of any rectangle in our diagram, we don''t
    have to iterate through all the pixels in that region. Let''s consider a rectangle
    formed by the top-left point in the image and any point, P, as the opposite corner.
    Let A[P] denote the area of this rectangle. For example, in the previous image,
    A[B] denotes the area of the 5 x 2 rectangle formed by taking the top-left point
    and **B** as opposite corners. Let''s look at the following diagram for clarity:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想计算图中任何矩形的面积，我们不必迭代该区域的所有像素。让我们考虑由图像中的左上角点和任何点P（作为对角点）形成的矩形。让A[P]表示这个矩形的面积。例如，在上一个图像中，A[B]表示由左上角点和**B**作为对角点形成的5
    x 2矩形的面积。为了清晰起见，让我们看一下以下图示：
- en: '![](img/fb75616a-63ba-49d5-8c05-d2cdfae54485.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb75616a-63ba-49d5-8c05-d2cdfae54485.png)'
- en: 'Let''s consider the top-left square in the previous image. The blue pixels
    indicate the the area between the top-left pixel and point **A**. This is denoted
    by A[A]. The remaining diagrams are denoted by their respective names: A[B], A[C],
    and A[D]. Now, if we want to calculate the area of the ABCD rectangle, as shown
    in the preceding diagram, we would use the following formula:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑上一张图片中的左上角方块。蓝色像素表示从左上角像素到点**A**之间的区域。这表示为A[A]。其余的图分别用它们各自的名字表示：A[B]、A[C]和A[D]。现在，如果我们想计算如图所示的ABCD矩形的面积，我们会使用以下公式：
- en: '**Area of the rectangle**: *ABCD* = *A[C]* - (*A[B]* + *A[D]* - *A[A]*)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**矩形面积**：*ABCD* = *A[C]* - (*A[B]* + *A[D]* - *A[A]*)'
- en: What's so special about this particular formula? As we know, extracting Haar
    features from the image includes computing these summations and we would have
    to do it for a lot of rectangles at multiple scales in the image. A lot of those
    calculations are repetitive because we would be iterating over the same pixels
    over and over again. It is so slow that building a real-time system wouldn't be
    feasible. Hence, we need this formula. As you can see, we don't have to iterate
    over the same pixels multiple times. If we want to compute the area of any rectangle,
    all the values on the right-hand side of the preceding equation are readily available
    in our integral image. We just pick up the right values, substitute them in the
    preceding equation, and extract the features.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定公式有什么特别之处呢？正如我们所知，从图像中提取Haar特征包括计算这些求和，并且我们不得不在图像的多个尺度上进行多次计算。许多这些计算是重复的，因为我们会在相同的像素上反复迭代。这非常慢，以至于构建实时系统是不切实际的。因此，我们需要这个公式。正如你所见，我们不必多次迭代相同的像素。如果我们想计算任何矩形的面积，前面方程右侧的所有值都在我们的积分图像中
    readily available。我们只需挑选正确的值，将它们代入前面的方程，并提取特征。
- en: Overlaying a face mask in a live video
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在实时视频中叠加人脸面具
- en: 'OpenCV provides a nice face detection framework. We just need to load the cascade
    file and use it to detect the faces in an image. When we capture a video stream
    from the webcam, we can overlay funny masks on our faces. It will look something
    like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV提供了一个优秀的面部检测框架。我们只需要加载级联文件，并使用它来检测图像中的面部。当我们从摄像头捕获视频流时，我们可以在我们的脸上叠加有趣的口罩。看起来可能像这样：
- en: '![](img/4b3881f9-9bf6-4642-864b-b47979fcd9dc.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b3881f9-9bf6-4642-864b-b47979fcd9dc.png)'
- en: 'Let''s look at the main parts of the code to see how to overlay this mask on
    the face in the input video stream. The full code is available in the downloadable
    code bundle provided along with this book:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看代码的主要部分，看看如何将这个面具叠加到输入视频流中的面部上。完整的代码可以在本书提供的可下载代码包中找到：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s take a quick stop to see what happened here. We start reading input
    frames from the webcam and resize it to our size of choice. The captured frame
    is a color image and face detection works on grayscale images. So, we convert
    it to grayscale and equalize the histogram. Why do we need to equalize the histogram?
    We need to do this to compensate for any issues, such as lighting or saturation.
    If the image is too bright or too dark, the detection will be poor. So, we need
    to equalize the histogram to ensure that our image has a healthy range of pixel
    values:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速停下来看看这里发生了什么。我们从摄像头读取输入帧并将其调整到我们选择的大小。捕获的帧是一个彩色图像，面部检测是在灰度图像上进行的。因此，我们将其转换为灰度并均衡直方图。为什么我们需要均衡直方图？我们需要这样做来补偿任何问题，例如光照或饱和度。如果图像太亮或太暗，检测效果会较差。因此，我们需要均衡直方图以确保我们的图像具有健康的像素值范围：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'At this point, we know where the face is. So we extract the region of interest
    to overlay the mask in the right position:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们已经知道脸的位置。因此，我们提取感兴趣的区域，以便在正确的位置叠加面具：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We isolate the pixels associated with the face mask. We want to overlay the
    mask in such a way that it doesn''t look like a rectangle. We want the exact boundaries
    of the overlaid object so that it looks natural. Let''s go ahead and overlay the
    mask now:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们隔离与面部面具相关的像素。我们希望以这种方式叠加面具，使其看起来不像一个矩形。我们希望叠加对象的精确边界，使其看起来自然。现在让我们叠加面具：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: What happened in the code?
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码中发生了什么？
- en: The first thing to note is that this code takes two input arguments—the **face
    cascade XML** file and the **mask image**. You can use the `haarcascade_frontalface_alt.xml`
    and `facemask.jpg` files that are provided under the `resources` folder. We need
    a classifier model that can be used to detect faces in an image and OpenCV provides
    a prebuilt XML file that can be used for this purpose. We use the `faceCascade.load()` function
    to load the XML file and also check whether the file is loaded correctly. We initiate
    the video-capture object to capture the input frames from the webcam. We then
    convert it to grayscale to run the detector. The `detectMultiScale` function is
    used to extract the boundaries of all the faces in the input image. We may have
    to scale down the image according to our needs, so the second argument in this
    function takes care of this. This scaling factor is the jump we take at each scale;
    since we need to look for faces at multiple scales, the next size will be 1.1
    times bigger than the current size. The last parameter is a threshold that specifies
    the number of adjacent rectangles needed to keep the current rectangle. It can
    be used to increase the robustness of the face detector. We start the `while`
    loop and keep detecting the face in every frame until the user presses the *Esc*
    key. Once we detect a face, we need to overlay a mask on it. We may have to modify
    the dimensions slightly to ensure that the mask fits nicely. This customization
    is slightly subjective and it depends on the mask that's being used. Now that
    we have extracted the region of interest, we need to place our mask on top of
    this region. If we overlay the mask with its white background, it will look weird.
    We have to extract the exact curvy boundaries of the mask and then overlay it.
    We want the skull mask pixels to be visible and the remaining area should be transparent.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，这段代码需要两个输入参数——**人脸级联XML**文件和**掩码图像**。你可以使用在`resources`文件夹下提供的`haarcascade_frontalface_alt.xml`和`facemask.jpg`文件。我们需要一个分类器模型，它可以用来检测图像中的面部，OpenCV提供了一个预构建的XML文件，可以用于此目的。我们使用`faceCascade.load()`函数来加载XML文件，并检查文件是否正确加载。我们初始化视频捕获对象以从摄像头捕获输入帧。然后将其转换为灰度以运行检测器。`detectMultiScale`函数用于提取输入图像中所有面的边界。我们可能需要根据需要调整图像的大小，因此该函数的第二个参数负责这一点。这个缩放因子是我们每次缩放时跳过的距离；由于我们需要在多个尺度上查找面部，下一个大小将是当前大小的1.1倍。最后一个参数是一个阈值，它指定了需要保留当前矩形的相邻矩形数量。它可以用来增加面部检测器的鲁棒性。我们启动`while`循环，并在用户按下*Esc*键之前，在每一帧中持续检测面部。一旦检测到面部，我们就需要在其上叠加一个面具。我们可能需要稍微调整尺寸以确保面具贴合得很好。这种定制略为主观，并且取决于所使用的面具。现在我们已经提取了感兴趣区域，我们需要在这个区域上方放置我们的面具。如果我们用其白色背景叠加面具，看起来会很奇怪。我们必须提取面具的确切曲线边界，然后进行叠加。我们希望颅骨面具的像素是可见的，而剩余区域应该是透明的。
- en: As we can see, the input mask has a white background. So, we create a mask by
    applying a threshold to the mask image. Using trial and error, we can see that
    a threshold of `240` works well. In the image, all the pixels with an intensity
    value greater than `240` will become `0`, and all others will become `255`. As
    far as the region of interest is concerned, we have to black out all the pixels
    in this region. To do that, we simply use the inverse of the mask that was just
    created. In the last step, we just add the masked versions to produce the final
    output image.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，输入面具有一个白色背景。因此，我们通过对掩码图像应用阈值来创建一个面具。通过试错，我们可以看到`240`的阈值效果很好。在图像中，所有强度值大于`240`的像素将变为`0`，而其他所有像素将变为`255`。至于感兴趣区域，我们必须在这个区域中熄灭所有像素。为此，我们只需使用刚刚创建的掩码的逆即可。在最后一步，我们只需将带掩码的版本相加，以产生最终的输出图像。
- en: Get your sunglasses on
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 戴上你的太阳镜
- en: 'Now that we understand how to detect faces, we can generalize that concept
    to detect different parts of the face. We will be using an eye detector to overlay
    sunglasses in a live video. It''s important to understand that the Viola-Jones
    framework can be applied to any object. The accuracy and robustness will depend
    on the uniqueness of the object. For example, the human face has very unique characteristics,
    so it''s easy to train our system to be robust. On the other hand, an object such
    as a towel is too generic, and it has no distinguishing characteristics as such,
    so it''s more difficult to build a robust towel detector. Once you build the eye
    detector and overlay the glasses, it will look something like this:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何检测人脸，我们可以将这个概念推广到检测人脸的不同部分。我们将使用眼睛检测器在实时视频中叠加太阳镜。重要的是要理解Viola-Jones框架可以应用于任何对象。准确性和鲁棒性将取决于对象的独特性。例如，人脸具有非常独特的特征，因此很容易训练我们的系统变得鲁棒。另一方面，像毛巾这样的对象太通用，它没有这样的区分特征，因此构建鲁棒的毛巾检测器更困难。一旦你构建了眼睛检测器和叠加眼镜，它看起来可能就像这样：
- en: '![](img/27045d2d-9d41-4a3f-8b1a-1906be8ea46a.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/27045d2d-9d41-4a3f-8b1a-1906be8ea46a.png)'
- en: 'Let''s look at the main parts of the code:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看代码的主要部分：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As we can see here, we run the eye detector only in the face region. We don''t
    need to search the entire image for eyes because we know eyes will always be on
    a face:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们只在人脸区域运行眼睛检测器。我们不需要在整个图像中搜索眼睛，因为我们知道眼睛总是在人脸上的：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We detect the eyes and store them only when we find both of them. We then use
    their coordinates to determine which one is the left eye and which one is the
    right eye:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只在找到两只眼睛时检测眼睛并将它们存储起来。然后我们使用它们的坐标来确定哪一个是左眼，哪一个是右眼：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the preceding code, we adjusted the size of the sunglasses to fit the scale
    of our faces in the webcam. Let''s check the remaining code:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们调整了太阳镜的大小，以适应我们在网络摄像头中的人脸比例。让我们检查剩余的代码：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Looking inside the code
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查看代码内部
- en: You may have noticed that the flow of the code looks similar to the face detection
    code that we discussed in the *Overlaying a face mask in a live video* section.
    We load a face detection cascade classifier as well as the eye detection cascade
    classifier. Now, why do we need to load the face cascade classifier when we are
    detecting eyes? Well, we don't really need to use the face detector, but it helps
    us in limiting our search for the eyes' location. We know that the eyes are always
    located on somebody's face, so we can limit eye detection to the face region.
    The first step would be to detect the face and then run our eye detector code
    on this region. Since we would be operating on a smaller region, it would be faster
    and way more efficient.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到代码的流程看起来与我们讨论的“在实时视频中叠加人脸遮罩”部分中的人脸检测代码相似。我们加载了一个人脸检测级联分类器以及眼睛检测级联分类器。那么，为什么在检测眼睛时我们需要加载人脸级联分类器呢？好吧，我们实际上并不需要使用人脸检测器，但它有助于我们限制眼睛位置的搜索。我们知道眼睛总是位于某人的脸上，因此我们可以将眼睛检测限制在人脸区域。第一步是检测人脸，然后在该区域运行我们的眼睛检测代码。由于我们将在一个更小的区域上操作，这将更快，效率更高。
- en: For each frame, we start by detecting the face. We then go ahead and detect
    the location of the eyes by operating on this region. After this step, we need
    to overlay the sunglasses. To do that, we need to resize the sunglasses image
    to make sure it fits our face. To get the proper scale, we can consider the distance
    between the two eyes that are being detected. We overlay the sunglasses only when
    we detect both eyes. That's why we run the eye detector first, collect all the
    centers, and then overlay the sunglasses. Once we have this, we just need to overlay
    the sunglasses mask. The principle used for masking is very similar to the principle
    we used to overlay the face mask. You may have to customize the sizing and position
    of the sunglasses, depending on what you want. You can play around with different
    types of sunglasses to see what they look like.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一帧，我们首先检测人脸。然后我们继续在这个区域检测眼睛的位置。完成这一步后，我们需要叠加太阳镜。为此，我们需要调整太阳镜图像的大小，确保它适合我们的脸。为了得到正确的比例，我们可以考虑被检测到的两只眼睛之间的距离。只有当我们检测到两只眼睛时，我们才叠加太阳镜。这就是为什么我们首先运行眼睛检测器，收集所有中心点，然后叠加太阳镜。一旦我们有了这个，我们只需要叠加太阳镜遮罩。用于遮罩的原理与我们用于叠加人脸遮罩的原理非常相似。你可能需要根据你的需求自定义太阳镜的大小和位置。你可以尝试不同的太阳镜类型，看看它们看起来如何。
- en: Tracking the nose, mouth, and ears
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪鼻子、嘴巴和耳朵
- en: 'Now that you know how to track different things using the framework, you can
    try tracking your nose, mouth, and ears too. Let''s use a nose detector to overlay
    a funny nose:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了如何使用该框架跟踪不同的事物，你也可以尝试跟踪你的鼻子、嘴巴和耳朵。让我们使用一个鼻子检测器来叠加一个有趣的鼻子：
- en: '![](img/b51f1ac4-21ab-427a-bed0-ad67d269f491.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b51f1ac4-21ab-427a-bed0-ad67d269f491.png)'
- en: You can refer to the code files for a full implementation of this detector.
    The `haarcascade_mcs_nose.xml`, `haarcascade_mcs_mouth.xml`, `haarcascade_mcs_leftear.xml`,
    and `haarcascade_mcs_rightear.xml` cascade files can be used to track the different
    face parts. Play around with them and try to overlay a mustache or Dracula ears
    on yourself.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考代码文件以获取此检测器的完整实现。以下级联文件`haarcascade_mcs_nose.xml`、`haarcascade_mcs_mouth.xml`、`haarcascade_mcs_leftear.xml`和`haarcascade_mcs_rightear.xml`可以用来跟踪不同的面部部位。尝试使用它们，并尝试给自己叠加一个胡须或德古拉耳朵。
- en: Summary
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed Haar cascades and integral images. We looked at
    how the face detection pipeline is built. We learned how to detect and track faces
    in a live video stream. We discussed using the face detection framework to detect
    various face parts, such as eyes, ears, nose, and mouth. Finally, we learned how
    to overlay masks on the input image using the results of face part detection.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了Haar级联和积分图像。我们了解了人脸检测流程是如何构建的。我们学习了如何在实时视频流中检测和跟踪人脸。我们讨论了如何使用人脸检测框架来检测各种面部部位，如眼睛、耳朵、鼻子和嘴巴。最后，我们学习了如何使用人脸部位检测的结果在输入图像上叠加面具。
- en: In the next chapter, we are going to learn about video surveillance, background
    removal, and morphological image processing.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习关于视频监控、背景去除和形态学图像处理的内容。
