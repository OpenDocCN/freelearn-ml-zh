- en: Getting Started with Ensemble Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用集成学习
- en: Ensemble learning involves a combination of techniques that allows multiple
    machine learning models, called base learners (or, sometimes, weak learners),
    to consolidate their predictions and output a single, optimal prediction, given
    their respective inputs and outputs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习涉及将多种技术结合在一起，允许多个机器学习模型（称为基本学习器或有时称为弱学习器）整合它们的预测，并在给定各自的输入和输出的情况下输出一个单一的、最优的预测。
- en: In this chapter, we will give an overview of the main problems that ensembles
    try to solve, namely, bias and variance, as well as the relationship between them.
    This will help us understand the motivation behind identifying the root cause
    of an under-performing model and using an ensemble to address it. Furthermore,
    we will go over the basic categories of the methodologies available, as well as
    the difficulties we can expect to encounter when implementing ensembles.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将概述集成学习尝试解决的主要问题，即偏差和方差，以及它们之间的关系。这将帮助我们理解识别表现不佳的模型的根本原因，并使用集成学习来解决该问题的动机。此外，我们还将介绍可用方法的基本类别，以及在实施集成学习时可能遇到的困难。
- en: 'The main topics covered in this chapter are the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主要主题如下：
- en: Bias, variance, and the trade-off between the two
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差、方差以及两者之间的权衡
- en: The motivation behind using ensemble learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用集成学习的动机
- en: Identifying the root cause of an under-performing model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别表现不佳的模型的根本原因
- en: Ensemble learning methods
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成学习方法
- en: Difficulties in applying ensemble learning successfully
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功应用集成学习的难点
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will require basic knowledge of machine learning techniques and algorithms.
    Furthermore, a knowledge of python conventions and syntax is required. Finally,
    familiarity with the NumPy library will greatly help the reader to understand
    some custom algorithm implementations.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要具备基本的机器学习技术和算法知识。此外，还需要了解 Python 语言的约定和语法。最后，熟悉 NumPy 库将大大有助于读者理解一些自定义算法的实现。
- en: 'The code files of this chapter can be found on GitHub:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在 GitHub 上找到：
- en: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter02](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter02)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter02](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter02)'
- en: Check out the following video to see the Code in Action: [http://bit.ly/2JKkWYS](http://bit.ly/2JKkWYS).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下视频，查看代码实践：[http://bit.ly/2JKkWYS](http://bit.ly/2JKkWYS)。
- en: Bias, variance, and the trade-off
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差、方差及其权衡
- en: Machine learning models are not perfect; they are prone to a number of errors.
    The two most common sources of errors are bias and variance. Although two distinct
    problems, they are interconnected and relate to a model's available degree of
    freedom or complexity.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型并不完美；它们容易出现许多错误。最常见的两种错误来源是偏差和方差。尽管这两者是不同的问题，但它们是相互关联的，并且与模型的自由度或复杂性有关。
- en: What is bias?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是偏差？
- en: Bias refers to the inability of a method to correctly estimate the target. This
    does not only apply to machine learning. For example, in statistics, if we want
    to measure a population's average and do not sample carefully, the estimated average
    will be biased. In simple terms, the method's (sampling) estimation will not closely
    match the actual target (average).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差是指方法无法正确估计目标。这不仅仅适用于机器学习。例如，在统计学中，如果我们想要测量一个人群的平均值，但没有仔细采样，那么估算出的平均值就会存在偏差。简单来说，方法（采样）估算的结果与实际目标（平均值）之间存在差距。
- en: 'In machine learning, bias refers to the difference between the expected prediction
    and its target. Biased models cannot properly fit the training data, resulting
    in poor in-sample performance and out-of-sample performance. A good example of
    a biased model arises when we try to fit a sine function with a simple linear
    regression. The model cannot fit the sine function, as it lacks the required complexity
    to do so. Thus, it will not be able to perform well in-sample or out-of-sample.
    This problem is called underfitting. A graphical example is illustrated in the
    following figure :'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，偏差指的是预期预测与目标之间的差异。偏差模型无法正确拟合训练数据，导致在样本内和样本外的表现都很差。一个偏差模型的经典例子是当我们尝试用简单的线性回归来拟合一个正弦函数时。该模型无法拟合正弦函数，因为它缺乏所需的复杂度。因此，它无法在样本内或样本外表现良好。这个问题被称为欠拟合。下图提供了一个图形示例：
- en: '![](img/7a3d6234-f46c-4cad-8796-7ab8628778b7.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a3d6234-f46c-4cad-8796-7ab8628778b7.png)'
- en: A biased linear regression model for sine function data
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对正弦函数数据的有偏线性回归模型
- en: 'The mathematical formula for bias is the difference between the target value
    and the expected prediction:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差的数学公式是目标值与期望预测值之间的差异：
- en: '![](img/65ccbf7c-cef5-44d5-9493-84f1cfd3b861.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65ccbf7c-cef5-44d5-9493-84f1cfd3b861.png)'
- en: What is variance?
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是方差？
- en: Variance refers to how much individuals vary within a group. Again, variance
    is a concept from statistics. Taking a sample from a population, variance indicates
    how much each individual's value differs from the mean.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 方差指的是个体在一个群体中的差异程度。同样，方差是统计学中的一个概念。从一个群体中抽取样本时，方差表示每个个体的数值与平均值的偏差程度。
- en: In machine learning, variance refers to the model's variability or sensitivity
    to data changes. This means that high-variance models can generally fit the training
    data well and so achieve high in-sample performance, but perform poorly out-of-sample.
    This is due to the model's complexity. For example, a decision tree can have high
    variance if it creates a rule for every single instance in the training dataset.
    This is called **overfitting**. The following figure depicts a decision tree trained
    on the preceding dataset. Blue dots represent the training data and orange dots
    represent the test data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，方差指的是模型对数据变化的敏感性或变动性。这意味着，高方差模型通常能够很好地拟合训练数据，从而在训练集上取得较高的表现，但在测试集上表现较差。这是由于模型的复杂性。例如，如果决策树为训练数据集中的每一个实例都创建一条规则，那么该决策树可能会有较高的方差。这被称为**过拟合**。下图展示了在前述数据集上训练的决策树。蓝色点代表训练数据，橙色点代表测试数据。
- en: 'As is evident, the model fits the training data perfectly but does not perform
    on the test data so well:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，模型能够完美拟合训练数据，但在测试数据上表现较差：
- en: '![](img/30604a53-1e14-4353-ad9b-39214f6d22b4.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30604a53-1e14-4353-ad9b-39214f6d22b4.png)'
- en: A high-variance decision tree model on the sine function
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 高方差的决策树模型在正弦函数上的表现
- en: 'The mathematical formula for variance is depicted as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 方差的数学公式如下所示：
- en: '![](img/257b5bbd-0482-441b-a392-ba62d837f00c.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/257b5bbd-0482-441b-a392-ba62d837f00c.png)'
- en: Essentially, this is the standard formula for population variance, assuming
    that our population is comprised of our models, as they have been produced by
    the machine learning algorithm. For example, as we saw earlier in [Chapter 1](57f23be7-7e0d-4fa5-b7a5-08e0caf8e704.xhtml), *A
    Machine Learning Refresher*, neural networks can have different training outcomes,
    depending on their initial weights. If we consider all the neural networks with
    the same architecture, but different initial weights, by training them, we will
    have a population of different models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这就是人口方差的标准公式，假设我们的人口是由模型组成的，因为这些模型是由机器学习算法生成的。例如，正如我们在[第1章](57f23be7-7e0d-4fa5-b7a5-08e0caf8e704.xhtml)《机器学习基础》中看到的，神经网络的训练结果可能不同，这取决于它们的初始权重。如果我们考虑所有具有相同架构但初始权重不同的神经网络，通过训练它们，我们将得到一组不同的模型。
- en: Trade-off
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权衡
- en: 'Bias and variance are two of the three major components that comprise a model''s
    error. The third is called the irreducible error and can be attributed to inherent
    randomness or variability in the data. The total error of a model can be decomposed
    as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差和方差是组成模型误差的三个主要组成部分中的两个。第三个是不可减少的误差，通常归因于数据中的固有随机性或变异性。模型的总误差可以分解如下：
- en: '![](img/78620b7b-b43f-4b03-a677-6c991c9c441f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78620b7b-b43f-4b03-a677-6c991c9c441f.png)'
- en: 'As we saw earlier, bias and variance stem from the same source: model complexity.
    While bias arises from too little complexity and freedom, variance thrives in
    complex models. Thus, it is not possible to reduce bias without increasing variance
    and vice versa. Nevertheless, there is an optimal point of complexity, where the
    error is minimized as bias and variance are at an optimal trade-off point. When
    the model''s complexity is at this optimal point (the red dotted line in the next
    figure), then the model performs best both in-sample and out-of-sample. As is
    evident in the next figure, the error can never be reduced to zero.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所见，偏差和方差都源自同一个因素：模型复杂度。偏差源于模型复杂度过低和自由度不足，而方差则在复杂模型中表现得更为突出。因此，不可能在不增加方差的情况下减少偏差，反之亦然。然而，存在一个复杂度的最优点，在这个点上，偏差和方差达到了最优的权衡，误差最小。当模型的复杂度达到这个最优点（下图中的红色虚线）时，模型在训练集和测试集上的表现都是最佳的。正如下图所示，误差永远不可能被减少到零。
- en: 'Furthermore, although some may think that it is better to reduce the bias,
    even at the cost of increased variance, it is clear that the model would not perform
    better, even if it was unbiased, due to the error that variance inevitably induces:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管有些人可能认为减少偏差，即使以增加方差为代价会更好，但显然即便模型没有偏差，由于方差不可避免地引起的误差，模型也不会表现得更好：
- en: '![](img/b59162a8-d7de-44fe-8b33-b599478bb9d8.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b59162a8-d7de-44fe-8b33-b599478bb9d8.png)'
- en: Bias-variance trade-off and its effect on the error
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差-方差权衡及其对误差的影响
- en: 'The following figure depicts the perfect model, with a minimum amount of combined
    bias and variance, or reducible error. Although the model does not fit the data
    perfectly, this is due to noise that is inherent in the dataset. If we try to
    fit the training data better, we will induce overfitting (variance). If we try
    to simplify the model further, we will induce underfitting (bias):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了完美模型，具有最小的偏差和方差的结合，或者说是可减少的误差。尽管该模型并没有完全拟合数据，但这是由于数据集中的噪声。如果我们尝试更好地拟合训练数据，将会引起过拟合（方差）。如果我们进一步简化模型，将会引起欠拟合（偏差）：
- en: '![](img/30ecba78-5dd9-4246-83f7-365dc7db0a82.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30ecba78-5dd9-4246-83f7-365dc7db0a82.png)'
- en: Perfect model for our data, a sine function
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的数据，完美的模型是一个正弦函数
- en: Ensemble learning
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习
- en: Ensemble learning involves a collection of machine learning methods aimed at
    improving the predictive performance of algorithms by combining many models. We
    will analyze the motivation behind using such methods to solve problems that arise
    from high bias and variance. Furthermore, we will present methods that allow the
    identification of bias and variance in machine learning models, as well as basic
    classes of ensemble learning methods.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习涉及一组机器学习方法，旨在通过结合多个模型来提高算法的预测性能。我们将分析使用这些方法来解决高偏差和方差问题的动机。此外，我们还将介绍识别机器学习模型中偏差和方差的方法，以及集成学习方法的基本分类。
- en: Motivation
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: Ensemble learning aims to solve the problems of bias and variance. By combining
    many models, we can reduce the ensemble's error, while retaining the individual
    models' complexities. As we saw earlier, there is a certain lower limit imposed
    on each model error, which is related to the model complexity.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习旨在解决偏差和方差的问题。通过结合多个模型，我们可以减少集成的误差，同时保留各个独立模型的复杂性。如前所述，每个模型误差都有一定的下限，这与模型的复杂性有关。
- en: Furthermore, we mentioned that the same algorithm can produce different models,
    due to the initial conditions, hyperparameters, and other factors. By combining
    different, diverse models, we can reduce the expected error of the group, while
    each individual model remains unchanged. This is due to statistics, rather than
    pure learning.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们提到同一个算法由于初始条件、超参数和其他因素的不同，可能会产生不同的模型。通过结合不同且多样化的模型，我们可以减少群体的预期误差，同时每个独立的模型保持不变。这是由于统计学原理，而非纯粹的学习。
- en: 'In order to better demonstrate this, let''s consider an ensemble of 11 base
    learners for a classification, each with a probability of misclassification (error)
    equal to *err*=0.15 or 15%. Now, we want to create a simple ensemble. We always
    assume that the output of most base learners is the correct answer. Assuming that
    they are diverse (in statistics, uncorrelated), the probability that the majority
    of them is wrong is 0.26%:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地展示这一点，假设我们有一个由11个基学习器组成的集成，用于分类，每个学习器的误分类（误差）概率为*err*=0.15或15%。现在，我们想要创建一个简单的集成。我们始终假设大多数基学习器的输出是正确的。假设它们是多样化的（在统计学中是无关的），那么大多数学习器出错的概率是0.26%：
- en: '![](img/83545e56-347d-450e-82b7-46f736d3c1f0.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/83545e56-347d-450e-82b7-46f736d3c1f0.png)'
- en: 'As is evident, the more base learners we add to the ensemble, the more accurate
    the ensemble will be, under the condition that each learner is uncorrelated to
    the others. Of course, this is increasingly difficult to achieve. Furthermore,
    the law of diminishing returns applies. Each new uncorrelated base learner contributes
    less to the overall error reduction than the previously added base learner. The
    following figure shows the ensemble error percentage for a number of uncorrelated
    base learners. As is evident, the greatest reduction is applied when we add two
    uncorrelated base learners:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 正如显而易见的，随着我们向集成中添加更多的基学习器，集成的准确度也会提高，前提是每个学习器之间彼此独立。当然，这一点越来越难以实现。此外，还存在递减收益法则。每增加一个不相关的基学习器，减少的整体误差会比之前添加的基学习器少。下图展示了多个不相关基学习器的集成误差百分比。显然，添加两个不相关基学习器时，误差的减少最大：
- en: '![](img/4da2f13b-6bd6-4d1d-915f-6a2600b83d12.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4da2f13b-6bd6-4d1d-915f-6a2600b83d12.png)'
- en: The relation between the number of base learners and the ensemble error
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 基学习器数量与集成误差之间的关系
- en: Identifying bias and variance
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别偏差和方差
- en: Although bias and variance have theoretical formulas, it is difficult to calculate
    their actual values. A simple way to estimate them empirically is with learning
    and validation curves.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管偏差和方差有理论公式，但很难计算它们的实际值。估算它们的一个简单方法是使用学习曲线和验证曲线进行经验计算。
- en: Validation curves
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证曲线
- en: Validation curves refer to an algorithm's achieved performance, given different
    hyperparameters. For each hyperparameter value, we perform k-fold cross validations
    and store the in-sample performance and out-of-sample performance. We then calculate
    and plot the mean and standard deviation of in-sample and out-of-sample performance
    for each hyperparameter value. By examining the relative and absolute performance,
    we can gauge the level of bias and variance in our model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 验证曲线指的是在不同超参数条件下，算法的实际表现。对于每个超参数值，我们执行K折交叉验证，并存储样本内表现和样本外表现。然后，我们计算并绘制每个超参数值的样本内和样本外表现的均值和标准差。通过检查相对和绝对表现，我们可以评估模型的偏差和方差水平。
- en: 'Borrowing the `KNeighborsClassifier` example from [Chapter 1](https://cdp.packtpub.com/hands_on_ensemble_learning_with_python/wp-admin/post.php?post=25&action=edit#post_24), *A
    Machine Learning Refresher*, we modify it in order to experiment with different
    neighbor numbers. We start by loading the required libraries and data. Notice
    that we import `validation_curve` from `sklearn.model_selection`. This is scikit-learn''s
    own implementation of validation curves:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 借用来自[第一章](https://cdp.packtpub.com/hands_on_ensemble_learning_with_python/wp-admin/post.php?post=25&action=edit#post_24)的`KNeighborsClassifier`示例，*《机器学习复习》*中，我们对其进行了修改，以便尝试不同的邻居数。我们首先加载所需的库和数据。请注意，我们从`sklearn.model_selection`导入了`validation_curve`，这是scikit-learn自带的验证曲线实现：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we define our features and targets (`x` and `y`), as well as our base
    learner. Furthermore, we define our parameter search space with `param_range =
    [2,3,4,5]` and use `validation_curve`. In order to use it, we must define our
    base learner, our features, targets, the parameter''s name that we wish to test,
    as well as the parameter''s values to test. Furthermore, we define the cross-validation''s
    K folds with `cv=10`, as well as the metric that we wish to calculate, with `scoring="accuracy"`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的特征和目标（`x`和`y`），以及我们的基学习器。此外，我们使用`param_range = [2,3,4,5]`定义参数搜索空间，并使用`validation_curve`。为了使用它，我们必须定义基学习器、特征、目标、我们希望测试的参数名称以及待测试的参数值。此外，我们还使用`cv=10`定义交叉验证的K折数，并设置我们希望计算的度量标准为`scoring="accuracy"`：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Afterward,we calculate the mean and standard deviation for both in-sample performance
    (`train_scores`) as well as out-of-sample performance (`test_scores`):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算样本内表现（`train_scores`）和样本外表现（`test_scores`）的均值和标准差：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we plot the means and deviations. We plot the means as curves, using
    `plt.plot`. In order to plot the standard deviations, we create a transparent
    rectangle surrounding the curves, with a width equal to the standard deviation
    at each hyperparameter value point. This is achieved with the use of `plt.fill_between`,
    by passing the value points as the first parameter, the lowest rectangle''s point
    as the second parameter, and the highest point as the third. Furthermore, `alpha=0.1`
    instructs `matplotlib` to make the rectangle transparent (combining the rectangle''s
    color with the background in a 10%-90% ratio, respectively):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们绘制均值和标准差。我们使用`plt.plot`绘制均值曲线。为了绘制标准差，我们创建一个透明矩形，包围这些曲线，矩形的宽度等于每个超参数值点的标准差。这是通过使用`plt.fill_between`实现的，方法是将值点作为第一个参数，最低矩形点作为第二个参数，最高点作为第三个参数。此外，`alpha=0.1`指示`matplotlib`使矩形透明（将矩形的颜色与背景按照10%-90%的比例进行混合）：
- en: Sections 3 and 4 are adapted from the scikit-learn examples found [https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 第3和第4节改编自[https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html)中的scikit-learn示例。
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The script finally outputs the following. As the curves close the distance between
    them, the variance generally reduces. The further away they both are from the
    desired accuracy (taking into account the irreducible error), the bias increases.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本最终输出以下结果。当曲线之间的距离缩小时，方差通常会减少。当它们远离期望的准确度时（考虑到不可减少的误差），偏差增加。
- en: 'Furthermore, the relative standard deviations are also an indicator of variance:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，相对标准差也是方差的一个指标：
- en: '![](img/2029909d-0236-464d-b286-61b25feaed3b.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2029909d-0236-464d-b286-61b25feaed3b.png)'
- en: Validation curves for K-Nearest-Neighbors, 2 to 5 neighbor
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: K-近邻算法的验证曲线，邻居数量从2到5
- en: 'The following table presents the bias and variance identification based on
    validation curves:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格展示了基于验证曲线的偏差和方差识别：
- en: '|  | **Great** | **Small** |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | **大** | **小** |'
- en: '| **Distance between curves** | High Variance | Low Variance |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| **曲线之间的距离** | 高方差 | 低方差 |'
- en: '| **Distance from desired accuracy** | High Bias | Low Bias |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| **与期望准确度的距离** | 高偏差 | 低偏差 |'
- en: '| **Relative rectangle area ratio** | High Variance | Low Variance |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| **相对矩形面积比** | 高方差 | 低方差 |'
- en: Bias and variance identification based on validation curves
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 基于验证曲线的偏差和方差识别
- en: Learning curves
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习曲线
- en: Another way to identify bias and variance is to generate learning curves. Like
    validation curves, we generate a number of in-sample and out-of-sample performance
    statistics with cross-validation. Instead of experimenting with different hyperparameter
    values, we utilize different amounts of training data. Again, by examining the
    means and standard deviations of in-sample and out-of-sample performance, we can
    get an idea about the amount of bias and variance inherent in our models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种识别偏差和方差的方法是生成学习曲线。与验证曲线类似，我们通过交叉验证生成一系列的样本内和样本外性能统计数据。我们不再尝试不同的超参数值，而是利用不同量的训练数据。通过检查样本内和样本外性能的均值和标准差，我们可以了解模型中固有的偏差和方差。
- en: 'Scikit-learn implements learning curves in the `sklearn.model_selection` module
    as `learning_curve`. Once again, we will use the `KNeighborsClassifier` example
    from [Chapter 1](57f23be7-7e0d-4fa5-b7a5-08e0caf8e704.xhtml), *A Machine Learning
    Refresher*. First, we import the required libraries and load the breast cancer
    dataset:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn在`sklearn.model_selection`模块中实现了学习曲线，名为`learning_curve`。我们将再次使用[第1章](57f23be7-7e0d-4fa5-b7a5-08e0caf8e704.xhtml)中*机器学习复习*的`KNeighborsClassifier`示例。首先，我们导入所需的库并加载乳腺癌数据集：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Following that, we define the amount of training instances that will be used
    at each cross-validation set with `train_sizes = [50, 100, 150, 200, 250, 300]`,
    instantiate the base learner, and call `learning_curve`. The function returns
    a tuple of the train set sizes, the in-sample performance scores, and out-of-sample
    performance scores. The function accepts the base learner, the dataset features
    and targets, and the train set sizes as parameters in a list with `train_sizes=train_sizes`
    and the number of cross-validation folds with `cv=10`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义每个交叉验证集将使用的训练实例数量为`train_sizes = [50, 100, 150, 200, 250, 300]`，实例化基础学习器，并调用`learning_curve`。该函数返回一个包含训练集大小、样本内性能得分和样本外性能得分的元组。该函数接受基础学习器、数据集特征和目标，以及训练集大小作为参数，其中`train_sizes=train_sizes`，并且交叉验证的折数为`cv=10`：
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Again, we calculate the mean and standard deviation of in-sample and out-of-sample
    performance:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们计算样本内和样本外性能的均值和标准差：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, we plot the means and standard deviations as curves and rectangles,
    as we did before:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们像之前一样，将均值和标准差绘制为曲线和矩形：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The final output is depicted as follows. The model seems to reduce its variance
    for the first 200 training samples. After that, it seems that the means diverge,
    as well as the standard deviation of the cross-validation score increasing, thus
    indicating an increase in variance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出如下图所示。模型似乎在前200个训练样本中降低了其方差。之后，均值似乎开始发散，同时交叉验证得分的标准差增加，从而表明方差的增加。
- en: Note that, although both curves have above 90% accuracy for training sets with
    at least 150 instances, this does not imply low bias. Datasets that are highly
    separable (good quality data with low noise) tend to produce such curves—no matter
    what combination of algorithms and hyperparameters we choose. Moreover, noisy
    datasets (for example, instances with the same features that have different targets)
    will not be able to produce high accuracy models—no matter what techniques we
    use.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管两条曲线在训练集（至少包含150个实例）的准确率都超过90%，但这并不意味着低偏差。高度可分的数据集（良好的质量数据，噪声低）往往会产生这样的曲线——无论我们选择什么样的算法组合和超参数。此外，噪声数据集（例如，具有相同特征但目标不同的实例）将无法生成高准确率的模型——无论我们使用什么技术。
- en: 'Thus, bias must be measured by comparing the learning and validation curves
    to a desired accuracy (one that is considered achievable, given the dataset quality),
    rather than its absolute value:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，偏差应通过将学习曲线和验证曲线与期望的准确率（考虑到数据集质量认为可以达到的准确率）进行比较来衡量，而不是通过其绝对值：
- en: '![](img/d1e5331a-a5fe-4ce5-bc55-42fedbddc08e.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1e5331a-a5fe-4ce5-bc55-42fedbddc08e.png)'
- en: Learning curves for K-Nearest-Neighbors, 50 to 300 training instances
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: K近邻的学习曲线，50到300个训练实例
- en: Ensemble methods
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成方法
- en: 'Ensemble methods are divided into two major classes or taxonomies: generative
    and non-generative methods. Non-generative methods are focused on combining the
    predictions of a set of pretrained models. These models are usually trained independently
    of one another, and the ensemble algorithm dictates how their predictions will
    be combined. Base classifiers are not affected by the fact that they exist in
    an ensemble.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法分为两大类或分类法：生成方法和非生成方法。非生成方法侧重于组合一组预训练模型的预测。这些模型通常是独立训练的，集成算法决定如何组合它们的预测。基础分类器不受其存在于集成中的影响。
- en: 'In this book, we will cover two main non-generative methods: voting and stacking.
    Voting, as the name implies(see [Chapter 3](ad9aa66b-7b30-4779-8914-0ff58140b3e8.xhtml), *Voting*),
    refers to techniques that allow models to vote in order to produce a single answer,
    similar to how individuals vote in national elections. The most popular (most
    voted for) answer is selected as the winner. [Chapter 4](49a05219-d6cb-4893-aaac-49280842b647.xhtml), *Stacking*,
    on the other hand, refers to methods that utilize a model (the meta-learner) that
    learns how to best combine the base learner''s predictions. Although stacking
    entails the generation of a new model, it does not affect the base learners, thus
    it is a non-generative method.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将涵盖两种主要的非生成方法：投票和堆叠。如其名所示（参见[第3章](ad9aa66b-7b30-4779-8914-0ff58140b3e8.xhtml)，*投票*），投票指的是允许模型投票以产生单一答案的技术，类似于个人在国家选举中的投票方式。最受欢迎（投票最多）的答案被选为赢家。[第4章](49a05219-d6cb-4893-aaac-49280842b647.xhtml)，*堆叠*，则指的是利用一个模型（元学习器）来学习如何最好地组合基础学习器的预测。尽管堆叠涉及生成一个新模型，但它不影响基础学习器，因此它是一种非生成方法。
- en: Generative methods, on the other hand, are able to generate and affect the base
    learners that they use. They can either tune their learning algorithm or the dataset
    used to train them, in order to ensure diversity and high model performance. Furthermore,
    some algorithms can induce randomness in models, in order to further enforce diversity.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，生成方法能够生成并影响它们所使用的基本学习器。它们可以调整其学习算法或用于训练它们的数据集，以确保多样性和高模型性能。此外，一些算法可以在模型中引入随机性，从而进一步加强多样性。
- en: The main generative methods that we will cover in this book are bagging, boosting,
    and random forests. Boosting is a technique mainly targeting biased models. Its
    main idea is to sequentially generate models, such that each new model addresses
    biases inherent in the previous models. Thus, by iteratively correcting previous
    errors, the final ensemble has a significantly lower bias. Bagging aims to reduce
    variance. The bagging algorithm resamples instances of the training dataset, creating
    many individual and diverse datasets, originating from the same dataset. Afterward,
    a separate model is trained on each sampled dataset, forcing diversity between
    the ensemble models. Finally, Random Forests, is similar to bagging, in that it
    resamples from the training dataset. Instead of sampling instances, it samples
    features, thus creating even more diverse trees, as features strongly correlated
    to the target may be absent in many trees.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们将介绍的主要生成方法包括袋装（bagging）、提升（boosting）和随机森林（random forests）。提升是一种主要针对偏差模型的技术，其基本思想是顺序生成模型，使得每个新模型都能解决前一个模型中的偏差。因此，通过迭代修正之前的错误，最终的集成模型偏差会显著降低。袋装旨在减少方差。袋装算法对训练数据集中的实例进行重采样，创建许多源自同一数据集的独立且多样化的数据集。随后，在每个采样的数据集上训练单独的模型，从而强制集成模型之间的多样性。最后，随机森林与袋装相似，都是对训练数据集进行重采样。不同的是，它采样的是特征，而不是实例，这样可以生成更多样化的树，因为与目标高度相关的特征可能在许多树中缺失。
- en: Difficulties in ensemble learning
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习中的难点
- en: Although ensemble learning can greatly increase the performance of machine learning
    models, it comes at a cost. There are difficulties and drawbacks in correctly
    implementing it. Some of these difficulties and drawbacks will now be discussed.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然集成学习可以显著提高机器学习模型的性能，但它也有成本。正确实现集成学习存在一些困难和缺点，接下来将讨论其中的一些困难和缺点。
- en: Weak or noisy data
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弱数据或噪声数据
- en: The most important ingredient of a successful model is the dataset. If the data
    contains noise or incomplete information, there is not a single machine learning
    technique that will generate a highly performant model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一个成功模型最重要的要素是数据集。如果数据中包含噪声或不完整信息，那么没有任何一种机器学习技术能够生成一个高性能的模型。
- en: Let's illustrate this with a simple example. Suppose we study populations (in
    the statistical sense) of cars and we gather data about the color, shape, and
    manufacturer. It is difficult to generate a very accurate model for either variable,
    as a lot of cars are the same color and shape but are made by a different manufacturer.
    The following table depicts this sample dataset.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来说明这一点。假设我们研究的是汽车的种群（统计学意义上的），并收集了关于颜色、形状和制造商的数据。对于任一变量，生成非常准确的模型都是困难的，因为很多汽车颜色和形状相同，但制造商不同。以下表格展示了这个样本数据集。
- en: 'The best any model can do is achieve 33% classification accuracy, as there
    are three viable choices for any given feature combination. Adding more features
    to the dataset can greatly improve the model''s performance. Adding more models
    to an ensemble cannot improve performance:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 任何模型能够做到的最好结果是达到33%的分类准确率，因为对于任一给定的特征组合，都有三种可能的选择。向数据集添加更多特征可以显著提高模型的性能。而向集成中添加更多模型则无法提高性能：
- en: '| **Color** | **Shape** | **Manufacturer** |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| **颜色** | **形状** | **制造商** |'
- en: '| Black | Sedan | BMW |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 黑色 | 轿车 | 宝马 |'
- en: '| Black | Sedan | Audi |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 黑色 | 轿车 | 奥迪 |'
- en: '| Black | Sedan | Alfa Romeo |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 黑色 | 轿车 | 阿尔法·罗密欧 |'
- en: '| Blue | Hatchback | Ford |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 蓝色 | 两厢车 | 福特 |'
- en: '| Blue | Hatchback | Opel |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 蓝色 | 两厢车 | 欧宝 |'
- en: '| Blue | Hatchback | Fiat |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 蓝色 | 两厢车 | 菲亚特 |'
- en: Car dataset
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 汽车数据集
- en: Understanding interpretability
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解可解释性
- en: By employing a large number of models, interpretability is greatly reduced.
    For example, a single decision tree can easily explain how it produced a prediction,
    by simply following the decisions made at each node. On the other hand, it is
    difficult to interpret why an ensemble of 1,000 trees predicted a single value.
    Moreover, depending on the ensemble method, there may be more to explain than
    the prediction process itself. How and why did the ensemble choose to train these
    specific models. Why did it not choose to train other models? Why did it not choose
    to train more models?
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用大量模型，模型的可解释性大大降低。例如，单个决策树可以通过简单地跟随每个节点的决策，轻松地解释它是如何产生预测的。另一方面，很难解释为什么一个由1000棵树组成的集成预测了一个单一的值。此外，根据集成方法的不同，可能需要解释的不仅仅是预测过程本身。集成是如何以及为什么选择训练这些特定的模型的？为什么它没有选择训练其他模型？为什么它没有选择训练更多的模型？
- en: When the model's results are to be presented to an audience, especially a not-so-highly-technical
    audience, simpler but more easily explainable models may be a better solution.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型的结果需要向观众呈现时，尤其是面对技术水平不高的观众时，简单且易于解释的模型可能是更好的解决方案。
- en: 'Furthermore, when the prediction must also include a probability (or confidence
    level), some ensemble methods (such as boosting) tend to deliver poor probability
    estimates:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当预测结果还需要包括概率（或置信度）时，一些集成方法（如提升法）往往会给出较差的概率估计：
- en: '![](img/ccb77e87-ebd1-47a0-b835-3e054f2bb8f3.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ccb77e87-ebd1-47a0-b835-3e054f2bb8f3.png)'
- en: Interpretability of a single tree versus a 1000
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 单棵树与1000棵树的可解释性
- en: Computational cost
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算成本
- en: Another drawback of ensembles is the computational cost they impose. Training
    a single neural network is computationally expensive. Training a 1000 of them
    requires a 1000 times more computational resources. Furthermore, some methods
    are sequential by nature. This means that it is not possible to harness the power
    of distributed computing. Instead, each new model must be trained when the previous
    model is completed. This imposes time penalties on the model's development process,
    on top of the increased computational cost.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型的另一个缺点是它们带来的计算成本。训练单个神经网络的计算成本很高。训练1000个神经网络则需要1000倍的计算资源。此外，一些方法本质上是顺序的。这意味着无法利用分布式计算的优势。相反，每个新的模型必须在前一个模型完成后才开始训练。这不仅增加了计算成本，还对模型开发过程带来了时间上的惩罚。
- en: Computational costs do not only hinder the development process; when the ensemble
    is put into production, the inference time will suffer as well. If the ensemble
    consists of 1,000 models, then all of those models must be fed with new data,
    produce predictions, and then those predictions must be combined in order to produce
    the ensemble output. In latency-sensitive settings (financial exchanges, real-time
    systems, and so on), sub-millisecond execution times are expected, thus a few
    microseconds of added latency can make a huge difference.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 计算成本不仅会阻碍开发过程；当集成模型投入生产时，推理时间也会受到影响。如果集成由1000个模型组成，那么所有这些模型必须输入新的数据，生成预测，然后将这些预测结合起来，才能产生集成输出。在延迟敏感的环境中（如金融交易、实时系统等），通常期望亚毫秒的执行时间，因此几微秒的延迟增加可能会造成巨大的差异。
- en: Choosing the right models
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的模型
- en: Finally, the models that comprise the ensemble must possess certain characteristics.
    There is no point in creating any ensemble from a number of identical models.
    Generative methods may produce their own models, but the algorithm used as well
    as its initial hyperparameters are usually selected by the analyst. Furthermore,
    the model's achievable diversity depends on a number of factors, such as the size
    and quality of the dataset, and the learning algorithm itself.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，组成集成模型的模型必须具备一定的特征。没有意义从多个相同的模型中创建集成。生成方法可以生成自己的模型，但所使用的算法以及其初始超参数通常由分析师选择。此外，模型的可实现多样性取决于多个因素，例如数据集的大小和质量，以及学习算法本身。
- en: A single model that is similar in behavior to the data-generating process will
    usually outperform any ensemble, both in terms of accuracy as well as latency.
    In our bias-variance example, the simple sine function will always outperform
    any ensemble, as the data is generated from the same function with some added
    noise. An ensemble of many linear regressions may be able to approximate the sine
    function, but it will always require more time to train and execute. Furthermore,
    it will not be able to generalize (predict out-of-sample) as well as the sine
    function.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一个与数据生成过程类似行为的单一模型通常会在准确性和延迟方面优于任何集成模型。在我们的偏差-方差示例中，简单的正弦函数始终会优于任何集成模型，因为数据是从同一个函数生成的，只是添加了一些噪声。许多线性回归的集成可能能够逼近正弦函数，但它们总是需要更多时间来训练和执行。此外，它们将无法像正弦函数那样很好地泛化（预测样本外数据）。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we presented the concepts of bias and variance, as well as
    the trade-off between them. They are essential in understanding how and why a
    model may under-perform, either in-sample or out-of-sample. We then introduced
    the concept and motivation of ensemble learning, how to identify bias and variance
    in models, as well as basic categories of ensemble learning methods. We presented
    ways to measure and plot bias and variance, using scikit-learn and matplotlib.
    Finally, we talked about the difficulties and drawbacks of implementing ensemble
    learning methods. Some key points to remember are the following.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了偏差和方差的概念及其之间的权衡。这些对于理解模型可能在样本内或样本外表现不佳的原因至关重要。然后，我们介绍了集成学习的概念和动机，以及如何在模型中识别偏差和方差，以及集成学习方法的基本类别。我们介绍了使用scikit-learn和matplotlib测量和绘制偏差和方差的方法。最后，我们讨论了实施集成学习方法的困难和缺点。记住的一些关键点如下。
- en: High-bias models usually have difficulty performing well in-sample. This is
    also called **underfitting**. It is due to the model's simplicity (or lack of
    complexity). High-variance models usually have difficulty generalizing or performing
    well out-of-sample, while they perform reasonably well in-sample. This is called
    **overfitting**. It is usually due to the model's unnecessary complexity. The
    **b****ias-variance trade-off** refers to the fact that as the model's complexity
    increases, its bias decreases, while its variance increases. Ensemble learning
    aims to address high bias or variance, by combining the predictions of many diverse
    models. These models are usually called **base-learners**. For model selection,
    **v****alidation curves** indicate how a model performs in-sample and out-of-sample
    for a given set of hyperparameters. **Learning curves** are the same as validation
    curves but instead of a set of hyperparameters, they use different train set sizes. Substantial
    distance between the train and test curves indicates high variance. A big rectangle
    area around the test curve also indicates high variance. A substantial distance
    between both curves from the target accuracy indicates high bias. Generative methods
    have control over the generation and training of their base learners; non-generative
    methods do not. Ensemble learning can have a negligible or negative impact on
    performance when data is poor or models are correlated. It can impact negatively
    on the interpretability of models and the computational resources required.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 高偏差模型通常在样本内表现不佳。这也被称为**欠拟合**。这是由于模型的简单性（或缺乏复杂性）导致的。高方差模型通常在样本内表现良好，但在样本外泛化或表现良好较难，这被称为**过拟合**。这通常是由于模型的不必要复杂性引起的。**偏差-方差权衡**指的是随着模型复杂性的增加，其偏差减少，而方差增加的事实。集成学习旨在通过结合许多不同模型的预测来解决高偏差或方差问题。这些模型通常被称为**基学习器**。对于模型选择，**验证曲线**指示了模型在给定一组超参数的情况下在样本内和样本外的表现。**学习曲线**与验证曲线相同，但它们使用不同的训练集大小而不是一组超参数。训练曲线和测试曲线之间的显著距离表示高方差。测试曲线周围的大矩形区域也表示高方差。两条曲线与目标准确率之间的显著距离表示高偏差。生成方法可以控制其基学习器的生成和训练；非生成方法则不能。当数据质量差或模型相关时，集成学习对性能可能会产生微乎其微或负面影响。它可能会对模型的解释能力和所需的计算资源产生负面影响。
- en: In the next chapter, we will present the Voting ensemble, as well as how to
    use it for both regression and classification problems.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍投票集成，以及如何将其用于回归和分类问题。
