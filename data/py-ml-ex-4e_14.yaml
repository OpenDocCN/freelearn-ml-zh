- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: 'Building an Image Search Engine Using CLIP: a Multimodal Approach'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CLIP 构建图像搜索引擎：一种多模态方法
- en: In the previous chapter, we focused on Transformer models such as BERT and GPT,
    leveraging their capabilities for sequence learning tasks. In this chapter, we’ll
    explore a multimodal model, which seamlessly connects visual and textual data.
    With its dual encoder architecture, this model learns the relationships between
    visual and textual concepts, enabling it to excel in tasks involving image and
    text. We will delve into its architecture, key components, and learning mechanisms,
    leading to a practical implementation of the model. We will then build a multimodal
    image search engine with text-to-image and image-to-image capabilities. To top
    it all off, we will tackle an awesome zero-shot image classification project!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们重点介绍了 Transformer 模型，如 BERT 和 GPT，利用它们在序列学习任务中的能力。在本章中，我们将探讨一种多模态模型，它能够无缝连接视觉和文本数据。通过其双编码器架构，该模型学习视觉和文本概念之间的关系，使其在涉及图像和文本的任务中表现出色。我们将深入探讨其架构、关键组件和学习机制，并进行模型的实际实现。接下来，我们将构建一个具备文本到图像和图像到图像能力的多模态图像搜索引擎。最后，我们将完成一个令人兴奋的零样本图像分类项目！
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introducing the CLIP model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 CLIP 模型
- en: Getting started with the dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用数据集
- en: Architecting the CLIP model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建 CLIP 模型架构
- en: Finding images with words
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用文字查找图像
- en: Introducing the CLIP model
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 CLIP 模型
- en: We have explored computer vision in *Chapter 11*, *Categorizing Images of Clothing
    with Convolutional Neural Networks*, and NLP in *Chapter 12*, *Making Predictions
    with Sequences Using Recurrent Neural Networks*, and *Chapter 13*, *Advancing
    Language Understanding and Generation with the Transformer Models*. In this chapter,
    we will delve into a model that bridges the realms of computer vision and NLP,
    the **Contrastive Language–Image Pre-Training** (**CLIP**) model developed by
    OpenAI. Unlike traditional models that are specialized for either computer vision
    or natural language processing, CLIP is trained to understand both **modalities**
    (image and text) in a unified manner. Hence, CLIP excels at understanding and
    generating relationships between images and natural language.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *第 11 章*《使用卷积神经网络对服装图像进行分类》中探讨了计算机视觉，在 *第 12 章*《使用循环神经网络进行序列预测》和 *第 13 章*《通过
    Transformer 模型提升语言理解和生成》中探讨了自然语言处理。在本章中，我们将深入研究一个连接计算机视觉与自然语言处理领域的模型——由 OpenAI
    开发的 **对比语言-图像预训练**（**CLIP**）模型。与传统的专门处理计算机视觉或自然语言处理的模型不同，CLIP 被训练以统一的方式理解这两种 **模态**（图像和文本）。因此，CLIP
    在理解和生成图像与自然语言之间的关系方面表现卓越。
- en: A modality in ML/AI is a specific way of representing information. Common modalities
    include text, images, audio, video, and even sensor data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习/人工智能中，**模态**是指表示信息的特定方式。常见的模态包括文本、图像、音频、视频，甚至传感器数据。
- en: Excited to delve into the workings of CLIP? Let’s explore and discover more
    about how it works!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 想深入了解 CLIP 的工作原理吗？让我们一起探索并发现它是如何工作的！
- en: Understanding the mechanism of the CLIP model
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 CLIP 模型的机制
- en: 'CLIP is designed to learn representations of images and corresponding textual
    descriptions simultaneously. The model learns to associate similar pairs and disassociate
    dissimilar pairs of images and text. Its unique architecture (see *Figure 14.1*
    below) enables it to develop semantic connections between images and their textual
    descriptions:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 旨在同时学习图像及其对应文本描述的表示。该模型学习将相似的图像和文本对关联起来，并将不相似的图像和文本对分开。其独特的架构（见下图 *Figure
    14.1*）使其能够在图像与文本描述之间建立语义联系：
- en: '![](img/B21047_14_01.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_14_01.png)'
- en: 'Figure 14.1: CLIP architecture (image based on Figure 1 in “Learning Transferable
    Visual Models From Natural Language Supervision”: [https://arxiv.org/pdf/2103.00020.pdf](https://arxiv.org/pdf/2103.00020.pdf))'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.1：CLIP 架构（图基于《从自然语言监督中学习可迁移视觉模型》中的图 1：[https://arxiv.org/pdf/2103.00020.pdf](https://arxiv.org/pdf/2103.00020.pdf)）
- en: As you can see, it utilizes a dual-encoder architecture that integrates both
    vision and text encoder. The output from the vision encoder and the output from
    the text encoder are projected into a shared space. It then evaluates the placement
    of these image-text pairs based on their similarity. This shared semantic space
    allows CLIP to perform various vision-language tasks, such as image classification,
    object detection, and image retrieval.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，它采用了一个双编码器架构，集成了视觉编码器和文本编码器。视觉编码器和文本编码器的输出被投射到一个共享空间中。然后，它基于这些图像-文本对的相似性来评估它们的位置。这个共享的语义空间使得CLIP能够执行各种视觉-语言任务，如图像分类、物体检测和图像检索。
- en: Here are the key components of the CLIP model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是CLIP模型的关键组件。
- en: Vision encoder
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 视觉编码器
- en: The vision encoder (also called image encoder) in CLIP is responsible for processing
    and encoding image inputs. It is typically implemented as a **CNN**. Recall that
    CNNs are well suited for image-related tasks, as they can effectively capture
    hierarchical features in images. The output of the vision encoder for an input
    image is a fixed-size vector representation. The embedding vector captures the
    semantic content of the image.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP中的视觉编码器（也称为图像编码器）负责处理和编码图像输入。它通常实现为**卷积神经网络（CNN）**。回想一下，CNN非常适合图像相关任务，因为它们可以有效地捕捉图像中的层次化特征。对于输入图像，视觉编码器的输出是一个固定大小的向量表示。这个嵌入向量捕捉了图像的语义内容。
- en: There are two main architectures used for the vision encoder. The first version
    is a modified **ResNet mode** based on the ResNet-50 model. Additionally, the
    average pooling layer is substituted with an attention pooling mechanism. This
    attention pooling is realized as a single layer of multi-head attention, with
    the query conditioned on the recently introduced **Vision Transformer** (`https://huggingface.co/google/vit-base-patch16-224`).
    It includes an additional normalization layer just before the Transformer as the
    only adjustment.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉编码器使用了两种主要架构。第一种版本是基于ResNet-50模型的**修改版ResNet模式**。此外，平均池化层被替换为注意力池化机制。这个注意力池化通过多头注意力的单层实现，查询条件是最近引入的**视觉变换器（Vision
    Transformer）**（`https://huggingface.co/google/vit-base-patch16-224`）。它在Transformer之前还包括了一个额外的归一化层，这是唯一的调整。
- en: It is important to note that the generated visual embeddings exist in **a shared
    space** with embeddings from the text encoder. This shared space projection enables
    direct comparisons between visual and textual representations. If an image and
    a textual description are semantically related, they would be mapped to closer
    points in this space. For example, an image of a cat and the corresponding text
    “a fluffy cat” would be close together in this space, indicating their semantic
    similarity.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，生成的视觉嵌入存在于与文本编码器嵌入**共享的空间**中。这个共享空间的投射使得视觉和文本表示可以直接进行比较。如果图像和文本描述在语义上相关，它们将在这个空间中被映射到更接近的位置。例如，一张猫的图片和对应的文本“a
    fluffy cat”将在该空间中靠得很近，表示它们的语义相似性。
- en: The vision encoder is pre-trained on a large and diverse dataset containing
    images and their associated textual descriptions. For example, OpenAI mentioned
    in the CLIP paper ([https://openai.com/index/clip](https://openai.com/index/clip))
    that their model was trained on a collection of 400 million image-text pairs obtained
    from crawling the internet. The pre-training process allows the vision encoder
    to learn rich and generalized visual representations. Furthermore, the learned
    representations are task-agnostic. Hence, we can fine-tune a CLIP model for a
    wide range of text-image applications.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉编码器在一个包含图像及其相关文本描述的大规模、多样化数据集上进行了预训练。例如，OpenAI在CLIP论文中提到（[https://openai.com/index/clip](https://openai.com/index/clip)）他们的模型是在一个包含4亿对图像-文本的集合上训练的，这些数据通过爬取互联网获得。预训练过程使得视觉编码器能够学习丰富且通用的视觉表示。此外，所学到的表示是任务无关的。因此，我们可以对CLIP模型进行微调，以适应各种文本-图像应用。
- en: Text encoder
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本编码器
- en: Similarly, the text encoder is responsible for processing and encoding textual
    inputs. The process begins with tokenization. The tokenized text is then passed
    through an embedding layer and converted into a fixed-size high-dimensional vector.
    Additionally, to preserve important sequential information in the text, we add
    **positional encoding** to the embeddings. The resulting embeddings can capture
    the semantic content of the text.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，文本编码器负责处理和编码文本输入。该过程从分词开始。分词后的文本通过嵌入层转换成固定大小的高维向量。此外，为了保留文本中的重要序列信息，我们会在嵌入中加入**位置编码**。最终生成的嵌入能够捕捉文本的语义内容。
- en: The text encoder is implemented as a Transformer with a particular architecture.
    For instance, the OpenAI team utilized a 12-level, 512-wide model with 8 attention
    heads and 63 million parameters in total, and its maximum sequence length was
    restricted to 76.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 文本编码器实现为一个具有特定架构的Transformer。例如，OpenAI团队使用了一个12层、512宽的模型，配备8个注意力头和总共6300万个参数，最大序列长度限制为76。
- en: As mentioned earlier, the text embeddings are in a shared space with the embeddings
    from the vision encoder. This allows direct comparisons between visual and textual
    inputs and cross-modal understanding. Similarly, pre-training on a diverse dataset
    enables the model to learn generalizable contextual understanding from various
    linguistic contexts. The text encoder can be fine-tuned for various downstream
    tasks in collaboration with the vision encoder.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，文本嵌入与视觉编码器的嵌入位于共享空间中。这使得视觉和文本输入可以直接比较，实现跨模态理解。同样，在多样化数据集上进行预训练使得模型能够从不同语言背景中学习可泛化的上下文理解。文本编码器可以与视觉编码器协作，针对各种下游任务进行微调。
- en: Contrastive learning
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对比学习
- en: Contrastive learning is the training strategy in CLIP. It teaches the model
    to differentiate between similar and dissimilar image-text pairs. During training,
    CLIP is presented with positive and negative image-text pairs. A positive pair
    consists of an image and description that are semantically related. On the other
    hand, a negative pair is formed by combining an image with a randomly chosen description,
    creating mismatches.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习是CLIP中的训练策略。它教会模型区分相似和不相似的图像-文本对。在训练过程中，CLIP会接收正向和负向的图像-文本对。正向对由语义相关的图像和描述组成；而负向对则是将一张图像与随机选取的描述配对，形成不匹配。
- en: 'Contrastive learning focuses on bringing embeddings of positive pairs closer
    together in the shared embedding space, while pushing embeddings of negative pairs
    further apart. This separation is achieved through a **contrastive loss function**.
    Let’s break down the contrastive loss calculation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习的核心是将正向对的嵌入拉近共享嵌入空间，同时将负向对的嵌入推得更远。这种分离是通过**对比损失函数**实现的。让我们来详细分析对比损失的计算：
- en: 'Embedding generation:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入生成：
- en: Given *N* images *I* and the corresponding text descriptions *T*, the CLIP model
    first creates image embeddings ![](img/B21047_14_001.png) and text embeddings
    ![](img/B21047_14_002.png) using its dual encoder (vision encoder and text encoder)
    architecture.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 给定*N*张图像*I*和相应的文本描述*T*，CLIP模型首先使用其双重编码器（视觉编码器和文本编码器）架构生成图像嵌入！[](img/B21047_14_001.png)和文本嵌入！[](img/B21047_14_002.png)。
- en: 'Similarity matrix calculation:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相似度矩阵计算：
- en: 'Since embeddings ![](img/B21047_14_001.png)`and` ![](img/B21047_14_002.png)
    are in the same space, we can calculate pair-wise similarities *S*. For image
    *i* and text *j*, their similarity ![](img/B21047_14_005.png) is the cosine similarity
    image embedding ![](img/B21047_14_006.png) and text embedding ![](img/B21047_14_007.png):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于嵌入！[](img/B21047_14_001.png)`和`！[](img/B21047_14_002.png)位于同一空间，我们可以计算成对的相似度*S*。对于图像*i*和文本*j*，它们的相似度！[](img/B21047_14_005.png)是图像嵌入！[](img/B21047_14_006.png)与文本嵌入！[](img/B21047_14_007.png)的余弦相似度：
- en: '![](img/B21047_14_008.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_14_008.png)'
- en: Here, ![](img/B21047_14_009.png). The goal is to maximize the similarity of
    image and text embeddings for *N* positive pairs, while minimizing the similarity
    for the embeddings of *N*² − *N* negative pairings.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B21047_14_009.png)。目标是最大化*N*个正向对的图像和文本嵌入的相似性，同时最小化*N*² − *N*个负向配对的嵌入相似性。
- en: 'Target matrix creation:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标矩阵创建：
- en: Next, we construct `the target ("ideal")` matrix *Y* `for learning.` `Here,`
    ![](img/B21047_14_010.png) if image *i* and text *j* are a `positive` pair (diagonal
    elements); ![](img/B21047_14_011.png) for all other pairs (off-diagonal elements).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们构造用于学习的目标（“理想”）矩阵 *Y*。`这里，` ![](img/B21047_14_010.png) 如果图像 *i* 和文本 *j*
    是 `正对`（对角元素）；![](img/B21047_14_011.png) 对于所有其他对（非对角元素）。
- en: 'Cross-entropy loss computation:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交叉熵损失计算：
- en: 'With the similarity matrix *S* and target matrix *Y*, we then compute the cross-entropy
    loss for both image and text modalities. Here is the loss for image alignment:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 给定相似度矩阵 *S* 和目标矩阵 *Y*，我们接着计算图像和文本模态的交叉熵损失。这里是图像对齐的损失：
- en: '![](img/B21047_14_012.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_14_012.png)'
- en: It measures how well the model predicts the correct image given a text description.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 它衡量的是模型在给定文本描述的情况下，预测正确图像的能力。
- en: 'The loss for text alignment, measuring how well the model predicts the correct
    description given an image, is:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 测量模型在给定图像的情况下预测正确描述的文本对齐损失是：
- en: '![](img/B21047_14_013.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_14_013.png)'
- en: 'Final loss computation:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终损失计算：
- en: 'The contrastive loss is the average of the image-based loss and the text-based
    loss:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对比损失是图像基础损失和文本基础损失的平均值：
- en: '![](img/B21047_14_014.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_14_014.png)'
- en: During training, the model’s parameters are updated to minimize the contrastive
    loss function. This drives the model to learn embeddings that align correctly
    paired images and text, while pushing apart mismatched pairs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，模型的参数会被更新以最小化对比损失函数。这驱使模型学习嵌入，使得正确配对的图像和文本对齐，而将不匹配的对分开。
- en: The contrastive learning objective contributes to effective cross-modal retrieval
    and understanding. Along with the dual-encoder architecture, a pre-trained CLIP
    model can perform various downstream image-text tasks without task-specific retraining.
    So what are those typical applications and scenarios? Let’s see next.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习目标有助于有效的跨模态检索和理解。结合双编码器架构，一个预训练的 CLIP 模型可以执行各种下游的图像-文本任务，而无需特定任务的重新训练。那么，这些典型的应用和场景有哪些呢？让我们接下来看看。
- en: Exploring applications of the CLIP model
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索 CLIP 模型的应用
- en: In this section, we will explain some common applications and use cases for
    the CLIP model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解释 CLIP 模型的一些常见应用和使用场景。
- en: Zero-shot image classification
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 零样本图像分类
- en: In a zero-shot learning setup, CLIP is presented with a task it has not been
    explicitly trained on. For instance, it might be asked to classify images into
    unseen categories, or to generate descriptions for images without having seen
    similar examples during pre-training.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在零样本学习的设置中，CLIP 面对的是一个它没有经过显式训练的任务。例如，它可能被要求将图像分类到从未见过的类别中，或在预训练期间没有见过类似示例的情况下，为图像生成描述。
- en: The first zero-shot application is image classification based on textual descriptions.
    We don’t need to perform any task-specific training thanks to the model’s pre-trained
    knowledge. The model can categorize the images based on their alignment with the
    descriptions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个零样本应用是基于文本描述的图像分类。由于模型的预训练知识，我们不需要进行任何特定任务的训练。模型可以根据图像与描述的对齐情况来对图像进行分类。
- en: 'For example, given three unseen images (*Image 1*: a photo of a red vintage
    car on a city street; *Image 2*: a painting depicting a red car in an urban setting;
    *Image 3*: a cartoon illustration of a city with a red car), we pass the query
    text “A vintage red car parked on a city street” to the CLIP model. It may correctly
    rank *Image 1* as the most relevant to the query, with a red vintage car on a
    city street. *Image 3* may be ranked the least relevant due to its cartoon style,
    which is least aligned with the query.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定三张未见过的图像（*图像 1*：一张红色复古车停在城市街道上的照片；*图像 2*：一幅描绘红色汽车在城市环境中的画作；*图像 3*：一幅描绘城市的卡通插画，城市中有一辆红色汽车），我们将查询文本“在城市街道上停放的复古红色汽车”传递给
    CLIP 模型。它可能会正确地将 *图像 1* 排名为与查询最相关的图像，因为它展示了街道上的红色复古车。由于其卡通风格，*图像 3* 可能会被评为最不相关的图像，因为它与查询的对齐程度最低。
- en: The zero-shot learning capability makes CLIP useful for tasks where labeled
    examples are scarce or even unavailable. We’ve seen it can categorize images even
    for categories never seen during pre-training. In the next section, we will use
    it for zero-shot text classification.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本学习能力使得 CLIP 在标签样本稀缺甚至无法获取的任务中也能发挥作用。我们已经看到，它能够为那些在预训练期间从未见过的类别对图像进行分类。在下一节中，我们将使用它进行零样本文本分类。
- en: Zero-shot text classification
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 零样本文本分类
- en: Similarly, CLIP can classify new textual descriptions based on images. In zero-shot
    setting, we don’t need to provide labeled examples for fine-tuning. The model
    can categorize text inputs based on their alignment with the images.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，CLIP 还可以根据图像对新的文本描述进行分类。在零样本设置下，我们不需要为微调提供标注的示例。该模型可以根据文本与图像的匹配度对文本输入进行分类。
- en: 'For example, given a query image of a mountain landscape and three potential
    descriptions (*Text 1*: “A view of a serene mountain range,” *Text 2*: “The majestic
    peaks and valleys of the mountains,” and *Text 3*: “Hiking trails in mountainous
    regions.”), the CLIP model may score *Text 1* the most relevant due to its highest
    alignment with the query image.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定一张山脉景观的查询图像和三种可能的描述（*文本 1*：“一览宁静的山脉景观”，“*文本 2*：“山脉的壮丽峰峦与山谷”，“*文本 3*：“山区的徒步小道”），CLIP
    模型可能会因与查询图像的最高匹配度而将*文本 1* 评分为最相关的描述。
- en: We’ve talked about CLIP for zero-shot image and text classification. In fact,
    we can extend it to content retrieval. Let’s see the next section.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论过 CLIP 用于零样本图像和文本分类。事实上，我们可以将其扩展到内容检索。接下来让我们看看下一部分。
- en: Image and text retrieval
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像与文本检索
- en: CLIP can be used to retrieve images relevant to a given text query, and vice
    versa,
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 可用于根据给定的文本查询检索相关的图像，反之亦然。
- en: For example, in image retrieval, we pass the text query “playful puppies” to
    an image search engine. The CLIP model retrieves images that best match the description
    of playful puppies. It also ranks them based on their alignment with the text
    query. Similarly, CLIP can also be used to retrieve and rank captions for images
    that accurately describe their content.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在图像检索中，我们将文本查询“顽皮的小狗”传递给图像搜索引擎。CLIP 模型检索出与“顽皮的小狗”描述最匹配的图像，并根据它们与文本查询的匹配度进行排序。类似地，CLIP
    也可以用来检索并排序准确描述图像内容的标题。
- en: We’ve demonstrated CLIP’s cross-modal retrieval abilities. In the next section,
    let’s look at its adoption in cross-modal generation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了 CLIP 的跨模态检索能力。在下一部分，我们将探讨其在跨模态生成中的应用。
- en: Image and text generation
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像与文本生成
- en: Beyond retrieving from the existing content pool, we can use CLIP to generate
    images based on textual prompts or to provide textual descriptions for images.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 除了从现有内容池中检索，我们还可以使用 CLIP 基于文本提示生成图像，或为图像提供文本描述。
- en: For instance, we can generate artistic images by giving the CLIP model a prompt
    like “a surreal painting of a robot riding a bicycle.” We can also ask the CLIP
    model to describe a picture of a modern kitchen. It may answer with “A contemporary
    kitchen design.”
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以通过给 CLIP 模型提供类似“一个机器人骑自行车的超现实主义画作”这样的提示，生成艺术图像。我们还可以要求 CLIP 模型描述一张现代厨房的图片，它可能回答：“一个现代化的厨房设计。”
- en: In fact, CLIP can answer many questions about the given images, beyond just
    providing descriptions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，CLIP 可以回答关于给定图像的许多问题，不仅仅是提供描述。
- en: Visual question answering (VQA)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉问答（VQA）
- en: CLIP can be adapted for visual question-answering tasks. It can be used to answer
    questions about images based on its knowledge of both visual and textual modalities.
    For example, we can use the model to answer questions like “What kind of animal
    is this?” or “How many people are in the photo?”
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 可以被调整用于视觉问答任务。它可以根据对视觉和文本模态的理解，回答关于图像的问题。例如，我们可以用该模型回答诸如“这是什么动物？”或者“照片中有多少人？”之类的问题。
- en: Transfer learning
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迁移学习
- en: Finally, we can fine-tune CLIP on specific downstream tasks, such as object
    detection, sentiment analysis, and custom classification tasks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以对 CLIP 进行微调，应用于特定的下游任务，例如物体检测、情感分析和自定义分类任务。
- en: During pre-training, the CLIP model gains a generalized understanding across
    modalities from a diverse range of images and text. Leveraging transfer learning,
    we don’t need to perform extensive task-specific training. It can be adopted for
    a wide range of vision and NLP applications.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练过程中，CLIP 模型通过多样的图像和文本，获得跨模态的广泛理解。利用迁移学习，我们不需要进行大量任务特定的训练，它可以广泛应用于视觉和 NLP
    的各种任务。
- en: Excited to start implementing CLIP? Let’s begin by delving into the dataset
    containing images and captions that we’ll use for the training process.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 迫不及待地想要开始实现 CLIP 吗？让我们从深入了解包含图像和标题的数据集开始，这将是我们训练过程中的基础。
- en: Getting started with the dataset
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用数据集
- en: 'We are going to use the `Flickr8k` dataset ([https://hockenmaier.cs.illinois.edu/8k-pictures.html](https://hockenmaier.cs.illinois.edu/8k-pictures.html)),
    created by M. Hodosh, P. Young, and J. Hockenmaier, described in *Framing Image
    Description as a Ranking Task: Data, Models and Evaluation Metrics*, *Journal
    of Artificial Intelligence Research*, Volume 47, pages 853–899 ([https://www.jair.org/index.php/jair/article/view/10833/25855](https://www.jair.org/index.php/jair/article/view/10833/25855)).
    It is commonly employed in various computer vision tasks, particularly image captioning.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用由M. Hodosh, P. Young和J. Hockenmaier创建的`Flickr8k`数据集（[https://hockenmaier.cs.illinois.edu/8k-pictures.html](https://hockenmaier.cs.illinois.edu/8k-pictures.html)），该数据集在《*将图像描述框架化为排序任务：数据、模型与评估指标*》一文中有所描述，刊登在《*人工智能研究期刊*》第47卷，853–899页（[https://www.jair.org/index.php/jair/article/view/10833/25855](https://www.jair.org/index.php/jair/article/view/10833/25855)）。该数据集通常应用于各种计算机视觉任务，尤其是图像标题生成。
- en: The `Flickr8k` dataset contains 8,000 images collected from the Flickr photo-sharing
    website. These images cover a diverse range of scenes, objects, and activities.
    Each image in the dataset is associated with five English sentences. These sentences
    serve as captions and provide textual descriptions of the image content.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`Flickr8k`数据集包含来自Flickr照片共享网站的8,000张图像。这些图像涵盖了各种各样的场景、物体和活动。数据集中的每张图像都与五个英文句子相关联，这些句子作为标题，提供了图像内容的文本描述。'
- en: One common use of the `Flickr8k` dataset is image captioning, where the goal
    is to train models to generate human-like captions for images. The `Flickr8k`
    dataset is often used by researchers and practitioners as a benchmark for image
    captioning models. It allows us to evaluate the ability of models to understand
    and describe visual content in natural language.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`Flickr8k`数据集的一个常见用途是图像标题生成，目标是训练模型为图像生成类人化的标题。`Flickr8k`数据集常被研究人员和实践者用作图像标题生成模型的基准。它使我们能够评估模型理解和描述视觉内容的能力，并以自然语言表达。'
- en: There is also an extended version called `Flickr30k`, which contains 30,000
    images with corresponding captions. The larger dataset provides a more extensive
    and diverse set of images for training and evaluation, but it consumes more computational
    resources. So we will focus on the `Flickr8k` dataset in this chapter.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个扩展版本叫做`Flickr30k`，它包含了30,000张带有对应标题的图像。这个更大的数据集提供了更广泛和多样化的图像集用于训练和评估，但它也消耗更多的计算资源。因此，在本章中我们将重点关注`Flickr8k`数据集。
- en: Obtaining the Flickr8k dataset
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取Flickr8k数据集
- en: 'To obtain the `Flickr8k` dataset, simply submit a request at [https://illinois.edu/fb/sec/1713398](https://illinois.edu/fb/sec/1713398).
    Upon request, dataset links will be emailed to you. One of the links will lead
    you to a downloadable file, `Flickr8k_Dataset.zip`, from which you can extract
    8,091 image files. Another link will direct you to a downloadable file called
    `Flickr8k_text.zip`. We will use the extracted file `Flickr8k.token.txt`, which
    contains the raw captions of the `Flickr8`k dataset. The first column is in the
    format of “image path # caption number,” and the second column is the corresponding
    caption.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '若要获取`Flickr8k`数据集，只需在[https://illinois.edu/fb/sec/1713398](https://illinois.edu/fb/sec/1713398)提交请求。收到请求后，数据集链接将通过电子邮件发送给你。链接之一将引导你下载文件`Flickr8k_Dataset.zip`，你可以从中提取8,091个图像文件。另一个链接会引导你下载名为`Flickr8k_text.zip`的文件。我们将使用提取的`Flickr8k.token.txt`文件，其中包含了`Flickr8k`数据集的原始标题。第一列的格式为“图像路径
    # 标题编号”，第二列则是对应的标题。'
- en: The dataset is also available on Kaggle, such as [https://www.kaggle.com/datasets/adityajn105/flickr8k/data](https://www.kaggle.com/datasets/adityajn105/flickr8k/data).
    The `captions.txt` file contains information mirroring that of the `Flickr8k.token.txt`
    file, but it is easier to use, as the first column contains only the image paths.
    For simplicity, we will use the `captions.txt` file instead of the original `Flickr8k.token.txt`
    file.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集也可以在Kaggle上获取，例如[https://www.kaggle.com/datasets/adityajn105/flickr8k/data](https://www.kaggle.com/datasets/adityajn105/flickr8k/data)。`captions.txt`文件包含了与`Flickr8k.token.txt`文件相似的信息，但更易于使用，因为第一列仅包含图像路径。为了简化操作，我们将使用`captions.txt`文件，而不是原始的`Flickr8k.token.txt`文件。
- en: Loading the Flickr8k dataset
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载Flickr8k数据集
- en: 'After extracting all the images from `Flickr8k_Dataset.zip` and getting the
    caption text file ready, we can now load the `Flickr8k` dataset into a custom
    PyTorch Dataset object. Follow the steps below:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在从`Flickr8k_Dataset.zip`中提取所有图像并准备好标题文本文件后，我们现在可以将`Flickr8k`数据集加载到自定义的PyTorch
    Dataset对象中。请按照以下步骤进行：
- en: 'First, we import the necessary packages:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的软件包：
- en: '[PRE0]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, the `Image` package will be used to load the image files.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`Image`包将用于加载图像文件。
- en: 'We then set the image directory and the caption file path as follows:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将图像目录和标题文件路径设置如下：
- en: '[PRE1]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we put all the extracted images in the `flickr8k/Flicker8k_Dataset` folder
    and the `captions.txt` file in the same root directory, `flickr8k`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将所有提取的图像放入`flickr8k/Flicker8k_Dataset`文件夹，并将`captions.txt`文件放在同一根目录`flickr8k`下。
- en: 'Next, we load the `DistilBRET` tokenizer as we did in the previous chapter,
    *Advancing Language Understanding and Generation with the Transformer Models*:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载`DistilBRET`分词器，就像在上一章中所做的那样，*《使用Transformer模型推进语言理解与生成》*：
- en: '[PRE2]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we create a custom PyTorch Dataset class for the `Flickr8k` dataset, as
    follows:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们为`Flickr8k`数据集创建一个自定义的PyTorch Dataset类，如下所示：
- en: '[PRE3]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Upon initialization, we define a`n` image transformation function using the
    `transforms` module from `torchvision`, including resizing the images to (224,
    224) pixels and converting them to tensors; we read the caption file line by line,
    extract image paths and captions, and store them in the `image_paths` and `captions`
    lists. The captions are tokenized and encoded using the given tokenizer, with
    options for truncation, padding, and a maximum length of 200 tokens. Results are
    stored in `caption_encodings`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化时，我们使用`torchvision`中的`transforms`模块定义一个`n`图像转换函数，包括将图像调整为（224，224）像素并转换为张量；我们逐行读取标题文件，提取图像路径和标题，并将它们存储在`image_paths`和`captions`列表中。标题使用给定的分词器进行标记化和编码，支持截断、填充以及最大长度为200个标记。结果存储在`caption_encodings`中。
- en: Upon retrieving an item from the dataset, the tokenized and encoded captions
    are stored in the `item` object along with the original caption. The image at
    the corresponding index is also loaded, transformed, and added to the `item`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据集中获取一个项目时，经过标记化和编码的标题会与原始标题一起存储在`item`对象中。相应索引的图像也会被加载、转换，并添加到`item`中。
- en: 'We initiate an instance of the custom `Dataset` class, as follows:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们初始化一个自定义的`Dataset`类实例，如下所示：
- en: '[PRE4]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Take a look at one data sample:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 看看一个数据样本：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The caption is `A child in a pink dress is climbing up a set of stairs in an
    entryway`*.* Let’s display the image itself using the following script:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 该标题是`一个穿粉色裙子的孩子正在爬上入口处的一组楼梯`*。*让我们使用以下脚本显示图像本身：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/B21047_14_02.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_14_02.png)'
- en: 'Figure 14.2: Image of the Flickr8k data sample (photo by Rick & Brenda Beerhorst,
    Flickr: [https://www.flickr.com/photos/studiobeerhorst/1000268201/](https://www.flickr.com/photos/studiobeerhorst/1000268201/))'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2：Flickr8k 数据样本的图像（照片来源：Rick & Brenda Beerhorst，Flickr：[https://www.flickr.com/photos/studiobeerhorst/1000268201/](https://www.flickr.com/photos/studiobeerhorst/1000268201/))
- en: 'The last step of data preparation is to create a `DataLoader` object to handle
    batching and shuffling. We set the batch size to 32 and initiate a `DataLoader`,
    based on the previously created dataset:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据准备的最后一步是创建一个`DataLoader`对象来处理批次和洗牌。我们将批次大小设置为32，并基于之前创建的数据集初始化一个`DataLoader`：
- en: '[PRE7]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that the dataset is prepared, let’s proceed to develop the CLIP model in
    the following section.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集准备好后，让我们继续开发下一节中的CLIP模型。
- en: Architecting the CLIP model
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建CLIP模型
- en: The vision encoder and text encoder are the two main components of the CLIP
    model. We will start with the vision encoder.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉编码器和文本编码器是CLIP模型的两个主要组成部分。我们将从视觉编码器开始。
- en: Vision encoder
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 视觉编码器
- en: Implementing the vision encoder is quite straightforward. We leverage the PyTorch
    `vision` library, which provides access to various pre-trained image models, including
    `ResNets` and `VisionTransformer`. Here, we opt for ResNet50 as our vision encoder
    as an example.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 实现视觉编码器非常简单。我们利用PyTorch的`vision`库，它提供了多种预训练的图像模型，包括`ResNets`和`VisionTransformer`。在这里，我们选择ResNet50作为我们的视觉编码器示例。
- en: 'The vision encoder ensures each image is encoded into a fixed-size vector,
    with the dimensionality matching the model’s output channels (in the case of ResNet50,
    the vector size is `2048`):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉编码器确保每个图像被编码为一个固定大小的向量，其维度与模型的输出通道匹配（对于ResNet50，向量大小为`2048`）：
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Upon initialization, we load the pre-trained ResNet50 model. Then, we remove
    the final classification layer, as we are using the ResNet50 model as a feature
    extractor instead of a classifier. Here, we freeze the model’s parameters by setting
    their `requires_grad` trainable attribute to false. You can also fine-tune the
    pre-trained ResNet50 model component by making the parameters trainable. The `forward`
    method is used to extract image embeddings from input images.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化时，我们加载预训练的ResNet50模型。然后，我们去除最终的分类层，因为我们将ResNet50模型用作特征提取器，而不是分类器。在这里，我们通过将模型的`requires_grad`训练属性设置为false，冻结模型的参数。你也可以通过使参数可训练来微调预训练的ResNet50模型组件。`forward`方法用于从输入图像中提取图像嵌入。
- en: We just implemented the `VisionEncoder` module based on the pre-trained ResNet50
    model. We use the model’s hidden layer output as the fixed-size vector representation
    for each image. Since we ignore its final classification layer, the ResNet50 model
    in this case is used as an image feature extractor.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚基于预训练的ResNet50模型实现了`VisionEncoder`模块。我们使用模型的隐藏层输出作为每张图像的固定大小向量表示。由于我们忽略了其最终的分类层，在这种情况下，ResNet50模型被用作图像特征提取器。
- en: We will continue with the text encoder module next.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将继续处理文本编码器模块。
- en: Text encoder
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本编码器
- en: For simplicity, we will employ DistilBERT as the text encoder. We extract the
    complete representation of a sentence by utilizing the final representations of
    the `[CLS]` token. The expectation is that this representation captures the overall
    meaning of the sentence (the image caption in this case). Conceptually, this is
    similar to the process applied to images, where they are transformed into fixed-size
    vectors. For DistilBERT (and BERT as well), each token’s output representation
    is a vector with a size of `768`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们将使用DistilBERT作为文本编码器。我们通过利用`[CLS]`标记的最终表示来提取句子的完整表示。预期是这个表示能够捕获句子的整体意义（在本例中是图片的描述）。从概念上讲，这类似于应用于图像的过程，其中图像被转换为固定大小的向量。对于DistilBERT（以及BERT），每个标记的输出表示是一个大小为`768`的向量。
- en: 'We implement the text encoder using the following code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下代码实现文本编码器：
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Upon initialization, we first load a pre-trained DistilBERT model from the Hugging
    Face Transformers library. Then, we freeze the parameters of the DistilBERT model
    by setting `requires_grad` to `False` for all parameters. Again, you can also
    fine-tune the pre-trained DistilBERT model by making the parameters trainable.
    In the forward pass, we feed the input into the DistilBERT model and extract the
    last hidden state from the model’s outputs. Finally, we return the vector corresponding
    to the [`CLS`] token, as the embedding representation of the input caption.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化时，我们首先从Hugging Face Transformers库加载一个预训练的DistilBERT模型。然后，通过将所有参数的`requires_grad`设置为`False`，我们冻结DistilBERT模型的参数。同样，你也可以通过使参数可训练来微调预训练的DistilBERT模型。在前向传播中，我们将输入传入DistilBERT模型，并从模型的输出中提取最后的隐藏状态。最后，我们返回与`[CLS]`标记对应的向量，作为输入描述的嵌入表示。
- en: We just implemented the `TextEncoder` module to encode textual input using the
    DistilBERT model. It uses the `[CLS]` token representation as the fixed-size vector
    representation of the input text sequence. Similar to what we did in the vision
    encoder for simplicity, we freeze the parameters in the DistilBERT model and use
    it as a text feature extractor without further training.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚实现了`TextEncoder`模块，用于使用DistilBERT模型对文本输入进行编码。它使用`[CLS]`标记表示作为输入文本序列的固定大小向量表示。与我们在视觉编码器中所做的类似，为了简化操作，我们冻结了DistilBERT模型中的参数，并将其用作文本特征提取器，而不进行进一步的训练。
- en: Projection head for contrastive learning
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对比学习的投影头
- en: Having encoded both images and texts into fixed-size vectors (2,048 for images
    and 768 for text), the next step is to project them into a shared space. This
    process enables the comparison of image and text embedding vectors. We can later
    train the CLIP model to distinguish between relevant and non-relevant image-text
    pairs.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在将图像和文本编码为固定大小的向量（图像为2,048，文本为768）后，下一步是将它们投影到共享空间中。这个过程使得图像和文本嵌入向量能够进行比较。我们稍后可以训练CLIP模型，以区分相关和不相关的图像-文本对。
- en: 'We develop the following head projection module to transform the initial 2,048-dimensional
    image vectors or 768-dimensional text vectors into a shared 256-dimensional space:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开发了以下头部投影模块，将初始的2,048维图像向量或768维文本向量转换为共享的256维空间：
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we first create a linear projection layer to transform input vectors from
    the size of `embedding_dim` to `projection_dim`. We then apply the **Gaussian
    Error Linear Unit** (**GELU**) activation function to introduce non-linearity.
    We add another fully connected layer and incorporate a dropout layer for regularization.
    Finally, we apply layer normalization for more efficient training.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先创建一个线性投影层，将输入向量的大小从`embedding_dim`转换为`projection_dim`。然后，我们应用**高斯误差线性单元**（**GELU**）激活函数来引入非线性。接着，我们添加另一个全连接层，并加入一个dropout层进行正则化。最后，我们应用层归一化以提高训练效率。
- en: 'GELU is an activation function that introduces non-linearity into neural networks.
    It is defined as:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: GELU是一种激活函数，它将非线性引入神经网络。其定义为：
- en: '![](img/B21047_14_015.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_14_015.png)'
- en: Compared to ReLU, GELU is much more complex (as you can see) and, hence, has
    smoother gradients. Also, GELU tends to perform better than ReLU in deeper or
    more complex networks. However, ReLU remains popular due to its simplicity and
    effectiveness in many scenarios.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 与ReLU相比，GELU要复杂得多（如你所见），因此具有更平滑的梯度。此外，GELU在更深或更复杂的网络中通常表现得比ReLU更好。然而，ReLU由于其简单性和在许多场景中的有效性，仍然保持着广泛的应用。
- en: '**Best Practice**'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: Layer normalization is used in deep neural networks to normalize the inputs
    of a layer. It aims to improve training stability and model generalization. Unlike
    batch normalization, which normalizes across the entire batch of data, layer normalization
    normalizes across the features for each individual data sample.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化被用于深度神经网络中，用于归一化每一层的输入。其目的是提高训练稳定性和模型的泛化能力。与批量归一化不同，批量归一化是对整个数据批次进行归一化，而层归一化是对每个单独的数据样本在特征维度上进行归一化。
- en: For each data point, layer normalization is applied independently across the
    features. Hence, layer normalization is beneficial for training with small mini-batches
    or online training, while batch normalization is more suitable for large batches
    or large datasets. They are both valuable techniques for stabilizing training
    in DL. You can choose one based on factors like dataset size and batch size.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一个数据点，层归一化是独立地在各特征上应用的。因此，层归一化对于小批量或在线训练非常有利，而批量归一化则更适合大批量或大数据集。它们都是深度学习中稳定训练的有价值技术。你可以根据数据集大小和批量大小等因素来选择使用哪种方法。
- en: In this context, `embedding_dim` represents the size of the input vectors (2,048
    for images and 768 for text), while `projection_dim` denotes the size of the output
    vectors, 256 in our case.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，`embedding_dim`表示输入向量的大小（对于图像为2,048，对于文本为768），而`projection_dim`表示输出向量的大小，在我们这个例子中是256。
- en: In summary, this projection head module is designed to transform both input
    image and text representation vectors into the same lower-dimensional space. As
    well as using linear projection, we add non-linearity and employ regularization
    techniques, such as dropout and layer normalization. The resulting projected vectors
    will become the building blocks for contrastive learning. Let’s figure out how
    they are used to learn semantical relationships between images and text in the
    next section.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这个投影头模块的设计是将输入的图像和文本表示向量转换到同一低维空间。除了使用线性投影外，我们还加入了非线性并采用了正则化技术，如dropout和层归一化。最终生成的投影向量将成为对比学习的基础构件。接下来，我们将探讨它们如何被用来学习图像和文本之间的语义关系。
- en: CLIP model
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CLIP模型
- en: 'This section is where the real excitement unfolds! We utilize the previously
    constructed modules to implement the primary CLIP model, as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 本节是整个过程的关键！我们将利用之前构建的模块来实现主要的CLIP模型，如下所示：
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The initialization is self-explanatory, where we create instances of the `VisionEncoder`
    and `TextEncoder` for images and text, respectively, and their corresponding head
    projection `ProjectionHead` instances. In the forward pass, we encode the input
    images into fixed-size vectors using the vision encoder, and we encode the input
    texts using the text encoder. Recall that the output size for encoded image and
    text vectors is 2,048 and 768, respectively. Subsequently, we employ separate
    projection modules to project the encoded vectors into a shared space, as previously
    mentioned. In this shared space, both encodings assume a similar shape (256 in
    our case). Following this, we compute the contrastive loss. Here are the details:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化过程是显而易见的，我们分别为图像和文本创建`VisionEncoder`和`TextEncoder`的实例，以及它们对应的头部投影`ProjectionHead`实例。在前向传递中，我们使用视觉编码器将输入图像编码为固定大小的向量，使用文本编码器将输入文本编码。回想一下，编码后的图像和文本向量的输出大小分别为2048和768。随后，我们使用单独的投影模块将编码向量投影到共享空间中，如前所述。在这个共享空间中，两个编码具有相似的形状（在我们的例子中是256）。接下来，我们计算对比损失。具体细节如下：
- en: First, we calculate the similarity between text and image embeddings using matrix
    multiplication (`text_embeddings @ image_embeddings.T`). Here, the `@` operator
    in PyTorch performs matrix multiplication, or dot product in this context, and
    `.T` is the transpose operation that we discussed previously. Recall that in linear
    algebra, calculating the dot product is a common method to gauge the similarity
    between two vectors. A higher result suggests greater similarity.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们通过矩阵乘法（`text_embeddings @ image_embeddings.T`）计算文本和图像嵌入的相似度。这里，PyTorch中的`@`操作符执行矩阵乘法，或者在此上下文中执行点积操作，而`.T`是我们之前讨论过的转置操作。回想一下，在线性代数中，计算点积是衡量两个向量相似度的常见方法。更高的结果表示更大的相似度。
- en: Next, we compute the similarities between image embeddings themselves, and the
    similarities between text respectively.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们计算图像嵌入之间的相似度，以及文本之间的相似度。
- en: We then combine the image and text similarities to create target distributions.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将图像和文本的相似度结合起来，生成目标分布。
- en: We calculate the cross-entropy loss between the predicted logits and target
    distributions for both images and texts.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算预测的logits与目标分布之间的交叉熵损失，分别针对图像和文本。
- en: Finally, we compute the final contrastive loss as the average of the image and
    text losses.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将图像和文本损失的平均值作为最终的对比损失。
- en: We just developed a module to train the CLIP model using a contrastive loss.
    The model takes a batch containing both image and text data, encodes them, and
    then projects them into a shared space. It then calculates the similarities and
    computes the contrastive loss. The goal is to bring similar pairs of image and
    text representations closer together and push dissimilar pairs further apart.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚开发了一个模块来使用对比损失训练CLIP模型。该模型接受一个包含图像和文本数据的批次，编码它们，然后将它们投影到共享空间中。接着，计算相似度并计算对比损失。目标是将相似的图像和文本表示拉近，而将不相似的图像和文本表示推远。
- en: Now that all the modules are prepared, it’s time to commence training the CLIP
    model.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，所有模块都已准备好，训练CLIP模型的时间到了。
- en: Finding images with words
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用文字找到图片
- en: In this section, we will first train a CLIP model that we implemented in the
    previous sections. We will then use the trained model to retrieve images given
    a query. Finally, we will use a pre-trained CLIP model to perform image searches
    and zero-shot predictions.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先训练我们在前几节实现的CLIP模型。然后，我们将使用训练好的模型根据查询检索图像。最后，我们将使用预训练的CLIP模型进行图像搜索和零-shot预测。
- en: Training a CLIP model
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练CLIP模型
- en: 'Let’s train a CLIP model in the following steps:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下步骤训练CLIP模型：
- en: 'First, we create a CLIP model and move it to system device (either a GPU or
    CPU):'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建一个CLIP模型并将其移动到系统设备（GPU或CPU）：
- en: '[PRE12]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we initialize an Adam optimizer to train the model and set the learning
    rate:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们初始化一个Adam优化器来训练模型，并设置学习率：
- en: '[PRE13]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As we did in previous chapters, we define the following training function to
    update the model:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如我们在前几章中所做的那样，我们定义以下训练函数来更新模型：
- en: '[PRE14]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We train the model for three epochs:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们训练模型三轮：
- en: '[PRE15]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The training completes after three epochs. Now, let’s proceed to conduct an
    image search using the trained CLIP model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 训练在三轮迭代后完成。现在，让我们使用训练好的CLIP模型进行图像搜索。
- en: Obtaining embeddings for images and text to identify matches
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取图像和文本的嵌入以识别匹配项
- en: To find matching images for a text query (or vice versa), the key process involves
    obtaining the projected embeddings for both the image candidates and the text
    query. The goal is to fetch the image that achieves the highest similarity score
    between its embedding and the text embedding.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到与文本查询（或反之）匹配的图像，关键过程是获得图像候选和文本查询的投影嵌入。目标是获取在其嵌入和文本嵌入之间达到最高相似度得分的图像。
- en: 'For illustrative purposes, we’ll utilize a single batch of image data as the
    pool of image candidates. Let’s explore the steps involved in searching for the
    pertinent image within this sample pool:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 为了举例说明，我们将使用单个批次的图像数据作为图像候选池。让我们来看看在这个样本池中搜索相关图像的步骤：
- en: 'First, we sample a batch of 32 data points from the `data_loader`:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从`data_loader`中采样一个32个数据点的批次：
- en: '[PRE16]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we compute the projected embeddings for the sampled images using the
    previously trained CLIP model:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用之前训练的CLIP模型计算所采样图像的投影嵌入：
- en: '[PRE17]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We now define the image search function, as follows:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在定义图像搜索函数，如下所示：
- en: '[PRE18]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, we first compute the projected text embeddings for a given text query.
    Next, we compute the dot product similarity between the text embedding and the
    precomputed image embeddings for each image candidate. We retrieve the top-n indices
    corresponding to the highest similarity scores. Don’t forget to set the trained
    model to evaluation mode, indicating that no gradients should be computed during
    inference.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先计算给定文本查询的投影文本嵌入。接下来，我们计算文本嵌入与每个图像候选的预计算图像嵌入之间的点积相似度。我们检索与最高相似度得分对应的前n个索引。别忘了将训练好的模型设置为评估模式，这表示在推理过程中不应计算梯度。
- en: 'Let’s observe its performance now! First, we search for “`a running dog`" using
    the image search function we just defined and display the search results:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们观察它的表现！首先，我们使用刚才定义的图像搜索函数搜索“`奔跑的狗`”并展示搜索结果：
- en: '[PRE19]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following screenshot shows the result:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了结果：
- en: '![](img/B21047_14_03.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_14_03.png)'
- en: 'Figure 14.3: Retrieved images for the query “a running dog” (Top photo by Ron
    Mandsager, Flickr: [https://www.flickr.com/photos/remandsager/3540416981/](https://www.flickr.com/photos/remandsager/3540416981/);
    bottom photo by Rob Burns-Sweeney, Flickr: [https://www.flickr.com/photos/mulberryphotographic/3368207495/](https://www.flickr.com/photos/mulberryphotographic/3368207495/))'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '图14.3：查询“奔跑的狗”返回的图像（顶部照片由Ron Mandsager提供，Flickr: [https://www.flickr.com/photos/remandsager/3540416981/](https://www.flickr.com/photos/remandsager/3540416981/);
    底部照片由Rob Burns-Sweeney提供，Flickr: [https://www.flickr.com/photos/mulberryphotographic/3368207495/](https://www.flickr.com/photos/mulberryphotographic/3368207495/))'
- en: The two retrieved images are highly pertinent to the query.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这两张检索到的图像与查询高度相关。
- en: 'Let’s try another query, “`kids jumping into a pool`", before we end this section:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在结束本节之前，让我们尝试另一个查询，“`孩子跳进游泳池`”：
- en: '[PRE20]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following screenshot shows the result:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了结果：
- en: '![](img/B21047_14_04.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_14_04.png)'
- en: 'Figure 14.4: Retrieved image for the query “kids jumping into a pool” (photo
    by Alecia, Flickr: [https://www.flickr.com/photos/jnjsmom2007/2602415701/](https://www.flickr.com/photos/jnjsmom2007/2602415701/))'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '图14.4：查询“孩子跳进游泳池”返回的图像（照片由Alecia提供，Flickr: [https://www.flickr.com/photos/jnjsmom2007/2602415701/](https://www.flickr.com/photos/jnjsmom2007/2602415701/))'
- en: The retrieved image is exactly what we are looking for.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 检索到的图像正是我们要找的。
- en: The CLIP model we implemented employs the pre-trained ResNet50 model as the
    vision encoder and the pre-trained DistilBERT model as the text encoder. Remember
    that we kept the parameters frozen for both ResNet50 and DistilBERT, utilizing
    them as image and text feature extractors. If desired, you can fine-tune these
    models by allowing their parameters to be trainable. This will be the exercise
    for this chapter. We trained our CLIP model using the `Flickr8k` dataset and performed
    image searches as a performance evaluation. Starting from the next section, we
    will use the pre-trained CLIP model, which learns from a much larger and more
    diverse dataset to perform image search, image-to-image search, and zero-shot
    prediction.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现的CLIP模型采用了预训练的ResNet50模型作为视觉编码器，预训练的DistilBERT模型作为文本编码器。请记住，我们将ResNet50和DistilBERT的参数冻结，利用它们作为图像和文本特征提取器。如果需要，您可以通过允许它们的参数可训练来微调这些模型。这将是本章的练习。我们使用`Flickr8k`数据集训练了我们的CLIP模型，并进行了图像搜索作为性能评估。从下一节开始，我们将使用预训练的CLIP模型，它从一个更大、更具多样性的数据集学习，用于执行图像搜索、图像到图像的搜索和零样本预测。
- en: Image search using the pre-trained CLIP model
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练的CLIP模型进行图像搜索
- en: A popular library SentenceTransformers ([https://www.sbert.net/index.html](https://www.sbert.net/index.html))
    offers a wrapper for the OpenAI CLIP model. The `SentenceTransformer` package
    is developed for sentence and text embeddings. It provides pre-trained models
    to encode sentences into high-dimensional vectors in a semantic space.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流行的库SentenceTransformers（[https://www.sbert.net/index.html](https://www.sbert.net/index.html)）提供了一个OpenAI
    CLIP模型的封装。`SentenceTransformer`包是为句子和文本嵌入而开发的。它提供了预训练模型，将句子编码为语义空间中的高维向量。
- en: 'Let’s perform the following tasks to search images, using a pre-trained CLIP
    model from `SentenceTransformer`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下任务进行图像搜索，使用从`SentenceTransformer`获得的预训练CLIP模型：
- en: 'First things first, install the `SentenceTransformers` library using the following
    command:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令安装`SentenceTransformers`库：
- en: '[PRE21]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: or
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '[PRE22]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Import the `SentenceTransformers` library and load the pre-trained CLIP model:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`SentenceTransformers`库并加载预训练的CLIP模型：
- en: '[PRE23]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Here, we use the **Vision Transformer** (**ViT**)-based CLIP model. The “`B-32`"
    designation refers to the size of the ViT model, which means it has 32 times more
    parameters than the base ViT model.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用基于**Vision Transformer**（**ViT**）的CLIP模型。 "B-32"的标记表示ViT模型的大小，这意味着它有比基础ViT模型多32倍的参数。
- en: 'Next, we need to compute the image embeddings for all the `Flickr8k` image
    candidates using the CLIP model we just loaded:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要使用刚加载的CLIP模型，为所有`Flickr8k`图像候选计算图像嵌入：
- en: '[PRE24]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `model.encode()` method can take in text or images and generate corresponding
    embeddings. Here, we store all the resulting image embeddings in `all_image_embeddings`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.encode()`方法可以接受文本或图像，并生成相应的嵌入。在这里，我们将所有生成的图像嵌入存储在`all_image_embeddings`中。'
- en: 'Similar to what we did in the previous section, we define the image search
    function as follows:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似于我们在前一部分做的那样，我们定义图像搜索功能如下：
- en: '[PRE25]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here, we use the `model.encode()` method again to obtain the embeddings for
    the given query. We employ the `util.semantic_search` utility function to fetch
    the top k images for the given text query, based on the similarities of their
    embeddings.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们再次使用`model.encode()`方法来获取给定查询的嵌入。我们使用`util.semantic_search`实用程序函数根据嵌入的相似性，获取给定文本查询的前k个图像。
- en: 'Now, let’s search for “`a swimming dog`", using the image search function we
    just defined, and display the search results:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用刚才定义的图像搜索功能搜索“`a swimming dog`”并显示搜索结果：
- en: '[PRE26]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following screenshot shows the result:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了结果：
- en: '![](img/B21047_14_05.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_14_05.png)'
- en: 'Figure 14.5: Retrieved image for the query “a swimming dog” (photo by Julia,
    Flickr: [https://www.flickr.com/photos/drakegsd/408233586/](https://www.flickr.com/photos/drakegsd/408233586/))'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5：为查询“游泳的狗”检索到的图像（照片来自Julia，Flickr：[https://www.flickr.com/photos/drakegsd/408233586/](https://www.flickr.com/photos/drakegsd/408233586/))
- en: This is very accurate.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法非常准确。
- en: 'We can go beyond a text-to-image search and perform an **image-to-image** search:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以超越文本到图像的搜索，执行**图像到图像**的搜索：
- en: '[PRE27]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We will take a random image, `240696675_7d05193aa0.jpg`, as the query image,
    feed it to the image search function, and display the retrieved images that follow
    the query image:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将选择一张随机图像，`240696675_7d05193aa0.jpg`，作为查询图像，将其输入图像搜索功能，并显示返回的与查询图像相似的图像：
- en: '[PRE28]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that we skip the first retrieved image because it is the query image.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们跳过了第一张检索到的图像，因为它就是查询图像。
- en: 'The following screenshot shows the result:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了结果：
- en: '![](img/B21047_14_06.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_14_06.png)'
- en: 'Figure 14.6: Query image and similar images (top photo by Rose, Flickr: [https://www.flickr.com/photos/rosespics/240696675/](https://www.flickr.com/photos/rosespics/240696675/);
    middle photo by Mark Dowling, Flickr: [https://www.flickr.com/photos/markdowlrods/421932359/](https://www.flickr.com/photos/markdowlrods/421932359/);
    bottom photo by Rob, Flickr: [https://www.flickr.com/photos/mind_the_goat/3419634480/](https://www.flickr.com/photos/mind_the_goat/3419634480/))'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6：查询图像和相似图像（顶部照片来自Rose，Flickr：[https://www.flickr.com/photos/rosespics/240696675/](https://www.flickr.com/photos/rosespics/240696675/);
    中间照片来自Mark Dowling，Flickr：[https://www.flickr.com/photos/markdowlrods/421932359/](https://www.flickr.com/photos/markdowlrods/421932359/);
    底部照片来自Rob，Flickr：[https://www.flickr.com/photos/mind_the_goat/3419634480/](https://www.flickr.com/photos/mind_the_goat/3419634480/))
- en: We can see that the retrieved images are highly similar to the query image.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，检索到的图像与查询图像高度相似。
- en: The pre-trained CLIP model excels in both text-to-image and image-to-image search
    tasks. Notably, the model may not have been specifically trained on the `Flickr8k`
    dataset, but it performs well in zero-shot learning, as you saw in this section.
    Finally, let’s take a look at another example of zero-shot prediction – classifying
    the CIFAR-100 dataset.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的 CLIP 模型在文本到图像和图像到图像的搜索任务中表现出色。值得注意的是，模型可能没有专门在 `Flickr8k` 数据集上进行训练，但在零样本学习中表现良好，正如您在本节中所看到的。最后，让我们看一个零样本预测的另一个例子——对
    CIFAR-100 数据集进行分类。
- en: Zero-shot classification
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零样本分类
- en: In the final part of this chapter, we will utilize the CLIP model to classify
    the `CIFAR-100` dataset. Both `CIFAR-10` and `CIFAR-100` ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html))
    are labeled subsets derived from the 80 Million Tiny Images dataset, a collection
    curated by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The `CIFAR-100` dataset
    comprises 60,000 32x32 color images. These images are categorized into 100 classes,
    with each class containing exactly 600 images. We can load the dataset directly
    from PyTorch, which includes 50,000 images for training and 10,000 images for
    testing.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最后部分，我们将利用 CLIP 模型对 `CIFAR-100` 数据集进行分类。`CIFAR-10` 和 `CIFAR-100` ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html))
    都是从 80 Million Tiny Images 数据集中提取的标注子集，该数据集由 Alex Krizhevsky、Vinod Nair 和 Geoffrey
    Hinton 精心策划。`CIFAR-100` 数据集包含 60,000 张 32x32 彩色图像，这些图像被分类为 100 个类别，每个类别恰好包含 600
    张图像。我们可以直接从 PyTorch 加载数据集，其中包括 50,000 张用于训练的图像和 10,000 张用于测试的图像。
- en: 'Let’s perform the following tasks to classify the `CIFAR-100` dataset:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下任务以分类 `CIFAR-100` 数据集：
- en: 'Firstly, we load the `CIFAR-100` dataset from PyTorch:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从 PyTorch 加载 `CIFAR-100` 数据集：
- en: '[PRE29]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Here, we only load the testing subset with 10,000 samples.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只加载包含 10,000 个样本的测试子集。
- en: 'Examine the classes of the dataset:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查数据集的类别：
- en: '[PRE30]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: There are 100 classes.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 共有 100 个类别。
- en: 'We start with one sample:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从一个样本开始：
- en: '[PRE31]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We will see if we can classify it correctly as a “`mountain`.”
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试是否能正确地将其分类为“`mountain`”（山脉）。
- en: 'We then generate the image embeddings for the selected data sample using the
    pre-trained CLIP model:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用预训练的 CLIP 模型生成所选数据样本的图像嵌入：
- en: '[PRE32]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, here’s the clever approach. We consider each of the 100 classes as a textual
    description, and our goal is to find the most appropriate description for a given
    image, in order to classify it. Therefore, we must generate text embeddings for
    each of the 100 classes:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，这里有一个巧妙的方法。我们将每个类别视为文本描述，目标是找到最合适的描述来对给定图像进行分类。因此，我们需要为 100 个类别生成文本嵌入：
- en: '[PRE33]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let’s search for the best class text description for the given image, as follows:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们为给定图像搜索最佳的类别文本描述，如下所示：
- en: '[PRE34]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We can correctly predict the right class for the sample image. What about the
    whole dataset? Let’s evaluate its performance in the next steps.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以正确预测样本图像的类别。那么整个数据集呢？我们将在接下来的步骤中评估其性能。
- en: 'Similarly, we compute the image embeddings for all images in the dataset:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，我们计算数据集中所有图像的图像嵌入：
- en: '[PRE35]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We also record the true class information.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还会记录真实的类别信息。
- en: 'Now, we search for the best class text description for each of the images:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们为每张图像搜索最佳的类别文本描述：
- en: '[PRE36]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Finally, we evaluate the classification accuracy:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们评估分类准确率：
- en: '[PRE37]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We attain a classification accuracy of 55% on the 100-class CIFAR dataset.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 100 类 CIFAR 数据集上获得了 55% 的分类准确率。
- en: This project demonstrates how CLIP can be used for zero-shot classification.
    The model predicts the textual labels for images without having seen specific
    image-label pairs during training. Feel free to adjust the textual descriptions
    and images based on your specific use case. For example, you may group several
    similar fine classes into one coarse class, such as “`boy`,” “`girl`,” “`man`,”
    and “`woman`" into “`people`.”
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目演示了 CLIP 如何用于零样本分类。该模型在训练时没有见过特定的图像-标签对，但仍能预测图像的文本标签。根据您的具体使用案例，可以自由调整文本描述和图像。例如，您可以将几个相似的细分类别合并为一个粗类，如将
    “`boy`” （男孩）、“`girl`”（女孩）、“`man`”（男人）和“`woman`”（女人）合并为“`people`”（人类）。
- en: Zero-shot classification using CLIP is powerful, but its performance is limited
    by training data used in a pre-trained model. Intuitively, this can be improved
    by leveraging pre-trained models on larger datasets. Another approach is knowledge
    distillation, which transfers knowledge from a complex and high-performance model
    to a smaller and faster model. You can read more about knowledge distillation
    in *Distilling the Knowledge in a Neural Network* (2015) by Geoffrey Hinton, Oriol
    Vinyals, and Jeff Dean ([https://arxiv.org/abs/2006.05525](https://arxiv.org/abs/2006.05525)).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CLIP进行零样本分类非常强大，但其性能受限于预训练模型所使用的训练数据。直观地说，可以通过利用在更大数据集上预训练的模型来改善这一点。另一种方法是知识蒸馏，它将复杂且高性能的模型的知识转移到一个更小、更快的模型中。你可以在*Distilling
    the Knowledge in a Neural Network*（2015年），作者Geoffrey Hinton、Oriol Vinyals和Jeff
    Dean中阅读更多关于知识蒸馏的内容（[https://arxiv.org/abs/2006.05525](https://arxiv.org/abs/2006.05525)）。
- en: Summary
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter introduced CLIP, a powerful DL model designed for cross-modal tasks,
    such as finding relevant images based on textual queries or vice versa. We learned
    that the model’s dual encoder architecture and contrastive learning mechanism
    enable it to understand both images and text in a shared space.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了CLIP，这是一种强大的深度学习模型，设计用于跨模态任务，例如根据文本查询查找相关图像或反之亦然。我们了解到，模型的双编码器架构和对比学习机制使其能够在共享空间中理解图像和文本。
- en: We implemented our customized versions of CLIP models, using the DistilBERT
    and ResNet50 models. Following an exploration of the `Flickr8k` dataset, we built
    a CLIP model and explored its capabilities in text-to-image and image-to-image
    searches. CLIP excels at zero-shot transfer learning. We showcased this by using
    a pre-trained CLIP model for image search and `CIFAR-100` classification.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了自定义版本的CLIP模型，使用了DistilBERT和ResNet50模型。在探索`Flickr8k` 数据集后，我们构建了一个CLIP模型，并探索了其在文本到图像和图像到图像搜索中的能力。CLIP在零样本迁移学习方面表现出色。我们通过使用预训练的CLIP模型进行图像搜索和`CIFAR-100`
    分类展示了这一点。
- en: 'In the next chapter, we will focus on the third type of machine learning problem:
    reinforcement learning. You will learn how the reinforcement learning model learns
    by interacting with the environment to reach its learning goal.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将重点介绍第三类机器学习问题：强化学习。你将学习强化学习模型如何通过与环境的互动来实现其学习目标。
- en: Exercises
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Fine-tune the pre-trained ResNet50 and DistilBERT models employed in our self-implemented
    CLIP model.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调我们自实现的CLIP模型中使用的预训练ResNet50和DistilBERT模型。
- en: Can you perform zero-shot classification on the 10-class `CIFAR-10` dataset?
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能在10类`CIFAR-10`数据集上进行零样本分类吗？
- en: Fine-tune the CLIP model using the training set of the `CIFAR-100` dataset,
    and see if you can get better performance on the test set.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`CIFAR-100` 数据集的训练集微调CLIP模型，并查看是否能在测试集上获得更好的性能。
- en: References
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Learning Transferable Visual Models From Natural Language Supervision*, by
    Alec Radford et al.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*从自然语言监督中学习可迁移的视觉模型*，Alec Radford等著。'
- en: '`Flickr8k` dataset: *Framing Image Description as a Ranking Task: Data, Models
    and Evaluation Metrics*, *Journal of Artificial Intelligence Research*, Volume
    47, pages 853–899'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Flickr8k` 数据集：*将图像描述构建为排序任务：数据、模型和评估指标*，*人工智能研究杂志*，第47卷，第853–899页'
- en: '`CIFAR-100` dataset: *Learning Multiple Layers of Features from Tiny Images*,
    Alex Krizhevsky, 2009.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CIFAR-100` 数据集：*从微小图像中学习多层次特征*，Alex Krizhevsky，2009年。'
- en: Join our book’s Discord space
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord讨论区
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord讨论区，与作者和其他读者交流：
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/yuxi](https://packt.link/yuxi)'
- en: '![](img/QR_Code1878468721786989681.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1878468721786989681.png)'
