- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Getting Started with Machine Learning and Python
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用机器学习和Python
- en: 'The concept of **artificial intelligence** (**AI**) outpacing human knowledge
    is often referred to as the “technological singularity.” Some predictions in the
    AI research community and other fields suggest that the singularity could happen
    within the next 30 years. Regardless of its time horizon, one thing is clear:
    the rise of AI highlights the growing importance of analytical and machine learning
    skills. Mastering these disciplines equips us to not only understand and interact
    with increasingly complex AI systems but also actively participate in shaping
    their development and application, ensuring they benefit humanity.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工智能**（**AI**）超越人类知识的概念通常被称为“技术奇点”。一些来自AI研究界及其他领域的预测表明，这一奇点可能会在未来30年内到来。无论它的时间表如何，有一点是明确的：AI的崛起突显了分析能力和机器学习技能日益重要。掌握这些学科不仅能帮助我们理解和与越来越复杂的AI系统互动，还能让我们积极参与塑造其发展与应用，确保它们造福人类。'
- en: In this chapter, we will kick off our machine learning journey with the basic,
    yet important, concepts of machine learning. We will start with what machine learning
    is all about, why we need it, and its evolution over a few decades. We will then
    discuss typical machine learning tasks and explore several essential techniques
    to work with data and models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从机器学习的一些基本而重要的概念开始我们的机器学习之旅。我们将从机器学习的定义开始，探讨我们为何需要它，以及它在几十年来的发展历程。接下来，我们将讨论典型的机器学习任务，并探索几种处理数据和模型的基本技巧。
- en: At the end of the chapter, we will set up the software for Python, the most
    popular language for machine learning and data science, and the libraries and
    tools that are required for this book.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后，我们将设置Python软件，这是机器学习和数据科学领域最流行的编程语言，并安装本书所需的库和工具。
- en: 'We will go into detail on the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细讨论以下主题：
- en: An introduction to machine learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习简介
- en: Knowing the prerequisites
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解前提条件
- en: Getting started with three types of machine learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始了解三种类型的机器学习
- en: Digging into the core of machine learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入探讨机器学习的核心
- en: Data preprocessing and feature engineering
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理和特征工程
- en: Combining models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型结合
- en: Installing software and setting up
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装软件并进行设置
- en: An introduction to machine learning
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习简介
- en: In this first section, we will kick off our machine learning journey with a
    brief introduction to machine learning, why we need it, how it differs from automation,
    and how it improves our lives.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将通过简要介绍机器学习、为何需要机器学习、机器学习与自动化的区别以及机器学习如何改善我们的生活，来开启我们的机器学习之旅。
- en: '**Machine learning** is a term that was coined around 1960, consisting of two
    words—**machine**, which corresponds to a computer, robot, or other device, and
    **learning**, which refers to an activity intended to acquire or discover event
    patterns, which we humans are good at. Interesting examples include facial recognition,
    language translation, responding to emails, making data-driven business decisions,
    and creating various types of content. You will see many more examples throughout
    this book.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**是一个大约在1960年左右被创造的术语，由两个词组成——**机器**，对应计算机、机器人或其他设备，和**学习**，指的是旨在获取或发现事件模式的活动，而这正是人类擅长的事情。有趣的例子包括人脸识别、语言翻译、回复电子邮件、做出数据驱动的商业决策，以及创建各种类型的内容。你将在本书中看到更多这样的例子。'
- en: Understanding why we need machine learning
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解我们为何需要机器学习
- en: 'Why do we need machine learning, and why do we want a machine to learn the
    same way as a human? We can look at it from three main perspectives: maintenance,
    risk mitigation, and enhanced performance.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要机器学习，为什么我们希望机器像人类一样学习呢？我们可以从三个主要角度来理解：维护、风险减轻和性能提升。
- en: First and foremost, of course, computers and robots can work 24/7 and don’t
    get tired. Machines cost a lot less in the long run. Also, for sophisticated problems
    that involve a variety of huge datasets or complex calculations, it’s much more
    justifiable, not to mention intelligent, to let computers do all the work. Machines
    driven by algorithms that are designed by humans can learn latent rules and inherent
    patterns, enabling them to carry out tasks effectively.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，当然，计算机和机器人可以全天候工作，不会感到疲倦。从长远来看，机器的成本远低于人工。而且，对于涉及大量庞大数据集或复杂计算的复杂问题，让计算机完成所有工作不仅更合理，而且更具智能。由人类设计的算法驱动的机器能够学习潜在的规则和内在的模式，从而有效地执行任务。
- en: 'Learning machines are better suited than humans for tasks that are routine,
    repetitive, or tedious. Beyond that, automation by machine learning can mitigate
    risks caused by fatigue or inattention. Self-driving cars, as shown in *Figure
    1.1*, are a great example: a vehicle is capable of navigating by sensing its environment
    and making decisions without human input. Another example is the use of robotic
    arms in production lines, which are capable of causing a significant reduction
    in injuries and costs.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 学习机器比人类更适合处理那些日常的、重复的或繁琐的任务。除此之外，机器学习的自动化可以减轻由于疲劳或注意力不集中而导致的风险。自动驾驶汽车，如*图1.1*所示，是一个很好的例子：一辆车能够通过感知环境并做出决策，而无需人类输入。另一个例子是在生产线上使用机器人臂，这能显著减少伤害和成本。
- en: '![](img/B21047_01_01.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_01_01.png)'
- en: 'Figure 1.1: An example of a self-driving car'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：自动驾驶汽车的示例
- en: Let’s assume that humans don’t fatigue or we have the resources to hire enough
    shift workers; would machine learning still have a place? Of course it would!
    There are many cases, reported and unreported, where machines perform comparably,
    or even better, than domain experts. As algorithms are designed to learn from
    the ground truth and the best thought-out decisions made by human experts, machines
    can perform just as well as experts.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设人类不会感到疲劳，或者我们有足够的资源雇佣足够的轮班工人；那么机器学习是否还会有一席之地？当然会！有许多已报告和未报告的案例表明，机器在某些方面表现得与领域专家相当，甚至更好。由于算法是通过从事实真相和人类专家所做的最经过深思熟虑的决策中学习，机器可以与专家一样表现出色。
- en: In reality, even the best expert makes mistakes. Machines can minimize the chance
    of making wrong decisions by utilizing collective intelligence from individual
    experts. A major study that identified that machines are better than doctors at
    diagnosing certain types of cancer is proof of this philosophy ([https://www.nature.com/articles/d41586-020-00847-2](https://www.nature.com/articles/d41586-020-00847-2)).
    **AlphaGo** ([https://deepmind.com/research/case-studies/alphago-the-story-so-far](https://deepmind.com/research/case-studies/alphago-the-story-so-far))
    is probably the best-known example of machines beating humans—an AI program created
    by DeepMind defeated Lee Sedol, a world champion Go player, in a five-game Go
    match.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，即使是最优秀的专家也会犯错误。机器通过利用来自个体专家的集体智慧，可以最大程度地减少做出错误决策的机会。一项主要研究表明，机器在诊断某些类型的癌症时优于医生，这正是这种理念的证明（[https://www.nature.com/articles/d41586-020-00847-2](https://www.nature.com/articles/d41586-020-00847-2)）。**AlphaGo**（[https://deepmind.com/research/case-studies/alphago-the-story-so-far](https://deepmind.com/research/case-studies/alphago-the-story-so-far)）可能是机器战胜人类最著名的例子——DeepMind创建的AI程序在一场五局围棋比赛中击败了世界围棋冠军李世石。
- en: Also, it’s much more scalable to deploy learning machines than to train individuals
    to become experts, from the perspective of economic and social barriers. Current
    diagnostic devices can achieve a level of performance similar to that of qualified
    doctors. We can distribute thousands of diagnostic devices across the globe within
    a week, but it’s almost impossible to recruit and assign the same number of qualified
    doctors within the same period.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，从经济和社会障碍的角度来看，部署学习机器比训练个人成为专家更加具备可扩展性。当前的诊断设备能够达到与合格医生相似的性能水平。我们可以在一周内将数千台诊断设备分布到全球各地，但几乎不可能在同一时间内招募并分配相同数量的合格医生。
- en: 'You may argue against this: what if we have sufficient resources and the capacity
    to hire the best domain experts and later aggregate their opinions—would machine
    learning still have a place? Probably not (at least right now)—learning machines
    might not perform better than the joint efforts of the most intelligent humans.
    However, individuals equipped with learning machines can outperform the best group
    of experts. This is an emerging concept called **AI-based assistance** or **AI
    plus human intelligence**, which advocates for combining the efforts of machines
    and humans. It provides support, guidance, or solutions to users. And more importantly,
    it can adapt and learn from user interactions to improve performance over time.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会反驳：如果我们有足够的资源和能力雇佣最优秀的领域专家，并随后汇聚他们的意见——机器学习是否还会有一席之地？可能不会（至少目前如此）——学习机器可能无法超越最聪明的人的联合努力。然而，配备学习机器的个人却能够超越最优秀的专家团队。这是一种新兴的概念，称为**基于AI的辅助**或**AI与人类智能结合**，它倡导机器与人类的共同努力。它为用户提供支持、指导或解决方案。更重要的是，它能够适应并从用户互动中学习，随着时间的推移不断提升性能。
- en: 'We can summarize the previous statement in the following inequality:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将之前的陈述总结为以下不等式：
- en: '*human + machine learning → most intelligent tireless human ≥ machine learning
    > human*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*人类 + 机器学习 → 最智能且不知疲倦的人类 ≥ 机器学习 > 人类*'
- en: '**Artificial intelligence-generated content** (**AIGC**) is one of the recent
    breakthroughs. It uses AI technologies to create or assist in creating various
    types of content, such as articles, product descriptions, music, images, and videos.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工智能生成内容** (**AIGC**)是近年来的一个突破。它利用AI技术创造或协助创造各种类型的内容，如文章、产品描述、音乐、图像和视频。'
- en: 'A medical operation involving robots is one great example of human and machine
    learning synergy. *Figure 1.2* shows robotic arms in an operation room alongside
    a surgeon:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人参与的医学手术是人类与机器学习协同作用的一个极好例子。*图1.2*展示了手术室中机器人臂与外科医生的合作：
- en: '![A picture containing person, clothing, medical equipment, technician  Description
    automatically generated](img/B21047_01_02.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing person, clothing, medical equipment, technician  Description
    automatically generated](img/B21047_01_02.png)'
- en: 'Figure 1.2: AI-assisted surgery'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：AI辅助手术
- en: Differentiating between machine learning and automation
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 区分机器学习和自动化
- en: So does machine learning simply equate to automation that involves the programming
    and execution of human-crafted or human-curated rule sets? A popular myth says
    that machine learning is the same as automation because it performs instructive
    and repetitive tasks and thinks no further. If the answer to that question is
    *yes*, why can’t we just hire many software programmers and continue programming
    new rules or extending old rules?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，机器学习是否仅仅等同于涉及编程和执行人工编写或人工策划规则集的自动化呢？一种流行的误解认为，机器学习与自动化相同，因为它执行指令性和重复性的任务，并且不再深入思考。如果答案是*是的*，那么我们为什么不能仅仅雇佣许多软件程序员，继续编写新规则或扩展旧规则呢？
- en: One reason is that defining, maintaining, and updating rules becomes increasingly
    expensive over time. The number of possible patterns for an activity or event
    could be enormous, and therefore, exhausting all enumeration isn’t practically
    feasible. It gets even more challenging when it comes to events that are dynamic,
    ever-changing, or evolve in real time. It’s much easier and more efficient to
    develop learning algorithms that command computers to learn, extract patterns,
    and figure things out themselves from abundant data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个原因是，定义、维护和更新规则随着时间推移变得越来越昂贵。活动或事件的可能模式数量可能是巨大的，因此，穷举所有模式在实际操作中是不可行的。尤其当事件是动态的、不断变化的或实时演变时，问题变得更加复杂。开发学习算法，让计算机从大量数据中学习、提取模式并自行解决问题，变得更加容易和高效。
- en: 'The difference between machine learning and traditional programming can be
    seen in *Figure 1.3*:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习与传统编程的区别可以从*图1.3*中看出：
- en: '![A diagram of a computer model  Description automatically generated with low
    confidence](img/B21047_01_03.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![A diagram of a computer model  Description automatically generated with low
    confidence](img/B21047_01_03.png)'
- en: 'Figure 1.3: Machine learning versus traditional programming'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：机器学习与传统编程的对比
- en: In traditional programming, the computer follows a set of predefined rules to
    process the input data and produce the outcome. In machine learning, the computer
    tries to mimic human thinking. It interacts with the input data, expected output,
    and the environment, and it derives patterns that are represented by one or more
    mathematical models. The models are then used to interact with future input data
    and generate outcomes. Unlike in automation, the computer in a machine learning
    setting doesn’t receive explicit and instructive coding.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统编程中，计算机遵循一组预定义的规则来处理输入数据并生成结果。在机器学习中，计算机试图模仿人类思维。它与输入数据、预期输出和环境进行互动，并推导出由一个或多个数学模型表示的模式。然后，这些模型用于与未来的输入数据互动并生成结果。与自动化不同，在机器学习的设置中，计算机并不会收到明确的指令性编码。
- en: The volume of data is growing exponentially. Nowadays, the floods of textual,
    audio, image, and video data are hard to fathom. The **Internet of Things** (**IoT**)
    is a recent development of a new kind of internet, which interconnects everyday
    devices. The IoT will bring data from household appliances and autonomous cars
    to the fore. This trend is likely to continue, and we will have more data that
    is generated and processed. Besides the quantity, the quality of data available
    has kept increasing in the past few years, partly due to cheaper storage. This
    has empowered the evolution of machine learning algorithms and data-driven solutions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据量正在呈指数增长。如今，文本、音频、图像和视频数据的洪流难以估量。**物联网**（**IoT**）是新型互联网的最新发展，它将日常设备互联起来。物联网将把家电和自动驾驶汽车的数据带到前台。这一趋势可能会继续，我们将拥有更多生成和处理的数据。除了数据量的增加，近年来可用数据的质量也在不断提高，部分原因是存储成本降低。这推动了机器学习算法和数据驱动解决方案的发展。
- en: Machine learning applications
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习应用
- en: Jack Ma, co-founder of the e-commerce company Alibaba, explained in a speech
    in 2018 that IT was the focus of the past 20 years, but for the next 30 years,
    we will be in the age of **data technology** (**DT**) ([https://www.alizila.com/jack-ma-dont-fear-smarter-computers/](https://www.alizila.com/jack-ma-dont-fear-smarter-computers/)).
    During the age of IT, companies grew larger and stronger thanks to computer software
    and infrastructure. Now that businesses in most industries have already gathered
    enormous amounts of data, it’s presently the right time to exploit DT to unlock
    insights, derive patterns, and boost new business growth. Broadly speaking, machine
    learning technologies enable businesses to better understand customer behavior,
    engage with customers, and optimize operations management.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 阿里巴巴的联合创始人马云在2018年的一次演讲中解释道，过去20年是IT的时代，但未来30年将是**数据技术**（**DT**）的时代（[https://www.alizila.com/jack-ma-dont-fear-smarter-computers/](https://www.alizila.com/jack-ma-dont-fear-smarter-computers/)）。在IT时代，企业因计算机软件和基础设施而不断壮大。如今，大多数行业的企业已经收集了大量数据，现在正是利用DT来解锁洞察、推导模式并推动新业务增长的最佳时机。广义上讲，机器学习技术使企业能够更好地理解客户行为、与客户互动并优化运营管理。
- en: As for us individuals, machine learning technologies are already making our
    lives better every day. One application of machine learning with which we’re all
    familiar is spam email filtering. Another is online advertising, where adverts
    are served automatically based on information advertisers have collected about
    us. Stay tuned for the next few chapters, where you will learn how to develop
    algorithms to solve these two problems and more.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们个人来说，机器学习技术已经在每天不断改善我们的生活。我们都熟悉的一个机器学习应用就是垃圾邮件过滤。另一个是在线广告，根据广告商收集到的关于我们的信息，自动投放广告。请继续关注接下来的章节，您将学习如何开发算法来解决这两个问题以及更多问题。
- en: A search engine is an application of machine learning we can’t imagine living
    without. It involves information retrieval, which parses what we look for, queries
    the related top records, and applies contextual ranking and personalized ranking,
    which sorts pages by topical relevance and user preference. E-commerce and media
    companies have been at the forefront of employing recommendation systems, which
    help customers find products, services, and articles faster.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎是我们无法想象没有的机器学习应用。它涉及信息检索，解析我们所寻找的内容，查询相关的最佳记录，并应用上下文排序和个性化排序，根据主题相关性和用户偏好对页面进行排序。电子商务和媒体公司在推荐系统的应用上处于前沿，这些系统帮助客户更快找到产品、服务和文章。
- en: 'The application of machine learning is boundless, and we just keep hearing
    new examples everyday: credit card fraud detection, presidential election prediction,
    instant speech translation, robo advisors, AI-generated art, chatbots for customer
    support, and medical or legal advice provided by generative AI technologies—you
    name it!'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的应用是无限的，我们每天都能听到新的例子：信用卡欺诈检测、总统选举预测、即时语音翻译、机器人顾问、AI生成艺术、客服聊天机器人以及由生成性AI技术提供的医学或法律咨询——应有尽有！
- en: In the 1983 *War Games* movie, a computer made life-and-death decisions that
    could have resulted in World War III. As far as we know, technology wasn’t able
    to pull off such feats at the time. However, in 1997, the Deep Blue supercomputer
    did manage to beat a world chess champion ([https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)](https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer))).
    In 2005, a Stanford self-driving car drove by itself for more than 130 miles in
    a desert ([https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_(2005)](https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_(2005))).
    In 2007, the car of another team drove through regular urban traffic for more
    than 60 miles ([https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_(2007)](https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_(2007))).
    In 2011, the Watson computer won a quiz against human opponents ([https://en.wikipedia.org/wiki/Watson_(computer)](https://en.wikipedia.org/wiki/Watson_(computer))).
    As mentioned earlier, the AlphaGo program beat one of the best Go players in the
    world in 2016\. As of 2023, ChatGPT has been widely used across various industries,
    such as customer support, content generation, market research, and training and
    education ([https://www.forbes.com/sites/bernardmarr/2023/05/30/10-amazing-real-world-examples-of-how-companies-are-using-chatgpt-in-2023](https://www.forbes.com/sites/bernardmarr/2023/05/30/10-amazing-real-world-examples-of-how-companies-are-using-chatgpt-in-2023)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在1983年的*战争游戏*电影中，一台计算机做出了生死攸关的决策，这些决策本可能导致第三次世界大战。就我们所知，当时的技术并未能完成如此壮举。然而，在1997年，深蓝超计算机成功击败了一位世界象棋冠军([https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)](https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)))。在2005年，一辆斯坦福大学的自动驾驶汽车在沙漠中自主行驶了超过130英里([https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_(2005)](https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_(2005)))。2007年，另一支队伍的汽车在城市道路上行驶了超过60英里([https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_(2007)](https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_(2007)))。2011年，沃森计算机在一场问答比赛中击败了人类对手([https://en.wikipedia.org/wiki/Watson_(computer)](https://en.wikipedia.org/wiki/Watson_(computer)))。正如前面提到的，AlphaGo程序在2016年击败了世界顶级围棋选手。截至2023年，ChatGPT已经在多个行业广泛应用，如客户支持、内容生成、市场研究以及培训与教育([https://www.forbes.com/sites/bernardmarr/2023/05/30/10-amazing-real-world-examples-of-how-companies-are-using-chatgpt-in-2023](https://www.forbes.com/sites/bernardmarr/2023/05/30/10-amazing-real-world-examples-of-how-companies-are-using-chatgpt-in-2023))。
- en: If we assume that computer hardware is the limiting factor, then we can try
    to extrapolate into the future. A famous American inventor and futurist, Ray Kurzweil,
    did just that, predicting in 2017 that we can expect AI to gain human-level intelligence
    around 2029 ([https://aibusiness.com/responsible-ai/ray-kurzweil-predicts-that-the-singularity-will-take-place-by-2045](https://aibusiness.com/responsible-ai/ray-kurzweil-predicts-that-the-singularity-will-take-place-by-2045)).
    What’s next?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设计算机硬件是限制因素，那么我们可以尝试预测未来。美国著名发明家和未来学家雷·库兹韦尔就是这样做的，他在2017年预测，人工智能将在大约2029年达到人类水平的智能([https://aibusiness.com/responsible-ai/ray-kurzweil-predicts-that-the-singularity-will-take-place-by-2045](https://aibusiness.com/responsible-ai/ray-kurzweil-predicts-that-the-singularity-will-take-place-by-2045))。接下来会发生什么？
- en: Can’t wait to launch your own machine learning journey? Let’s start with the
    prerequisites and the basic types of machine learning.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 迫不及待想开始自己的机器学习之旅吗？让我们从先决条件和机器学习的基本类型开始。
- en: Knowing the prerequisites
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解先决条件
- en: Machine learning mimicking human intelligence is a subfield of AI—a field of
    computer science concerned with creating systems. Software engineering is another
    field in computer science. Generally, we can label Python programming as a type
    of software engineering. Machine learning is also closely related to linear algebra,
    probability theory, statistics, and mathematical optimization. We usually build
    machine learning models based on statistics, probability theory, and linear algebra,
    and then optimize the models using mathematical optimization.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟人类智能的机器学习是人工智能的一个子领域——这是计算机科学中的一个领域，专注于创建系统。软件工程是计算机科学中的另一个领域。通常，我们可以将Python编程视为一种软件工程。机器学习与线性代数、概率论、统计学和数学优化也有着紧密的关系。我们通常基于统计学、概率论和线性代数构建机器学习模型，然后通过数学优化来优化这些模型。
- en: Most of you reading this book should have a good, or at least sufficient, command
    of Python programming. Those who aren’t feeling confident about mathematical knowledge
    might be wondering how much time should be spent learning or brushing up on the
    aforementioned subjects. Don’t panic; we will get machine learning to work for
    us without going into any deep mathematical details in this book. It just requires
    some basic 101 knowledge of probability theory and linear algebra, which helps
    us to understand the mechanics of machine learning techniques and algorithms.
    And it gets easier, as we will build models both from scratch and with popular
    packages in Python, a language we like and are familiar with.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本书的大多数人应该已经具备了良好的，或至少足够的Python编程能力。那些对数学知识不太自信的人，可能会想知道应该花多少时间学习或复习前面提到的科目。别担心；我们将在本书中不深入探讨数学细节的情况下，让机器学习为我们所用。这只需要一些概率论和线性代数的基础101知识，这有助于我们理解机器学习技术和算法的机制。而且它会变得越来越容易，因为我们将在Python这个我们喜欢且熟悉的语言中，从零开始和使用流行的包来构建模型。
- en: 'For those who want to learn or brush up on probability theory and linear algebra,
    feel free to search for basic probability theory and basic linear algebra. There
    are a lot of resources available online, for example, [https://people.ucsc.edu/~abrsvn/intro_prob_1.pdf](https://people.ucsc.edu/~abrsvn/intro_prob_1.pdf),
    the online course *Introduction to Probability* by Harvard University ([https://pll.harvard.edu/course/introduction-probability-edx](https://pll.harvard.edu/course/introduction-probability-edx))
    regarding *probability 101*, and the following paper regarding basic linear algebra:
    [http://www.maths.gla.ac.uk/~ajb/dvi-ps/2w-notes.pdf](http://www.maths.gla.ac.uk/~ajb/dvi-ps/2w-notes.pdf).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些想学习或复习概率论和线性代数的人，可以随时查找基础的概率论和基础线性代数资源。网上有许多资源，例如，[https://people.ucsc.edu/~abrsvn/intro_prob_1.pdf](https://people.ucsc.edu/~abrsvn/intro_prob_1.pdf)，哈佛大学的在线课程*Introduction
    to Probability*（[https://pll.harvard.edu/course/introduction-probability-edx](https://pll.harvard.edu/course/introduction-probability-edx)），讲授*概率101*，以及关于基础线性代数的论文：[http://www.maths.gla.ac.uk/~ajb/dvi-ps/2w-notes.pdf](http://www.maths.gla.ac.uk/~ajb/dvi-ps/2w-notes.pdf)。
- en: Those who want to study machine learning systematically can enroll in computer
    science, AI, and, more recently, data science and AI master’s programs. There
    are also various data science boot camps. However, the selection for boot camps
    is usually stricter, as they’re more job-oriented and the program duration is
    often short, ranging from 4 to 10 weeks. Another option is free **Massive Open
    Online Courses** (**MOOCs**), such as Andrew Ng’s popular course on machine learning.
    Last but not least, industry blogs and websites are great resources for us to
    keep up with the latest developments.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 想要系统学习机器学习的人可以报读计算机科学、人工智能，近年来也有数据科学与人工智能的硕士课程。同时，也有各种数据科学训练营。不过，训练营的选择通常较为严格，因为它们更侧重于就业导向，而且课程时长通常较短，范围从4到10周不等。另一个选择是免费的**大规模开放在线课程**（**MOOCs**），例如Andrew
    Ng的著名机器学习课程。最后但同样重要的是，行业博客和网站是我们了解最新进展的好资源。
- en: Machine learning is not only a skill but also a bit of a sport. We can compete
    in several machine learning competitions, such as Kaggle ([www.kaggle.com](https://www.kaggle.com))—sometimes
    for decent cash prizes, sometimes for joy, but most of the time to play to our
    strengths. However, to win these competitions, we may need to utilize certain
    techniques, which are only useful in the context of competitions and not in the
    context of trying to solve a business problem. That’s right—the **no free lunch**
    theorem ([https://en.wikipedia.org/wiki/No_free_lunch_theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem))
    applies here. In the context of machine learning, this theorem suggests that no
    single algorithm is universally superior across all possible datasets and problem
    domains.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习不仅仅是一项技能，它也有点像运动。我们可以参加几种机器学习竞赛，例如Kaggle（[www.kaggle.com](https://www.kaggle.com)）——有时是为了不错的现金奖励，有时是为了乐趣，但大多数时候是为了发挥我们的特长。不过，要赢得这些竞赛，我们可能需要利用某些技巧，这些技巧仅在竞赛环境中有用，而不适用于解决商业问题的环境。没错——**没有免费的午餐**定理（[https://en.wikipedia.org/wiki/No_free_lunch_theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem)）在这里适用。在机器学习的背景下，这一定理表明，没有任何单一算法能够在所有可能的数据集和问题领域中普遍优越。
- en: Next, we’ll take a look at the three types of machine learning.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看三种机器学习类型。
- en: Getting started with three types of machine learning
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门三种机器学习类型
- en: A machine learning system is fed with input data—this can be numerical, textual,
    visual, or audiovisual. The system usually has an output—this can be a floating-point
    number, for instance, the acceleration of a self-driving car, or an integer representing
    a category (also called a **class**), for example, a cat or tiger from image recognition.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器学习系统接收输入数据——这些数据可以是数值的、文本的、视觉的或视听的。系统通常有一个输出——这可以是一个浮点数，比如自驾车的加速度，或者是一个整数，表示一个类别（也称为**类**），例如通过图像识别分辨出猫或老虎。
- en: The main task of machine learning is to explore and construct algorithms that
    can learn from historical data and make predictions on new input data. For a data-driven
    solution, we need to define (or have it defined by an algorithm) an evaluation
    function called a **loss** or **cost function**, which measures how well the models
    learn. In this setup, we create an optimization problem with the goal of learning
    most efficiently and effectively.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的主要任务是探索和构建能够从历史数据中学习并对新输入数据做出预测的算法。对于数据驱动的解决方案，我们需要定义一个评估函数，称为**损失**或**代价函数**，它用于衡量模型学习的效果。在这种设置下，我们创建一个优化问题，目标是以最有效和最优化的方式进行学习。
- en: 'Depending on the nature of the learning data, machine learning tasks can be
    broadly classified into the following three categories:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 根据学习数据的性质，机器学习任务大致可以分为以下三类：
- en: '**Unsupervised learning**: When the learning data only contains indicative
    signals without any description attached (we call this **unlabeled data**), it’s
    up to us to find the structure of the data underneath, discover hidden information,
    or determine how to describe the data. Unsupervised learning can be used to detect
    anomalies, such as fraud or defective equipment, or group customers with similar
    online behaviors for a marketing campaign. Data visualization that makes data
    more digestible, as well as dimensionality reduction that distills relevant information
    from noisy data, are also in the family of unsupervised learning.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：当学习数据仅包含指示信号而没有附加任何描述（我们称之为**无标签数据**）时，必须由我们来找出数据背后的结构，发现隐藏的信息，或者确定如何描述数据。无监督学习可用于检测异常情况，如欺诈或设备故障，或将具有相似在线行为的客户分组进行营销活动。数据可视化使数据更易理解，降维则从噪声数据中提取相关信息，这些也属于无监督学习的范畴。'
- en: '**Supervised learning**: When learning data comes with a description, targets,
    or desired output besides indicative signals (we call this **labeled data**),
    the learning goal is to find a general rule that maps input to output. The learned
    rule is then used to label new data with unknown output. The labels are usually
    provided by event-logging systems or evaluated by human experts. Also, if feasible,
    they may be produced by human raters, through crowd-sourcing, for instance.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：当学习数据除了指示信号外，还附带描述、目标或期望输出（我们称之为**有标签数据**）时，学习目标是找到一个将输入映射到输出的一般规则。学习到的规则随后用于为新数据标注未知的输出。标签通常由事件日志系统提供，或者由人类专家评估。如果可行，标签也可以通过人类评审员、众包等方式生成。'
- en: Supervised learning is commonly used in daily applications, such as face and
    speech recognition, product or movie recommendations, sales forecasting, and spam
    email detection.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习通常用于日常应用中，比如人脸和语音识别、产品或电影推荐、销售预测和垃圾邮件检测。
- en: '**Reinforcement learning**: Learning data provides feedback so that a system
    adapts to dynamic conditions in order to ultimately achieve a certain goal. The
    system evaluates its performance based on the feedback responses and reacts accordingly.
    The best-known instances include robotics for industrial automation, self-driving
    cars, and the chess master AlphaGo. The key difference between reinforcement learning
    and supervised learning is the interaction with the environment.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强化学习**：学习数据提供反馈，系统根据反馈适应动态条件，最终实现特定目标。系统根据反馈评估其性能并作出相应反应。最著名的实例包括用于工业自动化的机器人、自驾车和国际象棋大师AlphaGo。强化学习与监督学习的关键区别在于与环境的互动。'
- en: 'The following diagram depicts the types of machine learning tasks:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了机器学习任务的类型：
- en: '![](img/B21047_01_04.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_01_04.png)'
- en: 'Figure 1.4: Types of machine learning tasks'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4：机器学习任务类型
- en: As shown in the diagram, we can further subdivide supervised learning into regression
    and classification. **Regression** trains on and predicts continuous-valued responses,
    for example, predicting house prices, while **classification** attempts to find
    the appropriate class label, such as analyzing a positive/negative sentiment and
    predicting a loan default.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，我们可以将监督学习进一步细分为回归和分类。**回归**训练并预测连续值的响应，例如预测房价，而**分类**则尝试找到适当的类别标签，比如分析正/负情感和预测贷款违约。
- en: If not all learning samples are labeled, but some are, we have **semi-supervised**
    **learning**. This makes use of unlabeled data (typically a large amount) for
    training, besides a small amount of labeled data. Semi-supervised learning is
    applied in cases where it is expensive to acquire a fully labeled dataset and
    more practical to label a small subset. For example, it often requires skilled
    experts to label hyperspectral remote sensing images, while acquiring unlabeled
    data is relatively easy.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果并非所有学习样本都被标记，但有一些被标记了，那么我们就有**半监督** **学习**。这利用未标记的数据（通常是大量的）进行训练，除了少量的标记数据。半监督学习适用于获取完全标记数据集代价昂贵，并且标记小部分数据更加实际的情况。例如，标记高光谱遥感图像通常需要熟练的专家，而获取未标记数据相对容易。
- en: Feeling a little bit confused by the abstract concepts? Don’t worry. We will
    encounter many concrete examples of these types of machine learning tasks later
    in this book. For example, in *Chapter 2*, *Building a Movie Recommendation Engine
    with Naïve Bayes*, we will dive into supervised learning classification and its
    popular algorithms and applications. Similarly, in *Chapter 5*, *Predicting Stock
    Prices with Regression Algorithms*, we will explore supervised learning regression.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些抽象概念感到有些困惑吗？别担心，我们将在本书后面遇到许多这些类型的机器学习任务的具体例子。例如，在*第2章*，*使用朴素贝叶斯构建电影推荐引擎*中，我们将深入探讨监督学习分类及其流行的算法和应用。同样，在*第5章*，*使用回归算法预测股票价格*中，我们将探索监督学习回归。
- en: We will focus on unsupervised techniques and algorithms in *Chapter 8*, *Discovering
    Underlying Topics in the Newsgroups Dataset with Clustering and Topic Modeling*.
    Last but not least, the third machine learning task, reinforcement learning, will
    be covered in *Chapter 15*, *Making Decisions in Complex Environments with Reinforcement
    Learning*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*第8章*，*使用聚类和主题建模在新闻组数据集中发现潜在主题*中重点介绍无监督学习技术和算法。最后但同样重要的是，第三种机器学习任务——强化学习，将在*第15章*，*在复杂环境中使用强化学习做出决策*中介绍。
- en: Besides categorizing machine learning based on the learning task, we can categorize
    it chronologically.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 除了根据学习任务对机器学习进行分类外，我们还可以按时间顺序进行分类。
- en: A brief history of the development of machine learning algorithms
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习算法发展的简要历史
- en: 'In fact, we have a whole zoo of machine learning algorithms that have experienced
    varying popularity over time. We can roughly categorize them into five main approaches:
    logic-based learning, statistical learning, artificial neural networks, genetic
    algorithms, and deep learning.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们拥有一个完整的机器学习算法“动物园”，这些算法经历了不同程度的流行。我们可以大致将它们分为五种主要的方式：基于逻辑的学习、统计学习、人工神经网络、遗传算法和深度学习。
- en: The **logic-based** systems were the first to be dominant. They used basic rules
    specified by human experts, and with these rules, systems tried to reason using
    formal logic, background knowledge, and hypotheses.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于逻辑的**系统是最早占主导地位的。它们使用由人工专家指定的基本规则，系统利用这些规则尝试通过形式逻辑、背景知识和假设进行推理。'
- en: '**Statistical learning** theory attempts to find a function to formalize the
    relationships between variables. In the mid-1980s, **artificial neural networks**
    (**ANNs**) came to the fore. ANNs imitate animal brains and consist of interconnected
    neurons that are also an imitation of biological neurons. They try to model complex
    relationships between input and output values and capture patterns in data. ANNs
    were superseded by statistical learning systems in the 1990s.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**统计学习**理论试图找到一个函数来形式化变量之间的关系。在1980年代中期，**人工神经网络**（**ANNs**）开始崭露头角。人工神经网络模仿动物的大脑，由相互连接的神经元组成，这些神经元也是生物神经元的模仿。它们试图建模输入与输出值之间的复杂关系，并捕捉数据中的模式。人工神经网络在1990年代被统计学习系统所取代。'
- en: '**Genetic algorithms** (**GA**) were popular in the 1990s. They mimic the biological
    process of evolution and try to find optimal solutions, using methods such as
    mutation and crossover.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**遗传算法**（**GA**）在1990年代很受欢迎。它们模仿生物进化过程，尝试通过变异和交叉等方法找到最优解。'
- en: In the 2000s, ensemble learning methods gained attention, which combined multiple
    models to improve performance.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在2000年代，集成学习方法引起了人们的关注，这些方法结合了多个模型以提高性能。
- en: We have seen **deep learning** become a dominant force since the late 2010s.
    The term deep learning was coined around 2006 and refers to deep neural networks
    with many layers. The breakthrough in deep learning was the result of the integration
    and utilization of **Graphical Processing Units** (**GPUs**), which massively
    speed up computation. The availability of large datasets also fuels the deep learning
    revolution.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 自2010年代末以来，我们看到**深度学习**成为主导力量。深度学习这个术语大约在2006年被提出，指的是具有多层的深度神经网络。深度学习的突破源于**图形处理单元**（**GPU**）的整合和应用，它们大大加速了计算。大数据集的可用性也推动了深度学习的革命。
- en: GPUs were originally developed to render video games and are very good in parallel
    matrix and vector algebra. It’s believed that deep learning resembles the way
    humans learn. Therefore, it may be able to deliver on the promise of sentient
    machines. Of course, in this book, we will dig deep into deep learning in *Chapter
    11*, *Categorizing Images of Clothing with Convolutional Neural Networks*, and
    *Chapter 12*, *Making Predictions with Sequences Using Recurrent Neural Networks*,
    after touching on it in *Chapter 6*, *Predicting Stock Prices with Artificial
    Neural Networks*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: GPU最初是为了渲染视频游戏而开发的，擅长并行矩阵和向量代数。人们认为深度学习与人类学习的方式相似。因此，它可能实现“有感知的机器”的承诺。当然，在本书中，我们将在*第11章*《使用卷积神经网络对服装图像进行分类》和*第12章*《利用循环神经网络进行序列预测》中深入探讨深度学习，在*第6章*《使用人工神经网络预测股价》中也有简要讨论。
- en: 'Machine learning algorithms continue to evolve rapidly, with ongoing research
    in areas including **transfer learning**, **generative models**, and reinforcement
    learning, which are the backbone of AIGC. We will explore the latest developments
    in *Chapter 13**, Advancing Language Understanding and Generation with the Transformer
    Models*, and *Chapter 14**, Building an Image Search Engine Using CLIP: a Multimodal
    Approach*.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法持续快速发展，研究领域涵盖**迁移学习**、**生成模型**和强化学习，这些是AIGC的核心支柱。我们将在*第13章*《通过Transformer模型推动语言理解与生成》和*第14章*《使用CLIP构建图像搜索引擎：一种多模态方法》中探讨这些最新进展。
- en: 'Some of us may have heard of **Moore’s law**—an empirical observation claiming
    that computer hardware improves exponentially with time. The law was first formulated
    by Gordon Moore, the co-founder of Intel, in 1965\. According to the law, the
    number of transistors on a chip should double every two years. In the following
    diagram, you can see that the law holds up nicely (the size of the bubbles corresponds
    to the average transistor count in GPUs):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们中的一些人可能听说过**摩尔定律**——这是一种经验法则，声称计算机硬件随时间呈指数级增长。该法则由英特尔的联合创始人戈登·摩尔（Gordon Moore）于1965年首次提出。根据摩尔定律，芯片上的晶体管数量每两年应该翻一倍。在下图中，你可以看到这一规律得到了很好的验证（气泡的大小对应GPU中晶体管的平均数量）：
- en: '![A picture containing text, screenshot, plot, line  Description automatically
    generated](img/B21047_01_05.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含文字、截图、图表、线条的图片  描述自动生成](img/B21047_01_05.png)'
- en: 'Figure 1.5: Transistor counts over the past decades'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5：过去几十年的晶体管数量
- en: The consensus seems to be that Moore’s law should continue to be valid for a
    couple of decades. This gives some credibility to Ray Kurzweil’s predictions of
    achieving true machine intelligence by 2029.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 共识似乎是，摩尔定律将在接下来的几十年继续有效。这为雷·库兹韦尔（Ray Kurzweil）预测在2029年实现真正的机器智能提供了一些可信度。
- en: Digging into the core of machine learning
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入挖掘机器学习的核心
- en: After discussing the categorization of machine learning algorithms, we are now
    going to dig into the core of machine learning—generalizing with data, the different
    levels of generalization, as well as the approaches to attain the right level
    of generalization.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了机器学习算法的分类后，我们现在将深入探讨机器学习的核心——用数据进行概括、不同层次的概括以及如何达到合适的概括层次的方法。
- en: Generalizing with data
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用数据进行概括
- en: The good thing about data is that there’s a lot of it in the world. The bad
    thing is that it’s hard to process this data. The challenge stems from the diversity
    and noisiness of it. We humans usually process data coming into our ears and eyes.
    These inputs are transformed into electrical or chemical signals. On a very basic
    level, computers and robots also work with electrical signals.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的好处是它在世界上有大量存在。坏处是处理这些数据非常困难。挑战来自于数据的多样性和噪声。我们人类通常处理进入我们耳朵和眼睛的数据。这些输入会转化为电信号或化学信号。在一个非常基础的层面，计算机和机器人也处理电信号。
- en: These electrical signals are then translated into ones and zeros. However, we
    program in Python in this book, and on that level, normally we represent the data
    either as numbers or texts. However, text isn’t very convenient, so we need to
    transform this into numerical values.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些电信号随后会被转化为0和1。然而，在本书中我们使用Python进行编程，在这个层面上，通常我们将数据表示为数字或文本。然而，文本并不是特别方便，因此我们需要将其转化为数值。
- en: Especially in the context of supervised learning, we have a scenario similar
    to studying for an exam. We have a set of practice questions and the actual exams.
    We should be able to answer exam questions without being exposed to identical
    questions beforehand. This is called **generalization**—we learn something from
    our practice questions and, hopefully, can apply this knowledge to other similar
    questions. In machine learning, these practice questions are called **training
    sets** or **training samples**. This is where the machine learning models derive
    patterns from. And the actual exams are **testing sets** or **testing samples**.
    They are where the models are eventually applied. Learning effectiveness is measured
    by the compatibility of the learning models and the testing.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是在监督学习的背景下，我们有一个类似于考试复习的场景。我们有一组练习题和实际考试。我们应该能够在没有提前接触相同问题的情况下回答考试问题。这被称为**泛化**——我们从练习题中学到一些东西，并希望能够将这些知识应用于其他类似的问题。在机器学习中，这些练习题被称为**训练集**或**训练样本**。机器学习模型从这些样本中推导出模式。而实际考试则是**测试集**或**测试样本**。模型最终将在这些测试集中应用。学习效果通过学习模型与测试的兼容性来衡量。
- en: Sometimes, between practice questions and actual exams, we have mock exams to
    assess how well we will do in actual exams and to aid revision. These mock exams
    are known as **validation sets** or **validation samples** in machine learning.
    They help us to verify how well the models will perform in a simulated setting,
    and then we fine-tune the models accordingly in order to achieve greater accuracy.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，在练习题和实际考试之间，我们会进行模拟考试来评估自己在实际考试中的表现，并帮助复习。这些模拟考试在机器学习中被称为**验证集**或**验证样本**。它们帮助我们验证模型在模拟环境中的表现，然后我们根据结果对模型进行微调，以实现更高的准确度。
- en: An old-fashioned programmer would talk to a business analyst or other expert,
    and then implement a tax rule that adds a certain value multiplied by another
    corresponding value, for instance. In a machine learning setting, we can give
    the computer a bunch of input and output examples; alternatively, if we want to
    be more ambitious, we can feed the program the actual tax texts. We can let the
    machine consume the data and figure out the tax rule, just as an autonomous car
    doesn’t need a lot of explicit human input.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的程序员会与业务分析师或其他专家进行沟通，然后实现一个税收规则，例如将某个值与另一个对应的值相乘。而在机器学习环境中，我们可以给计算机提供一堆输入和输出示例；或者，如果我们想更有雄心，可以将实际的税法文本输入程序。我们可以让机器消耗这些数据并自动推导出税收规则，就像自动驾驶汽车不需要太多明确的人类输入一样。
- en: In physics, we have almost the same situation. We want to know how the universe
    works and formulate laws in a mathematical language. Since we don’t know how it
    works, all we can do is measure the error produced in our attempt at law formulation
    and try to minimize it. In supervised learning tasks, we compare our results against
    the expected values. In unsupervised learning, we measure our success with related
    metrics. For instance, we want data points to be grouped based on similarities,
    forming clusters; the metrics could be how similar the data points within one
    cluster are, or how different the data points from two clusters are. In reinforcement
    learning, a program evaluates its moves, for example, by using a predefined function
    in a chess game.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在物理学中，我们也遇到几乎相同的情况。我们想要了解宇宙如何运作，并用数学语言来制定规律。由于我们不知道宇宙如何运作，唯一能做的就是在尝试制定规律时测量所产生的误差，并尽量减少它。在监督学习任务中，我们将结果与期望值进行比较。在无监督学习中，我们通过相关指标来衡量我们的成功。例如，我们希望数据点根据相似性进行分组，形成簇；这些指标可以是簇内数据点的相似度，或者两个簇之间数据点的差异度。在强化学习中，程序通过评估自己的操作来进行学习，例如，在国际象棋游戏中使用预定义的函数来评估其走法。
- en: Aside from correct generalization with data, there are two levels of generalization,
    overfitting and underfitting, which we will explore in the next section.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据的正确泛化外，还有两种泛化层次，过拟合和欠拟合，我们将在下一节中探讨这两个层次。
- en: Overfitting, underfitting, and the bias-variance trade-off
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合、欠拟合与偏差-方差权衡
- en: In this section, let’s take a look at both levels of generalization in detail
    and explore the bias-variance trade-off.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细探讨两种泛化层次，并深入分析偏差-方差权衡。
- en: Overfitting
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过拟合
- en: Reaching the right fit model is the goal of a machine learning task. What if
    the model overfits? **Overfitting** means a model fits the existing observations
    **too well** but fails to predict future new observations. Let’s look at the following
    analogy.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 达到合适的拟合模型是机器学习任务的目标。那么，如果模型出现过拟合怎么办？**过拟合**意味着模型对现有观察数据**拟合得过于完美**，但无法预测未来新的观察数据。让我们看一下以下的类比。
- en: If we go through many practice questions for an exam, we may start to find ways
    to answer questions that have nothing to do with the subject material. For instance,
    given only five practice questions, we might find that if there are two occurrences
    of *potatoes*, one of *tomato*, and three of *banana* in a multiple-choice question,
    the answer is always *A*, and if there is one occurrence of *potato*, three of
    *tomato*, and two of *banana* in a question, the answer is always *B*. We could
    then conclude that this is always true and apply such a theory later, even though
    the subject or answer may not be relevant to potatoes, tomatoes, or bananas. Or,
    even worse, we might memorize the answers to each question verbatim. We would
    then score highly on the practice questions, leading us to hope that the questions
    in the actual exams would be the same as the practice questions. However, in reality,
    we would score very low on the exam questions, as it’s rare that the exact same
    questions occur in exams.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们做很多考试的练习题，可能会开始找到一些与学科内容无关的答题方法。例如，给定只有五道练习题，我们可能会发现，如果选择题中有两次出现*土豆*，一次出现*西红柿*，三次出现*香蕉*，答案总是*A*；如果有一次出现*土豆*，三次出现*西红柿*，两次出现*香蕉*，答案总是*B*。然后我们可能会得出这样的结论：这总是成立的，并在之后应用这样的理论，尽管学科或答案与土豆、西红柿或香蕉无关。更糟糕的是，我们可能会逐字记住每一道题的答案。这样，我们在练习题上会得分很高，并希望实际考试中的问题与练习题相同。然而，实际上，我们在考试中的得分会很低，因为考试中很少会出现完全相同的问题。
- en: 'The phenomenon of memorization can cause overfitting. This can occur when we’re
    over-extracting too much information from the training sets and making our model
    just work well with them. At the same time, however, overfitting won’t help us
    to generalize it to new data and derive true patterns from it. The model, as a
    result, will perform poorly on datasets that weren’t seen before. We call this
    situation **high variance** in machine learning. Let’s quickly recap variance:
    *variance* measures the spread of the prediction, which is the variability of
    the prediction. It can be calculated as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆现象可能导致过拟合。这种情况发生在我们从训练集中过度提取信息，使得我们的模型只在这些训练数据上表现良好。然而，过拟合并不会帮助我们将模型推广到新数据，并从中推导出真正的规律。因此，模型在处理之前未见过的数据集时表现很差。我们称这种情况为机器学习中的**高方差**。让我们快速回顾一下方差：*方差*衡量的是预测结果的分散程度，即预测结果的变化性。它可以通过以下方式计算：
- en: '![](img/B21047_01_001.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_01_001.png)'
- en: Here, *ŷ* is the prediction, and E[] is the expectation or expected value that
    represents the average value of a random variable, based on its probability distribution
    in statistics.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*ŷ* 是预测值，E[] 是期望或期望值，表示基于概率分布的随机变量的平均值。
- en: 'The following example demonstrates what a typical instance of overfitting looks
    like, where the regression curve tries to flawlessly accommodate all observed
    samples:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了典型的过拟合情况，其中回归曲线试图完美地适应所有观察到的样本：
- en: '![A picture containing map, screenshot  Description automatically generated](img/B21047_01_06.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing map, screenshot  Description automatically generated](img/B21047_01_06.png)'
- en: 'Figure 1.6: Example of overfitting'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6：过拟合示例
- en: Overfitting occurs when we try to describe the learning rules based on too many
    parameters relative to the small number of observations, instead of the underlying
    relationship, such as the preceding potato, tomato, and banana example, where
    we deduced three parameters from only five learning samples. Overfitting also
    takes place when we make the model so excessively complex that it fits every training
    sample, such as memorizing the answers for all questions, as mentioned previously.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合发生在我们尝试基于相对于少量观察样本过多的参数来描述学习规则，而不是描述潜在关系，例如之前的土豆、西红柿和香蕉的例子，其中我们仅从五个学习样本中推导出三个参数。过拟合还发生在我们使模型过于复杂，以至于它完美拟合所有训练样本，就像之前提到的，记住所有问题的答案。
- en: Underfitting
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欠拟合
- en: 'The opposite scenario is **underfitting**. When a model is underfit, it doesn’t
    perform well on the training sets and won’t do so on the testing sets, which means
    it fails to capture the underlying trend of the data. Underfitting may occur if
    we don’t use enough data to train the model, just like we will fail the exam if
    we don’t review enough material; this may also happen if we try to fit a wrong
    model to the data, just as we will score low in any exercises or exams if we take
    the wrong approach and learn in the wrong way. We describe any of these situations
    as **high** **bias** in machine learning, although its variance is low, as the
    performance in training and test sets is consistent, in a bad way. If you need
    a quick recap of bias, here it is: **bias** is the difference between the average
    prediction and the true value. It is computed as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 相反的情况是**欠拟合**。当模型欠拟合时，它在训练集上的表现不好，在测试集上也不会表现好，这意味着它未能捕捉到数据的潜在趋势。如果我们用的数据不足来训练模型，就像我们不复习足够的材料而无法通过考试；如果我们尝试为数据拟合错误的模型，也会发生这种情况，就像我们如果采取错误的方法和错误的学习方式，做任何练习或考试都会得低分。我们将这些情况描述为机器学习中的**高**
    **偏差**，尽管它的方差较低，因为训练集和测试集中的表现一致，都是不好的。如果你需要快速回顾一下偏差，下面是它的定义：**偏差**是预测值和真实值之间的差异。它的计算方法如下：
- en: '![](img/B21047_01_002.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_01_002.png)'
- en: Here, *ŷ* is the prediction and *y* is the ground truth.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*ŷ* 是预测值，*y* 是真实值。
- en: 'The following example shows what typical underfitting looks like, where the
    regression curve doesn’t fit the data well enough or capture enough of the underlying
    pattern of the data:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了典型的欠拟合情况，其中回归曲线没有很好地拟合数据，或者没有捕捉到数据的潜在模式：
- en: '![A picture containing screenshot, line, diagram, plot  Description automatically
    generated](img/B21047_01_07.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing screenshot, line, diagram, plot  Description automatically
    generated](img/B21047_01_07.png)'
- en: 'Figure 1.7: Example of underfitting'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7：欠拟合示例
- en: 'Now, let’s look at what a well-fitting example should look like:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一个良好拟合的示例应该是什么样子：
- en: '![A picture containing screenshot, line, plot  Description automatically generated](img/B21047_01_08.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing screenshot, line, plot  Description automatically generated](img/B21047_01_08.png)'
- en: 'Figure 1.8: Example of desired fitting'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8：期望拟合的示例
- en: The bias-variance trade-off
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏差-方差权衡
- en: Obviously, we want to avoid both overfitting and underfitting. Recall that **bias**
    is the error stemming from incorrect assumptions in the learning algorithm; high
    bias results in underfitting. **Variance** measures how sensitive the model prediction
    is to variations in the datasets. Hence, we need to avoid cases where either bias
    or variance gets high. So, does it mean we should always make both bias and variance
    as low as possible? The answer is yes, if we can. But, in practice, there is an
    explicit trade-off between them, where decreasing one increases the other. This
    is the so-called **bias-variance trade-off**. Sounds abstract? Let’s look at the
    next example.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们希望避免过拟合和欠拟合。回想一下，**偏差**是学习算法中由不正确假设引起的误差；高偏差会导致欠拟合。**方差**衡量模型预测对数据集变化的敏感度。因此，我们需要避免偏差或方差过高的情况。那么，是否意味着我们应该始终尽量将偏差和方差都降到最低？答案是，如果可能的话，应该是的。但在实践中，偏差和方差之间存在明确的权衡关系，减少一个会增加另一个。这就是所谓的**偏差-方差权衡**。听起来有点抽象？让我们看下一个例子。
- en: Let’s say we’re asked to build a model to predict the probability of a candidate
    being the next president of America based on phone poll data. The poll is conducted
    using zip codes. We randomly choose samples from one zip code, and we estimate
    there’s a 61% chance the candidate will win. However, it turns out they lost the
    election. Where did our model go wrong? The first thing we might think of is the
    small size of samples from only one zip code. It’s a source of high bias also,
    as people in a geographic area tend to share similar demographics, although it
    results in a low variance of estimates. So can we fix it simply by using samples
    from a large number of zip codes? Yes, but don’t get happy too soon. This might
    cause an increased variance of estimates at the same time. We need to find the
    optimal sample size—the best number of zip codes to achieve the lowest overall
    bias and variance.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们被要求构建一个模型，基于电话民调数据预测候选人当选为美国下任总统的概率。该民调使用邮政编码进行调查。我们从某个邮政编码随机选择样本，估计该候选人获胜的概率为61%。然而，事实证明他们输了选举。我们的模型出错的地方在哪里？我们首先可能想到的是样本数量过少，仅来自一个邮政编码。这个问题也来源于高偏差，因为某个地区的人们通常有相似的人口特征，尽管这样做会导致估算结果的方差较小。那么，能否通过使用来自更多邮政编码的样本来解决这个问题呢？是的，但不要太高兴。这可能会导致估算结果的方差同时增加。我们需要找到最佳的样本大小——即选择最佳的邮政编码数量，以实现最低的整体偏差和方差。
- en: 'Minimizing the total error of a model requires a careful balancing of bias
    and variance. Given a set of training samples, *x*[1], *x*[2], …, *x*[n], and
    their targets, *y*[1], *y*[2], …, *y*[n], we want to find a regression function
    *ŷ*(*x*) that estimates the true relation *y*(*x*) as correctly as possible. We
    measure the error of estimation, i.e., how good (or bad) the regression model
    is, in **mean squared error** (**MSE**):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化模型的总误差需要谨慎平衡偏差和方差。给定一组训练样本，*x*[1]、*x*[2]、……、*x*[n]，以及它们的目标值，*y*[1]、*y*[2]、……、*y*[n]，我们希望找到一个回归函数*ŷ*(*x*)，使得它尽可能准确地估计出真实关系
    *y*(*x*)。我们通过**均方误差**（**MSE**）来衡量估算误差，即回归模型的好坏：
- en: '![](img/B21047_01_003.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_01_003.png)'
- en: 'The *E* denotes the expectation. This error can be decomposed into bias and
    variance components following the analytical derivation, as shown in the following
    formula (although it requires a bit of basic probability theory to understand):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*E*表示期望。这个误差可以通过以下公式分解为偏差和方差两个部分（尽管理解这一点需要一点基本的概率论知识）：'
- en: '![](img/B21047_01_004.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_01_004.png)'
- en: '![](img/B21047_01_005.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_01_005.png)'
- en: '![](img/B21047_01_006.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_01_006.png)'
- en: '![](img/B21047_01_007.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_01_007.png)'
- en: '![](img/B21047_01_008.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_01_008.png)'
- en: '![](img/B21047_01_009.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_01_009.png)'
- en: The term *Bias* measures the error of estimations, and the term *Variance* describes
    how much the estimation, *ŷ*, moves around its mean, *E*[*ŷ*]. The more complex
    the learning model *ŷ*(*x*) is, and the larger the size of the training samples
    is, the lower the bias will become. However, this will also create more adjustments
    to the model to better fit the increased data points. As a result, the variance
    will be lifted.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*Bias*项衡量估算误差，*Variance*项描述了估算值*ŷ*相对于其均值*E*[*ŷ*]的波动幅度。学习模型*ŷ*(*x*)越复杂，训练样本数量越大，偏差就越小。然而，这也会导致模型进行更多的调整，以更好地适应增多的数据点。结果，方差会增大。
- en: We usually employ the cross-validation technique, as well as regularization
    and feature reduction, to find the optimal model balancing bias and variance and
    diminish overfitting. We will discuss these next.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常采用交叉验证技术，以及正则化和特征降维，来找到平衡偏差和方差、减少过拟合的最优模型。接下来我们将讨论这些内容。
- en: 'You may ask why we only want to deal with overfitting: how about underfitting?
    This is because underfitting can be easily recognized: it occurs if a model doesn’t
    work well on a training set. When this occurs, we need to find a better model
    or tweak some parameters to better fit the data, which is a must under all circumstances.
    On the other hand, overfitting is hard to spot. Oftentimes, when we achieve a
    model that performs well on a training set, we are overly happy and think it is
    ready for production right away. This can be very dangerous. We should instead
    take extra steps to ensure that the great performance isn’t due to overfitting
    and that the great performance applies to data that excludes the training data.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问为什么我们只关注过拟合问题：欠拟合呢？这是因为欠拟合很容易识别：如果模型在训练集上表现不佳，则会发生。发生这种情况时，我们需要找到一个更好的模型或调整一些参数以更好地拟合数据，这在任何情况下都是必须的。另一方面，过拟合很难察觉。通常情况下，当我们得到一个在训练集上表现良好的模型时，会过于高兴并认为它已经可以立即投入生产。这可能非常危险。我们应该采取额外措施，确保出色的性能不是由于过拟合造成的，并且这种出色的性能适用于排除训练数据的数据。
- en: Avoiding overfitting with cross-validation
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用交叉验证避免过拟合
- en: You will see cross-validation in action multiple times later in this book. So
    don’t panic if you find this section difficult to understand, as you will become
    an expert on cross-validation very soon.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在本书的后续多次看到交叉验证的实际运用。因此，如果你发现这部分内容难以理解，请不要恐慌，因为你很快就会成为交叉验证的专家。
- en: Recall that between practice questions and actual exams, there are mock exams
    where we can assess how well we will perform in actual exams and use that information
    to conduct the necessary revision. In machine learning, the validation procedure
    helps to evaluate how models will generalize to independent or unseen datasets
    in a simulated setting. In a conventional validation setting, the original data
    is partitioned into three subsets, usually 60% for the training set, 20% for the
    validation set, and the rest (20%) for the testing set. This setting suffices
    if we have enough training samples after partitioning and we only need a rough
    estimate of simulated performance. Otherwise, cross-validation is preferable.
    Cross-validation helps to reduce variability and, therefore, limit overfitting.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在练习问题和实际考试之间，有模拟考试，我们可以评估我们在实际考试中的表现如何，并利用这些信息进行必要的复习。在机器学习中，验证过程有助于评估模型如何推广到独立或未见数据集的能力。在传统的验证设置中，原始数据通常被分成三个子集，通常为60%的训练集，20%的验证集，以及其余的20%作为测试集。如果在划分后有足够的训练样本，并且我们只需要一个模拟性能的粗略估计，那么这种设置就足够了。否则，交叉验证更可取。交叉验证有助于减少变异性，从而限制过拟合。
- en: In one round of cross-validation, the original data is divided into two subsets,
    for **training** and **testing** (or **validation**), respectively. The testing
    performance is recorded. Similarly, multiple rounds of cross-validation are performed
    under different partitions. Testing results from all rounds are finally averaged
    to generate a more reliable estimate of model prediction performance.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在一轮交叉验证中，原始数据被分为两个子集，分别用于**训练**和**测试**（或**验证**）。记录测试性能。类似地，通过不同的划分执行多轮交叉验证。所有轮次的测试结果最终平均以生成模型预测性能的更可靠估计。
- en: When the training size is very large, it’s often sufficient to split it into
    training, validation, and testing (three subsets) and conduct a performance check
    on the latter two. Cross-validation is less preferable in this case, since it’s
    computationally costly to train a model for each single round. But if you can
    afford it, there’s no reason not to use cross-validation. When the size isn’t
    so large, cross-validation is definitely a good choice.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练样本非常大时，通常将其分为训练、验证和测试（三个子集），并在后两者上进行性能检查。在这种情况下，交叉验证不太理想，因为为每一轮训练模型的计算成本很高。但如果你能负担得起，没有理由不使用交叉验证。当数据量不那么大时，交叉验证绝对是个不错的选择。
- en: 'There are mainly two cross-validation schemes in use: exhaustive and non-exhaustive.
    In the **exhaustive scheme**, we leave out a fixed number of observations in each
    round as testing (or validation) samples and use the remaining observations as
    training samples. This process is repeated until all possible different subsets
    of samples are used for testing once. For instance, we can apply **Leave-One-Out-Cross-Validation**
    (**LOOCV**), which lets each sample be in the testing set once. For a dataset
    of the size *n*, LOOCV requires *n* rounds of cross-validation. This can be slow
    when *n* gets large. The following diagram presents the workflow of LOOCV:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 目前主要有两种交叉验证方案：穷举式和非穷举式。在**穷举式方案**中，我们在每轮中留下固定数量的观测值作为测试（或验证）样本，使用其余的观测值作为训练样本。这个过程会重复进行，直到所有可能的不同样本子集都被用于测试一次。例如，我们可以应用**留一交叉验证**（**LOOCV**），让每个样本都成为一次测试集。对于一个大小为
    *n* 的数据集，LOOCV 需要进行 *n* 轮交叉验证。当 *n* 较大时，这可能会非常慢。下图展示了 LOOCV 的工作流：
- en: '![A screenshot of a test  Description automatically generated with low confidence](img/B21047_01_09.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![测试截图 自动生成的描述，信心较低](img/B21047_01_09.png)'
- en: 'Figure 1.9: Workflow of leave-one-out-cross-validation'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9：留一交叉验证的工作流
- en: A **non-exhaustive scheme**, on the other hand, as the name implies, doesn’t
    try out all possible partitions. The most widely used type of this scheme is **k-fold
    cross-validation**. First, we randomly split the original data into **k equal-sized**
    folds. In each trial, one of these folds becomes the testing set, and the rest
    of the data becomes the training set.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**非穷举式方案**，顾名思义，并不会尝试所有可能的分区。这个方案中最常用的一种是**k 折交叉验证**。首先，我们将原始数据随机分为 **k
    个等大小** 的折叠。在每次试验中，这些折叠中的一个会成为测试集，其余的数据将成为训练集。
- en: 'We repeat this process *k* times, with each fold being the designated testing
    set once. Finally, we average the *k* sets of test results for the purpose of
    evaluation. Common values for *k* are 3, 5, and 10\. The following table illustrates
    the setup for five-fold cross-validation:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个过程重复 *k* 次，每个折叠都作为指定的测试集。最后，我们将 *k* 组测试结果进行平均，以便评估。常见的 *k* 值为 3、5 和 10。下表展示了五折交叉验证的设置：
- en: '| **Round** | **Fold 1** | **Fold 2** | **Fold 3** | **Fold 4** | **Fold 5**
    |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| **轮次** | **折叠 1** | **折叠 2** | **折叠 3** | **折叠 4** | **折叠 5** |'
- en: '| 1 | **Testing** | Training | Training | Training | Training |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 1 | **测试** | 训练 | 训练 | 训练 | 训练 |'
- en: '| 2 | Training | **Testing** | Training | Training | Training |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 训练 | **测试** | 训练 | 训练 | 训练 |'
- en: '| 3 | Training | Training | **Testing** | Training | Training |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 训练 | 训练 | **测试** | 训练 | 训练 |'
- en: '| 4 | Training | Training | Training | **Testing** | Training |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 训练 | 训练 | 训练 | **测试** | 训练 |'
- en: '| 5 | Training | Training | Training | Training | **Testing** |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 训练 | 训练 | 训练 | 训练 | **测试** |'
- en: 'Table 1.1: Setup for five-fold cross-validation'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.1：五折交叉验证的设置
- en: K-fold cross-validation often has a lower variance compared to LOOCV, since
    we’re using a chunk of samples instead of a single one for validation.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 与 LOOCV 相比，K 折交叉验证通常具有较低的方差，因为我们使用的是一组样本，而不是单个样本进行验证。
- en: We can also randomly split the data into training and testing sets numerous
    times. This is formally called the **holdout** method. The problem with this algorithm
    is that some samples may never end up in the testing set, while some may be selected
    multiple times in the testing set.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以多次随机将数据分为训练集和测试集。这种方法正式称为**留出法**。这个算法的问题在于，有些样本可能永远不会进入测试集，而有些样本可能会被多次选入测试集。
- en: 'Last but not least, **nested cross-validation** is a combination of cross-validations.
    It consists of the following two phases:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，**嵌套交叉验证**是交叉验证的组合。它包括以下两个阶段：
- en: '**Inner cross-validation**: This phase is conducted to find the best fit and
    can be implemented as a *k*-fold cross-validation'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内部交叉验证**：这个阶段用于找到最佳拟合，可以实现为 *k* 折交叉验证。'
- en: '**Outer cross-validation**: This phase is used for performance evaluation and
    statistical analysis'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外部交叉验证**：这个阶段用于性能评估和统计分析。'
- en: We will apply cross-validation very intensively throughout this entire book.
    Before that, let’s look at cross-validation with an analogy next, which will help
    us to better understand it.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在整本书中非常密集地使用交叉验证。在此之前，让我们通过类比来了解交叉验证，这将帮助我们更好地理解它。
- en: A data scientist plans to take his car to work, and his goal is to arrive before
    9 a.m. every day. He needs to decide the departure time and the route to take.
    He tries out different combinations of these two parameters on certain Mondays,
    Tuesdays, and Wednesdays and records the arrival time for each trial. He then
    figures out the best schedule and applies it every day. However, it doesn’t work
    quite as well as expected.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一位数据科学家计划开车上班，他的目标是每天都在早上9点之前到达。他需要决定出发时间和路线。他尝试了周一、周二和周三这三天的不同参数组合，并记录了每次试验的到达时间。然后，他找出了最佳的调度并每天应用它。然而，效果并没有预期的好。
- en: It turns out the scheduling **model** is overfitted to the data points gathered
    in the first three days and may not work well on Thursdays and Fridays. A better
    solution would be to test the best combination of parameters derived from Mondays
    to Wednesdays on Thursdays and Fridays and similarly repeat this process, based
    on different sets of learning days and testing days of the week. This analogized
    cross-validation ensures that the selected schedule works for the whole week.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，调度**模型**对前面三天收集到的数据点发生了过拟合，可能在周四和周五效果不佳。更好的解决方案是，在周四和周五上测试从周一到周三得到的最佳参数组合，并根据不同的学习日和测试日组合，重复这一过程。这种类比交叉验证确保了所选调度在整个星期内都能有效。
- en: In summary, cross-validation derives a more accurate assessment of model performance
    by combining measures of prediction performance on different subsets of data.
    This technique not only reduces variance and avoids overfitting but also gives
    an insight into how a model will generally perform in practice.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，交叉验证通过结合对不同数据子集的预测性能评估，得出了更准确的模型表现评估。这项技术不仅减少了方差，避免了过拟合，还能让我们了解一个模型在实际中的总体表现。
- en: Avoiding overfitting with regularization
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过正则化避免过拟合
- en: Another way of preventing overfitting is **regularization**. Recall that the
    unnecessary complexity of a model is a source of overfitting. Regularization adds
    extra parameters to the error function we’re trying to minimize, in order to penalize
    complex models.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种防止过拟合的方法是**正则化**。回想一下，模型的不必要复杂性是过拟合的来源。正则化通过在我们试图最小化的误差函数中添加额外的参数，从而惩罚复杂模型。
- en: 'According to the principle of Occam’s razor, simpler methods are to be favored.
    William Occam was a monk and philosopher who, around the year 1320, came up with
    the idea that the simplest hypothesis that fits data should be preferred. One
    justification for this is that we can invent fewer simple models than complex
    models. For instance, intuitively, we know that there are more high-polynomial
    models than linear ones. The reason is that a line (*y* = *ax* + *b*) is governed
    by only two parameters—the intercept, *b*, and slope, *a*. The possible coefficients
    for a line span two-dimensional space. A quadratic polynomial adds an extra coefficient
    for the quadratic term, and we can span a three-dimensional space with the coefficients.
    Therefore, it is much easier to find a model that perfectly captures all training
    data points with a **high-order polynomial function**, as its search space is
    much larger than that of a linear function. However, these easily obtained models
    generalize worse than linear models, which are more prone to overfitting. Also,
    of course, simpler models require less computation time. The following diagram
    displays how we try to fit a linear function and a high order polynomial function,
    respectively, to the data:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 根据奥卡姆剃刀原理，应该偏好简单的方法。威廉·奥卡姆是一位僧侣和哲学家，大约在1320年，他提出了一个观点：应该选择最简单的、符合数据的假设。这样做的一个理由是，我们可以创造出比复杂模型更少的简单模型。例如，直观上，我们知道高次多项式模型比线性模型更多。原因在于，一条直线（*y*
    = *ax* + *b*）只由两个参数控制——截距 *b* 和斜率 *a*。直线的系数可以在二维空间内变化。而二次多项式则为二次项增加了一个额外的系数，我们可以通过系数在三维空间内进行表示。因此，使用**高次多项式函数**找到一个完全拟合所有训练数据点的模型要容易得多，因为它的搜索空间远大于线性函数。然而，这些容易得到的模型在泛化能力上远不如线性模型，更容易发生过拟合。当然，简单模型所需的计算时间也较少。下图展示了我们如何分别尝试将线性函数和高次多项式函数拟合到数据上：
- en: '![A picture containing line, screenshot, text, diagram  Description automatically
    generated](img/B21047_01_10.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含线条、截图、文本、图表的图片 说明自动生成](img/B21047_01_10.png)'
- en: 'Figure 1.10: Fitting data with a linear function and a polynomial function'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10：用线性函数和多项式函数拟合数据
- en: The linear model is preferable, as it may generalize better to more data points
    drawn from the underlying distribution. We can use regularization to reduce the
    influence of the high orders of a polynomial by imposing penalties on them. This
    will discourage complexity, even though a less accurate and less strict rule is
    learned from the training data.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型更为优选，因为它可能更好地推广到从底层分布中抽取的更多数据点。我们可以使用正则化通过对高阶多项式施加惩罚来减少其影响。即使从训练数据中学习到一个不那么准确且不那么严格的规则，这也能抑制模型的复杂性。
- en: We will employ regularization quite often in this book, starting from *Chapter
    4*, *Predicting Online Ad Click-Through with Logistic Regression*. For now, let’s
    look at an analogy that can help you better understand regularization.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将频繁使用正则化，从*第4章*《*使用逻辑回归预测在线广告点击率*》开始。现在，让我们看一个类比，帮助你更好地理解正则化。
- en: 'A data scientist wants to equip his robotic guard dog with the ability to identify
    strangers and his friends. He feeds it with the following learning samples:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一位数据科学家希望给他的机器人看门狗配备识别陌生人和朋友的能力。他为其提供了以下学习样本：
- en: '| Male | Young | Tall | With glasses | In grey | **Friend** |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 男性 | 年轻 | 高 | 戴眼镜 | 穿灰色 | **朋友** |'
- en: '| Female | Middle | Average | Without glasses | In black | **Stranger** |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 女性 | 中年 | 一般 | 无眼镜 | 穿黑色 | **陌生人** |'
- en: '| Male | Young | Short | With glasses | In white | **Friend** |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 男性 | 年轻 | 矮小 | 戴眼镜 | 穿白色 | **朋友** |'
- en: '| Male | Senior | Short | Without glasses | In black | **Stranger** |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 男性 | 年长 | 矮小 | 无眼镜 | 穿黑色 | **陌生人** |'
- en: '| Female | Young | Average | With glasses | In white | **Friend** |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 女性 | 年轻 | 一般 | 戴眼镜 | 穿白色 | **朋友** |'
- en: '| Male | Young | Short | Without glasses | In red | **Friend** |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 男性 | 年轻 | 矮小 | 无眼镜 | 穿红色 | **朋友** |'
- en: 'Table 1.2: Training samples for the robotic guard dog'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.2：机器人看门狗的训练样本
- en: 'The robot may quickly learn the following rules:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人可能会迅速学会以下规则：
- en: Any middle-aged female of average height without glasses and dressed in black
    is a stranger
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何中年女性，身高一般，未戴眼镜且穿着黑色衣服的，都是陌生人
- en: Any senior short male without glasses and dressed in black is a stranger
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何身材矮小的中年男性，戴眼镜且穿着黑色衣服的，都是陌生人
- en: Anyone else is his friend
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他任何人都是他的朋友
- en: 'Although these perfectly fit the training data, they seem too complicated and
    unlikely to generalize well to new visitors. In contrast, the data scientist limits
    the learning aspects. A loose rule that can work well for hundreds of other visitors
    could be as follows: anyone without glasses dressed in black is a stranger.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些规则完全符合训练数据，但它们似乎过于复杂，不太可能很好地推广到新访客。相比之下，数据科学家限制了学习的方面。一个对数百个其他访客有效的松散规则可能如下：任何没有眼镜、穿黑色衣服的人都是陌生人。
- en: Besides penalizing complexity, we can also stop a training procedure early as
    a technique to prevent overfitting. If we limit the time a model spends learning
    or set some internal stopping criteria, it’s more likely to produce a simpler
    model. The model complexity will be controlled in this way; hence, overfitting
    becomes less probable. This approach is called **early stopping** in machine learning.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 除了惩罚复杂性，我们还可以通过早期停止训练过程来防止过拟合。如果我们限制模型的学习时间或设置某些内部停止标准，更有可能得到一个更简单的模型。通过这种方式控制模型复杂度，因此，过拟合变得不太可能。这种方法在机器学习中称为**早期停止**。
- en: Last but not least, it’s worth noting that regularization should be kept at
    a moderate level or, to be more precise, fine-tuned to an optimal level. Too small
    a regularization doesn’t make any impact; too large a regularization will result
    in underfitting, as it moves the model away from the ground truth. We will explore
    how to achieve optimal regularization in *Chapter 4*, *Predicting Online Ad Click-Through
    with Logistic Regression*, *Chapter 5*, *Predicting Stock Prices with Regression
    Algorithms*, and *Chapter 6*, *Predicting Stock Prices with Artificial Neural
    Networks*.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，值得注意的是正则化应保持在适度水平，或者更准确地说，需要对其进行微调以达到最佳水平。正则化过小不会产生任何影响；正则化过大则会导致欠拟合，因为它会使模型远离真实值。我们将在*第4章*《*使用逻辑回归预测在线广告点击率*》、*第5章*《*使用回归算法预测股票价格*》和*第6章*《*使用人工神经网络预测股票价格*》中探讨如何实现最佳正则化。
- en: Avoiding overfitting with feature selection and dimensionality reduction
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过特征选择和降维来避免过拟合
- en: We typically represent data as a grid of numbers (a **matrix**). Each column
    represents a variable, which we call a **feature** in machine learning. In supervised
    learning, one of the variables is actually not a feature but the label that we’re
    trying to predict. And in supervised learning, each row is an example that we
    can use for training or testing.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将数据表示为一个数字网格（**矩阵**）。每一列代表一个变量，在机器学习中我们称之为**特征**。在监督学习中，其中一个变量实际上不是特征，而是我们试图预测的标签。在监督学习中，每一行是一个样本，我们可以用它来进行训练或测试。
- en: The number of features corresponds to the dimensionality of the data. Our machine
    learning approach depends on the number of dimensions versus the number of examples.
    For instance, text and image data are very high dimensional, while sensor data
    (such as temperature, pressure, or GPS) has relatively fewer dimensions.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 特征的数量对应于数据的维度。我们的机器学习方法依赖于维度的数量与样本数量之间的关系。例如，文本和图像数据是高维的，而传感器数据（如温度、压力或GPS）则维度相对较少。
- en: Fitting high-dimensional data is computationally expensive and prone to overfitting,
    due to the high complexity. Higher dimensions are also impossible to visualize,
    and therefore, we can’t use simple diagnostic methods.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合高维数据在计算上是昂贵的，并且容易出现过拟合，因为其复杂度较高。更高的维度也无法进行可视化，因此我们不能使用简单的诊断方法。
- en: Not all of the features are useful, and they may only add randomness to our
    results. Therefore, it’s often important to do good feature selection. **Feature
    selection** is the process of picking a subset of significant features for use
    in better model construction. In practice, not every feature in a dataset carries
    information useful for discriminating samples; some features are either redundant
    or irrelevant and, hence, can be discarded with little loss.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 并不是所有特征都是有用的，它们可能只是增加了结果的随机性。因此，通常需要进行良好的特征选择。**特征选择**是挑选出一个重要特征子集，以便更好地构建模型的过程。在实际应用中，并不是数据集中的每个特征都包含有助于区分样本的信息；一些特征要么是冗余的，要么是无关的，因此可以在损失较小的情况下丢弃。
- en: 'In principle, feature selection boils down to multiple binary decisions about
    whether to include a feature. For *n* features, we get *2*^n feature sets, which
    can be a very large number for a large number of features. For example, for 10
    features, we have 1,024 possible feature sets (for instance, if we’re deciding
    what clothes to wear, the features can be temperature, rain, the weather forecast,
    and where we’re going). Basically, we have two options: we either start with all
    of the features and remove features iteratively, or we start with a minimum set
    of features and add features iteratively. We then take the best feature sets for
    each iteration and compare them. At a certain point, brute-force evaluation becomes
    infeasible. Hence, more advanced feature selection algorithms were invented to
    distill the most useful features/signals. We will discuss in detail how to perform
    feature selection in *Chapter 4*, *Predicting Online Ad Click-Through with Logistic
    Regression*.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，特征选择归结为多次二进制决策，即是否包含某个特征。对于 *n* 个特征，我们可以得到 *2*^n 个特征集，对于特征数量较多时，这个数字可能非常大。例如，10个特征时，我们有1,024种可能的特征集（例如，如果我们在决定穿什么衣服，特征可以是温度、降雨、天气预报和我们要去的地方）。基本上，我们有两个选择：要么从所有特征开始，并逐步去除特征，要么从最小的特征集开始，并逐步添加特征。然后，我们将每次迭代中的最佳特征集进行比较。在某一时刻，暴力评估变得不可行。因此，发明了更先进的特征选择算法，用于提取最有用的特征/信号。我们将在*第4章*，*使用逻辑回归预测在线广告点击率*中详细讨论如何进行特征选择。
- en: Another common approach to reducing dimensionality is to transform high-dimensional
    data into lower-dimensional space. This is known as **dimensionality reduction**
    or **feature projection**. We will get into this in detail in *Chapter 7*, *Mining
    the 20 Newsgroups Dataset with Text Analysis Techniques*, where we will encode
    text data into two dimensions, and *Chapter 9*, *Recognizing Faces with Support
    Vector Machine*, where we will talk about projecting high-dimensional image data
    into low-dimensional space.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的降维方法是将高维数据转化为低维空间。这被称为**降维**或**特征投影**。我们将在*第7章*，*使用文本分析技术挖掘20个新闻组数据集*中详细讨论这一点，届时我们将把文本数据编码为二维空间；以及在*第9章*，*使用支持向量机识别面孔*中，我们将讨论如何将高维图像数据投影到低维空间。
- en: In this section, we talked about how the goal of machine learning is to find
    the optimal generalization to the data, and how to avoid ill-generalization. In
    the next two sections, we will explore tricks to get closer to the goal throughout
    individual phases of machine learning, including data preprocessing and feature
    engineering in the next section, and modeling in the section after that.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了机器学习的目标是找到数据的最佳泛化，并避免不良的泛化。在接下来的两节中，我们将探讨如何通过机器学习的各个阶段的技巧来接近这一目标，包括下一节中的数据预处理和特征工程，以及随后一节中的建模。
- en: Data preprocessing and feature engineering
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理和特征工程
- en: 'Data preprocessing and feature engineering play a crucial and foundational
    role in machine learning. It’s like laying the groundwork for a building – the
    stronger and better prepared the foundation, the better the final structure (machine
    learning model) will be. Here is a breakdown of their relationship:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理和特征工程在机器学习中起着至关重要的基础作用。这就像为一栋建筑奠定基础——基础越强大、准备得越充分，最终的结构（机器学习模型）就会越好。下面是它们关系的细分：
- en: '**Preprocessing prepares data for efficient learning**: Raw data from various
    sources often contains inconsistencies, errors, and irrelevant information. Preprocessing
    cleans, organizes, and transforms the data into a format suitable for the chosen
    machine learning algorithm. This allows the algorithm to understand the data more
    easily and efficiently, leading to better model performance.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理为高效学习准备数据**：来自各种来源的原始数据通常包含不一致、错误和无关信息。预处理通过清理、组织和转换数据，将其转化为适合所选机器学习算法的格式。这使得算法能够更轻松高效地理解数据，从而提高模型性能。'
- en: '**Preprocessing helps improve model accuracy and generalizability**: By handling
    missing values, outliers, and inconsistencies, preprocessing reduces noise in
    data. This enables a model to focus on the true patterns and relationships within
    the data, leading to more accurate predictions and better generalization on unseen
    data.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理有助于提高模型的准确性和泛化能力**：通过处理缺失值、异常值和不一致性，预处理减少了数据中的噪音。这使得模型能够专注于数据中的真实模式和关系，从而提高预测的准确性，并在未见过的数据上实现更好的泛化能力。'
- en: '**Feature engineering provides meaningful input variables**: Raw data is transformed
    and manipulated to create new features or select relevant ones. New features potentially
    improve model performance and insight generation.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征工程提供有意义的输入变量**：原始数据经过转换和处理，生成新的特征或选择相关特征。新特征可能会改善模型性能并产生有价值的洞见。'
- en: Overall, data preprocessing and feature engineering is an essential step in
    the machine learning workflow. By dedicating time and effort to proper preprocessing
    and feature engineering, you lay the foundation to build reliable, accurate, and
    generalizable machine learning models. We will cover the preprocessing phase first
    in this section.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，数据预处理和特征工程是机器学习工作流中至关重要的一步。通过投入时间和精力进行适当的预处理和特征工程，你为构建可靠、准确和具有广泛泛化能力的机器学习模型奠定了基础。在本节中，我们将首先讨论预处理阶段。
- en: Preprocessing and exploration
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理与探索
- en: When we learn, we require high-quality learning material. We can’t learn from
    gibberish, so we automatically ignore anything that doesn’t make sense. A machine
    learning system isn’t able to recognize gibberish, so we need to help it by cleaning
    the input data. It’s often claimed that cleaning the data forms a large part of
    machine learning. Sometimes, the cleaning is already done for us, but you shouldn’t
    count on it.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习过程中，我们需要高质量的学习材料。我们无法从胡言乱语中学习，因此会自动忽略那些不合理的内容。机器学习系统无法识别胡言乱语，因此我们需要通过清理输入数据来帮助它。人们常说，清理数据占机器学习的很大一部分。有时，数据清理工作已经为我们完成，但你不应依赖这种情况。
- en: To decide how to clean data, we need to be familiar with it. There are some
    projects that try to automatically explore the data and do something intelligent,
    such as producing a report. For now, unfortunately, we don’t have a solid solution
    in general, so you need to do some work.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 要决定如何清理数据，我们需要熟悉数据。有些项目尝试自动探索数据并做出一些智能的操作，比如生成报告。遗憾的是，目前我们还没有普遍的解决方案，因此你需要做一些工作。
- en: 'We can do two things, which aren’t mutually exclusive: first, scan the data,
    and second, visualize the data. This also depends on the type of data we’re dealing
    with—whether we have a grid of numbers, images, audio, text, or something else.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做两件事，它们并不是互相排斥的：首先，扫描数据，其次，可视化数据。这还取决于我们处理的数据类型——无论是数字网格、图像、音频、文本，还是其他什么类型。
- en: Ultimately, a grid of numbers is the most convenient form, and we will always
    work toward having numerical features. Let’s pretend that we have a table of numbers
    in the rest of this section.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，数字网格是最方便的形式，我们将始终致力于拥有数值特征。假设在本节的其余部分我们有一张数字表格。
- en: 'We want to know whether features have missing values, how the values are distributed,
    and what type of features we have. Values can approximately follow a normal distribution,
    a binomial distribution, a Poisson distribution, or another distribution altogether.
    Features can be binary: either yes or no, positive or negative, and so on. They
    can also be categorical: pertaining to a category, such as continents (Africa,
    Asia, Europe, South America, North America, and so on). Categorical variables
    can also be ordered, for instance, high, medium, and low. Features can also be
    quantitative, for example, the temperature in degrees or the price in dollars.
    Now, let’s dive into how we can cope with each of these situations.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想知道哪些特征有缺失值，缺失值如何分布，以及我们拥有哪些类型的特征。值大致可以遵循正态分布、二项分布、泊松分布，或其他分布。特征可以是二元的：要么是，是或否，正或负，等等。它们也可以是分类的：属于某个类别，例如大陆（非洲、亚洲、欧洲、南美洲、北美洲等）。分类变量也可以是有序的，例如高、中、低。特征也可以是定量的，例如温度（以度数表示）或价格（以美元表示）。现在，让我们深入探讨如何应对每种情况。
- en: Dealing with missing values
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: Quite often, we miss values for certain features. This could happen for various
    reasons. It can be inconvenient, expensive, or even impossible to always have
    a value. Maybe we weren’t able to measure a certain quantity in the past because
    we didn’t have the right equipment or just didn’t know that the feature was relevant.
    However, we’re stuck with missing values from the past.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 很多时候，我们会缺失某些特征的值。这可能由于各种原因发生。始终拥有一个值可能不方便、昂贵，甚至是不可能的。也许我们过去无法测量某个数量，因为我们没有合适的设备，或者根本没有意识到这个特征是相关的。然而，我们只能接受过去的缺失值。
- en: Sometimes, it’s easy to figure out that we’re missing values, and we can discover
    this just by scanning the data or counting the number of values we have for a
    feature and comparing this figure with the number of values we expect, based on
    the number of rows. Certain systems encode missing values with, for example, values
    such as 999,999 or -1\. This makes sense if the valid values are much smaller
    than 999,999\. If you’re lucky, you’ll have information about the features provided
    by whoever created the data in the form of a data dictionary or metadata.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们很容易就能发现缺失值，只需通过扫描数据或统计某个特征的值的数量，并将这个数量与根据行数预计的值的数量进行比较，就能发现缺失值。某些系统会用例如999,999或-1来编码缺失值。如果有效值远小于999,999，那么这种做法是合理的。如果幸运的话，你可能会有数据字典或元数据提供的信息，帮助你了解特征的详细情况。
- en: Once we know that we’re missing values, the question arises of how to deal with
    them. The simplest answer is to just ignore them. However, some algorithms can’t
    deal with missing values, and the program will just refuse to continue. In other
    circumstances, ignoring missing values will lead to inaccurate results. The second
    solution is to substitute missing values with a fixed value—this is called **imputing**.
    We can impute the arithmetic **mean**, **median**, or **mode** of the valid values
    of a certain feature. Ideally, we will have some prior knowledge of a variable
    that is somewhat reliable. For instance, we may know the seasonal averages of
    temperature for a certain location and be able to impute guesses for missing temperature
    values, given a date. We will talk about dealing with missing data in detail in
    *Chapter 10*, *Machine Learning Best Practices*. Similarly, techniques in the
    following sections will be discussed and employed in later chapters, just in case
    you feel uncertain about how they can be used.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道缺失值的存在，就会出现如何处理它们的问题。最简单的答案是忽略它们。然而，某些算法无法处理缺失值，程序会直接拒绝继续执行。在其他情况下，忽略缺失值会导致结果不准确。第二种解决方案是用固定值替代缺失值——这叫做**插补**。我们可以用某一特征有效值的算术**平均值**、**中位数**或**众数**来进行插补。理想情况下，我们会有一些相对可靠的变量的先验知识。例如，我们可能知道某个地点的季节性温度平均值，可以根据日期对缺失的温度值进行插补。我们将在*第10章*，*机器学习最佳实践*中详细讨论如何处理缺失数据。类似的，接下来几节的技术将在后续章节中讨论和应用，以防你对它们的使用方式感到不确定。
- en: Label encoding
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标签编码
- en: Humans are able to deal with various types of values. Machine learning algorithms
    (with some exceptions) require numerical values. If we offer a string such as
    `Ivan`, unless we’re using specialized software, the program won’t know what to
    do. In this example, we’re dealing with a categorical feature—names, probably.
    We can consider each unique value to be a label. (In this particular example,
    we also need to decide what to do with the case—is `Ivan` the same as `ivan`?).
    We can then replace each label with an integer—**label encoding**.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 人类能够处理各种类型的值。机器学习算法（有些例外）要求数值型的值。如果我们提供一个字符串，例如`Ivan`，除非我们使用专门的软件，否则程序不会知道如何处理。在这个例子中，我们处理的是一个类别特征——可能是名字。我们可以把每个独特值看作一个标签。（在这个特定例子中，我们还需要决定如何处理大小写——`Ivan`和`ivan`是否相同？）。然后我们可以用一个整数替换每个标签——**标签编码**。
- en: 'The following example shows how label encoding works:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了标签编码的工作原理：
- en: '| **Label** | **Encoded Label** |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| **标签** | **编码标签** |'
- en: '| Africa | 1 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 非洲 | 1 |'
- en: '| Asia | 2 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 亚洲 | 2 |'
- en: '| Europe | 3 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 欧洲 | 3 |'
- en: '| South America | 4 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 南美洲 | 4 |'
- en: '| North America | 5 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 北美洲 | 5 |'
- en: '| Other | 6 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | 6 |'
- en: 'Table 1.3: Example of label encoding'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.3：标签编码示例
- en: This approach can be problematic in some cases because the learner may conclude
    that there is an order (unless it is expected, for example, *bad=0*, *ok=1*, *good=2*,
    and *excellent=3*). In the preceding mapping table, `Asia` and `North America`
    in the preceding case differ by `4` after encoding, which is a bit counterintuitive,
    as it’s hard to quantify them. One-hot encoding in the next section takes an alternative
    approach.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在某些情况下可能会有问题，因为学习者可能会得出有顺序的结论（除非这是预期的，例如，*bad=0*，*ok=1*，*good=2*，和*excellent=3*）。在前面的映射表中，`Asia`和`North
    America`在编码后相差`4`，这有点不直观，因为很难量化它们。下一节的独热编码采用了不同的方法。
- en: One-hot encoding
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独热编码
- en: The **one-of-K**, or **one-hot encoding**, scheme uses dummy variables to encode
    categorical features. Originally, it was applied to digital circuits. The dummy
    variables have binary values such as bits, so they take the values zero or one
    (equivalent to true or false). For instance, if we want to encode continents,
    we will have dummy variables, such as `is_asia`, which will be true if the continent
    is `Asia` and false otherwise. In general, we need as many dummy variables as
    there are unique values minus one (or sometimes the exact number of unique values).
    We can determine one of the labels automatically from the dummy variables because
    they are exclusive.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**一对K**，或者说**独热编码**，方案使用虚拟变量来编码类别特征。最初，这一方法应用于数字电路。虚拟变量具有二进制值，如比特，因此它们取值为零或一（相当于真或假）。例如，如果我们想要编码大洲，我们将有虚拟变量，如`is_asia`，如果该大洲是`Asia`，则为真，否则为假。一般来说，我们需要的虚拟变量数量等于独特值的数量减去一（或者有时是独特值的确切数量）。我们可以从虚拟变量中自动确定一个标签，因为它们是互斥的。'
- en: 'If the dummy variables all have a false value, then the correct label is the
    label for which we don’t have a dummy variable. The following table illustrates
    the encoding for continents:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果虚拟变量的值都是假值，那么正确的标签就是没有虚拟变量的标签。下表展示了大陆的编码方式：
- en: '| **Continent** | **Is_africa** | **Is_asia** | **Is_europe** | **Is_sam**
    | **Is_nam** |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| **大陆** | **是否为非洲** | **是否为亚洲** | **是否为欧洲** | **是否为南美洲** | **是否为北美洲** |'
- en: '| Africa | 1 | 0 | 0 | 0 | 0 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 非洲 | 1 | 0 | 0 | 0 | 0 |'
- en: '| Asia | 0 | 1 | 0 | 0 | 0 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 亚洲 | 0 | 1 | 0 | 0 | 0 |'
- en: '| Europe | 0 | 0 | 1 | 0 | 0 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 欧洲 | 0 | 0 | 1 | 0 | 0 |'
- en: '| South America | 0 | 0 | 0 | 1 | 0 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 南美洲 | 0 | 0 | 0 | 1 | 0 |'
- en: '| North America | 0 | 0 | 0 | 0 | 1 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 北美 | 0 | 0 | 0 | 0 | 1 |'
- en: '| Other | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | 0 | 0 | 0 | 0 | 0 |'
- en: 'Table 1.4: Example of one-hot encoding'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '表1.4: 独热编码示例'
- en: The encoding produces a matrix (grid of numbers) with lots of zeros (false values)
    and occasional ones (true values). This type of matrix is called a **sparse matrix**.
    The sparse matrix representation is handled well by the `scipy` package, which
    we will discuss later in this chapter.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 编码会生成一个矩阵（数字网格），其中包含许多零（假值）和偶尔的1（真值）。这种类型的矩阵称为**稀疏矩阵**。稀疏矩阵表示由`scipy`包很好地处理，稍后我们将在本章中讨论它。
- en: Dense embedding
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 密集嵌入
- en: 'While one-hot encoding is a simple and sparse representation of categorical
    features, **dense embedding** provides a compact, continuous representation that
    captures semantic relationships based on the co-occurrence patterns in data. For
    example, using dense embedding, the continent categories might be represented
    by 3-dimensional continuous vectors like:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然独热编码是一种简单且稀疏的分类特征表示，**密集嵌入**提供了一个紧凑的、连续的表示，能够基于数据中的共现模式捕捉语义关系。例如，使用密集嵌入，可能将大陆类别表示为类似于以下的三维连续向量：
- en: 'Africa: [0.9, -0.2, 0.5]'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '非洲: [0.9, -0.2, 0.5]'
- en: 'Asia: [-0.1, 0.8, 0.6]'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '亚洲: [-0.1, 0.8, 0.6]'
- en: 'Europe: [0.6, 0.3, -0.7]'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '欧洲: [0.6, 0.3, -0.7]'
- en: 'South America: [0.5, 0.2, 0.1]'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '南美洲: [0.5, 0.2, 0.1]'
- en: 'North America: [0.4, 0.3, 0.2]'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '北美洲: [0.4, 0.3, 0.2]'
- en: 'Other: [-0.8, -0.5, 0.4]'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '其他: [-0.8, -0.5, 0.4]'
- en: In this example, you may notice the vectors of South America and North America
    are closer together than those of Africa and Asia. Dense embedding can capture
    the similarities between categories. In another example, you may see more closeness
    of the vectors of Europe and North America, based on cultural similarity.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，你可能会注意到南美和北美的向量比非洲和亚洲的向量更接近。密集嵌入能够捕捉类别之间的相似性。在另一个例子中，你可能会看到欧洲和北美的向量更接近，这基于文化上的相似性。
- en: We will explore dense embedding further in *Chapter 7*, *Mining the 20 Newsgroups
    Dataset with Text Analysis Techniques*.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*第7章*，*使用文本分析技术挖掘20个新闻组数据集*中进一步探讨密集嵌入。
- en: Scaling
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缩放
- en: Values of different features can differ by orders of magnitude. Sometimes, this
    can mean that the larger values dominate the smaller values. This depends on the
    algorithm we use. For certain algorithms to work properly, we’re required to scale
    data.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 不同特征的值可能相差几个数量级。有时，这意味着较大的值会主导较小的值。这取决于我们使用的算法。为了某些算法能够正常工作，我们需要对数据进行缩放。
- en: 'There are the following several common strategies that we can apply:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们可以应用的几种常见策略：
- en: Standardization removes the mean of a feature and divides it by the standard
    deviation. If the feature values are normally distributed, we will get a **Gaussian**,
    which is centered around zero with a variance of one.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化会移除特征的均值，并将其除以标准差。如果特征值是正态分布的，我们将得到一个**高斯分布**，它围绕零对称，方差为一。
- en: If the feature values aren’t normally distributed, we can remove the median
    and divide by the interquartile range. The **interquartile range** is the range
    between the first and third quartile (or 25^(th) and 75^(th) percentile).
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果特征值不是正态分布的，我们可以去除中位数并除以四分位数范围。**四分位数范围**是指第一四分位数和第三四分位数之间的范围（或第25^(th)和第75^(th)百分位数）。
- en: A range between zero and one is a common choice of range for feature scaling.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零到一之间的范围是特征缩放中常见的范围选择。
- en: We will use scaling in many projects throughout the book.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中的许多项目中，我们将使用缩放。
- en: An advanced version of data preprocessing is usually called feature engineering.
    We will cover that next.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理的高级版本通常称为特征工程。我们将在接下来的部分讨论这一点。
- en: Feature engineering
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程
- en: '**Feature engineering** is the process of creating or improving features. Features
    are often created based on common sense, domain knowledge, or prior experience.
    There are certain common techniques for feature creation; however, there is no
    guarantee that creating new features will improve your results. We are sometimes
    able to use the clusters found by unsupervised learning as extra features. **Deep
    neural networks** are often able to derive features automatically.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征工程**是创建或改进特征的过程。特征通常是基于常识、领域知识或先前的经验来创建的。特征创建有一些常见的技术；然而，并不能保证创建新特征会改善你的结果。有时，我们可以使用无监督学习找到的簇作为额外的特征。**深度神经网络**通常能够自动推导出特征。'
- en: 'We will briefly look at some feature engineering techniques: polynomial transformation
    and binning.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要介绍一些特征工程技术：多项式转换和分箱。
- en: Polynomial transformation
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多项式转换
- en: If we have two features, *a* and *b*, we can suspect that there is a polynomial
    relationship, such as *a*² + *ab* + *b*². We can consider a new feature an **interaction**
    between *a* and *b*, such as the product *ab*. An interaction doesn’t have to
    be a product—although this is the most common choice—it can also be a sum, a difference,
    or a ratio. If we use a ratio to avoid dividing by zero, we should add a small
    constant to the divisor and dividend.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有两个特征，*a*和*b*，我们可能会怀疑它们之间存在多项式关系，例如 *a*² + *ab* + *b*²。我们可以将一个新特征视为 *a*
    和 *b* 的**交互**，例如它们的乘积 *ab*。交互不一定是乘积——虽然这是最常见的选择——它也可以是和、差或比率。如果我们使用比率来避免除以零，我们应该在除数和被除数上都加上一个小常数。
- en: The number of features and the order of the polynomial for a polynomial relationship
    aren’t limited. However, if we follow the Occam’s razor principle, we should avoid
    higher-order polynomials and interactions of many features. In practice, complex
    polynomial relations tend to be more difficult to compute and tend to overfit,
    but if you really need better results, they may be worth considering. We will
    see polynomial transformation in action in *Best practice 12* – *performing feature
    engineering without domain expertise* section in *Chapter 10*, *Machine Learning
    Best Practices*.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式关系中的特征数量和多项式的阶数没有限制。然而，如果我们遵循奥卡姆剃刀原理，我们应该避免使用高阶多项式和多个特征的交互。在实际应用中，复杂的多项式关系往往更难计算，并且容易过拟合，但如果你确实需要更好的结果，它们可能值得考虑。我们将在*第10章《机器学习最佳实践》*中的*最佳实践12*——*在没有领域知识的情况下进行特征工程*部分中看到多项式转换的应用。
- en: Binning
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分箱
- en: Sometimes, it’s useful to separate feature values into several bins. For example,
    we may only be interested in whether it rained on a particular day. Given the
    precipitation values, we can binarize the values so that we get a true value if
    the precipitation value isn’t zero, and a false value otherwise. We can also use
    statistics to divide values into high, low, and medium bins. In marketing, we
    often care more about the age group, such as 18 to 24, than a specific age, such
    as 23.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，将特征值分到几个箱子里是有用的。例如，我们可能只关心某一天是否下雨。根据降水值，我们可以将值二值化，如果降水值不为零，则为真值，否则为假值。我们也可以使用统计方法将值分为高、中、低三个箱子。在营销中，我们通常更关心年龄段，比如18至24岁，而不是具体的年龄，比如23岁。
- en: The binning process inevitably leads to a loss of information. However, depending
    on your goals, this may not be an issue, actually reducing the chance of overfitting.
    Certainly, there will be improvements in speed and a reduction of memory or storage
    requirements and redundancy.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 分箱过程不可避免地会导致信息的丢失。然而，根据你的目标，这可能不是问题，实际上还可能减少过拟合的机会。当然，这样做会提高速度，并减少内存或存储需求及冗余。
- en: 'Any real-world machine learning system should have two modules: a data preprocessing
    module, which we just covered in this section, and a modeling module, which will
    be covered next.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 任何现实世界中的机器学习系统都应该有两个模块：一个数据预处理模块，我们在本节中已经覆盖，另一个是建模模块，将在下一节中介绍。
- en: Combining models
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型组合
- en: A model takes in data (usually preprocessed) and produces predictive results.
    What if we employ multiple models? Will we make better decisions by combining
    predictions from individual models? We will talk about this in this section.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型接受数据（通常是预处理过的数据）并产生预测结果。如果我们使用多个模型呢？通过结合各个模型的预测结果，我们能做出更好的决策吗？我们将在本节讨论这一点。
- en: Let’s start with an analogy. In high school, we sit together with other students
    and learn together, but we aren’t supposed to work together during the exam. The
    reason is, of course, that teachers want to know what we’ve learned, and if we
    just copy exam answers from friends, we may not have learned anything. Later in
    life, we discover that teamwork is important. For example, this book is the product
    of a whole team, or possibly a group of teams.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个类比开始。在高中时，我们会和其他同学一起坐在一起学习，但考试时我们不应该一起合作。原因当然是，老师想知道我们学到了什么，如果我们只是从朋友那里抄答案，我们可能什么也没学到。后来，在生活中我们发现团队合作很重要。例如，这本书是整个团队的成果，或者可能是多个团队的成果。
- en: 'Clearly, a team can produce better results than a single person. However, this
    goes against Occam’s razor, since a single person can come up with simpler theories
    compared to what a team will produce. In machine learning, we nevertheless prefer
    to have our models cooperate with the following model combination schemes:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，一个团队可以比单个个体产生更好的结果。然而，这与奥卡姆剃刀原理相悖，因为单个个体相比团队可以提出更简单的理论。在机器学习中，我们仍然倾向于让模型通过以下模型组合方案进行合作：
- en: Voting and averaging
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 投票和平均
- en: Bagging
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagging
- en: Boosting
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升
- en: Stacking
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠
- en: Let’s dive into each of them now.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来深入探讨它们。
- en: Voting and averaging
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 投票和平均
- en: This is probably the most understandable type of model aggregation. It just
    means the final output will be the **majority** or **average** of prediction output
    values from multiple models. It is also possible to assign different weights to
    individual models in the ensemble; for example, some models that are more reliable
    might be given two votes.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是最容易理解的模型集成类型。它只是意味着最终的输出将是多个模型预测输出值的**多数**或**平均值**。也可以为集成中的各个模型分配不同的权重；例如，某些更可靠的模型可能会被赋予两个投票权。
- en: Nonetheless, combining the results of models that are highly correlated to each
    other doesn’t guarantee a spectacular improvement. It is better to somehow diversify
    the models by using different features or different algorithms. If you find two
    models are strongly correlated, you may, for example, decide to remove one of
    them from the ensemble and increase proportionally the weight of the other model.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，结合彼此高度相关的模型的结果并不能保证显著的改进。最好通过使用不同的特征或不同的算法来某种程度上多样化模型。如果你发现两个模型高度相关，例如，你可以决定从集成中移除一个模型，并按比例增加另一个模型的权重。
- en: Bagging
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bagging
- en: '**Bootstrap aggregating**, or **bagging**, is an algorithm introduced by Leo
    Breiman, a distinguished statistician at the University of California, Berkeley,
    in 1994, which applies **bootstrapping** to machine learning problems. Bootstrapping
    is a statistical procedure that creates multiple datasets from an existing one
    by sampling data with replacement. Bootstrapping can be used to measure the properties
    of a model, such as bias and variance.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '**自助聚合**（**Bootstrap aggregating**，简称**bagging**）是由著名统计学家、加州大学伯克利分校的Leo Breiman于1994年提出的算法，它将**自助法**应用于机器学习问题。自助法是一种统计程序，通过有放回地抽样数据，从现有数据集中创建多个数据集。自助法可以用来衡量模型的属性，例如偏差和方差。'
- en: 'In general, a bagging algorithm follows these steps:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，bagging算法遵循以下步骤：
- en: We generate new training sets from input training data by sampling with replacement.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过有放回地抽样生成新的训练集。
- en: For each generated training set, we fit a new model.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个生成的训练集，我们拟合一个新的模型。
- en: We combine the results of the models by averaging or majority voting.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过平均或多数投票来结合模型的结果。
- en: 'The following diagram illustrates the steps for bagging, using classification
    as an example (the circles and crosses represent samples from two classes):'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了使用分类作为示例的bagging步骤（圆圈和叉号代表来自两个类别的样本）：
- en: '![](img/B21047_01_11.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_01_11.png)'
- en: 'Figure 1.11: Workflow of bagging for classification'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.11：用于分类的bagging工作流
- en: As you can imagine, bagging can reduce the chance of overfitting.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所想的，bagging可以减少过拟合的可能性。
- en: We will study bagging in depth in *Chapter 3*, *Predicting Online Ad Click-Through
    with Tree-Based Algorithms*.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*第三章*中深入研究bagging，*使用基于树的算法预测在线广告点击率*。
- en: Boosting
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升
- en: In the context of supervised learning, we define **weak learners** as learners
    who are just a little better than a baseline, such as randomly assigning classes
    or average values. Much like ants, weak learners are weak individually, but together,
    they have the power to do amazing things.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习的背景下，我们将**弱学习器**定义为比基线稍好一点的学习器，例如随机分配类别或平均值。就像蚂蚁一样，弱学习器个体很弱，但它们组合在一起，能做出令人惊讶的事情。
- en: It makes sense to take into account the strength of each individual learner
    using weights. This general idea is called **boosting**. In boosting, all models
    are trained in sequence, instead of in parallel as in bagging. Each model is trained
    on the same dataset, but each data sample has a different weight, factoring in
    the previous model’s success. The weights are reassigned after a model is trained,
    which will be used for the next training round. In general, weights for mispredicted
    samples are increased to stress their prediction difficulty.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到每个单独学习者的强度并使用权重是有意义的。这个总体思想叫做**提升**。在提升过程中，所有模型是按顺序训练的，而不是像集成方法（bagging）那样并行训练。每个模型都在相同的数据集上训练，但每个数据样本的权重不同，会考虑到前一个模型的成功。模型训练完成后，权重会被重新分配，用于下一轮训练。通常，对于预测错误的样本，会增加其权重，以加强对这些样本的预测难度。
- en: 'The following diagram illustrates the steps for boosting, again using classification
    as an example (the circles and crosses represent samples from two classes, and
    the size of a circle or cross indicates the weight assigned to it):'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了提升过程的步骤，仍以分类为例（圆圈和叉号代表来自两个类别的样本，圆圈或叉号的大小表示分配给它的权重）：
- en: '![A screenshot of a device  Description automatically generated with low confidence](img/B21047_01_12.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![设备截图 描述自动生成，信心较低](img/B21047_01_12.png)'
- en: 'Figure 1.12: Workflow of boosting for classification'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.12：分类任务中的提升工作流程
- en: There are many boosting algorithms; boosting algorithms differ mostly in their
    weighting scheme. If you’ve studied for an exam, you may have applied a similar
    technique by identifying the type of practice questions you had trouble with and
    focusing on the hard problems.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多种提升算法；这些提升算法的主要区别在于它们的加权方案。如果你曾为考试学习过，可能已经应用了类似的技巧，通过识别自己在练习题中遇到困难的类型，并集中精力攻克难题。
- en: Viola-Jones, a popular face detection framework, leverages the boosting algorithm
    to efficiently identify faces in images. Detecting faces in images or videos is
    supervised learning. We give the learner examples of regions containing faces.
    There’s an imbalance, since we usually have far more regions that don’t have faces
    than those that do (about 10,000 times more).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Viola-Jones是一个流行的人脸检测框架，它利用提升算法高效地识别图像中的人脸。在图像或视频中检测人脸属于监督学习。我们给学习者提供包含人脸区域的示例。这里存在不平衡问题，因为通常没有人脸的区域要远多于有脸的区域（大约多1万倍）。
- en: A cascade of classifiers progressively filters out these negative image areas
    stage by stage. In each progressive stage, the classifiers use progressively more
    features on fewer image windows. The idea is to spend the majority of time on
    image patches that contain faces. In this context, boosting is used to select
    features and combine results.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列分类器逐步筛选出这些负面图像区域。每个阶段，分类器使用越来越多的特征，并在较少的图像窗口上进行处理。其思想是将大部分时间花费在包含人脸的图像区域上。在这种情况下，使用提升（boosting）来选择特征并结合结果。
- en: Stacking
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠
- en: '**Stacking** takes the output values of machine learning models and then uses
    them as input values for another algorithm. You can, of course, feed the output
    of the higher-level algorithm to another predictor. It’s possible to use any arbitrary
    topology, but for practical reasons, you should try a simple setup first, as also
    dictated by Occam’s razor.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '**堆叠（Stacking）**方法是将机器学习模型的输出值作为另一个算法的输入值。你当然可以将更高层次算法的输出再作为另一个预测器的输入。你可以使用任何任意拓扑结构，但出于实际原因，你应该先尝试简单的设置，这也符合奥卡姆剃刀原则。'
- en: A fun fact is that stacking is commonly used in the winning models in the Kaggle
    competition. For instance, the first place for the Otto Group Product Classification
    Challenge ([www.kaggle.com/c/otto-group-product-classification-challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge))
    went to a stacking model composed of more than 30 different models.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的事实是，堆叠常被用于Kaggle比赛中的获胜模型。例如，奥托集团产品分类挑战赛的第一名（[www.kaggle.com/c/otto-group-product-classification-challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge)）就是由一个包含30多种不同模型的堆叠模型获得的。
- en: So far, we have covered the tricks required to more easily reach the right generalization
    for a machine learning model throughout the data preprocessing and modeling phase.
    I know you can’t wait to start working on a machine learning project. Let’s get
    ready by setting up the working environment.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了在数据预处理和建模阶段更容易达到机器学习模型正确泛化所需的一些技巧。我知道你迫不及待想开始一个机器学习项目。让我们通过设置工作环境来做好准备。
- en: Installing software and setting up
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装软件和设置
- en: As the book title says, Python is the language we will use to implement all
    machine learning algorithms and techniques throughout the entire book. We will
    also exploit many popular Python packages and tools, such as NumPy, SciPy, scikit-learn,
    TensorFlow, and PyTorch. By the end of this initial chapter, make sure you have
    set up the tools and working environment properly, even if you are already an
    expert in Python or familiar with some of the aforementioned tools.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 正如书名所示，Python 是我们在整本书中实现所有机器学习算法和技术的编程语言。我们还将使用许多流行的 Python 包和工具，如 NumPy、SciPy、scikit-learn、TensorFlow
    和 PyTorch。在本章结束时，确保你已经正确设置了工具和工作环境，即使你已经是 Python 专家或对上述一些工具非常熟悉。
- en: Setting up Python and environments
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 Python 和环境
- en: We will use Python 3 in this book. The Anaconda Python 3 distribution is one
    of the best options for data science and machine learning practitioners.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们将使用 Python 3。Anaconda 的 Python 3 发行版是数据科学和机器学习从业者的最佳选择之一。
- en: '**Anaconda** is a free Python distribution for data analysis and scientific
    computing. It has its own package manager, `conda`. The distribution ([https://docs.anaconda.com/free/anaconda/](https://docs.anaconda.com/free/anaconda/),
    depending on your OS, or Python version 3.7 to 3.11) includes around 700 Python
    packages (as of 2023), which makes it very convenient. For casual users, the **Miniconda**
    ([https://conda.io/miniconda.html](https://conda.io/miniconda.html)) distribution
    may be the better choice. Miniconda contains the `conda` package manager and Python.
    Obviously, Miniconda takes up much less disk space than Anaconda.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**Anaconda** 是一个免费的 Python 发行版，专为数据分析和科学计算设计。它有自己的包管理器 `conda`。该发行版（[https://docs.anaconda.com/free/anaconda/](https://docs.anaconda.com/free/anaconda/)，根据你的操作系统或
    Python 版本 3.7 到 3.11）包含大约 700 个 Python 包（截至 2023 年），使其非常方便。对于普通用户来说，**Miniconda**（[https://conda.io/miniconda.html](https://conda.io/miniconda.html)）发行版可能是更好的选择。Miniconda
    包含了 `conda` 包管理器和 Python。显然，Miniconda 占用的磁盘空间比 Anaconda 要小得多。'
- en: 'The procedures to install Anaconda and Miniconda are similar. You can follow
    the instructions from [https://docs.conda.io/projects/conda/en/latest/user-guide/install/](https://docs.conda.io/projects/conda/en/latest/user-guide/install/).
    First, you must download the appropriate installer for your OS and Python version,
    as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Anaconda 和 Miniconda 的过程是相似的。你可以按照[https://docs.conda.io/projects/conda/en/latest/user-guide/install/](https://docs.conda.io/projects/conda/en/latest/user-guide/install/)上的说明进行操作。首先，你需要根据你的操作系统和
    Python 版本下载合适的安装程序，如下所示：
- en: '![A picture containing text, screenshot, font  Description automatically generated](img/B21047_01_13.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text, screenshot, font  Description automatically generated](img/B21047_01_13.png)'
- en: 'Figure 1.13: Installation entry based on your OS'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13：根据你的操作系统选择的安装入口
- en: Follow the steps listed in your OS. You can choose between a GUI and a CLI.
    I personally find the latter easier.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 按照你的操作系统列出的步骤操作。你可以选择图形用户界面（GUI）或命令行界面（CLI）。我个人认为后者更容易。
- en: Anaconda comes with its own Python installation. On my machine, the Anaconda
    installer created an `anaconda` directory in my home directory and required about
    900 MB. Similarly, the `Miniconda` installer installs a `miniconda` directory
    in your home directory.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: Anaconda 附带了自己的 Python 安装。在我的机器上，Anaconda 安装程序在我的主目录中创建了一个 `anaconda` 目录，并大约需要
    900 MB 的空间。类似地，`Miniconda` 安装程序会在你的主目录中安装一个 `miniconda` 目录。
- en: 'Feel free to play around with it after you set it up. One way to verify that
    you have set up Anaconda properly is by entering the following command line in
    your terminal on Linux/Mac or Command Prompt on Windows (from now on, we will
    just mention Terminal):'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完毕后，随时可以进行尝试。验证是否正确设置 Anaconda 的一种方法是，在 Linux/Mac 的终端或 Windows 的命令提示符中输入以下命令（从现在开始，我们只提到终端）：
- en: '[PRE0]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding command line will display your Python running environment, as
    shown in the following screenshot:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令行将显示你的 Python 运行环境，如下图所示：
- en: '![](img/B21047_01_14.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_01_14.png)'
- en: 'Figure 1.14: Screenshot after running “python” in the terminal'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.14：在终端中运行“python”后的截图
- en: If you don’t see this, please check the system path or the path Python is running
    from.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有看到此信息，请检查系统路径或 Python 的运行路径。
- en: 'To wrap up this section, I want to emphasize the reasons why Python is the
    most popular language for machine learning and data science. First of all, Python
    is famous for its high readability and simplicity, which makes it easy to build
    machine learning models. We spend less time worrying about getting the right syntax
    and compilation and, as a result, have more time to find the right machine learning
    solution. Second, we have an extensive selection of Python libraries and frameworks
    for machine learning:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 总结这一部分，我想强调为什么 Python 是机器学习和数据科学中最受欢迎的语言。首先，Python 因其高可读性和简洁性而闻名，使得构建机器学习模型变得更加容易。我们可以花更少的时间去担心正确的语法和编译，从而有更多的时间去寻找合适的机器学习解决方案。其次，我们拥有大量的
    Python 库和框架来支持机器学习：
- en: '| **Tasks** | **Python libraries** |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| **任务** | **Python 库** |'
- en: '| **Data analysis** | NumPy, SciPy, and pandas |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| **数据分析** | NumPy、SciPy 和 pandas |'
- en: '| **Data visualization** | Matplotlib, and Seaborn |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| **数据可视化** | Matplotlib 和 Seaborn |'
- en: '| **Modeling** | scikit-learn, TensorFlow, Keras, and PyTorch |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| **建模** | scikit-learn、TensorFlow、Keras 和 PyTorch |'
- en: 'Table 1.5: Popular Python libraries for machine learning'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.5：机器学习中常用的 Python 库
- en: The next step involves setting up some of the packages that we will use throughout
    this book.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是设置一些本书中将要使用的包。
- en: Installing the main Python packages
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装主要的 Python 包
- en: For most projects in this book, we will use NumPy ([http://www.numpy.org/](http://www.numpy.org/)),
    SciPy ([https://scipy.org/](https://scipy.org/)), the `pandas` library ([https://pandas.pydata.org/](https://pandas.pydata.org/)),
    scikit-learn ([http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)),
    TensorFlow ([https://www.tensorflow.org/](https://www.tensorflow.org/)), and PyTorch
    ([https://pytorch.org/](https://pytorch.org/)).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的大多数项目中，我们将使用 NumPy ([http://www.numpy.org/](http://www.numpy.org/))、SciPy
    ([https://scipy.org/](https://scipy.org/))、`pandas` 库 ([https://pandas.pydata.org/](https://pandas.pydata.org/))、scikit-learn
    ([http://scikit-learn.org/stable/](http://scikit-learn.org/stable/))、TensorFlow
    ([https://www.tensorflow.org/](https://www.tensorflow.org/)) 和 PyTorch ([https://pytorch.org/](https://pytorch.org/))。
- en: In the sections that follow, we will cover the installation of several Python
    packages that we will mainly use in this book.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍几种本书中主要使用的 Python 包的安装方法。
- en: 'Conda environments provide a way to isolate dependencies and packages for different
    projects. So it is recommended to create and use an environment for a new project.
    Let’s create one using the following command to create an environment called “`pyml`":'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: Conda 环境提供了一种为不同项目隔离依赖项和包的方法。因此，建议为新项目创建并使用一个环境。我们可以使用以下命令创建一个名为“`pyml`”的环境：
- en: '[PRE1]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we also specify the Python version, `3.10`, which is optional but highly
    recommended. This is to avoid using the latest Python version by default, which
    may not be compatible with many Python packages. For example, at the time of writing
    (late 2023), PyTorch does not support Python `3.11`.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们还指定了 Python 版本 `3.10`，虽然这是可选的，但强烈推荐使用。这样做是为了避免默认使用最新版本的 Python，因为它可能与许多
    Python 包不兼容。例如，在编写本文时（2023 年底），PyTorch 不支持 Python `3.11`。
- en: 'To activate the newly created environment, we use the following command:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 为了激活新创建的环境，我们使用以下命令：
- en: '[PRE2]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The activated environment is displayed in front of your prompt like this:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 激活的环境会显示在提示符前，如下所示：
- en: '[PRE3]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: NumPy
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NumPy
- en: 'NumPy is the fundamental package for machine learning with Python. It offers
    powerful tools including the following:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 是使用 Python 进行机器学习的基础包。它提供了强大的工具，包括以下内容：
- en: The *N*-dimensional array (`ndarray`) class and several subclasses representing
    matrices and arrays
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N*维数组（`ndarray`）类及其多个子类，代表矩阵和数组'
- en: Various sophisticated array functions
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种复杂的数组函数
- en: Useful linear algebra capabilities
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有用的线性代数功能
- en: 'Installation instructions for NumPy can be found at [https://numpy.org/install/](https://numpy.org/install/).
    Alternatively, an easier method involves installing it with `conda` or `pip` in
    the command line, as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 的安装说明可以在 [https://numpy.org/install/](https://numpy.org/install/) 找到。或者，您也可以通过
    `conda` 或 `pip` 在命令行中安装，具体如下：
- en: '[PRE4]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: or
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '[PRE5]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A quick way to verify your installation is to import it in Python, as follows:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 验证安装的快捷方式是按照如下方式在 Python 中导入：
- en: '[PRE6]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It is installed correctly if no error message is visible.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有出现错误信息，则表示安装成功。
- en: SciPy
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SciPy
- en: 'In machine learning, we mainly use NumPy arrays to store data vectors or matrices
    composed of feature vectors. SciPy ([https://scipy.org/](https://scipy.org/))
    uses NumPy arrays and offers a variety of scientific and mathematical functions.
    Installing SciPy in the terminal is similar, again as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们主要使用 NumPy 数组来存储由特征向量组成的数据向量或矩阵。SciPy ([https://scipy.org/](https://scipy.org/))
    使用 NumPy 数组，并提供各种科学和数学函数。在终端中安装 SciPy 与以下方式类似：
- en: '[PRE7]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: or
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '[PRE8]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: pandas
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: pandas
- en: 'We also use the `pandas` library ([https://pandas.pydata.org/](https://pandas.pydata.org/))
    for data wrangling later in this book. The best way to get `pandas` is via `pip`
    or `conda`, for example:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用 `pandas` 库 ([https://pandas.pydata.org/](https://pandas.pydata.org/))
    在本书后面进行数据整理。获取 `pandas` 的最佳方法是通过 `pip` 或 `conda`，例如：
- en: '[PRE9]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: scikit-learn
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: scikit-learn
- en: 'The `scikit-learn` library is a Python machine learning package optimized for
    performance, as a lot of its code runs almost as fast as equivalent C code. The
    same statement is true for NumPy and SciPy. `scikit-learn` requires both NumPy
    and SciPy to be installed. As the installation guide in [http://scikit-learn.org/stable/install.html](http://scikit-learn.org/stable/install.html)
    states, the easiest way to install `scikit-learn` is to use `pip` or `conda`,
    as follows:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn` 库是一个优化性能的 Python 机器学习包，其大部分代码运行速度几乎与等效的 C 代码一样快。NumPy 和 SciPy
    也是如此。`scikit-learn` 需要安装 NumPy 和 SciPy。如 [http://scikit-learn.org/stable/install.html](http://scikit-learn.org/stable/install.html)
    中的安装指南所述，安装 `scikit-learn` 的最简单方法是使用 `pip` 或 `conda`，如下所示：'
- en: '[PRE10]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: or
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '[PRE11]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, we use the “`-c conda-forge`" option to tell `conda` to search for packages
    in the `conda-forge` channel, which is a community-driven channel with a wide
    range of open-source packages.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们使用“`-c conda-forge`”选项告诉 `conda` 在 `conda-forge` 渠道中搜索软件包，这是一个由社区驱动的渠道，提供广泛的开源软件包。
- en: TensorFlow
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow
- en: TensorFlow is a Python-friendly open-source library invented by the Google Brain
    team for high-performance numerical computation. It makes machine learning faster
    and deep learning easier, with the Python-based convenient frontend API and high-performance
    C++-based backend execution. TensorFlow 2 was largely a redesign of its first
    mature version, 1.0, and was released at the end of 2019.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是由 Google Brain 团队发明的 Python 友好的开源库，用于高性能数值计算。它使机器学习更快速，深度学习更容易，具有基于
    Python 的便捷前端 API 和基于高性能 C++ 的后端执行。TensorFlow 2 在其首个成熟版本 1.0 的基础上进行了大规模重设计，并于 2019
    年底发布。
- en: TensorFlow has been widely known for its deep learning modules. However, its
    most powerful point is **computation graphs**, which algorithms are built on.
    Basically, a computation graph is used to convey relationships between the input
    and the output via tensors.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 因其深度学习模块而广为人知。然而，其最强大之处在于**计算图**，其算法基于此构建。基本上，计算图用于通过张量传达输入与输出之间的关系。
- en: 'For instance, if we want to evaluate a linear relationship, *y = 3 * a + 2
    * b*, we can represent it in the following computation graph:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想评估线性关系 *y = 3 * a + 2 * b*，我们可以在以下计算图中表示它：
- en: '![A picture containing screenshot, circle, diagram, sketch  Description automatically
    generated](img/B21047_01_15.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![包含屏幕截图、圆圈、图表、草图的图片 自动生成描述](img/B21047_01_15.png)'
- en: 'Figure 1.15: Computation graph for a y = 3 * a + 2 * b machine'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.15：y = 3 * a + 2 * b 机器的计算图
- en: Here, *a* and *b* are the input tensors, *c* and *d* are the intermediate tensors,
    and y is the output.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*a* 和 *b* 是输入张量，*c* 和 *d* 是中间张量，y 是输出。
- en: You can think of a computation graph as a network of nodes connected by edges.
    Each node is a tensor, and each edge is an operation or function that takes its
    input node and returns a value to its output node. To train a machine learning
    model, TensorFlow builds the computation graph and computes the **gradients**
    accordingly (gradients are vectors that provide the steepest direction where an
    optimal solution is reached). In the upcoming chapters, you will see some examples
    of training machine learning models using `TensorFlow`.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将计算图视为由边缘连接的节点网络。每个节点是一个张量，每条边缘是一个操作或函数，它接受其输入节点并将值返回给其输出节点。为了训练机器学习模型，TensorFlow
    构建计算图并相应地计算**梯度**（梯度是向量，提供达到最优解的最陡峭方向）。在接下来的章节中，您将看到使用 `TensorFlow` 训练机器学习模型的示例。
- en: We highly recommend you go through [https://www.tensorflow.org/guide/data](https://www.tensorflow.org/guide/data)
    if you are interested in exploring more about TensorFlow and computation graphs.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣深入了解 TensorFlow 和计算图，我们强烈建议你访问 [https://www.tensorflow.org/guide/data](https://www.tensorflow.org/guide/data)。
- en: 'TensorFlow allows easy deployment of computation across CPUs and GPUs, which
    empowers expensive and large-scale machine learning. In this book, we will focus
    on the CPU as our computation platform. Hence, according to [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/),
    installing TensorFlow 2 is done via the following command line:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 允许在 CPU 和 GPU 之间轻松部署计算，这使得大规模的高成本机器学习成为可能。在本书中，我们将重点使用 CPU 作为计算平台。因此，按照
    [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/) 的说明，安装
    TensorFlow 2 的命令行如下：
- en: '[PRE12]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: or
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '[PRE13]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You can always verify the installation by importing it in Python.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在 Python 中导入 PyTorch 来验证安装是否成功。
- en: PyTorch
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch
- en: PyTorch is an open-source machine learning library primarily used to develop
    deep learning models. It provides a flexible and efficient framework to build
    neural networks and perform computations on GPUs. PyTorch was developed by Facebook’s
    AI Research lab and is widely used in both research and industry.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是一个开源机器学习库，主要用于开发深度学习模型。它提供了一个灵活高效的框架来构建神经网络并在 GPU 上执行计算。PyTorch 由 Facebook
    的 AI 研究实验室开发，并在研究和工业界广泛使用。
- en: Similar to TensorFlow, PyTorch performs its computations based on a **directed
    acyclic graph** (**DAG**). The difference is that PyTorch utilizes a **dynamic
    computational graph**, which allows for on-the-fly graph construction during runtime,
    while TensorFlow uses a **static** computational graph, where the graph structure
    is defined upfront and then executed. This dynamic nature enables greater flexibility
    in model design and easier debugging, and also facilitates dynamic control flow,
    making it suitable for a wide range of applications.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 与 TensorFlow 类似，PyTorch 的计算基于 **有向无环图**（**DAG**）。不同之处在于，PyTorch 使用 **动态计算图**，允许在运行时即时构建计算图，而
    TensorFlow 使用 **静态** 计算图，计算图结构在执行前已定义并执行。这种动态特性使得模型设计更加灵活，调试更加简便，也便于动态控制流，因此适用于广泛的应用。
- en: PyTorch has become a popular choice among researchers and practitioners in the
    field of deep learning, due to its flexibility, ease of use, and efficient computational
    capabilities. Its intuitive interface and strong community support make it a powerful
    tool for various applications, including computer vision, natural language processing,
    reinforcement learning, and more.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其灵活性、易用性和高效的计算能力，PyTorch 已成为深度学习领域研究人员和从业人员的热门选择。其直观的界面和强大的社区支持使其成为多种应用的有力工具，包括计算机视觉、自然语言处理、强化学习等。
- en: To install PyTorch, it is recommended to look up the command in the latest instructions
    on [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/),
    based on the system and method.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 PyTorch，建议根据系统和方法查阅 [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)
    上的最新安装指令。
- en: 'As an example, we install the latest stable version (`2.2.0` as of late 2023)
    via `conda` on a Mac using the following command:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们通过 `conda` 在 Mac 上安装最新稳定版本（截至 2023 年底为 `2.2.0`），使用以下命令：
- en: '[PRE14]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Best practice**'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'If you encounter issues in installation, please read more about the platform
    and package-specific recommendations provided on the instructions page. All PyTorch
    code in this book can be run on your CPU, unless specifically indicated for a
    GPU only. However, using a GPU is recommended if you want to expedite training
    neural network models and fully enjoy the benefits of PyTorch. If you have a graphics
    card, refer to the instructions and set up PyTorch with the appropriate compute
    platform. For example, I install it on Windows with a GPU using the following
    command:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在安装过程中遇到问题，请阅读说明页面上提供的平台和软件包特定的建议。本书中的所有 PyTorch 代码都可以在 CPU 上运行，除非特别指出仅适用于
    GPU。不过，如果你希望加快神经网络模型的训练并充分享受 PyTorch 的优势，建议使用 GPU。如果你有显卡，请参考安装说明并设置适当的计算平台。例如，我在
    Windows 上使用 GPU 安装时使用以下命令：
- en: '[PRE15]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To check if PyTorch with GPU support is installed correctly, run the following
    Python code:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查是否正确安装了带有 GPU 支持的 PyTorch，可以运行以下 Python 代码：
- en: '[PRE16]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Alternatively, you can use Google Colab ([https://colab.research.google.com/](https://colab.research.google.com/))
    to train some neural network models using GPUs for free.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你可以使用Google Colab ([https://colab.research.google.com/](https://colab.research.google.com/))，免费使用GPU训练一些神经网络模型。
- en: There are many other packages we will use intensively, for example, **Matplotlib**
    for plotting and visualization, **Seaborn** for visualization, **NLTK** for natural
    language processing tasks, **transformers** for state-of-the-art models pretrained
    on large datasets, and **OpenAI Gym** for reinforcement learning. We will provide
    installation details for any package when we first encounter it in this book.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将会大量使用其他一些包，例如**Matplotlib**用于绘图和可视化，**Seaborn**用于可视化，**NLTK**用于自然语言处理任务，**transformers**用于基于大型数据集预训练的先进模型，**OpenAI
    Gym**用于强化学习。每当我们首次遇到某个包时，我们会提供安装详情。
- en: Summary
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We just finished our first mile on the Python and machine learning journey!
    Throughout this chapter, we became familiar with the basics of machine learning.
    We started with what machine learning is all about, the importance of machine
    learning and its brief history, and looked at recent developments as well. We
    also learned typical machine learning tasks and explored several essential techniques
    to work with data and models. Now that we’re equipped with basic machine learning
    knowledge and have set up the software and tools, let’s get ready for the real-world
    machine learning examples ahead.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚完成了Python和机器学习之旅的第一步！在本章中，我们熟悉了机器学习的基础知识。我们从机器学习的定义、重要性和简短历史开始，还了解了最近的发展动态。我们还学习了典型的机器学习任务，并探索了几种处理数据和模型的基本技术。现在，我们已经掌握了基本的机器学习知识，并且设置好了相关的软件和工具，让我们为接下来的实际机器学习示例做好准备吧。
- en: In the next chapter, we will build a movie recommendation engine as our first
    machine learning project!
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将构建一个电影推荐引擎，作为我们的第一个机器学习项目！
- en: Exercises
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Can you tell the difference between machine learning and traditional programming
    (rule-based automation)?
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能说出机器学习和传统编程（基于规则的自动化）之间的区别吗？
- en: What’s overfitting, and how do we avoid it?
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是过拟合，我们如何避免它？
- en: Name two feature engineering approaches.
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列举两种特征工程方法。
- en: Name two ways to combine multiple models.
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列举两种组合多个模型的方法。
- en: Install Matplotlib ([https://matplotlib.org/](https://matplotlib.org/)) if this
    is of interest to you. We will use it for data visualization throughout the book.
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你感兴趣，可以安装Matplotlib ([https://matplotlib.org/](https://matplotlib.org/))。我们将在本书中使用它进行数据可视化。
- en: Join our book’s Discord space
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord社区
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，和作者及其他读者一起讨论：
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/yuxi](https://packt.link/yuxi)'
- en: '![](img/QR_Code1878468721786989681.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1878468721786989681.png)'
