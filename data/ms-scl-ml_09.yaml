- en: Chapter 9. NLP in Scala
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章：Scala中的NLP
- en: This chapter describes a few common techniques of **Natural Language Processing**
    (**NLP**), specifically, the ones that can benefit from Scala. There are some
    NLP packages in the open source out there. The most famous of them is probably
    NLTK ([http://www.nltk.org](http://www.nltk.org)), which is written in Python,
    and ostensibly even a larger number of proprietary software solutions emphasizing
    different aspects of NLP. It is worth mentioning Wolf ([https://github.com/wolfe-pack](https://github.com/wolfe-pack)),
    FACTORIE ([http://factorie.cs.umass.edu](http://factorie.cs.umass.edu)), and ScalaNLP
    ([http://www.scalanlp.org](http://www.scalanlp.org)), and skymind ([http://www.skymind.io](http://www.skymind.io)),
    which is partly proprietary. However, few open source projects in this area remain
    active for a long period of time for one or another reason. Most projects are
    being eclipsed by Spark and MLlib capabilities, particularly, in the scalability
    aspect.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述了几个常见的**自然语言处理**（**NLP**）技术，特别是那些可以从Scala中受益的技术。开源社区中存在一些NLP包。其中最著名的是NLTK（[http://www.nltk.org](http://www.nltk.org)），它是用Python编写的，并且可能还有更多强调NLP不同方面的专有软件解决方案。值得提及的是Wolf（[https://github.com/wolfe-pack](https://github.com/wolfe-pack)）、FACTORIE（[http://factorie.cs.umass.edu](http://factorie.cs.umass.edu)）、ScalaNLP（[http://www.scalanlp.org](http://www.scalanlp.org)）和skymind（[http://www.skymind.io](http://www.skymind.io)），其中skymind部分是专有的。然而，由于一个或多个原因，这个领域的少数开源项目在长时间内保持活跃。大多数项目正被Spark和MLlib的能力所取代，尤其是在可扩展性方面。
- en: Instead of giving a detailed description of each of the NLP projects, which
    also might include speech-to-text, text-to-speech, and language translators, I
    will provide a few basic techniques focused on leveraging Spark MLlib in this
    chapter. The chapter comes very naturally as the last analytics chapter in this
    book. Scala is a very natural-language looking computer language and this chapter
    will leverage the techniques I developed earlier.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会详细描述每个NLP项目，这些项目可能包括语音转文本、文本转语音和语言翻译等，而是将在本章提供一些基本技术，专注于利用Spark MLlib。这一章作为本书的最后一章分析章节，显得非常自然。Scala是一种非常自然语言看起来像的计算机语言，本章将利用我之前开发的技术。
- en: NLP arguably is the core of AI. Originally, the AI was created to mimic the
    humans, and natural language parsing and understanding is an indispensable part
    of it. Big data techniques has started to penetrate NLP, even though traditionally,
    NLP is very computationally intensive and is regarded as a small data problem.
    NLP often requires extensive deep learning techniques, and the volume of data
    of all written texts appears to be not so large compared to the logs volumes generated
    by all the machines today and analyzed by the big data machinery.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: NLP可以说是AI的核心。最初，AI的创建是为了模仿人类，而自然语言解析和理解是其不可或缺的一部分。大数据技术已经开始渗透到NLP中，尽管传统上，NLP计算量很大，被视为小数据问题。NLP通常需要广泛的深度学习技术，而所有书面文本的数据量似乎与今天所有机器生成的日志量以及大数据机器分析的数据量相比并不大。
- en: Even though the Library of Congress counts millions of documents, most of them
    can be digitized in PBs of actual digital data, a volume that any social websites
    is able to collect, store, and analyze within a few seconds. Complete works of
    most prolific authors can be stored within a few MBs of files (refer to *Table
    09-1*). Nonetheless, the social network and ADTECH companies parse text from millions
    of users and in hundreds of contexts every day.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管国会图书馆有数百万份文件，但其中大部分可以数字化为PB的实际数字数据，这是一个任何社交网站都能在几秒钟内收集、存储和分析的数据量。大多数多产作者的完整作品可以存储在几个MB的文件中（参见表09-1）。尽管如此，社交网络和ADTECH公司每天都会从数百万用户和数百个上下文中解析文本。
- en: '| The complete works of | When lived | Size |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| 完整作品 | 生前 | 大小 |'
- en: '| --- | --- | --- |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| *Plato* | 428/427 (or 424/423) - 348/347 BC | 2.1 MB |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| *柏拉图* | 公元前428/427年（或424/423年） - 公元前348/347年 | 2.1 MB |'
- en: '| *William Shakespeare* | 26 April 1564 (baptized) - 23 April 1616 | 3.8 MB
    |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| *威廉·莎士比亚* | 1564年4月26日（洗礼） - 1616年4月23日 | 3.8 MB |'
- en: '| *Fyodor Dostoevsky* | 11 November 1821 - 9 February 1881 | 5.9 MB |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| *费奥多尔·陀思妥耶夫斯基* | 1821年11月11日 - 1881年2月9日 | 5.9 MB |'
- en: '| *Leo Tolstoy* | 9 September 1828 - 20 November 1910 | 6.9 MB |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| *列夫·托尔斯泰* | 1828年9月9日 - 1910年11月20日 | 6.9 MB |'
- en: '| *Mark Twain* | November 30, 1835 - April 21, 1910 | 13 MB |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| *马克·吐温* | 1835年11月30日 - 1910年4月21日 | 13 MB |'
- en: Table 09-1\. Complete Works collections of some famous writers (most can be
    acquired on Amazon.com today for a few dollars, later authors, although readily
    digitized, are more expensive)
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 表09-1\. 一些著名作家全集的收集（大多数今天在Amazon.com上只需几美元即可购买，后来的作者，尽管可以轻松数字化，但价格更高）
- en: The natural language is a dynamic concept that changes over time, technology,
    and generations. We saw the appearance of emoticons, three-letter abbreviations,
    and so on. Foreign languages tend to borrow from each other; describing this dynamic
    ecosystem is a challenge on itself.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言是一个动态的概念，随着时间的推移、技术的发展和代际的变化而变化。我们看到了表情符号、三字母缩写等的出现。外语倾向于相互借鉴；描述这个动态生态系统本身就是一项挑战。
- en: As in the previous chapters, I will focus on how to use Scala as a tool to orchestrate
    the language analysis rather than rewriting the tools in Scala. As the topic is
    so large, I will not claim to cover all aspects of NLP here.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章一样，我将专注于如何使用Scala作为工具来编排语言分析，而不是在Scala中重写工具。由于这个主题如此庞大，我并不声称在这里涵盖NLP的所有方面。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Discussing NLP with the example of text processing pipeline and stages
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以文本处理流程和阶段为例讨论NLP
- en: Learning techniques for simple text analysis in terms of bags
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从词袋的角度学习简单的文本分析方法
- en: Learning about **Term Frequency Inverse Document Frequency** (**TF-IDF**) technique
    that goes beyond simple bag analysis and de facto the standard in **Information
    Retrieval** (**IR**)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习**词频逆文档频率**（**TF-IDF**）技术，它超越了简单的词袋分析，并在**信息检索**（**IR**）中成为事实上的标准
- en: Learning about document clustering with the example of the **Latent Dirichlet
    Allocation** (**LDA**) approach
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以**潜在狄利克雷分配**（**LDA**）方法为例学习文档聚类
- en: Performing semantic analysis using word2vec n-gram-based algorithms
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于word2vec n-gram的算法进行语义分析
- en: Text analysis pipeline
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分析流程
- en: Before we proceed to detailed algorithms, let's look at a generic text-processing
    pipeline depicted in *Figure 9-1*. In text analysis, the input is usually presented
    as a stream of characters (depending on the specific language).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续详细算法之前，让我们看看*图9-1*中描述的通用文本处理流程。在文本分析中，输入通常以字符流的形式呈现（具体取决于特定语言）。
- en: 'Lexical analysis has to do with breaking this stream into a sequence of words
    (or lexemes in linguistic analysis). Often it is also called tokenization (and
    the words called the tokens). **ANother Tool for Language Recognition** (**ANTLR**)
    ([http://www.antlr.org/](http://www.antlr.org/)) and Flex ([http://flex.sourceforge.net](http://flex.sourceforge.net))
    are probably the most famous in the open source community. One of the classical
    examples of ambiguity is lexical ambiguity. For example, in the phrase *I saw
    a bat.* *bat* can mean either an animal or a baseball bat. We usually need context
    to figure this out, which we will discuss next:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇分析涉及将这个流分解成一系列单词（或语言学分析中的词素）。通常它也被称为标记化（而单词被称为标记）。**ANother Tool for Language
    Recognition**（**ANTLR**）([http://www.antlr.org/](http://www.antlr.org/))和Flex([http://flex.sourceforge.net](http://flex.sourceforge.net))在开源社区中可能是最著名的。歧义的一个经典例子是词汇歧义。例如，在短语*I
    saw a bat.*中，*bat*可以指动物或棒球棒。我们通常需要上下文来弄清楚这一点，我们将在下一节讨论：
- en: '![Text analysis pipeline](img/B04935_09_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![文本分析流程](img/B04935_09_01.jpg)'
- en: Figure 9-1\. Typical stages of an NLP process.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-1\. NLP过程的典型阶段。
- en: Syntactic analysis, or parsing, traditionally deals with matching the structure
    of the text with grammar rules. This is relatively more important for computer
    languages that do not allow any ambiguity. In natural languages, this process
    is usually called chunking and tagging. In many cases, the meaning of the word
    in human language can be subject to context, intonation, or even body language
    or facial expression. The value of such analysis, as opposed to the big data approach,
    where the volume of data trumps complexity is still a contentious topic—one example
    of the latter is the word2vec approach, which will be described later.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 语法分析，或称为解析，传统上处理的是将文本结构与语法规则相匹配。这对于不允许任何歧义的计算机语言来说相对更重要。在自然语言中，这个过程通常被称为分块和标记。在许多情况下，人类语言中单词的意义可能受到上下文、语调，甚至肢体语言或面部表情的影响。与大数据方法相比，大数据方法中数据的量胜过复杂性，这种分析的价值仍然是一个有争议的话题——后者的一个例子是word2vec方法，稍后将会描述。
- en: 'Semantic analysis is the process of extracting language-independent meaning
    from the syntactic structures. As much as possible, it also involves removing
    features specific to particular cultural and linguistic contexts, to the extent
    that such a project is possible. The sources of ambiguity at this stage are: phrase
    attachment, conjunction, noun group structure, semantic ambiguity, anaphoric non-literal
    speech, and so on. Again, word2vec partially deals with these issues.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分析是从句法结构中提取与语言无关的意义的过程。在尽可能的范围内，它还涉及去除特定文化或语言背景中的特定特征，到这种项目可能实现的程度。这一阶段的歧义来源包括：短语附着、连词、名词组结构、语义歧义、反身非字面言语等。同样，word2vec部分解决了这些问题。
- en: 'Disclosure integration partially deals with the issue of the context: the meaning
    of a sentence or an idiom can depend on the sentences or paragraphs before that.
    Syntactic analysis and cultural background play an important role here.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 揭示整合部分处理了语境问题：一个句子或成语的意义可能取决于其之前的句子或段落。句法分析和文化背景在这里起着重要作用。
- en: Finally, pragmatic analysis is yet another layer of complexity trying to reinterpret
    what is said in terms of what the intention was. How does this change the state
    of the world? Is it actionable?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，语用分析是试图从意图的角度重新解释所说内容的另一层复杂性。这如何改变世界的状态？它是可执行的吗？
- en: Simple text analysis
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单文本分析
- en: 'The straightforward representation of the document is a bag of words. Scala,
    and Spark, provides an excellent paradigm to perform analysis on the word distributions.
    First, we read the whole collection of texts, and then count the unique words:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 文档的直接表示是一个词袋。Scala和Spark提供了一个出色的范例来执行对词分布的分析。首先，我们读取整个文本集合，然后计算独特的单词：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This gives us just an estimate of the number of distinct words in the repertoire
    of quite different authors. The simplest way to find intersection between the
    two corpuses is to find the common vocabulary (which will be quite different as
    *Leo Tolstoy* wrote in Russian and French, while *Shakespeare* was an English-writing
    author):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是对相当不同作者的词汇库中不同单词数量的一个估计。找到两个语料库之间交集的最简单方法就是找到共同的词汇（正如*列夫·托尔斯泰*用俄语和法语写作，而*莎士比亚*是一位英语作家，所以它们会有很大的不同）：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A few thousands word indices are manageable with the current implementations.
    For any new story, we can determine whether it is more likely to be written by
    Leo Tolstoy or *William Shakespeare*. Let''s take a look at *The King James Version
    of the Bible*, which also can be downloaded from Project Gutenberg ([https://www.gutenberg.org/files/10/10-h/10-h.htm](https://www.gutenberg.org/files/10/10-h/10-h.htm)):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 几千个单词索引在当前实现中是可管理的。对于任何新的故事，我们可以确定它更有可能是由列夫·托尔斯泰还是*威廉·莎士比亚*所写。让我们看看*《圣经的詹姆斯国王版*》，它也可以从Project
    Gutenberg下载（[https://www.gutenberg.org/files/10/10-h/10-h.htm](https://www.gutenberg.org/files/10/10-h/10-h.htm)）：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This seems reasonable as the religious language was popular during the Shakespearean
    time. On the other hand, plays by *Anton Chekhov* have a larger intersection with
    the *Leo Tolstoy* vocabulary:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎是合理的，因为宗教语言在莎士比亚时代很流行。另一方面，*安东·契诃夫*的戏剧与*列夫·托尔斯泰*的词汇有更大的交集：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is a very simple approach that works, but there are a number of commonly
    known improvements we can make. First, a common technique is to stem the words.
    In many languages, words have a common part, often called root, and a changeable
    prefix or suffix, which may depend on the context, gender, time, and so on. Stemming
    is the process of improving the distinct count and intersection by approximating
    this flexible word form to the root, base, or a stem form in general. The stem
    form does not need to be identical to the morphological root of the word, it is
    usually sufficient that related words map to the same stem, even if this stem
    is not in itself a valid grammatical root. Secondly, we probably should account
    for the frequency of the words—while we will describe more elaborate methods in
    the next section, for the purpose of this exercise, we'll exclude the words with
    very high count, that usually are present in any document such as articles and
    possessive pronouns, which are usually called stop words, and the words with very
    low count. Specifically, I'll use the optimized **Porter Stemmer** implementation
    that I described in more detail at the end of the chapter.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单但有效的方法，但我们可以进行一些常见的改进。首先，一个常见的技巧是对单词进行词干提取。在许多语言中，单词有一个共同的组成部分，通常称为词根，以及一个可变的前缀或后缀，这可能会根据上下文、性别、时间等因素而变化。词干提取是通过将这种灵活的词形近似到词根、词干或一般形式来提高区分计数和交集的过程。词干形式不需要与单词的形态学词根完全相同，通常只要相关单词映射到相同的词干就足够了，即使这个词干本身不是一个有效的语法词根。其次，我们可能应该考虑单词的频率——虽然我们将在下一节中描述更详细的方法，但为了这个练习的目的，我们将排除计数非常高的单词，这些单词通常在任何文档中都存在，如文章和所有格代词，这些通常被称为停用词，以及计数非常低的单词。具体来说，我将使用我详细描述在章节末尾的优化版**Porter
    词干提取器**实现。
- en: Note
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The [http://tartarus.org/martin/PorterStemmer/](http://tartarus.org/martin/PorterStemmer/)
    site contains some of the Porter Stemmer implementations in Scala and other languages,
    including a highly optimized ANSI C, which may be more efficient, but here I will
    provide another optimized Scala version that can be used immediately with Spark.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://tartarus.org/martin/PorterStemmer/](http://tartarus.org/martin/PorterStemmer/)
    网站包含一些 Porter 词干提取器的实现，包括高度优化的 ANSI C，这可能更高效，但在这里我将提供另一个优化的 Scala 版本，它可以立即与 Spark
    一起使用。'
- en: 'The Stemmer example will stem the words and count the relative intersections
    between them, removing the stop words:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Stemmer 示例将提取单词并计算它们之间的相对交集，同时移除停用词：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When one runs the main class example from the command line, it outputs the
    stemmed bag sizes and intersection for datasets specified as parameters (these
    are directories in the home filesystem with documents):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当从命令行运行主类示例时，它将输出指定为参数的数据集的提取词袋大小和交集（这些是主文件系统中的目录，包含文档）：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This, in this case, just confirms the hypothesis that Bible's vocabulary is
    closer to *William Shakespeare* than to Leo Tolstoy and other sources. Interestingly,
    modern vocabularies of *NY Times* articles and Enron's e-mails from the previous
    chapters are much closer to *Leo Tolstoy's*, which is probably more an indication
    of the translation quality.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，这仅仅证实了假设：圣经的词汇比列夫·托尔斯泰和其他来源更接近**威廉·莎士比亚**。有趣的是，现代的**《纽约时报》**文章和前几章中恩隆的电子邮件的词汇与现代的**列夫·托尔斯泰**的词汇更为接近，这可能是翻译质量的更好指示。
- en: Another thing to notice is that the pretty complex analysis took about *40*
    lines of Scala code (not counting the libraries, specifically the Porter Stemmer,
    which is about ~ *100* lines) and about 12 seconds. The power of Scala is that
    it can leverage other libraries very efficiently to write concise code.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点要注意的是，这个相当复杂的分析大约需要 40 行 Scala 代码（不包括库，特别是 Porter 词干提取器，大约有 100 行）和大约 12
    秒的时间。Scala 的强大之处在于它可以非常有效地利用其他库来编写简洁的代码。
- en: Note
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Serialization**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**序列化**'
- en: We already talked about serialization in [Chapter 6](ch06.xhtml "Chapter 6. Working
    with Unstructured Data"), *Working with Unstructured Data*. As Spark's tasks are
    executed in different threads and potentially JVMs, Spark does a lot of serialization/deserialization
    when passing the objects. Potentially, I could use `map { val stemmer = new Stemmer;
    stemmer.stem(_) }` instead of `map { stemmer.stem(_) }`, but the latter reuses
    the object for multiple iterations and seems to be linguistically more appealing.
    One suggested performance optimization is to use *Kryo serializer*, which is less
    flexible than the Java serializer, but more performant. However, for integrative
    purpose, it is much easier to just make every object in the pipeline serializable
    and use default Java serialization.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第6章](ch06.xhtml "第6章。处理非结构化数据")*处理非结构化数据*中讨论了序列化。由于Spark的任务在不同的线程和潜在的JVM中执行，Spark在传递对象时进行了大量的序列化和反序列化。潜在地，我可以用`map
    { val stemmer = new Stemmer; stemmer.stem(_) }`代替`map { stemmer.stem(_) }`，但后者在多次迭代中重用对象，在语言上似乎更吸引人。一种建议的性能优化是使用*Kryo序列化器*，它不如Java序列化器灵活，但性能更好。然而，为了集成目的，只需使管道中的每个对象可序列化并使用默认的Java序列化就更容易了。
- en: 'As another example, let''s compute the distribution of word frequencies, as
    follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个例子，让我们计算词频分布，如下所示：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The distribution of relative frequencies on the log-log scale is presented
    in the following diagram. With the exception of the first few tokens, the dependency
    of frequency on rank is almost linear:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图表中展示了对数-对数尺度上的相对频率分布。除了前几个标记之外，频率对排名的依赖几乎呈线性：
- en: '![Simple text analysis](img/B04935_09_02.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![简单的文本分析](img/B04935_09_02.jpg)'
- en: Figure 9-2\. A typical distribution of word relative frequencies on log-log
    scale (Zipf's Law)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-2\. 以对数-对数尺度为典型分布的词相对频率（Zipf定律）
- en: MLlib algorithms in Spark
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark中的MLlib算法
- en: Let's halt at MLlib that complements other NLP libraries written in Scala. MLlib
    is primarily important because of scalability, and thus supports a few of the
    data preparation and text processing algorithms, particularly in the area of feature
    construction ([http://spark.apache.org/docs/latest/ml-features.html](http://spark.apache.org/docs/latest/ml-features.html)).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂停在补充其他用Scala编写的NLP库的MLlib上。MLlib之所以重要，主要是因为其可扩展性，因此支持一些数据准备和文本处理算法，尤其是在特征构造领域（[http://spark.apache.org/docs/latest/ml-features.html](http://spark.apache.org/docs/latest/ml-features.html)）。
- en: TF-IDF
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Although the preceding analysis can already give a powerful insight, the piece
    of information that is missing from the analysis is term frequency information.
    The term frequencies are relatively more important in information retrieval, where
    the collection of documents need to be searched and ranked in relation to a few
    terms. The top documents are usually returned to the user.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面的分析已经可以提供强大的洞察力，但分析中缺失的信息是词频信息。在信息检索中，词频相对更为重要，因为需要根据几个词对文档集合进行搜索和排序。通常，最顶部的文档会被返回给用户。
- en: TF-IDF is a standard technique where term frequencies are offset by the frequencies
    of the terms in the corpus. Spark has an implementation of the TF-IDF. Spark uses
    a hash function to identify the terms. This approach avoids the need to compute
    a global term-to-index map, but can be subject to potential hash collisions, the
    probability of which is determined by the number of buckets of the hash table.
    The default feature dimension is *2^20=1,048,576*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF是一种标准技术，其中词频被语料库中词的频率所抵消。Spark实现了TF-IDF。Spark使用哈希函数来识别词。这种方法避免了计算全局词到索引映射的需要，但可能会受到潜在的哈希冲突的影响，其概率由哈希表桶的数量决定。默认的特征维度是*2^20=1,048,576*。
- en: 'In the Spark implementation, each document is a line in the dataset. We can
    convert it into to an RDD of iterables and compute the hashing by the following
    code:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark实现中，每个文档是数据集中的一行。我们可以将其转换为可迭代的RDD，并按以下代码进行哈希计算：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When computing `hashingTF`, we only need a single pass over the data, applying
    IDF needs two passes: first to compute the IDF vector and second to scale the
    term frequencies by IDF:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算`hashingTF`时，我们只需要对数据进行单次遍历，应用IDF需要两次遍历：首先计算IDF向量，然后按IDF缩放词频：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here we see each document represented by a set of terms and their scores.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到每个文档都由一组词及其分数表示。
- en: LDA
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LDA
- en: LDA in Spark MLlib is a clustering mechanism, where the feature vectors represent
    the counts of words in a document. The model maximizes the probability of observing
    the word counts, given the assumption that each document is a mixture of topics
    and the words in the documents are generated based on **Dirichlet distribution**
    (a generalization of beta distribution on multinomial case) for each of the topic
    independently. The goal is to derive the (latent) distribution of the topics and
    the parameters of the words generation statistical model.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark MLlib 中的 LDA 是一种聚类机制，其中特征向量表示文档中单词的计数。该模型最大化观察到的单词计数的概率，假设每个文档是主题的混合，并且文档中的单词是基于**狄利克雷分布**（多项式情况下的
    beta 分布的推广）独立生成的。目标是推导出（潜在）主题分布和单词生成统计模型的参数。
- en: The MLlib implementation is based on 2009 LDA paper ([http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf](http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf))
    and uses GraphX to implement a distributed **Expectation Maximization** (**EM**)
    algorithm for assigning topics to the documents.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 的实现基于 2009 年的 LDA 论文([http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf](http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf))，并使用
    GraphX 实现了一个分布式**期望最大化**（**EM**）算法，用于将主题分配给文档。
- en: 'Let''s take the Enron e-mail corpus discussed in [Chapter 7](ch07.xhtml "Chapter 7. Working
    with Graph Algorithms"), *Working with Graph Algorithms*, where we tried to figure
    out communications graph. For e-mail clustering, we need to extract the body of
    the e-mail and place is as a single line in the training file:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以第 7 章中讨论的 Enron 电子邮件语料库为例，*Working with Graph Algorithms*，在那里我们试图找出通信图。对于电子邮件聚类，我们需要提取电子邮件正文并将其作为单行放置在训练文件中：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, let''s use Scala/Spark to construct a corpus dataset containing the document
    ID, followed by a dense array of word counts in the bag:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 Scala/Spark 构建一个包含文档 ID 的语料库数据集，随后是一个密集的词频数组：
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can also list the words and their relative importance for the topic in the
    descending order:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以按降序列出单词及其在主题中的相对重要性：
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To find out the top documents per topic or top topics per document, we need
    to convert this model to `DistributedLDA` or `LocalLDAModel`, which extend `LDAModel`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出每个主题的前几篇文档或每篇文档的前几个主题，我们需要将此模型转换为 `DistributedLDA` 或 `LocalLDAModel`，它们扩展了
    `LDAModel`：
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Segmentation, annotation, and chunking
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词、标注和分块
- en: When the text is presented in digital form, it is relatively easy to find words
    as we can split the stream on non-word characters. This becomes more complex in
    spoken language analysis. In this case, segmenters try to optimize a metric, for
    example, to minimize the number of distinct words in the lexicon and the length
    or complexity of the phrase (*Natural Language Processing with Python* by *Steven
    Bird et al*, *O'Reilly Media Inc*, 2009).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当文本以数字形式呈现时，找到单词相对容易，因为我们可以在非单词字符上分割流。在口语语言分析中，这变得更加复杂。在这种情况下，分词器试图优化一个指标，例如，最小化词典中不同单词的数量以及短语的长短或复杂性（*《Python
    自然语言处理》* by *Steven Bird 等人*，*O'Reilly 媒体公司*，2009年）。
- en: Annotation usually refers to parts-of-speech tagging. In English, these are
    nouns, pronouns, verbs, adjectives, adverbs, articles, prepositions, conjunctions,
    and interjections. For example, in the phrase *we saw the yellow dog*, *we* is
    a pronoun, *saw* is a verb, *the* is an article, *yellow* is an adjective, and
    *dog* is a noun.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 标注通常指的是词性标注。在英语中，这些包括名词、代词、动词、形容词、副词、冠词、介词、连词和感叹词。例如，在短语 *we saw the yellow
    dog* 中，*we* 是代词，*saw* 是动词，*the* 是冠词，*yellow* 是形容词，而 *dog* 是名词。
- en: In some languages, the chunking and annotation depends on context. For example,
    in Chinese, *爱江山人* literally translates to *love country person* and can mean
    either *country-loving person* or *love country-person*. In Russian, *казнить
    нельзя помиловать*, literally translating to *execute not pardon*, can mean *execute,
    don't pardon*, or *don't execute, pardon*. While in written language, this can
    be disambiguated using commas, in a spoken language this is usually it is very
    hard to recognize the difference, even though sometimes the intonation can help
    to segment the phrase properly.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些语言中，分块和标注取决于上下文。例如，在中文中，*爱江山人* 直译为 *love country person*，可以表示 *country-loving
    person* 或 *love country-person*。在俄语中，*казнить нельзя помиловать* 直译为 *execute
    not pardon*，可以表示 *execute, don't pardon* 或 *don't execute, pardon*。虽然在书面语言中，这可以通过逗号来消除歧义，但在口语中，通常很难识别这种差异，尽管有时语调可以帮助正确地分割短语。
- en: For techniques based on word frequencies in the bags, some extremely common
    words, which are of little value in helping select documents, are explicitly excluded
    from the vocabulary. These words are called stop words. There is no good general
    strategy for determining a stop list, but in many cases, this is to exclude very
    frequent words that appear in almost every document and do not help to differentiate
    between them for classification or information retrieval purposes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于词袋中词频的技术，一些极其常见的词，它们在帮助选择文档方面价值不大，被明确地从词汇表中排除。这些词被称为停用词。没有好的通用策略来确定停用词表，但在许多情况下，这是排除几乎出现在每份文档中的非常高频词，这些词对于分类或信息检索目的没有帮助。
- en: POS tagging
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词性标注
- en: 'POS tagging probabilistically annotates each word with it''s grammatical function—noun,
    verb, adjective, and so on. Usually, POS tagging serves as an input to syntactic
    and semantic analysis. Let''s demonstrate POS tagging on the FACTORIE toolkit
    example, a software library written in Scala ([http://factorie.cs.umass.edu](http://factorie.cs.umass.edu)).
    To start, you need to download the binary image or source files from [https://github.com/factorie/factorie.git](https://github.com/factorie/factorie.git)
    and build it:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注以概率方式为每个词标注其语法功能——名词、动词、形容词等等。通常，词性标注作为句法和语义分析输入。让我们以FACTORIE工具包为例演示词性标注，这是一个用Scala编写的软件库（[http://factorie.cs.umass.edu](http://factorie.cs.umass.edu)）。首先，您需要从[https://github.com/factorie/factorie.git](https://github.com/factorie/factorie.git)下载二进制镜像或源文件并构建它：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After the build, which also includes model training, the following command
    will start a network server on `port 3228`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建完成后，这还包括模型训练，以下命令将在`端口3228`上启动一个网络服务器：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, all traffic to `port 3228` will be interpreted (as text), and the output
    will be tokenized and annotated:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，所有流向`端口3228`的流量都将被解释（作为文本），输出将被分词和标注：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This POS is a single-path left-right tagger that can process the text as a stream.
    Internally, the algorithm uses probabilistic techniques to find the most probable
    assignment. Let's also look at other techniques that do not use grammatical analysis
    and yet proved to be very useful for language understanding and interpretation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这种词性标注是一个单路径左右标注器，可以流式处理文本。内部，算法使用概率技术来找到最可能的分配。让我们也看看其他不使用语法分析但证明对语言理解和解释非常有用的技术。
- en: Using word2vec to find word relationships
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用word2vec寻找词关系
- en: 'Word2vec has been developed by Tomas Mikolov at Google, around 2012\. The original
    idea behind word2vec was to demonstrate that one might improve efficiency by trading
    the model''s complexity for efficiency. Instead of representing a document as
    bags of words, word2vec takes each word context into account by trying to analyze
    n-grams or skip-grams (a set of surrounding tokens with potential the token in
    question skipped). The words and word contexts themselves are represented by an
    array of floats/doubles ![Using word2vec to find word relationships](img/B04935_09_01F.jpg).
    The objective function is to maximize log likelihood:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec是由Google的Tomas Mikolov在2012年左右开发的。word2vec背后的原始想法是通过以效率换取模型复杂性来提高效率。word2vec不是将文档表示为词袋，而是通过尝试分析n-gram或skip-gram（一组可能的标记及其周围标记的集合，其中可能省略了问题标记）来考虑每个词的上下文。单词及其上下文本身由一个浮点数/双精度浮点数数组![使用word2vec寻找词关系](img/B04935_09_01F.jpg)表示。目标函数是最大化对数似然：
- en: '![Using word2vec to find word relationships](img/B04935_09_02F.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![使用word2vec寻找词关系](img/B04935_09_02F.jpg)'
- en: 'Where:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![Using word2vec to find word relationships](img/B04935_09_03F.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![使用word2vec寻找词关系](img/B04935_09_03F.jpg)'
- en: By choosing the optimal ![Using word2vec to find word relationships](img/B04935_09_01F.jpg)
    and to get a comprehensive word representation (also called **map optimization**).
    Similar words are found based on cosine similarity metric (dot product) of ![Using
    word2vec to find word relationships](img/B04935_09_01F.jpg). Spark implementation
    uses hierarchical softmax, which reduces the complexity of computing the conditional
    probability to ![Using word2vec to find word relationships](img/B04935_09_04F.jpg),
    or log of the vocabulary size *V*, as opposed to ![Using word2vec to find word
    relationships](img/B04935_09_05F.jpg), or proportional to *V*. The training is
    still linear in the dataset size, but is amenable to big data parallelization
    techniques.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择最优的![使用word2vec寻找词关系](img/B04935_09_01F.jpg)并获取全面的词表示（也称为**映射优化**）。基于![使用word2vec寻找词关系](img/B04935_09_01F.jpg)的余弦相似度度量（点积）找到相似词语。Spark实现使用层次softmax，将计算条件概率的复杂性降低到![使用word2vec寻找词关系](img/B04935_09_04F.jpg)，或词汇大小*V*的对数，而不是![使用word2vec寻找词关系](img/B04935_09_05F.jpg)，或与*V*成比例。训练在数据集大小上仍然是线性的，但适用于大数据并行化技术。
- en: '`Word2vec` is traditionally used to predict the most likely word given context
    or find similar words with a similar meaning (synonyms). The following code trains
    in `word2vec` model on *Leo Tolstoy''s Wars and Peace*, and finds synonyms for
    the word *circle*. I had to convert the Gutenberg''s representation of *War and
    Peace* to a single-line format by running the `cat 2600.txt | tr "\n\r" " " >
    warandpeace.txt` command:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`Word2vec`传统上用于预测给定上下文中最可能的词或找到具有相似意义的相似词（同义词）。以下代码在*列夫·托尔斯泰的战争与和平*上训练`word2vec`模型，并找到*circle*的同义词。我不得不通过运行`cat
    2600.txt | tr "\n\r" " " > warandpeace.txt`命令将古腾堡的*战争与和平*表示转换为单行格式：'
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: While in general, it is hard to some with an objective function, and `freedom`
    is not listed as a synonym to `life` in the English Thesaurus, the results do
    make sense.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在一般情况下，很难与一个目标函数相匹配，并且`freedom`在英语同义词典中没有列为`life`的同义词，但结果确实是有意义的。
- en: 'Each word in the word2vec model is represented as an array of doubles. Another
    interesting application is to find associations *a to b is the same as c to ?*
    by performing subtraction *vector(a) - vector(b) + vector(c)*:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在word2vec模型中，每个词都表示为一个双精度浮点数数组。另一个有趣的应用是找到关联*a到b与c到?相同*，通过执行减法*vector(a) - vector(b)
    + vector(c)*：
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This can be used to find relationships in the language.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以用来在语言中找到关系。
- en: A Porter Stemmer implementation of the code
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码的Porter Stemmer实现
- en: 'Porter Stemmer was first developed around the 1980s and there are many implementations.
    The detailed steps and original reference are provided at [http://tartarus.org/martin/PorterStemmer/def.txt](http://tartarus.org/martin/PorterStemmer/def.txt).
    It consists of roughly 6-9 steps of suffix/endings replacements, some of which
    are conditional on prefix or stem. I will provide a Scala-optimized version with
    the book code repository. For example, step 1 covers the majority of stemming
    cases and consists of 12 substitutions: the last 8 of which are conditional on
    the number of syllables and the presence of vowels in the stem:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Porter Stemmer最初在20世纪80年代开发，有许多实现。详细的步骤和原始参考可在[http://tartarus.org/martin/PorterStemmer/def.txt](http://tartarus.org/martin/PorterStemmer/def.txt)找到。它大致包括6-9步的词尾/结尾替换，其中一些取决于前缀或词根。我将提供一个与书籍代码仓库优化的Scala版本。例如，步骤1涵盖了大多数词干化情况，包括12个替换：最后8个取决于音节数和词根中的元音存在：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The complete code is available at [https://github.com/alexvk/ml-in-scala/blob/master/chapter09/src/main/scala/Stemmer.scala](https://github.com/alexvk/ml-in-scala/blob/master/chapter09/src/main/scala/Stemmer.scala).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可在[https://github.com/alexvk/ml-in-scala/blob/master/chapter09/src/main/scala/Stemmer.scala](https://github.com/alexvk/ml-in-scala/blob/master/chapter09/src/main/scala/Stemmer.scala)找到。
- en: Summary
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, I described basic NLP concepts and demonstrated a few basic
    techniques. I hoped to demonstrate that pretty complex NLP concepts could be expressed
    and tested in a few lines of Scala code. This is definitely just the tip of the
    iceberg as a lot of NLP techniques are being developed now, including the ones
    based on in-CPU parallelization as part of GPUs. (refer to, for example, **Puck**
    at [https://github.com/dlwh/puck](https://github.com/dlwh/puck)). I also gave
    a flavor of major Spark MLlib NLP implementations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我描述了基本的自然语言处理（NLP）概念，并演示了几种基本技术。我希望展示即使是相当复杂的NLP概念也可以用几行Scala代码来表示和测试。这无疑是冰山一角，因为现在正在开发许多NLP技术，包括作为GPU一部分的CPU内并行化技术。（例如，参考[https://github.com/dlwh/puck](https://github.com/dlwh/puck)上的**Puck**)。我还介绍了主要的Spark
    MLlib NLP实现。
- en: In the next chapter, which will be the final chapter of this book, I'll cover
    systems and model monitoring.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，也就是本书的最后一章，我将涵盖系统和模型监控的内容。
