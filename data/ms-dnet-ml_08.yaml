- en: Chapter 8. Feature Selection and Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。特征选择和优化
- en: 'In software engineering, there is an old saying: *make it work first, then
    make it fast*. In this book, we have adopted the strategy to *make it run, then
    make it better*. Many of the models that we covered in the initial chapters were
    correct in a very limited sense and could stand some optimization to make them
    more correct. This chapter is all about *making it better*.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件工程中，有一句古老的谚语：*先让它工作，再让它变快*。在这本书中，我们采用了*先让它运行，再让它变得更好*的策略。我们在第一章中讨论的许多模型在非常有限的意义上是正确的，并且可以通过一些优化来使它们更加正确。这一章完全是关于*让它变得更好*的。
- en: Cleaning data
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清洗数据
- en: As we saw in [Chapter 5](part0036_split_000.html#12AK81-a18db0be6c20485ba81f22e43ca13055
    "Chapter 5. Time Out – Obtaining Data"), *Time Out – Obtaining Data*, obtaining
    and shaping the data (which is often the largest problem in many projects) is
    a snap using F# type providers. However, once our data is local and shaped, our
    work in preparing the data for machine learning is not complete. There might still
    be abnormalities in each frame. Things like null values, empty values, and values
    outside a reasonable range need to be addressed. If you come from an R background,
    you will be familiar with `null.omit` and `na.omit`, which remove all of the rows
    from a data frame. We can achieve functional equivalence in F# by applying a filter
    function to the data. In the filter, you can search for null if it is a reference
    type, or `.isNone` if the column is an option type. While this is effective, it
    is a bit of a blunt hammer because you are throwing out a row that might have
    valid values in the other fields when only one field has an inappropriate value.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第5章](part0036_split_000.html#12AK81-a18db0be6c20485ba81f22e43ca13055 "第5章。时间到
    – 获取数据")中看到的，*时间到 – 获取数据*，使用F#类型提供者获取和塑造数据（这通常是许多项目中的最大问题）非常简单。然而，一旦我们的数据本地化和塑形，我们为机器学习准备数据的工作还没有完成。每个数据帧可能仍然存在异常。像空值、空值和超出合理范围的数据值等问题需要解决。如果你来自R背景，你将熟悉`null.omit`和`na.omit`，它们会从数据框中删除所有行。我们可以在F#中通过应用一个过滤函数来实现功能等价。在过滤中，如果你是引用类型，可以搜索空值，如果是可选类型，则使用`.isNone`。虽然这很有效，但它有点像一把钝锤，因为你正在丢弃可能在其他字段中有有效值的行。
- en: 'Another way to handle missing data is to replace it with a value that will
    not skew an analysis. Like most things in data science, there are plenty of opinions
    on the different techniques, and I won''t go into too much detail here. Rather,
    I want to make you aware of the issue and show you a common way to remediate it:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失数据的另一种方法是用一个不会扭曲分析的值来替换它。像数据科学中的大多数事情一样，关于不同技术的意见有很多，这里我不会过多详细说明。相反，我想让你意识到这个问题，并展示一种常见的补救方法：
- en: 'Go into Visual Studio and create a Visual F# Windows Library project called
    `FeatureCleaning`:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 进入Visual Studio，创建一个名为`FeatureCleaning`的Visual F# Windows库项目：
- en: '![Cleaning data](img/00104.jpeg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![清洗数据](img/00104.jpeg)'
- en: 'Locate `Script1.fsx` in the **Solution Explorer** and rename it `CleanData.fsx`:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在**解决方案资源管理器**中定位`Script1.fsx`并将其重命名为`CleanData.fsx`：
- en: '![Cleaning data](img/00105.jpeg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![清洗数据](img/00105.jpeg)'
- en: 'Open that script file, and replace the existing code with this:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 打开那个脚本文件，并用以下代码替换现有的代码：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Sending this to the FSI gives us the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 将其发送到FSI后，我们得到以下结果：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`User` is a record type that represents the users of an application while `users`
    is an array of three users. It looks pretty vanilla except user 3, Sally Price,
    has an age of `1000.0`. What we want to do is take that age out but still keep
    Sally''s record. To do that, let''s remove 1,000 and replace it with the average
    of the ages of all of remaining users. Go back to the script file and enter this:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`User`是一种记录类型，代表应用程序的用户，而`users`是一个包含三个用户的数组。它看起来相当普通，除了用户3，莎莉·普莱斯，她的年龄为`1000.0`。我们想要做的是去掉这个年龄，但仍然保留莎莉的记录。要做到这一点，让我们将1,000替换为所有剩余用户年龄的平均值。回到脚本文件，输入以下内容：'
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Sending this to the FSI should give you the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 将其发送到FSI应该得到以下结果：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that we create a subarray of the valid users and then get their average
    ages. We then create a subarray of invalid users and map in the average age. Since
    F# does not like mutability, we create a new record for each of the invalid users
    and use the `with` syntax effectively, creating a new record that has all the
    same values as the original record, except the age. We then wrap up by concatenating
    the valid users and the updated user back into a single array. Although this is
    a fairly rudimentary technique, it can be surprisingly effective. As you get further
    into machine learning, you will develop and refine your own techniques for dealing
    with invalid data—and you have to keep in mind that the model that you are using
    will dictate how you clean that data. In some models, taking the average might
    throw things off.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们创建了一个有效用户的子数组，然后获取他们的平均年龄。然后我们创建了一个无效用户的子数组，并映射平均年龄。由于F#不喜欢可变性，我们为每个无效用户创建了一个新记录，并有效地使用`with`语法，创建了一个具有所有相同值的新记录，除了年龄。然后我们通过将有效用户和更新的用户合并成一个单一数组来完成。尽管这是一种相当基础的技术，但它可以非常有效。当你进一步学习机器学习时，你会发展和完善自己的处理无效数据的技术——你必须记住，你使用的模型将决定你如何清理这些数据。在某些模型中，取平均值可能会使事情变得混乱。
- en: Selecting data
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择数据
- en: When we are confronted with a large number of independent variables, we often
    run into the problem of which values to select. In addition, the variable might
    be binned, combined with other variables, or altered—all of which might make or
    break a particular model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们面对大量独立变量时，我们常常会遇到选择哪些值的问题。此外，变量可能被分类、与其他变量结合或改变——这些都可能使某个模型成功或失败。
- en: Collinearity
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共线性
- en: 'Collinearity is when we have multiple x variables that are highly related to
    each other; they have a high degree of correlation. When using regressions, you
    always have to be on the watch for collinearity as you can''t be sure which individual
    variable really affects the outcome variable. Here is a classic example. Suppose
    you wanted to measure the happiness of a college student. You have the following
    input variables: age, sex, money available for beer, money available for textbooks.
    In this case, there is a direct relationship between money available for beer
    and money available for textbooks. The more money spent on textbooks, the less
    there is available for beer. To solve for collinearity, you can do a couple of
    things:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 共线性是指我们有多多个彼此高度相关的`x`变量；它们有高度的关联度。在使用回归时，你总是要警惕共线性，因为你不能确定哪个个别变量真正影响了结果变量。这里有一个经典例子。假设你想测量大学生的幸福感。你有以下输入变量：年龄、性别、用于啤酒的可用资金、用于教科书的可用资金。在这种情况下，用于啤酒的可用资金和用于教科书的可用资金之间存在直接关系。用于教科书的资金越多，用于啤酒的资金就越少。为了解决共线性问题，你可以做几件事情：
- en: Drop one of the highly-correlated variables. In this case, perhaps drop money
    available for text books.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除一个高度相关的变量。在这种情况下，可能需要删除用于教科书的可用资金。
- en: Combine correlated variables into a single variable. In this case, perhaps just
    have a category of money in checking account.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将相关的变量合并成一个单一变量。在这种情况下，可能只需要有一个检查账户中的金钱类别。
- en: 'A common way to test for collinearity is to run your multiple regressions several
    times, each time removing one `x` variable. If there is not a dramatic change
    when two different variables are removed, they are good candidates for collinearity.
    In addition, you can always do a visual scan of the correlation matrix of the
    `x` variables, which you can do using Accord.Net with the `Tools.Corrlelation`
    method. Let''s take a look at this. Go back into Visual Studio and add a new script
    file called `Accord.fsx`. Open the NuGet Package Manager Console and add in Accord:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 测试共线性的一个常见方法是多次运行你的多元回归，每次移除一个`x`变量。如果移除两个不同变量时没有显著变化，它们就是共线性的良好候选者。此外，你总是可以扫描`x`变量的相关矩阵，你可以使用Accord.Net的`Tools.Corrlelation`方法来做这件事。让我们看看这个。回到Visual
    Studio，添加一个名为`Accord.fsx`的新脚本文件。打开NuGet包管理器控制台，并添加Accord：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This represents three students who we interviewed. We asked each their age,
    their gender, how much money they had for textbooks, and how much money they had
    for beer. The first student is a 19-year-old, female, had $50.00 for text books,
    and $10.00 for beer.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这代表了我们采访的三名学生。我们询问了他们的年龄、性别、用于教科书的资金和用于啤酒的资金。第一位学生是19岁的女性，有50.00美元用于教科书，10.00美元用于啤酒。
- en: 'When you send this to the FSI, you get the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将此发送到FSI时，你会得到以下结果：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'It is a bit hard to read, so I reformatted it:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点难以阅读，所以我重新格式化了它：
- en: '|   | Age | Gender | $ Books | $ Beer |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|   | 年龄 | 性别 | $ 书籍 | $ 啤酒 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Age** | 1.0 | 0.76 | -0.84 | 0.88 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| **年龄** | 1.0 | 0.76 | -0.84 | 0.88 |'
- en: '| **Sex** | 0.76 | 1.0 | -0.28 | 0.35 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| **性别** | 0.76 | 1.0 | -0.28 | 0.35 |'
- en: '| **$ Books** | -0.84 | -0.28 | 1.0 | -0.99 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **$ 书籍** | -0.84 | -0.28 | 1.0 | -0.99 |'
- en: '| **$ Beer** | 0.88 | 0.35 | -0.99 | 1.0 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| **$ 啤酒** | 0.88 | 0.35 | -0.99 | 1.0 |'
- en: 'Notice the diagonal values in matrix, 1.0, which means that age is perfectly
    correlated with age, sex is perfectly correlated with sex, and so on. The key
    thing from this example is that there is an almost perfect negative correlation
    between the amount of money for books and the amount of money for beer: it is
    `-0.99`. What this means is that, if you have more money for books, you have less
    for beer, which makes sense. By reading the correlation matrix, you can get a
    quick understanding of what variables are correlated and can possibly be removed.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意矩阵中的对角线值，1.0，这意味着年龄与年龄完全相关，性别与性别完全相关，等等。从这个例子中，关键的一点是书籍金额和啤酒金额之间存在几乎完美的负相关：它是
    `-0.99`。这意味着如果你为书籍有更多的钱，那么你为啤酒的钱就会更少，这是有道理的。通过阅读相关矩阵，你可以快速了解哪些变量是相关的，可能可以被移除。
- en: A related topic to collinearity is to always keep your `y` variable as independent
    as possible from the `x` variable. For example, if you made a regression where
    you were trying to pick the amount of money available for beer for our student,
    you would not pick any independent variable that related to the amount of money
    the student has. Why? Because they are measuring the same thing.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与多重共线性相关的一个话题是始终尽可能使你的 `y` 变量与 `x` 变量独立。例如，如果你做了一个回归，试图挑选出我们学生的啤酒可用金额，你不会选择任何与该学生拥有的金额相关的独立变量。为什么？因为它们测量的是同一件事。
- en: Feature selection
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征选择
- en: 'A related topic to collinearity is feature selection. If you have a whole mess
    of `x` variables, how do you decide which ones will be the best ones for your
    analysis? You can start picking and choosing, but that is time-consuming and can
    possibly lead to errors. Instead of guessing, there are some modeling techniques
    that run simulations across all your data to determine the best combination of
    `x` variables to use. One of the most common techniques is called forward-selection
    step-wise regression. Consider a data frame that has five independent variables
    and one dependent variable:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与多重共线性相关的一个话题是特征选择。如果你有一堆 `x` 变量，你如何决定哪些变量最适合你的分析？你可以开始挑选，但这很耗时，可能还会出错。与其猜测，不如有一些建模技术可以在所有数据上运行模拟，以确定最佳的
    `x` 变量组合。其中最常见的技术之一被称为前向选择逐步回归。考虑一个包含五个自变量和一个因变量的数据框：
- en: '![Feature selection](img/00106.jpeg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![特征选择](img/00106.jpeg)'
- en: 'Using forward-selection step-wise regression, the technique starts out with
    a single variable, runs a regression, and calculates (in this case) a rmse:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前向选择逐步回归，该技术从单个变量开始，运行回归，并计算（在这种情况下）均方根误差（rmse）：
- en: '![Feature selection](img/00107.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![特征选择](img/00107.jpeg)'
- en: 'Next, the technique goes back and adds in another variable and calculates the
    rmse:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，技术会回过头来添加另一个变量并计算 rmse：
- en: '![Feature selection](img/00108.jpeg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![特征选择](img/00108.jpeg)'
- en: 'Next, the technique goes back and further adds in another variable and calculates
    the rmse:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，技术会回过头来添加另一个变量并计算 rmse：
- en: '![Feature selection](img/00109.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![特征选择](img/00109.jpeg)'
- en: By now, you probably have the idea. Depending on the implementation, the stepwise
    might be re-run with different combinations of independent variables and/or different
    test and training sets. When the step-wise is done, you can have a good idea about
    what features are important and what can be discarded.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你可能已经明白了。根据实现方式，逐步回归可能会用不同的自变量组合和/或不同的测试和训练集重新运行。当逐步回归完成后，你就可以对哪些特征是重要的以及哪些可以被舍弃有一个很好的了解。
- en: 'Let''s take a look at a step-wise regression example in Accord. Go back to
    your script and enter this code (note that this is verbatim from the Accord help
    file on stepwise regression):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看 Accord 中的逐步回归示例。回到你的脚本中，输入以下代码（注意，这直接来自 Accord 帮助文件中的逐步回归部分）：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Send this to the FSI to get the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 将其发送到 FSI，得到以下结果：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can tell from the comments in the code, the inputs are 20 fictional people
    that have been recently screened for cancer. The features are their ages and whether
    or not they smoke. The output is whether the person actually did have cancer.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如代码中的注释所示，输入的是20个最近接受癌症筛查的虚构人物。特征包括他们的年龄和是否吸烟。输出是这个人是否真的患有癌症。
- en: 'Go back to the script and add this:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 回到脚本中，添加以下内容：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When you send this to the FSI, you will see something very interesting. The
    `full.Coefficients` returns all of the variables but the `best.Coefficients` returns
    this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将这个数据发送到FSI时，你会看到一些非常有趣的东西。`full.Coefficients`返回所有变量，但`best.Coefficients`返回以下内容：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can now see that `Smoking` is the most important variable when predicting
    cancer. If two or more variables were considered important, Accord would have
    told you the number 1 variable, then the next one, and so on. Stepwise regressions
    are a bit on the outs these days as the community has moved to Lasso and some
    other techniques. However, it is still an important tool in your toolkit and is
    something that you should know about.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以看到`吸烟`是预测癌症时最重要的变量。如果有两个或更多变量被认为是重要的，Accord会告诉你第一个变量，然后是下一个，依此类推。逐步回归分析在当今社区已经转向Lasso和其他一些技术的情况下，有点过时了。然而，它仍然是你的工具箱中的一个重要工具，并且是你应该了解的内容。
- en: Normalization
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 归一化
- en: Sometimes our models can be improved by adjusting the data. I am not talking
    about "adjusting numbers" in the Enron accounting or US politician sense. I am
    talking about adjusting the data using some standard scientific techniques that
    might improve the model's accuracy. The general term for this is *normalization*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，通过调整数据，我们可以提高模型的效果。我说的不是在安然会计或美国政治家意义上的“调整数字”。我指的是使用一些标准的科学技术来调整数据，这些技术可能会提高模型的准确性。这个过程的通用术语是*归一化*。
- en: There are many different ways to normalize data. I want to show you two common
    ones that work well with regressions. First, if your data is clustered together,
    you can take the log of the values to help tease out relationships that might
    otherwise be hidden. For example, look at our scatterplot of product reviews from
    the beginning of [Chapter 2](part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055
    "Chapter 2. AdventureWorks Regression"), *AdventureWorks Regression*. Notice that
    most of the order quantity centered around 250 to 1,000.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据归一化的方法有很多种。我想向您展示两种与回归分析效果良好的常见方法。首先，如果你的数据是聚集在一起的，你可以对数值取对数，以帮助揭示可能被隐藏的关系。例如，看看我们来自[第二章](part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055
    "第二章。AdventureWorks 回归")的开头的产品评论散点图，*AdventureWorks 回归*。注意，大多数订单数量集中在250到1,000之间。
- en: '![Normalization](img/00110.jpeg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![归一化](img/00110.jpeg)'
- en: 'By applying the log to the order quantity and doing the same kind of scatterplot,
    you can see the relationship much more clearly:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对订单数量取对数并执行类似的散点图，你可以更清楚地看到关系：
- en: '![Normalization](img/00111.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![归一化](img/00111.jpeg)'
- en: Note that taking the log typically does not change the relationship among the
    dependent and independent variables, so you can use it safely in replacement of
    the natural values in regressions.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，取对数通常不会改变因变量和自变量之间的关系，因此你可以在回归分析中安全地用它来替换自然值。
- en: 'If you go back to the solution in [Chapter 2](part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055
    "Chapter 2. AdventureWorks Regression"), *AdventureWorks Regression*, you can
    open up the regression project and add a new file called `Accord.Net4.fsx`. Copy
    and paste in the contents from `Accord.Net2.fsx`. Next, replace the data reader
    lines of code with this:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回到[第二章](part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055 "第二章。AdventureWorks
    回归")的解决方案，*AdventureWorks 回归*，你可以打开回归项目并添加一个名为`Accord.Net4.fsx`的新文件。将`Accord.Net2.fsx`中的内容复制并粘贴进来。接下来，用以下代码替换数据读取器的代码行：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Sending this to the REPL, we get the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 将此发送到REPL，我们得到以下结果：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Notice the change. We are taking the `log()` of our `x` variables. Also, notice
    that our `r2` slightly decreases. The reason for this is that although the log
    does not change the relationship among `AvgReviews`, it does impact how it relates
    to the other `x` variables and potentially the `y` variable. You can see, in this
    case, that it didn't do much.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意变化。我们正在对`x`变量取对数。同时，注意我们的`r2`略有下降。原因是尽管对数没有改变`AvgReviews`之间的关系，但它确实影响了它与其他`x`变量以及可能`y`变量的关系。你可以看到，在这种情况下，它并没有做什么。
- en: Besides using log, we can trim outliers. Going back to our graph, do you notice
    that lonely dot at 2.2 average order quantity/3.90 average review?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用对数，我们还可以修剪异常值。回到我们的图表，你注意到那个平均订单数量为2.2、平均评论为3.90的孤独点吗？
- en: '![Normalization](img/00112.jpeg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![归一化](img/00112.jpeg)'
- en: 'Looking at all of the other data points, we would expect that a 3.90 average
    review should have a 2.75 average order quantity at least. Although we might want
    to dive into the details to figure out what is going on, we''ll save that exercise
    for another day. Right now, what it is really doing is messing up our model. Indeed,
    the biggest criticism of regressions is that they are overly sensitive to outliers.
    Let''s look at a simple example. Go to [Chapter 2](part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055
    "Chapter 2. AdventureWorks Regression"), *AdventureWorks Regression*, regression
    project and create a new script, called `Accord5.fsx`. Copy the first part of
    the code from `Accord1.fsx` into it:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 观察所有其他数据点，我们预计平均评论为3.90应该至少有2.75的平均订单数量。尽管我们可能想深入了解以找出问题所在，但我们将其留到另一天。现在，它实际上正在破坏我们的模型。确实，回归分析的最大批评之一就是它们对异常值过于敏感。让我们看看一个简单的例子。转到[第2章](part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055
    "第2章。AdventureWorks回归")，*AdventureWorks回归*回归项目，创建一个新的脚本，命名为`Accord5.fsx`。将`Accord1.fsx`中的代码的第一部分复制到其中：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, let''s add a child prodigy who is bored with school so he has a low GPA.
    Add in a student with an age of 10, an IQ of 150, and a GPA of 1.0:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们加入一个对学校感到无聊的天才儿童，他的平均绩点（GPA）很低。加入一个10岁的学生，智商（IQ）为150，平均绩点（GPA）为1.0：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Sending the entire script to the REPL gives us the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将整个脚本发送到REPL，我们得到以下结果：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Notice what happens to our model. Our `r2` moves from 0.79 to 0.66 and our rmse
    climbs from 0.18 to 0.56! Holy cow, that's dramatic! As you can guess, how you
    deal with outliers will have a large impact on your model. If the intention of
    the model is to predict a majority of students' GPAs, we can safely remove the
    outlier because it's not typical. Another way of handling outliers is to use a
    model that does a better job of dealing with them.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们的模型发生了什么。我们的`r2`从0.79降至0.66，我们的均方根误差（rmse）从0.18升至0.56！天哪，这是多么戏剧性的变化！正如你所猜到的，你如何处理异常值将对你的模型产生重大影响。如果模型的目的是预测大多数学生的平均绩点，我们可以安全地移除这个异常值，因为它并不典型。另一种处理异常值的方法是使用一个在处理它们方面做得更好的模型。
- en: 'With that under our belts, let''s try it with real data. Add a new script file
    and call it `AccordDotNet6.fsx`. Copy and paste all of `AccordDotNet2.fsx` into
    it. Next, locate these lines:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握这些知识后，让我们用实际数据来尝试。添加一个新的脚本文件，并将其命名为`AccordDotNet6.fsx`。将`AccordDotNet2.fsx`中的所有内容复制并粘贴到其中。接下来，找到以下这些行：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Sending this to the REPL, we get the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些内容发送到REPL，我们得到以下结果：
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `r2` moves up from 0.35 to 0.37 and our rmse drops from 2.65 to 2.59\. Quite
    an improvement for removing one data point! Feel free to move this change over
    to the AdventureWorks project if you want. I am not going to walk you through
    it, but you now have the skills to do it independently. Dropping outliers is a
    very powerful way to make regressions more accurate, but there's a cost. Before
    we start dropping data elements that don't work from our model, we have to use
    some judgement. In fact, there are textbooks devoted to the science of what to
    do with outliers and missing data. We are not going to get into that in this book,
    other than acknowledge that the issue exists and to advise you to use some common
    sense when dropping elements.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`r2`从0.35升至0.37，我们的rmse从2.65降至2.59。删除一个数据点就有如此大的改进！如果你愿意，可以将这个更改应用到AdventureWorks项目中。我不会带你走这一步，但现在你有了独立完成它的技能。删除异常值是使回归分析更准确的一种非常强大的方法，但这也存在代价。在我们开始从模型中删除不起作用的数据元素之前，我们必须做出一些判断。事实上，有一些教科书专门讨论处理异常值和缺失数据的科学。在这本书中，我们不会深入探讨这个问题，只是承认这个问题存在，并建议你在删除元素时使用一些常识。'
- en: Scaling
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩放
- en: I want to acknowledge a common misperception about normalization and units of
    measure. You might notice that the different `x` variables have significantly
    different units of measure in [Chapter 2](part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055
    "Chapter 2. AdventureWorks Regression"), *AdventureWorks Regression*, and [Chapter
    3](part0029_split_000.html#RL0A2-a18db0be6c20485ba81f22e43ca13055 "Chapter 3. More
    AdventureWorks Regression"), *More AdventureWorks Regression*. In our examples,
    the Units of Customer Review is a 1-5 rating and the Price of Bikes is 0-10,000
    US dollars. You might think that comparing such a large range of numbers would
    adversely affect the model. Without going into details, you can be rest assured
    that regressions are immune to different units of measure.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我想承认关于归一化和度量单位的一个常见误解。你可能会注意到，在[第 2 章](part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055
    "第 2 章。AdventureWorks 回归")，*AdventureWorks 回归*和[第 3 章](part0029_split_000.html#RL0A2-a18db0be6c20485ba81f22e43ca13055
    "第 3 章。更多 AdventureWorks 回归")中，不同的 `x` 变量具有显著不同的度量单位。在我们的示例中，客户评论的单位是 1-5 的评分，自行车的价格是
    0-10,000 美元。你可能会认为比较这样大的数字范围会对模型产生不利影响。不深入细节，你可以放心，回归对不同的度量单位具有免疫力。
- en: 'However, other models (especially classification and clustering models like
    k-NN, k-means, and PCA) are impacted. When we created these kinds of models in
    [Chapter 6](part0040_split_000.html#164MG1-a18db0be6c20485ba81f22e43ca13055 "Chapter 6. AdventureWorks
    Redux – k-NN and Naïve Bayes Classifiers"), *AdventureWorks Redux – k-NN and Naïve
    Bayes Classifiers*, and [Chapter 7](part0045_split_000.html#1AT9A2-a18db0be6c20485ba81f22e43ca13055
    "Chapter 7. Traffic Stops and Crash Locations – When Two Datasets Are Better Than
    One"), *Traffic Stops and Crash Locations – When Two Datasets Are Better Than
    One*, we ran a risk that we were getting erroneous results because the data was
    not scaled. Fortunately, the features we selected, and the libraries we used (Numl.net
    and Accord), bailed us out. Numl.NET automatically scales input variables in all
    of the classification models. Depending on the type of model, Accord might scale
    for you. For example, in the PCA we wrote in [Chapter 7](part0045_split_000.html#1AT9A2-a18db0be6c20485ba81f22e43ca13055
    "Chapter 7. Traffic Stops and Crash Locations – When Two Datasets Are Better Than
    One"), *Traffic Stops and Crash Locations – When Two Datasets Are Better Than
    One*, we passed in an input parameter called `AnalysisMethod.Center` on this line:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，其他模型（尤其是分类和聚类模型，如 k-NN、k-means 和 PCA）会受到 影响。当我们创建这些类型的模型在[第 6 章](part0040_split_000.html#164MG1-a18db0be6c20485ba81f22e43ca13055
    "第 6 章。AdventureWorks Redux – k-NN 和朴素贝叶斯分类器")和[第 7 章](part0045_split_000.html#1AT9A2-a18db0be6c20485ba81f22e43ca13055
    "第 7 章。交通拦截和事故地点 – 当两个数据集比一个更好时")中时，*Traffic Stops and Crash Locations – When
    Two Datasets Are Better Than One*，我们面临的风险是得到错误的结果，因为数据没有缩放。幸运的是，我们选择的功能和使用的库（Numl.net
    和 Accord）帮助我们摆脱了困境。Numl.NET 自动对所有分类模型中的输入变量进行缩放。根据模型类型，Accord 可能会为你进行缩放。例如，在我们在[第
    7 章](part0045_split_000.html#1AT9A2-a18db0be6c20485ba81f22e43ca13055 "第 7 章。交通拦截和事故地点
    – 当两个数据集比一个更好时")中编写的 PCA 中，我们在这行代码中传递了一个名为 `AnalysisMethod.Center` 的输入参数：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This scales the input variables to the mean, which is good enough for our analysis.
    When we did the k-NN in [Chapter 6](part0040_split_000.html#164MG1-a18db0be6c20485ba81f22e43ca13055
    "Chapter 6. AdventureWorks Redux – k-NN and Naïve Bayes Classifiers"), *AdventureWorks
    Redux – k-NN and Naïve Bayes Classifiers*, using Accord, we did not scale the
    data because our two input variables were categorical (`MartialStatus` and `Gender`)
    with only two possibilities (married or not, male or female) and you only need
    to scale continuous variables or categorical variables with more than two values.
    If we had used a continuous variable or a three-factor categorical variable in
    the k-NN, we would have had to scale it.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输入变量缩放到平均值，这对我们的分析来说已经足够好了。当我们使用 Accord 在[第 6 章](part0040_split_000.html#164MG1-a18db0be6c20485ba81f22e43ca13055
    "第 6 章。AdventureWorks Redux – k-NN 和朴素贝叶斯分类器")中执行 k-NN 时，*AdventureWorks Redux
    – k-NN 和朴素贝叶斯分类器*，我们没有缩放数据，因为我们的两个输入变量是分类变量（`MartialStatus` 和 `Gender`），只有两种可能性（已婚或未婚，男或女），并且你只需要缩放连续变量或具有两个以上值的分类变量。如果我们使用了连续变量或三个因子的分类变量在
    k-NN 中，我们就必须对其进行缩放。
- en: 'Let''s walk through a quick example of scaling using Accord. Open up the `FeatureCleaning`
    solution from this chapter and add a new script file called `AccordKNN`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个使用 Accord 的快速缩放示例来了解一下。打开本章的 `FeatureCleaning` 解决方案，并添加一个名为 `AccordKNN`
    的新脚本文件：
- en: '![Scaling](img/00113.jpeg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![缩放](img/00113.jpeg)'
- en: 'Go into the NuGet Package Manager Console and enter this:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 进入NuGet包管理器控制台，输入以下内容：
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Go into the `AccordKNN.fsx` file and add the code we used in [Chapter 6](part0040_split_000.html#164MG1-a18db0be6c20485ba81f22e43ca13055
    "Chapter 6. AdventureWorks Redux – k-NN and Naïve Bayes Classifiers"), *AdventureWorks
    Redux* *–**k-NN and Naïve Bayes Classifiers*, for students who study and drink
    beer:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 进入`AccordKNN.fsx`文件，并添加我们在[第6章](part0040_split_000.html#164MG1-a18db0be6c20485ba81f22e43ca13055
    "第6章。AdventureWorks Redux – k-NN和朴素贝叶斯分类器")中使用的代码，*AdventureWorks Redux – k-NN和朴素贝叶斯分类器*，供那些学习和喝酒的学生使用：
- en: '[PRE19]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, let''s scale the data so that studying and drinking beer are equivalent.
    We are going to take the simplest methodology of scaling called *mean scaling*.
    Go back to the script and enter this:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将数据缩放，使得学习和喝酒相当。我们将采用最简单的缩放方法，称为*均值缩放*。回到脚本中，输入以下内容：
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'When you send this to the REPL, you will see the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将其发送到REPL时，你会看到以下内容：
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Notice that the inputs are now relative to their means. The person who studied
    five hours and drank one beer now studied 73% more than the average and drank
    41% less than the average. This k-NN model is now scaled and will give a better
    "apples to apples" comparison when used in practice.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，输入现在相对于它们的平均值。那个学习了五小时并喝了一杯啤酒的人现在比平均水平多学习了73%，比平均水平少喝了41%。这个k-NN模型现在已缩放，当实际应用时将提供更好的“苹果对苹果”的比较。
- en: Overfitting and cross validation
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过度拟合与交叉验证
- en: 'If you remember from Chapters 2, 3, and 4, one of the problems with our methodology
    when building models was that we were guilty of overfitting. Overfitting, the
    bane of predictive analytics, is what happens when we build a model that does
    a great job with past data but then falls apart when new data is introduced. This
    phenomenon is not just for data science; it happens a lot in our society: Professional
    athletes get lucrative contracts and then fail to live up to their prior performances;
    fund managers get hefty salary bumps because of last year''s performance, and
    the list goes on.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得第2、3和4章，我们在构建模型时遇到的一个问题是过度拟合。过度拟合，预测分析的祸害，是指当我们构建一个在历史数据上做得很好的模型，但在引入新数据时却崩溃的现象。这种现象不仅限于数据科学；在我们的社会中发生得很多：职业运动员获得丰厚的合同，却未能达到先前的表现；基金经理因为去年的表现而获得大幅加薪，等等。
- en: Cross validation – train versus test
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证 – 训练与测试
- en: 'Unlike the Yankees, who never seem to learn, our profession has learned from
    its mistakes and has a great, if imperfect, tool to combat overfitting. We use
    the methodology of train/test/eval to build several models and then select the
    best one not based on how well it did against an existing dataset, but how it
    does against data it has never seen before. To accomplish that, we take our source
    data, import it, clean it, and split it into two subsets: training and testing.
    We then build our model on the training set and, if it seems viable, apply our
    test data to the model. If the model is still valid, we can think about pushing
    it to production. This is represented graphically as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 与从不学习的洋基队不同，我们的职业从错误中吸取了教训，并拥有一个伟大的、尽管不完美的工具来对抗过拟合。我们使用训练/测试/评估的方法构建多个模型，然后选择最佳模型，而不是基于它对现有数据集的表现，而是基于它对之前未见过的数据的表现。为了实现这一点，我们取我们的源数据，导入它，清理它，并将其分成两个子集：训练和测试。然后我们在训练集上构建我们的模型，如果它看起来可行，就将测试数据应用到模型上。如果模型仍然有效，我们可以考虑将其推向生产。这可以用以下图形表示：
- en: '![Cross validation – train versus test](img/00114.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![交叉验证 – 训练与测试](img/00114.jpeg)'
- en: 'But there is one more step we can add. We can split our data several times
    and build new models to be validated. The actual splitting of the dataset is its
    own science, but typically each time the base dataset is split into **Training**
    and **Testing** subsets, the records are selected randomly. That means if you
    split your base data five times, you will have five completely different training
    and test subsets:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们还可以添加一个额外的步骤。我们可以将数据分割多次，并构建新的模型进行验证。实际分割数据集本身就是一门科学，但通常每次将基本数据集分割成**训练**和**测试**子集时，记录都是随机选择的。这意味着如果你将基本数据分割五次，你将拥有五个完全不同的训练和测试子集：
- en: '![Cross validation – train versus test](img/00115.jpeg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![交叉验证 – 训练与测试](img/00115.jpeg)'
- en: This kind of technique can be more important than the actual model selection.
    Both Accord and Numl do some kind of splitting under the hoods and in this book,
    we will trust that they are doing a good job. However, once you start working
    on models in the wild, you will want to dedicate a certain amount of time on every
    project for cross validation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术可能比实际模型选择更重要。Accord和Numl在底层都进行了一些分割，在这本书中，我们将相信它们正在做好这项工作。然而，一旦你开始在野外工作模型，你将希望在每个项目上投入一定的时间进行交叉验证。
- en: Cross validation – the random and mean test
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证 – 随机和平均测试
- en: 'Going back to our k-NN example of students that studied and drank beer, how
    do we know if we are predicting accurately? If we want to guess whether a student
    passed or not, we could just flip a coin: heads they pass, tails they fail. The
    assumption in our analysis is that the number of hours studying and the number
    of beers consumed have some kind of causality on the exam outcome. If our model
    does no better than a coin flip, then it is not a model worth using. Open up Visual
    Studio and go back to the `AccordKNN.fsx` file. At the bottom, enter in the following
    code:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们关于学生学习和喝酒的k-NN示例，我们如何知道我们是否预测准确？如果我们想猜测一个学生是否通过，我们只需抛硬币：正面通过，反面失败。我们分析中的假设是，学习的小时数和喝的啤酒数对考试成绩有一定的影响。如果我们的模型不如抛硬币，那么它不是一个值得使用的模型。打开Visual
    Studio并回到`AccordKNN.fsx`文件。在底部，输入以下代码：
- en: '[PRE22]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Sending this to the FSI, we get the following (your results will be different):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些发送到FSI，我们得到以下结果（你的结果将不同）：
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, let''s enter in some information about each student: the number of hours
    they studied and the number of beers they drank and run the unscaled k-NN on it:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们输入有关每个学生的信息：他们学习的小时数和他们喝的啤酒数，并运行未缩放的k-NN：
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Sending this to the REPL gives us the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些发送到REPL，我们得到以下结果：
- en: '[PRE25]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, let''s see how they actually did on the exam. Add this to the script:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看它们在考试中的实际表现。将以下内容添加到脚本中：
- en: '[PRE26]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Sending this to the FSI gives us the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些发送到FSI，我们得到以下结果：
- en: '[PRE27]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Combining these arrays together in a chart, will give us the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些数组组合在一起在图表中，我们将得到以下结果：
- en: '![Cross validation – the random and mean test](img/00116.jpeg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![交叉验证 – 随机和平均测试](img/00116.jpeg)'
- en: 'If we then scored how well the random test and k-NN did predicting the actual
    results, we can see that the random test correctly predicted the result 66% of
    the time and k-NN correctly predicted the result 100% of the time:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们评分随机测试和k-NN在预测实际结果方面的表现，我们可以看到随机测试正确预测结果66%的时间，而k-NN正确预测结果100%的时间：
- en: '![Cross validation – the random and mean test](img/00117.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![交叉验证 – 随机和平均测试](img/00117.jpeg)'
- en: Because our k-NN did better than the random coin flip, we can consider the model
    useful.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的k-NN比随机抛硬币做得更好，我们可以认为这个模型是有用的。
- en: 'This kind of yes/no random test works well when our model is a logistic regression
    or a classification model like k-NN, but what about when the dependent (*Y*) variable
    is a continuous value like in a linear regression? In that case, instead of using
    a random coin flip, we can plug in the mean of the known values. If the outcome
    predicts better than the mean, we probably have a good model. If it does worse
    than the mean, we need to rethink our model. For example, consider predicting
    average bike reviews from AdventureWorks:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这种是/否随机测试在我们的模型是逻辑回归或k-NN这样的分类模型时效果很好，但当我们依赖的(*Y*)变量是像线性回归中的连续值时怎么办？在这种情况下，我们不是使用随机抛硬币，而是可以插入已知值的平均值。如果结果预测比平均值好，我们可能有一个好的模型。如果它比平均值差，我们需要重新思考我们的模型。例如，考虑从AdventureWorks预测平均自行车评论：
- en: '![Cross validation – the random and mean test](img/00118.jpeg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![交叉验证 – 随机和平均测试](img/00118.jpeg)'
- en: 'When you compare the predicted to the actual (taking the absolute value to
    account for being both higher and lower) and then aggregate the results, you can
    see that our linear regression did a better job in predicting the rating than
    the mean:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将预测值与实际值（考虑到可能更高或更低）进行比较，然后汇总结果时，你可以看到我们的线性回归在预测评分方面做得比平均值更好：
- en: '![Cross validation – the random and mean test](img/00119.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![交叉验证 – 随机和平均测试](img/00119.jpeg)'
- en: If you are thinking we have already done something like this in Chapters 2 and
    3, you are right—this is the same concept as the RMSE.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为我们在第2章和第3章已经做过类似的事情，你是正确的——这与RMSE的概念相同。
- en: Cross validation – the confusion matrix and AUC
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证 – 混淆矩阵和AUC
- en: 'Going back to our k-NN example, imagine that we ran our k-NN against many students.
    Sometimes the k-NN guessed correctly, sometimes the k-NN did not. There are actually
    four possible outcomes:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的k-NN示例，想象一下我们对许多学生运行了k-NN。有时k-NN猜对了，有时k-NN没有。实际上有四种可能的结果：
- en: k-NN predicted that the student would pass and they did pass
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-NN预测学生会通过，他们确实通过了
- en: k-NN predicted that the student would fail and they did fail
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-NN预测学生会失败，他们确实失败了
- en: k-NN predicted that the student would pass and they failed
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-NN预测学生会通过，但他们失败了
- en: k-NN predicted that the student would fail and they passed
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-NN预测学生会失败，但他们通过了
- en: 'Each of these outcomes has a special name:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些结果都有一个特殊的名称：
- en: '**Predict Pass and Did Pass**: True Positive'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测通过且实际通过**：真阳性'
- en: '**Predict Fail and Did Fail**: True Negative'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测失败且实际失败**：真阴性'
- en: '**Predict Pass and Failed**: False Positive'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测通过但失败了**：假阳性'
- en: '**Predict Fail and Passed**: False Negative'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测失败但通过了**：假阴性'
- en: 'And in a chart format, it would look like this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 以图表形式，它看起来是这样的：
- en: '![Cross validation – the confusion matrix and AUC](img/00120.jpeg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![交叉验证 – 混淆矩阵和AUC](img/00120.jpeg)'
- en: Sometimes the False Positive is called a Type I error and the False Negative
    is called a Type II error.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，假阳性被称为I型错误，而假阴性被称为II型错误。
- en: 'If we ran the k-NN against 100 students, we could add values to that chart
    like this:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对100名学生运行k-NN，我们可以在图表中添加如下值：
- en: '![Cross validation – the confusion matrix and AUC](img/00121.jpeg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![交叉验证 – 混淆矩阵和AUC](img/00121.jpeg)'
- en: Reading this chart, 52 students passed the exam. Of that, we correctly predicted
    50 of them would pass, but we incorrectly predicted two of the passing students
    would fail. Similarly, 43 failed the exam (must have been a tough exam!), 40 of
    which we correctly predicted would fail, and three we incorrectly predicted would
    pass. This matrix is often called a *confusion matrix*.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读这张图表，52名学生通过了考试。其中，我们正确预测了50人会通过，但错误地预测了两个通过的学生会失败。同样，43名学生没有通过考试（肯定是一场艰难的考试！），其中我们正确预测了40人会失败，而三个我们错误地预测了会通过。这个矩阵通常被称为*混淆矩阵*。
- en: 'With this confusion matrix, we can then do some basic statistics like:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个混淆矩阵，我们可以进行一些基本的统计，例如：
- en: '*Accuracy = True Positives + True Negatives / Total Population = (50 + 40)
    / 100 = 90%*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*准确率 = 真阳性 + 真阴性 / 总人口 = (50 + 40) / 100 = 90%*'
- en: '*True Positive Rate (TPR) = True Positives / Total Positives = 50 / 52 = 96%*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*真阳性率 (TPR) = 真阳性 / 总阳性 = 50 / 52 = 96%*'
- en: '*False Negative Rate (FNR) = False Negatives / Total Positives = 2 / 52 = 4%*'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*假阴性率 (FNR) = 假阴性 / 总阳性 = 2 / 52 = 4%*'
- en: '*False Positive Rate (FPR) = False Positives / Total Negatives = 3 / 43 = 7%*'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*假阳性率 (FPR) = 假阳性 / 总阴性 = 3 / 43 = 7%*'
- en: '*True Negative Rate (TNR) = True Negatives / Total Negatives = 40 / 43 = 93%*'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*真阴性率 (TNR) = 真阴性 / 总阴性 = 40 / 43 = 93%*'
- en: (Note that TPR is sometimes called Sensitivity, the FNR is sometimes called
    Miss Rate, the False Positive Rate is sometimes called Fall-Out and the TNR is
    sometimes called Specificity.)
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: （注意，TPR有时被称为灵敏度，FNR有时被称为漏报率，假阳性率有时被称为逃逸率，而TNR有时被称为特异性。）
- en: '*Positive Likelihood Ratio (LR+) = TPR / FPR = 96 % / 1 – 93% = 13.8*'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*阳性似然比 (LR+) = TPR / FPR = 96% / (1 – 93%) = 13.8*'
- en: '*Negative Likelihood Ratio (LR-) = FNR / TNR = 4% / 93% = .04*'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*阴性似然比 (LR-) = FNR / TNR = 4% / 93% = 0.04*'
- en: '*Diagnostic Odds Ratio (DOR) = LR+ / LR- = 33.3*'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*诊断优势比 (DOR) = LR+ / LR- = 33.3*'
- en: Since the DOR is greater than 1, we know that the model is working well.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DOR大于1，我们知道模型运行良好。
- en: 'Putting this into code, we could handwrite these formulas, but Accord.Net has
    already taken care of this for us. Go back into Visual Studio and open `AccordKNN.fsx`.
    At the bottom, enter in this code:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些放入代码中，我们可以手动写下这些公式，但Accord.Net已经为我们处理好了。回到Visual Studio，打开`AccordKNN.fsx`。在底部，输入以下代码：
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'On the next line, type `confusionMatrix` and hit dot to see all of the properties
    that are available to you:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一行，输入`confusionMatrix`并按点号以查看所有可用的属性：
- en: '![Cross validation – the confusion matrix and AUC](img/00122.jpeg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![交叉验证 – 混淆矩阵和AUC](img/00122.jpeg)'
- en: 'This is a very useful class indeed. Let''s select the odds ratio:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实是一个非常实用的课程。让我们选择优势比：
- en: '[PRE29]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'And then send the entire code block to the FSI:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将整个代码块发送到FSI：
- en: '[PRE30]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Since our k-NN is was 100% accurate, we got an odds ratio of infinity (and beyond).
    In a real-world model, the odds ratio would obviously be much lower.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的k-NN是100%准确的，我们得到了一个无限大的优势比（甚至更多）。在现实世界的模型中，优势比显然会低得多。
- en: Cross validation – unrelated variables
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证 – 无关变量
- en: There is one more technique that I want to cover to for cross-validation—adding
    in unrelated variables and seeing the impact on the model. If your model is truly
    useful, it should be able to handle extraneous "noise" variables without significantly
    impacting the model's result. As we saw in [Chapter 2](part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055
    "Chapter 2. AdventureWorks Regression"), *AdventureWorks Regression*, any additional
    variable will have a positive impact on most models, so this is a measure of degree.
    If adding an unrelated variable makes the model seem much more accurate, then
    the model itself is suspect. However, if the extra variable only has a marginal
    impact, then our model can be considered solid.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个技术我想介绍，用于交叉验证——添加无关变量并观察对模型的影响。如果你的模型真正有用，它应该能够处理额外的“噪声”变量，而不会对模型的结果产生重大影响。正如我们在[第二章](part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055
    "第二章. AdventureWorks 回归")中看到的，*AdventureWorks 回归*，任何额外的变量都会对大多数模型产生积极影响，所以这是一个程度的衡量。如果添加一个无关变量使模型看起来更加准确，那么模型本身就有问题。然而，如果额外变量只有轻微的影响，那么我们的模型可以被认为是可靠的。
- en: 'Let''s see this in action. Go back into `AccordKNN.fsx` and add the following
    code at the bottom:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看实际效果。回到 `AccordKNN.fsx` 并在底部添加以下代码：
- en: '[PRE31]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: I added a third variable that represents each student's zodiac symbol (1.0 =
    Aquarius, 2.0 = Pisces, and so on). When I passed in the same test input (also
    with random zodiac symbols), the predictions were the same as the original k-NN.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我添加了一个代表每个学生的星座符号的第三个变量（1.0 = 水瓶座，2.0 = 双鱼座，等等）。当我传入相同的测试输入（也带有随机的星座符号）时，预测结果与原始的
    k-NN 相同。
- en: '[PRE32]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We can conclude that the extra variable, although it had an impact at some point
    in the modeling process, was not important enough to alter our original model.
    We can then use this model with a higher degree of confidence.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得出结论，尽管额外变量在建模过程中某个时刻产生了影响，但它并不足以改变我们的原始模型。然后我们可以用更高的信心使用这个模型。
- en: Summary
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter is a bit different than other machine learning books that you might
    have read because it did not introduce any new models, but instead concentrated
    on the dirty job on gathering, cleaning, and selecting your data. Although not
    as glamorous, it is absolutely essential that you have a firm grasp on these concepts
    because they will often make or break a project. In fact, many projects spend
    over 90% of their time acquiring data, cleaning the data, selecting the correct
    features, and building the appropriate cross-validation methodology. In this chapter,
    we looked at cleaning data and how to account for missing and incomplete data.
    Next, we looked at collinearity and normalization. Finally, we wrapped up with
    some common cross-validation techniques.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 本章与其他你可能读过的机器学习书籍略有不同，因为它没有介绍任何新的模型，而是专注于脏活累活——收集、清理和选择你的数据。虽然不那么光鲜，但绝对有必要掌握这些概念，因为它们往往会使项目成功或失败。事实上，许多项目花费超过
    90% 的时间在获取数据、清理数据、选择正确的特征和建立适当的交叉验证方法上。在本章中，我们探讨了数据清理以及如何处理缺失和不完整的数据。接下来，我们探讨了多重共线性化和归一化。最后，我们总结了常见的交叉验证技术。
- en: We are going to apply all of these techniques in the coming chapters. Up next,
    let's go back to the AdventureWorks company and see if we can help them improve
    their production process using a machine learning model based on how the human
    brain works.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中应用所有这些技术。接下来，让我们回到 AdventureWorks 公司，看看我们是否可以用基于人类大脑工作原理的机器学习模型帮助他们改进生产流程。
