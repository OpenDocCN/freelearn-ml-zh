- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Exploring Diffusion Models for Synthetic Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索用于合成数据的扩散模型
- en: This chapter introduces you to diffusion models, which are cutting-edge approaches
    to synthetic data generation. We will highlight the pros and cons of this novel
    synthetic data generation approach. This will help you to make informed decisions
    about the best methods to utilize for your own problems. We will highlight the
    opportunities and challenges of diffusion models. Moreover, this chapter is enriched
    with a comprehensive practical example, providing hands-on experience in both
    generating and effectively employing synthetic data for a real-world ML application.
    As you go through diffusion models, you will learn about the main ethical issues
    and concerns around utilizing this synthetic data approach in practice. In addition
    to that, we will review some state-of-the-art research on this topic. Thus, this
    chapter will equip you with the necessary knowledge to thoroughly understand this
    novel synthetic data generation approach.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向您介绍扩散模型，这是合成数据生成的前沿方法。我们将强调这种新颖的合成数据生成方法的优缺点。这将帮助您根据自己的问题做出明智的决定，选择最佳的方法。我们将强调扩散模型的机会和挑战。此外，本章还包含一个全面的实践示例，提供生成和有效利用合成数据用于实际机器学习应用的动手经验。随着您对扩散模型的了解，您将了解在实践中使用这种合成数据方法的主要道德问题和关注点。此外，我们还将回顾一些关于这个主题的最新研究。因此，本章将为您提供必要的知识，以彻底理解这种新颖的合成数据生成方法。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: An introduction to diffusion models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散模型简介
- en: Diffusion models – the pros and cons
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散模型 – 优点与缺点
- en: Hands-on diffusion models in practice
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实践中的动手扩散模型
- en: Diffusion models – ethical issues
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散模型 – 道德问题
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Any code used in this chapter will be available under the corresponding chapter
    folder at this book’s GitHub repository: [https://github.com/PacktPublishing/Synthetic-Data-for-Machine-Learning](https://github.com/PacktPublishing/Synthetic-Data-for-Machine-Learning).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的任何代码都将可在本书GitHub仓库的相应章节文件夹中找到：[https://github.com/PacktPublishing/Synthetic-Data-for-Machine-Learning](https://github.com/PacktPublishing/Synthetic-Data-for-Machine-Learning)。
- en: An introduction to diffusion models
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩散模型简介
- en: In this section, we will explore diffusion models. We will compare them to **Variational
    Autoencoders** (**VAEs**) and **Generative Adversarial Networks** (**GANs**),
    which we covered in [*Chapter 7*](B18494_07.xhtml#_idTextAnchor120). This will
    help you to gain a holistic and comprehensive understanding of generative models.
    Additionally, it will make comparing and contrasting the architectures, training
    procedures, and data flow of these methods straightforward. Furthermore, we will
    also learn how to train a typical diffusion model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨扩散模型。我们将将其与我们在[*第7章*](B18494_07.xhtml#_idTextAnchor120)中介绍过的**变分自编码器**（**VAEs**）和**生成对抗网络**（**GANs**）进行比较。这将帮助您获得对生成模型的全面和综合理解。此外，这将使比较和对比这些方法的架构、训练过程和数据流变得简单。此外，我们还将学习如何训练一个典型的扩散模型。
- en: '**Diffusion Models** (**DMs**) are generative models that were recently proposed
    as a clever solution to generate images, audio, videos, time series, and texts.
    DMs are excellent at modeling complex probability distributions, structures, temporal
    dependencies, and correlations in data. The initial mathematical model behind
    DMs was first proposed and applied in the field of statistical mechanics to study
    the random motion of particles in gases and liquids. As we will see later, it
    is essential and crucial to learn about DMs, as they are powerful generative models
    that can usually generate higher-quality and more privacy-preserving synthetic
    data compared to other approaches. Additionally, DMs rely on strong mathematical
    and theoretical foundations.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩散模型**（**DMs**）是一种生成模型，最近被提出作为一种聪明的解决方案来生成图像、音频、视频、时间序列和文本。DMs在建模复杂概率分布、数据结构、时间依赖性和相关性方面表现出色。DMs背后的初始数学模型最初是在统计力学领域提出的，并应用于研究气体和液体中粒子的随机运动。正如我们稍后将看到的，了解DMs至关重要，因为它们是强大的生成模型，通常可以生成比其他方法更高质量和更保护隐私的合成数据。此外，DMs依赖于强大的数学和理论基础。'
- en: One of the first works to show that DMs can be utilized to generate photorealistic
    images was *Denoising Diffusion Probabilistic Models* ([https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)),
    which was proposed by researchers from UC Berkeley. This pioneering work was followed
    by another work by OpenAI titled *Diffusion Models Beat GANs on Image Synthesis*
    ([https://arxiv.org/pdf/2105.05233.pdf](https://arxiv.org/pdf/2105.05233.pdf)),
    showing that DMs are better at generating photorealistic synthetic images. Then,
    other researchers started to explore the potential of these DMs in different fields
    and compare them to VAEs and GANs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 首先展示DMs可以用于生成逼真图像的工作之一是*去噪扩散概率模型*([https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239))，该模型由加州大学伯克利分校的研究者提出。这项开创性工作随后被OpenAI的另一篇工作所跟进，题为*扩散模型在图像合成上击败GANs*([https://arxiv.org/pdf/2105.05233.pdf](https://arxiv.org/pdf/2105.05233.pdf))，表明DMs在生成逼真的合成图像方面更胜一筹。然后，其他研究者开始探索这些DMs在不同领域的潜力，并将它们与VAEs和GANs进行比较。
- en: '**Variational Autoencoders** (**VAEs**) are one of the earliest solutions for
    generating synthetic data. They are based on using an encoder to encode data from
    a high-dimensional space (such as RGB images) into a latent low-dimensional space.
    Then, the decoder is used to reconstruct these encoded samples from the latent
    space to the original high-dimensional space. In the training process, the VAE
    is forced to minimize the loss between the original training sample and the reconstructed
    one by the decoder. Assuming the model was trained on a sufficient number of training
    samples, it can then be used to generate new synthetic data by sampling points
    from the latent space and using the decoder to decode them, from the latent low-dimensional
    space to the high-dimensional one, as shown in *Figure 9**.1*.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**变分自编码器**（**VAEs**）是生成合成数据最早的解决方案之一。它们基于使用编码器将数据从高维空间（如RGB图像）编码到潜在的低维空间。然后，解码器用于将这些编码样本从潜在空间重建到原始的高维空间。在训练过程中，VAE被强制通过解码器最小化原始训练样本与重建样本之间的损失。假设模型在足够的训练样本上进行了训练，那么它可以用来通过从潜在空间采样点并使用解码器将它们解码到高维空间来生成新的合成数据，如图*图9**.1*所示。'
- en: The training process of DMs
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DMs的训练过程
- en: DMs are used to generate synthetic data with a distribution close to the probability
    distribution of the training data. Thus, an important task is to learn the distribution
    of our training data and then leverage our knowledge of the real data to generate
    an unlimited number of synthetic data samples. Usually, we would want to generate
    high-quality and diverse synthetic data using a fast-generation method. However,
    each generation method has its own advantages and disadvantages, as we will see
    later on.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: DMs（数据生成模型）用于生成与训练数据概率分布接近的合成数据。因此，一个重要的任务是学习我们训练数据的分布，然后利用我们对真实数据的了解来生成无限数量的合成数据样本。通常，我们希望使用快速生成方法生成高质量且多样化的合成数据。然而，每种生成方法都有其自身的优缺点，正如我们稍后将看到的。
- en: As illustrated in *Figure 9**.1*, given a training image, ![](img/B18494_F09_001.png),
    from the real data, the DM adds **Gaussian noise** to this image to become ![](img/B18494_F09_002.png).
    The process is repeated until the image simply becomes an image of random noise,
    ![](img/B18494_F09_003.png). This process is called **forward diffusion**. Following
    this, the model starts the denoising process, in which the DM takes the random
    noise, ![](img/B18494_F09_004.png), and reverses the previous process (i.e., forward
    diffusion) to reconstruct the training image. This process is known as **reverse
    diffusion**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图9**.1*所示，给定一个训练图像，来自真实数据，DM向该图像添加**高斯噪声**以形成![](img/B18494_F09_002.png)。这个过程会重复进行，直到图像仅仅变成随机噪声的图像，![](img/B18494_F09_003.png)。这个过程被称为**正向扩散**。随后，模型开始去噪过程，在这个过程中，DM接受随机噪声，![](img/B18494_F09_004.png)，并逆转之前的过程（即正向扩散）以重建训练图像。这个过程被称为**反向扩散**。
- en: '![Figure 9.1 – The training process and architectures of the main generative
    models – VAEs, GANs, and DMs](img/Figure_09_01_B18494.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 9.1 – 主要生成模型（VAEs、GANs和DMs）的训练过程和架构 –](img/Figure_09_01_B18494.jpg)'
- en: Figure 9.1 – The training process and architectures of the main generative models
    – VAEs, GANs, and DMs
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – 主要生成模型（VAEs、GANs和DMs）的训练过程和架构
- en: As we can see from *Figure 9**.1*, the forward diffusion process is a **Markov
    chain**. Each step of the process is a stochastic event, and each event depends
    only on the previous event or state. Thus, they form a sequence of stochastic
    events and, consequently, a Markov chain.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图9**.1*所示，前向扩散过程是一个**马尔可夫链**。过程的每一步都是一个随机事件，每个事件只依赖于前一个事件或状态。因此，它们形成了一系列随机事件，从而构成了一个马尔可夫链。
- en: A key idea of the DMs’ training process is using a neural network such as *U-Net*
    ([https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)) to predict
    the amount of noise that was added to a given noisy image from the training data.
    This is crucial to reverse the noising process and learn how to generate a synthetic
    sample given random noise.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: DMs训练过程中的一个关键思想是使用像*U-Net*（[https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)）这样的神经网络来预测添加到给定噪声图像中的噪声量。这对于反转噪声过程并学习如何根据随机噪声生成合成样本至关重要。
- en: When the diffusion model converges after a successful training process, we can
    give it random noise, ![](img/B18494_F09_005.png), and the DM, using the reverse
    diffusion path, will give us a synthetic sample based on the provided ![](img/B18494_F09_006.png).
    Thus, we can now generate an unlimited number of new synthetic samples from the
    same training data probability distribution.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功训练过程之后，当扩散模型收敛时，我们可以给它添加随机噪声，![](img/B18494_F09_005.png)，然后DM，通过反向扩散路径，将基于提供的![](img/B18494_F09_006.png)给出一个合成样本。因此，我们现在可以从相同的训练数据概率分布中生成无限数量的新合成样本。
- en: Please note the number of diffusion steps, ![](img/B18494_F09_007.png), in the
    noising/denoising process depends on how smooth you want the training process
    to be. In other words, a higher value of ![](img/B18494_F09_008.png) means less
    abrupt and more gradual noise will be added in the training process. Thus, the
    model’s weights will update steadily and the optimization loss will decrease smoothly.
    However, higher values of ![](img/B18494_F09_009.png) will make the process slower
    (usually, ![](img/B18494_F09_010.png) ).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在噪声/去噪过程中的扩散步骤的数量，![](img/B18494_F09_007.png)，取决于你希望训练过程有多平滑。换句话说，![](img/B18494_F09_008.png)的更高值意味着在训练过程中将添加更少突然和更多渐进的噪声。因此，模型的权重将稳步更新，优化损失将平滑下降。然而，![](img/B18494_F09_009.png)的更高值将使过程变慢（通常，![](img/B18494_F09_010.png)）。
- en: Applications of DMs
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DMs的应用
- en: DMs have been utilized for a wide range of applications in various domains such
    as computer vision, natural language processing, finance, and healthcare. Let’s
    briefly discuss some of these applications.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: DMs已在计算机视觉、自然语言处理、金融和医疗保健等各个领域的广泛应用中得到利用。让我们简要讨论一些这些应用。
- en: '**Computer vision**:'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算机视觉**：'
- en: '**Image generation**: Generating diverse photorealistic images is one of the
    ultimate aims of DMs. However, there is usually a trade-off between diversity
    and photorealism. Thus, to maintain good fidelity, the DMs are usually guided
    or conditioned on textual data. For more information, please refer to *GLIDE:
    Towards Photorealistic Image Generation and* *Editing with Text-Guided Diffusion
    Models* ([https://arxiv.org/abs/2112.10741](https://arxiv.org/abs/2112.10741)).
    As you can imagine, the generated synthetic images can be used for various applications,
    including data augmentation, creative art, prototyping, and visualization.'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像生成**：生成多样化的逼真图像是DMs的最终目标之一。然而，多样性和逼真性之间通常存在权衡。因此，为了保持良好的保真度，DMs通常在文本数据的引导或条件下进行。有关更多信息，请参阅*GLIDE：通过文本引导的扩散模型实现逼真图像生成和编辑*([https://arxiv.org/abs/2112.10741](https://arxiv.org/abs/2112.10741))。正如你可以想象的那样，生成的合成图像可以用于各种应用，包括数据增强、创意艺术、原型设计和可视化。'
- en: '**Video prediction**: Predicting the next frame has many useful applications,
    as it facilitates predicting future events, which is essential for planning and
    decision-making in fields such as robotics. Simultaneously, it has huge applications
    in fields such as surveillance and security. Video prediction ML models can be
    utilized to anticipate and forecast possible threats, hazards, and potential risks.
    Additionally, predicting future frames accurately can be utilized to generate
    synthetic videos to complement real training data. For more details, refer to
    *Diffusion Models for Video Prediction and Infilling* ([https://arxiv.org/pdf/2206.07696.pdf](https://arxiv.org/pdf/2206.07696.pdf)),
    *Video Diffusion Models* ([https://arxiv.org/pdf/2204.03458.pdf](https://arxiv.org/pdf/2204.03458.pdf)),
    and *Imagen Video: High-Definition Video Generation with Diffusion* *Models* ([https://arxiv.org/pdf/2210.02303.pdf](https://arxiv.org/pdf/2210.02303.pdf)).'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视频预测**：预测下一帧有许多有用的应用，因为它有助于预测未来事件，这对于机器人等领域的规划和决策至关重要。同时，它在监控和安全等领域也有巨大的应用。视频预测ML模型可以用来预测和预测可能的威胁、危害和潜在风险。此外，准确预测未来帧可以用来生成合成视频，以补充真实训练数据。更多详情，请参阅*扩散模型在视频预测和填充中的应用*
    ([https://arxiv.org/pdf/2206.07696.pdf](https://arxiv.org/pdf/2206.07696.pdf))、*视频扩散模型*
    ([https://arxiv.org/pdf/2204.03458.pdf](https://arxiv.org/pdf/2204.03458.pdf))和*Imagen
    Video：使用扩散模型的超高清视频生成* *模型* ([https://arxiv.org/pdf/2210.02303.pdf](https://arxiv.org/pdf/2210.02303.pdf))。'
- en: '**Image inpainting**: This is simply filling in or restoring missing or damaged
    parts in images. It is a fundamental task in areas such as historical archiving,
    privacy protection, and entertainment. Furthermore, it has also been recently
    utilized for synthetic data generation. For example, DMs were utilized recently
    to generate synthetic brain MRIs, with the ability to control tumoral and non-tumoral
    tissues. It was shown that using generated synthetic data can boost performance
    considerably. For more details, please refer to *Multitask Brain Tumor Inpainting
    with Diffusion Models: A Methodological Report* ([https://arxiv.org/ftp/arxiv/papers/2210/2210.12113.pdf](https://arxiv.org/ftp/arxiv/papers/2210/2210.12113.pdf))
    and *RePaint: Inpainting using Denoising Diffusion Probabilistic* *Models* ([https://arxiv.org/abs/2201.09865](https://arxiv.org/abs/2201.09865)).'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像修复**：这仅仅是填充或恢复图像中缺失或损坏的部分。它在历史存档、隐私保护和娱乐等领域是一个基本任务。此外，它最近也被用于合成数据生成。例如，最近利用DMs（扩散模型）生成合成脑部MRI，能够控制肿瘤和非肿瘤组织。研究表明，使用生成的合成数据可以显著提高性能。更多详情，请参阅*使用扩散模型的多任务脑肿瘤修复：方法报告*
    ([https://arxiv.org/ftp/arxiv/papers/2210/2210.12113.pdf](https://arxiv.org/ftp/arxiv/papers/2210/2210.12113.pdf))和*RePaint：使用去噪扩散概率*
    *模型* 进行修复 ([https://arxiv.org/abs/2201.09865](https://arxiv.org/abs/2201.09865))。'
- en: '**Image colorization**: This is the task of transferring grayscale images into
    colored ones. For example, this is essential to improve the photorealism of historical
    or old photographs. Adding color is important to make these photos more appealing
    and more emotionally engaging. DMs were shown to perform very well under this
    task. For further information, please read *Palette: Image-to-Image Diffusion*
    *Models* ([https://arxiv.org/pdf/2111.05826.pdf](https://arxiv.org/pdf/2111.05826.pdf)).'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像着色**：这是将灰度图像转换为彩色图像的任务。例如，这对于提高历史或旧照片的逼真度至关重要。添加颜色对于使这些照片更具吸引力并更具情感吸引力很重要。DMs（扩散模型）在此任务中表现出色。更多信息，请阅读*Palette：图像到图像扩散*
    *模型* ([https://arxiv.org/pdf/2111.05826.pdf](https://arxiv.org/pdf/2111.05826.pdf))。'
- en: '**Natural** **language processing**:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言处理**：'
- en: '**Text generation**: Virtual assistants, chatbots, and similar conversational
    text-based systems rely on text generation. DMs have been utilized recently for
    this task to improve the quality and diversity, as they are more capable of capturing
    complex distributions. For an example, please refer to *DiffuSeq: Sequence to
    Sequence Text Generation with Diffusion* *Models* ([https://arxiv.org/abs/2210.08933](https://arxiv.org/abs/2210.08933)).'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本生成**：虚拟助手、聊天机器人和类似的基于对话的文本系统依赖于文本生成。最近，为了提高质量和多样性，已经利用了DMs（扩散模型）来完成这项任务，因为它们更能捕捉复杂的分布。例如，请参阅*DiffuSeq：使用扩散模型的序列到序列文本生成*
    *模型* ([https://arxiv.org/abs/2210.08933](https://arxiv.org/abs/2210.08933))。'
- en: '**Text-to-speech synthesis**: This is the process of transforming text into
    audio. While it has many applications in fields such as **Human-Computer Interaction**
    (**HCI**), education and learning, and video games, it has been recently utilized
    to make textual content accessible to individuals with visual impairment. For
    more details about DM and text-to-speech synthesis, please refer to *Diff-TTS:
    A Denoising Diffusion Model for Text-to-Speech* ([https://arxiv.org/pdf/2104.01409.pdf](https://arxiv.org/pdf/2104.01409.pdf))
    and *Prodiff: Progressive fast diffusion model for high-quality text-to-speech*
    ([https://dl.acm.org/doi/abs/10.1145/3503161.3547855](https://dl.acm.org/doi/abs/10.1145/3503161.3547855)).
    Additionally, for a survey of recent text-to-speech DM-based methods, please refer
    to *A Survey on Audio Diffusion Models: Text to Speech Synthesis and Enhancement
    in Generative* *AI* ([https://arxiv.org/pdf/2303.13336.pdf](https://arxiv.org/pdf/2303.13336.pdf)).'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本到语音合成**：这是将文本转换为音频的过程。虽然它在人机交互（**HCI**）、教育和学习、视频游戏等领域有许多应用，但最近它被用来使文本内容对视力受损者可访问。有关扩散模型和文本到语音合成的更多详细信息，请参阅*Diff-TTS：用于文本到语音的降噪扩散模型*
    ([https://arxiv.org/pdf/2104.01409.pdf](https://arxiv.org/pdf/2104.01409.pdf))和*Prodiff：用于高质量文本到语音的渐进式快速扩散模型*
    ([https://dl.acm.org/doi/abs/10.1145/3503161.3547855](https://dl.acm.org/doi/abs/10.1145/3503161.3547855))。此外，关于基于文本到语音扩散模型的近期方法的综述，请参阅*A
    Survey on Audio Diffusion Models: Text to Speech Synthesis and Enhancement in
    Generative AI* ([https://arxiv.org/pdf/2303.13336.pdf](https://arxiv.org/pdf/2303.13336.pdf))。'
- en: '**Text-driven image generation**: This is another promising field where the
    aim is to generate visual content, such as images based on textual input. It has
    various applications in content generation, marketing, data augmentation, and
    assisted data-labeling tools. As expected, DMs are excellent at modeling complex
    data distribution and very powerful at generating diverse and appealing images.
    Thus, they have been utilized for text-driven image generation. For more details,
    please refer to *Text2Human: text-driven controllable human image* *generation*
    ([https://dl.acm.org/doi/abs/10.1145/3528223.3530104](https://dl.acm.org/doi/abs/10.1145/3528223.3530104)).'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本驱动图像生成**：这是另一个有前景的领域，其目标是根据文本输入生成视觉内容，例如图像。它在内容生成、营销、数据增强和辅助数据标注工具等方面有各种应用。正如预期的那样，扩散模型（DMs）在建模复杂数据分布方面表现出色，并且在生成多样化和吸引人的图像方面非常强大。因此，它们被用于文本驱动图像生成。有关更多详细信息，请参阅*Text2Human：文本驱动的可控人类图像生成*
    ([https://dl.acm.org/doi/abs/10.1145/3528223.3530104](https://dl.acm.org/doi/abs/10.1145/3528223.3530104))。'
- en: '**Other applications**:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**其他应用**：'
- en: '**Privacy in healthcare**: Real **Electronic Health Records** (**EHRs**) of
    patients carry rich and very useful information that ML models can leverage to
    help in disease diagnosis, predictive analytics, decision-making, and the optimization
    and management of resources. DMs were shown to generate high-quality and large-scale
    EHRs that can be leveraged in enormous applications, such as ML model training,
    healthcare research, and medical education. To delve into more details, please
    read *MedDiff: Generating Electronic Health Records using Accelerated Denoising
    Diffusion* *Model* ([https://arxiv.org/pdf/2302.04355.pdf](https://arxiv.org/pdf/2302.04355.pdf)).'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医疗保健中的隐私**：患者的真实**电子健康记录**（**EHRs**）包含丰富且非常有用的信息，这些信息可以被机器学习模型利用，以帮助疾病诊断、预测分析、决策制定以及资源和管理的优化。扩散模型已被证明可以生成高质量和大规模的EHRs，这些EHRs可以在包括机器学习模型训练、医疗保健研究和医学教育在内的巨大应用中发挥作用。要深入了解更多细节，请阅读*MedDiff：使用加速降噪扩散模型生成电子健康记录*
    ([https://arxiv.org/pdf/2302.04355.pdf](https://arxiv.org/pdf/2302.04355.pdf))。'
- en: '**Anomaly detection**: This is the task of detecting or identifying patterns
    and instances that are not in line with the expected behavior and distribution.
    It has a myriad of applications in cybersecurity, fraud detection, telecommunication,
    and manufacturing. DMs are usually robust to noise and more stable, which makes
    them ideal for these applications. For an example and more details about utilizing
    DMs for anomaly detection in healthcare, please refer to *Diffusion models for
    medical anomaly* *detection* ([https://link.springer.com/chapter/10.1007/978-3-031-16452-1_4](https://link.springer.com/chapter/10.1007/978-3-031-16452-1_4)).'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常检测**：这是检测或识别不符合预期行为和分布的模式和实例的任务。它在网络安全、欺诈检测、电信和制造等领域有无数的应用。扩散模型通常对噪声具有鲁棒性且更稳定，这使得它们非常适合这些应用。有关利用扩散模型进行医疗异常检测的示例和更多详细信息，请参阅*Diffusion
    models for medical anomaly detection* ([https://link.springer.com/chapter/10.1007/978-3-031-16452-1_4](https://link.springer.com/chapter/10.1007/978-3-031-16452-1_4))。'
- en: '**Text-to-motion**: Generating animation or motion from textual input has many
    applications in the training, education, media, and entertainment sectors. DMs
    have shown promising results by producing high-quality animations of human motion.
    For more details, please refer to *Human Motion Diffusion* *Model* ([https://arxiv.org/abs/2209.14916](https://arxiv.org/abs/2209.14916)).'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本到运动**：从文本输入生成动画或运动在培训、教育、媒体和娱乐领域有许多应用。DMs通过生成高质量的人体运动动画显示出有希望的结果。更多详情请参阅*Human
    Motion Diffusion* *模型* ([https://arxiv.org/abs/2209.14916](https://arxiv.org/abs/2209.14916))。'
- en: Now that we have an idea of the domains where diffusion models are used, in
    the next section, we closely examine the pros and cons of DMs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了扩散模型应用的一些领域，在下一节中，我们将仔细检查DMs的优缺点。
- en: Diffusion models – the pros and cons
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩散模型——优缺点
- en: In this section, you will learn about and examine the main pros and cons of
    using DMs for synthetic data generation. This will help you to weigh the advantages
    and disadvantages of each synthetic data generation method. Consequently, it will
    give you the wisdom to select the best approach for your own problems.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将了解并检查使用DMs进行合成数据生成的主要优缺点。这将帮助您权衡每种合成数据生成方法的优缺点。因此，这将为您选择最适合您自己问题的最佳方法提供智慧。
- en: As we learned in [*Chapter 7*](B18494_07.xhtml#_idTextAnchor120), GANs work
    very well for certain applications, such as style transfer and image-to-image
    translation, but they are usually very hard to train and unstable. Additionally,
    the generated synthetic samples are usually less diverse and photorealistic. Conversely,
    recent papers have shown that DM-based synthetic data generation approaches surpass
    GANs on many benchmarks. For more details, please refer to *Diffusion Models Beat
    GANs on Image Synthesis* ([https://arxiv.org/pdf/2105.05233.pdf](https://arxiv.org/pdf/2105.05233.pdf)).
    Like any other synthetic data generation approach, DMs have pros and cons. Thus,
    you need to consider them carefully for your particular application or problem.
    Then, you can select the best approach to generate the synthetic data that you
    want. With that aim in mind, we will examine the key advantages and disadvantages
    of using DMs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第7章*](B18494_07.xhtml#_idTextAnchor120)中学到的，GANs在特定应用（如风格迁移和图像到图像翻译）中表现良好，但它们通常很难训练且不稳定。此外，生成的合成样本通常多样性较低且不够逼真。相反，最近的研究表明，基于DM的合成数据生成方法在许多基准测试上超过了GANs。更多详情请参阅*Diffusion
    Models Beat GANs on Image Synthesis* ([https://arxiv.org/pdf/2105.05233.pdf](https://arxiv.org/pdf/2105.05233.pdf))。像任何其他合成数据生成方法一样，DMs也有其优缺点。因此，您需要仔细考虑它们对您特定应用或问题的影响。然后，您可以选择最佳方法来生成您想要的合成数据。带着这个目标，我们将检查使用DMs的关键优势和劣势。
- en: The pros of using DMs
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用DMs的优点
- en: 'In general, DMs are excellent at modeling complex data probability distributions
    and capturing temporal dependencies and hidden patterns. This is possible with
    DMs because they use a diffusion process to model data distributions, using a
    sequence of conditional distributions. Thus, we can highlight the main strengths
    and merits of DMs as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，DMs在建模复杂数据概率分布、捕捉时间依赖性和隐藏模式方面表现出色。这是可能的，因为它们使用扩散过程来建模数据分布，使用一系列条件分布。因此，我们可以强调DMs的主要优势和优点如下：
- en: '**Generalizability and applicability to a wide range of problems**: Unlike
    other generative methods, which are limited to image and video generation, DMs
    can be utilized to generate images, audio, videos, texts, molecular structures,
    and many other data types and modalities'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**泛化能力和广泛适用性**：与其他仅限于图像和视频生成的生成方法不同，DMs可以用于生成图像、音频、视频、文本、分子结构以及许多其他数据类型和模态'
- en: '**Stability in the training process**: The architecture, the training process,
    and the optimization technique of DMs make them more stable compared to other
    generative models such as GANs'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练过程中的稳定性**：DMs的架构、训练过程和优化技术使它们比其他生成模型（如GANs）更稳定'
- en: '**High-quality synthetic data generation**: Due to their distinctive architecture
    and innovative gradual and iterative training process, DMs generate high-quality
    synthetic data that surpasses other generative models such as VAEs and GANs'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高质量合成数据生成**：由于它们独特的架构和创新性的逐步和迭代训练过程，DMs生成的高质量合成数据超越了其他生成模型，如VAEs和GANs'
- en: The cons of using DMS
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用DMS的缺点
- en: 'The two main limitations and shortcomings of using DMs can be described in
    terms of the computational complexity of the training and inference process, as
    well as the large-scale training datasets required by DMs:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DMs的两大主要局限和不足可以从训练和推理过程的计算复杂度以及DMs所需的大规模训练数据集来描述：
- en: '**Computational complexity**: DMs are computationally heavy. They are usually
    slow compared to other generative models, as the forward and reverse diffusion
    processes are composed of hundreds of steps (usually, the number of steps, ![](img/B18494_F09_011.png),
    is close to ![](img/B18494_F09_012.png)).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算复杂度**：DMs计算量大。与其他生成模型相比，它们通常较慢，因为正向和反向扩散过程由数百个步骤组成（通常，步骤数，![](img/B18494_F09_011.png)，接近![](img/B18494_F09_012.png)）。'
- en: '**More training data is required**: DMs require large-scale training datasets
    to converge. Obtaining such datasets is not suitable for certain fields, which
    limits the usability of DMs for certain applications.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**需要更多训练数据**：DMs需要大规模的训练数据集才能收敛。获取此类数据集对于某些领域来说并不合适，这限制了DMs在某些应用中的实用性。'
- en: Now, we can clearly identify the pros and cons of using DMs for synthetic data
    generation. Let’s practice utilizing DMs to generate synthetic data to train ML
    models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以清楚地识别出使用DMs进行合成数据生成时的优缺点。让我们练习利用DMs生成合成数据以训练机器学习模型。
- en: Hands-on diffusion models in practice
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在实践中使用手头的扩散模型
- en: Let’s study a practical example that demonstrates the usability of synthetic
    data in the computer vision field. For that aim, we will generate and prepare
    our dataset, build our ML model from scratch, train it, and evaluate its performance.
    The dataset is available at *Kaggle* ([https://www.kaggle.com/datasets/abdulrahmankerim/crash-car-image-hybrid-dataset-ccih](https://www.kaggle.com/datasets/abdulrahmankerim/crash-car-image-hybrid-dataset-ccih)).
    The full code, the trained model, and the results are available on GitHub under
    the corresponding chapter folder in the book’s repository.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们研究一个实际例子，以展示合成数据在计算机视觉领域的实用性。为此，我们将生成和准备我们的数据集，从头开始构建我们的机器学习模型，对其进行训练，并评估其性能。数据集可在
    *Kaggle* 上找到（[https://www.kaggle.com/datasets/abdulrahmankerim/crash-car-image-hybrid-dataset-ccih](https://www.kaggle.com/datasets/abdulrahmankerim/crash-car-image-hybrid-dataset-ccih)）。完整的代码、训练好的模型和结果可在GitHub上找到，位于书籍仓库中相应章节的文件夹下。
- en: Context
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文
- en: 'We want to build an ML model that can classify car images into two distinct
    categories – images depicting car accidents and those that do not. As you can
    imagine, curating such a real dataset is time-consuming and error-prone. It could
    be easy to collect car images without accidents. However, collecting images of
    cars with accidents, collisions, fires, and other dangerous scenarios is extremely
    hard. To solve this problem and to prove the usability of synthetic data, let’s
    first generate our training dataset. We can use a single synthetic data generation
    approach for that aim. However, let’s combine different methods and tools to collect
    more diverse data and practice different approaches as well. In this example,
    we will use the following methods:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望构建一个机器学习模型，能够将汽车图像分类为两个不同的类别——描绘交通事故的图像和那些不描绘交通事故的图像。正如你可以想象的那样，精心制作这样一个真实的数据集既耗时又容易出错。收集没有事故的汽车图像可能很容易。然而，收集发生事故、碰撞、火灾和其他危险场景的汽车图像则极为困难。为了解决这个问题并证明合成数据的实用性，让我们首先生成我们的训练数据集。为此，我们可以使用单一合成数据生成方法。然而，让我们结合不同的方法和工具来收集更多样化的数据，并实践不同的方法。在这个例子中，我们将使用以下方法：
- en: The DALL·E 2 image generator
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DALL·E 2图像生成器
- en: The DeepAI text-to-image generator
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepAI文本到图像生成器
- en: A simulator built using a game engine such as Silver
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用游戏引擎（如Silver）构建的模拟器
- en: Dataset
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: First, let’s leverage the remarkable capabilities of recent generative models
    such as *DALL·E 2* ([https://openai.com/dall-e-2](https://openai.com/dall-e-2))
    to generate some car accident images.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们利用最近生成的模型（如 *DALL·E 2* [https://openai.com/dall-e-2](https://openai.com/dall-e-2)）的显著能力来生成一些交通事故图像。
- en: '![Figure 9.2 – Generating synthetic images using the DALL·E 2 web application](img/Figure_09_02_B18494.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2 – 使用DALL·E 2网络应用程序生成合成图像](img/Figure_09_02_B18494.jpg)'
- en: Figure 9.2 – Generating synthetic images using the DALL·E 2 web application
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 – 使用DALL·E 2网络应用程序生成合成图像
- en: 'We can simply use the following prompts to generate these images (see *Figure
    9**.2*):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下提示简单地生成这些图像（参见 *图9.2*）：
- en: '`Car accidents`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`交通事故`'
- en: '`White` `car accidents`'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`白色` `交通事故`'
- en: '`Red` `car accidents`'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`红色` `交通事故`'
- en: '`Blue` `car accidents`'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`蓝色` `交通事故`'
- en: '`Ambulance` `car accident`'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`救护车` `交通事故`'
- en: As you can see from *Figure 9**.3*, the generated images look photorealistic
    and diverse, which is exactly what we need to train our ML model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从 *图 9.3* 中所见，生成的图像看起来非常逼真且多样化，这正是我们训练我们的机器学习模型所需要的。
- en: '![Figure 9.3 – Car accident images generated using DALL·E 2](img/Figure_09_03_B18494.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – 使用 DALL·E 2 生成的汽车事故图像](img/Figure_09_03_B18494.jpg)'
- en: Figure 9.3 – Car accident images generated using DALL·E 2
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – 使用 DALL·E 2 生成的汽车事故图像
- en: We also use images collected from the *car-accident(resnet)* dataset, licensed
    under CC BY 4.0 ([https://universe.roboflow.com/resnet-car-accident/car-accident-resnet-n7jei](https://universe.roboflow.com/resnet-car-accident/car-accident-resnet-n7jei))
    using Roboflow ([https://roboflow.com](https://roboflow.com)). We chose this dataset
    as the images are similar to what they would be if they were sourced from a video
    game, such as *BeamNG Drive Crashes* ([https://www.beamng.com/game](https://www.beamng.com/game)).
    We will add these images to our dataset to further improve its diversity. A sample
    of these images is shown in *Figure 9**.4*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用从 *car-accident(resnet)* 数据集中收集的图像，该数据集获得 CC BY 4.0 许可（[https://universe.roboflow.com/resnet-car-accident/car-accident-resnet-n7jei](https://universe.roboflow.com/resnet-car-accident/car-accident-resnet-n7jei)），使用
    Roboflow ([https://roboflow.com](https://roboflow.com)）。我们选择这个数据集，因为图像与从视频游戏（如
    *BeamNG Drive Crashes* [https://www.beamng.com/game](https://www.beamng.com/game)）中获取的图像相似。我们将将这些图像添加到我们的数据集中，以进一步提高其多样性。这些图像的样本在
    *图 9.4* 中展示。
- en: '![Figure 9.4 – Car accident images collected from a video game](img/Figure_09_04_B18494.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – 从视频游戏中收集的汽车事故图像](img/Figure_09_04_B18494.jpg)'
- en: Figure 9.4 – Car accident images collected from a video game
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – 从视频游戏中收集的汽车事故图像
- en: 'Then, we need to generate similar images for the other category, images of
    cars with no accidents. This time, let’s use the *DeepAI* tool to generate these
    images. As did earlier, we can simply use the following prompts to get the required
    images:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要为其他类别生成类似的图像，即无事故的汽车图像。这次，让我们使用 *DeepAI* 工具生成这些图像。与之前一样，我们可以简单地使用以下提示来获取所需的图像：
- en: '`Car` `under rain`'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`雨中的汽车`'
- en: '`White car`'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`白色汽车`'
- en: '`Car` `under fog`'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`雾中的汽车`'
- en: '`Blue car`'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`蓝色汽车`'
- en: As we can see in *Figure 9**.5*, we effortlessly obtained another 30 car images
    with no accidents.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 *图 9.5* 中所见，我们轻松地获得了另外 30 张无事故的汽车图像。
- en: '![Figure 9.5 – Intact car images generated using DeepAI](img/Figure_09_05_B18494.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5 – 使用 DeepAI 生成的完整汽车图像](img/Figure_09_05_B18494.jpg)'
- en: Figure 9.5 – Intact car images generated using DeepAI
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – 使用 DeepAI 生成的完整汽车图像
- en: Now, we have 60 synthetic images, but we still need more images to appropriately
    train our ML model. We can generate any number of images using the previous generative
    models, but let’s explore another way – using the *Silver* simulator ([https://github.com/lsmcolab/Silver](https://github.com/lsmcolab/Silver)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有 60 张合成图像，但我们仍然需要更多图像来适当地训练我们的机器学习模型。我们可以使用之前的生成模型生成任意数量的图像，但让我们探索另一种方法
    – 使用 *Silver* 模拟器 ([https://github.com/lsmcolab/Silver](https://github.com/lsmcolab/Silver))。
- en: By specifying the number of images that we want, we can generate the following
    images for this category (*Figure 9**.6*).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定我们想要的图像数量，我们可以为这个类别生成以下图像（*图 9.6*）。
- en: '![Figure 9.6 – Intact car images generated using Silver](img/Figure_09_06_B18494.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6 – 使用 Silver 生成的完整汽车图像](img/Figure_09_06_B18494.jpg)'
- en: Figure 9.6 – Intact car images generated using Silver
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – 使用 Silver 生成的完整汽车图像
- en: At this point, we have collected `600` synthetic images. Now, to assess the
    performance of our trained model, we should test it on real car images. Let’s
    collect real images using the *Unsplash* website ([https://unsplash.com](https://unsplash.com)).
    To further improve our dataset, let’s also manually add images using Roboflow.
    *Figure 9**.7* shows a sample of these images.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经收集了 `600` 张合成图像。现在，为了评估我们训练的模型性能，我们应该在真实汽车图像上对其进行测试。让我们使用 *Unsplash*
    网站收集真实图像（[https://unsplash.com](https://unsplash.com)）。为了进一步提高我们的数据集，我们还可以使用 Roboflow
    手动添加图像。*图 9.7* 展示了这些图像的样本。
- en: '![Figure 9.7 – Sample images of our real dataset](img/Figure_09_07_B18494.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7 – 我们真实数据集的样本图像](img/Figure_09_07_B18494.jpg)'
- en: Figure 9.7 – Sample images of our real dataset
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 我们真实数据集的样本图像
- en: Finally, our dataset is composed of `600` synthetic images and `250` real ones,
    as shown in *Table 9.1*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的数据集由 `600` 张合成图像和 `250` 张真实图像组成，如 *表 9.1* 所示。
- en: '| **Split** | **Synthetic** | **Real** |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| **分割** | **合成** | **真实** |'
- en: '| Training | 540 | - |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 训练 | 540 | - |'
- en: '| Validation | 60 | 62 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 验证 | 60 | 62 |'
- en: '| Test | - | 188 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | - | 188 |'
- en: '| Total | 600 | 250 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 600 | 250 |'
- en: Table 9.1 – Our final dataset’s splits and number of images
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1 – 我们最终数据集的分割和图像数量
- en: Note that we will train only on synthetic data and test only on real data. Also,
    note in the validation split that we used both synthetic and real data because
    we are training and testing on two different domains – synthetic and real. Thus,
    a balanced mixture of data from both domains is necessary to give a good understanding
    of our model’s learning of the synthetic data and generalizability to the real
    domain.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将在合成数据上训练，并在真实数据上测试。此外，注意在验证分割中，我们使用了合成和真实数据，因为我们正在两个不同的领域（合成和真实）进行训练和测试。因此，需要从这两个领域的数据中取得平衡混合，以便更好地理解我们的模型对合成数据的理解和泛化到真实领域的能力。
- en: ML model
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习模型
- en: Our ML model is composed of four convolutional and three fully connected layers,
    max pooling, dropout, and batch normalization. We will use the `train.py` file
    in the corresponding chapter’s folder of the book’s GitHub repository.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的机器学习模型由四个卷积层和三个全连接层、最大池化、dropout和批量归一化组成。我们将使用书中GitHub仓库相应章节文件夹中的`train.py`文件。
- en: Training
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 培训
- en: We train our model from scratch on the training split shown in *Table 9.1* for
    `30` epochs. Then, we will select the best model using the validation split. The
    training loss is shown in *Figure 9**.8*. The loss is smoothly decreasing, as
    expected, which means that our model trains well on our synthetic training dataset.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图9.1所示的训练分割上从头开始训练我们的模型`30`个epoch。然后，我们将使用验证分割选择最佳模型。训练损失如图9.8所示。损失值如预期的那样平滑下降，这意味着我们的模型在我们的合成训练数据集上训练得很好。
- en: '![Figure 9.8 – The training loss during the training stage](img/Figure_09_08_B18494.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图9.8 – 训练阶段的训练损失](img/Figure_09_08_B18494.jpg)'
- en: Figure 9.8 – The training loss during the training stage
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – 训练阶段的训练损失
- en: From the validation accuracy shown in *Figure 9**.9*, we can see that our model
    achieved the best results on the validation set at epoch `27`. Thus, we will use
    this model for testing.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从图9.9所示的验证准确率中，我们可以看到我们的模型在第27个epoch时在验证集上取得了最佳结果。因此，我们将使用这个模型进行测试。
- en: '![Figure 9.9 – Validation accuracy during the training stage](img/Figure_09_09_B18494.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图9.9 – 训练阶段的验证准确率](img/Figure_09_09_B18494.jpg)'
- en: Figure 9.9 – Validation accuracy during the training stage
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 – 训练阶段的验证准确率
- en: Testing
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试
- en: Let’s see the performance of our best model on the test split. As you can see
    from the confusion matrix shown in *Figure 9**.10*, our model classifies no-accident
    cars with an accuracy of *84%*, while it classifies accidents with an accuracy
    of *74%*. Our model achieved a total accuracy of *80%*, which is excellent given
    that our model was trained only on synthetic data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的最佳模型在测试集上的表现。如图9.10所示的混淆矩阵所示，我们的模型以84%的准确率对无事故车辆进行分类，而以74%的准确率对事故进行分类。我们的模型的总准确率为80%，考虑到我们的模型仅使用合成数据进行训练，这是一个非常出色的成绩。
- en: '![Figure 9.10 – The confusion matrix illustrating classification results](img/Figure_09_10_B18494.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图9.10 – 展示分类结果的混淆矩阵](img/Figure_09_10_B18494.jpg)'
- en: Figure 9.10 – The confusion matrix illustrating classification results
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 – 展示分类结果的混淆矩阵
- en: We have provided a detailed example of how to generate and leverage synthetic
    data for training ML models. Now, let’s delve into DMs and the main ethical issues.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一个详细的示例，说明如何生成和利用合成数据来训练机器学习模型。现在，让我们深入了解数据生成模型（DMs）及其主要伦理问题。
- en: Diffusion models – ethical issues
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩散模型 – 伦理问题
- en: In this section, you will learn about the main ethical issues associated with
    using DMs for synthetic data generation.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解与使用DMs进行合成数据生成相关的伦理问题。
- en: Diffusion-based generative models are emerging and powerful technologies. Thus,
    their pros and cons need to be considered carefully. Their advantages are huge
    for businesses, industry, and research. However, they possess dangerous capabilities
    that can be leveraged to cause harm to individuals, businesses, societies, and
    so on.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 基于扩散的生成模型是新兴且强大的技术。因此，它们的优缺点需要仔细考虑。它们对商业、工业和研究具有巨大的优势。然而，它们具有可能被利用来对个人、企业、社会等造成伤害的危险能力。
- en: 'Let’s list the main ethical issues usually associated with generative models
    and, especially, DMs:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们列出通常与生成模型，尤其是DMs相关的伦理问题：
- en: Copyright
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版权
- en: Bias
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差
- en: Inappropriate content
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不适当的内容
- en: Responsibility issues
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 责任问题
- en: Privacy issues
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐私问题
- en: Fraud and identity theft
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欺诈和身份盗窃
- en: Now, let’s delve into some of the main ethical issues behind using DMs in practice.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解使用DMs实践中的一些主要伦理问题。
- en: Copyright
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 版权
- en: DMs are usually trained on large-scale real datasets. For example, DALL E 2
    was trained on more than 650 million text-image pairs. Thus, obtaining permission
    from the owners of these data samples, such as images, artworks, video clips,
    and others, is not feasible. Additionally, DMs may misrepresent or represent copyrighted
    and intellectual properties with minor changes. For example, *Universal Music*
    asked streaming services and companies to prevent AI/ML companies from accessing
    their songs for training ([https://www.billboard.com/pro/universal-music-asks-spotify-apple-stop-ai-access-songs](https://www.billboard.com/pro/universal-music-asks-spotify-apple-stop-ai-access-songs)).
    Thus, it creates many difficult questions for regulators to address and regulations
    for companies to comply with. Taking into account the huge, rapid progress in
    this field, the regulations may not be able to provide an ethical frame to control
    copyright issues.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: DMs通常在大规模真实数据集上进行训练。例如，DALL E 2在超过6.5亿个文本-图像对上进行了训练。因此，从这些数据样本的所有者，如图像、艺术品、视频片段等，那里获得许可是不切实际的。此外，DMs可能会通过细微的修改来错误地表示或表示受版权和知识产权保护的内容。例如，*环球音乐*要求流媒体服务和公司阻止AI/ML公司访问他们的歌曲进行训练（[https://www.billboard.com/pro/universal-music-asks-spotify-apple-stop-ai-access-songs](https://www.billboard.com/pro/universal-music-asks-spotify-apple-stop-ai-access-songs)）。因此，这给监管机构解决和公司遵守的法规带来了许多难题。考虑到这一领域的巨大、快速进步，法规可能无法提供一个道德框架来控制版权问题。
- en: Bias
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏见
- en: As we have outlined in this chapter, DMs learn to generate synthetic data following
    the same distribution of the training data. Thus, if the training data is biased,
    the DM will also generate biased synthetic data. What is more difficult with these
    complex models is assessing this bias compared to examining the raw training data,
    such as images and videos. It becomes even more complex and severe when these
    models are leveraged in decision-making by non-experts. For example, a professional
    comic artist may identify gender, political, and age biases within a comic book.
    However, training a DM on millions of comic books and then assessing the biases
    of the generated comic books, using this model, may not be possible!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中概述的那样，DMs学习生成与训练数据具有相同分布的合成数据。因此，如果训练数据存在偏见，DM也会生成偏见的合成数据。与检查原始训练数据（如图像和视频）相比，评估这种偏见更为困难。当这些模型被非专家用于决策时，问题变得更加复杂和严重。例如，一位专业漫画艺术家可能能在漫画书中识别出性别、政治和年龄偏见。然而，在数百万本漫画书上训练DM，然后使用该模型评估生成的漫画书的偏见可能是不可能的！
- en: Inappropriate content
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不适当的内容
- en: DMs may generate inappropriate content without users’ consent. It may include
    inappropriate language, sexual and violent content, hate speech, and racism. Thus,
    DMs need to utilize an effective filtering mechanism to discard unsuitable data.
    Simultaneously, they need to utilize a safeguarding procedure to prevent generating
    inappropriate content.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: DMs可能在未经用户同意的情况下生成不适当的内容。这可能包括不适当的语言、色情和暴力内容、仇恨言论和种族主义。因此，DMs需要利用有效的过滤机制来丢弃不合适的数据。同时，他们需要利用保护程序来防止生成不适当的内容。
- en: Responsibility
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 责任
- en: Utilizing generative models such as DMs in decision-making will be almost unavoidable
    in the future. However, it is not possible, with current DMs, to understand why
    a certain decision was made. Thus, it is challenging to understand who is responsible
    for wrong decisions made by DMs that may cause death, injuries, or damage to properties.
    Thus, we need to develop suitable mechanisms to ensure transparency, accountability,
    and tracking in the process of decision-making.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来，利用如DMs（决策模型）等生成模型进行决策几乎不可避免。然而，使用当前的DMs，我们无法理解为什么做出了某个特定的决策。因此，理解谁应对DMs做出的可能导致死亡、伤害或财产损失的错误决策负责是具有挑战性的。因此，我们需要开发合适的机制来确保决策过程中的透明度、问责制和追踪。
- en: Privacy
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐私
- en: DMs may reveal sensitive information about individuals and organizations since
    the generated synthetic data still follows the statistical properties of the original
    real training data. These models may disclose information that could be utilized
    by a third party to cause harm, loss, or unwanted consequences. For example, ChatGPT
    was banned in Italy due to privacy issues.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于生成的合成数据仍然遵循原始真实训练数据的统计特性，DMs可能会泄露关于个人和组织的敏感信息。这些模型可能会披露第三方可能利用的信息，造成损害、损失或不良后果。例如，由于隐私问题，ChatGPT在意大利被禁止使用。
- en: Fraud and identity theft
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欺诈和身份盗窃
- en: DMs are powerful and capable of mimicking the human voice, photos, and videos.
    Thus, generated fake media can be exploited for many fraudulent purposes, such
    as accessing personal information, money laundering, credit card fraud, and cybercrime.
    Furthermore, DMs can be used to impersonate a celebrity, activist, or politician
    to get access to private information, classified material, and documents.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式模型（DMs）功能强大，能够模仿人类的声音、照片和视频。因此，生成的虚假媒体可以用于许多欺诈目的，例如获取个人信息、洗钱、信用卡欺诈和网络犯罪。此外，DMs还可以用来冒充名人、活动家或政治家，以获取私人信息、机密材料和文件。
- en: Summary
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced a novel and powerful method to generate synthetic
    data – using DMs. We compared DMs to other state-of-the-art generative models,
    and then, we highlighted the training process of DMs. Furthermore, we discussed
    the pros and cons of utilizing DMs. Additionally, we learned how to generate and
    utilize synthetic data in practice. We also examined the main ethical considerations
    usually raised when deploying DMs for synthetic data generation. You developed
    a comprehensive understanding of generative models, and you learned about standard
    DM architecture, the training process, and the main advantages, benefits, and
    limitations of utilizing DMs in practice. In the next chapter, we will shed light
    on several case studies, highlighting how synthetic data has been successfully
    utilized to improve computer vision solutions in practice. The chapter aims to
    inspire and motivate you to explore the potential of synthetic data in your own
    applications.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一种新颖且强大的生成合成数据的方法——使用DMs。我们比较了DMs与其他最先进的生成模型，然后强调了DMs的训练过程。此外，我们讨论了利用DMs的优缺点。此外，我们学习了如何在实践中生成和利用合成数据。我们还考察了在部署DMs进行合成数据生成时通常提出的主要伦理考量。您对生成模型有了全面的理解，并学习了标准DM架构、训练过程以及在实际应用中利用DMs的主要优势、益处和局限性。在下一章中，我们将探讨几个案例研究，突出合成数据如何成功应用于实践中改善计算机视觉解决方案。本章旨在激发和激励您探索合成数据在您自己的应用中的潜力。
- en: Part 4:Case Studies and Best Practices
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4部分：案例研究和最佳实践
- en: 'In this part, you will be introduced to rich and diverse case studies in three
    cutting-edge areas in the field of ML: **Computer Vision** (**CV**), **Natural
    Language Processing** (**NLP**), and **Predictive Analytics** (**PA**). You will
    comprehend the benefits of employing synthetic data in these fields and identify
    the main challenges and issues. Following this, you will learn about best practices
    that improve the usability of synthetic data in practice.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分，您将了解机器学习领域三个前沿领域的丰富多样的案例研究：**计算机视觉**（CV）、**自然语言处理**（NLP）和**预测分析**（PA）。您将理解在这些领域中采用合成数据的益处，并识别主要挑战和问题。随后，您将学习提高合成数据在实际应用中可用性的最佳实践。
- en: 'This part has the following chapters:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 10*](B18494_10.xhtml#_idTextAnchor178), *Case Study 1 – Computer
    Vision*'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B18494_10.xhtml#_idTextAnchor178)，*案例研究1 – 计算机视觉*'
- en: '[*Chapter 11*](B18494_11.xhtml#_idTextAnchor188), *Case Study 2 – Natural Language
    Processing*'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B18494_11.xhtml#_idTextAnchor188)，*案例研究2 – 自然语言处理*'
- en: '[*Chapter 12*](B18494_12.xhtml#_idTextAnchor203), *Case Study 3 – Predictive
    Analytics*'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第12章*](B18494_12.xhtml#_idTextAnchor203)，*案例研究3 – 预测分析*'
- en: '[*Chapter 13*](B18494_13.xhtml#_idTextAnchor216), *Best Practices for Applying
    Synthetic Data*'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第13章*](B18494_13.xhtml#_idTextAnchor216)，*应用合成数据的最佳实践*'
