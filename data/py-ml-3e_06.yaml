- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Learning Best Practices for Model Evaluation and Hyperparameter Tuning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习模型评估和超参数调优的最佳实践
- en: 'In the previous chapters, you learned about the essential machine learning
    algorithms for classification and how to get our data into shape before we feed
    it into those algorithms. Now, it''s time to learn about the best practices of
    building good machine learning models by fine-tuning the algorithms and evaluating
    the performance of the models. In this chapter, we will learn how to do the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，你学习了用于分类的基本机器学习算法，并且了解了在将数据输入算法之前，如何将数据整理成适合的格式。现在，是时候学习通过微调算法和评估模型性能来构建优秀机器学习模型的最佳实践了。在本章中，我们将学习如何完成以下任务：
- en: Assess the performance of machine learning models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估机器学习模型的性能
- en: Diagnose the common problems of machine learning algorithms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诊断机器学习算法中的常见问题
- en: Fine-tune machine learning models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调机器学习模型
- en: Evaluate predictive models using different performance metrics
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的性能指标评估预测模型
- en: Streamlining workflows with pipelines
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用管道简化工作流
- en: When we applied different preprocessing techniques in the previous chapters,
    such as standardization for feature scaling in *Chapter 4*, *Building Good Training
    Datasets – Data Preprocessing*, or principal component analysis for data compression
    in *Chapter 5*, *Compressing Data via Dimensionality Reduction*, you learned that
    we have to reuse the parameters that were obtained during the fitting of the training
    data to scale and compress any new data, such as the examples in the separate
    test dataset. In this section, you will learn about an extremely handy tool, the
    `Pipeline` class in scikit-learn. It allows us to fit a model including an arbitrary
    number of transformation steps and apply it to make predictions about new data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在前几章应用不同的预处理技术时，如在*第4章*，*构建良好的训练数据集 – 数据预处理*中进行的特征缩放标准化，或在*第5章*，*通过降维压缩数据*中进行的主成分分析（PCA），你已经学到，我们必须重用在训练数据拟合过程中获得的参数来缩放和压缩任何新的数据，例如在单独的测试数据集中出现的示例。在本节中，你将学习一个非常实用的工具，scikit-learn中的`Pipeline`类。它允许我们拟合一个模型，其中包括任意数量的转换步骤，并将其应用于对新数据进行预测。
- en: Loading the Breast Cancer Wisconsin dataset
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载乳腺癌威斯康星数据集
- en: In this chapter, we will be working with the Breast Cancer Wisconsin dataset,
    which contains 569 examples of malignant and benign tumor cells. The first two
    columns in the dataset store the unique ID numbers of the examples and the corresponding
    diagnoses (`M` = malignant, `B` = benign), respectively. Columns 3-32 contain
    30 real-valued features that have been computed from digitized images of the cell
    nuclei, which can be used to build a model to predict whether a tumor is benign
    or malignant. The Breast Cancer Wisconsin dataset has been deposited in the UCI
    Machine Learning Repository, and more detailed information about this dataset
    can be found at [https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用乳腺癌威斯康星数据集，该数据集包含569个恶性和良性肿瘤细胞的示例。数据集中的前两列分别存储示例的唯一ID号和相应的诊断（`M` =
    恶性，`B` = 良性）。第3至第32列包含30个实值特征，这些特征是通过数字化细胞核图像计算得到的，可以用来构建模型预测肿瘤是良性还是恶性。乳腺癌威斯康星数据集已经被存储在UCI机器学习库中，关于该数据集的更多详细信息可以在[https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))找到。
- en: '**Obtaining the Breast Cancer Wisconsin dataset**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**获取乳腺癌威斯康星数据集**'
- en: 'You can find a copy of the dataset (and all other datasets used in this book)
    in the code bundle of this book, which you can use if you are working offline
    or the UCI server at [https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data)
    is temporarily unavailable. For instance, to load the dataset from a local directory,
    you can replace the following lines:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的代码包中找到数据集的副本（以及本书中使用的所有其他数据集），如果你在离线工作，或者UCI服务器[https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data)暂时无法访问，可以使用这些副本。例如，要从本地目录加载数据集，你可以替换以下几行：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'with these:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与以下内容一起使用：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In this section, we will read in the dataset and split it into training and
    test datasets in three simple steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将读取数据集并通过三个简单的步骤将其拆分为训练数据集和测试数据集：
- en: 'We will start by reading in the dataset directly from the UCI website using
    pandas:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先通过pandas直接从UCI网站读取数据集：
- en: '[PRE2]'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we will assign the 30 features to a NumPy array, `X`. Using a `LabelEncoder`
    object, we will transform the class labels from their original string representation
    (`''M''` and `''B''`) into integers:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将把30个特征赋值给一个NumPy数组`X`。通过使用`LabelEncoder`对象，我们将把类别标签从原始字符串表示（`'M'`和`'B'`）转换为整数：
- en: '[PRE3]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After encoding the class labels (diagnosis) in an array, `y`, the malignant
    tumors are now represented as class `1`, and the benign tumors are represented
    as class `0`, respectively. We can double-check this mapping by calling the `transform`
    method of the fitted `LabelEncoder` on two dummy class labels:'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在将类别标签（诊断结果）编码到数组`y`后，恶性肿瘤现在表示为类别`1`，良性肿瘤表示为类别`0`。我们可以通过调用拟合后的`LabelEncoder`的`transform`方法，使用两个虚拟类别标签来再次检查这个映射：
- en: '[PRE4]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Before we construct our first model pipeline in the following subsection, let''s
    divide the dataset into a separate training dataset (80 percent of the data) and
    a separate test dataset (20 percent of the data):'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们在下一小节中构建第一个模型管道之前，先将数据集划分为单独的训练数据集（占数据的80%）和单独的测试数据集（占数据的20%）：
- en: '[PRE5]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Combining transformers and estimators in a pipeline
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在管道中组合转换器和估算器
- en: In the previous chapter, you learned that many learning algorithms require input
    features on the same scale for optimal performance. Since the features in the
    Breast Cancer Wisconsin dataset are measured on various different scales, we will
    standardize the columns in the Breast Cancer Wisconsin dataset before we feed
    them to a linear classifier, such as logistic regression. Furthermore, let's assume
    that we want to compress our data from the initial 30 dimensions onto a lower
    two-dimensional subspace via **principal component analysis** (**PCA**), a feature
    extraction technique for dimensionality reduction that was introduced in *Chapter
    5*, *Compressing Data via Dimensionality Reduction*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习到许多学习算法需要输入特征具有相同的尺度才能获得最佳性能。由于乳腺癌威斯康星数据集中的特征是以不同的尺度测量的，我们将在将其输入线性分类器（如逻辑回归）之前，对乳腺癌威斯康星数据集的列进行标准化。此外，假设我们想通过**主成分分析**（**PCA**）将数据从初始的30个维度压缩到一个较低的二维子空间，PCA是一种用于降维的特征提取技术，已经在*第5章*《通过降维压缩数据》中介绍过。
- en: 'Instead of going through the model fitting and data transformation steps for
    the training and test datasets separately, we can chain the `StandardScaler`,
    `PCA`, and `LogisticRegression` objects in a pipeline:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将`StandardScaler`、`PCA`和`LogisticRegression`对象连接在一个管道中，而不是分别为训练数据集和测试数据集进行模型拟合和数据转换步骤：
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `make_pipeline` function takes an arbitrary number of scikit-learn transformers
    (objects that support the `fit` and `transform` methods as input), followed by
    a scikit-learn estimator that implements the `fit` and `predict` methods. In our
    preceding code example, we provided two transformers, `StandardScaler` and `PCA`,
    and a `LogisticRegression` estimator as inputs to the `make_pipeline` function,
    which constructs a scikit-learn `Pipeline` object from these objects.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`make_pipeline`函数接受任意数量的scikit-learn转换器（支持`fit`和`transform`方法的对象）作为输入，后面跟着一个实现`fit`和`predict`方法的scikit-learn估算器。在我们之前的代码示例中，我们提供了两个转换器，`StandardScaler`和`PCA`，以及一个`LogisticRegression`估算器作为输入传递给`make_pipeline`函数，这会从这些对象构建一个scikit-learn的`Pipeline`对象。'
- en: We can think of a scikit-learn `Pipeline` as a meta-estimator or wrapper around
    those individual transformers and estimators. If we call the `fit` method of `Pipeline`,
    the data will be passed down a series of transformers via `fit` and `transform`
    calls on these intermediate steps until it arrives at the estimator object (the
    final element in a pipeline). The estimator will then be fitted to the transformed
    training data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将scikit-learn的`Pipeline`视为这些独立转换器和估算器的元估算器或包装器。如果我们调用`Pipeline`的`fit`方法，数据将通过这些中间步骤中的`fit`和`transform`调用传递到转换器，直到它到达估算器对象（管道中的最后一个元素）。然后，估算器将被拟合到转换后的训练数据上。
- en: When we executed the `fit` method on the `pipe_lr` pipeline in the preceding
    code example, `StandardScaler` first performed `fit` and `transform` calls on
    the training data. Second, the transformed training data was passed on to the
    next object in the pipeline, `PCA`. Similar to the previous step, `PCA` also executed
    `fit` and `transform` on the scaled input data and passed it to the final element
    of the pipeline, the estimator.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在前面的代码示例中对`pipe_lr`管道执行`fit`方法时，`StandardScaler`首先对训练数据执行`fit`和`transform`调用。然后，转换后的训练数据被传递给管道中的下一个对象`PCA`。与前一步类似，`PCA`也对经过缩放的输入数据执行`fit`和`transform`，并将其传递给管道的最后一个元素——估计器。
- en: Finally, the `LogisticRegression` estimator was fit to the training data after
    it underwent transformations via `StandardScaler` and `PCA`. Again, we should
    note that there is no limit to the number of intermediate steps in a pipeline;
    however, the last pipeline element has to be an estimator.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，`LogisticRegression`估计器在经过`StandardScaler`和`PCA`的转换后，拟合了训练数据。再次提醒，我们应注意，管道中间步骤的数量没有限制；但是，最后一个管道元素必须是一个估计器。
- en: Similar to calling `fit` on a pipeline, pipelines also implement a `predict`
    method. If we feed a dataset to the `predict` call of a `Pipeline` object instance,
    the data will pass through the intermediate steps via `transform` calls. In the
    final step, the estimator object will then return a prediction on the transformed
    data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于在管道上调用`fit`，管道还实现了`predict`方法。如果我们将数据集传递给`Pipeline`对象实例的`predict`调用，数据将通过`transform`调用经过中间步骤。在最后一步，估计器对象将对转换后的数据进行预测并返回结果。
- en: 'The pipelines of the scikit-learn library are immensely useful wrapper tools,
    which we will use frequently throughout the rest of this book. To make sure that
    you''ve got a good grasp of how the `Pipeline` object works, please take a close
    look at the following illustration, which summarizes our discussion from the previous
    paragraphs:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn库的管道是非常有用的封装工具，我们将在本书的剩余部分频繁使用它们。为了确保你充分理解`Pipeline`对象的工作原理，请仔细查看以下插图，它总结了我们前述段落的讨论：
- en: '![](img/B13208_06_01.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_06_01.png)'
- en: Using k-fold cross-validation to assess model performance
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用k折交叉验证评估模型性能
- en: One of the key steps in building a machine learning model is to estimate its
    performance on data that the model hasn't seen before. Let's assume that we fit
    our model on a training dataset and use the same data to estimate how well it
    performs on new data. We remember from the *Tackling overfitting via regularization*
    section in *Chapter 3*, *A Tour of Machine Learning Classifiers Using scikit-learn*,
    that a model can suffer from underfitting (high bias) if the model is too simple,
    or it can overfit the training data (high variance) if the model is too complex
    for the underlying training data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 构建机器学习模型的关键步骤之一是估计模型在未见过数据上的表现。假设我们在训练数据集上拟合了模型，并使用相同的数据估算它在新数据上的表现。我们从*第3章，使用scikit-learn的机器学习分类器概览*中的*通过正则化解决过拟合问题*部分中记得，如果模型过于简单，可能会出现欠拟合（高偏差）；如果模型对于训练数据过于复杂，则可能会对训练数据过拟合（高方差）。
- en: To find an acceptable bias-variance tradeoff, we need to evaluate our model
    carefully. In this section, you will learn about the common cross-validation techniques
    **holdout cross-validation** and **k-fold cross-validation**, which can help us
    to obtain reliable estimates of the model's generalization performance, that is,
    how well the model performs on unseen data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到一个可接受的偏差-方差权衡，我们需要仔细评估我们的模型。在本节中，你将学习常见的交叉验证技术**保留法交叉验证**和**k折交叉验证**，它们可以帮助我们可靠地估计模型的泛化性能，即模型在未见过的数据上的表现。
- en: The holdout method
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保留法
- en: A classic and popular approach for estimating the generalization performance
    of machine learning models is holdout cross-validation. Using the holdout method,
    we split our initial dataset into separate training and test datasets—the former
    is used for model training, and the latter is used to estimate its generalization
    performance. However, in typical machine learning applications, we are also interested
    in tuning and comparing different parameter settings to further improve the performance
    for making predictions on unseen data. This process is called **model selection**,
    with the name referring to a given classification problem for which we want to
    select the *optimal* values of *tuning parameters* (also called **hyperparameters**).
    However, if we reuse the same test dataset over and over again during model selection,
    it will become part of our training data and thus the model will be more likely
    to overfit. Despite this issue, many people still use the test dataset for model
    selection, which is not a good machine learning practice.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一种经典且广泛使用的机器学习模型泛化性能估计方法是留出交叉验证。使用留出法时，我们将初始数据集划分为训练集和测试集——前者用于模型训练，后者用于估计模型的泛化性能。然而，在典型的机器学习应用中，我们还需要调整和比较不同的参数设置，以进一步提升模型在未见数据上的预测性能。这个过程称为**模型选择**，其名称指的是我们希望为给定的分类问题选择*最优*的*调参参数*（也称为**超参数**）。然而，如果在模型选择过程中不断重复使用相同的测试数据集，它将成为训练数据的一部分，从而使模型更容易出现过拟合。尽管存在这个问题，许多人仍然使用测试数据集进行模型选择，这并不是一种好的机器学习实践。
- en: 'A better way of using the holdout method for model selection is to separate
    the data into three parts: a training dataset, a validation dataset, and a test
    dataset. The training dataset is used to fit the different models, and the performance
    on the validation dataset is then used for the model selection. The advantage
    of having a test dataset that the model hasn''t seen before during the training
    and model selection steps is that we can obtain a less biased estimate of its
    ability to generalize to new data. The following figure illustrates the concept
    of holdout cross-validation, where we use a validation dataset to repeatedly evaluate
    the performance of the model after training using different hyperparameter values.
    Once we are satisfied with the tuning of hyperparameter values, we estimate the
    model''s generalization performance on the test dataset:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用留出法进行模型选择的更好方法是将数据分为三部分：训练数据集、验证数据集和测试数据集。训练数据集用于拟合不同的模型，随后使用验证数据集的性能进行模型选择。拥有一个在训练和模型选择过程中未曾见过的测试数据集的优点在于，我们可以更少偏见地估计模型对新数据的泛化能力。下图展示了留出交叉验证的概念，我们使用验证数据集反复评估不同超参数值训练后的模型性能。一旦对超参数值的调整满意后，我们会在测试数据集上估计模型的泛化性能：
- en: '![](img/B13208_06_02.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_06_02.png)'
- en: A disadvantage of the holdout method is that the performance estimate may be
    very sensitive to how we partition the training dataset into the training and
    validation subsets; the estimate will vary for different examples of the data.
    In the next subsection, we will take a look at a more robust technique for performance
    estimation, k-fold cross-validation, where we repeat the holdout method *k* times
    on *k* subsets of the training data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 留出法的一个缺点是，性能估计可能会非常敏感于我们如何将训练数据集划分为训练子集和验证子集；对于不同的数据示例，估计会有所不同。在下一小节中，我们将探讨一种更稳健的性能估计技术——k折交叉验证，在这种方法中，我们会在训练数据的*k*个子集上重复留出法进行*k*次。
- en: K-fold cross-validation
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K折交叉验证
- en: In k-fold cross-validation, we randomly split the training dataset into *k*
    folds without replacement, where *k* – 1 folds are used for the model training,
    and one fold is used for performance evaluation. This procedure is repeated *k*
    times so that we obtain *k* models and performance estimates.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在k折交叉验证中，我们将训练数据集随机划分为*k*个折叠，且不放回，其中*k* - 1个折叠用于模型训练，1个折叠用于性能评估。该过程会重复进行*k*次，以便获得*k*个模型和性能估计。
- en: '**Sampling with and without replacement**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**有放回和无放回的抽样**'
- en: We looked at an example to illustrate sampling with and without replacement
    in *Chapter 3*, *A Tour of Machine Learning Classifiers Using scikit-learn*. If
    you haven't read that chapter, or want a refresher, refer to the information box
    titled *Sampling with and without replacement* in the *Combining multiple decision
    trees via random forests* section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第3章*，*使用scikit-learn的机器学习分类器概览*中查看了一个例子来说明有放回和无放回采样。如果你没有阅读过那一章，或者想要复习一下，请参考*通过随机森林结合多个决策树*部分中的信息框，标题为*有放回与无放回采样*。
- en: We then calculate the average performance of the models based on the different,
    independent test folds to obtain a performance estimate that is less sensitive
    to the sub-partitioning of the training data compared to the holdout method. Typically,
    we use k-fold cross-validation for model tuning, that is, finding the optimal
    hyperparameter values that yield a satisfying generalization performance, which
    is estimated from evaluating the model performance on the test folds.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们根据不同的独立测试折叠计算模型的平均性能，以获得一个相比于保留法更加不敏感于训练数据子划分的性能估计。通常，我们使用k折交叉验证来进行模型调优，也就是寻找能够获得令人满意的泛化性能的最佳超参数值，这些性能是通过在测试折叠上评估模型性能得到的。
- en: Once we have found satisfactory hyperparameter values, we can retrain the model
    on the complete training dataset and obtain a final performance estimate using
    the independent test dataset. The rationale behind fitting a model to the whole
    training dataset after k-fold cross-validation is that providing more training
    examples to a learning algorithm usually results in a more accurate and robust
    model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们找到了令人满意的超参数值，我们可以在完整的训练数据集上重新训练模型，并使用独立的测试数据集来获得最终的性能估计。对模型进行k折交叉验证后再用整个训练数据集进行训练的理论依据是，给学习算法提供更多的训练样本通常会产生一个更加准确且鲁棒的模型。
- en: Since k-fold cross-validation is a resampling technique without replacement,
    the advantage of this approach is that each example will be used for training
    and validation (as part of a test fold) exactly once, which yields a lower-variance
    estimate of the model performance than the holdout method. The following figure
    summarizes the concept behind k-fold cross-validation with *k* = 10\. The training
    dataset is divided into 10 folds, and during the 10 iterations, nine folds are
    used for training, and one fold will be used as the test dataset for the model
    evaluation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于k折交叉验证是一种无放回的重采样技术，这种方法的优点是每个样本都会被用于训练和验证（作为测试折叠的一部分）一次，这相比于保留法能够提供一个更低方差的模型性能估计。下图总结了k折交叉验证的概念，其中*k*
    = 10。训练数据集被划分为10个折叠，在10次迭代过程中，九个折叠用于训练，剩下一个折叠用于模型评估的测试数据集。
- en: 'Also, the estimated performances, ![](img/B13208_06_001.png) (for example,
    classification accuracy or error), for each fold are then used to calculate the
    estimated average performance, *E*, of the model:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，每个折叠的估计性能，![](img/B13208_06_001.png)（例如，分类准确率或错误率），然后被用来计算模型的估计平均性能，*E*：
- en: '![](img/B13208_06_03.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_06_03.png)'
- en: 'A good standard value for *k* in k-fold cross-validation is 10, as empirical
    evidence shows. For instance, experiments by Ron Kohavi on various real-world
    datasets suggest that 10-fold cross-validation offers the best tradeoff between
    bias and variance (*A Study of Cross-Validation and Bootstrap for Accuracy Estimation
    and Model Selection*, *Kohavi, Ron*, *International Joint Conference on Artificial
    Intelligence (IJCAI)*, 14 (12): 1137-43, *1995*).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '在k折交叉验证中，*k*的一个好的标准值是10，正如经验研究所表明的。例如，Ron Kohavi对多个现实世界数据集的实验表明，10折交叉验证在偏差和方差之间提供了最佳的折中（*A
    Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection*，*Kohavi,
    Ron*，*International Joint Conference on Artificial Intelligence (IJCAI)*，14 (12):
    1137-43，*1995*）。'
- en: However, if we are working with relatively small training sets, it can be useful
    to increase the number of folds. If we increase the value of *k*, more training
    data will be used in each iteration, which results in a lower pessimistic bias
    toward estimating the generalization performance by averaging the individual model
    estimates. However, large values of *k* will also increase the runtime of the
    cross-validation algorithm and yield estimates with higher variance, since the
    training folds will be more similar to each other. On the other hand, if we are
    working with large datasets, we can choose a smaller value for *k*, for example,
    *k* = 5, and still obtain an accurate estimate of the average performance of the
    model while reducing the computational cost of refitting and evaluating the model
    on the different folds.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们处理的是相对较小的训练集，增加折数可能会很有用。如果我们增加*k*的值，那么每次迭代中将使用更多的训练数据，这会导致通过平均单个模型估计来估算泛化性能时产生较低的悲观偏差。然而，较大的*k*值也会增加交叉验证算法的运行时间，并且由于训练折更相似，这会导致估计的方差增大。另一方面，如果我们处理的是大规模数据集，可以选择较小的*k*值，例如*k*
    = 5，依然能够准确估算模型的平均性能，同时减少重新拟合和评估模型在不同折上的计算成本。
- en: '**Leave-one-out cross-validation**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**留一交叉验证**'
- en: A special case of k-fold cross-validation is the **leave-one-out cross-validation**
    (**LOOCV**) method. In LOOCV, we set the number of folds equal to the number of
    training examples (*k* = *n*) so that only one training example is used for testing
    during each iteration, which is a recommended approach for working with very small
    datasets.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: k折交叉验证的一个特例是**留一交叉验证**（**LOOCV**）方法。在LOOCV中，我们将折数设置为训练示例的数量（*k* = *n*），这样在每次迭代时，只使用一个训练示例进行测试，这是一种处理非常小的数据集时推荐的方法。
- en: 'A slight improvement over the standard k-fold cross-validation approach is
    stratified k-fold cross-validation, which can yield better bias and variance estimates,
    especially in cases of unequal class proportions, which has also been been shown
    in the same study by Ron Kohavi referenced previously in this section. In stratified
    cross-validation, the class label proportions are preserved in each fold to ensure
    that each fold is representative of the class proportions in the training dataset,
    which we will illustrate by using the `StratifiedKFold` iterator in scikit-learn:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于标准的k折交叉验证方法，分层k折交叉验证稍作改进，尤其在类别不均衡的情况下，可以提供更好的偏差和方差估计，这一点也在本节中前面提到的Ron Kohavi的同一研究中有所展示。在分层交叉验证中，每个折中的类别标签比例得以保持，确保每个折都能代表训练数据集中类别的比例，我们将通过在scikit-learn中使用`StratifiedKFold`迭代器来进行说明：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: First, we initialized the `StratifiedKFold` iterator from the `sklearn.model_selection`
    module with the `y_train` class labels in the training dataset, and we specified
    the number of folds via the `n_splits` parameter. When we used the `kfold` iterator
    to loop through the `k` folds, we used the returned indices in `train` to fit
    the logistic regression pipeline that we set up at the beginning of this chapter.
    Using the `pipe_lr` pipeline, we ensured that the examples were scaled properly
    (for instance, standardized) in each iteration. We then used the `test` indices
    to calculate the accuracy score of the model, which we collected in the `scores`
    list to calculate the average accuracy and the standard deviation of the estimate.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从`sklearn.model_selection`模块初始化了`StratifiedKFold`迭代器，并使用训练数据集中的`y_train`类别标签，此外通过`n_splits`参数指定了折数。当我们使用`kfold`迭代器遍历`k`折时，我们使用返回的`train`索引来拟合我们在本章开始时设置的逻辑回归管道。通过使用`pipe_lr`管道，我们确保在每次迭代中，示例都被正确地缩放（例如标准化）。然后，我们使用`test`索引计算模型的准确性分数，并将其收集到`scores`列表中，用于计算平均准确率和估计的标准差。
- en: 'Although the previous code example was useful to illustrate how k-fold cross-validation
    works, scikit-learn also implements a k-fold cross-validation scorer, which allows
    us to evaluate our model using stratified k-fold cross-validation less verbosely:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管之前的代码示例有助于说明k折交叉验证的工作原理，scikit-learn还实现了一个k折交叉验证评分器，允许我们以更简洁的方式使用分层k折交叉验证来评估模型：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: An extremely useful feature of the `cross_val_score` approach is that we can
    distribute the evaluation of the different folds across multiple central processing
    units (CPUs) on our machine. If we set the `n_jobs` parameter to `1`, only one
    CPU will be used to evaluate the performances, just like in our `StratifiedKFold`
    example previously. However, by setting `n_jobs=2`, we could distribute the 10
    rounds of cross-validation to two CPUs (if available on our machine), and by setting
    `n_jobs=-1`, we can use all available CPUs on our machine to do the computation
    in parallel.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`cross_val_score` 方法的一个极其有用的功能是，我们可以将不同折叠的评估分配到机器上多个中央处理单元（CPU）上。如果我们将 `n_jobs`
    参数设置为 `1`，则只会使用一个 CPU 来评估性能，就像我们之前的 `StratifiedKFold` 示例一样。然而，通过将 `n_jobs=2`，我们可以将
    10 轮交叉验证分配到两个 CPU（如果机器上有的话），而将 `n_jobs=-1`，我们可以使用机器上所有可用的 CPU 来并行计算。'
- en: '**Estimating generalization performance**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**估计泛化性能**'
- en: Please note that a detailed discussion of how the variance of the generalization
    performance is estimated in cross-validation is beyond the scope of this book,
    but you can refer to a comprehensive article about model evaluation and cross-validation
    (*Model evaluation, model selection, and algorithm selection in machine learning*.
    *Raschka S*. arXiv preprint arXiv:1811.12808, 2018) that discusses these topics
    in more depth. The article is freely available from [https://arxiv.org/abs/1811.12808](https://arxiv.org/abs/1811.12808).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，关于如何在交叉验证中估计泛化性能方差的详细讨论超出了本书的范围，但你可以参考一篇关于模型评估和交叉验证的综合文章（*《机器学习中的模型评估、模型选择与算法选择》*，*Raschka
    S*，arXiv 预印本 arXiv:1811.12808，2018），该文章更深入地讨论了这些主题。文章可以免费从[https://arxiv.org/abs/1811.12808](https://arxiv.org/abs/1811.12808)获取。
- en: 'In addition, you can find a detailed discussion in this excellent article by
    M. Markatou and others (*Analysis of Variance of Cross-validation Estimators of
    the Generalization Error*, *M. Markatou*, *H. Tian*, *S. Biswas*, and *G. M. Hripcsak*,
    *Journal of Machine Learning Research*, 6: 1127-1168, *2005*).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还可以在 M. Markatou 等人写的这篇优秀文章中找到详细讨论（*《交叉验证估计器的方差与泛化误差分析》*，*M. Markatou*，*H.
    Tian*，*S. Biswas* 和 *G. M. Hripcsak*，*机器学习研究期刊*，6：1127-1168，*2005*）。
- en: 'You can also read about alternative cross-validation techniques, such as the
    .632 Bootstrap cross-validation method (*Improvements on Cross-validation: The
    .632+ Bootstrap Method*, *B. Efron* and *R. Tibshirani*, *Journal of the American
    Statistical Association*, 92(438): 548-560, *1997*).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以阅读关于替代交叉验证技术的相关资料，比如 .632 Bootstrap 交叉验证方法（*《交叉验证的改进：.632+ Bootstrap 方法》*，*B.
    Efron* 和 *R. Tibshirani*，*美国统计学会期刊*，92(438)：548-560，*1997*）。
- en: Debugging algorithms with learning and validation curves
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用学习曲线和验证曲线调试算法
- en: 'In this section, we will take a look at two very simple yet powerful diagnostic
    tools that can help us to improve the performance of a learning algorithm: **learning
    curves** and **validation curves**. In the next subsections, we will discuss how
    we can use learning curves to diagnose whether a learning algorithm has a problem
    with overfitting (high variance) or underfitting (high bias). Furthermore, we
    will take a look at validation curves that can help us to address the common issues
    of a learning algorithm.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍两个非常简单但又强大的诊断工具，帮助我们提升学习算法的表现：**学习曲线**和**验证曲线**。在接下来的小节中，我们将讨论如何使用学习曲线诊断学习算法是否存在过拟合（高方差）或欠拟合（高偏差）的问题。此外，我们还将介绍验证曲线，这能帮助我们解决学习算法中的常见问题。
- en: Diagnosing bias and variance problems with learning curves
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用学习曲线诊断偏差和方差问题
- en: If a model is too complex for a given training dataset—there are too many degrees
    of freedom or parameters in this model—the model tends to overfit the training
    data and does not generalize well to unseen data. Often, it can help to collect
    more training examples to reduce the degree of overfitting.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个模型对于给定的训练数据集过于复杂——即模型中有过多的自由度或参数——模型往往会出现过拟合，并且在面对未见过的数据时，表现不佳。通常，收集更多的训练样本有助于减少过拟合的程度。
- en: 'However, in practice, it can often be very expensive or simply not feasible
    to collect more data. By plotting the model training and validation accuracies
    as functions of the training dataset size, we can easily detect whether the model
    suffers from high variance or high bias, and whether the collection of more data
    could help to address this problem. But before we discuss how to plot learning
    curves in scikit-learn, let''s discuss those two common model issues by walking
    through the following illustration:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实际应用中，收集更多数据通常非常昂贵或根本不可行。通过将模型的训练准确率和验证准确率绘制成训练数据集大小的函数，我们可以轻松检测模型是否存在高方差或高偏差问题，以及收集更多数据是否能帮助解决这个问题。在我们讨论如何在
    scikit-learn 中绘制学习曲线之前，让我们通过以下插图来讨论这两种常见的模型问题：
- en: '![](img/B13208_06_04.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_06_04.png)'
- en: The graph in the upper-left shows a model with high bias. This model has both
    low training and cross-validation accuracy, which indicates that it underfits
    the training data. Common ways to address this issue are to increase the number
    of parameters of the model, for example, by collecting or constructing additional
    features, or by decreasing the degree of regularization, for example, in **support
    vector machine** (**SVM**) or logistic regression classifiers.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 左上角的图显示了一个具有高偏差的模型。该模型的训练准确率和交叉验证准确率都很低，表明它在训练数据上存在欠拟合问题。解决这个问题的常见方法是增加模型的参数数量，例如，通过收集或构造额外的特征，或通过减少正则化的程度，例如，在**支持向量机**（**SVM**）或逻辑回归分类器中。
- en: The graph in the upper-right shows a model that suffers from high variance,
    which is indicated by the large gap between the training and cross-validation
    accuracy. To address this problem of overfitting, we can collect more training
    data, reduce the complexity of the model, or increase the regularization parameter,
    for example.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 右上角的图显示了一个存在高方差的模型，这从训练准确率和交叉验证准确率之间的巨大差距可以看出。为了应对这个过拟合问题，我们可以收集更多的训练数据，减少模型的复杂度，或增加正则化参数等。
- en: For unregularized models, it can also help to decrease the number of features
    via feature selection (*Chapter 4*, *Building Good Training Datasets – Data Preprocessing*)
    or feature extraction (*Chapter 5*, *Compressing Data via Dimensionality Reduction*)
    to decrease the degree of overfitting. While collecting more training data usually
    tends to decrease the chance of overfitting, it may not always help, for example,
    if the training data is extremely noisy or the model is already very close to
    optimal.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无正则化的模型，通过特征选择（*第 4 章*，*构建良好的训练数据集 – 数据预处理*）或特征提取（*第 5 章*，*通过降维压缩数据*）减少特征数量，也有助于降低过拟合的程度。虽然收集更多的训练数据通常有助于减少过拟合的机会，但它并不总是有效，例如，当训练数据噪声极大或模型已经非常接近最优时。
- en: 'In the next subsection, we will see how to address those model issues using
    validation curves, but let''s first see how we can use the learning curve function
    from scikit-learn to evaluate the model:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我们将看到如何使用验证曲线来解决这些模型问题，但首先让我们看看如何使用 scikit-learn 的学习曲线函数来评估模型：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Note that we passed `max_iter=10000` as an additional argument when instantiating
    the `LogisticRegression` object (which uses 1,000 iterations as a default) to
    avoid convergence issues for the smaller dataset sizes or extreme regularization
    parameter values (covered in the next section). After we have successfully executed
    the preceding code, we will obtain the following learning curve plot:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当实例化 `LogisticRegression` 对象时，我们传递了 `max_iter=10000` 作为附加参数（默认使用 1,000 次迭代），以避免在较小数据集大小或极端正则化参数值下出现收敛问题（将在下一节中讨论）。在成功执行前面的代码后，我们将获得以下学习曲线图：
- en: '![](img/B13208_06_05.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_06_05.png)'
- en: Via the `train_sizes` parameter in the `learning_curve` function, we can control
    the absolute or relative number of training examples that are used to generate
    the learning curves. Here, we set `train_sizes=np.linspace(0.1, 1.0, 10)` to use
    10 evenly spaced, relative intervals for the training dataset sizes. By default,
    the `learning_curve` function uses stratified k-fold cross-validation to calculate
    the cross-validation accuracy of a classifier, and we set *k*=10 via the `cv`
    parameter for 10-fold stratified cross-validation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `learning_curve` 函数中的 `train_sizes` 参数，我们可以控制用于生成学习曲线的训练样本的绝对数量或相对数量。在这里，我们将
    `train_sizes=np.linspace(0.1, 1.0, 10)` 设置为使用 10 个均匀间隔的相对训练数据集大小。默认情况下，`learning_curve`
    函数使用分层 k 折交叉验证来计算分类器的交叉验证准确率，并通过 `cv` 参数设置 *k*=10 进行 10 折分层交叉验证。
- en: Then, we simply calculated the average accuracies from the returned cross-validated
    training and test scores for the different sizes of the training dataset, which
    we plotted using Matplotlib's `plot` function. Furthermore, we added the standard
    deviation of the average accuracy to the plot using the `fill_between` function
    to indicate the variance of the estimate.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们简单地计算了不同训练数据集大小的交叉验证训练和测试得分的平均准确率，并使用 Matplotlib 的`plot`函数将其绘制出来。此外，我们还通过`fill_between`函数将平均准确率的标准差添加到图表中，以表示估计的方差。
- en: As we can see in the preceding learning curve plot, our model performs quite
    well on both the training and validation datasets if it has seen more than 250
    examples during training. We can also see that the training accuracy increases
    for training datasets with fewer than 250 examples, and the gap between validation
    and training accuracy widens—an indicator of an increasing degree of overfitting.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的学习曲线图中看到的，如果模型在训练过程中见过超过250个示例，它在训练和验证数据集上的表现都非常好。我们还可以看到，对于少于250个示例的训练数据集，训练准确率有所提高，同时验证和训练准确率之间的差距加大——这是过拟合程度增加的一个指示。
- en: Addressing over- and underfitting with validation curves
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过验证曲线解决过拟合和欠拟合问题
- en: 'Validation curves are a useful tool for improving the performance of a model
    by addressing issues such as overfitting or underfitting. Validation curves are
    related to learning curves, but instead of plotting the training and test accuracies
    as functions of the sample size, we vary the values of the model parameters, for
    example, the inverse regularization parameter, `C`, in logistic regression. Let''s
    go ahead and see how we create validation curves via scikit-learn:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 验证曲线是通过解决过拟合或欠拟合等问题来提高模型性能的有用工具。验证曲线与学习曲线相关，但不是将训练和测试的准确率作为样本大小的函数进行绘制，而是通过调整模型参数的值，例如逻辑回归中的逆正则化参数`C`。让我们继续看看如何通过
    scikit-learn 创建验证曲线：
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Using the preceding code, we obtained the validation curve plot for the parameter
    `C`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的代码，我们获得了参数`C`的验证曲线图：
- en: '![](img/B13208_06_06.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_06_06.png)'
- en: Similar to the `learning_curve` function, the `validation_curve` function uses
    stratified k-fold cross-validation by default to estimate the performance of the
    classifier. Inside the `validation_curve` function, we specified the parameter
    that we wanted to evaluate. In this case, it is `C`, the inverse regularization
    parameter of the `LogisticRegression` classifier, which we wrote as `'logisticregression__C'`
    to access the `LogisticRegression` object inside the scikit-learn pipeline for
    a specified value range that we set via the `param_range` parameter. Similar to
    the learning curve example in the previous section, we plotted the average training
    and cross-validation accuracies and the corresponding standard deviations.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`learning_curve`函数，`validation_curve`函数默认使用分层k折交叉验证来估计分类器的性能。在`validation_curve`函数内部，我们指定了要评估的参数。在这种情况下，是`C`，即`LogisticRegression`分类器的逆正则化参数，我们将其写为`'logisticregression__C'`，以访问
    scikit-learn 管道中`LogisticRegression`对象，并通过`param_range`参数设置了指定值范围。与前一部分中的学习曲线示例类似，我们绘制了平均训练和交叉验证准确率及其相应的标准差。
- en: Although the differences in the accuracy for varying values of `C` are subtle,
    we can see that the model slightly underfits the data when we increase the regularization
    strength (small values of `C`). However, for large values of `C`, it means lowering
    the strength of regularization, so the model tends to slightly overfit the data.
    In this case, the sweet spot appears to be between 0.01 and 0.1 of the `C` value.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`C`值不同所导致的准确率差异较小，但我们可以看到，当增加正则化强度（即`C`的较小值）时，模型略微欠拟合数据。然而，对于较大的`C`值，这意味着减弱正则化强度，因此模型倾向于轻微地过拟合数据。在这种情况下，最佳的`C`值似乎介于0.01到0.1之间。
- en: Fine-tuning machine learning models via grid search
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过网格搜索微调机器学习模型
- en: 'In machine learning, we have two types of parameters: those that are learned
    from the training data, for example, the weights in logistic regression, and the
    parameters of a learning algorithm that are optimized separately. The latter are
    the tuning parameters (or hyperparameters) of a model, for example, the regularization
    parameter in logistic regression or the depth parameter of a decision tree.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们有两种类型的参数：一种是从训练数据中学习到的参数，例如逻辑回归中的权重，另一种是独立优化的学习算法参数。后者是模型的调参参数（或超参数），例如逻辑回归中的正则化参数或决策树的深度参数。
- en: In the previous section, we used validation curves to improve the performance
    of a model by tuning one of its hyperparameters. In this section, we will take
    a look at a popular hyperparameter optimization technique called **grid search**,
    which can further help to improve the performance of a model by finding the *optimal*
    combination of hyperparameter values.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们通过调节模型的一个超参数来使用验证曲线提高模型的表现。在本节中，我们将介绍一种流行的超参数优化技术——**网格搜索**，它通过找到超参数值的*最佳*组合，进一步帮助提升模型的性能。
- en: Tuning hyperparameters via grid search
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过网格搜索调优超参数
- en: 'The grid search approach is quite simple: it''s a brute-force exhaustive search
    paradigm where we specify a list of values for different hyperparameters, and
    the computer evaluates the model performance for each combination to obtain the
    optimal combination of values from this set:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索方法非常简单：它是一种暴力穷举搜索的范式，我们指定不同超参数的值列表，计算机会评估每种组合下的模型性能，以获得这个集合中值的最佳组合：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Using the preceding code, we initialized a `GridSearchCV` object from the `sklearn.model_selection`
    module to train and tune an SVM pipeline. We set the `param_grid` parameter of
    `GridSearchCV` to a list of dictionaries to specify the parameters that we'd want
    to tune. For the linear SVM, we only evaluated the inverse regularization parameter,
    `C`; for the RBF kernel SVM, we tuned both the `svc__C` and `svc__gamma` parameters.
    Note that the `svc__gamma` parameter is specific to kernel SVMs.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的代码，我们从`sklearn.model_selection`模块初始化了一个`GridSearchCV`对象来训练和调优SVM管道。我们将`GridSearchCV`的`param_grid`参数设置为字典列表，以指定我们希望调优的参数。对于线性SVM，我们只评估了逆正则化参数`C`；对于RBF核SVM，我们调优了`svc__C`和`svc__gamma`两个参数。请注意，`svc__gamma`参数是专门针对核SVM的。
- en: 'After we used the training data to perform the grid search, we obtained the
    score of the best-performing model via the `best_score_` attribute and looked
    at its parameters, which can be accessed via the `best_params_` attribute. In
    this particular case, the RBF kernel SVM model with `svc__C = 100.0` yielded the
    best k-fold cross-validation accuracy: 98.5 percent.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们使用训练数据进行网格搜索后，我们通过`best_score_`属性获得了最佳表现模型的得分，并查看了其参数，这些参数可以通过`best_params_`属性进行访问。在这个特定的例子中，使用`svc__C
    = 100.0`的RBF核SVM模型获得了最佳的k折交叉验证准确率：98.5%。
- en: 'Finally, we use the independent test dataset to estimate the performance of
    the best-selected model, which is available via the `best_estimator_` attribute
    of the `GridSearchCV` object:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用独立的测试数据集来估计最佳选定模型的性能，模型可以通过`GridSearchCV`对象的`best_estimator_`属性获得：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Please note that fitting a model with the best settings (`gs.best_estimator_`)
    on the training set manually via `clf.fit(X_train, y_train)` after completing
    the grid search is not necessary. The `GridSearchCV` class has a `refit` parameter,
    which will refit the `gs.best_estimator_` to the whole training set automatically
    if we set `refit=True` (default).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在完成网格搜索后，手动使用`clf.fit(X_train, y_train)`在训练集上拟合具有最佳设置的模型（`gs.best_estimator_`）并非必要。`GridSearchCV`类有一个`refit`参数，如果我们设置`refit=True`（默认为True），它会自动将`gs.best_estimator_`重新拟合到整个训练集。
- en: '**Randomized hyperparameter search**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机超参数搜索**'
- en: Although grid search is a powerful approach for finding the optimal set of parameters,
    the evaluation of all possible parameter combinations is also computationally
    very expensive. An alternative approach for sampling different parameter combinations
    using scikit-learn is *randomized search*. Randomized search usually performs
    about as well as grid search but is much more cost- and time-effective. In particular,
    if we only sample 60 parameter combinations via randomized search, we already
    have a 95 percent probability of obtaining solutions within 5 percent of the optimal
    performance (*Random search for hyper-parameter optimization*. *Bergstra J*, *Bengio
    Y*. *Journal of Machine Learning Research*. pp. 281-305, 2012).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管网格搜索是寻找最优参数集的强大方法，但评估所有可能的参数组合在计算上是非常昂贵的。使用 scikit-learn 进行不同参数组合采样的替代方法是*随机搜索*。随机搜索通常与网格搜索表现相当，但在成本和时间上更加高效。特别是，如果我们通过随机搜索仅采样
    60 个参数组合，我们就已经有 95% 的概率在最优性能的 5% 范围内获得解（*用于超参数优化的随机搜索*。*Bergstra J*, *Bengio Y*.
    *机器学习研究期刊*。第281-305页，2012年）。
- en: Using the `RandomizedSearchCV` class in scikit-learn, we can draw random parameter
    combinations from sampling distributions with a specified budget. More details
    and examples of its usage can be found at [http://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization](http://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 中的 `RandomizedSearchCV` 类，我们可以在指定的预算范围内从采样分布中随机选择参数组合。更多详细信息和使用示例可以在
    [http://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization](http://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization)
    找到。
- en: Algorithm selection with nested cross-validation
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带嵌套交叉验证的算法选择
- en: 'Using k-fold cross-validation in combination with grid search is a useful approach
    for fine-tuning the performance of a machine learning model by varying its hyperparameter
    values, as we saw in the previous subsection. If we want to select among different
    machine learning algorithms, though, another recommended approach is nested cross-validation.
    In a nice study on the bias in error estimation, Sudhir Varma and Richard Simon
    concluded that the true error of the estimate is almost unbiased relative to the
    test dataset when nested cross-validation is used (*Bias in Error Estimation When
    Using Cross-Validation for Model Selection*, *BMC Bioinformatics*, *S. Varma*
    and *R. Simon*, 7(1): 91, *2006*).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '将 k 折交叉验证与网格搜索结合使用是通过改变超参数值来微调机器学习模型性能的有用方法，正如我们在前一小节中所见。如果我们想在不同的机器学习算法之间进行选择，另一种推荐的方法是嵌套交叉验证。在一项关于误差估计偏差的研究中，Sudhir
    Varma 和 Richard Simon 得出结论，使用嵌套交叉验证时，估计的真实误差相对于测试数据集几乎没有偏差（*使用交叉验证进行模型选择时的误差估计偏差*，*BMC
    生物信息学*，*S. Varma* 和 *R. Simon*，7(1): 91，*2006*）。'
- en: 'In nested cross-validation, we have an outer k-fold cross-validation loop to
    split the data into training and test folds, and an inner loop is used to select
    the model using k-fold cross-validation on the training fold. After model selection,
    the test fold is then used to evaluate the model performance. The following figure
    explains the concept of nested cross-validation with only five outer and two inner
    folds, which can be useful for large datasets where computational performance
    is important; this particular type of nested cross-validation is also known as
    **5x2 cross-validation**:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌套交叉验证中，我们有一个外部的 k 折交叉验证循环，用于将数据拆分为训练集和测试集，内部循环则使用 k 折交叉验证在训练集上选择模型。模型选择完成后，测试集用于评估模型性能。下图解释了仅有五个外部折叠和两个内部折叠的嵌套交叉验证概念，这对于计算性能要求较高的大型数据集非常有用；这种特定类型的嵌套交叉验证也被称为**5x2交叉验证**：
- en: '![](img/B13208_06_07.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_06_07.png)'
- en: 'In scikit-learn, we can perform nested cross-validation as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，我们可以按如下方式执行嵌套交叉验证：
- en: '[PRE13]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The returned average cross-validation accuracy gives us a good estimate of what
    to expect if we tune the hyperparameters of a model and use it on unseen data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的平均交叉验证准确率可以很好地估计，如果我们调整模型的超参数并将其应用于未见过的数据时，期望的结果是什么。
- en: 'For example, we can use the nested cross-validation approach to compare an
    SVM model to a simple decision tree classifier; for simplicity, we will only tune
    its depth parameter:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用嵌套交叉验证方法将 SVM 模型与简单的决策树分类器进行比较；为了简化，我们将只调整它的深度参数：
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As we can see, the nested cross-validation performance of the SVM model (97.4
    percent) is notably better than the performance of the decision tree (93.4 percent),
    and thus, we'd expect that it might be the better choice to classify new data
    that comes from the same population as this particular dataset.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，SVM 模型的嵌套交叉验证性能（97.4%）明显优于决策树的性能（93.4%），因此，我们可以预期它可能是分类来自与此数据集相同人群的新数据的更好选择。
- en: Looking at different performance evaluation metrics
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查看不同的性能评估指标
- en: In the previous sections and chapters, we evaluated different machine learning
    models using the prediction accuracy, which is a useful metric with which to quantify
    the performance of a model in general. However, there are several other performance
    metrics that can be used to measure a model's relevance, such as precision, recall,
    and the **F1 score**.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们使用预测准确度评估了不同的机器学习模型，这个指标是量化模型表现的一个有用标准。然而，还有一些其他的性能指标可以用来衡量模型的相关性，如精确度、召回率和**F1
    分数**。
- en: Reading a confusion matrix
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解读混淆矩阵
- en: Before we get into the details of different scoring metrics, let's take a look
    at a **confusion matrix**, a matrix that lays out the performance of a learning
    algorithm.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论不同的评分指标之前，让我们先看看一个**混淆矩阵**，这是一个展示学习算法表现的矩阵。
- en: 'A confusion matrix is simply a square matrix that reports the counts of the
    **true positive** (**TP**), **true negative** (**TN**), **false positive** (**FP**),
    and **false negative** (**FN**) predictions of a classifier, as shown in the following
    figure:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵只是一个方阵，报告分类器的**真正例**（**TP**）、**真负例**（**TN**）、**假正例**（**FP**）和**假负例**（**FN**）的预测计数，如下图所示：
- en: '![](img/B13208_06_08.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_06_08.png)'
- en: 'Although these metrics can be easily computed manually by comparing the true
    and predicted class labels, scikit-learn provides a convenient `confusion_matrix`
    function that we can use, as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通过比较真实和预测的类别标签，可以轻松手动计算这些指标，scikit-learn 提供了一个方便的 `confusion_matrix` 函数，我们可以使用它，如下所示：
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The array that was returned after executing the code provides us with information
    about the different types of error the classifier made on the test dataset. We
    can map this information onto the confusion matrix illustration in the previous
    figure using Matplotlib''s `matshow` function:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后返回的数组为我们提供了有关分类器在测试数据集上所犯的不同类型错误的信息。我们可以使用 Matplotlib 的 `matshow` 函数将这些信息映射到前面图示中的混淆矩阵：
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, the following confusion matrix plot, with the added labels, should make
    the results a little bit easier to interpret:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，下面的混淆矩阵图表，添加了标签后，应该使结果更容易理解：
- en: '![](img/B13208_06_09.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_06_09.png)'
- en: Assuming that class `1` (malignant) is the positive class in this example, our
    model correctly classified 71 of the examples that belong to class `0` (TN) and
    40 examples that belong to class `1` (TP), respectively. However, our model also
    incorrectly misclassified two examples from class `1` as class `0` (FN), and it
    predicted that one example is malignant although it is a benign tumor (FP). In
    the next subsection, we will learn how we can use this information to calculate
    various error metrics.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在本例中类 `1`（恶性）是正类，我们的模型正确地将属于类 `0`（TN）的71个示例分类为负类，将属于类 `1`（TP）的40个示例分类为正类。然而，我们的模型也错误地将来自类
    `1` 的两个示例误分类为类 `0`（FN），并且它错误地预测了一个示例是恶性肿瘤，尽管它实际上是良性肿瘤（FP）。在下一小节中，我们将学习如何利用这些信息来计算各种错误指标。
- en: Optimizing the precision and recall of a classification model
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化分类模型的精确度和召回率
- en: 'Both the prediction **error** (**ERR**) and **accuracy** (**ACC**) provide
    general information about how many examples are misclassified. The error can be
    understood as the sum of all false predictions divided by the number of total
    predictions, and the accuracy is calculated as the sum of correct predictions
    divided by the total number of predictions, respectively:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 预测**误差**（**ERR**）和**准确度**（**ACC**）都提供了关于多少示例被误分类的总体信息。误差可以理解为所有假预测的总和除以总预测数量，而准确度则是正确预测的总和除以总预测数量：
- en: '![](img/B13208_06_002.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_06_002.png)'
- en: 'The prediction accuracy can then be calculated directly from the error:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，预测准确度可以直接从错误中计算得出：
- en: '![](img/B13208_06_003.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_06_003.png)'
- en: 'The **true positive rate** (**TPR**) and **false positive rate** (**FPR**)
    are performance metrics that are especially useful for imbalanced class problems:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**真正例率** (**TPR**) 和 **假正例率** (**FPR**) 是特别适用于类别不平衡问题的性能指标：'
- en: '![](img/B13208_06_004.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_06_004.png)'
- en: In tumor diagnosis, for example, we are more concerned about the detection of
    malignant tumors in order to help a patient with the appropriate treatment. However,
    it is also important to decrease the number of benign tumors incorrectly classified
    as malignant (FP) to not unnecessarily concern patients. In contrast to the FPR,
    the TPR provides useful information about the fraction of positive (or relevant)
    examples that were correctly identified out of the total pool of positives (P).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以肿瘤诊断为例，我们更关注的是恶性肿瘤的检测，以帮助患者获得适当的治疗。然而，同样重要的是减少将良性肿瘤错误分类为恶性肿瘤（假正例，FP）的数量，以免不必要地让患者担忧。与假正例率（FPR）相比，真正例率（TPR）提供了有关在所有正例（P）中，正确识别出的正例（或相关例子）所占的比例的有用信息。
- en: 'The performance metrics **precision** (**PRE**) and **recall** (**REC**) are
    related to those TP and TN rates, and in fact, REC is synonymous with TPR:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 性能指标 **精确度** (**PRE**) 和 **召回率** (**REC**) 与真正例和真负例的比率有关，实际上，REC 与 TPR 同义：
- en: '![](img/B13208_06_005.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_06_005.png)'
- en: Revisiting the malignant tumor detection example, optimizing for recall helps
    with minimizing the chance of not detecting a malignant tumor. However, this comes
    at the cost of predicting malignant tumors in patients although the patients are
    healthy (a high number of FP). If we optimize for precision, on the other hand,
    we emphasize correctness if we predict that a patient has a malignant tumor. However,
    this comes at the cost of missing malignant tumors more frequently (a high number
    of FN).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 重新审视恶性肿瘤检测的例子，优化召回率有助于最小化漏掉恶性肿瘤的可能性。然而，这会导致将健康患者误判为患有恶性肿瘤（较高的假正例数量）。另一方面，如果我们优化精确度，则强调预测患者是否患有恶性肿瘤的正确性，但这会以更频繁地漏掉恶性肿瘤（较高的假负例数量）为代价。
- en: 'To balance the up- and down-sides of optimizing PRE and REC, often a combination
    of PRE and REC is used, the so-called F1 score:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了平衡优化精确度（PRE）和召回率（REC）的优缺点，通常使用精确度和召回率的组合，即所谓的 F1 分数：
- en: '![](img/B13208_06_006.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_06_006.png)'
- en: '**Further reading on precision and recall**'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读精确度和召回率**'
- en: 'If you are interested in a more thorough discussion of the different performance
    metrics, such as precision and recall, read David M. W. Powers'' technical report
    *Evaluation: From Precision, Recall and F-Factor to ROC, Informedness, Markedness
    & Correlation*, which is freely available at [http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf](http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你对精确度和召回率等不同性能指标有更深入的讨论兴趣，可以阅读 David M. W. Powers 的技术报告 *Evaluation: From
    Precision, Recall and F-Factor to ROC, Informedness, Markedness & Correlation*，该报告可在
    [http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf](http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf)
    免费获取。'
- en: 'Those scoring metrics are all implemented in scikit-learn and can be imported
    from the `sklearn.metrics` module as shown in the following snippet:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这些评分指标都已在 scikit-learn 中实现，可以从 `sklearn.metrics` 模块导入，以下代码片段展示了如何操作：
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Furthermore, we can use a different scoring metric than accuracy in the `GridSearchCV`
    via the scoring parameter. A complete list of the different values that are accepted
    by the scoring parameter can be found at [http://scikit-learn.org/stable/modules/model_evaluation.html](http://scikit-learn.org/stable/modules/model_evaluation.html).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以通过 `GridSearchCV` 中的 `scoring` 参数使用不同于准确率的评分指标。`scoring` 参数接受的不同值的完整列表可以在
    [http://scikit-learn.org/stable/modules/model_evaluation.html](http://scikit-learn.org/stable/modules/model_evaluation.html)
    查找到。
- en: 'Remember that the positive class in scikit-learn is the class that is labeled
    as class `1`. If we want to specify a different *positive label*, we can construct
    our own scorer via the `make_scorer` function, which we can then directly provide
    as an argument to the `scoring` parameter in `GridSearchCV` (in this example,
    using the `f1_score` as a metric):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在 scikit-learn 中，正类是标记为类 `1` 的类别。如果我们希望指定不同的 *正类标签*，可以通过 `make_scorer` 函数构造自己的评分器，然后将其直接作为
    `GridSearchCV` 中 `scoring` 参数的参数（在这个例子中，使用 `f1_score` 作为度量标准）：
- en: '[PRE18]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Plotting a receiver operating characteristic
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制接收器操作特性曲线
- en: '**Receiver operating characteristic** (**ROC**) graphs are useful tools to
    select models for classification based on their performance with respect to the
    FPR and TPR, which are computed by shifting the decision threshold of the classifier.
    The diagonal of a ROC graph can be interpreted as *random guessing*, and classification
    models that fall below the diagonal are considered as worse than random guessing.
    A perfect classifier would fall into the top-left corner of the graph with a TPR
    of 1 and an FPR of 0\. Based on the ROC curve, we can then compute the so-called
    **ROC area under the curve** (**ROC AUC**) to characterize the performance of
    a classification model.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收器操作特征**（**ROC**）图是用于根据分类器在FPR和TPR上的表现选择分类模型的有用工具，FPR和TPR是通过调整分类器的决策阈值计算的。ROC图的对角线可以解释为*随机猜测*，而低于对角线的分类模型被认为比随机猜测差。一个完美的分类器将在图的左上角，具有TPR为1和FPR为0。基于ROC曲线，我们可以进一步计算所谓的**ROC曲线下面积**（**ROC
    AUC**），以表征分类模型的性能。'
- en: Similar to ROC curves, we can compute **precision-recall curves** for different
    probability thresholds of a classifier. A function for plotting those precision-recall
    curves is also implemented in scikit-learn and is documented at [http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于ROC曲线，我们也可以计算**精确率-召回率曲线**，用于分类器在不同概率阈值下的表现。scikit-learn中也实现了绘制这些精确率-召回率曲线的函数，相关文档请见[http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html)。
- en: 'Executing the following code example, we will plot a ROC curve of a classifier
    that only uses two features from the Breast Cancer Wisconsin dataset to predict
    whether a tumor is benign or malignant. Although we are going to use the same
    logistic regression pipeline that we defined previously, we are only using two
    features this time. This is to make the classification task more challenging for
    the classifier, by witholding useful information contained in the other features,
    so that the resulting ROC curve becomes visually more interesting. For similar
    reasons, we are also reducing the number of folds in the `StratifiedKFold` validator
    to three. The code is as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下代码示例，我们将绘制一个ROC曲线，该曲线来自一个分类器，该分类器只使用来自乳腺癌威斯康星数据集的两个特征来预测肿瘤是良性还是恶性。尽管我们将使用之前定义的相同的逻辑回归管道，但这次我们只使用两个特征。这是为了使分类任务对分类器更具挑战性，因通过省略其他特征中包含的有用信息，使得生成的ROC曲线在视觉上更具趣味性。出于类似的原因，我们还将`StratifiedKFold`验证器中的折数减少到三折。代码如下：
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the preceding code example, we used the already familiar `StratifiedKFold`
    class from scikit-learn and calculated the ROC performance of the `LogisticRegression`
    classifier in our `pipe_lr` pipeline using the `roc_curve` function from the `sklearn.metrics`
    module separately for each iteration. Furthermore, we interpolated the average
    ROC curve from the three folds via the `interp` function that we imported from
    SciPy and calculated the area under the curve via the `auc` function. The resulting
    ROC curve indicates that there is a certain degree of variance between the different
    folds, and the average ROC AUC (0.76) falls between a perfect score (1.0) and
    random guessing (0.5):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们使用了scikit-learn中已经熟悉的`StratifiedKFold`类，并使用来自`sklearn.metrics`模块的`roc_curve`函数单独计算了我们`pipe_lr`管道中`LogisticRegression`分类器的ROC性能，每次迭代都如此。此外，我们通过从SciPy导入的`interp`函数对三折的平均ROC曲线进行了插值，并通过`auc`函数计算了曲线下面积。结果显示，ROC曲线表明不同折之间存在一定的方差，且平均ROC
    AUC（0.76）介于完美得分（1.0）与随机猜测（0.5）之间：
- en: '![](img/B13208_06_10.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_06_10.png)'
- en: Note that if we are just interested in the ROC AUC score, we could also directly
    import the `roc_auc_score` function from the `sklearn.metrics` submodule, which
    can be used similarly to the other scoring functions (for example, `precision_score`)
    that were introduced in the previous sections.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果我们只对ROC AUC分数感兴趣，我们也可以直接从`sklearn.metrics`子模块导入`roc_auc_score`函数，该函数的使用方式与前面介绍的其他评分函数（例如`precision_score`）相似。
- en: 'Reporting the performance of a classifier as the ROC AUC can yield further
    insights into a classifier''s performance with respect to imbalanced samples.
    However, while the accuracy score can be interpreted as a single cut-off point
    on an ROC curve, A. P. Bradley showed that the ROC AUC and accuracy metrics mostly
    agree with each other: *The use of the area under the ROC curve in the evaluation
    of machine learning algorithms*, *A. P. Bradley*, *Pattern Recognition*, 30(7):
    1145-1159, *1997*.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '通过报告分类器的 ROC AUC 性能，可以进一步了解分类器在处理不平衡样本时的表现。然而，尽管准确率得分可以解释为 ROC 曲线上的一个单一截断点，A.
    P. Bradley 表明 ROC AUC 和准确率指标通常是相一致的：*《在机器学习算法评估中使用 ROC 曲线下的面积》*，*A. P. Bradley*，*模式识别*，30(7):
    1145-1159，*1997*。'
- en: Scoring metrics for multiclass classification
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多类别分类的评分指标
- en: 'The scoring metrics that we''ve discussed so far are specific to binary classification
    systems. However, scikit-learn also implements macro and micro averaging methods
    to extend those scoring metrics to multiclass problems via **one-vs.-all** (**OvA**)
    classification. The micro-average is calculated from the individual TPs, TNs,
    FPs, and FNs of the system. For example, the micro-average of the precision score
    in a *k*-class system can be calculated as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们讨论的评分指标是特定于二分类系统的。然而，scikit-learn 还实现了宏平均和微平均方法，通过 **一对多**（**OvA**）分类将这些评分指标扩展到多类别问题。微平均是通过系统的单个真正例（TP）、真负例（TN）、假正例（FP）和假负例（FN）来计算的。例如，*k*
    类系统中精确度得分的微平均可以按如下方式计算：
- en: '![](img/B13208_06_007.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_06_007.png)'
- en: 'The macro-average is simply calculated as the average scores of the different
    systems:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 宏平均值是通过不同系统的平均得分来简单计算的：
- en: '![](img/B13208_06_008.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_06_008.png)'
- en: Micro-averaging is useful if we want to weight each instance or prediction equally,
    whereas macro-averaging weights all classes equally to evaluate the overall performance
    of a classifier with regard to the most frequent class labels.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 微平均适用于当我们想平等地对每个实例或预测加权时，而宏平均则是将所有类别平等加权，以评估分类器在处理最常见类别标签时的整体表现。
- en: If we are using binary performance metrics to evaluate multiclass classification
    models in scikit-learn, a normalized or weighted variant of the macro-average
    is used by default. The weighted macro-average is calculated by weighting the
    score of each class label by the number of true instances when calculating the
    average. The weighted macro-average is useful if we are dealing with class imbalances,
    that is, different numbers of instances for each label.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用二分类性能指标来评估 scikit-learn 中的多类别分类模型，默认情况下会使用宏平均的归一化或加权变体。加权宏平均是通过在计算平均值时根据每个类别标签的真实实例数量加权每个类别的得分来计算的。加权宏平均在处理类别不平衡时非常有用，也就是说，当每个标签的实例数量不同。
- en: 'While the weighted macro-average is the default for multiclass problems in
    scikit-learn, we can specify the averaging method via the `average` parameter
    inside the different scoring functions that we import from the `sklearn.metrics`
    module, for example, the `precision_score` or `make_scorer` functions:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然加权宏平均是 scikit-learn 中多类别问题的默认设置，但我们可以通过在从 `sklearn.metrics` 模块导入的不同评分函数中使用
    `average` 参数来指定平均方法，例如 `precision_score` 或 `make_scorer` 函数：
- en: '[PRE20]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Dealing with class imbalance
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理类别不平衡
- en: We've mentioned class imbalances several times throughout this chapter, and
    yet we haven't actually discussed how to deal with such scenarios appropriately
    if they occur. Class imbalance is a quite common problem when working with real-world
    data—examples from one class or multiple classes are over-represented in a dataset.
    We can think of several domains where this may occur, such as spam filtering,
    fraud detection, or screening for diseases.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中已经多次提到类别不平衡的问题，但实际上我们还没有讨论如何在发生这种情况时适当地处理。类别不平衡是处理实际数据时一个非常常见的问题——数据集中某一类别或多个类别的样本被过度表示。我们可以想到一些可能出现这种情况的领域，例如垃圾邮件过滤、欺诈检测或疾病筛查。
- en: Imagine that the Breast Cancer Wisconsin dataset that we've been working with
    in this chapter consisted of 90 percent healthy patients. In this case, we could
    achieve 90 percent accuracy on the test dataset by just predicting the majority
    class (benign tumor) for all examples, without the help of a supervised machine
    learning algorithm. Thus, training a model on such a dataset that achieves approximately
    90 percent test accuracy would mean our model hasn't learned anything useful from
    the features provided in this dataset.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在本章中使用的乳腺癌威斯康星数据集由90%的健康患者组成。在这种情况下，通过仅对所有示例预测多数类（良性肿瘤），我们就能在测试数据集上达到90%的准确率，而无需借助监督学习算法。因此，在这样的数据集上训练一个大约达到90%测试准确率的模型，意味着我们的模型没有从数据集中提供的特征中学到任何有用的东西。
- en: 'In this section, we will briefly go over some of the techniques that could
    help with imbalanced datasets. But before we discuss different methods to approach
    this problem, let''s create an imbalanced dataset from our dataset, which originally
    consisted of 357 benign tumors (class `0`) and 212 malignant tumors (class `1`):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要介绍一些可以帮助处理不平衡数据集的技术。但在讨论解决这一问题的不同方法之前，让我们先从原本包含357个良性肿瘤（类别`0`）和212个恶性肿瘤（类别`1`）的数据集中创建一个不平衡数据集：
- en: '[PRE21]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Thus, when we fit classifiers on such datasets, it would make sense to focus
    on other metrics than accuracy when comparing different models, such as precision,
    recall, the ROC curve—whatever we care most about in our application. For instance,
    our priority might be to identify the majority of patients with malignant cancer
    to recommend an additional screening, so recall should be our metric of choice.
    In spam filtering, where we don't want to label emails as spam if the system is
    not very certain, precision might be a more appropriate metric.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们在这类数据集上训练分类器时，比较不同模型时，除了准确率外，关注其他指标是有意义的，比如精确度、召回率、ROC曲线——根据我们的应用场景，选择我们最关心的指标。例如，我们的优先目标可能是识别大部分患有恶性癌症的患者，以推荐额外的筛查，因此召回率应该是我们选择的指标。在垃圾邮件过滤中，如果系统不太确定，我们不希望将邮件标记为垃圾邮件，此时精确度可能是更合适的指标。
- en: Aside from evaluating machine learning models, class imbalance influences a
    learning algorithm during model fitting itself. Since machine learning algorithms
    typically optimize a reward or cost function that is computed as a sum over the
    training examples that it sees during fitting, the decision rule is likely going
    to be biased toward the majority class.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 除了评估机器学习模型之外，类别不平衡还会在模型拟合过程中影响学习算法。由于机器学习算法通常优化的是一个奖励或成本函数，该函数是根据它在拟合过程中看到的训练样本总和来计算的，因此决策规则很可能会偏向多数类。
- en: In other words, the algorithm implicitly learns a model that optimizes the predictions
    based on the most abundant class in the dataset, in order to minimize the cost
    or maximize the reward during training.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，算法隐式地学习一个模型，该模型根据数据集中最丰富的类别优化预测，以最小化成本或最大化训练过程中的奖励。
- en: One way to deal with imbalanced class proportions during model fitting is to
    assign a larger penalty to wrong predictions on the minority class. Via scikit-learn,
    adjusting such a penalty is as convenient as setting the `class_weight` parameter
    to `class_weight='balanced'`, which is implemented for most classifiers.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 处理类别不平衡问题的一种方法是在模型拟合过程中对少数类的错误预测赋予更大的惩罚。通过scikit-learn，调整这种惩罚与将`class_weight`参数设置为`class_weight='balanced'`一样方便，这对于大多数分类器都已经实现。
- en: Other popular strategies for dealing with class imbalance include upsampling
    the minority class, downsampling the majority class, and the generation of synthetic
    training examples. Unfortunately, there's no universally best solution or technique
    that works best across different problem domains. Thus, in practice, it is recommended
    to try out different strategies on a given problem, evaluate the results, and
    choose the technique that seems most appropriate.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 处理类别不平衡的其他流行策略包括对少数类进行上采样、对多数类进行下采样以及生成合成训练样本。不幸的是，没有一种通用的最佳解决方案或技术能够在不同问题领域中表现最好。因此，实际上，建议在给定问题上尝试不同的策略，评估结果，并选择看似最合适的技术。
- en: 'The scikit-learn library implements a simple `resample` function that can help
    with the upsampling of the minority class by drawing new samples from the dataset
    with replacement. The following code will take the minority class from our imbalanced
    Breast Cancer Wisconsin dataset (here, class `1`) and repeatedly draw new samples
    from it until it contains the same number of examples as class label `0`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn库实现了一个简单的`resample`函数，可以通过从数据集中有放回地抽取新样本来帮助上采样少数类。以下代码将从我们不平衡的乳腺癌威斯康星数据集中提取少数类（这里是类别`1`），并不断从中抽取新样本，直到它包含与类别标签`0`相同数量的样本：
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'After resampling, we can then stack the original class `0` samples with the
    upsampled class `1` subset to obtain a balanced dataset as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新采样后，我们可以将原始类别`0`的样本与上采样的类别`1`子集堆叠在一起，以获得一个平衡的数据集，如下所示：
- en: '[PRE24]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Consequently, a majority vote prediction rule would only achieve 50 percent
    accuracy:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，多数投票预测规则将仅达到50%的准确率：
- en: '[PRE25]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Similarly, we could downsample the majority class by removing training examples
    from the dataset. To perform downsampling using the `resample` function, we could
    simply swap the class `1` label with class `0` in the previous code example and
    vice versa.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以通过从数据集中删除训练样本来对多数类进行下采样。要使用`resample`函数执行下采样，我们可以简单地在之前的代码示例中交换类别`1`标签和类别`0`标签，反之亦然。
- en: '**Generating new training data to address class-imbalance**'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成新的训练数据以解决类别不平衡问题**'
- en: 'Another technique for dealing with class imbalance is the generation of synthetic
    training examples, which is beyond the scope of this book. Probably the most widely
    used algorithm for synthetic training data generation is **Synthetic Minority
    Over-sampling Technique** (**SMOTE**), and you can learn more about this technique
    in the original research article by Nitesh Chawla and others: *SMOTE: Synthetic
    Minority Over-sampling Technique*, *Journal of Artificial Intelligence Research*,
    16: 321-357, *2002*. It is also highly recommended to check out `imbalanced-learn`,
    a Python library that is entirely focused on imbalanced datasets, including an
    implementation of SMOTE. You can learn more about `imbalanced-learn` at [https://github.com/scikit-learn-contrib/imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种处理类别不平衡的技术是生成合成训练样本，这超出了本书的范围。可能最广泛使用的合成训练数据生成算法是**合成少数类过采样技术**（**SMOTE**），您可以通过Nitesh
    Chawla等人发表的原始研究文章了解更多关于此技术的信息：《SMOTE: Synthetic Minority Over-sampling Technique》，《人工智能研究期刊》，16:
    321-357，*2002*。同时，我们强烈推荐您查看`imbalanced-learn`，这是一个专注于不平衡数据集的Python库，其中包括SMOTE的实现。您可以在[https://github.com/scikit-learn-contrib/imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn)了解更多关于`imbalanced-learn`的信息。'
- en: Summary
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: At the beginning of this chapter, we discussed how to chain different transformation
    techniques and classifiers in convenient model pipelines that help us to train
    and evaluate machine learning models more efficiently. We then used those pipelines
    to perform k-fold cross-validation, one of the essential techniques for model
    selection and evaluation. Using k-fold cross-validation, we plotted learning and
    validation curves to diagnose common problems of learning algorithms, such as
    overfitting and underfitting.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开始，我们讨论了如何在便捷的模型管道中链式地组合不同的转换技术和分类器，这些管道帮助我们更高效地训练和评估机器学习模型。然后，我们使用这些管道进行了k折交叉验证，这是一种模型选择和评估的关键技术。通过k折交叉验证，我们绘制了学习曲线和验证曲线，以诊断学习算法中常见的问题，如过拟合和欠拟合。
- en: Using grid search, we further fine-tuned our model. We then used confusion matrices
    and various performance metrics to evaluate and optimize a model's performance
    for specific problem tasks. Finally, we concluded this chapter by discussing different
    methods for dealing with imbalanced data, which is a common problem in many real-world
    applications. Now, you should be well-equipped with the essential techniques to
    build supervised machine learning models for classification successfully.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 通过网格搜索，我们进一步对模型进行了微调。然后，我们使用混淆矩阵和各种性能指标来评估和优化模型在特定问题任务中的表现。最后，我们通过讨论处理不平衡数据的不同方法来结束本章，这在许多现实世界应用中是一个常见问题。现在，您应该已经掌握了构建监督式机器学习分类模型的基本技术。
- en: 'In the next chapter, we will look at ensemble methods: methods that allow us
    to combine multiple models and classification algorithms to boost the predictive
    performance of a machine learning system even further.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨集成方法：这些方法允许我们结合多个模型和分类算法，以进一步提升机器学习系统的预测性能。
