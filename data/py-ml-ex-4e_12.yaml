- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Making Predictions with Sequences Using Recurrent Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用循环神经网络（RNN）进行序列预测
- en: In the previous chapter, we focused on **Convolutional Neural Networks** (**CNNs**)
    and used them to deal with image-related tasks. In this chapter, we will explore
    **Recurrent Neural Networks** (**RNNs**), which are suitable for sequential data
    and time-dependent data, such as daily temperature, DNA sequences, and customers’
    shopping transactions over time. You will learn how the recurrent architecture
    works and see variants of the model. We will then work on their applications,
    including sentiment analysis, time series prediction, and text generation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们关注了**卷积神经网络**（**CNNs**），并利用它们处理图像相关任务。在本章中，我们将探索**循环神经网络**（**RNNs**），它们适用于顺序数据和时间依赖数据，如日常温度、DNA序列以及顾客随时间变化的购物交易。你将学习循环架构的工作原理，并看到该模型的变体。接着我们将研究它们的应用，包括情感分析、时间序列预测和文本生成。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Tracking sequential learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪顺序学习
- en: Learning the RNN architecture by example
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过示例学习RNN架构
- en: Training an RNN model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个RNN模型
- en: Overcoming long-term dependencies with **Long Short-Term Memory** (**LSTM**)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克服长期依赖问题，使用**长短期记忆**（**LSTM**）
- en: Analyzing movie review sentiment with RNNs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RNN分析电影评论情感
- en: Revisiting stock price forecasting with LSTM
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新审视使用LSTM进行股票价格预测
- en: Writing your own War and Peace with LSTM
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用LSTM写你自己的《战争与和平》
- en: Introducing sequential learning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入序列学习
- en: The machine learning problems we have solved so far in this book have been time
    independent. For example, ad click-through doesn’t depend on the user’s historical
    ad clicks under our previous approach; in face classification, the model only
    takes in the current face image, not previous ones. However, there are many cases
    in life that depend on time. For example, in financial fraud detection, we can’t
    just look at the present transaction; we should also consider previous transactions
    so that we can model based on their discrepancy. Another example is **Part-of-Speech**
    (**PoS**) tagging, where we assign a PoS (verb, noun, adverb, and so on) to a
    word. Instead of solely focusing on the given word, we must look at some previous
    words, and sometimes the next words too.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们迄今解决的机器学习问题都是时间独立的。例如，广告点击率不依赖于用户历史的广告点击，我们之前的方法中也没有考虑这一点；在面部分类中，模型只输入当前的面部图像，而不是之前的图像。然而，生活中有很多情形是依赖时间的。例如，在金融欺诈检测中，我们不能仅仅看当前交易；我们还需要考虑之前的交易，以便基于它们的差异进行建模。另一个例子是**词性**（**PoS**）标注，我们为一个词分配词性（动词、名词、副词等）。我们不仅要关注给定的词，还必须查看一些前面的词，有时还需要查看后面的词。
- en: In time-dependent cases like those just mentioned, the current output is dependent
    on not only the current input but also the previous inputs; note that the length
    of the previous inputs is not fixed. Using machine learning to solve such problems
    is called **sequence learning** or **sequence modeling**. Obviously, the time-dependent
    event is called a **sequence**. Besides events that occur in disjointed time intervals
    (such as financial transactions and phone calls), text, speech, and video are
    also sequential data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在像上述提到的时间依赖情况中，当前的输出不仅依赖于当前输入，还依赖于之前的输入；请注意，之前输入的长度是没有固定的。使用机器学习解决这类问题叫做**序列学习**或**序列建模**。显然，时间依赖事件叫做**序列**。除了发生在不重叠时间间隔的事件（例如金融交易和电话通话）之外，文本、语音和视频也是顺序数据。
- en: You may be wondering why we can’t just regularly model the sequential data by
    feeding in the entire sequence. This can be quite limiting as we have to fix the
    input size. One problem is that we will lose information if an important event
    lies outside of the fixed window. But can we just use a very large time window?
    Note that the feature space grows along with the window size. The feature space
    will become excessive if we want to cover enough events in a certain time window.
    Hence, overfitting can be another problem.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么我们不能通过直接输入整个序列来常规建模顺序数据。这可能会带来很大的限制，因为我们必须固定输入大小。一个问题是，如果一个重要事件位于固定窗口之外，我们将丧失信息。但是我们能否使用一个非常大的时间窗口呢？请注意，特征空间会随着窗口大小的增加而增加。如果我们希望覆盖某一时间窗口中的足够事件，特征空间将变得过于庞大。因此，过拟合可能是另一个问题。
- en: 'I hope you now see why we need to model sequential data in a different way.
    In the next section, we will talk about an example of a modeling technique used
    for modern sequence learning: RNNs.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你现在明白了为什么我们需要用不同的方式建模顺序数据。在接下来的章节中，我们将讨论现代序列学习中使用的建模技术之一：RNN。
- en: Learning the RNN architecture by example
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过示例学习 RNN 架构
- en: As you can imagine, RNNs stand out because of their recurrent mechanism. We
    will start with a detailed explanation of this in the next section. We will talk
    about different types of RNNs after that, along with some typical applications.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所想，RNN 的突出特点在于其递归机制。我们将在下一节开始详细解释这一点，之后会讨论不同类型的 RNN 以及一些典型的应用。
- en: Recurrent mechanism
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 递归机制
- en: 'Recall that in feedforward networks (such as vanilla neural networks and CNNs),
    data moves one way, from the input layer to the output layer. In RNNs, the recurrent
    architecture allows data to circle back to the input layer. This means that data
    is not limited to a feedforward direction. Specifically, in a hidden layer of
    an RNN, the output from the previous time point will become part of the input
    for the current time point. The following diagram illustrates how data flows in
    an RNN in general:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在前馈网络（如普通神经网络和 CNN）中，数据是单向流动的，从输入层到输出层。而在 RNN 中，递归架构允许数据回流至输入层。这意味着数据不局限于单向传播。具体来说，在
    RNN 的一个隐藏层中，前一个时间点的输出将成为当前时间点输入的一部分。下图展示了 RNN 中数据的一般流动方式：
- en: '![A diagram of a network  Description automatically generated with medium confidence](img/B21047_12_01.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![一个网络的示意图  描述自动生成，可信度中等](img/B21047_12_01.png)'
- en: 'Figure 12.1: The general form of an RNN'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1：RNN 的一般形式
- en: Such a recurrent architecture makes RNNs work well with sequential data, including
    time series (such as daily temperatures, daily product sales, and clinical EEG
    recordings) and general consecutive data with order (such as words in a sentence
    and DNA sequences). Take a financial fraud detector as an example; the output
    features from the previous transaction go into the training for the current transaction.
    In the end, the prediction for one transaction depends on all of its previous
    transactions. Let me explain the recurrent mechanism in a mathematical and visual
    way.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种递归结构使得 RNN 能够很好地处理顺序数据，包括时间序列（如每日温度、每日产品销量和临床脑电图记录）以及具有顺序的通用连续数据（如句子中的单词和
    DNA 序列）。以金融欺诈检测器为例，前一交易的输出特征会作为当前交易的训练输入。最终，对一笔交易的预测依赖于所有先前的交易。接下来，我将通过数学和可视化的方式解释递归机制。
- en: Suppose we have some inputs, *x*[t]. Here, *t* represents a time step or a sequential
    order. In a feedforward neural network, we simply assume that inputs at different
    *t* are independent of each other. We denote the output of a hidden layer at a
    time step, *t*, as *h*[t] = *f*(*x*[t]), where *f* is the abstract of the hidden
    layer.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一些输入 *x*[t]。这里，*t* 代表时间步或顺序位置。在前馈神经网络中，我们通常假设不同 *t* 的输入是相互独立的。我们将时间步 *t*
    时隐藏层的输出表示为 *h*[t] = *f*(*x*[t])，其中 *f* 是隐藏层的抽象函数。
- en: 'This is depicted in the following diagram:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这在下图中有所体现：
- en: '![A picture containing screenshot, diagram, font, design  Description automatically
    generated](img/B21047_12_02.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含截图、图表、字体、设计的图片  描述自动生成](img/B21047_12_02.png)'
- en: 'Figure 12.2: General form of a feedforward neural network'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2：前馈神经网络的一般形式
- en: 'On the contrary, the feedback loop in an RNN feeds the information of the previous
    state to the current state. The output of a hidden layer of an RNN at a time step,
    *t*, can be expressed as *h*[t] = *f*(*h*[t][−1], *x*[t]). This is depicted in
    the following diagram:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，RNN 中的反馈循环将前一状态的信息传递给当前状态。在某一时间步 *t* 的 RNN 隐藏层输出可以表示为 *h*[t] = *f*(*h*[t][−1],
    *x*[t])。这在下图中有所体现：
- en: '![A picture containing screenshot, diagram, rectangle, square  Description
    automatically generated](img/B21047_12_03.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含截图、图表、矩形、方块的图片  描述自动生成](img/B21047_12_03.png)'
- en: 'Figure 12.3: Unfolded recurrent layer over time steps'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3：随时间步展开的递归层
- en: The same task, *f*, is performed on each element of the sequence, and the output,
    *h*[t], is dependent on the output that’s generated from previous computations,
    *h*[t][−1]. The chain-like architecture captures the “memory” that has been calculated
    so far. This is what makes RNNs so successful in dealing with sequential data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的任务，*f*，在序列的每个元素上执行，输出 *h*[t] 依赖于先前计算中生成的输出 *h*[t][−1]。这种链式结构捕获了迄今为止计算的“记忆”。这也是
    RNN 在处理序列数据时如此成功的原因。
- en: 'Moreover, thanks to the recurrent architecture, RNNs also have great flexibility
    in dealing with different combinations of input sequences and/or output sequences.
    In the next section, we will talk about different categories of RNNs based on
    input and output, including the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于递归结构的存在，RNN 在处理不同组合的输入序列和/或输出序列时也具有很大的灵活性。在接下来的章节中，我们将讨论基于输入和输出的不同 RNN
    分类，包括以下内容：
- en: Many to one
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多对一
- en: One to many
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对多
- en: Many to many (synced)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多对多（同步）
- en: Many to many (unsynced)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多对多（未同步）
- en: We will start by looking at many-to-one RNNs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从多对一 RNN 开始。
- en: Many-to-one RNNs
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多对一 RNN
- en: 'The most intuitive type of RNN is probably **many to one**. A many-to-one RNN
    can have input sequences with as many time steps as you want, but it only produces
    one output after going through the entire sequence. The following diagram depicts
    the general structure of a many-to-one RNN:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最直观的 RNN 类型可能是**多对一**。多对一 RNN 可以拥有任意数量的时间步输入序列，但在处理完整个序列后，它只会生成一个输出。以下图表展示了多对一
    RNN 的一般结构：
- en: '![A picture containing screenshot, diagram, rectangle  Description automatically
    generated](img/B21047_12_04.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![包含截图、图表、矩形的图片，描述自动生成](img/B21047_12_04.png)'
- en: 'Figure 12.4: General form of a many-to-one RNN'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4：多对一 RNN 的一般形式
- en: 'Here, *f* represents one or more recurrent hidden layers, where an individual
    layer takes in its own output from the previous time step. Here is an example
    of three hidden layers stacking up:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*f* 表示一个或多个递归隐藏层，每个隐藏层接收来自前一个时间步的输出。以下是三个隐藏层堆叠的示例：
- en: '![A picture containing screenshot, diagram, rectangle, square  Description
    automatically generated](img/B21047_12_05.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![包含截图、图表、矩形、正方形的图片，描述自动生成](img/B21047_12_05.png)'
- en: 'Figure 12.5: Example of three recurrent layers stacking up'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5：三个递归层堆叠的示例
- en: Many-to-one RNNs are widely used for classifying sequential data. Sentiment
    analysis is a good example of this and is where the RNN reads the entire customer
    review, for instance, and assigns a sentiment score (positive, neutral, or negative
    sentiment). Similarly, we can also use RNNs of this kind in the topic classification
    of news articles. Identifying the genre of a song is another application as the
    model can read the entire audio stream. We can also use many-to-one RNNs to determine
    whether a patient is having a seizure based on an EEG trace.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 多对一 RNN 广泛应用于序列数据的分类。情感分析就是一个很好的例子，其中 RNN 会读取整个客户评论，并分配一个情感评分（正面、 中立或负面情感）。类似地，我们也可以在新闻文章的主题分类中使用这种类型的
    RNN。识别歌曲的类型是另一个应用，因为模型可以读取整个音频流。我们还可以使用多对一 RNN 来判断患者是否正在发生癫痫发作，这可以通过脑电图（EEG）迹线来实现。
- en: One-to-many RNNs
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一对多 RNN
- en: '**One-to-many** RNNs are the exact opposite of many-to-one RNNs. They take
    in only one input (not a sequence) and generate a sequence of outputs. A typical
    one-to-many RNN is presented in the following diagram:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**一对多** RNN 完全是多对一 RNN 的相反。它们仅接收一个输入（而非序列），并生成一系列输出。典型的一对多 RNN 如下图所示：'
- en: '![A diagram of a flowchart  Description automatically generated with low confidence](img/B21047_12_06.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![一个流程图的图示，描述自动生成，信心较低](img/B21047_12_06.png)'
- en: 'Figure 12.6: General form of a one-to-many RNN'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6：一对多 RNN 的一般形式
- en: Again, *f* represents one or more recurrent hidden layers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 再次说明，*f* 代表一个或多个递归隐藏层。
- en: Note that “one” here refers to a single time step or a non-sequential input,
    rather than the number of input features.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里的“一”指的是单个时间步或非序列输入，而不是输入特征的数量。
- en: 'One-to-many RNNs are commonly used as sequence generators. For example, we
    can generate a piece of music given a starting note and/or a genre. Similarly,
    we can write a movie script like a professional screenwriter using one-to-many
    RNNs with a starting word we specify. Image captioning is another interesting
    application: the RNN takes in an image and outputs the description (a sentence
    of words) of the image.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一对多RNN通常用作序列生成器。例如，我们可以根据起始音符和/或风格生成一段音乐。类似地，我们可以使用一对多RNN和指定的起始词像专业编剧一样编写电影剧本。图像标注是另一个有趣的应用：RNN接受图像并输出图像的描述（一个句子的词语）。
- en: Many-to-many (synced) RNNs
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多对多（同步）RNN
- en: 'The third type of RNN, many to many (synced), allows each element in the input
    sequence to have an output. Let’s look at how data flows in the following many-to-many
    (synced) RNN:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种类型的RNN——多对多（同步）RNN，允许输入序列中的每个元素都有一个输出。让我们看看以下多对多（同步）RNN中数据的流动方式：
- en: '![A diagram of a flowchart  Description automatically generated with low confidence](img/B21047_12_07.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![一个流程图的示意图  自动生成的描述可信度较低](img/B21047_12_07.png)'
- en: 'Figure 12.7: General form of a many-to-many (synced) RNN'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7：多对多（同步）RNN的一般形式
- en: As you can see, each output is calculated based on its corresponding input and
    all the previous outputs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，每个输出都是基于其对应的输入和所有之前的输出计算得出的。
- en: 'One common use case for this type of RNN is time series forecasting, where
    we want to perform rolling prediction at every time step based on the current
    and previously observed data. Here are some examples of time series forecasting
    where we can leverage many-to-many (synced) RNNs:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的RNN的一个常见应用场景是时间序列预测，在这里我们希望根据当前和之前观察到的数据，在每个时间步进行滚动预测。以下是一些我们可以利用多对多（同步）RNN进行时间序列预测的例子：
- en: Product sales each day for a store
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个商店每天的产品销售量
- en: The daily closing price of a stock
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 股票的每日收盘价
- en: Power consumption of a factory each hour
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个工厂每小时的电力消耗
- en: They are also widely used in solving NLP problems, including PoS tagging, named-entity
    recognition, and real-time speech recognition.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 它们还广泛用于解决自然语言处理（NLP）问题，包括词性标注、命名实体识别和实时语音识别。
- en: Many-to-many (unsynced) RNNs
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多对多（不同步）RNN
- en: Sometimes, we only want to generate the output sequence *after* we’ve processed
    the entire input sequence. This is the **unsynced** version of a many-to-many
    RNN.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们只希望在处理完整个输入序列之后再生成输出序列。这是多对多RNN的**不同步**版本。
- en: 'Refer to the following diagram for the general structure of a many-to-many
    (unsynced) RNN:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考下图，了解一个多对多（不同步）RNN的一般结构：
- en: '![A picture containing screenshot, diagram, white, font  Description automatically
    generated](img/B21047_12_08.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![一个图片，包含截图、图表、白色背景、字体  自动生成的描述](img/B21047_12_08.png)'
- en: 'Figure 12.8: General form of a many-to-many (unsynced) RNN'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8：多对多（不同步）RNN的一般形式
- en: Note that the length of the output sequence (T*y* in the preceding diagram)
    can be different from that of the input sequence (T*x* in the preceding diagram).
    This provides us with some flexibility.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输出序列的长度（前面图中的T*y*）可以与输入序列的长度（前面图中的T*x*）不同。这为我们提供了一些灵活性。
- en: 'This type of RNN is the go-to model for machine translation. In French-English
    translation, for example, the model first reads a complete sentence in French
    and then produces a translated sentence in English. Multi-step ahead forecasting
    is another popular example: sometimes, we are asked to predict sales for multiple
    days in the future when given data from the past month.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的RNN是机器翻译的首选模型。例如，在法英翻译中，模型首先读取法语的完整句子，然后生成翻译后的英语句子。多步预测是另一个常见的例子：有时，我们被要求在给定过去一个月的数据时预测未来几天的销售额。
- en: You have now learned about four types of RNN based on the model’s input and
    output.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经了解了基于模型输入和输出的四种类型的RNN。
- en: Wait, what about one-to-one RNNs? There is no such thing. One-to-one is just
    a regular feedforward model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，那一对一RNN呢？并没有这种东西。一对一只是一个普通的前馈模型。
- en: We will be applying some of these types of RNN to solve projects, including
    sentiment analysis and word generation, later in this chapter. Now, let’s figure
    out how an RNN model is trained.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章稍后的部分，我们将应用这些类型的RNN来解决项目问题，包括情感分析和词语生成。现在，让我们弄清楚如何训练一个RNN模型。
- en: Training an RNN model
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练RNN模型
- en: 'To explain how we optimize the weights (parameters) of an RNN, we first annotate
    the weights and the data on the network, as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明我们如何优化RNN的权重（参数），我们首先在网络上标注权重和数据，如下所示：
- en: '*U* denotes the weights connecting the input layer and the hidden layer.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*U*表示连接输入层和隐藏层的权重。'
- en: '*V* denotes the weights between the hidden layer and the output layer. Note
    here that we use only one recurrent layer for simplicity.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V*表示隐藏层与输出层之间的权重。请注意，这里我们仅使用一个递归层以简化问题。'
- en: '*W* denotes the weights of the recurrent layer; that is, the feedback layer.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*W*表示递归层的权重；即反馈层的权重。'
- en: '*x*[t] denotes the inputs at time step *t*.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[t]表示时间步*t*的输入。'
- en: '*s*[t] denotes the hidden state at time step *t*.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*s*[t]表示时间步*t*的隐藏状态。'
- en: '*h*[t] denotes the outputs at time step *t*.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*h*[t]表示时间步*t*的输出。'
- en: 'Next, we unfold the simple RNN model over three time steps: *t* − 1, *t*, and
    *t* + 1, as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展开简单的RNN模型，涵盖三个时间步：*t* - 1，*t*，和*t* + 1，如下所示：
- en: '![A picture containing screenshot, diagram  Description automatically generated](img/B21047_12_09.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing screenshot, diagram  Description automatically generated](img/B21047_12_09.png)'
- en: 'Figure 12.9: Unfolding a recurrent layer'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.9：展开递归层
- en: 'We describe the mathematical relationships between the layers as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将层之间的数学关系描述如下：
- en: We let *a* denote the activation function for the hidden layer. In RNNs, we
    usually choose tanh or ReLU as the activation function for the hidden layers.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们让*a*表示隐藏层的激活函数。在RNN中，我们通常选择tanh或ReLU作为隐藏层的激活函数。
- en: Given the current input, *x*[t], and the previous hidden state, *s*[t][−1],
    we compute the current hidden state, *s*[t], by *s*[t] = *a*(*Ux*[t] + *Ws*[t][−1]).
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定当前输入*x*[t]和之前的隐藏状态*s*[t][−1]，我们通过*s*[t] = *a*(*Ux*[t] + *Ws*[t][−1])计算当前的隐藏状态。
- en: Feel free to read *Chapter 6*, *Predicting Stock Prices with Artificial Neural
    Networks*, again to brush up on your knowledge of neural networks.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 随时可以再读一遍*第6章*，*使用人工神经网络预测股票价格*，以复习你对神经网络的知识。
- en: In a similar manner, we compute *s*[t][−1] based on
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似地，我们根据以下公式计算*s*[t][−1]：
- en: '*s*[t][-2]:*s*[t][-1]=a(*Ux*[t][-1]+*Ws*[t][-2])'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*s*[t][-2]:*s*[t][-1]=a(*Ux*[t][-1]+*Ws*[t][-2])'
- en: 'We repeat this until *s*[1], which depends on:'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们重复这个过程，直到*s*[1]，它依赖于：
- en: '*s*[0]:*s*[1]=a(*Ux*[1]+*Ws*[0])'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*s*[0]:*s*[1]=a(*Ux*[1]+*Ws*[0])'
- en: We usually set *s*[0] to all zeros.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将*s*[0]设置为全零。
- en: We let *g* denote the activation function for the output layer. It can be a
    sigmoid function if we want to perform binary classification, a softmax function
    for multi-class classification, and a simple linear function (that is, no activation)
    for regression.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们让*g*表示输出层的激活函数。如果我们想执行二分类任务，它可以是一个sigmoid函数；对于多分类任务，它可以是softmax函数；对于回归问题，它可以是一个简单的线性函数（即没有激活函数）。
- en: 'Finally, we compute the output at time step *t*:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们计算时间步*t*的输出：
- en: '*h*[t]:*h*[t]=*g*(*Vs*[t])'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*h*[t]:*h*[t]=*g*(*Vs*[t])'
- en: With the dependency in hidden states over time steps (that is, *s*[t] depends
    on *s*[t][−1], *s*[t][−1] depends on *s*[t][−2], and so on), the recurrent layer
    brings memory to the network, which captures and retains information from all
    the previous time steps.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于隐藏状态在各个时间步之间的依赖关系（即，*s*[t]依赖于*s*[t][−1]，*s*[t][−1]依赖于*s*[t][−2]，以此类推），递归层为网络引入了记忆，它能够捕捉并保留所有之前时间步的信息。
- en: As we did for traditional neural networks, we apply the backpropagation algorithm
    to optimize all the weights, *U*, *V*, and *W*, in RNNs. However, as you may have
    noticed, the output at a time step is indirectly dependent on all the previous
    time steps (*h*^t depends on *s*[t], while *s*[t] depends on all the previous
    ones). Hence, we need to compute the loss over all previous *t*-1 time steps,
    besides the current time step. Consequently, the gradients of the weights are
    calculated this way. For example, if we want to compute the gradients at time
    step *t* = 4, we need to backpropagate the previous four time steps (*t* = 3,
    *t* = 2, *t* = 1, *t* = 0) and sum up the gradients over these five time steps.
    This version of the backpropagation algorithm is called **Back Propagation Through
    Time** (**BPTT**).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在传统神经网络中做的那样，我们应用反向传播算法来优化RNN中的所有权重：*U*、*V*和*W*。然而，正如你可能已经注意到的，某个时间步的输出间接地依赖于所有之前的时间步（*h*^t依赖于*s*[t]，而*s*[t]依赖于所有之前的时间步）。因此，我们需要计算当前时间步之外的所有先前时间步的损失。因此，权重的梯度是这样计算的。例如，如果我们想要计算时间步*t*
    = 4的梯度，我们需要反向传播前四个时间步（*t* = 3，*t* = 2，*t* = 1，*t* = 0），并将这五个时间步的梯度加总起来。这种版本的反向传播算法被称为**时间反向传播**（**BPTT**）。
- en: The recurrent architecture enables RNNs to capture information from the very
    beginning of the input sequence. This advances the predictive capability of sequence
    learning. You may be wondering whether vanilla RNNs can handle long sequences.
    They can in theory, but not in practice due to the **vanishing gradient** problem.
    A vanishing gradient means the gradient will become vanishingly small over long
    time steps, which prevents the weight from updating. I will explain this in detail
    in the next section, as well as introduce a variant architecture, LSTM, that helps
    solve this issue.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）的结构使其能够从输入序列的最开始捕捉信息。这提升了序列学习的预测能力。你可能会想，传统的 RNN 能否处理长序列。理论上它们是可以的，但实际上由于**梯度消失**问题，它们无法有效处理长序列。梯度消失意味着梯度在长时间步长下会变得极小，从而阻碍了权重的更新。我将在下一节详细解释这个问题，并介绍一种变体架构——LSTM，它有助于解决这一问题。
- en: Overcoming long-term dependencies with LSTM
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 LSTM 克服长期依赖问题
- en: Let’s start with the vanishing gradient issue in vanilla RNNs. Where does it
    come from? Recall that during backpropagation, the gradient decays along with
    each time step in the RNN (that is, *s*[t]=*a*(*Ux*[t]+*Ws*[t-1]); early elements
    in a long input sequence will have little contribution to the computation of the
    current gradient. This means that vanilla RNNs can only capture the temporal dependencies
    within a short time window. However, dependencies between time steps that are
    far away are sometimes critical signals to the prediction. RNN variants, including
    LSTM and **gated recurrent units** (**GRUs**), are specifically designed to solve
    problems that require learning long-term dependencies.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从传统 RNN 的梯度消失问题开始。这个问题是怎么产生的呢？回想一下，在反向传播过程中，梯度会随着每个时间步的传递而衰减（即，*s*[t]=*a*(*Ux*[t]+*Ws*[t-1])；长输入序列中的早期元素对当前梯度的计算几乎没有贡献。这意味着传统
    RNN 只能捕捉短时间窗口内的时序依赖。然而，时间步之间的远距离依赖有时是预测中的关键信号。包括 LSTM 和**门控循环单元**（**GRU**）在内的
    RNN 变体正是为了解决需要学习长期依赖关系的问题。
- en: We will be focusing on LSTM in this book as it is a lot more popular than GRU.
    LSTM was introduced a decade earlier and is more mature than GRU. If you are interested
    in learning more about GRU and its applications, feel free to check out *Hands-On
    Deep Learning Architectures with Python* by Yuxi Hayden Liu (Packt Publishing).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将重点介绍 LSTM，因为它比 GRU 更为流行。LSTM 是十年前提出的，比 GRU 更成熟。如果你有兴趣深入了解 GRU 及其应用，可以查阅 Yuxi
    Hayden Liu（Packt Publishing）所著的《Hands-On Deep Learning Architectures with Python》一书。
- en: In LSTM, we use a grating mechanism to handle long-term dependencies. Its magic
    comes from a memory unit and three information gates built on top of the recurrent
    cell. The word “gate” is taken from the logic gate in a circuit ([https://en.wikipedia.org/wiki/Logic_gate](https://en.wikipedia.org/wiki/Logic_gate)).
    It is basically a sigmoid function whose output value ranges from `0` to `1`.
    `0` represents the “off” logic, while `1` represents the “on” logic.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LSTM 中，我们使用了一个门控机制来处理长期依赖。它的“魔力”来源于一个记忆单元和三个基于循环单元的门控结构。“门”（gate）这个词源自于电路中的逻辑门（[https://en.wikipedia.org/wiki/Logic_gate](https://en.wikipedia.org/wiki/Logic_gate)）。它本质上是一个
    Sigmoid 函数，其输出值范围从 `0` 到 `1`。`0` 表示“关闭”逻辑，而 `1` 表示“开启”逻辑。
- en: 'The LSTM version of the recurrent cell is depicted in the following diagram,
    right after the vanilla version for comparison:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 版本的循环单元如下图所示，在传统版本之后以便对比：
- en: '![A picture containing diagram, text, technical drawing, plan  Description
    automatically generated](img/B21047_12_10.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing diagram, text, technical drawing, plan  Description
    automatically generated](img/B21047_12_10.png)'
- en: 'Figure 12.10: Recurrent cell in vanilla RNNs versus LSTM RNNs'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.10：传统 RNN 和 LSTM RNN 的循环单元对比
- en: 'Let’s look at the LSTM recurrent cell in detail from left to right:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从左到右详细看一下 LSTM 循环单元：
- en: '*c*[t] is the **memory unit**. It memorizes information from the very beginning
    of the input sequence.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c*[t] 是**记忆单元**。它从输入序列的最开始就开始记忆信息。'
- en: '*f* stands for the **forget gate**. It determines how much information from
    the previous memory state, *c*[t][−1], to forget, or, in other words, how much
    information to pass forward. Let *W*^f denote the weights between the forget gate
    and the previous hidden state, *s*[t][−1], and *U*^f denote the weights between
    the forget gate and the current input, *x*[t].'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f* 代表**遗忘门**。它决定了从之前的记忆状态 *c*[t][−1] 中需要遗忘多少信息，或者换句话说，需要传递多少信息。设 *W*^f 为遗忘门和先前隐藏状态
    *s*[t][−1] 之间的权重，*U*^f 为遗忘门和当前输入 *x*[t] 之间的权重。'
- en: '*i* represents the **input gate**. It controls how much information from the
    current input to put through. *W*^i and *U*^i are the weights connecting the input
    gate to the previous hidden state, *s*[t][−1], and the current input, *x*[t],
    respectively.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*i* 代表 **输入门**。它控制从当前输入中传递多少信息。*W*^i 和 *U*^i 是连接输入门与前一个隐藏状态 *s*[t][−1] 和当前输入
    *x*[t] 的权重。'
- en: '*tanh* is simply the activation function for the hidden state. It acts as the
    *a* in the vanilla RNN. Its output is computed based on the current input, *x*[t],
    along with the associated weights, *U*^c, the previous hidden state, *s*[t][−1],
    and the corresponding weights, *W*^c.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*tanh* 是隐藏状态的激活函数。它充当了普通RNN中的 *a*。它的输出是基于当前输入 *x*[t]，以及相关的权重 *U*^c，前一个隐藏状态
    *s*[t][−1] 和相应的权重 *W*^c 计算出来的。'
- en: '`o` serves as the **output gate**. It defines how much information is extracted
    from the internal memory for the output of the entire recurrent cell. As always,
    *W*^o and *U*^o are the associated weights for the previous hidden state and current
    input, respectively.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`o` 作为 **输出门**。它定义了从内部记忆中提取多少信息作为整个循环单元的输出。像往常一样，*W*^o 和 *U*^o 是分别与前一个隐藏状态和当前输入相关的权重。'
- en: 'We describe the relationship between these components as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述这些组件之间的关系如下：
- en: The output of the forget gate, *f*, at time step *t* is computed as
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忘记门的输出，*f*，在时间步 *t* 的计算为：
- en: '*f*=*sigmoid*(*W*^f*s*[t-1]+*U*^f*x*[t])'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*f* = *sigmoid*(*W*^f * s*[t-1] + *U*^f * x*[t])'
- en: The output of the input gate, *i*, at time step *t* is computed as
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入门的输出，*i*，在时间步* t *的计算为：
- en: '*i*=*sigmoid*(*W*^i*s*[t-1]+*U*^i*x*[t])'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*i* = *sigmoid*(*W*^i * s*[t-1] + *U*^i * x*[t])'
- en: The output of the tanh activation, *c’*, at time step *t* is computed as
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tanh激活函数的输出，*c’*，在时间步 *t* 的计算为：
- en: '*c’*=*tanh*(*W*^c*s*[t-1]+*U*^c*x*[t])'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*c’* = *tanh*(*W*^c * s*[t-1] + *U*^c * x*[t])'
- en: The output of the output gate, *o*, at time step *t* is computed as
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出门的输出，*o*，在时间步 *t* 的计算为：
- en: '*o*=*sigmoid*(*W*^o*s*[t-1]+*U*^o*x*[t])'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*o* = *sigmoid*(*W*^o * s*[t-1] + *U*^o * x*[t])'
- en: The memory unit, *c*[t], at time step *t* is updated using *c*[t]=*f*.**c*[t-1]+*i*.**c’*
    (here, the operator .* denotes element-wise multiplication). Again, the output
    of a sigmoid function has a value from 0 to 1\. Hence, the forget gate, *f*, and
    input gate, *i*, control how much of the previous memory, *c*[t][−1], and the
    current memory input, *c’*, to carry forward, respectively.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记忆单元 *c*[t] 在时间步 *t* 被更新为 *c*[t] = *f*.**c*[t-1] + *i*.**c’*（其中，操作符 .* 表示逐元素乘法）。同样，sigmoid函数的输出值在0到1之间。因此，忘记门
    *f* 和输入门 *i* 分别控制从先前的记忆 *c*[t][−1] 和当前记忆输入 *c’* 中携带多少信息。
- en: Finally, we update the hidden state, *s*[t], at time step *t* by *s*[t]=*o*.**c*[t]
    . Here, the output gate, *o*, governs how much of the updated memory unit, *c*[t],
    will be used as the output of the entire cell.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们通过 *s*[t] = *o*.**c*[t] 更新隐藏状态 *s*[t]，其中 *o* 是输出门，控制更新后的记忆单元 *c*[t] 被用于整个单元的输出的程度。
- en: '**Best practice**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'LSTM is often considered the default choice for RNN models in practice due
    to its ability to effectively capture long-term dependencies in sequential data
    while mitigating the vanishing gradient problem. However, GRUs are also commonly
    used depending on the specific task and dataset characteristics. The choice between
    LSTM and GRU depends on the following factors:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM通常被认为是实际应用中RNN模型的默认选择，因为它能够有效地捕捉序列数据中的长期依赖关系，同时缓解梯度消失问题。然而，GRU也常用于特定任务和数据集特征的情况下。LSTM和GRU的选择取决于以下因素：
- en: '**Model complexity**: LSTMs typically have more parameters than GRUs due to
    their additional gating mechanisms. If you have limited computational resources
    or are working with smaller datasets, GRUs may be more suitable due to their simpler
    architecture.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型复杂度**：LSTM通常比GRU有更多的参数，因为它们有额外的门控机制。如果你有有限的计算资源或处理较小的数据集，GRU由于其更简单的架构可能更适合。'
- en: '**Training speed**: GRUs are generally faster to train than LSTMs. If training
    time is a concern, GRUs might be a better choice.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练速度**：GRU通常比LSTM训练速度更快。如果训练时间是一个考虑因素，GRU可能是更好的选择。'
- en: '**Performance**: LSTMs tend to have better performance on tasks that require
    modeling long-term dependencies in sequential data. If your task involves capturing
    complex temporal patterns and you’re concerned about overfitting, LSTMs might
    be preferable.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**：LSTM通常在需要建模序列数据中长期依赖关系的任务中表现更好。如果你的任务涉及捕捉复杂的时间模式并且担心过拟合，那么LSTM可能更合适。'
- en: As always, we apply the **BPTT** algorithm to train all the weights in LSTM
    RNNs, including four sets each of weights, *U* and *W*, associated with three
    gates and the tanh activation function. By learning these weights, the LSTM network
    explicitly models long-term dependencies in an efficient way. Hence, LSTM is the
    go-to or default RNN model in practice.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，我们应用**BPTT**算法训练LSTM RNN中的所有权重，包括与三个门和tanh激活函数相关的四组权重，分别是*U*和*W*。通过学习这些权重，LSTM网络高效地显式建模了长期依赖关系。因此，LSTM是实际应用中首选的RNN模型。
- en: Next, you will learn how to use LSTM RNNs to solve real-world problems. We will
    start by categorizing movie review sentiment.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将学习如何使用LSTM RNN解决实际问题。我们将从对电影评论情感进行分类开始。
- en: Analyzing movie review sentiment with RNNs
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RNN分析电影评论情感
- en: 'So, here comes our first RNN project: movie review sentiment. We’ll use the
    IMDb ([https://www.imdb.com/](https://www.imdb.com/)) movie review dataset ([https://ai.stanford.edu/~amaas/data/sentiment/](https://ai.stanford.edu/~amaas/data/sentiment/))
    as an example. It contains 25,000 highly popular movie reviews for training and
    another 25,000 for testing. Each review is labeled as 1 (positive) or 0 (negative).
    We’ll build our RNN-based movie sentiment classifier in the following three sections:
    *Analyzing and preprocessing the movie review data, Developing a simple LSTM network,*
    and *Boosting the performance with multiple LSTM layers*.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，进入我们的第一个RNN项目：电影评论情感分析。我们将以IMDb（[https://www.imdb.com/](https://www.imdb.com/)）电影评论数据集（[https://ai.stanford.edu/~amaas/data/sentiment/](https://ai.stanford.edu/~amaas/data/sentiment/)）为例。该数据集包含25,000条用于训练的高人气电影评论和另外25,000条用于测试的评论。每条评论都标记为1（正面）或0（负面）。我们将通过以下三个部分构建基于RNN的电影情感分类器：*分析与预处理电影评论数据，开发简单的LSTM网络*，以及*通过多个LSTM层提升性能*。
- en: Analyzing and preprocessing the data
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分析与预处理
- en: 'We’ll start with data analysis and preprocessing, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从数据分析和预处理开始，具体步骤如下：
- en: 'PyTorch’s `torchtext` has a built-in IMDb dataset, so first, we load the dataset:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PyTorch的`torchtext`内置了IMDb数据集，因此我们首先加载该数据集：
- en: '[PRE0]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We just load 25,000 training samples and 25,000 test samples.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加载25,000个训练样本和25,000个测试样本。
- en: 'If you encounter any errors while running the code, consider installing the
    `torchtext` and `portalocker` packages. You could use the following commands for
    installation via `conda`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在运行代码时遇到任何错误，请考虑安装`torchtext`和`portalocker`包。你可以使用以下命令通过`conda`安装：
- en: '[PRE1]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Or, via `pip`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，通过`pip`安装：
- en: '[PRE2]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let’s explore the vocabulary within the training set:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们探索训练集中的词汇：
- en: '[PRE3]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we define a function to extract tokens (words, in our case) from a given
    document (movie review, in our case). It first removes HTML-like tags, then extracts
    and standardizes emoticons, removes non-alphanumeric characters, and tokenizes
    the text into a list of words for further processing. We store the tokens and
    their occurrences in the `Counter` object `token_counts`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义一个函数，从给定的文档（在本例中为电影评论）中提取令牌（单词）。该函数首先移除类似HTML的标签，然后提取并标准化表情符号，去除非字母数字字符，并将文本分割成单词列表以供进一步处理。我们将令牌及其出现次数存储在`Counter`对象`token_counts`中。
- en: As evident, the training set comprises approximately 76,000 unique words, and
    it exhibits a perfect balance with an equal count of positive (labeled as “`2`")
    and negative (labeled as “`1`") samples.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，训练集包含大约76,000个独特的词汇，并且正负样本数量完全平衡，正样本标记为“`2`”，负样本标记为“`1`”。
- en: 'We will feed the word tokens into an embedding layer, `nn.Embedding`. The embedding
    layer requires integer input because it’s specifically designed to handle discrete
    categorical data, such as word indices, and transform them into continuous representations
    that a neural network can work with and learn from. Therefore, we need to first
    encode each token into a unique integer as follows:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将词汇令牌输入到嵌入层`nn.Embedding`中。嵌入层需要整数输入，因为它专门设计用于处理离散的类别数据（如单词索引），并将其转化为神经网络可以处理并学习的连续表示。因此，我们需要首先将每个令牌编码为唯一的整数，如下所示：
- en: '[PRE4]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We use the `vocab` module in PyTorch to create a vocabulary (token mapping)
    based on the frequency of words in the corpus. But this vocabulary is not complete
    yet. Let’s see why in the next two steps.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用PyTorch中的`vocab`模块，根据语料库中单词的频率创建词汇表（令牌映射）。但这个词汇表还不完整，接下来两个步骤我们将说明原因。
- en: 'When examining the document lengths within the training set, you’ll notice
    that they range from 10 to 2,498 words. It’s common practice to apply padding
    to sequences to ensure uniform length during batch processing. So, we insert the
    special token, `"<pad>"`, representing padding into the vocabulary mapping at
    index `0` as a **placeholder**:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检查训练集中文档的长度时，你会注意到它们的长度从10个词到2498个词不等。通常的做法是对序列进行填充，以确保批处理时长度一致。因此，我们将特殊符号`"<pad>"`（表示填充）插入词汇映射，位置为索引`0`，作为**占位符**：
- en: '[PRE5]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We also need to handle unseen words during inference. Similar to the previous
    step, we insert the special token `"<unk>"` (short for “unknown”) into the vocabulary
    mapping at index `1`. The token represents out-of-vocabulary words or tokens that
    are not found in the training data:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要在推理过程中处理未见过的单词。与之前的步骤类似，我们将特殊符号`"<unk>"`（代表“未知”）插入到词汇映射中，位置为索引`1`。该符号表示词汇表外的单词或在训练数据中找不到的单词：
- en: '[PRE6]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We also set the default vocabulary mapping to `1`. This means `"<unk>"` (index
    1) is used as the default index for unseen or out-of-vocabulary words.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将默认的词汇映射设置为`1`。这意味着`"<unk>"`（索引1）被用作未见过的或词汇表外单词的默认索引。
- en: 'Let’s take a look at the following examples showing the mappings of given words,
    including an unseen one:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下以下示例，展示给定单词的映射，包括一个未见过的单词：
- en: '[PRE7]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: By now, we have the complete vocabulary mapping.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经完成了完整的词汇映射。
- en: '**Best practice**'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'Using special tokens like `<pad>` and `<unk>` in RNNs is a common practice
    for handling variable-length sequences and out-of-vocabulary words. Here are some
    best practices for their usage:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在RNN中使用特殊符号如`<pad>`和`<unk>`是处理变长序列和词汇表外单词的常见做法。以下是它们使用的一些最佳实践：
- en: Use `<pad>` tokens to pad sequences to a fixed length. This ensures that all
    input sequences have the same length, which is necessary for efficient batch processing
    in neural networks. Pad sequences at the end rather than the beginning to preserve
    the order of the input data. When tokenizing text data, assign a unique integer
    index to the `<pad>` token and ensure that it corresponds to a vector of zeros
    in the embedding matrix.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`<pad>`符号将序列填充为固定长度。这样可以确保所有输入序列具有相同的长度，这是神经网络中高效批处理所必需的。将填充添加到序列的末尾，而不是开头，以保持输入数据的顺序。在对文本数据进行分词时，为`<pad>`符号分配一个唯一的整数索引，并确保它在嵌入矩阵中对应一个零向量。
- en: Use `<unk>` tokens to represent out-of-vocabulary words that are not present
    in the vocabulary of the model. During inference, replace any words that are not
    present in the vocabulary with the `<unk>` token to ensure that the model can
    process the input.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`<unk>`符号表示不在模型词汇表中的词汇表外单词。在推理时，将任何不在词汇表中的单词替换为`<unk>`符号，以确保模型能够处理输入。
- en: Exclude `<pad>` tokens from contributing to the loss during training to avoid
    skewing the learning process.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，排除`<pad>`符号对损失的贡献，以避免扭曲学习过程。
- en: Monitor the distribution of `<unk>` tokens in the dataset to assess the prevalence
    of out-of-vocabulary words and adjust the vocabulary size accordingly.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控数据集中`<unk>`符号的分布，以评估词汇表外单词的普遍性，并相应地调整词汇表的大小。
- en: 'Next, we define the function defining how batches of samples should be collated:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数，定义如何将样本批次进行整理：
- en: '[PRE8]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Besides generating inputs and label outputs as we used to do, we also generate
    the length of individual samples in a given batch. Note that we convert the positive
    label from the raw 2 to 1 here, for label standardization and loss function compatibility
    for binary classification. The length information is used for handling variable-length
    sequences efficiently. Take a small batch of four samples and examine the processed
    batch:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 除了像以前那样生成输入和标签输出外，我们还会生成给定批次中每个样本的长度信息。注意，在这里我们将原始标签中的正标签从2转换为1，以进行标签标准化和适配二元分类的损失函数。长度信息用于高效处理变长序列。取一个包含四个样本的小批次，并检查处理后的批次：
- en: '[PRE9]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can see the processed text sequences have been standardized to a length
    of 247 tokens, with the first, second, and fourth samples padded with 0s.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，处理过的文本序列已标准化为247个标记的长度，第一、第二和第四个样本使用0进行填充。
- en: 'Finally, we batch the training and testing set:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们对训练集和测试集进行批处理：
- en: '[PRE10]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The generated data loaders are ready to use for sentiment prediction.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的数据加载器已准备好用于情感预测。
- en: Let’s move on to building an LSTM network.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续构建LSTM网络。
- en: Building a simple LSTM network
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建一个简单的LSTM网络
- en: 'Now that the training and testing data loaders are ready, we can build our
    first RNN model with an embedding layer that encodes the input word tokens, and
    an LSTM layer followed by a fully connected layer:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，训练和测试数据加载器已经准备好，我们可以构建第一个RNN模型，该模型包含一个将输入单词标记编码的嵌入层，一个LSTM层，随后是一个全连接层：
- en: 'First, we define the network hyperparameters, including the input dimension
    and the embedding dimension of the embedding layer:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们定义网络超参数，包括输入维度和嵌入层的嵌入维度：
- en: '[PRE11]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We also define the number of hidden nodes in the LSTM layer and the fully connected
    layer:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了LSTM层和全连接层中的隐藏节点数量：
- en: '[PRE12]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we build our RNN model class:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们构建我们的RNN模型类：
- en: '[PRE13]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `nn.Embedding` layer is used to convert input word indices into dense word
    embeddings. The `padding_idx` parameter is set to 0, indicating that padding tokens
    should be ignored during embedding.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.Embedding`层用于将输入的单词索引转换为密集的词嵌入。`padding_idx`参数设置为0，表示在嵌入过程中应忽略填充标记。'
- en: The recurrent layer, `nn.LSTM`, takes the embedded input sequence and processes
    it sequentially. `batch_first=True` means that the input has a batch size as the
    first dimension.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 递归层`nn.LSTM`接受嵌入的输入序列并按顺序处理。`batch_first=True`表示输入的第一维是批次大小。
- en: The fully connected hidden layer, `fc1`, follows the LSTM layer, and the ReLU
    activation is applied to the output of the fully connected layer.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接隐藏层`fc1`位于LSTM层之后，对全连接层的输出应用ReLU激活函数。
- en: The final layer has a single output because this model is used for binary classification
    (sentiment analysis).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层只有一个输出，因为该模型用于二分类（情感分析）。
- en: In the forward pass method, `pack_padded_sequence` is used to pack and pad sequences
    for efficient processing in the LSTM layer. The packed sequence is passed through
    the LSTM layer, and the final hidden state (`hidden[-1, :, :]`) is extracted.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传递方法中，`pack_padded_sequence`用于打包和填充序列，以便在LSTM层中高效处理。打包后的序列通过LSTM层处理，并提取最终的隐藏状态（`hidden[-1,
    :, :]`）。
- en: 'We then create an instance of the LSTM model with the specific hyperparameters
    we defined earlier. We also ensure that the model is placed on the specified computing
    device (GPU if available) for training and inference:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们创建一个LSTM模型的实例，使用我们之前定义的特定超参数。我们还确保将模型放置在指定的计算设备上（如果有GPU，则使用GPU）进行训练和推理：
- en: '[PRE14]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As for the loss function, we use `nn.BCELoss()` since it is a binary classification
    problem. We also set the corresponding optimizer and try with a learning rate
    of `0.003` as follows:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 至于损失函数，我们使用`nn.BCELoss()`，因为这是一个二分类问题。我们还设置了相应的优化器，并尝试使用学习率为`0.003`，如下所示：
- en: '[PRE15]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we define a training function responsible for training the model for one
    iteration:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义一个训练函数，负责训练模型进行一次迭代：
- en: '[PRE16]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It also displays the training loss and accuracy at the end of an iteration.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 它还显示了每次迭代结束时的训练损失和准确率。
- en: 'We then train the model for 10 iterations:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们训练模型10次迭代：
- en: '[PRE17]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The training accuracy is close to 100% after 10 iterations.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 经过10次迭代后，训练准确率接近100%。
- en: 'Finally, we evaluate the performance on the test set:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们在测试集上评估性能：
- en: '[PRE18]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We obtained a test accuracy of 86%.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得了86%的测试准确率。
- en: Stacking multiple LSTM layers
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠多个LSTM层
- en: 'We can also stack two (or more) recurrent layers. The following diagram shows
    how two recurrent layers can be stacked:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以堆叠两个（或更多）递归层。以下图示展示了如何堆叠两个递归层：
- en: '![A picture containing screenshot, diagram, line  Description automatically
    generated](img/B21047_12_11.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing screenshot, diagram, line  Description automatically
    generated](img/B21047_12_11.png)'
- en: 'Figure 12.11: Unfolding two stacked recurrent layers'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.11：展开两个堆叠的递归层
- en: 'In PyTorch, stacking multiple RNN layers is simple. Using LSTM as an example
    once more, it suffices to specify the number of LSTM layers in the `num_layers`
    argument:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，堆叠多个RNN层非常简单。以LSTM为例，只需在`num_layers`参数中指定LSTM层的数量：
- en: '[PRE19]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In this example, we stack two LSTM layers. Feel free to experiment with a multi-layer
    RNN model and see whether you can beat the previous single-layered model.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们堆叠了两个LSTM层。可以尝试使用多层RNN模型，看看是否能超过之前的单层模型。
- en: With that, we’ve just finished the review sentiment classification project using
    RNNs. In the next project, we will revisit stock price prediction and solve it
    using RNNs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们刚刚完成了使用RNN进行情感分类项目的回顾。在下一个项目中，我们将重新探讨股票价格预测，并使用RNN解决它。
- en: Revisiting stock price forecasting with LSTM
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LSTM重新预测股票价格
- en: 'Recall in *Chapter 6**, Predicting Stock Prices with Artificial Neural Networks*,
    we derived features from past prices and performance within a specific time step
    and then trained a standard neural network. In this instance, we will utilize
    RNNs as the sequential model and harness features from five consecutive time steps
    rather than just one. Let’s examine the process in the following steps:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下在*第六章*《使用人工神经网络预测股票价格》中，我们从过去的价格和特定时间步内的表现中提取特征，然后训练标准神经网络。在本例中，我们将利用RNN作为顺序模型，利用五个连续时间步的特征，而不仅仅是一个。让我们通过以下步骤来查看过程：
- en: 'Initially, we load the stock data, create the features and labels, and then
    split it into training and test sets, mirroring our approach in *Chapter 6*:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最初，我们加载股票数据，创建特征和标签，然后将其拆分为训练集和测试集，模仿我们在*第六章*中的做法：
- en: '[PRE20]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here, we reuse the feature and label generation function, `generate_features`,
    defined in *Chapter 6*. Similarly, we scale the feature space using `StandardScaler`
    and covert data into `FloatTensor`:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们重用了在*第六章*中定义的特征和标签生成函数`generate_features`。类似地，我们使用`StandardScaler`对特征空间进行缩放，并将数据转换为`FloatTensor`：
- en: '[PRE21]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we define a function to create sequences:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数来创建序列：
- en: '[PRE22]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here, every generated sequence comprises two components: the input sequence,
    which encompasses features from five successive days, and the label, representing
    the price of the last day in that five-day period. We generate sequences for the
    training and test sets respectively.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个生成的序列由两部分组成：输入序列，包含五天连续的特征；标签，表示这五天期间最后一天的价格。我们分别为训练集和测试集生成序列。
- en: 'Subsequently, we establish a data loader for the training sequences in preparation
    for model construction and training:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随后，我们为训练序列建立一个数据加载器，为模型构建和训练做准备：
- en: '[PRE23]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We set 128 as the batch size in this project.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将批量大小设置为128。
- en: 'Now, we define an RNN model with a two-layered LSTM followed by a fully connected
    layer and an output layer for regression:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义一个包含两层LSTM的RNN模型，后接一个全连接层和一个回归输出层：
- en: '[PRE24]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The LSTM layer captures sequential dependencies in the input data, and the fully
    connected layers perform the final regression.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM层捕捉输入数据中的顺序依赖性，全连接层执行最终的回归任务。
- en: 'Next, we initiate a model after specifying the input dimension and hidden layer
    dimensions, and use MSE as the loss function:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们在指定输入维度和隐层维度后初始化模型，并使用MSE作为损失函数：
- en: '[PRE25]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Small to medium values (like 16) are often used as starting points for hidden
    dimensions in RNNs, for computational efficiency. The chosen optimizer (Adam)
    and learning rate (`0.01`) are hyperparameters that can be tuned for better performance.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 小到中等的数值（如16）通常作为RNN隐层维度的起始值，以提高计算效率。所选的优化器（Adam）和学习率（`0.01`）是可以调优的超参数，以获得更好的性能。
- en: 'Next, we train the model for 1000 iterations as follows:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将模型训练1000次，步骤如下：
- en: '[PRE26]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The MSE during training is displayed every 100 iterations.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 训练期间每100次迭代显示一次MSE。
- en: 'Finally, we apply the trained model on the test set and evaluate the performance:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将训练好的模型应用于测试集并评估性能：
- en: '[PRE27]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We are able to obtain an *R*² of `0.9` on the test set. You may observe that
    this doesn’t outperform our previous standard neural network. The reason is that
    we have a relatively small training dataset of only eight thousand samples. RNNs
    typically require a larger dataset to excel.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在测试集上获得了`R`²为`0.9`的结果。你可能会注意到，这并未超越我们之前的标准神经网络。原因是我们的训练数据集相对较小，仅有八千个样本。RNN通常需要更大的数据集才能表现优异。
- en: The two RNN models we’ve explored up to this point followed the many-to-one
    structure. In our upcoming project, we’ll create an RNN using the many-to-many
    structure, and the objective is to generate a “novel.”
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们探讨的两个RNN模型都遵循了多对一结构。在接下来的项目中，我们将使用多对多结构创建一个RNN，目标是生成一部“小说”。
- en: Writing your own War and Peace with RNNs
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RNN编写自己的《战争与和平》
- en: In this project, we’ll work on an interesting language modeling problem – text
    generation.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将解决一个有趣的语言建模问题——文本生成。
- en: An RNN-based text generator can write anything, depending on what text we feed
    it. The training text can be from a novel such as *A Game of Thrones*, a poem
    from Shakespeare, or the movie scripts for *The Matrix*. The artificial text that’s
    generated should read similarly (but not identically) to the original one if the
    model is well trained. In this section, we are going to write our own *War and
    Peace* with RNNs, a novel written by the Russian author Leo Tolstoy. Feel free
    to train your own RNNs on any of your favorite books.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 基于RNN的文本生成器可以根据我们输入的文本生成任何内容。训练文本可以来自小说，如*权力的游戏*，莎士比亚的诗歌，或电影剧本如*黑客帝国*。如果模型训练良好，生成的人工文本应与原文相似（但不完全相同）。在这一部分，我们将使用RNN编写我们自己的*战争与和平*，这是俄罗斯作家列夫·托尔斯泰所著的一部小说。你也可以根据你喜欢的任何书籍训练自己的RNN。
- en: We will start with data acquisition and analysis before constructing the training
    set. After that, we will build and train an RNN model for text generation.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在构建训练集之前进行数据获取和分析。之后，我们将构建并训练一个用于文本生成的RNN模型。
- en: Acquiring and analyzing the training data
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取和分析训练数据
- en: I recommend downloading text data for training from books that are not currently
    protected by copyright. Project Gutenberg ([www.gutenberg.org](https://www.gutenberg.org))
    is a great place for this. It provides over 60,000 free e-books whose copyright
    has expired.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议从目前不受版权保护的书籍中下载文本数据进行训练。《古腾堡计划》([www.gutenberg.org](https://www.gutenberg.org))是一个很好的选择，它提供超过60,000本版权已过期的免费电子书。
- en: 'The original work, *War and Peace*, can be downloaded from [http://www.gutenberg.org/ebooks/2600](http://www.gutenberg.org/ebooks/2600),
    but note that there will be some cleanup, such as removing the extra beginning
    section “*The Project Gutenberg EBook*,” the table of contents, and the extra
    appendix “*End of the Project Gutenberg EBook of War and Peace*” of the plain
    text UTF-8 file ([http://www.gutenberg.org/files/2600/2600-0.txt](http://www.gutenberg.org/files/2600/2600-0.txt)),
    required. So, instead of doing this, we will download the cleaned text file directly
    from [https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt](https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt).
    Let’s get started:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 原作*战争与和平*可以从[http://www.gutenberg.org/ebooks/2600](http://www.gutenberg.org/ebooks/2600)下载，但请注意，需要进行一些清理工作，如去除额外的开头部分“*The
    Project Gutenberg EBook*”、目录和纯文本UTF-8文件的额外附录“*End of the Project Gutenberg EBook
    of War and Peace*”([http://www.gutenberg.org/files/2600/2600-0.txt](http://www.gutenberg.org/files/2600/2600-0.txt))。因此，我们将直接从[https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt](https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt)下载已清理的文本文件。让我们开始吧：
- en: 'First, we read the file and convert the text into lowercase:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们读取文件并将文本转换为小写字母：
- en: '[PRE28]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then, we take a quick look at the training text data by printing out the first
    200 characters:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们通过打印出前200个字符来快速查看训练文本数据：
- en: '[PRE29]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we count the number of unique words:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们统计唯一单词的数量：
- en: '[PRE30]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, we count the total number of characters:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们统计总字符数：
- en: '[PRE31]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'From these 3 million characters, we obtain the unique characters, as follows:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这300万个字符中，我们获得唯一的字符，如下所示：
- en: '[PRE32]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The raw training text is made up of 57 unique characters and close to 40,000
    unique words. Generating words, which requires computing 40,000 probabilities
    at one step, is far more difficult than generating characters, which requires
    computing only 57 probabilities at one step. Hence, we treat a character as a
    token, and the vocabulary here is composed of 57 characters.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 原始训练文本由57个唯一字符和接近40,000个唯一单词组成。生成单词的难度远大于生成字符，因为生成单词需要一步计算40,000个概率，而生成字符只需要一步计算57个概率。因此，我们将字符视为一个token，词汇表由57个字符组成。
- en: So, how can we feed the characters to the RNN model and generate output characters?
    Let’s see in the next section.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何将字符输入到RNN模型中并生成输出字符呢？我们将在下一部分中见到。
- en: Constructing the training set for the RNN text generator
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为RNN文本生成器构建训练集
- en: Recall that in a synced “many-to-many” RNN, the network takes in a sequence
    and simultaneously produces a sequence; the model captures the relationships among
    the elements in a sequence and reproduces a new sequence based on the learned
    patterns. As for our text generator, we can feed in fixed-length sequences of
    characters and let it generate sequences of the same length, where each output
    sequence is one character shifted from its input sequence. The following example
    will help you understand this better.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，在一个同步的“多对多”RNN中，网络接受一个序列并同时生成一个序列；模型捕捉序列中各元素之间的关系，并基于学习到的模式生成一个新的序列。对于我们的文本生成器，我们可以输入固定长度的字符序列，让它生成相同长度的序列，其中每个输出序列比输入序列向右偏移一个字符。以下示例将帮助你更好地理解这一点。
- en: 'Say that we have a raw text sample, “`learning`,” and we want the sequence
    length to be 5\. Here, we can have an input sequence, “`learn`,” and an output
    sequence, “`earni`.” We can put them into the network as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个原始文本样本“`learning`”，并且我们希望序列长度为5。在这种情况下，我们可以有一个输入序列“`learn`”和一个输出序列“`earni`”。我们可以将它们输入到网络中，如下所示：
- en: '![A diagram of a flowchart  Description automatically generated with low confidence](img/B21047_12_12.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![一个流程图的示意图，自动生成的描述，信心较低](img/B21047_12_12.png)'
- en: 'Figure 12.12: Feeding a training set (“learn,” “earni”) to the RNN'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.12：将训练集（“learn”，“earni”）输入RNN
- en: 'We’ve just constructed a training sample `("learn`,” “`earni`"). Similarly,
    to construct training samples from the entire original text, first, we need to
    split the original text into fixed-length sequences, *X*; then, we need to ignore
    the first character of the original text and split shift it into sequences of
    the same length, *Y*. A sequence from *X* is the input of a training sample, while
    the corresponding sequence from *Y* is the output of the sample. Let’s say we
    have a raw text sample, “machine learning by example,” and we set the sequence
    length to 5\. We will construct the following training samples:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚构造了一个训练样本`("learn","` “`earni`")。类似地，要从整个原始文本构造训练样本，首先，我们需要将原始文本拆分成固定长度的序列，*X*；然后，我们需要忽略原始文本的第一个字符，并将其拆分成与原序列长度相同的序列，*Y*。*X*中的一个序列是训练样本的输入，而*Y*中的对应序列是样本的输出。假设我们有一个原始文本样本“machine
    learning by example”，并且我们将序列长度设置为5，我们将构造以下训练样本：
- en: '![A picture containing screenshot, number  Description automatically generated](img/B21047_12_13.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含截图的图片，自动生成的描述](img/B21047_12_13.png)'
- en: 'Figure 12.13: Training samples constructed from “machine learning by example”'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.13：从“machine learning by example”构建的训练样本
- en: Here, ![](img/Icon.png) denotes space. Note that the remaining subsequence,
    “le,” is not long enough, so we simply ditch it.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Icon.png)表示空格。请注意，剩余的子序列“le”长度不足，因此我们将其丢弃。
- en: 'We also need to one-hot encode the input and output characters since neural
    network models only take in numerical data. We simply map the 57 unique characters
    to indices from 0 to 56, as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要对输入和输出字符进行独热编码，因为神经网络模型只接受数字数据。我们简单地将57个独特字符映射到从0到56的索引，如下所示：
- en: '[PRE33]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: For instance, the character `c` becomes a vector of length 57 with `1` in index
    28 and `0`s in all other indices; the character `h` becomes a vector of length
    57 with `1` in index 33 and `0`s in all other indices.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，字符`c`变成了一个长度为57的向量，其中索引28的位置为`1`，其他索引位置都是`0`；字符`h`变成了一个长度为57的向量，其中索引33的位置为`1`，其他索引位置都是`0`。
- en: 'Now that the character lookup dictionary is ready, we can construct the entire
    training set, as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在字符查找字典已经准备好了，我们可以构建整个训练集，如下所示：
- en: '[PRE34]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Here, we set the sequence length to `40` and obtain training samples of a length
    of `41` where the first 40 elements represent the input, and the last 40 elements
    represent the target.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将序列长度设置为`40`，并获得长度为`41`的训练样本，其中前40个元素表示输入，最后40个元素表示目标。
- en: 'Next, we initialize the training dataset object and data loader, which will
    be used for model training:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们初始化训练数据集对象和数据加载器，这将用于模型训练：
- en: '[PRE35]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We just create a data loader in batches of 64 sequences, shuffle the data at
    the beginning of each epoch, and drop any remaining data points that don’t fit
    into a complete batch.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需创建一个每批64个序列的数据加载器，在每个训练周期开始时打乱数据，并丢弃任何无法完整组成批次的剩余数据点。
- en: We finally got the training set ready and it is time to build and fit the RNN
    model. Let’s do this in the next section.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于准备好了训练集，现在是时候构建和拟合RNN模型了。让我们在下一节中进行操作。
- en: Building and training an RNN text generator
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建和训练RNN文本生成器
- en: 'We first build the RNN model as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先构建RNN模型，如下所示：
- en: '[PRE36]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This class defines a sequence-to-sequence model that takes tokenized input,
    converts token indices into dense vector representation with an embedding layer,
    processes the dense vectors through an LSTM layer, and generates logits for the
    next token in the sequence.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类定义了一个序列到序列的模型，它接受标记化的输入，将标记索引转换为密集的向量表示，经过嵌入层处理后，通过LSTM层处理这些密集向量，并生成序列中下一个标记的logits。
- en: 'In this class, the `init_hidden` method initializes the hidden state and cell
    state of the LSTM. It takes `batch_size` as a parameter, which is used to determine
    the batch size for the initial states. Two tensors are created: `hidden` and `cell`,
    both initialized with zeros. The `forward` method receives two additional inputs,
    `hidden` and `cell`, which correspond to the many-to-many architecture of our
    RNN model.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类中，`init_hidden`方法初始化LSTM的隐藏状态和细胞状态。它接受`batch_size`作为参数，用于确定初始状态的批次大小。创建了两个张量：`hidden`和`cell`，它们都初始化为零。`forward`方法接收两个额外的输入：`hidden`和`cell`，它们对应于我们RNN模型的多对多架构。
- en: One more thing to note, we use logits as outputs of the model here instead of
    probabilities, as we will sample from the predicted logits to generate new sequences
    of characters.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一点需要注意的是，这里我们使用logits作为模型的输出，而不是概率，因为我们将从预测的logits中进行采样，以生成新的字符序列。
- en: 'Now, let’s train the RNN model we just defined as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们按照如下方式训练刚刚定义的RNN模型：
- en: 'First, we specify the embedding dimension and the size of the LSTM hidden layer,
    and initiate the RNN model object:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们指定嵌入维度和LSTM隐藏层的大小，并初始化RNN模型对象：
- en: '[PRE37]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: A relatively high embedding dimension (like 256) allows for capturing richer
    semantic information about words. This can be beneficial for tasks like text generation.
    However, excessively high dimensions can increase computational cost and might
    lead to overfitting. 256 provides a good balance between these factors.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 相对较高的嵌入维度（如256）有助于捕捉单词的更丰富的语义信息。这对于像文本生成这样的任务是有益的。然而，过高的维度会增加计算成本，并可能导致过拟合。256维度在这些因素之间提供了一个良好的平衡。
- en: Text generation often requires the model to learn long-term dependencies between
    words in a sequence. A hidden layer size of 512 offers a good capacity to capture
    these complex relationships.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成通常要求模型学习序列中单词之间的长期依赖关系。512的隐藏层大小能够较好地捕捉这些复杂的关系。
- en: 'The next task is to define a loss function and an optimizer. In the case of
    multiclass classification, where there is a single logit output for each target
    character, we utilize `CrossEntropyLoss` as the appropriate loss function:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是定义损失函数和优化器。在多类分类的情况下，每个目标字符都有一个单独的logit输出，我们使用`CrossEntropyLoss`作为适当的损失函数：
- en: '[PRE38]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now, we train the model for 10,000 epochs. In each epoch, we train our many-to-many
    RNN on one training batch selected from the data loader, and we display the training
    loss for every 500 epochs:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们训练模型10,000个epoch。在每个epoch中，我们在从数据加载器中选取的一个训练批次上训练我们的多对多RNN，并且每500个epoch显示一次训练损失：
- en: '[PRE39]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: For each element in a given sequence, we feed the recurrent layer with the previous
    hidden state along with the current input.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定序列中的每个元素，我们将前一个隐藏状态与当前输入一起馈送到递归层。
- en: 'Model training is complete, and now it’s time to assess its performance. We
    can generate text by providing a few starting words, for instance:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练完成，现在是时候评估其性能了。我们可以通过提供几个起始单词来生成文本，例如：
- en: '[PRE40]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We generate a 500-character text beginning with our given input “the emperor.”
    Specifically, we first initialize the hidden and cell state for the RNN model.
    This is required to start generating text. Then, in the `for` loop, we iterate
    over the characters in the starting text except the last one. For each character
    in the input, we pass it through the model, updating the hidden and cell states.
    To generate the next character index, we predict the logits for all possible characters
    and sample it based on the logits utilizing a `Categorical` distribution. With
    that, we’ve successfully used a many-to-many type of RNN to generate text.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成一个500字符的文本，起始输入为“the emperor”。具体来说，我们首先初始化RNN模型的隐藏状态和细胞状态。这个步骤是生成文本所必需的。然后，在`for`循环中，我们遍历起始文本中的字符，除了最后一个字符。对于输入中的每个字符，我们将其通过模型，更新隐藏状态和细胞状态。为了生成下一个字符索引，我们预测所有可能字符的logits，并根据logits使用`Categorical`分布进行采样。这样，我们就成功地使用了多对多类型的RNN来生成文本。
- en: Feel free to tweak the model so that the RNN-based text generator can write
    a more realistic and interesting version of *War and Peace*.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 随意调整模型，使基于 RNN 的文本生成器能够写出更现实且有趣的*《战争与和平》*版本。
- en: An RNN with a many-to-many structure is a type of sequence-to-sequence (seq2seq)
    model that takes in a sequence and outputs another sequence. A typical example
    is machine translation, where a sequence of words from one language is transformed
    into a sequence in another language. The state-of-the-art seq2seq model is the
    **Transformer** model, and it was developed by Google Brain. We will discuss it
    in the next chapter.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 具有多对多结构的 RNN 是一种序列到序列（seq2seq）模型，它接收一个序列并输出另一个序列。一个典型的例子是机器翻译，其中一种语言的单词序列被转化为另一种语言的序列。最先进的
    seq2seq 模型是**Transformer**模型，它由 Google Brain 开发。我们将在下一章中讨论它。
- en: Summary
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we worked on three NLP projects: sentiment analysis, stock
    price prediction, and text generation using RNNs. We started with a detailed explanation
    of the recurrent mechanism and different RNN structures for different forms of
    input and output sequences. You also learned how LSTM improves vanilla RNNs.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们完成了三个自然语言处理项目：情感分析、股票价格预测和使用 RNN 进行文本生成。我们从对递归机制和不同 RNN 结构的详细解释开始，探讨了不同输入和输出序列的形式。你还了解了
    LSTM 如何改进传统的 RNN。
- en: In the next chapter, we will focus on the Transformer, a recent state-of-the-art
    sequential learning model, and generative models.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将重点讨论 Transformer，这一近期最先进的序列学习模型，以及生成模型。
- en: Exercises
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'Use a bi-directional recurrent layer (it is easy enough to learn about it by
    yourself) and apply it to the sentiment analysis project. Can you beat what we
    achieved? Hint: set the `bidirectional` argument to `True` in the LSTM layer.'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用双向递归层（你可以轻松地自己学习）并将其应用到情感分析项目中。你能超越我们所取得的成果吗？提示：在 LSTM 层中将 `bidirectional`
    参数设置为 `True`。
- en: Feel free to fine-tune the hyperparameters in the text generator, and see whether
    you can generate a more realistic and interesting version of *War and Peace*.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随意调整文本生成器中的超参数，看看你能否生成一个更现实且有趣的*《战争与和平》*版本。
- en: Can you train an RNN model on any of your favorite books in order to write your
    own version?
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能否在你喜欢的任何一本书上训练一个 RNN 模型，以写出你自己的版本？
- en: Join our book’s Discord space
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/yuxi](https://packt.link/yuxi)'
- en: '![](img/QR_Code187846872178698968.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code187846872178698968.png)'
