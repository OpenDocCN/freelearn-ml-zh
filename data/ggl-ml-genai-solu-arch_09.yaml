- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Feature Engineering and Dimensionality Reduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程和降维
- en: In this chapter, we will dive progressively deeper into the kinds of data processing
    steps that are common in many data science projects, and how to perform those
    steps using Vertex AI in Google Cloud. We’ll begin this chapter by taking a more
    detailed look at how features are used in machine learning workloads, and what
    kinds of challenges often arise concerning how features are used.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将逐步深入探讨许多数据科学项目中常见的数据处理步骤，以及如何使用 Google Cloud 中的 Vertex AI 来执行这些步骤。我们将从更详细地查看特征在机器学习工作负载中的使用方式以及与特征使用相关的常见挑战开始本章。
- en: We will then transition our discussion to focus on how to address those challenges,
    and how to use our machine learning features effectively in Google Cloud.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将讨论如何解决这些挑战，以及如何在 Google Cloud 中有效地使用我们的机器学习特征。
- en: 'This chapter covers the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: Fundamental concepts related to dimensions or features in machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与机器学习中的维度或特征相关的基本概念
- en: An introduction to the curse of dimensionality
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度灾难的介绍
- en: Dimensionality reduction
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维
- en: Feature engineering
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程
- en: Vertex AI Feature Store
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vertex AI 特征存储
- en: Fundamental concepts in this chapter
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本章的基本概念
- en: In this section, we’ll briefly cover concepts that provide additional context
    for this chapter’s learning activities.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要介绍为本章的学习活动提供额外背景的概念。
- en: Dimensions and features
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维度和特征
- en: We introduced the concept of features in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015)
    and described examples of features using the King County housing sales dataset
    for illustration. To briefly recap, features are individual, measurable properties
    or characteristics of the observations in our dataset. They are the aspects of
    our dataset from which a machine learning algorithm learns to create a model.
    In other words, a model can be seen as a representation of patterns learned by
    the algorithm from the features in our dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第一章*](B18143_01.xhtml#_idTextAnchor015)中介绍了特征的概念，并使用金县房屋销售数据集作为示例描述了特征。为了简要回顾，特征是我们数据集中观察到的单个、可度量的属性或特征。它们是我们数据集的方面，机器学习算法从中学习以创建模型。换句话说，模型可以被视为算法从我们数据集中的特征中学习到的模式的表示。
- en: 'The features of a house, for example, include information such as how many
    rooms it contains, the year it was constructed, where it is located, and other
    factors that describe the house, as depicted in *Table 7.1*:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，房屋的特征包括诸如房间数量、建造年份、位置以及其他描述房屋的因素等信息，如*表 7.1*所示：
- en: '![Table 7.1: King County house sales features ](img/B18143_01_9.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![表 7.1：金县房屋销售特征](img/B18143_01_9.jpg)'
- en: 'Table 7.1: King County house sales features'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.1：金县房屋销售特征
- en: When we’re dealing with tabular data, features are generally represented as
    columns in our dataset, and each row represents an individual data point or observation,
    sometimes referred to as an **instance**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理表格数据时，特征通常以数据集中的列的形式表示，每一行代表一个单独的数据点或观察结果，有时也被称为**实例**。
- en: Features are also referred to as variables, attributes, or dimensions. So, when
    we talk about the dimensionality of a dataset, it relates to how many features
    or dimensions our dataset has, and how that affects our machine learning workloads.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 特征也被称为变量、属性或维度。因此，当我们谈论数据集的维度时，它与我们数据集中有多少个特征或维度有关，以及这如何影响我们的机器学习工作负载有关。
- en: Overfitting, underfitting, and regularization
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合、欠拟合和正则化
- en: We briefly discussed the concepts of overfitting and underfitting in [*Chapter
    2*](B18143_02.xhtml#_idTextAnchor035), and we will continue to revisit these topics
    in more detail throughout this book since they are so fundamentally important
    to the process of machine learning. In this section, we’ll discuss how the number
    of features in our dataset can affect how our algorithms learn from our data.
    A key concept to keep in mind is that overfitting and underfitting can be strongly
    influenced by how many observations we have in our dataset, and how many features
    we have for each observation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第二章*](B18143_02.xhtml#_idTextAnchor035)中简要讨论了过拟合和欠拟合的概念，由于这些概念对于机器学习过程至关重要，因此在本书中我们将继续更详细地回顾这些主题。在本节中，我们将讨论数据集中特征的数量如何影响我们的算法从数据中学习的方式。一个需要记住的关键概念是，过拟合和欠拟合可以受到我们数据集中观察到的数量以及每个观察到的特征数量的强烈影响。
- en: We usually need to find the right balance between these two aspects of our dataset.
    For example, if we have very few observations and a lot of features for each observation,
    our model is likely to overfit the dataset because it learns very specific patterns
    for those observations and their features, but it cannot generalize well to new
    observations. Conversely, if we have many observations, but very few pieces of
    information (that is, features) for each observation, then our model may not be
    able to learn any valuable patterns, meaning it will underfit our dataset. Because
    of this, reducing the number of features can help reduce overfitting, but only
    to an extent – removing too many features may result in underfitting. Also, we
    don’t want to remove features that contain useful information for our models to
    learn, so another way that we can address overfitting while keeping many of our
    features is to use a mechanism called regularization. We also briefly mentioned
    this in [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035), and it’s something we
    will discuss in more detail here.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常需要在数据集的这两个方面之间找到合适的平衡。例如，如果我们对每个观察到的数据点有非常少的观测值和很多特征，那么我们的模型很可能会过度拟合数据集，因为它为这些观测值及其特征学习到了非常具体的模式，但它无法很好地泛化到新的观测值。相反，如果我们有大量的观测值，但每个观测值只有很少的信息（即特征），那么我们的模型可能无法学习到任何有价值的模式，这意味着它将欠拟合我们的数据集。正因为如此，减少特征的数量可以帮助减少过度拟合，但只能到一定程度——去除过多的特征可能会导致欠拟合。此外，我们不想去除那些对模型学习有用的信息，因此，我们可以在保持许多特征的同时，通过使用一种称为正则化的机制来解决过度拟合问题。我们也在[*第二章*](B18143_02.xhtml#_idTextAnchor035)中简要提到了这一点，我们将在这里更详细地讨论它。
- en: Regularization
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化
- en: To begin our discussion on regularization, we need to bring up the concept of
    the loss function in machine learning once more, something we introduced in [*Chapter
    1*](B18143_01.xhtml#_idTextAnchor015). Remember that many machine learning algorithms
    work by trying to find the best coefficients (or weights) for each feature that
    result in the closest approximation of the target feature. So, overfitting is
    influenced by the mathematical relationship between the features and their coefficients.
    If we find that a model is overfitting to specific features, we can use regularization
    to reduce the influence of those features and their coefficients on the model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始我们关于正则化的讨论，我们需要再次提及机器学习中的损失函数的概念，这是我们已经在[*第一章*](B18143_01.xhtml#_idTextAnchor015)中介绍过的。记住，许多机器学习算法通过尝试找到每个特征的最好系数（或权重），以实现对目标特征的最近似来工作。因此，过度拟合受到特征与其系数之间的数学关系的影响。如果我们发现模型对特定特征过度拟合，我们可以使用正则化来减少这些特征及其系数对模型的影响。
- en: Because overfitting usually happens when a model is too complex, such as having
    too many features relative to the number of observations, regularization addresses
    this issue by adding a penalty to the loss function, which discourages the model
    from assigning too much importance to any feature. This helps improve the generalizability
    of the model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于过度拟合通常发生在模型过于复杂的情况下，例如相对于观测值的数量有太多的特征，正则化通过向损失函数中添加惩罚来解决这一问题，这会阻止模型对任何特征赋予过多的重视。这有助于提高模型的泛化能力。
- en: There are several ways to implement regularization in machine learning, but
    I’ll explain two of the most common types here – that is, **L1** and **L2** regularization
    – as well as a combination of both approaches, referred to as **elastic net**.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中实现正则化的方法有很多，但我会解释两种最常见类型——即**L1**和**L2**正则化——以及这两种方法的组合，称为**弹性网络**。
- en: L1 regularization
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: L1正则化
- en: 'This type of regularization is also known as **Lasso** regularization, and
    it works by adding a penalty equivalent to the L1 norm (or the absolute value)
    of the coefficients or weights in the cost function by using the following formula:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的正则化也称为**Lasso**正则化，它通过以下公式通过添加与成本函数中系数或权重的L1范数（或绝对值）相等的惩罚来实现：
- en: Cost Function + λ * |weights|
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数 + λ * |权重|
- en: Here, λ is the regularization parameter, which controls the strength of the
    penalty and can be considered a hyperparameter whose optimal value can vary depending
    on the problem. Note that if the penalty is too strong, it can result in underfitting,
    so it’s important to find the right balance in this regard.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，λ是正则化参数，它控制惩罚的强度，可以被认为是一个超参数，其最佳值可能因问题而异。请注意，如果惩罚太强，可能会导致欠拟合，因此在这方面找到合适的平衡很重要。
- en: The effect of L1 regularization is to shrink some of the model’s coefficients
    to exactly zero, effectively excluding the corresponding feature from the model,
    which makes L1 regularization useful for feature selection (something we’ll cover
    in more detail shortly) when dealing with high-dimensional data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: L1 正则化的作用是将模型的一些系数缩小到正好为零，从而有效地排除相应的特征，这使得 L1 正则化在处理高维数据时对于特征选择（我们将在稍后更详细地介绍）非常有用。
- en: L2 regularization
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: L2 正则化
- en: 'This type of regularization is also known as **Ridge** regularization. This
    method adds a penalty equivalent to the L2 norm (or the square) of the coefficients
    in the cost function by using the following formula:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种正则化也称为 **岭** 正则化。这种方法通过以下公式在成本函数中添加一个相当于 L2 范数（或平方）的惩罚：
- en: Cost Function + λ * (weights^2)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数 + λ * (权重^2)
- en: Unlike L1 regularization, L2 regularization doesn’t result in the exclusion
    of features but rather pushes the coefficients close to zero, distributing the
    weights evenly among the features. This can be beneficial when we’re dealing with
    correlated features as it allows the model to keep all of them under consideration.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与 L1 正则化不同，L2 正则化不会导致特征的排除，而是将系数推向零附近，在特征之间均匀分配权重。当我们处理相关特征时，这可能是有益的，因为它允许模型考虑所有这些特征。
- en: Elastic net
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 弹性网络
- en: 'Elastic net, as a combination of L1 and L2 regularization, was designed to
    provide a compromise between these two methods, incorporating the strengths of
    both. Like L1 and L2 regularization, elastic net adds a penalty to the loss function,
    but instead of adding an L1 penalty or an L2 penalty, it adds a weighted sum of
    both by using the following formula:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性网络作为 L1 和 L2 正则化的组合，旨在在这两种方法之间提供折衷方案，结合两者的优点。与 L1 和 L2 正则化一样，弹性网络向损失函数添加惩罚，但它不是添加
    L1 惩罚或 L2 惩罚，而是通过以下公式添加两者的加权总和：
- en: Cost Function + λ1 * |weights| + λ2 * (weights^2)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数 + λ1 * |权重| + λ2 * (权重^2)
- en: Here, λ1 and λ2 are hyperparameters that control the strength of the L1 and
    L2 penalties, respectively. If λ1 is zero, elastic net reduces to Ridge regression,
    and if λ2 is zero, it reduces to Lasso regression.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，λ1 和 λ2 是控制 L1 和 L2 惩罚强度的超参数。如果 λ1 为零，弹性网络回归将退化为岭回归，而如果 λ2 为零，则退化为 Lasso
    回归。
- en: Elastic net has the feature selection capability of L1 regularization (since
    it can shrink coefficients to zero) and the regularization strength of L2 regularization
    (since it can distribute weights evenly among correlated features). The trade-off
    with elastic net is that it has two hyperparameters to tune, rather than just
    one in the case of Lasso or Ridge, which can make the model more complex and computationally
    intensive to train.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性网络具有 L1 正则化的特征选择能力（因为它可以将系数缩小到零），以及 L2 正则化的正则化强度（因为它可以在相关特征之间均匀分配权重）。与弹性网络相关的权衡是，它有两个超参数需要调整，而不是
    Lasso 或岭回归中只有一个，这可能会使模型更复杂，训练起来计算量更大。
- en: Now that we’ve covered the important topic of regularization in more detail,
    let’s dive into feature selection and feature engineering.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经更详细地介绍了正则化这个重要话题，让我们深入探讨特征选择和特征工程。
- en: Feature selection and feature engineering
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征选择和特征工程
- en: We briefly discussed feature engineering in previous chapters of this book,
    but we will explore these concepts in more detail here. In [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035),
    we used the example of creating a new feature named `price-per-square-foot` in
    our housing data by dividing the total cost of each house by the total area of
    that house in square feet. We will explore many additional examples of feature
    engineering in this chapter.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前几章中，我们简要地讨论了特征工程，但在这里我们将更详细地探讨这些概念。在[*第二章*](B18143_02.xhtml#_idTextAnchor035)中，我们通过将每栋房子的总成本除以该房子的总面积（平方英尺）来创建一个新的特征
    `price-per-square-foot`，作为我们的住房数据示例。在本章中，我们将探讨许多额外的特征工程示例。
- en: However, creating new features from existing ones is not the only type of activity
    we need to perform on our features when preparing our dataset for training a machine
    learning model. We also need to select which features we think could be most important
    for achieving the task that we want our model to achieve, such as predicting the
    price of a house. As we saw in [*Chapter 6*](B18143_06.xhtml#_idTextAnchor187),
    we may also need to perform transformations on our features, such as ensuring
    that they are all represented on a common, standardized scale.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从现有特征中创建新特征并不是我们在为训练机器学习模型准备数据集时需要在特征上进行的唯一活动。我们还需要选择我们认为对实现我们希望模型完成的任务（例如预测房价）最重要的特征。正如我们在[*第6章*](B18143_06.xhtml#_idTextAnchor187)中看到的，我们可能还需要对特征进行转换，例如确保它们都表示在共同的、标准化的尺度上。
- en: The goal in selecting and engineering features is to provide our model with
    the most relevant information, in the most digestible format, so that it can most
    effectively learn from the data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择和构建特征时的目标是，以最易于消化的格式为我们模型提供最相关的信息，以便它可以从数据中有效地学习。
- en: The curse of dimensionality
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维度诅咒
- en: High-dimensional datasets contain many dimensions or features for each observation
    in the dataset. It would be possible to presume that, the more features we include
    for each data point, the more information our model will learn, and therefore,
    the more accurate our model will be. However, note that not all features are equally
    useful. Some may contain little to no useful information for our model, others
    may contain redundant information, and some may even be harmful to the model’s
    ability to learn. Part of the art and science of machine learning is figuring
    out which features to use and how to prepare them in a way that allows the model
    to perform its best.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 高维数据集包含许多维度或特征，每个数据点都有很多。我们可能会假设，我们为每个数据点包含的特征越多，我们的模型学到的信息就越多，因此，我们的模型就越准确。然而，请注意，并非所有特征都同等有用。一些可能对我们的模型几乎没有或没有有用的信息，其他可能包含冗余信息，甚至可能对模型学习的能力有害。机器学习艺术和科学的一部分就是确定使用哪些特征以及如何准备它们，以便模型能够发挥最佳性能。
- en: Also, bear in mind that, the more information we have in our dataset, the more
    information our model has to process. This directly translates to additional computing
    resources being required for our machine learning algorithm to process our dataset,
    which, in turn, directly translates to longer model training times, and increased
    monetary cost. Too much irrelevant data or **noise** in the dataset can also make
    it harder for the algorithm to identify (that is, to learn) patterns in the data.
    The ideal scenario, then, is to find the minimum number of features that provide
    the maximum amount of useful information to our model. If we can achieve the same
    results with three features or ten features, for example, it’s generally better
    to go with the option of using three features. The “maximum amount of useful information”
    can be measured in terms of **variance**, whereby features with high relative
    variance are those that influence our model’s outcomes most prominently, and features
    with little relative variance are often not as useful in training our model to
    identify patterns.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请记住，我们数据集中包含的信息越多，我们的模型需要处理的信息就越多。这直接导致我们的机器学习算法处理数据集时需要更多的计算资源，这反过来又直接导致模型训练时间更长，成本增加。数据集中过多的无关数据或**噪声**也可能使算法更难识别（即学习）数据中的模式。因此，理想的情况是找到提供最大有用信息的最小特征数量。例如，如果我们可以用三个特征或十个特征达到相同的结果，那么通常选择使用三个特征会更好。“最大有用信息”可以通过**方差**来衡量，其中具有高相对方差的特征对我们的模型结果影响最为显著，而相对方差小的特征在训练模型识别模式时通常不太有用。
- en: The “curse of dimensionality” is a term that’s used in the data science industry
    to describe the challenges that arise when dealing with datasets that contain
    higher numbers of dimensions. Let’s take a look at what some of these challenges
    are. In subsequent sections, we will discuss mechanisms to address these challenges.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: “维度诅咒”是数据科学行业用来描述处理包含更多维度的数据集时出现的挑战的术语。让我们看看这些挑战中的一些。在随后的章节中，我们将讨论解决这些挑战的机制。
- en: Data exploration challenges
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据探索挑战
- en: This is an important point at which we can introduce the link between “dimensions”
    in our dataset, and the dimensions of physical space. As we all know, and as mentioned
    earlier in this book, humans can only perceive our physical world in up to a maximum
    of three dimensions (width, height, and depth), with “time” considered as the
    fourth dimension of our physical reality. For datasets that have two or three
    dimensions, we can easily create visualizations representing various aspects of
    those datasets, but we can’t create graphs or other visual representations of
    higher-dimensional datasets. In such cases, it helps if we can try to find other
    ways of visually interpreting those datasets, such as by projecting them down
    into lower-dimensional representations, something we will explore in more detail
    shortly.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们可以引入数据集中的“维度”与物理空间维度之间联系的重要点。众所周知，正如本书前面提到的，人类只能感知到最多三个维度的物理世界（宽度、高度和深度），而“时间”被视为我们物理现实的第四维度。对于具有两个或三个维度的数据集，我们可以轻松创建表示那些数据集各个方面的可视化，但我们无法创建更高维数据集的图表或其他视觉表示。在这种情况下，如果我们能尝试找到其他方法来视觉解释这些数据集，比如将它们投影到低维表示中，这会很有帮助，我们将在稍后更详细地探讨这一点。
- en: Feature sparsity
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征稀疏性
- en: In high-dimensional space, points (that is, instances or samples) in our dataset
    tend to be far away from each other, leading to sparsity. Generally, as the number
    of features increases relative to the number of observations in our dataset, the
    feature space becomes increasingly sparse, and this sparsity makes it more difficult
    for algorithms to learn from the data since they have fewer examples to learn
    from in the vicinity of any given point.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在高维空间中，数据集中的点（即实例或样本）往往彼此相距甚远，导致稀疏性。一般来说，随着特征数量相对于数据集中观察值的数量的增加，特征空间变得越来越稀疏，这种稀疏性使得算法从数据中学习变得更加困难，因为它们在任意给定点的附近有更少的例子可以学习。
- en: As an example, let’s imagine that our dataset consists of information regarding
    a company’s customers, in which case each instance in the dataset represents a
    person, and each feature represents some characteristic of a person. If our dataset
    stores hundreds or even thousands of characteristics, then it’s unlikely that
    all characteristics for every person will be populated. As such, the overall feature
    space for our dataset will be sparsely populated. On the other hand, if we have
    fewer features, this is less likely to occur.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们假设我们的数据集包含有关公司客户的信息，在这种情况下，数据集中的每个实例代表一个人，每个特征代表一个人的某种特征。如果我们的数据集存储了数百甚至数千个特征，那么不太可能每个特征都会被填充。因此，我们数据集的整体特征空间将是稀疏的。另一方面，如果我们有较少的特征，这种情况发生的可能性较小。
- en: Distance measurements
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 距离测量
- en: In previous chapters, we talked about how Euclidean distance is used in many
    machine learning algorithms to find potential relationships or differences between
    data points in our dataset. One of the most direct examples of this concept that
    we’ve explored already is the K-means clustering algorithm. In high-dimensional
    spaces, distances can sometimes become less meaningful, because the difference
    between the maximum and minimum possible distances becomes increasingly smaller.
    This means that traditional distance metrics, such as Euclidean distance, also
    become less meaningful, which can particularly affect algorithms that rely on
    distance, such as **k-nearest neighbors** (**kNN**) or clustering algorithms.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了欧几里得距离在许多机器学习算法中的应用，用于寻找数据集中数据点之间潜在的关联或差异。我们已经探讨的这种概念的最直接例子之一是K-means聚类算法。在高维空间中，距离有时可能变得不那么有意义，因为最大可能距离和最小可能距离之间的差异变得越来越小。这意味着传统的距离度量，如欧几里得距离，也变得不那么有意义，这可能会特别影响依赖于距离的算法，例如**k近邻**（**kNN**）或聚类算法。
- en: Overfitting and increased data requirements
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过度拟合和增加的数据需求
- en: Referring back to our discussion of how overfitting or underfitting are influenced
    by the ratio of observations to features in our dataset, high-dimensional datasets
    are more prone to overfitting unless we have enormous amounts of observations
    to help our model generalize. Bear in mind what we said earlier in this chapter,
    regarding the relationship between the amount of data our algorithms need to process
    and the cost of training our models. Datasets with lots of observations and lots
    of features will be more expensive to train and manage.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们之前关于过拟合或欠拟合如何受数据集中观察值与特征比例影响的讨论，高维数据集更容易过拟合，除非我们有大量的观察值来帮助我们的模型泛化。记住我们在本章前面提到的关于算法处理数据量与训练模型成本之间关系的内容。具有大量观察值和大量特征的数据集将更昂贵且更难训练和管理。
- en: Interpretability and explainability
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可解释性和可解释性
- en: Interpretability and explainability refer to our ability to understand and explain
    how our machine learning models work. This is important for numerous reasons,
    all of which we will discuss briefly here. Firstly, a lack of understanding of
    how our models work inhibits our ability to improve those models. However, more
    importantly, we need to ensure that our models are as fair and unbiased as possible,
    and this is where explainability plays a crucial role. If we cannot explain why
    our models are producing specific results, then we cannot adequately assess their
    fairness. Generally, the higher the dimensionality of our dataset, the more complex
    our models tend to be, which can directly affect (that is, reduce) interpretability
    and explainability.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性和可解释性指的是我们理解和解释我们的机器学习模型如何工作的能力。这有多个原因，我们将在下面简要讨论。首先，对我们模型工作原理的不理解阻碍了我们改进这些模型的能力。然而，更重要的是，我们需要确保我们的模型尽可能公平和无偏见，这就是可解释性发挥关键作用的地方。如果我们不能解释我们的模型为何产生特定的结果，那么我们就无法充分评估其公平性。一般来说，我们的数据集维度越高，我们的模型往往越复杂，这会直接影响（即减少）可解释性和可解释性。
- en: Now that we’ve reviewed some of the common challenges associated with high-dimensional
    datasets, let’s take a look at some mechanisms to address those challenges.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了一些与高维数据集相关联的常见挑战，让我们来看看一些解决这些挑战的机制。
- en: Dimensionality reduction
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维度降低
- en: 'As you might imagine, one of the first ways to address the challenge of having
    too many dimensions is to reduce the number of dimensions, and there are two main
    types of techniques we can use for this purpose: feature selection and feature
    projection.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想，解决维度过多挑战的第一种方法就是减少维度数量，为此我们可以使用两种主要的技术：特征选择和特征投影。
- en: Feature selection
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征选择
- en: 'This involves selecting a subset of the original features. There are several
    strategies for feature selection, including the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及到从原始特征中选择一个子集。特征选择有几种策略，包括以下几种：
- en: Filter methods, which rank features based on statistical measures and select
    a subset of features with the highest ranking
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤方法，这些方法根据统计指标对特征进行排序，并选择排名最高的特征子集
- en: Wrapper methods, which evaluate multiple models using different subsets of input
    features and select the subset that results in the highest model performance
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包装方法，这些方法使用不同的输入特征子集评估多个模型，并选择导致最高模型性能的子集
- en: Embedded methods, which use machine learning algorithms that have built-in feature
    selection methods (such as Lasso regularization)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入方法，这些方法使用具有内置特征选择方法的机器学习算法（例如Lasso正则化）
- en: It’s also important to understand that the feature projection methods we will
    discuss can be used to help select the most important subset of features from
    our dataset, so there is some overlap between these concepts.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是要理解，我们将讨论的特征投影方法可以用来帮助我们选择数据集中最重要的特征子集，因此这些概念之间有一些重叠。
- en: Feature projection
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征投影
- en: 'With feature projection, we use mathematical transformations to project our
    features down into a lower-dimensional space. In this section, we’ll introduce
    three popular feature projection techniques: **Principal Component Analysis**
    (**PCA**), **Linear Discriminant Analysis** (**LDA**), and **t-distributed Stochastic
    Neighbor** **Embedding** (**t-SNE**).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征投影中，我们使用数学变换将特征投影到低维空间。在本节中，我们将介绍三种流行的特征投影技术：**主成分分析**（**PCA**）、**线性判别分析**（**LDA**）和**t-分布随机邻域嵌入**（**t-SNE**）。
- en: PCA
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PCA
- en: 'PCA is an **unsupervised** algorithm that aims to reduce the dimensionality
    of our feature space while maintaining as much of the original data’s variance
    as possible. In the high-dimensional data space, PCA identifies the axes (principal
    components) along which the variation in the data is maximized. These principal
    components are orthogonal, meaning they’re at right angles to each other in this
    multi-dimensional space. The **first principal component** (**PC1**) captures
    the direction of the greatest variance in the data. The **second principal component**
    (**PC2**) captures the maximum amount of remaining variance while being orthogonal
    to PC1, and so on. The process of PCA generally involves the following steps:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）是一种**无监督**算法，旨在在尽可能保持原始数据方差的同时，降低我们的特征空间的维度。在高维数据空间中，PCA识别出数据变化最大的轴（主成分）。这些主成分是正交的，意味着在这个多维空间中它们彼此之间成直角。**第一个主成分**（**PC1**）捕捉数据中最大方差的方向。**第二个主成分**（**PC2**）在正交于PC1的同时，捕捉剩余的最大方差，依此类推。PCA的过程通常包括以下步骤：
- en: Standardize the data if the features have different scales. This is important
    because PCA is sensitive to feature scales, whereby features with larger scales
    could mistakenly be perceived as more dominant.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果特征具有不同的尺度，则对数据进行标准化。这是很重要的，因为PCA对特征尺度很敏感，尺度较大的特征可能会被错误地认为是更占主导地位的。
- en: 'Calculate a covariance matrix to understand how different features vary together.
    The covariance matrix is a square matrix that contains the covariances between
    each pair of features. The covariance between two features measures how those
    features vary together: a positive covariance indicates that the features increase
    or decrease together, while a negative covariance indicates that one feature increases
    while the other decreases.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算协方差矩阵以了解不同特征是如何一起变化的。协方差矩阵是一个方阵，包含了每对特征之间的协方差。两个特征之间的协方差衡量了这些特征是如何一起变化的：正协方差表示特征一起增加或减少，而负协方差表示一个特征增加而另一个减少。
- en: Calculate the **eigenvalues** and **eigenvectors** of the covariance matrix.
    The eigenvectors represent the directions or components of the new space, and
    the eigenvalues represent the magnitude or explained variance for each component.
    The eigenvectors are often called the principal components of the data, and they
    form a basis for the new feature space.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算协方差矩阵的**特征值**和**特征向量**。特征向量代表新空间的方向或成分，特征值代表每个成分的大小或解释方差。特征向量通常被称为数据的**主成分**，它们构成了新特征空间的一个基。
- en: Sort the eigenvalues and their corresponding eigenvectors. After computing the
    eigenvalues and their associated eigenvectors, the next step is to sort the eigenvalues
    in descending order. The eigenvector with the highest corresponding eigenvalue
    is PC1\. The eigenvector with the second highest corresponding eigenvalue is PC2,
    and so on. The reason for this ordering is that the significance of each eigenvector
    is given by the magnitude of its eigenvalue.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对特征值及其对应的特征向量进行排序。在计算了特征值及其相关的特征向量之后，下一步是对特征值按降序排序。具有最高对应特征值的特征向量是PC1。具有第二高对应特征值的特征向量是PC2，依此类推。这种排序的原因是每个特征向量的重要性由其特征值的大小给出。
- en: Select a subset of the principal components. PCA creates as many principal components
    as there are variables in the original dataset. However, since the goal of PCA
    is dimensionality reduction, we usually select a subset of the principal components,
    referred to as the **top k** principal components, which capture the most variance
    in the data. This step is what reduces dimensionality because the smaller eigenvalues
    and their vectors are dropped.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择主成分的子集。PCA创建与原始数据集中变量数量一样多的主成分。然而，由于PCA的目标是降维，我们通常选择主成分的子集，称为**前k个**主成分，这些主成分捕捉了数据中的最大方差。这一步是降低维度的原因，因为较小的特征值及其向量被丢弃。
- en: Transform the original data. The final step in PCA is to transform the original
    data into the reduced subspace defined by the selected principal components, which
    is done by multiplying the original data matrix by the matrix of the top *k* eigenvectors.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换原始数据。PCA的最后一步是将原始数据转换成由所选主成分定义的降维子空间，这是通过将原始数据矩阵乘以前*k*个特征向量的矩阵来实现的。
- en: 'The transformed data is now ready to be used for further analysis and visualization
    (as depicted in *Figure 7**.1*), or used as input to a machine learning algorithm.
    Importantly, the reduced dataset retains as much of the variance in the original
    data as possible (given the reduced number of dimensions):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的数据现在可以用于进一步的分析和可视化（如图*图7.1*所示），或者作为机器学习算法的输入。重要的是，这个减少的数据集尽可能地保留了原始数据中的方差（考虑到减少的维度数）：
- en: '![Figure 7.1: PCA visualization of European genetic structure (source: https://commons.wikimedia.org/wiki/File:PCA_plot_of_European_individuals.png)](img/B18143_07_1.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1：欧洲遗传结构的PCA可视化（来源：https://commons.wikimedia.org/wiki/File:PCA_plot_of_European_individuals.png）](img/B18143_07_1.jpg)'
- en: 'Figure 7.1: PCA visualization of European genetic structure (source: https://commons.wikimedia.org/wiki/File:PCA_plot_of_European_individuals.png)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：欧洲遗传结构的PCA可视化（来源：https://commons.wikimedia.org/wiki/File:PCA_plot_of_European_individuals.png）
- en: PCA is a powerful technique with many uses, but it also has limitations. For
    example, it assumes that the principal components are a linear combination of
    the original features. If this is not the case (that is, if the underlying structure
    in the data is non-linear), then PCA may not be the best dimensionality reduction
    technique to use. It’s also worth noting that the principal components are less
    interpretable than the original features – they don’t have an intuitive meaning
    in terms of the original features.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种强大的技术，用途广泛，但它也有局限性。例如，它假设主成分是原始特征的线性组合。如果这不是这种情况（即，如果数据中的潜在结构是非线性的），那么PCA可能不是最佳降维技术。还值得注意的是，主成分比原始特征更难以解释——它们在原始特征方面没有直观的意义。
- en: LDA
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LDA
- en: 'LDA is a **supervised** algorithm that aims to find a linear combination of
    features that best separates classes of objects. The resulting combination can
    then be used for dimensionality reduction. It involves the following steps:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: LDA是一种**监督**算法，旨在找到最佳分离对象类别的特征线性组合。然后可以使用这个组合进行降维。它涉及以下步骤：
- en: Compute the class means. For each class in the dataset, calculate the mean vector,
    which is simply the average of all vectors in that class.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算类均值。对于数据集中的每个类别，计算均值向量，这仅仅是该类别中所有向量的平均值。
- en: Compute the within-class covariance matrix, which measures how the individual
    classes are dispersed around their respective means.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算类内协方差矩阵，它衡量各个类别如何围绕各自的均值分散。
- en: Compute the between-class covariance matrix, which measures how the class means
    are dispersed around the overall mean of the data.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算类间协方差矩阵，它衡量类别均值在数据总体均值周围如何分散。
- en: Compute the linear discriminants, which are the directions in the feature space
    along which the classes are best separated.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算线性判别方向，这些方向是特征空间中类别最佳分离的方向。
- en: Sort the linear discriminants. Just like in PCA, the eigenvectors are sorted
    by their corresponding eigenvalues in descending order. The eigenvalues represent
    the amount of the data’s variance that is accounted for by each discriminant.
    The first few linear discriminants, corresponding to the largest eigenvalues,
    are the ones that account for the most variance.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对线性判别方向进行排序。就像在PCA中一样，特征向量按照它们对应的特征值降序排列。特征值表示每个判别所解释的数据方差量。前几个线性判别方向，对应于最大的特征值，是解释最大方差的方向。
- en: Finally, the data is projected onto the space spanned by the first few linear
    discriminants.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，数据被投影到由前几个线性判别方向张成的空间中。
- en: 'This results in a lower-dimensional representation of the data where the classes
    are maximally separated. We can visualize this as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致数据在低维表示中类别最大化分离。我们可以如下可视化：
- en: '![Figure 7.2: LDA plots of wine varieties](img/B18143_07_02.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2：葡萄酒品种的LDA图](img/B18143_07_02.jpg)'
- en: 'Figure 7.2: LDA plots of wine varieties'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：葡萄酒品种的LDA图
- en: It’s important to note that the main assumption of LDA is that the classes have
    identical covariance matrices. If this assumption is not met, LDA might not perform
    well.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，LDA的主要假设是类别具有相同的协方差矩阵。如果这个假设不成立，LDA可能表现不佳。
- en: t-SNE
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: t-SNE
- en: 'This method has perhaps the coolest name of them all. It’s an **unsupervised**,
    non-linear dimensionality reduction algorithm that is particularly well-suited
    for embedding high-dimensional data into a space of two or three dimensions while
    aiming to keep similar instances close and dissimilar instances apart. It does
    this by mapping the high-dimensional data to a lower-dimensional space in a way
    that retains much of the relative distances between points. It involves the following
    steps:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可能拥有所有方法中最酷的名字。它是一个**无监督**的非线性降维算法，特别适合将高维数据嵌入到二维或三维空间中，同时旨在使相似实例靠近，不同实例分离。它是通过将高维数据映射到低维空间，以保留点之间的大部分相对距离来实现这一点的。它包括以下步骤：
- en: '**Compute similarities in the high-dimensional space**: t-SNE begins by calculating
    the probability that pairs of data points in the high-dimensional space are similar.
    Points that are close to each other have a higher probability of being picked,
    while points that are far away have a lower probability.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在高维空间中计算相似性**：t-SNE首先计算高维空间中数据点对相似的概率。彼此靠近的点有更高的被选中概率，而彼此远离的点有更低的被选中概率。'
- en: '**Computing similarities in low-dimensional space**: t-SNE then calculates
    the probabilities of similarity for pairs of points in the low-dimensional representation.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在低维空间中计算相似性**：t-SNE随后计算低维表示中点对相似性的概率。'
- en: '**Optimization**: Finally, t-SNE uses gradient descent to minimize the difference
    between the probabilities in the high-dimensional and low-dimensional spaces.
    The goal is to have similar objects modeled by nearby points and dissimilar objects
    modeled by distant points in the low-dimensional space.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**优化**：最后，t-SNE使用梯度下降来最小化高维和低维空间中概率的差异。目标是使相似对象由低维空间中的邻近点表示，不同对象由低维空间中的远点表示。'
- en: The result of t-SNE is a map that reveals the structure of the high-dimensional
    data in a way that it’s easier for humans to comprehend.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE的结果是一张地图，以人类更容易理解的方式揭示了高维数据的结构。
- en: 'It should be noted that while t-SNE is excellent for visualization and can
    reveal clusters and structure in your data (as depicted in *Figure 7**.3*), it
    doesn’t provide explicit information about the importance or meaning of the features
    in your data like PCA does. It’s more of an exploratory tool that can help in
    dimensionality reduction, rather than a formal dimensionality reduction technique:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，虽然t-SNE在可视化方面非常出色，可以揭示数据中的簇和结构（如图*图7.3*所示），但它并不像PCA那样提供关于数据中特征重要性或含义的明确信息。它更多的是一种探索性工具，可以帮助进行降维，而不是一种正式的降维技术：
- en: '![Figure 7.3: t-SNE visualization of digits dataset](img/B18143_07_03.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3：数字数据集的t-SNE可视化](img/B18143_07_03.jpg)'
- en: 'Figure 7.3: t-SNE visualization of digits dataset'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：数字数据集的t-SNE可视化
- en: Now that we’ve covered some of the most popular feature projection techniques,
    let’s discuss one more set of important concepts regarding how the features in
    our datasets influence model training.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了一些最受欢迎的特征投影技术，让我们再讨论一组关于我们数据集中的特征如何影响模型训练的重要概念。
- en: Using PCA and LDA for dimensionality reduction
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PCA和LDA进行降维
- en: We’ll start our hands-on activities in this chapter with dimensionality reduction
    using PCA and LDA. We can use the wine dataset within scikit-learn as an example.
    I always wish I could impress my friends by being a wine expert, but I can barely
    tell a $10 bottle from a $500 bottle, so instead, I’ll use data science to develop
    impressive knowledge.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中通过使用PCA和LDA进行降维来开始我们的动手活动。我们可以使用scikit-learn中的葡萄酒数据集作为例子。我总是希望我能通过成为一个葡萄酒专家来给我的朋友们留下深刻印象，但我几乎无法区分10美元一瓶的酒和500美元一瓶的酒，所以，我将使用数据科学来发展令人印象深刻的知识。
- en: The wine dataset is an example of a multivariate dataset that contains the results
    of a chemical analysis of wines grown in the same region in Italy but derived
    from three different types of grapes (referred to as `cultivars`). The analysis
    focused on quantifying 13 constituents found in each of the three types of wines.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 葡萄酒数据集是一个多变量数据集的例子，它包含了在意大利同一地区种植但来自三种不同葡萄品种（称为`栽培品种`）的葡萄酒的化学分析结果。分析的重点是量化三种葡萄酒类型中发现的13种成分。
- en: Using PCA on this dataset will help us to understand the important features.
    By looking at the weights of the original features in the principal components,
    we can see which features contribute most to the variability in the wine dataset.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PCA处理这个数据集将帮助我们理解重要的特征。通过观察原始特征在主成分中的权重，我们可以看到哪些特征对葡萄酒数据集中的变异性贡献最大。
- en: 'Again, we can use the same Vertex AI Workbench Notebook Instance that we created
    in [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168) for this purpose. Please open
    JupyterLab on that notebook instance and perform the following steps:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以使用在[*第5章*](B18143_05.xhtml#_idTextAnchor168)中创建的相同的Vertex AI Workbench笔记本实例来完成此目的。请在笔记本实例上打开JupyterLab并执行以下步骤：
- en: In the directory explorer on the left-hand side of the screen, navigate to the
    `Chapter-07` directory and open the `dimensionality-reduction.ipynb` notebook.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕左侧的目录浏览器中，导航到`Chapter-07`目录并打开`dimensionality-reduction.ipynb`笔记本。
- en: Choose **Python (Local**) as the kernel.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**Python (Local**)作为内核。
- en: Run each cell in the notebook by selecting the cell and pressing *Shift* + *Enter*
    on your keyboard.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过选择单元格并按键盘上的*Shift* + *Enter*来运行笔记本中的每个单元格。
- en: 'The code in the notebook performs the following activities:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本中的代码执行以下活动：
- en: First, it imports the necessary libraries.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，它导入必要的库。
- en: Then, it loads the dataset.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它加载数据集。
- en: Next, it standardizes the features of the wine dataset. It applies PCA to reduce
    the dimensionality to two dimensions (that is, the first two principal components).
    This is done using the `fit_transform()` method, which fits the PCA model to the
    data and then transforms the data.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，它标准化了葡萄酒数据集的特征。它应用主成分分析（PCA）将维度降低到二维（即前两个主成分）。这是通过使用`fit_transform()`方法完成的，该方法将PCA模型拟合到数据上，然后转换数据。
- en: 'Finally, it visualizes the data in the space of those two principal components,
    coloring the points according to the type of wine:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，它在两个主成分的空间中可视化数据，根据葡萄酒的类型对点进行着色：
- en: '[PRE0]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The resulting visualization should look similar to what’s shown in *Figure
    7**.4*:'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果可视化应类似于*图7**.4*所示：
- en: '![Figure 7.4: PCA scatter plot for the wine dataset](img/B18143_07_4.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4：葡萄酒数据集的PCA散点图](img/B18143_07_4.jpg)'
- en: 'Figure 7.4: PCA scatter plot for the wine dataset'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：葡萄酒数据集的PCA散点图
- en: The scatter plot should show a clear separation between the different types
    of wine, which suggests that the type of wine is closely related to its chemical
    constituents. Moreover, the PCA object (named `pca` in our code) stores the `components_`
    attribute, which contains the mappings of each feature concerning the principal
    components. By examining these, we can find out which features are the most important
    in distinguishing between the types of wine.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图应显示不同类型葡萄酒之间的清晰分离，这表明葡萄酒的类型与其化学成分密切相关。此外，PCA对象（在我们的代码中命名为`pca`）存储了`components_`属性，其中包含每个特征与主成分的映射。通过检查这些，我们可以找出哪些特征在区分葡萄酒类型时最为重要。
- en: Each data point in the visualization represents a single sample from our dataset,
    but instead of being plotted in the original, high-dimensional feature space,
    it’s plotted in the lower-dimensional space defined by the principal components.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化中的每个数据点代表我们数据集中的一个单独样本，但不是在原始的高维特征空间中绘制，而是在由主成分定义的较低维空间中绘制。
- en: In the context of the wine dataset, each data point in the PCA visualization
    represents a single wine sample. Because we have mapped our original 13 features
    to two PCA dimensions, the position of each point on the *X* and *Y* axes corresponds
    to the values of the first and second principal components for that wine sample.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在葡萄酒数据集的上下文中，PCA可视化中的每个数据点代表一个单独的葡萄酒样本。因为我们已经将原始的13个特征映射到两个PCA维度，所以每个点在*X*和*Y*轴上的位置对应于该葡萄酒样本的第一个和第二个主成分的值。
- en: The color of each point represents the true class of the wine sample (derived
    from three different cultivars). By coloring the points according to their true
    class, you can see how well the PCA transformation separates the different classes
    in the reduced dimensional space.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 每个点的颜色代表葡萄酒样本的真实类别（来自三种不同的品种）。通过根据它们的真实类别着色点，你可以看到PCA转换在降维空间中如何将不同的类别分开。
- en: Remember that each principal component is a linear combination of the original
    features, so the position of each point is still determined by the values of its
    original features. In this way, PCA enables us to visualize the high-dimensional
    data, and it highlights the dimensions of greatest variance, which are often the
    most informative.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，每个主成分是原始特征的线性组合，因此每个点的位置仍然由其原始特征值决定。这样，PCA使我们能够可视化高维数据，并突出显示最大方差维度，这些维度通常是最具信息量的。
- en: We can then access the `components_` attribute of the fitted PCA object to view
    the individual components. This attribute returns a matrix where each row corresponds
    to a principal component and each column corresponds to an original feature.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以访问拟合的PCA对象的`components_`属性来查看各个成分。该属性返回一个矩阵，其中每一行对应一个主成分，每一列对应一个原始特征。
- en: 'So, the following code will print a table, where the values in the table represent
    the weights of each feature in each component:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，以下代码将打印一个表格，其中表格中的值代表每个成分中每个特征的权重：
- en: '[PRE1]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The results should look similar to what’s shown in *Table 7.2*:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该类似于*表7.2*中所示：
- en: '![Table 7.2: PCA components and features for the wine dataset](img/B18143_07_Table2.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![表7.2：葡萄酒数据集的PCA成分和特征](img/B18143_07_Table2.jpg)'
- en: 'Table 7.2: PCA components and features for the wine dataset'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.2：葡萄酒数据集的PCA成分和特征
- en: By looking at the absolute values of these weights, we can determine which features
    are most important for each principal component. Large absolute values correspond
    to features that play a significant role in the variation captured by that principal
    component, and the sign (positive or negative) of the weight can tell us about
    the direction of the relationship between the feature and the principal component.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看这些权重的绝对值，我们可以确定哪些特征对每个主成分最重要。大的绝对值对应于在捕捉该主成分变化中起重要作用的特征，权重的符号（正或负）可以告诉我们特征与主成分之间关系的方向。
- en: 'Next, let’s see how we could use LDA to identify the constituents that account
    for the most variance between the types of wine. Again, we’ll first import the
    necessary libraries and standardize the data. We’ll then apply LDA to the standardized
    features, specifying `n_components=2` to get a two-dimensional projection, and
    then fit the LDA model to the data and transform the data for the first two LDA
    components. Finally, we’ll visualize the transformed data:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何使用LDA来识别导致不同类型葡萄酒之间差异最大的成分。同样，我们首先导入必要的库并标准化数据。然后，我们将对标准化特征应用LDA，指定`n_components=2`以获得二维投影，然后拟合LDA模型到数据并对前两个LDA成分进行数据转换。最后，我们将可视化转换后的数据：
- en: '[PRE2]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The resulting visualization should look similar to what’s shown in *Figure
    7**.5*:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可视化应该类似于*图7**.5*中所示：
- en: '![Figure 7.5: LDA scatter plot for the wine dataset](img/B18143_07_5.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5：葡萄酒数据集的LDA散点图](img/B18143_07_5.jpg)'
- en: 'Figure 7.5: LDA scatter plot for the wine dataset'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：葡萄酒数据集的LDA散点图
- en: In this case, the scatter plot shows the data points in the space of the first
    two LDA components, and the points are again colored according to the type of
    wine. We should also see a clear separation between the different types of wine,
    indicating that they have different distributions of their various chemical constituents.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，散点图显示了第一二个LDA成分空间中的数据点，点再次根据葡萄酒类型着色。我们还应该看到不同类型葡萄酒之间的清晰分离，这表明它们具有不同的化学成分分布。
- en: 'As we did with the `components_` attribute of the fitted PCA object, we can
    inspect the `coef_` attribute of the fitted LDA object to view the most discriminative
    features, as shown in the following code and its respective output:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们检查拟合的PCA对象的`components_`属性一样，我们可以查看拟合的LDA对象的`coef_`属性来查看最具判别性的特征，如下面的代码及其相应的输出所示：
- en: '[PRE3]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The results should look similar to what’s shown in *Table 7.3*:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该类似于*表7.3*中所示：
- en: '![Table 7.3: LDA classes and features for the wine dataset](img/B18143_07_Table3.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![表7.3：葡萄酒数据集的LDA类别和特征](img/B18143_07_Table3.jpg)'
- en: 'Table 7.3: LDA classes and features for the wine dataset'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.3：葡萄酒数据集的LDA类别和特征
- en: In the resulting table, each row corresponds to a class (compared to the rest),
    and each column corresponds to an original feature. So, the values in the table
    represent the coefficients of each feature in the context of the linear discriminants.
    Similar to our PCA assessment, large absolute values indicate features that contribute
    significantly to separating the classes.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成的表中，每一行对应一个类别（与其他类别相比），每一列对应一个原始特征。因此，表中的值代表线性判别分析中每个特征的系数。类似于我们的 PCA 评估，绝对值大的特征表示对区分类别有显著贡献的特征。
- en: Now that we’ve looked at how to reduce the dimensionality of our datasets, let’s
    assume that we’ve identified our required features and begin to explore how we
    could further engineer features to ensure we have the best possible set of features
    to train our models.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何降低数据集的维度，让我们假设我们已经确定了所需的特征，并开始探索我们如何进一步工程特征以确保我们有最佳可能的特征集来训练我们的模型。
- en: Feature engineering
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程
- en: Feature engineering can constitute a large portion of a data scientist’s activities,
    and it can be just as important to their success, or sometimes even more important,
    than choosing the right machine learning algorithm. In this section, we will dive
    deeper into feature engineering, which can be considered both an art and a science.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程可以构成数据科学家活动的大部分内容，它对他们成功的重要性可能与其选择正确的机器学习算法一样重要，有时甚至更重要。在本节中，我们将更深入地探讨特征工程，这可以被视为一种艺术和科学。
- en: We will use the *Titanic* dataset available on OpenML ([https://www.openml.org/search?type=data&sort=runs&id=40945](https://www.openml.org/search?type=data&sort=runs&id=40945))
    for our examples in this section. This dataset contains information about passengers
    aboard the Titanic, including demographic data, ticket class, fare, and whether
    they survived the sinking of the ship.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 OpenML 上可用的 *Titanic* 数据集（[https://www.openml.org/search?type=data&sort=runs&id=40945](https://www.openml.org/search?type=data&sort=runs&id=40945)）在本节中的示例。此数据集包含关于泰坦尼克号乘客的信息，包括人口统计数据、票务等级、票价以及他们是否在船沉没中幸存。
- en: In the `Chapter-07` directory in JupyterLab on your Vertex AI Workbench Notebook
    Instance, open the `feature-eng-titanic.ipynb` notebook and choose **Python (Local)**
    as the kernel. Again, run each cell in the notebook by selecting the cell and
    pressing *Shift* + *Enter* on your keyboard.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的 Vertex AI Workbench Notebook 实例的 JupyterLab 中的 `Chapter-07` 目录下，打开 `feature-eng-titanic.ipynb`
    笔记本，并将内核选择为 **Python (Local)**。再次，通过选择单元格并在键盘上按下 *Shift* + *Enter* 来运行笔记本中的每个单元格。
- en: 'In this notebook, the code performs the following steps:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个笔记本中，代码执行以下步骤：
- en: First, it imports the necessary libraries.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，它导入必要的库。
- en: Then, it loads the dataset.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它加载数据集。
- en: After, it performs some initial exploration to see what our dataset looks like.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它执行一些初步探索，以查看我们的数据集看起来如何。
- en: Finally, it engineers new features.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，它工程新的特征。
- en: 'Let’s take a look at each step in more detail, starting with importing the
    required libraries, and loading and exploring the dataset. We use the following
    code to perform those tasks:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地查看每个步骤，从导入所需的库、加载数据集和探索数据集开始。我们使用以下代码来完成这些任务：
- en: '[PRE4]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The resulting output from the `head()` method should look similar to what’s
    shown in *Table 7.4*:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`head()` 方法的输出结果应类似于 *表 7.4* 中所示：'
- en: '![Table 7.4: Titanic dataset head() output](img/B18143_07_Table4.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![表 7.4：Titanic 数据集 head() 输出](img/B18143_07_Table4.jpg)'
- en: 'Table 7.4: Titanic dataset head() output'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.4：Titanic 数据集 head() 输出
- en: 'The fields in the dataset are as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的字段如下：
- en: '`survived`: Indicates if a passenger survived or not. It’s a binary feature
    where 1 stands for survived and 0 stands for not survived.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`survived`：表示乘客是否幸存。这是一个二进制特征，其中 1 表示幸存，0 表示未幸存。'
- en: '`pclass` (passenger class): Indicates the class of the passenger’s ticket.
    It has three categories: 1 for first class, 2 for second class, and 3 for third
    class. This could also indicate the socio-economic status of the passengers.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pclass`（乘客等级）：表示乘客票的等级。它有三个类别：1 表示头等舱，2 表示二等舱，3 表示三等舱。这也可以表示乘客的社会经济状况。'
- en: '`name`: The name of the passenger.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`：乘客的名字。'
- en: '`sex`: The gender of the passenger; male or female.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sex`：乘客的性别；男性或女性。'
- en: '`age`: The age of the passenger. Some ages are fractional, and for passengers
    less than 1 year old, the age is estimated as a fraction.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`age`：乘客的年龄。有些年龄是分数，对于年龄小于 1 岁的乘客，年龄估计为分数。'
- en: '`sibsp`: The total number of the passengers’ siblings and spouses aboard the
    Titanic.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sibsp`: 乘客在泰坦尼克号上的兄弟姐妹和配偶总数。'
- en: '`parch`: The total number of the passengers’ parents and children aboard the
    Titanic.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parch`: 乘客在泰坦尼克号上的父母和子女总数。'
- en: '`ticket`: The ticket number of the passenger.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ticket`: 乘客的票号。'
- en: '`fare`: The passenger fare – that is, how much the ticket cost.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fare`: 乘客票价——即票的成本。'
- en: '`cabin`: The cabin number where the passenger was staying. Some entries are
    NaN, indicating that the cabin number is missing from the data.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cabin`: 乘客所住的船舱号。一些条目是 NaN，表示数据中缺少船舱号。'
- en: '`embarked`: The port where the passenger boarded the Titanic. C is for Cherbourg;
    Q is for Queenstown; S is for Southampton.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embarked`: 乘客登船的港口。C 代表瑟堡；Q 代表昆士敦；S 代表南安普顿。'
- en: '`boat`: Which lifeboat the passenger was assigned to (if the passenger survived).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boat`: 乘客被分配到的救生艇（如果乘客幸存）。'
- en: '`body`: Body number (if the passenger did not survive and their body was recovered).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`body`: 身体编号（如果乘客未幸存且其尸体被找回）。'
- en: '`home.dest`: The passenger’s home and destination.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`home.dest`: 乘客的家乡和目的地。'
- en: Assuming that we want to use this dataset to build a model that could predict
    the likelihood of a passenger surviving based on the information recorded about
    them, let’s see if we can use some domain knowledge to assess which features are
    likely to be the biggest contributors to the outcome of surviving or not surviving,
    and whether we could use any data manipulation techniques to engineer more useful
    features.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要使用这个数据集来构建一个模型，该模型可以根据乘客记录的信息预测乘客生存的可能性，让我们看看我们是否可以使用一些领域知识来评估哪些特征可能是对生存或死亡结果影响最大的贡献者，以及我们是否可以使用任何数据操作技术来构建更有用的特征。
- en: '`Passenger Class` stands out as a potentially important feature to begin with
    because the first-class and second-class passengers had cabins on higher levels
    of the ship, which were closer to the lifeboats.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`Passenger Class` 是一个可能的重要特征，我们可以从它开始考虑，因为头等舱和二等舱乘客的船舱位于更高的船层，这些船舱更靠近救生艇。'
- en: 'It’s unlikely that the passenger’s name would affect the outcome, nor their
    ticket number or port of embarkation. However, we could engineer a new feature
    named `Title` that’s extracted from the passengers’ names and could provide valuable
    information related to social status, occupation, marital status, and age, which
    might not be immediately apparent from the other features. We could also clean
    up this new feature by merging similar titles such as `Miss` and `Ms` and identifying
    elevated titles as `Distinguished`. The code to do that would be as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 乘客的名字不太可能影响结果，他们的票号或登船港口也是如此。然而，我们可以构建一个名为 `Title` 的新特征，该特征从乘客的名字中提取出来，并可能提供有关社会地位、职业、婚姻状况和年龄的有价值信息，这些信息可能不会立即从其他特征中明显看出。我们还可以通过合并类似头衔，如
    `Miss` 和 `Ms`，并将高级头衔标识为 `Distinguished` 来清理这个新特征。执行此操作的代码如下：
- en: '[PRE5]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, let’s consider the `Fare` and `Cabin` features. These could be somewhat
    correlated with class, but we will dive into these features in more detail. For
    the `Cabin` feature, we could extract another feature named `CabinClass`, which
    more clearly represents the class associated with each entry. We could do this,
    for example, by extracting the first letter from the cabin number, using it to
    represent the cabin class (for example, A, B, C, and so on), and storing it in
    the new `CabinClass` feature. The code to do that would be as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们考虑 `Fare` 和 `Cabin` 特征。这些特征可能与阶级有些相关，但我们将更详细地探讨这些特征。对于 `Cabin` 特征，我们可以提取另一个名为
    `CabinClass` 的新特征，它更清楚地表示与每个条目相关的阶级。例如，我们可以通过从船舱号中提取第一个字母，用它来表示船舱阶级（例如，A、B、C 等），并将其存储在新的
    `CabinClass` 特征中。执行此操作的代码如下：
- en: '[PRE6]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s also ensure that we represent the fare as accurately as possible by considering
    that people may have purchased fares as families traveling together. To do this,
    we can create a new feature named `FamilySize` as a combination of the `SibSp`
    and `Parch` features (adding an additional “1” to account for the current passenger),
    and then compute `FarePerPerson` by dividing the `Fare` feature by the `FamilySize`
    feature by using the following code:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确保尽可能准确地表示票价，考虑到人们可能作为一家人一起旅行购买了票价。为此，我们可以创建一个名为 `FamilySize` 的新特征，它是 `SibSp`
    和 `Parch` 特征的组合（为当前乘客添加一个额外的“1”），然后通过以下代码通过将 `Fare` 特征除以 `FamilySize` 特征来计算 `FarePerPerson`：
- en: '[PRE7]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Whether somebody is traveling alone or with their family could also affect
    their chances of survival. For example, family members could help each other when
    trying to get to the lifeboats. So, let’s create a feature from the `FamilySize`
    feature that identifies whether a passenger was traveling alone:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一个人是独自旅行还是与家人同行也可能影响他们的生存机会。例如，家庭成员在试图到达救生艇时可以互相帮助。因此，让我们从 `FamilySize` 特征中创建一个特征，以确定乘客是否独自旅行：
- en: '[PRE8]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, let’s consider how age affects the likelihood of survival. People who
    are very young, or elderly, may, unfortunately, have less likelihood of surviving
    unless they have people to help them. However, we may not need yearly and fractional-yearly
    granularity when considering age in this context, and perhaps grouping people
    into age groups may be more effective. In that case, we can use the following
    code to create a new feature named `AgeGroup` that will group passengers by decades
    such as 0-9, 10-19, 20-29, and so on:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们考虑年龄如何影响生存的可能性。非常年轻或年长的人，不幸的是，如果没有他人的帮助，生存的可能性可能较小。然而，在考虑年龄的这种情况下，我们可能不需要按年或按分数年进行粒度划分，也许将人们分组到年龄组可能更有效。在这种情况下，我们可以使用以下代码创建一个名为
    `AgeGroup` 的新特征，该特征将按十年分组，例如 0-9、10-19、20-29 等：
- en: '[PRE9]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We also want to convert the categorical features into numerical values using
    one-hot encoding since machine learning models typically require numeric values.
    We could do this as follows (we need to do this for all of our categorical features):'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望将分类特征转换为数值，使用独热编码，因为机器学习模型通常需要数值。我们可以这样做（我们需要为所有分类特征都这样做）：
- en: '[PRE10]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let’s drop the features that we know will not be valuable for predicting
    the likelihood of survival (as well as the original features that we encoded,
    because only their encoded versions are needed):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们删除那些我们知道对预测生存可能性（以及我们编码的原始特征）没有价值的特征：
- en: '[PRE11]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we can take a quick peek at what our updated dataset looks like:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以快速查看我们的更新数据集看起来像什么：
- en: '[PRE12]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The resulting output from the `head()` method should look similar to what’s
    shown in *Table 7.5*:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`head()` 方法的输出结果应类似于 *表 7.5* 中所示：'
- en: '![Table 7.5: Output from the head() method for the updated dataset](img/B18143_07_Table5.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![表 7.5：更新数据集的 head() 方法输出](img/B18143_07_Table5.jpg)'
- en: 'Table 7.5: Output from the head() method for the updated dataset'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.5：更新数据集的 head() 方法输出
- en: At this point, we have an augmented dataset with engineered features that can
    be used to train a model. It’s important to bear in mind that any feature engineering
    steps we perform on our source dataset also need to be taken into account when
    we use our model to make predictions. This common need in machine learning is
    what gave rise to the requirement for Google Cloud to develop a service named
    Feature Store. We’ll explore this in the next section.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们有一个增强的数据集，其中包含可用于训练模型的工程化特征。重要的是要记住，我们在源数据集上执行的任何特征工程步骤，在用我们的模型进行预测时也需要考虑。这种在机器学习中的常见需求促使
    Google Cloud 开发了一个名为特征存储的服务。我们将在下一节中探讨这一点。
- en: Vertex AI Feature Store
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Vertex AI 特征存储
- en: We’ve done a lot of feature engineering work in this chapter. Bear in mind that
    we performed data transformations and engineered new features because we had reason
    to believe that the raw data was insufficient to train a machine learning model
    to suit our business case. This means that the raw data our model will see in
    the real world would usually not contain the enhancements we performed on the
    data during training. After all of that work, we would generally want to save
    the updated features we’ve engineered so that our model can reference them when
    it needs to make predictions. Vertex AI Feature Store was created for this purpose.
    We briefly mentioned Vertex AI Feature Store in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059),
    and in this section, we will dive into more detail regarding what it is and how
    we can use it to store and serve features for both training and inference.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们进行了大量的特征工程工作。请注意，我们执行了数据转换并创建了新的特征，因为我们有理由相信原始数据不足以训练一个适合我们业务案例的机器学习模型。这意味着模型在现实世界中看到的原始数据通常不会包含我们在训练期间对数据进行增强的部分。在完成所有这些工作之后，我们通常会保存我们已工程化的更新特征，以便模型在需要做出预测时可以引用它们。Vertex
    AI 特征存储就是为了这个目的而创建的。我们在 [*第 3 章*](B18143_03.xhtml#_idTextAnchor059) 中简要提到了 Vertex
    AI 特征存储，在本节中，我们将更详细地探讨它是什么以及我们如何使用它来存储和提供训练和推理所需的特征。
- en: Introduction to Vertex AI Feature Store
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Vertex AI Feature Store 简介
- en: 'Here’s the official definition from the Google Cloud documentation:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是来自 Google Cloud 文档的官方定义：
- en: Vertex AI Feature Store is a managed, cloud-native feature store service that’s
    integral to Vertex AI. It streamlines your machine learning feature management
    and online serving processes by letting you manage your feature data in a BigQuery
    table or view. You can then serve features online directly from the BigQuery data
    source. Vertex AI Feature Store provisions resources that let you set up online
    serving by specifying your feature data sources. It then acts as a metadata layer
    interfacing with the BigQuery data sources and serves the latest feature values
    directly from BigQuery for online predictions at low latencies.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI Feature Store 是一个托管、云原生的特征存储服务，它是 Vertex AI 的核心组成部分。它通过允许你在 BigQuery
    表或视图中管理你的特征数据，简化了你的机器学习特征管理和在线服务流程。然后，你可以直接从 BigQuery 数据源在线提供特征。Vertex AI Feature
    Store 提供资源，允许你通过指定你的特征数据源来设置在线服务。然后，它作为元数据层与 BigQuery 数据源接口，以低延迟直接从 BigQuery 为在线预测提供最新的特征值。
- en: In addition to storing and serving our features, Vertex AI Feature Store also
    integrates with Google Cloud Dataplex to provide feature governance capabilities,
    including the ability to track feature metadata such as feature labels and versions.
    In [*Chapter 13*](B18143_13.xhtml#_idTextAnchor328), we will dive into the importance
    of data governance and discuss how Dataplex can be used as an important component
    in building a robust governance framework.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 除了存储和提供我们的特征外，Vertex AI Feature Store 还与 Google Cloud Dataplex 集成，提供特征治理能力，包括跟踪特征元数据（如特征标签和版本）的能力。在[*第13章*](B18143_13.xhtml#_idTextAnchor328)中，我们将深入探讨数据治理的重要性，并讨论如何将
    Dataplex 用作构建强大治理框架的重要组件。
- en: At this point, it’s important to highlight that an entirely new version of Vertex
    AI Feature Store was launched in 2023\. As a result, there are now two different
    variants of the Feature Store service we can choose in Google Cloud, whereby the
    prior version is referred to as Vertex AI Feature Store (Legacy), and the new
    version is simply referred to as Vertex AI Feature Store. We will discuss both
    variants in this chapter, as well as some of the main distinctions between them.
    To provide context for the content in subsequent sections, I will briefly describe
    the topic of online versus offline feature serving.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，重要的是要强调，Vertex AI Feature Store 在 2023 年推出了一个全新的版本。因此，现在在 Google Cloud
    中我们可以选择两种不同的特征存储服务版本，先前版本被称为 Vertex AI Feature Store（Legacy），而新版本则简单地称为 Vertex
    AI Feature Store。我们将在本章中讨论这两种版本，以及它们之间的一些主要区别。为了为后续章节的内容提供背景，我将简要描述在线与离线特征服务的话题。
- en: Online versus offline feature serving
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在线与离线特征服务
- en: Simply put, **online serving** refers to a scenario in which the interaction
    is happening in real time – that is, the requesting entity or client sends a request
    and synchronously waits for a response. In this scenario, latency needs to be
    reduced as much as possible. On the other side of the coin is **offline serving**,
    which refers to a scenario in which the requesting entity or client does not synchronously
    wait for a response, and the operation is allowed to happen over a longer period.
    In this case, latency is generally not a primary concern. This concept relates
    closely to the topic of online and offline inference, which we will cover in detail
    in [*Chapter 10*](B18143_10.xhtml#_idTextAnchor259).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，**在线服务**指的是交互发生在实时的情况——也就是说，请求实体或客户端发送请求并同步等待响应。在这种情况下，需要尽可能减少延迟。另一方面是**离线服务**，它指的是请求实体或客户端不同步等待响应，操作允许在更长的时间内发生。在这种情况下，延迟通常不是主要关注的问题。这个概念与在线和离线推理的主题密切相关，我们将在[*第10章*](B18143_10.xhtml#_idTextAnchor259)中详细讨论。
- en: In the case of offline feature serving, Vertex AI Feature Store allows us to
    store and serve features directly in a Google Cloud BigQuery dataset. This is
    quite a convenient option as many Google Cloud customers already use BigQuery
    to store and analyze large amounts of their data.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在离线特征服务的情况下，Vertex AI Feature Store 允许我们直接在 Google Cloud BigQuery 数据集中存储和提供特征。这是一个相当方便的选项，因为许多
    Google Cloud 客户已经使用 BigQuery 来存储和分析大量数据。
- en: In the case of online feature serving, there are now two ways in which we can
    serve our features in Vertex AI Feature Store. The first option uses Google Cloud
    Bigtable to serve our features. Google Cloud Bigtable is a powerful service that
    is designed for serving large data volumes (terabytes of data).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在在线特征服务的情况下，现在有两种方式可以在Vertex AI特征存储中提供我们的特征。第一种选择使用Google Cloud Bigtable来提供我们的特征。Google
    Cloud Bigtable是一个专为服务大量数据（数以兆字节计的数据）而设计的强大服务。
- en: The second option for online feature serving, which is referred to as **optimized
    online serving**, and was added as part of the new version of Vertex AI Feature
    Store, allows us to create an online store that is optimized specifically for
    serving feature data at ultra-low latencies.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在线特征服务的第二种选择，被称为**优化在线服务**，作为Vertex AI特征存储新版本的一部分被添加，允许我们创建一个专门针对以极低延迟提供特征数据的在线商店。
- en: Choosing one option or the other depends on the needs of your use case, specifically
    whether you need to handle very large volumes of data or whether you need to serve
    your features with ultra-low latency. Cost is also a consideration in this decision,
    bearing in mind that the Bigtable solution generally costs less than the optimized
    online serving solution.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 选择哪种选项取决于你的用例需求，特别是你是否需要处理非常大的数据量，或者你是否需要以极低延迟提供你的特征。成本也是这个决定的一个考虑因素，考虑到Bigtable解决方案通常比优化在线服务解决方案成本低。
- en: We will focus primarily on the optimized online serving approach in this chapter
    and the accompanying Jupyter Notebook. The following section dives deeper into
    the process of setting up online feature serving in Vertex AI Feature Store.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和随附的Jupyter Notebook中，我们将主要关注优化在线服务方法。以下部分将更深入地探讨在Vertex AI特征存储中设置在线特征服务的过程。
- en: Online feature serving
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在线特征服务
- en: 'At a high level, the following steps are required to set up online serving
    using Vertex AI Feature Store. We will elaborate on these steps in subsequent
    sections:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，以下步骤是使用Vertex AI特征存储设置在线服务所需的。我们将在后续章节中详细阐述这些步骤：
- en: Prepare data sources in BigQuery.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在BigQuery中准备数据源。
- en: 'Optional: Register data sources in the feature registry by creating feature
    groups and features.'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可选：通过创建特征组和特征在特征注册表中注册数据源。
- en: Set up the online store and feature view resources to present the feature data
    sources.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置在线商店并展示资源以呈现特征数据源。
- en: Serve the latest online feature values from a feature view.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从特征视图中提供最新的在线特征值。
- en: Let’s take a look at these concepts in more detail.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看这些概念。
- en: Feature registry
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征注册
- en: When using the optimized online serving approach, we can perform an optional
    step to register our features in the Vertex AI feature registry, which has also
    been added as a component of the new version of Vertex AI Feature Store.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用优化的在线服务方法时，我们可以选择一个步骤来在我们的Vertex AI特征注册表中注册我们的特征，该注册表也已被添加为Vertex AI特征存储新版本的一个组件。
- en: This involves the process of creating resources referred to as **feature groups**,
    which represent logical groupings of feature columns and are associated with specific
    BigQuery source tables or views. In turn, feature groups contain resources referred
    to as **features**, which represent specific columns containing feature values
    within the data source represented by the feature group.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及到创建称为**特征组**的资源的过程，这些特征组代表特征列的逻辑分组，并与特定的BigQuery源表或视图相关联。反过来，特征组包含称为**特征**的资源，这些特征代表特征组所表示的数据源中包含特征值的特定列。
- en: We can still serve features online even if we don’t add our BigQuery data sources
    to the feature registry, but note that the feature registry provides additional
    functionality, such as storing historical time series data associated with your
    features. As a result, we will use the feature registry in the practical exercises
    accompanying this chapter. Now, let’s take a look at the process of setting up
    online feature serving in more detail.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们不将我们的BigQuery数据源添加到特征注册表中，我们仍然可以在网上提供特征，但请注意，特征注册表提供了额外的功能，例如存储与你的特征相关的历史时间序列数据。因此，我们将在这个章节的实践练习中使用特征注册表。现在，让我们更详细地看看设置在线特征服务的过程。
- en: Online feature store and feature views
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在线特征存储和特征视图
- en: 'After we have set up our feature data in BigQuery, and optionally registered
    feature groups and features in the feature registry, there are two main types
    of resources that we need to set up to enable online feature serving:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们设置好 BigQuery 中的特征数据后，并可选地在特征注册表中注册特征组和特征，我们需要设置两种主要类型的资源来启用在线特征服务：
- en: An **online serving cluster instance**, which is referred to as the **online
    store**. Remember that we can either use Bigtable for online feature serving or
    the newly released optimized online feature serving option.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**在线服务集群实例**，也称为**在线存储**。请记住，我们可以使用 Bigtable 进行在线特征服务或新发布的优化在线特征服务选项。
- en: One or more **feature view instances**, where each feature view is associated
    with a feature data source, such as a feature group in our feature registry (if
    we have chosen the option of registering our features in the feature registry),
    or a BigQuery source table or view.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个或多个**特征视图实例**，其中每个特征视图都与一个特征数据源相关联，例如在我们的特征注册表中的特征组（如果我们选择了在特征注册表中注册我们的特征选项），或者一个
    BigQuery 源表或视图。
- en: After we create a feature view, we can configure synchronization settings to
    ensure that our feature data in BigQuery is synchronized with our feature view.
    We can trigger a synchronization manually, but if our source data is expected
    to be updated over time, then we can also configure a schedule to periodically
    refresh the contents of our feature view from the data source.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建特征视图后，我们可以配置同步设置，以确保我们的 BigQuery 中的特征数据与我们的特征视图同步。我们可以手动触发同步，但如果我们的源数据预计会随时间更新，我们还可以配置一个计划，定期从数据源刷新我们的特征视图内容。
- en: Now that we’ve covered many of the important concepts related to Vertex AI Feature
    Store, it’s time to dive in and build our very own feature store!
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了与 Vertex AI Feature Store 相关的许多重要概念，是时候深入其中，构建我们自己的特征存储库了！
- en: Building our feature store
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建我们的特征存储库
- en: In this section, we will perform practical exercises that implement the concepts
    we learned about in the previous sections.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进行实际练习，以实现我们在前一节中学到的概念。
- en: Use our Vertex AI notebook to build the feature store
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用我们的 Vertex AI 笔记本构建特征存储库
- en: 'In the Vertex AI Notebook Instance we created in [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168),
    we can perform the following steps to build the feature store:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 5 章*](B18143_05.xhtml#_idTextAnchor168)中我们创建的 Vertex AI 笔记本实例中，我们可以执行以下步骤来构建特征存储库：
- en: Select **Open JupyterLab** for the Vertex AI Notebook Instance we created in
    [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168).
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**打开 JupyterLab**以打开我们在[*第 5 章*](B18143_05.xhtml#_idTextAnchor168)中创建的 Vertex
    AI 笔记本实例。
- en: When the JupyterLab opens, you should see a folder in your notebook called `Google-Machine-Learning-for-Solutions-Architects`.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 JupyterLab 打开时，您应该会在笔记本中看到一个名为 `Google-Machine-Learning-for-Solutions-Architects`
    的文件夹。
- en: Double-click on that folder, then double-click on the `Chapter-07` folder within
    it, and then double-click on the `feature-store.ipynb` file to open it.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双击该文件夹，然后在该文件夹内双击 `Chapter-07` 文件夹，最后双击 `feature-store.ipynb` 文件以打开它。
- en: On the **Select Kernel** screen that appears, select **Python (Local).**
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在出现的**选择内核**屏幕上，选择**Python (Local)**。
- en: Press *Shift* + *Enter* to run each of the cells in the notebook and read the
    explanations in the Markdown and comments to understand what we’re doing.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按*Shift* + *Enter*运行笔记本中的每个单元格，并阅读 Markdown 和注释中的说明，以了解我们在做什么。
- en: Now that you’ve followed the steps in the notebook to perform feature selection
    and engineering, have built a feature store, and used some of the features to
    train a model, let’s take a look at how those features could be used during inference
    time. In later chapters, you will learn how to deploy models for online inference
    and send inference requests to those models, but for now, I’ll explain the process
    at a conceptual level.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经按照笔记本中的步骤执行了特征选择和工程，构建了特征存储库，并使用了一些特征来训练模型，让我们看看这些特征在推理时间如何被使用。在后面的章节中，您将学习如何部署模型进行在线推理并向这些模型发送推理请求，但就目前而言，我将从概念层面解释这个过程。
- en: How features are used during online inference
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征在在线推理过程中的使用
- en: In this section, I’ll use the taxi fare prediction model use case that we built
    in the accompanying Jupyter Notebook as an example to explain how we can use features
    from our feature store during inference. We’ll take a look at each step in the
    process.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将使用我们在配套的 Jupyter Notebook 中构建的出租车费用预测模型用例作为示例，来解释我们如何在推理过程中使用特征存储库中的特征。我们将查看过程中的每个步骤。
- en: Combining real-time and precomputed features
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合实时和预计算特征
- en: Features such as the current pickup time (`pickup_datetime`), `pickup_location`,
    and `passenger_count` can be obtained in real time as each taxi journey begins.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当前接车时间（`pickup_datetime`）、`pickup_location`和`passenger_count`等特征可以在每次出租车行程开始时实时获取。
- en: Our feature store also contains precomputed features, such as historical trip
    distances, fares per mile, pickup times, and locations. These features can be
    selected based on the current journey’s context from the available real-time features.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的特征存储还包含预计算的特征，例如历史行程距离、每英里费用、接车时间和位置。这些特征可以根据当前行程的上下文从可用的实时特征中选择。
- en: To get the precomputed features, the application handling the taxi journey can
    send a request to our feature store, passing identifiers such as the current time
    and location, after which the feature store can return the relevant feature values
    for these identifiers.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取预计算的特征，处理出租车行程的应用程序可以向我们的特征存储发送请求，传递标识符，如当前时间和位置，之后特征存储可以返回这些标识符的相关特征值。
- en: Data assembly for prediction
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测数据组装
- en: At this point, we can assemble the real-time data and fetched features into
    a feature vector matching the format expected by the model, and then pass the
    assembled feature vector to the model. The model then processes this vector and
    outputs a fare prediction, which can then be displayed in the app.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以将实时数据和获取到的特征组装成一个与模型期望格式匹配的特征向量，然后将组装好的特征向量传递给模型。模型随后处理这个向量，并输出一个费用预测，该预测随后可以在应用程序中显示。
- en: Well done! You have successfully built a feature store on Google Cloud. Let’s
    summarize everything we’ve discussed in this chapter.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！你已经成功在Google Cloud上构建了一个特征存储。让我们总结一下本章中我们讨论的所有内容。
- en: Summary
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how the quality of our features, and the ratio
    of features to observations in our dataset, influences how our algorithms learn
    from our data. We discussed challenges that can occur when our dataset contains
    many features and how to address those challenges by using mechanisms such as
    dimensionality reduction. We dived into details on dimensionality reduction techniques
    such as feature selection and feature projection, including algorithms such as
    PCA, LDA, and t-SNE, and we looked at examples of how to use some of these algorithms
    using hands-on activities.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了我们的特征质量以及数据集中特征与观察值的比率如何影响我们的算法从数据中学习。我们讨论了当我们的数据集中包含许多特征时可能出现的挑战，以及如何通过使用诸如降维等机制来解决这些挑战。我们深入探讨了降维技术，如特征选择和特征投影，包括PCA、LDA和t-SNE等算法，并查看了一些使用这些算法的实例，通过实际操作活动来实现。
- en: Next, we dived into feature engineering techniques in which we augmented a source
    dataset to create new features that contained information that was not readily
    available in the original dataset. Finally, we dived into Vertex AI Feature Store
    to learn about how we can use that service to store and serve our engineered feature
    sets.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们深入探讨了特征工程技术，其中我们增强了一个源数据集以创建包含原始数据集中不易获得的信息的新特征。最后，我们深入探讨了Vertex AI特征存储，了解我们如何使用该服务来存储和提供我们的工程特征集。
- en: In the next chapter, we will shift our focus away from the datasets and parameters
    that our models learn from and discuss different types of parameters that influence
    how our models learn. There, we’ll explore the concepts of hyperparameters and
    hyperparameter optimization.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把我们的关注点从模型所学习的数据集和参数转移到讨论影响模型学习方式的不同类型的参数。在那里，我们将探讨超参数和超参数优化的概念。
