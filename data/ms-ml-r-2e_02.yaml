- en: Linear Regression - The Blocking and Tackling of Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归——机器学习的阻挡和冲撞
- en: '"Some people try to find things in this game that don''t exist, but football
    is only two things - blocking and tackling."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '"有些人试图在这场比赛中寻找不存在的东西，但足球只关乎两件事——阻挡和冲撞。"'
- en: '- Vince Lombardi, Hall of Fame Football Coach'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 文斯·隆巴迪，名人堂足球教练'
- en: 'It is important that we get started with a simple, yet extremely effective
    technique that has been used for a long time: **linear regression**. Albert Einstein
    is believed to have remarked at one time or another that things should be made
    as simple as possible, but no simpler. This is sage advice and a good rule of
    thumb in the development of algorithms for machine learning. Considering the other
    techniques that we will discuss later, there is no simpler model than tried and
    tested linear regression, which uses the **least squares approach** to predict
    a quantitative outcome. In fact, one can consider it to be the foundation of all
    the methods that we will discuss later, many of which are mere extensions. If
    you can master the linear regression method, well, then quite frankly, I believe
    you can master the rest of this book. Therefore, let us consider this a good starting point
    for our journey towards becoming a machine learning guru.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始使用一种简单而极其有效的技术，这种技术已经被使用了很长时间：**线性回归**。阿尔伯特·爱因斯坦曾说过，事情应该尽可能简单，但不能更简单。这是明智的建议，也是开发机器学习算法时的一条好规则。考虑到我们稍后将要讨论的其他技术，没有比经过检验的线性回归更简单的模型了，它使用**最小二乘法**来预测定量结果。事实上，我们可以将其视为我们稍后将要讨论的所有方法的基石，其中许多只是扩展。如果您能掌握线性回归方法，那么坦白说，我相信您也可以掌握这本书的其余部分。因此，让我们把这视为我们成为机器学习大师之旅的一个良好起点。
- en: This chapter covers introductory material, and an expert in this subject can
    skip ahead to the next topic. Otherwise, ensure that you thoroughly understand
    this topic before venturing to other, more complex learning methods. I believe
    you will discover that many of your projects can be addressed by just applying
    what is discussed in the following section. Linear regression is probably the
    easiest model to explain to your customers, most of whom will have at least a
    cursory understanding of **R-squared**. Many of them will have been exposed to
    it at great depth and thus be comfortable with variable contribution, collinearity,
    and the like.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了入门材料，该领域的专家可以跳过直接进入下一主题。否则，在尝试其他更复杂的学习方法之前，请确保您彻底理解这个主题。我相信您会发现，许多项目只需应用以下章节中讨论的内容就可以解决。线性回归可能是向客户解释的最简单的模型，他们中的大多数至少对**R-squared**有一个初步的了解。其中许多人已经对其有深入的了解，因此对变量贡献、多重共线性等问题感到舒适。
- en: Univariate linear regression
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单变量线性回归
- en: We begin by looking at a simple way to predict a quantitative response, *Y*,
    with one predictor variable, *x*, assuming that *Y* has a linear relationship
    with *x*. The model for this can be written as, *Y = B0 + B1x + e*. We can state
    it as the expected value of *Y* being a function of the parameters *B0* (the intercept)
    plus *B1* (the slope) times *x*, plus an error term *e*. The least squares approach
    chooses the model parameters that minimize the **Residual Sum of Squares** (**RSS**)
    of the predicted *y* values versus the actual *Y* values. For a simple example,
    let's say we have the actual values of *Y1* and *Y2* equal to *10* and *20* respectively,
    along with the predictions of *y1* and *y2* as *12* and *18*. To calculate RSS,
    we add the squared differences *RSS = (Y1 - y1)² + (Y2 - y2)²*, which, with simple
    substitution, yields *(10 - 12)² + (20 - 18)² = 8*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先探讨一种简单的方法来预测一个定量响应*Y*，其中一个预测变量*x*，假设*Y*与*x*之间存在线性关系。这个模型的公式可以写成，*Y = B0
    + B1x + e*。我们可以将其表述为*Y*的期望值是参数*B0*（截距）加上*B1*（斜率）乘以*x*，再加上一个误差项*e*。最小二乘法选择模型参数，以最小化预测*y*值与实际*Y*值之间的**残差平方和**（**RSS**）。以一个简单的例子来说，假设我们得到的*Y1*和*Y2*的实际值分别为*10*和*20*，以及*y1*和*y2*的预测值分别为*12*和*18*。为了计算RSS，我们将平方差相加，即*RSS
    = (Y1 - y1)² + (Y2 - y2)²*，通过简单的代入，得到*(10 - 12)² + (20 - 18)² = 8*。
- en: I once remarked to a peer during our Lean Six Sigma Black Belt training that
    it's all about the sum of squares; understand the sum of squares and the rest
    will flow naturally. Perhaps that is true, at least to some extent.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾在我们的精益六西格玛黑带培训期间对一位同行说过，这关乎平方和的总和；理解平方和，其他问题就会自然而然地解决。也许这是真的，至少在某种程度上。
- en: Before we begin with an application, I want to point out that, if you read the
    headlines of various research breakthroughs, you should do so with a jaded eye
    and a skeptical mind as the conclusion put forth by the media may not be valid.
    As we shall see, R, and any other software for that matter, will give us a solution
    regardless of the inputs. However, just because the math makes sense and a high
    correlation or R-squared statistic is reported doesn't mean that the conclusion
    is valid.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始应用之前，我想指出，如果您阅读各种研究突破的头版新闻，您应该带着怀疑的眼光和批判性的思维去阅读，因为媒体提出的结论可能并不有效。正如我们将看到的，R和任何其他软件都将给出一个解决方案，无论输入如何。然而，仅仅因为数学上有意义，或者报告了高相关系数或R平方统计量，并不意味着结论是有效的。
- en: 'To drive this point home, let''s have a look at the famous `Anscombe` dataset,
    which is available in R. The statistician Francis Anscombe produced this set to
    highlight the importance of data visualization and outliers when analyzing data.
    It consists of four pairs of *X* and *Y* variables that have the same statistical
    properties but when plotted show something very different. I have used the data
    to train colleagues and to educate business partners on the hazards of fixating
    on statistics without exploring the data and checking assumptions. I think this
    is a good place to start should you have a similar need. It is a brief digression
    before moving on to serious modeling:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个观点更加明确，让我们看看著名的`Anscombe`数据集，该数据集在R中可用。统计学家Francis Anscombe制作了这个集合，以强调数据分析时数据可视化和异常值的重要性。它由四对具有相同统计特性的*X*和*Y*变量组成，但绘制时却显示出非常不同的结果。我已使用这些数据来培训同事，并教育商业伙伴关于专注于统计数据而不探索数据和检查假设的危险。如果您有类似的需求，我认为这是一个很好的开始。这是在继续进行严肃建模之前的一个简短离题：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As we shall see, each of the pairs has the same correlation coefficient: `0.816`.
    The first two are as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将看到的，每一对都有相同的相关系数：`0.816`。前两个如下所示：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The real insight here, as `Anscombe` intended, is when we plot all the four
    pairs together, as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如`Anscombe`所意图的，真正的洞察在于我们将所有四个对一起绘制时，如下所示：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Downloading the example code
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下载示例代码
- en: You can download the example code files for all Packt books you have purchased
    from your account at [http://www.packtpub.com](http://www.packtpub.com). If you
    purchased this book elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[http://www.packtpub.com](http://www.packtpub.com)的账户下载您购买的所有Packt书籍的示例代码文件。如果您在其他地方购买了这本书，您可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册，以便将文件直接通过电子邮件发送给您。
- en: 'The output of the preceding code is as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码的输出如下：
- en: '![](img/image_02_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_02_01.png)'
- en: As we can see, **Plot 1** appears to have a true linear relationship, **Plot
    2** is curvilinear, **Plot 3** has a dangerous outlier, and **Plot 4** is driven
    by one outlier. There you have it, a cautionary tale about  the dangers of solely
    relying on correlation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，**图1**似乎有一个真正的线性关系，**图2**是曲线关系，**图3**有一个危险的异常值，而**图4**则是由一个异常值驱动的。这就是一个关于仅依赖相关性的危险警告故事。
- en: Business understanding
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商业理解
- en: 'Our first case focuses on the goal of predicting the water yield (in inches)
    of the Snake River Watershed in Wyoming, USA, as a function of the water content
    of the year''s snowfall. This forecast will be useful in managing the water flow
    and reservoir levels as the Snake River provides much-needed irrigation water
    for the farms and ranches of several western states. The `snake` dataset is available
    in the `alr3` package (note that alr stands for applied linear regression):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一案例关注的是预测美国怀俄明州蛇河流域的水产量（以英寸为单位），作为当年降雪含水量的函数。这个预测将有助于管理水流和水库水平，因为蛇河为几个西部州提供了急需的灌溉用水。`snake`数据集可在`alr3`包中找到（注意，alr代表应用线性回归）：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that we have `17` observations, data exploration can begin. But first,
    let''s change `X` and `Y` to meaningful variable names, as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了`17`个观测值，可以开始数据探索。但在开始之前，让我们将`X`和`Y`改为有意义的变量名，如下所示：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码的输出如下：
- en: '![](img/image_02_02.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_02_02.png)'
- en: This is an interesting plot as the data is linear and has a slight curvilinear
    shape driven by two potential outliers at both ends of the extreme. As a result,
    transforming the data or deleting an outlying observation may be warranted.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的图，因为数据呈线性，并且由于两端存在两个潜在的异常值而呈现出轻微的曲线形状。因此，转换数据或删除异常观测值可能是合理的。
- en: 'To perform a linear regression in R, one uses the `lm()` function to create
    a model in the standard form of *fit = lm(Y ~ X)*. You can then test your assumptions
    using various functions on your fitted model by using the following code:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 R 中执行线性回归，可以使用 `lm()` 函数以标准形式 *fit = lm(Y ~ X)* 创建模型。然后，你可以使用以下代码通过拟合模型上的各种函数来测试你的假设：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With the `summary()` function, we can examine a number of items including the
    model specification, descriptive statistics about the residuals, the coefficients,
    codes to model significance, and a summary on model error and fit. Right now,
    let's focus on the parameter coefficient estimates, see if our predictor variable
    has a significant p-value, and if the overall model F-test has a significant p-value.
    Looking at the parameter estimates, the model tells us that the `yield` is equal
    to `0.72538` plus `0.49808` times the `content`. It can be stated that, for every
    1 unit change in the content, the yield will increase by `0.49808` units. The `F-statistic`
    is used to test the null hypothesis that the model coefficients are all 0.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `summary()` 函数，我们可以检查包括模型规范、残差的描述性统计、系数、模型显著性的代码以及模型误差和拟合的总结在内的多个项目。现在，让我们专注于参数系数估计，看看我们的预测变量是否有显著的
    p-value，以及整体模型的 F-检验是否有显著的 p-value。查看参数估计，模型告诉我们 `yield` 等于 `0.72538` 加上 `0.49808`
    乘以 `content`。可以说，对于 `content` 的每 1 个单位变化，`yield` 将增加 `0.49808` 个单位。`F-统计量` 用于检验模型系数都为
    0 的原假设。
- en: Since the `p-value` is highly significant, we can reject the null and move on
    to the t-test for content, which tests the null hypothesis that it is 0\. Again,
    we can reject the null. Additionally, we can see `Multiple R-squared` and `Adjusted
    R-squared` values. `Adjusted R-squared` will be covered under the multivariate
    regression topic, so let's zero in on `Multiple R-squared`; here we see that it
    is `0.8709`. In theory, it can range from 0 to 1 and is a measure of the strength
    of the association between *X* and *Y*. The interpretation in this case is that
    87 percent of the variation in the **water yield** can be explained by the **water
    content of snow**. On a side note, R-squared is nothing more than the correlation
    coefficient of [X, Y] squared.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `p-value` 非常显著，我们可以拒绝原假设，并继续进行内容检验的 t-检验，该检验检验的是原假设为 0。再次，我们可以拒绝原假设。此外，我们还可以看到
    `Multiple R-squared` 和 `Adjusted R-squared` 的值。`Adjusted R-squared` 将在多元回归主题下进行讨论，所以让我们专注于
    `Multiple R-squared`；在这里我们看到它是 `0.8709`。从理论上讲，它可以从 0 到 1 变化，是 *X* 和 *Y* 之间关联强度的度量。在这种情况下，解释是
    87% 的 **水产量** 变化可以由 **雪水含量** 解释。顺便提一下，R-squared 仅仅是 [X, Y] 的相关系数的平方。
- en: 'We can recall our scatterplot and now add the best fit line produced by our
    model using the following code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以回忆我们的散点图，并使用以下代码添加由我们的模型产生的最佳拟合线：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/image_02_03.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_02_03.png)'
- en: 'A linear regression model is only as good as the validity of its assumptions,
    which can be summarized as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型的好坏取决于其假设的有效性，可以总结如下：
- en: '**Linearity**: This is a linear relationship between the predictor and the
    response variables. If this relationship is not clearly present, transformations
    (log, polynomial, exponent, and so on) of *X* or *Y* may solve the problem.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性关系**：这是预测变量和响应变量之间的线性关系。如果这种关系不明显，可以通过对 *X* 或 *Y* 进行变换（对数、多项式、指数等）来解决问题。'
- en: '**Non-correlation of errors**: A common problem in time series and panel data
    where *e[n] = beta[n-1]*; if the errors are correlated, you run the risk of creating
    a poorly specified model.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**误差的非相关性**：在时间序列和面板数据中，这是一个常见问题，其中 *e[n] = beta[n-1]*；如果误差相关，你可能会创建一个定义不良的模型。'
- en: '**Homoscedasticity**: Normally the distributed and constant variance of errors,
    which means that the variance of errors is constant across different values of
    inputs. Violations of this assumption can create biased coefficient estimates,
    leading to statistical tests for significance that can be either too high or too
    low. This, in turn, leads to a wrong conclusion. This violation is referred to
    as **heteroscedasticity.**'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同方差性**：通常误差的正态分布和恒定方差，这意味着误差的方差在不同输入值上是恒定的。违反这个假设可以创建有偏的系数估计，导致显著性检验过高或过低。这反过来又会导致错误的结论。这种违反被称为**异方差性**。'
- en: '**No collinearity**: No linear relationship between two predictor variables,
    which is to say that there should be no correlation between the features. This,
    again, can lead to biased estimates.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无多重共线性**：两个预测变量之间没有线性关系，也就是说，特征之间应该没有相关性。这同样可能导致有偏的估计。'
- en: '**Presence of outliers**: Outliers can severely skew the estimation, and ideally
    they must be removed prior to fitting a model using linear regression; As we saw
    in the Anscombe example, this can lead to a biased estimate.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常值的存在**：异常值会严重扭曲估计，理想情况下，在用线性回归拟合模型之前必须将其移除；正如我们在Anscombe示例中看到的那样，这可能导致有偏的估计。'
- en: 'As we are building a univariate model independent of time, we will concern
    ourselves only with linearity and heteroscedasticity. The other assumptions will
    become important in the next section. The best way to initially check the assumptions
    is by producing plots. The `plot()` function, when combined with a linear model
    fit, will automatically produce four plots allowing you to examine the assumptions.
    R produces the plots one at a time and you advance through them by hitting the
    *Enter* key. It is best to examine all four simultaneously and we do it in the
    following manner:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在构建一个独立于时间的单变量模型，我们只关心线性性和异方差性。其他假设将在下一节中变得重要。最初检查假设的最好方法是生成图表。当`plot()`函数与线性模型拟合结合使用时，将自动生成四个图表，允许你检查假设。R一次生成一个图表，你可以通过按*Enter*键来浏览它们。最好同时检查所有四个，我们以下列方式进行检查：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/image_02_04.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_02_04.png)'
- en: The two plots on the left allow us to examine the homoscedasticity of errors
    and non-linearity. What we are looking for is some type of pattern or, more importantly,
    that no pattern exists. Given the sample size of only 17 observations, nothing
    obvious can be seen. Common heteroscedastic errors will appear to be u-shaped,
    inverted u-shaped, or clustered close together on the left of the plot. They will
    become wider as the fitted values increase (a funnel shape). It is safe to conclude
    that no violation of homoscedasticity is apparent in our model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的两个图表允许我们检查误差的同方差性和非线性。我们寻找的是某种类型的模式，或者更重要的是，没有任何模式存在。鉴于只有17个观测值的样本量，没有什么明显的可以观察到。常见的异方差误差看起来像是U形、倒U形，或者紧密地聚集在图表的左侧。随着拟合值的增加，它们会变得更宽（漏斗形状）。可以安全地得出结论，我们的模型中没有出现同方差性的违反。
- en: The **Normal Q-Q** plot in the upper-right corner helps us to determine if the
    residuals are normally distributed. The **Quantile-Quantile** (**Q-Q**) represents
    the quantile values of one variable plotted against the quantile values of another.
    It appears that the outliers (observations **7**, **9**, and **10**), may be causing
    a violation of the assumption. The **Residuals vs Leverage** plot can tell us
    what observations, if any, are unduly influencing the model; in other words, if
    there are any outliers we should be concerned about. The statistic is **Cook's
    distance** or **Cook's D**, and it is generally accepted that a value greater
    than 1 should be worthy of further inspection.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 上右角的**正态Q-Q图**帮助我们确定残差是否呈正态分布。**分位数-分位数**（**Q-Q**）图表示一个变量的分位数值与另一个变量的分位数值相对应。看起来异常值（观测值**7**、**9**和**10**）可能导致了假设的违反。**残差与杠杆**图可以告诉我们哪些观测值（如果有），过度影响了模型；换句话说，如果有任何异常值，我们应该关注。这个统计量是**库克距离**或**库克D**，通常认为大于1的值值得进一步检查。
- en: What exactly is further inspection? This is where art meets science. The easy
    way out would be to simply delete the observation, in this case number **9**,
    and redo the model. However, a better option may be to transform the predictor
    and/or the response variables. If we just delete observation **9**, then maybe
    observations **10** and **13** would fall outside the band for greater than 1\.
    I believe that this is where domain expertise can be critical. More times than
    I can count, I have found that exploring and understanding outliers can yield
    valuable insights. When we first examined the previous scatterplot, I pointed
    out the potential outliers and these happen to be observations number **9** and
    number **13**. As an analyst, it would be critical to discuss with the appropriate
    subject matter experts to understand why this is the case. Is it a measurement
    error? Is there a logical explanation for these observations? I certainly don't
    know, but this is an opportunity to increase the value that you bring to an organization.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 究竟什么是进一步检查？这正是艺术与科学的交汇点。最简单的解决办法可能是简单地删除观察结果，在这种情况下是数字**9**，然后重新构建模型。然而，更好的选择可能是转换预测变量和/或响应变量。如果我们仅仅删除观察结果**9**，那么观察结果**10**和**13**可能会超出大于1的带区。我相信这正是领域专业知识可能至关重要的地方。我数不清有多少次发现，探索和理解异常值可以带来宝贵的见解。当我们第一次检查之前的散点图时，我指出了潜在的异常值，而这些恰好是观察结果编号**9**和**13**。作为一个分析师，与适当的主题专家讨论以了解这种情况的原因是至关重要的。这是一个测量错误吗？这些观察结果有逻辑上的解释吗？我当然不知道，但这是一个增加你为组织带来价值的机会。
- en: 'Having said that, we can drill down on the current model by examining, in more
    detail, the **Normal Q-Q** plot. R does not provide confidence intervals to the
    default Q-Q plot, and given our concerns in looking at the base plot, we should
    check the confidence intervals. The `qqPlot()` function of the `car` package automatically
    provides these confidence intervals. Since the `car` package is loaded along with
    the `alr3` package, I can produce the plot with one line of code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们可以通过更详细地检查当前的模型，特别是**正态Q-Q图**来深入挖掘。R默认的Q-Q图不提供置信区间，鉴于我们对基础图的担忧，我们应该检查置信区间。`car`包中的`qqPlot()`函数自动提供这些置信区间。由于`car`包与`alr3`包一起加载，我可以用一行代码生成该图：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/image_02_05.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_02_05.png)'
- en: According to the plot, the residuals are normally distributed. I think this
    can give us the confidence to select the model with all the observations. A clear
    rationale and judgment would be needed to attempt other models. If we could clearly
    reject the assumption of normally distributed errors, then we would probably have
    to examine the variable transformations and/or observation deletion.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图示，残差呈正态分布。我认为这可以让我们有信心选择包含所有观察结果的模型。尝试其他模型需要清晰的推理和判断。如果我们能够明确地拒绝误差正态分布的假设，那么我们可能不得不检查变量转换和/或观察结果删除。
- en: Multivariate linear regression
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多元线性回归
- en: You may be asking yourself whether you will ever have just one predictor variable
    in the real world. That is indeed a fair question and certainly a very rare case
    (time series can be a common exception). Most likely, several, if not many, predictor
    variables or features--as they are affectionately termed in machine learning--will
    have to be included in your model. And with that, let's move on to multivariate
    linear regression and a new business case.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问自己，在现实世界中你是否会只有一个预测变量。这确实是一个公平的问题，当然是一个非常罕见的情况（时间序列可能是一个常见的例外）。很可能会包含几个，如果不是很多预测变量或特征——在机器学习中亲切地称为特征——必须包含在你的模型中。有了这个，让我们继续讨论多元线性回归和新的业务案例。
- en: Business understanding
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 业务理解
- en: In keeping with the water conservation/prediction theme, let's look at another
    dataset in the `alr3` package, appropriately named `water`. During the writing
    of the first edition of this book, the severe drought in Southern California caused
    much alarm. Even the Governor, Jerry Brown, began to take action with a call to
    citizens to reduce water usage by 20 percent. For this exercise, let's say we
    have been commissioned by the state of California to predict water availability.
    The data provided to us contains 43 years of snow precipitation, measured at six
    different sites in the Owens Valley. It also contains a response variable for
    water availability as the stream runoff volume near Bishop, California, which
    feeds into the Owens Valley aqueduct, and eventually the Los Angeles aqueduct.
    Accurate predictions of the stream runoff will allow engineers, planners, and
    policy makers to plan conservation measures more effectively. The model we are
    looking to create will consist of the form *Y = B0 + B1x1 +...Bnxn + e*, where
    the predictor variables (features) can be from 1 to *n*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 按照水资源保护/预测的主题，让我们看看 `alr3` 包中的另一个数据集，它恰当地命名为 `water`。在本书第一版编写期间，南加州的严重干旱引起了极大的恐慌。即使是州长杰里·布朗也开始采取行动，呼吁公民减少
    20% 的用水量。在这个练习中，让我们假设我们已被加利福尼亚州委托预测水资源可用性。提供给我们的数据包含 43 年的雪降水量，测量了奥文谷六个不同地点的数据。它还包含一个响应变量，即加利福尼亚州比什普附近的溪流径流体积，该径流最终流入奥文谷水渠，并最终流入洛杉矶水渠。准确的径流预测将允许工程师、规划者和政策制定者更有效地制定节水措施。我们试图创建的模型将具有以下形式
    *Y = B0 + B1x1 +...Bnxn + e*，其中预测变量（特征）可以是 1 到 *n*。
- en: Data understanding and preparation
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据理解和准备
- en: 'To begin, we will load the dataset named `water` and define the structure of
    the `str()` function as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将加载名为 `water` 的数据集，并定义 `str()` 函数的结构如下：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here we have eight features and one response variable, `BSAAM`. The observations
    start in 1943 and run for 43 consecutive years. Since for this exercise we are
    not concerned with what year the observations occurred in, it makes sense to create
    a new data frame excluding the year vector. This is quite easy to do. With one
    line of code, we can create the new data frame, and then verify that it works
    with the `head()` function:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有八个特征和一个响应变量 `BSAAM`。观测数据从 1943 年开始，连续进行了 43 年。由于在这个练习中我们并不关心观测发生在哪一年，因此创建一个新的数据框，排除年份向量是有意义的。这相当简单。我们只需一行代码就可以创建新的数据框，然后使用
    `head()` 函数来验证它是否工作：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: With all the features being quantitative, it makes sense to look at the correlation
    statistics and then produce a matrix of scatterplots. The correlation coefficient
    or **Pearson's r**, is a measure of both the strength and direction of the linear
    relationship between two variables. The statistic will be a number between -1
    and 1, where -1 is the total negative correlation and +1 is the total positive
    correlation. The calculation of the coefficient is the covariance of the two variables
    divided by the product of their standard deviations. As previously discussed,
    if you square the correlation coefficient, you will end up with R-squared.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有特征都是定量数据，查看相关统计并生成散点图矩阵是有意义的。相关系数或 **Pearson's r** 是衡量两个变量之间线性关系强度和方向的指标。该统计量将是一个介于
    -1 和 1 之间的数字，其中 -1 是完全负相关，+1 是完全正相关。系数的计算是两个变量的协方差除以它们标准差的乘积。正如之前讨论的，如果你平方相关系数，你将得到
    R-squared。
- en: 'There are a number of ways to produce a matrix of correlation plots. Some prefer
    to produce **heatmaps**, but I am a big fan of what is produced with the `corrplot`
    package. It can produce a number of different variations including ellipse, circle,
    square, number, shade, color, and pie. I like the `ellipse` method, but feel free
    to experiment with the others. Let''s load the `corrplot` package, create a correlation
    object using the base `cor()` function, and examine the following results:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以生成相关图矩阵。有些人喜欢生成 **热图**，但我非常喜欢 `corrplot` 包生成的结果。它可以生成多种不同的变体，包括椭圆、圆形、方形、数字、阴影、颜色和饼图。我喜欢椭圆方法，但你可以自由地尝试其他方法。让我们加载
    `corrplot` 包，使用基本的 `cor()` 函数创建一个相关对象，并检查以下结果：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'So, what does this tell us? First of all, the response variable is highly and
    positively correlated with the OP features with `OPBPC` as `0.8857`, `OPRC` as
    `0.9196`, and `OPSLAKE` as `0.9384`. Also note that the AP features are highly
    correlated with each other and the OP features as well. The implication is that
    we may run into the issue of multi-collinearity. The correlation plot matrix provides
    a nice visual of the correlations as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这告诉我们什么呢？首先，响应变量与OP特征高度正相关，其中`OPBPC`为`0.8857`，`OPRC`为`0.9196`，`OPSLAKE`为`0.9384`。此外，请注意，AP特征彼此之间以及与OP特征高度相关。这意味着我们可能会遇到多重共线性问题。相关图矩阵提供了以下相关性的良好视觉表示：
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output of the preceding code snippet is as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段的输出如下：
- en: '![](img/image_02_06.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_02_06.png)'
- en: 'Another popular visual is a scatterplot matrix. This can be called with the
    `pairs()` function. It reinforces what we saw in the correlation plot in the previous
    output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的可视化方法是散点图矩阵。这可以通过`pairs()`函数调用。它加强了我们在前一个输出中的相关图所看到的内容：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output of the preceding code snippet is as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段的输出如下：
- en: '![](img/image_02_07.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_02_07.png)'
- en: Modeling and evaluation
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模与评估
- en: One of the key elements that we will cover here is the very important task of
    feature selection. In this chapter, we will discuss the best subsets regression
    methods stepwise, using the `leaps` package. Later chapters will cover more advanced
    techniques.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里要讨论的关键要素之一是至关重要的特征选择任务。在本章中，我们将讨论使用`leaps`包的逐步最佳子集回归方法。后面的章节将介绍更高级的技术。
- en: '**Forward stepwise selection** starts with a model that has zero features;
    it then adds the features one at a time until all the features are added. A selected
    feature is added in the process that creates a model with the lowest RSS. So in
    theory, the first feature selected should be the one that explains the response
    variable better than any of the others, and so on.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**正向逐步选择**从一个没有特征的模型开始；然后逐个添加特征，直到所有特征都被添加。在创建具有最低RSS的模型的过程中添加了一个选定的特征。所以从理论上讲，第一个选定的特征应该是比其他任何特征更好地解释响应变量的那个，依此类推。'
- en: It is important to note that adding a feature will always decrease RSS and increase
    R-squared, but it will not necessarily improve the model `fit` and interpretability.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，添加一个特征总是会降低RSS并增加R-squared，但它并不一定会改善模型的`拟合度`和可解释性。
- en: '**Backward stepwise regression** begins with all the features in the model
    and removes the least useful, one at a time. A hybrid approach is available where
    the features are added through forward stepwise regression, but the algorithm
    then examines if any features that no longer improve the model fit can be removed.
    Once the model is built, the analyst can examine the output and use various statistics
    to select the features they believe provide the best fit.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向逐步回归**从模型中的所有特征开始，逐个移除最不有用的特征。有一种混合方法，其中特征通过正向逐步回归添加，但算法随后检查是否可以移除任何不再提高模型拟合度的特征。一旦构建了模型，分析师可以检查输出并使用各种统计量来选择他们认为提供最佳拟合的特征。'
- en: It is important to add here that stepwise techniques can suffer from serious
    issues. You can perform a forward stepwise on a dataset, then a backward stepwise,
    and end up with two completely conflicting models. The bottomline is that stepwise
    can produce biased regression coefficients; in other words, they are too large
    and the confidence intervals are too narrow (Tibshirani, 1996).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里补充一点很重要，逐步技术可能会遇到严重的问题。你可以在数据集上执行正向逐步，然后是反向逐步，最终得到两个完全冲突的模型。底线是，逐步可能会产生有偏的回归系数；换句话说，它们太大，置信区间太窄（Tibshirani，1996）。
- en: Best subsets regression can be a satisfactory alternative to the stepwise methods
    for feature selection. In best subsets regression, the algorithm fits a model
    for all the possible feature combinations; so if you have 3 features, 7 models
    will be created. As with stepwise regression, the analyst will need to apply judgment
    or statistical analysis to select the optimal model. Model selection will be the
    key topic in the discussion that follows. As you might have guessed, if your dataset
    has many features, this can be quite a task, and the method does not perform well
    when you have more features than observations (`p` is greater than `n`).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳子集回归可以是特征选择中逐步方法的满意替代。在最佳子集回归中，算法为所有可能的特征组合拟合模型；因此，如果您有3个特征，将创建7个模型。与逐步回归一样，分析师需要应用判断或统计分析来选择最佳模型。模型选择将是以下讨论的关键主题。正如您可能已经猜到的，如果您的数据集具有许多特征，这可能是一项相当大的任务，并且当您有比观察值更多的特征时（`p`大于`n`），该方法表现不佳。
- en: Certainly, these limitations for best subsets do not apply to our task at hand.
    Given its limitations, we will forgo stepwise, but please feel free to give it
    a try. We will begin by loading the `leaps` package. In order that we may see
    how feature selection works, we will first build and examine a model with all
    the features, then drill down with best subsets to select the best fit.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些最佳子集的限制不适用于我们当前的任务。鉴于其局限性，我们将放弃逐步方法，但请随意尝试。我们将首先加载`leaps`包。为了了解特征选择是如何工作的，我们将首先构建并检查包含所有特征的模型，然后通过最佳子集进行深入选择以确定最佳拟合。
- en: 'To build a linear model with all the features, we can again use the `lm()`
    function. It will follow the form: *fit = lm(y ~ x1 + x2 + x3...xn)*. A neat shortcut,
    if you want to include all the features, is to use a period after the tilde symbol
    instead of having to type them all in. For starters, let''s load the `leaps` package
    and build a model with all the features for examination as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建包含所有特征的线性模型，我们再次可以使用`lm()`函数。其形式如下：*fit = lm(y ~ x1 + x2 + x3...xn)*。如果您想包含所有特征，可以使用波浪号后的点号作为快捷方式，而不是必须全部输入。为了开始，让我们加载`leaps`包并构建一个包含所有特征的模型以供检查，如下所示：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Just like univariate regression, we examine the `p-value` on the `F-statistic`
    to verify that at least one of the coefficients is not zero.  Indeed, the `p-value`
    is highly significant. We should also have significant `p-values` for the `OPRC`
    and `OPSLAKE` parameters. Interestingly, `OPBPC` is not significant despite being
    highly correlated with the response variable. In short, when we control for the
    other OP features, `OPBPC` no longer explains any meaningful variation of the
    predictor, which is to say that the feature `OPBPC` adds nothing from a statistical
    standpoint with `OPRC` and `OPSLAKE` in the model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 就像单变量回归一样，我们检查`F-统计量`上的`p-value`以验证至少有一个系数不为零。实际上，`p-value`非常显著。我们还应该有对`OPRC`和`OPSLAKE`参数的显著`p-value`。有趣的是，尽管与响应变量高度相关，但`OPBPC`并不显著。简而言之，当我们控制其他OP特征时，`OPBPC`不再解释预测变量的任何有意义的变化，也就是说，特征`OPBPC`在模型中与`OPRC`和`OPSLAKE`一起不再从统计角度增加任何内容。
- en: 'With the first model built, let''s move on to best subsets. We create the `sub.fit`
    object using the `regsubsets()` function of the `leaps` package as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建了第一个模型之后，让我们继续转向最佳子集。我们使用`leaps`包中的`regsubsets()`函数创建`sub.fit`对象，如下所示：
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then we create the `best.summary` object to examine the models further. As
    with all R objects, you can use the `names()` function to list what outputs are
    available:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建`best.summary`对象以进一步检查模型。与所有R对象一样，您可以使用`names()`函数列出可用的输出：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Other valuable functions in model selection include `which.min()` and `which.max()`.
    These functions will provide the model that has the minimum or maximum value respectively,
    as shown in following code snippet:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择中其他有价值的函数包括`which.min()`和`which.max()`。这些函数将提供具有最小或最大值的模型，如下代码片段所示：
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The code tells us that the model with six features has the smallest RSS, which
    it should have, as that is the maximum number of inputs and more inputs mean a
    lower RSS. An important point here is that adding features will always decrease
    RSS! Furthermore, it will always increase R-squared. We could add a completely
    irrelevant feature such as the number of wins for the Los Angeles Lakers and RSS
    would decrease and R-squared would increase. The amount would likely be miniscule,
    but present nonetheless. As such, we need an effective method to properly select
    the relevant features.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 代码告诉我们，具有六个特征的模型具有最小的RSS，这是应该的，因为那是最大数量的输入，更多的输入意味着更低的RSS。这里的一个重要观点是，添加特征总是会降低RSS！此外，它总是会增加R-squared。我们可以添加一个完全不相关的特征，比如洛杉矶湖人队的胜利次数，RSS会降低，R-squared会增加。这个数量可能非常小，但确实存在。因此，我们需要一个有效的方法来正确选择相关特征。
- en: 'For feature selection, there are four statistical methods that we will talk
    about in this chapter: **Aikake''s Information** **Criterion** (**AIC**), **Mallow''s
    Cp** (**Cp**), **Bayesian Information Criterion** (**BIC**), and the adjusted
    R-squared. With the first three, the goal is to minimize the value of the statistic;
    with adjusted R-squared, the goal is to maximize the statistics value. The purpose
    of these statistics is to create as parsimonious a model as possible, in other
    words, to penalize model complexity.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特征选择，在本章中我们将讨论四种统计方法：**赤池信息量准则**（**AIC**）、**马尔可夫Cp准则**（**Cp**）、**贝叶斯信息准则**（**BIC**）和调整后的R-squared。对于前三种，目标是使统计量的值最小化；对于调整后的R-squared，目标是使统计量的值最大化。这些统计量的目的是创建尽可能简约的模型，换句话说，就是惩罚模型复杂性。
- en: 'The formulation of these four statistics is as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个统计量的公式如下：
- en: '![](img/image_02_08-1.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_02_08-1.png)'
- en: 'In a linear model, `AIC` and `Cp` are proportional to each other, so we will
    only concern ourselves with `Cp`, which follows the output available in the `leaps`
    package. `BIC` tends to select the models with fewer variables than `Cp`, so we
    will compare both. To do so, we can create and analyze two plots side by side.
    Let''s do this for `Cp`, followed by `BIC`,with the help of the following code
    snippet:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性模型中，`AIC`和`Cp`是成比例的，所以我们只关注`Cp`，它遵循`leaps`包中的输出。`BIC`倾向于选择比`Cp`变量更少的模型，因此我们将比较两者。为此，我们可以创建并分析两个并排的图。让我们先对`Cp`进行操作，然后是`BIC`，以下代码片段将帮助我们：
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output of the preceding code snippet is as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个代码片段的输出如下：
- en: '![](img/image_02_08.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_02_08.png)'
- en: 'In the plot on the left-hand side, the model with three features has the lowest
    **cp**. The plot on the right-hand side displays those features that provide the
    lowest **Cp**. The way to read this plot is to select the lowest **Cp** value
    at the top of the y axis, which is **1.2**. Then, move to the right and look at
    the colored blocks corresponding to the x axis. Doing this, we see that **APSLAKE**,
    **OPRC**, and **OPSLAKE** are the features included in this specific model. By
    using the `which.min()` and `which.max()` functions, we can identify how **cp**
    compares to BIC and the adjusted R-squared:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧的图中，具有三个特征的模型具有最低的**cp**。右侧的图显示了提供最低**Cp**的特征。阅读这个图的方法是选择y轴顶部的最低**Cp**值，即**1.2**。然后，向右移动并查看对应于x轴的彩色方块。这样做，我们看到**APSLAKE**、**OPRC**和**OPSLAKE**是包含在这个特定模型中的特征。通过使用`which.min()`和`which.max()`函数，我们可以确定**cp**与BIC和调整后的R-squared的比较：
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In this example, BIC and adjusted R-squared match the **Cp** for the optimal
    model. Now, just as with univariate regression, we need to examine the model and
    test the assumptions. We''ll do this by creating a linear model object and examining
    the plots much as we did earlier, as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，BIC和调整后的R-squared与最优模型的**Cp**相匹配。现在，就像多元回归一样，我们需要检查模型并测试假设。我们将通过创建一个线性模型对象并检查与之前相同的图来完成这项工作，如下所示：
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With the three-feature model, `F-statistic` and all the t-tests have significant
    p-values. Having passed the first test, we can produce our diagnostic plots:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用三特征模型，`F统计量`和所有t检验都具有显著的p值。通过第一次测试后，我们可以生成我们的诊断图：
- en: '[PRE21]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output of the preceding code snippet is as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个代码片段的输出如下：
- en: '![](img/image_02_09.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_02_09.png)'
- en: Looking at the plots, it seems safe to assume that the residuals have a constant
    variance and are normally distributed. There is nothing in the leverage plot that
    would indicate a requirement for further investigation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 观察图表，可以安全地假设残差具有恒定的方差，并且服从正态分布。在杠杆图中没有迹象表明需要进一步调查。
- en: To investigate the issue of collinearity, one can call up the **Variance Inflation
    Factor** (**VIF**) statistic. VIF is the ratio of the variance of a feature's
    coefficient, when fitting the full model, divided by the feature's coefficient
    variance when fitted by itself. The formula is *1 / (1-R²[i])*, where `R2i` is
    the R-squared for our feature of interest, `i`, being regressed by all the other
    features. The minimum value that the VIF can take is 1, which means no collinearity
    at all. There are no hard and fast rules, but in general a VIF value that exceeds
    5 (or some say 10) indicates a problematic amount of collinearity (James, p.101,
    2013). A precise value is difficult to select, because there is no hard statistical
    cut-off point for when multi-collinearity makes your model unacceptable.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调查共线性问题，可以调用**方差膨胀因子**（**VIF**）统计量。VIF是特征系数在拟合完整模型时的方差与特征系数在单独拟合时的方差之比。公式是*1
    / (1-R²[i]*)，其中`R²i`是我们感兴趣的特征`i`的R-squared值，通过所有其他特征进行回归。VIF可以取的最小值是1，这意味着完全没有共线性。没有硬性规则，但一般来说，VIF值超过5（或者有人说10）表明存在问题的共线性量（James，第101页，2013年）。精确值难以选择，因为没有多共线性使你的模型不可接受的硬性统计截止点。
- en: 'The `vif()` function in the `car` package is all that is needed to produce
    the values, as can be seen in the following code snippet:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`car`包中的`vif()`函数是生成这些值所需的所有，如下面的代码片段所示：'
- en: '[PRE22]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'It shouldn''t be surprising that we have a potential collinearity problem with
    **OPRC** and **OPSLAKE** (values greater than 5) based on the correlation analysis.
    A plot of the two variables drives the point home, as seen in the following screenshot:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 根据相关性分析，我们与**OPRC**和**OPSLAKE**（值大于5）存在潜在的共线性问题并不令人惊讶。以下截图中的两个变量的图表清楚地说明了这一点：
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令的输出如下：
- en: '![](img/image_02_10.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_02_10.png)'
- en: 'The simple solution to address collinearity is to drop the variables to remove
    the problem without compromising the predictive ability. If we look at the adjusted
    R-squared from the best subsets, we can see that the two-variable model of APSLAKE
    and OPSLAKE has produced a value of `0.90`, while adding OPRC has only marginally
    increased it to `0.92`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 解决共线性问题的简单方法是通过删除变量来消除问题，同时不损害预测能力。如果我们查看最佳子集的调整R-squared值，我们可以看到，APSLAKE和OPSLAKE的双变量模型产生了`0.90`的值，而添加OPRC仅略微将其增加到`0.92`：
- en: '[PRE24]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s have a look at the two-variable model and test its assumptions:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看双变量模型并测试其假设：
- en: '[PRE25]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output of the preceding code snippet is as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段的输出如下：
- en: '![](img/image_02_11.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_02_11.png)'
- en: 'The model is significant, and the diagnostics do not seem to be a cause for
    concern. This should take care of our collinearity problem as well and we can
    check that using the `vif()` function again:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是显著的，诊断结果似乎没有引起担忧。这应该也解决了我们的共线性问题，我们可以使用`vif()`函数再次检查：
- en: '[PRE26]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As I stated previously, I don''t believe the plot of fits versus residuals
    is of concern, but if you have any doubts you can formally test the assumption
    of the constant variance of errors in R. This test is known as the **Breusch-Pagan**
    (**BP**) test. For this, we need to load the `lmtest` package, and run one line
    of code. The BP test has the null hypothesis that the error variances are zero
    versus the alternative of not zero:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前所述，我认为拟合值与残差的关系图并不令人担忧，但如果你有任何疑问，可以在R中正式测试误差常量方差的假设。这个测试被称为**布雷斯-帕甘**（**BP**）测试。为此，我们需要加载`lmtest`包，并运行一行代码。BP测试的零假设是误差方差为零，备择假设不是零：
- en: '[PRE27]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We do not have evidence to reject the null that implies that the error variances
    are zero because `p-value = 0.9977`. The `BP = 0.0046` value in the summary of
    the test is the chi-squared value.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`p-value = 0.9977`，我们没有证据拒绝零假设，即误差方差为零。测试摘要中的`BP = 0.0046`值是卡方值。
- en: 'All things considered, it appears that the best predictive model is with the
    two features APSLAKE and OPSLAKE. The model can explain 90 percent of the variation
    in the stream runoff volume. To forecast the runoff, it would be equal to 19,145
    (the intercept) plus 1,769 times the measurement at APSLAKE plus 3,690 times the
    measurement at OPSLAKE. A scatterplot of the `Predicted vs. Actual` values can
    be done in base R using the fitted values from the model and the response variable
    values as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有因素，似乎最佳预测模型是包含两个特征APSALA和OPSALA的模型。该模型可以解释流量径流体积变化的90%。为了预测径流，它等于19,145（截距）加上APSALA测量值的1,769倍加上OPSALA测量值的3,690倍。可以使用基础R中的拟合值和响应变量值来绘制`预测值
    vs. 实际值`的散点图，如下所示：
- en: '[PRE28]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output of the preceding code snippet is as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段的输出如下：
- en: '![](img/image_02_12.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_02_12.png)'
- en: 'Although informative, the base graphics of R are not necessarily ready for
    a presentation to be made to business partners. However, we can easily spruce
    up this plot in R. Several packages to improve graphics are available for this
    example, I will use `ggplot2`. Before producing the plot, we must put the predicted
    values into our data frame, `socal.water`. I also want to rename `BSAAM` as `Actual` and
    put in a new vector within the data frame, as shown in the following code snippet:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然信息丰富，但R的基础图形不一定适合向商业伙伴展示。然而，我们可以在R中轻松地美化这个图形。对于这个例子，有几个用于改进图形的包可用，我将使用`ggplot2`。在生成图形之前，我们必须将预测值放入我们的数据框`socal.water`中。我还想将`BSAAM`重命名为`Actual`，并在数据框中放入一个新的向量，如下所示：
- en: '[PRE29]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next we will load the `ggplot2` package and produce a nicer graphic with just
    one line of code:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将加载`ggplot2`包，并使用一行代码生成更美观的图形：
- en: '[PRE30]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/image_02_13.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_02_13.png)'
- en: 'Let''s examine one final model selection technique before moving on. In the
    upcoming chapters, we will be discussing cross-validation at some length. Cross-validation
    is a widely used and effective method of model selection and testing. Why is this
    necessary at all? It comes down to the bias-variance trade-off. Professor Tarpey
    of Wright State University has a nice quote on the subject:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们考察一种最终模型选择技术。在接下来的章节中，我们将详细讨论交叉验证。交叉验证是模型选择和测试中广泛使用且有效的方法。为什么这有必要呢？这归结于偏差-方差权衡。赖特州立大学的Tarpey教授对这个主题有一个很好的引用：
- en: '"Often we use regression models to predict future observations. We can use
    our data to fit the model. However, it is cheating to then access how well the
    model predicts responses using the same data that was used to estimate the model
    - this will tend to give overly optimistic results in terms of how well a model
    is able to predict future observations. If we leave out an observation, fit the
    model and then predict the left out response, then this will give a less biased
    idea of how well the model predicts."'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: “我们经常使用回归模型来预测未来的观测值。我们可以使用我们的数据来拟合模型。然而，使用估计模型时使用的相同数据来评估模型的好坏是作弊行为——这往往会给出过于乐观的结果，关于模型预测未来观测值的能力。如果我们省略一个观测值，拟合模型然后预测省略的响应，这将给出一个更少偏差的想法，关于模型预测能力的好坏。”
- en: 'The cross-validation technique discussed by Professor Tarpey in the preceding
    quote is known as the **Leave-One-Out Cross-Validation** (**LOOCV**). In linear
    models, you can easily perform an LOOCV by examining the **Prediction Error Sum
    of Squares** (**PRESS**) statistic and selecting the model that has the lowest
    value. The R library `MPV` will calculate the statistic for you, as shown in the
    following code:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在前引引用中，Tarpey教授讨论的交叉验证技术被称为**留一法交叉验证**（**LOOCV**）。在线性模型中，你可以通过检查**预测误差平方和**（**PRESS**）统计量，选择具有最低值的模型来轻松执行LOOCV。R库`MPV`会为你计算这个统计量，如下所示：
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'By this statistic alone, we could select our `best.fit` model. However, as
    described previously, I still believe that the more parsimonious model is better
    in this case. You can build a simple function to calculate the statistic on your
    own, taking advantage of some elegant matrix algebra as shown in the following
    code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 仅凭这个统计量，我们就可以选择我们的`最佳拟合`模型。然而，如前所述，我认为在这种情况下，更简约的模型更好。你可以构建一个简单的函数来计算这个统计量，利用以下代码中所示的一些优雅的矩阵代数：
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '"What are `hatvalues`?" you might ask. Well, if we take our linear model *Y
    = B0 + B1x + e*, we can turn this into a matrix notation: *Y = XB + E*. In this
    notation, `Y` remains unchanged, `X` is the matrix of the input values, `B` is
    the coefficient, and `E` represents the errors. This linear model solves for the
    value of `B`. Without going into the painful details of matrix multiplication,
    the regression process yields what is known as a **Hat Matrix**. This matrix maps,
    or as some say projects, the calculated values of your model to the actual values;
    as a result, it captures how influential a specific observation is in your model.
    So, the sum of the squared residuals divided by 1 minus `hatvalues` is the same
    as LOOCV.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: “什么是`hatvalues`？”你可能会问。嗯，如果我们考虑我们的线性模型 *Y = B0 + B1x + e*，我们可以将其转换为矩阵表示：*Y =
    XB + E*。在这个表示法中，`Y`保持不变，`X`是输入值的矩阵，`B`是系数，而`E`代表误差。这个线性模型求解`B`的值。不深入矩阵乘法的痛苦细节，回归过程产生了一个被称为**帽子矩阵**的结果。这个矩阵映射，或者说有些人说是投影，你的模型计算值到实际值；因此，它捕捉了特定观察在你模型中的影响力。所以，残差平方和除以1减去`hatvalues`与LOOCV相同。
- en: Other linear model considerations
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他线性模型考虑因素
- en: Before moving on, there are two additional linear model topics that we need
    to discuss. The first is the inclusion of a qualitative feature, and the second
    is an interaction term; both are explained in the following sections.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们需要讨论两个额外的线性模型主题。第一个是包含定性特征，第二个是交互项；这两个主题将在以下章节中解释。
- en: Qualitative features
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定性特征
- en: A qualitative feature, also referred to as a factor, can take on two or more
    levels such as Male/Female or Bad/Neutral/Good. If we have a feature with two
    levels, say gender, then we can create what is known as an indicator or dummy
    feature, arbitrarily assigning one level as `0` and the other as `1`. If we create
    a model with just the indicator, our linear model would still follow the same
    formulation as before, that is, *Y = B0 + B1x + e*. If we code the feature as
    male being equal to 0 and female equal to 1, then the expectation for male would
    just be the intercept *B0*, while for female it would be *B0 + B1x*. In the situation
    where you have more than two levels of the feature, you can create n-1 indicators;
    so, for three levels you would have two indicators. If you created as many indicators
    as levels, you would fall into the dummy variable trap, which results in perfect
    multi-collinearity.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一个定性特征，也称为因子，可以具有两个或更多级别，例如男性/女性或差/中性/好。如果我们有一个具有两个级别的特征，比如说性别，那么我们可以创建一个所谓的指示符或虚拟特征，任意地将一个级别分配为`0`，另一个分配为`1`。如果我们只创建一个指示符模型，我们的线性模型仍然遵循之前的公式，即*Y
    = B0 + B1x + e*。如果我们把特征编码为男性等于0，女性等于1，那么男性的期望值就只是截距*B0*，而女性的期望值就是*B0 + B1x*。在你有特征超过两个级别的情况下，你可以创建n-1个指示符；所以，对于三个级别，你会有两个指示符。如果你创建了与级别一样多的指示符，你就会陷入虚拟变量陷阱，这会导致完美的多重共线性。
- en: 'We can examine a simple example to learn how to interpret the output. Let''s
    load the `ISLR` package and build a model with the `Carseats` dataset using the
    following code snippet:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个简单的例子来学习如何解释输出。让我们加载`ISLR`包，并使用以下代码片段使用`Carseats`数据集构建一个模型：
- en: '[PRE33]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'For this example, we will predict the sales of `Carseats` using just `Advertising`,
    a quantitative feature and the qualitative feature `ShelveLoc`, which is a factor
    of three levels: `Bad`, `Good`, and `Medium`. With factors, R will automatically
    code the indicators for the analysis. We build and analyze the model as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将仅使用`Advertising`（广告），一个定量特征和定性特征`ShelveLoc`（货架位置），它是一个有三个级别的因子：`Bad`（差），`Good`（好）和`Medium`（中等）来预测`Carseats`的销售。对于因子，R会自动为分析编码指示符。我们按照以下方式构建和分析模型：
- en: '[PRE34]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If the shelving location is good, the estimate of sales is almost double of
    that when the location is bad, given an intercept of `4.89662`. To see how R codes
    the indicator features, you can use the `contrasts()` function:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果货架位置是好的，给定截距`4.89662`，销售估计几乎是位置差时的两倍。要查看R如何编码指示符特征，你可以使用`contrasts()`函数：
- en: '[PRE35]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Interaction terms
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交互项
- en: 'Interaction terms are similarly easy to code in R. Two features interact if
    the effect on the prediction of one feature depends on the value of the other
    feature. This would follow the formulation, *Y = B0 + B1x + B2x + B1B2x + e*.
    An example is available in the `MASS` package with the `Boston` dataset. The response
    is the median home value, which is `medv` in the output. We will use two features:
    the percentage of homes with a low socioeconomic status, which is termed `lstat`,
    and the age of the home in years, which is termed `age` in the following output:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中，交互项同样容易编码。如果某个特征对预测的影响取决于另一个特征的价值，则两个特征之间存在交互。这遵循以下公式：*Y = B0 + B1x + B2x
    + B1B2x + e*。一个例子可以在`MASS`包中的`Boston`数据集中找到。响应是中值房屋价值，输出中的`medv`。我们将使用两个特征：低社会经济地位房屋的百分比，称为`lstat`，以及房屋的年龄（以年为单位），在以下输出中称为`age`：
- en: '[PRE36]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Using *feature1*feature2* with the `lm()` function in the code puts both the
    features as well as their interaction term in the model, as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中使用`lm()`函数和*feature1*feature2*将两个特征及其交互项都放入模型中，如下所示：
- en: '[PRE37]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Examining the output, we can see that, while the socioeconomic status is a highly
    predictive feature, the age of the home is not. However, the two features have
    a significant interaction to positively explain the home value.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 检查输出，我们可以看到，尽管社会经济地位是一个高度预测的特征，但房屋的年龄并不是。然而，这两个特征有一个显著的交互作用，可以正解释房屋价值。
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In the context of machine learning, we train a model and test it to predict
    or forecast an outcome. In this chapter, we had an in-depth look at the simple
    yet extremely effective method of linear regression to predict a quantitative
    response. Later chapters will cover more advanced techniques, but many of them
    are mere extensions of what we have learned in this chapter. We discussed the
    problem of not visually inspecting the dataset and simply relying on the statistics
    to guide you in model selection.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下，我们训练一个模型并对其进行测试，以预测或预报一个结果。在本章中，我们深入探讨了线性回归这一简单而极其有效的方法，用于预测定量响应。后续章节将涵盖更多高级技术，但其中许多只是本章所学内容的扩展。我们讨论了不直观检查数据集，而仅仅依赖统计数据来指导模型选择的问题。
- en: With just a few lines of code, you can make powerful and insightful predictions
    to support decision-making. Not only is it simple and effective, but also you
    can include quantitative variables and interaction terms among the features. Indeed,
    this is a method that anyone delving into the world of machine learning must master.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 只需几行代码，你就可以做出强大而有洞察力的预测，以支持决策。这不仅简单有效，而且你还可以包括特征之间的定量变量和交互项。实际上，这是任何深入研究机器学习世界的人都必须掌握的方法。
