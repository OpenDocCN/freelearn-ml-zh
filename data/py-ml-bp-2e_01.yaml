- en: The Python Machine Learning Ecosystem
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 机器学习生态系统
- en: Machine learning is rapidly changing our world. As the centerpiece of artificial
    intelligence, it is difficult to go a day without reading how it will lead us
    into either a techno-utopia along the lines of the Singularity, or into some sort
    of global Blade Runner-esque nightmare scenario. While pundits may enjoy discussing
    these hyperbolic futures, the more mundane reality is that machine learning is
    rapidly becoming a fixture of our daily lives. Through subtle but progressive
    improvements in how we interact with computers and the world around us, machine
    learning is progressively making our lives better.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习正在迅速改变我们的世界。作为人工智能的核心，几乎每天我们都会看到有关它将引领我们进入类似奇点的技术乌托邦，或是某种全球性《银翼杀手》式噩梦场景的讨论。虽然评论员可能喜欢讨论这些夸张的未来，但更为平凡的现实是，机器学习正迅速成为我们日常生活的一部分。通过在我们与计算机及周围世界互动方式中的微妙但逐步改进，机器学习正在逐步改善我们的生活。
- en: If you shop at online retailers such as Amazon.com, use streaming music or movie
    services such as Spotify or Netflix, or have even just done a Google search, you
    have encountered an application that utilizes machine learning. These services
    collect vast amounts of data—much of it from their users—that is used to build
    models that improve the user experience.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在亚马逊等在线零售商购物，使用 Spotify 或 Netflix 等流媒体音乐或电影服务，或者甚至只是进行过 Google 搜索，那么你已经遇到了一个利用机器学习的应用程序。这些服务收集了大量数据——其中很多来自用户——并用这些数据来构建改善用户体验的模型。
- en: It's an ideal time to dive into developing machine learning applications, and,
    as you will discover, Python is an ideal choice with which to develop them. Python
    has a deep and active developer community, many with roots in the scientific community.
    This heritage has provided Python with an unparalleled array of libraries for
    scientific computing. In this book, we will discuss and use a number of the libraries
    included in this **Python Scientific Stack**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是深入开发机器学习应用程序的理想时机，正如你将发现的那样，Python 是开发机器学习应用程序的理想选择。Python 拥有一个深厚且活跃的开发者社区，其中许多人根植于科学界。这一遗产为
    Python 提供了无与伦比的科学计算库阵列。在本书中，我们将讨论并使用 **Python 科学栈** 中的若干库。
- en: In the chapters that follow, we'll learn how to build a wide variety of machine
    learning applications step by step. Before we begin in earnest though, we'll spend
    the remainder of this chapter discussing the features of these key libraries and
    how to prepare your environment to best utilize them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将一步一步地学习如何构建各种机器学习应用程序。然而，在我们正式开始之前，本章的其余部分将讨论这些关键库的特性，以及如何准备你的环境以最佳方式利用它们。
- en: 'These are the topics that will be covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The data science/machine learning workflow
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学/机器学习工作流
- en: Libraries for each stage of the workflow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个工作流阶段的库
- en: Setting up your environment
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置你的环境
- en: Data science/machine learning workflow
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学/机器学习工作流
- en: 'Building machine learning applications, while similar in many respects to the
    standard engineering paradigm, differs in one crucial aspect: the need to work
    with data as a raw material. The success of your project will, in large part,
    depend on the quality of the data you acquire, as well as your handling of that
    data. And because working with data falls into the domain of data science, it
    is helpful to understand the data science workflow:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 构建机器学习应用程序在许多方面与标准工程范式类似，但有一个关键的不同点：需要将数据作为原材料来处理。你的项目成功与否，主要取决于你获得的数据质量，以及你对这些数据的处理方式。而且，因为数据处理属于数据科学领域，理解数据科学工作流会很有帮助：
- en: '![](img/59e5ab55-ba98-4e7c-8779-64d5559fd999.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/59e5ab55-ba98-4e7c-8779-64d5559fd999.png)'
- en: Data science workflow
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学工作流
- en: 'The process involves these six steps in the following order:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程包括以下六个步骤，按顺序进行：
- en: Acquisition
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取
- en: Inspection
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查
- en: Preparation
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备
- en: Modeling
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建模
- en: Evaluation
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估
- en: Deployment
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署
- en: Frequently, there is a need to circle back to prior steps, such as when inspecting
    and preparing the data, or when evaluating and modeling, but the process at a
    high level can be as described in the preceding list.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，需要回到前一步骤，比如检查和准备数据，或评估和建模，但在高层次上，这一过程可以按前述列表描述的顺序进行。
- en: Let's now discuss each step in detail.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来详细讨论每一步。
- en: Acquisition
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取
- en: Data for machine learning applications can come from any number of sources;
    it may be emailed to you as a CSV file, it may come from pulling down server logs,
    or it may require building a custom web scraper. Data is also likely to exist
    in any number of formats. In most cases, you will be dealing with text-based data,
    but, as we'll see, machine learning applications may just as easily be built that
    utilize images or even video files. Regardless of the format, once you have secured
    the data, it is crucial that you understand what's in the data, as well as what
    isn't.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习应用的数据可以来自许多来源；它可能通过电子邮件发送给你作为CSV文件，可能来自提取服务器日志，或者可能需要构建一个自定义的网页爬虫。数据也可能以各种格式存在。在大多数情况下，你将处理基于文本的数据，但正如我们将看到的，机器学习应用也可以轻松地处理图像或视频文件。无论数据格式如何，一旦你获得了数据，理解数据中包含的内容以及未包含的内容至关重要。
- en: Inspection
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查
- en: Once you have acquired your data, the next step is to inspect it. The primary
    goal at this stage is to sanity check the data, and the best way to accomplish
    this is to look for things that are either impossible or highly unlikely. As an
    example, if the data has a unique identifier, check to see that there is indeed
    only one; if the data is price-based, check that it is always positive; and whatever
    the data type, check the most extreme cases. Do they make sense? A good practice
    is to run some simple statistical tests on the data, and visualize it. The outcome
    of your models is only as good as the data you put in, so it is crucial to get
    this step right.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你获取了数据，下一步就是检查它。在这一阶段的主要目标是对数据进行合理性检查，而完成此目标的最佳方法是寻找那些不可能或极不可能出现的情况。例如，如果数据中有唯一标识符，检查是否确实只有一个；如果数据是基于价格的，检查它是否始终为正数；无论数据类型如何，检查最极端的情况。它们是否有意义？一个好的做法是对数据进行一些简单的统计测试，并进行可视化。你的模型结果好坏取决于输入的数据，因此，确保这个步骤的正确性至关重要。
- en: Preparation
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备
- en: When you are confident you have your data in order, next you will need to prepare
    it by placing it in a format that is amenable to modeling. This stage encompasses
    a number of processes, such as filtering, aggregating, imputing, and transforming.
    The type of actions you need to take will be highly dependent on the type of data
    you're working with, as well as the libraries and algorithms you will be utilizing.
    For example, if you are working with natural language-based texts, the transformations
    required will be very different from those required for time-series data. We'll
    see a number of examples of these types of transformations throughout the book.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当你确信数据已经整理好后，接下来需要将数据准备成适合建模的格式。这个阶段包括多个过程，如过滤、聚合、填充和转换。你需要采取的操作将高度依赖于你所处理的数据类型，以及你将使用的库和算法。例如，如果你正在处理基于自然语言的文本，所需的转换将与处理时间序列数据所需的转换大不相同。在本书中，我们将看到这些类型转换的多个实例。
- en: Modeling
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模
- en: 'Once the data preparation is complete, the next phase is modeling. Here, you
    will be selecting an appropriate algorithm and using the data to train your model.
    There are a number of best practices to adhere to during this stage, and we will
    discuss them in detail, but the basic steps involve splitting your data into training,
    testing, and validation sets. This splitting up of the data may seem illogical—especially
    when more data typically yields better models—but as we''ll see, doing this allows
    us to get better feedback on how the model will perform in the real world, and
    prevents us from the cardinal sin of modeling: overfitting. We will talk more
    about this in later chapters.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备完成后，下一阶段是建模。在这里，你将选择一个合适的算法，并利用数据来训练模型。在这一阶段有许多最佳实践需要遵循，我们将详细讨论它们，但基本步骤包括将数据分为训练集、测试集和验证集。将数据拆分可能看起来不合逻辑——尤其是当更多数据通常会带来更好的模型时——但正如我们将看到的那样，这样做可以让我们更好地评估模型在现实世界中的表现，并防止我们犯下建模中的最大错误：过拟合。我们将在后面的章节中详细讨论这一点。
- en: Evaluation
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: So, now you've got a shiny new model, but exactly how good is that model? This
    is the question that the evaluation phase seeks to answer. There are a number
    of ways to measure the performance of a model, and again it is largely dependent
    on the type of data you are working with and the type of model used, but on the
    whole, we are seeking to answer the question of how close the model's predictions
    are to the actual value. There is an array of confusing sounding terms, such as
    root mean-square error, or Euclidean distance, or F1 score. But in the end, they
    are all just a measure of distance between the actual prediction and the estimated
    prediction.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了一个崭新的模型，但这个模型到底有多好呢？这是评估阶段试图回答的问题。评估模型性能有很多方法，这在很大程度上取决于你使用的数据类型和模型类型，但总的来说，我们试图回答的问题是：模型的预测与实际值有多接近。这里有一些听起来令人困惑的术语，比如均方根误差、欧几里得距离或F1得分。但最终，它们都只是衡量实际预测与估算预测之间距离的标准。
- en: Deployment
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: Once you are comfortable with the performance of your model, you'll want to
    deploy it. This can take a number of forms depending on the use case, but common
    scenarios include utilization as a feature within another larger application,
    a bespoke web application, or even just a simple cron job.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你对模型的性能感到满意，你就可能想要部署它。根据使用场景，部署可能有很多种形式，常见的情况包括作为另一个更大应用程序中的一个功能、定制的Web应用程序，甚至只是一个简单的定时任务。
- en: Python libraries and functions for each stage of the data science workflow
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学工作流程中每个阶段的Python库和函数
- en: Now that you have an understanding of each step in the data science workflow,
    we'll take a look at a selection of useful Python libraries and functions within
    those libraries for each step.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了数据科学工作流程中的每个步骤，我们将看看一些在每个步骤中有用的Python库和库中的函数。
- en: Acquisition
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取
- en: Since one of the more common ways to access data is through a RESTful API, one
    library that you'll want to be aware of is the Python Requests library, [http://www.python-requests.org/en/latest/](http://www.python-requests.org/en/latest/).
    Dubbed *HTTP for humans*, it makes interacting with APIs a clean and simple experience.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于访问数据的常见方式之一是通过RESTful API，你需要了解的一个库是Python Requests库，[http://www.python-requests.org/en/latest/](http://www.python-requests.org/en/latest/)。被称为*人类的HTTP*，它使与API的交互变得简洁而简单。
- en: 'Let''s take a look at a sample interaction, using `requests` to pull down data
    from GitHub''s API. Here, we will make a call to the API and request a list of
    starred repositories for a user:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例交互来看看，使用`requests`从GitHub的API中获取数据。在这里，我们将调用API并请求某个用户的收藏仓库列表：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will return a JSON of all the repositories the user has starred, along
    with attributes about each. Here is a snippet of the output for the preceding
    call:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回该用户所有收藏仓库的JSON数据，并附带每个仓库的属性。以下是前述调用的输出片段：
- en: '![](img/b91dee33-9507-4fe6-bc61-6f7ccf15254c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b91dee33-9507-4fe6-bc61-6f7ccf15254c.png)'
- en: Output snippet when we return a JSON of all the repositories
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 返回所有仓库的JSON时的输出片段
- en: The `requests` library has an amazing number of features—far too many to cover
    here, but I do suggest you check out the documentation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`requests`库功能强大—它的功能多到这里无法一一介绍，但我建议你查看文档以了解更多。'
- en: Inspection
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查
- en: Because inspecting your data is such a critical step in the development of machine
    learning applications, we'll now take an in-depth look at several libraries that
    will serve you well in this task.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于检查数据在机器学习应用开发中的关键作用，我们将深入探讨几种能帮助你完成这项任务的优秀库。
- en: The Jupyter Notebook
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jupyter Notebook
- en: 'There are a number of libraries that will make the data inspection process
    easier. The first is Jupyter Notebook with IPython ([http://ipython.org/](http://ipython.org/)).
    This is a fully-fledged, interactive computing environment, and it is ideal for
    data exploration. Unlike most development environments, Jupyter Notebook is a
    web-based frontend (to the IPython kernel) that is divided into individual code
    blocks or cells. Cells can be run individually or all at once, depending on the
    need. This allows the developer to run a scenario, see the output, then step back
    through the code, make adjustments, and see the resulting changes—all without
    leaving the notebook. Here is a sample interaction in the Jupyter Notebook:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多库可以简化数据检查过程。第一个是 Jupyter Notebook 配合 IPython ([http://ipython.org/](http://ipython.org/))。这是一种完全成熟的互动计算环境，非常适合数据探索。与大多数开发环境不同，Jupyter
    Notebook 是一个基于网页的前端（与 IPython 内核连接），它将内容划分为独立的代码块或单元格。单元格可以根据需要单独运行，也可以一次性运行所有单元格。这使得开发人员可以运行一个场景，查看输出，然后回过头调整代码，看到相应的变化——所有这些都无需离开笔记本。以下是在
    Jupyter Notebook 中的一个示例交互：
- en: '![](img/0f2d7643-e871-491e-ada7-57402abffdf7.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f2d7643-e871-491e-ada7-57402abffdf7.png)'
- en: Sample interaction in the Jupyter Notebook
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter Notebook 中的示例交互
- en: 'You will notice that we have done a number of things here and have interacted
    with not only the IPython backend, but the terminal shell as well. Here, I have
    imported the Python `os` library and made a call to find the current working directory
    (cell #2), which you can see is the output below my input code cell. I then changed
    directories using the `os` library in cell #3, but stopped utilizing the `os`
    library and began using Linux-based commands in cell #4\. This is done by adding
    the `!` prepend to the cell. In cell #6, you can see that I was even able to save
    the shell output to a Python variable (`file_two`). This is a great feature that
    makes file operations a simple task.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '你会注意到，在这里我们做了很多事情，不仅与 IPython 后端交互，还与终端 shell 进行了互动。在这里，我导入了 Python 的 `os`
    库，并调用它来查找当前的工作目录（单元格 #2），你可以看到输出在我的输入代码单元下方。然后我在单元格 #3 使用 `os` 库更改了目录，但在单元格 #4
    中停止使用 `os` 库，转而开始使用基于 Linux 的命令。这是通过在单元格前加上 `!` 来实现的。在单元格 #6 中，你可以看到我甚至将 shell
    输出保存到了 Python 变量 (`file_two`) 中。这是一个非常棒的功能，它使得文件操作变得简单。'
- en: Note that the results would obviously differ slightly on your machine, since
    this displays information on the user under which it runs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，结果在你的机器上会有所不同，因为它显示的是运行时的用户信息。
- en: Now, let's take a look at some simple data operations using the notebook. This
    will also be our first introduction to another indispensable library, pandas.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看使用笔记本进行的一些简单数据操作。这也是我们首次接触到另一个不可或缺的库——pandas。
- en: Pandas
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pandas
- en: 'Pandas is a remarkable tool for data analysis that aims to be the most powerful
    and flexible open source data analysis/manipulation tool available in any language. And,
    as you will soon see, if it doesn''t already live up to this claim, it can''t
    be too far off. Let''s now take a look:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 是一个出色的数据分析工具，旨在成为任何语言中最强大、最灵活的开源数据分析/处理工具。正如你很快会看到的那样，如果它目前还没有完全实现这一目标，那也不远了。现在让我们来看看：
- en: '![](img/9656705b-7e70-4d66-b324-9e2deeed7cb1.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9656705b-7e70-4d66-b324-9e2deeed7cb1.png)'
- en: Importing the iris dataset
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 导入鸢尾花数据集
- en: You can see from the preceding screenshot that I have imported a classic machine
    learning dataset, the `iris` dataset (also available at [https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)),
    using scikit-learn, a library we'll examine in detail later. I then passed the
    data into a `pandas` DataFrame, making sure to assign the column headers. One
    DataFrame contains flower measurement data, and the other DataFrame contains a
    number that represents the `iris` species. This is coded `0`, `1`, and `2` for
    `setosa`, `versicolor`, and `virginica` respectively. I then concatenated the
    two DataFrames.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的截图中你可以看到，我已经导入了一个经典的机器学习数据集——`iris` 数据集（也可以在 [https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)
    找到），使用了 scikit-learn 这个我们稍后会详细讨论的库。接着，我将数据传入了一个 `pandas` DataFrame，并确保为其分配了列标题。一个
    DataFrame 包含了花卉测量数据，另一个 DataFrame 包含了代表 `iris` 物种的数字。这些数字分别编码为 `0`、`1` 和 `2`，分别对应
    `setosa`、`versicolor` 和 `virginica`。接着，我将两个 DataFrame 合并在一起。
- en: For working with datasets that will fit on a single machine, pandas is the ultimate
    tool; you can think of it a bit like Excel on steroids. And, like the popular
    spreadsheet program, the basic units of operation are columns and rows of data
    that form tables. In the terminology of pandas, columns of data are series and
    the table is a DataFrame.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于能够在单台机器上运行的数据集，pandas 是终极工具；你可以把它想象成是超级增强版的 Excel。而且，像流行的电子表格程序一样，操作的基本单元是数据的列和行，它们构成了表格。在
    pandas 的术语中，数据列是 Series，表格是 DataFrame。
- en: 'Using the same `iris` DataFrame we loaded previously, let''s now take a look
    at a few common operations, including the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们之前加载的相同的 `iris` DataFrame，接下来我们来看几个常见操作，包括以下内容：
- en: '![](img/10aa8f4a-04fb-4db6-a38b-bc8c4d1fd422.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10aa8f4a-04fb-4db6-a38b-bc8c4d1fd422.png)'
- en: The first action was just to use the `.head()` command to get the first five
    rows. The second command was to select a single column from the DataFrame by referencing
    it by its column name. Another way we perform this **data slicing** is to use
    the `.iloc[row,column]` or `.loc[row,column]` notation. The former slices data
    using a numeric index for the columns and rows (positional indexing), while the
    latter uses a numeric index for the rows, but allows for using named columns (label-based
    indexing).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个操作只是使用 `.head()` 命令来获取前五行。第二个命令是通过列名称引用来从 DataFrame 中选择单列。我们进行这种**数据切片**的另一种方式是使用
    `.iloc[row,column]` 或 `.loc[row,column]` 语法。前者使用数字索引对列和行进行切片（位置索引），而后者对行使用数字索引，但允许使用命名列（基于标签的索引）。
- en: 'Let''s select the first two columns and the first four rows using the `.iloc`
    notation. We''ll then look at the `.loc` notation:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `.iloc` 语法选择前两列和前四行。接着，我们再看一下 `.loc` 语法：
- en: '![](img/3a7b24de-b62e-4118-afb7-aa8c649ed7cb.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a7b24de-b62e-4118-afb7-aa8c649ed7cb.png)'
- en: Using the `.iloc` notation and the Python list slicing syntax, we were able
    to select a slice of this DataFrame.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `.iloc` 语法和 Python 列表切片语法，我们能够选择这个 DataFrame 的一个切片。
- en: 'Now, let''s try something more advanced. We''ll use a list iterator to select
    just the width feature columns:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试一些更高级的内容。我们将使用列表迭代器来选择仅包含宽度特征的列：
- en: '![](img/0c9b8e0e-6855-45ff-8c6a-521b129482d3.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c9b8e0e-6855-45ff-8c6a-521b129482d3.png)'
- en: What we have done here is create a list that is a subset of all columns. `df.columns`
    returns a list of all columns, and our iteration uses a conditional statement
    to select only those with `width` in the title. Obviously, in this situation,
    we could have just as easily typed out the columns we wanted into a list, but
    this gives you a sense of the power available when dealing with much larger datasets.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做的是创建一个包含所有列子集的列表。`df.columns` 返回所有列的列表，我们的迭代使用条件语句仅选择标题中包含 `width` 的列。显然，在这种情况下，我们本来也可以直接将想要的列名列出，但这给了你一个在处理更大数据集时可用的强大功能的感觉。
- en: 'We''ve seen how to select slices based on their position within the DataFrame,
    but let''s now look at another method to select data. This time, we will select
    a subset of the data based upon satisfying conditions that we specify:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何根据位置选择切片，但现在让我们看看另一种选择数据的方法。这一次，我们将根据满足我们指定的条件来选择数据的一个子集：
- en: 'Let''s now see the unique list of `species` available, and select just one
    of those:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下可用的`species`独特列表，并选择其中一个：
- en: '![](img/a2244c3e-0207-47d4-8f7c-4ce31f2fa31f.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2244c3e-0207-47d4-8f7c-4ce31f2fa31f.png)'
- en: 'In the far-right column, you will notice that our DataFrame only contains data
    for the `Iris-virginica` species (represented by the `2`) now. In fact, the size
    of the DataFrame is now 50 rows, down from the original 150 rows:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最右侧的列中，你会注意到我们的 DataFrame 现在只包含 `Iris-virginica` 物种的数据（由 `2` 表示）。实际上，DataFrame
    的大小现在是 50 行，比原始的 150 行少了：
- en: '![](img/0039c8de-1b67-47a5-8893-df42c4cf4a49.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0039c8de-1b67-47a5-8893-df42c4cf4a49.png)'
- en: 'You can also see that the index on the left retains the original row numbers.
    If we wanted to save just this data, we could save it as a new DataFrame, and
    reset the index as shown in the following diagram:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以看到，左侧的索引保留了原始的行号。如果我们想仅保存这些数据，我们可以将其保存为一个新的 DataFrame，并按以下图示重置索引：
- en: '![](img/1081ae3b-92d2-4009-813b-461fbf0a02e7.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1081ae3b-92d2-4009-813b-461fbf0a02e7.png)'
- en: 'We have selected data by placing a condition on one column; let''s now add
    more conditions. We''ll go back to our original DataFrame and add two conditions:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过在一列上设置条件来选择数据；现在，让我们添加更多的条件。我们将返回到原始的 DataFrame，并添加两个条件：
- en: '![](img/ace6fa80-3595-4244-9956-238b2050fd54.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ace6fa80-3595-4244-9956-238b2050fd54.png)'
- en: The DataFrame now only includes data from the `virginica` species with a petal
    width greater than `2.2`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，DataFrame 只包含 `virginica` 物种且花瓣宽度大于 `2.2` 的数据。
- en: 'Let''s now move on to using pandas to get some quick descriptive statistics
    from our `iris` dataset:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 pandas 来快速获取 `iris` 数据集的一些描述性统计数据：
- en: '![](img/4cc98651-0e08-4d8a-9e02-b38cf7ddf095.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cc98651-0e08-4d8a-9e02-b38cf7ddf095.png)'
- en: 'With a call to the `.describe()` function, I have received a breakdown of the
    descriptive statistics for each of the relevant columns. (Notice that species
    was automatically removed as it is not relevant for this.) I could also pass in
    my own percentiles if I wanted more granular information:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用 `.describe()` 函数，我得到了相关列的描述性统计信息的详细数据。（注意，物种列被自动移除，因为它与此无关。）如果需要更详细的信息，我也可以传入自己的百分位数：
- en: '![](img/434fcf61-d65d-46b1-86fc-fa823575b981.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/434fcf61-d65d-46b1-86fc-fa823575b981.png)'
- en: 'Next, let''s check whether there is any correlation between these features.
    That can be done by calling `.corr()` on our DataFrame:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们检查这些特征之间是否存在任何相关性。这可以通过在 DataFrame 上调用 `.corr()` 来完成：
- en: '![](img/dec8063d-60fd-4bd5-8fe8-3cddd2a9917e.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dec8063d-60fd-4bd5-8fe8-3cddd2a9917e.png)'
- en: The default returns the **Pearson correlation coefficient** for each row-column
    pair. This can be switched to **Kendall's Tau** or **Spearman's rank correlation
    coefficient** by passing in a method argument (for example, `.corr(method="spearman")`
    or `.corr(method="kendall")`).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下返回每一行列对的**皮尔逊相关系数**。你可以通过传入方法参数（例如 `.corr(method="spearman")` 或 `.corr(method="kendall")`）将其切换为**肯德尔τ系数**或**斯皮尔曼等级相关系数**。
- en: Visualization
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化
- en: So far, we have seen how to select portions of a DataFrame and how to get summary
    statistics from our data, but let's now move on to learning how to visually inspect
    the data. But first, why even bother with visual inspection? Let's see an example
    to understand why.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何从 DataFrame 中选择部分数据，并如何获取数据的总结统计信息，但接下来我们要学习如何进行数据的可视化检查。但首先，为什么要进行可视化检查呢？我们来看一个例子，理解一下为什么这么做。
- en: 'Here is the summary statistics for four distinct series of *x* and *y* values:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是四个不同的 *x* 和 *y* 值系列的总结统计数据：
- en: '| **Series of *x* and *y*** | **Values** |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| **x 和 *y* 的系列** | **值** |'
- en: '| Mean of *x* | 9 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| *x* 的均值 | 9 |'
- en: '| Mean of *y* | 7.5 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| *y* 的均值 | 7.5 |'
- en: '| Sample variance of *x* | 11 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| *x* 的样本方差 | 11 |'
- en: '| Sample variance of *y* | 4.1 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| *y* 的样本方差 | 4.1 |'
- en: '| Correlation between *x* and *y* | 0.816 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| *x* 和 *y* 之间的相关性 | 0.816 |'
- en: '| Regression line | *y* = 3.00 + 0.500*x* |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 回归线 | *y* = 3.00 + 0.500 *x* |'
- en: 'Based on the series having identical summary statistics, you might assume that
    these series would appear visually similar. You would, of course, be wrong. Very
    wrong. The four series are part of **Anscombe''s quartet**, and they were deliberately
    created to illustrate the importance of visual data inspection. Each series is
    plotted as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些系列具有相同的总结统计数据，你可能会假设这些系列在视觉上会相似。当然，你会错的，非常错。这四个系列是**安斯科姆四重奏**的一部分，它们故意被创建用来说明数据可视化检查的重要性。每个系列的图形如下：
- en: '![](img/8a039b4d-bd6a-41a3-be70-72f761c00ab1.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a039b4d-bd6a-41a3-be70-72f761c00ab1.png)'
- en: Clearly, we would not treat these datasets as identical after having visualized
    them. So, now that we understand the importance of visualization, let's take a
    look at a pair of useful Python libraries for this.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，在可视化这些数据集后，我们不会将它们视为相同的数据集。因此，现在我们已经理解了可视化的重要性，接下来我们来看看一对有用的 Python 库。
- en: The matplotlib library
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: matplotlib 库
- en: 'The first library we''ll take a look at is `matplotlib`. The `matplotlib` library
    is the center of the Python plotting library universe. Originally created to emulate
    the plotting functionality of MATLAB, it grew into a fully-featured library in
    its own right with an enormous range of functionality. If you have not come from
    a MATLAB background, it can be hard to understand how all the pieces work together
    to create the graphs you see. I''ll do my best to break down the pieces into logical
    components so you can get up to speed quickly. But before diving into `matplotlib`
    in full, let''s set up our Jupyter Notebook to allow us to see our graphs inline.
    To do this, add the following lines to your `import` statements:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要看的库是`matplotlib`。`matplotlib`库是Python绘图库的核心。最初为了模拟MATLAB的绘图功能而创建，它逐渐发展成了一个功能完备的库，拥有广泛的功能。如果你没有MATLAB背景，可能很难理解所有组件如何协同工作以生成你看到的图表。我会尽力将这些部分拆解成逻辑组件，以帮助你快速上手。在全面深入`matplotlib`之前，让我们先设置我们的Jupyter
    Notebook，以便在其中查看图表。为此，请在你的`import`语句中添加以下几行：
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first line imports `matplotlib`, the second line sets the styling to approximate
    R's `ggplot` library (requires matplotlib 1.41 or greater), and the last line
    sets the plots so that they are visible within the notebook.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行导入了`matplotlib`，第二行将样式设置为近似R的`ggplot`库（需要matplotlib 1.41或更高版本），最后一行设置图表使其能够在笔记本中显示。
- en: 'Now, let''s generate our first graph using our `iris` dataset:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用我们的`iris`数据集生成第一个图表：
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding code generates the following output:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码生成了以下输出：
- en: '![](img/2d5d7bf0-1f1e-468d-b4d8-1f71fafd2967.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d5d7bf0-1f1e-468d-b4d8-1f71fafd2967.png)'
- en: 'There is a lot going on even in this simple example, but we''ll break it down
    line by line. The first line creates a single subplot with a width of `6` inches
    and a height of `4` inches. We then plot a histogram of the petal width from our
    `iris` DataFrame by calling `.hist()` and passing in our data. We also set the
    bar color to `black` here. The next two lines place labels on our *y* and *x*
    axes, respectively, and the final line sets the title for our graph. We tweak
    the title''s *y* position relative to the top of the graph with the *y* parameter,
    and increase the font size slightly over the default. This gives us a nice histogram
    of our petal width data. Let''s now expand on that, and generate histograms for
    each column of our `iris` dataset:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在这个简单的示例中，也有很多内容，但我们会逐行解析。第一行创建了一个宽度为`6`英寸、高度为`4`英寸的单个子图。接下来，我们通过调用`.hist()`并传入数据，绘制了我们`iris`数据框中花瓣宽度的直方图。我们还在这里将柱状图的颜色设置为`黑色`。接下来的两行分别为我们的*y*轴和*x*轴添加了标签，最后一行设置了图表的标题。我们通过*y*参数调整标题相对于图表顶部的位置，并略微增大字体大小。这样我们就得到了一个漂亮的花瓣宽度数据直方图。现在让我们扩展这个图表，生成我们`iris`数据集每一列的直方图：
- en: '[PRE3]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output for the preceding code is shown in the following diagram:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 上面代码的输出如下图所示：
- en: '![](img/d6900b26-a80f-4f45-8f0f-e7a40f2bf1ee.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6900b26-a80f-4f45-8f0f-e7a40f2bf1ee.png)'
- en: Obviously, this is not the most efficient way to code this, but it is useful
    for demonstrating how `matplotlib` works. Notice that instead of the single subplot
    object, `ax`, as we had in the first example, we now have four subplots, which
    are accessed through what is now the `ax` array. A new addition to the code is
    the call to `plt.tight_layout()`; this function will nicely auto-space your subplots
    to avoid crowding.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这不是编写代码的最有效方式，但它对于演示`matplotlib`的工作原理非常有用。注意，和我们在第一个示例中使用的单一子图对象`ax`不同，现在我们有了四个子图，这些子图通过现在的`ax`数组来访问。代码中的新添加部分是对`plt.tight_layout()`的调用；这个函数会自动调整子图的间距，以避免重叠。
- en: 'Let''s now take a look at a few other types of plots available in `matplotlib`.
    One useful plot is a **scatterplot**. Here, we will plot the petal width against
    the petal length:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看`matplotlib`中可用的几种其他类型的图表。一种有用的图表是**散点图**。在这里，我们将绘制花瓣宽度与花瓣长度的关系：
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code generates the following output:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码生成了以下输出：
- en: '![](img/bed222bf-d0e9-4ce9-92f2-3240ee9a3459.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bed222bf-d0e9-4ce9-92f2-3240ee9a3459.png)'
- en: As before, we could add in multiple subplots to examine each facet.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们可以添加多个子图来检查每个方面。
- en: 'Another plot we could examine is a simple line plot. Here, we will look at
    a plot of the petal length:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查的另一个图表是一个简单的折线图。在这里，我们将查看花瓣长度的图表：
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding code generates the following output:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码生成了以下输出：
- en: '![](img/76a9ae21-4b20-4963-b1f7-da912d47c549.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76a9ae21-4b20-4963-b1f7-da912d47c549.png)'
- en: We can already begin to see, based on this simple line plot, that there are
    distinctive clusters of lengths for each species—remember our sample dataset had
    50 ordered examples of each type. This tells us that petal length is likely to
    be a useful feature to discriminate between the species if we were to build a
    classifier.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经可以通过这个简单的折线图看到，每个物种的花瓣长度都有明显的聚类——记得我们的样本数据集每种类型有 50 个有序示例。这告诉我们，如果我们要构建一个分类器，花瓣长度可能是一个有用的特征，可以用来区分这些物种。
- en: 'Let''s look at one final type of chart from the `matplotlib` library, the bar
    chart. This is perhaps one of the more common charts you''ll see. Here, we''ll
    plot a bar chart for the mean of each feature for the three species of irises,
    and to make it more interesting, we''ll make it a stacked bar chart with a number
    of additional `matplotlib` features:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下 `matplotlib` 库中的最后一种图表类型——柱状图。这可能是你最常见的一种图表。这里，我们将为三种鸢尾花的每个特征绘制柱状图，并为了使图表更有趣，我们将其制作成堆叠柱状图，并添加一些额外的
    `matplotlib` 特性：
- en: '[PRE6]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output for the preceding snippet is given here:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段的输出结果如下：
- en: '![](img/1d9e505e-c134-40d2-adce-a3ca7804caf8.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d9e505e-c134-40d2-adce-a3ca7804caf8.png)'
- en: To generate the bar chart, we need to pass the `x` and `y` values into the `.bar()`
    function. In this case, the `x` values will just be an array of the length of
    the features we are interested in—four here, or one for each column in our DataFrame.
    The `np.arange()` function is an easy way to generate this, but we could nearly
    as easily input this array manually. Since we don't want the *x* axis to display
    this as 1 through 4, we call the `.set_xticklabels()` function and pass in the
    column names we wish to display. To line up the `x` labels properly, we also need
    to adjust the spacing of the labels. This is why we set the `xticks` to `x` plus
    half the size of the `bar_width`, which we also set earlier at `0.8`. The `y`
    values come from taking the mean of each feature for each species. We then plot
    each by calling `.bar()`. It is important to note that we pass in a `bottom` parameter
    for each series, which sets the minimum *y* point and the maximum *y* point of
    the series below it. This creates the stacked bars. And finally, we add a legend,
    which describes each series. The names are inserted into the legend list in order
    of the placement of the bars from top to bottom.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成柱状图，我们需要将 `x` 和 `y` 值传递给 `.bar()` 函数。在这种情况下，`x` 值将只是我们感兴趣的特征长度的数组——这里是四个，或者说是我们
    DataFrame 中每一列的一个值。`np.arange()` 函数是生成此数组的简便方法，但我们也几乎可以手动输入这个数组。由于我们不希望 *x* 轴显示为从
    1 到 4，我们调用 `.set_xticklabels()` 函数并传入我们希望显示的列名。为了正确对齐 `x` 标签，我们还需要调整标签的间距。这就是为什么我们将
    `xticks` 设置为 `x` 加上 `bar_width` 大小的一半，我们之前也将其设置为 `0.8`。`y` 值来自于每个特征在每个物种上的均值。然后我们通过调用
    `.bar()` 来绘制每一组数据。需要注意的是，我们为每个系列传入了一个 `bottom` 参数，它设置了该系列的最小 *y* 值和下面系列的最大 *y*
    值。这就创建了堆叠的柱状图。最后，我们添加了一个图例，说明了每个系列。图例中的名称按从上到下的顺序插入。
- en: The seaborn library
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Seaborn 库
- en: The next visualization library we'll look at is called `seaborn`, ([http://seaborn.pydata.org/index.html](http://seaborn.pydata.org/index.html)).
    It is a library that was created specifically for statistical visualizations.
    In fact, it is perfect for use with `pandas` DataFrames, where the columns are
    features and the rows are observations. This style of DataFrame is called **tidy**
    data, and is the most common form for machine learning applications.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们要查看的可视化库叫做 `seaborn`，([http://seaborn.pydata.org/index.html](http://seaborn.pydata.org/index.html))。它是一个专为统计可视化创建的库。事实上，它非常适合与
    `pandas` DataFrame 一起使用，其中列是特征，行是观察值。这种形式的 DataFrame 被称为 **tidy** 数据，是机器学习应用中最常见的数据形式。
- en: 'Let''s now take a look at the power of `seaborn`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看 `seaborn` 的强大功能：
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'With just those two lines of code, we get the following:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 仅凭这两行代码，我们就能得到以下结果：
- en: '![](img/755a6034-36e0-495b-bcef-83c32e7d3267.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/755a6034-36e0-495b-bcef-83c32e7d3267.png)'
- en: Seaborn plot
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Seaborn 图
- en: 'Having just detailed the intricate nuances of `matplotlib`, you will immediately
    appreciate the simplicity with which we generated this plot. All of our features
    have been plotted against each other and properly labeled with just two lines
    of code. You might wonder if I just wasted dozens of pages teaching you `matplotlib` when
    `seaborn` makes these types of visualizations so simple. Well, that isn''t the
    case, as `seaborn` is built on top of `matplotlib`. In fact, you can use all of
    what you learned about `matplotlib` to modify and work with `seaborn`. Let''s
    take a look at another visualization:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细介绍了`matplotlib`的复杂细节之后，你会立刻体会到我们生成这个图表的简便性。我们只用了两行代码，就把所有特征都绘制并正确标注了。你可能会想，我是不是浪费了几十页的篇幅教你`matplotlib`，而`seaborn`却能让这些类型的可视化变得如此简单。其实并非如此，因为`seaborn`是建立在`matplotlib`之上的。事实上，你可以利用你学到的所有`matplotlib`知识来修改和使用`seaborn`。让我们来看看另一个可视化：
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code generates the following output:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码生成了以下输出：
- en: '![](img/b7f548cf-894e-42d8-ab6a-8b4b4cebe4ea.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7f548cf-894e-42d8-ab6a-8b4b4cebe4ea.png)'
- en: Violin Plots
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 小提琴图
- en: Here, we have generated a violin plot for each of the four features. A violin
    plot displays the distribution of the features. For example, you can easily see
    that the petal length of `setosa` (0) is highly clustered between 1 cm and 2 cm,
    while `virginica` (2) is much more dispersed, from nearly 4 cm to over 7 cm. You
    will also notice that we have used much of the same code we used when constructing
    the `matplotlib` graphs. The main difference is the addition of the `sns.plot()`
    calls, in place of the `ax.plot()` calls previously. We have also added a title
    above all of the subplots, rather than over each individually, with the `fig.suptitle()`
    function. One other notable addition is the iteration over each of the subplots
    to change the rotation of the `xticklabels`. We call `ax.flat()` and then iterate
    over each subplot axis to set a particular property using `.setp()`. This prevents
    us from having to individually type out `ax[0][0]...ax[1][1]` and set the properties,
    as we did previously in the earlier `matplotlib` subplot code.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们为四个特征生成了小提琴图。小提琴图展示了特征的分布。例如，你可以很容易地看到`setosa`（0）的花瓣长度高度集中在1厘米到2厘米之间，而`virginica`（2）的花瓣长度则更加分散，从接近4厘米到超过7厘米。你还会注意到，我们使用了与构建`matplotlib`图形时大部分相同的代码。主要的区别在于我们用`ax.plot()`替换成了`sns.plot()`。我们还在所有子图上方添加了一个标题，而不是在每个子图上单独添加标题，这是通过`fig.suptitle()`函数实现的。另一个值得注意的变化是迭代每个子图以改变`xticklabels`的旋转角度。我们调用了`ax.flat()`，然后迭代每个子图轴，使用`.setp()`设置特定属性。这避免了我们像之前的`matplotlib`子图代码那样，必须单独输入`ax[0][0]...ax[1][1]`并设置属性。
- en: There are hundreds of styles of graphs you can generate using `matplotlib` and
    `seaborn`, and I highly recommend digging into the documentation for these two
    libraries—it will be time well spent—but the graphs I have detailed in the preceding section
    should go a long way toward helping you to understand the dataset you have, which
    in turn will help you when building your machine learning models.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`matplotlib`和`seaborn`，你可以生成数百种不同风格的图表，我强烈建议你深入研究这两个库的文档——那将是值得的时间——不过我在前一节中详细介绍的图表应该能够帮助你很大程度地理解你的数据集，而这将有助于你在构建机器学习模型时。
- en: Preparation
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: We've learned a great deal about inspecting the data we have, but now let's
    move on to learning how to process and manipulate our data. Here, we will learn
    about the `.map()`, `.apply()`, `.applymap()`, and `.groupby()` functions of pandas.
    These are invaluable for working with data, and are especially useful in the context
    of machine learning for feature engineering, a concept we will discuss in detail
    in later chapters.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学到了很多关于检查我们拥有的数据的知识，现在让我们继续学习如何处理和操作这些数据。在这里，我们将学习`pandas`的`.map()`、`.apply()`、`.applymap()`和`.groupby()`函数。这些函数在数据处理中非常宝贵，尤其在机器学习中的特征工程中极为重要，我们将在后续章节中详细讨论这一概念。
- en: map
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: map
- en: 'We''ll now begin with the `map` function. The `map` function works on series,
    so in our case we will use it to transform a column of our DataFrame, which you
    will recall is just a pandas series. Suppose we decide that the species numbers
    are not suitable for our needs. We''ll use the `map` function with a Python dictionary
    as the argument to accomplish this. We''ll pass in a replacement for each of the
    unique `iris` types:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将从`map`函数开始。`map`函数适用于序列，所以在我们的例子中，我们将使用它来转换DataFrame中的一列，正如你所记得，它实际上是一个pandas序列。假设我们决定物种编号不适合我们的需求，我们将使用带有Python字典作为参数的`map`函数来完成这个任务。我们会为每个独特的`iris`类型传递一个替代值：
- en: '![](img/a142343c-7ea1-4a61-9819-b6a992fbeda2.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a142343c-7ea1-4a61-9819-b6a992fbeda2.png)'
- en: Let's look at what we have done here. We have run the `map` function over each
    of the values of the existing `species` column. As each value was found in the
    Python dictionary, it was added to the return series. We assigned this return
    series to the same `species` name, so it replaced our original `species` column.
    Had we chosen a different name, say `short code`, that column would have been
    appended to the DataFrame, and we would then have the original `species` column
    plus the new `short code` column.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下我们所做的事情。我们对现有的`species`列中的每个值都应用了`map`函数。当每个值在Python字典中找到时，它会被添加到返回的序列中。我们将这个返回序列赋给了相同的`species`名称，因此它替换了我们原始的`species`列。如果我们选择了一个不同的名称，比如`short
    code`，那么那一列会被附加到DataFrame中，之后我们将拥有原始的`species`列以及新的`short code`列。
- en: We could have instead passed the `map` function a series or a function to perform
    this transformation on a column, but this is a functionality that is also available
    through the `apply` function, which we'll take a look at next. The dictionary
    functionality is unique to the `map` function, and the most common reason to choose
    `map` over `apply` for a single column transformation. But, let's now take a look
    at the `apply` function.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以将`map`函数传递给一个序列或函数，来对某一列进行转换，但这种功能也可以通过`apply`函数实现，我们接下来将了解它。字典功能是`map`函数特有的，且选择`map`而非`apply`进行单列转换的最常见原因。现在，让我们来看看`apply`函数。
- en: apply
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: apply
- en: The `apply` function allows us to work with both DataFrames and series. We'll
    start with an example that would work equally well with `map`, before moving on
    to examples that would only work with `apply`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply`函数允许我们同时处理DataFrame和序列。我们将从一个可以同样适用于`map`的例子开始，然后再介绍一些只适用于`apply`的例子。'
- en: 'Using our `iris` DataFrame, let''s make a new column based on petal width.
    We previously saw that the mean for the petal width was `1.3`. Let''s now create
    a new column in our DataFrame, `wide petal`, that contains binary values based
    on the value in the `petal width` column. If the `petal width` is equal to or
    wider than the median, we will code it with a `1`, and if it is less than the
    median, we will code it `0`. We''ll do this using the `apply` function on the
    `petal width` column:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的`iris` DataFrame，让我们基于花瓣宽度创建一个新列。我们之前看到，花瓣宽度的均值是`1.3`。现在，让我们在DataFrame中创建一个新列`wide
    petal`，该列基于`petal width`列的值包含二进制值。如果`petal width`大于或等于中位数，我们将其编码为`1`，如果小于中位数，我们将其编码为`0`。我们将使用`apply`函数对`petal
    width`列执行这个操作：
- en: '![](img/130a51fb-e890-4b1d-b68a-855ba42b2ca6.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/130a51fb-e890-4b1d-b68a-855ba42b2ca6.png)'
- en: A few things happened here, so let's walk through them step by step. The first
    is that we were able to append a new column to the DataFrame simply by using the
    column selection syntax for a column name, which we want to create, in this case
    `wide petal`. We set that new column equal to the output of the `apply` function.
    Here, we ran `apply` on the `petal width` column that returned the corresponding
    values in the `wide petal` column. The `apply` function works by running through
    each value of the `petal width` column. If the value is greater than or equal
    to `1.3`, the function returns `1`, otherwise it returns `0`. This type of transformation
    is a fairly common feature engineering transformation in machine learning, so
    it is good to be familiar with how to perform it.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了几件事情，我们一步步来分析。首先，我们能够通过使用列选择语法来简单地将新列附加到DataFrame中，这里是`wide petal`。我们将新列设置为`apply`函数的输出。在这里，我们对`petal
    width`列运行了`apply`，并返回了对应的`wide petal`列的值。`apply`函数通过遍历`petal width`列中的每个值来工作。如果该值大于或等于`1.3`，函数返回`1`，否则返回`0`。这种类型的转换是机器学习中常见的特征工程转换，因此熟悉如何执行它是很有用的。
- en: 'Let''s now take a look at using `apply` on a DataFrame rather than a single
    series. We''ll now create a feature based on the `petal area`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将演示如何在`DataFrame`上使用`apply`，而不是单个系列。我们将基于`花瓣面积`创建一个新特征：
- en: '![](img/50618111-1c9a-4d1a-bd5c-d5b9212ddbf7.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50618111-1c9a-4d1a-bd5c-d5b9212ddbf7.png)'
- en: Creating a new feature
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新特征
- en: Notice that we called `apply` not on a series here, but on the entire DataFrame,
    and because `apply` was called on the entire DataFrame, we passed in `axis=1`
    in order to tell pandas that we want to apply the function row-wise. If we passed
    in `axis=0`, then the function would operate column-wise. Here, each column is
    processed sequentially, and we choose to multiply the values from the `petal length
    (cm)` and `petal width (cm)` columns. The resultant series then becomes the `petal
    area` column in our DataFrame. This type of power and flexibility is what makes
    pandas an indispensable tool for data manipulation.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在这里调用`apply`时不是对单个系列进行操作，而是对整个`DataFrame`进行操作。因为`apply`是对整个`DataFrame`调用的，所以我们传入了`axis=1`，以告知pandas我们希望按行应用该函数。如果传入`axis=0`，则函数会按列操作。在这里，每一列会被顺序处理，我们选择将`petal
    length (cm)`和`petal width (cm)`两列的值相乘，结果系列成为`petal area`列。这种灵活性和强大功能正是pandas成为数据处理不可或缺工具的原因。
- en: applymap
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: applymap
- en: 'We''ve looked at manipulating columns and explained how to work with rows,
    but suppose you''d like to perform a function across all data cells in your DataFrame.
    This is where `applymap` is the correct tool. Let''s take a look at an example:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论过如何操作列并解释了如何处理行，但假设你想在`DataFrame`的所有数据单元格上执行一个函数。这个时候，`applymap`就是正确的工具。让我们看一个例子：
- en: '![](img/dc75d666-8800-4472-a5ad-4aab04203a7a.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc75d666-8800-4472-a5ad-4aab04203a7a.png)'
- en: Using applymap function
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用applymap函数
- en: Here, we called `applymap` on our DataFrame in order to get the log of every
    value (`np.log()` utilizes the NumPy library to return this value), if that value
    is of the float type. This type checking prevents returning an error or a float
    for the `species` or `wide petal` columns, which are string and integer values
    respectively. Common uses of `applymap` include transforming or formatting each
    cell based on meeting a number of conditional criteria.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们对`DataFrame`调用了`applymap`，以便获取每个值的对数（`np.log()`利用NumPy库返回此值），前提是该值是浮动类型。这个类型检查可以防止在`species`或`wide
    petal`列（分别是字符串和整数值）上返回错误或浮动类型。`applymap`的常见用法包括根据满足一系列条件对每个单元格进行转换或格式化。
- en: groupby
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: groupby
- en: 'Let''s now look at an operation that is highly useful, but often difficult
    for new pandas users to get their heads around: the `.groupby()` function. We''ll
    walk through a number of examples step by step in order to illustrate the most
    important functionality.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看一个非常有用，但对于新手来说常常难以理解的操作：`.groupby()`函数。我们将通过多个示例逐步演示，以说明其最重要的功能。
- en: 'The `groupby` operation does exactly what it says: it groups data based on
    some class or classes you choose. Let''s take a look at a simple example using
    our `iris` dataset. We''ll go back and reimport our original `iris` dataset, and
    run our first `groupby` operation:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupby`操作如其名所示：它根据你选择的某些类别或类对数据进行分组。让我们用`iris`数据集看一个简单的例子。我们将重新导入原始的`iris`数据集，并执行第一次`groupby`操作：'
- en: '![](img/95d40a45-53e1-4933-b659-e49668bab553.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95d40a45-53e1-4933-b659-e49668bab553.png)'
- en: 'Here, data for each species is partitioned and the mean for each feature is
    provided. Let''s take it a step further now and get full descriptive statistics
    for each `species`:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个物种的数据被分区并提供了每个特征的平均值。现在我们更进一步，获取每个`species`的完整描述性统计数据：
- en: '![](img/1d842d79-d829-4838-afc0-3e120126af1e.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d842d79-d829-4838-afc0-3e120126af1e.png)'
- en: Statistics for each species
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 每个物种的统计数据
- en: 'And now, we can see the full breakdown bucketed by `species`. Let''s now look
    at some other `groupby` operations we can perform. We saw previously that petal
    length and width had some relatively clear boundaries between species. Now, let''s
    examine how we might use `groupby` to see that:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到按`species`分组的完整数据。接下来，我们将看看还可以执行哪些其他的`groupby`操作。我们之前看到过，花瓣的长度和宽度在不同物种之间有相对明确的边界。现在，让我们研究如何使用`groupby`来观察这一点：
- en: '![](img/dae8f90a-1b47-4c14-9c37-bc7c2ceaf936.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dae8f90a-1b47-4c14-9c37-bc7c2ceaf936.png)'
- en: In this case, we have grouped each unique species by the `petal width` they
    were associated with. This is a manageable number of measurements to group by,
    but if it were to become much larger, we would likely need to partition the measurements
    into brackets. As we saw previously, that can be accomplished by means of the
    `apply` function.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们根据每个独特物种的`花瓣宽度`对它们进行分组。这是一个可以管理的分组依据，但如果数据量变得更大，我们可能需要将测量数据划分为多个区间。正如我们之前看到的，这可以通过`apply`函数来实现。
- en: 'Let''s now take a look at a custom aggregation function:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看一个自定义聚合函数：
- en: '![](img/ee9e15bb-4bbc-4886-b084-5d777252a6ed.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee9e15bb-4bbc-4886-b084-5d777252a6ed.png)'
- en: In this code, we grouped petal width by species using the `.max()` and `.min()` functions,
    and a `lambda` function that returns a maximum petal width less than the minimum
    petal width.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们使用`.max()`和`.min()`函数以及返回最大花瓣宽度小于最小花瓣宽度的`lambda`函数对花瓣宽度按物种进行分组。
- en: We've only just touched on the functionality of the `groupby` function; there
    is a lot more to learn, so I encourage you to read the documentation available
    at [http://pandas.pydata.org/pandas-docs/stable/](http://pandas.pydata.org/pandas-docs/stable/).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚触及了`groupby`函数的一些功能，实际上它有更多的应用，建议你阅读[http://pandas.pydata.org/pandas-docs/stable/](http://pandas.pydata.org/pandas-docs/stable/)中的文档。
- en: Hopefully, you now have a solid base-level understanding of how to manipulate
    and prepare data in preparation for our next step, which is modeling. We will
    now move on to discuss the primary libraries in the Python machine learning ecosystem.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你现在已经对如何操作和准备数据有了坚实的基础理解，为接下来的建模步骤做准备。接下来，我们将讨论Python机器学习生态系统中的主要库。
- en: Modeling and evaluation
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模与评估
- en: In this section ,we will go through different libraries such as `statsmodels`
    and `Scikit-learn` and also understand what is deployment.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将介绍不同的库，如`statsmodels`和`Scikit-learn`，并理解什么是部署。
- en: Statsmodels
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Statsmodels
- en: The first library we'll cover is the `statsmodels` library ([http://statsmodels.sourceforge.net/](http://statsmodels.sourceforge.net/)). Statsmodels
    is a Python package that is well documented and developed for exploring data,
    estimating models, and running statistical tests. Let's use it here to build a
    simple linear regression model of the relationship between sepal length and sepal
    width for the `setosa` species.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍的第一个库是`statsmodels`库（[http://statsmodels.sourceforge.net/](http://statsmodels.sourceforge.net/)）。Statsmodels是一个Python包，功能强大，文档完善，专为数据探索、模型估计和统计测试而设计。在这里，我们将使用它来建立`setosa`物种的花萼长度和花萼宽度之间关系的简单线性回归模型。
- en: 'First, let''s visually inspect the relationship with a scatterplot:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们通过散点图直观地检查这种关系：
- en: '[PRE9]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding code generates the following output:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/8dd510a9-5de5-4967-a9fb-6f804204c0a3.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8dd510a9-5de5-4967-a9fb-6f804204c0a3.png)'
- en: 'So, we can see that there appears to be a positive linear relationship; that
    is, as the sepal width increases, the sepal length does as well. We''ll next run
    a linear regression on the data using `statsmodels` to estimate the strength of
    that relationship:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以看到，似乎存在正线性关系；也就是说，随着花萼宽度的增加，花萼长度也增加。接下来，我们将使用`statsmodels`对数据进行线性回归，以估计这种关系的强度：
- en: '[PRE10]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding code generates the following output:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/7d3255fb-4a45-4918-800c-7d5c4ab75ba6.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d3255fb-4a45-4918-800c-7d5c4ab75ba6.png)'
- en: In the preceding diagram, we have the results of our simple regression model.
    Since this is a linear regression, the model takes the format of *Y = Β[0]+ Β[1]X*,
    where *B[0]* is the intercept and *B[1]* is the regression coefficient. Here,
    the formula would be *Sepal Length = 2.6447 + 0.6909 * Sepal Width*. We can also
    see that the *R²* for the model is a respectable `0.558`, and the *p*-value, (`Prob`),
    is highly significant—at least for this species.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们展示了简单回归模型的结果。由于这是一个线性回归模型，模型的形式为*Y = Β[0]+ Β[1]X*，其中*B[0]*是截距，*B[1]*是回归系数。这里的公式为*花萼长度
    = 2.6447 + 0.6909 * 花萼宽度*。我们还可以看到，模型的*R²*值是一个令人满意的`0.558`，而*p*值（`Prob`）非常显著—至少对于这个物种来说。
- en: 'Let''s now use the `results` object to plot our regression line:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用`results`对象来绘制回归线：
- en: '[PRE11]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The preceding code generates the following output:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/03f2c04e-4350-4284-981e-e9e0627cb691.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03f2c04e-4350-4284-981e-e9e0627cb691.png)'
- en: By plotting `results.fittedvalues`, we can get the resulting regression line
    from our regression.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通过绘制`results.fittedvalues`，我们可以得到回归分析的回归线。
- en: 'There are a number of other statistical functions and tests in the `statsmodels`
    package, and I invite you to explore them. It is an exceptionally useful package
    for standard statistical modeling in Python. Let''s now move on to the king of
    Python machine learning packages: scikit-learn.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`statsmodels` 包中还有许多其他统计函数和测试，欢迎你去探索它们。它是 Python 中进行标准统计建模的一个非常有用的包。现在，让我们继续深入了解
    Python 机器学习库中的王者：scikit-learn。'
- en: Scikit-learn
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-learn
- en: 'Scikit-learn is an amazing Python library with unrivaled documentation, designed
    to provide a consistent API to dozens of algorithms. It is built upon, and is
    itself, a core component of the Python scientific stack, which includes NumPy,
    SciPy, pandas, and matplotlib. Here are some of the areas scikit-learn covers:
    classification, regression, clustering, dimensionality reduction, model selection,
    and preprocessing.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 是一个了不起的 Python 库，具有无与伦比的文档，旨在为众多算法提供一致的 API。它建立在 Python 科学计算堆栈的基础上，并且本身也是这个堆栈的核心组件，堆栈包括
    NumPy、SciPy、pandas 和 matplotlib。Scikit-learn 涉及的领域包括：分类、回归、聚类、降维、模型选择和预处理。
- en: 'We''ll look at a few examples. First, we will build a classifier using our
    `iris` data, and then we''ll look at how we can evaluate our model using the tools
    of scikit-learn:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看一些示例。首先，我们将使用 `iris` 数据集构建一个分类器，然后我们将看看如何使用 scikit-learn 的工具来评估我们的模型：
- en: The first step to building a machine learning model in scikit-learn is understanding
    how the data must be structured.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中构建机器学习模型的第一步是理解数据应该如何结构化。
- en: The independent variables should be a numeric *n ×** m* matrix, *X*, and the
    dependent variable, *y*, an *n ×** 1* vector.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自变量应该是一个数值型 *n × m* 矩阵 *X*，因变量 *y* 是一个 *n × 1* 向量。
- en: The *y* vector may be either a numeric continuous or categorical, or a string
    categorical.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*y* 向量可以是数值型连续变量、分类变量，或者是字符串型分类变量。'
- en: These are then passed into the `.fit()` method on the chosen classifier.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些数据随后被传递给所选分类器的 `.fit()` 方法。
- en: 'This is the great benefit of using scikit-learn: each classifier utilizes the
    same methods to the extent possible. This makes swapping them in and out a breeze.'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这就是使用 scikit-learn 的巨大好处：每个分类器尽可能使用相同的方法。这使得更换分类器变得非常简单。
- en: 'Let''s see this in action in our first example:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在第一个示例中看看实际效果：
- en: '[PRE12]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding code generates the following output:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/9e72ccce-769a-406c-8334-e70e7a7a5176.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e72ccce-769a-406c-8334-e70e7a7a5176.png)'
- en: 'Now, let''s execute the following line of code:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们执行以下代码：
- en: '[PRE13]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code generates the following output:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/acb8d103-530a-4462-b7b3-befed4ea8e0c.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acb8d103-530a-4462-b7b3-befed4ea8e0c.png)'
- en: In the preceding few lines of code, we built, trained, and tested a classifier
    that has a 95% accuracy level on our `iris` dataset. Let's unpack each of the
    steps. Up at the top, we made a couple of imports; the first two are from scikit-learn,
    which thankfully is shortened to `sklearn` in import statements. The first import
    is a random forest classifier, and the second is a module for splitting your data
    into training and testing cohorts. This data partitioning is critical in building
    machine learning applications for a number of reasons. We'll get into this in
    later chapters, but suffice to say at this point it is a must. This `train_test_split`
    module also shuffles your data, which again is important as the order can contain
    information that would bias your actual predictions.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的几行代码中，我们构建、训练并测试了一个分类器，该分类器在我们的 `iris` 数据集上的准确率为 95%。让我们逐步解析每个步骤。首先，我们做了几个导入；前两个是来自
    scikit-learn，幸运的是，在导入语句中它们被缩写为 `sklearn`。第一个导入是随机森林分类器，第二个是用于将数据拆分为训练集和测试集的模块。这种数据划分在构建机器学习应用时至关重要，原因有很多。我们将在后面的章节中详细讨论，但在此时可以说，它是必须的。这个
    `train_test_split` 模块还会对数据进行洗牌，这一点也非常重要，因为数据的顺序可能包含会影响预测的偏置信息。
- en: The first curious-looking line after the imports instantiates our classifier,
    in this case a random forest classifier. We select a forest that uses 10 decision
    tress, and each tree is allowed a maximum split depth of five. This is put in
    place to avoid overfitting, something we will discuss in depth in later chapters.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 导入语句后面的第一行看起来有些奇怪，它实例化了我们的分类器，在这个例子中是一个随机森林分类器。我们选择了一个包含 10 棵决策树的森林，并且每棵树的最大分裂深度为五。这样设置是为了避免过拟合，这是我们将在后面章节中深入讨论的内容。
- en: 'The next two lines create our *X* matrix and *y* vector. If you remember our
    original `iris` DataFrame, it contained four features: petal width and length,
    and sepal width and length. These features are selected and become our independent
    feature matrix, *X*. The last column, the `iris` class names, then becomes our
    dependent *y* vector.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两行代码创建了我们的*X*矩阵和*y*向量。如果你记得我们最初的`iris` DataFrame，它包含了四个特征：花瓣宽度、花瓣长度、萼片宽度和萼片长度。这些特征被选中，成为我们的自变量特征矩阵*X*。最后一列，`iris`类别名称，则成为我们的因变量*y*向量。
- en: These are then passed into the `train_test_split` method, which shuffles and
    partitions our data into four subsets, `X_train`, `X_test`, `y_train`, and `y_test`.
    The `test_size` parameter is set to `.3`, which means 30% of our dataset will
    be allocated to the `X_test` and `y_test` partitions, while the rest will be allocated
    to the training partitions, `X_train` and `y_train`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这些数据被传入`train_test_split`方法，该方法会打乱并将我们的数据分为四个子集，`X_train`、`X_test`、`y_train`和`y_test`。`test_size`参数设置为`.3`，这意味着30%的数据集将分配给`X_test`和`y_test`，其余的则分配给训练集`X_train`和`y_train`。
- en: 'Next, our model is fitted using the training data. Having trained the model,
    we then call the predict method on our classifier using our test data. Remember,
    the test data is data the classifier has not seen. The return of this prediction
    is a list of prediction labels. We then create a DataFrame of the actual labels
    versus the predicted labels. We finally total the correct predictions and divide
    by the total number of instances, which we can see gave us a very accurate prediction.
    Let''s now see which features gave us the most discriminative or predictive power:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用训练数据来拟合我们的模型。训练完成后，我们使用测试数据调用分类器的预测方法。请记住，测试数据是分类器未曾见过的数据。这个预测返回的是一个预测标签的列表。然后，我们创建一个包含实际标签与预测标签的DataFrame。最后，我们统计正确预测的数量，并除以总实例数，从而得到一个非常准确的预测。现在，让我们看看哪些特征为我们提供了最具判别力或预测力的能力：
- en: '[PRE14]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding code generates the following output:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/ab3a7c70-9cf5-4832-bfe3-30d93822cd23.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab3a7c70-9cf5-4832-bfe3-30d93822cd23.png)'
- en: As we expected, based upon our earlier visual analysis, the petal length and
    width have more discriminative power when differentiating between the `iris` classes.
    Where exactly did these numbers come from though? The random forest has a method
    called `.feature_importances_` that returns the relative performance of the feature
    for splitting at the leaves. If a feature is able to consistently and cleanly
    split a group into distinct classes, it will have a high feature importance. This
    number will always total one. As you will notice here, we have included the standard
    deviation, which helps to illustrate how consistent each feature is. This is generated
    by taking the feature importance, for each of the features, for each ten trees,
    and calculating the standard deviation.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们预期的，根据之前的可视化分析，花瓣的长度和宽度在区分`iris`类别时具有更强的判别力。那么，这些数字究竟是从哪里来的呢？随机森林有一个叫做`.feature_importances_`的方法，它返回特征在叶节点划分时的相对表现。如果一个特征能够一致且干净地将一组数据划分成不同的类别，那么它的特征重要性就会很高。这个数值的总和始终为1。正如你在这里看到的，我们还包括了标准差，这有助于说明每个特征的一致性。这是通过对每个特征的特征重要性进行计算，针对每十棵树，计算标准差来生成的。
- en: 'Let''s now take a look at one more example using scikit-learn. We will now
    switch out our classifier and use a **support vector machine** (**SVM**):'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看另一个使用scikit-learn的例子。我们将更换分类器，使用**支持向量机**（**SVM**）：
- en: '[PRE15]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preceding code generates the following output:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/aac65c4a-d541-4397-be05-a2c38437f19c.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aac65c4a-d541-4397-be05-a2c38437f19c.png)'
- en: 'Now, let''s execute the following line of code:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们执行以下代码行：
- en: '[PRE16]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The preceding code generates the following output:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/56d1a215-3be1-4641-9683-59b9520e6bc9.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56d1a215-3be1-4641-9683-59b9520e6bc9.png)'
- en: Here, we have swapped in an SVM without changing virtually any of our code.
    The only changes were the ones related to the importing of the SVM instead of
    the random forest, and the line that instantiates the classifier. (I did have
    to make one small change to the format of the `y` labels, as the SVM wasn't able
    to interpret them as NumPy strings like the random forest classifier was. Sometimes,
    these data type conversions have to be made specific or it will result in an error,
    but it's a minor annoyance.)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了支持向量机（SVM），而几乎没有更改任何代码。唯一的变化是与导入SVM代替随机森林相关的部分，以及实例化分类器的那一行。（我确实需要对`y`标签的格式做一个小小的调整，因为SVM无法像随机森林分类器那样将它们解释为NumPy字符串。有时，必须做出特定的数据类型转换，否则会导致错误，但这只是一个小小的烦恼。）
- en: This is only a small sample of the functionality of scikit-learn, but it should
    give you a hint of the power of this magnificent tool for machine learning applications.
    There are a number of additional machine learning libraries we won't have a chance
    to discuss here but will explore in later chapters, but I strongly suggest that
    if this is your first time utilizing a machine learning library, and you want
    a strong general-purpose tool, scikit-learn is your go-to choice.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是scikit-learn功能的一个小样本，但它应该能给你一些提示，说明这个强大的机器学习工具的威力。还有许多其他机器学习库，我们在这里无法详细讨论，但会在后续章节中探讨。如果这是你第一次使用机器学习库，并且你需要一个强大的通用工具，我强烈建议你选择scikit-learn。
- en: Deployment
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: There are a number of options you can choose from when you decide to put your
    machine learning model into production. It depends substantially on the nature
    of the application. Deployment could include anything from a cron job run on your
    local machine to a full-scale implementation deployed on an Amazon EC2 instance.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 当你决定将机器学习模型投入生产时，有许多选择可以选择。这主要取决于应用程序的性质。部署可以包括从本地机器上运行的定时任务到在Amazon EC2实例上部署的全规模实施。
- en: We won't go into detail regarding specific implementations here, but we will
    have a chance to delve into different deployment examples throughout the book.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会详细讨论具体的实现，但在全书中我们会有机会深入探讨不同的部署实例。
- en: Setting up your machine learning environment
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置你的机器学习环境
- en: We've covered a number of libraries, and it could be somewhat of a chore to
    install if you were to do each individually—which you certainly can, since most
    can be installed with pip, Python's package manager, but I would strongly urge
    you to go with a prepacked solution such as the Anaconda Python distribution ([http://anaconda.org](http://anaconda.org)).
    This allows you to download and install a single executable with all the packages
    and dependencies handled for you. And since the distribution is targeted to Python
    scientific stack users, it is essentially a one-and-done solution.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经覆盖了许多库，如果你要单独安装每一个库，可能会有些麻烦——当然，你完全可以这样做，因为大多数库都可以通过pip，Python的包管理器来安装，但我强烈建议你使用像Anaconda
    Python发行版（[http://anaconda.org](http://anaconda.org)）这样的预打包解决方案。这样，你可以下载并安装一个包含所有包和依赖项的单一可执行文件，而所有的依赖问题都由它为你处理。由于该发行版针对Python科学计算栈用户，因此它本质上是一个一站式解决方案。
- en: Anaconda also includes a package manager that makes updating your packages a
    simple task. Simply type `conda update <package_name>`, and you will be updated
    to the most recent stable release.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Anaconda还包括一个包管理器，使得更新包变得非常简单。只需键入`conda update <package_name>`，就可以将包更新到最新的稳定版本。
- en: Summary
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about the data science/machine learning workflow.
    We learned how to take our data step by step through each stage of the pipeline,
    going from acquisition all the way through to deployment. We also learned key
    features of each of the most important libraries in the Python scientific stack.
    We will now take this knowledge and these lessons and begin to apply them to create
    unique and useful machine learning applications. Let's get started!
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们了解了数据科学/机器学习的工作流程。我们学习了如何一步步将数据从获取阶段通过每个阶段处理，直到部署阶段。我们还了解了Python科学计算栈中最重要的每个库的关键特性。现在，我们将应用这些知识和经验，开始创建独特且有用的机器学习应用。让我们开始吧！
