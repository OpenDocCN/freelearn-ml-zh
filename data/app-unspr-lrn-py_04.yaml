- en: '*Chapter 4*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*'
- en: Dimension Reduction and PCA
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维和PCA
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将能够：
- en: Apply dimension reduction techniques.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用降维技术。
- en: Describe the concepts behind principal components and dimensionality reduction.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述主成分和降维背后的概念。
- en: Apply principal component analysis (PCA) when solving problems using scikit-learn.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用scikit-learn解决问题时应用主成分分析（PCA）。
- en: Compare manual PCA versus scikit-learn.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较手动PCA与scikit-learn的PCA。
- en: In this chapter, we will look at dimension reduction and different dimension
    reduction techniques.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨降维及其不同的降维技术。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: This chapter is the first of a series of three chapters that investigate the
    use of different feature sets (or spaces) in our unsupervised learning algorithms,
    and we will start with a discussion around dimensionality reduction, specifically,
    PCA. We will then extend upon our understanding of the benefits of the different
    feature spaces through an exploration of two independently powerful machine learning
    architectures in neural network-based auto-encoders. Neural networks certainly
    have a well-deserved reputation for being powerful models in supervised learning
    problems, and, through the use of an autoencoder stage, have been shown to be
    sufficiently flexible for their application to unsupervised learning problems.
    Finally, we will build on our neural network implementation and dimensionality
    reduction as we cover t-distributed nearest neighbors in the final chapter of
    this micro-series.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是三章系列中的第一章，探讨我们在无监督学习算法中使用不同特征集（或空间）的应用，我们将从降维的讨论开始，特别是主成分分析（PCA）。接下来，我们将通过探索两种独立且强大的机器学习架构——基于神经网络的自编码器，扩展我们对不同特征空间好处的理解。神经网络在监督学习问题中无疑有着应得的声誉，而通过使用自编码器阶段，它们已被证明在无监督学习问题的应用上足够灵活。最后，在本微系列的最后一章中，我们将基于神经网络实现和降维的基础，讨论t分布最近邻算法。
- en: What Is Dimensionality Reduction?
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是降维？
- en: 'Dimensionality reduction is an important tool in any data scientists'' toolkit,
    and, due to its wide variety of use cases, is essentially assumed knowledge within
    the field. So, before we can consider reducing the dimensionality and why we would
    want to reduce it, we must first have a good understanding of what dimensionality
    is. To put it simply, dimensionality is the number of dimensions, features, or
    variables associated with a sample of data. Often, this can be thought of as a
    number of columns in a spreadsheet, where each sample is on a new row, and each
    column describes some attribute of the sample. The following table is an example:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 降维是数据科学家工具包中的一个重要工具，并且由于其广泛的应用场景，它在该领域几乎被视为基本知识。因此，在我们考虑降维及其必要性之前，我们首先需要理解什么是维度。简单来说，维度是与数据样本相关的维度、特征或变量的数量。通常，可以将其视为电子表格中的列数，其中每个样本占据一行，每一列描述样本的某个属性。以下表格就是一个例子：
- en: '![](img/C12626_04_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_04_01.jpg)'
- en: 'Figure 4.1: Two samples of data with three different features'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.1：具有三个不同特征的两个数据样本
- en: In *Figure 4.1*, we have two samples of data, each with three independent features
    or dimensions. Depending upon the problem being solved, or the origin of this
    dataset, we may want to reduce the number of dimensions per sample without losing
    the provided information. This is where dimensionality reduction can be helpful.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 4.1*中，我们有两个数据样本，每个样本有三个独立的特征或维度。根据所解决的问题或数据集的来源，我们可能希望在不丢失已提供信息的情况下，减少每个样本的维度数量。这就是降维发挥作用的地方。
- en: 'But how exactly can dimensionality reduction help us in solving problems? We
    will cover the applications in more detail in the following section; but let''s
    say that we had a very large dataset of time series data, such as echo-cardiogram
    or ECG (also known as an EKG in some countries) signals as shown in the following
    figure:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，降维究竟如何帮助我们解决问题呢？我们将在接下来的部分更详细地介绍应用；但假设我们有一个非常大的时间序列数据集，例如回声心动图或心电图（在一些国家也称为EKG）信号，如下图所示：
- en: '![](img/C12626_04_02.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_04_02.jpg)'
- en: 'Figure 4.2: Electrocardiogram (ECG or EKG)'
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.2：心电图（ECG或EKG）
- en: 'These signals were captured from your company''s new model of watch, and we
    need to look for signs of a heart attack or stroke. In looking through the dataset,
    we can make a few observations:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信号是从您公司新型号的手表中捕获的，我们需要寻找心脏病或中风的迹象。在查看数据集时，我们可以做出以下几项观察：
- en: Most of the individual heartbeats are very similar.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数单独的心跳信号非常相似。
- en: There is some noise in the data from the recording system or from the patient
    moving during the recording.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中存在来自录音系统或患者在录音过程中移动的噪音。
- en: Despite the noise, the heartbeat signals are still visible.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管有噪音，心跳信号仍然可见。
- en: There is a lot of data – too much to be able to process using the hardware available
    on the watch.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据量非常大——超出了手表可用硬件的处理能力。
- en: It is in such a situation that dimensionality reduction really shines! By using
    dimensionality reduction, we are able to remove much of the noise from the signal,
    which, in turn, will assist with the performance of the algorithms that are applied
    to the data as well as reduce the size of the dataset to allow for reduced hardware
    requirements. The techniques that we are going to discuss in this chapter, in
    particular, PCA and autoencoders, have been well applied in research and industry
    to effectively process, cluster, and classify such datasets. By the end of this
    chapter, you will be able to apply these techniques to your own data and hopefully
    see an increase in the performance of your own machine learning systems.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 正是在这种情况下，降维技术真正显示其优势！通过使用降维技术，我们能够从信号中去除大量噪音，这反过来将有助于提高应用于数据的算法性能，并减少数据集的大小，从而降低硬件要求。本章中我们将讨论的技术，特别是PCA和自编码器，在研究和行业中已被广泛应用于有效地处理、聚类和分类这类数据集。在本章结束时，您将能够将这些技术应用于您自己的数据，并希望看到您自己机器学习系统性能的提升。
- en: Applications of Dimensionality Reduction
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 降维的应用
- en: 'Before we start a detailed investigation of dimensionality reduction and PCA,
    we will discuss some of the common applications for these techniques:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始详细研究降维和PCA之前，我们将讨论这些技术的一些常见应用：
- en: '**Pre-processing/feature engineering**: One of the most common implementations
    is in the pre-processing or feature engineering stages of developing a machine
    learning solution. The quality of the information provided during the algorithm
    development, as well as the correlation between the input data and the desired
    result, is critical in order for a high-performing solution to be designed. In
    this situation, PCA can provide assistance, as we are able to isolate the most
    important components of information from the data and provide this to the model
    so that only the most relevant information is being provided. This can also have
    a secondary benefit in that we have reduced the number of features being provided
    to the model, so there can be a corresponding reduction in the number of calculations
    to be completed. This can reduce the overall training time for the system.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理/特征工程**：这种方法最常见的应用是在机器学习解决方案开发的预处理或特征工程阶段。在算法开发过程中提供的信息质量，以及输入数据与期望结果之间的相关性，对于设计高性能的解决方案至关重要。在这种情况下，PCA可以提供帮助，因为我们能够从数据中提取出最重要的信息成分，并将其提供给模型，从而确保只提供最相关的信息。这还可以带来第二个好处，即我们减少了提供给模型的特征数量，因此计算量也能相应减少，这可以减少系统的整体训练时间。'
- en: '**Noise reduction**: Dimensionality reduction can also be used as an effective
    noise reduction/filtering technique. It is expected that the noise within a signal
    or dataset does not comprise a large component of the variation within the data.
    Thus, we can remove some of the noise from the signal by removing the smaller
    components of variation and then restoring the data back to the original dataspace.
    In the following example, the image on the left has been filtered to the first
    20 most significant sources of data, which gives us the image on the right. We
    can see that the quality of the image has reduced, but the critical information
    is still there:'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪音减少**：降维也可以作为一种有效的噪音减少/滤波技术。预期信号或数据集中的噪音并不占数据变异的主要成分。因此，我们可以通过去除变异较小的成分来去除信号中的一部分噪音，然后将数据恢复到原始数据空间。在以下示例中，左侧的图像已经过滤掉了最重要的20个数据源，生成了右侧的图像。我们可以看到图像质量有所降低，但关键信息仍然保留：'
- en: '![Figure 4.3: An image filtered with dimensionality reduction. Left: The original
    image (Photo by Arthur Brognoli from Pexels), Right: The filtered image](img/C12626_04_03.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3：经过维度减少滤波的图像。左：原始图像（照片由来自 Pexels 的Arthur Brognoli拍摄），右：滤波后的图像](img/C12626_04_03.jpg)'
- en: 'Figure 4.3: An image filtered with dimensionality reduction. Left: The original
    image (Photo by Arthur Brognoli from Pexels), Right: The filtered image'
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.3：经过维度减少滤波的图像。左：原始图像（照片由来自 Pexels 的Arthur Brognoli拍摄），右：滤波后的图像
- en: Note
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This photograph was taken by Arthur Brognoli from Pexels and is available for
    free use under [https://www.pexels.com/photo-license/](https://www.pexels.com/photo-license/).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这张照片是由来自 Pexels 的Arthur Brognoli拍摄，并可在[https://www.pexels.com/photo-license/](https://www.pexels.com/photo-license/)上免费使用。
- en: '**Generating plausible artificial datasets**: As PCA divides the dataset into
    the components of information (or variation), we can investigate the effects of
    each components or generate new dataset samples by adjusting the ratios between
    the eigenvalues. We can scale these components, which, in effect, increases or
    decreases the importance of that specific component. This is also referred to
    as **statistical shape modelling**, as one common method is to use it to create
    plausible variants of shapes. It is also used detect facial landmarks in images
    in the process of **active shape modelling**.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成可信的人工数据集**：由于PCA将数据集分解为信息（或变化）的组件，我们可以通过调整特征值之间的比率来研究每个组件的效果或生成新的数据集样本。我们可以缩放这些组件，从而增加或减少特定组件的重要性。这也被称为**统计形状建模**，因为其中一种常见方法是使用它来创建形状的合理变体。它还被用来在图像中检测面部特征点，这是**主动形状建模**过程中的一个步骤。'
- en: '**Financial modelling/risk analysis**: Dimensionality reduction provides a
    useful toolkit for the finance industry, as being able to consolidate a large
    number of individual market metrics or signals into a smaller number of components
    allows for faster, and more efficient computations. Similarly, the components
    can be used to highlight those higher-risk products/companies.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金融建模/风险分析**：降维为金融行业提供了一个有用的工具箱，因为能够将大量单独的市场指标或信号整合为较少的组件，可以加快和更高效地进行计算。同样，这些组件可以用来突出那些高风险的产品/公司。'
- en: The Curse of Dimensionality
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 维数灾难
- en: 'Before we can understand the benefits to using dimensionality reduction techniques,
    we must first understand why the dimensionality of feature sets need to be reduced
    at all. The **curse of dimensionality** is a phrase commonly used to describe
    issues that arise when working with data that has a high number of dimensions
    in the feature space; for example, the number of attributes that are collected
    for each sample. Consider a dataset of point locations within a game of *Pac-Man*.
    Your character, Pac-Man, occupies a position within the virtual world defined
    by two dimensions or coordinates (*x*, *y*). Let''s say that we are creating a
    new computer enemy: an AI-driven ghost to play against, and that it requires some
    information regarding our character to make its own game logic decisions. For
    the bot to be effective, we require the player''s position (*x*, *y*) and their
    velocity in each of the directions (*vx*, *vy*) in addition to the players last
    five (*x*, *y*) positions, the number of remaining hearts, and the number of remaining
    power-pellets in the maze (power-pellets temporarily allow Pac-Man to eat ghosts).
    Now, for each moment in time, our bot requires 16 individual features (or dimensions)
    to make its decisions. This is clearly a lot more than just the two dimensions
    as provided by the position.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们理解使用降维技术的好处之前，我们必须先了解为什么需要减少特征集的维度。**维数灾难**是一个常用的术语，用来描述在处理具有高维度特征空间的数据时出现的问题；例如，为每个样本收集的属性数量。考虑一个《吃豆人》游戏中点位置的数据集。你的角色吃豆人在虚拟世界中的位置由两个维度或坐标（*x*，*y*）定义。假设我们正在创建一个新的电脑敌人：一个由AI驱动的幽灵来对抗玩家，并且它需要一些关于我们角色的信息来做出自己的游戏逻辑决策。为了使这个机器人有效，我们需要玩家的位置（*x*，*y*）以及每个方向上的速度（*vx*，*vy*），此外还需要玩家最后五个（*x*，*y*）位置，剩余的心数以及迷宫中剩余的能量豆数（能量豆暂时允许吃豆人吃掉幽灵）。现在，对于每个时间点，我们的机器人需要16个单独的特征（或维度）来做出决策。显然，这比只提供位置的两个维度要多得多。
- en: '![](img/C12626_04_04.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_04_04.jpg)'
- en: 'Figure 4.4: Dimensions in a PacMan game'
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.4：《吃豆人》游戏中的维度
- en: To explain the concept of dimensionality reduction, we will consider a fictional
    dataset (see [*Figure 4.5*](C12626_04_ePub_Final_SZ.xhtml#_idTextAnchor082)) of
    *x* and *y* coordinates as features, giving two dimensions in the feature space.
    It should be noted that this example is by no means a mathematical proof, but
    is rather intended to provide a means of visualizing the effect of increased dimensionality.
    In this dataset, we have six individual samples (or points) and we can visualize
    the currently occupied volume within the feature space of approximately (3 – 1)
    x (4 – 2) = 2 x 2 = 4 squared units.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释降维的概念，我们将考虑一个虚构的数据集（见[*图 4.5*](C12626_04_ePub_Final_SZ.xhtml#_idTextAnchor082)），其中
    *x* 和 *y* 坐标作为特征，形成了特征空间中的两个维度。需要注意的是，这个例子绝不是数学证明，而是旨在提供一种可视化增加维度后影响的方式。在这个数据集中，我们有六个独立的样本（或点），我们可以可视化当前在特征空间内占据的体积，约为
    (3 - 1) x (4 - 2) = 2 x 2 = 4 平方单位。
- en: '![](img/C12626_04_05.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_04_05.jpg)'
- en: 'Figure 4.5: Data in a 2D feature space'
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.5：二维特征空间中的数据
- en: 'Suppose, the dataset comprises the same number of points, but with an additional
    feature (the *z* coordinate) to each sample. The occupied data volume is now approximately
    2 x 2 x 2 = 8 cubed units. So, we now have the same number of samples, but the
    space enclosing the dataset is now larger. As such, the data takes up less relative
    volume in the available space and is now sparser. This is the curse of dimensionality;
    as we increase the number of available features, we increase the sparsity of the
    data, and, in turn, make statistically valid correlations more difficult. Looking
    back to our example of creating a video game bot to play against a human player,
    we have 12 features that are a mix of different feature types: speed, velocity,
    acceleration, skill level, selected weapon, and available ammunition. Depending
    on the range of possible values for each of these features and the variance to
    the dataset provided by each feature, the data could be extremely sparse. Even
    within the constrained world of Pac-Man, the potential variance of each of the
    features could be quite large, some much larger than others.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据集包含相同数量的点，但每个样本都有一个额外的特征（*z* 坐标）。此时，所占数据体积大约为 2 x 2 x 2 = 8 立方单位。因此，我们现在有相同数量的样本，但包围数据集的空间变得更大。这样，数据在可用空间中占据的相对体积变小，数据变得更加稀疏。这就是维度灾难；随着可用特征数量的增加，数据的稀疏性增加，从而使得统计上有效的相关性变得更加困难。回到我们创建视频游戏机器人来与人类玩家对战的例子，我们有
    12 个特征，包含不同类型的特征：速度、速度变化、加速度、技能水平、选择的武器和可用弹药。根据这些特征的可能值范围以及每个特征对数据集方差的贡献，数据可能非常稀疏。即使在受限的吃豆人世界中，每个特征的潜在方差也可能非常大，有些特征的方差远大于其他特征。
- en: So, without dealing with the sparsity of the dataset, we have more information
    with the additional feature(s), but may not be able to improve the performance
    of our machine learning model, as the statistical correlations are more difficult.
    What we would like to do is keep the useful information provided by the extra
    features but minimize the negative effect of sparsity. This is exactly what dimensionality
    reduction techniques are designed to do and these can be extremely powerful in
    increasing the performance of your machine learning model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在不处理数据集稀疏性的情况下，我们通过额外的特征获得了更多信息，但可能无法提高机器学习模型的性能，因为统计相关性变得更加困难。我们希望做的是保留额外特征提供的有用信息，同时最小化稀疏性的负面影响。这正是降维技术的设计目的，而这些技术在提高机器学习模型性能方面可能非常强大。
- en: Throughout this chapter, we will discuss a number of different dimensionality
    reduction techniques and will cover one of the most important and useful methods,
    PCA, in greater detail with a worked example.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论多种降维技术，并将在更详细的工作示例中介绍其中一种最重要和有用的方法——主成分分析（PCA）。
- en: Overview of Dimensionality Reduction Techniques
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 降维技术概述
- en: 'As discussed in the Introduction section, the goal of any dimensionality reduction
    technique is to manage the sparsity of the dataset while keeping the useful information
    that is provided, so dimensionality reduction is typically an important pre-processing
    step used before a classification stage. Most dimensionality reduction techniques
    aim to complete this task using a process of **feature projection**, which adjusts
    the data from the higher dimensional space into a space with fewer dimensions
    to remove the sparsity from the data. Again, as a means of visualizing the projection
    process, consider a sphere in a 3D space. We can project the sphere into lower
    2D space into a circle with some information loss (the value for the *z* coordinate)
    but retaining much of the information that describes its original shape. We still
    know the origin, radius, and manifold (outline) of the shape, and it is still
    very clear that it is a circle. So, if we were given just the 2D projection, it
    would also be possible to re-create the original 3D shape with this information.
    So, depending upon the problem that we are trying to solve, we may have reduced
    the dimensionality while retaining the important information:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在引言部分所讨论的，任何降维技术的目标都是在保持提供的有用信息的同时管理数据集的稀疏性，因此降维通常是分类阶段前的一个重要预处理步骤。大多数降维技术旨在通过**特征投影**的过程来完成这一任务，将数据从高维空间调整到较低维度的空间，以去除数据的稀疏性。再次通过可视化投影过程来理解这一点，可以考虑在三维空间中的一个球体。我们可以将球体投影到二维空间，变成一个圆形，虽然会有一些信息丢失（*z*
    坐标的值），但仍保留了描述其原始形状的大部分信息。我们仍然知道原点、半径和流形（轮廓），而且仍然非常清楚它是一个圆。因此，如果我们仅仅得到了二维投影，凭借这些信息也能够重新构建原始的三维形状。所以，根据我们尝试解决的问题，我们可能已经在保留重要信息的同时减少了维度：
- en: '![Figure 4.6: A projection of a 3D sphere into a 2D space](img/C12626_04_06.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6：将一个三维球体投影到二维空间](img/C12626_04_06.jpg)'
- en: 'Figure 4.6: A projection of a 3D sphere into a 2D space'
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.6：将一个三维球体投影到二维空间
- en: The secondary benefit that can be obtained by pre-processing the dataset with
    a dimensionality reduction stage is the improved computational performance that
    can be achieved. As the data has been projected into a lower dimensional space,
    it will contain fewer, but potentially more powerful, features. The fact that
    there are fewer features means that, during later classification or regression
    stages, the size of the dataset being processed is significantly smaller. This
    will potentially reduce the required system resources and processing time for
    classification/regression, and, in some cases, the dimensionality reduction technique
    can also be used directly to complete the analysis..
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在降维阶段对数据集进行预处理，能够获得的附加好处是提高的计算性能。由于数据已经被投影到较低维度的空间，它将包含更少但可能更强大的特征。特征较少意味着在后续的分类或回归阶段，处理的数据集的大小显著较小。这将可能减少分类/回归所需的系统资源和处理时间，在某些情况下，降维技术还可以直接用于完成分析。
- en: This analogy also introduces one of the important considerations of dimensionality
    reduction. We are always trying to balance the information loss resulting from
    the projection into lower dimensional space with reducing the sparsity of the
    data. Depending upon the nature of the problem and the dataset being used, the
    correct balance could present itself and be relatively straightforward. In some
    applications, this decision may rely on the outcome of additional validation methods,
    such as cross-validation (particularly in supervised learning problems) or the
    assessment of experts in your problem domain.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类比还引入了降维的一个重要考虑因素。我们总是试图在将数据投影到低维空间时，平衡信息丢失和减少数据稀疏性之间的关系。根据问题的性质和使用的数据集，正确的平衡可能会自然出现，并且相对直接。在某些应用中，这个决策可能依赖于额外验证方法的结果，比如交叉验证（特别是在监督学习问题中）或领域专家的评估。
- en: One way we like to think about this trade-off in dimensionality reduction is
    to consider compressing a file or image on a computer for transfer. Dimensionality
    reduction techniques, such as PCA, are essentially methods of compressing information
    into a smaller size for transfer, and, in many compression methods, some losses
    occur as a result of the compression process. Sometimes, these losses are acceptable;
    if we are transferring a 50 MB image and need to shrink it to 5 MB for transfer,
    we can expect to still be able to see the main subject of the image, but perhaps
    some smaller background features will become too blurry to see. We would also
    not expect to be able to restore the original image to a pixel-perfect representation
    from the compressed version, but we could expect to restore it with some additional
    artefacts, such as blurring.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们喜欢将降维中的这一权衡方式比作在计算机上传输文件或图像时的压缩过程。降维技术，如PCA，实质上是将信息压缩成较小的大小以便传输，而在许多压缩方法中，压缩过程中会发生一些信息丢失。有时，这些丢失是可以接受的；例如，如果我们要传输一张50MB的图像并需要将其压缩到5MB，我们可以预期仍然能够看到图像的主要内容，但一些较小的背景细节可能会变得模糊不清。我们也不会期望从压缩后的图像恢复出完全无损的原始图像，但可以期望在恢复时会出现一些附加的伪影，比如模糊。
- en: Dimensionality Reduction and Unsupervised Learning
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 降维与无监督学习
- en: 'Dimensionality reduction techniques have many uses in machine learning, as
    the ability to extract the useful information of a dataset can provide performance
    boosts in many machine learning problems. They are particularly useful in unsupervised
    learning as opposed to supervised learning methods, as the dataset does not contain
    any ground truth labels or targets to achieve. In unsupervised learning, the training
    environment is being used to organize the data in a way that is appropriate for
    the problem being solved (for example, clustering in a classification problem),
    which is typically based on the most important information in the dataset. Dimensionality
    reduction provides an effective means of extracting the important information,
    and, as there are a number of different methods that we could use, it is beneficial
    to review some of the available options:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 降维技术在机器学习中有许多用途，因为能够提取数据集中的有用信息可以在许多机器学习问题中提高性能。与监督学习方法不同，降维技术在无监督学习中尤其有用，因为数据集不包含任何实际标签或目标。无监督学习中，训练环境用于以适合问题解决的方式组织数据（例如，分类问题中的聚类），这种组织方式通常基于数据集中的最重要信息。降维提供了提取重要信息的有效手段，且由于我们可以使用多种方法，因此回顾一些可用选项是有益的：
- en: '**Linear Discriminant Analysis** (**LDA**): This is a particularly handy technique
    that can be used for both classification as well as dimensionality reduction.
    LDA will be covered in more detail in *Chapter 7*: *Topic Modeling*.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性判别分析**（**LDA**）：这是一种非常实用的技术，既可以用于分类，也可以用于降维。LDA将在*第7章*中更详细地讲解：*主题建模*。'
- en: '**Non-negative matrix factorization** (**NNMF**): Like many of the dimensionality
    reduction techniques, this relies upon the properties of linear algebra to reduce
    the number of features in the dataset. NNMF will also be covered in more detail
    in *Chapter 7*, *Topic Modeling*.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非负矩阵分解**（**NNMF**）：与许多降维技术一样，这种方法依赖于线性代数的性质来减少数据集中的特征数量。NNMF也将在*第7章*，*主题建模*中进行更详细的讨论。'
- en: '**Singular Value Decomposition** (**SVD**): This is somewhat related to PCA
    (which is covered in more detail in this chapter) and is also a matrix decomposition
    process not too dissimilar to NNMF.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奇异值分解**（**SVD**）：这与PCA（本章中将详细讨论）有些相关，也是一个矩阵分解过程，与NNMF并无太大不同。'
- en: '**Independant Component Analysis** (**ICA**): This also shares some similarities
    to SVD and PCA, but relaxing the assumption of the data being a Gaussian distribution
    allows for non-Gaussian data to be separated.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立成分分析**（**ICA**）：这与SVD和PCA有一些相似之处，但通过放宽数据为高斯分布的假设，可以实现非高斯数据的分离。'
- en: Each of the methods described so far all use linear separation to reduce the
    sparsity of the data in their original implementation. Some of these methods also
    have variants that use non-linear kernel functions in the separation process,
    providing the ability to reduce the sparsity in a non-linear fashion. Depending
    on the dataset being used, a non-linear kernel may be more effective at extracting
    the most useful information from the signal.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止描述的每种方法都使用线性分离来减少数据在其原始实现中的稀疏性。一些方法还有使用非线性核函数的变体，能够以非线性的方式减少稀疏性。根据所使用的数据集，非线性核可能在从信号中提取最有用的信息方面更为有效。
- en: PCA
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）
- en: As we described previously, PCA is a commonly used and very effective dimensionality
    reduction technique, which often forms a pre-processing stage for a number of
    machine learning models and techniques. For this reason, we will dedicate this
    section of the book to looking at PCA in more detail than any of the other methods.
    PCA reduces the sparsity in the dataset by separating the data into a series of
    components where each component represents a source of information within the
    data. As its name suggests, the first component produced in PCA, **the principal
    component** comprises the majority of information or variance within the data.
    The principal component can often be thought of as contributing the most amount
    of interesting information in addition to the mean. With each subsequent component,
    less information, but more subtlety, is contributed to the compressed data. If
    we consider all of these components together, there will be no benefit from using
    PCA, as the original dataset will be returned. To clarify this process and the
    information returned by PCA, we will use a worked example, completing the PCA
    calculations by hand. But first, we must review some foundational statistical
    concepts, which are required to execute the PCA calculations.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，PCA是一种常用且非常有效的降维技术，通常作为许多机器学习模型和技术的预处理阶段。因此，我们将在本书中专门花一章更详细地探讨PCA，超越其他方法。PCA通过将数据分解为一系列组件来减少数据的稀疏性，每个组件代表数据中的一个信息源。顾名思义，PCA中产生的第一个组件，**主成分**，包含了数据中大部分的信息或方差。主成分通常可以被认为是除了均值之外，贡献最多有趣信息的部分。随着每个后续组件的加入，数据中传递的信息减少，但更加微妙。如果我们将所有这些组件都考虑在内，使用PCA将没有任何好处，因为它将恢复原始数据集。为了澄清这个过程以及PCA返回的信息，我们将使用一个实际的例子，通过手动完成PCA计算。但首先，我们需要回顾一些基础的统计学概念，这些概念是进行PCA计算所必需的。
- en: Mean
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 均值
- en: The mean, or the average value, is simply the addition of all values divided
    by the number of values in the set.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 均值，或称平均值，简单来说就是将所有值相加后，除以数据集中值的数量。
- en: Standard Deviation
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准差
- en: Often referred to as the spread of the data and related to the variance, the
    standard deviation is a measure of how much of the data lies within proximity
    to the mean. In a normally distributed dataset, approximately 68% of the dataset
    lies within one standard deviation of the mean.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵通常被称为数据的分布，与方差相关，标准差是衡量数据与均值的接近程度的指标。在正态分布的数据集中，大约68%的数据位于均值的一个标准差范围内。
- en: The relationship between the variance and standard deviation is quite a simple
    one – the variance is the standard deviation squared.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 方差与标准差之间的关系相当简单——方差是标准差的平方。
- en: Covariance
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 协方差
- en: Where standard deviation or variance is the spread of the data calculated on
    a single dimension, the covariance is the variance of one dimension (or feature)
    against another. When the covariance of a dimension is computed against itself,
    the result is the same as simply calculating the variance for the dimension.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当标准差或方差是计算单一维度数据的分布时，协方差是一个维度（或特征）与另一个维度的方差。当一个维度的协方差与其自身计算时，结果与仅计算该维度的方差相同。
- en: Covariance Matrix
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 协方差矩阵
- en: A covariance matrix is a matrix representation of the possible covariance values
    that can be computed for a dataset. Other than being particularly useful in data
    exploration, covariance matrices are also required for executing the PCA of a
    dataset. To determine the variance of one feature with respect to another, we
    simply look up the corresponding value in the covariance matrix. Referring to
    *Figure 4.7* we can see that, in column 1, row 2, the value is the variance of
    feature or dataset *Y* with respect to *X* (*cov(Y, X))*. We can also see that
    there is a diagonal column of covariance values computed against the same feature
    or dataset; for example, *cov(X, X)*. In this situation, the value is simply the
    variance of *X*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵是可以计算数据集协方差值的矩阵表示。除了在数据探索中非常有用外，协方差矩阵在执行PCA（主成分分析）时也是必需的。为了确定一个特征相对于另一个特征的方差，我们只需查找协方差矩阵中对应的值。参见*图4.7*，我们可以看到，在第1列、第2行，值是特征或数据集*Y*相对于*X*的方差（*cov(Y,
    X))*。我们还可以看到，有一列协方差值是针对同一特征或数据集计算的；例如，*cov(X, X)*。在这种情况下，值就是*X*的方差。
- en: '![Figure 4.7: The covariance matrix](img/C12626_04_07.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图4.7：协方差矩阵](img/C12626_04_07.jpg)'
- en: 'Figure 4.7: The covariance matrix'
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.7：协方差矩阵
- en: Typically, the exact values of each of the covariances are not as interesting
    as looking at the magnitude and relative size of each of the covariances within
    the matrix. A large value of the covariance of one feature against another would
    suggest that one feature changes significantly with respect to the other, while
    a value close to zero would signify very little change. The other interesting
    aspect of the covariance to look for is the sign associated with the covariance;
    a positive value indicates that as one feature increases or decreases then so
    does the other, while a negative covariance indicates that the two features diverge
    from each other with one increasing as the other decreases or vice versa.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，每个协方差的具体数值并不像观察矩阵中每个协方差的大小和相对大小那样有趣。某个特征与另一个特征的协方差较大，意味着一个特征与另一个特征有显著的变化，而接近零的值则表示变化极小。另一个值得关注的协方差特性是其符号；正值表示当一个特征增加或减少时，另一个特征也随之增加或减少，而负协方差则表示两个特征相互背离，一个增加时另一个减少，反之亦然。
- en: Thankfully, `numpy` and `scipy` provide functions to efficiently make these
    calculations for you. In the next exercise, we will compute these values in Python.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 值得庆幸的是，`numpy`和`scipy`提供了高效的函数来为你完成这些计算。在下一个练习中，我们将使用Python来计算这些值。
- en: 'Exercise 11: Understanding the Foundational Concepts of Statistics'
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习11：理解统计学基础概念
- en: 'In this exercise, we will briefly review how to compute some of the foundational
    statistical concepts using both the `numpy` and `pandas` Python packages. In this
    exercise, we will use dataset of measurements of different Iris flower species,
    created in 1936 by the British biologist and statistician Sir Ronald Fisher. The
    dataset, which can be found in the accompanying source code, comprises four individual
    measurements (sepal width and length, and petal width and length) of three different
    Iris flower varieties: Iris setosa, Iris versicolor, and Iris virginica.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将简要回顾如何使用`numpy`和`pandas`这两个Python包来计算一些基础的统计概念。在本练习中，我们将使用一个包含不同鸢尾花物种测量数据集，该数据集由英国生物学家和统计学家罗纳德·费舍尔爵士于1936年创建。该数据集可以在随附的源代码中找到，包含了三种不同鸢尾花品种（鸢尾花Setosa、鸢尾花Versicolor和鸢尾花Virginica）的四个独立测量值（花萼宽度和长度，花瓣宽度和长度）。
- en: Note
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
    It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise11](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise11).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来自 [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/)。它可以从
    [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise11](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise11)
    下载。
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: UCI机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚州欧文市：加利福尼亚大学信息与计算机科学学院。
- en: 'The steps to be performed are as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 执行的步骤如下：
- en: 'Import the `pandas`, `numpy`, and `matplotlib` packages for use:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas`、`numpy` 和 `matplotlib` 包以供使用：
- en: '[PRE0]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load the dataset and preview the first five lines of data:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集并预览前五行数据：
- en: '[PRE1]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output is as follows:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.8: The head of the data](img/C12626_04_08.jpg)'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.8：数据的头部](img/C12626_04_08.jpg)'
- en: 'Figure 4.8: The head of the data'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.8：数据的头部
- en: 'We only require the `Sepal Length` and `Sepal Width` features, so remove the
    other columns:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只需要 `Sepal Length` 和 `Sepal Width` 特征，因此删除其他列：
- en: '[PRE2]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output is as follows:'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.9: The head after cleaning the data](img/C12626_04_09.jpg)'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.9：清洗后的数据头部](img/C12626_04_09.jpg)'
- en: 'Figure 4.9: The head after cleaning the data'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.9：清洗后的数据头部
- en: 'Visualize the dataset by plotting the `Sepal Length` versus `Sepal Width` values:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过绘制 `Sepal Length` 与 `Sepal Width` 的值来可视化数据集：
- en: '[PRE3]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is as follows:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.10: Plot of the data](img/C12626_04_10.jpg)'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.10：数据的图示](img/C12626_04_10.jpg)'
- en: 'Figure 4.10: Plot of the data'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.10：数据的图示
- en: 'Compute the mean value using the `pandas` method:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `pandas` 方法计算均值：
- en: '[PRE4]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is as follows:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE5]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Compute the mean value using the `numpy` method:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `numpy` 方法计算均值：
- en: '[PRE6]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE7]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Compute the standard deviation value using the `pandas` method:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `pandas` 方法计算标准差值：
- en: '[PRE8]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE9]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Compute the standard deviation value using the `numpy` method:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `numpy` 方法计算标准差值：
- en: '[PRE10]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE11]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Compute the variance values using the `pandas` method:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `pandas` 方法计算方差值：
- en: '[PRE12]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE13]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Compute the variance values using the `numpy` method:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `numpy` 方法计算方差值：
- en: '[PRE14]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE15]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Compute the covariance matrix using the `pandas` method:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `pandas` 方法计算协方差矩阵：
- en: '[PRE16]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.11: Covariance matrix using the Pandas method](img/C12626_04_11.jpg)'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.11：使用 Pandas 方法的协方差矩阵](img/C12626_04_11.jpg)'
- en: 'Figure 4.11: Covariance matrix using the Pandas method'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.11：使用 Pandas 方法的协方差矩阵
- en: 'Compute the covariance matrix using the `numpy` method:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `numpy` 方法计算协方差矩阵：
- en: '[PRE17]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.12: The covariance matrix using the NumPy method](img/C12626_04_12.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.12：使用 NumPy 方法的协方差矩阵](img/C12626_04_12.jpg)'
- en: 'Figure 4.12: The covariance matrix using the NumPy method'
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.12：使用 NumPy 方法的协方差矩阵
- en: Now that we know how to compute the foundational statistic values, we will turn
    our attention to the remaining components of PCA.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何计算基础的统计值，接下来我们将重点讨论 PCA 的其他组成部分。
- en: Eigenvalues and Eigenvectors
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征值和特征向量
- en: 'The mathematical concept of eigenvalues and eigenvectors is a very important
    one in the fields of physics and engineering, and they also form the final steps
    in computing the principal components of a dataset. The exact mathematical definition
    of eigenvalues and eigenvectors is outside the scope of this book, as it is quite
    involved and requires a reasonable understanding of linear algebra. The linear
    algebra equation to decompose a dataset (*a)* into eigenvalues (*S*) and eigenvectors
    (*U*) is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值和特征向量的数学概念在物理学和工程学领域中非常重要，它们也是计算数据集主成分的最后步骤。特征值和特征向量的精确定义超出了本书的范围，因为它涉及较为复杂的内容，并且需要有一定的线性代数基础。将数据集
    (*a)* 分解为特征值 (*S*) 和特征向量 (*U*) 的线性代数方程如下：
- en: '![](img/C12626_04_13.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_04_13.jpg)'
- en: 'Figure 4.13: An eigenvector/eigenvalue decomposition'
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.13：特征向量/特征值分解
- en: In *Figure 4.13*, *U* and *V* are related as the left and right values of dataset
    *a*. If *a* has the shape *m x n*, then *U* will contain values in the shape *m
    x m* and *V* in the shape *n x n*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 4.13* 中，*U* 和 *V* 作为数据集 *a* 的左右值相关。如果 *a* 的形状为 *m x n*，则 *U* 将包含形状为 *m
    x m* 的值，*V* 的形状为 *n x n*。
- en: 'Put simply, in the context of PCA:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在 PCA 的上下文中：
- en: '**Eigenvectors** (*U*) are the components contributing information to the dataset
    as described in the first paragraph of this section on principal components. Each
    eigenvector describes some amount of variability within the dataset.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征向量** (*U*) 是对数据集做出信息贡献的成分，如本节第一段所述的主成分。每个特征向量描述了数据集中的某种变异性。'
- en: '**Eigenvalues** (*S*) are the individual values that describe how much contribution
    each eigenvector provides to the dataset. As we described previously, the signal
    eigenvector that describes the largest contribution is referred to as the principal
    component, and as such, will have the largest eigenvalue. Accordingly, the eigenvector
    with the smallest eigenvalue contributes the least amount of variance or information
    to the data.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征值** (*S*) 是描述每个特征向量对数据集贡献多少的单独数值。如我们之前所述，描述最大贡献的信号特征向量称为主成分，因此它将具有最大的特征值。因此，具有最小特征值的特征向量对数据的方差或信息贡献最少。'
- en: 'Exercise 12: Computing Eigenvalues and Eigenvectors'
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 12：计算特征值和特征向量
- en: 'As we discussed previously, deriving and computing the eigenvalues and eigenvectors
    manually is a little involved and is not in the scope of this book. Thankfully,
    `numpy` provides all the functionality for us to compute these values. Again,
    we will use the Iris dataset for this example:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所讨论的，手动推导和计算特征值及特征向量稍显复杂，并且超出了本书的范围。幸运的是，`numpy` 为我们提供了计算这些值的所有功能。再次说明，我们将使用
    Iris 数据集作为示例：
- en: Note
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来自 [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/)。
- en: It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise12](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise12).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从 [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise12](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise12)
    下载。
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: UCI 机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚州尔湾：加利福尼亚大学信息与计算机科学学院。
- en: 'Import the `pandas` and `numpy` packages:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas` 和 `numpy` 包：
- en: '[PRE18]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Load the dataset:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集：
- en: '[PRE19]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 4.14: The first five rows of the dataset](img/C12626_04_14.jpg)'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.14: 数据集的前五行](img/C12626_04_14.jpg)'
- en: 'Figure 4.14: The first five rows of the dataset'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.14: 数据集的前五行'
- en: 'Again, we only require the `Sepal Length` and `Sepal Width` features, so remove
    the other columns:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，我们只需要`花萼长度`和`花萼宽度`特征，因此删除其他列：
- en: '[PRE20]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is as follows:'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 4.15: The Sepal Length and Sepal Width feature](img/C12626_04_15.jpg)'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.15: 花萼长度和花萼宽度特征](img/C12626_04_15.jpg)'
- en: 'Figure 4.15: The Sepal Length and Sepal Width feature'
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.15: 花萼长度和花萼宽度特征'
- en: 'From NumPy''s linear algebra module, use the single value decomposition function
    to compute the `eigenvalues` and `eigenvectors`:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 NumPy 的线性代数模块中，使用单值分解函数来计算`特征值`和`特征向量`：
- en: '[PRE21]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The use of the `full_matrices=False` function argument is a flag for the function
    to return the eigenvectors in the shape we need; that is, # Samples x # Features.'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '使用`full_matrices=False`函数参数是一个标志，表示函数返回我们需要形状的特征向量；即：# 样本 x # 特征。'
- en: 'Look at the eigenvalues; we can see that the first value is the largest, so
    the first eigenvector contributes the most information:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察特征值，我们可以看到第一个值是最大的，因此第一个特征向量贡献了最多的信息：
- en: '[PRE22]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output is as follows:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE23]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'It is handy to look at eigenvalues as a percentage of the total variance within
    the dataset. We will use a cumulative sum function to do this:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察特征值作为数据集总方差的百分比非常方便。我们将使用累积和函数来实现这一点：
- en: '[PRE24]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is as follows:'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE25]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Divide by the last or maximum value to convert to a percentage:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除以最后一个或最大值来转换为百分比：
- en: '[PRE26]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is as follows:'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE27]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We can see here that the first (or principal) component comprises 92% of the
    variation within the data, and thus, most of the information.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，这里第一个（或主）成分包含了数据中92%的变化量，因此包含了大部分信息。
- en: 'Now, let''s look at the `eigenvectors`:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看 `特征向量`：
- en: '[PRE28]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'A section of the output is as follows:'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的一部分如下所示：
- en: '![](img/C12626_04_16.jpg)'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12626_04_16.jpg)'
- en: 'Figure 4.16: Eigenvectors'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.16: 特征向量'
- en: 'Confirm that the shape of the eigenvector matrix is in the for `# Samples x
    # Features`; that is, `150` x `2`:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '确认特征向量矩阵的形状为 `# 样本 x # 特征`；即，`150` x `2`：'
- en: '[PRE29]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output is as follows:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE30]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'So, from the eigenvalues, we saw that the principal component was the first
    eigenvector. Look at the values for the first eigenvector:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，从特征值中我们可以看出，第一个特征向量是主成分。看看第一个特征向量的值：
- en: '[PRE31]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE32]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We have decomposed the dataset down into the principal components, and, using
    the eigenvectors, we can further reduce the dimensionality of the available data.
    In the later examples, we will consider PCA and apply this technique to an example
    dataset.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将数据集分解为主成分，并且利用特征向量，我们可以进一步减少可用数据的维度。在后续的示例中，我们将考虑PCA并将该技术应用于示例数据集。
- en: The Process of PCA
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PCA的过程
- en: Now we have all of the pieces ready to complete PCA to reduce the number of
    dimensions in a dataset.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好所有步骤来完成PCA，减少数据集的维度。
- en: 'The overall algorithm for completing PCA is as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 完成PCA的总体算法如下：
- en: Import the required Python packages (`numpy` and `pandas`).
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的Python包（`numpy`和`pandas`）。
- en: Load the entire dataset.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载整个数据集。
- en: From the available data, select the features that you wish to use in the dimensionality
    reduction.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从可用数据中选择你希望用于降维的特征。
- en: Note
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: If there is a significant difference in the scale between the features of the
    dataset; for example, one feature ranges in values between 0 and 1, and another
    between 100 and 1,000, you may need to normalize one of the features, as such
    differences in magnitude can eliminate the effect of the smaller features. In
    such a situation, you may need to divide the larger feature into its maximum value.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果数据集的特征之间存在显著的尺度差异，例如，一个特征的值范围在0到1之间，而另一个在100到1,000之间，你可能需要对其中一个特征进行归一化，因为这种量级差异会消除较小特征的影响。在这种情况下，你可能需要将较大特征除以其最大值。
- en: 'As an example, have a look at this:'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 举个例子，看看这个：
- en: '`x1 = [0.1, 0.23, 0.54, 0.76, 0.78]`'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`x1 = [0.1, 0.23, 0.54, 0.76, 0.78]`'
- en: '`x2 = [121, 125, 167, 104, 192]`'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`x2 = [121, 125, 167, 104, 192]`'
- en: '`x2 = x2 / np.max(x2) # Normalise x2 to be between 0 and 1`'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`x2 = x2 / np.max(x2) # 将x2归一化到0和1之间`'
- en: Compute the `covariance` matrix of the selected (and possibly normalized) data.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所选（并可能已归一化）数据的`协方差`矩阵。
- en: Compute the eigenvalues and eigenvectors of the `covariance` matrix.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算`协方差`矩阵的特征值和特征向量。
- en: Sort the eigenvalues (and corresponding eigenvectors) from highest to lowest
    value eigenvalue.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按从高到低的顺序对特征值（及其对应的特征向量）进行排序。
- en: Compute the eigenvalues as a percentage of the total variance within the dataset.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算特征值在数据集总方差中的百分比。
- en: Select the number of eigenvalues (and corresponding eigenvectors) required to
    comprise a pre-determined value of a minimum composition variance.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择所需的特征值（及其对应的特征向量）数量，以组成一个预定的最小组成方差值。
- en: Note
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: At this stage, the sorted eigenvalues represent a percentage of the total variance
    within the dataset. As such, we can use these values to select the number of eigenvectors
    required, either for the problem being solved or to sufficiently reduce the size
    of the dataset being applied in the model. For example, say that we required at
    least 90% of the variance to be accounted for within the output of PCA. We would
    then select the number of eigenvalues (and corresponding eigenvectors) that comprise
    at least 90% of the variance.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这一阶段，排序后的特征值表示数据集总方差的百分比。因此，我们可以利用这些值来选择所需的特征向量的数量，无论是为了解决问题，还是为了充分减少应用于模型的数据集的大小。例如，假设我们要求PCA输出中至少包含90%的方差。那么，我们将选择那些包含至少90%方差的特征值（及其对应的特征向量）的数量。
- en: Multiply the dataset by the selected eigenvectors and you have completed a PCA,
    reducing the number of features representing the data.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集与选定的特征向量相乘，你就完成了PCA，减少了表示数据的特征数量。
- en: Plot the result.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制结果。
- en: Before moving on to the next exercise, note that **transpose** is a term from
    linear algebra that means to swap the rows with the columns and vice versa. Say
    we had a matrix of ![](img/C12626_04_Formula_01.png), then the transpose of *X*
    would be ![](img/C12626_04_Formula_02.png).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行下一个练习之前，请注意，**转置**是线性代数中的一个术语，意思是将行和列互换。假设我们有一个矩阵 ![](img/C12626_04_Formula_01.png)，那么
    *X* 的转置将是 ![](img/C12626_04_Formula_02.png)。
- en: 'Exercise 13: Manually Executing PCA'
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习13：手动执行PCA
- en: 'For this exercise, we will be completing PCA manually, again using the Iris
    dataset. For this example, we want to sufficiently reduce the number of dimensions
    within the dataset to comprise at least 75% of the available variance:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将手动完成主成分分析（PCA），再次使用鸢尾花数据集。在这个例子中，我们希望将数据集中的维度数减少到足以包含至少 75% 的可用方差：
- en: Note
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
    It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise13](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise13).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集取自 [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/)。可以从
    [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise13](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise13)
    下载。
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: UCI 机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚州欧文：加利福尼亚大学信息与计算机科学学院。
- en: 'Import the `pandas` and `numpy` packages:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas` 和 `numpy` 包：
- en: '[PRE33]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Load the dataset:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集：
- en: '[PRE34]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output is as follows:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.17: The first five rows of the dataset](img/C12626_04_17.jpg)'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.17：数据集的前五行](img/C12626_04_17.jpg)'
- en: 'Figure 4.17: The first five rows of the dataset'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.17：数据集的前五行
- en: 'Again, we only require the `Sepal Length` and `Sepal Width` features, so remove
    the other columns. In this example, we are not normalizing the selected dataset:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，我们只需要 `花萼长度` 和 `花萼宽度` 特征，因此去除其他列。在这个例子中，我们没有对所选数据集进行归一化：
- en: '[PRE35]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is as follows:'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.18: The sepal length and sepal width feature](img/C12626_04_18.jpg)'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.18：花萼长度和花萼宽度特征](img/C12626_04_18.jpg)'
- en: 'Figure 4.18: The sepal length and sepal width feature'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.18：花萼长度和花萼宽度特征
- en: 'Compute the `covariance` matrix for the selected data. Note that we need to
    take the transpose of the `covariance` matrix to ensure that it is based on the
    number of features (2) and not samples (150):'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所选数据的`协方差`矩阵。请注意，我们需要对`协方差`矩阵进行转置，以确保它基于特征数（2）而不是样本数（150）：
- en: '[PRE36]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output is as follows:'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/C12626_04_19.jpg)'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12626_04_19.jpg)'
- en: 'Figure 4.19: The covariance matrix for the selected data'
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.19：所选数据的协方差矩阵
- en: 'Compute the eigenvectors and eigenvalues for the covariance matrix, Again,
    use the `full_matrices` function argument:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算协方差矩阵的特征向量和特征值。再次使用`full_matrices`函数参数：
- en: '[PRE37]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'What are the eigenvalues? These are returned sorted from the highest to lowest
    value:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征值是什么？这些特征值按从高到低的顺序返回：
- en: '[PRE38]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is as follows:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE39]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: What are the corresponding eigenvectors?
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对应的特征向量是什么？
- en: '[PRE40]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output is as follows:'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/C12626_04_20.jpg)'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12626_04_20.jpg)'
- en: 'Figure 4.20: Eigenvectors'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.20：特征向量
- en: 'Compute the eigenvalues as a percentage of the variance within the dataset:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算特征值作为数据集中方差的百分比：
- en: '[PRE41]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is as follows:'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE42]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: As per the introduction to the exercise, we need to describe the data with at
    least 75% of the available variance. As per *Step* [*7*](C12626_04_ePub_Final_SZ.xhtml#_idTextAnchor097),
    the principal component comprises 78% of the available variance. As such, we require
    only the principal component from the dataset. What are the principal components?
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据练习介绍，我们需要描述至少包含 75% 可用方差的数据。根据*步骤* [*7*](C12626_04_ePub_Final_SZ.xhtml#_idTextAnchor097)，主成分包含
    78% 的可用方差。因此，我们只需要数据集中的主成分。主成分是什么？
- en: '[PRE43]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output is as follows:'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE44]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Now, we can apply the dimensionality reduction process. Execute a matrix multiplication
    of the principal component with the transpose of the dataset.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们可以应用降维过程。执行主成分与数据集转置矩阵的矩阵乘法。
- en: Note
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The dimensionality reduction process is a matrix multiplication of the selected
    eigenvectors and the data to be transformed.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 降维过程是所选特征向量与待转换数据的矩阵乘法。
- en: 'Without taking the transpose of the `df.values` matrix, multiplication could
    not occur:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果不对`df.values`矩阵进行转置，就无法进行矩阵乘法：
- en: '[PRE45]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'A section of the output is as follows:'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的一部分如下：
- en: '![](img/C12626_04_21.jpg)'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12626_04_21.jpg)'
- en: 'Figure 4.21: The result of matrix multiplication'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.21：矩阵乘法结果
- en: Note
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The transpose of the dataset is required to execute matrix multiplication, as
    the **inner dimensions of the matrix must be the same** for matrix multiplication
    to occur. For **A.dot(B)** to be valid, **A** must have the shape *m x n* and
    **B** must have the shape *n x p*. In this example, the inner dimensions of **A**
    and **B** are both *n*.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了执行矩阵乘法，数据集的转置是必需的，因为**矩阵的内维必须相同**才能进行矩阵乘法。为了使 **A.dot(B)** 有效，**A** 必须具有 *m
    x n* 的形状，**B** 必须具有 *n x p* 的形状。在本例中，**A** 和 **B** 的内维都是 *n*。
- en: 'In the following example, the output of the PCA is a single-column, 150-sample
    dataset. As such, we have just reduced the size of the initial dataset by half,
    comprising approximately 79% of the variance within the data:'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在以下示例中，PCA 的输出是一个单列、150 个样本的数据集。因此，我们只是将初始数据集的大小减少了一半，包含了数据中约 79% 的方差：
- en: '![Figure 4.22: The output of PCA](img/C12626_04_22.jpg)'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.22: PCA 输出结果](img/C12626_04_22.jpg)'
- en: 'Figure 4.22: The output of PCA'
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.22: PCA 输出结果'
- en: 'Plot the values of the principal component:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制主成分的值：
- en: '[PRE46]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output is as follows:'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.23: The Iris dataset transformed by using a manual PCA](img/C12626_04_23.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.23: 使用手动 PCA 转换后的 Iris 数据集](img/C12626_04_23.jpg)'
- en: 'Figure 4.23: The Iris dataset transformed by using a manual PCA'
  id: totrans-269
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.23: 使用手动 PCA 转换后的 Iris 数据集'
- en: In this exercise, we simply computed the covariance matrix of the dataset without
    applying any transformations to the dataset beforehand. If the two features have
    roughly the same mean and standard deviation, this is perfectly fine. However,
    if one feature is much larger in value (and has a somewhat different mean) than
    the other, then this feature may dominate the other when decomposing into components.
    This could have the effect of removing the information provided by the smaller
    feature altogether. One simple normalization technique before computing the covariance
    matrix would be to subtract the respective means from the features, thus centering
    the dataset around zero. We will demonstrate this in *Exercise 15*, *Visualizing
    Variance Reduction with Manual PCA*.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们简单地计算了数据集的协方差矩阵，而没有对数据集进行任何预处理。如果两个特征的均值和标准差大致相同，这是完全可以接受的。然而，如果一个特征的值远大于另一个特征（并且均值也有所不同），那么在分解为主成分时，这个特征可能会主导另一个特征，从而可能会完全丧失较小特征所提供的信息。在计算协方差矩阵之前，一种简单的归一化方法是从特征中减去各自的均值，从而使数据集围绕零进行中心化。我们将在*练习
    15*，*通过手动 PCA 可视化方差减少*中演示这一过程。
- en: 'Exercise 14: Scikit-Learn PCA'
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '练习 14: Scikit-Learn PCA'
- en: 'Typically, we will not complete PCA manually, especially when scikit-learn
    provides an optimized API with convenient methods that will allow us to easily
    transform the data to and from the reduced-dimensional space. In this exercise,
    we will look at using a scikit-learn PCA on the Iris dataset in more detail:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们不会手动完成 PCA，尤其是当 scikit-learn 提供了一个优化的 API，并且它的便捷方法能让我们轻松地将数据转换到低维空间并返回时。在本次练习中，我们将更详细地研究如何在
    Iris 数据集上使用 scikit-learn 的 PCA：
- en: Note
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来自 [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/)。
- en: It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise14](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise14).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以从 [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise14](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise14)
    下载。
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: UCI 机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚州尔湾市：加利福尼亚大学信息与计算机科学学院。
- en: 'Import the `pandas`, `numpy`, and `PCA` modules from the `sklearn` packages:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `sklearn` 包中导入 `pandas`、`numpy` 和 `PCA` 模块：
- en: '[PRE47]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Load the dataset:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集：
- en: '[PRE48]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output is as follows:'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.24: The first five rows of the dataset](img/C12626_04_24.jpg)'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.24: 数据集的前五行](img/C12626_04_24.jpg)'
- en: 'Figure 4.24: The first five rows of the dataset'
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.24: 数据集的前五行'
- en: 'Again, we only require the `Sepal Length` and `Sepal Width` features, so remove
    the other columns. In this example, we are not normalizing the selected dataset:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，我们只需要 `花萼长度` 和 `花萼宽度` 两个特征，因此需要删除其他列。在这个示例中，我们没有对选定的数据集进行归一化处理：
- en: '[PRE49]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output is as follows:'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.25: The Sepal Length and Sepal Width features](img/C12626_04_25.jpg)'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.25：花萼长度和花萼宽度特征](img/C12626_04_25.jpg)'
- en: 'Figure 4.25: The Sepal Length and Sepal Width features'
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.25：花萼长度和花萼宽度特征
- en: 'Fit the data to a scikit-learn PCA model of the covariance data. Using the
    default values, as we have here, produces the maximum number of eigenvalues and
    eigenvectors possible for the dataset:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拟合到scikit-learn的PCA模型上，使用协方差数据。使用默认值，就像我们这里所做的那样，会生成该数据集可能的最大特征值和特征向量数量：
- en: '[PRE50]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is as follows:'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.26: Fitting data to a PCA model](img/C12626_04_26.jpg)'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.26：将数据拟合到PCA模型](img/C12626_04_26.jpg)'
- en: 'Figure 4.26: Fitting data to a PCA model'
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.26：将数据拟合到PCA模型
- en: Here, `copy` indicates that the data fit within the model is copied before any
    calculations are applied. `iterated_power` shows that the `Sepal Length` and `Sepal
    Width` featuresis the number of principal components to keep. The default value
    is `None`, which selects the number of components as one less than the minimum
    of either the number of samples or number of features. `random_state` allows the
    user to specify a seed for the random number generator used by the SVD solver.
    `svd_solver` specifies the SVD solver to be used during PCA. `tol` is the tolerance
    values used by the SVD solver. With `whiten`, the component vectors are multiplied
    by the square root of the number of samples. This will remove some information,
    but can improve the performance of some downstream estimators.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，`copy`表示数据在模型内的拟合会在应用任何计算之前进行复制。`iterated_power`显示`花萼长度`和`花萼宽度`特征是保留的主成分数量。默认值为`None`，它会选择组件数量为样本数或特征数中的最小值减一。`random_state`允许用户为SVD求解器使用的随机数生成器指定种子。`svd_solver`指定在PCA过程中使用的SVD求解器。`tol`是SVD求解器使用的容差值。通过`whiten`，组件向量会乘以样本数的平方根。这将删除一些信息，但可以改善某些后续估计器的性能。
- en: 'The percentage of variance described by the components (eigenvalues) is contained
    within the `explained_variance_ratio_` property. Display the values for `explained_variance_ratio_`:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成分（特征值）所描述的方差百分比包含在`explained_variance_ratio_`属性中。显示`explained_variance_ratio_`的值：
- en: '[PRE51]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output is as follows:'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE52]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Display the eigenvectors via the `components_` property:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`components_`属性显示特征向量：
- en: '[PRE53]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The output is as follows:'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.27: Eigenvectors](img/C12626_04_27.jpg)'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.27：特征向量](img/C12626_04_27.jpg)'
- en: 'Figure 4.27: Eigenvectors'
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.27：特征向量
- en: 'In this exercise, we will again only use the primary component, so we will
    create a new `PCA` model, this time specifying the number of components (eigenvectors/eigenvalues)
    to be `1`:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将再次只使用主要成分，因此我们将创建一个新的`PCA`模型，这次指定成分（特征向量/特征值）的数量为`1`：
- en: '[PRE54]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Use the `fit` method to fit the `covariance` matrix to the `PCA` model and
    generate the corresponding eigenvalues/eigenvectors:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fit`方法将`covariance`矩阵拟合到`PCA`模型，并生成相应的特征值/特征向量：
- en: '[PRE55]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![Figure 4.28: The maximum number of eigenvalues and eigenvectors](img/C12626_04_28.jpg)'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.28：特征值和特征向量的最大数量](img/C12626_04_28.jpg)'
- en: 'Figure 4.28: The maximum number of eigenvalues and eigenvectors'
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.28：特征值和特征向量的最大数量
- en: The model is fitted using a number of default parameters, as listed in the preceding
    output. `copy = True` is the data provided to the `fit` method, which is copied
    before PCA is applied. `iterated_power='auto'` is used to define the number of
    iterations by the internal SVD solver. `n_components=1` specifies that the PCA
    model is to return only the principal component. `random_state=None` specifies
    the random number generator to be used by the internal SVD solver if required.
    `svd_solver='auto'` is the type of SVD solver used. `tol=0.0` is the tolerance
    value for the SVD solver to deem converged. `whiten=False` specifies that the
    eigenvectors are not to be modified. If set to `True`, whitening further modifies
    the components by multiplying by the square root of the number of samples and
    dividing by the singular values. This can help to improve the performance of later
    algorithm steps.
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用多个默认参数拟合模型，具体参数如前面的输出所示。`copy = True`是提供给`fit`方法的数据，该数据在应用PCA之前会被复制。`iterated_power='auto'`用于定义内部SVD求解器的迭代次数。`n_components=1`指定PCA模型只返回主成分。`random_state=None`指定需要时内部SVD求解器使用的随机数生成器。`svd_solver='auto'`是使用的SVD求解器类型。`tol=0.0`是SVD求解器认为已收敛的容差值。`whiten=False`指定不修改特征向量。如果设置为`True`，白化会进一步通过乘以样本数量的平方根并除以奇异值来修改成分。这可以帮助改善后续算法步骤的性能。
- en: Typically, you will not need to worry about adjusting any of these parameters,
    other than the number of components (`n_components`), which you can pass to the
    `fit` method, for example, `model.fit(data, n_components=2)`.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通常，除了组件数量（`n_components`）之外，你无需担心调整任何这些参数。例如，你可以将`n_components`传递给`fit`方法，如`model.fit(data,
    n_components=2)`。
- en: 'Display the eigenvectors using the `components_` property:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`components_`属性显示特征向量：
- en: '[PRE56]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output is as follows:'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE57]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Transform the Iris dataset into the lower space by using the `fit_transform`
    method of the model on the dataset. Assign the transformed values to the `data_t`
    variable.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型的`fit_transform`方法将鸢尾花数据集转换到低维空间。将转换后的值赋给`data_t`变量。
- en: '[PRE58]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Plot the transformed values to visualize the result:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制转换后的值以可视化结果：
- en: '[PRE59]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output is as follows:'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.29: The Iris dataset transformed using the scikit-learn PCA](img/C12626_04_29.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![图4.29：使用scikit-learn PCA转换的鸢尾花数据集](img/C12626_04_29.jpg)'
- en: 'Figure 4.29: The Iris dataset transformed using the scikit-learn PCA'
  id: totrans-322
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.29：使用scikit-learn PCA转换的鸢尾花数据集
- en: Congratulations! You have just reduced the dimensionality of the Iris dataset
    using manual PCA, as well as the scikit-learn API. But before we celebrate too
    early, compare Figure 4.23 and Figure 4.29; these plots should be identical, shouldn't
    they? We used two separate methods to complete a PCA on the same dataset and selected
    the principal component for both. In the next activity, we will investigate why
    there are differences between the two.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你！你刚刚使用手动PCA以及scikit-learn API减少了鸢尾花数据集的维度。但在我们过早庆祝之前，比较图4.23和图4.29；这两张图应该是相同的，对吧？我们使用了两种不同的方法在同一数据集上完成PCA，并且都选择了主成分。在下一个活动中，我们将探讨为什么两者之间存在差异。
- en: 'Activity 6: Manual PCA versus scikit-learn'
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动6：手动PCA与scikit-learn
- en: Suppose that you have been asked to port some legacy code from an older application
    executing PCA manually, to a newer application that uses scikit-learn. During
    the porting process, you notice some differences between the output of the manual
    PCA versus your port. Why is there a difference between the output of our manual
    PCA and scikit-learn? Compare the results of the two approaches on the Iris dataset.
    What are the differences between them?
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你被要求将一个旧应用程序中手动执行PCA的遗留代码移植到一个新的使用scikit-learn的应用程序。在移植过程中，你注意到手动PCA的输出与移植后的输出之间存在一些差异。为什么手动PCA和scikit-learn之间会有输出差异？比较两种方法在鸢尾花数据集上的结果。它们之间有什么区别？
- en: Note
  id: totrans-326
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
    It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Activity06](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Activity06).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来源于[https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/)。可以从[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Activity06](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Activity06)下载。
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: UCI机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚州尔湾：加利福尼亚大学信息与计算机科学学院。
- en: Import the `pandas`, `numpy`, and `matplotlib` plotting libraries and the scikit-learn
    `PCA` model.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`、`numpy`和`matplotlib`绘图库以及scikit-learn的`PCA`模型。
- en: Load the dataset and select only the sepal features as per the previous exercises.
    Display the first five rows of the data.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集并按照之前的练习仅选择萼片特征。显示数据的前五行。
- en: Compute the `covariance` matrix for the data.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算数据的`协方差`矩阵。
- en: Transform the data using the scikit-learn API and only the first principal component.
    Store the transformed data in the `sklearn_pca` variable.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用scikit-learn API并仅使用第一个主成分转换数据。将转换后的数据存储在`sklearn_pca`变量中。
- en: Transform the data using the manual PCA and only the first principal component.
    Store the transformed data in the `manual_pca` variable.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用手动PCA和仅第一个主成分转换数据。将转换后的数据存储在`manual_pca`变量中。
- en: Plot the `sklearn_pca` and `manual_pca` values on the same plot to visualize
    the difference.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一图表上绘制`sklearn_pca`和`manual_pca`的值，以可视化它们的差异。
- en: Notice that the two plots look almost identical, but with some key differences.
    What are these differences?
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，两个图表看起来几乎相同，但有一些关键的差异。这些差异是什么？
- en: See whether you can modify the output of the manual PCA process to bring it
    in line with the scikit-learn version.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看看是否能够修改手动PCA过程的输出，使其与scikit-learn版本一致。
- en: Note
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: As a hint, the scikit-learn API subtracts the mean of the data prior to the
    transform.
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示：scikit-learn API在转换前会减去数据的均值。
- en: 'Expected output: By the end of this activity, you will have transformed the
    dataset using both the manual and scikit-learn PCA methods. You will have produced
    a plot demonstrating that the two reduced datasets are, in fact, identical, and
    you should have an understanding of why they initially looked quite different.
    The final plot should look similar to the following:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出：在本活动结束时，你将使用手动PCA和scikit-learn PCA方法对数据集进行转化。你将生成一张图表，展示两个降维数据集实际上是相同的，并且你应该理解为什么它们最初看起来有很大的不同。最终图表应类似于以下内容：
- en: '![Figure 4.30: The expected final plot](img/C12626_04_30.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.30：预期的最终图表](img/C12626_04_30.jpg)'
- en: 'Figure 4.30: The expected final plot'
  id: totrans-341
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.30：预期的最终图表
- en: This plot will demonstrate that the dimensionality reduction completed by the
    two methods are, in fact, the same.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 该图将展示通过两种方法完成的降维实际上是相同的。
- en: Note
  id: totrans-343
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 324.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第324页找到。
- en: Restoring the Compressed Dataset
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 恢复压缩的数据集
- en: Now that we have covered a few different examples of transforming a dataset
    into a lower-dimensional space, we should consider what practical effect this
    transformation has had on the data. Using PCA as a pre-processing step to condense
    the number of features in the data will result in some of the variance being discarded.
    The following exercise will walk us through this process so that we can see how
    much information has been discarded by the transformation.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经覆盖了一些不同的将数据集转化为低维空间的例子，我们应当考虑这种转化对数据产生了什么实际效果。将PCA作为预处理步骤来压缩数据中的特征数量，会导致部分方差被丢弃。以下练习将引导我们完成这一过程，帮助我们了解通过转化丢弃了多少信息。
- en: 'Exercise 15: Visualizing Variance Reduction with Manual PCA'
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 15：通过手动PCA可视化方差减少
- en: 'One of the most important aspects of dimensionality reduction is understanding
    how much information has been removed from the dataset as a result of the dimensionality
    reduction process. Removing too much information will add additional challenges
    to later processing, while not removing enough defeats the purpose of PCA or other
    techniques. In this exercise, we will visualize the amount of information that
    has been removed from the Iris dataset as a result of PCA:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 降维的一个最重要的方面是理解由于降维过程，从数据集中移除了多少信息。移除过多的信息会给后续处理带来额外挑战，而移除的信息不足则会破坏PCA或其他技术的目的。在本练习中，我们将可视化PCA将Iris数据集移除的多少信息：
- en: Note
  id: totrans-349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来自于[https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/)。
- en: It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise15](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise15).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以从[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise15](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise15)下载。
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: UCI机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚大学欧文分校，信息与计算机科学学院。
- en: 'Import the `pandas`, `numpy`, and `matplotlib` plotting libraries:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`、`numpy`和`matplotlib`绘图库：
- en: '[PRE60]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Read in the `Sepal` features from the Iris dataset:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从鸢尾花数据集读取`花萼`特征：
- en: '[PRE61]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output is as follows:'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.31: Sepal features](img/C12626_04_31.jpg)'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.31：花萼特征](img/C12626_04_31.jpg)'
- en: 'Figure 4.31: Sepal features'
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.31：花萼特征
- en: 'Centre the dataset around zero by subtracting the respective means:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过减去相应的均值，使数据集围绕零居中：
- en: Note
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE62]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The output is as follows:'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE63]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'To calculate the data and print the results, use the following code:'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了计算数据并打印结果，请使用以下代码：
- en: '[PRE64]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'A section of the output is as follows:'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的部分如下：
- en: '![Figure 4.32: Section of the output](img/C12626_04_32.jpg)'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.32：输出的部分](img/C12626_04_32.jpg)'
- en: 'Figure 4.32: Section of the output'
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.32：输出的部分
- en: 'Use manual PCA to transform the data on the basis of the first principal component:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用手动PCA基于第一个主成分来变换数据：
- en: '[PRE65]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The output is as follows:'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE66]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Transform the data into the lower-dimensional space:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据转换为低维空间：
- en: '[PRE67]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Reshape the principal components for later use:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重塑主成分以便后续使用：
- en: '[PRE68]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'To compute the inverse transform of the reduced dataset, we need to restore
    the selected eigenvectors into the higher-dimensional space. To do this, we will
    invert the matrix. Matrix inversion is another linear algebra technique that we
    will only cover very briefly. A square matrix, *A*, is said to be invertible if
    there exists another square matrix, *B*, and if *AB=BA=I*, where *I* is a special
    matrix known as an identity matrix, consisting of values of `1` only through the
    center diagonal:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了计算减少数据集的逆变换，我们需要将选定的特征向量恢复到更高维空间。为此，我们将对矩阵进行求逆。矩阵求逆是另一种线性代数技术，这里我们只会简单介绍。一个方阵，*A*，如果存在另一个方阵*B*，且满足*AB=BA=I*，其中*I*是一个特殊矩阵，称为单位矩阵，只有主对角线上的值为`1`，则该方阵被称为可逆矩阵：
- en: '[PRE69]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The output is as follows:'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE70]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Prepare the transformed data for use in the matrix multiplication:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为矩阵乘法准备变换后的数据：
- en: '[PRE71]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Compute the inverse transform of the reduced data and plot the result to visualize
    the effect of removing the variance from the data:'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算减少数据的逆变换，并绘制结果以可视化去除数据方差的效果：
- en: '[PRE72]'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'A section of the output is as follows:'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的部分如下：
- en: '![Figure 4.33: The inverse transform of the reduced data](img/C12626_04_33.jpg)'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.33：减少数据的逆变换](img/C12626_04_33.jpg)'
- en: 'Figure 4.33: The inverse transform of the reduced data'
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.33：减少数据的逆变换
- en: 'Add the `means` back to the transformed data:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`means`添加回变换后的数据：
- en: '[PRE73]'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Visualize the result by plotting the original dataset and the transformed dataset:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过绘制原始数据集和变换后的数据集来可视化结果：
- en: '[PRE74]'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The output is as follows:'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.34: The inverse transform after removing variance](img/C12626_04_34.jpg)'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.34：去除方差后的逆变换](img/C12626_04_34.jpg)'
- en: 'Figure 4.34: The inverse transform after removing variance'
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.34：去除方差后的逆变换
- en: 'There are only two components of variation in this dataset. If we do not remove
    any of the components, what will be the result of the inverse transform? Again,
    transform the data into the lower-dimensional space, but this time, use all of
    the eigenvectors:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该数据集只有两个变化成分。如果我们不去除任何成分，那么逆变换的结果会是什么？再次将数据转换为低维空间，但这次使用所有的特征向量：
- en: '[PRE75]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Transpose `data_transformed` to put it into the correct shape for matrix multiplication:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转置`data_transformed`，使其具有正确的形状以进行矩阵乘法：
- en: '[PRE76]'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Now, restore the data back to the higher-dimensional space:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将数据恢复到更高维空间：
- en: '[PRE77]'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'A section of the output is as follows:'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的部分如下：
- en: '![Figure 4.35: The restored data](img/C12626_04_35.jpg)'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.35：恢复的数据](img/C12626_04_35.jpg)'
- en: 'Figure 4.35: The restored data'
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.35：恢复的数据
- en: 'Add the means back to the restored data:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将均值添加回恢复的数据：
- en: '[PRE78]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Visualize the restored data in the context of the original dataset:'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在原始数据集的背景下可视化恢复的数据：
- en: '[PRE79]'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The output is as follows:'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/C12626_04_36.jpg)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_04_36.jpg)'
- en: 'Figure 4.36: The inverse transform after removing the variance'
  id: totrans-411
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.36：去除方差后的逆变换
- en: If we compare the two plots produced in this exercise, we can see that the PCA
    reduced, and the restored dataset is essentially a negative linear trend line
    between the two feature sets. We can compare this to the dataset restored from
    all available components, where we have recreated the original dataset as a whole.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们比较本练习中生成的两个图，我们可以看到，PCA 降维后的数据集与恢复的数据集基本上是两个特征集之间的负线性趋势线。我们可以将其与从所有可用成分恢复的数据集进行比较，在该数据集中我们已经完整地重建了原始数据集。
- en: 'Exercise 16: Visualizing Variance Reduction with'
  id: totrans-413
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 16：通过可视化降方差
- en: 'In this exercise, we will again visualize the effect of reducing the dimensionality
    of the dataset; however, this time, we will be using the scikit-learn API. This
    is this method that you will commonly use in practical applications, due to the
    power and simplicity of the scikit-learn model:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将再次可视化降维对数据集的影响；不过这次，我们将使用 scikit-learn API。由于 scikit-learn 模型的强大功能和简便性，这也是在实际应用中常用的方法：
- en: Note
  id: totrans-415
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
    It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise16](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise16).
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来自 [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/)。可以从
    [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise16](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise16)
    下载。
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: UCI 机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚州欧文市：加利福尼亚大学信息与计算机科学学院。
- en: 'Import the `pandas`, `numpy`, and `matplotlib` plotting libraries and the `PCA`
    model from scikit-learn:'
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas`、`numpy` 和 `matplotlib` 绘图库，以及从 scikit-learn 中导入 `PCA` 模型：
- en: '[PRE80]'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Read in the `Sepal` features from the Iris dataset:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从鸢尾花数据集中读取 `Sepal` 特征：
- en: '[PRE81]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'The output is as follows:'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.37: The Sepal features from the Iris dataset](img/C12626_04_37.jpg)'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.37：来自鸢尾花数据集的 Sepal 特征](img/C12626_04_37.jpg)'
- en: 'Figure 4.37: The Sepal features from the Iris dataset'
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.37：来自鸢尾花数据集的 Sepal 特征
- en: 'Use the scitkit-learn API to transform the data on the basis of the first principal
    component:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 scikit-learn API 基于第一个主成分对数据进行变换：
- en: '[PRE82]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The output is as follows:'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: 'Compute the inverse transform of the reduced data and plot the result to visualize
    the effect of removing the variance from the data:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算降维数据的逆变换，并绘制结果以可视化去除数据中方差的效果：
- en: '[PRE83]'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'The output is as follows:'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.38: The inverse transform after removing the variance](img/C12626_04_38.jpg)'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.38：去除方差后的逆变换](img/C12626_04_38.jpg)'
- en: 'Figure 4.38: The inverse transform after removing the variance'
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.38：去除方差后的逆变换
- en: There are only two components of variation in this dataset. If we do not remove
    any of the components, what will the result of the inverse transform be?
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该数据集中只有两个变化成分。如果我们不去除任何成分，逆变换的结果会是什么？
- en: '[PRE84]'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The output is as follows:'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.39: The inverse transform after removing the variance](img/C12626_04_39.jpg)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.39：去除方差后的逆变换](img/C12626_04_39.jpg)'
- en: 'Figure 4.39: The inverse transform after removing the variance'
  id: totrans-437
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.39：去除方差后的逆变换
- en: Again, we have demonstrated the effect of removing information from the dataset
    and the ability to recreate the original data using all the available eigenvectors.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们展示了从数据集中去除信息的效果，并且展示了如何使用所有可用的特征向量重新创建原始数据。
- en: The previous exercises specified the reduction of the dimensionality using PCA
    to two dimensions, partly to allow the results to be easily visualized. We can,
    however, use PCA to reduce the dimensions to any value less than that of the original
    set. The following example demonstrates how PCA can be used to reduce a dataset
    to three dimensions, allowing visualizations.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的练习指定了使用 PCA 将数据维度减少到二维，部分原因是为了方便结果的可视化。然而，我们也可以使用 PCA 将数据维度减少到任何小于原始数据集的值。以下示例演示了如何使用
    PCA 将数据集减少到三维，从而实现可视化。
- en: 'Exercise 17: Plotting 3D Plots in Matplotlib'
  id: totrans-440
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 17：在 Matplotlib 中绘制 3D 图形
- en: 'Creating 3D scatter plots in matplotlib are unfortunately not as simple as
    providing a series of (*x*, *y*, *z*)coordinates to a scatter plot. In this exercise
    we will work through a simple 3D plotting example, using the Iris dataset:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 在 matplotlib 中创建 3D 散点图并不像简单地将一系列 (*x*, *y*, *z*) 坐标提供给散点图那么简单。在本练习中，我们将通过一个简单的
    3D 绘图示例，使用鸢尾花数据集进行操作：
- en: Note
  id: totrans-442
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 本数据集来自 [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/)。
- en: It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise17](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise17).
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从 [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise17](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise17)
    下载。
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: UCI 机器学习数据库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚州欧文市：加利福尼亚大学信息与计算机科学学院。
- en: 'Import `pandas` and `matplotlib`. To enable 3D plotting, you will also need
    to import `Axes3D`:'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas` 和 `matplotlib`。为了启用 3D 绘图，你还需要导入 `Axes3D`：
- en: '[PRE85]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Read in the dataset and select the `Sepal Length`, `Sepal Width`, and `Petal
    Width` columns
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取数据集并选择 `Sepal Length`、`Sepal Width` 和 `Petal Width` 列
- en: '[PRE86]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The output is as follows:'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.40: The first five rows of the data](img/C12626_04_40.jpg)'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.40：数据的前五行](img/C12626_04_40.jpg)'
- en: 'Figure 4.40: The first five rows of the data'
  id: totrans-452
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.40：数据的前五行
- en: 'Plot the data in three dimensions and use the `projection=''3d''` argument
    with the `add_subplot` method to create the 3D plot:'
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在三维空间中绘制数据，并使用 `projection='3d'` 参数与 `add_subplot` 方法来创建 3D 图：
- en: '[PRE87]'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'The plot will look as follows:'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘图结果如下：
- en: '![Figure 4.41: The expanded Iris dataset](img/C12626_04_41.jpg)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.41：扩展版鸢尾花数据集](img/C12626_04_41.jpg)'
- en: 'Figure 4.41: The expanded Iris dataset'
  id: totrans-457
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.41：扩展版鸢尾花数据集
- en: Note
  id: totrans-458
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: While the Axes3D was imported but not directly used, it is required for configuring
    the plot window in three dimensions. If the import of Axes3D was omitted, the
    `projection='3d'` argument would return an `AttributeError`.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管导入了 Axes3D，但没有直接使用，它对于配置三维绘图窗口是必需的。如果省略了 Axes3D 的导入，`projection='3d'` 参数将返回一个
    `AttributeError`。
- en: 'Activity 7: PCA Using the Expanded Iris Dataset'
  id: totrans-460
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 7：使用扩展版鸢尾花数据集进行 PCA
- en: 'In this activity, we are going to use the complete Iris dataset to look at
    the effect of selecting a differing number of components in the PCA decomposition.
    This activity aims to simulate the process that is typically completed in a real-world
    problem as we try to determine the optimum number of components to select, attempting
    to balance the extent of dimensionality reduction and information loss. Henceforth,
    we will be using the scikit-learn PCA model:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将使用完整的鸢尾花数据集，观察选择不同数量组件进行 PCA 分解的效果。本活动旨在模拟一个真实世界问题中的过程，我们试图确定选择最佳组件数，同时平衡降维程度和信息丢失。因此，我们将使用
    scikit-learn 的 PCA 模型：
- en: Note
  id: totrans-462
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 本数据集来自 [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/)。
- en: It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Activity07](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Activity07).
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从 [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Activity07](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Activity07)
    下载。
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: UCI机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚州欧文市：加利福尼亚大学信息与计算机科学学院。
- en: Import `pandas` and `matplotlib`. To enable 3D plotting, you will also need
    to import `Axes3D`.
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`和`matplotlib`。为了启用三维绘图，您还需要导入`Axes3D`。
- en: Read in the dataset and select the `Sepal Length`, `Sepal Width`, and `Petal
    Width` columns.
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取数据集，并选择`花萼长度`、`花萼宽度`和`花瓣宽度`列。
- en: Plot the data in three dimensions.
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在三维空间中绘制数据。
- en: Create a `PCA` model without specifying the number of components.
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`PCA`模型，未指定组件数量。
- en: Fit the model to the dataset.
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型拟合到数据集。
- en: Display the eigenvalues or `explained_variance_ratio_`.
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示特征值或`explained_variance_ratio_`。
- en: We want to reduce the dimensionality of the dataset but still keep at least
    90% of the variance. What are the minimum number of components required to keep
    90% of the variance?
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望减少数据集的维度，但仍保持至少90%的方差。为保持90%的方差，所需的最小组件数是多少？
- en: Create a new `PCA` model, this time specifying the number of components required
    to keep at least 90% of the variance.
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的`PCA`模型，这次指定所需的组件数量，以保持至少90%的方差。
- en: Transform the data using the new model.
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新模型变换数据。
- en: Plot the transformed data.
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制变换后的数据。
- en: Restore the transformed data to the original dataspace.
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将变换后的数据恢复到原始数据空间。
- en: 'Plot the restored data in three dimensions in one subplot and the original
    data in a second subplot to visualize the effect of removing some of the variance:'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个子图中绘制恢复后的三维数据，在第二个子图中绘制原始数据，以可视化去除部分方差的效果：
- en: '[PRE88]'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Expected Output: The final plot will look as follows:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出：最终图形将如下所示：
- en: '![Figure 4.42: Expected plots](img/C12626_04_42.jpg)'
  id: totrans-480
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.42：预期图](img/C12626_04_42.jpg)'
- en: 'Figure 4.42: Expected plots'
  id: totrans-481
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.42：预期图
- en: Note
  id: totrans-482
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 328.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第328页找到。
- en: Summary
  id: totrans-484
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the process of dimensionality reduction and PCA.
    We completed a number of exercises and developed the skills to reduce the size
    of a dataset by extracting only the most important components of variance within
    the data, using both a manual PCA process and the model provided by scikit-learn.
    During this chapter, we also returned the reduced datasets back to the original
    dataspace and observed the effect of removing the variance on the original data.
    Finally, we discussed a number of potential applications for PCA and other dimensionality
    reduction processes. In our next chapter, we will introduce neural network-based
    autoencoders and use the Keras package to implement them.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了降维和PCA的过程。我们完成了一些练习，并发展了提取数据中最重要方差成分的技能，以减少数据集的大小，既使用手动PCA过程，也使用scikit-learn提供的模型。在本章中，我们还将降维后的数据集恢复到原始数据空间，并观察去除方差对原始数据的影响。最后，我们讨论了PCA和其他降维过程的多种潜在应用。在下一章中，我们将介绍基于神经网络的自编码器，并使用Keras包实现它们。
