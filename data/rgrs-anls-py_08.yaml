- en: Chapter 8. Advanced Regression Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 8 章. 高级回归方法
- en: 'In this chapter, we will introduce some advanced regression methods. Since
    many of them are very complex, we will skip most of the mathematical formulations,
    providing the readers instead with the ideas underneath the techniques and some
    practical advice, such as explaining when and when not to use the technique. We
    will illustrate:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一些高级回归方法。由于其中许多方法非常复杂，我们将跳过大多数数学公式，而是提供技术背后的思想以及一些实用建议，例如解释何时以及何时不使用该技术。我们将举例说明：
- en: Least Angle Regression (LARS)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小角度回归（LARS）
- en: Bayesian regression
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯回归
- en: SGD classification with hinge loss (note that this is not a regressor, it's
    a classifier)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 hinge loss 的 SGD 分类（注意这不是一个回归器，而是一个分类器）
- en: Regression trees
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归树
- en: Ensemble of regressors (bagging and boosting)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归器的集成（袋装和提升）
- en: Gradient Boosting Regressor with Least Angle Deviation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最小角度偏差的梯度提升回归器
- en: Least Angle Regression
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最小角度回归
- en: Although very similar to Lasso (seen in [Chapter 6](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 6. Achieving Generalization"), *Achieving Generalization*), Least Angle
    Regression, or simply LARS, is a regression algorithm that, in a fast and smart
    way, selects the best features to use in the model, even though they're very closely
    correlated to each other. LARS is an evolution of the Forward Selection (also
    called Forward Stepwise Regression) algorithm and of the Forward Stagewise Regression
    algorithm.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管与 Lasso（在第 6 章[Chapter 6](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6
    "第 6 章. 实现泛化")中看到）非常相似，但最小角度回归（或简称 LARS）是一种回归算法，它以快速和智能的方式选择模型中使用的最佳特征，即使它们彼此之间非常相关。LARS
    是前向选择算法（也称为前向逐步回归）和前向分步回归算法的演变。
- en: 'Here is how the Forward Selection algorithm works, based on the hypothesis
    that all the variables, including the target one, have been previously normalized:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前向选择算法的工作原理，基于所有变量（包括目标变量）都已被先前归一化的假设：
- en: Of all the possible predictors for a problem, the one with the largest absolute
    correlation with the target variable *y* is selected (that is, the one with the
    most explanatory capability). Let's call it *p[1]*.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于一个问题的所有可能的预测因子中，与目标变量 *y* 具有最大绝对相关性的那个被选中（即具有最大解释能力的那个）。让我们称它为 *p[1]*。
- en: All the other predictors are now projected onto *p[1]* Least Angle Regression,
    and the projection is removed, creating a vector of residuals orthogonal to *p[1]*.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有其他预测因子现在都投影到 *p[1]* 最小角度回归上，并移除投影，创建一个与 *p[1]* 正交的残差向量。
- en: Step 1 is repeated on the residual vectors, and the most correlated predictor
    is again selected. Let's name it *p²* Apply subscript.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步在残差向量上重复进行，并再次选择相关性最高的预测因子。让我们称它为 *p²* 并应用下标。
- en: Step 2 is repeated, using *p[2]*, creating a vector of residuals orthogonal
    to *p[2]* (and also *p[1]*).
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二步重复进行，使用 *p[2]*，创建一个与 *p[2]*（以及 *p[1]*）正交的残差向量。
- en: This process continues until the prediction is satisfying, or when the largest
    absolute correlation falls below a set threshold. After each iteration, a new
    predictor is added to the list of predictors, and the residual is orthogonal to
    all of them.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此过程持续进行，直到预测令人满意，或者当最大绝对相关性低于设定的阈值。在每次迭代后，将一个新的预测因子添加到预测因子列表中，残差与它们都正交。
- en: 'This method is not very popular because it has a serious limitation due to
    its extremely greedy approach; however, it''s fairly quick. Let''s now consider
    that we have a regression problem with two highly correlated variables. Forward
    Selection, on this dataset, will select the predictor on the basis of the first
    or the second variable, and then, since the residual will be very low, will reconsider
    the other variable in a far later step (eventually, never). This fact will lead
    to overfitting problems on the model. Wouldn''t it be better if the two highly
    correlated variables were selected together, balancing the new predictor? That''s
    practically the core idea of the Forward Stagewise Regression algorithm, where,
    in each step, the best predictor is partially added to the model. Let''s provide
    the details here:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法并不非常受欢迎，因为它由于其极端贪婪的方法存在严重的限制；然而，它相当快。现在让我们考虑我们有一个具有两个高度相关变量的回归问题。在这个数据集上，前向选择将基于第一个或第二个变量选择预测因子，然后，由于残差将非常低，将在远晚的步骤中重新考虑另一个变量（最终，永远不会）。这一事实将导致模型上的过拟合问题。如果两个高度相关的变量一起被选中，平衡新的预测因子，不是更好吗？这正是前向分步回归算法的核心思想，其中，在每一步中，最佳预测因子部分地添加到模型中。让我们在这里提供详细情况：
- en: In the model, every feature has an associate weight of zero—that is, *w[i] =
    0* for each feature *i*.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在该模型中，每个特征都有一个关联权重为零——也就是说，对于每个特征 *i*，*w[i] = 0*。
- en: Of all the possible predictors for a problem, the one with the largest (absolute)
    correlation with the target variable *y* is partially added to the model—that
    is, in the model, the weight of *w[i]* is increased by *ε*.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于一个问题的所有可能的预测因子中，与目标变量 *y* 具有最大（绝对）相关性的那个部分添加到模型中——也就是说，在模型中，*w[i]* 的权重增加 *ε*。
- en: Repeat step 2, until the exploratory power is below a predefined threshold.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 2，直到探索能力低于预定义的阈值。
- en: This method represents a great improvement on the Forward Selected because,
    in the case of correlated features, both of them will be in the final model with
    a similar weight. The result is very good, but the enormous number of iterations
    needed to create the model is the really big problem with this algorithm. Again,
    the method becomes impractical because of its running time.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与前向选择相比，这种方法有很大的改进，因为在相关特征的情况下，两者都将以相似权重出现在最终模型中。结果是很好的，但创建模型所需的巨大迭代次数是这个算法真正的大问题。再次，由于运行时间，这种方法变得不切实际。
- en: 'The LARS algorithm instead operates as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: LARS 算法的工作方式如下：
- en: In the model, every feature has an associate weight of zero—that is, *w[i] =
    0* for each feature *i*.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型中，每个特征都有一个关联权重为零——也就是说，对于每个特征 *i*，*w[i] = 0*。
- en: Of the possible predictors for a problem, the one with the largest (absolute)
    correlation with the target variable *y* is partially added to the model—that
    is, in the model, the weight of *w[i]* is increased by *ε*.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于一个问题的所有可能的预测因子中，与目标变量 *y* 具有最大（绝对）相关性的那个部分添加到模型中——也就是说，在模型中，*w[i]* 的权重增加 *ε*。
- en: Keep increasing *w[i]* till any other predictor (let's say *j*) has as much
    correlation with the residual vector as the current predictor has.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续增加 *w[i]*，直到任何其他预测因子（比如说 *j*）与残差向量的相关性等于当前预测因子的相关性。
- en: Increase *w[i]* and *w[j]* simultaneously until another predictor has as much
    correlation with the residual vector as the current predictors have.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时增加 *w[i]* 和 *w[j]*，直到另一个预测因子与残差向量的相关性等于当前预测因子的相关性。
- en: Keep adding predictors and weights until all the predictors are in the model
    or it meets another termination criterion, such as the number of iterations.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续添加预测因子和权重，直到所有预测因子都在模型中，或者达到另一个终止标准，例如迭代次数。
- en: 'This solution is able to compose the best pieces of Forward Selection and Stagewise
    Regression, creating a solution that is stable, not so prone to overfitting, and
    fast. Before getting to the examples, you may wonder why it is named Least Angle
    Regression. The answer is very simple: if the features and output are represented
    as vectors in the Cartesian space, at every iteration LARS includes in the model
    the variable most correlated with the residual vector, which is the one that generates
    the least angle with the residual. Actually, the whole process can be expressed
    visually.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解决方案能够组合最佳的前向选择和分步回归的部分，创建一个稳定、不太容易过拟合且快速的解决方案。在到达示例之前，你可能想知道为什么它被称为最小角度回归。答案非常简单：如果特征和输出在笛卡尔空间中表示为向量，在每次迭代中，LARS
    都会将与残差向量最相关的变量包含在模型中，这是与残差产生最小角度的那个变量。实际上，整个过程可以直观地表达。
- en: Visual showcase of LARS
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LARS的视觉展示
- en: '![Visual showcase of LARS](img/00120.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![LARS的视觉展示](img/00120.jpeg)'
- en: 'Here is the visual situation: two predictors (**x1** and **x2**), not necessarily
    orthogonal, and the target (**y**). Note that, at the beginning, the residual
    corresponds to the target. Our model starts at **u0** (where all the weights are
    *0*).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是视觉情况：两个预测因子（**x1**和**x2**），不一定正交，以及目标（**y**）。请注意，一开始，残差对应于目标。我们的模型从**u0**（所有权重都是*0*）开始。
- en: 'Then, since **x2** makes a smaller angle with the residual compared to **x1**,
    we start *walking* in the direction of **x2**, while we keep computing the residual
    vector. Now, a question: where should we stop?'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，由于**x2**与残差之间的角度比**x1**小，我们开始向**x2**的方向*行走*，同时我们继续计算残差向量。现在，一个问题：我们应该在哪里停止？
- en: '![Visual showcase of LARS](img/00121.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![LARS的视觉展示](img/00121.jpeg)'
- en: We should stop at **u1**, where the angle between the residual and **x1** is
    the same as the angle between the residual and **x2**. We then walk in the direction
    of the composition **x1** and **x2**, reaching **y**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该在**u1**处停止，此时残差与**x1**之间的角度与残差与**x2**之间的角度相同。然后我们沿着**x1**和**x2**的合成方向前进，达到**y**。
- en: '![Visual showcase of LARS](img/00122.jpeg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![LARS的视觉展示](img/00122.jpeg)'
- en: A code example
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码示例
- en: 'Let''s now see LARS in action in Python on the Diabetic dataset, which consists
    of 10 numerical variables (age, sex, weight, blood pressure, and so on) measured
    on 442 patients, and an indication of disease progression after one year. First,
    we want to visualize the path of the weights of the coefficients. To do so, the
    `lars_path()` class comes to our help (especially if its training is verbose):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在在Python中使用Diabetic数据集来观察LARS的实际操作，该数据集包含10个数值变量（年龄、性别、体重、血压等），在442名患者身上测量，并在一年后显示疾病进展情况。首先，我们想可视化系数的权重路径。为此，`lars_path()`类对我们有所帮助（尤其是如果其训练是冗长的）：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![A code example](img/00123.jpeg)![A code example](img/00124.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![代码示例](img/00123.jpeg)![代码示例](img/00124.jpeg)'
- en: In the output table, you can see that the first feature inserted in the model
    is the number 2, followed by the number 8 and so on. In the image, instead, you
    can simultaneously see the values of the coefficients (colored lines) and the
    steps (dotted lines). Remember that, at every step, one coefficient becomes non-zero,
    and all the coefficients in the model are updated linearly. On the right side
    of the image, you can find the final values of the weights.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出表中，你可以看到模型中插入的第一个特征是数字2，接着是数字8，以此类推。在图像中，相反，你可以同时看到系数的值（彩色线条）和步骤（虚线线条）。记住，在每一步，一个系数变为非零，模型中的所有系数都是线性更新的。在图像的右侧，你可以找到权重的最终值。
- en: 'This is the graphical way to see the LARS coefficients; if we only need a regressor
    (exactly as we''ve seen in the previous chapters), we can just use the `Lars`
    class:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是查看LARS系数的图形方式；如果我们只需要回归器（正如我们在前面的章节中看到的），我们只需使用`Lars`类：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As you may expect, the regressor object can be fitted with the method `.fit`,
    and its weights (coefficients) are exactly the ones shown in the previous screenshot.
    To get the quality of the model, in a similar fashion to the other regressors,
    you can use the method score. In respect of the training data, here''s the scoring
    output:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所预期，回归对象可以使用`.fit`方法进行拟合，其权重（系数）正是之前截图中所显示的。为了获得模型的质量，与其他回归器类似，你可以使用score方法。关于训练数据，以下是评分输出：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: LARS wrap up
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LARS总结
- en: 'Pros:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: The smart way in which coefficients are updated produces low overfitting
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系数更新的智能方式产生了低过拟合
- en: The model is intuitive and easily interpretable
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型直观且易于解释
- en: The training is as fast as Forward Selection
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练速度与Forward Selection相同
- en: It is great when the number of features is comparable with, or greater than,
    the number of observations
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当特征数量与观测数量相当或更大时，它非常出色
- en: 'Cons:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: It might not work very well when the number of features is very large—that is,
    where the number of features is far greater than the number of observations, since
    in such an occurrence it's very probable you'll find spurious correlations
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当特征数量非常大时，即特征数量远大于观测数量时，它可能工作得不是很好，因为在这种情况下，你很可能找到虚假的相关性
- en: It won't work with very noisy features
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不适用于非常嘈杂的特征
- en: Bayesian regression
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯回归
- en: 'Bayesian regression is similar to linear regression, as seen in [Chapter 3](part0023_split_000.html#LTSU2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 3. Multiple Regression in Action"), *Multiple Regression in Action*,
    but, instead of predicting a value, it predicts its probability distribution.
    Let''s start with an example: given `X`, the training observation matrix, and
    `y`, the target vector, linear regression creates a model (that is a series of
    coefficients) that fits the line that has the minimal error with the training
    points. Then, when a new observation arrives, the model is applied to that point,
    and a predicted value is outputted. That''s the only output from linear regression,
    and no conclusions can be made as to whether the prediction, for that specific
    point, is accurate or not. Let''s take a very simple example in code: the observed
    phenomenon has only one feature, and the number of observations is just `10`:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯回归与线性回归类似，如[第3章](part0023_split_000.html#LTSU2-a2faae6898414df7b4ff4c9a487a20c6
    "第3章。实际中的多重回归")《实际中的多重回归》中所述，但它不是预测一个值，而是预测其概率分布。让我们从一个例子开始：给定`X`，训练观察矩阵，和`y`，目标向量，线性回归创建一个模型（即一系列系数），该模型拟合与训练点具有最小误差的线。然后，当新的观察值到来时，模型应用于该点，并输出一个预测值。这就是线性回归的唯一输出，无法得出结论，该特定点的预测是否准确。让我们用一个非常简单的代码示例来说明：观察现象只有一个特征，观察的数量仅为`10`：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, let''s fit a *classic* linear regression model, and let''s try to predict
    the regression value for a point outside the training support (in this simple
    example, we predict the value for a point whose `x` value is double the max of
    the training values):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们拟合一个*经典*的线性回归模型，并尝试预测训练支持范围外的回归值（在这个简单示例中，我们预测一个`x`值是训练值最大值两倍的点的值）：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s now plot the training points, the fitted line, and the predicted test
    point (on the extreme right of the image):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来绘制训练点、拟合线和预测的测试点（图像最右侧的点）：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Bayesian regression](img/00125.jpeg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯回归](img/00125.jpeg)'
- en: To have a probability density function of the predicted value, we should start
    from the beginning and change a hypothesis and some steps in the linear regressor.
    Since this is an advanced algorithm, the math involved is very heavy and we prefer
    to communicate the idea underlying the methods, instead of exposing pages and
    pages of math formulation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要有一个预测值的概率密度函数，我们应该从开始就改变假设和线性回归器中的某些步骤。由于这是一个高级算法，涉及的数学非常复杂，我们更愿意传达方法背后的思想，而不是展示一页又一页的数学公式。
- en: First, we are only able to infer a distribution on the predicted value if every
    variable is modeled as a distribution. In fact, weights in this model are treated
    as random variables with a normal distribution, centered in zero (that is, a spherical
    Gaussian) and having an unknown variance (learnt from the data). The regularization
    imposed by this algorithm is very similar to the one set by Ridge regression.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们只能推断预测值的分布，如果每个变量都被建模为分布。实际上，这个模型中的权重被视为具有正态分布的随机变量，以零为中心（即球形的高斯分布）并且具有未知的方差（从数据中学习）。该算法施加的正则化与岭回归设置的正则化非常相似。
- en: 'The output of a prediction is a value (exactly as in linear regression) and
    a variance value. Using the value as the mean, and the variance as an actual variance,
    we can then represent the probability distribution of the output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的输出是一个值（与线性回归完全一样）和一个方差值。使用该值作为均值，方差作为实际方差，我们就可以表示输出的概率分布：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Bayesian regression](img/00126.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯回归](img/00126.jpeg)'
- en: Bayesian regression wrap up
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯回归总结
- en: 'Pros:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: Robustness to Gaussian noise
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对高斯噪声的鲁棒性
- en: Great if the number of features is comparable to the number of observations
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果特征数量与观察数量相当，则非常好
- en: 'Cons:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: Time-consuming
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 耗时
- en: The hypotheses imposed on the variables are often far from real
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对变量施加的假设通常与实际情况相差甚远
- en: SGD classification with hinge loss
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 hinge 损失的 SGD 分类
- en: 'In [Chapter 4](part0029_split_000.html#RL0A2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 4. Logistic Regression"), *Logistic Regression* we explored a classifier
    based on a regressor, logistic regression. Its goal was to fit the best probabilistic
    function associated with the probability of one point to be classified with a
    label. Now, the core function of the algorithm considers all the training points
    of the dataset: what if it''s only built on the boundary ones? That''s exactly
    the case with the linear **Support Vector Machine** (SVM) classifier, where a
    linear decision plane is drawn by only considering the points close to the separation
    boundary itself.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 章[Chapter 4](part0029_split_000.html#RL0A2-a2faae6898414df7b4ff4c9a487a20c6
    "第 4 章。逻辑回归")中，*逻辑回归*，我们探讨了基于回归器的分类器，逻辑回归。其目标是拟合与一个点被分类为标签的概率相关的最佳概率函数。现在，算法的核心功能考虑了数据集的所有训练点：如果它只基于边界点呢？这正是线性
    **支持向量机** (SVM) 分类器的情况，其中通过仅考虑接近分离边界的点来绘制线性决策平面。
- en: 'Beyond working on the support vectors (the closest points to the boundary),
    SVM uses a new decision loss, called **hinge**. Here''s its formulation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在支撑向量（边界最近的点）上工作之外，SVM 还使用了一种新的决策损失，称为 **hinge**。以下是它的公式：
- en: '![SGD classification with hinge loss](img/00127.jpeg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![使用 hinge 损失的 SGD 分类](img/00127.jpeg)'
- en: Where t is the intended label of the point x and w the set of weights in the
    classifier. The hinge loss is also sometimes called **softmax**, because it's
    actually a clipped max. In this formula, just the boundary points (that is, the
    support vectors) are used.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 t 是点 x 的预期标签，w 是分类器中的权重集。Hinge 损失有时也称为 **softmax**，因为它实际上是一个截断的最大值。在这个公式中，只使用了边界点（即支撑向量）。
- en: In the first instance, this function, although convex, is non differentiable,
    so approaches based on stochastic gradient descent (SGD) are theoretically invalid.
    In practical terms, since it's a continuous function, it has a piecewise derivative.
    This leads to the fact that SGD can be actively used in this technique to derive
    a quick and approximate solution.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，这个函数虽然凸，但不可微，因此基于随机梯度下降（SGD）的方法在理论上是无效的。在实践中，由于它是一个连续函数，它有一个分段导数。这导致
    SGD 可以在这个技术中被积极用于推导出快速和近似的解。
- en: 'Here''s an example in Python: let''s use the `SGDClassifier` class (as seen
    in [Chapter 4](part0029_split_000.html#RL0A2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 4. Logistic Regression"), *Logistic Regression*) with the `hinge` loss,
    applied on a dataset of `100` points drawn from `2` classes. With this piece of
    code, we''re interested in seeing the decision boundary and the support vectors
    chosen by the classifier:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个 Python 中的例子：让我们使用 `SGDClassifier` 类（如第 4 章[Chapter 4](part0029_split_000.html#RL0A2-a2faae6898414df7b4ff4c9a487a20c6
    "第 4 章。逻辑回归")中所示），*逻辑回归*，并使用 `hinge` 损失，应用于从 `2` 个类别中抽取的 `100` 个点的数据集。通过这段代码，我们感兴趣的是看到分类器的决策边界和选择的支撑向量：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![SGD classification with hinge loss](img/00128.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![使用 hinge 损失的 SGD 分类](img/00128.jpeg)'
- en: The image presents the points belonging to the two classes' points (the dots
    on the right and left) and the decision boundary (the solid line between the classes).
    In addition, it contains two dotted lines, which connect the support vectors for
    each class (that is, points on these lines are support vectors). The decision
    boundary is, simply, the line at the same distance between them.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图像展示了属于两个类别的点（左右两侧的点）和决策边界（类别之间的实线）。此外，它还包含两条虚线，它们连接每个类别的支撑向量（即这些线上的点是支撑向量）。决策边界简单地说是它们之间相同距离的线。
- en: Comparison with logistic regression
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与逻辑回归的比较
- en: 'The logistic regression learner is intended to make use of all the input points
    of the training set, and emit a probability as output. SGD with hinge loss, instead,
    directly produces a label, and only uses the points on the boundary to improve
    the model. How are their performances? Let''s make a test with an artificial dataset
    with 20 features (of them, 5 are informative, 5 redundant, and 10 random) and
    10,000 observations. Then, we split the data into 70/30 as training set and test
    set and we train two SGD classifiers: one with the hinge loss function and the
    second with the logistic loss function. Finally, we compare the accuracy of their
    predictions on their test set:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归学习者的目的是利用训练集的所有输入点，并输出一个概率。相反，带有铰链损失的 SGD 直接产生标签，并且只使用边界点来改进模型。它们的性能如何？让我们用一个具有
    20 个特征（其中 5 个是有信息的，5 个是冗余的，10 个是随机的）和 10,000 个观察值的合成数据集进行测试。然后，我们将数据分为 70/30 作为训练集和测试集，并训练两个
    SGD 分类器：一个带有铰链损失函数，另一个带有逻辑损失函数。最后，我们比较它们在测试集上的预测准确率：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As a rule of thumb, SVM is generically more accurate than logistic regression,
    but its performance is not extraordinary. SVM, though, is slower during the training
    process; in fact, with regard to training times, logistic regression is more than
    30% faster than SVM.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 按照惯例，SVM 通常比逻辑回归更准确，但其性能并不出众。然而，SVM 在训练过程中的速度较慢；实际上，在训练时间方面，逻辑回归比 SVM 快 30%
    以上。
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: SVR
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SVR
- en: 'As for linear regressor/logistic regression, even SVM has a regression counterpart,
    called **Support Vector Regressor** (SVR). Its math formulation is very long and
    beyond the scope of this book. However, since it''s very effective, we believe
    it is important to depict how it works in practice, as applied to the Boston dataset
    and compared with a linear regressor model:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于线性回归器/逻辑回归，即使 SVM 有回归对应物，称为 **支持向量回归器** (SVR)。它的数学公式非常长，超出了本书的范围。然而，由于它非常有效，我们认为描述它在实践中是如何工作的很重要，特别是应用于波士顿数据集并与线性回归模型进行比较：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: SVM wrap up
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SVM 总结
- en: 'The pros are as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 优点如下：
- en: Can use SGD to speed up the processing
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 SGD 加速处理
- en: Output is usually more accurate than logistic regression (since only boundary
    points are in the formula)
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出通常比逻辑回归更准确（因为只有边界点在公式中）
- en: 'The cons are as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点如下：
- en: It works very well if the points of the two classes are linearly separable,
    although an extension for non-linearly separable classes is available. In this
    case, though complexity is very high, results are still usually great.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两个类别的点线性可分，则效果非常好，尽管对于非线性可分类别的扩展也是可用的。在这种情况下，尽管复杂性非常高，但结果通常仍然很好。
- en: As for logistic regression, it can be used for two-class problems.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至于逻辑回归，它可以用于二分类问题。
- en: Regression trees (CART)
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归树（CART）
- en: A very common learner, recently used very much due to its speed, is the regression
    tree. It's a non-linear learner, can work with both categorical and numerical
    features, and can be used alternately for classification or regression; that's
    why it's often called **Classification and Regression Tree** (CART). Here, in
    this section, we will see how regression trees work.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常常见的学习者，最近由于其速度而广泛使用，是回归树。它是一个非线性学习者，可以处理分类和数值特征，并且可以交替用于分类或回归；这就是为什么它通常被称为
    **分类和回归树** (CART)。在本节中，我们将了解回归树是如何工作的。
- en: A tree is composed of a series of nodes that split the branch into two children.
    Each branch, then, can go in another node, or remain a leaf with the predicted
    value (or class).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 树由一系列节点组成，这些节点将分支分成两个子节点。然后，每个分支都可以进入另一个节点，或者保持为叶子节点，带有预测值（或类别）。
- en: 'Starting from the root (that is, the whole dataset):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从根节点（即整个数据集）开始：
- en: 'The best feature with which to split the dataset, *F1*, is identified as well
    as the best splitting value. If the feature is numerical, the splitting value
    is a threshold *T1*: in this case, the left child branch will be the set of observations
    where *F1* is below *T1*, and the right one is the set of observations where *F1*
    is greater than, or equal to, *T1*. If the feature is categorical, the splitting
    is done on a subset of levels *S1*: observations where the *F1* feature is one
    of these levels compose the left branch child, all the others compose the right
    branch child.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时确定用于分割数据集的最佳特征*F1*以及最佳分割值。如果特征是数值型，分割值是一个阈值*T1*：在这种情况下，左子节点分支将是*F1*低于*T1*的观测值的集合，而右子节点分支是*F1*大于或等于*T1*的观测值的集合。如果特征是分类型，分割是在子集*S1*上进行的：*F1*特征属于这些级别的观测值组成左子节点分支，其余的观测值组成右子节点分支。
- en: This operation is then run again (independently) for each branch, recursively,
    until there's no more chance to split.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后该操作（独立地）对每个分支再次运行，递归地，直到没有更多分裂的机会。
- en: When the splits are completed, a leaf is created. Leaves denote output values.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当分裂完成后，会创建一个叶子节点。叶子表示输出值。
- en: 'You can immediately see that making the prediction is immediate: you just need
    to traverse the tree from the root to the leaves and, in each node, check whether
    a feature is below (or not) a threshold or, alternatively, has a value inside
    (or outside) a set.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以立即看到预测是即时的：你只需从根节点遍历到叶子节点，并在每个节点检查一个特征是否低于（或不低于）一个阈值，或者，是否有值在（或不在）一个集合内。
- en: 'As a concluding remark, we discuss how to define the best feature to split.
    What about the best value or subset? Well, for regression trees, we use the criteria
    of the variance reduction: in each node, an extensive search is run among all
    features and among all values or levels in that feature. The combination that
    achieves the best possible variance in both the right branch and left branches,
    compared with the input set, is selected and marked as *best*.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作为总结，我们讨论如何定义最佳的分割特征。关于最佳值或子集呢？对于回归树，我们使用方差减少的标准：在每个节点，对所有特征以及在该特征的所有值或级别进行广泛搜索。选择能够实现左右分支相对于输入集最佳方差组合，并将其标记为*最佳*。
- en: Note that regression trees decide, for each node, the optimal split. Such a
    local optimization approach unfortunately leads to a suboptimal result. In addition,
    it is advisable that the regression tree should be pruned; that is, you should
    remove some leaves to prevent overfitting (for example, by setting a minimum threshold
    to the variance reduction measure). Such are the drawbacks of regression trees.
    On the other hand, they are somehow accurate and relatively quick to train and
    test.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，回归树为每个节点决定最优分割。这种局部优化方法不幸地导致次优结果。此外，建议对回归树进行剪枝；也就是说，你应该移除一些叶子节点以防止过拟合（例如，通过设置方差减少测量的最小阈值）。这就是回归树的缺点。另一方面，它们在某种程度上是准确的，并且相对快速地进行训练和测试。
- en: 'In the code, regression trees are as easy as the other regressors:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，回归树与其他回归器一样简单：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Regression tree wrap up
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归树总结
- en: 'Pros:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: They can model non-linear behaviors
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以模拟非线性行为
- en: Great for categorical features and numerical features, without normalization
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常适合分类特征和数值特征，无需归一化
- en: Same approach for classification and regression
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类和回归使用相同的方法
- en: Fast training, fast prediction time, and small memory fingerprint
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速训练，快速预测时间，内存占用小
- en: 'Cons:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: 'Greedy algorithm: it doesn''t optimize the full solution, just the best choice.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机算法：它不优化完整解，只优化最佳选择。
- en: It doesn't work very well when the number of features is significant.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当特征数量很多时，效果不佳。
- en: Leaves can be very specific. In this case, we need to "prune the tree", removing
    some nodes.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 叶子可以非常具体。在这种情况下，我们需要“剪枝树”，移除一些节点。
- en: Bagging and boosting
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bagging和boosting
- en: Bagging and boosting are two techniques used to combine learners. These techniques
    are classified under the generic name of **ensembles** (or meta-algorithm) because
    the ultimate goal is actually to ensemble *weak* learners to create a more sophisticated,
    but more accurate, model. There is no formal definition of a weak learner, but
    ideally it's a fast, sometimes linear model that not necessarily produces excellent
    results (it suffices that they are just better than a random guess). The final
    ensemble is typically a non-linear learner whose performance increases with the
    number of weak learners in the model (note that the relation is strictly non-linear).
    Let's now see how they work.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging和boosting是两种用于结合学习者的技术。这些技术被归类为通用名称**集成**（或元算法），因为最终目标实际上是将**弱**学习者集成起来，以创建一个更复杂但更准确的模型。弱学习者的定义并不正式，但理想情况下，它是一个快速、有时是线性的模型，不一定产生优秀的结果（只要它们比随机猜测好就足够了）。最终的集成通常是具有非线性学习者的模型，其性能随着模型中弱学习者数量的增加而提高（注意，这种关系是严格非线性的）。现在让我们看看它们是如何工作的。
- en: Bagging
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bagging
- en: 'Bagging stands for **Bootstrap Aggregating**, and its ultimate goal is to reduce
    variance by averaging weak learners'' results. Let''s now see the code; we will
    explain how it works. As a dataset, we will reuse the Boston dataset (and its
    validation split) from the previous example:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging代表**Bootstrap Aggregating**，其最终目标是通过对弱学习者的结果进行平均来减少方差。现在让我们看看代码；我们将解释它是如何工作的。作为一个数据集，我们将重用之前示例中的波士顿数据集（及其验证分割）：
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `BaggingRegressor` class, from the `submodule` ensemble of Scikit-learn,
    is the base class to create bagging regressors. It requires the weak learner (in
    the example, it''s a `SGDRegressor`), the total number of regressors (1,000),
    and the maximum number of features to be used in each regressor (80% of the total
    number). Then, the bagging learner is trained as with the other learners seen
    so far, with the method fit. At this point, for each weak learner:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn的`BaggingRegressor`类是创建Bagging回归器的基类。它需要弱学习者（在示例中，它是一个`SGDRegressor`），回归器的总数（1,000），以及每个回归器中要使用的最大特征数（总数的80%）。然后，Bagging学习者就像之前看到的其他学习者一样，通过fit方法进行训练。在这个阶段，对于每个弱学习者：
- en: 80% of the features composing the *X* train dataset are selected at random
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X训练数据集组成特征的80%是随机选择的
- en: The weak learner is trained just on the selected features on a bootstrap with
    a replacement set of observations in the training set
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弱学习者在训练集的替换样本的bootstrap上仅针对选定的特征进行训练
- en: At the end, the bagging model contains 1,000 trained `SGDRegressors`. When a
    prediction is requested from the ensemble, each of the 1,000 weak learners makes
    its prediction, then the results are averaged, producing the ensemble prediction.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Bagging模型包含1,000个训练好的`SGDRegressors`。当从集成请求预测时，每个弱学习者都会做出预测，然后对结果进行平均，产生集成预测。
- en: Please note that both training and prediction operations are per-weak learner;
    therefore they can be parallelized on multiple CPUs (that's why `n_jobs` is `-1`
    in the example; that is, we use all the cores).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，训练和预测操作都是针对弱学习者的；因此，它们可以在多个CPU上并行化（这就是为什么示例中的`n_jobs`是`-1`的原因；即，我们使用所有核心）。
- en: The final result, in terms of MAE, should be better than a single `SGDRegressor`;
    on the other hand, the model is about 1,000 times more complex.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果，从MAE的角度来看，应该比单个`SGDRegressor`更好；另一方面，模型大约复杂了1,000倍。
- en: 'Typically, ensembles are associated with decision or regression trees. In that
    case, the name of the regression ensemble changes to Random Forest Regressor (that
    is, a forest, composed of multiple trees). Since this technique is often used
    as the *default* bagging ensemble, there is an ad hoc class in Scikit-learn:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，集成与决策树或回归树相关联。在这种情况下，回归集成的名称变为随机森林回归器（即，由多个树组成的森林）。由于这种技术通常用作*默认*的Bagging集成，Scikit-learn中有一个专门的类：
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'One additional feature of Random Forests is their ability to rank feature importance
    in the model (that is, they detect which features produce the highest variation
    of the predicted variable). Here''s the code; always remember to normalize the
    feature matrix first (we''ve already done it in the previous section):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的一个额外特性是它们能够对模型中的特征重要性进行排序（即，它们检测哪些特征产生了预测变量的最高变异）。以下是代码；请始终记住首先对特征矩阵进行归一化（我们已经在上一节中这样做过了）：
- en: '[PRE14]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Bagging](img/00129.jpeg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![Bagging](img/00129.jpeg)'
- en: The list is sorted from the most important feature to the least important (for
    this ensemble). If you change the weak learner, or any other parameter, this list
    may change.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 列表按从最重要的特征到最不重要的特征排序（对于这个集成）。如果你更改弱学习者或任何其他参数，这个列表可能会改变。
- en: Boosting
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升
- en: 'Boosting is a way to combine (ensemble) weak learners, primarily to reduce
    prediction bias. Instead of creating a pool of predictors, as in bagging, boosting
    produces a cascade of them, where each output is the input for the following learner.
    We''ll start with an example, exactly as we''ve done in the previous sub-section:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 提升是一种结合（集成）弱学习者的方法，主要是为了减少预测偏差。与袋装不同，提升产生了一系列的预测者，其中每个输出都是下一个学习者的输入。我们将从一个例子开始，就像我们在前面的子节中做的那样：
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `AdaBoostRegressor` class, from the `submodule` ensemble of Scikit-learn,
    is the base class to create a Boosted Regressor. As for the bagging, it requires
    the weak learner (an `SGDRegressor`), the total number of regressors (100), and
    the learning rate (0.01). Starting from an unfitted ensemble, for each weak learner
    the training is:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Scikit-learn 的 `submodule` 集成中的 `AdaBoostRegressor` 类是创建提升回归器的基类。至于袋装，它需要一个弱学习者（一个
    `SGDRegressor`），回归器的总数（100），以及学习率（0.01）。从一个未拟合的集成开始，对于每个弱学习者，训练如下：
- en: Given the training set, the cascade of already-fit learners produces a prediction
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定训练集，已经拟合好的学习者的级联产生了一个预测
- en: The error between the actual values and the predicted ones, multiplied by the
    learning rate, is computed
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际值与预测值之间的误差，乘以学习率，被计算
- en: A new weak learner is trained on that error set, and inserted as the last stage
    in the cascade of already trained learners
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在那个错误集上训练一个新的弱学习者，并将其插入到已经训练好的学习者的级联的最后阶段
- en: 'At the end of the training stage, the ensemble contains 100 trained `SGDRegressors`
    organized in a cascade. When a prediction is requested from the ensemble, the
    final value is a recursive operation: starting from the last stage, the output
    value is the value predicted by the previous stage plus the learning rate multiplied
    by the prediction of the current stage.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段结束时，集成包含 100 个训练好的 `SGDRegressors`，它们以级联的形式组织。当从集成请求预测时，最终值是一个递归操作：从最后阶段开始，输出值是前一个阶段的预测值加上学习率乘以当前阶段的预测值。
- en: The learning rate is similar to the one from the stochastic gradient descent.
    A smaller learning rate will require more steps to approach the results, but the
    granularity of the output will be better. A bigger rate will require fewer steps,
    but will probably approach a less accurate result.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率类似于随机梯度下降中的学习率。较小的学习率需要更多步骤来接近结果，但输出将更精细。较大的学习率需要更少的步骤，但可能接近一个不太准确的结果。
- en: Please note here that training and testing cannot be done independently on each
    weak learner, since to train a model you need the chain of outputs of the previous
    ones. This fact limits the CPU usage to only one, limiting the length of the cascade.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于训练和测试不能独立地对每个弱学习者进行，因为要训练一个模型你需要前一个输出链，所以这限制了 CPU 的使用仅限于一个，限制了级联的长度。
- en: 'In the case of boosting with Decision/Regression Trees, the Scikit-learn package
    offers a pre-build class called `GradientBoostingRegressor`. A short code snippet
    should suffice to demonstrate how it works:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用决策/回归树进行提升的情况下，Scikit-learn 包提供了一个预构建的类，称为 `GradientBoostingRegressor`。一个简短的代码片段应该足以演示它是如何工作的：
- en: '[PRE16]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Even with Boosting, it is possible to rank feature importance. In fact, it''s
    the very same method:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用提升，也有可能对特征重要性进行排序。事实上，这正是相同的方法：
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Boosting](img/00130.jpeg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![提升](img/00130.jpeg)'
- en: Ensemble wrap up
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成总结
- en: 'The pros are as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 优点如下：
- en: Strong learners based on weak learners
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于弱学习者的强学习者
- en: They enable stochastic learning
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们使随机学习成为可能
- en: The randomness of the process creates a robust solution
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过程的随机性产生了一个稳健的解决方案
- en: 'The cons are as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点如下：
- en: Training time is considerable, as well as the memory footprint
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练时间相当长，以及内存占用
- en: The learning step (in the boosted ensemble) can be very tricky to properly set,
    similar to the update step (alpha) in the stochastic gradient descent
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提升集成中，学习步骤（alpha）的设置可能非常棘手，类似于随机梯度下降中的更新步骤
- en: Gradient Boosting Regressor with LAD
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有 LAD 的梯度提升回归器
- en: More than a new technique, this is an ensemble of technologies already seen
    in this book, with a new loss function, the **Least Absolute Deviations** (LAD).
    With respect to the least square function, seen in the previous chapter, with
    LAD the L1 norm of the error is computed.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅仅是一种新技术，它还是一种技术集合，这些技术已经在本书中出现过，并引入了一个新的损失函数，即**最小绝对偏差**（LAD）。与之前章节中提到的最小二乘函数相比，使用LAD计算误差的L1范数。
- en: 'Regressor learners based on LAD are typically robust but unstable, because
    of the multiple minima of the loss function (leading therefore to multiple best
    solutions). Alone, this loss function seems to bear little value, but paired with
    gradient boosting, it creates a very stable regressor, due to the fact that boosting
    overcomes LAD regression limitations. With the code, this is very simple to achieve:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LAD的回归学习器通常是鲁棒的但不够稳定，因为损失函数存在多个最小值（因此导致多个最佳解）。单独来看，这个损失函数似乎价值不大，但与梯度提升相结合，它创建了一个非常稳定的回归器，因为提升算法克服了LAD回归的限制。通过代码，这非常简单实现：
- en: '[PRE18]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Remember to specify to use the `'lad'` loss, otherwise the default least square
    (L²) is used. In addition, another loss function, `huber`, combines the least
    square loss and the least absolute deviation loss to create a loss function even
    more robust. To try it, just insert the string value `'huber'` instead of `'lad'`
    in the last run piece of code.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 记得指定使用`'lad'`损失，否则默认使用最小二乘（L²）损失。此外，另一个损失函数`huber`结合了最小二乘损失和最小绝对偏差损失，创建了一个更加鲁棒的损失函数。要尝试它，只需在最后一段代码中插入字符串值`'huber'`代替`'lad'`。
- en: GBM with LAD wrap up
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于LAD的GBM总结
- en: The pros are that it combines the strength of a boosted ensemble to the LAD
    loss, producing a very stable and robust learner and the cons are that training
    time is very high (exactly the same as training N consecutive LAD learners, one
    after the other).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 优点是它结合了提升集成算法的强度与LAD损失，产生了一个非常稳定且鲁棒的学习者；缺点是训练时间非常高（与连续训练N个LAD学习器的时间完全相同）。
- en: Summary
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter concludes the long journey around regression methods we have taken
    throughout this book. We have seen how to deal with different kinds of regression
    modeling, how to pre-process data, and how to evaluate the results. In the present
    chapter, we glanced at some cutting-edge techniques. In the next, and last, chapter
    of the book, we apply regression in real-world examples and invite you to experiment
    with some concrete examples.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结了我们在本书中关于回归方法的漫长旅程。我们看到了如何处理不同类型的回归建模，如何预处理数据，以及如何评估结果。在本章中，我们简要介绍了某些前沿技术。在下一章，也是最后一章中，我们将回归应用于现实世界的例子，并邀请您尝试一些具体的例子。
