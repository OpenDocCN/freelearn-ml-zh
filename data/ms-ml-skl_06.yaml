- en: Chapter 6. Clustering with K-Means
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 K-Means聚类
- en: In the previous chapters we discussed supervised learning tasks; we examined
    algorithms for regression and classification that learned from labeled training
    data. In this chapter we will discuss an unsupervised learning task called clustering.
    Clustering is used to find groups of similar observations within a set of unlabeled
    data. We will discuss the K-Means clustering algorithm, apply it to an image compression
    problem, and learn to measure its performance. Finally, we will work through a
    semi-supervised learning problem that combines clustering with classification.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们讨论了监督学习任务；我们研究了从标注训练数据中学习的回归和分类算法。在本章中，我们将讨论一种无监督学习任务——聚类。聚类用于在未标注的数据集中寻找相似观测值的群组。我们将讨论K-Means聚类算法，并将其应用于图像压缩问题，学习如何评估其性能。最后，我们将探讨一个结合聚类和分类的半监督学习问题。
- en: 'Recall from [Chapter 1](ch01.html "Chapter 1. The Fundamentals of Machine Learning"),
    *The Fundamentals of Machine Learning*, that the goal of unsupervised learning
    is to discover hidden structure or patterns in unlabeled training data. **Clustering**,
    or **cluster analysis**, is the task of grouping observations such that members
    of the same group, or cluster, are more similar to each other by a given metric
    than they are to the members of the other clusters. As with supervised learning,
    we will represent an observation as an *n*-dimensional vector. For example, assume
    that your training data consists of the samples plotted in the following figure:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 请回想一下[第1章](ch01.html "第1章 机器学习基础")，*机器学习基础*中提到的，无监督学习的目标是发现未标注训练数据中隐藏的结构或模式。**聚类**，或称为**聚类分析**，是将观测结果分组的任务，使得同一组或簇内的成员在给定的度量标准下比其他簇的成员更相似。与监督学习一样，我们将观测值表示为*n*维向量。例如，假设你的训练数据由下图中的样本组成：
- en: '![Clustering with K-Means](img/8365OS_06_01.jpg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![K-Means聚类](img/8365OS_06_01.jpg)'
- en: 'Clustering might reveal the following two groups, indicated by squares and
    circles:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类可能会揭示以下两个群体，用方框和圆圈表示：
- en: '![Clustering with K-Means](img/8365OS_06_02.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![K-Means聚类](img/8365OS_06_02.jpg)'
- en: 'Clustering could also reveal the following four groups:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类也可能揭示以下四个群体：
- en: '![Clustering with K-Means](img/8365OS_06_03.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![K-Means聚类](img/8365OS_06_03.jpg)'
- en: Clustering is commonly used to explore a dataset. Social networks can be clustered
    to identify communities and to suggest missing connections between people. In
    biology, clustering is used to find groups of genes with similar expression patterns.
    Recommendation systems sometimes employ clustering to identify products or media
    that might appeal to a user. In marketing, clustering is used to find segments
    of similar consumers. In the following sections, we will work through an example
    of using the K-Means algorithm to cluster a dataset.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类常用于探索数据集。社交网络可以进行聚类，以识别社区并建议人们之间缺失的连接。在生物学中，聚类用于寻找具有相似表达模式的基因群组。推荐系统有时会使用聚类来识别可能吸引用户的产品或媒体。在营销中，聚类用于寻找相似消费者的细分群体。在接下来的章节中，我们将通过一个使用K-Means算法进行数据集聚类的示例。
- en: Clustering with the K-Means algorithm
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用K-Means算法进行聚类
- en: The K-Means algorithm is a clustering method that is popular because of its
    speed and scalability. K-Means is an iterative process of moving the centers of
    the clusters, or the **centroids**, to the mean position of their constituent
    points, and re-assigning instances to their closest clusters. The titular ![Clustering
    with the K-Means algorithm](img/8365OS_06_18.jpg) is a hyperparameter that specifies
    the number of clusters that should be created; K-Means automatically assigns observations
    to clusters but cannot determine the appropriate number of clusters. ![Clustering
    with the K-Means algorithm](img/8365OS_06_18.jpg) must be a positive integer that
    is less than the number of instances in the training set. Sometimes, the number
    of clusters is specified by the clustering problem's context. For example, a company
    that manufactures shoes might know that it is able to support manufacturing three
    new models. To understand what groups of customers to target with each model,
    it surveys customers and creates three clusters from the results. That is, the
    value of ![Clustering with the K-Means algorithm](img/8365OS_06_18.jpg) was specified
    by the problem's context. Other problems may not require a specific number of
    clusters, and the optimal number of clusters may be ambiguous. We will discuss
    a heuristic to estimate the optimal number of clusters called the elbow method
    later in this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means算法是一种流行的聚类方法，因其速度和可扩展性而受到青睐。K-Means是一个迭代过程，通过将聚类的中心，即**质心**，移动到其组成点的均值位置，并重新将实例分配到它们最近的聚类中。标题中的![K-Means算法的聚类](img/8365OS_06_18.jpg)是一个超参数，用于指定应该创建的聚类数；K-Means会自动将观察值分配到聚类中，但无法确定适当的聚类数量。![K-Means算法的聚类](img/8365OS_06_18.jpg)必须是一个小于训练集实例数的正整数。有时，聚类问题的上下文会指定聚类数。例如，一个生产鞋子的公司可能知道它能够支持生产三种新款式。为了了解每种款式应该面向哪些客户群体，它对客户进行了调查，并从结果中创建了三个聚类。也就是说，![K-Means算法的聚类](img/8365OS_06_18.jpg)的值是由问题的上下文指定的。其他问题可能不需要特定数量的聚类，且最优聚类数可能模糊不清。我们将在本章后面讨论一种估算最优聚类数的启发式方法，称为肘部法则。
- en: 'The parameters of K-Means are the positions of the clusters'' centroids and
    the observations that are assigned to each cluster. Like generalized linear models
    and decision trees, the optimal values of K-Means'' parameters are found by minimizing
    a cost function. The cost function for K-Means is given by the following equation:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means的参数包括聚类质心的位置和分配给每个聚类的观察值。像广义线性模型和决策树一样，K-Means参数的最优值是通过最小化一个成本函数来找到的。K-Means的成本函数由以下公式给出：
- en: '![Clustering with the K-Means algorithm](img/8365OS_06_04.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![K-Means算法的聚类](img/8365OS_06_04.jpg)'
- en: 'In the preceding equation, ![Clustering with the K-Means algorithm](img/8365OS_06_19.jpg)
    is the centroid for the cluster ![Clustering with the K-Means algorithm](img/8365OS_06_20.jpg).
    The cost function sums the distortions of the clusters. Each cluster''s distortion
    is equal to the sum of the squared distances between its centroid and its constituent
    instances. The distortion is small for compact clusters and large for clusters
    that contain scattered instances. The parameters that minimize the cost function
    are learned through an iterative process of assigning observations to clusters
    and then moving the clusters. First, the clusters'' centroids are initialized
    to random positions. In practice, setting the centroids'' positions equal to the
    positions of randomly selected observations yields the best results. During each
    iteration, K-Means assigns observations to the cluster that they are closest to,
    and then moves the centroids to their assigned observations'' mean location. Let''s
    work through an example by hand using the training data shown in the following
    table:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述公式中，![K-Means算法的聚类](img/8365OS_06_19.jpg)是聚类![K-Means算法的聚类](img/8365OS_06_20.jpg)的质心。成本函数对聚类的扭曲进行求和。每个聚类的扭曲等于其质心与其组成实例之间的平方距离之和。紧凑的聚类扭曲较小，而包含分散实例的聚类扭曲较大。通过一个迭代过程来学习最小化成本函数的参数，过程包括将观察值分配到聚类中，然后移动聚类。首先，聚类的质心被初始化为随机位置。实际上，将质心的位置设置为随机选择的观察值的位置通常能得到最佳结果。在每次迭代中，K-Means将观察值分配到它们最近的聚类中，然后将质心移动到它们分配的观察值的均值位置。我们通过手动操作一个例子来演示，使用如下表中的训练数据：
- en: '| Instance | X0 | X1 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 实例 | X0 | X1 |'
- en: '| --- | --- | --- |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 7 | 5 |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 7 | 5 |'
- en: '| 2 | 5 | 7 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 5 | 7 |'
- en: '| 3 | 7 | 7 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 7 | 7 |'
- en: '| 4 | 3 | 3 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 3 | 3 |'
- en: '| 5 | 4 | 6 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 4 | 6 |'
- en: '| 6 | 1 | 4 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1 | 4 |'
- en: '| 7 | 0 | 0 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0 | 0 |'
- en: '| 8 | 2 | 2 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 2 | 2 |'
- en: '| 9 | 8 | 7 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 8 | 7 |'
- en: '| 10 | 6 | 8 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 6 | 8 |'
- en: '| 11 | 5 | 5 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 5 | 5 |'
- en: '| 12 | 3 | 7 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 3 | 7 |'
- en: 'There are two explanatory variables and each instance has two features. The
    instances are plotted in the following figure:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个解释变量，每个实例有两个特征。实例的散点图如下所示：
- en: '![Clustering with the K-Means algorithm](img/8365OS_06_05.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![使用 K-Means 算法的聚类](img/8365OS_06_05.jpg)'
- en: 'Assume that K-Means initializes the centroid for the first cluster to the fifth
    instance and the centroid for the second cluster to the eleventh instance. For
    each instance, we will calculate its distance to both centroids, and assign it
    to the cluster with the closest centroid. The initial assignments are shown in
    the **Cluster** column of the following table:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 K-Means 将第一个聚类的重心初始化为第五个实例，将第二个聚类的重心初始化为第十一个实例。对于每个实例，我们将计算其到两个重心的距离，并将其分配给距离最近的聚类。初始分配情况显示在下表的**Cluster**列中：
- en: '| Instance | X0 | X1 | C1 distance | C2 distance | Last cluster | Cluster |
    Changed? |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 实例 | X0 | X1 | C1 距离 | C2 距离 | 上一个聚类 | 新聚类 | 是否改变？ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 7 | 5 | 3.16228 | 2 | None | C2 | Yes |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 7 | 5 | 3.16228 | 2 | 无 | C2 | 是 |'
- en: '| 2 | 5 | 7 | 1.41421 | 2 | None | C1 | Yes |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 5 | 7 | 1.41421 | 2 | 无 | C1 | 是 |'
- en: '| 3 | 7 | 7 | 3.16228 | 2.82843 | None | C2 | Yes |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 7 | 7 | 3.16228 | 2.82843 | 无 | C2 | 是 |'
- en: '| 4 | 3 | 3 | 3.16228 | 2.82843 | None | C2 | Yes |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 3 | 3 | 3.16228 | 2.82843 | 无 | C2 | 是 |'
- en: '| 5 | 4 | 6 | 0 | 1.41421 | None | C1 | Yes |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 4 | 6 | 0 | 1.41421 | 无 | C1 | 是 |'
- en: '| 6 | 1 | 4 | 3.60555 | 4.12311 | None | C1 | Yes |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1 | 4 | 3.60555 | 4.12311 | 无 | C1 | 是 |'
- en: '| 7 | 0 | 0 | 7.21110 | 7.07107 | None | C2 | Yes |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0 | 0 | 7.21110 | 7.07107 | 无 | C2 | 是 |'
- en: '| 8 | 2 | 2 | 4.47214 | 4.24264 | None | C2 | Yes |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 2 | 2 | 4.47214 | 4.24264 | 无 | C2 | 是 |'
- en: '| 9 | 8 | 7 | 4.12311 | 3.60555 | None | C2 | Yes |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 8 | 7 | 4.12311 | 3.60555 | 无 | C2 | 是 |'
- en: '| 10 | 6 | 8 | 2.82843 | 3.16228 | None | C1 | Yes |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 6 | 8 | 2.82843 | 3.16228 | 无 | C1 | 是 |'
- en: '| 11 | 5 | 5 | 1.41421 | 0 | None | C2 | Yes |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 5 | 5 | 1.41421 | 0 | 无 | C2 | 是 |'
- en: '| 12 | 3 | 7 | 1.41421 | 2.82843 | None | C1 | Yes |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 3 | 7 | 1.41421 | 2.82843 | 无 | C1 | 是 |'
- en: '| C1 centroid | 4 | 6 |   |   |   |   |   |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| C1 重心 | 4 | 6 |   |   |   |   |   |'
- en: '| C2 centroid | 5 | 5 |   |   |   |   |   |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| C2 重心 | 5 | 5 |   |   |   |   |   |'
- en: The plotted centroids and the initial cluster assignments are shown in the following
    graph. Instances assigned to the first cluster are marked with **Xs**, and instances
    assigned to the second cluster are marked with dots. The markers for the centroids
    are larger than the markers for the instances.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制的重心和初始聚类分配如下图所示。分配给第一个聚类的实例用**X**标记，分配给第二个聚类的实例用点标记。重心的标记比实例的标记大。
- en: '![Clustering with the K-Means algorithm](img/8365OS_06_06.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![使用 K-Means 算法的聚类](img/8365OS_06_06.jpg)'
- en: 'Now we will move both centroids to the means of their constituent instances,
    recalculate the distances of the training instances to the centroids, and reassign
    the instances to the closest centroids:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将两个重心移动到其组成实例的均值位置，重新计算训练实例到重心的距离，并将实例重新分配到距离最近的重心：
- en: '| Instance | X0 | X1 | C1 distance | C2 distance | Last Cluster | New Cluster
    | Changed? |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 实例 | X0 | X1 | C1 距离 | C2 距离 | 上一个聚类 | 新聚类 | 是否改变？ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 7 | 5 | 3.492850 | 2.575394 | C2 | C2 | No |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 7 | 5 | 3.492850 | 2.575394 | C2 | C2 | 否 |'
- en: '| 2 | 5 | 7 | 1.341641 | 2.889107 | C1 | C1 | No |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 5 | 7 | 1.341641 | 2.889107 | C1 | C1 | 否 |'
- en: '| 3 | 7 | 7 | 3.255764 | 3.749830 | C2 | C1 | Yes |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 7 | 7 | 3.255764 | 3.749830 | C2 | C1 | 是 |'
- en: '| 4 | 3 | 3 | 3.492850 | 1.943067 | C2 | C2 | No |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 3 | 3 | 3.492850 | 1.943067 | C2 | C2 | 否 |'
- en: '| 5 | 4 | 6 | 0.447214 | 1.943067 | C1 | C1 | No |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 4 | 6 | 0.447214 | 1.943067 | C1 | C1 | 否 |'
- en: '| 6 | 1 | 4 | 3.687818 | 3.574285 | C1 | C2 | Yes |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1 | 4 | 3.687818 | 3.574285 | C1 | C2 | 是 |'
- en: '| 7 | 0 | 0 | 7.443118 | 6.169378 | C2 | C2 | No |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0 | 0 | 7.443118 | 6.169378 | C2 | C2 | 否 |'
- en: '| 8 | 2 | 2 | 4.753946 | 3.347250 | C2 | C2 | No |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 2 | 2 | 4.753946 | 3.347250 | C2 | C2 | 否 |'
- en: '| 9 | 8 | 7 | 4.242641 | 4.463000 | C2 | C1 | Yes |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 8 | 7 | 4.242641 | 4.463000 | C2 | C1 | 是 |'
- en: '| 10 | 6 | 8 | 2.720294 | 4.113194 | C1 | C1 | No |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 6 | 8 | 2.720294 | 4.113194 | C1 | C1 | 否 |'
- en: '| 11 | 5 | 5 | 1.843909 | 0.958315 | C2 | C2 | No |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 5 | 5 | 1.843909 | 0.958315 | C2 | C2 | 否 |'
- en: '| 12 | 3 | 7 | 1 | 3.260775 | C1 | C1 | No |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 3 | 7 | 1 | 3.260775 | C1 | C1 | 否 |'
- en: '| C1 centroid | 3.8 | 6.4 |   |   |   |   |   |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| C1 重心 | 3.8 | 6.4 |   |   |   |   |   |'
- en: '| C2 centroid | 4.571429 | 4.142857 |   |   |   |   |   |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| C2 重心 | 4.571429 | 4.142857 |   |   |   |   |   |'
- en: 'The new clusters are plotted in the following graph. Note that the centroids
    are diverging and several instances have changed their assignments:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 新的聚类在下图中进行了绘制。请注意，质心在分散，并且几个实例已改变了其分配：
- en: '![Clustering with the K-Means algorithm](img/8365OS_06_07.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![K-Means算法聚类](img/8365OS_06_07.jpg)'
- en: 'Now, we will move the centroids to the means of their constituents'' locations
    again and reassign the instances to their nearest centroids. The centroids continue
    to diverge, as shown in the following figure:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将再次将质心移至其构成实例的位置的均值，并重新将实例分配给最近的质心。质心继续分散，如下图所示：
- en: '![Clustering with the K-Means algorithm](img/8365OS_06_08.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![K-Means算法聚类](img/8365OS_06_08.jpg)'
- en: None of the instances' centroid assignments will change in the next iteration;
    K-Means will continue iterating until some stopping criteria is satisfied. Usually,
    this criterion is either a threshold for the difference between the values of
    the cost function for subsequent iterations, or a threshold for the change in
    the positions of the centroids between subsequent iterations. If these stopping
    criteria are small enough, K-Means will converge on an optimum. This optimum will
    not necessarily be the global optimum.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一次迭代中，实例的质心分配不会发生变化；K-Means将继续迭代，直到满足某个停止准则。通常，这个准则是后续迭代中成本函数值之间的差异阈值，或质心位置变化的阈值。如果这些停止准则足够小，K-Means会收敛到一个最优解。这个最优解不一定是全局最优解。
- en: Local optima
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局部最优解
- en: 'Recall that K-Means initially sets the positions of the clusters'' centroids
    to the positions of randomly selected observations. Sometimes, the random initialization
    is unlucky and the centroids are set to positions that cause K-Means to converge
    to a local optimum. For example, assume that K-Means randomly initializes two
    cluster centroids to the following positions:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，K-Means最初将聚类的质心位置设置为随机选定的观察值位置。有时，随机初始化不太幸运，质心被设置到导致K-Means收敛到局部最优解的位置。例如，假设K-Means随机初始化了两个聚类质心，位置如下：
- en: '![Local optima](img/8365OS_06_09.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![局部最优解](img/8365OS_06_09.jpg)'
- en: K-Means will eventually converge on a local optimum like that shown in the following
    figure. These clusters may be informative, but it is more likely that the top
    and bottom groups of observations are more informative clusters. To avoid local
    optima, K-Means is often repeated dozens or even hundreds of times. In each iteration,
    it is randomly initialized to different starting cluster positions. The initialization
    that minimizes the cost function best is selected.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means最终会收敛到一个局部最优解，如下图所示。这些聚类可能有信息价值，但更可能的是，顶部和底部的观察组会形成更具信息性的聚类。为了避免局部最优解，K-Means通常会重复执行数十次甚至数百次。在每次迭代中，它会随机初始化到不同的起始聚类位置。选择使成本函数最小化的初始化方案。
- en: '![Local optima](img/8365OS_06_10.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![局部最优解](img/8365OS_06_10.jpg)'
- en: The elbow method
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 肘部法则
- en: 'If ![The elbow method](img/8365OS_06_18.jpg) is not specified by the problem''s
    context, the optimal number of clusters can be estimated using a technique called
    the **elbow method**. The elbow method plots the value of the cost function produced
    by different values of ![The elbow method](img/8365OS_06_18.jpg). As ![The elbow
    method](img/8365OS_06_18.jpg) increases, the average distortion will decrease;
    each cluster will have fewer constituent instances, and the instances will be
    closer to their respective centroids. However, the improvements to the average
    distortion will decline as ![The elbow method](img/8365OS_06_18.jpg) increases.
    The value of ![The elbow method](img/8365OS_06_18.jpg) at which the improvement
    to the distortion declines the most is called the elbow. Let''s use the elbow
    method to choose the number of clusters for a dataset. The following scatter plot
    visualizes a dataset with two obvious clusters:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果问题的背景没有指定![肘部法则](img/8365OS_06_18.jpg)，则可以使用一种叫做**肘部法则**的技术来估计最佳聚类数。肘部法则通过不同的![肘部法则](img/8365OS_06_18.jpg)值绘制由成本函数产生的值。当![肘部法则](img/8365OS_06_18.jpg)增加时，平均失真度会减小；每个聚类的构成实例会更少，且实例会更接近各自的质心。然而，随着![肘部法则](img/8365OS_06_18.jpg)的增加，平均失真度的改善会逐渐减少。在失真度改善下降最明显的![肘部法则](img/8365OS_06_18.jpg)值处，称为“肘部”。我们可以使用肘部法则来选择数据集的聚类数。下图的散点图可视化了一个有两个明显聚类的数据集：
- en: '![The elbow method](img/8365OS_06_11.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![肘部法则](img/8365OS_06_11.jpg)'
- en: 'We will calculate and plot the mean distortion of the clusters for each value
    of ![The elbow method](img/8365OS_06_18.jpg) from 1 to 10 with the following code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码计算并绘制每个![肘部法则](img/8365OS_06_18.jpg)值从1到10的聚类平均失真：
- en: '[PRE0]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![The elbow method](img/8365OS_06_12.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![肘部法则](img/8365OS_06_12.jpg)'
- en: 'The average distortion improves rapidly as we increase ![The elbow method](img/8365OS_06_18.jpg)
    from **1** to **2**. There is little improvement for values of ![The elbow method](img/8365OS_06_18.jpg)
    greater than 2\. Now let''s use the elbow method on the following dataset with
    three clusters:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们将![肘部法则](img/8365OS_06_18.jpg)从**1**增加到**2**，平均失真迅速改善。对于大于2的![肘部法则](img/8365OS_06_18.jpg)值，改进非常小。现在，我们将肘部法则应用于具有三个聚类的以下数据集：
- en: '![The elbow method](img/8365OS_06_13.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![肘部法则](img/8365OS_06_13.jpg)'
- en: The following figure shows the elbow plot for the dataset. From this, we can
    see that the rate of improvement to the average distortion declines the most when
    adding a fourth cluster, that is, the elbow method confirms that ![The elbow method](img/8365OS_06_18.jpg)
    should be set to three for this dataset.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图形展示了该数据集的肘部图。由此可以看出，当添加第四个聚类时，平均失真的改善速度急剧下降，也就是说，肘部法则确认对于该数据集，![肘部法则](img/8365OS_06_18.jpg)应设置为三。
- en: '![The elbow method](img/8365OS_06_14.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![肘部法则](img/8365OS_06_14.jpg)'
- en: Evaluating clusters
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估聚类
- en: 'We defined machine learning as the design and study of systems that learn from
    experience to improve their performance of a task as measured by a given metric.
    K-Means is an unsupervised learning algorithm; there are no labels or ground truth
    to compare with the clusters. However, we can still evaluate the performance of
    the algorithm using intrinsic measures. We have already discussed measuring the
    distortions of the clusters. In this section, we will discuss another performance
    measure for clustering called the **silhouette coefficient**. The silhouette coefficient
    is a measure of the compactness and separation of the clusters. It increases as
    the quality of the clusters increase; it is large for compact clusters that are
    far from each other and small for large, overlapping clusters. The silhouette
    coefficient is calculated per instance; for a set of instances, it is calculated
    as the mean of the individual samples'' scores. The silhouette coefficient for
    an instance is calculated with the following equation:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将机器学习定义为设计和研究从经验中学习以提高任务执行性能的系统，任务的性能通过给定的度量进行衡量。K-Means是一种无监督学习算法；没有标签或基准真值来与聚类进行比较。然而，我们仍然可以使用内在度量来评估算法的性能。我们已经讨论了测量聚类失真度。在本节中，我们将讨论另一种聚类性能度量，称为**轮廓系数**。轮廓系数是衡量聚类紧凑性和分离度的指标。随着聚类质量的提高，它会增加；对于远离彼此的紧凑聚类，轮廓系数较大；对于大的重叠聚类，轮廓系数较小。轮廓系数是针对每个实例计算的；对于一组实例，它是个体样本分数的平均值。实例的轮廓系数通过以下公式计算：
- en: '![Evaluating clusters](img/8365OS_06_15.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![评估聚类](img/8365OS_06_15.jpg)'
- en: '*a* is the mean distance between the instances in the cluster. *b* is the mean
    distance between the instance and the instances in the next closest cluster. The
    following example runs K-Means four times to create two, three, four, and eight
    clusters from a toy dataset and calculates the silhouette coefficient for each
    run:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*是聚类中实例之间的平均距离。*b*是实例与下一个最接近的聚类中实例之间的平均距离。以下示例运行四次K-Means算法，分别创建两个、三个、四个和八个聚类，并计算每次运行的轮廓系数：'
- en: '[PRE1]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This script produces the following figure:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本生成了以下图形：
- en: '![Evaluating clusters](img/8365OS_06_16.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![评估聚类](img/8365OS_06_16.jpg)'
- en: The dataset contains three obvious clusters. Accordingly, the silhouette coefficient
    is greatest when ![Evaluating clusters](img/8365OS_06_18.jpg) is equal to three.
    Setting ![Evaluating clusters](img/8365OS_06_18.jpg) equal to eight produces clusters
    of instances that are as close to each other as they are to the instances in some
    of the other clusters, and the silhouette coefficient of these clusters is smallest.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含三个明显的聚类。因此，当![评估聚类](img/8365OS_06_18.jpg)等于三时，轮廓系数最大。将![评估聚类](img/8365OS_06_18.jpg)设置为八时，产生的实例聚类相互之间的距离与它们与其他聚类的实例之间的距离相当，且这些聚类的轮廓系数最小。
- en: Image quantization
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像量化
- en: 'In the previous sections, we used clustering to explore the structure of a
    dataset. Now let''s apply it to a different problem. Image quantization is a lossy
    compression method that replaces a range of similar colors in an image with a
    single color. Quantization reduces the size of the image file since fewer bits
    are required to represent the colors. In the following example, we will use clustering
    to discover a compressed palette for an image that contains its most important
    colors. We will then rebuild the image using the compressed palette. This example
    requires the `mahotas` image processing library, which can be installed using
    `pip install mahotas`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们使用聚类来探索数据集的结构。现在让我们将其应用于一个不同的问题。图像量化是一种有损压缩方法，它用单一颜色替代图像中一系列相似的颜色。量化减少了图像文件的大小，因为表示颜色所需的位数较少。在以下示例中，我们将使用聚类来发现图像的压缩调色板，其中包含其最重要的颜色。然后，我们将使用压缩调色板重建图像。此示例需要`mahotas`图像处理库，可以通过`pip
    install mahotas`安装：
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'First we read and flatten the image:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们读取并展平图像：
- en: '[PRE3]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We then use K-Means to create 64 clusters from a sample of 1,000 randomly selected
    colors. Each of the clusters will be a color in the compressed palette. The code
    is as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用 K-Means 从 1,000 个随机选择的颜色样本中创建 64 个聚类。每个聚类将是压缩调色板中的一种颜色。代码如下：
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we predict the cluster assignment for each of the pixels in the original
    image:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们预测原始图像中每个像素的聚类分配：
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we create the compressed image from the compressed palette and cluster
    assignments:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们从压缩调色板和聚类分配中创建压缩图像：
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The original and compressed versions of the image are show in the following
    figure:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的原始版本和压缩版本如下图所示：
- en: '![Image quantization](img/8365OS_06_17.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图像量化](img/8365OS_06_17.jpg)'
- en: Clustering to learn features
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类学习特征
- en: In this example, we will combine clustering with classification in a semi-supervised
    learning problem. You will learn features by clustering unlabeled data and use
    the learned features to build a supervised classifier.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将在半监督学习问题中将聚类与分类结合起来。你将通过对无标签数据进行聚类来学习特征，并使用学习到的特征构建监督分类器。
- en: Suppose you own a cat and a dog. Suppose that you have purchased a smartphone,
    ostensibly to use to communicate with humans, but in practice just to use to photograph
    your cat and dog. Your photographs are awesome and you are certain that your friends
    and co-workers would love to review all of them in detail. You'd like to be courteous
    and respect that some people will only want to see your cat photos, while others
    will only want to see your dog photos, but separating the photos is laborious.
    Let's build a semi-supervised learning system that can classify images of cats
    and dogs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你拥有一只猫和一只狗。假设你购买了一部智能手机，表面上是为了与人类交流，但实际上只是为了拍摄你的猫和狗。你的照片非常棒，你确信你的朋友和同事们会喜欢详细查看这些照片。你希望能够体贴一些，尊重有些人只想看到猫的照片，而有些人只想看到狗的照片，但将这些照片分开是费力的。让我们构建一个半监督学习系统，可以分类猫和狗的图像。
- en: 'Recall from [Chapter 3](ch03.html "Chapter 3. Feature Extraction and Preprocessing"),
    *Feature Extraction and Preprocessing*, that a naïve approach to classifying images
    is to use the intensities, or brightnesses, of all of the pixels as explanatory
    variables. This approach produces high-dimensional feature vectors for even small
    images. Unlike the high-dimensional feature vectors we used to represent documents,
    these vectors are not sparse. Furthermore, it is obvious that this approach is
    sensitive to the image''s illumination, scale, and orientation. In [Chapter 3](ch03.html
    "Chapter 3. Feature Extraction and Preprocessing"), *Feature Extraction and Preprocessing*,
    we also discussed SIFT and SURF descriptors, which describe interesting regions
    of an image in ways that are invariant to scale, rotation, and illumination. In
    this example, we will cluster the descriptors extracted from all of the images
    to learn features. We will then represent an image with a vector with one element
    for each cluster. Each element will encode the number of descriptors extracted
    from the image that were assigned to the cluster. This approach is sometimes called
    the **bag-of-features** representation, as the collection of clusters is analogous
    to the bag-of-words representation''s vocabulary. We will use 1,000 images of
    cats and 1,000 images of dogs from the training set for Kaggle''s *Dogs vs. Cats*
    competition. The dataset can be downloaded from [https://www.kaggle.com/c/dogs-vs-cats/data](https://www.kaggle.com/c/dogs-vs-cats/data).
    We will label cats as the positive class and dogs as the negative class. Note
    that the images have different dimensions; since our feature vectors do not represent
    pixels, we do not need to resize the images to have the same dimensions. We will
    train using the first 60 percent of the images, and test on the remaining 40 percent:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下从[第三章](ch03.html "第三章。特征提取和预处理")，*特征提取和预处理*，我们可以看到，对图像进行分类的一个天真的方法是使用所有像素的强度或亮度作为解释变量。即使是小图像，这种方法也会产生高维特征向量。与我们用来表示文档的高维特征向量不同，这些向量并不是稀疏的。此外，显而易见的是，这种方法对图像的光照、尺度和方向非常敏感。在[第三章](ch03.html
    "第三章。特征提取和预处理")中，*特征提取和预处理*，我们还讨论了 SIFT 和 SURF 描述符，它们以一种对尺度、旋转和光照不变的方式描述图像的有趣区域。在这个例子中，我们将聚类从所有图像中提取的描述符，以学习特征。然后，我们将用一个向量表示图像，向量的每个元素代表一个聚类。每个元素将编码从分配给聚类的图像中提取的描述符的数量。这种方法有时被称为**特征包表示法**，因为聚类的集合类似于词袋表示法的词汇表。我们将使用
    Kaggle 的 *Dogs vs. Cats* 竞赛训练集中的 1,000 张猫和 1,000 张狗的图像。数据集可以从 [https://www.kaggle.com/c/dogs-vs-cats/data](https://www.kaggle.com/c/dogs-vs-cats/data)
    下载。我们将猫标记为正类，狗标记为负类。请注意，这些图像有不同的尺寸；由于我们的特征向量不表示像素，因此我们不需要调整图像的尺寸使其具有相同的尺寸。我们将使用图像的前
    60% 进行训练，然后在剩余的 40% 上进行测试：
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'First, we load the images, convert them to grayscale, and extract the SURF
    descriptors. SURF descriptors can be extracted more quickly than many similar
    features, but extracting descriptors from 2,000 images is still computationally
    expensive. Unlike the previous examples, this script requires several minutes
    to execute on most computers:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载图像，将它们转换为灰度图像，并提取 SURF 描述符。与许多类似特征相比，SURF 描述符可以更快地提取，但从 2,000 张图像中提取描述符仍然是计算密集型的。与之前的例子不同，这个脚本在大多数计算机上执行需要几分钟时间。
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We then group the extracted descriptors into 300 clusters in the following
    code sample. We use `MiniBatchKMeans`, a variation of K-Means that uses a random
    sample of the instances in each iteration. As it computes the distances to the
    centroids for only a sample of the instances in each iteration, `MiniBatchKMeans`
    converges more quickly but its clusters'' distortions may be greater. In practice,
    the results are similar, and this compromise is acceptable.:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在下面的代码示例中将提取的描述符分组到 300 个聚类中。我们使用 `MiniBatchKMeans`，这是 K-Means 的一种变体，每次迭代使用实例的随机样本。由于它仅计算每次迭代中一些实例到质心的距离，`MiniBatchKMeans`
    收敛更快，但其聚类的失真可能更大。实际上，结果是相似的，这种折衷是可以接受的。
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we construct feature vectors for the training and testing data. We find
    the cluster associated with each of the extracted SURF descriptors, and count
    them using NumPy''s `binCount()` function. The following code produces a 300-dimensional
    feature vector for each instance:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为训练和测试数据构建特征向量。我们找到与每个提取的 SURF 描述符相关联的聚类，并使用 NumPy 的 `binCount()` 函数对它们进行计数。以下代码为每个实例生成一个
    300 维的特征向量：
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, we train a logistic regression classifier on the feature vectors and
    targets, and assess its precision, recall, and accuracy:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在特征向量和目标上训练了一个逻辑回归分类器，并评估了其精度、召回率和准确率：
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This semi-supervised system has better precision and recall than a logistic
    regression classifier that uses only the pixel intensities as features. Furthermore,
    our feature representations have only 300 dimensions; even small 100 x 100 pixel
    images would have 10,000 dimensions.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这个半监督系统比仅使用像素强度作为特征的逻辑回归分类器具有更好的精度和召回率。此外，我们的特征表示仅有300维；即使是小的100 x 100像素图像也会有10,000维。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we discussed our first unsupervised learning task: clustering.
    Clustering is used to discover structure in unlabeled data. You learned about
    the K-Means clustering algorithm, which iteratively assigns instances to clusters
    and refines the positions of the cluster centroids. While K-Means learns from
    experience without supervision, its performance is still measurable; you learned
    to use distortion and the silhouette coefficient to evaluate clusters. We applied
    K-Means to two different problems. First, we used K-Means for image quantization,
    a compression technique that represents a range of colors with a single color.
    We also used K-Means to learn features in a semi-supervised image classification
    problem.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们讨论了第一个无监督学习任务：聚类。聚类用于发现无标签数据中的结构。你学习了K均值聚类算法，该算法通过迭代地将实例分配到聚类中，并细化聚类中心的位置。虽然K均值是通过经验进行学习的，而非监督学习，但其性能依然可衡量；你学会了使用失真度和轮廓系数来评估聚类。我们将K均值应用于两个不同的问题。首先，我们使用K均值进行图像量化，这是一种通过单一颜色表示一系列颜色的压缩技术。我们还将K均值用于半监督图像分类问题中的特征学习。
- en: In the next chapter, we will discuss another unsupervised learning task called
    dimensionality reduction. Like the semi-supervised feature representations we
    created to classify images of cats and dogs, dimensionality reduction can be used
    to reduce the dimensions of a set of explanatory variables while retaining as
    much information as possible.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论另一种无监督学习任务——降维。就像我们为猫狗图像分类所创建的半监督特征表示一样，降维可以用来减少一组解释变量的维度，同时尽可能保留更多信息。
