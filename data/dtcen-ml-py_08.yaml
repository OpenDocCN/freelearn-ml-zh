- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Techniques for Identifying and Removing Bias
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别和消除偏差的技术
- en: 'In the realm of data-centric machine learning, the pursuit of unbiased and
    fair models is paramount. The consequences of biased algorithms can range from
    poor performance to ethically questionable decisions. It is important to recognize
    that bias can manifest at two key stages of the machine learning pipeline: data
    and model. While model-centric approaches have garnered significant attention
    in recent years, this chapter sheds light on the equally crucial data-centric
    strategies that are often overlooked.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在以数据为中心的机器学习领域，追求无偏差和公平的模型至关重要。偏差算法的后果可能从性能不佳到道德上有疑问的决定。重要的是要认识到，偏差可以在机器学习管道的两个关键阶段显现：数据和模型。虽然以模型为中心的方法近年来受到了广泛关注，但本章将重点介绍通常被忽视的同样重要的以数据为中心的策略。
- en: In this chapter, we will explore the intricacies of bias in machine learning,
    emphasizing why data-centricity is a fundamental aspect of bias mitigation. We
    will explore real-world examples from finance, human resources, and healthcare,
    where the failure to address bias has had or could have far-reaching implications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨机器学习中偏差的复杂性，强调数据中心性是偏差缓解的基本方面。我们将探讨来自金融、人力资源和医疗保健等领域的真实世界案例，其中未能解决偏差已经或可能产生深远的影响。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: The bias conundrum
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差难题
- en: Types of bias
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差类型
- en: The data-centric imperative
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中心性必要性
- en: Case study
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 案例研究
- en: The bias conundrum
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差难题
- en: Bias in machine learning is not a novel concern. It is deeply rooted in the
    data we collect and the algorithms we design. Bias can arise from historical disparities,
    societal prejudices, and even the human decisions made during data collection
    and annotation. Ignoring bias, or addressing it solely through model-centric techniques,
    can lead to detrimental outcomes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的偏差并非一个新问题。它深深植根于我们收集的数据和我们设计的算法中。偏差可能源于历史差异、社会偏见，甚至在数据收集和标注过程中人类所做的决策。忽视偏差，或者仅仅通过以模型为中心的技术来处理偏差，可能会导致不良后果。
- en: 'Consider the following scenarios, which illustrate the multifaceted nature
    of bias:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下场景，这些场景说明了偏差的多面性：
- en: '**Bias in finance**: In the financial sector, machine learning models play
    a pivotal role in credit scoring, fraud detection, and investment recommendations.
    However, if historical lending practices favor certain demographic groups over
    others, these biases can seep into the data used to train models. As a result,
    marginalized communities may face unfair lending practices, perpetuating socioeconomic
    inequalities.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金融中的偏差**：在金融领域，机器学习模型在信用评分、欺诈检测和投资建议中发挥着关键作用。然而，如果历史贷款实践偏向某些人口群体而忽视其他群体，这些偏差可能会渗透到用于训练模型的数据库中。结果，边缘化社区可能会面临不公平的贷款实践，加剧社会经济不平等。'
- en: '**Bias in human resources**: The use of AI in human resources has gained momentum
    for recruitment, employee performance assessment, and even salary negotiations.
    If job postings or historical hiring data are biased toward specific genders,
    ethnicities, or backgrounds, the AI systems can inadvertently perpetuate discrimination,
    leading to a lack of diversity and inclusion in the workplace.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人力资源中的偏差**：人工智能在人力资源中的应用在招聘、员工绩效评估甚至薪资谈判方面势头强劲。如果职位发布或历史招聘数据偏向特定性别、种族或背景，AI系统可能会无意中延续歧视，导致工作场所缺乏多样性和包容性。'
- en: '**Bias in healthcare**: In healthcare, diagnostic algorithms are relied upon
    for disease detection and treatment recommendations. If training data predominantly
    represents certain demographics, individuals from underrepresented groups may
    receive suboptimal care or face delayed diagnoses. The implications can be life-altering,
    underscoring the need for equitable healthcare AI.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医疗保健中的偏差**：在医疗保健领域，诊断算法被用于疾病检测和治疗建议。如果训练数据主要代表某些人口群体，来自代表性不足群体的个人可能会接受不理想的护理或面临延迟诊断。其影响可能是改变一生的，强调了公平医疗保健AI的必要性。'
- en: Now that we have covered areas where bias can arise, in the next section, we
    will cover the types of bias prevalent in machine learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了可能产生偏差的领域，在下一节中，我们将探讨机器学习中普遍存在的偏差类型。
- en: Types of bias
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差类型
- en: In machine learning, there are generally five categories of bias that warrant
    attention. Although the list provided isn't exhaustive, these categories represent
    the most prevalent types of bias, each of which can be further subdivided.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，通常有五种需要关注的偏差类别。尽管提供的列表并不全面，但这些类别代表了最普遍的偏差类型，每种类型都可以进一步细分。
- en: Easy to identify bias
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容易识别的偏差
- en: Some types of bias can be easy to identify using active monitoring and by conducting
    analysis. These include the following.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一些类型的偏差可以通过主动监控和分析轻松识别。以下是一些例子。
- en: Reporting bias
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 报告偏差
- en: This type of bias occurs when the data producers, data annotators, or data capturers
    miss out on important elements, which results in data not being representative
    of the real world. For instance, a healthcare business might be interested in
    patients’ sentiments toward a health program; however, the data annotators may
    decide to focus on negative and positive sentiments, and sentiments that were
    neutral may be underrepresented. A model trained on such data will be good at
    identifying positive and negative sentiments but may fail to accurately predict
    neutral sentiments. This type of bias can be identified with active monitoring,
    where predictions on live data show drift from predictions on training data. To
    reduce reporting bias, it is important to articulate data points needed for the
    problem at the beginning phase of ML system design. It is also important to ensure
    that data used for training represents real-life data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种偏差发生在数据生产者、数据标注者或数据收集者遗漏了重要元素时，导致数据不能代表现实世界。例如，一家医疗保健企业可能对病人对健康计划的看法感兴趣；然而，数据标注者可能会决定专注于负面和正面情绪，而中性的情绪可能被不足代表。在这样数据上训练的模型擅长识别正面和负面情绪，但可能无法准确预测中性情绪。这种偏差可以通过主动监控来识别，其中对实时数据的预测与对训练数据的预测存在偏差。为了减少报告偏差，在机器学习系统设计初期明确所需的数据点非常重要。同样重要的是确保用于训练的数据代表真实世界数据。
- en: Automation bias
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动化偏差
- en: This type of bias occurs due to relying on automated ways of data collection
    and assuming data capture is not error-prone. As AI is becoming better, reliance
    on humans has significantly reduced, and hence it is often assumed that if an
    automated system is put in place, then it will magically solve all problems. Using
    active monitoring can help identify this type of bias, where the model accuracy
    is highly poor on real-life data. Another way of identifying this is by using
    humans to annotate labels and measure human performance versus algorithmic performance.
    As covered in [*Chapter 6*](B19297_06.xhtml#_idTextAnchor089)*, Techniques for
    Programmatic Labeling in Machine Learning* systems fail and can lead to missing
    data or inaccurate data. AI is as good as the data it was trained on. One of the
    key principles of data-centricity is to keep humans in the loop; hence, when building
    automated systems, we should ensure the data generated represents real-world scenarios
    and data is diverse rather than overrepresented or underrepresented.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这种偏差发生在依赖于自动化的数据收集方式，并假设数据捕获不会出错的情况下。随着人工智能的日益完善，对人类的依赖显著减少，因此通常假设如果实施自动化系统，那么它将神奇地解决所有问题。使用主动监控可以帮助识别这种类型的偏差，其中模型在真实数据上的准确性非常低。另一种识别方法是通过让人类标注标签并衡量人类性能与算法性能。如[*第6章*](B19297_06.xhtml#_idTextAnchor089)*，机器学习系统中程序化标注技术的失败可能导致数据丢失或不准确。人工智能的好坏取决于其训练数据。数据中心化的一个关键原则是让人类参与其中；因此，在构建自动化系统时，我们应该确保生成数据代表真实世界场景，并且数据多样化，而不是过度或不足代表。
- en: Selection bias
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择偏差
- en: 'This type of bias occurs when data selected for training the model is not representative
    of real-life data. This bias can take multiple forms:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这种偏差发生在用于训练模型的所选数据不能代表真实世界数据时。这种偏差可以有多种形式：
- en: '**Coverage bias**: This bias can occur when data is not collected in a representative
    manner. This can happen when the business and practitioners are focused on outcomes,
    and ignore data points that do not contribute to the outcome. In healthcare, insurance
    companies may want to predict hospital admissions; however, data on people churning
    on insurance companies and using competitive insurance products, or data on people
    not claiming benefits to go into the hospital may not be readily available and,
    as a result, these groups of people may not be represented well in the training
    data.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**覆盖偏差**：这种偏差可能发生在数据不是以代表性的方式收集时。这可能发生在企业和从业者专注于结果，而忽略了那些对结果没有贡献的数据点。在医疗保健领域，保险公司可能想要预测医院的入院人数；然而，关于那些在保险公司频繁更换并使用竞争性保险产品的人，或者那些没有申请福利而进入医院的人的数据可能并不容易获得，因此，这些人群在训练数据中可能没有得到很好的代表。'
- en: '**Participation bias**: This bias can occur due to participants opting out
    of data collection processes, leading to one group being overrepresented over
    another group. For example, a model is trained to predict churn using survey data,
    where 80% of people who have moved to a new competitor are unlikely to respond
    to the survey, and their data is highly underrepresented in the sample.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参与偏差**：这种偏差可能由于参与者选择退出数据收集过程，导致某一群体在另一群体中过度代表。例如，一个模型被训练用来根据调查数据预测流失率，其中80%已经转移到新竞争对手的人不太可能回应调查，他们的数据在样本中高度代表性不足。'
- en: '**Sampling bias**: This bias can occur when data collectors do not use proper
    randomization methods in data collection processes. For example, a model is trained
    to predict health scores based on survey data; instead of targeting the population
    at random, the surveyors chose 80% of people who are highly engaged with their
    health and are more likely to respond, compared to the rest of the responders.
    In the health industry, people who are more engaged with their health are likely
    to have a better health score than people who are less engaged, thus leading to
    a biased model toward healthy people.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抽样偏差**：这种偏差可能发生在数据收集者没有在数据收集过程中使用适当的随机化方法时。例如，一个模型被训练用来根据调查数据预测健康分数；调查者没有随机地针对人口，而是选择了80%高度关注健康并且更有可能回应的人，与那些不太可能回应的其他受访者相比。在健康产业中，那些更关注健康的人可能比那些不太关注健康的人有更好的健康分数，这可能导致模型偏向于健康人群。'
- en: Selection biases are difficult to identify; however, if drift is noted frequently
    in the data and highly frequent retraining is done to ensure the model quality
    does not degrade, then it will be a good time to investigate and check whether
    the data captured represents real-life data. Two types of analysis in regression
    modeling can help to identify this bias. One is conducting bivariate analysis,
    where a sensitive variable can be represented on the *x* axis and the target variable
    can be put on the *y* axis. If there is a strong association between the two variables,
    then it is important to evaluate the difference in the association metric at training
    time and post-scoring time. If the difference is significant, it is quite possible
    that the data used for training is not representative of real life. The second
    technique is to use multivariate analysis by comparing the possible outcomes when
    data is not fully represented and when data is fully represented. This can be
    done by separating the subgroups into data points that were included and the ones
    that were excluded at training time. We can run a multi-regression model by creating
    an independent variable group by labeling group 1 for data included and group
    2 for data not included. We will add this new variable as a feature to the model
    training and then compare whether there is a significant difference in outcome
    between groups 1 and 2\. If there is a difference, then the data collection was
    biased.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 选择偏差难以识别；然而，如果数据中频繁出现漂移，并且频繁重新训练以确保模型质量不下降，那么就是调查和检查所捕获的数据是否代表真实世界数据的好时机。回归模型中的两种分析方法可以帮助识别这种偏差。一种是进行双变量分析，其中敏感变量可以表示在*x*轴上，目标变量可以放在*y*轴上。如果两个变量之间存在强烈的关联，那么在训练时间和评分后时间评估关联指标差异是很重要的。如果差异显著，那么用于训练的数据很可能不代表真实生活。第二种技术是通过比较数据未完全代表和完全代表时的可能结果来进行多元分析。这可以通过将子群体分为训练时包含的数据点和排除的数据点来完成。我们可以通过创建一个独立变量组来运行多回归模型，将组1标记为包含的数据，组2标记为未包含的数据。然后我们将这个新变量作为特征添加到模型训练中，并比较组1和组2之间是否存在显著的差异。如果存在差异，那么数据收集存在偏差。
- en: In classification examples, we can use false positive rates and/or false negative
    rates across sensitive subgroups to see whether these are vastly different. If
    they are, data is likely to be biased toward one or a couple of subgroups. Another
    metric that can be used to check whether bias persists is demographic parity,
    which is a probability comparison of the likelihood of selection from one subgroup
    over another. If the ratio of probabilities between the higher selection subgroup
    and the lower selection subgroup is below 0.8, it is quite likely data is biased
    and does not have enough representative samples. It is recommended to check multiple
    metrics to understand bias in the data and the algorithm.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类示例中，我们可以通过查看敏感子群体中的假阳性率和/或假阴性率来观察这些值是否差异很大。如果差异很大，数据很可能偏向于一个或几个子群体。另一个可以用来检查偏差是否持续存在的指标是人口统计学上的平等性，它是对从一个子群体到另一个子群体选择可能性的概率比较。如果高选择子群体与低选择子群体之间的概率比率低于0.8，那么数据很可能存在偏差，并且代表性样本不足。建议检查多个指标以了解数据及算法中的偏差。
- en: To treat such biases, it is recommended, when collecting data, to use techniques
    such as stratified sampling to ensure that different groups are represented proportionally
    in the dataset. Now that we have covered types of bias that are easy to identify,
    in the next section, we will cover some types of bias that are difficult to identify.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这类偏差，建议在收集数据时采用分层抽样等技巧，以确保数据集中不同群体按比例代表。现在我们已经介绍了易于识别的偏差类型，在下一节中，我们将讨论一些难以识别的偏差类型。
- en: Difficult to identify bias
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 难以识别的偏差
- en: Some types of bias can be challenging because they are biases that individuals
    may not be consciously aware of. These biases often operate at a subconscious
    level and can influence perceptions, attitudes, and behaviors. In order to capture
    these, organizations and individuals need processes and training to ensure these
    biases are not present in the workspace. Once it has been identified that there
    was bias in the data collection process or data labeling process, then sensitive
    labels can be defined to measure and check whether the model is free from bias
    or whether there is an acceptable level of bias in the model. Some of these biases
    are described next.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一些类型的偏见可能具有挑战性，因为它们是个人可能没有意识到的偏见。这些偏见通常在潜意识层面运作，并可能影响感知、态度和行为。为了捕捉这些偏见，组织和个人需要流程和培训来确保这些偏见不会存在于工作场所。一旦确定数据收集过程或数据标注过程中存在偏见，就可以定义敏感标签来衡量和检查模型是否没有偏见，或者模型中是否存在可接受的偏见水平。以下将描述一些这些偏见。
- en: Group attribution bias
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 组别归因偏见
- en: 'This type of bias occurs when attribution is done for the entire data based
    on some data points. This usually occurs when the data creators have preconceived
    biases about the types of attributes present in the data. This type of bias can
    take two forms:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的偏见发生在对整个数据进行归因时基于某些数据点。这通常发生在数据创建者对数据中存在的属性类型有先入为主的偏见时。这种偏见可以采取两种形式：
- en: '**In-group bias**: This is a preconceived bias where associated data points
    resonate with the data creator, hence those data points get a favorable outcome
    – for example, if a data engineering manager is designing a resume selection algorithm
    where they believe someone doing a Udacity nanodegree is qualified for the role.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**群体偏见**: 这是一种先入为主的偏见，其中相关数据点与数据创建者产生共鸣，因此这些数据点获得有利的结果——例如，如果一位数据工程经理在设计简历筛选算法时，他们认为完成Udacity纳米学位的人符合该职位要求。'
- en: '**Out-group homogeneity bias**: This is a preconceived bias where data points
    do not resonate with the data creator, hence those data points get a negative
    outcome – for example, if a data engineering manager is designing a resume selection
    algorithm where they believe someone not doing a Udacity nanodegree is not qualified
    for the role.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**群体同质性偏见**: 这是一种先入为主的偏见，其中数据点与数据创建者不产生共鸣，因此这些数据点获得不利的结果——例如，如果一位数据工程经理在设计简历筛选算法时，他们认为没有完成Udacity纳米学位的人不符合该职位要求。'
- en: Let’s move on to another type of bias that is difficult to identify.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续讨论另一种难以识别的偏见。
- en: Implicit bias
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐性偏见
- en: This type of bias occurs when data creators make assumptions about the data
    based on their own mental models and personal experiences. For example, a sentiment
    analysis model trained on airline food service review data is likely to associate
    the word “okay” with neutral sentiment. However, some regions of the world use
    the word “okay” to signify a positive sentiment.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的偏见发生在数据创建者根据自己的心理模型和个人经验对数据进行假设时。例如，在航空食品服务评论数据上训练的情感分析模型可能会将“ okay”这个词与中性情感联系起来。然而，世界上的一些地区使用“
    okay”这个词来表示积极情感。
- en: 'Bias in machine learning can take many forms; hence, we categorize these biases
    into two main types, **easy to identify** biases and **difficult to identify**
    biases. Practitioners are known to take a model-centric approach to treat these
    biases, where modifying the algorithm or using bias-friendly algorithms has been
    considered acceptable practice. In the next section, we will take an alternative
    view to the model-centric approach: the data-centric approach.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的偏见可以采取多种形式；因此，我们将这些偏见分为两大类，**易于识别**的偏见和**难以识别**的偏见。从业者通常采用以模型为中心的方法来处理这些偏见，其中修改算法或使用偏见友好型算法已被视为可接受的做法。在下一节中，我们将从以模型为中心的方法转向另一种观点：以数据为中心的方法。
- en: The data-centric imperative
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据中心化命令
- en: Addressing bias in machine learning necessitates a holistic approach, with data-centric
    strategies complementing model-centric techniques. Data-centricity involves taking
    proactive steps to curate, clean, and enhance the dataset itself, thus minimizing
    the bias that models can inherit. By embracing data-centric practices, organizations
    can foster fairness, accountability, and ethical AI.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 解决机器学习中的偏见需要一种全面的方法，其中以数据为中心的策略补充以模型为中心的技术。数据中心化涉及采取主动措施来编辑、清理和增强数据集本身，从而最大限度地减少模型可能继承的偏见。通过采用数据中心化实践，组织可以培养公平性、责任感和道德人工智能。
- en: In the remainder of this chapter, we will explore a spectrum of data-centric
    strategies that empower machine learning practitioners to reduce bias. These include
    data resampling, augmentation, cleansing, feature selection, and more. Real-world
    examples will illustrate the tangible impact of these strategies in the domains
    of finance, human resources, and healthcare.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将探讨一系列以数据为中心的策略，这些策略赋予机器学习从业者减少偏差的能力。这些包括数据重采样、增强、净化、特征选择等。现实世界的例子将说明这些策略在金融、人力资源和医疗保健领域的实际影响。
- en: If data is fairly and accurately captured or created, then it is quite likely
    algorithms will be mostly free from bias. However, the techniques we will cover
    in this chapter are post-data creation, where ML practitioners have to work with
    provided data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据被公平且准确地捕获或创建，那么算法很可能大部分都是无偏的。然而，本章中我们将涵盖的技术是在数据创建之后，机器学习从业者必须与提供的数据一起工作。
- en: In the following subsections, we will discuss some data-centric strategies for
    reducing bias in machine learning without changing the algorithm. These can be
    referred to as data debiasing techniques.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下子节中，我们将讨论一些数据为中心的策略，以在不改变算法的情况下减少机器学习中的偏差。这些可以被称为数据去偏技术。
- en: Sampling methods
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 样本方法
- en: Sampling methods such as undersampling and oversampling address class imbalances.
    Undersampling reduces majority class instances, whereas oversampling augments
    minority class examples. Integrating both mitigates overfitting and information
    loss, balancing class representation effectively. These methods can be combined
    with outlier treatment and Shapley values to further sample the data where harder-to-classify
    or harder-to-estimate data points can be removed or introduced to enhance the
    fairness metrics. These techniques are covered next.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 下采样和过采样等样本方法解决类别不平衡问题。下采样减少多数类实例，而过采样增强少数类示例。将两者结合起来可以减轻过拟合和信息损失，有效地平衡类别表示。这些方法可以与异常值处理和Shapley值结合使用，以进一步采样数据，其中难以分类或难以估计的数据点可以被移除或引入，以增强公平性指标。这些技术将在下一部分进行介绍。
- en: Undersampling
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下采样
- en: In undersampling, we remove random or strategic subsets of overrepresented data
    points to balance class distributions – deleting data points from overrepresented
    classes where examples are difficult to classify or at random is a commonly used
    technique. We can also use outlier removal for regression tasks.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在下采样中，我们移除随机或策略性的多数代表数据点的子集，以平衡类别分布——从难以分类或随机删除的多数类中删除数据点是常用的技术。我们还可以使用异常值移除进行回归任务。
- en: Oversampling
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过采样
- en: In oversampling, we add random or strategic subsets of underrepresented data
    points to provide more examples to the algorithm. We can duplicate or generate
    synthetic data points for underrepresented classes to balance class distributions.
    We can use techniques such as the **Synthetic Minority Oversampling Technique**
    (**SMOTE**) and random oversampling for classification tasks. Alternatively, we
    can utilize outlier or edge case addition/removal for regression tasks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在过采样中，我们添加随机或策略性的少数代表数据点的子集，为算法提供更多示例。我们可以为少数类重复或生成合成数据点，以平衡类别分布。我们可以使用**SMOTE（合成少数过采样技术**）和随机过采样等技术进行分类任务。或者，我们可以利用异常值或边缘案例的添加/删除进行回归任务。
- en: Combination of undersampling and oversampling
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下采样和过采样的组合
- en: These cover techniques such as `SMOTEENN` or `SMOTETomek`, where `SMOTE` is
    utilized to oversample the minority class. Techniques such as **Edited Nearest
    Neighbors** (**ENN**) or Tomek Links are used to remove the examples that are
    difficult to classify or agree on using nearest neighbors, as these points are
    close to the boundary and there is no clear separation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包括如`SMOTEENN`或`SMOTETomek`等技术，其中`SMOTE`用于过采样少数类。**编辑最近邻（ENN**）或Tomek链接等技术用于移除难以分类或难以使用最近邻达成一致意见的示例，因为这些点接近边界，没有明显的分离。
- en: Anomaly detection for oversampling and undersampling the data
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对数据进行过采样和下采样的异常检测
- en: This covers using an anomaly detection technique to identify data points that
    are edge cases, and then these points can be reintroduced multiple times or removed
    so the model can get a better signal or become more generalized.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括使用异常检测技术来识别边缘情况的数据点，然后这些点可以被多次重新引入或移除，以便模型能够获得更好的信号或变得更加通用。
- en: Use of Shapley values for oversampling and undersampling data
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Shapley值进行过采样和下采样数据
- en: This covers using Shapley values to oversample or undersample data. Shapley
    values quantify feature importance by assessing each feature’s contribution to
    a model’s prediction. High Shapley values highlight influential features. Removing
    instances with high Shapley values but wrong predictions might enhance model accuracy
    by reducing outliers. Oversampling instances with high Shapley values and correct
    predictions can reinforce the model’s understanding of crucial patterns, potentially
    improving performance.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了使用Shapley值进行数据过采样或欠采样。Shapley值通过评估每个特征对模型预测的贡献来量化特征的重要性。高Shapley值突出了有影响力的特征。移除具有高Shapley值但预测错误的实例可能会通过减少异常值来提高模型精度。对具有高Shapley值和正确预测的实例进行过采样可以加强模型对关键模式的理解，从而可能提高性能。
- en: Other data-centric techniques
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他以数据为中心的技术
- en: Besides sampling methods, there are other data-centric techniques that can be
    used to reduce bias, some of which have been covered in previous chapters, and
    some we will utilize in the case study. The three main ones are described next.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 除了采样方法之外，还有其他以数据为中心的技术可以用来减少偏差，其中一些在之前的章节中已经介绍过，还有一些我们将在案例研究中使用。以下描述了三种主要方法。
- en: Data cleansing
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据清洗
- en: This includes removing missing data, where the inclusion of missing data can
    lead to unfair outcomes. These techniques were covered in [*Chapter 5*](B19297_05.xhtml#_idTextAnchor070),
    *Techniques for Data Cleaning*, where missing data was classified as “missing
    not at random.”
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '这包括移除缺失数据，因为包含缺失数据可能导致不公平的结果。这些技术已在[*第5章*](B19297_05.xhtml#_idTextAnchor070)，*数据清洗技术*中介绍，其中缺失数据被归类为“非随机缺失”。 '
- en: Feature selection
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征选择
- en: This includes selecting specific features or eliminating features that will
    reduce bias. This may mean identifying a variable that is highly associated with
    a sensitive variable and outcome label, and removing such indirect variables or
    removing sensitive variables.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括选择特定的特征或消除会减少偏差的特征。这可能意味着识别出一个与敏感变量和结果标签高度相关的变量，并移除这样的间接变量或移除敏感变量。
- en: Feature engineering
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征工程
- en: Feature engineering offers potent tools to mitigate model bias. Techniques such
    as re-encoding sensitive attributes, creating interaction terms, or introducing
    proxy variables enable models to learn without direct access to sensitive information.
    Feature selection and dimensionality reduction methods trim irrelevant or redundant
    features, fostering fairer and more robust models. Additionally, generating synthetic
    features or utilizing domain-specific knowledge helps improve models with a better
    understanding of data, aiding in fairer decision-making while improving overall
    model performance and reducing bias. We will create a synthetic variable, “Interest,”
    in the example to show how the model is biased toward one subgroup over another.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程提供了减轻模型偏差的有效工具。例如，重新编码敏感属性、创建交互项或引入代理变量等技术，使模型能够在没有直接访问敏感信息的情况下学习。特征选择和降维方法裁剪了无关或冗余的特征，促进了更公平和更健壮的模型。此外，生成合成特征或利用特定领域的知识有助于提高对数据有更好理解的模型，从而有助于更公平的决策，同时提高整体模型性能并减少偏差。在示例中，我们将创建一个合成变量“兴趣”，以展示模型相对于另一个子群体是如何偏向的。
- en: Now that we have covered data-centric methods, in the next section, we will
    describe the problem statement, and walk through examples of how we can identify
    and reduce bias in real life.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了以数据为中心的方法，在下一节中，我们将描述问题陈述，并举例说明我们如何在现实生活中识别和减少偏差。
- en: Case study
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究
- en: The challenge at hand centers on uncovering and addressing potential bias within
    a dataset pertaining to credit card defaults in Taiwan. Acquired from the UC Irvine
    Machine Learning Repository ([https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients](https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients)),
    this dataset comprises information from 30,000 credit card clients over a six-month
    span, including demographic factors such as gender, marital status, and education.
    The key concern is whether these demographic features introduce bias into a decision
    tree classifier trained on all available features, with a specific focus on gender-related
    bias. The overarching objective of this example is to not only identify but also
    mitigate any biased outcomes through the application of data-centric techniques.
    By reevaluating the algorithm’s performance using fairness metrics, the example
    aims to shed light on the real-world implications of bias in financial decision-making,
    particularly how these biases can impact individuals based on gender and other
    demographic factors, potentially leading to unequal treatment in credit assessments
    and financial opportunities. Addressing and rectifying such biases is crucial
    for promoting fairness and equity in financial systems.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当前面临的挑战集中在揭示和解决台湾信用卡违约数据集中可能存在的潜在偏差。该数据集来自加州大学欧文分校机器学习库（[https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients](https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients)），包含过去六个月内30,000名信用卡客户的详细信息，包括性别、婚姻状况和教育等人口统计因素。关键问题是这些人口统计特征是否将这些特征训练的决策树分类器引入偏差，特别是关注与性别相关的偏差。本例的总体目标是不仅识别偏差，而且通过应用数据为中心的技术来减轻任何偏差的结果。通过使用公平性指标重新评估算法的性能，本例旨在揭示金融决策中偏差的现实影响，特别是这些偏差如何基于性别和其他人口统计因素影响个人，可能导致信用评估和金融机会的不平等对待。解决和纠正此类偏差对于促进金融系统的公平性和平等至关重要。
- en: 'We will use two key metrics to check the fairness of the algorithm:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用两个关键指标来检查算法的公平性：
- en: '**Equalized odds difference**: This metric compares the false negative rate
    and false positive rate across the sensitive variables, then takes the maximum
    difference between the false negative rate and false positive rate. For instance,
    on the test set, the false positive rate among men and women is 0.3 and 0.2 (difference
    of 0.1), whereas the false negative rate among men and women is 0.15 and 0.12
    (difference of 0.03). Since the difference is larger on the false positive rate,
    the equalized odds will be 0.1.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均衡机会差异**: 该指标比较敏感变量（如性别、种族或年龄）的假阴性率和假阳性率，然后取假阴性率和假阳性率之间的最大差异。例如，在测试集中，男性和女性的假阳性率分别为0.3和0.2（差异为0.1），而男性和女性的假阴性率分别为0.15和0.12（差异为0.03）。由于假阳性率的差异更大，均衡机会将为0.1。'
- en: '**Demographic parity ratio**: This metric measures whether the predictions
    made by a model are independent of a sensitive variable, such as race, gender,
    or age. Given this is a ratio, it measures the ratio of a lower selection rate
    to that of a higher selection rate. A ratio of 1 means that demographic parity
    is achieved, whereas below 0.8 usually means that the algorithm is highly biased
    toward one group of individuals over the others.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人口统计平等比率**: 该指标衡量模型做出的预测是否独立于敏感变量，如种族、性别或年龄。鉴于这是一个比率，它衡量的是低选择率与高选择率之间的比率。比率为1表示实现了人口统计平等，而低于0.8通常意味着算法对某一群体的高度偏见，超过其他群体。'
- en: 'The following is a description of the features in the dataset:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对数据集中特征的描述：
- en: '`LIMIT_BAL`: Amount of the given credit in NT dollars, including both the individual
    consumer credit and their family (supplementary) credit.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LIMIT_BAL`: 给定信用的金额（新台币），包括个人消费者信用及其家庭（补充）信用。'
- en: '`Sex`: Gender (1 = male; 2 = female).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Sex`: 性别（1 = 男；2 = 女）。'
- en: '`Education X3`: Education (1 = graduate school; 2 = university; 3 = high school;
    4 = others)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Education X3`: 教育（1 = 研究生；2 = 大学；3 = 高中；4 = 其他）'
- en: '`Marriage X4`: Marital status (1 = married; 2 = single; 3 = others)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Marriage X4`: 婚姻状况（1 = 已婚；2 = 未婚；3 = 其他）'
- en: '`Age X5`: Age of the person in years'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Age X5`: 人的年龄（以年为单位）'
- en: 'PAY_0- PAY_5; X6 - X11: History of past payments, which includes the past monthly
    payment records (from April to September 2005), where PAY_0X6 = the repayment
    status in September, PAY_2; X7 = the repayment status in August 2005; ... PAY_6;
    X11 = the repayment status in April 2005\. The measurement scale for the repayment
    status is -1 = amount paid duly; 1 = payment delay for 1 month; 2 = payment delay
    for 2 months; ... ; 8 = payment delay for 8 months; 9 = payment delay for 9 months,
    and so on.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PAY_0- PAY_5; X6 - X11: 过去付款的历史记录，包括从 2005 年 4 月到 9 月的过去每月付款记录，其中 PAY_0X6 =
    9 月的还款状态，PAY_2; X7 = 2005 年 8 月的还款状态；... PAY_6; X11 = 2005 年 4 月的还款状态。还款状态的测量尺度为
    -1 = 按时支付金额；1 = 延迟 1 个月付款；2 = 延迟 2 个月付款；...；8 = 延迟 8 个月付款；9 = 延迟 9 个月，依此类推。'
- en: 'BILL_AMT1 . BILL_AMT6; X12-X17: Bill statement amount (in Taiwan dollars).
    BILL_AMT1;X12 means the amount on the credit card statement as of September 2005,
    while BILL_AMT6;X17 means the amount on the credit card statement as of April
    2005.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'BILL_AMT1 . BILL_AMT6; X12-X17: 账单金额（以新台币计）。BILL_AMT1;X12 表示 2005 年 9 月的信用卡账单金额，而
    BILL_AMT6;X17 表示 2005 年 4 月的信用卡账单金额。'
- en: 'PAY_AMT1-PAY_AMT6; X18-X23: Amount of payments made based on the previous month''s
    bill statement. PAY_AMT1;X18 means amount paid in September 2005, while PAY_AMT6;X23
    means amount paid on April 2005.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PAY_AMT1-PAY_AMT6; X18-X23: 根据上个月账单金额支付的金额。PAY_AMT1;X18 表示 2005 年 9 月支付的金额，而
    PAY_AMT6;X23 表示 2005 年 4 月支付的金额。'
- en: '`default payment next month`: Whether a person defaulted on the next month’s
    payment (Yes = 1, No = 0), in 2005'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`default payment next month`: 一个人是否在下个月的付款中违约（是 = 1，否 = 0），在 2005 年'
- en: 'To import the dataset, you need to install `pandas`. We will also use the `os`
    library to navigate the path and store the dataset. This library is native to
    Python. We will call the `loan_dataset.csv` file and save it in the same directory,
    from where we will run this example:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要导入数据集，您需要安装 `pandas`。我们还将使用 `os` 库来导航路径并存储数据集。此库是 Python 的原生库。我们将调用 `loan_dataset.csv`
    文件，并将其保存在运行此示例的同一目录中：
- en: '[PRE0]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The file takes a couple of seconds to a minute based on internet speed, so
    when we run this example for the first time, the file will be stored locally.
    However, on the subsequent runs, with the help of the `os` library, we will check
    that the file exists, else download it. We will rename two variables: `PAY_0`
    to `PAY_1`, and also `default payment next month` to `default`. We don’t believe
    the `ID` column will be useful for machine learning, hence we will drop it:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 文件加载所需时间取决于网络速度，从几秒到一分钟不等，因此当我们第一次运行此示例时，文件将存储在本地。然而，在后续运行中，借助 `os` 库，我们将检查文件是否存在，否则将其下载。我们将重命名两个变量：`PAY_0`
    到 `PAY_1`，并将“下个月默认付款”重命名为 `default`。我们不认为 `ID` 列对机器学习有用，因此我们将删除它：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we load the file from the local directory into a DataFrame called `dataset`.
    There are 30,000 rows and 24 columns including the target variable:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将从本地目录将文件加载到名为 `dataset` 的 DataFrame 中。该文件包含 30,000 行和 24 列，包括目标变量：
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we run the `dataset.info()` method to check whether there are any missing
    values or wrongly encoded columns:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们运行 `dataset.info()` 方法来检查是否存在任何缺失值或编码错误的列：
- en: '![Figure 8.1 – Output of the dataset.info() method](img/B19297_08_1.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – `dataset.info()` 方法的输出](img/B19297_08_1.jpg)'
- en: Figure 8.1 – Output of the dataset.info() method
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – `dataset.info()` 方法的输出
- en: 'We don’t have any missing data; however, three categorical columns (`SEX`,
    `EDUCATION`, and `MARRIAGE`) have integer data types, which we may have to convert
    to strings. Since values in `SEX` might be ordinal, we will first remap them to
    `1` and `0`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有缺失数据；然而，有三个分类列（`SEX`、`EDUCATION` 和 `MARRIAGE`）的数据类型为整数，我们可能需要将它们转换为字符串。由于
    `SEX` 中的值可能是序数，因此我们将首先将它们重新映射到 `1` 和 `0`：
- en: '[PRE3]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we rerun `dataset.info()`, we will see that the data type for the three
    columns is now `category`; we can now one-hot encode them. We exclude `SEX` from
    one-hot encoding since a person is either a male or female (in this dataset) and
    that information can be captured in one column. We will also extract `SEX` and
    store it in another variable, `A`, and separate the target variable and independent
    features. Next, we create a mapping for values in the `SEX` feature to be used
    for analysis and visualization, to help interpret the results, so `1` will be
    mapped to `male` values and `0` will be mapped to `female` values. We store this
    mapping in the `A_str` variable:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次运行 `dataset.info()`，我们会看到三个列的数据类型现在是 `category`；我们现在可以一维编码它们。我们排除 `SEX`
    进行一维编码，因为在这个数据集中，一个人要么是男性要么是女性，并且该信息可以包含在一列中。我们还将提取 `SEX` 并将其存储在另一个变量 `A` 中，并分离目标变量和独立特征。接下来，我们为
    `SEX` 特征中的值创建一个映射，用于分析和可视化，以帮助解释结果，因此 `1` 将映射到 `male` 值，而 `0` 将映射到 `female` 值。我们将此映射存储在
    `A_str` 变量中：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, let’s load all the required libraries.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们加载所有必需的库。
- en: Loading the libraries
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载库
- en: 'To run the example, you will need the following additional libraries:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行示例，你需要以下额外的库：
- en: '`sklearn` (scikit-learn) for data preprocessing and fitting the models'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn`（scikit-learn）用于数据预处理和拟合模型'
- en: '`numpy` to calculate some metrics and do some data wrangling'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy` 用于计算一些指标和一些数据处理'
- en: '`imblearn` for over and undersampling'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imblearn` 用于过采样和欠采样'
- en: '`fairlearn` to calculate bias and fairness scores'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fairlearn` 用于计算偏差和公平性分数'
- en: '`shap` to visualize the interpretations of the model'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shap` 用于可视化模型的解释'
- en: 'We load all the libraries at the start:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在开始时加载所有库：
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we split the dataset into `train` and `test`, using `train_test_split`,
    and assign 20% of the data to test. We also split `A_str` into `A_train` and `A_test`,
    so we can calculate fairness scores on test data:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `train_test_split` 将数据集分为 `train` 和 `test`，并将 20% 的数据分配给测试。我们还把 `A_str`
    分为 `A_train` 和 `A_test`，这样我们就可以在测试数据上计算公平性分数：
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we create the decision tree classifier pipeline and train the algorithm
    with the sensitive features:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建决策树分类器管道并用敏感特征训练算法：
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we calculate the ROC score and extract the predictions. We also visualize
    the confusion matrix:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算 ROC 分数并提取预测值。我们还可视化混淆矩阵：
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The code generates the following confusion matrix:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成了以下混淆矩阵：
- en: '![Figure 8.2 – Output confusion matrix](img/B19297_08_2.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – 输出混淆矩阵](img/B19297_08_2.jpg)'
- en: Figure 8.2 – Output confusion matrix
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 输出混淆矩阵
- en: In order to check whether the algorithm is fair or not, we will first calculate
    false positive and false negative rates, and then compare those across male and
    female cohorts on the test dataset to see whether there are big differences between
    the two cohorts.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查算法是否公平，我们首先计算假阳性率和假阴性率，然后比较测试数据集中男性和女性群体之间的差异，以查看两个群体之间是否存在很大差异。
- en: 'In the following code block, we have created two functions to calculate a false
    positive rate and a false negative rate. We have further created a dictionary
    of fairness metrics, in which we use the false positive rate and false negative
    rate, alongside a balanced accuracy metric from scikit-learn. We have then created
    a list of fairness metrics and stored them in a variable for easy access:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们创建了两个函数来计算假阳性率和假阴性率。我们进一步创建了一个公平性度量字典，其中我们使用了假阳性率和假阴性率，以及来自 scikit-learn
    的平衡准确度指标。然后我们创建了一个公平性度量列表，并将其存储在一个变量中以方便访问：
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We have also created a function to report the differences between male and
    female cohorts on the fairness metrics. We first create a DataFrame called `metricframe`
    using the convenience function from `fairlearn` called `MetricFrame`. It takes
    in true labels, predictions, and sensitive feature values, along with a dictionary
    of metrics to report on. We then leverage the `.by_group` property to report on
    fairness metrics for each cohort. Within the function, we also report on `equalised_odds_difference`
    and `demographic_parity_ratio` from the `fairlearn` library to understand the
    overall fairness of the model:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还创建了一个函数来报告男性和女性群体在公平性度量上的差异。我们首先使用 `fairlearn` 的便利函数 `MetricFrame` 创建一个名为
    `metricframe` 的 DataFrame。它接受真实标签、预测和敏感特征值，以及一个报告的度量字典。然后我们利用 `.by_group` 属性报告每个群体的公平性度量。在函数内部，我们还报告了来自
    `fairlearn` 库的 `equalised_odds_difference` 和 `demographic_parity_ratio`，以了解模型的总体公平性：
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We now run the function and calculate the fairness scores. It is evident that
    the model is quite similar in male and female cohorts since false positive rates
    and false negative rates are similar among the cohorts. Since the difference in
    the false positive rate is larger than the false negative rate, the equalized
    odds difference is the same as the difference between the false positive rate
    of the two groups. We can also see that the demographic parity ratio is above
    0.8, which means that both cohorts are quite likely to get selected for a positive
    outcome:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在运行函数并计算公平性分数。很明显，由于在各个群体中假阳性率和假阴性率相似，模型在男性和女性群体中相当相似。由于假阳性率与假阴性率的差异大于假阴性率，均衡机会差异与两组假阳性率之间的差异相同。我们还可以看到，人口比例率高于0.8，这意味着两个群体都有相当大的可能性获得积极的结果：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will display the following output:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示以下输出：
- en: '![Figure 8.3 – Fairness scores](img/B19297_08_3.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – 公平性分数](img/B19297_08_3.jpg)'
- en: Figure 8.3 – Fairness scores
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 公平性分数
- en: 'To illustrate bias in the dataset, we may need to generate a synthetic variable
    that correlates with a real-world scenario where, based on history, a cohort is
    treated more unfairly. First, we compare the default rate across males and females
    in the training dataset. We then add the synthetic noise:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明数据集中的偏差，我们可能需要生成一个与真实世界场景相关的合成变量，在该场景中，根据历史数据，一个群体受到更不公平的对待。首先，我们比较训练数据集中男性和女性的违约率。然后我们添加合成噪声：
- en: '[PRE12]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Given that the male default is higher than the female, we can replicate a biased
    scenario where applicants with lower default rates will have lower interest rates,
    but applicants with higher default rates will have higher interest rates imposed
    by the bank. Let’s assume the bank managers believe males are more likely to default,
    and instead of generalizing the scenario, the bank decides to charge higher interest
    rates to males.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于男性的违约率高于女性，我们可以复制一个偏差场景，其中违约率较低的申请者将获得较低的利率，而违约率较高的申请者将受到银行施加的较高利率。让我们假设银行经理认为男性更有可能违约，并且银行决定不对场景进行泛化，而是对男性收取更高的利率。
- en: To mimic this scenario, we will introduce a new feature, `Interest_rate`, following
    a Gaussian distribution. The mean will be 0 where someone hasn’t defaulted, but
    will be 2 times 1 where someone has defaulted. We also set the standard deviation
    to 2 for males and 1 for females.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟这个场景，我们将引入一个新的特征，`Interest_rate`，遵循高斯分布。当某人没有违约时，平均值将是0，但如果有违约，将是1的两倍。我们还为男性设置标准差为2，为女性设置标准差为1。
- en: 'To generate the synthetic Gaussian distribution, we use the `numpy.random.normal`
    method, with a seed of `42` for reproducibility:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成合成的高斯分布，我们使用`numpy.random.normal`方法，种子为`42`以确保可重复性：
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that we have added the noise, we retrain the algorithm with the interest
    variable and recalculate the fairness metrics. We first split the data, then retrain
    and recalculate the fairness metrics. We resplit the data into `train` and `test`,
    as shown previously, and retrain the algorithm. Once retrained, we calculate the
    impact.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经添加了噪声，我们使用利率变量重新训练算法并重新计算公平性指标。我们首先分割数据，然后重新训练并重新计算公平性指标。我们像之前一样将数据重新分割成`train`和`test`，然后重新训练算法。一旦重新训练，我们计算影响。
- en: 'We can see, in the following code, that by adding the synthetic interest variable,
    we have improved the ROC metric:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们可以看到，通过添加合成的利率变量，我们提高了ROC指标：
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It is clear from the following output that we now have a more biased algorithm,
    based on equalized odds. The false negative rate is quite high in males, which
    means that more males who are unlikely to pay back to the bank are likely to be
    given a loan, and if this model was productionized, this could result in unfair
    outcomes:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下输出中可以清楚地看出，我们现在有一个基于均衡机会的更具偏差的算法。在男性中，假阴性率相当高，这意味着不太可能偿还银行的男性更有可能获得贷款，如果这个模型被投入生产，可能会导致不公平的结果：
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This will print the following information:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印以下信息：
- en: '![Figure 8.4 – Fairness scores](img/B19297_08_4.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – 公平性分数](img/B19297_08_4.jpg)'
- en: Figure 8.4 – Fairness scores
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 公平性分数
- en: To reduce the bias, we will apply the first data-centric debiasing technique
    under the feature selection by removing the sensitive variable from the algorithm.
    This can be done by retraining the algorithm without the `SEX` variable.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少偏差，我们将在特征选择中应用第一种以数据为中心的偏差消除技术，通过从算法中移除敏感变量来实现。这可以通过重新训练不带`SEX`变量的算法来完成。
- en: 'Given that the dataset is biased toward one gender due to a higher variation
    in interest rates, it is recommended in the real world that data engineers and
    data scientists work with domain experts and data producers to reduce this bias
    in the dataset. For instance, instead of using `SEX` to determine the interest
    rate, other features could be used, such as payment history, credit history, and
    income. In the training step, we can drop the `SEX` variable:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集因利率差异较大而偏向于某一性别，在现实世界中，建议数据工程师和数据科学家与领域专家和数据生产者合作，以减少数据集中的这种偏差。例如，与其使用`SEX`来确定利率，不如使用其他特征，如支付历史、信用历史和收入。在训练步骤中，我们可以删除`SEX`变量：
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'From the following output, we can see that by removing the `SEX` variable,
    the ROC score has dropped from 0.846 to 0.839:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下输出中，我们可以看到，通过移除`SEX`变量，ROC分数从0.846下降到0.839：
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Looking at the following fairness metrics, it is obvious that when the outcome
    is biased based on the cohort of data, removing the variable from the training
    can debias the algorithm. The false negative rate in `male` has decreased, whereas,
    in `female`, it has increased; however, the algorithm is more fair compared to
    when the `SEX` variable was used. The equalized odds have dropped from 0.18 to
    0.07, but the demographic parity ratio has reduced, which means one group has
    more chance of getting a loan than the other:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 观察以下公平性指标，很明显，当结果基于数据集的队列存在偏差时，从训练中移除变量可以消除算法的偏差。在`male`中的假阴性率有所下降，而在`female`中有所上升；然而，与使用`SEX`变量相比，算法更加公平。均衡机会从0.18下降到0.07，但人口比例比有所降低，这意味着一个群体获得贷款的机会比另一个群体更多：
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.5 – Fairness metrics](img/B19297_08_5.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – 公平性指标](img/B19297_08_5.jpg)'
- en: Figure 8.5 – Fairness metrics
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 公平性指标
- en: Next, we will show you how to apply undersampling techniques to ensure the outcome
    variable is balanced.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将向您展示如何应用欠采样技术以确保结果变量平衡。
- en: AllKNN undersampling method
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AllKNN欠采样方法
- en: We will start with the AllKNN algorithm from the `imblearn` package, and then
    try the instant hardness algorithm. Since the algorithms use KNN under the hood,
    which is a distance-based measure, we need to ensure that we scale the features
    using the scikit-learn `StandardScaler` method. We will first scale the variables,
    then run the sampling algorithm, and then train the decision tree. We will run
    the algorithm with 5 k cross-validation, and ensure the function returns the model
    trained. Cross-validation will be scored on `roc_auc` and balanced accuracy.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从`imblearn`包中的AllKNN算法开始，然后尝试即时硬度算法。由于这些算法在底层使用KNN，这是一种基于距离的度量，我们需要确保使用scikit-learn的`StandardScaler`方法对特征进行缩放。我们将首先缩放变量，然后运行采样算法，然后训练决策树。我们将使用5
    k交叉验证运行算法，并确保函数返回训练好的模型。交叉验证将在`roc_auc`和平衡准确度上进行评分。
- en: We will first try an undersampling technique, `AllKNN`, from `imblearn`. This
    algorithm does not aim at balancing majority and minority classes; however, it
    removes instances that are harder to classify from the majority class. It does
    that iteratively where, first, the model is trained on the entire dataset. Then,
    in the prediction step of the majority class, if a disagreement occurs between
    any of the neighbors about the predicted outcome, the data point is removed from
    the majority class. In the first iteration, a 1-KNN model is trained and some
    samples are removed, and then in the next iteration, a 2-KNN model is trained,
    and in the following iteration, a 3-KNN model is trained. Usually, the algorithm
    (by default) will end at the 3-KNN iteration; however, the practitioner can choose
    more iterations, and the algorithm will not stop until the number of samples between
    the majority and minority class becomes the same or a maximum number of iterations
    is reached – whichever happens earlier.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先尝试一个来自`imblearn`的欠采样技术，`AllKNN`。这个算法并不旨在平衡多数和少数类别；然而，它从多数类别中移除难以分类的实例。它是通过迭代实现的，首先在完整数据集上训练模型。然后在多数类别的预测步骤中，如果邻居之间关于预测结果存在任何不一致，该数据点将从多数类别中移除。在第一次迭代中，训练一个1-KNN模型并移除一些样本，然后在下一次迭代中，训练一个2-KNN模型，在接下来的迭代中，训练一个3-KNN模型。通常，算法（默认情况下）将在3-KNN迭代结束时停止；然而，实践者可以选择更多的迭代，算法将不会停止，直到多数和少数类别的样本数量相同或达到最大迭代次数——哪个先到为止。
- en: 'Let’s first define the scaler and sampler method:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来定义缩放器和采样方法：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we create a pipeline object and pass the scaler, sampler, and estimator
    to the pipeline:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个管道对象，并将缩放器、采样器和估计器传递到管道中：
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then we pass the training data and run the cross-validation. We set the cross-validation
    method to return the estimator (pipeline) by setting `return_estimator=True`,
    so that we can use it to make predictions on the test data:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们传递训练数据并运行交叉验证。我们将交叉验证方法设置为通过设置 `return_estimator=True` 返回估计器（管道），这样我们就可以用它来对测试数据进行预测：
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we print the mean and standard deviation of ROC and balanced accuracy
    from the cross-validation step, returned from prediction results in each step,
    where at each step, four folds were used on training and the prediction was made
    on the fifth fold:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们打印交叉验证步骤返回的预测结果中每个步骤的ROC和平衡准确率的平均值和标准差，在每个步骤中，训练使用了四个折，并在第五个折上进行预测：
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can see that by removing hard examples using the undersampling technique,
    `roc_auc` on `test` data bumped from 0.839 in the previous step to 0.85:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，通过使用下采样技术移除困难示例，`test` 数据上的 `roc_auc` 从上一步的 0.839 上升到 0.85：
- en: '[PRE23]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we calculate the fairness metrics. Although the false negative rate has
    decreased for both males and females, the false positive rate has increased, and
    the equalized odds difference has also increased from the previous step. This
    might be because cases with male samples that were difficult to classify have
    been removed:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算公平性指标。尽管男性和女性的假阴性率都有所下降，但假阳性率有所上升，与上一步相比，均衡概率差异也有所增加。这可能是由于难以分类的男性样本已被移除：
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![Figure 8.6 – Fairness metrics](img/B19297_08_6.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – 公平性指标](img/B19297_08_6.jpg)'
- en: Figure 8.6 – Fairness metrics
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 公平性指标
- en: We will now explore the impact on the fairness metrics by introducing hard cases.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将通过引入困难案例来探索对公平性指标的影响。
- en: Instance hardness undersampling method
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实例硬度下采样方法
- en: As the name suggests, the instance hardness method focuses on samples that are
    harder to classify, which are usually at the boundary or overlap with other classes.
    Usually, this depends on the algorithm used (as some algorithms are better at
    some hard cases than others) and the level of overlap between the classes. For
    such samples, the learning algorithm will usually show the low probability prediction
    on the hard cases, which means the lower the probability, the higher the instance
    hardness. Under the hood, the method has the capability to retain the right number
    of samples, based on the class imbalance.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，实例硬度方法专注于难以分类的样本，这些样本通常位于边界或与其他类别重叠。通常，这取决于所使用的算法（因为一些算法在某些困难情况下比其他算法更好）以及类别之间的重叠程度。对于这样的样本，学习算法通常会显示对困难案例的低概率预测，这意味着概率越低，实例硬度越高。在底层，该方法具有根据类别不平衡保留正确数量样本的能力。
- en: In the first step, we will define the algorithm, and the algorithm will be passed
    on to the instance hardness step. We will then define the instance hardness undersampling
    method, with three-fold cross-validation.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们将定义算法，并将算法传递到实例硬度步骤。然后我们将定义实例硬度下采样方法，采用三折交叉验证。
- en: 'Next, we create the decision tree estimator. Finally, we combine the steps
    in the pipeline with scaling the dataset, then undersampling the data, and finally,
    training the model. When the pipeline is defined, we run the cross-validation
    similar to the previous pipeline:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建决策树估计器。最后，我们将管道中的步骤与缩放数据集、下采样数据和最终训练模型相结合。当管道定义后，我们运行与之前管道类似的交叉验证：
- en: '[PRE25]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Both `AllKNN` and `InstanceHardness` returned similar cross-validation results:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`AllKNN` 和 `InstanceHardness` 返回了类似的交叉验证结果：'
- en: '[PRE26]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The ROC slightly bumped from 0.85 to 0.854 on the `test` data when using the
    instance hardness method:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用实例硬度方法时，`test` 数据上的ROC从0.85略微上升到0.854：
- en: '[PRE27]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The fairness metrics are quite similar to the previous undersampling technique,
    and probably due to similar reasons, where, by removing difficult cases, the model
    is unable to deal with predicting difficult `male` cases. However, in both undersampling
    methods, the equalized odds have increased, compared to the feature selection
    step. Also, the demographic parity ratio is still under 0.8, which means one subclass
    of gender is more likely to be selected over another when predicting `default`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性指标与之前的欠采样技术相当相似，可能由于类似的原因，通过去除困难案例，模型无法处理预测困难的`男性`案例。然而，在两种欠采样方法中，与特征选择步骤相比，均衡机会增加了，而且人口统计学平等比率仍然低于0.8，这意味着在预测`违约`时，一个性别子类更有可能被选中而不是另一个：
- en: '[PRE28]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![Figure 8.7 – Fairness metrics](img/B19297_08_7.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 - 公平性指标](img/B19297_08_7.jpg)'
- en: Figure 8.7 – Fairness metrics
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 - 公平性指标
- en: Next, let’s look at oversampling methods.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看过采样方法。
- en: Oversampling methods
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过采样方法
- en: Another way of improving model performance and fairness metrics is by introducing
    additional examples. The next two oversampling techniques, `SMOTE` and `ADASYN`,
    were introduced in [*Chapter 7*](B19297_07.xhtml#_idTextAnchor111), *Using Synthetic
    Data in Data-Centric Machine Learning*, hence we will not cover the details behind
    the algorithm. We will use these techniques in the context of improving fairness
    metrics by adding additional examples, in the hope that the model is able to learn
    better with additional data points.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 提高模型性能和公平性指标的一种方法是引入额外的示例。接下来的两种过采样技术，`SMOTE`和`ADASYN`，在[*第7章*](B19297_07.xhtml#_idTextAnchor111)中介绍，*在以数据为中心的机器学习中使用合成数据*，因此我们不会详细介绍算法背后的细节。我们将使用这些技术，通过添加额外的示例来提高公平性指标，希望模型能够通过额外的数据点更好地学习。
- en: For each of the methods, we will first scale the dataset, add additional minority
    class examples, and then train the model. We will print the cross-validation scores
    and the `test` ROC score, as well as fairness metrics.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每种方法，我们首先将数据集进行缩放，添加额外的少数类示例，然后训练模型。我们将打印交叉验证分数和`测试`ROC分数，以及公平性指标。
- en: SMOTE
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SMOTE
- en: 'Given that we used this algorithm in [*Chapter 7*](B19297_07.xhtml#_idTextAnchor111),
    *Using Synthetic Data in Data-Centric Machine Learning*, we will dive straight
    into the code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在[*第7章*](B19297_07.xhtml#_idTextAnchor111)中使用了此算法，*在以数据为中心的机器学习中使用合成数据*，我们将直接进入代码：
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The validation metrics and the `test` ROC score show poorer results compared
    to the undersampling methods covered previously. In the next step, we explore
    the fairness metrics:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前介绍的欠采样方法相比，验证指标和`测试`ROC分数显示出较差的结果。在下一步中，我们将探索公平性指标：
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![Figure 8.8 – Fairness metrics](img/B19297_08_8.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8 - 公平性指标](img/B19297_08_8.jpg)'
- en: Figure 8.8 – Fairness metrics
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 - 公平性指标
- en: The fairness metrics are better in comparison with the undersampling methods
    – that is, the difference between false positive and false negative rates is reduced
    between men and women and, based on the demographic parity ratio, the model is
    more likely to select both types of gender applicants for loan default. In the
    next section, we will use the `ADASYN` algorithm and compare it with `SMOTE` and
    other undersampling methods.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 与欠采样方法相比，公平性指标更好——也就是说，男性和女性之间的假阳性率和假阴性率之间的差异减小了，基于人口统计学平等比率，模型更有可能同时选择两种性别的贷款违约申请人。在下一节中，我们将使用`ADASYN`算法，并将其与`SMOTE`和其他欠采样方法进行比较。
- en: ADASYN
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ADASYN
- en: 'Similarly to the `SMOTE` method, we covered `ADASYN` in [*Chapter 7*](B19297_07.xhtml#_idTextAnchor111),
    *Using Synthetic Data in Data-Centric Machine Learning*, hence we will dive straight
    into the code, in which we oversample the minority class:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 与`SMOTE`方法类似，我们在[*第7章*](B19297_07.xhtml#_idTextAnchor111)中介绍了`ADASYN`，*在以数据为中心的机器学习中使用合成数据*，因此我们将直接进入代码，其中我们将对少数类进行过采样：
- en: '[PRE31]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The validation metrics and `test` ROC score are slightly below the `SMOTE`
    results and undersampling methods. Now, let’s review the fairness metrics:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 验证指标和`测试`ROC分数略低于`SMOTE`结果和欠采样方法。现在，让我们回顾公平性指标：
- en: '[PRE32]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![Figure 8.9 – Fairness metrics](img/B19297_08_9.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图8.9 - 公平性指标](img/B19297_08_9.jpg)'
- en: Figure 8.9 – Fairness metrics
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 - 公平性指标
- en: The equalized odds are slightly higher for `ADASYN`, whereas demographic parity
    is slightly better when compared to `SMOTE`, and both oversampling techniques
    guarantee higher fairness over undersampling methods, but slightly poorer ROC
    performance.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`ADASYN`的均衡机会略高，而与`SMOTE`相比，人口比例略好，两种过采样技术都保证了比欠采样方法更高的公平性，但ROC性能略差。'
- en: We have now seen that, despite balancing the classes, model fairness is compromised,
    and it is mostly `male` examples where the model is making more errors. So, in
    the next section, we will randomly introduce some additional `male` examples where
    the model is misclassifying positive cases.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，尽管平衡了类别，但模型公平性受到了损害，模型在大多数`男性`例子上犯的错误更多。因此，在下一节中，我们将随机引入一些额外的`男性`例子，其中模型错误地将阳性案例分类为阴性。
- en: Oversampling plus misclassified examples at random
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机过采样和错误分类的例子
- en: We will first balance the dataset using `ADASYN` and avoid undersampling techniques
    since we want to retain hard cases that are difficult to classify. We then train
    the model and identify `male` cases that the model believes should be positive
    but wrongly classifies as negative. We then randomly select 10% of these cases,
    add them back to the training dataset, and retrain the model with the same algorithm.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用`ADASYN`平衡数据集，并避免欠采样技术，因为我们希望保留难以分类的困难案例。然后我们训练模型，并识别模型认为应该是阳性但错误地将其分类为阴性的`男性`案例。然后我们随机选择这些案例的10%，将它们重新添加到训练数据集中，并使用相同的算法重新训练模型。
- en: At the end, we review the model metrics and fairness metrics on the `test` data.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们回顾`测试`数据上的模型指标和公平性指标。
- en: 'We utilize oversampling and reintroduce misclassified data points at random.
    Let’s run the pipeline with the `ADASYN` oversampling method:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用过采样并在随机位置重新引入被错误分类的数据点。让我们使用`ADASYN`过采样方法运行管道：
- en: '[PRE33]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we identify the examples from the training dataset where the model is
    making errors on the male population – that is, examples where the model predicts
    false negatives. We first subset the data associated with males and then run predictions
    over this data:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们识别训练数据集中模型在男性人口上犯错误的例子——即模型预测错误的阴性例子。我们首先对与男性相关的数据进行子集划分，然后在此数据上运行预测：
- en: '[PRE34]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then we subset this data where the `true` label is `1` but model predictions
    are `0`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们选取`true`标签为`1`但模型预测为`0`的数据子集：
- en: '[PRE35]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We randomly select 10% of the values and add them to the `X_train` dataset.
    We leverage the `.sample` method, and this random selection is done with replacement:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随机选择10%的值并将它们添加到`X_train`数据集中。我们利用`.sample`方法，并且这个随机选择是带替换进行的：
- en: '[PRE36]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, we add this 10% to `X_train` and `y_train` and create a new dataset:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这10%添加到`X_train`和`y_train`中，并创建一个新的数据集：
- en: '[PRE37]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Then, we train the algorithm on this new dataset and print out the validation
    metrics and the `test` ROC score:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在新的数据集上训练算法，并打印出验证指标和`测试`ROC分数：
- en: '[PRE38]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Compared to the oversampling section, the validation metrics are quite similar,
    as is the `test` ROC score. Next, we review the fairness metrics to check whether
    they have improved:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 与过采样部分相比，验证指标和`测试`ROC分数相当相似。接下来，我们回顾公平性指标，以检查它们是否有所改善：
- en: '[PRE39]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![Figure 8.10 – Fairness metrics](img/B19297_08_10.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图8.10 – 公平性指标](img/B19297_08_10.jpg)'
- en: Figure 8.10 – Fairness metrics
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 – 公平性指标
- en: By adding some false negative `male` examples, we can see that the equalized
    odds have improved slightly to 0.098 and the demographic ratio has also improved,
    increasing to 0.85\. We believe that even better results can be achieved if we
    add false positive examples and false negative examples and combine these with
    undersampling and oversampling techniques.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加一些错误的阴性`男性`例子，我们可以看到均衡机会略微提高到了0.098，人口比例也有所改善，增加到0.85。我们相信，如果我们添加错误的阳性例子和错误的阴性例子，并将这些与欠采样和过采样技术结合起来，我们可以取得更好的结果。
- en: To demonstrate this, we will iterate over four undersampling techniques (`AllKNN`,
    `RepeatedEditedNearestNeighbours`, `InstanceHardnessThreshold`, and `Tomek`),
    two oversampling techniques (`SMOTE` and `ADASYN`), and two combinations of over
    and undersampling techniques (`SMOTEENN` and `SMOTETomek`). How these algorithms
    work is outside the scope of this example. Instead, the goal is to demonstrate
    how these data techniques can lead to better selection and a generalized model
    with slightly poorer performance, but higher fairness.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示这一点，我们将遍历四种欠采样技术（`AllKNN`、`RepeatedEditedNearestNeighbours`、`InstanceHardnessThreshold`
    和 `Tomek`），两种过采样技术（`SMOTE` 和 `ADASYN`），以及两种过采样和欠采样的组合技术（`SMOTEENN` 和 `SMOTETomek`）。这些算法的工作原理超出了本例的范围。相反，目标是演示这些数据技术如何导致更好的选择和具有略微较差性能但更高公平性的泛化模型。
- en: We will now develop a mechanism where we first train the algorithm and then
    add false positive examples and false negative examples. Once the examples are
    added, we run the pipeline by sampling the dataset, using the previous algorithms.
    We'll record fairness outcomes and the ROC score to find the technique that best
    fosters a balance between fairness and performance in our algorithm.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将开发一种机制，首先训练算法，然后添加假阳性和假阴性示例。一旦添加了示例，我们将通过采样数据集并使用先前算法运行管道。我们将记录公平性结果和 ROC
    分数，以找到在我们算法中最好地促进公平性和性能平衡的技术。
- en: 'We will first create a dictionary with a configuration of each of the aforementioned
    sampling techniques so we can iterate over it. We can call this AutoML for the
    sampling technique:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个字典，包含上述每种采样技术的配置，这样我们就可以遍历它。我们可以将这个采样技术称为 AutoML：
- en: '[PRE40]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, we create two functions that take the training dataset, model, column,
    and its subset value to help create random samples. The following function will
    sample false positives:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建了两个函数，这些函数接受训练数据集、模型、列及其子集值，以帮助创建随机样本。以下函数将采样假阳性：
- en: '[PRE41]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'And this function samples false negatives. By default, both methods will add
    10% random examples with replacement:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将采样假阴性。默认情况下，两种方法都会添加 10% 的随机示例，并替换它们：
- en: '[PRE42]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, we create a function that calculates test metrics post-data improvements.
    The function takes the test data and estimator and returns model metrics and fairness
    metrics:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个函数，该函数在数据改进后计算测试指标。该函数接受测试数据和估计器，并返回模型指标和公平性指标：
- en: '[PRE43]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Next, we create a pipeline that will sample the dataset and then create random
    false positive `male` and false negative `male` examples. We then combine these
    into the training data, one at a time, and retrain the same algorithm. We then
    calculate the metrics and store them in a list called `results` with columns.
    Each iteration adds false negative and false positive examples with model performance
    and fairness metrics. We then use this list to compare the results across algorithms.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个管道，该管道将采样数据集，然后创建随机的假阳性 `male` 和假阴性 `male` 示例。然后我们将这些示例逐个合并到训练数据中，并重新训练相同的算法。然后我们计算指标并将它们存储在一个名为
    `results` 的列表中，其中包含列。每次迭代都会添加带有模型性能和公平性指标的假阴性和假阳性示例。然后我们使用这个列表来比较算法之间的结果。
- en: Note
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The code for creating the pipeline is pretty lengthy. Please refer to GitHub
    for the full code: [https://github.com/PacktPublishing/Data-Centric-Machine-Learning-with-Python/tree/main/Chapter%208%20-%20Techniques%20for%20identifying%20and%20removing%20bias](https://github.com/PacktPublishing/Data-Centric-Machine-Learning-with-Python/tree/main/Chapter%208%20-%20Techniques%20for%20identifying%20and%20removing%20bias)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 创建管道的代码相当长。请参阅 GitHub 以获取完整代码：[https://github.com/PacktPublishing/Data-Centric-Machine-Learning-with-Python/tree/main/Chapter%208%20-%20Techniques%20for%20identifying%20and%20removing%20bias](https://github.com/PacktPublishing/Data-Centric-Machine-Learning-with-Python/tree/main/Chapter%208%20-%20Techniques%20for%20identifying%20and%20removing%20bias)
- en: 'Next, we create a DataFrame called `df` and add all the `test` metrics so we
    can compare which method reaped the best model performance and fairness metrics:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个名为 `df` 的 DataFrame，并将所有的 `test` 指标添加进去，以便我们可以比较哪种方法获得了最佳模型性能和公平性指标：
- en: '[PRE44]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let’s sort the DataFrame based on equalized odds:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们根据均衡机会对 DataFrame 进行排序：
- en: '[PRE45]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We can see that when the dataset was sampled with Tomek Links, where difficult
    cases were removed from the boundary and combined with additional false positive
    `male` training samples, this resulted in the best equalized odds of 0.075; however,
    a demographic parity of 0.8 was not achieved. When the SMOTETomek technique was
    used in combination with false negative `male` examples, the model achieved a
    0.088 equalized odds ratio, which was the best among the sampling methods, and
    the model also achieved a high demographic parity ratio.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，当数据集使用Tomek Links进行采样时，从边界移除了困难案例，并与额外的错误阳性`男性`训练样本结合，这导致了最佳均衡概率为0.075；然而，没有达到0.8的人口比例。当使用SMOTETomek技术与错误阴性`男性`示例结合时，模型实现了0.088的均衡概率比，这是所有采样方法中最好的，模型也实现了高的人口比例比。
- en: '![Figure 8.11 – Resulting output dataset](img/B19297_08_11.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图8.11 – 结果输出数据集](img/B19297_08_11.jpg)'
- en: Figure 8.11 – Resulting output dataset
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 – 结果输出数据集
- en: Oversampling with anomalies
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用异常值进行过采样
- en: In the previous steps, we learned that by adding poorly classified examples
    to the training dataset, we were able to improve model fairness. In the next step,
    instead of choosing samples at random, we will utilize an algorithm that identifies
    anomalies and then we add these anomalies to the training dataset as an oversampling
    mechanism.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们了解到通过将错误分类的示例添加到训练数据集中，我们能够提高模型公平性。在下一步中，我们不会随机选择样本，而是将利用一个识别异常值的算法，然后我们将这些异常值添加到训练数据集中作为过采样机制。
- en: 'First, we create a pipeline to oversample the minority class:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个管道来过采样少数类：
- en: '[PRE46]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, we extract the oversampled data. There is no reason why undersampling
    or no sampling could not have been chosen. Once the oversampled data is extracted,
    we then scale it back to the original feature space:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们提取过采样数据。没有理由为什么不能选择欠采样或无采样。一旦提取过采样数据，我们将其缩放回原始特征空间：
- en: '[PRE47]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Next, we train the isolation forest to identify 10% of anomalies. To do that,
    we set the contamination to `0.1`. We then fit the model on resampled data, and
    run prediction on this data. We store the results in a column called `IF_anomaly`
    and add it to the resampled dataset. We then extract these anomalies, as isolation
    forest labels with a value of `-1`:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们训练隔离森林以识别10%的异常值。为此，我们将污染率设置为`0.1`。然后我们在重采样数据上拟合模型，并在此数据上运行预测。我们将结果存储在一个名为`IF_anomaly`的列中，并将其添加到重采样数据集中。然后我们提取这些异常值，作为隔离森林标签，其值为`-1`：
- en: '[PRE48]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Next, we add these additional data points to the original dataset and train
    the decision tree model. Once the model is fitted, we calculate the ROC score
    on the `test` data. We can see that this is 0.82:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将这些额外的数据点添加到原始数据集中，并训练决策树模型。一旦模型拟合完成，我们就在`测试`数据上计算ROC分数。我们可以看到这是0.82：
- en: '[PRE49]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, we calculate the fairness metrics. Based on the following results, we
    can say that the model trained in the previous section produced better fairness
    and demographic parity ratio scores:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算公平性指标。根据以下结果，我们可以说，在上一节中训练的模型产生了更好的公平性和人口比例比分数：
- en: '![Figure 8.12 – Fairness metrics](img/B19297_08_12.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![图8.12 – 公平性指标](img/B19297_08_12.jpg)'
- en: Figure 8.12 – Fairness metrics
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 – 公平性指标
- en: Now that we have utilized various examples of undersampling and oversampling
    data, including reintroducing random misclassified examples and anomalies, in
    the next section, we will utilize an advanced technique, where we will be more
    selective with which examples to add and which examples to remove, to further
    reduce bias in the algorithm.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经利用了各种欠采样和过采样的数据示例，包括重新引入随机错误分类的示例和异常值，在下一节中，我们将利用一种高级技术，我们将更慎重地选择添加哪些示例以及移除哪些示例，以进一步减少算法中的偏差。
- en: Shapley values to detect bias, oversample, and undersample data
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Shapley值来检测偏差、过采样和欠采样数据
- en: In this section, we will utilize Shapley values to identify examples where the
    model struggles to make the correct prediction. We will use the impact score to
    either add, eliminate, or use a combination of both to improve the fairness metrics.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将利用Shapley值来识别模型难以做出正确预测的示例。我们将使用影响分数来添加、消除或使用两者的组合来提高公平性指标。
- en: '**SHAP** (which stands for **Shapley Additive exPlanations**) is a model-agnostic
    approach in machine learning that is built on the principles of game theory. It
    helps study the importance of the feature and the feature interaction on the final
    outcome by assigning it a score, similar to how it would be done in a game where
    each player’s contribution at a given time is calculated in the output of the
    score.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '**SHAP**（代表**Shapley Additive exPlanations**）是一种基于博弈论原理的无模型偏见机器学习方法。它通过分配一个分数来帮助研究特征及其交互对最终结果的重要性，类似于在游戏中计算每个玩家在特定时间点的贡献，就像在计算分数输出时那样。'
- en: Shapley values can help provide global importance (the overall impact of the
    feature on all the predictions), but also local importance (the impact of each
    feature on a single outcome). It can also help understand the direction of impact
    – that is, whether a feature has a positive impact or a negative impact.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Shapley值可以帮助提供全局重要性（特征对所有预测的整体影响），也可以提供局部重要性（每个特征对单个结果的影响）。它还可以帮助理解影响的方向——也就是说，一个特征是否有积极影响或消极影响。
- en: Hence, there are a lot of use cases for Shapley values in machine learning,
    such as bias detection, local and global model debugging, model auditing, and
    model interpretability.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在机器学习中，Shapley值有很多应用场景，例如偏差检测、局部和全局模型调试、模型审计和模型可解释性。
- en: 'We use Shapley values in this section to understand the model and feature impacts
    on the outcomes. We leverage the impacts of these features and identify where
    the model is likely to make the most mistakes. We then apply two techniques: one
    to remove these rows from the data and the other to oversample the data with these
    rows.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用Shapley值来理解模型和特征对结果的影响。我们利用这些特征的影响，并确定模型最有可能犯错误的地方。然后我们应用两种技术：一种是从数据中删除这些行，另一种是对包含这些行的数据进行过采样。
- en: 'First, we import SHAP and then train the decision tree model on the oversampled
    dataset. At the end of the step, we have a model and oversampled `X` and `y` samples.
    We include the `SEX` variable in the training data to see whether Shapley values
    can help us detect bias. First, we need to resplit the data into `train` and `test`
    sets, as done in the previous sections:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入SHAP库，然后在过采样数据集上训练决策树模型。在步骤结束时，我们有一个模型和过采样的`X`和`y`样本。我们将`SEX`变量包含在训练数据中，以查看Shapley值是否可以帮助我们检测偏差。首先，我们需要将数据重新拆分为`train`和`test`集，就像前几节所做的那样：
- en: '[PRE50]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Next, we define the SHAP tree explainer, by providing the decision tree model
    and then extract the Shapley values for the `train` set using the `.``shap_values`
    method:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义SHAP树解释器，通过提供决策树模型，然后使用`.shap_values`方法提取`train`集的Shapley值：
- en: '[PRE51]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let’s extract the first row of Shapley values, for class 0\. The array contains
    the contribution of each feature value to decide the final output. Positive values
    mean the corresponding features have a positive impact on predicting the output
    as class 0, while negative values negatively contribute toward predicting class
    0:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们提取Shapley值的第一个行，针对类别0。数组包含每个特征值对最终输出的贡献。正值表示相应的特征对预测类别0有积极影响，而负值则对预测类别0产生负面影响：
- en: '[PRE52]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This will print out the following array:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出以下数组：
- en: '![Figure 8.13 – Resulting output array](img/B19297_08_13.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![图8.13 – 结果输出数组](img/B19297_08_13.jpg)'
- en: Figure 8.13 – Resulting output array
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 – 结果输出数组
- en: 'Next, we generate a summary plot for class label 0:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为类别标签0生成一个摘要图：
- en: '[PRE53]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This will generate the following plot:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![Figure 8.14 – SHAP values](img/B19297_08_14.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![图8.14 – SHAP值](img/B19297_08_14.jpg)'
- en: Figure 8.14 – SHAP values
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 – SHAP值
- en: The red dots represent the high value of a feature while the blue dots represent
    the low value of the corresponding feature. The *x* axis denotes the Shapley value,
    where the positive value means the data point has a positive impact in predicting
    class 0, whereas the negative value means the data point for the corresponding
    feature negatively affects the prediction for class 0\. If we look at *Figure
    8**.13,* it is quite evident that high interest rates and male customers negatively
    affect the prediction of class 0\. Shapley values do indicate a model bias toward
    male customers.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 红点代表特征的高值，而蓝点代表相应特征的低值。*x*轴表示Shapley值，正值表示数据点在预测类别0时具有积极影响，而负值表示对应特征的数据点对类别0的预测产生负面影响。如果我们看*图8.13*，高利率和男性客户对类别0的预测产生负面影响是非常明显的。Shapley值确实表明模型对男性客户存在偏见。
- en: 'Next, we generate a summary plot for class label 1:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为类别标签 1 生成一个摘要图：
- en: '[PRE54]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This will generate the following summary plot:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下摘要图：
- en: '![Figure 8.15 – SHAP summary plot](img/B19297_08_15.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.15 – SHAP 摘要图](img/B19297_08_15.jpg)'
- en: Figure 8.15 – SHAP summary plot
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.15 – SHAP 摘要图
- en: In comparison with the summary plot for class 0, high interest rates and male
    customers positively impact defaulting on the loan – that is, if you are a male
    and previously had a higher interest rate, you are likely to default on the loan.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 与类别 0 的摘要图相比，高利率和男性客户对贷款违约有积极影响——也就是说，如果你是男性并且之前有更高的利率，你很可能会违约。
- en: 'We previously learned that by removing the `SEX` feature from model training,
    the model becomes fairer, and Shapley values are clearly indicated using summary
    plots. Now, we extract the Shapley values by training the new model without the
    `SEX` feature. We then score the training data to first identify all the rows
    corresponding to false negatives and false positives. We then calculate the sum
    of Shapley values for each row where the model made errors, and then hold out
    the ones with the lowest impact. We run two experiments: first, we undersample
    the training dataset and calculate fairness metrics, and second, we oversample
    the training dataset to give a better signal to the model and recalculate fairness
    metrics.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前了解到，通过从模型训练中移除 `SEX` 特征，模型变得更加公平，并且使用摘要图清楚地指示 Shapley 值。现在，我们通过训练不带 `SEX`
    特征的新模型来提取 Shapley 值。然后我们对训练数据进行评分，首先识别所有对应于假阴性和假阳性的行。然后我们计算模型出错时每行的 Shapley 值总和，并保留影响最低的行。我们运行两个实验：首先，我们对训练数据集进行下采样并计算公平性指标，其次，我们对训练数据集进行上采样以向模型提供更好的信号并重新计算公平性指标。
- en: 'First, let’s train the model without the `SEX` feature:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们在不包含 `SEX` 特征的情况下训练模型：
- en: '[PRE55]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Next, we extract Shapley values:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们提取 Shapley 值：
- en: '[PRE56]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We score the training data, calculate predictions, and store these in `Y_pred`:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对训练数据进行评分，计算预测值，并将这些值存储在 `Y_pred` 中：
- en: '[PRE57]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Then we will check the sum of the Shapley value for class 0 and class 1 at
    index 0, and print the corresponding prediction and `true` value:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将检查索引 0 处类别 0 和类别 1 的 Shapley 值总和，并打印相应的预测值和 `true` 值：
- en: '[PRE58]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The model predicted `0`. Next, we extract the Shapley values where the model
    made a mistake. For that, we use the list comprehension with zip functionality.
    The first value of the array will be the index location of the data point so we
    know which Shapley value is associated with which row. The next values are in
    order of prediction, the `true` value, the sum of Shapley values for the row for
    class 0, and the sum of Shapley values for class 1\. Once we have extracted those,
    we create a DataFrame and store the values in `df`, and we sample through the
    DataFrame to see five values. We use a random seed for reproducibility:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预测了 `0`。接下来，我们提取模型出错时的 Shapley 值。为此，我们使用带有 zip 功能的列表推导。数组的第一个值将是数据点的索引位置，这样我们就可以知道哪个
    Shapley 值与哪一行相关。接下来的值按照预测顺序排列，包括 `true` 值、类别 0 的 Shapley 值行总和以及类别 1 的 Shapley
    值总和。一旦我们提取了这些值，我们就创建一个 DataFrame 并将值存储在 `df` 中，然后通过 DataFrame 进行采样以查看五个值。我们使用随机种子以确保可重复性：
- en: '[PRE59]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'This generate the following output:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![Figure 8.16 – DataFrame displaying the Shapley values that made a mistake](img/B19297_08_16.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.16 – 显示造成错误的 Shapley 值的 DataFrame](img/B19297_08_16.jpg)'
- en: Figure 8.16 – DataFrame displaying the Shapley values that made a mistake
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.16 – 显示造成错误的 Shapley 值的 DataFrame
- en: For index `7915`, the Shapley values are close, meaning feature contributions
    to the model prediction are closer to `0` for each class, whereas for index `4255`,
    the Shapley values are far apart from `0` and features are discriminatory in predicting
    each class.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 对于索引 `7915`，Shapley 值很接近，这意味着每个类别的特征对模型预测的贡献更接近 `0`，而对于索引 `4255`，Shapley 值与
    `0` 相差较远，特征在预测每个类别时具有区分性。
- en: Given that we can extract the SHAP impact of features for each class, we want
    to know the rows where the Shapley value impact is highest so we can eliminate
    such data points from training; where the impact is low and quite close to the
    boundary, we can oversample the data.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以提取每个类别的特征 SHAP 影响值，我们想知道 Shapley 值影响最高的行，以便我们可以从训练中消除这样的数据点；影响低且接近边界的地方，我们可以对数据进行过采样。
- en: 'Looking at the force plots for index `4255`, for the expected class `0`, the
    model is likely to predict `1`, given that `f(x)` is quite low, and the model
    wrongly predicts `1`, whereas the force plot for the expected class `1` shows
    an `f(x)` value of `0.7`. Such data points can be eliminated from the dataset:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 观察索引`4255`的力图，对于预期的类`0`，由于`f(x)`相当低，模型可能会预测`1`，而模型错误地预测了`1`，而预期类`1`的力图显示了`f(x)`值为`0.7`。这样的数据点可以从数据集中删除：
- en: '[PRE60]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'This will generate the following plot:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![Figure 8.17 – Force plot for class 0](img/B19297_08_17.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![图8.17 – 类0的力图](img/B19297_08_17.jpg)'
- en: Figure 8.17 – Force plot for class 0
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17 – 类0的力图
- en: 'Let’s look at the force plot for class `1`:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看类`1`的力图：
- en: '[PRE61]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This will display the following output:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示以下输出：
- en: '![Figure 8.18 – Force plot for class 1](img/B19297_08_18.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图8.18 – 类1的力图](img/B19297_08_18.jpg)'
- en: Figure 8.18 – Force plot for class 1
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.18 – 类1的力图
- en: 'Now, we calculate the Shapley impact of row index `4255` since it’s a false
    positive prediction. The row index is at the `422` location in the DataFrame.
    We take the absolute value of the Shapley impact and, where the Shapley impact
    is highest and the prediction is wrong, those values can be eliminated to improve
    model performance:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算行索引`4255`的Shapley影响，因为它是一个假阳性预测。行索引在DataFrame的`422`位置。我们取Shapley影响的绝对值，并在Shapley影响最高且预测错误的地方，删除这些值以提高模型性能：
- en: '[PRE62]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Next, we create a function that calculates the Shapley impact. We are interested
    in those rows where a single feature has a minimum of `0.2` Shapley impact. First,
    we get the absolute impact of each feature in an array, and then we extract the
    maximum value. If the maximum value is greater than `0.2`, we proceed with that
    row. Next, we check where the prediction doesn’t match the actual value, and for
    such rows, we extract the SHAP impact:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个计算Shapley影响的函数。我们感兴趣的是那些单个特征具有至少`0.2` Shapley影响的行。首先，我们获取数组中每个特征的绝对影响，然后提取最大值。如果最大值大于`0.2`，我们继续处理该行。接下来，我们检查预测值与实际值不匹配的地方，并从这样的行中提取SHAP影响：
- en: '[PRE63]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We create a holdout dataset, where `X_train` will be further divided into training
    and validation datasets. We leverage 80% for training and 20% for validation:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个保留数据集，其中`X_train`将被进一步分为训练集和验证集。我们利用80%进行训练，20%进行验证：
- en: '[PRE64]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Then we resample the dataset using `SMOTETomek`, which was the best sampling
    method for fairness and performance, and performed by adding difficult examples
    back to the dataset. Once the dataset is resampled, we train the standard decision
    tree as in the previous steps, and calculate the ROC score on the holdout validation
    dataset:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`SMOTETomek`重新采样数据集，这是公平性和性能方面最好的采样方法，通过将困难示例重新添加到数据集中来实现。一旦数据集被重新采样，我们就按照前面的步骤训练标准决策树，并在保留的验证数据集上计算ROC分数：
- en: '[PRE65]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We calculate the fairness and performance metrics on the testing dataset. The
    equalized odds are high but the demographic parity ratio is within the accepted
    range:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在测试数据集上计算公平性和性能指标。均衡机会很高，但人口统计平等等比率在可接受范围内：
- en: '[PRE66]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Next, we calculate the fairness metrics:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算公平性指标：
- en: '[PRE67]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'This will print out the following metrics:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出以下指标：
- en: '![Figure 8.19 – Fairness metrics](img/B19297_08_19.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![图8.19 – 公平性指标](img/B19297_08_19.jpg)'
- en: Figure 8.19 – Fairness metrics
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.19 – 公平性指标
- en: 'We extract the Shapley values using the SHAP explainer. We have ensured that
    the `SEX` feature is removed:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用SHAP解释器提取Shapley值。我们确保已删除`SEX`特征：
- en: '[PRE68]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The fairness metrics demonstrate that the gap between false negative rates is
    higher between the subclass of men and women, hence we focus on reducing false
    negative cases for males using Shapley values.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性指标表明，男性和女性子类之间的假阴性率差距更大，因此我们专注于使用Shapley值减少男性的假阴性案例。
- en: 'In the next step, we extract Shapley values where the model predicts class
    `0,` but the `true` value is `1`. Hence, we are interested in Shapley values for
    class `1`, as a high SHAP impact for class `1` where the model made an error could
    be a data point that the model is unable to make a correct prediction on:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们提取模型预测为类`0`但真实值为`1`的Shapley值。因此，我们对类`1`的Shapley值感兴趣，因为在模型出错的地方，类`1`的SHAP影响高可能是一个模型无法正确预测的数据点：
- en: '[PRE69]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We are also interested in those data points where both Shapley values and model
    prediction agree with the actual values. Hence, we focus on those data points
    where the model rightly predicts class `0` for `male` data points. Once we have
    extracted those, we focus on high-impact Shapley values for class `0` so we can
    oversample the dataset with those, such that the model can get a better signal
    for such data points:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还对那些Shapley值和模型预测都与实际值一致的数据点感兴趣。因此，我们关注那些模型正确预测`男性`数据点的类别为`0`的数据点。一旦我们提取了这些数据点，我们就关注类别`0`的高影响力Shapley值，以便我们可以对这些数据进行过采样，从而使模型可以对这些数据点获得更好的信号：
- en: '[PRE70]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Next, we sort the false negative Shapley values so we can extract the high-impact
    data points:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们排序假阴性Shapley值，以便我们可以提取高影响力的数据点：
- en: '[PRE71]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Similar to the preceding, we are interested in true negative high-impact Shapley
    values, so we sort the list according to high-impact Shapley values:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面类似，我们感兴趣的是具有高影响力的真阴性Shapley值，因此我们根据高影响力的Shapley值对列表进行排序：
- en: '[PRE72]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Now that we have extracted and sorted the Shapley values for false negative
    and true negative `male` data points, we pick the top 100 data points to eliminate
    from the false negative list and pick the top 100 data points from the true negative
    list to add back to the training data. The top 100 data points from the true negative
    list will be shuffled and only 50 data points from there will be added at random
    with a replacement strategy. We encourage practitioners to try another ratio for
    shuffling. Once the data points are identified for elimination and reintroduction
    to the final training set, we update the training data. These are named `X_train_final`
    and `y_train_final`:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经提取并排序了假阴性和真阴性`男性`数据点的Shapley值，我们从假阴性列表中挑选前100个数据点进行消除，并从真阴性列表中挑选前100个数据点重新添加到训练数据中。真阴性列表中的前100个数据点将被随机打乱，并且只随机添加其中的50个数据点，采用替换策略。我们鼓励从业者尝试另一个打乱比例。一旦确定了用于消除和重新引入到最终训练集中的数据点，我们就更新训练数据。这些数据被命名为`X_train_final`和`y_train_final`：
- en: '[PRE73]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Next, we train the updated training data and calculate fairness metrics and
    performance metrics on the `test` data. It is evident that the gap between false
    negative rates has reduced, the equalized odds ratio has improved to 0.082, and
    the ROC score has slightly improved from the previous step, from 0.825 to 0.826:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们训练更新的训练数据，并在`测试`数据上计算公平性指标和性能指标。很明显，假阴性率之间的差距已经减少，均衡的几率比已提高到0.082，ROC分数从先前的0.825略微提高到0.826：
- en: '[PRE74]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We recalculate the fairness metrics:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重新计算公平性指标：
- en: '[PRE75]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The output is as follows:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 8.20 – Fairness metrics](img/B19297_08_20.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![图8.20 – 公平性指标](img/B19297_08_20.jpg)'
- en: Figure 8.20 – Fairness metrics
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.20 – 公平性指标
- en: Now that we have determined that by using Shapley values we can identify data
    points that are difficult to classify and easy to classify, we can build an automatic
    mechanism to iterate over the data points such that we can reach a better fairness
    score than previously.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经确定通过使用Shapley值可以识别难以分类且易于分类的数据点，我们可以构建一个自动机制来迭代数据点，从而可以达到比之前更好的公平性分数。
- en: 'Next, we create a range of percentages to iterate so we can leverage and sort
    through the top data points as a percentage of top data points to eliminate and
    reintroduce. We will leverage the NumPy `linspace` method to create a list of
    percentage values to iterate. We choose 10 values from `0.05` through `0.5` (5
    to 50 percent). We call this list `perc_points_to_eliminate`:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一系列百分比以进行迭代，这样我们就可以利用并排序消除和重新引入的前数据点的百分比。我们将使用NumPy的`linspace`方法创建一个百分比值的列表以进行迭代。我们从`0.05`到`0.5`（5%到50%）中选择10个值。我们称这个列表为`perc_points_to_eliminate`：
- en: '[PRE76]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: We iterate through these percentages and repeat the preceding step where we
    eliminated some values and reintroduced some values. However, this time, instead
    of 100, we use percentages to remove the top percent of data points or introduce
    the top percent of data points.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历这些百分比，并重复先前的步骤，其中我们消除了一些值并重新引入了一些值。然而，这次，我们使用百分比来移除顶部百分比的数据点或引入顶部百分比的数据点。
- en: We also create an empty list of data so, for each iteration, we capture the
    percentage of data points eliminated or reintroduced, false negative and false
    positive rates on test data, equalized odds ratio, and demographic parity ratio.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还创建了一个空的数据列表，因此，对于每次迭代，我们捕获消除或重新引入的数据点的百分比、测试数据中的假阴性率和假阳性率、均衡的几率比和人口比例比。
- en: 'Once we have iterated over all the values – 10*10 iterations, we store them
    in a DataFrame to see how many data points need to be removed and how many added
    to lead to the best fairness metrics:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们遍历了所有值 - 10*10 次迭代，我们将它们存储在一个 DataFrame 中，以查看需要移除多少数据点以及需要添加多少数据点才能达到最佳的公平性指标：
- en: '[PRE77]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Next, we create a DataFrame called `df_shapley` for the metadata for each iteration
    and sort it by equalized odds ratio:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个名为 `df_shapley` 的 DataFrame，用于存储每个迭代的元数据，并按均衡机会比进行排序：
- en: '[PRE78]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'This will output the following DataFrame:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下 DataFrame：
- en: '![Figure  8.21 – The df_shapley DataFrame after sorting](img/B19297_08_21.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.21 – 排序后的 df_shapley DataFrame](img/B19297_08_21.jpg)'
- en: Figure 8.21 – The df_shapley DataFrame after sorting
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.21 – 排序后的 df_shapley DataFrame
- en: It’s evident that when the top 25% of false negative data points are removed
    and 30% of the top true negative data points are reintroduced, the model can achieve
    an equalized odds ratio of 0.074 with an optimum demographic parity ratio score
    of 0.85.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，当移除顶部 25% 的误报数据点并重新引入顶部 30% 的真实负数据点时，模型可以达到均衡机会比 0.074，以及最佳人口统计平等等级比得分 0.85。
- en: 'Finally, we extract the top percentages and train the final model:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们提取顶部百分比并训练最终模型：
- en: '[PRE79]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'We train the model and calculate the fairness and model metrics. We can see
    that the false negative rate for `female` has increased but the gap between `male`
    and `female` has reduced, and the false positive rate for `male` has reduced.
    The ROC score achieved is 0.82, but the model is much fairer based on two fairness
    metrics:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练模型并计算公平性和模型指标。我们可以看到，对于 `female` 的误报率有所增加，但 `male` 和 `female` 之间的差距已经减小，而
    `male` 的误报率有所降低。ROC 得分达到 0.82，但根据两个公平性指标，模型更加公平：
- en: '[PRE80]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Finally, we calculate the fairness metrics.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算公平性指标。
- en: '[PRE81]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'This will print out the following:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出以下内容：
- en: '![Figure 8.22 – Fairness metrics](img/B19297_08_22.jpg)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.22 – 公平性指标](img/B19297_08_22.jpg)'
- en: Figure 8.22 – Fairness metrics
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.22 – 公平性指标
- en: Now that we have explored different data-centric techniques for reducing bias
    by improving data quality, we encourage you to experiment with the previous techniques
    and try a combination of these. Once you have exhausted these data-centric approaches,
    we encourage you use some model-centric approaches, such as utilizing algorithms
    that are fairness aware and trying ensembling methods, AutoML, or iterating through
    your own list of algorithms.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了通过提高数据质量来减少偏差的不同数据中心技术，我们鼓励您尝试之前的技巧，并尝试这些技巧的组合。一旦您用尽了这些数据中心方法，我们鼓励您使用一些模型中心方法，例如利用公平性感知的算法，尝试集成方法、AutoML
    或遍历您自己的算法列表。
- en: Summary
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter provided an extensive exploration of the pervasive challenge of
    bias in machine learning. It started by explaining various forms of bias inherent
    in machine learning models and examined their impact on different industries.
    The emphasis was on recognizing, monitoring, and mitigating bias, underscoring
    the importance of collecting data with minimal selection and sampling bias.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 本章广泛探讨了机器学习中普遍存在的偏差挑战。它从解释机器学习模型中固有的各种偏差形式开始，并检查了它们对各个行业的影响。重点是识别、监控和减轻偏差，强调收集具有最小选择和抽样偏差的数据的重要性。
- en: The central theme advocated a data-centric imperative over a model-centric one
    in addressing bias. Techniques such as oversampling, undersampling, feature selection
    enhancement, and anomaly detection were explored for bias rectification. Shapley
    values play a crucial role in bias identification, emphasizing the removal of
    examples with misaligned high Shapley values and the reintroduction of data points
    with replacement to improve ratios. Stratification of misclassified examples based
    on sensitive variables such as `SEX` was outlined for targeted bias correction.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 该主题倡导在解决偏差问题时，以数据为中心的强制措施优于以模型为中心的强制措施。探讨了过采样、欠采样、特征选择增强和异常检测等技术，用于偏差校正。Shapley
    值在偏差识别中起着至关重要的作用，强调移除具有不匹配高 Shapley 值的示例，并通过替换重新引入数据点以改善比率。根据敏感变量（如 `SEX`）对误分类示例进行分层，以实现有针对性的偏差校正。
- en: The chapter concluded by highlighting the significance of refining and balancing
    datasets concerning sensitive variables as a foundational step. It suggested progressing
    toward model-centric approaches, such as ensembling and fairness algorithms, once
    the dataset itself has been improved. These subsequent model-centric strategies
    aim to enhance both performance and fairness metrics, establishing a foundation
    for more generalized and equitable AI models.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过强调对敏感变量进行数据集精炼和平衡的重要性作为基础步骤。它建议在数据集本身得到改进后，转向以模型为中心的方法，例如集成和公平性算法。这些后续的以模型为中心的策略旨在提高性能和公平性指标，为更通用和公平的人工智能模型奠定基础。
- en: This comprehensive approach strives to create a balanced dataset as a precursor
    to applying model-centric techniques, promoting performance and fairness in AI
    systems.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 这种全面的方法旨在创建一个平衡的数据集，作为应用以模型为中心的技术的前奏，以促进人工智能系统中的性能和公平性。
