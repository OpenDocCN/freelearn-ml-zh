- en: Making Decisions with Trees
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 使用树做决策
- en: 'In this chapter, we are going to start by looking at our first supervised learning
    algorithm—decision trees. The decision tree algorithm is versatile and easy to
    understand. It is widely used and also serves as a building block for the numerous
    advanced algorithms that we will encounter later on in this book. In this chapter,
    we will learn how to train a decision tree and use it for either classification
    or regression problems. We will also understand the details of its learning process
    in order to know how to set its different hyperparameters. Furthermore, we will
    use a real-world dataset to apply what we are going to learn here in practice.
    We will start by getting and preparing the data and apply our algorithm to it.
    Along the way, we will also try to understand key machine learning concepts, such
    as cross-validation and model evaluation metrics. By the end of this chapter,
    you will have a very good understanding of the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章，我们将从查看我们的第一个监督学习算法——决策树开始。决策树算法多功能且易于理解。它被广泛使用，并且是我们在本书后续将遇到的许多高级算法的构建模块。在这一章中，我们将学习如何训练一个决策树，并将其应用于分类或回归问题。我们还将了解它的学习过程的细节，以便知道如何设置不同的超参数。此外，我们将使用一个现实世界的数据集，将我们在这里学到的内容付诸实践。我们将首先获取并准备数据，并将我们的算法应用于数据。在此过程中，我们还将尝试理解一些关键的机器学习概念，如交叉验证和模型评估指标。在本章结束时，你将对以下主题有非常好的理解：
- en: Understanding decision trees
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解决策树
- en: How do decision trees learn?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树是如何学习的？
- en: Getting a more reliable score
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取更可靠的分数
- en: Tuning the hyperparameters for higher accuracy
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整超参数以提高准确性
- en: Visualizing the tree's decision boundaries
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化树的决策边界
- en: Building decision tree regressors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建决策树回归器
- en: Understanding decision trees
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解决策树
- en: I chose to start this book with decision trees because I've noticed that the
    majority of new machine learning practitioners have previous experience in one
    of two fields—software development, or statistics and mathematics. Decision trees
    can conceptually resemble some of the concepts software developers are used to,
    such as nested `if-else` conditions and binary search trees. As for the statisticians,
    bear with me—soon, you will feel at home when we reach the chapter about linear
    models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择从决策树开始这本书，因为我注意到大多数新手机器学习从业者在两个领域中有之前的经验——软件开发或统计与数学。决策树在概念上可以类似于软件开发人员习惯的一些概念，例如嵌套的`if-else`条件和二叉搜索树。至于统计学家，忍耐一下——很快，当我们进入线性模型这一章时，你们会感到非常熟悉。
- en: What are decision trees?
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是决策树？
- en: 'I think the best way to explain what decision trees are is by showing the rules
    they generate after they are trained. Luckily, we can access those rules and print
    them. Here is an example of how decision tree rules look:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为解释决策树是什么的最佳方式是通过展示它们在训练后生成的规则。幸运的是，我们可以访问这些规则并将其打印出来。以下是决策树规则的一个例子：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As you can see, it''s basically a set of conditions. If the chance of rain
    falling is above `0.6` (60%), then I need to take an umbrella with me. If it is
    below `0.6`, then it all depends on the UV index. If the UV index is above `7`,
    then an umbrella is needed; otherwise, I will be fine without one. Now, you might
    be thinking <q>well, a few nested `if-else` conditions will do the trick.</q>
    True, but the main difference here is that I didn''t write any of these conditions
    myself. The algorithm just learned the preceding conditions automatically after
    it went through the following data:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，这基本上是一组条件。如果降雨的概率高于`0.6`（60%），那么我需要带伞。如果低于`0.6`，那么就取决于紫外线指数。如果紫外线指数高于`7`，那么需要带伞；否则，我没有伞也没关系。现在，你可能会想
    <q>好吧，几个嵌套的`if-else`条件就能解决这个问题。</q> 没错，但这里的主要区别是我并没有自己编写这些条件。算法在处理以下数据后，自动学会了这些前提条件：
- en: '![](img/cd239667-c242-4ff7-8ac2-2ced3473d72d.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd239667-c242-4ff7-8ac2-2ced3473d72d.png)'
- en: Of course, for this simple case, anyone can manually go through the data and
    come up with the same conditions. Nevertheless, when dealing with a bigger dataset,
    the number of conditions we need to program will quickly grow with the number
    of columns and the values in each column. At such a scale, it is not possible
    to manually perform the same job, and an algorithm that can learn the conditions
    from the data is needed.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '当然，对于这个简单的案例，任何人都可以手动查看数据并得出相同的条件。然而，在处理更大的数据集时，我们需要编程的条件数目会随着列数和每列中的值的增多而迅速增长。在这种规模下，手动完成相同的工作是不可行的，因此需要一个可以从数据中学习条件的算法。  '
- en: Conversely, it is also possible to map a constructed tree back to the nested
    `if-else` conditions. This means that you can use Python to build a tree from
    data, then export the underlying conditions to be implemented in a different language
    or even to put them in **Microsoft Excel** if you want.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '另一方面，也可以将构建的树映射回嵌套的`if-else`条件。这意味着你可以使用Python从数据中构建一棵树，然后将底层的条件导出，以便在其他语言中实现，甚至可以将它们放入**Microsoft
    Excel**中。  '
- en: Iris classification
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'Iris分类  '
- en: scikit-learn comes loaded with a number of datasets that we can use to test
    new algorithms. One of these datasets is the Iris set. Iris is a genus of 260–300
    species of flowering plants with showy flowers. However, in our dataset, just
    three species are covered—**Setosa**, **Versicolor**, and **Virginica**. Each
    example in our dataset has the length and the widths of the sepal and petal of
    each plant (the features), along with whether it is a Setosa, a Versicolor, or
    a Virginica (the target). Our task is to be able to identify the species of a
    plant given its sepal and petal dimensions. Clearly, this is a classification
    problem. It is a supervised learning problem since the targets are provided with
    the data. Furthermore, it is a classification problem since we take a limited
    number of predefined values (three species).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 'scikit-learn内置了许多数据集，我们可以用来测试新的算法。其中一个数据集是Iris数据集。Iris是一种有260到300个物种的开花植物属，具有显眼的花朵。然而，在我们的数据集中，只包含三种物种——**Setosa**、**Versicolor**和**Virginica**。数据集中的每个样本都有每株植物的萼片和花瓣的长度和宽度（特征），以及它是Setosa、Versicolor还是Virginica（目标）。我们的任务是根据植物的萼片和花瓣尺寸来识别其物种。显然，这是一个分类问题。由于数据中提供了目标，因此这是一个监督学习问题。此外，这是一个分类问题，因为我们有有限的预定义值（三种物种）。  '
- en: Loading the Iris dataset
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '加载Iris数据集  '
- en: 'Let''s now startby loading the dataset:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '现在让我们开始加载数据集：  '
- en: 'We import the dataset''s module from scikit-learn, and then load the Iris data
    into a variable, which we are going to call `iris` as well:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '我们从scikit-learn导入数据集模块，然后将Iris数据加载到一个变量中，我们也称之为`iris`：  '
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Using`dir`, we can see what methods and attributes the dataset provides:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '使用`dir`，我们可以查看数据集提供了哪些方法和属性：  '
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We get a list of the `DESCR`, `data`, `feature_names`, `filename`, `target`,
    and `target_names`methods.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了一些方法的列表，包括`DESCR`、`data`、`feature_names`、`filename`、`target`和`target_names`。
- en: It's nice of the data creators to provide descriptions with each one, which
    we can access using `DESCR`.*This is rarely the case with real-life data, however.
    Usually, in real life, we need to talk to the people who produced the data in
    the first place to understand what each value means, or at least use some descriptive
    statistics to understand the data before using it.*
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '数据创建者提供每个数据的描述是非常贴心的，我们可以通过`DESCR`访问它们。*然而，真实世界中的数据往往没有这么周到。通常，我们需要与数据的生产者进行沟通，才能理解每个值的含义，或者至少通过一些描述性统计来理解数据，然后再使用它。*  '
- en: '*3.  For now, let''s print the Iris data''s description:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*3.  现在，让我们打印出Iris数据集的描述：  '
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Have a look at the description now and try to think of some of the main takeaways
    from it. I will list my own takeaways afterward:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '现在查看描述并尝试思考一下从中得到的一些主要结论。我稍后会列出我的结论：  '
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This description holds some useful information for us, and I found the following
    points the most interesting:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '这个描述包含了一些有用的信息，我认为以下几点最为有趣：  '
- en: The data is composed of 150 rows (or 150 samples). This is a reasonably small
    dataset. Later on, we will see how to deal with this fact when evaluating our
    model.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '数据集由150行（或150个样本）组成。这是一个相当小的数据集。稍后，我们将看到如何在评估模型时处理这个事实。  '
- en: The class labels or targets take three values—`Iris-Setosa`, `Iris-Versicolor`,
    and `Iris-Virginica`. Some classification algorithms can only deal with two class
    labels; we call them binary classifiers. Luckily, the decision tree algorithm
    can deal with more than two classes, so we have no problems this time.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别标签或目标有三个值 —— `Iris-Setosa`、`Iris-Versicolor` 和 `Iris-Virginica`。一些分类算法只能处理两个类别标签；我们称它们为二元分类器。幸运的是，决策树算法可以处理多于两个类别，所以这次我们没有问题。
- en: The data is balanced; there are 50 samples for each class. This is something
    we need to keep in mind when training and evaluating our model later on.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是平衡的；每个类别有 50 个样本。这是我们在训练和评估模型时需要牢记的一点。
- en: We have four features—`sepal length`, `sepal width`, `petal length`, and `petal
    width`—and all four features are numeric. In [Chapter 3](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=26&action=edit),
    *Preparing Your Data*, we will learn how to deal with non-numeric data.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有四个特征 —— `sepal length`、`sepal width`、`petal length` 和 `petal width` —— 所有四个特征都是数值型的。在
    [第三章](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=26&action=edit)，*数据准备*，我们将学习如何处理非数值型数据。
- en: There are no missing attribute values. In other words, none of our samples contains
    null values. Later on in this book, we will learn how to deal with missing values
    if we encounter them.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有缺失的属性值。换句话说，我们的样本中没有空值。在本书的后续部分，如果遇到缺失值，我们将学习如何处理它们。
- en: The petal dimensions correlate with the class values more than the sepal dimensions.
    I wish we had never seen this piece of information. Understanding your data is
    useful, but the problem here is that this correlation is calculated for the entire
    dataset. Ideally, we will only calculate it for our training data. Anyway, let's
    ignore this information for now and just use it for a sanity check later on.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣尺寸与类别值的相关性比萼片尺寸更高。我希望我们从未看到这条信息。了解数据是有用的，但问题在于这种相关性是针对整个数据集计算的。理想情况下，我们只会为我们的训练数据计算它。无论如何，现在让我们暂时忽略这些信息，稍后再用它进行健全性检查。
- en: It's time to put all the dataset information into one DataFrame.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是时候将所有数据集信息放入一个 DataFrame 中了。
- en: The `feature_names`methodreturns the names of our features, while the `data`
    method returns their values in the form of a NumPy array. Similarly, the `target`variable
    has the values of the target in the form of zeros, ones, and twos, and `target_names`
    maps `0`, `1`, and `2` to`Iris-Setosa`, `Iris-Versicolor`, and `Iris-Virginica`,
    respectively.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`feature_names` 方法返回我们特征的名称，而 `data` 方法以 NumPy 数组的形式返回它们的值。同样，`target` 变量以零、一和二的形式返回目标的值，而
    `target_names` 则将 `0`、`1` 和 `2` 映射到 `Iris-Setosa`、`Iris-Versicolor` 和 `Iris-Virginica`。'
- en: NumPy arrays are efficient to deal with, but they do not allow columns to have
    names. I find column names to be useful for debugging purposes. I find `pandas`
    DataFrames to be more suitable here since we can use column names and combine
    the features and target into one DataFrame.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 数组在处理上是高效的，但它们不允许列具有名称。我发现列名在调试过程中非常有用。在这里，我认为 `pandas` 的 DataFrame 更加合适，因为我们可以使用列名将特征和目标组合到一个
    DataFrame 中。
- en: 'Here, we can see the first eight rows we get using`iris.data[:8]`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到使用 `iris.data[:8]` 得到的前八行数据：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following code uses the `data`, `feature_names`, and `target` methods to
    combine all the dataset information into one DataFrame and assign its column names
    accordingly:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用 `data`、`feature_names` 和 `target` 方法将所有数据集信息合并到一个 DataFrame 中，并相应地分配其列名：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: scikit-learn versions 0.23 and up support loading datasets as `pandas` DataFrames
    right away. You can do this by setting `as_frame=True` in `datasets.load_iris`
    and its similar data-loading methods. Nevertheless, this has not been tested in
    this book since version 0.22 is the most stable release at the time of writing.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 的版本 0.23 及更高版本支持将数据集直接加载为 `pandas` 的 DataFrame。您可以在 `datasets.load_iris`
    及其类似的数据加载方法中设置 `as_frame=True` 来实现这一点。然而，在写作时，这本书尚未测试过此功能，因为版本 0.22 是最稳定的版本。
- en: 'The `target` column now has the class IDs. However, for more clarity, we can
    also create a new column called `target_names`, where we can map our numerical
    target values to the class names:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`target` 列现在包含类别 ID。然而，为了更清晰起见，我们还可以创建一个名为 `target_names` 的新列，将我们的数值目标值映射到类别名称：'
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, let''s print a sample of six rows to see how our new DataFrame looks.
    Running the following code in a Jupyter notebook or a Jupyter lab will just print
    the contents of the DataFrame; otherwise, you need to surround your code with
    a `print` statement. I will assume that a Jupyter notebook environment is used
    in all later code snippets:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们打印六行样本来看看我们新创建的DataFrame是什么样子的。在Jupyter notebook或Jupyter lab中运行以下代码将直接打印DataFrame的内容；否则，你需要用`print`语句将代码包裹起来。我假设在所有后续的代码示例中都使用Jupyter
    notebook环境：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This gave me the following random sample:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我带来了以下随机样本：
- en: '![](img/4904624a-669a-4eb1-b625-1e3e15cf59b7.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4904624a-669a-4eb1-b625-1e3e15cf59b7.png)'
- en: The sample methods picked six random rows to display. This means that you will
    get a different set of rows each time you run the same code. Sometimes, we need
    to get the same random results every time we run the same code. Then, we use a
    pseudo-random number generator with a preset seed. A pseudo-random number generator
    initialized with the same seed will produce the same results every time it runs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 样本方法随机选择了六行来展示。这意味着每次运行相同的代码时，你将得到一组不同的行。有时，我们需要每次运行相同的代码时得到相同的随机结果。那么，我们就使用一个具有预设种子的伪随机数生成器。一个用相同种子初始化的伪随机数生成器每次运行时都会产生相同的结果。
- en: 'So, set the`random_state`parameter in the`sample()`method to `42`, as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，将`random_state`参数设置为`42`，如下所示：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You will get the exact same rows shown earlier.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到与之前展示的完全相同的行。
- en: Splitting the data
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据分割
- en: 'Let''s split the DataFrame we have just created into two—70% of the records
    (that is, 105 records) should go into the training set, while 30% (45 records)
    should go into testing. The choice of 70/30 is arbitrary for now. We will use
    the `train_test_split()` function provided by scikit-learn and specify`test_size`
    to be `0.3`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将刚刚创建的DataFrame分成两部分——70%的记录（即105条记录）应进入训练集，而30%（45条记录）应进入测试集。选择70/30的比例目前是任意的。我们将使用scikit-learn提供的`train_test_split()`函数，并指定`test_size`为`0.3`：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can use`df_train.shape[0]`and `df_test.shape[0]`to check how many rows thereare
    in the newly created DataFrames. We can also list the columns of the new DataFrames
    using `df_train.columns` and `df_test.columns`. They both have the same six columns:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`df_train.shape[0]`和`df_test.shape[0]`来检查新创建的DataFrame中有多少行。我们还可以使用`df_train.columns`和`df_test.columns`列出新DataFrame的列名。它们都有相同的六列：
- en: '`sepal length (cm)`'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sepal length (cm)`'
- en: '`sepal width (cm)`'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sepal width (cm)`'
- en: '`petal length (cm)`'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`petal length (cm)`'
- en: '`petal width (cm)`'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`petal width (cm)`'
- en: '`target`'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target`'
- en: '`target_names`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_names`'
- en: The first four columns are our features, while the fifth column is our target
    (or label). The sixthcolumn will not be needed for now. Visually, you could say
    that we have split our data vertically into training and test sets. Usually, it
    makes sense to further split each of our DataFrames horizontally into two parts—one
    part for the features, which we usually call *x*, and another part for the targets,
    which is usually called *y*. We will continue to use this *x* and *y* naming convention
    throughout the rest of this book.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 前四列是我们的特征，而第五列是我们的目标（或标签）。第六列目前不需要。直观地说，你可以说我们将数据在垂直方向上分成了训练集和测试集。通常，将我们的DataFrame在水平方向上进一步分成两部分是有意义的——一部分是特征，通常我们称之为*x*，另一部分是目标，通常称之为*y*。在本书的剩余部分，我们将继续使用这种*x*和*y*的命名约定。
- en: Some prefer to use a capital *X* to illustrate that it is a two-dimensional
    array (or DataFrames) and use a small letter for *y* when it is a single-dimensional
    array (or series). I find it more practical to stick to a single case.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人喜欢用大写的*X*来表示二维数组（或DataFrame），而用小写字母*y*表示一维数组（或系列）。我发现坚持使用单一大小写更为实用。
- en: 'As you know, the`feature_names` method in`iris` contains a list of the corresponding
    column names to our features. We will use this information, along with the `target`
    label, to create our *x* and *y* sets, as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，`iris`中的`feature_names`方法包含与我们的特征相对应的列名列表。我们将使用这些信息，以及`target`标签，来创建我们的*x*和*y*集合，如下所示：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Training the model and using it for prediction
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型并用于预测
- en: To get a feel for how everything works, we will train our algorithm using its
    default configuration for now. Later on in this chapter, I will explain the details
    of the decision tree algorithms and how to configure them.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解一切是如何运作的，我们现在将使用算法的默认配置进行训练。稍后在本章中，我将解释决策树算法的详细信息及如何配置它们。
- en: 'We need to import `DecisionTreeClassifier` first, and then create an instance
    of it, as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要导入`DecisionTreeClassifier`，然后创建它的实例，代码如下：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'One commonly used synonym for training is fitting. This is how an algorithm
    uses the training data (*x* and *y*) to learn its parameters. All scikit-learn
    models implement a`fit()`method that takes`x_train`and`y_train`, and `DecisionTreeClassifier`
    is no different:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的一个常用同义词是拟合。它是指算法如何利用训练数据（*x*和*y*）来学习其参数。所有的scikit-learn模型都实现了一个`fit()`方法，它接收`x_train`和`y_train`，`DecisionTreeClassifier`也不例外：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'By calling the `fit()` method, the `clf` instance is trained and ready to be
    used for predictions. We then call the`predict()`method on`x_test`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用`fit()`方法，`clf`实例被训练并准备好用于预测。接着我们在`x_test`上调用`predict()`方法：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: When predicting, we usually don't know the actual targets (*y*) for our features
    (*x*). That's why we only provide the `predict()` method here with`x_test`. In
    this particular case, we happened to know`y_test`; nevertheless, we will pretend
    that we don't know it for now, and only use it later on for evaluation. As our
    actual targets are called `y_test`, we will call the predicted ones `y_test_pred`
    and compare the two later on.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测时，我们通常不知道特征（*x*）的实际目标值（*y*）。这就是为什么我们在这里只提供`predict()`方法，并且传入`x_test`。在这个特定的情况下，我们恰好知道`y_test`；然而，为了演示，我们暂时假装不知道它，稍后再用它进行评估。由于我们的实际目标是`y_test`，我们将预测结果称为`y_test_pred`，并稍后进行比较。
- en: Evaluating our predictions
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估我们的预测
- en: As we have `y_test_predict`, all we need now is to compare it to`y_test` to
    check how good our predictions are. If you remember from the previous chapter,
    there are multiple metrics for evaluating a classifier, such as`precision`,`recall`,
    and`accuracy`. The Iris dataset is a balanced dataset; it has the same number
    of instances for each class. Therefore, it is apt to use the accuracy metric here.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有了`y_test_predict`，现在我们只需要将它与`y_test`进行比较，以检查我们的预测效果如何。如果你记得上一章，评估分类器有多种指标，比如`precision`、`recall`和`accuracy`。鸢尾花数据集是一个平衡数据集，每个类别的实例数相同。因此，在这里使用准确率作为评估指标是合适的。
- en: 'Calculating the accuracy, as follows, gives us a score of`0.91`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 计算准确率，结果如下，得分为`0.91`：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Did you get a different score than mine? Don't worry. In the *Getting a more
    reliable score* section, I will explain why the accuracy score calculated here
    may vary.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你的得分与我的不同吗？别担心。在*获取更可靠的得分*部分，我将解释为什么这里计算的准确率分数可能会有所不同。
- en: 'Congratulations! You''ve just trained your first supervised learning algorithm.
    From now on, all the algorithms we are going to use in this book have a similar
    interface:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你！你刚刚训练了你的第一个监督学习算法。从现在开始，本书中所有我们将使用的算法都有类似的接口：
- en: The `fit()` method takes the *x* and *y* parts of your training data.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fit()`方法接收你的训练数据的*x*和*y*部分。'
- en: The `predict()` method takes *x* only and returns a predicted *y*.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict()`方法只接收*x*并返回预测的*y*。'
- en: Which features were more important?
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 哪些特征更重要？
- en: 'We may now ask ourselves,*Which features did the model find more useful in
    deciding the iris species?* Luckily,`DecisionTreeClassifier` has a method called
    `feature_importances_`, which is computed after the classifier is fitted and scores
    how important each feature is to the model''s decision. In the following code
    snippet, we will create a DataFrames where we will put the features'' names and
    their importance together and then sort the features by their importance:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以问自己，*模型在决定鸢尾花种类时，认为哪些特征更有用？* 幸运的是，`DecisionTreeClassifier`有一个名为`feature_importances_`的方法，它会在分类器拟合后计算，并评估每个特征对模型决策的重要性。在以下代码片段中，我们将创建一个DataFrame，将特征名称和它们的重要性放在一起，然后按重要性对特征进行排序：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This is the output we get:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们得到的输出：
- en: '![](img/7886cc3b-3c70-4db3-ac76-e483c6f2140e.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7886cc3b-3c70-4db3-ac76-e483c6f2140e.png)'
- en: As you will recall, when we printed the dataset's description, the petal length
    and width values started to correlate highly with the target. They also have high
    feature importance scores here, which confirms what is stated in the description.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你会记得的，当我们打印数据集描述时，花瓣的长度和宽度值开始与目标变量高度相关。它们在这里也有很高的特征重要性分数，这验证了描述中的说法。
- en: Displaying the internal tree decisions
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 显示内部树的决策
- en: 'We can also print the internal structure of the learned tree using the following
    code snippet:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用以下代码片段打印学习到的树的内部结构：
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will print the following text:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印以下文本：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If you print the complete dataset description, you will notice that toward
    the end, it says the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打印出完整的数据集描述，你会注意到在最后，它写着以下内容：
- en: One class is linearly separable from the other two; the latter are NOT linearly
    separable from each other.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一个类别可以与其他两个类别线性分开；后者不能彼此线性分开。
- en: This means that one class is easier to separate from the other two, while the
    other two are harder to separate from each other. Now, look at the internal tree's
    structure. You may notice that in the first step, it decided that anything with
    a petal width below or equal to `0.8` belongs to class `0` (`Setosa`). Then, for
    petal widths above `0.8`, the tree kept on branching, trying to differentiate
    between classes `1` and `2` (`Versicolor` and `Virginica`). Generally, the harder
    it is to separate classes, the deeper the branching goes.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着一个类别比其他两个类别更容易被分开，而其他两个类别则更难相互分开。现在，看看内部树的结构。你可能会注意到，在第一步中，它决定将花瓣宽度小于或等于`0.8`的样本归类为类别`0`（`Setosa`）。然后，对于花瓣宽度大于`0.8`的样本，树继续分支，试图区分类别`1`和`2`（`Versicolor`和`Virginica`）。一般来说，类别之间分离越困难，分支就越深。
- en: How do decision trees learn?
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树是如何学习的？
- en: It's time to find out how decision trees actually learn in order to configure
    them. In the internal structure we just printed, the tree decided to use a petal
    width of `0.8` as its initial splitting decision. This was done because decision
    trees try to build the smallest possible tree using the following technique.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候了解决策树是如何学习的，以便配置它们。在我们刚刚打印的内部结构中，树决定使用`0.8`的花瓣宽度作为其初始分割决策。这是因为决策树试图使用以下技术构建尽可能小的树。
- en: It went through all the features trying to find a feature (`petal width`, here)
    and a value within that feature (`0.8`, here) so that if we split all our training
    data into two parts (one for `petal width ≤ 0.8`, and one for `petal width > 0.8`),
    we get the purest split possible. In other words, it tries to find a condition
    where we can separate our classes as much as possible. Then, for each side, it
    iteratively tries to split the data further using the same technique.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 它遍历所有特征，试图找到一个特征（此处为`花瓣宽度`）和该特征中的一个值（此处为`0.8`），这样如果我们将所有训练数据分成两部分（一个部分是`花瓣宽度
    ≤ 0.8`，另一个部分是`花瓣宽度 > 0.8`），我们就能得到最纯净的分割。换句话说，它试图找到一个条件，在这个条件下，我们可以尽可能地将类别分开。然后，对于每一边，它迭代地使用相同的技术进一步分割数据。
- en: Splitting criteria
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分割标准
- en: If we onlyhad two classes, an ideal split would put members of one class on
    one side and members of the others on the other side. In our case, we succeeded
    in putting members of class `0` on one side and members of classes `1` and `2`
    on the other. Obviously, we are not always guaranteed to get such a pure split.
    As we can see in the other branches further down the tree, we always had a mix
    of samples from classes `1` and `2` on each side.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只有两个类别，理想的分割应该将一个类别的成员放在一侧，另一个类别的成员放在另一侧。在我们的例子中，我们成功地将类别`0`的成员放在一侧，将类别`1`和`2`的成员放在另一侧。显然，我们并不总是能得到如此纯净的分割。正如我们在树的其他分支中看到的那样，每一侧总是混合了类别`1`和`2`的样本。
- en: Having said that, we need a way to measure purity. We need a criterion based
    on if one split is purer than the other. There are two criteria that scikit-learn
    uses for classifiers' purity—`gini` and `entropy`—with the `gini` criterion as
    its default option. When it comes to decision tree regression, there are other
    criteria that we will come across later on.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们需要一种衡量纯度的方法。我们需要一个标准来判断哪个分割比另一个更纯净。`scikit-learn`为分类器纯度提供了两个标准——`gini`和`entropy`——其中`gini`是默认选项。对于决策树回归，还有其他标准，我们稍后会接触到。
- en: Preventing overfitting
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 防止过拟合
- en: '"If you look for perfection, you''ll never be content."'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: “如果你追求完美，你将永远不会满足。”
- en: – Leo Tolstoy
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: – 列夫·托尔斯泰
- en: 'After the first split, the tree went on to try to separate between the remaining
    classes; the `Versicolor` and the `Virginica` irises. However, are we really sure
    that our training data is detailed enough to explain all the nuances that differentiate
    between the two classes? Isn''t it possible that all those branches are driving
    the algorithm to learn things that happen to exist in the training data, but will
    not generalize well enough when faced with future data? Allowing a tree to grow
    so much results in what is called overfitting. The tree tries to perfectly fit
    the training data, forgetting that the data it may encounter in the future may
    be different. To prevent overfitting, the following settings may be used to limit
    the growth of a tree:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次分裂后，树继续尝试区分剩下的类别；即`Versicolor`和`Virginica`鸢尾花。然而，我们真的确定我们的训练数据足够详细，能够解释区分这两类的所有细微差别吗？难道所有这些分支不是在引导算法学习一些仅存在于训练数据中的特征，而当面对未来数据时，它们并不会很好地泛化吗？让树生长过多会导致所谓的过拟合。树会尽力完美拟合训练数据，却忽视了未来可能遇到的数据可能会有所不同。为了防止过拟合，可以使用以下设置来限制树的生长：
- en: '`max_depth`:This is the maximum depth a tree can get to. A lower number means
    that the tree will stop branching earlier. Setting it to `None` means that the
    tree will continue to grow until all the leaves are pure or until all the leaves
    contain fewer than the `min_samples_split` samples.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：这是树可以达到的最大深度。较小的数字意味着树会更早停止分枝。将其设置为`None`意味着树会继续生长，直到所有叶节点都纯净，或直到所有叶节点包含的样本数少于`min_samples_split`。'
- en: '`min_samples_split`: The minimum number of samples needed in a level to allow
    further splitting there. A higher number means that the tree will stop branching
    earlier.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_split`：在一个层级中，允许进一步分裂所需的最小样本数。更高的数字意味着树会更早停止分枝。'
- en: '`min_samples_leaf`:**The minimum number of samples needed in a level to allow
    it to become a leaf node. A leaf node is a node where there are no further splits
    and where decisions are made. A higher number may have the effect of smoothing
    the model, especially in regression.**'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_leaf`：**允许成为叶节点的层级中所需的最小样本数。叶节点是没有进一步分裂的节点，是做出决策的地方。更高的数字可能会对模型产生平滑效果，尤其是在回归模型中。**'
- en: '**One quick way to check for overfitting is to compare the classifier''s accuracy
    on the test set to its accuracy on the training set. Having a much higher score
    for your training set compared to the test set is a sign of overfitting. A smaller
    and more pruned tree is recommended in this case.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**检查过拟合的一个快速方法是比较分类器在测试集上的准确度与在训练集上的准确度。如果训练集的得分明显高于测试集的得分，那就是过拟合的迹象。在这种情况下，推荐使用一个较小且修剪过的树。**'
- en: If `max_depth` is not set at training time to limit the tree's growth, then
    alternatively, you can prune the tree after it has been built. Curious readers
    can check the `cost_complexity_pruning_path()` method of the decision tree and
    find out how to use it to prune an already-grown tree.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在训练时没有设置`max_depth`来限制树的生长，那么在树构建后，你也可以修剪这棵树。有兴趣的读者可以查看决策树的`cost_complexity_pruning_path()`方法，了解如何使用它来修剪已经生长的树。
- en: Predictions
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测
- en: At the end of the training process, nodes that aren't split any further are
    called leaf nodes. Within a leaf node, we may have five samples—four of them from
    class `1`, one from class `2`, and none from class `0`. Then, at prediction time,
    if a sample ends up in the same leaf node, we can easily decide that the new sample
    belongs to class `1` since this leaf node had a 4:1 ratio of its training samples
    from class `1` compared to the other two classes.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程结束时，那些不再分裂的节点被称为叶节点。在叶节点内，我们可能有五个样本——其中四个来自类别`1`，一个来自类别`2`，没有来自类别`0`。然后，在预测时，如果一个样本最终落入相同的叶节点，我们可以轻松判断该新样本属于类别`1`，因为这个叶节点中的训练样本中有4:1的比例来自类别`1`，而其他两个类别的样本较少。
- en: When we make predictions on the test set, we can evaluate the classifier's accuracy
    versus the actual labels we have in the test set. Nevertheless, the manner in
    which we split our data may affect the reliability of the scores we get. In the
    next section, we will see how to get more reliable scores.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在测试集上进行预测时，我们可以评估分类器的准确度与我们在测试集中的实际标签之间的差异。然而，我们划分数据的方式可能会影响我们得到的分数的可靠性。在接下来的部分中，我们将看到如何获得更可靠的分数。
- en: Getting a more reliable score
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取更可靠的分数
- en: The Iris dataset is a small set of just 150 samples. When we randomly split
    it into training and test sets, we ended up with 45 instances in the test set.
    With such a small number, we may have variations in the distribution of our targets.
    For example, when I randomly split the data, I got 13 samples from class `0` and
    16 samples from each one of the two other classesin my test set. Knowing that
    predicting class `0` is easier than the other two classes in this particular dataset,
    we can tell that if I was luckier and had more samples of class `0` in the test
    set, I'd have had a higher score. Furthermore, decision trees are very sensitive
    to data changes, and you may get a very different tree with every slight change
    in your training data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花数据集是一个只有150个样本的小型数据集。当我们将其随机拆分为训练集和测试集时，测试集中最终有45个实例。由于样本量如此之小，我们可能会在目标的分布上看到一些变化。例如，当我随机拆分数据时，我在测试集中得到了13个类`0`的样本，以及从另外两个类中各得到16个样本。考虑到在这个特定数据集中，预测类`0`比其他两个类更容易，我们可以推测，如果我运气好一些，在测试集中有更多类`0`的样本，我的得分就会更高。此外，决策树对数据变化非常敏感，每次轻微变化训练数据时，你可能得到一棵完全不同的树。
- en: What to do now to get a more reliable score
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现在该做什么以获得更可靠的评分
- en: 'A statistician would say <q>let''s run the whole process of data splitting,
    training, and predicting, more than once, and get the distribution of the different
    accuracy scores we get each time</q>. The following code does exactly that for
    100 iterations:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学家会说：<q>让我们多次运行整个数据拆分、训练和预测的过程，并得到每次获得的不同准确度分数的分布</q>。以下代码正是实现了这一点，迭代了100次：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After importing the required modules and defining an `accuracy_scores` list
    to store the scores we are going get with each iteration, it is time to write
    a `for` loop to freshly split the data and recalculate the classifier''s accuracy
    with each iteration:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入所需模块并定义一个`accuracy_scores`列表来存储每次迭代的得分后，就该编写一个`for`循环来重新拆分数据，并在每次迭代时重新计算分类器的准确度：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following snippet lets us plot the accuracy''s distribution using a box
    plot:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段让我们通过箱型图绘制准确度的分布：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This will give us the following graphical analysis of the accuracy. Your results
    might vary slightlydue to the random split of the training and test sets and the
    random initial settings of the decision trees. Almost all of the scikit-learn
    modules support a pseudo-random number generator that can be initialized via a
    `random_state` hyperparameter. This can be used to enforce code reproducibility.
    Nevertheless, I deliberately ignored it this time to show how the model''s results
    may vary from one run to the other, and to show the importance of estimating the
    distributions of your models'' errors via iterations:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供以下准确度的图形分析。由于训练集和测试集的随机拆分以及决策树的随机初始设置，你的结果可能会略有不同。几乎所有的scikit-learn模块都支持一个伪随机数生成器，可以通过`random_state`超参数进行初始化。这可以用来确保代码的可重复性。然而，我这次故意忽略了它，以展示模型结果如何因运行而异，并强调通过迭代估计模型误差分布的重要性：
- en: '![](img/b2fcecc8-a3cc-43a5-85ab-bee4e8a38b41.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2fcecc8-a3cc-43a5-85ab-bee4e8a38b41.png)'
- en: Box plots are good at showing distributions. Rather than having a single number,
    we now have an estimation of the best- and the worst-case scenarios of our classifier's
    performance.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 箱型图在展示分布方面非常有效。与其只有一个数字，我们现在得到了对分类器性能的最佳和最差情况的估计。
- en: If, at any point, you do not have access to NumPy, you can still calculate a
    sample's mean and standard deviation using the `mean()` and `stdev()` methods
    provided by Python's built-in `statistics` module. It also provides functionalities
    for calculating the geometric and harmonic mean, as well as the median and quantiles.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在任何时候无法访问NumPy，你仍然可以使用Python内置的`statistics`模块提供的`mean()`和`stdev()`方法计算样本的均值和标准差。该模块还提供了计算几何平均数、调和平均数、中位数和分位数的功能。
- en: ShuffleSplit
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ShuffleSplit
- en: Generating different train and test splits is called cross-validation. This
    helps us have a more reliable estimation of our model's accuracy. What we did
    in the previous section is one of many cross-validation strategies called repeated
    random sub-sampling validation, or Monte Carlo cross-validation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 生成不同的训练和测试拆分被称为交叉验证。这帮助我们更可靠地估计模型的准确性。我们在上一节中所做的就是一种叫做重复随机子抽样验证（Monte Carlo交叉验证）的交叉验证策略。
- en: In probability theory, the law of large numbers states that if we repeat the
    same experiment a large number of times, the average of the results obtained should
    be close to the expected outcome. The Monte Carlo methods make use of random sampling
    in order to repeat an experiment over and over to reach better estimates for the
    results, thanks to the law of large numbers. The Monte Carlo methods were made
    possible due to the existence of computers, and here we use the same method to
    repeat the training/test split over and over to reach a better estimation of the
    model's accuracy.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率论中，大数法则指出，如果我们多次重复相同的实验，得到的结果的平均值应该接近预期结果。蒙特卡罗方法利用随机采样来不断重复实验，从而根据大数法则获得更好的结果估计。蒙特卡罗方法的实现得益于计算机的存在，在这里我们使用相同的方法来重复训练/测试数据拆分，以便获得更好的模型准确性估计。
- en: 'scikit-learn''s `ShuffleSplit` module provides us with the functionality to
    perform Monte Carlo cross-validation. Rather than us splitting the data ourselves,
    `ShuffleSplit` gives us lists of indices to use for splitting our data. In the
    following code, we are going to use the DataFrame''s `loc()` method and the indices
    we get from `ShuffleSplit`to randomly split the dataset into 100 training and
    test pairs:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 的 `ShuffleSplit` 模块提供了执行蒙特卡罗交叉验证的功能。我们无需自己拆分数据，`ShuffleSplit` 会为我们提供用于拆分数据的索引列表。在接下来的代码中，我们将使用
    DataFrame 的 `loc()` 方法和 `ShuffleSplit` 提供的索引来随机拆分数据集，生成 100 对训练集和测试集：
- en: '[PRE22]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Alternatively, we can simplify the preceding code even further by using scikit-learn's`cross_validate`**functionality.
    This time, we are not event splitting the data into training and test sets ourselves.
    We will give `cross_validate` the`x`and`y` values for the entire set, and then
    give it our `ShuffleSplit` instance for it to use internally to split the data.
    We also give it the classifier and specify what kind of scoring metric to use.
    When done, it will give us back a list with the calculated test set scores:**
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以通过使用 scikit-learn 的`cross_validate`**功能进一步简化之前的代码。这一次，我们甚至不需要自己将数据拆分为训练集和测试集。我们将
    `x` 和 `y` 的值传递给 `cross_validate`，并将 `ShuffleSplit` 实例传递给它以供内部使用，进行数据拆分。我们还将传递分类器并指定要使用的评分标准。完成后，它将返回一个包含计算出的测试集分数的列表：**
- en: '**[PRE23]'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**[PRE23]'
- en: We can plot the resulting series of accuracy scores now to get the same box
    plot as earlier. Cross-validation is recommended when dealing with a small dataset
    since a group of accuracy scores will give us a better understanding of the classifier's
    performance compared to a single score calculated after a single trial.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以绘制结果的准确性分数序列，得到与之前相同的箱型图。当处理小数据集时，推荐使用交叉验证，因为一组准确性分数能比单次实验后计算出的单个分数更好地帮助我们理解分类器的性能。
- en: Tuning the hyperparameters for higher accuracy
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整超参数以提高准确性
- en: 'Now that we have learned how to evaluate the model''s accuracy more reliably
    using the `ShuffleSplit` cross-validation method, it is time to test our earlier
    hypothesis: would a smaller tree be more accurate?'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何使用 `ShuffleSplit` 交叉验证方法更可靠地评估模型的准确性，接下来是检验我们之前的假设：更小的树是否更准确？
- en: 'Here is what we are going to do in the following sub sections:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将在接下来的子章节中进行的操作：
- en: Split the data into training and test sets.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集和测试集。
- en: Keep the test side to one side now.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在将测试集放在一边。
- en: Limit the tree's growth using different values of `max_depth`.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用不同的 `max_depth` 值限制决策树的生长。
- en: For each `max_depth` setting, we will use the `ShuffleSplit` cross-validation
    method on the training set to get an estimation of the classifier's accuracy.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个 `max_depth` 设置，我们将使用 `ShuffleSplit` 交叉验证方法在训练集上获取分类器的准确性估计。
- en: Once we decide which value to use for `max_depth`, we will train the algorithm
    one last time on the entire training set and predict on the test set.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们决定了要使用的 `max_depth` 值，我们将最后一次在整个训练集上训练算法，并在测试集上进行预测。
- en: Splitting the data
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拆分数据
- en: 'Here is the usual code for splitting the data into training and test sets:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是将数据拆分为训练集和测试集的常用代码：
- en: '[PRE24]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Trying different hyperparameter values
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尝试不同的超参数值
- en: 'If we allowed our earlier treeto grow indefinitely, we would get a tree depth
    of `4`. You can check the depth of a tree by calling`clf.get_depth()`once it is
    trained. So, it doesn''t make sense to try any `max_depth` values above `4`. Here,
    we are going to loop over the maximum depths from `1` to `4` and use `ShuffleSplit`
    to get the classifier''s accuracy:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们允许之前的树无限生长，我们会得到一个深度为`4`的树。你可以通过调用`clf.get_depth()`来检查树的深度，一旦它被训练好。所以，尝试任何大于`4`的`max_depth`值是没有意义的。在这里，我们将循环遍历从`1`到`4`的最大深度，并使用`ShuffleSplit`来获取分类器的准确度：
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We called the `cross_validate()` method as we did earlier, giving it the classifier's
    instance, as well as the `ShuffleSplit` instance. We also defined our evaluation
    score as `accuracy`. Finally, we print the scores we get with each iteration.
    We will look more at the printed values in the next section.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像之前一样调用了`cross_validate()`方法，传入了分类器的实例和`ShuffleSplit`实例。我们还将评估分数定义为`accuracy`。最后，我们打印出每次迭代得到的得分。在下一节中，我们将更详细地查看打印的值。
- en: Comparing the accuracy scores
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较准确度得分
- en: Since we have a list of scores for each iteration, we can calculate their mean,
    or, as we will do here, we will print their 10^(th) and 90^(th) percentiles to
    get an idea of the accuracy ranges versus each `max_depth`setting.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有每次迭代的得分列表，我们可以计算它们的平均值，或者像我们这里做的那样，打印它们的第10百分位和第90百分位，以了解每个`max_depth`设置下的准确度范围。
- en: 'Running the preceding code gave me the following results:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码给出了以下结果：
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: One thing I am sure about now is that a single-level tree (usually called a
    stub) is not as accurate as deeper trees. In other words, having a single decision
    based on whether the petal width is less than `0.8` is not enough. Allowing the
    tree to grow further improves the accuracy, but I can't see many differences between
    trees of depths `2`, `3`, and `4`. I'd conclude that contrary to my earlier speculations,
    we shouldn't worry too much about overfitting here.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我现在确定的一点是，单层树（通常称为stub）的准确度不如深层树。换句话说，仅根据花瓣宽度是否小于`0.8`来做出决策是不够的。允许树进一步生长会提高准确度，但我看不出深度为`2`、`3`和`4`的树之间有太大的差异。我得出结论，与我之前的猜测相反，在这里我们不必过于担心过拟合问题。
- en: Here, we tried different values for a single parameter, `max_depth`. That's
    why a simple `for` loop over its different values was feasible. In later chapters,
    we will see what to do when we need to tune multiple hyperparameters at once to
    reach a combination that gives the best accuracy.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们尝试了不同的单一参数值，`max_depth`。因此，简单地对其不同值使用`for`循环是可行的。在后续章节中，我们将学习当需要同时调整多个超参数以找到最佳准确度组合时该如何处理。
- en: Finally, you can train your model once more using the entire training set and
    a `max_depth` value of, say, `3`. Then, use the trained model to predict the classes
    for the test set in order to evaluate your final model. I won't bore you with
    the code for it this time as you can easily do it yourself.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以再次使用整个训练集和一个`max_depth`值，例如`3`来训练你的模型。然后，使用训练好的模型预测测试集的类别，以评估最终模型。这次我不会再赘述代码部分，因为你完全可以自己轻松完成。
- en: In addition to printing the classifier's decision and descriptive statistics
    about its accuracy, it is useful to also see its decision boundaries visually.
    Mapping those boundaries versus the data samples helps us understand why the classifier
    made certain mistakes. In the next section, we are going to check the decision
    boundaries we got for the Iris dataset.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 除了打印分类器的决策和其准确度的描述性统计数据外，查看分类器的决策边界也是非常有用的。将这些边界与数据样本进行映射有助于我们理解为什么分类器会做出某些错误决策。在下一节中，我们将检查我们在鸢尾花数据集上得到的决策边界。
- en: Visualizing the tree's decision boundaries
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化树的决策边界
- en: 'To be able to pick the right algorithm for the problem, it is important to
    have a conceptual understanding of how an algorithm makes its decision. As we
    already know by now, decision trees pick one feature at a time and try to split
    the data accordingly. Nevertheless, it is important to be able to visualize those
    decisions as well. Let me first plot our classes versus our features, then I will
    explain further:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够为问题选择正确的算法，理解算法如何做出决策是非常重要的。正如我们现在已经知道的，决策树一次选择一个特征，并试图根据这个特征来划分数据。然而，能够可视化这些决策也同样重要。让我先绘制我们的类别与特征的关系图，然后再进一步解释：
- en: '![](img/5c2e9fe5-3635-4869-968b-cca3102ef337.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c2e9fe5-3635-4869-968b-cca3102ef337.png)'
- en: When the tree made a decision to split the data around a petal width of `0.8`,
    you can think of it as drawing a horizontal line in the right-hand side graph
    at the value of `0.8`. Then, with every later split, the tree splits the space
    further using combinations of horizontal and vertical lines. By knowing this,
    you should not expect the algorithm to use curves or 45-degree lines to separate
    the classes.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当树决定以`0.8`的花瓣宽度将数据分割时，可以将其视为在右侧图表上画一条水平线，值为`0.8`。然后，每一次后续的分割，树将继续使用水平和垂直线的组合进一步划分空间。了解这一点后，你就不应该期待算法使用曲线或45度的线来分隔类别。
- en: 'One trick to plot the decision boundaries that a tree has after it has been
    trained is to use contour plots. For simplicity, let''s assume we only have two
    features—petal length and petal width. We then generate almost all the possible
    values for those two features and predict the class labels for our new hypothetical
    data. Then, we create a contour plot using those predictions to see the boundaries
    between the classes. The following function, created by Richard Johanssonof the
    University of Gothenburg, does exactly that:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制树训练后决策边界的一个技巧是使用等高线图。为了简化，假设我们只有两个特征——花瓣长度和花瓣宽度。我们接着生成这两个特征的几乎所有可能值，并预测新假设数据的类别标签。然后，我们使用这些预测创建等高线图，以查看类别之间的边界。以下函数，由哥德堡大学的理查德·约翰松（Richard
    Johansson）创建，正是完成这个工作的：
- en: '[PRE27]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This time, we will train our classifier using two features only, and then call
    the preceding function using the newly trained model:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们将仅使用两个特征训练分类器，然后使用新训练的模型调用前面的函数：
- en: '[PRE28]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Richard Johansson''s functions overlay the contour graph over our samples to
    give us the following graph:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 理查德·约翰松的函数将等高线图叠加到我们的样本上，从而生成以下图表：
- en: '![](img/cb06f70a-1971-4b6c-af62-8b99a2cf0943.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb06f70a-1971-4b6c-af62-8b99a2cf0943.png)'
- en: By seeing the decision boundaries as well as the data samples, you can make
    better decisions on whether one algorithm is good for the problem at hand.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看决策边界以及数据样本，你可以更好地判断一个算法是否适合当前的问题。
- en: Feature engineering
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程
- en: '"Every man takes the limits of his own field of vision for the limits of the
    world."'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: “每个人将自己视野的边界当作世界的边界。”
- en: – Arthur Schopenhauer
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: – 阿图尔·叔本华
- en: 'On seeing the class distribution versus the petal lengths and widths, you may
    wonder: *what if the decision trees could also draw boundaries that are at 40
    degrees? Wouldn''t 40-degree boundaries be more apt than those horizontal and
    vertical jigsaws?*Unfortunately, decision trees cannot do that, but let''s put
    the algorithm aside for a moment and think about the data instead. How about creating
    a new axis where the class boundaries change their orientation?'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看花瓣长度和宽度与类别分布之间的关系，你可能会想：*如果决策树也能绘制40度的边界呢？40度的边界是不是比那些水平和垂直的拼图更合适呢？* 不幸的是，决策树做不到这一点，但我们暂时放下算法，转而思考数据本身。怎么样，如果我们创建一个新的轴，让类别边界改变它们的方向呢？
- en: 'Let''s create two new columns—`petal length x width (cm)` and `sepal length
    x width (cm)`—and see how the class distribution will look:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建两个新列——`花瓣长度 x 宽度 (cm)`和`萼片长度 x 宽度 (cm)`——看看类别分布会是什么样子：
- en: '[PRE29]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following code will plot the classes versus the newly derived features:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将绘制类别与新生成特征之间的关系：
- en: '[PRE30]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Running this code will produce the following graph:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码将生成以下图表：
- en: '![](img/64d6b59e-2a67-4eb3-9178-a5e76bc876de.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64d6b59e-2a67-4eb3-9178-a5e76bc876de.png)'
- en: This new projection looks better; it makes the data more vertically separable.
    Nevertheless, the proof of the pudding is still in the eating. So, let's train
    two classifiers—one on the original features and one on the newly derived features—and
    see
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的投影看起来更好，它使数据在垂直方向上更加可分离。不过，结果还是要看实际效果。所以，我们训练两个分类器——一个使用原始特征，另一个使用新生成的特征——来看看结果如何。
- en: 'how their accuracies compare. The following code goes through 500 iterations,
    each time splitting the data randomly, and then training both models, each with
    its own set of features, and storing the accuracy we get with each iteration:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的准确率如何比较。以下代码将执行500次迭代，每次随机分割数据，然后训练两个模型，每个模型使用不同的特征集，并存储每次迭代得到的准确率：
- en: '[PRE31]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, we can use box plots to compare the accuracies of the two classifiers:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用箱线图来比较两个分类器的准确率：
- en: '[PRE32]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here, we put the top plots side by side to be able to compare them to each
    other:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将顶部的图表并排放置，以便相互比较：
- en: '![](img/9df33baf-3354-44c7-bc30-e30c7c988f9f.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9df33baf-3354-44c7-bc30-e30c7c988f9f.png)'
- en: Clearly, the derived features helped a bit. Its accuracy is higher on average
    (`0.96` versus `0.93`), and its lower bound is also higher.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，所得到的特征有所帮助。它的准确度平均更高（`0.96`对比`0.93`），并且它的下限也更高。
- en: Building decision tree regressors
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建决策树回归器
- en: Decision tree regressors work in a similar fashion to their classifier counterparts.
    The algorithm splits the data recursively using one feature at a time. At the
    end of the process, we end up with leaf nodes—that is, nodes where there are no
    further splits. In the case of a classifier, if, at training time, a leaf node
    has three instances of class `A` and one instance of class `B`, then at prediction
    time, if an instance lands in the same leaf node, the classifier decides that
    it belongs to the majority class (class `A`). In the case of a regressor, if,
    at training time, a leaf node has three instances of values `12`, `10`, and `8`,then,
    at prediction time, if an instance lands in the same leaf node, the regressor
    predicts its value to be `10` (the average of the three values at training time).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树回归器的工作方式与其分类器版本类似。该算法递归地使用一个特征进行数据分割。最终，我们会得到叶节点——即没有进一步分裂的节点。对于分类器来说，如果在训练时，一个叶节点有三个属于类别`A`的实例和一个属于类别`B`的实例，那么在预测时，如果一个实例落入该叶节点，分类器会判定它属于多数类别（类别`A`）。对于回归器来说，如果在训练时，一个叶节点有三个值`12`、`10`和`8`，那么在预测时，如果一个实例落入该叶节点，回归器会预测它的值为`10`（即训练时三个值的平均值）。
- en: Actually, picking the average is not always the best case. It rather depends
    on the splitting criterion used. In the next section, we are going to see this
    in more detail with the help of an example.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，选择平均值并不总是最佳的情况。它实际上取决于所使用的分裂标准。在下一节中，我们将通过一个例子来更详细地了解这一点。
- en: Predicting people's heights
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测人们的身高
- en: Say we have two populations. Population `1` has an average height of 155 cm
    for females, with a standard deviation of `4`, and an average height of 175 cm
    for males, with a standard deviation of `5`. Population 2 has an average height
    of 165 cm for females, with a standard deviation of `15`, and an average height
    of 185 cm for males, with a standard deviation of `12`. We decide to take 200
    males and 200 females from each population. To be able to simulate this, we can
    use a function provided by NumPy that draws random samples from a normal (Gaussian)
    distribution.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个群体。群体`1`中，女性的平均身高为155厘米，标准差为`4`，男性的平均身高为175厘米，标准差为`5`。群体2中，女性的平均身高为165厘米，标准差为`15`，男性的平均身高为185厘米，标准差为`12`。我们决定从每个群体中各取200名男性和200名女性。为了模拟这一点，我们可以使用NumPy提供的一个函数，从正态（高斯）分布中抽取随机样本。
- en: 'Here is the code for generating random samples:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是生成随机样本的代码：
- en: '[PRE33]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'At the moment, we don''t actually care about which population each sample comes
    from. So, we will use`concatenate`to group all the males and all the females together:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们实际上并不关心每个样本来自哪个群体。因此，我们将使用`concatenate`将所有男性和所有女性合并在一起：
- en: '[PRE34]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We then put this data into a DataFrame (`df_height`) to be able to deal with
    it easily. There, we also give a label of `1` to females and `2` to males:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这些数据放入一个DataFrame（`df_height`）中，以便更容易处理。在这里，我们还将女性标记为`1`，男性标记为`2`：
- en: '[PRE35]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let''s plot our fictional data using histograms to see the height distributions
    among each gender:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用直方图绘制我们虚构的数据，以查看每个性别的身高分布：
- en: '[PRE36]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The preceding code gives us the following graph:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码给我们生成了以下图表：
- en: '![](img/f1e0d999-9406-45ad-ac4b-8ec91eff3917.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1e0d999-9406-45ad-ac4b-8ec91eff3917.png)'
- en: 'As you can see, the resulting distributions are notsymmetrical. Although normal
    distributions are symmetrical, these artificial distributions are made of two
    sub-distributions combined. We can use this line of code to see that their mean
    and median values are not equal:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，得到的分布并不对称。尽管正态分布是对称的，但这些人工分布是由两个子分布组合而成。我们可以使用这行代码查看它们的均值和中位数不相等：
- en: '[PRE37]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Here, we have the mean and median heights for each group:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是每个群体的均值和中位数身高：
- en: '![](img/c33b936d-270e-47fd-816a-13413360c83c.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c33b936d-270e-47fd-816a-13413360c83c.png)'
- en: 'Now, we want to predict people''s heights using one feature—their gender. Therefore,
    we are going to split our data into training and test sets and create our *x*
    and *y* sets, as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想要通过一个特征——性别来预测人们的身高。因此，我们将数据分为训练集和测试集，并创建我们的*x*和*y*集，具体如下：
- en: '[PRE38]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Remember that in the case of classifications, the trees use either `gini` or
    `entropy` to decide the best split at each step during the training process. The
    goal for these criteria was to find a split where each of the two resulting sub-groups
    is as pure as possible. In the case of regression, we have a different goal. We
    want the members of each group to have target values that are as close as possible
    to the predictions they make. scikit-learn implements two criteria to achieve
    this goal:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在分类问题中，决策树使用`gini`或`entropy`来决定训练过程中每一步的最佳划分。这些标准的目标是找到一个划分，使得结果的两个子组尽可能纯净。在回归问题中，我们的目标不同。我们希望每个组的成员目标值尽可能接近它们所做出的预测值。scikit-learn实现了两种标准来达到这个目标：
- en: '**Mean squared error** (**MSE or L2**):Say after the split, we get three samples
    in one group with targets of `5`, `5`, and `8`. We calculate the mean value of
    these three numbers (`6`). Then, we calculate the squared differences between
    each sample and the calculated mean—`1`, `1`, and `4`. We then take the mean of
    these squared differences, which is `2`.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差** (**MSE 或 L2**)：假设划分后，我们得到三组样本，其目标值为`5`、`5`和`8`。我们计算这三个数字的均值（`6`）。然后，我们计算每个样本与计算得到的均值之间的平方差——`1`、`1`和`4`。接着，我们计算这些平方差的均值，即`2`。'
- en: '**Mean absolute error** (**MAE or L1**): Say after the split, we get three
    samples in one group with targets of `5`, `5`, and `8`. We calculate the median
    value of these three numbers (`5`). Then, we calculate the absolute differences
    between each sample and the calculated median—`0`, `0`, and `3`. We then take
    the mean of these absolute differences, which is `1`.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均绝对误差** (**MAE 或 L1**)：假设划分后，我们得到三组样本，其目标值为`5`、`5`和`8`。我们计算这三个数字的中位数（`5`）。然后，我们计算每个样本与计算得到的中位数之间的绝对差值——`0`、`0`和`3`。接着，我们计算这些绝对差值的均值，即`1`。'
- en: For each possible split at training time, the tree calculates either L1 or L2
    for each of the expected sub-groups after the split. A split with the minimum
    L1 or L2 is then chosen at this step. L1 may be preferred sometimes due to its
    robustness to outliers. The other important difference to keep in mind is that
    L1 uses median while L2 uses mean in its calculations.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时，对于每一个可能的划分，决策树会计算每个预期子组的L1或L2值。然后，在这一阶段，选择具有最小L1或L2值的划分。由于L1对异常值具有鲁棒性，因此有时会优先选择L1。需要注意的另一个重要区别是，L1在计算时使用中位数，而L2则使用均值。
- en: If, at training time, we see 10 samples with almost identical features but different
    targets, they may all end up together in one leaf node. Now, if we use L1 as the
    splitting criterion when building our regressor, then if we get a sample at prediction
    time with identical features to the 10 training samples, we should expect the
    prediction to be close to the median value of the targets of the 10 training samples.
    Likewise, if L2 is used for building the regressor, we should then expect the
    prediction of the new sample to be close to the mean value of the targets of the
    10 training samples.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在训练时，我们看到10个样本具有几乎相同的特征，但目标值不同，它们可能最终会被分到同一个叶节点中。现在，如果我们在构建回归模型时使用L1作为划分标准，那么如果我们在预测时得到一个特征与这10个训练样本相同的样本，我们应该期望该预测值接近这10个训练样本目标值的中位数。同样，如果使用L2来构建回归模型，我们应该期望新样本的预测值接近这10个训练样本目标值的均值。
- en: 'Let''s now compare the effect of the splitting criteria on our height dataset:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们比较划分标准对身高数据集的影响：
- en: '[PRE39]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We get the following two trees depending on the chosen criterion:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 根据选择的标准，我们得到以下两棵树：
- en: '[PRE40]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As expected, when MSE was used, the predictions were close to the mean of each
    gender, while for MAE, the predictions were close to the median.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，当使用MSE时，预测值接近每个性别的均值，而使用MAE时，预测值接近中位数。
- en: Of course, we only had one binary feature in our dataset—gender. That's why
    we had a very shallow tree with a single split (a **stub**). Actually, in this
    case, we do not even need to train a decision tree; we could have easily calculated
    the mean heights for males and females and used them as our expected values right
    away. The decisions made by such a shallow tree are called biased decisions. If
    we would have allowed each individual to express themselves using more information,
    rather than just their gender, then we would have been able to make more accurate
    predictions for each individual.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们的数据集中只有一个二元特征——性别。这就是为什么我们得到了一棵非常浅的树，只有一个分裂（一个**存根**）。实际上，在这种情况下，我们甚至不需要训练决策树；我们完全可以直接计算男性和女性的平均身高，并将其作为预期值来使用。由这样一个浅层树做出的决策被称为偏倚决策。如果我们允许每个个体使用更多的信息来表达自己，而不仅仅是性别，那么我们将能够为每个个体做出更准确的预测。
- en: Finally, just as in the classification trees, we have the same knobs, such as
    `max_depth`, `min_samples_split`, and `min_samples_leaf`**,** to control the growth
    of a regression tree.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，就像分类树一样，我们也有相同的控制参数，例如`max_depth`、`min_samples_split`和`min_samples_leaf`**，**用于控制回归树的生长。
- en: Regressor's evaluation
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归模型的评估
- en: 'The very same MSE and MAE scores can also be used to evaluate a regressor''s
    accuracy. We use them to compare the regressor''s predictions to the actual targets
    in the test set. Here is the code predicting and evaluating the predictions made:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的MSE和MAE分数也可以用来评估回归模型的准确性。我们使用它们将回归模型的预测与测试集中的实际目标进行比较。以下是预测并评估预测结果的代码：
- en: '[PRE41]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Using MSE as a splitting criterion gives us an MSE of `117.2` and an MAE of
    `8.2`, while using MAE as a splitting criterion gives us an MSE of `123.3`and
    an MAE of `7.8`. Clearly, using MAE as the splitting criterion gives a lower MAE
    at test time, and vice versa. In other words, if your aim is to reduce the error
    of your predictions based on a certain metric, it is advised to use the same metric
    when growing your tree at the time of training.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用均方误差（MSE）作为分裂标准时，我们得到的MSE为`117.2`，MAE为`8.2`，而使用绝对误差（MAE）作为分裂标准时，MSE为`123.3`，MAE为`7.8`。显然，使用MAE作为分裂标准在测试时给出了更低的MAE，反之亦然。换句话说，如果你的目标是基于某个指标减少预测误差，建议在训练时使用相同的指标来生长决策树。
- en: Setting sample weights
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置样本权重
- en: 'Both the decision tree classifiers and the regressors allow us to give more
    or less emphasis to the individual training samples via setting their weights
    while fitting. This is a common feature in many estimators, and decision trees
    are no exception here. To see the effect of sample weights, we are going to give
    10 times more weight to users above 150 cm versus the remaining users:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是决策树分类器还是回归器，都允许我们通过设置训练样本的权重，来对个别样本赋予更多或更少的权重。这是许多估计器的共同特性，决策树也不例外。为了查看样本权重的效果，我们将给身高超过150厘米的用户赋予10倍的权重，与其他用户进行对比：
- en: '[PRE42]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Conversely, we can also give more weights to users who are 150 cm and below
    by changing the `sample_weight` calculations, as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，我们也可以通过修改`sample_weight`计算，为身高150厘米及以下的用户赋予更多的权重，如下所示：
- en: '[PRE43]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: By using the `export_text()` function, as we did in the previous section, we
    can display the resulting trees. We can see how `sample_weight`**affected their
    final structures:**
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`export_text()`函数，正如我们在前一节中所做的，我们可以显示结果树。我们可以看到`sample_weight`**如何影响它们的最终结构：**
- en: '**[PRE44]'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**[PRE44]**'
- en: By default, all samples are given the same weight. Weighting individual samples
    differently is useful when dealing with imbalanced data or imbalanced business
    decisions; maybe you can tolerate delaying a shipment for a new customer more
    than you can do for your loyal ones. In [Chapter 8](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=30&action=edit),
    *Ensembles – When One Model Is Not Enough*, we will also see how sample weights
    are an integral part of how the AdaBoost algorithm learns.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，所有样本被赋予相同的权重。对单个样本赋予不同的权重在处理不平衡数据或不平衡的商业决策时非常有用；也许你可以更容忍对新客户延迟发货，而对忠实客户则不能。在[第8章](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=30&action=edit)中，*集成方法——当一个模型不够时*，我们将看到样本权重是AdaBoost算法学习的核心部分。
- en: Summary
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Decision trees are intuitive algorithms that are capable of performing classification
    and regression tasks. They allow users to print out their decision rules, which
    is a plus when communicating the decisions you made to business personnel and
    non-technical third parties. Additionally, decision trees are easy to configure
    since they have a limited number of hyperparameters. The two main decisions you
    need to make when training a decision tree are your splitting criterion and how
    to control the growth of your tree to have a good balance between *overfitting*
    and *underfitting*. Your understanding of the limitations of the tree's decision
    boundaries is paramount in deciding whether the algorithm is good enough for the
    problem at hand.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是直观的算法，能够执行分类和回归任务。它们允许用户打印出决策规则，这对于向业务人员和非技术人员传达你所做的决策非常有利。此外，决策树易于配置，因为它们的超参数数量有限。在训练决策树时，你需要做出的两个主要决定是：选择你的划分标准以及如何控制树的生长，以在*过拟合*和*欠拟合*之间取得良好的平衡。你对树的决策边界局限性的理解，在决定算法是否足够适应当前问题时至关重要。
- en: In this chapter, we looked at how decision trees learn and used them to classify
    a well-known dataset. We also learned about the different evaluation metrics and
    how the size of our data affects our confidence in a model's accuracy. We then
    learned how to deal with the evaluation's uncertainties using different data-splitting
    strategies. We saw how to tune the algorithm's hyperparameters for a good balance
    between overfitting and underfitting. Finally, we built on the knowledge we gained
    to build decision tree regressors and learned how the choice of a splitting criterion
    affects our resulting predictions.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了决策树如何学习，并使用它们对一个著名的数据集进行分类。我们还学习了不同的评估指标，以及数据的大小如何影响我们对模型准确性的信心。接着，我们学习了如何使用不同的数据分割策略来应对评估中的不确定性。我们看到如何调整算法的超参数，以在过拟合和欠拟合之间取得良好的平衡。最后，我们在获得的知识基础上，构建了决策树回归器，并学习了划分标准的选择如何影响我们的预测结果。
- en: I hope this chapter has served as a good introduction to scikit-learn and its
    consistent interface. With this knowledge at hand, we can move on to our next
    algorithm and see how it compares to this one. In the next chapter, we will learn
    about linear models. This set of algorithms has its roots back in the 18^(th)
    century, and it is still one of the most commonly used algorithms today.*******
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望本章能为你提供一个良好的scikit-learn和其一致接口的介绍。有了这些知识，我们可以继续研究下一个算法，看看它与决策树算法有何不同。在下一章中，我们将学习线性模型。这组算法可以追溯到18世纪，它至今仍然是最常用的算法之一。
