- en: Twitter Sentiment Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Twitter情感分析
- en: In this chapter, we are going to expand our knowledge of building classification
    models in C#. Along with the two packages, Accord.NET and Deedle, which we used
    in the previous chapter, we are going to start using the Stanford CoreNLP package
    to apply more advanced **natural language processing** (**NLP**) techniques, such
    as tokenization, **part of speech** (**POS**) tagging, and lemmatization. Using
    these packages, our goal for this chapter is to build a multi-class classification
    model that predicts the sentiments of tweets. We will be working with a raw Twitter
    dataset that contains not only words, but also emoticons, and will use it to train
    a **machine learning** (**ML**) model for sentiment prediction. We will be following
    the same steps that we follow when building ML models. We are going to start with
    the problem definition and then data preparation and analysis, feature engineering,
    and model development and validation. During our feature engineering step, we
    will expand our knowledge of NLP techniques and explore how we can apply tokenization,
    POS tagging, and lemmatization to build more advanced text features. In the model
    building step, we are going to explore a new classification algorithm, a random
    forest classifier, and compare its performance to the Naive Bayes classifier.
    Lastly, in our model validation step, we are going to expand our knowledge of
    confusion matrixes, precision, and recall, which we covered in the previous chapter,
    and discuss what the **Receiver Operating Characteristic** (**ROC**) curve and
    **area under the curve** (**AUC**) are and how these concepts can be used to evaluate
    our ML models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将扩展我们在C#中构建分类模型的知识。除了我们在上一章中使用过的Accord.NET和Deedle这两个包，我们还将开始使用Stanford
    CoreNLP包来应用更高级的自然语言处理（NLP）技术，例如分词、词性标注和词元化。使用这些包，本章的目标是构建一个多类分类模型，用于预测推文的情感。我们将使用一个包含不仅只有单词，还有表情符号的原始Twitter数据集，并使用它来训练一个用于情感预测的机器学习（ML）模型。我们将遵循构建ML模型时遵循的相同步骤。我们将从问题定义开始，然后进行数据准备和分析，特征工程，以及模型开发和验证。在我们的特征工程步骤中，我们将扩展我们对NLP技术的知识，并探讨如何将分词、词性标注和词元化应用于构建更高级的文本特征。在模型构建步骤中，我们将探索一个新的分类算法，即随机森林分类器，并将其性能与朴素贝叶斯分类器进行比较。最后，在我们的模型验证步骤中，我们将扩展我们对混淆矩阵、精确率和召回率的了解，这些我们在上一章中已经介绍过，并讨论**接收者操作特征**（ROC）曲线和**曲线下面积**（AUC）是什么，以及这些概念如何用于评估我们的ML模型。
- en: 'In this chapter, we will cover the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Setting up the environment with the Stanford CoreNLP package
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Stanford CoreNLP包设置环境
- en: Problem definition for the Twitter sentiment analysis project
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Twitter情感分析项目的定义问题
- en: Data preparation using Stanford CoreNLP
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Stanford CoreNLP进行数据准备
- en: Data analysis using lemmas as tokens
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用词元作为标记的数据分析
- en: Feature engineering using lemmatization and emoticons
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用词元化和表情符号进行特征工程
- en: Naive Bayes versus random forest
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯与随机森林的比较
- en: Model validations using the ROC curve and AUC metrics
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ROC曲线和AUC指标进行模型验证
- en: Setting up the environment
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置环境
- en: 'Before we dive into our Twitter sentiment analysis project, let''s set up our
    development environment with the Stanford CoreNLP package that we are going to
    use throughout this chapter. Multiple steps are required to get your environment
    ready with the Standford CoreNLP package, so it is a good idea to work through
    this:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入Twitter情感分析项目之前，让我们设置我们的开发环境，我们将使用Stanford CoreNLP包来完成本章的所有工作。要准备好包含Stanford
    CoreNLP包的环境，需要多个步骤，所以最好按以下步骤进行：
- en: 'The first step is to create a new Console App (.NET Framework) project in Visual
    Studio. Make sure you use a .NET Framework version higher than or equal to 4.6.1\.
    If you have an older version installed, go to [https://docs.microsoft.com/en-us/dotnet/framework/install/guide-for-developers](https://docs.microsoft.com/en-us/dotnet/framework/install/guide-for-developers) and
    follow the installation guide. Following is a screenshot of the project setup
    page (note that you can select your .NET Framework version in the top bar):'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是在Visual Studio中创建一个新的控制台应用程序（.NET Framework）项目。确保你使用的是4.6.1或更高版本的.NET Framework。如果你安装了较旧版本，请访问[https://docs.microsoft.com/en-us/dotnet/framework/install/guide-for-developers](https://docs.microsoft.com/en-us/dotnet/framework/install/guide-for-developers)并遵循安装指南。以下是一个项目设置页面的截图（注意：你可以在顶部栏中选择你的.NET
    Framework版本）：
- en: '![](img/00040.jpeg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00040.jpeg)'
- en: 'Now, let''s install the Stanford CoreNLP package. You can type in the following
    command in your Package Manager Console:'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们安装 Stanford CoreNLP 包。你可以在你的包管理控制台中输入以下命令：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The version we are going to use in this chapter is `Stanford.NLP.CoreNLP` 3.9.1\.
    Over time, the versions might change and you might have to update your installations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中将使用的是 `Stanford.NLP.CoreNLP` 3.9.1 版本。随着时间的推移，版本可能会发生变化，你可能需要更新你的安装。
- en: 'We just have to do one more thing and our environment will be ready to start
    using the package. We need to install the CoreNLP models JAR, which contains various
    models for parsing, POS tagging, **Named Entity Recognition** (**NER**), and some
    other tools. Follow this link to download and unzip Stanford CoreNLP: [https://stanfordnlp.github.io/CoreNLP/](https://stanfordnlp.github.io/CoreNLP/).
    Once you have downloaded and unzipped it, you will see multiple files in there.
    The particular file of interest is `stanford-corenlp-<version-number>-models.jar`.
    We need to extract the contents from that jar file into a directory so that we
    can load all the model files within our C# project. You can use the following
    command to extract the contents from `stanford-corenlp-<version-number>-models.jar`:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只需再做一些事情，我们的环境就准备好开始使用这个包了。我们需要安装 CoreNLP 模型 JAR 文件，它包含用于解析、POS 标记、**命名实体识别**（**NER**）和其他一些工具的各种模型。点击此链接下载并解压
    Stanford CoreNLP：[https://stanfordnlp.github.io/CoreNLP/](https://stanfordnlp.github.io/CoreNLP/).
    下载并解压后，你将看到那里有多个文件。我们感兴趣的特定文件是 `stanford-corenlp-<版本号>-models.jar`。我们需要从该 jar
    文件中提取内容到一个目录中，以便我们可以在我们的 C# 项目中加载所有模型文件。你可以使用以下命令从 `stanford-corenlp-<版本号>-models.jar`
    中提取内容：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When you are done extracting all the model files from the models jar file, you
    are now ready to start using the Stanford CoreNLP package in your C# project.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当你从模型 jar 文件中提取完所有模型文件后，你现在就可以开始在 C# 项目中使用 Stanford CoreNLP 包了。
- en: 'Now, let''s check whether our installation was successful. The following code
    is a slight modification for this example ([https://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html](https://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html))
    :'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，让我们检查我们的安装是否成功。以下代码是对本例的轻微修改 ([https://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html](https://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html))
    :'
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If your installation was successful, you should see output similar to the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的安装成功，你应该会看到以下类似的输出：
- en: '![](img/00041.gif)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00041.gif)'
- en: 'Let''s take a closer look at this output. Tokens are character sequences that
    are grouped as individual semantic units. Often, tokens are *words* or *terms*.
    In each token output, we can see the original text, such as `We`, `''re`, and
    `going`. The `PartOfSpeech` tag refers to the category of each word, such as noun,
    verb, and adjective. For example, the `PartOfSpeech` tag of the first token in
    our example, `We`, is `PRP` and it stands for *personal pronoun*. The `PartOfSpeech`
    tag of the second token in our example, `''re`, is `VBP` and it stands for *v**erb,
    non-third-person singular present*. The complete list of POS tags can be found
    here ([http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html))
    or in the following screenshot:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这个输出。标记是作为单个语义单元组合的字符序列。通常，标记是*单词*或*术语*。在每一行标记输出中，我们可以看到原始文本，例如 `We`，`'re`
    和 `going`。`PartOfSpeech` 标签指的是每个单词的类别，例如名词、动词和形容词。例如，我们例子中第一个标记 `We` 的 `PartOfSpeech`
    标签是 `PRP`，它代表*人称代词*。我们例子中第二个标记 `'re` 的 `PartOfSpeech` 标签是 `VBP`，它代表*动词，非第三人称单数现在时*。完整的
    POS 标签列表可以在以下位置找到 ([http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html))
    或在以下屏幕截图：
- en: '![](img/00042.jpeg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00042.jpeg)'
- en: A list of POS tags
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: POS 标签列表
- en: Lastly, the `Lemma` tag in our tokenization example refers to the standard form
    of the given word. For example, the lemma of `am` and `are` is `be`. In our example,
    the word `going` in our third token has `go` as its lemma. We will discuss how
    we can use word lemmatization for feature engineering in the following sections.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在我们标记化示例中的 `Lemma` 标签指的是给定单词的标准形式。例如，`am` 和 `are` 的词元是 `be`。在我们的例子中，第三个标记中的单词
    `going` 的词元是 `go`。我们将在以下章节中讨论如何使用词元化进行特征工程。
- en: Problem definition for Twitter sentiment analysis
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Twitter 情感分析问题定义
- en: 'Let''s start our Twitter sentiment analysis project by clearly defining what
    models we will be building and what they are going to predict. You might have
    heard the term **sentiment analysis** in the past already. Sentiment analysis
    is essentially a process of computationally determining whether a given text expresses
    a positive, neutral, or negative emotion. Sentiment analysis for social media
    content can be used in various ways. For example, it can be used by marketers
    to identify how effective a marketing campaign was and how it affected consumers''
    opinions and attitudes towards a certain product or company. Sentiment analysis
    can also be used to predict stock market changes. Positive news and aggregate
    positive emotions towards a certain company often move its stock price in a positive
    direction, and sentiment analysis in the news and social media for a given company
    can be used to predict how stock prices will move in the near future. To experiment
    with how we can build a sentiment analysis model, we are going to use a precompiled
    and labeled airline sentiment Twitter dataset that originally came from CrowdFlower''s
    Data for Everyone library ([https://www.figure-eight.com/data-for-everyone/](https://www.figure-eight.com/data-for-everyone/)).
    Then, we are going to apply some NLP techniques, especially word tokenization,
    POS tagging, and lemmatization, to build meaningful text and emoticon features
    from raw tweet data. Since we want to predict three different emotions (positive,
    neutral, and negative) for each tweet, we are going to build a multi-class classification
    model and experiment with different learning algorithms—Naive Bayes and random
    forest. Once we build the sentiment analysis models, we are going to evaluate
    the performance mainly via these three metrics: precision, recall, and AUC.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过明确定义我们将构建的模型及其预测内容来开始我们的Twitter情感分析项目。你可能已经听说过“情感分析”这个术语。情感分析本质上是一个计算过程，用于确定给定的文本表达的是积极、中性还是消极情感。社交媒体内容的情感分析可以用于多种方式。例如，营销人员可以使用它来识别营销活动有多有效，以及它如何影响消费者对某个产品或公司的看法和态度。情感分析还可以用于预测股市变化。对某个公司的正面新闻和整体正面情感往往推动其股价上涨，而对于某个公司的新闻和社交媒体中的情感分析可以用来预测股价在不久的将来会如何变动。为了实验如何构建情感分析模型，我们将使用来自CrowdFlower的Data
    for Everyone库的预编译和标记的航空情感Twitter数据集（[https://www.figure-eight.com/data-for-everyone/](https://www.figure-eight.com/data-for-everyone/)）。然后，我们将应用一些NLP技术，特别是词素化、词性标注和词形还原，从原始推文数据中构建有意义的文本和表情符号特征。由于我们想要预测每条推文的三个不同情感（积极、中性和消极），我们将构建一个多类分类模型，并尝试不同的学习算法——朴素贝叶斯和随机森林。一旦我们构建了情感分析模型，我们将主要通过以下三个指标来评估其性能：精确度、召回率和AUC。
- en: 'Let''s summarize our problem definition for the Twitter sentiment analysis
    project:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下Twitter情感分析项目的需求定义：
- en: What is the problem? We need a Twitter sentiment analysis model to computationally
    identify the emotions in tweets.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题是什么？我们需要一个Twitter情感分析模型来计算识别推文中的情感。
- en: Why is it a problem? Identifying and measuring the emotions of users or consumers
    about a certain topic, such as a product, company, advertisement, and so forth,
    are often an essential tool to measure the impact and success of certain tasks.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么这是一个问题？识别和衡量用户或消费者对某个主题（如产品、公司、广告等）的情感，通常是衡量某些任务影响力和成功的重要工具。
- en: What are some of the approaches to solving this problem? We are going to use
    the Stanford CoreNLP package to apply various NLP techniques, such as tokenization,
    POS tagging, and lemmatization, to build meaningful features from a raw Twitter
    dataset. With these features, we are going to experiment with different learning
    algorithms to build a sentiment analysis model. We will use precision, recall,
    and AUC measures to evaluate the performance of the models.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决这个问题的方法有哪些？我们将使用斯坦福CoreNLP包来应用各种NLP技术，如分词、词性标注和词形还原，从原始Twitter数据集中构建有意义的特征。有了这些特征，我们将尝试不同的学习算法来构建情感分析模型。我们将使用精确度、召回率和AUC指标来评估模型的性能。
- en: What are the success criteria? We want high precision rates, without sacrificing
    too much for recall rates, as correctly classifying a tweet into one of three
    emotion buckets (positive, neutral, and negative) is more important than a higher
    retrieval rate. Also, we want a high AUC number, which we will discuss in more
    detail in later sections of this chapter.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功的标准是什么？我们希望有高精确率，同时不牺牲太多的召回率，因为正确地将一条推文分类到三个情感类别（正面、中立和负面）比更高的检索率更重要。此外，我们希望有高AUC值，我们将在本章后面的部分详细讨论。
- en: Data preparation using Stanford CoreNLP
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用斯坦福CoreNLP进行数据准备
- en: 'Now that we know what our goals are in this chapter, it is time to dive into
    the data. Similar to the last chapter, we are going to use precompiled and pre-labeled
    Twitter sentiment data. We are going to use a dataset from CrowdFlower''s Data
    for Everyone library ([https://www.figure-eight.com/data-for-everyone/](https://www.figure-eight.com/data-for-everyone/)) and
    you can download the data from this link: [https://www.kaggle.com/crowdflower/twitter-airline-sentiment](https://www.kaggle.com/crowdflower/twitter-airline-sentiment).
    The data we have here is about 15,000 tweets about US airlines. This Twitter data
    was scraped from February of 2015 and was then labeled into three buckets—positive,
    negative, and neutral. The link provides you with two types of data: a CSV file
    and an SQLite database. We are going to work with the CSV file for this project.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经知道了本章的目标，现在是时候深入数据了。与上一章类似，我们将使用预编译和预标记的Twitter情感数据。我们将使用来自CrowdFlower的Data
    for Everyone库的数据集([https://www.figure-eight.com/data-for-everyone/](https://www.figure-eight.com/data-for-everyone/))，你可以从这个链接下载数据：[https://www.kaggle.com/crowdflower/twitter-airline-sentiment](https://www.kaggle.com/crowdflower/twitter-airline-sentiment)。这里的数据是关于大约15,000条关于美国航空公司的推文。这些Twitter数据是从2015年2月抓取的，然后被标记为三个类别——正面、负面和中立。链接提供了两种类型的数据：CSV文件和SQLite数据库。我们将在这个项目中使用CSV文件。
- en: 'Once you have downloaded this data, we need to get it prepared for our future
    analysis and model building. The two columns of interest in the dataset are `airline_sentiment`
    and `text`. The `airline_sentiment` column contains information about the sentiment—whether
    a tweet has positive, negative, or neutral sentiments—and the `text` column contains
    the raw Twitter text. To make this raw data readily available for our future data
    analysis and model building steps, we need to do the following tasks:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你下载了这些数据，我们需要为未来的分析和模型构建做准备。数据集中我们感兴趣的两大列是`airline_sentiment`和`text`。`airline_sentiment`列包含关于情感的信息——一条推文是否有积极、消极或中性的情感——而`text`列包含原始的Twitter文本。为了使这些原始数据便于我们未来的数据分析和管理模型构建步骤，我们需要完成以下任务：
- en: '**Clean up unnecessary text**: It''s hard to justify some parts of the text
    as providing many insights and much information for our models to learn from,
    such as URLs, user IDs, and raw numbers. So, the first step to prepare our raw
    data is to clean up unnecessary text that does not contain much information. In
    this example, we removed the URLs, Twitter user IDs, numbers, and hash signs in
    hashtags. We used `Regex` to replace such texts with empty strings. The following
    code illustrates the `Regex` expressions we used to filter out those texts:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清理不必要的文本**：很难证明文本的某些部分提供了许多见解和信息，供我们的模型学习，例如URL、用户ID和原始数字。因此，准备我们原始数据的第一个步骤是清理不包含太多信息的无用文本。在这个例子中，我们移除了URL、Twitter用户ID、数字和标签符号。我们使用`Regex`将此类文本替换为空字符串。以下代码展示了我们用来过滤这些文本的`Regex`表达式：'
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As you can see from this code, there are two ways to replace a string that
    matches a `Regex` pattern. You can instantiate a `Regex` object and then replace
    matching strings with the other string, as shown in the first two cases. You can
    also directly call the static `Regex.Replace` method for the same purpose, as
    shown in the last two cases. The static method is going to create a `Regex` object
    each time you call the `Regex.Replace` method, so if you are using the same pattern
    in multiple places, it will be better to go with the first approach:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，有两种方式可以替换匹配`Regex`模式的字符串。你可以实例化一个`Regex`对象，然后用另一个字符串替换匹配的字符串，如前两个案例所示。你也可以直接调用静态的`Regex.Replace`方法来达到同样的目的，如最后两个案例所示。静态方法会在每次调用`Regex.Replace`方法时创建一个`Regex`对象，所以如果你在多个地方使用相同的模式，第一种方法会更好：
- en: '**Group and encode similar emoticons together**: Emoticons, such as smiley
    faces and sad faces, are frequently used in tweets and provide useful insights
    about the emotion of each tweet. Intuitively, one user will use smiley face emoticons
    to tweet about positive events, while another will use sad face emoticons to tweet
    about negative events. However, different smiley faces show similar positive emotions
    and can be grouped together. For example, a smiley face with a parenthesis, `:)`,
    will have the same meaning as another smiley face with a capital letter `D`, `:D`.
    So, we want to group these similar emoticons together and encode them as one group
    rather than having them in separate groups. We will use the R code that Romain
    Paulus and Jeffrey Pennington shared ([https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb](https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb)),
    translate it into C#, and then apply it to our raw Twitter dataset. The following
    is how we translated the emoticon `Regex` codes, written in R, into C#, so that
    we can group and encode similar emoticons together:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将相似的表情符号分组并编码**：表情符号，如笑脸和悲伤脸，在推文中经常被使用，并提供了关于每条推文情感的见解。直观地，一个用户会用笑脸表情符号来推文关于积极事件，而另一个用户会用悲伤脸表情符号来推文关于负面事件。然而，不同的笑脸表现出相似的正向情感，可以分组在一起。例如，带括号的笑脸`:)`与带大写字母`D`的笑脸`:D`具有相同的意义。因此，我们希望将这些相似的表情符号分组在一起，并将它们编码为一个组，而不是让它们分别在不同的组中。我们将使用Romain
    Paulus和Jeffrey Pennington分享的R代码([https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb](https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb))，将其翻译成C#，然后将其应用于我们的原始Twitter数据集。以下是如何将R中编写的表情符号`Regex`代码翻译成C#，以便我们可以将相似的表情符号分组并编码：'
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Group and encode additional helpful expressions together**: Lastly, there
    are some more expressions that can help our models detect the emotions of tweets.
    Repeated punctuation, such as `!!!` and `???`, and elongated words, such as `wayyyy`
    and `soooo`, can provide some extra information about the sentiments of tweets.
    We will group and encode them separately so that our models can learn from such
    expressions. The following code shows how we encoded such expressions:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将其他有用的表达式分组并编码**：最后，还有一些可以帮助我们的模型检测推文情感的表达式。重复的标点符号，如`!!!`和`???`，以及长单词，如`wayyyy`和`soooo`，可以提供一些关于推文情感的额外信息。我们将分别将它们分组并编码，以便我们的模型可以从这些表达式中学习。以下代码展示了如何编码这样的表达式：'
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As shown in the code, for repeated punctuation we appended a string with a suffix,
    `_repeat`. For example, `!!!` will become `!_repeat` and `???` will become `?_repeat`.
    For elongated words, we appended a string with a suffix, `_emphasized`. For example,
    `wayyyy` will become `way_emphasized` and `soooo` will become `so_emphasized`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如代码所示，对于重复的标点符号，我们在字符串后附加一个后缀`_repeat`。例如，`!!!`将变成`!_repeat`，而`???`将变成`?_repeat`。对于长单词，我们在字符串后附加一个后缀`_emphasized`。例如，`wayyyy`将变成`way_emphasized`，而`soooo`将变成`so_emphasized`。
- en: 'The full code that takes the raw dataset, processes individual Twitter text
    as discussed previously, and exports the processed Twitter text into another data
    file can be found in this repository: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/DataProcessor.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/DataProcessor.cs).
    Let''s briefly walk through the code. It first reads the raw `Tweets.csv` dataset
    into a Deedle data frame (lines 76–82). Then, it calls a method named `FormatTweets` with
    a column series that contains all the raw Twitter text. The `FormatTweets` method
    code in lines 56–65 looks like the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始数据集处理成单个Twitter文本，并导出处理后的Twitter文本到另一个数据文件的全代码可以在本存储库中找到：[https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/DataProcessor.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/DataProcessor.cs)。让我们简要地浏览一下代码。它首先将原始`Tweets.csv`数据集读入一个Deedle数据框（第76-82行）。然后，它调用一个名为`FormatTweets`的方法，该方法包含一个包含所有原始Twitter文本的列序列。第56-65行的`FormatTweets`方法代码如下所示：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This `FormatTweets`method iterates through each element in the series, which
    is the raw tweets, and calls the `CleanTweet` method. Within the `CleanTweet`
    method, each raw tweet is run against all the `Regex` patterns that we defined
    previously and is then processed as discussed earlier. The `CleanTweet` method
    in lines 11–54 looks as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`FormatTweets`方法遍历序列中的每个元素，即原始推文，并调用`CleanTweet`方法。在`CleanTweet`方法中，每条原始推文都会与之前定义的所有`Regex`模式进行匹配，然后按照之前讨论的方式进行处理。第11-54行的`CleanTweet`方法如下所示：'
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once all the raw Twitter tweets are cleaned and processed, the result gets
    added to the original Deedle data frame as a separate column with `tweet` as its
    column name. The following code (line 89) shows how you can add an array of strings
    to a data frame:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有原始的Twitter推文都被清理和加工处理，结果就会被添加到原始的Deedle数据框中作为一个单独的列，其列名为`tweet`。以下代码（第89行）展示了如何将字符串数组添加到数据框中：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once you have come this far, the only additional step we need to do is export
    the processed data. Using Deedle data frame''s `SaveCsv` method, you can easily
    export a data frame into a CSV file. The following code shows how we exported
    the processed data into a CSV file:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当您已经走到这一步时，我们唯一需要做的额外步骤就是导出处理后的数据。使用Deedle数据框的`SaveCsv`方法，您可以轻松地将数据框导出为CSV文件。以下代码展示了我们如何将处理后的数据导出为CSV文件：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now that we have clean Twitter text, let''s tokenize and create a matrix representation
    of tweets. Similar to what we did in [Chapter 2](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470), *Spam
    Email Filtering*, we are going to break a string into words. However, we are going
    to use the Stanford CoreNLP package that we installed in the previous section
    of this chapter and utilize the sample code that we wrote in the previous section.
    The code to tokenize tweets and build a matrix representation of them is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了干净的Twitter文本，让我们对其进行分词并创建推文的矩阵表示。类似于我们在[第2章](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470)中做的，*垃圾邮件过滤*，我们将字符串分解成单词。然而，我们将使用我们在本章前一部分安装的Stanford
    CoreNLP包，并利用我们在前一部分编写的示例代码。分词推文并构建其矩阵表示的代码如下：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see from the code, the main difference between this code and the
    sample code in the previous section is that this code iterates over each tweet
    and stores the tokens into a Deedle's data frame. As in [Chapter 2](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470), *Spam
    Email Filtering,* we are using one-hot encoding to assign each term's value (0
    versus 1) within the matrix. One thing to note here is how we have an option to
    create the matrix with lemmas or words. Words are the original untouched terms
    that are broken down from each tweet. For example, a string, `I am a data scientist`,
    will be broken down into `I`, `am`, `a`, `data`, and `scientist`, if you use words
    as tokens. Lemmas are standard forms of words in each token. For example, the
    same string, `I am a data scientist`, will be broken down into `I`, `be`, `a`,
    `data`, and `scientist`, if you use lemmas as tokens. Note that `be` is a lemma
    for `am`. We will discuss what lemmas are and what lemmatization is in the *Feature
    engineering using lemmatization and emoticons* section.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从代码中可以看到，这段代码与上一节中的示例代码的主要区别在于，这段代码会遍历每条推文并将标记存储到Deedle的数据框中。正如在[第2章](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470)中，*垃圾邮件过滤*，我们使用独热编码来分配矩阵中每个术语的值（0或1）。在这里需要注意的一点是我们有创建包含词元或单词的矩阵的选项。单词是从每条推文中分解出来的原始未修改的术语。例如，字符串`I
    am a data scientist`，如果您使用单词作为标记，将会分解成`I`、`am`、`a`、`data`和`scientist`。词元是每个标记中单词的标准形式。例如，相同的字符串`I
    am a data scientist`，如果您使用词元作为标记，将会分解成`I`、`be`、`a`、`data`和`scientist`。请注意`be`是`am`的词元。我们将在*使用词元化和表情符号进行特征工程*部分讨论词元是什么以及词元化是什么。
- en: 'The full code to tokenize and create a matrix representation of tweets can
    be found here: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/TwitterTokenizer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/TwitterTokenizer.cs).
    There are a few things to note in this code. First, let''s look at how it counts
    how many samples we have for each sentiment. The following code snippet (lines
    122–127) shows how we computed the number of samples per sentiment:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 分词和创建推文矩阵表示的完整代码可以在以下链接找到：[https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/TwitterTokenizer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/TwitterTokenizer.cs)。在这段代码中有几点需要注意。首先，让我们看看它是如何计算每种情感样本数量的。以下代码片段（第122-127行）展示了我们如何计算每种情感的样本数量：
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you can see from this code, we first get the sentiment column, `airline_sentiment`,
    and group it by the values, where the values can be `neutral`, `negative`, or
    `positive`. Then, it counts the number of occurrences and returns the count.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从这段代码中可以看到，我们首先获取情感列，`airline_sentiment`，并按值对其进行分组，其中值可以是`中立`、`负面`或`正面`。然后，它计算出现的次数并返回计数。
- en: 'The second thing to note in the TwitterTokenizer code is how we encoded sentiments
    with integer values. The following is what you see in lines 149–154 of the full
    code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在`TwitterTokenizer`代码中需要注意的第二件事是我们如何用整数值编码情感。以下是在完整代码的第149-154行中看到的内容：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Lastly, note how we are calling the `CreateWordVec` method twice—once without
    lemmatization (lines 135-144) and once with lemmatization (lines 147-156). If
    we create a term matrix with one-hot encodings without lemmatization, we are essentially
    taking all the words as individual tokens in our term matrix. As you can imagine,
    this will create a much larger and more sparse matrix than one with lemmatization.
    We left both codes there for you to explore both options. You can try building
    ML models with a matrix with words as columns and compare them against those with
    lemmas as columns. In this chapter, we are going to use the matrix with lemmas
    instead of words.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意我们是如何两次调用`CreateWordVec`方法的——一次没有词形还原（第135-144行），一次有词形还原（第147-156行）。如果我们创建一个没有词形还原的单热编码的术语矩阵，我们实际上是将所有单词作为术语矩阵中的单个标记。正如您所想象的，这将比有词形还原的矩阵大得多，稀疏性也更高。我们留下了这两段代码供您探索两种选项。您可以尝试使用以单词为列的矩阵构建ML模型，并与以词元为列的模型进行比较。在本章中，我们将使用词元矩阵而不是单词矩阵。
- en: 'When you run this code, it will output a bar chart that shows the sentiment
    distribution in the sample set. As you can see in the following chart, we have
    about 3,000 neutral tweets, 2,000 positive tweets, and 9,000 negative tweets in
    our sample set. The chart looks as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行此代码时，它将输出一个条形图，显示样本集中的情感分布。如您在以下图表中看到的，在我们的样本集中大约有3,000条中性推文，2,000条积极推文和9,000条消极推文。图表如下：
- en: '![](img/00043.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/00043.jpeg)'
- en: Data analysis using lemmas as tokens
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用词元作为标记的数据分析
- en: It is now time to look at the actual data and seek any patterns or differences
    in the distributions of term frequencies along with the different sentiments of
    tweets. We are going to take the output from the previous step and get the distributions
    of the top seven most frequently occurring tokens for each sentiment. In this
    example, we use a term matrix with lemmas. Feel free to run the same analysis
    for a term matrix with words. The code to analyze the top N most frequently used
    tokens in each sentiment of tweets can be found here: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/DataAnalyzer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/DataAnalyzer.cs).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候查看实际数据，并寻找术语频率分布与推文不同情感之间的任何模式或差异了。我们将使用上一步的输出，并获取每个情感中最常出现的七个标记的分布。在这个例子中，我们使用了一个包含词元的术语矩阵。您可以自由地运行相同的分析，使用以单词为列的术语矩阵。分析推文中每个情感中最常使用的N个标记的代码可以在以下位置找到：[https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/DataAnalyzer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/DataAnalyzer.cs)。
- en: 'There is one thing to note in this code. Unlike in the previous chapter, we
    need to compute term frequencies for three sentiment classes—neutral, negative,
    and positive. The following is the code snippet from the full code (lines 54-73):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中有一点需要注意。与上一章不同，我们需要为三个情感类别——中性、消极和积极——计算术语频率。以下是从完整代码中摘录的代码片段（第54-73行）：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As you can see from the code, we call the `ColumnWiseSum` method for each sentiment
    class, and the code for this method is as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从代码中可以看到，我们为每个情感类别调用了`ColumnWiseSum`方法，这个方法的代码如下：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see from this code, it iterates through each column or term and sums
    all the values within that column. Since we used one-hot encodings, a simple column-wise
    sum will give us the number of occurrences for each term in our Twitter dataset.
    Once we have computed all the column-wise summations, we return them as a Deedle
    series object. With these results, we rank-order the terms by their frequencies
    and store this information into three separate files, `neutral-frequencies.csv`,
    `negative-frequencies.csv`, and `positive-frequencies.csv`. We are going to use
    the term frequency output in later sections for feature engineering and model
    building.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从这段代码中看到的，它遍历每一列或术语，并计算该列内的所有值。由于我们使用了单热编码，简单的列求和将给我们Twitter数据集中每个术语的出现次数。一旦我们计算了所有列求和，我们就将它们作为Deedle系列对象返回。有了这些结果，我们按频率对术语进行排名，并将这些信息存储在三个单独的文件中，分别是`neutral-frequencies.csv`、`negative-frequencies.csv`和`positive-frequencies.csv`。我们将在后面的章节中使用术语频率输出进行特征工程和模型构建。
- en: 'When you run the code, it will generate the following charts:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行代码时，它将生成以下图表：
- en: '![](img/00044.jpeg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00044.jpeg)'
- en: As you can see from the charts, there are some obvious differences in distributions
    among different sentiments. Words such as **thanks** and **great** were two of
    the top seven frequently occurring terms in positive tweets, while words like
    **delay** and **cancelled** were two of the top seven frequently occurring terms
    in negative tweets. Intuitively, these make sense. You typically use words **thanks**
    and **great** when you express positive feelings towards someone or something.
    On the other hand, **delay** and **cancelled** are related to negative events
    in the context of flights or airlines. Maybe some of the users' flights were delayed
    or cancelled and they tweeted about their frustrations. Another interesting thing
    to note is how the term `emo_smiley` was ranked seventh of the most frequently
    occurring terms in positive tweets. If you remember, in the previous step we grouped
    and encoded all smiley face emoticons (such as `:)`, `:D`, and so on) as `emo_smiley`.
    This tells us that emoticons may play an important role for our models to learn
    how to classify the sentiment of each tweet. Now that we have a rough idea of
    what our data looks like and what kinds of terminology appear for each sentiment,
    let's talk about the feature engineering techniques we will employ in this chapter.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从图表中可以看到，不同情感之间的分布存在一些明显的差异。例如，**谢谢**和**很好**是积极推文中出现频率最高的七个词中的两个，而**延误**和**取消**则是消极推文中出现频率最高的七个词中的两个。直观上看，这些是有道理的。您通常会在表达对某人或某事的积极感受时使用**谢谢**和**很好**。另一方面，**延误**和**取消**与飞行或航空领域的负面事件相关。也许有些用户的航班延误或取消，他们就在推特上表达了自己的挫败感。另一个值得注意的有趣现象是，`emo_smiley`这个术语在积极推文中被列为出现频率最高的七个词中的第七位。如果您还记得，在上一个步骤中，我们将所有笑脸表情符号（如`:)`、`:D`等）分组并编码为`emo_smiley`。这告诉我们，表情符号可能在我们的模型学习如何分类每条推文的情感方面发挥重要作用。现在我们已经对数据的外观以及每种情感出现的术语有了大致的了解，让我们来谈谈在本章中我们将采用的特征工程技术。
- en: Feature engineering using lemmatization and emoticons
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用词元化和表情符号进行特征工程
- en: We briefly talked about lemmas in the previous section. Let's take a deeper
    look at what lemmas are and what lemmatization is. Depending on how and where
    a word is being used in a sentence, the word is going to be in different forms.
    For example, the word `like` can take the form of `likes` or `liked` depending
    on what came before. If we simply tokenize sentences into words, then our program
    is going to see the words `like`, `likes`, and `liked` as three different tokens.
    However, that might not be something we want. Those three words share the same
    meaning and when we are building models, it would be useful to group those words
    as one token in our feature set. This is what lemmatization does. A lemma is the
    base form of a word and lemmatization is transforming each word into a lemma based
    on the part of the sentence each word was used in. In the preceding example, `like`
    is the lemma for `likes` and `liked`, and systematically transforming `likes`
    and `liked` into `like` is a lemmatization.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们简要地讨论了词元。让我们更深入地了解一下什么是词元以及什么是词元化。根据一个词在句子中的使用方式和位置，这个词会以不同的形式出现。例如，单词`like`可以以`likes`或`liked`的形式出现，这取决于前面的内容。如果我们只是简单地将句子分词成单词，那么我们的程序将会把`like`、`likes`和`liked`看作是三个不同的标记。然而，这可能不是我们想要的。这三个词具有相同的意义，当我们构建模型时，将它们作为特征集中的同一个标记分组会很有用。这就是词元化的作用。词元是一个词的基本形式，词元化是根据每个词在句子中的使用部分将每个词转换成词元。在上面的例子中，`like`是`likes`和`liked`的词元，将`likes`和`liked`系统地转换成`like`就是词元化。
- en: 'Following is an example of a lemmatization using Stanford CoreNLP:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个使用Stanford CoreNLP进行词元化的例子：
- en: '![](img/00045.jpeg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00045.jpeg)'
- en: 'Here, you can see that both `likes` and `like` were lemmatized into `like`.
    This is because both of those words were used as verbs in a sentence and the lemma
    for the verbal form is `like`. Let''s look at another example:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以看到`likes`和`like`都被词元化为`like`。这是因为这两个词在句子中都被用作动词，而动词形式的词元是`like`。让我们再看另一个例子：
- en: '![](img/00046.jpeg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00046.jpeg)'
- en: Here, the first `likes` and the second `likes` have different lemmas. The first
    one has `like` as its lemma, while the second one has `likes` as its lemma. This
    is because the first one is used as a verb, while the second one as a noun. As
    you can see from these examples, depending on the parts of the sentence, the lemmas
    for the same words can be different. Using lemmatization for your text dataset
    can greatly reduce the sparsity and dimensions of your feature space and can help
    your models learn better without being exposed to too much noise.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，第一个 `likes` 和第二个 `likes` 有不同的词干。第一个有一个 `like` 作为其词干，而第二个有一个 `likes` 作为其词干。这是因为第一个被用作动词，而第二个被用作名词。正如您可以从这些例子中看到的那样，根据句子的不同部分，相同单词的词干可能会有所不同。对您的文本数据集进行词形还原可以大大减少特征空间的稀疏性和维度，并有助于模型在没有过多噪声的情况下更好地学习。
- en: Similar to lemmatization, we also grouped similar emoticons into the same group.
    This is based on the assumption that similar emoticons have similar meanings.
    For example, `:)` and `:D` have almost the same meanings, if not exactly the same.
    In another case, depending on the users, the positions of the colon and parenthesis
    can differ. Some users might type `:)`, but some others might type `(:`. However,
    the only different between these two is the positioning of the colon and parenthesis
    and the meanings are the same. In all of these cases, we want our models to learn
    the same emotion and not create any noise. Grouping similar emoticons into the
    same group, as we did in the previous step, helps reduce unnecessary noise for
    our models and help them learn the most from these emoticons.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于词形还原，我们也把相似的表情符号分到了同一个组。这是基于这样的假设：相似的表情符号具有相似的含义。例如，`:)` 和 `:D` 几乎具有相同的含义，如果不是完全相同。在另一种情况下，根据用户的不同，冒号和括号的顺序可能不同。一些用户可能会输入
    `:)`，但另一些用户可能会输入 `(:`。然而，这两者之间唯一的区别是冒号和括号的顺序，而含义是相同的。在所有这些情况下，我们都希望我们的模型能够学习到相同的情感，并且不会产生任何噪声。将相似的表情符号分组到同一个组，就像我们在上一步所做的那样，有助于减少模型的不必要噪声，并帮助它们从这些表情符号中学习到最多。
- en: Naive Bayes versus random forest
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯贝叶斯与随机森林
- en: It is finally time to train our ML models to predict the sentiments of tweets.
    In this section, we are going to experiment with Naive Bayes and random forest
    classifiers. There are two things that we are going to do differently from the
    previous chapter. First, we are going to split our sample set into a train set
    and a validation set, instead of running k-fold cross-validation. This is also
    a frequently used technique, where the models learn only from a subset of the
    sample set and then they are tested and validated with the rest, which they did
    not observe at training time. This way, we can test how the models will perform
    in the unforeseen dataset and simulate how they are going to behave in a real-world
    case. We are going to use the `SplitSetValidation` class in the Accord.NET package
    to split our sample set into train and validation sets with pre-defined proportions
    for each set and fit a learning algorithm to the train set.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在终于到了训练我们的机器学习模型来预测推文的情感的时候了。在本节中，我们将尝试使用朴素贝叶斯和随机森林分类器。我们将要做两件与上一章不同的事情。首先，我们将把我们的样本集分成训练集和验证集，而不是运行k折交叉验证。这也是一种常用的技术，其中模型只从样本集的一个子集中学习，然后它们用未在训练时观察到的其余部分进行测试和验证。这样，我们可以测试模型在不可预见的数据集上的表现，并模拟它们在实际世界中的行为。我们将使用
    Accord.NET 包中的 `SplitSetValidation` 类，将我们的样本集分成训练集和验证集，并为每个集合预先定义比例，并将学习算法拟合到训练集。
- en: Secondly, our target variable is no longer binary (0 or 1), unlike in the previous
    [Chapter 2](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470)*, Spam Email
    Filtering*. Instead, it can take any values from 0, 1, or 2, where 0 stands for
    neutral sentiment tweets, 1 for positive sentiment tweets, and 2 for negative sentiment
    tweets. So, we are now dealing with a multi-class classification problem, rather
    than a binary classification problem. We will have to approach things differently
    when evaluating our models. We will have to modify our accuracy, precision, and
    recall calculation codes from the previous chapter to compute those numbers for
    each of the three target sentiment classes in this project. Also, we will have
    to use a one-versus-rest approach when we look at certain metrics, such as a ROC
    curve and AUC, which we will be discussing in the following section.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们的目标变量不再是二进制（0或1），与之前的[第2章](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470)*垃圾邮件过滤*不同。相反，它可以取0、1或2的任何值，其中0代表中性情感推文，1代表积极情感推文，2代表消极情感推文。因此，我们现在处理的是一个多类分类问题，而不是二类分类问题。在评估我们的模型时，我们必须采取不同的方法。我们必须修改上一章中的准确率、精确率和召回率的计算代码，以计算本项目三个目标情感类别中的每个类别的这些数字。此外，当我们查看某些指标时，例如ROC曲线和AUC，我们将在下一节讨论这些指标，我们必须使用一对一的方法。
- en: 'Let''s first look at how to instantiate our learning algorithms with the `SplitSetValidation`
    class in the Accord.NET Framework. The following is how you can instantiate a `SplitSetValidation`object
    with the Naive Bayes classifier algorithm:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看如何在 Accord.NET 框架中使用 `SplitSetValidation` 类实例化我们的学习算法。以下是如何使用朴素贝叶斯分类器算法实例化一个
    `SplitSetValidation` 对象的方法：
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Similar to how we instantiated a `SplitSetValidation` object with the Naive
    Bayes classifier, you can instantiate another one with the random forest classifier
    as in the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们使用朴素贝叶斯分类器实例化 `SplitSetValidation` 对象的方式类似，你还可以按照以下方式实例化另一个对象：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We replaced the previous code with random forest as a model and `RandomForestLearning` as
    a learning algorithm. If you look closely, there are some hyperparameters that
    we can tune for `RandomForestLearning`. The first one is `NumberOfTrees`. This
    hyperparameter lets you choose the number of decision trees that go into your
    random forest. In general, having more trees in a random forest results in better
    performance, as you are essentially building more decision trees in the forest.
    However, the performance lift comes at the cost of training and prediction time.
    It will take more time to train and make predictions as you increase the number
    of trees in your random forest. The other two parameters to note here are `CoverageRatio`and
    `SampleRatio`. `CoverageRatio`sets the proportion of the feature set to be used
    in each tree, while `SampleRatio`sets the proportion of the train set to be used
    in each tree. Having a higher `CoverageRatio` and`SampleRatio` increases the performance
    of individual trees in the forest, but it also increases the correlation among
    the trees. Lower correlation among the trees helps reduce the generalization error;
    thus, finding a good balance between the prediction powers of individual trees
    and correlation among the trees will be essential in building a good random forest
    model. Tuning and experimenting with various combinations of these hyperparameters
    can help you avoid overfitting issues, as well as improving your model performance
    when training a random forest model. We recommend you build a number of random
    forest classifiers with various combinations of those hyperparameters and experiment
    with their effects on your model performances.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将之前的代码替换为随机森林作为模型，以及 `RandomForestLearning` 作为学习算法。如果你仔细观察，会发现一些我们可以调整的 `RandomForestLearning`
    的超参数。第一个是 `NumberOfTrees`。这个超参数允许你选择要进入你的随机森林中的决策树的数量。一般来说，随机森林中的树越多，性能越好，因为你实际上在森林中构建了更多的决策树。然而，性能的提升是以训练和预测时间为代价的。随着你在随机森林中增加树的数量，训练和预测将需要更多的时间。这里需要注意的其他两个参数是
    `CoverageRatio` 和 `SampleRatio`。`CoverageRatio` 设置了每个树中使用的特征集的比例，而 `SampleRatio`
    设置了每个树中使用的训练集的比例。较高的 `CoverageRatio` 和 `SampleRatio` 会提高森林中单个树的表现，但也会增加树之间的相关性。树之间的低相关性有助于减少泛化误差；因此，在单个树预测能力和树之间的相关性之间找到一个良好的平衡对于构建一个好的随机森林模型至关重要。调整和实验这些超参数的各种组合可以帮助你避免过拟合问题，并在训练随机森林模型时提高你的模型性能。我们建议你构建多个具有不同超参数组合的随机森林分类器，并实验它们对模型性能的影响。
- en: The full code that we used to train Naive Bayes and random forest classification
    models and output validation results can be found here: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/TwitterSentimentModeling.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/TwitterSentimentModeling.cs).
    Let's take a closer look at this code. In lines 36-41, it first reads in the token
    matrix file, `tweet-lemma.csv`, which we built in the data preparation step. Then
    in lines 43-51, we read in the term frequency files, `positive-frequencies.csv`
    and `negative-frequencies.csv`, which we built in the data analysis step. Similar
    to what we did in the previous chapter, we do feature selection based on the number
    of term occurrences in line 64\. In this example, we experimented with 5, 10,
    50, 100, and 150 as the thresholds for the minimum number of term occurrences
    in our sample tweets. From line 65, we iterate through those thresholds and start
    training and evaluating Naive Bayes and random forest classifiers. Each time a
    model is trained on a train set, it is then run against the validation set that
    was not observed during the training time.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来训练朴素贝叶斯和随机森林分类模型并输出验证结果的完整代码可以在以下链接找到：[https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/TwitterSentimentModeling.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/TwitterSentimentModeling.cs)。让我们更仔细地看看这段代码。在第36-41行，它首先读取我们数据准备步骤中构建的标记矩阵文件`tweet-lemma.csv`。然后在第43-51行，我们读取我们数据分析步骤中构建的词频文件`positive-frequencies.csv`和`negative-frequencies.csv`。类似于我们在上一章中做的，我们在第64行基于词的出现次数进行特征选择。在这个例子中，我们尝试了5、10、50、100和150作为样本推文中词出现次数的最小阈值。从第65行开始，我们迭代这些阈值，并开始训练和评估朴素贝叶斯和随机森林分类器。每次在训练集上训练一个模型后，它就会对在训练时间内未观察到的验证集进行运行。
- en: 'Following is part of the full code (lines 113-135) that runs the trained Naive
    Bayes model on the train and validation sets to measure in-sample and out-of-sample
    performance:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在训练集和验证集上运行训练好的朴素贝叶斯模型以测量样本内和样本外性能的完整代码（第113-135行）的一部分：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Following is the part of the full code (lines 167-189) that runs the trained
    random forest model on the train and validation sets to measure in-sample and
    out-of-sample performance:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在训练集和验证集上运行训练好的随机森林模型以测量样本内和样本外性能的完整代码（第167-189行）的一部分：
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Let's take a closer look at these. For brevity, we will only take a look at
    the random forest model case, as it will be the same for the Naive Bayes classifier.
    In line 168, we first get the trained model from the learned results. Then, we
    get the indexes of in-sample (train set) and out-of-sample (test/validation set)
    sets from the `SplitSetValidation` object in lines 170-171, so that we can iterate
    through each row or record and make predictions. We iterate this process twice—once
    for the in-sample training set in lines 175-181 and again for the out-of-sample
    validation set in lines 183-189.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这些。为了简洁起见，我们只看随机森林模型的情况，因为朴素贝叶斯分类器的情况将相同。在第168行，我们首先从学习结果中获取训练好的模型。然后，在第170-171行，我们从`SplitSetValidation`对象中获取样本内（训练集）和样本外（测试/验证集）的索引，以便我们可以迭代每一行或记录并做出预测。我们迭代这个过程两次——一次在第175-181行的样本内训练集上，再次在第183-189行的样本外验证集上。
- en: Once we have the prediction results on the train and test sets, we run those
    results through some validation methods (lines 138-141 for the Naive Bayes classifier
    and lines 192-196 for the random forest classifier). There are two methods that
    we wrote specifically for the model validation for this project—`PrintConfusionMatrix`
    and `DrawROCCurve`. `PrintConfusionMatrix`is an updated version of what we had
    in [Chapter 2](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470), *Spam Email
    Filtering*, where it now prints a 3 x 3 confusion matrix, instead of a 2 x 2 confusion
    matrix. On the other hand, the `DrawROCCurve`method brings in some new concepts
    and new model validation methods for this project. Let's discuss those new evaluation
    metrics, which we are using for this project, in greater detail in the following
    section.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在训练集和测试集上获得了预测结果，我们就将这些结果通过一些验证方法进行验证（第138-141行用于朴素贝叶斯分类器，第192-196行用于随机森林分类器）。我们为这个项目专门编写了两种方法来验证模型——`PrintConfusionMatrix`和`DrawROCCurve`。`PrintConfusionMatrix`是我们在[第2章](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470)，“垃圾邮件过滤”中使用的更新版本，现在它打印的是一个3
    x 3的混淆矩阵，而不是2 x 2的混淆矩阵。另一方面，`DrawROCCurve`方法为这个项目引入了一些新的概念和新的模型验证方法。让我们在下一节更详细地讨论这些新的评估指标，这是我们在这个项目中使用的。
- en: Model validations – ROC curve and AUC
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型验证——ROC曲线和AUC
- en: 'As mentioned before, we are using different model validation metrics in this
    chapter: the ROC curve and AUC. The ROC curve is a plot of a true positive rate
    against a false positive rate at various thresholds. Each point in the curve represents
    the true positive and false positive rate pair corresponding at a certain probability
    threshold. It is commonly used to select the best and the most optimal models
    among different model candidates.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们在本章中使用不同的模型验证指标：ROC曲线和AUC。ROC曲线是在各种阈值下，真实正率与假正率的关系图。曲线上的每个点代表在某个概率阈值下对应的真实正率和假正率对。它通常用于从不同的模型候选者中选择最佳和最优化模型。
- en: The area under the ROC curve (AUC) measures how well the model can distinguish
    the two classes. In the case of a binary classification, AUC measures how well
    a model distinguishes the positive outcomes from the negative outcomes. Since
    we are dealing with a multi-class classification problem in this project, we are
    using a one-versus-rest approach to build the ROC curve and compute the AUC. For
    example, one ROC curve can take positive tweets as positive outcomes and neutral
    and negative tweets as negative outcomes, while another ROC curve can take neutral
    tweets as positive outcomes and positive and negative tweets as negative outcomes.
    As shown in the following charts, we drew three ROC charts for each model we built—one
    for Neutral verses Rest (Positive and Negative), one for Positive versus Rest
    (Neutral and Negative), and one for Negative versus Rest (Neutral and Positive).
    The higher the AUC number is, the better the model is as it suggests that the
    model can distinguish positive classes from negative classes with much better
    chance.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线下的面积（AUC）衡量模型区分两个类别的好坏。在二元分类的情况下，AUC衡量模型区分正结果和负结果的好坏。由于我们在这个项目中处理的是一个多类分类问题，我们使用一对一的方法来构建ROC曲线并计算AUC。例如，一条ROC曲线可以将正面推文作为正面结果，将中立和负面推文作为负面结果，而另一条ROC曲线可以将中立推文作为正面结果，将正面和负面推文作为负面结果。如图表所示，我们为每个构建的模型绘制了三个ROC图表——一个用于中立与剩余（正面和负面）的对比，一个用于正面与剩余（中立和负面）的对比，以及一个用于负面与剩余（中立和正面）的对比。AUC数值越高，模型越好，因为它表明模型有更大的可能性区分正类别和负类别。
- en: 'The following charts show ROC curves for Naive Bayes classifiers with the minimum
    number of term occurrences at **10**:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了具有**10**个最小词频的朴素贝叶斯分类器的ROC曲线：
- en: '![](img/00047.jpeg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00047.jpeg)'
- en: 'The following charts show ROC curves for Naive Bayes classifiers with the minimum
    number of term occurrences at **50**:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了具有**50**个最小词频的朴素贝叶斯分类器的ROC曲线：
- en: '![](img/00048.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00048.jpeg)'
- en: 'The following charts show ROC curves for Naive Bayes classifiers with the minimum
    number of term occurrences at **150**:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了具有**150**个最小词频的朴素贝叶斯分类器的ROC曲线：
- en: '![](img/00049.jpeg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00049.jpeg)'
- en: As you can see from the charts, we can also detect overfitting issues from ROC
    charts by looking at the gaps between the curves from training and testing results.
    The larger the gap is, the more the model is overfitting. If you look at the first
    case, where we only filter out those terms that appear in tweets fewer than ten
    times, the gap between the two curves is large. As we increase the threshold,
    we can see that the gap decreases. When we are choosing the final model, we want
    the train ROC curve and the test/validation ROC curve to be as small as possible.
    As this resolution comes at the expense of the model performance, we need to find
    the right cutoff line for this trade-off.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从图表中可以看到，我们也可以通过观察训练和测试结果曲线之间的差距来从ROC图表中检测过拟合问题。差距越大，模型过拟合的程度就越高。如果您看第一个案例，我们只过滤掉那些在推文中出现次数少于十次的术语，两个曲线之间的差距就很大。随着我们提高阈值，我们可以看到差距减小。当我们选择最终模型时，我们希望训练ROC曲线和测试/验证ROC曲线尽可能小。由于这种分辨率是以模型性能为代价的，我们需要找到这个权衡的正确截止线。
- en: 'Let''s now look at a sample of how one of our random forest classifiers did.
    The following is a sample result from fitting a random forest classifier:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在看看我们的随机森林分类器中的一个样本。以下是从拟合随机森林分类器中得到的一个样本结果：
- en: '![](img/00050.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00050.jpeg)'
- en: Ensemble methods, such as random forest, generally work well for classification
    problems and accuracy can be improved by ensembling with more trees. However,
    they come with some limitations, one of which is shown in the previous sample
    results for the random forest classifier. As is true for all decision tree-based
    models, the random forest model tends to overfit, especially when it tries to
    learn from many categorical variables. As you can see from the ROC curves for
    the random forest classifier, the gap between the train and test ROC curves is
    large, especially when compared to those for the Naive Bayes classifier. The Naive
    Bayes classifier with a minimum number of term occurrences threshold at 150 has
    almost no gap between the train and test ROC curves, whereas a random forest classifier
    at the same threshold shows a large gap between the two ROC curves. When dealing
    with such a dataset, where there are lots of categorical variables, we need to
    be careful about which model to choose and pay special attention to tuning the
    hyperparameters, such as `NumberOfTrees`, `CoverageRatio`, and `SampleRatio`*,*
    for a random forest model.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法，例如随机森林，通常在分类问题中表现良好，通过集成更多树可以提高准确率。然而，它们也有一些局限性，其中之一在前面的随机森林分类器示例结果中已经展示。对于所有基于决策树模型而言，随机森林模型倾向于过拟合，尤其是在它试图从许多分类变量中学习时。正如从随机森林分类器的ROC曲线中可以看到的，训练集和测试集ROC曲线之间的差距很大，尤其是与朴素贝叶斯分类器的ROC曲线相比。具有最小词频出现阈值150的朴素贝叶斯分类器在训练集和测试集ROC曲线之间几乎没有差距，而相同阈值下的随机森林分类器在两个ROC曲线之间显示出较大的差距。在处理存在大量分类变量的数据集时，我们需要小心选择模型，并特别注意调整超参数，例如`NumberOfTrees`、`CoverageRatio`和`SampleRatio`*，*以优化随机森林模型。
- en: Summary
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we built and trained more advanced classification models for
    Twitter sentiment analysis. We applied what we have learned in the previous chapter
    to a multi-class classification problem with more complex text data. We first
    started off by setting up our environment with the Stanford CoreNLP package that
    we used for tokenization, POS tagging, and lemmatization in the data preparation
    and analysis steps. Then, we transformed the raw Twitter dataset into a one-hot
    encoded matrix by tokenizing and lemmatizing the tweets. During this data preparation
    step, we also discussed how we could use Regex to group similar emoticons together
    and remove unnecessary text, such as URLs, Twitter IDs, and raw numbers, from
    tweets. We further analyzed the distribution of frequently used terms and emoticons
    in our data analysis step and we saw how lemmatization and grouping similar emoticons
    together help in reducing unnecessary noise in the dataset. With data and insights
    from previous steps, we experimented with building multi-class classification
    models using Naive Bayes and random forest classifiers. As we built these models,
    we covered a frequently used model validation technique, where we split a sample
    set into two subsets, the train set and validation set, and used the train set
    to fit a model and the validation set to evaluate the model performance. We also
    covered new model validation metrics, the ROC curve and AUC, which we can use
    to select the best and most optimal model among model candidates.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们为Twitter情感分析构建和训练了更高级的分类模型。我们将前一章学到的知识应用于一个具有更复杂文本数据的多元分类问题。我们首先通过设置我们的环境开始，使用斯坦福CoreNLP包进行分词、词性标注和词形还原，在数据准备和分析步骤中。然后，我们将原始Twitter数据集通过分词和词形还原转换为一个独热编码矩阵。在数据准备步骤中，我们还讨论了如何使用正则表达式将相似的表情符号分组，并从推文中移除不必要的文本，例如URL、Twitter
    ID和原始数字。在数据分析步骤中，我们进一步分析了常用术语和表情符号的分布，并看到词形还原和将相似的表情符号分组如何有助于减少数据集中的噪声。在之前的步骤中获取数据和洞察后，我们尝试使用朴素贝叶斯和随机森林分类器构建多元分类模型。在构建这些模型的过程中，我们介绍了一种常用的模型验证技术，即将样本集分为两个子集，训练集和验证集，使用训练集来拟合模型，使用验证集来评估模型性能。我们还介绍了新的模型验证指标，ROC曲线和AUC，我们可以使用这些指标在模型候选者中选择最佳和最优化模型。
- en: In the next chapter, we are going to switch gears and start building regression
    models where the target variables are continuous variables. We will use a foreign
    exchange rate dataset to build time series features and explore some other ML
    models for regression problems. We will also discuss how evaluating the performance
    of regression models is different from that of classification models.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将转换方向，开始构建回归模型，其中目标变量是连续变量。我们将使用外汇汇率数据集来构建时间序列特征，并探索一些其他用于回归问题的机器学习模型。我们还将讨论评估回归模型性能与分类模型性能的不同之处。
