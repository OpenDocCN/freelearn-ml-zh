- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Deep Learning, Generative AI, and LLMOps
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习、生成式AI和LLMOps
- en: The world is changing. Fast. At the time of writing in mid-2023, **machine learning**
    (**ML**) and **artificial intelligence** (**AI**) have entered the public consciousness
    in a way that even a few months ago seemed impossible. With the rollout of ChatGPT
    in late 2022, as well as a wave of new tools from labs and organizations across
    the world, hundreds of millions of people are now using ML solutions every day
    to create, analyze, and develop. On top of this, innovation seems to only be speeding
    up, with what seems like a new announcement of a record-beating model or new tool
    every day. ChatGPT is only one example of a solution that uses what is now known
    as **generative artificial intelligence** (**generative AI or GenAI**). While
    ChatGPT, Bing AI, and Google Bard are examples of text-based generative AI tools,
    there is also DALL-E and Midjourney in the image space and now a whole suite of
    multi-modal models combining these and other types of data. Given the complexity
    of the ecosystem that is evolving and the models that are being developed by leading
    AI labs around the world, it would be easy to feel overwhelmed. But fear not,
    as this chapter is all about answering the question, “What does this mean for
    me as a budding ML engineer?”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 世界正在快速变化。截至2023年中期，**机器学习**（**ML**）和**人工智能**（**AI**）以一种甚至几个月前看起来不可能的方式进入了公众意识。随着2022年底ChatGPT的推出，以及来自世界各地实验室和组织的全新工具的涌现，数亿人现在每天都在使用ML解决方案来创造、分析和开发。除此之外，创新似乎正在加速，每天都有新的记录打破模型或新工具的宣布。ChatGPT只是使用现在所知的**生成式人工智能**（**生成AI或GenAI**）的解决方案的一个例子。虽然ChatGPT、Bing
    AI和Google Bard是文本生成AI工具的例子，但在图像空间中还有DALL-E和Midjourney，现在还有一系列结合这些和其他类型数据的跨模态模型。鉴于正在演变的生态系统和全球领先AI实验室正在开发的模型，很容易感到不知所措。但不必担心，因为这一章完全是关于回答“这对作为新兴ML工程师的我意味着什么？”这个问题。
- en: In this chapter, we will take the same strategy as in the other chapters of
    the book and focus on the core concepts and on building solid foundations that
    you can use in your own projects for years to come. We will start with the fundamental
    algorithmic approach that has been at the heart of many cutting-edge developments
    in ML since the 2010s with a review of **deep learning**. We will then discuss
    how you can build and host your own deep learning models before moving on to GenAI,
    where we will explore the general landscape before going into a deep dive into
    the approach behind ChatGPT and other powerful text models, **Large Language Models**
    (**LLMs**).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将采取与本书其他章节相同的策略，专注于核心概念，并构建您可以在未来的项目中使用多年的坚实基础。我们将从自2010年代以来一直是许多ML前沿发展核心的基本算法方法开始，对**深度学习**进行回顾。然后，我们将讨论您如何构建和托管自己的深度学习模型，然后过渡到GenAI，在那里我们将探讨一般格局，然后深入探讨ChatGPT和其他强大文本模型背后的方法，**大型语言模型**（**LLMs**）。
- en: This will then transition smoothly into an exploration of how ML engineering
    and MLOps can be applied to LLMs, including a discussion of the new challenges
    this brings. This is such a new area that much of what we will discuss in this
    chapter will reflect my views and understanding at the time of writing. As an
    ML community, we are only now starting to define what best practice means for
    these models, so we are going to be contributing to this brave new world together
    in the next few pages. I hope you enjoy the ride!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将顺利过渡到探索如何将机器学习工程和MLOps应用于LLMs，包括讨论这带来的新挑战。这是一个如此新的领域，我们将在本章中讨论的大部分内容将反映我在写作时的观点和理解。作为一个机器学习社区，我们目前正在开始定义这些模型的最佳实践意味着什么，所以我们将在这接下来的几页中共同为这个勇敢的新世界做出贡献。我希望你们享受这次旅程！
- en: 'We will cover all of this in the following sections:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下章节中涵盖所有这些内容：
- en: Going deep with deep learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入学习深度学习
- en: Going big with LLMs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLMs进行大规模开发
- en: Building the future with LLMOps
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLMOps构建未来
- en: Going deep with deep learning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入学习深度学习
- en: In this book, we have worked with relatively “classical” ML models so far, which
    rely on a variety of different mathematical and statistical approaches to learn
    from data. These algorithms in general are not modeled on any biological theory
    of learning and are at their heart motivated by finding procedures to explicitly
    optimize the loss function in different ways. A slightly different approach that
    the reader will likely be aware of, and that we met briefly in the section on
    *Learning about learning* in *Chapter 3*, *From Model to Model Factory*, is that
    taken by **Artificial Neural Networks** (**ANNs**), which originated in the 1950s
    and were based on idealized models of neuronal activity in the brain. The core
    concept of an ANN is that through connecting relatively simple computational units
    called neurons or nodes (modeled on biological neurons), we can build systems
    that can effectively model any mathematical function (see the information box
    below for more details). The neuron in this case is a small component of the system
    that will return an output based on an input and the transformation of that input
    using some pre-determined mathematical formula. They are inherently non-linear
    and when acting in combination can very quickly begin to model quite complex data.
    Artificial neurons can be thought of as being arranged in layers, where neurons
    from one layer have connections to neurons in the next layer. At the level of
    small neural networks with not many neurons and not many layers, many of the techniques
    we have discussed in this book around retraining and drift detection still apply
    without modification. When we get to ANNs with many layers and neurons, so-called
    **Deep Neural Networks** (**DNNs**), then we have to consider some additional
    concepts, which we will cover in this section.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们迄今为止一直使用相对“经典”的机器学习模型，这些模型依赖于各种不同的数学和统计方法来从数据中学习。这些算法在一般情况下并不是基于任何学习生物学理论，其核心动机是找到不同的方法来显式优化损失函数。读者可能已经了解的一个稍微不同的方法，我们在第3章“从模型到模型工厂”中关于*学习学习*的部分简要介绍过，那就是**人工神经网络**（**ANNs**）所采用的方法，它起源于20世纪50年代，并基于大脑中神经元活动的理想化模型。人工神经网络的核心概念是通过连接相对简单的计算单元，称为神经元或节点（基于生物神经元建模），我们可以构建能够有效模拟任何数学函数的系统（下方的信息框中提供了更多细节）。在这个案例中，神经元是系统的一个小组成部分，它将根据输入以及使用某些预先确定的数学公式对输入进行转换来返回输出。它们本质上是非线性的，当它们组合在一起时，可以非常快速地开始模拟相当复杂的数据。人工神经元可以被认为是按层排列的，其中一层的神经元与下一层的神经元相连。在具有不多神经元和不多层的较小神经网络层面，我们在这本书中讨论的许多关于重新训练和漂移检测的技术仍然适用，无需修改。当我们达到具有许多层和神经元的所谓**深度神经网络**（**DNNs**）时，我们必须考虑一些额外的概念，这些概念我们将在本节中介绍。
- en: The ability of neural networks to represent a huge variety of functions has
    a theoretical basis in what are known as **Universal Approximation Theorems**.
    These are rigorous mathematical results that prove that multilayer neural networks
    can approximate classes of mathematical functions to arbitrary levels of precision.
    These results don’t say which specific neural networks will do this, but they
    tell us that with enough hidden neurons or nodes, we can be sure that with enough
    data we should be able to represent our target function. Some of the most important
    results for these theorems were established in the late 1980s in papers like *Hornik,
    K., Stinchcombe, M. and White, H. (1989) “Multilayer feedforward networks are
    universal approximators”, Neural Networks, 2(5), pp. 359–366* and *Cybenko, G.
    (1989) “Approximation by superpositions of a sigmoidal function”, Mathematics
    of Control, Signals, and Systems, 2(4), pp. 303–314*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络能够表示各种各样函数的能力，在所谓的**万能逼近定理**中有着理论基础。这些是严格的数学结果，证明了多层神经网络可以逼近数学函数类，达到任意精度的近似。这些结果并没有说明哪些具体的神经网络会做到这一点，但它们告诉我们，只要有足够的隐藏神经元或节点，我们就可以确信，只要有足够的数据，我们应当能够表示我们的目标函数。这些定理中一些最重要的结果是在20世纪80年代末通过像*Hornik,
    K., Stinchcombe, M. and White, H. (1989) “Multilayer feedforward networks are
    universal approximators”, Neural Networks, 2(5), pp. 359–366*和*Cybenko, G. (1989)
    “Approximation by superpositions of a sigmoidal function”, Mathematics of Control,
    Signals, and Systems, 2(4), pp. 303–314*这样的论文中确立的。
- en: DNNs have taken the world by storm in the last few years. From computer vision
    to natural language processing and from StableDiffusion to ChatGPT, there are
    now countless amazing examples of DNNs doing what was once considered the sole
    purview of humans. The in-depth mathematical details of deep learning models are
    covered in so much literature elsewhere, such as in the classic *Deep Learning*
    by Goodfellow, Bengio, Courville, MIT Press, 2016, that we would never be able
    to do them justice here. Although covering the detailed theory is far beyond the
    scope of this chapter, I will attempt to provide an overview of the main concepts
    and techniques you need in order to have a good working knowledge and to be able
    to start using these models in your ML engineering projects.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，深度神经网络（DNNs）风靡全球。从计算机视觉到自然语言处理，从StableDiffusion到ChatGPT，现在有无数令人惊叹的例子表明DNNs正在做以前被认为是人类专属的事情。深度学习模型的深入数学细节在其他许多文献中都有涉及，例如Goodfellow、Bengio、Courville的经典著作《深度学习》，由麻省理工学院出版社于2016年出版，我们在这里无法充分展示。尽管详细的理论超出了本章的范围，但我将尝试提供一个概述，包括你需要了解的主要概念和技术，以便你能够具备良好的工作知识，并能够开始在你的机器学习工程项目中使用这些模型。
- en: As mentioned, ANNs are based on ideas borrowed from biology, and just like in
    a biological brain, the ANN is built up of many individual *neurons*. Neurons
    can be thought of as providing the unit of computation in the ANN. The neuron
    works by taking multiple inputs and then combining them in a specified recipe
    to produce a single output, which can then act as one of the inputs for another
    neuron or as part of the overall model’s output. The inputs to the neurons in
    a biological setting flow along *dendrites* and the outputs are channeled along
    *axons*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如所述，人工神经网络基于从生物学中借鉴的思想，就像在生物大脑中一样，ANN由许多单个**神经元**组成。神经元可以被视为在ANN中提供计算单元。神经元通过接收多个输入并将它们按照特定的配方组合起来以产生单个输出来工作，这个输出可以随后作为另一个神经元的输入或作为整体模型输出的部分。在生物环境中，神经元的输入沿着**树突**流动，输出则沿着**轴突**传导。
- en: But how do the inputs get transformed into the outputs? There are a few concepts
    we need to bring together to understand this process.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但输入是如何转换为输出的呢？我们需要将几个概念结合起来才能理解这个过程。
- en: '**Weight**: Assigned to each connection between the neurons in the network
    is a numerical value that can be thought of as the “strength” of the connection.
    During the training of the neural network, the weights are one of the sets of
    values that are altered to minimize the loss. This is in line with the explanation
    of model training provided in *Chapter 3*, *From Model to Model Factory*.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重**：网络中每个神经元之间的连接都分配了一个数值，这个数值可以被视为连接的“强度”。在神经网络训练过程中，权重是用于最小化损失的一组值之一。这与*第三章*中提供的模型训练解释相一致，即*从模型到模型工厂*。'
- en: '**Bias**: Every neuron in the network is given an another parameter that acts
    as an offset to the activation (defined below). This number is also updated during
    training and it gives the neural network more *degrees of freedom* to fit to the
    data. You can think of the bias as shifting the level at which the neuron will
    “fire” (or produce a certain output) and so having this as a variable value means
    there is more adaptability to the neuron.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差**：网络中的每个神经元都给定了一个额外的参数，该参数作为激活（以下定义）的偏移量。这个数值在训练过程中也会更新，它为神经网络提供了更多的**自由度**来拟合数据。你可以将偏差视为改变神经元“放电”（或产生特定输出）的水平，因此作为变量值意味着神经元有更多的适应性。'
- en: '**Inputs**: These can be thought of as the raw data points that are fed to
    the neuron before we take into account the weights or the bias. If the neuron
    is being fed features based on the data, then the inputs are the feature values;
    if the neuron is being fed outputs from other neurons, then those are the values
    in that case.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**：这些可以被视为在考虑权重或偏差之前馈送到神经元的原始数据点。如果神经元是根据数据提供特征，则输入是特征值；如果神经元是接收来自其他神经元的输出，那么这些就是那种情况下的值。'
- en: '**Activation**: The neuron in an ANN receives multiple inputs; the activation
    is the linear combination of the inputs multiplied by the appropriate weights
    plus the bias term. This translates the multiple pieces of incoming data into
    a single number that can then be used to determine what the neuron’s output should
    be.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活**：ANN中的神经元接收多个输入；激活是输入的线性组合，乘以适当的权重加上偏差项。这把多块传入数据转换成一个单一的数值，然后可以用来确定神经元的输出应该是什么。'
- en: '**Activation function**: The activation is just a number, but the activation
    function is how we decide what that number means for the neuron. There is a variety
    of activation functions that are very popular in deep learning today but the important
    characteristic is that when this function acts on the activation value, it produces
    a number that is the output of the neuron or node.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**：激活只是一个数字，但激活函数是我们决定这个数字对神经元意味着什么的方式。目前深度学习中非常流行的激活函数有很多，但重要的特征是，当这个函数作用于激活值时，它产生一个数字，这是神经元或节点的输出。'
- en: 'These concepts are brought together diagrammatically in *Figure 7.1*. Deep
    learning models do not have a strict definition, but for our purposes, we can
    consider an ANN as deep as soon as it consists of three or more layers. This then
    means we must define some of the important characteristics of these layers, which
    we will do now:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念在*图7.1*中以图表形式呈现。深度学习模型没有严格的定义，但就我们的目的而言，一旦一个人工神经网络（ANN）由三个或更多层组成，我们就可以认为它是深层的。这意味着我们必须定义这些层的一些重要特征，我们现在就来做这件事：
- en: '**Input layer**: This is the first layer of neurons that has as its input the
    raw data or prepared features created from the data.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：这是第一个神经元层，其输入是原始数据或从数据中创建的预处理特征。'
- en: '**Hidden layers**: These are the layers between the input and output layers,
    and can be thought of as where the main bulk of non-linear transformations of
    the data are performed. This is often simply because there are lots of hidden
    layers with lots of neurons! The way the neurons in the hidden layers are organized
    and connected are key parts of the *neural network architecture*.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：这些是输入层和输出层之间的层，可以认为是在这里执行数据的主要非线性变换。这通常是因为有很多隐藏层和神经元！隐藏层中神经元的组织和连接是*神经网络架构*的关键部分。'
- en: '**Output layer**: The output layer is the one responsible for translating the
    outcome of the transformations that have been carried out in the neural network
    into a result that can be interpreted appropriately for the problem at hand. As
    an example, if we are using a neural network to classify an image, we need the
    final layer to output either a 1 or 0 for the specified class or we could have
    it output a series of probabilities for different classes.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：输出层负责将神经网络中执行过的变换的结果转换为可以适当解释的结果。例如，如果我们使用神经网络来分类图像，我们需要最终层输出指定类别的1或0，或者我们可以让它输出不同类别的概率序列。'
- en: These concepts are useful background, but how do we start working with them
    in Python? The two most popular deep learning frameworks in the world are Tensorflow,
    released by Google Brain in 2015, and PyTorch, released by Meta AI in 2016\. In
    this chapter, we will focus on examples using PyTorch, but many of the concepts
    apply equally well to TensorFlow with some modifications.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念是有用的背景知识，但我们在Python中如何开始使用它们呢？世界上两个最受欢迎的深度学习框架是Tensorflow，由谷歌大脑在2015年发布，以及PyTorch，由Meta
    AI在2016年发布。在本章中，我们将专注于使用PyTorch的示例，但许多概念在经过一些修改后同样适用于TensorFlow。
- en: '![](img/B19525_07_01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19525_07_01.png)'
- en: 'Figure 7.1: A schematic representation of a “neuron” in an ANN and how this
    takes input data, x, and transforms it into output, y.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：人工神经网络（ANN）中“神经元”的示意图以及它如何接收输入数据x并将其转换为输出y。
- en: Getting started with PyTorch
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用PyTorch
- en: 'First, if you haven’t already, install PyTorch. You can do this by following
    the PyTorch documentation at [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/),
    for installing locally on Macbook, or use:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果你还没有安装PyTorch，你可以通过遵循[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)上的PyTorch文档来安装，用于在Macbook上本地安装，或者使用：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'There are some importance concepts and features of the PyTorch API that are
    useful to bear in mind when using PyTorch:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用PyTorch时，有一些重要的概念和特性是值得记住的：
- en: '`torch.Tensor`: Tensors are mathematical objects that can be represented by
    multi-dimensional arrays and are core components of any modern deep learning framework.
    The data we feed into the network should be cast as a tensor, for example:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.Tensor`：张量是可以通过多维数组表示的数学对象，并且是任何现代深度学习框架的核心组件。我们输入网络的数据应该被转换为张量，例如：'
- en: '[PRE1]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`torch.nn`: This is the main module used to define our neural network models.
    For example, we can use this to define a basic classification neural network containing
    three hidden layers, each with a **Rectified Linear Unit** (**ReLU**) activation
    function. When defining a model in PyTorch using this method, you should also
    write a method called `forward`, which defines how data is passed through the
    network during training. The following code shows how you can build a basic neural
    network inside a class that inherits from the `torch.nn.Module` object. This network
    has four linear layers with ReLU activation functions and a simple forward-pass
    function:'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.nn`: 这是定义我们的神经网络模型所使用的主要模块。例如，我们可以使用它来定义一个包含三个隐藏层的基本分类神经网络，每个隐藏层都有一个**修正线性单元**（**ReLU**）激活函数。当使用这种方法在PyTorch中定义模型时，你还应该编写一个名为`forward`的方法，该方法定义了在训练过程中数据如何通过网络。以下代码展示了如何在继承自`torch.nn.Module`对象的类中构建一个基本神经网络。这个网络有四个线性层，每个层都有ReLU激活函数，以及一个简单的正向传递函数：'
- en: '[PRE2]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Loss functions**: In the `torch.nn` module, there is a series of loss functions
    that can be used for training the network. A popular choice is the cross-entropy
    loss, but there are many more to choose from in the documentation:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数**: 在`torch.nn`模块中，有一系列损失函数可用于训练网络。一个流行的选择是交叉熵损失，但在文档中还有更多可供选择：'
- en: '[PRE3]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`torch.optim.Optimizer`: This is the base class for all optimizers in PyTorch.
    This allows for the implementation of most of the optimizers discussed in *Chapter
    3*, *From Model to Model Factory*.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.optim.Optimizer`: 这是PyTorch中所有优化器的基类。这允许实现*第3章*，“从模型到模型工厂”中讨论的大多数优化器。'
- en: 'When defining the optimizer in PyTorch, in most cases, you pass in the instantiated
    model’s parameters and then the relevant parameters for the particular optimizer.
    For example, if we define an Adam optimizer with a learning rate of `0.001`, this
    is as simple as:'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在PyTorch中定义优化器时，在大多数情况下，你需要传入实例化模型的参数以及特定优化器的相关参数。例如，如果我们定义一个学习率为`0.001`的Adam优化器，这就像这样简单：
- en: '[PRE4]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`torch.autograd`: Recall that training an ML model is really an optimization
    process that leverages a combination of linear algebra, calculus and some statistics.
    PyTorch performs model optimization using *automatic differentiation*, a method
    for converting the problem of finding partial derivatives of a function into the
    application of a series of primitives that is easy to compute but still results
    in calculating the differentiation to good precision. This is not to be confused
    with *finite differences or symbolic differentiation*. You call this implicitly
    by using a loss function and calling the `backward` method, which uses autograd
    to calculate the gradients for the weight updates in each epoch; this is then
    used in the optimizer by calling `optimizer.step()`. During a training run, it
    is important to reset any input tensors as tensors in PyTorch are mutable (operations
    change their data), and it is important to reset any gradients calculated in the
    optimizer as well using `optimizer.zero_grad()`. Given this, an example training
    run with five hundred epochs would look like the following:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.autograd`: 回想一下，训练一个机器学习模型实际上是一个利用线性代数、微积分和一些统计学的优化过程。PyTorch通过使用*自动微分*来执行模型优化，这是一种将函数的偏导数求解问题转化为一系列易于计算的原语应用的方法，尽管如此，它仍然能够以良好的精度计算微分。这不同于*有限差分法或符号微分法*。你可以通过使用损失函数并调用`backward`方法来隐式地调用它，该方法使用autograd来计算每个epoch中权重更新的梯度；然后通过调用`optimizer.step()`在优化器中使用这些梯度。在训练过程中，重置任何输入张量是很重要的，因为在PyTorch中张量是可变的（操作会改变其数据），同样，使用`optimizer.zero_grad()`重置优化器中计算的任何梯度也很重要。基于此，一个包含五百个epoch的示例训练运行如下：'
- en: '[PRE5]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`torch.save and torch.load`: You can probably guess what these methods do from
    their names! But it is still important to show how to save and load your PyTorch
    models. When training deep learning models, it is also important to save the model
    periodically during the training process, as this often takes a long time. This
    is called “checkpointing” and means that you can pick up where you left off if
    anything goes wrong during the training runs. To save a PyTorch checkpoint, we
    can add syntax like the following to the training loop:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.save`和`torch.load`: 你可能可以从它们的名字中猜出这些方法的作用！但仍然重要的是要展示如何保存和加载你的PyTorch模型。在训练深度学习模型时，在训练过程中定期保存模型也很重要，因为这通常需要很长时间。这被称为“检查点”，意味着如果在训练运行中出现任何问题，你可以从上次停止的地方继续。为了保存PyTorch检查点，我们可以在训练循环中添加如下语法：'
- en: '[PRE6]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To then load in the model, you need to initialize another instance of your
    neural network class and of an optimizer object before reading in their states
    from the `checkpoint` object:'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要加载模型，你需要初始化你的神经网络类和优化器对象的另一个实例，然后从`checkpoint`对象中读取它们的状态：
- en: '[PRE7]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`model.eval()`and`model.train()`: Once you have loaded in a PyTorch checkpoint,
    you need to set the model to the appropriate mode for the task you want to perform
    or there may be downstream issues. For example, if you want to perform testing
    and validation or you want to perform inference on new data with your model, then
    you need to call `model.eval()` before using it. This freezes any batch normalization
    or dropout layers you have included as they calculate statistics and perform updates
    during training that you do not want to be active during testing. Similarly, `model.train()`
    ensures these layers are ready to continue performing updates as expected during
    a training run.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.eval()`和`model.train()`: 一旦你加载了PyTorch的检查点，你需要将模型设置为执行任务所需的适当模式，否则可能会出现下游问题。例如，如果你想进行测试和验证，或者你想使用你的模型对新数据进行推理，那么在使用模型之前，你需要调用`model.eval()`。这将冻结任何包含的批归一化或dropout层，因为它们在训练期间计算统计数据和执行更新，而这些更新在测试期间你不希望是活跃的。同样，`model.train()`确保这些层在训练运行期间可以继续按预期执行更新。'
- en: 'It should be noted that there is a more extreme setting than `model.eval()`
    where you can entirely turn off any autograd functionality in your context by
    using the following syntax:'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应该注意的是，有一种比`model.eval()`更极端的设置，你可以使用以下语法完全关闭你上下文中的任何autograd功能：
- en: '[PRE8]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This can give you added performance on inference but should only be used if
    you are certain that you do not need any gradient or tensor updates tracked or
    performed.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这可以在推理时提供额外的性能，但应该只在确定你不需要任何梯度或张量更新跟踪或执行时使用。
- en: '**Evaluation**: If you wanted to test the model we have just trained in the
    example above you could calculate an accuracy using something like the syntax
    below, but any of the methods we have discussed in this book for model validation
    apply!'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估**：如果你想测试上面示例中我们刚刚训练的模型，你可以使用类似以下语法计算准确率，但本书中讨论的任何模型验证方法都适用！'
- en: '[PRE9]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: And with that, you can now build, train, save, load and evaluate your first
    PyTorch model. We will now discuss how we take this further by considering some
    of the challenges of taking deep learning models into production.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，你现在可以构建、训练、保存、加载和评估你的第一个PyTorch模型。我们将现在讨论如何通过考虑将深度学习模型投入生产的挑战来进一步扩展。
- en: Scaling and taking deep learning into production
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规模化和将深度学习投入生产
- en: 'Now we will move on to how to run deep learning models in a production system.
    To do this, we need to consider a few specific points that mark out DNNs from
    the other classical ML algorithms:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将转向如何在生产系统中运行深度学习模型。为此，我们需要考虑一些特定的点，这些点将DNN与其他经典机器学习算法区分开来：
- en: '**They are data-hungry**: DNNs often require relatively large amounts of data
    compared to other ML algorithms, due to the fact that they are performing an extremely
    complex multi-dimensional optimization, with the parameters of each neuron adding
    degrees of freedom. This means that for you to consider training a DNN from scratch,
    you have to do the leg work upfront to make sure you have enough data and that
    it is of a suitable variety to adequately train the model. The data requirements
    also typically mean that you need to be able to store a lot of data in memory
    as well, so this often has to be thought about ahead of time.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它们是数据饥渴的**：与其他机器学习算法相比，DNN通常需要相对大量的数据，这是因为它们正在执行极其复杂的多元优化，每个神经元的参数增加了自由度。这意味着为了从头开始训练DNN，你必须提前做一些工作，确保你有足够的数据，并且数据种类适合充分训练模型。数据需求通常还意味着你需要能够将大量数据存储在内存中，因此这通常需要提前考虑。'
- en: '**Training is more complex**: This point is related to the above but is subtly
    different. The very complex non-linear optimization problem we are solving means
    that during training, there are often many ways that the model can “get lost”
    and reach a sub-optimal local minimum. Techniques like the *checkpointing* example
    we described in the previous section are widespread in the deep learning community
    as you will often have to stop training at some step when the loss is not going
    in the right direction or stagnating, roll back, and try something different.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练更加复杂**：这一点与上面提到的内容相关，但有所不同。我们正在解决的非常复杂的非线性优化问题意味着在训练过程中，模型往往有多种方式“迷失方向”并达到次优局部最小值。正如我们在上一节中描述的*checkpointing*示例，在深度学习社区中这些技术非常普遍，因为你经常需要在损失没有朝着正确的方向移动或停滞不前时停止训练，回滚，并尝试不同的方法。'
- en: '**You have a new choice to make, the model architecture**: DNNs are also very
    different from classical ML algorithms because now you do not just have to worry
    about a few hyperparameters, but you also need to decide the architecture or shape
    of your neural network. This is often a non-trivial exercise and can require detailed
    knowledge of neural networks. Even if you are working with a standard architecture
    like the Transformer architecture (see *Figure 7.2*), you should still have a
    solid grasp of what all the components are doing in order to effectively diagnose
    and resolve any issues. Techniques like automated architecture search as was discussed
    in *Chapter 3* in the section on *Learning about learning* can help speed up architecture
    design but sound foundational knowledge is still important.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**你面临一个新的选择，即模型架构**：深度神经网络（DNNs）也与经典机器学习算法有很大不同，因为你现在不仅需要担心几个超参数，还需要决定你的神经网络架构或形状。这通常是一项非同小可的练习，可能需要深入了解神经网络。即使你使用的是标准的架构，如Transformer架构（见图7.2），你也应该对所有组件的功能有一个稳固的理解，以便有效地诊断和解决任何问题。正如在第3章“关于学习的知识”部分讨论的自动架构搜索等技术可以帮助加快架构设计，但坚实的知识基础仍然很重要。'
- en: '**Explainability is inherently harder**: A criticism that has been leveled
    at DNNs over the past few years is that their results can be very hard to explain.
    This is to be expected since the point is very much that DNN abstracts away a
    lot of the specifics of any problem into a more abstract approach. This can be
    fine in many scenarios but has now led to several high-profile cases of DNNs exhibiting
    undesired behavior like racial or gender bias, which can then be harder to explain
    and remediate. A challenge also arises in heavily regulated industries, like healthcare
    or finance, where your organization may have a legal duty to be able to evidence
    why a specific decision was made. If you used a DNN to help make this decision,
    this can often be quite challenging.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性固有的更难**：过去几年中，针对深度神经网络（DNNs）的一个批评是，其结果可能非常难以解释。这是可以预料的，因为重点确实在于DNN将任何问题的许多具体细节抽象化成一个更抽象的方法。这在许多情况下可能没问题，但现在已导致几个高调案例，DNNs表现出不希望的行为，如种族或性别偏见，这可能导致更难解释和补救。在高度监管的行业，如医疗保健或金融，你的组织可能负有法律义务能够证明为什么做出了特定的决定。如果你使用DNN来帮助做出这个决定，这通常会相当具有挑战性。'
- en: '![](img/B19525_07_02.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19525_07_02.png)'
- en: 'Figure 7.2: The Transformer architecture as originally published in the paper
    “Attention is all you need” by Google Brain, https://arxiv.org/abs/1706.03762.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：Transformer架构如图所示，最初在谷歌大脑发表的论文“Attention is all you need”中提出，https://arxiv.org/abs/1706.03762。
- en: Given all of this, what are some of the main things we should consider when
    using deep learning models for our ML-engineered systems? Well, one of the first
    things you can do is use existing pre-trained models, rather than train your own.
    This obviously comes with some risks around ensuring that the model and the data
    it was fed were of sufficient quality for your application, so always proceed
    with caution and do your due diligence.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些，我们在使用深度学习模型为我们的机器学习系统时应该考虑哪些主要事项呢？嗯，你可以做的第一件事是使用现有的预训练模型，而不是自己训练。这显然带来了一些风险，即确保模型及其提供的数据对你的应用来说是足够高质量的，所以总是要谨慎行事，并做好你的尽职调查。
- en: In many cases, however, this approach is absolutely fine as we may be using
    a model that has been put through its paces in quite a public way and it may be
    known to be performant on the tasks we wish to use it for. Furthermore, we may
    have a use case where we are willing to accept the operational risk of importing
    and using this pre-existing model, contingent on our own testing. Let us assume
    we are in such an example now, and we want to build a basic pipeline to summarize
    some text conversations between clients and employees of a fictional organization.
    We can do this using an off-the-shelf transformer model, like that shown in *Figure
    7.2*, from the Hugging Face `transformers` library.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在许多情况下，这种方法绝对是可行的，因为我们可能正在使用一个以相当公开的方式经过测试的模型，并且它可能在我们希望使用的任务上已知表现良好。此外，我们可能有一个用例，我们愿意接受导入和使用这个预存模型的运营风险，前提是我们自己的测试。让我们假设我们现在处于这样一个例子中，我们想要构建一个基本的管道来总结一个虚构组织客户和员工之间的文本对话。我们可以使用现成的转换器模型，如*图
    7.2*所示，来自 Hugging Face 的 `transformers` 库。
- en: 'All you need to get started is to know the name of the model you want to download
    from the Hugging Face model server; in this case, we will use the Pegasus text
    summarization model. Hugging Face provides a “`pipeline`" API to wrap around the
    model and make it easy to use:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用，你只需要知道你想要从 Hugging Face 模型服务器下载的模型名称；在这种情况下，我们将使用 Pegasus 文本摘要模型。Hugging
    Face 提供了一个“`pipeline`" API，用于包装模型并使其易于使用：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Performing our first deep learning model inference is then as easy as just
    passing in some inputs to this pipeline. So, for the fictional bot-human interaction
    described above, we can just pass in some example text and see what this returns.
    Let’s do this to summarize a fictional conversation between a customer and a chatbot,
    where the customer is trying to get more information on an order they have placed.
    The conversation is shown below:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 执行我们的第一个深度学习模型推理就像只是将一些输入传递给这个管道一样简单。因此，对于上面描述的虚构人机交互，我们只需传递一些示例文本，看看它返回什么。让我们这样做，以总结一个虚构的客户和聊天机器人之间的对话，其中客户正在尝试获取他们已下订单的更多信息。对话如下所示：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will then feed this conversation into the summarizer `pipeline` object,
    and print the result:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将把这个对话输入到摘要器 `pipeline` 对象中，并打印结果：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The result shows that the model has actually given a good summary of the nature
    of this interaction, highlighting how easy it is to start to do something that
    would probably have been very difficult or even impossible before the deep learning
    revolution.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，该模型实际上已经很好地总结了这种交互的本质，突出了在深度学习革命之前可能非常困难或甚至不可能开始做的事情现在变得多么容易。
- en: We have just seen an example of using a pre-trained transformer model to perform
    some specific task, in this case text summarization, without any need for the
    model to be updated based on exposure to new data. In the next section, we will
    explore what to do when you want to update the model based on your own data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到了一个使用预训练的转换器模型来执行某些特定任务的例子，在这种情况下是文本摘要，而无需根据新数据更新模型。在下一节中，我们将探讨当你想要根据你自己的数据更新模型时应该做什么。
- en: Fine-tuning and transfer learning
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调和迁移学习
- en: In the previous section, we showed how easy it was to get started building solutions
    with existing deep learning models if ones could be found that were appropriate
    to your task. A good question to ask ourselves, however, is “What can I do if
    these models are not exactly right for my specific problem?” This is where the
    concepts of **fine-tuning** and **transfer learning** come in. Fine-tuning is
    when we take an existing deep learning model and then continue training the model
    on some new data. This means we are not starting from scratch and so can arrive
    at an optimized network far faster. Transfer learning is when we freeze most of
    the neural network’s state and retrain the last layer(s) with new data in order
    to perform some slightly different task or perform the same task in a way that
    is more appropriate to our problem. In both of these cases, this usually means
    that we can keep many of the powerful features of the original model, such as
    its feature representations, but start to adapt it for our specific use case.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们展示了如果能够找到适合您任务的现有深度学习模型，开始构建解决方案是多么容易。然而，一个值得我们自问的好问题是：“如果这些模型并不完全适合我的具体问题，我能做什么？”这就是**微调**和**迁移学习**概念发挥作用的地方。微调是指我们取一个现有的深度学习模型，然后在一些新数据上继续训练该模型。这意味着我们不是从头开始，因此可以更快地达到一个优化的网络。迁移学习是指我们冻结神经网络的大部分状态，并使用新数据重新训练最后几层，以便执行一些稍微不同的任务，或者以更适合我们问题的方法执行相同的任务。在这两种情况下，这通常意味着我们可以保留原始模型中的许多强大功能，例如其特征表示，但开始为我们的特定用例进行调整。
- en: To make this more concrete, we will now walk through an example of transfer
    learning in action. Fine-tuning can follow a similar process but just does not
    involve the adaptations to the neural network that we will implement. We will
    use the Hugging Face `datasets` and `evaluate` packages in this example, which
    will show how we can use a base **Bidirectional Encoder Representations from Transformers**
    (**BERT**) model and then use transfer learning to create a classifier that will
    estimate the star rating of reviews written in English on the Multilingual Amazon
    Reviews Corpus ([https://registry.opendata.aws/amazon-reviews-ml/](https://registry.opendata.aws/amazon-reviews-ml/)).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个例子更加具体，我们现在将演示一个迁移学习在实际中的应用示例。微调可以遵循类似的过程，但并不涉及我们将要实施的神经网络调整。在这个例子中，我们将使用Hugging
    Face的`datasets`和`evaluate`包，这将展示我们如何使用基础**双向编码器表示从Transformer**（**BERT**）模型，然后使用迁移学习来创建一个分类器，该分类器将估计在多语言亚马逊评论语料库（[https://registry.opendata.aws/amazon-reviews-ml/](https://registry.opendata.aws/amazon-reviews-ml/)）中用英语撰写的评论的星级评分。
- en: '*Figure 7.3* shows an example rating from this dataset:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.3*展示了该数据集的一个示例评分：'
- en: '![](img/B19525_07_03.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19525_07_03.png)'
- en: 'Figure 7.3: This shows an example review and star rating from the Multilingual
    Amazon Reviews Corpus.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：这展示了来自多语言亚马逊评论语料库的一个示例评论和星级评分。
- en: Although we have used the BERT model in the following example, there are many
    variants that will work with the same example, such as DistilBERT or AlBERT, which
    are smaller models which aim to be quicker to train and to retain most of the
    performance of the original BERT model. You can play around with all of these,
    and may even find these models are faster to download due to their reduced size!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在以下示例中使用了BERT模型，但还有许多变体可以与相同的示例一起工作，例如DistilBERT或AlBERT，这些是更小的模型，旨在更快地训练并保留原始BERT模型的大部分性能。您可以尝试所有这些，甚至可能会发现这些模型由于尺寸减小而下载速度更快！
- en: 'To start our transfer learning example:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始我们的迁移学习示例：
- en: 'First, we can use the `datasets` package to retrieve the dataset. We will use
    the concept of “configurations” and “splits” available for Hugging Face datasets,
    which specify specific subsets of the data and whether you want the train, test,
    or validate splits of the data. For this case, we want the English reviews and
    we will use the train split of the data initially. *Figure 7.3* shows an example
    record from the dataset. The data is retrieved with the following syntax:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们可以使用`datasets`包来检索数据集。我们将使用Hugging Face数据集提供的“配置”和“拆分”概念，这些概念指定了数据的具体子集以及您是否想要数据的训练、测试或验证拆分。对于这个案例，我们想要英语评论，并且最初将使用数据的训练拆分。*图7.3*展示了数据集的一个示例记录。数据检索的语法如下：
- en: '[PRE14]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The next step is to tokenize the dataset. To do this, we will use the `AutoTokenizer`
    that pairs with the BERT model we will use. Before we pull in that specific tokenizer,
    let’s write a function that will use the selected tokenizer to transform the dataset.
    We will also define the logic to take the dataset and make it of the right form
    for use in later PyTorch processes. I have also added an option to downsample
    the data for testing:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是标记化数据集。为此，我们将使用与我们将使用的 BERT 模型配对的 `AutoTokenizer`。在我们引入那个特定的分词器之前，让我们编写一个函数，该函数将使用所选的分词器来转换数据集。我们还将定义将数据集转换为适合在后续
    PyTorch 过程中使用的形式的逻辑。我还添加了一个选项来对测试数据进行下采样：
- en: '[PRE15]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we need to create the PyTorch `dataloader` for feeding the data into
    the model:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建 PyTorch `dataloader` 以将数据输入到模型中：
- en: '[PRE16]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Before we define the logic for training the model, it will be useful to write
    a helper function for defining the learning scheduler and the optimizer for the
    training run. This can then be called in our training function, which we will
    define in the next step. We will use the AdamW optimizer in this example:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们定义训练模型的逻辑之前，编写一个用于定义学习调度器和训练运行优化器的辅助函数将很有用。然后我们可以在我们的训练函数中调用它，我们将在下一步定义。在这个例子中，我们将使用
    AdamW 优化器：
- en: '[PRE17]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can now define the model that we want to train using transfer learning.
    The `transformers` library from Hugging Face provides a very helpful wrapper to
    help you alter the classification head of a neural network based on a core BERT
    model. We instantiate this model and pass in the number of classes, which implicitly
    creates an update to the neural network architecture to give the logits for each
    of the classes upon running a prediction. When running inference, we will then
    take the class corresponding to the maximum of these logits as the inferred class.
    First, let’s define the logic for training the model in a function:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以定义我们想要使用迁移学习训练的模型。Hugging Face 的 `transformers` 库提供了一个非常有用的包装器，可以帮助您根据核心
    BERT 模型更改神经网络的分类头。我们实例化这个模型并传入类别数，这隐式地更新了神经网络架构，以便在运行预测时为每个类别提供 logits。在运行推理时，我们将取这些
    logits 中的最大值对应的类别作为推断类别。首先，让我们在函数中定义训练模型的逻辑：
- en: '[PRE18]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we can call all of these methods together to grab the tokenizer, pull
    in the dataset, transform it, define the model, configure the learning scheduler
    and optimizer, and finally perform the transfer learning to create the final model:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以调用所有这些方法来获取分词器，引入数据集，转换它，定义模型，配置学习调度器和优化器，并最终执行迁移学习以创建最终模型：
- en: '[PRE19]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can then evaluate the performance of the model on the test split of the
    data using the Hugging Face `evaluate` package or any method we like. Note that
    in the example below, we call `model.eval()` so that the model is in evaluation
    mode as discussed previously:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用 Hugging Face 的 `evaluate` 包或任何我们喜欢的方法来评估模型在数据测试分割上的性能。注意，在下面的示例中，我们调用
    `model.eval()` 以使模型处于评估模式，正如之前讨论的那样：
- en: '[PRE20]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will return a dictionary with the value of the calculated metric like
    that shown below:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将返回一个包含计算出的指标值的字典，如下所示：
- en: '[PRE21]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: And that is how you can use PyTorch and the Hugging Face `transformers` library
    to perform transfer learning.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是您如何使用 PyTorch 和 Hugging Face 的 `transformers` 库来执行迁移学习。
- en: 'The Hugging Face `transformers` library also now provides a very powerful Trainer
    API to help you perform fine-tuning in a more abstract way. If we take the same
    tokenizer and model from the previous examples, to use the Trainer API, we can
    just do the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 的 `transformers` 库现在还提供了一个非常强大的 Trainer API，以帮助您以更抽象的方式执行微调。如果我们从之前的示例中取相同的分词器和模型，要使用
    Trainer API，我们只需做以下操作：
- en: 'When using the Trainer API, you need to define a `TrainingArguments` object,
    which can include hyperparameters and a few other flags. Let’s just accept all
    of the default values but supply a path for checkpoints to be outputted to:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当使用 Trainer API 时，您需要定义一个 `TrainingArguments` 对象，它可以包括超参数和一些其他标志。我们只需接受所有默认值，但提供一个输出检查点的路径：
- en: '[PRE22]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can then use the same `evaluate` package we used in the previous example
    to define a function for calculating any specified metrics, which we will pass
    into the main `trainer` object:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用之前示例中使用的相同 `evaluate` 包来定义一个计算任何指定指标的功能，我们将将其传递给主 `trainer` 对象：
- en: '[PRE23]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You then define the `trainer` object with all the relevant input objects:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后您定义一个包含所有相关输入对象的 `trainer` 对象：
- en: '[PRE24]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You train the model with these specified configurations and objects by calling
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过调用这些指定的配置和对象来训练模型
- en: '[PRE25]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: And that is how you perform your own training on an existing model hosted on
    Hugging Face.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是您在Hugging Face上对现有模型进行自己训练的方法。
- en: 'It is also useful to note that the Trainer API provides a really nice way to
    use a tool like **Optuna**, which we met in *Chapter 3*, *From Model to Model
    Factory*, in order to perform hyperparameter optimization. You can do this by
    specifying an Optuna trial search space:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得注意的是，Trainer API提供了一种非常不错的方式来使用**Optuna**这样的工具，我们在**第3章**，**从模型到模型工厂**中遇到过，以执行超参数优化。您可以通过指定Optuna试验搜索空间来完成此操作：
- en: '[PRE26]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'And then defining a function for initializing the neural network during every
    state of the hyperparameter search:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后定义一个函数，用于在超参数搜索的每个状态下初始化神经网络：
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You then just need to pass this into the `Trainer` object:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您只需将此传递给`Trainer`对象：
- en: '[PRE28]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, you can then run the hyperparameter search and retrieve the best run:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以运行超参数搜索并检索最佳运行：
- en: '[PRE29]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: And that concludes our example of transfer learning and fine-tuning of PyTorch
    deep learning models using the tools from Hugging Face. An important point to
    note is that both fine-tuning and transfer learning are still training processes
    and so can still be applied to the model factory methodology that was laid out
    in *Chapter 3*, *From Model to Model Factory*. For example, when we say “train”
    in the “train-run” process outlined in *Chapter 3*, this may now refer to the
    fine-tuning or transfer learning of a pre-trained deep learning model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就完成了使用Hugging Face工具进行PyTorch深度学习模型的迁移学习和微调的示例。需要注意的是，微调和迁移学习仍然是训练过程，因此仍然可以应用于第3章中概述的模型工厂方法。例如，当我们说“训练”时，在**第3章**中概述的“train-run”过程中，这可能现在指的是预训练深度学习模型的微调或迁移学习。
- en: As we have covered extensively already, deep learning models can be very powerful
    tools for solving a variety of problems. One of the trends that has been explored
    aggressively in recent years by many groups and organizations is the question
    of what is possible as these models get larger and larger. In the next section,
    we are going to start answering that question by exploring what happens when deep
    learning models get extremely large. It is time to enter the world of LLMs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前已经广泛讨论过的，深度学习模型可以是非常强大的工具，用于解决各种问题。近年来，许多团体和组织积极探索的一个趋势是，随着这些模型变得越来越大，可能实现什么。在下一节中，我们将开始通过探索深度学习模型变得极其大时会发生什么来回答这个问题。是时候进入大型语言模型（LLMs）的世界了。
- en: Living it large with LLMs
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与大型语言模型（LLMs）一起生活
- en: At the time of writing, GPT-4 has been released only a few months previously,
    in March 2023, by OpenAI. This model is potentially the largest ML model ever
    developed, with a reported one trillion parameters, although OpenAI has not confirmed
    the exact number. Since then, Microsoft and Google have announced advanced chat
    capabilities using similarly large models in their product suites and a raft of
    open-source packages and toolkits have been released. All of these solutions leverage
    some of the largest neural network models ever developed, LLMs. LLMs are part
    of an even wider class of models known as **foundation models**, which span not
    just text applications but video and audio as well. These models are roughly classified
    by the author as being too large for most organizations to consider training from
    scratch. This will mean organizations will either consume these models as third-party
    services or host and then fine-tune existing models. Solving this integration
    challenge in a safe and reliable way represents one of the main challenges in
    modern ML engineering. There is no time to lose, as new models and capabilities
    seem to be released every day; so let’s go!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，GPT-4仅在前几个月的2023年3月由OpenAI发布。这个模型可能是迄今为止开发的最大机器学习模型，据报道有1000亿个参数，尽管OpenAI尚未确认确切数字。从那时起，微软和谷歌已经宣布在其产品套件中使用类似的大型模型提供高级聊天功能，并发布了一系列开源软件包和工具包。所有这些解决方案都利用了迄今为止开发的一些最大的神经网络模型，即LLMs。LLMs是被称为**基础模型**的更广泛模型类别的一部分，不仅涵盖文本应用，还包括视频和音频。作者将这些模型大致分类为对于大多数组织来说太大，无法从头开始训练。这意味着组织将要么作为第三方服务消费这些模型，要么托管并微调现有模型。以安全可靠的方式解决这一集成挑战是现代机器学习工程的主要挑战之一。没有时间可以浪费，因为新的模型和功能似乎每天都在发布；所以让我们行动起来吧！
- en: Understanding LLMs
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解大型语言模型（LLMs）
- en: The main focus of LLM-based systems is to create human-like responses to a wide
    range of text-based inputs. LLMs are based on transformer architectures, which
    we have already met. This enables these models to process input in parallel, significantly
    reducing the amount of time for training on the same volume of data compared to
    other deep learning models.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大型语言模型（LLM）的系统的主要焦点是针对各种基于文本的输入创建类似人类的响应。LLM基于我们已经接触过的转换器架构。这使得这些模型能够并行处理输入，与其它深度学习模型相比，在相同数据量的训练上显著减少了所需的时间。
- en: The architecture of LLMs, as for any transformer, consists of a series of encoders
    and decoders that leverages self-attention and feed-forward neural networks.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何转换器来说，LLM的架构由一系列编码器和解码器组成，这些编码器和解码器利用了自注意力和前馈神经网络。
- en: At a high level, you can think of the encoders as being responsible for processing
    the input, transforming it into an appropriate numerical representation, and then
    feeding this into the decoders, from which the output can be generated. The magic
    of transformers comes from the use of **self-attention**, which is a mechanism
    for capturing the contextual relationships between words in a sentence. This results
    in **attention vectors** that represent this numerically, and when multiples of
    these are being calculated, it is called **multi-headed attention**. Both the
    encoder and decoder use self-attention mechanisms to capture the contextual dependencies
    of the input and output sequences.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，你可以将编码器视为负责处理输入，将其转换成适当的数值表示，然后将这个表示输入到解码器中，从解码器中生成输出。转换器的魔力来自于**自注意力**的使用，这是一种捕捉句子中词语之间上下文关系的机制。这导致了表示这种上下文关系的**注意力向量**，当这些向量的多个被计算时，就称为**多头注意力**。编码器和解码器都使用自注意力机制来捕捉输入和输出序列的上下文依赖关系。
- en: One of the most popular transformer-based models used in LLMs is the BERT model.
    BERT was developed by Google and is a pre-trained model that can be fine-tuned
    for various natural language tasks.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM中使用的基于转换器的最流行的模型之一是BERT模型。BERT是由谷歌开发的，是一个预训练模型，可以针对各种自然语言任务进行微调。
- en: Another popular architecture is the **Generative Pre-trained Transformer** (**GPT**),
    created by OpenAI. The ChatGPT system, released by OpenAI in November 2022, apparently
    utilized a third-generation GPT model when it took the world by storm. At the
    time of writing in March 2023, these models are up to their fourth generation
    and are incredibly powerful. Although GPT-4 is still relatively new, it is already
    sparking a heated debate about the future of AI and whether or not we have reached
    **artificial general intelligence** (**AGI**). The author does not believe we
    have, but what an exciting time to be in this space anyway!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行的架构是**生成预训练转换器**（**GPT**），由OpenAI创建。OpenAI在2022年11月发布的ChatGPT系统，显然在引起世界轰动时使用了第三代GPT模型。截至2023年3月写作时，这些模型已经发展到第四代，并且非常强大。尽管GPT-4仍然相对较新，但它已经引发了关于AI未来的激烈辩论，以及我们是否已经达到了**人工通用智能**（**AGI**）。作者并不认为我们已经达到，但无论如何，这是一个多么激动人心的领域啊！
- en: The thing that makes LLMs infeasible to train anew in every new business context
    or organization is that they are trained on colossal datasets. GPT-3, which was
    released in 2020, was trained on almost 500 billion tokens of text. A token in
    this instance is a small fragment of a word used for the training and inferences
    process in LLMs, roughly around 4 characters in English. That is a lot of text!
    The costs for training these models are therefore concomitantly large and even
    inference can be hugely costly. This means that organizations whose sole focus
    is not producing these models will likely fail to see the economies of scale and
    the returns required to justify investing in them at this scale. This is before
    you even consider the need for specialized skill sets, optimized infrastructure,
    and the ability to grab all of that data. There are a lot of parallels with the
    advent of the public cloud several years ago, where organizations no longer had
    to invest in as much on-premises infrastructure or expertise and instead started
    to pay on a “what you use” basis. The same thing is now happening with the most
    sophisticated ML models. This is not to say that smaller, more domain-specific
    models have been ruled out. In fact, I think that this will remain one of the
    ways that organizations can leverage their own unique datasets to drive advantage
    over competitors and build out better products. The most successful teams will
    be those that combine this approach with the approach from the largest models
    in a robust way.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使得 LLM 在每个新的商业环境或组织中重新训练变得不可行的是，它们是在庞大的数据集上训练的。2020 年发布的 GPT-3 在近 5000 亿次文本标记上进行了训练。在这个例子中，一个标记是用于
    LLM 训练和推理过程中单词的小片段，大约有 4 个英文字符左右。这可是大量的文本！因此，训练这些模型的成本相应地也很高，甚至推理也可能非常昂贵。这意味着，那些唯一关注不生产这些模型的组织可能无法看到规模经济和投资这些模型所需的回报。在考虑需要专业技能、优化基础设施以及获取所有这些数据的能力之前，这种情况就已经存在了。这与几年前公共云的出现有很多相似之处，当时组织不再需要投资大量的本地基础设施或专业知识，而是开始按“使用即付费”的方式支付费用。现在，这种情况正在最复杂的机器学习模型中发生。这并不是说较小的、更专业化的模型已被排除在外。事实上，我认为这将是组织利用他们独特的数据集来驱动竞争优势和构建更好产品的一种方式。最成功的团队将是那些能够以稳健的方式将这种方法与最大模型的方法相结合的团队。
- en: Scale is not the only important component though. ChatGPT and GPT-4 were not
    only trained on huge amounts of data but they were also then fine-tuned using
    a technique called **Reinforcement Learning with Human Feedback** (**RLHF**).
    During this process, the model is presented with a prompt, such as a conversational
    question, and generates a series of potential responses. The responses are then
    presented to a human evaluator who provides feedback on the quality of the response,
    usually by ranking them, which is then used to train a **reward model**. This
    model is then used to fine-tune the underlying language model through techniques
    like **Proximal Policy Optimization** (**PPO**). The details of all of this are
    well beyond the scope of this book but hopefully, you are gaining an intuition
    for how this is not run-of-the-mill data science that any team can quickly scale
    up. And since this is the case, we have to learn how to work with these tools
    as more of a “black box” and consume them as third-party solutions. We will cover
    this in the next section.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管规模不是唯一重要的组成部分。ChatGPT 和 GPT-4 不仅在大量数据上进行了训练，而且还使用了一种称为**人类反馈强化学习**（**RLHF**）的技术进行了微调。在这个过程中，模型会接收到一个提示，例如一个对话式问题，然后生成一系列可能的回答。这些回答随后会被展示给人类评估者，他们会对回答的质量提供反馈，通常是通过排名，这些反馈随后用于训练一个**奖励模型**。然后，该模型会通过**近端策略优化**（**PPO**）等技术来微调底层语言模型。所有这些细节都远远超出了本书的范围，但希望您已经对这种并非普通数据科学，任何团队都无法迅速扩展的方法有了直观的认识。既然如此，我们就必须学习如何将这些工具视为更类似于“黑盒”的东西，并将它们作为第三方解决方案来使用。我们将在下一节中介绍这一点。
- en: Consuming LLMs via API
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 API 消费 LLM
- en: As discussed in the previous sections, the main change in our way of thinking
    as ML engineers who want to interact with LLMs, and foundation models in general,
    is that we can no longer assume we have access to the model artifact, the training
    data, or testing data. We have to instead treat the model as a third-party service
    that we should call out to for consumption. Luckily, there are many tools and
    techniques for implementing this.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几节所述，我们作为想要与LLMs和一般基础模型交互的ML工程师的思维方式的重大变化是，我们不能再假设我们有权访问模型工件、训练数据或测试数据。相反，我们必须将模型视为一个第三方服务，我们应该调用它以进行消费。幸运的是，有许多工具和技术可以实现这一点。
- en: The next example will show you how to build a pipeline that leverages LLMs by
    using the popular **LangChain** package. The name comes from the fact that to
    leverage the power of LLMs, we often have to chain together many interactions
    with them with calls to other systems and sources of information. LangChain also
    provides a wide variety of functionality that is useful when dealing with NLP
    and text-based applications more generally. For example, there are utilities for
    text splitting, working with vector databases, document loading and retrieval,
    and conversational state persistence, among others. This makes it a worthwhile
    package to check out even in projects where you are not necessarily working with
    LLMs specifically.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例将展示如何使用流行的**LangChain**包构建利用LLMs的管道。这个名字来源于这样一个事实：为了利用LLMs的力量，我们通常需要通过调用其他系统和信息来源与它们进行许多交互。LangChain还提供了一系列在处理NLP和基于文本的应用时非常有用的功能。例如，有文本拆分、处理向量数据库、文档加载和检索以及会话状态持久化的工具。这使得它即使在不是专门与LLMs工作的项目中也是一个值得检查的包。
- en: 'First, we walk through a basic example of calling the OpenAI API:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过一个基本示例来调用OpenAI API：
- en: 'Install the `langchain` and `openai` Python bindings:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装`langchain`和`openai`Python绑定：
- en: '[PRE30]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We assume the user has set up an OpenAI account and has access to an API key.
    You can set this as an environment variable or use a secrets manager for storage,
    like the one that GitHub provides. We will assume the key is accessible as an
    environment variable:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们假设用户已经设置了OpenAI账户并有权访问API密钥。你可以将其设置为环境变量或使用像GitHub提供的那样一个秘密管理器进行存储。我们将假设密钥可以通过环境变量访问：
- en: '[PRE31]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, in our Python script or module, we can define the model we will call using
    the OpenAI API as accessed via the `langchain` wrapper. Here we will work with
    the `gpt-3.5-turbo` model, which is the most advanced of the GPT-3.5 chat models:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在我们的Python脚本或模块中，我们可以定义我们将通过`langchain`包装器访问的OpenAI API调用的模型。这里我们将使用`gpt-3.5-turbo`模型，这是GPT-3.5聊天模型中最先进的：
- en: '[PRE32]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'LangChain then facilitates the building up of pipelines using LLMs via prompt
    templates, which allow you to standardize how we will prompt and parse the response
    of the models:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LangChain随后通过提示模板促进使用LLMs构建管道，这些模板允许您标准化我们将如何提示和解析模型的响应：
- en: '[PRE33]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We can then create our first “chain,” which is the mechanism for pulling together
    related steps in `langchain`. This first chain is a simple one that takes a prompt
    template and the input to create an appropriate prompt to the LLM API, before
    returning an appropriately formatted response:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以创建我们的第一个“链”，这是在`langchain`中拉取相关步骤的机制。这个第一个链是一个简单的链，它接受一个提示模板和输入，创建一个适当的提示发送给LLM
    API，然后返回一个格式适当的响应：
- en: '[PRE34]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You can then run this question and print the result to the terminal as a test:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以运行这个问题并将结果打印到终端作为测试：
- en: '[PRE35]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This returns:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这返回：
- en: '[PRE36]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Given that the I am an ML engineer employed by a large bank and am based in
    Glasgow, United Kingdom, you can see that even the most sophisticated LLMs will
    get things wrong. This is an example of what we term a *hallucination*, where
    an LLM gives an incorrect but plausible answer. We will return to the topic of
    when LLMs get things wrong in the section on Building the future with *LLMOps*.
    This is still a good example of building a basic mechanism through which we can
    programmatically interact with LLMs in a standardized way.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我是一名受雇于大型银行并驻扎在英国格拉斯哥的ML工程师，你可以看到即使是功能最复杂的LLMs也会出错。这是我们所说的“幻觉”的一个例子，其中LLM给出了一个错误但看似合理的答案。我们将在关于构建未来与*LLMOps*的章节中回到LLMs出错的话题。这仍然是一个通过程序化方式以标准化方式与LLMs交互的基本机制的例子。
- en: 'LangChain also provides the ability to pull multiple prompts together using
    a method in the chain called `generate`:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain还提供了使用链中的`generate`方法将多个提示组合在一起的能力：
- en: '[PRE37]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The response from this series of questions is rather verbose, but here is the
    first element of the returned object:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这一系列问题的回答相当冗长，但以下是返回对象的第一部分：
- en: '[PRE38]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Again, not *quite* right. You get the idea though! With some prompt engineering
    and better conversation design, this could quite easily be a lot better. I’ll
    leave you to play around and have some fun with it.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，并不完全正确。不过你大概明白了！通过一些提示工程和更好的对话设计，这可以很容易地变得更好。我将让你自己尝试并享受其中的乐趣。
- en: This quick introduction to LangChain and LLMs only scratches the surface, but
    hopefully gives you enough to fold in calls to these models into your ML workflows.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这份关于LangChain和LLMs的快速介绍只是触及了表面，但希望这能给你足够的信息，将调用这些模型的代码整合到你的机器学习工作流程中。
- en: Let’s move on to discuss another important way that LLMs are becoming part of
    the ML engineering toolkit, as we explore software development using AI assistants.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续讨论LLMs成为机器学习工程工具包重要组成部分的另一种方式，正如我们探索使用人工智能助手进行软件开发时。
- en: Coding with LLMs
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LLMs进行编码
- en: LLMs are not only useful for creating and analyzing natural language; they can
    also be applied to programming languages. This is the purpose of the OpenAI Codex
    family of models, which has been trained on millions of code repositories with
    the aim of being able to produce reasonable-looking and performing code when prompted.
    Since GitHub Copilot, an AI assistant for coding, was launched, the concept of
    an AI assistant helping you code has entered the mainstream. Many people have
    argued that these solutions provide massive productivity boosts and improved enjoyment
    when executing their own work. GitHub has published some of its own research suggesting
    that 60-75% of 2,000 developers asked reported less frustration and improved satisfaction
    when developing software. On a far smaller cohort of 95 developers, with 50 being
    in the control group, they also showed a speedup when developing an HTTP server
    in JavaScript with a given specification. I believe there should be much more
    work done on this topic before we declare that AI coding assistants are obviously
    making us all happier and faster, but the GitHub survey and test results definitely
    suggest they are a useful tool to try out. These results are published at [https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/](https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/).
    To this previous point, an interesting pre-print paper on the arXiv by researchers
    from Stanford University, `arXiv:2211.03622` **[cs.CR]**, seems to show that developers
    using an AI coding assistant based on the OpenAI `codex-davinci-002` model were
    more likely to introduce security vulnerabilities into their code and that users
    of the model would feel more confident in their work even though it contained
    these issues! It should be noted that the model they used is relatively old in
    terms of the family of LLMs that OpenAI offers now, so again more research is
    needed. This does raise the interesting possibility that AI coding assistants
    may provide a speed boost but also introduce more bugs. Time will tell. This area
    has also started to heat up with the introduction of powerful open-source contenders.
    A particular one to call out is StarCoder, which was developed through a collaboration
    between Hugging Face and ServiceNow [https://huggingface.co/blog/starcoder](https://huggingface.co/blog/starcoder).
    The one thing that is certain is that these assistants are not going anywhere
    and they are only going to improve with time. In this section, we will start to
    explore the possibilities of working with these AI assistants in a variety of
    guises. Learning to work with AI is likely going to be a critical part of the
    ML engineering workflow in the near future, so let’s get learning!
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs不仅对创建和分析自然语言有用；它们还可以应用于编程语言。这就是OpenAI Codex系列模型的目的，这些模型在数百万个代码仓库上进行了训练，目的是在提示时能够生成看起来合理且性能良好的代码。自从GitHub
    Copilot，一个编码人工智能助手推出以来，AI助手帮助编码的概念已经进入主流。许多人认为这些解决方案在执行自己的工作时提供了巨大的生产力提升和更愉快的体验。GitHub发布了一些自己的研究，表明在询问的2,000名开发者中，有60-75%的人表示在开发软件时感到的挫败感减少，满意度提高。在95名开发者的一个小群体中，其中50名是对照组，他们使用给定规范在JavaScript中开发HTTP服务器时也显示了速度提升。我相信在我们宣布AI编码助手显然使我们所有人更快乐、更高效之前，应该在这个主题上做更多的工作，但GitHub的调查和测试结果确实表明它们是值得尝试的有用工具。这些结果发布在[https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/](https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/)。关于这一点，斯坦福大学的研究人员在一篇有趣的arXiv预印本论文中，`arXiv:2211.03622`
    **[cs.CR]**，似乎表明使用基于OpenAI `codex-davinci-002`模型的AI编码助手的开发者更有可能在其代码中引入安全漏洞，并且即使存在这些问题，模型的使用者也会对自己的工作更有信心！应该注意的是，他们使用的模型在OpenAI现在提供的LLM家族中相对较旧，因此还需要更多的研究。这确实提出了一个有趣的可能性，即AI编码助手可能提供速度提升，但也可能引入更多的错误。时间将证明一切。随着强大开源竞争者的引入，这一领域也开始变得热门。其中一个值得指出的是StarCoder，它是通过Hugging
    Face和ServiceNow的合作开发的[https://huggingface.co/blog/starcoder](https://huggingface.co/blog/starcoder)。有一点是肯定的，这些助手不会消失，并且随着时间的推移只会变得更好。在本节中，我们将开始探索以各种形式与这些AI助手一起工作的可能性。学习与AI合作很可能是未来机器学习工程工作流程的一个关键部分，所以让我们开始学习吧！
- en: First, when would I want to use an AI coding assistant as an ML engineer? The
    consensus in the community, and in the GitHub research, looks to be that these
    assistants help with the development of boilerplate code on established languages,
    Python among them. They do not seem to be ideal for when you want to do something
    particularly innovative or different; however, we will explore this also.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，作为一个机器学习工程师，我什么时候会想使用AI编码助手呢？社区和GitHub的研究共识似乎表明，这些助手有助于在已建立的语言（如Python）上开发样板代码。它们似乎并不适合当你想做一些特别创新或不同的事情时；然而，我们也会探讨这一点。
- en: 'So, how do you actually work with an AI to help you code? At the time of writing,
    there seem to be two main methods (but given the pace of innovation, it could
    be that you are working with an AI through a brain-computer interface in no time;
    who knows?), each of which has its own advantages and disadvantages:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你实际上是如何与AI合作来帮助你编写代码的呢？在撰写本文时，似乎有两种主要方法（但考虑到创新的步伐，你可能会很快通过脑机接口与AI合作；谁知道呢？），每种方法都有其自身的优缺点：
- en: '**Direct editor or IDE integration**: In Copilot-supported code editors and
    IDEs, which at the time of writing include the PyCharm and VS Code environments
    we have used in this book, then you can enable Copilot to offer autocomplete suggestions
    for your code as you type it. You can also provide prompt information for the
    LLM model in your comments in the code. This mode of integration will likely always
    be there as long as developers are using these environments, but I can foresee
    a large number of AI assistant offerings in the future.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直接编辑器或IDE集成**：在Copilot支持的代码编辑器和IDE中，包括撰写本文时我们在这本书中使用的PyCharm和VS Code环境，你可以启用Copilot在你输入代码时提供自动补全建议。你还可以在代码的注释中提供LLM模型的提示信息。这种集成方式只要开发者使用这些环境，就可能会一直存在，但我预见未来会有大量的AI助手服务。'
- en: '**Chat interface**: If you are not using Copilot but instead another LLM, for
    example, OpenAI’s GPT-4, then you will likely need to work in some sort of chat
    interface and copy and paste relevant information between your coding environment
    and the chat. This may seem a bit more clunky but is definitely far more versatile
    as it means you can easily switch between the models of your choice, or even combine
    multiples. You could actually build your own code to feed in your code with these
    models if you have the relevant access permissions and APIs to hit, but at that
    point, you are just redeveloping a tool like Copilot!'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聊天界面**：如果你不使用Copilot而是使用其他LLM，例如OpenAI的GPT-4，那么你可能需要在一个聊天界面中工作，并在你的编码环境和聊天之间复制粘贴相关信息。这可能看起来有点笨拙，但确实更加灵活，这意味着你可以轻松地在你选择的模型之间切换，甚至组合多个模型。如果你有相关的访问权限和API来调用，你实际上可以构建自己的代码来将这些模型输入你的代码中，但到了那个阶段，你只是在重新开发一个像Copilot这样的工具！'
- en: We will walk through an example showing both and highlighting how this may potentially
    help you in your future ML engineering projects.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个示例来展示这两种方法，并突出它们如何可能在你未来的机器学习工程项目中帮助你。
- en: 'If you navigate to the GitHub Copilot web page, you can sign up for an individual
    subscription for a monthly fee and they offer a free trial. Once you have done
    that, you can follow the setup instructions for your chosen code editor here:
    [https://docs.github.com/en/copilot/getting-started-with-github-copilot](https://docs.github.com/en/copilot/getting-started-with-github-copilot).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你导航到GitHub Copilot网页，你可以为个人订阅支付月费并享受免费试用。一旦你完成了这个步骤，你就可以遵循这里为你选择的代码编辑器的设置说明：[https://docs.github.com/en/copilot/getting-started-with-github-copilot](https://docs.github.com/en/copilot/getting-started-with-github-copilot)。
- en: Once you have this setup, as I have done for VS Code, then you can start to
    use Copilot straight away. For example, I opened a new Python file and started
    typing some typical imports. When I started to write my first function, Copilot
    kicked in with a suggestion to complete the entire function, as shown in *Figure
    7.4*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你设置了这个环境，就像我为VS Code所做的那样，你就可以立即开始使用Copilot。例如，我打开了一个新的Python文件并开始输入一些典型的导入语句。当我开始编写我的第一个函数时，Copilot就提出了一个建议，来完成整个函数，如图7.4所示。
- en: '![](img/B19525_07_04.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19525_07_04.png)'
- en: 'Figure 7.4: A suggested autocompletion from GitHub Copilot in VS Code.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：GitHub Copilot在VS Code中建议的自动补全。
- en: As mentioned above, this is not the only way you can provide input to Copilot;
    you can also use comments to provide more information to the model. In *Figure
    7.5*, we can see that providing some commentary in a leading comment line helps
    define the logic we want to be contained in the function.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，这并不是向Copilot提供输入的唯一方式；您还可以使用注释来向模型提供更多信息。在*图7.5*中，我们可以看到在首行注释中提供一些评论有助于定义我们希望在函数中包含的逻辑。
- en: '![](img/B19525_07_05.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19525_07_05.png)'
- en: 'Figure 7.5: By providing a leading comment, you can help Copilot suggest the
    logic you want for your code.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：通过提供首行注释，您可以帮助Copilot为您代码建议所需的逻辑。
- en: 'There are a few things that help get the best out of Copilot that are useful
    to bear in mind as you use it:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Copilot时，以下是一些有助于发挥其最佳效果的事项，值得您牢记：
- en: '**Be very modular**: The more modular you can make your code, the better. We’ve
    already discussed why this benefits maintenance and quicker development, but here
    it also helps the Codex model create more appropriate auto-completion suggestions.
    If your functions are going to be long, complex objects, then the likelihood is
    that the Copilot suggestion wouldn’t be great.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非常模块化**：您能将代码做得越模块化，效果越好。我们之前已经讨论过这有利于维护和快速开发，但在这里它也有助于Codex模型创建更合适的自动补全建议。如果您的函数将要变得很长，很复杂，那么Copilot的建议可能就不会很好。'
- en: '**Write clear comments**: This is always a good practice of course but it really
    helps Copilot understand what code you need. It can help to write longer comments
    at the top of the file describing what you want the solution to do and then write
    shorter but very precise comments before your functions. The example in *Figure
    7.5* shows a comment that specifies the way in which I wanted the function to
    perform the feature preparation, but if the comment simply said “*standardize
    features*,” the suggestion would likely not be as complete.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编写清晰的注释**：这当然是一种良好的实践，但它确实有助于Copilot理解您需要的代码。在文件顶部编写较长的注释，描述您希望解决方案执行的操作，然后在函数之前编写较短但非常精确的注释可能会有所帮助。*图7.5*中的示例显示了一个注释，它指定了我想让函数执行特征准备的方式，但如果注释只是说“*标准化特征*”，那么建议可能就不会那么完整。'
- en: '**Write the interfaces and function signatures**: As in *Figure 7.5*, it helps
    if you start the piece of code off by providing the function signature and the
    types or the first line of the class definition if this was a class. This acts
    to prime the model to complete the rest of the code block.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编写接口和函数签名**：正如*图7.5*所示，如果您在代码块开始时提供函数签名和类型或类定义的第一行（如果是类的话），这有助于启动模型以完成代码块的其余部分。'
- en: Hopefully, this is enough to get you started on your journey working with AI
    to build your solutions. I think that as these tools become more ubiquitous, there
    are going to be many opportunities to use them to supercharge your development
    workflows.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这足以让您开始与AI合作构建解决方案的旅程。我认为随着这些工具变得更加普遍，将会有很多机会使用它们来加速您的工作流程。
- en: Now that we know how to build some pipelines using LLMs and we know how to start
    leveraging them to aid our own development, we can turn to what I think is one
    of the most important topics in this field. I also think it is the one with the
    most unanswered questions and so it is a very exciting one to explore. This all
    relates to the question of the operational implications of leveraging LLMs, now
    being termed **LLMOps**.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了如何使用LLMs构建一些管道，并且知道了如何开始利用它们来辅助我们的开发，我们可以转向我认为这个领域最重要的一个话题。我也认为这是最具未解之谜的话题，因此它是一个非常激动人心的探索方向。这一切都与利用LLMs的操作影响有关，现在被称为**LLMOps**。
- en: Building the future with LLMOps
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用LLMOps构建未来
- en: Given the rise in interest in LLMs recently, there has been no shortage of people
    expressing the desire to integrate these models into all sorts of software systems.
    For us as ML engineers, this should immediately trigger us to ask the question,
    “What will that mean operationally?” As discussed throughout this book, the marrying
    together of operations and development of ML systems is termed MLOps. Working
    with LLMs is likely to lead to its own interesting challenges, however, and so
    a new term, LLMOps, has arisen to give this sub-field of MLOps some good marketing.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 近期对大型语言模型（LLMs）的兴趣日益增长，很多人表达了将这类模型集成到各种软件系统中的愿望。对于我们这些机器学习工程师来说，这应该立即引发我们思考，“这将对我们的操作意味着什么？”正如本书中多次讨论的那样，将操作与机器学习系统的开发相结合被称为MLOps。然而，与LLMs一起工作可能会带来一些有趣的挑战，因此出现了一个新术语，LLMOps，以给MLOps的子领域带来一些良好的市场营销。
- en: 'Is this really any different? I don’t think it is *that* different, but should
    be viewed as a sub-field of MLOps with its own additional challenges. Some of
    the main challenges that I see in this area are:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这真的有什么不同吗？我认为它并没有**那么**不同，但应该被视为MLOps的一个子领域，它有自己的额外挑战。我在这个领域看到的一些主要挑战包括：
- en: '**Larger infrastructure, even for fine-tuning**: As discussed previously, these
    models are far too large for typical organizations or teams to consider training
    their own, so instead teams will have to leverage third-party models, be they
    open-source or proprietary, and fine-tune them. Fine-tuning models of this scale
    will still be very expensive and so there will be a higher premium on building
    very efficient data ingestion, preparation, and training pipelines.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**即使是微调，也需要更大的基础设施**：正如之前讨论的那样，这些模型对于典型的组织或团队来说太大，无法考虑自己训练，因此团队将不得不利用第三方模型，无论是开源的还是专有的，并对它们进行微调。微调如此规模的模型仍然会非常昂贵，因此构建非常高效的数据摄取、准备和训练管道将更加重要。'
- en: '**Model management is different**: When you train your own models, as we showed
    several times in *Chapter 3*, *From Model to Model Factory*, effective ML engineering
    requires us to define good practices for versioning our models and storing metadata
    that provide lineage of the experiments and training runs we have gone through
    to produce these models. In a world where models are more often hosted externally,
    this is slightly harder to do, as we do not have access to the training data,
    to the core model artifacts, and probably not even to the detailed model architecture.
    Versioning metadata will then likely default to the publicly available metadata
    for the model, think along the lines of `gpt-4-v1.3` and similar-sounding names.
    That is not a lot of information to go on, and so you will likely have to think
    of ways to enrich this metadata, perhaps with your own example runs and test results
    in order to understand how that model behaved in certain scenarios. This then
    also links to the next point.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型管理有所不同**：当你自己训练模型时，正如我们在第3章“从模型到模型工厂”中多次展示的那样，有效的机器学习工程需要我们为模型的版本控制和存储提供实验和训练过程的历史记录的元数据定义良好的实践。在一个模型更常由外部托管的世界里，这会稍微困难一些，因为我们无法访问训练数据、核心模型工件，甚至可能连详细的模型架构都无法访问。版本控制元数据可能默认为模型的公开可用元数据，例如`gpt-4-v1.3`和类似名称。这并不是很多信息，因此你可能会考虑想出方法来丰富这些元数据，可能包括你自己的示例运行和测试结果，以便了解该模型在特定场景下的行为。这也就与下一个点相关联。'
- en: '**Rollbacks become more challenging**: If your model is hosted externally by
    a third party, you do not control the roadmap of that service. This means that
    if there is an issue with version 5 of a model and you want to roll back to version
    4, that option might not be available to you. This is a different kind of “drift”
    from the model performance drift we’ve discussed at length in this book but it
    is going to become increasingly important. This will mean that you should have
    your own model, perhaps with nowhere near the same level of functionality or scale,
    ready as a last resort default to switch to in case of issues.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回滚变得更加困难**：如果你的模型由第三方托管，你无法控制该服务的路线图。这意味着，如果模型版本5存在问题，你想回滚到版本4，你可能没有这个选项。这与我们在本书中详细讨论过的模型性能漂移是不同类型的“漂移”，但它将变得越来越重要。这意味着你应该准备自己的模型，可能功能或规模远不及这些模型，作为最后的手段，在出现问题时切换到默认选项。'
- en: '**Model performance is more of a challenge**: As mentioned in the previous
    point, with foundation models being served as externally hosted services, you
    are no longer in as much control as you were. This then means that if you do detect
    any issues with the model you are consuming, be they drift or some other bugs,
    you are very limited in what you can do and you will need to consider that default
    rollback we just discussed.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型性能是一个更大的挑战**：正如前一点提到的，随着基础模型作为外部托管服务提供，你不再像以前那样有那么多控制权。这意味着如果你检测到你所消费的模型有任何问题，无论是漂移还是其他错误，你所能做的非常有限，你将需要考虑我们刚才讨论的默认回滚。'
- en: '**Applying your own guardrails will be key**: LLMs hallucinate, they get things
    wrong, they can regurgitate training data, and they might even inadvertently offend
    the person interacting with them. All of this means that as these models are adopted
    by more organizations, there will be a growing need to develop methods for applying
    bespoke guardrails to systems utilizing them. As an example, if an LLM was being
    used to power a next-generation chatbot, you could envisage that between the LLM
    service and the chat interface, you could have a system layer that checked for
    abrupt sentiment changes and important keywords or data that should be obfuscated.
    This layer could utilize simpler ML models and a variety of other techniques.
    At its most sophisticated, it could try and ensure that the chatbot did not lead
    to a violation of ethical or other norms established by the organization. If your
    organization has made the climate crisis an area of focus, you may want to screen
    the conversation in real time for information that goes against critical scientific
    findings in this area as an example.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用自己的安全措施将是关键**：LLM会幻想，它们会出错，它们可能会重复训练数据，甚至可能无意中冒犯与之互动的人。所有这些都意味着随着这些模型被更多组织采用，将会有越来越多的需求来开发为利用这些模型构建的系统应用定制安全措施的方法。例如，如果某个LLM被用来驱动下一代聊天机器人，你可以设想在LLM服务和聊天界面之间，可以有一个系统层来检查突然的情感变化和应该被隐藏的重要关键词或数据。这一层可以利用更简单的机器学习模型和多种其他技术。在其最复杂的形式下，它可能试图确保聊天机器人不会导致违反组织建立的道德或其他规范。如果你的组织将气候危机作为一个重点关注的领域，你可能希望实时筛选对话中的信息，以避免与该领域关键科学发现相悖，例如。'
- en: Since the era of foundation models has only just begun, it is likely that more
    and more complex challenges will arise to keep us busy as ML engineers for a long
    time to come. To me, this is one of the most exciting challenges we face as a
    community, how we harness one of the most sophisticated and cutting-edge capabilities
    ever developed by the ML community in a way that still allows the software to
    run safely, efficiently, and robustly for users day in and day out. Are you ready
    to take on that challenge?
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基础模型的时代才刚刚开始，很可能会出现越来越多的复杂挑战，让我们作为机器学习工程师在接下来的很长时间里都忙碌不已。对我来说，这是我们作为一个社区面临的最激动人心的挑战之一，那就是如何以仍然允许软件每天对用户安全、高效和稳健运行的方式，利用机器学习社区开发出的最复杂和最前沿的能力。你准备好接受这个挑战了吗？
- en: Let’s dive into some of these topics in a bit more detail, first with a discussion
    of LLM validation.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地探讨一些这些话题，首先从LLM验证开始讨论。
- en: Validating LLMs
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证LLM
- en: The validation of generative AI models is inherently different from and seemingly
    more complex than the same for other ML models. The main reasons for this are
    that when you are *generating* content, you are often creating very complex data
    in your results that has never existed! If an LLM returns a paragraph of generated
    text when asked to help summarize and analyze some document, how do you determine
    if the answer is “good”? If you ask an LLM to reformat some data into a table,
    how can you build a suitable metric that captures if it has done this correctly?
    In a generative context, what do “model performance” and “drift” really mean and
    how do I calculate them? Other questions may be more use case dependent, for example,
    if you are building an information retrieval or Retrieval-Augmented Generation
    (see *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*, [https://arxiv.org/pdf/2005.11401.pdf](https://arxiv.org/pdf/2005.11401.pdf))
    solution, how do you evaluate the truthfulness of the text generated by the LLM?
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI模型的验证本质上与其它ML模型的验证不同，看起来也更复杂。主要原因在于，当你正在**生成**内容时，你通常会在结果中创建非常复杂的数据，这些数据以前从未存在过！如果LLM在请求帮助总结和分析某些文档时返回一段生成的文本，你如何判断这个答案是否“好”？如果你要求LLM将一些数据重新格式化为表格，你如何构建一个合适的指标来捕捉它是否正确地完成了这项任务？在生成环境中，“模型性能”和“漂移”究竟意味着什么，我该如何计算它们？其他问题可能更依赖于具体的应用场景，例如，如果你正在构建一个信息检索或检索增强生成（见*Retrieval-Augmented
    Generation for Knowledge-Intensive NLP Tasks*，[https://arxiv.org/pdf/2005.11401.pdf](https://arxiv.org/pdf/2005.11401.pdf)）解决方案，你如何评估LLM生成的文本的真实性？
- en: There are also important considerations around how we screen the LLM-generated
    outputs for any potential biased or toxic outputs that may cause harm or reputational
    damage to the organization running the model. The world of LLM validation is complex!
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在如何筛选LLM生成的输出以避免可能对运行该模型的组织造成伤害或声誉损害的偏见或有害输出方面，也有一些重要的考虑因素。LLM验证的世界非常复杂！
- en: 'What can we do? Thankfully, this has not all happened in a vacuum and there
    have been several benchmarking tools and datasets released that can help us on
    our journey. Things are so young that there are not many worked examples of these
    tools yet, but we will discuss the key points so that you are aware of the landscape
    and can keep on top of how things are evolving. Let’s list some of the higher-profile
    evaluation frameworks and datasets for LLMs:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做些什么呢？幸运的是，这一切并非在真空中发生，已经有几个基准工具和数据集发布，可以帮助我们在旅途中。由于事物还处于初级阶段，因此这些工具的实例并不多，但我们将讨论关键点，以便您了解这一领域，并跟上事物的发展。以下是LLM的一些高知名度评估框架和数据集：
- en: '**OpenAI Evals**: This is a framework whereby OpenAI allows for the crowdsourced
    development of tests against proposed text completions generated by LLMs. The
    core concept at the heart of evals is the “Completion Function Protocol,” which
    is a mechanism for standardizing the testing of the strings returned when interacting
    with an LLM. The framework is available on GitHub at [https://github.com/openai/evals](https://github.com/openai/evals).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI Evals**：这是一个框架，OpenAI允许通过众包开发针对LLM生成的文本补全的测试。evals的核心概念是“补全函数协议”，这是一种标准化与LLM交互时返回的字符串测试的机制。该框架可在GitHub上找到，[https://github.com/openai/evals](https://github.com/openai/evals)。'
- en: '**Holistic Evaluation of Language Models** (**HELM**): This project, from Stanford
    University, styles itself as a “living benchmark” for LLM performance. It gives
    you a wide variety of datasets, models, and metrics and shows the performance
    across these different combinations. It is a very powerful resource that you can
    use to base your own test scenarios on, or indeed just to use the information
    directly to understand the risks and potential benefits of using any specific
    LLM for your use case. The HELM benchmarks are available at [https://crfm.stanford.edu/helm/latest/](https://crfm.stanford.edu/helm/latest/).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全面评估语言模型**（**HELM**）：这个由斯坦福大学发起的项目将自己定位为LLM性能的“活基准”。它提供了各种数据集、模型和指标，并展示了这些不同组合的性能。这是一个非常强大的资源，您可以用它来基于自己的测试场景，或者直接使用这些信息来了解使用任何特定LLM的潜在风险和收益。HELM基准可在[https://crfm.stanford.edu/helm/latest/](https://crfm.stanford.edu/helm/latest/)找到。'
- en: '**Guardrails AI**: This is a Python package that allows you to do validation
    on LLM outputs in the same style as `pydantic`, which is a very powerful idea!
    You can also use it to build control flows with the LLM for when issues arise
    like a response to a prompt not meeting your set criteria; in this case, you can
    use Guardrails AI to re-prompt the LLM in the hope of getting a different response.
    To use Guardrails AI, you specify a **Reliable AI Markup Language** (**RAIL**)
    file that defines the prompt format and expected behavior in an XML-like file.
    Guardrails AI is available on GitHub at [https://shreyar.github.io/guardrails/](https://shreyar.github.io/guardrails/).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Guardrails AI**：这是一个Python包，允许你以类似于`pydantic`的方式对LLM的输出进行验证，这是一个非常强大的想法！你还可以用它来为LLM构建控制流程，以应对诸如对提示的响应不符合你设定的标准等问题；在这种情况下，你可以使用Guardrails
    AI重新提示LLM，希望得到不同的响应。要使用Guardrails AI，你需要指定一个**可靠的AI标记语言**（**RAIL**）文件，该文件以XML类似的文件定义了提示格式和预期的行为。Guardrails
    AI可在GitHub上找到：[https://shreyar.github.io/guardrails/](https://shreyar.github.io/guardrails/)。'
- en: There are several more of these frameworks being created all the time, but getting
    familiar with the core concepts and datasets out there will become increasingly
    important as more organizations want to take LLM-based systems from fun proofs-of-concept
    to production solutions. In the penultimate section of this chapter, we will briefly
    discuss some specific challenges I see around the management of “prompts” when
    building LLM applications.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 每时每刻都有更多这样的框架被创建，但随着越来越多的组织希望将基于LLM的系统从有趣的证明概念转变为生产解决方案，熟悉核心概念和现有数据集将变得越来越重要。在本章的最后部分，我们将简要讨论我在构建LLM应用时围绕“提示”管理所看到的一些具体挑战。
- en: PromptOps
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PromptOps
- en: When working with generative AI that takes text inputs, the data we input is
    often referred to as “prompts” to capture the conversational origin of working
    with these models and the concept that an input demands a response, the same way
    a prompt from a person would. For simplicity, we will call any input data that
    we feed to an LLM a prompt, whether this is in a user interface or via an API
    call and irrespective of the nature of the content we provide to the LLM.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当与需要文本输入的生成式AI工作时，我们输入的数据通常被称为“提示”（prompts），以捕捉与这些模型互动的对话起源以及输入需要响应的概念，就像人的提示一样。为了简单起见，我们将任何我们提供给LLM的输入数据都称为提示，无论这是通过用户界面还是通过API调用，也不论我们提供给LLM的内容性质如何。
- en: Prompts are often quite different beasts from the data we typically feed into
    an ML model. They can be effectively freeform, have a variety of lengths, and,
    in most cases, express the intent for how we want the model to act. In other ML
    modeling problems, we can certainly feed in unstructured textual data, but this
    intent piece is missing. This all leads to some important considerations for us
    as ML engineers working with these models.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 提示通常与我们通常喂给ML模型的数据大不相同。它们可以是自由形式的，长度各异，并且在大多数情况下，表达了我们希望模型如何行动的意图。在其他ML建模问题中，我们当然可以输入非结构化文本数据，但这个意图部分是缺失的。这导致我们作为与这些模型一起工作的ML工程师需要考虑一些重要因素。
- en: First, the shaping of prompts is important. The term **prompt engineering**
    has become popular in the data community recently and refers to the fact that
    there is often a lot of thought that goes into designing the content and format
    of these prompts. This is something we need to bear in mind when designing our
    ML systems with these models. We should be asking questions like “Can I standardize
    the prompt formats for my application or use case?”, “Can I provide appropriate
    additional formatting or content on top of what a user or input system provides
    to get a better outcome?”, and similar questions. I will stick with calling this
    prompt engineering.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，提示的塑造很重要。术语**提示工程**最近在数据社区中变得流行，它指的是在设计和这些提示的内容和格式时往往需要大量的思考。当我们用这些模型设计我们的ML系统时，我们需要牢记这一点。我们应该问自己像“我能为我的应用程序或用例标准化提示格式吗？”“我能在用户或输入系统提供的之上提供适当的额外格式或内容，以获得更好的结果吗？”等问题。我将坚持使用这个术语来称呼提示工程。
- en: Secondly, prompts are not your typical ML input, and tracking and managing them
    is a new, interesting challenge. This challenge is compounded by the fact that
    the same prompt may give very different outputs for different models, or even
    with different versions of the same model. We should think carefully about tracking
    the lineage of our prompts and the outputs they generate. I term this challenge
    **prompt management**.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，提示并非典型的机器学习输入，跟踪和管理它们是一个新的、有趣的挑战。这个挑战由于同一个提示可能对不同模型或同一模型的不同版本产生非常不同的输出而变得更加复杂。我们应该仔细考虑跟踪我们提示的谱系以及它们产生的输出。我将这个挑战称为**提示管理**。
- en: Finally, we have a challenge that is not necessarily unique to prompts but definitely
    becomes a more pertinent one if we allow users of a system to feed in their own
    prompts, for example in chat interfaces. In this case, we need to apply some sort
    of screening and obfuscation rules to data coming in and coming out of the model
    to ensure that the model is not “jailbroken” in some way to evade any guardrails.
    We would also want to guard against adversarial attacks that may be designed to
    extract training data from these systems, thereby gaining personally identifiable
    or other critical information that we do not wish to be shared.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们面临的一个挑战并非必然与提示相关，但如果允许系统用户输入自己的提示，例如在聊天界面中，它就变得更为相关。在这种情况下，我们需要对进入和离开模型的数据应用某种筛选和混淆规则，以确保模型不会被以某种方式“越狱”，从而规避任何安全措施。我们还想防范可能旨在从这些系统中提取训练数据，从而获取我们不希望共享的个人身份信息或其他关键信息的对抗性攻击。
- en: As you begin to explore this brave new world of LLMOps with the rest of the
    world, it will be important to keep these prompt-related challenges in mind. We
    will now conclude the chapter with a brief summary of what we have covered.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 当你们开始和全世界一起探索这个充满挑战的新世界LLMOps时，牢记这些与提示相关的挑战将非常重要。现在，我们将以一个简要总结来结束本章，总结我们已经讨论的内容。
- en: Summary
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we focused on deep learning. In particular, we covered the
    key theoretical concepts behind deep learning, before moving on to discuss how
    to build and train your own neural networks. We walked through examples of using
    off-the-shelf models for inference and then adapting them to your specific use
    cases through fine-tuning and transfer learning. All of the examples shown were
    based on heavy use of the PyTorch deep learning framework and the Hugging Face
    APIs.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于深度学习。特别是，我们讨论了深度学习背后的关键理论概念，然后转向讨论如何构建和训练自己的神经网络。我们通过使用现成的模型进行推理的示例，然后通过微调和迁移学习将它们适应到特定的用例。所有展示的示例都是基于大量使用PyTorch深度学习框架和Hugging
    Face API。
- en: We then moved on to the topical question of the largest models ever built, LLMs,
    and what they mean for ML engineering. We explored a little of their important
    design principles and behaviors before showing how to interact with them in pipelines
    using the popular LangChain package and OpenAI APIs. We also explored the potential
    for using LLMs to help with improving software development productivity, and what
    this will mean for you as an ML engineer.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着讨论了当前的热门话题：迄今为止构建的最大模型——大型语言模型（LLMs），以及它们对机器学习工程的意义。我们在展示如何使用流行的LangChain包和OpenAI
    API在管道中与它们交互之前，简要探讨了它们重要的设计原则和行为。我们还探讨了使用LLMs来提高软件开发生产力的潜力，以及这对作为机器学习工程师的你们意味着什么。
- en: We finished the chapter with an exploration of the new topic of LLMOps, which
    is all about applying the principles of ML engineering and MLOps that we have
    been discussing throughout this book to LLMs. This covered the core components
    of LLMOps and also some new capabilities, frameworks, and datasets that can be
    used to validate your LLMs. We concluded with some pointers on managing your LLM
    prompts and how the concepts around experiment tracking we covered in *Chapter
    3*, *From Model to Model Factory*, should be translated to apply in this case.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以对LLMOps这一新主题的探讨结束本章，LLMOps是关于将本书中讨论的机器学习工程和MLOps原则应用于LLMs。这涵盖了LLMOps的核心组件，以及一些可以用来验证你的LLMs的新功能、框架和数据集。我们最后提供了一些关于管理你的LLM提示的指导，以及如何将我们在*第3章*，“从模型到模型工厂”中讨论的实验跟踪概念应用到这种情况。
- en: The next chapter will begin the final section of the book and will cover a detailed
    end-to-end example where we will build an ML microservice using Kubernetes. This
    will allow us to apply many of the skills we have learned throguh the book.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将开始书的最后一部分，并将涵盖一个详细的端到端示例，我们将使用 Kubernetes 构建一个 ML 微服务。这将使我们能够应用我们在书中学到的许多技能。
- en: Join our community on Discord
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussion with the author and other
    readers:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/mle](https://packt.link/mle)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mle](https://packt.link/mle)'
- en: '![](img/QR_Code102810325355484.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code102810325355484.png)'
