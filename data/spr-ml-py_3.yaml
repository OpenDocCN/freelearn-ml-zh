- en: Working with Non-Parametric Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用非参数模型
- en: In the last chapter, we introduced parametric models and explored how to implement
    linear and logistic regression. In this chapter, we will cover the non-parametric
    model family. We will start by covering the bias-variance trade-off, and explaining
    how parametric and non-parametric models differ at a fundamental level. Later,
    we'll get into decision trees and clustering methods. Finally, we'll address some
    of the pros and cons of the non-parametric models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了参数模型并探讨了如何实现线性回归和逻辑回归。在本章中，我们将讨论非参数模型系列。我们将从讨论偏差-方差权衡开始，并解释参数模型和非参数模型在基本层面上的不同。稍后，我们将深入探讨决策树和聚类方法。最后，我们将讨论非参数模型的一些优缺点。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: The bias/variance trade-off
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差/方差权衡
- en: An introduction to non-parametric models and decision trees
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非参数模型和决策树简介
- en: Decision trees
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Implementing a decision tree from scratch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始实现决策树
- en: Various clustering methods
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种聚类方法
- en: Implementing **K-Nearest Neighbors** (**KNNs**) from scratch
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始实现**K-最近邻**（**KNN**）
- en: Non-parametric models – the pros and cons
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非参数模型——优缺点
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you will need to install the following software, if you haven''t
    already done so:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，如果你尚未安装以下软件，你需要先安装：
- en: Jupyter Notebook
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jupyter Notebook
- en: Anaconda
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anaconda
- en: Python
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python
- en: The code files for this chapter can be found at [https:/​/​github.​com/​PacktPublishing/
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在[https:/​/​github.​com/​PacktPublishing/](https:/​/​github.​com/​PacktPublishing/)找到。
- en: Supervised-Machine-Learning-with-Python](https:/%E2%80%8B/%E2%80%8Bgithub.%E2%80%8Bcom/%E2%80%8BPacktPublishing/%20Supervised-Machine-Learning-with-Python).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[监督式机器学习与Python](https:/​/​github.​com/​PacktPublishing/​Supervised-Machine-Learning-with-Python)'
- en: The bias/variance trade-off
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差/方差权衡
- en: In this section, we're going to continue our discussion of error due to **bias**,
    and introduce a new source of error called **variance**. We will begin by clarifying
    what we mean by error terms and then dissect various sources of modeling errors.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将继续讨论由**偏差**引起的误差，并介绍一种新的误差来源——**方差**。我们将首先澄清误差项的含义，然后剖析建模误差的各种来源。
- en: Error terms
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 误差项
- en: One of the central topics of model building is reducing error. However, there
    are several types of errors, two of which we have control over to some extent.
    These are called **bias** and **variance**. There is a trade-off in the ability
    for a model to minimize either bias or variance, and this is called the **bias-variance
    trade-off** or the **bias-variance dilemma**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建的核心主题之一是减少误差。然而，误差有多种类型，其中有两种是我们在某种程度上可以控制的。这两种被称为**偏差**和**方差**。模型在减少偏差或方差的能力上存在权衡，这被称为**偏差-方差权衡**或**偏差-方差困境**。
- en: Some models do well at controlling both to an extent. However, this is a dilemma
    that, for the most part, is always going to be present in your modeling considerations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型在一定程度上能很好地控制这两者。然而，这个困境在大多数情况下总会出现在你的建模过程中。
- en: Error due to bias
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 由偏差引起的误差
- en: 'High bias can also be called underfitting or over-generalization. High bias
    generally leads to an inflexible model that misses the true relationship between
    features in the target function that we are modeling. In the following diagram,
    the true relationship between *x* and *y* is oversimplified and the true function
    of *f(x)*, which is essentially a logic function, is missed:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 高偏差也可以称为欠拟合或过度泛化。高偏差通常会导致模型缺乏灵活性，错过我们正在建模的目标函数中特征与特征之间的真实关系。在下图中，*x*与*y*之间的真实关系被过于简化，错过了真实的*
    f(x) *函数，它本质上是一个逻辑函数：
- en: '![](img/52bed5c5-28a1-4bc7-8087-1ed23193d643.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52bed5c5-28a1-4bc7-8087-1ed23193d643.png)'
- en: Parametric models tend to suffer high bias problems more than non-parametric
    models. Examples of this include linear and logistic regression, which we will
    explore in more detail in the final section of this chapter.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 参数模型往往比非参数模型更容易受到高偏差问题的影响。线性回归和逻辑回归就是这类问题的例子，我们将在本章的最后一部分更详细地探讨这些内容。
- en: Error due to variance
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 由方差引起的误差
- en: 'In contrast, for the high bias that you''re now familiar with, error due to
    variance can be thought of as the variability of a model''s prediction for a given
    sample. Imagine you repeat the modeling process many times; the variance is how
    much the predictions for a given sample will vary across different inductions
    of the model. High variance models are commonly referred to as overfitting, and
    suffer the exact inverse of high bias. That is, they do not generalize enough.
    High variance usually comes from a model''s insensitivity to the signal as a result
    of its hypersensitivity to noise. Generally, as model complexity increases, variance
    becomes our primary concern. Notice in the diagram that a polynomial term has
    led to a very overfitting model, where a simple **logit** function would have
    sufficed:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，对于你现在已经熟悉的高偏差，方差引起的误差可以看作是模型对于给定样本预测的变化性。想象你多次重复建模过程；方差就是不同模型推导下，对于给定样本的预测结果的波动。高方差模型通常被称为过拟合，且正好与高偏差相反。也就是说，它们的泛化能力不足。高方差通常源于模型对信号的敏感性不足，而对噪声的过度敏感。一般而言，随着模型复杂度的增加，方差成为我们主要关注的问题。请注意，在图表中，多项式项导致了一个过度拟合的模型，而一个简单的**logit**函数就足够了：
- en: '![](img/caf73d07-a9f2-4b3d-9759-7957874afafd.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/caf73d07-a9f2-4b3d-9759-7957874afafd.png)'
- en: Unlike high bias problems, high variance problems can be addressed with more
    training data, which can help the model learn to generalize a bit better. So,
    examples of high variance models, which we haven't yet covered, are decision trees
    and KNN. We're going to cover both of these in this chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与高偏差问题不同，高方差问题可以通过更多的训练数据来解决，这有助于模型更好地学习和泛化。所以，尚未覆盖的高方差模型的例子有决策树和KNN。我们将在本章中介绍这两者。
- en: Learning curves
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习曲线
- en: 'In this section, we will examine a handy way to diagnose high bias or variance
    called **learning curves**. In this example Python snippet, we will leverage the
    function in the `packtml.utils` submodule called `plot_learning_curve`, as shown
    in the following code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究一种方便的方式来诊断高偏差或高方差，称为**学习曲线**。在这个Python示例代码中，我们将利用`packtml.utils`子模块中的`plot_learning_curve`函数，如下所示：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This function is going to take an estimator and fit it on various sizes of
    training data defined in the `train_sizes` parameter. What is displayed is the
    model performance on the train and the corresponding validation set for each incremental
    model fit. So, this example uses our linear regression class to model the Boston
    housing data, which is a regression problem and displays symptoms of high bias.
    Notice that our error is very similar for the training and validation sets. It
    got there very rapidly, but it''s still relatively high. They don''t improve as
    our training set grows at all. We get the output for the preceding code as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将接受一个估计器，并将其在`train_sizes`参数定义的不同训练数据集大小上进行拟合。显示的是每次增量模型拟合后的训练集和对应验证集的模型性能。因此，这个示例使用我们的线性回归类来建模波士顿房价数据，这是一个回归问题，且显示出高偏差的症状。注意到我们的误差在训练集和验证集之间非常相似。它迅速达到了某个值，但仍然相对较高。随着训练集的增长，它们并没有得到改善。我们得到的输出如下：
- en: '![](img/5bb09654-b826-464c-981a-9d322d1a7c55.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5bb09654-b826-464c-981a-9d322d1a7c55.png)'
- en: 'Alternatively, if we model the same data with a decision tree regressor, we
    notice the symptoms of high variance or overfitting:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果我们使用决策树回归器来建模相同的数据，我们会注意到高方差或过拟合的症状：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'There is a huge discrepancy between the **Training score** and **Validation
    score**, and even though it gets better with more data, it never quite reaches
    convergence. We get the following output:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练得分**和**验证得分**之间存在巨大的差异，尽管随着数据量的增加，得分有所改善，但始终没有完全收敛。我们得到如下输出：'
- en: '![](img/e0e0dc71-135b-4802-8a4a-996d3e6db219.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0e0dc71-135b-4802-8a4a-996d3e6db219.png)'
- en: Strategies for handling high bias
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理高偏差的策略
- en: 'If you determine that you''re suffering from a high bias problem, you can try
    making your model more complex by engineering more informative signal-rich features.
    For example, here, one thing you could try doing is creating new features that
    are polynomial combinations of your *x1* so, you can create logit function of
    *x1*, and that would model our function perfectly. You can also try tuning some
    of the hyperparameters, for instance, KNNs, even though it''s a high variance
    model, and it can become highly biased very quickly as you increase the *k* hyperparameter,
    and vice versa:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你确定自己正在遭遇高偏差问题，你可以通过构造更多富有信息的信号特征来使模型变得更复杂。例如，在这里，你可以尝试创建新的特征，这些特征是 *x1* 的多项式组合，这样，你就可以创建
    *x1* 的对数几率函数，从而完美地建模我们的函数。你还可以尝试调整一些超参数，例如 KNN，尽管它是一个高方差模型，但随着 *k* 超参数的增加，它可能会迅速变得高度偏差，反之亦然：
- en: '![](img/8d8f5203-9b9c-4b73-8e96-2f0c952b665a.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d8f5203-9b9c-4b73-8e96-2f0c952b665a.png)'
- en: Strategies for handling high variance
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理高方差的策略
- en: 'If you, instead, find yourself facing a high variance problem, we''ve already
    seen how more training data can help, to an extent. You can also perform some
    feature selection to pare down the model''s complexity. The most robust solution
    lies in bagging or ensembling, which combines the output to mini models, which
    all, in turn, vote on each sample''s label or output regression score:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你面临的是高方差问题，我们已经看到更多的训练数据在一定程度上能有所帮助。你还可以进行一些特征选择，以减少模型的复杂性。最稳健的解决方案是袋装法或集成方法，它将多个小模型的输出结合起来，所有这些模型都会对每个样本的标签或回归分数进行投票：
- en: '![](img/477ce929-ccc1-4fd0-8f69-882886f9d56f.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/477ce929-ccc1-4fd0-8f69-882886f9d56f.png)'
- en: In the next section, we're going to more formally define non-parametric learning
    algorithms and introduce decision trees.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更加正式地定义非参数学习算法，并介绍决策树。
- en: Introduction to non-parametric models and decision trees
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非参数模型和决策树简介
- en: In this section, we're going to formally define what non-parametric learning
    algorithms are, and introduce some of the concepts and math behind our first algorithm,
    called **decision trees**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将正式定义什么是非参数学习算法，并介绍我们第一个算法——**决策树**背后的某些概念和数学原理。
- en: Non-parametric learning
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非参数学习
- en: Non-parametric models do not learn parameters. They do learn characteristics
    or attributes about the data, but not parameters in the formal sense. We will
    not end up extracting a vector of coefficients. The easiest example is a decision
    tree. A decision tree is going to learn where to recursively split data so that
    its leaves are as pure as possible. So, in that sense, the decision function is
    a splitting point for each leaf that is not a parameter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 非参数模型不学习参数。它们确实学习数据的特征或属性，但不是在正式意义上学习参数。我们最终不会提取一个系数向量。最简单的例子是决策树。决策树会学习如何递归地划分数据，以便它的叶子尽可能纯净。因此，从这个意义上来说，决策函数是每个叶子的划分点，而不是参数。
- en: Characteristics of non-parametric learning algorithms
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非参数学习算法的特点
- en: Non-parametric models tend to be a bit more flexible and do not make as many
    assumptions about the underlying structure of the data. Many linear models, or
    parametric models, for instance, assume that a normal distribution for each feature
    is required to be independent of one another. This is not the case with most non-parametric
    models. As we covered in the last section, the bias-variance trade-off also knows
    that non-parametric models will require more data to train, so as not to be as
    afflicted by high variance problems.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 非参数模型通常更加灵活，并且不会对数据的底层结构做出过多假设。例如，许多线性模型或参数化模型假设每个特征都需要满足正态分布，并且它们相互独立。然而，大多数非参数模型并不这样假设。正如我们在上一节中讨论的，偏差-方差权衡也表明，非参数模型需要更多的数据来训练，以避免高方差问题的困扰。
- en: Is a model parametric or not?
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个模型是参数化的还是非参数化的？
- en: If you find yourself wondering whether or not a model is parametric, it's probably
    not the most important question to answer. You should select the modeling technique
    that best suits your data. However, a good rule of thumb is how many characteristics
    or parameters a model learns. If it's related to the feature space or dimensionality,
    it's probably parametric, for instance, learning the number of coefficients theta
    in a linear regression. If, instead, it's related to the number of samples, it's
    probably non-parametric, for instance, the depth of the decision tree or the number
    of neighbors in clustering.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在想，是否一个模型是参数化的，这可能不是最重要的问题。你应该选择最适合你数据的建模技术。不过，一个好的经验法则是，模型学习了多少特征或参数。如果它与特征空间或维度有关，那么它很可能是参数化的，例如，学习线性回归中的系数θ。如果它与样本数有关，那么它很可能是非参数化的，例如，决策树的深度或聚类中的邻居数。
- en: An intuitive example – decision tree
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个直观的例子——决策树
- en: 'A decision tree will start out with all of the data, iteratively making splits
    until each leaf has maximized its purity or some other stopping criteria is met.
    In this example, we will start out with three samples. The tree learns that splitting
    on the color feature will be our most informative step towards maximizing its
    leaf purity. So, that''s the first thing to note. The first split is the most
    informative split that will best segment the data into two pieces. As shown in
    the following diagram, the potato class is isolated on the left by splitting on
    color. We have perfectly classified the potato. However, the other two samples
    still need to be split. So, the tree learns that, if it''s orange and round, it''s
    a sweet potato. Otherwise, if it''s just orange and not round, it''s a carrot,
    and it goes left one more time. Here, we can see a perfect split of all of our
    classes:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树将从所有数据开始，迭代地进行分裂，直到每个叶子节点的纯度最大化或满足其他停止标准。在这个例子中，我们从三个样本开始。树学习到，在颜色特征上进行分裂将是我们最有信息量的一步，有助于最大化叶子节点的纯度。所以，第一个需要注意的点是，第一次分裂是最有信息量的分裂，它能最好地将数据分成两部分。如下面的图所示，土豆类别通过颜色分裂被隔离在左边。我们已经完美地分类了土豆。然而，另外两个样本仍然需要进一步分裂。于是，决策树学到，如果是橙色且圆形的，它就是一个甘薯；否则，如果它只是橙色且不圆，它就是胡萝卜，然后再向左分裂一次。在这里，我们可以看到所有类别的完美分裂：
- en: '![](img/f35a17a3-c73c-4c27-8952-478a124f2999.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f35a17a3-c73c-4c27-8952-478a124f2999.png)'
- en: Decision trees – an introduction
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树——简介
- en: 'What we''re interested in doing with decision trees is defining a flexible
    extensible algorithm that can achieve the decision tree. This is where the **Classification
    and Regression Trees** (**CART**) algorithm comes in. CART is generalizable to
    either task and it learns, essentially, by asking questions of the data. At each
    split point, CART will scan the entire feature space, sampling values from each
    feature to identify the best feature and value for the split. It does this by
    evaluating the information gain formula, which seeks to maximize a gain in purity
    in the split, which is pretty intuitive. *Gini Impurity* is computed at the leaf
    level, and is a way of measuring how pure or impure a leaf is; its formula is
    as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对决策树感兴趣的，是定义一个灵活且可扩展的算法，能够实现决策树。这里，**分类与回归树**（**CART**）算法发挥了作用。CART算法可以广泛应用于这两类任务，基本上是通过向数据提问来学习。在每一个分裂点，CART会扫描整个特征空间，从每个特征中抽取值，以确定最佳的特征和分裂值。它通过评估信息增益公式来实现这一点，该公式旨在最大化分裂中纯度的增益，这一点是相当直观的。*基尼不纯度*在叶子节点级别进行计算，它是衡量叶子节点纯度或不纯度的一种方式；其公式如下：
- en: '![](img/d6268b54-06ba-49be-8da4-7dc894aee979.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6268b54-06ba-49be-8da4-7dc894aee979.png)'
- en: '*IG* at the bottom is our information gain, and it''s the gini of the root
    node, as shown here:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 底部的*IG*是我们的信息增益，它是根节点的基尼指数，如下所示：
- en: '![](img/fce97018-e381-4527-9384-4960853433e3.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fce97018-e381-4527-9384-4960853433e3.png)'
- en: How do decision trees make decisions?
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树是如何做出决策的？
- en: We will first address the objective before looking at the math. We will compute
    the information gain of a split to determine the best splitting point. If information
    gain is positive, that means we have learned something from that split, which
    might be the optimal point. If information gain is negative, it means we're actually
    going in the wrong direction. What we have done is created a non-informative split.
    Each split in the tree will select the point that maximizes information gain.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先处理目标，然后再看数学部分。我们将计算一个拆分的**信息增益**，以确定最佳拆分点。如果信息增益为正，这意味着我们从该拆分中学到了东西，这可能是最佳点。如果信息增益为负，这意味着我们实际上走错了方向。我们所做的就是创建了一个没有信息的拆分。树中的每个拆分将选择最大化信息增益的点。
- en: 'So, here''s the setup:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，下面是设置：
- en: '![](img/a2152363-2f8b-4a2f-8327-3bf389e2ece8.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2152363-2f8b-4a2f-8327-3bf389e2ece8.png)'
- en: A Gini impurity of 0 would be particularly pure. A higher impurity essentially
    means that a more random collection of classes has found itself in that leaf.
    So, our root is fairly impure. Now our tree will scan the entire feature space,
    sampling values from each feature. It will evaluate the information gained on
    if we were to split there. So, let's say that our tree selects *x12*. We will
    split along the same value that's sampled that variable. What we want to know
    is, if we end up getting more pure leaf nodes from this split, we will compute
    the information gain. To do that, we have to compute the Gini for each of the
    leaf nodes that we just created.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼不纯度为0时表示特别纯净。较高的不纯度本质上意味着在该叶子节点中找到了更多随机的类别。所以，我们的根节点相当不纯。现在我们的树将扫描整个特征空间，从每个特征中采样值。它将评估如果在这里进行拆分，信息增益会如何。假设我们的树选择了*x12*，我们将沿着样本值拆分该变量。我们想知道的是，如果通过这个拆分我们得到更纯净的叶子节点，我们将计算信息增益。为此，我们必须计算刚刚创建的每个叶子节点的基尼不纯度。
- en: 'We will look at an example of this problem using the `packtml` library. We
    have the `example_information_gain.py` file, which is in the `examples/decision_tree`
    directory:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用`packtml`库来查看这个问题的示例。我们有`example_information_gain.py`文件，它位于`examples/decision_tree`目录下：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we will compute the information gain using the `InformationGain` class
    from `packtml.decision_tree.metrics`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`packtml.decision_tree.metrics`中的`InformationGain`类来计算信息增益：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will get the following output when we run `example_information_gain.py`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行`example_information_gain.py`时，将得到以下输出：
- en: '![](img/064c2ef2-3f84-4a24-8500-8b100b2fd139.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/064c2ef2-3f84-4a24-8500-8b100b2fd139.png)'
- en: In the next section, we're going to go a bit deeper and learn how a decision
    tree produces the candidate split for us to evaluate.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将更深入地了解并学习决策树是如何生成候选拆分供我们评估的。
- en: Decision trees
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: In the previous section, we computed the information gained for a given split.
    Recall that it's computed or calculated by computing the Gini impurity for the
    parent node in each `LeafNode`. A higher information again is better, which means
    we have successfully reduced the impurities of the child nodes with our split.
    However, we need to know how a candidate split is produced to be evaluated.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分中，我们计算了给定拆分的**信息增益**。回想一下，它是通过计算每个`LeafNode`中的父节点的基尼不纯度来计算的。较高的信息增益更好，这意味着我们通过拆分成功地减少了子节点的杂质。然而，我们需要知道如何生成候选拆分，以便进行评估。
- en: 'For each split, beginning with the root, the algorithm will scan all the features
    in the data, selecting a random number of values for each. There are various strategies
    to select these values. For the general use case, we will describe and select
    a *k* random approach:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个拆分，从根节点开始，算法将扫描数据中的所有特征，为每个特征选择一个随机值。选择这些值的策略有很多种。对于一般的使用案例，我们将描述并选择一个*k*随机方法：
- en: For each of the sample values in each feature, we simulate a candidate split
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个特征中的每个样本值，我们模拟一个候选拆分。
- en: Values above the sampled value go to one direction, say left, and values above
    that go the other direction, that is, to the right
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高于采样值的值会朝一个方向，比如左侧，超过该值的会朝另一个方向，即右侧。
- en: Now, for each candidate split, we're going to compute the information gain,
    and select the feature value combination that produces the highest information
    gain, which is the best split
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，对于每个候选拆分，我们将计算信息增益，并选择产生最高信息增益的特征值组合，即最佳拆分。
- en: From the best split, we will recurse down each split as a new parent until the
    stopping criteria are met
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从最佳拆分开始，我们将递归向下遍历每个拆分，直到满足停止标准。
- en: Now, regarding where and when to stop the criteria, there are various methods
    we can use for this. A common one is maximum tree depth. If we get too deep, we
    start to overfit. So, we might prune our tree when it grows five times deep, for
    instance. Another, is a minimum number of samples per leaf. If we have 1 million
    training samples, we grow our tree until there's one sample per leaf; we're also
    probably overfitting. So, the min samples leaf parameter will allow us to stop
    splitting a leaf once there are, say, 50 samples remaining after a split. This
    is a tunable hyperparameter that you can work within your cross-validation procedure.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关于在哪里以及何时停止划分标准，我们可以使用多种方法来实现。一个常见的方法是最大树深度。如果树深度过大，我们会开始过拟合。例如，当树生长到五层深时，我们可能会进行剪枝。另一个方法是每个叶子的最小样本数。如果我们有100万个训练样本，我们会继续生长树，直到每个叶子只有一个样本；这也可能会过拟合。因此，最小样本数叶子参数可以让我们在划分后，如果每个叶子剩余的样本数，比如说50个，便停止划分。这是一个可以调节的超参数，你可以在交叉验证过程中进行调整。
- en: Splitting a tree by hand
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动划分决策树
- en: 'We will now look into an exercise. Let''s imagine we have this training set:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来进行一个练习。假设我们有如下的训练集：
- en: '![](img/cbadade4-c934-4312-b1d4-61859101f0cb.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cbadade4-c934-4312-b1d4-61859101f0cb.png)'
- en: From the preceding data, where is the optimal split point? What feature or value
    combination should we use to define our rule?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的数据来看，最佳的划分点在哪里？我们应该使用什么特征或值的组合来定义我们的规则？
- en: If we split on x1
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果我们在x1上划分
- en: 'First, we will compute the Gini impurity of the root node, which is the pre-split
    state. We get *0.444*, as shown:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将计算根节点的基尼不纯度，这就是未划分的状态。我们得到*0.444*，如图所示：
- en: '![](img/6ace640a-6b82-49f9-938a-94cd878bf06f.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ace640a-6b82-49f9-938a-94cd878bf06f.png)'
- en: 'The next stage in the algorithm is to iterate each feature. There are three
    cases, shown as follows. Using our *IG* formula, we can compute which is the best
    split point for this feature. The first happens to be the best, in this case:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的下一步是遍历每个特征。下面是三种情况。使用我们的*IG*公式，我们可以计算出哪个是这个特征的最佳划分点。第一种情况恰好是最佳的：
- en: '![](img/cbf032eb-f9f3-498e-9bbe-a0a42ca5c3ea.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cbf032eb-f9f3-498e-9bbe-a0a42ca5c3ea.png)'
- en: 'Splitting on the second case, where *x1* is greater than or equal to *4*, is
    not a good idea since the result is no different than the state at the root. Therefore,
    our information gain is *0*:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种情况下，*x1*大于等于*4*时进行划分并不是一个好主意，因为结果与根节点的状态没有不同。因此，我们的信息增益是*0*：
- en: '![](img/72157416-b405-492b-a5ae-d6668cac95a4.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/72157416-b405-492b-a5ae-d6668cac95a4.png)'
- en: 'In the last case, splitting when *x1* is greater than or equal to *37* does
    yield a positive IG since we have successfully split one sample of the positive
    class away from the others:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一种情况下，当*x1*大于等于*37*时，的确产生了正的信息增益，因为我们成功地将一个正类样本与其他样本分开：
- en: '![](img/cadc0ea0-fc63-4d29-9e00-94f92d172e50.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cadc0ea0-fc63-4d29-9e00-94f92d172e50.png)'
- en: If we split on x2
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果我们在x2上划分
- en: 'However, we don''t know if we''re done yet. So, we will iterate to *x2*, where
    there might be a better split point:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还不确定是否已经完成。因此，我们将迭代到*x2*，看看是否有更好的划分点：
- en: '![](img/5f99b20b-6803-4822-aee8-4094b8f563c3.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f99b20b-6803-4822-aee8-4094b8f563c3.png)'
- en: The candidate split shows us that neither potential split is the optimal split
    when compared to the current best that we've identified in *x1*.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 候选划分点显示，当我们与当前在*x1*中识别到的最佳划分点进行比较时，两个潜在的划分都不是最佳划分。
- en: 'Therefore, the best split is *x1* greater than or equal to *21*, which will
    perfectly separate our class labels. You can see in this decision tree when we
    produce that split, sure enough, we get perfectly separated classes:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最佳的划分是*x1*大于等于*21*，这将完美地分离我们的类别标签。你可以在这个决策树中看到，当我们进行这个划分时，确实能够得到完全分离的类别：
- en: '![](img/2aee3630-4c83-49b8-9d60-fc674c593e8b.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2aee3630-4c83-49b8-9d60-fc674c593e8b.png)'
- en: However, in a larger example, we may not have perfectly separated our classes,
    if we had millions of samples, for instance. Hence, we would recurse at this point,
    finding new split points for each node until we hit our stopping criteria. At
    this point, let's use our `packtml` library to run this exact example and show
    that we do in fact identify the same optimal split point, and prove that is not
    just trickery of the hand.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在更大的例子中，我们可能无法完全分离我们的类别，如果我们有数百万个样本的话。例如，在这种情况下，我们会在此时递归，找到每个节点的新划分点，直到满足停止标准。此时，我们可以使用我们的`packtml`库来运行这个例子，并展示我们确实能够识别出相同的最佳划分点，从而证明这不是手段上的巧妙操作。
- en: 'In PyCharm, the `example_classification_split.py` file is open. This is located
    inside your `examples` directory and within the `decision_tree` examples directory.
    You can see we''re going to import two things from `packtml`. Both of them happen
    to be inside the `decision_tree` submodule where you got `RandomSplitter`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyCharm 中，`example_classification_split.py` 文件已经打开。这个文件位于你的 `examples` 目录下，并且在
    `decision_tree` 示例目录内。你可以看到我们将从 `packtml` 中导入两个东西，它们恰好都在 `decision_tree` 子模块内，你会找到
    `RandomSplitter`：
- en: '[PRE4]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We already looked at `InformationGain` a little bit in the last section to
    compute our information gain candidate split. Here, we will look at how we actually
    create the candidate split. We get the following data along with the corresponding
    class labels:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节中已经稍微看过 `InformationGain`，用来计算我们的信息增益候选拆分。这里，我们将看到我们如何实际创建候选拆分。我们得到如下数据和相应的类别标签：
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`RandomSplitter` will evaluate each of the preceding values since `n_val_sample`
    is `3`. So, it''s going to compute three candidates split points for each feature,
    and we will find out which of them are the best:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomSplitter` 会评估前面提到的每个值，因为 `n_val_sample` 是 `3`。所以，它将为每个特征计算三个候选拆分点，我们将找出其中哪个是最好的：'
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When we run the preceding code, we see `best_feature` is `0` and `best_value`
    is `21`, meaning that anything greater than or equal to `21` in feature `0` will
    go left, and everything else goes right. The `InformationGain` we get is `0.444`,
    which, sure enough, when we computed it by hand, is exactly what we expected:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行前面的代码时，我们看到 `best_feature` 是 `0`，`best_value` 是 `21`，这意味着在特征 `0` 中，任何大于或等于
    `21` 的值将走左侧，而其他值则走右侧。我们得到的 `InformationGain` 是 `0.444`，这确实是我们手动计算时得到的结果，正是我们预期的：
- en: '![](img/5b3a8a2e-2d07-4764-9e59-4ebfdc49d163.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b3a8a2e-2d07-4764-9e59-4ebfdc49d163.png)'
- en: In the next section, we'll cover how we can implement a decision tree from scratch
    inside the `packtml` library.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论如何在 `packtml` 库中从零开始实现决策树。
- en: Implementing a decision tree from scratch
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始实现决策树
- en: We will start out by looking at the implementation of our splitting metrics.
    Then we'll cover some of our splitting logic, and finally, we'll see how we can
    wrap the tree so that we can generalize from classification and regression tasks.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从查看拆分度量的实现开始。然后我们会覆盖一些拆分逻辑，最后，我们将看到如何包装树，以便能够从分类和回归任务中进行泛化。
- en: Classification tree
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类树
- en: Let's go ahead and walk through a classification tree example. We will be using
    the information gain criteria. In PyCharm there are three scripts open, two of
    which are `metrics.py` and `cart.py`, both of which are found inside of the `packtml/decision_tree`
    submodule. Then we have the `example_classification_decision_tree.py` file, which
    is in `examples/decision_tree`. Let's start with metrics.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续看一个分类树的例子。我们将使用信息增益准则。在 PyCharm 中打开了三个脚本，其中两个是 `metrics.py` 和 `cart.py`，它们都位于
    `packtml/decision_tree` 子模块内。然后我们还有一个 `example_classification_decision_tree.py`
    文件，它在 `examples/decision_tree` 目录下。让我们从度量开始。
- en: 'If you open up the `cart.py` file, we have an order in which we should step
    through this so that you can understand how the decision tree class is going to
    work:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打开 `cart.py` 文件，我们有一个顺序步骤，以帮助你理解决策树类如何工作：
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Starting with the `metrics.py` file from the top, you can see that `_all_`
    is going to include four different metrics:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `metrics.py` 文件的顶部开始，你可以看到 `_all_` 将包括四个不同的度量：
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`entropy` and `gini_impurity` are both classification metrics. We have talked
    about `gini_impurity`. You can see here that both of them are calling the `clf_metric`
    private function as shown:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`entropy` 和 `gini_impurity` 都是分类度量。我们已经讨论过 `gini_impurity`。你可以看到这里它们都在调用 `clf_metric`
    私有函数，如下所示：'
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, `gini` and `entropy` acts essentially the same way, except that at the
    end, `gini` computes a norm essentially on itself where `entropy` is `log2`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`gini` 和 `entropy` 基本上是以相同的方式起作用，除了最后，`gini` 基本上是计算其自身的范数，而 `entropy` 是计算
    `log2`：
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: One thing to note here is that entropy and Gini are going to make a huge difference
    in how your tree performs. Gini is actually canon for the CART algorithm, but
    we included entropy here so you could see that this is something you can use if
    you want to.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一点是，熵和基尼指数将对决策树的表现产生巨大的影响。基尼指数实际上是 CART 算法的标准，但我们在这里加入了熵，以便你能看到这是你在需要时可以使用的内容。
- en: '`BaseCriterion` is our base class for a splitting criterion. We have two splitting
    criteria, `InformationGain` and `VarianceReduction`. Both of them are going to
    implement `compute_uncertainty`:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`BaseCriterion`是我们的分裂标准的基类。我们有两个分裂标准，`InformationGain`和`VarianceReduction`。它们都会实现`compute_uncertainty`：'
- en: '[PRE11]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If you remember from the last section, uncertainty is essentially the level
    of impurity, or entropy, induced by the split. When we compute `InformationGain`
    using either `gini` or `entropy`, our uncertainty is going to be `metric` pre-split:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得上一节，`uncertainty`本质上是由分裂引起的不纯度或熵。当我们使用`gini`或`entropy`计算`InformationGain`时，我们的`uncertainty`将是分裂前的`metric`：
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If we compute `uncertainty`, we would pass in a node and say compute Gini,
    for instance, on all of the samples inside of the node before we split, and then,
    when we call to actually compute `InformationGain`, we pass in `mask` for whether
    something is going `left` or `right`. We will compute the Gini on the left and
    right side, and return `InformationGain`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计算`uncertainty`，我们会传入一个节点，并在分割之前，计算节点内所有样本的基尼系数（Gini），然后当我们实际调用计算`InformationGain`时，我们传入`mask`，用来表示某个样本是去`left`还是去`right`。我们将在左右两侧分别计算基尼系数，并返回`InformationGain`：
- en: '[PRE13]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This is how we compute `InformationGain`, and this is just the wrapper class
    that we have built. `VarianceReduction` is very similar, except the `compute_uncertainty`
    function is simply going to return the variance of *y*. When we call this, we
    are subtracting the uncertainty of the pre-split node, minus the sum of the uncertainties
    for the left and right on the split. What we''re doing here is maximizing the
    reduction of the variance between each split respectively. That way, we can know
    if a split is good. It separates along a relatively intuitive line, as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们计算`InformationGain`的方法，而这只是我们构建的包装类。`VarianceReduction`非常相似，不同的是`compute_uncertainty`函数将简单地返回*y*的方差。当我们调用时，我们将前分裂节点的不确定性减去左右分裂后的不确定性之和。我们在这里所做的就是最大化每次分裂之间方差的减少。这样，我们就可以知道一个分裂是否是好的。它沿着相对直观的线进行分割，如下所示：
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'These are our two splitting criteria: `InformationGain` and `VarianceReduction`.
    We''re going to use `InformationGain` for classification and `VarianceReduction`
    for regression. Since we''re talking about classification right now, let''s focus
    on `InformationGain`. Moving over to the `cart.py` file, we see that the next
    thing we want to talk about is `RandomSplitter`.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的两个分裂标准：`InformationGain`和`VarianceReduction`。我们将使用`InformationGain`进行分类，使用`VarianceReduction`进行回归。既然我们现在讨论的是分类，让我们集中讨论`InformationGain`。接下来在`cart.py`文件中，我们看到下一个要讨论的内容是`RandomSplitter`。
- en: In one of the last sections, we learned about a strategy to produce candidate
    splits. This is essentially `RandomSplitter`. There are a lot of different strategies
    you can use here. We're going to use a bit of entropy so that we can get through
    this class and this algorithm relatively quickly, without getting into the nitty-gritty.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的一个部分中，我们了解了一种生成候选分裂的策略。这本质上就是`RandomSplitter`。这里有很多不同的策略可以使用。我们将使用一些熵（entropy），这样我们可以相对快速地完成这个类和算法，而无需深入细节。
- en: '`RandomSplitter` will take several arguments. We want `random_state` so that
    we can replicate this split later. The criterion is an instance of either `InformationGain`
    or `VarianceReduction` and the number of values that we want to sample from each
    feature:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomSplitter`将接受几个参数。我们需要`random_state`，以便以后可以重复此分裂。标准（criterion）是`InformationGain`或`VarianceReduction`的实例，以及我们希望从每个特征中采样的值的数量：'
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'So, our `find_best` function will scan the entire feature space, sample the
    number of values per split or per feature, and determine `best_value` and `best_feature`
    on which to split. This will produce our best split for the tree at the time.
    So, `best_gain` will start as `0`. If it''s negative it''s a bad one, so we don''t
    want to split at all. If it''s positive then it''s better than our current best,
    and so we''ll take that and increment it to find our best. We want to find our
    `best_feature` and our `best_value`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们的`find_best`函数将扫描整个特征空间，按每个分裂或每个特征采样值的数量，并确定`best_value`和`best_feature`作为分裂的依据。这将产生我们在当时的最佳分裂。所以，`best_gain`将从`0`开始。如果是负值，则表示不好，我们不想进行分裂。如果是正值，则意味着它比当前的最佳分裂更好，我们就采用它并继续寻找最佳分裂。我们要找到我们的`best_feature`和`best_value`：
- en: '[PRE16]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, for each of the columns of our dataset, we''re going to go ahead and grab
    out the feature. This is just a NumPy array, a 1D NumPy array:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于我们数据集的每一列，我们将提取出特征。这只是一个NumPy数组，一个一维的NumPy数组：
- en: '[PRE17]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will create a set so that we can keep track of which values we have already
    seen, if we happen to sample the same one over and over again. We will permute
    this feature so that we can shuffle it up and scan over each of the values in
    the feature. One thing you''ll note here is that we could collect just the unique
    values of the feature. But first of all, it''s kind of expensive to get the unique
    values. Secondly, that throws away all the distributional information about the
    feature. By doing this, we happen to have more of a certain value than another,
    or more values grouped more closely together. This is going to allow us to get
    a little bit more of a true sample of the feature itself:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个集合，以便跟踪我们已经看到的值，以防我们不断重复抽样相同的值。我们会对该特征进行排列，以便可以将其打乱并扫描特征中的每个值。你会注意到的一点是，我们本可以仅收集特征的唯一值。但首先，获取唯一值的成本相对较高。其次，这样会丢失关于该特征的所有分布信息。通过这种方式，我们实际上会有更多某个值，或者有更多值更加紧密地聚集在一起。这将使我们能够得到该特征更真实的样本：
- en: '[PRE18]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If the number of `seen_values` in our set is equal to the number of values
    that we want to sample, we''re going to break out. So, if we say there are `100`
    unique values, but we''ve already seen `25`, we''re going to break out. Otherwise,
    if we have already seen this value in that set, we''re going to keep going. We
    don''t want to compute the same thing over a value that we have already computed.
    So, here we will add that value to the set, and create our mask for whether we
    split left or right:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们集合中的`seen_values`的数量等于我们想要抽样的值的数量，那么我们将退出循环。所以，如果我们说有`100`个唯一值，但我们已经看到了`25`个，那么我们将退出。否则，如果我们在该集合中已经看到了这个值，我们将继续。我们不想对已经计算过的值再做重复计算。因此，在这里，我们会将该值添加到集合中，并为是否划分为左或右创建我们的掩码：
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, there''s one more corner case. If we have grabbed the minimum value, then
    our mask is going to take everything in one direction, which is what this is checking.
    We don''t want that, because, otherwise, we''re not creating a true split. So,
    if that''s the case, then we `continue`, and sample again:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，还有一个特殊情况。如果我们已经抓取了最小值，那么我们的掩码将把所有东西都划分到一个方向，这正是我们正在检查的内容。我们不希望这样，因为否则我们就没有创建一个真实的划分。所以，如果是这种情况，我们就`continue`，重新进行抽样：
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now let''s compute the gain, either `InformationGain` or `VarianceReduction`,
    which computes the Gini on the left and right side and subtracts that from the
    original uncertainty. If the `gain` is good, meaning if it''s better than the
    current best we''ve seen, then we have a new `best_feature` and a new `best_value`,
    and we store that. So, we loop over this and go over the randomly sampled values
    within each feature and determine the `best_feature` to split on and the `best_value`
    in that feature to split on. If we don''t have one, it means we never found a
    viable split, which happens in rare cases:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算增益，计算`InformationGain`或`VarianceReduction`，它会计算左侧和右侧的基尼指数，并从原始不确定性中减去它。如果`gain`很好，也就是说，如果它比我们看到的当前最佳值更好，那么我们就有了一个新的`best_feature`和新的`best_value`，并且我们会存储它。所以，我们会循环遍历这个过程，检查每个特征内随机抽样的值，并确定要划分的`best_feature`和在该特征中要划分的`best_value`。如果我们没有找到，这意味着我们没有找到一个有效的划分，这种情况发生在少数几种情况下：
- en: '[PRE21]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we will look in `LeafNode`. If you have ever built a binary tree before,
    then you would be familiar with the concept of `LeafNode`. `LeafNode` is going
    to store a left and a right pointer, both typically initialized to null to show
    that there''s nothing there. So, the leaf node, in this case, is going to be the
    guts of our decision tree. It provides the skeleton where the tree itself is just
    a wrapper:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看`LeafNode`。如果你之前构建过二叉树，那么你应该熟悉`LeafNode`的概念。`LeafNode`将存储一个左指针和一个右指针，这两个指针通常都初始化为null，表示那里没有内容。因此，在这种情况下，叶节点将是我们决策树的核心部分。它提供了一个骨架，而树本身仅仅是一个外壳：
- en: '[PRE22]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`LeafNode` is going to store `split_col`, the feature that we are splitting
    on, `split_val`, and `split_gain`, as well as `class_statistic`. So, `class_statistic`
    for classification is going to be the node, where we vote for the most common
    value. In regression, it''s going to be the mean. If you want to get really fancy
    you might use the median or some other strategy for regression. However, we''re
    just going to use the mean because we''re keeping it simple here. So, a constructor
    is going to store these values and initialize our left and right as null again:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`LeafNode`将存储`split_col`，即我们正在分割的特征，`split_val`和`split_gain`，以及`class_statistic`。因此，分类问题中的`class_statistic`将是节点，在这里我们投票选出最常见的值；而在回归问题中，它将是均值。如果你想要做得更复杂，可以使用中位数或其他回归策略。不过，我们在这里保持简单，使用均值即可。所以，构造函数将存储这些值，并再次将左节点和右节点初始化为null：'
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now in the `create_split` function, we actually get to the tree structure itself.
    But this is going to essentially split the node and create a new left and right.
    Hence, it goes from the terminal node to the next split downward, which we can
    recurse over. We will take the current set for that current dataset from the `X`
    and `y` split. Given that the value in the feature that we have already initialized
    will create our mask for left and right, if we''re going all left or all right,
    that''s where it stores. Otherwise, it''s going to produce this split, segmenting
    out the rows on the left side and the rows on the right side, or else the rows
    are none. If there''s no split on the left/right, we just use none and we will
    return `X_left`, `X_right`, `y_left` and `y_right`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在`create_split`函数中，我们实际上开始处理树的结构。但这实际上是将节点进行分割，并创建新的左节点和右节点。因此，它从终端节点开始，向下进行下一个分割，我们可以对其进行递归操作。我们将从`X`和`y`的分割中获取当前数据集的当前集合。由于我们已经初始化的特征值将创建用于左侧和右侧的掩码，如果我们是全左或全右，它就会存储在这里。否则，它将进行分割，将左侧的行和右侧的行区分开来，或者如果没有行，它就不进行分割。如果左侧/右侧没有分割，我们只使用`none`，然后返回`X_left`、`X_right`、`y_left`和`y_right`：
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The terminal is just a shortcut here for left and right. If we have either,
    then it''s not terminal. But, if it has both null for left and right, then it''s
    a terminal node:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 终端节点只是左节点和右节点的快捷方式。如果我们有任一节点，那么它就不是终端节点。但如果左右节点都为null，那它就是一个终端节点：
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will use the `predict_record` function internally for producing predictions
    inside `LeafNode`. This is going to use that `class_statistic` function that we
    have. `class_statistic` is either the mode for classification or the mean for
    regression. For predicting whether or not a record goes left or right, we recurse
    down, and that is just what is happening here in `predict`, which we''ll get to,
    and look at how we produce predictions:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`predict_record`函数在`LeafNode`内部进行预测。这个函数将使用我们已有的`class_statistic`函数。`class_statistic`对于分类问题是众数，对于回归问题是均值。为了预测一个记录是向左还是向右，我们将递归操作，而这正是`predict`函数所做的，我们稍后会讲解，并查看如何生成预测：
- en: '[PRE26]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, the trees themselves are two classes. We have `CARTRegressor` and `CARTClassifier`.
    Both of these are going to wrap the `BaseCART` class, which we will walk through
    right now. `BaseCART`, as with most of our base simple estimators that we''ve
    already walked through, is going to take two arguments for certain, which are
    `X` and `y`—our training data and our training labels. It''s also going to take
    our criterion, which we will pass at the bottom. It''s either your `InformationGain`
    for classification, `VarianceReduction` for regression, `min_samples_split`, and
    all these other hyperparameters, which we''ve already kind of talked through.
    The first thing we''re going to do is, as usual, check our `X` and `y` to make
    sure that we have all continuous values, that we''re not missing any data. This
    is just assigning `self` attributes for the hyperparameters and we will create
    our `splitter` as `RandomSplitter`, which we''re going to use in this process.
    This is how we grow the tree. It all happens in `find_next_split`. So, this is
    going to take three arguments. We''ve got our `X`, our `y`, and then the count:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，树本身有两个类别。我们有`CARTRegressor`和`CARTClassifier`。这两者都将封装`BaseCART`类，我们现在就来讲解这个类。`BaseCART`，和我们之前讲解过的大多数基本估计器一样，必定会接受两个参数，分别是`X`和`y`——我们的训练数据和训练标签。它还会接受我们的标准（criterion），我们会在后面传入它。标准可以是用于分类的`InformationGain`，用于回归的`VarianceReduction`，`min_samples_split`，以及我们已经讨论过的其他超参数。我们首先要做的，和往常一样，是检查我们的`X`和`y`，确保它们都是连续值，并且没有缺失数据。这只是为超参数分配`self`属性，我们将创建一个`splitter`，它是`RandomSplitter`，我们将在这个过程中使用它。这就是我们生长树的方式，所有操作都发生在`find_next_split`中。这个函数将接受三个参数：`X`，`y`，和样本计数：
- en: '[PRE27]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Essentially, we will recurse over the `find_next_split` function until our
    tree is fully grown or pruned. Since we''re recursing, we always set our base
    case first. If `current_depth` is equal to `maximum_depth` that we want to grow
    a tree, or the size, the number of samples in `X`, is less than or equal to the
    `min_samples_split` in our split, both of which are our terminal criteria, and
    we will return `None`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们将递归地调用`find_next_split`函数，直到树完全生长或修剪完毕。由于我们在递归，我们始终首先设置我们的基本情况。如果`current_depth`等于我们希望树生长的最大深度，或者`X`中样本的大小小于或等于`min_samples_split`，这两个条件都是我们的终止标准，这时我们将返回`None`：
- en: '[PRE28]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Otherwise, we will grab our splitter and find the best split between `X` and
    `y`, which gives us our `best_feature`, `best_value`, and `gain`, either `VarianceReduction`
    or `InformationGain`. Next, we have just found our first split. So, now we will
    create the node that corresponds to that split. The node is going to take all
    of those same arguments, plus the target statistics. When we produce predictions
    for the node, if it''s terminal, we return the node for that label; otherwise,
    we return the mean for our training labels. That''s how we assign that prediction
    there. So, now we have our node, and we want to create our split. So, we get `X_right`
    and `X_left`. We can recurse down both sides of the tree. We will use that node
    to create the split on `X` and `Y`. So, if `X` is `None`, `X_left` is `None`,
    which means we''re not going to go down to the left side anymore. If it is not
    `None`, then we can assign a node to the left, which is going to recurse on `find_next_split`.
    If `X_right` is `None` then it means we''re not going to grow it on the right
    anymore. If it''s not `None`, we can do the same thing. So, we''re going to assign
    our right side by recursing down `find_next_split`. We recurse over this, continually
    adding `current_depth + 1`, until one side has reached its `maximum_depth`. Otherwise,
    the size of the splits are no longer long enough for `min_sample_split` and we
    stop growing. So, we reach that point where we stop growing:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，我们将获取我们的`splitter`，并在`X`和`y`之间找到最佳分裂，这将给我们带来`best_feature`，`best_value`和增益`gain`，增益可以是`VarianceReduction`或`InformationGain`。接下来，我们刚刚找到了第一个分裂。那么，现在我们将创建一个对应于该分裂的节点。该节点将接受所有这些相同的参数，外加目标统计信息。当我们为节点生成预测时，如果它是终端节点，我们将返回该标签的节点；否则，我们将返回训练标签的均值。这就是我们在此处分配预测的方式。现在我们有了节点，并且希望创建分裂。因此，我们获得`X_right`和`X_left`，我们可以递归地处理树的两侧。我们将使用该节点来在`X`和`Y`上创建分裂。所以，如果`X`是`None`，`X_left`是`None`，意味着我们将不再向左递归。如果它不是`None`，那么我们可以将一个节点分配给左侧，这样它就会递归调用`find_next_split`。如果`X_right`是`None`，那么意味着我们不再继续在右侧生长。如果它不是`None`，我们可以做同样的事情。因此，我们将通过递归调用`find_next_split`来为右侧分配节点。我们会继续递归这个过程，不断地增加`current_depth
    + 1`，直到一侧达到了最大深度。否则，分裂的大小不足以满足`min_sample_split`，我们就停止生长。因此，我们到达了停止生长的临界点：
- en: '[PRE29]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, for predicting, we will traverse down the tree until we find the point
    where a record belongs. So, for each row in `X`, we will predict a row, which
    we have already looked at in the `LeafNode` class, traversing down the left or
    right until we find the node where that row belongs. Then we''ll return `class_statistics`.
    So, if the row gets to a node, it says this belongs here. If the node for that
    class, for classification, was `1`, then we return `1`. Otherwise, if the mean
    was, say, `5.6`, then we return the same. That''s how we produce these predictions,
    which we''re just going to bundle into a NumPy array:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于预测，我们将沿着树遍历，直到找到记录所属的节点。所以，对于`X`中的每一行，我们将预测一行，这在`LeafNode`类中已经看过，通过左右遍历，直到找到该行所属的节点。然后我们将返回`class_statistics`。因此，如果该行到达某个节点，它会说这个行属于这里。如果该类的节点，分类时为`1`，那么我们返回`1`。否则，如果均值是，例如，`5.6`，那么我们返回相同的值。这就是我们如何生成这些预测，我们将其打包成一个NumPy数组：
- en: '[PRE30]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'So, let''s look at how a classification decision tree can perform on some real
    data. In the following example script, we will import `CARTClassifier`:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们看看分类决策树如何在一些真实数据上表现。在下面的示例脚本中，我们将导入`CARTClassifier`：
- en: '[PRE31]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We will create two different bubbles inside of our 2D access on `multivariate_normal`.
    Using this `multivariate_normal` inside of `RandomState`, we will stack that all
    together and produce `train_test_split` as usual:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在`multivariate_normal`的2D空间中创建两个不同的气泡。使用`RandomState`中的`multivariate_normal`，我们将它们全部堆叠在一起，并像往常一样生成`train_test_split`：
- en: '[PRE32]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We will fit `CARTClassifier` and perform two different classifiers. We will
    do the first one. Knowing what you now know about variance and bias, you know
    that classifier or non-parametric models, particularly the decision tree, are
    capable of having very high variance: they can overfit really easily. So, if we
    use a really shallow depth, then we''re more likely to not overfit. In the second
    one, we''re going to try to overfit as much as we can with a max depth of `25`.
    Since we have a pretty small dataset, we can be reasonably certain that this is
    probably going to overfit. We''ll see that when we look at the actual output of
    this example:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将拟合`CARTClassifier`并执行两种不同的分类器。我们将首先进行第一个。现在你已经了解了方差和偏差，你知道分类器或非参数模型，特别是决策树，具有很高的方差：它们很容易发生过拟合。所以，如果我们使用非常浅的深度，那么我们更有可能不会过拟合。在第二个中，我们将尽可能地进行过拟合，最大深度设为`25`。由于我们拥有一个相对较小的数据集，我们可以合理地确定这可能会发生过拟合。我们将在查看这个示例的实际输出时看到这一点：
- en: '[PRE33]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'So, we fit the two of these, look at the accuracy, and plot them. Let''s go
    ahead and run the code and see how it looks:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们拟合这两棵树，查看准确性并绘制它们。让我们继续运行代码，看看结果如何：
- en: '![](img/b83c0c9e-95c3-4083-986c-9d3ad10eb106.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b83c0c9e-95c3-4083-986c-9d3ad10eb106.png)'
- en: 'If you run the preceding code, we get the test''s accuracy of 95% on our underfitted
    tree as shown:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行前面的代码，我们得到的是在我们欠拟合的树上的95%测试准确率，如下所示：
- en: '![](img/b9a554fc-c31a-41a8-82cb-c0070e92a9a6.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9a554fc-c31a-41a8-82cb-c0070e92a9a6.png)'
- en: Regression tree
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归树
- en: Now let's see how a regression tree can perform. We walked through the same
    exact implementation of our regression tree, except we're going to use the variance
    reduction. Rather than using the mode voting here for producing predictions, we're
    going to use the mean.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看回归树如何表现。我们走过了回归树的完全相同的实现，只不过我们这次将使用方差减少。与其使用模式投票来生成预测，我们将使用均值。
- en: 'Inside the `examples` directory, we have the `example_regression_decision_tree.py`
    file. So, here we will import `CARTRegressor` and use `mean_squared_error` as
    our loss function to determine how well we did:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在`examples`目录下，我们有`example_regression_decision_tree.py`文件。因此，在这里我们将导入`CARTRegressor`并使用`mean_squared_error`作为我们的损失函数，以确定我们的表现如何：
- en: '[PRE34]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We will just create random values here in a sine wave. That''s what we want
    to be our function as our output here:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里生成一个正弦波的随机值。这就是我们希望作为输出的函数：
- en: '[PRE35]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We''re going to do the same kind of thing that we did in the classification
    tree. We will fit a simple `max_depth=3` tree for a regression tree and then a
    `max_depth=10` tree for the second one. It''s not going to overfit quite as much,
    but it''ll show how we increase our predictive capacity as we grow a bit deeper:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将做与分类树中相同的事情。我们将拟合一个简单的`max_depth=3`的回归树，然后拟合一个`max_depth=10`的树作为第二个。它不会过拟合那么多，但它会展示随着我们加深模型，预测能力如何增加：
- en: '[PRE36]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here, we''re just plotting the outputs:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们只是绘制输出：
- en: '[PRE37]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let''s go ahead and run this. Rather than `example_classification_decision_tree.py`,
    we''re going to run `example_regression_decision_tree.py`:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始运行这个。与其运行`example_classification_decision_tree.py`，我们将运行`example_regression_decision_tree.py`：
- en: '![](img/6040481b-c400-4601-89a7-2bbfa8b7a408.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6040481b-c400-4601-89a7-2bbfa8b7a408.png)'
- en: 'So, first, you can see that our mean squared error decreases with the `max_depth`
    growing, which is good. You can also see that our outcome starts to model this
    sine wave pretty well as we increase the depth, and we''re able to learn this
    non-linear function very well:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，首先，你可以看到我们的均方误差随着`max_depth`的增加而减小，这是一个好现象。你还可以看到，随着深度的增加，我们的结果开始很好地模拟这个正弦波，而且我们能够很好地学习这个非线性函数：
- en: '![](img/1d363e0b-682a-4d90-a01e-4099456ded17.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d363e0b-682a-4d90-a01e-4099456ded17.png)'
- en: In the next section, we're going to look at clustering methods and move on from
    decision trees.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将讨论聚类方法并从决策树开始。
- en: Various clustering methods
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 各种聚类方法
- en: In this section, we will cover the different clustering methods. First, let's
    look at what clustering is. Then we'll explain some of the mathematical tricks
    that we can use in clustering. And finally, we're going to introduce our newest
    non-parametric algorithm KNN.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将介绍不同的聚类方法。首先，我们来看看什么是聚类。然后，我们将解释一些在聚类中可以使用的数学技巧。最后，我们将介绍我们最新的非参数算法
    KNN。
- en: What is clustering?
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是聚类？
- en: Clustering is about as intuitive as it gets in terms of machine learning models.
    The idea is we can segment groups of samples based on their nearness to one another.
    The hypothesis is the samples that are closer are more similar in some respects.
    So, there are two reasons we might want to cluster. The first is for discovery
    purposes, and we usually do this when we make no assumptions about the underlying
    structure of the data, or don't have labels. And so, this typically is done in
    a purely unsupervised sense. But as this is obviously a supervised learning book,
    we're going to focus on the second use case, which uses clustering as a classification
    technique.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是机器学习模型中最直观的一个。其思想是，我们可以根据样本之间的接近程度将其分组。假设前提是，距离较近的样本在某些方面更为相似。因此，我们可能想进行聚类的原因有两个。第一个是为了发现目的，通常在我们对数据的基础结构没有假设，或没有标签时，我们会这么做。因此，这通常是完全无监督的做法。但由于这是一本显然是监督学习的书籍，我们将关注第二个用途，即将聚类作为分类技术。
- en: Distance metrics
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 距离度量
- en: So, before we get into the algorithms, I want to address some mathematical esotericism.
    When you have two points, or any number of points, in a 2D space, it's fairly
    easy to conceptualize. It's basically calculating the hypotenuse along some right
    triangle, in terms of measuring the distance. However, what happens when you have
    a really high dimensional space? That's what we're going to get into, and we have
    a lot of clustering problems.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在进入算法之前，我想谈一谈一些数学上的深奥概念。当你有两个点，或者任意数量的点，在二维空间中时，它相当容易理解。基本上，这是计算直角三角形中的斜边长度，用来测量距离。然而，当你有一个非常高维的空间时，会发生什么呢？这就是我们将要讨论的内容，我们有很多聚类问题。
- en: So, the most common distance metric is the Euclidean distance. This is essentially
    a generalization of the 2D approach. It's the square root of the sum of squared
    differences between two vectors, and it can be used in any dimensional space.
    There's a lot of others that we're not going to get into. Two of them are **Manhattan**
    and **Mahalanobis**. Manhattan is a lot like the Euclidean distance. However,
    rather than having the squared difference, it's going to have the absolute value
    of the difference.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，最常见的距离度量是欧几里得距离。它本质上是二维方法的推广。它是两个向量之间平方差和的平方根，可以用于任何维度的空间。还有很多其他方法我们暂时不涉及，其中有两种是**曼哈顿距离**和**马氏距离**。曼哈顿距离与欧几里得距离相似，但它不是平方差，而是取差值的绝对值。
- en: KNN – introduction
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KNN – 介绍
- en: 'KNN is a really simple intuitive approach to building a clustering classifier.
    The idea is, given a set of labeled samples, when a new sample is introduced,
    we look at the k nearest points, and we make an estimate for its class membership
    based on the majority of points around it. So, in the following diagram we would
    classify this new question mark as a positive sample since the majority of its
    neighbors are positive:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 是一种非常简单直观的聚类分类方法。其思想是，给定一组标记样本，当一个新样本被引入时，我们会查看其 k 个最近的点，并根据周围点的多数决定其类别。所以，在下图中，我们会将这个新的问号标记为正样本，因为它的大多数邻居是正样本：
- en: '![](img/a81eab9b-1197-4342-acde-ce1ae5e74b3e.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a81eab9b-1197-4342-acde-ce1ae5e74b3e.png)'
- en: KNN – considerations
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KNN – 考虑因素
- en: There are a few considerations you should take into account here. KNN is a bit
    interesting and can fluctuate between wildly high bias or high variance depending
    on its hyperparameter *K*. If *K* is too large and you're comparing a new sample
    to the entire training set, it favors the majority class. Essentially, whichever
    is more, we vote that way. This would be a highly underfitted model. If *K* is
    too small, it gives higher priority to the immediately adjacent samples, which
    means that the model is extremely overfitted. In addition to considerations around
    *K*, you may also want to consider centering and scaling your data. Otherwise,
    your distance metric will not be very sensitive to small-scale features. For instance,
    if one of your features is thousands of dollars for a house and the other feature
    is the number of bathrooms, *1.5* to *3.5* or so, you're going to implicitly be
    favoring the dollars versus the number of bathrooms. So, you might want to center
    and scale.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你应该考虑一些因素。KNN 算法比较有趣，可能会因为超参数*K*的不同而在高偏差和高方差之间波动。如果*K*太大，而你将新样本与整个训练集进行比较，那么它就会偏向多数类。实际上，不管哪个类别多，我们就投票支持那个类别。这会导致模型严重欠拟合。如果*K*太小，它会更倾向于给紧邻的样本更高的优先级，这样模型就会极度过拟合。除了*K*的考虑外，你还可能需要对数据进行中心化和标准化。否则，你的距离度量对小规模特征的敏感性会较低。例如，如果一个特征是房屋价格（几千美元），另一个特征是浴室数量（比如*1.5*到*3.5*），你就会在隐式上更关注价格而忽略浴室数量。因此，你可能需要对数据进行中心化和标准化。
- en: A classic KNN algorithm
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经典的 KNN 算法
- en: A classic KNN algorithm will compute the distances between the training samples
    and store them in a distance-partitioned heap structure, such as **KDTree** or
    a ball tree, which are essentially sorted-heap binary trees. We then query the
    tree for test samples. Our approach in this class is going to be a little bit
    different in order to be a bit more intuitive and readable.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的 KNN 算法会计算训练样本之间的距离，并将其存储在一个距离分区堆结构中，如**KDTree**或球树，它们本质上是排序堆的二叉树。然后我们查询树来获取测试样本。在本课程中，我们将采用稍微不同的方法，以便更直观、更易读。
- en: In the next section, we'll cover how we can implement it from scratch.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论如何从零实现 KNN。
- en: Implementing KNNs from scratch
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零实现 KNN
- en: In this section, we will jump into the `packtml` code base, and see how we can
    implement it from scratch. We'll start by revisiting the classic algorithm we
    covered in the last section, and then we'll look at the actual Python code, which
    has some implementation changes.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨`packtml`代码库，并了解如何从零开始实现它。我们将从回顾上一节中讨论的经典算法开始，然后查看实际的 Python 代码，看看它有哪些实现上的变化。
- en: Recall the archetypal KNN algorithm. The efficient implementation is going to
    be to pre-compute the distances and store them in a special heap. Of course, with
    most things in computer science, there's the clever way and then there's the easy-to-read
    way. We're going to do things a bit differently in an effort to maximize the readability,
    but it's the same fundamental algorithm.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下典型的 KNN 算法。高效的实现方法是预先计算距离并将其存储在一个特殊的堆中。当然，计算机科学中总有两种方式，一种是巧妙的方式，另一种是容易阅读的方式。我们会稍微不同一点，努力提高代码的可读性，但它本质上是相同的基本算法。
- en: KNN clustering
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KNN 聚类
- en: We've got two files we want to look at. The first is the source code in the
    `packtml` Python package. Second, we're going to look an example of the KNN applied
    to the `iris` dataset. Let's go ahead and jump over to PyCharm, where there are
    two files open. Inside of the `clustering` submodule, we have the `knn.py` file
    open. This is where we're going to find the KNN class and all the implementation
    details. Then in the examples directory, in the `clustering` subdirectory, we
    have the `example_knn_classifier.py` file open as well. We'll walk through that
    after we've gone through the implementation details.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个文件需要查看。第一个是`packtml` Python 包中的源代码。第二个是一个将 KNN 应用于`iris`数据集的示例。现在，让我们跳转到
    PyCharm，在那里打开了两个文件。在`clustering`子模块中，我们打开了`knn.py`文件。在这个文件中，我们可以找到 KNN 类和所有的实现细节。然后，在`examples`目录下的`clustering`子目录中，我们也打开了`example_knn_classifier.py`文件。在我们完成实现细节的讨论后，再一起走一遍这个文件。
- en: 'Now, regarding other libraries, we''re going to use scikit-learn''s utils to
    validate the `X`, `y`, and classification targets. However, we''re also going
    to use the `metrics.pairwise` submodule to use `euclidean_distances`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关于其他库，我们将使用scikit-learn的utils来验证`X`、`y`和分类目标。然而，我们还将使用`metrics.pairwise`子模块来使用`euclidean_distances`：
- en: '[PRE38]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'If we want to use a different distance metric, we could also import Manhattan,
    as mentioned in the earlier section. But for this, we''re just going to use Euclidean.
    So, if you want to adjust that later, feel free. Our KNN class here is going to
    take three parameters. As usual for `BaseSimpleEstimator`, we''re going to take
    our `X` and `y`, which are our training vectors and our training label, and then
    `k`, which is our tuning parameter for the number of neighbors that we want to
    compute around each sample:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想使用不同的距离度量，我们还可以导入曼哈顿距离，正如之前部分所提到的。但是对于这个例子，我们将使用欧几里得距离。所以，如果以后你想调整这个，可以随时更改。我们的KNN类将接受三个参数。和`BaseSimpleEstimator`一样，我们将获取`X`和`y`，它们分别是我们的训练向量和训练标签，然后是`k`，它是我们希望计算的邻居数的调优参数：
- en: '[PRE39]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'So, our constructor is pretty simple. We''re going to check our `X` and `y`
    and basically store them. Then we assign `k` to a `self` attribute. Now, in other
    implementations we might go ahead and compute our KDTree or our ball tree:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们的构造函数非常简单。我们将检查我们的`X`和`y`并基本存储它们。然后我们将`k`分配给`self`属性。在其他实现中，我们可能会继续计算我们的KDTree或我们的球树：
- en: '[PRE40]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'So, we''re going to do a brute force method, where we don''t compute the distances
    until we predict. This is a lazy evaluation of distances. In our predict function,
    we are going to take our `X`, which is our test array. `X`, which we assigned
    in the constructor. We will compute `euclidean_distances` between our training
    array and our test array. Here, we get an `M` by `M` matrix, where `M` is the
    number of samples in our test array:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们将使用一种暴力法，在预测之前不计算距离。这是对距离的懒惰计算。在我们的预测函数中，我们将获取我们的`X`，它是我们的测试数组。`X`是在构造函数中分配的。我们将计算训练数组和测试数组之间的`euclidean_distances`。在这里，我们得到一个`M`乘以`M`的矩阵，其中`M`是测试数组中样本的数量：
- en: '[PRE41]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'In order to find the nearest distance, we `argsort` distances by the column
    to show which samples are closest. Following, is the array of distances, and we
    are going to `argsort` it along the `axis` column, such that we get the samples
    that are closest, based on the distance:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到最近的距离，我们将通过列对距离进行`argsort`，以显示哪些样本最接近。接下来是距离数组，我们将沿着`axis`列对它进行`argsort`，这样我们就能基于距离找到最接近的样本：
- en: '[PRE42]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We will slice the labels based on `top_k` along `y`. These are basically the
    class labels:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将根据`top_k`在`y`上切片标签。这些基本上是类标签：
- en: '[PRE43]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Since it''s a classification, we''re interested in `mode`. Take the mode using
    the `mode` function along that `axis` and `ravel` it into a NumPy array:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个分类问题，我们关注的是`mode`。使用`mode`函数沿着该`axis`获取众数，并将其转换为NumPy数组：
- en: '[PRE44]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Hence, we''re just computing the distances for the predict function, argsorting
    the closest distances, then finding the corresponding labels, and taking the mode.
    Now, over in the `examples/clustering` directory, go to `example_knn_classifier.py`.
    We''re going to use the `load_iris` function from scikit-learn:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们只是为预测函数计算距离，进行`argsort`以找出最接近的距离，然后找到相应的标签，并获取众数。现在，在`examples/clustering`目录下，进入`example_knn_classifier.py`。我们将使用scikit-learn中的`load_iris`函数：
- en: '[PRE45]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We will only use the first two dimensions so that we can visualize it in a
    relatively intuitive fashion. Perform the training split, and then center and
    scale using `StandardScaler`:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只使用前两个维度，这样我们就可以以相对直观的方式进行可视化。执行训练集划分，然后使用`StandardScaler`进行标准化和缩放：
- en: '[PRE46]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Fit the `KNNClassifier` with `k=10`:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`k=10`来拟合`KNNClassifier`：
- en: '[PRE47]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Finally, we will plot it by typing the following command. Make sure you''ve
    got your environment activated, as usual:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将通过输入以下命令来绘制图形。确保你的环境已经激活，和往常一样：
- en: '![](img/eed9a1bd-cf77-4838-801e-e3921dbe7935.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eed9a1bd-cf77-4838-801e-e3921dbe7935.png)'
- en: 'The output for `k = 10`, and we get about 73-74% test accuracy. Note that we''re
    only using two dimensions:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`k = 10`，输出结果为大约73-74%的测试准确度。请注意，我们只使用了两个维度：
- en: '![](img/d072a43a-558f-42b2-983c-feb58cb1ba79.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d072a43a-558f-42b2-983c-feb58cb1ba79.png)'
- en: So, now that you're a KNN expert, you can build one from scratch. In the next
    section, we will compare non-parametric models to parametric models.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在你已经是KNN专家了，你可以从零开始构建一个。接下来的部分，我们将比较非参数模型与参数模型。
- en: Non-parametric models – pros/cons
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非参数模型 – 优缺点
- en: In this section, we will discuss every statistician's favorite philosophical
    debate, which is the pros and cons of non-parametric models versus parametric
    models.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论每个统计学家最喜欢的哲学辩论，即非参数模型与参数模型的优缺点。
- en: Pros of non-parametric models
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非参数模型的优点
- en: Non-parametric models are able to learn some really complex relationships between
    your predictors and the output variable, which can make them really powerful for
    non-trivial modeling problems. Just like the regression sinusoidal wave we modeled
    in the decision trees, a lot of non-parametric models are fairly tolerant to data
    scale as well. The major exception here is the clustering techniques, but these
    techniques can pose a major advantage for models such as decision trees, which
    don't require the same level of pre-processing that parametric models might. Finally,
    if you find yourself suffering from high variance, you can always add more training
    data, with which your model is likely to get better.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 非参数模型能够学习预测变量与输出变量之间一些非常复杂的关系，这使得它们在处理复杂建模问题时非常强大。就像我们在决策树中建模的回归正弦波一样，许多非参数模型对数据规模也相当宽容。唯一的主要例外是聚类技术，但这些技术对诸如决策树之类的模型可能具有显著优势，因为这些模型不需要像参数模型那样进行大量的预处理。最后，如果你发现自己面临高方差问题，你总是可以通过增加更多的训练数据来改善模型性能。
- en: Cons of non-parametric models
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非参数模型的缺点
- en: There are the not-so-good parts of non-parametric models as well. Several of
    these we have already covered. So, as you may know, they can be slower to fit
    or predict, and less intuitive in many cases than a lot of parametric models.
    If speed is less critical than accuracy, non-parametric models may be a great
    candidate for your model. Likewise, with explainability, these models can be over-complicated
    and tough to understand. Finally, one of the advantages of non-parametric models
    is the ability to get better with more data, which can be a weakness if data is
    hard to get. They generally do require a bit more data to train effectively than
    their parametric brethren.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 非参数模型也有一些不太好的地方。我们已经讨论过其中的一些。正如你所知道的，它们可能在拟合或预测时更慢，并且在许多情况下比许多参数模型更不直观。如果速度比准确性更不重要，非参数模型可能是你模型的一个好选择。同样，在可解释性方面，这些模型可能过于复杂，难以理解。最后，非参数模型的一个优势是随着数据增多会变得更好，这在数据难以获取时可能成为弱点。它们通常需要比参数模型更多的数据才能有效训练。
- en: Which model to use?
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用哪个模型？
- en: Parametric models that we've already covered have some really great and convenient
    attributes. There are several reasons you may opt for a parametric model over
    a non-parametric model. Particularly if you're in a regulated industry, we need
    to explain the models more easily. Non-parametric models, on the other hand, may
    create a better, more complex model. But if you don't have a good chunk of data,
    it may not perform very well. It is best not to get overly philosophical about
    which one you should or should not use. Just use whichever best fits your data
    and meets your business requirements.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论过的参数模型具有一些非常出色且便捷的特点。你可能会选择参数模型而非非参数模型的原因有很多。尤其是在受监管的行业中，我们需要更容易地解释模型。另一方面，非参数模型可能会创建更好、更复杂的模型。但如果你没有足够的数据，它的表现可能不太理想。最好不要过于哲学化地考虑该使用哪个模型。只需使用最适合你数据并满足业务需求的模型即可。
- en: Summary
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we initially got introduced to non-parametric models and then
    we walked through the decision trees. In the next sections, we learned the splitting
    criteria and how they produce splits. We also learned about the bias-variance
    trade-off, and how non-parametric models tend to favor a higher variance set of
    error, while parametric models favor high bias. Next, we looked into clustering
    methods and even coded a KNN class from scratch. Finally, we wrapped up with the
    pros and cons of non-parametric methods.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们首先介绍了非参数模型，然后我们走过了决策树。在接下来的部分中，我们学习了分裂标准及其如何生成分裂。我们还了解了偏差-方差权衡，以及非参数模型通常倾向于偏好更高方差的误差集，而参数模型则偏好高偏差。接着，我们研究了聚类方法，甚至从零开始编写了一个KNN类。最后，我们总结了非参数方法的优缺点。
- en: In the next chapter, we will get into some more of the advanced topics in supervised
    machine learning, including recommender systems and neural networks.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨监督学习中的一些更高级话题，包括推荐系统和神经网络。
