- en: '*Chapter 7*: AIOps and Root Cause Analysis'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第7章*：AIOps和根本原因分析'
- en: Up until this point, we have extensively explained the value of detecting anomalies
    across metrics and logs separately. This is extremely valuable, of course. In
    some cases, however, the knowledge that a particular metric or log file has gone
    awry may not tell the whole story of what is going on. It may, for example, be
    pointing to a symptom and not the cause of the problem. To have a better understanding
    of the full scope of an emerging problem, it is often helpful to look holistically
    at many aspects of a system or situation. This involves smartly analyzing multiple
    kinds of related datasets together.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经广泛解释了分别检测指标和日志中异常的价值。这当然非常有价值。然而，在某些情况下，关于特定指标或日志文件出现问题的知识可能并不能完全说明正在发生的事情。例如，它可能只是指向问题的症状而不是原因。为了更好地理解一个新兴问题的全貌，通常需要从系统或情况的多个方面进行整体分析。这涉及到智能地分析多种相关数据集。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Demystifying the term ''AIOps''
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 揭秘“AIOps”术语
- en: Understanding the importance and limitations of KPIs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解KPI的重要性及其局限性
- en: Moving beyond KPIs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超越KPI
- en: Organizing data for better analysis
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为更好的分析组织数据
- en: Leveraging the contextual information
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用上下文信息
- en: Bringing it all together for RCA
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有内容整合进行根本原因分析（RCA）
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The information and examples demonstrated in this chapter are relevant as of
    v7.11 of the Elastic Stack and utilize sample datasets from the GitHub repo found
    at [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中展示的信息和示例适用于Elastic Stack的v7.11版本，并使用GitHub仓库中的示例数据集，该仓库位于[https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition)。
- en: Demystifying the term ''AIOps''
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭秘“AIOps”术语
- en: 'We learned in [*Chapter 1*](B17040_01_Epub_AM.xhtml#_idTextAnchor016), *Machine
    Learning for IT*, that many companies are drowning in an ever-increasing cascade
    of IT data while simultaneously being asked to ''''do more with less'''' (fewer
    people, fewer costs, and so on). Some of that data is collected and/or stored
    in specialized tools, but some may be collected in general-purpose data platforms
    such as the Elastic Stack. But the question still remains: what percentage of
    that data is being paid attention to? By this, we mean the percentage of collected
    data that is actively inspected by humans or being watched by some type of automated
    means (defined alarms based on rules, thresholds, and so on). Even generous estimates
    might put the percentage in the range of single digits. So, with 90% or more data
    being collected going unwatched, what''s being missed? The proper answer might
    be that we don''t actually know.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第1章*](B17040_01_Epub_AM.xhtml#_idTextAnchor016)，*IT机器学习*中了解到，许多公司在被要求“用更少的资源做更多的事情”（更少的人，更少的成本等）的同时，正陷入不断增长的IT数据洪流中。其中一些数据被收集和/或存储在专用工具中，但一些可能被收集在通用数据平台中，如Elastic
    Stack。但问题仍然存在：有多少比例的数据被关注？通过这种方式，我们指的是被人类积极检查或通过某种类型的自动化手段（基于规则、阈值等）监视的数据收集比例。即使是非常宽泛的估计，这个比例也可能只有个位数。因此，90%或更多的收集数据未被关注，我们错过了什么？正确的答案可能就是我们实际上并不了解。
- en: 'Before we admonish IT organizations for the sin of collecting piles of data
    but not watching it, we need to understand the magnitude of the challenge associated
    with such an operation. A typical user-facing application may do the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们谴责IT组织收集大量数据却不去关注它们的罪过之前，我们需要了解与此类操作相关的挑战的规模。一个典型的面向用户的应用程序可能执行以下操作：
- en: Span hundreds of physical servers
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 涵盖数百个物理服务器
- en: Have dozens (if not hundreds) of microservices, each of which may have dozens
    or hundreds of operational metrics or log entries that describe its operation
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有数十（如果不是数百）个微服务，每个微服务可能有数十或数百个操作指标或日志条目来描述其操作
- en: The combinatorics of this can easily rise to a six- or seven-figure range of
    unique measurement points. Additionally, there may be dozens or even hundreds
    of such applications under the umbrella of management by the IT organization.
    It's no wonder that the amount of data being collected by these systems per day
    can easily be measured in terabytes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这种组合的数学组合可以轻松达到六位数或七位数的独特测量点。此外，在IT组织的管理之下，可能有数十个甚至数百个这样的应用。因此，这些系统每天收集的数据量很容易以太字节来衡量。
- en: So, it is quite natural that the desired solution could involve a combination
    of automation and artificial intelligence to lessen the burden on human analysts.
    Some clever marketing person somewhere figured out that coining the term ''AIOps''
    encapsulated a projected solution to the problem – augment what humans can't (or
    don't have the time or capacity to do manually) with some amount of intelligent
    automation. Now, what an AIOps solution actually does to accomplish that goal
    is often left to a discerning user to interpret.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，所期望的解决方案很自然地可能涉及自动化和人工智能的结合，以减轻对人类分析师的负担。某个聪明的市场营销人员想到了创造“AIops”这个术语，它封装了对问题的预期解决方案——通过一定程度的智能自动化来增强人类无法（或没有时间或能力手动完成）的事情。现在，AIops解决方案实际上如何实现这一目标，通常留给有洞察力的用户去解释。
- en: 'So, let''s demystify the term by not focusing on the term itself (let''s leave
    that to the marketing folks), but rather articulating the kinds of things we would
    want to have this intelligent technology do to help us in our situation:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们通过不关注术语本身（让我们把那留给市场营销人员）来消除这个术语的神秘感，而是明确我们希望这种智能技术为我们的情况做些什么：
- en: Autonomously inspect data and assess its relevance, importance, and notability
    based upon an automatically learned set of constraints, rules, and behavior.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动检查数据并基于自动学习的一组约束、规则和行为，评估其相关性、重要性和显著程度。
- en: Filter out the noise of irrelevant behaviors so as to not distract human analysts
    from the things that actually matter.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤掉无关行为的噪音，以免分散人类分析师对真正重要的事情的注意力。
- en: Obtain a certain amount of proactive early warnings regarding problems that
    may be brewing but have not necessarily caused an outage yet.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得有关可能正在酝酿但尚未导致停机的问题的某些主动早期预警。
- en: Automatically gather related/correlated evidence around a problem to assist
    with **Root** **Cause** **Analysis** (**RCA**).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动收集围绕问题的相关/相关证据，以协助进行**根本原因分析**（**RCA**）。
- en: Uncover operational inefficiencies in order to maximize infrastructure performance.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 揭示运营中的低效率，以最大化基础设施性能。
- en: Suggest an action or next step for remediation, based upon past remediations
    and their effectiveness.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于以往修复措施及其有效性，建议一个行动或下一步的修复步骤。
- en: While this list is in no way comprehensive, we can see the gist of what we're
    getting at here – which is that intelligent automation and analysis can pay big
    dividends and allow IT departments to drive efficiencies and thus maximize business
    outcomes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个列表远非详尽无遗，但我们已经看到了我们在这里想要表达的核心——那就是智能自动化和分析可以带来巨大的回报，并允许IT部门推动效率，从而最大化业务成果。
- en: Except for the suggested remediations mentioned in number six in the preceding
    list (at least at this moment), Elastic **Machine Learning** (**ML**) can very
    much be an important part of all the other goals on this list. We've seen already
    how Elastic ML can automatically find anomalous behavior, forecast trends, proactively
    alert, and so on. But we must also recognize that Elastic ML is a generic ML platform
    – it is not purpose-built for IT operations/observability or security analytics.
    As such, there still needs to be an orientation of how Elastic ML is used in the
    context of operations, and that will be discussed throughout this chapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面列表中第六点提到的建议修复措施（至少在目前这个阶段），Elastic **机器学习**（**ML**）可以非常重要的是实现本列表上所有其他目标的一部分。我们已经看到Elastic
    ML如何自动发现异常行为、预测趋势、主动警报等等。但我们也必须认识到Elastic ML是一个通用的ML平台——它并非专门为IT运营/可观察性或安全分析而构建。因此，仍然需要确定Elastic
    ML在运营环境中的使用方向，这一点将在本章中讨论。
- en: It is also important to note that there are still a large number of IT operation
    groups that currently use no intelligent automation and analysis. They often claim
    that they would like to employ an AI-based approach to improve their current situation,
    but that they are not quite ready to take the plunge. So, let's challenge the
    notion that the only way to benefit from AI is to do every single thing that is
    possible on day 1\. Let's instead build up some practical applications of Elastic
    ML in the context of IT operations and how it can be used to satisfy most of the
    goals articulated in the preceding list.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，仍有大量IT运维团队目前没有使用智能自动化和分析。他们经常声称他们希望采用基于AI的方法来改善当前状况，但他们还没有准备好采取行动。因此，让我们挑战这种观念，即从AI中获益的唯一方式是在第一天就做所有可能的事情。相反，让我们在IT运维的背景下构建一些Elastic
    ML的实际应用，以及它如何能够满足前面列出的大多数目标。
- en: We will first start with the notion of the **Key** **Performance** **Indicator**
    (**KPI**) and why it is the logical choice for the best place to get started with
    Elastic ML.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从**关键** **绩效** **指标**（**KPI**）的概念开始，并解释为什么它是开始使用Elastic ML的合理选择。
- en: Understanding the importance and limitations of KPIs
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解KPI的重要性和局限性
- en: 'Because of the problem of scale and the desire to make some amount of progress
    in making the collected data actionable, it is natural that some of the first
    metrics to be tackled for active inspection are those that are the best indicators
    of performance or operation. The KPIs that an IT organization chooses for measurement,
    tracking, and flagging can span diverse indicators, including the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于规模问题和使收集的数据具有行动力的愿望，自然地，在主动检查中首先解决的指标是那些最能指示性能或操作的指标。IT组织选择的用于测量、跟踪和标记的KPI可以跨越各种指标，包括以下内容：
- en: '**Customer experience**: These metrics measure customer experience, such as
    application response times or error rates.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户体验**：这些指标衡量客户体验，例如应用程序响应时间或错误率。'
- en: '**Availability**: Metrics such as uptime or **Mean** **Time** **to** **Repair**
    (**MTTR**) are often important to track.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性**：如正常运行时间或**平均** **修复** **时间**（**MTTR**）等指标通常需要跟踪。'
- en: '**Business**: Here we may have metrics that directly measure business performance,
    such as orders per minute or number of active users.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**业务**：在这里，我们可能有直接衡量业务绩效的指标，例如每分钟的订单数或活跃用户数。'
- en: As such, these types of metrics are usually displayed, front and center, on
    most high-level operational dashboards or on staff reports for employees ranging
    from technicians to executives. A quick Google image search for a KPI dashboard
    will return countless examples of charts, gauges, dials, maps, and other eye candy.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这类指标通常在大多数高级操作仪表板上或从技术人员到高管的各种员工的工作报告中，以最显眼的位置展示。在谷歌图片搜索中输入“KPI仪表板”将返回无数图表、仪表、旋钮、地图和其他视觉吸引物的示例。
- en: 'While there is great value in such displays of information that can be consumed
    with a mere glance, there are still fundamental challenges with manual inspection:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种信息展示有很大的价值，只需一眼就能消费，但手动检查仍然存在基本挑战：
- en: '**Interpretation**: There may be difficulty in understanding the difference
    between normal operation and abnormal, unless that difference is already intrinsically
    understood by the human.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释**：除非人类已经本能地理解了这种差异，否则在理解正常操作和异常操作之间的差异可能会有困难。'
- en: '**Challenges of scale**: Despite the fact that KPIs are already a distillation
    of all metrics down to a set of important ones, there still may be more KPIs to
    display than is feasible given the real estate of the screen that the dashboard
    is displayed upon. The end result may be crowded visualizations or lengthy dashboards
    that require scrolling/paging.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规模挑战**：尽管KPI已经是将所有指标浓缩成一组重要指标的过程，但可能仍有比在仪表板显示的屏幕空间内可行更多的KPI需要展示。最终结果可能是拥挤的视觉展示或需要滚动/分页的冗长仪表板。'
- en: '**Lack of proactivity**: Many dashboards such as this do not have their metrics
    also tied to alerts, thus requiring constant supervision if it''s proactively
    known that a KPI that is faltering is important.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏主动性**：许多像这样的仪表板没有将它们的指标与警报联系起来，因此如果主动知道一个表现不佳的KPI很重要，就需要持续的监督。'
- en: The bottom line is that KPIs are an extremely important step in the process
    of identifying and tracking meaningful indicators of the health and behavior of
    an IT system. However, it should be obvious that the mere act of identifying and
    tracking a set of KPIs with a visual-only paradigm is going to leave some significant
    deficiencies in the strategy of a successful IT operations plan.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重要的是，KPIs是识别和跟踪IT系统健康和行为有意义指标过程中的一个极其重要的步骤。然而，很明显，仅仅通过视觉范式识别和跟踪一组KPIs，将会在成功的IT运维计划策略中留下一些重大的缺陷。
- en: 'It should be obvious that KPIs are a great candidate for metrics that can be
    tracked with Elastic ML''s anomaly detection. For example, say we have some data
    that looks like the following (from the `it_ops_kpi` sample dataset in the GitHub
    repo):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，KPIs是Elastic ML的异常检测可以跟踪的度量标准的好候选者。例如，假设我们有一些看起来如下所示的数据（来自GitHub仓库中的`it_ops_kpi`样本数据集）：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this case, the KPI (the field called `events_per_min`) represents the summarized
    total number of purchases per minute for some online transaction processing system.
    We could easily track this KPI over time with an anomaly detection job with a
    `sum` function on the `events_per_min` field and a bucket span of 15 minutes.
    An unexpected dip in online sales (to a value of `921`) is detected and flagged
    as anomalous:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，关键绩效指标（称为`events_per_min`的字段）代表某些在线交易处理系统每分钟购买的汇总总数。我们可以通过在`events_per_min`字段上使用`sum`函数以及15分钟的桶跨度，轻松地随时间跟踪这个KPI。检测到在线销售额（降至`921`）的意外下降，并将其标记为异常：
- en: '![Figure 7.1 – A KPI being analyzed with a typical anomaly detection job'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 7.1 – 使用典型异常检测作业分析的关键绩效指标'
- en: '](img/B17040_07_1.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_07_1.jpg]'
- en: Figure 7.1 – A KPI being analyzed with a typical anomaly detection job
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 使用典型异常检测作业分析的关键绩效指标
- en: 'In this case, the KPI is just a single, overall metric. If there was another
    categorical field in the data that allowed it to be segmented (for example, sales
    by product ID, product category, geographical region, and so on), then ML could
    easily split the analysis along that field to expand the analysis in a parallel
    fashion (as we saw in [*Chapter 3*](B17040_03_Epub_AM.xhtml#_idTextAnchor049),
    *Anomaly Detection*). But let''s not lose sight of what we''re accomplishing here:
    a proactive analysis of a key metric that someone likely cares about. The number
    of online sales per unit of time is directly tied to incoming revenue and thus
    is an obvious KPI.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，KPI只是一个单一的、总体的指标。如果数据中存在另一个可以对其进行分割的类别字段（例如，按产品ID、产品类别、地理区域等销售的销售额），那么机器学习可以轻松地沿着该字段分割分析，以并行方式扩展分析（如我们在[*第3章*](B17040_03_Epub_AM.xhtml#_idTextAnchor049)，*异常检测*）中看到的那样）。但让我们不要忽视我们在这里所取得的成果：对某人可能关心的关键指标的主动分析。单位时间内的在线销售额直接与收入相关，因此是一个明显的KPI。
- en: However, despite the importance of knowing that something unusual is happening
    with our KPI, there is still no insight as to *why* it is happening. Is there
    an operational problem with one of the backend systems that supports this customer-facing
    application? Was there a user interface coding error in the latest release that
    makes it harder for users to complete the transaction? Is there a problem with
    the third-party payment processing provider that is relied upon? None of these
    questions can be answered by merely scrutinizing the KPI.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管了解我们的KPI出现异常情况的重要性，但我们仍然没有关于*为什么*它发生的原因。是否支持该面向客户的应用的某个后端系统存在操作问题？是否在最新版本中存在用户界面编码错误，使得用户完成交易更困难？是否依赖于第三方支付处理提供商存在问题？仅通过审查KPI无法回答这些问题。
- en: To get that kind of insight, we will need to broaden our analysis to include
    other sets of relevant and related information.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得这种洞察力，我们需要扩大我们的分析范围，包括其他相关和相关的信息集。
- en: Moving beyond KPIs
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越关键绩效指标（KPIs）
- en: The process of selecting KPIs, in general, should be relatively easy, as it
    is likely obvious what metrics are the best indicators (if online sales are down,
    then the application is likely not working). But if we want to get a more holistic
    view of what may be contributing to an operational problem, we must expand our
    analysis beyond the KPIs to indicators that emanate from the underlying systems
    and technology that support the application.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，选择KPI的过程应该是相对简单的，因为很可能很明显哪些指标是最好的指标（如果在线销售额下降，那么应用程序可能没有正常工作）。但如果我们想更全面地了解可能对运营问题有贡献的因素，我们必须将我们的分析扩展到支持应用程序的底层系统和技术的指标之外。
- en: 'Fortunately, there are a plethora of ways to collect all kinds of data for
    centralization in the Elastic Stack. The **Elastic Agent**, for example, is a
    single, unified agent that you can deploy to hosts or containers to collect data
    and send it to the Elastic Stack. Behind the scenes, the Elastic Agent runs the
    Beats shippers or Elastic Endpoint required for your configuration. Starting from
    version 7.11, the Elastic Agent is managed in Kibana in the **Fleet** user interface
    and can be used to add and manage integrations for popular services and platforms:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '幸运的是，有大量的方法可以收集各种数据并将其集中到Elastic Stack中。例如，**Elastic Agent**是一个单一的、统一的代理，您可以将其部署到主机或容器中，以收集数据并将其发送到Elastic
    Stack。在幕后，Elastic Agent运行所需的Beats shippers或Elastic Endpoint。从版本7.11开始，Elastic
    Agent在Kibana的**Fleet**用户界面中进行管理，并可用于添加和管理流行服务和平台的集成：  '
- en: '![Figure 7.2 – The Integrations section of the Fleet user interface in Kibana'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.2 – Kibana中Fleet用户界面的集成部分'
- en: '](img/B17040_07_2.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_07_2.jpg)'
- en: Figure 7.2 – The Integrations section of the Fleet user interface in Kibana
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – Kibana中Fleet用户界面的集成部分
- en: 'Using these different integrations, the user can easily collect data and centralize
    it in the Elastic Stack. While this chapter is not meant to be a tutorial on Fleet
    and the Elastic Agent, the important point is that regardless of what tools you
    use to gather the underlying application and system data, one thing is likely
    true: there will be a lot of data when all is said and done. Remember that our
    ultimate goal is to proactively and holistically pay attention to a larger percentage
    of the overall dataset. To do that, we must first organize this data so that we
    can effectively analyze it with Elastic ML.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些不同的集成，用户可以轻松收集数据并将其集中到Elastic Stack中。虽然本章的目的不是介绍Fleet和Elastic Agent，但重要的是，无论您使用什么工具来收集底层应用程序和系统数据，有一点很可能是真的：最终会有大量的数据。记住，我们的最终目标是主动和全面地关注更大比例的整体数据集。为了做到这一点，我们必须首先组织这些数据，以便我们可以使用Elastic
    ML有效地分析它。
- en: Organizing data for better analysis
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组织数据以进行更好的分析
- en: One of the nicest things about ingesting data via the Elastic Agent is that
    by default, the data collected is normalized using the **Elastic Common Schema**
    (**ECS**). ECS is an open source specification that defines a common taxonomy
    and naming conventions across data that is stored in the Elastic Stack. As such,
    the data becomes easier to manage, analyze, visualize, and correlate across disparate
    data types – including across both performance metrics and log files.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Elastic Agent摄取数据的一个非常好的特点是默认情况下，收集的数据使用**Elastic Common Schema**（**ECS**）进行标准化。ECS是一个开源规范，它定义了存储在Elastic
    Stack中的数据的一个通用分类法和命名约定。因此，数据变得更容易管理、分析、可视化和跨不同数据类型（包括性能指标和日志文件）进行关联。
- en: Even if you are not using the Elastic Agent or other legacy Elastic ingest tools
    (such as Beats and Logstash) and are instead relying on other, third-party data
    collection or ingest pipelines, it is still recommended that you conform your
    data to ECS because it will pay big dividends when users expect to use this data
    for queries, dashboards, and, of course, ML jobs.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您没有使用Elastic Agent或其他遗留的Elastic摄取工具（如Beats和Logstash），而是依赖其他第三方数据收集或摄取管道，仍然建议您将数据符合ECS，因为当用户期望使用这些数据进行查询、仪表板和当然，机器学习作业时，这将带来巨大的回报。
- en: Note
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: More information on ECS can be found in the reference section of the website
    at [https://www.elastic.co/guide/en/ecs/current/ecs-reference.html](https://www.elastic.co/guide/en/ecs/current/ecs-reference.html).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在网站的参考部分可以找到有关ECS的更多信息，请参阅[https://www.elastic.co/guide/en/ecs/current/ecs-reference.html](https://www.elastic.co/guide/en/ecs/current/ecs-reference.html)。
- en: Among many of the important fields within ECS is the `host.name` field, which
    defines which host the data was collected from. By default, most data collection
    strategies in the Elastic Stack involve putting data in indices that are oriented
    around the data type, and thus potentially contain interleaved documents from
    many different hosts. Perhaps some of our hosts in our environment support one
    application (that is, online purchases), but other hosts support a different application
    (such as invoice processing). With all hosts reporting their data into a single
    index, if we are interested in orienting our reporting and analysis of the data
    for one or both applications, it is obviously inappropriate to orient the analysis
    based solely on the index – we will need our analysis to be application-centric.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在ECS中的许多重要字段中，`host.name`字段定义了数据是从哪个宿主机收集的。默认情况下，Elastic Stack中的大多数数据收集策略都涉及将数据放入以数据类型为中心的索引中，因此可能包含来自许多不同宿主机的交错文档。也许我们环境中的一些宿主机支持一个应用程序（即在线购物），而其他宿主机支持不同的应用程序（如发票处理）。如果所有宿主机都将数据报告到单个索引中，如果我们对针对一个或两个应用程序的数据报告和分析感兴趣，显然仅基于索引进行分析是不合适的——我们需要我们的分析以应用程序为中心。
- en: 'In order to accomplish this, we have a few options:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们有几种选择：
- en: Modifying the base query of the anomaly detection job so that it filters the
    data for only the hosts associated with the application of interest
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改异常检测作业的基本查询，以便仅过滤与感兴趣的应用程序相关的宿主机的数据
- en: Modifying the data on ingest to enrich it, to insert additional contextual information
    into each document, which will later be used to filter the query made by the anomaly
    detection job
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在摄取数据时修改数据以丰富它，向每个文档中插入额外的上下文信息，这些信息将随后被用于过滤异常检测作业发出的查询
- en: Both require customization of the datafeed query that the anomaly detection
    job makes to the raw data in the source indices. The first option may result in
    a relatively complex query and the second option requires an interstitial step
    of data enrichment using custom ingest pipelines. Let's briefly discuss each.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都需要对异常检测作业对源索引中的原始数据进行的数据流查询进行定制。第一种方法可能会导致查询相对复杂，第二种方法则需要使用自定义摄取管道进行数据丰富化的中间步骤。让我们简要讨论一下每种方法。
- en: Custom queries for anomaly detection datafeeds
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于异常检测数据流的自定义查询
- en: When a new job is created in the anomaly detection UI, the first step is to
    choose either an index pattern or a Kibana saved search. If the former is chosen,
    then a `{''match_all'':{}}` Elasticsearch query (return every record in the index)
    is invoked. If the job is created via the API or the advanced job wizard, then
    the user can specify just about any valid Elasticsearch DSL for filtering the
    data. Free-form composing Elasticsearch DSL can be a little error-prone for non-expert
    users. Therefore, a more intuitive way would be to approach this from Kibana via
    saved searches.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在异常检测UI中创建新作业时，第一步是选择索引模式或Kibana保存的搜索。如果选择前者，则将调用一个`{''match_all'':{}}` Elasticsearch查询（返回索引中的每条记录）。如果通过API或高级作业向导创建作业，则用户可以指定几乎任何有效的Elasticsearch
    DSL来过滤数据。对于非专家用户来说，自由组合Elasticsearch DSL可能会有些容易出错。因此，更直观的方法是从Kibana通过保存的搜索来处理这个问题。
- en: 'For example, let''s say that we have an index of log files and the appropriate
    hosts associated with the application we would like to monitor and analyze consist
    of two servers, `esxserver1.acme.com` and `esxserver2.acme.com`. On Kibana''s
    **Discover** page, we can build a filtered query using **KQL** using the search
    box at the top of the user interface:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个日志文件索引，我们想要监控和分析的应用程序相关的适当宿主机由两个服务器组成，`esxserver1.acme.com`和`esxserver2.acme.com`。在Kibana的**发现**页面上，我们可以使用用户界面顶部的搜索框构建一个使用**KQL**的过滤查询：
- en: '![Figure 7.3 – Building a filtered query using KQL'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.3 – 使用KQL构建过滤查询'
- en: '](img/B17040_07_3.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_07_3.jpg)'
- en: Figure 7.3 – Building a filtered query using KQL
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 使用KQL构建过滤查询
- en: 'The text of this KQL query would be as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此KQL查询的文本如下：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you were curious about the actual Elasticsearch DSL that is invoked by Kibana
    to get this filtered query, you could click the **Inspect** button in the top
    right and select the **Request** tab to see the Elasticsearch DSL:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对Kibana调用的实际Elasticsearch DSL以获取此过滤查询感到好奇，可以点击右上角的**检查**按钮并选择**请求**选项卡以查看Elasticsearch
    DSL：
- en: '![Figure 7.4 – Inspecting the Elasticsearch DSL that runs for the KQL filter'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.4 – 检查为KQL过滤器运行的Elasticsearch DSL'
- en: '](img/B17040_07_4.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_07_4.jpg)'
- en: Figure 7.4 – Inspecting the Elasticsearch DSL that runs for the KQL filter
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 检查为KQL过滤器运行的Elasticsearch DSL
- en: It is probably worth noting that despite the way the KQL query gets translated
    to Elasticsearch DSL in this specific example (using `match_phrase`, for example),
    it is not the only way to achieve the desired results. A query filter using `terms`
    is yet another way, but assessing the merits of one over the other is beyond the
    scope of this book.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 可能值得注意，尽管在这个特定示例中KQL查询被转换成Elasticsearch DSL的方式（例如使用`match_phrase`），但这并不是实现预期结果唯一的方式。使用`terms`的查询过滤器是另一种方式，但评估这两种方式的优劣超出了本书的范围。
- en: 'Regardless of the Elasticsearch DSL that runs behind the scenes, the key thing
    is that we have a query that filters the raw data to identify only the servers
    of interest for the application we would like to analyze with Elastic ML. To keep
    this filtered search, a click of the **Save** button in the top right and naming
    the search is necessary:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 不论后台运行的Elasticsearch DSL是什么，关键是我们有一个查询，它过滤原始数据，仅识别我们希望使用Elastic ML进行分析的应用程序感兴趣的服务器。为了保持这个过滤搜索，需要在右上角点击**保存**按钮并命名搜索：
- en: '![Figure 7.5 – Saving the search for later use in Elastic ML'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.5 – 在Elastic ML中保存搜索以供以后使用'
- en: '](img/B17040_07_5.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_07_5.jpg)'
- en: Figure 7.5 – Saving the search for later use in Elastic ML
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 在Elastic ML中保存搜索以供以后使用
- en: 'Later on, you could then select this saved search when configuring a new anomaly
    detection job:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，您可以在配置新的异常检测作业时选择此保存的搜索：
- en: '![Figure 7.6 – Leveraging a saved search in an anomaly detection job'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.6 – 在异常检测作业中使用保存的搜索'
- en: '](img/B17040_07_6.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_07_6.jpg)'
- en: Figure 7.6 – Leveraging a saved search in an anomaly detection job
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 在异常检测作业中使用保存的搜索
- en: As such, our ML job will now only run for the hosts of interest for this specific
    application. Thus, we have been able to effectively limit and segment the data
    analysis to the hosts that we've defined to have made a contribution to this application.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的ML作业现在将仅针对此特定应用程序感兴趣的主机运行。因此，我们已经能够有效地限制和分割数据分析，仅限于我们定义的为该应用程序做出贡献的主机。
- en: Data enrichment on ingest
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据摄取时的数据丰富
- en: Another option is to move the decision-making about which hosts belong to which
    applications further upstream to the time of ingest. If Logstash was part of the
    ingest pipeline, you could use a filter plugin to add additional fields to the
    data based upon a lookup against an asset list (file, database, and so on). Consult
    the Logstash documentation at [https://www.elastic.co/guide/en/logstash/current/lookup-enrichment.html](https://www.elastic.co/guide/en/logstash/current/lookup-enrichment.html),
    which shows you how to dynamically enrich the indexed documents with additional
    fields to provide context. If you were not using Logstash (merely using Beats/Elastic
    Agent and the ingest node), perhaps a simpler way would be to use the enrich processor
    instead. Consult the documentation at [https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-enriching-data.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-enriching-data.html).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是将关于哪些主机属于哪些应用的决策进一步上移到数据摄取的时间。如果Logstash是摄取管道的一部分，您可以使用过滤器插件根据资产列表（文件、数据库等）的查找来向数据添加额外的字段。请参阅Logstash文档[https://www.elastic.co/guide/en/logstash/current/lookup-enrichment.html](https://www.elastic.co/guide/en/logstash/current/lookup-enrichment.html)，其中展示了如何使用额外字段动态丰富索引文档以提供上下文。如果您没有使用Logstash（仅使用Beats/Elastic
    Agent和摄取节点），可能更简单的方法是使用 enrich 处理器。请参阅[https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-enriching-data.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-enriching-data.html)文档。
- en: 'For example, you could have this enrichment add an `application_name` field
    and dynamically populate the value of this field with the appropriate name of
    the application, such as the following (truncated JSON here):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以让这种丰富添加一个`application_name`字段，并动态填充该字段的值，例如以下内容（这里截断的JSON）：
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Or you could have the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 或者您还可以有如下内容：
- en: '[PRE3]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once the value of this field is set and inserted into the indexed documents,
    then you would use the `application_name` field, along with the ability to filter
    the query for the anomaly detection job (as previously described), to limit your
    data analysis to the pertinent application of interest. The addition of the data
    enrichment step may seem like a little more up-front effort, but it should pay
    dividends in the long term as it will be easier to maintain as asset names change
    or evolve, since the first method requires hardcoding the asset names into the
    searches of the ML jobs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置了该字段的值并将其插入到索引文档中，然后您就可以使用`application_name`字段，结合之前描述的过滤查询异常检测作业的能力，将数据分析限制在感兴趣的相关应用程序上。添加数据丰富步骤可能看起来需要更多的前期努力，但从长远来看，它应该会带来回报，因为当资产名称更改或演变时，这将更容易维护，因为第一种方法需要在机器学习作业的搜索中硬编码资产名称。
- en: Now that we have organized our data and perhaps even enriched it, let's now
    see how we can leverage that contextual information to make our anomaly detection
    jobs more effective.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经组织了我们的数据，也许甚至丰富了它，那么现在让我们看看我们如何可以利用这些上下文信息来使我们的异常检测作业更有效。
- en: Leveraging the contextual information
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用上下文信息
- en: With our data organized and/or enriched, the two primary ways we can leverage
    contextual information is via analysis **splits** and statistical **influencers**.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据组织/丰富后，我们可以利用上下文信息的两种主要方式是通过分析**拆分**和统计**影响因素**。
- en: Analysis splits
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析拆分
- en: We have already seen that an **anomaly** **detection** job can be split based
    on any categorical field. As such, we can individually model behavior separately
    for each instance of that field. This could be extremely valuable, especially
    in a case where each instance needs its own separate model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，异常检测作业可以根据任何分类字段进行拆分。因此，我们可以为该字段的每个实例单独建模行为。这可能会非常有价值，尤其是在每个实例需要其自己的单独模型的情况下。
- en: 'Take, for example, the case where we have data for different regions of the
    world:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以我们拥有世界不同地区的数据为例：
- en: '![Figure 7.7 – Differing data behaviors based on region'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.7 – 根据区域不同的数据行为](img/B17040_07_7.jpg)'
- en: '](img/B17040_07_7.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.7 – 根据区域不同的数据行为](img/B17040_07_7.jpg)'
- en: Figure 7.7 – Differing data behaviors based on region
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 根据区域不同的数据行为
- en: Whatever data this is (sales KPIs, utilization metrics, and so on), clearly
    it has very distinctive patterns that are unique to each region. In this case,
    it makes sense to split any analysis we do with anomaly detection for each region
    to capitalize on this uniqueness. We would be able to detect anomalies in the
    behavior that are specific to each region.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 无论这些数据是什么（销售KPI、利用率指标等），显然它们具有非常独特的模式，这些模式是每个区域独有的。在这种情况下，将任何分析拆分到每个区域进行异常检测以利用这种独特性是有意义的。我们将能够检测到特定于每个区域的行为异常。
- en: Let's also imagine that, within each region, a fleet of servers support the
    application and transaction processing, but they are load-balanced and contribute
    equally to the performance/operation. In that way, there's nothing unique about
    each server's contribution to a region. As such, it probably doesn't make sense
    to split the analysis per server.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也想象一下，在每一个区域内，一组服务器支持应用程序和事务处理，但它们是负载均衡的，并且对性能/操作的贡献是相等的。这样，每个服务器对区域的贡献就没有什么独特之处。因此，按服务器进行分析可能没有意义。
- en: We've naturally come to the conclusion that splitting by region is more effective
    than splitting by server. But what if a particular server within a region is having
    problems contributing to the anomalies that are being detected? Wouldn't we want
    to have this information available immediately, instead of having to manually
    diagnose further? This is possible to know via influencers.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们自然得出结论，按区域拆分比按服务器拆分更有效。但如果区域内的某个服务器在向被检测到的异常贡献方面存在问题呢？我们难道不想立即获得这些信息，而不是手动进一步诊断吗？这是通过影响因素可以做到的。
- en: Statistical influencers
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计影响因素
- en: We introduced the concept of influencers in [*Chapter 5*](B17040_05_Epub_AM.xhtml#_idTextAnchor090),
    *Interpreting Results*. As a reminder, an influencer is a field that describes
    an entity where you would like to know whether it ''influences'' (is to blame
    for) the existence of the anomaly or at least had a significant contribution.
    Remember that any field chosen as a candidate to be an influencer doesn't need
    to be part of the detection logic, although it is natural to pick fields that
    are used as splits to also be influencers. It is also important that influencers
    are chosen when the anomaly detection jobs are created as they cannot be added
    to the configuration later.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第五章*](B17040_05_Epub_AM.xhtml#_idTextAnchor090)“解释结果”中介绍了影响因素的概念。作为提醒，影响因素是一个字段，它描述了一个实体，你想要知道它是否“影响”（是异常的原因）或至少对异常有重大贡献。记住，任何被选为候选影响因素的字段不需要是检测逻辑的一部分，尽管选择用作拆分的字段作为影响因素是自然的。同样重要的是，在创建异常检测作业时选择影响因素，因为以后无法将其添加到配置中。
- en: It is also key to understand that the process of finding potential influencers
    happens after the anomaly detection job finds the anomaly. In other words, it
    does not affect any of the probability calculations that are made as part of the
    detection. Once the anomaly has been determined, ML will systematically go through
    all instances of each candidate influencer field and remove that instance's contribution
    to the data in that time bucket. If, once removed, the remaining data is no longer
    anomalous, then via counterfactual reasoning, that instance's contribution must
    have been influential and is scored accordingly (with an `influencer_score` value
    in the results).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要理解的是，在异常检测作业发现异常之后，才会进行寻找潜在影响因素的过程。换句话说，它不会影响检测过程中所做的任何概率计算。一旦确定了异常，机器学习将系统地遍历每个候选影响因素字段的每个实例，并从该时间桶中的数据中移除该实例的贡献。如果移除后，剩余的数据不再异常，那么通过反事实推理，该实例的贡献必须是具有影响力的，并且会相应地进行评分（结果中的`influencer_score`值）。
- en: What we will see in the next section, however, is how influencers can be leveraged
    when viewing the results of not just a single anomaly detection job, but potentially
    several related jobs. Let's now move on to discuss the process of grouping and
    viewing jobs together to assist with RCA.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在下一节中，我们将看到如何利用影响者在查看单个异常检测作业的结果时，以及可能的相关作业的结果。现在，让我们继续讨论如何将作业分组并一起查看，以协助根本原因分析。
- en: Bringing it all together for RCA
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有内容整合进行根本原因分析
- en: We are at the point now where we can now discuss how we can bring everything
    together. In our desire to increase our effectiveness in IT operations and look
    more holistically at application health, we now need to operationalize what we've
    prepared in the prior sections and configure our anomaly detection jobs accordingly.
    To that end, let's work through a real-life scenario in which Elastic ML helped
    us get to the root cause of an operational problem.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经到了可以讨论如何将所有事情整合在一起的程度。在我们希望提高IT运营效率并更全面地查看应用程序健康状态的需求下，我们现在需要将之前章节中准备的内容进行操作化，并相应地配置我们的异常检测作业。为此，让我们通过一个真实场景来探讨，在这个场景中，Elastic
    ML帮助我们找到了运营问题的根本原因。
- en: Outage background
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 故障背景
- en: This scenario is loosely based on a real application outage, although the data
    has been somewhat simplified and sanitized to obfuscate the original customer.
    The problem was with a retail application that processed gift card transactions.
    Occasionally, the app would stop working and transactions could not be processed.
    This would only be discovered when individual stores called headquarters to complain.
    The root cause of the issue was unknown and couldn't be ascertained easily by
    the customer. Because they never got to the root cause, and because the problem
    could be fixed by simply rebooting the application servers, the problem would
    randomly reoccur and plagued them for months.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这个场景大致基于一个真实的应用程序故障，尽管数据已经被简化并清洗过，以掩盖原始客户的信息。问题出在一个处理礼品卡交易的零售应用程序上。偶尔，该应用程序会停止工作，交易无法处理。这种情况只有在各个门店向总部投诉时才会被发现。问题的根本原因未知，客户无法轻易确定。由于他们从未找到根本原因，而且只需重新启动应用程序服务器即可解决问题，因此问题会随机复发，困扰了他们数月。
- en: 'The following data was collected and included in the analysis to help understand
    the origins of the problem. This data included the following (and is supplied
    in the GitHub repo):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 收集并包含在分析中的以下数据有助于理解问题的起源。这些数据包括以下内容（并在GitHub仓库中提供）：
- en: A summarized (1-minute) count of transaction volume (the main KPI)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交易量的总结（1分钟计数）（主要KPI）
- en: Application logs (semi-structured text-based messages) from the transaction
    processing engine
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自交易处理引擎的应用程序日志（基于半结构化文本的消息）
- en: SQL Server performance metrics from the database that backed the transaction
    processing engine
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自支持交易处理引擎的数据库的SQL Server性能指标
- en: Network utilization performance metrics from the network the transaction processing
    engine operates on
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自交易处理引擎运行的网络的网络利用率性能指标
- en: 'As such, four ML jobs were configured against the data. They were as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，针对数据配置了四个机器学习（ML）任务。它们如下所示：
- en: '`sum` on the number of transactions processed per minute'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每分钟处理交易数量的`sum`
- en: '`count` by the `mlcategory` detector to count the number of log messages by
    type, but using dynamic ML-based categorization to delineate different message
    types'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`mlcategory`检测器按类型计数日志消息的数量，但使用基于动态机器学习的分类来区分不同的消息类型
- en: '`mean` analysis of every SQL Server metric in the index'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指数中每个SQL Server指标的`mean`分析
- en: '`mean` analysis of every network performance metric in the index'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指数中每个网络性能指标的`mean`分析
- en: 'These four jobs were configured and run on the data when the problem occurred
    in the application. Anomalies were found, especially in the KPI that tracked the
    number of transactions being processed. In fact, this is the same KPI that we
    saw at the beginning of this chapter, where an unexpected dip in order processing
    was the main indicator that a problem was occurring:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序出现问题时，这四个作业在数据上进行了配置和运行。发现了异常，尤其是在跟踪正在处理的交易数量的KPI中。实际上，这正是我们在本章开头看到的KPI，其中订单处理的意外下降是问题发生的首要指标：
- en: '![Figure 7.8 – The KPI of the number of transactions processed'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.8 – 处理的交易数量关键绩效指标（KPI）'
- en: '](img/B17040_07_8.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_07_8.jpg)'
- en: Figure 7.8 – The KPI of the number of transactions processed
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 处理的交易数量关键绩效指标（KPI）
- en: However, the root cause wasn't understood until this KPI's anomaly was correlated
    with the anomalies in the other three ML jobs that were looking at the data in
    the underlying technology and infrastructure. Let's see how the power of visual
    correlation and shared influencers allowed the underlying cause to be discovered.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，直到这个KPI的异常与查看底层技术和基础设施数据的其他三个ML任务的异常相关联，才理解了根本原因。让我们看看视觉关联和共享影响因素的力量是如何帮助发现根本原因的。
- en: Correlation and shared influencers
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关性和共享影响因素
- en: 'In addition to the anomaly in the transactions processed KPI (in which an unexpected
    dip occurs), the other three anomaly detection jobs (for the network metrics,
    the application logs, and the SQL database metrics) were superimposed onto the
    same time frame in the **Anomaly Explorer**. The following screenshot shows the
    results of this:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 除了处理交易KPI中的异常（其中出现意外的下降）之外，其他三个异常检测作业（针对网络指标、应用程序日志和SQL数据库指标）被叠加到**异常探索器**的相同时间框架中。以下截图显示了这些结果：
- en: '![Figure 7.9 – Anomaly Explorer showing results of multiple jobs'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.9 – 异常探索器显示多个作业的结果'
- en: '](img/B17040_07_9.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_07_9.jpg)'
- en: Figure 7.9 – Anomaly Explorer showing results of multiple jobs
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 异常探索器显示多个作业的结果
- en: 'In particular, notice that the day the KPI was exhibiting problems (February
    8, 2021, as shown in *Figure 7.8*), the three other jobs in *Figure 7.9* exhibit
    correlated anomalies, shown by the circled area. Upon closer inspection (by clicking
    on the red tile for the `it_ops_sql` job), you can see that there were issues
    with several of the SQL Server metrics going haywire at the same time:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 特别注意，KPI出现问题的那天（2021年2月8日，如图7.8所示），图7.9中的其他三个作业表现出相关的异常，如图中圈出的区域所示。通过点击`it_ops_sql`作业的红色瓷砖进行更仔细的检查，可以看到有几个SQL
    Server指标同时出现混乱：
- en: '![Figure 7.10 – Anomaly Explorer showing anomalies for SQL Server'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.10 – 异常探索器显示SQL Server的异常'
- en: '](img/B17040_07_10.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_07_10.jpg)'
- en: Figure 7.10 – Anomaly Explorer showing anomalies for SQL Server
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 异常探索器显示SQL Server的异常
- en: Note
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: The shaded area of the charts is highlighting the window of time associated
    with the width of the selected tile in the swim lane. This window of time might
    be larger than the bucket span of the analysis (as is the case here) and therefore
    the shaded area can contain many individual anomalies during that time frame.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图表中的阴影区域突出了与泳道中选定瓷砖宽度相关的时间窗口。这个时间窗口可能比分析的桶跨度要大（正如这里的情况），因此阴影区域可能包含该时间段内的许多单个异常。
- en: 'If we look at the anomalies in the anomaly detection job for the application
    log, there is an influx of errors all referencing the database (further corroborating
    an unstable SQL server):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看应用程序日志异常检测作业中的异常，会发现大量错误都指向数据库（进一步证实了SQL服务器不稳定）：
- en: '![Figure 7.11 – Anomaly Explorer showing anomalies for the application log'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.11 – 异常探索器显示应用程序日志的异常'
- en: '](img/B17040_07_11.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_07_11.jpg)'
- en: Figure 7.11 – Anomaly Explorer showing anomalies for the application log
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 – 异常探索器显示应用程序日志的异常
- en: 'However, interesting things were also happening on the network:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，网络上也发生了有趣的事情：
- en: '![Figure 7.12 – Anomaly Explorer showing anomalies for the network data'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.12 – 异常探索器显示网络数据的异常'
- en: '](img/B17040_07_12.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_07_12.jpg)'
- en: Figure 7.12 – Anomaly Explorer showing anomalies for the network data
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 – 异常探索器显示网络数据的异常
- en: Specifically, there was a large spike in network traffic (shown by the **Out_Octets**
    metric), and a high spike in packets getting dropped at the network interface
    (shown by the **Out_Discards** metric).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，网络流量（通过**Out_Octets**指标显示）出现了大幅上升，并且网络接口处丢弃的数据包数量也急剧增加（通过**Out_Discards**指标显示）。
- en: At this point, there was clear suspicion that this network spike might have
    something to do with the database problem. And, while correlation is not always
    causation, it was enough of a clue to entice the operations team to look back
    over some historical data from prior outages. In every other outage, this large
    network spike and packet drops pattern also existed.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 到这个时候，很明显怀疑这个网络峰值可能与数据库问题有关。虽然相关性不总是因果关系，但这足以吸引运维团队回顾一些历史数据，以了解之前的故障情况。在其他每次故障中，都存在这种大的网络峰值和丢包模式。
- en: 'The ultimate cause of the network spike was VMware''s action of moving VMs
    to new ESX servers. Someone had misconfigured the network switch and VMware was
    sending this massive burst of traffic over the application VLAN instead of the
    management VLAN. When this occurred (randomly, of course), the transaction processing
    app would temporarily lose connection to the database and attempt to reconnect.
    However, there was a critical flaw in this reconnection code in that it would
    not attempt the reconnection to the database at the remote IP address that belonged
    to SQL Server. Instead, it attempted the reconnection to localhost (IP address
    `127.0.01`), where, of course, there was no such database. The clue to this bug
    was seen in one of the example log lines that Elastic ML displayed in the **Examples**
    section (circled in the following screenshot):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 网络峰值的最根本原因是VMware将虚拟机移动到新的ESX服务器。有人错误配置了网络交换机，VMware将这股巨大的流量发送到应用程序VLAN而不是管理VLAN。当然，当这种情况发生时（随机发生），事务处理应用程序会暂时失去与数据库的连接并尝试重新连接。然而，在这个重新连接代码中存在一个关键缺陷，即它不会尝试连接到属于SQL服务器的远程IP地址。相反，它尝试连接到本地主机（IP地址`127.0.01`），当然，那里没有这样的数据库。这个错误的线索在Elastic
    ML在**示例**部分显示的一个示例日志行中可以看到（以下截图中的圆圈所示）：
- en: '![Figure 7.13 – Anomaly Explorer showing the root cause of the reconnection
    problem'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.13 – 异常探索器显示重新连接问题的根本原因'
- en: '](img/B17040_07_13.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_07_13.jpg)'
- en: Figure 7.13 – Anomaly Explorer showing the root cause of the reconnection problem
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 – 异常探索器显示重新连接问题的根本原因
- en: Once the problem occurred, the connection to SQL Server was therefore only possible
    if the application server was completely rebooted, the startup configuration files
    were reread, and the IP address of SQL Server was relearned. This was why a full
    reboot always fixed the problem.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦问题发生，只有当应用程序服务器完全重启、重新读取启动配置文件以及重新学习SQL服务器的IP地址时，才能连接到SQL服务器。这就是为什么完整的重启总是能解决问题。
- en: 'One key thing to notice is how the influencers in the user interface also assist
    with narrowing down the scope of who''s at fault for the anomalies:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一个需要注意的关键点是用户界面中的影响因素如何帮助缩小异常责任人的范围：
- en: '![Figure 7.14 – Anomaly Explorer showing the top influencers'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.14 – 异常探索器显示顶级影响因素'
- en: '](img/B17040_07_14.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 B17040_07_14.jpg](img/B17040_07_14.jpg)'
- en: Figure 7.14 – Anomaly Explorer showing the top influencers
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 – 异常探索器显示顶级影响因素
- en: The top-scoring influencers over the time span selected in the dashboard are
    listed in the **Top influencers** section on the left. For each influencer, the
    maximum influencer score (in any bucket) is displayed, together with the total
    influencer score over the dashboard time range (summed across all buckets). And,
    if multiple jobs are being displayed together, then those influencers that are
    common across jobs have higher sums, thus pushing their ranking higher.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在仪表板上选择的时间范围内得分最高的影响因素列在左侧的**顶级影响因素**部分。对于每个影响因素，显示最大影响因素得分（在任何桶中），以及仪表板时间范围内的总影响因素得分（跨所有桶求和）。如果同时显示多个作业，那么在作业中共同的那些影响因素具有更高的总和，从而推动它们的排名更高。
- en: This is a very key point because now it is very easy to see commonalities in
    offending entities across jobs. If `esxserver1.acme.com` is the only physical
    host that surfaces as an influencer when viewing multiple jobs, then we immediately
    know which machine to focus on; we know it is not a widespread problem.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常重要的观点，因为现在很容易在多个作业中看到违规实体之间的共性。如果 `esxserver1.acme.com` 是在查看多个作业时作为影响因素出现的唯一物理主机，那么我们立即知道应该关注哪台机器；我们知道这不是一个普遍的问题。
- en: In the end, the customer was able to mitigate the system by both correcting
    the network misconfiguration and addressing the bug in the database reconnection
    code. They were able to narrow in on this root cause quite quickly because Elastic
    ML allowed them to narrow the focus of their investigation, thus saving time and
    preventing future occurrences.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，客户通过纠正网络配置错误和解决数据库重连代码中的错误，成功地缓解了系统。他们能够迅速缩小根本原因，因为 Elastic ML 允许他们缩小调查范围，从而节省时间并防止未来发生。
- en: Summary
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Elastic ML can certainly boost the amount of data that IT organizations *pay
    attention to*, and thus get more insight and proactive value out of their data.
    The ability to organize, correlate, and holistically view related anomalies across
    data types is critical to problem isolation and root cause identification. It
    reduces application downtime and limits the possibility of problem recurrence.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Elastic ML 确实可以增加 IT 组织**关注**的数据量，从而从他们的数据中获得更多洞察和主动价值。能够组织、关联并全面查看跨数据类型的相关异常对于问题隔离和根本原因识别至关重要。它减少了应用程序的停机时间，并限制了问题再次发生的可能性。
- en: In the next chapter, we will see how other apps within the Elastic Stack (APM,
    Security, and Logs) take advantage of Elastic ML to provide an out-of-the-box
    experience that's custom-tailored for specific use cases.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到 Elastic Stack（APM、安全和日志）中的其他应用程序如何利用 Elastic ML 提供即插即用的体验，该体验针对特定用例进行了定制。
