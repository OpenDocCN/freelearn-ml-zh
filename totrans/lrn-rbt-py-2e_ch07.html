<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Interfacing Vision Sensors with ROS</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Interfacing Vision Sensors with ROS</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we looked at actuators and how to interface the robot's sensors using the Tiva-C LaunchPad board. In this chapter, we will mainly look at vision sensors and the interface that they use with our robot.</p>
<p>The robot we are designing will have a 3D vision sensor, and we will be able to interface it with vision libraries such as <strong>Open Source Computer Vision</strong> (<strong>OpenCV</strong>), <strong>Open Natural Interaction (OpenNI)</strong>, and <strong>Point Cloud Library</strong> (<strong>PCL</strong>). The main application of the 3D vision sensor in our robot is autonomous navigation.</p>
<p>We will also look at how to interface the vision sensors with ROS and process the images that it senses using vision libraries such as OpenCV. In the last section of this chapter, we will look at the mapping and localization algorithm that we will use in our robot, called <strong>SLAM</strong> (<strong>simultaneous localization and mapping</strong>), and its implementation using a 3D vision sensor, ROS, and image-processing libraries.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>List of robotic vision sensors and image libraries</li>
<li>Introduction to OpenCV, OpenNI, and PCL</li>
<li>The ROS-OpenCV interface</li>
<li>Point cloud processing using the PCL-ROS interface</li>
<li>Conversion of point cloud data to laser scan data</li>
<li>Introduction to SLAM</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You will need an Ubuntu 16.04 system with ROS Kinetic installed, as well as a web camera and a depth camera in order to try out the example in this chapter.</p>
<p>In the first section, we will look at the 2D and 3D vision sensors that are available in the market that can be used in different robots.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">List of robotic vision sensors and image libraries</h1>
                </header>
            
            <article>
                
<p>A 2D vision sensor or an ordinary camera delivers 2D image frames of the surroundings, whereas a 3D vision sensor delivers 2D image frames and an additional parameter called the depth of each image point. We can find the <em>x</em>, <em>y</em>, and <em>z</em> distance of each point from the 3D sensor with respect to the sensor's axis.</p>
<p>There are quite a few vision sensors available on the market. Some of the 2D and 3D vision sensors that can be used in our robot are mentioned in this chapter.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Pixy2/CMUcam5</h1>
                </header>
            
            <article>
                
<p>The following picture&#160;shows the latest 2D vision sensor, called Pixy2/CMUcam5 (<a href="https://pixycam.com/pixy-cmucam5/">https://pixycam.com/pixy-cmucam5/</a>), which is able to detect color objects with high speed and accuracy, and can be interfaced with an Arduino board. Pixy can be used for fast object detection, and the user can teach it which object it needs to track. The Pixy module has a CMOS sensor and NXP LPC4330 (<a href="http://www.nxp.com/"><span class="URLPACKT">http://www.nxp.com/</span></a>) based on Arm Cortex M4/M0 cores for picture processing. The following image shows the Pixy/CMUcam5:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/451ea8e0-1fe5-48cf-9cec-ca024aa0fc3d.jpg" style="width:20.00em;height:20.00em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Pixy/CMUcam5 (<a href="http://a.co/1t91hn6">http://a.co/fZtPqck</a>)</div>
<p>The most commonly available 2D vision sensors are webcams. They contain a CMOS sensor and USB interface, but they do not have any inbuilt vision-processing capabilities like Pixy has.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Logitech C920 webcam</h1>
                </header>
            
            <article>
                
<p>The following picture shows a popular webcam from Logitech that can capture pictures of up to 5-megapixel resolution and HD videos:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/caf46080-ba33-4b2b-a455-c740153411cd.png" style="width:18.83em;height:15.67em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Logitech HD C920 webcam (http://a.co/02DUUYd)</div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Kinect 360</h1>
                </header>
            
            <article>
                
<p>We will now take a look at some of the 3D vision sensors available on the market. Some of the more popular sensors are Kinect, the Intel RealSense D400 series, and Orbbec Astra.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/584c1ed3-0591-44d6-a5de-6e86baa35746.png" style="width:24.42em;height:15.92em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Kinect sensor</div>
<p>Kinect is a 3D vision sensor originally developed for the Microsoft Xbox 360 game console. It mainly contains an RGB camera, an infrared projector, an IR depth camera, a microphone array, and a motor to alter its tilt. The RGB camera and depth camera capture images at a resolution of 640 x 480 at 30 Hz. The RGB camera captures 2D color images, whereas the depth camera captures monochrome depth images. Kinect has a depth-sensing range of between 0.8 m and 4 m.</p>
<p>Some of the applications of Kinect are 3D motion capture, skeleton tracking, face recognition, and voice recognition.</p>
<p>Kinect can be interfaced with a PC using the USB 2.0 interface and programmed using Kinect SDK, OpenNI, and OpenCV. Kinect SDK is only available for Windows platforms, and SDK is developed and supplied by Microsoft. The other two libraries are open source and available for all platforms. The Kinect we are using here is the first version of Kinect; the latest versions of Kinect only support Kinect SDK when it is running on Windows (see <a href="https://www.microsoft.com/en-us/download/details.aspx?id=40278">https://www.microsoft.com/en-us/download/details.aspx?id=40278</a> for more details).</p>
<div class="packt_infobox">The production of Kinect series sensors is discontinued, but you can still find the sensor on Amazon and eBay.</div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Intel RealSense D400 series</h1>
                </header>
            
            <article>
                
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/d8bd3629-00ce-4fa3-9c73-77e997e8bc7a.jpg" style="width:33.33em;height:9.00em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Intel RealSense D400 series (https://realsense.intel.com/)</div>
<p>The Intel RealSense D400 depth sensors are stereo cameras that come with an IR projector to enhance the depth data (see <a href="https://software.intel.com/en-us/realsense/d400">https://software.intel.com/en-us/realsense/d400</a> for more details), as shown in Figure 4. The more popular sensor models in the D400 series are D415 and D435. In Figure 4, the sensor on the left is D415 and the sensor on the right is D435. Each consists of a stereo camera pair, an RGB camera, and an IR projector. The stereo camera pair computes the depth of the environment with the help of the IR projector.</p>
<p>The major features of this depth camera are that it can work in an indoor and outdoor environment. It can deliver the depth image stream with 1280 x 720 resolution at 90 fps, and the RGB camera can deliver a resolution of up to 1920 x 1080. It has a USB-C interface, which enables fast data transfer between the sensor and the computer. It has a small form factor and is lightweight, which is ideal for a robotics vision application.</p>
<p>The applications of Kinect and Intel RealSense are the same, except for speech recognition. They will work in Windows, Linux, and Mac. We can develop applications by using ROS, OpenNI, and OpenCV. The following diagram shows the block diagram of the D400 series camera:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/78da5af1-86a5-4c4b-a3b8-2a420ae33cb6.png" style="width:31.33em;height:21.25em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Block diagram of the Intel RealSense D400 series</div>
<div class="packt_infobox">You can find the datasheet of the Intel RealSense series at the following link:<br/>
<a href="https://software.intel.com/sites/default/files/Intel_RealSense_Depth_Cam_D400_Series_Datasheet.pdf">https://software.intel.com/sites/default/files/Intel_RealSense_Depth_Cam_D400_Series_Datasheet.pdf<br/></a>A research paper about Intel RealSense's depth sensor can be found at the following link:<br/>
<a href="https://arxiv.org/abs/1705.05548">https://arxiv.org/abs/1705.05548<br/></a>You can find the Intel RealSense SDK at the following link:<br/>
<a href="https://github.com/IntelRealSense/librealsense">https://github.com/IntelRealSense/librealsense</a></div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Orbbec Astra depth sensor</h1>
                </header>
            
            <article>
                
<p>The new Orbbec Astra sensor is one of the alternatives to Kinect available on the market. It has similar specs compared to Kinect and uses similar technology to obtain depth information. Similar to Kinect, it has an IR projector, RGB camera, and IR sensor. It also comes with a microphone, which helps for voice recognition applications. The following image shows all parts of the Orbbec Astra depth sensor:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/d2d5d043-de46-472a-8b5c-94e6ecda1f28.jpg" style="width:36.25em;height:19.08em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Orbbec Astra depth sensor (https://orbbec3d.com/product-astra/)</div>
<p>The Astra sensor comes in two models: Astra and Astra S. The main difference between these two models is the depth range. The Astra has a depth range of 0.6-8 m, whereas the Astra S has a range of 0.4-2 m. The Astra S is best suited for 3D scanning, whereas the Astra can be used in robotics applications. The size and weight of Astra is much lower than that of Kinect. These two models can both deliver depth data and an RGB image of 640 x 480 resolution at 30 fps. You can use a higher resolution, such as 1280 x 960, but it may reduce the frame rate. They also have the ability to track skeletons, like Kinect.</p>
<p>The sensor is compliant with the OpenNI framework, so an application built using OpenNI can also work using this sensor. We are going to use this sensor in our robot.</p>
<p>The SDK is compatible with Windows, Linux, and Mac OS X. For more information, you can go to the sensor's development website at <a href="https://orbbec3d.com/develop/">https://orbbec3d.com/develop/</a>.</p>
<p>One of the sensors you can also refer to is the ZED Camera (https://www.stereolabs.com/zed/). It is a stereo vision camera system which can able to deliver high resolution with good frame rate.&#160; The price is around 450 USD which is higher than above sensors. This can be used for high-end robotics applications required good accuracy from sensors.</p>
<p>We can see the ROS interfacing for this sensor in the upcoming section.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to OpenCV, OpenNI, and PCL</h1>
                </header>
            
            <article>
                
<p>Let's look at the software frameworks and libraries that we will be using in our robots. First, let's look at OpenCV. This is one of the libraries that we are going to use in this robot for object detection and other image-processing capabilities.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">What is OpenCV?</h1>
                </header>
            
            <article>
                
<p><strong>OpenCV</strong> is an open source, BSD-licensed computer vision library that includes the implementations of hundreds of computer-vision algorithms. The library, mainly intended for real-time computer vision, was developed by Intel Russia's research, and is now actively supported by Itseez (<a href="https://github.com/Itseez">https://github.com/Itseez</a>). In 2016, Intel acquired Itseez.</p>
<p>OpenCV is written mainly in C and C++, and its primary interface is in C++. It also has good interfaces in Python, Java, and MATLAB/Octave, and also has wrappers in other languages (such as C# and Ruby).</p>
<p>In the latest version of OpenCV, there is support for CUDA and OpenCL to enable GPU acceleration (<a href="http://www.nvidia.com/object/cuda_home_new.html"><span class="URLPACKT">http://www.nvidia.com/object/cuda_home_new.html</span></a>).</p>
<p>OpenCV will run on most OS platforms (such as Windows, Linux, Mac OS X, Android, FreeBSD, OpenBSD, iOS, and BlackBerry).</p>
<p>In Ubuntu, OpenCV, the Python wrapper, and the ROS wrapper are already installed when we install the <kbd>ros-kinetic-desktop-full</kbd> or <kbd>ros-melodic-desktop-full</kbd> package. The following commands install the OpenCV-ROS package individually.</p>
<p>In Kinetic:</p>
<pre>    <strong>$ sudo apt-get install ros-kinetic-vision-opencv</strong>  </pre>
<p>In Melodic:</p>
<pre>    <strong>$ sudo apt-get install ros-melodic-vision-opencv</strong></pre>
<p>If you want to verify that the OpenCV-Python module is installed on your system, take a Linux Terminal, and enter the <em>python</em> command. You should then see the Python interpreter. Try to execute the following commands in the Python terminal to verify the OpenCV installation:</p>
<pre>    <strong>&gt;&gt;&gt; import cv2</strong>
    <strong>&gt;&gt;&gt; cv2.__version__</strong></pre>
<p>If this command is successful, this version of OpenCV will be installed on your system. The version might be either 3.3.x or 3.2.x.</p>
<div class="packt_infobox">If you want to try OpenCV in Windows, you can try the following link:<br/>
<a href="https://docs.opencv.org/3.3.1/d5/de5/tutorial_py_setup_in_windows.html">https://docs.opencv.org/3.3.1/d5/de5/tutorial_py_setup_in_windows.html<br/></a>The following link will guide you through the installation process of OpenCV on Mac OS X:<br/>
<a href="https://www.learnopencv.com/install-opencv3-on-macos/">https://www.learnopencv.com/install-opencv3-on-macos/</a></div>
<p>The main applications of OpenCV are in the following fields:</p>
<ul>
<li>Object detection</li>
<li>Gesture recognition</li>
<li>Human-computer interaction</li>
<li>Mobile robotics</li>
<li>Motion tracking</li>
<li>Facial-recognition systems</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Installation of OpenCV from the source code in Ubuntu</h1>
                </header>
            
            <article>
                
<p>The OpenCV installation can be customized. If you want to customize your OpenCV installation, you can try to install it from the source code. You can find out how to do this installation at <a href="https://docs.opencv.org/trunk/d7/d9f/tutorial_linux_install.html"><span class="URLPACKT">https://docs.opencv.org/trunk/d7/d9f/tutorial_linux_install.html</span></a>.</p>
<p>To work with the examples in this chapter, it's best that you work with OpenCV installed, along with ROS.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Reading and displaying an image using the Python-OpenCV interface</h1>
                </header>
            
            <article>
                
<p>The first example will load an image in grayscale and display it on the screen.</p>
<p>In the following section of code, we will import the <kbd>numpy</kbd> module for handling the image array. The <kbd>cv2</kbd> module is the OpenCV wrapper for Python, which we can use to access OpenCV Python APIs. NumPy is an extension to the Python programming language, adding support for large multidimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays (see <a href="https://pypi.python.org/pypi/numpy"><span class="URLPACKT">https://pypi.python.org/pypi/numpy</span></a> for more information):</p>
<pre>#!/usr/bin/env python 
import numpy as np 
import cv2 </pre>
<p>The following function will read the <kbd>robot.jpg</kbd> image and load this image in grayscale. The first argument of the <kbd>cv2.imread()</kbd> function is the name of the image and the next argument is a flag that specifies the color type of the loaded image. If the flag is greater than 0, the image returns a three-channel RGB color image; if the flag is 0, the loaded image will be a grayscale image; and if the flag is less than 0, it will return the same image as was loaded:</p>
<pre>img = cv2.imread('robot.jpg',0) </pre>
<p>The following section of code will show the read image using the <kbd>imshow()</kbd> function. The <kbd>cv2.waitKey(0)</kbd> function is a keyboard-binding function. Its argument is time in milliseconds. If it's 0, it will wait indefinitely for a key stroke:</p>
<pre>cv2.imshow('image', img) 
cv2.waitKey(0) </pre>
<p>The <kbd>cv2.destroyAllWindows()</kbd> function simply destroys all the windows we created:</p>
<pre>cv2.destroyAllWindows() </pre>
<p>Save the preceding code as <kbd>image_read.py</kbd> and copy a JPG file and name it <kbd>robot.jpg</kbd>. Execute the code using the following command:</p>
<pre>    <strong>$python image_read.py</strong></pre>
<p>The output will load an image in grayscale because we used <kbd>0</kbd> as the value in the <kbd>imread()</kbd> function:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/6f9b4a8a-56d6-4dd0-a6ec-b1b088ce5844.png" style="width:17.00em;height:22.50em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Output of read image code</div>
<p>The following example will try to use an open webcam. The program will quit when the user presses any button.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Capturing from the web camera</h1>
                </header>
            
            <article>
                
<p>The following code will capture an image using the webcam with the device name <kbd>/dev/video0</kbd> or <kbd>/dev/video1</kbd>.</p>
<p>We need to import the <em>numpy</em> and <em>cv2</em> modules for capturing an image from a camera:</p>
<pre>#!/usr/bin/env python 
import numpy as np 
import cv2</pre>
<p>The following function will create a <kbd>VideoCapture</kbd> object. The <kbd>VideoCapture</kbd> class is used to capture videos from video files or cameras. The initialization argument of the <kbd>VideoCapture</kbd> class is the index of a camera or the name of a video file. The device index is just a number that is used to specify the camera. The first camera index is 0, and has the device name <kbd>/dev/video0</kbd>-that's why we will put <kbd>0</kbd> in the following code:</p>
<pre>cap = cv2.VideoCapture(0) </pre>
<p>The following section of code is looped to read image frames from the <kbd>VideoCapture</kbd> object, and shows each frame. It will quit when any key is pressed:</p>
<pre>while(True): 
    # Capture frame-by-frame 
    ret, frame = cap.read() 
    # Display the resulting frame 
    cv2.imshow('frame', frame) 
    k = cv2.waitKey(30) 
    if k &gt; 0: 
        break </pre>
<p>The following is a screenshot of the program output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/8f0f6487-3d71-4b6f-afe4-c1ff2a195530.png" style="width:32.33em;height:25.58em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Output of the video capture</div>
<p>You can explore more OpenCV-Python tutorials at</p>
<p><a href="http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_tutorials.html"><span class="URLPACKT">http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_tutorials.html</span></a>.</p>
<p>In the next section, we will look at the OpenNI library and its application.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">What is OpenNI?</h1>
                </header>
            
            <article>
                
<p>OpenNI is a multilanguage, cross-platform framework that defines APIs in order to write applications using <strong>natural interaction</strong> (<strong>NI</strong>) (see <a href="https://structure.io/openni">https://structure.io/openni</a> for more information). Natural interaction refers to the way in which people naturally communicate through gestures, expressions, and movements, and discover the world by looking around and manipulating physical objects and materials.</p>
<p>OpenNI APIs are composed of a set of interfaces that are used to write NI applications. The following figure shows a three-layered view of the OpenNI library:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/6e270787-c408-45fc-ab89-7cc198e14a74.png" style="width:41.50em;height:27.08em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">OpenNI framework software architecture</div>
<p>The top layer represents the application layer that implements the natural interaction-based application. The middle layer is the OpenNI layer, and it will provide communication interfaces that interact with sensors and middleware components that analyze the data from the sensor. Middleware can be used for full-body analysis, hand-point analysis, gesture detection, and so on. One example of a middle layer component is NITE (<a href="http://www.openni.ru/files/nite/index.html">http://www.openni.ru/files/nite/index.html</a>), which can detect gestures and skeletons.</p>
<p>The bottom layer contains the hardware devices that capture the visual and audio elements of the scene. It can include 3D sensors, RGB cameras, IR cameras, and microphones.</p>
<p>The latest version of OpenNI is OpenNI 2, which support sensors such as Asus Xtion Pro, and Primesense Carmine. The first version of OpenNI mainly supports the Kinect 360 sensor.</p>
<p>OpenNI is cross platform, and has been successfully compiled and deployed on Linux, Mac OS X, and Windows.</p>
<p>In the next section, we will see how we to install OpenNI in Ubuntu.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Installing OpenNI in Ubuntu</h1>
                </header>
            
            <article>
                
<p>We can install the OpenNI library along with ROS packages. ROS is already interfaced with OpenNI, but the ROS desktop full installation may not install OpenNI packages; if so, we need to install it from the package manager.</p>
<p>The following command will install the ROS-OpenNI library (which is mainly supported by the Kinect Xbox 360 sensor) in Kinetic and Melodic:</p>
<pre><strong>$ sudo apt-get install ros-&lt;version&gt;-openni-launch</strong></pre>
<p>The following command will install the ROS-OpenNI 2 library (which is mainly supported by Asus Xtion Pro and Primesense Carmine):</p>
<pre>    <strong>$ sudo apt-get install ros-&lt;version&gt;-openni2-launch </strong>  </pre>
<p>The source code and latest build of OpenNI for Windows, Linux, and MacOS X is available at <a href="http://structure.io/openni"><span class="URLPACKT">http://structure.io/openni</span></a>.</p>
<p>In the next section, we will look at how to install PCL.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">What is PCL?</h1>
                </header>
            
            <article>
                
<p>A <strong>point cloud</strong> is a set of data points in space that represent a 3D object or an environment. Generally, a point cloud is generated from depth sensors, such as Kinect and LIDAR. PCL (Point Cloud Library) is a large scale, open project for 2D/3D images and point-cloud processing. The PCL framework contains numerous algorithms that perform filtering, feature estimation, surface reconstruction, registration, model fitting, and segmentation. Using these methods, we can process the point cloud, extract key descriptors to recognize objects in the world based on their geometric appearance, create surfaces from the point clouds, and visualize them.</p>
<p>PCL is released under the BSD license. It's open source, free for commercial use, and free for research use. PCL is cross platform and has been successfully compiled and deployed on Linux, macOS X, Windows, and Android/iOS.</p>
<p>You can download PCL at <a href="http://pointclouds.org/downloads/"><span class="URLPACKT">http://pointclouds.org/downloads/</span></a>.</p>
<p>PCL is already integrated into ROS. The PCL library and its ROS interface are included in a ROS full desktop installation. PCL is the 3D-processing backbone of ROS. Refer to <span class="URLPACKT">http://wiki.ros.org/pcl</span> for details on the ROS-PCL package.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Programming Kinect with Python using ROS, OpenCV, and OpenNI</h1>
                </header>
            
            <article>
                
<p>Let's look at how we can interface and work with the Kinect sensor in ROS. ROS is bundled with the OpenNI driver, which can fetch the RGB and depth image of Kinect. The OpenNI and OpenNI 2 package in ROS can be used for interfacing with Microsoft Kinect, Primesense Carmine, Asus Xtion Pro, and Pro Live.</p>
<p>When we install ROS's <kbd>openni_launch</kbd> package, it will also install its dependent packages, such as <kbd>openni_camera</kbd>. The <kbd>openni_camera</kbd> package is the Kinect driver that publishes raw data and sensor information, whereas the <kbd>openni_launch</kbd> package contains ROS launch files. These launch files launch multiple nodes at a time and publish data such as the raw depth, RGB, and IR images, and the point cloud.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to launch the OpenNI driver</h1>
                </header>
            
            <article>
                
<p>You can connect the Kinect sensor to your computer using a USB interface and make sure it is detected on your PC using the <kbd>dmesg</kbd> command in the terminal. After setting up Kinect, we can start ROS's OpenNI driver to get data from the device.</p>
<p>The following command will open the OpenNI device and load all nodelets (see <a href="http://wiki.ros.org/nodelet">http://wiki.ros.org/nodelet</a> for more information) to convert raw depth/RGB/IR streams to depth images, disparity images, and point clouds. The ROS <kbd>nodelet</kbd> package is designed to provide a way to run multiple algorithms in the same process with zero copy transport between algorithms:</p>
<pre>    <strong>$ roslaunch openni_launch openni.launch</strong>  </pre>
<p>After starting the driver, you can list out the various topics published by the driver using the following command:</p>
<pre>    <strong>$ rostopic list</strong>  </pre>
<p>You can view the RGB image using a ROS tool called <kbd>image_view</kbd>:</p>
<pre>    <strong>$ rosrun image_view image_view image:=/camera/rgb/image_color</strong>  </pre>
<p>In the next section, we will learn how to interface these images with OpenCV for image processing.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The ROS interface with OpenCV</h1>
                </header>
            
            <article>
                
<p>OpenCV is also integrated into ROS, mainly for image processing. The <kbd>vision_opencv</kbd> ROS stack includes the complete OpenCV library and the interface with ROS.</p>
<p>The <kbd>vision_opencv</kbd> meta package consists of individual packages:</p>
<ul>
<li><kbd>cv_bridge</kbd>: This contains the <kbd>CvBridge</kbd> class. This class converts ROS image messages to the OpenCV image data type and vice versa.</li>
<li><kbd>image_geometry</kbd>: This contains a collection of methods to handle image and pixel geometry.</li>
</ul>
<p>The following diagram shows how OpenCV is interfaced with ROS:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/46faa908-19fe-479d-a6e2-30d47d4c37bd.png" style="width:14.92em;height:13.33em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">OpenCV-ROS interfacing</div>
<p>The image data types of OpenCV are <kbd>IplImage</kbd> and <kbd>Mat</kbd>. If we want to work with OpenCV in ROS, we have to convert <kbd>IplImage</kbd> or <kbd>Mat</kbd> to ROS image messages. The ROS package <kbd>vision_opencv</kbd> has the <kbd>CvBridge</kbd> class; this class can convert <kbd>IplImage</kbd> to a ROS image and vice versa. Once we get the ROS image topics from any kind of vision sensor, we can use ROS CvBridge in order to convert it from ROS topic to Mat or IplImage format.</p>
<p>The following section shows you how to create a ROS package; this package contains a node to subscribe to RGB and depth images, process RGB images to detect edges and display all images after converting them to an image type equivalent to OpenCV.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Creating a ROS package with OpenCV support</h1>
                </header>
            
            <article>
                
<p>We can create a package called <kbd>sample_opencv_pkg</kbd> with the following dependencies: <kbd>sensor_msgs</kbd>, <kbd>cv_bridge</kbd>, <kbd>rospy</kbd>, and <kbd>std_msgs</kbd>. The <kbd>sensor_msgs</kbd> dependency defines ROS messages for commonly used sensors, including cameras and scanning-laser rangefinders. The <kbd>cv_bridge</kbd> dependency is the OpenCV interface of ROS.</p>
<p>The following command will create the ROS package with the aforementioned dependencies:</p>
<pre>    <strong>$ catkin-create-pkg sample_opencv_pkg sensor_msgs cv_bridge <br/> rospy std_msgs</strong>  </pre>
<p>After creating the package, create a <kbd>scripts</kbd> folder inside the package; we will use it as a location in which to save the code that will be mentioned in the next section.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Displaying Kinect images using Python, ROS, and cv_bridge</h1>
                </header>
            
            <article>
                
<p>The first section of the Python code is given in the following code fragment. It mainly involves importing <kbd>rospy</kbd>, <kbd>sys</kbd>, <kbd>cv2</kbd>, <kbd>sensor_msgs</kbd>, <kbd>cv_bridge,</kbd> and the <kbd>numpy</kbd> module. The <kbd>sensor_msgs</kbd> dependency imports the ROS data type of both image and camera information type. The <kbd>cv_bridge</kbd> module imports the <kbd>CvBridge</kbd> class for converting the ROS image data type to the OpenCV data type and vice versa:</p>
<pre class="mce-root">import rospy<br/>import sys<br/>import cv2<br/>from sensor_msgs.msg import Image, CameraInfo<br/>from cv_bridge import CvBridge, CvBridgeError<br/>from std_msgs.msg import String<br/>import numpy as np</pre>
<p>The following section of code is a class definition in Python that we will use to demonstrate <kbd>CvBridge</kbd> functions. The class is called <kbd>cvBridgeDemo</kbd>:</p>
<pre>class cvBridgeDemo(): 
    def __init__(self): 
        self.node_name = "cv_bridge_demo" 
        #Initialize the ros node 
        rospy.init_node(self.node_name) 
 
        # What we do during shutdown 
        rospy.on_shutdown(self.cleanup) 
 
        # Create the cv_bridge object 
        self.bridge = CvBridge() 
 
        # Subscribe to the camera image and depth topics and set 
        # the appropriate callbacks 
        self.image_sub = <br/> rospy.Subscriber("/camera/rgb/image_raw", Image, <br/> self.image_callback)        self.depth_sub = <br/> rospy.Subscriber("/camera/depth/image_raw", Image, <br/> self.depth_callback) 
 
 #Callback executed when the timer timeout 
      rospy.Timer(rospy.Duration(0.03), self.show_img_cb) 
 
      rospy.loginfo("Waiting for image topics...")</pre>
<p>Here is the callback to visualize the actual RGB image, processed RGB image, and depth image:</p>
<pre>    def show_img_cb(self,event): 
         try:  
 
 
             cv2.namedWindow("RGB_Image", cv2.WINDOW_NORMAL) 
             cv2.moveWindow("RGB_Image", 25, 75) 
          
             cv2.namedWindow("Processed_Image", cv2.WINDOW_NORMAL) 
             cv2.moveWindow("Processed_Image", 500, 75) 
 
             # And one for the depth image 
             cv2.moveWindow("Depth_Image", 950, 75) 
             cv2.namedWindow("Depth_Image", cv2.WINDOW_NORMAL) 
 
             cv2.imshow("RGB_Image",self.frame) 
             cv2.imshow("Processed_Image",self.display_image) 
             cv2.imshow("Depth_Image",self.depth_display_image) 
             cv2.waitKey(3) 
         except: 
             pass </pre>
<p>The following code gives a callback function of the color image from Kinect. When a color image is received on the <kbd>/camera/rgb/image_raw</kbd> topic, it will call this function. This function will process the color frame for edge detection and show the edge detected and the raw color image:</p>
<pre>    def image_callback(self, ros_image): 
        # Use cv_bridge() to convert the ROS image to OpenCV format 
        try: 
            self.frame = self.bridge.imgmsg_to_cv2(ros_image, "bgr8") 
        except CvBridgeError, e: 
            print e 
       pass 
 
        # Convert the image to a Numpy array since most cv2 functions 
        # require Numpy arrays. 
        frame = np.array(self.frame, dtype=np.uint8) 
         
        # Process the frame using the process_image() function 
        self.display_image = self.process_image(frame) 
 </pre>
<p>The following code gives a callback function of the depth image from Kinect. When a depth image is received on the <kbd>/camera/depth/raw_image</kbd> topic, it will call this function. This function will show the raw depth image:</p>
<pre>       def depth_callback(self, ros_image): 
        # Use cv_bridge() to convert the ROS image to OpenCV format 
        try: 
            # The depth image is a single-channel float32 image 
            depth_image = self.bridge.imgmsg_to_cv2(ros_image, "32FC1") 
        except CvBridgeError, e: 
            print e 
       pass 
        # Convert the depth image to a Numpy array since most cv2 functions 
        # require Numpy arrays. 
        depth_array = np.array(depth_image, dtype=np.float32) 
                 
        # Normalize the depth image to fall between 0 (black) and 1 (white) 
        cv2.normalize(depth_array, depth_array, 0, 1, cv2.NORM_MINMAX) 
         
        # Process the depth image 
        self.depth_display_image = self.process_depth_image(depth_array) </pre>
<p>The following function is called <kbd>process_image(),</kbd> and will convert the color image to grayscale, then blur the image, and find the edges using the canny edge filter:</p>
<pre>    def process_image(self, frame): 
        # Convert to grayscale 
        grey = cv2.cvtColor(frame, cv.CV_BGR2GRAY) 
 
        # Blur the image 
        grey = cv2.blur(grey, (7, 7)) 
 
        # Compute edges using the Canny edge filter 
        edges = cv2.Canny(grey, 15.0, 30.0) 
 
        return edges </pre>
<p>The following function is called <kbd>process_depth_image()</kbd>. It simply returns the depth frame:</p>
<pre>    def process_depth_image(self, frame): 
        # Just return the raw image for this demo 
        return frame </pre>
<p>The following function will close the image window when the node shuts down:</p>
<pre>    def cleanup(self): 
        print "Shutting down vision node." 
        cv2.destroyAllWindows()</pre>
<p>The following code is the <kbd>main()</kbd> function. It will initialize the <kbd>cvBridgeDemo()</kbd> class and call the <kbd>rospy.spin()</kbd> function:</p>
<pre>def main(args): 
    try: 
        cvBridgeDemo() 
        rospy.spin() 
    except KeyboardInterrupt: 
        print "Shutting down vision node." 
        cv.DestroyAllWindows() 
 
if __name__ == '__main__': 
    main(sys.argv) </pre>
<p>Save the preceding code as <kbd>cv_bridge_demo.py</kbd> and change the permission of the node using the following command. The nodes are only visible to the <kbd>rosrun</kbd> command if we give it executable permission:</p>
<pre>    <strong>$ chmod +X cv_bridge_demo.py</strong>  </pre>
<p>The following are the commands to start the driver and node. Start the Kinect driver using the following command:</p>
<pre>    <strong>$ roslaunch openni_launch openni.launch</strong>  </pre>
<p>Run the node using the following command:</p>
<pre>    <strong>$ rosrun sample_opencv_pkg cv_bridge_demo.py</strong>  </pre>
<p>The following is a screenshot of the output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/feee3af4-7c2b-44cd-a94a-f9e951f1dfd3.png" style="width:63.00em;height:17.50em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">RGB, depth, and edge images</div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Interfacing Orbbec Astra with ROS</h1>
                </header>
            
            <article>
                
<p>One of the alternatives to Kinect is Orbbec Astra. There are ROS drivers available for Astra, and we can see how to set up that driver and get the image, depth, and point cloud from this sensor.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Installing the Astraâ€“ROS driver</h1>
                </header>
            
            <article>
                
<p>The complete instructions to set up the Astra-ROS driver in Ubuntu are mentioned at <a href="https://github.com/orbbec/ros_astra_camera">https://github.com/orbbec/ros_astra_camera</a> and <a href="http://wiki.ros.org/Sensors/OrbbecAstra">http://wiki.ros.org/Sensors/OrbbecAstra</a>. After installing the driver, you can launch it using the following command:</p>
<pre>    <strong>$ roslaunch astra_launch astra.launch</strong>  </pre>
<p>You can also install the Astra driver from the ROS package repository. Here is the command to install those packages:</p>
<pre>    <strong>$ sudo apt-get install ros-kinetic-astra-camera</strong>
    <strong>$ sudo apt-get install ros-kinetic-astra-launch</strong>  </pre>
<p>After installing these packages, you have to set the permission of the device in order to work with the device, as described at <a href="http://wiki.ros.org/astra_camera">http://wiki.ros.org/astra_camera</a>. You can check the ROS topics that are generated from this driver using the <kbd>rostopic</kbd> list command in the terminal. In addition, we can use the same Python code for image processing that we mentioned in the previous section.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Working with point clouds using Kinect, ROS, OpenNI, and PCL</h1>
                </header>
            
            <article>
                
<p>A 3D point cloud is a way of representing a 3D environment and 3D objects as collection points along the x, y, and z axes. We can get a point cloud from various sources: Either we can create our point cloud by writing a program or we can generate it from depth sensors or laser scanners.</p>
<p>PCL supports the OpenNI 3D interfaces natively; thus, it can acquire and process data from devices (such as Prime Sensor's 3D cameras, Microsoft Kinect, or Asus Xtion Pro).</p>
<p>PCL will be included in the ROS full desktop installation. Let's see how we can generate and visualize a point cloud in RViz, a data visualization tool in ROS.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Opening the device and generating a point cloud</h1>
                </header>
            
            <article>
                
<p>Open a new terminal and launch the ROS-OpenNI driver, along with the point cloud generator nodes, using the following command:</p>
<pre>    <strong>$ roslaunch openni_launch openni.launch</strong>  </pre>
<p>This command will activate the Kinect driver and process the raw data into convenient outputs, such as a point cloud.</p>
<p>If you are using Orbbec Astra, you can use the following command:</p>
<pre>    <strong>$ roslaunch astra_launch astra.launch</strong>  </pre>
<p>We will use the RViz 3D visualization tool to view our point clouds.</p>
<p>The following command will start the RViz tool:</p>
<pre>    <strong>$ rosrun rviz rviz</strong>  </pre>
<p>Set the RViz options for <span class="packt_screen">Fixed Frame</span> (at the top of the <span class="packt_screen">Displays</span> panel under <span class="packt_screen">Global Options</span>) to <span class="packt_screen">camera_link</span>.</p>
<p>From the left-hand side panel of the <span class="packt_screen">RViz</span> panel, click on the <span class="packt_screen">Add</span> button and choose the <span class="packt_screen">PointCloud2</span> display option. Set its topic to <span class="packt_screen">/camera/depth/points</span> (this is the topic for Kinect; it will be different for other sensors)</p>
<p>Change the <span class="packt_screen">Color Transformer</span> of <span class="packt_screen">PointCloud2</span> to <span class="packt_screen">AxisColor</span>.</p>
<p>The following screenshot shows a screenshot of the RViz point cloud data. You can see the nearest objects are marked in red and the farthest objects are marked in violet and blue. The objects in front of the Kinect are represented as a cylinder and cube:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="images/1639b5ac-df7e-4fee-932b-0eaf19437307.png" style="width:82.67em;height:46.92em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Visualizing point cloud data in Rviz</div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Conversion of point cloud data to laser scan data</h1>
                </header>
            
            <article>
                
<p>We are using Astra in this robot to replicate the function of an expensive laser range scanner. The depth image is processed and converted to the data equivalent of a laser scanner using ROS's <kbd>depthimage_to_laserscan</kbd> package (see <a href="http://wiki.ros.org/depthimage_to_laserscan">http://wiki.ros.org/depthimage_to_laserscan</a> for more information).</p>
<p>You can either install this package from the source code or use the Ubuntu package manager. Here is the command to install this package from the Ubuntu package manager</p>
<pre>    <strong>$ sudo apt-get install ros-&lt;version&gt;-depthimage-to-laserscan</strong></pre>
<p>The main function of this package is to slice a section of the depth image and convert it to an equivalent laser scan data type. The ROS <kbd>sensor_msgs/LaserScan</kbd> message type is used for publishing the laser scan data. This <kbd>depthimage_to_laserscan</kbd> package will perform this conversion and fake the laser scanner data. The laser scanner output can be viewed using RViz. In order to run the conversion, we have to start the convertor nodelets that will perform this operation. We have to specify this in our launch file in order to start the conversion. The following is the required code in the launch file to start the <kbd>depthimage_to_laserscan</kbd> conversion:</p>
<pre>  &lt;!-- Fake laser --&gt; 
  &lt;node pkg="nodelet" type="nodelet" <br/> name="laserscan_nodelet_manager" args="manager"/&gt;  &lt;node pkg="nodelet" type="nodelet" <br/> name="depthimage_to_laserscan"        args="load depthimage_to_laserscan/DepthImageToLaserScanNodelet <br/> laserscan_nodelet_manager"&gt; 
    &lt;param name="scan_height" value="10"/&gt; 
    &lt;param name="output_frame_id" value="/camera_depth_frame"/&gt; 
    &lt;param name="range_min" value="0.45"/&gt; 
    &lt;remap from="image" to="/camera/depth/image_raw"/&gt; 
    &lt;remap from="scan" to="/scan"/&gt; 
  &lt;/node&gt; </pre>
<p>The topic of the depth image can be changed in each sensor; you have to change the topic name according to your depth image topic.</p>
<p>As well as starting the nodelet, we need to set certain parameters of the nodelet for better conversion. Refer to <a href="http://wiki.ros.org/depthimage_to_laserscan"><span class="URLPACKT">http://wiki.ros.org/depthimage_to_laserscan</span></a> for a detailed explanation of each parameter.</p>
<p>The laser scan of the preceding view is given in the following screenshot. To view the laser scan, add the <span class="packt_screen">LaserScan</span> option. This is similar to how we add the <span class="packt_screen">PointCloud2</span> option and change the <span class="packt_screen">Topic</span> value of <span class="packt_screen">LaserSan</span> to <span class="packt_screen">/scan</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/c4b6ceac-d6d3-43eb-a6a8-91d112b6cf71.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Visualizing laser scan data in Rviz</div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Working with SLAM using ROS and Kinect</h1>
                </header>
            
            <article>
                
<p>The main aim of deploying vision sensors in our robot is to detect objects and navigate the robot through an environment. SLAM is a algorithm that is used in mobile robots to build up a map of an unknown environment or update a map within a known environment by tracking the current location of the robot.</p>
<p>Maps are used to plan the robot's trajectory and to navigate through this path. Using maps, the robot will get an idea about the environment. The two main challenges in mobile robot navigation are mapping and localization.</p>
<p>Mapping involves generating a profile of obstacles around the robot. Through mapping, the robot will understand what the world looks like. Localization is the process of estimating the position of the robot relative to the map we build.</p>
<p>SLAM fetches data from different sensors and uses it to build maps. The 2D/3D vision sensor can be used to input data into SLAM. 2D vision sensors, such as web cameras, and 3D sensors, such as Kinect, are mainly used as inputs for the SLAM algorithm.</p>
<p>A SLAM library called OpenSlam (<a href="http://openslam.org/gmapping.html"><span class="URLPACKT">http://openslam.org/gmapping.html</span></a>) is integrated with ROS as a package called gmapping. The <kbd>gmapping</kbd> package provides a node to perform laser-based SLAM processing, called <kbd>slam_gmapping</kbd>. This can create a 2D map from the laser and position data collected by the mobile robot.</p>
<p>The <kbd>gmapping</kbd> package is available at <a href="http://wiki.ros.org/gmapping"><span class="URLPACKT">http://wiki.ros.org/gmapping</span></a>.</p>
<p>To use the <kbd>slam_gmapping</kbd> node, we have to input the odometry data of the robot and the laser scan output from the laser range finder, which is mounted horizontally on the robot.</p>
<p>The <kbd>slam_gmapping</kbd> node subscribes to the <kbd>sensor_msgs/LaserScan</kbd> messages and <kbd>nav_msgs/Odometry</kbd> messages to build the map (<kbd>nav_msgs/OccupancyGrid</kbd>). The generated map can be retrieved via a ROS topic or service.</p>
<p>We have used the following launch file to use SLAM in our Chefbot. This launch file launches the <kbd>slam_gmapping</kbd> node and contains the necessary parameters to start mapping the robot's environment:</p>
<pre>    <strong>$ roslaunch chefbot_gazebo gmapping_demo.launch</strong>  </pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at the various vision sensors that can be used in Chefbot. We used Kinect and Astra in our robot and learned about OpenCV, OpenNI, PCL, and their application. We also discussed the role of vision sensors in robot navigation, the popular SLAM technique, and its application using ROS. In the next chapter, we will see the complete interfacing of the robot and learn how to perform autonomous navigation with our Chefbot.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What are 3D sensors and how are they different from ordinary cameras?</li>
<li>What are the main features of ROS?</li>
<li>What are the applications of OpenCV, OpenNI, and PCL?</li>
<li>What is SLAM?</li>
<li>What is RGB-D SLAM and how does it work?</li>
</ol>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>You can read more about the robotic vision package in ROS at the following links:</p>
<ul>
<li><a href="http://wiki.ros.org/vision_opencv">http://wiki.ros.org/vision_opencv</a></li>
<li><a href="http://wiki.ros.org/pcl">http://wiki.ros.org/pcl</a></li>
</ul>


            </article>

            
        </section>
    </div>
</body>
</html>