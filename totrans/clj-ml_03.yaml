- en: Chapter 3. Categorizing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore **classification**, which is yet another interesting
    problem in supervised learning. We will examine a handful of techniques for classifying
    data and also study how we can leverage these techniques in Clojure.
  prefs: []
  type: TYPE_NORMAL
- en: Classification can be defined as the problem of identifying the category or
    class of the observed data based on some empirical training data. The training
    data will have the values of the observable features or independent variables.
    It will also have a known category for these observed values. Classification is
    similar to regression in the sense that we predict a value based on another set
    of values. However, for classification, we are interested in the category of the
    observed values rather than predicting a value based on the given set of values.
    For example, if we train a linear regression model from a set of output values
    ranging from *0* to *5*, the trained classifier could predict the output value
    as *10* or *-1* for a set of input values. In classification, however, the predicted
    value of the output variable always belongs to a discrete set of values.
  prefs: []
  type: TYPE_NORMAL
- en: The independent variables of a classification model are also termed as the *explanatory
    variables* of the model, and the dependent variable is also called the *outcome*,
    *category*, or *class* of the observed values. The outcome of a classification
    model is always a discrete value, that is, a value from a predetermined set of
    values. This is one of the primary differences between classification and regression,
    as we predict a variable that can have a continuous range of values in regression
    modeling. Note that the terms "category" and "class" are used interchangeably
    in the context of classification.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms that implement classification techniques are called classifiers.
    A **classifier** can be formally defined as a function that maps a set of values
    to a category or class. Classification is still an active area of research in
    computer science, and there are several prominent classifier algorithms that are
    used in software today. There are several practical applications of classification,
    such as data mining, machine vision, speech and handwriting recognition, biological
    classification, and geostatistics.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the binary and multiclass classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will first study some theoretical aspects about data classification. As with
    other supervised machine learning techniques, the goal is to estimate a model
    or classifier from the sample data and use it to predict a given set of outcomes.
    Classification can be thought of as a way of determining a function that maps
    the features of the sample data to a specific class. The predicted class is selected
    from a given set of predetermined classes. Thus, similar to regression, the problem
    of classifying the observed values for the given independent variables is analogous
    to determining a best-fit function for the given training data.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, we might be interested in only a single class, that is, whether
    the observed values belong to a specific class. This form of classification is
    termed as **binary classification**, and the output variable of the model can
    have either the value *0* or *1*. Thus, we can say that ![Understanding the binary
    and multiclass classification](img/4351OS_03_01.jpg), where *y* is the outcome
    or dependent variable of the classification model. The outcome is said to be negative
    when ![Understanding the binary and multiclass classification](img/4351OS_03_03.jpg),
    and conversely, the outcome is termed positive when ![Understanding the binary
    and multiclass classification](img/4351OS_03_04.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this perspective, when some observed values for the independent variables
    of the model are provided, we must be able to determine the probability of a positive
    outcome. Thus, the estimated model of the given sample data has the probability
    ![Understanding the binary and multiclass classification](img/4351OS_03_05.jpg)
    and can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, the parameters ![Understanding the binary and multiclass
    classification](img/4351OS_03_07.jpg) represent the independent variables of the
    estimated classification model ![Understanding the binary and multiclass classification](img/4351OS_03_08.jpg),
    and the term ![Understanding the binary and multiclass classification](img/4351OS_03_09.jpg)
    represents the estimated parameter vector of this model.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of this kind of classification is deciding whether a new e-mail
    is spam or not, depending on the sender or the content within the e-mail. Another
    simple example of binary classification is determining the possibility of rainfall
    on a particular day depending on the observed humidity and the minimum and maximum
    temperatures on that day. The training data for this example might appear similar
    to the data in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A mathematical function that we can use to model binary classification is the
    **sigmoid** or **logistic** function. If the outcome for the feature *X* has an
    estimated parameter vector ![Understanding the binary and multiclass classification](img/4351OS_03_12.jpg),
    we can define the estimated probability of the positive outcome *Y* (as a sigmoid
    function) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To visualize the preceding equation, we can simplify it by substituting *Z*
    as ![Understanding the binary and multiclass classification](img/4351OS_03_14.jpg)
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We could model the data using several other functions as well. However, the
    sample data for a binary classifier can be easily transformed such that it can
    be modeled using a sigmoid function. This modeling of a classification problem
    using the logistic function is called **logistic regression**. The simplified
    sigmoid function defined in the preceding equation produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that if the term *Z* has a negative value, the plot appears reversed and
    is a mirror of the previous plot. We can visualize how the sigmoid function varies
    with respect to the term *Z*, through the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the previous graph, the sigmoid function is shown for different values of
    the term ![Understanding the binary and multiclass classification](img/4351OS_03_20.jpg);
    it ranges from *-5* to *5*. Note that for two dimensions, the term ![Understanding
    the binary and multiclass classification](img/4351OS_03_20.jpg) is a linear function
    of the independent variable *x*. Interestingly, for ![Understanding the binary
    and multiclass classification](img/4351OS_03_21.jpg) and ![Understanding the binary
    and multiclass classification](img/4351OS_03_22.jpg), the sigmoid function looks
    more or less like a straight line. This function reduces to a straight line when
    ![Understanding the binary and multiclass classification](img/4351OS_03_23.jpg)
    and can be represented by a constant *y* value (the equation ![Understanding the
    binary and multiclass classification](img/4351OS_03_24.jpg) in this case).
  prefs: []
  type: TYPE_NORMAL
- en: We observe that the estimated outcome *Y* is always between *0* and *1*, as
    it represents the probability of a positive outcome for the given observed values.
    Also, this range of the outcome *Y* is not affected by the sign of the term ![Understanding
    the binary and multiclass classification](img/4351OS_03_25.jpg). Thus, in retrospect,
    the sigmoid function is an effective representation of binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the logistic function to estimate a classification model from the training
    data, we can define the cost function of a logistic regression model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation essentially sums up the differences between the actual
    and predicted values of the output variables in our model, just like linear regression.
    However, as we are dealing with probability values between *0* and *1*, we use
    the preceding log function to effectively measure the differences between the
    actual and predicted output values. Note that the term *N* denotes the number
    of samples in the training data. We can apply the gradient descent to this cost
    function to determine the local minimum or rather the predicted class of a set
    of observed values. This equation can be regularized to produce the following
    cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that in this equation, the second summation term is added as a regularization
    term, like we discussed in [Chapter 2](ch02.html "Chapter 2. Understanding Linear
    Regression"), *Understanding Linear Regression*. This term basically prevents
    the *underfitting* and *overfitting* of the estimated model over the sample data.
    Note that the term ![Understanding the binary and multiclass classification](img/4351OS_03_28.jpg)
    is the regularization parameter and has to be appropriately selected depending
    on how accurate we want the model to be.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiclass classification**, which is the other form of classification, predicts
    the outcome of the classification as a value from a specific set of predetermined
    values. Thus, the outcome is selected from *k* discrete values, that is, ![Understanding
    the binary and multiclass classification](img/4351OS_03_29.jpg). This model produces
    *k* probabilities for each possible class of the observed values. This brings
    us to the following formal definition of multiclass classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, in multiclass classification, we predict *k* distinct values, in which
    each value indicates the probability of the input values belonging to a particular
    class. Interestingly, binary classification can be reasoned as a specialization
    of multiclass classification in which there are only two possible classes, that
    is, ![Understanding the binary and multiclass classification](img/4351OS_03_31.jpg)
    and ![Understanding the binary and multiclass classification](img/4351OS_03_01.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'As a special case of multiclass classification, we can say that the class with
    the maximum probability is the outcome or simply, the predicted class of the given
    set of observed values. This specialization of multiclass classification is called
    **one-vs-all** classification. Here, a single class with the maximum (or minimum)
    probability of occurrence is determined from a given set of observed values instead
    of finding the probabilities of the occurrences of all the possible classes in
    our model. Thus, if we intend to predict a single class from a specific set of
    classes, we can define the outcome *C* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For example, let's assume that we want to determine the classification model
    for a fish packing plant. In this scenario, the fish are separated into two distinct
    classes. Let's say that we can categorize the fish either as a sea bass or salmon.
    We can create some training data for our model by selecting a sufficiently large
    sample of fish and analyzing their distributions over some selected features.
    Let's say that we've identified two features to categorize the data, namely, the
    length of the fish and the lightness of its skin.
  prefs: []
  type: TYPE_NORMAL
- en: 'The distribution of the first feature, that is, the length of the fish, can
    be visualized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, the distribution of the lightness of the skin of the fish from the
    sample data can be visualized through the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding graphs, we can say that only specifying the length of the
    fish is not enough information to determine its type. Thus, this feature has a
    smaller coefficient in the classification model. On the contrary, since the lightness
    of the skin of the fish plays a larger role in determining the type of the fish,
    this feature will have a larger coefficient in the parameter vector of the estimated
    classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have modeled a given classification problem, we can partition the training
    data into two (or more) sets. The surface in the vector space that partitions
    these two sets is called the **decision boundary** of the formulated classification
    model. All the points on one side of the decision boundary are part of one class,
    while the points on the other side of the decision boundary are part of the other
    class. An obvious corollary is that depending on the number of distinct classes,
    a given classification model can have several such decision boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now combine these two features to train our model, and this produces
    an estimated decision boundary between the two categories of fish. This boundary
    can be visualized over a scatter plot of the training data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding plot, we approximate the classification model by using a straight
    line, and hence, we effectively model the classification as a linear function.
    We can alternatively model our data as a polynomial function, as it would produce
    a more accurate classification model. Such a model produces a decision boundary
    that can be visualized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The decision boundary partitions the sample data into two dimensions as shown
    in the preceding graphs. The decision boundary will become more complex to visualize
    when the sample data has a higher number of features or dimensions. For example,
    for three features, the decision boundary will be a three-dimensional surface,
    as shown in the following plot. Note that the sample data points are not shown
    for the sake of clarity. Also, two of the plotted features are assumed to vary
    within the range ![Understanding the binary and multiclass classification](img/4351OS_03_41.jpg),
    and the third feature is assumed to vary within the range ![Understanding the
    binary and multiclass classification](img/4351OS_03_42.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding the Bayesian classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now explore the Bayesian techniques that are used to classify data.
    A **Bayes** classifier is essentially a probabilistic classifier that is built
    using the Bayes' theorem of conditional probability. A model based on the Bayes
    classification assumes that the sample data has strongly independent features.
    By *independent*, we mean that every feature of the model can vary independent
    of the other features in the model. In other words, the features of the model
    are mutually exclusive. Thus, a Bayes classifier assumes that the presence or
    absence of a particular feature is completely independent of the presence or absence
    of the other features of the classification model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The term ![Understanding the Bayesian classification](img/4351OS_03_45.jpg)
    is used to represent the probability of occurrence of the condition or the feature
    *A*. Its value is always a fractional value within the range of *0* and *1*, both
    inclusive. It can also be represented as a percentage valve. For example, the
    probability *0.5* is also written as *50%* or *50 percent*. Let''s assume that
    we want to find the probability of occurrence of a feature *A* or ![Understanding
    the Bayesian classification](img/4351OS_03_46.jpg), from a given number of samples.
    Thus, a higher value of ![Understanding the Bayesian classification](img/4351OS_03_46.jpg)
    indicates a higher chance of occurrence of the feature *A*. We can formally represent
    the probability ![Understanding the Bayesian classification](img/4351OS_03_46.jpg)
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Bayesian classification](img/4351OS_03_47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If *A* and *B* are two conditions or features in our classification model,
    then we use the term ![Understanding the Bayesian classification](img/4351OS_03_48.jpg)
    to represent the occurrence of *A* when *B* is known to have occurred. This value
    is called the **conditional probability** of *A* given *B*, and the term ![Understanding
    the Bayesian classification](img/4351OS_03_48.jpg) is also read as the probability
    of *A* given *B*. In the term ![Understanding the Bayesian classification](img/4351OS_03_48.jpg),
    *B* is also called the evidence of *A*. In conditional probability, the two events,
    *A* and *B*, may or may not be independent of each other. However, if *A* and
    *B* are indeed independent conditions, then the probability ![Understanding the
    Bayesian classification](img/4351OS_03_48.jpg) is equal to the product of the
    probabilities of the separate occurrences of *A* and *B*. We can express this
    axiom as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Bayesian classification](img/4351OS_03_49.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Bayes'' theorem describes a relation between the conditional probabilities,
    ![Understanding the Bayesian classification](img/4351OS_03_48.jpg) and ![Understanding
    the Bayesian classification](img/4351OS_03_50.jpg), and the probabilities, ![Understanding
    the Bayesian classification](img/4351OS_03_46.jpg) and ![Understanding the Bayesian
    classification](img/4351OS_03_51.jpg). It is formally expressed using the following
    equality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Bayesian classification](img/4351OS_03_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Of course, the probabilities, ![Understanding the Bayesian classification](img/4351OS_03_46.jpg)
    and ![Understanding the Bayesian classification](img/4351OS_03_51.jpg), must both
    be greater than *0* for the preceding relation to be true.
  prefs: []
  type: TYPE_NORMAL
- en: Let's revisit the classification example of the fish packaging plant that we
    described earlier. The problem is that we need to determine whether a fish is
    a sea bass or salmon depending on its physical features. We will now implement
    a solution to this problem using a Bayes classifier. Then, we will use Bayes'
    theorem to model our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that each category of fish has three independent and distinct
    features, namely, the lightness of its skin and its length and width. Hence, our
    training data will look like that in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Bayesian classification](img/4351OS_03_53.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For simplicity in implementation, let''s use the Clojure symbols to represent
    these features. We need to first generate the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, we define two functions, `make-sea-bass` and `make-salmon`, to create
    a set of symbols to represent the two categories of fish. We conveniently use
    the`:salmon` and `:sea-bass` keywords to represent these two categories. Similarly,
    we can also use Clojure keywords to enumerate the features of a fish. In this
    example, the lightness of skin is either `:light` or `:dark`, the length is either
    `:long` or `:short`, and the width is either `:fat` or `:thin`. Also, we define
    the `make-sample-fish` function to randomly create a fish that is represented
    by the set of features defined earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we define these two categories of fish such that the sea bass are
    mostly long and light in skin color, and the salmon are mostly fat and dark. Also,
    we generate more salmon than sea bass in the `make-sample-fish` function. We add
    this partiality in our data only to provide more illustrative results, and the
    reader is encouraged to experiment with a more realistic distribution of data.
    The *Iris* dataset, which is available in the Incanter library that we introduced
    in [Chapter 2](ch02.html "Chapter 2. Understanding Linear Regression"), *Understanding
    Linear Regression*, is an example of a real-world dataset that can be used to
    study classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will implement the following function to calculate the probability
    of a particular condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We essentially implement the basic definition of probability by the number of
    occurrences.
  prefs: []
  type: TYPE_NORMAL
- en: The `probability` function defined in the preceding code requires a single argument
    to represent the attribute or condition whose probability of occurrence we want
    to calculate. Also, the function accepts several optional arguments, such as the
    data to be used to calculate this value, which defaults to the `fish-training-data`
    sequence that we had defined earlier, and a category, which can be reasoned simply
    as another condition. The arguments, `category` and `attribute`, are in fact analogous
    to the conditions *A* and *B* in the ![Understanding the Bayesian classification](img/4351OS_03_48.jpg)
    probability. The `probability` function determines the total positive occurrences
    of the condition by filtering the training data using the `filter` function. It
    then determines the number of negative occurrences by calculating the difference
    between the positive and total number of values represented by `(count by-category)`,
    in the sample data. The function finally returns the ratio of the positive occurrences
    of the condition to the total number of occurrences in the given data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the `probability` function to tell us a bit about our training data
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, the probability that a salmon is dark in appearance
    is high, or specifically, `1204/1733`. The probabilities of a sea bass being dark
    and a salmon being light are also low when compared to the probabilities of a
    sea bass being light and a salmon being dark.
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume that our observed values for the features of a fish are that it
    is dark-skinned, long, and fat. Given these conditions, we need to classify the
    fish as either a sea bass or a salmon. In terms of probability, we need to determine
    the probability that a fish is a salmon or a sea bass given that the fish is dark,
    long, and fat. Formally, this probability is represented by the terms ![Understanding
    the Bayesian classification](img/4351OS_03_55.jpg) and ![Understanding the Bayesian
    classification](img/4351OS_03_56.jpg) for either category of fish. If we calculate
    these two probabilities, we can select the category with the highest of these
    two probabilities to determine the category of the fish.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Bayes'' theorem, we define the terms, ![Understanding the Bayesian classification](img/4351OS_03_55.jpg)
    and ![Understanding the Bayesian classification](img/4351OS_03_56.jpg), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Bayesian classification](img/4351OS_03_57.jpg)![Understanding
    the Bayesian classification](img/4351OS_03_58.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The terms, ![Understanding the Bayesian classification](img/4351OS_03_55.jpg)
    and ![Understanding the Bayesian classification](img/4351OS_03_59.jpg), might
    seem a bit confusing, but the difference between these two terms is the order
    of occurrence of the specified conditions. The term, ![Understanding the Bayesian
    classification](img/4351OS_03_60.jpg), represents the probability that a fish
    that is dark, long, and fat is a salmon, while the term, ![Understanding the Bayesian
    classification](img/4351OS_03_61.jpg), represents the probability that a salmon
    is a dark, long, and fat fish.
  prefs: []
  type: TYPE_NORMAL
- en: The ![Understanding the Bayesian classification](img/4351OS_03_62.jpg) probability
    can be calculated from the given training data as follows. As the three features
    of the fish are assumed to be mutually independent, the term, ![Understanding
    the Bayesian classification](img/4351OS_03_62.jpg), is simply the product of the
    probabilities of the occurrences of each individual feature. By mutually independent,
    we mean that the variance or distribution of these features does not depend on
    any of the other features of the classification model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The term, ![Understanding the Bayesian classification](img/4351OS_03_62.jpg),
    is also called the **evidence** of the given category, which is the category "salmon"
    in this case. We can express the ![Understanding the Bayesian classification](img/4351OS_03_62.jpg)
    probability as the product of the probabilities of the independent features of
    the model; this is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Bayesian classification](img/4351OS_03_63.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Interestingly, the terms, ![Understanding the Bayesian classification](img/4351OS_03_64.jpg),
    ![Understanding the Bayesian classification](img/4351OS_03_65.jpg), and ![Understanding
    the Bayesian classification](img/4351OS_03_66.jpg), can be easily calculated from
    the training data and the `probability` function, which we had implemented earlier.
    Similarly, we can find the probability that a fish is a salmon or ![Understanding
    the Bayesian classification](img/4351OS_03_67.jpg). Thus, the only term that's
    not accounted for in the definition of ![Understanding the Bayesian classification](img/4351OS_03_60.jpg)
    is the term, ![Understanding the Bayesian classification](img/4351OS_03_68.jpg).
    We can actually avoid calculating this term altogether using a simple trick in
    probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that a fish is dark, long, and fat, it can either be a salmon or a sea
    bass. The two probabilities of occurrence of either category of fish are both
    complementary, that is, they both account for all the possible conditions that
    could occur in our model. In other words, these two probabilities both add up
    to a probability of *1*. Thus, we can formally express the term, ![Understanding
    the Bayesian classification](img/4351OS_03_68.jpg), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Bayesian classification](img/4351OS_03_70.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Both terms on the right-hand side of the preceding equality can be determined
    from the training data, which is similar to the terms, ![Understanding the Bayesian
    classification](img/4351OS_03_67.jpg), ![Understanding the Bayesian classification](img/4351OS_03_71.jpg),
    and so on. Hence, we can calculate the ![Understanding the Bayesian classification](img/4351OS_03_60.jpg)
    probability directly from our training data. We express this probability through
    the following equality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Bayesian classification](img/4351OS_03_72.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s implement the preceding equality using the training data and the
    `probability` function, which we defined earlier. Firstly, the evidence of a fish
    being a salmon, given that it''s dark, long, and fat in appearance, can be expressed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To be explicit, we implement a function to calculate the probability of the
    term, ![Understanding the Bayesian classification](img/4351OS_03_73.jpg), from
    the given training data. The equality of the terms, ![Understanding the Bayesian
    classification](img/4351OS_03_62.jpg), ![Understanding the Bayesian classification](img/4351OS_03_64.jpg),
    ![Understanding the Bayesian classification](img/4351OS_03_65.jpg), and ![Understanding
    the Bayesian classification](img/4351OS_03_66.jpg) will be used as a base for
    this implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code, we determine the terms, ![Understanding the Bayesian
    classification](img/4351OS_03_67.jpg) and ![Understanding the Bayesian classification](img/4351OS_03_74.jpg),
    for all the attributes or conditions of *i* by using the `probability` function.
    Then, we multiply all these terms using a composition of the `apply` and `*` functions.
    Since all the calculated probabilities are ratios returned by the `probability`
    function, we cast the final ratio to a floating-point value using the `float`
    function . We can try out this function in the REPL as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As the REPL output indicates, 48.16 percent of all the fish in the training
    data are salmon with dark skin. Similarly, 23.96 percent of all the fish are dark
    and long salmon, and 69.32 percent of all the fish are salmon. The value returned
    by the `(evidence-of-salmon :dark :long)` call can be expressed as ![Understanding
    the Bayesian classification](img/4351OS_03_75.jpg), and similarly, ![Understanding
    the Bayesian classification](img/4351OS_03_76.jpg) is returned by `(evidence-of-salmon)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can define the `evidence-of-sea-bass` function that determines
    the evidence of occurrence of a sea bass given some observed features of the fish.
    As we are dealing with only two categories, ![Understanding the Bayesian classification](img/4351OS_03_77.jpg),
    we can easily verify this equality in the REPL. Interestingly, a small error is
    observed, but this error is not related to the training data. This small error
    is, in fact, a floating-point rounding error, which arises due to the limitations
    of floating-point numbers. In practice, we can avoid this using the decimal or
    `BigDecimal` (from `java.lang`) data types, instead of floating-point numbers.
    We can verify this using the `evidence-of-sea-bass` and `evidence-of-salmon` functions
    in the REPL as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can generalize the `evidence-of-salmon` and `evidence-of-sea-bass` functions
    such that we are able to determine the probability of any category with some observed
    features; this is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The function defined in the preceding code returns values that agree with those
    returned by the following `evidence-of-salmon` and `evidence-of-sea-bass` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `evidence-of-salmon` and `evidence-of-sea-bass` functions, we can
    calculate the probability in terms of `probability-dark-long-fat-is-salmon` as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can inspect the `probability-dark-long-fat-is-salmon` value in the REPL
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `probability-dark-long-fat-is-salmon` value indicates that a fish that is
    dark, long, and fat and has a 95.7 percent probability of being a salmon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the preceding definition of the `probability-dark-long-fat-is-salmon`
    function as a template, we can generalize the calculations that it performs. Let''s
    first define a simple data structure that can be passed around. In the spirit
    of idiomatic Clojure, we conveniently use a map for this purpose. Using a map,
    we can represent a category in our model along with the evidence and probability
    of its occurrence. Also, given the evidences for several categories, we can calculate
    the total probability of occurrence of a particular category as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `make-category-probability-pair` function uses the `evidence-category-with-attrs`
    function we defined in the preceding code to calculate the evidence of a category
    and its conditions or attributes. Then, it returns this value, as a map, along
    with the category itself. Also, we define the `calculate-probability-of-category`
    function, which calculates the total probability of a category and its conditions
    using the `sum-of-evidences` parameter and a value returned by the `make-category-probability-pair`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can compose the preceding two functions to determine the total probability
    of all the categories given some observed values and then select the category
    with the highest probability, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `classify-by-attrs` function defined in the preceding code maps all the
    possible categories over the `make-category-probability-pair` function, given
    some conditions or observed values for the features of our model. As we are dealing
    with a sequence of pairs returned by `make-category-probability-pair`, we can
    use a simple composition of the `reduce`, `map`, and `+` functions to calculate
    the sum of all the evidence in this sequence. We then map the `calculate-probability-of-category`
    function over the sequence of category-evidence pairs and select the category-evidence
    pair with the highest probability. We do this by sorting the sequence through
    ascending probabilities and selecting the last element in the sorted sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can use the `classify-by-attrs` function to determine the probability
    that an observed fish, which is dark, long, and fat, is a salmon. It is also represented
    by the `probability-dark-long-fat-is-salmon` value, which we defined earlier.
    Both expressions produce the same probability of 95.7 percent of a fish being
    a salmon, given that it''s dark, long, and fat in appearance. We will implement
    the `classify-by-attrs` function as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `classify-by-attrs` function also returns the predicted category (that
    is, `:salmon`) of the given observed conditions `:dark`, :`long`, and `:fat`.
    We can use this function to tell us more about the training data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding code, a fish that is dark in appearance is mostly
    a salmon, and the one that is light in appearance is mostly a sea bass. Also,
    a fish that''s thin is most likely a sea bass. The following values do, in fact,
    agree with the training data that we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that calling the `classify-by-attrs` function with only `[:salmon]` as
    a parameter returns the probability that any given fish is a salmon. An obvious
    corollary is that given a single category, the `classify-by-attrs` function always
    predicts the supplied category with complete certainty, that is, a probability
    of *1.0*. However, the evidence returned by this function vary depending on the
    observed features passed to it as well as the sample data that we used to train
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, the preceding implementation describes a Bayes classifier that
    can be trained using some sample data. It also classifies some observed values
    for the features of our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can describe a generic Bayes classifier by building upon the definition
    of the ![Understanding the Bayesian classification](img/4351OS_03_60.jpg) probability
    from our previous example. To quickly recap, the term ![Understanding the Bayesian
    classification](img/4351OS_03_60.jpg) can be formally expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Bayesian classification](img/4351OS_03_72.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equality, we deal with a single class, namely salmon, and
    three mutually independent features, namely the length, width, and lightness of
    the skin of a fish. We can generalize this equality for *N* features as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Bayesian classification](img/4351OS_03_78.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the term *Z* is the evidence of the classification model, which we described
    in the preceding equation. We can use the sum and product notation to describe
    the preceding equality more concisely as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Bayesian classification](img/4351OS_03_79.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equality describes the probability of occurrence of a single
    class, *C*. If we are given a number of classes to choose from, we must select
    the class with the highest probability of occurrence. This brings us to the basic
    definition of a Bayes classifier, which is formally expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Bayesian classification](img/4351OS_03_80.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, the function ![Understanding the Bayesian classification](img/4351OS_03_81.jpg)
    describes a Bayes classifier that selects the class with the highest probability
    of occurrence. Note that the terms ![Understanding the Bayesian classification](img/4351OS_03_82.jpg)
    represent the various features of our classification model, whereas the terms
    ![Understanding the Bayesian classification](img/4351OS_03_83.jpg) represent the
    set of observed values of these features. Also, the variable *c*, on the right-hand
    side of the equation, can have values from within a set of all the distinct classes
    in the classification model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can further simplify the preceding equation of a Bayes classifier via the
    **Maximum a Posteriori** (**MAP**) estimation, which can be thought of as a regularization
    of the features in Bayesian statistics. A simplified Bayes classifier can be formally
    expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Bayesian classification](img/4351OS_03_84.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This definition essentially means that the *classify* function determines the
    class with the maximum probability of occurrence for the given features. Thus,
    the preceding equation describes a Bayes classifier that can be trained using
    some sample data and then be used to predict the class of a given set of observed
    values. We will now focus on using an existing implementation of a Bayes classifier
    to model a given classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: The `clj-ml` library ([https://github.com/joshuaeckroth/clj-ml](https://github.com/joshuaeckroth/clj-ml))
    contains several implemented algorithms that we can choose from to model a given
    classification problem. This library is actually just a Clojure wrapper for the
    popular **Weka** library ([http://www.cs.waikato.ac.nz/ml/weka/](http://www.cs.waikato.ac.nz/ml/weka/)),
    which is a Java library that contains implementations for several machine learning
    algorithms. It also has several methods for evaluating and validating a generated
    classification model. However, we will concentrate on the `clj-ml` library's implementations
    of classifiers within the context of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `clj-ml` library can be added to a Leiningen project by adding the following
    dependency to the `project.clj` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'For the upcoming example, the namespace declaration should look similar to
    the following declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll now introduce the `clj-ml` library using an implementation of a Bayes
    classifier to model our previous problem involving the fish packaging plant. First,
    let''s refine our training data to use numeric values rather than the keywords,
    which we described earlier, for the various features of our model. Of course,
    we will still maintain the partiality in our training data such that the salmon
    are mostly fat and dark-skinned, while the sea bass are mostly long and light-skinned.
    The following code implements this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, we define the `rand-in-range` function, which simply generates random
    integers within a given range of values. We then redefine the `make-sea-bass`
    and `make-salmon` functions to use the `rand-in-range` function to generate values
    within the range of `0` and `10` for the three features of a fish, namely its
    length, width, and the darkness of its skin. A fish with a lighter skin color
    is indicated by a higher value for this feature. Note that we reuse the definitions
    of the `make-sample-fish` function and `fish-dataset` variable to generate our
    training data. Also, a fish is represented by a vector rather than a set, as described
    in the earlier definitions of the `make-sea-bass` and `make-salmon` functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a classifier from the `clj-ml` library using the `make-classifier`
    function, which can be found in the `clj-ml.classifiers` namespace. We can specify
    the type of classifier to be used by passing two keywords as arguments to the
    functions. As we intend to use a Bayes classifier, we supply the keywords, `:bayes`
    and `:naive`, to the `make-classifier` function. In a nutshell, we can use the
    following declaration to create a Bayes classifier. Note that the keyword, `:naive`,
    used in the following code signifies a naïve Bayes classifier that assumes that
    the features in our model are independent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `clj-ml` library''s classifier implementations use datasets that are defined
    or generated using functions from the `clj-ml.data` namespace. We can convert
    the `fish-dataset` sequence, which is a sequence of vectors, into such a dataset
    using the `make-dataset` function. This function requires an arbitrary string
    name for the dataset, a template for each item in the collection, and a collection
    of items to add to the dataset. The template supplied to the `make-dataset` function
    is easily represented by a map, which is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `fish-template` map defined in the preceding code simply says that a fish,
    as represented by a vector, comprises the fish's category, length, width, and
    lightness of its skin, in that specific order. Note that the category of the fish
    is described using either `:salmon` or `:sea-bass`. We can now use `fish-dataset`
    to train the classifier represented by the `bayes-classifier` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the `fish-template` map defines all the attributes of a fish, it''s
    still lacking one important detail. It doesn''t specify which of these attributes
    represent the class or category of the fish. In order to specify a particular
    attribute in the vector to represent the category of an entire set of observed
    values, we use the `dataset-set-class` function. This function takes a single
    argument that specifies the index of an attribute and is used to represent the
    category of the set of observed values in the vector. Note that this function
    does actually mutate or modify the dataset it''s supplied with. We can then train
    our classifier using the `classifier-train` function, which takes a classifier
    and a dataset as parameters; this is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding `train-bayes-classifier` function simply calls the `dataset-set-class`
    and `classifier-train` functions to train our classifier. When we call the `train-bayes-classifier`
    function, the classifier is trained with the following supplied data and then
    printed to the REPL output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This output gives us some basic information about the training data, such as
    the mean and standard deviation of the various features that we model. We can
    now use this trained classifier to predict the category of a set of observed values
    for the features of our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first define the observed values that we intend to classify. To do so,
    we use the following `make-instance` function, which requires a dataset and a
    vector of observed values that agree with the data template of the supplied dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we simply defined a sample fish using the `make-instance` function. We
    can now predict the class of the fish represented by `sample-fish` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, the fish is classified as a `salmon`. Note that
    although we provide the class of the fish as `:salmon` while defining `sample-fish`,
    it's only for conformance with the data template defined by `fish-dataset`. In
    fact, we could specify the class of `sample-fish` as `:sea-bass` or a third value,
    say `:unknown`, to represent an undefined value, and the classifier would still
    classify `sample-fish` as a `salmon`.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with the continuous values for the various features of the given
    classification model, we can specify a Bayes classifier to use discretization
    of continuous features. By this, we mean that all the values for the various features
    of the model will be converted to discrete values by the probability density estimation.
    We can specify this option to the `make-classifier` function by simply passing
    an extra argument, `{:supervised-discretization true}`, to the function. This
    map actually describes all the possible options that can be provided to the specified
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, the `clj-ml` library provides a fully operational Bayes classifier
    that we can use to classify arbitrary data. Although we generated the training
    data ourselves in the previous example, this data can be fetched from the Web
    or a database as well.
  prefs: []
  type: TYPE_NORMAL
- en: Using the k-nearest neighbors algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A simple technique that can be used to classify a set of observed values is
    the **k-nearest neighbors** (abbreviated as **k-NN**) algorithm. This algorithm
    is a form of **lazy learning** in which all the computation is deferred until
    classification. Also, in the classification phase, the k-NN algorithm approximates
    the class of the observed values using only a few values from the training data,
    and the reading of other values is deferred until they are actually needed.
  prefs: []
  type: TYPE_NORMAL
- en: While we now explore the k-NN algorithm in the context of classification, it
    can be applied to regression as well by simply selecting the predicted value as
    the average of the nearest values of the dependent variable for a set of observed
    feature values. Interestingly, this technique of modeling regression is, in fact,
    a generalization of **linear interpolation** (for more information, refer to *An
    introduction to kernel and nearest-neighbor nonparametric regression*).
  prefs: []
  type: TYPE_NORMAL
- en: The k-NN algorithm reads some training data and analyzes this data lazily, that
    is, only when needed. Apart from the training data, the algorithm requires a set
    of observed values and a constant *k* as parameters to classify the set of observed
    values. To classify these observed values, the algorithm predicts the class that
    is the most frequent among the *k* training samples nearest to the set of observed
    values. By nearest, we mean a point with the least Euclidean distance from the
    point that is represented by a set of observed values in the Euclidean space of
    the training data.
  prefs: []
  type: TYPE_NORMAL
- en: An obvious corollary is that when ![Using the k-nearest neighbors algorithm](img/4351OS_03_85.jpg),
    the predicted class is the class of the single neighbor nearest to the set of
    observed values. This special case of the k-NN algorithm is called the **nearest
    neighbor** algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a classifier that uses the k-NN algorithm using the `clj-ml`
    library''s `make-classifier` function. Such a classifier is specified using the
    keywords `:lazy` and `:ibk` as arguments to the `make-classifier` function. We
    will now use such a classifier to model our previous example of a fish packaging
    plant, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code defines a k-NN classifier as `K1-classifier` and a `train-K1-classifier`
    function to train the classifier with the training data using `fish-dataset`,
    which we defined in the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the `make-classifier` function defaults the constant *k* or rather
    the number of neighbors to *1*, which implies a single nearest neighbor. We can
    optionally specify the constant *k* as a key-value pair with the`:num-neighbors`
    key to the `make-classifier` function as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now call the `train-K1-classifier` function to train the classifier
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use the `classifier-classify` function to classify the fish represented
    by `sample-fish`, which we had defined earlier, using the classifier represented
    by the `K1-classifier` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, the k-NN classifier predicts the fish class
    as salmon, thus agreeing with our earlier predictions that used a Bayes classifier.
    In conclusion, the `clj-ml` library provides a concise implementation of a classifier
    that uses the k-NN algorithm to predict the class of a set of observed values.
  prefs: []
  type: TYPE_NORMAL
- en: The k-NN classifier provided by the `clj-ml` library performs the normalization
    of the features of the classification model by default using the mean and standard
    deviation of the values of these features. We can specify an option to the `make-classifier`
    function to skip this normalization phase by passing a map entry with the`:no-normalization`
    key in the map of options passed to the `make-classifier` function.
  prefs: []
  type: TYPE_NORMAL
- en: Using decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can also use decision trees to model a given classification problem. A decision
    tree is, in fact, constructed from the given training data, and we can use this
    decision tree to predict the class of a given set of observed values. The process
    of constructing a decision tree is loosely based on the concepts of information
    entropy and information gain from information theory (for more information, refer
    to *Induction of Decision Trees*). It is also often termed as **decision tree
    learning**. Unfortunately, a detailed study of information theory is beyond the
    scope of this book. However, in this section, we will explore some concepts in
    information theory that will be used in the context of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree is a tree or graph that describes a model of decisions and their
    possible consequences. An internal node in a decision tree represents a decision,
    or rather a condition of a particular feature in the context of classification.
    It has two possible outcomes that are represented by the left and right subtrees
    of the node. Of course, a node in the decision tree could also have more than
    two subtrees. Each leaf node in a decision tree represents a particular class,
    or a consequence, in our classification model.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, our previous classification problem involving the fish packaging
    plant could have the following decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using decision trees](img/4351OS_03_86.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The previously illustrated decision tree uses two conditions to classify a fish
    as either a salmon or a sea bass. The internal nodes represent the two conditions
    based on the features of our classification model. Note that the decision tree
    uses only two of the three features of our classification model. We can thus say
    the tree is *pruned*. We shall briefly explore this technique as well in this
    section.
  prefs: []
  type: TYPE_NORMAL
- en: To classify a set of observed values using a decision tree, we traverse the
    tree starting from the root node until we reach a leaf node that represents the
    predicted class of the set of observed values. This technique of predicting the
    class of a set of observed values from a decision tree is always the same, irrespective
    of how the decision tree was constructed. For the decision tree described earlier,
    we can classify a fish by first comparing its length followed by the lightness
    of its skin. The second comparison is only needed if the length of the fish is
    greater than **6** as specified by the internal node with the expression **Length
    < 6** in the decision tree. If the length of the fish is indeed greater than **6**,
    we use the lightness of the skin of the fish to decide whether it's a salmon or
    a sea bass.
  prefs: []
  type: TYPE_NORMAL
- en: There are actually several algorithms that are used to construct a decision
    tree from some training data. Generally, the tree is constructed by splitting
    the set of sample values in the training data into smaller subsets based on an
    attribute value test. The process is repeated on each subset until splitting a
    given subset of sample values no longer adds internal nodes to the decision tree.
    As we mentioned earlier, it's possible for an internal node in a decision tree
    to have more than two subtrees.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now explore the **C4.5** algorithm to construct a decision tree (for
    more information, refer to *C4.5: Programs for Machine Learning*). This algorithm
    uses the concept of information entropy to decide the feature and the corresponding
    value on which the set of sample values must be partitioned. **Information entropy**
    is defined as the measure of uncertainty in a given feature or random variable
    (for more information, refer to "A Mathematical Theory of Communication").'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a given feature or attribute *f*, which has values within the range of
    *1* to *m*, we can define the information entropy of the ![Using decision trees](img/4351OS_03_88.jpg)
    feature as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using decision trees](img/4351OS_03_89.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, the term ![Using decision trees](img/4351OS_03_90.jpg)
    represents the number of occurrences of the feature *f* with respect to the value
    *i*. Based on this definition of the information entropy of a feature, we define
    the normalized information gain ![Using decision trees](img/4351OS_03_91.jpg).
    In the following equality, the term *T* refers to the set of sample values or
    training data supplied to the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using decision trees](img/4351OS_03_92.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In terms of information entropy, the preceding definition of the information
    gain of a given attribute is the change in information entropy of the total set
    of values when the attribute *f* is removed from the given set of features in
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm selects a feature *A* from the given set of features in the training
    data such that the feature *A* has the maximum possible information gain in the
    set of features. We can represent this with the help of the following equality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using decision trees](img/4351OS_03_93.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, ![Using decision trees](img/4351OS_03_94.jpg) represents
    the set of all the possible values that a feature *a* is known to have. The ![Using
    decision trees](img/4351OS_03_95.jpg) set represents the observed values in which
    the feature *a* has the value *v* and the term ![Using decision trees](img/4351OS_03_96.jpg)
    represents the information entropy of this set of values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the preceding equation to select a feature with the maximum information
    gain from the training data, we can describe the C4.5 algorithm through the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: For each feature *a*, find the normalized information gain from partitioning
    the sample data on the feature *a*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the feature *A* with the maximum normalized information gain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an internal decision node based on the selected feature *A*. Both the
    subtrees created from this step are either leaf nodes or a new set of sample values
    to be partitioned further.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat this process on each partitioned set of sample values produced from the
    previous step. We repeat the preceding steps until all the features in a subset
    of sample values have the same information entropy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once a decision tree has been created, we can optionally perform **pruning**
    on the tree. Pruning is simply the process of removing any extraneous decision
    nodes from the tree. This can be thought of as a form for the regularization of
    decision trees through which we prevent underfitting or overfitting of the estimated
    decision tree model.
  prefs: []
  type: TYPE_NORMAL
- en: '**J48** is an open source implementation of the C4.5 algorithm in Java, and
    the `clj-ml` library contains a working J48 decision tree classifier. We can create
    a decision tree classifier using the `make-classifier` function, and we supply
    the keywords `:decision-tree` and `:c45` as parameters to this function to create
    a J48 classifier as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `train-DT-classifier` function defined in the preceding code simply trains
    the classifier represented by `DT-classifier` with the training data from our
    previous example of the fish packaging plant. The `classifier-train` function
    also prints the following trained classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output gives a good idea of what the decision tree of the trained
    classifier looks like as well as the size and number of leaf nodes in the decision
    tree. Apparently, the decision tree has three distinct internal nodes. The root
    node of the tree is based on the width of a fish, the subsequent node is based
    on the length of a fish, and the last decision node is based on the lightness
    of the skin of a fish.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now use the decision tree classifier to predict the class of a fish,
    and we use the following `classifier-classify` function to perform this classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, the classifier predicts the class of the fish
    represented by `sample-fish` as a `:salmon` keyword just like the other classifiers
    used in the earlier examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The J48 decision tree classifier implementation provided by the `clj-ml` library
    performs pruning as a final step while training the classifier. We can generate
    an unpruned tree by specifying the `:unpruned` key in the map of options passed
    to the `make-classifier` function as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The previously defined classifier will not perform pruning on the decision
    tree generated from training the classifier with the given training data. We can
    inspect what an unpruned tree looks like by defining and calling the `train-UDT-classifier`
    function, which simply trains the classifier using the `classifier-train` function
    with the `fish-dataset` training data. This function can be defined as being analogous
    to the `train-UDT-classifier` function and produces the following output when
    it is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding code, the unpruned decision tree has a lot more internal
    decision nodes as compared to the decision tree that is generated after pruning
    it. We can now use the following `classifier-classify` function to predict the
    class of a fish using the trained classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, the unpruned tree also predicts the class of the fish represented
    by `sample-fish` as `:salmon`, thus agreeing with the class predicted by the pruned
    decision tree, which we had described earlier. In summary, the `clj-ml` library
    provides us with a working implementation of a decision tree classifier based
    on the C4.5 algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The `make-classifier` function supports several interesting options for the
    J48 decision tree classifier. We've already explored the `:unpruned` option, which
    indicated that the decision tree is not pruned. We can specify the`:reduced-error-pruning`
    option to the `make-classifier` function to force the usage of reduced error pruning
    (for more information, refer to "Pessimistic decision tree pruning based on tree
    size"), which is a form of pruning based on reducing the overall error of the
    model. Another interesting option that we can specify to the `make-classifier`
    function is the maximum number of internal nodes or folds that can be removed
    by pruning the decision tree. We can specify this option using the `:pruning-number-of-folds`
    option, and by default, the `make-classifier` function imposes no such limit while
    pruning the decision tree. Also, we can specify that each internal decision node
    in the decision tree has only two subtrees by specifying the `:only-binary-splits`
    option to the `make-classifier` function.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we explored classification and the various algorithms that
    can be used to model a given classification problem. Although classification techniques
    are very useful, they do not perform too well when the sample data has a large
    number of dimensions. Also, the features may vary in a nonlinear manner, as we
    will describe in [Chapter 4](ch04.html "Chapter 4. Building Neural Networks"),
    *Building Neural Networks*. We will also explore more about these aspects and
    the alternate methods of supervised learning in the following chapters. The following
    are few of the points that we looked at in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: We described two broad types of classifications, namely, binary and multiclass
    classification. We also briefly studied the logistic function and how it can be
    used to model classification problems through logistic regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We studied and implemented a Bayes classifier, which used a probabilistic model
    used for modeling classification. We also described how we could use the `clj-ml`
    library's Bayes classifier implementation to model a given classification problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also explored the simple k-nearest neighbor algorithm and how we can leverage
    it using the `clj-ml` library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We studied decision trees and the C4.5 algorithm. The `clj-ml` library provides
    us with a configurable implementation of a classifier based on the C4.5 algorithm,
    and we described how this implementation could be used as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will explore artificial neural networks in the following chapter. Interestingly,
    we can use artificial neural networks to model regression and classification problems,
    and we will study these aspects of neural networks as well.
  prefs: []
  type: TYPE_NORMAL
