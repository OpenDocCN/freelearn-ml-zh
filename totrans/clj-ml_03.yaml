- en: Chapter 3. Categorizing Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章. 数据分类
- en: In this chapter, we will explore **classification**, which is yet another interesting
    problem in supervised learning. We will examine a handful of techniques for classifying
    data and also study how we can leverage these techniques in Clojure.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨**分类**，这是监督学习中的另一个有趣问题。我们将检查一些用于分类数据的技术，并研究我们如何在Clojure中利用这些技术。
- en: Classification can be defined as the problem of identifying the category or
    class of the observed data based on some empirical training data. The training
    data will have the values of the observable features or independent variables.
    It will also have a known category for these observed values. Classification is
    similar to regression in the sense that we predict a value based on another set
    of values. However, for classification, we are interested in the category of the
    observed values rather than predicting a value based on the given set of values.
    For example, if we train a linear regression model from a set of output values
    ranging from *0* to *5*, the trained classifier could predict the output value
    as *10* or *-1* for a set of input values. In classification, however, the predicted
    value of the output variable always belongs to a discrete set of values.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 分类可以定义为根据一些经验训练数据识别观测数据的类别或类别的问题。训练数据将包含可观测特征或独立变量的值。它还将包含这些观测值的已知类别。在某种程度上，分类与回归相似，因为我们根据另一组值预测一个值。然而，对于分类，我们感兴趣的不仅仅是观测值的类别，而是基于给定的值集预测一个值。例如，如果我们从一个输出值范围在*0*到*5*的集合中训练一个线性回归模型，训练好的分类器可以预测一组输入值的输出值为*10*或*-1*。然而，在分类中，输出变量的预测值始终属于一组离散的值。
- en: The independent variables of a classification model are also termed as the *explanatory
    variables* of the model, and the dependent variable is also called the *outcome*,
    *category*, or *class* of the observed values. The outcome of a classification
    model is always a discrete value, that is, a value from a predetermined set of
    values. This is one of the primary differences between classification and regression,
    as we predict a variable that can have a continuous range of values in regression
    modeling. Note that the terms "category" and "class" are used interchangeably
    in the context of classification.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型的独立变量也被称为模型的*解释变量*，而因变量也被称为观测值的*结果*、*类别*或*类别*。分类模型的结果始终是离散值，即来自一组预定的值。这是分类与回归之间的一项主要区别，因为在回归建模中，我们预测的变量可以有一个连续的范围。请注意，在分类的上下文中，“类别”和“类别”这两个术语可以互换使用。
- en: Algorithms that implement classification techniques are called classifiers.
    A **classifier** can be formally defined as a function that maps a set of values
    to a category or class. Classification is still an active area of research in
    computer science, and there are several prominent classifier algorithms that are
    used in software today. There are several practical applications of classification,
    such as data mining, machine vision, speech and handwriting recognition, biological
    classification, and geostatistics.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 实现分类技术的算法被称为分类器。一个**分类器**可以正式定义为将一组值映射到类别或类别的函数。分类仍然是计算机科学中的一个活跃研究领域，今天在软件中使用了几个突出的分类器算法。分类有几个实际应用，例如数据挖掘、机器视觉、语音和手写识别、生物分类和地理统计学。
- en: Understanding the binary and multiclass classification
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解二分类和多分类
- en: We will first study some theoretical aspects about data classification. As with
    other supervised machine learning techniques, the goal is to estimate a model
    or classifier from the sample data and use it to predict a given set of outcomes.
    Classification can be thought of as a way of determining a function that maps
    the features of the sample data to a specific class. The predicted class is selected
    from a given set of predetermined classes. Thus, similar to regression, the problem
    of classifying the observed values for the given independent variables is analogous
    to determining a best-fit function for the given training data.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将研究一些关于数据分类的理论方面。与其他监督机器学习技术一样，目标是根据样本数据估计一个模型或分类器，并使用它来预测一组给定的结果。分类可以被视为确定一个函数，该函数将样本数据的特征映射到特定的类别。预测的类别是从一组预定的类别中选择出来的。因此，与回归类似，对给定独立变量的观测值进行分类的问题与确定给定训练数据的最优拟合函数是相似的。
- en: In some cases, we might be interested in only a single class, that is, whether
    the observed values belong to a specific class. This form of classification is
    termed as **binary classification**, and the output variable of the model can
    have either the value *0* or *1*. Thus, we can say that ![Understanding the binary
    and multiclass classification](img/4351OS_03_01.jpg), where *y* is the outcome
    or dependent variable of the classification model. The outcome is said to be negative
    when ![Understanding the binary and multiclass classification](img/4351OS_03_03.jpg),
    and conversely, the outcome is termed positive when ![Understanding the binary
    and multiclass classification](img/4351OS_03_04.jpg).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可能只对单个类别感兴趣，即观测值是否属于特定类别。这种分类形式被称为**二进制分类**，模型的输出变量可以是*0*或*1*的值。因此，我们可以说![理解二进制和多类分类](img/4351OS_03_01.jpg)，其中*y*是分类模型的输出或因变量。当![理解二进制和多类分类](img/4351OS_03_03.jpg)时，结果被认为是负的，反之，当![理解二进制和多类分类](img/4351OS_03_04.jpg)时，结果被认为是正的。
- en: 'In this perspective, when some observed values for the independent variables
    of the model are provided, we must be able to determine the probability of a positive
    outcome. Thus, the estimated model of the given sample data has the probability
    ![Understanding the binary and multiclass classification](img/4351OS_03_05.jpg)
    and can be expressed as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，当提供了模型独立变量的某些观测值时，我们必须能够确定正结果的概率。因此，给定样本数据的估计模型具有![理解二进制和多类分类](img/4351OS_03_05.jpg)的概率，可以表示如下：
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_06.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![理解二进制和多类分类](img/4351OS_03_06.jpg)'
- en: In the preceding equation, the parameters ![Understanding the binary and multiclass
    classification](img/4351OS_03_07.jpg) represent the independent variables of the
    estimated classification model ![Understanding the binary and multiclass classification](img/4351OS_03_08.jpg),
    and the term ![Understanding the binary and multiclass classification](img/4351OS_03_09.jpg)
    represents the estimated parameter vector of this model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，参数![理解二进制和多类分类](img/4351OS_03_07.jpg)代表估计分类模型![理解二进制和多类分类](img/4351OS_03_08.jpg)的独立变量，而项![理解二进制和多类分类](img/4351OS_03_09.jpg)代表该模型的估计参数向量。
- en: 'An example of this kind of classification is deciding whether a new e-mail
    is spam or not, depending on the sender or the content within the e-mail. Another
    simple example of binary classification is determining the possibility of rainfall
    on a particular day depending on the observed humidity and the minimum and maximum
    temperatures on that day. The training data for this example might appear similar
    to the data in the following table:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分类的一个例子是决定一封新邮件是否为垃圾邮件，这取决于发件人或邮件中的内容。另一个简单的二进制分类例子是根据某一天的观测湿度以及当天的最低和最高温度来确定该日降雨的可能性。这个例子中的训练数据可能类似于以下表格中的数据：
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_10.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![理解二进制和多类分类](img/4351OS_03_10.jpg)'
- en: 'A mathematical function that we can use to model binary classification is the
    **sigmoid** or **logistic** function. If the outcome for the feature *X* has an
    estimated parameter vector ![Understanding the binary and multiclass classification](img/4351OS_03_12.jpg),
    we can define the estimated probability of the positive outcome *Y* (as a sigmoid
    function) as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用来建模二进制分类的数学函数是**sigmoid**或**logistic**函数。如果特征*X*的输出有一个估计的参数向量![理解二进制和多类分类](img/4351OS_03_12.jpg)，我们可以定义正结果的估计概率*Y*（作为sigmoid函数）如下：
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_13.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![理解二进制和多类分类](img/4351OS_03_13.jpg)'
- en: 'To visualize the preceding equation, we can simplify it by substituting *Z*
    as ![Understanding the binary and multiclass classification](img/4351OS_03_14.jpg)
    as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化前面的方程，我们可以通过将*Z*替换为![理解二进制和多类分类](img/4351OS_03_14.jpg)来简化它，如下所示：
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_15.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![理解二进制和多类分类](img/4351OS_03_15.jpg)'
- en: 'We could model the data using several other functions as well. However, the
    sample data for a binary classifier can be easily transformed such that it can
    be modeled using a sigmoid function. This modeling of a classification problem
    using the logistic function is called **logistic regression**. The simplified
    sigmoid function defined in the preceding equation produces the following plot:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用其他几个函数来模拟数据。然而，二分类器的样本数据可以很容易地转换，使其可以用S型函数进行模拟。使用逻辑函数对分类问题进行建模的过程称为**逻辑回归**。前面方程中定义的简化S型函数产生了以下图表：
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_16.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![理解二分类和多分类](img/4351OS_03_16.jpg)'
- en: 'Note that if the term *Z* has a negative value, the plot appears reversed and
    is a mirror of the previous plot. We can visualize how the sigmoid function varies
    with respect to the term *Z*, through the following plot:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果术语*Z*具有负值，则图表会反转，并且是前一个图表的镜像。我们可以通过以下图表可视化S型函数相对于术语*Z*的变化：
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_18.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![理解二分类和多分类](img/4351OS_03_18.jpg)'
- en: In the previous graph, the sigmoid function is shown for different values of
    the term ![Understanding the binary and multiclass classification](img/4351OS_03_20.jpg);
    it ranges from *-5* to *5*. Note that for two dimensions, the term ![Understanding
    the binary and multiclass classification](img/4351OS_03_20.jpg) is a linear function
    of the independent variable *x*. Interestingly, for ![Understanding the binary
    and multiclass classification](img/4351OS_03_21.jpg) and ![Understanding the binary
    and multiclass classification](img/4351OS_03_22.jpg), the sigmoid function looks
    more or less like a straight line. This function reduces to a straight line when
    ![Understanding the binary and multiclass classification](img/4351OS_03_23.jpg)
    and can be represented by a constant *y* value (the equation ![Understanding the
    binary and multiclass classification](img/4351OS_03_24.jpg) in this case).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，展示了不同值下的S型函数![理解二分类和多分类](img/4351OS_03_20.jpg)；其范围从*-5*到*5*。请注意，对于二维情况，术语![理解二分类和多分类](img/4351OS_03_20.jpg)是独立变量*x*的线性函数。有趣的是，对于![理解二分类和多分类](img/4351OS_03_21.jpg)和![理解二分类和多分类](img/4351OS_03_22.jpg)，S型函数看起来或多或少像一条直线。当![理解二分类和多分类](img/4351OS_03_23.jpg)时，该函数简化为一条直线，可以用常数*y*值（在这种情况下，方程![理解二分类和多分类](img/4351OS_03_24.jpg)）表示。
- en: We observe that the estimated outcome *Y* is always between *0* and *1*, as
    it represents the probability of a positive outcome for the given observed values.
    Also, this range of the outcome *Y* is not affected by the sign of the term ![Understanding
    the binary and multiclass classification](img/4351OS_03_25.jpg). Thus, in retrospect,
    the sigmoid function is an effective representation of binary classification.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，估计的输出*Y*始终在*0*和*1*之间，因为它代表给定观察值的正结果概率。此外，输出*Y*的范围不受术语![理解二分类和多分类](img/4351OS_03_25.jpg)符号的影响。因此，回顾起来，S型函数是二分类的有效表示。
- en: 'To use the logistic function to estimate a classification model from the training
    data, we can define the cost function of a logistic regression model as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用逻辑函数从训练数据估计分类模型，我们可以将逻辑回归模型的成本函数定义为以下：
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_26.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![理解二分类和多分类](img/4351OS_03_26.jpg)'
- en: 'The preceding equation essentially sums up the differences between the actual
    and predicted values of the output variables in our model, just like linear regression.
    However, as we are dealing with probability values between *0* and *1*, we use
    the preceding log function to effectively measure the differences between the
    actual and predicted output values. Note that the term *N* denotes the number
    of samples in the training data. We can apply the gradient descent to this cost
    function to determine the local minimum or rather the predicted class of a set
    of observed values. This equation can be regularized to produce the following
    cost function:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方程本质上总结了模型输出变量实际值和预测值之间的差异，就像线性回归一样。然而，由于我们处理的是介于*0*和*1*之间的概率值，我们使用前面的对数函数来有效地衡量实际值和预测输出值之间的差异。请注意，术语*N*表示训练数据中的样本数量。我们可以将梯度下降应用于此成本函数，以确定一组观察值的局部最小值或预测类别。此方程可以正则化，产生以下成本函数：
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_27.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![理解二分类和多类分类](img/4351OS_03_27.jpg)'
- en: Note that in this equation, the second summation term is added as a regularization
    term, like we discussed in [Chapter 2](ch02.html "Chapter 2. Understanding Linear
    Regression"), *Understanding Linear Regression*. This term basically prevents
    the *underfitting* and *overfitting* of the estimated model over the sample data.
    Note that the term ![Understanding the binary and multiclass classification](img/4351OS_03_28.jpg)
    is the regularization parameter and has to be appropriately selected depending
    on how accurate we want the model to be.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个公式中，第二个求和项被添加为一个正则化项，就像我们在[第二章](ch02.html "第二章。理解线性回归")中讨论的那样，*理解线性回归*。这个项基本上防止了估计模型在样本数据上的欠拟合和过拟合。注意，项
    ![理解二分类和多类分类](img/4351OS_03_28.jpg) 是正则化参数，必须根据我们希望模型有多准确来适当选择。
- en: '**Multiclass classification**, which is the other form of classification, predicts
    the outcome of the classification as a value from a specific set of predetermined
    values. Thus, the outcome is selected from *k* discrete values, that is, ![Understanding
    the binary and multiclass classification](img/4351OS_03_29.jpg). This model produces
    *k* probabilities for each possible class of the observed values. This brings
    us to the following formal definition of multiclass classification:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**多类分类**，这是分类的另一种形式，将分类结果预测为特定预定义值集中的值。因此，结果是从 *k* 个离散值中选择，即 ![理解二分类和多类分类](img/4351OS_03_29.jpg)。该模型为每个可能的观察值类别产生
    *k* 个概率。这使我们得到了多类分类的以下正式定义：'
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_30.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![理解二分类和多类分类](img/4351OS_03_30.jpg)'
- en: Thus, in multiclass classification, we predict *k* distinct values, in which
    each value indicates the probability of the input values belonging to a particular
    class. Interestingly, binary classification can be reasoned as a specialization
    of multiclass classification in which there are only two possible classes, that
    is, ![Understanding the binary and multiclass classification](img/4351OS_03_31.jpg)
    and ![Understanding the binary and multiclass classification](img/4351OS_03_01.jpg).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在多类分类中，我们预测 *k* 个不同的值，其中每个值表示输入值属于特定类别的概率。有趣的是，二分类可以被视为多类分类的一种特殊情况，其中只有两个可能的类别，即
    ![理解二分类和多类分类](img/4351OS_03_31.jpg) 和 ![理解二分类和多类分类](img/4351OS_03_01.jpg)。
- en: 'As a special case of multiclass classification, we can say that the class with
    the maximum probability is the outcome or simply, the predicted class of the given
    set of observed values. This specialization of multiclass classification is called
    **one-vs-all** classification. Here, a single class with the maximum (or minimum)
    probability of occurrence is determined from a given set of observed values instead
    of finding the probabilities of the occurrences of all the possible classes in
    our model. Thus, if we intend to predict a single class from a specific set of
    classes, we can define the outcome *C* as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 作为多类分类的一个特殊情况，我们可以说，具有最大概率的类别是结果，或者简单地说，给定观察值集合的预测类别。这种多类分类的特殊化称为**一对多**分类。在这里，从给定的观察值集合中确定具有最大（或最小）发生概率的单个类别，而不是找到我们模型中所有可能类别的发生概率。因此，如果我们打算从特定类别集合中预测单个类别，我们可以定义结果
    *C* 如下：
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_32.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![理解二分类和多类分类](img/4351OS_03_32.jpg)'
- en: For example, let's assume that we want to determine the classification model
    for a fish packing plant. In this scenario, the fish are separated into two distinct
    classes. Let's say that we can categorize the fish either as a sea bass or salmon.
    We can create some training data for our model by selecting a sufficiently large
    sample of fish and analyzing their distributions over some selected features.
    Let's say that we've identified two features to categorize the data, namely, the
    length of the fish and the lightness of its skin.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想要确定一个鱼包装厂的分类模型。在这种情况下，鱼被分为两个不同的类别。比如说，我们可以将鱼分类为海鲈鱼或三文鱼。我们可以通过选择足够大的鱼样本并分析它们在某些选定特征上的分布来为我们的模型创建一些训练数据。比如说，我们已经确定了两个特征来分类数据，即鱼的长度和皮肤的亮度。
- en: 'The distribution of the first feature, that is, the length of the fish, can
    be visualized as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第一特征，即鱼的长度的分布可以如下可视化：
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_33.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![理解二分类和多分类](img/4351OS_03_33.jpg)'
- en: 'Similarly, the distribution of the lightness of the skin of the fish from the
    sample data can be visualized through the following plot:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，样本数据中鱼皮光亮度的分布可以通过以下图来可视化：
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_35.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![理解二分类和多分类](img/4351OS_03_35.jpg)'
- en: From the preceding graphs, we can say that only specifying the length of the
    fish is not enough information to determine its type. Thus, this feature has a
    smaller coefficient in the classification model. On the contrary, since the lightness
    of the skin of the fish plays a larger role in determining the type of the fish,
    this feature will have a larger coefficient in the parameter vector of the estimated
    classification model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，我们可以看出，仅指定鱼的长度的信息不足以确定其类型。因此，这个特征在分类模型中的系数较小。相反，由于鱼皮的光亮度在确定鱼类型方面起着更大的作用，这个特征将在估计的分类模型的参数向量中具有更大的系数。
- en: Once we have modeled a given classification problem, we can partition the training
    data into two (or more) sets. The surface in the vector space that partitions
    these two sets is called the **decision boundary** of the formulated classification
    model. All the points on one side of the decision boundary are part of one class,
    while the points on the other side of the decision boundary are part of the other
    class. An obvious corollary is that depending on the number of distinct classes,
    a given classification model can have several such decision boundaries.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们建模了一个给定的分类问题，我们可以将训练数据划分为两个（或更多）集合。在向量空间中分割这两个集合的表面被称为所制定分类模型的**决策边界**。决策边界一侧的所有点属于一个类别，而另一侧的点属于另一个类别。一个明显的推论是，根据不同类别的数量，一个给定的分类模型可以有几个这样的决策边界。
- en: 'We can now combine these two features to train our model, and this produces
    an estimated decision boundary between the two categories of fish. This boundary
    can be visualized over a scatter plot of the training data as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将这两个特征结合起来训练我们的模型，这会产生两个鱼类别之间的估计决策边界。这个边界可以在训练数据的散点图上如下可视化：
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_37.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![理解二分类和多分类](img/4351OS_03_37.jpg)'
- en: 'In the preceding plot, we approximate the classification model by using a straight
    line, and hence, we effectively model the classification as a linear function.
    We can alternatively model our data as a polynomial function, as it would produce
    a more accurate classification model. Such a model produces a decision boundary
    that can be visualized as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们通过使用直线来近似分类模型，因此，我们有效地将分类建模为线性函数。我们也可以将数据建模为多项式函数，因为它会产生更准确的分类模型。这样的模型产生的决策边界可以如下可视化：
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_39.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![理解二分类和多分类](img/4351OS_03_39.jpg)'
- en: The decision boundary partitions the sample data into two dimensions as shown
    in the preceding graphs. The decision boundary will become more complex to visualize
    when the sample data has a higher number of features or dimensions. For example,
    for three features, the decision boundary will be a three-dimensional surface,
    as shown in the following plot. Note that the sample data points are not shown
    for the sake of clarity. Also, two of the plotted features are assumed to vary
    within the range ![Understanding the binary and multiclass classification](img/4351OS_03_41.jpg),
    and the third feature is assumed to vary within the range ![Understanding the
    binary and multiclass classification](img/4351OS_03_42.jpg).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 决策边界将样本数据划分为两个维度，如图中所示。当样本数据具有更多特征或维度时，决策边界将变得更加复杂，难以可视化。例如，对于三个特征，决策边界将是一个三维表面，如图中所示。请注意，为了清晰起见，未显示样本数据点。此外，假设图中绘制的两个特征在![理解二分类和多分类](img/4351OS_03_41.jpg)范围内变化，第三个特征在![理解二分类和多分类](img/4351OS_03_42.jpg)范围内变化。
- en: '![Understanding the binary and multiclass classification](img/4351OS_03_43.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![理解二分类和多分类](img/4351OS_03_43.jpg)'
- en: Understanding the Bayesian classification
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解贝叶斯分类
- en: We will now explore the Bayesian techniques that are used to classify data.
    A **Bayes** classifier is essentially a probabilistic classifier that is built
    using the Bayes' theorem of conditional probability. A model based on the Bayes
    classification assumes that the sample data has strongly independent features.
    By *independent*, we mean that every feature of the model can vary independent
    of the other features in the model. In other words, the features of the model
    are mutually exclusive. Thus, a Bayes classifier assumes that the presence or
    absence of a particular feature is completely independent of the presence or absence
    of the other features of the classification model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探讨用于分类数据的贝叶斯技术。**贝叶斯** 分类器本质上是一种基于贝叶斯条件概率定理构建的概率分类器。基于贝叶斯分类的模型假设样本数据具有高度独立的特征。这里的“独立”意味着模型中的每个特征可以独立于模型中的其他特征变化。换句话说，模型的特征是互斥的。因此，贝叶斯分类器假设特定特征的呈现与否完全独立于分类模型中其他特征的呈现与否。
- en: 'The term ![Understanding the Bayesian classification](img/4351OS_03_45.jpg)
    is used to represent the probability of occurrence of the condition or the feature
    *A*. Its value is always a fractional value within the range of *0* and *1*, both
    inclusive. It can also be represented as a percentage valve. For example, the
    probability *0.5* is also written as *50%* or *50 percent*. Let''s assume that
    we want to find the probability of occurrence of a feature *A* or ![Understanding
    the Bayesian classification](img/4351OS_03_46.jpg), from a given number of samples.
    Thus, a higher value of ![Understanding the Bayesian classification](img/4351OS_03_46.jpg)
    indicates a higher chance of occurrence of the feature *A*. We can formally represent
    the probability ![Understanding the Bayesian classification](img/4351OS_03_46.jpg)
    as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 ![理解贝叶斯分类](img/4351OS_03_45.jpg) 用于表示条件或特征 *A* 发生的概率。其值始终是介于 *0* 和 *1* 之间的分数值，包括两端。它也可以表示为百分比值。例如，概率
    *0.5* 也可以写作 *50%* 或 *50 percent*。假设我们想要找到从给定样本中发生特征 *A* 或 ![理解贝叶斯分类](img/4351OS_03_46.jpg)
    的概率。因此，![理解贝叶斯分类](img/4351OS_03_46.jpg) 的值越高，特征 *A* 发生的可能性就越高。我们可以将概率 ![理解贝叶斯分类](img/4351OS_03_46.jpg)
    正式表示如下：
- en: '![Understanding the Bayesian classification](img/4351OS_03_47.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![理解贝叶斯分类](img/4351OS_03_47.jpg)'
- en: 'If *A* and *B* are two conditions or features in our classification model,
    then we use the term ![Understanding the Bayesian classification](img/4351OS_03_48.jpg)
    to represent the occurrence of *A* when *B* is known to have occurred. This value
    is called the **conditional probability** of *A* given *B*, and the term ![Understanding
    the Bayesian classification](img/4351OS_03_48.jpg) is also read as the probability
    of *A* given *B*. In the term ![Understanding the Bayesian classification](img/4351OS_03_48.jpg),
    *B* is also called the evidence of *A*. In conditional probability, the two events,
    *A* and *B*, may or may not be independent of each other. However, if *A* and
    *B* are indeed independent conditions, then the probability ![Understanding the
    Bayesian classification](img/4351OS_03_48.jpg) is equal to the product of the
    probabilities of the separate occurrences of *A* and *B*. We can express this
    axiom as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *A* 和 *B* 是我们分类模型中的两个条件或特征，那么我们使用术语 ![理解贝叶斯分类](img/4351OS_03_48.jpg) 来表示当已知
    *B* 已经发生时 *A* 的发生。这个值被称为 *A* 在 *B* 条件下的 **条件概率**，术语 ![理解贝叶斯分类](img/4351OS_03_48.jpg)
    也可以读作 *A* 在 *B* 条件下的概率。在术语 ![理解贝叶斯分类](img/4351OS_03_48.jpg) 中，*B* 也被称为 *A* 的证据。在条件概率中，两个事件
    *A* 和 *B* 可能相互独立，也可能不独立。然而，如果 *A* 和 *B* 确实是相互独立的条件，那么概率 ![理解贝叶斯分类](img/4351OS_03_48.jpg)
    等于 *A* 和 *B* 单独发生概率的乘积。我们可以将这个公理表达如下：
- en: '![Understanding the Bayesian classification](img/4351OS_03_49.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![理解贝叶斯分类](img/4351OS_03_49.jpg)'
- en: 'Bayes'' theorem describes a relation between the conditional probabilities,
    ![Understanding the Bayesian classification](img/4351OS_03_48.jpg) and ![Understanding
    the Bayesian classification](img/4351OS_03_50.jpg), and the probabilities, ![Understanding
    the Bayesian classification](img/4351OS_03_46.jpg) and ![Understanding the Bayesian
    classification](img/4351OS_03_51.jpg). It is formally expressed using the following
    equality:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理描述了条件概率![理解贝叶斯分类](img/4351OS_03_48.jpg)和![理解贝叶斯分类](img/4351OS_03_50.jpg)与概率![理解贝叶斯分类](img/4351OS_03_46.jpg)和![理解贝叶斯分类](img/4351OS_03_51.jpg)之间的关系。它使用以下等式正式表达：
- en: '![Understanding the Bayesian classification](img/4351OS_03_52.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![理解贝叶斯分类](img/4351OS_03_52.jpg)'
- en: Of course, the probabilities, ![Understanding the Bayesian classification](img/4351OS_03_46.jpg)
    and ![Understanding the Bayesian classification](img/4351OS_03_51.jpg), must both
    be greater than *0* for the preceding relation to be true.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，为了使前面的关系成立，概率![理解贝叶斯分类](img/4351OS_03_46.jpg)和![理解贝叶斯分类](img/4351OS_03_51.jpg)都必须大于**0**。
- en: Let's revisit the classification example of the fish packaging plant that we
    described earlier. The problem is that we need to determine whether a fish is
    a sea bass or salmon depending on its physical features. We will now implement
    a solution to this problem using a Bayes classifier. Then, we will use Bayes'
    theorem to model our data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新回顾一下我们之前描述的鱼包装厂的分类示例。问题是，我们需要根据鱼的外部特征来确定它是不是海鲈鱼还是三文鱼。现在，我们将使用贝叶斯分类器来实现这个问题的解决方案。然后，我们将使用贝叶斯定理来建模我们的数据。
- en: 'Let''s assume that each category of fish has three independent and distinct
    features, namely, the lightness of its skin and its length and width. Hence, our
    training data will look like that in the following table:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 假设每种鱼类都有三个独立且不同的特征，即皮肤的光泽度、长度和宽度。因此，我们的训练数据将类似于以下表格：
- en: '![Understanding the Bayesian classification](img/4351OS_03_53.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![理解贝叶斯分类](img/4351OS_03_53.jpg)'
- en: 'For simplicity in implementation, let''s use the Clojure symbols to represent
    these features. We need to first generate the following data:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化实现，让我们使用Clojure符号来表示这些特征。我们需要首先生成以下数据：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we define two functions, `make-sea-bass` and `make-salmon`, to create
    a set of symbols to represent the two categories of fish. We conveniently use
    the`:salmon` and `:sea-bass` keywords to represent these two categories. Similarly,
    we can also use Clojure keywords to enumerate the features of a fish. In this
    example, the lightness of skin is either `:light` or `:dark`, the length is either
    `:long` or `:short`, and the width is either `:fat` or `:thin`. Also, we define
    the `make-sample-fish` function to randomly create a fish that is represented
    by the set of features defined earlier.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了两个函数，`make-sea-bass`和`make-salmon`，以创建一组符号来表示两种鱼类的类别。我们方便地使用`:salmon`和`:sea-bass`关键字来表示这两个类别。同样，我们也可以使用Clojure关键字来枚举鱼的特征。在这个例子中，皮肤的光泽度是`:light`或`:dark`，长度是`:long`或`:short`，宽度是`:fat`或`:thin`。此外，我们定义了`make-sample-fish`函数来随机创建一个由之前定义的特征集表示的鱼。
- en: Note that we define these two categories of fish such that the sea bass are
    mostly long and light in skin color, and the salmon are mostly fat and dark. Also,
    we generate more salmon than sea bass in the `make-sample-fish` function. We add
    this partiality in our data only to provide more illustrative results, and the
    reader is encouraged to experiment with a more realistic distribution of data.
    The *Iris* dataset, which is available in the Incanter library that we introduced
    in [Chapter 2](ch02.html "Chapter 2. Understanding Linear Regression"), *Understanding
    Linear Regression*, is an example of a real-world dataset that can be used to
    study classification.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们定义这两种鱼类的类别，使得海鲈鱼通常较长且皮肤颜色较浅，而三文鱼通常较肥且颜色较深。此外，我们在`make-sample-fish`函数中生成了更多的三文鱼而不是海鲈鱼。我们只在数据中添加这种偏差，以提供更具说明性的结果，并鼓励读者尝试更真实的数据分布。在[第2章](ch02.html
    "第2章。理解线性回归")中介绍的Incanter库中可用的*Iris*数据集，是可用于研究分类的真实世界数据集的例子。
- en: 'Now, we will implement the following function to calculate the probability
    of a particular condition:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将实现以下函数来计算特定条件的概率：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We essentially implement the basic definition of probability by the number of
    occurrences.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上通过出现次数的基本定义来实现概率。
- en: The `probability` function defined in the preceding code requires a single argument
    to represent the attribute or condition whose probability of occurrence we want
    to calculate. Also, the function accepts several optional arguments, such as the
    data to be used to calculate this value, which defaults to the `fish-training-data`
    sequence that we had defined earlier, and a category, which can be reasoned simply
    as another condition. The arguments, `category` and `attribute`, are in fact analogous
    to the conditions *A* and *B* in the ![Understanding the Bayesian classification](img/4351OS_03_48.jpg)
    probability. The `probability` function determines the total positive occurrences
    of the condition by filtering the training data using the `filter` function. It
    then determines the number of negative occurrences by calculating the difference
    between the positive and total number of values represented by `(count by-category)`,
    in the sample data. The function finally returns the ratio of the positive occurrences
    of the condition to the total number of occurrences in the given data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一段代码中定义的`probability`函数需要一个参数来表示我们想要计算其发生概率的属性或条件。此外，该函数还接受几个可选参数，例如用于计算此值的`fish-training-data`序列，默认为我们之前定义的，以及一个类别，这可以简单地理解为另一个条件。参数`category`和`attribute`实际上与![理解贝叶斯分类](img/4351OS_03_48.jpg)中的条件*A*和*B*相对应。`probability`函数通过使用`filter`函数过滤训练数据来确定条件的总积极发生次数。然后，它通过计算样本数据中由`(count
    by-category)`表示的积极值和总值的差来确定消极发生次数。最后，该函数返回条件积极发生次数与给定数据中总发生次数的比率。
- en: 'Let''s use the `probability` function to tell us a bit about our training data
    as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`probability`函数来了解我们的训练数据如下：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As shown in the preceding code, the probability that a salmon is dark in appearance
    is high, or specifically, `1204/1733`. The probabilities of a sea bass being dark
    and a salmon being light are also low when compared to the probabilities of a
    sea bass being light and a salmon being dark.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，三文鱼外观为暗色的概率很高，具体为`1204/1733`。与海鲈鱼为暗色和三文鱼为亮色的概率相比，海鲈鱼为亮色和三文鱼为暗色的概率也较低。
- en: Let's assume that our observed values for the features of a fish are that it
    is dark-skinned, long, and fat. Given these conditions, we need to classify the
    fish as either a sea bass or a salmon. In terms of probability, we need to determine
    the probability that a fish is a salmon or a sea bass given that the fish is dark,
    long, and fat. Formally, this probability is represented by the terms ![Understanding
    the Bayesian classification](img/4351OS_03_55.jpg) and ![Understanding the Bayesian
    classification](img/4351OS_03_56.jpg) for either category of fish. If we calculate
    these two probabilities, we can select the category with the highest of these
    two probabilities to determine the category of the fish.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们对鱼的特征的观察值是它皮肤暗，长，且胖。在这些条件下，我们需要将鱼分类为海鲈鱼或三文鱼。从概率的角度来看，我们需要确定在鱼是暗、长、胖的情况下，鱼是三文鱼或海鲈鱼的概率。正式来说，这个概率由![理解贝叶斯分类](img/4351OS_03_55.jpg)和![理解贝叶斯分类](img/4351OS_03_56.jpg)这两个术语表示，针对鱼的任何一个类别。如果我们计算这两个概率，我们可以选择这两个概率中较高的类别来确定鱼的类别。
- en: 'Using Bayes'' theorem, we define the terms, ![Understanding the Bayesian classification](img/4351OS_03_55.jpg)
    and ![Understanding the Bayesian classification](img/4351OS_03_56.jpg), as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯定理，我们定义术语![理解贝叶斯分类](img/4351OS_03_55.jpg)和![理解贝叶斯分类](img/4351OS_03_56.jpg)如下：
- en: '![Understanding the Bayesian classification](img/4351OS_03_57.jpg)![Understanding
    the Bayesian classification](img/4351OS_03_58.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![理解贝叶斯分类](img/4351OS_03_57.jpg)![理解贝叶斯分类](img/4351OS_03_58.jpg)'
- en: The terms, ![Understanding the Bayesian classification](img/4351OS_03_55.jpg)
    and ![Understanding the Bayesian classification](img/4351OS_03_59.jpg), might
    seem a bit confusing, but the difference between these two terms is the order
    of occurrence of the specified conditions. The term, ![Understanding the Bayesian
    classification](img/4351OS_03_60.jpg), represents the probability that a fish
    that is dark, long, and fat is a salmon, while the term, ![Understanding the Bayesian
    classification](img/4351OS_03_61.jpg), represents the probability that a salmon
    is a dark, long, and fat fish.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 术语![理解贝叶斯分类](img/4351OS_03_55.jpg)和![理解贝叶斯分类](img/4351OS_03_59.jpg)可能有点令人困惑，但这两个术语之间的区别在于指定条件的出现顺序。术语![理解贝叶斯分类](img/4351OS_03_60.jpg)表示深色、长而胖的鱼是三文鱼的概率，而术语![理解贝叶斯分类](img/4351OS_03_61.jpg)表示三文鱼是深色、长而胖的鱼的概率。
- en: The ![Understanding the Bayesian classification](img/4351OS_03_62.jpg) probability
    can be calculated from the given training data as follows. As the three features
    of the fish are assumed to be mutually independent, the term, ![Understanding
    the Bayesian classification](img/4351OS_03_62.jpg), is simply the product of the
    probabilities of the occurrences of each individual feature. By mutually independent,
    we mean that the variance or distribution of these features does not depend on
    any of the other features of the classification model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![理解贝叶斯分类](img/4351OS_03_62.jpg)概率可以从给定的训练数据中计算如下。由于假设鱼的三个特征是相互独立的，因此术语![理解贝叶斯分类](img/4351OS_03_62.jpg)仅仅是每个单个特征发生概率的乘积。相互独立意味着这些特征的方差或分布不依赖于分类模型中的任何其他特征。'
- en: 'The term, ![Understanding the Bayesian classification](img/4351OS_03_62.jpg),
    is also called the **evidence** of the given category, which is the category "salmon"
    in this case. We can express the ![Understanding the Bayesian classification](img/4351OS_03_62.jpg)
    probability as the product of the probabilities of the independent features of
    the model; this is shown as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 术语![理解贝叶斯分类](img/4351OS_03_62.jpg)也称为给定类别的**证据**，在本例中是类别“三文鱼”。我们可以将![理解贝叶斯分类](img/4351OS_03_62.jpg)概率表示为模型独立特征的概率乘积；如下所示：
- en: '![Understanding the Bayesian classification](img/4351OS_03_63.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![理解贝叶斯分类](img/4351OS_03_63.jpg)'
- en: Interestingly, the terms, ![Understanding the Bayesian classification](img/4351OS_03_64.jpg),
    ![Understanding the Bayesian classification](img/4351OS_03_65.jpg), and ![Understanding
    the Bayesian classification](img/4351OS_03_66.jpg), can be easily calculated from
    the training data and the `probability` function, which we had implemented earlier.
    Similarly, we can find the probability that a fish is a salmon or ![Understanding
    the Bayesian classification](img/4351OS_03_67.jpg). Thus, the only term that's
    not accounted for in the definition of ![Understanding the Bayesian classification](img/4351OS_03_60.jpg)
    is the term, ![Understanding the Bayesian classification](img/4351OS_03_68.jpg).
    We can actually avoid calculating this term altogether using a simple trick in
    probability.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，术语![理解贝叶斯分类](img/4351OS_03_64.jpg)、![理解贝叶斯分类](img/4351OS_03_65.jpg)和![理解贝叶斯分类](img/4351OS_03_66.jpg)可以很容易地从训练数据和之前实现的`probability`函数中计算出来。同样，我们可以找到鱼是三文鱼或![理解贝叶斯分类](img/4351OS_03_67.jpg)的概率。因此，在![理解贝叶斯分类](img/4351OS_03_60.jpg)的定义中，唯一没有考虑到的术语是![理解贝叶斯分类](img/4351OS_03_68.jpg)。实际上，我们可以使用概率中的一个简单技巧来完全避免计算这个术语。
- en: 'Given that a fish is dark, long, and fat, it can either be a salmon or a sea
    bass. The two probabilities of occurrence of either category of fish are both
    complementary, that is, they both account for all the possible conditions that
    could occur in our model. In other words, these two probabilities both add up
    to a probability of *1*. Thus, we can formally express the term, ![Understanding
    the Bayesian classification](img/4351OS_03_68.jpg), as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于鱼是深色、长而胖的，它可能是三文鱼或海鲈鱼。这两种鱼类的发生概率都是互补的，也就是说，它们共同解释了我们模型中可能出现的所有条件。换句话说，这两个概率的总和为*1*。因此，我们可以正式地表达术语，![理解贝叶斯分类](img/4351OS_03_68.jpg)，如下所示：
- en: '![Understanding the Bayesian classification](img/4351OS_03_70.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![理解贝叶斯分类](img/4351OS_03_70.jpg)'
- en: 'Both terms on the right-hand side of the preceding equality can be determined
    from the training data, which is similar to the terms, ![Understanding the Bayesian
    classification](img/4351OS_03_67.jpg), ![Understanding the Bayesian classification](img/4351OS_03_71.jpg),
    and so on. Hence, we can calculate the ![Understanding the Bayesian classification](img/4351OS_03_60.jpg)
    probability directly from our training data. We express this probability through
    the following equality:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 上述等式右侧的两个术语都可以从训练数据中确定，这些术语类似于 ![理解贝叶斯分类](img/4351OS_03_67.jpg)、![理解贝叶斯分类](img/4351OS_03_71.jpg)
    以及等等。因此，我们可以直接从我们的训练数据中计算出 ![理解贝叶斯分类](img/4351OS_03_60.jpg) 概率。我们通过以下等式来表示这个概率：
- en: '![Understanding the Bayesian classification](img/4351OS_03_72.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![理解贝叶斯分类](img/4351OS_03_72.jpg)'
- en: 'Now, let''s implement the preceding equality using the training data and the
    `probability` function, which we defined earlier. Firstly, the evidence of a fish
    being a salmon, given that it''s dark, long, and fat in appearance, can be expressed
    as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用训练数据和之前定义的 `probability` 函数来实现前面的等式。首先，给定鱼的外观是深色、长和胖，鱼是鲑鱼的证据可以表示如下：
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To be explicit, we implement a function to calculate the probability of the
    term, ![Understanding the Bayesian classification](img/4351OS_03_73.jpg), from
    the given training data. The equality of the terms, ![Understanding the Bayesian
    classification](img/4351OS_03_62.jpg), ![Understanding the Bayesian classification](img/4351OS_03_64.jpg),
    ![Understanding the Bayesian classification](img/4351OS_03_65.jpg), and ![Understanding
    the Bayesian classification](img/4351OS_03_66.jpg) will be used as a base for
    this implementation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确起见，我们实现一个函数来计算从给定训练数据中术语 ![理解贝叶斯分类](img/4351OS_03_73.jpg) 的概率。术语 ![理解贝叶斯分类](img/4351OS_03_62.jpg)、![理解贝叶斯分类](img/4351OS_03_64.jpg)、![理解贝叶斯分类](img/4351OS_03_65.jpg)
    和 ![理解贝叶斯分类](img/4351OS_03_66.jpg) 的等式将作为此实现的基线。
- en: 'In the preceding code, we determine the terms, ![Understanding the Bayesian
    classification](img/4351OS_03_67.jpg) and ![Understanding the Bayesian classification](img/4351OS_03_74.jpg),
    for all the attributes or conditions of *i* by using the `probability` function.
    Then, we multiply all these terms using a composition of the `apply` and `*` functions.
    Since all the calculated probabilities are ratios returned by the `probability`
    function, we cast the final ratio to a floating-point value using the `float`
    function . We can try out this function in the REPL as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用 `probability` 函数确定 *i* 的所有属性或条件的术语 ![理解贝叶斯分类](img/4351OS_03_67.jpg)
    和 ![理解贝叶斯分类](img/4351OS_03_74.jpg)。然后，我们使用 `apply` 和 `*` 函数的组合乘以所有这些术语。由于所有计算出的概率都是
    `probability` 函数返回的比率，我们使用 `float` 函数将最终比率转换为浮点值。我们可以在 REPL 中尝试此函数如下：
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As the REPL output indicates, 48.16 percent of all the fish in the training
    data are salmon with dark skin. Similarly, 23.96 percent of all the fish are dark
    and long salmon, and 69.32 percent of all the fish are salmon. The value returned
    by the `(evidence-of-salmon :dark :long)` call can be expressed as ![Understanding
    the Bayesian classification](img/4351OS_03_75.jpg), and similarly, ![Understanding
    the Bayesian classification](img/4351OS_03_76.jpg) is returned by `(evidence-of-salmon)`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如 REPL 输出所示，训练数据中所有鱼中有 48.16% 的鱼是皮肤较暗的鲑鱼。同样，所有鱼中有 23.96% 的鱼是深色长鲑鱼，而所有鱼中有 69.32%
    的鱼是鲑鱼。`(evidence-of-salmon :dark :long)` 调用返回的值可以表示为 ![理解贝叶斯分类](img/4351OS_03_75.jpg)，同样，`(evidence-of-salmon)`
    也返回 ![理解贝叶斯分类](img/4351OS_03_76.jpg)。
- en: 'Similarly, we can define the `evidence-of-sea-bass` function that determines
    the evidence of occurrence of a sea bass given some observed features of the fish.
    As we are dealing with only two categories, ![Understanding the Bayesian classification](img/4351OS_03_77.jpg),
    we can easily verify this equality in the REPL. Interestingly, a small error is
    observed, but this error is not related to the training data. This small error
    is, in fact, a floating-point rounding error, which arises due to the limitations
    of floating-point numbers. In practice, we can avoid this using the decimal or
    `BigDecimal` (from `java.lang`) data types, instead of floating-point numbers.
    We can verify this using the `evidence-of-sea-bass` and `evidence-of-salmon` functions
    in the REPL as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以定义一个`evidence-of-sea-bass`函数，该函数根据鱼的一些观察到的特征来确定海鲈鱼出现的证据。由于我们只处理两个类别，![理解贝叶斯分类](img/4351OS_03_77.jpg)，我们可以在REPL中轻松验证这个等式。有趣的是，观察到一个小错误，但这个错误与训练数据无关。实际上，这个小错误是一个浮点数舍入错误，这是由于浮点数的限制造成的。在实践中，我们可以使用十进制或`BigDecimal`（来自`java.lang`）数据类型来避免这种情况，而不是使用浮点数。我们可以使用REPL中的`evidence-of-sea-bass`和`evidence-of-salmon`函数来验证这一点，如下所示：
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can generalize the `evidence-of-salmon` and `evidence-of-sea-bass` functions
    such that we are able to determine the probability of any category with some observed
    features; this is shown in the following code:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将`evidence-of-salmon`和`evidence-of-sea-bass`函数泛化，以便我们能够根据一些观察特征确定任何类别的概率；以下代码展示了这一点：
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The function defined in the preceding code returns values that agree with those
    returned by the following `evidence-of-salmon` and `evidence-of-sea-bass` functions:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码中定义的函数返回的值与以下`evidence-of-salmon`和`evidence-of-sea-bass`函数返回的值一致：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Using the `evidence-of-salmon` and `evidence-of-sea-bass` functions, we can
    calculate the probability in terms of `probability-dark-long-fat-is-salmon` as
    follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`evidence-of-salmon`和`evidence-of-sea-bass`函数，我们可以按照以下方式计算以`probability-dark-long-fat-is-salmon`为单位的概率：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can inspect the `probability-dark-long-fat-is-salmon` value in the REPL
    as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在REPL中检查`probability-dark-long-fat-is-salmon`值，如下所示：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `probability-dark-long-fat-is-salmon` value indicates that a fish that is
    dark, long, and fat and has a 95.7 percent probability of being a salmon.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`probability-dark-long-fat-is-salmon`值表明，一条深色、长而胖的鱼有95.7%的概率是鲑鱼。'
- en: 'Using the preceding definition of the `probability-dark-long-fat-is-salmon`
    function as a template, we can generalize the calculations that it performs. Let''s
    first define a simple data structure that can be passed around. In the spirit
    of idiomatic Clojure, we conveniently use a map for this purpose. Using a map,
    we can represent a category in our model along with the evidence and probability
    of its occurrence. Also, given the evidences for several categories, we can calculate
    the total probability of occurrence of a particular category as shown in the following
    code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面定义的`probability-dark-long-fat-is-salmon`函数作为模板，我们可以泛化它所执行的计算。让我们首先定义一个简单的数据结构，它可以被传递。在Clojure的惯用风格中，我们方便地使用映射来完成这个目的。使用映射，我们可以在我们的模型中表示一个类别及其出现的证据和概率。此外，给定几个类别的证据，我们可以计算特定类别出现的总概率，如下面的代码所示：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `make-category-probability-pair` function uses the `evidence-category-with-attrs`
    function we defined in the preceding code to calculate the evidence of a category
    and its conditions or attributes. Then, it returns this value, as a map, along
    with the category itself. Also, we define the `calculate-probability-of-category`
    function, which calculates the total probability of a category and its conditions
    using the `sum-of-evidences` parameter and a value returned by the `make-category-probability-pair`
    function.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`make-category-probability-pair`函数使用我们在前面代码中定义的`evidence-category-with-attrs`函数来计算类别的证据及其条件或属性。然后，它以映射的形式返回这个值，以及类别本身。此外，我们还定义了`calculate-probability-of-category`函数，该函数使用`sum-of-evidences`参数和`make-category-probability-pair`函数返回的值来计算类别及其条件的总概率。'
- en: 'We can compose the preceding two functions to determine the total probability
    of all the categories given some observed values and then select the category
    with the highest probability, as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将前面两个函数组合起来，确定给定一些观察值的所有类别的总概率，然后选择概率最高的类别，如下所示：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `classify-by-attrs` function defined in the preceding code maps all the
    possible categories over the `make-category-probability-pair` function, given
    some conditions or observed values for the features of our model. As we are dealing
    with a sequence of pairs returned by `make-category-probability-pair`, we can
    use a simple composition of the `reduce`, `map`, and `+` functions to calculate
    the sum of all the evidence in this sequence. We then map the `calculate-probability-of-category`
    function over the sequence of category-evidence pairs and select the category-evidence
    pair with the highest probability. We do this by sorting the sequence through
    ascending probabilities and selecting the last element in the sorted sequence.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码中定义的`classify-by-attrs`函数将所有可能的类别映射到`make-category-probability-pair`函数，给定一些条件或我们模型特征的观察值。由于我们处理的是`make-category-probability-pair`返回的序列对，我们可以使用`reduce`、`map`和`+`函数的简单组合来计算此序列中所有证据的总和。然后，我们将`calculate-probability-of-category`函数映射到类别-证据对的序列，并选择概率最高的类别-证据对。我们通过按概率升序排序序列来实现这一点，并选择排序序列中的最后一个元素。
- en: 'Now, we can use the `classify-by-attrs` function to determine the probability
    that an observed fish, which is dark, long, and fat, is a salmon. It is also represented
    by the `probability-dark-long-fat-is-salmon` value, which we defined earlier.
    Both expressions produce the same probability of 95.7 percent of a fish being
    a salmon, given that it''s dark, long, and fat in appearance. We will implement
    the `classify-by-attrs` function as shown in the following code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`classify-by-attrs`函数来确定一个观察到的鱼（外观为暗、长、胖）是鲑鱼的概率。它也由我们之前定义的`probability-dark-long-fat-is-salmon`值表示。这两个表达式都产生了相同的概率，即95.7%，表示外观为暗、长、胖的鱼是鲑鱼。我们将在以下代码中实现`classify-by-attrs`函数：
- en: '[PRE12]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `classify-by-attrs` function also returns the predicted category (that
    is, `:salmon`) of the given observed conditions `:dark`, :`long`, and `:fat`.
    We can use this function to tell us more about the training data as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`classify-by-attrs`函数还返回给定观察条件`:dark`、`:long`和`:fat`的预测类别（即`:salmon`）。我们可以使用此函数来了解更多关于训练数据的信息，如下所示：'
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As shown in the preceding code, a fish that is dark in appearance is mostly
    a salmon, and the one that is light in appearance is mostly a sea bass. Also,
    a fish that''s thin is most likely a sea bass. The following values do, in fact,
    agree with the training data that we defined earlier:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，外观为暗的鱼主要是鲑鱼，外观为亮的鱼主要是海鲈鱼。此外，体型瘦的鱼很可能是海鲈鱼。以下值实际上与我们之前定义的训练数据相符：
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that calling the `classify-by-attrs` function with only `[:salmon]` as
    a parameter returns the probability that any given fish is a salmon. An obvious
    corollary is that given a single category, the `classify-by-attrs` function always
    predicts the supplied category with complete certainty, that is, a probability
    of *1.0*. However, the evidence returned by this function vary depending on the
    observed features passed to it as well as the sample data that we used to train
    our model.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，仅使用`[:salmon]`作为参数调用`classify-by-attrs`函数会返回任何给定鱼是鲑鱼的概率。一个明显的推论是，给定一个单一类别，`classify-by-attrs`函数总是以完全的确定性预测提供的类别，即概率为*1.0*。然而，该函数返回的证据取决于传递给它的观察特征以及我们用来训练模型的样本数据。
- en: In a nutshell, the preceding implementation describes a Bayes classifier that
    can be trained using some sample data. It also classifies some observed values
    for the features of our model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，前面的实现描述了一个可以使用一些样本数据进行训练的贝叶斯分类器。它还分类了我们的模型特征的某些观察值。
- en: 'We can describe a generic Bayes classifier by building upon the definition
    of the ![Understanding the Bayesian classification](img/4351OS_03_60.jpg) probability
    from our previous example. To quickly recap, the term ![Understanding the Bayesian
    classification](img/4351OS_03_60.jpg) can be formally expressed as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过构建我们之前示例中![理解贝叶斯分类](img/4351OS_03_60.jpg)概率的定义来描述一个通用的贝叶斯分类器。为了快速回顾，![理解贝叶斯分类](img/4351OS_03_60.jpg)这个术语可以正式表达如下：
- en: '![Understanding the Bayesian classification](img/4351OS_03_72.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![理解贝叶斯分类](img/4351OS_03_72.jpg)'
- en: 'In the preceding equality, we deal with a single class, namely salmon, and
    three mutually independent features, namely the length, width, and lightness of
    the skin of a fish. We can generalize this equality for *N* features as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个等式中，我们处理一个单一类别，即鲑鱼，以及三个相互独立特征，即鱼皮的长度、宽度和亮度。我们可以将这个等式推广到 *N* 个特征，如下所示：
- en: '![Understanding the Bayesian classification](img/4351OS_03_78.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![理解贝叶斯分类](img/4351OS_03_78.jpg)'
- en: 'Here, the term *Z* is the evidence of the classification model, which we described
    in the preceding equation. We can use the sum and product notation to describe
    the preceding equality more concisely as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，术语 *Z* 是分类模型的证据，我们在前一个方程中进行了描述。我们可以使用求和和乘积符号来更简洁地描述前一个等式，如下所示：
- en: '![Understanding the Bayesian classification](img/4351OS_03_79.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![理解贝叶斯分类](img/4351OS_03_79.jpg)'
- en: 'The preceding equality describes the probability of occurrence of a single
    class, *C*. If we are given a number of classes to choose from, we must select
    the class with the highest probability of occurrence. This brings us to the basic
    definition of a Bayes classifier, which is formally expressed as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个等式描述了单个类别 *C* 的发生概率。如果我们给定多个类别可供选择，我们必须选择发生概率最高的类别。这引出了贝叶斯分类器的基本定义，其形式表达如下：
- en: '![Understanding the Bayesian classification](img/4351OS_03_80.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![理解贝叶斯分类](img/4351OS_03_80.jpg)'
- en: In the preceding equation, the function ![Understanding the Bayesian classification](img/4351OS_03_81.jpg)
    describes a Bayes classifier that selects the class with the highest probability
    of occurrence. Note that the terms ![Understanding the Bayesian classification](img/4351OS_03_82.jpg)
    represent the various features of our classification model, whereas the terms
    ![Understanding the Bayesian classification](img/4351OS_03_83.jpg) represent the
    set of observed values of these features. Also, the variable *c*, on the right-hand
    side of the equation, can have values from within a set of all the distinct classes
    in the classification model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个方程中，函数 ![理解贝叶斯分类](img/4351OS_03_81.jpg) 描述了一个贝叶斯分类器，它选择发生概率最高的类别。请注意，术语
    ![理解贝叶斯分类](img/4351OS_03_82.jpg) 代表我们分类模型的各个特征，而术语 ![理解贝叶斯分类](img/4351OS_03_83.jpg)
    代表这些特征的观测值集合。此外，方程右侧的变量 *c* 可以取分类模型中所有不同类别的值。
- en: 'We can further simplify the preceding equation of a Bayes classifier via the
    **Maximum a Posteriori** (**MAP**) estimation, which can be thought of as a regularization
    of the features in Bayesian statistics. A simplified Bayes classifier can be formally
    expressed as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 **最大后验概率** (**MAP**) 估计进一步简化贝叶斯分类器的先前的方程，这可以被视为贝叶斯统计中特征的正规化。简化的贝叶斯分类器可以形式表达如下：
- en: '![Understanding the Bayesian classification](img/4351OS_03_84.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![理解贝叶斯分类](img/4351OS_03_84.jpg)'
- en: This definition essentially means that the *classify* function determines the
    class with the maximum probability of occurrence for the given features. Thus,
    the preceding equation describes a Bayes classifier that can be trained using
    some sample data and then be used to predict the class of a given set of observed
    values. We will now focus on using an existing implementation of a Bayes classifier
    to model a given classification problem.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义本质上意味着 *classify* 函数确定给定特征的最高发生概率的类别。因此，前一个方程描述了一个可以使用一些样本数据进行训练，然后用于预测给定观测值集合类别的贝叶斯分类器。我们现在将专注于使用现有的贝叶斯分类器实现来建模给定的分类问题。
- en: The `clj-ml` library ([https://github.com/joshuaeckroth/clj-ml](https://github.com/joshuaeckroth/clj-ml))
    contains several implemented algorithms that we can choose from to model a given
    classification problem. This library is actually just a Clojure wrapper for the
    popular **Weka** library ([http://www.cs.waikato.ac.nz/ml/weka/](http://www.cs.waikato.ac.nz/ml/weka/)),
    which is a Java library that contains implementations for several machine learning
    algorithms. It also has several methods for evaluating and validating a generated
    classification model. However, we will concentrate on the `clj-ml` library's implementations
    of classifiers within the context of this chapter.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`clj-ml`库([https://github.com/joshuaeckroth/clj-ml](https://github.com/joshuaeckroth/clj-ml))包含几个实现算法，我们可以从中选择来模拟给定的分类问题。实际上，这个库只是流行的**Weka**库([http://www.cs.waikato.ac.nz/ml/weka/](http://www.cs.waikato.ac.nz/ml/weka/))的Clojure包装器，Weka是一个包含多个机器学习算法实现的Java库。它还有几个用于评估和验证生成的分类模型的方法。然而，我们将专注于本章中`clj-ml`库的分类器实现。'
- en: Note
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 备注
- en: 'The `clj-ml` library can be added to a Leiningen project by adding the following
    dependency to the `project.clj` file:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在`project.clj`文件中添加以下依赖项将`clj-ml`库添加到Leiningen项目中：
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'For the upcoming example, the namespace declaration should look similar to
    the following declaration:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于即将到来的示例，命名空间声明应类似于以下声明：
- en: '[PRE16]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We''ll now introduce the `clj-ml` library using an implementation of a Bayes
    classifier to model our previous problem involving the fish packaging plant. First,
    let''s refine our training data to use numeric values rather than the keywords,
    which we described earlier, for the various features of our model. Of course,
    we will still maintain the partiality in our training data such that the salmon
    are mostly fat and dark-skinned, while the sea bass are mostly long and light-skinned.
    The following code implements this:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过一个贝叶斯分类器的实现来介绍`clj-ml`库，以模拟我们之前涉及鱼包装厂的问题。首先，让我们精炼我们的训练数据，使用数值而不是我们之前描述的关键字来表示模型的各种特征。当然，我们将在训练数据中保持部分性，使得鲑鱼大多是肥的和深色的，而海鲈鱼大多是长的和浅色的。以下代码实现了这一点：
- en: '[PRE17]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, we define the `rand-in-range` function, which simply generates random
    integers within a given range of values. We then redefine the `make-sea-bass`
    and `make-salmon` functions to use the `rand-in-range` function to generate values
    within the range of `0` and `10` for the three features of a fish, namely its
    length, width, and the darkness of its skin. A fish with a lighter skin color
    is indicated by a higher value for this feature. Note that we reuse the definitions
    of the `make-sample-fish` function and `fish-dataset` variable to generate our
    training data. Also, a fish is represented by a vector rather than a set, as described
    in the earlier definitions of the `make-sea-bass` and `make-salmon` functions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了`rand-in-range`函数，该函数简单地生成给定值范围内的随机整数。然后我们重新定义了`make-sea-bass`和`make-salmon`函数，使用`rand-in-range`函数来生成鱼的三个特征（长度、宽度和皮肤颜色深浅）的值，这些值在`0`到`10`之间。皮肤颜色较浅的鱼，这个特征的值会更高。请注意，我们重用了`make-sample-fish`函数的定义和`fish-dataset`变量的定义来生成我们的训练数据。此外，鱼是由一个向量而不是一个集合来表示的，正如在`make-sea-bass`和`make-salmon`函数的早期定义中所述。
- en: 'We can create a classifier from the `clj-ml` library using the `make-classifier`
    function, which can be found in the `clj-ml.classifiers` namespace. We can specify
    the type of classifier to be used by passing two keywords as arguments to the
    functions. As we intend to use a Bayes classifier, we supply the keywords, `:bayes`
    and `:naive`, to the `make-classifier` function. In a nutshell, we can use the
    following declaration to create a Bayes classifier. Note that the keyword, `:naive`,
    used in the following code signifies a naïve Bayes classifier that assumes that
    the features in our model are independent:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`clj-ml`库中的`make-classifier`函数创建一个分类器，该函数位于`clj-ml.classifiers`命名空间中。我们可以通过将两个关键字作为参数传递给函数来指定要使用的分类器类型。由于我们打算使用贝叶斯分类器，我们将关键字`:bayes`和`:naive`传递给`make-classifier`函数。简而言之，我们可以使用以下声明来创建一个贝叶斯分类器。请注意，以下代码中使用的关键字`:naive`表示一个朴素贝叶斯分类器，它假设我们模型中的特征是独立的：
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `clj-ml` library''s classifier implementations use datasets that are defined
    or generated using functions from the `clj-ml.data` namespace. We can convert
    the `fish-dataset` sequence, which is a sequence of vectors, into such a dataset
    using the `make-dataset` function. This function requires an arbitrary string
    name for the dataset, a template for each item in the collection, and a collection
    of items to add to the dataset. The template supplied to the `make-dataset` function
    is easily represented by a map, which is shown as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`clj-ml` 库的分类器实现使用通过 `clj-ml.data` 命名空间中的函数定义或生成的数据集。我们可以使用 `make-dataset`
    函数将 `fish-dataset` 序列（一个向量序列）转换为这样的数据集。此函数需要一个数据集的任意字符串名称、每个集合项的模板以及要添加到数据集中的项目集合。提供给
    `make-dataset` 函数的模板很容易用映射表示，如下所示：'
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `fish-template` map defined in the preceding code simply says that a fish,
    as represented by a vector, comprises the fish's category, length, width, and
    lightness of its skin, in that specific order. Note that the category of the fish
    is described using either `:salmon` or `:sea-bass`. We can now use `fish-dataset`
    to train the classifier represented by the `bayes-classifier` variable.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的 `fish-template` 映射只是简单地说明，作为一个向量表示的鱼，由鱼的类别、长度、宽度和皮肤的亮度组成，顺序如下。请注意，鱼的类别是用
    `:salmon` 或 `:sea-bass` 描述的。现在我们可以使用 `fish-dataset` 来训练由 `bayes-classifier` 变量表示的分类器。
- en: 'Although the `fish-template` map defines all the attributes of a fish, it''s
    still lacking one important detail. It doesn''t specify which of these attributes
    represent the class or category of the fish. In order to specify a particular
    attribute in the vector to represent the category of an entire set of observed
    values, we use the `dataset-set-class` function. This function takes a single
    argument that specifies the index of an attribute and is used to represent the
    category of the set of observed values in the vector. Note that this function
    does actually mutate or modify the dataset it''s supplied with. We can then train
    our classifier using the `classifier-train` function, which takes a classifier
    and a dataset as parameters; this is shown in the following code:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然该 `fish-template` 映射定义了鱼的全部属性，但它仍然缺少一个重要的细节。它没有指定这些属性中哪一个代表鱼的类别或分类。为了指定向量中的一个特定属性以代表整个观察值集合的类别，我们使用
    `dataset-set-class` 函数。此函数接受一个参数，指定属性的索引，并用于在向量中代表观察值集合的类别。请注意，此函数实际上会修改或修改它提供的数据集。然后我们可以使用
    `classifier-train` 函数来训练我们的分类器，该函数接受一个分类器和数据集作为参数；如下代码所示：
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding `train-bayes-classifier` function simply calls the `dataset-set-class`
    and `classifier-train` functions to train our classifier. When we call the `train-bayes-classifier`
    function, the classifier is trained with the following supplied data and then
    printed to the REPL output:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的 `train-bayes-classifier` 函数只是调用了 `dataset-set-class` 和 `classifier-train`
    函数来训练我们的分类器。当我们调用 `train-bayes-classifier` 函数时，分类器会使用以下提供的数据进行训练，然后打印到 REPL 输出：
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This output gives us some basic information about the training data, such as
    the mean and standard deviation of the various features that we model. We can
    now use this trained classifier to predict the category of a set of observed values
    for the features of our model.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出为我们提供了关于训练数据的一些基本信息，例如我们模型中各种特征的均值和标准差。现在我们可以使用这个训练好的分类器来预测我们模型特征的观察值的类别。
- en: 'Let''s first define the observed values that we intend to classify. To do so,
    we use the following `make-instance` function, which requires a dataset and a
    vector of observed values that agree with the data template of the supplied dataset:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先定义我们打算分类的观察值。为此，我们使用以下 `make-instance` 函数，该函数需要一个数据集和一个与提供的数据集数据模板相匹配的观察值向量：
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here, we simply defined a sample fish using the `make-instance` function. We
    can now predict the class of the fish represented by `sample-fish` as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只是使用 `make-instance` 函数定义了一个样本鱼。现在我们可以如下预测由 `sample-fish` 表示的鱼的类别：
- en: '[PRE23]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As shown in the preceding code, the fish is classified as a `salmon`. Note that
    although we provide the class of the fish as `:salmon` while defining `sample-fish`,
    it's only for conformance with the data template defined by `fish-dataset`. In
    fact, we could specify the class of `sample-fish` as `:sea-bass` or a third value,
    say `:unknown`, to represent an undefined value, and the classifier would still
    classify `sample-fish` as a `salmon`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，鱼被分类为`salmon`。请注意，尽管我们在定义`sample-fish`时提供了鱼的类别为`:salmon`，但这只是为了符合`fish-dataset`定义的数据模板。实际上，我们可以将`sample-fish`的类别指定为`:sea-bass`或第三个值，例如`:unknown`，以表示一个未定义的值，分类器仍然会将`sample-fish`分类为`salmon`。
- en: When dealing with the continuous values for the various features of the given
    classification model, we can specify a Bayes classifier to use discretization
    of continuous features. By this, we mean that all the values for the various features
    of the model will be converted to discrete values by the probability density estimation.
    We can specify this option to the `make-classifier` function by simply passing
    an extra argument, `{:supervised-discretization true}`, to the function. This
    map actually describes all the possible options that can be provided to the specified
    classifier.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理给定分类模型的各种特征的连续值时，我们可以指定一个贝叶斯分类器来使用连续特征的离散化。这意味着模型的各种特征的值将通过概率密度估计转换为离散值。我们可以通过简单地向`make-classifier`函数传递一个额外的参数`{:supervised-discretization
    true}`来指定此选项。这个映射实际上描述了可以提供给指定分类器的所有可能选项。
- en: In conclusion, the `clj-ml` library provides a fully operational Bayes classifier
    that we can use to classify arbitrary data. Although we generated the training
    data ourselves in the previous example, this data can be fetched from the Web
    or a database as well.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，`clj-ml`库提供了一个完全可操作的贝叶斯分类器，我们可以用它来对任意数据进行分类。尽管我们在前面的例子中自己生成了训练数据，但这些数据也可以从网络或数据库中获取。
- en: Using the k-nearest neighbors algorithm
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用k-最近邻算法
- en: A simple technique that can be used to classify a set of observed values is
    the **k-nearest neighbors** (abbreviated as **k-NN**) algorithm. This algorithm
    is a form of **lazy learning** in which all the computation is deferred until
    classification. Also, in the classification phase, the k-NN algorithm approximates
    the class of the observed values using only a few values from the training data,
    and the reading of other values is deferred until they are actually needed.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可以用来对一组观测值进行分类的简单技术是**k-最近邻**（简称**k-NN**）算法。这是一种**懒惰学习**的形式，其中所有计算都推迟到分类阶段。在分类阶段，k-NN算法仅使用训练数据中的少数几个值来近似观测值的类别，其他值的读取则推迟到实际需要时。
- en: While we now explore the k-NN algorithm in the context of classification, it
    can be applied to regression as well by simply selecting the predicted value as
    the average of the nearest values of the dependent variable for a set of observed
    feature values. Interestingly, this technique of modeling regression is, in fact,
    a generalization of **linear interpolation** (for more information, refer to *An
    introduction to kernel and nearest-neighbor nonparametric regression*).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们现在在分类的背景下探索k-NN算法，但它也可以通过简单地选择预测值作为一组观测特征值的依赖变量的最近值的平均值来应用于回归。有趣的是，这种建模回归的技术实际上是对**线性插值**（更多信息，请参阅*An
    introduction to kernel and nearest-neighbor nonparametric regression*）的推广。
- en: The k-NN algorithm reads some training data and analyzes this data lazily, that
    is, only when needed. Apart from the training data, the algorithm requires a set
    of observed values and a constant *k* as parameters to classify the set of observed
    values. To classify these observed values, the algorithm predicts the class that
    is the most frequent among the *k* training samples nearest to the set of observed
    values. By nearest, we mean a point with the least Euclidean distance from the
    point that is represented by a set of observed values in the Euclidean space of
    the training data.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN算法读取一些训练数据，并懒惰地分析这些数据，也就是说，只有在需要时才会分析。除了训练数据外，该算法还需要一组观测值和一个常数*k*作为参数来对观测值集进行分类。为了对这些观测值进行分类，算法预测与观测值集最近的*k*个训练样本中最频繁的类别。这里的“最近”是指，在训练数据的欧几里得空间中，代表观测值集的点与具有最小欧几里得距离的点。
- en: An obvious corollary is that when ![Using the k-nearest neighbors algorithm](img/4351OS_03_85.jpg),
    the predicted class is the class of the single neighbor nearest to the set of
    observed values. This special case of the k-NN algorithm is called the **nearest
    neighbor** algorithm.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一个明显的推论是，当![使用k最近邻算法](img/4351OS_03_85.jpg)时，预测的类别是观察值集合最近的单个邻居的类别。k-NN算法的这个特殊情况被称为**最近邻**算法。
- en: 'We can create a classifier that uses the k-NN algorithm using the `clj-ml`
    library''s `make-classifier` function. Such a classifier is specified using the
    keywords `:lazy` and `:ibk` as arguments to the `make-classifier` function. We
    will now use such a classifier to model our previous example of a fish packaging
    plant, as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`clj-ml`库的`make-classifier`函数创建一个使用k-NN算法的分类器。这样的分类器通过将`:lazy`和`:ibk`作为`make-classifier`函数的参数来指定。我们现在将使用这样的分类器来模拟我们之前的鱼包装厂示例，如下所示：
- en: '[PRE24]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The preceding code defines a k-NN classifier as `K1-classifier` and a `train-K1-classifier`
    function to train the classifier with the training data using `fish-dataset`,
    which we defined in the preceding code.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码定义了一个k-NN分类器为`K1-classifier`，以及一个`train-K1-classifier`函数，使用`fish-dataset`（我们在前述代码中定义）来训练分类器。
- en: 'Note that the `make-classifier` function defaults the constant *k* or rather
    the number of neighbors to *1*, which implies a single nearest neighbor. We can
    optionally specify the constant *k* as a key-value pair with the`:num-neighbors`
    key to the `make-classifier` function as shown in the following code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`make-classifier`函数默认将常数*k*或更确切地说，邻居的数量设置为*1*，这意味着单个最近邻。我们可以选择性地通过将`:num-neighbors`键作为键值对传递给`make-classifier`函数来指定常数*k*，如下述代码所示：
- en: '[PRE25]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can now call the `train-K1-classifier` function to train the classifier
    as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以调用`train-K1-classifier`函数来按照以下方式训练分类器：
- en: '[PRE26]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can now use the `classifier-classify` function to classify the fish represented
    by `sample-fish`, which we had defined earlier, using the classifier represented
    by the `K1-classifier` variable:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用`classifier-classify`函数来对之前定义的`sample-fish`表示的鱼进行分类，使用的是由`K1-classifier`变量表示的分类器：
- en: '[PRE27]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As shown in the preceding code, the k-NN classifier predicts the fish class
    as salmon, thus agreeing with our earlier predictions that used a Bayes classifier.
    In conclusion, the `clj-ml` library provides a concise implementation of a classifier
    that uses the k-NN algorithm to predict the class of a set of observed values.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，k-NN分类器预测鱼类为鲑鱼，这与我们之前使用贝叶斯分类器做出的预测一致。总之，`clj-ml`库提供了一个简洁的实现，使用k-NN算法来预测一组观察值的类别。
- en: The k-NN classifier provided by the `clj-ml` library performs the normalization
    of the features of the classification model by default using the mean and standard
    deviation of the values of these features. We can specify an option to the `make-classifier`
    function to skip this normalization phase by passing a map entry with the`:no-normalization`
    key in the map of options passed to the `make-classifier` function.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`clj-ml`库提供的k-NN分类器默认使用这些特征的值均值和标准差对分类模型的特征进行归一化。我们可以通过传递一个包含`:no-normalization`键的映射条目到`make-classifier`函数的选项映射中，来指定一个选项给`make-classifier`函数以跳过这个归一化阶段。'
- en: Using decision trees
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用决策树
- en: We can also use decision trees to model a given classification problem. A decision
    tree is, in fact, constructed from the given training data, and we can use this
    decision tree to predict the class of a given set of observed values. The process
    of constructing a decision tree is loosely based on the concepts of information
    entropy and information gain from information theory (for more information, refer
    to *Induction of Decision Trees*). It is also often termed as **decision tree
    learning**. Unfortunately, a detailed study of information theory is beyond the
    scope of this book. However, in this section, we will explore some concepts in
    information theory that will be used in the context of machine learning.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用决策树来建模给定的分类问题。实际上，决策树是从给定的训练数据构建的，我们可以使用这个决策树来预测一组给定观察值的类别。构建决策树的过程大致基于信息论中的信息熵和信息增益的概念（更多信息，请参阅*决策树归纳*）。它也常被称为**决策树学习**。不幸的是，对信息论进行详细研究超出了本书的范围。然而，在本节中，我们将探讨一些将在机器学习环境中使用的信息论概念。
- en: A decision tree is a tree or graph that describes a model of decisions and their
    possible consequences. An internal node in a decision tree represents a decision,
    or rather a condition of a particular feature in the context of classification.
    It has two possible outcomes that are represented by the left and right subtrees
    of the node. Of course, a node in the decision tree could also have more than
    two subtrees. Each leaf node in a decision tree represents a particular class,
    or a consequence, in our classification model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种树或图，描述了决策及其可能后果的模型。决策树中的一个内部节点代表一个决策，或者更确切地说，在分类的上下文中，代表特定特征的某种条件。它有两个可能的结果，分别由节点的左右子树表示。当然，决策树中的节点也可能有超过两个的子树。决策树中的每个叶节点代表我们分类模型中的一个特定类别或后果。
- en: 'For example, our previous classification problem involving the fish packaging
    plant could have the following decision tree:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们之前的涉及鱼包装工厂的分类问题可能有以下决策树：
- en: '![Using decision trees](img/4351OS_03_86.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![使用决策树](img/4351OS_03_86.jpg)'
- en: The previously illustrated decision tree uses two conditions to classify a fish
    as either a salmon or a sea bass. The internal nodes represent the two conditions
    based on the features of our classification model. Note that the decision tree
    uses only two of the three features of our classification model. We can thus say
    the tree is *pruned*. We shall briefly explore this technique as well in this
    section.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 之前展示的决策树使用两个条件来分类鱼是鲑鱼还是海鲢。内部节点代表基于我们分类模型特征的两种条件。请注意，决策树只使用了我们分类模型的三个特征中的两个。因此，我们可以说这棵树是*剪枝*的。我们将在本节中简要探讨这项技术。
- en: To classify a set of observed values using a decision tree, we traverse the
    tree starting from the root node until we reach a leaf node that represents the
    predicted class of the set of observed values. This technique of predicting the
    class of a set of observed values from a decision tree is always the same, irrespective
    of how the decision tree was constructed. For the decision tree described earlier,
    we can classify a fish by first comparing its length followed by the lightness
    of its skin. The second comparison is only needed if the length of the fish is
    greater than **6** as specified by the internal node with the expression **Length
    < 6** in the decision tree. If the length of the fish is indeed greater than **6**,
    we use the lightness of the skin of the fish to decide whether it's a salmon or
    a sea bass.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用决策树对一组观测值进行分类，我们从根节点开始遍历树，直到达到代表观测值集合预测类别的叶节点。从决策树预测一组观测值类别的这种技术始终相同，无论决策树是如何构建的。对于之前描述的决策树，我们可以通过首先比较鱼的长度，然后比较其皮肤的亮度来对鱼进行分类。第二次比较只有在鱼的长度大于**6**（如决策树中带有表达式**Length
    < 6**的内部节点所指定）时才需要。如果鱼的长度确实大于**6**，我们使用鱼的皮肤亮度来决定它是鲑鱼还是海鲢。
- en: There are actually several algorithms that are used to construct a decision
    tree from some training data. Generally, the tree is constructed by splitting
    the set of sample values in the training data into smaller subsets based on an
    attribute value test. The process is repeated on each subset until splitting a
    given subset of sample values no longer adds internal nodes to the decision tree.
    As we mentioned earlier, it's possible for an internal node in a decision tree
    to have more than two subtrees.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，有几种算法用于从一些训练数据中构建决策树。通常，树是通过根据属性值测试将训练数据中的样本值集合分割成更小的子集来构建的。这个过程在每个子集上重复进行，直到分割给定样本值子集不再向决策树添加内部节点。正如我们之前提到的，决策树中的内部节点可能有多于两个的子树。
- en: 'We will now explore the **C4.5** algorithm to construct a decision tree (for
    more information, refer to *C4.5: Programs for Machine Learning*). This algorithm
    uses the concept of information entropy to decide the feature and the corresponding
    value on which the set of sample values must be partitioned. **Information entropy**
    is defined as the measure of uncertainty in a given feature or random variable
    (for more information, refer to "A Mathematical Theory of Communication").'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将探讨**C4.5**算法来构建决策树（更多信息，请参阅*《C4.5：机器学习程序》*）。这个算法使用信息熵的概念来决定必须对样本值集合进行分割的特征和相应的值。**信息熵**被定义为给定特征或随机变量的不确定性度量（更多信息，请参阅“通信的数学理论”）。
- en: 'For a given feature or attribute *f*, which has values within the range of
    *1* to *m*, we can define the information entropy of the ![Using decision trees](img/4351OS_03_88.jpg)
    feature as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定特征或属性 *f*，其值在 *1* 到 *m* 的范围内，我们可以定义该![使用决策树](img/4351OS_03_88.jpg)特征的信息熵如下：
- en: '![Using decision trees](img/4351OS_03_89.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![使用决策树](img/4351OS_03_89.jpg)'
- en: 'In the preceding equation, the term ![Using decision trees](img/4351OS_03_90.jpg)
    represents the number of occurrences of the feature *f* with respect to the value
    *i*. Based on this definition of the information entropy of a feature, we define
    the normalized information gain ![Using decision trees](img/4351OS_03_91.jpg).
    In the following equality, the term *T* refers to the set of sample values or
    training data supplied to the algorithm:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，术语![使用决策树](img/4351OS_03_90.jpg)表示特征 *f* 相对于值 *i* 的出现次数。基于特征信息熵的定义，我们定义归一化信息增益![使用决策树](img/4351OS_03_91.jpg)。在以下等式中，术语
    *T* 指的是提供给算法的样本值或训练数据集：
- en: '![Using decision trees](img/4351OS_03_92.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![使用决策树](img/4351OS_03_92.jpg)'
- en: In terms of information entropy, the preceding definition of the information
    gain of a given attribute is the change in information entropy of the total set
    of values when the attribute *f* is removed from the given set of features in
    the model.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 从信息熵的角度来看，给定属性的先前的信息增益定义是当从模型中给定的特征集中移除属性 *f* 时，整个值集的信息熵的变化。
- en: 'The algorithm selects a feature *A* from the given set of features in the training
    data such that the feature *A* has the maximum possible information gain in the
    set of features. We can represent this with the help of the following equality:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 算法从训练数据中选择的特征 *A*，使得特征 *A* 在特征集中具有最大的可能信息增益。我们可以借助以下等式来表示这一点：
- en: '![Using decision trees](img/4351OS_03_93.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![使用决策树](img/4351OS_03_93.jpg)'
- en: In the preceding equation, ![Using decision trees](img/4351OS_03_94.jpg) represents
    the set of all the possible values that a feature *a* is known to have. The ![Using
    decision trees](img/4351OS_03_95.jpg) set represents the observed values in which
    the feature *a* has the value *v* and the term ![Using decision trees](img/4351OS_03_96.jpg)
    represents the information entropy of this set of values.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，![使用决策树](img/4351OS_03_94.jpg)表示特征 *a* 所知可能具有的所有可能值集。![使用决策树](img/4351OS_03_95.jpg)集表示特征
    *a* 具有值 *v* 的观察值集，而术语![使用决策树](img/4351OS_03_96.jpg)表示该值集的信息熵。
- en: 'Using the preceding equation to select a feature with the maximum information
    gain from the training data, we can describe the C4.5 algorithm through the following
    steps:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的方程从训练数据中选择具有最大信息增益的特征，我们可以通过以下步骤描述 C4.5 算法：
- en: For each feature *a*, find the normalized information gain from partitioning
    the sample data on the feature *a*.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个特征 *a*，找到在特征 *a* 上划分样本数据时的归一化信息增益。
- en: Select the feature *A* with the maximum normalized information gain.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择具有最大归一化信息增益的特征 *A*。
- en: Create an internal decision node based on the selected feature *A*. Both the
    subtrees created from this step are either leaf nodes or a new set of sample values
    to be partitioned further.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据所选特征 *A* 创建一个内部决策节点。从这个步骤创建的两个子树要么是叶节点，要么是进一步划分的新样本值集。
- en: Repeat this process on each partitioned set of sample values produced from the
    previous step. We repeat the preceding steps until all the features in a subset
    of sample values have the same information entropy.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上一步产生的每个样本值分区集上重复此过程。我们重复前面的步骤，直到样本值子集中的所有特征都具有相同的信息熵。
- en: Once a decision tree has been created, we can optionally perform **pruning**
    on the tree. Pruning is simply the process of removing any extraneous decision
    nodes from the tree. This can be thought of as a form for the regularization of
    decision trees through which we prevent underfitting or overfitting of the estimated
    decision tree model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了一个决策树，我们可以选择性地对该树进行**剪枝**。剪枝简单地说就是从树中移除任何多余的决策节点。这可以被视为通过正则化决策树来防止估计的决策树模型欠拟合或过拟合的一种形式。
- en: '**J48** is an open source implementation of the C4.5 algorithm in Java, and
    the `clj-ml` library contains a working J48 decision tree classifier. We can create
    a decision tree classifier using the `make-classifier` function, and we supply
    the keywords `:decision-tree` and `:c45` as parameters to this function to create
    a J48 classifier as shown in the following code:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**J48**是C4.5算法在Java中的开源实现，`clj-ml`库包含一个有效的J48决策树分类器。我们可以使用`make-classifier`函数创建一个决策树分类器，并向此函数提供`:decision-tree`和`:c45`关键字作为参数来创建一个J48分类器，如下述代码所示：'
- en: '[PRE28]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `train-DT-classifier` function defined in the preceding code simply trains
    the classifier represented by `DT-classifier` with the training data from our
    previous example of the fish packaging plant. The `classifier-train` function
    also prints the following trained classifier:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码中定义的`train-DT-classifier`函数简单地将`DT-classifier`分类器用我们之前鱼包装厂示例中的训练数据训练。`classifier-train`函数还会打印以下训练好的分类器：
- en: '[PRE29]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The preceding output gives a good idea of what the decision tree of the trained
    classifier looks like as well as the size and number of leaf nodes in the decision
    tree. Apparently, the decision tree has three distinct internal nodes. The root
    node of the tree is based on the width of a fish, the subsequent node is based
    on the length of a fish, and the last decision node is based on the lightness
    of the skin of a fish.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出很好地说明了训练好的决策树的外观，以及决策树的大小和叶节点数量。显然，决策树有三个不同的内部节点。树的根节点基于鱼的宽度，后续节点基于鱼的长度，最后一个决策节点基于鱼皮肤的亮度。
- en: 'We can now use the decision tree classifier to predict the class of a fish,
    and we use the following `classifier-classify` function to perform this classification:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用决策树分类器来预测鱼的类别，我们使用以下`classifier-classify`函数来进行这个分类：
- en: '[PRE30]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As shown in the preceding code, the classifier predicts the class of the fish
    represented by `sample-fish` as a `:salmon` keyword just like the other classifiers
    used in the earlier examples.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，分类器将代表`sample-fish`的鱼的类别预测为`:salmon`关键字，就像之前示例中使用的其他分类器一样。
- en: 'The J48 decision tree classifier implementation provided by the `clj-ml` library
    performs pruning as a final step while training the classifier. We can generate
    an unpruned tree by specifying the `:unpruned` key in the map of options passed
    to the `make-classifier` function as shown in the following code:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`clj-ml`库提供的J48决策树分类器实现，在训练分类器时将剪枝作为最后一步。我们可以通过在传递给`make-classifier`函数的选项映射中指定`:unpruned`键来生成未经修剪的树，如下述代码所示：'
- en: '[PRE31]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The previously defined classifier will not perform pruning on the decision
    tree generated from training the classifier with the given training data. We can
    inspect what an unpruned tree looks like by defining and calling the `train-UDT-classifier`
    function, which simply trains the classifier using the `classifier-train` function
    with the `fish-dataset` training data. This function can be defined as being analogous
    to the `train-UDT-classifier` function and produces the following output when
    it is called:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 之前定义的分类器在用给定的训练数据训练决策树时不会进行剪枝。我们可以通过定义和调用`train-UDT-classifier`函数来检查未经修剪的树的外观，该函数简单地使用`classifier-train`函数和`fish-dataset`训练数据来训练分类器。此函数可以定义为与`train-UDT-classifier`函数类似，并在调用时产生以下输出：
- en: '[PRE32]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As shown in the preceding code, the unpruned decision tree has a lot more internal
    decision nodes as compared to the decision tree that is generated after pruning
    it. We can now use the following `classifier-classify` function to predict the
    class of a fish using the trained classifier:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，未经修剪的决策树与修剪后的决策树相比，拥有更多的内部决策节点。现在我们可以使用以下`classifier-classify`函数来预测一条鱼的类别，使用的是训练好的分类器：
- en: '[PRE33]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Interestingly, the unpruned tree also predicts the class of the fish represented
    by `sample-fish` as `:salmon`, thus agreeing with the class predicted by the pruned
    decision tree, which we had described earlier. In summary, the `clj-ml` library
    provides us with a working implementation of a decision tree classifier based
    on the C4.5 algorithm.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，未经修剪的树也预测了代表`sample-fish`的鱼的类别为`:salmon`，因此与之前描述的修剪决策树预测的类别一致。总之，`clj-ml`库为我们提供了一个基于C4.5算法的决策树分类器的有效实现。
- en: The `make-classifier` function supports several interesting options for the
    J48 decision tree classifier. We've already explored the `:unpruned` option, which
    indicated that the decision tree is not pruned. We can specify the`:reduced-error-pruning`
    option to the `make-classifier` function to force the usage of reduced error pruning
    (for more information, refer to "Pessimistic decision tree pruning based on tree
    size"), which is a form of pruning based on reducing the overall error of the
    model. Another interesting option that we can specify to the `make-classifier`
    function is the maximum number of internal nodes or folds that can be removed
    by pruning the decision tree. We can specify this option using the `:pruning-number-of-folds`
    option, and by default, the `make-classifier` function imposes no such limit while
    pruning the decision tree. Also, we can specify that each internal decision node
    in the decision tree has only two subtrees by specifying the `:only-binary-splits`
    option to the `make-classifier` function.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`make-classifier`函数支持J48决策树分类器的一些有趣选项。我们已经探讨了`:unpruned`选项，它表示决策树没有被剪枝。我们可以将`:reduced-error-pruning`选项指定给`make-classifier`函数，以强制使用减少误差剪枝（更多信息，请参阅“基于树大小的悲观决策树剪枝”），这是一种基于减少模型整体误差的剪枝形式。我们还可以指定给`make-classifier`函数的另一个有趣选项是剪枝决策树时可以移除的最大内部节点或折叠数。我们可以使用`:pruning-number-of-folds`选项来指定此选项，并且默认情况下，`make-classifier`函数在剪枝决策树时不会施加此类限制。此外，我们还可以通过指定`:only-binary-splits`选项给`make-classifier`函数，来指定决策树中的每个内部决策节点只有两个子树。'
- en: Summary
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we explored classification and the various algorithms that
    can be used to model a given classification problem. Although classification techniques
    are very useful, they do not perform too well when the sample data has a large
    number of dimensions. Also, the features may vary in a nonlinear manner, as we
    will describe in [Chapter 4](ch04.html "Chapter 4. Building Neural Networks"),
    *Building Neural Networks*. We will also explore more about these aspects and
    the alternate methods of supervised learning in the following chapters. The following
    are few of the points that we looked at in this chapter:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了分类以及可以用来对给定分类问题进行建模的各种算法。尽管分类技术非常有用，但当样本数据具有大量维度时，它们的性能并不太好。此外，特征可能以非线性方式变化，正如我们将在[第4章](ch04.html
    "第4章。构建神经网络")“构建神经网络”中描述的那样。我们将在接下来的章节中进一步探讨这些方面以及监督学习的替代方法。以下是本章中我们关注的一些要点：
- en: We described two broad types of classifications, namely, binary and multiclass
    classification. We also briefly studied the logistic function and how it can be
    used to model classification problems through logistic regression.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们描述了两种广泛的分类类型，即二分类和多分类。我们还简要研究了逻辑函数以及如何通过逻辑回归来使用它来建模分类问题。
- en: We studied and implemented a Bayes classifier, which used a probabilistic model
    used for modeling classification. We also described how we could use the `clj-ml`
    library's Bayes classifier implementation to model a given classification problem.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们研究和实现了贝叶斯分类器，它使用用于建模分类的概率模型。我们还描述了如何使用`clj-ml`库的贝叶斯分类器实现来对给定的分类问题进行建模。
- en: We also explored the simple k-nearest neighbor algorithm and how we can leverage
    it using the `clj-ml` library.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还探讨了简单的k-最近邻算法以及我们如何利用`clj-ml`库来利用它。
- en: We studied decision trees and the C4.5 algorithm. The `clj-ml` library provides
    us with a configurable implementation of a classifier based on the C4.5 algorithm,
    and we described how this implementation could be used as well.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们研究了决策树和C4.5算法。`clj-ml`库为我们提供了一个基于C4.5算法的可配置分类器实现，我们描述了如何使用此实现。
- en: We will explore artificial neural networks in the following chapter. Interestingly,
    we can use artificial neural networks to model regression and classification problems,
    and we will study these aspects of neural networks as well.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨人工神经网络。有趣的是，我们可以使用人工神经网络来建模回归和分类问题，我们也将研究这些神经网络的方面。
