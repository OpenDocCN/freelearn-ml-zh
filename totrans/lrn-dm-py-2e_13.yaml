- en: Next Steps...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During the course, there were lots of avenues not taken, options not presented,
    and subjects not fully explored. In this appendix, I've created a collection of
    next steps for those wishing to undertake extra learning and progress their data
    mining with Python.
  prefs: []
  type: TYPE_NORMAL
- en: This appendix is for learning more about data mining. Also included are some
    challenges to extend the work performed. Some of these will be small improvements;
    some will be quite a bit more work—I've made a note of those more tasks that are
    noticeably more difficult and involved than the others.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with Data Mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter following are a few avenues that reader can explore:'
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn tutorials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'URL: [http://scikit-learn.org/stable/tutorial/index.html](http://scikit-learn.org/stable/tutorial/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Included in the scikit-learn documentation is a series of tutorials on data
    mining. The tutorials range from basic introductions to toy datasets, all the
    way through to comprehensive tutorials on techniques used in recent research.
    The tutorials here will take quite a while to get through—they are very comprehensive—but
    are well worth the effort to learn.
  prefs: []
  type: TYPE_NORMAL
- en: There are also a large number of algorithms that have been implemented for compatability
    with scikit-learn. These algorithms are not always included in scikit-learn itself
    for a number of reasons, but a list of many of these is maintained at [https://github.com/scikit-learn/scikit-learn/wiki/Third-party-projects-and-code-snippets](https://github.com/scikit-learn/scikit-learn/wiki/Third-party-projects-and-code-snippets).
  prefs: []
  type: TYPE_NORMAL
- en: Extending the Jupyter Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'URL: [http://ipython.org/ipython-doc/1/interactive/public_server.html](http://ipython.org/ipython-doc/1/interactive/public_server.html)'
  prefs: []
  type: TYPE_NORMAL
- en: The Jupyter Notebook is a powerful tool. It can be extended in many ways, and
    one of those is to create a server to run your Notebooks, separately from your
    main computer. This is very useful if you use a low-power main computer, such
    as a small laptop, but have more powerful computers at your disposal. In addition,
    you can set up nodes to perform parallelized computations.
  prefs: []
  type: TYPE_NORMAL
- en: More datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [http://archive.ics.uci.edu/ml/](http://archive.ics.uci.edu/ml/)
  prefs: []
  type: TYPE_NORMAL
- en: There are many datasets available on the Internet from a number of different
    sources. These include academic, commercial, and government datasets. A collection
    of well-labelled datasets is available at the UCI ML library, which is one of
    the best options to find datasets for testing your algorithms. Try out the OneR
    algorithm with some of these different datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Other Evaluation Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is a wide range of evaluation metrics for other takes. Some notable ones
    to investigate are:'
  prefs: []
  type: TYPE_NORMAL
- en: The Lift Metric: [https://en.wikipedia.org/wiki/Lift_(data_mining)](https://en.wikipedia.org/wiki/Lift_(data_mining))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segment evaluation metrics: [http://segeval.readthedocs.io/en/latest/](http://segeval.readthedocs.io/en/latest/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pearson's Correlation Coefficient: [https://en.wikipedia.org/wiki/Pearson_correlation_coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Area under the ROC Curve: [http://gim.unmc.edu/dxtests/roc3.htm](http://gim.unmc.edu/dxtests/roc3.htm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalized Mutual Information: [http://scikit-learn.org/stable/modules/clustering.html#mutual-info-score](http://scikit-learn.org/stable/modules/clustering.html#mutual-info-score)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these metrics was developed with a particular application in mind. For
    example, the segment evaluation metrics evaluate how accurate breaking a document
    of text into chunks is, allowing for some variation between chunk boundaries.
    A good understanding of where evaluation metrics can be applied and where they
    can not is critical to ongoing success in data mining.
  prefs: []
  type: TYPE_NORMAL
- en: More application ideas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [https://datapipeline.com.au/](https://datapipeline.com.au/)
  prefs: []
  type: TYPE_NORMAL
- en: If you are looking for more ideas on data mining applications, specifically
    those for businesses, check out my company's blog. I post regularly about applications
    of data mining, focusing on practical outcomes for businesses.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying with scikit-learn Estimators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A naïve implementation of the nearest neighbor algorithm is quite slow—it checks
    all pairs of points to find those that are close together. Better implementations
    exist, with some implemented in scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability with the nearest neighbor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'URL: [https://github.com/jnothman/scikit-learn/tree/pr2532](https://github.com/jnothman/scikit-learn/tree/pr2532)'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a kd-tree can be created that speeds up the algorithm (and this
    is already included in scikit-learn).
  prefs: []
  type: TYPE_NORMAL
- en: Another way to speed up this search is to use locality-sensitive hashing,  Locality-Sensitive
    Hashing (LSH). This is a proposed improvement for scikit-learn, and hasn't made
    it into the package at the time of writing. The preceding link gives a development
    branch of scikit-learn that will allow you to test out LSH on a dataset. Read
    through the documentation attached to this branch for details on doing this.
  prefs: []
  type: TYPE_NORMAL
- en: To install it, clone the repository and follow the instructions to install the
    Bleeding Edge code available at[ http://scikit-learn.org/stable/install.html](http://scikit-learn.org/stable/install.html) on
    your computer. Remember to use the repository's code rather than the official
    source. I recommend that you use Anaconda for playing around with bleeding-edge
    packages so that they don't interfere with other libraries on your system.
  prefs: []
  type: TYPE_NORMAL
- en: More complex pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [http://scikit-learn.org/stable/modules/pipeline.html#featureunion-composite-feature-spaces](http://scikit-learn.org/stable/modules/pipeline.html#featureunion-composite-feature-spaces)
  prefs: []
  type: TYPE_NORMAL
- en: The Pipelines we have used here follow a single stream—the output of one step
    is the input of another step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipelines follow the transformer and estimator interfaces as well—this allows
    us to embed Pipelines within Pipelines. This is a useful construct for very complex
    models, but becomes very powerful when combined with Feature Unions, as shown
    in the preceding link.This allows us to extract multiple types of features at
    a time and then combine them to form a single dataset. For more details, see this
    example: [http://scikit-learn.org/stable/auto_examples/feature_stacker.html](http://scikit-learn.org/stable/auto_examples/feature_stacker.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are lots of classifiers in scikit-learn that are ready to use. The one
    you choose for a particular task is going to be based on a variety of factors.
    You can compare the f1-score to see which method is better, and you can investigate
    the deviation of those scores to see if that result is statistically significant.
  prefs: []
  type: TYPE_NORMAL
- en: An important factor is that they are trained and tested on the same data—that
    is, the test set for one classifier is the test set for all classifiers. Our use
    of random states allows us to ensure that this is the case—an important factor
    for replicating experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Automated Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [http://rhiever.github.io/tpot/](http://rhiever.github.io/tpot/)
  prefs: []
  type: TYPE_NORMAL
- en: URL: [https://github.com/automl/auto-sklearn](https://github.com/automl/auto-sklearn)
  prefs: []
  type: TYPE_NORMAL
- en: It's almost cheating, but these packages will investigate a wide range of possible
    models for your data mining experiments for you. This removes the need to create
    a workflow testing a large number of parameters for a larger number of classifier
    types, and lets you focus on other things, such as feature extract--still critically
    important and not yet automated!
  prefs: []
  type: TYPE_NORMAL
- en: The general idea is that you extract your features and pass the resulting matrix
    onto one of these automated classification algorithms (or regression algorithms).
    It does the search for you and even exports the best model for you. In the case
    of TPOT, it even gives you Python code to create the model from scratch without
    having to install TPOT on your server.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting Sports Winners with Decision Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [http://pandas.pydata.org/pandas-docs/stable/tutorials.html](http://pandas.pydata.org/pandas-docs/stable/tutorials.html)
  prefs: []
  type: TYPE_NORMAL
- en: The pandas library is a great package—anything you normally write to do data
    loading is probably already implemented in pandas. You can learn more about it
    from their tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also a great blog post written by Chris Moffitt that overviews common
    tasks people do in Excel and how to do them in pandas: [http://pbpython.com/excel-pandas-comp.html](http://pbpython.com/excel-pandas-comp.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also handle large datasets with pandas; see the answer, from user Jeff,
    to this StackOverflow question for an extensive overview of the process: [http://stackoverflow.com/a/14268804/307363](http://stackoverflow.com/a/14268804/307363).'
  prefs: []
  type: TYPE_NORMAL
- en: Another great tutorial on pandas is written by Brian Connelly: [http://bconnelly.net/2013/10/summarizing-data-in-python-with-pandas/](http://bconnelly.net/2013/10/summarizing-data-in-python-with-pandas/).
  prefs: []
  type: TYPE_NORMAL
- en: More complex features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [http://www.basketball-reference.com/teams/ORL/2014_roster_status.html](http://www.basketball-reference.com/teams/ORL/2014_roster_status.html)
  prefs: []
  type: TYPE_NORMAL
- en: Larger exercise!
  prefs: []
  type: TYPE_NORMAL
- en: Sports teams change regularly from game to game. An easy win for a team can
    turn into a difficult game if a couple of the best players are suddenly injured.
    You can get the team rosters from basketball-reference as well. For example, the
    roster for the 2013-2014 season for the Orlando Magic is available at the preceding
    link. Similar data is available for all NBA teams.
  prefs: []
  type: TYPE_NORMAL
- en: Writing code to integrate how much a team changes and using that to add new
    features can improve the model significantly. This task will take quite a bit
    of work though!
  prefs: []
  type: TYPE_NORMAL
- en: Dask
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [http://dask.pydata.org/en/latest/](http://dask.pydata.org/en/latest/)
  prefs: []
  type: TYPE_NORMAL
- en: If you want to take the features of pandas and increase its scalability, then
    Dask is for you. Dask provides parallelized versions of NumPy arrays, Pandas DataFrames,
    and task scheduling. Often, the interface is *nearly *the same as the original
    NumPy or Pandas versions.
  prefs: []
  type: TYPE_NORMAL
- en: Research
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'URL: [https://scholar.google.com.au/](https://scholar.google.com.au/)'
  prefs: []
  type: TYPE_NORMAL
- en: Larger exercise!As you might imagine, there has been a lot of work performed
    on predicting NBA games, as well as for all sports. Search "<SPORT> prediction"
    in Google Scholar to find research on predicting your favorite <SPORT>.
  prefs: []
  type: TYPE_NORMAL
- en: Recommending Movies Using Affinity Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many recommendation-based datasets that are worth investigating, each
    with its own issues.
  prefs: []
  type: TYPE_NORMAL
- en: New datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [http://www2.informatik.uni-freiburg.de/~cziegler/BX/](http://www2.informatik.uni-freiburg.de/~cziegler/BX/)
  prefs: []
  type: TYPE_NORMAL
- en: Larger exercise!
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many recommendation-based datasets that are worth investigating,
    each with its own issues. For example, the Book-Crossing dataset contains more
    than 278,000 users and over a million ratings. Some of these ratings are explicit
    (the user did give a rating), while others are more implicit. The weighting to
    these implicit ratings probably shouldn''t be as high as for explicit ratings.
    The music website www.last.fm has released a great dataset for music recommendation:
    [http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/.](http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also a joke recommendation dataset! See here: [http://eigentaste.berkeley.edu/dataset/.](http://eigentaste.berkeley.edu/dataset/.)'
  prefs: []
  type: TYPE_NORMAL
- en: The Eclat algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [http://www.borgelt.net/eclat.html](http://www.borgelt.net/eclat.html)
  prefs: []
  type: TYPE_NORMAL
- en: The APriori algorithm implemented here is easily the most famous of the association
    rule mining graphs, but isn't necessarily the best. Eclat is a more modern algorithm
    that can be implemented relatively easily.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative Filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [https://github.com/python-recsys](https://github.com/python-recsys)
  prefs: []
  type: TYPE_NORMAL
- en: For those wanting to got much further with recommendation engines, it is necessary
    to investigate other formats for recommendations, such as collaborative filtering.
    This library provides some background into the algorithms and implementations,
    along with some tutorials. There is also a good overview at [http://blogs.gartner.com/martin-kihn/how-to-build-a-recommender-system-in-python/](http://blogs.gartner.com/martin-kihn/how-to-build-a-recommender-system-in-python/).
  prefs: []
  type: TYPE_NORMAL
- en: Extracting Features with Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following topics, according to me, are also relevant when it comes to deeper
    understanding of Extracting Features with Transformers
  prefs: []
  type: TYPE_NORMAL
- en: Adding noise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We covered removing noise to improve features; however, improved performance
    can be obtained for some datasets by adding noise. The reason for this is simple—it
    helps stop overfitting by forcing the classifier to generalize its rules a little
    (although too much noise will make the model too general). Try implementing a
    Transformer that can add a given amount of noise to a dataset. Test that out on
    some of the datasets from UCI ML and see if it improves test-set performance.
  prefs: []
  type: TYPE_NORMAL
- en: Vowpal Wabbit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [http://hunch.net/~vw/](http://hunch.net/~vw/)
  prefs: []
  type: TYPE_NORMAL
- en: Vowpal Wabbit is a great project, providing very fast feature extraction for
    text-based problems. It comes with a Python wrapper, allowing you to call it from
    with Python code. Test it out on large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: word2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [https://radimrehurek.com/gensim/models/word2vec.html](https://radimrehurek.com/gensim/models/word2vec.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'Word embeddings are receiving a lot of interest from research and industry,
    for a good reason: they perform very well on many text mining tasks. They are
    a big more complicated than the bag-of-words model and create larger models. Word
    embeddings are great features when you have lots of data and can even help in
    some cases with smaller amounts.'
  prefs: []
  type: TYPE_NORMAL
- en: Social Media Insight Using Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Do consider the following points after finishing with Social Media Insight Using
    Native Bayes.
  prefs: []
  type: TYPE_NORMAL
- en: Spam detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter](http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)
  prefs: []
  type: TYPE_NORMAL
- en: Using the concepts here, you can create a spam detection method that is able
    to view a social media post and determine whether it is spam or not. Try this
    out by first creating a dataset of spam/not-spam posts, implementing the text
    mining algorithms, and then evaluating them.
  prefs: []
  type: TYPE_NORMAL
- en: One important consideration with spam detection is the false-positive/false-negative
    ratio. Many people would prefer to have a couple of spam messages slip through,
    rather than miss out on a legitimate message because the filter was too aggressive
    in stopping the spam. In order to turn your method for this, you can use a Grid
    Search with the f1-score as the evaluation criteria. See the preceding link for
    information on how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing and part-of-speech tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [http://www.nltk.org/book/ch05.html](http://www.nltk.org/book/ch05.html)
  prefs: []
  type: TYPE_NORMAL
- en: The techniques we used here are quite lightweight compared to some of the linguistic
    models employed in other areas. For example, part-of-speech tagging can help disambiguate
    word forms, allowing for higher accuracy. It comes with NLTK.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering Accounts to Follow Using Graph Mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Do give the following a read when done with the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: More complex algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'URL: [https://www.cs.cornell.edu/home/kleinber/link-pred.pdf](https://www.cs.cornell.edu/home/kleinber/link-pred.pdf)Larger
    exercise!'
  prefs: []
  type: TYPE_NORMAL
- en: There has been extensive research on predicting links in graphs, including for
    social networks. For instance, David Liben-Nowell and Jon Kleinberg published
    a paper on this topic that would serve as a great place for more complex algorithms,
    linked previously.
  prefs: []
  type: TYPE_NORMAL
- en: NetworkX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [https://networkx.github.io/](https://networkx.github.io/)
  prefs: []
  type: TYPE_NORMAL
- en: If you are going to be using graphs and networks more, going in-depth into the
    NetworkX package is well worth your time—the visualization options are great and
    the algorithms are well implemented. Another library called SNAP is also available
    with Python bindings, at [http://snap.stanford.edu/snappy/index.html](http://snap.stanford.edu/snappy/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: Beating CAPTCHAs with Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You may find the following topics interesting as well:'
  prefs: []
  type: TYPE_NORMAL
- en: Better (worse?) CAPTCHAs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [http://scikit-image.org/docs/dev/auto_examples/applications/plot_geometric.html](http://scikit-image.org/docs/dev/auto_examples/applications/plot_geometric.html)
  prefs: []
  type: TYPE_NORMAL
- en: Larger exercise!
  prefs: []
  type: TYPE_NORMAL
- en: 'The CAPTCHAs we beat in this example were not as complex as those normally
    used today. You can create more complex variants using a number of techniques
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying different transformations such as the ones in scikit-image (see the
    preceding link)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using different colors and colors that don't translate well to grayscale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adding lines or other shapes to the image: [http://scikit-image.org/docs/dev/api/skimage.draw.html](http://scikit-image.org/docs/dev/api/skimage.draw.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deeper networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These techniques will probably fool our current implementation, so improvements
    will need to be made to make the method better. Try some of the deeper networks
    we used. Larger networks need more data, though, so you will probably need to
    generate more than the few thousand samples we did here in order to get good performance.
    Generating these datasets is a good candidate for parallelization—lots of small
    tasks that can be performed independently.
  prefs: []
  type: TYPE_NORMAL
- en: A good idea for increasing your dataset size, which applies to other datasets
    as well, is to create variants of existing images. Flip images upside down, crop
    them weirdly, add noise, blur the image, make some random pixels black and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [http://pybrain.org/docs/tutorial/reinforcement-learning.html](http://pybrain.org/docs/tutorial/reinforcement-learning.html)
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is gaining traction as the next big thing in data mining—although
    it has been around a long time! PyBrain has some reinforcement learning algorithms
    that are worth checking out with this dataset (and others!).
  prefs: []
  type: TYPE_NORMAL
- en: Authorship Attribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to Authorship Attribution do give the following topics a read.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the sample size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Enron application we used ended up using just a portion of the overall dataset.
    There is lots more data available in this dataset. Increasing the number of authors
    will likely lead to a drop in accuracy, but it is possible to boost the accuracy
    further than was achieved here, using similar methods. Using a Grid Search, try
    different values for n-grams and different parameters for support vector machines,
    in order to get better performance on a larger number of authors.
  prefs: []
  type: TYPE_NORMAL
- en: Blogs dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset used, provides authorship-based classes (each blogger ID is a separate
    author). This dataset can be tested using this kind of method as well. In addition,
    there are the other classes of gender, age, industry, and star sign that can be
    tested—are authorship-based methods good for these classification tasks?
  prefs: []
  type: TYPE_NORMAL
- en: Local n-grams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [https://github.com/robertlayton/authorship_tutorials/blob/master/LNGTutorial.ipynb](https://github.com/robertlayton/authorship_tutorials/blob/master/LNGTutorial.ipynb)
  prefs: []
  type: TYPE_NORMAL
- en: Another form of classifier is local n-gram, which involves choosing the best
    features per-author, not globally for the entire dataset. I wrote a tutorial on
    using local n-grams for authorship attribution, available at the preceding link.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering News Articles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It won't hurt to read a little on the following topics
  prefs: []
  type: TYPE_NORMAL
- en: Clustering Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The evaluation of clustering algorithms is a difficult problem—on the one hand,
    we can sort of tell what good clusters look like; on the other hand, if we really
    know that, we should label some instances and use a supervised classifier! Much
    has been written on this topic. One slideshow on the topic that is a good introduction
    to the challenges follows: [http://www.cs.kent.edu/~jin/DM08/ClusterValidation.pdf](http://www.cs.kent.edu/~jin/DM08/ClusterValidation.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, a very comprehensive (although now a little dated) paper on this
    topic is here: [http://web.itu.edu.tr/sgunduz/courses/verimaden/paper/validity_survey.pdf.](http://web.itu.edu.tr/sgunduz/courses/verimaden/paper/validity_survey.pdf.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The scikit-learn package does implement a number of the metrics described in
    those links, with an overview here: [http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation](http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation).'
  prefs: []
  type: TYPE_NORMAL
- en: Using some of these, you can start evaluating which parameters need to be used
    for better clusterings. Using a Grid Search, we can find parameters that maximize
    a metric—just like in classification.
  prefs: []
  type: TYPE_NORMAL
- en: Temporal analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Larger exercise!
  prefs: []
  type: TYPE_NORMAL
- en: The code we developed here can be rerun over many months. By adding some tags
    to each cluster, you can track which topics stay active over time, getting a longitudinal
    viewpoint of what is being discussed in the world news. To compare the clusters,
    consider a metric such as the adjusted mutual information score, which was linked
    to the scikit-learn documentation earlier. See how the clusters change after one
    month, two months, six months, and a year.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time clusterings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-means algorithm can be iteratively trained and updated over time, rather
    than discrete analyses at given time frames. Cluster movement can be tracked in
    a number of ways—for instance, you can track which words are popular in each cluster
    and how much the centroids move per day. Keep the API limits in mind—you probably
    only need to do one check every few hours to keep your algorithm up-to-date.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying Objects in Images Using Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following topics are also important when deeper study into Classifying objects
    is considered.
  prefs: []
  type: TYPE_NORMAL
- en: Mahotas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'URL: [http://luispedro.org/software/mahotas/](http://luispedro.org/software/mahotas/)'
  prefs: []
  type: TYPE_NORMAL
- en: Another package for image processing is Mahotas, including better and more complex
    image processing techniques that can help achieve better accuracy, although they
    may come at a high computational cost. However, many image processing tasks are
    good candidates for parallelization. More techniques on image classification can
    be found in the research literature, with this survey paper as a good start: [http://ijarcce.com/upload/january/22-A%20Survey%20on%20Image%20Classification.pdf](http://ijarcce.com/upload/january/22-A%20Survey%20on%20Image%20Classification.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Other image datasets are available at [http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html).
  prefs: []
  type: TYPE_NORMAL
- en: There are many datasets of images available from a number of academic and industry-based
    sources. The linked website lists a bunch of datasets and some of the best algorithms
    to use on them. Implementing some of the better algorithms will require significant
    amounts of custom code, but the payoff can be well worth the pain.
  prefs: []
  type: TYPE_NORMAL
- en: Magenta
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [https://github.com/tensorflow/magenta/tree/master/magenta/reviews](https://github.com/tensorflow/magenta/tree/master/magenta/reviews)
  prefs: []
  type: TYPE_NORMAL
- en: This repository contains a few high-quality deep learning papers that are worth
    reading, along with in-depth reviews of the paper and their techniques. If you
    want to go deep into deep learning, check out these papers first before expanding
    outwards.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Big Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following resources on Big Data would be helpful
  prefs: []
  type: TYPE_NORMAL
- en: Courses on Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both Yahoo and Google have great tutorials on Hadoop, which go from beginner
    to quite advanced levels. They don't specifically address using Python, but learning
    the Hadoop concepts and then applying them in Pydoop or a similar library can
    yield great results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yahoo''s tutorial: [https://developer.yahoo.com/hadoop/tutorial/](https://developer.yahoo.com/hadoop/tutorial/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Google''s tutorial: [https://cloud.google.com/hadoop/what-is-hadoop](https://cloud.google.com/hadoop/what-is-hadoop)'
  prefs: []
  type: TYPE_NORMAL
- en: Pydoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [http://crs4.github.io/pydoop/tutorial/index.html](http://crs4.github.io/pydoop/tutorial/index.html)
  prefs: []
  type: TYPE_NORMAL
- en: Pydoop is a python library to run Hadoop jobs. Pydoop also works with HDFS,
    the Hadoop File System, although you can get that functionality in mrjob as well.
    Pydoop will give you a bit more control over running some jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Recommendation engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building a large recommendation engine is a good test of your Big data skills.
    A great blog post by Mark Litwintschik covers an engine using Apache Spark, a
    big data technology: [http://tech.marksblogg.com/recommendation-engine-spark-python.html](http://tech.marksblogg.com/recommendation-engine-spark-python.html)'
  prefs: []
  type: TYPE_NORMAL
- en: W.I.L.L
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'URL: [https://github.com/ironman5366/W.I.L.L](https://github.com/ironman5366/W.I.L.L)'
  prefs: []
  type: TYPE_NORMAL
- en: Very large project!
  prefs: []
  type: TYPE_NORMAL
- en: This open source personal assistant can be your next JARVIS from Iron Man. You
    can add to this project using data mining techniques to allow it to learn to do
    some tasks that you need to do regularly. This is not easy, but the potential
    productivity gains are worth it.
  prefs: []
  type: TYPE_NORMAL
- en: More resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following would serve as a really good resource for additional information:'
  prefs: []
  type: TYPE_NORMAL
- en: Kaggle competitions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [www.kaggle.com/](http://www.kaggle.com/)
  prefs: []
  type: TYPE_NORMAL
- en: Kaggle runs data mining competitions regularly, often with monetary prizes.
  prefs: []
  type: TYPE_NORMAL
- en: Testing your skills on Kaggle competitions is a fast and great way to learn
    to work with real-world data mining problems. The forums are nice and share environments—often,
    you will see code released for a top-10 entry during the competition!
  prefs: []
  type: TYPE_NORMAL
- en: Coursera
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: URL: [www.coursera.org](http://www.coursera.org)
  prefs: []
  type: TYPE_NORMAL
- en: 'Coursera contains many courses on data mining and data science. Many of the
    courses are specialized, such as big data and image processing. A great general
    one to start with is Andrew Ng''s famous course: [https://www.coursera.org/learn/machine-learning/](https://www.coursera.org/learn/machine-learning/).'
  prefs: []
  type: TYPE_NORMAL
- en: It is a bit more advanced than this and would be a great next step for interested
    readers.
  prefs: []
  type: TYPE_NORMAL
- en: 'For neural networks, check out this course: [https://www.coursera.org/course/neuralnets](https://www.coursera.org/course/neuralnets).'
  prefs: []
  type: TYPE_NORMAL
- en: If you complete all of these, try out the course on probabilistic graphical
    models at [https://www.coursera.org/course/pgm](https://www.coursera.org/course/pgm).
  prefs: []
  type: TYPE_NORMAL
