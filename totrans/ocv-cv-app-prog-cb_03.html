<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;3.&#xA0;Processing Color Images with Classes"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03" class="calibre1"/>Chapter 3. Processing Color Images with Classes</h1></div></div></div><p class="calibre8">In this chapter, we will cover the following recipes:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Using the Strategy pattern in an algorithm design</li><li class="listitem">Using a Controller design pattern to communicate with processing modules</li><li class="listitem">Converting color representations</li><li class="listitem">Representing colors with hue, saturation, and brightness</li></ul></div></div>

<div class="book" title="Chapter&#xA0;3.&#xA0;Processing Color Images with Classes">
<div class="book" title="Introduction"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch03lvl1sec21" class="calibre1"/>Introduction</h1></div></div></div><p class="calibre8">Good computer vision programs begin with good programming practices. Building a bug-free application is just the beginning. What you really want is an application that you and the programmers working with you will be able to easily adapt and evolve as new requirements come in. This chapter will show you how to make the best use of some of the object-oriented programming principles in order to build good quality software programs. In particular, we will introduce a few important design patterns that will help you build applications with components that are easy to test, maintain, and reuse.</p><p class="calibre8">Design patterns are a well-known concept in software engineering. Basically, a design pattern is a sound, reusable solution to a generic problem that occurs frequently in software designing. Many software patterns have been introduced and well documented. Good programmers should build a working knowledge of these existing patterns.</p><p class="calibre8">This chapter also has a secondary objective. It will teach you how to play with image colors. The example used throughout this chapter will show you how to detect the pixels of a given color, and the last two recipes will explain how to work with different color spaces.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Using the Strategy pattern in an algorithm design"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec22" class="calibre1"/>Using the Strategy pattern in an algorithm design</h1></div></div></div><p class="calibre8">The objective of the Strategy design pattern<a id="id184" class="calibre1"/> is to encapsulate an algorithm in a class. This way, it becomes easier to replace a given algorithm <a id="id185" class="calibre1"/>by another one or to chain several algorithms together in order to build a more complex process. In addition, this pattern facilitates the deployment of an algorithm by hiding as much of its complexity as possible behind an intuitive programming interface.</p></div>

<div class="book" title="Using the Strategy pattern in an algorithm design">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec57" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre8">Let's say we <a id="id186" class="calibre1"/>want to build a simple algorithm that will identify all of the pixels in an image that have a given color. For this, the algorithm has to accept an image and a color as input and will return a binary image showing the pixels that have the specified color. The tolerance with which we want to accept a color will be another parameter to be specified before running the algorithm.</p></div></div>

<div class="book" title="Using the Strategy pattern in an algorithm design">
<div class="book" title="How to do it…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec58" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre8">Once an algorithm has been encapsulated in a class using the Strategy design pattern, it can be deployed by creating an instance of this class. Typically, the instance will be created when the program is initialized. At the time of construction, the class instance will initialize the different parameters of the algorithm with their default values such that it will immediately be ready to be used. The algorithm's parameter values can also be read and set using appropriate methods. In the case of an application with a GUI, these parameters can be displayed and modified using different widgets (text fields, sliders, and so on) so that a user can easily play with them.</p><p class="calibre8">We will show you the structure of a Strategy class in the next section; let's start with an example on how it can be deployed and used. Let's write a simple main function that will run our proposed color detection algorithm:</p><div class="informalexample"><pre class="programlisting">int main()
{
   // 1. Create image processor object
   ColorDetector cdetect;

   // 2. Read input image
   cv::Mat image= cv::imread("boldt.jpg");
   if (image.empty())
      return 0;
   // 3. Set input parameters
   cdetect.setTargetColor(230,190,130); // here blue sky

   cv::namedWindow("result");

   // 4. Process the image and display the result
   cv::imshow("result",cdetect.process(image));

   cv::waitKey();

   return 0;
}</pre></div><p class="calibre8">Running <a id="id187" class="calibre1"/>this program to detect a blue sky <a id="id188" class="calibre1"/>in the colored version of the <span class="strong"><em class="calibre9">Castle</em></span> image presented in the previous chapter produces the following output:</p><div class="mediaobject"><img src="../images/00019.jpeg" alt="How to do it…" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, a white pixel indicates a positive detection of the sought color, and black indicates negative.</p><p class="calibre8">Obviously, the <a id="id189" class="calibre1"/>algorithm we encapsulated in this class is relatively simple (as we will see next, it is composed of just one scanning loop and one tolerance parameter). The Strategy design pattern becomes really powerful when the algorithm to be implemented is more complex, has many steps, and includes several parameters.</p></div></div>

<div class="book" title="Using the Strategy pattern in an algorithm design">
<div class="book" title="How it works…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch03lvl2sec59" class="calibre1"/>How it works…</h2></div></div></div><p class="calibre8">The core <a id="id190" class="calibre1"/>process of this algorithm is easy to build. It is a simple scanning loop that goes over each pixel, comparing its color with the target color. Using what we learned in the <span class="strong"><em class="calibre9">Scanning an image with iterators</em></span> recipe of the previous chapter, this loop can be written as follows:</p><div class="informalexample"><pre class="programlisting">     // get the iterators
     cv::Mat_&lt;cv::Vec3b&gt;::const_iterator it= 
                           image.begin&lt;cv::Vec3b&gt;();
     cv::Mat_&lt;cv::Vec3b&gt;::const_iterator itend= 
                           image.end&lt;cv::Vec3b&gt;();
     cv::Mat_&lt;uchar&gt;::iterator itout= result.begin&lt;uchar&gt;();

     // for each pixel
     for ( ; it!= itend; ++it, ++itout) {

            // compute distance from target color
            if (getDistanceToTargetColor(*it)&lt;=maxDist) {
                 *itout= 255;
            } else {
                 *itout= 0;
            }
    }</pre></div><p class="calibre8">The <code class="email">cv::Mat</code> variable's <code class="email">image</code> refers to the input image, while <code class="email">result</code> refers to the binary output image. Therefore, the first step consists of setting up the required iterators. The scanning loop then becomes easy to implement. The distance between the current pixel color and the target color is evaluated on each iteration in order to check whether it is within the tolerance parameter defined by <code class="email">maxDist</code>. If that is the case, the value <code class="email">255</code> (white) is then assigned to the output image; if not, <code class="email">0</code> (black) is assigned. To compute the distance to the target color, the <code class="email">getDistanceToTargetColor</code> method is used. There are different ways to compute this distance. One could, for example, calculate the Euclidean distance between the three vectors that contain the RGB color values. To keep this computation simple, we<a id="id191" class="calibre1"/> simply sum the absolute differences of the RGB values (this is also known as the city-block distance) in our case. Note that in modern architecture, a floating-point Euclidean distance can be faster to compute than a simple city-block distance; this is also something to take into consideration in your design. Also, for more flexibility, we write the <code class="email">getDistanceToTargetColor</code> method in terms of a <code class="email">getColorDistance</code> method, as follows:</p><div class="informalexample"><pre class="programlisting">// Computes the distance from target color.
int getDistanceToTargetColor(const cv::Vec3b&amp; color) const {
  return getColorDistance(color, target);
}
// Computes the city-block distance between two colors.
int getColorDistance(const cv::Vec3b&amp; color1, 
                     const cv::Vec3b&amp; color2) const {
  return abs(color1[0]-color2[0])+
                abs(color1[1]-color2[1])+
                abs(color1[2]-color2[2]);
}</pre></div><p class="calibre8">Note how we used <code class="email">cv::Vec3d</code> to hold the three unsigned char that represent the RGB values <a id="id192" class="calibre1"/>of a color. The <code class="email">target</code> variable obviously refers to the specified target color, and as we will see, it is defined as a member variable in the class algorithm that we will define. Now, let's complete the definition of the processing method. Users will provide an input image, and the result will be returned once the image scanning is completed:</p><div class="informalexample"><pre class="programlisting">cv::Mat ColorDetector::process(const cv::Mat &amp;image) {

     // re-allocate binary map if necessary
     // same size as input image, but 1-channel
     result.create(image.size(),CV_8U);
     // processing loop above goes here
      ...

     return result;
}</pre></div><p class="calibre8">Each time this method is called, it is important to check if the output image that contains the resulting binary map needs to be reallocated to fit the size of the input image. This is why we use the <code class="email">create</code> method of <code class="email">cv::Mat</code>. Remember that this method will only proceed to reallocation if the specified size or depth do not correspond to the current image structure.</p><p class="calibre8">Now that we have the core processing method defined, let's see what additional methods should be added in order to deploy this algorithm. We have previously determined what input and output data our algorithm requires. Therefore, we will first define the class attributes that will hold this data:</p><div class="informalexample"><pre class="programlisting">class ColorDetector {

  private:

     // minimum acceptable distance
     int maxDist; 

     // target color
     cv::Vec3b target;

     // image containing resulting binary map
     cv::Mat result;</pre></div><p class="calibre8">In order to<a id="id193" class="calibre1"/> create an instance of the class that encapsulates our algorithm (which we have named <code class="email">ColorDetector</code>), we need to define a <a id="id194" class="calibre1"/>constructor. Remember that one of the objectives of the Strategy design pattern is to make algorithm deployment as easy as possible. The simplest constructor that can be defined is an empty one. It will create an instance of the class algorithm in a valid state. We then want the constructor to initialize all the input parameters to their default values (or the values that are known to generally give a good result). In our case, we decided that a distance of <code class="email">100</code> is generally an acceptable tolerance parameter. We also set the default target color. We chose black for no particular reason. The idea is to make sure we always start with predictable and valid input values:</p><div class="informalexample"><pre class="programlisting">     // empty constructor
     // default parameter initialization here
     ColorDetector() : maxDist(100), target(0,0,0) {}</pre></div><p class="calibre8">At this point, a user who creates an instance of our class algorithm can immediately call the process method with a valid image and obtain a valid output. This is another objective of the Strategy pattern, that is, to make sure that the algorithm always runs with valid parameters. Obviously, the users of this class will want to use their own settings. This is done by providing the user with the appropriate getters and setters. Let's start with the <code class="email">color</code> tolerance parameter:</p><div class="informalexample"><pre class="programlisting">     // Sets the color distance threshold.
     // Threshold must be positive, 
     // otherwise distance threshold is set to 0.
     void setColorDistanceThreshold(int distance) {

        if (distance&lt;0)
           distance=0;
        maxDist= distance;
     }

     // Gets the color distance threshold
     int getColorDistanceThreshold() const {

        return maxDist;
     }</pre></div><p class="calibre8">Note how <a id="id195" class="calibre1"/>we first check the validity of the input. Again, this <a id="id196" class="calibre1"/>is to make sure that our algorithm will never be run in an invalid state. The target color can be set in a similar manner as follows:</p><div class="informalexample"><pre class="programlisting">     // Sets the color to be detected
     void setTargetColor(uchar blue, 
                         uchar green, 
                         uchar red) {
       // BGR order
       target = cv::Vec3b(blue, green, red);
     }

       // Sets the color to be detected
     void setTargetColor(cv::Vec3b color) {

     target= color;
     }

       // Gets the color to be detected
     cv::Vec3b getTargetColor() const {

       return target;
     }</pre></div><p class="calibre8">This time it is interesting to note that we have provided the user with two definitions of the <code class="email">setTargetColor</code> method. In the first version of the definition, the three color components are specified as three arguments, while in the second version, <code class="email">cv::Vec3b</code> is used to hold the color values. Again, the objective is to facilitate the use of our class algorithm. The user can simply select the setter that best fits their needs.</p></div></div>

<div class="book" title="Using the Strategy pattern in an algorithm design">
<div class="book" title="There's more…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch03lvl2sec60" class="calibre1"/>There's more…</h2></div></div></div><p class="calibre8">This recipe introduced you to the idea of encapsulating an algorithm in a class using the Strategy design pattern. The example algorithm used in this recipe consisted of identifying the pixels of an image that has a color sufficiently close to a specified target color. This computation could have been done otherwise. Also, the implementation of a Strategy design pattern could be complemented using function objects.</p><div class="book" title="Computing the distance between two color vectors"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch03lvl3sec16" class="calibre1"/>Computing the distance between two color vectors</h3></div></div></div><p class="calibre8">To <a id="id197" class="calibre1"/>compute the distance between two color vectors, we used the following simple formula:</p><div class="informalexample"><pre class="programlisting">return abs(color[0]-target[0])+
       abs(color[1]-target[1])+
       abs(color[2]-target[2]);</pre></div><p class="calibre8">However, OpenCV includes a function to compute the Euclidean norm of a vector. Consequently, we could have computed our distance as follows:</p><div class="informalexample"><pre class="programlisting">return static_cast&lt;int&gt;(
   cv::norm&lt;int,3&gt;(cv::Vec3i(color[0]-target[0],
                             color[1]-target[1],
                             color[2]-target[2])));</pre></div><p class="calibre8">A very similar result would then be obtained using this definition of the <code class="email">getDistance</code> method. Here, we use <code class="email">cv::Vec3i</code> (a 3-vector array of integers) because the result of the subtraction is an integer value.</p><p class="calibre8">It is also interesting to recall from <a class="calibre1" title="Chapter 2. Manipulating Pixels" href="part0019_split_000.html#page">Chapter 2</a>, <span class="strong"><em class="calibre9">Manipulating Pixels</em></span>, that the OpenCV matrix and vector data structures include a definition of the basic arithmetic operators. Consequently, one could have proposed the following definition for the distance computation:</p><div class="informalexample"><pre class="programlisting">return static_cast&lt;int&gt;(
   cv::norm&lt;uchar,3&gt;(color-target)); // wrong!</pre></div><p class="calibre8">This definition may look right at the first glance; however, it is wrong. This is because all these operators always include a call to <code class="email">saturate_cast</code> (see the <span class="strong"><em class="calibre9">Scanning an image with neighbor access</em></span> recipe in the previous chapter) in order to ensure that the results stay within the domain of the input type (here, it is <code class="email">uchar</code>). Therefore, in the cases where the target value is greater than the corresponding color value, the value <code class="email">0</code> will be assigned instead of the negative value that one would have expected. A correct formulation would then be as follows:</p><div class="informalexample"><pre class="programlisting">   cv::Vec3b dist;
   cv::absdiff(color,target,dist);
   return cv::sum(dist)[0];</pre></div><p class="calibre8">However, using two function calls to compute the distance between two 3-vector arrays is inefficient.</p></div><div class="book" title="Using OpenCV functions"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch03lvl3sec17" class="calibre1"/>Using OpenCV functions</h3></div></div></div><p class="calibre8">In this<a id="id198" class="calibre1"/> recipe, we used a loop with iterators in order to perform our computation. Alternatively, we could have achieved the same result by calling a sequence of OpenCV functions. The color detection method will then be written as follows:</p><div class="informalexample"><pre class="programlisting">  cv::Mat ColorDetector::process(const cv::Mat &amp;image) {
  
         cv::Mat output;
         // compute absolute difference with target color
         cv::absdiff(image,cv::Scalar(target),output);
         // split the channels into 3 images
         std::vector&lt;cv::Mat&gt; images;
         cv::split(output,images);
         // add the 3 channels (saturation might occurs here)
         output= images[0]+images[1]+images[2];
         // apply threshold
         cv::threshold(output,  // input image
                       output,  // output image
                       maxDist, // threshold (must be &lt; 256)
                       255,     // max value
         cv::THRESH_BINARY_INV); // thresholding mode
  
         return output;
   }</pre></div><p class="calibre8">This method uses the <code class="email">absdiff</code> function that computes the absolute difference between the pixels of an image and, in this case, a scalar value. Instead of a scalar value, another image can be provided as the second argument to this function. In the latter case, a pixel-by-pixel difference will be applied; consequently, the two images must be of the same size. The individual channels of the difference image are then extracted using the <code class="email">split</code> function (discussed in the <span class="strong"><em class="calibre9">There's more…</em></span> section of the <span class="strong"><em class="calibre9">Performing simple image arithmetic</em></span> recipe of <a class="calibre1" title="Chapter 2. Manipulating Pixels" href="part0019_split_000.html#page">Chapter 2</a>, <span class="strong"><em class="calibre9">Manipulating Pixels</em></span>) in order to be able to add them together. It is important to note that the result of this sum may sometimes be greater than <code class="email">255</code>, but because saturation is always applied, the result will be stopped at <code class="email">255</code>. The consequence is that with this version, the <code class="email">maxDist</code> parameter must also be less than <code class="email">256</code>; this should be corrected if you consider this behavior unacceptable. The last step is to create a binary image by using the threshold function. This function is commonly used to compare all the pixels with a threshold value (the third parameter), and in the regular thresholding mode (<code class="email">cv::THRESH_BINARY</code>), it assigns the defined maximum value (the fourth parameter) to all the pixels greater than threshold and <code class="email">0</code>. Here, we used the inverse mode (<code class="email">cv::THRESH_BINARY_INV</code>) in which the defined maximum value is assigned to the pixels that have a value lower than or equal to the threshold. Of interest are also the <code class="email">cv::THRESH_TOZERO_INV</code> and <code class="email">cv::THRESH_TOZERO_INV</code> modes, which leave the pixels greater than or lower than the threshold unchanged.</p><p class="calibre8">Using the <a id="id199" class="calibre1"/>OpenCV functions is always a good idea. You can then quickly build complex applications and potentially reduce the number of bugs. The result is often more efficient (thanks to the optimization efforts invested by the OpenCV contributors). However, when many intermediate steps are performed, you may find that the resulting method consumes more memory.</p></div><div class="book" title="The functor or function object"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch03lvl3sec18" class="calibre1"/>The functor or function object</h3></div></div></div><p class="calibre8">Using<a id="id200" class="calibre1"/> the C++ operator overloading, it is possible to create a class for which its instances behave as functions. The idea is to overload the <code class="email">operator()</code> method such that a call to the processing method of a class behaves exactly like a simple function call. The resulting class<a id="id201" class="calibre1"/> instance is called a function object or a <span class="strong"><strong class="calibre2">functor</strong></span>. Often, a functor includes a full constructor such that it can be used immediately after being created. For example, you can add the following constructor to your <code class="email">ColorDetector</code> class:</p><div class="informalexample"><pre class="programlisting">  // full constructor
  ColorDetector(uchar blue, uchar green, uchar red, 
                int maxDist=100): maxDist(maxDist) { 

    // target color
    setTargetColor(blue, green, red);
  }</pre></div><p class="calibre8">Obviously, you can still use the setters and getters that have been defined previously. The functor method can be defined as follows:</p><div class="informalexample"><pre class="programlisting">  cv::Mat operator()(const cv::Mat &amp;image) {

    // color detection code here …
  }</pre></div><p class="calibre8">To detect a given color with this functor method, simply write the following code snippet:</p><div class="informalexample"><pre class="programlisting">ColorDetector colordetector(230,190,130,  // color
                                    100); // threshold
cv::Mat result= colordetector(image);   // functor call</pre></div><p class="calibre8">As you can see, the call to the color detection method now looks like a function call. As a matter of fact, the <code class="email">colordetector</code> variable can be used as if it were the name of a function.</p></div></div></div>

<div class="book" title="Using the Strategy pattern in an algorithm design">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch03lvl2sec61" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">The policy-based class design, introduced by A. Alexandrescu, is an interesting variant of the Strategy design pattern in which algorithms are selected at compile time</li><li class="listitem">The <span class="strong"><em class="calibre9">Design Patterns: Elements of Reusable Object-Oriented Software</em></span>, <span class="strong"><em class="calibre9">Erich Gamma et al, Addison-Wesley, 1994</em></span>, is one of the classic books on the subject</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Using a Controller design pattern to communicate with processing modules"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec23" class="calibre1"/>Using a Controller design pattern to communicate with processing modules</h1></div></div></div><p class="calibre8">As you build <a id="id202" class="calibre1"/>more complex applications, you will need to create multiple algorithms that can be combined together in order to accomplish some advanced tasks. Consequently, to properly set up the<a id="id203" class="calibre1"/> application and have all the classes communicate together will become more and more complex. It then becomes advantageous to centralize the control of the application in a single class. This is the idea behind the Controller design pattern. A Controller is a particular object that plays a central role in an application, and we will explore this in this recipe.</p></div>

<div class="book" title="Using a Controller design pattern to communicate with processing modules">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec62" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre8">Using your favorite IDE, create a simple dialog-based application with two buttons; one button to select an image, and another button to start the processing, shown as follows:</p><div class="mediaobject"><img src="../images/00020.jpeg" alt="Getting ready" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, we use the <code class="email">ColorDetector</code> class of the previous recipe.</p></div></div>

<div class="book" title="Using a Controller design pattern to communicate with processing modules">
<div class="book" title="How to do it…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec63" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre8">The role of the <code class="email">Controller</code> class<a id="id204" class="calibre1"/> is to first create the classes required to execute the application. Here, there<a id="id205" class="calibre1"/> is<a id="id206" class="calibre1"/> only one class, but in a more complex application, several classes would be created. In addition, we need two member variables in order to hold a reference to the input and output results:</p><div class="informalexample"><pre class="programlisting">class ColorDetectController {

  private:

   // the algorithm class
   ColorDetector *cdetect;

   cv::Mat image;   // The image to be processed
   cv::Mat result;  // The image result

  public:

   ColorDetectController() { 

        //setting up the application
        cdetect= new ColorDetector();
   }</pre></div><p class="calibre8">Here, we chose to use a dynamic allocation for our class; you can also simply declare a class variable. You <a id="id207" class="calibre1"/>then <a id="id208" class="calibre1"/>need to define all of the setters and getters that a user would need to control the application:</p><div class="informalexample"><pre class="programlisting">     // Sets the color distance threshold
     void setColorDistanceThreshold(int distance) {

        cdetect-&gt;setColorDistanceThreshold(distance);
     }

     // Gets the color distance threshold
     int getColorDistanceThreshold() const {

        return cdetect-&gt;getColorDistanceThreshold();
     }

     // Sets the color to be detected
     void setTargetColor(unsigned char red, 
        unsigned char green, unsigned char blue) {	
             cdetect-&gt;setTargetColor(blue,green,red);
     }

     // Gets the color to be detected
     void getTargetColor(unsigned char &amp;red, 
        unsigned char &amp;green, unsigned char &amp;blue) const {

        cv::Vec3b color= cdetect-&gt;getTargetColor();

        red= color[2];
        green= color[1];
        blue= color[0];
     }

     // Sets the input image. Reads it from file.
     bool setInputImage(std::string filename) {

        image= cv::imread(filename);

        return !image.empty();
     }

     // Returns the current input image.
     const cv::Mat getInputImage() const {

        return image;
     }</pre></div><p class="calibre8">You also need a method, which will be invoked, to start the process:</p><div class="informalexample"><pre class="programlisting">     // Performs image processing.
     void process() {

        result= cdetect-&gt;process(image);
     }</pre></div><p class="calibre8">Moreover, you<a id="id209" class="calibre1"/> will need a method to obtain the result of the processing:</p><div class="informalexample"><pre class="programlisting">     // Returns the image result from the latest processing.
     const cv::Mat getLastResult() const {

        return result;
     }</pre></div><p class="calibre8">Finally, it is important to clean up everything when the application terminates (and the <code class="email">Controller</code> class is released):</p><div class="informalexample"><pre class="programlisting">     // Deletes processor objects created by the controller.
     ~ColorDetectController() {

        delete cdetect; // release memory of dynamically
     }                  // allocated class instance</pre></div></div></div>

<div class="book" title="Using a Controller design pattern to communicate with processing modules">
<div class="book" title="How it works…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch03lvl2sec64" class="calibre1"/>How it works…</h2></div></div></div><p class="calibre8">Using the previously mentioned <code class="email">Controller</code> class, a programmer can easily build an interface for <a id="id210" class="calibre1"/>an application that will execute your algorithm. There is no need for the programmer to understand how all the classes are connected together or to find out which methods in which class must be called to have everything running properly. All this is done by the <code class="email">Controller</code> class. The only requirement is to create an instance of the <code class="email">Controller</code> class.</p><p class="calibre8">The setters and getters that are defined in the <code class="email">Controller</code> class are the ones that are required to deploy your algorithm. Most often, these methods simply call the corresponding ones in the appropriate class. The simple example used here includes only one class algorithm, but in general, several class instances will be involved. Therefore, the role of <code class="email">Controller</code> is to redirect the request to the appropriate class (in object-oriented programming, this mechanism is called delegation). Another objective of the Controller pattern is to simplify the interface for the application classes. As an example of such simplification, consider the <code class="email">setTargetColor</code> and <code class="email">getTargetColor</code> methods. Both use <code class="email">uchar</code> to set and get the color of interest. This eliminates the necessity for the application programmer to know anything about the <code class="email">cv::Vec3b</code> class.</p><p class="calibre8">In some cases, the Controller also <a id="id211" class="calibre1"/>prepares the data provided by the application programmer. This is<a id="id212" class="calibre1"/> what we did in the case of the <code class="email">setInputImage</code> method, in which the image that corresponds to the given filename is loaded in the memory. The method returns <code class="email">true</code> or <code class="email">false</code> depending on whether the loading operation was successful (an exception could also have been thrown to handle this situation).</p><p class="calibre8">Finally, the <code class="email">process</code> method is the one that runs the algorithm. This method does not return a result, and another method must be called in order to get the result of the latest processing performed.</p><p class="calibre8">Now, to create a very basic dialog-based application using this controller, just add a <code class="email">ColorDetectController</code> member variable to the dialog class (called <code class="email">colordetect</code> here). As an example, using the MS Visual Studio framework, the <code class="email">Open button</code> callback method of an MFC dialog would look as follows:</p><div class="informalexample"><pre class="programlisting">// Callback method of "Open" button.
void OnOpen()
{
    // MFC widget to select a file of type bmp or jpg
    CFileDialog dlg(TRUE, _T("*.bmp"), NULL,
     OFN_FILEMUSTEXIST|OFN_PATHMUSTEXIST|OFN_HIDEREADONLY,
     _T("image files (*.bmp; *.jpg) 
         |*.bmp;*.jpg|All Files (*.*)|*.*||"),NULL);

    dlg.m_ofn.lpstrTitle= _T("Open Image");

    // if a filename has been selected
    if (dlg.DoModal() == IDOK) {

      // get the path of the selected filename
      std::string filename= dlg.GetPathName();  

      // set and display the input image
      colordetect.setInputImage(filename);
      cv::imshow("Input Image",colordetect.getInputImage());
    }
}</pre></div><p class="calibre8">The second button executes the <code class="email">Process</code> method and displays the result as follows:</p><div class="informalexample"><pre class="programlisting">// Callback method of "Process" button.
void OnProcess()
{
   // target color is hard-coded here
   colordetect.setTargetColor(130,190,230);
   // process the input image and display result
   colordetect.process();
   cv::imshow("Output Result",colordetect.getLastResult());
}</pre></div><p class="calibre8">Obviously, a <a id="id213" class="calibre1"/>more complete<a id="id214" class="calibre1"/> application would include additional widgets in order to allow the user to set the algorithm parameters.</p></div></div>

<div class="book" title="Using a Controller design pattern to communicate with processing modules">
<div class="book" title="There's more…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch03lvl2sec65" class="calibre1"/>There's more…</h2></div></div></div><p class="calibre8">When you build an application, always take the time to structure it such that it will be easy to maintain and evolve. There exist a number of architectural patterns that can help you meet this objective.</p><div class="book" title="The Model-View-Controller architecture"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch03lvl3sec19" class="calibre1"/>The Model-View-Controller architecture</h3></div></div></div><p class="calibre8">The <a id="id215" class="calibre1"/>
<span class="strong"><strong class="calibre2">Model-View-Controller</strong></span> (<span class="strong"><strong class="calibre2">MVC</strong></span>) architecture has the objective to produce an application that clearly separates the application logic from the user interface. As the name suggests, the MVC pattern involves three main components.</p><p class="calibre8">The <a id="id216" class="calibre1"/>
<span class="strong"><strong class="calibre2">Model</strong></span><a id="id217" class="calibre1"/> contains information concerning the application. It holds all the data that is processed by the application. When new data is produced, it will inform the Controller (often asynchronously), which in turn will ask the view to display the new results. Often, the Model will group together several algorithms, possibly implemented following the Strategy pattern. All these algorithms are a part of the Model.</p><p class="calibre8">The <a id="id218" class="calibre1"/>
<span class="strong"><strong class="calibre2">View</strong></span><a id="id219" class="calibre1"/> corresponds to the user interface. It is composed of the different widgets that present the data to the user and allow the user to interact with the application. One of its roles is to send the commands issued by the user to the Controller. When new data is available, it refreshes itself in order to display the new information.</p><p class="calibre8">The <a id="id220" class="calibre1"/>
<span class="strong"><strong class="calibre2">Controller</strong></span><a id="id221" class="calibre1"/> is the module that bridges the View and the Model together. It receives requests from the View and relays them to the appropriate methods in the model. It is also informed when the Model changes its state; consequently, the Controller asks the View to refresh in order to display this new information.</p><p class="calibre8">Under the MVC architecture, the user interface calls the Controller methods. It does not contain any application data and does not implement any application logic. Consequently, it is easy to substitute an interface with another one. The designer of the GUI does not need to understand the functioning of the application. Reciprocally, the application logic can be modified without the GUI being affected.</p></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Converting color representations"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec24" class="calibre1"/>Converting color representations</h1></div></div></div><p class="calibre8">The earlier <a id="id222" class="calibre1"/>recipes taught you how to encapsulate an algorithm into a class. This way, the algorithm becomes easier to use through a simplified interface. Encapsulation also permits you to modify an algorithm's implementation without impacting the classes that use it. This principle is illustrated in the next recipe, where we will modify the <code class="email">ColorDetector</code> class algorithm in order to use another color space. Therefore, this recipe will also be an opportunity to introduce color conversions with OpenCV.</p></div>

<div class="book" title="Converting color representations">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec66" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre8">The RGB color space<a id="id223" class="calibre1"/> is based on the use of the red, green, and blue additive primary colors. These have been selected because when they are combined together, they can produce a wide gamut of different colors. In fact, the human visual system is also based on the trichromatic perception of colors, with cone cell sensitivity located around the red, green, and blue spectrum. It is often the default color space in digital imagery because that is the way they are acquired. Captured light goes through the red, green, and blue filters. Additionally, in digital images, the red, green, and blue channels are adjusted such that when combined in equal amounts, a gray-level intensity is obtained, that is, from black <code class="email">(0,0,0)</code> to white <code class="email">(255,255,255)</code>.</p><p class="calibre8">Unfortunately, computing the distance between the colors using the RGB color space is not the best way to measure the similarity between two given colors. Indeed, RGB is not a perceptually uniform color space. This means that two colors at a given distance might look very similar, while two other colors separated by the same distance might look very different.</p><p class="calibre8">To solve this problem, other color representations that have the property of being perceptually uniform have been introduced. In particular, the CIE L*a*b* is one such color model. By converting our images to this representation, the Euclidean distance between an image pixel and the target color will then be a meaningful measure of the visual similarity between the two colors. In this recipe, we will show you how to modify the previous application in order to <a id="id224" class="calibre1"/>work with CIE L*a*b*.</p></div></div>

<div class="book" title="Converting color representations">
<div class="book" title="How to do it…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec67" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre8">Conversion of images between different color spaces is easily done through the use of the <code class="email">cv::cvtColor</code> OpenCV function. Let's convert the input image to the CIE L*a*b* color space at the beginning of the process method:</p><div class="informalexample"><pre class="programlisting">cv::Mat ColorDetector::process(const cv::Mat &amp;image) {

     // re-allocate binary map if necessary
     // same size as input image, but 1-channel
     result.create(image.rows,image.cols,CV_8U);
     // Converting to Lab color space 
     cv::cvtColor(image, converted, CV_BGR2Lab);


     // get the iterators of the converted image 
     cv::Mat_&lt;cv::Vec3b&gt;::iterator it= 
                 converted.begin&lt;cv::Vec3b&gt;();
     cv::Mat_&lt;cv::Vec3b&gt;::iterator itend= 
                 converted.end&lt;cv::Vec3b&gt;();
     // get the iterator of the output image 
     cv::Mat_&lt;uchar&gt;::iterator itout= result.begin&lt;uchar&gt;();

     // for each pixel
     for ( ; it!= itend; ++it, ++itout) {
     …</pre></div><p class="calibre8">The converted <a id="id225" class="calibre1"/>variable contains the image after color conversion. In the <code class="email">ColorDetector</code> class, it is defined as a class attribute:</p><div class="informalexample"><pre class="programlisting">class ColorDetector {

  private:
     // image containing color converted image
     cv::Mat converted;</pre></div><p class="calibre8">You also need to convert the input target color. You can do this by creating a temporary image that contains only one pixel. Note that you need to keep the same signature as in the earlier recipes, that is, the user continues to supply the target color in RGB:</p><div class="informalexample"><pre class="programlisting">     // Sets the color to be detected
     void setTargetColor(unsigned char red, 
           unsigned char green, unsigned char blue) {

         // Temporary 1-pixel image
         cv::Mat tmp(1,1,CV_8UC3);
         tmp.at&lt;cv::Vec3b&gt;(0,0)= cv::Vec3b(blue, green, red);

         // Converting the target to Lab color space 
         cv::cvtColor(tmp, tmp, CV_BGR2Lab);

         target= tmp.at&lt;cv::Vec3b&gt;(0,0);
     }</pre></div><p class="calibre8">If the application of the preceding recipe is compiled with this modified class, it will now detect the pixels of the target color using the CIE L*a*b* color model.</p></div></div>

<div class="book" title="Converting color representations">
<div class="book" title="How it works…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch03lvl2sec68" class="calibre1"/>How it works…</h2></div></div></div><p class="calibre8">When an image<a id="id226" class="calibre1"/> is converted from one color space to another, a linear or nonlinear transformation is applied on each input pixel to produce the output pixels. The pixel type of the output image will match the one of the input image. Even if you work with 8-bit pixels most of the time, you can also use a color conversion with floating-point images (in which case, the pixel values are generally assumed to vary between <code class="email">0</code> and <code class="email">1.0</code>) or with integer images (with pixels generally varying between <code class="email">0</code> and <code class="email">65535</code>). However, the exact domain of the pixel values depends on the specific color space and destination image type. For example, with the CIE L*a*b* color space, the <code class="email">L</code> channel, which represents the brightness of each pixel, varies between <code class="email">0</code> and <code class="email">100</code>, and it is rescaled between <code class="email">0</code> and <code class="email">255</code> in the case of the 8-bit images. The <code class="email">a</code> and <code class="email">b</code> channels correspond to the chromaticity components. These channels contain information about the color of a pixel, independent of its brightness. Their values vary between <code class="email">-127</code> and <code class="email">127</code>; for 8-bit images, <code class="email">128</code> is added to each value in order to make it fit within the <code class="email">0</code> to <code class="email">255</code> interval. However, note that the 8-bit color conversion will introduce rounding errors that will make the transformation imperfectly reversible.</p><p class="calibre8">Most commonly used color spaces are available. It is just a question of providing the right color space conversion code to the OpenCV function (for CIE L*a*b*, this code is <code class="email">CV_BGR2Lab</code>). Among these is YCrCb, which is the color space used in a JPEG compression. To convert a color space from BGR to YCrCb, the code will be <code class="email">CV_BGR2YCrCb</code>. Note that all the conversions that involve the three regular primary colors, red, green, and blue, are available in the RGB and BGR order.</p><p class="calibre8">The CIE L*u*v* color space<a id="id227" class="calibre1"/> is another perceptually uniform color space. You can convert from BGR to CIE L*u*v by using the <code class="email">CV_BGR2Luv</code> code. Both L*a*b* and L*u*v* use the same conversion formula for the brightness channel but use a different representation for the chromaticity channels. Also, note that since these two color spaces distort the RGB color domain in order to make it perceptually uniform, these transformations are nonlinear (therefore, they are costly to compute).</p><p class="calibre8">There is also the <a id="id228" class="calibre1"/>CIE XYZ color space (with the <code class="email">CV_BGR2XYZ</code> code). It is a standard color space used to represent any perceptible color in a device-independent way. In the computation of the L*u*v and L*a*b color spaces, the XYZ color space is used as an intermediate representation. The transformation between RGB and XYZ is linear. It is also interesting to note that the <code class="email">Y</code> channel corresponds to a gray-level version of the image.</p><p class="calibre8">HSV and HLS are interesting color spaces because they decompose the colors into their hue and saturation components plus the value or luminance component, which is a more natural way for humans to describe colors.</p><p class="calibre8">You can also convert color images to a gray-level intensity. The output will be a one-channel image:</p><div class="informalexample"><pre class="programlisting">         cv::cvtColor(color, gray, CV_BGR2Gray);</pre></div><p class="calibre8">It is also possible to<a id="id229" class="calibre1"/> do the conversion in another direction, but the three channels of the resulting color image will then be identically filled with the corresponding values in the gray-level image.</p></div></div>

<div class="book" title="Converting color representations">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch03lvl2sec69" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">The <span class="strong"><em class="calibre9">Using the mean shift algorithm to find an object</em></span> recipe in <a class="calibre1" title="Chapter 4. Counting the Pixels with Histograms" href="part0032_split_000.html#page">Chapter 4</a>, <span class="strong"><em class="calibre9">Counting the Pixels with Histograms</em></span>, uses the HSV color space in order to find an object in an image.</li><li class="listitem">Many good references are available on the color space theory. Among them, the following is a complete reference: <span class="strong"><em class="calibre9">The Structure and Properties of Color Spaces and the Representation of Color Images</em></span>, <span class="strong"><em class="calibre9">E. Dubois</em></span>, <span class="strong"><em class="calibre9">Morgan and Claypool Publishers</em></span>, <span class="strong"><em class="calibre9">2009</em></span>.</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Representing colors with hue, saturation, and brightness"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec25" class="calibre1"/>Representing colors with hue, saturation, and brightness</h1></div></div></div><p class="calibre8">In this chapter, we played with image colors. We used different color spaces and tried to identify image areas that have a specific color. The RGB color space, for instance, was considered, and although <a id="id230" class="calibre1"/>it is an effective representation<a id="id231" class="calibre1"/> for the capture and display of colors in electronic imaging systems, this representation is not <a id="id232" class="calibre1"/>very intuitive. This is not the way humans think about colors. We talk about colors in terms of their tint, brightness, or colorfulness (that is, whether it is a vivid or pastel color). The <span class="strong"><strong class="calibre2">phenomenal color spaces</strong></span><a id="id233" class="calibre1"/> based on the concept of hue, saturation, and brightness were introduced to help users to specify the colors using properties that are more intuitive to them. In this recipe, we will explore the concepts of hue, saturation, and brightness as a means to describe colors.</p></div>

<div class="book" title="Representing colors with hue, saturation, and brightness">
<div class="book" title="How to do it…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec70" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre8">The <a id="id234" class="calibre1"/>conversion of a BGR image into a phenomenal color space is done using the <code class="email">cv::cvtColor</code> function that was explored in the previous recipe. Here, we will use the <code class="email">CV_BGR2HSV</code> conversion code:</p><div class="informalexample"><pre class="programlisting">    // convert into HSV space
    cv::Mat hsv;
    cv::cvtColor(image, hsv, CV_BGR2HSV);</pre></div><p class="calibre8">We can go back to the BGR space using the <code class="email">CV_HSV2BGR</code> code. We can visualize each of the HSV components by splitting the converted image channels into three independent images, as follows:</p><div class="informalexample"><pre class="programlisting">    // split the 3 channels into 3 images
    std::vector&lt;cv::Mat&gt; channels;
    cv::split(hsv,channels);
    // channels[0] is the Hue
    // channels[1] is the Saturation
    // channels[2] is the Value</pre></div><p class="calibre8">Since we are working on 8-bit images, OpenCV rescales the channel values to cover the <code class="email">0</code> to <code class="email">255</code> range (except for the hue, which is rescaled between <code class="email">0</code> and <code class="email">180</code> as it will be explained in the next section). This is very convenient as we are able to display these channels as gray-level images. The value channel of the castle image will then look as follows:</p><div class="mediaobject"><img src="../images/00021.jpeg" alt="How to do it…" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The same image in the <a id="id235" class="calibre1"/>saturation<a id="id236" class="calibre1"/> channel will look as follows:</p><div class="mediaobject"><img src="../images/00022.jpeg" alt="How to do it…" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Finally, the image with<a id="id237" class="calibre1"/> the <a id="id238" class="calibre1"/>hue channel is as follows:</p><div class="mediaobject"><img src="../images/00023.jpeg" alt="How to do it…" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">These images are interpreted in the next section.</p></div></div>

<div class="book" title="Representing colors with hue, saturation, and brightness">
<div class="book" title="How it works…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec71" class="calibre1"/>How it works…</h2></div></div></div><p class="calibre8">The phenomenal color spaces have been introduced because they correspond to the way humans tend to naturally organize colors. Indeed, humans prefer to describe colors with intuitive attributes such as tint, colorfulness, and brightness. These three attributes are the basis of most phenomenal color spaces. <span class="strong"><strong class="calibre2">Hue</strong></span><a id="id239" class="calibre1"/> designates the dominant color; the names that we give to colors (such as green, yellow, blue, and red) correspond to the different hue values. <span class="strong"><strong class="calibre2">Saturation</strong></span><a id="id240" class="calibre1"/> tells us how vivid the color is; pastel colors have low saturation, while the colors of the rainbow are highly saturated. Finally, <span class="strong"><strong class="calibre2">brightness</strong></span><a id="id241" class="calibre1"/> is a subjective attribute that refers to the luminosity of a color. Other phenomenal color spaces use the concept of color <span class="strong"><strong class="calibre2">value</strong></span><a id="id242" class="calibre1"/> or color <span class="strong"><strong class="calibre2">lightness</strong></span><a id="id243" class="calibre1"/> as a way to characterize the relative color intensity.</p><p class="calibre8">These color components try to mimic the intuitive human perception of colors. In consequence, there is no standard definition for them. In the literature, you will find several different definitions and formulae of the hue, saturation, and brightness. OpenCV proposes two implementations of phenomenal color spaces: the HSV and the HLS color spaces. The conversion formulas are slightly different, but they give very similar results.</p><p class="calibre8">The value component is probably the easiest to interpret. In the OpenCV implementation of the HSV space, it is defined as the maximum value of the three BGR components. It is a very simplistic implementation of the brightness concept. For a definition that matches the human visual system better, you should use the <code class="email">L</code> channel of the L*a*b* or L*u*v* color spaces.</p><p class="calibre8">To compute<a id="id244" class="calibre1"/> the saturation, OpenCV uses a formula based on the minimum and maximum values of the BGR components:</p><div class="mediaobject"><img src="../images/00024.jpeg" alt="How it works…" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The idea is that a grayscale color in which the three R, G, and B components are all equal will correspond to a perfectly desaturated color; therefore, it will have a saturation value of <code class="email">0</code>. Saturation is then a value between <code class="email">0</code> and <code class="email">1.0</code>. For 8-bit images, saturation is rescaled to a value between <code class="email">0</code> and <code class="email">255</code>, and when displayed as a gray-level image, brighter areas correspond to the colors that have a higher saturation color. For example, from the saturation image in the previous section, it can be seen that the blue of the water is more saturated than the light blue pastel color of the sky, as expected. The different shades of gray have, by definition, a saturation value equal to zero (because, in this case, all the three BGR components are equal). This can be observed on the different roofs of the castle, which are made of a dark gray stone. Finally, in the saturation image, you may have noticed some white spots located at areas that correspond to very dark regions of the original image. These are a consequence of the used definition for saturation. Indeed, because saturation measures only the relative difference between the maximum and minimum BGR values, a triplet such as (1,0,0) gives a perfect saturation of <code class="email">1.0</code>, even if this color would be seen as black. Consequently, the saturation values measured at dark regions are unreliable and should not be considered.</p><p class="calibre8">The hue of <a id="id245" class="calibre1"/>a color is generally represented by an angle value between <code class="email">0</code> and <code class="email">360</code>, with the red color at <code class="email">0</code> degree. In the case of an 8-bit image, OpenCV divides this angle by two to fit within the single byte range. Therefore, each hue value corresponds to a given color tint independent of its brightness and saturation. For example, both the sky and the water have the same hue value, approximately <code class="email">200</code> degrees (intensity, <code class="email">100</code>), which corresponds to the blue shade; the green color of the trees in the background has a hue of around <code class="email">90</code> degrees. It is important to note that hue is less reliable when evaluated for colors that have a very low saturation.</p><p class="calibre8">The HSB color space<a id="id246" class="calibre1"/> is often represented by a cone, where each point inside corresponds to a particular color. The angular position corresponds to the hue of the color, the saturation is the distance from the central axis, and the brightness is given by the height. The tip of the cone corresponds to the black color for which the hue and saturation are undefined.</p><div class="mediaobject"><img src="../images/00025.jpeg" alt="How it works…" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Interesting effects can be created by playing with the HSV values. Several color effects that can be created using photo editing software are accomplished by this color space. For example, you may decide to modify an image by assigning a constant brightness to all the pixels of an image without changing the hue and saturation. This can be done as follows:</p><div class="informalexample"><pre class="programlisting">  // convert into HSV space
  cv::Mat hsv;
  cv::cvtColor(image, hsv, CV_BGR2HSV);
  // split the 3 channels into 3 images
  std::vector&lt;cv::Mat&gt; channels;
  cv::split(hsv,channels);
  // Value channel will be 255 for all pixels
  channels[2]= 255;  
  // merge back the channels
  cv::merge(channels,hsv);
  // reconvert to BGR
  cv::Mat newImage;
  cv::cvtColor(hsv,newImage,CV_HSV2BGR);</pre></div><p class="calibre8">This gives the<a id="id247" class="calibre1"/> following screenshot, which now looks like a drawing (see the book's graphic bundle to view this image in color):</p><div class="mediaobject"><img src="../images/00026.jpeg" alt="How it works…" class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div class="book" title="Representing colors with hue, saturation, and brightness">
<div class="book" title="There's more…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch03lvl2sec72" class="calibre1"/>There's more…</h2></div></div></div><p class="calibre8">The HSV color space<a id="id248" class="calibre1"/> can also be very convenient to use when you want to look for objects of specific colors.</p><div class="book" title="Using colors for detection – skin tone detection"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch03lvl3sec20" class="calibre1"/>Using colors for detection – skin tone detection</h3></div></div></div><p class="calibre8">Color information <a id="id249" class="calibre1"/>can be very useful for the initial detection<a id="id250" class="calibre1"/> of specific objects. For example, the detection of road signs in a driver-assistance application could rely on the colors of standard signs in order to quickly extract potential road sign candidates. The detection of skin color is another example in which the detected skin regions could be used as an indicator of the presence of a human in an image; this approach is very often used in gesture recognition where skin tone detection is used to detect hand positions.</p><p class="calibre8">In general, to <a id="id251" class="calibre1"/>detect an object using color, you first need to collect a large database of image samples that contain the object captured from different viewing <a id="id252" class="calibre1"/>conditions. These will be used to define the parameters of your classifier. You also need to select the color representation that you will use for classification. For skin tone detection, many studies have shown that skin color from the diverse ethnical groups clusters well in the hue-saturation space. For this reason, we will simply use the hue and saturation values to identify the skin tones in the following image (see the book's graphic bundle to view this image in color):</p><div class="mediaobject"><img src="../images/00027.jpeg" alt="Using colors for detection – skin tone detection" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Therefore, we have defined a function that classifies the pixels of an image as skin or non-skin simply based on an interval of values (the minimum and maximum hue, and the minimum and maximum saturation):</p><div class="informalexample"><pre class="programlisting">void detectHScolor(const cv::Mat&amp; image,  // input image 
  double minHue, double maxHue,  // Hue interval 
  double minSat, double maxSat,  // saturation interval
  cv::Mat&amp; mask) {               // output mask

  // convert into HSV space
  cv::Mat hsv;
  cv::cvtColor(image, hsv, CV_BGR2HSV);

  // split the 3 channels into 3 images
  std::vector&lt;cv::Mat&gt; channels;
  cv::split(hsv, channels);
  // channels[0] is the Hue
  // channels[1] is the Saturation
  // channels[2] is the Value
  // Hue masking
  cv::Mat mask1; // under maxHue
  cv::threshold(channels[0], mask1, maxHue, 255,
  cv::THRESH_BINARY_INV);
  cv::Mat mask2; // over minHue
  cv::threshold(channels[0], mask2, minHue, 255,
  cv::THRESH_BINARY);

  cv::Mat hueMask; // hue mask
  if (minHue &lt; maxHue)
      hueMask = mask1 &amp; mask2;
  else // if interval crosses the zero-degree axis
      hueMask = mask1 | mask2;

  // Saturation masking
  // under maxSat
  cv::threshold(channels[1], mask1, maxSat, 255,
  cv::THRESH_BINARY_INV);
  // over minSat
  cv::threshold(channels[1], mask2, minSat, 255,
  cv::THRESH_BINARY);

  cv::Mat satMask; // saturation mask
  satMask = mask1 &amp; mask2;

  // combined mask
  mask = hueMask&amp;satMask;
}</pre></div><p class="calibre8">Having a large <a id="id253" class="calibre1"/>set of skin (and non-skin) samples at our disposal, we could have used a probabilistic approach in which the likelihood of observing a given color in the skin class versus that of observing the same color in the non-skin class. Here, we <a id="id254" class="calibre1"/>empirically defined an acceptable hue-saturation interval for <a id="id255" class="calibre1"/>our test image (remember that the 8-bit version of the hue goes from 0 to 180 and saturation goes from 0 to 255):</p><div class="informalexample"><pre class="programlisting">  // detect skin tone
  cv::Mat mask;
  detectHScolor(image, 
        160, 10, // hue from 320 degrees to 20 degrees 
        25, 166, // saturation from ~0.1 to 0.65
        mask);

  // show masked image
  cv::Mat detected(image.size(), CV_8UC3, cv::Scalar(0, 0, 0));
  image.copyTo(detected, mask);</pre></div><p class="calibre8">The following detection image is obtained as the result:</p><div class="mediaobject"><img src="../images/00028.jpeg" alt="Using colors for detection – skin tone detection" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Note that, for simplicity, we have not considered color saturation in the detection. In practice, excluding<a id="id256" class="calibre1"/> the colors with a high saturation would have reduced the possibility of the wrong detection of bright reddish colors as skin. Obviously, a reliable and accurate detection of skin color would require a much more elaborate<a id="id257" class="calibre1"/> analysis that would have to be based on a large number of skin samples. It is also very difficult to guarantee good detection across different images because many factors influence the color rendering in photography, such as white balancing and lighting conditions. Nevertheless, as shown in this chapter, only using hue information as an initial detector gives us acceptable results.</p></div></div></div></body></html>