- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Machine Learning Engineering and MLOps with Google Cloud
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Google Cloud 进行机器学习工程和 MLOps
- en: It’s generally estimated that almost 90% of data science projects never make
    it to production. Data scientists spend a lot of time training and experimenting
    with models in the lab, but often don’t succeed in bringing those workloads out
    into the real world. A major reason for this is because, as we have discussed
    in the previous chapters of this book, there are difficult challenges at every
    step in the model development lifecycle. Following on from our previous chapter,
    we will now dive into more detail on deployment concepts and challenges, and describe
    the importance of **Machine Learning Operations** (**MLOps**) in addressing these
    challenges for large-scale production AI/ML workloads.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通常估计，几乎 90% 的数据科学项目从未进入生产阶段。数据科学家在实验室花费大量时间训练和实验模型，但往往无法成功地将这些工作负载带入现实世界。主要原因是我们已经在本书的前几章中讨论过，在模型开发生命周期中的每一步都存在困难挑战。继我们上一章的内容之后，我们现在将更详细地探讨部署概念和挑战，并描述
    **机器学习运维**（**MLOps**）在解决这些针对大规模生产 AI/ML 工作负载的挑战中的重要性。
- en: 'Specifically, this chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，本章将涵盖以下主题：
- en: An introduction to MLOps
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLOps 简介
- en: Why MLOps is needed for deploying large-scale ML workloads
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么需要 MLOps 来部署大规模机器学习工作负载
- en: MLOps tools
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLOps 工具
- en: Implementing MLOps on Google Cloud using Vertex AI Pipelines
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Google Cloud 上使用 Vertex AI Pipelines 实施MLOps
- en: While we’ve already touched on some of these concepts so far in the book, we’ll
    kick off this chapter with a more formal introduction to MLOps.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经在本书的早期部分触及了一些这些概念，但我们将以对 MLOps 的更正式介绍开始本章。
- en: An introduction to MLOps
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLOps 简介
- en: MLOps is an extension of the DevOps concept from the software development industry
    but with a specific focus on managing and automating ML model and project lifecycles.
    It goes beyond the tactical steps required to create machine learning models and
    addresses requirements that come to light when companies need to manage the entire
    lifecycle of data science use cases at scale. This is a good time to reflect on
    the ML model lifecycle stages we outlined in previous chapters in this book, as
    depicted in *Figure 11**.1*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps 是软件开发行业 DevOps 概念的扩展，但专注于管理和自动化机器学习模型和项目生命周期。它超越了创建机器学习模型所需的战术步骤，并解决了当公司需要以规模管理数据科学用例的整个生命周期时出现的需求。现在是一个反思我们在本书前几章中概述的机器学习模型生命周期阶段的好时机，如图
    *图 11.1* 所示。
- en: '![Figure 11.1: Data science lifecycle stages](img/B18143_11_1.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1：数据科学生命周期阶段](img/B18143_11_1.jpg)'
- en: 'Figure 11.1: Data science lifecycle stages'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1：数据科学生命周期阶段
- en: At a high level, MLOps aims to automate all of the various steps in the ML model
    lifecycle, such as data collection, data cleaning and preprocessing, model training,
    model evaluation, model deployment, and model monitoring. As such, it can be seen
    as a type of engineering culture that aims to unify ML system development and
    ML system day-to-day operations. This includes a practice for collaboration and
    communication among all relevant stakeholders in an ML project, such as a company’s
    data scientists, ML engineers, data engineers, business analysts, and operations
    staff, to help manage the overall machine learning lifecycle on an ongoing basis.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，MLOps 的目标是自动化机器学习模型生命周期的所有各种步骤，例如数据收集、数据清洗和预处理、模型训练、模型评估、模型部署和模型监控。因此，它可以被视为一种旨在统一机器学习系统开发和日常运营的工程文化。这包括在机器学习项目中所有相关利益相关者之间的协作和沟通实践，例如公司的数据科学家、机器学习工程师、数据工程师、业务分析师和运营人员，以帮助持续管理整个机器学习生命周期。
- en: In this context, managing the ML lifecycle includes defining, implementing,
    testing, deploying, monitoring, and managing ML models to ensure that they work
    reliably and efficiently in a production environment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，管理机器学习生命周期包括定义、实施、测试、部署、监控和管理机器学习模型，以确保它们在生产环境中可靠且高效地运行。
- en: In another similarity to DevOps, MLOps practices also encompass the concepts
    of continuous integration, continuous delivery, and continuous deployment (CI/CD),
    but with a specific focus on how ML models are developed. This ensures that new
    changes to the models are correctly integrated, thoroughly tested, and can be
    deployed to production in a systematic, reliable, and repeatable way. The goal
    in this regard is to ensure that any part of the ML lifecycle (data preprocessing,
    training, etc.) can be repeated at a later point with the same results. Just as
    with CI/CD in the context of software DevOps, this includes automating steps such
    as validation checks and integration tests, but in the case of ML model development,
    it also adds extra components such as data quality checks and model quality evaluations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与DevOps的另一个相似之处在于，MLOps实践也涵盖了持续集成、持续交付和持续部署（CI/CD）的概念，但重点在于如何开发机器学习模型。这确保了模型的新更改被正确集成、彻底测试，并且可以以系统化、可靠和可重复的方式部署到生产环境中。在这方面，目标是确保机器学习生命周期的任何部分（如数据预处理、训练等）都可以在以后以相同的结果重复。正如在软件DevOps的CI/CD环境中一样，这包括自动化步骤，如验证检查和集成测试，但在机器学习模型开发的情况下，它还增加了额外的组件，例如数据质量检查和模型质量评估。
- en: As we discussed in previous chapters, deploying a model to production is a great
    achievement, but the work doesn’t stop there. Once ML models are deployed to production,
    they need to be continuously monitored to ensure their performance does not degrade
    over time due to changes in the underlying data or other factors. If any issues
    are detected, the models need to be updated or replaced with newer models. MLOps
    also includes tools to automate that part of the process, such as automatically
    retraining (for example, with fresh data), validating, and deploying a new model
    version.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中讨论的那样，将模型部署到生产环境是一项巨大的成就，但工作并没有就此结束。一旦机器学习模型被部署到生产环境中，就需要持续监控它们，以确保由于底层数据或其他因素的变化，其性能不会随时间退化。如果检测到任何问题，模型需要更新或用更新的模型替换。MLOps还包括自动化该过程的工具，例如自动重新训练（例如，使用新鲜数据）、验证和部署新的模型版本。
- en: Another important component of this is keeping track of various versions of
    the artifacts (e.g., data, models, and hyperparameter values) produced at various
    stages in our data science project. Not only does this enable easy rollback to
    previous versions if required, but it also provides an audit trail for compliance
    purposes, and to help with model explainability and transparency, as well as appropriate
    governance mechanisms, helping to ensure that ML models are used responsibly and
    ethically.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这其中的另一个重要组成部分是跟踪我们在数据科学项目各个阶段产生的各种工件版本（例如，数据、模型和超参数值）。这不仅使得在需要时可以轻松回滚到以前的版本，而且还提供了合规性审计跟踪，有助于模型的可解释性和透明度，以及适当的治理机制，有助于确保机器学习模型被负责任和道德地使用。
- en: Now that we understand what MLOps is, let’s dive into more detail on why it’s
    especially important when developing, deploying, and managing models at a large
    scale.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了MLOps是什么，让我们更深入地探讨为什么它在开发、部署和管理大规模模型时尤为重要。
- en: Note
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Later in this book, we will discuss generative AI/ML. There are particular types
    of ML models that are used in generative AI/ML that are called **Large Language
    Models** (**LLMs**). These models also have additional kinds of resources and
    artifacts associated with them, and a new term, LLMOps has emerged in the industry
    to refer to the operationalization of those workloads. For now, we will focus
    on traditional MLOps. While many of these concepts also apply to LLMOps workloads,
    we will discuss the additional considerations for LLMOps in later chapters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的后续部分，我们将讨论生成式人工智能/机器学习。在生成式人工智能/机器学习中，有一些特定的机器学习模型被用于其中，被称为**大型语言模型**（**LLMs**）。这些模型还与一些额外的资源和工件相关联，并且行业中出现了一个新术语LLMOps，用来指代这些工作负载的运营化。目前，我们将专注于传统的MLOps。虽然许多这些概念也适用于LLMOps工作负载，但我们将在后面的章节中讨论LLMOps的额外考虑因素。
- en: Why MLOps is needed for deploying large-scale ML workloads
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么需要MLOps来部署大规模机器学习工作负载
- en: The most important aspect of MLOps is that it helps organizations develop ML
    models in a faster, more efficient, and reliable manner, and it allows data science
    teams to experiment and innovate while also meeting operational requirements.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps最重要的方面是它帮助组织以更快、更高效和更可靠的方式开发机器学习模型，并允许数据科学团队在满足运营要求的同时进行实验和创新。
- en: We know by now that ML has become an essential component of many industries
    and sectors, providing invaluable insights and decision-making capabilities, but
    that deploying ML models, especially at scale, presents many challenges. Some
    of these are challenges that can only be solved by MLOps, and we dive into more
    detail on such challenges in this section, as well as providing examples of how
    MLOps helps to address them.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，我们知道机器学习已经成为许多行业和部门的必要组成部分，提供了宝贵的见解和决策能力，但部署机器学习模型，尤其是在大规模上，面临着许多挑战。其中一些挑战只能通过MLOps来解决，我们将在本节中深入探讨这些挑战，并提供MLOps如何帮助解决这些挑战的例子。
- en: Before we dive in, I’m going to point out that the kinds of challenges we will
    discuss in this section actually apply to any industry that creates products at
    a large scale, whether those products are cars, safety pins, toys, or machine
    learning models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨之前，我要指出，本节中我们将讨论的这类挑战实际上适用于任何大规模生产产品的行业，无论这些产品是汽车、安全别针、玩具还是机器学习模型。
- en: I’m going to take an analogy from the automobile industry in order to highlight
    the kinds of concepts that are required for large-scale production of any type
    of product. Consider that before cars were invented, a major form of transport
    would have been horse-drawn carts, and such carts would have been made in a kind
    of workshop like the one shown in *Figure 11**.2*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我将从一个汽车行业的类比入手，以突出大规模生产任何类型产品所需的概念。考虑在汽车发明之前，主要的交通工具可能是马车，而这种马车可能是在类似*图11.2*中展示的工坊中制造的。
- en: '![Figure 11.2: Forge (source: https://www.hippopx.com/en/middle-ages-forge-workshop-old-castle-wagon-wheel-297217)](img/B18143_11_2.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图11.2：锻造车间（来源：https://www.hippopx.com/en/middle-ages-forge-workshop-old-castle-wagon-wheel-297217）](img/B18143_11_2.jpg)'
- en: 'Figure 11.2: Forge (source: https://www.hippopx.com/en/middle-ages-forge-workshop-old-castle-wagon-wheel-297217)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2：锻造车间（来源：https://www.hippopx.com/en/middle-ages-forge-workshop-old-castle-wagon-wheel-297217）
- en: 'Notice the characteristics of the production environment shown in *Figure 11**.2*:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意*图11.2*中展示的生产环境的特征：
- en: It’s a small environment, which could only accommodate a few people working
    together
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个小环境，只能容纳少数人一起工作。
- en: Whoever works here has lots of random tools lying around, and there doesn’t
    seem to be much standardization implemented
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里工作的每个人周围都散落着许多随机工具，似乎并没有实施太多的标准化。
- en: With those kinds of characteristics, any kind of large-scale collaboration would
    not be possible, and at most, only three or four people could work together on
    creating a product. In such circumstances, a company could not possibly create
    a large number of products every month (for example). Bear in mind that this is
    how most companies begin to implement data science projects, in which data scientists
    perform various experiments and develop models on their own computers, using any
    random tools based on personal preferences, and trying to share learnings and
    artifacts with colleagues in a non-standardized and unsystematic way.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 具有这些特征，任何类型的大规模协作都是不可能的，最多只能有三四个人一起工作来创造产品。在这种情况下，公司不可能每个月都生产大量产品（例如）。记住，大多数公司就是这样开始实施数据科学项目的，数据科学家在自己的电脑上执行各种实验并开发模型，使用基于个人偏好的任何随机工具，并试图以非标准化和非系统化的方式与同事分享学习和成果。
- en: To overcome these kinds of challenges, the automotive industry invented the
    concept of the assembly line, and some of the first assembly lines would have
    looked similar to the one shown in *Figure 11**.3*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些挑战，汽车行业发明了流水线的概念，一些最早的流水线可能看起来与*图11.3*中展示的相似。
- en: '![Figure 11.3: Assembly line (source:  https://pxhere.com/en/photo/798929)](img/B18143_11_3.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图11.3：流水线（来源：https://pxhere.com/en/photo/798929）](img/B18143_11_3.jpg)'
- en: 'Figure 11.3: Assembly line (source: https://pxhere.com/en/photo/798929)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：流水线（来源：https://pxhere.com/en/photo/798929）
- en: 'Notice the characteristics of the production environment shown in *Figure 11**.3*:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意*图11.3*中展示的生产环境的特征：
- en: The environment can accommodate many people working together
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境可以容纳很多人一起工作。
- en: The tools and manufacturing processes have been standardized
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具和制造工艺已经标准化。
- en: This kind of environment enables large-scale collaboration, and it’s possible
    to produce many more products in any given timeframe.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这种环境能够实现大规模协作，并且在任何给定的时间框架内可以生产出更多的产品。
- en: As automotive companies continued to evolve, they continued to apply more standardization
    and automation to each step in the process, leading to more efficient processes
    and reproducibility, until today’s assembly lines look like the one shown in *Figure
    11**.4*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 随着汽车公司继续发展，它们继续将标准化和自动化应用于流程的每个步骤，从而提高了流程的效率和可重复性，直到今天的装配线看起来就像*图11**.4*中所示的那样。
- en: '![Figure 11.4: Modern assembly line (source:  https://www.rawpixel.com/image/12417375/photo-image-technology-factory-green)](img/B18143_11_4.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图11.4：现代装配线（来源：https://www.rawpixel.com/image/12417375/photo-image-technology-factory-green）](img/B18143_11_4.jpg)'
- en: 'Figure 11.4: Modern assembly line (source: https://www.rawpixel.com/image/12417375/photo-image-technology-factory-green)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4：现代装配线（来源：https://www.rawpixel.com/image/12417375/photo-image-technology-factory-green）
- en: As we can see in *Figure 11**.4*, the production process has become highly automated
    and more efficient to operate at scale. With this analogy in mind, let’s look
    at how this idea maps to the concepts and steps in the ML model development lifecycle.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*图11**.4*中可以看到的，生产过程已经变得高度自动化，并且在大规模操作上更加高效。带着这个类比，让我们看看这个想法如何映射到机器学习模型开发生命周期的概念和步骤。
- en: Model management and versioning
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型管理和版本控制
- en: As an organization scales up its ML efforts, the number of models in operation
    can increase exponentially. Managing these models, keeping track of their versions,
    and knowing when to update or retire them can become quite difficult. Without
    a proper versioning and management system, ML models and their corresponding datasets
    can become disorganized, which leads to confusion and errors in the model development
    process. In fact, when we remember that many large companies have hundreds or
    even thousands of models in production, and are constantly developing new versions
    of those models to improve their performance, managing all of those models without
    specialized tools is basically impossible. MLOps tools provide mechanisms to facilitate
    easier model management and versioning, allowing teams to track and control their
    models in an organized and efficient way.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 随着组织扩大其机器学习努力，正在运行中的模型数量可以呈指数增长。管理这些模型、跟踪它们的版本以及知道何时更新或淘汰它们可能变得相当困难。如果没有适当的版本控制和管理系统，机器学习模型及其相应的数据集可能会变得混乱，这会导致模型开发过程中的混淆和错误。事实上，当我们想到许多大型公司有数百甚至数千个模型在生产中，并且不断开发这些模型的新版本以改进其性能时，没有专业工具管理所有这些模型基本上是不可能的。MLOps工具提供了促进更轻松模型管理和版本控制的机制，使团队能够以有组织和高效的方式跟踪和控制他们的模型。
- en: Productivity and automation
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生产力和自动化
- en: Training, testing, deploying, and retraining models often involves many repetitive
    tasks, and as the scale of ML workloads increases, the time and effort required
    to manually manage these processes can become prohibitive. Considering that MLOps
    introduces automation at various stages of the ML lifecycle, such as data preprocessing,
    model training, testing, deployment, and monitoring, this frees up the data science
    teams to focus on more valuable tasks. Another important aspect of automation
    is that it reduces the likelihood of human error, which can help to avoid business-affecting
    mistakes, and increase productivity.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 训练、测试、部署和重新训练模型通常涉及许多重复性任务，随着机器学习工作负载规模的增加，手动管理这些过程所需的时间和精力可能会变得难以承受。考虑到MLOps在机器学习生命周期的各个阶段引入了自动化，例如数据预处理、模型训练、测试、部署和监控，这使得数据科学团队能够专注于更有价值的任务。自动化的重要方面之一是它减少了人为错误的可能性，这有助于避免影响业务的错误，并提高生产力。
- en: Reproducibility
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可重复性
- en: Reproducibility is arguably one of the most important factors in any industry
    that creates products at a large scale. Imagine if every time you tried to purchase
    the same product from a company, you received something slightly different. Most
    likely, you would lose faith in that company and would stop purchasing their products.
    Without well-established reproducibility in production processes, a company simply
    will not scale. In the context of machine learning, when we have a large number
    of models and datasets, and different types of development environments, ensuring
    the reproducibility of experiments can be a significant challenge. It can be difficult
    to recreate the exact conditions of a previous experiment due to the lack of version
    control on data, models, and code. This could lead to inconsistent results and
    make it difficult to build upon previous work. MLOps frameworks provide tools
    and practices for maintaining a record of all experiments, including the data
    used, the parameters set, the model architecture, and the outcomes. This allows
    experiments to be easily repeated, compared, and audited.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 可重现性可以说是任何大规模生产产品的行业中最重要的因素之一。想象一下，每次你尝试从一家公司购买相同的产品时，你都会收到一些略有不同的事物。很可能会失去对该公司的信任，并停止购买他们的产品。如果没有在生产过程中建立良好的可重现性，公司根本无法实现规模化。在机器学习的背景下，当我们拥有大量模型和数据集，以及不同类型的开发环境时，确保实验的可重现性可能是一个重大的挑战。由于数据、模型和代码缺乏版本控制，可能很难重现之前实验的精确条件。这可能导致结果不一致，并使基于先前工作的构建变得困难。MLOps框架提供了维护所有实验记录的工具和实践，包括使用的数据、设置的参数、模型架构和结果。这使得实验可以轻松重复、比较和审计。
- en: Continuous Integration/Continuous Deployment (CI/CD)
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持续集成/持续部署（CI/CD）
- en: In addition to being able to reliably reproduce a product, developing products
    at a large scale also requires the ability to incrementally improve your products
    over time. CI/CD pipelines automate the testing and deployment of models, ensuring
    that new changes are integrated and deployed seamlessly, efficiently, and with
    minimal errors. This enables data scientists to experiment and try out different
    kinds of updates quickly, easily, and in a controlled manner that conforms to
    your company’s required standards. We should note that CI/CD is such an integral
    component of DevOps that the terms are often used almost interchangeably.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意的是，CI/CD是DevOps的一个如此重要的组成部分，以至于这两个术语经常几乎可以互换使用。
- en: What’s even more interesting is that we can also manage an MLOps pipeline within
    a standard DevOps CI/CD pipeline. For example, when using Vertex AI Pipelines,
    we can define our pipeline using code, and that code can be stored in a code repository
    such as Google Cloud Source Repositories and then managed via a DevOps CI/CD pipeline
    in Google Cloud Build. This enables us to apply all the benefits of DevOps, such
    as code versioning and automated testing, to the code that defines our pipelines,
    so that we can control how updates are made to our pipeline definitions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 更有趣的是，我们还可以在标准的DevOps CI/CD管道内管理MLOps管道。例如，当使用Vertex AI Pipelines时，我们可以使用代码定义我们的管道，并将该代码存储在代码仓库中，如Google
    Cloud Source Repositories，然后通过Google Cloud Build中的DevOps CI/CD管道进行管理。这使得我们可以将DevOps的所有好处，如代码版本控制和自动化测试，应用于定义我们管道的代码，从而控制对管道定义的更新方式。
- en: Model validation and testing
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型验证和测试
- en: Testing machine learning models involves unique challenges due to their probabilistic
    nature, and traditional DevOps software testing techniques may not be sufficient.
    MLOps introduces practices and methodologies specifically for automating the validation
    and testing of ML models, ensuring they perform as expected before being deployed
    to production, and for their entire lifetime in production after deployment.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习模型的概率性质，测试机器学习模型涉及独特的挑战，传统的DevOps软件测试技术可能不足以应对。MLOps引入了专门用于自动化验证和测试ML模型的实践和方法，确保在部署到生产之前，它们的表现符合预期，并在部署后的整个生产生命周期中保持如此。此外，能够可靠地重现产品，在大规模开发产品时也要求能够随着时间的推移逐步改进产品。CI/CD管道自动化了模型的测试和部署，确保新更改能够无缝、高效地集成和部署，并尽可能减少错误。这使得数据科学家能够快速、轻松、以符合公司要求标准的方式实验和尝试不同的更新。
- en: Monitoring and maintenance
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控和维护
- en: As we’ve discussed, once ML models are deployed, their performance needs to
    be continuously monitored to ensure they consistently provide accurate predictions.
    We also discussed how changes in real-world data can lead to a decrease in model
    performance over time, and as a result, we need tools and processes for continuous
    monitoring and alerting, enabling teams to react quickly to any degradation in
    performance and retrain models as necessary. This is an important component of
    any MLOps framework.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论的，一旦ML模型部署，它们的性能需要持续监控，以确保它们始终提供准确的预测。我们也讨论了现实世界数据的变化如何导致模型性能随时间下降，因此，我们需要工具和流程来进行持续监控和警报，使团队能够快速响应任何性能下降，并在必要时重新训练模型。这是任何MLOps框架的重要组件。
- en: Collaboration and communication
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协作与沟通
- en: As ML projects scale, they typically require collaboration across various teams
    and roles, including data scientists, ML engineers, data engineers, business analysts,
    and IT operations. In many organizations, there is a gap between the data scientists
    who develop ML models and the IT operations teams who deploy and maintain these
    models in production. Without a common platform and standard practices, different
    team members might use inconsistent methods, tools, and data, leading to inefficient
    workflows and potential mistakes. MLOps fosters effective collaboration and communication
    among these stakeholders, enabling them to work together more efficiently and
    avoid misunderstandings or misalignments.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 随着ML项目的扩展，它们通常需要跨各种团队和角色进行协作，包括数据科学家、ML工程师、数据工程师、业务分析师和IT运维人员。在许多组织中，开发ML模型的数据科学家和在生产中部署和维护这些模型的IT运维团队之间存在差距。没有共同的平台和标准实践，不同的团队成员可能会使用不一致的方法、工具和数据，导致工作流程低效和潜在的错误。MLOps促进了这些利益相关者之间的有效协作和沟通，使他们能够更有效地合作，避免误解或错位。
- en: Regulatory compliance and governance
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监管合规与治理
- en: In industries such as healthcare and finance, ML models must comply with specific
    regulatory requirements. MLOps provides mechanisms for ensuring transparency,
    interpretability, and auditability of models, therefore helping to maintain regulatory
    compliance. Also, without MLOps, managing multiple models across different teams
    and business units can become a challenge. MLOps allows for centralized model
    governance, making it easier to keep track of various models’ performance, status,
    and owners.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗保健和金融等行业，ML模型必须符合特定的监管要求。MLOps提供了确保模型透明度、可解释性和可审计性的机制，因此有助于维护监管合规。此外，没有MLOps，跨不同团队和业务单元管理多个模型可能会成为一个挑战。MLOps允许集中式模型治理，这使得跟踪各种模型的表现、状态和所有者变得更加容易。
- en: Now that we have dived into why MLOps is needed, especially in the context of
    managing ML workloads at scale, let’s take a look at some of the popular tools
    that have been developed in the industry for implementing those concepts.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经深入探讨了为什么MLOps是必要的，尤其是在管理大规模ML工作负载的背景下，让我们来看看行业内为实施这些概念而开发的一些流行工具。
- en: MLOps tools
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLOps工具
- en: So far, we’ve talked about the goals and benefits of MLOps, but how can we actually
    achieve these goals and benefits in the real world? Many tools have been developed
    to facilitate various aspects of MLOps, from data versioning to model deployment
    and monitoring. In this section, we discuss the tools that make MLOps a reality.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了MLOps的目标和好处，但在现实世界中我们如何实现这些目标和好处呢？许多工具已经开发出来，以促进MLOps的各个方面，从数据版本化到模型部署和监控。在本节中，我们讨论使MLOps成为现实的具体工具。
- en: Pipeline orchestration tools
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道编排工具
- en: When we want to standardize the steps in a process and run them automatically,
    we typically configure them as a set of sequential actions. This sequential set
    of actions is often referred to as a pipeline. However, simply configuring the
    order of the steps is not enough; we also need some kind of system to “orchestrate”
    (i.e., execute and manage) the pipeline, and we can use some popular workflow
    orchestration tools for this purpose, such as Kubeflow, Airflow, and **TensorFlow
    Extended** (**TFX**). We already covered Airflow in [*Chapter 6*](B18143_06.xhtml#_idTextAnchor187),
    but let’s take a look at Kubeflow and TFX in more detail.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要标准化流程中的步骤并自动运行它们时，我们通常将它们配置为一系列顺序动作。这个顺序动作集通常被称为管道。然而，仅仅配置步骤的顺序是不够的；我们还需要某种类型的系统来“编排”（即执行和管理）管道，我们可以使用一些流行的流程编排工具来完成这个任务，例如Kubeflow、Airflow和**TensorFlow
    Extended**（**TFX**）。我们已经在[*第6章*](B18143_06.xhtml#_idTextAnchor187)中介绍了Airflow，但让我们更详细地看看Kubeflow和TFX。
- en: Kubeflow
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kubeflow
- en: 'This is an open source project created by Google, and now maintained by an
    open source community of contributors, aimed at making it easier to run machine
    learning workflows on Kubernetes. Fundamental principles of Kubeflow include portability,
    scalability, and extensibility. Portability refers to the concept of “write once,
    run anywhere,” which builds on the core tenet of containerization, whereby you
    can run your workloads across different types of computing environments in a consistent
    manner. Scalability refers to the ability to use Kubernetes to easily scale your
    model training and prediction workloads as needed, and extensibility means that
    you can implement customized use cases by adding popular tools and libraries that
    easily integrate with Kubeflow. In this case, rather than simply being a tool,
    Kubeflow is a framework and ecosystem that we can use to orchestrate and manage
    our machine learning workflows. The following are some core components of the
    Kubeflow ecosystem:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个由谷歌创建的开源项目，现在由一个开源贡献者社区维护，旨在使在Kubernetes上运行机器学习工作流程更加容易。Kubeflow的基本原则包括可移植性、可扩展性和可扩展性。可移植性指的是“一次编写，到处运行”的概念，这是基于容器化的核心原则，即您可以在不同的计算环境中以一致的方式运行您的作业。可扩展性指的是使用Kubernetes轻松扩展您的模型训练和预测工作负载的能力，而可扩展性意味着您可以通过添加与Kubeflow轻松集成的流行工具和库来实现定制化用例。在这种情况下，Kubeflow不仅仅是一个工具，而是一个框架和生态系统，我们可以用它来编排和管理我们的机器学习工作流程。以下是Kubeflow生态系统的一些核心组件：
- en: '**Kubeflow Pipelines** (**KFP**): For defining, deploying, and managing ML
    workflows'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubeflow Pipelines**（**KFP**）：用于定义、部署和管理机器学习工作流程'
- en: '**Katib**: For hyperparameter tuning'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Katib**：用于超参数调整'
- en: '**KFServing**: For serving models in a scalable way'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KFServing**：用于以可扩展的方式提供模型'
- en: '**TensorFlow and PyTorch Operators**: For running TensorFlow and PyTorch jobs'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow和PyTorch操作符**：用于运行TensorFlow和PyTorch作业'
- en: '**Training Operators**: For distributed training'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练操作符**：用于分布式训练'
- en: We will particularly dive deep into Kubeflow Pipelines in the practical exercises
    associated with this chapter, in which we will build a pipeline and execute it
    in Google Cloud Vertex AI. In addition to KFP, Vertex AI also supports using TFX
    to build and manage ML pipelines. Let’s take a look at that next.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章相关的实践练习中，我们将特别深入探讨Kubeflow Pipelines，我们将构建一个管道并在Google Cloud Vertex AI中执行它。除了KFP之外，Vertex
    AI还支持使用TFX来构建和管理机器学习管道。让我们看看下一个。
- en: TensorFlow Extended (TFX)
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow Extended（TFX）
- en: 'TFX is an end-to-end platform, also created by Google, that manages and deploys
    production ML pipelines. As the name suggests, it was designed as an extension
    to TensorFlow, with the aim of making it easier to bring trained models to production.
    Just like KFP, TFX offers components for implementing each stage of the ML lifecycle,
    covering everything from data ingestion and validation to model training, tuning,
    serving, and monitoring. Also similar to KFP, TFX’s core principles include scalability,
    and of course, extensibility. Other core principles of TFX include reproducibility
    and modularity. Reproducibility, as we discussed earlier in this chapter, is a
    critical requirement for producing pretty much anything at a large scale. Modularity,
    in this context, refers to the fact that the various components of TFX can be
    used individually or together, enabling customization. Speaking of which, the
    various components and responsibilities of TFX include the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: TFX是一个端到端平台，也是由谷歌创建的，用于管理和部署生产级机器学习流水线。正如其名所示，它被设计为TensorFlow的扩展，旨在使将训练好的模型投入生产变得更加容易。就像KFP一样，TFX提供了实现机器学习生命周期每个阶段的组件，涵盖了从数据摄取和验证到模型训练、调优、服务以及监控的各个方面。同样，TFX的核心原则包括可扩展性和当然的扩展性。TFX的其他核心原则还包括可重复性和模块化。可重复性，正如我们在本章前面讨论的，是大规模生产几乎所有内容的关键要求。在这个上下文中，模块化指的是TFX的各个组件可以单独使用或组合使用，从而实现定制化。说到这里，TFX的各个组件和职责包括以下内容：
- en: ExampleGen, which imports and ingests data into the TFX pipeline
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ExampleGen，将数据导入TFX流水线
- en: StatisticsGen, which computes statistics for the ingested data, essential for
    understanding the data and feature engineering
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StatisticsGen，计算导入数据的统计信息，这对于理解数据和特征工程至关重要
- en: SchemaGen, which examines dataset statistics and creates a schema for the dataset
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SchemaGen，检查数据集统计信息并为数据集创建模式
- en: ExampleValidator, which identifies and analyzes anomalies and missing values
    in the dataset
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ExampleValidator，用于识别和分析数据集中的异常值和缺失值
- en: Transform, which can be used for feature engineering on the dataset
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transform，可用于在数据集上进行特征工程
- en: Trainer, which defines and trains a model using TensorFlow
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trainer，使用TensorFlow定义和训练模型
- en: Tuner, which can be used to optimize hyperparameters using Keras Tuner
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tuner，可用于使用Keras Tuner优化超参数
- en: InfraValidator, which ensures that the model can be loaded and served in a production
    environment
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: InfraValidator，确保模型可以在生产环境中加载和提供服务
- en: Evaluator, which uses the TensorFlow Model Analysis library to evaluate the
    metrics of the trained model
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Evaluator，使用TensorFlow模型分析库评估训练模型的指标
- en: BulkInferrer, which performs batch processing on a model.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BulkInferrer，对模型进行批量处理
- en: 'Blessing and deployment: If the model is validated, it can be “blessed” and
    then deployed using a serving infrastructure such as TensorFlow Serving.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 祝福和部署：如果模型经过验证，它可以被“祝福”，然后使用如TensorFlow Serving之类的服务基础设施进行部署。
- en: 'Considering that both TFX and KFP can be used to build and run pipelines in
    Google Cloud Vertex AI, it may be challenging to decide which one to use. On that
    matter, the official Google Cloud documentation recommends the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到TFX和KFP都可以用于在Google Cloud Vertex AI中构建和运行流水线，选择使用哪一个可能会有些挑战。关于这个问题，官方的Google
    Cloud文档建议如下：
- en: '*“If you use TensorFlow in an ML workflow that processes terabytes of structured
    data or text data, we recommend that you build your pipeline using TFX… For other
    use cases, we recommend that you build your pipeline using the Kubeflow* *Pipelines
    SDK.”*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*“如果你在处理TB级结构化数据或文本数据的ML工作流中使用TensorFlow，我们建议你使用TFX构建你的流水线……对于其他用例，我们建议你使用Kubeflow的Pipelines
    SDK构建你的流水线。”*'
- en: 'We’re not going to focus on TFX in a lot of detail in this chapter, but if
    you would like to learn how to build a TFX pipeline, then there’s a useful tutorial
    in the TensorFlow documentation at the following link: [https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中不会过多地关注TFX的细节，但如果你想了解如何构建TFX流水线，TensorFlow文档中有一个有用的教程，链接如下：[https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple)。
- en: There are also many other open source tools for implementing MLOps pipelines,
    such as MLFlow, which provides interfaces for tracking experiments, packaging
    code into reproducible runs, and sharing and deploying models.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有许多其他开源工具可用于实现 MLOps 流程，例如 MLFlow，它提供了跟踪实验、将代码打包成可重复运行以及共享和部署模型的接口。
- en: While KFP and TFX can be used to orchestrate your entire MLOps pipeline, there
    are also tools that focus on more specific components of the ML lifecycle, and
    that can integrate with KFP and TFX in order to customize your overall pipeline.
    Let’s take a look at some of those tools next.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 KFP 和 TFX 可以用来编排您的整个 MLOps 流程，但也有专注于 ML 生命周期更具体组件的工具，并且可以与 KFP 和 TFX 集成，以便定制您的整体流程。接下来，让我们看看这些工具中的一些。
- en: Experiment and lineage tracking tools
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验和谱系跟踪工具
- en: As we discussed earlier in this chapter, and in earlier chapters, when running
    ML workloads at a large scale, it’s important to keep track of every step in the
    development of every model, so that you can easily understand how any given model
    was created. This is required for characteristics such as reproducibility, explainability,
    and compliance. Imagine if you have thousands of models in production, and compliance
    regulators ask you to explain how a particular model was created. Without a well-established
    system for tracking every aspect of every model’s creation, such as which versions
    of which datasets were used to train the model, what kinds of algorithms and hyperparameters
    were used, what the initial evaluation metrics were, and who deployed the model
    to production, this would be an impossible task. This is where experiment and
    lineage tracking tools come into the picture. Let’s consider what kinds of tools
    we have at our disposal in this regard.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章以及前几章中讨论的那样，在运行大规模 ML 工作负载时，跟踪每个模型的每个开发步骤非常重要，这样您就可以轻松理解任何给定模型是如何创建的。这对于可重复性、可解释性和合规性等特性是必需的。想象一下，如果您有数千个模型在生产中运行，合规监管机构要求您解释某个特定模型是如何创建的。如果没有一个良好的系统来跟踪每个模型的每个创建方面的详细信息，例如用于训练模型的数据集版本、使用了哪些类型的算法和超参数、初始评估指标是什么，以及谁将模型部署到生产中，这将是一项不可能完成的任务。这就是实验和谱系跟踪工具发挥作用的地方。让我们考虑一下在这方面我们有哪些工具可以利用。
- en: Vertex AI Experiments
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Vertex AI 实验服务
- en: Vertex AI Experiments is a managed service that helps you track, compare, and
    analyze your machine learning experiments. It provides a central place to manage
    your experiments, and it makes it easy to compare different models and hyperparameters.
    Once an experiment is created (using the Vertex AI SDK or the Vertex AI user interface),
    you can start tracking your training runs, and Vertex AI Experiments will automatically
    collect metrics from your training runs, such as accuracy, loss, and training
    time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 实验服务是一个托管服务，帮助您跟踪、比较和分析您的机器学习实验。它提供了一个集中管理实验的地方，并使得比较不同的模型和超参数变得容易。一旦创建了一个实验（使用
    Vertex AI SDK 或 Vertex AI 用户界面），您就可以开始跟踪您的训练运行，Vertex AI 实验服务将自动收集训练运行中的指标，如准确率、损失和训练时间。
- en: Once your training runs are complete, you can view graphs and tables of the
    metrics, and you can use statistical tests to determine which model is the best.
    You can also use the Experiments dashboard to visualize your results, and you
    can use the Experiments API to export your results to a spreadsheet or a notebook.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练运行完成，您可以查看指标图和表格，并可以使用统计测试来确定哪个模型是最好的。您还可以使用实验仪表板来可视化您的结果，并可以使用实验 API 将结果导出到电子表格或笔记本中。
- en: TensorBoard
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorBoard
- en: TensorBoard is a web-based tool that comes with TensorFlow and is designed to
    help visualize and understand machine learning models, as well as to debug and
    optimize them. It provides an interactive interface for various aspects of machine
    learning models and training processes and can make it easy to create metric graphs
    such as ROC curves, which we introduced in previous chapters.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 是 TensorFlow 附带的一个基于网页的工具，旨在帮助可视化和理解机器学习模型，以及调试和优化它们。它为机器学习模型和训练过程的各个方面提供了一个交互式界面，并可以轻松创建如我们在前几章中介绍的
    ROC 曲线等指标图。
- en: Model deployment and monitoring tools
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型部署和监控工具
- en: In this context, we have tools such as Vertex AI Prediction and Model Monitoring,
    which we reviewed in detail in [*Chapter 10*](B18143_10.xhtml#_idTextAnchor259).
    There are also open source tools such as **TensorFlow Serving** (**TFServing**)
    for TensorFlow models, and TorchServe.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，我们有Vertex AI Prediction和Model Monitoring等工具，我们在[*第10章*](B18143_10.xhtml#_idTextAnchor259)中对其进行了详细审查。还有像**TensorFlow
    Serving**（简称**TFServing**）这样的开源工具，用于TensorFlow模型，以及TorchServe。
- en: Model interpretability and explainability tools
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型可解释性和可解释性工具
- en: As we’ve discussed previously, model interpretability and explainability are
    extremely important concepts in machine learning. They relate to reproducibility,
    compliance, and performance. For example, if you don’t have a good understanding
    of how your models work, then it will be harder to continuously improve them in
    a systematic way. These concepts also relate to fairness. That is, in order to
    ensure that a model is making fair and ethical predictions, you need to understand
    how it works in great detail. Fortunately, there are tools that can help us in
    this regard, which we discuss here.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所讨论的，模型可解释性和可解释性是机器学习中的极其重要的概念。它们与可重复性、合规性和性能相关。例如，如果你对你的模型工作原理没有很好的理解，那么在系统性地持续改进它们时将会更加困难。这些概念也与公平性相关。也就是说，为了确保模型做出公平和道德的预测，你需要详细了解其工作原理。幸运的是，有一些工具可以帮助我们在这方面，我们在这里讨论这些工具。
- en: Google Cloud Vertex AI provides tools such as Explainable AI and Fairness Indicators,
    which can help us to understand how our machine learning models work, and how
    they would behave under different conditions. There are also open source tools
    such as **SHAP** (short for **SHapley Additive exPlanations**), which uses a game
    theory approach to explain the output of any machine learning model, and **LIME**
    (short for **Local Interpretable Model-agnostic Explanations**), which is a Python
    library that allows us to explain the predictions of any machine learning classifier.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Vertex AI提供了诸如可解释AI和公平性指标等工具，这些工具可以帮助我们了解我们的机器学习模型是如何工作的，以及它们在不同条件下会如何表现。还有像**SHAP**（代表**SHapley
    Additive exPlanations**）这样的开源工具，它使用博弈论方法来解释任何机器学习模型的输出，以及**LIME**（代表**Local Interpretable
    Model-agnostic Explanations**），这是一个Python库，允许我们解释任何机器学习分类器的预测。
- en: The next chapter in this book is dedicated to the concepts of bias, fairness,
    explainability, and lineage, so we will dive into these concepts and tools in
    a lot more detail there.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 本书下一章将致力于介绍偏差、公平性、可解释性和谱系等概念，因此我们将更详细地探讨这些概念和工具。
- en: Now that we’ve covered many of the different kinds of tools that can be used
    to implement MLOps workloads, let’s dive into how we can specifically do this
    on Google Cloud Vertex AI.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了许多可以用于实现MLOps工作负载的不同类型的工具，让我们深入探讨如何在Google Cloud Vertex AI上具体实现这一点。
- en: Implementing MLOps on Google Cloud using Vertex AI Pipelines
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Vertex AI Pipelines在Google Cloud上实现MLOps
- en: In this section, we will cover the steps to build an MLOps pipeline on Google
    Cloud using Vertex AI Pipelines.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍在Google Cloud上使用Vertex AI Pipelines构建MLOps管道的步骤。
- en: 'Prerequisite: IAM permissions'
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前提条件：IAM权限
- en: 'In this section, we will use the same Vertex AI Workbench notebook instance
    that we created in [*Chapter* *5*](B18143_05.xhtml#_idTextAnchor168). That user-managed
    notebook uses the default Compute Engine service account, which is granted the
    IAM basic Editor role by default. When we build and execute our pipeline in our
    notebook, we decide to let our pipeline inherit the same permissions used by our
    notebook. This is a decision we make by not specifying a different role for our
    pipeline executions. We could specify a different role if we wanted to, but for
    simplicity purposes, we’ll take the approach of having our pipeline use the default
    Compute Engine service account. By default, the Editor role (used by the default
    Compute Engine service account) will allow us to perform all of the required activities
    in Vertex AI in this chapter, but our pipeline will also need to run some steps
    on Dataproc. For this reason, we will add the Dataproc Worker and Dataproc Editor
    roles to the default Compute Engine service account. To do that, perform the following
    steps:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用我们在[*第5章*](B18143_05.xhtml#_idTextAnchor168)中创建的相同的Vertex AI Workbench笔记本实例。该用户管理的笔记本使用默认的Compute
    Engine服务账户，该账户默认被授予IAM基本编辑器角色。当我们在我们笔记本中构建和执行我们的管道时，我们决定让我们的管道继承笔记本使用的相同权限。这是通过不指定不同角色给我们的管道执行所做的决定。如果我们想的话，我们可以指定不同的角色，但为了简单起见，我们将采用让我们的管道使用默认Compute
    Engine服务账户的方法。默认情况下，编辑器角色（默认Compute Engine服务账户使用的角色）将允许我们在本章的Vertex AI中执行所有必需的活动，但我们的管道还需要在Dataproc上运行一些步骤。因此，我们将Dataproc
    Worker和Dataproc编辑器角色添加到默认Compute Engine服务账户中。为此，执行以下步骤：
- en: In the Google Cloud console, navigate to **Google Cloud services** → **IAM &
    Admin** → **IAM**.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Cloud控制台中，导航到**Google Cloud服务** → **IAM & Admin** → **IAM**。
- en: In the list of principals, click on the pencil symbol for the default Compute
    Engine service account (the service account name will have the format `-[PROJECT_NUMBER]-compute@developer.gserviceaccount.com`;
    see *Figure 11**.5* for reference).
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在主体列表中，点击默认Compute Engine服务账户的铅笔符号（服务账户名称将具有格式`-[项目编号]-compute@developer.gserviceaccount.com`；参见*图11**.5*以供参考）。
- en: '![Figure 11.5: Edit service account](img/B18143_11_5.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图11.5：编辑服务账户](img/B18143_11_5.jpg)'
- en: 'Figure 11.5: Edit service account'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5：编辑服务账户
- en: On the screen that appears, select `Dataproc` and select **Dataproc Editor**
    from the list of roles.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在出现的屏幕上，选择`Dataproc`，并从角色列表中选择**Dataproc编辑器**。
- en: Repeat step 3 and select **Dataproc Worker**.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤3，并选择**Dataproc Worker**。
- en: The updated roles will look as shown in *Figure 11**.6*.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新后的角色将如图*图11**.6*所示。
- en: '![Figure 11.6: Updated roles](img/B18143_11_6.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图11.6：更新后的角色](img/B18143_11_6.jpg)'
- en: 'Figure 11.6: Updated roles'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6：更新后的角色
- en: Click **Save**.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**保存**。
- en: And that’s it – you have successfully added the required roles.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样——你已经成功添加了所需的角色。
- en: Implementation
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: As I mentioned in the previous section, we can use the same Vertex AI Workbench
    notebook instance that we created in [*Chapter* *5*](B18143_05.xhtml#_idTextAnchor168)
    to build our MLOps pipeline. Please open JupyterLab on that notebook instance.
    In the directory explorer on the left side of the screen, navigate to the `Chapter-11`
    directory and open the `mlops.ipynb` notebook. You can choose **Python (Local)**
    as the kernel. Again, you can run each cell in the notebook by selecting the cell
    and pressing *Shift* + *Enter* on your keyboard. In addition to the relevant code,
    the notebook contains markdown text that describes what the code is doing.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如前节所述，我们可以使用我们在[*第5章*](B18143_05.xhtml#_idTextAnchor168)中创建的相同的Vertex AI Workbench笔记本实例来构建我们的MLOps管道。请在该笔记本实例上打开JupyterLab。在屏幕左侧的目录浏览器中，导航到`Chapter-11`目录并打开`mlops.ipynb`笔记本。您可以选择**Python
    (Local)**作为内核。同样，您可以通过选择单元格并在键盘上按*Shift* + *Enter*来运行笔记本中的每个单元格。除了相关代码外，笔记本还包含描述代码正在做什么的Markdown文本。
- en: In the practical exercises, we will build and run an MLOps pipeline that will
    execute all of the phases of the ML model development lifecycle that we have covered
    so far in this book. Our pipeline is depicted in *Figure 11**.5*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际练习中，我们将构建并运行一个MLOps管道，该管道将执行我们在本书中迄今为止所涵盖的ML模型开发生命周期的所有阶段。我们的管道如图*图11**.5*所示。
- en: '![Figure 11.7: Vertex AI pipeline](img/B18143_11_7.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图11.7：Vertex AI管道](img/B18143_11_7.jpg)'
- en: 'Figure 11.7: Vertex AI pipeline'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7：Vertex AI管道
- en: 'At a high level, our pipeline will perform the following steps:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，我们的管道将执行以下步骤：
- en: '**Ingest data**: First, we need to get our data into the Google Cloud environment.
    We’re using Google Cloud Storage to store our data, and when our pipeline kicks
    off a data processing and model training job, our data is read in from there.'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据采集**：首先，我们需要将我们的数据导入 Google Cloud 环境。我们使用 Google Cloud Storage 来存储我们的数据，当我们的流程启动数据处理和模型训练作业时，我们的数据将从那里读取。'
- en: '**Preprocess our data**: Google Cloud offers several tools for data preprocessing,
    including Dataflow, Dataprep, and Dataproc, which we explored in [*Chapter 6*](B18143_06.xhtml#_idTextAnchor187).
    More recently, Google Cloud also released a service named Serverless Spark, which
    enables us to run Spark jobs without having to provision and manage the required
    underlying infrastructure. This is what we use to implement our data preprocessing
    job in our pipeline in the practical exercises.'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预处理我们的数据**：Google Cloud 提供了多种数据预处理工具，包括 Dataflow、Dataprep 和 Dataproc，我们已在
    [*第 6 章*](B18143_06.xhtml#_idTextAnchor187) 中进行了探讨。最近，Google Cloud 还发布了一项名为 Serverless
    Spark 的服务，它使我们能够在不配置和管理所需的基础设施的情况下运行 Spark 作业。这就是我们在实际练习中在流程中实现数据预处理作业所使用的工具。'
- en: '**Develop and train our model**: Our pipeline trains a TensorFlow model in
    Vertex AI, using the processed data created in the previous step.'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**开发和训练我们的模型**：我们的流程在 Vertex AI 中使用前一步创建的已处理数据训练 TensorFlow 模型。'
- en: Register our model in the Vertex AI Model Registry.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Vertex AI 模型注册表中注册我们的模型。
- en: '**Deploy our model**: Our pipeline moves to the next step, which is to deploy
    our model to production. In this case, our pipeline creates a Vertex AI endpoint
    and hosts our model on that endpoint.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**部署我们的模型**：我们的流程进入下一步，即将我们的模型部署到生产环境中。在这种情况下，我们的流程创建一个 Vertex AI 端点，并在该端点上托管我们的模型。'
- en: Let’s take a look at what our pipeline is doing in more detail, by inspecting
    the solution architecture that we have just created, as depicted in *Figure 11**.6*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过检查我们刚刚创建的解决方案架构来更详细地了解我们的流程正在做什么，如图 *图 11*.*6* 所示。
- en: '![Figure 11.8: MLOps pipeline in Vertex AI](img/B18143_11_8.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.8：Vertex AI 中的 MLOps 流程](img/B18143_11_8.jpg)'
- en: 'Figure 11.8: MLOps pipeline in Vertex AI'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8：Vertex AI 中的 MLOps 流程
- en: 'In *Figure 11**.6*, the horizontal rectangular section that spans across the
    figure represents our pipeline that runs in Vertex AI Pipelines. Each of the steps
    in the process is numbered, and the numbers represent the following actions:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 11*.*6* 中，横跨整个图的水平矩形部分代表我们在 Vertex AI Pipelines 中运行的流程。流程中的每个步骤都进行了编号，编号代表以下操作：
- en: The data processing step in our pipeline submits a Spark job to Dataproc Serverless
    in order to execute our PySpark processing script.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们流程中的数据处理步骤向 Dataproc Serverless 提交一个 Spark 作业以执行我们的 PySpark 处理脚本。
- en: Dataproc fetches our PySpark script and the raw data from Google Cloud Storage
    and executes the PySpark script to process the raw data.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dataproc 从 Google Cloud Storage 获取我们的 PySpark 脚本和原始数据，并执行 PySpark 脚本来处理原始数据。
- en: Dataproc stores the resulting processed data in Google Cloud Storage.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dataproc 将处理后的数据存储在 Google Cloud Storage 中。
- en: The data processing job status is complete.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据处理作业状态已完成。
- en: The next step in our pipeline — the model training step — is invoked.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 流程中的下一步——模型训练步骤——被调用。
- en: Vertex AI Pipelines submits a model training job to the Vertex AI Training service.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vertex AI Pipelines 向 Vertex AI 训练服务提交一个模型训练作业。
- en: In order to execute our custom training job, the Vertex AI Training service
    fetches our custom Docker container from Google Artifact Registry, and it fetches
    the training data from Google Cloud Storage. This is the same data that was stored
    in Google Cloud Storage by our data processing job (i.e., it is the processed
    data that was created by our data processing job).
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了执行我们的自定义训练作业，Vertex AI 训练服务从 Google Artifact Registry 获取我们的自定义 Docker 容器，并从
    Google Cloud Storage 获取训练数据。这些数据就是我们的数据处理作业存储在 Google Cloud Storage 中的数据（即，这是由我们的数据处理作业创建的已处理数据）。
- en: When our model has been trained, the trained model artifacts are saved in Google
    Cloud Storage.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们的模型训练完成后，训练好的模型工件被保存在 Google Cloud Storage 中。
- en: The model training job status is complete.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练作业状态已完成。
- en: The next step in our pipeline — the model import step — is invoked. This is
    an intermediate step that prepares the model metadata to be referenced in later
    components of our pipeline. The relevant metadata in this case consists of the
    location of the model artifacts in Google Cloud Storage and the specification
    of the Docker container image in Google Artifact Registry that will be used to
    serve our model.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们管道的下一步——模型导入步骤——被调用。这是一个中间步骤，为我们的管道的后续组件准备模型元数据。在这种情况下，相关的元数据包括模型工件在Google
    Cloud Storage中的位置以及用于提供我们模型的Docker容器镜像在Google Artifact Registry中的规范。
- en: The next step in our pipeline — the model upload step — is invoked. This step
    references the metadata from the model import step.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们管道的下一步——模型上传步骤——被调用。此步骤引用了模型导入步骤中的元数据。
- en: The model metadata is used to register the model in Vertex AI Model Registry.
    This makes it easy to deploy our model for serving traffic in Vertex AI.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型元数据用于在Vertex AI模型注册表中注册模型。这使得在Vertex AI中部署我们的模型以处理流量变得容易。
- en: The model upload job status is complete.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型上传作业状态已完成。
- en: The next step in our pipeline — the endpoint creation step — is invoked.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们管道的下一步——端点创建步骤——被调用。
- en: An endpoint is created in the Vertex AI Prediction service. This endpoint will
    be used to host our model.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Vertex AI预测服务中创建了一个端点。这个端点将用于托管我们的模型。
- en: The endpoint creation job status is complete.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 端点创建作业状态已完成。
- en: The next step in our pipeline — the model deployment step — is invoked.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们管道的下一步——模型部署步骤——被调用。
- en: Our model is deployed to our endpoint in the Vertex AI Prediction service. This
    step references the metadata of the endpoint that has just been created by our
    pipeline, as well as the metadata of our model in the Vertex AI Model Registry.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的模式已部署到Vertex AI预测服务中的端点。此步骤引用了由我们的管道刚刚创建的端点的元数据，以及Vertex AI模型注册表中我们的模型的元数据。
- en: The model deployment job status is complete.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型部署作业状态已完成。
- en: In addition to all of the above steps that are explicitly performed by our pipeline,
    the Vertex AI Pipelines service also registers metadata related to our pipeline
    execution in the Vertex ML Metadata service.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们管道明确执行的上述所有步骤之外，Vertex AI Pipelines服务还将在Vertex ML Metadata服务中注册与我们的管道执行相关的元数据。
- en: Our model is now ready to serve inference requests! Isn’t it impressive that
    all of these activities and API calls across multiple Google Cloud services are
    being orchestrated automatically by our pipeline? After our pipeline has been
    defined, it can run automatically whenever we wish, without any need for human
    interaction throughout the process, unless we deem it necessary for humans to
    be involved in any of the steps.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模式现在已准备好处理推理请求！是不是很令人印象深刻，所有这些跨越多个Google Cloud服务的活动和API调用都是由我们的管道自动编排的？在我们定义了管道之后，它可以在我们希望的时候自动运行，在整个过程中不需要任何人工交互，除非我们认为有必要让人类参与任何步骤。
- en: If you have completed the activities in the notebook, then you are now officially
    an AI/ML guru! Seriously, you have just implemented an end-to-end MLOps pipeline.
    That is an extremely complex and advanced task in the ML industry.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经完成了笔记本中的活动，那么你现在正式成为AI/ML大师！说真的，你刚刚实现了一个端到端的MLOps管道。这是ML行业中的一个极其复杂和高级的任务。
- en: With all of that success under your belt, let’s take some time to reflect on
    what we learned in this chapter.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在取得所有这些成功之后，让我们花些时间来反思我们在本章中学到了什么。
- en: Summary
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we started with an introduction to MLOps at a high level, which
    is essentially a blend of machine learning, DevOps, and data engineering, in which
    the main goal is to automate the ML lifecycle, resulting in improved workflows
    and collaborations between data scientists and engineers. We discussed how MLOps
    allows organizations to streamline their ML operations, increase the speed of
    deployment, and maintain high-quality models in production, leading to a more
    efficient, effective, and reliable ML workflow, and thereby maximizing the value
    that organizations get from their ML initiatives.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从高层次介绍了MLOps，这基本上是机器学习、DevOps和数据工程的结合，其主要目标是自动化ML生命周期，从而改善工作流程和科学家与工程师之间的协作。我们讨论了MLOps如何使组织简化其ML操作，加快部署速度，并在生产中保持高质量模型，从而实现更高效、有效和可靠的ML工作流程，并最大限度地提高组织从其ML项目中获得的价值。
- en: We touched upon the various pain points that MLOps addresses, including but
    not limited to challenges related to managing and versioning models, ensuring
    reproducibility and consistency, monitoring and maintaining models, and fostering
    collaboration between different teams.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了MLOps解决的各种痛点，包括但不限于与管理和版本控制模型、确保可重复性和一致性、监控和维护模型以及促进不同团队间协作相关的挑战。
- en: We then dived into why MLOps is important for deploying large-scale machine
    learning workloads. It resolves many challenges that can crop up in machine learning
    systems, ranging from managing and versioning models to ensuring the reproducibility
    of experiments. It also facilitates continuous integration, deployment, monitoring,
    and validation of models and promotes better collaboration among teams.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们深入探讨了为什么MLOps对于部署大规模机器学习工作负载至关重要。它解决了机器学习系统中可能出现的一系列挑战，从管理和版本控制模型到确保实验的可重复性。它还促进了模型的持续集成、部署、监控和验证，并促进了团队间的更好协作。
- en: Next, we discussed various MLOps tools such as Kubeflow Pipelines and TensorFlow
    Extended, among others. Each of these tools offers unique functionalities catering
    to different stages of the ML lifecycle, including data versioning, experiment
    tracking, and model deployment.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们讨论了各种MLOps工具，如Kubeflow Pipelines和TensorFlow Extended等。这些工具中的每一个都提供独特的功能，满足ML生命周期不同阶段的需求，包括数据版本控制、实验跟踪和模型部署。
- en: We then performed an implementation of MLOps on Google Cloud using Vertex AI
    Pipelines, which involved multiple steps, including managing datasets, preprocessing
    data, training models, and monitoring models.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用Google Cloud上的Vertex AI Pipelines实现了MLOps，这涉及多个步骤，包括管理数据集、预处理数据、训练模型和监控模型。
- en: Next, we will explore four important and somewhat interrelated topics in the
    machine learning industry, which are bias, explainability, fairness, and lineage.
    Let’s move on to the next chapter to explore these concepts in detail.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨机器学习行业中的四个重要且相互关联的主题，即偏差、可解释性、公平性和溯源。让我们进入下一章，详细探讨这些概念。
