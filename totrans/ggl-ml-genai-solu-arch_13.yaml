- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning Engineering and MLOps with Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s generally estimated that almost 90% of data science projects never make
    it to production. Data scientists spend a lot of time training and experimenting
    with models in the lab, but often don’t succeed in bringing those workloads out
    into the real world. A major reason for this is because, as we have discussed
    in the previous chapters of this book, there are difficult challenges at every
    step in the model development lifecycle. Following on from our previous chapter,
    we will now dive into more detail on deployment concepts and challenges, and describe
    the importance of **Machine Learning Operations** (**MLOps**) in addressing these
    challenges for large-scale production AI/ML workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, this chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to MLOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why MLOps is needed for deploying large-scale ML workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLOps tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing MLOps on Google Cloud using Vertex AI Pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While we’ve already touched on some of these concepts so far in the book, we’ll
    kick off this chapter with a more formal introduction to MLOps.
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to MLOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLOps is an extension of the DevOps concept from the software development industry
    but with a specific focus on managing and automating ML model and project lifecycles.
    It goes beyond the tactical steps required to create machine learning models and
    addresses requirements that come to light when companies need to manage the entire
    lifecycle of data science use cases at scale. This is a good time to reflect on
    the ML model lifecycle stages we outlined in previous chapters in this book, as
    depicted in *Figure 11**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1: Data science lifecycle stages](img/B18143_11_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: Data science lifecycle stages'
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, MLOps aims to automate all of the various steps in the ML model
    lifecycle, such as data collection, data cleaning and preprocessing, model training,
    model evaluation, model deployment, and model monitoring. As such, it can be seen
    as a type of engineering culture that aims to unify ML system development and
    ML system day-to-day operations. This includes a practice for collaboration and
    communication among all relevant stakeholders in an ML project, such as a company’s
    data scientists, ML engineers, data engineers, business analysts, and operations
    staff, to help manage the overall machine learning lifecycle on an ongoing basis.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, managing the ML lifecycle includes defining, implementing,
    testing, deploying, monitoring, and managing ML models to ensure that they work
    reliably and efficiently in a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: In another similarity to DevOps, MLOps practices also encompass the concepts
    of continuous integration, continuous delivery, and continuous deployment (CI/CD),
    but with a specific focus on how ML models are developed. This ensures that new
    changes to the models are correctly integrated, thoroughly tested, and can be
    deployed to production in a systematic, reliable, and repeatable way. The goal
    in this regard is to ensure that any part of the ML lifecycle (data preprocessing,
    training, etc.) can be repeated at a later point with the same results. Just as
    with CI/CD in the context of software DevOps, this includes automating steps such
    as validation checks and integration tests, but in the case of ML model development,
    it also adds extra components such as data quality checks and model quality evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in previous chapters, deploying a model to production is a great
    achievement, but the work doesn’t stop there. Once ML models are deployed to production,
    they need to be continuously monitored to ensure their performance does not degrade
    over time due to changes in the underlying data or other factors. If any issues
    are detected, the models need to be updated or replaced with newer models. MLOps
    also includes tools to automate that part of the process, such as automatically
    retraining (for example, with fresh data), validating, and deploying a new model
    version.
  prefs: []
  type: TYPE_NORMAL
- en: Another important component of this is keeping track of various versions of
    the artifacts (e.g., data, models, and hyperparameter values) produced at various
    stages in our data science project. Not only does this enable easy rollback to
    previous versions if required, but it also provides an audit trail for compliance
    purposes, and to help with model explainability and transparency, as well as appropriate
    governance mechanisms, helping to ensure that ML models are used responsibly and
    ethically.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand what MLOps is, let’s dive into more detail on why it’s
    especially important when developing, deploying, and managing models at a large
    scale.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Later in this book, we will discuss generative AI/ML. There are particular types
    of ML models that are used in generative AI/ML that are called **Large Language
    Models** (**LLMs**). These models also have additional kinds of resources and
    artifacts associated with them, and a new term, LLMOps has emerged in the industry
    to refer to the operationalization of those workloads. For now, we will focus
    on traditional MLOps. While many of these concepts also apply to LLMOps workloads,
    we will discuss the additional considerations for LLMOps in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Why MLOps is needed for deploying large-scale ML workloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most important aspect of MLOps is that it helps organizations develop ML
    models in a faster, more efficient, and reliable manner, and it allows data science
    teams to experiment and innovate while also meeting operational requirements.
  prefs: []
  type: TYPE_NORMAL
- en: We know by now that ML has become an essential component of many industries
    and sectors, providing invaluable insights and decision-making capabilities, but
    that deploying ML models, especially at scale, presents many challenges. Some
    of these are challenges that can only be solved by MLOps, and we dive into more
    detail on such challenges in this section, as well as providing examples of how
    MLOps helps to address them.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive in, I’m going to point out that the kinds of challenges we will
    discuss in this section actually apply to any industry that creates products at
    a large scale, whether those products are cars, safety pins, toys, or machine
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: I’m going to take an analogy from the automobile industry in order to highlight
    the kinds of concepts that are required for large-scale production of any type
    of product. Consider that before cars were invented, a major form of transport
    would have been horse-drawn carts, and such carts would have been made in a kind
    of workshop like the one shown in *Figure 11**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2: Forge (source: https://www.hippopx.com/en/middle-ages-forge-workshop-old-castle-wagon-wheel-297217)](img/B18143_11_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.2: Forge (source: https://www.hippopx.com/en/middle-ages-forge-workshop-old-castle-wagon-wheel-297217)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the characteristics of the production environment shown in *Figure 11**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s a small environment, which could only accommodate a few people working
    together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whoever works here has lots of random tools lying around, and there doesn’t
    seem to be much standardization implemented
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With those kinds of characteristics, any kind of large-scale collaboration would
    not be possible, and at most, only three or four people could work together on
    creating a product. In such circumstances, a company could not possibly create
    a large number of products every month (for example). Bear in mind that this is
    how most companies begin to implement data science projects, in which data scientists
    perform various experiments and develop models on their own computers, using any
    random tools based on personal preferences, and trying to share learnings and
    artifacts with colleagues in a non-standardized and unsystematic way.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome these kinds of challenges, the automotive industry invented the
    concept of the assembly line, and some of the first assembly lines would have
    looked similar to the one shown in *Figure 11**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3: Assembly line (source:  https://pxhere.com/en/photo/798929)](img/B18143_11_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.3: Assembly line (source: https://pxhere.com/en/photo/798929)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the characteristics of the production environment shown in *Figure 11**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: The environment can accommodate many people working together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tools and manufacturing processes have been standardized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This kind of environment enables large-scale collaboration, and it’s possible
    to produce many more products in any given timeframe.
  prefs: []
  type: TYPE_NORMAL
- en: As automotive companies continued to evolve, they continued to apply more standardization
    and automation to each step in the process, leading to more efficient processes
    and reproducibility, until today’s assembly lines look like the one shown in *Figure
    11**.4*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4: Modern assembly line (source:  https://www.rawpixel.com/image/12417375/photo-image-technology-factory-green)](img/B18143_11_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.4: Modern assembly line (source: https://www.rawpixel.com/image/12417375/photo-image-technology-factory-green)'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 11**.4*, the production process has become highly automated
    and more efficient to operate at scale. With this analogy in mind, let’s look
    at how this idea maps to the concepts and steps in the ML model development lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Model management and versioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an organization scales up its ML efforts, the number of models in operation
    can increase exponentially. Managing these models, keeping track of their versions,
    and knowing when to update or retire them can become quite difficult. Without
    a proper versioning and management system, ML models and their corresponding datasets
    can become disorganized, which leads to confusion and errors in the model development
    process. In fact, when we remember that many large companies have hundreds or
    even thousands of models in production, and are constantly developing new versions
    of those models to improve their performance, managing all of those models without
    specialized tools is basically impossible. MLOps tools provide mechanisms to facilitate
    easier model management and versioning, allowing teams to track and control their
    models in an organized and efficient way.
  prefs: []
  type: TYPE_NORMAL
- en: Productivity and automation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training, testing, deploying, and retraining models often involves many repetitive
    tasks, and as the scale of ML workloads increases, the time and effort required
    to manually manage these processes can become prohibitive. Considering that MLOps
    introduces automation at various stages of the ML lifecycle, such as data preprocessing,
    model training, testing, deployment, and monitoring, this frees up the data science
    teams to focus on more valuable tasks. Another important aspect of automation
    is that it reduces the likelihood of human error, which can help to avoid business-affecting
    mistakes, and increase productivity.
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reproducibility is arguably one of the most important factors in any industry
    that creates products at a large scale. Imagine if every time you tried to purchase
    the same product from a company, you received something slightly different. Most
    likely, you would lose faith in that company and would stop purchasing their products.
    Without well-established reproducibility in production processes, a company simply
    will not scale. In the context of machine learning, when we have a large number
    of models and datasets, and different types of development environments, ensuring
    the reproducibility of experiments can be a significant challenge. It can be difficult
    to recreate the exact conditions of a previous experiment due to the lack of version
    control on data, models, and code. This could lead to inconsistent results and
    make it difficult to build upon previous work. MLOps frameworks provide tools
    and practices for maintaining a record of all experiments, including the data
    used, the parameters set, the model architecture, and the outcomes. This allows
    experiments to be easily repeated, compared, and audited.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Integration/Continuous Deployment (CI/CD)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to being able to reliably reproduce a product, developing products
    at a large scale also requires the ability to incrementally improve your products
    over time. CI/CD pipelines automate the testing and deployment of models, ensuring
    that new changes are integrated and deployed seamlessly, efficiently, and with
    minimal errors. This enables data scientists to experiment and try out different
    kinds of updates quickly, easily, and in a controlled manner that conforms to
    your company’s required standards. We should note that CI/CD is such an integral
    component of DevOps that the terms are often used almost interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: What’s even more interesting is that we can also manage an MLOps pipeline within
    a standard DevOps CI/CD pipeline. For example, when using Vertex AI Pipelines,
    we can define our pipeline using code, and that code can be stored in a code repository
    such as Google Cloud Source Repositories and then managed via a DevOps CI/CD pipeline
    in Google Cloud Build. This enables us to apply all the benefits of DevOps, such
    as code versioning and automated testing, to the code that defines our pipelines,
    so that we can control how updates are made to our pipeline definitions.
  prefs: []
  type: TYPE_NORMAL
- en: Model validation and testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Testing machine learning models involves unique challenges due to their probabilistic
    nature, and traditional DevOps software testing techniques may not be sufficient.
    MLOps introduces practices and methodologies specifically for automating the validation
    and testing of ML models, ensuring they perform as expected before being deployed
    to production, and for their entire lifetime in production after deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and maintenance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we’ve discussed, once ML models are deployed, their performance needs to
    be continuously monitored to ensure they consistently provide accurate predictions.
    We also discussed how changes in real-world data can lead to a decrease in model
    performance over time, and as a result, we need tools and processes for continuous
    monitoring and alerting, enabling teams to react quickly to any degradation in
    performance and retrain models as necessary. This is an important component of
    any MLOps framework.
  prefs: []
  type: TYPE_NORMAL
- en: Collaboration and communication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As ML projects scale, they typically require collaboration across various teams
    and roles, including data scientists, ML engineers, data engineers, business analysts,
    and IT operations. In many organizations, there is a gap between the data scientists
    who develop ML models and the IT operations teams who deploy and maintain these
    models in production. Without a common platform and standard practices, different
    team members might use inconsistent methods, tools, and data, leading to inefficient
    workflows and potential mistakes. MLOps fosters effective collaboration and communication
    among these stakeholders, enabling them to work together more efficiently and
    avoid misunderstandings or misalignments.
  prefs: []
  type: TYPE_NORMAL
- en: Regulatory compliance and governance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In industries such as healthcare and finance, ML models must comply with specific
    regulatory requirements. MLOps provides mechanisms for ensuring transparency,
    interpretability, and auditability of models, therefore helping to maintain regulatory
    compliance. Also, without MLOps, managing multiple models across different teams
    and business units can become a challenge. MLOps allows for centralized model
    governance, making it easier to keep track of various models’ performance, status,
    and owners.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have dived into why MLOps is needed, especially in the context of
    managing ML workloads at scale, let’s take a look at some of the popular tools
    that have been developed in the industry for implementing those concepts.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve talked about the goals and benefits of MLOps, but how can we actually
    achieve these goals and benefits in the real world? Many tools have been developed
    to facilitate various aspects of MLOps, from data versioning to model deployment
    and monitoring. In this section, we discuss the tools that make MLOps a reality.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline orchestration tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we want to standardize the steps in a process and run them automatically,
    we typically configure them as a set of sequential actions. This sequential set
    of actions is often referred to as a pipeline. However, simply configuring the
    order of the steps is not enough; we also need some kind of system to “orchestrate”
    (i.e., execute and manage) the pipeline, and we can use some popular workflow
    orchestration tools for this purpose, such as Kubeflow, Airflow, and **TensorFlow
    Extended** (**TFX**). We already covered Airflow in [*Chapter 6*](B18143_06.xhtml#_idTextAnchor187),
    but let’s take a look at Kubeflow and TFX in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is an open source project created by Google, and now maintained by an
    open source community of contributors, aimed at making it easier to run machine
    learning workflows on Kubernetes. Fundamental principles of Kubeflow include portability,
    scalability, and extensibility. Portability refers to the concept of “write once,
    run anywhere,” which builds on the core tenet of containerization, whereby you
    can run your workloads across different types of computing environments in a consistent
    manner. Scalability refers to the ability to use Kubernetes to easily scale your
    model training and prediction workloads as needed, and extensibility means that
    you can implement customized use cases by adding popular tools and libraries that
    easily integrate with Kubeflow. In this case, rather than simply being a tool,
    Kubeflow is a framework and ecosystem that we can use to orchestrate and manage
    our machine learning workflows. The following are some core components of the
    Kubeflow ecosystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubeflow Pipelines** (**KFP**): For defining, deploying, and managing ML
    workflows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Katib**: For hyperparameter tuning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KFServing**: For serving models in a scalable way'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow and PyTorch Operators**: For running TensorFlow and PyTorch jobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training Operators**: For distributed training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will particularly dive deep into Kubeflow Pipelines in the practical exercises
    associated with this chapter, in which we will build a pipeline and execute it
    in Google Cloud Vertex AI. In addition to KFP, Vertex AI also supports using TFX
    to build and manage ML pipelines. Let’s take a look at that next.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Extended (TFX)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TFX is an end-to-end platform, also created by Google, that manages and deploys
    production ML pipelines. As the name suggests, it was designed as an extension
    to TensorFlow, with the aim of making it easier to bring trained models to production.
    Just like KFP, TFX offers components for implementing each stage of the ML lifecycle,
    covering everything from data ingestion and validation to model training, tuning,
    serving, and monitoring. Also similar to KFP, TFX’s core principles include scalability,
    and of course, extensibility. Other core principles of TFX include reproducibility
    and modularity. Reproducibility, as we discussed earlier in this chapter, is a
    critical requirement for producing pretty much anything at a large scale. Modularity,
    in this context, refers to the fact that the various components of TFX can be
    used individually or together, enabling customization. Speaking of which, the
    various components and responsibilities of TFX include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: ExampleGen, which imports and ingests data into the TFX pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StatisticsGen, which computes statistics for the ingested data, essential for
    understanding the data and feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SchemaGen, which examines dataset statistics and creates a schema for the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ExampleValidator, which identifies and analyzes anomalies and missing values
    in the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transform, which can be used for feature engineering on the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trainer, which defines and trains a model using TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuner, which can be used to optimize hyperparameters using Keras Tuner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: InfraValidator, which ensures that the model can be loaded and served in a production
    environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluator, which uses the TensorFlow Model Analysis library to evaluate the
    metrics of the trained model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BulkInferrer, which performs batch processing on a model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blessing and deployment: If the model is validated, it can be “blessed” and
    then deployed using a serving infrastructure such as TensorFlow Serving.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Considering that both TFX and KFP can be used to build and run pipelines in
    Google Cloud Vertex AI, it may be challenging to decide which one to use. On that
    matter, the official Google Cloud documentation recommends the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“If you use TensorFlow in an ML workflow that processes terabytes of structured
    data or text data, we recommend that you build your pipeline using TFX… For other
    use cases, we recommend that you build your pipeline using the Kubeflow* *Pipelines
    SDK.”*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re not going to focus on TFX in a lot of detail in this chapter, but if
    you would like to learn how to build a TFX pipeline, then there’s a useful tutorial
    in the TensorFlow documentation at the following link: [https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple).'
  prefs: []
  type: TYPE_NORMAL
- en: There are also many other open source tools for implementing MLOps pipelines,
    such as MLFlow, which provides interfaces for tracking experiments, packaging
    code into reproducible runs, and sharing and deploying models.
  prefs: []
  type: TYPE_NORMAL
- en: While KFP and TFX can be used to orchestrate your entire MLOps pipeline, there
    are also tools that focus on more specific components of the ML lifecycle, and
    that can integrate with KFP and TFX in order to customize your overall pipeline.
    Let’s take a look at some of those tools next.
  prefs: []
  type: TYPE_NORMAL
- en: Experiment and lineage tracking tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed earlier in this chapter, and in earlier chapters, when running
    ML workloads at a large scale, it’s important to keep track of every step in the
    development of every model, so that you can easily understand how any given model
    was created. This is required for characteristics such as reproducibility, explainability,
    and compliance. Imagine if you have thousands of models in production, and compliance
    regulators ask you to explain how a particular model was created. Without a well-established
    system for tracking every aspect of every model’s creation, such as which versions
    of which datasets were used to train the model, what kinds of algorithms and hyperparameters
    were used, what the initial evaluation metrics were, and who deployed the model
    to production, this would be an impossible task. This is where experiment and
    lineage tracking tools come into the picture. Let’s consider what kinds of tools
    we have at our disposal in this regard.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vertex AI Experiments is a managed service that helps you track, compare, and
    analyze your machine learning experiments. It provides a central place to manage
    your experiments, and it makes it easy to compare different models and hyperparameters.
    Once an experiment is created (using the Vertex AI SDK or the Vertex AI user interface),
    you can start tracking your training runs, and Vertex AI Experiments will automatically
    collect metrics from your training runs, such as accuracy, loss, and training
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Once your training runs are complete, you can view graphs and tables of the
    metrics, and you can use statistical tests to determine which model is the best.
    You can also use the Experiments dashboard to visualize your results, and you
    can use the Experiments API to export your results to a spreadsheet or a notebook.
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TensorBoard is a web-based tool that comes with TensorFlow and is designed to
    help visualize and understand machine learning models, as well as to debug and
    optimize them. It provides an interactive interface for various aspects of machine
    learning models and training processes and can make it easy to create metric graphs
    such as ROC curves, which we introduced in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Model deployment and monitoring tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this context, we have tools such as Vertex AI Prediction and Model Monitoring,
    which we reviewed in detail in [*Chapter 10*](B18143_10.xhtml#_idTextAnchor259).
    There are also open source tools such as **TensorFlow Serving** (**TFServing**)
    for TensorFlow models, and TorchServe.
  prefs: []
  type: TYPE_NORMAL
- en: Model interpretability and explainability tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we’ve discussed previously, model interpretability and explainability are
    extremely important concepts in machine learning. They relate to reproducibility,
    compliance, and performance. For example, if you don’t have a good understanding
    of how your models work, then it will be harder to continuously improve them in
    a systematic way. These concepts also relate to fairness. That is, in order to
    ensure that a model is making fair and ethical predictions, you need to understand
    how it works in great detail. Fortunately, there are tools that can help us in
    this regard, which we discuss here.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Vertex AI provides tools such as Explainable AI and Fairness Indicators,
    which can help us to understand how our machine learning models work, and how
    they would behave under different conditions. There are also open source tools
    such as **SHAP** (short for **SHapley Additive exPlanations**), which uses a game
    theory approach to explain the output of any machine learning model, and **LIME**
    (short for **Local Interpretable Model-agnostic Explanations**), which is a Python
    library that allows us to explain the predictions of any machine learning classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter in this book is dedicated to the concepts of bias, fairness,
    explainability, and lineage, so we will dive into these concepts and tools in
    a lot more detail there.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered many of the different kinds of tools that can be used
    to implement MLOps workloads, let’s dive into how we can specifically do this
    on Google Cloud Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing MLOps on Google Cloud using Vertex AI Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover the steps to build an MLOps pipeline on Google
    Cloud using Vertex AI Pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prerequisite: IAM permissions'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will use the same Vertex AI Workbench notebook instance
    that we created in [*Chapter* *5*](B18143_05.xhtml#_idTextAnchor168). That user-managed
    notebook uses the default Compute Engine service account, which is granted the
    IAM basic Editor role by default. When we build and execute our pipeline in our
    notebook, we decide to let our pipeline inherit the same permissions used by our
    notebook. This is a decision we make by not specifying a different role for our
    pipeline executions. We could specify a different role if we wanted to, but for
    simplicity purposes, we’ll take the approach of having our pipeline use the default
    Compute Engine service account. By default, the Editor role (used by the default
    Compute Engine service account) will allow us to perform all of the required activities
    in Vertex AI in this chapter, but our pipeline will also need to run some steps
    on Dataproc. For this reason, we will add the Dataproc Worker and Dataproc Editor
    roles to the default Compute Engine service account. To do that, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to **Google Cloud services** → **IAM &
    Admin** → **IAM**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the list of principals, click on the pencil symbol for the default Compute
    Engine service account (the service account name will have the format `-[PROJECT_NUMBER]-compute@developer.gserviceaccount.com`;
    see *Figure 11**.5* for reference).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.5: Edit service account](img/B18143_11_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.5: Edit service account'
  prefs: []
  type: TYPE_NORMAL
- en: On the screen that appears, select `Dataproc` and select **Dataproc Editor**
    from the list of roles.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 3 and select **Dataproc Worker**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The updated roles will look as shown in *Figure 11**.6*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.6: Updated roles](img/B18143_11_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.6: Updated roles'
  prefs: []
  type: TYPE_NORMAL
- en: Click **Save**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And that’s it – you have successfully added the required roles.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As I mentioned in the previous section, we can use the same Vertex AI Workbench
    notebook instance that we created in [*Chapter* *5*](B18143_05.xhtml#_idTextAnchor168)
    to build our MLOps pipeline. Please open JupyterLab on that notebook instance.
    In the directory explorer on the left side of the screen, navigate to the `Chapter-11`
    directory and open the `mlops.ipynb` notebook. You can choose **Python (Local)**
    as the kernel. Again, you can run each cell in the notebook by selecting the cell
    and pressing *Shift* + *Enter* on your keyboard. In addition to the relevant code,
    the notebook contains markdown text that describes what the code is doing.
  prefs: []
  type: TYPE_NORMAL
- en: In the practical exercises, we will build and run an MLOps pipeline that will
    execute all of the phases of the ML model development lifecycle that we have covered
    so far in this book. Our pipeline is depicted in *Figure 11**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7: Vertex AI pipeline](img/B18143_11_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.7: Vertex AI pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, our pipeline will perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ingest data**: First, we need to get our data into the Google Cloud environment.
    We’re using Google Cloud Storage to store our data, and when our pipeline kicks
    off a data processing and model training job, our data is read in from there.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Preprocess our data**: Google Cloud offers several tools for data preprocessing,
    including Dataflow, Dataprep, and Dataproc, which we explored in [*Chapter 6*](B18143_06.xhtml#_idTextAnchor187).
    More recently, Google Cloud also released a service named Serverless Spark, which
    enables us to run Spark jobs without having to provision and manage the required
    underlying infrastructure. This is what we use to implement our data preprocessing
    job in our pipeline in the practical exercises.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Develop and train our model**: Our pipeline trains a TensorFlow model in
    Vertex AI, using the processed data created in the previous step.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register our model in the Vertex AI Model Registry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Deploy our model**: Our pipeline moves to the next step, which is to deploy
    our model to production. In this case, our pipeline creates a Vertex AI endpoint
    and hosts our model on that endpoint.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s take a look at what our pipeline is doing in more detail, by inspecting
    the solution architecture that we have just created, as depicted in *Figure 11**.6*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8: MLOps pipeline in Vertex AI](img/B18143_11_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.8: MLOps pipeline in Vertex AI'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 11**.6*, the horizontal rectangular section that spans across the
    figure represents our pipeline that runs in Vertex AI Pipelines. Each of the steps
    in the process is numbered, and the numbers represent the following actions:'
  prefs: []
  type: TYPE_NORMAL
- en: The data processing step in our pipeline submits a Spark job to Dataproc Serverless
    in order to execute our PySpark processing script.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dataproc fetches our PySpark script and the raw data from Google Cloud Storage
    and executes the PySpark script to process the raw data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dataproc stores the resulting processed data in Google Cloud Storage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data processing job status is complete.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step in our pipeline — the model training step — is invoked.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vertex AI Pipelines submits a model training job to the Vertex AI Training service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In order to execute our custom training job, the Vertex AI Training service
    fetches our custom Docker container from Google Artifact Registry, and it fetches
    the training data from Google Cloud Storage. This is the same data that was stored
    in Google Cloud Storage by our data processing job (i.e., it is the processed
    data that was created by our data processing job).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When our model has been trained, the trained model artifacts are saved in Google
    Cloud Storage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model training job status is complete.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step in our pipeline — the model import step — is invoked. This is
    an intermediate step that prepares the model metadata to be referenced in later
    components of our pipeline. The relevant metadata in this case consists of the
    location of the model artifacts in Google Cloud Storage and the specification
    of the Docker container image in Google Artifact Registry that will be used to
    serve our model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step in our pipeline — the model upload step — is invoked. This step
    references the metadata from the model import step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model metadata is used to register the model in Vertex AI Model Registry.
    This makes it easy to deploy our model for serving traffic in Vertex AI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model upload job status is complete.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step in our pipeline — the endpoint creation step — is invoked.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An endpoint is created in the Vertex AI Prediction service. This endpoint will
    be used to host our model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The endpoint creation job status is complete.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step in our pipeline — the model deployment step — is invoked.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our model is deployed to our endpoint in the Vertex AI Prediction service. This
    step references the metadata of the endpoint that has just been created by our
    pipeline, as well as the metadata of our model in the Vertex AI Model Registry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model deployment job status is complete.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In addition to all of the above steps that are explicitly performed by our pipeline,
    the Vertex AI Pipelines service also registers metadata related to our pipeline
    execution in the Vertex ML Metadata service.
  prefs: []
  type: TYPE_NORMAL
- en: Our model is now ready to serve inference requests! Isn’t it impressive that
    all of these activities and API calls across multiple Google Cloud services are
    being orchestrated automatically by our pipeline? After our pipeline has been
    defined, it can run automatically whenever we wish, without any need for human
    interaction throughout the process, unless we deem it necessary for humans to
    be involved in any of the steps.
  prefs: []
  type: TYPE_NORMAL
- en: If you have completed the activities in the notebook, then you are now officially
    an AI/ML guru! Seriously, you have just implemented an end-to-end MLOps pipeline.
    That is an extremely complex and advanced task in the ML industry.
  prefs: []
  type: TYPE_NORMAL
- en: With all of that success under your belt, let’s take some time to reflect on
    what we learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started with an introduction to MLOps at a high level, which
    is essentially a blend of machine learning, DevOps, and data engineering, in which
    the main goal is to automate the ML lifecycle, resulting in improved workflows
    and collaborations between data scientists and engineers. We discussed how MLOps
    allows organizations to streamline their ML operations, increase the speed of
    deployment, and maintain high-quality models in production, leading to a more
    efficient, effective, and reliable ML workflow, and thereby maximizing the value
    that organizations get from their ML initiatives.
  prefs: []
  type: TYPE_NORMAL
- en: We touched upon the various pain points that MLOps addresses, including but
    not limited to challenges related to managing and versioning models, ensuring
    reproducibility and consistency, monitoring and maintaining models, and fostering
    collaboration between different teams.
  prefs: []
  type: TYPE_NORMAL
- en: We then dived into why MLOps is important for deploying large-scale machine
    learning workloads. It resolves many challenges that can crop up in machine learning
    systems, ranging from managing and versioning models to ensuring the reproducibility
    of experiments. It also facilitates continuous integration, deployment, monitoring,
    and validation of models and promotes better collaboration among teams.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we discussed various MLOps tools such as Kubeflow Pipelines and TensorFlow
    Extended, among others. Each of these tools offers unique functionalities catering
    to different stages of the ML lifecycle, including data versioning, experiment
    tracking, and model deployment.
  prefs: []
  type: TYPE_NORMAL
- en: We then performed an implementation of MLOps on Google Cloud using Vertex AI
    Pipelines, which involved multiple steps, including managing datasets, preprocessing
    data, training models, and monitoring models.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explore four important and somewhat interrelated topics in the
    machine learning industry, which are bias, explainability, fairness, and lineage.
    Let’s move on to the next chapter to explore these concepts in detail.
  prefs: []
  type: TYPE_NORMAL
