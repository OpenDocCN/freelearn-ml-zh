<html><head></head><body>
		<div><h1 id="_idParaDest-275" class="chapter-number"><a id="_idTextAnchor1323"/><st c="0">10</st></h1>
			<h1 id="_idParaDest-276"><a id="_idTextAnchor1324"/><a id="_idTextAnchor1325"/><st c="3">Creating Features from a Time Series with tsfresh</st></h1>
			<p><st c="53">Throughout this book, we’ve discussed feature engineering methods and tools tailored for tabular and relational datasets. </st><st c="176">In this chapter, we will shift our focus to time-series data. </st><st c="238">A time series is a sequence of observations taken sequentially over time. </st><st c="312">Examples include energy generation and demand, temperature, air pollutant concentration, stock prices, and sales revenue. </st><st c="434">Each of these examples represents a variable and their values change </st><st c="503">over time.</st></p>
			<p><st c="513">The widespread availability of affordable sensors capable of measuring motion, movement, humidity, glucose, and other parameters has significantly increased the amount of temporally annotated data. </st><st c="712">These time series can be utilized in various classification tasks. </st><st c="779">For instance, by analyzing the electricity usage pattern of a household at a given time interval, we can infer whether a particular appliance was being used. </st><st c="937">Similarly, the signal of an ultrasound sensor can help determine the probability of a (gas) pipeline failure, and the characteristics of a sound wavelength can help predict whether a listener will like a song. </st><st c="1147">Time-series data is also valuable for regression tasks. </st><st c="1203">For example, signals from machinery sensors can be used to predict the remaining useful life of </st><st c="1299">the device.</st></p>
			<p><st c="1310">To use time series with traditional supervised machine learning models, such as linear and logistic regression, or decision-tree-based algorithms, we need to map each time series into a well-defined feature vector that captures its characteristics. </st><st c="1560">Time-series patterns, including trends, seasonality, and periodicity, among other things, can be captured by a combination of simple and complex mathematical operations. </st><st c="1730">Simple calculations include, for instance, taking the mean and the standard deviation of the time series. </st><st c="1836">More complex methods include determining correlation or entropy, for example. </st><st c="1914">In addition, we can apply non-linear time-series analysis functions to decompose the time-series signal, for example, Fourier or wavelet transformations, and use the parameters of these functions as features of the </st><st c="2129">supervised models.</st></p>
			<p><st c="2147">Creating features from time series can be very time-consuming; we need to apply various signal processing and time-series analysis algorithms to identify and extract meaningful features. </st><st c="2335">The </st><code><st c="2339">tsfresh</st></code><st c="2346"> Python package, which stands for </st><code><st c="2597">tsfresh</st></code><st c="2604"> includes a feature selection algorithm that identifies the most predictive features for a given time series. </st><st c="2714">By automating the application of complex time-series methods, </st><code><st c="2776">tsfresh</st></code><st c="2783"> bridges the gap between signal-processing experts and machine learning practitioners, making it easier to extract valuable features from </st><st c="2921">time-series data.</st></p>
			<p><st c="2938">In this chapter, we will learn how to automatically create hundreds of features from time-series data by utilizing </st><code><st c="3054">tsfresh</st></code><st c="3061">. Following that, we will discuss how to fine-tune this feature creation process by selecting the most relevant features, extracting different features from different time series, and integrating the feature creation process into a scikit-learn pipeline to classify </st><st c="3327">time-series data.</st></p>
			<p><st c="3344">In this chapter, we will go through the </st><st c="3385">following recipes:</st></p>
			<ul>
				<li><st c="3403">Extracting hundreds of features automatically from a </st><st c="3457">time series</st></li>
				<li><st c="3468">Automatically creating and selecting predictive features from </st><st c="3531">time-series data</st></li>
				<li><st c="3547">Extracting different features from different </st><st c="3593">time series</st></li>
				<li><st c="3604">Creating a subset of features identified through </st><st c="3654">feature selection</st></li>
				<li><st c="3671">Embedding feature creation into a </st><st c="3706">scikit-learn pipeline</st></li>
			</ul>
			<h1 id="_idParaDest-277"><a id="_idTextAnchor1326"/><a id="_idTextAnchor1327"/><st c="3727">Technical requirements</st></h1>
			<p><st c="3750">In this chapter, we will use the open source </st><code><st c="3796">tsfresh</st></code><st c="3803"> Python library. </st><st c="3820">You can install </st><code><st c="3836">tsfresh</st></code><st c="3843"> with </st><code><st c="3849">pip</st></code><st c="3852"> by executing </st><code><st c="3866">pip </st></code><code><st c="3870">install tsfresh</st></code><st c="3885">.</st></p>
			<p class="callout-heading"><st c="3886">Note</st></p>
			<p class="callout"><st c="3891">If you have an old Microsoft operating system, you may need to update the Microsoft C++ build tools to proceed with the </st><code><st c="4012">tsfresh</st></code><st c="4019"> package’s installation. </st><st c="4044">Follow the steps in this thread to do </st><st c="4082">so: </st><a href="https://stackoverflow.com/questions/64261546/how-to-solve-error-microsoft-visual-c-14-0-or-greater-is-required-when-inst"><st c="4086">https://stackoverflow.com/questions/64261546/how-to-solve-error-microsoft-visual-c-14-0-or-greater-is-required-when-inst</st></a><st c="4206">.</st></p>
			<p><st c="4207">We will work</st><a id="_idIndexMarker750"/><st c="4220"> with the </st><strong class="bold"><st c="4230">Occupancy Detection</st></strong><st c="4249"> dataset from the UCI Machine Learning Repository, available at </st><a href="http://archive.ics.uci.edu/ml/datasets/Occupancy+Detection"><st c="4313">http://archive.ics.uci.edu/ml/datasets/Occupancy+Detection</st></a><st c="4371"> and licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) license: </st><a href="https://creativecommons.org/licenses/by/4.0/legalcode"><st c="4461">https://creativecommons.org/licenses/by/4.0/legalcode</st></a><st c="4514">. The corresponding citation for this data is </st><st c="4560">as follows:</st></p>
			<p><st c="4571">Candanedo, Luis. </st><st c="4589">(2016). </st><st c="4597">Occupancy Detection. </st><st c="4618">UCI Machine Learning </st><st c="4639">Repository. </st><a href="https://doi.org/10.24432/C5X01N"><st c="4651">https://doi.org/10.24432/C5X01N</st></a><st c="4682">.</st></p>
			<p><st c="4683">I downloaded and modified the data as shown in this </st><st c="4736">notebook: </st><a href="https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh/prepare-occupancy-dataset.ipynb"><st c="4746">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh/prepare-occupancy-dataset.ipynb</st></a></p>
			<p><st c="4885">For a copy of the modified dataset and a target variable, check out the files called </st><code><st c="4971">occupancy.csv</st></code><st c="4984"> and </st><code><st c="4989">occupancy_target.csv</st></code><st c="5009">, available at the following </st><st c="5038">link: </st><a href="https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh"><st c="5044">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh</st></a></p>
			<p><st c="5151">The Occupancy Detection dataset contains time-series data taken over 135 hours at one-minute intervals. </st><st c="5256">The variables measure the temperature, humidity, </st><img src="img/38.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.996em;width:1.605em"/><st c="5305"/><st c="5309"> level, and light consumption in an office. </st><st c="5352">Camera footage was used to determine whether someone was in the office. </st><st c="5424">The target variable shows whether the office was occupied at any one hour. </st><st c="5499">If the target takes the value </st><code><st c="5529">1</st></code><st c="5530">, it means that the office was occupied during that hour; otherwise, it takes the </st><st c="5612">value </st><code><st c="5618">0</st></code><st c="5619">.</st></p>
			<p><st c="5620">The dataset with the time series and that with the target variable have different numbers of rows. </st><st c="5720">The time-series dataset contains 135 hours of records at one-minute intervals – that is, 8,100 rows. </st><st c="5821">The target has only 135 rows, with a label indicating whether the office was occupied at each of the </st><st c="5922">135 hours.</st></p>
			<p class="callout-heading"><st c="5932">Note</st></p>
			<p class="callout"><st c="5937">Check out the notebook in this book’s GitHub repository for plots of the different time series to become familiar with the </st><st c="6061">dataset: </st><a href="https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh/prepare-occupancy-dataset.ipynb"><st c="6070">https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh/prepare-occupancy-dataset.ipynb</st></a><a id="_idTextAnchor1328"/><a id="_idTextAnchor1329"/></p>
			<h1 id="_idParaDest-278"><a id="_idTextAnchor1330"/><st c="6209">Extracting hundreds of features automatically from a time series</st></h1>
			<p><st c="6274">Time series </st><a id="_idIndexMarker751"/><st c="6287">a</st><a id="_idTextAnchor1331"/><st c="6288">re data points indexed in time </st><a id="_idIndexMarker752"/><st c="6319">order. </st><st c="6326">Analyzing time-series sequences allows us to make various predictions. </st><st c="6397">For example, sensor data can be used to predict pipeline failures, sound data can help identify music genres, health history or personal measurements such as glucose levels can indicate whether a person is sick, and, as we will show in this recipe, patterns of light usage, humidity, and </st><img src="img/39.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.996em;width:1.591em"/><st c="6685"/><st c="6689"> levels can determine whether an office </st><st c="6728">is occupied.</st></p>
			<p><st c="6740">To tra</st><a id="_idTextAnchor1332"/><st c="6747">in regression and classification models using traditional machine learning algorithms, such as linear regression or random forests, we require a dataset of size </st><em class="italic"><st c="6909">M x N</st></em><st c="6914">, where M is the number of rows and N is the number of features or columns. </st><st c="6990">However, with time-series data, what we have is a collection of </st><em class="italic"><st c="7054">M</st></em><st c="7055"> time series, and each time series has multiple rows indexed chronologically. </st><st c="7133">To use time series in supervised learning models, each time series needs to be mapped into a well-defined feature vector, </st><em class="italic"><st c="7255">N</st></em><st c="7256">, as shown in the </st><st c="7274">following diagram</st><a id="_idTextAnchor1333"/><st c="7291">:</st></p>
			<div><div><img src="img/B22396_10_01.jpg" alt="Figure 10.1 – Diagram showing the process of feature creation from a time series for classification or regression"/><st c="7293"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="7294">Figure 10.1 – Diagram showing the process of feature creation from a time series for classification or regression</st></p>
			<p><st c="7407">The</st><a id="_idTextAnchor1334"/><st c="7411">se feature vectors, which are represented as </st><img src="img/40.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:0.836em"/><st c="7457"/><st c="7458">, </st><img src="img/41.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:0.838em"/><st c="7460"/><st c="7461">, and </st><img src="img/42.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.340em;height:0.788em;width:0.840em"/><st c="7467"/><st c="7468"> in </st><em class="italic"><st c="7472">Figure 10</st></em><em class="italic"><st c="7481">.1</st></em><st c="7483">, should capture the characteristics of the time series. </st><st c="7540">For example, </st><img src="img/43.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:0.744em"/><st c="7553"/><st c="7554">could be the mean value of the time series and </st><img src="img/44.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:0.744em"/><st c="7601"/><st c="7602">its variance. </st><st c="7616">We can crea</st><a id="_idTextAnchor1335"/><st c="7627">te many features to characterize the time series concerning the distribution of data points, correlation properties, stationarity, or entropy, among others. </st><st c="7785">Therefore, the feature vector, N, can be constructed by applying a series </st><a id="_idIndexMarker753"/><st c="7859">of </st><strong class="bold"><st c="7862">characterization methods</st></strong><st c="7886"> t</st><a id="_idTextAnchor1336"/><st c="7888">hat take a time series as input and return one or more scalars as output. </st><st c="7962">The mean, or the sum, takes the time-series sequence as input and returns a single scalar as output, with the mean value of the time series or the sum of its values. </st><st c="8128">We can also fit a linear trend to the time-series sequence, which will return two scalars – one with the slope and one with </st><st c="8252">the intercep</st><a id="_idTextAnchor1337"/><st c="8264">t.</st></p>
			<p><code><st c="8267">tsfresh</st></code><st c="8275"> applies 63 characterization methods to a time series, each of which returns one or more scalars, therefore resulting in more than 750 features for any given time series. </st><st c="8446">In this recipe, we will use </st><code><st c="8474">tsfresh</st></code><st c="8481"> to transform time-series data into an M x N feature </st><a id="_idIndexMarker754"/><st c="8534">table, which </st><a id="_idIndexMarker755"/><st c="8547">we will then use to predict </st><st c="8575">office occupa</st><a id="_idTextAnchor1338"/><a id="_idTextAnchor1339"/><st c="8588">ncy.</st></p>
			<h2 id="_idParaDest-279"><a id="_idTextAnchor1340"/><st c="8593">Getting ready</st></h2>
			<p><st c="8607">In this recipe, we wil</st><a id="_idTextAnchor1341"/><st c="8630">l use the Occupancy Detection dataset described in the </st><em class="italic"><st c="8686">Technical requirements</st></em><st c="8708"> section. </st><st c="8718">This dataset contains measurements of temperature, humidity, </st><img src="img/45.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.996em;width:1.522em"/><st c="8779"/><st c="8782"> level, and light consumption in an office taken at one-minute intervals. </st><st c="8855">There are 135 hours of measurements, and each hour is flagged with a unique identifier. </st><st c="8943">There is also a dataset with a target variable that indicates in which of these 135 hours the office was occupied. </st><st c="9058">Let’s load the data and make some plot</st><a id="_idTextAnchor1342"/><st c="9096">s to understand </st><st c="9113">its patterns:</st></p>
			<ol>
				<li><st c="9126">Let’s load </st><code><st c="9138">pandas</st></code> <st c="9144">and </st><code><st c="9149">matplotlib</st></code><st c="9159">:</st><pre class="source-code"><st c="9161">
import matplotlib.pyplot as plt
import pandas as pd</st></pre></li>				<li><st c="9213">Load the dataset and display the first </st><st c="9253">five rows:</st><pre class="source-code"><st c="9263">
X = pd.read_csv(
    "occupancy.csv", parse_dates=["date"])
X.head()</st></pre><p class="list-inset"><st c="9328">In the following figure, we can see the dataset containing a unique identifier, followed by the date and time of the measurements and values for five time series capturing temperature, humidity, lights, and </st><img src="img/39.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.996em;width:1.591em"/><st c="9536"/><st c="9540"> levels in </st><st c="9550">the offi</st><a id="_idTextAnchor1343"/><st c="9558">ce:</st></p></li>			</ol>
			<div><div><img src="img/B22396_10_02.jpg" alt="Figure 10.2 – DataFrame with the time-series data"/><st c="9562"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="9934">Figure 10.2 – DataFrame with the time-series data</st></p>
			<ol>
				<li value="3"><st c="9983">Let’s creat</st><a id="_idTextAnchor1344"/><st c="9995">e a </st><a id="_idIndexMarker756"/><st c="10000">function to</st><a id="_idIndexMarker757"/><st c="10011"> plot the time series from </st><em class="italic"><st c="10038">step 2</st></em><st c="10044"> at a given hour (the </st><code><st c="10066">id</st></code><st c="10068"> column is a unique identifier for each of the 135 hours </st><st c="10125">of records):</st><pre class="source-code"><st c="10137">
def plot_timeseries(n_id):
    fig, axes = plt.subplots(nrows=2, ncols=3,
    figsize=(20, 10))
    X[X[«id»] == n_id]["temperature"].plot(
        ax=axes[0, 0], title="temperature")
    X[X[«id»] == n_id]["humidity"].plot(
        ax=axes[0, 1], title="humidity")
    X[X[«id»] == n_id]["light"].plot(
        ax=axes[0, 2], title="light")
    X[X[«id»] == n_id]["co2"].plot(
    ax=axes[1, 0], title="co2")
    X[X[«id»] == n_id]["humidity_ratio"].plot(
        ax=axes[1,1], title="humidity_ratio")
    plt.show()</st></pre></li>				<li><st c="10587">Let’s</st><a id="_idTextAnchor1345"/><st c="10593"> plot the time series corresponding to an hour when the office was </st><st c="10660">not occupied:</st><pre class="source-code"><st c="10673">
plot_timeseries(2)</st></pre><p class="list-inset"><st c="10692">In the following figure, we can see the time-series values during the second hour of </st><a id="_idIndexMarker758"/><st c="10778">records, when the office</st><a id="_idIndexMarker759"/> <st c="10802">was </st><a id="_idTextAnchor1346"/><st c="10807">empty:</st></p></li>			</ol>
			<div><div><img src="img/B22396_10_03.jpg" alt="Figure 10.3 – Time-series values during the second hour of data collection when the office was empt﻿y"/><st c="10813"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="11177">Figure 10.3 – Time-series values during the second hour of data collection when the office was empt</st><a id="_idTextAnchor1347"/><st c="11276">y</st></p>
			<p class="list-inset"><st c="11278">Note that the lights were off, and that </st><a id="_idTextAnchor1348"/><st c="11318">is why we see the flat line at 0 in the plot of </st><code><st c="11366">light</st></code><st c="11371"> consumption in the </st><st c="11391">top-right corner.</st></p>
			<ol>
				<li value="5"><st c="11408">Now, let’s plot the time-series data corresponding to an hour when the office </st><st c="11487">was occupied:</st><pre class="source-code"><st c="11500">
plot_timeseries(15)</st></pre><p class="list-inset"><st c="11520">In the following figure, we can see the time-series values during the fifteenth hour of records, when the office </st><st c="11634">was occu</st><a id="_idTextAnchor1349"/><st c="11642">pied:</st></p></li>			</ol>
			<div><div><img src="img/B22396_10_04.jpg" alt="Figure 10.4 – Time-series values during the fifteenth hour of data collection, when the office was occupied"/><st c="11648"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="12022">Figure 10.4 – Time-series values during the fifteenth hour of data collection, when the office was occupied</st></p>
			<p><st c="12129">Notice that the lights were on this time (</st><st c="12172">top-right panel).</st></p>
			<p><st c="12190">In this recipe, we will extract features from each of these one-hour windows of time-series data, capturing</st><a id="_idIndexMarker760"/><st c="12298"> various aspects of their </st><a id="_idIndexMarker761"/><st c="12324">character</st><a id="_idTextAnchor1350"/><st c="12333">istics. </st><st c="12342">From each of these 60-minute time-series segments, we will automatically generate more than 750 features using </st><code><st c="12453">tsfresh</st></code><st c="12460">, ensuring a comprehensive representation of the </st><st c="12509">data’s prop</st><a id="_idTextAnchor1351"/><a id="_idTextAnchor1352"/><st c="12520">erties.</st></p>
			<h2 id="_idParaDest-280"><a id="_idTextAnchor1353"/><st c="12528">How to do it...</st></h2>
			<p><st c="12544">We will begin by automatically creating hundreds of features from one time series, </st><code><st c="12628">lights</st></code><st c="12634">, and then use those features to predict whether the office was occupied at any </st><st c="12714">given hour:</st></p>
			<ol>
				<li><st c="12725">Let’s import the required Python libraries </st><st c="12769">and functions:</st><pre class="source-code"><st c="12783">
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from tsfresh import extract_features
from tsfresh.utilities.dataframe_functions import (
    impute
)</st></pre></li>				<li><st c="13056">Load the dataset described in the </st><em class="italic"><st c="13091">Technical </st></em><em class="italic"><st c="13101">requirements</st></em><st c="13113"> section:</st><pre class="source-code"><st c="13122">
X = pd.read_csv("occupancy.csv", parse_dates=["date"])</st></pre></li>				<li><st c="13177">L</st><a id="_idTextAnchor1354"/><st c="13179">oad the target variable into a </st><code><st c="13210">pandas</st></code><st c="13216"> Series:</st><pre class="source-code"><st c="13224">
y = pd.read_csv("occupancy_target.csv",
    index_col="id")["occupancy"]</st></pre></li>				<li><st c="13293">Let’s create hundreds of features automatically for each hour of light records using </st><code><st c="13379">tsfresh</st></code><st c="13386">. To create features from the </st><code><st c="13416">light</st></code><st c="13421"> variable, we pass the DataFrame containing this variable and the unique identifier for each time series to the </st><code><st c="13533">extract_features</st></code><st c="13549"> function </st><st c="13559">from </st><code><st c="13564">tsfresh</st></code><st c="13571">:</st><pre class="source-code"><st c="13573">
features = extract_features(
    X[[«id», «light»]], column_id="id")</st></pre><p class="list-inset"><st c="13638">If we execute </st><code><st c="13653">features.shape</st></code><st c="13667">, we’ll obtain </st><code><st c="13682">(135, 789)</st></code><st c="13692"> corresponding to the size </st><a id="_idIndexMarker762"/><st c="13719">of </st><a id="_idIndexMarker763"/><st c="13722">the resulting DataFrame, where each row represents an hour of records and each column one of the features created by </st><code><st c="13839">tsfresh</st></code><st c="13846">. There are 789 features that characterize light consumption at any given hour. </st><st c="13926">Go ahead and execute </st><code><st c="13947">features.head()</st></code><st c="13962"> to get a view of the resulting DataFrame. </st><st c="14005">For space reasons, we can’t display the entire DataFrame in the book. </st><st c="14075">So, instead, we will explore some of </st><st c="14112">the features.</st></p></li>				<li><st c="14125">Let’s capture the names of five of the created features in </st><st c="14185">an array:</st><pre class="source-code"><st c="14194">
feats = features.columns[10:15]</st></pre><p class="list-inset"><st c="14226">If we execute </st><code><st c="14241">feats</st></code><st c="14246">, we’ll see the names of five features corresponding to the mean, length, standard deviation, coefficient of variation, and variance of the light consumption </st><st c="14404">per hour:</st></p><pre class="source-code"><strong class="bold"><st c="14413">Index(['light__mean', 'light__length', </st></strong><strong class="bold"><st c="14453">'light__standard_deviation', </st></strong><strong class="bold"><st c="14482">'light__variation_coefficient', </st></strong><strong class="bold"><st c="14514">'light__variance'], dtype='object')</st></strong></pre></li>				<li><a id="_idTextAnchor1355"/><st c="14549">Now, let’s display the values of the features from </st><em class="italic"><st c="14601">step 5</st></em><st c="14607"> for the first </st><st c="14622">five hours:</st><pre class="source-code"><st c="14633">
features[feats].head()</st></pre><p class="list-inset"><st c="14656">In the following DataFrame, we see the features extracted from the time series for the first</st><a id="_idIndexMarker764"/><st c="14749"> five hours of </st><st c="14764">light </st><a id="_idIndexMarker765"/><st c="14770">co</st><a id="_idTextAnchor1356"/><st c="14772">nsumption:</st></p></li>			</ol>
			<div><div><img src="img/B22396_10_05.jpg" alt="Figure 10.5 – Features created for each hour of light consumption"/><st c="14783"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="15066">Figure 10.5 – Features created for each hour of light consumption</st></p>
			<p class="list-inset"><st c="15131">Looking at the mean value of light consumption in </st><em class="italic"><st c="15182">Figure 10</st></em><em class="italic"><st c="15191">.4</st></em><st c="15193">, we can see that the lights were on during the first hour, and then </st><a id="_idTextAnchor1357"/><st c="15262">off in the following four hours. </st><st c="15295">The length of the time series is 60 because we have 60 minutes of records </st><st c="15369">per hour.</st></p>
			<p class="callout-heading"><st c="15378">Note</st></p>
			<p class="callout"><code><st c="15383">tsfresh</st></code><st c="15391"> applies 63 feature creation methods to a time series. </st><st c="15446">Based on the characteristics of the time series, such as its length or its variability, some of the methods will return missing values or infinite values. </st><st c="15601">For example, in </st><em class="italic"><st c="15617">Figure 10</st></em><em class="italic"><st c="15626">.4</st></em><st c="15628">, w</st><a id="_idTextAnchor1358"/><st c="15631">e see that the variation coefficient could not be calculated for those hours where the light consumption is constant. </st><st c="15750">And the variance is also </st><code><st c="15775">0</st></code><st c="15776"> in those cases. </st><st c="15793">In fact, for our dataset, many of the resulting features contain only </st><code><st c="15863">NaN</st></code><st c="15866"> values, or are constant, like the length, and are therefore not useful for training machine </st><st c="15959">learning models.</st></p>
			<ol>
				<li value="7"><code><st c="15975">tsfresh</st></code><st c="15983"> includes an imputation function to impute features that contain </st><code><st c="16048">NaN</st></code><st c="16051"> values. </st><st c="16060">Let’s go ahead and impute </st><st c="16086">our features:</st><pre class="source-code"><st c="16099">
impute(features)</st></pre><p class="list-inset"><st c="16116">The </st><code><st c="16121">impute</st></code><st c="16127"> function from </st><code><st c="16142">tsfresh</st></code><st c="16149"> replaces </st><code><st c="16159">NaN</st></code><st c="16162">, </st><code><st c="16164">-Inf</st></code><st c="16168">, and </st><code><st c="16174">Inf</st></code><st c="16177"> values with the variable’s median, minimum, or maximum </st><st c="16233">values, respectively.</st></p><p class="list-inset"><st c="16254">Let’s use these features to train a logistic regression model and predict whether the office </st><st c="16348">was occupied.</st></p></li>				<li><st c="16361">Let’s begin</st><a id="_idIndexMarker766"/><st c="16373"> by separating the</st><a id="_idIndexMarker767"/><st c="16391"> dataset into training and </st><st c="16418">test sets:</st><pre class="source-code"><st c="16428">
X_train, X_test, y_train, y_test = train_test_split(
    features,
    y,
    test_size=0.1,
    random_state=42,
)</st></pre></li>				<li><st c="16528">Now, let’s set up and train a logistic regression model, and then evaluate </st><st c="16604">its performance:</st><pre class="source-code"><st c="16620">
cls = LogisticRegression(random_state=10, C=0.01)
cls.fit(X_train, y_train)
print(classification_report(
     y_test, cls.predict(X_test)))</st></pre><p class="list-inset"><st c="16755">In t</st><a id="_idTextAnchor1359"/><st c="16760">he following output, we see the values of evaluation metrics that are commonly used for classification analysis, which suggests that the created features ar</st><a id="_idTextAnchor1360"/><st c="16917">e useful for predicting </st><st c="16942">office occupancy:</st></p><pre class="source-code"><strong class="bold"><st c="16959">                  precision     recall</st></strong><strong class="bold"><st c="16976">  f1-score   support</st></strong>
<strong class="bold"><st c="16993">               0         1.00        1.00        1.00           11</st></strong>
<strong class="bold"><st c="17013">               1         1.00        </st></strong><strong class="bold"><st c="17021">1.00        1.00            3</st></strong>
<strong class="bold"><st c="17032">     accuracy                                       1.00           14</st></strong>
<strong class="bold"><st c="17049">   macro avg         1.00        1.00        1.00</st></strong><strong class="bold"><st c="17074">           14</st></strong>
<strong class="bold"><st c="17077">weighted avg         1.00        1.00        1.00           14</st></strong></pre></li>			</ol>
			<p class="callout-heading"><st c="17108">Note</st></p>
			<p class="callout"><st c="17113">To keep the recipe simple, I have not optimized the model hyperparameters or tuned the probability threshold – things that we normally do to ensure our models </st><st c="17273">are accurate.</st></p>
			<ol>
				<li value="10"><st c="17286">To finish </st><a id="_idIndexMarker768"/><st c="17297">off, let’s </st><a id="_idIndexMarker769"/><st c="17308">extract features from every time series, that is, </st><code><st c="17358">light</st></code><st c="17363">, </st><code><st c="17365">temperature</st></code><st c="17376">, </st><code><st c="17378">humidity</st></code><st c="17386">, and </st><code><st c="17392">co2</st></code><st c="17395">, and this time, we will impute the features right after </st><st c="17452">the extraction:</st><pre class="source-code"><st c="17467">
features = extract_features(
    X,
    column_id="id",
    impute_function=impute,
    column_sort="date",
)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="17561">Note</st></p>
			<p class="callout"><st c="17566">In </st><em class="italic"><st c="17570">step 10</st></em><st c="17577">, we indicated that we want to sort our time series based on the timestamp containing the time and date of the measurement, by passing the </st><code><st c="17716">date</st></code><st c="17720"> variable to the </st><code><st c="17737">column_sort</st></code><st c="17748"> parameter. </st><st c="17760">This is useful when our time series are not equidistant or not ordered chronologically. </st><st c="17848">If we leave this parameter set to </st><code><st c="17882">None</st></code><st c="17886">, </st><code><st c="17888">tsfresh</st></code><st c="17895"> assumes that the time series are ordered </st><st c="17937">and eq</st><a id="_idTextAnchor1361"/><st c="17943">uidistant.</st></p>
			<p><st c="17954">The output of </st><em class="italic"><st c="17969">step 10</st></em><st c="17976"> consists of a DataFrame with 135 rows, containing 3,945 features (execute </st><code><st c="18051">features.shape</st></code><st c="18065"> to check that out) that characterize the five original time series – temperature, light, humidity and its ratio, and </st><img src="img/47.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.996em;width:1.583em"/><st c="18183"/><st c="18187"> in the office. </st><st c="18202">These</st><a id="_idTextAnchor1362"/><st c="18207"> features were imputed in </st><em class="italic"><st c="18233">step 10</st></em><st c="18240">, so you can go ahead and use this DataFrame to train another logistic </st><a id="_idIndexMarker770"/><st c="18311">regression</st><a id="_idIndexMarker771"/><st c="18321"> model to predict </st><st c="18339">o</st><a id="_idTextAnchor1363"/><a id="_idTextAnchor1364"/><st c="18340">ffice occupancy.</st></p>
			<h2 id="_idParaDest-281"><a id="_idTextAnchor1365"/><st c="18356">How it works...</st></h2>
			<p><st c="18372">In this recipe, we used </st><code><st c="18397">tsfresh</st></code><st c="18404"> to automatically create hundreds of features from five time series, and then used those features to train a logistic regression model to predict whether the office </st><st c="18569">was occupied.</st></p>
			<p class="callout-heading"><st c="18582">Note</st></p>
			<p class="callout"><st c="18587">To create features with </st><code><st c="18612">tsfresh</st></code><st c="18619">, the time-series interval from which we want to extract features must be marked </st><a id="_idIndexMarker772"/><st c="18700">with a </st><code><st c="18755">id</st></code><st c="18757"> variable.</st></p>
			<p><st c="18767">To create features from time series, we used the </st><code><st c="18817">extract_features</st></code><st c="18833"> function from </st><code><st c="18848">tsfresh</st></code><st c="18855">. This function takes the DataFrame containing the time series and the unique identifier as input and returns a DataFrame containing the extracted features </st><st c="19011">as output.</st></p>
			<p><code><st c="19021">extract_features</st></code><st c="19038"> has three key parameters: </st><code><st c="19065">column_id</st></code><st c="19074">, </st><code><st c="19076">column_sort</st></code><st c="19087">, and </st><code><st c="19093">impute_function</st></code><st c="19108">. </st><code><st c="19110">column_id</st></code><st c="19119"> receives the name of the column with the unique identifier for each sequence that’ll be used to extract features. </st><code><st c="19234">column_sort</st></code><st c="19245"> is used to reorder the time series before extracting features. </st><st c="19309">When </st><code><st c="19314">column_sort</st></code><st c="19325"> is left to </st><code><st c="19337">None</st></code><st c="19341">, </st><code><st c="19343">tsfresh</st></code><st c="19350"> assumes that the data is ordered chronologically and that the timestamps are equidistant. </st><st c="19441">In </st><em class="italic"><st c="19444">step 10</st></em><st c="19451">, we passed the </st><code><st c="19467">date</st></code><st c="19471"> variable as the sorting variable, which informs </st><code><st c="19520">tsfresh</st></code><st c="19527"> how to sort the data before extracting </st><st c="19567">the features.</st></p>
			<p class="callout-heading"><st c="19580">Note</st></p>
			<p class="callout"><st c="19585">In our dataset, leaving </st><code><st c="19610">column_sort</st></code><st c="19621"> set to </st><code><st c="19629">None</st></code><st c="19633"> or passing the </st><code><st c="19649">date</st></code><st c="19653"> variable made no difference, because our time series were already ordered chronologically and the timestamps were equidistant. </st><st c="19781">If this is not the case in your time series, use this parameter to create </st><st c="19855">features correctly.</st></p>
			<p><st c="19874">Finally, </st><code><st c="19884">extract_features</st></code><st c="19900"> also accepts the </st><code><st c="19918">impute</st></code><st c="19924"> function through the </st><code><st c="19946">impute_function</st></code><st c="19961"> parameter, to</st><a id="_idIndexMarker773"/><st c="19975"> automatically </st><a id="_idIndexMarker774"/><st c="19990">remove infinite and </st><code><st c="20010">NaN</st></code><st c="20013"> values from the created features. </st><st c="20048">Will discuss additional parameters of </st><code><st c="20086">extract_features</st></code><st c="20102"> in the </st><st c="20110">coming recipes.</st></p>
			<p class="callout-heading"><st c="20125">Note</st></p>
			<p class="callout"><st c="20130">For more details </st><a id="_idIndexMarker775"/><st c="20148">about the </st><code><st c="20158">extract_features</st></code><st c="20174"> function, </st><st c="20185">visit </st><a href="https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html#module-tsfresh.feature_extraction.extraction"><st c="20191">https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html#module-tsfresh.feature_extraction.extraction</st></a><st c="20312">.</st></p>
			<p><st c="20313">The </st><code><st c="20318">im</st><a id="_idTextAnchor1366"/><st c="20320">pute</st></code><st c="20325"> function, which can be used independently, as we did in </st><em class="italic"><st c="20382">step 7</st></em><st c="20388">, or within the </st><code><st c="20404">extract_features</st></code><st c="20420"> function, as we did in </st><em class="italic"><st c="20444">step 10</st></em><st c="20451">, replaced </st><code><st c="20462">NAN</st></code><st c="20465">, </st><code><st c="20467">-Inf</st></code><st c="20471">, and </st><code><st c="20477">Inf</st></code><st c="20480"> values</st><a id="_idTextAnchor1367"/><st c="20487"> with the variable’s median, minimum, or maximum values, respectively. </st><st c="20558">If the feature contains only </st><code><st c="20587">NaN</st></code><st c="20590"> values, they are replaced by zeroes. </st><st c="20628">The imputation occurs in place – that is, in the same DataFrame that is </st><st c="20700">being imputed.</st></p>
			<p><st c="20714">The </st><code><st c="20719">extract_features</st></code><st c="20735"> function returns a DataFrame containing as many rows as unique identifiers in the data. </st><st c="20824">In our case, it returned a DataFrame with 135 rows. </st><st c="20876">The columns of the resulting DataFrame correspond to the 789 values that were returned by 63 characterization methods applied to each of the 135 60-minute </st><st c="21031">time series.</st></p>
			<p><st c="21043">In </st><em class="italic"><st c="21047">step 5</st></em><st c="21053">, we explored some of the resulting features, which captured the time series mean, variance, and coefficient of variation, as well as their length. </st><st c="21201">Let’s explore a few more of the </st><st c="21233">resulting features.</st></p>
			<p><st c="21252">Some of the created variables are self-explanatory. </st><st c="21305">For example, the </st><code><st c="21322">'light__skewness'</st></code><st c="21339"> and </st><code><st c="21344">'light__kurtosis'</st></code><st c="21361"> variables contain the skewness and kurtosis coefficients, which characterize the data distribution. </st><st c="21462">The </st><code><st c="21466">'light__has_duplicate_max'</st></code><st c="21492">, </st><code><st c="21494">'light__has_duplicate_min'</st></code><st c="21520">, and </st><code><st c="21526">'light__has_duplicate'</st></code><st c="21548"> variables indicate whether the time series has duplicated values or duplicated minimum or maximum values within the time interval. </st><st c="21680">The </st><code><st c="21684">'light__quantile__q_0.1'</st></code><st c="21708">, </st><code><st c="21710">'light__quantile__q_0.2'</st></code><st c="21734">, and </st><code><st c="21740">'light__quantile__q_0.3'</st></code><st c="21764"> variables display the different quantile values of the time series. </st><st c="21833">Finally, the </st><code><st c="21846">'light__autocorrelation__lag_0'</st></code><st c="21877">, </st><code><st c="21879">'light__autocorrelation__lag_1'</st></code><st c="21910">, and </st><code><st c="21916">'light__autocorrelation__lag_2'</st></code><st c="21947"> variables show the autocorrelation of the time series with its past values, lagged by 0, 1, or 2 steps – information that is generally useful </st><st c="22090">i</st><a id="_idTextAnchor1368"/><st c="22091">n forecasting.</st></p>
			<p><st c="22105">Other characterization methods return fe</st><a id="_idTextAnchor1369"/><st c="22146">atures obtained from signal processing</st><a id="_idTextAnchor1370"/><st c="22185"> algorithms, such </st><a id="_idIndexMarker776"/><st c="22203">as </st><a id="_idIndexMarker777"/><st c="22206">the continuous wavelet transform for the Ricker wavelet, which returns the </st><code><st c="22281">'light__cwt_coefficients__coeff_0__w_2__widths_(2, 5, 10, 20)'</st></code><st c="22343">, </st><code><st c="22345">'light__cwt_coefficients__coeff_0__w_5__widths_(2, 5, 10, 20)'</st></code><st c="22407">, </st><code><st c="22409">'light__cwt_coefficients__coeff_0__w_10__widths_(2, 5, 10, 20)'</st></code><st c="22472">, and </st><code><st c="22478">'light__cwt_coefficients__coeff_0__w_20__widths_(2, 5, 10, 20)'</st></code><st c="22541"> features, </st><st c="22552">among others.</st></p>
			<p class="callout-heading"><st c="22565">Note</st></p>
			<p class="callout"><st c="22570">We can’t discuss each of these feature characterization methods or their outputs in detail in this book because there are too many. </st><st c="22703">You can find more details about the transformations </st><a id="_idIndexMarker778"/><st c="22755">supported by </st><code><st c="22768">tsfresh</st></code><st c="22775"> and their formulation </st><st c="22798">at </st><a href="https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html"><st c="22801">https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html</st></a><st c="22877">.</st></p>
			<p><st c="22878">Some of the features that are automatically created by </st><code><st c="22934">tsfresh</st></code><st c="22941"> may not make sense or even be possible to calculate for some time series because they require a certain length or data variability, or the time series must meet certain distribution assumptions. </st><st c="23137">Therefore, the suitability of the features will depend on the nature of the </st><st c="23213">time series.</st></p>
			<p class="callout-heading"><st c="23225">Note</st></p>
			<p class="callout"><st c="23230">You can decide which features to extract from your time series based on domain knowledge, or by creating all possible features and then applying feature selection algorithms or following up with data analysis. </st><st c="23441">In fact, from our dataset, many of the resulting features were either constant or contained only missing data. </st><st c="23552">Hence, we can reduce the feature space to informative features by taking those fe</st><a id="_idTextAnchor1371"/><a id="_idTextAnchor1372"/><st c="23633">atures out of </st><st c="23648">the data.</st></p>
			<h2 id="_idParaDest-282"><a id="_idTextAnchor1373"/><st c="23657">See also</st></h2>
			<p><st c="23666">For more</st><a id="_idIndexMarker779"/><st c="23675"> details about </st><code><st c="23690">tsfresh</st></code><st c="23697">, check </st><a id="_idIndexMarker780"/><st c="23705">out the article Christ M., Braun N., , Neuffer J., and Kempa-Liehr A., (2018). </st><em class="italic"><st c="23784">Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests (tsfresh – A Python package). </st><st c="23883">Neurocomputing 307 (2018). </st><st c="23910">Pages </st></em><em class="italic"><st c="23916">72-77.</st></em> <a href="https://dl.acm.org/doi/10.1016/j.neucom.2018.03.067"><st c="23922">https://dl.acm.org/doi/10.1</st><st c="23950">016/j.neucom.2018.03.067</st></a><st c="23975">.</st></p>
			<h1 id="_idParaDest-283"><a id="_idTextAnchor1376"/><st c="23976">Automatically creating and selecting predictive features fr</st><a id="_idTextAnchor1377"/><st c="24036">om time-series data</st></h1>
			<p><st c="24056">In the previous </st><a id="_idIndexMarker781"/><st c="24073">recipe, we </st><a id="_idIndexMarker782"/><st c="24084">automatically extracted several hundred features from time-series variables using </st><code><st c="24166">tsfresh</st></code><st c="24173">. If we have more than one time-series variable, we can easily end up with a dataset containing thousands of features. </st><st c="24292">In addition, many of the resulting features had only missing data or were constant and were therefore not useful for training mach</st><a id="_idTextAnchor1378"/><st c="24422">ine </st><st c="24427">learning models.</st></p>
			<p><st c="24443">When we create classiﬁcation and regression models to solve real-life problems, we often want our models to take a small number of relevant features as input to produce interpretable machine learning outputs. </st><st c="24653">Simpler models have many advantages. </st><st c="24690">First, their output is easier to interpret. </st><st c="24734">Second, simpler models are cheaper to store and faster to train. </st><st c="24799">They also return th</st><a id="_idTextAnchor1379"/><st c="24818">eir </st><st c="24823">outputs faster.</st></p>
			<p><code><st c="24838">tsfresh</st></code><st c="24846"> includes a highly parallelizable feature selection algorithm based on non-parametric statistical hypothesis tests, which can be executed at the back of the feature creation procedure to quickly remove irrelevant features. </st><st c="25069">The feature selection procedure utilizes different tests for</st><a id="_idTextAnchor1380"/> <st c="25129">different features.</st></p>
			<p><code><st c="25149">tsfresh</st></code><st c="25157"> uses the following tests to </st><st c="25186">select features:</st></p>
			<ul>
				<li><st c="25202">Fisher’s exact test of independence, if both the feature and the target </st><st c="25275">are binary</st></li>
				<li><st c="25285">Kolmogorov-Smirnov test, if either the feature or the target </st><st c="25347">is binary</st></li>
				<li><st c="25356">Kendall rank test, if neither the feature nor the target </st><st c="25414">is binary</st></li>
			</ul>
			<p><st c="25423">The advantage of these tests is that they are non-parametric, and thus make no assumptions on the underlying distribution of the variables </st><a id="_idTextAnchor1381"/><st c="25563">being tested.</st></p>
			<p><st c="25576">The result of these tests is a vector of p-values that measures the significance of the association between each feature and the target. </st><st c="25714">These p-values are then evaluated based on</st><a id="_idIndexMarker783"/><st c="25756"> the </st><a id="_idIndexMarker784"/><st c="25761">Benjamini-Yekutieli procedure to decide which features </st><st c="25816">to keep.</st></p>
			<p class="callout-heading"><st c="25824">Note</st></p>
			<p class="callout"><st c="25829">For more details abo</st><a id="_idTextAnchor1382"/><st c="25850">ut </st><code><st c="25854">tsfresh</st></code><st c="25861">’s featu</st><a id="_idTextAnchor1383"/><st c="25870">re selection procedure, check out the article Christ, Kempa-Liehr, and Feindt, </st><em class="italic"><st c="25950">Distributed and parallel time series feature extraction for industrial big data applications</st></em><st c="26042">. Asian Machine Learning Conference (ACML) 2016, Workshop on Learning on Big Data (WLBD), Hamilton (New Zealand), </st><st c="26156">arXiv, </st><a href="https://arxiv.org/abs/1610.07717v1"><st c="26163">https://arxiv.org/abs/1610.07717v1</st></a><st c="26197">.</st></p>
			<p><st c="26198">In this recipe, we will automatically create hundreds of features from various time series, and then select the most relevant f</st><a id="_idTextAnchor1384"/><a id="_idTextAnchor1385"/><st c="26326">eatures by </st><st c="26338">utilizing </st><code><st c="26348">tsfresh</st></code><st c="26355">.</st></p>
			<h2 id="_idParaDest-284"><st c="26356">How</st><a id="_idTextAnchor1386"/><st c="26360"> to do it...</st></h2>
			<p><st c="26372">We will begin by automatically creating and selecting features from one time series, </st><code><st c="26458">lights</st></code><st c="26464">, and then we will automate the procedure for multiple </st><st c="26519">time series:</st></p>
			<ol>
				<li><st c="26531">Let’s import the required Python libraries </st><st c="26575">and functions:</st><pre class="source-code"><st c="26589">
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from tsfresh import (
    extract_features,
    extract_relevant_features,
    select_features,
)
from tsfresh.utilities.dataframe_functions import impute</st></pre></li>				<li><st c="26907">Load the</st><a id="_idIndexMarker785"/><st c="26916"> dataset </st><a id="_idIndexMarker786"/><st c="26925">and the target variable described in the </st><em class="italic"><st c="26966">Technical </st></em><em class="italic"><st c="26976">requirements</st></em><st c="26988"> section:</st><pre class="source-code"><st c="26997">
X = pd.read_csv("occupancy.csv", parse_dates=["date"])
y = pd.read_csv("occupancy_target.csv",
    index_col</st><a id="_idTextAnchor1387"/><st c="27102">="id")["occupancy"]</st></pre></li>				<li><st c="27122">Let’s create hundreds of features automatically for each hour of </st><code><st c="27188">light</st></code><st c="27193"> use records and impute the </st><st c="27221">resulting features:</st><pre class="source-code"><st c="27240">
features = extract_features(
    X[[«id», «light»]],
    column_id="id",
    impute_function=impute,
)</st></pre><p class="list-inset"><st c="27331">The output of the previous step is a DataFrame with 135 rows and 789 columns, corresponding to the features created from each hour of </st><st c="27466">light consumption.</st></p></li>			</ol>
			<p class="callout-heading"><st c="27484">Note</st></p>
			<p class="callout"><st c="27489">For more details about </st><em class="italic"><st c="27513">step 3</st></em><st c="27519">, or the Occupancy Detection dataset, check out the </st><em class="italic"><st c="27571">Extracting hundreds of features automatically from a time </st></em><em class="italic"><st c="27629">seri</st><a id="_idTextAnchor1388"/><st c="27633">es</st></em><st c="27636"> recipe.</st></p>
			<ol>
				<li value="4"><st c="27644">Now, let’s select the features based on the non-parametric tests that we mentioned in the introduction of </st><st c="27751">this recipe:</st><pre class="source-code"><st c="27763">
features = select_features(features, y)</st></pre><p class="list-inset"><st c="27803">If we execute </st><code><st c="27818">len(features)</st></code><st c="27831">, we’ll see the value </st><code><st c="27853">135</st></code><st c="27856">, which means that from the 789 features created in </st><em class="italic"><st c="27908">step 3</st></em><st c="27914">, only 135 are statistically significant. </st><st c="27956">Go ahead and execute </st><code><st c="27977">features.head()</st></code><st c="27992"> to display the first five rows of the </st><st c="28031">resulting DataFrame.</st></p></li>				<li><st c="28051">For space reasons, we will only display the first </st><st c="28102">five features:</st><pre class="source-code"><st c="28116">
feats = features.columns[0:5]
features[feats].head()</st></pre><p class="list-inset"><st c="28169">In</st><a id="_idTextAnchor1389"/> <a id="_idIndexMarker787"/><st c="28172">the</st><a id="_idIndexMarker788"/><st c="28176"> following DataFrame, we see the values of the first five features for the first </st><a id="_idTextAnchor1390"/><st c="28257">five hours of </st><st c="28271">light consumption:</st></p></li>			</ol>
			<div><div><img src="img/B22396_10_06.jpg" alt="Figure 10.6 – DataFrame with five of the selected features created from each hour of light consumption"/><st c="28289"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="28549">Figure 10.6 – DataFrame with five of the selected features created from each hour of light consumption</st></p>
			<p class="list-inset"><st c="28651">Check the discussion in the </st><em class="italic"><st c="28680">How it works…</st></em><st c="28693"> section for a more detailed analysis of the DataFrame resulting from </st><em class="italic"><st c="28763">step 4</st></em><st c="28769">.</st></p>
			<ol>
				<li value="6"><st c="28770">Now, we will use the features from </st><em class="italic"><st c="28806">step 4</st></em><st c="28812"> to train a logistic regression model and predict whether the office was occupied. </st><st c="28895">Let’s begin by separating the dataset into training and </st><st c="28951">test sets:</st><pre class="source-code"><st c="28961">
X_train, X_test, y_train, y_test = train_test_split(
    features,
    y,
    test_size=0.1,
    random_state=42,
)</st></pre></li>				<li><st c="29061">Let’s set </st><a id="_idIndexMarker789"/><st c="29072">up</st><a id="_idIndexMarker790"/><st c="29074"> and train a logistic regression model and then evaluate </st><st c="29131">its performance:</st><pre class="source-code"><st c="29147">
cls = LogisticRegression(
    random_state=10, C=0.1, max_iter=1000)
cls.fit(X_train, y_train)
print(classification_report(
    y</st><a id="_idTextAnchor1391"/><st c="29269">_test, cls.predict(X_test)))</st></pre><p class="list-inset"><st c="29298">In the following output, we see the values of commonly used evaluation metrics for classification analysis. </st><st c="29407">These suggest that the selected features are useful for predicting </st><st c="29474">office occupancy:</st></p><pre class="source-code"><strong class="bold"><st c="29491">                  precision     recall  f1-score   support</st></strong>
<strong class="bold"><st c="29525">               0</st></strong><strong class="bold"><st c="29527">         1.00        0.91        0.95           11</st></strong>
<strong class="bold"><st c="29545">               1         0.75        1.00        0.86            3</st></strong>
<strong class="bold"/><strong class="bold"><st c="29564">accuracy                                       0.93           14</st></strong>
<strong class="bold"><st c="29581">   macro avg         0.88        0.95        0.90           14</st></strong>
<code><st c="29980">extract_relevant_features</st></code><st c="30005">, and, like this, combine </st><em class="italic"><st c="30031">steps 3</st></em><st c="30038"> and </st><em class="italic"><st c="30043">4</st></em><st c="30044">. We’ll do that to create and select features automatically for the five time series in </st><st c="30132">our dataset:</st><pre class="source-code"><st c="30144">
features = extract_relevant_features(
    X,
    y,
    column_id="id",
    column_sort="date",
)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="30226">Note</st></p>
			<p class="callout"><st c="30231">The parameters of </st><code><st c="30250">extract_relevant_features</st></code><st c="30275"> are very similar to those of </st><code><st c="30305">extract_features</st></code><st c="30321">. Note, however, that the former will automatically perform imputation to be able to proceed with the feature selection. </st><st c="30442">We discussed the parameters of </st><code><st c="30473">extract_features</st></code><st c="30489"> in the </st><em class="italic"><st c="30497">Extracting hundreds of features automatically from time </st></em><em class="italic"><st c="30553">series</st></em><st c="30559"> recipe.</st></p>
			<p><st c="30567">The output of </st><em class="italic"><st c="30582">step 8</st></em><st c="30588"> consists of a DataFrame with 135 rows and 968 features, from the original 3,945 that are returned by default by </st><code><st c="30701">tsfresh</st></code><st c="30708"> (you can check that out by executing </st><code><st c="30746">features.shape</st></code><st c="30760">). </st><st c="30764">Go ahead and use this DataFrame to train another logistic regressio</st><a id="_idTextAnchor1393"/><a id="_idTextAnchor1394"/><st c="30831">n model to predict </st><st c="30851">office occupancy</st><a id="_idTextAnchor1395"/><st c="30867">.</st></p>
			<h2 id="_idParaDest-285"><a id="_idTextAnchor1396"/><st c="30868">How it works...</st></h2>
			<p><st c="30884">In this</st><a id="_idIndexMarker793"/><st c="30892"> recipe, we </st><a id="_idIndexMarker794"/><st c="30904">created hundreds of features from a time series and then selected the most relevant features based on non-parametric statistical tests. </st><st c="31040">The feature creation and selection procedures were carried out automatically </st><st c="31117">by </st><code><st c="31120">tsfresh</st></code><st c="31127">.</st></p>
			<p><st c="31128">To create the features, we used </st><code><st c="31161">tsfresh</st></code><st c="31168">’s </st><code><st c="31172">extract_features</st></code><st c="31188"> function, which we described in detail in the </st><em class="italic"><st c="31235">Extracting hundreds of features automatical</st><a id="_idTextAnchor1397"/><st c="31278">ly from a time </st></em><em class="italic"><st c="31294">series</st></em><st c="31300"> recipe.</st></p>
			<p><st c="31308">To select features, we used the </st><code><st c="31341">select_features</st></code><st c="31356"> function, also from </st><code><st c="31377">tsfresh</st></code><st c="31384">. This function applies different statistical tests, depending on the nature of the feature and the target. </st><st c="31492">Briefly, if the feature and target are binary, it tests their relationship with Fisher’s exact test. </st><st c="31593">If either the feature or the target is binary, and the other variable is continuous, it tests their relationship by using the Kolmogorov-Smirnov test. </st><st c="31744">If neither the features nor the target is binary, it uses the Kendall </st><st c="31814">rank test.</st></p>
			<p><st c="31824">The result of these tests is a vector with one p-value per feature. </st><st c="31893">Next, </st><code><st c="31899">tsfresh</st></code><st c="31906"> applies the Benjamini-Yekutieli procedure, which aims to reduce the false discovery rate, to select which features to keep based on the p-values. </st><st c="32053">This feature selection procedure has some advantages, the main one being that statistical tests are fast to compute, and therefore the selection algorithm is scalable and can be parallelized. </st><st c="32245">Another advantage is that the tests are non-parametric and hence suitable for linear and </st><st c="32334">non-linear models.</st></p>
			<p><st c="32352">However, feature selection methods that evaluate each feature individually are unable to remove redundant features. </st><st c="32469">In fact, many of the features automatically created by </st><code><st c="32524">tsfresh</st></code><st c="32531"> will be highly correlated, like those capturing the different quantiles of light consumption. </st><st c="32626">Hence, they will show similar p-values and be retained. </st><st c="32682">But in practice, we only need one or a few of them to capture the information of the time series. </st><st c="32780">I’d recommend following up the </st><code><st c="32811">tsfresh</st></code><st c="32818"> selection procedure with alternative feature selection methods that are able to pick up </st><st c="32907">feature interactions.</st></p>
			<p><st c="32928">Finally, in </st><em class="italic"><st c="32941">step 8</st></em><st c="32947">, we combined the feature creation step (</st><em class="italic"><st c="32988">step 3</st></em><st c="32995">) with the feature selection step (</st><em class="italic"><st c="33031">step 4</st></em><st c="33038">) by using the </st><code><st c="33054">extract_relevant_features</st></code><st c="33079"> function. </st><code><st c="33090">extract_relevant_features</st></code><st c="33115"> applies the </st><code><st c="33128">extract_features</st></code><st c="33144"> function to create the features from each time series and imputes them. </st><st c="33217">Next, it applies the </st><code><st c="33238">select_features</st></code><st c="33253"> function to return a DataFrame containing one row per unique identifier, and the features that were selected for each time series. </st><st c="33385">Note that different features can</st><a id="_idIndexMarker795"/><a id="_idTextAnchor1398"/><a id="_idTextAnchor1399"/><st c="33417"> be </st><a id="_idIndexMarker796"/><st c="33421">selected for different </st><st c="33444">time series.</st></p>
			<h2 id="_idParaDest-286"><a id="_idTextAnchor1400"/><st c="33456">See also</st></h2>
			<p><st c="33465">The selection algorithm from </st><code><st c="33495">tsfresh</st></code><st c="33502"> offers a quick method to remove irrelevant features. </st><st c="33556">However, it does not find the best feature subset for the classification or regression task. </st><st c="33649">Other feature selection methods can be applied at the back of </st><code><st c="33711">tsfresh</st></code><st c="33718">’s algorithm to reduce the feature </st><st c="33754">space further.</st></p>
			<p><st c="33768">For more details on feature selection algorithms, check out the book </st><em class="italic"><st c="33838">Feature Selection in Machine Learning with Python</st></em><st c="33887"> by Soledad Galli on </st><st c="33908">Leanpub: </st><a href="https://leanpub.com/feature-selection-in-machine-learning/"><st c="33917">https://leanpub.com</st><st c="33936">/feature-selection-in-machine-learning/</st></a><st c="33976">.</st></p>
			<h1 id="_idParaDest-287"><st c="33977">Extracting diffe</st><a id="_idTextAnchor1403"/><st c="33994">rent features from different time series</st></h1>
			<p><code><st c="34035">tsfresh</st></code><st c="34043"> extracts </st><a id="_idIndexMarker797"/><st c="34053">many features based on the time-series characteristics and distribution, such as their correlation properties, stationarity, and entropy. </st><st c="34191">It also applies non-linear time-series analysis functions, which decompose the time-series signal through, for example, Fourier or wavelet transformations. </st><st c="34347">Depending on the nature of the time series, some of these transformations make more sense than others. </st><st c="34450">For example, wavelength decomposition methods can make sense for time series resulting from signals or sensors but are not always useful for time series representing sales or </st><st c="34625">stock prices.</st></p>
			<p><st c="34638">In this recipe, we will discuss how to optimize the feature extracti</st><a id="_idTextAnchor1404"/><st c="34707">on procedure to extract specific features from each time series, and then use th</st><a id="_idTextAnchor1405"/><a id="_idTextAnchor1406"/><st c="34788">ese features to predict </st><st c="34813">office occupancy.</st></p>
			<h2 id="_idParaDest-288"><a id="_idTextAnchor1407"/><st c="34830">How to do it...</st></h2>
			<p><code><st c="34846">tsfresh</st></code><st c="34854"> accesses the methods that will be used to create features through a dictionary that contains the method names as keys and, if they need a parameter, it has the parameter as a value. </st><code><st c="35037">tsfresh</st></code><st c="35044"> includes some predefined dictionaries as well. </st><st c="35092">We’ll explore these predefined dictionaries first, which can be accessed through the </st><code><st c="35177">settings</st></code><st c="35185"> module:</st></p>
			<ol>
				<li><st c="35193">Let’s import the required Python libraries and functions and the </st><code><st c="35259">settings</st></code><st c="35267"> module:</st><pre class="source-code"><st c="35275">
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from tsfresh.feature_extraction import (
    extract_features
)
from tsfre</st><a id="_idTextAnchor1408"/><st c="35521">sh.feature_extraction import settings</st></pre></li>				<li><st c="35559">Load the dataset and the target variable described in the </st><em class="italic"><st c="35618">Technical </st></em><em class="italic"><st c="35628">requirements</st></em><st c="35640"> section:</st><pre class="source-code"><st c="35649">
X = pd.read_csv("occupancy.csv", parse_dates=["date"])
y = pd.read_csv("occupancy_target.csv",
    index_col="id")["occupancy"]</st></pre><p class="list-inset"><code><st c="35773">tsfresh</st></code><st c="35781"> includes </st><a id="_idIndexMarker798"/><st c="35791">three main dictionaries that control the feature creation output: </st><code><st c="35857">settings.ComprehensiveFCParameters</st></code><st c="35891">, </st><code><st c="35893">settings.EfficientFCParameters</st></code><st c="35923">, and </st><code><st c="35929">settings.MinimalFCParameters</st></code><st c="35957">. Here, we’ll explore the dictionary that returns the fewest features. </st><st c="36028">You can repeat the steps to explore the </st><st c="36068">additional dictionaries.</st></p></li>				<li><st c="36092">Display the feature creation methods that will be applied when using the dictionary that returns the </st><st c="36194">fewest features:</st><pre class="source-code"><st c="36210">
minimal_feat = settings.MinimalFCParameters()
minimal_feat.items()</st></pre><p class="list-inset"><st c="36277">In the output of </st><em class="italic"><st c="36295">step 3</st></em><st c="36301">, we see a dictionary with the feature extraction method names as keys, and the parameters used by those methods, if any, </st><st c="36423">as values:</st></p><pre class="source-code"><strong class="bold"><st c="36433">ItemsView({'sum_values': None, 'median': None, 'mean': None, 'length': None, 'standard_deviation': None, 'variance': None, 'root_mean_square': None, 'maximum': None, 'absolute_maximum': None, 'minimum': None})</st></strong></pre></li>			</ol>
			<p class="callout-heading"><st c="36643">Note</st></p>
			<p class="callout"><st c="36648">Go ahead and explore the other two predefined dictionaries, </st><code><st c="36709">settings.ComprehensiveFCParameters</st></code><st c="36743"> and </st><code><st c="36748">settings.EfficientFCParameters</st></code><st c="36778">, by ad</st><a id="_idTextAnchor1409"/><st c="36785">apting the code from </st><em class="italic"><st c="36807">step 3</st></em><st c="36813">.</st></p>
			<ol>
				<li value="4"><st c="36814">Now, let’s use the dictionary from </st><em class="italic"><st c="36850">step 3</st></em><st c="36856"> to extract only those features from the </st><code><st c="36897">light</st></code><st c="36902"> time</st><a id="_idIndexMarker799"/><st c="36907"> series and then display the shape of the </st><st c="36949">resulting DataFrame:</st><pre class="source-code"><st c="36969">
features = extract_features(
    X[[«id», «light»]],
    column_id="id",
    </st><strong class="bold"><st c="37035">default_fc_parameters=minimal_feat,</st></strong><st c="37070">
)
features.shape</st></pre><p class="list-inset"><st c="37087">The output of </st><em class="italic"><st c="37102">step 4</st></em><st c="37108"> is </st><code><st c="37112">(135, 10)</st></code><st c="37121">, which means that only 10 features were created for each of th</st><a id="_idTextAnchor1410"/><st c="37184">e 135 hours of light </st><st c="37206">consumption data.</st></p></li>				<li><st c="37223">Let’s display the </st><st c="37242">resulting DataFrame:</st><pre class="source-code"><st c="37262">
features.head()</st></pre><p class="list-inset"><st c="37278">We see the values of the resulting features for the first five hours of lig</st><a id="_idTextAnchor1411"/><st c="37354">ht consumption in the </st><st c="37377">following DataFrame:</st></p></li>			</ol>
			<div><div><img src="img/B22396_10_07.jpg" alt="Figure 10.7 – DataFrame with the features created ﻿for each hour of light consumption"/><st c="37397"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="37945">Figure 10.7 – DataFrame with the features created </st><a id="_idTextAnchor1412"/><st c="37995">for each hour of light consumption</st></p>
			<p class="list-inset"><a id="_idTextAnchor1413"/><st c="38029">Now, we will use these features to train a logistic regression model to predict whether </st><a id="_idIndexMarker800"/><st c="38118">the office </st><st c="38129">was occupied.</st></p>
			<ol>
				<li value="6"><st c="38142">Let’s begin by separating the dataset into training and </st><st c="38199">test sets:</st><pre class="source-code"><st c="38209">
X_train, X_test, y_train, y_test = train_test_split(
    features,
    y,
    test_size=0.1,
    random_state=42,
)</st></pre></li>				<li><st c="38309">Now, let’s set up and train a logistic regression model, and then evaluate </st><st c="38385">its performance:</st><pre class="source-code"><st c="38401">
cls = LogisticRegression(random_state=10, C=0.01)
cls.fit(X_train, y_train)
print(classification_repor</st><a id="_idTextAnchor1414"/><st c="38504">t(
    y_test, cls.p</st><a id="_idTextAnchor1415"/><st c="38521">redict(X_test)))</st></pre><p class="list-inset"><st c="38538">In the following output, we see the evaluation metrics that are commonly used for classification analysis. </st><st c="38646">These suggest that the selected features are useful for predicting </st><st c="38713">office occupancy:</st></p><pre class="source-code"><strong class="bold"><st c="38730">                    precision     recall  f1-score   support</st></strong>
<strong class="bold"/><strong class="bold"><st c="38764">0         1.00        0.91        0.95           11</st></strong>
<strong class="bold"><st c="38784">               1         0.75        1.00        0.86            3</st></strong>
<strong class="bold"><st c="38803">     accuracy                                       0.93           14</st></strong>
<strong class="bold"><st c="38820">   macro avg         0.88        0.95        0.90           14</st></strong>
<strong class="bold"><st c="38848">weighted avg         0.95        0.93</st></strong><strong class="bold"><st c="38871">        0.93           14</st></strong></pre></li>			</ol>
			<p class="callout-heading"><st c="38879">Note</st></p>
			<p class="callout"><st c="38884">Because light consumption is a very good indicator of office occupancy, with very simple features, we can obtain a predictive logistic </st><st c="39020">regression model.</st></p>
			<p class="list-inset"><st c="39037">Now, let’s </st><a id="_idIndexMarker801"/><st c="39049">learn how to specify the creation of different features for different </st><st c="39119">time series.</st></p>
			<ol>
				<li value="8"><st c="39131">Let’s create a dictionary with the names of the methods that we want to use to create features from the </st><code><st c="39236">light</st></code><st c="39241"> time series. </st><st c="39255">We enter the method’s names as keys, and if the methods take a parameter, we pass it as an additional dictionary to the corresponding key; otherwise, we pass </st><code><st c="39413">None</st></code><st c="39417"> as </st><st c="39421">the values:</st><pre class="source-code"><st c="39432">
light_feat = {
    «sum_values": None,
    "median": None,
    «standard_deviation": None,
    "quan</st><a id="_idTextAnchor1416"/><st c="39517">tile": [{"q": 0.2}, {"q":</st><a id="_idTextAnchor1417"/><st c="39543"> 0.7}],
}</st></pre></li>				<li><st c="39552">Now, let’s create a dictionary with the features that we want to create from the </st><code><st c="39634">co2</st></code> <st c="39637">time series:</st><pre class="source-code"><st c="39650">
co2_feat = {
    «root_mean_square": None,
    «number_peaks": [{"n": 1}, {"n": 2}],
}</st></pre></li>				<li><st c="39729">Let’s</st><a id="_idIndexMarker802"/><st c="39735"> combine these dictionaries into a </st><st c="39770">new dictionary:</st><pre class="source-code"><st c="39785">
kind_to_fc_parameters = {
    «light»: light_feat,
    "co2": co2_feat,
}</st></pre></li>				<li><st c="39851">Finally, let’s use the dictionary from </st><em class="italic"><st c="39891">step 10</st></em><st c="39898"> to create the features from both </st><st c="39932">time series:</st><pre class="source-code"><st c="39944">
features = extract_features(
    X[[«id», «light», «co2»]],
    column_id="id",
    kind_to_fc_parameters=kind_to_fc_parameters,
)</st></pre><p class="list-inset"><st c="40063">The output of </st><em class="italic"><st c="40078">step 11</st></em><st c="40085"> consists of a DataFrame with 135 rows and 8 features. </st><st c="40140">If we execute </st><code><st c="40154">features.columns</st></code><st c="40170">, we will see the names of the </st><st c="40201">created features:</st></p><pre class="source-code"><strong class="bold"><st c="40218">Index(['light__sum_values', 'light__median',</st></strong>
<strong class="bold"><st c="40263">    'light__standard_deviation',</st></strong>
<strong class="bold"><st c="40292">    'light__quantile__q_0.2',</st></strong>
<strong class="bold"><st c="40318">    'light__quantile__q_0.7',</st></strong>
<strong class="bold"><st c="40344">    'co2__root_mean_square',</st></strong>
<strong class="bold"><st c="40369">    'co2__number_peaks__n_1',</st></strong>
<strong class="bold"><st c="40395">    'co2__numb</st><a id="_idTextAnchor1418"/><st c="40406">er_peaks__n_2'],</st></strong>
<strong class="bold"><st c="40423">    dtype='object')</st></strong></pre></li>			</ol>
			<p><st c="40439">N</st><a id="_idTextAnchor1419"/><st c="40441">ote that in the output from </st><em class="italic"><st c="40469">step 11</st></em><st c="40476">, different variables have been create</st><a id="_idTextAnchor1420"/><a id="_idTextAnchor1421"/><st c="40514">d from each of the </st><code><st c="40534">light</st></code><st c="40539"> and </st><code><st c="40544">co2</st></code> <st c="40547">time series.</st></p>
			<h2 id="_idParaDest-289"><a id="_idTextAnchor1422"/><st c="40560">How it works...</st></h2>
			<p><st c="40576">In this </st><a id="_idIndexMarker803"/><st c="40585">recipe, we extracted specific features from our time-series data. </st><st c="40651">First, we created features based on a predefined dictionary that comes with </st><code><st c="40727">tsfresh</st></code><st c="40734">. Next, we created our own dictionary, specifying the creation of different features for different </st><st c="40833">time series.</st></p>
			<p><st c="40845">The </st><code><st c="40850">tsfresh</st></code><st c="40857"> package comes with some predefined dictionaries that can be accessed through the </st><code><st c="40939">settings</st></code><st c="40947"> module. </st><st c="40956">The </st><code><st c="40960">MinimalFCParameters</st></code><st c="40979"> dictionary is used to create 10 simple features based on basic statistical parameters of the time-series distribution, such as the mean, median, standard deviation, variance, sum of its values, count (or length), and minimum and maximum values. </st><st c="41225">In </st><em class="italic"><st c="41228">step 3</st></em><st c="41234">, we displayed the dictionary, with the method names as keys, and, as these methods do not require additional parameters, each key had </st><code><st c="41369">None</st></code><st c="41373"> as </st><st c="41377">the value.</st></p>
			<p><code><st c="41387">tsfresh</st></code><st c="41395"> has two additional predefined dictionaries. </st><code><st c="41440">EfficientFCParameters</st></code><st c="41461"> is used to apply methods that are fast to compute, whereas </st><code><st c="41521">ComprehensiveFCParameters</st></code><st c="41546"> returns all possible features and is the one used by default by the </st><code><st c="41615">extract_</st><a id="_idTextAnchor1423"/><st c="41623">features</st></code><st c="41632"> function.</st></p>
			<p class="callout-heading"><st c="41642">Note</st></p>
			<p class="callout"><st c="41647">For more details about the predefined dictionaries, check out </st><code><st c="41710">tsfresh</st></code><st c="41717">’s </st><st c="41721">documentation: </st><a href="https://tsfresh.readthedocs.io/en/latest/text/feature_extraction_settings.html"><st c="41736">https://tsfresh.readthedocs.io/en/latest</st><st c="41776">/text/feature_extraction_setti</st><st c="41807">ngs.html</st></a></p>
			<p><st c="41816">By using these predefined dictionaries in the </st><code><st c="41863">default_fc_parameters</st></code><st c="41884"> parameter of </st><code><st c="41898">tsfresh</st></code><st c="41905">’s </st><code><st c="41909">extract_features</st></code><st c="41925"> function, we can create specific features from one or more time series, as we did in </st><em class="italic"><st c="42011">step 4</st></em><st c="42017">. Note that </st><code><st c="42029">default_fc_parameters</st></code><st c="42050"> instructs </st><code><st c="42061">extract_features</st></code><st c="42077"> to create the same features from </st><em class="italic"><st c="42111">all</st></em><st c="42114"> the time series. </st><st c="42132">What if we want to extract different features from different </st><st c="42193">time series?</st></p>
			<p><st c="42205">To create different features for different time series, we can use the </st><code><st c="42277">kind_to_fc_parameters</st></code><st c="42298"> parameter of </st><code><st c="42312">tsfresh</st></code><st c="42319">’s </st><code><st c="42323">extract_features</st></code><st c="42339"> function. </st><st c="42350">This parameter takes a dictionary of dictionaries, specifying the methods to apply to each </st><st c="42441">time series.</st></p>
			<p><st c="42453">In </st><em class="italic"><st c="42457">step 8</st></em><st c="42463">, we created a dictionary to specify the creation of specific features from the </st><code><st c="42543">light</st></code><st c="42548"> time series. </st><st c="42562">Note that the </st><code><st c="42576">"sum_values"</st></code><st c="42588"> and </st><code><st c="42593">"mean"</st></code><st c="42599"> methods take </st><code><st c="42613">None</st></code><st c="42617"> as values, but the </st><code><st c="42637">quantile</st></code><st c="42645"> method needs additional parameters corresponding to the quantiles that should be returned from the time series. </st><st c="42758">In </st><em class="italic"><st c="42761">step 9</st></em><st c="42767">, we created a dictionary to specify the creation of features from the </st><code><st c="42838">co2</st></code><st c="42841"> time series. </st><st c="42855">In </st><em class="italic"><st c="42858">step 10</st></em><st c="42865">, we combined both dictionaries into one that takes the name of the time series as the key and the feature creation dictionaries as values. </st><st c="43005">Then, we passed this dictionary to the </st><code><st c="43044">kind_to_fc_parameters</st></code><st c="43065">  parameter of </st><code><st c="43079">tsfresh</st></code><st c="43086">’s </st><code><st c="43090">extract_features</st></code><st c="43106"> function. </st><st c="43117">This way of specifying features is suitable if we use domain knowledge to create the features, or if we only create a small number </st><st c="43248">of features.</st></p>
			<p><st c="43260">Do we need to type each method by hand into a dictionary if we want to create multiple features for various</st><a id="_idIndexMarker804"/><st c="43368"> time series? </st><st c="43382">Not really. </st><st c="43394">In the following recipe, we will learn how to specify which features</st><a id="_idTextAnchor1426"/><a id="_idTextAnchor1427"/><st c="43462"> to create based on features selected </st><st c="43500">by Lasso.</st></p>
			<h1 id="_idParaDest-290"><a id="_idTextAnchor1428"/><st c="43509">Creating a subset of features identified through feature selection</st></h1>
			<p><st c="43576">In the </st><em class="italic"><st c="43584">Automatically creating and selecting predictive features from time-series data</st></em><st c="43662"> recipe, we</st><a id="_idIndexMarker805"/><st c="43673"> learned </st><a id="_idIndexMarker806"/><st c="43682">how to select relevant features using </st><code><st c="43720">tsfresh</st></code><st c="43727">. We also discussed the limitations of </st><code><st c="43766">tsfresh</st></code><st c="43773">’s selection procedures and suggested following up with alternative feature selection methods to identify predictive featu</st><a id="_idTextAnchor1429"/><st c="43896">res while </st><st c="43907">avoiding redundancy.</st></p>
			<p><st c="43927">In this recipe, we will create and select features using </st><code><st c="43985">tsfresh</st></code><st c="43992">. Following that, we will reduce the feature space further by utilizing Lasso regularization. </st><st c="44086">Then, we will learn how to create a dictionary from the selected feature names to trigger the creatio</st><a id="_idTextAnchor1430"/><a id="_idTextAnchor1431"/><st c="44187">n of those features </st><em class="italic"><st c="44208">only</st></em><st c="44212"> from future </st><st c="44225">time series.</st></p>
			<h2 id="_idParaDest-291"><a id="_idTextAnchor1432"/><st c="44237">How to do it...</st></h2>
			<p><st c="44253">Let’s begin by importing the necessary libraries and getting the </st><st c="44319">dataset ready:</st></p>
			<ol>
				<li><st c="44333">Let’s import </st><a id="_idIndexMarker807"/><st c="44347">the</st><a id="_idIndexMarker808"/><st c="44350"> required libraries </st><st c="44370">and functions:</st><pre class="source-code"><st c="44384">
import pandas as pd
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LogisticRegression
from tsfresh import (
    extract_features,
    extract_relevant_features,
)
from tsfresh.feature_extraction import settings</st></pre></li>				<li><st c="44627">Load the Occupancy Detection dataset described in the </st><em class="italic"><st c="44682">Technical </st></em><em class="italic"><st c="44692">requirements</st></em><st c="44704"> section:</st><pre class="source-code"><st c="44713">
X = pd.read_csv("occupancy.csv", parse_dates=["date"])
y = pd.read_csv(
    "occupancy_target.csv",
    index_col="id")["occupancy"]</st></pre></li>				<li><st c="44838">Create and select features from our five time series and then display the shape of the </st><st c="44926">resulting DataFrame:</st><pre class="source-code"><st c="44946">
features = extract_relevant_features(
    X,
    y,
    column_id="id",
    column_sort="date",
)
features.shape</st></pre><p class="list-inset"><st c="45043">The output of </st><em class="italic"><st c="45058">step 3</st></em><st c="45064"> is </st><code><st c="45068">(135, 968)</st></code><st c="45078">, indicating that 968 features were returned from the five original time series, for each hour </st><st c="45173">of records.</st></p></li>			</ol>
			<p class="callout-heading"><st c="45184">Note</st></p>
			<p class="callout"><st c="45189">We discussed the function from </st><em class="italic"><st c="45221">step 3</st></em><st c="45227"> in the </st><em class="italic"><st c="45235">Automatically creating and selecting pred</st><a id="_idTextAnchor1433"/><st c="45276">ictive features from time-series </st></em><em class="italic"><st c="45310">data</st></em><st c="45314"> recipe.</st></p>
			<p class="list-inset"><st c="45322">Let’s reduce </st><a id="_idIndexMarker809"/><st c="45336">the </st><a id="_idIndexMarker810"/><st c="45340">feature space further by selecting features with </st><st c="45389">Lasso regularization.</st></p>
			<ol>
				<li value="4"><st c="45410">Set up logistic regression with Lasso regularization, which is the </st><code><st c="45478">"l1"</st></code><st c="45482"> penalty. </st><st c="45492">I also set some additional </st><st c="45519">parameters arbitrarily:</st><pre class="source-code"><st c="45542">
cls = LogisticRegression(
    penalty="l1",
    solver=»liblinear",
    random_state=10,
    C=0.05,
    max_iter=1000,
)</st></pre></li>				<li><st c="45644">Let’s set up a transformer to retain those features whose logistic regression coefficients are different </st><st c="45750">from 0:</st><pre class="source-code"><st c="45757">
selector = SelectFromModel(cls)</st></pre></li>				<li><st c="45789">Train the logistic regression model and select </st><st c="45837">the features:</st><pre class="source-code"><st c="45850">
selector.fit(features, y)</st></pre></li>				<li><st c="45876">Now, capture the selected features in </st><st c="45915">a variable:</st><pre class="source-code"><st c="45926">
features = selector.get_feature_names_out()</st></pre><p class="list-inset"><st c="45970">If we</st><a id="_idIndexMarker811"/><st c="45976"> execute </st><code><st c="45985">features</st></code><st c="45993">, we’ll </st><a id="_idIndexMarker812"/><st c="46001">see the names of the </st><st c="46022">selected features:</st></p><pre class="source-code"><st c="46040">array([
'light__sum_of_reoccurring_data_points',
'co2__fft_coefficient__attr_"abs"__coeff_0',
'co2__spkt_welch_density__coeff_2', 'co2__variance',
'temperature__c3__lag_1', 'temperature__abs_energy',
'temperature__c3__lag_2', 'temperature__c3__lag_3',
'co2__sum_of_reoccurring_data_points',
'light__spkt_welch_density__coeff_8',
'light__agg_linear_trend__attr_"intercept"__chunk_len_50__f_agg_"var"',
         'light__agg_linear_trend__attr_"slope"__chunk_len_50__f_agg_"var"',  'light__agg_linear_trend__attr_"intercept"__chunk_len_10__f_agg_"var"'],
dtype=object)</st></pre></li>				<li><st c="46596">To extract just the features from </st><em class="italic"><st c="46631">step 6</st></em><st c="46637"> from the time series, we need to capture the feature creation method name and corresponding parameters in a dictionary. </st><st c="46758">We can do this automatically from the feature names </st><st c="46810">with </st><code><st c="46815">tsfresh</st></code><st c="46822">:</st><pre class="source-code"><st c="46824">
kind_to_fc_parameters = settings.from_columns(
    selector.get_feature_names_out(),
)</st></pre><p class="list-inset"><st c="46907">If we execute </st><code><st c="46922">kind_to_fc_parameters</st></code><st c="46943">, we’ll see the dictionary that was created from the </st><a id="_idIndexMarker813"/><st c="46996">names of the features from </st><em class="italic"><st c="47023">step 6</st></em><st c="47029">:</st></p><pre class="source-code"><st c="47031">{'light':
    {‹sum_of_reoccurring_data_points': None,
    ‹spkt_welch_density': [{'coeff': 8}],
    'variance': None,
    ‹agg_linear_trend': [
        {‹attr': 'slope','chunk_len': 50,
            'f_agg': 'var'},
        {‹attr': 'intercept',
            'chunk_len': 10,'f_agg':'var'}
        ]
    },
'co2':
    {‹spkt_welch_density': [{'coeff': 2}],
    'variance': None,
    ‹sum_of_reoccurring_data_points': None
    },
    'temperature': {
        'c3': [{'lag': 1}, {'la</st><a id="_idTextAnchor1434"/><st c="47415">g': 2}, {'lag':3}],
        'abs_energy': None}
}</st></pre></li>				<li><st c="47457">Now, we can use</st><a id="_idIndexMarker814"/><st c="47473"> the dictionary from </st><em class="italic"><st c="47494">step 8</st></em><st c="47500"> together with the </st><code><st c="47519">extract_features</st></code><st c="47535"> function to create only those features from </st><st c="47580">our dataset:</st><pre class="source-code"><st c="47592">
features = extract_features(
    X,
    column_id="id",
    column_sort="date",
    kind_to_fc_parameters=kind_to_fc_parameters,
)</st></pre></li>			</ol>
			<p><st c="47707">The new DataFrame, which can be displayed by executing </st><code><st c="47763">features.head()</st></code><st c="47778">, only contains the 12 features that were selected by Las</st><a id="_idTextAnchor1435"/><a id="_idTextAnchor1436"/><st c="47835">so. </st><st c="47840">Go ahead and corroborate the re</st><a id="_idTextAnchor1437"/><st c="47871">sult on </st><st c="47880">your computer.</st></p>
			<h2 id="_idParaDest-292"><a id="_idTextAnchor1438"/><st c="47894">How it works...</st></h2>
			<p><st c="47910">In this recipe, we </st><a id="_idIndexMarker815"/><st c="47930">created 968 features </st><a id="_idIndexMarker816"/><st c="47951">from 5 time series. </st><st c="47971">Next, we reduced the feature space to 12 features by using Lasso regularization. </st><st c="48052">Finally, we captured the specifications of the selected features in a dictionary so that, looking forward, we only created those features from our </st><st c="48199">time series.</st></p>
			<p><st c="48211">To automatically create and select features with </st><code><st c="48261">tsfresh</st></code><st c="48268">, we used the </st><code><st c="48282">extract_relevant_features</st></code><st c="48307"> function, which we described in detail in the </st><em class="italic"><st c="48354">Automatically creating and selecting predictive features from time-series </st></em><em class="italic"><st c="48428">data</st></em><st c="48432"> recipe.</st></p>
			<p><st c="48440">Lasso regularization has the intrinsic ability to reduce some of the coefficients of the logistic regression model to 0. </st><st c="48562">The contribution of the features whose coefficient is 0 to the prediction of office occupancy is null and can therefore be removed. </st><st c="48694">The </st><code><st c="48698">SelectFromModel()</st></code><st c="48715"> class can identify and remove those features. </st><st c="48762">We set up an instance of </st><code><st c="48787">SelectFromModel()</st></code><st c="48804"> with a logistic regression model that used Lasso regularization to find the model coefficients. </st><st c="48901">With </st><code><st c="48906">fit()</st></code><st c="48911">, </st><code><st c="48913">SelectFromModel()</st></code><st c="48930"> trained the logistic regression model using the 968 features created from our time series and identified those whose coefficients were different from 0. </st><st c="49084">Then, with the </st><code><st c="49099">get_feature_names_out()</st></code><st c="49122"> method, we captured the names of the selected features in a </st><st c="49183">new variable.</st></p>
			<p><st c="49196">To create only the 12 features selected by Lasso regularization, we created a dictionary from the variable names by using the </st><code><st c="49323">from_columns()</st></code><st c="49337"> function from </st><code><st c="49352">tsfresh</st></code><st c="49359">. This function returned a dictionary with the variables from which features were selected as keys. </st><st c="49459">The values were additional dictionaries, containing the methods used to create features as keys, and the parameters used, if any, as values. </st><st c="49600">To create the new features, we used this </st><a id="_idIndexMarker817"/><st c="49641">dictionary </st><a id="_idIndexMarker818"/><st c="49652">together with the </st><code><st c="49670">extract_features</st></code><st c="49686"> function.</st></p>
			<p class="callout-heading"><st c="49696">Note</st></p>
			<p class="callout"><st c="49701">In </st><em class="italic"><st c="49705">step 9</st></em><st c="49711">, we passed the entire dataset </st><a id="_idIndexMarker819"/><st c="49742">to the </st><code><st c="49749">extract_features</st></code><st c="49765"> function. </st><st c="49776">The resulting features only contained features extracted from three of the </st><a id="_idTextAnchor1439"/><a id="_idTextAnchor1440"/><st c="49851">five time series. </st><st c="49869">The additional two time series </st><st c="49900">were ignored.</st></p>
			<h1 id="_idParaDest-293"><st c="49913">Embe</st><a id="_idTextAnchor1441"/><st c="49918">dding feature creation into a scikit-learn pipeline</st></h1>
			<p><st c="49970">Throughout this</st><a id="_idIndexMarker820"/><st c="49986"> chapter, we’ve discussed how to automatical</st><a id="_idTextAnchor1442"/><st c="50030">ly create and select features from time-series data by utilizing </st><code><st c="50096">tsfresh</st></code><st c="50103">. Then, we used these features to train a classification model to predict whether an office was occupied at any </st><st c="50215">given hour.</st></p>
			<p><code><st c="50226">tsfresh</st></code><st c="50234"> includes </st><em class="italic"><st c="50244">wrapper</st></em><st c="50251"> classes around its main functions, </st><code><st c="50287">extract_features</st></code><st c="50303"> and </st><code><st c="50308">extract_relevant_features</st></code><st c="50333">, to make the creation and selection of features compatible with the </st><st c="50402">scikit-learn pipeline.</st></p>
			<p><st c="50424">In this recipe, we will set up a scikit-learn pipeline that extracts features from time series using </st><code><st c="50526">tsfresh</st></code><st c="50533"> and then trains a logistic re</st><a id="_idTextAnchor1443"/><a id="_idTextAnchor1444"/><st c="50563">gression model with those features to predict </st><st c="50610">office occupancy.</st></p>
			<h2 id="_idParaDest-294"><a id="_idTextAnchor1445"/><st c="50627">How to do it...</st></h2>
			<p><st c="50643">Let’s begin by importing the necessary libraries and getting the </st><st c="50709">dataset ready:</st></p>
			<ol>
				<li><st c="50723">Let’s import the required libraries </st><st c="50760">and functions:</st><pre class="source-code"><st c="50774">
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from tsfresh.transformers import (
    RelevantFeatureAugmenter)</st></pre></li>				<li><st c="51048">Load the Occupancy Detection dataset described in the </st><em class="italic"><st c="51103">Technical </st></em><em class="italic"><st c="51113">requirements</st></em><st c="51125"> section:</st><pre class="source-code"><st c="51134">
X = pd.read_csv("occupancy.csv", parse_dates=["date"])
y = pd.read_csv(
    "occupancy_target.csv",
    index_col="id")["occupancy"]</st></pre></li>				<li><st c="51259">Create an empty DataFrame that contains the index of the </st><st c="51317">target variable:</st><pre class="source-code"><st c="51333">
tmp = pd.DataFrame(index=y.index)</st></pre></li>				<li><st c="51367">Now, let’s split</st><a id="_idIndexMarker821"/><st c="51384"> the DataFrame from </st><em class="italic"><st c="51404">step 3</st></em><st c="51410"> and the target from </st><em class="italic"><st c="51431">step 2</st></em><st c="51437"> into training and </st><st c="51456">test sets:</st><pre class="source-code"><st c="51466">
X_train, X_test, y_train, y_test = train_test_split(
    tmp, y, random_state=0)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="51543">Note</st></p>
			<p class="callout"><code><st c="51548">X_train</st></code><st c="51556"> and </st><code><st c="51561">X_test</st></code><st c="51567"> will be used as containers to store the features created by </st><code><st c="51628">tsfresh</st></code><st c="51635">. They are needed for the functionality of </st><code><st c="51678">RelevantFe</st><a id="_idTextAnchor1446"/><st c="51688">atureAugmenter()</st></code><st c="51705"> that we will discuss in the </st><st c="51734">coming steps.</st></p>
			<ol>
				<li value="5"><st c="51747">Let’s create a dictionary specifying the features to extract from each time series (I defined the following </st><st c="51856">features arbitrarily):</st><pre class="source-code"><st c="51878">
kind_to_fc_parameters = {
    "light": {
        "c3": [{"lag": 3}, {"lag": 2}, {"lag": 1}],
        «abs_energy": None,
        «sum_values": None,
        «fft_coefficient": [
            {«attr": "real", "coeff": 0},
            {«attr": "abs", "coeff": 0}],
        «spkt_welch_density": [
            {«coeff": 2}, {"coeff":5}, {"coeff": 8}
        ],
        «agg_linear_trend": [
            {«attr": "intercept",
            „chunk_len": 50, „f_agg": „var"},
            {"attr": "slope",
            «chunk_len": 50, "f_agg":"var"},
        ],
        «change_quantiles": [
            {«f_agg": "var", "isabs": False,
            «qh": 1.0,"ql": 0.8},
            {«f_agg": "var", "isabs": True,
            «qh": 1.0,"ql": 0.8},
        ],
    },
"co2": {
    «fft_coefficient": [
        {«attr": "real", "coeff": 0},
        {«attr": "abs", "coeff": 0}],
    "c3": [{"lag": 3}, {"lag": 2}, {"lag": 1}],
    «sum_values": None,
    «abs_energy": None,
    «sum_of_reoccurring_data_points": None,
    «sum_of_reoccurring_values": None,
    },
"temperature": {"c3": [{"lag": 1},
    {«lag»: 2},{«lag»: 3}], «abs_energy": None},
}</st></pre><p class="list-inset"><st c="52750">We discussed the parameters of this dictionary in the </st><em class="italic"><st c="52805">Extrac</st><a id="_idTextAnchor1447"/><st c="52811">ting different features from different ti</st><a id="_idTextAnchor1448"/><st c="52853">me </st></em><em class="italic"><st c="52857">series</st></em><st c="52863"> recipe.</st></p></li>				<li><st c="52871">Let’s set </st><a id="_idIndexMarker822"/><st c="52882">up </st><code><st c="52885">RelevantFeatureAugmenter()</st></code><st c="52911">, which is a wrapper around the </st><code><st c="52943">extract_relevant_features</st></code><st c="52968"> function, to create the features specified in </st><em class="italic"><st c="53015">step 5</st></em><st c="53021">:</st><pre class="source-code"><st c="53023">
augmenter = RelevantFeatureAugmenter(
    column_id="id",
    column_sort="date",
    kind_to_fc_parameters=kind_to_fc_parameters,
)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="53144">Note</st></p>
			<p class="callout"><st c="53149">To create all possible features, use the </st><code><st c="53191">FeatureAugmenter()</st></code><st c="53209"> class instead in </st><em class="italic"><st c="53227">step 6</st></em><st c="53233">.</st></p>
			<ol>
				<li value="7"><st c="53234">Let’s combine the feature creation instance from </st><em class="italic"><st c="53284">step 6</st></em><st c="53290"> with a logistic regression model in a </st><st c="53329">scikit-learn pipeline:</st><pre class="source-code"><st c="53351">
pipe = Pipeline(
    [
        ("augmenter", augmenter),
        («classifier», LogisticRegression(
    random_state=10, C=0.01)),
    ]
)</st></pre></li>				<li><st c="53462">Now, let’s</st><a id="_idTextAnchor1449"/><st c="53473"> tell </st><code><st c="53479">RelevantFeatureAugmenter()</st></code><st c="53505"> which dataset it needs to use to create </st><st c="53546">the feat</st><a id="_idTextAnchor1450"/><st c="53554">ures:</st><pre class="source-code"><st c="53560">
pipe.set_params(augmenter__timeseries_container=X)</st></pre></li>				<li><st c="53611">Let’s fit the pipeline, which will trigger the feature creation process, followed by the training of the logistic </st><st c="53726">regression model:</st><pre class="source-code"><st c="53743">
pipe.fit(X_train, y_train)</st></pre></li>				<li><st c="53770">Now, let’s obtain predictions using the time series in the test set and evaluate the model’s performance through a </st><st c="53886">classification report:</st><pre class="source-code"><st c="53908">
print(classification_report(
    y_test, pipe.predict(X_test)))</st></pre><p class="list-inset"><st c="53968">We can see the </st><a id="_idIndexMarker823"/><st c="53984">output of </st><em class="italic"><st c="53994">step </st></em><em class="italic"><st c="53999">10</st></em><st c="54001"> here:</st></p><pre class="source-code"><strong class="bold"><st c="54007">                  precision     recall  f1-score   support</st></strong>
<strong class="bold"><st c="54041">               0         1.00        0.96        0.98</st></strong><strong class="bold"><st c="54058">           28</st></strong>
<strong class="bold"><st c="54061">               1         0.86        1.00        0.92            6</st></strong>
<strong class="bold"><st c="54080">     accuracy                                       0.97           34</st></strong>
<strong class="bold"/><strong class="bold"><st c="54097">macro avg         0.93        0.98        0.95           34</st></strong>
<strong class="bold"><st c="54125">weighted avg         0.97        0.97        0.97           34</st></strong></pre></li>			</ol>
			<p><st c="54156">The values of the classification report suggest that the extracted features are suitabl</st><a id="_idTextAnchor1451"/><a id="_idTextAnchor1452"/><st c="54244">e for predicting whether the office is occupied at any </st><st c="54300">given hour.</st></p>
			<h2 id="_idParaDest-295"><a id="_idTextAnchor1453"/><st c="54311">How it works...</st></h2>
			<p><st c="54327">In this recipe, we combined creating features from a time series with </st><code><st c="54398">tsfresh</st></code><st c="54405"> with training a machine learning algorith</st><a id="_idTextAnchor1454"/><st c="54447">m from the scikit-learn library in </st><st c="54483">a pip</st><a id="_idTextAnchor1455"/><st c="54488">eline.</st></p>
			<p><st c="54495">The </st><code><st c="54500">tsfresh</st></code><st c="54507"> library includes two wrapper classes around its main functions to make the feature creation process compatible with the scikit-learn pipeline. </st><st c="54651">In this recipe, we used the </st><code><st c="54679">RelevantFeatureAugmenter()</st></code><st c="54705"> class, which wraps the </st><code><st c="54729">extract_relevant_features</st></code><st c="54754"> function to create and then select features from a </st><st c="54806">time series.</st></p>
			<p><code><st c="54818">RelevantFeatureAugmenter()</st></code><st c="54845"> works as follows; with </st><code><st c="54869">fit()</st></code><st c="54874">, it creates and selects features by using </st><code><st c="54917">extract_relevant_features</st></code><st c="54942">. The names of the selected features are then stored internally in the transformer. </st><st c="55026">With </st><code><st c="55031">transform()</st></code><st c="55042">, </st><code><st c="55044">RelevantFeatureAugmenter()</st></code><st c="55070"> creates the selected features from the </st><st c="55110">time series.</st></p>
			<p><st c="55122">We overrode the default </st><a id="_idIndexMarker824"/><st c="55147">functionality of </st><code><st c="55164">RelevantFeatureAugmenter()</st></code><st c="55190"> by passing a dictionary with the features we wanted to create to its </st><code><st c="55260">kind_to_fc_parameters</st></code><st c="55281"> parameter. </st><st c="55293">Therefore, with </st><code><st c="55309">transform()</st></code><st c="55320">, </st><code><st c="55322">RelevantFeatureAugmenter()</st></code><st c="55348"> created the indicated features from the </st><st c="55389">time series.</st></p>
			<p><st c="55401">To create all features from the time series, </st><code><st c="55447">tsfresh</st></code><st c="55454"> includes the </st><code><st c="55468">FeatureAugmenter()</st></code><st c="55486"> class, which has the same functionality as </st><code><st c="55530">RelevantFeatureAugmenter()</st></code><st c="55556">, but without the feature </st><st c="55582">selection step.</st></p>
			<p><st c="55597">Both </st><code><st c="55603">RelevantFeatureAugmenter()</st></code><st c="55629"> and </st><code><st c="55634">FeatureAugmenter()</st></code><st c="55652"> need two DataFrames to work. </st><st c="55682">The first DataFrame contains the time-series data and the unique identifiers (we loaded this DataFrame in </st><em class="italic"><st c="55788">step 2</st></em><st c="55794">). </st><st c="55798">The second DataFrame should be empty and contain the unique identifiers </st><em class="italic"><st c="55870">in its index</st></em><st c="55882"> (we created this DataFrame in </st><em class="italic"><st c="55913">step 3</st></em><st c="55919">). </st><st c="55923">The features are extracted from the first DataFrame with the time series (when applying </st><code><st c="56011">transform()</st></code><st c="56022">) and subsequently added to the second DataFrame, which is then used to train the logistic regression or obtain </st><st c="56135">its predictions.</st></p>
			<p class="callout-heading"><st c="56151">Note</st></p>
			<p class="callout"><st c="56156">The index of the empty DataFrame is used by </st><code><st c="56201">RelevantFeatureAugmenter()</st></code><st c="56227"> and </st><code><st c="56232">FeatureAugmenter()</st></code><st c="56250"> to identify the time series from which to extract the features. </st><st c="56315">Hence, when applying </st><code><st c="56336">fit()</st></code><st c="56341"> while passing </st><code><st c="56356">X_train</st></code><st c="56363">, features were extracted from time series whose </st><code><st c="56412">id</st></code><st c="56414"> value was in the training set. </st><st c="56446">After that, the model was evaluated by observing predictions made using the test set, which triggered the creation of features from time series whose </st><code><st c="56596">id</st></code><st c="56598"> value was </st><st c="56609">in </st><code><st c="56612">X_test</st></code><st c="56618">.</st></p>
			<p><st c="56619">When we used </st><code><st c="56633">fit()</st></code><st c="56638"> on the pipeline, we created features from our raw time series and trained a logistic regression model with the resulting features. </st><st c="56770">With the </st><code><st c="56779">predict()</st></code><st c="56788"> method, we created features from the test set and obtained </st><a id="_idTextAnchor1456"/><a id="_idTextAnchor1457"/><st c="56848">the predictions of the</st><a id="_idIndexMarker825"/><st c="56870"> logistic regression based on </st><st c="56900">those features.</st></p>
			<h2 id="_idParaDest-296"><a id="_idTextAnchor1458"/><st c="56915">See also</st></h2>
			<p><st c="56924">For more details about the classes </st><a id="_idIndexMarker826"/><st c="56960">and procedures used in this recipe, visit the </st><st c="57006">following links:</st></p>
			<ul>
				<li><st c="57022">The </st><code><st c="57027">tsfresh</st></code> <st c="57034">documentation: </st><a href="https://tsfresh.readthedocs.io/en/latest/api/tsfresh.transformers.html#tsfresh.transformers.relevant_feature_augmenter.RelevantFeatureAugmenter"><st c="57050">https://tsfresh.readthedocs.io/en/latest/api/tsfresh.transformers.html#tsfresh.transformers.relevant_feature_augmenter.RelevantFeatureAugmenter</st></a></li>
				<li><st c="57193">A Jupyter notebook with a </st><st c="57220">demo: </st><a href="https://github.com/blue-yonder/tsfresh/blob/main/notebooks/02%20sklearn%20Pipeline.ipynb"><st c="57226">https://github.com/blue-yonder/tsfresh/blob/main/notebooks/02%20sklearn%20Pipeline.ipynb</st></a></li>
			</ul>
		</div>
	<div></body></html>